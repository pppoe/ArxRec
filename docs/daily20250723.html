<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250722.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface\n  Reconstruction under Various Light Conditions", "author": "Qingyuan Zhou and Yuehu Gong and Weidong Yang and Jiaze Li and Yeqi Luo and Baixin Xu and Shuhao Li and Ben Fei and Ying He", "abstract": "  Novel view synthesis (NVS) and surface reconstruction (SR) are essential\ntasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks\nare often addressed independently, with GS-based rendering methods struggling\nunder diverse light conditions and failing to produce accurate surfaces, while\nGS-based reconstruction methods frequently compromise rendering quality. This\nraises a central question: must rendering and reconstruction always involve a\ntrade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian\nsplatting for Surface Reconstruction that enhances both rendering quality and\n3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS\nand the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,\nproviding precise geometry information to the 3D-GS branch. Leveraging this\ngeometry, the 3D-GS branch employs a geometry-guided illumination decomposition\nmodule that captures reflected and transmitted components, enabling realistic\nrendering under varied light conditions. Using the transmitted component as\nsupervision, the 2D-GS branch also achieves high-fidelity surface\nreconstruction. Throughout the optimization process, the 2D-GS and 3D-GS\nbranches undergo alternating optimization, providing mutual supervision. Prior\nto this, each branch completes an independent warm-up phase, with an early\nstopping strategy implemented to reduce computational costs. We evaluate MGSR\non a diverse set of synthetic and real-world datasets, at both object and scene\nlevels, demonstrating strong performance in rendering and surface\nreconstruction. Code is available at https://github.com/TsingyuanChou/MGSR.\n", "link": "http://arxiv.org/abs/2503.05182v2", "date": "2025-07-22", "relevancy": 3.4811, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7393}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.675}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MGSR%3A%202D/3D%20Mutual-boosted%20Gaussian%20Splatting%20for%20High-fidelity%20Surface%0A%20%20Reconstruction%20under%20Various%20Light%20Conditions&body=Title%3A%20MGSR%3A%202D/3D%20Mutual-boosted%20Gaussian%20Splatting%20for%20High-fidelity%20Surface%0A%20%20Reconstruction%20under%20Various%20Light%20Conditions%0AAuthor%3A%20Qingyuan%20Zhou%20and%20Yuehu%20Gong%20and%20Weidong%20Yang%20and%20Jiaze%20Li%20and%20Yeqi%20Luo%20and%20Baixin%20Xu%20and%20Shuhao%20Li%20and%20Ben%20Fei%20and%20Ying%20He%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20%28NVS%29%20and%20surface%20reconstruction%20%28SR%29%20are%20essential%0Atasks%20in%203D%20Gaussian%20Splatting%20%283D-GS%29.%20Despite%20recent%20progress%2C%20these%20tasks%0Aare%20often%20addressed%20independently%2C%20with%20GS-based%20rendering%20methods%20struggling%0Aunder%20diverse%20light%20conditions%20and%20failing%20to%20produce%20accurate%20surfaces%2C%20while%0AGS-based%20reconstruction%20methods%20frequently%20compromise%20rendering%20quality.%20This%0Araises%20a%20central%20question%3A%20must%20rendering%20and%20reconstruction%20always%20involve%20a%0Atrade-off%3F%20To%20address%20this%2C%20we%20propose%20MGSR%2C%20a%202D/3D%20Mutual-boosted%20Gaussian%0Asplatting%20for%20Surface%20Reconstruction%20that%20enhances%20both%20rendering%20quality%20and%0A3D%20reconstruction%20accuracy.%20MGSR%20introduces%20two%20branches--one%20based%20on%202D-GS%0Aand%20the%20other%20on%203D-GS.%20The%202D-GS%20branch%20excels%20in%20surface%20reconstruction%2C%0Aproviding%20precise%20geometry%20information%20to%20the%203D-GS%20branch.%20Leveraging%20this%0Ageometry%2C%20the%203D-GS%20branch%20employs%20a%20geometry-guided%20illumination%20decomposition%0Amodule%20that%20captures%20reflected%20and%20transmitted%20components%2C%20enabling%20realistic%0Arendering%20under%20varied%20light%20conditions.%20Using%20the%20transmitted%20component%20as%0Asupervision%2C%20the%202D-GS%20branch%20also%20achieves%20high-fidelity%20surface%0Areconstruction.%20Throughout%20the%20optimization%20process%2C%20the%202D-GS%20and%203D-GS%0Abranches%20undergo%20alternating%20optimization%2C%20providing%20mutual%20supervision.%20Prior%0Ato%20this%2C%20each%20branch%20completes%20an%20independent%20warm-up%20phase%2C%20with%20an%20early%0Astopping%20strategy%20implemented%20to%20reduce%20computational%20costs.%20We%20evaluate%20MGSR%0Aon%20a%20diverse%20set%20of%20synthetic%20and%20real-world%20datasets%2C%20at%20both%20object%20and%20scene%0Alevels%2C%20demonstrating%20strong%20performance%20in%20rendering%20and%20surface%0Areconstruction.%20Code%20is%20available%20at%20https%3A//github.com/TsingyuanChou/MGSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05182v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMGSR%253A%25202D/3D%2520Mutual-boosted%2520Gaussian%2520Splatting%2520for%2520High-fidelity%2520Surface%250A%2520%2520Reconstruction%2520under%2520Various%2520Light%2520Conditions%26entry.906535625%3DQingyuan%2520Zhou%2520and%2520Yuehu%2520Gong%2520and%2520Weidong%2520Yang%2520and%2520Jiaze%2520Li%2520and%2520Yeqi%2520Luo%2520and%2520Baixin%2520Xu%2520and%2520Shuhao%2520Li%2520and%2520Ben%2520Fei%2520and%2520Ying%2520He%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520%2528NVS%2529%2520and%2520surface%2520reconstruction%2520%2528SR%2529%2520are%2520essential%250Atasks%2520in%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529.%2520Despite%2520recent%2520progress%252C%2520these%2520tasks%250Aare%2520often%2520addressed%2520independently%252C%2520with%2520GS-based%2520rendering%2520methods%2520struggling%250Aunder%2520diverse%2520light%2520conditions%2520and%2520failing%2520to%2520produce%2520accurate%2520surfaces%252C%2520while%250AGS-based%2520reconstruction%2520methods%2520frequently%2520compromise%2520rendering%2520quality.%2520This%250Araises%2520a%2520central%2520question%253A%2520must%2520rendering%2520and%2520reconstruction%2520always%2520involve%2520a%250Atrade-off%253F%2520To%2520address%2520this%252C%2520we%2520propose%2520MGSR%252C%2520a%25202D/3D%2520Mutual-boosted%2520Gaussian%250Asplatting%2520for%2520Surface%2520Reconstruction%2520that%2520enhances%2520both%2520rendering%2520quality%2520and%250A3D%2520reconstruction%2520accuracy.%2520MGSR%2520introduces%2520two%2520branches--one%2520based%2520on%25202D-GS%250Aand%2520the%2520other%2520on%25203D-GS.%2520The%25202D-GS%2520branch%2520excels%2520in%2520surface%2520reconstruction%252C%250Aproviding%2520precise%2520geometry%2520information%2520to%2520the%25203D-GS%2520branch.%2520Leveraging%2520this%250Ageometry%252C%2520the%25203D-GS%2520branch%2520employs%2520a%2520geometry-guided%2520illumination%2520decomposition%250Amodule%2520that%2520captures%2520reflected%2520and%2520transmitted%2520components%252C%2520enabling%2520realistic%250Arendering%2520under%2520varied%2520light%2520conditions.%2520Using%2520the%2520transmitted%2520component%2520as%250Asupervision%252C%2520the%25202D-GS%2520branch%2520also%2520achieves%2520high-fidelity%2520surface%250Areconstruction.%2520Throughout%2520the%2520optimization%2520process%252C%2520the%25202D-GS%2520and%25203D-GS%250Abranches%2520undergo%2520alternating%2520optimization%252C%2520providing%2520mutual%2520supervision.%2520Prior%250Ato%2520this%252C%2520each%2520branch%2520completes%2520an%2520independent%2520warm-up%2520phase%252C%2520with%2520an%2520early%250Astopping%2520strategy%2520implemented%2520to%2520reduce%2520computational%2520costs.%2520We%2520evaluate%2520MGSR%250Aon%2520a%2520diverse%2520set%2520of%2520synthetic%2520and%2520real-world%2520datasets%252C%2520at%2520both%2520object%2520and%2520scene%250Alevels%252C%2520demonstrating%2520strong%2520performance%2520in%2520rendering%2520and%2520surface%250Areconstruction.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/TsingyuanChou/MGSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05182v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MGSR%3A%202D/3D%20Mutual-boosted%20Gaussian%20Splatting%20for%20High-fidelity%20Surface%0A%20%20Reconstruction%20under%20Various%20Light%20Conditions&entry.906535625=Qingyuan%20Zhou%20and%20Yuehu%20Gong%20and%20Weidong%20Yang%20and%20Jiaze%20Li%20and%20Yeqi%20Luo%20and%20Baixin%20Xu%20and%20Shuhao%20Li%20and%20Ben%20Fei%20and%20Ying%20He&entry.1292438233=%20%20Novel%20view%20synthesis%20%28NVS%29%20and%20surface%20reconstruction%20%28SR%29%20are%20essential%0Atasks%20in%203D%20Gaussian%20Splatting%20%283D-GS%29.%20Despite%20recent%20progress%2C%20these%20tasks%0Aare%20often%20addressed%20independently%2C%20with%20GS-based%20rendering%20methods%20struggling%0Aunder%20diverse%20light%20conditions%20and%20failing%20to%20produce%20accurate%20surfaces%2C%20while%0AGS-based%20reconstruction%20methods%20frequently%20compromise%20rendering%20quality.%20This%0Araises%20a%20central%20question%3A%20must%20rendering%20and%20reconstruction%20always%20involve%20a%0Atrade-off%3F%20To%20address%20this%2C%20we%20propose%20MGSR%2C%20a%202D/3D%20Mutual-boosted%20Gaussian%0Asplatting%20for%20Surface%20Reconstruction%20that%20enhances%20both%20rendering%20quality%20and%0A3D%20reconstruction%20accuracy.%20MGSR%20introduces%20two%20branches--one%20based%20on%202D-GS%0Aand%20the%20other%20on%203D-GS.%20The%202D-GS%20branch%20excels%20in%20surface%20reconstruction%2C%0Aproviding%20precise%20geometry%20information%20to%20the%203D-GS%20branch.%20Leveraging%20this%0Ageometry%2C%20the%203D-GS%20branch%20employs%20a%20geometry-guided%20illumination%20decomposition%0Amodule%20that%20captures%20reflected%20and%20transmitted%20components%2C%20enabling%20realistic%0Arendering%20under%20varied%20light%20conditions.%20Using%20the%20transmitted%20component%20as%0Asupervision%2C%20the%202D-GS%20branch%20also%20achieves%20high-fidelity%20surface%0Areconstruction.%20Throughout%20the%20optimization%20process%2C%20the%202D-GS%20and%203D-GS%0Abranches%20undergo%20alternating%20optimization%2C%20providing%20mutual%20supervision.%20Prior%0Ato%20this%2C%20each%20branch%20completes%20an%20independent%20warm-up%20phase%2C%20with%20an%20early%0Astopping%20strategy%20implemented%20to%20reduce%20computational%20costs.%20We%20evaluate%20MGSR%0Aon%20a%20diverse%20set%20of%20synthetic%20and%20real-world%20datasets%2C%20at%20both%20object%20and%20scene%0Alevels%2C%20demonstrating%20strong%20performance%20in%20rendering%20and%20surface%0Areconstruction.%20Code%20is%20available%20at%20https%3A//github.com/TsingyuanChou/MGSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05182v2&entry.124074799=Read"},
{"title": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes", "author": "Xinjie Zhang and Zhening Liu and Yifan Zhang and Xingtong Ge and Dailan He and Tongda Xu and Yan Wang and Zehong Lin and Shuicheng Yan and Jun Zhang", "abstract": "  4D Gaussian Splatting (4DGS) has recently emerged as a promising technique\nfor capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D\nGaussian representation and a GPU-friendly rasterizer, enabling rapid rendering\nspeeds. Despite its advantages, 4DGS faces significant challenges, notably the\nrequirement of millions of 4D Gaussians, each with extensive associated\nattributes, leading to substantial memory and storage cost. This paper\nintroduces a memory-efficient framework for 4DGS. We streamline the color\nattribute by decomposing it into a per-Gaussian direct color component with\nonly 3 parameters and a shared lightweight alternating current color predictor.\nThis approach eliminates the need for spherical harmonics coefficients, which\ntypically involve up to 144 parameters in classic 4DGS, thereby creating a\nmemory-efficient 4D Gaussian representation. Furthermore, we introduce an\nentropy-constrained Gaussian deformation technique that uses a deformation\nfield to expand the action range of each Gaussian and integrates an\nopacity-based entropy loss to limit the number of Gaussians, thus forcing our\nmodel to use as few Gaussians as possible to fit a dynamic scene well. With\nsimple half-precision storage and zip compression, our framework achieves a\nstorage reduction by approximately 190$\\times$ and 125$\\times$ on the\nTechnicolor and Neural 3D Video datasets, respectively, compared to the\noriginal 4DGS. Meanwhile, it maintains comparable rendering speeds and scene\nrepresentation quality, setting a new standard in the field. Code is available\nat https://github.com/Xinjie-Q/MEGA.\n", "link": "http://arxiv.org/abs/2410.13613v3", "date": "2025-07-22", "relevancy": 3.3755, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7082}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6779}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGA%3A%20Memory-Efficient%204D%20Gaussian%20Splatting%20for%20Dynamic%20Scenes&body=Title%3A%20MEGA%3A%20Memory-Efficient%204D%20Gaussian%20Splatting%20for%20Dynamic%20Scenes%0AAuthor%3A%20Xinjie%20Zhang%20and%20Zhening%20Liu%20and%20Yifan%20Zhang%20and%20Xingtong%20Ge%20and%20Dailan%20He%20and%20Tongda%20Xu%20and%20Yan%20Wang%20and%20Zehong%20Lin%20and%20Shuicheng%20Yan%20and%20Jun%20Zhang%0AAbstract%3A%20%20%204D%20Gaussian%20Splatting%20%284DGS%29%20has%20recently%20emerged%20as%20a%20promising%20technique%0Afor%20capturing%20complex%20dynamic%203D%20scenes%20with%20high%20fidelity.%20It%20utilizes%20a%204D%0AGaussian%20representation%20and%20a%20GPU-friendly%20rasterizer%2C%20enabling%20rapid%20rendering%0Aspeeds.%20Despite%20its%20advantages%2C%204DGS%20faces%20significant%20challenges%2C%20notably%20the%0Arequirement%20of%20millions%20of%204D%20Gaussians%2C%20each%20with%20extensive%20associated%0Aattributes%2C%20leading%20to%20substantial%20memory%20and%20storage%20cost.%20This%20paper%0Aintroduces%20a%20memory-efficient%20framework%20for%204DGS.%20We%20streamline%20the%20color%0Aattribute%20by%20decomposing%20it%20into%20a%20per-Gaussian%20direct%20color%20component%20with%0Aonly%203%20parameters%20and%20a%20shared%20lightweight%20alternating%20current%20color%20predictor.%0AThis%20approach%20eliminates%20the%20need%20for%20spherical%20harmonics%20coefficients%2C%20which%0Atypically%20involve%20up%20to%20144%20parameters%20in%20classic%204DGS%2C%20thereby%20creating%20a%0Amemory-efficient%204D%20Gaussian%20representation.%20Furthermore%2C%20we%20introduce%20an%0Aentropy-constrained%20Gaussian%20deformation%20technique%20that%20uses%20a%20deformation%0Afield%20to%20expand%20the%20action%20range%20of%20each%20Gaussian%20and%20integrates%20an%0Aopacity-based%20entropy%20loss%20to%20limit%20the%20number%20of%20Gaussians%2C%20thus%20forcing%20our%0Amodel%20to%20use%20as%20few%20Gaussians%20as%20possible%20to%20fit%20a%20dynamic%20scene%20well.%20With%0Asimple%20half-precision%20storage%20and%20zip%20compression%2C%20our%20framework%20achieves%20a%0Astorage%20reduction%20by%20approximately%20190%24%5Ctimes%24%20and%20125%24%5Ctimes%24%20on%20the%0ATechnicolor%20and%20Neural%203D%20Video%20datasets%2C%20respectively%2C%20compared%20to%20the%0Aoriginal%204DGS.%20Meanwhile%2C%20it%20maintains%20comparable%20rendering%20speeds%20and%20scene%0Arepresentation%20quality%2C%20setting%20a%20new%20standard%20in%20the%20field.%20Code%20is%20available%0Aat%20https%3A//github.com/Xinjie-Q/MEGA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13613v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGA%253A%2520Memory-Efficient%25204D%2520Gaussian%2520Splatting%2520for%2520Dynamic%2520Scenes%26entry.906535625%3DXinjie%2520Zhang%2520and%2520Zhening%2520Liu%2520and%2520Yifan%2520Zhang%2520and%2520Xingtong%2520Ge%2520and%2520Dailan%2520He%2520and%2520Tongda%2520Xu%2520and%2520Yan%2520Wang%2520and%2520Zehong%2520Lin%2520and%2520Shuicheng%2520Yan%2520and%2520Jun%2520Zhang%26entry.1292438233%3D%2520%25204D%2520Gaussian%2520Splatting%2520%25284DGS%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%2520technique%250Afor%2520capturing%2520complex%2520dynamic%25203D%2520scenes%2520with%2520high%2520fidelity.%2520It%2520utilizes%2520a%25204D%250AGaussian%2520representation%2520and%2520a%2520GPU-friendly%2520rasterizer%252C%2520enabling%2520rapid%2520rendering%250Aspeeds.%2520Despite%2520its%2520advantages%252C%25204DGS%2520faces%2520significant%2520challenges%252C%2520notably%2520the%250Arequirement%2520of%2520millions%2520of%25204D%2520Gaussians%252C%2520each%2520with%2520extensive%2520associated%250Aattributes%252C%2520leading%2520to%2520substantial%2520memory%2520and%2520storage%2520cost.%2520This%2520paper%250Aintroduces%2520a%2520memory-efficient%2520framework%2520for%25204DGS.%2520We%2520streamline%2520the%2520color%250Aattribute%2520by%2520decomposing%2520it%2520into%2520a%2520per-Gaussian%2520direct%2520color%2520component%2520with%250Aonly%25203%2520parameters%2520and%2520a%2520shared%2520lightweight%2520alternating%2520current%2520color%2520predictor.%250AThis%2520approach%2520eliminates%2520the%2520need%2520for%2520spherical%2520harmonics%2520coefficients%252C%2520which%250Atypically%2520involve%2520up%2520to%2520144%2520parameters%2520in%2520classic%25204DGS%252C%2520thereby%2520creating%2520a%250Amemory-efficient%25204D%2520Gaussian%2520representation.%2520Furthermore%252C%2520we%2520introduce%2520an%250Aentropy-constrained%2520Gaussian%2520deformation%2520technique%2520that%2520uses%2520a%2520deformation%250Afield%2520to%2520expand%2520the%2520action%2520range%2520of%2520each%2520Gaussian%2520and%2520integrates%2520an%250Aopacity-based%2520entropy%2520loss%2520to%2520limit%2520the%2520number%2520of%2520Gaussians%252C%2520thus%2520forcing%2520our%250Amodel%2520to%2520use%2520as%2520few%2520Gaussians%2520as%2520possible%2520to%2520fit%2520a%2520dynamic%2520scene%2520well.%2520With%250Asimple%2520half-precision%2520storage%2520and%2520zip%2520compression%252C%2520our%2520framework%2520achieves%2520a%250Astorage%2520reduction%2520by%2520approximately%2520190%2524%255Ctimes%2524%2520and%2520125%2524%255Ctimes%2524%2520on%2520the%250ATechnicolor%2520and%2520Neural%25203D%2520Video%2520datasets%252C%2520respectively%252C%2520compared%2520to%2520the%250Aoriginal%25204DGS.%2520Meanwhile%252C%2520it%2520maintains%2520comparable%2520rendering%2520speeds%2520and%2520scene%250Arepresentation%2520quality%252C%2520setting%2520a%2520new%2520standard%2520in%2520the%2520field.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/Xinjie-Q/MEGA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13613v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGA%3A%20Memory-Efficient%204D%20Gaussian%20Splatting%20for%20Dynamic%20Scenes&entry.906535625=Xinjie%20Zhang%20and%20Zhening%20Liu%20and%20Yifan%20Zhang%20and%20Xingtong%20Ge%20and%20Dailan%20He%20and%20Tongda%20Xu%20and%20Yan%20Wang%20and%20Zehong%20Lin%20and%20Shuicheng%20Yan%20and%20Jun%20Zhang&entry.1292438233=%20%204D%20Gaussian%20Splatting%20%284DGS%29%20has%20recently%20emerged%20as%20a%20promising%20technique%0Afor%20capturing%20complex%20dynamic%203D%20scenes%20with%20high%20fidelity.%20It%20utilizes%20a%204D%0AGaussian%20representation%20and%20a%20GPU-friendly%20rasterizer%2C%20enabling%20rapid%20rendering%0Aspeeds.%20Despite%20its%20advantages%2C%204DGS%20faces%20significant%20challenges%2C%20notably%20the%0Arequirement%20of%20millions%20of%204D%20Gaussians%2C%20each%20with%20extensive%20associated%0Aattributes%2C%20leading%20to%20substantial%20memory%20and%20storage%20cost.%20This%20paper%0Aintroduces%20a%20memory-efficient%20framework%20for%204DGS.%20We%20streamline%20the%20color%0Aattribute%20by%20decomposing%20it%20into%20a%20per-Gaussian%20direct%20color%20component%20with%0Aonly%203%20parameters%20and%20a%20shared%20lightweight%20alternating%20current%20color%20predictor.%0AThis%20approach%20eliminates%20the%20need%20for%20spherical%20harmonics%20coefficients%2C%20which%0Atypically%20involve%20up%20to%20144%20parameters%20in%20classic%204DGS%2C%20thereby%20creating%20a%0Amemory-efficient%204D%20Gaussian%20representation.%20Furthermore%2C%20we%20introduce%20an%0Aentropy-constrained%20Gaussian%20deformation%20technique%20that%20uses%20a%20deformation%0Afield%20to%20expand%20the%20action%20range%20of%20each%20Gaussian%20and%20integrates%20an%0Aopacity-based%20entropy%20loss%20to%20limit%20the%20number%20of%20Gaussians%2C%20thus%20forcing%20our%0Amodel%20to%20use%20as%20few%20Gaussians%20as%20possible%20to%20fit%20a%20dynamic%20scene%20well.%20With%0Asimple%20half-precision%20storage%20and%20zip%20compression%2C%20our%20framework%20achieves%20a%0Astorage%20reduction%20by%20approximately%20190%24%5Ctimes%24%20and%20125%24%5Ctimes%24%20on%20the%0ATechnicolor%20and%20Neural%203D%20Video%20datasets%2C%20respectively%2C%20compared%20to%20the%0Aoriginal%204DGS.%20Meanwhile%2C%20it%20maintains%20comparable%20rendering%20speeds%20and%20scene%0Arepresentation%20quality%2C%20setting%20a%20new%20standard%20in%20the%20field.%20Code%20is%20available%0Aat%20https%3A//github.com/Xinjie-Q/MEGA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13613v3&entry.124074799=Read"},
{"title": "Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models", "author": "Xiaoyan Wang and Zeju Li and Yifan Xu and Jiaxing Qi and Zhifei Yang and Ruifei Ma and Xiangde Liu and Chao Zhang", "abstract": "  New era has unlocked exciting possibilities for extending Large Language\nModels (LLMs) to tackle 3D vision-language tasks. However, most existing 3D\nmultimodal LLMs (MLLMs) rely on compressing holistic 3D scene information or\nsegmenting independent objects to perform these tasks, which limits their\nspatial awareness due to insufficient representation of the richness inherent\nin 3D scenes. To overcome these limitations, we propose Spatial 3D-LLM, a 3D\nMLLM specifically designed to enhance spatial awareness for 3D vision-language\ntasks by enriching the spatial embeddings of 3D scenes. Spatial 3D-LLM\nintegrates an LLM backbone with a progressive spatial awareness scheme that\nprogressively captures spatial information as the perception field expands,\ngenerating location-enriched 3D scene embeddings to serve as visual prompts.\nFurthermore, we introduce two novel tasks: 3D object distance measurement and\n3D layout editing, and construct a 3D instruction dataset, MODEL, to evaluate\nthe model's spatial awareness capabilities. Experimental results demonstrate\nthat Spatial 3D-LLM achieves state-of-the-art performance across a wide range\nof 3D vision-language tasks, revealing the improvements stemmed from our\nprogressive spatial awareness scheme of mining more profound spatial\ninformation. Our code is available at\nhttps://github.com/bjshuyuan/Spatial-3D-LLM.\n", "link": "http://arxiv.org/abs/2507.16524v1", "date": "2025-07-22", "relevancy": 3.3401, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6823}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%203D-LLM%3A%20Exploring%20Spatial%20Awareness%20in%203D%20Vision-Language%20Models&body=Title%3A%20Spatial%203D-LLM%3A%20Exploring%20Spatial%20Awareness%20in%203D%20Vision-Language%20Models%0AAuthor%3A%20Xiaoyan%20Wang%20and%20Zeju%20Li%20and%20Yifan%20Xu%20and%20Jiaxing%20Qi%20and%20Zhifei%20Yang%20and%20Ruifei%20Ma%20and%20Xiangde%20Liu%20and%20Chao%20Zhang%0AAbstract%3A%20%20%20New%20era%20has%20unlocked%20exciting%20possibilities%20for%20extending%20Large%20Language%0AModels%20%28LLMs%29%20to%20tackle%203D%20vision-language%20tasks.%20However%2C%20most%20existing%203D%0Amultimodal%20LLMs%20%28MLLMs%29%20rely%20on%20compressing%20holistic%203D%20scene%20information%20or%0Asegmenting%20independent%20objects%20to%20perform%20these%20tasks%2C%20which%20limits%20their%0Aspatial%20awareness%20due%20to%20insufficient%20representation%20of%20the%20richness%20inherent%0Ain%203D%20scenes.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Spatial%203D-LLM%2C%20a%203D%0AMLLM%20specifically%20designed%20to%20enhance%20spatial%20awareness%20for%203D%20vision-language%0Atasks%20by%20enriching%20the%20spatial%20embeddings%20of%203D%20scenes.%20Spatial%203D-LLM%0Aintegrates%20an%20LLM%20backbone%20with%20a%20progressive%20spatial%20awareness%20scheme%20that%0Aprogressively%20captures%20spatial%20information%20as%20the%20perception%20field%20expands%2C%0Agenerating%20location-enriched%203D%20scene%20embeddings%20to%20serve%20as%20visual%20prompts.%0AFurthermore%2C%20we%20introduce%20two%20novel%20tasks%3A%203D%20object%20distance%20measurement%20and%0A3D%20layout%20editing%2C%20and%20construct%20a%203D%20instruction%20dataset%2C%20MODEL%2C%20to%20evaluate%0Athe%20model%27s%20spatial%20awareness%20capabilities.%20Experimental%20results%20demonstrate%0Athat%20Spatial%203D-LLM%20achieves%20state-of-the-art%20performance%20across%20a%20wide%20range%0Aof%203D%20vision-language%20tasks%2C%20revealing%20the%20improvements%20stemmed%20from%20our%0Aprogressive%20spatial%20awareness%20scheme%20of%20mining%20more%20profound%20spatial%0Ainformation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/bjshuyuan/Spatial-3D-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%25203D-LLM%253A%2520Exploring%2520Spatial%2520Awareness%2520in%25203D%2520Vision-Language%2520Models%26entry.906535625%3DXiaoyan%2520Wang%2520and%2520Zeju%2520Li%2520and%2520Yifan%2520Xu%2520and%2520Jiaxing%2520Qi%2520and%2520Zhifei%2520Yang%2520and%2520Ruifei%2520Ma%2520and%2520Xiangde%2520Liu%2520and%2520Chao%2520Zhang%26entry.1292438233%3D%2520%2520New%2520era%2520has%2520unlocked%2520exciting%2520possibilities%2520for%2520extending%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520to%2520tackle%25203D%2520vision-language%2520tasks.%2520However%252C%2520most%2520existing%25203D%250Amultimodal%2520LLMs%2520%2528MLLMs%2529%2520rely%2520on%2520compressing%2520holistic%25203D%2520scene%2520information%2520or%250Asegmenting%2520independent%2520objects%2520to%2520perform%2520these%2520tasks%252C%2520which%2520limits%2520their%250Aspatial%2520awareness%2520due%2520to%2520insufficient%2520representation%2520of%2520the%2520richness%2520inherent%250Ain%25203D%2520scenes.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Spatial%25203D-LLM%252C%2520a%25203D%250AMLLM%2520specifically%2520designed%2520to%2520enhance%2520spatial%2520awareness%2520for%25203D%2520vision-language%250Atasks%2520by%2520enriching%2520the%2520spatial%2520embeddings%2520of%25203D%2520scenes.%2520Spatial%25203D-LLM%250Aintegrates%2520an%2520LLM%2520backbone%2520with%2520a%2520progressive%2520spatial%2520awareness%2520scheme%2520that%250Aprogressively%2520captures%2520spatial%2520information%2520as%2520the%2520perception%2520field%2520expands%252C%250Agenerating%2520location-enriched%25203D%2520scene%2520embeddings%2520to%2520serve%2520as%2520visual%2520prompts.%250AFurthermore%252C%2520we%2520introduce%2520two%2520novel%2520tasks%253A%25203D%2520object%2520distance%2520measurement%2520and%250A3D%2520layout%2520editing%252C%2520and%2520construct%2520a%25203D%2520instruction%2520dataset%252C%2520MODEL%252C%2520to%2520evaluate%250Athe%2520model%2527s%2520spatial%2520awareness%2520capabilities.%2520Experimental%2520results%2520demonstrate%250Athat%2520Spatial%25203D-LLM%2520achieves%2520state-of-the-art%2520performance%2520across%2520a%2520wide%2520range%250Aof%25203D%2520vision-language%2520tasks%252C%2520revealing%2520the%2520improvements%2520stemmed%2520from%2520our%250Aprogressive%2520spatial%2520awareness%2520scheme%2520of%2520mining%2520more%2520profound%2520spatial%250Ainformation.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/bjshuyuan/Spatial-3D-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%203D-LLM%3A%20Exploring%20Spatial%20Awareness%20in%203D%20Vision-Language%20Models&entry.906535625=Xiaoyan%20Wang%20and%20Zeju%20Li%20and%20Yifan%20Xu%20and%20Jiaxing%20Qi%20and%20Zhifei%20Yang%20and%20Ruifei%20Ma%20and%20Xiangde%20Liu%20and%20Chao%20Zhang&entry.1292438233=%20%20New%20era%20has%20unlocked%20exciting%20possibilities%20for%20extending%20Large%20Language%0AModels%20%28LLMs%29%20to%20tackle%203D%20vision-language%20tasks.%20However%2C%20most%20existing%203D%0Amultimodal%20LLMs%20%28MLLMs%29%20rely%20on%20compressing%20holistic%203D%20scene%20information%20or%0Asegmenting%20independent%20objects%20to%20perform%20these%20tasks%2C%20which%20limits%20their%0Aspatial%20awareness%20due%20to%20insufficient%20representation%20of%20the%20richness%20inherent%0Ain%203D%20scenes.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Spatial%203D-LLM%2C%20a%203D%0AMLLM%20specifically%20designed%20to%20enhance%20spatial%20awareness%20for%203D%20vision-language%0Atasks%20by%20enriching%20the%20spatial%20embeddings%20of%203D%20scenes.%20Spatial%203D-LLM%0Aintegrates%20an%20LLM%20backbone%20with%20a%20progressive%20spatial%20awareness%20scheme%20that%0Aprogressively%20captures%20spatial%20information%20as%20the%20perception%20field%20expands%2C%0Agenerating%20location-enriched%203D%20scene%20embeddings%20to%20serve%20as%20visual%20prompts.%0AFurthermore%2C%20we%20introduce%20two%20novel%20tasks%3A%203D%20object%20distance%20measurement%20and%0A3D%20layout%20editing%2C%20and%20construct%20a%203D%20instruction%20dataset%2C%20MODEL%2C%20to%20evaluate%0Athe%20model%27s%20spatial%20awareness%20capabilities.%20Experimental%20results%20demonstrate%0Athat%20Spatial%203D-LLM%20achieves%20state-of-the-art%20performance%20across%20a%20wide%20range%0Aof%203D%20vision-language%20tasks%2C%20revealing%20the%20improvements%20stemmed%20from%20our%0Aprogressive%20spatial%20awareness%20scheme%20of%20mining%20more%20profound%20spatial%0Ainformation.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/bjshuyuan/Spatial-3D-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16524v1&entry.124074799=Read"},
{"title": "Sparse-View 3D Reconstruction: Recent Advances and Open Challenges", "author": "Tanveer Younis and Zhanglin Cheng", "abstract": "  Sparse-view 3D reconstruction is essential for applications in which dense\nimage acquisition is impractical, such as robotics, augmented/virtual reality\n(AR/VR), and autonomous systems. In these settings, minimal image overlap\nprevents reliable correspondence matching, causing traditional methods, such as\nstructure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey\nreviews the latest advances in neural implicit models (e.g., NeRF and its\nregularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian\nSplatting), and hybrid frameworks that leverage priors from diffusion and\nvision foundation models (VFMs).We analyze how geometric regularization,\nexplicit shape modeling, and generative inference are used to mitigate\nartifacts such as floaters and pose ambiguities in sparse-view settings.\nComparative results on standard benchmarks reveal key trade-offs between the\nreconstruction accuracy, efficiency, and generalization. Unlike previous\nreviews, our survey provides a unified perspective on geometry-based, neural\nimplicit, and generative (diffusion-based) methods. We highlight the persistent\nchallenges in domain generalization and pose-free reconstruction and outline\nfuture directions for developing 3D-native generative priors and achieving\nreal-time, unconstrained sparse-view reconstruction.\n", "link": "http://arxiv.org/abs/2507.16406v1", "date": "2025-07-22", "relevancy": 3.25, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6556}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6472}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse-View%203D%20Reconstruction%3A%20Recent%20Advances%20and%20Open%20Challenges&body=Title%3A%20Sparse-View%203D%20Reconstruction%3A%20Recent%20Advances%20and%20Open%20Challenges%0AAuthor%3A%20Tanveer%20Younis%20and%20Zhanglin%20Cheng%0AAbstract%3A%20%20%20Sparse-view%203D%20reconstruction%20is%20essential%20for%20applications%20in%20which%20dense%0Aimage%20acquisition%20is%20impractical%2C%20such%20as%20robotics%2C%20augmented/virtual%20reality%0A%28AR/VR%29%2C%20and%20autonomous%20systems.%20In%20these%20settings%2C%20minimal%20image%20overlap%0Aprevents%20reliable%20correspondence%20matching%2C%20causing%20traditional%20methods%2C%20such%20as%0Astructure-from-motion%20%28SfM%29%20and%20multiview%20stereo%20%28MVS%29%2C%20to%20fail.%20This%20survey%0Areviews%20the%20latest%20advances%20in%20neural%20implicit%20models%20%28e.g.%2C%20NeRF%20and%20its%0Aregularized%20versions%29%2C%20explicit%20point-cloud-based%20approaches%20%28e.g.%2C%203D%20Gaussian%0ASplatting%29%2C%20and%20hybrid%20frameworks%20that%20leverage%20priors%20from%20diffusion%20and%0Avision%20foundation%20models%20%28VFMs%29.We%20analyze%20how%20geometric%20regularization%2C%0Aexplicit%20shape%20modeling%2C%20and%20generative%20inference%20are%20used%20to%20mitigate%0Aartifacts%20such%20as%20floaters%20and%20pose%20ambiguities%20in%20sparse-view%20settings.%0AComparative%20results%20on%20standard%20benchmarks%20reveal%20key%20trade-offs%20between%20the%0Areconstruction%20accuracy%2C%20efficiency%2C%20and%20generalization.%20Unlike%20previous%0Areviews%2C%20our%20survey%20provides%20a%20unified%20perspective%20on%20geometry-based%2C%20neural%0Aimplicit%2C%20and%20generative%20%28diffusion-based%29%20methods.%20We%20highlight%20the%20persistent%0Achallenges%20in%20domain%20generalization%20and%20pose-free%20reconstruction%20and%20outline%0Afuture%20directions%20for%20developing%203D-native%20generative%20priors%20and%20achieving%0Areal-time%2C%20unconstrained%20sparse-view%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse-View%25203D%2520Reconstruction%253A%2520Recent%2520Advances%2520and%2520Open%2520Challenges%26entry.906535625%3DTanveer%2520Younis%2520and%2520Zhanglin%2520Cheng%26entry.1292438233%3D%2520%2520Sparse-view%25203D%2520reconstruction%2520is%2520essential%2520for%2520applications%2520in%2520which%2520dense%250Aimage%2520acquisition%2520is%2520impractical%252C%2520such%2520as%2520robotics%252C%2520augmented/virtual%2520reality%250A%2528AR/VR%2529%252C%2520and%2520autonomous%2520systems.%2520In%2520these%2520settings%252C%2520minimal%2520image%2520overlap%250Aprevents%2520reliable%2520correspondence%2520matching%252C%2520causing%2520traditional%2520methods%252C%2520such%2520as%250Astructure-from-motion%2520%2528SfM%2529%2520and%2520multiview%2520stereo%2520%2528MVS%2529%252C%2520to%2520fail.%2520This%2520survey%250Areviews%2520the%2520latest%2520advances%2520in%2520neural%2520implicit%2520models%2520%2528e.g.%252C%2520NeRF%2520and%2520its%250Aregularized%2520versions%2529%252C%2520explicit%2520point-cloud-based%2520approaches%2520%2528e.g.%252C%25203D%2520Gaussian%250ASplatting%2529%252C%2520and%2520hybrid%2520frameworks%2520that%2520leverage%2520priors%2520from%2520diffusion%2520and%250Avision%2520foundation%2520models%2520%2528VFMs%2529.We%2520analyze%2520how%2520geometric%2520regularization%252C%250Aexplicit%2520shape%2520modeling%252C%2520and%2520generative%2520inference%2520are%2520used%2520to%2520mitigate%250Aartifacts%2520such%2520as%2520floaters%2520and%2520pose%2520ambiguities%2520in%2520sparse-view%2520settings.%250AComparative%2520results%2520on%2520standard%2520benchmarks%2520reveal%2520key%2520trade-offs%2520between%2520the%250Areconstruction%2520accuracy%252C%2520efficiency%252C%2520and%2520generalization.%2520Unlike%2520previous%250Areviews%252C%2520our%2520survey%2520provides%2520a%2520unified%2520perspective%2520on%2520geometry-based%252C%2520neural%250Aimplicit%252C%2520and%2520generative%2520%2528diffusion-based%2529%2520methods.%2520We%2520highlight%2520the%2520persistent%250Achallenges%2520in%2520domain%2520generalization%2520and%2520pose-free%2520reconstruction%2520and%2520outline%250Afuture%2520directions%2520for%2520developing%25203D-native%2520generative%2520priors%2520and%2520achieving%250Areal-time%252C%2520unconstrained%2520sparse-view%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse-View%203D%20Reconstruction%3A%20Recent%20Advances%20and%20Open%20Challenges&entry.906535625=Tanveer%20Younis%20and%20Zhanglin%20Cheng&entry.1292438233=%20%20Sparse-view%203D%20reconstruction%20is%20essential%20for%20applications%20in%20which%20dense%0Aimage%20acquisition%20is%20impractical%2C%20such%20as%20robotics%2C%20augmented/virtual%20reality%0A%28AR/VR%29%2C%20and%20autonomous%20systems.%20In%20these%20settings%2C%20minimal%20image%20overlap%0Aprevents%20reliable%20correspondence%20matching%2C%20causing%20traditional%20methods%2C%20such%20as%0Astructure-from-motion%20%28SfM%29%20and%20multiview%20stereo%20%28MVS%29%2C%20to%20fail.%20This%20survey%0Areviews%20the%20latest%20advances%20in%20neural%20implicit%20models%20%28e.g.%2C%20NeRF%20and%20its%0Aregularized%20versions%29%2C%20explicit%20point-cloud-based%20approaches%20%28e.g.%2C%203D%20Gaussian%0ASplatting%29%2C%20and%20hybrid%20frameworks%20that%20leverage%20priors%20from%20diffusion%20and%0Avision%20foundation%20models%20%28VFMs%29.We%20analyze%20how%20geometric%20regularization%2C%0Aexplicit%20shape%20modeling%2C%20and%20generative%20inference%20are%20used%20to%20mitigate%0Aartifacts%20such%20as%20floaters%20and%20pose%20ambiguities%20in%20sparse-view%20settings.%0AComparative%20results%20on%20standard%20benchmarks%20reveal%20key%20trade-offs%20between%20the%0Areconstruction%20accuracy%2C%20efficiency%2C%20and%20generalization.%20Unlike%20previous%0Areviews%2C%20our%20survey%20provides%20a%20unified%20perspective%20on%20geometry-based%2C%20neural%0Aimplicit%2C%20and%20generative%20%28diffusion-based%29%20methods.%20We%20highlight%20the%20persistent%0Achallenges%20in%20domain%20generalization%20and%20pose-free%20reconstruction%20and%20outline%0Afuture%20directions%20for%20developing%203D-native%20generative%20priors%20and%20achieving%0Areal-time%2C%20unconstrained%20sparse-view%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16406v1&entry.124074799=Read"},
{"title": "Do large language vision models understand 3D shapes?", "author": "Sagi Eppel", "abstract": "  Large vision language models (LVLM) are the leading A.I approach for\nachieving a general visual understanding of the world. Models such as GPT,\nClaude, Gemini, and LLama can use images to understand and analyze complex\nvisual scenes. 3D objects and shapes are the basic building blocks of the\nworld, recognizing them is a fundamental part of human perception. The goal of\nthis work is to test whether LVLMs truly understand 3D shapes by testing the\nmodels ability to identify and match objects of the exact same 3D shapes but\nwith different orientations and materials/textures. A large number of test\nimages were created using CGI with a huge number of highly diverse objects,\nmaterials, and scenes. The results of this test show that the ability of such\nmodels to match 3D shapes is significantly below humans but much higher than\nrandom guesses. Suggesting that the models have gained some abstract\nunderstanding of 3D shapes but still trail far beyond humans in this task.\nMainly it seems that the models can easily identify the same object with a\ndifferent orientation as well as matching identical 3D shapes of the same\norientation but with different materials and textures. However, when both the\nobject material and orientation are changed, all models perform poorly relative\nto humans. Code and benchmark are available.\n", "link": "http://arxiv.org/abs/2412.10908v5", "date": "2025-07-22", "relevancy": 3.1527, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6488}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20large%20language%20vision%20models%20understand%203D%20shapes%3F&body=Title%3A%20Do%20large%20language%20vision%20models%20understand%203D%20shapes%3F%0AAuthor%3A%20Sagi%20Eppel%0AAbstract%3A%20%20%20Large%20vision%20language%20models%20%28LVLM%29%20are%20the%20leading%20A.I%20approach%20for%0Aachieving%20a%20general%20visual%20understanding%20of%20the%20world.%20Models%20such%20as%20GPT%2C%0AClaude%2C%20Gemini%2C%20and%20LLama%20can%20use%20images%20to%20understand%20and%20analyze%20complex%0Avisual%20scenes.%203D%20objects%20and%20shapes%20are%20the%20basic%20building%20blocks%20of%20the%0Aworld%2C%20recognizing%20them%20is%20a%20fundamental%20part%20of%20human%20perception.%20The%20goal%20of%0Athis%20work%20is%20to%20test%20whether%20LVLMs%20truly%20understand%203D%20shapes%20by%20testing%20the%0Amodels%20ability%20to%20identify%20and%20match%20objects%20of%20the%20exact%20same%203D%20shapes%20but%0Awith%20different%20orientations%20and%20materials/textures.%20A%20large%20number%20of%20test%0Aimages%20were%20created%20using%20CGI%20with%20a%20huge%20number%20of%20highly%20diverse%20objects%2C%0Amaterials%2C%20and%20scenes.%20The%20results%20of%20this%20test%20show%20that%20the%20ability%20of%20such%0Amodels%20to%20match%203D%20shapes%20is%20significantly%20below%20humans%20but%20much%20higher%20than%0Arandom%20guesses.%20Suggesting%20that%20the%20models%20have%20gained%20some%20abstract%0Aunderstanding%20of%203D%20shapes%20but%20still%20trail%20far%20beyond%20humans%20in%20this%20task.%0AMainly%20it%20seems%20that%20the%20models%20can%20easily%20identify%20the%20same%20object%20with%20a%0Adifferent%20orientation%20as%20well%20as%20matching%20identical%203D%20shapes%20of%20the%20same%0Aorientation%20but%20with%20different%20materials%20and%20textures.%20However%2C%20when%20both%20the%0Aobject%20material%20and%20orientation%20are%20changed%2C%20all%20models%20perform%20poorly%20relative%0Ato%20humans.%20Code%20and%20benchmark%20are%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.10908v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520large%2520language%2520vision%2520models%2520understand%25203D%2520shapes%253F%26entry.906535625%3DSagi%2520Eppel%26entry.1292438233%3D%2520%2520Large%2520vision%2520language%2520models%2520%2528LVLM%2529%2520are%2520the%2520leading%2520A.I%2520approach%2520for%250Aachieving%2520a%2520general%2520visual%2520understanding%2520of%2520the%2520world.%2520Models%2520such%2520as%2520GPT%252C%250AClaude%252C%2520Gemini%252C%2520and%2520LLama%2520can%2520use%2520images%2520to%2520understand%2520and%2520analyze%2520complex%250Avisual%2520scenes.%25203D%2520objects%2520and%2520shapes%2520are%2520the%2520basic%2520building%2520blocks%2520of%2520the%250Aworld%252C%2520recognizing%2520them%2520is%2520a%2520fundamental%2520part%2520of%2520human%2520perception.%2520The%2520goal%2520of%250Athis%2520work%2520is%2520to%2520test%2520whether%2520LVLMs%2520truly%2520understand%25203D%2520shapes%2520by%2520testing%2520the%250Amodels%2520ability%2520to%2520identify%2520and%2520match%2520objects%2520of%2520the%2520exact%2520same%25203D%2520shapes%2520but%250Awith%2520different%2520orientations%2520and%2520materials/textures.%2520A%2520large%2520number%2520of%2520test%250Aimages%2520were%2520created%2520using%2520CGI%2520with%2520a%2520huge%2520number%2520of%2520highly%2520diverse%2520objects%252C%250Amaterials%252C%2520and%2520scenes.%2520The%2520results%2520of%2520this%2520test%2520show%2520that%2520the%2520ability%2520of%2520such%250Amodels%2520to%2520match%25203D%2520shapes%2520is%2520significantly%2520below%2520humans%2520but%2520much%2520higher%2520than%250Arandom%2520guesses.%2520Suggesting%2520that%2520the%2520models%2520have%2520gained%2520some%2520abstract%250Aunderstanding%2520of%25203D%2520shapes%2520but%2520still%2520trail%2520far%2520beyond%2520humans%2520in%2520this%2520task.%250AMainly%2520it%2520seems%2520that%2520the%2520models%2520can%2520easily%2520identify%2520the%2520same%2520object%2520with%2520a%250Adifferent%2520orientation%2520as%2520well%2520as%2520matching%2520identical%25203D%2520shapes%2520of%2520the%2520same%250Aorientation%2520but%2520with%2520different%2520materials%2520and%2520textures.%2520However%252C%2520when%2520both%2520the%250Aobject%2520material%2520and%2520orientation%2520are%2520changed%252C%2520all%2520models%2520perform%2520poorly%2520relative%250Ato%2520humans.%2520Code%2520and%2520benchmark%2520are%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10908v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20large%20language%20vision%20models%20understand%203D%20shapes%3F&entry.906535625=Sagi%20Eppel&entry.1292438233=%20%20Large%20vision%20language%20models%20%28LVLM%29%20are%20the%20leading%20A.I%20approach%20for%0Aachieving%20a%20general%20visual%20understanding%20of%20the%20world.%20Models%20such%20as%20GPT%2C%0AClaude%2C%20Gemini%2C%20and%20LLama%20can%20use%20images%20to%20understand%20and%20analyze%20complex%0Avisual%20scenes.%203D%20objects%20and%20shapes%20are%20the%20basic%20building%20blocks%20of%20the%0Aworld%2C%20recognizing%20them%20is%20a%20fundamental%20part%20of%20human%20perception.%20The%20goal%20of%0Athis%20work%20is%20to%20test%20whether%20LVLMs%20truly%20understand%203D%20shapes%20by%20testing%20the%0Amodels%20ability%20to%20identify%20and%20match%20objects%20of%20the%20exact%20same%203D%20shapes%20but%0Awith%20different%20orientations%20and%20materials/textures.%20A%20large%20number%20of%20test%0Aimages%20were%20created%20using%20CGI%20with%20a%20huge%20number%20of%20highly%20diverse%20objects%2C%0Amaterials%2C%20and%20scenes.%20The%20results%20of%20this%20test%20show%20that%20the%20ability%20of%20such%0Amodels%20to%20match%203D%20shapes%20is%20significantly%20below%20humans%20but%20much%20higher%20than%0Arandom%20guesses.%20Suggesting%20that%20the%20models%20have%20gained%20some%20abstract%0Aunderstanding%20of%203D%20shapes%20but%20still%20trail%20far%20beyond%20humans%20in%20this%20task.%0AMainly%20it%20seems%20that%20the%20models%20can%20easily%20identify%20the%20same%20object%20with%20a%0Adifferent%20orientation%20as%20well%20as%20matching%20identical%203D%20shapes%20of%20the%20same%0Aorientation%20but%20with%20different%20materials%20and%20textures.%20However%2C%20when%20both%20the%0Aobject%20material%20and%20orientation%20are%20changed%2C%20all%20models%20perform%20poorly%20relative%0Ato%20humans.%20Code%20and%20benchmark%20are%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.10908v5&entry.124074799=Read"},
{"title": "DOFA-CLIP: Multimodal Vision-Language Foundation Models for Earth\n  Observation", "author": "Zhitong Xiong and Yi Wang and Weikang Yu and Adam J Stewart and Jie Zhao and Nils Lehmann and Thomas Dujardin and Zhenghang Yuan and Pedram Ghamisi and Xiao Xiang Zhu", "abstract": "  Earth observation (EO) spans a broad spectrum of modalities, including\noptical, radar, multispectral, and hyperspectral data, each capturing distinct\nenvironmental signals. However, current vision-language models in EO,\nparticularly CLIP-based variants, remain confined to individual modalities,\nlimiting generalization and scalability across diverse tasks. We present\nDOFA-CLIP (Dynamic-One-For-All CLIP), a unified vision-language foundation\nmodel that dynamically adapts to EO modalities with flexible spectral\nconfigurations through a single Transformer backbone. Our approach introduces\nthree key contributions: 1) the construction of GeoLangBind-2M, a large-scale\nEO image-text dataset covering six heterogeneous modalities with rich natural\nlanguage descriptions; 2) a novel training strategy called VECT (Vision-models\nEnhanced Contrastive Text-image pretraining), which enhances the spatial\nawareness of CLIP features with multiple vision foundation models; and 3) a\nModality-aware Knowledge Agglomeration (MaKA) module that refines feature\ndistillation with modality-specific awareness. DOFA-CLIP achieves\nstate-of-the-art zero-shot performance across a wide range of EO benchmarks,\nincluding unseen modalities and a diverse number of input spectral bands.\nTogether, these contributions establish a scalable foundation for multimodal EO\nunderstanding and open new avenues for integrating heterogeneous EO data with\nlarge language models. Code and datasets will be released. Code and datasets\nare publicly available.\n", "link": "http://arxiv.org/abs/2503.06312v2", "date": "2025-07-22", "relevancy": 3.0976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6328}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DOFA-CLIP%3A%20Multimodal%20Vision-Language%20Foundation%20Models%20for%20Earth%0A%20%20Observation&body=Title%3A%20DOFA-CLIP%3A%20Multimodal%20Vision-Language%20Foundation%20Models%20for%20Earth%0A%20%20Observation%0AAuthor%3A%20Zhitong%20Xiong%20and%20Yi%20Wang%20and%20Weikang%20Yu%20and%20Adam%20J%20Stewart%20and%20Jie%20Zhao%20and%20Nils%20Lehmann%20and%20Thomas%20Dujardin%20and%20Zhenghang%20Yuan%20and%20Pedram%20Ghamisi%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20Earth%20observation%20%28EO%29%20spans%20a%20broad%20spectrum%20of%20modalities%2C%20including%0Aoptical%2C%20radar%2C%20multispectral%2C%20and%20hyperspectral%20data%2C%20each%20capturing%20distinct%0Aenvironmental%20signals.%20However%2C%20current%20vision-language%20models%20in%20EO%2C%0Aparticularly%20CLIP-based%20variants%2C%20remain%20confined%20to%20individual%20modalities%2C%0Alimiting%20generalization%20and%20scalability%20across%20diverse%20tasks.%20We%20present%0ADOFA-CLIP%20%28Dynamic-One-For-All%20CLIP%29%2C%20a%20unified%20vision-language%20foundation%0Amodel%20that%20dynamically%20adapts%20to%20EO%20modalities%20with%20flexible%20spectral%0Aconfigurations%20through%20a%20single%20Transformer%20backbone.%20Our%20approach%20introduces%0Athree%20key%20contributions%3A%201%29%20the%20construction%20of%20GeoLangBind-2M%2C%20a%20large-scale%0AEO%20image-text%20dataset%20covering%20six%20heterogeneous%20modalities%20with%20rich%20natural%0Alanguage%20descriptions%3B%202%29%20a%20novel%20training%20strategy%20called%20VECT%20%28Vision-models%0AEnhanced%20Contrastive%20Text-image%20pretraining%29%2C%20which%20enhances%20the%20spatial%0Aawareness%20of%20CLIP%20features%20with%20multiple%20vision%20foundation%20models%3B%20and%203%29%20a%0AModality-aware%20Knowledge%20Agglomeration%20%28MaKA%29%20module%20that%20refines%20feature%0Adistillation%20with%20modality-specific%20awareness.%20DOFA-CLIP%20achieves%0Astate-of-the-art%20zero-shot%20performance%20across%20a%20wide%20range%20of%20EO%20benchmarks%2C%0Aincluding%20unseen%20modalities%20and%20a%20diverse%20number%20of%20input%20spectral%20bands.%0ATogether%2C%20these%20contributions%20establish%20a%20scalable%20foundation%20for%20multimodal%20EO%0Aunderstanding%20and%20open%20new%20avenues%20for%20integrating%20heterogeneous%20EO%20data%20with%0Alarge%20language%20models.%20Code%20and%20datasets%20will%20be%20released.%20Code%20and%20datasets%0Aare%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDOFA-CLIP%253A%2520Multimodal%2520Vision-Language%2520Foundation%2520Models%2520for%2520Earth%250A%2520%2520Observation%26entry.906535625%3DZhitong%2520Xiong%2520and%2520Yi%2520Wang%2520and%2520Weikang%2520Yu%2520and%2520Adam%2520J%2520Stewart%2520and%2520Jie%2520Zhao%2520and%2520Nils%2520Lehmann%2520and%2520Thomas%2520Dujardin%2520and%2520Zhenghang%2520Yuan%2520and%2520Pedram%2520Ghamisi%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520Earth%2520observation%2520%2528EO%2529%2520spans%2520a%2520broad%2520spectrum%2520of%2520modalities%252C%2520including%250Aoptical%252C%2520radar%252C%2520multispectral%252C%2520and%2520hyperspectral%2520data%252C%2520each%2520capturing%2520distinct%250Aenvironmental%2520signals.%2520However%252C%2520current%2520vision-language%2520models%2520in%2520EO%252C%250Aparticularly%2520CLIP-based%2520variants%252C%2520remain%2520confined%2520to%2520individual%2520modalities%252C%250Alimiting%2520generalization%2520and%2520scalability%2520across%2520diverse%2520tasks.%2520We%2520present%250ADOFA-CLIP%2520%2528Dynamic-One-For-All%2520CLIP%2529%252C%2520a%2520unified%2520vision-language%2520foundation%250Amodel%2520that%2520dynamically%2520adapts%2520to%2520EO%2520modalities%2520with%2520flexible%2520spectral%250Aconfigurations%2520through%2520a%2520single%2520Transformer%2520backbone.%2520Our%2520approach%2520introduces%250Athree%2520key%2520contributions%253A%25201%2529%2520the%2520construction%2520of%2520GeoLangBind-2M%252C%2520a%2520large-scale%250AEO%2520image-text%2520dataset%2520covering%2520six%2520heterogeneous%2520modalities%2520with%2520rich%2520natural%250Alanguage%2520descriptions%253B%25202%2529%2520a%2520novel%2520training%2520strategy%2520called%2520VECT%2520%2528Vision-models%250AEnhanced%2520Contrastive%2520Text-image%2520pretraining%2529%252C%2520which%2520enhances%2520the%2520spatial%250Aawareness%2520of%2520CLIP%2520features%2520with%2520multiple%2520vision%2520foundation%2520models%253B%2520and%25203%2529%2520a%250AModality-aware%2520Knowledge%2520Agglomeration%2520%2528MaKA%2529%2520module%2520that%2520refines%2520feature%250Adistillation%2520with%2520modality-specific%2520awareness.%2520DOFA-CLIP%2520achieves%250Astate-of-the-art%2520zero-shot%2520performance%2520across%2520a%2520wide%2520range%2520of%2520EO%2520benchmarks%252C%250Aincluding%2520unseen%2520modalities%2520and%2520a%2520diverse%2520number%2520of%2520input%2520spectral%2520bands.%250ATogether%252C%2520these%2520contributions%2520establish%2520a%2520scalable%2520foundation%2520for%2520multimodal%2520EO%250Aunderstanding%2520and%2520open%2520new%2520avenues%2520for%2520integrating%2520heterogeneous%2520EO%2520data%2520with%250Alarge%2520language%2520models.%2520Code%2520and%2520datasets%2520will%2520be%2520released.%2520Code%2520and%2520datasets%250Aare%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DOFA-CLIP%3A%20Multimodal%20Vision-Language%20Foundation%20Models%20for%20Earth%0A%20%20Observation&entry.906535625=Zhitong%20Xiong%20and%20Yi%20Wang%20and%20Weikang%20Yu%20and%20Adam%20J%20Stewart%20and%20Jie%20Zhao%20and%20Nils%20Lehmann%20and%20Thomas%20Dujardin%20and%20Zhenghang%20Yuan%20and%20Pedram%20Ghamisi%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20Earth%20observation%20%28EO%29%20spans%20a%20broad%20spectrum%20of%20modalities%2C%20including%0Aoptical%2C%20radar%2C%20multispectral%2C%20and%20hyperspectral%20data%2C%20each%20capturing%20distinct%0Aenvironmental%20signals.%20However%2C%20current%20vision-language%20models%20in%20EO%2C%0Aparticularly%20CLIP-based%20variants%2C%20remain%20confined%20to%20individual%20modalities%2C%0Alimiting%20generalization%20and%20scalability%20across%20diverse%20tasks.%20We%20present%0ADOFA-CLIP%20%28Dynamic-One-For-All%20CLIP%29%2C%20a%20unified%20vision-language%20foundation%0Amodel%20that%20dynamically%20adapts%20to%20EO%20modalities%20with%20flexible%20spectral%0Aconfigurations%20through%20a%20single%20Transformer%20backbone.%20Our%20approach%20introduces%0Athree%20key%20contributions%3A%201%29%20the%20construction%20of%20GeoLangBind-2M%2C%20a%20large-scale%0AEO%20image-text%20dataset%20covering%20six%20heterogeneous%20modalities%20with%20rich%20natural%0Alanguage%20descriptions%3B%202%29%20a%20novel%20training%20strategy%20called%20VECT%20%28Vision-models%0AEnhanced%20Contrastive%20Text-image%20pretraining%29%2C%20which%20enhances%20the%20spatial%0Aawareness%20of%20CLIP%20features%20with%20multiple%20vision%20foundation%20models%3B%20and%203%29%20a%0AModality-aware%20Knowledge%20Agglomeration%20%28MaKA%29%20module%20that%20refines%20feature%0Adistillation%20with%20modality-specific%20awareness.%20DOFA-CLIP%20achieves%0Astate-of-the-art%20zero-shot%20performance%20across%20a%20wide%20range%20of%20EO%20benchmarks%2C%0Aincluding%20unseen%20modalities%20and%20a%20diverse%20number%20of%20input%20spectral%20bands.%0ATogether%2C%20these%20contributions%20establish%20a%20scalable%20foundation%20for%20multimodal%20EO%0Aunderstanding%20and%20open%20new%20avenues%20for%20integrating%20heterogeneous%20EO%20data%20with%0Alarge%20language%20models.%20Code%20and%20datasets%20will%20be%20released.%20Code%20and%20datasets%0Aare%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06312v2&entry.124074799=Read"},
{"title": "Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian\n  Representation", "author": "Xueming Fu and Pei Wu and Yingtai Li and Xin Luo and Zihang Jiang and Junhao Mei and Jian Lu and Gao-Jun Teng and S. Kevin Zhou", "abstract": "  Accurate analysis of cardiac motion is crucial for evaluating cardiac\nfunction. While dynamic cardiac magnetic resonance imaging (CMR) can capture\ndetailed tissue motion throughout the cardiac cycle, the fine-grained 4D\ncardiac motion tracking remains challenging due to the homogeneous nature of\nmyocardial tissue and the lack of distinctive features. Existing approaches can\nbe broadly categorized into image based and representation-based, each with its\nlimitations. Image-based methods, including both raditional and deep\nlearning-based registration approaches, either struggle with topological\nconsistency or rely heavily on extensive training data. Representation-based\nmethods, while promising, often suffer from loss of image-level details. To\naddress these limitations, we propose Dynamic 3D Gaussian Representation\n(Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation\nwith implicit neural motion field modeling. Our method simultaneously optimizes\ncardiac structure and motion in a self-supervised manner, eliminating the need\nfor extensive training data or point-to-point correspondences. Through\ndifferentiable volumetric rendering, Dyna3DGR efficiently bridges continuous\nmotion representation with image-space alignment while preserving both\ntopological and temporal consistency. Comprehensive evaluations on the ACDC\ndataset demonstrate that our approach surpasses state-of-the-art deep\nlearning-based diffeomorphic registration methods in tracking accuracy. The\ncode will be available in https://github.com/windrise/Dyna3DGR.\n", "link": "http://arxiv.org/abs/2507.16608v1", "date": "2025-07-22", "relevancy": 3.0623, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6322}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6251}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dyna3DGR%3A%204D%20Cardiac%20Motion%20Tracking%20with%20Dynamic%203D%20Gaussian%0A%20%20Representation&body=Title%3A%20Dyna3DGR%3A%204D%20Cardiac%20Motion%20Tracking%20with%20Dynamic%203D%20Gaussian%0A%20%20Representation%0AAuthor%3A%20Xueming%20Fu%20and%20Pei%20Wu%20and%20Yingtai%20Li%20and%20Xin%20Luo%20and%20Zihang%20Jiang%20and%20Junhao%20Mei%20and%20Jian%20Lu%20and%20Gao-Jun%20Teng%20and%20S.%20Kevin%20Zhou%0AAbstract%3A%20%20%20Accurate%20analysis%20of%20cardiac%20motion%20is%20crucial%20for%20evaluating%20cardiac%0Afunction.%20While%20dynamic%20cardiac%20magnetic%20resonance%20imaging%20%28CMR%29%20can%20capture%0Adetailed%20tissue%20motion%20throughout%20the%20cardiac%20cycle%2C%20the%20fine-grained%204D%0Acardiac%20motion%20tracking%20remains%20challenging%20due%20to%20the%20homogeneous%20nature%20of%0Amyocardial%20tissue%20and%20the%20lack%20of%20distinctive%20features.%20Existing%20approaches%20can%0Abe%20broadly%20categorized%20into%20image%20based%20and%20representation-based%2C%20each%20with%20its%0Alimitations.%20Image-based%20methods%2C%20including%20both%20raditional%20and%20deep%0Alearning-based%20registration%20approaches%2C%20either%20struggle%20with%20topological%0Aconsistency%20or%20rely%20heavily%20on%20extensive%20training%20data.%20Representation-based%0Amethods%2C%20while%20promising%2C%20often%20suffer%20from%20loss%20of%20image-level%20details.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20Dynamic%203D%20Gaussian%20Representation%0A%28Dyna3DGR%29%2C%20a%20novel%20framework%20that%20combines%20explicit%203D%20Gaussian%20representation%0Awith%20implicit%20neural%20motion%20field%20modeling.%20Our%20method%20simultaneously%20optimizes%0Acardiac%20structure%20and%20motion%20in%20a%20self-supervised%20manner%2C%20eliminating%20the%20need%0Afor%20extensive%20training%20data%20or%20point-to-point%20correspondences.%20Through%0Adifferentiable%20volumetric%20rendering%2C%20Dyna3DGR%20efficiently%20bridges%20continuous%0Amotion%20representation%20with%20image-space%20alignment%20while%20preserving%20both%0Atopological%20and%20temporal%20consistency.%20Comprehensive%20evaluations%20on%20the%20ACDC%0Adataset%20demonstrate%20that%20our%20approach%20surpasses%20state-of-the-art%20deep%0Alearning-based%20diffeomorphic%20registration%20methods%20in%20tracking%20accuracy.%20The%0Acode%20will%20be%20available%20in%20https%3A//github.com/windrise/Dyna3DGR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyna3DGR%253A%25204D%2520Cardiac%2520Motion%2520Tracking%2520with%2520Dynamic%25203D%2520Gaussian%250A%2520%2520Representation%26entry.906535625%3DXueming%2520Fu%2520and%2520Pei%2520Wu%2520and%2520Yingtai%2520Li%2520and%2520Xin%2520Luo%2520and%2520Zihang%2520Jiang%2520and%2520Junhao%2520Mei%2520and%2520Jian%2520Lu%2520and%2520Gao-Jun%2520Teng%2520and%2520S.%2520Kevin%2520Zhou%26entry.1292438233%3D%2520%2520Accurate%2520analysis%2520of%2520cardiac%2520motion%2520is%2520crucial%2520for%2520evaluating%2520cardiac%250Afunction.%2520While%2520dynamic%2520cardiac%2520magnetic%2520resonance%2520imaging%2520%2528CMR%2529%2520can%2520capture%250Adetailed%2520tissue%2520motion%2520throughout%2520the%2520cardiac%2520cycle%252C%2520the%2520fine-grained%25204D%250Acardiac%2520motion%2520tracking%2520remains%2520challenging%2520due%2520to%2520the%2520homogeneous%2520nature%2520of%250Amyocardial%2520tissue%2520and%2520the%2520lack%2520of%2520distinctive%2520features.%2520Existing%2520approaches%2520can%250Abe%2520broadly%2520categorized%2520into%2520image%2520based%2520and%2520representation-based%252C%2520each%2520with%2520its%250Alimitations.%2520Image-based%2520methods%252C%2520including%2520both%2520raditional%2520and%2520deep%250Alearning-based%2520registration%2520approaches%252C%2520either%2520struggle%2520with%2520topological%250Aconsistency%2520or%2520rely%2520heavily%2520on%2520extensive%2520training%2520data.%2520Representation-based%250Amethods%252C%2520while%2520promising%252C%2520often%2520suffer%2520from%2520loss%2520of%2520image-level%2520details.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520Dynamic%25203D%2520Gaussian%2520Representation%250A%2528Dyna3DGR%2529%252C%2520a%2520novel%2520framework%2520that%2520combines%2520explicit%25203D%2520Gaussian%2520representation%250Awith%2520implicit%2520neural%2520motion%2520field%2520modeling.%2520Our%2520method%2520simultaneously%2520optimizes%250Acardiac%2520structure%2520and%2520motion%2520in%2520a%2520self-supervised%2520manner%252C%2520eliminating%2520the%2520need%250Afor%2520extensive%2520training%2520data%2520or%2520point-to-point%2520correspondences.%2520Through%250Adifferentiable%2520volumetric%2520rendering%252C%2520Dyna3DGR%2520efficiently%2520bridges%2520continuous%250Amotion%2520representation%2520with%2520image-space%2520alignment%2520while%2520preserving%2520both%250Atopological%2520and%2520temporal%2520consistency.%2520Comprehensive%2520evaluations%2520on%2520the%2520ACDC%250Adataset%2520demonstrate%2520that%2520our%2520approach%2520surpasses%2520state-of-the-art%2520deep%250Alearning-based%2520diffeomorphic%2520registration%2520methods%2520in%2520tracking%2520accuracy.%2520The%250Acode%2520will%2520be%2520available%2520in%2520https%253A//github.com/windrise/Dyna3DGR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dyna3DGR%3A%204D%20Cardiac%20Motion%20Tracking%20with%20Dynamic%203D%20Gaussian%0A%20%20Representation&entry.906535625=Xueming%20Fu%20and%20Pei%20Wu%20and%20Yingtai%20Li%20and%20Xin%20Luo%20and%20Zihang%20Jiang%20and%20Junhao%20Mei%20and%20Jian%20Lu%20and%20Gao-Jun%20Teng%20and%20S.%20Kevin%20Zhou&entry.1292438233=%20%20Accurate%20analysis%20of%20cardiac%20motion%20is%20crucial%20for%20evaluating%20cardiac%0Afunction.%20While%20dynamic%20cardiac%20magnetic%20resonance%20imaging%20%28CMR%29%20can%20capture%0Adetailed%20tissue%20motion%20throughout%20the%20cardiac%20cycle%2C%20the%20fine-grained%204D%0Acardiac%20motion%20tracking%20remains%20challenging%20due%20to%20the%20homogeneous%20nature%20of%0Amyocardial%20tissue%20and%20the%20lack%20of%20distinctive%20features.%20Existing%20approaches%20can%0Abe%20broadly%20categorized%20into%20image%20based%20and%20representation-based%2C%20each%20with%20its%0Alimitations.%20Image-based%20methods%2C%20including%20both%20raditional%20and%20deep%0Alearning-based%20registration%20approaches%2C%20either%20struggle%20with%20topological%0Aconsistency%20or%20rely%20heavily%20on%20extensive%20training%20data.%20Representation-based%0Amethods%2C%20while%20promising%2C%20often%20suffer%20from%20loss%20of%20image-level%20details.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20Dynamic%203D%20Gaussian%20Representation%0A%28Dyna3DGR%29%2C%20a%20novel%20framework%20that%20combines%20explicit%203D%20Gaussian%20representation%0Awith%20implicit%20neural%20motion%20field%20modeling.%20Our%20method%20simultaneously%20optimizes%0Acardiac%20structure%20and%20motion%20in%20a%20self-supervised%20manner%2C%20eliminating%20the%20need%0Afor%20extensive%20training%20data%20or%20point-to-point%20correspondences.%20Through%0Adifferentiable%20volumetric%20rendering%2C%20Dyna3DGR%20efficiently%20bridges%20continuous%0Amotion%20representation%20with%20image-space%20alignment%20while%20preserving%20both%0Atopological%20and%20temporal%20consistency.%20Comprehensive%20evaluations%20on%20the%20ACDC%0Adataset%20demonstrate%20that%20our%20approach%20surpasses%20state-of-the-art%20deep%0Alearning-based%20diffeomorphic%20registration%20methods%20in%20tracking%20accuracy.%20The%0Acode%20will%20be%20available%20in%20https%3A//github.com/windrise/Dyna3DGR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16608v1&entry.124074799=Read"},
{"title": "DFR: A Decompose-Fuse-Reconstruct Framework for Multi-Modal Few-Shot\n  Segmentation", "author": "Shuai Chen and Fanman Meng and Xiwei Zhang and Haoran Wei and Chenhao Wu and Qingbo Wu and Hongliang Li", "abstract": "  This paper presents DFR (Decompose, Fuse and Reconstruct), a novel framework\nthat addresses the fundamental challenge of effectively utilizing multi-modal\nguidance in few-shot segmentation (FSS). While existing approaches primarily\nrely on visual support samples or textual descriptions, their single or\ndual-modal paradigms limit exploitation of rich perceptual information\navailable in real-world scenarios. To overcome this limitation, the proposed\napproach leverages the Segment Anything Model (SAM) to systematically integrate\nvisual, textual, and audio modalities for enhanced semantic understanding. The\nDFR framework introduces three key innovations: 1) Multi-modal Decompose: a\nhierarchical decomposition scheme that extracts visual region proposals via\nSAM, expands textual semantics into fine-grained descriptors, and processes\naudio features for contextual enrichment; 2) Multi-modal Contrastive Fuse: a\nfusion strategy employing contrastive learning to maintain consistency across\nvisual, textual, and audio modalities while enabling dynamic semantic\ninteractions between foreground and background features; 3) Dual-path\nReconstruct: an adaptive integration mechanism combining semantic guidance from\ntri-modal fused tokens with geometric cues from multi-modal location priors.\nExtensive experiments across visual, textual, and audio modalities under both\nsynthetic and real settings demonstrate DFR's substantial performance\nimprovements over state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2507.16736v1", "date": "2025-07-22", "relevancy": 2.9994, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.607}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DFR%3A%20A%20Decompose-Fuse-Reconstruct%20Framework%20for%20Multi-Modal%20Few-Shot%0A%20%20Segmentation&body=Title%3A%20DFR%3A%20A%20Decompose-Fuse-Reconstruct%20Framework%20for%20Multi-Modal%20Few-Shot%0A%20%20Segmentation%0AAuthor%3A%20Shuai%20Chen%20and%20Fanman%20Meng%20and%20Xiwei%20Zhang%20and%20Haoran%20Wei%20and%20Chenhao%20Wu%20and%20Qingbo%20Wu%20and%20Hongliang%20Li%0AAbstract%3A%20%20%20This%20paper%20presents%20DFR%20%28Decompose%2C%20Fuse%20and%20Reconstruct%29%2C%20a%20novel%20framework%0Athat%20addresses%20the%20fundamental%20challenge%20of%20effectively%20utilizing%20multi-modal%0Aguidance%20in%20few-shot%20segmentation%20%28FSS%29.%20While%20existing%20approaches%20primarily%0Arely%20on%20visual%20support%20samples%20or%20textual%20descriptions%2C%20their%20single%20or%0Adual-modal%20paradigms%20limit%20exploitation%20of%20rich%20perceptual%20information%0Aavailable%20in%20real-world%20scenarios.%20To%20overcome%20this%20limitation%2C%20the%20proposed%0Aapproach%20leverages%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20systematically%20integrate%0Avisual%2C%20textual%2C%20and%20audio%20modalities%20for%20enhanced%20semantic%20understanding.%20The%0ADFR%20framework%20introduces%20three%20key%20innovations%3A%201%29%20Multi-modal%20Decompose%3A%20a%0Ahierarchical%20decomposition%20scheme%20that%20extracts%20visual%20region%20proposals%20via%0ASAM%2C%20expands%20textual%20semantics%20into%20fine-grained%20descriptors%2C%20and%20processes%0Aaudio%20features%20for%20contextual%20enrichment%3B%202%29%20Multi-modal%20Contrastive%20Fuse%3A%20a%0Afusion%20strategy%20employing%20contrastive%20learning%20to%20maintain%20consistency%20across%0Avisual%2C%20textual%2C%20and%20audio%20modalities%20while%20enabling%20dynamic%20semantic%0Ainteractions%20between%20foreground%20and%20background%20features%3B%203%29%20Dual-path%0AReconstruct%3A%20an%20adaptive%20integration%20mechanism%20combining%20semantic%20guidance%20from%0Atri-modal%20fused%20tokens%20with%20geometric%20cues%20from%20multi-modal%20location%20priors.%0AExtensive%20experiments%20across%20visual%2C%20textual%2C%20and%20audio%20modalities%20under%20both%0Asynthetic%20and%20real%20settings%20demonstrate%20DFR%27s%20substantial%20performance%0Aimprovements%20over%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDFR%253A%2520A%2520Decompose-Fuse-Reconstruct%2520Framework%2520for%2520Multi-Modal%2520Few-Shot%250A%2520%2520Segmentation%26entry.906535625%3DShuai%2520Chen%2520and%2520Fanman%2520Meng%2520and%2520Xiwei%2520Zhang%2520and%2520Haoran%2520Wei%2520and%2520Chenhao%2520Wu%2520and%2520Qingbo%2520Wu%2520and%2520Hongliang%2520Li%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520DFR%2520%2528Decompose%252C%2520Fuse%2520and%2520Reconstruct%2529%252C%2520a%2520novel%2520framework%250Athat%2520addresses%2520the%2520fundamental%2520challenge%2520of%2520effectively%2520utilizing%2520multi-modal%250Aguidance%2520in%2520few-shot%2520segmentation%2520%2528FSS%2529.%2520While%2520existing%2520approaches%2520primarily%250Arely%2520on%2520visual%2520support%2520samples%2520or%2520textual%2520descriptions%252C%2520their%2520single%2520or%250Adual-modal%2520paradigms%2520limit%2520exploitation%2520of%2520rich%2520perceptual%2520information%250Aavailable%2520in%2520real-world%2520scenarios.%2520To%2520overcome%2520this%2520limitation%252C%2520the%2520proposed%250Aapproach%2520leverages%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520to%2520systematically%2520integrate%250Avisual%252C%2520textual%252C%2520and%2520audio%2520modalities%2520for%2520enhanced%2520semantic%2520understanding.%2520The%250ADFR%2520framework%2520introduces%2520three%2520key%2520innovations%253A%25201%2529%2520Multi-modal%2520Decompose%253A%2520a%250Ahierarchical%2520decomposition%2520scheme%2520that%2520extracts%2520visual%2520region%2520proposals%2520via%250ASAM%252C%2520expands%2520textual%2520semantics%2520into%2520fine-grained%2520descriptors%252C%2520and%2520processes%250Aaudio%2520features%2520for%2520contextual%2520enrichment%253B%25202%2529%2520Multi-modal%2520Contrastive%2520Fuse%253A%2520a%250Afusion%2520strategy%2520employing%2520contrastive%2520learning%2520to%2520maintain%2520consistency%2520across%250Avisual%252C%2520textual%252C%2520and%2520audio%2520modalities%2520while%2520enabling%2520dynamic%2520semantic%250Ainteractions%2520between%2520foreground%2520and%2520background%2520features%253B%25203%2529%2520Dual-path%250AReconstruct%253A%2520an%2520adaptive%2520integration%2520mechanism%2520combining%2520semantic%2520guidance%2520from%250Atri-modal%2520fused%2520tokens%2520with%2520geometric%2520cues%2520from%2520multi-modal%2520location%2520priors.%250AExtensive%2520experiments%2520across%2520visual%252C%2520textual%252C%2520and%2520audio%2520modalities%2520under%2520both%250Asynthetic%2520and%2520real%2520settings%2520demonstrate%2520DFR%2527s%2520substantial%2520performance%250Aimprovements%2520over%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DFR%3A%20A%20Decompose-Fuse-Reconstruct%20Framework%20for%20Multi-Modal%20Few-Shot%0A%20%20Segmentation&entry.906535625=Shuai%20Chen%20and%20Fanman%20Meng%20and%20Xiwei%20Zhang%20and%20Haoran%20Wei%20and%20Chenhao%20Wu%20and%20Qingbo%20Wu%20and%20Hongliang%20Li&entry.1292438233=%20%20This%20paper%20presents%20DFR%20%28Decompose%2C%20Fuse%20and%20Reconstruct%29%2C%20a%20novel%20framework%0Athat%20addresses%20the%20fundamental%20challenge%20of%20effectively%20utilizing%20multi-modal%0Aguidance%20in%20few-shot%20segmentation%20%28FSS%29.%20While%20existing%20approaches%20primarily%0Arely%20on%20visual%20support%20samples%20or%20textual%20descriptions%2C%20their%20single%20or%0Adual-modal%20paradigms%20limit%20exploitation%20of%20rich%20perceptual%20information%0Aavailable%20in%20real-world%20scenarios.%20To%20overcome%20this%20limitation%2C%20the%20proposed%0Aapproach%20leverages%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20systematically%20integrate%0Avisual%2C%20textual%2C%20and%20audio%20modalities%20for%20enhanced%20semantic%20understanding.%20The%0ADFR%20framework%20introduces%20three%20key%20innovations%3A%201%29%20Multi-modal%20Decompose%3A%20a%0Ahierarchical%20decomposition%20scheme%20that%20extracts%20visual%20region%20proposals%20via%0ASAM%2C%20expands%20textual%20semantics%20into%20fine-grained%20descriptors%2C%20and%20processes%0Aaudio%20features%20for%20contextual%20enrichment%3B%202%29%20Multi-modal%20Contrastive%20Fuse%3A%20a%0Afusion%20strategy%20employing%20contrastive%20learning%20to%20maintain%20consistency%20across%0Avisual%2C%20textual%2C%20and%20audio%20modalities%20while%20enabling%20dynamic%20semantic%0Ainteractions%20between%20foreground%20and%20background%20features%3B%203%29%20Dual-path%0AReconstruct%3A%20an%20adaptive%20integration%20mechanism%20combining%20semantic%20guidance%20from%0Atri-modal%20fused%20tokens%20with%20geometric%20cues%20from%20multi-modal%20location%20priors.%0AExtensive%20experiments%20across%20visual%2C%20textual%2C%20and%20audio%20modalities%20under%20both%0Asynthetic%20and%20real%20settings%20demonstrate%20DFR%27s%20substantial%20performance%0Aimprovements%20over%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16736v1&entry.124074799=Read"},
{"title": "Curve-Aware Gaussian Splatting for 3D Parametric Curve Reconstruction", "author": "Zhirui Gao and Renjiao Yi and Yaqiao Dai and Xuening Zhu and Wei Chen and Chenyang Zhu and Kai Xu", "abstract": "  This paper presents an end-to-end framework for reconstructing 3D parametric\ncurves directly from multi-view edge maps. Contrasting with existing two-stage\nmethods that follow a sequential ``edge point cloud reconstruction and\nparametric curve fitting'' pipeline, our one-stage approach optimizes 3D\nparametric curves directly from 2D edge maps, eliminating error accumulation\ncaused by the inherent optimization gap between disconnected stages. However,\nparametric curves inherently lack suitability for rendering-based multi-view\noptimization, necessitating a complementary representation that preserves their\ngeometric properties while enabling differentiable rendering. We propose a\nnovel bi-directional coupling mechanism between parametric curves and\nedge-oriented Gaussian components. This tight correspondence formulates a\ncurve-aware Gaussian representation, \\textbf{CurveGaussian}, that enables\ndifferentiable rendering of 3D curves, allowing direct optimization guided by\nmulti-view evidence. Furthermore, we introduce a dynamically adaptive topology\noptimization framework during training to refine curve structures through\nlinearization, merging, splitting, and pruning operations. Comprehensive\nevaluations on the ABC dataset and real-world benchmarks demonstrate our\none-stage method's superiority over two-stage alternatives, particularly in\nproducing cleaner and more robust reconstructions. Additionally, by directly\noptimizing parametric curves, our method significantly reduces the parameter\ncount during training, achieving both higher efficiency and superior\nperformance compared to existing approaches.\n", "link": "http://arxiv.org/abs/2506.21401v3", "date": "2025-07-22", "relevancy": 2.9841, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.605}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5974}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curve-Aware%20Gaussian%20Splatting%20for%203D%20Parametric%20Curve%20Reconstruction&body=Title%3A%20Curve-Aware%20Gaussian%20Splatting%20for%203D%20Parametric%20Curve%20Reconstruction%0AAuthor%3A%20Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Yaqiao%20Dai%20and%20Xuening%20Zhu%20and%20Wei%20Chen%20and%20Chenyang%20Zhu%20and%20Kai%20Xu%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20end-to-end%20framework%20for%20reconstructing%203D%20parametric%0Acurves%20directly%20from%20multi-view%20edge%20maps.%20Contrasting%20with%20existing%20two-stage%0Amethods%20that%20follow%20a%20sequential%20%60%60edge%20point%20cloud%20reconstruction%20and%0Aparametric%20curve%20fitting%27%27%20pipeline%2C%20our%20one-stage%20approach%20optimizes%203D%0Aparametric%20curves%20directly%20from%202D%20edge%20maps%2C%20eliminating%20error%20accumulation%0Acaused%20by%20the%20inherent%20optimization%20gap%20between%20disconnected%20stages.%20However%2C%0Aparametric%20curves%20inherently%20lack%20suitability%20for%20rendering-based%20multi-view%0Aoptimization%2C%20necessitating%20a%20complementary%20representation%20that%20preserves%20their%0Ageometric%20properties%20while%20enabling%20differentiable%20rendering.%20We%20propose%20a%0Anovel%20bi-directional%20coupling%20mechanism%20between%20parametric%20curves%20and%0Aedge-oriented%20Gaussian%20components.%20This%20tight%20correspondence%20formulates%20a%0Acurve-aware%20Gaussian%20representation%2C%20%5Ctextbf%7BCurveGaussian%7D%2C%20that%20enables%0Adifferentiable%20rendering%20of%203D%20curves%2C%20allowing%20direct%20optimization%20guided%20by%0Amulti-view%20evidence.%20Furthermore%2C%20we%20introduce%20a%20dynamically%20adaptive%20topology%0Aoptimization%20framework%20during%20training%20to%20refine%20curve%20structures%20through%0Alinearization%2C%20merging%2C%20splitting%2C%20and%20pruning%20operations.%20Comprehensive%0Aevaluations%20on%20the%20ABC%20dataset%20and%20real-world%20benchmarks%20demonstrate%20our%0Aone-stage%20method%27s%20superiority%20over%20two-stage%20alternatives%2C%20particularly%20in%0Aproducing%20cleaner%20and%20more%20robust%20reconstructions.%20Additionally%2C%20by%20directly%0Aoptimizing%20parametric%20curves%2C%20our%20method%20significantly%20reduces%20the%20parameter%0Acount%20during%20training%2C%20achieving%20both%20higher%20efficiency%20and%20superior%0Aperformance%20compared%20to%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21401v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurve-Aware%2520Gaussian%2520Splatting%2520for%25203D%2520Parametric%2520Curve%2520Reconstruction%26entry.906535625%3DZhirui%2520Gao%2520and%2520Renjiao%2520Yi%2520and%2520Yaqiao%2520Dai%2520and%2520Xuening%2520Zhu%2520and%2520Wei%2520Chen%2520and%2520Chenyang%2520Zhu%2520and%2520Kai%2520Xu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520end-to-end%2520framework%2520for%2520reconstructing%25203D%2520parametric%250Acurves%2520directly%2520from%2520multi-view%2520edge%2520maps.%2520Contrasting%2520with%2520existing%2520two-stage%250Amethods%2520that%2520follow%2520a%2520sequential%2520%2560%2560edge%2520point%2520cloud%2520reconstruction%2520and%250Aparametric%2520curve%2520fitting%2527%2527%2520pipeline%252C%2520our%2520one-stage%2520approach%2520optimizes%25203D%250Aparametric%2520curves%2520directly%2520from%25202D%2520edge%2520maps%252C%2520eliminating%2520error%2520accumulation%250Acaused%2520by%2520the%2520inherent%2520optimization%2520gap%2520between%2520disconnected%2520stages.%2520However%252C%250Aparametric%2520curves%2520inherently%2520lack%2520suitability%2520for%2520rendering-based%2520multi-view%250Aoptimization%252C%2520necessitating%2520a%2520complementary%2520representation%2520that%2520preserves%2520their%250Ageometric%2520properties%2520while%2520enabling%2520differentiable%2520rendering.%2520We%2520propose%2520a%250Anovel%2520bi-directional%2520coupling%2520mechanism%2520between%2520parametric%2520curves%2520and%250Aedge-oriented%2520Gaussian%2520components.%2520This%2520tight%2520correspondence%2520formulates%2520a%250Acurve-aware%2520Gaussian%2520representation%252C%2520%255Ctextbf%257BCurveGaussian%257D%252C%2520that%2520enables%250Adifferentiable%2520rendering%2520of%25203D%2520curves%252C%2520allowing%2520direct%2520optimization%2520guided%2520by%250Amulti-view%2520evidence.%2520Furthermore%252C%2520we%2520introduce%2520a%2520dynamically%2520adaptive%2520topology%250Aoptimization%2520framework%2520during%2520training%2520to%2520refine%2520curve%2520structures%2520through%250Alinearization%252C%2520merging%252C%2520splitting%252C%2520and%2520pruning%2520operations.%2520Comprehensive%250Aevaluations%2520on%2520the%2520ABC%2520dataset%2520and%2520real-world%2520benchmarks%2520demonstrate%2520our%250Aone-stage%2520method%2527s%2520superiority%2520over%2520two-stage%2520alternatives%252C%2520particularly%2520in%250Aproducing%2520cleaner%2520and%2520more%2520robust%2520reconstructions.%2520Additionally%252C%2520by%2520directly%250Aoptimizing%2520parametric%2520curves%252C%2520our%2520method%2520significantly%2520reduces%2520the%2520parameter%250Acount%2520during%2520training%252C%2520achieving%2520both%2520higher%2520efficiency%2520and%2520superior%250Aperformance%2520compared%2520to%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21401v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curve-Aware%20Gaussian%20Splatting%20for%203D%20Parametric%20Curve%20Reconstruction&entry.906535625=Zhirui%20Gao%20and%20Renjiao%20Yi%20and%20Yaqiao%20Dai%20and%20Xuening%20Zhu%20and%20Wei%20Chen%20and%20Chenyang%20Zhu%20and%20Kai%20Xu&entry.1292438233=%20%20This%20paper%20presents%20an%20end-to-end%20framework%20for%20reconstructing%203D%20parametric%0Acurves%20directly%20from%20multi-view%20edge%20maps.%20Contrasting%20with%20existing%20two-stage%0Amethods%20that%20follow%20a%20sequential%20%60%60edge%20point%20cloud%20reconstruction%20and%0Aparametric%20curve%20fitting%27%27%20pipeline%2C%20our%20one-stage%20approach%20optimizes%203D%0Aparametric%20curves%20directly%20from%202D%20edge%20maps%2C%20eliminating%20error%20accumulation%0Acaused%20by%20the%20inherent%20optimization%20gap%20between%20disconnected%20stages.%20However%2C%0Aparametric%20curves%20inherently%20lack%20suitability%20for%20rendering-based%20multi-view%0Aoptimization%2C%20necessitating%20a%20complementary%20representation%20that%20preserves%20their%0Ageometric%20properties%20while%20enabling%20differentiable%20rendering.%20We%20propose%20a%0Anovel%20bi-directional%20coupling%20mechanism%20between%20parametric%20curves%20and%0Aedge-oriented%20Gaussian%20components.%20This%20tight%20correspondence%20formulates%20a%0Acurve-aware%20Gaussian%20representation%2C%20%5Ctextbf%7BCurveGaussian%7D%2C%20that%20enables%0Adifferentiable%20rendering%20of%203D%20curves%2C%20allowing%20direct%20optimization%20guided%20by%0Amulti-view%20evidence.%20Furthermore%2C%20we%20introduce%20a%20dynamically%20adaptive%20topology%0Aoptimization%20framework%20during%20training%20to%20refine%20curve%20structures%20through%0Alinearization%2C%20merging%2C%20splitting%2C%20and%20pruning%20operations.%20Comprehensive%0Aevaluations%20on%20the%20ABC%20dataset%20and%20real-world%20benchmarks%20demonstrate%20our%0Aone-stage%20method%27s%20superiority%20over%20two-stage%20alternatives%2C%20particularly%20in%0Aproducing%20cleaner%20and%20more%20robust%20reconstructions.%20Additionally%2C%20by%20directly%0Aoptimizing%20parametric%20curves%2C%20our%20method%20significantly%20reduces%20the%20parameter%0Acount%20during%20training%2C%20achieving%20both%20higher%20efficiency%20and%20superior%0Aperformance%20compared%20to%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21401v3&entry.124074799=Read"},
{"title": "ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness", "author": "Boqian Li and Haiwen Feng and Zeyu Cai and Michael J. Black and Yuliang Xiu", "abstract": "  Fitting a body to a 3D clothed human point cloud is a common yet challenging\ntask. Traditional optimization-based approaches use multi-stage pipelines that\nare sensitive to pose initialization, while recent learning-based methods often\nstruggle with generalization across diverse poses and garment types. We propose\nEquivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline\nthat estimates cloth-to-body surface mapping through locally approximate SE(3)\nequivariance, encoding tightness as displacement vectors from the cloth surface\nto the underlying body. Following this mapping, pose-invariant body features\nregress sparse body markers, simplifying clothed human fitting into an\ninner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show\nthat ETCH significantly outperforms state-of-the-art methods -- both\ntightness-agnostic and tightness-aware -- in body fitting accuracy on loose\nclothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant\ntightness design can even reduce directional errors by (67.2% ~ 89.8%) in\none-shot (or out-of-distribution) settings (~ 1% data). Qualitative results\ndemonstrate strong generalization of ETCH, regardless of challenging poses,\nunseen shapes, loose clothing, and non-rigid dynamics. We will release the code\nand models soon for research purposes at https://boqian-li.github.io/ETCH/.\n", "link": "http://arxiv.org/abs/2503.10624v2", "date": "2025-07-22", "relevancy": 2.94, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5966}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5864}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ETCH%3A%20Generalizing%20Body%20Fitting%20to%20Clothed%20Humans%20via%20Equivariant%0A%20%20Tightness&body=Title%3A%20ETCH%3A%20Generalizing%20Body%20Fitting%20to%20Clothed%20Humans%20via%20Equivariant%0A%20%20Tightness%0AAuthor%3A%20Boqian%20Li%20and%20Haiwen%20Feng%20and%20Zeyu%20Cai%20and%20Michael%20J.%20Black%20and%20Yuliang%20Xiu%0AAbstract%3A%20%20%20Fitting%20a%20body%20to%20a%203D%20clothed%20human%20point%20cloud%20is%20a%20common%20yet%20challenging%0Atask.%20Traditional%20optimization-based%20approaches%20use%20multi-stage%20pipelines%20that%0Aare%20sensitive%20to%20pose%20initialization%2C%20while%20recent%20learning-based%20methods%20often%0Astruggle%20with%20generalization%20across%20diverse%20poses%20and%20garment%20types.%20We%20propose%0AEquivariant%20Tightness%20Fitting%20for%20Clothed%20Humans%2C%20or%20ETCH%2C%20a%20novel%20pipeline%0Athat%20estimates%20cloth-to-body%20surface%20mapping%20through%20locally%20approximate%20SE%283%29%0Aequivariance%2C%20encoding%20tightness%20as%20displacement%20vectors%20from%20the%20cloth%20surface%0Ato%20the%20underlying%20body.%20Following%20this%20mapping%2C%20pose-invariant%20body%20features%0Aregress%20sparse%20body%20markers%2C%20simplifying%20clothed%20human%20fitting%20into%20an%0Ainner-body%20marker%20fitting%20task.%20Extensive%20experiments%20on%20CAPE%20and%204D-Dress%20show%0Athat%20ETCH%20significantly%20outperforms%20state-of-the-art%20methods%20--%20both%0Atightness-agnostic%20and%20tightness-aware%20--%20in%20body%20fitting%20accuracy%20on%20loose%0Aclothing%20%2816.7%25%20~%2069.5%25%29%20and%20shape%20accuracy%20%28average%2049.9%25%29.%20Our%20equivariant%0Atightness%20design%20can%20even%20reduce%20directional%20errors%20by%20%2867.2%25%20~%2089.8%25%29%20in%0Aone-shot%20%28or%20out-of-distribution%29%20settings%20%28~%201%25%20data%29.%20Qualitative%20results%0Ademonstrate%20strong%20generalization%20of%20ETCH%2C%20regardless%20of%20challenging%20poses%2C%0Aunseen%20shapes%2C%20loose%20clothing%2C%20and%20non-rigid%20dynamics.%20We%20will%20release%20the%20code%0Aand%20models%20soon%20for%20research%20purposes%20at%20https%3A//boqian-li.github.io/ETCH/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10624v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DETCH%253A%2520Generalizing%2520Body%2520Fitting%2520to%2520Clothed%2520Humans%2520via%2520Equivariant%250A%2520%2520Tightness%26entry.906535625%3DBoqian%2520Li%2520and%2520Haiwen%2520Feng%2520and%2520Zeyu%2520Cai%2520and%2520Michael%2520J.%2520Black%2520and%2520Yuliang%2520Xiu%26entry.1292438233%3D%2520%2520Fitting%2520a%2520body%2520to%2520a%25203D%2520clothed%2520human%2520point%2520cloud%2520is%2520a%2520common%2520yet%2520challenging%250Atask.%2520Traditional%2520optimization-based%2520approaches%2520use%2520multi-stage%2520pipelines%2520that%250Aare%2520sensitive%2520to%2520pose%2520initialization%252C%2520while%2520recent%2520learning-based%2520methods%2520often%250Astruggle%2520with%2520generalization%2520across%2520diverse%2520poses%2520and%2520garment%2520types.%2520We%2520propose%250AEquivariant%2520Tightness%2520Fitting%2520for%2520Clothed%2520Humans%252C%2520or%2520ETCH%252C%2520a%2520novel%2520pipeline%250Athat%2520estimates%2520cloth-to-body%2520surface%2520mapping%2520through%2520locally%2520approximate%2520SE%25283%2529%250Aequivariance%252C%2520encoding%2520tightness%2520as%2520displacement%2520vectors%2520from%2520the%2520cloth%2520surface%250Ato%2520the%2520underlying%2520body.%2520Following%2520this%2520mapping%252C%2520pose-invariant%2520body%2520features%250Aregress%2520sparse%2520body%2520markers%252C%2520simplifying%2520clothed%2520human%2520fitting%2520into%2520an%250Ainner-body%2520marker%2520fitting%2520task.%2520Extensive%2520experiments%2520on%2520CAPE%2520and%25204D-Dress%2520show%250Athat%2520ETCH%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520--%2520both%250Atightness-agnostic%2520and%2520tightness-aware%2520--%2520in%2520body%2520fitting%2520accuracy%2520on%2520loose%250Aclothing%2520%252816.7%2525%2520~%252069.5%2525%2529%2520and%2520shape%2520accuracy%2520%2528average%252049.9%2525%2529.%2520Our%2520equivariant%250Atightness%2520design%2520can%2520even%2520reduce%2520directional%2520errors%2520by%2520%252867.2%2525%2520~%252089.8%2525%2529%2520in%250Aone-shot%2520%2528or%2520out-of-distribution%2529%2520settings%2520%2528~%25201%2525%2520data%2529.%2520Qualitative%2520results%250Ademonstrate%2520strong%2520generalization%2520of%2520ETCH%252C%2520regardless%2520of%2520challenging%2520poses%252C%250Aunseen%2520shapes%252C%2520loose%2520clothing%252C%2520and%2520non-rigid%2520dynamics.%2520We%2520will%2520release%2520the%2520code%250Aand%2520models%2520soon%2520for%2520research%2520purposes%2520at%2520https%253A//boqian-li.github.io/ETCH/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10624v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ETCH%3A%20Generalizing%20Body%20Fitting%20to%20Clothed%20Humans%20via%20Equivariant%0A%20%20Tightness&entry.906535625=Boqian%20Li%20and%20Haiwen%20Feng%20and%20Zeyu%20Cai%20and%20Michael%20J.%20Black%20and%20Yuliang%20Xiu&entry.1292438233=%20%20Fitting%20a%20body%20to%20a%203D%20clothed%20human%20point%20cloud%20is%20a%20common%20yet%20challenging%0Atask.%20Traditional%20optimization-based%20approaches%20use%20multi-stage%20pipelines%20that%0Aare%20sensitive%20to%20pose%20initialization%2C%20while%20recent%20learning-based%20methods%20often%0Astruggle%20with%20generalization%20across%20diverse%20poses%20and%20garment%20types.%20We%20propose%0AEquivariant%20Tightness%20Fitting%20for%20Clothed%20Humans%2C%20or%20ETCH%2C%20a%20novel%20pipeline%0Athat%20estimates%20cloth-to-body%20surface%20mapping%20through%20locally%20approximate%20SE%283%29%0Aequivariance%2C%20encoding%20tightness%20as%20displacement%20vectors%20from%20the%20cloth%20surface%0Ato%20the%20underlying%20body.%20Following%20this%20mapping%2C%20pose-invariant%20body%20features%0Aregress%20sparse%20body%20markers%2C%20simplifying%20clothed%20human%20fitting%20into%20an%0Ainner-body%20marker%20fitting%20task.%20Extensive%20experiments%20on%20CAPE%20and%204D-Dress%20show%0Athat%20ETCH%20significantly%20outperforms%20state-of-the-art%20methods%20--%20both%0Atightness-agnostic%20and%20tightness-aware%20--%20in%20body%20fitting%20accuracy%20on%20loose%0Aclothing%20%2816.7%25%20~%2069.5%25%29%20and%20shape%20accuracy%20%28average%2049.9%25%29.%20Our%20equivariant%0Atightness%20design%20can%20even%20reduce%20directional%20errors%20by%20%2867.2%25%20~%2089.8%25%29%20in%0Aone-shot%20%28or%20out-of-distribution%29%20settings%20%28~%201%25%20data%29.%20Qualitative%20results%0Ademonstrate%20strong%20generalization%20of%20ETCH%2C%20regardless%20of%20challenging%20poses%2C%0Aunseen%20shapes%2C%20loose%20clothing%2C%20and%20non-rigid%20dynamics.%20We%20will%20release%20the%20code%0Aand%20models%20soon%20for%20research%20purposes%20at%20https%3A//boqian-li.github.io/ETCH/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10624v2&entry.124074799=Read"},
{"title": "A Multimodal Deviation Perceiving Framework for Weakly-Supervised\n  Temporal Forgery Localization", "author": "Wenbo Xu and Junyan Wu and Wei Lu and Xiangyang Luo and Qian Wang", "abstract": "  Current researches on Deepfake forensics often treat detection as a\nclassification task or temporal forgery localization problem, which are usually\nrestrictive, time-consuming, and challenging to scale for large datasets. To\nresolve these issues, we present a multimodal deviation perceiving framework\nfor weakly-supervised temporal forgery localization (MDP), which aims to\nidentify temporal partial forged segments using only video-level annotations.\nThe MDP proposes a novel multimodal interaction mechanism (MI) and an\nextensible deviation perceiving loss to perceive multimodal deviation, which\nachieves the refined start and end timestamps localization of forged segments.\nSpecifically, MI introduces a temporal property preserving cross-modal\nattention to measure the relevance between the visual and audio modalities in\nthe probabilistic embedding space. It could identify the inter-modality\ndeviation and construct comprehensive video features for temporal forgery\nlocalization. To explore further temporal deviation for weakly-supervised\nlearning, an extensible deviation perceiving loss has been proposed, aiming at\nenlarging the deviation of adjacent segments of the forged samples and reducing\nthat of genuine samples. Extensive experiments demonstrate the effectiveness of\nthe proposed framework and achieve comparable results to fully-supervised\napproaches in several evaluation metrics.\n", "link": "http://arxiv.org/abs/2507.16596v1", "date": "2025-07-22", "relevancy": 2.8671, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5936}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5744}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multimodal%20Deviation%20Perceiving%20Framework%20for%20Weakly-Supervised%0A%20%20Temporal%20Forgery%20Localization&body=Title%3A%20A%20Multimodal%20Deviation%20Perceiving%20Framework%20for%20Weakly-Supervised%0A%20%20Temporal%20Forgery%20Localization%0AAuthor%3A%20Wenbo%20Xu%20and%20Junyan%20Wu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%20and%20Qian%20Wang%0AAbstract%3A%20%20%20Current%20researches%20on%20Deepfake%20forensics%20often%20treat%20detection%20as%20a%0Aclassification%20task%20or%20temporal%20forgery%20localization%20problem%2C%20which%20are%20usually%0Arestrictive%2C%20time-consuming%2C%20and%20challenging%20to%20scale%20for%20large%20datasets.%20To%0Aresolve%20these%20issues%2C%20we%20present%20a%20multimodal%20deviation%20perceiving%20framework%0Afor%20weakly-supervised%20temporal%20forgery%20localization%20%28MDP%29%2C%20which%20aims%20to%0Aidentify%20temporal%20partial%20forged%20segments%20using%20only%20video-level%20annotations.%0AThe%20MDP%20proposes%20a%20novel%20multimodal%20interaction%20mechanism%20%28MI%29%20and%20an%0Aextensible%20deviation%20perceiving%20loss%20to%20perceive%20multimodal%20deviation%2C%20which%0Aachieves%20the%20refined%20start%20and%20end%20timestamps%20localization%20of%20forged%20segments.%0ASpecifically%2C%20MI%20introduces%20a%20temporal%20property%20preserving%20cross-modal%0Aattention%20to%20measure%20the%20relevance%20between%20the%20visual%20and%20audio%20modalities%20in%0Athe%20probabilistic%20embedding%20space.%20It%20could%20identify%20the%20inter-modality%0Adeviation%20and%20construct%20comprehensive%20video%20features%20for%20temporal%20forgery%0Alocalization.%20To%20explore%20further%20temporal%20deviation%20for%20weakly-supervised%0Alearning%2C%20an%20extensible%20deviation%20perceiving%20loss%20has%20been%20proposed%2C%20aiming%20at%0Aenlarging%20the%20deviation%20of%20adjacent%20segments%20of%20the%20forged%20samples%20and%20reducing%0Athat%20of%20genuine%20samples.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20framework%20and%20achieve%20comparable%20results%20to%20fully-supervised%0Aapproaches%20in%20several%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16596v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multimodal%2520Deviation%2520Perceiving%2520Framework%2520for%2520Weakly-Supervised%250A%2520%2520Temporal%2520Forgery%2520Localization%26entry.906535625%3DWenbo%2520Xu%2520and%2520Junyan%2520Wu%2520and%2520Wei%2520Lu%2520and%2520Xiangyang%2520Luo%2520and%2520Qian%2520Wang%26entry.1292438233%3D%2520%2520Current%2520researches%2520on%2520Deepfake%2520forensics%2520often%2520treat%2520detection%2520as%2520a%250Aclassification%2520task%2520or%2520temporal%2520forgery%2520localization%2520problem%252C%2520which%2520are%2520usually%250Arestrictive%252C%2520time-consuming%252C%2520and%2520challenging%2520to%2520scale%2520for%2520large%2520datasets.%2520To%250Aresolve%2520these%2520issues%252C%2520we%2520present%2520a%2520multimodal%2520deviation%2520perceiving%2520framework%250Afor%2520weakly-supervised%2520temporal%2520forgery%2520localization%2520%2528MDP%2529%252C%2520which%2520aims%2520to%250Aidentify%2520temporal%2520partial%2520forged%2520segments%2520using%2520only%2520video-level%2520annotations.%250AThe%2520MDP%2520proposes%2520a%2520novel%2520multimodal%2520interaction%2520mechanism%2520%2528MI%2529%2520and%2520an%250Aextensible%2520deviation%2520perceiving%2520loss%2520to%2520perceive%2520multimodal%2520deviation%252C%2520which%250Aachieves%2520the%2520refined%2520start%2520and%2520end%2520timestamps%2520localization%2520of%2520forged%2520segments.%250ASpecifically%252C%2520MI%2520introduces%2520a%2520temporal%2520property%2520preserving%2520cross-modal%250Aattention%2520to%2520measure%2520the%2520relevance%2520between%2520the%2520visual%2520and%2520audio%2520modalities%2520in%250Athe%2520probabilistic%2520embedding%2520space.%2520It%2520could%2520identify%2520the%2520inter-modality%250Adeviation%2520and%2520construct%2520comprehensive%2520video%2520features%2520for%2520temporal%2520forgery%250Alocalization.%2520To%2520explore%2520further%2520temporal%2520deviation%2520for%2520weakly-supervised%250Alearning%252C%2520an%2520extensible%2520deviation%2520perceiving%2520loss%2520has%2520been%2520proposed%252C%2520aiming%2520at%250Aenlarging%2520the%2520deviation%2520of%2520adjacent%2520segments%2520of%2520the%2520forged%2520samples%2520and%2520reducing%250Athat%2520of%2520genuine%2520samples.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%250Athe%2520proposed%2520framework%2520and%2520achieve%2520comparable%2520results%2520to%2520fully-supervised%250Aapproaches%2520in%2520several%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16596v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multimodal%20Deviation%20Perceiving%20Framework%20for%20Weakly-Supervised%0A%20%20Temporal%20Forgery%20Localization&entry.906535625=Wenbo%20Xu%20and%20Junyan%20Wu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%20and%20Qian%20Wang&entry.1292438233=%20%20Current%20researches%20on%20Deepfake%20forensics%20often%20treat%20detection%20as%20a%0Aclassification%20task%20or%20temporal%20forgery%20localization%20problem%2C%20which%20are%20usually%0Arestrictive%2C%20time-consuming%2C%20and%20challenging%20to%20scale%20for%20large%20datasets.%20To%0Aresolve%20these%20issues%2C%20we%20present%20a%20multimodal%20deviation%20perceiving%20framework%0Afor%20weakly-supervised%20temporal%20forgery%20localization%20%28MDP%29%2C%20which%20aims%20to%0Aidentify%20temporal%20partial%20forged%20segments%20using%20only%20video-level%20annotations.%0AThe%20MDP%20proposes%20a%20novel%20multimodal%20interaction%20mechanism%20%28MI%29%20and%20an%0Aextensible%20deviation%20perceiving%20loss%20to%20perceive%20multimodal%20deviation%2C%20which%0Aachieves%20the%20refined%20start%20and%20end%20timestamps%20localization%20of%20forged%20segments.%0ASpecifically%2C%20MI%20introduces%20a%20temporal%20property%20preserving%20cross-modal%0Aattention%20to%20measure%20the%20relevance%20between%20the%20visual%20and%20audio%20modalities%20in%0Athe%20probabilistic%20embedding%20space.%20It%20could%20identify%20the%20inter-modality%0Adeviation%20and%20construct%20comprehensive%20video%20features%20for%20temporal%20forgery%0Alocalization.%20To%20explore%20further%20temporal%20deviation%20for%20weakly-supervised%0Alearning%2C%20an%20extensible%20deviation%20perceiving%20loss%20has%20been%20proposed%2C%20aiming%20at%0Aenlarging%20the%20deviation%20of%20adjacent%20segments%20of%20the%20forged%20samples%20and%20reducing%0Athat%20of%20genuine%20samples.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%0Athe%20proposed%20framework%20and%20achieve%20comparable%20results%20to%20fully-supervised%0Aapproaches%20in%20several%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16596v1&entry.124074799=Read"},
{"title": "Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and\n  Diverse Emotion Control", "author": "Zongzheng Zhang and Jiawen Yang and Ziqiao Peng and Meng Yang and Jianzhu Ma and Lin Cheng and Huazhe Xu and Hang Zhao and Hao Zhao", "abstract": "  Previous animatronic faces struggle to express emotions effectively due to\nhardware and software limitations. On the hardware side, earlier approaches\neither use rigid-driven mechanisms, which provide precise control but are\ndifficult to design within constrained spaces, or tendon-driven mechanisms,\nwhich are more space-efficient but challenging to control. In contrast, we\npropose a hybrid actuation approach that combines the best of both worlds. The\neyes and mouth-key areas for emotional expression-are controlled using rigid\nmechanisms for precise movement, while the nose and cheek, which convey subtle\nfacial microexpressions, are driven by strings. This design allows us to build\na compact yet versatile hardware platform capable of expressing a wide range of\nemotions. On the algorithmic side, our method introduces a self-modeling\nnetwork that maps motor actions to facial landmarks, allowing us to\nautomatically establish the relationship between blendshape coefficients for\ndifferent facial expressions and the corresponding motor control signals\nthrough gradient backpropagation. We then train a neural network to map speech\ninput to corresponding blendshape controls. With our method, we can generate\ndistinct emotional expressions such as happiness, fear, disgust, and anger,\nfrom any given sentence, each with nuanced, emotion-specific control signals-a\nfeature that has not been demonstrated in earlier systems. We release the\nhardware design and code at https://github.com/ZZongzheng0918/Morpheus-Hardware\nand https://github.com/ZZongzheng0918/Morpheus-Software.\n", "link": "http://arxiv.org/abs/2507.16645v1", "date": "2025-07-22", "relevancy": 2.8487, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5862}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5615}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morpheus%3A%20A%20Neural-driven%20Animatronic%20Face%20with%20Hybrid%20Actuation%20and%0A%20%20Diverse%20Emotion%20Control&body=Title%3A%20Morpheus%3A%20A%20Neural-driven%20Animatronic%20Face%20with%20Hybrid%20Actuation%20and%0A%20%20Diverse%20Emotion%20Control%0AAuthor%3A%20Zongzheng%20Zhang%20and%20Jiawen%20Yang%20and%20Ziqiao%20Peng%20and%20Meng%20Yang%20and%20Jianzhu%20Ma%20and%20Lin%20Cheng%20and%20Huazhe%20Xu%20and%20Hang%20Zhao%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Previous%20animatronic%20faces%20struggle%20to%20express%20emotions%20effectively%20due%20to%0Ahardware%20and%20software%20limitations.%20On%20the%20hardware%20side%2C%20earlier%20approaches%0Aeither%20use%20rigid-driven%20mechanisms%2C%20which%20provide%20precise%20control%20but%20are%0Adifficult%20to%20design%20within%20constrained%20spaces%2C%20or%20tendon-driven%20mechanisms%2C%0Awhich%20are%20more%20space-efficient%20but%20challenging%20to%20control.%20In%20contrast%2C%20we%0Apropose%20a%20hybrid%20actuation%20approach%20that%20combines%20the%20best%20of%20both%20worlds.%20The%0Aeyes%20and%20mouth-key%20areas%20for%20emotional%20expression-are%20controlled%20using%20rigid%0Amechanisms%20for%20precise%20movement%2C%20while%20the%20nose%20and%20cheek%2C%20which%20convey%20subtle%0Afacial%20microexpressions%2C%20are%20driven%20by%20strings.%20This%20design%20allows%20us%20to%20build%0Aa%20compact%20yet%20versatile%20hardware%20platform%20capable%20of%20expressing%20a%20wide%20range%20of%0Aemotions.%20On%20the%20algorithmic%20side%2C%20our%20method%20introduces%20a%20self-modeling%0Anetwork%20that%20maps%20motor%20actions%20to%20facial%20landmarks%2C%20allowing%20us%20to%0Aautomatically%20establish%20the%20relationship%20between%20blendshape%20coefficients%20for%0Adifferent%20facial%20expressions%20and%20the%20corresponding%20motor%20control%20signals%0Athrough%20gradient%20backpropagation.%20We%20then%20train%20a%20neural%20network%20to%20map%20speech%0Ainput%20to%20corresponding%20blendshape%20controls.%20With%20our%20method%2C%20we%20can%20generate%0Adistinct%20emotional%20expressions%20such%20as%20happiness%2C%20fear%2C%20disgust%2C%20and%20anger%2C%0Afrom%20any%20given%20sentence%2C%20each%20with%20nuanced%2C%20emotion-specific%20control%20signals-a%0Afeature%20that%20has%20not%20been%20demonstrated%20in%20earlier%20systems.%20We%20release%20the%0Ahardware%20design%20and%20code%20at%20https%3A//github.com/ZZongzheng0918/Morpheus-Hardware%0Aand%20https%3A//github.com/ZZongzheng0918/Morpheus-Software.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorpheus%253A%2520A%2520Neural-driven%2520Animatronic%2520Face%2520with%2520Hybrid%2520Actuation%2520and%250A%2520%2520Diverse%2520Emotion%2520Control%26entry.906535625%3DZongzheng%2520Zhang%2520and%2520Jiawen%2520Yang%2520and%2520Ziqiao%2520Peng%2520and%2520Meng%2520Yang%2520and%2520Jianzhu%2520Ma%2520and%2520Lin%2520Cheng%2520and%2520Huazhe%2520Xu%2520and%2520Hang%2520Zhao%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Previous%2520animatronic%2520faces%2520struggle%2520to%2520express%2520emotions%2520effectively%2520due%2520to%250Ahardware%2520and%2520software%2520limitations.%2520On%2520the%2520hardware%2520side%252C%2520earlier%2520approaches%250Aeither%2520use%2520rigid-driven%2520mechanisms%252C%2520which%2520provide%2520precise%2520control%2520but%2520are%250Adifficult%2520to%2520design%2520within%2520constrained%2520spaces%252C%2520or%2520tendon-driven%2520mechanisms%252C%250Awhich%2520are%2520more%2520space-efficient%2520but%2520challenging%2520to%2520control.%2520In%2520contrast%252C%2520we%250Apropose%2520a%2520hybrid%2520actuation%2520approach%2520that%2520combines%2520the%2520best%2520of%2520both%2520worlds.%2520The%250Aeyes%2520and%2520mouth-key%2520areas%2520for%2520emotional%2520expression-are%2520controlled%2520using%2520rigid%250Amechanisms%2520for%2520precise%2520movement%252C%2520while%2520the%2520nose%2520and%2520cheek%252C%2520which%2520convey%2520subtle%250Afacial%2520microexpressions%252C%2520are%2520driven%2520by%2520strings.%2520This%2520design%2520allows%2520us%2520to%2520build%250Aa%2520compact%2520yet%2520versatile%2520hardware%2520platform%2520capable%2520of%2520expressing%2520a%2520wide%2520range%2520of%250Aemotions.%2520On%2520the%2520algorithmic%2520side%252C%2520our%2520method%2520introduces%2520a%2520self-modeling%250Anetwork%2520that%2520maps%2520motor%2520actions%2520to%2520facial%2520landmarks%252C%2520allowing%2520us%2520to%250Aautomatically%2520establish%2520the%2520relationship%2520between%2520blendshape%2520coefficients%2520for%250Adifferent%2520facial%2520expressions%2520and%2520the%2520corresponding%2520motor%2520control%2520signals%250Athrough%2520gradient%2520backpropagation.%2520We%2520then%2520train%2520a%2520neural%2520network%2520to%2520map%2520speech%250Ainput%2520to%2520corresponding%2520blendshape%2520controls.%2520With%2520our%2520method%252C%2520we%2520can%2520generate%250Adistinct%2520emotional%2520expressions%2520such%2520as%2520happiness%252C%2520fear%252C%2520disgust%252C%2520and%2520anger%252C%250Afrom%2520any%2520given%2520sentence%252C%2520each%2520with%2520nuanced%252C%2520emotion-specific%2520control%2520signals-a%250Afeature%2520that%2520has%2520not%2520been%2520demonstrated%2520in%2520earlier%2520systems.%2520We%2520release%2520the%250Ahardware%2520design%2520and%2520code%2520at%2520https%253A//github.com/ZZongzheng0918/Morpheus-Hardware%250Aand%2520https%253A//github.com/ZZongzheng0918/Morpheus-Software.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morpheus%3A%20A%20Neural-driven%20Animatronic%20Face%20with%20Hybrid%20Actuation%20and%0A%20%20Diverse%20Emotion%20Control&entry.906535625=Zongzheng%20Zhang%20and%20Jiawen%20Yang%20and%20Ziqiao%20Peng%20and%20Meng%20Yang%20and%20Jianzhu%20Ma%20and%20Lin%20Cheng%20and%20Huazhe%20Xu%20and%20Hang%20Zhao%20and%20Hao%20Zhao&entry.1292438233=%20%20Previous%20animatronic%20faces%20struggle%20to%20express%20emotions%20effectively%20due%20to%0Ahardware%20and%20software%20limitations.%20On%20the%20hardware%20side%2C%20earlier%20approaches%0Aeither%20use%20rigid-driven%20mechanisms%2C%20which%20provide%20precise%20control%20but%20are%0Adifficult%20to%20design%20within%20constrained%20spaces%2C%20or%20tendon-driven%20mechanisms%2C%0Awhich%20are%20more%20space-efficient%20but%20challenging%20to%20control.%20In%20contrast%2C%20we%0Apropose%20a%20hybrid%20actuation%20approach%20that%20combines%20the%20best%20of%20both%20worlds.%20The%0Aeyes%20and%20mouth-key%20areas%20for%20emotional%20expression-are%20controlled%20using%20rigid%0Amechanisms%20for%20precise%20movement%2C%20while%20the%20nose%20and%20cheek%2C%20which%20convey%20subtle%0Afacial%20microexpressions%2C%20are%20driven%20by%20strings.%20This%20design%20allows%20us%20to%20build%0Aa%20compact%20yet%20versatile%20hardware%20platform%20capable%20of%20expressing%20a%20wide%20range%20of%0Aemotions.%20On%20the%20algorithmic%20side%2C%20our%20method%20introduces%20a%20self-modeling%0Anetwork%20that%20maps%20motor%20actions%20to%20facial%20landmarks%2C%20allowing%20us%20to%0Aautomatically%20establish%20the%20relationship%20between%20blendshape%20coefficients%20for%0Adifferent%20facial%20expressions%20and%20the%20corresponding%20motor%20control%20signals%0Athrough%20gradient%20backpropagation.%20We%20then%20train%20a%20neural%20network%20to%20map%20speech%0Ainput%20to%20corresponding%20blendshape%20controls.%20With%20our%20method%2C%20we%20can%20generate%0Adistinct%20emotional%20expressions%20such%20as%20happiness%2C%20fear%2C%20disgust%2C%20and%20anger%2C%0Afrom%20any%20given%20sentence%2C%20each%20with%20nuanced%2C%20emotion-specific%20control%20signals-a%0Afeature%20that%20has%20not%20been%20demonstrated%20in%20earlier%20systems.%20We%20release%20the%0Ahardware%20design%20and%20code%20at%20https%3A//github.com/ZZongzheng0918/Morpheus-Hardware%0Aand%20https%3A//github.com/ZZongzheng0918/Morpheus-Software.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16645v1&entry.124074799=Read"},
{"title": "Towards a Universal 3D Medical Multi-modality Generalization via\n  Learning Personalized Invariant Representation", "author": "Zhaorui Tan and Xi Yang and Tan Pan and Tianyi Liu and Chen Jiang and Xin Guo and Qiufeng Wang and Anh Nguyen and Yuan Qi and Kaizhu Huang and Yuan Cheng", "abstract": "  The differences among medical imaging modalities, driven by distinct\nunderlying principles, pose significant challenges for generalization in\nmulti-modal medical tasks. Beyond modality gaps, individual variations, such as\ndifferences in organ size and metabolic rate, further impede a model's ability\nto generalize effectively across both modalities and diverse populations.\nDespite the importance of personalization, existing approaches to multi-modal\ngeneralization often neglect individual differences, focusing solely on common\nanatomical features. This limitation may result in weakened generalization in\nvarious medical tasks. In this paper, we unveil that personalization is\ncritical for multi-modal generalization. Specifically, we propose an approach\nto achieve personalized generalization through approximating the underlying\npersonalized invariant representation ${X}_h$ across various modalities by\nleveraging individual-level constraints and a learnable biological prior. We\nvalidate the feasibility and benefits of learning a personalized ${X}_h$,\nshowing that this representation is highly generalizable and transferable\nacross various multi-modal medical tasks. Extensive experimental results\nconsistently show that the additionally incorporated personalization\nsignificantly improves performance and generalization across diverse scenarios,\nconfirming its effectiveness.\n", "link": "http://arxiv.org/abs/2411.06106v3", "date": "2025-07-22", "relevancy": 2.8361, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5545}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Universal%203D%20Medical%20Multi-modality%20Generalization%20via%0A%20%20Learning%20Personalized%20Invariant%20Representation&body=Title%3A%20Towards%20a%20Universal%203D%20Medical%20Multi-modality%20Generalization%20via%0A%20%20Learning%20Personalized%20Invariant%20Representation%0AAuthor%3A%20Zhaorui%20Tan%20and%20Xi%20Yang%20and%20Tan%20Pan%20and%20Tianyi%20Liu%20and%20Chen%20Jiang%20and%20Xin%20Guo%20and%20Qiufeng%20Wang%20and%20Anh%20Nguyen%20and%20Yuan%20Qi%20and%20Kaizhu%20Huang%20and%20Yuan%20Cheng%0AAbstract%3A%20%20%20The%20differences%20among%20medical%20imaging%20modalities%2C%20driven%20by%20distinct%0Aunderlying%20principles%2C%20pose%20significant%20challenges%20for%20generalization%20in%0Amulti-modal%20medical%20tasks.%20Beyond%20modality%20gaps%2C%20individual%20variations%2C%20such%20as%0Adifferences%20in%20organ%20size%20and%20metabolic%20rate%2C%20further%20impede%20a%20model%27s%20ability%0Ato%20generalize%20effectively%20across%20both%20modalities%20and%20diverse%20populations.%0ADespite%20the%20importance%20of%20personalization%2C%20existing%20approaches%20to%20multi-modal%0Ageneralization%20often%20neglect%20individual%20differences%2C%20focusing%20solely%20on%20common%0Aanatomical%20features.%20This%20limitation%20may%20result%20in%20weakened%20generalization%20in%0Avarious%20medical%20tasks.%20In%20this%20paper%2C%20we%20unveil%20that%20personalization%20is%0Acritical%20for%20multi-modal%20generalization.%20Specifically%2C%20we%20propose%20an%20approach%0Ato%20achieve%20personalized%20generalization%20through%20approximating%20the%20underlying%0Apersonalized%20invariant%20representation%20%24%7BX%7D_h%24%20across%20various%20modalities%20by%0Aleveraging%20individual-level%20constraints%20and%20a%20learnable%20biological%20prior.%20We%0Avalidate%20the%20feasibility%20and%20benefits%20of%20learning%20a%20personalized%20%24%7BX%7D_h%24%2C%0Ashowing%20that%20this%20representation%20is%20highly%20generalizable%20and%20transferable%0Aacross%20various%20multi-modal%20medical%20tasks.%20Extensive%20experimental%20results%0Aconsistently%20show%20that%20the%20additionally%20incorporated%20personalization%0Asignificantly%20improves%20performance%20and%20generalization%20across%20diverse%20scenarios%2C%0Aconfirming%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06106v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Universal%25203D%2520Medical%2520Multi-modality%2520Generalization%2520via%250A%2520%2520Learning%2520Personalized%2520Invariant%2520Representation%26entry.906535625%3DZhaorui%2520Tan%2520and%2520Xi%2520Yang%2520and%2520Tan%2520Pan%2520and%2520Tianyi%2520Liu%2520and%2520Chen%2520Jiang%2520and%2520Xin%2520Guo%2520and%2520Qiufeng%2520Wang%2520and%2520Anh%2520Nguyen%2520and%2520Yuan%2520Qi%2520and%2520Kaizhu%2520Huang%2520and%2520Yuan%2520Cheng%26entry.1292438233%3D%2520%2520The%2520differences%2520among%2520medical%2520imaging%2520modalities%252C%2520driven%2520by%2520distinct%250Aunderlying%2520principles%252C%2520pose%2520significant%2520challenges%2520for%2520generalization%2520in%250Amulti-modal%2520medical%2520tasks.%2520Beyond%2520modality%2520gaps%252C%2520individual%2520variations%252C%2520such%2520as%250Adifferences%2520in%2520organ%2520size%2520and%2520metabolic%2520rate%252C%2520further%2520impede%2520a%2520model%2527s%2520ability%250Ato%2520generalize%2520effectively%2520across%2520both%2520modalities%2520and%2520diverse%2520populations.%250ADespite%2520the%2520importance%2520of%2520personalization%252C%2520existing%2520approaches%2520to%2520multi-modal%250Ageneralization%2520often%2520neglect%2520individual%2520differences%252C%2520focusing%2520solely%2520on%2520common%250Aanatomical%2520features.%2520This%2520limitation%2520may%2520result%2520in%2520weakened%2520generalization%2520in%250Avarious%2520medical%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520unveil%2520that%2520personalization%2520is%250Acritical%2520for%2520multi-modal%2520generalization.%2520Specifically%252C%2520we%2520propose%2520an%2520approach%250Ato%2520achieve%2520personalized%2520generalization%2520through%2520approximating%2520the%2520underlying%250Apersonalized%2520invariant%2520representation%2520%2524%257BX%257D_h%2524%2520across%2520various%2520modalities%2520by%250Aleveraging%2520individual-level%2520constraints%2520and%2520a%2520learnable%2520biological%2520prior.%2520We%250Avalidate%2520the%2520feasibility%2520and%2520benefits%2520of%2520learning%2520a%2520personalized%2520%2524%257BX%257D_h%2524%252C%250Ashowing%2520that%2520this%2520representation%2520is%2520highly%2520generalizable%2520and%2520transferable%250Aacross%2520various%2520multi-modal%2520medical%2520tasks.%2520Extensive%2520experimental%2520results%250Aconsistently%2520show%2520that%2520the%2520additionally%2520incorporated%2520personalization%250Asignificantly%2520improves%2520performance%2520and%2520generalization%2520across%2520diverse%2520scenarios%252C%250Aconfirming%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06106v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Universal%203D%20Medical%20Multi-modality%20Generalization%20via%0A%20%20Learning%20Personalized%20Invariant%20Representation&entry.906535625=Zhaorui%20Tan%20and%20Xi%20Yang%20and%20Tan%20Pan%20and%20Tianyi%20Liu%20and%20Chen%20Jiang%20and%20Xin%20Guo%20and%20Qiufeng%20Wang%20and%20Anh%20Nguyen%20and%20Yuan%20Qi%20and%20Kaizhu%20Huang%20and%20Yuan%20Cheng&entry.1292438233=%20%20The%20differences%20among%20medical%20imaging%20modalities%2C%20driven%20by%20distinct%0Aunderlying%20principles%2C%20pose%20significant%20challenges%20for%20generalization%20in%0Amulti-modal%20medical%20tasks.%20Beyond%20modality%20gaps%2C%20individual%20variations%2C%20such%20as%0Adifferences%20in%20organ%20size%20and%20metabolic%20rate%2C%20further%20impede%20a%20model%27s%20ability%0Ato%20generalize%20effectively%20across%20both%20modalities%20and%20diverse%20populations.%0ADespite%20the%20importance%20of%20personalization%2C%20existing%20approaches%20to%20multi-modal%0Ageneralization%20often%20neglect%20individual%20differences%2C%20focusing%20solely%20on%20common%0Aanatomical%20features.%20This%20limitation%20may%20result%20in%20weakened%20generalization%20in%0Avarious%20medical%20tasks.%20In%20this%20paper%2C%20we%20unveil%20that%20personalization%20is%0Acritical%20for%20multi-modal%20generalization.%20Specifically%2C%20we%20propose%20an%20approach%0Ato%20achieve%20personalized%20generalization%20through%20approximating%20the%20underlying%0Apersonalized%20invariant%20representation%20%24%7BX%7D_h%24%20across%20various%20modalities%20by%0Aleveraging%20individual-level%20constraints%20and%20a%20learnable%20biological%20prior.%20We%0Avalidate%20the%20feasibility%20and%20benefits%20of%20learning%20a%20personalized%20%24%7BX%7D_h%24%2C%0Ashowing%20that%20this%20representation%20is%20highly%20generalizable%20and%20transferable%0Aacross%20various%20multi-modal%20medical%20tasks.%20Extensive%20experimental%20results%0Aconsistently%20show%20that%20the%20additionally%20incorporated%20personalization%0Asignificantly%20improves%20performance%20and%20generalization%20across%20diverse%20scenarios%2C%0Aconfirming%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06106v3&entry.124074799=Read"},
{"title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and\n  LLM-Based High-Quality Image-Text Dataset Generation", "author": "Yiguo He and Junjie Zhu and Yiying Li and Xiaoyu Zhang and Chunping Qiu and Jun Wang and Qiangjuan Huang and Ke Yang", "abstract": "  The application of Vision-language foundation models (VLFMs) to remote\nsensing (RS) imagery has garnered significant attention due to their superior\ncapability in various downstream tasks. A key challenge lies in the scarcity of\nhigh-quality, large-scale, image-text paired training data. Recently, several\nworks introduced extensive image-text datasets for RS and trained their VLFMs.\nHowever, due to the rudimentary methods used for generating captions, the\nquality of datasets is suboptimal, requiring larger volumes of training data,\nwhile only yielding modest performance improvements. In this paper, we propose\na two-stage method named MpGI(Multi-Perspective Generation and Integration) for\ngenerating high-quality text captions for RS images. Firstly, we generate\ndistinct and detailed descriptions from different perspectives using\nRule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs\ngeneration methods. Next, we utilize Large Language Models (LLMs) to integrate\nthese diverse descriptions into comprehensive captions, capturing details from\nmultiple perspectives. Finally, we have created the HQRS-IT-210K dataset,\nincluding about 210,000 RS images and 1.3 million captions. We fine-tuned two\nVLFMs using our dataset: CLIP, a discriminative model, and CoCa, an\nimage-to-text generative model. This process resulted in our proposed HQRS-CLIP\nand RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed\nthe previous SOTA RS CLIP model in various downstream tasks while using only\n4.2\\% of the training data. RS-CoCa outperforms other advanced approaches\nacross benchmark datasets and can generate captions for RS images that rival or\neven exceed manual annotations. Dataset, pre-trained models, and codes will be\nreleased at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.\n", "link": "http://arxiv.org/abs/2507.16716v1", "date": "2025-07-22", "relevancy": 2.7846, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Remote%20Sensing%20Vision-Language%20Models%20Through%20MLLM%20and%0A%20%20LLM-Based%20High-Quality%20Image-Text%20Dataset%20Generation&body=Title%3A%20Enhancing%20Remote%20Sensing%20Vision-Language%20Models%20Through%20MLLM%20and%0A%20%20LLM-Based%20High-Quality%20Image-Text%20Dataset%20Generation%0AAuthor%3A%20Yiguo%20He%20and%20Junjie%20Zhu%20and%20Yiying%20Li%20and%20Xiaoyu%20Zhang%20and%20Chunping%20Qiu%20and%20Jun%20Wang%20and%20Qiangjuan%20Huang%20and%20Ke%20Yang%0AAbstract%3A%20%20%20The%20application%20of%20Vision-language%20foundation%20models%20%28VLFMs%29%20to%20remote%0Asensing%20%28RS%29%20imagery%20has%20garnered%20significant%20attention%20due%20to%20their%20superior%0Acapability%20in%20various%20downstream%20tasks.%20A%20key%20challenge%20lies%20in%20the%20scarcity%20of%0Ahigh-quality%2C%20large-scale%2C%20image-text%20paired%20training%20data.%20Recently%2C%20several%0Aworks%20introduced%20extensive%20image-text%20datasets%20for%20RS%20and%20trained%20their%20VLFMs.%0AHowever%2C%20due%20to%20the%20rudimentary%20methods%20used%20for%20generating%20captions%2C%20the%0Aquality%20of%20datasets%20is%20suboptimal%2C%20requiring%20larger%20volumes%20of%20training%20data%2C%0Awhile%20only%20yielding%20modest%20performance%20improvements.%20In%20this%20paper%2C%20we%20propose%0Aa%20two-stage%20method%20named%20MpGI%28Multi-Perspective%20Generation%20and%20Integration%29%20for%0Agenerating%20high-quality%20text%20captions%20for%20RS%20images.%20Firstly%2C%20we%20generate%0Adistinct%20and%20detailed%20descriptions%20from%20different%20perspectives%20using%0ARule-MLLM%28Multimodal%20Large%20Language%20Model%29%20Relay%20Generation%20and%20MLLMs%0Ageneration%20methods.%20Next%2C%20we%20utilize%20Large%20Language%20Models%20%28LLMs%29%20to%20integrate%0Athese%20diverse%20descriptions%20into%20comprehensive%20captions%2C%20capturing%20details%20from%0Amultiple%20perspectives.%20Finally%2C%20we%20have%20created%20the%20HQRS-IT-210K%20dataset%2C%0Aincluding%20about%20210%2C000%20RS%20images%20and%201.3%20million%20captions.%20We%20fine-tuned%20two%0AVLFMs%20using%20our%20dataset%3A%20CLIP%2C%20a%20discriminative%20model%2C%20and%20CoCa%2C%20an%0Aimage-to-text%20generative%20model.%20This%20process%20resulted%20in%20our%20proposed%20HQRS-CLIP%0Aand%20RS-CoCa%20models.%20Experimental%20results%20demonstrate%20that%20HQRS-CLIP%20surpassed%0Athe%20previous%20SOTA%20RS%20CLIP%20model%20in%20various%20downstream%20tasks%20while%20using%20only%0A4.2%5C%25%20of%20the%20training%20data.%20RS-CoCa%20outperforms%20other%20advanced%20approaches%0Aacross%20benchmark%20datasets%20and%20can%20generate%20captions%20for%20RS%20images%20that%20rival%20or%0Aeven%20exceed%20manual%20annotations.%20Dataset%2C%20pre-trained%20models%2C%20and%20codes%20will%20be%0Areleased%20at%20https%3A//github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16716v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Remote%2520Sensing%2520Vision-Language%2520Models%2520Through%2520MLLM%2520and%250A%2520%2520LLM-Based%2520High-Quality%2520Image-Text%2520Dataset%2520Generation%26entry.906535625%3DYiguo%2520He%2520and%2520Junjie%2520Zhu%2520and%2520Yiying%2520Li%2520and%2520Xiaoyu%2520Zhang%2520and%2520Chunping%2520Qiu%2520and%2520Jun%2520Wang%2520and%2520Qiangjuan%2520Huang%2520and%2520Ke%2520Yang%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520Vision-language%2520foundation%2520models%2520%2528VLFMs%2529%2520to%2520remote%250Asensing%2520%2528RS%2529%2520imagery%2520has%2520garnered%2520significant%2520attention%2520due%2520to%2520their%2520superior%250Acapability%2520in%2520various%2520downstream%2520tasks.%2520A%2520key%2520challenge%2520lies%2520in%2520the%2520scarcity%2520of%250Ahigh-quality%252C%2520large-scale%252C%2520image-text%2520paired%2520training%2520data.%2520Recently%252C%2520several%250Aworks%2520introduced%2520extensive%2520image-text%2520datasets%2520for%2520RS%2520and%2520trained%2520their%2520VLFMs.%250AHowever%252C%2520due%2520to%2520the%2520rudimentary%2520methods%2520used%2520for%2520generating%2520captions%252C%2520the%250Aquality%2520of%2520datasets%2520is%2520suboptimal%252C%2520requiring%2520larger%2520volumes%2520of%2520training%2520data%252C%250Awhile%2520only%2520yielding%2520modest%2520performance%2520improvements.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520two-stage%2520method%2520named%2520MpGI%2528Multi-Perspective%2520Generation%2520and%2520Integration%2529%2520for%250Agenerating%2520high-quality%2520text%2520captions%2520for%2520RS%2520images.%2520Firstly%252C%2520we%2520generate%250Adistinct%2520and%2520detailed%2520descriptions%2520from%2520different%2520perspectives%2520using%250ARule-MLLM%2528Multimodal%2520Large%2520Language%2520Model%2529%2520Relay%2520Generation%2520and%2520MLLMs%250Ageneration%2520methods.%2520Next%252C%2520we%2520utilize%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520integrate%250Athese%2520diverse%2520descriptions%2520into%2520comprehensive%2520captions%252C%2520capturing%2520details%2520from%250Amultiple%2520perspectives.%2520Finally%252C%2520we%2520have%2520created%2520the%2520HQRS-IT-210K%2520dataset%252C%250Aincluding%2520about%2520210%252C000%2520RS%2520images%2520and%25201.3%2520million%2520captions.%2520We%2520fine-tuned%2520two%250AVLFMs%2520using%2520our%2520dataset%253A%2520CLIP%252C%2520a%2520discriminative%2520model%252C%2520and%2520CoCa%252C%2520an%250Aimage-to-text%2520generative%2520model.%2520This%2520process%2520resulted%2520in%2520our%2520proposed%2520HQRS-CLIP%250Aand%2520RS-CoCa%2520models.%2520Experimental%2520results%2520demonstrate%2520that%2520HQRS-CLIP%2520surpassed%250Athe%2520previous%2520SOTA%2520RS%2520CLIP%2520model%2520in%2520various%2520downstream%2520tasks%2520while%2520using%2520only%250A4.2%255C%2525%2520of%2520the%2520training%2520data.%2520RS-CoCa%2520outperforms%2520other%2520advanced%2520approaches%250Aacross%2520benchmark%2520datasets%2520and%2520can%2520generate%2520captions%2520for%2520RS%2520images%2520that%2520rival%2520or%250Aeven%2520exceed%2520manual%2520annotations.%2520Dataset%252C%2520pre-trained%2520models%252C%2520and%2520codes%2520will%2520be%250Areleased%2520at%2520https%253A//github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16716v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Remote%20Sensing%20Vision-Language%20Models%20Through%20MLLM%20and%0A%20%20LLM-Based%20High-Quality%20Image-Text%20Dataset%20Generation&entry.906535625=Yiguo%20He%20and%20Junjie%20Zhu%20and%20Yiying%20Li%20and%20Xiaoyu%20Zhang%20and%20Chunping%20Qiu%20and%20Jun%20Wang%20and%20Qiangjuan%20Huang%20and%20Ke%20Yang&entry.1292438233=%20%20The%20application%20of%20Vision-language%20foundation%20models%20%28VLFMs%29%20to%20remote%0Asensing%20%28RS%29%20imagery%20has%20garnered%20significant%20attention%20due%20to%20their%20superior%0Acapability%20in%20various%20downstream%20tasks.%20A%20key%20challenge%20lies%20in%20the%20scarcity%20of%0Ahigh-quality%2C%20large-scale%2C%20image-text%20paired%20training%20data.%20Recently%2C%20several%0Aworks%20introduced%20extensive%20image-text%20datasets%20for%20RS%20and%20trained%20their%20VLFMs.%0AHowever%2C%20due%20to%20the%20rudimentary%20methods%20used%20for%20generating%20captions%2C%20the%0Aquality%20of%20datasets%20is%20suboptimal%2C%20requiring%20larger%20volumes%20of%20training%20data%2C%0Awhile%20only%20yielding%20modest%20performance%20improvements.%20In%20this%20paper%2C%20we%20propose%0Aa%20two-stage%20method%20named%20MpGI%28Multi-Perspective%20Generation%20and%20Integration%29%20for%0Agenerating%20high-quality%20text%20captions%20for%20RS%20images.%20Firstly%2C%20we%20generate%0Adistinct%20and%20detailed%20descriptions%20from%20different%20perspectives%20using%0ARule-MLLM%28Multimodal%20Large%20Language%20Model%29%20Relay%20Generation%20and%20MLLMs%0Ageneration%20methods.%20Next%2C%20we%20utilize%20Large%20Language%20Models%20%28LLMs%29%20to%20integrate%0Athese%20diverse%20descriptions%20into%20comprehensive%20captions%2C%20capturing%20details%20from%0Amultiple%20perspectives.%20Finally%2C%20we%20have%20created%20the%20HQRS-IT-210K%20dataset%2C%0Aincluding%20about%20210%2C000%20RS%20images%20and%201.3%20million%20captions.%20We%20fine-tuned%20two%0AVLFMs%20using%20our%20dataset%3A%20CLIP%2C%20a%20discriminative%20model%2C%20and%20CoCa%2C%20an%0Aimage-to-text%20generative%20model.%20This%20process%20resulted%20in%20our%20proposed%20HQRS-CLIP%0Aand%20RS-CoCa%20models.%20Experimental%20results%20demonstrate%20that%20HQRS-CLIP%20surpassed%0Athe%20previous%20SOTA%20RS%20CLIP%20model%20in%20various%20downstream%20tasks%20while%20using%20only%0A4.2%5C%25%20of%20the%20training%20data.%20RS-CoCa%20outperforms%20other%20advanced%20approaches%0Aacross%20benchmark%20datasets%20and%20can%20generate%20captions%20for%20RS%20images%20that%20rival%20or%0Aeven%20exceed%20manual%20annotations.%20Dataset%2C%20pre-trained%20models%2C%20and%20codes%20will%20be%0Areleased%20at%20https%3A//github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16716v1&entry.124074799=Read"},
{"title": "Visual-Language Model Knowledge Distillation Method for Image Quality\n  Assessment", "author": "Yongkang Hou and Jiarun Song", "abstract": "  Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.\n", "link": "http://arxiv.org/abs/2507.15680v2", "date": "2025-07-22", "relevancy": 2.7581, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-Language%20Model%20Knowledge%20Distillation%20Method%20for%20Image%20Quality%0A%20%20Assessment&body=Title%3A%20Visual-Language%20Model%20Knowledge%20Distillation%20Method%20for%20Image%20Quality%0A%20%20Assessment%0AAuthor%3A%20Yongkang%20Hou%20and%20Jiarun%20Song%0AAbstract%3A%20%20%20Image%20Quality%20Assessment%20%28IQA%29%20is%20a%20core%20task%20in%20computer%20vision.%20Multimodal%0Amethods%20based%20on%20vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20demonstrated%0Aexceptional%20generalization%20capabilities%20in%20IQA%20tasks.%20To%20address%20the%20issues%20of%0Aexcessive%20parameter%20burden%20and%20insufficient%20ability%20to%20identify%20local%20distorted%0Afeatures%20in%20CLIP%20for%20IQA%2C%20this%20study%20proposes%20a%20visual-language%20model%20knowledge%0Adistillation%20method%20aimed%20at%20guiding%20the%20training%20of%20models%20with%20architectural%0Aadvantages%20using%20CLIP%27s%20IQA%20knowledge.%20First%2C%20quality-graded%20prompt%20templates%0Awere%20designed%20to%20guide%20CLIP%20to%20output%20quality%20scores.%20Then%2C%20CLIP%20is%20fine-tuned%0Ato%20enhance%20its%20capabilities%20in%20IQA%20tasks.%20Finally%2C%20a%20modality-adaptive%0Aknowledge%20distillation%20strategy%20is%20proposed%20to%20achieve%20guidance%20from%20the%20CLIP%0Ateacher%20model%20to%20the%20student%20model.%20Our%20experiments%20were%20conducted%20on%20multiple%0AIQA%20datasets%2C%20and%20the%20results%20show%20that%20the%20proposed%20method%20significantly%0Areduces%20model%20complexity%20while%20outperforming%20existing%20IQA%20methods%2C%0Ademonstrating%20strong%20potential%20for%20practical%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15680v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-Language%2520Model%2520Knowledge%2520Distillation%2520Method%2520for%2520Image%2520Quality%250A%2520%2520Assessment%26entry.906535625%3DYongkang%2520Hou%2520and%2520Jiarun%2520Song%26entry.1292438233%3D%2520%2520Image%2520Quality%2520Assessment%2520%2528IQA%2529%2520is%2520a%2520core%2520task%2520in%2520computer%2520vision.%2520Multimodal%250Amethods%2520based%2520on%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520have%2520demonstrated%250Aexceptional%2520generalization%2520capabilities%2520in%2520IQA%2520tasks.%2520To%2520address%2520the%2520issues%2520of%250Aexcessive%2520parameter%2520burden%2520and%2520insufficient%2520ability%2520to%2520identify%2520local%2520distorted%250Afeatures%2520in%2520CLIP%2520for%2520IQA%252C%2520this%2520study%2520proposes%2520a%2520visual-language%2520model%2520knowledge%250Adistillation%2520method%2520aimed%2520at%2520guiding%2520the%2520training%2520of%2520models%2520with%2520architectural%250Aadvantages%2520using%2520CLIP%2527s%2520IQA%2520knowledge.%2520First%252C%2520quality-graded%2520prompt%2520templates%250Awere%2520designed%2520to%2520guide%2520CLIP%2520to%2520output%2520quality%2520scores.%2520Then%252C%2520CLIP%2520is%2520fine-tuned%250Ato%2520enhance%2520its%2520capabilities%2520in%2520IQA%2520tasks.%2520Finally%252C%2520a%2520modality-adaptive%250Aknowledge%2520distillation%2520strategy%2520is%2520proposed%2520to%2520achieve%2520guidance%2520from%2520the%2520CLIP%250Ateacher%2520model%2520to%2520the%2520student%2520model.%2520Our%2520experiments%2520were%2520conducted%2520on%2520multiple%250AIQA%2520datasets%252C%2520and%2520the%2520results%2520show%2520that%2520the%2520proposed%2520method%2520significantly%250Areduces%2520model%2520complexity%2520while%2520outperforming%2520existing%2520IQA%2520methods%252C%250Ademonstrating%2520strong%2520potential%2520for%2520practical%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15680v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-Language%20Model%20Knowledge%20Distillation%20Method%20for%20Image%20Quality%0A%20%20Assessment&entry.906535625=Yongkang%20Hou%20and%20Jiarun%20Song&entry.1292438233=%20%20Image%20Quality%20Assessment%20%28IQA%29%20is%20a%20core%20task%20in%20computer%20vision.%20Multimodal%0Amethods%20based%20on%20vision-language%20models%2C%20such%20as%20CLIP%2C%20have%20demonstrated%0Aexceptional%20generalization%20capabilities%20in%20IQA%20tasks.%20To%20address%20the%20issues%20of%0Aexcessive%20parameter%20burden%20and%20insufficient%20ability%20to%20identify%20local%20distorted%0Afeatures%20in%20CLIP%20for%20IQA%2C%20this%20study%20proposes%20a%20visual-language%20model%20knowledge%0Adistillation%20method%20aimed%20at%20guiding%20the%20training%20of%20models%20with%20architectural%0Aadvantages%20using%20CLIP%27s%20IQA%20knowledge.%20First%2C%20quality-graded%20prompt%20templates%0Awere%20designed%20to%20guide%20CLIP%20to%20output%20quality%20scores.%20Then%2C%20CLIP%20is%20fine-tuned%0Ato%20enhance%20its%20capabilities%20in%20IQA%20tasks.%20Finally%2C%20a%20modality-adaptive%0Aknowledge%20distillation%20strategy%20is%20proposed%20to%20achieve%20guidance%20from%20the%20CLIP%0Ateacher%20model%20to%20the%20student%20model.%20Our%20experiments%20were%20conducted%20on%20multiple%0AIQA%20datasets%2C%20and%20the%20results%20show%20that%20the%20proposed%20method%20significantly%0Areduces%20model%20complexity%20while%20outperforming%20existing%20IQA%20methods%2C%0Ademonstrating%20strong%20potential%20for%20practical%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15680v2&entry.124074799=Read"},
{"title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning", "author": "Ang Li and Charles Wang and Kaiyu Yue and Zikui Cai and Ollie Liu and Deqing Fu and Peng Guo and Wang Bill Zhu and Vatsal Sharan and Robin Jia and Willie Neiswanger and Furong Huang and Tom Goldstein and Micah Goldblum", "abstract": "  Humans often use visual aids, for example diagrams or sketches, when solving\ncomplex problems. Training multimodal models to do the same, known as Visual\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\nof high-quality visual CoT training data. We introduce $\\textbf{Zebra-CoT}$, a\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\ninterleaved text-image reasoning traces. We focus on four categories of tasks\nwhere sketching or visual reasoning is especially natural, spanning scientific\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\nmulti-hop inference, embodied and robot planning; visual logic problems and\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\ntraining corpus results in an improvement of +12% in our test-set accuracy and\nyields up to +13% performance gain on standard VLM benchmark evaluations.\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\nmultimodal reasoning abilities. We open-source our dataset and models to\nsupport development and evaluation of visual CoT.\n", "link": "http://arxiv.org/abs/2507.16746v1", "date": "2025-07-22", "relevancy": 2.7576, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.559}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zebra-CoT%3A%20A%20Dataset%20for%20Interleaved%20Vision%20Language%20Reasoning&body=Title%3A%20Zebra-CoT%3A%20A%20Dataset%20for%20Interleaved%20Vision%20Language%20Reasoning%0AAuthor%3A%20Ang%20Li%20and%20Charles%20Wang%20and%20Kaiyu%20Yue%20and%20Zikui%20Cai%20and%20Ollie%20Liu%20and%20Deqing%20Fu%20and%20Peng%20Guo%20and%20Wang%20Bill%20Zhu%20and%20Vatsal%20Sharan%20and%20Robin%20Jia%20and%20Willie%20Neiswanger%20and%20Furong%20Huang%20and%20Tom%20Goldstein%20and%20Micah%20Goldblum%0AAbstract%3A%20%20%20Humans%20often%20use%20visual%20aids%2C%20for%20example%20diagrams%20or%20sketches%2C%20when%20solving%0Acomplex%20problems.%20Training%20multimodal%20models%20to%20do%20the%20same%2C%20known%20as%20Visual%0AChain%20of%20Thought%20%28Visual%20CoT%29%2C%20is%20challenging%20due%20to%3A%20%281%29%20poor%20off-the-shelf%0Avisual%20CoT%20performance%2C%20which%20hinders%20reinforcement%20learning%2C%20and%20%282%29%20the%20lack%0Aof%20high-quality%20visual%20CoT%20training%20data.%20We%20introduce%20%24%5Ctextbf%7BZebra-CoT%7D%24%2C%20a%0Adiverse%20large-scale%20dataset%20with%20182%2C384%20samples%2C%20containing%20logically%20coherent%0Ainterleaved%20text-image%20reasoning%20traces.%20We%20focus%20on%20four%20categories%20of%20tasks%0Awhere%20sketching%20or%20visual%20reasoning%20is%20especially%20natural%2C%20spanning%20scientific%0Aquestions%20such%20as%20geometry%2C%20physics%2C%20and%20algorithms%3B%202D%20visual%20reasoning%20tasks%0Alike%20visual%20search%20and%20jigsaw%20puzzles%3B%203D%20reasoning%20tasks%20including%203D%0Amulti-hop%20inference%2C%20embodied%20and%20robot%20planning%3B%20visual%20logic%20problems%20and%0Astrategic%20games%20like%20chess.%20Fine-tuning%20the%20Anole-7B%20model%20on%20the%20Zebra-CoT%0Atraining%20corpus%20results%20in%20an%20improvement%20of%20%2B12%25%20in%20our%20test-set%20accuracy%20and%0Ayields%20up%20to%20%2B13%25%20performance%20gain%20on%20standard%20VLM%20benchmark%20evaluations.%0AFine-tuning%20Bagel-7B%20yields%20a%20model%20that%20generates%20high-quality%20interleaved%0Avisual%20reasoning%20chains%2C%20underscoring%20Zebra-CoT%27s%20effectiveness%20for%20developing%0Amultimodal%20reasoning%20abilities.%20We%20open-source%20our%20dataset%20and%20models%20to%0Asupport%20development%20and%20evaluation%20of%20visual%20CoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZebra-CoT%253A%2520A%2520Dataset%2520for%2520Interleaved%2520Vision%2520Language%2520Reasoning%26entry.906535625%3DAng%2520Li%2520and%2520Charles%2520Wang%2520and%2520Kaiyu%2520Yue%2520and%2520Zikui%2520Cai%2520and%2520Ollie%2520Liu%2520and%2520Deqing%2520Fu%2520and%2520Peng%2520Guo%2520and%2520Wang%2520Bill%2520Zhu%2520and%2520Vatsal%2520Sharan%2520and%2520Robin%2520Jia%2520and%2520Willie%2520Neiswanger%2520and%2520Furong%2520Huang%2520and%2520Tom%2520Goldstein%2520and%2520Micah%2520Goldblum%26entry.1292438233%3D%2520%2520Humans%2520often%2520use%2520visual%2520aids%252C%2520for%2520example%2520diagrams%2520or%2520sketches%252C%2520when%2520solving%250Acomplex%2520problems.%2520Training%2520multimodal%2520models%2520to%2520do%2520the%2520same%252C%2520known%2520as%2520Visual%250AChain%2520of%2520Thought%2520%2528Visual%2520CoT%2529%252C%2520is%2520challenging%2520due%2520to%253A%2520%25281%2529%2520poor%2520off-the-shelf%250Avisual%2520CoT%2520performance%252C%2520which%2520hinders%2520reinforcement%2520learning%252C%2520and%2520%25282%2529%2520the%2520lack%250Aof%2520high-quality%2520visual%2520CoT%2520training%2520data.%2520We%2520introduce%2520%2524%255Ctextbf%257BZebra-CoT%257D%2524%252C%2520a%250Adiverse%2520large-scale%2520dataset%2520with%2520182%252C384%2520samples%252C%2520containing%2520logically%2520coherent%250Ainterleaved%2520text-image%2520reasoning%2520traces.%2520We%2520focus%2520on%2520four%2520categories%2520of%2520tasks%250Awhere%2520sketching%2520or%2520visual%2520reasoning%2520is%2520especially%2520natural%252C%2520spanning%2520scientific%250Aquestions%2520such%2520as%2520geometry%252C%2520physics%252C%2520and%2520algorithms%253B%25202D%2520visual%2520reasoning%2520tasks%250Alike%2520visual%2520search%2520and%2520jigsaw%2520puzzles%253B%25203D%2520reasoning%2520tasks%2520including%25203D%250Amulti-hop%2520inference%252C%2520embodied%2520and%2520robot%2520planning%253B%2520visual%2520logic%2520problems%2520and%250Astrategic%2520games%2520like%2520chess.%2520Fine-tuning%2520the%2520Anole-7B%2520model%2520on%2520the%2520Zebra-CoT%250Atraining%2520corpus%2520results%2520in%2520an%2520improvement%2520of%2520%252B12%2525%2520in%2520our%2520test-set%2520accuracy%2520and%250Ayields%2520up%2520to%2520%252B13%2525%2520performance%2520gain%2520on%2520standard%2520VLM%2520benchmark%2520evaluations.%250AFine-tuning%2520Bagel-7B%2520yields%2520a%2520model%2520that%2520generates%2520high-quality%2520interleaved%250Avisual%2520reasoning%2520chains%252C%2520underscoring%2520Zebra-CoT%2527s%2520effectiveness%2520for%2520developing%250Amultimodal%2520reasoning%2520abilities.%2520We%2520open-source%2520our%2520dataset%2520and%2520models%2520to%250Asupport%2520development%2520and%2520evaluation%2520of%2520visual%2520CoT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zebra-CoT%3A%20A%20Dataset%20for%20Interleaved%20Vision%20Language%20Reasoning&entry.906535625=Ang%20Li%20and%20Charles%20Wang%20and%20Kaiyu%20Yue%20and%20Zikui%20Cai%20and%20Ollie%20Liu%20and%20Deqing%20Fu%20and%20Peng%20Guo%20and%20Wang%20Bill%20Zhu%20and%20Vatsal%20Sharan%20and%20Robin%20Jia%20and%20Willie%20Neiswanger%20and%20Furong%20Huang%20and%20Tom%20Goldstein%20and%20Micah%20Goldblum&entry.1292438233=%20%20Humans%20often%20use%20visual%20aids%2C%20for%20example%20diagrams%20or%20sketches%2C%20when%20solving%0Acomplex%20problems.%20Training%20multimodal%20models%20to%20do%20the%20same%2C%20known%20as%20Visual%0AChain%20of%20Thought%20%28Visual%20CoT%29%2C%20is%20challenging%20due%20to%3A%20%281%29%20poor%20off-the-shelf%0Avisual%20CoT%20performance%2C%20which%20hinders%20reinforcement%20learning%2C%20and%20%282%29%20the%20lack%0Aof%20high-quality%20visual%20CoT%20training%20data.%20We%20introduce%20%24%5Ctextbf%7BZebra-CoT%7D%24%2C%20a%0Adiverse%20large-scale%20dataset%20with%20182%2C384%20samples%2C%20containing%20logically%20coherent%0Ainterleaved%20text-image%20reasoning%20traces.%20We%20focus%20on%20four%20categories%20of%20tasks%0Awhere%20sketching%20or%20visual%20reasoning%20is%20especially%20natural%2C%20spanning%20scientific%0Aquestions%20such%20as%20geometry%2C%20physics%2C%20and%20algorithms%3B%202D%20visual%20reasoning%20tasks%0Alike%20visual%20search%20and%20jigsaw%20puzzles%3B%203D%20reasoning%20tasks%20including%203D%0Amulti-hop%20inference%2C%20embodied%20and%20robot%20planning%3B%20visual%20logic%20problems%20and%0Astrategic%20games%20like%20chess.%20Fine-tuning%20the%20Anole-7B%20model%20on%20the%20Zebra-CoT%0Atraining%20corpus%20results%20in%20an%20improvement%20of%20%2B12%25%20in%20our%20test-set%20accuracy%20and%0Ayields%20up%20to%20%2B13%25%20performance%20gain%20on%20standard%20VLM%20benchmark%20evaluations.%0AFine-tuning%20Bagel-7B%20yields%20a%20model%20that%20generates%20high-quality%20interleaved%0Avisual%20reasoning%20chains%2C%20underscoring%20Zebra-CoT%27s%20effectiveness%20for%20developing%0Amultimodal%20reasoning%20abilities.%20We%20open-source%20our%20dataset%20and%20models%20to%0Asupport%20development%20and%20evaluation%20of%20visual%20CoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16746v1&entry.124074799=Read"},
{"title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for\n  Zero-Shot Anomaly Detection", "author": "Ziteng Yang and Jingzehua Xu and Yanshu Li and Zepeng Li and Yeqiang Wang and Xinghui Li", "abstract": "  Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization.\n", "link": "http://arxiv.org/abs/2505.17692v2", "date": "2025-07-22", "relevancy": 2.749, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViP%24%5E2%24-CLIP%3A%20Visual-Perception%20Prompting%20with%20Unified%20Alignment%20for%0A%20%20Zero-Shot%20Anomaly%20Detection&body=Title%3A%20ViP%24%5E2%24-CLIP%3A%20Visual-Perception%20Prompting%20with%20Unified%20Alignment%20for%0A%20%20Zero-Shot%20Anomaly%20Detection%0AAuthor%3A%20Ziteng%20Yang%20and%20Jingzehua%20Xu%20and%20Yanshu%20Li%20and%20Zepeng%20Li%20and%20Yeqiang%20Wang%20and%20Xinghui%20Li%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20aims%20to%20detect%20anomalies%20without%20any%0Atarget%20domain%20training%20samples%2C%20relying%20solely%20on%20external%20auxiliary%20data.%0AExisting%20CLIP-based%20methods%20attempt%20to%20activate%20the%20model%27s%20ZSAD%20potential%20via%0Ahandcrafted%20or%20static%20learnable%20prompts.%20The%20former%20incur%20high%20engineering%0Acosts%20and%20limited%20semantic%20coverage%2C%20whereas%20the%20latter%20apply%20identical%0Adescriptions%20across%20diverse%20anomaly%20types%2C%20thus%20fail%20to%20adapt%20to%20complex%0Avariations.%20Furthermore%2C%20since%20CLIP%20is%20originally%20pretrained%20on%20large-scale%0Aclassification%20tasks%2C%20its%20anomaly%20segmentation%20quality%20is%20highly%20sensitive%20to%0Athe%20exact%20wording%20of%20class%20names%2C%20severely%20constraining%20prompting%20strategies%0Athat%20depend%20on%20class%20labels.%20To%20address%20these%20challenges%2C%20we%20introduce%0AViP%24%5E%7B2%7D%24-CLIP.%20The%20key%20insight%20of%20ViP%24%5E%7B2%7D%24-CLIP%20is%20a%20Visual-Perception%0APrompting%20%28ViP-Prompt%29%20mechanism%2C%20which%20fuses%20global%20and%20multi-scale%20local%0Avisual%20context%20to%20adaptively%20generate%20fine-grained%20textual%20prompts%2C%20eliminating%0Amanual%20templates%20and%20class-name%20priors.%20This%20design%20enables%20our%20model%20to%20focus%0Aon%20precise%20abnormal%20regions%2C%20making%20it%20particularly%20valuable%20when%20category%0Alabels%20are%20ambiguous%20or%20privacy-constrained.%20Extensive%20experiments%20on%2015%0Aindustrial%20and%20medical%20benchmarks%20demonstrate%20that%20ViP%24%5E%7B2%7D%24-CLIP%20achieves%0Astate-of-the-art%20performance%20and%20robust%20cross-domain%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.17692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViP%2524%255E2%2524-CLIP%253A%2520Visual-Perception%2520Prompting%2520with%2520Unified%2520Alignment%2520for%250A%2520%2520Zero-Shot%2520Anomaly%2520Detection%26entry.906535625%3DZiteng%2520Yang%2520and%2520Jingzehua%2520Xu%2520and%2520Yanshu%2520Li%2520and%2520Zepeng%2520Li%2520and%2520Yeqiang%2520Wang%2520and%2520Xinghui%2520Li%26entry.1292438233%3D%2520%2520Zero-shot%2520anomaly%2520detection%2520%2528ZSAD%2529%2520aims%2520to%2520detect%2520anomalies%2520without%2520any%250Atarget%2520domain%2520training%2520samples%252C%2520relying%2520solely%2520on%2520external%2520auxiliary%2520data.%250AExisting%2520CLIP-based%2520methods%2520attempt%2520to%2520activate%2520the%2520model%2527s%2520ZSAD%2520potential%2520via%250Ahandcrafted%2520or%2520static%2520learnable%2520prompts.%2520The%2520former%2520incur%2520high%2520engineering%250Acosts%2520and%2520limited%2520semantic%2520coverage%252C%2520whereas%2520the%2520latter%2520apply%2520identical%250Adescriptions%2520across%2520diverse%2520anomaly%2520types%252C%2520thus%2520fail%2520to%2520adapt%2520to%2520complex%250Avariations.%2520Furthermore%252C%2520since%2520CLIP%2520is%2520originally%2520pretrained%2520on%2520large-scale%250Aclassification%2520tasks%252C%2520its%2520anomaly%2520segmentation%2520quality%2520is%2520highly%2520sensitive%2520to%250Athe%2520exact%2520wording%2520of%2520class%2520names%252C%2520severely%2520constraining%2520prompting%2520strategies%250Athat%2520depend%2520on%2520class%2520labels.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AViP%2524%255E%257B2%257D%2524-CLIP.%2520The%2520key%2520insight%2520of%2520ViP%2524%255E%257B2%257D%2524-CLIP%2520is%2520a%2520Visual-Perception%250APrompting%2520%2528ViP-Prompt%2529%2520mechanism%252C%2520which%2520fuses%2520global%2520and%2520multi-scale%2520local%250Avisual%2520context%2520to%2520adaptively%2520generate%2520fine-grained%2520textual%2520prompts%252C%2520eliminating%250Amanual%2520templates%2520and%2520class-name%2520priors.%2520This%2520design%2520enables%2520our%2520model%2520to%2520focus%250Aon%2520precise%2520abnormal%2520regions%252C%2520making%2520it%2520particularly%2520valuable%2520when%2520category%250Alabels%2520are%2520ambiguous%2520or%2520privacy-constrained.%2520Extensive%2520experiments%2520on%252015%250Aindustrial%2520and%2520medical%2520benchmarks%2520demonstrate%2520that%2520ViP%2524%255E%257B2%257D%2524-CLIP%2520achieves%250Astate-of-the-art%2520performance%2520and%2520robust%2520cross-domain%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.17692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViP%24%5E2%24-CLIP%3A%20Visual-Perception%20Prompting%20with%20Unified%20Alignment%20for%0A%20%20Zero-Shot%20Anomaly%20Detection&entry.906535625=Ziteng%20Yang%20and%20Jingzehua%20Xu%20and%20Yanshu%20Li%20and%20Zepeng%20Li%20and%20Yeqiang%20Wang%20and%20Xinghui%20Li&entry.1292438233=%20%20Zero-shot%20anomaly%20detection%20%28ZSAD%29%20aims%20to%20detect%20anomalies%20without%20any%0Atarget%20domain%20training%20samples%2C%20relying%20solely%20on%20external%20auxiliary%20data.%0AExisting%20CLIP-based%20methods%20attempt%20to%20activate%20the%20model%27s%20ZSAD%20potential%20via%0Ahandcrafted%20or%20static%20learnable%20prompts.%20The%20former%20incur%20high%20engineering%0Acosts%20and%20limited%20semantic%20coverage%2C%20whereas%20the%20latter%20apply%20identical%0Adescriptions%20across%20diverse%20anomaly%20types%2C%20thus%20fail%20to%20adapt%20to%20complex%0Avariations.%20Furthermore%2C%20since%20CLIP%20is%20originally%20pretrained%20on%20large-scale%0Aclassification%20tasks%2C%20its%20anomaly%20segmentation%20quality%20is%20highly%20sensitive%20to%0Athe%20exact%20wording%20of%20class%20names%2C%20severely%20constraining%20prompting%20strategies%0Athat%20depend%20on%20class%20labels.%20To%20address%20these%20challenges%2C%20we%20introduce%0AViP%24%5E%7B2%7D%24-CLIP.%20The%20key%20insight%20of%20ViP%24%5E%7B2%7D%24-CLIP%20is%20a%20Visual-Perception%0APrompting%20%28ViP-Prompt%29%20mechanism%2C%20which%20fuses%20global%20and%20multi-scale%20local%0Avisual%20context%20to%20adaptively%20generate%20fine-grained%20textual%20prompts%2C%20eliminating%0Amanual%20templates%20and%20class-name%20priors.%20This%20design%20enables%20our%20model%20to%20focus%0Aon%20precise%20abnormal%20regions%2C%20making%20it%20particularly%20valuable%20when%20category%0Alabels%20are%20ambiguous%20or%20privacy-constrained.%20Extensive%20experiments%20on%2015%0Aindustrial%20and%20medical%20benchmarks%20demonstrate%20that%20ViP%24%5E%7B2%7D%24-CLIP%20achieves%0Astate-of-the-art%20performance%20and%20robust%20cross-domain%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.17692v2&entry.124074799=Read"},
{"title": "A2Mamba: Attention-augmented State Space Models for Visual Recognition", "author": "Meng Lou and Yunxiang Fu and Yizhou Yu", "abstract": "  Transformers and Mamba, initially invented for natural language processing,\nhave inspired backbone architectures for visual recognition. Recent studies\nintegrated Local Attention Transformers with Mamba to capture both local\ndetails and global contexts. Despite competitive performance, these methods are\nlimited to simple stacking of Transformer and Mamba layers without any\ninteraction mechanism between them. Thus, deep integration between Transformer\nand Mamba layers remains an open problem. We address this problem by proposing\nA2Mamba, a powerful Transformer-Mamba hybrid network architecture, featuring a\nnew token mixer termed Multi-scale Attention-augmented State Space Model\n(MASS), where multi-scale attention maps are integrated into an\nattention-augmented SSM (A2SSM). A key step of A2SSM performs a variant of\ncross-attention by spatially aggregating the SSM's hidden states using the\nmulti-scale attention maps, which enhances spatial dependencies pertaining to a\ntwo-dimensional space while improving the dynamic modeling capabilities of\nSSMs. Our A2Mamba outperforms all previous ConvNet-, Transformer-, and\nMamba-based architectures in visual recognition tasks. For instance, A2Mamba-L\nachieves an impressive 86.1% top-1 accuracy on ImageNet-1K. In semantic\nsegmentation, A2Mamba-B exceeds CAFormer-S36 by 2.5% in mIoU, while exhibiting\nhigher efficiency. In object detection and instance segmentation with Cascade\nMask R-CNN, A2Mamba-S surpasses MambaVision-B by 1.2%/0.9% in AP^b/AP^m, while\nhaving 40% less parameters. Code is publicly available at\nhttps://github.com/LMMMEng/A2Mamba.\n", "link": "http://arxiv.org/abs/2507.16624v1", "date": "2025-07-22", "relevancy": 2.7288, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5848}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5359}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A2Mamba%3A%20Attention-augmented%20State%20Space%20Models%20for%20Visual%20Recognition&body=Title%3A%20A2Mamba%3A%20Attention-augmented%20State%20Space%20Models%20for%20Visual%20Recognition%0AAuthor%3A%20Meng%20Lou%20and%20Yunxiang%20Fu%20and%20Yizhou%20Yu%0AAbstract%3A%20%20%20Transformers%20and%20Mamba%2C%20initially%20invented%20for%20natural%20language%20processing%2C%0Ahave%20inspired%20backbone%20architectures%20for%20visual%20recognition.%20Recent%20studies%0Aintegrated%20Local%20Attention%20Transformers%20with%20Mamba%20to%20capture%20both%20local%0Adetails%20and%20global%20contexts.%20Despite%20competitive%20performance%2C%20these%20methods%20are%0Alimited%20to%20simple%20stacking%20of%20Transformer%20and%20Mamba%20layers%20without%20any%0Ainteraction%20mechanism%20between%20them.%20Thus%2C%20deep%20integration%20between%20Transformer%0Aand%20Mamba%20layers%20remains%20an%20open%20problem.%20We%20address%20this%20problem%20by%20proposing%0AA2Mamba%2C%20a%20powerful%20Transformer-Mamba%20hybrid%20network%20architecture%2C%20featuring%20a%0Anew%20token%20mixer%20termed%20Multi-scale%20Attention-augmented%20State%20Space%20Model%0A%28MASS%29%2C%20where%20multi-scale%20attention%20maps%20are%20integrated%20into%20an%0Aattention-augmented%20SSM%20%28A2SSM%29.%20A%20key%20step%20of%20A2SSM%20performs%20a%20variant%20of%0Across-attention%20by%20spatially%20aggregating%20the%20SSM%27s%20hidden%20states%20using%20the%0Amulti-scale%20attention%20maps%2C%20which%20enhances%20spatial%20dependencies%20pertaining%20to%20a%0Atwo-dimensional%20space%20while%20improving%20the%20dynamic%20modeling%20capabilities%20of%0ASSMs.%20Our%20A2Mamba%20outperforms%20all%20previous%20ConvNet-%2C%20Transformer-%2C%20and%0AMamba-based%20architectures%20in%20visual%20recognition%20tasks.%20For%20instance%2C%20A2Mamba-L%0Aachieves%20an%20impressive%2086.1%25%20top-1%20accuracy%20on%20ImageNet-1K.%20In%20semantic%0Asegmentation%2C%20A2Mamba-B%20exceeds%20CAFormer-S36%20by%202.5%25%20in%20mIoU%2C%20while%20exhibiting%0Ahigher%20efficiency.%20In%20object%20detection%20and%20instance%20segmentation%20with%20Cascade%0AMask%20R-CNN%2C%20A2Mamba-S%20surpasses%20MambaVision-B%20by%201.2%25/0.9%25%20in%20AP%5Eb/AP%5Em%2C%20while%0Ahaving%2040%25%20less%20parameters.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/LMMMEng/A2Mamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA2Mamba%253A%2520Attention-augmented%2520State%2520Space%2520Models%2520for%2520Visual%2520Recognition%26entry.906535625%3DMeng%2520Lou%2520and%2520Yunxiang%2520Fu%2520and%2520Yizhou%2520Yu%26entry.1292438233%3D%2520%2520Transformers%2520and%2520Mamba%252C%2520initially%2520invented%2520for%2520natural%2520language%2520processing%252C%250Ahave%2520inspired%2520backbone%2520architectures%2520for%2520visual%2520recognition.%2520Recent%2520studies%250Aintegrated%2520Local%2520Attention%2520Transformers%2520with%2520Mamba%2520to%2520capture%2520both%2520local%250Adetails%2520and%2520global%2520contexts.%2520Despite%2520competitive%2520performance%252C%2520these%2520methods%2520are%250Alimited%2520to%2520simple%2520stacking%2520of%2520Transformer%2520and%2520Mamba%2520layers%2520without%2520any%250Ainteraction%2520mechanism%2520between%2520them.%2520Thus%252C%2520deep%2520integration%2520between%2520Transformer%250Aand%2520Mamba%2520layers%2520remains%2520an%2520open%2520problem.%2520We%2520address%2520this%2520problem%2520by%2520proposing%250AA2Mamba%252C%2520a%2520powerful%2520Transformer-Mamba%2520hybrid%2520network%2520architecture%252C%2520featuring%2520a%250Anew%2520token%2520mixer%2520termed%2520Multi-scale%2520Attention-augmented%2520State%2520Space%2520Model%250A%2528MASS%2529%252C%2520where%2520multi-scale%2520attention%2520maps%2520are%2520integrated%2520into%2520an%250Aattention-augmented%2520SSM%2520%2528A2SSM%2529.%2520A%2520key%2520step%2520of%2520A2SSM%2520performs%2520a%2520variant%2520of%250Across-attention%2520by%2520spatially%2520aggregating%2520the%2520SSM%2527s%2520hidden%2520states%2520using%2520the%250Amulti-scale%2520attention%2520maps%252C%2520which%2520enhances%2520spatial%2520dependencies%2520pertaining%2520to%2520a%250Atwo-dimensional%2520space%2520while%2520improving%2520the%2520dynamic%2520modeling%2520capabilities%2520of%250ASSMs.%2520Our%2520A2Mamba%2520outperforms%2520all%2520previous%2520ConvNet-%252C%2520Transformer-%252C%2520and%250AMamba-based%2520architectures%2520in%2520visual%2520recognition%2520tasks.%2520For%2520instance%252C%2520A2Mamba-L%250Aachieves%2520an%2520impressive%252086.1%2525%2520top-1%2520accuracy%2520on%2520ImageNet-1K.%2520In%2520semantic%250Asegmentation%252C%2520A2Mamba-B%2520exceeds%2520CAFormer-S36%2520by%25202.5%2525%2520in%2520mIoU%252C%2520while%2520exhibiting%250Ahigher%2520efficiency.%2520In%2520object%2520detection%2520and%2520instance%2520segmentation%2520with%2520Cascade%250AMask%2520R-CNN%252C%2520A2Mamba-S%2520surpasses%2520MambaVision-B%2520by%25201.2%2525/0.9%2525%2520in%2520AP%255Eb/AP%255Em%252C%2520while%250Ahaving%252040%2525%2520less%2520parameters.%2520Code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/LMMMEng/A2Mamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A2Mamba%3A%20Attention-augmented%20State%20Space%20Models%20for%20Visual%20Recognition&entry.906535625=Meng%20Lou%20and%20Yunxiang%20Fu%20and%20Yizhou%20Yu&entry.1292438233=%20%20Transformers%20and%20Mamba%2C%20initially%20invented%20for%20natural%20language%20processing%2C%0Ahave%20inspired%20backbone%20architectures%20for%20visual%20recognition.%20Recent%20studies%0Aintegrated%20Local%20Attention%20Transformers%20with%20Mamba%20to%20capture%20both%20local%0Adetails%20and%20global%20contexts.%20Despite%20competitive%20performance%2C%20these%20methods%20are%0Alimited%20to%20simple%20stacking%20of%20Transformer%20and%20Mamba%20layers%20without%20any%0Ainteraction%20mechanism%20between%20them.%20Thus%2C%20deep%20integration%20between%20Transformer%0Aand%20Mamba%20layers%20remains%20an%20open%20problem.%20We%20address%20this%20problem%20by%20proposing%0AA2Mamba%2C%20a%20powerful%20Transformer-Mamba%20hybrid%20network%20architecture%2C%20featuring%20a%0Anew%20token%20mixer%20termed%20Multi-scale%20Attention-augmented%20State%20Space%20Model%0A%28MASS%29%2C%20where%20multi-scale%20attention%20maps%20are%20integrated%20into%20an%0Aattention-augmented%20SSM%20%28A2SSM%29.%20A%20key%20step%20of%20A2SSM%20performs%20a%20variant%20of%0Across-attention%20by%20spatially%20aggregating%20the%20SSM%27s%20hidden%20states%20using%20the%0Amulti-scale%20attention%20maps%2C%20which%20enhances%20spatial%20dependencies%20pertaining%20to%20a%0Atwo-dimensional%20space%20while%20improving%20the%20dynamic%20modeling%20capabilities%20of%0ASSMs.%20Our%20A2Mamba%20outperforms%20all%20previous%20ConvNet-%2C%20Transformer-%2C%20and%0AMamba-based%20architectures%20in%20visual%20recognition%20tasks.%20For%20instance%2C%20A2Mamba-L%0Aachieves%20an%20impressive%2086.1%25%20top-1%20accuracy%20on%20ImageNet-1K.%20In%20semantic%0Asegmentation%2C%20A2Mamba-B%20exceeds%20CAFormer-S36%20by%202.5%25%20in%20mIoU%2C%20while%20exhibiting%0Ahigher%20efficiency.%20In%20object%20detection%20and%20instance%20segmentation%20with%20Cascade%0AMask%20R-CNN%2C%20A2Mamba-S%20surpasses%20MambaVision-B%20by%201.2%25/0.9%25%20in%20AP%5Eb/AP%5Em%2C%20while%0Ahaving%2040%25%20less%20parameters.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/LMMMEng/A2Mamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16624v1&entry.124074799=Read"},
{"title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "author": "Fei Tang and Zhangxuan Gu and Zhengxi Lu and Xuyang Liu and Shuheng Shen and Changhua Meng and Wen Wang and Wenqi Zhang and Yongliang Shen and Weiming Lu and Jun Xiao and Yueting Zhuang", "abstract": "  Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.\n", "link": "http://arxiv.org/abs/2507.15846v2", "date": "2025-07-22", "relevancy": 2.7009, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5456}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5433}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI-G%24%5E2%24%3A%20Gaussian%20Reward%20Modeling%20for%20GUI%20Grounding&body=Title%3A%20GUI-G%24%5E2%24%3A%20Gaussian%20Reward%20Modeling%20for%20GUI%20Grounding%0AAuthor%3A%20Fei%20Tang%20and%20Zhangxuan%20Gu%20and%20Zhengxi%20Lu%20and%20Xuyang%20Liu%20and%20Shuheng%20Shen%20and%20Changhua%20Meng%20and%20Wen%20Wang%20and%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%20maps%20natural%20language%20instructions%0Ato%20precise%20interface%20locations%20for%20autonomous%20interaction.%20Current%0Areinforcement%20learning%20approaches%20use%20binary%20rewards%20that%20treat%20elements%20as%0Ahit-or-miss%20targets%2C%20creating%20sparse%20signals%20that%20ignore%20the%20continuous%20nature%0Aof%20spatial%20interactions.%20Motivated%20by%20human%20clicking%20behavior%20that%20naturally%0Aforms%20Gaussian%20distributions%20centered%20on%20target%20elements%2C%20we%20introduce%20GUI%0AGaussian%20Grounding%20Rewards%20%28GUI-G%24%5E2%24%29%2C%20a%20principled%20reward%20framework%20that%0Amodels%20GUI%20elements%20as%20continuous%20Gaussian%20distributions%20across%20the%20interface%0Aplane.%20GUI-G%24%5E2%24%20incorporates%20two%20synergistic%20mechanisms%3A%20Gaussian%20point%0Arewards%20model%20precise%20localization%20through%20exponentially%20decaying%20distributions%0Acentered%20on%20element%20centroids%2C%20while%20coverage%20rewards%20assess%20spatial%20alignment%0Aby%20measuring%20the%20overlap%20between%20predicted%20Gaussian%20distributions%20and%20target%0Aregions.%20To%20handle%20diverse%20element%20scales%2C%20we%20develop%20an%20adaptive%20variance%0Amechanism%20that%20calibrates%20reward%20distributions%20based%20on%20element%20dimensions.%0AThis%20framework%20transforms%20GUI%20grounding%20from%20sparse%20binary%20classification%20to%0Adense%20continuous%20optimization%2C%20where%20Gaussian%20distributions%20generate%20rich%0Agradient%20signals%20that%20guide%20models%20toward%20optimal%20interaction%20positions.%0AExtensive%20experiments%20across%20ScreenSpot%2C%20ScreenSpot-v2%2C%20and%20ScreenSpot-Pro%0Abenchmarks%20demonstrate%20that%20GUI-G%24%5E2%24%2C%20substantially%20outperforms%0Astate-of-the-art%20method%20UI-TARS-72B%2C%20with%20the%20most%20significant%20improvement%20of%0A24.7%25%20on%20ScreenSpot-Pro.%20Our%20analysis%20reveals%20that%20continuous%20modeling%20provides%0Asuperior%20robustness%20to%20interface%20variations%20and%20enhanced%20generalization%20to%0Aunseen%20layouts%2C%20establishing%20a%20new%20paradigm%20for%20spatial%20reasoning%20in%20GUI%0Ainteraction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI-G%2524%255E2%2524%253A%2520Gaussian%2520Reward%2520Modeling%2520for%2520GUI%2520Grounding%26entry.906535625%3DFei%2520Tang%2520and%2520Zhangxuan%2520Gu%2520and%2520Zhengxi%2520Lu%2520and%2520Xuyang%2520Liu%2520and%2520Shuheng%2520Shen%2520and%2520Changhua%2520Meng%2520and%2520Wen%2520Wang%2520and%2520Wenqi%2520Zhang%2520and%2520Yongliang%2520Shen%2520and%2520Weiming%2520Lu%2520and%2520Jun%2520Xiao%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520grounding%2520maps%2520natural%2520language%2520instructions%250Ato%2520precise%2520interface%2520locations%2520for%2520autonomous%2520interaction.%2520Current%250Areinforcement%2520learning%2520approaches%2520use%2520binary%2520rewards%2520that%2520treat%2520elements%2520as%250Ahit-or-miss%2520targets%252C%2520creating%2520sparse%2520signals%2520that%2520ignore%2520the%2520continuous%2520nature%250Aof%2520spatial%2520interactions.%2520Motivated%2520by%2520human%2520clicking%2520behavior%2520that%2520naturally%250Aforms%2520Gaussian%2520distributions%2520centered%2520on%2520target%2520elements%252C%2520we%2520introduce%2520GUI%250AGaussian%2520Grounding%2520Rewards%2520%2528GUI-G%2524%255E2%2524%2529%252C%2520a%2520principled%2520reward%2520framework%2520that%250Amodels%2520GUI%2520elements%2520as%2520continuous%2520Gaussian%2520distributions%2520across%2520the%2520interface%250Aplane.%2520GUI-G%2524%255E2%2524%2520incorporates%2520two%2520synergistic%2520mechanisms%253A%2520Gaussian%2520point%250Arewards%2520model%2520precise%2520localization%2520through%2520exponentially%2520decaying%2520distributions%250Acentered%2520on%2520element%2520centroids%252C%2520while%2520coverage%2520rewards%2520assess%2520spatial%2520alignment%250Aby%2520measuring%2520the%2520overlap%2520between%2520predicted%2520Gaussian%2520distributions%2520and%2520target%250Aregions.%2520To%2520handle%2520diverse%2520element%2520scales%252C%2520we%2520develop%2520an%2520adaptive%2520variance%250Amechanism%2520that%2520calibrates%2520reward%2520distributions%2520based%2520on%2520element%2520dimensions.%250AThis%2520framework%2520transforms%2520GUI%2520grounding%2520from%2520sparse%2520binary%2520classification%2520to%250Adense%2520continuous%2520optimization%252C%2520where%2520Gaussian%2520distributions%2520generate%2520rich%250Agradient%2520signals%2520that%2520guide%2520models%2520toward%2520optimal%2520interaction%2520positions.%250AExtensive%2520experiments%2520across%2520ScreenSpot%252C%2520ScreenSpot-v2%252C%2520and%2520ScreenSpot-Pro%250Abenchmarks%2520demonstrate%2520that%2520GUI-G%2524%255E2%2524%252C%2520substantially%2520outperforms%250Astate-of-the-art%2520method%2520UI-TARS-72B%252C%2520with%2520the%2520most%2520significant%2520improvement%2520of%250A24.7%2525%2520on%2520ScreenSpot-Pro.%2520Our%2520analysis%2520reveals%2520that%2520continuous%2520modeling%2520provides%250Asuperior%2520robustness%2520to%2520interface%2520variations%2520and%2520enhanced%2520generalization%2520to%250Aunseen%2520layouts%252C%2520establishing%2520a%2520new%2520paradigm%2520for%2520spatial%2520reasoning%2520in%2520GUI%250Ainteraction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI-G%24%5E2%24%3A%20Gaussian%20Reward%20Modeling%20for%20GUI%20Grounding&entry.906535625=Fei%20Tang%20and%20Zhangxuan%20Gu%20and%20Zhengxi%20Lu%20and%20Xuyang%20Liu%20and%20Shuheng%20Shen%20and%20Changhua%20Meng%20and%20Wen%20Wang%20and%20Wenqi%20Zhang%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Jun%20Xiao%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Graphical%20User%20Interface%20%28GUI%29%20grounding%20maps%20natural%20language%20instructions%0Ato%20precise%20interface%20locations%20for%20autonomous%20interaction.%20Current%0Areinforcement%20learning%20approaches%20use%20binary%20rewards%20that%20treat%20elements%20as%0Ahit-or-miss%20targets%2C%20creating%20sparse%20signals%20that%20ignore%20the%20continuous%20nature%0Aof%20spatial%20interactions.%20Motivated%20by%20human%20clicking%20behavior%20that%20naturally%0Aforms%20Gaussian%20distributions%20centered%20on%20target%20elements%2C%20we%20introduce%20GUI%0AGaussian%20Grounding%20Rewards%20%28GUI-G%24%5E2%24%29%2C%20a%20principled%20reward%20framework%20that%0Amodels%20GUI%20elements%20as%20continuous%20Gaussian%20distributions%20across%20the%20interface%0Aplane.%20GUI-G%24%5E2%24%20incorporates%20two%20synergistic%20mechanisms%3A%20Gaussian%20point%0Arewards%20model%20precise%20localization%20through%20exponentially%20decaying%20distributions%0Acentered%20on%20element%20centroids%2C%20while%20coverage%20rewards%20assess%20spatial%20alignment%0Aby%20measuring%20the%20overlap%20between%20predicted%20Gaussian%20distributions%20and%20target%0Aregions.%20To%20handle%20diverse%20element%20scales%2C%20we%20develop%20an%20adaptive%20variance%0Amechanism%20that%20calibrates%20reward%20distributions%20based%20on%20element%20dimensions.%0AThis%20framework%20transforms%20GUI%20grounding%20from%20sparse%20binary%20classification%20to%0Adense%20continuous%20optimization%2C%20where%20Gaussian%20distributions%20generate%20rich%0Agradient%20signals%20that%20guide%20models%20toward%20optimal%20interaction%20positions.%0AExtensive%20experiments%20across%20ScreenSpot%2C%20ScreenSpot-v2%2C%20and%20ScreenSpot-Pro%0Abenchmarks%20demonstrate%20that%20GUI-G%24%5E2%24%2C%20substantially%20outperforms%0Astate-of-the-art%20method%20UI-TARS-72B%2C%20with%20the%20most%20significant%20improvement%20of%0A24.7%25%20on%20ScreenSpot-Pro.%20Our%20analysis%20reveals%20that%20continuous%20modeling%20provides%0Asuperior%20robustness%20to%20interface%20variations%20and%20enhanced%20generalization%20to%0Aunseen%20layouts%2C%20establishing%20a%20new%20paradigm%20for%20spatial%20reasoning%20in%20GUI%0Ainteraction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15846v2&entry.124074799=Read"},
{"title": "A Partitioned Sparse Variational Gaussian Process for Fast, Distributed\n  Spatial Modeling", "author": "Michael Grosskopf and Kellin Rumsey and Ayan Biswas and Earl Lawrence", "abstract": "  The next generation of Department of Energy supercomputers will be capable of\nexascale computation. For these machines, far more computation will be possible\nthan that which can be saved to disk. As a result, users will be unable to rely\non post-hoc access to data for uncertainty quantification and other statistical\nanalyses and there will be an urgent need for sophisticated machine learning\nalgorithms which can be trained in situ. Algorithms deployed in this setting\nmust be highly scalable, memory efficient and capable of handling data which is\ndistributed across nodes as spatially contiguous partitions. One suitable\napproach involves fitting a sparse variational Gaussian process (SVGP) model\nindependently and in parallel to each spatial partition. The resulting model is\nscalable, efficient and generally accurate, but produces the undesirable effect\nof constructing discontinuous response surfaces due to the disagreement between\nneighboring models at their shared boundary. In this paper, we extend this idea\nby allowing for a small amount of communication between neighboring spatial\npartitions which encourages better alignment of the local models, leading to\nsmoother spatial predictions and a better fit in general. Due to our\ndecentralized communication scheme, the proposed extension remains highly\nscalable and adds very little overhead in terms of computation (and none, in\nterms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the\nEnergy Exascale Earth System Model (E3SM) and compare the results to the\nindependent SVGP case.\n", "link": "http://arxiv.org/abs/2507.16771v1", "date": "2025-07-22", "relevancy": 2.6991, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5535}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5363}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5297}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Partitioned%20Sparse%20Variational%20Gaussian%20Process%20for%20Fast%2C%20Distributed%0A%20%20Spatial%20Modeling&body=Title%3A%20A%20Partitioned%20Sparse%20Variational%20Gaussian%20Process%20for%20Fast%2C%20Distributed%0A%20%20Spatial%20Modeling%0AAuthor%3A%20Michael%20Grosskopf%20and%20Kellin%20Rumsey%20and%20Ayan%20Biswas%20and%20Earl%20Lawrence%0AAbstract%3A%20%20%20The%20next%20generation%20of%20Department%20of%20Energy%20supercomputers%20will%20be%20capable%20of%0Aexascale%20computation.%20For%20these%20machines%2C%20far%20more%20computation%20will%20be%20possible%0Athan%20that%20which%20can%20be%20saved%20to%20disk.%20As%20a%20result%2C%20users%20will%20be%20unable%20to%20rely%0Aon%20post-hoc%20access%20to%20data%20for%20uncertainty%20quantification%20and%20other%20statistical%0Aanalyses%20and%20there%20will%20be%20an%20urgent%20need%20for%20sophisticated%20machine%20learning%0Aalgorithms%20which%20can%20be%20trained%20in%20situ.%20Algorithms%20deployed%20in%20this%20setting%0Amust%20be%20highly%20scalable%2C%20memory%20efficient%20and%20capable%20of%20handling%20data%20which%20is%0Adistributed%20across%20nodes%20as%20spatially%20contiguous%20partitions.%20One%20suitable%0Aapproach%20involves%20fitting%20a%20sparse%20variational%20Gaussian%20process%20%28SVGP%29%20model%0Aindependently%20and%20in%20parallel%20to%20each%20spatial%20partition.%20The%20resulting%20model%20is%0Ascalable%2C%20efficient%20and%20generally%20accurate%2C%20but%20produces%20the%20undesirable%20effect%0Aof%20constructing%20discontinuous%20response%20surfaces%20due%20to%20the%20disagreement%20between%0Aneighboring%20models%20at%20their%20shared%20boundary.%20In%20this%20paper%2C%20we%20extend%20this%20idea%0Aby%20allowing%20for%20a%20small%20amount%20of%20communication%20between%20neighboring%20spatial%0Apartitions%20which%20encourages%20better%20alignment%20of%20the%20local%20models%2C%20leading%20to%0Asmoother%20spatial%20predictions%20and%20a%20better%20fit%20in%20general.%20Due%20to%20our%0Adecentralized%20communication%20scheme%2C%20the%20proposed%20extension%20remains%20highly%0Ascalable%20and%20adds%20very%20little%20overhead%20in%20terms%20of%20computation%20%28and%20none%2C%20in%0Aterms%20of%20memory%29.%20We%20demonstrate%20this%20Partitioned%20SVGP%20%28PSVGP%29%20approach%20for%20the%0AEnergy%20Exascale%20Earth%20System%20Model%20%28E3SM%29%20and%20compare%20the%20results%20to%20the%0Aindependent%20SVGP%20case.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Partitioned%2520Sparse%2520Variational%2520Gaussian%2520Process%2520for%2520Fast%252C%2520Distributed%250A%2520%2520Spatial%2520Modeling%26entry.906535625%3DMichael%2520Grosskopf%2520and%2520Kellin%2520Rumsey%2520and%2520Ayan%2520Biswas%2520and%2520Earl%2520Lawrence%26entry.1292438233%3D%2520%2520The%2520next%2520generation%2520of%2520Department%2520of%2520Energy%2520supercomputers%2520will%2520be%2520capable%2520of%250Aexascale%2520computation.%2520For%2520these%2520machines%252C%2520far%2520more%2520computation%2520will%2520be%2520possible%250Athan%2520that%2520which%2520can%2520be%2520saved%2520to%2520disk.%2520As%2520a%2520result%252C%2520users%2520will%2520be%2520unable%2520to%2520rely%250Aon%2520post-hoc%2520access%2520to%2520data%2520for%2520uncertainty%2520quantification%2520and%2520other%2520statistical%250Aanalyses%2520and%2520there%2520will%2520be%2520an%2520urgent%2520need%2520for%2520sophisticated%2520machine%2520learning%250Aalgorithms%2520which%2520can%2520be%2520trained%2520in%2520situ.%2520Algorithms%2520deployed%2520in%2520this%2520setting%250Amust%2520be%2520highly%2520scalable%252C%2520memory%2520efficient%2520and%2520capable%2520of%2520handling%2520data%2520which%2520is%250Adistributed%2520across%2520nodes%2520as%2520spatially%2520contiguous%2520partitions.%2520One%2520suitable%250Aapproach%2520involves%2520fitting%2520a%2520sparse%2520variational%2520Gaussian%2520process%2520%2528SVGP%2529%2520model%250Aindependently%2520and%2520in%2520parallel%2520to%2520each%2520spatial%2520partition.%2520The%2520resulting%2520model%2520is%250Ascalable%252C%2520efficient%2520and%2520generally%2520accurate%252C%2520but%2520produces%2520the%2520undesirable%2520effect%250Aof%2520constructing%2520discontinuous%2520response%2520surfaces%2520due%2520to%2520the%2520disagreement%2520between%250Aneighboring%2520models%2520at%2520their%2520shared%2520boundary.%2520In%2520this%2520paper%252C%2520we%2520extend%2520this%2520idea%250Aby%2520allowing%2520for%2520a%2520small%2520amount%2520of%2520communication%2520between%2520neighboring%2520spatial%250Apartitions%2520which%2520encourages%2520better%2520alignment%2520of%2520the%2520local%2520models%252C%2520leading%2520to%250Asmoother%2520spatial%2520predictions%2520and%2520a%2520better%2520fit%2520in%2520general.%2520Due%2520to%2520our%250Adecentralized%2520communication%2520scheme%252C%2520the%2520proposed%2520extension%2520remains%2520highly%250Ascalable%2520and%2520adds%2520very%2520little%2520overhead%2520in%2520terms%2520of%2520computation%2520%2528and%2520none%252C%2520in%250Aterms%2520of%2520memory%2529.%2520We%2520demonstrate%2520this%2520Partitioned%2520SVGP%2520%2528PSVGP%2529%2520approach%2520for%2520the%250AEnergy%2520Exascale%2520Earth%2520System%2520Model%2520%2528E3SM%2529%2520and%2520compare%2520the%2520results%2520to%2520the%250Aindependent%2520SVGP%2520case.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Partitioned%20Sparse%20Variational%20Gaussian%20Process%20for%20Fast%2C%20Distributed%0A%20%20Spatial%20Modeling&entry.906535625=Michael%20Grosskopf%20and%20Kellin%20Rumsey%20and%20Ayan%20Biswas%20and%20Earl%20Lawrence&entry.1292438233=%20%20The%20next%20generation%20of%20Department%20of%20Energy%20supercomputers%20will%20be%20capable%20of%0Aexascale%20computation.%20For%20these%20machines%2C%20far%20more%20computation%20will%20be%20possible%0Athan%20that%20which%20can%20be%20saved%20to%20disk.%20As%20a%20result%2C%20users%20will%20be%20unable%20to%20rely%0Aon%20post-hoc%20access%20to%20data%20for%20uncertainty%20quantification%20and%20other%20statistical%0Aanalyses%20and%20there%20will%20be%20an%20urgent%20need%20for%20sophisticated%20machine%20learning%0Aalgorithms%20which%20can%20be%20trained%20in%20situ.%20Algorithms%20deployed%20in%20this%20setting%0Amust%20be%20highly%20scalable%2C%20memory%20efficient%20and%20capable%20of%20handling%20data%20which%20is%0Adistributed%20across%20nodes%20as%20spatially%20contiguous%20partitions.%20One%20suitable%0Aapproach%20involves%20fitting%20a%20sparse%20variational%20Gaussian%20process%20%28SVGP%29%20model%0Aindependently%20and%20in%20parallel%20to%20each%20spatial%20partition.%20The%20resulting%20model%20is%0Ascalable%2C%20efficient%20and%20generally%20accurate%2C%20but%20produces%20the%20undesirable%20effect%0Aof%20constructing%20discontinuous%20response%20surfaces%20due%20to%20the%20disagreement%20between%0Aneighboring%20models%20at%20their%20shared%20boundary.%20In%20this%20paper%2C%20we%20extend%20this%20idea%0Aby%20allowing%20for%20a%20small%20amount%20of%20communication%20between%20neighboring%20spatial%0Apartitions%20which%20encourages%20better%20alignment%20of%20the%20local%20models%2C%20leading%20to%0Asmoother%20spatial%20predictions%20and%20a%20better%20fit%20in%20general.%20Due%20to%20our%0Adecentralized%20communication%20scheme%2C%20the%20proposed%20extension%20remains%20highly%0Ascalable%20and%20adds%20very%20little%20overhead%20in%20terms%20of%20computation%20%28and%20none%2C%20in%0Aterms%20of%20memory%29.%20We%20demonstrate%20this%20Partitioned%20SVGP%20%28PSVGP%29%20approach%20for%20the%0AEnergy%20Exascale%20Earth%20System%20Model%20%28E3SM%29%20and%20compare%20the%20results%20to%20the%0Aindependent%20SVGP%20case.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16771v1&entry.124074799=Read"},
{"title": "Task-Specific Zero-shot Quantization-Aware Training for Object Detection", "author": "Changhao Li and Xinrui Chen and Ji Wang and Kang Zhao and Jianfei Chen", "abstract": "  Quantization is a key technique to reduce network size and computational\ncomplexity by representing the network parameters with a lower precision.\nTraditional quantization methods rely on access to original training data,\nwhich is often restricted due to privacy concerns or security challenges.\nZero-shot Quantization (ZSQ) addresses this by using synthetic data generated\nfrom pre-trained models, eliminating the need for real training data. Recently,\nZSQ has been extended to object detection. However, existing methods use\nunlabeled task-agnostic synthetic images that lack the specific information\nrequired for object detection, leading to suboptimal performance. In this\npaper, we propose a novel task-specific ZSQ framework for object detection\nnetworks, which consists of two main stages. First, we introduce a bounding box\nand category sampling strategy to synthesize a task-specific calibration set\nfrom the pre-trained network, reconstructing object locations, sizes, and\ncategory distributions without any prior knowledge. Second, we integrate\ntask-specific training into the knowledge distillation process to restore the\nperformance of quantized detection networks. Extensive experiments conducted on\nthe MS-COCO and Pascal VOC datasets demonstrate the efficiency and\nstate-of-the-art performance of our method. Our code is publicly available at:\nhttps://github.com/DFQ-Dojo/dfq-toolkit .\n", "link": "http://arxiv.org/abs/2507.16782v1", "date": "2025-07-22", "relevancy": 2.6708, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.545}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5366}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Specific%20Zero-shot%20Quantization-Aware%20Training%20for%20Object%20Detection&body=Title%3A%20Task-Specific%20Zero-shot%20Quantization-Aware%20Training%20for%20Object%20Detection%0AAuthor%3A%20Changhao%20Li%20and%20Xinrui%20Chen%20and%20Ji%20Wang%20and%20Kang%20Zhao%20and%20Jianfei%20Chen%0AAbstract%3A%20%20%20Quantization%20is%20a%20key%20technique%20to%20reduce%20network%20size%20and%20computational%0Acomplexity%20by%20representing%20the%20network%20parameters%20with%20a%20lower%20precision.%0ATraditional%20quantization%20methods%20rely%20on%20access%20to%20original%20training%20data%2C%0Awhich%20is%20often%20restricted%20due%20to%20privacy%20concerns%20or%20security%20challenges.%0AZero-shot%20Quantization%20%28ZSQ%29%20addresses%20this%20by%20using%20synthetic%20data%20generated%0Afrom%20pre-trained%20models%2C%20eliminating%20the%20need%20for%20real%20training%20data.%20Recently%2C%0AZSQ%20has%20been%20extended%20to%20object%20detection.%20However%2C%20existing%20methods%20use%0Aunlabeled%20task-agnostic%20synthetic%20images%20that%20lack%20the%20specific%20information%0Arequired%20for%20object%20detection%2C%20leading%20to%20suboptimal%20performance.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20task-specific%20ZSQ%20framework%20for%20object%20detection%0Anetworks%2C%20which%20consists%20of%20two%20main%20stages.%20First%2C%20we%20introduce%20a%20bounding%20box%0Aand%20category%20sampling%20strategy%20to%20synthesize%20a%20task-specific%20calibration%20set%0Afrom%20the%20pre-trained%20network%2C%20reconstructing%20object%20locations%2C%20sizes%2C%20and%0Acategory%20distributions%20without%20any%20prior%20knowledge.%20Second%2C%20we%20integrate%0Atask-specific%20training%20into%20the%20knowledge%20distillation%20process%20to%20restore%20the%0Aperformance%20of%20quantized%20detection%20networks.%20Extensive%20experiments%20conducted%20on%0Athe%20MS-COCO%20and%20Pascal%20VOC%20datasets%20demonstrate%20the%20efficiency%20and%0Astate-of-the-art%20performance%20of%20our%20method.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/DFQ-Dojo/dfq-toolkit%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Specific%2520Zero-shot%2520Quantization-Aware%2520Training%2520for%2520Object%2520Detection%26entry.906535625%3DChanghao%2520Li%2520and%2520Xinrui%2520Chen%2520and%2520Ji%2520Wang%2520and%2520Kang%2520Zhao%2520and%2520Jianfei%2520Chen%26entry.1292438233%3D%2520%2520Quantization%2520is%2520a%2520key%2520technique%2520to%2520reduce%2520network%2520size%2520and%2520computational%250Acomplexity%2520by%2520representing%2520the%2520network%2520parameters%2520with%2520a%2520lower%2520precision.%250ATraditional%2520quantization%2520methods%2520rely%2520on%2520access%2520to%2520original%2520training%2520data%252C%250Awhich%2520is%2520often%2520restricted%2520due%2520to%2520privacy%2520concerns%2520or%2520security%2520challenges.%250AZero-shot%2520Quantization%2520%2528ZSQ%2529%2520addresses%2520this%2520by%2520using%2520synthetic%2520data%2520generated%250Afrom%2520pre-trained%2520models%252C%2520eliminating%2520the%2520need%2520for%2520real%2520training%2520data.%2520Recently%252C%250AZSQ%2520has%2520been%2520extended%2520to%2520object%2520detection.%2520However%252C%2520existing%2520methods%2520use%250Aunlabeled%2520task-agnostic%2520synthetic%2520images%2520that%2520lack%2520the%2520specific%2520information%250Arequired%2520for%2520object%2520detection%252C%2520leading%2520to%2520suboptimal%2520performance.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520task-specific%2520ZSQ%2520framework%2520for%2520object%2520detection%250Anetworks%252C%2520which%2520consists%2520of%2520two%2520main%2520stages.%2520First%252C%2520we%2520introduce%2520a%2520bounding%2520box%250Aand%2520category%2520sampling%2520strategy%2520to%2520synthesize%2520a%2520task-specific%2520calibration%2520set%250Afrom%2520the%2520pre-trained%2520network%252C%2520reconstructing%2520object%2520locations%252C%2520sizes%252C%2520and%250Acategory%2520distributions%2520without%2520any%2520prior%2520knowledge.%2520Second%252C%2520we%2520integrate%250Atask-specific%2520training%2520into%2520the%2520knowledge%2520distillation%2520process%2520to%2520restore%2520the%250Aperformance%2520of%2520quantized%2520detection%2520networks.%2520Extensive%2520experiments%2520conducted%2520on%250Athe%2520MS-COCO%2520and%2520Pascal%2520VOC%2520datasets%2520demonstrate%2520the%2520efficiency%2520and%250Astate-of-the-art%2520performance%2520of%2520our%2520method.%2520Our%2520code%2520is%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/DFQ-Dojo/dfq-toolkit%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Specific%20Zero-shot%20Quantization-Aware%20Training%20for%20Object%20Detection&entry.906535625=Changhao%20Li%20and%20Xinrui%20Chen%20and%20Ji%20Wang%20and%20Kang%20Zhao%20and%20Jianfei%20Chen&entry.1292438233=%20%20Quantization%20is%20a%20key%20technique%20to%20reduce%20network%20size%20and%20computational%0Acomplexity%20by%20representing%20the%20network%20parameters%20with%20a%20lower%20precision.%0ATraditional%20quantization%20methods%20rely%20on%20access%20to%20original%20training%20data%2C%0Awhich%20is%20often%20restricted%20due%20to%20privacy%20concerns%20or%20security%20challenges.%0AZero-shot%20Quantization%20%28ZSQ%29%20addresses%20this%20by%20using%20synthetic%20data%20generated%0Afrom%20pre-trained%20models%2C%20eliminating%20the%20need%20for%20real%20training%20data.%20Recently%2C%0AZSQ%20has%20been%20extended%20to%20object%20detection.%20However%2C%20existing%20methods%20use%0Aunlabeled%20task-agnostic%20synthetic%20images%20that%20lack%20the%20specific%20information%0Arequired%20for%20object%20detection%2C%20leading%20to%20suboptimal%20performance.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20task-specific%20ZSQ%20framework%20for%20object%20detection%0Anetworks%2C%20which%20consists%20of%20two%20main%20stages.%20First%2C%20we%20introduce%20a%20bounding%20box%0Aand%20category%20sampling%20strategy%20to%20synthesize%20a%20task-specific%20calibration%20set%0Afrom%20the%20pre-trained%20network%2C%20reconstructing%20object%20locations%2C%20sizes%2C%20and%0Acategory%20distributions%20without%20any%20prior%20knowledge.%20Second%2C%20we%20integrate%0Atask-specific%20training%20into%20the%20knowledge%20distillation%20process%20to%20restore%20the%0Aperformance%20of%20quantized%20detection%20networks.%20Extensive%20experiments%20conducted%20on%0Athe%20MS-COCO%20and%20Pascal%20VOC%20datasets%20demonstrate%20the%20efficiency%20and%0Astate-of-the-art%20performance%20of%20our%20method.%20Our%20code%20is%20publicly%20available%20at%3A%0Ahttps%3A//github.com/DFQ-Dojo/dfq-toolkit%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16782v1&entry.124074799=Read"},
{"title": "Online Episodic Memory Visual Query Localization with Egocentric\n  Streaming Object Memory", "author": "Zaira Manigrasso and Matteo Dunnhofer and Antonino Furnari and Moritz Nottebaum and Antonio Finocchiaro and Davide Marana and Rosario Forte and Giovanni Maria Farinella and Christian Micheloni", "abstract": "  Episodic memory retrieval enables wearable cameras to recall objects or\nevents previously observed in video. However, existing formulations assume an\n\"offline\" setting with full video access at query time, limiting their\napplicability in real-world scenarios with power and storage-constrained\nwearable devices. Towards more application-ready episodic memory systems, we\nintroduce Online Visual Query 2D (OVQ2D), a task where models process video\nstreams online, observing each frame only once, and retrieve object\nlocalizations using a compact memory instead of full video history. We address\nOVQ2D with ESOM (Egocentric Streaming Object Memory), a novel framework\nintegrating an object discovery module, an object tracking module, and a memory\nmodule that find, track, and store spatio-temporal object information for\nefficient querying. Experiments on Ego4D demonstrate ESOM's superiority over\nother online approaches, though OVQ2D remains challenging, with top performance\nat only ~4% success. ESOM's accuracy increases markedly with perfect object\ntracking (31.91%), discovery (40.55%), or both (81.92%), underscoring the need\nof applied research on these components.\n", "link": "http://arxiv.org/abs/2411.16934v2", "date": "2025-07-22", "relevancy": 2.6646, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Episodic%20Memory%20Visual%20Query%20Localization%20with%20Egocentric%0A%20%20Streaming%20Object%20Memory&body=Title%3A%20Online%20Episodic%20Memory%20Visual%20Query%20Localization%20with%20Egocentric%0A%20%20Streaming%20Object%20Memory%0AAuthor%3A%20Zaira%20Manigrasso%20and%20Matteo%20Dunnhofer%20and%20Antonino%20Furnari%20and%20Moritz%20Nottebaum%20and%20Antonio%20Finocchiaro%20and%20Davide%20Marana%20and%20Rosario%20Forte%20and%20Giovanni%20Maria%20Farinella%20and%20Christian%20Micheloni%0AAbstract%3A%20%20%20Episodic%20memory%20retrieval%20enables%20wearable%20cameras%20to%20recall%20objects%20or%0Aevents%20previously%20observed%20in%20video.%20However%2C%20existing%20formulations%20assume%20an%0A%22offline%22%20setting%20with%20full%20video%20access%20at%20query%20time%2C%20limiting%20their%0Aapplicability%20in%20real-world%20scenarios%20with%20power%20and%20storage-constrained%0Awearable%20devices.%20Towards%20more%20application-ready%20episodic%20memory%20systems%2C%20we%0Aintroduce%20Online%20Visual%20Query%202D%20%28OVQ2D%29%2C%20a%20task%20where%20models%20process%20video%0Astreams%20online%2C%20observing%20each%20frame%20only%20once%2C%20and%20retrieve%20object%0Alocalizations%20using%20a%20compact%20memory%20instead%20of%20full%20video%20history.%20We%20address%0AOVQ2D%20with%20ESOM%20%28Egocentric%20Streaming%20Object%20Memory%29%2C%20a%20novel%20framework%0Aintegrating%20an%20object%20discovery%20module%2C%20an%20object%20tracking%20module%2C%20and%20a%20memory%0Amodule%20that%20find%2C%20track%2C%20and%20store%20spatio-temporal%20object%20information%20for%0Aefficient%20querying.%20Experiments%20on%20Ego4D%20demonstrate%20ESOM%27s%20superiority%20over%0Aother%20online%20approaches%2C%20though%20OVQ2D%20remains%20challenging%2C%20with%20top%20performance%0Aat%20only%20~4%25%20success.%20ESOM%27s%20accuracy%20increases%20markedly%20with%20perfect%20object%0Atracking%20%2831.91%25%29%2C%20discovery%20%2840.55%25%29%2C%20or%20both%20%2881.92%25%29%2C%20underscoring%20the%20need%0Aof%20applied%20research%20on%20these%20components.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.16934v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Episodic%2520Memory%2520Visual%2520Query%2520Localization%2520with%2520Egocentric%250A%2520%2520Streaming%2520Object%2520Memory%26entry.906535625%3DZaira%2520Manigrasso%2520and%2520Matteo%2520Dunnhofer%2520and%2520Antonino%2520Furnari%2520and%2520Moritz%2520Nottebaum%2520and%2520Antonio%2520Finocchiaro%2520and%2520Davide%2520Marana%2520and%2520Rosario%2520Forte%2520and%2520Giovanni%2520Maria%2520Farinella%2520and%2520Christian%2520Micheloni%26entry.1292438233%3D%2520%2520Episodic%2520memory%2520retrieval%2520enables%2520wearable%2520cameras%2520to%2520recall%2520objects%2520or%250Aevents%2520previously%2520observed%2520in%2520video.%2520However%252C%2520existing%2520formulations%2520assume%2520an%250A%2522offline%2522%2520setting%2520with%2520full%2520video%2520access%2520at%2520query%2520time%252C%2520limiting%2520their%250Aapplicability%2520in%2520real-world%2520scenarios%2520with%2520power%2520and%2520storage-constrained%250Awearable%2520devices.%2520Towards%2520more%2520application-ready%2520episodic%2520memory%2520systems%252C%2520we%250Aintroduce%2520Online%2520Visual%2520Query%25202D%2520%2528OVQ2D%2529%252C%2520a%2520task%2520where%2520models%2520process%2520video%250Astreams%2520online%252C%2520observing%2520each%2520frame%2520only%2520once%252C%2520and%2520retrieve%2520object%250Alocalizations%2520using%2520a%2520compact%2520memory%2520instead%2520of%2520full%2520video%2520history.%2520We%2520address%250AOVQ2D%2520with%2520ESOM%2520%2528Egocentric%2520Streaming%2520Object%2520Memory%2529%252C%2520a%2520novel%2520framework%250Aintegrating%2520an%2520object%2520discovery%2520module%252C%2520an%2520object%2520tracking%2520module%252C%2520and%2520a%2520memory%250Amodule%2520that%2520find%252C%2520track%252C%2520and%2520store%2520spatio-temporal%2520object%2520information%2520for%250Aefficient%2520querying.%2520Experiments%2520on%2520Ego4D%2520demonstrate%2520ESOM%2527s%2520superiority%2520over%250Aother%2520online%2520approaches%252C%2520though%2520OVQ2D%2520remains%2520challenging%252C%2520with%2520top%2520performance%250Aat%2520only%2520~4%2525%2520success.%2520ESOM%2527s%2520accuracy%2520increases%2520markedly%2520with%2520perfect%2520object%250Atracking%2520%252831.91%2525%2529%252C%2520discovery%2520%252840.55%2525%2529%252C%2520or%2520both%2520%252881.92%2525%2529%252C%2520underscoring%2520the%2520need%250Aof%2520applied%2520research%2520on%2520these%2520components.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16934v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Episodic%20Memory%20Visual%20Query%20Localization%20with%20Egocentric%0A%20%20Streaming%20Object%20Memory&entry.906535625=Zaira%20Manigrasso%20and%20Matteo%20Dunnhofer%20and%20Antonino%20Furnari%20and%20Moritz%20Nottebaum%20and%20Antonio%20Finocchiaro%20and%20Davide%20Marana%20and%20Rosario%20Forte%20and%20Giovanni%20Maria%20Farinella%20and%20Christian%20Micheloni&entry.1292438233=%20%20Episodic%20memory%20retrieval%20enables%20wearable%20cameras%20to%20recall%20objects%20or%0Aevents%20previously%20observed%20in%20video.%20However%2C%20existing%20formulations%20assume%20an%0A%22offline%22%20setting%20with%20full%20video%20access%20at%20query%20time%2C%20limiting%20their%0Aapplicability%20in%20real-world%20scenarios%20with%20power%20and%20storage-constrained%0Awearable%20devices.%20Towards%20more%20application-ready%20episodic%20memory%20systems%2C%20we%0Aintroduce%20Online%20Visual%20Query%202D%20%28OVQ2D%29%2C%20a%20task%20where%20models%20process%20video%0Astreams%20online%2C%20observing%20each%20frame%20only%20once%2C%20and%20retrieve%20object%0Alocalizations%20using%20a%20compact%20memory%20instead%20of%20full%20video%20history.%20We%20address%0AOVQ2D%20with%20ESOM%20%28Egocentric%20Streaming%20Object%20Memory%29%2C%20a%20novel%20framework%0Aintegrating%20an%20object%20discovery%20module%2C%20an%20object%20tracking%20module%2C%20and%20a%20memory%0Amodule%20that%20find%2C%20track%2C%20and%20store%20spatio-temporal%20object%20information%20for%0Aefficient%20querying.%20Experiments%20on%20Ego4D%20demonstrate%20ESOM%27s%20superiority%20over%0Aother%20online%20approaches%2C%20though%20OVQ2D%20remains%20challenging%2C%20with%20top%20performance%0Aat%20only%20~4%25%20success.%20ESOM%27s%20accuracy%20increases%20markedly%20with%20perfect%20object%0Atracking%20%2831.91%25%29%2C%20discovery%20%2840.55%25%29%2C%20or%20both%20%2881.92%25%29%2C%20underscoring%20the%20need%0Aof%20applied%20research%20on%20these%20components.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.16934v2&entry.124074799=Read"},
{"title": "VGGT-Long: Chunk it, Loop it, Align it -- Pushing VGGT's Limits on\n  Kilometer-scale Long RGB Sequences", "author": "Kai Deng and Zexin Ti and Jiawei Xu and Jian Yang and Jin Xie", "abstract": "  Foundation models for 3D vision have recently demonstrated remarkable\ncapabilities in 3D perception. However, extending these models to large-scale\nRGB stream 3D reconstruction remains challenging due to memory limitations. In\nthis work, we propose VGGT-Long, a simple yet effective system that pushes the\nlimits of monocular 3D reconstruction to kilometer-scale, unbounded outdoor\nenvironments. Our approach addresses the scalability bottlenecks of existing\nmodels through a chunk-based processing strategy combined with overlapping\nalignment and lightweight loop closure optimization. Without requiring camera\ncalibration, depth supervision or model retraining, VGGT-Long achieves\ntrajectory and reconstruction performance comparable to traditional methods. We\nevaluate our method on KITTI, Waymo, and Virtual KITTI datasets. VGGT-Long not\nonly runs successfully on long RGB sequences where foundation models typically\nfail, but also produces accurate and consistent geometry across various\nconditions. Our results highlight the potential of leveraging foundation models\nfor scalable monocular 3D scene in real-world settings, especially for\nautonomous driving scenarios. Code is available at\nhttps://github.com/DengKaiCQ/VGGT-Long.\n", "link": "http://arxiv.org/abs/2507.16443v1", "date": "2025-07-22", "relevancy": 2.5833, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6964}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6099}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VGGT-Long%3A%20Chunk%20it%2C%20Loop%20it%2C%20Align%20it%20--%20Pushing%20VGGT%27s%20Limits%20on%0A%20%20Kilometer-scale%20Long%20RGB%20Sequences&body=Title%3A%20VGGT-Long%3A%20Chunk%20it%2C%20Loop%20it%2C%20Align%20it%20--%20Pushing%20VGGT%27s%20Limits%20on%0A%20%20Kilometer-scale%20Long%20RGB%20Sequences%0AAuthor%3A%20Kai%20Deng%20and%20Zexin%20Ti%20and%20Jiawei%20Xu%20and%20Jian%20Yang%20and%20Jin%20Xie%0AAbstract%3A%20%20%20Foundation%20models%20for%203D%20vision%20have%20recently%20demonstrated%20remarkable%0Acapabilities%20in%203D%20perception.%20However%2C%20extending%20these%20models%20to%20large-scale%0ARGB%20stream%203D%20reconstruction%20remains%20challenging%20due%20to%20memory%20limitations.%20In%0Athis%20work%2C%20we%20propose%20VGGT-Long%2C%20a%20simple%20yet%20effective%20system%20that%20pushes%20the%0Alimits%20of%20monocular%203D%20reconstruction%20to%20kilometer-scale%2C%20unbounded%20outdoor%0Aenvironments.%20Our%20approach%20addresses%20the%20scalability%20bottlenecks%20of%20existing%0Amodels%20through%20a%20chunk-based%20processing%20strategy%20combined%20with%20overlapping%0Aalignment%20and%20lightweight%20loop%20closure%20optimization.%20Without%20requiring%20camera%0Acalibration%2C%20depth%20supervision%20or%20model%20retraining%2C%20VGGT-Long%20achieves%0Atrajectory%20and%20reconstruction%20performance%20comparable%20to%20traditional%20methods.%20We%0Aevaluate%20our%20method%20on%20KITTI%2C%20Waymo%2C%20and%20Virtual%20KITTI%20datasets.%20VGGT-Long%20not%0Aonly%20runs%20successfully%20on%20long%20RGB%20sequences%20where%20foundation%20models%20typically%0Afail%2C%20but%20also%20produces%20accurate%20and%20consistent%20geometry%20across%20various%0Aconditions.%20Our%20results%20highlight%20the%20potential%20of%20leveraging%20foundation%20models%0Afor%20scalable%20monocular%203D%20scene%20in%20real-world%20settings%2C%20especially%20for%0Aautonomous%20driving%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/DengKaiCQ/VGGT-Long.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVGGT-Long%253A%2520Chunk%2520it%252C%2520Loop%2520it%252C%2520Align%2520it%2520--%2520Pushing%2520VGGT%2527s%2520Limits%2520on%250A%2520%2520Kilometer-scale%2520Long%2520RGB%2520Sequences%26entry.906535625%3DKai%2520Deng%2520and%2520Zexin%2520Ti%2520and%2520Jiawei%2520Xu%2520and%2520Jian%2520Yang%2520and%2520Jin%2520Xie%26entry.1292438233%3D%2520%2520Foundation%2520models%2520for%25203D%2520vision%2520have%2520recently%2520demonstrated%2520remarkable%250Acapabilities%2520in%25203D%2520perception.%2520However%252C%2520extending%2520these%2520models%2520to%2520large-scale%250ARGB%2520stream%25203D%2520reconstruction%2520remains%2520challenging%2520due%2520to%2520memory%2520limitations.%2520In%250Athis%2520work%252C%2520we%2520propose%2520VGGT-Long%252C%2520a%2520simple%2520yet%2520effective%2520system%2520that%2520pushes%2520the%250Alimits%2520of%2520monocular%25203D%2520reconstruction%2520to%2520kilometer-scale%252C%2520unbounded%2520outdoor%250Aenvironments.%2520Our%2520approach%2520addresses%2520the%2520scalability%2520bottlenecks%2520of%2520existing%250Amodels%2520through%2520a%2520chunk-based%2520processing%2520strategy%2520combined%2520with%2520overlapping%250Aalignment%2520and%2520lightweight%2520loop%2520closure%2520optimization.%2520Without%2520requiring%2520camera%250Acalibration%252C%2520depth%2520supervision%2520or%2520model%2520retraining%252C%2520VGGT-Long%2520achieves%250Atrajectory%2520and%2520reconstruction%2520performance%2520comparable%2520to%2520traditional%2520methods.%2520We%250Aevaluate%2520our%2520method%2520on%2520KITTI%252C%2520Waymo%252C%2520and%2520Virtual%2520KITTI%2520datasets.%2520VGGT-Long%2520not%250Aonly%2520runs%2520successfully%2520on%2520long%2520RGB%2520sequences%2520where%2520foundation%2520models%2520typically%250Afail%252C%2520but%2520also%2520produces%2520accurate%2520and%2520consistent%2520geometry%2520across%2520various%250Aconditions.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520leveraging%2520foundation%2520models%250Afor%2520scalable%2520monocular%25203D%2520scene%2520in%2520real-world%2520settings%252C%2520especially%2520for%250Aautonomous%2520driving%2520scenarios.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/DengKaiCQ/VGGT-Long.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGGT-Long%3A%20Chunk%20it%2C%20Loop%20it%2C%20Align%20it%20--%20Pushing%20VGGT%27s%20Limits%20on%0A%20%20Kilometer-scale%20Long%20RGB%20Sequences&entry.906535625=Kai%20Deng%20and%20Zexin%20Ti%20and%20Jiawei%20Xu%20and%20Jian%20Yang%20and%20Jin%20Xie&entry.1292438233=%20%20Foundation%20models%20for%203D%20vision%20have%20recently%20demonstrated%20remarkable%0Acapabilities%20in%203D%20perception.%20However%2C%20extending%20these%20models%20to%20large-scale%0ARGB%20stream%203D%20reconstruction%20remains%20challenging%20due%20to%20memory%20limitations.%20In%0Athis%20work%2C%20we%20propose%20VGGT-Long%2C%20a%20simple%20yet%20effective%20system%20that%20pushes%20the%0Alimits%20of%20monocular%203D%20reconstruction%20to%20kilometer-scale%2C%20unbounded%20outdoor%0Aenvironments.%20Our%20approach%20addresses%20the%20scalability%20bottlenecks%20of%20existing%0Amodels%20through%20a%20chunk-based%20processing%20strategy%20combined%20with%20overlapping%0Aalignment%20and%20lightweight%20loop%20closure%20optimization.%20Without%20requiring%20camera%0Acalibration%2C%20depth%20supervision%20or%20model%20retraining%2C%20VGGT-Long%20achieves%0Atrajectory%20and%20reconstruction%20performance%20comparable%20to%20traditional%20methods.%20We%0Aevaluate%20our%20method%20on%20KITTI%2C%20Waymo%2C%20and%20Virtual%20KITTI%20datasets.%20VGGT-Long%20not%0Aonly%20runs%20successfully%20on%20long%20RGB%20sequences%20where%20foundation%20models%20typically%0Afail%2C%20but%20also%20produces%20accurate%20and%20consistent%20geometry%20across%20various%0Aconditions.%20Our%20results%20highlight%20the%20potential%20of%20leveraging%20foundation%20models%0Afor%20scalable%20monocular%203D%20scene%20in%20real-world%20settings%2C%20especially%20for%0Aautonomous%20driving%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/DengKaiCQ/VGGT-Long.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16443v1&entry.124074799=Read"},
{"title": "Rethinking Data Input for Point Cloud Upsampling", "author": "Tongxu Zhang", "abstract": "  Point cloud upsampling is crucial for tasks like 3D reconstruction. While\nexisting methods rely on patch-based inputs, and there is no research\ndiscussing the differences and principles between point cloud model full input\nand patch based input. Ergo, we propose a novel approach using whole model\ninputs i.e. Average Segment input. Our experiments on PU1K and ABC datasets\nreveal that patch-based inputs consistently outperform whole model inputs. To\nunderstand this, we will delve into factors in feature extraction, and network\narchitecture that influence upsampling results.\n", "link": "http://arxiv.org/abs/2407.04476v3", "date": "2025-07-22", "relevancy": 2.5795, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5281}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5145}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Data%20Input%20for%20Point%20Cloud%20Upsampling&body=Title%3A%20Rethinking%20Data%20Input%20for%20Point%20Cloud%20Upsampling%0AAuthor%3A%20Tongxu%20Zhang%0AAbstract%3A%20%20%20Point%20cloud%20upsampling%20is%20crucial%20for%20tasks%20like%203D%20reconstruction.%20While%0Aexisting%20methods%20rely%20on%20patch-based%20inputs%2C%20and%20there%20is%20no%20research%0Adiscussing%20the%20differences%20and%20principles%20between%20point%20cloud%20model%20full%20input%0Aand%20patch%20based%20input.%20Ergo%2C%20we%20propose%20a%20novel%20approach%20using%20whole%20model%0Ainputs%20i.e.%20Average%20Segment%20input.%20Our%20experiments%20on%20PU1K%20and%20ABC%20datasets%0Areveal%20that%20patch-based%20inputs%20consistently%20outperform%20whole%20model%20inputs.%20To%0Aunderstand%20this%2C%20we%20will%20delve%20into%20factors%20in%20feature%20extraction%2C%20and%20network%0Aarchitecture%20that%20influence%20upsampling%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04476v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Data%2520Input%2520for%2520Point%2520Cloud%2520Upsampling%26entry.906535625%3DTongxu%2520Zhang%26entry.1292438233%3D%2520%2520Point%2520cloud%2520upsampling%2520is%2520crucial%2520for%2520tasks%2520like%25203D%2520reconstruction.%2520While%250Aexisting%2520methods%2520rely%2520on%2520patch-based%2520inputs%252C%2520and%2520there%2520is%2520no%2520research%250Adiscussing%2520the%2520differences%2520and%2520principles%2520between%2520point%2520cloud%2520model%2520full%2520input%250Aand%2520patch%2520based%2520input.%2520Ergo%252C%2520we%2520propose%2520a%2520novel%2520approach%2520using%2520whole%2520model%250Ainputs%2520i.e.%2520Average%2520Segment%2520input.%2520Our%2520experiments%2520on%2520PU1K%2520and%2520ABC%2520datasets%250Areveal%2520that%2520patch-based%2520inputs%2520consistently%2520outperform%2520whole%2520model%2520inputs.%2520To%250Aunderstand%2520this%252C%2520we%2520will%2520delve%2520into%2520factors%2520in%2520feature%2520extraction%252C%2520and%2520network%250Aarchitecture%2520that%2520influence%2520upsampling%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04476v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Data%20Input%20for%20Point%20Cloud%20Upsampling&entry.906535625=Tongxu%20Zhang&entry.1292438233=%20%20Point%20cloud%20upsampling%20is%20crucial%20for%20tasks%20like%203D%20reconstruction.%20While%0Aexisting%20methods%20rely%20on%20patch-based%20inputs%2C%20and%20there%20is%20no%20research%0Adiscussing%20the%20differences%20and%20principles%20between%20point%20cloud%20model%20full%20input%0Aand%20patch%20based%20input.%20Ergo%2C%20we%20propose%20a%20novel%20approach%20using%20whole%20model%0Ainputs%20i.e.%20Average%20Segment%20input.%20Our%20experiments%20on%20PU1K%20and%20ABC%20datasets%0Areveal%20that%20patch-based%20inputs%20consistently%20outperform%20whole%20model%20inputs.%20To%0Aunderstand%20this%2C%20we%20will%20delve%20into%20factors%20in%20feature%20extraction%2C%20and%20network%0Aarchitecture%20that%20influence%20upsampling%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04476v3&entry.124074799=Read"},
{"title": "Graph Neural Networks Gone Hogwild", "author": "Olga Solodova and Nick Richardson and Deniz Oktay and Ryan P. Adams", "abstract": "  Graph neural networks (GNNs) appear to be powerful tools to learn state\nrepresentations for agents in distributed, decentralized multi-agent systems,\nbut generate catastrophically incorrect predictions when nodes update\nasynchronously during inference. This failure under asynchrony effectively\nexcludes these architectures from many potential applications where synchrony\nis difficult or impossible to enforce, e.g., robotic swarms or sensor networks.\nIn this work we identify \"implicitly-defined\" GNNs as a class of architectures\nwhich is provably robust to asynchronous \"hogwild\" inference, adapting\nconvergence guarantees from work in asynchronous and distributed optimization.\nWe then propose a novel implicitly-defined GNN architecture, which we call an\n'energy GNN'. We show that this architecture outperforms other GNNs from this\nclass on a variety of synthetic tasks inspired by multi-agent systems.\n", "link": "http://arxiv.org/abs/2407.00494v2", "date": "2025-07-22", "relevancy": 2.5562, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5228}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5161}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Networks%20Gone%20Hogwild&body=Title%3A%20Graph%20Neural%20Networks%20Gone%20Hogwild%0AAuthor%3A%20Olga%20Solodova%20and%20Nick%20Richardson%20and%20Deniz%20Oktay%20and%20Ryan%20P.%20Adams%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20appear%20to%20be%20powerful%20tools%20to%20learn%20state%0Arepresentations%20for%20agents%20in%20distributed%2C%20decentralized%20multi-agent%20systems%2C%0Abut%20generate%20catastrophically%20incorrect%20predictions%20when%20nodes%20update%0Aasynchronously%20during%20inference.%20This%20failure%20under%20asynchrony%20effectively%0Aexcludes%20these%20architectures%20from%20many%20potential%20applications%20where%20synchrony%0Ais%20difficult%20or%20impossible%20to%20enforce%2C%20e.g.%2C%20robotic%20swarms%20or%20sensor%20networks.%0AIn%20this%20work%20we%20identify%20%22implicitly-defined%22%20GNNs%20as%20a%20class%20of%20architectures%0Awhich%20is%20provably%20robust%20to%20asynchronous%20%22hogwild%22%20inference%2C%20adapting%0Aconvergence%20guarantees%20from%20work%20in%20asynchronous%20and%20distributed%20optimization.%0AWe%20then%20propose%20a%20novel%20implicitly-defined%20GNN%20architecture%2C%20which%20we%20call%20an%0A%27energy%20GNN%27.%20We%20show%20that%20this%20architecture%20outperforms%20other%20GNNs%20from%20this%0Aclass%20on%20a%20variety%20of%20synthetic%20tasks%20inspired%20by%20multi-agent%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00494v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Networks%2520Gone%2520Hogwild%26entry.906535625%3DOlga%2520Solodova%2520and%2520Nick%2520Richardson%2520and%2520Deniz%2520Oktay%2520and%2520Ryan%2520P.%2520Adams%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520appear%2520to%2520be%2520powerful%2520tools%2520to%2520learn%2520state%250Arepresentations%2520for%2520agents%2520in%2520distributed%252C%2520decentralized%2520multi-agent%2520systems%252C%250Abut%2520generate%2520catastrophically%2520incorrect%2520predictions%2520when%2520nodes%2520update%250Aasynchronously%2520during%2520inference.%2520This%2520failure%2520under%2520asynchrony%2520effectively%250Aexcludes%2520these%2520architectures%2520from%2520many%2520potential%2520applications%2520where%2520synchrony%250Ais%2520difficult%2520or%2520impossible%2520to%2520enforce%252C%2520e.g.%252C%2520robotic%2520swarms%2520or%2520sensor%2520networks.%250AIn%2520this%2520work%2520we%2520identify%2520%2522implicitly-defined%2522%2520GNNs%2520as%2520a%2520class%2520of%2520architectures%250Awhich%2520is%2520provably%2520robust%2520to%2520asynchronous%2520%2522hogwild%2522%2520inference%252C%2520adapting%250Aconvergence%2520guarantees%2520from%2520work%2520in%2520asynchronous%2520and%2520distributed%2520optimization.%250AWe%2520then%2520propose%2520a%2520novel%2520implicitly-defined%2520GNN%2520architecture%252C%2520which%2520we%2520call%2520an%250A%2527energy%2520GNN%2527.%2520We%2520show%2520that%2520this%2520architecture%2520outperforms%2520other%2520GNNs%2520from%2520this%250Aclass%2520on%2520a%2520variety%2520of%2520synthetic%2520tasks%2520inspired%2520by%2520multi-agent%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00494v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Networks%20Gone%20Hogwild&entry.906535625=Olga%20Solodova%20and%20Nick%20Richardson%20and%20Deniz%20Oktay%20and%20Ryan%20P.%20Adams&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20appear%20to%20be%20powerful%20tools%20to%20learn%20state%0Arepresentations%20for%20agents%20in%20distributed%2C%20decentralized%20multi-agent%20systems%2C%0Abut%20generate%20catastrophically%20incorrect%20predictions%20when%20nodes%20update%0Aasynchronously%20during%20inference.%20This%20failure%20under%20asynchrony%20effectively%0Aexcludes%20these%20architectures%20from%20many%20potential%20applications%20where%20synchrony%0Ais%20difficult%20or%20impossible%20to%20enforce%2C%20e.g.%2C%20robotic%20swarms%20or%20sensor%20networks.%0AIn%20this%20work%20we%20identify%20%22implicitly-defined%22%20GNNs%20as%20a%20class%20of%20architectures%0Awhich%20is%20provably%20robust%20to%20asynchronous%20%22hogwild%22%20inference%2C%20adapting%0Aconvergence%20guarantees%20from%20work%20in%20asynchronous%20and%20distributed%20optimization.%0AWe%20then%20propose%20a%20novel%20implicitly-defined%20GNN%20architecture%2C%20which%20we%20call%20an%0A%27energy%20GNN%27.%20We%20show%20that%20this%20architecture%20outperforms%20other%20GNNs%20from%20this%0Aclass%20on%20a%20variety%20of%20synthetic%20tasks%20inspired%20by%20multi-agent%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00494v2&entry.124074799=Read"},
{"title": "Denoising-While-Completing Network (DWCNet): Robust Point Cloud\n  Completion Under Corruption", "author": "Keneni W. Tesema and Lyndon Hill and Mark W. Jones and Gary K. L. Tam", "abstract": "  Point cloud completion is crucial for 3D computer vision tasks in autonomous\ndriving, augmented reality, and robotics. However, obtaining clean and complete\npoint clouds from real-world environments is challenging due to noise and\nocclusions. Consequently, most existing completion networks -- trained on\nsynthetic data -- struggle with real-world degradations. In this work, we\ntackle the problem of completing and denoising highly corrupted partial point\nclouds affected by multiple simultaneous degradations. To benchmark robustness,\nwe introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which\nhighlights the limitations of current methods under diverse corruptions.\nBuilding on these insights, we propose DWCNet (Denoising-While-Completing\nNetwork), a completion framework enhanced with a Noise Management Module (NMM)\nthat leverages contrastive learning and self-attention to suppress noise and\nmodel structural relationships. DWCNet achieves state-of-the-art performance on\nboth clean and corrupted, synthetic and real-world datasets. The dataset and\ncode will be publicly available at\nhttps://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions\n", "link": "http://arxiv.org/abs/2507.16743v1", "date": "2025-07-22", "relevancy": 2.5559, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5232}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5224}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Denoising-While-Completing%20Network%20%28DWCNet%29%3A%20Robust%20Point%20Cloud%0A%20%20Completion%20Under%20Corruption&body=Title%3A%20Denoising-While-Completing%20Network%20%28DWCNet%29%3A%20Robust%20Point%20Cloud%0A%20%20Completion%20Under%20Corruption%0AAuthor%3A%20Keneni%20W.%20Tesema%20and%20Lyndon%20Hill%20and%20Mark%20W.%20Jones%20and%20Gary%20K.%20L.%20Tam%0AAbstract%3A%20%20%20Point%20cloud%20completion%20is%20crucial%20for%203D%20computer%20vision%20tasks%20in%20autonomous%0Adriving%2C%20augmented%20reality%2C%20and%20robotics.%20However%2C%20obtaining%20clean%20and%20complete%0Apoint%20clouds%20from%20real-world%20environments%20is%20challenging%20due%20to%20noise%20and%0Aocclusions.%20Consequently%2C%20most%20existing%20completion%20networks%20--%20trained%20on%0Asynthetic%20data%20--%20struggle%20with%20real-world%20degradations.%20In%20this%20work%2C%20we%0Atackle%20the%20problem%20of%20completing%20and%20denoising%20highly%20corrupted%20partial%20point%0Aclouds%20affected%20by%20multiple%20simultaneous%20degradations.%20To%20benchmark%20robustness%2C%0Awe%20introduce%20the%20Corrupted%20Point%20Cloud%20Completion%20Dataset%20%28CPCCD%29%2C%20which%0Ahighlights%20the%20limitations%20of%20current%20methods%20under%20diverse%20corruptions.%0ABuilding%20on%20these%20insights%2C%20we%20propose%20DWCNet%20%28Denoising-While-Completing%0ANetwork%29%2C%20a%20completion%20framework%20enhanced%20with%20a%20Noise%20Management%20Module%20%28NMM%29%0Athat%20leverages%20contrastive%20learning%20and%20self-attention%20to%20suppress%20noise%20and%0Amodel%20structural%20relationships.%20DWCNet%20achieves%20state-of-the-art%20performance%20on%0Aboth%20clean%20and%20corrupted%2C%20synthetic%20and%20real-world%20datasets.%20The%20dataset%20and%0Acode%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenoising-While-Completing%2520Network%2520%2528DWCNet%2529%253A%2520Robust%2520Point%2520Cloud%250A%2520%2520Completion%2520Under%2520Corruption%26entry.906535625%3DKeneni%2520W.%2520Tesema%2520and%2520Lyndon%2520Hill%2520and%2520Mark%2520W.%2520Jones%2520and%2520Gary%2520K.%2520L.%2520Tam%26entry.1292438233%3D%2520%2520Point%2520cloud%2520completion%2520is%2520crucial%2520for%25203D%2520computer%2520vision%2520tasks%2520in%2520autonomous%250Adriving%252C%2520augmented%2520reality%252C%2520and%2520robotics.%2520However%252C%2520obtaining%2520clean%2520and%2520complete%250Apoint%2520clouds%2520from%2520real-world%2520environments%2520is%2520challenging%2520due%2520to%2520noise%2520and%250Aocclusions.%2520Consequently%252C%2520most%2520existing%2520completion%2520networks%2520--%2520trained%2520on%250Asynthetic%2520data%2520--%2520struggle%2520with%2520real-world%2520degradations.%2520In%2520this%2520work%252C%2520we%250Atackle%2520the%2520problem%2520of%2520completing%2520and%2520denoising%2520highly%2520corrupted%2520partial%2520point%250Aclouds%2520affected%2520by%2520multiple%2520simultaneous%2520degradations.%2520To%2520benchmark%2520robustness%252C%250Awe%2520introduce%2520the%2520Corrupted%2520Point%2520Cloud%2520Completion%2520Dataset%2520%2528CPCCD%2529%252C%2520which%250Ahighlights%2520the%2520limitations%2520of%2520current%2520methods%2520under%2520diverse%2520corruptions.%250ABuilding%2520on%2520these%2520insights%252C%2520we%2520propose%2520DWCNet%2520%2528Denoising-While-Completing%250ANetwork%2529%252C%2520a%2520completion%2520framework%2520enhanced%2520with%2520a%2520Noise%2520Management%2520Module%2520%2528NMM%2529%250Athat%2520leverages%2520contrastive%2520learning%2520and%2520self-attention%2520to%2520suppress%2520noise%2520and%250Amodel%2520structural%2520relationships.%2520DWCNet%2520achieves%2520state-of-the-art%2520performance%2520on%250Aboth%2520clean%2520and%2520corrupted%252C%2520synthetic%2520and%2520real-world%2520datasets.%2520The%2520dataset%2520and%250Acode%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Denoising-While-Completing%20Network%20%28DWCNet%29%3A%20Robust%20Point%20Cloud%0A%20%20Completion%20Under%20Corruption&entry.906535625=Keneni%20W.%20Tesema%20and%20Lyndon%20Hill%20and%20Mark%20W.%20Jones%20and%20Gary%20K.%20L.%20Tam&entry.1292438233=%20%20Point%20cloud%20completion%20is%20crucial%20for%203D%20computer%20vision%20tasks%20in%20autonomous%0Adriving%2C%20augmented%20reality%2C%20and%20robotics.%20However%2C%20obtaining%20clean%20and%20complete%0Apoint%20clouds%20from%20real-world%20environments%20is%20challenging%20due%20to%20noise%20and%0Aocclusions.%20Consequently%2C%20most%20existing%20completion%20networks%20--%20trained%20on%0Asynthetic%20data%20--%20struggle%20with%20real-world%20degradations.%20In%20this%20work%2C%20we%0Atackle%20the%20problem%20of%20completing%20and%20denoising%20highly%20corrupted%20partial%20point%0Aclouds%20affected%20by%20multiple%20simultaneous%20degradations.%20To%20benchmark%20robustness%2C%0Awe%20introduce%20the%20Corrupted%20Point%20Cloud%20Completion%20Dataset%20%28CPCCD%29%2C%20which%0Ahighlights%20the%20limitations%20of%20current%20methods%20under%20diverse%20corruptions.%0ABuilding%20on%20these%20insights%2C%20we%20propose%20DWCNet%20%28Denoising-While-Completing%0ANetwork%29%2C%20a%20completion%20framework%20enhanced%20with%20a%20Noise%20Management%20Module%20%28NMM%29%0Athat%20leverages%20contrastive%20learning%20and%20self-attention%20to%20suppress%20noise%20and%0Amodel%20structural%20relationships.%20DWCNet%20achieves%20state-of-the-art%20performance%20on%0Aboth%20clean%20and%20corrupted%2C%20synthetic%20and%20real-world%20datasets.%20The%20dataset%20and%0Acode%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16743v1&entry.124074799=Read"},
{"title": "Combined Image Data Augmentations diminish the benefits of Adaptive\n  Label Smoothing", "author": "Georg Siedel and Ekagra Gupta and Weijia Shao and Silvia Vock and Andrey Morozov", "abstract": "  Soft augmentation regularizes the supervised learning process of image\nclassifiers by reducing label confidence of a training sample based on the\nmagnitude of random-crop augmentation applied to it. This paper extends this\nadaptive label smoothing framework to other types of aggressive augmentations\nbeyond random-crop. Specifically, we demonstrate the effectiveness of the\nmethod for random erasing and noise injection data augmentation. Adaptive label\nsmoothing permits stronger regularization via higher-intensity Random Erasing.\nHowever, its benefits vanish when applied with a diverse range of image\ntransformations as in the state-of-the-art TrivialAugment method, and excessive\nlabel smoothing harms robustness to common corruptions. Our findings suggest\nthat adaptive label smoothing should only be applied when the training data\ndistribution is dominated by a limited, homogeneous set of image transformation\ntypes.\n", "link": "http://arxiv.org/abs/2507.16427v1", "date": "2025-07-22", "relevancy": 2.5496, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5147}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5101}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combined%20Image%20Data%20Augmentations%20diminish%20the%20benefits%20of%20Adaptive%0A%20%20Label%20Smoothing&body=Title%3A%20Combined%20Image%20Data%20Augmentations%20diminish%20the%20benefits%20of%20Adaptive%0A%20%20Label%20Smoothing%0AAuthor%3A%20Georg%20Siedel%20and%20Ekagra%20Gupta%20and%20Weijia%20Shao%20and%20Silvia%20Vock%20and%20Andrey%20Morozov%0AAbstract%3A%20%20%20Soft%20augmentation%20regularizes%20the%20supervised%20learning%20process%20of%20image%0Aclassifiers%20by%20reducing%20label%20confidence%20of%20a%20training%20sample%20based%20on%20the%0Amagnitude%20of%20random-crop%20augmentation%20applied%20to%20it.%20This%20paper%20extends%20this%0Aadaptive%20label%20smoothing%20framework%20to%20other%20types%20of%20aggressive%20augmentations%0Abeyond%20random-crop.%20Specifically%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Amethod%20for%20random%20erasing%20and%20noise%20injection%20data%20augmentation.%20Adaptive%20label%0Asmoothing%20permits%20stronger%20regularization%20via%20higher-intensity%20Random%20Erasing.%0AHowever%2C%20its%20benefits%20vanish%20when%20applied%20with%20a%20diverse%20range%20of%20image%0Atransformations%20as%20in%20the%20state-of-the-art%20TrivialAugment%20method%2C%20and%20excessive%0Alabel%20smoothing%20harms%20robustness%20to%20common%20corruptions.%20Our%20findings%20suggest%0Athat%20adaptive%20label%20smoothing%20should%20only%20be%20applied%20when%20the%20training%20data%0Adistribution%20is%20dominated%20by%20a%20limited%2C%20homogeneous%20set%20of%20image%20transformation%0Atypes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombined%2520Image%2520Data%2520Augmentations%2520diminish%2520the%2520benefits%2520of%2520Adaptive%250A%2520%2520Label%2520Smoothing%26entry.906535625%3DGeorg%2520Siedel%2520and%2520Ekagra%2520Gupta%2520and%2520Weijia%2520Shao%2520and%2520Silvia%2520Vock%2520and%2520Andrey%2520Morozov%26entry.1292438233%3D%2520%2520Soft%2520augmentation%2520regularizes%2520the%2520supervised%2520learning%2520process%2520of%2520image%250Aclassifiers%2520by%2520reducing%2520label%2520confidence%2520of%2520a%2520training%2520sample%2520based%2520on%2520the%250Amagnitude%2520of%2520random-crop%2520augmentation%2520applied%2520to%2520it.%2520This%2520paper%2520extends%2520this%250Aadaptive%2520label%2520smoothing%2520framework%2520to%2520other%2520types%2520of%2520aggressive%2520augmentations%250Abeyond%2520random-crop.%2520Specifically%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Amethod%2520for%2520random%2520erasing%2520and%2520noise%2520injection%2520data%2520augmentation.%2520Adaptive%2520label%250Asmoothing%2520permits%2520stronger%2520regularization%2520via%2520higher-intensity%2520Random%2520Erasing.%250AHowever%252C%2520its%2520benefits%2520vanish%2520when%2520applied%2520with%2520a%2520diverse%2520range%2520of%2520image%250Atransformations%2520as%2520in%2520the%2520state-of-the-art%2520TrivialAugment%2520method%252C%2520and%2520excessive%250Alabel%2520smoothing%2520harms%2520robustness%2520to%2520common%2520corruptions.%2520Our%2520findings%2520suggest%250Athat%2520adaptive%2520label%2520smoothing%2520should%2520only%2520be%2520applied%2520when%2520the%2520training%2520data%250Adistribution%2520is%2520dominated%2520by%2520a%2520limited%252C%2520homogeneous%2520set%2520of%2520image%2520transformation%250Atypes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combined%20Image%20Data%20Augmentations%20diminish%20the%20benefits%20of%20Adaptive%0A%20%20Label%20Smoothing&entry.906535625=Georg%20Siedel%20and%20Ekagra%20Gupta%20and%20Weijia%20Shao%20and%20Silvia%20Vock%20and%20Andrey%20Morozov&entry.1292438233=%20%20Soft%20augmentation%20regularizes%20the%20supervised%20learning%20process%20of%20image%0Aclassifiers%20by%20reducing%20label%20confidence%20of%20a%20training%20sample%20based%20on%20the%0Amagnitude%20of%20random-crop%20augmentation%20applied%20to%20it.%20This%20paper%20extends%20this%0Aadaptive%20label%20smoothing%20framework%20to%20other%20types%20of%20aggressive%20augmentations%0Abeyond%20random-crop.%20Specifically%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Amethod%20for%20random%20erasing%20and%20noise%20injection%20data%20augmentation.%20Adaptive%20label%0Asmoothing%20permits%20stronger%20regularization%20via%20higher-intensity%20Random%20Erasing.%0AHowever%2C%20its%20benefits%20vanish%20when%20applied%20with%20a%20diverse%20range%20of%20image%0Atransformations%20as%20in%20the%20state-of-the-art%20TrivialAugment%20method%2C%20and%20excessive%0Alabel%20smoothing%20harms%20robustness%20to%20common%20corruptions.%20Our%20findings%20suggest%0Athat%20adaptive%20label%20smoothing%20should%20only%20be%20applied%20when%20the%20training%20data%0Adistribution%20is%20dominated%20by%20a%20limited%2C%20homogeneous%20set%20of%20image%20transformation%0Atypes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16427v1&entry.124074799=Read"},
{"title": "Learning novel representations of variable sources from multi-modal\n  $\\textit{Gaia}$ data via autoencoders", "author": "P. Huijse and J. De Ridder and L. Eyer and L. Rimoldini and B. Holl and N. Chornay and J. Roquette and K. Nienartowicz and G. Jevardat de Fombelle and D. J. Fritzewski and A. Kemp and V. Vanlaer and M. Vanrespaille and H. Wang and M. I. Carnerero and C. M. Raiteri and G. Marton and M. Madar\u00e1sz and G. Clementini and P. Gavras and C. Aerts", "abstract": "  Gaia Data Release 3 (DR3) published for the first time epoch photometry,\nBP/RP (XP) low-resolution mean spectra, and supervised classification results\nfor millions of variable sources. This extensive dataset offers a unique\nopportunity to study their variability by combining multiple Gaia data\nproducts. In preparation for DR4, we propose and evaluate a machine learning\nmethodology capable of ingesting multiple Gaia data products to achieve an\nunsupervised classification of stellar and quasar variability. A dataset of 4\nmillion Gaia DR3 sources is used to train three variational autoencoders (VAE),\nwhich are artificial neural networks (ANNs) designed for data compression and\ngeneration. One VAE is trained on Gaia XP low-resolution spectra, another on a\nnovel approach based on the distribution of magnitude differences in the Gaia G\nband, and the third on folded Gaia G band light curves. Each Gaia source is\ncompressed into 15 numbers, representing the coordinates in a 15-dimensional\nlatent space generated by combining the outputs of these three models. The\nlearned latent representation produced by the ANN effectively distinguishes\nbetween the main variability classes present in Gaia DR3, as demonstrated\nthrough both supervised and unsupervised classification analysis of the latent\nspace. The results highlight a strong synergy between light curves and\nlow-resolution spectral data, emphasising the benefits of combining the\ndifferent Gaia data products. A two-dimensional projection of the latent\nvariables reveals numerous overdensities, most of which strongly correlate with\nastrophysical properties, showing the potential of this latent space for\nastrophysical discovery. We show that the properties of our novel latent\nrepresentation make it highly valuable for variability analysis tasks,\nincluding classification, clustering and outlier detection.\n", "link": "http://arxiv.org/abs/2505.16320v2", "date": "2025-07-22", "relevancy": 2.539, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5597}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.491}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20novel%20representations%20of%20variable%20sources%20from%20multi-modal%0A%20%20%24%5Ctextit%7BGaia%7D%24%20data%20via%20autoencoders&body=Title%3A%20Learning%20novel%20representations%20of%20variable%20sources%20from%20multi-modal%0A%20%20%24%5Ctextit%7BGaia%7D%24%20data%20via%20autoencoders%0AAuthor%3A%20P.%20Huijse%20and%20J.%20De%20Ridder%20and%20L.%20Eyer%20and%20L.%20Rimoldini%20and%20B.%20Holl%20and%20N.%20Chornay%20and%20J.%20Roquette%20and%20K.%20Nienartowicz%20and%20G.%20Jevardat%20de%20Fombelle%20and%20D.%20J.%20Fritzewski%20and%20A.%20Kemp%20and%20V.%20Vanlaer%20and%20M.%20Vanrespaille%20and%20H.%20Wang%20and%20M.%20I.%20Carnerero%20and%20C.%20M.%20Raiteri%20and%20G.%20Marton%20and%20M.%20Madar%C3%A1sz%20and%20G.%20Clementini%20and%20P.%20Gavras%20and%20C.%20Aerts%0AAbstract%3A%20%20%20Gaia%20Data%20Release%203%20%28DR3%29%20published%20for%20the%20first%20time%20epoch%20photometry%2C%0ABP/RP%20%28XP%29%20low-resolution%20mean%20spectra%2C%20and%20supervised%20classification%20results%0Afor%20millions%20of%20variable%20sources.%20This%20extensive%20dataset%20offers%20a%20unique%0Aopportunity%20to%20study%20their%20variability%20by%20combining%20multiple%20Gaia%20data%0Aproducts.%20In%20preparation%20for%20DR4%2C%20we%20propose%20and%20evaluate%20a%20machine%20learning%0Amethodology%20capable%20of%20ingesting%20multiple%20Gaia%20data%20products%20to%20achieve%20an%0Aunsupervised%20classification%20of%20stellar%20and%20quasar%20variability.%20A%20dataset%20of%204%0Amillion%20Gaia%20DR3%20sources%20is%20used%20to%20train%20three%20variational%20autoencoders%20%28VAE%29%2C%0Awhich%20are%20artificial%20neural%20networks%20%28ANNs%29%20designed%20for%20data%20compression%20and%0Ageneration.%20One%20VAE%20is%20trained%20on%20Gaia%20XP%20low-resolution%20spectra%2C%20another%20on%20a%0Anovel%20approach%20based%20on%20the%20distribution%20of%20magnitude%20differences%20in%20the%20Gaia%20G%0Aband%2C%20and%20the%20third%20on%20folded%20Gaia%20G%20band%20light%20curves.%20Each%20Gaia%20source%20is%0Acompressed%20into%2015%20numbers%2C%20representing%20the%20coordinates%20in%20a%2015-dimensional%0Alatent%20space%20generated%20by%20combining%20the%20outputs%20of%20these%20three%20models.%20The%0Alearned%20latent%20representation%20produced%20by%20the%20ANN%20effectively%20distinguishes%0Abetween%20the%20main%20variability%20classes%20present%20in%20Gaia%20DR3%2C%20as%20demonstrated%0Athrough%20both%20supervised%20and%20unsupervised%20classification%20analysis%20of%20the%20latent%0Aspace.%20The%20results%20highlight%20a%20strong%20synergy%20between%20light%20curves%20and%0Alow-resolution%20spectral%20data%2C%20emphasising%20the%20benefits%20of%20combining%20the%0Adifferent%20Gaia%20data%20products.%20A%20two-dimensional%20projection%20of%20the%20latent%0Avariables%20reveals%20numerous%20overdensities%2C%20most%20of%20which%20strongly%20correlate%20with%0Aastrophysical%20properties%2C%20showing%20the%20potential%20of%20this%20latent%20space%20for%0Aastrophysical%20discovery.%20We%20show%20that%20the%20properties%20of%20our%20novel%20latent%0Arepresentation%20make%20it%20highly%20valuable%20for%20variability%20analysis%20tasks%2C%0Aincluding%20classification%2C%20clustering%20and%20outlier%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16320v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520novel%2520representations%2520of%2520variable%2520sources%2520from%2520multi-modal%250A%2520%2520%2524%255Ctextit%257BGaia%257D%2524%2520data%2520via%2520autoencoders%26entry.906535625%3DP.%2520Huijse%2520and%2520J.%2520De%2520Ridder%2520and%2520L.%2520Eyer%2520and%2520L.%2520Rimoldini%2520and%2520B.%2520Holl%2520and%2520N.%2520Chornay%2520and%2520J.%2520Roquette%2520and%2520K.%2520Nienartowicz%2520and%2520G.%2520Jevardat%2520de%2520Fombelle%2520and%2520D.%2520J.%2520Fritzewski%2520and%2520A.%2520Kemp%2520and%2520V.%2520Vanlaer%2520and%2520M.%2520Vanrespaille%2520and%2520H.%2520Wang%2520and%2520M.%2520I.%2520Carnerero%2520and%2520C.%2520M.%2520Raiteri%2520and%2520G.%2520Marton%2520and%2520M.%2520Madar%25C3%25A1sz%2520and%2520G.%2520Clementini%2520and%2520P.%2520Gavras%2520and%2520C.%2520Aerts%26entry.1292438233%3D%2520%2520Gaia%2520Data%2520Release%25203%2520%2528DR3%2529%2520published%2520for%2520the%2520first%2520time%2520epoch%2520photometry%252C%250ABP/RP%2520%2528XP%2529%2520low-resolution%2520mean%2520spectra%252C%2520and%2520supervised%2520classification%2520results%250Afor%2520millions%2520of%2520variable%2520sources.%2520This%2520extensive%2520dataset%2520offers%2520a%2520unique%250Aopportunity%2520to%2520study%2520their%2520variability%2520by%2520combining%2520multiple%2520Gaia%2520data%250Aproducts.%2520In%2520preparation%2520for%2520DR4%252C%2520we%2520propose%2520and%2520evaluate%2520a%2520machine%2520learning%250Amethodology%2520capable%2520of%2520ingesting%2520multiple%2520Gaia%2520data%2520products%2520to%2520achieve%2520an%250Aunsupervised%2520classification%2520of%2520stellar%2520and%2520quasar%2520variability.%2520A%2520dataset%2520of%25204%250Amillion%2520Gaia%2520DR3%2520sources%2520is%2520used%2520to%2520train%2520three%2520variational%2520autoencoders%2520%2528VAE%2529%252C%250Awhich%2520are%2520artificial%2520neural%2520networks%2520%2528ANNs%2529%2520designed%2520for%2520data%2520compression%2520and%250Ageneration.%2520One%2520VAE%2520is%2520trained%2520on%2520Gaia%2520XP%2520low-resolution%2520spectra%252C%2520another%2520on%2520a%250Anovel%2520approach%2520based%2520on%2520the%2520distribution%2520of%2520magnitude%2520differences%2520in%2520the%2520Gaia%2520G%250Aband%252C%2520and%2520the%2520third%2520on%2520folded%2520Gaia%2520G%2520band%2520light%2520curves.%2520Each%2520Gaia%2520source%2520is%250Acompressed%2520into%252015%2520numbers%252C%2520representing%2520the%2520coordinates%2520in%2520a%252015-dimensional%250Alatent%2520space%2520generated%2520by%2520combining%2520the%2520outputs%2520of%2520these%2520three%2520models.%2520The%250Alearned%2520latent%2520representation%2520produced%2520by%2520the%2520ANN%2520effectively%2520distinguishes%250Abetween%2520the%2520main%2520variability%2520classes%2520present%2520in%2520Gaia%2520DR3%252C%2520as%2520demonstrated%250Athrough%2520both%2520supervised%2520and%2520unsupervised%2520classification%2520analysis%2520of%2520the%2520latent%250Aspace.%2520The%2520results%2520highlight%2520a%2520strong%2520synergy%2520between%2520light%2520curves%2520and%250Alow-resolution%2520spectral%2520data%252C%2520emphasising%2520the%2520benefits%2520of%2520combining%2520the%250Adifferent%2520Gaia%2520data%2520products.%2520A%2520two-dimensional%2520projection%2520of%2520the%2520latent%250Avariables%2520reveals%2520numerous%2520overdensities%252C%2520most%2520of%2520which%2520strongly%2520correlate%2520with%250Aastrophysical%2520properties%252C%2520showing%2520the%2520potential%2520of%2520this%2520latent%2520space%2520for%250Aastrophysical%2520discovery.%2520We%2520show%2520that%2520the%2520properties%2520of%2520our%2520novel%2520latent%250Arepresentation%2520make%2520it%2520highly%2520valuable%2520for%2520variability%2520analysis%2520tasks%252C%250Aincluding%2520classification%252C%2520clustering%2520and%2520outlier%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16320v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20novel%20representations%20of%20variable%20sources%20from%20multi-modal%0A%20%20%24%5Ctextit%7BGaia%7D%24%20data%20via%20autoencoders&entry.906535625=P.%20Huijse%20and%20J.%20De%20Ridder%20and%20L.%20Eyer%20and%20L.%20Rimoldini%20and%20B.%20Holl%20and%20N.%20Chornay%20and%20J.%20Roquette%20and%20K.%20Nienartowicz%20and%20G.%20Jevardat%20de%20Fombelle%20and%20D.%20J.%20Fritzewski%20and%20A.%20Kemp%20and%20V.%20Vanlaer%20and%20M.%20Vanrespaille%20and%20H.%20Wang%20and%20M.%20I.%20Carnerero%20and%20C.%20M.%20Raiteri%20and%20G.%20Marton%20and%20M.%20Madar%C3%A1sz%20and%20G.%20Clementini%20and%20P.%20Gavras%20and%20C.%20Aerts&entry.1292438233=%20%20Gaia%20Data%20Release%203%20%28DR3%29%20published%20for%20the%20first%20time%20epoch%20photometry%2C%0ABP/RP%20%28XP%29%20low-resolution%20mean%20spectra%2C%20and%20supervised%20classification%20results%0Afor%20millions%20of%20variable%20sources.%20This%20extensive%20dataset%20offers%20a%20unique%0Aopportunity%20to%20study%20their%20variability%20by%20combining%20multiple%20Gaia%20data%0Aproducts.%20In%20preparation%20for%20DR4%2C%20we%20propose%20and%20evaluate%20a%20machine%20learning%0Amethodology%20capable%20of%20ingesting%20multiple%20Gaia%20data%20products%20to%20achieve%20an%0Aunsupervised%20classification%20of%20stellar%20and%20quasar%20variability.%20A%20dataset%20of%204%0Amillion%20Gaia%20DR3%20sources%20is%20used%20to%20train%20three%20variational%20autoencoders%20%28VAE%29%2C%0Awhich%20are%20artificial%20neural%20networks%20%28ANNs%29%20designed%20for%20data%20compression%20and%0Ageneration.%20One%20VAE%20is%20trained%20on%20Gaia%20XP%20low-resolution%20spectra%2C%20another%20on%20a%0Anovel%20approach%20based%20on%20the%20distribution%20of%20magnitude%20differences%20in%20the%20Gaia%20G%0Aband%2C%20and%20the%20third%20on%20folded%20Gaia%20G%20band%20light%20curves.%20Each%20Gaia%20source%20is%0Acompressed%20into%2015%20numbers%2C%20representing%20the%20coordinates%20in%20a%2015-dimensional%0Alatent%20space%20generated%20by%20combining%20the%20outputs%20of%20these%20three%20models.%20The%0Alearned%20latent%20representation%20produced%20by%20the%20ANN%20effectively%20distinguishes%0Abetween%20the%20main%20variability%20classes%20present%20in%20Gaia%20DR3%2C%20as%20demonstrated%0Athrough%20both%20supervised%20and%20unsupervised%20classification%20analysis%20of%20the%20latent%0Aspace.%20The%20results%20highlight%20a%20strong%20synergy%20between%20light%20curves%20and%0Alow-resolution%20spectral%20data%2C%20emphasising%20the%20benefits%20of%20combining%20the%0Adifferent%20Gaia%20data%20products.%20A%20two-dimensional%20projection%20of%20the%20latent%0Avariables%20reveals%20numerous%20overdensities%2C%20most%20of%20which%20strongly%20correlate%20with%0Aastrophysical%20properties%2C%20showing%20the%20potential%20of%20this%20latent%20space%20for%0Aastrophysical%20discovery.%20We%20show%20that%20the%20properties%20of%20our%20novel%20latent%0Arepresentation%20make%20it%20highly%20valuable%20for%20variability%20analysis%20tasks%2C%0Aincluding%20classification%2C%20clustering%20and%20outlier%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16320v2&entry.124074799=Read"},
{"title": "Graph Neural Network-Based Distributed Optimal Control for Linear\n  Networked Systems: An Online Distributed Training Approach", "author": "Zihao Song and Shirantha Welikala and Panos J. Antsaklis and Hai Lin", "abstract": "  In this paper, we consider the distributed optimal control problem for\ndiscrete-time linear networked systems. In particular, we are interested in\nlearning distributed optimal controllers using graph recurrent neural networks\n(GRNNs). Most of the existing approaches result in centralized optimal\ncontrollers with offline training processes. However, as the increasing demand\nof network resilience, the optimal controllers are further expected to be\ndistributed, and are desirable to be trained in an online distributed fashion,\nwhich are also the main contributions of our work. To solve this problem, we\nfirst propose a GRNN-based distributed optimal control method, and we cast the\nproblem as a self-supervised learning problem. Then, the distributed online\ntraining is achieved via distributed gradient computation, and inspired by the\n(consensus-based) distributed optimization idea, a distributed online training\noptimizer is designed. Furthermore, the local closed-loop stability of the\nlinear networked system under our proposed GRNN-based controller is provided by\nassuming that the nonlinear activation function of the GRNN-based controller is\nboth local sector-bounded and slope-restricted. The effectiveness of our\nproposed method is illustrated by numerical simulations using a specifically\ndeveloped simulator.\n", "link": "http://arxiv.org/abs/2504.06439v2", "date": "2025-07-22", "relevancy": 2.5157, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5342}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5143}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Neural%20Network-Based%20Distributed%20Optimal%20Control%20for%20Linear%0A%20%20Networked%20Systems%3A%20An%20Online%20Distributed%20Training%20Approach&body=Title%3A%20Graph%20Neural%20Network-Based%20Distributed%20Optimal%20Control%20for%20Linear%0A%20%20Networked%20Systems%3A%20An%20Online%20Distributed%20Training%20Approach%0AAuthor%3A%20Zihao%20Song%20and%20Shirantha%20Welikala%20and%20Panos%20J.%20Antsaklis%20and%20Hai%20Lin%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20the%20distributed%20optimal%20control%20problem%20for%0Adiscrete-time%20linear%20networked%20systems.%20In%20particular%2C%20we%20are%20interested%20in%0Alearning%20distributed%20optimal%20controllers%20using%20graph%20recurrent%20neural%20networks%0A%28GRNNs%29.%20Most%20of%20the%20existing%20approaches%20result%20in%20centralized%20optimal%0Acontrollers%20with%20offline%20training%20processes.%20However%2C%20as%20the%20increasing%20demand%0Aof%20network%20resilience%2C%20the%20optimal%20controllers%20are%20further%20expected%20to%20be%0Adistributed%2C%20and%20are%20desirable%20to%20be%20trained%20in%20an%20online%20distributed%20fashion%2C%0Awhich%20are%20also%20the%20main%20contributions%20of%20our%20work.%20To%20solve%20this%20problem%2C%20we%0Afirst%20propose%20a%20GRNN-based%20distributed%20optimal%20control%20method%2C%20and%20we%20cast%20the%0Aproblem%20as%20a%20self-supervised%20learning%20problem.%20Then%2C%20the%20distributed%20online%0Atraining%20is%20achieved%20via%20distributed%20gradient%20computation%2C%20and%20inspired%20by%20the%0A%28consensus-based%29%20distributed%20optimization%20idea%2C%20a%20distributed%20online%20training%0Aoptimizer%20is%20designed.%20Furthermore%2C%20the%20local%20closed-loop%20stability%20of%20the%0Alinear%20networked%20system%20under%20our%20proposed%20GRNN-based%20controller%20is%20provided%20by%0Aassuming%20that%20the%20nonlinear%20activation%20function%20of%20the%20GRNN-based%20controller%20is%0Aboth%20local%20sector-bounded%20and%20slope-restricted.%20The%20effectiveness%20of%20our%0Aproposed%20method%20is%20illustrated%20by%20numerical%20simulations%20using%20a%20specifically%0Adeveloped%20simulator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.06439v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Neural%2520Network-Based%2520Distributed%2520Optimal%2520Control%2520for%2520Linear%250A%2520%2520Networked%2520Systems%253A%2520An%2520Online%2520Distributed%2520Training%2520Approach%26entry.906535625%3DZihao%2520Song%2520and%2520Shirantha%2520Welikala%2520and%2520Panos%2520J.%2520Antsaklis%2520and%2520Hai%2520Lin%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520the%2520distributed%2520optimal%2520control%2520problem%2520for%250Adiscrete-time%2520linear%2520networked%2520systems.%2520In%2520particular%252C%2520we%2520are%2520interested%2520in%250Alearning%2520distributed%2520optimal%2520controllers%2520using%2520graph%2520recurrent%2520neural%2520networks%250A%2528GRNNs%2529.%2520Most%2520of%2520the%2520existing%2520approaches%2520result%2520in%2520centralized%2520optimal%250Acontrollers%2520with%2520offline%2520training%2520processes.%2520However%252C%2520as%2520the%2520increasing%2520demand%250Aof%2520network%2520resilience%252C%2520the%2520optimal%2520controllers%2520are%2520further%2520expected%2520to%2520be%250Adistributed%252C%2520and%2520are%2520desirable%2520to%2520be%2520trained%2520in%2520an%2520online%2520distributed%2520fashion%252C%250Awhich%2520are%2520also%2520the%2520main%2520contributions%2520of%2520our%2520work.%2520To%2520solve%2520this%2520problem%252C%2520we%250Afirst%2520propose%2520a%2520GRNN-based%2520distributed%2520optimal%2520control%2520method%252C%2520and%2520we%2520cast%2520the%250Aproblem%2520as%2520a%2520self-supervised%2520learning%2520problem.%2520Then%252C%2520the%2520distributed%2520online%250Atraining%2520is%2520achieved%2520via%2520distributed%2520gradient%2520computation%252C%2520and%2520inspired%2520by%2520the%250A%2528consensus-based%2529%2520distributed%2520optimization%2520idea%252C%2520a%2520distributed%2520online%2520training%250Aoptimizer%2520is%2520designed.%2520Furthermore%252C%2520the%2520local%2520closed-loop%2520stability%2520of%2520the%250Alinear%2520networked%2520system%2520under%2520our%2520proposed%2520GRNN-based%2520controller%2520is%2520provided%2520by%250Aassuming%2520that%2520the%2520nonlinear%2520activation%2520function%2520of%2520the%2520GRNN-based%2520controller%2520is%250Aboth%2520local%2520sector-bounded%2520and%2520slope-restricted.%2520The%2520effectiveness%2520of%2520our%250Aproposed%2520method%2520is%2520illustrated%2520by%2520numerical%2520simulations%2520using%2520a%2520specifically%250Adeveloped%2520simulator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.06439v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Neural%20Network-Based%20Distributed%20Optimal%20Control%20for%20Linear%0A%20%20Networked%20Systems%3A%20An%20Online%20Distributed%20Training%20Approach&entry.906535625=Zihao%20Song%20and%20Shirantha%20Welikala%20and%20Panos%20J.%20Antsaklis%20and%20Hai%20Lin&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20the%20distributed%20optimal%20control%20problem%20for%0Adiscrete-time%20linear%20networked%20systems.%20In%20particular%2C%20we%20are%20interested%20in%0Alearning%20distributed%20optimal%20controllers%20using%20graph%20recurrent%20neural%20networks%0A%28GRNNs%29.%20Most%20of%20the%20existing%20approaches%20result%20in%20centralized%20optimal%0Acontrollers%20with%20offline%20training%20processes.%20However%2C%20as%20the%20increasing%20demand%0Aof%20network%20resilience%2C%20the%20optimal%20controllers%20are%20further%20expected%20to%20be%0Adistributed%2C%20and%20are%20desirable%20to%20be%20trained%20in%20an%20online%20distributed%20fashion%2C%0Awhich%20are%20also%20the%20main%20contributions%20of%20our%20work.%20To%20solve%20this%20problem%2C%20we%0Afirst%20propose%20a%20GRNN-based%20distributed%20optimal%20control%20method%2C%20and%20we%20cast%20the%0Aproblem%20as%20a%20self-supervised%20learning%20problem.%20Then%2C%20the%20distributed%20online%0Atraining%20is%20achieved%20via%20distributed%20gradient%20computation%2C%20and%20inspired%20by%20the%0A%28consensus-based%29%20distributed%20optimization%20idea%2C%20a%20distributed%20online%20training%0Aoptimizer%20is%20designed.%20Furthermore%2C%20the%20local%20closed-loop%20stability%20of%20the%0Alinear%20networked%20system%20under%20our%20proposed%20GRNN-based%20controller%20is%20provided%20by%0Aassuming%20that%20the%20nonlinear%20activation%20function%20of%20the%20GRNN-based%20controller%20is%0Aboth%20local%20sector-bounded%20and%20slope-restricted.%20The%20effectiveness%20of%20our%0Aproposed%20method%20is%20illustrated%20by%20numerical%20simulations%20using%20a%20specifically%0Adeveloped%20simulator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.06439v2&entry.124074799=Read"},
{"title": "Automatic Fine-grained Segmentation-assisted Report Generation", "author": "Frederic Jonske and Constantin Seibold and Osman Alperen Koras and Fin Bahnsen and Marie Bauer and Amin Dada and Hamza Kalisch and Anton Schily and Jens Kleesiek", "abstract": "  Reliable end-to-end clinical report generation has been a longstanding goal\nof medical ML research. The end goal for this process is to alleviate\nradiologists' workloads and provide second opinions to clinicians or patients.\nThus, a necessary prerequisite for report generation models is a strong general\nperformance and some type of innate grounding capability, to convince\nclinicians or patients of the veracity of the generated reports. In this paper,\nwe present ASaRG (\\textbf{A}utomatic \\textbf{S}egmentation-\\textbf{a}ssisted\n\\textbf{R}eport \\textbf{G}eneration), an extension of the popular LLaVA\narchitecture that aims to tackle both of these problems. ASaRG proposes to fuse\nintermediate features and fine-grained segmentation maps created by specialist\nradiological models into LLaVA's multi-modal projection layer via simple\nconcatenation. With a small number of added parameters, our approach achieves a\n+0.89\\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA\nbaseline when using only intermediate features, and +2.77\\% performance gain\n($p<0.001$) when adding a combination of intermediate features and fine-grained\nsegmentation maps. Compared with COMG and ORID, two other report generation\nmethods that utilize segmentations, the performance gain amounts to 6.98\\% and\n6.28\\% in F1 score, respectively. ASaRG is not mutually exclusive with other\nchanges made to the LLaVA architecture, potentially allowing our method to be\ncombined with other advances in the field. Finally, the use of an arbitrary\nnumber of segmentations as part of the input demonstrably allows tracing\nelements of the report to the corresponding segmentation maps and verifying the\ngroundedness of assessments. Our code will be made publicly available at a\nlater date.\n", "link": "http://arxiv.org/abs/2507.16623v1", "date": "2025-07-22", "relevancy": 2.5145, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5192}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5098}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Fine-grained%20Segmentation-assisted%20Report%20Generation&body=Title%3A%20Automatic%20Fine-grained%20Segmentation-assisted%20Report%20Generation%0AAuthor%3A%20Frederic%20Jonske%20and%20Constantin%20Seibold%20and%20Osman%20Alperen%20Koras%20and%20Fin%20Bahnsen%20and%20Marie%20Bauer%20and%20Amin%20Dada%20and%20Hamza%20Kalisch%20and%20Anton%20Schily%20and%20Jens%20Kleesiek%0AAbstract%3A%20%20%20Reliable%20end-to-end%20clinical%20report%20generation%20has%20been%20a%20longstanding%20goal%0Aof%20medical%20ML%20research.%20The%20end%20goal%20for%20this%20process%20is%20to%20alleviate%0Aradiologists%27%20workloads%20and%20provide%20second%20opinions%20to%20clinicians%20or%20patients.%0AThus%2C%20a%20necessary%20prerequisite%20for%20report%20generation%20models%20is%20a%20strong%20general%0Aperformance%20and%20some%20type%20of%20innate%20grounding%20capability%2C%20to%20convince%0Aclinicians%20or%20patients%20of%20the%20veracity%20of%20the%20generated%20reports.%20In%20this%20paper%2C%0Awe%20present%20ASaRG%20%28%5Ctextbf%7BA%7Dutomatic%20%5Ctextbf%7BS%7Degmentation-%5Ctextbf%7Ba%7Dssisted%0A%5Ctextbf%7BR%7Deport%20%5Ctextbf%7BG%7Deneration%29%2C%20an%20extension%20of%20the%20popular%20LLaVA%0Aarchitecture%20that%20aims%20to%20tackle%20both%20of%20these%20problems.%20ASaRG%20proposes%20to%20fuse%0Aintermediate%20features%20and%20fine-grained%20segmentation%20maps%20created%20by%20specialist%0Aradiological%20models%20into%20LLaVA%27s%20multi-modal%20projection%20layer%20via%20simple%0Aconcatenation.%20With%20a%20small%20number%20of%20added%20parameters%2C%20our%20approach%20achieves%20a%0A%2B0.89%5C%25%20performance%20gain%20%28%24p%3D0.012%24%29%20in%20CE%20F1%20score%20compared%20to%20the%20LLaVA%0Abaseline%20when%20using%20only%20intermediate%20features%2C%20and%20%2B2.77%5C%25%20performance%20gain%0A%28%24p%3C0.001%24%29%20when%20adding%20a%20combination%20of%20intermediate%20features%20and%20fine-grained%0Asegmentation%20maps.%20Compared%20with%20COMG%20and%20ORID%2C%20two%20other%20report%20generation%0Amethods%20that%20utilize%20segmentations%2C%20the%20performance%20gain%20amounts%20to%206.98%5C%25%20and%0A6.28%5C%25%20in%20F1%20score%2C%20respectively.%20ASaRG%20is%20not%20mutually%20exclusive%20with%20other%0Achanges%20made%20to%20the%20LLaVA%20architecture%2C%20potentially%20allowing%20our%20method%20to%20be%0Acombined%20with%20other%20advances%20in%20the%20field.%20Finally%2C%20the%20use%20of%20an%20arbitrary%0Anumber%20of%20segmentations%20as%20part%20of%20the%20input%20demonstrably%20allows%20tracing%0Aelements%20of%20the%20report%20to%20the%20corresponding%20segmentation%20maps%20and%20verifying%20the%0Agroundedness%20of%20assessments.%20Our%20code%20will%20be%20made%20publicly%20available%20at%20a%0Alater%20date.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Fine-grained%2520Segmentation-assisted%2520Report%2520Generation%26entry.906535625%3DFrederic%2520Jonske%2520and%2520Constantin%2520Seibold%2520and%2520Osman%2520Alperen%2520Koras%2520and%2520Fin%2520Bahnsen%2520and%2520Marie%2520Bauer%2520and%2520Amin%2520Dada%2520and%2520Hamza%2520Kalisch%2520and%2520Anton%2520Schily%2520and%2520Jens%2520Kleesiek%26entry.1292438233%3D%2520%2520Reliable%2520end-to-end%2520clinical%2520report%2520generation%2520has%2520been%2520a%2520longstanding%2520goal%250Aof%2520medical%2520ML%2520research.%2520The%2520end%2520goal%2520for%2520this%2520process%2520is%2520to%2520alleviate%250Aradiologists%2527%2520workloads%2520and%2520provide%2520second%2520opinions%2520to%2520clinicians%2520or%2520patients.%250AThus%252C%2520a%2520necessary%2520prerequisite%2520for%2520report%2520generation%2520models%2520is%2520a%2520strong%2520general%250Aperformance%2520and%2520some%2520type%2520of%2520innate%2520grounding%2520capability%252C%2520to%2520convince%250Aclinicians%2520or%2520patients%2520of%2520the%2520veracity%2520of%2520the%2520generated%2520reports.%2520In%2520this%2520paper%252C%250Awe%2520present%2520ASaRG%2520%2528%255Ctextbf%257BA%257Dutomatic%2520%255Ctextbf%257BS%257Degmentation-%255Ctextbf%257Ba%257Dssisted%250A%255Ctextbf%257BR%257Deport%2520%255Ctextbf%257BG%257Deneration%2529%252C%2520an%2520extension%2520of%2520the%2520popular%2520LLaVA%250Aarchitecture%2520that%2520aims%2520to%2520tackle%2520both%2520of%2520these%2520problems.%2520ASaRG%2520proposes%2520to%2520fuse%250Aintermediate%2520features%2520and%2520fine-grained%2520segmentation%2520maps%2520created%2520by%2520specialist%250Aradiological%2520models%2520into%2520LLaVA%2527s%2520multi-modal%2520projection%2520layer%2520via%2520simple%250Aconcatenation.%2520With%2520a%2520small%2520number%2520of%2520added%2520parameters%252C%2520our%2520approach%2520achieves%2520a%250A%252B0.89%255C%2525%2520performance%2520gain%2520%2528%2524p%253D0.012%2524%2529%2520in%2520CE%2520F1%2520score%2520compared%2520to%2520the%2520LLaVA%250Abaseline%2520when%2520using%2520only%2520intermediate%2520features%252C%2520and%2520%252B2.77%255C%2525%2520performance%2520gain%250A%2528%2524p%253C0.001%2524%2529%2520when%2520adding%2520a%2520combination%2520of%2520intermediate%2520features%2520and%2520fine-grained%250Asegmentation%2520maps.%2520Compared%2520with%2520COMG%2520and%2520ORID%252C%2520two%2520other%2520report%2520generation%250Amethods%2520that%2520utilize%2520segmentations%252C%2520the%2520performance%2520gain%2520amounts%2520to%25206.98%255C%2525%2520and%250A6.28%255C%2525%2520in%2520F1%2520score%252C%2520respectively.%2520ASaRG%2520is%2520not%2520mutually%2520exclusive%2520with%2520other%250Achanges%2520made%2520to%2520the%2520LLaVA%2520architecture%252C%2520potentially%2520allowing%2520our%2520method%2520to%2520be%250Acombined%2520with%2520other%2520advances%2520in%2520the%2520field.%2520Finally%252C%2520the%2520use%2520of%2520an%2520arbitrary%250Anumber%2520of%2520segmentations%2520as%2520part%2520of%2520the%2520input%2520demonstrably%2520allows%2520tracing%250Aelements%2520of%2520the%2520report%2520to%2520the%2520corresponding%2520segmentation%2520maps%2520and%2520verifying%2520the%250Agroundedness%2520of%2520assessments.%2520Our%2520code%2520will%2520be%2520made%2520publicly%2520available%2520at%2520a%250Alater%2520date.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Fine-grained%20Segmentation-assisted%20Report%20Generation&entry.906535625=Frederic%20Jonske%20and%20Constantin%20Seibold%20and%20Osman%20Alperen%20Koras%20and%20Fin%20Bahnsen%20and%20Marie%20Bauer%20and%20Amin%20Dada%20and%20Hamza%20Kalisch%20and%20Anton%20Schily%20and%20Jens%20Kleesiek&entry.1292438233=%20%20Reliable%20end-to-end%20clinical%20report%20generation%20has%20been%20a%20longstanding%20goal%0Aof%20medical%20ML%20research.%20The%20end%20goal%20for%20this%20process%20is%20to%20alleviate%0Aradiologists%27%20workloads%20and%20provide%20second%20opinions%20to%20clinicians%20or%20patients.%0AThus%2C%20a%20necessary%20prerequisite%20for%20report%20generation%20models%20is%20a%20strong%20general%0Aperformance%20and%20some%20type%20of%20innate%20grounding%20capability%2C%20to%20convince%0Aclinicians%20or%20patients%20of%20the%20veracity%20of%20the%20generated%20reports.%20In%20this%20paper%2C%0Awe%20present%20ASaRG%20%28%5Ctextbf%7BA%7Dutomatic%20%5Ctextbf%7BS%7Degmentation-%5Ctextbf%7Ba%7Dssisted%0A%5Ctextbf%7BR%7Deport%20%5Ctextbf%7BG%7Deneration%29%2C%20an%20extension%20of%20the%20popular%20LLaVA%0Aarchitecture%20that%20aims%20to%20tackle%20both%20of%20these%20problems.%20ASaRG%20proposes%20to%20fuse%0Aintermediate%20features%20and%20fine-grained%20segmentation%20maps%20created%20by%20specialist%0Aradiological%20models%20into%20LLaVA%27s%20multi-modal%20projection%20layer%20via%20simple%0Aconcatenation.%20With%20a%20small%20number%20of%20added%20parameters%2C%20our%20approach%20achieves%20a%0A%2B0.89%5C%25%20performance%20gain%20%28%24p%3D0.012%24%29%20in%20CE%20F1%20score%20compared%20to%20the%20LLaVA%0Abaseline%20when%20using%20only%20intermediate%20features%2C%20and%20%2B2.77%5C%25%20performance%20gain%0A%28%24p%3C0.001%24%29%20when%20adding%20a%20combination%20of%20intermediate%20features%20and%20fine-grained%0Asegmentation%20maps.%20Compared%20with%20COMG%20and%20ORID%2C%20two%20other%20report%20generation%0Amethods%20that%20utilize%20segmentations%2C%20the%20performance%20gain%20amounts%20to%206.98%5C%25%20and%0A6.28%5C%25%20in%20F1%20score%2C%20respectively.%20ASaRG%20is%20not%20mutually%20exclusive%20with%20other%0Achanges%20made%20to%20the%20LLaVA%20architecture%2C%20potentially%20allowing%20our%20method%20to%20be%0Acombined%20with%20other%20advances%20in%20the%20field.%20Finally%2C%20the%20use%20of%20an%20arbitrary%0Anumber%20of%20segmentations%20as%20part%20of%20the%20input%20demonstrably%20allows%20tracing%0Aelements%20of%20the%20report%20to%20the%20corresponding%20segmentation%20maps%20and%20verifying%20the%0Agroundedness%20of%20assessments.%20Our%20code%20will%20be%20made%20publicly%20available%20at%20a%0Alater%20date.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16623v1&entry.124074799=Read"},
{"title": "The Sweet Danger of Sugar: Debunking Representation Learning for\n  Encrypted Traffic Classification", "author": "Yuqi Zhao and Giovanni Dettori and Matteo Boffa and Luca Vassio and Marco Mellia", "abstract": "  Recently we have witnessed the explosion of proposals that, inspired by\nLanguage Models like BERT, exploit Representation Learning models to create\ntraffic representations. All of them promise astonishing performance in\nencrypted traffic classification (up to 98% accuracy). In this paper, with a\nnetworking expert mindset, we critically reassess their performance. Through\nextensive analysis, we demonstrate that the reported successes are heavily\ninfluenced by data preparation problems, which allow these models to find easy\nshortcuts - spurious correlation between features and labels - during\nfine-tuning that unrealistically boost their performance. When such shortcuts\nare not present - as in real scenarios - these models perform poorly. We also\nintroduce Pcap-Encoder, an LM-based representation learning model that we\nspecifically design to extract features from protocol headers. Pcap-Encoder\nappears to be the only model that provides an instrumental representation for\ntraffic classification. Yet, its complexity questions its applicability in\npractical settings. Our findings reveal flaws in dataset preparation and model\ntraining, calling for a better and more conscious test design. We propose a\ncorrect evaluation methodology and stress the need for rigorous benchmarking.\n", "link": "http://arxiv.org/abs/2507.16438v1", "date": "2025-07-22", "relevancy": 2.5142, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5129}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Sweet%20Danger%20of%20Sugar%3A%20Debunking%20Representation%20Learning%20for%0A%20%20Encrypted%20Traffic%20Classification&body=Title%3A%20The%20Sweet%20Danger%20of%20Sugar%3A%20Debunking%20Representation%20Learning%20for%0A%20%20Encrypted%20Traffic%20Classification%0AAuthor%3A%20Yuqi%20Zhao%20and%20Giovanni%20Dettori%20and%20Matteo%20Boffa%20and%20Luca%20Vassio%20and%20Marco%20Mellia%0AAbstract%3A%20%20%20Recently%20we%20have%20witnessed%20the%20explosion%20of%20proposals%20that%2C%20inspired%20by%0ALanguage%20Models%20like%20BERT%2C%20exploit%20Representation%20Learning%20models%20to%20create%0Atraffic%20representations.%20All%20of%20them%20promise%20astonishing%20performance%20in%0Aencrypted%20traffic%20classification%20%28up%20to%2098%25%20accuracy%29.%20In%20this%20paper%2C%20with%20a%0Anetworking%20expert%20mindset%2C%20we%20critically%20reassess%20their%20performance.%20Through%0Aextensive%20analysis%2C%20we%20demonstrate%20that%20the%20reported%20successes%20are%20heavily%0Ainfluenced%20by%20data%20preparation%20problems%2C%20which%20allow%20these%20models%20to%20find%20easy%0Ashortcuts%20-%20spurious%20correlation%20between%20features%20and%20labels%20-%20during%0Afine-tuning%20that%20unrealistically%20boost%20their%20performance.%20When%20such%20shortcuts%0Aare%20not%20present%20-%20as%20in%20real%20scenarios%20-%20these%20models%20perform%20poorly.%20We%20also%0Aintroduce%20Pcap-Encoder%2C%20an%20LM-based%20representation%20learning%20model%20that%20we%0Aspecifically%20design%20to%20extract%20features%20from%20protocol%20headers.%20Pcap-Encoder%0Aappears%20to%20be%20the%20only%20model%20that%20provides%20an%20instrumental%20representation%20for%0Atraffic%20classification.%20Yet%2C%20its%20complexity%20questions%20its%20applicability%20in%0Apractical%20settings.%20Our%20findings%20reveal%20flaws%20in%20dataset%20preparation%20and%20model%0Atraining%2C%20calling%20for%20a%20better%20and%20more%20conscious%20test%20design.%20We%20propose%20a%0Acorrect%20evaluation%20methodology%20and%20stress%20the%20need%20for%20rigorous%20benchmarking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Sweet%2520Danger%2520of%2520Sugar%253A%2520Debunking%2520Representation%2520Learning%2520for%250A%2520%2520Encrypted%2520Traffic%2520Classification%26entry.906535625%3DYuqi%2520Zhao%2520and%2520Giovanni%2520Dettori%2520and%2520Matteo%2520Boffa%2520and%2520Luca%2520Vassio%2520and%2520Marco%2520Mellia%26entry.1292438233%3D%2520%2520Recently%2520we%2520have%2520witnessed%2520the%2520explosion%2520of%2520proposals%2520that%252C%2520inspired%2520by%250ALanguage%2520Models%2520like%2520BERT%252C%2520exploit%2520Representation%2520Learning%2520models%2520to%2520create%250Atraffic%2520representations.%2520All%2520of%2520them%2520promise%2520astonishing%2520performance%2520in%250Aencrypted%2520traffic%2520classification%2520%2528up%2520to%252098%2525%2520accuracy%2529.%2520In%2520this%2520paper%252C%2520with%2520a%250Anetworking%2520expert%2520mindset%252C%2520we%2520critically%2520reassess%2520their%2520performance.%2520Through%250Aextensive%2520analysis%252C%2520we%2520demonstrate%2520that%2520the%2520reported%2520successes%2520are%2520heavily%250Ainfluenced%2520by%2520data%2520preparation%2520problems%252C%2520which%2520allow%2520these%2520models%2520to%2520find%2520easy%250Ashortcuts%2520-%2520spurious%2520correlation%2520between%2520features%2520and%2520labels%2520-%2520during%250Afine-tuning%2520that%2520unrealistically%2520boost%2520their%2520performance.%2520When%2520such%2520shortcuts%250Aare%2520not%2520present%2520-%2520as%2520in%2520real%2520scenarios%2520-%2520these%2520models%2520perform%2520poorly.%2520We%2520also%250Aintroduce%2520Pcap-Encoder%252C%2520an%2520LM-based%2520representation%2520learning%2520model%2520that%2520we%250Aspecifically%2520design%2520to%2520extract%2520features%2520from%2520protocol%2520headers.%2520Pcap-Encoder%250Aappears%2520to%2520be%2520the%2520only%2520model%2520that%2520provides%2520an%2520instrumental%2520representation%2520for%250Atraffic%2520classification.%2520Yet%252C%2520its%2520complexity%2520questions%2520its%2520applicability%2520in%250Apractical%2520settings.%2520Our%2520findings%2520reveal%2520flaws%2520in%2520dataset%2520preparation%2520and%2520model%250Atraining%252C%2520calling%2520for%2520a%2520better%2520and%2520more%2520conscious%2520test%2520design.%2520We%2520propose%2520a%250Acorrect%2520evaluation%2520methodology%2520and%2520stress%2520the%2520need%2520for%2520rigorous%2520benchmarking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Sweet%20Danger%20of%20Sugar%3A%20Debunking%20Representation%20Learning%20for%0A%20%20Encrypted%20Traffic%20Classification&entry.906535625=Yuqi%20Zhao%20and%20Giovanni%20Dettori%20and%20Matteo%20Boffa%20and%20Luca%20Vassio%20and%20Marco%20Mellia&entry.1292438233=%20%20Recently%20we%20have%20witnessed%20the%20explosion%20of%20proposals%20that%2C%20inspired%20by%0ALanguage%20Models%20like%20BERT%2C%20exploit%20Representation%20Learning%20models%20to%20create%0Atraffic%20representations.%20All%20of%20them%20promise%20astonishing%20performance%20in%0Aencrypted%20traffic%20classification%20%28up%20to%2098%25%20accuracy%29.%20In%20this%20paper%2C%20with%20a%0Anetworking%20expert%20mindset%2C%20we%20critically%20reassess%20their%20performance.%20Through%0Aextensive%20analysis%2C%20we%20demonstrate%20that%20the%20reported%20successes%20are%20heavily%0Ainfluenced%20by%20data%20preparation%20problems%2C%20which%20allow%20these%20models%20to%20find%20easy%0Ashortcuts%20-%20spurious%20correlation%20between%20features%20and%20labels%20-%20during%0Afine-tuning%20that%20unrealistically%20boost%20their%20performance.%20When%20such%20shortcuts%0Aare%20not%20present%20-%20as%20in%20real%20scenarios%20-%20these%20models%20perform%20poorly.%20We%20also%0Aintroduce%20Pcap-Encoder%2C%20an%20LM-based%20representation%20learning%20model%20that%20we%0Aspecifically%20design%20to%20extract%20features%20from%20protocol%20headers.%20Pcap-Encoder%0Aappears%20to%20be%20the%20only%20model%20that%20provides%20an%20instrumental%20representation%20for%0Atraffic%20classification.%20Yet%2C%20its%20complexity%20questions%20its%20applicability%20in%0Apractical%20settings.%20Our%20findings%20reveal%20flaws%20in%20dataset%20preparation%20and%20model%0Atraining%2C%20calling%20for%20a%20better%20and%20more%20conscious%20test%20design.%20We%20propose%20a%0Acorrect%20evaluation%20methodology%20and%20stress%20the%20need%20for%20rigorous%20benchmarking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16438v1&entry.124074799=Read"},
{"title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "author": "Shanbo Cheng and Yu Bao and Qian Cao and Luyang Huang and Liyan Kang and Zhicheng Liu and Yu Lu and Wenhao Zhu and Jingwen Chen and Zhichao Huang and Tao Li and Yifu Li and Huiying Lin and Sitong Liu and Ningxin Peng and Shuaijie She and Lu Xu and Nuo Xu and Sen Yang and Runsheng Yu and Yiming Yu and Liehao Zou and Hang Li and Lu Lu and Yuxuan Wang and Yonghui Wu", "abstract": "  Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.\n", "link": "http://arxiv.org/abs/2507.13618v2", "date": "2025-07-22", "relevancy": 2.5112, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seed-X%3A%20Building%20Strong%20Multilingual%20Translation%20LLM%20with%207B%20Parameters&body=Title%3A%20Seed-X%3A%20Building%20Strong%20Multilingual%20Translation%20LLM%20with%207B%20Parameters%0AAuthor%3A%20Shanbo%20Cheng%20and%20Yu%20Bao%20and%20Qian%20Cao%20and%20Luyang%20Huang%20and%20Liyan%20Kang%20and%20Zhicheng%20Liu%20and%20Yu%20Lu%20and%20Wenhao%20Zhu%20and%20Jingwen%20Chen%20and%20Zhichao%20Huang%20and%20Tao%20Li%20and%20Yifu%20Li%20and%20Huiying%20Lin%20and%20Sitong%20Liu%20and%20Ningxin%20Peng%20and%20Shuaijie%20She%20and%20Lu%20Xu%20and%20Nuo%20Xu%20and%20Sen%20Yang%20and%20Runsheng%20Yu%20and%20Yiming%20Yu%20and%20Liehao%20Zou%20and%20Hang%20Li%20and%20Lu%20Lu%20and%20Yuxuan%20Wang%20and%20Yonghui%20Wu%0AAbstract%3A%20%20%20Multilingual%20translation%20stands%20as%20a%20challenging%20task%20for%20large%20language%0Amodels%20%28LLMs%29%20to%20handle%20intricate%20language%20patterns%20and%20stilted%20translations%0Athat%20arise%20in%20automated%20translations.%20In%20this%20paper%2C%20we%20introduce%20Seed-X%2C%20a%0Afamily%20of%20open-source%20LLMs%20comprising%20instruct%20and%20reasoning%20models%2C%20pushing%0Athe%20limits%20of%20translation%20capability%20with%207B%20parameter%20size.%20The%20base%20model%20is%0Apre-trained%20on%20a%20diverse%2C%20high-quality%20dataset%20encompassing%20both%20monolingual%0Aand%20bilingual%20content%20across%2028%20languages%2C%20harnessing%20the%20full%20potential%20of%0Amultilingual%20data.%20The%20instruct%20model%20is%20then%20finetuned%20to%20translate%20by%0AChain-of-Thought%20%28CoT%29%20reasoning%20and%20further%20enhanced%20through%20reinforcement%0Alearning%20%28RL%29%20to%20achieve%20better%20generalization%20across%20diverse%20language%20pairs.%0ASeed-X%20achieves%20performance%20comparable%20to%20leading%20closed-source%20models%2C%0Aincluding%20Gemini-2.5%20and%20GPT-4o%2C%20across%2028%20languages%2C%20and%20significantly%0Aoutperforms%20larger%20open-source%20models%20in%20both%20automatic%20metrics%20and%20human%0Aevaluations.%20We%20share%20the%20best%20practices%20through%20our%20optimization%20process%2C%20and%0Amake%20the%20parameter%20public%20available%20for%20advancing%20translation%20research%20and%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13618v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeed-X%253A%2520Building%2520Strong%2520Multilingual%2520Translation%2520LLM%2520with%25207B%2520Parameters%26entry.906535625%3DShanbo%2520Cheng%2520and%2520Yu%2520Bao%2520and%2520Qian%2520Cao%2520and%2520Luyang%2520Huang%2520and%2520Liyan%2520Kang%2520and%2520Zhicheng%2520Liu%2520and%2520Yu%2520Lu%2520and%2520Wenhao%2520Zhu%2520and%2520Jingwen%2520Chen%2520and%2520Zhichao%2520Huang%2520and%2520Tao%2520Li%2520and%2520Yifu%2520Li%2520and%2520Huiying%2520Lin%2520and%2520Sitong%2520Liu%2520and%2520Ningxin%2520Peng%2520and%2520Shuaijie%2520She%2520and%2520Lu%2520Xu%2520and%2520Nuo%2520Xu%2520and%2520Sen%2520Yang%2520and%2520Runsheng%2520Yu%2520and%2520Yiming%2520Yu%2520and%2520Liehao%2520Zou%2520and%2520Hang%2520Li%2520and%2520Lu%2520Lu%2520and%2520Yuxuan%2520Wang%2520and%2520Yonghui%2520Wu%26entry.1292438233%3D%2520%2520Multilingual%2520translation%2520stands%2520as%2520a%2520challenging%2520task%2520for%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520handle%2520intricate%2520language%2520patterns%2520and%2520stilted%2520translations%250Athat%2520arise%2520in%2520automated%2520translations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Seed-X%252C%2520a%250Afamily%2520of%2520open-source%2520LLMs%2520comprising%2520instruct%2520and%2520reasoning%2520models%252C%2520pushing%250Athe%2520limits%2520of%2520translation%2520capability%2520with%25207B%2520parameter%2520size.%2520The%2520base%2520model%2520is%250Apre-trained%2520on%2520a%2520diverse%252C%2520high-quality%2520dataset%2520encompassing%2520both%2520monolingual%250Aand%2520bilingual%2520content%2520across%252028%2520languages%252C%2520harnessing%2520the%2520full%2520potential%2520of%250Amultilingual%2520data.%2520The%2520instruct%2520model%2520is%2520then%2520finetuned%2520to%2520translate%2520by%250AChain-of-Thought%2520%2528CoT%2529%2520reasoning%2520and%2520further%2520enhanced%2520through%2520reinforcement%250Alearning%2520%2528RL%2529%2520to%2520achieve%2520better%2520generalization%2520across%2520diverse%2520language%2520pairs.%250ASeed-X%2520achieves%2520performance%2520comparable%2520to%2520leading%2520closed-source%2520models%252C%250Aincluding%2520Gemini-2.5%2520and%2520GPT-4o%252C%2520across%252028%2520languages%252C%2520and%2520significantly%250Aoutperforms%2520larger%2520open-source%2520models%2520in%2520both%2520automatic%2520metrics%2520and%2520human%250Aevaluations.%2520We%2520share%2520the%2520best%2520practices%2520through%2520our%2520optimization%2520process%252C%2520and%250Amake%2520the%2520parameter%2520public%2520available%2520for%2520advancing%2520translation%2520research%2520and%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13618v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seed-X%3A%20Building%20Strong%20Multilingual%20Translation%20LLM%20with%207B%20Parameters&entry.906535625=Shanbo%20Cheng%20and%20Yu%20Bao%20and%20Qian%20Cao%20and%20Luyang%20Huang%20and%20Liyan%20Kang%20and%20Zhicheng%20Liu%20and%20Yu%20Lu%20and%20Wenhao%20Zhu%20and%20Jingwen%20Chen%20and%20Zhichao%20Huang%20and%20Tao%20Li%20and%20Yifu%20Li%20and%20Huiying%20Lin%20and%20Sitong%20Liu%20and%20Ningxin%20Peng%20and%20Shuaijie%20She%20and%20Lu%20Xu%20and%20Nuo%20Xu%20and%20Sen%20Yang%20and%20Runsheng%20Yu%20and%20Yiming%20Yu%20and%20Liehao%20Zou%20and%20Hang%20Li%20and%20Lu%20Lu%20and%20Yuxuan%20Wang%20and%20Yonghui%20Wu&entry.1292438233=%20%20Multilingual%20translation%20stands%20as%20a%20challenging%20task%20for%20large%20language%0Amodels%20%28LLMs%29%20to%20handle%20intricate%20language%20patterns%20and%20stilted%20translations%0Athat%20arise%20in%20automated%20translations.%20In%20this%20paper%2C%20we%20introduce%20Seed-X%2C%20a%0Afamily%20of%20open-source%20LLMs%20comprising%20instruct%20and%20reasoning%20models%2C%20pushing%0Athe%20limits%20of%20translation%20capability%20with%207B%20parameter%20size.%20The%20base%20model%20is%0Apre-trained%20on%20a%20diverse%2C%20high-quality%20dataset%20encompassing%20both%20monolingual%0Aand%20bilingual%20content%20across%2028%20languages%2C%20harnessing%20the%20full%20potential%20of%0Amultilingual%20data.%20The%20instruct%20model%20is%20then%20finetuned%20to%20translate%20by%0AChain-of-Thought%20%28CoT%29%20reasoning%20and%20further%20enhanced%20through%20reinforcement%0Alearning%20%28RL%29%20to%20achieve%20better%20generalization%20across%20diverse%20language%20pairs.%0ASeed-X%20achieves%20performance%20comparable%20to%20leading%20closed-source%20models%2C%0Aincluding%20Gemini-2.5%20and%20GPT-4o%2C%20across%2028%20languages%2C%20and%20significantly%0Aoutperforms%20larger%20open-source%20models%20in%20both%20automatic%20metrics%20and%20human%0Aevaluations.%20We%20share%20the%20best%20practices%20through%20our%20optimization%20process%2C%20and%0Amake%20the%20parameter%20public%20available%20for%20advancing%20translation%20research%20and%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13618v2&entry.124074799=Read"},
{"title": "A Survey of Deep Learning for Geometry Problem Solving", "author": "Jianzhe Ma and Wenxuan Wang and Qin Jin", "abstract": "  Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.\n", "link": "http://arxiv.org/abs/2507.11936v2", "date": "2025-07-22", "relevancy": 2.4959, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5005}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Deep%20Learning%20for%20Geometry%20Problem%20Solving&body=Title%3A%20A%20Survey%20of%20Deep%20Learning%20for%20Geometry%20Problem%20Solving%0AAuthor%3A%20Jianzhe%20Ma%20and%20Wenxuan%20Wang%20and%20Qin%20Jin%0AAbstract%3A%20%20%20Geometry%20problem%20solving%20is%20a%20key%20area%20of%20mathematical%20reasoning%2C%20which%20is%0Awidely%20involved%20in%20many%20important%20fields%20such%20as%20education%2C%20mathematical%0Aability%20assessment%20of%20artificial%20intelligence%2C%20and%20multimodal%20ability%0Aassessment.%20In%20recent%20years%2C%20the%20rapid%20development%20of%20deep%20learning%20technology%2C%0Aespecially%20the%20rise%20of%20multimodal%20large%20language%20models%2C%20has%20triggered%20a%0Awidespread%20research%20boom.%20This%20paper%20provides%20a%20survey%20of%20the%20applications%20of%0Adeep%20learning%20in%20geometry%20problem%20solving%2C%20including%20%28i%29%20a%20comprehensive%0Asummary%20of%20the%20relevant%20tasks%20in%20geometry%20problem%20solving%3B%20%28ii%29%20a%20thorough%0Areview%20of%20related%20deep%20learning%20methods%3B%20%28iii%29%20a%20detailed%20analysis%20of%0Aevaluation%20metrics%20and%20methods%3B%20and%20%28iv%29%20a%20critical%20discussion%20of%20the%20current%0Achallenges%20and%20future%20directions%20that%20can%20be%20explored.%20Our%20goal%20is%20to%20provide%20a%0Acomprehensive%20and%20practical%20reference%20of%20deep%20learning%20for%20geometry%20problem%0Asolving%20to%20promote%20further%20developments%20in%20this%20field.%20We%20create%20a%20continuously%0Aupdated%20list%20of%20papers%20on%20GitHub%3A%20https%3A//github.com/majianz/dl4gps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11936v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Deep%2520Learning%2520for%2520Geometry%2520Problem%2520Solving%26entry.906535625%3DJianzhe%2520Ma%2520and%2520Wenxuan%2520Wang%2520and%2520Qin%2520Jin%26entry.1292438233%3D%2520%2520Geometry%2520problem%2520solving%2520is%2520a%2520key%2520area%2520of%2520mathematical%2520reasoning%252C%2520which%2520is%250Awidely%2520involved%2520in%2520many%2520important%2520fields%2520such%2520as%2520education%252C%2520mathematical%250Aability%2520assessment%2520of%2520artificial%2520intelligence%252C%2520and%2520multimodal%2520ability%250Aassessment.%2520In%2520recent%2520years%252C%2520the%2520rapid%2520development%2520of%2520deep%2520learning%2520technology%252C%250Aespecially%2520the%2520rise%2520of%2520multimodal%2520large%2520language%2520models%252C%2520has%2520triggered%2520a%250Awidespread%2520research%2520boom.%2520This%2520paper%2520provides%2520a%2520survey%2520of%2520the%2520applications%2520of%250Adeep%2520learning%2520in%2520geometry%2520problem%2520solving%252C%2520including%2520%2528i%2529%2520a%2520comprehensive%250Asummary%2520of%2520the%2520relevant%2520tasks%2520in%2520geometry%2520problem%2520solving%253B%2520%2528ii%2529%2520a%2520thorough%250Areview%2520of%2520related%2520deep%2520learning%2520methods%253B%2520%2528iii%2529%2520a%2520detailed%2520analysis%2520of%250Aevaluation%2520metrics%2520and%2520methods%253B%2520and%2520%2528iv%2529%2520a%2520critical%2520discussion%2520of%2520the%2520current%250Achallenges%2520and%2520future%2520directions%2520that%2520can%2520be%2520explored.%2520Our%2520goal%2520is%2520to%2520provide%2520a%250Acomprehensive%2520and%2520practical%2520reference%2520of%2520deep%2520learning%2520for%2520geometry%2520problem%250Asolving%2520to%2520promote%2520further%2520developments%2520in%2520this%2520field.%2520We%2520create%2520a%2520continuously%250Aupdated%2520list%2520of%2520papers%2520on%2520GitHub%253A%2520https%253A//github.com/majianz/dl4gps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11936v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Deep%20Learning%20for%20Geometry%20Problem%20Solving&entry.906535625=Jianzhe%20Ma%20and%20Wenxuan%20Wang%20and%20Qin%20Jin&entry.1292438233=%20%20Geometry%20problem%20solving%20is%20a%20key%20area%20of%20mathematical%20reasoning%2C%20which%20is%0Awidely%20involved%20in%20many%20important%20fields%20such%20as%20education%2C%20mathematical%0Aability%20assessment%20of%20artificial%20intelligence%2C%20and%20multimodal%20ability%0Aassessment.%20In%20recent%20years%2C%20the%20rapid%20development%20of%20deep%20learning%20technology%2C%0Aespecially%20the%20rise%20of%20multimodal%20large%20language%20models%2C%20has%20triggered%20a%0Awidespread%20research%20boom.%20This%20paper%20provides%20a%20survey%20of%20the%20applications%20of%0Adeep%20learning%20in%20geometry%20problem%20solving%2C%20including%20%28i%29%20a%20comprehensive%0Asummary%20of%20the%20relevant%20tasks%20in%20geometry%20problem%20solving%3B%20%28ii%29%20a%20thorough%0Areview%20of%20related%20deep%20learning%20methods%3B%20%28iii%29%20a%20detailed%20analysis%20of%0Aevaluation%20metrics%20and%20methods%3B%20and%20%28iv%29%20a%20critical%20discussion%20of%20the%20current%0Achallenges%20and%20future%20directions%20that%20can%20be%20explored.%20Our%20goal%20is%20to%20provide%20a%0Acomprehensive%20and%20practical%20reference%20of%20deep%20learning%20for%20geometry%20problem%0Asolving%20to%20promote%20further%20developments%20in%20this%20field.%20We%20create%20a%20continuously%0Aupdated%20list%20of%20papers%20on%20GitHub%3A%20https%3A//github.com/majianz/dl4gps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11936v2&entry.124074799=Read"},
{"title": "Deep Unfolding Network for Nonlinear Multi-Frequency Electrical\n  Impedance Tomography", "author": "Giovanni S. Alberti and Damiana Lazzaro and Serena Morigi and Luca Ratti and Matteo Santacesaria", "abstract": "  Multi-frequency Electrical Impedance Tomography (mfEIT) represents a\npromising biomedical imaging modality that enables the estimation of tissue\nconductivities across a range of frequencies. Addressing this challenge, we\npresent a novel variational network, a model-based learning paradigm that\nstrategically merges the advantages and interpretability of classical iterative\nreconstruction with the power of deep learning. This approach integrates graph\nneural networks (GNNs) within the iterative Proximal Regularized Gauss Newton\n(PRGN) framework. By unrolling the PRGN algorithm, where each iteration\ncorresponds to a network layer, we leverage the physical insights of nonlinear\nmodel fitting alongside the GNN's capacity to capture inter-frequency\ncorrelations. Notably, the GNN architecture preserves the irregular triangular\nmesh structure used in the solution of the nonlinear forward model, enabling\naccurate reconstruction of overlapping tissue fraction concentrations.\n", "link": "http://arxiv.org/abs/2507.16678v1", "date": "2025-07-22", "relevancy": 2.454, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5071}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4838}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Unfolding%20Network%20for%20Nonlinear%20Multi-Frequency%20Electrical%0A%20%20Impedance%20Tomography&body=Title%3A%20Deep%20Unfolding%20Network%20for%20Nonlinear%20Multi-Frequency%20Electrical%0A%20%20Impedance%20Tomography%0AAuthor%3A%20Giovanni%20S.%20Alberti%20and%20Damiana%20Lazzaro%20and%20Serena%20Morigi%20and%20Luca%20Ratti%20and%20Matteo%20Santacesaria%0AAbstract%3A%20%20%20Multi-frequency%20Electrical%20Impedance%20Tomography%20%28mfEIT%29%20represents%20a%0Apromising%20biomedical%20imaging%20modality%20that%20enables%20the%20estimation%20of%20tissue%0Aconductivities%20across%20a%20range%20of%20frequencies.%20Addressing%20this%20challenge%2C%20we%0Apresent%20a%20novel%20variational%20network%2C%20a%20model-based%20learning%20paradigm%20that%0Astrategically%20merges%20the%20advantages%20and%20interpretability%20of%20classical%20iterative%0Areconstruction%20with%20the%20power%20of%20deep%20learning.%20This%20approach%20integrates%20graph%0Aneural%20networks%20%28GNNs%29%20within%20the%20iterative%20Proximal%20Regularized%20Gauss%20Newton%0A%28PRGN%29%20framework.%20By%20unrolling%20the%20PRGN%20algorithm%2C%20where%20each%20iteration%0Acorresponds%20to%20a%20network%20layer%2C%20we%20leverage%20the%20physical%20insights%20of%20nonlinear%0Amodel%20fitting%20alongside%20the%20GNN%27s%20capacity%20to%20capture%20inter-frequency%0Acorrelations.%20Notably%2C%20the%20GNN%20architecture%20preserves%20the%20irregular%20triangular%0Amesh%20structure%20used%20in%20the%20solution%20of%20the%20nonlinear%20forward%20model%2C%20enabling%0Aaccurate%20reconstruction%20of%20overlapping%20tissue%20fraction%20concentrations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Unfolding%2520Network%2520for%2520Nonlinear%2520Multi-Frequency%2520Electrical%250A%2520%2520Impedance%2520Tomography%26entry.906535625%3DGiovanni%2520S.%2520Alberti%2520and%2520Damiana%2520Lazzaro%2520and%2520Serena%2520Morigi%2520and%2520Luca%2520Ratti%2520and%2520Matteo%2520Santacesaria%26entry.1292438233%3D%2520%2520Multi-frequency%2520Electrical%2520Impedance%2520Tomography%2520%2528mfEIT%2529%2520represents%2520a%250Apromising%2520biomedical%2520imaging%2520modality%2520that%2520enables%2520the%2520estimation%2520of%2520tissue%250Aconductivities%2520across%2520a%2520range%2520of%2520frequencies.%2520Addressing%2520this%2520challenge%252C%2520we%250Apresent%2520a%2520novel%2520variational%2520network%252C%2520a%2520model-based%2520learning%2520paradigm%2520that%250Astrategically%2520merges%2520the%2520advantages%2520and%2520interpretability%2520of%2520classical%2520iterative%250Areconstruction%2520with%2520the%2520power%2520of%2520deep%2520learning.%2520This%2520approach%2520integrates%2520graph%250Aneural%2520networks%2520%2528GNNs%2529%2520within%2520the%2520iterative%2520Proximal%2520Regularized%2520Gauss%2520Newton%250A%2528PRGN%2529%2520framework.%2520By%2520unrolling%2520the%2520PRGN%2520algorithm%252C%2520where%2520each%2520iteration%250Acorresponds%2520to%2520a%2520network%2520layer%252C%2520we%2520leverage%2520the%2520physical%2520insights%2520of%2520nonlinear%250Amodel%2520fitting%2520alongside%2520the%2520GNN%2527s%2520capacity%2520to%2520capture%2520inter-frequency%250Acorrelations.%2520Notably%252C%2520the%2520GNN%2520architecture%2520preserves%2520the%2520irregular%2520triangular%250Amesh%2520structure%2520used%2520in%2520the%2520solution%2520of%2520the%2520nonlinear%2520forward%2520model%252C%2520enabling%250Aaccurate%2520reconstruction%2520of%2520overlapping%2520tissue%2520fraction%2520concentrations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Unfolding%20Network%20for%20Nonlinear%20Multi-Frequency%20Electrical%0A%20%20Impedance%20Tomography&entry.906535625=Giovanni%20S.%20Alberti%20and%20Damiana%20Lazzaro%20and%20Serena%20Morigi%20and%20Luca%20Ratti%20and%20Matteo%20Santacesaria&entry.1292438233=%20%20Multi-frequency%20Electrical%20Impedance%20Tomography%20%28mfEIT%29%20represents%20a%0Apromising%20biomedical%20imaging%20modality%20that%20enables%20the%20estimation%20of%20tissue%0Aconductivities%20across%20a%20range%20of%20frequencies.%20Addressing%20this%20challenge%2C%20we%0Apresent%20a%20novel%20variational%20network%2C%20a%20model-based%20learning%20paradigm%20that%0Astrategically%20merges%20the%20advantages%20and%20interpretability%20of%20classical%20iterative%0Areconstruction%20with%20the%20power%20of%20deep%20learning.%20This%20approach%20integrates%20graph%0Aneural%20networks%20%28GNNs%29%20within%20the%20iterative%20Proximal%20Regularized%20Gauss%20Newton%0A%28PRGN%29%20framework.%20By%20unrolling%20the%20PRGN%20algorithm%2C%20where%20each%20iteration%0Acorresponds%20to%20a%20network%20layer%2C%20we%20leverage%20the%20physical%20insights%20of%20nonlinear%0Amodel%20fitting%20alongside%20the%20GNN%27s%20capacity%20to%20capture%20inter-frequency%0Acorrelations.%20Notably%2C%20the%20GNN%20architecture%20preserves%20the%20irregular%20triangular%0Amesh%20structure%20used%20in%20the%20solution%20of%20the%20nonlinear%20forward%20model%2C%20enabling%0Aaccurate%20reconstruction%20of%20overlapping%20tissue%20fraction%20concentrations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16678v1&entry.124074799=Read"},
{"title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion", "author": "Shang Liu and Chenjie Cao and Chaohui Yu and Wen Qian and Jing Wang and Fan Wang", "abstract": "  Despite the remarkable developments achieved by recent 3D generation works,\nscaling these methods to geographic extents, such as modeling thousands of\nsquare kilometers of Earth's surface, remains an open challenge. We address\nthis through a dual innovation in data infrastructure and model architecture.\nFirst, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,\nconsisting of 50k curated scenes (each measuring 600m x 600m) captured across\nthe U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene\nprovides pose-annotated multi-view images, depth maps, normals, semantic\nsegmentation, and camera poses, with explicit quality control to ensure terrain\ndiversity. Building on this foundation, we propose EarthCrafter, a tailored\nframework for large-scale 3D Earth generation via sparse-decoupled latent\ndiffusion. Our architecture separates structural and textural generation: 1)\nDual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D\nGaussian Splats (2DGS) into compact latent spaces, largely alleviating the\ncostly computation suffering from vast geographic scales while preserving\ncritical information. 2) We propose condition-aware flow matching models\ntrained on mixed inputs (semantics, images, or neither) to flexibly model\nlatent geometry and texture features independently. Extensive experiments\ndemonstrate that EarthCrafter performs substantially better in extremely\nlarge-scale generation. The framework further supports versatile applications,\nfrom semantic-guided urban layout generation to unconditional terrain\nsynthesis, while maintaining geographic plausibility through our rich data\npriors from Aerial-Earth3D.\n", "link": "http://arxiv.org/abs/2507.16535v1", "date": "2025-07-22", "relevancy": 2.4519, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6174}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6174}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EarthCrafter%3A%20Scalable%203D%20Earth%20Generation%20via%20Dual-Sparse%20Latent%0A%20%20Diffusion&body=Title%3A%20EarthCrafter%3A%20Scalable%203D%20Earth%20Generation%20via%20Dual-Sparse%20Latent%0A%20%20Diffusion%0AAuthor%3A%20Shang%20Liu%20and%20Chenjie%20Cao%20and%20Chaohui%20Yu%20and%20Wen%20Qian%20and%20Jing%20Wang%20and%20Fan%20Wang%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20developments%20achieved%20by%20recent%203D%20generation%20works%2C%0Ascaling%20these%20methods%20to%20geographic%20extents%2C%20such%20as%20modeling%20thousands%20of%0Asquare%20kilometers%20of%20Earth%27s%20surface%2C%20remains%20an%20open%20challenge.%20We%20address%0Athis%20through%20a%20dual%20innovation%20in%20data%20infrastructure%20and%20model%20architecture.%0AFirst%2C%20we%20introduce%20Aerial-Earth3D%2C%20the%20largest%203D%20aerial%20dataset%20to%20date%2C%0Aconsisting%20of%2050k%20curated%20scenes%20%28each%20measuring%20600m%20x%20600m%29%20captured%20across%0Athe%20U.S.%20mainland%2C%20comprising%2045M%20multi-view%20Google%20Earth%20frames.%20Each%20scene%0Aprovides%20pose-annotated%20multi-view%20images%2C%20depth%20maps%2C%20normals%2C%20semantic%0Asegmentation%2C%20and%20camera%20poses%2C%20with%20explicit%20quality%20control%20to%20ensure%20terrain%0Adiversity.%20Building%20on%20this%20foundation%2C%20we%20propose%20EarthCrafter%2C%20a%20tailored%0Aframework%20for%20large-scale%203D%20Earth%20generation%20via%20sparse-decoupled%20latent%0Adiffusion.%20Our%20architecture%20separates%20structural%20and%20textural%20generation%3A%201%29%0ADual%20sparse%203D-VAEs%20compress%20high-resolution%20geometric%20voxels%20and%20textural%202D%0AGaussian%20Splats%20%282DGS%29%20into%20compact%20latent%20spaces%2C%20largely%20alleviating%20the%0Acostly%20computation%20suffering%20from%20vast%20geographic%20scales%20while%20preserving%0Acritical%20information.%202%29%20We%20propose%20condition-aware%20flow%20matching%20models%0Atrained%20on%20mixed%20inputs%20%28semantics%2C%20images%2C%20or%20neither%29%20to%20flexibly%20model%0Alatent%20geometry%20and%20texture%20features%20independently.%20Extensive%20experiments%0Ademonstrate%20that%20EarthCrafter%20performs%20substantially%20better%20in%20extremely%0Alarge-scale%20generation.%20The%20framework%20further%20supports%20versatile%20applications%2C%0Afrom%20semantic-guided%20urban%20layout%20generation%20to%20unconditional%20terrain%0Asynthesis%2C%20while%20maintaining%20geographic%20plausibility%20through%20our%20rich%20data%0Apriors%20from%20Aerial-Earth3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarthCrafter%253A%2520Scalable%25203D%2520Earth%2520Generation%2520via%2520Dual-Sparse%2520Latent%250A%2520%2520Diffusion%26entry.906535625%3DShang%2520Liu%2520and%2520Chenjie%2520Cao%2520and%2520Chaohui%2520Yu%2520and%2520Wen%2520Qian%2520and%2520Jing%2520Wang%2520and%2520Fan%2520Wang%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520developments%2520achieved%2520by%2520recent%25203D%2520generation%2520works%252C%250Ascaling%2520these%2520methods%2520to%2520geographic%2520extents%252C%2520such%2520as%2520modeling%2520thousands%2520of%250Asquare%2520kilometers%2520of%2520Earth%2527s%2520surface%252C%2520remains%2520an%2520open%2520challenge.%2520We%2520address%250Athis%2520through%2520a%2520dual%2520innovation%2520in%2520data%2520infrastructure%2520and%2520model%2520architecture.%250AFirst%252C%2520we%2520introduce%2520Aerial-Earth3D%252C%2520the%2520largest%25203D%2520aerial%2520dataset%2520to%2520date%252C%250Aconsisting%2520of%252050k%2520curated%2520scenes%2520%2528each%2520measuring%2520600m%2520x%2520600m%2529%2520captured%2520across%250Athe%2520U.S.%2520mainland%252C%2520comprising%252045M%2520multi-view%2520Google%2520Earth%2520frames.%2520Each%2520scene%250Aprovides%2520pose-annotated%2520multi-view%2520images%252C%2520depth%2520maps%252C%2520normals%252C%2520semantic%250Asegmentation%252C%2520and%2520camera%2520poses%252C%2520with%2520explicit%2520quality%2520control%2520to%2520ensure%2520terrain%250Adiversity.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520propose%2520EarthCrafter%252C%2520a%2520tailored%250Aframework%2520for%2520large-scale%25203D%2520Earth%2520generation%2520via%2520sparse-decoupled%2520latent%250Adiffusion.%2520Our%2520architecture%2520separates%2520structural%2520and%2520textural%2520generation%253A%25201%2529%250ADual%2520sparse%25203D-VAEs%2520compress%2520high-resolution%2520geometric%2520voxels%2520and%2520textural%25202D%250AGaussian%2520Splats%2520%25282DGS%2529%2520into%2520compact%2520latent%2520spaces%252C%2520largely%2520alleviating%2520the%250Acostly%2520computation%2520suffering%2520from%2520vast%2520geographic%2520scales%2520while%2520preserving%250Acritical%2520information.%25202%2529%2520We%2520propose%2520condition-aware%2520flow%2520matching%2520models%250Atrained%2520on%2520mixed%2520inputs%2520%2528semantics%252C%2520images%252C%2520or%2520neither%2529%2520to%2520flexibly%2520model%250Alatent%2520geometry%2520and%2520texture%2520features%2520independently.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520EarthCrafter%2520performs%2520substantially%2520better%2520in%2520extremely%250Alarge-scale%2520generation.%2520The%2520framework%2520further%2520supports%2520versatile%2520applications%252C%250Afrom%2520semantic-guided%2520urban%2520layout%2520generation%2520to%2520unconditional%2520terrain%250Asynthesis%252C%2520while%2520maintaining%2520geographic%2520plausibility%2520through%2520our%2520rich%2520data%250Apriors%2520from%2520Aerial-Earth3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EarthCrafter%3A%20Scalable%203D%20Earth%20Generation%20via%20Dual-Sparse%20Latent%0A%20%20Diffusion&entry.906535625=Shang%20Liu%20and%20Chenjie%20Cao%20and%20Chaohui%20Yu%20and%20Wen%20Qian%20and%20Jing%20Wang%20and%20Fan%20Wang&entry.1292438233=%20%20Despite%20the%20remarkable%20developments%20achieved%20by%20recent%203D%20generation%20works%2C%0Ascaling%20these%20methods%20to%20geographic%20extents%2C%20such%20as%20modeling%20thousands%20of%0Asquare%20kilometers%20of%20Earth%27s%20surface%2C%20remains%20an%20open%20challenge.%20We%20address%0Athis%20through%20a%20dual%20innovation%20in%20data%20infrastructure%20and%20model%20architecture.%0AFirst%2C%20we%20introduce%20Aerial-Earth3D%2C%20the%20largest%203D%20aerial%20dataset%20to%20date%2C%0Aconsisting%20of%2050k%20curated%20scenes%20%28each%20measuring%20600m%20x%20600m%29%20captured%20across%0Athe%20U.S.%20mainland%2C%20comprising%2045M%20multi-view%20Google%20Earth%20frames.%20Each%20scene%0Aprovides%20pose-annotated%20multi-view%20images%2C%20depth%20maps%2C%20normals%2C%20semantic%0Asegmentation%2C%20and%20camera%20poses%2C%20with%20explicit%20quality%20control%20to%20ensure%20terrain%0Adiversity.%20Building%20on%20this%20foundation%2C%20we%20propose%20EarthCrafter%2C%20a%20tailored%0Aframework%20for%20large-scale%203D%20Earth%20generation%20via%20sparse-decoupled%20latent%0Adiffusion.%20Our%20architecture%20separates%20structural%20and%20textural%20generation%3A%201%29%0ADual%20sparse%203D-VAEs%20compress%20high-resolution%20geometric%20voxels%20and%20textural%202D%0AGaussian%20Splats%20%282DGS%29%20into%20compact%20latent%20spaces%2C%20largely%20alleviating%20the%0Acostly%20computation%20suffering%20from%20vast%20geographic%20scales%20while%20preserving%0Acritical%20information.%202%29%20We%20propose%20condition-aware%20flow%20matching%20models%0Atrained%20on%20mixed%20inputs%20%28semantics%2C%20images%2C%20or%20neither%29%20to%20flexibly%20model%0Alatent%20geometry%20and%20texture%20features%20independently.%20Extensive%20experiments%0Ademonstrate%20that%20EarthCrafter%20performs%20substantially%20better%20in%20extremely%0Alarge-scale%20generation.%20The%20framework%20further%20supports%20versatile%20applications%2C%0Afrom%20semantic-guided%20urban%20layout%20generation%20to%20unconditional%20terrain%0Asynthesis%2C%20while%20maintaining%20geographic%20plausibility%20through%20our%20rich%20data%0Apriors%20from%20Aerial-Earth3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16535v1&entry.124074799=Read"},
{"title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction", "author": "Zhixiong Zhang and Shuangrui Ding and Xiaoyi Dong and Songxin He and Jianfan Lin and Junsong Tang and Yuhang Zang and Yuhang Cao and Dahua Lin and Jiaqi Wang", "abstract": "  Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.\n", "link": "http://arxiv.org/abs/2507.15852v2", "date": "2025-07-22", "relevancy": 2.4372, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6251}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6251}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeC%3A%20Advancing%20Complex%20Video%20Object%20Segmentation%20via%20Progressive%20Concept%0A%20%20Construction&body=Title%3A%20SeC%3A%20Advancing%20Complex%20Video%20Object%20Segmentation%20via%20Progressive%20Concept%0A%20%20Construction%0AAuthor%3A%20Zhixiong%20Zhang%20and%20Shuangrui%20Ding%20and%20Xiaoyi%20Dong%20and%20Songxin%20He%20and%20Jianfan%20Lin%20and%20Junsong%20Tang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20Video%20Object%20Segmentation%20%28VOS%29%20is%20a%20core%20task%20in%20computer%20vision%2C%20requiring%0Amodels%20to%20track%20and%20segment%20target%20objects%20across%20video%20frames.%20Despite%20notable%0Aadvances%20with%20recent%20efforts%2C%20current%20techniques%20still%20lag%20behind%20human%0Acapabilities%20in%20handling%20drastic%20visual%20variations%2C%20occlusions%2C%20and%20complex%0Ascene%20changes.%20This%20limitation%20arises%20from%20their%20reliance%20on%20appearance%0Amatching%2C%20neglecting%20the%20human-like%20conceptual%20understanding%20of%20objects%20that%0Aenables%20robust%20identification%20across%20temporal%20dynamics.%20Motivated%20by%20this%20gap%2C%0Awe%20propose%20Segment%20Concept%20%28SeC%29%2C%20a%20concept-driven%20segmentation%20framework%20that%0Ashifts%20from%20conventional%20feature%20matching%20to%20the%20progressive%20construction%20and%0Autilization%20of%20high-level%2C%20object-centric%20representations.%20SeC%20employs%20Large%0AVision-Language%20Models%20%28LVLMs%29%20to%20integrate%20visual%20cues%20across%20diverse%20frames%2C%0Aconstructing%20robust%20conceptual%20priors.%20During%20inference%2C%20SeC%20forms%20a%0Acomprehensive%20semantic%20representation%20of%20the%20target%20based%20on%20processed%20frames%2C%0Arealizing%20robust%20segmentation%20of%20follow-up%20frames.%20Furthermore%2C%20SeC%20adaptively%0Abalances%20LVLM-based%20semantic%20reasoning%20with%20enhanced%20feature%20matching%2C%0Adynamically%20adjusting%20computational%20efforts%20based%20on%20scene%20complexity.%20To%0Arigorously%20assess%20VOS%20methods%20in%20scenarios%20demanding%20high-level%20conceptual%0Areasoning%20and%20robust%20semantic%20understanding%2C%20we%20introduce%20the%20Semantic%20Complex%0AScenarios%20Video%20Object%20Segmentation%20benchmark%20%28SeCVOS%29.%20SeCVOS%20comprises%20160%0Amanually%20annotated%20multi-scenario%20videos%20designed%20to%20challenge%20models%20with%0Asubstantial%20appearance%20variations%20and%20dynamic%20scene%20transformations.%20In%0Aparticular%2C%20SeC%20achieves%20an%2011.8-point%20improvement%20over%20SAM%202.1%20on%20SeCVOS%2C%0Aestablishing%20a%20new%20state-of-the-art%20in%20concept-aware%20video%20object%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15852v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeC%253A%2520Advancing%2520Complex%2520Video%2520Object%2520Segmentation%2520via%2520Progressive%2520Concept%250A%2520%2520Construction%26entry.906535625%3DZhixiong%2520Zhang%2520and%2520Shuangrui%2520Ding%2520and%2520Xiaoyi%2520Dong%2520and%2520Songxin%2520He%2520and%2520Jianfan%2520Lin%2520and%2520Junsong%2520Tang%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520Video%2520Object%2520Segmentation%2520%2528VOS%2529%2520is%2520a%2520core%2520task%2520in%2520computer%2520vision%252C%2520requiring%250Amodels%2520to%2520track%2520and%2520segment%2520target%2520objects%2520across%2520video%2520frames.%2520Despite%2520notable%250Aadvances%2520with%2520recent%2520efforts%252C%2520current%2520techniques%2520still%2520lag%2520behind%2520human%250Acapabilities%2520in%2520handling%2520drastic%2520visual%2520variations%252C%2520occlusions%252C%2520and%2520complex%250Ascene%2520changes.%2520This%2520limitation%2520arises%2520from%2520their%2520reliance%2520on%2520appearance%250Amatching%252C%2520neglecting%2520the%2520human-like%2520conceptual%2520understanding%2520of%2520objects%2520that%250Aenables%2520robust%2520identification%2520across%2520temporal%2520dynamics.%2520Motivated%2520by%2520this%2520gap%252C%250Awe%2520propose%2520Segment%2520Concept%2520%2528SeC%2529%252C%2520a%2520concept-driven%2520segmentation%2520framework%2520that%250Ashifts%2520from%2520conventional%2520feature%2520matching%2520to%2520the%2520progressive%2520construction%2520and%250Autilization%2520of%2520high-level%252C%2520object-centric%2520representations.%2520SeC%2520employs%2520Large%250AVision-Language%2520Models%2520%2528LVLMs%2529%2520to%2520integrate%2520visual%2520cues%2520across%2520diverse%2520frames%252C%250Aconstructing%2520robust%2520conceptual%2520priors.%2520During%2520inference%252C%2520SeC%2520forms%2520a%250Acomprehensive%2520semantic%2520representation%2520of%2520the%2520target%2520based%2520on%2520processed%2520frames%252C%250Arealizing%2520robust%2520segmentation%2520of%2520follow-up%2520frames.%2520Furthermore%252C%2520SeC%2520adaptively%250Abalances%2520LVLM-based%2520semantic%2520reasoning%2520with%2520enhanced%2520feature%2520matching%252C%250Adynamically%2520adjusting%2520computational%2520efforts%2520based%2520on%2520scene%2520complexity.%2520To%250Arigorously%2520assess%2520VOS%2520methods%2520in%2520scenarios%2520demanding%2520high-level%2520conceptual%250Areasoning%2520and%2520robust%2520semantic%2520understanding%252C%2520we%2520introduce%2520the%2520Semantic%2520Complex%250AScenarios%2520Video%2520Object%2520Segmentation%2520benchmark%2520%2528SeCVOS%2529.%2520SeCVOS%2520comprises%2520160%250Amanually%2520annotated%2520multi-scenario%2520videos%2520designed%2520to%2520challenge%2520models%2520with%250Asubstantial%2520appearance%2520variations%2520and%2520dynamic%2520scene%2520transformations.%2520In%250Aparticular%252C%2520SeC%2520achieves%2520an%252011.8-point%2520improvement%2520over%2520SAM%25202.1%2520on%2520SeCVOS%252C%250Aestablishing%2520a%2520new%2520state-of-the-art%2520in%2520concept-aware%2520video%2520object%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15852v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeC%3A%20Advancing%20Complex%20Video%20Object%20Segmentation%20via%20Progressive%20Concept%0A%20%20Construction&entry.906535625=Zhixiong%20Zhang%20and%20Shuangrui%20Ding%20and%20Xiaoyi%20Dong%20and%20Songxin%20He%20and%20Jianfan%20Lin%20and%20Junsong%20Tang%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20Video%20Object%20Segmentation%20%28VOS%29%20is%20a%20core%20task%20in%20computer%20vision%2C%20requiring%0Amodels%20to%20track%20and%20segment%20target%20objects%20across%20video%20frames.%20Despite%20notable%0Aadvances%20with%20recent%20efforts%2C%20current%20techniques%20still%20lag%20behind%20human%0Acapabilities%20in%20handling%20drastic%20visual%20variations%2C%20occlusions%2C%20and%20complex%0Ascene%20changes.%20This%20limitation%20arises%20from%20their%20reliance%20on%20appearance%0Amatching%2C%20neglecting%20the%20human-like%20conceptual%20understanding%20of%20objects%20that%0Aenables%20robust%20identification%20across%20temporal%20dynamics.%20Motivated%20by%20this%20gap%2C%0Awe%20propose%20Segment%20Concept%20%28SeC%29%2C%20a%20concept-driven%20segmentation%20framework%20that%0Ashifts%20from%20conventional%20feature%20matching%20to%20the%20progressive%20construction%20and%0Autilization%20of%20high-level%2C%20object-centric%20representations.%20SeC%20employs%20Large%0AVision-Language%20Models%20%28LVLMs%29%20to%20integrate%20visual%20cues%20across%20diverse%20frames%2C%0Aconstructing%20robust%20conceptual%20priors.%20During%20inference%2C%20SeC%20forms%20a%0Acomprehensive%20semantic%20representation%20of%20the%20target%20based%20on%20processed%20frames%2C%0Arealizing%20robust%20segmentation%20of%20follow-up%20frames.%20Furthermore%2C%20SeC%20adaptively%0Abalances%20LVLM-based%20semantic%20reasoning%20with%20enhanced%20feature%20matching%2C%0Adynamically%20adjusting%20computational%20efforts%20based%20on%20scene%20complexity.%20To%0Arigorously%20assess%20VOS%20methods%20in%20scenarios%20demanding%20high-level%20conceptual%0Areasoning%20and%20robust%20semantic%20understanding%2C%20we%20introduce%20the%20Semantic%20Complex%0AScenarios%20Video%20Object%20Segmentation%20benchmark%20%28SeCVOS%29.%20SeCVOS%20comprises%20160%0Amanually%20annotated%20multi-scenario%20videos%20designed%20to%20challenge%20models%20with%0Asubstantial%20appearance%20variations%20and%20dynamic%20scene%20transformations.%20In%0Aparticular%2C%20SeC%20achieves%20an%2011.8-point%20improvement%20over%20SAM%202.1%20on%20SeCVOS%2C%0Aestablishing%20a%20new%20state-of-the-art%20in%20concept-aware%20video%20object%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15852v2&entry.124074799=Read"},
{"title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in\n  Pruned Large Vision-Language Models", "author": "Yue Li and Xin Yi and Dongsheng Shi and Gerard de Melo and Xiaoling Wang and Linlin Wang", "abstract": "  With the increasing size of Large Vision-Language Models (LVLMs), network\npruning techniques aimed at compressing models for deployment in\nresource-constrained environments have garnered significant attention. However,\nwe observe that pruning often leads to a degradation in safety performance. To\naddress this issue, we present a novel and lightweight approach, termed\nHierarchical Safety Realignment (HSR). HSR operates by first quantifying the\ncontribution of each attention head to safety, identifying the most critical\nones, and then selectively restoring neurons directly within these attention\nheads that play a pivotal role in maintaining safety. This process\nhierarchically realigns the safety of pruned LVLMs, progressing from the\nattention head level to the neuron level. We validate HSR across various models\nand pruning strategies, consistently achieving notable improvements in safety\nperformance. To our knowledge, this is the first work explicitly focused on\nrestoring safety in LVLMs post-pruning.\n", "link": "http://arxiv.org/abs/2505.16104v2", "date": "2025-07-22", "relevancy": 2.398, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4815}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4815}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Safety%20Realignment%3A%20Lightweight%20Restoration%20of%20Safety%20in%0A%20%20Pruned%20Large%20Vision-Language%20Models&body=Title%3A%20Hierarchical%20Safety%20Realignment%3A%20Lightweight%20Restoration%20of%20Safety%20in%0A%20%20Pruned%20Large%20Vision-Language%20Models%0AAuthor%3A%20Yue%20Li%20and%20Xin%20Yi%20and%20Dongsheng%20Shi%20and%20Gerard%20de%20Melo%20and%20Xiaoling%20Wang%20and%20Linlin%20Wang%0AAbstract%3A%20%20%20With%20the%20increasing%20size%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20network%0Apruning%20techniques%20aimed%20at%20compressing%20models%20for%20deployment%20in%0Aresource-constrained%20environments%20have%20garnered%20significant%20attention.%20However%2C%0Awe%20observe%20that%20pruning%20often%20leads%20to%20a%20degradation%20in%20safety%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20present%20a%20novel%20and%20lightweight%20approach%2C%20termed%0AHierarchical%20Safety%20Realignment%20%28HSR%29.%20HSR%20operates%20by%20first%20quantifying%20the%0Acontribution%20of%20each%20attention%20head%20to%20safety%2C%20identifying%20the%20most%20critical%0Aones%2C%20and%20then%20selectively%20restoring%20neurons%20directly%20within%20these%20attention%0Aheads%20that%20play%20a%20pivotal%20role%20in%20maintaining%20safety.%20This%20process%0Ahierarchically%20realigns%20the%20safety%20of%20pruned%20LVLMs%2C%20progressing%20from%20the%0Aattention%20head%20level%20to%20the%20neuron%20level.%20We%20validate%20HSR%20across%20various%20models%0Aand%20pruning%20strategies%2C%20consistently%20achieving%20notable%20improvements%20in%20safety%0Aperformance.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20explicitly%20focused%20on%0Arestoring%20safety%20in%20LVLMs%20post-pruning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16104v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Safety%2520Realignment%253A%2520Lightweight%2520Restoration%2520of%2520Safety%2520in%250A%2520%2520Pruned%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DYue%2520Li%2520and%2520Xin%2520Yi%2520and%2520Dongsheng%2520Shi%2520and%2520Gerard%2520de%2520Melo%2520and%2520Xiaoling%2520Wang%2520and%2520Linlin%2520Wang%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520size%2520of%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%252C%2520network%250Apruning%2520techniques%2520aimed%2520at%2520compressing%2520models%2520for%2520deployment%2520in%250Aresource-constrained%2520environments%2520have%2520garnered%2520significant%2520attention.%2520However%252C%250Awe%2520observe%2520that%2520pruning%2520often%2520leads%2520to%2520a%2520degradation%2520in%2520safety%2520performance.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520present%2520a%2520novel%2520and%2520lightweight%2520approach%252C%2520termed%250AHierarchical%2520Safety%2520Realignment%2520%2528HSR%2529.%2520HSR%2520operates%2520by%2520first%2520quantifying%2520the%250Acontribution%2520of%2520each%2520attention%2520head%2520to%2520safety%252C%2520identifying%2520the%2520most%2520critical%250Aones%252C%2520and%2520then%2520selectively%2520restoring%2520neurons%2520directly%2520within%2520these%2520attention%250Aheads%2520that%2520play%2520a%2520pivotal%2520role%2520in%2520maintaining%2520safety.%2520This%2520process%250Ahierarchically%2520realigns%2520the%2520safety%2520of%2520pruned%2520LVLMs%252C%2520progressing%2520from%2520the%250Aattention%2520head%2520level%2520to%2520the%2520neuron%2520level.%2520We%2520validate%2520HSR%2520across%2520various%2520models%250Aand%2520pruning%2520strategies%252C%2520consistently%2520achieving%2520notable%2520improvements%2520in%2520safety%250Aperformance.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520explicitly%2520focused%2520on%250Arestoring%2520safety%2520in%2520LVLMs%2520post-pruning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16104v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Safety%20Realignment%3A%20Lightweight%20Restoration%20of%20Safety%20in%0A%20%20Pruned%20Large%20Vision-Language%20Models&entry.906535625=Yue%20Li%20and%20Xin%20Yi%20and%20Dongsheng%20Shi%20and%20Gerard%20de%20Melo%20and%20Xiaoling%20Wang%20and%20Linlin%20Wang&entry.1292438233=%20%20With%20the%20increasing%20size%20of%20Large%20Vision-Language%20Models%20%28LVLMs%29%2C%20network%0Apruning%20techniques%20aimed%20at%20compressing%20models%20for%20deployment%20in%0Aresource-constrained%20environments%20have%20garnered%20significant%20attention.%20However%2C%0Awe%20observe%20that%20pruning%20often%20leads%20to%20a%20degradation%20in%20safety%20performance.%20To%0Aaddress%20this%20issue%2C%20we%20present%20a%20novel%20and%20lightweight%20approach%2C%20termed%0AHierarchical%20Safety%20Realignment%20%28HSR%29.%20HSR%20operates%20by%20first%20quantifying%20the%0Acontribution%20of%20each%20attention%20head%20to%20safety%2C%20identifying%20the%20most%20critical%0Aones%2C%20and%20then%20selectively%20restoring%20neurons%20directly%20within%20these%20attention%0Aheads%20that%20play%20a%20pivotal%20role%20in%20maintaining%20safety.%20This%20process%0Ahierarchically%20realigns%20the%20safety%20of%20pruned%20LVLMs%2C%20progressing%20from%20the%0Aattention%20head%20level%20to%20the%20neuron%20level.%20We%20validate%20HSR%20across%20various%20models%0Aand%20pruning%20strategies%2C%20consistently%20achieving%20notable%20improvements%20in%20safety%0Aperformance.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20explicitly%20focused%20on%0Arestoring%20safety%20in%20LVLMs%20post-pruning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16104v2&entry.124074799=Read"},
{"title": "HOComp: Interaction-Aware Human-Object Composition", "author": "Dong Liang and Jinyuan Jia and Yuhao Liu and Rynson W. H. Lau", "abstract": "  While existing image-guided composition methods may help insert a foreground\nobject onto a user-specified region of a background image, achieving natural\nblending inside the region with the rest of the image unchanged, we observe\nthat these existing methods often struggle in synthesizing seamless\ninteraction-aware compositions when the task involves human-object\ninteractions. In this paper, we first propose HOComp, a novel approach for\ncompositing a foreground object onto a human-centric background image, while\nensuring harmonious interactions between the foreground object and the\nbackground person and their consistent appearances. Our approach includes two\nkey designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes\nMLLMs to identify the interaction region as well as the interaction type (e.g.,\nholding and lefting) to provide coarse-to-fine constraints to the generated\npose for the interaction while incorporating human pose landmarks to track\naction variations and enforcing fine-grained pose constraints; and (2)\nDetail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware\nattention modulation mechanism, a multi-view appearance loss, and a background\nconsistency loss to ensure consistent shapes/textures of the foreground and\nfaithful reproduction of the background human. We then propose the first\ndataset, named Interaction-aware Human-Object Composition (IHOC), for the task.\nExperimental results on our dataset show that HOComp effectively generates\nharmonious human-object interactions with consistent appearances, and\noutperforms relevant methods qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2507.16813v1", "date": "2025-07-22", "relevancy": 2.3943, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6413}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5935}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HOComp%3A%20Interaction-Aware%20Human-Object%20Composition&body=Title%3A%20HOComp%3A%20Interaction-Aware%20Human-Object%20Composition%0AAuthor%3A%20Dong%20Liang%20and%20Jinyuan%20Jia%20and%20Yuhao%20Liu%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20While%20existing%20image-guided%20composition%20methods%20may%20help%20insert%20a%20foreground%0Aobject%20onto%20a%20user-specified%20region%20of%20a%20background%20image%2C%20achieving%20natural%0Ablending%20inside%20the%20region%20with%20the%20rest%20of%20the%20image%20unchanged%2C%20we%20observe%0Athat%20these%20existing%20methods%20often%20struggle%20in%20synthesizing%20seamless%0Ainteraction-aware%20compositions%20when%20the%20task%20involves%20human-object%0Ainteractions.%20In%20this%20paper%2C%20we%20first%20propose%20HOComp%2C%20a%20novel%20approach%20for%0Acompositing%20a%20foreground%20object%20onto%20a%20human-centric%20background%20image%2C%20while%0Aensuring%20harmonious%20interactions%20between%20the%20foreground%20object%20and%20the%0Abackground%20person%20and%20their%20consistent%20appearances.%20Our%20approach%20includes%20two%0Akey%20designs%3A%20%281%29%20MLLMs-driven%20Region-based%20Pose%20Guidance%20%28MRPG%29%2C%20which%20utilizes%0AMLLMs%20to%20identify%20the%20interaction%20region%20as%20well%20as%20the%20interaction%20type%20%28e.g.%2C%0Aholding%20and%20lefting%29%20to%20provide%20coarse-to-fine%20constraints%20to%20the%20generated%0Apose%20for%20the%20interaction%20while%20incorporating%20human%20pose%20landmarks%20to%20track%0Aaction%20variations%20and%20enforcing%20fine-grained%20pose%20constraints%3B%20and%20%282%29%0ADetail-Consistent%20Appearance%20Preservation%20%28DCAP%29%2C%20which%20unifies%20a%20shape-aware%0Aattention%20modulation%20mechanism%2C%20a%20multi-view%20appearance%20loss%2C%20and%20a%20background%0Aconsistency%20loss%20to%20ensure%20consistent%20shapes/textures%20of%20the%20foreground%20and%0Afaithful%20reproduction%20of%20the%20background%20human.%20We%20then%20propose%20the%20first%0Adataset%2C%20named%20Interaction-aware%20Human-Object%20Composition%20%28IHOC%29%2C%20for%20the%20task.%0AExperimental%20results%20on%20our%20dataset%20show%20that%20HOComp%20effectively%20generates%0Aharmonious%20human-object%20interactions%20with%20consistent%20appearances%2C%20and%0Aoutperforms%20relevant%20methods%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHOComp%253A%2520Interaction-Aware%2520Human-Object%2520Composition%26entry.906535625%3DDong%2520Liang%2520and%2520Jinyuan%2520Jia%2520and%2520Yuhao%2520Liu%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520While%2520existing%2520image-guided%2520composition%2520methods%2520may%2520help%2520insert%2520a%2520foreground%250Aobject%2520onto%2520a%2520user-specified%2520region%2520of%2520a%2520background%2520image%252C%2520achieving%2520natural%250Ablending%2520inside%2520the%2520region%2520with%2520the%2520rest%2520of%2520the%2520image%2520unchanged%252C%2520we%2520observe%250Athat%2520these%2520existing%2520methods%2520often%2520struggle%2520in%2520synthesizing%2520seamless%250Ainteraction-aware%2520compositions%2520when%2520the%2520task%2520involves%2520human-object%250Ainteractions.%2520In%2520this%2520paper%252C%2520we%2520first%2520propose%2520HOComp%252C%2520a%2520novel%2520approach%2520for%250Acompositing%2520a%2520foreground%2520object%2520onto%2520a%2520human-centric%2520background%2520image%252C%2520while%250Aensuring%2520harmonious%2520interactions%2520between%2520the%2520foreground%2520object%2520and%2520the%250Abackground%2520person%2520and%2520their%2520consistent%2520appearances.%2520Our%2520approach%2520includes%2520two%250Akey%2520designs%253A%2520%25281%2529%2520MLLMs-driven%2520Region-based%2520Pose%2520Guidance%2520%2528MRPG%2529%252C%2520which%2520utilizes%250AMLLMs%2520to%2520identify%2520the%2520interaction%2520region%2520as%2520well%2520as%2520the%2520interaction%2520type%2520%2528e.g.%252C%250Aholding%2520and%2520lefting%2529%2520to%2520provide%2520coarse-to-fine%2520constraints%2520to%2520the%2520generated%250Apose%2520for%2520the%2520interaction%2520while%2520incorporating%2520human%2520pose%2520landmarks%2520to%2520track%250Aaction%2520variations%2520and%2520enforcing%2520fine-grained%2520pose%2520constraints%253B%2520and%2520%25282%2529%250ADetail-Consistent%2520Appearance%2520Preservation%2520%2528DCAP%2529%252C%2520which%2520unifies%2520a%2520shape-aware%250Aattention%2520modulation%2520mechanism%252C%2520a%2520multi-view%2520appearance%2520loss%252C%2520and%2520a%2520background%250Aconsistency%2520loss%2520to%2520ensure%2520consistent%2520shapes/textures%2520of%2520the%2520foreground%2520and%250Afaithful%2520reproduction%2520of%2520the%2520background%2520human.%2520We%2520then%2520propose%2520the%2520first%250Adataset%252C%2520named%2520Interaction-aware%2520Human-Object%2520Composition%2520%2528IHOC%2529%252C%2520for%2520the%2520task.%250AExperimental%2520results%2520on%2520our%2520dataset%2520show%2520that%2520HOComp%2520effectively%2520generates%250Aharmonious%2520human-object%2520interactions%2520with%2520consistent%2520appearances%252C%2520and%250Aoutperforms%2520relevant%2520methods%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HOComp%3A%20Interaction-Aware%20Human-Object%20Composition&entry.906535625=Dong%20Liang%20and%20Jinyuan%20Jia%20and%20Yuhao%20Liu%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20While%20existing%20image-guided%20composition%20methods%20may%20help%20insert%20a%20foreground%0Aobject%20onto%20a%20user-specified%20region%20of%20a%20background%20image%2C%20achieving%20natural%0Ablending%20inside%20the%20region%20with%20the%20rest%20of%20the%20image%20unchanged%2C%20we%20observe%0Athat%20these%20existing%20methods%20often%20struggle%20in%20synthesizing%20seamless%0Ainteraction-aware%20compositions%20when%20the%20task%20involves%20human-object%0Ainteractions.%20In%20this%20paper%2C%20we%20first%20propose%20HOComp%2C%20a%20novel%20approach%20for%0Acompositing%20a%20foreground%20object%20onto%20a%20human-centric%20background%20image%2C%20while%0Aensuring%20harmonious%20interactions%20between%20the%20foreground%20object%20and%20the%0Abackground%20person%20and%20their%20consistent%20appearances.%20Our%20approach%20includes%20two%0Akey%20designs%3A%20%281%29%20MLLMs-driven%20Region-based%20Pose%20Guidance%20%28MRPG%29%2C%20which%20utilizes%0AMLLMs%20to%20identify%20the%20interaction%20region%20as%20well%20as%20the%20interaction%20type%20%28e.g.%2C%0Aholding%20and%20lefting%29%20to%20provide%20coarse-to-fine%20constraints%20to%20the%20generated%0Apose%20for%20the%20interaction%20while%20incorporating%20human%20pose%20landmarks%20to%20track%0Aaction%20variations%20and%20enforcing%20fine-grained%20pose%20constraints%3B%20and%20%282%29%0ADetail-Consistent%20Appearance%20Preservation%20%28DCAP%29%2C%20which%20unifies%20a%20shape-aware%0Aattention%20modulation%20mechanism%2C%20a%20multi-view%20appearance%20loss%2C%20and%20a%20background%0Aconsistency%20loss%20to%20ensure%20consistent%20shapes/textures%20of%20the%20foreground%20and%0Afaithful%20reproduction%20of%20the%20background%20human.%20We%20then%20propose%20the%20first%0Adataset%2C%20named%20Interaction-aware%20Human-Object%20Composition%20%28IHOC%29%2C%20for%20the%20task.%0AExperimental%20results%20on%20our%20dataset%20show%20that%20HOComp%20effectively%20generates%0Aharmonious%20human-object%20interactions%20with%20consistent%20appearances%2C%20and%0Aoutperforms%20relevant%20methods%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16813v1&entry.124074799=Read"},
{"title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,\n  Long Context, and Next Generation Agentic Capabilities", "author": "Gheorghe Comanici and Eric Bieber and Mike Schaekermann and Ice Pasupat and Noveen Sachdeva and Inderjit Dhillon and Marcel Blistein and Ori Ram and Dan Zhang and Evan Rosen and Luke Marris and Sam Petulla and Colin Gaffney and Asaf Aharoni and Nathan Lintz and Tiago Cardal Pais and Henrik Jacobsson and Idan Szpektor and Nan-Jiang Jiang and Krishna Haridasan and Ahmed Omran and Nikunj Saunshi and Dara Bahri and Gaurav Mishra and Eric Chu and Toby Boyd and Brad Hekman and Aaron Parisi and Chaoyi Zhang and Kornraphop Kawintiranon and Tania Bedrax-Weiss and Oliver Wang and Ya Xu and Ollie Purkiss and Uri Mendlovic and Ila\u00ef Deutel and Nam Nguyen and Adam Langley and Flip Korn and Lucia Rossazza and Alexandre Ram\u00e9 and Sagar Waghmare and Helen Miller and Nathan Byrd and Ashrith Sheshan and Raia Hadsell Sangnie Bhardwaj and Pawel Janus and Tero Rissa and Dan Horgan and Sharon Silver and Ayzaan Wahid and Sergey Brin and Yves Raimond and Klemen Kloboves and Cindy Wang and Nitesh Bharadwaj Gundavarapu and Ilia Shumailov and Bo Wang and Mantas Pajarskas and Joe Heyward and Martin Nikoltchev and Maciej Kula and Hao Zhou and Zachary Garrett and Sushant Kafle and Sercan Arik and Ankita Goel and Mingyao Yang and Jiho Park and Koji Kojima and Parsa Mahmoudieh and Koray Kavukcuoglu and Grace Chen and Doug Fritz and Anton Bulyenov and Sudeshna Roy and Dimitris Paparas and Hadar Shemtov and Bo-Juen Chen and Robin Strudel and David Reitter and Aurko Roy and Andrey Vlasov and Changwan Ryu and Chas Leichner and Haichuan Yang and Zelda Mariet and Denis Vnukov and Tim Sohn and Amy Stuart and Wei Liang and Minmin Chen and Praynaa Rawlani and Christy Koh and JD Co-Reyes and Guangda Lai and Praseem Banzal and Dimitrios Vytiniotis and Jieru Mei and Mu Cai and Mohammed Badawi and Corey Fry and Ale Hartman and Daniel Zheng and Eric Jia and James Keeling and Annie Louis and Ying Chen and Efren Robles and Wei-Chih Hung and Howard Zhou and Nikita Saxena and Sonam Goenka and Olivia Ma and Zach Fisher and Mor Hazan Taege and Emily Graves and David Steiner and Yujia Li and Sarah Nguyen and Rahul Sukthankar and Joe Stanton and Ali Eslami and Gloria Shen and Berkin Akin and Alexey Guseynov and Yiqian Zhou and Jean-Baptiste Alayrac and Armand Joulin and Efrat Farkash and Ashish Thapliyal and Stephen Roller and Noam Shazeer and Todor Davchev and Terry Koo and Hannah Forbes-Pollard and Kartik Audhkhasi and Greg Farquhar and Adi Mayrav Gilady and Maggie Song and John Aslanides and Piermaria Mendolicchio and Alicia Parrish and John Blitzer and Pramod Gupta and Xiaoen Ju and Xiaochen Yang and Puranjay Datta and Andrea Tacchetti and Sanket Vaibhav Mehta and Gregory Dibb and Shubham Gupta and Federico Piccinini and Raia Hadsell and Sujee Rajayogam and Jiepu Jiang and Patrick Griffin and Patrik Sundberg and Jamie Hayes and Alexey Frolov and Tian Xie and Adam Zhang and Kingshuk Dasgupta and Uday Kalra and Lior Shani and Klaus Macherey and Tzu-Kuo Huang and Liam MacDermed and Karthik Duddu and Paulo Zacchello and Zi Yang and Jessica Lo and Kai Hui and Matej Kastelic and Derek Gasaway and Qijun Tan and Summer Yue and Pablo Barrio and John Wieting and Weel Yang and Andrew Nystrom and Solomon Demmessie and Anselm Levskaya and Fabio Viola and Chetan Tekur and Greg Billock and George Necula and Mandar Joshi and Rylan Schaeffer and Swachhand Lokhande and Christina Sorokin and Pradeep Shenoy and Mia Chen and Mark Collier and Hongji Li and Taylor Bos and Nevan Wichers and Sun Jae Lee and Ang\u00e9line Pouget and Santhosh Thangaraj and Kyriakos Axiotis and Phil Crone and Rachel Sterneck and Nikolai Chinaev and Victoria Krakovna and Oleksandr Ferludin and Ian Gemp and Stephanie Winkler and Dan Goldberg and Ivan Korotkov and Kefan Xiao and Malika Mehrotra and Sandeep Mariserla and Vihari Piratla and Terry Thurk and Khiem Pham and Hongxu Ma and Alexandre Senges and Ravi Kumar and Clemens Meyer and Ellie Talius and Nuo Wang Pierse and Ballie Sandhu and Horia Toma and Kuo Lin and Swaroop Nath and Tom Stone and Dorsa Sadigh and Nikita Gupta and Arthur Guez and Avi Singh and Matt Thomas and Tom Duerig and Yuan Gong and Richard Tanburn and Lydia Lihui Zhang and Phuong Dao and Mohamed Hammad and Sirui Xie and Shruti Rijhwani and Ben Murdoch and Duhyeon Kim and Will Thompson and Heng-Tze Cheng and Daniel Sohn and Pablo Sprechmann and Qiantong Xu and Srinivas Tadepalli and Peter Young and Ye Zhang and Hansa Srinivasan and Miranda Aperghis and Aditya Ayyar and Hen Fitoussi and Ryan Burnell and David Madras and Mike Dusenberry and Xi Xiong and Tayo Oguntebi and Ben Albrecht and J\u00f6rg Bornschein and Jovana Mitrovi\u0107 and Mason Dimarco and Bhargav Kanagal Shamanna and Premal Shah and Eren Sezener and Shyam Upadhyay and Dave Lacey and Craig Schiff and Sebastien Baur and Sanjay Ganapathy and Eva Schnider and Mateo Wirth and Connor Schenck and Andrey Simanovsky and Yi-Xuan Tan and Philipp Fr\u00e4nken and Dennis Duan and Bharath Mankalale and Nikhil Dhawan and Kevin Sequeira and Zichuan Wei and Shivanker Goel and Caglar Unlu and Yukun Zhu and Haitian Sun and Ananth Balashankar and Kurt Shuster and Megh Umekar and Mahmoud Alnahlawi and A\u00e4ron van den Oord and Kelly Chen and Yuexiang Zhai and Zihang Dai and Kuang-Huei Lee and Eric Doi and Lukas Zilka and Rohith Vallu and Disha Shrivastava and Jason Lee and Hisham Husain and Honglei Zhuang and Vincent Cohen-Addad and Jarred Barber and James Atwood and Adam Sadovsky and Quentin Wellens and Steven Hand and Arunkumar Rajendran and Aybuke Turker and CJ Carey and Yuanzhong Xu and Hagen Soltau and Zefei Li and Xinying Song and Conglong Li and Iurii Kemaev and Sasha Brown and Andrea Burns and Viorica Patraucean and Piotr Stanczyk and Renga Aravamudhan and Mathieu Blondel and Hila Noga and Lorenzo Blanco and Will Song and Michael Isard and Mandar Sharma and Reid Hayes and Dalia El Badawy and Avery Lamp and Itay Laish and Olga Kozlova and Kelvin Chan and Sahil Singla and Srinivas Sunkara and Mayank Upadhyay and Chang Liu and Aijun Bai and Jarek Wilkiewicz and Martin Zlocha and Jeremiah Liu and Zhuowan Li and Haiguang Li and Omer Barak and Ganna Raboshchuk and Jiho Choi and Fangyu Liu and Erik Jue and Mohit Sharma and Andreea Marzoca and Robert Busa-Fekete and Anna Korsun and Andre Elisseeff and Zhe Shen and Sara Mc Carthy and Kay Lamerigts and Anahita Hosseini and Hanzhao Lin and Charlie Chen and Fan Yang and Kushal Chauhan and Mark Omernick and Dawei Jia and Karina Zainullina and Demis Hassabis and Danny Vainstein and Ehsan Amid and Xiang Zhou and Ronny Votel and Eszter V\u00e9rtes and Xinjian Li and Zongwei Zhou and Angeliki Lazaridou and Brendan McMahan and Arjun Narayanan and Hubert Soyer and Sujoy Basu and Kayi Lee and Bryan Perozzi and Qin Cao and Leonard Berrada and Rahul Arya and Ke Chen and  Katrina and  Xu and Matthias Lochbrunner and Alex Hofer and Sahand Sharifzadeh and Renjie Wu and Sally Goldman and Pranjal Awasthi and Xuezhi Wang and Yan Wu and Claire Sha and Biao Zhang and Maciej Miku\u0142a and Filippo Graziano and Siobhan Mcloughlin and Irene Giannoumis and Youhei Namiki and Chase Malik and Carey Radebaugh and Jamie Hall and Ramiro Leal-Cavazos and Jianmin Chen and Vikas Sindhwani and David Kao and David Greene and Jordan Griffith and Chris Welty and Ceslee Montgomery and Toshihiro Yoshino and Liangzhe Yuan and Noah Goodman and Assaf Hurwitz Michaely and Kevin Lee and KP Sawhney and Wei Chen and Zheng Zheng and Megan Shum and Nikolay Savinov and Etienne Pot and Alex Pak and Morteza Zadimoghaddam and Sijal Bhatnagar and Yoad Lewenberg and Blair Kutzman and Ji Liu and Lesley Katzen and Jeremy Selier and Josip Djolonga and Dmitry Lepikhin and Kelvin Xu and Jacky Liang and Jiewen Tan and Benoit Schillings and Muge Ersoy and Pete Blois and Bernd Bandemer and Abhimanyu Singh and Sergei Lebedev and Pankaj Joshi and Adam R. Brown and Evan Palmer and Shreya Pathak and Komal Jalan and Fedir Zubach and Shuba Lall and Randall Parker and Alok Gunjan and Sergey Rogulenko and Sumit Sanghai and Zhaoqi Leng and Zoltan Egyed and Shixin Li and Maria Ivanova and Kostas Andriopoulos and Jin Xie and Elan Rosenfeld and Auriel Wright and Ankur Sharma and Xinyang Geng and Yicheng Wang and Sam Kwei and Renke Pan and Yujing Zhang and Gabby Wang and Xi Liu and Chak Yeung and Elizabeth Cole and Aviv Rosenberg and Zhen Yang and Phil Chen and George Polovets and Pranav Nair and Rohun Saxena and Josh Smith and Shuo-yiin Chang and Aroma Mahendru and Svetlana Grant and Anand Iyer and Irene Cai and Jed McGiffin and Jiaming Shen and Alanna Walton and Antonious Girgis and Oliver Woodman and Rosemary Ke and Mike Kwong and Louis Rouillard and Jinmeng Rao and Zhihao Li and Yuntao Xu and Flavien Prost and Chi Zou and Ziwei Ji and Alberto Magni and Tyler Liechty and Dan A. Calian and Deepak Ramachandran and Igor Krivokon and Hui Huang and Terry Chen and Anja Hauth and Anastasija Ili\u0107 and Weijuan Xi and Hyeontaek Lim and Vlad-Doru Ion and Pooya Moradi and Metin Toksoz-Exley and Kalesha Bullard and Miltos Allamanis and Xiaomeng Yang and Sophie Wang and Zhi Hong and Anita Gergely and Cheng Li and Bhavishya Mittal and Vitaly Kovalev and Victor Ungureanu and Jane Labanowski and Jan Wassenberg and Nicolas Lacasse and Geoffrey Cideron and Petar Devi\u0107 and Annie Marsden and Lynn Nguyen and Michael Fink and Yin Zhong and Tatsuya Kiyono and Desi Ivanov and Sally Ma and Max Bain and Kiran Yalasangi and Jennifer She and Anastasia Petrushkina and Mayank Lunayach and Carla Bromberg and Sarah Hodkinson and Vilobh Meshram and Daniel Vlasic and Austin Kyker and Steve Xu and Jeff Stanway and Zuguang Yang and Kai Zhao and Matthew Tung and Seth Odoom and Yasuhisa Fujii and Justin Gilmer and Eunyoung Kim and Felix Halim and Quoc Le and Bernd Bohnet and Seliem El-Sayed and Behnam Neyshabur and Malcolm Reynolds and Dean Reich and Yang Xu and Erica Moreira and Anuj Sharma and Zeyu Liu and Mohammad Javad Hosseini and Naina Raisinghani and Yi Su and Ni Lao and Daniel Formoso and Marco Gelmi and Almog Gueta and Tapomay Dey and Elena Gribovskaya and Domagoj \u0106evid and Sidharth Mudgal and Garrett Bingham and Jianling Wang and Anurag Kumar and Alex Cullum and Feng Han and Konstantinos Bousmalis and Diego Cedillo and Grace Chu and Vladimir Magay and Paul Michel and Ester Hlavnova and Daniele Calandriello and Setareh Ariafar and Kaisheng Yao and Vikash Sehwag and Arpi Vezer and Agustin Dal Lago and Zhenkai Zhu and Paul Kishan Rubenstein and Allen Porter and Anirudh Baddepudi and Oriana Riva and Mihai Dorin Istin and Chih-Kuan Yeh and Zhi Li and Andrew Howard and Nilpa Jha and Jeremy Chen and Raoul de Liedekerke and Zafarali Ahmed and Mikel Rodriguez and Tanuj Bhatia and Bangju Wang and Ali Elqursh and David Klinghoffer and Peter Chen and Pushmeet Kohli and Te I and Weiyang Zhang and Zack Nado and Jilin Chen and Maxwell Chen and George Zhang and Aayush Singh and Adam Hillier and Federico Lebron and Yiqing Tao and Ting Liu and Gabriel Dulac-Arnold and Jingwei Zhang and Shashi Narayan and Buhuang Liu and Orhan Firat and Abhishek Bhowmick and Bingyuan Liu and Hao Zhang and Zizhao Zhang and Georges Rotival and Nathan Howard and Anu Sinha and Alexander Grushetsky and Benjamin Beyret and Keerthana Gopalakrishnan and James Zhao and Kyle He and Szabolcs Payrits and Zaid Nabulsi and Zhaoyi Zhang and Weijie Chen and Edward Lee and Nova Fallen and Sreenivas Gollapudi and Aurick Zhou and Filip Paveti\u0107 and Thomas K\u00f6ppe and Shiyu Huang and Rama Pasumarthi and Nick Fernando and Felix Fischer and Daria \u0106urko and Yang Gao and James Svensson and Austin Stone and Haroon Qureshi and Abhishek Sinha and Apoorv Kulshreshtha and Martin Matysiak and Jieming Mao and Carl Saroufim and Aleksandra Faust and Qingnan Duan and Gil Fidel and Kaan Katircioglu and Rapha\u00ebl Lopez Kaufman and Dhruv Shah and Weize Kong and Abhishek Bapna and Gell\u00e9rt Weisz and Emma Dunleavy and Praneet Dutta and Tianqi Liu and Rahma Chaabouni and Carolina Parada and Marcus Wu and Alexandra Belias and Alessandro Bissacco and Stanislav Fort and Li Xiao and Fantine Huot and Chris Knutsen and Yochai Blau and Gang Li and Jennifer Prendki and Juliette Love and Yinlam Chow and Pichi Charoenpanit and Hidetoshi Shimokawa and Vincent Coriou and Karol Gregor and Tomas Izo and Arjun Akula and Mario Pinto and Chris Hahn and Dominik Paulus and Jiaxian Guo and Neha Sharma and Cho-Jui Hsieh and Adaeze Chukwuka and Kazuma Hashimoto and Nathalie Rauschmayr and Ling Wu and Christof Angermueller and Yulong Wang and Sebastian Gerlach and Michael Pliskin and Daniil Mirylenka and Min Ma and Lexi Baugher and Bryan Gale and Shaan Bijwadia and Nemanja Raki\u0107evi\u0107 and David Wood and Jane Park and Chung-Ching Chang and Babi Seal and Chris Tar and Kacper Krasowiak and Yiwen Song and Georgi Stephanov and Gary Wang and Marcello Maggioni and Stein Xudong Lin and Felix Wu and Shachi Paul and Zixuan Jiang and Shubham Agrawal and Bilal Piot and Alex Feng and Cheolmin Kim and Tulsee Doshi and Jonathan Lai and  Chuqiao and  Xu and Sharad Vikram and Ciprian Chelba and Sebastian Krause and Vincent Zhuang and Jack Rae and Timo Denk and Adrian Collister and Lotte Weerts and Xianghong Luo and Yifeng Lu and H\u00e5vard Garnes and Nitish Gupta and Terry Spitz and Avinatan Hassidim and Lihao Liang and Izhak Shafran and Peter Humphreys and Kenny Vassigh and Phil Wallis and Virat Shejwalkar and Nicolas Perez-Nieves and Rachel Hornung and Melissa Tan and Beka Westberg and Andy Ly and Richard Zhang and Brian Farris and Jongbin Park and Alec Kosik and Zeynep Cankara and Andrii Maksai and Yunhan Xu and Albin Cassirer and Sergi Caelles and Abbas Abdolmaleki and Mencher Chiang and Alex Fabrikant and Shravya Shetty and Luheng He and Mai Gim\u00e9nez and Hadi Hashemi and Sheena Panthaplackel and Yana Kulizhskaya and Salil Deshmukh and Daniele Pighin and Robin Alazard and Disha Jindal and Seb Noury and Pradeep Kumar S and Siyang Qin and Xerxes Dotiwalla and Stephen Spencer and Mohammad Babaeizadeh and Blake JianHang Chen and Vaibhav Mehta and Jennie Lees and Andrew Leach and Penporn Koanantakool and Ilia Akolzin and Ramona Comanescu and Junwhan Ahn and Alexey Svyatkovskiy and Basil Mustafa and David D'Ambrosio and Shiva Mohan Reddy Garlapati and Pascal Lamblin and Alekh Agarwal and Shuang Song and Pier Giuseppe Sessa and Pauline Coquinot and John Maggs and Hussain Masoom and Divya Pitta and Yaqing Wang and Patrick Morris-Suzuki and Billy Porter and Johnson Jia and Jeffrey Dudek and Raghavender R and Cosmin Paduraru and Alan Ansell and Tolga Bolukbasi and Tony Lu and Ramya Ganeshan and Zi Wang and Henry Griffiths and Rodrigo Benenson and Yifan He and James Swirhun and George Papamakarios and Aditya Chawla and Kuntal Sengupta and Yan Wang and Vedrana Milutinovic and Igor Mordatch and Zhipeng Jia and Jamie Smith and Will Ng and Shitij Nigam and Matt Young and Eugen Vu\u0161ak and Blake Hechtman and Sheela Goenka and Avital Zipori and Kareem Ayoub and Ashok Popat and Trilok Acharya and Luo Yu and Dawn Bloxwich and Hugo Song and Paul Roit and Haiqiong Li and Aviel Boag and Nigamaa Nayakanti and Bilva Chandra and Tianli Ding and Aahil Mehta and Cath Hope and Jiageng Zhang and Idan Heimlich Shtacher and Kartikeya Badola and Ryo Nakashima and Andrei Sozanschi and Iulia Com\u015fa and Ante \u017du\u017eul and Emily Caveness and Julian Odell and Matthew Watson and Dario de Cesare and Phillip Lippe and Derek Lockhart and Siddharth Verma and Huizhong Chen and Sean Sun and Lin Zhuo and Aditya Shah and Prakhar Gupta and Alex Muzio and Ning Niu and Amir Zait and Abhinav Singh and Meenu Gaba and Fan Ye and Prajit Ramachandran and Mohammad Saleh and Raluca Ada Popa and Ayush Dubey and Frederick Liu and Sara Javanmardi and Mark Epstein and Ross Hemsley and Richard Green and Nishant Ranka and Eden Cohen and Chuyuan Kelly Fu and Sanjay Ghemawat and Jed Borovik and James Martens and Anthony Chen and Pranav Shyam and Andr\u00e9 Susano Pinto and Ming-Hsuan Yang and Alexandru \u0162ifrea and David Du and Boqing Gong and Ayushi Agarwal and Seungyeon Kim and Christian Frank and Saloni Shah and Xiaodan Song and Zhiwei Deng and Ales Mikhalap and Kleopatra Chatziprimou and Timothy Chung and Toni Creswell and Susan Zhang and Yennie Jun and Carl Lebsack and Will Truong and Slavica Anda\u010di\u0107 and Itay Yona and Marco Fornoni and Rong Rong and Serge Toropov and Afzal Shama Soudagar and Andrew Audibert and Salah Zaiem and Zaheer Abbas and Andrei Rusu and Sahitya Potluri and Shitao Weng and Anastasios Kementsietsidis and Anton Tsitsulin and Daiyi Peng and Natalie Ha and Sanil Jain and Tejasi Latkar and Simeon Ivanov and Cory McLean and Anirudh GP and Rajesh Venkataraman and Canoee Liu and Dilip Krishnan and Joel D'sa and Roey Yogev and Paul Collins and Benjamin Lee and Lewis Ho and Carl Doersch and Gal Yona and Shawn Gao and Felipe Tiengo Ferreira and Adnan Ozturel and Hannah Muckenhirn and Ce Zheng and Gargi Balasubramaniam and Mudit Bansal and George van den Driessche and Sivan Eiger and Salem Haykal and Vedant Misra and Abhimanyu Goyal and Danilo Martins and Gary Leung and Jonas Valfridsson and Four Flynn and Will Bishop and Chenxi Pang and Yoni Halpern and Honglin Yu and Lawrence Moore and  Yuvein and  Zhu and Sridhar Thiagarajan and Yoel Drori and Zhisheng Xiao and Lucio Dery and Rolf Jagerman and Jing Lu and Eric Ge and Vaibhav Aggarwal and Arjun Khare and Vinh Tran and Oded Elyada and Ferran Alet and James Rubin and Ian Chou and David Tian and Libin Bai and Lawrence Chan and Lukasz Lew and Karolis Misiunas and Taylan Bilal and Aniket Ray and Sindhu Raghuram and Alex Castro-Ros and Viral Carpenter and CJ Zheng and Michael Kilgore and Josef Broder and Emily Xue and Praveen Kallakuri and Dheeru Dua and Nancy Yuen and Steve Chien and John Schultz and Saurabh Agrawal and Reut Tsarfaty and Jingcao Hu and Ajay Kannan and Dror Marcus and Nisarg Kothari and Baochen Sun and Ben Horn and Matko Bo\u0161njak and Ferjad Naeem and Dean Hirsch and Lewis Chiang and Boya Fang and Jie Han and Qifei Wang and Ben Hora and Antoine He and Mario Lu\u010di\u0107 and Beer Changpinyo and Anshuman Tripathi and John Youssef and Chester Kwak and Philippe Schlattner and Cat Graves and R\u00e9mi Leblond and Wenjun Zeng and Anders Andreassen and Gabriel Rasskin and Yue Song and Eddie Cao and Junhyuk Oh and Matt Hoffman and Wojtek Skut and Yichi Zhang and Jon Stritar and Xingyu Cai and Saarthak Khanna and Kathie Wang and Shriya Sharma and Christian Reisswig and Younghoon Jun and Aman Prasad and Tatiana Sholokhova and Preeti Singh and Adi Gerzi Rosenthal and Anian Ruoss and Fran\u00e7oise Beaufays and Sean Kirmani and Dongkai Chen and Johan Schalkwyk and Jonathan Herzig and Been Kim and Josh Jacob and Damien Vincent and Adrian N Reyes and Ivana Balazevic and L\u00e9onard Hussenot and Jon Schneider and Parker Barnes and Luis Castro and Spandana Raj Babbula and Simon Green and Serkan Cabi and Nico Duduta and Danny Driess and Rich Galt and Noam Velan and Junjie Wang and Hongyang Jiao and Matthew Mauger and Du Phan and Miteyan Patel and Vlado Gali\u0107 and Jerry Chang and Eyal Marcus and Matt Harvey and Julian Salazar and Elahe Dabir and Suraj Satishkumar Sheth and Amol Mandhane and Hanie Sedghi and Jeremiah Willcock and Amir Zandieh and Shruthi Prabhakara and Aida Amini and Antoine Miech and Victor Stone and Massimo Nicosia and Paul Niemczyk and Ying Xiao and Lucy Kim and S\u0142awek Kwasiborski and Vikas Verma and Ada Maksutaj Oflazer and Christoph Hirnschall and Peter Sung and Lu Liu and Richard Everett and Michiel Bakker and \u00c1goston Weisz and Yufei Wang and Vivek Sampathkumar and Uri Shaham and Bibo Xu and Yasemin Altun and Mingqiu Wang and Takaaki Saeki and Guanjie Chen and Emanuel Taropa and Shanthal Vasanth and Sophia Austin and Lu Huang and Goran Petrovic and Qingyun Dou and Daniel Golovin and Grigory Rozhdestvenskiy and Allie Culp and Will Wu and Motoki Sano and Divya Jain and Julia Proskurnia and S\u00e9bastien Cevey and Alejandro Cruzado Ruiz and Piyush Patil and Mahdi Mirzazadeh and Eric Ni and Javier Snaider and Lijie Fan and Alexandre Fr\u00e9chette and AJ Pierigiovanni and Shariq Iqbal and Kenton Lee and Claudio Fantacci and Jinwei Xing and Lisa Wang and Alex Irpan and David Raposo and Yi Luan and Zhuoyuan Chen and Harish Ganapathy and Kevin Hui and Jiazhong Nie and Isabelle Guyon and Heming Ge and Roopali Vij and Hui Zheng and Dayeong Lee and Alfonso Casta\u00f1o and Khuslen Baatarsukh and Gabriel Ibagon and Alexandra Chronopoulou and Nicholas FitzGerald and Shashank Viswanadha and Safeen Huda and Rivka Moroshko and Georgi Stoyanov and Prateek Kolhar and Alain Vaucher and Ishaan Watts and Adhi Kuncoro and Henryk Michalewski and Satish Kambala and Bat-Orgil Batsaikhan and Alek Andreev and Irina Jurenka and Maigo Le and Qihang Chen and Wael Al Jishi and Sarah Chakera and Zhe Chen and Aditya Kini and Vikas Yadav and Aditya Siddhant and Ilia Labzovsky and Balaji Lakshminarayanan and Carrie Grimes Bostock and Pankil Botadra and Ankesh Anand and Colton Bishop and Sam Conway-Rahman and Mohit Agarwal and Yani Donchev and Achintya Singhal and F\u00e9lix de Chaumont Quitry and Natalia Ponomareva and Nishant Agrawal and Bin Ni and Kalpesh Krishna and Masha Samsikova and John Karro and Yilun Du and Tamara von Glehn and Caden Lu and Christopher A. Choquette-Choo and Zhen Qin and Tingnan Zhang and Sicheng Li and Divya Tyam and Swaroop Mishra and Wing Lowe and Colin Ji and Weiyi Wang and Manaal Faruqui and Ambrose Slone and Valentin Dalibard and Arunachalam Narayanaswamy and John Lambert and Pierre-Antoine Manzagol and Dan Karliner and Andrew Bolt and Ivan Lobov and Aditya Kusupati and Chang Ye and Xuan Yang and Heiga Zen and Nelson George and Mukul Bhutani and Olivier Lacombe and Robert Riachi and Gagan Bansal and Rachel Soh and Yue Gao and Yang Yu and Adams Yu and Emily Nottage and Tania Rojas-Esponda and James Noraky and Manish Gupta and Ragha Kotikalapudi and Jichuan Chang and Sanja Deur and Dan Graur and Alex Mossin and Erin Farnese and Ricardo Figueira and Alexandre Moufarek and Austin Huang and Patrik Zochbauer and Ben Ingram and Tongzhou Chen and Zelin Wu and Adri\u00e0 Puigdom\u00e8nech and Leland Rechis and Da Yu and Sri Gayatri Sundara Padmanabhan and Rui Zhu and Chu-ling Ko and Andrea Banino and Samira Daruki and Aarush Selvan and Dhruva Bhaswar and Daniel Hernandez Diaz and Chen Su and Salvatore Scellato and Jennifer Brennan and Woohyun Han and Grace Chung and Priyanka Agrawal and Urvashi Khandelwal and Khe Chai Sim and Morgane Lustman and Sam Ritter and Kelvin Guu and Jiawei Xia and Prateek Jain and Emma Wang and Tyrone Hill and Mirko Rossini and Marija Kostelac and Tautvydas Misiunas and Amit Sabne and Kyuyeun Kim and Ahmet Iscen and Congchao Wang and Jos\u00e9 Leal and Ashwin Sreevatsa and Utku Evci and Manfred Warmuth and Saket Joshi and Daniel Suo and James Lottes and Garrett Honke and Brendan Jou and Stefani Karp and Jieru Hu and Himanshu Sahni and Adrien Ali Ta\u00efga and William Kong and Samrat Ghosh and Renshen Wang and Jay Pavagadhi and Natalie Axelsson and Nikolai Grigorev and Patrick Siegler and Rebecca Lin and Guohui Wang and Emilio Parisotto and Sharath Maddineni and Krishan Subudhi and Eyal Ben-David and Elena Pochernina and Orgad Keller and Thi Avrahami and Zhe Yuan and Pulkit Mehta and Jialu Liu and Sherry Yang and Wendy Kan and Katherine Lee and Tom Funkhouser and Derek Cheng and Hongzhi Shi and Archit Sharma and Joe Kelley and Matan Eyal and Yury Malkov and Corentin Tallec and Yuval Bahat and Shen Yan and  Xintian and  Wu and David Lindner and Chengda Wu and Avi Caciularu and Xiyang Luo and Rodolphe Jenatton and Tim Zaman and Yingying Bi and Ilya Kornakov and Ganesh Mallya and Daisuke Ikeda and Itay Karo and Anima Singh and Colin Evans and Praneeth Netrapalli and Vincent Nallatamby and Isaac Tian and Yannis Assael and Vikas Raunak and Victor Carbune and Ioana Bica and Lior Madmoni and Dee Cattle and Snchit Grover and Krishna Somandepalli and Sid Lall and Amelio V\u00e1zquez-Reina and Riccardo Patana and Jiaqi Mu and Pranav Talluri and Maggie Tran and Rajeev Aggarwal and RJ Skerry-Ryan and Jun Xu and Mike Burrows and Xiaoyue Pan and Edouard Yvinec and Di Lu and Zhiying Zhang and Duc Dung Nguyen and Hairong Mu and Gabriel Barcik and Helen Ran and Lauren Beltrone and Krzysztof Choromanski and Dia Kharrat and Samuel Albanie and Sean Purser-haskell and David Bieber and Carrie Zhang and Jing Wang and Tom Hudson and Zhiyuan Zhang and Han Fu and Johannes Mauerer and Mohammad Hossein Bateni and AJ Maschinot and Bing Wang and Muye Zhu and Arjun Pillai and Tobias Weyand and Shuang Liu and Oscar Akerlund and Fred Bertsch and Vittal Premachandran and Alicia Jin and Vincent Roulet and Peter de Boursac and Shubham Mittal and Ndaba Ndebele and Georgi Karadzhov and Sahra Ghalebikesabi and Ricky Liang and Allen Wu and Yale Cong and Nimesh Ghelani and Sumeet Singh and Bahar Fatemi and  Warren and  Chen and Charles Kwong and Alexey Kolganov and Steve Li and Richard Song and Chenkai Kuang and Sobhan Miryoosefi and Dale Webster and James Wendt and Arkadiusz Socala and Guolong Su and Artur Mendon\u00e7a and Abhinav Gupta and Xiaowei Li and Tomy Tsai and  Qiong and  Hu and Kai Kang and Angie Chen and Sertan Girgin and Yongqin Xian and Andrew Lee and Nolan Ramsden and Leslie Baker and Madeleine Clare Elish and Varvara Krayvanova and Rishabh Joshi and Jiri Simsa and Yao-Yuan Yang and Piotr Ambroszczyk and Dipankar Ghosh and Arjun Kar and Yuan Shangguan and Yumeya Yamamori and Yaroslav Akulov and Andy Brock and Haotian Tang and Siddharth Vashishtha and Rich Munoz and Andreas Steiner and Kalyan Andra and Daniel Eppens and Qixuan Feng and Hayato Kobayashi and Sasha Goldshtein and Mona El Mahdy and Xin Wang and  Jilei and  Wang and Richard Killam and Tom Kwiatkowski and Kavya Kopparapu and Serena Zhan and Chao Jia and Alexei Bendebury and Sheryl Luo and Adri\u00e0 Recasens and Timothy Knight and Jing Chen and Mohak Patel and YaGuang Li and Ben Withbroe and Dean Weesner and Kush Bhatia and Jie Ren and Danielle Eisenbud and Ebrahim Songhori and Yanhua Sun and Travis Choma and Tasos Kementsietsidis and Lucas Manning and Brian Roark and Wael Farhan and Jie Feng and Susheel Tatineni and James Cobon-Kerr and Yunjie Li and Lisa Anne Hendricks and Isaac Noble and Chris Breaux and Nate Kushman and Liqian Peng and Fuzhao Xue and Taylor Tobin and Jamie Rogers and Josh Lipschultz and Chris Alberti and Alexey Vlaskin and Mostafa Dehghani and Roshan Sharma and Tris Warkentin and Chen-Yu Lee and Benigno Uria and Da-Cheng Juan and Angad Chandorkar and Hila Sheftel and Ruibo Liu and Elnaz Davoodi and Borja De Balle Pigem and Kedar Dhamdhere and David Ross and Jonathan Hoech and Mahdis Mahdieh and Li Liu and Qiujia Li and Liam McCafferty and Chenxi Liu and Markus Mircea and Yunting Song and Omkar Savant and Alaa Saade and Colin Cherry and Vincent Hellendoorn and Siddharth Goyal and Paul Pucciarelli and David Vilar Torres and Zohar Yahav and Hyo Lee and Lars Lowe Sjoesund and Christo Kirov and Bo Chang and Deepanway Ghoshal and Lu Li and Gilles Baechler and S\u00e9bastien Pereira and Tara Sainath and Anudhyan Boral and Dominik Grewe and Afief Halumi and Nguyet Minh Phu and Tianxiao Shen and Marco Tulio Ribeiro and Dhriti Varma and Alex Kaskasoli and Vlad Feinberg and Navneet Potti and Jarrod Kahn and Matheus Wisniewski and Shakir Mohamed and Arnar Mar Hrafnkelsson and Bobak Shahriari and Jean-Baptiste Lespiau and Lisa Patel and Legg Yeung and Tom Paine and Lantao Mei and Alex Ramirez and Rakesh Shivanna and Li Zhong and Josh Woodward and Guilherme Tubone and Samira Khan and Heng Chen and Elizabeth Nielsen and Catalin Ionescu and Utsav Prabhu and Mingcen Gao and Qingze Wang and Sean Augenstein and Neesha Subramaniam and Jason Chang and Fotis Iliopoulos and Jiaming Luo and Myriam Khan and Weicheng Kuo and Denis Teplyashin and Florence Perot and Logan Kilpatrick and Amir Globerson and Hongkun Yu and Anfal Siddiqui and Nick Sukhanov and Arun Kandoor and Umang Gupta and Marco Andreetto and Moran Ambar and Donnie Kim and Pawe\u0142 Weso\u0142owski and Sarah Perrin and Ben Limonchik and Wei Fan and Jim Stephan and Ian Stewart-Binks and Ryan Kappedal and Tong He and Sarah Cogan and Romina Datta and Tong Zhou and Jiayu Ye and Leandro Kieliger and Ana Ramalho and Kyle Kastner and Fabian Mentzer and Wei-Jen Ko and Arun Suggala and Tianhao Zhou and Shiraz Butt and Hana Strej\u010dek and Lior Belenki and Subhashini Venugopalan and Mingyang Ling and Evgenii Eltyshev and Yunxiao Deng and Geza Kovacs and Mukund Raghavachari and Hanjun Dai and Tal Schuster and Steven Schwarcz and Richard Nguyen and Arthur Nguyen and Gavin Buttimore and Shrestha Basu Mallick and Sudeep Gandhe and Seth Benjamin and Michal Jastrzebski and Le Yan and Sugato Basu and Chris Apps and Isabel Edkins and James Allingham and Immanuel Odisho and Tomas Kocisky and Jewel Zhao and Linting Xue and Apoorv Reddy and Chrysovalantis Anastasiou and Aviel Atias and Sam Redmond and Kieran Milan and Nicolas Heess and Herman Schmit and Allan Dafoe and Daniel Andor and Tynan Gangwani and Anca Dragan and Sheng Zhang and Ashyana Kachra and Gang Wu and Siyang Xue and Kevin Aydin and Siqi Liu and Yuxiang Zhou and Mahan Malihi and Austin Wu and Siddharth Gopal and Candice Schumann and Peter Stys and Alek Wang and Mirek Ol\u0161\u00e1k and Dangyi Liu and Christian Schallhart and Yiran Mao and Demetra Brady and Hao Xu and Tomas Mery and Chawin Sitawarin and Siva Velusamy and Tom Cobley and Alex Zhai and Christian Walder and Nitzan Katz and Ganesh Jawahar and Chinmay Kulkarni and Antoine Yang and Adam Paszke and Yinan Wang and Bogdan Damoc and Zal\u00e1n Borsos and Ray Smith and Jinning Li and Mansi Gupta and Andrei Kapishnikov and Sushant Prakash and Florian Luisier and Rishabh Agarwal and Will Grathwohl and Kuangyuan Chen and Kehang Han and Nikhil Mehta and Andrew Over and Shekoofeh Azizi and Lei Meng and Niccol\u00f2 Dal Santo and Kelvin Zheng and Jane Shapiro and Igor Petrovski and Jeffrey Hui and Amin Ghafouri and Jasper Snoek and James Qin and Mandy Jordan and Caitlin Sikora and Jonathan Malmaud and Yuheng Kuang and Aga \u015awietlik and Ruoxin Sang and Chongyang Shi and Leon Li and Andrew Rosenberg and Shubin Zhao and Andy Crawford and Jan-Thorsten Peter and Yun Lei and Xavier Garcia and Long Le and Todd Wang and Julien Amelot and Dave Orr and Praneeth Kacham and Dana Alon and Gladys Tyen and Abhinav Arora and James Lyon and Alex Kurakin and Mimi Ly and Theo Guidroz and Zhipeng Yan and Rina Panigrahy and Pingmei Xu and Thais Kagohara and Yong Cheng and Eric Noland and Jinhyuk Lee and Jonathan Lee and Cathy Yip and Maria Wang and Efrat Nehoran and Alexander Bykovsky and Zhihao Shan and Ankit Bhagatwala and Chaochao Yan and Jie Tan and Guillermo Garrido and Dan Ethier and Nate Hurley and Grace Vesom and Xu Chen and Siyuan Qiao and Abhishek Nayyar and Julian Walker and Paramjit Sandhu and Mihaela Rosca and Danny Swisher and Mikhail Dektiarev and Josh Dillon and George-Cristian Muraru and Manuel Tragut and Artiom Myaskovsky and David Reid and Marko Velic and Owen Xiao and Jasmine George and Mark Brand and Jing Li and Wenhao Yu and Shane Gu and Xiang Deng and Fran\u00e7ois-Xavier Aubet and Soheil Hassas Yeganeh and Fred Alcober and Celine Smith and Trevor Cohn and Kay McKinney and Michael Tschannen and Ramesh Sampath and Gowoon Cheon and Liangchen Luo and Luyang Liu and Jordi Orbay and Hui Peng and Gabriela Botea and Xiaofan Zhang and Charles Yoon and Cesar Magalhaes and Pawe\u0142 Stradomski and Ian Mackinnon and Steven Hemingray and Kumaran Venkatesan and Rhys May and Jaeyoun Kim and Alex Druinsky and Jingchen Ye and Zheng Xu and Terry Huang and Jad Al Abdallah and Adil Dostmohamed and Rachana Fellinger and Tsendsuren Munkhdalai and Akanksha Maurya and Peter Garst and Yin Zhang and Maxim Krikun and Simon Bucher and Aditya Srikanth Veerubhotla and Yaxin Liu and Sheng Li and Nishesh Gupta and Jakub Adamek and Hanwen Chen and Bernett Orlando and Aleksandr Zaks and Joost van Amersfoort and Josh Camp and Hui Wan and HyunJeong Choe and Zhichun Wu and Kate Olszewska and Weiren Yu and Archita Vadali and Martin Scholz and Daniel De Freitas and Jason Lin and Amy Hua and Xin Liu and Frank Ding and Yichao Zhou and Boone Severson and Katerina Tsihlas and Samuel Yang and Tammo Spalink and Varun Yerram and Helena Pankov and Rory Blevins and Ben Vargas and Sarthak Jauhari and Matt Miecnikowski and Ming Zhang and Sandeep Kumar and Clement Farabet and Charline Le Lan and Sebastian Flennerhag and Yonatan Bitton and Ada Ma and Arthur Bra\u017einskas and Eli Collins and Niharika Ahuja and Sneha Kudugunta and Anna Bortsova and Minh Giang and Wanzheng Zhu and Ed Chi and Scott Lundberg and Alexey Stern and Subha Puttagunta and Jing Xiong and Xiao Wu and Yash Pande and Amit Jhindal and Daniel Murphy and Jon Clark and Marc Brockschmidt and Maxine Deines and Kevin R. McKee and Dan Bahir and Jiajun Shen and Minh Truong and Daniel McDuff and Andrea Gesmundo and Edouard Rosseel and Bowen Liang and Ken Caluwaerts and Jessica Hamrick and Joseph Kready and Mary Cassin and Rishikesh Ingale and Li Lao and Scott Pollom and Yifan Ding and Wei He and Lizzetth Bellot and Joana Iljazi and Ramya Sree Boppana and Shan Han and Tara Thompson and Amr Khalifa and Anna Bulanova and Blagoj Mitrevski and Bo Pang and Emma Cooney and Tian Shi and Rey Coaguila and Tamar Yakar and Marc'aurelio Ranzato and Nikola Momchev and Chris Rawles and Zachary Charles and Young Maeng and Yuan Zhang and Rishabh Bansal and Xiaokai Zhao and Brian Albert and Yuan Yuan and Sudheendra Vijayanarasimhan and Roy Hirsch and Vinay Ramasesh and Kiran Vodrahalli and Xingyu Wang and Arushi Gupta and DJ Strouse and Jianmo Ni and Roma Patel and Gabe Taubman and Zhouyuan Huo and Dero Gharibian and Marianne Monteiro and Hoi Lam and Shobha Vasudevan and Aditi Chaudhary and Isabela Albuquerque and Kilol Gupta and Sebastian Riedel and Chaitra Hegde and Avraham Ruderman and Andr\u00e1s Gy\u00f6rgy and Marcus Wainwright and Ashwin Chaugule and Burcu Karagol Ayan and Tomer Levinboim and Sam Shleifer and Yogesh Kalley and Vahab Mirrokni and Abhishek Rao and Prabakar Radhakrishnan and Jay Hartford and Jialin Wu and Zhenhai Zhu and Francesco Bertolini and Hao Xiong and Nicolas Serrano and Hamish Tomlinson and Myle Ott and Yifan Chang and Mark Graham and Jian Li and Marco Liang and Xiangzhu Long and Sebastian Borgeaud and Yanif Ahmad and Alex Grills and Diana Mincu and Martin Izzard and Yuan Liu and Jinyu Xie and Louis O'Bryan and Sameera Ponda and Simon Tong and Michelle Liu and Dan Malkin and Khalid Salama and Yuankai Chen and Rohan Anil and Anand Rao and Rigel Swavely and Misha Bilenko and Nina Anderson and Tat Tan and Jing Xie and Xing Wu and Lijun Yu and Oriol Vinyals and Andrey Ryabtsev and Rumen Dangovski and Kate Baumli and Daniel Keysers and Christian Wright and Zoe Ashwood and Betty Chan and Artem Shtefan and Yaohui Guo and Ankur Bapna and Radu Soricut and Steven Pecht and Sabela Ramos and Rui Wang and Jiahao Cai and Trieu Trinh and Paul Barham and Linda Friso and Eli Stickgold and Xiangzhuo Ding and Siamak Shakeri and Diego Ardila and Eleftheria Briakou and Phil Culliton and Adam Raveret and Jingyu Cui and David Saxton and Subhrajit Roy and Javad Azizi and Pengcheng Yin and Lucia Loher and Andrew Bunner and Min Choi and Faruk Ahmed and Eric Li and Yin Li and Shengyang Dai and Michael Elabd and Sriram Ganapathy and Shivani Agrawal and Yiqing Hua and Paige Kunkle and Sujeevan Rajayogam and Arun Ahuja and Arthur Conmy and Alex Vasiloff and Parker Beak and Christopher Yew and Jayaram Mudigonda and Bartek Wydrowski and Jon Blanton and Zhengdong Wang and Yann Dauphin and Zhuo Xu and Martin Polacek and Xi Chen and Hexiang Hu and Pauline Sho and Markus Kunesch and Mehdi Hafezi Manshadi and Eliza Rutherford and Bo Li and Sissie Hsiao and Iain Barr and Alex Tudor and Matija Kecman and Arsha Nagrani and Vladimir Pchelin and Martin Sundermeyer and Aishwarya P S and Abhijit Karmarkar and Yi Gao and Grishma Chole and Olivier Bachem and Isabel Gao and Arturo BC and Matt Dibb and Mauro Verzetti and Felix Hernandez-Campos and Yana Lunts and Matthew Johnson and Julia Di Trapani and Raphael Koster and Idan Brusilovsky and Binbin Xiong and Megha Mohabey and Han Ke and Joe Zou and Tea Saboli\u0107 and V\u00edctor Campos and John Palowitch and Alex Morris and Linhai Qiu and Pranavaraj Ponnuramu and Fangtao Li and Vivek Sharma and Kiranbir Sodhia and Kaan Tekelioglu and Aleksandr Chuklin and Madhavi Yenugula and Erika Gemzer and Theofilos Strinopoulos and Sam El-Husseini and Huiyu Wang and Yan Zhong and Edouard Leurent and Paul Natsev and Weijun Wang and Dre Mahaarachchi and Tao Zhu and Songyou Peng and Sami Alabed and Cheng-Chun Lee and Anthony Brohan and Arthur Szlam and GS Oh and Anton Kovsharov and Jenny Lee and Renee Wong and Megan Barnes and Gregory Thornton and Felix Gimeno and Omer Levy and Martin Sevenich and Melvin Johnson and Jonathan Mallinson and Robert Dadashi and Ziyue Wang and Qingchun Ren and Preethi Lahoti and Arka Dhar and Josh Feldman and Dan Zheng and Thatcher Ulrich and Liviu Panait and Michiel Blokzijl and Cip Baetu and Josip Matak and Jitendra Harlalka and Maulik Shah and Tal Marian and Daniel von Dincklage and Cosmo Du and Ruy Ley-Wild and Bethanie Brownfield and Max Schumacher and Yury Stuken and Shadi Noghabi and Sonal Gupta and Xiaoqi Ren and Eric Malmi and Felix Weissenberger and Blanca Huergo and Maria Bauza and Thomas Lampe and Arthur Douillard and Mojtaba Seyedhosseini and Roy Frostig and Zoubin Ghahramani and Kelvin Nguyen and Kashyap Krishnakumar and Chengxi Ye and Rahul Gupta and Alireza Nazari and Robert Geirhos and Pete Shaw and Ahmed Eleryan and Dima Damen and Jennimaria Palomaki and Ted Xiao and Qiyin Wu and Quan Yuan and Phoenix Meadowlark and Matthew Bilotti and Raymond Lin and Mukund Sridhar and Yannick Schroecker and Da-Woon Chung and Jincheng Luo and Trevor Strohman and Tianlin Liu and Anne Zheng and Jesse Emond and Wei Wang and Andrew Lampinen and Toshiyuki Fukuzawa and Folawiyo Campbell-Ajala and Monica Roy and James Lee-Thorp and Lily Wang and Iftekhar Naim and  Tony and  Nguy\\~\u00ean and Guy Bensky and Aditya Gupta and Dominika Rogozi\u0144ska and Justin Fu and Thanumalayan Sankaranarayana Pillai and Petar Veli\u010dkovi\u0107 and Shahar Drath and Philipp Neubeck and Vaibhav Tulsyan and Arseniy Klimovskiy and Don Metzler and Sage Stevens and Angel Yeh and Junwei Yuan and Tianhe Yu and Kelvin Zhang and Alec Go and Vincent Tsang and Ying Xu and Andy Wan and Isaac Galatzer-Levy and Sam Sobell and Abodunrinwa Toki and Elizabeth Salesky and Wenlei Zhou and Diego Antognini and Sholto Douglas and Shimu Wu and Adam Lelkes and Frank Kim and Paul Cavallaro and Ana Salazar and Yuchi Liu and James Besley and Tiziana Refice and Yiling Jia and Zhang Li and Michal Sokolik and Arvind Kannan and Jon Simon and Jo Chick and Avia Aharon and Meet Gandhi and Mayank Daswani and Keyvan Amiri and Vighnesh Birodkar and Abe Ittycheriah and Peter Grabowski and Oscar Chang and Charles Sutton and  Zhixin and  Lai and Umesh Telang and Susie Sargsyan and Tao Jiang and Raphael Hoffmann and Nicole Brichtova and Matteo Hessel and Jonathan Halcrow and Sammy Jerome and Geoff Brown and Alex Tomala and Elena Buchatskaya and Dian Yu and Sachit Menon and Pol Moreno and Yuguo Liao and Vicky Zayats and Luming Tang and SQ Mah and Ashish Shenoy and Alex Siegman and Majid Hadian and Okwan Kwon and Tao Tu and Nima Khajehnouri and Ryan Foley and Parisa Haghani and Zhongru Wu and Vaishakh Keshava and Khyatti Gupta and Tony Bruguier and Rui Yao and Danny Karmon and Luisa Zintgraf and Zhicheng Wang and Enrique Piqueras and Junehyuk Jung and Jenny Brennan and Diego Machado and Marissa Giustina and MH Tessler and Kamyu Lee and Qiao Zhang and Joss Moore and Kaspar Daugaard and Alexander Fr\u00f6mmgen and Jennifer Beattie and Fred Zhang and Daniel Kasenberg and Ty Geri and Danfeng Qin and Gaurav Singh Tomar and Tom Ouyang and Tianli Yu and Luowei Zhou and Rajiv Mathews and Andy Davis and Yaoyiran Li and Jai Gupta and Damion Yates and Linda Deng and Elizabeth Kemp and Ga-Young Joung and Sergei Vassilvitskii and Mandy Guo and Pallavi LV and Dave Dopson and Sami Lachgar and Lara McConnaughey and Himadri Choudhury and Dragos Dena and Aaron Cohen and Joshua Ainslie and Sergey Levi and Parthasarathy Gopavarapu and Polina Zablotskaia and Hugo Vallet and Sanaz Bahargam and Xiaodan Tang and Nenad Tomasev and Ethan Dyer and Daniel Balle and Hongrae Lee and William Bono and Jorge Gonzalez Mendez and Vadim Zubov and Shentao Yang and Ivor Rendulic and Yanyan Zheng and Andrew Hogue and Golan Pundak and Ralph Leith and Avishkar Bhoopchand and Michael Han and Mislav \u017dani\u0107 and Tom Schaul and Manolis Delakis and Tejas Iyer and Guanyu Wang and Harman Singh and Abdelrahman Abdelhamed and Tara Thomas and Siddhartha Brahma and Hilal Dib and Naveen Kumar and Wenxuan Zhou and Liang Bai and Pushkar Mishra and Jiao Sun and Valentin Anklin and Roykrong Sukkerd and Lauren Agubuzu and Anton Briukhov and Anmol Gulati and Maximilian Sieb and Fabio Pardo and Sara Nasso and Junquan Chen and Kexin Zhu and Tiberiu Sosea and Alex Goldin and Keith Rush and Spurthi Amba Hombaiah and Andreas Noever and Allan Zhou and Sam Haves and Mary Phuong and Jake Ades and Yi-ting Chen and Lin Yang and Joseph Pagadora and Stan Bileschi and Victor Cotruta and Rachel Saputro and Arijit Pramanik and Sean Ammirati and Dan Garrette and Kevin Villela and Tim Blyth and Canfer Akbulut and Neha Jha and Alban Rrustemi and Arissa Wongpanich and Chirag Nagpal and Yonghui Wu and Morgane Rivi\u00e8re and Sergey Kishchenko and Pranesh Srinivasan and Alice Chen and Animesh Sinha and Trang Pham and Bill Jia and Tom Hennigan and Anton Bakalov and Nithya Attaluri and Drew Garmon and Daniel Rodriguez and Dawid Wegner and Wenhao Jia and Evan Senter and Noah Fiedel and Denis Petek and Yuchuan Liu and Cassidy Hardin and Harshal Tushar Lehri and Joao Carreira and Sara Smoot and Marcel Prasetya and Nami Akazawa and Anca Stefanoiu and Chia-Hua Ho and Anelia Angelova and Kate Lin and Min Kim and Charles Chen and Marcin Sieniek and Alice Li and Tongfei Guo and Sorin Baltateanu and Pouya Tafti and Michael Wunder and Nadav Olmert and Divyansh Shukla and Jingwei Shen and Neel Kovelamudi and Balaji Venkatraman and Seth Neel and Romal Thoppilan and Jerome Connor and Frederik Benzing and Axel Stjerngren and Golnaz Ghiasi and Alex Polozov and Joshua Howland and Theophane Weber and Justin Chiu and Ganesh Poomal Girirajan and Andreas Terzis and Pidong Wang and Fangda Li and Yoav Ben Shalom and Dinesh Tewari and Matthew Denton and Roee Aharoni and Norbert Kalb and Heri Zhao and Junlin Zhang and Angelos Filos and Matthew Rahtz and Lalit Jain and Connie Fan and Vitor Rodrigues and Ruth Wang and Richard Shin and Jacob Austin and Roman Ring and Mariella Sanchez-Vargas and Mehadi Hassen and Ido Kessler and Uri Alon and Gufeng Zhang and Wenhu Chen and Yenai Ma and Xiance Si and Le Hou and Azalia Mirhoseini and Marc Wilson and Geoff Bacon and Becca Roelofs and Lei Shu and Gautam Vasudevan and Jonas Adler and Artur Dwornik and Tayfun Terzi and Matt Lawlor and Harry Askham and Mike Bernico and Xuanyi Dong and Chris Hidey and Kevin Kilgour and Ga\u00ebl Liu and Surya Bhupatiraju and Luke Leonhard and Siqi Zuo and Partha Talukdar and Qing Wei and Aliaksei Severyn and V\u00edt List\u00edk and Jong Lee and Aditya Tripathi and SK Park and Yossi Matias and Hao Liu and Alex Ruiz and Rajesh Jayaram and Jackson Tolins and Pierre Marcenac and Yiming Wang and Bryan Seybold and Henry Prior and Deepak Sharma and Jack Weber and Mikhail Sirotenko and Yunhsuan Sung and Dayou Du and Ellie Pavlick and Stefan Zinke and Markus Freitag and Max Dylla and Montse Gonzalez Arenas and Natan Potikha and Omer Goldman and Connie Tao and Rachita Chhaparia and Maria Voitovich and Pawan Dogra and Andrija Ra\u017enatovi\u0107 and Zak Tsai and Chong You and Oleaser Johnson and George Tucker and Chenjie Gu and Jae Yoo and Maryam Majzoubi and Valentin Gabeur and Bahram Raad and Rocky Rhodes and Kashyap Kolipaka and Heidi Howard and Geta Sampemane and Benny Li and Chulayuth Asawaroengchai and Duy Nguyen and Chiyuan Zhang and Timothee Cour and Xinxin Yu and Zhao Fu and Joe Jiang and Po-Sen Huang and Gabriela Surita and I\u00f1aki Iturrate and Yael Karov and Michael Collins and Martin Baeuml and Fabian Fuchs and Shilpa Shetty and Swaroop Ramaswamy and Sayna Ebrahimi and Qiuchen Guo and Jeremy Shar and Gabe Barth-Maron and Sravanti Addepalli and Bryan Richter and Chin-Yi Cheng and Eug\u00e9nie Rives and Fei Zheng and Johannes Griesser and Nishanth Dikkala and Yoel Zeldes and Ilkin Safarli and Dipanjan Das and Himanshu Srivastava and Sadh MNM Khan and Xin Li and Aditya Pandey and Larisa Markeeva and Dan Belov and Qiqi Yan and Miko\u0142aj Rybi\u0144ski and Tao Chen and Megha Nawhal and Michael Quinn and Vineetha Govindaraj and Sarah York and Reed Roberts and Roopal Garg and Namrata Godbole and Jake Abernethy and Anil Das and Lam Nguyen Thiet and Jonathan Tompson and John Nham and Neera Vats and Ben Caine and Wesley Helmholz and Francesco Pongetti and Yeongil Ko and James An and Clara Huiyi Hu and Yu-Cheng Ling and Julia Pawar and Robert Leland and Keisuke Kinoshita and Waleed Khawaja and Marco Selvi and Eugene Ie and Danila Sinopalnikov and Lev Proleev and Nilesh Tripuraneni and Michele Bevilacqua and Seungji Lee and Clayton Sanford and Dan Suh and Dustin Tran and Jeff Dean and Simon Baumgartner and Jens Heitkaemper and Sagar Gubbi and Kristina Toutanova and Yichong Xu and Chandu Thekkath and Keran Rong and Palak Jain and Annie Xie and Yan Virin and Yang Li and Lubo Litchev and Richard Powell and Tarun Bharti and Adam Kraft and Nan Hua and Marissa Ikonomidis and Ayal Hitron and Sanjiv Kumar and Loic Matthey and Sophie Bridgers and Lauren Lax and Ishaan Malhi and Ondrej Skopek and Ashish Gupta and Jiawei Cao and Mitchelle Rasquinha and Siim P\u00f5der and Wojciech Stokowiec and Nicholas Roth and Guowang Li and Micha\u00ebl Sander and Joshua Kessinger and Vihan Jain and Edward Loper and Wonpyo Park and Michal Yarom and Liqun Cheng and Guru Guruganesh and Kanishka Rao and Yan Li and Catarina Barros and Mikhail Sushkov and Chun-Sung Ferng and Rohin Shah and Ophir Aharoni and Ravin Kumar and Tim McConnell and Peiran Li and Chen Wang and Fernando Pereira and Craig Swanson and Fayaz Jamil and Yan Xiong and Anitha Vijayakumar and Prakash Shroff and Kedar Soparkar and Jindong Gu and Livio Baldini Soares and Eric Wang and Kushal Majmundar and Aurora Wei and Kai Bailey and Nora Kassner and Chizu Kawamoto and Goran \u017du\u017ei\u0107 and Victor Gomes and Abhirut Gupta and Michael Guzman and Ishita Dasgupta and Xinyi Bai and Zhufeng Pan and Francesco Piccinno and Hadas Natalie Vogel and Octavio Ponce and Adrian Hutter and Paul Chang and Pan-Pan Jiang and Ionel Gog and Vlad Ionescu and James Manyika and Fabian Pedregosa and Harry Ragan and Zach Behrman and Ryan Mullins and Coline Devin and Aroonalok Pyne and Swapnil Gawde and Martin Chadwick and Yiming Gu and Sasan Tavakkol and Andy Twigg and Naman Goyal and Ndidi Elue and Anna Goldie and Srinivasan Venkatachary and Hongliang Fei and Ziqiang Feng and Marvin Ritter and Isabel Leal and Sudeep Dasari and Pei Sun and Alif Raditya Rochman and Brendan O'Donoghue and Yuchen Liu and Jim Sproch and Kai Chen and Natalie Clay and Slav Petrov and Sailesh Sidhwani and Ioana Mihailescu and Alex Panagopoulos and AJ Piergiovanni and Yunfei Bai and George Powell and Deep Karkhanis and Trevor Yacovone and Petr Mitrichev and Joe Kovac and Dave Uthus and Amir Yazdanbakhsh and David Amos and Steven Zheng and Bing Zhang and Jin Miao and Bhuvana Ramabhadran and Soroush Radpour and Shantanu Thakoor and Josh Newlan and Oran Lang and Orion Jankowski and Shikhar Bharadwaj and Jean-Michel Sarr and Shereen Ashraf and Sneha Mondal and Jun Yan and Ankit Singh Rawat and Sarmishta Velury and Greg Kochanski and Tom Eccles and Franz Och and Abhanshu Sharma and Ethan Mahintorabi and Alex Gurney and Carrie Muir and Vered Cohen and Saksham Thakur and Adam Bloniarz and Asier Mujika and Alexander Pritzel and Paul Caron and Altaf Rahman and Fiona Lang and Yasumasa Onoe and Petar Sirkovic and Jay Hoover and Ying Jian and Pablo Duque and Arun Narayanan and David Soergel and Alex Haig and Loren Maggiore and Shyamal Buch and Josef Dean and Ilya Figotin and Igor Karpov and Shaleen Gupta and Denny Zhou and Muhuan Huang and Ashwin Vaswani and Christopher Semturs and Kaushik Shivakumar and Yu Watanabe and Vinodh Kumar Rajendran and Eva Lu and Yanhan Hou and Wenting Ye and Shikhar Vashishth and Nana Nti and Vytenis Sakenas and Darren Ni and Doug DeCarlo and Michael Bendersky and Sumit Bagri and Nacho Cano and Elijah Peake and Simon Tokumine and Varun Godbole and Carlos Gu\u00eda and Tanya Lando and Vittorio Selo and Seher Ellis and Danny Tarlow and Daniel Gillick and Alessandro Epasto and Siddhartha Reddy Jonnalagadda and Meng Wei and Meiyan Xie and Ankur Taly and Michela Paganini and Mukund Sundararajan and Daniel Toyama and Ting Yu and Dessie Petrova and Aneesh Pappu and Rohan Agrawal and Senaka Buthpitiya and Justin Frye and Thomas Buschmann and Remi Crocker and Marco Tagliasacchi and Mengchao Wang and Da Huang and Sagi Perel and Brian Wieder and Hideto Kazawa and Weiyue Wang and Jeremy Cole and Himanshu Gupta and Ben Golan and Seojin Bang and Nitish Kulkarni and Ken Franko and Casper Liu and Doug Reid and Sid Dalmia and Jay Whang and Kevin Cen and Prasha Sundaram and Johan Ferret and Berivan Isik and Lucian Ionita and Guan Sun and Anna Shekhawat and Muqthar Mohammad and Philip Pham and Ronny Huang and Karthik Raman and Xingyi Zhou and Ross Mcilroy and Austin Myers and Sheng Peng and Jacob Scott and Paul Covington and Sofia Erell and Pratik Joshi and Jo\u00e3o Gabriel Oliveira and Natasha Noy and Tajwar Nasir and Jake Walker and Vera Axelrod and Tim Dozat and Pu Han and Chun-Te Chu and Eugene Weinstein and Anand Shukla and Shreyas Chandrakaladharan and Petra Poklukar and Bonnie Li and Ye Jin and Prem Eruvbetine and Steven Hansen and Avigail Dabush and Alon Jacovi and Samrat Phatale and Chen Zhu and Steven Baker and Mo Shomrat and Yang Xiao and Jean Pouget-Abadie and Mingyang Zhang and Fanny Wei and Yang Song and Helen King and Yiling Huang and Yun Zhu and Ruoxi Sun and Juliana Vicente Franco and Chu-Cheng Lin and Sho Arora and  Hui and  Li and Vivian Xia and Luke Vilnis and Mariano Schain and Kaiz Alarakyia and Laurel Prince and Aaron Phillips and Caleb Habtegebriel and Luyao Xu and Huan Gui and Santiago Ontanon and Lora Aroyo and Karan Gill and Peggy Lu and Yash Katariya and Dhruv Madeka and Shankar Krishnan and Shubha Srinivas Raghvendra and James Freedman and Yi Tay and Gaurav Menghani and Peter Choy and Nishita Shetty and Dan Abolafia and Doron Kukliansky and Edward Chou and Jared Lichtarge and Ken Burke and Ben Coleman and Dee Guo and Larry Jin and Indro Bhattacharya and Victoria Langston and Yiming Li and Suyog Kotecha and Alex Yakubovich and Xinyun Chen and Petre Petrov and Tolly Powell and Yanzhang He and Corbin Quick and Kanav Garg and Dawsen Hwang and Yang Lu and Srinadh Bhojanapalli and Kristian Kjems and Ramin Mehran and Aaron Archer and Hado van Hasselt and Ashwin Balakrishna and JK Kearns and Meiqi Guo and Jason Riesa and Mikita Sazanovich and Xu Gao and Chris Sauer and Chengrun Yang and XiangHai Sheng and Thomas Jimma and Wouter Van Gansbeke and Vitaly Nikolaev and Wei Wei and Katie Millican and Ruizhe Zhao and Justin Snyder and Levent Bolelli and Maura O'Brien and Shawn Xu and Fei Xia and Wentao Yuan and Arvind Neelakantan and David Barker and Sachin Yadav and Hannah Kirkwood and Farooq Ahmad and Joel Wee and Jordan Grimstad and Boyu Wang and Matthew Wiethoff and Shane Settle and Miaosen Wang and Charles Blundell and Jingjing Chen and Chris Duvarney and Grace Hu and Olaf Ronneberger and Alex Lee and Yuanzhen Li and Abhishek Chakladar and Alena Butryna and Georgios Evangelopoulos and Guillaume Desjardins and Jonni Kanerva and Henry Wang and Averi Nowak and Nick Li and Alyssa Loo and Art Khurshudov and Laurent El Shafey and Nagabhushan Baddi and Karel Lenc and Yasaman Razeghi and Tom Lieber and Amer Sinha and Xiao Ma and Yao Su and James Huang and Asahi Ushio and Hanna Klimczak-Pluci\u0144ska and Kareem Mohamed and JD Chen and Simon Osindero and Stav Ginzburg and Lampros Lamprou and Vasilisa Bashlovkina and Duc-Hieu Tran and Ali Khodaei and Ankit Anand and Yixian Di and Ramy Eskander and Manish Reddy Vuyyuru and Jasmine Liu and Aishwarya Kamath and Roman Goldenberg and Mathias Bellaiche and Juliette Pluto and Bill Rosgen and Hassan Mansoor and William Wong and Suhas Ganesh and Eric Bailey and Scott Baird and Dan Deutsch and Jinoo Baek and Xuhui Jia and Chansoo Lee and Abe Friesen and Nathaniel Braun and Kate Lee and Amayika Panda and Steven M. Hernandez and Duncan Williams and Jianqiao Liu and Ethan Liang and Arnaud Autef and Emily Pitler and Deepali Jain and Phoebe Kirk and Oskar Bunyan and Jaume Sanchez Elias and Tongxin Yin and Machel Reid and Aedan Pope and Nikita Putikhin and Bidisha Samanta and Sergio Guadarrama and Dahun Kim and Simon Rowe and Marcella Valentine and Geng Yan and Alex Salcianu and David Silver and Gan Song and Richa Singh and Shuai Ye and Hannah DeBalsi and Majd Al Merey and Eran Ofek and Albert Webson and Shibl Mourad and Ashwin Kakarla and Silvio Lattanzi and Nick Roy and Evgeny Sluzhaev and Christina Butterfield and Alessio Tonioni and Nathan Waters and Sudhindra Kopalle and Jason Chase and James Cohan and Girish Ramchandra Rao and Robert Berry and Michael Voznesensky and Shuguang Hu and Kristen Chiafullo and Sharat Chikkerur and George Scrivener and Ivy Zheng and Jeremy Wiesner and Wolfgang Macherey and Timothy Lillicrap and Fei Liu and Brian Walker and David Welling and Elinor Davies and Yangsibo Huang and Lijie Ren and Nir Shabat and Alessandro Agostini and Mariko Iinuma and Dustin Zelle and Rohit Sathyanarayana and Andrea D'olimpio and Morgan Redshaw and Matt Ginsberg and Ashwin Murthy and Mark Geller and Tatiana Matejovicova and Ayan Chakrabarti and Ryan Julian and Christine Chan and Qiong Hu and Daniel Jarrett and Manu Agarwal and Jeshwanth Challagundla and Tao Li and Sandeep Tata and Wen Ding and Maya Meng and Zhuyun Dai and Giulia Vezzani and Shefali Garg and Jannis Bulian and Mary Jasarevic and Honglong Cai and Harish Rajamani and Adam Santoro and Florian Hartmann and Chen Liang and Bartek Perz and Apoorv Jindal and Fan Bu and Sungyong Seo and Ryan Poplin and Adrian Goedeckemeyer and Badih Ghazi and Nikhil Khadke and Leon Liu and Kevin Mather and Mingda Zhang and Ali Shah and Alex Chen and Jinliang Wei and Keshav Shivam and Yuan Cao and Donghyun Cho and Angelo Scorza Scarpati and Michael Moffitt and Clara Barbu and Ivan Jurin and Ming-Wei Chang and Hongbin Liu and Hao Zheng and Shachi Dave and Christine Kaeser-Chen and Xiaobin Yu and Alvin Abdagic and Lucas Gonzalez and Yanping Huang and Peilin Zhong and Cordelia Schmid and Bryce Petrini and Alex Wertheim and Jifan Zhu and Hoang Nguyen and Kaiyang Ji and Yanqi Zhou and Tao Zhou and Fangxiaoyu Feng and Regev Cohen and David Rim and Shubham Milind Phal and Petko Georgiev and Ariel Brand and Yue Ma and Wei Li and Somit Gupta and Chao Wang and Pavel Dubov and Jean Tarbouriech and Kingshuk Majumder and Huijian Li and Norman Rink and Apurv Suman and Yang Guo and Yinghao Sun and Arun Nair and Xiaowei Xu and Mohamed Elhawaty and Rodrigo Cabrera and Guangxing Han and Julian Eisenschlos and Junwen Bai and Yuqi Li and Yamini Bansal and Thibault Sellam and Mina Khan and Hung Nguyen and Justin Mao-Jones and Nikos Parotsidis and Jake Marcus and Cindy Fan and Roland Zimmermann and Yony Kochinski and Laura Graesser and Feryal Behbahani and Alvaro Caceres and Michael Riley and Patrick Kane and Sandra Lefdal and Rob Willoughby and Paul Vicol and Lun Wang and Shujian Zhang and Ashleah Gill and Yu Liang and Gautam Prasad and Soroosh Mariooryad and Mehran Kazemi and Zifeng Wang and Kritika Muralidharan and Paul Voigtlaender and Jeffrey Zhao and Huanjie Zhou and Nina D'Souza and Aditi Mavalankar and S\u00e9b Arnold and Nick Young and Obaid Sarvana and Chace Lee and Milad Nasr and Tingting Zou and Seokhwan Kim and Lukas Haas and Kaushal Patel and Neslihan Bulut and David Parkinson and Courtney Biles and Dmitry Kalashnikov and Chi Ming To and Aviral Kumar and Jessica Austin and Alex Greve and Lei Zhang and Megha Goel and Yeqing Li and Sergey Yaroshenko and Max Chang and Abhishek Jindal and Geoff Clark and Hagai Taitelbaum and Dale Johnson and Ofir Roval and Jeongwoo Ko and Anhad Mohananey and Christian Schuler and Shenil Dodhia and Ruichao Li and Kazuki Osawa and Claire Cui and Peng Xu and Rushin Shah and Tao Huang and Ela Gruzewska and Nathan Clement and Mudit Verma and Olcan Sercinoglu and Hai Qian and Viral Shah and Masa Yamaguchi and Abhinit Modi and Takahiro Kosakai and Thomas Strohmann and Junhao Zeng and Beliz Gunel and Jun Qian and Austin Tarango and Krzysztof Jastrz\u0119bski and Robert David and Jyn Shan and Parker Schuh and Kunal Lad and Willi Gierke and Mukundan Madhavan and Xinyi Chen and Mark Kurzeja and Rebeca Santamaria-Fernandez and Dawn Chen and Alexandra Cordell and Yuri Chervonyi and Frankie Garcia and Nithish Kannen and Vincent Perot and Nan Ding and Shlomi Cohen-Ganor and Victor Lavrenko and Junru Wu and Georgie Evans and Cicero Nogueira dos Santos and Madhavi Sewak and Ashley Brown and Andrew Hard and Joan Puigcerver and Zeyu Zheng and Yizhong Liang and Evgeny Gladchenko and Reeve Ingle and Uri First and Pierre Sermanet and Charlotte Magister and Mihajlo Velimirovi\u0107 and Sashank Reddi and Susanna Ricco and Eirikur Agustsson and Hartwig Adam and Nir Levine and David Gaddy and Dan Holtmann-Rice and Xuanhui Wang and Ashutosh Sathe and Abhijit Guha Roy and Bla\u017e Bratani\u010d and Alen Carin and Harsh Mehta and Silvano Bonacina and Nicola De Cao and Mara Finkelstein and Verena Rieser and Xinyi Wu and Florent Altch\u00e9 and Dylan Scandinaro and Li Li and Nino Vieillard and Nikhil Sethi and Garrett Tanzer and Zhi Xing and Shibo Wang and Parul Bhatia and Gui Citovsky and Thomas Anthony and Sharon Lin and Tianze Shi and Shoshana Jakobovits and Gena Gibson and Raj Apte and Lisa Lee and Mingqing Chen and Arunkumar Byravan and Petros Maniatis and Kellie Webster and Andrew Dai and Pu-Chin Chen and Jiaqi Pan and Asya Fadeeva and Zach Gleicher and Thang Luong and Niket Kumar Bhumihar", "abstract": "  In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.\n", "link": "http://arxiv.org/abs/2507.06261v4", "date": "2025-07-22", "relevancy": 2.374, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemini%202.5%3A%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%0A%20%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities&body=Title%3A%20Gemini%202.5%3A%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%0A%20%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities%0AAuthor%3A%20Gheorghe%20Comanici%20and%20Eric%20Bieber%20and%20Mike%20Schaekermann%20and%20Ice%20Pasupat%20and%20Noveen%20Sachdeva%20and%20Inderjit%20Dhillon%20and%20Marcel%20Blistein%20and%20Ori%20Ram%20and%20Dan%20Zhang%20and%20Evan%20Rosen%20and%20Luke%20Marris%20and%20Sam%20Petulla%20and%20Colin%20Gaffney%20and%20Asaf%20Aharoni%20and%20Nathan%20Lintz%20and%20Tiago%20Cardal%20Pais%20and%20Henrik%20Jacobsson%20and%20Idan%20Szpektor%20and%20Nan-Jiang%20Jiang%20and%20Krishna%20Haridasan%20and%20Ahmed%20Omran%20and%20Nikunj%20Saunshi%20and%20Dara%20Bahri%20and%20Gaurav%20Mishra%20and%20Eric%20Chu%20and%20Toby%20Boyd%20and%20Brad%20Hekman%20and%20Aaron%20Parisi%20and%20Chaoyi%20Zhang%20and%20Kornraphop%20Kawintiranon%20and%20Tania%20Bedrax-Weiss%20and%20Oliver%20Wang%20and%20Ya%20Xu%20and%20Ollie%20Purkiss%20and%20Uri%20Mendlovic%20and%20Ila%C3%AF%20Deutel%20and%20Nam%20Nguyen%20and%20Adam%20Langley%20and%20Flip%20Korn%20and%20Lucia%20Rossazza%20and%20Alexandre%20Ram%C3%A9%20and%20Sagar%20Waghmare%20and%20Helen%20Miller%20and%20Nathan%20Byrd%20and%20Ashrith%20Sheshan%20and%20Raia%20Hadsell%20Sangnie%20Bhardwaj%20and%20Pawel%20Janus%20and%20Tero%20Rissa%20and%20Dan%20Horgan%20and%20Sharon%20Silver%20and%20Ayzaan%20Wahid%20and%20Sergey%20Brin%20and%20Yves%20Raimond%20and%20Klemen%20Kloboves%20and%20Cindy%20Wang%20and%20Nitesh%20Bharadwaj%20Gundavarapu%20and%20Ilia%20Shumailov%20and%20Bo%20Wang%20and%20Mantas%20Pajarskas%20and%20Joe%20Heyward%20and%20Martin%20Nikoltchev%20and%20Maciej%20Kula%20and%20Hao%20Zhou%20and%20Zachary%20Garrett%20and%20Sushant%20Kafle%20and%20Sercan%20Arik%20and%20Ankita%20Goel%20and%20Mingyao%20Yang%20and%20Jiho%20Park%20and%20Koji%20Kojima%20and%20Parsa%20Mahmoudieh%20and%20Koray%20Kavukcuoglu%20and%20Grace%20Chen%20and%20Doug%20Fritz%20and%20Anton%20Bulyenov%20and%20Sudeshna%20Roy%20and%20Dimitris%20Paparas%20and%20Hadar%20Shemtov%20and%20Bo-Juen%20Chen%20and%20Robin%20Strudel%20and%20David%20Reitter%20and%20Aurko%20Roy%20and%20Andrey%20Vlasov%20and%20Changwan%20Ryu%20and%20Chas%20Leichner%20and%20Haichuan%20Yang%20and%20Zelda%20Mariet%20and%20Denis%20Vnukov%20and%20Tim%20Sohn%20and%20Amy%20Stuart%20and%20Wei%20Liang%20and%20Minmin%20Chen%20and%20Praynaa%20Rawlani%20and%20Christy%20Koh%20and%20JD%20Co-Reyes%20and%20Guangda%20Lai%20and%20Praseem%20Banzal%20and%20Dimitrios%20Vytiniotis%20and%20Jieru%20Mei%20and%20Mu%20Cai%20and%20Mohammed%20Badawi%20and%20Corey%20Fry%20and%20Ale%20Hartman%20and%20Daniel%20Zheng%20and%20Eric%20Jia%20and%20James%20Keeling%20and%20Annie%20Louis%20and%20Ying%20Chen%20and%20Efren%20Robles%20and%20Wei-Chih%20Hung%20and%20Howard%20Zhou%20and%20Nikita%20Saxena%20and%20Sonam%20Goenka%20and%20Olivia%20Ma%20and%20Zach%20Fisher%20and%20Mor%20Hazan%20Taege%20and%20Emily%20Graves%20and%20David%20Steiner%20and%20Yujia%20Li%20and%20Sarah%20Nguyen%20and%20Rahul%20Sukthankar%20and%20Joe%20Stanton%20and%20Ali%20Eslami%20and%20Gloria%20Shen%20and%20Berkin%20Akin%20and%20Alexey%20Guseynov%20and%20Yiqian%20Zhou%20and%20Jean-Baptiste%20Alayrac%20and%20Armand%20Joulin%20and%20Efrat%20Farkash%20and%20Ashish%20Thapliyal%20and%20Stephen%20Roller%20and%20Noam%20Shazeer%20and%20Todor%20Davchev%20and%20Terry%20Koo%20and%20Hannah%20Forbes-Pollard%20and%20Kartik%20Audhkhasi%20and%20Greg%20Farquhar%20and%20Adi%20Mayrav%20Gilady%20and%20Maggie%20Song%20and%20John%20Aslanides%20and%20Piermaria%20Mendolicchio%20and%20Alicia%20Parrish%20and%20John%20Blitzer%20and%20Pramod%20Gupta%20and%20Xiaoen%20Ju%20and%20Xiaochen%20Yang%20and%20Puranjay%20Datta%20and%20Andrea%20Tacchetti%20and%20Sanket%20Vaibhav%20Mehta%20and%20Gregory%20Dibb%20and%20Shubham%20Gupta%20and%20Federico%20Piccinini%20and%20Raia%20Hadsell%20and%20Sujee%20Rajayogam%20and%20Jiepu%20Jiang%20and%20Patrick%20Griffin%20and%20Patrik%20Sundberg%20and%20Jamie%20Hayes%20and%20Alexey%20Frolov%20and%20Tian%20Xie%20and%20Adam%20Zhang%20and%20Kingshuk%20Dasgupta%20and%20Uday%20Kalra%20and%20Lior%20Shani%20and%20Klaus%20Macherey%20and%20Tzu-Kuo%20Huang%20and%20Liam%20MacDermed%20and%20Karthik%20Duddu%20and%20Paulo%20Zacchello%20and%20Zi%20Yang%20and%20Jessica%20Lo%20and%20Kai%20Hui%20and%20Matej%20Kastelic%20and%20Derek%20Gasaway%20and%20Qijun%20Tan%20and%20Summer%20Yue%20and%20Pablo%20Barrio%20and%20John%20Wieting%20and%20Weel%20Yang%20and%20Andrew%20Nystrom%20and%20Solomon%20Demmessie%20and%20Anselm%20Levskaya%20and%20Fabio%20Viola%20and%20Chetan%20Tekur%20and%20Greg%20Billock%20and%20George%20Necula%20and%20Mandar%20Joshi%20and%20Rylan%20Schaeffer%20and%20Swachhand%20Lokhande%20and%20Christina%20Sorokin%20and%20Pradeep%20Shenoy%20and%20Mia%20Chen%20and%20Mark%20Collier%20and%20Hongji%20Li%20and%20Taylor%20Bos%20and%20Nevan%20Wichers%20and%20Sun%20Jae%20Lee%20and%20Ang%C3%A9line%20Pouget%20and%20Santhosh%20Thangaraj%20and%20Kyriakos%20Axiotis%20and%20Phil%20Crone%20and%20Rachel%20Sterneck%20and%20Nikolai%20Chinaev%20and%20Victoria%20Krakovna%20and%20Oleksandr%20Ferludin%20and%20Ian%20Gemp%20and%20Stephanie%20Winkler%20and%20Dan%20Goldberg%20and%20Ivan%20Korotkov%20and%20Kefan%20Xiao%20and%20Malika%20Mehrotra%20and%20Sandeep%20Mariserla%20and%20Vihari%20Piratla%20and%20Terry%20Thurk%20and%20Khiem%20Pham%20and%20Hongxu%20Ma%20and%20Alexandre%20Senges%20and%20Ravi%20Kumar%20and%20Clemens%20Meyer%20and%20Ellie%20Talius%20and%20Nuo%20Wang%20Pierse%20and%20Ballie%20Sandhu%20and%20Horia%20Toma%20and%20Kuo%20Lin%20and%20Swaroop%20Nath%20and%20Tom%20Stone%20and%20Dorsa%20Sadigh%20and%20Nikita%20Gupta%20and%20Arthur%20Guez%20and%20Avi%20Singh%20and%20Matt%20Thomas%20and%20Tom%20Duerig%20and%20Yuan%20Gong%20and%20Richard%20Tanburn%20and%20Lydia%20Lihui%20Zhang%20and%20Phuong%20Dao%20and%20Mohamed%20Hammad%20and%20Sirui%20Xie%20and%20Shruti%20Rijhwani%20and%20Ben%20Murdoch%20and%20Duhyeon%20Kim%20and%20Will%20Thompson%20and%20Heng-Tze%20Cheng%20and%20Daniel%20Sohn%20and%20Pablo%20Sprechmann%20and%20Qiantong%20Xu%20and%20Srinivas%20Tadepalli%20and%20Peter%20Young%20and%20Ye%20Zhang%20and%20Hansa%20Srinivasan%20and%20Miranda%20Aperghis%20and%20Aditya%20Ayyar%20and%20Hen%20Fitoussi%20and%20Ryan%20Burnell%20and%20David%20Madras%20and%20Mike%20Dusenberry%20and%20Xi%20Xiong%20and%20Tayo%20Oguntebi%20and%20Ben%20Albrecht%20and%20J%C3%B6rg%20Bornschein%20and%20Jovana%20Mitrovi%C4%87%20and%20Mason%20Dimarco%20and%20Bhargav%20Kanagal%20Shamanna%20and%20Premal%20Shah%20and%20Eren%20Sezener%20and%20Shyam%20Upadhyay%20and%20Dave%20Lacey%20and%20Craig%20Schiff%20and%20Sebastien%20Baur%20and%20Sanjay%20Ganapathy%20and%20Eva%20Schnider%20and%20Mateo%20Wirth%20and%20Connor%20Schenck%20and%20Andrey%20Simanovsky%20and%20Yi-Xuan%20Tan%20and%20Philipp%20Fr%C3%A4nken%20and%20Dennis%20Duan%20and%20Bharath%20Mankalale%20and%20Nikhil%20Dhawan%20and%20Kevin%20Sequeira%20and%20Zichuan%20Wei%20and%20Shivanker%20Goel%20and%20Caglar%20Unlu%20and%20Yukun%20Zhu%20and%20Haitian%20Sun%20and%20Ananth%20Balashankar%20and%20Kurt%20Shuster%20and%20Megh%20Umekar%20and%20Mahmoud%20Alnahlawi%20and%20A%C3%A4ron%20van%20den%20Oord%20and%20Kelly%20Chen%20and%20Yuexiang%20Zhai%20and%20Zihang%20Dai%20and%20Kuang-Huei%20Lee%20and%20Eric%20Doi%20and%20Lukas%20Zilka%20and%20Rohith%20Vallu%20and%20Disha%20Shrivastava%20and%20Jason%20Lee%20and%20Hisham%20Husain%20and%20Honglei%20Zhuang%20and%20Vincent%20Cohen-Addad%20and%20Jarred%20Barber%20and%20James%20Atwood%20and%20Adam%20Sadovsky%20and%20Quentin%20Wellens%20and%20Steven%20Hand%20and%20Arunkumar%20Rajendran%20and%20Aybuke%20Turker%20and%20CJ%20Carey%20and%20Yuanzhong%20Xu%20and%20Hagen%20Soltau%20and%20Zefei%20Li%20and%20Xinying%20Song%20and%20Conglong%20Li%20and%20Iurii%20Kemaev%20and%20Sasha%20Brown%20and%20Andrea%20Burns%20and%20Viorica%20Patraucean%20and%20Piotr%20Stanczyk%20and%20Renga%20Aravamudhan%20and%20Mathieu%20Blondel%20and%20Hila%20Noga%20and%20Lorenzo%20Blanco%20and%20Will%20Song%20and%20Michael%20Isard%20and%20Mandar%20Sharma%20and%20Reid%20Hayes%20and%20Dalia%20El%20Badawy%20and%20Avery%20Lamp%20and%20Itay%20Laish%20and%20Olga%20Kozlova%20and%20Kelvin%20Chan%20and%20Sahil%20Singla%20and%20Srinivas%20Sunkara%20and%20Mayank%20Upadhyay%20and%20Chang%20Liu%20and%20Aijun%20Bai%20and%20Jarek%20Wilkiewicz%20and%20Martin%20Zlocha%20and%20Jeremiah%20Liu%20and%20Zhuowan%20Li%20and%20Haiguang%20Li%20and%20Omer%20Barak%20and%20Ganna%20Raboshchuk%20and%20Jiho%20Choi%20and%20Fangyu%20Liu%20and%20Erik%20Jue%20and%20Mohit%20Sharma%20and%20Andreea%20Marzoca%20and%20Robert%20Busa-Fekete%20and%20Anna%20Korsun%20and%20Andre%20Elisseeff%20and%20Zhe%20Shen%20and%20Sara%20Mc%20Carthy%20and%20Kay%20Lamerigts%20and%20Anahita%20Hosseini%20and%20Hanzhao%20Lin%20and%20Charlie%20Chen%20and%20Fan%20Yang%20and%20Kushal%20Chauhan%20and%20Mark%20Omernick%20and%20Dawei%20Jia%20and%20Karina%20Zainullina%20and%20Demis%20Hassabis%20and%20Danny%20Vainstein%20and%20Ehsan%20Amid%20and%20Xiang%20Zhou%20and%20Ronny%20Votel%20and%20Eszter%20V%C3%A9rtes%20and%20Xinjian%20Li%20and%20Zongwei%20Zhou%20and%20Angeliki%20Lazaridou%20and%20Brendan%20McMahan%20and%20Arjun%20Narayanan%20and%20Hubert%20Soyer%20and%20Sujoy%20Basu%20and%20Kayi%20Lee%20and%20Bryan%20Perozzi%20and%20Qin%20Cao%20and%20Leonard%20Berrada%20and%20Rahul%20Arya%20and%20Ke%20Chen%20and%20%20Katrina%20and%20%20Xu%20and%20Matthias%20Lochbrunner%20and%20Alex%20Hofer%20and%20Sahand%20Sharifzadeh%20and%20Renjie%20Wu%20and%20Sally%20Goldman%20and%20Pranjal%20Awasthi%20and%20Xuezhi%20Wang%20and%20Yan%20Wu%20and%20Claire%20Sha%20and%20Biao%20Zhang%20and%20Maciej%20Miku%C5%82a%20and%20Filippo%20Graziano%20and%20Siobhan%20Mcloughlin%20and%20Irene%20Giannoumis%20and%20Youhei%20Namiki%20and%20Chase%20Malik%20and%20Carey%20Radebaugh%20and%20Jamie%20Hall%20and%20Ramiro%20Leal-Cavazos%20and%20Jianmin%20Chen%20and%20Vikas%20Sindhwani%20and%20David%20Kao%20and%20David%20Greene%20and%20Jordan%20Griffith%20and%20Chris%20Welty%20and%20Ceslee%20Montgomery%20and%20Toshihiro%20Yoshino%20and%20Liangzhe%20Yuan%20and%20Noah%20Goodman%20and%20Assaf%20Hurwitz%20Michaely%20and%20Kevin%20Lee%20and%20KP%20Sawhney%20and%20Wei%20Chen%20and%20Zheng%20Zheng%20and%20Megan%20Shum%20and%20Nikolay%20Savinov%20and%20Etienne%20Pot%20and%20Alex%20Pak%20and%20Morteza%20Zadimoghaddam%20and%20Sijal%20Bhatnagar%20and%20Yoad%20Lewenberg%20and%20Blair%20Kutzman%20and%20Ji%20Liu%20and%20Lesley%20Katzen%20and%20Jeremy%20Selier%20and%20Josip%20Djolonga%20and%20Dmitry%20Lepikhin%20and%20Kelvin%20Xu%20and%20Jacky%20Liang%20and%20Jiewen%20Tan%20and%20Benoit%20Schillings%20and%20Muge%20Ersoy%20and%20Pete%20Blois%20and%20Bernd%20Bandemer%20and%20Abhimanyu%20Singh%20and%20Sergei%20Lebedev%20and%20Pankaj%20Joshi%20and%20Adam%20R.%20Brown%20and%20Evan%20Palmer%20and%20Shreya%20Pathak%20and%20Komal%20Jalan%20and%20Fedir%20Zubach%20and%20Shuba%20Lall%20and%20Randall%20Parker%20and%20Alok%20Gunjan%20and%20Sergey%20Rogulenko%20and%20Sumit%20Sanghai%20and%20Zhaoqi%20Leng%20and%20Zoltan%20Egyed%20and%20Shixin%20Li%20and%20Maria%20Ivanova%20and%20Kostas%20Andriopoulos%20and%20Jin%20Xie%20and%20Elan%20Rosenfeld%20and%20Auriel%20Wright%20and%20Ankur%20Sharma%20and%20Xinyang%20Geng%20and%20Yicheng%20Wang%20and%20Sam%20Kwei%20and%20Renke%20Pan%20and%20Yujing%20Zhang%20and%20Gabby%20Wang%20and%20Xi%20Liu%20and%20Chak%20Yeung%20and%20Elizabeth%20Cole%20and%20Aviv%20Rosenberg%20and%20Zhen%20Yang%20and%20Phil%20Chen%20and%20George%20Polovets%20and%20Pranav%20Nair%20and%20Rohun%20Saxena%20and%20Josh%20Smith%20and%20Shuo-yiin%20Chang%20and%20Aroma%20Mahendru%20and%20Svetlana%20Grant%20and%20Anand%20Iyer%20and%20Irene%20Cai%20and%20Jed%20McGiffin%20and%20Jiaming%20Shen%20and%20Alanna%20Walton%20and%20Antonious%20Girgis%20and%20Oliver%20Woodman%20and%20Rosemary%20Ke%20and%20Mike%20Kwong%20and%20Louis%20Rouillard%20and%20Jinmeng%20Rao%20and%20Zhihao%20Li%20and%20Yuntao%20Xu%20and%20Flavien%20Prost%20and%20Chi%20Zou%20and%20Ziwei%20Ji%20and%20Alberto%20Magni%20and%20Tyler%20Liechty%20and%20Dan%20A.%20Calian%20and%20Deepak%20Ramachandran%20and%20Igor%20Krivokon%20and%20Hui%20Huang%20and%20Terry%20Chen%20and%20Anja%20Hauth%20and%20Anastasija%20Ili%C4%87%20and%20Weijuan%20Xi%20and%20Hyeontaek%20Lim%20and%20Vlad-Doru%20Ion%20and%20Pooya%20Moradi%20and%20Metin%20Toksoz-Exley%20and%20Kalesha%20Bullard%20and%20Miltos%20Allamanis%20and%20Xiaomeng%20Yang%20and%20Sophie%20Wang%20and%20Zhi%20Hong%20and%20Anita%20Gergely%20and%20Cheng%20Li%20and%20Bhavishya%20Mittal%20and%20Vitaly%20Kovalev%20and%20Victor%20Ungureanu%20and%20Jane%20Labanowski%20and%20Jan%20Wassenberg%20and%20Nicolas%20Lacasse%20and%20Geoffrey%20Cideron%20and%20Petar%20Devi%C4%87%20and%20Annie%20Marsden%20and%20Lynn%20Nguyen%20and%20Michael%20Fink%20and%20Yin%20Zhong%20and%20Tatsuya%20Kiyono%20and%20Desi%20Ivanov%20and%20Sally%20Ma%20and%20Max%20Bain%20and%20Kiran%20Yalasangi%20and%20Jennifer%20She%20and%20Anastasia%20Petrushkina%20and%20Mayank%20Lunayach%20and%20Carla%20Bromberg%20and%20Sarah%20Hodkinson%20and%20Vilobh%20Meshram%20and%20Daniel%20Vlasic%20and%20Austin%20Kyker%20and%20Steve%20Xu%20and%20Jeff%20Stanway%20and%20Zuguang%20Yang%20and%20Kai%20Zhao%20and%20Matthew%20Tung%20and%20Seth%20Odoom%20and%20Yasuhisa%20Fujii%20and%20Justin%20Gilmer%20and%20Eunyoung%20Kim%20and%20Felix%20Halim%20and%20Quoc%20Le%20and%20Bernd%20Bohnet%20and%20Seliem%20El-Sayed%20and%20Behnam%20Neyshabur%20and%20Malcolm%20Reynolds%20and%20Dean%20Reich%20and%20Yang%20Xu%20and%20Erica%20Moreira%20and%20Anuj%20Sharma%20and%20Zeyu%20Liu%20and%20Mohammad%20Javad%20Hosseini%20and%20Naina%20Raisinghani%20and%20Yi%20Su%20and%20Ni%20Lao%20and%20Daniel%20Formoso%20and%20Marco%20Gelmi%20and%20Almog%20Gueta%20and%20Tapomay%20Dey%20and%20Elena%20Gribovskaya%20and%20Domagoj%20%C4%86evid%20and%20Sidharth%20Mudgal%20and%20Garrett%20Bingham%20and%20Jianling%20Wang%20and%20Anurag%20Kumar%20and%20Alex%20Cullum%20and%20Feng%20Han%20and%20Konstantinos%20Bousmalis%20and%20Diego%20Cedillo%20and%20Grace%20Chu%20and%20Vladimir%20Magay%20and%20Paul%20Michel%20and%20Ester%20Hlavnova%20and%20Daniele%20Calandriello%20and%20Setareh%20Ariafar%20and%20Kaisheng%20Yao%20and%20Vikash%20Sehwag%20and%20Arpi%20Vezer%20and%20Agustin%20Dal%20Lago%20and%20Zhenkai%20Zhu%20and%20Paul%20Kishan%20Rubenstein%20and%20Allen%20Porter%20and%20Anirudh%20Baddepudi%20and%20Oriana%20Riva%20and%20Mihai%20Dorin%20Istin%20and%20Chih-Kuan%20Yeh%20and%20Zhi%20Li%20and%20Andrew%20Howard%20and%20Nilpa%20Jha%20and%20Jeremy%20Chen%20and%20Raoul%20de%20Liedekerke%20and%20Zafarali%20Ahmed%20and%20Mikel%20Rodriguez%20and%20Tanuj%20Bhatia%20and%20Bangju%20Wang%20and%20Ali%20Elqursh%20and%20David%20Klinghoffer%20and%20Peter%20Chen%20and%20Pushmeet%20Kohli%20and%20Te%20I%20and%20Weiyang%20Zhang%20and%20Zack%20Nado%20and%20Jilin%20Chen%20and%20Maxwell%20Chen%20and%20George%20Zhang%20and%20Aayush%20Singh%20and%20Adam%20Hillier%20and%20Federico%20Lebron%20and%20Yiqing%20Tao%20and%20Ting%20Liu%20and%20Gabriel%20Dulac-Arnold%20and%20Jingwei%20Zhang%20and%20Shashi%20Narayan%20and%20Buhuang%20Liu%20and%20Orhan%20Firat%20and%20Abhishek%20Bhowmick%20and%20Bingyuan%20Liu%20and%20Hao%20Zhang%20and%20Zizhao%20Zhang%20and%20Georges%20Rotival%20and%20Nathan%20Howard%20and%20Anu%20Sinha%20and%20Alexander%20Grushetsky%20and%20Benjamin%20Beyret%20and%20Keerthana%20Gopalakrishnan%20and%20James%20Zhao%20and%20Kyle%20He%20and%20Szabolcs%20Payrits%20and%20Zaid%20Nabulsi%20and%20Zhaoyi%20Zhang%20and%20Weijie%20Chen%20and%20Edward%20Lee%20and%20Nova%20Fallen%20and%20Sreenivas%20Gollapudi%20and%20Aurick%20Zhou%20and%20Filip%20Paveti%C4%87%20and%20Thomas%20K%C3%B6ppe%20and%20Shiyu%20Huang%20and%20Rama%20Pasumarthi%20and%20Nick%20Fernando%20and%20Felix%20Fischer%20and%20Daria%20%C4%86urko%20and%20Yang%20Gao%20and%20James%20Svensson%20and%20Austin%20Stone%20and%20Haroon%20Qureshi%20and%20Abhishek%20Sinha%20and%20Apoorv%20Kulshreshtha%20and%20Martin%20Matysiak%20and%20Jieming%20Mao%20and%20Carl%20Saroufim%20and%20Aleksandra%20Faust%20and%20Qingnan%20Duan%20and%20Gil%20Fidel%20and%20Kaan%20Katircioglu%20and%20Rapha%C3%ABl%20Lopez%20Kaufman%20and%20Dhruv%20Shah%20and%20Weize%20Kong%20and%20Abhishek%20Bapna%20and%20Gell%C3%A9rt%20Weisz%20and%20Emma%20Dunleavy%20and%20Praneet%20Dutta%20and%20Tianqi%20Liu%20and%20Rahma%20Chaabouni%20and%20Carolina%20Parada%20and%20Marcus%20Wu%20and%20Alexandra%20Belias%20and%20Alessandro%20Bissacco%20and%20Stanislav%20Fort%20and%20Li%20Xiao%20and%20Fantine%20Huot%20and%20Chris%20Knutsen%20and%20Yochai%20Blau%20and%20Gang%20Li%20and%20Jennifer%20Prendki%20and%20Juliette%20Love%20and%20Yinlam%20Chow%20and%20Pichi%20Charoenpanit%20and%20Hidetoshi%20Shimokawa%20and%20Vincent%20Coriou%20and%20Karol%20Gregor%20and%20Tomas%20Izo%20and%20Arjun%20Akula%20and%20Mario%20Pinto%20and%20Chris%20Hahn%20and%20Dominik%20Paulus%20and%20Jiaxian%20Guo%20and%20Neha%20Sharma%20and%20Cho-Jui%20Hsieh%20and%20Adaeze%20Chukwuka%20and%20Kazuma%20Hashimoto%20and%20Nathalie%20Rauschmayr%20and%20Ling%20Wu%20and%20Christof%20Angermueller%20and%20Yulong%20Wang%20and%20Sebastian%20Gerlach%20and%20Michael%20Pliskin%20and%20Daniil%20Mirylenka%20and%20Min%20Ma%20and%20Lexi%20Baugher%20and%20Bryan%20Gale%20and%20Shaan%20Bijwadia%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20David%20Wood%20and%20Jane%20Park%20and%20Chung-Ching%20Chang%20and%20Babi%20Seal%20and%20Chris%20Tar%20and%20Kacper%20Krasowiak%20and%20Yiwen%20Song%20and%20Georgi%20Stephanov%20and%20Gary%20Wang%20and%20Marcello%20Maggioni%20and%20Stein%20Xudong%20Lin%20and%20Felix%20Wu%20and%20Shachi%20Paul%20and%20Zixuan%20Jiang%20and%20Shubham%20Agrawal%20and%20Bilal%20Piot%20and%20Alex%20Feng%20and%20Cheolmin%20Kim%20and%20Tulsee%20Doshi%20and%20Jonathan%20Lai%20and%20%20Chuqiao%20and%20%20Xu%20and%20Sharad%20Vikram%20and%20Ciprian%20Chelba%20and%20Sebastian%20Krause%20and%20Vincent%20Zhuang%20and%20Jack%20Rae%20and%20Timo%20Denk%20and%20Adrian%20Collister%20and%20Lotte%20Weerts%20and%20Xianghong%20Luo%20and%20Yifeng%20Lu%20and%20H%C3%A5vard%20Garnes%20and%20Nitish%20Gupta%20and%20Terry%20Spitz%20and%20Avinatan%20Hassidim%20and%20Lihao%20Liang%20and%20Izhak%20Shafran%20and%20Peter%20Humphreys%20and%20Kenny%20Vassigh%20and%20Phil%20Wallis%20and%20Virat%20Shejwalkar%20and%20Nicolas%20Perez-Nieves%20and%20Rachel%20Hornung%20and%20Melissa%20Tan%20and%20Beka%20Westberg%20and%20Andy%20Ly%20and%20Richard%20Zhang%20and%20Brian%20Farris%20and%20Jongbin%20Park%20and%20Alec%20Kosik%20and%20Zeynep%20Cankara%20and%20Andrii%20Maksai%20and%20Yunhan%20Xu%20and%20Albin%20Cassirer%20and%20Sergi%20Caelles%20and%20Abbas%20Abdolmaleki%20and%20Mencher%20Chiang%20and%20Alex%20Fabrikant%20and%20Shravya%20Shetty%20and%20Luheng%20He%20and%20Mai%20Gim%C3%A9nez%20and%20Hadi%20Hashemi%20and%20Sheena%20Panthaplackel%20and%20Yana%20Kulizhskaya%20and%20Salil%20Deshmukh%20and%20Daniele%20Pighin%20and%20Robin%20Alazard%20and%20Disha%20Jindal%20and%20Seb%20Noury%20and%20Pradeep%20Kumar%20S%20and%20Siyang%20Qin%20and%20Xerxes%20Dotiwalla%20and%20Stephen%20Spencer%20and%20Mohammad%20Babaeizadeh%20and%20Blake%20JianHang%20Chen%20and%20Vaibhav%20Mehta%20and%20Jennie%20Lees%20and%20Andrew%20Leach%20and%20Penporn%20Koanantakool%20and%20Ilia%20Akolzin%20and%20Ramona%20Comanescu%20and%20Junwhan%20Ahn%20and%20Alexey%20Svyatkovskiy%20and%20Basil%20Mustafa%20and%20David%20D%27Ambrosio%20and%20Shiva%20Mohan%20Reddy%20Garlapati%20and%20Pascal%20Lamblin%20and%20Alekh%20Agarwal%20and%20Shuang%20Song%20and%20Pier%20Giuseppe%20Sessa%20and%20Pauline%20Coquinot%20and%20John%20Maggs%20and%20Hussain%20Masoom%20and%20Divya%20Pitta%20and%20Yaqing%20Wang%20and%20Patrick%20Morris-Suzuki%20and%20Billy%20Porter%20and%20Johnson%20Jia%20and%20Jeffrey%20Dudek%20and%20Raghavender%20R%20and%20Cosmin%20Paduraru%20and%20Alan%20Ansell%20and%20Tolga%20Bolukbasi%20and%20Tony%20Lu%20and%20Ramya%20Ganeshan%20and%20Zi%20Wang%20and%20Henry%20Griffiths%20and%20Rodrigo%20Benenson%20and%20Yifan%20He%20and%20James%20Swirhun%20and%20George%20Papamakarios%20and%20Aditya%20Chawla%20and%20Kuntal%20Sengupta%20and%20Yan%20Wang%20and%20Vedrana%20Milutinovic%20and%20Igor%20Mordatch%20and%20Zhipeng%20Jia%20and%20Jamie%20Smith%20and%20Will%20Ng%20and%20Shitij%20Nigam%20and%20Matt%20Young%20and%20Eugen%20Vu%C5%A1ak%20and%20Blake%20Hechtman%20and%20Sheela%20Goenka%20and%20Avital%20Zipori%20and%20Kareem%20Ayoub%20and%20Ashok%20Popat%20and%20Trilok%20Acharya%20and%20Luo%20Yu%20and%20Dawn%20Bloxwich%20and%20Hugo%20Song%20and%20Paul%20Roit%20and%20Haiqiong%20Li%20and%20Aviel%20Boag%20and%20Nigamaa%20Nayakanti%20and%20Bilva%20Chandra%20and%20Tianli%20Ding%20and%20Aahil%20Mehta%20and%20Cath%20Hope%20and%20Jiageng%20Zhang%20and%20Idan%20Heimlich%20Shtacher%20and%20Kartikeya%20Badola%20and%20Ryo%20Nakashima%20and%20Andrei%20Sozanschi%20and%20Iulia%20Com%C5%9Fa%20and%20Ante%20%C5%BDu%C5%BEul%20and%20Emily%20Caveness%20and%20Julian%20Odell%20and%20Matthew%20Watson%20and%20Dario%20de%20Cesare%20and%20Phillip%20Lippe%20and%20Derek%20Lockhart%20and%20Siddharth%20Verma%20and%20Huizhong%20Chen%20and%20Sean%20Sun%20and%20Lin%20Zhuo%20and%20Aditya%20Shah%20and%20Prakhar%20Gupta%20and%20Alex%20Muzio%20and%20Ning%20Niu%20and%20Amir%20Zait%20and%20Abhinav%20Singh%20and%20Meenu%20Gaba%20and%20Fan%20Ye%20and%20Prajit%20Ramachandran%20and%20Mohammad%20Saleh%20and%20Raluca%20Ada%20Popa%20and%20Ayush%20Dubey%20and%20Frederick%20Liu%20and%20Sara%20Javanmardi%20and%20Mark%20Epstein%20and%20Ross%20Hemsley%20and%20Richard%20Green%20and%20Nishant%20Ranka%20and%20Eden%20Cohen%20and%20Chuyuan%20Kelly%20Fu%20and%20Sanjay%20Ghemawat%20and%20Jed%20Borovik%20and%20James%20Martens%20and%20Anthony%20Chen%20and%20Pranav%20Shyam%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Ming-Hsuan%20Yang%20and%20Alexandru%20%C5%A2ifrea%20and%20David%20Du%20and%20Boqing%20Gong%20and%20Ayushi%20Agarwal%20and%20Seungyeon%20Kim%20and%20Christian%20Frank%20and%20Saloni%20Shah%20and%20Xiaodan%20Song%20and%20Zhiwei%20Deng%20and%20Ales%20Mikhalap%20and%20Kleopatra%20Chatziprimou%20and%20Timothy%20Chung%20and%20Toni%20Creswell%20and%20Susan%20Zhang%20and%20Yennie%20Jun%20and%20Carl%20Lebsack%20and%20Will%20Truong%20and%20Slavica%20Anda%C4%8Di%C4%87%20and%20Itay%20Yona%20and%20Marco%20Fornoni%20and%20Rong%20Rong%20and%20Serge%20Toropov%20and%20Afzal%20Shama%20Soudagar%20and%20Andrew%20Audibert%20and%20Salah%20Zaiem%20and%20Zaheer%20Abbas%20and%20Andrei%20Rusu%20and%20Sahitya%20Potluri%20and%20Shitao%20Weng%20and%20Anastasios%20Kementsietsidis%20and%20Anton%20Tsitsulin%20and%20Daiyi%20Peng%20and%20Natalie%20Ha%20and%20Sanil%20Jain%20and%20Tejasi%20Latkar%20and%20Simeon%20Ivanov%20and%20Cory%20McLean%20and%20Anirudh%20GP%20and%20Rajesh%20Venkataraman%20and%20Canoee%20Liu%20and%20Dilip%20Krishnan%20and%20Joel%20D%27sa%20and%20Roey%20Yogev%20and%20Paul%20Collins%20and%20Benjamin%20Lee%20and%20Lewis%20Ho%20and%20Carl%20Doersch%20and%20Gal%20Yona%20and%20Shawn%20Gao%20and%20Felipe%20Tiengo%20Ferreira%20and%20Adnan%20Ozturel%20and%20Hannah%20Muckenhirn%20and%20Ce%20Zheng%20and%20Gargi%20Balasubramaniam%20and%20Mudit%20Bansal%20and%20George%20van%20den%20Driessche%20and%20Sivan%20Eiger%20and%20Salem%20Haykal%20and%20Vedant%20Misra%20and%20Abhimanyu%20Goyal%20and%20Danilo%20Martins%20and%20Gary%20Leung%20and%20Jonas%20Valfridsson%20and%20Four%20Flynn%20and%20Will%20Bishop%20and%20Chenxi%20Pang%20and%20Yoni%20Halpern%20and%20Honglin%20Yu%20and%20Lawrence%20Moore%20and%20%20Yuvein%20and%20%20Zhu%20and%20Sridhar%20Thiagarajan%20and%20Yoel%20Drori%20and%20Zhisheng%20Xiao%20and%20Lucio%20Dery%20and%20Rolf%20Jagerman%20and%20Jing%20Lu%20and%20Eric%20Ge%20and%20Vaibhav%20Aggarwal%20and%20Arjun%20Khare%20and%20Vinh%20Tran%20and%20Oded%20Elyada%20and%20Ferran%20Alet%20and%20James%20Rubin%20and%20Ian%20Chou%20and%20David%20Tian%20and%20Libin%20Bai%20and%20Lawrence%20Chan%20and%20Lukasz%20Lew%20and%20Karolis%20Misiunas%20and%20Taylan%20Bilal%20and%20Aniket%20Ray%20and%20Sindhu%20Raghuram%20and%20Alex%20Castro-Ros%20and%20Viral%20Carpenter%20and%20CJ%20Zheng%20and%20Michael%20Kilgore%20and%20Josef%20Broder%20and%20Emily%20Xue%20and%20Praveen%20Kallakuri%20and%20Dheeru%20Dua%20and%20Nancy%20Yuen%20and%20Steve%20Chien%20and%20John%20Schultz%20and%20Saurabh%20Agrawal%20and%20Reut%20Tsarfaty%20and%20Jingcao%20Hu%20and%20Ajay%20Kannan%20and%20Dror%20Marcus%20and%20Nisarg%20Kothari%20and%20Baochen%20Sun%20and%20Ben%20Horn%20and%20Matko%20Bo%C5%A1njak%20and%20Ferjad%20Naeem%20and%20Dean%20Hirsch%20and%20Lewis%20Chiang%20and%20Boya%20Fang%20and%20Jie%20Han%20and%20Qifei%20Wang%20and%20Ben%20Hora%20and%20Antoine%20He%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Beer%20Changpinyo%20and%20Anshuman%20Tripathi%20and%20John%20Youssef%20and%20Chester%20Kwak%20and%20Philippe%20Schlattner%20and%20Cat%20Graves%20and%20R%C3%A9mi%20Leblond%20and%20Wenjun%20Zeng%20and%20Anders%20Andreassen%20and%20Gabriel%20Rasskin%20and%20Yue%20Song%20and%20Eddie%20Cao%20and%20Junhyuk%20Oh%20and%20Matt%20Hoffman%20and%20Wojtek%20Skut%20and%20Yichi%20Zhang%20and%20Jon%20Stritar%20and%20Xingyu%20Cai%20and%20Saarthak%20Khanna%20and%20Kathie%20Wang%20and%20Shriya%20Sharma%20and%20Christian%20Reisswig%20and%20Younghoon%20Jun%20and%20Aman%20Prasad%20and%20Tatiana%20Sholokhova%20and%20Preeti%20Singh%20and%20Adi%20Gerzi%20Rosenthal%20and%20Anian%20Ruoss%20and%20Fran%C3%A7oise%20Beaufays%20and%20Sean%20Kirmani%20and%20Dongkai%20Chen%20and%20Johan%20Schalkwyk%20and%20Jonathan%20Herzig%20and%20Been%20Kim%20and%20Josh%20Jacob%20and%20Damien%20Vincent%20and%20Adrian%20N%20Reyes%20and%20Ivana%20Balazevic%20and%20L%C3%A9onard%20Hussenot%20and%20Jon%20Schneider%20and%20Parker%20Barnes%20and%20Luis%20Castro%20and%20Spandana%20Raj%20Babbula%20and%20Simon%20Green%20and%20Serkan%20Cabi%20and%20Nico%20Duduta%20and%20Danny%20Driess%20and%20Rich%20Galt%20and%20Noam%20Velan%20and%20Junjie%20Wang%20and%20Hongyang%20Jiao%20and%20Matthew%20Mauger%20and%20Du%20Phan%20and%20Miteyan%20Patel%20and%20Vlado%20Gali%C4%87%20and%20Jerry%20Chang%20and%20Eyal%20Marcus%20and%20Matt%20Harvey%20and%20Julian%20Salazar%20and%20Elahe%20Dabir%20and%20Suraj%20Satishkumar%20Sheth%20and%20Amol%20Mandhane%20and%20Hanie%20Sedghi%20and%20Jeremiah%20Willcock%20and%20Amir%20Zandieh%20and%20Shruthi%20Prabhakara%20and%20Aida%20Amini%20and%20Antoine%20Miech%20and%20Victor%20Stone%20and%20Massimo%20Nicosia%20and%20Paul%20Niemczyk%20and%20Ying%20Xiao%20and%20Lucy%20Kim%20and%20S%C5%82awek%20Kwasiborski%20and%20Vikas%20Verma%20and%20Ada%20Maksutaj%20Oflazer%20and%20Christoph%20Hirnschall%20and%20Peter%20Sung%20and%20Lu%20Liu%20and%20Richard%20Everett%20and%20Michiel%20Bakker%20and%20%C3%81goston%20Weisz%20and%20Yufei%20Wang%20and%20Vivek%20Sampathkumar%20and%20Uri%20Shaham%20and%20Bibo%20Xu%20and%20Yasemin%20Altun%20and%20Mingqiu%20Wang%20and%20Takaaki%20Saeki%20and%20Guanjie%20Chen%20and%20Emanuel%20Taropa%20and%20Shanthal%20Vasanth%20and%20Sophia%20Austin%20and%20Lu%20Huang%20and%20Goran%20Petrovic%20and%20Qingyun%20Dou%20and%20Daniel%20Golovin%20and%20Grigory%20Rozhdestvenskiy%20and%20Allie%20Culp%20and%20Will%20Wu%20and%20Motoki%20Sano%20and%20Divya%20Jain%20and%20Julia%20Proskurnia%20and%20S%C3%A9bastien%20Cevey%20and%20Alejandro%20Cruzado%20Ruiz%20and%20Piyush%20Patil%20and%20Mahdi%20Mirzazadeh%20and%20Eric%20Ni%20and%20Javier%20Snaider%20and%20Lijie%20Fan%20and%20Alexandre%20Fr%C3%A9chette%20and%20AJ%20Pierigiovanni%20and%20Shariq%20Iqbal%20and%20Kenton%20Lee%20and%20Claudio%20Fantacci%20and%20Jinwei%20Xing%20and%20Lisa%20Wang%20and%20Alex%20Irpan%20and%20David%20Raposo%20and%20Yi%20Luan%20and%20Zhuoyuan%20Chen%20and%20Harish%20Ganapathy%20and%20Kevin%20Hui%20and%20Jiazhong%20Nie%20and%20Isabelle%20Guyon%20and%20Heming%20Ge%20and%20Roopali%20Vij%20and%20Hui%20Zheng%20and%20Dayeong%20Lee%20and%20Alfonso%20Casta%C3%B1o%20and%20Khuslen%20Baatarsukh%20and%20Gabriel%20Ibagon%20and%20Alexandra%20Chronopoulou%20and%20Nicholas%20FitzGerald%20and%20Shashank%20Viswanadha%20and%20Safeen%20Huda%20and%20Rivka%20Moroshko%20and%20Georgi%20Stoyanov%20and%20Prateek%20Kolhar%20and%20Alain%20Vaucher%20and%20Ishaan%20Watts%20and%20Adhi%20Kuncoro%20and%20Henryk%20Michalewski%20and%20Satish%20Kambala%20and%20Bat-Orgil%20Batsaikhan%20and%20Alek%20Andreev%20and%20Irina%20Jurenka%20and%20Maigo%20Le%20and%20Qihang%20Chen%20and%20Wael%20Al%20Jishi%20and%20Sarah%20Chakera%20and%20Zhe%20Chen%20and%20Aditya%20Kini%20and%20Vikas%20Yadav%20and%20Aditya%20Siddhant%20and%20Ilia%20Labzovsky%20and%20Balaji%20Lakshminarayanan%20and%20Carrie%20Grimes%20Bostock%20and%20Pankil%20Botadra%20and%20Ankesh%20Anand%20and%20Colton%20Bishop%20and%20Sam%20Conway-Rahman%20and%20Mohit%20Agarwal%20and%20Yani%20Donchev%20and%20Achintya%20Singhal%20and%20F%C3%A9lix%20de%20Chaumont%20Quitry%20and%20Natalia%20Ponomareva%20and%20Nishant%20Agrawal%20and%20Bin%20Ni%20and%20Kalpesh%20Krishna%20and%20Masha%20Samsikova%20and%20John%20Karro%20and%20Yilun%20Du%20and%20Tamara%20von%20Glehn%20and%20Caden%20Lu%20and%20Christopher%20A.%20Choquette-Choo%20and%20Zhen%20Qin%20and%20Tingnan%20Zhang%20and%20Sicheng%20Li%20and%20Divya%20Tyam%20and%20Swaroop%20Mishra%20and%20Wing%20Lowe%20and%20Colin%20Ji%20and%20Weiyi%20Wang%20and%20Manaal%20Faruqui%20and%20Ambrose%20Slone%20and%20Valentin%20Dalibard%20and%20Arunachalam%20Narayanaswamy%20and%20John%20Lambert%20and%20Pierre-Antoine%20Manzagol%20and%20Dan%20Karliner%20and%20Andrew%20Bolt%20and%20Ivan%20Lobov%20and%20Aditya%20Kusupati%20and%20Chang%20Ye%20and%20Xuan%20Yang%20and%20Heiga%20Zen%20and%20Nelson%20George%20and%20Mukul%20Bhutani%20and%20Olivier%20Lacombe%20and%20Robert%20Riachi%20and%20Gagan%20Bansal%20and%20Rachel%20Soh%20and%20Yue%20Gao%20and%20Yang%20Yu%20and%20Adams%20Yu%20and%20Emily%20Nottage%20and%20Tania%20Rojas-Esponda%20and%20James%20Noraky%20and%20Manish%20Gupta%20and%20Ragha%20Kotikalapudi%20and%20Jichuan%20Chang%20and%20Sanja%20Deur%20and%20Dan%20Graur%20and%20Alex%20Mossin%20and%20Erin%20Farnese%20and%20Ricardo%20Figueira%20and%20Alexandre%20Moufarek%20and%20Austin%20Huang%20and%20Patrik%20Zochbauer%20and%20Ben%20Ingram%20and%20Tongzhou%20Chen%20and%20Zelin%20Wu%20and%20Adri%C3%A0%20Puigdom%C3%A8nech%20and%20Leland%20Rechis%20and%20Da%20Yu%20and%20Sri%20Gayatri%20Sundara%20Padmanabhan%20and%20Rui%20Zhu%20and%20Chu-ling%20Ko%20and%20Andrea%20Banino%20and%20Samira%20Daruki%20and%20Aarush%20Selvan%20and%20Dhruva%20Bhaswar%20and%20Daniel%20Hernandez%20Diaz%20and%20Chen%20Su%20and%20Salvatore%20Scellato%20and%20Jennifer%20Brennan%20and%20Woohyun%20Han%20and%20Grace%20Chung%20and%20Priyanka%20Agrawal%20and%20Urvashi%20Khandelwal%20and%20Khe%20Chai%20Sim%20and%20Morgane%20Lustman%20and%20Sam%20Ritter%20and%20Kelvin%20Guu%20and%20Jiawei%20Xia%20and%20Prateek%20Jain%20and%20Emma%20Wang%20and%20Tyrone%20Hill%20and%20Mirko%20Rossini%20and%20Marija%20Kostelac%20and%20Tautvydas%20Misiunas%20and%20Amit%20Sabne%20and%20Kyuyeun%20Kim%20and%20Ahmet%20Iscen%20and%20Congchao%20Wang%20and%20Jos%C3%A9%20Leal%20and%20Ashwin%20Sreevatsa%20and%20Utku%20Evci%20and%20Manfred%20Warmuth%20and%20Saket%20Joshi%20and%20Daniel%20Suo%20and%20James%20Lottes%20and%20Garrett%20Honke%20and%20Brendan%20Jou%20and%20Stefani%20Karp%20and%20Jieru%20Hu%20and%20Himanshu%20Sahni%20and%20Adrien%20Ali%20Ta%C3%AFga%20and%20William%20Kong%20and%20Samrat%20Ghosh%20and%20Renshen%20Wang%20and%20Jay%20Pavagadhi%20and%20Natalie%20Axelsson%20and%20Nikolai%20Grigorev%20and%20Patrick%20Siegler%20and%20Rebecca%20Lin%20and%20Guohui%20Wang%20and%20Emilio%20Parisotto%20and%20Sharath%20Maddineni%20and%20Krishan%20Subudhi%20and%20Eyal%20Ben-David%20and%20Elena%20Pochernina%20and%20Orgad%20Keller%20and%20Thi%20Avrahami%20and%20Zhe%20Yuan%20and%20Pulkit%20Mehta%20and%20Jialu%20Liu%20and%20Sherry%20Yang%20and%20Wendy%20Kan%20and%20Katherine%20Lee%20and%20Tom%20Funkhouser%20and%20Derek%20Cheng%20and%20Hongzhi%20Shi%20and%20Archit%20Sharma%20and%20Joe%20Kelley%20and%20Matan%20Eyal%20and%20Yury%20Malkov%20and%20Corentin%20Tallec%20and%20Yuval%20Bahat%20and%20Shen%20Yan%20and%20%20Xintian%20and%20%20Wu%20and%20David%20Lindner%20and%20Chengda%20Wu%20and%20Avi%20Caciularu%20and%20Xiyang%20Luo%20and%20Rodolphe%20Jenatton%20and%20Tim%20Zaman%20and%20Yingying%20Bi%20and%20Ilya%20Kornakov%20and%20Ganesh%20Mallya%20and%20Daisuke%20Ikeda%20and%20Itay%20Karo%20and%20Anima%20Singh%20and%20Colin%20Evans%20and%20Praneeth%20Netrapalli%20and%20Vincent%20Nallatamby%20and%20Isaac%20Tian%20and%20Yannis%20Assael%20and%20Vikas%20Raunak%20and%20Victor%20Carbune%20and%20Ioana%20Bica%20and%20Lior%20Madmoni%20and%20Dee%20Cattle%20and%20Snchit%20Grover%20and%20Krishna%20Somandepalli%20and%20Sid%20Lall%20and%20Amelio%20V%C3%A1zquez-Reina%20and%20Riccardo%20Patana%20and%20Jiaqi%20Mu%20and%20Pranav%20Talluri%20and%20Maggie%20Tran%20and%20Rajeev%20Aggarwal%20and%20RJ%20Skerry-Ryan%20and%20Jun%20Xu%20and%20Mike%20Burrows%20and%20Xiaoyue%20Pan%20and%20Edouard%20Yvinec%20and%20Di%20Lu%20and%20Zhiying%20Zhang%20and%20Duc%20Dung%20Nguyen%20and%20Hairong%20Mu%20and%20Gabriel%20Barcik%20and%20Helen%20Ran%20and%20Lauren%20Beltrone%20and%20Krzysztof%20Choromanski%20and%20Dia%20Kharrat%20and%20Samuel%20Albanie%20and%20Sean%20Purser-haskell%20and%20David%20Bieber%20and%20Carrie%20Zhang%20and%20Jing%20Wang%20and%20Tom%20Hudson%20and%20Zhiyuan%20Zhang%20and%20Han%20Fu%20and%20Johannes%20Mauerer%20and%20Mohammad%20Hossein%20Bateni%20and%20AJ%20Maschinot%20and%20Bing%20Wang%20and%20Muye%20Zhu%20and%20Arjun%20Pillai%20and%20Tobias%20Weyand%20and%20Shuang%20Liu%20and%20Oscar%20Akerlund%20and%20Fred%20Bertsch%20and%20Vittal%20Premachandran%20and%20Alicia%20Jin%20and%20Vincent%20Roulet%20and%20Peter%20de%20Boursac%20and%20Shubham%20Mittal%20and%20Ndaba%20Ndebele%20and%20Georgi%20Karadzhov%20and%20Sahra%20Ghalebikesabi%20and%20Ricky%20Liang%20and%20Allen%20Wu%20and%20Yale%20Cong%20and%20Nimesh%20Ghelani%20and%20Sumeet%20Singh%20and%20Bahar%20Fatemi%20and%20%20Warren%20and%20%20Chen%20and%20Charles%20Kwong%20and%20Alexey%20Kolganov%20and%20Steve%20Li%20and%20Richard%20Song%20and%20Chenkai%20Kuang%20and%20Sobhan%20Miryoosefi%20and%20Dale%20Webster%20and%20James%20Wendt%20and%20Arkadiusz%20Socala%20and%20Guolong%20Su%20and%20Artur%20Mendon%C3%A7a%20and%20Abhinav%20Gupta%20and%20Xiaowei%20Li%20and%20Tomy%20Tsai%20and%20%20Qiong%20and%20%20Hu%20and%20Kai%20Kang%20and%20Angie%20Chen%20and%20Sertan%20Girgin%20and%20Yongqin%20Xian%20and%20Andrew%20Lee%20and%20Nolan%20Ramsden%20and%20Leslie%20Baker%20and%20Madeleine%20Clare%20Elish%20and%20Varvara%20Krayvanova%20and%20Rishabh%20Joshi%20and%20Jiri%20Simsa%20and%20Yao-Yuan%20Yang%20and%20Piotr%20Ambroszczyk%20and%20Dipankar%20Ghosh%20and%20Arjun%20Kar%20and%20Yuan%20Shangguan%20and%20Yumeya%20Yamamori%20and%20Yaroslav%20Akulov%20and%20Andy%20Brock%20and%20Haotian%20Tang%20and%20Siddharth%20Vashishtha%20and%20Rich%20Munoz%20and%20Andreas%20Steiner%20and%20Kalyan%20Andra%20and%20Daniel%20Eppens%20and%20Qixuan%20Feng%20and%20Hayato%20Kobayashi%20and%20Sasha%20Goldshtein%20and%20Mona%20El%20Mahdy%20and%20Xin%20Wang%20and%20%20Jilei%20and%20%20Wang%20and%20Richard%20Killam%20and%20Tom%20Kwiatkowski%20and%20Kavya%20Kopparapu%20and%20Serena%20Zhan%20and%20Chao%20Jia%20and%20Alexei%20Bendebury%20and%20Sheryl%20Luo%20and%20Adri%C3%A0%20Recasens%20and%20Timothy%20Knight%20and%20Jing%20Chen%20and%20Mohak%20Patel%20and%20YaGuang%20Li%20and%20Ben%20Withbroe%20and%20Dean%20Weesner%20and%20Kush%20Bhatia%20and%20Jie%20Ren%20and%20Danielle%20Eisenbud%20and%20Ebrahim%20Songhori%20and%20Yanhua%20Sun%20and%20Travis%20Choma%20and%20Tasos%20Kementsietsidis%20and%20Lucas%20Manning%20and%20Brian%20Roark%20and%20Wael%20Farhan%20and%20Jie%20Feng%20and%20Susheel%20Tatineni%20and%20James%20Cobon-Kerr%20and%20Yunjie%20Li%20and%20Lisa%20Anne%20Hendricks%20and%20Isaac%20Noble%20and%20Chris%20Breaux%20and%20Nate%20Kushman%20and%20Liqian%20Peng%20and%20Fuzhao%20Xue%20and%20Taylor%20Tobin%20and%20Jamie%20Rogers%20and%20Josh%20Lipschultz%20and%20Chris%20Alberti%20and%20Alexey%20Vlaskin%20and%20Mostafa%20Dehghani%20and%20Roshan%20Sharma%20and%20Tris%20Warkentin%20and%20Chen-Yu%20Lee%20and%20Benigno%20Uria%20and%20Da-Cheng%20Juan%20and%20Angad%20Chandorkar%20and%20Hila%20Sheftel%20and%20Ruibo%20Liu%20and%20Elnaz%20Davoodi%20and%20Borja%20De%20Balle%20Pigem%20and%20Kedar%20Dhamdhere%20and%20David%20Ross%20and%20Jonathan%20Hoech%20and%20Mahdis%20Mahdieh%20and%20Li%20Liu%20and%20Qiujia%20Li%20and%20Liam%20McCafferty%20and%20Chenxi%20Liu%20and%20Markus%20Mircea%20and%20Yunting%20Song%20and%20Omkar%20Savant%20and%20Alaa%20Saade%20and%20Colin%20Cherry%20and%20Vincent%20Hellendoorn%20and%20Siddharth%20Goyal%20and%20Paul%20Pucciarelli%20and%20David%20Vilar%20Torres%20and%20Zohar%20Yahav%20and%20Hyo%20Lee%20and%20Lars%20Lowe%20Sjoesund%20and%20Christo%20Kirov%20and%20Bo%20Chang%20and%20Deepanway%20Ghoshal%20and%20Lu%20Li%20and%20Gilles%20Baechler%20and%20S%C3%A9bastien%20Pereira%20and%20Tara%20Sainath%20and%20Anudhyan%20Boral%20and%20Dominik%20Grewe%20and%20Afief%20Halumi%20and%20Nguyet%20Minh%20Phu%20and%20Tianxiao%20Shen%20and%20Marco%20Tulio%20Ribeiro%20and%20Dhriti%20Varma%20and%20Alex%20Kaskasoli%20and%20Vlad%20Feinberg%20and%20Navneet%20Potti%20and%20Jarrod%20Kahn%20and%20Matheus%20Wisniewski%20and%20Shakir%20Mohamed%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Bobak%20Shahriari%20and%20Jean-Baptiste%20Lespiau%20and%20Lisa%20Patel%20and%20Legg%20Yeung%20and%20Tom%20Paine%20and%20Lantao%20Mei%20and%20Alex%20Ramirez%20and%20Rakesh%20Shivanna%20and%20Li%20Zhong%20and%20Josh%20Woodward%20and%20Guilherme%20Tubone%20and%20Samira%20Khan%20and%20Heng%20Chen%20and%20Elizabeth%20Nielsen%20and%20Catalin%20Ionescu%20and%20Utsav%20Prabhu%20and%20Mingcen%20Gao%20and%20Qingze%20Wang%20and%20Sean%20Augenstein%20and%20Neesha%20Subramaniam%20and%20Jason%20Chang%20and%20Fotis%20Iliopoulos%20and%20Jiaming%20Luo%20and%20Myriam%20Khan%20and%20Weicheng%20Kuo%20and%20Denis%20Teplyashin%20and%20Florence%20Perot%20and%20Logan%20Kilpatrick%20and%20Amir%20Globerson%20and%20Hongkun%20Yu%20and%20Anfal%20Siddiqui%20and%20Nick%20Sukhanov%20and%20Arun%20Kandoor%20and%20Umang%20Gupta%20and%20Marco%20Andreetto%20and%20Moran%20Ambar%20and%20Donnie%20Kim%20and%20Pawe%C5%82%20Weso%C5%82owski%20and%20Sarah%20Perrin%20and%20Ben%20Limonchik%20and%20Wei%20Fan%20and%20Jim%20Stephan%20and%20Ian%20Stewart-Binks%20and%20Ryan%20Kappedal%20and%20Tong%20He%20and%20Sarah%20Cogan%20and%20Romina%20Datta%20and%20Tong%20Zhou%20and%20Jiayu%20Ye%20and%20Leandro%20Kieliger%20and%20Ana%20Ramalho%20and%20Kyle%20Kastner%20and%20Fabian%20Mentzer%20and%20Wei-Jen%20Ko%20and%20Arun%20Suggala%20and%20Tianhao%20Zhou%20and%20Shiraz%20Butt%20and%20Hana%20Strej%C4%8Dek%20and%20Lior%20Belenki%20and%20Subhashini%20Venugopalan%20and%20Mingyang%20Ling%20and%20Evgenii%20Eltyshev%20and%20Yunxiao%20Deng%20and%20Geza%20Kovacs%20and%20Mukund%20Raghavachari%20and%20Hanjun%20Dai%20and%20Tal%20Schuster%20and%20Steven%20Schwarcz%20and%20Richard%20Nguyen%20and%20Arthur%20Nguyen%20and%20Gavin%20Buttimore%20and%20Shrestha%20Basu%20Mallick%20and%20Sudeep%20Gandhe%20and%20Seth%20Benjamin%20and%20Michal%20Jastrzebski%20and%20Le%20Yan%20and%20Sugato%20Basu%20and%20Chris%20Apps%20and%20Isabel%20Edkins%20and%20James%20Allingham%20and%20Immanuel%20Odisho%20and%20Tomas%20Kocisky%20and%20Jewel%20Zhao%20and%20Linting%20Xue%20and%20Apoorv%20Reddy%20and%20Chrysovalantis%20Anastasiou%20and%20Aviel%20Atias%20and%20Sam%20Redmond%20and%20Kieran%20Milan%20and%20Nicolas%20Heess%20and%20Herman%20Schmit%20and%20Allan%20Dafoe%20and%20Daniel%20Andor%20and%20Tynan%20Gangwani%20and%20Anca%20Dragan%20and%20Sheng%20Zhang%20and%20Ashyana%20Kachra%20and%20Gang%20Wu%20and%20Siyang%20Xue%20and%20Kevin%20Aydin%20and%20Siqi%20Liu%20and%20Yuxiang%20Zhou%20and%20Mahan%20Malihi%20and%20Austin%20Wu%20and%20Siddharth%20Gopal%20and%20Candice%20Schumann%20and%20Peter%20Stys%20and%20Alek%20Wang%20and%20Mirek%20Ol%C5%A1%C3%A1k%20and%20Dangyi%20Liu%20and%20Christian%20Schallhart%20and%20Yiran%20Mao%20and%20Demetra%20Brady%20and%20Hao%20Xu%20and%20Tomas%20Mery%20and%20Chawin%20Sitawarin%20and%20Siva%20Velusamy%20and%20Tom%20Cobley%20and%20Alex%20Zhai%20and%20Christian%20Walder%20and%20Nitzan%20Katz%20and%20Ganesh%20Jawahar%20and%20Chinmay%20Kulkarni%20and%20Antoine%20Yang%20and%20Adam%20Paszke%20and%20Yinan%20Wang%20and%20Bogdan%20Damoc%20and%20Zal%C3%A1n%20Borsos%20and%20Ray%20Smith%20and%20Jinning%20Li%20and%20Mansi%20Gupta%20and%20Andrei%20Kapishnikov%20and%20Sushant%20Prakash%20and%20Florian%20Luisier%20and%20Rishabh%20Agarwal%20and%20Will%20Grathwohl%20and%20Kuangyuan%20Chen%20and%20Kehang%20Han%20and%20Nikhil%20Mehta%20and%20Andrew%20Over%20and%20Shekoofeh%20Azizi%20and%20Lei%20Meng%20and%20Niccol%C3%B2%20Dal%20Santo%20and%20Kelvin%20Zheng%20and%20Jane%20Shapiro%20and%20Igor%20Petrovski%20and%20Jeffrey%20Hui%20and%20Amin%20Ghafouri%20and%20Jasper%20Snoek%20and%20James%20Qin%20and%20Mandy%20Jordan%20and%20Caitlin%20Sikora%20and%20Jonathan%20Malmaud%20and%20Yuheng%20Kuang%20and%20Aga%20%C5%9Awietlik%20and%20Ruoxin%20Sang%20and%20Chongyang%20Shi%20and%20Leon%20Li%20and%20Andrew%20Rosenberg%20and%20Shubin%20Zhao%20and%20Andy%20Crawford%20and%20Jan-Thorsten%20Peter%20and%20Yun%20Lei%20and%20Xavier%20Garcia%20and%20Long%20Le%20and%20Todd%20Wang%20and%20Julien%20Amelot%20and%20Dave%20Orr%20and%20Praneeth%20Kacham%20and%20Dana%20Alon%20and%20Gladys%20Tyen%20and%20Abhinav%20Arora%20and%20James%20Lyon%20and%20Alex%20Kurakin%20and%20Mimi%20Ly%20and%20Theo%20Guidroz%20and%20Zhipeng%20Yan%20and%20Rina%20Panigrahy%20and%20Pingmei%20Xu%20and%20Thais%20Kagohara%20and%20Yong%20Cheng%20and%20Eric%20Noland%20and%20Jinhyuk%20Lee%20and%20Jonathan%20Lee%20and%20Cathy%20Yip%20and%20Maria%20Wang%20and%20Efrat%20Nehoran%20and%20Alexander%20Bykovsky%20and%20Zhihao%20Shan%20and%20Ankit%20Bhagatwala%20and%20Chaochao%20Yan%20and%20Jie%20Tan%20and%20Guillermo%20Garrido%20and%20Dan%20Ethier%20and%20Nate%20Hurley%20and%20Grace%20Vesom%20and%20Xu%20Chen%20and%20Siyuan%20Qiao%20and%20Abhishek%20Nayyar%20and%20Julian%20Walker%20and%20Paramjit%20Sandhu%20and%20Mihaela%20Rosca%20and%20Danny%20Swisher%20and%20Mikhail%20Dektiarev%20and%20Josh%20Dillon%20and%20George-Cristian%20Muraru%20and%20Manuel%20Tragut%20and%20Artiom%20Myaskovsky%20and%20David%20Reid%20and%20Marko%20Velic%20and%20Owen%20Xiao%20and%20Jasmine%20George%20and%20Mark%20Brand%20and%20Jing%20Li%20and%20Wenhao%20Yu%20and%20Shane%20Gu%20and%20Xiang%20Deng%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Soheil%20Hassas%20Yeganeh%20and%20Fred%20Alcober%20and%20Celine%20Smith%20and%20Trevor%20Cohn%20and%20Kay%20McKinney%20and%20Michael%20Tschannen%20and%20Ramesh%20Sampath%20and%20Gowoon%20Cheon%20and%20Liangchen%20Luo%20and%20Luyang%20Liu%20and%20Jordi%20Orbay%20and%20Hui%20Peng%20and%20Gabriela%20Botea%20and%20Xiaofan%20Zhang%20and%20Charles%20Yoon%20and%20Cesar%20Magalhaes%20and%20Pawe%C5%82%20Stradomski%20and%20Ian%20Mackinnon%20and%20Steven%20Hemingray%20and%20Kumaran%20Venkatesan%20and%20Rhys%20May%20and%20Jaeyoun%20Kim%20and%20Alex%20Druinsky%20and%20Jingchen%20Ye%20and%20Zheng%20Xu%20and%20Terry%20Huang%20and%20Jad%20Al%20Abdallah%20and%20Adil%20Dostmohamed%20and%20Rachana%20Fellinger%20and%20Tsendsuren%20Munkhdalai%20and%20Akanksha%20Maurya%20and%20Peter%20Garst%20and%20Yin%20Zhang%20and%20Maxim%20Krikun%20and%20Simon%20Bucher%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Yaxin%20Liu%20and%20Sheng%20Li%20and%20Nishesh%20Gupta%20and%20Jakub%20Adamek%20and%20Hanwen%20Chen%20and%20Bernett%20Orlando%20and%20Aleksandr%20Zaks%20and%20Joost%20van%20Amersfoort%20and%20Josh%20Camp%20and%20Hui%20Wan%20and%20HyunJeong%20Choe%20and%20Zhichun%20Wu%20and%20Kate%20Olszewska%20and%20Weiren%20Yu%20and%20Archita%20Vadali%20and%20Martin%20Scholz%20and%20Daniel%20De%20Freitas%20and%20Jason%20Lin%20and%20Amy%20Hua%20and%20Xin%20Liu%20and%20Frank%20Ding%20and%20Yichao%20Zhou%20and%20Boone%20Severson%20and%20Katerina%20Tsihlas%20and%20Samuel%20Yang%20and%20Tammo%20Spalink%20and%20Varun%20Yerram%20and%20Helena%20Pankov%20and%20Rory%20Blevins%20and%20Ben%20Vargas%20and%20Sarthak%20Jauhari%20and%20Matt%20Miecnikowski%20and%20Ming%20Zhang%20and%20Sandeep%20Kumar%20and%20Clement%20Farabet%20and%20Charline%20Le%20Lan%20and%20Sebastian%20Flennerhag%20and%20Yonatan%20Bitton%20and%20Ada%20Ma%20and%20Arthur%20Bra%C5%BEinskas%20and%20Eli%20Collins%20and%20Niharika%20Ahuja%20and%20Sneha%20Kudugunta%20and%20Anna%20Bortsova%20and%20Minh%20Giang%20and%20Wanzheng%20Zhu%20and%20Ed%20Chi%20and%20Scott%20Lundberg%20and%20Alexey%20Stern%20and%20Subha%20Puttagunta%20and%20Jing%20Xiong%20and%20Xiao%20Wu%20and%20Yash%20Pande%20and%20Amit%20Jhindal%20and%20Daniel%20Murphy%20and%20Jon%20Clark%20and%20Marc%20Brockschmidt%20and%20Maxine%20Deines%20and%20Kevin%20R.%20McKee%20and%20Dan%20Bahir%20and%20Jiajun%20Shen%20and%20Minh%20Truong%20and%20Daniel%20McDuff%20and%20Andrea%20Gesmundo%20and%20Edouard%20Rosseel%20and%20Bowen%20Liang%20and%20Ken%20Caluwaerts%20and%20Jessica%20Hamrick%20and%20Joseph%20Kready%20and%20Mary%20Cassin%20and%20Rishikesh%20Ingale%20and%20Li%20Lao%20and%20Scott%20Pollom%20and%20Yifan%20Ding%20and%20Wei%20He%20and%20Lizzetth%20Bellot%20and%20Joana%20Iljazi%20and%20Ramya%20Sree%20Boppana%20and%20Shan%20Han%20and%20Tara%20Thompson%20and%20Amr%20Khalifa%20and%20Anna%20Bulanova%20and%20Blagoj%20Mitrevski%20and%20Bo%20Pang%20and%20Emma%20Cooney%20and%20Tian%20Shi%20and%20Rey%20Coaguila%20and%20Tamar%20Yakar%20and%20Marc%27aurelio%20Ranzato%20and%20Nikola%20Momchev%20and%20Chris%20Rawles%20and%20Zachary%20Charles%20and%20Young%20Maeng%20and%20Yuan%20Zhang%20and%20Rishabh%20Bansal%20and%20Xiaokai%20Zhao%20and%20Brian%20Albert%20and%20Yuan%20Yuan%20and%20Sudheendra%20Vijayanarasimhan%20and%20Roy%20Hirsch%20and%20Vinay%20Ramasesh%20and%20Kiran%20Vodrahalli%20and%20Xingyu%20Wang%20and%20Arushi%20Gupta%20and%20DJ%20Strouse%20and%20Jianmo%20Ni%20and%20Roma%20Patel%20and%20Gabe%20Taubman%20and%20Zhouyuan%20Huo%20and%20Dero%20Gharibian%20and%20Marianne%20Monteiro%20and%20Hoi%20Lam%20and%20Shobha%20Vasudevan%20and%20Aditi%20Chaudhary%20and%20Isabela%20Albuquerque%20and%20Kilol%20Gupta%20and%20Sebastian%20Riedel%20and%20Chaitra%20Hegde%20and%20Avraham%20Ruderman%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Marcus%20Wainwright%20and%20Ashwin%20Chaugule%20and%20Burcu%20Karagol%20Ayan%20and%20Tomer%20Levinboim%20and%20Sam%20Shleifer%20and%20Yogesh%20Kalley%20and%20Vahab%20Mirrokni%20and%20Abhishek%20Rao%20and%20Prabakar%20Radhakrishnan%20and%20Jay%20Hartford%20and%20Jialin%20Wu%20and%20Zhenhai%20Zhu%20and%20Francesco%20Bertolini%20and%20Hao%20Xiong%20and%20Nicolas%20Serrano%20and%20Hamish%20Tomlinson%20and%20Myle%20Ott%20and%20Yifan%20Chang%20and%20Mark%20Graham%20and%20Jian%20Li%20and%20Marco%20Liang%20and%20Xiangzhu%20Long%20and%20Sebastian%20Borgeaud%20and%20Yanif%20Ahmad%20and%20Alex%20Grills%20and%20Diana%20Mincu%20and%20Martin%20Izzard%20and%20Yuan%20Liu%20and%20Jinyu%20Xie%20and%20Louis%20O%27Bryan%20and%20Sameera%20Ponda%20and%20Simon%20Tong%20and%20Michelle%20Liu%20and%20Dan%20Malkin%20and%20Khalid%20Salama%20and%20Yuankai%20Chen%20and%20Rohan%20Anil%20and%20Anand%20Rao%20and%20Rigel%20Swavely%20and%20Misha%20Bilenko%20and%20Nina%20Anderson%20and%20Tat%20Tan%20and%20Jing%20Xie%20and%20Xing%20Wu%20and%20Lijun%20Yu%20and%20Oriol%20Vinyals%20and%20Andrey%20Ryabtsev%20and%20Rumen%20Dangovski%20and%20Kate%20Baumli%20and%20Daniel%20Keysers%20and%20Christian%20Wright%20and%20Zoe%20Ashwood%20and%20Betty%20Chan%20and%20Artem%20Shtefan%20and%20Yaohui%20Guo%20and%20Ankur%20Bapna%20and%20Radu%20Soricut%20and%20Steven%20Pecht%20and%20Sabela%20Ramos%20and%20Rui%20Wang%20and%20Jiahao%20Cai%20and%20Trieu%20Trinh%20and%20Paul%20Barham%20and%20Linda%20Friso%20and%20Eli%20Stickgold%20and%20Xiangzhuo%20Ding%20and%20Siamak%20Shakeri%20and%20Diego%20Ardila%20and%20Eleftheria%20Briakou%20and%20Phil%20Culliton%20and%20Adam%20Raveret%20and%20Jingyu%20Cui%20and%20David%20Saxton%20and%20Subhrajit%20Roy%20and%20Javad%20Azizi%20and%20Pengcheng%20Yin%20and%20Lucia%20Loher%20and%20Andrew%20Bunner%20and%20Min%20Choi%20and%20Faruk%20Ahmed%20and%20Eric%20Li%20and%20Yin%20Li%20and%20Shengyang%20Dai%20and%20Michael%20Elabd%20and%20Sriram%20Ganapathy%20and%20Shivani%20Agrawal%20and%20Yiqing%20Hua%20and%20Paige%20Kunkle%20and%20Sujeevan%20Rajayogam%20and%20Arun%20Ahuja%20and%20Arthur%20Conmy%20and%20Alex%20Vasiloff%20and%20Parker%20Beak%20and%20Christopher%20Yew%20and%20Jayaram%20Mudigonda%20and%20Bartek%20Wydrowski%20and%20Jon%20Blanton%20and%20Zhengdong%20Wang%20and%20Yann%20Dauphin%20and%20Zhuo%20Xu%20and%20Martin%20Polacek%20and%20Xi%20Chen%20and%20Hexiang%20Hu%20and%20Pauline%20Sho%20and%20Markus%20Kunesch%20and%20Mehdi%20Hafezi%20Manshadi%20and%20Eliza%20Rutherford%20and%20Bo%20Li%20and%20Sissie%20Hsiao%20and%20Iain%20Barr%20and%20Alex%20Tudor%20and%20Matija%20Kecman%20and%20Arsha%20Nagrani%20and%20Vladimir%20Pchelin%20and%20Martin%20Sundermeyer%20and%20Aishwarya%20P%20S%20and%20Abhijit%20Karmarkar%20and%20Yi%20Gao%20and%20Grishma%20Chole%20and%20Olivier%20Bachem%20and%20Isabel%20Gao%20and%20Arturo%20BC%20and%20Matt%20Dibb%20and%20Mauro%20Verzetti%20and%20Felix%20Hernandez-Campos%20and%20Yana%20Lunts%20and%20Matthew%20Johnson%20and%20Julia%20Di%20Trapani%20and%20Raphael%20Koster%20and%20Idan%20Brusilovsky%20and%20Binbin%20Xiong%20and%20Megha%20Mohabey%20and%20Han%20Ke%20and%20Joe%20Zou%20and%20Tea%20Saboli%C4%87%20and%20V%C3%ADctor%20Campos%20and%20John%20Palowitch%20and%20Alex%20Morris%20and%20Linhai%20Qiu%20and%20Pranavaraj%20Ponnuramu%20and%20Fangtao%20Li%20and%20Vivek%20Sharma%20and%20Kiranbir%20Sodhia%20and%20Kaan%20Tekelioglu%20and%20Aleksandr%20Chuklin%20and%20Madhavi%20Yenugula%20and%20Erika%20Gemzer%20and%20Theofilos%20Strinopoulos%20and%20Sam%20El-Husseini%20and%20Huiyu%20Wang%20and%20Yan%20Zhong%20and%20Edouard%20Leurent%20and%20Paul%20Natsev%20and%20Weijun%20Wang%20and%20Dre%20Mahaarachchi%20and%20Tao%20Zhu%20and%20Songyou%20Peng%20and%20Sami%20Alabed%20and%20Cheng-Chun%20Lee%20and%20Anthony%20Brohan%20and%20Arthur%20Szlam%20and%20GS%20Oh%20and%20Anton%20Kovsharov%20and%20Jenny%20Lee%20and%20Renee%20Wong%20and%20Megan%20Barnes%20and%20Gregory%20Thornton%20and%20Felix%20Gimeno%20and%20Omer%20Levy%20and%20Martin%20Sevenich%20and%20Melvin%20Johnson%20and%20Jonathan%20Mallinson%20and%20Robert%20Dadashi%20and%20Ziyue%20Wang%20and%20Qingchun%20Ren%20and%20Preethi%20Lahoti%20and%20Arka%20Dhar%20and%20Josh%20Feldman%20and%20Dan%20Zheng%20and%20Thatcher%20Ulrich%20and%20Liviu%20Panait%20and%20Michiel%20Blokzijl%20and%20Cip%20Baetu%20and%20Josip%20Matak%20and%20Jitendra%20Harlalka%20and%20Maulik%20Shah%20and%20Tal%20Marian%20and%20Daniel%20von%20Dincklage%20and%20Cosmo%20Du%20and%20Ruy%20Ley-Wild%20and%20Bethanie%20Brownfield%20and%20Max%20Schumacher%20and%20Yury%20Stuken%20and%20Shadi%20Noghabi%20and%20Sonal%20Gupta%20and%20Xiaoqi%20Ren%20and%20Eric%20Malmi%20and%20Felix%20Weissenberger%20and%20Blanca%20Huergo%20and%20Maria%20Bauza%20and%20Thomas%20Lampe%20and%20Arthur%20Douillard%20and%20Mojtaba%20Seyedhosseini%20and%20Roy%20Frostig%20and%20Zoubin%20Ghahramani%20and%20Kelvin%20Nguyen%20and%20Kashyap%20Krishnakumar%20and%20Chengxi%20Ye%20and%20Rahul%20Gupta%20and%20Alireza%20Nazari%20and%20Robert%20Geirhos%20and%20Pete%20Shaw%20and%20Ahmed%20Eleryan%20and%20Dima%20Damen%20and%20Jennimaria%20Palomaki%20and%20Ted%20Xiao%20and%20Qiyin%20Wu%20and%20Quan%20Yuan%20and%20Phoenix%20Meadowlark%20and%20Matthew%20Bilotti%20and%20Raymond%20Lin%20and%20Mukund%20Sridhar%20and%20Yannick%20Schroecker%20and%20Da-Woon%20Chung%20and%20Jincheng%20Luo%20and%20Trevor%20Strohman%20and%20Tianlin%20Liu%20and%20Anne%20Zheng%20and%20Jesse%20Emond%20and%20Wei%20Wang%20and%20Andrew%20Lampinen%20and%20Toshiyuki%20Fukuzawa%20and%20Folawiyo%20Campbell-Ajala%20and%20Monica%20Roy%20and%20James%20Lee-Thorp%20and%20Lily%20Wang%20and%20Iftekhar%20Naim%20and%20%20Tony%20and%20%20Nguy%5C~%C3%AAn%20and%20Guy%20Bensky%20and%20Aditya%20Gupta%20and%20Dominika%20Rogozi%C5%84ska%20and%20Justin%20Fu%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Shahar%20Drath%20and%20Philipp%20Neubeck%20and%20Vaibhav%20Tulsyan%20and%20Arseniy%20Klimovskiy%20and%20Don%20Metzler%20and%20Sage%20Stevens%20and%20Angel%20Yeh%20and%20Junwei%20Yuan%20and%20Tianhe%20Yu%20and%20Kelvin%20Zhang%20and%20Alec%20Go%20and%20Vincent%20Tsang%20and%20Ying%20Xu%20and%20Andy%20Wan%20and%20Isaac%20Galatzer-Levy%20and%20Sam%20Sobell%20and%20Abodunrinwa%20Toki%20and%20Elizabeth%20Salesky%20and%20Wenlei%20Zhou%20and%20Diego%20Antognini%20and%20Sholto%20Douglas%20and%20Shimu%20Wu%20and%20Adam%20Lelkes%20and%20Frank%20Kim%20and%20Paul%20Cavallaro%20and%20Ana%20Salazar%20and%20Yuchi%20Liu%20and%20James%20Besley%20and%20Tiziana%20Refice%20and%20Yiling%20Jia%20and%20Zhang%20Li%20and%20Michal%20Sokolik%20and%20Arvind%20Kannan%20and%20Jon%20Simon%20and%20Jo%20Chick%20and%20Avia%20Aharon%20and%20Meet%20Gandhi%20and%20Mayank%20Daswani%20and%20Keyvan%20Amiri%20and%20Vighnesh%20Birodkar%20and%20Abe%20Ittycheriah%20and%20Peter%20Grabowski%20and%20Oscar%20Chang%20and%20Charles%20Sutton%20and%20%20Zhixin%20and%20%20Lai%20and%20Umesh%20Telang%20and%20Susie%20Sargsyan%20and%20Tao%20Jiang%20and%20Raphael%20Hoffmann%20and%20Nicole%20Brichtova%20and%20Matteo%20Hessel%20and%20Jonathan%20Halcrow%20and%20Sammy%20Jerome%20and%20Geoff%20Brown%20and%20Alex%20Tomala%20and%20Elena%20Buchatskaya%20and%20Dian%20Yu%20and%20Sachit%20Menon%20and%20Pol%20Moreno%20and%20Yuguo%20Liao%20and%20Vicky%20Zayats%20and%20Luming%20Tang%20and%20SQ%20Mah%20and%20Ashish%20Shenoy%20and%20Alex%20Siegman%20and%20Majid%20Hadian%20and%20Okwan%20Kwon%20and%20Tao%20Tu%20and%20Nima%20Khajehnouri%20and%20Ryan%20Foley%20and%20Parisa%20Haghani%20and%20Zhongru%20Wu%20and%20Vaishakh%20Keshava%20and%20Khyatti%20Gupta%20and%20Tony%20Bruguier%20and%20Rui%20Yao%20and%20Danny%20Karmon%20and%20Luisa%20Zintgraf%20and%20Zhicheng%20Wang%20and%20Enrique%20Piqueras%20and%20Junehyuk%20Jung%20and%20Jenny%20Brennan%20and%20Diego%20Machado%20and%20Marissa%20Giustina%20and%20MH%20Tessler%20and%20Kamyu%20Lee%20and%20Qiao%20Zhang%20and%20Joss%20Moore%20and%20Kaspar%20Daugaard%20and%20Alexander%20Fr%C3%B6mmgen%20and%20Jennifer%20Beattie%20and%20Fred%20Zhang%20and%20Daniel%20Kasenberg%20and%20Ty%20Geri%20and%20Danfeng%20Qin%20and%20Gaurav%20Singh%20Tomar%20and%20Tom%20Ouyang%20and%20Tianli%20Yu%20and%20Luowei%20Zhou%20and%20Rajiv%20Mathews%20and%20Andy%20Davis%20and%20Yaoyiran%20Li%20and%20Jai%20Gupta%20and%20Damion%20Yates%20and%20Linda%20Deng%20and%20Elizabeth%20Kemp%20and%20Ga-Young%20Joung%20and%20Sergei%20Vassilvitskii%20and%20Mandy%20Guo%20and%20Pallavi%20LV%20and%20Dave%20Dopson%20and%20Sami%20Lachgar%20and%20Lara%20McConnaughey%20and%20Himadri%20Choudhury%20and%20Dragos%20Dena%20and%20Aaron%20Cohen%20and%20Joshua%20Ainslie%20and%20Sergey%20Levi%20and%20Parthasarathy%20Gopavarapu%20and%20Polina%20Zablotskaia%20and%20Hugo%20Vallet%20and%20Sanaz%20Bahargam%20and%20Xiaodan%20Tang%20and%20Nenad%20Tomasev%20and%20Ethan%20Dyer%20and%20Daniel%20Balle%20and%20Hongrae%20Lee%20and%20William%20Bono%20and%20Jorge%20Gonzalez%20Mendez%20and%20Vadim%20Zubov%20and%20Shentao%20Yang%20and%20Ivor%20Rendulic%20and%20Yanyan%20Zheng%20and%20Andrew%20Hogue%20and%20Golan%20Pundak%20and%20Ralph%20Leith%20and%20Avishkar%20Bhoopchand%20and%20Michael%20Han%20and%20Mislav%20%C5%BDani%C4%87%20and%20Tom%20Schaul%20and%20Manolis%20Delakis%20and%20Tejas%20Iyer%20and%20Guanyu%20Wang%20and%20Harman%20Singh%20and%20Abdelrahman%20Abdelhamed%20and%20Tara%20Thomas%20and%20Siddhartha%20Brahma%20and%20Hilal%20Dib%20and%20Naveen%20Kumar%20and%20Wenxuan%20Zhou%20and%20Liang%20Bai%20and%20Pushkar%20Mishra%20and%20Jiao%20Sun%20and%20Valentin%20Anklin%20and%20Roykrong%20Sukkerd%20and%20Lauren%20Agubuzu%20and%20Anton%20Briukhov%20and%20Anmol%20Gulati%20and%20Maximilian%20Sieb%20and%20Fabio%20Pardo%20and%20Sara%20Nasso%20and%20Junquan%20Chen%20and%20Kexin%20Zhu%20and%20Tiberiu%20Sosea%20and%20Alex%20Goldin%20and%20Keith%20Rush%20and%20Spurthi%20Amba%20Hombaiah%20and%20Andreas%20Noever%20and%20Allan%20Zhou%20and%20Sam%20Haves%20and%20Mary%20Phuong%20and%20Jake%20Ades%20and%20Yi-ting%20Chen%20and%20Lin%20Yang%20and%20Joseph%20Pagadora%20and%20Stan%20Bileschi%20and%20Victor%20Cotruta%20and%20Rachel%20Saputro%20and%20Arijit%20Pramanik%20and%20Sean%20Ammirati%20and%20Dan%20Garrette%20and%20Kevin%20Villela%20and%20Tim%20Blyth%20and%20Canfer%20Akbulut%20and%20Neha%20Jha%20and%20Alban%20Rrustemi%20and%20Arissa%20Wongpanich%20and%20Chirag%20Nagpal%20and%20Yonghui%20Wu%20and%20Morgane%20Rivi%C3%A8re%20and%20Sergey%20Kishchenko%20and%20Pranesh%20Srinivasan%20and%20Alice%20Chen%20and%20Animesh%20Sinha%20and%20Trang%20Pham%20and%20Bill%20Jia%20and%20Tom%20Hennigan%20and%20Anton%20Bakalov%20and%20Nithya%20Attaluri%20and%20Drew%20Garmon%20and%20Daniel%20Rodriguez%20and%20Dawid%20Wegner%20and%20Wenhao%20Jia%20and%20Evan%20Senter%20and%20Noah%20Fiedel%20and%20Denis%20Petek%20and%20Yuchuan%20Liu%20and%20Cassidy%20Hardin%20and%20Harshal%20Tushar%20Lehri%20and%20Joao%20Carreira%20and%20Sara%20Smoot%20and%20Marcel%20Prasetya%20and%20Nami%20Akazawa%20and%20Anca%20Stefanoiu%20and%20Chia-Hua%20Ho%20and%20Anelia%20Angelova%20and%20Kate%20Lin%20and%20Min%20Kim%20and%20Charles%20Chen%20and%20Marcin%20Sieniek%20and%20Alice%20Li%20and%20Tongfei%20Guo%20and%20Sorin%20Baltateanu%20and%20Pouya%20Tafti%20and%20Michael%20Wunder%20and%20Nadav%20Olmert%20and%20Divyansh%20Shukla%20and%20Jingwei%20Shen%20and%20Neel%20Kovelamudi%20and%20Balaji%20Venkatraman%20and%20Seth%20Neel%20and%20Romal%20Thoppilan%20and%20Jerome%20Connor%20and%20Frederik%20Benzing%20and%20Axel%20Stjerngren%20and%20Golnaz%20Ghiasi%20and%20Alex%20Polozov%20and%20Joshua%20Howland%20and%20Theophane%20Weber%20and%20Justin%20Chiu%20and%20Ganesh%20Poomal%20Girirajan%20and%20Andreas%20Terzis%20and%20Pidong%20Wang%20and%20Fangda%20Li%20and%20Yoav%20Ben%20Shalom%20and%20Dinesh%20Tewari%20and%20Matthew%20Denton%20and%20Roee%20Aharoni%20and%20Norbert%20Kalb%20and%20Heri%20Zhao%20and%20Junlin%20Zhang%20and%20Angelos%20Filos%20and%20Matthew%20Rahtz%20and%20Lalit%20Jain%20and%20Connie%20Fan%20and%20Vitor%20Rodrigues%20and%20Ruth%20Wang%20and%20Richard%20Shin%20and%20Jacob%20Austin%20and%20Roman%20Ring%20and%20Mariella%20Sanchez-Vargas%20and%20Mehadi%20Hassen%20and%20Ido%20Kessler%20and%20Uri%20Alon%20and%20Gufeng%20Zhang%20and%20Wenhu%20Chen%20and%20Yenai%20Ma%20and%20Xiance%20Si%20and%20Le%20Hou%20and%20Azalia%20Mirhoseini%20and%20Marc%20Wilson%20and%20Geoff%20Bacon%20and%20Becca%20Roelofs%20and%20Lei%20Shu%20and%20Gautam%20Vasudevan%20and%20Jonas%20Adler%20and%20Artur%20Dwornik%20and%20Tayfun%20Terzi%20and%20Matt%20Lawlor%20and%20Harry%20Askham%20and%20Mike%20Bernico%20and%20Xuanyi%20Dong%20and%20Chris%20Hidey%20and%20Kevin%20Kilgour%20and%20Ga%C3%ABl%20Liu%20and%20Surya%20Bhupatiraju%20and%20Luke%20Leonhard%20and%20Siqi%20Zuo%20and%20Partha%20Talukdar%20and%20Qing%20Wei%20and%20Aliaksei%20Severyn%20and%20V%C3%ADt%20List%C3%ADk%20and%20Jong%20Lee%20and%20Aditya%20Tripathi%20and%20SK%20Park%20and%20Yossi%20Matias%20and%20Hao%20Liu%20and%20Alex%20Ruiz%20and%20Rajesh%20Jayaram%20and%20Jackson%20Tolins%20and%20Pierre%20Marcenac%20and%20Yiming%20Wang%20and%20Bryan%20Seybold%20and%20Henry%20Prior%20and%20Deepak%20Sharma%20and%20Jack%20Weber%20and%20Mikhail%20Sirotenko%20and%20Yunhsuan%20Sung%20and%20Dayou%20Du%20and%20Ellie%20Pavlick%20and%20Stefan%20Zinke%20and%20Markus%20Freitag%20and%20Max%20Dylla%20and%20Montse%20Gonzalez%20Arenas%20and%20Natan%20Potikha%20and%20Omer%20Goldman%20and%20Connie%20Tao%20and%20Rachita%20Chhaparia%20and%20Maria%20Voitovich%20and%20Pawan%20Dogra%20and%20Andrija%20Ra%C5%BEnatovi%C4%87%20and%20Zak%20Tsai%20and%20Chong%20You%20and%20Oleaser%20Johnson%20and%20George%20Tucker%20and%20Chenjie%20Gu%20and%20Jae%20Yoo%20and%20Maryam%20Majzoubi%20and%20Valentin%20Gabeur%20and%20Bahram%20Raad%20and%20Rocky%20Rhodes%20and%20Kashyap%20Kolipaka%20and%20Heidi%20Howard%20and%20Geta%20Sampemane%20and%20Benny%20Li%20and%20Chulayuth%20Asawaroengchai%20and%20Duy%20Nguyen%20and%20Chiyuan%20Zhang%20and%20Timothee%20Cour%20and%20Xinxin%20Yu%20and%20Zhao%20Fu%20and%20Joe%20Jiang%20and%20Po-Sen%20Huang%20and%20Gabriela%20Surita%20and%20I%C3%B1aki%20Iturrate%20and%20Yael%20Karov%20and%20Michael%20Collins%20and%20Martin%20Baeuml%20and%20Fabian%20Fuchs%20and%20Shilpa%20Shetty%20and%20Swaroop%20Ramaswamy%20and%20Sayna%20Ebrahimi%20and%20Qiuchen%20Guo%20and%20Jeremy%20Shar%20and%20Gabe%20Barth-Maron%20and%20Sravanti%20Addepalli%20and%20Bryan%20Richter%20and%20Chin-Yi%20Cheng%20and%20Eug%C3%A9nie%20Rives%20and%20Fei%20Zheng%20and%20Johannes%20Griesser%20and%20Nishanth%20Dikkala%20and%20Yoel%20Zeldes%20and%20Ilkin%20Safarli%20and%20Dipanjan%20Das%20and%20Himanshu%20Srivastava%20and%20Sadh%20MNM%20Khan%20and%20Xin%20Li%20and%20Aditya%20Pandey%20and%20Larisa%20Markeeva%20and%20Dan%20Belov%20and%20Qiqi%20Yan%20and%20Miko%C5%82aj%20Rybi%C5%84ski%20and%20Tao%20Chen%20and%20Megha%20Nawhal%20and%20Michael%20Quinn%20and%20Vineetha%20Govindaraj%20and%20Sarah%20York%20and%20Reed%20Roberts%20and%20Roopal%20Garg%20and%20Namrata%20Godbole%20and%20Jake%20Abernethy%20and%20Anil%20Das%20and%20Lam%20Nguyen%20Thiet%20and%20Jonathan%20Tompson%20and%20John%20Nham%20and%20Neera%20Vats%20and%20Ben%20Caine%20and%20Wesley%20Helmholz%20and%20Francesco%20Pongetti%20and%20Yeongil%20Ko%20and%20James%20An%20and%20Clara%20Huiyi%20Hu%20and%20Yu-Cheng%20Ling%20and%20Julia%20Pawar%20and%20Robert%20Leland%20and%20Keisuke%20Kinoshita%20and%20Waleed%20Khawaja%20and%20Marco%20Selvi%20and%20Eugene%20Ie%20and%20Danila%20Sinopalnikov%20and%20Lev%20Proleev%20and%20Nilesh%20Tripuraneni%20and%20Michele%20Bevilacqua%20and%20Seungji%20Lee%20and%20Clayton%20Sanford%20and%20Dan%20Suh%20and%20Dustin%20Tran%20and%20Jeff%20Dean%20and%20Simon%20Baumgartner%20and%20Jens%20Heitkaemper%20and%20Sagar%20Gubbi%20and%20Kristina%20Toutanova%20and%20Yichong%20Xu%20and%20Chandu%20Thekkath%20and%20Keran%20Rong%20and%20Palak%20Jain%20and%20Annie%20Xie%20and%20Yan%20Virin%20and%20Yang%20Li%20and%20Lubo%20Litchev%20and%20Richard%20Powell%20and%20Tarun%20Bharti%20and%20Adam%20Kraft%20and%20Nan%20Hua%20and%20Marissa%20Ikonomidis%20and%20Ayal%20Hitron%20and%20Sanjiv%20Kumar%20and%20Loic%20Matthey%20and%20Sophie%20Bridgers%20and%20Lauren%20Lax%20and%20Ishaan%20Malhi%20and%20Ondrej%20Skopek%20and%20Ashish%20Gupta%20and%20Jiawei%20Cao%20and%20Mitchelle%20Rasquinha%20and%20Siim%20P%C3%B5der%20and%20Wojciech%20Stokowiec%20and%20Nicholas%20Roth%20and%20Guowang%20Li%20and%20Micha%C3%ABl%20Sander%20and%20Joshua%20Kessinger%20and%20Vihan%20Jain%20and%20Edward%20Loper%20and%20Wonpyo%20Park%20and%20Michal%20Yarom%20and%20Liqun%20Cheng%20and%20Guru%20Guruganesh%20and%20Kanishka%20Rao%20and%20Yan%20Li%20and%20Catarina%20Barros%20and%20Mikhail%20Sushkov%20and%20Chun-Sung%20Ferng%20and%20Rohin%20Shah%20and%20Ophir%20Aharoni%20and%20Ravin%20Kumar%20and%20Tim%20McConnell%20and%20Peiran%20Li%20and%20Chen%20Wang%20and%20Fernando%20Pereira%20and%20Craig%20Swanson%20and%20Fayaz%20Jamil%20and%20Yan%20Xiong%20and%20Anitha%20Vijayakumar%20and%20Prakash%20Shroff%20and%20Kedar%20Soparkar%20and%20Jindong%20Gu%20and%20Livio%20Baldini%20Soares%20and%20Eric%20Wang%20and%20Kushal%20Majmundar%20and%20Aurora%20Wei%20and%20Kai%20Bailey%20and%20Nora%20Kassner%20and%20Chizu%20Kawamoto%20and%20Goran%20%C5%BDu%C5%BEi%C4%87%20and%20Victor%20Gomes%20and%20Abhirut%20Gupta%20and%20Michael%20Guzman%20and%20Ishita%20Dasgupta%20and%20Xinyi%20Bai%20and%20Zhufeng%20Pan%20and%20Francesco%20Piccinno%20and%20Hadas%20Natalie%20Vogel%20and%20Octavio%20Ponce%20and%20Adrian%20Hutter%20and%20Paul%20Chang%20and%20Pan-Pan%20Jiang%20and%20Ionel%20Gog%20and%20Vlad%20Ionescu%20and%20James%20Manyika%20and%20Fabian%20Pedregosa%20and%20Harry%20Ragan%20and%20Zach%20Behrman%20and%20Ryan%20Mullins%20and%20Coline%20Devin%20and%20Aroonalok%20Pyne%20and%20Swapnil%20Gawde%20and%20Martin%20Chadwick%20and%20Yiming%20Gu%20and%20Sasan%20Tavakkol%20and%20Andy%20Twigg%20and%20Naman%20Goyal%20and%20Ndidi%20Elue%20and%20Anna%20Goldie%20and%20Srinivasan%20Venkatachary%20and%20Hongliang%20Fei%20and%20Ziqiang%20Feng%20and%20Marvin%20Ritter%20and%20Isabel%20Leal%20and%20Sudeep%20Dasari%20and%20Pei%20Sun%20and%20Alif%20Raditya%20Rochman%20and%20Brendan%20O%27Donoghue%20and%20Yuchen%20Liu%20and%20Jim%20Sproch%20and%20Kai%20Chen%20and%20Natalie%20Clay%20and%20Slav%20Petrov%20and%20Sailesh%20Sidhwani%20and%20Ioana%20Mihailescu%20and%20Alex%20Panagopoulos%20and%20AJ%20Piergiovanni%20and%20Yunfei%20Bai%20and%20George%20Powell%20and%20Deep%20Karkhanis%20and%20Trevor%20Yacovone%20and%20Petr%20Mitrichev%20and%20Joe%20Kovac%20and%20Dave%20Uthus%20and%20Amir%20Yazdanbakhsh%20and%20David%20Amos%20and%20Steven%20Zheng%20and%20Bing%20Zhang%20and%20Jin%20Miao%20and%20Bhuvana%20Ramabhadran%20and%20Soroush%20Radpour%20and%20Shantanu%20Thakoor%20and%20Josh%20Newlan%20and%20Oran%20Lang%20and%20Orion%20Jankowski%20and%20Shikhar%20Bharadwaj%20and%20Jean-Michel%20Sarr%20and%20Shereen%20Ashraf%20and%20Sneha%20Mondal%20and%20Jun%20Yan%20and%20Ankit%20Singh%20Rawat%20and%20Sarmishta%20Velury%20and%20Greg%20Kochanski%20and%20Tom%20Eccles%20and%20Franz%20Och%20and%20Abhanshu%20Sharma%20and%20Ethan%20Mahintorabi%20and%20Alex%20Gurney%20and%20Carrie%20Muir%20and%20Vered%20Cohen%20and%20Saksham%20Thakur%20and%20Adam%20Bloniarz%20and%20Asier%20Mujika%20and%20Alexander%20Pritzel%20and%20Paul%20Caron%20and%20Altaf%20Rahman%20and%20Fiona%20Lang%20and%20Yasumasa%20Onoe%20and%20Petar%20Sirkovic%20and%20Jay%20Hoover%20and%20Ying%20Jian%20and%20Pablo%20Duque%20and%20Arun%20Narayanan%20and%20David%20Soergel%20and%20Alex%20Haig%20and%20Loren%20Maggiore%20and%20Shyamal%20Buch%20and%20Josef%20Dean%20and%20Ilya%20Figotin%20and%20Igor%20Karpov%20and%20Shaleen%20Gupta%20and%20Denny%20Zhou%20and%20Muhuan%20Huang%20and%20Ashwin%20Vaswani%20and%20Christopher%20Semturs%20and%20Kaushik%20Shivakumar%20and%20Yu%20Watanabe%20and%20Vinodh%20Kumar%20Rajendran%20and%20Eva%20Lu%20and%20Yanhan%20Hou%20and%20Wenting%20Ye%20and%20Shikhar%20Vashishth%20and%20Nana%20Nti%20and%20Vytenis%20Sakenas%20and%20Darren%20Ni%20and%20Doug%20DeCarlo%20and%20Michael%20Bendersky%20and%20Sumit%20Bagri%20and%20Nacho%20Cano%20and%20Elijah%20Peake%20and%20Simon%20Tokumine%20and%20Varun%20Godbole%20and%20Carlos%20Gu%C3%ADa%20and%20Tanya%20Lando%20and%20Vittorio%20Selo%20and%20Seher%20Ellis%20and%20Danny%20Tarlow%20and%20Daniel%20Gillick%20and%20Alessandro%20Epasto%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Meng%20Wei%20and%20Meiyan%20Xie%20and%20Ankur%20Taly%20and%20Michela%20Paganini%20and%20Mukund%20Sundararajan%20and%20Daniel%20Toyama%20and%20Ting%20Yu%20and%20Dessie%20Petrova%20and%20Aneesh%20Pappu%20and%20Rohan%20Agrawal%20and%20Senaka%20Buthpitiya%20and%20Justin%20Frye%20and%20Thomas%20Buschmann%20and%20Remi%20Crocker%20and%20Marco%20Tagliasacchi%20and%20Mengchao%20Wang%20and%20Da%20Huang%20and%20Sagi%20Perel%20and%20Brian%20Wieder%20and%20Hideto%20Kazawa%20and%20Weiyue%20Wang%20and%20Jeremy%20Cole%20and%20Himanshu%20Gupta%20and%20Ben%20Golan%20and%20Seojin%20Bang%20and%20Nitish%20Kulkarni%20and%20Ken%20Franko%20and%20Casper%20Liu%20and%20Doug%20Reid%20and%20Sid%20Dalmia%20and%20Jay%20Whang%20and%20Kevin%20Cen%20and%20Prasha%20Sundaram%20and%20Johan%20Ferret%20and%20Berivan%20Isik%20and%20Lucian%20Ionita%20and%20Guan%20Sun%20and%20Anna%20Shekhawat%20and%20Muqthar%20Mohammad%20and%20Philip%20Pham%20and%20Ronny%20Huang%20and%20Karthik%20Raman%20and%20Xingyi%20Zhou%20and%20Ross%20Mcilroy%20and%20Austin%20Myers%20and%20Sheng%20Peng%20and%20Jacob%20Scott%20and%20Paul%20Covington%20and%20Sofia%20Erell%20and%20Pratik%20Joshi%20and%20Jo%C3%A3o%20Gabriel%20Oliveira%20and%20Natasha%20Noy%20and%20Tajwar%20Nasir%20and%20Jake%20Walker%20and%20Vera%20Axelrod%20and%20Tim%20Dozat%20and%20Pu%20Han%20and%20Chun-Te%20Chu%20and%20Eugene%20Weinstein%20and%20Anand%20Shukla%20and%20Shreyas%20Chandrakaladharan%20and%20Petra%20Poklukar%20and%20Bonnie%20Li%20and%20Ye%20Jin%20and%20Prem%20Eruvbetine%20and%20Steven%20Hansen%20and%20Avigail%20Dabush%20and%20Alon%20Jacovi%20and%20Samrat%20Phatale%20and%20Chen%20Zhu%20and%20Steven%20Baker%20and%20Mo%20Shomrat%20and%20Yang%20Xiao%20and%20Jean%20Pouget-Abadie%20and%20Mingyang%20Zhang%20and%20Fanny%20Wei%20and%20Yang%20Song%20and%20Helen%20King%20and%20Yiling%20Huang%20and%20Yun%20Zhu%20and%20Ruoxi%20Sun%20and%20Juliana%20Vicente%20Franco%20and%20Chu-Cheng%20Lin%20and%20Sho%20Arora%20and%20%20Hui%20and%20%20Li%20and%20Vivian%20Xia%20and%20Luke%20Vilnis%20and%20Mariano%20Schain%20and%20Kaiz%20Alarakyia%20and%20Laurel%20Prince%20and%20Aaron%20Phillips%20and%20Caleb%20Habtegebriel%20and%20Luyao%20Xu%20and%20Huan%20Gui%20and%20Santiago%20Ontanon%20and%20Lora%20Aroyo%20and%20Karan%20Gill%20and%20Peggy%20Lu%20and%20Yash%20Katariya%20and%20Dhruv%20Madeka%20and%20Shankar%20Krishnan%20and%20Shubha%20Srinivas%20Raghvendra%20and%20James%20Freedman%20and%20Yi%20Tay%20and%20Gaurav%20Menghani%20and%20Peter%20Choy%20and%20Nishita%20Shetty%20and%20Dan%20Abolafia%20and%20Doron%20Kukliansky%20and%20Edward%20Chou%20and%20Jared%20Lichtarge%20and%20Ken%20Burke%20and%20Ben%20Coleman%20and%20Dee%20Guo%20and%20Larry%20Jin%20and%20Indro%20Bhattacharya%20and%20Victoria%20Langston%20and%20Yiming%20Li%20and%20Suyog%20Kotecha%20and%20Alex%20Yakubovich%20and%20Xinyun%20Chen%20and%20Petre%20Petrov%20and%20Tolly%20Powell%20and%20Yanzhang%20He%20and%20Corbin%20Quick%20and%20Kanav%20Garg%20and%20Dawsen%20Hwang%20and%20Yang%20Lu%20and%20Srinadh%20Bhojanapalli%20and%20Kristian%20Kjems%20and%20Ramin%20Mehran%20and%20Aaron%20Archer%20and%20Hado%20van%20Hasselt%20and%20Ashwin%20Balakrishna%20and%20JK%20Kearns%20and%20Meiqi%20Guo%20and%20Jason%20Riesa%20and%20Mikita%20Sazanovich%20and%20Xu%20Gao%20and%20Chris%20Sauer%20and%20Chengrun%20Yang%20and%20XiangHai%20Sheng%20and%20Thomas%20Jimma%20and%20Wouter%20Van%20Gansbeke%20and%20Vitaly%20Nikolaev%20and%20Wei%20Wei%20and%20Katie%20Millican%20and%20Ruizhe%20Zhao%20and%20Justin%20Snyder%20and%20Levent%20Bolelli%20and%20Maura%20O%27Brien%20and%20Shawn%20Xu%20and%20Fei%20Xia%20and%20Wentao%20Yuan%20and%20Arvind%20Neelakantan%20and%20David%20Barker%20and%20Sachin%20Yadav%20and%20Hannah%20Kirkwood%20and%20Farooq%20Ahmad%20and%20Joel%20Wee%20and%20Jordan%20Grimstad%20and%20Boyu%20Wang%20and%20Matthew%20Wiethoff%20and%20Shane%20Settle%20and%20Miaosen%20Wang%20and%20Charles%20Blundell%20and%20Jingjing%20Chen%20and%20Chris%20Duvarney%20and%20Grace%20Hu%20and%20Olaf%20Ronneberger%20and%20Alex%20Lee%20and%20Yuanzhen%20Li%20and%20Abhishek%20Chakladar%20and%20Alena%20Butryna%20and%20Georgios%20Evangelopoulos%20and%20Guillaume%20Desjardins%20and%20Jonni%20Kanerva%20and%20Henry%20Wang%20and%20Averi%20Nowak%20and%20Nick%20Li%20and%20Alyssa%20Loo%20and%20Art%20Khurshudov%20and%20Laurent%20El%20Shafey%20and%20Nagabhushan%20Baddi%20and%20Karel%20Lenc%20and%20Yasaman%20Razeghi%20and%20Tom%20Lieber%20and%20Amer%20Sinha%20and%20Xiao%20Ma%20and%20Yao%20Su%20and%20James%20Huang%20and%20Asahi%20Ushio%20and%20Hanna%20Klimczak-Pluci%C5%84ska%20and%20Kareem%20Mohamed%20and%20JD%20Chen%20and%20Simon%20Osindero%20and%20Stav%20Ginzburg%20and%20Lampros%20Lamprou%20and%20Vasilisa%20Bashlovkina%20and%20Duc-Hieu%20Tran%20and%20Ali%20Khodaei%20and%20Ankit%20Anand%20and%20Yixian%20Di%20and%20Ramy%20Eskander%20and%20Manish%20Reddy%20Vuyyuru%20and%20Jasmine%20Liu%20and%20Aishwarya%20Kamath%20and%20Roman%20Goldenberg%20and%20Mathias%20Bellaiche%20and%20Juliette%20Pluto%20and%20Bill%20Rosgen%20and%20Hassan%20Mansoor%20and%20William%20Wong%20and%20Suhas%20Ganesh%20and%20Eric%20Bailey%20and%20Scott%20Baird%20and%20Dan%20Deutsch%20and%20Jinoo%20Baek%20and%20Xuhui%20Jia%20and%20Chansoo%20Lee%20and%20Abe%20Friesen%20and%20Nathaniel%20Braun%20and%20Kate%20Lee%20and%20Amayika%20Panda%20and%20Steven%20M.%20Hernandez%20and%20Duncan%20Williams%20and%20Jianqiao%20Liu%20and%20Ethan%20Liang%20and%20Arnaud%20Autef%20and%20Emily%20Pitler%20and%20Deepali%20Jain%20and%20Phoebe%20Kirk%20and%20Oskar%20Bunyan%20and%20Jaume%20Sanchez%20Elias%20and%20Tongxin%20Yin%20and%20Machel%20Reid%20and%20Aedan%20Pope%20and%20Nikita%20Putikhin%20and%20Bidisha%20Samanta%20and%20Sergio%20Guadarrama%20and%20Dahun%20Kim%20and%20Simon%20Rowe%20and%20Marcella%20Valentine%20and%20Geng%20Yan%20and%20Alex%20Salcianu%20and%20David%20Silver%20and%20Gan%20Song%20and%20Richa%20Singh%20and%20Shuai%20Ye%20and%20Hannah%20DeBalsi%20and%20Majd%20Al%20Merey%20and%20Eran%20Ofek%20and%20Albert%20Webson%20and%20Shibl%20Mourad%20and%20Ashwin%20Kakarla%20and%20Silvio%20Lattanzi%20and%20Nick%20Roy%20and%20Evgeny%20Sluzhaev%20and%20Christina%20Butterfield%20and%20Alessio%20Tonioni%20and%20Nathan%20Waters%20and%20Sudhindra%20Kopalle%20and%20Jason%20Chase%20and%20James%20Cohan%20and%20Girish%20Ramchandra%20Rao%20and%20Robert%20Berry%20and%20Michael%20Voznesensky%20and%20Shuguang%20Hu%20and%20Kristen%20Chiafullo%20and%20Sharat%20Chikkerur%20and%20George%20Scrivener%20and%20Ivy%20Zheng%20and%20Jeremy%20Wiesner%20and%20Wolfgang%20Macherey%20and%20Timothy%20Lillicrap%20and%20Fei%20Liu%20and%20Brian%20Walker%20and%20David%20Welling%20and%20Elinor%20Davies%20and%20Yangsibo%20Huang%20and%20Lijie%20Ren%20and%20Nir%20Shabat%20and%20Alessandro%20Agostini%20and%20Mariko%20Iinuma%20and%20Dustin%20Zelle%20and%20Rohit%20Sathyanarayana%20and%20Andrea%20D%27olimpio%20and%20Morgan%20Redshaw%20and%20Matt%20Ginsberg%20and%20Ashwin%20Murthy%20and%20Mark%20Geller%20and%20Tatiana%20Matejovicova%20and%20Ayan%20Chakrabarti%20and%20Ryan%20Julian%20and%20Christine%20Chan%20and%20Qiong%20Hu%20and%20Daniel%20Jarrett%20and%20Manu%20Agarwal%20and%20Jeshwanth%20Challagundla%20and%20Tao%20Li%20and%20Sandeep%20Tata%20and%20Wen%20Ding%20and%20Maya%20Meng%20and%20Zhuyun%20Dai%20and%20Giulia%20Vezzani%20and%20Shefali%20Garg%20and%20Jannis%20Bulian%20and%20Mary%20Jasarevic%20and%20Honglong%20Cai%20and%20Harish%20Rajamani%20and%20Adam%20Santoro%20and%20Florian%20Hartmann%20and%20Chen%20Liang%20and%20Bartek%20Perz%20and%20Apoorv%20Jindal%20and%20Fan%20Bu%20and%20Sungyong%20Seo%20and%20Ryan%20Poplin%20and%20Adrian%20Goedeckemeyer%20and%20Badih%20Ghazi%20and%20Nikhil%20Khadke%20and%20Leon%20Liu%20and%20Kevin%20Mather%20and%20Mingda%20Zhang%20and%20Ali%20Shah%20and%20Alex%20Chen%20and%20Jinliang%20Wei%20and%20Keshav%20Shivam%20and%20Yuan%20Cao%20and%20Donghyun%20Cho%20and%20Angelo%20Scorza%20Scarpati%20and%20Michael%20Moffitt%20and%20Clara%20Barbu%20and%20Ivan%20Jurin%20and%20Ming-Wei%20Chang%20and%20Hongbin%20Liu%20and%20Hao%20Zheng%20and%20Shachi%20Dave%20and%20Christine%20Kaeser-Chen%20and%20Xiaobin%20Yu%20and%20Alvin%20Abdagic%20and%20Lucas%20Gonzalez%20and%20Yanping%20Huang%20and%20Peilin%20Zhong%20and%20Cordelia%20Schmid%20and%20Bryce%20Petrini%20and%20Alex%20Wertheim%20and%20Jifan%20Zhu%20and%20Hoang%20Nguyen%20and%20Kaiyang%20Ji%20and%20Yanqi%20Zhou%20and%20Tao%20Zhou%20and%20Fangxiaoyu%20Feng%20and%20Regev%20Cohen%20and%20David%20Rim%20and%20Shubham%20Milind%20Phal%20and%20Petko%20Georgiev%20and%20Ariel%20Brand%20and%20Yue%20Ma%20and%20Wei%20Li%20and%20Somit%20Gupta%20and%20Chao%20Wang%20and%20Pavel%20Dubov%20and%20Jean%20Tarbouriech%20and%20Kingshuk%20Majumder%20and%20Huijian%20Li%20and%20Norman%20Rink%20and%20Apurv%20Suman%20and%20Yang%20Guo%20and%20Yinghao%20Sun%20and%20Arun%20Nair%20and%20Xiaowei%20Xu%20and%20Mohamed%20Elhawaty%20and%20Rodrigo%20Cabrera%20and%20Guangxing%20Han%20and%20Julian%20Eisenschlos%20and%20Junwen%20Bai%20and%20Yuqi%20Li%20and%20Yamini%20Bansal%20and%20Thibault%20Sellam%20and%20Mina%20Khan%20and%20Hung%20Nguyen%20and%20Justin%20Mao-Jones%20and%20Nikos%20Parotsidis%20and%20Jake%20Marcus%20and%20Cindy%20Fan%20and%20Roland%20Zimmermann%20and%20Yony%20Kochinski%20and%20Laura%20Graesser%20and%20Feryal%20Behbahani%20and%20Alvaro%20Caceres%20and%20Michael%20Riley%20and%20Patrick%20Kane%20and%20Sandra%20Lefdal%20and%20Rob%20Willoughby%20and%20Paul%20Vicol%20and%20Lun%20Wang%20and%20Shujian%20Zhang%20and%20Ashleah%20Gill%20and%20Yu%20Liang%20and%20Gautam%20Prasad%20and%20Soroosh%20Mariooryad%20and%20Mehran%20Kazemi%20and%20Zifeng%20Wang%20and%20Kritika%20Muralidharan%20and%20Paul%20Voigtlaender%20and%20Jeffrey%20Zhao%20and%20Huanjie%20Zhou%20and%20Nina%20D%27Souza%20and%20Aditi%20Mavalankar%20and%20S%C3%A9b%20Arnold%20and%20Nick%20Young%20and%20Obaid%20Sarvana%20and%20Chace%20Lee%20and%20Milad%20Nasr%20and%20Tingting%20Zou%20and%20Seokhwan%20Kim%20and%20Lukas%20Haas%20and%20Kaushal%20Patel%20and%20Neslihan%20Bulut%20and%20David%20Parkinson%20and%20Courtney%20Biles%20and%20Dmitry%20Kalashnikov%20and%20Chi%20Ming%20To%20and%20Aviral%20Kumar%20and%20Jessica%20Austin%20and%20Alex%20Greve%20and%20Lei%20Zhang%20and%20Megha%20Goel%20and%20Yeqing%20Li%20and%20Sergey%20Yaroshenko%20and%20Max%20Chang%20and%20Abhishek%20Jindal%20and%20Geoff%20Clark%20and%20Hagai%20Taitelbaum%20and%20Dale%20Johnson%20and%20Ofir%20Roval%20and%20Jeongwoo%20Ko%20and%20Anhad%20Mohananey%20and%20Christian%20Schuler%20and%20Shenil%20Dodhia%20and%20Ruichao%20Li%20and%20Kazuki%20Osawa%20and%20Claire%20Cui%20and%20Peng%20Xu%20and%20Rushin%20Shah%20and%20Tao%20Huang%20and%20Ela%20Gruzewska%20and%20Nathan%20Clement%20and%20Mudit%20Verma%20and%20Olcan%20Sercinoglu%20and%20Hai%20Qian%20and%20Viral%20Shah%20and%20Masa%20Yamaguchi%20and%20Abhinit%20Modi%20and%20Takahiro%20Kosakai%20and%20Thomas%20Strohmann%20and%20Junhao%20Zeng%20and%20Beliz%20Gunel%20and%20Jun%20Qian%20and%20Austin%20Tarango%20and%20Krzysztof%20Jastrz%C4%99bski%20and%20Robert%20David%20and%20Jyn%20Shan%20and%20Parker%20Schuh%20and%20Kunal%20Lad%20and%20Willi%20Gierke%20and%20Mukundan%20Madhavan%20and%20Xinyi%20Chen%20and%20Mark%20Kurzeja%20and%20Rebeca%20Santamaria-Fernandez%20and%20Dawn%20Chen%20and%20Alexandra%20Cordell%20and%20Yuri%20Chervonyi%20and%20Frankie%20Garcia%20and%20Nithish%20Kannen%20and%20Vincent%20Perot%20and%20Nan%20Ding%20and%20Shlomi%20Cohen-Ganor%20and%20Victor%20Lavrenko%20and%20Junru%20Wu%20and%20Georgie%20Evans%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Madhavi%20Sewak%20and%20Ashley%20Brown%20and%20Andrew%20Hard%20and%20Joan%20Puigcerver%20and%20Zeyu%20Zheng%20and%20Yizhong%20Liang%20and%20Evgeny%20Gladchenko%20and%20Reeve%20Ingle%20and%20Uri%20First%20and%20Pierre%20Sermanet%20and%20Charlotte%20Magister%20and%20Mihajlo%20Velimirovi%C4%87%20and%20Sashank%20Reddi%20and%20Susanna%20Ricco%20and%20Eirikur%20Agustsson%20and%20Hartwig%20Adam%20and%20Nir%20Levine%20and%20David%20Gaddy%20and%20Dan%20Holtmann-Rice%20and%20Xuanhui%20Wang%20and%20Ashutosh%20Sathe%20and%20Abhijit%20Guha%20Roy%20and%20Bla%C5%BE%20Bratani%C4%8D%20and%20Alen%20Carin%20and%20Harsh%20Mehta%20and%20Silvano%20Bonacina%20and%20Nicola%20De%20Cao%20and%20Mara%20Finkelstein%20and%20Verena%20Rieser%20and%20Xinyi%20Wu%20and%20Florent%20Altch%C3%A9%20and%20Dylan%20Scandinaro%20and%20Li%20Li%20and%20Nino%20Vieillard%20and%20Nikhil%20Sethi%20and%20Garrett%20Tanzer%20and%20Zhi%20Xing%20and%20Shibo%20Wang%20and%20Parul%20Bhatia%20and%20Gui%20Citovsky%20and%20Thomas%20Anthony%20and%20Sharon%20Lin%20and%20Tianze%20Shi%20and%20Shoshana%20Jakobovits%20and%20Gena%20Gibson%20and%20Raj%20Apte%20and%20Lisa%20Lee%20and%20Mingqing%20Chen%20and%20Arunkumar%20Byravan%20and%20Petros%20Maniatis%20and%20Kellie%20Webster%20and%20Andrew%20Dai%20and%20Pu-Chin%20Chen%20and%20Jiaqi%20Pan%20and%20Asya%20Fadeeva%20and%20Zach%20Gleicher%20and%20Thang%20Luong%20and%20Niket%20Kumar%20Bhumihar%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%202.X%20model%20family%3A%20Gemini%202.5%20Pro%20and%0AGemini%202.5%20Flash%2C%20as%20well%20as%20our%20earlier%20Gemini%202.0%20Flash%20and%20Flash-Lite%0Amodels.%20Gemini%202.5%20Pro%20is%20our%20most%20capable%20model%20yet%2C%20achieving%20SoTA%0Aperformance%20on%20frontier%20coding%20and%20reasoning%20benchmarks.%20In%20addition%20to%20its%0Aincredible%20coding%20and%20reasoning%20skills%2C%20Gemini%202.5%20Pro%20is%20a%20thinking%20model%20that%0Aexcels%20at%20multimodal%20understanding%20and%20it%20is%20now%20able%20to%20process%20up%20to%203%20hours%0Aof%20video%20content.%20Its%20unique%20combination%20of%20long%20context%2C%20multimodal%20and%0Areasoning%20capabilities%20can%20be%20combined%20to%20unlock%20new%20agentic%20workflows.%20Gemini%0A2.5%20Flash%20provides%20excellent%20reasoning%20abilities%20at%20a%20fraction%20of%20the%20compute%0Aand%20latency%20requirements%20and%20Gemini%202.0%20Flash%20and%20Flash-Lite%20provide%20high%0Aperformance%20at%20low%20latency%20and%20cost.%20Taken%20together%2C%20the%20Gemini%202.X%20model%0Ageneration%20spans%20the%20full%20Pareto%20frontier%20of%20model%20capability%20vs%20cost%2C%20allowing%0Ausers%20to%20explore%20the%20boundaries%20of%20what%20is%20possible%20with%20complex%20agentic%0Aproblem%20solving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06261v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemini%25202.5%253A%2520Pushing%2520the%2520Frontier%2520with%2520Advanced%2520Reasoning%252C%2520Multimodality%252C%250A%2520%2520Long%2520Context%252C%2520and%2520Next%2520Generation%2520Agentic%2520Capabilities%26entry.906535625%3DGheorghe%2520Comanici%2520and%2520Eric%2520Bieber%2520and%2520Mike%2520Schaekermann%2520and%2520Ice%2520Pasupat%2520and%2520Noveen%2520Sachdeva%2520and%2520Inderjit%2520Dhillon%2520and%2520Marcel%2520Blistein%2520and%2520Ori%2520Ram%2520and%2520Dan%2520Zhang%2520and%2520Evan%2520Rosen%2520and%2520Luke%2520Marris%2520and%2520Sam%2520Petulla%2520and%2520Colin%2520Gaffney%2520and%2520Asaf%2520Aharoni%2520and%2520Nathan%2520Lintz%2520and%2520Tiago%2520Cardal%2520Pais%2520and%2520Henrik%2520Jacobsson%2520and%2520Idan%2520Szpektor%2520and%2520Nan-Jiang%2520Jiang%2520and%2520Krishna%2520Haridasan%2520and%2520Ahmed%2520Omran%2520and%2520Nikunj%2520Saunshi%2520and%2520Dara%2520Bahri%2520and%2520Gaurav%2520Mishra%2520and%2520Eric%2520Chu%2520and%2520Toby%2520Boyd%2520and%2520Brad%2520Hekman%2520and%2520Aaron%2520Parisi%2520and%2520Chaoyi%2520Zhang%2520and%2520Kornraphop%2520Kawintiranon%2520and%2520Tania%2520Bedrax-Weiss%2520and%2520Oliver%2520Wang%2520and%2520Ya%2520Xu%2520and%2520Ollie%2520Purkiss%2520and%2520Uri%2520Mendlovic%2520and%2520Ila%25C3%25AF%2520Deutel%2520and%2520Nam%2520Nguyen%2520and%2520Adam%2520Langley%2520and%2520Flip%2520Korn%2520and%2520Lucia%2520Rossazza%2520and%2520Alexandre%2520Ram%25C3%25A9%2520and%2520Sagar%2520Waghmare%2520and%2520Helen%2520Miller%2520and%2520Nathan%2520Byrd%2520and%2520Ashrith%2520Sheshan%2520and%2520Raia%2520Hadsell%2520Sangnie%2520Bhardwaj%2520and%2520Pawel%2520Janus%2520and%2520Tero%2520Rissa%2520and%2520Dan%2520Horgan%2520and%2520Sharon%2520Silver%2520and%2520Ayzaan%2520Wahid%2520and%2520Sergey%2520Brin%2520and%2520Yves%2520Raimond%2520and%2520Klemen%2520Kloboves%2520and%2520Cindy%2520Wang%2520and%2520Nitesh%2520Bharadwaj%2520Gundavarapu%2520and%2520Ilia%2520Shumailov%2520and%2520Bo%2520Wang%2520and%2520Mantas%2520Pajarskas%2520and%2520Joe%2520Heyward%2520and%2520Martin%2520Nikoltchev%2520and%2520Maciej%2520Kula%2520and%2520Hao%2520Zhou%2520and%2520Zachary%2520Garrett%2520and%2520Sushant%2520Kafle%2520and%2520Sercan%2520Arik%2520and%2520Ankita%2520Goel%2520and%2520Mingyao%2520Yang%2520and%2520Jiho%2520Park%2520and%2520Koji%2520Kojima%2520and%2520Parsa%2520Mahmoudieh%2520and%2520Koray%2520Kavukcuoglu%2520and%2520Grace%2520Chen%2520and%2520Doug%2520Fritz%2520and%2520Anton%2520Bulyenov%2520and%2520Sudeshna%2520Roy%2520and%2520Dimitris%2520Paparas%2520and%2520Hadar%2520Shemtov%2520and%2520Bo-Juen%2520Chen%2520and%2520Robin%2520Strudel%2520and%2520David%2520Reitter%2520and%2520Aurko%2520Roy%2520and%2520Andrey%2520Vlasov%2520and%2520Changwan%2520Ryu%2520and%2520Chas%2520Leichner%2520and%2520Haichuan%2520Yang%2520and%2520Zelda%2520Mariet%2520and%2520Denis%2520Vnukov%2520and%2520Tim%2520Sohn%2520and%2520Amy%2520Stuart%2520and%2520Wei%2520Liang%2520and%2520Minmin%2520Chen%2520and%2520Praynaa%2520Rawlani%2520and%2520Christy%2520Koh%2520and%2520JD%2520Co-Reyes%2520and%2520Guangda%2520Lai%2520and%2520Praseem%2520Banzal%2520and%2520Dimitrios%2520Vytiniotis%2520and%2520Jieru%2520Mei%2520and%2520Mu%2520Cai%2520and%2520Mohammed%2520Badawi%2520and%2520Corey%2520Fry%2520and%2520Ale%2520Hartman%2520and%2520Daniel%2520Zheng%2520and%2520Eric%2520Jia%2520and%2520James%2520Keeling%2520and%2520Annie%2520Louis%2520and%2520Ying%2520Chen%2520and%2520Efren%2520Robles%2520and%2520Wei-Chih%2520Hung%2520and%2520Howard%2520Zhou%2520and%2520Nikita%2520Saxena%2520and%2520Sonam%2520Goenka%2520and%2520Olivia%2520Ma%2520and%2520Zach%2520Fisher%2520and%2520Mor%2520Hazan%2520Taege%2520and%2520Emily%2520Graves%2520and%2520David%2520Steiner%2520and%2520Yujia%2520Li%2520and%2520Sarah%2520Nguyen%2520and%2520Rahul%2520Sukthankar%2520and%2520Joe%2520Stanton%2520and%2520Ali%2520Eslami%2520and%2520Gloria%2520Shen%2520and%2520Berkin%2520Akin%2520and%2520Alexey%2520Guseynov%2520and%2520Yiqian%2520Zhou%2520and%2520Jean-Baptiste%2520Alayrac%2520and%2520Armand%2520Joulin%2520and%2520Efrat%2520Farkash%2520and%2520Ashish%2520Thapliyal%2520and%2520Stephen%2520Roller%2520and%2520Noam%2520Shazeer%2520and%2520Todor%2520Davchev%2520and%2520Terry%2520Koo%2520and%2520Hannah%2520Forbes-Pollard%2520and%2520Kartik%2520Audhkhasi%2520and%2520Greg%2520Farquhar%2520and%2520Adi%2520Mayrav%2520Gilady%2520and%2520Maggie%2520Song%2520and%2520John%2520Aslanides%2520and%2520Piermaria%2520Mendolicchio%2520and%2520Alicia%2520Parrish%2520and%2520John%2520Blitzer%2520and%2520Pramod%2520Gupta%2520and%2520Xiaoen%2520Ju%2520and%2520Xiaochen%2520Yang%2520and%2520Puranjay%2520Datta%2520and%2520Andrea%2520Tacchetti%2520and%2520Sanket%2520Vaibhav%2520Mehta%2520and%2520Gregory%2520Dibb%2520and%2520Shubham%2520Gupta%2520and%2520Federico%2520Piccinini%2520and%2520Raia%2520Hadsell%2520and%2520Sujee%2520Rajayogam%2520and%2520Jiepu%2520Jiang%2520and%2520Patrick%2520Griffin%2520and%2520Patrik%2520Sundberg%2520and%2520Jamie%2520Hayes%2520and%2520Alexey%2520Frolov%2520and%2520Tian%2520Xie%2520and%2520Adam%2520Zhang%2520and%2520Kingshuk%2520Dasgupta%2520and%2520Uday%2520Kalra%2520and%2520Lior%2520Shani%2520and%2520Klaus%2520Macherey%2520and%2520Tzu-Kuo%2520Huang%2520and%2520Liam%2520MacDermed%2520and%2520Karthik%2520Duddu%2520and%2520Paulo%2520Zacchello%2520and%2520Zi%2520Yang%2520and%2520Jessica%2520Lo%2520and%2520Kai%2520Hui%2520and%2520Matej%2520Kastelic%2520and%2520Derek%2520Gasaway%2520and%2520Qijun%2520Tan%2520and%2520Summer%2520Yue%2520and%2520Pablo%2520Barrio%2520and%2520John%2520Wieting%2520and%2520Weel%2520Yang%2520and%2520Andrew%2520Nystrom%2520and%2520Solomon%2520Demmessie%2520and%2520Anselm%2520Levskaya%2520and%2520Fabio%2520Viola%2520and%2520Chetan%2520Tekur%2520and%2520Greg%2520Billock%2520and%2520George%2520Necula%2520and%2520Mandar%2520Joshi%2520and%2520Rylan%2520Schaeffer%2520and%2520Swachhand%2520Lokhande%2520and%2520Christina%2520Sorokin%2520and%2520Pradeep%2520Shenoy%2520and%2520Mia%2520Chen%2520and%2520Mark%2520Collier%2520and%2520Hongji%2520Li%2520and%2520Taylor%2520Bos%2520and%2520Nevan%2520Wichers%2520and%2520Sun%2520Jae%2520Lee%2520and%2520Ang%25C3%25A9line%2520Pouget%2520and%2520Santhosh%2520Thangaraj%2520and%2520Kyriakos%2520Axiotis%2520and%2520Phil%2520Crone%2520and%2520Rachel%2520Sterneck%2520and%2520Nikolai%2520Chinaev%2520and%2520Victoria%2520Krakovna%2520and%2520Oleksandr%2520Ferludin%2520and%2520Ian%2520Gemp%2520and%2520Stephanie%2520Winkler%2520and%2520Dan%2520Goldberg%2520and%2520Ivan%2520Korotkov%2520and%2520Kefan%2520Xiao%2520and%2520Malika%2520Mehrotra%2520and%2520Sandeep%2520Mariserla%2520and%2520Vihari%2520Piratla%2520and%2520Terry%2520Thurk%2520and%2520Khiem%2520Pham%2520and%2520Hongxu%2520Ma%2520and%2520Alexandre%2520Senges%2520and%2520Ravi%2520Kumar%2520and%2520Clemens%2520Meyer%2520and%2520Ellie%2520Talius%2520and%2520Nuo%2520Wang%2520Pierse%2520and%2520Ballie%2520Sandhu%2520and%2520Horia%2520Toma%2520and%2520Kuo%2520Lin%2520and%2520Swaroop%2520Nath%2520and%2520Tom%2520Stone%2520and%2520Dorsa%2520Sadigh%2520and%2520Nikita%2520Gupta%2520and%2520Arthur%2520Guez%2520and%2520Avi%2520Singh%2520and%2520Matt%2520Thomas%2520and%2520Tom%2520Duerig%2520and%2520Yuan%2520Gong%2520and%2520Richard%2520Tanburn%2520and%2520Lydia%2520Lihui%2520Zhang%2520and%2520Phuong%2520Dao%2520and%2520Mohamed%2520Hammad%2520and%2520Sirui%2520Xie%2520and%2520Shruti%2520Rijhwani%2520and%2520Ben%2520Murdoch%2520and%2520Duhyeon%2520Kim%2520and%2520Will%2520Thompson%2520and%2520Heng-Tze%2520Cheng%2520and%2520Daniel%2520Sohn%2520and%2520Pablo%2520Sprechmann%2520and%2520Qiantong%2520Xu%2520and%2520Srinivas%2520Tadepalli%2520and%2520Peter%2520Young%2520and%2520Ye%2520Zhang%2520and%2520Hansa%2520Srinivasan%2520and%2520Miranda%2520Aperghis%2520and%2520Aditya%2520Ayyar%2520and%2520Hen%2520Fitoussi%2520and%2520Ryan%2520Burnell%2520and%2520David%2520Madras%2520and%2520Mike%2520Dusenberry%2520and%2520Xi%2520Xiong%2520and%2520Tayo%2520Oguntebi%2520and%2520Ben%2520Albrecht%2520and%2520J%25C3%25B6rg%2520Bornschein%2520and%2520Jovana%2520Mitrovi%25C4%2587%2520and%2520Mason%2520Dimarco%2520and%2520Bhargav%2520Kanagal%2520Shamanna%2520and%2520Premal%2520Shah%2520and%2520Eren%2520Sezener%2520and%2520Shyam%2520Upadhyay%2520and%2520Dave%2520Lacey%2520and%2520Craig%2520Schiff%2520and%2520Sebastien%2520Baur%2520and%2520Sanjay%2520Ganapathy%2520and%2520Eva%2520Schnider%2520and%2520Mateo%2520Wirth%2520and%2520Connor%2520Schenck%2520and%2520Andrey%2520Simanovsky%2520and%2520Yi-Xuan%2520Tan%2520and%2520Philipp%2520Fr%25C3%25A4nken%2520and%2520Dennis%2520Duan%2520and%2520Bharath%2520Mankalale%2520and%2520Nikhil%2520Dhawan%2520and%2520Kevin%2520Sequeira%2520and%2520Zichuan%2520Wei%2520and%2520Shivanker%2520Goel%2520and%2520Caglar%2520Unlu%2520and%2520Yukun%2520Zhu%2520and%2520Haitian%2520Sun%2520and%2520Ananth%2520Balashankar%2520and%2520Kurt%2520Shuster%2520and%2520Megh%2520Umekar%2520and%2520Mahmoud%2520Alnahlawi%2520and%2520A%25C3%25A4ron%2520van%2520den%2520Oord%2520and%2520Kelly%2520Chen%2520and%2520Yuexiang%2520Zhai%2520and%2520Zihang%2520Dai%2520and%2520Kuang-Huei%2520Lee%2520and%2520Eric%2520Doi%2520and%2520Lukas%2520Zilka%2520and%2520Rohith%2520Vallu%2520and%2520Disha%2520Shrivastava%2520and%2520Jason%2520Lee%2520and%2520Hisham%2520Husain%2520and%2520Honglei%2520Zhuang%2520and%2520Vincent%2520Cohen-Addad%2520and%2520Jarred%2520Barber%2520and%2520James%2520Atwood%2520and%2520Adam%2520Sadovsky%2520and%2520Quentin%2520Wellens%2520and%2520Steven%2520Hand%2520and%2520Arunkumar%2520Rajendran%2520and%2520Aybuke%2520Turker%2520and%2520CJ%2520Carey%2520and%2520Yuanzhong%2520Xu%2520and%2520Hagen%2520Soltau%2520and%2520Zefei%2520Li%2520and%2520Xinying%2520Song%2520and%2520Conglong%2520Li%2520and%2520Iurii%2520Kemaev%2520and%2520Sasha%2520Brown%2520and%2520Andrea%2520Burns%2520and%2520Viorica%2520Patraucean%2520and%2520Piotr%2520Stanczyk%2520and%2520Renga%2520Aravamudhan%2520and%2520Mathieu%2520Blondel%2520and%2520Hila%2520Noga%2520and%2520Lorenzo%2520Blanco%2520and%2520Will%2520Song%2520and%2520Michael%2520Isard%2520and%2520Mandar%2520Sharma%2520and%2520Reid%2520Hayes%2520and%2520Dalia%2520El%2520Badawy%2520and%2520Avery%2520Lamp%2520and%2520Itay%2520Laish%2520and%2520Olga%2520Kozlova%2520and%2520Kelvin%2520Chan%2520and%2520Sahil%2520Singla%2520and%2520Srinivas%2520Sunkara%2520and%2520Mayank%2520Upadhyay%2520and%2520Chang%2520Liu%2520and%2520Aijun%2520Bai%2520and%2520Jarek%2520Wilkiewicz%2520and%2520Martin%2520Zlocha%2520and%2520Jeremiah%2520Liu%2520and%2520Zhuowan%2520Li%2520and%2520Haiguang%2520Li%2520and%2520Omer%2520Barak%2520and%2520Ganna%2520Raboshchuk%2520and%2520Jiho%2520Choi%2520and%2520Fangyu%2520Liu%2520and%2520Erik%2520Jue%2520and%2520Mohit%2520Sharma%2520and%2520Andreea%2520Marzoca%2520and%2520Robert%2520Busa-Fekete%2520and%2520Anna%2520Korsun%2520and%2520Andre%2520Elisseeff%2520and%2520Zhe%2520Shen%2520and%2520Sara%2520Mc%2520Carthy%2520and%2520Kay%2520Lamerigts%2520and%2520Anahita%2520Hosseini%2520and%2520Hanzhao%2520Lin%2520and%2520Charlie%2520Chen%2520and%2520Fan%2520Yang%2520and%2520Kushal%2520Chauhan%2520and%2520Mark%2520Omernick%2520and%2520Dawei%2520Jia%2520and%2520Karina%2520Zainullina%2520and%2520Demis%2520Hassabis%2520and%2520Danny%2520Vainstein%2520and%2520Ehsan%2520Amid%2520and%2520Xiang%2520Zhou%2520and%2520Ronny%2520Votel%2520and%2520Eszter%2520V%25C3%25A9rtes%2520and%2520Xinjian%2520Li%2520and%2520Zongwei%2520Zhou%2520and%2520Angeliki%2520Lazaridou%2520and%2520Brendan%2520McMahan%2520and%2520Arjun%2520Narayanan%2520and%2520Hubert%2520Soyer%2520and%2520Sujoy%2520Basu%2520and%2520Kayi%2520Lee%2520and%2520Bryan%2520Perozzi%2520and%2520Qin%2520Cao%2520and%2520Leonard%2520Berrada%2520and%2520Rahul%2520Arya%2520and%2520Ke%2520Chen%2520and%2520%2520Katrina%2520and%2520%2520Xu%2520and%2520Matthias%2520Lochbrunner%2520and%2520Alex%2520Hofer%2520and%2520Sahand%2520Sharifzadeh%2520and%2520Renjie%2520Wu%2520and%2520Sally%2520Goldman%2520and%2520Pranjal%2520Awasthi%2520and%2520Xuezhi%2520Wang%2520and%2520Yan%2520Wu%2520and%2520Claire%2520Sha%2520and%2520Biao%2520Zhang%2520and%2520Maciej%2520Miku%25C5%2582a%2520and%2520Filippo%2520Graziano%2520and%2520Siobhan%2520Mcloughlin%2520and%2520Irene%2520Giannoumis%2520and%2520Youhei%2520Namiki%2520and%2520Chase%2520Malik%2520and%2520Carey%2520Radebaugh%2520and%2520Jamie%2520Hall%2520and%2520Ramiro%2520Leal-Cavazos%2520and%2520Jianmin%2520Chen%2520and%2520Vikas%2520Sindhwani%2520and%2520David%2520Kao%2520and%2520David%2520Greene%2520and%2520Jordan%2520Griffith%2520and%2520Chris%2520Welty%2520and%2520Ceslee%2520Montgomery%2520and%2520Toshihiro%2520Yoshino%2520and%2520Liangzhe%2520Yuan%2520and%2520Noah%2520Goodman%2520and%2520Assaf%2520Hurwitz%2520Michaely%2520and%2520Kevin%2520Lee%2520and%2520KP%2520Sawhney%2520and%2520Wei%2520Chen%2520and%2520Zheng%2520Zheng%2520and%2520Megan%2520Shum%2520and%2520Nikolay%2520Savinov%2520and%2520Etienne%2520Pot%2520and%2520Alex%2520Pak%2520and%2520Morteza%2520Zadimoghaddam%2520and%2520Sijal%2520Bhatnagar%2520and%2520Yoad%2520Lewenberg%2520and%2520Blair%2520Kutzman%2520and%2520Ji%2520Liu%2520and%2520Lesley%2520Katzen%2520and%2520Jeremy%2520Selier%2520and%2520Josip%2520Djolonga%2520and%2520Dmitry%2520Lepikhin%2520and%2520Kelvin%2520Xu%2520and%2520Jacky%2520Liang%2520and%2520Jiewen%2520Tan%2520and%2520Benoit%2520Schillings%2520and%2520Muge%2520Ersoy%2520and%2520Pete%2520Blois%2520and%2520Bernd%2520Bandemer%2520and%2520Abhimanyu%2520Singh%2520and%2520Sergei%2520Lebedev%2520and%2520Pankaj%2520Joshi%2520and%2520Adam%2520R.%2520Brown%2520and%2520Evan%2520Palmer%2520and%2520Shreya%2520Pathak%2520and%2520Komal%2520Jalan%2520and%2520Fedir%2520Zubach%2520and%2520Shuba%2520Lall%2520and%2520Randall%2520Parker%2520and%2520Alok%2520Gunjan%2520and%2520Sergey%2520Rogulenko%2520and%2520Sumit%2520Sanghai%2520and%2520Zhaoqi%2520Leng%2520and%2520Zoltan%2520Egyed%2520and%2520Shixin%2520Li%2520and%2520Maria%2520Ivanova%2520and%2520Kostas%2520Andriopoulos%2520and%2520Jin%2520Xie%2520and%2520Elan%2520Rosenfeld%2520and%2520Auriel%2520Wright%2520and%2520Ankur%2520Sharma%2520and%2520Xinyang%2520Geng%2520and%2520Yicheng%2520Wang%2520and%2520Sam%2520Kwei%2520and%2520Renke%2520Pan%2520and%2520Yujing%2520Zhang%2520and%2520Gabby%2520Wang%2520and%2520Xi%2520Liu%2520and%2520Chak%2520Yeung%2520and%2520Elizabeth%2520Cole%2520and%2520Aviv%2520Rosenberg%2520and%2520Zhen%2520Yang%2520and%2520Phil%2520Chen%2520and%2520George%2520Polovets%2520and%2520Pranav%2520Nair%2520and%2520Rohun%2520Saxena%2520and%2520Josh%2520Smith%2520and%2520Shuo-yiin%2520Chang%2520and%2520Aroma%2520Mahendru%2520and%2520Svetlana%2520Grant%2520and%2520Anand%2520Iyer%2520and%2520Irene%2520Cai%2520and%2520Jed%2520McGiffin%2520and%2520Jiaming%2520Shen%2520and%2520Alanna%2520Walton%2520and%2520Antonious%2520Girgis%2520and%2520Oliver%2520Woodman%2520and%2520Rosemary%2520Ke%2520and%2520Mike%2520Kwong%2520and%2520Louis%2520Rouillard%2520and%2520Jinmeng%2520Rao%2520and%2520Zhihao%2520Li%2520and%2520Yuntao%2520Xu%2520and%2520Flavien%2520Prost%2520and%2520Chi%2520Zou%2520and%2520Ziwei%2520Ji%2520and%2520Alberto%2520Magni%2520and%2520Tyler%2520Liechty%2520and%2520Dan%2520A.%2520Calian%2520and%2520Deepak%2520Ramachandran%2520and%2520Igor%2520Krivokon%2520and%2520Hui%2520Huang%2520and%2520Terry%2520Chen%2520and%2520Anja%2520Hauth%2520and%2520Anastasija%2520Ili%25C4%2587%2520and%2520Weijuan%2520Xi%2520and%2520Hyeontaek%2520Lim%2520and%2520Vlad-Doru%2520Ion%2520and%2520Pooya%2520Moradi%2520and%2520Metin%2520Toksoz-Exley%2520and%2520Kalesha%2520Bullard%2520and%2520Miltos%2520Allamanis%2520and%2520Xiaomeng%2520Yang%2520and%2520Sophie%2520Wang%2520and%2520Zhi%2520Hong%2520and%2520Anita%2520Gergely%2520and%2520Cheng%2520Li%2520and%2520Bhavishya%2520Mittal%2520and%2520Vitaly%2520Kovalev%2520and%2520Victor%2520Ungureanu%2520and%2520Jane%2520Labanowski%2520and%2520Jan%2520Wassenberg%2520and%2520Nicolas%2520Lacasse%2520and%2520Geoffrey%2520Cideron%2520and%2520Petar%2520Devi%25C4%2587%2520and%2520Annie%2520Marsden%2520and%2520Lynn%2520Nguyen%2520and%2520Michael%2520Fink%2520and%2520Yin%2520Zhong%2520and%2520Tatsuya%2520Kiyono%2520and%2520Desi%2520Ivanov%2520and%2520Sally%2520Ma%2520and%2520Max%2520Bain%2520and%2520Kiran%2520Yalasangi%2520and%2520Jennifer%2520She%2520and%2520Anastasia%2520Petrushkina%2520and%2520Mayank%2520Lunayach%2520and%2520Carla%2520Bromberg%2520and%2520Sarah%2520Hodkinson%2520and%2520Vilobh%2520Meshram%2520and%2520Daniel%2520Vlasic%2520and%2520Austin%2520Kyker%2520and%2520Steve%2520Xu%2520and%2520Jeff%2520Stanway%2520and%2520Zuguang%2520Yang%2520and%2520Kai%2520Zhao%2520and%2520Matthew%2520Tung%2520and%2520Seth%2520Odoom%2520and%2520Yasuhisa%2520Fujii%2520and%2520Justin%2520Gilmer%2520and%2520Eunyoung%2520Kim%2520and%2520Felix%2520Halim%2520and%2520Quoc%2520Le%2520and%2520Bernd%2520Bohnet%2520and%2520Seliem%2520El-Sayed%2520and%2520Behnam%2520Neyshabur%2520and%2520Malcolm%2520Reynolds%2520and%2520Dean%2520Reich%2520and%2520Yang%2520Xu%2520and%2520Erica%2520Moreira%2520and%2520Anuj%2520Sharma%2520and%2520Zeyu%2520Liu%2520and%2520Mohammad%2520Javad%2520Hosseini%2520and%2520Naina%2520Raisinghani%2520and%2520Yi%2520Su%2520and%2520Ni%2520Lao%2520and%2520Daniel%2520Formoso%2520and%2520Marco%2520Gelmi%2520and%2520Almog%2520Gueta%2520and%2520Tapomay%2520Dey%2520and%2520Elena%2520Gribovskaya%2520and%2520Domagoj%2520%25C4%2586evid%2520and%2520Sidharth%2520Mudgal%2520and%2520Garrett%2520Bingham%2520and%2520Jianling%2520Wang%2520and%2520Anurag%2520Kumar%2520and%2520Alex%2520Cullum%2520and%2520Feng%2520Han%2520and%2520Konstantinos%2520Bousmalis%2520and%2520Diego%2520Cedillo%2520and%2520Grace%2520Chu%2520and%2520Vladimir%2520Magay%2520and%2520Paul%2520Michel%2520and%2520Ester%2520Hlavnova%2520and%2520Daniele%2520Calandriello%2520and%2520Setareh%2520Ariafar%2520and%2520Kaisheng%2520Yao%2520and%2520Vikash%2520Sehwag%2520and%2520Arpi%2520Vezer%2520and%2520Agustin%2520Dal%2520Lago%2520and%2520Zhenkai%2520Zhu%2520and%2520Paul%2520Kishan%2520Rubenstein%2520and%2520Allen%2520Porter%2520and%2520Anirudh%2520Baddepudi%2520and%2520Oriana%2520Riva%2520and%2520Mihai%2520Dorin%2520Istin%2520and%2520Chih-Kuan%2520Yeh%2520and%2520Zhi%2520Li%2520and%2520Andrew%2520Howard%2520and%2520Nilpa%2520Jha%2520and%2520Jeremy%2520Chen%2520and%2520Raoul%2520de%2520Liedekerke%2520and%2520Zafarali%2520Ahmed%2520and%2520Mikel%2520Rodriguez%2520and%2520Tanuj%2520Bhatia%2520and%2520Bangju%2520Wang%2520and%2520Ali%2520Elqursh%2520and%2520David%2520Klinghoffer%2520and%2520Peter%2520Chen%2520and%2520Pushmeet%2520Kohli%2520and%2520Te%2520I%2520and%2520Weiyang%2520Zhang%2520and%2520Zack%2520Nado%2520and%2520Jilin%2520Chen%2520and%2520Maxwell%2520Chen%2520and%2520George%2520Zhang%2520and%2520Aayush%2520Singh%2520and%2520Adam%2520Hillier%2520and%2520Federico%2520Lebron%2520and%2520Yiqing%2520Tao%2520and%2520Ting%2520Liu%2520and%2520Gabriel%2520Dulac-Arnold%2520and%2520Jingwei%2520Zhang%2520and%2520Shashi%2520Narayan%2520and%2520Buhuang%2520Liu%2520and%2520Orhan%2520Firat%2520and%2520Abhishek%2520Bhowmick%2520and%2520Bingyuan%2520Liu%2520and%2520Hao%2520Zhang%2520and%2520Zizhao%2520Zhang%2520and%2520Georges%2520Rotival%2520and%2520Nathan%2520Howard%2520and%2520Anu%2520Sinha%2520and%2520Alexander%2520Grushetsky%2520and%2520Benjamin%2520Beyret%2520and%2520Keerthana%2520Gopalakrishnan%2520and%2520James%2520Zhao%2520and%2520Kyle%2520He%2520and%2520Szabolcs%2520Payrits%2520and%2520Zaid%2520Nabulsi%2520and%2520Zhaoyi%2520Zhang%2520and%2520Weijie%2520Chen%2520and%2520Edward%2520Lee%2520and%2520Nova%2520Fallen%2520and%2520Sreenivas%2520Gollapudi%2520and%2520Aurick%2520Zhou%2520and%2520Filip%2520Paveti%25C4%2587%2520and%2520Thomas%2520K%25C3%25B6ppe%2520and%2520Shiyu%2520Huang%2520and%2520Rama%2520Pasumarthi%2520and%2520Nick%2520Fernando%2520and%2520Felix%2520Fischer%2520and%2520Daria%2520%25C4%2586urko%2520and%2520Yang%2520Gao%2520and%2520James%2520Svensson%2520and%2520Austin%2520Stone%2520and%2520Haroon%2520Qureshi%2520and%2520Abhishek%2520Sinha%2520and%2520Apoorv%2520Kulshreshtha%2520and%2520Martin%2520Matysiak%2520and%2520Jieming%2520Mao%2520and%2520Carl%2520Saroufim%2520and%2520Aleksandra%2520Faust%2520and%2520Qingnan%2520Duan%2520and%2520Gil%2520Fidel%2520and%2520Kaan%2520Katircioglu%2520and%2520Rapha%25C3%25ABl%2520Lopez%2520Kaufman%2520and%2520Dhruv%2520Shah%2520and%2520Weize%2520Kong%2520and%2520Abhishek%2520Bapna%2520and%2520Gell%25C3%25A9rt%2520Weisz%2520and%2520Emma%2520Dunleavy%2520and%2520Praneet%2520Dutta%2520and%2520Tianqi%2520Liu%2520and%2520Rahma%2520Chaabouni%2520and%2520Carolina%2520Parada%2520and%2520Marcus%2520Wu%2520and%2520Alexandra%2520Belias%2520and%2520Alessandro%2520Bissacco%2520and%2520Stanislav%2520Fort%2520and%2520Li%2520Xiao%2520and%2520Fantine%2520Huot%2520and%2520Chris%2520Knutsen%2520and%2520Yochai%2520Blau%2520and%2520Gang%2520Li%2520and%2520Jennifer%2520Prendki%2520and%2520Juliette%2520Love%2520and%2520Yinlam%2520Chow%2520and%2520Pichi%2520Charoenpanit%2520and%2520Hidetoshi%2520Shimokawa%2520and%2520Vincent%2520Coriou%2520and%2520Karol%2520Gregor%2520and%2520Tomas%2520Izo%2520and%2520Arjun%2520Akula%2520and%2520Mario%2520Pinto%2520and%2520Chris%2520Hahn%2520and%2520Dominik%2520Paulus%2520and%2520Jiaxian%2520Guo%2520and%2520Neha%2520Sharma%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Adaeze%2520Chukwuka%2520and%2520Kazuma%2520Hashimoto%2520and%2520Nathalie%2520Rauschmayr%2520and%2520Ling%2520Wu%2520and%2520Christof%2520Angermueller%2520and%2520Yulong%2520Wang%2520and%2520Sebastian%2520Gerlach%2520and%2520Michael%2520Pliskin%2520and%2520Daniil%2520Mirylenka%2520and%2520Min%2520Ma%2520and%2520Lexi%2520Baugher%2520and%2520Bryan%2520Gale%2520and%2520Shaan%2520Bijwadia%2520and%2520Nemanja%2520Raki%25C4%2587evi%25C4%2587%2520and%2520David%2520Wood%2520and%2520Jane%2520Park%2520and%2520Chung-Ching%2520Chang%2520and%2520Babi%2520Seal%2520and%2520Chris%2520Tar%2520and%2520Kacper%2520Krasowiak%2520and%2520Yiwen%2520Song%2520and%2520Georgi%2520Stephanov%2520and%2520Gary%2520Wang%2520and%2520Marcello%2520Maggioni%2520and%2520Stein%2520Xudong%2520Lin%2520and%2520Felix%2520Wu%2520and%2520Shachi%2520Paul%2520and%2520Zixuan%2520Jiang%2520and%2520Shubham%2520Agrawal%2520and%2520Bilal%2520Piot%2520and%2520Alex%2520Feng%2520and%2520Cheolmin%2520Kim%2520and%2520Tulsee%2520Doshi%2520and%2520Jonathan%2520Lai%2520and%2520%2520Chuqiao%2520and%2520%2520Xu%2520and%2520Sharad%2520Vikram%2520and%2520Ciprian%2520Chelba%2520and%2520Sebastian%2520Krause%2520and%2520Vincent%2520Zhuang%2520and%2520Jack%2520Rae%2520and%2520Timo%2520Denk%2520and%2520Adrian%2520Collister%2520and%2520Lotte%2520Weerts%2520and%2520Xianghong%2520Luo%2520and%2520Yifeng%2520Lu%2520and%2520H%25C3%25A5vard%2520Garnes%2520and%2520Nitish%2520Gupta%2520and%2520Terry%2520Spitz%2520and%2520Avinatan%2520Hassidim%2520and%2520Lihao%2520Liang%2520and%2520Izhak%2520Shafran%2520and%2520Peter%2520Humphreys%2520and%2520Kenny%2520Vassigh%2520and%2520Phil%2520Wallis%2520and%2520Virat%2520Shejwalkar%2520and%2520Nicolas%2520Perez-Nieves%2520and%2520Rachel%2520Hornung%2520and%2520Melissa%2520Tan%2520and%2520Beka%2520Westberg%2520and%2520Andy%2520Ly%2520and%2520Richard%2520Zhang%2520and%2520Brian%2520Farris%2520and%2520Jongbin%2520Park%2520and%2520Alec%2520Kosik%2520and%2520Zeynep%2520Cankara%2520and%2520Andrii%2520Maksai%2520and%2520Yunhan%2520Xu%2520and%2520Albin%2520Cassirer%2520and%2520Sergi%2520Caelles%2520and%2520Abbas%2520Abdolmaleki%2520and%2520Mencher%2520Chiang%2520and%2520Alex%2520Fabrikant%2520and%2520Shravya%2520Shetty%2520and%2520Luheng%2520He%2520and%2520Mai%2520Gim%25C3%25A9nez%2520and%2520Hadi%2520Hashemi%2520and%2520Sheena%2520Panthaplackel%2520and%2520Yana%2520Kulizhskaya%2520and%2520Salil%2520Deshmukh%2520and%2520Daniele%2520Pighin%2520and%2520Robin%2520Alazard%2520and%2520Disha%2520Jindal%2520and%2520Seb%2520Noury%2520and%2520Pradeep%2520Kumar%2520S%2520and%2520Siyang%2520Qin%2520and%2520Xerxes%2520Dotiwalla%2520and%2520Stephen%2520Spencer%2520and%2520Mohammad%2520Babaeizadeh%2520and%2520Blake%2520JianHang%2520Chen%2520and%2520Vaibhav%2520Mehta%2520and%2520Jennie%2520Lees%2520and%2520Andrew%2520Leach%2520and%2520Penporn%2520Koanantakool%2520and%2520Ilia%2520Akolzin%2520and%2520Ramona%2520Comanescu%2520and%2520Junwhan%2520Ahn%2520and%2520Alexey%2520Svyatkovskiy%2520and%2520Basil%2520Mustafa%2520and%2520David%2520D%2527Ambrosio%2520and%2520Shiva%2520Mohan%2520Reddy%2520Garlapati%2520and%2520Pascal%2520Lamblin%2520and%2520Alekh%2520Agarwal%2520and%2520Shuang%2520Song%2520and%2520Pier%2520Giuseppe%2520Sessa%2520and%2520Pauline%2520Coquinot%2520and%2520John%2520Maggs%2520and%2520Hussain%2520Masoom%2520and%2520Divya%2520Pitta%2520and%2520Yaqing%2520Wang%2520and%2520Patrick%2520Morris-Suzuki%2520and%2520Billy%2520Porter%2520and%2520Johnson%2520Jia%2520and%2520Jeffrey%2520Dudek%2520and%2520Raghavender%2520R%2520and%2520Cosmin%2520Paduraru%2520and%2520Alan%2520Ansell%2520and%2520Tolga%2520Bolukbasi%2520and%2520Tony%2520Lu%2520and%2520Ramya%2520Ganeshan%2520and%2520Zi%2520Wang%2520and%2520Henry%2520Griffiths%2520and%2520Rodrigo%2520Benenson%2520and%2520Yifan%2520He%2520and%2520James%2520Swirhun%2520and%2520George%2520Papamakarios%2520and%2520Aditya%2520Chawla%2520and%2520Kuntal%2520Sengupta%2520and%2520Yan%2520Wang%2520and%2520Vedrana%2520Milutinovic%2520and%2520Igor%2520Mordatch%2520and%2520Zhipeng%2520Jia%2520and%2520Jamie%2520Smith%2520and%2520Will%2520Ng%2520and%2520Shitij%2520Nigam%2520and%2520Matt%2520Young%2520and%2520Eugen%2520Vu%25C5%25A1ak%2520and%2520Blake%2520Hechtman%2520and%2520Sheela%2520Goenka%2520and%2520Avital%2520Zipori%2520and%2520Kareem%2520Ayoub%2520and%2520Ashok%2520Popat%2520and%2520Trilok%2520Acharya%2520and%2520Luo%2520Yu%2520and%2520Dawn%2520Bloxwich%2520and%2520Hugo%2520Song%2520and%2520Paul%2520Roit%2520and%2520Haiqiong%2520Li%2520and%2520Aviel%2520Boag%2520and%2520Nigamaa%2520Nayakanti%2520and%2520Bilva%2520Chandra%2520and%2520Tianli%2520Ding%2520and%2520Aahil%2520Mehta%2520and%2520Cath%2520Hope%2520and%2520Jiageng%2520Zhang%2520and%2520Idan%2520Heimlich%2520Shtacher%2520and%2520Kartikeya%2520Badola%2520and%2520Ryo%2520Nakashima%2520and%2520Andrei%2520Sozanschi%2520and%2520Iulia%2520Com%25C5%259Fa%2520and%2520Ante%2520%25C5%25BDu%25C5%25BEul%2520and%2520Emily%2520Caveness%2520and%2520Julian%2520Odell%2520and%2520Matthew%2520Watson%2520and%2520Dario%2520de%2520Cesare%2520and%2520Phillip%2520Lippe%2520and%2520Derek%2520Lockhart%2520and%2520Siddharth%2520Verma%2520and%2520Huizhong%2520Chen%2520and%2520Sean%2520Sun%2520and%2520Lin%2520Zhuo%2520and%2520Aditya%2520Shah%2520and%2520Prakhar%2520Gupta%2520and%2520Alex%2520Muzio%2520and%2520Ning%2520Niu%2520and%2520Amir%2520Zait%2520and%2520Abhinav%2520Singh%2520and%2520Meenu%2520Gaba%2520and%2520Fan%2520Ye%2520and%2520Prajit%2520Ramachandran%2520and%2520Mohammad%2520Saleh%2520and%2520Raluca%2520Ada%2520Popa%2520and%2520Ayush%2520Dubey%2520and%2520Frederick%2520Liu%2520and%2520Sara%2520Javanmardi%2520and%2520Mark%2520Epstein%2520and%2520Ross%2520Hemsley%2520and%2520Richard%2520Green%2520and%2520Nishant%2520Ranka%2520and%2520Eden%2520Cohen%2520and%2520Chuyuan%2520Kelly%2520Fu%2520and%2520Sanjay%2520Ghemawat%2520and%2520Jed%2520Borovik%2520and%2520James%2520Martens%2520and%2520Anthony%2520Chen%2520and%2520Pranav%2520Shyam%2520and%2520Andr%25C3%25A9%2520Susano%2520Pinto%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Alexandru%2520%25C5%25A2ifrea%2520and%2520David%2520Du%2520and%2520Boqing%2520Gong%2520and%2520Ayushi%2520Agarwal%2520and%2520Seungyeon%2520Kim%2520and%2520Christian%2520Frank%2520and%2520Saloni%2520Shah%2520and%2520Xiaodan%2520Song%2520and%2520Zhiwei%2520Deng%2520and%2520Ales%2520Mikhalap%2520and%2520Kleopatra%2520Chatziprimou%2520and%2520Timothy%2520Chung%2520and%2520Toni%2520Creswell%2520and%2520Susan%2520Zhang%2520and%2520Yennie%2520Jun%2520and%2520Carl%2520Lebsack%2520and%2520Will%2520Truong%2520and%2520Slavica%2520Anda%25C4%258Di%25C4%2587%2520and%2520Itay%2520Yona%2520and%2520Marco%2520Fornoni%2520and%2520Rong%2520Rong%2520and%2520Serge%2520Toropov%2520and%2520Afzal%2520Shama%2520Soudagar%2520and%2520Andrew%2520Audibert%2520and%2520Salah%2520Zaiem%2520and%2520Zaheer%2520Abbas%2520and%2520Andrei%2520Rusu%2520and%2520Sahitya%2520Potluri%2520and%2520Shitao%2520Weng%2520and%2520Anastasios%2520Kementsietsidis%2520and%2520Anton%2520Tsitsulin%2520and%2520Daiyi%2520Peng%2520and%2520Natalie%2520Ha%2520and%2520Sanil%2520Jain%2520and%2520Tejasi%2520Latkar%2520and%2520Simeon%2520Ivanov%2520and%2520Cory%2520McLean%2520and%2520Anirudh%2520GP%2520and%2520Rajesh%2520Venkataraman%2520and%2520Canoee%2520Liu%2520and%2520Dilip%2520Krishnan%2520and%2520Joel%2520D%2527sa%2520and%2520Roey%2520Yogev%2520and%2520Paul%2520Collins%2520and%2520Benjamin%2520Lee%2520and%2520Lewis%2520Ho%2520and%2520Carl%2520Doersch%2520and%2520Gal%2520Yona%2520and%2520Shawn%2520Gao%2520and%2520Felipe%2520Tiengo%2520Ferreira%2520and%2520Adnan%2520Ozturel%2520and%2520Hannah%2520Muckenhirn%2520and%2520Ce%2520Zheng%2520and%2520Gargi%2520Balasubramaniam%2520and%2520Mudit%2520Bansal%2520and%2520George%2520van%2520den%2520Driessche%2520and%2520Sivan%2520Eiger%2520and%2520Salem%2520Haykal%2520and%2520Vedant%2520Misra%2520and%2520Abhimanyu%2520Goyal%2520and%2520Danilo%2520Martins%2520and%2520Gary%2520Leung%2520and%2520Jonas%2520Valfridsson%2520and%2520Four%2520Flynn%2520and%2520Will%2520Bishop%2520and%2520Chenxi%2520Pang%2520and%2520Yoni%2520Halpern%2520and%2520Honglin%2520Yu%2520and%2520Lawrence%2520Moore%2520and%2520%2520Yuvein%2520and%2520%2520Zhu%2520and%2520Sridhar%2520Thiagarajan%2520and%2520Yoel%2520Drori%2520and%2520Zhisheng%2520Xiao%2520and%2520Lucio%2520Dery%2520and%2520Rolf%2520Jagerman%2520and%2520Jing%2520Lu%2520and%2520Eric%2520Ge%2520and%2520Vaibhav%2520Aggarwal%2520and%2520Arjun%2520Khare%2520and%2520Vinh%2520Tran%2520and%2520Oded%2520Elyada%2520and%2520Ferran%2520Alet%2520and%2520James%2520Rubin%2520and%2520Ian%2520Chou%2520and%2520David%2520Tian%2520and%2520Libin%2520Bai%2520and%2520Lawrence%2520Chan%2520and%2520Lukasz%2520Lew%2520and%2520Karolis%2520Misiunas%2520and%2520Taylan%2520Bilal%2520and%2520Aniket%2520Ray%2520and%2520Sindhu%2520Raghuram%2520and%2520Alex%2520Castro-Ros%2520and%2520Viral%2520Carpenter%2520and%2520CJ%2520Zheng%2520and%2520Michael%2520Kilgore%2520and%2520Josef%2520Broder%2520and%2520Emily%2520Xue%2520and%2520Praveen%2520Kallakuri%2520and%2520Dheeru%2520Dua%2520and%2520Nancy%2520Yuen%2520and%2520Steve%2520Chien%2520and%2520John%2520Schultz%2520and%2520Saurabh%2520Agrawal%2520and%2520Reut%2520Tsarfaty%2520and%2520Jingcao%2520Hu%2520and%2520Ajay%2520Kannan%2520and%2520Dror%2520Marcus%2520and%2520Nisarg%2520Kothari%2520and%2520Baochen%2520Sun%2520and%2520Ben%2520Horn%2520and%2520Matko%2520Bo%25C5%25A1njak%2520and%2520Ferjad%2520Naeem%2520and%2520Dean%2520Hirsch%2520and%2520Lewis%2520Chiang%2520and%2520Boya%2520Fang%2520and%2520Jie%2520Han%2520and%2520Qifei%2520Wang%2520and%2520Ben%2520Hora%2520and%2520Antoine%2520He%2520and%2520Mario%2520Lu%25C4%258Di%25C4%2587%2520and%2520Beer%2520Changpinyo%2520and%2520Anshuman%2520Tripathi%2520and%2520John%2520Youssef%2520and%2520Chester%2520Kwak%2520and%2520Philippe%2520Schlattner%2520and%2520Cat%2520Graves%2520and%2520R%25C3%25A9mi%2520Leblond%2520and%2520Wenjun%2520Zeng%2520and%2520Anders%2520Andreassen%2520and%2520Gabriel%2520Rasskin%2520and%2520Yue%2520Song%2520and%2520Eddie%2520Cao%2520and%2520Junhyuk%2520Oh%2520and%2520Matt%2520Hoffman%2520and%2520Wojtek%2520Skut%2520and%2520Yichi%2520Zhang%2520and%2520Jon%2520Stritar%2520and%2520Xingyu%2520Cai%2520and%2520Saarthak%2520Khanna%2520and%2520Kathie%2520Wang%2520and%2520Shriya%2520Sharma%2520and%2520Christian%2520Reisswig%2520and%2520Younghoon%2520Jun%2520and%2520Aman%2520Prasad%2520and%2520Tatiana%2520Sholokhova%2520and%2520Preeti%2520Singh%2520and%2520Adi%2520Gerzi%2520Rosenthal%2520and%2520Anian%2520Ruoss%2520and%2520Fran%25C3%25A7oise%2520Beaufays%2520and%2520Sean%2520Kirmani%2520and%2520Dongkai%2520Chen%2520and%2520Johan%2520Schalkwyk%2520and%2520Jonathan%2520Herzig%2520and%2520Been%2520Kim%2520and%2520Josh%2520Jacob%2520and%2520Damien%2520Vincent%2520and%2520Adrian%2520N%2520Reyes%2520and%2520Ivana%2520Balazevic%2520and%2520L%25C3%25A9onard%2520Hussenot%2520and%2520Jon%2520Schneider%2520and%2520Parker%2520Barnes%2520and%2520Luis%2520Castro%2520and%2520Spandana%2520Raj%2520Babbula%2520and%2520Simon%2520Green%2520and%2520Serkan%2520Cabi%2520and%2520Nico%2520Duduta%2520and%2520Danny%2520Driess%2520and%2520Rich%2520Galt%2520and%2520Noam%2520Velan%2520and%2520Junjie%2520Wang%2520and%2520Hongyang%2520Jiao%2520and%2520Matthew%2520Mauger%2520and%2520Du%2520Phan%2520and%2520Miteyan%2520Patel%2520and%2520Vlado%2520Gali%25C4%2587%2520and%2520Jerry%2520Chang%2520and%2520Eyal%2520Marcus%2520and%2520Matt%2520Harvey%2520and%2520Julian%2520Salazar%2520and%2520Elahe%2520Dabir%2520and%2520Suraj%2520Satishkumar%2520Sheth%2520and%2520Amol%2520Mandhane%2520and%2520Hanie%2520Sedghi%2520and%2520Jeremiah%2520Willcock%2520and%2520Amir%2520Zandieh%2520and%2520Shruthi%2520Prabhakara%2520and%2520Aida%2520Amini%2520and%2520Antoine%2520Miech%2520and%2520Victor%2520Stone%2520and%2520Massimo%2520Nicosia%2520and%2520Paul%2520Niemczyk%2520and%2520Ying%2520Xiao%2520and%2520Lucy%2520Kim%2520and%2520S%25C5%2582awek%2520Kwasiborski%2520and%2520Vikas%2520Verma%2520and%2520Ada%2520Maksutaj%2520Oflazer%2520and%2520Christoph%2520Hirnschall%2520and%2520Peter%2520Sung%2520and%2520Lu%2520Liu%2520and%2520Richard%2520Everett%2520and%2520Michiel%2520Bakker%2520and%2520%25C3%2581goston%2520Weisz%2520and%2520Yufei%2520Wang%2520and%2520Vivek%2520Sampathkumar%2520and%2520Uri%2520Shaham%2520and%2520Bibo%2520Xu%2520and%2520Yasemin%2520Altun%2520and%2520Mingqiu%2520Wang%2520and%2520Takaaki%2520Saeki%2520and%2520Guanjie%2520Chen%2520and%2520Emanuel%2520Taropa%2520and%2520Shanthal%2520Vasanth%2520and%2520Sophia%2520Austin%2520and%2520Lu%2520Huang%2520and%2520Goran%2520Petrovic%2520and%2520Qingyun%2520Dou%2520and%2520Daniel%2520Golovin%2520and%2520Grigory%2520Rozhdestvenskiy%2520and%2520Allie%2520Culp%2520and%2520Will%2520Wu%2520and%2520Motoki%2520Sano%2520and%2520Divya%2520Jain%2520and%2520Julia%2520Proskurnia%2520and%2520S%25C3%25A9bastien%2520Cevey%2520and%2520Alejandro%2520Cruzado%2520Ruiz%2520and%2520Piyush%2520Patil%2520and%2520Mahdi%2520Mirzazadeh%2520and%2520Eric%2520Ni%2520and%2520Javier%2520Snaider%2520and%2520Lijie%2520Fan%2520and%2520Alexandre%2520Fr%25C3%25A9chette%2520and%2520AJ%2520Pierigiovanni%2520and%2520Shariq%2520Iqbal%2520and%2520Kenton%2520Lee%2520and%2520Claudio%2520Fantacci%2520and%2520Jinwei%2520Xing%2520and%2520Lisa%2520Wang%2520and%2520Alex%2520Irpan%2520and%2520David%2520Raposo%2520and%2520Yi%2520Luan%2520and%2520Zhuoyuan%2520Chen%2520and%2520Harish%2520Ganapathy%2520and%2520Kevin%2520Hui%2520and%2520Jiazhong%2520Nie%2520and%2520Isabelle%2520Guyon%2520and%2520Heming%2520Ge%2520and%2520Roopali%2520Vij%2520and%2520Hui%2520Zheng%2520and%2520Dayeong%2520Lee%2520and%2520Alfonso%2520Casta%25C3%25B1o%2520and%2520Khuslen%2520Baatarsukh%2520and%2520Gabriel%2520Ibagon%2520and%2520Alexandra%2520Chronopoulou%2520and%2520Nicholas%2520FitzGerald%2520and%2520Shashank%2520Viswanadha%2520and%2520Safeen%2520Huda%2520and%2520Rivka%2520Moroshko%2520and%2520Georgi%2520Stoyanov%2520and%2520Prateek%2520Kolhar%2520and%2520Alain%2520Vaucher%2520and%2520Ishaan%2520Watts%2520and%2520Adhi%2520Kuncoro%2520and%2520Henryk%2520Michalewski%2520and%2520Satish%2520Kambala%2520and%2520Bat-Orgil%2520Batsaikhan%2520and%2520Alek%2520Andreev%2520and%2520Irina%2520Jurenka%2520and%2520Maigo%2520Le%2520and%2520Qihang%2520Chen%2520and%2520Wael%2520Al%2520Jishi%2520and%2520Sarah%2520Chakera%2520and%2520Zhe%2520Chen%2520and%2520Aditya%2520Kini%2520and%2520Vikas%2520Yadav%2520and%2520Aditya%2520Siddhant%2520and%2520Ilia%2520Labzovsky%2520and%2520Balaji%2520Lakshminarayanan%2520and%2520Carrie%2520Grimes%2520Bostock%2520and%2520Pankil%2520Botadra%2520and%2520Ankesh%2520Anand%2520and%2520Colton%2520Bishop%2520and%2520Sam%2520Conway-Rahman%2520and%2520Mohit%2520Agarwal%2520and%2520Yani%2520Donchev%2520and%2520Achintya%2520Singhal%2520and%2520F%25C3%25A9lix%2520de%2520Chaumont%2520Quitry%2520and%2520Natalia%2520Ponomareva%2520and%2520Nishant%2520Agrawal%2520and%2520Bin%2520Ni%2520and%2520Kalpesh%2520Krishna%2520and%2520Masha%2520Samsikova%2520and%2520John%2520Karro%2520and%2520Yilun%2520Du%2520and%2520Tamara%2520von%2520Glehn%2520and%2520Caden%2520Lu%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520Zhen%2520Qin%2520and%2520Tingnan%2520Zhang%2520and%2520Sicheng%2520Li%2520and%2520Divya%2520Tyam%2520and%2520Swaroop%2520Mishra%2520and%2520Wing%2520Lowe%2520and%2520Colin%2520Ji%2520and%2520Weiyi%2520Wang%2520and%2520Manaal%2520Faruqui%2520and%2520Ambrose%2520Slone%2520and%2520Valentin%2520Dalibard%2520and%2520Arunachalam%2520Narayanaswamy%2520and%2520John%2520Lambert%2520and%2520Pierre-Antoine%2520Manzagol%2520and%2520Dan%2520Karliner%2520and%2520Andrew%2520Bolt%2520and%2520Ivan%2520Lobov%2520and%2520Aditya%2520Kusupati%2520and%2520Chang%2520Ye%2520and%2520Xuan%2520Yang%2520and%2520Heiga%2520Zen%2520and%2520Nelson%2520George%2520and%2520Mukul%2520Bhutani%2520and%2520Olivier%2520Lacombe%2520and%2520Robert%2520Riachi%2520and%2520Gagan%2520Bansal%2520and%2520Rachel%2520Soh%2520and%2520Yue%2520Gao%2520and%2520Yang%2520Yu%2520and%2520Adams%2520Yu%2520and%2520Emily%2520Nottage%2520and%2520Tania%2520Rojas-Esponda%2520and%2520James%2520Noraky%2520and%2520Manish%2520Gupta%2520and%2520Ragha%2520Kotikalapudi%2520and%2520Jichuan%2520Chang%2520and%2520Sanja%2520Deur%2520and%2520Dan%2520Graur%2520and%2520Alex%2520Mossin%2520and%2520Erin%2520Farnese%2520and%2520Ricardo%2520Figueira%2520and%2520Alexandre%2520Moufarek%2520and%2520Austin%2520Huang%2520and%2520Patrik%2520Zochbauer%2520and%2520Ben%2520Ingram%2520and%2520Tongzhou%2520Chen%2520and%2520Zelin%2520Wu%2520and%2520Adri%25C3%25A0%2520Puigdom%25C3%25A8nech%2520and%2520Leland%2520Rechis%2520and%2520Da%2520Yu%2520and%2520Sri%2520Gayatri%2520Sundara%2520Padmanabhan%2520and%2520Rui%2520Zhu%2520and%2520Chu-ling%2520Ko%2520and%2520Andrea%2520Banino%2520and%2520Samira%2520Daruki%2520and%2520Aarush%2520Selvan%2520and%2520Dhruva%2520Bhaswar%2520and%2520Daniel%2520Hernandez%2520Diaz%2520and%2520Chen%2520Su%2520and%2520Salvatore%2520Scellato%2520and%2520Jennifer%2520Brennan%2520and%2520Woohyun%2520Han%2520and%2520Grace%2520Chung%2520and%2520Priyanka%2520Agrawal%2520and%2520Urvashi%2520Khandelwal%2520and%2520Khe%2520Chai%2520Sim%2520and%2520Morgane%2520Lustman%2520and%2520Sam%2520Ritter%2520and%2520Kelvin%2520Guu%2520and%2520Jiawei%2520Xia%2520and%2520Prateek%2520Jain%2520and%2520Emma%2520Wang%2520and%2520Tyrone%2520Hill%2520and%2520Mirko%2520Rossini%2520and%2520Marija%2520Kostelac%2520and%2520Tautvydas%2520Misiunas%2520and%2520Amit%2520Sabne%2520and%2520Kyuyeun%2520Kim%2520and%2520Ahmet%2520Iscen%2520and%2520Congchao%2520Wang%2520and%2520Jos%25C3%25A9%2520Leal%2520and%2520Ashwin%2520Sreevatsa%2520and%2520Utku%2520Evci%2520and%2520Manfred%2520Warmuth%2520and%2520Saket%2520Joshi%2520and%2520Daniel%2520Suo%2520and%2520James%2520Lottes%2520and%2520Garrett%2520Honke%2520and%2520Brendan%2520Jou%2520and%2520Stefani%2520Karp%2520and%2520Jieru%2520Hu%2520and%2520Himanshu%2520Sahni%2520and%2520Adrien%2520Ali%2520Ta%25C3%25AFga%2520and%2520William%2520Kong%2520and%2520Samrat%2520Ghosh%2520and%2520Renshen%2520Wang%2520and%2520Jay%2520Pavagadhi%2520and%2520Natalie%2520Axelsson%2520and%2520Nikolai%2520Grigorev%2520and%2520Patrick%2520Siegler%2520and%2520Rebecca%2520Lin%2520and%2520Guohui%2520Wang%2520and%2520Emilio%2520Parisotto%2520and%2520Sharath%2520Maddineni%2520and%2520Krishan%2520Subudhi%2520and%2520Eyal%2520Ben-David%2520and%2520Elena%2520Pochernina%2520and%2520Orgad%2520Keller%2520and%2520Thi%2520Avrahami%2520and%2520Zhe%2520Yuan%2520and%2520Pulkit%2520Mehta%2520and%2520Jialu%2520Liu%2520and%2520Sherry%2520Yang%2520and%2520Wendy%2520Kan%2520and%2520Katherine%2520Lee%2520and%2520Tom%2520Funkhouser%2520and%2520Derek%2520Cheng%2520and%2520Hongzhi%2520Shi%2520and%2520Archit%2520Sharma%2520and%2520Joe%2520Kelley%2520and%2520Matan%2520Eyal%2520and%2520Yury%2520Malkov%2520and%2520Corentin%2520Tallec%2520and%2520Yuval%2520Bahat%2520and%2520Shen%2520Yan%2520and%2520%2520Xintian%2520and%2520%2520Wu%2520and%2520David%2520Lindner%2520and%2520Chengda%2520Wu%2520and%2520Avi%2520Caciularu%2520and%2520Xiyang%2520Luo%2520and%2520Rodolphe%2520Jenatton%2520and%2520Tim%2520Zaman%2520and%2520Yingying%2520Bi%2520and%2520Ilya%2520Kornakov%2520and%2520Ganesh%2520Mallya%2520and%2520Daisuke%2520Ikeda%2520and%2520Itay%2520Karo%2520and%2520Anima%2520Singh%2520and%2520Colin%2520Evans%2520and%2520Praneeth%2520Netrapalli%2520and%2520Vincent%2520Nallatamby%2520and%2520Isaac%2520Tian%2520and%2520Yannis%2520Assael%2520and%2520Vikas%2520Raunak%2520and%2520Victor%2520Carbune%2520and%2520Ioana%2520Bica%2520and%2520Lior%2520Madmoni%2520and%2520Dee%2520Cattle%2520and%2520Snchit%2520Grover%2520and%2520Krishna%2520Somandepalli%2520and%2520Sid%2520Lall%2520and%2520Amelio%2520V%25C3%25A1zquez-Reina%2520and%2520Riccardo%2520Patana%2520and%2520Jiaqi%2520Mu%2520and%2520Pranav%2520Talluri%2520and%2520Maggie%2520Tran%2520and%2520Rajeev%2520Aggarwal%2520and%2520RJ%2520Skerry-Ryan%2520and%2520Jun%2520Xu%2520and%2520Mike%2520Burrows%2520and%2520Xiaoyue%2520Pan%2520and%2520Edouard%2520Yvinec%2520and%2520Di%2520Lu%2520and%2520Zhiying%2520Zhang%2520and%2520Duc%2520Dung%2520Nguyen%2520and%2520Hairong%2520Mu%2520and%2520Gabriel%2520Barcik%2520and%2520Helen%2520Ran%2520and%2520Lauren%2520Beltrone%2520and%2520Krzysztof%2520Choromanski%2520and%2520Dia%2520Kharrat%2520and%2520Samuel%2520Albanie%2520and%2520Sean%2520Purser-haskell%2520and%2520David%2520Bieber%2520and%2520Carrie%2520Zhang%2520and%2520Jing%2520Wang%2520and%2520Tom%2520Hudson%2520and%2520Zhiyuan%2520Zhang%2520and%2520Han%2520Fu%2520and%2520Johannes%2520Mauerer%2520and%2520Mohammad%2520Hossein%2520Bateni%2520and%2520AJ%2520Maschinot%2520and%2520Bing%2520Wang%2520and%2520Muye%2520Zhu%2520and%2520Arjun%2520Pillai%2520and%2520Tobias%2520Weyand%2520and%2520Shuang%2520Liu%2520and%2520Oscar%2520Akerlund%2520and%2520Fred%2520Bertsch%2520and%2520Vittal%2520Premachandran%2520and%2520Alicia%2520Jin%2520and%2520Vincent%2520Roulet%2520and%2520Peter%2520de%2520Boursac%2520and%2520Shubham%2520Mittal%2520and%2520Ndaba%2520Ndebele%2520and%2520Georgi%2520Karadzhov%2520and%2520Sahra%2520Ghalebikesabi%2520and%2520Ricky%2520Liang%2520and%2520Allen%2520Wu%2520and%2520Yale%2520Cong%2520and%2520Nimesh%2520Ghelani%2520and%2520Sumeet%2520Singh%2520and%2520Bahar%2520Fatemi%2520and%2520%2520Warren%2520and%2520%2520Chen%2520and%2520Charles%2520Kwong%2520and%2520Alexey%2520Kolganov%2520and%2520Steve%2520Li%2520and%2520Richard%2520Song%2520and%2520Chenkai%2520Kuang%2520and%2520Sobhan%2520Miryoosefi%2520and%2520Dale%2520Webster%2520and%2520James%2520Wendt%2520and%2520Arkadiusz%2520Socala%2520and%2520Guolong%2520Su%2520and%2520Artur%2520Mendon%25C3%25A7a%2520and%2520Abhinav%2520Gupta%2520and%2520Xiaowei%2520Li%2520and%2520Tomy%2520Tsai%2520and%2520%2520Qiong%2520and%2520%2520Hu%2520and%2520Kai%2520Kang%2520and%2520Angie%2520Chen%2520and%2520Sertan%2520Girgin%2520and%2520Yongqin%2520Xian%2520and%2520Andrew%2520Lee%2520and%2520Nolan%2520Ramsden%2520and%2520Leslie%2520Baker%2520and%2520Madeleine%2520Clare%2520Elish%2520and%2520Varvara%2520Krayvanova%2520and%2520Rishabh%2520Joshi%2520and%2520Jiri%2520Simsa%2520and%2520Yao-Yuan%2520Yang%2520and%2520Piotr%2520Ambroszczyk%2520and%2520Dipankar%2520Ghosh%2520and%2520Arjun%2520Kar%2520and%2520Yuan%2520Shangguan%2520and%2520Yumeya%2520Yamamori%2520and%2520Yaroslav%2520Akulov%2520and%2520Andy%2520Brock%2520and%2520Haotian%2520Tang%2520and%2520Siddharth%2520Vashishtha%2520and%2520Rich%2520Munoz%2520and%2520Andreas%2520Steiner%2520and%2520Kalyan%2520Andra%2520and%2520Daniel%2520Eppens%2520and%2520Qixuan%2520Feng%2520and%2520Hayato%2520Kobayashi%2520and%2520Sasha%2520Goldshtein%2520and%2520Mona%2520El%2520Mahdy%2520and%2520Xin%2520Wang%2520and%2520%2520Jilei%2520and%2520%2520Wang%2520and%2520Richard%2520Killam%2520and%2520Tom%2520Kwiatkowski%2520and%2520Kavya%2520Kopparapu%2520and%2520Serena%2520Zhan%2520and%2520Chao%2520Jia%2520and%2520Alexei%2520Bendebury%2520and%2520Sheryl%2520Luo%2520and%2520Adri%25C3%25A0%2520Recasens%2520and%2520Timothy%2520Knight%2520and%2520Jing%2520Chen%2520and%2520Mohak%2520Patel%2520and%2520YaGuang%2520Li%2520and%2520Ben%2520Withbroe%2520and%2520Dean%2520Weesner%2520and%2520Kush%2520Bhatia%2520and%2520Jie%2520Ren%2520and%2520Danielle%2520Eisenbud%2520and%2520Ebrahim%2520Songhori%2520and%2520Yanhua%2520Sun%2520and%2520Travis%2520Choma%2520and%2520Tasos%2520Kementsietsidis%2520and%2520Lucas%2520Manning%2520and%2520Brian%2520Roark%2520and%2520Wael%2520Farhan%2520and%2520Jie%2520Feng%2520and%2520Susheel%2520Tatineni%2520and%2520James%2520Cobon-Kerr%2520and%2520Yunjie%2520Li%2520and%2520Lisa%2520Anne%2520Hendricks%2520and%2520Isaac%2520Noble%2520and%2520Chris%2520Breaux%2520and%2520Nate%2520Kushman%2520and%2520Liqian%2520Peng%2520and%2520Fuzhao%2520Xue%2520and%2520Taylor%2520Tobin%2520and%2520Jamie%2520Rogers%2520and%2520Josh%2520Lipschultz%2520and%2520Chris%2520Alberti%2520and%2520Alexey%2520Vlaskin%2520and%2520Mostafa%2520Dehghani%2520and%2520Roshan%2520Sharma%2520and%2520Tris%2520Warkentin%2520and%2520Chen-Yu%2520Lee%2520and%2520Benigno%2520Uria%2520and%2520Da-Cheng%2520Juan%2520and%2520Angad%2520Chandorkar%2520and%2520Hila%2520Sheftel%2520and%2520Ruibo%2520Liu%2520and%2520Elnaz%2520Davoodi%2520and%2520Borja%2520De%2520Balle%2520Pigem%2520and%2520Kedar%2520Dhamdhere%2520and%2520David%2520Ross%2520and%2520Jonathan%2520Hoech%2520and%2520Mahdis%2520Mahdieh%2520and%2520Li%2520Liu%2520and%2520Qiujia%2520Li%2520and%2520Liam%2520McCafferty%2520and%2520Chenxi%2520Liu%2520and%2520Markus%2520Mircea%2520and%2520Yunting%2520Song%2520and%2520Omkar%2520Savant%2520and%2520Alaa%2520Saade%2520and%2520Colin%2520Cherry%2520and%2520Vincent%2520Hellendoorn%2520and%2520Siddharth%2520Goyal%2520and%2520Paul%2520Pucciarelli%2520and%2520David%2520Vilar%2520Torres%2520and%2520Zohar%2520Yahav%2520and%2520Hyo%2520Lee%2520and%2520Lars%2520Lowe%2520Sjoesund%2520and%2520Christo%2520Kirov%2520and%2520Bo%2520Chang%2520and%2520Deepanway%2520Ghoshal%2520and%2520Lu%2520Li%2520and%2520Gilles%2520Baechler%2520and%2520S%25C3%25A9bastien%2520Pereira%2520and%2520Tara%2520Sainath%2520and%2520Anudhyan%2520Boral%2520and%2520Dominik%2520Grewe%2520and%2520Afief%2520Halumi%2520and%2520Nguyet%2520Minh%2520Phu%2520and%2520Tianxiao%2520Shen%2520and%2520Marco%2520Tulio%2520Ribeiro%2520and%2520Dhriti%2520Varma%2520and%2520Alex%2520Kaskasoli%2520and%2520Vlad%2520Feinberg%2520and%2520Navneet%2520Potti%2520and%2520Jarrod%2520Kahn%2520and%2520Matheus%2520Wisniewski%2520and%2520Shakir%2520Mohamed%2520and%2520Arnar%2520Mar%2520Hrafnkelsson%2520and%2520Bobak%2520Shahriari%2520and%2520Jean-Baptiste%2520Lespiau%2520and%2520Lisa%2520Patel%2520and%2520Legg%2520Yeung%2520and%2520Tom%2520Paine%2520and%2520Lantao%2520Mei%2520and%2520Alex%2520Ramirez%2520and%2520Rakesh%2520Shivanna%2520and%2520Li%2520Zhong%2520and%2520Josh%2520Woodward%2520and%2520Guilherme%2520Tubone%2520and%2520Samira%2520Khan%2520and%2520Heng%2520Chen%2520and%2520Elizabeth%2520Nielsen%2520and%2520Catalin%2520Ionescu%2520and%2520Utsav%2520Prabhu%2520and%2520Mingcen%2520Gao%2520and%2520Qingze%2520Wang%2520and%2520Sean%2520Augenstein%2520and%2520Neesha%2520Subramaniam%2520and%2520Jason%2520Chang%2520and%2520Fotis%2520Iliopoulos%2520and%2520Jiaming%2520Luo%2520and%2520Myriam%2520Khan%2520and%2520Weicheng%2520Kuo%2520and%2520Denis%2520Teplyashin%2520and%2520Florence%2520Perot%2520and%2520Logan%2520Kilpatrick%2520and%2520Amir%2520Globerson%2520and%2520Hongkun%2520Yu%2520and%2520Anfal%2520Siddiqui%2520and%2520Nick%2520Sukhanov%2520and%2520Arun%2520Kandoor%2520and%2520Umang%2520Gupta%2520and%2520Marco%2520Andreetto%2520and%2520Moran%2520Ambar%2520and%2520Donnie%2520Kim%2520and%2520Pawe%25C5%2582%2520Weso%25C5%2582owski%2520and%2520Sarah%2520Perrin%2520and%2520Ben%2520Limonchik%2520and%2520Wei%2520Fan%2520and%2520Jim%2520Stephan%2520and%2520Ian%2520Stewart-Binks%2520and%2520Ryan%2520Kappedal%2520and%2520Tong%2520He%2520and%2520Sarah%2520Cogan%2520and%2520Romina%2520Datta%2520and%2520Tong%2520Zhou%2520and%2520Jiayu%2520Ye%2520and%2520Leandro%2520Kieliger%2520and%2520Ana%2520Ramalho%2520and%2520Kyle%2520Kastner%2520and%2520Fabian%2520Mentzer%2520and%2520Wei-Jen%2520Ko%2520and%2520Arun%2520Suggala%2520and%2520Tianhao%2520Zhou%2520and%2520Shiraz%2520Butt%2520and%2520Hana%2520Strej%25C4%258Dek%2520and%2520Lior%2520Belenki%2520and%2520Subhashini%2520Venugopalan%2520and%2520Mingyang%2520Ling%2520and%2520Evgenii%2520Eltyshev%2520and%2520Yunxiao%2520Deng%2520and%2520Geza%2520Kovacs%2520and%2520Mukund%2520Raghavachari%2520and%2520Hanjun%2520Dai%2520and%2520Tal%2520Schuster%2520and%2520Steven%2520Schwarcz%2520and%2520Richard%2520Nguyen%2520and%2520Arthur%2520Nguyen%2520and%2520Gavin%2520Buttimore%2520and%2520Shrestha%2520Basu%2520Mallick%2520and%2520Sudeep%2520Gandhe%2520and%2520Seth%2520Benjamin%2520and%2520Michal%2520Jastrzebski%2520and%2520Le%2520Yan%2520and%2520Sugato%2520Basu%2520and%2520Chris%2520Apps%2520and%2520Isabel%2520Edkins%2520and%2520James%2520Allingham%2520and%2520Immanuel%2520Odisho%2520and%2520Tomas%2520Kocisky%2520and%2520Jewel%2520Zhao%2520and%2520Linting%2520Xue%2520and%2520Apoorv%2520Reddy%2520and%2520Chrysovalantis%2520Anastasiou%2520and%2520Aviel%2520Atias%2520and%2520Sam%2520Redmond%2520and%2520Kieran%2520Milan%2520and%2520Nicolas%2520Heess%2520and%2520Herman%2520Schmit%2520and%2520Allan%2520Dafoe%2520and%2520Daniel%2520Andor%2520and%2520Tynan%2520Gangwani%2520and%2520Anca%2520Dragan%2520and%2520Sheng%2520Zhang%2520and%2520Ashyana%2520Kachra%2520and%2520Gang%2520Wu%2520and%2520Siyang%2520Xue%2520and%2520Kevin%2520Aydin%2520and%2520Siqi%2520Liu%2520and%2520Yuxiang%2520Zhou%2520and%2520Mahan%2520Malihi%2520and%2520Austin%2520Wu%2520and%2520Siddharth%2520Gopal%2520and%2520Candice%2520Schumann%2520and%2520Peter%2520Stys%2520and%2520Alek%2520Wang%2520and%2520Mirek%2520Ol%25C5%25A1%25C3%25A1k%2520and%2520Dangyi%2520Liu%2520and%2520Christian%2520Schallhart%2520and%2520Yiran%2520Mao%2520and%2520Demetra%2520Brady%2520and%2520Hao%2520Xu%2520and%2520Tomas%2520Mery%2520and%2520Chawin%2520Sitawarin%2520and%2520Siva%2520Velusamy%2520and%2520Tom%2520Cobley%2520and%2520Alex%2520Zhai%2520and%2520Christian%2520Walder%2520and%2520Nitzan%2520Katz%2520and%2520Ganesh%2520Jawahar%2520and%2520Chinmay%2520Kulkarni%2520and%2520Antoine%2520Yang%2520and%2520Adam%2520Paszke%2520and%2520Yinan%2520Wang%2520and%2520Bogdan%2520Damoc%2520and%2520Zal%25C3%25A1n%2520Borsos%2520and%2520Ray%2520Smith%2520and%2520Jinning%2520Li%2520and%2520Mansi%2520Gupta%2520and%2520Andrei%2520Kapishnikov%2520and%2520Sushant%2520Prakash%2520and%2520Florian%2520Luisier%2520and%2520Rishabh%2520Agarwal%2520and%2520Will%2520Grathwohl%2520and%2520Kuangyuan%2520Chen%2520and%2520Kehang%2520Han%2520and%2520Nikhil%2520Mehta%2520and%2520Andrew%2520Over%2520and%2520Shekoofeh%2520Azizi%2520and%2520Lei%2520Meng%2520and%2520Niccol%25C3%25B2%2520Dal%2520Santo%2520and%2520Kelvin%2520Zheng%2520and%2520Jane%2520Shapiro%2520and%2520Igor%2520Petrovski%2520and%2520Jeffrey%2520Hui%2520and%2520Amin%2520Ghafouri%2520and%2520Jasper%2520Snoek%2520and%2520James%2520Qin%2520and%2520Mandy%2520Jordan%2520and%2520Caitlin%2520Sikora%2520and%2520Jonathan%2520Malmaud%2520and%2520Yuheng%2520Kuang%2520and%2520Aga%2520%25C5%259Awietlik%2520and%2520Ruoxin%2520Sang%2520and%2520Chongyang%2520Shi%2520and%2520Leon%2520Li%2520and%2520Andrew%2520Rosenberg%2520and%2520Shubin%2520Zhao%2520and%2520Andy%2520Crawford%2520and%2520Jan-Thorsten%2520Peter%2520and%2520Yun%2520Lei%2520and%2520Xavier%2520Garcia%2520and%2520Long%2520Le%2520and%2520Todd%2520Wang%2520and%2520Julien%2520Amelot%2520and%2520Dave%2520Orr%2520and%2520Praneeth%2520Kacham%2520and%2520Dana%2520Alon%2520and%2520Gladys%2520Tyen%2520and%2520Abhinav%2520Arora%2520and%2520James%2520Lyon%2520and%2520Alex%2520Kurakin%2520and%2520Mimi%2520Ly%2520and%2520Theo%2520Guidroz%2520and%2520Zhipeng%2520Yan%2520and%2520Rina%2520Panigrahy%2520and%2520Pingmei%2520Xu%2520and%2520Thais%2520Kagohara%2520and%2520Yong%2520Cheng%2520and%2520Eric%2520Noland%2520and%2520Jinhyuk%2520Lee%2520and%2520Jonathan%2520Lee%2520and%2520Cathy%2520Yip%2520and%2520Maria%2520Wang%2520and%2520Efrat%2520Nehoran%2520and%2520Alexander%2520Bykovsky%2520and%2520Zhihao%2520Shan%2520and%2520Ankit%2520Bhagatwala%2520and%2520Chaochao%2520Yan%2520and%2520Jie%2520Tan%2520and%2520Guillermo%2520Garrido%2520and%2520Dan%2520Ethier%2520and%2520Nate%2520Hurley%2520and%2520Grace%2520Vesom%2520and%2520Xu%2520Chen%2520and%2520Siyuan%2520Qiao%2520and%2520Abhishek%2520Nayyar%2520and%2520Julian%2520Walker%2520and%2520Paramjit%2520Sandhu%2520and%2520Mihaela%2520Rosca%2520and%2520Danny%2520Swisher%2520and%2520Mikhail%2520Dektiarev%2520and%2520Josh%2520Dillon%2520and%2520George-Cristian%2520Muraru%2520and%2520Manuel%2520Tragut%2520and%2520Artiom%2520Myaskovsky%2520and%2520David%2520Reid%2520and%2520Marko%2520Velic%2520and%2520Owen%2520Xiao%2520and%2520Jasmine%2520George%2520and%2520Mark%2520Brand%2520and%2520Jing%2520Li%2520and%2520Wenhao%2520Yu%2520and%2520Shane%2520Gu%2520and%2520Xiang%2520Deng%2520and%2520Fran%25C3%25A7ois-Xavier%2520Aubet%2520and%2520Soheil%2520Hassas%2520Yeganeh%2520and%2520Fred%2520Alcober%2520and%2520Celine%2520Smith%2520and%2520Trevor%2520Cohn%2520and%2520Kay%2520McKinney%2520and%2520Michael%2520Tschannen%2520and%2520Ramesh%2520Sampath%2520and%2520Gowoon%2520Cheon%2520and%2520Liangchen%2520Luo%2520and%2520Luyang%2520Liu%2520and%2520Jordi%2520Orbay%2520and%2520Hui%2520Peng%2520and%2520Gabriela%2520Botea%2520and%2520Xiaofan%2520Zhang%2520and%2520Charles%2520Yoon%2520and%2520Cesar%2520Magalhaes%2520and%2520Pawe%25C5%2582%2520Stradomski%2520and%2520Ian%2520Mackinnon%2520and%2520Steven%2520Hemingray%2520and%2520Kumaran%2520Venkatesan%2520and%2520Rhys%2520May%2520and%2520Jaeyoun%2520Kim%2520and%2520Alex%2520Druinsky%2520and%2520Jingchen%2520Ye%2520and%2520Zheng%2520Xu%2520and%2520Terry%2520Huang%2520and%2520Jad%2520Al%2520Abdallah%2520and%2520Adil%2520Dostmohamed%2520and%2520Rachana%2520Fellinger%2520and%2520Tsendsuren%2520Munkhdalai%2520and%2520Akanksha%2520Maurya%2520and%2520Peter%2520Garst%2520and%2520Yin%2520Zhang%2520and%2520Maxim%2520Krikun%2520and%2520Simon%2520Bucher%2520and%2520Aditya%2520Srikanth%2520Veerubhotla%2520and%2520Yaxin%2520Liu%2520and%2520Sheng%2520Li%2520and%2520Nishesh%2520Gupta%2520and%2520Jakub%2520Adamek%2520and%2520Hanwen%2520Chen%2520and%2520Bernett%2520Orlando%2520and%2520Aleksandr%2520Zaks%2520and%2520Joost%2520van%2520Amersfoort%2520and%2520Josh%2520Camp%2520and%2520Hui%2520Wan%2520and%2520HyunJeong%2520Choe%2520and%2520Zhichun%2520Wu%2520and%2520Kate%2520Olszewska%2520and%2520Weiren%2520Yu%2520and%2520Archita%2520Vadali%2520and%2520Martin%2520Scholz%2520and%2520Daniel%2520De%2520Freitas%2520and%2520Jason%2520Lin%2520and%2520Amy%2520Hua%2520and%2520Xin%2520Liu%2520and%2520Frank%2520Ding%2520and%2520Yichao%2520Zhou%2520and%2520Boone%2520Severson%2520and%2520Katerina%2520Tsihlas%2520and%2520Samuel%2520Yang%2520and%2520Tammo%2520Spalink%2520and%2520Varun%2520Yerram%2520and%2520Helena%2520Pankov%2520and%2520Rory%2520Blevins%2520and%2520Ben%2520Vargas%2520and%2520Sarthak%2520Jauhari%2520and%2520Matt%2520Miecnikowski%2520and%2520Ming%2520Zhang%2520and%2520Sandeep%2520Kumar%2520and%2520Clement%2520Farabet%2520and%2520Charline%2520Le%2520Lan%2520and%2520Sebastian%2520Flennerhag%2520and%2520Yonatan%2520Bitton%2520and%2520Ada%2520Ma%2520and%2520Arthur%2520Bra%25C5%25BEinskas%2520and%2520Eli%2520Collins%2520and%2520Niharika%2520Ahuja%2520and%2520Sneha%2520Kudugunta%2520and%2520Anna%2520Bortsova%2520and%2520Minh%2520Giang%2520and%2520Wanzheng%2520Zhu%2520and%2520Ed%2520Chi%2520and%2520Scott%2520Lundberg%2520and%2520Alexey%2520Stern%2520and%2520Subha%2520Puttagunta%2520and%2520Jing%2520Xiong%2520and%2520Xiao%2520Wu%2520and%2520Yash%2520Pande%2520and%2520Amit%2520Jhindal%2520and%2520Daniel%2520Murphy%2520and%2520Jon%2520Clark%2520and%2520Marc%2520Brockschmidt%2520and%2520Maxine%2520Deines%2520and%2520Kevin%2520R.%2520McKee%2520and%2520Dan%2520Bahir%2520and%2520Jiajun%2520Shen%2520and%2520Minh%2520Truong%2520and%2520Daniel%2520McDuff%2520and%2520Andrea%2520Gesmundo%2520and%2520Edouard%2520Rosseel%2520and%2520Bowen%2520Liang%2520and%2520Ken%2520Caluwaerts%2520and%2520Jessica%2520Hamrick%2520and%2520Joseph%2520Kready%2520and%2520Mary%2520Cassin%2520and%2520Rishikesh%2520Ingale%2520and%2520Li%2520Lao%2520and%2520Scott%2520Pollom%2520and%2520Yifan%2520Ding%2520and%2520Wei%2520He%2520and%2520Lizzetth%2520Bellot%2520and%2520Joana%2520Iljazi%2520and%2520Ramya%2520Sree%2520Boppana%2520and%2520Shan%2520Han%2520and%2520Tara%2520Thompson%2520and%2520Amr%2520Khalifa%2520and%2520Anna%2520Bulanova%2520and%2520Blagoj%2520Mitrevski%2520and%2520Bo%2520Pang%2520and%2520Emma%2520Cooney%2520and%2520Tian%2520Shi%2520and%2520Rey%2520Coaguila%2520and%2520Tamar%2520Yakar%2520and%2520Marc%2527aurelio%2520Ranzato%2520and%2520Nikola%2520Momchev%2520and%2520Chris%2520Rawles%2520and%2520Zachary%2520Charles%2520and%2520Young%2520Maeng%2520and%2520Yuan%2520Zhang%2520and%2520Rishabh%2520Bansal%2520and%2520Xiaokai%2520Zhao%2520and%2520Brian%2520Albert%2520and%2520Yuan%2520Yuan%2520and%2520Sudheendra%2520Vijayanarasimhan%2520and%2520Roy%2520Hirsch%2520and%2520Vinay%2520Ramasesh%2520and%2520Kiran%2520Vodrahalli%2520and%2520Xingyu%2520Wang%2520and%2520Arushi%2520Gupta%2520and%2520DJ%2520Strouse%2520and%2520Jianmo%2520Ni%2520and%2520Roma%2520Patel%2520and%2520Gabe%2520Taubman%2520and%2520Zhouyuan%2520Huo%2520and%2520Dero%2520Gharibian%2520and%2520Marianne%2520Monteiro%2520and%2520Hoi%2520Lam%2520and%2520Shobha%2520Vasudevan%2520and%2520Aditi%2520Chaudhary%2520and%2520Isabela%2520Albuquerque%2520and%2520Kilol%2520Gupta%2520and%2520Sebastian%2520Riedel%2520and%2520Chaitra%2520Hegde%2520and%2520Avraham%2520Ruderman%2520and%2520Andr%25C3%25A1s%2520Gy%25C3%25B6rgy%2520and%2520Marcus%2520Wainwright%2520and%2520Ashwin%2520Chaugule%2520and%2520Burcu%2520Karagol%2520Ayan%2520and%2520Tomer%2520Levinboim%2520and%2520Sam%2520Shleifer%2520and%2520Yogesh%2520Kalley%2520and%2520Vahab%2520Mirrokni%2520and%2520Abhishek%2520Rao%2520and%2520Prabakar%2520Radhakrishnan%2520and%2520Jay%2520Hartford%2520and%2520Jialin%2520Wu%2520and%2520Zhenhai%2520Zhu%2520and%2520Francesco%2520Bertolini%2520and%2520Hao%2520Xiong%2520and%2520Nicolas%2520Serrano%2520and%2520Hamish%2520Tomlinson%2520and%2520Myle%2520Ott%2520and%2520Yifan%2520Chang%2520and%2520Mark%2520Graham%2520and%2520Jian%2520Li%2520and%2520Marco%2520Liang%2520and%2520Xiangzhu%2520Long%2520and%2520Sebastian%2520Borgeaud%2520and%2520Yanif%2520Ahmad%2520and%2520Alex%2520Grills%2520and%2520Diana%2520Mincu%2520and%2520Martin%2520Izzard%2520and%2520Yuan%2520Liu%2520and%2520Jinyu%2520Xie%2520and%2520Louis%2520O%2527Bryan%2520and%2520Sameera%2520Ponda%2520and%2520Simon%2520Tong%2520and%2520Michelle%2520Liu%2520and%2520Dan%2520Malkin%2520and%2520Khalid%2520Salama%2520and%2520Yuankai%2520Chen%2520and%2520Rohan%2520Anil%2520and%2520Anand%2520Rao%2520and%2520Rigel%2520Swavely%2520and%2520Misha%2520Bilenko%2520and%2520Nina%2520Anderson%2520and%2520Tat%2520Tan%2520and%2520Jing%2520Xie%2520and%2520Xing%2520Wu%2520and%2520Lijun%2520Yu%2520and%2520Oriol%2520Vinyals%2520and%2520Andrey%2520Ryabtsev%2520and%2520Rumen%2520Dangovski%2520and%2520Kate%2520Baumli%2520and%2520Daniel%2520Keysers%2520and%2520Christian%2520Wright%2520and%2520Zoe%2520Ashwood%2520and%2520Betty%2520Chan%2520and%2520Artem%2520Shtefan%2520and%2520Yaohui%2520Guo%2520and%2520Ankur%2520Bapna%2520and%2520Radu%2520Soricut%2520and%2520Steven%2520Pecht%2520and%2520Sabela%2520Ramos%2520and%2520Rui%2520Wang%2520and%2520Jiahao%2520Cai%2520and%2520Trieu%2520Trinh%2520and%2520Paul%2520Barham%2520and%2520Linda%2520Friso%2520and%2520Eli%2520Stickgold%2520and%2520Xiangzhuo%2520Ding%2520and%2520Siamak%2520Shakeri%2520and%2520Diego%2520Ardila%2520and%2520Eleftheria%2520Briakou%2520and%2520Phil%2520Culliton%2520and%2520Adam%2520Raveret%2520and%2520Jingyu%2520Cui%2520and%2520David%2520Saxton%2520and%2520Subhrajit%2520Roy%2520and%2520Javad%2520Azizi%2520and%2520Pengcheng%2520Yin%2520and%2520Lucia%2520Loher%2520and%2520Andrew%2520Bunner%2520and%2520Min%2520Choi%2520and%2520Faruk%2520Ahmed%2520and%2520Eric%2520Li%2520and%2520Yin%2520Li%2520and%2520Shengyang%2520Dai%2520and%2520Michael%2520Elabd%2520and%2520Sriram%2520Ganapathy%2520and%2520Shivani%2520Agrawal%2520and%2520Yiqing%2520Hua%2520and%2520Paige%2520Kunkle%2520and%2520Sujeevan%2520Rajayogam%2520and%2520Arun%2520Ahuja%2520and%2520Arthur%2520Conmy%2520and%2520Alex%2520Vasiloff%2520and%2520Parker%2520Beak%2520and%2520Christopher%2520Yew%2520and%2520Jayaram%2520Mudigonda%2520and%2520Bartek%2520Wydrowski%2520and%2520Jon%2520Blanton%2520and%2520Zhengdong%2520Wang%2520and%2520Yann%2520Dauphin%2520and%2520Zhuo%2520Xu%2520and%2520Martin%2520Polacek%2520and%2520Xi%2520Chen%2520and%2520Hexiang%2520Hu%2520and%2520Pauline%2520Sho%2520and%2520Markus%2520Kunesch%2520and%2520Mehdi%2520Hafezi%2520Manshadi%2520and%2520Eliza%2520Rutherford%2520and%2520Bo%2520Li%2520and%2520Sissie%2520Hsiao%2520and%2520Iain%2520Barr%2520and%2520Alex%2520Tudor%2520and%2520Matija%2520Kecman%2520and%2520Arsha%2520Nagrani%2520and%2520Vladimir%2520Pchelin%2520and%2520Martin%2520Sundermeyer%2520and%2520Aishwarya%2520P%2520S%2520and%2520Abhijit%2520Karmarkar%2520and%2520Yi%2520Gao%2520and%2520Grishma%2520Chole%2520and%2520Olivier%2520Bachem%2520and%2520Isabel%2520Gao%2520and%2520Arturo%2520BC%2520and%2520Matt%2520Dibb%2520and%2520Mauro%2520Verzetti%2520and%2520Felix%2520Hernandez-Campos%2520and%2520Yana%2520Lunts%2520and%2520Matthew%2520Johnson%2520and%2520Julia%2520Di%2520Trapani%2520and%2520Raphael%2520Koster%2520and%2520Idan%2520Brusilovsky%2520and%2520Binbin%2520Xiong%2520and%2520Megha%2520Mohabey%2520and%2520Han%2520Ke%2520and%2520Joe%2520Zou%2520and%2520Tea%2520Saboli%25C4%2587%2520and%2520V%25C3%25ADctor%2520Campos%2520and%2520John%2520Palowitch%2520and%2520Alex%2520Morris%2520and%2520Linhai%2520Qiu%2520and%2520Pranavaraj%2520Ponnuramu%2520and%2520Fangtao%2520Li%2520and%2520Vivek%2520Sharma%2520and%2520Kiranbir%2520Sodhia%2520and%2520Kaan%2520Tekelioglu%2520and%2520Aleksandr%2520Chuklin%2520and%2520Madhavi%2520Yenugula%2520and%2520Erika%2520Gemzer%2520and%2520Theofilos%2520Strinopoulos%2520and%2520Sam%2520El-Husseini%2520and%2520Huiyu%2520Wang%2520and%2520Yan%2520Zhong%2520and%2520Edouard%2520Leurent%2520and%2520Paul%2520Natsev%2520and%2520Weijun%2520Wang%2520and%2520Dre%2520Mahaarachchi%2520and%2520Tao%2520Zhu%2520and%2520Songyou%2520Peng%2520and%2520Sami%2520Alabed%2520and%2520Cheng-Chun%2520Lee%2520and%2520Anthony%2520Brohan%2520and%2520Arthur%2520Szlam%2520and%2520GS%2520Oh%2520and%2520Anton%2520Kovsharov%2520and%2520Jenny%2520Lee%2520and%2520Renee%2520Wong%2520and%2520Megan%2520Barnes%2520and%2520Gregory%2520Thornton%2520and%2520Felix%2520Gimeno%2520and%2520Omer%2520Levy%2520and%2520Martin%2520Sevenich%2520and%2520Melvin%2520Johnson%2520and%2520Jonathan%2520Mallinson%2520and%2520Robert%2520Dadashi%2520and%2520Ziyue%2520Wang%2520and%2520Qingchun%2520Ren%2520and%2520Preethi%2520Lahoti%2520and%2520Arka%2520Dhar%2520and%2520Josh%2520Feldman%2520and%2520Dan%2520Zheng%2520and%2520Thatcher%2520Ulrich%2520and%2520Liviu%2520Panait%2520and%2520Michiel%2520Blokzijl%2520and%2520Cip%2520Baetu%2520and%2520Josip%2520Matak%2520and%2520Jitendra%2520Harlalka%2520and%2520Maulik%2520Shah%2520and%2520Tal%2520Marian%2520and%2520Daniel%2520von%2520Dincklage%2520and%2520Cosmo%2520Du%2520and%2520Ruy%2520Ley-Wild%2520and%2520Bethanie%2520Brownfield%2520and%2520Max%2520Schumacher%2520and%2520Yury%2520Stuken%2520and%2520Shadi%2520Noghabi%2520and%2520Sonal%2520Gupta%2520and%2520Xiaoqi%2520Ren%2520and%2520Eric%2520Malmi%2520and%2520Felix%2520Weissenberger%2520and%2520Blanca%2520Huergo%2520and%2520Maria%2520Bauza%2520and%2520Thomas%2520Lampe%2520and%2520Arthur%2520Douillard%2520and%2520Mojtaba%2520Seyedhosseini%2520and%2520Roy%2520Frostig%2520and%2520Zoubin%2520Ghahramani%2520and%2520Kelvin%2520Nguyen%2520and%2520Kashyap%2520Krishnakumar%2520and%2520Chengxi%2520Ye%2520and%2520Rahul%2520Gupta%2520and%2520Alireza%2520Nazari%2520and%2520Robert%2520Geirhos%2520and%2520Pete%2520Shaw%2520and%2520Ahmed%2520Eleryan%2520and%2520Dima%2520Damen%2520and%2520Jennimaria%2520Palomaki%2520and%2520Ted%2520Xiao%2520and%2520Qiyin%2520Wu%2520and%2520Quan%2520Yuan%2520and%2520Phoenix%2520Meadowlark%2520and%2520Matthew%2520Bilotti%2520and%2520Raymond%2520Lin%2520and%2520Mukund%2520Sridhar%2520and%2520Yannick%2520Schroecker%2520and%2520Da-Woon%2520Chung%2520and%2520Jincheng%2520Luo%2520and%2520Trevor%2520Strohman%2520and%2520Tianlin%2520Liu%2520and%2520Anne%2520Zheng%2520and%2520Jesse%2520Emond%2520and%2520Wei%2520Wang%2520and%2520Andrew%2520Lampinen%2520and%2520Toshiyuki%2520Fukuzawa%2520and%2520Folawiyo%2520Campbell-Ajala%2520and%2520Monica%2520Roy%2520and%2520James%2520Lee-Thorp%2520and%2520Lily%2520Wang%2520and%2520Iftekhar%2520Naim%2520and%2520%2520Tony%2520and%2520%2520Nguy%255C~%25C3%25AAn%2520and%2520Guy%2520Bensky%2520and%2520Aditya%2520Gupta%2520and%2520Dominika%2520Rogozi%25C5%2584ska%2520and%2520Justin%2520Fu%2520and%2520Thanumalayan%2520Sankaranarayana%2520Pillai%2520and%2520Petar%2520Veli%25C4%258Dkovi%25C4%2587%2520and%2520Shahar%2520Drath%2520and%2520Philipp%2520Neubeck%2520and%2520Vaibhav%2520Tulsyan%2520and%2520Arseniy%2520Klimovskiy%2520and%2520Don%2520Metzler%2520and%2520Sage%2520Stevens%2520and%2520Angel%2520Yeh%2520and%2520Junwei%2520Yuan%2520and%2520Tianhe%2520Yu%2520and%2520Kelvin%2520Zhang%2520and%2520Alec%2520Go%2520and%2520Vincent%2520Tsang%2520and%2520Ying%2520Xu%2520and%2520Andy%2520Wan%2520and%2520Isaac%2520Galatzer-Levy%2520and%2520Sam%2520Sobell%2520and%2520Abodunrinwa%2520Toki%2520and%2520Elizabeth%2520Salesky%2520and%2520Wenlei%2520Zhou%2520and%2520Diego%2520Antognini%2520and%2520Sholto%2520Douglas%2520and%2520Shimu%2520Wu%2520and%2520Adam%2520Lelkes%2520and%2520Frank%2520Kim%2520and%2520Paul%2520Cavallaro%2520and%2520Ana%2520Salazar%2520and%2520Yuchi%2520Liu%2520and%2520James%2520Besley%2520and%2520Tiziana%2520Refice%2520and%2520Yiling%2520Jia%2520and%2520Zhang%2520Li%2520and%2520Michal%2520Sokolik%2520and%2520Arvind%2520Kannan%2520and%2520Jon%2520Simon%2520and%2520Jo%2520Chick%2520and%2520Avia%2520Aharon%2520and%2520Meet%2520Gandhi%2520and%2520Mayank%2520Daswani%2520and%2520Keyvan%2520Amiri%2520and%2520Vighnesh%2520Birodkar%2520and%2520Abe%2520Ittycheriah%2520and%2520Peter%2520Grabowski%2520and%2520Oscar%2520Chang%2520and%2520Charles%2520Sutton%2520and%2520%2520Zhixin%2520and%2520%2520Lai%2520and%2520Umesh%2520Telang%2520and%2520Susie%2520Sargsyan%2520and%2520Tao%2520Jiang%2520and%2520Raphael%2520Hoffmann%2520and%2520Nicole%2520Brichtova%2520and%2520Matteo%2520Hessel%2520and%2520Jonathan%2520Halcrow%2520and%2520Sammy%2520Jerome%2520and%2520Geoff%2520Brown%2520and%2520Alex%2520Tomala%2520and%2520Elena%2520Buchatskaya%2520and%2520Dian%2520Yu%2520and%2520Sachit%2520Menon%2520and%2520Pol%2520Moreno%2520and%2520Yuguo%2520Liao%2520and%2520Vicky%2520Zayats%2520and%2520Luming%2520Tang%2520and%2520SQ%2520Mah%2520and%2520Ashish%2520Shenoy%2520and%2520Alex%2520Siegman%2520and%2520Majid%2520Hadian%2520and%2520Okwan%2520Kwon%2520and%2520Tao%2520Tu%2520and%2520Nima%2520Khajehnouri%2520and%2520Ryan%2520Foley%2520and%2520Parisa%2520Haghani%2520and%2520Zhongru%2520Wu%2520and%2520Vaishakh%2520Keshava%2520and%2520Khyatti%2520Gupta%2520and%2520Tony%2520Bruguier%2520and%2520Rui%2520Yao%2520and%2520Danny%2520Karmon%2520and%2520Luisa%2520Zintgraf%2520and%2520Zhicheng%2520Wang%2520and%2520Enrique%2520Piqueras%2520and%2520Junehyuk%2520Jung%2520and%2520Jenny%2520Brennan%2520and%2520Diego%2520Machado%2520and%2520Marissa%2520Giustina%2520and%2520MH%2520Tessler%2520and%2520Kamyu%2520Lee%2520and%2520Qiao%2520Zhang%2520and%2520Joss%2520Moore%2520and%2520Kaspar%2520Daugaard%2520and%2520Alexander%2520Fr%25C3%25B6mmgen%2520and%2520Jennifer%2520Beattie%2520and%2520Fred%2520Zhang%2520and%2520Daniel%2520Kasenberg%2520and%2520Ty%2520Geri%2520and%2520Danfeng%2520Qin%2520and%2520Gaurav%2520Singh%2520Tomar%2520and%2520Tom%2520Ouyang%2520and%2520Tianli%2520Yu%2520and%2520Luowei%2520Zhou%2520and%2520Rajiv%2520Mathews%2520and%2520Andy%2520Davis%2520and%2520Yaoyiran%2520Li%2520and%2520Jai%2520Gupta%2520and%2520Damion%2520Yates%2520and%2520Linda%2520Deng%2520and%2520Elizabeth%2520Kemp%2520and%2520Ga-Young%2520Joung%2520and%2520Sergei%2520Vassilvitskii%2520and%2520Mandy%2520Guo%2520and%2520Pallavi%2520LV%2520and%2520Dave%2520Dopson%2520and%2520Sami%2520Lachgar%2520and%2520Lara%2520McConnaughey%2520and%2520Himadri%2520Choudhury%2520and%2520Dragos%2520Dena%2520and%2520Aaron%2520Cohen%2520and%2520Joshua%2520Ainslie%2520and%2520Sergey%2520Levi%2520and%2520Parthasarathy%2520Gopavarapu%2520and%2520Polina%2520Zablotskaia%2520and%2520Hugo%2520Vallet%2520and%2520Sanaz%2520Bahargam%2520and%2520Xiaodan%2520Tang%2520and%2520Nenad%2520Tomasev%2520and%2520Ethan%2520Dyer%2520and%2520Daniel%2520Balle%2520and%2520Hongrae%2520Lee%2520and%2520William%2520Bono%2520and%2520Jorge%2520Gonzalez%2520Mendez%2520and%2520Vadim%2520Zubov%2520and%2520Shentao%2520Yang%2520and%2520Ivor%2520Rendulic%2520and%2520Yanyan%2520Zheng%2520and%2520Andrew%2520Hogue%2520and%2520Golan%2520Pundak%2520and%2520Ralph%2520Leith%2520and%2520Avishkar%2520Bhoopchand%2520and%2520Michael%2520Han%2520and%2520Mislav%2520%25C5%25BDani%25C4%2587%2520and%2520Tom%2520Schaul%2520and%2520Manolis%2520Delakis%2520and%2520Tejas%2520Iyer%2520and%2520Guanyu%2520Wang%2520and%2520Harman%2520Singh%2520and%2520Abdelrahman%2520Abdelhamed%2520and%2520Tara%2520Thomas%2520and%2520Siddhartha%2520Brahma%2520and%2520Hilal%2520Dib%2520and%2520Naveen%2520Kumar%2520and%2520Wenxuan%2520Zhou%2520and%2520Liang%2520Bai%2520and%2520Pushkar%2520Mishra%2520and%2520Jiao%2520Sun%2520and%2520Valentin%2520Anklin%2520and%2520Roykrong%2520Sukkerd%2520and%2520Lauren%2520Agubuzu%2520and%2520Anton%2520Briukhov%2520and%2520Anmol%2520Gulati%2520and%2520Maximilian%2520Sieb%2520and%2520Fabio%2520Pardo%2520and%2520Sara%2520Nasso%2520and%2520Junquan%2520Chen%2520and%2520Kexin%2520Zhu%2520and%2520Tiberiu%2520Sosea%2520and%2520Alex%2520Goldin%2520and%2520Keith%2520Rush%2520and%2520Spurthi%2520Amba%2520Hombaiah%2520and%2520Andreas%2520Noever%2520and%2520Allan%2520Zhou%2520and%2520Sam%2520Haves%2520and%2520Mary%2520Phuong%2520and%2520Jake%2520Ades%2520and%2520Yi-ting%2520Chen%2520and%2520Lin%2520Yang%2520and%2520Joseph%2520Pagadora%2520and%2520Stan%2520Bileschi%2520and%2520Victor%2520Cotruta%2520and%2520Rachel%2520Saputro%2520and%2520Arijit%2520Pramanik%2520and%2520Sean%2520Ammirati%2520and%2520Dan%2520Garrette%2520and%2520Kevin%2520Villela%2520and%2520Tim%2520Blyth%2520and%2520Canfer%2520Akbulut%2520and%2520Neha%2520Jha%2520and%2520Alban%2520Rrustemi%2520and%2520Arissa%2520Wongpanich%2520and%2520Chirag%2520Nagpal%2520and%2520Yonghui%2520Wu%2520and%2520Morgane%2520Rivi%25C3%25A8re%2520and%2520Sergey%2520Kishchenko%2520and%2520Pranesh%2520Srinivasan%2520and%2520Alice%2520Chen%2520and%2520Animesh%2520Sinha%2520and%2520Trang%2520Pham%2520and%2520Bill%2520Jia%2520and%2520Tom%2520Hennigan%2520and%2520Anton%2520Bakalov%2520and%2520Nithya%2520Attaluri%2520and%2520Drew%2520Garmon%2520and%2520Daniel%2520Rodriguez%2520and%2520Dawid%2520Wegner%2520and%2520Wenhao%2520Jia%2520and%2520Evan%2520Senter%2520and%2520Noah%2520Fiedel%2520and%2520Denis%2520Petek%2520and%2520Yuchuan%2520Liu%2520and%2520Cassidy%2520Hardin%2520and%2520Harshal%2520Tushar%2520Lehri%2520and%2520Joao%2520Carreira%2520and%2520Sara%2520Smoot%2520and%2520Marcel%2520Prasetya%2520and%2520Nami%2520Akazawa%2520and%2520Anca%2520Stefanoiu%2520and%2520Chia-Hua%2520Ho%2520and%2520Anelia%2520Angelova%2520and%2520Kate%2520Lin%2520and%2520Min%2520Kim%2520and%2520Charles%2520Chen%2520and%2520Marcin%2520Sieniek%2520and%2520Alice%2520Li%2520and%2520Tongfei%2520Guo%2520and%2520Sorin%2520Baltateanu%2520and%2520Pouya%2520Tafti%2520and%2520Michael%2520Wunder%2520and%2520Nadav%2520Olmert%2520and%2520Divyansh%2520Shukla%2520and%2520Jingwei%2520Shen%2520and%2520Neel%2520Kovelamudi%2520and%2520Balaji%2520Venkatraman%2520and%2520Seth%2520Neel%2520and%2520Romal%2520Thoppilan%2520and%2520Jerome%2520Connor%2520and%2520Frederik%2520Benzing%2520and%2520Axel%2520Stjerngren%2520and%2520Golnaz%2520Ghiasi%2520and%2520Alex%2520Polozov%2520and%2520Joshua%2520Howland%2520and%2520Theophane%2520Weber%2520and%2520Justin%2520Chiu%2520and%2520Ganesh%2520Poomal%2520Girirajan%2520and%2520Andreas%2520Terzis%2520and%2520Pidong%2520Wang%2520and%2520Fangda%2520Li%2520and%2520Yoav%2520Ben%2520Shalom%2520and%2520Dinesh%2520Tewari%2520and%2520Matthew%2520Denton%2520and%2520Roee%2520Aharoni%2520and%2520Norbert%2520Kalb%2520and%2520Heri%2520Zhao%2520and%2520Junlin%2520Zhang%2520and%2520Angelos%2520Filos%2520and%2520Matthew%2520Rahtz%2520and%2520Lalit%2520Jain%2520and%2520Connie%2520Fan%2520and%2520Vitor%2520Rodrigues%2520and%2520Ruth%2520Wang%2520and%2520Richard%2520Shin%2520and%2520Jacob%2520Austin%2520and%2520Roman%2520Ring%2520and%2520Mariella%2520Sanchez-Vargas%2520and%2520Mehadi%2520Hassen%2520and%2520Ido%2520Kessler%2520and%2520Uri%2520Alon%2520and%2520Gufeng%2520Zhang%2520and%2520Wenhu%2520Chen%2520and%2520Yenai%2520Ma%2520and%2520Xiance%2520Si%2520and%2520Le%2520Hou%2520and%2520Azalia%2520Mirhoseini%2520and%2520Marc%2520Wilson%2520and%2520Geoff%2520Bacon%2520and%2520Becca%2520Roelofs%2520and%2520Lei%2520Shu%2520and%2520Gautam%2520Vasudevan%2520and%2520Jonas%2520Adler%2520and%2520Artur%2520Dwornik%2520and%2520Tayfun%2520Terzi%2520and%2520Matt%2520Lawlor%2520and%2520Harry%2520Askham%2520and%2520Mike%2520Bernico%2520and%2520Xuanyi%2520Dong%2520and%2520Chris%2520Hidey%2520and%2520Kevin%2520Kilgour%2520and%2520Ga%25C3%25ABl%2520Liu%2520and%2520Surya%2520Bhupatiraju%2520and%2520Luke%2520Leonhard%2520and%2520Siqi%2520Zuo%2520and%2520Partha%2520Talukdar%2520and%2520Qing%2520Wei%2520and%2520Aliaksei%2520Severyn%2520and%2520V%25C3%25ADt%2520List%25C3%25ADk%2520and%2520Jong%2520Lee%2520and%2520Aditya%2520Tripathi%2520and%2520SK%2520Park%2520and%2520Yossi%2520Matias%2520and%2520Hao%2520Liu%2520and%2520Alex%2520Ruiz%2520and%2520Rajesh%2520Jayaram%2520and%2520Jackson%2520Tolins%2520and%2520Pierre%2520Marcenac%2520and%2520Yiming%2520Wang%2520and%2520Bryan%2520Seybold%2520and%2520Henry%2520Prior%2520and%2520Deepak%2520Sharma%2520and%2520Jack%2520Weber%2520and%2520Mikhail%2520Sirotenko%2520and%2520Yunhsuan%2520Sung%2520and%2520Dayou%2520Du%2520and%2520Ellie%2520Pavlick%2520and%2520Stefan%2520Zinke%2520and%2520Markus%2520Freitag%2520and%2520Max%2520Dylla%2520and%2520Montse%2520Gonzalez%2520Arenas%2520and%2520Natan%2520Potikha%2520and%2520Omer%2520Goldman%2520and%2520Connie%2520Tao%2520and%2520Rachita%2520Chhaparia%2520and%2520Maria%2520Voitovich%2520and%2520Pawan%2520Dogra%2520and%2520Andrija%2520Ra%25C5%25BEnatovi%25C4%2587%2520and%2520Zak%2520Tsai%2520and%2520Chong%2520You%2520and%2520Oleaser%2520Johnson%2520and%2520George%2520Tucker%2520and%2520Chenjie%2520Gu%2520and%2520Jae%2520Yoo%2520and%2520Maryam%2520Majzoubi%2520and%2520Valentin%2520Gabeur%2520and%2520Bahram%2520Raad%2520and%2520Rocky%2520Rhodes%2520and%2520Kashyap%2520Kolipaka%2520and%2520Heidi%2520Howard%2520and%2520Geta%2520Sampemane%2520and%2520Benny%2520Li%2520and%2520Chulayuth%2520Asawaroengchai%2520and%2520Duy%2520Nguyen%2520and%2520Chiyuan%2520Zhang%2520and%2520Timothee%2520Cour%2520and%2520Xinxin%2520Yu%2520and%2520Zhao%2520Fu%2520and%2520Joe%2520Jiang%2520and%2520Po-Sen%2520Huang%2520and%2520Gabriela%2520Surita%2520and%2520I%25C3%25B1aki%2520Iturrate%2520and%2520Yael%2520Karov%2520and%2520Michael%2520Collins%2520and%2520Martin%2520Baeuml%2520and%2520Fabian%2520Fuchs%2520and%2520Shilpa%2520Shetty%2520and%2520Swaroop%2520Ramaswamy%2520and%2520Sayna%2520Ebrahimi%2520and%2520Qiuchen%2520Guo%2520and%2520Jeremy%2520Shar%2520and%2520Gabe%2520Barth-Maron%2520and%2520Sravanti%2520Addepalli%2520and%2520Bryan%2520Richter%2520and%2520Chin-Yi%2520Cheng%2520and%2520Eug%25C3%25A9nie%2520Rives%2520and%2520Fei%2520Zheng%2520and%2520Johannes%2520Griesser%2520and%2520Nishanth%2520Dikkala%2520and%2520Yoel%2520Zeldes%2520and%2520Ilkin%2520Safarli%2520and%2520Dipanjan%2520Das%2520and%2520Himanshu%2520Srivastava%2520and%2520Sadh%2520MNM%2520Khan%2520and%2520Xin%2520Li%2520and%2520Aditya%2520Pandey%2520and%2520Larisa%2520Markeeva%2520and%2520Dan%2520Belov%2520and%2520Qiqi%2520Yan%2520and%2520Miko%25C5%2582aj%2520Rybi%25C5%2584ski%2520and%2520Tao%2520Chen%2520and%2520Megha%2520Nawhal%2520and%2520Michael%2520Quinn%2520and%2520Vineetha%2520Govindaraj%2520and%2520Sarah%2520York%2520and%2520Reed%2520Roberts%2520and%2520Roopal%2520Garg%2520and%2520Namrata%2520Godbole%2520and%2520Jake%2520Abernethy%2520and%2520Anil%2520Das%2520and%2520Lam%2520Nguyen%2520Thiet%2520and%2520Jonathan%2520Tompson%2520and%2520John%2520Nham%2520and%2520Neera%2520Vats%2520and%2520Ben%2520Caine%2520and%2520Wesley%2520Helmholz%2520and%2520Francesco%2520Pongetti%2520and%2520Yeongil%2520Ko%2520and%2520James%2520An%2520and%2520Clara%2520Huiyi%2520Hu%2520and%2520Yu-Cheng%2520Ling%2520and%2520Julia%2520Pawar%2520and%2520Robert%2520Leland%2520and%2520Keisuke%2520Kinoshita%2520and%2520Waleed%2520Khawaja%2520and%2520Marco%2520Selvi%2520and%2520Eugene%2520Ie%2520and%2520Danila%2520Sinopalnikov%2520and%2520Lev%2520Proleev%2520and%2520Nilesh%2520Tripuraneni%2520and%2520Michele%2520Bevilacqua%2520and%2520Seungji%2520Lee%2520and%2520Clayton%2520Sanford%2520and%2520Dan%2520Suh%2520and%2520Dustin%2520Tran%2520and%2520Jeff%2520Dean%2520and%2520Simon%2520Baumgartner%2520and%2520Jens%2520Heitkaemper%2520and%2520Sagar%2520Gubbi%2520and%2520Kristina%2520Toutanova%2520and%2520Yichong%2520Xu%2520and%2520Chandu%2520Thekkath%2520and%2520Keran%2520Rong%2520and%2520Palak%2520Jain%2520and%2520Annie%2520Xie%2520and%2520Yan%2520Virin%2520and%2520Yang%2520Li%2520and%2520Lubo%2520Litchev%2520and%2520Richard%2520Powell%2520and%2520Tarun%2520Bharti%2520and%2520Adam%2520Kraft%2520and%2520Nan%2520Hua%2520and%2520Marissa%2520Ikonomidis%2520and%2520Ayal%2520Hitron%2520and%2520Sanjiv%2520Kumar%2520and%2520Loic%2520Matthey%2520and%2520Sophie%2520Bridgers%2520and%2520Lauren%2520Lax%2520and%2520Ishaan%2520Malhi%2520and%2520Ondrej%2520Skopek%2520and%2520Ashish%2520Gupta%2520and%2520Jiawei%2520Cao%2520and%2520Mitchelle%2520Rasquinha%2520and%2520Siim%2520P%25C3%25B5der%2520and%2520Wojciech%2520Stokowiec%2520and%2520Nicholas%2520Roth%2520and%2520Guowang%2520Li%2520and%2520Micha%25C3%25ABl%2520Sander%2520and%2520Joshua%2520Kessinger%2520and%2520Vihan%2520Jain%2520and%2520Edward%2520Loper%2520and%2520Wonpyo%2520Park%2520and%2520Michal%2520Yarom%2520and%2520Liqun%2520Cheng%2520and%2520Guru%2520Guruganesh%2520and%2520Kanishka%2520Rao%2520and%2520Yan%2520Li%2520and%2520Catarina%2520Barros%2520and%2520Mikhail%2520Sushkov%2520and%2520Chun-Sung%2520Ferng%2520and%2520Rohin%2520Shah%2520and%2520Ophir%2520Aharoni%2520and%2520Ravin%2520Kumar%2520and%2520Tim%2520McConnell%2520and%2520Peiran%2520Li%2520and%2520Chen%2520Wang%2520and%2520Fernando%2520Pereira%2520and%2520Craig%2520Swanson%2520and%2520Fayaz%2520Jamil%2520and%2520Yan%2520Xiong%2520and%2520Anitha%2520Vijayakumar%2520and%2520Prakash%2520Shroff%2520and%2520Kedar%2520Soparkar%2520and%2520Jindong%2520Gu%2520and%2520Livio%2520Baldini%2520Soares%2520and%2520Eric%2520Wang%2520and%2520Kushal%2520Majmundar%2520and%2520Aurora%2520Wei%2520and%2520Kai%2520Bailey%2520and%2520Nora%2520Kassner%2520and%2520Chizu%2520Kawamoto%2520and%2520Goran%2520%25C5%25BDu%25C5%25BEi%25C4%2587%2520and%2520Victor%2520Gomes%2520and%2520Abhirut%2520Gupta%2520and%2520Michael%2520Guzman%2520and%2520Ishita%2520Dasgupta%2520and%2520Xinyi%2520Bai%2520and%2520Zhufeng%2520Pan%2520and%2520Francesco%2520Piccinno%2520and%2520Hadas%2520Natalie%2520Vogel%2520and%2520Octavio%2520Ponce%2520and%2520Adrian%2520Hutter%2520and%2520Paul%2520Chang%2520and%2520Pan-Pan%2520Jiang%2520and%2520Ionel%2520Gog%2520and%2520Vlad%2520Ionescu%2520and%2520James%2520Manyika%2520and%2520Fabian%2520Pedregosa%2520and%2520Harry%2520Ragan%2520and%2520Zach%2520Behrman%2520and%2520Ryan%2520Mullins%2520and%2520Coline%2520Devin%2520and%2520Aroonalok%2520Pyne%2520and%2520Swapnil%2520Gawde%2520and%2520Martin%2520Chadwick%2520and%2520Yiming%2520Gu%2520and%2520Sasan%2520Tavakkol%2520and%2520Andy%2520Twigg%2520and%2520Naman%2520Goyal%2520and%2520Ndidi%2520Elue%2520and%2520Anna%2520Goldie%2520and%2520Srinivasan%2520Venkatachary%2520and%2520Hongliang%2520Fei%2520and%2520Ziqiang%2520Feng%2520and%2520Marvin%2520Ritter%2520and%2520Isabel%2520Leal%2520and%2520Sudeep%2520Dasari%2520and%2520Pei%2520Sun%2520and%2520Alif%2520Raditya%2520Rochman%2520and%2520Brendan%2520O%2527Donoghue%2520and%2520Yuchen%2520Liu%2520and%2520Jim%2520Sproch%2520and%2520Kai%2520Chen%2520and%2520Natalie%2520Clay%2520and%2520Slav%2520Petrov%2520and%2520Sailesh%2520Sidhwani%2520and%2520Ioana%2520Mihailescu%2520and%2520Alex%2520Panagopoulos%2520and%2520AJ%2520Piergiovanni%2520and%2520Yunfei%2520Bai%2520and%2520George%2520Powell%2520and%2520Deep%2520Karkhanis%2520and%2520Trevor%2520Yacovone%2520and%2520Petr%2520Mitrichev%2520and%2520Joe%2520Kovac%2520and%2520Dave%2520Uthus%2520and%2520Amir%2520Yazdanbakhsh%2520and%2520David%2520Amos%2520and%2520Steven%2520Zheng%2520and%2520Bing%2520Zhang%2520and%2520Jin%2520Miao%2520and%2520Bhuvana%2520Ramabhadran%2520and%2520Soroush%2520Radpour%2520and%2520Shantanu%2520Thakoor%2520and%2520Josh%2520Newlan%2520and%2520Oran%2520Lang%2520and%2520Orion%2520Jankowski%2520and%2520Shikhar%2520Bharadwaj%2520and%2520Jean-Michel%2520Sarr%2520and%2520Shereen%2520Ashraf%2520and%2520Sneha%2520Mondal%2520and%2520Jun%2520Yan%2520and%2520Ankit%2520Singh%2520Rawat%2520and%2520Sarmishta%2520Velury%2520and%2520Greg%2520Kochanski%2520and%2520Tom%2520Eccles%2520and%2520Franz%2520Och%2520and%2520Abhanshu%2520Sharma%2520and%2520Ethan%2520Mahintorabi%2520and%2520Alex%2520Gurney%2520and%2520Carrie%2520Muir%2520and%2520Vered%2520Cohen%2520and%2520Saksham%2520Thakur%2520and%2520Adam%2520Bloniarz%2520and%2520Asier%2520Mujika%2520and%2520Alexander%2520Pritzel%2520and%2520Paul%2520Caron%2520and%2520Altaf%2520Rahman%2520and%2520Fiona%2520Lang%2520and%2520Yasumasa%2520Onoe%2520and%2520Petar%2520Sirkovic%2520and%2520Jay%2520Hoover%2520and%2520Ying%2520Jian%2520and%2520Pablo%2520Duque%2520and%2520Arun%2520Narayanan%2520and%2520David%2520Soergel%2520and%2520Alex%2520Haig%2520and%2520Loren%2520Maggiore%2520and%2520Shyamal%2520Buch%2520and%2520Josef%2520Dean%2520and%2520Ilya%2520Figotin%2520and%2520Igor%2520Karpov%2520and%2520Shaleen%2520Gupta%2520and%2520Denny%2520Zhou%2520and%2520Muhuan%2520Huang%2520and%2520Ashwin%2520Vaswani%2520and%2520Christopher%2520Semturs%2520and%2520Kaushik%2520Shivakumar%2520and%2520Yu%2520Watanabe%2520and%2520Vinodh%2520Kumar%2520Rajendran%2520and%2520Eva%2520Lu%2520and%2520Yanhan%2520Hou%2520and%2520Wenting%2520Ye%2520and%2520Shikhar%2520Vashishth%2520and%2520Nana%2520Nti%2520and%2520Vytenis%2520Sakenas%2520and%2520Darren%2520Ni%2520and%2520Doug%2520DeCarlo%2520and%2520Michael%2520Bendersky%2520and%2520Sumit%2520Bagri%2520and%2520Nacho%2520Cano%2520and%2520Elijah%2520Peake%2520and%2520Simon%2520Tokumine%2520and%2520Varun%2520Godbole%2520and%2520Carlos%2520Gu%25C3%25ADa%2520and%2520Tanya%2520Lando%2520and%2520Vittorio%2520Selo%2520and%2520Seher%2520Ellis%2520and%2520Danny%2520Tarlow%2520and%2520Daniel%2520Gillick%2520and%2520Alessandro%2520Epasto%2520and%2520Siddhartha%2520Reddy%2520Jonnalagadda%2520and%2520Meng%2520Wei%2520and%2520Meiyan%2520Xie%2520and%2520Ankur%2520Taly%2520and%2520Michela%2520Paganini%2520and%2520Mukund%2520Sundararajan%2520and%2520Daniel%2520Toyama%2520and%2520Ting%2520Yu%2520and%2520Dessie%2520Petrova%2520and%2520Aneesh%2520Pappu%2520and%2520Rohan%2520Agrawal%2520and%2520Senaka%2520Buthpitiya%2520and%2520Justin%2520Frye%2520and%2520Thomas%2520Buschmann%2520and%2520Remi%2520Crocker%2520and%2520Marco%2520Tagliasacchi%2520and%2520Mengchao%2520Wang%2520and%2520Da%2520Huang%2520and%2520Sagi%2520Perel%2520and%2520Brian%2520Wieder%2520and%2520Hideto%2520Kazawa%2520and%2520Weiyue%2520Wang%2520and%2520Jeremy%2520Cole%2520and%2520Himanshu%2520Gupta%2520and%2520Ben%2520Golan%2520and%2520Seojin%2520Bang%2520and%2520Nitish%2520Kulkarni%2520and%2520Ken%2520Franko%2520and%2520Casper%2520Liu%2520and%2520Doug%2520Reid%2520and%2520Sid%2520Dalmia%2520and%2520Jay%2520Whang%2520and%2520Kevin%2520Cen%2520and%2520Prasha%2520Sundaram%2520and%2520Johan%2520Ferret%2520and%2520Berivan%2520Isik%2520and%2520Lucian%2520Ionita%2520and%2520Guan%2520Sun%2520and%2520Anna%2520Shekhawat%2520and%2520Muqthar%2520Mohammad%2520and%2520Philip%2520Pham%2520and%2520Ronny%2520Huang%2520and%2520Karthik%2520Raman%2520and%2520Xingyi%2520Zhou%2520and%2520Ross%2520Mcilroy%2520and%2520Austin%2520Myers%2520and%2520Sheng%2520Peng%2520and%2520Jacob%2520Scott%2520and%2520Paul%2520Covington%2520and%2520Sofia%2520Erell%2520and%2520Pratik%2520Joshi%2520and%2520Jo%25C3%25A3o%2520Gabriel%2520Oliveira%2520and%2520Natasha%2520Noy%2520and%2520Tajwar%2520Nasir%2520and%2520Jake%2520Walker%2520and%2520Vera%2520Axelrod%2520and%2520Tim%2520Dozat%2520and%2520Pu%2520Han%2520and%2520Chun-Te%2520Chu%2520and%2520Eugene%2520Weinstein%2520and%2520Anand%2520Shukla%2520and%2520Shreyas%2520Chandrakaladharan%2520and%2520Petra%2520Poklukar%2520and%2520Bonnie%2520Li%2520and%2520Ye%2520Jin%2520and%2520Prem%2520Eruvbetine%2520and%2520Steven%2520Hansen%2520and%2520Avigail%2520Dabush%2520and%2520Alon%2520Jacovi%2520and%2520Samrat%2520Phatale%2520and%2520Chen%2520Zhu%2520and%2520Steven%2520Baker%2520and%2520Mo%2520Shomrat%2520and%2520Yang%2520Xiao%2520and%2520Jean%2520Pouget-Abadie%2520and%2520Mingyang%2520Zhang%2520and%2520Fanny%2520Wei%2520and%2520Yang%2520Song%2520and%2520Helen%2520King%2520and%2520Yiling%2520Huang%2520and%2520Yun%2520Zhu%2520and%2520Ruoxi%2520Sun%2520and%2520Juliana%2520Vicente%2520Franco%2520and%2520Chu-Cheng%2520Lin%2520and%2520Sho%2520Arora%2520and%2520%2520Hui%2520and%2520%2520Li%2520and%2520Vivian%2520Xia%2520and%2520Luke%2520Vilnis%2520and%2520Mariano%2520Schain%2520and%2520Kaiz%2520Alarakyia%2520and%2520Laurel%2520Prince%2520and%2520Aaron%2520Phillips%2520and%2520Caleb%2520Habtegebriel%2520and%2520Luyao%2520Xu%2520and%2520Huan%2520Gui%2520and%2520Santiago%2520Ontanon%2520and%2520Lora%2520Aroyo%2520and%2520Karan%2520Gill%2520and%2520Peggy%2520Lu%2520and%2520Yash%2520Katariya%2520and%2520Dhruv%2520Madeka%2520and%2520Shankar%2520Krishnan%2520and%2520Shubha%2520Srinivas%2520Raghvendra%2520and%2520James%2520Freedman%2520and%2520Yi%2520Tay%2520and%2520Gaurav%2520Menghani%2520and%2520Peter%2520Choy%2520and%2520Nishita%2520Shetty%2520and%2520Dan%2520Abolafia%2520and%2520Doron%2520Kukliansky%2520and%2520Edward%2520Chou%2520and%2520Jared%2520Lichtarge%2520and%2520Ken%2520Burke%2520and%2520Ben%2520Coleman%2520and%2520Dee%2520Guo%2520and%2520Larry%2520Jin%2520and%2520Indro%2520Bhattacharya%2520and%2520Victoria%2520Langston%2520and%2520Yiming%2520Li%2520and%2520Suyog%2520Kotecha%2520and%2520Alex%2520Yakubovich%2520and%2520Xinyun%2520Chen%2520and%2520Petre%2520Petrov%2520and%2520Tolly%2520Powell%2520and%2520Yanzhang%2520He%2520and%2520Corbin%2520Quick%2520and%2520Kanav%2520Garg%2520and%2520Dawsen%2520Hwang%2520and%2520Yang%2520Lu%2520and%2520Srinadh%2520Bhojanapalli%2520and%2520Kristian%2520Kjems%2520and%2520Ramin%2520Mehran%2520and%2520Aaron%2520Archer%2520and%2520Hado%2520van%2520Hasselt%2520and%2520Ashwin%2520Balakrishna%2520and%2520JK%2520Kearns%2520and%2520Meiqi%2520Guo%2520and%2520Jason%2520Riesa%2520and%2520Mikita%2520Sazanovich%2520and%2520Xu%2520Gao%2520and%2520Chris%2520Sauer%2520and%2520Chengrun%2520Yang%2520and%2520XiangHai%2520Sheng%2520and%2520Thomas%2520Jimma%2520and%2520Wouter%2520Van%2520Gansbeke%2520and%2520Vitaly%2520Nikolaev%2520and%2520Wei%2520Wei%2520and%2520Katie%2520Millican%2520and%2520Ruizhe%2520Zhao%2520and%2520Justin%2520Snyder%2520and%2520Levent%2520Bolelli%2520and%2520Maura%2520O%2527Brien%2520and%2520Shawn%2520Xu%2520and%2520Fei%2520Xia%2520and%2520Wentao%2520Yuan%2520and%2520Arvind%2520Neelakantan%2520and%2520David%2520Barker%2520and%2520Sachin%2520Yadav%2520and%2520Hannah%2520Kirkwood%2520and%2520Farooq%2520Ahmad%2520and%2520Joel%2520Wee%2520and%2520Jordan%2520Grimstad%2520and%2520Boyu%2520Wang%2520and%2520Matthew%2520Wiethoff%2520and%2520Shane%2520Settle%2520and%2520Miaosen%2520Wang%2520and%2520Charles%2520Blundell%2520and%2520Jingjing%2520Chen%2520and%2520Chris%2520Duvarney%2520and%2520Grace%2520Hu%2520and%2520Olaf%2520Ronneberger%2520and%2520Alex%2520Lee%2520and%2520Yuanzhen%2520Li%2520and%2520Abhishek%2520Chakladar%2520and%2520Alena%2520Butryna%2520and%2520Georgios%2520Evangelopoulos%2520and%2520Guillaume%2520Desjardins%2520and%2520Jonni%2520Kanerva%2520and%2520Henry%2520Wang%2520and%2520Averi%2520Nowak%2520and%2520Nick%2520Li%2520and%2520Alyssa%2520Loo%2520and%2520Art%2520Khurshudov%2520and%2520Laurent%2520El%2520Shafey%2520and%2520Nagabhushan%2520Baddi%2520and%2520Karel%2520Lenc%2520and%2520Yasaman%2520Razeghi%2520and%2520Tom%2520Lieber%2520and%2520Amer%2520Sinha%2520and%2520Xiao%2520Ma%2520and%2520Yao%2520Su%2520and%2520James%2520Huang%2520and%2520Asahi%2520Ushio%2520and%2520Hanna%2520Klimczak-Pluci%25C5%2584ska%2520and%2520Kareem%2520Mohamed%2520and%2520JD%2520Chen%2520and%2520Simon%2520Osindero%2520and%2520Stav%2520Ginzburg%2520and%2520Lampros%2520Lamprou%2520and%2520Vasilisa%2520Bashlovkina%2520and%2520Duc-Hieu%2520Tran%2520and%2520Ali%2520Khodaei%2520and%2520Ankit%2520Anand%2520and%2520Yixian%2520Di%2520and%2520Ramy%2520Eskander%2520and%2520Manish%2520Reddy%2520Vuyyuru%2520and%2520Jasmine%2520Liu%2520and%2520Aishwarya%2520Kamath%2520and%2520Roman%2520Goldenberg%2520and%2520Mathias%2520Bellaiche%2520and%2520Juliette%2520Pluto%2520and%2520Bill%2520Rosgen%2520and%2520Hassan%2520Mansoor%2520and%2520William%2520Wong%2520and%2520Suhas%2520Ganesh%2520and%2520Eric%2520Bailey%2520and%2520Scott%2520Baird%2520and%2520Dan%2520Deutsch%2520and%2520Jinoo%2520Baek%2520and%2520Xuhui%2520Jia%2520and%2520Chansoo%2520Lee%2520and%2520Abe%2520Friesen%2520and%2520Nathaniel%2520Braun%2520and%2520Kate%2520Lee%2520and%2520Amayika%2520Panda%2520and%2520Steven%2520M.%2520Hernandez%2520and%2520Duncan%2520Williams%2520and%2520Jianqiao%2520Liu%2520and%2520Ethan%2520Liang%2520and%2520Arnaud%2520Autef%2520and%2520Emily%2520Pitler%2520and%2520Deepali%2520Jain%2520and%2520Phoebe%2520Kirk%2520and%2520Oskar%2520Bunyan%2520and%2520Jaume%2520Sanchez%2520Elias%2520and%2520Tongxin%2520Yin%2520and%2520Machel%2520Reid%2520and%2520Aedan%2520Pope%2520and%2520Nikita%2520Putikhin%2520and%2520Bidisha%2520Samanta%2520and%2520Sergio%2520Guadarrama%2520and%2520Dahun%2520Kim%2520and%2520Simon%2520Rowe%2520and%2520Marcella%2520Valentine%2520and%2520Geng%2520Yan%2520and%2520Alex%2520Salcianu%2520and%2520David%2520Silver%2520and%2520Gan%2520Song%2520and%2520Richa%2520Singh%2520and%2520Shuai%2520Ye%2520and%2520Hannah%2520DeBalsi%2520and%2520Majd%2520Al%2520Merey%2520and%2520Eran%2520Ofek%2520and%2520Albert%2520Webson%2520and%2520Shibl%2520Mourad%2520and%2520Ashwin%2520Kakarla%2520and%2520Silvio%2520Lattanzi%2520and%2520Nick%2520Roy%2520and%2520Evgeny%2520Sluzhaev%2520and%2520Christina%2520Butterfield%2520and%2520Alessio%2520Tonioni%2520and%2520Nathan%2520Waters%2520and%2520Sudhindra%2520Kopalle%2520and%2520Jason%2520Chase%2520and%2520James%2520Cohan%2520and%2520Girish%2520Ramchandra%2520Rao%2520and%2520Robert%2520Berry%2520and%2520Michael%2520Voznesensky%2520and%2520Shuguang%2520Hu%2520and%2520Kristen%2520Chiafullo%2520and%2520Sharat%2520Chikkerur%2520and%2520George%2520Scrivener%2520and%2520Ivy%2520Zheng%2520and%2520Jeremy%2520Wiesner%2520and%2520Wolfgang%2520Macherey%2520and%2520Timothy%2520Lillicrap%2520and%2520Fei%2520Liu%2520and%2520Brian%2520Walker%2520and%2520David%2520Welling%2520and%2520Elinor%2520Davies%2520and%2520Yangsibo%2520Huang%2520and%2520Lijie%2520Ren%2520and%2520Nir%2520Shabat%2520and%2520Alessandro%2520Agostini%2520and%2520Mariko%2520Iinuma%2520and%2520Dustin%2520Zelle%2520and%2520Rohit%2520Sathyanarayana%2520and%2520Andrea%2520D%2527olimpio%2520and%2520Morgan%2520Redshaw%2520and%2520Matt%2520Ginsberg%2520and%2520Ashwin%2520Murthy%2520and%2520Mark%2520Geller%2520and%2520Tatiana%2520Matejovicova%2520and%2520Ayan%2520Chakrabarti%2520and%2520Ryan%2520Julian%2520and%2520Christine%2520Chan%2520and%2520Qiong%2520Hu%2520and%2520Daniel%2520Jarrett%2520and%2520Manu%2520Agarwal%2520and%2520Jeshwanth%2520Challagundla%2520and%2520Tao%2520Li%2520and%2520Sandeep%2520Tata%2520and%2520Wen%2520Ding%2520and%2520Maya%2520Meng%2520and%2520Zhuyun%2520Dai%2520and%2520Giulia%2520Vezzani%2520and%2520Shefali%2520Garg%2520and%2520Jannis%2520Bulian%2520and%2520Mary%2520Jasarevic%2520and%2520Honglong%2520Cai%2520and%2520Harish%2520Rajamani%2520and%2520Adam%2520Santoro%2520and%2520Florian%2520Hartmann%2520and%2520Chen%2520Liang%2520and%2520Bartek%2520Perz%2520and%2520Apoorv%2520Jindal%2520and%2520Fan%2520Bu%2520and%2520Sungyong%2520Seo%2520and%2520Ryan%2520Poplin%2520and%2520Adrian%2520Goedeckemeyer%2520and%2520Badih%2520Ghazi%2520and%2520Nikhil%2520Khadke%2520and%2520Leon%2520Liu%2520and%2520Kevin%2520Mather%2520and%2520Mingda%2520Zhang%2520and%2520Ali%2520Shah%2520and%2520Alex%2520Chen%2520and%2520Jinliang%2520Wei%2520and%2520Keshav%2520Shivam%2520and%2520Yuan%2520Cao%2520and%2520Donghyun%2520Cho%2520and%2520Angelo%2520Scorza%2520Scarpati%2520and%2520Michael%2520Moffitt%2520and%2520Clara%2520Barbu%2520and%2520Ivan%2520Jurin%2520and%2520Ming-Wei%2520Chang%2520and%2520Hongbin%2520Liu%2520and%2520Hao%2520Zheng%2520and%2520Shachi%2520Dave%2520and%2520Christine%2520Kaeser-Chen%2520and%2520Xiaobin%2520Yu%2520and%2520Alvin%2520Abdagic%2520and%2520Lucas%2520Gonzalez%2520and%2520Yanping%2520Huang%2520and%2520Peilin%2520Zhong%2520and%2520Cordelia%2520Schmid%2520and%2520Bryce%2520Petrini%2520and%2520Alex%2520Wertheim%2520and%2520Jifan%2520Zhu%2520and%2520Hoang%2520Nguyen%2520and%2520Kaiyang%2520Ji%2520and%2520Yanqi%2520Zhou%2520and%2520Tao%2520Zhou%2520and%2520Fangxiaoyu%2520Feng%2520and%2520Regev%2520Cohen%2520and%2520David%2520Rim%2520and%2520Shubham%2520Milind%2520Phal%2520and%2520Petko%2520Georgiev%2520and%2520Ariel%2520Brand%2520and%2520Yue%2520Ma%2520and%2520Wei%2520Li%2520and%2520Somit%2520Gupta%2520and%2520Chao%2520Wang%2520and%2520Pavel%2520Dubov%2520and%2520Jean%2520Tarbouriech%2520and%2520Kingshuk%2520Majumder%2520and%2520Huijian%2520Li%2520and%2520Norman%2520Rink%2520and%2520Apurv%2520Suman%2520and%2520Yang%2520Guo%2520and%2520Yinghao%2520Sun%2520and%2520Arun%2520Nair%2520and%2520Xiaowei%2520Xu%2520and%2520Mohamed%2520Elhawaty%2520and%2520Rodrigo%2520Cabrera%2520and%2520Guangxing%2520Han%2520and%2520Julian%2520Eisenschlos%2520and%2520Junwen%2520Bai%2520and%2520Yuqi%2520Li%2520and%2520Yamini%2520Bansal%2520and%2520Thibault%2520Sellam%2520and%2520Mina%2520Khan%2520and%2520Hung%2520Nguyen%2520and%2520Justin%2520Mao-Jones%2520and%2520Nikos%2520Parotsidis%2520and%2520Jake%2520Marcus%2520and%2520Cindy%2520Fan%2520and%2520Roland%2520Zimmermann%2520and%2520Yony%2520Kochinski%2520and%2520Laura%2520Graesser%2520and%2520Feryal%2520Behbahani%2520and%2520Alvaro%2520Caceres%2520and%2520Michael%2520Riley%2520and%2520Patrick%2520Kane%2520and%2520Sandra%2520Lefdal%2520and%2520Rob%2520Willoughby%2520and%2520Paul%2520Vicol%2520and%2520Lun%2520Wang%2520and%2520Shujian%2520Zhang%2520and%2520Ashleah%2520Gill%2520and%2520Yu%2520Liang%2520and%2520Gautam%2520Prasad%2520and%2520Soroosh%2520Mariooryad%2520and%2520Mehran%2520Kazemi%2520and%2520Zifeng%2520Wang%2520and%2520Kritika%2520Muralidharan%2520and%2520Paul%2520Voigtlaender%2520and%2520Jeffrey%2520Zhao%2520and%2520Huanjie%2520Zhou%2520and%2520Nina%2520D%2527Souza%2520and%2520Aditi%2520Mavalankar%2520and%2520S%25C3%25A9b%2520Arnold%2520and%2520Nick%2520Young%2520and%2520Obaid%2520Sarvana%2520and%2520Chace%2520Lee%2520and%2520Milad%2520Nasr%2520and%2520Tingting%2520Zou%2520and%2520Seokhwan%2520Kim%2520and%2520Lukas%2520Haas%2520and%2520Kaushal%2520Patel%2520and%2520Neslihan%2520Bulut%2520and%2520David%2520Parkinson%2520and%2520Courtney%2520Biles%2520and%2520Dmitry%2520Kalashnikov%2520and%2520Chi%2520Ming%2520To%2520and%2520Aviral%2520Kumar%2520and%2520Jessica%2520Austin%2520and%2520Alex%2520Greve%2520and%2520Lei%2520Zhang%2520and%2520Megha%2520Goel%2520and%2520Yeqing%2520Li%2520and%2520Sergey%2520Yaroshenko%2520and%2520Max%2520Chang%2520and%2520Abhishek%2520Jindal%2520and%2520Geoff%2520Clark%2520and%2520Hagai%2520Taitelbaum%2520and%2520Dale%2520Johnson%2520and%2520Ofir%2520Roval%2520and%2520Jeongwoo%2520Ko%2520and%2520Anhad%2520Mohananey%2520and%2520Christian%2520Schuler%2520and%2520Shenil%2520Dodhia%2520and%2520Ruichao%2520Li%2520and%2520Kazuki%2520Osawa%2520and%2520Claire%2520Cui%2520and%2520Peng%2520Xu%2520and%2520Rushin%2520Shah%2520and%2520Tao%2520Huang%2520and%2520Ela%2520Gruzewska%2520and%2520Nathan%2520Clement%2520and%2520Mudit%2520Verma%2520and%2520Olcan%2520Sercinoglu%2520and%2520Hai%2520Qian%2520and%2520Viral%2520Shah%2520and%2520Masa%2520Yamaguchi%2520and%2520Abhinit%2520Modi%2520and%2520Takahiro%2520Kosakai%2520and%2520Thomas%2520Strohmann%2520and%2520Junhao%2520Zeng%2520and%2520Beliz%2520Gunel%2520and%2520Jun%2520Qian%2520and%2520Austin%2520Tarango%2520and%2520Krzysztof%2520Jastrz%25C4%2599bski%2520and%2520Robert%2520David%2520and%2520Jyn%2520Shan%2520and%2520Parker%2520Schuh%2520and%2520Kunal%2520Lad%2520and%2520Willi%2520Gierke%2520and%2520Mukundan%2520Madhavan%2520and%2520Xinyi%2520Chen%2520and%2520Mark%2520Kurzeja%2520and%2520Rebeca%2520Santamaria-Fernandez%2520and%2520Dawn%2520Chen%2520and%2520Alexandra%2520Cordell%2520and%2520Yuri%2520Chervonyi%2520and%2520Frankie%2520Garcia%2520and%2520Nithish%2520Kannen%2520and%2520Vincent%2520Perot%2520and%2520Nan%2520Ding%2520and%2520Shlomi%2520Cohen-Ganor%2520and%2520Victor%2520Lavrenko%2520and%2520Junru%2520Wu%2520and%2520Georgie%2520Evans%2520and%2520Cicero%2520Nogueira%2520dos%2520Santos%2520and%2520Madhavi%2520Sewak%2520and%2520Ashley%2520Brown%2520and%2520Andrew%2520Hard%2520and%2520Joan%2520Puigcerver%2520and%2520Zeyu%2520Zheng%2520and%2520Yizhong%2520Liang%2520and%2520Evgeny%2520Gladchenko%2520and%2520Reeve%2520Ingle%2520and%2520Uri%2520First%2520and%2520Pierre%2520Sermanet%2520and%2520Charlotte%2520Magister%2520and%2520Mihajlo%2520Velimirovi%25C4%2587%2520and%2520Sashank%2520Reddi%2520and%2520Susanna%2520Ricco%2520and%2520Eirikur%2520Agustsson%2520and%2520Hartwig%2520Adam%2520and%2520Nir%2520Levine%2520and%2520David%2520Gaddy%2520and%2520Dan%2520Holtmann-Rice%2520and%2520Xuanhui%2520Wang%2520and%2520Ashutosh%2520Sathe%2520and%2520Abhijit%2520Guha%2520Roy%2520and%2520Bla%25C5%25BE%2520Bratani%25C4%258D%2520and%2520Alen%2520Carin%2520and%2520Harsh%2520Mehta%2520and%2520Silvano%2520Bonacina%2520and%2520Nicola%2520De%2520Cao%2520and%2520Mara%2520Finkelstein%2520and%2520Verena%2520Rieser%2520and%2520Xinyi%2520Wu%2520and%2520Florent%2520Altch%25C3%25A9%2520and%2520Dylan%2520Scandinaro%2520and%2520Li%2520Li%2520and%2520Nino%2520Vieillard%2520and%2520Nikhil%2520Sethi%2520and%2520Garrett%2520Tanzer%2520and%2520Zhi%2520Xing%2520and%2520Shibo%2520Wang%2520and%2520Parul%2520Bhatia%2520and%2520Gui%2520Citovsky%2520and%2520Thomas%2520Anthony%2520and%2520Sharon%2520Lin%2520and%2520Tianze%2520Shi%2520and%2520Shoshana%2520Jakobovits%2520and%2520Gena%2520Gibson%2520and%2520Raj%2520Apte%2520and%2520Lisa%2520Lee%2520and%2520Mingqing%2520Chen%2520and%2520Arunkumar%2520Byravan%2520and%2520Petros%2520Maniatis%2520and%2520Kellie%2520Webster%2520and%2520Andrew%2520Dai%2520and%2520Pu-Chin%2520Chen%2520and%2520Jiaqi%2520Pan%2520and%2520Asya%2520Fadeeva%2520and%2520Zach%2520Gleicher%2520and%2520Thang%2520Luong%2520and%2520Niket%2520Kumar%2520Bhumihar%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520introduce%2520the%2520Gemini%25202.X%2520model%2520family%253A%2520Gemini%25202.5%2520Pro%2520and%250AGemini%25202.5%2520Flash%252C%2520as%2520well%2520as%2520our%2520earlier%2520Gemini%25202.0%2520Flash%2520and%2520Flash-Lite%250Amodels.%2520Gemini%25202.5%2520Pro%2520is%2520our%2520most%2520capable%2520model%2520yet%252C%2520achieving%2520SoTA%250Aperformance%2520on%2520frontier%2520coding%2520and%2520reasoning%2520benchmarks.%2520In%2520addition%2520to%2520its%250Aincredible%2520coding%2520and%2520reasoning%2520skills%252C%2520Gemini%25202.5%2520Pro%2520is%2520a%2520thinking%2520model%2520that%250Aexcels%2520at%2520multimodal%2520understanding%2520and%2520it%2520is%2520now%2520able%2520to%2520process%2520up%2520to%25203%2520hours%250Aof%2520video%2520content.%2520Its%2520unique%2520combination%2520of%2520long%2520context%252C%2520multimodal%2520and%250Areasoning%2520capabilities%2520can%2520be%2520combined%2520to%2520unlock%2520new%2520agentic%2520workflows.%2520Gemini%250A2.5%2520Flash%2520provides%2520excellent%2520reasoning%2520abilities%2520at%2520a%2520fraction%2520of%2520the%2520compute%250Aand%2520latency%2520requirements%2520and%2520Gemini%25202.0%2520Flash%2520and%2520Flash-Lite%2520provide%2520high%250Aperformance%2520at%2520low%2520latency%2520and%2520cost.%2520Taken%2520together%252C%2520the%2520Gemini%25202.X%2520model%250Ageneration%2520spans%2520the%2520full%2520Pareto%2520frontier%2520of%2520model%2520capability%2520vs%2520cost%252C%2520allowing%250Ausers%2520to%2520explore%2520the%2520boundaries%2520of%2520what%2520is%2520possible%2520with%2520complex%2520agentic%250Aproblem%2520solving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06261v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemini%202.5%3A%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%0A%20%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities&entry.906535625=Gheorghe%20Comanici%20and%20Eric%20Bieber%20and%20Mike%20Schaekermann%20and%20Ice%20Pasupat%20and%20Noveen%20Sachdeva%20and%20Inderjit%20Dhillon%20and%20Marcel%20Blistein%20and%20Ori%20Ram%20and%20Dan%20Zhang%20and%20Evan%20Rosen%20and%20Luke%20Marris%20and%20Sam%20Petulla%20and%20Colin%20Gaffney%20and%20Asaf%20Aharoni%20and%20Nathan%20Lintz%20and%20Tiago%20Cardal%20Pais%20and%20Henrik%20Jacobsson%20and%20Idan%20Szpektor%20and%20Nan-Jiang%20Jiang%20and%20Krishna%20Haridasan%20and%20Ahmed%20Omran%20and%20Nikunj%20Saunshi%20and%20Dara%20Bahri%20and%20Gaurav%20Mishra%20and%20Eric%20Chu%20and%20Toby%20Boyd%20and%20Brad%20Hekman%20and%20Aaron%20Parisi%20and%20Chaoyi%20Zhang%20and%20Kornraphop%20Kawintiranon%20and%20Tania%20Bedrax-Weiss%20and%20Oliver%20Wang%20and%20Ya%20Xu%20and%20Ollie%20Purkiss%20and%20Uri%20Mendlovic%20and%20Ila%C3%AF%20Deutel%20and%20Nam%20Nguyen%20and%20Adam%20Langley%20and%20Flip%20Korn%20and%20Lucia%20Rossazza%20and%20Alexandre%20Ram%C3%A9%20and%20Sagar%20Waghmare%20and%20Helen%20Miller%20and%20Nathan%20Byrd%20and%20Ashrith%20Sheshan%20and%20Raia%20Hadsell%20Sangnie%20Bhardwaj%20and%20Pawel%20Janus%20and%20Tero%20Rissa%20and%20Dan%20Horgan%20and%20Sharon%20Silver%20and%20Ayzaan%20Wahid%20and%20Sergey%20Brin%20and%20Yves%20Raimond%20and%20Klemen%20Kloboves%20and%20Cindy%20Wang%20and%20Nitesh%20Bharadwaj%20Gundavarapu%20and%20Ilia%20Shumailov%20and%20Bo%20Wang%20and%20Mantas%20Pajarskas%20and%20Joe%20Heyward%20and%20Martin%20Nikoltchev%20and%20Maciej%20Kula%20and%20Hao%20Zhou%20and%20Zachary%20Garrett%20and%20Sushant%20Kafle%20and%20Sercan%20Arik%20and%20Ankita%20Goel%20and%20Mingyao%20Yang%20and%20Jiho%20Park%20and%20Koji%20Kojima%20and%20Parsa%20Mahmoudieh%20and%20Koray%20Kavukcuoglu%20and%20Grace%20Chen%20and%20Doug%20Fritz%20and%20Anton%20Bulyenov%20and%20Sudeshna%20Roy%20and%20Dimitris%20Paparas%20and%20Hadar%20Shemtov%20and%20Bo-Juen%20Chen%20and%20Robin%20Strudel%20and%20David%20Reitter%20and%20Aurko%20Roy%20and%20Andrey%20Vlasov%20and%20Changwan%20Ryu%20and%20Chas%20Leichner%20and%20Haichuan%20Yang%20and%20Zelda%20Mariet%20and%20Denis%20Vnukov%20and%20Tim%20Sohn%20and%20Amy%20Stuart%20and%20Wei%20Liang%20and%20Minmin%20Chen%20and%20Praynaa%20Rawlani%20and%20Christy%20Koh%20and%20JD%20Co-Reyes%20and%20Guangda%20Lai%20and%20Praseem%20Banzal%20and%20Dimitrios%20Vytiniotis%20and%20Jieru%20Mei%20and%20Mu%20Cai%20and%20Mohammed%20Badawi%20and%20Corey%20Fry%20and%20Ale%20Hartman%20and%20Daniel%20Zheng%20and%20Eric%20Jia%20and%20James%20Keeling%20and%20Annie%20Louis%20and%20Ying%20Chen%20and%20Efren%20Robles%20and%20Wei-Chih%20Hung%20and%20Howard%20Zhou%20and%20Nikita%20Saxena%20and%20Sonam%20Goenka%20and%20Olivia%20Ma%20and%20Zach%20Fisher%20and%20Mor%20Hazan%20Taege%20and%20Emily%20Graves%20and%20David%20Steiner%20and%20Yujia%20Li%20and%20Sarah%20Nguyen%20and%20Rahul%20Sukthankar%20and%20Joe%20Stanton%20and%20Ali%20Eslami%20and%20Gloria%20Shen%20and%20Berkin%20Akin%20and%20Alexey%20Guseynov%20and%20Yiqian%20Zhou%20and%20Jean-Baptiste%20Alayrac%20and%20Armand%20Joulin%20and%20Efrat%20Farkash%20and%20Ashish%20Thapliyal%20and%20Stephen%20Roller%20and%20Noam%20Shazeer%20and%20Todor%20Davchev%20and%20Terry%20Koo%20and%20Hannah%20Forbes-Pollard%20and%20Kartik%20Audhkhasi%20and%20Greg%20Farquhar%20and%20Adi%20Mayrav%20Gilady%20and%20Maggie%20Song%20and%20John%20Aslanides%20and%20Piermaria%20Mendolicchio%20and%20Alicia%20Parrish%20and%20John%20Blitzer%20and%20Pramod%20Gupta%20and%20Xiaoen%20Ju%20and%20Xiaochen%20Yang%20and%20Puranjay%20Datta%20and%20Andrea%20Tacchetti%20and%20Sanket%20Vaibhav%20Mehta%20and%20Gregory%20Dibb%20and%20Shubham%20Gupta%20and%20Federico%20Piccinini%20and%20Raia%20Hadsell%20and%20Sujee%20Rajayogam%20and%20Jiepu%20Jiang%20and%20Patrick%20Griffin%20and%20Patrik%20Sundberg%20and%20Jamie%20Hayes%20and%20Alexey%20Frolov%20and%20Tian%20Xie%20and%20Adam%20Zhang%20and%20Kingshuk%20Dasgupta%20and%20Uday%20Kalra%20and%20Lior%20Shani%20and%20Klaus%20Macherey%20and%20Tzu-Kuo%20Huang%20and%20Liam%20MacDermed%20and%20Karthik%20Duddu%20and%20Paulo%20Zacchello%20and%20Zi%20Yang%20and%20Jessica%20Lo%20and%20Kai%20Hui%20and%20Matej%20Kastelic%20and%20Derek%20Gasaway%20and%20Qijun%20Tan%20and%20Summer%20Yue%20and%20Pablo%20Barrio%20and%20John%20Wieting%20and%20Weel%20Yang%20and%20Andrew%20Nystrom%20and%20Solomon%20Demmessie%20and%20Anselm%20Levskaya%20and%20Fabio%20Viola%20and%20Chetan%20Tekur%20and%20Greg%20Billock%20and%20George%20Necula%20and%20Mandar%20Joshi%20and%20Rylan%20Schaeffer%20and%20Swachhand%20Lokhande%20and%20Christina%20Sorokin%20and%20Pradeep%20Shenoy%20and%20Mia%20Chen%20and%20Mark%20Collier%20and%20Hongji%20Li%20and%20Taylor%20Bos%20and%20Nevan%20Wichers%20and%20Sun%20Jae%20Lee%20and%20Ang%C3%A9line%20Pouget%20and%20Santhosh%20Thangaraj%20and%20Kyriakos%20Axiotis%20and%20Phil%20Crone%20and%20Rachel%20Sterneck%20and%20Nikolai%20Chinaev%20and%20Victoria%20Krakovna%20and%20Oleksandr%20Ferludin%20and%20Ian%20Gemp%20and%20Stephanie%20Winkler%20and%20Dan%20Goldberg%20and%20Ivan%20Korotkov%20and%20Kefan%20Xiao%20and%20Malika%20Mehrotra%20and%20Sandeep%20Mariserla%20and%20Vihari%20Piratla%20and%20Terry%20Thurk%20and%20Khiem%20Pham%20and%20Hongxu%20Ma%20and%20Alexandre%20Senges%20and%20Ravi%20Kumar%20and%20Clemens%20Meyer%20and%20Ellie%20Talius%20and%20Nuo%20Wang%20Pierse%20and%20Ballie%20Sandhu%20and%20Horia%20Toma%20and%20Kuo%20Lin%20and%20Swaroop%20Nath%20and%20Tom%20Stone%20and%20Dorsa%20Sadigh%20and%20Nikita%20Gupta%20and%20Arthur%20Guez%20and%20Avi%20Singh%20and%20Matt%20Thomas%20and%20Tom%20Duerig%20and%20Yuan%20Gong%20and%20Richard%20Tanburn%20and%20Lydia%20Lihui%20Zhang%20and%20Phuong%20Dao%20and%20Mohamed%20Hammad%20and%20Sirui%20Xie%20and%20Shruti%20Rijhwani%20and%20Ben%20Murdoch%20and%20Duhyeon%20Kim%20and%20Will%20Thompson%20and%20Heng-Tze%20Cheng%20and%20Daniel%20Sohn%20and%20Pablo%20Sprechmann%20and%20Qiantong%20Xu%20and%20Srinivas%20Tadepalli%20and%20Peter%20Young%20and%20Ye%20Zhang%20and%20Hansa%20Srinivasan%20and%20Miranda%20Aperghis%20and%20Aditya%20Ayyar%20and%20Hen%20Fitoussi%20and%20Ryan%20Burnell%20and%20David%20Madras%20and%20Mike%20Dusenberry%20and%20Xi%20Xiong%20and%20Tayo%20Oguntebi%20and%20Ben%20Albrecht%20and%20J%C3%B6rg%20Bornschein%20and%20Jovana%20Mitrovi%C4%87%20and%20Mason%20Dimarco%20and%20Bhargav%20Kanagal%20Shamanna%20and%20Premal%20Shah%20and%20Eren%20Sezener%20and%20Shyam%20Upadhyay%20and%20Dave%20Lacey%20and%20Craig%20Schiff%20and%20Sebastien%20Baur%20and%20Sanjay%20Ganapathy%20and%20Eva%20Schnider%20and%20Mateo%20Wirth%20and%20Connor%20Schenck%20and%20Andrey%20Simanovsky%20and%20Yi-Xuan%20Tan%20and%20Philipp%20Fr%C3%A4nken%20and%20Dennis%20Duan%20and%20Bharath%20Mankalale%20and%20Nikhil%20Dhawan%20and%20Kevin%20Sequeira%20and%20Zichuan%20Wei%20and%20Shivanker%20Goel%20and%20Caglar%20Unlu%20and%20Yukun%20Zhu%20and%20Haitian%20Sun%20and%20Ananth%20Balashankar%20and%20Kurt%20Shuster%20and%20Megh%20Umekar%20and%20Mahmoud%20Alnahlawi%20and%20A%C3%A4ron%20van%20den%20Oord%20and%20Kelly%20Chen%20and%20Yuexiang%20Zhai%20and%20Zihang%20Dai%20and%20Kuang-Huei%20Lee%20and%20Eric%20Doi%20and%20Lukas%20Zilka%20and%20Rohith%20Vallu%20and%20Disha%20Shrivastava%20and%20Jason%20Lee%20and%20Hisham%20Husain%20and%20Honglei%20Zhuang%20and%20Vincent%20Cohen-Addad%20and%20Jarred%20Barber%20and%20James%20Atwood%20and%20Adam%20Sadovsky%20and%20Quentin%20Wellens%20and%20Steven%20Hand%20and%20Arunkumar%20Rajendran%20and%20Aybuke%20Turker%20and%20CJ%20Carey%20and%20Yuanzhong%20Xu%20and%20Hagen%20Soltau%20and%20Zefei%20Li%20and%20Xinying%20Song%20and%20Conglong%20Li%20and%20Iurii%20Kemaev%20and%20Sasha%20Brown%20and%20Andrea%20Burns%20and%20Viorica%20Patraucean%20and%20Piotr%20Stanczyk%20and%20Renga%20Aravamudhan%20and%20Mathieu%20Blondel%20and%20Hila%20Noga%20and%20Lorenzo%20Blanco%20and%20Will%20Song%20and%20Michael%20Isard%20and%20Mandar%20Sharma%20and%20Reid%20Hayes%20and%20Dalia%20El%20Badawy%20and%20Avery%20Lamp%20and%20Itay%20Laish%20and%20Olga%20Kozlova%20and%20Kelvin%20Chan%20and%20Sahil%20Singla%20and%20Srinivas%20Sunkara%20and%20Mayank%20Upadhyay%20and%20Chang%20Liu%20and%20Aijun%20Bai%20and%20Jarek%20Wilkiewicz%20and%20Martin%20Zlocha%20and%20Jeremiah%20Liu%20and%20Zhuowan%20Li%20and%20Haiguang%20Li%20and%20Omer%20Barak%20and%20Ganna%20Raboshchuk%20and%20Jiho%20Choi%20and%20Fangyu%20Liu%20and%20Erik%20Jue%20and%20Mohit%20Sharma%20and%20Andreea%20Marzoca%20and%20Robert%20Busa-Fekete%20and%20Anna%20Korsun%20and%20Andre%20Elisseeff%20and%20Zhe%20Shen%20and%20Sara%20Mc%20Carthy%20and%20Kay%20Lamerigts%20and%20Anahita%20Hosseini%20and%20Hanzhao%20Lin%20and%20Charlie%20Chen%20and%20Fan%20Yang%20and%20Kushal%20Chauhan%20and%20Mark%20Omernick%20and%20Dawei%20Jia%20and%20Karina%20Zainullina%20and%20Demis%20Hassabis%20and%20Danny%20Vainstein%20and%20Ehsan%20Amid%20and%20Xiang%20Zhou%20and%20Ronny%20Votel%20and%20Eszter%20V%C3%A9rtes%20and%20Xinjian%20Li%20and%20Zongwei%20Zhou%20and%20Angeliki%20Lazaridou%20and%20Brendan%20McMahan%20and%20Arjun%20Narayanan%20and%20Hubert%20Soyer%20and%20Sujoy%20Basu%20and%20Kayi%20Lee%20and%20Bryan%20Perozzi%20and%20Qin%20Cao%20and%20Leonard%20Berrada%20and%20Rahul%20Arya%20and%20Ke%20Chen%20and%20%20Katrina%20and%20%20Xu%20and%20Matthias%20Lochbrunner%20and%20Alex%20Hofer%20and%20Sahand%20Sharifzadeh%20and%20Renjie%20Wu%20and%20Sally%20Goldman%20and%20Pranjal%20Awasthi%20and%20Xuezhi%20Wang%20and%20Yan%20Wu%20and%20Claire%20Sha%20and%20Biao%20Zhang%20and%20Maciej%20Miku%C5%82a%20and%20Filippo%20Graziano%20and%20Siobhan%20Mcloughlin%20and%20Irene%20Giannoumis%20and%20Youhei%20Namiki%20and%20Chase%20Malik%20and%20Carey%20Radebaugh%20and%20Jamie%20Hall%20and%20Ramiro%20Leal-Cavazos%20and%20Jianmin%20Chen%20and%20Vikas%20Sindhwani%20and%20David%20Kao%20and%20David%20Greene%20and%20Jordan%20Griffith%20and%20Chris%20Welty%20and%20Ceslee%20Montgomery%20and%20Toshihiro%20Yoshino%20and%20Liangzhe%20Yuan%20and%20Noah%20Goodman%20and%20Assaf%20Hurwitz%20Michaely%20and%20Kevin%20Lee%20and%20KP%20Sawhney%20and%20Wei%20Chen%20and%20Zheng%20Zheng%20and%20Megan%20Shum%20and%20Nikolay%20Savinov%20and%20Etienne%20Pot%20and%20Alex%20Pak%20and%20Morteza%20Zadimoghaddam%20and%20Sijal%20Bhatnagar%20and%20Yoad%20Lewenberg%20and%20Blair%20Kutzman%20and%20Ji%20Liu%20and%20Lesley%20Katzen%20and%20Jeremy%20Selier%20and%20Josip%20Djolonga%20and%20Dmitry%20Lepikhin%20and%20Kelvin%20Xu%20and%20Jacky%20Liang%20and%20Jiewen%20Tan%20and%20Benoit%20Schillings%20and%20Muge%20Ersoy%20and%20Pete%20Blois%20and%20Bernd%20Bandemer%20and%20Abhimanyu%20Singh%20and%20Sergei%20Lebedev%20and%20Pankaj%20Joshi%20and%20Adam%20R.%20Brown%20and%20Evan%20Palmer%20and%20Shreya%20Pathak%20and%20Komal%20Jalan%20and%20Fedir%20Zubach%20and%20Shuba%20Lall%20and%20Randall%20Parker%20and%20Alok%20Gunjan%20and%20Sergey%20Rogulenko%20and%20Sumit%20Sanghai%20and%20Zhaoqi%20Leng%20and%20Zoltan%20Egyed%20and%20Shixin%20Li%20and%20Maria%20Ivanova%20and%20Kostas%20Andriopoulos%20and%20Jin%20Xie%20and%20Elan%20Rosenfeld%20and%20Auriel%20Wright%20and%20Ankur%20Sharma%20and%20Xinyang%20Geng%20and%20Yicheng%20Wang%20and%20Sam%20Kwei%20and%20Renke%20Pan%20and%20Yujing%20Zhang%20and%20Gabby%20Wang%20and%20Xi%20Liu%20and%20Chak%20Yeung%20and%20Elizabeth%20Cole%20and%20Aviv%20Rosenberg%20and%20Zhen%20Yang%20and%20Phil%20Chen%20and%20George%20Polovets%20and%20Pranav%20Nair%20and%20Rohun%20Saxena%20and%20Josh%20Smith%20and%20Shuo-yiin%20Chang%20and%20Aroma%20Mahendru%20and%20Svetlana%20Grant%20and%20Anand%20Iyer%20and%20Irene%20Cai%20and%20Jed%20McGiffin%20and%20Jiaming%20Shen%20and%20Alanna%20Walton%20and%20Antonious%20Girgis%20and%20Oliver%20Woodman%20and%20Rosemary%20Ke%20and%20Mike%20Kwong%20and%20Louis%20Rouillard%20and%20Jinmeng%20Rao%20and%20Zhihao%20Li%20and%20Yuntao%20Xu%20and%20Flavien%20Prost%20and%20Chi%20Zou%20and%20Ziwei%20Ji%20and%20Alberto%20Magni%20and%20Tyler%20Liechty%20and%20Dan%20A.%20Calian%20and%20Deepak%20Ramachandran%20and%20Igor%20Krivokon%20and%20Hui%20Huang%20and%20Terry%20Chen%20and%20Anja%20Hauth%20and%20Anastasija%20Ili%C4%87%20and%20Weijuan%20Xi%20and%20Hyeontaek%20Lim%20and%20Vlad-Doru%20Ion%20and%20Pooya%20Moradi%20and%20Metin%20Toksoz-Exley%20and%20Kalesha%20Bullard%20and%20Miltos%20Allamanis%20and%20Xiaomeng%20Yang%20and%20Sophie%20Wang%20and%20Zhi%20Hong%20and%20Anita%20Gergely%20and%20Cheng%20Li%20and%20Bhavishya%20Mittal%20and%20Vitaly%20Kovalev%20and%20Victor%20Ungureanu%20and%20Jane%20Labanowski%20and%20Jan%20Wassenberg%20and%20Nicolas%20Lacasse%20and%20Geoffrey%20Cideron%20and%20Petar%20Devi%C4%87%20and%20Annie%20Marsden%20and%20Lynn%20Nguyen%20and%20Michael%20Fink%20and%20Yin%20Zhong%20and%20Tatsuya%20Kiyono%20and%20Desi%20Ivanov%20and%20Sally%20Ma%20and%20Max%20Bain%20and%20Kiran%20Yalasangi%20and%20Jennifer%20She%20and%20Anastasia%20Petrushkina%20and%20Mayank%20Lunayach%20and%20Carla%20Bromberg%20and%20Sarah%20Hodkinson%20and%20Vilobh%20Meshram%20and%20Daniel%20Vlasic%20and%20Austin%20Kyker%20and%20Steve%20Xu%20and%20Jeff%20Stanway%20and%20Zuguang%20Yang%20and%20Kai%20Zhao%20and%20Matthew%20Tung%20and%20Seth%20Odoom%20and%20Yasuhisa%20Fujii%20and%20Justin%20Gilmer%20and%20Eunyoung%20Kim%20and%20Felix%20Halim%20and%20Quoc%20Le%20and%20Bernd%20Bohnet%20and%20Seliem%20El-Sayed%20and%20Behnam%20Neyshabur%20and%20Malcolm%20Reynolds%20and%20Dean%20Reich%20and%20Yang%20Xu%20and%20Erica%20Moreira%20and%20Anuj%20Sharma%20and%20Zeyu%20Liu%20and%20Mohammad%20Javad%20Hosseini%20and%20Naina%20Raisinghani%20and%20Yi%20Su%20and%20Ni%20Lao%20and%20Daniel%20Formoso%20and%20Marco%20Gelmi%20and%20Almog%20Gueta%20and%20Tapomay%20Dey%20and%20Elena%20Gribovskaya%20and%20Domagoj%20%C4%86evid%20and%20Sidharth%20Mudgal%20and%20Garrett%20Bingham%20and%20Jianling%20Wang%20and%20Anurag%20Kumar%20and%20Alex%20Cullum%20and%20Feng%20Han%20and%20Konstantinos%20Bousmalis%20and%20Diego%20Cedillo%20and%20Grace%20Chu%20and%20Vladimir%20Magay%20and%20Paul%20Michel%20and%20Ester%20Hlavnova%20and%20Daniele%20Calandriello%20and%20Setareh%20Ariafar%20and%20Kaisheng%20Yao%20and%20Vikash%20Sehwag%20and%20Arpi%20Vezer%20and%20Agustin%20Dal%20Lago%20and%20Zhenkai%20Zhu%20and%20Paul%20Kishan%20Rubenstein%20and%20Allen%20Porter%20and%20Anirudh%20Baddepudi%20and%20Oriana%20Riva%20and%20Mihai%20Dorin%20Istin%20and%20Chih-Kuan%20Yeh%20and%20Zhi%20Li%20and%20Andrew%20Howard%20and%20Nilpa%20Jha%20and%20Jeremy%20Chen%20and%20Raoul%20de%20Liedekerke%20and%20Zafarali%20Ahmed%20and%20Mikel%20Rodriguez%20and%20Tanuj%20Bhatia%20and%20Bangju%20Wang%20and%20Ali%20Elqursh%20and%20David%20Klinghoffer%20and%20Peter%20Chen%20and%20Pushmeet%20Kohli%20and%20Te%20I%20and%20Weiyang%20Zhang%20and%20Zack%20Nado%20and%20Jilin%20Chen%20and%20Maxwell%20Chen%20and%20George%20Zhang%20and%20Aayush%20Singh%20and%20Adam%20Hillier%20and%20Federico%20Lebron%20and%20Yiqing%20Tao%20and%20Ting%20Liu%20and%20Gabriel%20Dulac-Arnold%20and%20Jingwei%20Zhang%20and%20Shashi%20Narayan%20and%20Buhuang%20Liu%20and%20Orhan%20Firat%20and%20Abhishek%20Bhowmick%20and%20Bingyuan%20Liu%20and%20Hao%20Zhang%20and%20Zizhao%20Zhang%20and%20Georges%20Rotival%20and%20Nathan%20Howard%20and%20Anu%20Sinha%20and%20Alexander%20Grushetsky%20and%20Benjamin%20Beyret%20and%20Keerthana%20Gopalakrishnan%20and%20James%20Zhao%20and%20Kyle%20He%20and%20Szabolcs%20Payrits%20and%20Zaid%20Nabulsi%20and%20Zhaoyi%20Zhang%20and%20Weijie%20Chen%20and%20Edward%20Lee%20and%20Nova%20Fallen%20and%20Sreenivas%20Gollapudi%20and%20Aurick%20Zhou%20and%20Filip%20Paveti%C4%87%20and%20Thomas%20K%C3%B6ppe%20and%20Shiyu%20Huang%20and%20Rama%20Pasumarthi%20and%20Nick%20Fernando%20and%20Felix%20Fischer%20and%20Daria%20%C4%86urko%20and%20Yang%20Gao%20and%20James%20Svensson%20and%20Austin%20Stone%20and%20Haroon%20Qureshi%20and%20Abhishek%20Sinha%20and%20Apoorv%20Kulshreshtha%20and%20Martin%20Matysiak%20and%20Jieming%20Mao%20and%20Carl%20Saroufim%20and%20Aleksandra%20Faust%20and%20Qingnan%20Duan%20and%20Gil%20Fidel%20and%20Kaan%20Katircioglu%20and%20Rapha%C3%ABl%20Lopez%20Kaufman%20and%20Dhruv%20Shah%20and%20Weize%20Kong%20and%20Abhishek%20Bapna%20and%20Gell%C3%A9rt%20Weisz%20and%20Emma%20Dunleavy%20and%20Praneet%20Dutta%20and%20Tianqi%20Liu%20and%20Rahma%20Chaabouni%20and%20Carolina%20Parada%20and%20Marcus%20Wu%20and%20Alexandra%20Belias%20and%20Alessandro%20Bissacco%20and%20Stanislav%20Fort%20and%20Li%20Xiao%20and%20Fantine%20Huot%20and%20Chris%20Knutsen%20and%20Yochai%20Blau%20and%20Gang%20Li%20and%20Jennifer%20Prendki%20and%20Juliette%20Love%20and%20Yinlam%20Chow%20and%20Pichi%20Charoenpanit%20and%20Hidetoshi%20Shimokawa%20and%20Vincent%20Coriou%20and%20Karol%20Gregor%20and%20Tomas%20Izo%20and%20Arjun%20Akula%20and%20Mario%20Pinto%20and%20Chris%20Hahn%20and%20Dominik%20Paulus%20and%20Jiaxian%20Guo%20and%20Neha%20Sharma%20and%20Cho-Jui%20Hsieh%20and%20Adaeze%20Chukwuka%20and%20Kazuma%20Hashimoto%20and%20Nathalie%20Rauschmayr%20and%20Ling%20Wu%20and%20Christof%20Angermueller%20and%20Yulong%20Wang%20and%20Sebastian%20Gerlach%20and%20Michael%20Pliskin%20and%20Daniil%20Mirylenka%20and%20Min%20Ma%20and%20Lexi%20Baugher%20and%20Bryan%20Gale%20and%20Shaan%20Bijwadia%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20David%20Wood%20and%20Jane%20Park%20and%20Chung-Ching%20Chang%20and%20Babi%20Seal%20and%20Chris%20Tar%20and%20Kacper%20Krasowiak%20and%20Yiwen%20Song%20and%20Georgi%20Stephanov%20and%20Gary%20Wang%20and%20Marcello%20Maggioni%20and%20Stein%20Xudong%20Lin%20and%20Felix%20Wu%20and%20Shachi%20Paul%20and%20Zixuan%20Jiang%20and%20Shubham%20Agrawal%20and%20Bilal%20Piot%20and%20Alex%20Feng%20and%20Cheolmin%20Kim%20and%20Tulsee%20Doshi%20and%20Jonathan%20Lai%20and%20%20Chuqiao%20and%20%20Xu%20and%20Sharad%20Vikram%20and%20Ciprian%20Chelba%20and%20Sebastian%20Krause%20and%20Vincent%20Zhuang%20and%20Jack%20Rae%20and%20Timo%20Denk%20and%20Adrian%20Collister%20and%20Lotte%20Weerts%20and%20Xianghong%20Luo%20and%20Yifeng%20Lu%20and%20H%C3%A5vard%20Garnes%20and%20Nitish%20Gupta%20and%20Terry%20Spitz%20and%20Avinatan%20Hassidim%20and%20Lihao%20Liang%20and%20Izhak%20Shafran%20and%20Peter%20Humphreys%20and%20Kenny%20Vassigh%20and%20Phil%20Wallis%20and%20Virat%20Shejwalkar%20and%20Nicolas%20Perez-Nieves%20and%20Rachel%20Hornung%20and%20Melissa%20Tan%20and%20Beka%20Westberg%20and%20Andy%20Ly%20and%20Richard%20Zhang%20and%20Brian%20Farris%20and%20Jongbin%20Park%20and%20Alec%20Kosik%20and%20Zeynep%20Cankara%20and%20Andrii%20Maksai%20and%20Yunhan%20Xu%20and%20Albin%20Cassirer%20and%20Sergi%20Caelles%20and%20Abbas%20Abdolmaleki%20and%20Mencher%20Chiang%20and%20Alex%20Fabrikant%20and%20Shravya%20Shetty%20and%20Luheng%20He%20and%20Mai%20Gim%C3%A9nez%20and%20Hadi%20Hashemi%20and%20Sheena%20Panthaplackel%20and%20Yana%20Kulizhskaya%20and%20Salil%20Deshmukh%20and%20Daniele%20Pighin%20and%20Robin%20Alazard%20and%20Disha%20Jindal%20and%20Seb%20Noury%20and%20Pradeep%20Kumar%20S%20and%20Siyang%20Qin%20and%20Xerxes%20Dotiwalla%20and%20Stephen%20Spencer%20and%20Mohammad%20Babaeizadeh%20and%20Blake%20JianHang%20Chen%20and%20Vaibhav%20Mehta%20and%20Jennie%20Lees%20and%20Andrew%20Leach%20and%20Penporn%20Koanantakool%20and%20Ilia%20Akolzin%20and%20Ramona%20Comanescu%20and%20Junwhan%20Ahn%20and%20Alexey%20Svyatkovskiy%20and%20Basil%20Mustafa%20and%20David%20D%27Ambrosio%20and%20Shiva%20Mohan%20Reddy%20Garlapati%20and%20Pascal%20Lamblin%20and%20Alekh%20Agarwal%20and%20Shuang%20Song%20and%20Pier%20Giuseppe%20Sessa%20and%20Pauline%20Coquinot%20and%20John%20Maggs%20and%20Hussain%20Masoom%20and%20Divya%20Pitta%20and%20Yaqing%20Wang%20and%20Patrick%20Morris-Suzuki%20and%20Billy%20Porter%20and%20Johnson%20Jia%20and%20Jeffrey%20Dudek%20and%20Raghavender%20R%20and%20Cosmin%20Paduraru%20and%20Alan%20Ansell%20and%20Tolga%20Bolukbasi%20and%20Tony%20Lu%20and%20Ramya%20Ganeshan%20and%20Zi%20Wang%20and%20Henry%20Griffiths%20and%20Rodrigo%20Benenson%20and%20Yifan%20He%20and%20James%20Swirhun%20and%20George%20Papamakarios%20and%20Aditya%20Chawla%20and%20Kuntal%20Sengupta%20and%20Yan%20Wang%20and%20Vedrana%20Milutinovic%20and%20Igor%20Mordatch%20and%20Zhipeng%20Jia%20and%20Jamie%20Smith%20and%20Will%20Ng%20and%20Shitij%20Nigam%20and%20Matt%20Young%20and%20Eugen%20Vu%C5%A1ak%20and%20Blake%20Hechtman%20and%20Sheela%20Goenka%20and%20Avital%20Zipori%20and%20Kareem%20Ayoub%20and%20Ashok%20Popat%20and%20Trilok%20Acharya%20and%20Luo%20Yu%20and%20Dawn%20Bloxwich%20and%20Hugo%20Song%20and%20Paul%20Roit%20and%20Haiqiong%20Li%20and%20Aviel%20Boag%20and%20Nigamaa%20Nayakanti%20and%20Bilva%20Chandra%20and%20Tianli%20Ding%20and%20Aahil%20Mehta%20and%20Cath%20Hope%20and%20Jiageng%20Zhang%20and%20Idan%20Heimlich%20Shtacher%20and%20Kartikeya%20Badola%20and%20Ryo%20Nakashima%20and%20Andrei%20Sozanschi%20and%20Iulia%20Com%C5%9Fa%20and%20Ante%20%C5%BDu%C5%BEul%20and%20Emily%20Caveness%20and%20Julian%20Odell%20and%20Matthew%20Watson%20and%20Dario%20de%20Cesare%20and%20Phillip%20Lippe%20and%20Derek%20Lockhart%20and%20Siddharth%20Verma%20and%20Huizhong%20Chen%20and%20Sean%20Sun%20and%20Lin%20Zhuo%20and%20Aditya%20Shah%20and%20Prakhar%20Gupta%20and%20Alex%20Muzio%20and%20Ning%20Niu%20and%20Amir%20Zait%20and%20Abhinav%20Singh%20and%20Meenu%20Gaba%20and%20Fan%20Ye%20and%20Prajit%20Ramachandran%20and%20Mohammad%20Saleh%20and%20Raluca%20Ada%20Popa%20and%20Ayush%20Dubey%20and%20Frederick%20Liu%20and%20Sara%20Javanmardi%20and%20Mark%20Epstein%20and%20Ross%20Hemsley%20and%20Richard%20Green%20and%20Nishant%20Ranka%20and%20Eden%20Cohen%20and%20Chuyuan%20Kelly%20Fu%20and%20Sanjay%20Ghemawat%20and%20Jed%20Borovik%20and%20James%20Martens%20and%20Anthony%20Chen%20and%20Pranav%20Shyam%20and%20Andr%C3%A9%20Susano%20Pinto%20and%20Ming-Hsuan%20Yang%20and%20Alexandru%20%C5%A2ifrea%20and%20David%20Du%20and%20Boqing%20Gong%20and%20Ayushi%20Agarwal%20and%20Seungyeon%20Kim%20and%20Christian%20Frank%20and%20Saloni%20Shah%20and%20Xiaodan%20Song%20and%20Zhiwei%20Deng%20and%20Ales%20Mikhalap%20and%20Kleopatra%20Chatziprimou%20and%20Timothy%20Chung%20and%20Toni%20Creswell%20and%20Susan%20Zhang%20and%20Yennie%20Jun%20and%20Carl%20Lebsack%20and%20Will%20Truong%20and%20Slavica%20Anda%C4%8Di%C4%87%20and%20Itay%20Yona%20and%20Marco%20Fornoni%20and%20Rong%20Rong%20and%20Serge%20Toropov%20and%20Afzal%20Shama%20Soudagar%20and%20Andrew%20Audibert%20and%20Salah%20Zaiem%20and%20Zaheer%20Abbas%20and%20Andrei%20Rusu%20and%20Sahitya%20Potluri%20and%20Shitao%20Weng%20and%20Anastasios%20Kementsietsidis%20and%20Anton%20Tsitsulin%20and%20Daiyi%20Peng%20and%20Natalie%20Ha%20and%20Sanil%20Jain%20and%20Tejasi%20Latkar%20and%20Simeon%20Ivanov%20and%20Cory%20McLean%20and%20Anirudh%20GP%20and%20Rajesh%20Venkataraman%20and%20Canoee%20Liu%20and%20Dilip%20Krishnan%20and%20Joel%20D%27sa%20and%20Roey%20Yogev%20and%20Paul%20Collins%20and%20Benjamin%20Lee%20and%20Lewis%20Ho%20and%20Carl%20Doersch%20and%20Gal%20Yona%20and%20Shawn%20Gao%20and%20Felipe%20Tiengo%20Ferreira%20and%20Adnan%20Ozturel%20and%20Hannah%20Muckenhirn%20and%20Ce%20Zheng%20and%20Gargi%20Balasubramaniam%20and%20Mudit%20Bansal%20and%20George%20van%20den%20Driessche%20and%20Sivan%20Eiger%20and%20Salem%20Haykal%20and%20Vedant%20Misra%20and%20Abhimanyu%20Goyal%20and%20Danilo%20Martins%20and%20Gary%20Leung%20and%20Jonas%20Valfridsson%20and%20Four%20Flynn%20and%20Will%20Bishop%20and%20Chenxi%20Pang%20and%20Yoni%20Halpern%20and%20Honglin%20Yu%20and%20Lawrence%20Moore%20and%20%20Yuvein%20and%20%20Zhu%20and%20Sridhar%20Thiagarajan%20and%20Yoel%20Drori%20and%20Zhisheng%20Xiao%20and%20Lucio%20Dery%20and%20Rolf%20Jagerman%20and%20Jing%20Lu%20and%20Eric%20Ge%20and%20Vaibhav%20Aggarwal%20and%20Arjun%20Khare%20and%20Vinh%20Tran%20and%20Oded%20Elyada%20and%20Ferran%20Alet%20and%20James%20Rubin%20and%20Ian%20Chou%20and%20David%20Tian%20and%20Libin%20Bai%20and%20Lawrence%20Chan%20and%20Lukasz%20Lew%20and%20Karolis%20Misiunas%20and%20Taylan%20Bilal%20and%20Aniket%20Ray%20and%20Sindhu%20Raghuram%20and%20Alex%20Castro-Ros%20and%20Viral%20Carpenter%20and%20CJ%20Zheng%20and%20Michael%20Kilgore%20and%20Josef%20Broder%20and%20Emily%20Xue%20and%20Praveen%20Kallakuri%20and%20Dheeru%20Dua%20and%20Nancy%20Yuen%20and%20Steve%20Chien%20and%20John%20Schultz%20and%20Saurabh%20Agrawal%20and%20Reut%20Tsarfaty%20and%20Jingcao%20Hu%20and%20Ajay%20Kannan%20and%20Dror%20Marcus%20and%20Nisarg%20Kothari%20and%20Baochen%20Sun%20and%20Ben%20Horn%20and%20Matko%20Bo%C5%A1njak%20and%20Ferjad%20Naeem%20and%20Dean%20Hirsch%20and%20Lewis%20Chiang%20and%20Boya%20Fang%20and%20Jie%20Han%20and%20Qifei%20Wang%20and%20Ben%20Hora%20and%20Antoine%20He%20and%20Mario%20Lu%C4%8Di%C4%87%20and%20Beer%20Changpinyo%20and%20Anshuman%20Tripathi%20and%20John%20Youssef%20and%20Chester%20Kwak%20and%20Philippe%20Schlattner%20and%20Cat%20Graves%20and%20R%C3%A9mi%20Leblond%20and%20Wenjun%20Zeng%20and%20Anders%20Andreassen%20and%20Gabriel%20Rasskin%20and%20Yue%20Song%20and%20Eddie%20Cao%20and%20Junhyuk%20Oh%20and%20Matt%20Hoffman%20and%20Wojtek%20Skut%20and%20Yichi%20Zhang%20and%20Jon%20Stritar%20and%20Xingyu%20Cai%20and%20Saarthak%20Khanna%20and%20Kathie%20Wang%20and%20Shriya%20Sharma%20and%20Christian%20Reisswig%20and%20Younghoon%20Jun%20and%20Aman%20Prasad%20and%20Tatiana%20Sholokhova%20and%20Preeti%20Singh%20and%20Adi%20Gerzi%20Rosenthal%20and%20Anian%20Ruoss%20and%20Fran%C3%A7oise%20Beaufays%20and%20Sean%20Kirmani%20and%20Dongkai%20Chen%20and%20Johan%20Schalkwyk%20and%20Jonathan%20Herzig%20and%20Been%20Kim%20and%20Josh%20Jacob%20and%20Damien%20Vincent%20and%20Adrian%20N%20Reyes%20and%20Ivana%20Balazevic%20and%20L%C3%A9onard%20Hussenot%20and%20Jon%20Schneider%20and%20Parker%20Barnes%20and%20Luis%20Castro%20and%20Spandana%20Raj%20Babbula%20and%20Simon%20Green%20and%20Serkan%20Cabi%20and%20Nico%20Duduta%20and%20Danny%20Driess%20and%20Rich%20Galt%20and%20Noam%20Velan%20and%20Junjie%20Wang%20and%20Hongyang%20Jiao%20and%20Matthew%20Mauger%20and%20Du%20Phan%20and%20Miteyan%20Patel%20and%20Vlado%20Gali%C4%87%20and%20Jerry%20Chang%20and%20Eyal%20Marcus%20and%20Matt%20Harvey%20and%20Julian%20Salazar%20and%20Elahe%20Dabir%20and%20Suraj%20Satishkumar%20Sheth%20and%20Amol%20Mandhane%20and%20Hanie%20Sedghi%20and%20Jeremiah%20Willcock%20and%20Amir%20Zandieh%20and%20Shruthi%20Prabhakara%20and%20Aida%20Amini%20and%20Antoine%20Miech%20and%20Victor%20Stone%20and%20Massimo%20Nicosia%20and%20Paul%20Niemczyk%20and%20Ying%20Xiao%20and%20Lucy%20Kim%20and%20S%C5%82awek%20Kwasiborski%20and%20Vikas%20Verma%20and%20Ada%20Maksutaj%20Oflazer%20and%20Christoph%20Hirnschall%20and%20Peter%20Sung%20and%20Lu%20Liu%20and%20Richard%20Everett%20and%20Michiel%20Bakker%20and%20%C3%81goston%20Weisz%20and%20Yufei%20Wang%20and%20Vivek%20Sampathkumar%20and%20Uri%20Shaham%20and%20Bibo%20Xu%20and%20Yasemin%20Altun%20and%20Mingqiu%20Wang%20and%20Takaaki%20Saeki%20and%20Guanjie%20Chen%20and%20Emanuel%20Taropa%20and%20Shanthal%20Vasanth%20and%20Sophia%20Austin%20and%20Lu%20Huang%20and%20Goran%20Petrovic%20and%20Qingyun%20Dou%20and%20Daniel%20Golovin%20and%20Grigory%20Rozhdestvenskiy%20and%20Allie%20Culp%20and%20Will%20Wu%20and%20Motoki%20Sano%20and%20Divya%20Jain%20and%20Julia%20Proskurnia%20and%20S%C3%A9bastien%20Cevey%20and%20Alejandro%20Cruzado%20Ruiz%20and%20Piyush%20Patil%20and%20Mahdi%20Mirzazadeh%20and%20Eric%20Ni%20and%20Javier%20Snaider%20and%20Lijie%20Fan%20and%20Alexandre%20Fr%C3%A9chette%20and%20AJ%20Pierigiovanni%20and%20Shariq%20Iqbal%20and%20Kenton%20Lee%20and%20Claudio%20Fantacci%20and%20Jinwei%20Xing%20and%20Lisa%20Wang%20and%20Alex%20Irpan%20and%20David%20Raposo%20and%20Yi%20Luan%20and%20Zhuoyuan%20Chen%20and%20Harish%20Ganapathy%20and%20Kevin%20Hui%20and%20Jiazhong%20Nie%20and%20Isabelle%20Guyon%20and%20Heming%20Ge%20and%20Roopali%20Vij%20and%20Hui%20Zheng%20and%20Dayeong%20Lee%20and%20Alfonso%20Casta%C3%B1o%20and%20Khuslen%20Baatarsukh%20and%20Gabriel%20Ibagon%20and%20Alexandra%20Chronopoulou%20and%20Nicholas%20FitzGerald%20and%20Shashank%20Viswanadha%20and%20Safeen%20Huda%20and%20Rivka%20Moroshko%20and%20Georgi%20Stoyanov%20and%20Prateek%20Kolhar%20and%20Alain%20Vaucher%20and%20Ishaan%20Watts%20and%20Adhi%20Kuncoro%20and%20Henryk%20Michalewski%20and%20Satish%20Kambala%20and%20Bat-Orgil%20Batsaikhan%20and%20Alek%20Andreev%20and%20Irina%20Jurenka%20and%20Maigo%20Le%20and%20Qihang%20Chen%20and%20Wael%20Al%20Jishi%20and%20Sarah%20Chakera%20and%20Zhe%20Chen%20and%20Aditya%20Kini%20and%20Vikas%20Yadav%20and%20Aditya%20Siddhant%20and%20Ilia%20Labzovsky%20and%20Balaji%20Lakshminarayanan%20and%20Carrie%20Grimes%20Bostock%20and%20Pankil%20Botadra%20and%20Ankesh%20Anand%20and%20Colton%20Bishop%20and%20Sam%20Conway-Rahman%20and%20Mohit%20Agarwal%20and%20Yani%20Donchev%20and%20Achintya%20Singhal%20and%20F%C3%A9lix%20de%20Chaumont%20Quitry%20and%20Natalia%20Ponomareva%20and%20Nishant%20Agrawal%20and%20Bin%20Ni%20and%20Kalpesh%20Krishna%20and%20Masha%20Samsikova%20and%20John%20Karro%20and%20Yilun%20Du%20and%20Tamara%20von%20Glehn%20and%20Caden%20Lu%20and%20Christopher%20A.%20Choquette-Choo%20and%20Zhen%20Qin%20and%20Tingnan%20Zhang%20and%20Sicheng%20Li%20and%20Divya%20Tyam%20and%20Swaroop%20Mishra%20and%20Wing%20Lowe%20and%20Colin%20Ji%20and%20Weiyi%20Wang%20and%20Manaal%20Faruqui%20and%20Ambrose%20Slone%20and%20Valentin%20Dalibard%20and%20Arunachalam%20Narayanaswamy%20and%20John%20Lambert%20and%20Pierre-Antoine%20Manzagol%20and%20Dan%20Karliner%20and%20Andrew%20Bolt%20and%20Ivan%20Lobov%20and%20Aditya%20Kusupati%20and%20Chang%20Ye%20and%20Xuan%20Yang%20and%20Heiga%20Zen%20and%20Nelson%20George%20and%20Mukul%20Bhutani%20and%20Olivier%20Lacombe%20and%20Robert%20Riachi%20and%20Gagan%20Bansal%20and%20Rachel%20Soh%20and%20Yue%20Gao%20and%20Yang%20Yu%20and%20Adams%20Yu%20and%20Emily%20Nottage%20and%20Tania%20Rojas-Esponda%20and%20James%20Noraky%20and%20Manish%20Gupta%20and%20Ragha%20Kotikalapudi%20and%20Jichuan%20Chang%20and%20Sanja%20Deur%20and%20Dan%20Graur%20and%20Alex%20Mossin%20and%20Erin%20Farnese%20and%20Ricardo%20Figueira%20and%20Alexandre%20Moufarek%20and%20Austin%20Huang%20and%20Patrik%20Zochbauer%20and%20Ben%20Ingram%20and%20Tongzhou%20Chen%20and%20Zelin%20Wu%20and%20Adri%C3%A0%20Puigdom%C3%A8nech%20and%20Leland%20Rechis%20and%20Da%20Yu%20and%20Sri%20Gayatri%20Sundara%20Padmanabhan%20and%20Rui%20Zhu%20and%20Chu-ling%20Ko%20and%20Andrea%20Banino%20and%20Samira%20Daruki%20and%20Aarush%20Selvan%20and%20Dhruva%20Bhaswar%20and%20Daniel%20Hernandez%20Diaz%20and%20Chen%20Su%20and%20Salvatore%20Scellato%20and%20Jennifer%20Brennan%20and%20Woohyun%20Han%20and%20Grace%20Chung%20and%20Priyanka%20Agrawal%20and%20Urvashi%20Khandelwal%20and%20Khe%20Chai%20Sim%20and%20Morgane%20Lustman%20and%20Sam%20Ritter%20and%20Kelvin%20Guu%20and%20Jiawei%20Xia%20and%20Prateek%20Jain%20and%20Emma%20Wang%20and%20Tyrone%20Hill%20and%20Mirko%20Rossini%20and%20Marija%20Kostelac%20and%20Tautvydas%20Misiunas%20and%20Amit%20Sabne%20and%20Kyuyeun%20Kim%20and%20Ahmet%20Iscen%20and%20Congchao%20Wang%20and%20Jos%C3%A9%20Leal%20and%20Ashwin%20Sreevatsa%20and%20Utku%20Evci%20and%20Manfred%20Warmuth%20and%20Saket%20Joshi%20and%20Daniel%20Suo%20and%20James%20Lottes%20and%20Garrett%20Honke%20and%20Brendan%20Jou%20and%20Stefani%20Karp%20and%20Jieru%20Hu%20and%20Himanshu%20Sahni%20and%20Adrien%20Ali%20Ta%C3%AFga%20and%20William%20Kong%20and%20Samrat%20Ghosh%20and%20Renshen%20Wang%20and%20Jay%20Pavagadhi%20and%20Natalie%20Axelsson%20and%20Nikolai%20Grigorev%20and%20Patrick%20Siegler%20and%20Rebecca%20Lin%20and%20Guohui%20Wang%20and%20Emilio%20Parisotto%20and%20Sharath%20Maddineni%20and%20Krishan%20Subudhi%20and%20Eyal%20Ben-David%20and%20Elena%20Pochernina%20and%20Orgad%20Keller%20and%20Thi%20Avrahami%20and%20Zhe%20Yuan%20and%20Pulkit%20Mehta%20and%20Jialu%20Liu%20and%20Sherry%20Yang%20and%20Wendy%20Kan%20and%20Katherine%20Lee%20and%20Tom%20Funkhouser%20and%20Derek%20Cheng%20and%20Hongzhi%20Shi%20and%20Archit%20Sharma%20and%20Joe%20Kelley%20and%20Matan%20Eyal%20and%20Yury%20Malkov%20and%20Corentin%20Tallec%20and%20Yuval%20Bahat%20and%20Shen%20Yan%20and%20%20Xintian%20and%20%20Wu%20and%20David%20Lindner%20and%20Chengda%20Wu%20and%20Avi%20Caciularu%20and%20Xiyang%20Luo%20and%20Rodolphe%20Jenatton%20and%20Tim%20Zaman%20and%20Yingying%20Bi%20and%20Ilya%20Kornakov%20and%20Ganesh%20Mallya%20and%20Daisuke%20Ikeda%20and%20Itay%20Karo%20and%20Anima%20Singh%20and%20Colin%20Evans%20and%20Praneeth%20Netrapalli%20and%20Vincent%20Nallatamby%20and%20Isaac%20Tian%20and%20Yannis%20Assael%20and%20Vikas%20Raunak%20and%20Victor%20Carbune%20and%20Ioana%20Bica%20and%20Lior%20Madmoni%20and%20Dee%20Cattle%20and%20Snchit%20Grover%20and%20Krishna%20Somandepalli%20and%20Sid%20Lall%20and%20Amelio%20V%C3%A1zquez-Reina%20and%20Riccardo%20Patana%20and%20Jiaqi%20Mu%20and%20Pranav%20Talluri%20and%20Maggie%20Tran%20and%20Rajeev%20Aggarwal%20and%20RJ%20Skerry-Ryan%20and%20Jun%20Xu%20and%20Mike%20Burrows%20and%20Xiaoyue%20Pan%20and%20Edouard%20Yvinec%20and%20Di%20Lu%20and%20Zhiying%20Zhang%20and%20Duc%20Dung%20Nguyen%20and%20Hairong%20Mu%20and%20Gabriel%20Barcik%20and%20Helen%20Ran%20and%20Lauren%20Beltrone%20and%20Krzysztof%20Choromanski%20and%20Dia%20Kharrat%20and%20Samuel%20Albanie%20and%20Sean%20Purser-haskell%20and%20David%20Bieber%20and%20Carrie%20Zhang%20and%20Jing%20Wang%20and%20Tom%20Hudson%20and%20Zhiyuan%20Zhang%20and%20Han%20Fu%20and%20Johannes%20Mauerer%20and%20Mohammad%20Hossein%20Bateni%20and%20AJ%20Maschinot%20and%20Bing%20Wang%20and%20Muye%20Zhu%20and%20Arjun%20Pillai%20and%20Tobias%20Weyand%20and%20Shuang%20Liu%20and%20Oscar%20Akerlund%20and%20Fred%20Bertsch%20and%20Vittal%20Premachandran%20and%20Alicia%20Jin%20and%20Vincent%20Roulet%20and%20Peter%20de%20Boursac%20and%20Shubham%20Mittal%20and%20Ndaba%20Ndebele%20and%20Georgi%20Karadzhov%20and%20Sahra%20Ghalebikesabi%20and%20Ricky%20Liang%20and%20Allen%20Wu%20and%20Yale%20Cong%20and%20Nimesh%20Ghelani%20and%20Sumeet%20Singh%20and%20Bahar%20Fatemi%20and%20%20Warren%20and%20%20Chen%20and%20Charles%20Kwong%20and%20Alexey%20Kolganov%20and%20Steve%20Li%20and%20Richard%20Song%20and%20Chenkai%20Kuang%20and%20Sobhan%20Miryoosefi%20and%20Dale%20Webster%20and%20James%20Wendt%20and%20Arkadiusz%20Socala%20and%20Guolong%20Su%20and%20Artur%20Mendon%C3%A7a%20and%20Abhinav%20Gupta%20and%20Xiaowei%20Li%20and%20Tomy%20Tsai%20and%20%20Qiong%20and%20%20Hu%20and%20Kai%20Kang%20and%20Angie%20Chen%20and%20Sertan%20Girgin%20and%20Yongqin%20Xian%20and%20Andrew%20Lee%20and%20Nolan%20Ramsden%20and%20Leslie%20Baker%20and%20Madeleine%20Clare%20Elish%20and%20Varvara%20Krayvanova%20and%20Rishabh%20Joshi%20and%20Jiri%20Simsa%20and%20Yao-Yuan%20Yang%20and%20Piotr%20Ambroszczyk%20and%20Dipankar%20Ghosh%20and%20Arjun%20Kar%20and%20Yuan%20Shangguan%20and%20Yumeya%20Yamamori%20and%20Yaroslav%20Akulov%20and%20Andy%20Brock%20and%20Haotian%20Tang%20and%20Siddharth%20Vashishtha%20and%20Rich%20Munoz%20and%20Andreas%20Steiner%20and%20Kalyan%20Andra%20and%20Daniel%20Eppens%20and%20Qixuan%20Feng%20and%20Hayato%20Kobayashi%20and%20Sasha%20Goldshtein%20and%20Mona%20El%20Mahdy%20and%20Xin%20Wang%20and%20%20Jilei%20and%20%20Wang%20and%20Richard%20Killam%20and%20Tom%20Kwiatkowski%20and%20Kavya%20Kopparapu%20and%20Serena%20Zhan%20and%20Chao%20Jia%20and%20Alexei%20Bendebury%20and%20Sheryl%20Luo%20and%20Adri%C3%A0%20Recasens%20and%20Timothy%20Knight%20and%20Jing%20Chen%20and%20Mohak%20Patel%20and%20YaGuang%20Li%20and%20Ben%20Withbroe%20and%20Dean%20Weesner%20and%20Kush%20Bhatia%20and%20Jie%20Ren%20and%20Danielle%20Eisenbud%20and%20Ebrahim%20Songhori%20and%20Yanhua%20Sun%20and%20Travis%20Choma%20and%20Tasos%20Kementsietsidis%20and%20Lucas%20Manning%20and%20Brian%20Roark%20and%20Wael%20Farhan%20and%20Jie%20Feng%20and%20Susheel%20Tatineni%20and%20James%20Cobon-Kerr%20and%20Yunjie%20Li%20and%20Lisa%20Anne%20Hendricks%20and%20Isaac%20Noble%20and%20Chris%20Breaux%20and%20Nate%20Kushman%20and%20Liqian%20Peng%20and%20Fuzhao%20Xue%20and%20Taylor%20Tobin%20and%20Jamie%20Rogers%20and%20Josh%20Lipschultz%20and%20Chris%20Alberti%20and%20Alexey%20Vlaskin%20and%20Mostafa%20Dehghani%20and%20Roshan%20Sharma%20and%20Tris%20Warkentin%20and%20Chen-Yu%20Lee%20and%20Benigno%20Uria%20and%20Da-Cheng%20Juan%20and%20Angad%20Chandorkar%20and%20Hila%20Sheftel%20and%20Ruibo%20Liu%20and%20Elnaz%20Davoodi%20and%20Borja%20De%20Balle%20Pigem%20and%20Kedar%20Dhamdhere%20and%20David%20Ross%20and%20Jonathan%20Hoech%20and%20Mahdis%20Mahdieh%20and%20Li%20Liu%20and%20Qiujia%20Li%20and%20Liam%20McCafferty%20and%20Chenxi%20Liu%20and%20Markus%20Mircea%20and%20Yunting%20Song%20and%20Omkar%20Savant%20and%20Alaa%20Saade%20and%20Colin%20Cherry%20and%20Vincent%20Hellendoorn%20and%20Siddharth%20Goyal%20and%20Paul%20Pucciarelli%20and%20David%20Vilar%20Torres%20and%20Zohar%20Yahav%20and%20Hyo%20Lee%20and%20Lars%20Lowe%20Sjoesund%20and%20Christo%20Kirov%20and%20Bo%20Chang%20and%20Deepanway%20Ghoshal%20and%20Lu%20Li%20and%20Gilles%20Baechler%20and%20S%C3%A9bastien%20Pereira%20and%20Tara%20Sainath%20and%20Anudhyan%20Boral%20and%20Dominik%20Grewe%20and%20Afief%20Halumi%20and%20Nguyet%20Minh%20Phu%20and%20Tianxiao%20Shen%20and%20Marco%20Tulio%20Ribeiro%20and%20Dhriti%20Varma%20and%20Alex%20Kaskasoli%20and%20Vlad%20Feinberg%20and%20Navneet%20Potti%20and%20Jarrod%20Kahn%20and%20Matheus%20Wisniewski%20and%20Shakir%20Mohamed%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Bobak%20Shahriari%20and%20Jean-Baptiste%20Lespiau%20and%20Lisa%20Patel%20and%20Legg%20Yeung%20and%20Tom%20Paine%20and%20Lantao%20Mei%20and%20Alex%20Ramirez%20and%20Rakesh%20Shivanna%20and%20Li%20Zhong%20and%20Josh%20Woodward%20and%20Guilherme%20Tubone%20and%20Samira%20Khan%20and%20Heng%20Chen%20and%20Elizabeth%20Nielsen%20and%20Catalin%20Ionescu%20and%20Utsav%20Prabhu%20and%20Mingcen%20Gao%20and%20Qingze%20Wang%20and%20Sean%20Augenstein%20and%20Neesha%20Subramaniam%20and%20Jason%20Chang%20and%20Fotis%20Iliopoulos%20and%20Jiaming%20Luo%20and%20Myriam%20Khan%20and%20Weicheng%20Kuo%20and%20Denis%20Teplyashin%20and%20Florence%20Perot%20and%20Logan%20Kilpatrick%20and%20Amir%20Globerson%20and%20Hongkun%20Yu%20and%20Anfal%20Siddiqui%20and%20Nick%20Sukhanov%20and%20Arun%20Kandoor%20and%20Umang%20Gupta%20and%20Marco%20Andreetto%20and%20Moran%20Ambar%20and%20Donnie%20Kim%20and%20Pawe%C5%82%20Weso%C5%82owski%20and%20Sarah%20Perrin%20and%20Ben%20Limonchik%20and%20Wei%20Fan%20and%20Jim%20Stephan%20and%20Ian%20Stewart-Binks%20and%20Ryan%20Kappedal%20and%20Tong%20He%20and%20Sarah%20Cogan%20and%20Romina%20Datta%20and%20Tong%20Zhou%20and%20Jiayu%20Ye%20and%20Leandro%20Kieliger%20and%20Ana%20Ramalho%20and%20Kyle%20Kastner%20and%20Fabian%20Mentzer%20and%20Wei-Jen%20Ko%20and%20Arun%20Suggala%20and%20Tianhao%20Zhou%20and%20Shiraz%20Butt%20and%20Hana%20Strej%C4%8Dek%20and%20Lior%20Belenki%20and%20Subhashini%20Venugopalan%20and%20Mingyang%20Ling%20and%20Evgenii%20Eltyshev%20and%20Yunxiao%20Deng%20and%20Geza%20Kovacs%20and%20Mukund%20Raghavachari%20and%20Hanjun%20Dai%20and%20Tal%20Schuster%20and%20Steven%20Schwarcz%20and%20Richard%20Nguyen%20and%20Arthur%20Nguyen%20and%20Gavin%20Buttimore%20and%20Shrestha%20Basu%20Mallick%20and%20Sudeep%20Gandhe%20and%20Seth%20Benjamin%20and%20Michal%20Jastrzebski%20and%20Le%20Yan%20and%20Sugato%20Basu%20and%20Chris%20Apps%20and%20Isabel%20Edkins%20and%20James%20Allingham%20and%20Immanuel%20Odisho%20and%20Tomas%20Kocisky%20and%20Jewel%20Zhao%20and%20Linting%20Xue%20and%20Apoorv%20Reddy%20and%20Chrysovalantis%20Anastasiou%20and%20Aviel%20Atias%20and%20Sam%20Redmond%20and%20Kieran%20Milan%20and%20Nicolas%20Heess%20and%20Herman%20Schmit%20and%20Allan%20Dafoe%20and%20Daniel%20Andor%20and%20Tynan%20Gangwani%20and%20Anca%20Dragan%20and%20Sheng%20Zhang%20and%20Ashyana%20Kachra%20and%20Gang%20Wu%20and%20Siyang%20Xue%20and%20Kevin%20Aydin%20and%20Siqi%20Liu%20and%20Yuxiang%20Zhou%20and%20Mahan%20Malihi%20and%20Austin%20Wu%20and%20Siddharth%20Gopal%20and%20Candice%20Schumann%20and%20Peter%20Stys%20and%20Alek%20Wang%20and%20Mirek%20Ol%C5%A1%C3%A1k%20and%20Dangyi%20Liu%20and%20Christian%20Schallhart%20and%20Yiran%20Mao%20and%20Demetra%20Brady%20and%20Hao%20Xu%20and%20Tomas%20Mery%20and%20Chawin%20Sitawarin%20and%20Siva%20Velusamy%20and%20Tom%20Cobley%20and%20Alex%20Zhai%20and%20Christian%20Walder%20and%20Nitzan%20Katz%20and%20Ganesh%20Jawahar%20and%20Chinmay%20Kulkarni%20and%20Antoine%20Yang%20and%20Adam%20Paszke%20and%20Yinan%20Wang%20and%20Bogdan%20Damoc%20and%20Zal%C3%A1n%20Borsos%20and%20Ray%20Smith%20and%20Jinning%20Li%20and%20Mansi%20Gupta%20and%20Andrei%20Kapishnikov%20and%20Sushant%20Prakash%20and%20Florian%20Luisier%20and%20Rishabh%20Agarwal%20and%20Will%20Grathwohl%20and%20Kuangyuan%20Chen%20and%20Kehang%20Han%20and%20Nikhil%20Mehta%20and%20Andrew%20Over%20and%20Shekoofeh%20Azizi%20and%20Lei%20Meng%20and%20Niccol%C3%B2%20Dal%20Santo%20and%20Kelvin%20Zheng%20and%20Jane%20Shapiro%20and%20Igor%20Petrovski%20and%20Jeffrey%20Hui%20and%20Amin%20Ghafouri%20and%20Jasper%20Snoek%20and%20James%20Qin%20and%20Mandy%20Jordan%20and%20Caitlin%20Sikora%20and%20Jonathan%20Malmaud%20and%20Yuheng%20Kuang%20and%20Aga%20%C5%9Awietlik%20and%20Ruoxin%20Sang%20and%20Chongyang%20Shi%20and%20Leon%20Li%20and%20Andrew%20Rosenberg%20and%20Shubin%20Zhao%20and%20Andy%20Crawford%20and%20Jan-Thorsten%20Peter%20and%20Yun%20Lei%20and%20Xavier%20Garcia%20and%20Long%20Le%20and%20Todd%20Wang%20and%20Julien%20Amelot%20and%20Dave%20Orr%20and%20Praneeth%20Kacham%20and%20Dana%20Alon%20and%20Gladys%20Tyen%20and%20Abhinav%20Arora%20and%20James%20Lyon%20and%20Alex%20Kurakin%20and%20Mimi%20Ly%20and%20Theo%20Guidroz%20and%20Zhipeng%20Yan%20and%20Rina%20Panigrahy%20and%20Pingmei%20Xu%20and%20Thais%20Kagohara%20and%20Yong%20Cheng%20and%20Eric%20Noland%20and%20Jinhyuk%20Lee%20and%20Jonathan%20Lee%20and%20Cathy%20Yip%20and%20Maria%20Wang%20and%20Efrat%20Nehoran%20and%20Alexander%20Bykovsky%20and%20Zhihao%20Shan%20and%20Ankit%20Bhagatwala%20and%20Chaochao%20Yan%20and%20Jie%20Tan%20and%20Guillermo%20Garrido%20and%20Dan%20Ethier%20and%20Nate%20Hurley%20and%20Grace%20Vesom%20and%20Xu%20Chen%20and%20Siyuan%20Qiao%20and%20Abhishek%20Nayyar%20and%20Julian%20Walker%20and%20Paramjit%20Sandhu%20and%20Mihaela%20Rosca%20and%20Danny%20Swisher%20and%20Mikhail%20Dektiarev%20and%20Josh%20Dillon%20and%20George-Cristian%20Muraru%20and%20Manuel%20Tragut%20and%20Artiom%20Myaskovsky%20and%20David%20Reid%20and%20Marko%20Velic%20and%20Owen%20Xiao%20and%20Jasmine%20George%20and%20Mark%20Brand%20and%20Jing%20Li%20and%20Wenhao%20Yu%20and%20Shane%20Gu%20and%20Xiang%20Deng%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Soheil%20Hassas%20Yeganeh%20and%20Fred%20Alcober%20and%20Celine%20Smith%20and%20Trevor%20Cohn%20and%20Kay%20McKinney%20and%20Michael%20Tschannen%20and%20Ramesh%20Sampath%20and%20Gowoon%20Cheon%20and%20Liangchen%20Luo%20and%20Luyang%20Liu%20and%20Jordi%20Orbay%20and%20Hui%20Peng%20and%20Gabriela%20Botea%20and%20Xiaofan%20Zhang%20and%20Charles%20Yoon%20and%20Cesar%20Magalhaes%20and%20Pawe%C5%82%20Stradomski%20and%20Ian%20Mackinnon%20and%20Steven%20Hemingray%20and%20Kumaran%20Venkatesan%20and%20Rhys%20May%20and%20Jaeyoun%20Kim%20and%20Alex%20Druinsky%20and%20Jingchen%20Ye%20and%20Zheng%20Xu%20and%20Terry%20Huang%20and%20Jad%20Al%20Abdallah%20and%20Adil%20Dostmohamed%20and%20Rachana%20Fellinger%20and%20Tsendsuren%20Munkhdalai%20and%20Akanksha%20Maurya%20and%20Peter%20Garst%20and%20Yin%20Zhang%20and%20Maxim%20Krikun%20and%20Simon%20Bucher%20and%20Aditya%20Srikanth%20Veerubhotla%20and%20Yaxin%20Liu%20and%20Sheng%20Li%20and%20Nishesh%20Gupta%20and%20Jakub%20Adamek%20and%20Hanwen%20Chen%20and%20Bernett%20Orlando%20and%20Aleksandr%20Zaks%20and%20Joost%20van%20Amersfoort%20and%20Josh%20Camp%20and%20Hui%20Wan%20and%20HyunJeong%20Choe%20and%20Zhichun%20Wu%20and%20Kate%20Olszewska%20and%20Weiren%20Yu%20and%20Archita%20Vadali%20and%20Martin%20Scholz%20and%20Daniel%20De%20Freitas%20and%20Jason%20Lin%20and%20Amy%20Hua%20and%20Xin%20Liu%20and%20Frank%20Ding%20and%20Yichao%20Zhou%20and%20Boone%20Severson%20and%20Katerina%20Tsihlas%20and%20Samuel%20Yang%20and%20Tammo%20Spalink%20and%20Varun%20Yerram%20and%20Helena%20Pankov%20and%20Rory%20Blevins%20and%20Ben%20Vargas%20and%20Sarthak%20Jauhari%20and%20Matt%20Miecnikowski%20and%20Ming%20Zhang%20and%20Sandeep%20Kumar%20and%20Clement%20Farabet%20and%20Charline%20Le%20Lan%20and%20Sebastian%20Flennerhag%20and%20Yonatan%20Bitton%20and%20Ada%20Ma%20and%20Arthur%20Bra%C5%BEinskas%20and%20Eli%20Collins%20and%20Niharika%20Ahuja%20and%20Sneha%20Kudugunta%20and%20Anna%20Bortsova%20and%20Minh%20Giang%20and%20Wanzheng%20Zhu%20and%20Ed%20Chi%20and%20Scott%20Lundberg%20and%20Alexey%20Stern%20and%20Subha%20Puttagunta%20and%20Jing%20Xiong%20and%20Xiao%20Wu%20and%20Yash%20Pande%20and%20Amit%20Jhindal%20and%20Daniel%20Murphy%20and%20Jon%20Clark%20and%20Marc%20Brockschmidt%20and%20Maxine%20Deines%20and%20Kevin%20R.%20McKee%20and%20Dan%20Bahir%20and%20Jiajun%20Shen%20and%20Minh%20Truong%20and%20Daniel%20McDuff%20and%20Andrea%20Gesmundo%20and%20Edouard%20Rosseel%20and%20Bowen%20Liang%20and%20Ken%20Caluwaerts%20and%20Jessica%20Hamrick%20and%20Joseph%20Kready%20and%20Mary%20Cassin%20and%20Rishikesh%20Ingale%20and%20Li%20Lao%20and%20Scott%20Pollom%20and%20Yifan%20Ding%20and%20Wei%20He%20and%20Lizzetth%20Bellot%20and%20Joana%20Iljazi%20and%20Ramya%20Sree%20Boppana%20and%20Shan%20Han%20and%20Tara%20Thompson%20and%20Amr%20Khalifa%20and%20Anna%20Bulanova%20and%20Blagoj%20Mitrevski%20and%20Bo%20Pang%20and%20Emma%20Cooney%20and%20Tian%20Shi%20and%20Rey%20Coaguila%20and%20Tamar%20Yakar%20and%20Marc%27aurelio%20Ranzato%20and%20Nikola%20Momchev%20and%20Chris%20Rawles%20and%20Zachary%20Charles%20and%20Young%20Maeng%20and%20Yuan%20Zhang%20and%20Rishabh%20Bansal%20and%20Xiaokai%20Zhao%20and%20Brian%20Albert%20and%20Yuan%20Yuan%20and%20Sudheendra%20Vijayanarasimhan%20and%20Roy%20Hirsch%20and%20Vinay%20Ramasesh%20and%20Kiran%20Vodrahalli%20and%20Xingyu%20Wang%20and%20Arushi%20Gupta%20and%20DJ%20Strouse%20and%20Jianmo%20Ni%20and%20Roma%20Patel%20and%20Gabe%20Taubman%20and%20Zhouyuan%20Huo%20and%20Dero%20Gharibian%20and%20Marianne%20Monteiro%20and%20Hoi%20Lam%20and%20Shobha%20Vasudevan%20and%20Aditi%20Chaudhary%20and%20Isabela%20Albuquerque%20and%20Kilol%20Gupta%20and%20Sebastian%20Riedel%20and%20Chaitra%20Hegde%20and%20Avraham%20Ruderman%20and%20Andr%C3%A1s%20Gy%C3%B6rgy%20and%20Marcus%20Wainwright%20and%20Ashwin%20Chaugule%20and%20Burcu%20Karagol%20Ayan%20and%20Tomer%20Levinboim%20and%20Sam%20Shleifer%20and%20Yogesh%20Kalley%20and%20Vahab%20Mirrokni%20and%20Abhishek%20Rao%20and%20Prabakar%20Radhakrishnan%20and%20Jay%20Hartford%20and%20Jialin%20Wu%20and%20Zhenhai%20Zhu%20and%20Francesco%20Bertolini%20and%20Hao%20Xiong%20and%20Nicolas%20Serrano%20and%20Hamish%20Tomlinson%20and%20Myle%20Ott%20and%20Yifan%20Chang%20and%20Mark%20Graham%20and%20Jian%20Li%20and%20Marco%20Liang%20and%20Xiangzhu%20Long%20and%20Sebastian%20Borgeaud%20and%20Yanif%20Ahmad%20and%20Alex%20Grills%20and%20Diana%20Mincu%20and%20Martin%20Izzard%20and%20Yuan%20Liu%20and%20Jinyu%20Xie%20and%20Louis%20O%27Bryan%20and%20Sameera%20Ponda%20and%20Simon%20Tong%20and%20Michelle%20Liu%20and%20Dan%20Malkin%20and%20Khalid%20Salama%20and%20Yuankai%20Chen%20and%20Rohan%20Anil%20and%20Anand%20Rao%20and%20Rigel%20Swavely%20and%20Misha%20Bilenko%20and%20Nina%20Anderson%20and%20Tat%20Tan%20and%20Jing%20Xie%20and%20Xing%20Wu%20and%20Lijun%20Yu%20and%20Oriol%20Vinyals%20and%20Andrey%20Ryabtsev%20and%20Rumen%20Dangovski%20and%20Kate%20Baumli%20and%20Daniel%20Keysers%20and%20Christian%20Wright%20and%20Zoe%20Ashwood%20and%20Betty%20Chan%20and%20Artem%20Shtefan%20and%20Yaohui%20Guo%20and%20Ankur%20Bapna%20and%20Radu%20Soricut%20and%20Steven%20Pecht%20and%20Sabela%20Ramos%20and%20Rui%20Wang%20and%20Jiahao%20Cai%20and%20Trieu%20Trinh%20and%20Paul%20Barham%20and%20Linda%20Friso%20and%20Eli%20Stickgold%20and%20Xiangzhuo%20Ding%20and%20Siamak%20Shakeri%20and%20Diego%20Ardila%20and%20Eleftheria%20Briakou%20and%20Phil%20Culliton%20and%20Adam%20Raveret%20and%20Jingyu%20Cui%20and%20David%20Saxton%20and%20Subhrajit%20Roy%20and%20Javad%20Azizi%20and%20Pengcheng%20Yin%20and%20Lucia%20Loher%20and%20Andrew%20Bunner%20and%20Min%20Choi%20and%20Faruk%20Ahmed%20and%20Eric%20Li%20and%20Yin%20Li%20and%20Shengyang%20Dai%20and%20Michael%20Elabd%20and%20Sriram%20Ganapathy%20and%20Shivani%20Agrawal%20and%20Yiqing%20Hua%20and%20Paige%20Kunkle%20and%20Sujeevan%20Rajayogam%20and%20Arun%20Ahuja%20and%20Arthur%20Conmy%20and%20Alex%20Vasiloff%20and%20Parker%20Beak%20and%20Christopher%20Yew%20and%20Jayaram%20Mudigonda%20and%20Bartek%20Wydrowski%20and%20Jon%20Blanton%20and%20Zhengdong%20Wang%20and%20Yann%20Dauphin%20and%20Zhuo%20Xu%20and%20Martin%20Polacek%20and%20Xi%20Chen%20and%20Hexiang%20Hu%20and%20Pauline%20Sho%20and%20Markus%20Kunesch%20and%20Mehdi%20Hafezi%20Manshadi%20and%20Eliza%20Rutherford%20and%20Bo%20Li%20and%20Sissie%20Hsiao%20and%20Iain%20Barr%20and%20Alex%20Tudor%20and%20Matija%20Kecman%20and%20Arsha%20Nagrani%20and%20Vladimir%20Pchelin%20and%20Martin%20Sundermeyer%20and%20Aishwarya%20P%20S%20and%20Abhijit%20Karmarkar%20and%20Yi%20Gao%20and%20Grishma%20Chole%20and%20Olivier%20Bachem%20and%20Isabel%20Gao%20and%20Arturo%20BC%20and%20Matt%20Dibb%20and%20Mauro%20Verzetti%20and%20Felix%20Hernandez-Campos%20and%20Yana%20Lunts%20and%20Matthew%20Johnson%20and%20Julia%20Di%20Trapani%20and%20Raphael%20Koster%20and%20Idan%20Brusilovsky%20and%20Binbin%20Xiong%20and%20Megha%20Mohabey%20and%20Han%20Ke%20and%20Joe%20Zou%20and%20Tea%20Saboli%C4%87%20and%20V%C3%ADctor%20Campos%20and%20John%20Palowitch%20and%20Alex%20Morris%20and%20Linhai%20Qiu%20and%20Pranavaraj%20Ponnuramu%20and%20Fangtao%20Li%20and%20Vivek%20Sharma%20and%20Kiranbir%20Sodhia%20and%20Kaan%20Tekelioglu%20and%20Aleksandr%20Chuklin%20and%20Madhavi%20Yenugula%20and%20Erika%20Gemzer%20and%20Theofilos%20Strinopoulos%20and%20Sam%20El-Husseini%20and%20Huiyu%20Wang%20and%20Yan%20Zhong%20and%20Edouard%20Leurent%20and%20Paul%20Natsev%20and%20Weijun%20Wang%20and%20Dre%20Mahaarachchi%20and%20Tao%20Zhu%20and%20Songyou%20Peng%20and%20Sami%20Alabed%20and%20Cheng-Chun%20Lee%20and%20Anthony%20Brohan%20and%20Arthur%20Szlam%20and%20GS%20Oh%20and%20Anton%20Kovsharov%20and%20Jenny%20Lee%20and%20Renee%20Wong%20and%20Megan%20Barnes%20and%20Gregory%20Thornton%20and%20Felix%20Gimeno%20and%20Omer%20Levy%20and%20Martin%20Sevenich%20and%20Melvin%20Johnson%20and%20Jonathan%20Mallinson%20and%20Robert%20Dadashi%20and%20Ziyue%20Wang%20and%20Qingchun%20Ren%20and%20Preethi%20Lahoti%20and%20Arka%20Dhar%20and%20Josh%20Feldman%20and%20Dan%20Zheng%20and%20Thatcher%20Ulrich%20and%20Liviu%20Panait%20and%20Michiel%20Blokzijl%20and%20Cip%20Baetu%20and%20Josip%20Matak%20and%20Jitendra%20Harlalka%20and%20Maulik%20Shah%20and%20Tal%20Marian%20and%20Daniel%20von%20Dincklage%20and%20Cosmo%20Du%20and%20Ruy%20Ley-Wild%20and%20Bethanie%20Brownfield%20and%20Max%20Schumacher%20and%20Yury%20Stuken%20and%20Shadi%20Noghabi%20and%20Sonal%20Gupta%20and%20Xiaoqi%20Ren%20and%20Eric%20Malmi%20and%20Felix%20Weissenberger%20and%20Blanca%20Huergo%20and%20Maria%20Bauza%20and%20Thomas%20Lampe%20and%20Arthur%20Douillard%20and%20Mojtaba%20Seyedhosseini%20and%20Roy%20Frostig%20and%20Zoubin%20Ghahramani%20and%20Kelvin%20Nguyen%20and%20Kashyap%20Krishnakumar%20and%20Chengxi%20Ye%20and%20Rahul%20Gupta%20and%20Alireza%20Nazari%20and%20Robert%20Geirhos%20and%20Pete%20Shaw%20and%20Ahmed%20Eleryan%20and%20Dima%20Damen%20and%20Jennimaria%20Palomaki%20and%20Ted%20Xiao%20and%20Qiyin%20Wu%20and%20Quan%20Yuan%20and%20Phoenix%20Meadowlark%20and%20Matthew%20Bilotti%20and%20Raymond%20Lin%20and%20Mukund%20Sridhar%20and%20Yannick%20Schroecker%20and%20Da-Woon%20Chung%20and%20Jincheng%20Luo%20and%20Trevor%20Strohman%20and%20Tianlin%20Liu%20and%20Anne%20Zheng%20and%20Jesse%20Emond%20and%20Wei%20Wang%20and%20Andrew%20Lampinen%20and%20Toshiyuki%20Fukuzawa%20and%20Folawiyo%20Campbell-Ajala%20and%20Monica%20Roy%20and%20James%20Lee-Thorp%20and%20Lily%20Wang%20and%20Iftekhar%20Naim%20and%20%20Tony%20and%20%20Nguy%5C~%C3%AAn%20and%20Guy%20Bensky%20and%20Aditya%20Gupta%20and%20Dominika%20Rogozi%C5%84ska%20and%20Justin%20Fu%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Petar%20Veli%C4%8Dkovi%C4%87%20and%20Shahar%20Drath%20and%20Philipp%20Neubeck%20and%20Vaibhav%20Tulsyan%20and%20Arseniy%20Klimovskiy%20and%20Don%20Metzler%20and%20Sage%20Stevens%20and%20Angel%20Yeh%20and%20Junwei%20Yuan%20and%20Tianhe%20Yu%20and%20Kelvin%20Zhang%20and%20Alec%20Go%20and%20Vincent%20Tsang%20and%20Ying%20Xu%20and%20Andy%20Wan%20and%20Isaac%20Galatzer-Levy%20and%20Sam%20Sobell%20and%20Abodunrinwa%20Toki%20and%20Elizabeth%20Salesky%20and%20Wenlei%20Zhou%20and%20Diego%20Antognini%20and%20Sholto%20Douglas%20and%20Shimu%20Wu%20and%20Adam%20Lelkes%20and%20Frank%20Kim%20and%20Paul%20Cavallaro%20and%20Ana%20Salazar%20and%20Yuchi%20Liu%20and%20James%20Besley%20and%20Tiziana%20Refice%20and%20Yiling%20Jia%20and%20Zhang%20Li%20and%20Michal%20Sokolik%20and%20Arvind%20Kannan%20and%20Jon%20Simon%20and%20Jo%20Chick%20and%20Avia%20Aharon%20and%20Meet%20Gandhi%20and%20Mayank%20Daswani%20and%20Keyvan%20Amiri%20and%20Vighnesh%20Birodkar%20and%20Abe%20Ittycheriah%20and%20Peter%20Grabowski%20and%20Oscar%20Chang%20and%20Charles%20Sutton%20and%20%20Zhixin%20and%20%20Lai%20and%20Umesh%20Telang%20and%20Susie%20Sargsyan%20and%20Tao%20Jiang%20and%20Raphael%20Hoffmann%20and%20Nicole%20Brichtova%20and%20Matteo%20Hessel%20and%20Jonathan%20Halcrow%20and%20Sammy%20Jerome%20and%20Geoff%20Brown%20and%20Alex%20Tomala%20and%20Elena%20Buchatskaya%20and%20Dian%20Yu%20and%20Sachit%20Menon%20and%20Pol%20Moreno%20and%20Yuguo%20Liao%20and%20Vicky%20Zayats%20and%20Luming%20Tang%20and%20SQ%20Mah%20and%20Ashish%20Shenoy%20and%20Alex%20Siegman%20and%20Majid%20Hadian%20and%20Okwan%20Kwon%20and%20Tao%20Tu%20and%20Nima%20Khajehnouri%20and%20Ryan%20Foley%20and%20Parisa%20Haghani%20and%20Zhongru%20Wu%20and%20Vaishakh%20Keshava%20and%20Khyatti%20Gupta%20and%20Tony%20Bruguier%20and%20Rui%20Yao%20and%20Danny%20Karmon%20and%20Luisa%20Zintgraf%20and%20Zhicheng%20Wang%20and%20Enrique%20Piqueras%20and%20Junehyuk%20Jung%20and%20Jenny%20Brennan%20and%20Diego%20Machado%20and%20Marissa%20Giustina%20and%20MH%20Tessler%20and%20Kamyu%20Lee%20and%20Qiao%20Zhang%20and%20Joss%20Moore%20and%20Kaspar%20Daugaard%20and%20Alexander%20Fr%C3%B6mmgen%20and%20Jennifer%20Beattie%20and%20Fred%20Zhang%20and%20Daniel%20Kasenberg%20and%20Ty%20Geri%20and%20Danfeng%20Qin%20and%20Gaurav%20Singh%20Tomar%20and%20Tom%20Ouyang%20and%20Tianli%20Yu%20and%20Luowei%20Zhou%20and%20Rajiv%20Mathews%20and%20Andy%20Davis%20and%20Yaoyiran%20Li%20and%20Jai%20Gupta%20and%20Damion%20Yates%20and%20Linda%20Deng%20and%20Elizabeth%20Kemp%20and%20Ga-Young%20Joung%20and%20Sergei%20Vassilvitskii%20and%20Mandy%20Guo%20and%20Pallavi%20LV%20and%20Dave%20Dopson%20and%20Sami%20Lachgar%20and%20Lara%20McConnaughey%20and%20Himadri%20Choudhury%20and%20Dragos%20Dena%20and%20Aaron%20Cohen%20and%20Joshua%20Ainslie%20and%20Sergey%20Levi%20and%20Parthasarathy%20Gopavarapu%20and%20Polina%20Zablotskaia%20and%20Hugo%20Vallet%20and%20Sanaz%20Bahargam%20and%20Xiaodan%20Tang%20and%20Nenad%20Tomasev%20and%20Ethan%20Dyer%20and%20Daniel%20Balle%20and%20Hongrae%20Lee%20and%20William%20Bono%20and%20Jorge%20Gonzalez%20Mendez%20and%20Vadim%20Zubov%20and%20Shentao%20Yang%20and%20Ivor%20Rendulic%20and%20Yanyan%20Zheng%20and%20Andrew%20Hogue%20and%20Golan%20Pundak%20and%20Ralph%20Leith%20and%20Avishkar%20Bhoopchand%20and%20Michael%20Han%20and%20Mislav%20%C5%BDani%C4%87%20and%20Tom%20Schaul%20and%20Manolis%20Delakis%20and%20Tejas%20Iyer%20and%20Guanyu%20Wang%20and%20Harman%20Singh%20and%20Abdelrahman%20Abdelhamed%20and%20Tara%20Thomas%20and%20Siddhartha%20Brahma%20and%20Hilal%20Dib%20and%20Naveen%20Kumar%20and%20Wenxuan%20Zhou%20and%20Liang%20Bai%20and%20Pushkar%20Mishra%20and%20Jiao%20Sun%20and%20Valentin%20Anklin%20and%20Roykrong%20Sukkerd%20and%20Lauren%20Agubuzu%20and%20Anton%20Briukhov%20and%20Anmol%20Gulati%20and%20Maximilian%20Sieb%20and%20Fabio%20Pardo%20and%20Sara%20Nasso%20and%20Junquan%20Chen%20and%20Kexin%20Zhu%20and%20Tiberiu%20Sosea%20and%20Alex%20Goldin%20and%20Keith%20Rush%20and%20Spurthi%20Amba%20Hombaiah%20and%20Andreas%20Noever%20and%20Allan%20Zhou%20and%20Sam%20Haves%20and%20Mary%20Phuong%20and%20Jake%20Ades%20and%20Yi-ting%20Chen%20and%20Lin%20Yang%20and%20Joseph%20Pagadora%20and%20Stan%20Bileschi%20and%20Victor%20Cotruta%20and%20Rachel%20Saputro%20and%20Arijit%20Pramanik%20and%20Sean%20Ammirati%20and%20Dan%20Garrette%20and%20Kevin%20Villela%20and%20Tim%20Blyth%20and%20Canfer%20Akbulut%20and%20Neha%20Jha%20and%20Alban%20Rrustemi%20and%20Arissa%20Wongpanich%20and%20Chirag%20Nagpal%20and%20Yonghui%20Wu%20and%20Morgane%20Rivi%C3%A8re%20and%20Sergey%20Kishchenko%20and%20Pranesh%20Srinivasan%20and%20Alice%20Chen%20and%20Animesh%20Sinha%20and%20Trang%20Pham%20and%20Bill%20Jia%20and%20Tom%20Hennigan%20and%20Anton%20Bakalov%20and%20Nithya%20Attaluri%20and%20Drew%20Garmon%20and%20Daniel%20Rodriguez%20and%20Dawid%20Wegner%20and%20Wenhao%20Jia%20and%20Evan%20Senter%20and%20Noah%20Fiedel%20and%20Denis%20Petek%20and%20Yuchuan%20Liu%20and%20Cassidy%20Hardin%20and%20Harshal%20Tushar%20Lehri%20and%20Joao%20Carreira%20and%20Sara%20Smoot%20and%20Marcel%20Prasetya%20and%20Nami%20Akazawa%20and%20Anca%20Stefanoiu%20and%20Chia-Hua%20Ho%20and%20Anelia%20Angelova%20and%20Kate%20Lin%20and%20Min%20Kim%20and%20Charles%20Chen%20and%20Marcin%20Sieniek%20and%20Alice%20Li%20and%20Tongfei%20Guo%20and%20Sorin%20Baltateanu%20and%20Pouya%20Tafti%20and%20Michael%20Wunder%20and%20Nadav%20Olmert%20and%20Divyansh%20Shukla%20and%20Jingwei%20Shen%20and%20Neel%20Kovelamudi%20and%20Balaji%20Venkatraman%20and%20Seth%20Neel%20and%20Romal%20Thoppilan%20and%20Jerome%20Connor%20and%20Frederik%20Benzing%20and%20Axel%20Stjerngren%20and%20Golnaz%20Ghiasi%20and%20Alex%20Polozov%20and%20Joshua%20Howland%20and%20Theophane%20Weber%20and%20Justin%20Chiu%20and%20Ganesh%20Poomal%20Girirajan%20and%20Andreas%20Terzis%20and%20Pidong%20Wang%20and%20Fangda%20Li%20and%20Yoav%20Ben%20Shalom%20and%20Dinesh%20Tewari%20and%20Matthew%20Denton%20and%20Roee%20Aharoni%20and%20Norbert%20Kalb%20and%20Heri%20Zhao%20and%20Junlin%20Zhang%20and%20Angelos%20Filos%20and%20Matthew%20Rahtz%20and%20Lalit%20Jain%20and%20Connie%20Fan%20and%20Vitor%20Rodrigues%20and%20Ruth%20Wang%20and%20Richard%20Shin%20and%20Jacob%20Austin%20and%20Roman%20Ring%20and%20Mariella%20Sanchez-Vargas%20and%20Mehadi%20Hassen%20and%20Ido%20Kessler%20and%20Uri%20Alon%20and%20Gufeng%20Zhang%20and%20Wenhu%20Chen%20and%20Yenai%20Ma%20and%20Xiance%20Si%20and%20Le%20Hou%20and%20Azalia%20Mirhoseini%20and%20Marc%20Wilson%20and%20Geoff%20Bacon%20and%20Becca%20Roelofs%20and%20Lei%20Shu%20and%20Gautam%20Vasudevan%20and%20Jonas%20Adler%20and%20Artur%20Dwornik%20and%20Tayfun%20Terzi%20and%20Matt%20Lawlor%20and%20Harry%20Askham%20and%20Mike%20Bernico%20and%20Xuanyi%20Dong%20and%20Chris%20Hidey%20and%20Kevin%20Kilgour%20and%20Ga%C3%ABl%20Liu%20and%20Surya%20Bhupatiraju%20and%20Luke%20Leonhard%20and%20Siqi%20Zuo%20and%20Partha%20Talukdar%20and%20Qing%20Wei%20and%20Aliaksei%20Severyn%20and%20V%C3%ADt%20List%C3%ADk%20and%20Jong%20Lee%20and%20Aditya%20Tripathi%20and%20SK%20Park%20and%20Yossi%20Matias%20and%20Hao%20Liu%20and%20Alex%20Ruiz%20and%20Rajesh%20Jayaram%20and%20Jackson%20Tolins%20and%20Pierre%20Marcenac%20and%20Yiming%20Wang%20and%20Bryan%20Seybold%20and%20Henry%20Prior%20and%20Deepak%20Sharma%20and%20Jack%20Weber%20and%20Mikhail%20Sirotenko%20and%20Yunhsuan%20Sung%20and%20Dayou%20Du%20and%20Ellie%20Pavlick%20and%20Stefan%20Zinke%20and%20Markus%20Freitag%20and%20Max%20Dylla%20and%20Montse%20Gonzalez%20Arenas%20and%20Natan%20Potikha%20and%20Omer%20Goldman%20and%20Connie%20Tao%20and%20Rachita%20Chhaparia%20and%20Maria%20Voitovich%20and%20Pawan%20Dogra%20and%20Andrija%20Ra%C5%BEnatovi%C4%87%20and%20Zak%20Tsai%20and%20Chong%20You%20and%20Oleaser%20Johnson%20and%20George%20Tucker%20and%20Chenjie%20Gu%20and%20Jae%20Yoo%20and%20Maryam%20Majzoubi%20and%20Valentin%20Gabeur%20and%20Bahram%20Raad%20and%20Rocky%20Rhodes%20and%20Kashyap%20Kolipaka%20and%20Heidi%20Howard%20and%20Geta%20Sampemane%20and%20Benny%20Li%20and%20Chulayuth%20Asawaroengchai%20and%20Duy%20Nguyen%20and%20Chiyuan%20Zhang%20and%20Timothee%20Cour%20and%20Xinxin%20Yu%20and%20Zhao%20Fu%20and%20Joe%20Jiang%20and%20Po-Sen%20Huang%20and%20Gabriela%20Surita%20and%20I%C3%B1aki%20Iturrate%20and%20Yael%20Karov%20and%20Michael%20Collins%20and%20Martin%20Baeuml%20and%20Fabian%20Fuchs%20and%20Shilpa%20Shetty%20and%20Swaroop%20Ramaswamy%20and%20Sayna%20Ebrahimi%20and%20Qiuchen%20Guo%20and%20Jeremy%20Shar%20and%20Gabe%20Barth-Maron%20and%20Sravanti%20Addepalli%20and%20Bryan%20Richter%20and%20Chin-Yi%20Cheng%20and%20Eug%C3%A9nie%20Rives%20and%20Fei%20Zheng%20and%20Johannes%20Griesser%20and%20Nishanth%20Dikkala%20and%20Yoel%20Zeldes%20and%20Ilkin%20Safarli%20and%20Dipanjan%20Das%20and%20Himanshu%20Srivastava%20and%20Sadh%20MNM%20Khan%20and%20Xin%20Li%20and%20Aditya%20Pandey%20and%20Larisa%20Markeeva%20and%20Dan%20Belov%20and%20Qiqi%20Yan%20and%20Miko%C5%82aj%20Rybi%C5%84ski%20and%20Tao%20Chen%20and%20Megha%20Nawhal%20and%20Michael%20Quinn%20and%20Vineetha%20Govindaraj%20and%20Sarah%20York%20and%20Reed%20Roberts%20and%20Roopal%20Garg%20and%20Namrata%20Godbole%20and%20Jake%20Abernethy%20and%20Anil%20Das%20and%20Lam%20Nguyen%20Thiet%20and%20Jonathan%20Tompson%20and%20John%20Nham%20and%20Neera%20Vats%20and%20Ben%20Caine%20and%20Wesley%20Helmholz%20and%20Francesco%20Pongetti%20and%20Yeongil%20Ko%20and%20James%20An%20and%20Clara%20Huiyi%20Hu%20and%20Yu-Cheng%20Ling%20and%20Julia%20Pawar%20and%20Robert%20Leland%20and%20Keisuke%20Kinoshita%20and%20Waleed%20Khawaja%20and%20Marco%20Selvi%20and%20Eugene%20Ie%20and%20Danila%20Sinopalnikov%20and%20Lev%20Proleev%20and%20Nilesh%20Tripuraneni%20and%20Michele%20Bevilacqua%20and%20Seungji%20Lee%20and%20Clayton%20Sanford%20and%20Dan%20Suh%20and%20Dustin%20Tran%20and%20Jeff%20Dean%20and%20Simon%20Baumgartner%20and%20Jens%20Heitkaemper%20and%20Sagar%20Gubbi%20and%20Kristina%20Toutanova%20and%20Yichong%20Xu%20and%20Chandu%20Thekkath%20and%20Keran%20Rong%20and%20Palak%20Jain%20and%20Annie%20Xie%20and%20Yan%20Virin%20and%20Yang%20Li%20and%20Lubo%20Litchev%20and%20Richard%20Powell%20and%20Tarun%20Bharti%20and%20Adam%20Kraft%20and%20Nan%20Hua%20and%20Marissa%20Ikonomidis%20and%20Ayal%20Hitron%20and%20Sanjiv%20Kumar%20and%20Loic%20Matthey%20and%20Sophie%20Bridgers%20and%20Lauren%20Lax%20and%20Ishaan%20Malhi%20and%20Ondrej%20Skopek%20and%20Ashish%20Gupta%20and%20Jiawei%20Cao%20and%20Mitchelle%20Rasquinha%20and%20Siim%20P%C3%B5der%20and%20Wojciech%20Stokowiec%20and%20Nicholas%20Roth%20and%20Guowang%20Li%20and%20Micha%C3%ABl%20Sander%20and%20Joshua%20Kessinger%20and%20Vihan%20Jain%20and%20Edward%20Loper%20and%20Wonpyo%20Park%20and%20Michal%20Yarom%20and%20Liqun%20Cheng%20and%20Guru%20Guruganesh%20and%20Kanishka%20Rao%20and%20Yan%20Li%20and%20Catarina%20Barros%20and%20Mikhail%20Sushkov%20and%20Chun-Sung%20Ferng%20and%20Rohin%20Shah%20and%20Ophir%20Aharoni%20and%20Ravin%20Kumar%20and%20Tim%20McConnell%20and%20Peiran%20Li%20and%20Chen%20Wang%20and%20Fernando%20Pereira%20and%20Craig%20Swanson%20and%20Fayaz%20Jamil%20and%20Yan%20Xiong%20and%20Anitha%20Vijayakumar%20and%20Prakash%20Shroff%20and%20Kedar%20Soparkar%20and%20Jindong%20Gu%20and%20Livio%20Baldini%20Soares%20and%20Eric%20Wang%20and%20Kushal%20Majmundar%20and%20Aurora%20Wei%20and%20Kai%20Bailey%20and%20Nora%20Kassner%20and%20Chizu%20Kawamoto%20and%20Goran%20%C5%BDu%C5%BEi%C4%87%20and%20Victor%20Gomes%20and%20Abhirut%20Gupta%20and%20Michael%20Guzman%20and%20Ishita%20Dasgupta%20and%20Xinyi%20Bai%20and%20Zhufeng%20Pan%20and%20Francesco%20Piccinno%20and%20Hadas%20Natalie%20Vogel%20and%20Octavio%20Ponce%20and%20Adrian%20Hutter%20and%20Paul%20Chang%20and%20Pan-Pan%20Jiang%20and%20Ionel%20Gog%20and%20Vlad%20Ionescu%20and%20James%20Manyika%20and%20Fabian%20Pedregosa%20and%20Harry%20Ragan%20and%20Zach%20Behrman%20and%20Ryan%20Mullins%20and%20Coline%20Devin%20and%20Aroonalok%20Pyne%20and%20Swapnil%20Gawde%20and%20Martin%20Chadwick%20and%20Yiming%20Gu%20and%20Sasan%20Tavakkol%20and%20Andy%20Twigg%20and%20Naman%20Goyal%20and%20Ndidi%20Elue%20and%20Anna%20Goldie%20and%20Srinivasan%20Venkatachary%20and%20Hongliang%20Fei%20and%20Ziqiang%20Feng%20and%20Marvin%20Ritter%20and%20Isabel%20Leal%20and%20Sudeep%20Dasari%20and%20Pei%20Sun%20and%20Alif%20Raditya%20Rochman%20and%20Brendan%20O%27Donoghue%20and%20Yuchen%20Liu%20and%20Jim%20Sproch%20and%20Kai%20Chen%20and%20Natalie%20Clay%20and%20Slav%20Petrov%20and%20Sailesh%20Sidhwani%20and%20Ioana%20Mihailescu%20and%20Alex%20Panagopoulos%20and%20AJ%20Piergiovanni%20and%20Yunfei%20Bai%20and%20George%20Powell%20and%20Deep%20Karkhanis%20and%20Trevor%20Yacovone%20and%20Petr%20Mitrichev%20and%20Joe%20Kovac%20and%20Dave%20Uthus%20and%20Amir%20Yazdanbakhsh%20and%20David%20Amos%20and%20Steven%20Zheng%20and%20Bing%20Zhang%20and%20Jin%20Miao%20and%20Bhuvana%20Ramabhadran%20and%20Soroush%20Radpour%20and%20Shantanu%20Thakoor%20and%20Josh%20Newlan%20and%20Oran%20Lang%20and%20Orion%20Jankowski%20and%20Shikhar%20Bharadwaj%20and%20Jean-Michel%20Sarr%20and%20Shereen%20Ashraf%20and%20Sneha%20Mondal%20and%20Jun%20Yan%20and%20Ankit%20Singh%20Rawat%20and%20Sarmishta%20Velury%20and%20Greg%20Kochanski%20and%20Tom%20Eccles%20and%20Franz%20Och%20and%20Abhanshu%20Sharma%20and%20Ethan%20Mahintorabi%20and%20Alex%20Gurney%20and%20Carrie%20Muir%20and%20Vered%20Cohen%20and%20Saksham%20Thakur%20and%20Adam%20Bloniarz%20and%20Asier%20Mujika%20and%20Alexander%20Pritzel%20and%20Paul%20Caron%20and%20Altaf%20Rahman%20and%20Fiona%20Lang%20and%20Yasumasa%20Onoe%20and%20Petar%20Sirkovic%20and%20Jay%20Hoover%20and%20Ying%20Jian%20and%20Pablo%20Duque%20and%20Arun%20Narayanan%20and%20David%20Soergel%20and%20Alex%20Haig%20and%20Loren%20Maggiore%20and%20Shyamal%20Buch%20and%20Josef%20Dean%20and%20Ilya%20Figotin%20and%20Igor%20Karpov%20and%20Shaleen%20Gupta%20and%20Denny%20Zhou%20and%20Muhuan%20Huang%20and%20Ashwin%20Vaswani%20and%20Christopher%20Semturs%20and%20Kaushik%20Shivakumar%20and%20Yu%20Watanabe%20and%20Vinodh%20Kumar%20Rajendran%20and%20Eva%20Lu%20and%20Yanhan%20Hou%20and%20Wenting%20Ye%20and%20Shikhar%20Vashishth%20and%20Nana%20Nti%20and%20Vytenis%20Sakenas%20and%20Darren%20Ni%20and%20Doug%20DeCarlo%20and%20Michael%20Bendersky%20and%20Sumit%20Bagri%20and%20Nacho%20Cano%20and%20Elijah%20Peake%20and%20Simon%20Tokumine%20and%20Varun%20Godbole%20and%20Carlos%20Gu%C3%ADa%20and%20Tanya%20Lando%20and%20Vittorio%20Selo%20and%20Seher%20Ellis%20and%20Danny%20Tarlow%20and%20Daniel%20Gillick%20and%20Alessandro%20Epasto%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Meng%20Wei%20and%20Meiyan%20Xie%20and%20Ankur%20Taly%20and%20Michela%20Paganini%20and%20Mukund%20Sundararajan%20and%20Daniel%20Toyama%20and%20Ting%20Yu%20and%20Dessie%20Petrova%20and%20Aneesh%20Pappu%20and%20Rohan%20Agrawal%20and%20Senaka%20Buthpitiya%20and%20Justin%20Frye%20and%20Thomas%20Buschmann%20and%20Remi%20Crocker%20and%20Marco%20Tagliasacchi%20and%20Mengchao%20Wang%20and%20Da%20Huang%20and%20Sagi%20Perel%20and%20Brian%20Wieder%20and%20Hideto%20Kazawa%20and%20Weiyue%20Wang%20and%20Jeremy%20Cole%20and%20Himanshu%20Gupta%20and%20Ben%20Golan%20and%20Seojin%20Bang%20and%20Nitish%20Kulkarni%20and%20Ken%20Franko%20and%20Casper%20Liu%20and%20Doug%20Reid%20and%20Sid%20Dalmia%20and%20Jay%20Whang%20and%20Kevin%20Cen%20and%20Prasha%20Sundaram%20and%20Johan%20Ferret%20and%20Berivan%20Isik%20and%20Lucian%20Ionita%20and%20Guan%20Sun%20and%20Anna%20Shekhawat%20and%20Muqthar%20Mohammad%20and%20Philip%20Pham%20and%20Ronny%20Huang%20and%20Karthik%20Raman%20and%20Xingyi%20Zhou%20and%20Ross%20Mcilroy%20and%20Austin%20Myers%20and%20Sheng%20Peng%20and%20Jacob%20Scott%20and%20Paul%20Covington%20and%20Sofia%20Erell%20and%20Pratik%20Joshi%20and%20Jo%C3%A3o%20Gabriel%20Oliveira%20and%20Natasha%20Noy%20and%20Tajwar%20Nasir%20and%20Jake%20Walker%20and%20Vera%20Axelrod%20and%20Tim%20Dozat%20and%20Pu%20Han%20and%20Chun-Te%20Chu%20and%20Eugene%20Weinstein%20and%20Anand%20Shukla%20and%20Shreyas%20Chandrakaladharan%20and%20Petra%20Poklukar%20and%20Bonnie%20Li%20and%20Ye%20Jin%20and%20Prem%20Eruvbetine%20and%20Steven%20Hansen%20and%20Avigail%20Dabush%20and%20Alon%20Jacovi%20and%20Samrat%20Phatale%20and%20Chen%20Zhu%20and%20Steven%20Baker%20and%20Mo%20Shomrat%20and%20Yang%20Xiao%20and%20Jean%20Pouget-Abadie%20and%20Mingyang%20Zhang%20and%20Fanny%20Wei%20and%20Yang%20Song%20and%20Helen%20King%20and%20Yiling%20Huang%20and%20Yun%20Zhu%20and%20Ruoxi%20Sun%20and%20Juliana%20Vicente%20Franco%20and%20Chu-Cheng%20Lin%20and%20Sho%20Arora%20and%20%20Hui%20and%20%20Li%20and%20Vivian%20Xia%20and%20Luke%20Vilnis%20and%20Mariano%20Schain%20and%20Kaiz%20Alarakyia%20and%20Laurel%20Prince%20and%20Aaron%20Phillips%20and%20Caleb%20Habtegebriel%20and%20Luyao%20Xu%20and%20Huan%20Gui%20and%20Santiago%20Ontanon%20and%20Lora%20Aroyo%20and%20Karan%20Gill%20and%20Peggy%20Lu%20and%20Yash%20Katariya%20and%20Dhruv%20Madeka%20and%20Shankar%20Krishnan%20and%20Shubha%20Srinivas%20Raghvendra%20and%20James%20Freedman%20and%20Yi%20Tay%20and%20Gaurav%20Menghani%20and%20Peter%20Choy%20and%20Nishita%20Shetty%20and%20Dan%20Abolafia%20and%20Doron%20Kukliansky%20and%20Edward%20Chou%20and%20Jared%20Lichtarge%20and%20Ken%20Burke%20and%20Ben%20Coleman%20and%20Dee%20Guo%20and%20Larry%20Jin%20and%20Indro%20Bhattacharya%20and%20Victoria%20Langston%20and%20Yiming%20Li%20and%20Suyog%20Kotecha%20and%20Alex%20Yakubovich%20and%20Xinyun%20Chen%20and%20Petre%20Petrov%20and%20Tolly%20Powell%20and%20Yanzhang%20He%20and%20Corbin%20Quick%20and%20Kanav%20Garg%20and%20Dawsen%20Hwang%20and%20Yang%20Lu%20and%20Srinadh%20Bhojanapalli%20and%20Kristian%20Kjems%20and%20Ramin%20Mehran%20and%20Aaron%20Archer%20and%20Hado%20van%20Hasselt%20and%20Ashwin%20Balakrishna%20and%20JK%20Kearns%20and%20Meiqi%20Guo%20and%20Jason%20Riesa%20and%20Mikita%20Sazanovich%20and%20Xu%20Gao%20and%20Chris%20Sauer%20and%20Chengrun%20Yang%20and%20XiangHai%20Sheng%20and%20Thomas%20Jimma%20and%20Wouter%20Van%20Gansbeke%20and%20Vitaly%20Nikolaev%20and%20Wei%20Wei%20and%20Katie%20Millican%20and%20Ruizhe%20Zhao%20and%20Justin%20Snyder%20and%20Levent%20Bolelli%20and%20Maura%20O%27Brien%20and%20Shawn%20Xu%20and%20Fei%20Xia%20and%20Wentao%20Yuan%20and%20Arvind%20Neelakantan%20and%20David%20Barker%20and%20Sachin%20Yadav%20and%20Hannah%20Kirkwood%20and%20Farooq%20Ahmad%20and%20Joel%20Wee%20and%20Jordan%20Grimstad%20and%20Boyu%20Wang%20and%20Matthew%20Wiethoff%20and%20Shane%20Settle%20and%20Miaosen%20Wang%20and%20Charles%20Blundell%20and%20Jingjing%20Chen%20and%20Chris%20Duvarney%20and%20Grace%20Hu%20and%20Olaf%20Ronneberger%20and%20Alex%20Lee%20and%20Yuanzhen%20Li%20and%20Abhishek%20Chakladar%20and%20Alena%20Butryna%20and%20Georgios%20Evangelopoulos%20and%20Guillaume%20Desjardins%20and%20Jonni%20Kanerva%20and%20Henry%20Wang%20and%20Averi%20Nowak%20and%20Nick%20Li%20and%20Alyssa%20Loo%20and%20Art%20Khurshudov%20and%20Laurent%20El%20Shafey%20and%20Nagabhushan%20Baddi%20and%20Karel%20Lenc%20and%20Yasaman%20Razeghi%20and%20Tom%20Lieber%20and%20Amer%20Sinha%20and%20Xiao%20Ma%20and%20Yao%20Su%20and%20James%20Huang%20and%20Asahi%20Ushio%20and%20Hanna%20Klimczak-Pluci%C5%84ska%20and%20Kareem%20Mohamed%20and%20JD%20Chen%20and%20Simon%20Osindero%20and%20Stav%20Ginzburg%20and%20Lampros%20Lamprou%20and%20Vasilisa%20Bashlovkina%20and%20Duc-Hieu%20Tran%20and%20Ali%20Khodaei%20and%20Ankit%20Anand%20and%20Yixian%20Di%20and%20Ramy%20Eskander%20and%20Manish%20Reddy%20Vuyyuru%20and%20Jasmine%20Liu%20and%20Aishwarya%20Kamath%20and%20Roman%20Goldenberg%20and%20Mathias%20Bellaiche%20and%20Juliette%20Pluto%20and%20Bill%20Rosgen%20and%20Hassan%20Mansoor%20and%20William%20Wong%20and%20Suhas%20Ganesh%20and%20Eric%20Bailey%20and%20Scott%20Baird%20and%20Dan%20Deutsch%20and%20Jinoo%20Baek%20and%20Xuhui%20Jia%20and%20Chansoo%20Lee%20and%20Abe%20Friesen%20and%20Nathaniel%20Braun%20and%20Kate%20Lee%20and%20Amayika%20Panda%20and%20Steven%20M.%20Hernandez%20and%20Duncan%20Williams%20and%20Jianqiao%20Liu%20and%20Ethan%20Liang%20and%20Arnaud%20Autef%20and%20Emily%20Pitler%20and%20Deepali%20Jain%20and%20Phoebe%20Kirk%20and%20Oskar%20Bunyan%20and%20Jaume%20Sanchez%20Elias%20and%20Tongxin%20Yin%20and%20Machel%20Reid%20and%20Aedan%20Pope%20and%20Nikita%20Putikhin%20and%20Bidisha%20Samanta%20and%20Sergio%20Guadarrama%20and%20Dahun%20Kim%20and%20Simon%20Rowe%20and%20Marcella%20Valentine%20and%20Geng%20Yan%20and%20Alex%20Salcianu%20and%20David%20Silver%20and%20Gan%20Song%20and%20Richa%20Singh%20and%20Shuai%20Ye%20and%20Hannah%20DeBalsi%20and%20Majd%20Al%20Merey%20and%20Eran%20Ofek%20and%20Albert%20Webson%20and%20Shibl%20Mourad%20and%20Ashwin%20Kakarla%20and%20Silvio%20Lattanzi%20and%20Nick%20Roy%20and%20Evgeny%20Sluzhaev%20and%20Christina%20Butterfield%20and%20Alessio%20Tonioni%20and%20Nathan%20Waters%20and%20Sudhindra%20Kopalle%20and%20Jason%20Chase%20and%20James%20Cohan%20and%20Girish%20Ramchandra%20Rao%20and%20Robert%20Berry%20and%20Michael%20Voznesensky%20and%20Shuguang%20Hu%20and%20Kristen%20Chiafullo%20and%20Sharat%20Chikkerur%20and%20George%20Scrivener%20and%20Ivy%20Zheng%20and%20Jeremy%20Wiesner%20and%20Wolfgang%20Macherey%20and%20Timothy%20Lillicrap%20and%20Fei%20Liu%20and%20Brian%20Walker%20and%20David%20Welling%20and%20Elinor%20Davies%20and%20Yangsibo%20Huang%20and%20Lijie%20Ren%20and%20Nir%20Shabat%20and%20Alessandro%20Agostini%20and%20Mariko%20Iinuma%20and%20Dustin%20Zelle%20and%20Rohit%20Sathyanarayana%20and%20Andrea%20D%27olimpio%20and%20Morgan%20Redshaw%20and%20Matt%20Ginsberg%20and%20Ashwin%20Murthy%20and%20Mark%20Geller%20and%20Tatiana%20Matejovicova%20and%20Ayan%20Chakrabarti%20and%20Ryan%20Julian%20and%20Christine%20Chan%20and%20Qiong%20Hu%20and%20Daniel%20Jarrett%20and%20Manu%20Agarwal%20and%20Jeshwanth%20Challagundla%20and%20Tao%20Li%20and%20Sandeep%20Tata%20and%20Wen%20Ding%20and%20Maya%20Meng%20and%20Zhuyun%20Dai%20and%20Giulia%20Vezzani%20and%20Shefali%20Garg%20and%20Jannis%20Bulian%20and%20Mary%20Jasarevic%20and%20Honglong%20Cai%20and%20Harish%20Rajamani%20and%20Adam%20Santoro%20and%20Florian%20Hartmann%20and%20Chen%20Liang%20and%20Bartek%20Perz%20and%20Apoorv%20Jindal%20and%20Fan%20Bu%20and%20Sungyong%20Seo%20and%20Ryan%20Poplin%20and%20Adrian%20Goedeckemeyer%20and%20Badih%20Ghazi%20and%20Nikhil%20Khadke%20and%20Leon%20Liu%20and%20Kevin%20Mather%20and%20Mingda%20Zhang%20and%20Ali%20Shah%20and%20Alex%20Chen%20and%20Jinliang%20Wei%20and%20Keshav%20Shivam%20and%20Yuan%20Cao%20and%20Donghyun%20Cho%20and%20Angelo%20Scorza%20Scarpati%20and%20Michael%20Moffitt%20and%20Clara%20Barbu%20and%20Ivan%20Jurin%20and%20Ming-Wei%20Chang%20and%20Hongbin%20Liu%20and%20Hao%20Zheng%20and%20Shachi%20Dave%20and%20Christine%20Kaeser-Chen%20and%20Xiaobin%20Yu%20and%20Alvin%20Abdagic%20and%20Lucas%20Gonzalez%20and%20Yanping%20Huang%20and%20Peilin%20Zhong%20and%20Cordelia%20Schmid%20and%20Bryce%20Petrini%20and%20Alex%20Wertheim%20and%20Jifan%20Zhu%20and%20Hoang%20Nguyen%20and%20Kaiyang%20Ji%20and%20Yanqi%20Zhou%20and%20Tao%20Zhou%20and%20Fangxiaoyu%20Feng%20and%20Regev%20Cohen%20and%20David%20Rim%20and%20Shubham%20Milind%20Phal%20and%20Petko%20Georgiev%20and%20Ariel%20Brand%20and%20Yue%20Ma%20and%20Wei%20Li%20and%20Somit%20Gupta%20and%20Chao%20Wang%20and%20Pavel%20Dubov%20and%20Jean%20Tarbouriech%20and%20Kingshuk%20Majumder%20and%20Huijian%20Li%20and%20Norman%20Rink%20and%20Apurv%20Suman%20and%20Yang%20Guo%20and%20Yinghao%20Sun%20and%20Arun%20Nair%20and%20Xiaowei%20Xu%20and%20Mohamed%20Elhawaty%20and%20Rodrigo%20Cabrera%20and%20Guangxing%20Han%20and%20Julian%20Eisenschlos%20and%20Junwen%20Bai%20and%20Yuqi%20Li%20and%20Yamini%20Bansal%20and%20Thibault%20Sellam%20and%20Mina%20Khan%20and%20Hung%20Nguyen%20and%20Justin%20Mao-Jones%20and%20Nikos%20Parotsidis%20and%20Jake%20Marcus%20and%20Cindy%20Fan%20and%20Roland%20Zimmermann%20and%20Yony%20Kochinski%20and%20Laura%20Graesser%20and%20Feryal%20Behbahani%20and%20Alvaro%20Caceres%20and%20Michael%20Riley%20and%20Patrick%20Kane%20and%20Sandra%20Lefdal%20and%20Rob%20Willoughby%20and%20Paul%20Vicol%20and%20Lun%20Wang%20and%20Shujian%20Zhang%20and%20Ashleah%20Gill%20and%20Yu%20Liang%20and%20Gautam%20Prasad%20and%20Soroosh%20Mariooryad%20and%20Mehran%20Kazemi%20and%20Zifeng%20Wang%20and%20Kritika%20Muralidharan%20and%20Paul%20Voigtlaender%20and%20Jeffrey%20Zhao%20and%20Huanjie%20Zhou%20and%20Nina%20D%27Souza%20and%20Aditi%20Mavalankar%20and%20S%C3%A9b%20Arnold%20and%20Nick%20Young%20and%20Obaid%20Sarvana%20and%20Chace%20Lee%20and%20Milad%20Nasr%20and%20Tingting%20Zou%20and%20Seokhwan%20Kim%20and%20Lukas%20Haas%20and%20Kaushal%20Patel%20and%20Neslihan%20Bulut%20and%20David%20Parkinson%20and%20Courtney%20Biles%20and%20Dmitry%20Kalashnikov%20and%20Chi%20Ming%20To%20and%20Aviral%20Kumar%20and%20Jessica%20Austin%20and%20Alex%20Greve%20and%20Lei%20Zhang%20and%20Megha%20Goel%20and%20Yeqing%20Li%20and%20Sergey%20Yaroshenko%20and%20Max%20Chang%20and%20Abhishek%20Jindal%20and%20Geoff%20Clark%20and%20Hagai%20Taitelbaum%20and%20Dale%20Johnson%20and%20Ofir%20Roval%20and%20Jeongwoo%20Ko%20and%20Anhad%20Mohananey%20and%20Christian%20Schuler%20and%20Shenil%20Dodhia%20and%20Ruichao%20Li%20and%20Kazuki%20Osawa%20and%20Claire%20Cui%20and%20Peng%20Xu%20and%20Rushin%20Shah%20and%20Tao%20Huang%20and%20Ela%20Gruzewska%20and%20Nathan%20Clement%20and%20Mudit%20Verma%20and%20Olcan%20Sercinoglu%20and%20Hai%20Qian%20and%20Viral%20Shah%20and%20Masa%20Yamaguchi%20and%20Abhinit%20Modi%20and%20Takahiro%20Kosakai%20and%20Thomas%20Strohmann%20and%20Junhao%20Zeng%20and%20Beliz%20Gunel%20and%20Jun%20Qian%20and%20Austin%20Tarango%20and%20Krzysztof%20Jastrz%C4%99bski%20and%20Robert%20David%20and%20Jyn%20Shan%20and%20Parker%20Schuh%20and%20Kunal%20Lad%20and%20Willi%20Gierke%20and%20Mukundan%20Madhavan%20and%20Xinyi%20Chen%20and%20Mark%20Kurzeja%20and%20Rebeca%20Santamaria-Fernandez%20and%20Dawn%20Chen%20and%20Alexandra%20Cordell%20and%20Yuri%20Chervonyi%20and%20Frankie%20Garcia%20and%20Nithish%20Kannen%20and%20Vincent%20Perot%20and%20Nan%20Ding%20and%20Shlomi%20Cohen-Ganor%20and%20Victor%20Lavrenko%20and%20Junru%20Wu%20and%20Georgie%20Evans%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Madhavi%20Sewak%20and%20Ashley%20Brown%20and%20Andrew%20Hard%20and%20Joan%20Puigcerver%20and%20Zeyu%20Zheng%20and%20Yizhong%20Liang%20and%20Evgeny%20Gladchenko%20and%20Reeve%20Ingle%20and%20Uri%20First%20and%20Pierre%20Sermanet%20and%20Charlotte%20Magister%20and%20Mihajlo%20Velimirovi%C4%87%20and%20Sashank%20Reddi%20and%20Susanna%20Ricco%20and%20Eirikur%20Agustsson%20and%20Hartwig%20Adam%20and%20Nir%20Levine%20and%20David%20Gaddy%20and%20Dan%20Holtmann-Rice%20and%20Xuanhui%20Wang%20and%20Ashutosh%20Sathe%20and%20Abhijit%20Guha%20Roy%20and%20Bla%C5%BE%20Bratani%C4%8D%20and%20Alen%20Carin%20and%20Harsh%20Mehta%20and%20Silvano%20Bonacina%20and%20Nicola%20De%20Cao%20and%20Mara%20Finkelstein%20and%20Verena%20Rieser%20and%20Xinyi%20Wu%20and%20Florent%20Altch%C3%A9%20and%20Dylan%20Scandinaro%20and%20Li%20Li%20and%20Nino%20Vieillard%20and%20Nikhil%20Sethi%20and%20Garrett%20Tanzer%20and%20Zhi%20Xing%20and%20Shibo%20Wang%20and%20Parul%20Bhatia%20and%20Gui%20Citovsky%20and%20Thomas%20Anthony%20and%20Sharon%20Lin%20and%20Tianze%20Shi%20and%20Shoshana%20Jakobovits%20and%20Gena%20Gibson%20and%20Raj%20Apte%20and%20Lisa%20Lee%20and%20Mingqing%20Chen%20and%20Arunkumar%20Byravan%20and%20Petros%20Maniatis%20and%20Kellie%20Webster%20and%20Andrew%20Dai%20and%20Pu-Chin%20Chen%20and%20Jiaqi%20Pan%20and%20Asya%20Fadeeva%20and%20Zach%20Gleicher%20and%20Thang%20Luong%20and%20Niket%20Kumar%20Bhumihar&entry.1292438233=%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%202.X%20model%20family%3A%20Gemini%202.5%20Pro%20and%0AGemini%202.5%20Flash%2C%20as%20well%20as%20our%20earlier%20Gemini%202.0%20Flash%20and%20Flash-Lite%0Amodels.%20Gemini%202.5%20Pro%20is%20our%20most%20capable%20model%20yet%2C%20achieving%20SoTA%0Aperformance%20on%20frontier%20coding%20and%20reasoning%20benchmarks.%20In%20addition%20to%20its%0Aincredible%20coding%20and%20reasoning%20skills%2C%20Gemini%202.5%20Pro%20is%20a%20thinking%20model%20that%0Aexcels%20at%20multimodal%20understanding%20and%20it%20is%20now%20able%20to%20process%20up%20to%203%20hours%0Aof%20video%20content.%20Its%20unique%20combination%20of%20long%20context%2C%20multimodal%20and%0Areasoning%20capabilities%20can%20be%20combined%20to%20unlock%20new%20agentic%20workflows.%20Gemini%0A2.5%20Flash%20provides%20excellent%20reasoning%20abilities%20at%20a%20fraction%20of%20the%20compute%0Aand%20latency%20requirements%20and%20Gemini%202.0%20Flash%20and%20Flash-Lite%20provide%20high%0Aperformance%20at%20low%20latency%20and%20cost.%20Taken%20together%2C%20the%20Gemini%202.X%20model%0Ageneration%20spans%20the%20full%20Pareto%20frontier%20of%20model%20capability%20vs%20cost%2C%20allowing%0Ausers%20to%20explore%20the%20boundaries%20of%20what%20is%20possible%20with%20complex%20agentic%0Aproblem%20solving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06261v4&entry.124074799=Read"},
{"title": "PICACO: Pluralistic In-Context Value Alignment of LLMs via Total\n  Correlation Optimization", "author": "Han Jiang and Dongyao Zhu and Zhihua Wei and Xiaoyuan Yi and Ziang Xiao and Xing Xie", "abstract": "  In-Context Learning has shown great potential for aligning Large Language\nModels (LLMs) with human values, helping reduce harmful outputs and accommodate\ndiverse preferences without costly post-training, known as In-Context Alignment\n(ICA). However, LLMs' comprehension of input prompts remains agnostic, limiting\nICA's ability to address value tensions--human values are inherently\npluralistic, often imposing conflicting demands, e.g., stimulation vs.\ntradition. Current ICA methods therefore face the Instruction Bottleneck\nchallenge, where LLMs struggle to reconcile multiple intended values within a\nsingle prompt, leading to incomplete or biased alignment. To address this, we\npropose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO\noptimizes a meta-instruction that navigates multiple values to better elicit\nLLMs' understanding of them and improve their alignment. This is achieved by\nmaximizing the total correlation between specified values and LLM responses,\ntheoretically reinforcing value correlation while reducing distractive noise,\nresulting in effective value instructions. Extensive experiments on five value\nsets show that PICACO works well with both black-box and open-source LLMs,\noutperforms several recent strong baselines, and achieves a better balance\nacross up to 8 distinct values.\n", "link": "http://arxiv.org/abs/2507.16679v1", "date": "2025-07-22", "relevancy": 2.37, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4741}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PICACO%3A%20Pluralistic%20In-Context%20Value%20Alignment%20of%20LLMs%20via%20Total%0A%20%20Correlation%20Optimization&body=Title%3A%20PICACO%3A%20Pluralistic%20In-Context%20Value%20Alignment%20of%20LLMs%20via%20Total%0A%20%20Correlation%20Optimization%0AAuthor%3A%20Han%20Jiang%20and%20Dongyao%20Zhu%20and%20Zhihua%20Wei%20and%20Xiaoyuan%20Yi%20and%20Ziang%20Xiao%20and%20Xing%20Xie%0AAbstract%3A%20%20%20In-Context%20Learning%20has%20shown%20great%20potential%20for%20aligning%20Large%20Language%0AModels%20%28LLMs%29%20with%20human%20values%2C%20helping%20reduce%20harmful%20outputs%20and%20accommodate%0Adiverse%20preferences%20without%20costly%20post-training%2C%20known%20as%20In-Context%20Alignment%0A%28ICA%29.%20However%2C%20LLMs%27%20comprehension%20of%20input%20prompts%20remains%20agnostic%2C%20limiting%0AICA%27s%20ability%20to%20address%20value%20tensions--human%20values%20are%20inherently%0Apluralistic%2C%20often%20imposing%20conflicting%20demands%2C%20e.g.%2C%20stimulation%20vs.%0Atradition.%20Current%20ICA%20methods%20therefore%20face%20the%20Instruction%20Bottleneck%0Achallenge%2C%20where%20LLMs%20struggle%20to%20reconcile%20multiple%20intended%20values%20within%20a%0Asingle%20prompt%2C%20leading%20to%20incomplete%20or%20biased%20alignment.%20To%20address%20this%2C%20we%0Apropose%20PICACO%2C%20a%20novel%20pluralistic%20ICA%20method.%20Without%20fine-tuning%2C%20PICACO%0Aoptimizes%20a%20meta-instruction%20that%20navigates%20multiple%20values%20to%20better%20elicit%0ALLMs%27%20understanding%20of%20them%20and%20improve%20their%20alignment.%20This%20is%20achieved%20by%0Amaximizing%20the%20total%20correlation%20between%20specified%20values%20and%20LLM%20responses%2C%0Atheoretically%20reinforcing%20value%20correlation%20while%20reducing%20distractive%20noise%2C%0Aresulting%20in%20effective%20value%20instructions.%20Extensive%20experiments%20on%20five%20value%0Asets%20show%20that%20PICACO%20works%20well%20with%20both%20black-box%20and%20open-source%20LLMs%2C%0Aoutperforms%20several%20recent%20strong%20baselines%2C%20and%20achieves%20a%20better%20balance%0Aacross%20up%20to%208%20distinct%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPICACO%253A%2520Pluralistic%2520In-Context%2520Value%2520Alignment%2520of%2520LLMs%2520via%2520Total%250A%2520%2520Correlation%2520Optimization%26entry.906535625%3DHan%2520Jiang%2520and%2520Dongyao%2520Zhu%2520and%2520Zhihua%2520Wei%2520and%2520Xiaoyuan%2520Yi%2520and%2520Ziang%2520Xiao%2520and%2520Xing%2520Xie%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520has%2520shown%2520great%2520potential%2520for%2520aligning%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520with%2520human%2520values%252C%2520helping%2520reduce%2520harmful%2520outputs%2520and%2520accommodate%250Adiverse%2520preferences%2520without%2520costly%2520post-training%252C%2520known%2520as%2520In-Context%2520Alignment%250A%2528ICA%2529.%2520However%252C%2520LLMs%2527%2520comprehension%2520of%2520input%2520prompts%2520remains%2520agnostic%252C%2520limiting%250AICA%2527s%2520ability%2520to%2520address%2520value%2520tensions--human%2520values%2520are%2520inherently%250Apluralistic%252C%2520often%2520imposing%2520conflicting%2520demands%252C%2520e.g.%252C%2520stimulation%2520vs.%250Atradition.%2520Current%2520ICA%2520methods%2520therefore%2520face%2520the%2520Instruction%2520Bottleneck%250Achallenge%252C%2520where%2520LLMs%2520struggle%2520to%2520reconcile%2520multiple%2520intended%2520values%2520within%2520a%250Asingle%2520prompt%252C%2520leading%2520to%2520incomplete%2520or%2520biased%2520alignment.%2520To%2520address%2520this%252C%2520we%250Apropose%2520PICACO%252C%2520a%2520novel%2520pluralistic%2520ICA%2520method.%2520Without%2520fine-tuning%252C%2520PICACO%250Aoptimizes%2520a%2520meta-instruction%2520that%2520navigates%2520multiple%2520values%2520to%2520better%2520elicit%250ALLMs%2527%2520understanding%2520of%2520them%2520and%2520improve%2520their%2520alignment.%2520This%2520is%2520achieved%2520by%250Amaximizing%2520the%2520total%2520correlation%2520between%2520specified%2520values%2520and%2520LLM%2520responses%252C%250Atheoretically%2520reinforcing%2520value%2520correlation%2520while%2520reducing%2520distractive%2520noise%252C%250Aresulting%2520in%2520effective%2520value%2520instructions.%2520Extensive%2520experiments%2520on%2520five%2520value%250Asets%2520show%2520that%2520PICACO%2520works%2520well%2520with%2520both%2520black-box%2520and%2520open-source%2520LLMs%252C%250Aoutperforms%2520several%2520recent%2520strong%2520baselines%252C%2520and%2520achieves%2520a%2520better%2520balance%250Aacross%2520up%2520to%25208%2520distinct%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PICACO%3A%20Pluralistic%20In-Context%20Value%20Alignment%20of%20LLMs%20via%20Total%0A%20%20Correlation%20Optimization&entry.906535625=Han%20Jiang%20and%20Dongyao%20Zhu%20and%20Zhihua%20Wei%20and%20Xiaoyuan%20Yi%20and%20Ziang%20Xiao%20and%20Xing%20Xie&entry.1292438233=%20%20In-Context%20Learning%20has%20shown%20great%20potential%20for%20aligning%20Large%20Language%0AModels%20%28LLMs%29%20with%20human%20values%2C%20helping%20reduce%20harmful%20outputs%20and%20accommodate%0Adiverse%20preferences%20without%20costly%20post-training%2C%20known%20as%20In-Context%20Alignment%0A%28ICA%29.%20However%2C%20LLMs%27%20comprehension%20of%20input%20prompts%20remains%20agnostic%2C%20limiting%0AICA%27s%20ability%20to%20address%20value%20tensions--human%20values%20are%20inherently%0Apluralistic%2C%20often%20imposing%20conflicting%20demands%2C%20e.g.%2C%20stimulation%20vs.%0Atradition.%20Current%20ICA%20methods%20therefore%20face%20the%20Instruction%20Bottleneck%0Achallenge%2C%20where%20LLMs%20struggle%20to%20reconcile%20multiple%20intended%20values%20within%20a%0Asingle%20prompt%2C%20leading%20to%20incomplete%20or%20biased%20alignment.%20To%20address%20this%2C%20we%0Apropose%20PICACO%2C%20a%20novel%20pluralistic%20ICA%20method.%20Without%20fine-tuning%2C%20PICACO%0Aoptimizes%20a%20meta-instruction%20that%20navigates%20multiple%20values%20to%20better%20elicit%0ALLMs%27%20understanding%20of%20them%20and%20improve%20their%20alignment.%20This%20is%20achieved%20by%0Amaximizing%20the%20total%20correlation%20between%20specified%20values%20and%20LLM%20responses%2C%0Atheoretically%20reinforcing%20value%20correlation%20while%20reducing%20distractive%20noise%2C%0Aresulting%20in%20effective%20value%20instructions.%20Extensive%20experiments%20on%20five%20value%0Asets%20show%20that%20PICACO%20works%20well%20with%20both%20black-box%20and%20open-source%20LLMs%2C%0Aoutperforms%20several%20recent%20strong%20baselines%2C%20and%20achieves%20a%20better%20balance%0Aacross%20up%20to%208%20distinct%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16679v1&entry.124074799=Read"},
{"title": "Improving Model Classification by Optimizing the Training Dataset", "author": "Morad Tukan and Loay Mualem and Eitan Netzer and Liran Sigalat", "abstract": "  In the era of data-centric AI, the ability to curate high-quality training\ndata is as crucial as model design. Coresets offer a principled approach to\ndata reduction, enabling efficient learning on large datasets through\nimportance sampling. However, conventional sensitivity-based coreset\nconstruction often falls short in optimizing for classification performance\nmetrics, e.g., $F1$ score, focusing instead on loss approximation. In this\nwork, we present a systematic framework for tuning the coreset generation\nprocess to enhance downstream classification quality. Our method introduces new\ntunable parameters--including deterministic sampling, class-wise allocation,\nand refinement via active sampling, beyond traditional sensitivity scores.\nThrough extensive experiments on diverse datasets and classifiers, we\ndemonstrate that tuned coresets can significantly outperform both vanilla\ncoresets and full dataset training on key classification metrics, offering an\neffective path towards better and more efficient model training.\n", "link": "http://arxiv.org/abs/2507.16729v1", "date": "2025-07-22", "relevancy": 2.3688, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4767}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.476}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Model%20Classification%20by%20Optimizing%20the%20Training%20Dataset&body=Title%3A%20Improving%20Model%20Classification%20by%20Optimizing%20the%20Training%20Dataset%0AAuthor%3A%20Morad%20Tukan%20and%20Loay%20Mualem%20and%20Eitan%20Netzer%20and%20Liran%20Sigalat%0AAbstract%3A%20%20%20In%20the%20era%20of%20data-centric%20AI%2C%20the%20ability%20to%20curate%20high-quality%20training%0Adata%20is%20as%20crucial%20as%20model%20design.%20Coresets%20offer%20a%20principled%20approach%20to%0Adata%20reduction%2C%20enabling%20efficient%20learning%20on%20large%20datasets%20through%0Aimportance%20sampling.%20However%2C%20conventional%20sensitivity-based%20coreset%0Aconstruction%20often%20falls%20short%20in%20optimizing%20for%20classification%20performance%0Ametrics%2C%20e.g.%2C%20%24F1%24%20score%2C%20focusing%20instead%20on%20loss%20approximation.%20In%20this%0Awork%2C%20we%20present%20a%20systematic%20framework%20for%20tuning%20the%20coreset%20generation%0Aprocess%20to%20enhance%20downstream%20classification%20quality.%20Our%20method%20introduces%20new%0Atunable%20parameters--including%20deterministic%20sampling%2C%20class-wise%20allocation%2C%0Aand%20refinement%20via%20active%20sampling%2C%20beyond%20traditional%20sensitivity%20scores.%0AThrough%20extensive%20experiments%20on%20diverse%20datasets%20and%20classifiers%2C%20we%0Ademonstrate%20that%20tuned%20coresets%20can%20significantly%20outperform%20both%20vanilla%0Acoresets%20and%20full%20dataset%20training%20on%20key%20classification%20metrics%2C%20offering%20an%0Aeffective%20path%20towards%20better%20and%20more%20efficient%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Model%2520Classification%2520by%2520Optimizing%2520the%2520Training%2520Dataset%26entry.906535625%3DMorad%2520Tukan%2520and%2520Loay%2520Mualem%2520and%2520Eitan%2520Netzer%2520and%2520Liran%2520Sigalat%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520data-centric%2520AI%252C%2520the%2520ability%2520to%2520curate%2520high-quality%2520training%250Adata%2520is%2520as%2520crucial%2520as%2520model%2520design.%2520Coresets%2520offer%2520a%2520principled%2520approach%2520to%250Adata%2520reduction%252C%2520enabling%2520efficient%2520learning%2520on%2520large%2520datasets%2520through%250Aimportance%2520sampling.%2520However%252C%2520conventional%2520sensitivity-based%2520coreset%250Aconstruction%2520often%2520falls%2520short%2520in%2520optimizing%2520for%2520classification%2520performance%250Ametrics%252C%2520e.g.%252C%2520%2524F1%2524%2520score%252C%2520focusing%2520instead%2520on%2520loss%2520approximation.%2520In%2520this%250Awork%252C%2520we%2520present%2520a%2520systematic%2520framework%2520for%2520tuning%2520the%2520coreset%2520generation%250Aprocess%2520to%2520enhance%2520downstream%2520classification%2520quality.%2520Our%2520method%2520introduces%2520new%250Atunable%2520parameters--including%2520deterministic%2520sampling%252C%2520class-wise%2520allocation%252C%250Aand%2520refinement%2520via%2520active%2520sampling%252C%2520beyond%2520traditional%2520sensitivity%2520scores.%250AThrough%2520extensive%2520experiments%2520on%2520diverse%2520datasets%2520and%2520classifiers%252C%2520we%250Ademonstrate%2520that%2520tuned%2520coresets%2520can%2520significantly%2520outperform%2520both%2520vanilla%250Acoresets%2520and%2520full%2520dataset%2520training%2520on%2520key%2520classification%2520metrics%252C%2520offering%2520an%250Aeffective%2520path%2520towards%2520better%2520and%2520more%2520efficient%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Model%20Classification%20by%20Optimizing%20the%20Training%20Dataset&entry.906535625=Morad%20Tukan%20and%20Loay%20Mualem%20and%20Eitan%20Netzer%20and%20Liran%20Sigalat&entry.1292438233=%20%20In%20the%20era%20of%20data-centric%20AI%2C%20the%20ability%20to%20curate%20high-quality%20training%0Adata%20is%20as%20crucial%20as%20model%20design.%20Coresets%20offer%20a%20principled%20approach%20to%0Adata%20reduction%2C%20enabling%20efficient%20learning%20on%20large%20datasets%20through%0Aimportance%20sampling.%20However%2C%20conventional%20sensitivity-based%20coreset%0Aconstruction%20often%20falls%20short%20in%20optimizing%20for%20classification%20performance%0Ametrics%2C%20e.g.%2C%20%24F1%24%20score%2C%20focusing%20instead%20on%20loss%20approximation.%20In%20this%0Awork%2C%20we%20present%20a%20systematic%20framework%20for%20tuning%20the%20coreset%20generation%0Aprocess%20to%20enhance%20downstream%20classification%20quality.%20Our%20method%20introduces%20new%0Atunable%20parameters--including%20deterministic%20sampling%2C%20class-wise%20allocation%2C%0Aand%20refinement%20via%20active%20sampling%2C%20beyond%20traditional%20sensitivity%20scores.%0AThrough%20extensive%20experiments%20on%20diverse%20datasets%20and%20classifiers%2C%20we%0Ademonstrate%20that%20tuned%20coresets%20can%20significantly%20outperform%20both%20vanilla%0Acoresets%20and%20full%20dataset%20training%20on%20key%20classification%20metrics%2C%20offering%20an%0Aeffective%20path%20towards%20better%20and%20more%20efficient%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16729v1&entry.124074799=Read"},
{"title": "PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium\n  Specimens", "author": "Youcef Sklab and Florian Castanet and Hanane Ariouat and Souhila Arib and Jean-Daniel Zucker and Eric Chenin and Edi Prifti", "abstract": "  Deep learning-based classification of herbarium images is hampered by\nbackground heterogeneity, which introduces noise and artifacts that can\npotentially mislead models and reduce classification accuracy. Addressing these\nbackground-related challenges is critical to improving model performance. We\nintroduce PlantSAM, an automated segmentation pipeline that integrates YOLOv10\nfor plant region detection and the Segment Anything Model (SAM2) for\nsegmentation. YOLOv10 generates bounding box prompts to guide SAM2, enhancing\nsegmentation accuracy. Both models were fine-tuned on herbarium images and\nevaluated using Intersection over Union (IoU) and Dice coefficient metrics.\nPlantSAM achieved state-of-the-art segmentation performance, with an IoU of\n0.94 and a Dice coefficient of 0.97. Incorporating segmented images into\nclassification models led to consistent performance improvements across five\ntested botanical traits, with accuracy gains of up to 4.36% and F1-score\nimprovements of 4.15%. Our findings highlight the importance of background\nremoval in herbarium image analysis, as it significantly enhances\nclassification accuracy by allowing models to focus more effectively on the\nforeground plant structures.\n", "link": "http://arxiv.org/abs/2507.16506v1", "date": "2025-07-22", "relevancy": 2.3641, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4849}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4746}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PlantSAM%3A%20An%20Object%20Detection-Driven%20Segmentation%20Pipeline%20for%20Herbarium%0A%20%20Specimens&body=Title%3A%20PlantSAM%3A%20An%20Object%20Detection-Driven%20Segmentation%20Pipeline%20for%20Herbarium%0A%20%20Specimens%0AAuthor%3A%20Youcef%20Sklab%20and%20Florian%20Castanet%20and%20Hanane%20Ariouat%20and%20Souhila%20Arib%20and%20Jean-Daniel%20Zucker%20and%20Eric%20Chenin%20and%20Edi%20Prifti%0AAbstract%3A%20%20%20Deep%20learning-based%20classification%20of%20herbarium%20images%20is%20hampered%20by%0Abackground%20heterogeneity%2C%20which%20introduces%20noise%20and%20artifacts%20that%20can%0Apotentially%20mislead%20models%20and%20reduce%20classification%20accuracy.%20Addressing%20these%0Abackground-related%20challenges%20is%20critical%20to%20improving%20model%20performance.%20We%0Aintroduce%20PlantSAM%2C%20an%20automated%20segmentation%20pipeline%20that%20integrates%20YOLOv10%0Afor%20plant%20region%20detection%20and%20the%20Segment%20Anything%20Model%20%28SAM2%29%20for%0Asegmentation.%20YOLOv10%20generates%20bounding%20box%20prompts%20to%20guide%20SAM2%2C%20enhancing%0Asegmentation%20accuracy.%20Both%20models%20were%20fine-tuned%20on%20herbarium%20images%20and%0Aevaluated%20using%20Intersection%20over%20Union%20%28IoU%29%20and%20Dice%20coefficient%20metrics.%0APlantSAM%20achieved%20state-of-the-art%20segmentation%20performance%2C%20with%20an%20IoU%20of%0A0.94%20and%20a%20Dice%20coefficient%20of%200.97.%20Incorporating%20segmented%20images%20into%0Aclassification%20models%20led%20to%20consistent%20performance%20improvements%20across%20five%0Atested%20botanical%20traits%2C%20with%20accuracy%20gains%20of%20up%20to%204.36%25%20and%20F1-score%0Aimprovements%20of%204.15%25.%20Our%20findings%20highlight%20the%20importance%20of%20background%0Aremoval%20in%20herbarium%20image%20analysis%2C%20as%20it%20significantly%20enhances%0Aclassification%20accuracy%20by%20allowing%20models%20to%20focus%20more%20effectively%20on%20the%0Aforeground%20plant%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlantSAM%253A%2520An%2520Object%2520Detection-Driven%2520Segmentation%2520Pipeline%2520for%2520Herbarium%250A%2520%2520Specimens%26entry.906535625%3DYoucef%2520Sklab%2520and%2520Florian%2520Castanet%2520and%2520Hanane%2520Ariouat%2520and%2520Souhila%2520Arib%2520and%2520Jean-Daniel%2520Zucker%2520and%2520Eric%2520Chenin%2520and%2520Edi%2520Prifti%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520classification%2520of%2520herbarium%2520images%2520is%2520hampered%2520by%250Abackground%2520heterogeneity%252C%2520which%2520introduces%2520noise%2520and%2520artifacts%2520that%2520can%250Apotentially%2520mislead%2520models%2520and%2520reduce%2520classification%2520accuracy.%2520Addressing%2520these%250Abackground-related%2520challenges%2520is%2520critical%2520to%2520improving%2520model%2520performance.%2520We%250Aintroduce%2520PlantSAM%252C%2520an%2520automated%2520segmentation%2520pipeline%2520that%2520integrates%2520YOLOv10%250Afor%2520plant%2520region%2520detection%2520and%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM2%2529%2520for%250Asegmentation.%2520YOLOv10%2520generates%2520bounding%2520box%2520prompts%2520to%2520guide%2520SAM2%252C%2520enhancing%250Asegmentation%2520accuracy.%2520Both%2520models%2520were%2520fine-tuned%2520on%2520herbarium%2520images%2520and%250Aevaluated%2520using%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520and%2520Dice%2520coefficient%2520metrics.%250APlantSAM%2520achieved%2520state-of-the-art%2520segmentation%2520performance%252C%2520with%2520an%2520IoU%2520of%250A0.94%2520and%2520a%2520Dice%2520coefficient%2520of%25200.97.%2520Incorporating%2520segmented%2520images%2520into%250Aclassification%2520models%2520led%2520to%2520consistent%2520performance%2520improvements%2520across%2520five%250Atested%2520botanical%2520traits%252C%2520with%2520accuracy%2520gains%2520of%2520up%2520to%25204.36%2525%2520and%2520F1-score%250Aimprovements%2520of%25204.15%2525.%2520Our%2520findings%2520highlight%2520the%2520importance%2520of%2520background%250Aremoval%2520in%2520herbarium%2520image%2520analysis%252C%2520as%2520it%2520significantly%2520enhances%250Aclassification%2520accuracy%2520by%2520allowing%2520models%2520to%2520focus%2520more%2520effectively%2520on%2520the%250Aforeground%2520plant%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlantSAM%3A%20An%20Object%20Detection-Driven%20Segmentation%20Pipeline%20for%20Herbarium%0A%20%20Specimens&entry.906535625=Youcef%20Sklab%20and%20Florian%20Castanet%20and%20Hanane%20Ariouat%20and%20Souhila%20Arib%20and%20Jean-Daniel%20Zucker%20and%20Eric%20Chenin%20and%20Edi%20Prifti&entry.1292438233=%20%20Deep%20learning-based%20classification%20of%20herbarium%20images%20is%20hampered%20by%0Abackground%20heterogeneity%2C%20which%20introduces%20noise%20and%20artifacts%20that%20can%0Apotentially%20mislead%20models%20and%20reduce%20classification%20accuracy.%20Addressing%20these%0Abackground-related%20challenges%20is%20critical%20to%20improving%20model%20performance.%20We%0Aintroduce%20PlantSAM%2C%20an%20automated%20segmentation%20pipeline%20that%20integrates%20YOLOv10%0Afor%20plant%20region%20detection%20and%20the%20Segment%20Anything%20Model%20%28SAM2%29%20for%0Asegmentation.%20YOLOv10%20generates%20bounding%20box%20prompts%20to%20guide%20SAM2%2C%20enhancing%0Asegmentation%20accuracy.%20Both%20models%20were%20fine-tuned%20on%20herbarium%20images%20and%0Aevaluated%20using%20Intersection%20over%20Union%20%28IoU%29%20and%20Dice%20coefficient%20metrics.%0APlantSAM%20achieved%20state-of-the-art%20segmentation%20performance%2C%20with%20an%20IoU%20of%0A0.94%20and%20a%20Dice%20coefficient%20of%200.97.%20Incorporating%20segmented%20images%20into%0Aclassification%20models%20led%20to%20consistent%20performance%20improvements%20across%20five%0Atested%20botanical%20traits%2C%20with%20accuracy%20gains%20of%20up%20to%204.36%25%20and%20F1-score%0Aimprovements%20of%204.15%25.%20Our%20findings%20highlight%20the%20importance%20of%20background%0Aremoval%20in%20herbarium%20image%20analysis%2C%20as%20it%20significantly%20enhances%0Aclassification%20accuracy%20by%20allowing%20models%20to%20focus%20more%20effectively%20on%20the%0Aforeground%20plant%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16506v1&entry.124074799=Read"},
{"title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding", "author": "Ran Wang and Xiaoxuan Liu and Hao Ren and Gang Chen and Fanchao Qi and Maosong Sun", "abstract": "  Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.\n", "link": "http://arxiv.org/abs/2507.16768v1", "date": "2025-07-22", "relevancy": 2.3628, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WGRAMMAR%3A%20Leverage%20Prior%20Knowledge%20to%20Accelerate%20Structured%20Decoding&body=Title%3A%20WGRAMMAR%3A%20Leverage%20Prior%20Knowledge%20to%20Accelerate%20Structured%20Decoding%0AAuthor%3A%20Ran%20Wang%20and%20Xiaoxuan%20Liu%20and%20Hao%20Ren%20and%20Gang%20Chen%20and%20Fanchao%20Qi%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Structured%20decoding%20enables%20large%20language%20models%20%28LLMs%29%20to%20generate%20outputs%0Ain%20formats%20required%20by%20downstream%20systems%2C%20such%20as%20HTML%20or%20JSON.%20However%2C%0Aexisting%20methods%20suffer%20from%20efficiency%20bottlenecks%20due%20to%20grammar%20compilation%2C%0Astate%20tracking%2C%20and%20mask%20creation.%20We%20observe%20that%20many%20real-world%20tasks%20embed%0Astrong%20prior%20knowledge%20about%20output%20structure.%20Leveraging%20this%2C%20we%20propose%20a%0Adecomposition%20of%20constraints%20into%20static%20and%20dynamic%20components%20--%20precompiling%0Astatic%20structures%20offline%20and%20instantiating%20dynamic%20arguments%20at%20runtime%20using%0Agrammar%20snippets.%20Instead%20of%20relying%20on%20pushdown%20automata%2C%20we%20employ%20a%0Acompositional%20set%20of%20operators%20to%20model%20regular%20formats%2C%20achieving%20lower%0Atransition%20latency.%20We%20introduce%20wgrammar%2C%20a%20lightweight%20decoding%20engine%20that%0Aintegrates%20domain-aware%20simplification%2C%20constraint%20decomposition%2C%20and%20mask%0Acaching%2C%20achieving%20up%20to%20250x%20speedup%20over%20existing%20systems.%20wgrammar%27s%20source%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/wrran/wgrammar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWGRAMMAR%253A%2520Leverage%2520Prior%2520Knowledge%2520to%2520Accelerate%2520Structured%2520Decoding%26entry.906535625%3DRan%2520Wang%2520and%2520Xiaoxuan%2520Liu%2520and%2520Hao%2520Ren%2520and%2520Gang%2520Chen%2520and%2520Fanchao%2520Qi%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Structured%2520decoding%2520enables%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520generate%2520outputs%250Ain%2520formats%2520required%2520by%2520downstream%2520systems%252C%2520such%2520as%2520HTML%2520or%2520JSON.%2520However%252C%250Aexisting%2520methods%2520suffer%2520from%2520efficiency%2520bottlenecks%2520due%2520to%2520grammar%2520compilation%252C%250Astate%2520tracking%252C%2520and%2520mask%2520creation.%2520We%2520observe%2520that%2520many%2520real-world%2520tasks%2520embed%250Astrong%2520prior%2520knowledge%2520about%2520output%2520structure.%2520Leveraging%2520this%252C%2520we%2520propose%2520a%250Adecomposition%2520of%2520constraints%2520into%2520static%2520and%2520dynamic%2520components%2520--%2520precompiling%250Astatic%2520structures%2520offline%2520and%2520instantiating%2520dynamic%2520arguments%2520at%2520runtime%2520using%250Agrammar%2520snippets.%2520Instead%2520of%2520relying%2520on%2520pushdown%2520automata%252C%2520we%2520employ%2520a%250Acompositional%2520set%2520of%2520operators%2520to%2520model%2520regular%2520formats%252C%2520achieving%2520lower%250Atransition%2520latency.%2520We%2520introduce%2520wgrammar%252C%2520a%2520lightweight%2520decoding%2520engine%2520that%250Aintegrates%2520domain-aware%2520simplification%252C%2520constraint%2520decomposition%252C%2520and%2520mask%250Acaching%252C%2520achieving%2520up%2520to%2520250x%2520speedup%2520over%2520existing%2520systems.%2520wgrammar%2527s%2520source%250Acode%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/wrran/wgrammar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WGRAMMAR%3A%20Leverage%20Prior%20Knowledge%20to%20Accelerate%20Structured%20Decoding&entry.906535625=Ran%20Wang%20and%20Xiaoxuan%20Liu%20and%20Hao%20Ren%20and%20Gang%20Chen%20and%20Fanchao%20Qi%20and%20Maosong%20Sun&entry.1292438233=%20%20Structured%20decoding%20enables%20large%20language%20models%20%28LLMs%29%20to%20generate%20outputs%0Ain%20formats%20required%20by%20downstream%20systems%2C%20such%20as%20HTML%20or%20JSON.%20However%2C%0Aexisting%20methods%20suffer%20from%20efficiency%20bottlenecks%20due%20to%20grammar%20compilation%2C%0Astate%20tracking%2C%20and%20mask%20creation.%20We%20observe%20that%20many%20real-world%20tasks%20embed%0Astrong%20prior%20knowledge%20about%20output%20structure.%20Leveraging%20this%2C%20we%20propose%20a%0Adecomposition%20of%20constraints%20into%20static%20and%20dynamic%20components%20--%20precompiling%0Astatic%20structures%20offline%20and%20instantiating%20dynamic%20arguments%20at%20runtime%20using%0Agrammar%20snippets.%20Instead%20of%20relying%20on%20pushdown%20automata%2C%20we%20employ%20a%0Acompositional%20set%20of%20operators%20to%20model%20regular%20formats%2C%20achieving%20lower%0Atransition%20latency.%20We%20introduce%20wgrammar%2C%20a%20lightweight%20decoding%20engine%20that%0Aintegrates%20domain-aware%20simplification%2C%20constraint%20decomposition%2C%20and%20mask%0Acaching%2C%20achieving%20up%20to%20250x%20speedup%20over%20existing%20systems.%20wgrammar%27s%20source%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/wrran/wgrammar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16768v1&entry.124074799=Read"},
{"title": "VICI: VLM-Instructed Cross-view Image-localisation", "author": "Xiaohan Zhang and Tavis Shore and Chen Chen and Oscar Mendez and Simon Hadfield and Safwan Wshah", "abstract": "  In this paper, we present a high-performing solution to the UAVM 2025\nChallenge, which focuses on matching narrow FOV street-level images to\ncorresponding satellite imagery using the University-1652 dataset. As panoramic\nCross-View Geo-Localisation nears peak performance, it becomes increasingly\nimportant to explore more practical problem formulations. Real-world scenarios\nrarely offer panoramic street-level queries; instead, queries typically consist\nof limited-FOV images captured with unknown camera parameters. Our work\nprioritises discovering the highest achievable performance under these\nconstraints, pushing the limits of existing architectures. Our method begins by\nretrieving candidate satellite image embeddings for a given query, followed by\na re-ranking stage that selectively enhances retrieval accuracy within the top\ncandidates. This two-stage approach enables more precise matching, even under\nthe significant viewpoint and scale variations inherent in the task. Through\nexperimentation, we demonstrate that our approach achieves competitive results\n-specifically attaining R@1 and R@10 retrieval rates of \\topone\\% and \\topten\\%\nrespectively. This underscores the potential of optimised retrieval and\nre-ranking strategies in advancing practical geo-localisation performance. Code\nis available at https://github.com/tavisshore/VICI.\n", "link": "http://arxiv.org/abs/2507.04107v2", "date": "2025-07-22", "relevancy": 2.3617, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6235}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5677}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VICI%3A%20VLM-Instructed%20Cross-view%20Image-localisation&body=Title%3A%20VICI%3A%20VLM-Instructed%20Cross-view%20Image-localisation%0AAuthor%3A%20Xiaohan%20Zhang%20and%20Tavis%20Shore%20and%20Chen%20Chen%20and%20Oscar%20Mendez%20and%20Simon%20Hadfield%20and%20Safwan%20Wshah%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20high-performing%20solution%20to%20the%20UAVM%202025%0AChallenge%2C%20which%20focuses%20on%20matching%20narrow%20FOV%20street-level%20images%20to%0Acorresponding%20satellite%20imagery%20using%20the%20University-1652%20dataset.%20As%20panoramic%0ACross-View%20Geo-Localisation%20nears%20peak%20performance%2C%20it%20becomes%20increasingly%0Aimportant%20to%20explore%20more%20practical%20problem%20formulations.%20Real-world%20scenarios%0Ararely%20offer%20panoramic%20street-level%20queries%3B%20instead%2C%20queries%20typically%20consist%0Aof%20limited-FOV%20images%20captured%20with%20unknown%20camera%20parameters.%20Our%20work%0Aprioritises%20discovering%20the%20highest%20achievable%20performance%20under%20these%0Aconstraints%2C%20pushing%20the%20limits%20of%20existing%20architectures.%20Our%20method%20begins%20by%0Aretrieving%20candidate%20satellite%20image%20embeddings%20for%20a%20given%20query%2C%20followed%20by%0Aa%20re-ranking%20stage%20that%20selectively%20enhances%20retrieval%20accuracy%20within%20the%20top%0Acandidates.%20This%20two-stage%20approach%20enables%20more%20precise%20matching%2C%20even%20under%0Athe%20significant%20viewpoint%20and%20scale%20variations%20inherent%20in%20the%20task.%20Through%0Aexperimentation%2C%20we%20demonstrate%20that%20our%20approach%20achieves%20competitive%20results%0A-specifically%20attaining%20R%401%20and%20R%4010%20retrieval%20rates%20of%20%5Ctopone%5C%25%20and%20%5Ctopten%5C%25%0Arespectively.%20This%20underscores%20the%20potential%20of%20optimised%20retrieval%20and%0Are-ranking%20strategies%20in%20advancing%20practical%20geo-localisation%20performance.%20Code%0Ais%20available%20at%20https%3A//github.com/tavisshore/VICI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04107v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVICI%253A%2520VLM-Instructed%2520Cross-view%2520Image-localisation%26entry.906535625%3DXiaohan%2520Zhang%2520and%2520Tavis%2520Shore%2520and%2520Chen%2520Chen%2520and%2520Oscar%2520Mendez%2520and%2520Simon%2520Hadfield%2520and%2520Safwan%2520Wshah%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520high-performing%2520solution%2520to%2520the%2520UAVM%25202025%250AChallenge%252C%2520which%2520focuses%2520on%2520matching%2520narrow%2520FOV%2520street-level%2520images%2520to%250Acorresponding%2520satellite%2520imagery%2520using%2520the%2520University-1652%2520dataset.%2520As%2520panoramic%250ACross-View%2520Geo-Localisation%2520nears%2520peak%2520performance%252C%2520it%2520becomes%2520increasingly%250Aimportant%2520to%2520explore%2520more%2520practical%2520problem%2520formulations.%2520Real-world%2520scenarios%250Ararely%2520offer%2520panoramic%2520street-level%2520queries%253B%2520instead%252C%2520queries%2520typically%2520consist%250Aof%2520limited-FOV%2520images%2520captured%2520with%2520unknown%2520camera%2520parameters.%2520Our%2520work%250Aprioritises%2520discovering%2520the%2520highest%2520achievable%2520performance%2520under%2520these%250Aconstraints%252C%2520pushing%2520the%2520limits%2520of%2520existing%2520architectures.%2520Our%2520method%2520begins%2520by%250Aretrieving%2520candidate%2520satellite%2520image%2520embeddings%2520for%2520a%2520given%2520query%252C%2520followed%2520by%250Aa%2520re-ranking%2520stage%2520that%2520selectively%2520enhances%2520retrieval%2520accuracy%2520within%2520the%2520top%250Acandidates.%2520This%2520two-stage%2520approach%2520enables%2520more%2520precise%2520matching%252C%2520even%2520under%250Athe%2520significant%2520viewpoint%2520and%2520scale%2520variations%2520inherent%2520in%2520the%2520task.%2520Through%250Aexperimentation%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520competitive%2520results%250A-specifically%2520attaining%2520R%25401%2520and%2520R%254010%2520retrieval%2520rates%2520of%2520%255Ctopone%255C%2525%2520and%2520%255Ctopten%255C%2525%250Arespectively.%2520This%2520underscores%2520the%2520potential%2520of%2520optimised%2520retrieval%2520and%250Are-ranking%2520strategies%2520in%2520advancing%2520practical%2520geo-localisation%2520performance.%2520Code%250Ais%2520available%2520at%2520https%253A//github.com/tavisshore/VICI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04107v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VICI%3A%20VLM-Instructed%20Cross-view%20Image-localisation&entry.906535625=Xiaohan%20Zhang%20and%20Tavis%20Shore%20and%20Chen%20Chen%20and%20Oscar%20Mendez%20and%20Simon%20Hadfield%20and%20Safwan%20Wshah&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20high-performing%20solution%20to%20the%20UAVM%202025%0AChallenge%2C%20which%20focuses%20on%20matching%20narrow%20FOV%20street-level%20images%20to%0Acorresponding%20satellite%20imagery%20using%20the%20University-1652%20dataset.%20As%20panoramic%0ACross-View%20Geo-Localisation%20nears%20peak%20performance%2C%20it%20becomes%20increasingly%0Aimportant%20to%20explore%20more%20practical%20problem%20formulations.%20Real-world%20scenarios%0Ararely%20offer%20panoramic%20street-level%20queries%3B%20instead%2C%20queries%20typically%20consist%0Aof%20limited-FOV%20images%20captured%20with%20unknown%20camera%20parameters.%20Our%20work%0Aprioritises%20discovering%20the%20highest%20achievable%20performance%20under%20these%0Aconstraints%2C%20pushing%20the%20limits%20of%20existing%20architectures.%20Our%20method%20begins%20by%0Aretrieving%20candidate%20satellite%20image%20embeddings%20for%20a%20given%20query%2C%20followed%20by%0Aa%20re-ranking%20stage%20that%20selectively%20enhances%20retrieval%20accuracy%20within%20the%20top%0Acandidates.%20This%20two-stage%20approach%20enables%20more%20precise%20matching%2C%20even%20under%0Athe%20significant%20viewpoint%20and%20scale%20variations%20inherent%20in%20the%20task.%20Through%0Aexperimentation%2C%20we%20demonstrate%20that%20our%20approach%20achieves%20competitive%20results%0A-specifically%20attaining%20R%401%20and%20R%4010%20retrieval%20rates%20of%20%5Ctopone%5C%25%20and%20%5Ctopten%5C%25%0Arespectively.%20This%20underscores%20the%20potential%20of%20optimised%20retrieval%20and%0Are-ranking%20strategies%20in%20advancing%20practical%20geo-localisation%20performance.%20Code%0Ais%20available%20at%20https%3A//github.com/tavisshore/VICI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04107v2&entry.124074799=Read"},
{"title": "A Comprehensive Data-centric Overview of Federated Graph Learning", "author": "Zhengyu Wu and Xunkai Li and Yinlin Zhu and Zekai Chen and Guochen Yan and Yanyu Yan and Hao Zhang and Yuming Ai and Xinmo Jin and Rong-Hua Li and Guoren Wang", "abstract": "  In the era of big data applications, Federated Graph Learning (FGL) has\nemerged as a prominent solution that reconcile the tradeoff between optimizing\nthe collective intelligence between decentralized datasets holders and\npreserving sensitive information to maximum. Existing FGL surveys have\ncontributed meaningfully but largely focus on integrating Federated Learning\n(FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that\nemphasis on methodology and simulated scenarios. Notably, a data centric\nperspective, which systematically examines FGL methods through the lens of data\nproperties and usage, remains unadapted to reorganize FGL research, yet it is\ncritical to assess how FGL studies manage to tackle data centric constraints to\nenhance model performances. This survey propose a two-level data centric\ntaxonomy: Data Characteristics, which categorizes studies based on the\nstructural and distributional properties of datasets used in FGL, and Data\nUtilization, which analyzes the training procedures and techniques employed to\novercome key data centric challenges. Each taxonomy level is defined by three\northogonal criteria, each representing a distinct data centric configuration.\nBeyond taxonomy, this survey examines FGL integration with Pretrained Large\nModels, showcases realistic applications, and highlights future direction\naligned with emerging trends in GML.\n", "link": "http://arxiv.org/abs/2507.16541v1", "date": "2025-07-22", "relevancy": 2.3545, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4699}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Data-centric%20Overview%20of%20Federated%20Graph%20Learning&body=Title%3A%20A%20Comprehensive%20Data-centric%20Overview%20of%20Federated%20Graph%20Learning%0AAuthor%3A%20Zhengyu%20Wu%20and%20Xunkai%20Li%20and%20Yinlin%20Zhu%20and%20Zekai%20Chen%20and%20Guochen%20Yan%20and%20Yanyu%20Yan%20and%20Hao%20Zhang%20and%20Yuming%20Ai%20and%20Xinmo%20Jin%20and%20Rong-Hua%20Li%20and%20Guoren%20Wang%0AAbstract%3A%20%20%20In%20the%20era%20of%20big%20data%20applications%2C%20Federated%20Graph%20Learning%20%28FGL%29%20has%0Aemerged%20as%20a%20prominent%20solution%20that%20reconcile%20the%20tradeoff%20between%20optimizing%0Athe%20collective%20intelligence%20between%20decentralized%20datasets%20holders%20and%0Apreserving%20sensitive%20information%20to%20maximum.%20Existing%20FGL%20surveys%20have%0Acontributed%20meaningfully%20but%20largely%20focus%20on%20integrating%20Federated%20Learning%0A%28FL%29%20and%20Graph%20Machine%20Learning%20%28GML%29%2C%20resulting%20in%20early%20stage%20taxonomies%20that%0Aemphasis%20on%20methodology%20and%20simulated%20scenarios.%20Notably%2C%20a%20data%20centric%0Aperspective%2C%20which%20systematically%20examines%20FGL%20methods%20through%20the%20lens%20of%20data%0Aproperties%20and%20usage%2C%20remains%20unadapted%20to%20reorganize%20FGL%20research%2C%20yet%20it%20is%0Acritical%20to%20assess%20how%20FGL%20studies%20manage%20to%20tackle%20data%20centric%20constraints%20to%0Aenhance%20model%20performances.%20This%20survey%20propose%20a%20two-level%20data%20centric%0Ataxonomy%3A%20Data%20Characteristics%2C%20which%20categorizes%20studies%20based%20on%20the%0Astructural%20and%20distributional%20properties%20of%20datasets%20used%20in%20FGL%2C%20and%20Data%0AUtilization%2C%20which%20analyzes%20the%20training%20procedures%20and%20techniques%20employed%20to%0Aovercome%20key%20data%20centric%20challenges.%20Each%20taxonomy%20level%20is%20defined%20by%20three%0Aorthogonal%20criteria%2C%20each%20representing%20a%20distinct%20data%20centric%20configuration.%0ABeyond%20taxonomy%2C%20this%20survey%20examines%20FGL%20integration%20with%20Pretrained%20Large%0AModels%2C%20showcases%20realistic%20applications%2C%20and%20highlights%20future%20direction%0Aaligned%20with%20emerging%20trends%20in%20GML.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Data-centric%2520Overview%2520of%2520Federated%2520Graph%2520Learning%26entry.906535625%3DZhengyu%2520Wu%2520and%2520Xunkai%2520Li%2520and%2520Yinlin%2520Zhu%2520and%2520Zekai%2520Chen%2520and%2520Guochen%2520Yan%2520and%2520Yanyu%2520Yan%2520and%2520Hao%2520Zhang%2520and%2520Yuming%2520Ai%2520and%2520Xinmo%2520Jin%2520and%2520Rong-Hua%2520Li%2520and%2520Guoren%2520Wang%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520big%2520data%2520applications%252C%2520Federated%2520Graph%2520Learning%2520%2528FGL%2529%2520has%250Aemerged%2520as%2520a%2520prominent%2520solution%2520that%2520reconcile%2520the%2520tradeoff%2520between%2520optimizing%250Athe%2520collective%2520intelligence%2520between%2520decentralized%2520datasets%2520holders%2520and%250Apreserving%2520sensitive%2520information%2520to%2520maximum.%2520Existing%2520FGL%2520surveys%2520have%250Acontributed%2520meaningfully%2520but%2520largely%2520focus%2520on%2520integrating%2520Federated%2520Learning%250A%2528FL%2529%2520and%2520Graph%2520Machine%2520Learning%2520%2528GML%2529%252C%2520resulting%2520in%2520early%2520stage%2520taxonomies%2520that%250Aemphasis%2520on%2520methodology%2520and%2520simulated%2520scenarios.%2520Notably%252C%2520a%2520data%2520centric%250Aperspective%252C%2520which%2520systematically%2520examines%2520FGL%2520methods%2520through%2520the%2520lens%2520of%2520data%250Aproperties%2520and%2520usage%252C%2520remains%2520unadapted%2520to%2520reorganize%2520FGL%2520research%252C%2520yet%2520it%2520is%250Acritical%2520to%2520assess%2520how%2520FGL%2520studies%2520manage%2520to%2520tackle%2520data%2520centric%2520constraints%2520to%250Aenhance%2520model%2520performances.%2520This%2520survey%2520propose%2520a%2520two-level%2520data%2520centric%250Ataxonomy%253A%2520Data%2520Characteristics%252C%2520which%2520categorizes%2520studies%2520based%2520on%2520the%250Astructural%2520and%2520distributional%2520properties%2520of%2520datasets%2520used%2520in%2520FGL%252C%2520and%2520Data%250AUtilization%252C%2520which%2520analyzes%2520the%2520training%2520procedures%2520and%2520techniques%2520employed%2520to%250Aovercome%2520key%2520data%2520centric%2520challenges.%2520Each%2520taxonomy%2520level%2520is%2520defined%2520by%2520three%250Aorthogonal%2520criteria%252C%2520each%2520representing%2520a%2520distinct%2520data%2520centric%2520configuration.%250ABeyond%2520taxonomy%252C%2520this%2520survey%2520examines%2520FGL%2520integration%2520with%2520Pretrained%2520Large%250AModels%252C%2520showcases%2520realistic%2520applications%252C%2520and%2520highlights%2520future%2520direction%250Aaligned%2520with%2520emerging%2520trends%2520in%2520GML.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Data-centric%20Overview%20of%20Federated%20Graph%20Learning&entry.906535625=Zhengyu%20Wu%20and%20Xunkai%20Li%20and%20Yinlin%20Zhu%20and%20Zekai%20Chen%20and%20Guochen%20Yan%20and%20Yanyu%20Yan%20and%20Hao%20Zhang%20and%20Yuming%20Ai%20and%20Xinmo%20Jin%20and%20Rong-Hua%20Li%20and%20Guoren%20Wang&entry.1292438233=%20%20In%20the%20era%20of%20big%20data%20applications%2C%20Federated%20Graph%20Learning%20%28FGL%29%20has%0Aemerged%20as%20a%20prominent%20solution%20that%20reconcile%20the%20tradeoff%20between%20optimizing%0Athe%20collective%20intelligence%20between%20decentralized%20datasets%20holders%20and%0Apreserving%20sensitive%20information%20to%20maximum.%20Existing%20FGL%20surveys%20have%0Acontributed%20meaningfully%20but%20largely%20focus%20on%20integrating%20Federated%20Learning%0A%28FL%29%20and%20Graph%20Machine%20Learning%20%28GML%29%2C%20resulting%20in%20early%20stage%20taxonomies%20that%0Aemphasis%20on%20methodology%20and%20simulated%20scenarios.%20Notably%2C%20a%20data%20centric%0Aperspective%2C%20which%20systematically%20examines%20FGL%20methods%20through%20the%20lens%20of%20data%0Aproperties%20and%20usage%2C%20remains%20unadapted%20to%20reorganize%20FGL%20research%2C%20yet%20it%20is%0Acritical%20to%20assess%20how%20FGL%20studies%20manage%20to%20tackle%20data%20centric%20constraints%20to%0Aenhance%20model%20performances.%20This%20survey%20propose%20a%20two-level%20data%20centric%0Ataxonomy%3A%20Data%20Characteristics%2C%20which%20categorizes%20studies%20based%20on%20the%0Astructural%20and%20distributional%20properties%20of%20datasets%20used%20in%20FGL%2C%20and%20Data%0AUtilization%2C%20which%20analyzes%20the%20training%20procedures%20and%20techniques%20employed%20to%0Aovercome%20key%20data%20centric%20challenges.%20Each%20taxonomy%20level%20is%20defined%20by%20three%0Aorthogonal%20criteria%2C%20each%20representing%20a%20distinct%20data%20centric%20configuration.%0ABeyond%20taxonomy%2C%20this%20survey%20examines%20FGL%20integration%20with%20Pretrained%20Large%0AModels%2C%20showcases%20realistic%20applications%2C%20and%20highlights%20future%20direction%0Aaligned%20with%20emerging%20trends%20in%20GML.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16541v1&entry.124074799=Read"},
{"title": "Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation", "author": "Shukang Yin and Chaoyou Fu and Sirui Zhao and Chunjiang Ge and Yan Yang and Yuhan Dai and Yongdong Luo and Tong Xu and Caifeng Shan and Enhong Chen", "abstract": "  Recent years have seen the success of Multimodal Large Language Models\n(MLLMs) in the domain of vision understanding. The success of these models can\nlargely be attributed to the dominant scaling law, which states that larger\nparameter sizes and data volumes contribute to better performance. Notably,\ndata scaling has been primarily driven by automatic data pipelines, which focus\non the self-instruction of LLMs. The paradigm has been taken for granted for\nquite some time, but the study of the effectiveness of scaling with these data\nhas been neglected for a long time. In this context, this work revisits scaling\nwith synthetic data and focuses on developing video-LLMs from a data-centric\nperspective. Our primary study approach involves fine-tuning pre-trained\nimage-LLMs with video data and examining learning efficiency through data\nscaling. Results from our preliminary experiments reveal a low learning\nefficiency phenomenon when simply scaling up video data samples, which, through\nour probing, can be ascribed to a lack of instruction diversity. Aiming at this\nissue, we propose a data augmentation method called Sparrow, which synthesizes\nvideo-like samples from pure text instruction data. Mixing these synthetic\nsamples with the video data enables a more efficient training scheme. Through\ncomprehensive experiments, we demonstrate that our proposed method achieves\nperformance comparable to or even superior to that of baselines trained with\nsignificantly more samples. Meanwhile, we find that incorporating these\nsynthetic samples can enhance the performance of long video understanding\nwithout requiring training on long video data. The code and data examples are\navailable at https://github.com/VITA-MLLM/Sparrow.\n", "link": "http://arxiv.org/abs/2411.19951v5", "date": "2025-07-22", "relevancy": 2.3358, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5899}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparrow%3A%20Data-Efficient%20Video-LLM%20with%20Text-to-Image%20Augmentation&body=Title%3A%20Sparrow%3A%20Data-Efficient%20Video-LLM%20with%20Text-to-Image%20Augmentation%0AAuthor%3A%20Shukang%20Yin%20and%20Chaoyou%20Fu%20and%20Sirui%20Zhao%20and%20Chunjiang%20Ge%20and%20Yan%20Yang%20and%20Yuhan%20Dai%20and%20Yongdong%20Luo%20and%20Tong%20Xu%20and%20Caifeng%20Shan%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20the%20success%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20in%20the%20domain%20of%20vision%20understanding.%20The%20success%20of%20these%20models%20can%0Alargely%20be%20attributed%20to%20the%20dominant%20scaling%20law%2C%20which%20states%20that%20larger%0Aparameter%20sizes%20and%20data%20volumes%20contribute%20to%20better%20performance.%20Notably%2C%0Adata%20scaling%20has%20been%20primarily%20driven%20by%20automatic%20data%20pipelines%2C%20which%20focus%0Aon%20the%20self-instruction%20of%20LLMs.%20The%20paradigm%20has%20been%20taken%20for%20granted%20for%0Aquite%20some%20time%2C%20but%20the%20study%20of%20the%20effectiveness%20of%20scaling%20with%20these%20data%0Ahas%20been%20neglected%20for%20a%20long%20time.%20In%20this%20context%2C%20this%20work%20revisits%20scaling%0Awith%20synthetic%20data%20and%20focuses%20on%20developing%20video-LLMs%20from%20a%20data-centric%0Aperspective.%20Our%20primary%20study%20approach%20involves%20fine-tuning%20pre-trained%0Aimage-LLMs%20with%20video%20data%20and%20examining%20learning%20efficiency%20through%20data%0Ascaling.%20Results%20from%20our%20preliminary%20experiments%20reveal%20a%20low%20learning%0Aefficiency%20phenomenon%20when%20simply%20scaling%20up%20video%20data%20samples%2C%20which%2C%20through%0Aour%20probing%2C%20can%20be%20ascribed%20to%20a%20lack%20of%20instruction%20diversity.%20Aiming%20at%20this%0Aissue%2C%20we%20propose%20a%20data%20augmentation%20method%20called%20Sparrow%2C%20which%20synthesizes%0Avideo-like%20samples%20from%20pure%20text%20instruction%20data.%20Mixing%20these%20synthetic%0Asamples%20with%20the%20video%20data%20enables%20a%20more%20efficient%20training%20scheme.%20Through%0Acomprehensive%20experiments%2C%20we%20demonstrate%20that%20our%20proposed%20method%20achieves%0Aperformance%20comparable%20to%20or%20even%20superior%20to%20that%20of%20baselines%20trained%20with%0Asignificantly%20more%20samples.%20Meanwhile%2C%20we%20find%20that%20incorporating%20these%0Asynthetic%20samples%20can%20enhance%20the%20performance%20of%20long%20video%20understanding%0Awithout%20requiring%20training%20on%20long%20video%20data.%20The%20code%20and%20data%20examples%20are%0Aavailable%20at%20https%3A//github.com/VITA-MLLM/Sparrow.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19951v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparrow%253A%2520Data-Efficient%2520Video-LLM%2520with%2520Text-to-Image%2520Augmentation%26entry.906535625%3DShukang%2520Yin%2520and%2520Chaoyou%2520Fu%2520and%2520Sirui%2520Zhao%2520and%2520Chunjiang%2520Ge%2520and%2520Yan%2520Yang%2520and%2520Yuhan%2520Dai%2520and%2520Yongdong%2520Luo%2520and%2520Tong%2520Xu%2520and%2520Caifeng%2520Shan%2520and%2520Enhong%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520the%2520success%2520of%2520Multimodal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520in%2520the%2520domain%2520of%2520vision%2520understanding.%2520The%2520success%2520of%2520these%2520models%2520can%250Alargely%2520be%2520attributed%2520to%2520the%2520dominant%2520scaling%2520law%252C%2520which%2520states%2520that%2520larger%250Aparameter%2520sizes%2520and%2520data%2520volumes%2520contribute%2520to%2520better%2520performance.%2520Notably%252C%250Adata%2520scaling%2520has%2520been%2520primarily%2520driven%2520by%2520automatic%2520data%2520pipelines%252C%2520which%2520focus%250Aon%2520the%2520self-instruction%2520of%2520LLMs.%2520The%2520paradigm%2520has%2520been%2520taken%2520for%2520granted%2520for%250Aquite%2520some%2520time%252C%2520but%2520the%2520study%2520of%2520the%2520effectiveness%2520of%2520scaling%2520with%2520these%2520data%250Ahas%2520been%2520neglected%2520for%2520a%2520long%2520time.%2520In%2520this%2520context%252C%2520this%2520work%2520revisits%2520scaling%250Awith%2520synthetic%2520data%2520and%2520focuses%2520on%2520developing%2520video-LLMs%2520from%2520a%2520data-centric%250Aperspective.%2520Our%2520primary%2520study%2520approach%2520involves%2520fine-tuning%2520pre-trained%250Aimage-LLMs%2520with%2520video%2520data%2520and%2520examining%2520learning%2520efficiency%2520through%2520data%250Ascaling.%2520Results%2520from%2520our%2520preliminary%2520experiments%2520reveal%2520a%2520low%2520learning%250Aefficiency%2520phenomenon%2520when%2520simply%2520scaling%2520up%2520video%2520data%2520samples%252C%2520which%252C%2520through%250Aour%2520probing%252C%2520can%2520be%2520ascribed%2520to%2520a%2520lack%2520of%2520instruction%2520diversity.%2520Aiming%2520at%2520this%250Aissue%252C%2520we%2520propose%2520a%2520data%2520augmentation%2520method%2520called%2520Sparrow%252C%2520which%2520synthesizes%250Avideo-like%2520samples%2520from%2520pure%2520text%2520instruction%2520data.%2520Mixing%2520these%2520synthetic%250Asamples%2520with%2520the%2520video%2520data%2520enables%2520a%2520more%2520efficient%2520training%2520scheme.%2520Through%250Acomprehensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520proposed%2520method%2520achieves%250Aperformance%2520comparable%2520to%2520or%2520even%2520superior%2520to%2520that%2520of%2520baselines%2520trained%2520with%250Asignificantly%2520more%2520samples.%2520Meanwhile%252C%2520we%2520find%2520that%2520incorporating%2520these%250Asynthetic%2520samples%2520can%2520enhance%2520the%2520performance%2520of%2520long%2520video%2520understanding%250Awithout%2520requiring%2520training%2520on%2520long%2520video%2520data.%2520The%2520code%2520and%2520data%2520examples%2520are%250Aavailable%2520at%2520https%253A//github.com/VITA-MLLM/Sparrow.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19951v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparrow%3A%20Data-Efficient%20Video-LLM%20with%20Text-to-Image%20Augmentation&entry.906535625=Shukang%20Yin%20and%20Chaoyou%20Fu%20and%20Sirui%20Zhao%20and%20Chunjiang%20Ge%20and%20Yan%20Yang%20and%20Yuhan%20Dai%20and%20Yongdong%20Luo%20and%20Tong%20Xu%20and%20Caifeng%20Shan%20and%20Enhong%20Chen&entry.1292438233=%20%20Recent%20years%20have%20seen%20the%20success%20of%20Multimodal%20Large%20Language%20Models%0A%28MLLMs%29%20in%20the%20domain%20of%20vision%20understanding.%20The%20success%20of%20these%20models%20can%0Alargely%20be%20attributed%20to%20the%20dominant%20scaling%20law%2C%20which%20states%20that%20larger%0Aparameter%20sizes%20and%20data%20volumes%20contribute%20to%20better%20performance.%20Notably%2C%0Adata%20scaling%20has%20been%20primarily%20driven%20by%20automatic%20data%20pipelines%2C%20which%20focus%0Aon%20the%20self-instruction%20of%20LLMs.%20The%20paradigm%20has%20been%20taken%20for%20granted%20for%0Aquite%20some%20time%2C%20but%20the%20study%20of%20the%20effectiveness%20of%20scaling%20with%20these%20data%0Ahas%20been%20neglected%20for%20a%20long%20time.%20In%20this%20context%2C%20this%20work%20revisits%20scaling%0Awith%20synthetic%20data%20and%20focuses%20on%20developing%20video-LLMs%20from%20a%20data-centric%0Aperspective.%20Our%20primary%20study%20approach%20involves%20fine-tuning%20pre-trained%0Aimage-LLMs%20with%20video%20data%20and%20examining%20learning%20efficiency%20through%20data%0Ascaling.%20Results%20from%20our%20preliminary%20experiments%20reveal%20a%20low%20learning%0Aefficiency%20phenomenon%20when%20simply%20scaling%20up%20video%20data%20samples%2C%20which%2C%20through%0Aour%20probing%2C%20can%20be%20ascribed%20to%20a%20lack%20of%20instruction%20diversity.%20Aiming%20at%20this%0Aissue%2C%20we%20propose%20a%20data%20augmentation%20method%20called%20Sparrow%2C%20which%20synthesizes%0Avideo-like%20samples%20from%20pure%20text%20instruction%20data.%20Mixing%20these%20synthetic%0Asamples%20with%20the%20video%20data%20enables%20a%20more%20efficient%20training%20scheme.%20Through%0Acomprehensive%20experiments%2C%20we%20demonstrate%20that%20our%20proposed%20method%20achieves%0Aperformance%20comparable%20to%20or%20even%20superior%20to%20that%20of%20baselines%20trained%20with%0Asignificantly%20more%20samples.%20Meanwhile%2C%20we%20find%20that%20incorporating%20these%0Asynthetic%20samples%20can%20enhance%20the%20performance%20of%20long%20video%20understanding%0Awithout%20requiring%20training%20on%20long%20video%20data.%20The%20code%20and%20data%20examples%20are%0Aavailable%20at%20https%3A//github.com/VITA-MLLM/Sparrow.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19951v5&entry.124074799=Read"},
{"title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering\n  and Dense Captioning for Accident Scene Understanding", "author": "Younggun Kim and Ahmed S. Abdelrahman and Mohamed Abdel-Aty", "abstract": "  Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, is a critical challenge for autonomous driving systems, as crashes\ninvolving VRUs often result in severe or fatal consequences. While multimodal\nlarge language models (MLLMs) have shown promise in enhancing scene\nunderstanding and decision making in autonomous vehicles, there is currently no\nstandardized benchmark to quantitatively evaluate their reasoning abilities in\ncomplex, safety-critical scenarios involving VRUs. To address this gap, we\npresent VRU-Accident, a large-scale vision-language benchmark designed to\nevaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident\ncomprises 1K real-world dashcam accident videos, annotated with 6K\nmultiple-choice question-answer pairs across six safety-critical categories\n(with 24K candidate options and 3.4K unique answer choices), as well as 1K\ndense scene descriptions. Unlike prior works, our benchmark focuses explicitly\non VRU-vehicle accidents, providing rich, fine-grained annotations that capture\nboth spatial-temporal dynamics and causal semantics of accidents. To assess the\ncurrent landscape of MLLMs, we conduct a comprehensive evaluation of 17\nstate-of-the-art models on the multiple-choice VQA task and on the dense\ncaptioning task. Our findings reveal that while MLLMs perform reasonably well\non visually grounded attributes, they face significant challenges in reasoning\nand describing accident causes, types, and preventability.\n", "link": "http://arxiv.org/abs/2507.09815v2", "date": "2025-07-22", "relevancy": 2.3322, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5878}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VRU-Accident%3A%20A%20Vision-Language%20Benchmark%20for%20Video%20Question%20Answering%0A%20%20and%20Dense%20Captioning%20for%20Accident%20Scene%20Understanding&body=Title%3A%20VRU-Accident%3A%20A%20Vision-Language%20Benchmark%20for%20Video%20Question%20Answering%0A%20%20and%20Dense%20Captioning%20for%20Accident%20Scene%20Understanding%0AAuthor%3A%20Younggun%20Kim%20and%20Ahmed%20S.%20Abdelrahman%20and%20Mohamed%20Abdel-Aty%0AAbstract%3A%20%20%20Ensuring%20the%20safety%20of%20vulnerable%20road%20users%20%28VRUs%29%2C%20such%20as%20pedestrians%20and%0Acyclists%2C%20is%20a%20critical%20challenge%20for%20autonomous%20driving%20systems%2C%20as%20crashes%0Ainvolving%20VRUs%20often%20result%20in%20severe%20or%20fatal%20consequences.%20While%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20have%20shown%20promise%20in%20enhancing%20scene%0Aunderstanding%20and%20decision%20making%20in%20autonomous%20vehicles%2C%20there%20is%20currently%20no%0Astandardized%20benchmark%20to%20quantitatively%20evaluate%20their%20reasoning%20abilities%20in%0Acomplex%2C%20safety-critical%20scenarios%20involving%20VRUs.%20To%20address%20this%20gap%2C%20we%0Apresent%20VRU-Accident%2C%20a%20large-scale%20vision-language%20benchmark%20designed%20to%0Aevaluate%20MLLMs%20in%20high-risk%20traffic%20scenarios%20involving%20VRUs.%20VRU-Accident%0Acomprises%201K%20real-world%20dashcam%20accident%20videos%2C%20annotated%20with%206K%0Amultiple-choice%20question-answer%20pairs%20across%20six%20safety-critical%20categories%0A%28with%2024K%20candidate%20options%20and%203.4K%20unique%20answer%20choices%29%2C%20as%20well%20as%201K%0Adense%20scene%20descriptions.%20Unlike%20prior%20works%2C%20our%20benchmark%20focuses%20explicitly%0Aon%20VRU-vehicle%20accidents%2C%20providing%20rich%2C%20fine-grained%20annotations%20that%20capture%0Aboth%20spatial-temporal%20dynamics%20and%20causal%20semantics%20of%20accidents.%20To%20assess%20the%0Acurrent%20landscape%20of%20MLLMs%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%2017%0Astate-of-the-art%20models%20on%20the%20multiple-choice%20VQA%20task%20and%20on%20the%20dense%0Acaptioning%20task.%20Our%20findings%20reveal%20that%20while%20MLLMs%20perform%20reasonably%20well%0Aon%20visually%20grounded%20attributes%2C%20they%20face%20significant%20challenges%20in%20reasoning%0Aand%20describing%20accident%20causes%2C%20types%2C%20and%20preventability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVRU-Accident%253A%2520A%2520Vision-Language%2520Benchmark%2520for%2520Video%2520Question%2520Answering%250A%2520%2520and%2520Dense%2520Captioning%2520for%2520Accident%2520Scene%2520Understanding%26entry.906535625%3DYounggun%2520Kim%2520and%2520Ahmed%2520S.%2520Abdelrahman%2520and%2520Mohamed%2520Abdel-Aty%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520safety%2520of%2520vulnerable%2520road%2520users%2520%2528VRUs%2529%252C%2520such%2520as%2520pedestrians%2520and%250Acyclists%252C%2520is%2520a%2520critical%2520challenge%2520for%2520autonomous%2520driving%2520systems%252C%2520as%2520crashes%250Ainvolving%2520VRUs%2520often%2520result%2520in%2520severe%2520or%2520fatal%2520consequences.%2520While%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%2520promise%2520in%2520enhancing%2520scene%250Aunderstanding%2520and%2520decision%2520making%2520in%2520autonomous%2520vehicles%252C%2520there%2520is%2520currently%2520no%250Astandardized%2520benchmark%2520to%2520quantitatively%2520evaluate%2520their%2520reasoning%2520abilities%2520in%250Acomplex%252C%2520safety-critical%2520scenarios%2520involving%2520VRUs.%2520To%2520address%2520this%2520gap%252C%2520we%250Apresent%2520VRU-Accident%252C%2520a%2520large-scale%2520vision-language%2520benchmark%2520designed%2520to%250Aevaluate%2520MLLMs%2520in%2520high-risk%2520traffic%2520scenarios%2520involving%2520VRUs.%2520VRU-Accident%250Acomprises%25201K%2520real-world%2520dashcam%2520accident%2520videos%252C%2520annotated%2520with%25206K%250Amultiple-choice%2520question-answer%2520pairs%2520across%2520six%2520safety-critical%2520categories%250A%2528with%252024K%2520candidate%2520options%2520and%25203.4K%2520unique%2520answer%2520choices%2529%252C%2520as%2520well%2520as%25201K%250Adense%2520scene%2520descriptions.%2520Unlike%2520prior%2520works%252C%2520our%2520benchmark%2520focuses%2520explicitly%250Aon%2520VRU-vehicle%2520accidents%252C%2520providing%2520rich%252C%2520fine-grained%2520annotations%2520that%2520capture%250Aboth%2520spatial-temporal%2520dynamics%2520and%2520causal%2520semantics%2520of%2520accidents.%2520To%2520assess%2520the%250Acurrent%2520landscape%2520of%2520MLLMs%252C%2520we%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%252017%250Astate-of-the-art%2520models%2520on%2520the%2520multiple-choice%2520VQA%2520task%2520and%2520on%2520the%2520dense%250Acaptioning%2520task.%2520Our%2520findings%2520reveal%2520that%2520while%2520MLLMs%2520perform%2520reasonably%2520well%250Aon%2520visually%2520grounded%2520attributes%252C%2520they%2520face%2520significant%2520challenges%2520in%2520reasoning%250Aand%2520describing%2520accident%2520causes%252C%2520types%252C%2520and%2520preventability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VRU-Accident%3A%20A%20Vision-Language%20Benchmark%20for%20Video%20Question%20Answering%0A%20%20and%20Dense%20Captioning%20for%20Accident%20Scene%20Understanding&entry.906535625=Younggun%20Kim%20and%20Ahmed%20S.%20Abdelrahman%20and%20Mohamed%20Abdel-Aty&entry.1292438233=%20%20Ensuring%20the%20safety%20of%20vulnerable%20road%20users%20%28VRUs%29%2C%20such%20as%20pedestrians%20and%0Acyclists%2C%20is%20a%20critical%20challenge%20for%20autonomous%20driving%20systems%2C%20as%20crashes%0Ainvolving%20VRUs%20often%20result%20in%20severe%20or%20fatal%20consequences.%20While%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20have%20shown%20promise%20in%20enhancing%20scene%0Aunderstanding%20and%20decision%20making%20in%20autonomous%20vehicles%2C%20there%20is%20currently%20no%0Astandardized%20benchmark%20to%20quantitatively%20evaluate%20their%20reasoning%20abilities%20in%0Acomplex%2C%20safety-critical%20scenarios%20involving%20VRUs.%20To%20address%20this%20gap%2C%20we%0Apresent%20VRU-Accident%2C%20a%20large-scale%20vision-language%20benchmark%20designed%20to%0Aevaluate%20MLLMs%20in%20high-risk%20traffic%20scenarios%20involving%20VRUs.%20VRU-Accident%0Acomprises%201K%20real-world%20dashcam%20accident%20videos%2C%20annotated%20with%206K%0Amultiple-choice%20question-answer%20pairs%20across%20six%20safety-critical%20categories%0A%28with%2024K%20candidate%20options%20and%203.4K%20unique%20answer%20choices%29%2C%20as%20well%20as%201K%0Adense%20scene%20descriptions.%20Unlike%20prior%20works%2C%20our%20benchmark%20focuses%20explicitly%0Aon%20VRU-vehicle%20accidents%2C%20providing%20rich%2C%20fine-grained%20annotations%20that%20capture%0Aboth%20spatial-temporal%20dynamics%20and%20causal%20semantics%20of%20accidents.%20To%20assess%20the%0Acurrent%20landscape%20of%20MLLMs%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%2017%0Astate-of-the-art%20models%20on%20the%20multiple-choice%20VQA%20task%20and%20on%20the%20dense%0Acaptioning%20task.%20Our%20findings%20reveal%20that%20while%20MLLMs%20perform%20reasonably%20well%0Aon%20visually%20grounded%20attributes%2C%20they%20face%20significant%20challenges%20in%20reasoning%0Aand%20describing%20accident%20causes%2C%20types%2C%20and%20preventability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09815v2&entry.124074799=Read"},
{"title": "The Unified Cognitive Consciousness Theory for Language Models:\n  Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning", "author": "Edward Y. Chang and Zeyneb N. Kaya and Ethan Chang", "abstract": "  Large language models (LLMs) are vast repositories of latent patterns, but\nwithout structured guidance, they lack explicit reasoning, semantic grounding,\nand goal-directed intelligence. We propose Unified Cognitive Consciousness\nTheory (UCCT), a unified model that reinterprets LLMs as unconscious substrates\nrequiring external mechanisms, few-shot prompting, RAG, fine-tuning, and\nmulti-agent reasoning, to semantically anchor latent representations. UCCT\nformalizes this anchoring process through a Bayesian formulation, revealing a\nthreshold-crossing dynamic characterized by 1/sqrt(n) scaling that explains the\nsudden capability transitions observed across diverse tasks. The theory unifies\npreviously disparate techniques, few-shot prompting, RAG, fine-tuning, and\nmulti-agent reasoning, as special cases of a general anchoring architecture.\nThrough case studies in simple math, visual recognition, and structured debate\ntasks, we confirm the predictive power of UCCT. Furthermore, our experiment in\narithmetic in three numeral systems validates the theories of UCCT. Rather than\ntreating intelligence as an intrinsic property of LLMs, UCCT demonstrates that\nLLMs are merely unconscious pattern repositories with no inherent intelligence.\nIntelligence emerges only when external anchoring mechanisms assign target\nsemantics to these latent patterns, transforming unconscious representations\ninto conscious, goal-directed capabilities.\n", "link": "http://arxiv.org/abs/2506.02139v3", "date": "2025-07-22", "relevancy": 2.3221, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5884}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Unified%20Cognitive%20Consciousness%20Theory%20for%20Language%20Models%3A%0A%20%20Anchoring%20Semantics%2C%20Thresholds%20of%20Activation%2C%20and%20Emergent%20Reasoning&body=Title%3A%20The%20Unified%20Cognitive%20Consciousness%20Theory%20for%20Language%20Models%3A%0A%20%20Anchoring%20Semantics%2C%20Thresholds%20of%20Activation%2C%20and%20Emergent%20Reasoning%0AAuthor%3A%20Edward%20Y.%20Chang%20and%20Zeyneb%20N.%20Kaya%20and%20Ethan%20Chang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20vast%20repositories%20of%20latent%20patterns%2C%20but%0Awithout%20structured%20guidance%2C%20they%20lack%20explicit%20reasoning%2C%20semantic%20grounding%2C%0Aand%20goal-directed%20intelligence.%20We%20propose%20Unified%20Cognitive%20Consciousness%0ATheory%20%28UCCT%29%2C%20a%20unified%20model%20that%20reinterprets%20LLMs%20as%20unconscious%20substrates%0Arequiring%20external%20mechanisms%2C%20few-shot%20prompting%2C%20RAG%2C%20fine-tuning%2C%20and%0Amulti-agent%20reasoning%2C%20to%20semantically%20anchor%20latent%20representations.%20UCCT%0Aformalizes%20this%20anchoring%20process%20through%20a%20Bayesian%20formulation%2C%20revealing%20a%0Athreshold-crossing%20dynamic%20characterized%20by%201/sqrt%28n%29%20scaling%20that%20explains%20the%0Asudden%20capability%20transitions%20observed%20across%20diverse%20tasks.%20The%20theory%20unifies%0Apreviously%20disparate%20techniques%2C%20few-shot%20prompting%2C%20RAG%2C%20fine-tuning%2C%20and%0Amulti-agent%20reasoning%2C%20as%20special%20cases%20of%20a%20general%20anchoring%20architecture.%0AThrough%20case%20studies%20in%20simple%20math%2C%20visual%20recognition%2C%20and%20structured%20debate%0Atasks%2C%20we%20confirm%20the%20predictive%20power%20of%20UCCT.%20Furthermore%2C%20our%20experiment%20in%0Aarithmetic%20in%20three%20numeral%20systems%20validates%20the%20theories%20of%20UCCT.%20Rather%20than%0Atreating%20intelligence%20as%20an%20intrinsic%20property%20of%20LLMs%2C%20UCCT%20demonstrates%20that%0ALLMs%20are%20merely%20unconscious%20pattern%20repositories%20with%20no%20inherent%20intelligence.%0AIntelligence%20emerges%20only%20when%20external%20anchoring%20mechanisms%20assign%20target%0Asemantics%20to%20these%20latent%20patterns%2C%20transforming%20unconscious%20representations%0Ainto%20conscious%2C%20goal-directed%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02139v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Unified%2520Cognitive%2520Consciousness%2520Theory%2520for%2520Language%2520Models%253A%250A%2520%2520Anchoring%2520Semantics%252C%2520Thresholds%2520of%2520Activation%252C%2520and%2520Emergent%2520Reasoning%26entry.906535625%3DEdward%2520Y.%2520Chang%2520and%2520Zeyneb%2520N.%2520Kaya%2520and%2520Ethan%2520Chang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520vast%2520repositories%2520of%2520latent%2520patterns%252C%2520but%250Awithout%2520structured%2520guidance%252C%2520they%2520lack%2520explicit%2520reasoning%252C%2520semantic%2520grounding%252C%250Aand%2520goal-directed%2520intelligence.%2520We%2520propose%2520Unified%2520Cognitive%2520Consciousness%250ATheory%2520%2528UCCT%2529%252C%2520a%2520unified%2520model%2520that%2520reinterprets%2520LLMs%2520as%2520unconscious%2520substrates%250Arequiring%2520external%2520mechanisms%252C%2520few-shot%2520prompting%252C%2520RAG%252C%2520fine-tuning%252C%2520and%250Amulti-agent%2520reasoning%252C%2520to%2520semantically%2520anchor%2520latent%2520representations.%2520UCCT%250Aformalizes%2520this%2520anchoring%2520process%2520through%2520a%2520Bayesian%2520formulation%252C%2520revealing%2520a%250Athreshold-crossing%2520dynamic%2520characterized%2520by%25201/sqrt%2528n%2529%2520scaling%2520that%2520explains%2520the%250Asudden%2520capability%2520transitions%2520observed%2520across%2520diverse%2520tasks.%2520The%2520theory%2520unifies%250Apreviously%2520disparate%2520techniques%252C%2520few-shot%2520prompting%252C%2520RAG%252C%2520fine-tuning%252C%2520and%250Amulti-agent%2520reasoning%252C%2520as%2520special%2520cases%2520of%2520a%2520general%2520anchoring%2520architecture.%250AThrough%2520case%2520studies%2520in%2520simple%2520math%252C%2520visual%2520recognition%252C%2520and%2520structured%2520debate%250Atasks%252C%2520we%2520confirm%2520the%2520predictive%2520power%2520of%2520UCCT.%2520Furthermore%252C%2520our%2520experiment%2520in%250Aarithmetic%2520in%2520three%2520numeral%2520systems%2520validates%2520the%2520theories%2520of%2520UCCT.%2520Rather%2520than%250Atreating%2520intelligence%2520as%2520an%2520intrinsic%2520property%2520of%2520LLMs%252C%2520UCCT%2520demonstrates%2520that%250ALLMs%2520are%2520merely%2520unconscious%2520pattern%2520repositories%2520with%2520no%2520inherent%2520intelligence.%250AIntelligence%2520emerges%2520only%2520when%2520external%2520anchoring%2520mechanisms%2520assign%2520target%250Asemantics%2520to%2520these%2520latent%2520patterns%252C%2520transforming%2520unconscious%2520representations%250Ainto%2520conscious%252C%2520goal-directed%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02139v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Unified%20Cognitive%20Consciousness%20Theory%20for%20Language%20Models%3A%0A%20%20Anchoring%20Semantics%2C%20Thresholds%20of%20Activation%2C%20and%20Emergent%20Reasoning&entry.906535625=Edward%20Y.%20Chang%20and%20Zeyneb%20N.%20Kaya%20and%20Ethan%20Chang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20vast%20repositories%20of%20latent%20patterns%2C%20but%0Awithout%20structured%20guidance%2C%20they%20lack%20explicit%20reasoning%2C%20semantic%20grounding%2C%0Aand%20goal-directed%20intelligence.%20We%20propose%20Unified%20Cognitive%20Consciousness%0ATheory%20%28UCCT%29%2C%20a%20unified%20model%20that%20reinterprets%20LLMs%20as%20unconscious%20substrates%0Arequiring%20external%20mechanisms%2C%20few-shot%20prompting%2C%20RAG%2C%20fine-tuning%2C%20and%0Amulti-agent%20reasoning%2C%20to%20semantically%20anchor%20latent%20representations.%20UCCT%0Aformalizes%20this%20anchoring%20process%20through%20a%20Bayesian%20formulation%2C%20revealing%20a%0Athreshold-crossing%20dynamic%20characterized%20by%201/sqrt%28n%29%20scaling%20that%20explains%20the%0Asudden%20capability%20transitions%20observed%20across%20diverse%20tasks.%20The%20theory%20unifies%0Apreviously%20disparate%20techniques%2C%20few-shot%20prompting%2C%20RAG%2C%20fine-tuning%2C%20and%0Amulti-agent%20reasoning%2C%20as%20special%20cases%20of%20a%20general%20anchoring%20architecture.%0AThrough%20case%20studies%20in%20simple%20math%2C%20visual%20recognition%2C%20and%20structured%20debate%0Atasks%2C%20we%20confirm%20the%20predictive%20power%20of%20UCCT.%20Furthermore%2C%20our%20experiment%20in%0Aarithmetic%20in%20three%20numeral%20systems%20validates%20the%20theories%20of%20UCCT.%20Rather%20than%0Atreating%20intelligence%20as%20an%20intrinsic%20property%20of%20LLMs%2C%20UCCT%20demonstrates%20that%0ALLMs%20are%20merely%20unconscious%20pattern%20repositories%20with%20no%20inherent%20intelligence.%0AIntelligence%20emerges%20only%20when%20external%20anchoring%20mechanisms%20assign%20target%0Asemantics%20to%20these%20latent%20patterns%2C%20transforming%20unconscious%20representations%0Ainto%20conscious%2C%20goal-directed%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02139v3&entry.124074799=Read"},
{"title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning", "author": "Chi-Pin Huang and Yueh-Hua Wu and Min-Hung Chen and Yu-Chiang Frank Wang and Fu-En Yang", "abstract": "  Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.\n", "link": "http://arxiv.org/abs/2507.16815v1", "date": "2025-07-22", "relevancy": 2.2874, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ThinkAct%3A%20Vision-Language-Action%20Reasoning%20via%20Reinforced%20Visual%20Latent%0A%20%20Planning&body=Title%3A%20ThinkAct%3A%20Vision-Language-Action%20Reasoning%20via%20Reinforced%20Visual%20Latent%0A%20%20Planning%0AAuthor%3A%20Chi-Pin%20Huang%20and%20Yueh-Hua%20Wu%20and%20Min-Hung%20Chen%20and%20Yu-Chiang%20Frank%20Wang%20and%20Fu-En%20Yang%0AAbstract%3A%20%20%20Vision-language-action%20%28VLA%29%20reasoning%20tasks%20require%20agents%20to%20interpret%0Amultimodal%20instructions%2C%20perform%20long-horizon%20planning%2C%20and%20act%20adaptively%20in%0Adynamic%20environments.%20Existing%20approaches%20typically%20train%20VLA%20models%20in%20an%0Aend-to-end%20fashion%2C%20directly%20mapping%20inputs%20to%20actions%20without%20explicit%0Areasoning%2C%20which%20hinders%20their%20ability%20to%20plan%20over%20multiple%20steps%20or%20adapt%20to%0Acomplex%20task%20variations.%20In%20this%20paper%2C%20we%20propose%20ThinkAct%2C%20a%20dual-system%0Aframework%20that%20bridges%20high-level%20reasoning%20with%20low-level%20action%20execution%20via%0Areinforced%20visual%20latent%20planning.%20ThinkAct%20trains%20a%20multimodal%20LLM%20to%20generate%0Aembodied%20reasoning%20plans%20guided%20by%20reinforcing%20action-aligned%20visual%20rewards%0Abased%20on%20goal%20completion%20and%20trajectory%20consistency.%20These%20reasoning%20plans%20are%0Acompressed%20into%20a%20visual%20plan%20latent%20that%20conditions%20a%20downstream%20action%20model%0Afor%20robust%20action%20execution%20on%20target%20environments.%20Extensive%20experiments%20on%0Aembodied%20reasoning%20and%20robot%20manipulation%20benchmarks%20demonstrate%20that%20ThinkAct%0Aenables%20few-shot%20adaptation%2C%20long-horizon%20planning%2C%20and%20self-correction%0Abehaviors%20in%20complex%20embodied%20AI%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThinkAct%253A%2520Vision-Language-Action%2520Reasoning%2520via%2520Reinforced%2520Visual%2520Latent%250A%2520%2520Planning%26entry.906535625%3DChi-Pin%2520Huang%2520and%2520Yueh-Hua%2520Wu%2520and%2520Min-Hung%2520Chen%2520and%2520Yu-Chiang%2520Frank%2520Wang%2520and%2520Fu-En%2520Yang%26entry.1292438233%3D%2520%2520Vision-language-action%2520%2528VLA%2529%2520reasoning%2520tasks%2520require%2520agents%2520to%2520interpret%250Amultimodal%2520instructions%252C%2520perform%2520long-horizon%2520planning%252C%2520and%2520act%2520adaptively%2520in%250Adynamic%2520environments.%2520Existing%2520approaches%2520typically%2520train%2520VLA%2520models%2520in%2520an%250Aend-to-end%2520fashion%252C%2520directly%2520mapping%2520inputs%2520to%2520actions%2520without%2520explicit%250Areasoning%252C%2520which%2520hinders%2520their%2520ability%2520to%2520plan%2520over%2520multiple%2520steps%2520or%2520adapt%2520to%250Acomplex%2520task%2520variations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ThinkAct%252C%2520a%2520dual-system%250Aframework%2520that%2520bridges%2520high-level%2520reasoning%2520with%2520low-level%2520action%2520execution%2520via%250Areinforced%2520visual%2520latent%2520planning.%2520ThinkAct%2520trains%2520a%2520multimodal%2520LLM%2520to%2520generate%250Aembodied%2520reasoning%2520plans%2520guided%2520by%2520reinforcing%2520action-aligned%2520visual%2520rewards%250Abased%2520on%2520goal%2520completion%2520and%2520trajectory%2520consistency.%2520These%2520reasoning%2520plans%2520are%250Acompressed%2520into%2520a%2520visual%2520plan%2520latent%2520that%2520conditions%2520a%2520downstream%2520action%2520model%250Afor%2520robust%2520action%2520execution%2520on%2520target%2520environments.%2520Extensive%2520experiments%2520on%250Aembodied%2520reasoning%2520and%2520robot%2520manipulation%2520benchmarks%2520demonstrate%2520that%2520ThinkAct%250Aenables%2520few-shot%2520adaptation%252C%2520long-horizon%2520planning%252C%2520and%2520self-correction%250Abehaviors%2520in%2520complex%2520embodied%2520AI%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThinkAct%3A%20Vision-Language-Action%20Reasoning%20via%20Reinforced%20Visual%20Latent%0A%20%20Planning&entry.906535625=Chi-Pin%20Huang%20and%20Yueh-Hua%20Wu%20and%20Min-Hung%20Chen%20and%20Yu-Chiang%20Frank%20Wang%20and%20Fu-En%20Yang&entry.1292438233=%20%20Vision-language-action%20%28VLA%29%20reasoning%20tasks%20require%20agents%20to%20interpret%0Amultimodal%20instructions%2C%20perform%20long-horizon%20planning%2C%20and%20act%20adaptively%20in%0Adynamic%20environments.%20Existing%20approaches%20typically%20train%20VLA%20models%20in%20an%0Aend-to-end%20fashion%2C%20directly%20mapping%20inputs%20to%20actions%20without%20explicit%0Areasoning%2C%20which%20hinders%20their%20ability%20to%20plan%20over%20multiple%20steps%20or%20adapt%20to%0Acomplex%20task%20variations.%20In%20this%20paper%2C%20we%20propose%20ThinkAct%2C%20a%20dual-system%0Aframework%20that%20bridges%20high-level%20reasoning%20with%20low-level%20action%20execution%20via%0Areinforced%20visual%20latent%20planning.%20ThinkAct%20trains%20a%20multimodal%20LLM%20to%20generate%0Aembodied%20reasoning%20plans%20guided%20by%20reinforcing%20action-aligned%20visual%20rewards%0Abased%20on%20goal%20completion%20and%20trajectory%20consistency.%20These%20reasoning%20plans%20are%0Acompressed%20into%20a%20visual%20plan%20latent%20that%20conditions%20a%20downstream%20action%20model%0Afor%20robust%20action%20execution%20on%20target%20environments.%20Extensive%20experiments%20on%0Aembodied%20reasoning%20and%20robot%20manipulation%20benchmarks%20demonstrate%20that%20ThinkAct%0Aenables%20few-shot%20adaptation%2C%20long-horizon%20planning%2C%20and%20self-correction%0Abehaviors%20in%20complex%20embodied%20AI%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16815v1&entry.124074799=Read"},
{"title": "A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System", "author": "Lorenzo Gentilini and Pierpaolo Serio and Valentina Donzella and Lorenzo Pollini", "abstract": "  Extrinsic Calibration represents the cornerstone of autonomous driving. Its\naccuracy plays a crucial role in the perception pipeline, as any errors can\nhave implications for the safety of the vehicle. Modern sensor systems collect\ndifferent types of data from the environment, making it harder to align the\ndata. To this end, we propose a target-based extrinsic calibration system\ntailored for a multi-LiDAR and multi-camera sensor suite. This system enables\ncross-calibration between LiDARs and cameras with limited prior knowledge using\na custom ChArUco board and a tailored nonlinear optimization method. We test\nthe system with real-world data gathered in a warehouse. Results demonstrated\nthe effectiveness of the proposed method, highlighting the feasibility of a\nunique pipeline tailored for various types of sensors.\n", "link": "http://arxiv.org/abs/2507.16621v1", "date": "2025-07-22", "relevancy": 2.2783, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5797}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5695}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Target-based%20Multi-LiDAR%20Multi-Camera%20Extrinsic%20Calibration%20System&body=Title%3A%20A%20Target-based%20Multi-LiDAR%20Multi-Camera%20Extrinsic%20Calibration%20System%0AAuthor%3A%20Lorenzo%20Gentilini%20and%20Pierpaolo%20Serio%20and%20Valentina%20Donzella%20and%20Lorenzo%20Pollini%0AAbstract%3A%20%20%20Extrinsic%20Calibration%20represents%20the%20cornerstone%20of%20autonomous%20driving.%20Its%0Aaccuracy%20plays%20a%20crucial%20role%20in%20the%20perception%20pipeline%2C%20as%20any%20errors%20can%0Ahave%20implications%20for%20the%20safety%20of%20the%20vehicle.%20Modern%20sensor%20systems%20collect%0Adifferent%20types%20of%20data%20from%20the%20environment%2C%20making%20it%20harder%20to%20align%20the%0Adata.%20To%20this%20end%2C%20we%20propose%20a%20target-based%20extrinsic%20calibration%20system%0Atailored%20for%20a%20multi-LiDAR%20and%20multi-camera%20sensor%20suite.%20This%20system%20enables%0Across-calibration%20between%20LiDARs%20and%20cameras%20with%20limited%20prior%20knowledge%20using%0Aa%20custom%20ChArUco%20board%20and%20a%20tailored%20nonlinear%20optimization%20method.%20We%20test%0Athe%20system%20with%20real-world%20data%20gathered%20in%20a%20warehouse.%20Results%20demonstrated%0Athe%20effectiveness%20of%20the%20proposed%20method%2C%20highlighting%20the%20feasibility%20of%20a%0Aunique%20pipeline%20tailored%20for%20various%20types%20of%20sensors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Target-based%2520Multi-LiDAR%2520Multi-Camera%2520Extrinsic%2520Calibration%2520System%26entry.906535625%3DLorenzo%2520Gentilini%2520and%2520Pierpaolo%2520Serio%2520and%2520Valentina%2520Donzella%2520and%2520Lorenzo%2520Pollini%26entry.1292438233%3D%2520%2520Extrinsic%2520Calibration%2520represents%2520the%2520cornerstone%2520of%2520autonomous%2520driving.%2520Its%250Aaccuracy%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520perception%2520pipeline%252C%2520as%2520any%2520errors%2520can%250Ahave%2520implications%2520for%2520the%2520safety%2520of%2520the%2520vehicle.%2520Modern%2520sensor%2520systems%2520collect%250Adifferent%2520types%2520of%2520data%2520from%2520the%2520environment%252C%2520making%2520it%2520harder%2520to%2520align%2520the%250Adata.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520target-based%2520extrinsic%2520calibration%2520system%250Atailored%2520for%2520a%2520multi-LiDAR%2520and%2520multi-camera%2520sensor%2520suite.%2520This%2520system%2520enables%250Across-calibration%2520between%2520LiDARs%2520and%2520cameras%2520with%2520limited%2520prior%2520knowledge%2520using%250Aa%2520custom%2520ChArUco%2520board%2520and%2520a%2520tailored%2520nonlinear%2520optimization%2520method.%2520We%2520test%250Athe%2520system%2520with%2520real-world%2520data%2520gathered%2520in%2520a%2520warehouse.%2520Results%2520demonstrated%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520method%252C%2520highlighting%2520the%2520feasibility%2520of%2520a%250Aunique%2520pipeline%2520tailored%2520for%2520various%2520types%2520of%2520sensors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Target-based%20Multi-LiDAR%20Multi-Camera%20Extrinsic%20Calibration%20System&entry.906535625=Lorenzo%20Gentilini%20and%20Pierpaolo%20Serio%20and%20Valentina%20Donzella%20and%20Lorenzo%20Pollini&entry.1292438233=%20%20Extrinsic%20Calibration%20represents%20the%20cornerstone%20of%20autonomous%20driving.%20Its%0Aaccuracy%20plays%20a%20crucial%20role%20in%20the%20perception%20pipeline%2C%20as%20any%20errors%20can%0Ahave%20implications%20for%20the%20safety%20of%20the%20vehicle.%20Modern%20sensor%20systems%20collect%0Adifferent%20types%20of%20data%20from%20the%20environment%2C%20making%20it%20harder%20to%20align%20the%0Adata.%20To%20this%20end%2C%20we%20propose%20a%20target-based%20extrinsic%20calibration%20system%0Atailored%20for%20a%20multi-LiDAR%20and%20multi-camera%20sensor%20suite.%20This%20system%20enables%0Across-calibration%20between%20LiDARs%20and%20cameras%20with%20limited%20prior%20knowledge%20using%0Aa%20custom%20ChArUco%20board%20and%20a%20tailored%20nonlinear%20optimization%20method.%20We%20test%0Athe%20system%20with%20real-world%20data%20gathered%20in%20a%20warehouse.%20Results%20demonstrated%0Athe%20effectiveness%20of%20the%20proposed%20method%2C%20highlighting%20the%20feasibility%20of%20a%0Aunique%20pipeline%20tailored%20for%20various%20types%20of%20sensors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16621v1&entry.124074799=Read"},
{"title": "Structural Effect and Spectral Enhancement of High-Dimensional\n  Regularized Linear Discriminant Analysis", "author": "Yonghan Zhang and Zhangni Pu and Lu Yan and Jiang Hu", "abstract": "  Regularized linear discriminant analysis (RLDA) is a widely used tool for\nclassification and dimensionality reduction, but its performance in\nhigh-dimensional scenarios is inconsistent. Existing theoretical analyses of\nRLDA often lack clear insight into how data structure affects classification\nperformance. To address this issue, we derive a non-asymptotic approximation of\nthe misclassification rate and thus analyze the structural effect and\nstructural adjustment strategies of RLDA. Based on this, we propose the\nSpectral Enhanced Discriminant Analysis (SEDA) algorithm, which optimizes the\ndata structure by adjusting the spiked eigenvalues of the population covariance\nmatrix. By developing a new theoretical result on eigenvectors in random matrix\ntheory, we derive an asymptotic approximation on the misclassification rate of\nSEDA. The bias correction algorithm and parameter selection strategy are then\nobtained. Experiments on synthetic and real datasets show that SEDA achieves\nhigher classification accuracy and dimensionality reduction compared to\nexisting LDA methods.\n", "link": "http://arxiv.org/abs/2507.16682v1", "date": "2025-07-22", "relevancy": 2.262, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4569}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4514}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Effect%20and%20Spectral%20Enhancement%20of%20High-Dimensional%0A%20%20Regularized%20Linear%20Discriminant%20Analysis&body=Title%3A%20Structural%20Effect%20and%20Spectral%20Enhancement%20of%20High-Dimensional%0A%20%20Regularized%20Linear%20Discriminant%20Analysis%0AAuthor%3A%20Yonghan%20Zhang%20and%20Zhangni%20Pu%20and%20Lu%20Yan%20and%20Jiang%20Hu%0AAbstract%3A%20%20%20Regularized%20linear%20discriminant%20analysis%20%28RLDA%29%20is%20a%20widely%20used%20tool%20for%0Aclassification%20and%20dimensionality%20reduction%2C%20but%20its%20performance%20in%0Ahigh-dimensional%20scenarios%20is%20inconsistent.%20Existing%20theoretical%20analyses%20of%0ARLDA%20often%20lack%20clear%20insight%20into%20how%20data%20structure%20affects%20classification%0Aperformance.%20To%20address%20this%20issue%2C%20we%20derive%20a%20non-asymptotic%20approximation%20of%0Athe%20misclassification%20rate%20and%20thus%20analyze%20the%20structural%20effect%20and%0Astructural%20adjustment%20strategies%20of%20RLDA.%20Based%20on%20this%2C%20we%20propose%20the%0ASpectral%20Enhanced%20Discriminant%20Analysis%20%28SEDA%29%20algorithm%2C%20which%20optimizes%20the%0Adata%20structure%20by%20adjusting%20the%20spiked%20eigenvalues%20of%20the%20population%20covariance%0Amatrix.%20By%20developing%20a%20new%20theoretical%20result%20on%20eigenvectors%20in%20random%20matrix%0Atheory%2C%20we%20derive%20an%20asymptotic%20approximation%20on%20the%20misclassification%20rate%20of%0ASEDA.%20The%20bias%20correction%20algorithm%20and%20parameter%20selection%20strategy%20are%20then%0Aobtained.%20Experiments%20on%20synthetic%20and%20real%20datasets%20show%20that%20SEDA%20achieves%0Ahigher%20classification%20accuracy%20and%20dimensionality%20reduction%20compared%20to%0Aexisting%20LDA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Effect%2520and%2520Spectral%2520Enhancement%2520of%2520High-Dimensional%250A%2520%2520Regularized%2520Linear%2520Discriminant%2520Analysis%26entry.906535625%3DYonghan%2520Zhang%2520and%2520Zhangni%2520Pu%2520and%2520Lu%2520Yan%2520and%2520Jiang%2520Hu%26entry.1292438233%3D%2520%2520Regularized%2520linear%2520discriminant%2520analysis%2520%2528RLDA%2529%2520is%2520a%2520widely%2520used%2520tool%2520for%250Aclassification%2520and%2520dimensionality%2520reduction%252C%2520but%2520its%2520performance%2520in%250Ahigh-dimensional%2520scenarios%2520is%2520inconsistent.%2520Existing%2520theoretical%2520analyses%2520of%250ARLDA%2520often%2520lack%2520clear%2520insight%2520into%2520how%2520data%2520structure%2520affects%2520classification%250Aperformance.%2520To%2520address%2520this%2520issue%252C%2520we%2520derive%2520a%2520non-asymptotic%2520approximation%2520of%250Athe%2520misclassification%2520rate%2520and%2520thus%2520analyze%2520the%2520structural%2520effect%2520and%250Astructural%2520adjustment%2520strategies%2520of%2520RLDA.%2520Based%2520on%2520this%252C%2520we%2520propose%2520the%250ASpectral%2520Enhanced%2520Discriminant%2520Analysis%2520%2528SEDA%2529%2520algorithm%252C%2520which%2520optimizes%2520the%250Adata%2520structure%2520by%2520adjusting%2520the%2520spiked%2520eigenvalues%2520of%2520the%2520population%2520covariance%250Amatrix.%2520By%2520developing%2520a%2520new%2520theoretical%2520result%2520on%2520eigenvectors%2520in%2520random%2520matrix%250Atheory%252C%2520we%2520derive%2520an%2520asymptotic%2520approximation%2520on%2520the%2520misclassification%2520rate%2520of%250ASEDA.%2520The%2520bias%2520correction%2520algorithm%2520and%2520parameter%2520selection%2520strategy%2520are%2520then%250Aobtained.%2520Experiments%2520on%2520synthetic%2520and%2520real%2520datasets%2520show%2520that%2520SEDA%2520achieves%250Ahigher%2520classification%2520accuracy%2520and%2520dimensionality%2520reduction%2520compared%2520to%250Aexisting%2520LDA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Effect%20and%20Spectral%20Enhancement%20of%20High-Dimensional%0A%20%20Regularized%20Linear%20Discriminant%20Analysis&entry.906535625=Yonghan%20Zhang%20and%20Zhangni%20Pu%20and%20Lu%20Yan%20and%20Jiang%20Hu&entry.1292438233=%20%20Regularized%20linear%20discriminant%20analysis%20%28RLDA%29%20is%20a%20widely%20used%20tool%20for%0Aclassification%20and%20dimensionality%20reduction%2C%20but%20its%20performance%20in%0Ahigh-dimensional%20scenarios%20is%20inconsistent.%20Existing%20theoretical%20analyses%20of%0ARLDA%20often%20lack%20clear%20insight%20into%20how%20data%20structure%20affects%20classification%0Aperformance.%20To%20address%20this%20issue%2C%20we%20derive%20a%20non-asymptotic%20approximation%20of%0Athe%20misclassification%20rate%20and%20thus%20analyze%20the%20structural%20effect%20and%0Astructural%20adjustment%20strategies%20of%20RLDA.%20Based%20on%20this%2C%20we%20propose%20the%0ASpectral%20Enhanced%20Discriminant%20Analysis%20%28SEDA%29%20algorithm%2C%20which%20optimizes%20the%0Adata%20structure%20by%20adjusting%20the%20spiked%20eigenvalues%20of%20the%20population%20covariance%0Amatrix.%20By%20developing%20a%20new%20theoretical%20result%20on%20eigenvectors%20in%20random%20matrix%0Atheory%2C%20we%20derive%20an%20asymptotic%20approximation%20on%20the%20misclassification%20rate%20of%0ASEDA.%20The%20bias%20correction%20algorithm%20and%20parameter%20selection%20strategy%20are%20then%0Aobtained.%20Experiments%20on%20synthetic%20and%20real%20datasets%20show%20that%20SEDA%20achieves%0Ahigher%20classification%20accuracy%20and%20dimensionality%20reduction%20compared%20to%0Aexisting%20LDA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16682v1&entry.124074799=Read"},
{"title": "Combining Language and Topic Models for Hierarchical Text Classification", "author": "Jaco du Toit and Marcel Dunaiski", "abstract": "  Hierarchical text classification (HTC) is a natural language processing task\nwhich has the objective of categorising text documents into a set of classes\nfrom a predefined structured class hierarchy. Recent HTC approaches use various\ntechniques to incorporate the hierarchical class structure information with the\nnatural language understanding capabilities of pre-trained language models\n(PLMs) to improve classification performance. Furthermore, using topic models\nalong with PLMs to extract features from text documents has been shown to be an\neffective approach for multi-label text classification tasks. The rationale\nbehind the combination of these feature extractor models is that the PLM\ncaptures the finer-grained contextual and semantic information while the topic\nmodel obtains high-level representations which consider the corpus of documents\nas a whole. In this paper, we use a HTC approach which uses a PLM and a topic\nmodel to extract features from text documents which are used to train a\nclassification model. Our objective is to determine whether the combination of\nthe features extracted from the two models is beneficial to HTC performance in\ngeneral. In our approach, the extracted features are passed through separate\nconvolutional layers whose outputs are combined and passed to a label-wise\nattention mechanisms which obtains label-specific document representations by\nweighing the most important features for each class separately. We perform\ncomprehensive experiments on three HTC benchmark datasets and show that using\nthe features extracted from the topic model generally decreases classification\nperformance compared to only using the features obtained by the PLM. In\ncontrast to previous work, this shows that the incorporation of features\nextracted from topic models for text classification tasks should not be assumed\nbeneficial.\n", "link": "http://arxiv.org/abs/2507.16490v1", "date": "2025-07-22", "relevancy": 2.2586, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4407}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Combining%20Language%20and%20Topic%20Models%20for%20Hierarchical%20Text%20Classification&body=Title%3A%20Combining%20Language%20and%20Topic%20Models%20for%20Hierarchical%20Text%20Classification%0AAuthor%3A%20Jaco%20du%20Toit%20and%20Marcel%20Dunaiski%0AAbstract%3A%20%20%20Hierarchical%20text%20classification%20%28HTC%29%20is%20a%20natural%20language%20processing%20task%0Awhich%20has%20the%20objective%20of%20categorising%20text%20documents%20into%20a%20set%20of%20classes%0Afrom%20a%20predefined%20structured%20class%20hierarchy.%20Recent%20HTC%20approaches%20use%20various%0Atechniques%20to%20incorporate%20the%20hierarchical%20class%20structure%20information%20with%20the%0Anatural%20language%20understanding%20capabilities%20of%20pre-trained%20language%20models%0A%28PLMs%29%20to%20improve%20classification%20performance.%20Furthermore%2C%20using%20topic%20models%0Aalong%20with%20PLMs%20to%20extract%20features%20from%20text%20documents%20has%20been%20shown%20to%20be%20an%0Aeffective%20approach%20for%20multi-label%20text%20classification%20tasks.%20The%20rationale%0Abehind%20the%20combination%20of%20these%20feature%20extractor%20models%20is%20that%20the%20PLM%0Acaptures%20the%20finer-grained%20contextual%20and%20semantic%20information%20while%20the%20topic%0Amodel%20obtains%20high-level%20representations%20which%20consider%20the%20corpus%20of%20documents%0Aas%20a%20whole.%20In%20this%20paper%2C%20we%20use%20a%20HTC%20approach%20which%20uses%20a%20PLM%20and%20a%20topic%0Amodel%20to%20extract%20features%20from%20text%20documents%20which%20are%20used%20to%20train%20a%0Aclassification%20model.%20Our%20objective%20is%20to%20determine%20whether%20the%20combination%20of%0Athe%20features%20extracted%20from%20the%20two%20models%20is%20beneficial%20to%20HTC%20performance%20in%0Ageneral.%20In%20our%20approach%2C%20the%20extracted%20features%20are%20passed%20through%20separate%0Aconvolutional%20layers%20whose%20outputs%20are%20combined%20and%20passed%20to%20a%20label-wise%0Aattention%20mechanisms%20which%20obtains%20label-specific%20document%20representations%20by%0Aweighing%20the%20most%20important%20features%20for%20each%20class%20separately.%20We%20perform%0Acomprehensive%20experiments%20on%20three%20HTC%20benchmark%20datasets%20and%20show%20that%20using%0Athe%20features%20extracted%20from%20the%20topic%20model%20generally%20decreases%20classification%0Aperformance%20compared%20to%20only%20using%20the%20features%20obtained%20by%20the%20PLM.%20In%0Acontrast%20to%20previous%20work%2C%20this%20shows%20that%20the%20incorporation%20of%20features%0Aextracted%20from%20topic%20models%20for%20text%20classification%20tasks%20should%20not%20be%20assumed%0Abeneficial.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombining%2520Language%2520and%2520Topic%2520Models%2520for%2520Hierarchical%2520Text%2520Classification%26entry.906535625%3DJaco%2520du%2520Toit%2520and%2520Marcel%2520Dunaiski%26entry.1292438233%3D%2520%2520Hierarchical%2520text%2520classification%2520%2528HTC%2529%2520is%2520a%2520natural%2520language%2520processing%2520task%250Awhich%2520has%2520the%2520objective%2520of%2520categorising%2520text%2520documents%2520into%2520a%2520set%2520of%2520classes%250Afrom%2520a%2520predefined%2520structured%2520class%2520hierarchy.%2520Recent%2520HTC%2520approaches%2520use%2520various%250Atechniques%2520to%2520incorporate%2520the%2520hierarchical%2520class%2520structure%2520information%2520with%2520the%250Anatural%2520language%2520understanding%2520capabilities%2520of%2520pre-trained%2520language%2520models%250A%2528PLMs%2529%2520to%2520improve%2520classification%2520performance.%2520Furthermore%252C%2520using%2520topic%2520models%250Aalong%2520with%2520PLMs%2520to%2520extract%2520features%2520from%2520text%2520documents%2520has%2520been%2520shown%2520to%2520be%2520an%250Aeffective%2520approach%2520for%2520multi-label%2520text%2520classification%2520tasks.%2520The%2520rationale%250Abehind%2520the%2520combination%2520of%2520these%2520feature%2520extractor%2520models%2520is%2520that%2520the%2520PLM%250Acaptures%2520the%2520finer-grained%2520contextual%2520and%2520semantic%2520information%2520while%2520the%2520topic%250Amodel%2520obtains%2520high-level%2520representations%2520which%2520consider%2520the%2520corpus%2520of%2520documents%250Aas%2520a%2520whole.%2520In%2520this%2520paper%252C%2520we%2520use%2520a%2520HTC%2520approach%2520which%2520uses%2520a%2520PLM%2520and%2520a%2520topic%250Amodel%2520to%2520extract%2520features%2520from%2520text%2520documents%2520which%2520are%2520used%2520to%2520train%2520a%250Aclassification%2520model.%2520Our%2520objective%2520is%2520to%2520determine%2520whether%2520the%2520combination%2520of%250Athe%2520features%2520extracted%2520from%2520the%2520two%2520models%2520is%2520beneficial%2520to%2520HTC%2520performance%2520in%250Ageneral.%2520In%2520our%2520approach%252C%2520the%2520extracted%2520features%2520are%2520passed%2520through%2520separate%250Aconvolutional%2520layers%2520whose%2520outputs%2520are%2520combined%2520and%2520passed%2520to%2520a%2520label-wise%250Aattention%2520mechanisms%2520which%2520obtains%2520label-specific%2520document%2520representations%2520by%250Aweighing%2520the%2520most%2520important%2520features%2520for%2520each%2520class%2520separately.%2520We%2520perform%250Acomprehensive%2520experiments%2520on%2520three%2520HTC%2520benchmark%2520datasets%2520and%2520show%2520that%2520using%250Athe%2520features%2520extracted%2520from%2520the%2520topic%2520model%2520generally%2520decreases%2520classification%250Aperformance%2520compared%2520to%2520only%2520using%2520the%2520features%2520obtained%2520by%2520the%2520PLM.%2520In%250Acontrast%2520to%2520previous%2520work%252C%2520this%2520shows%2520that%2520the%2520incorporation%2520of%2520features%250Aextracted%2520from%2520topic%2520models%2520for%2520text%2520classification%2520tasks%2520should%2520not%2520be%2520assumed%250Abeneficial.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Language%20and%20Topic%20Models%20for%20Hierarchical%20Text%20Classification&entry.906535625=Jaco%20du%20Toit%20and%20Marcel%20Dunaiski&entry.1292438233=%20%20Hierarchical%20text%20classification%20%28HTC%29%20is%20a%20natural%20language%20processing%20task%0Awhich%20has%20the%20objective%20of%20categorising%20text%20documents%20into%20a%20set%20of%20classes%0Afrom%20a%20predefined%20structured%20class%20hierarchy.%20Recent%20HTC%20approaches%20use%20various%0Atechniques%20to%20incorporate%20the%20hierarchical%20class%20structure%20information%20with%20the%0Anatural%20language%20understanding%20capabilities%20of%20pre-trained%20language%20models%0A%28PLMs%29%20to%20improve%20classification%20performance.%20Furthermore%2C%20using%20topic%20models%0Aalong%20with%20PLMs%20to%20extract%20features%20from%20text%20documents%20has%20been%20shown%20to%20be%20an%0Aeffective%20approach%20for%20multi-label%20text%20classification%20tasks.%20The%20rationale%0Abehind%20the%20combination%20of%20these%20feature%20extractor%20models%20is%20that%20the%20PLM%0Acaptures%20the%20finer-grained%20contextual%20and%20semantic%20information%20while%20the%20topic%0Amodel%20obtains%20high-level%20representations%20which%20consider%20the%20corpus%20of%20documents%0Aas%20a%20whole.%20In%20this%20paper%2C%20we%20use%20a%20HTC%20approach%20which%20uses%20a%20PLM%20and%20a%20topic%0Amodel%20to%20extract%20features%20from%20text%20documents%20which%20are%20used%20to%20train%20a%0Aclassification%20model.%20Our%20objective%20is%20to%20determine%20whether%20the%20combination%20of%0Athe%20features%20extracted%20from%20the%20two%20models%20is%20beneficial%20to%20HTC%20performance%20in%0Ageneral.%20In%20our%20approach%2C%20the%20extracted%20features%20are%20passed%20through%20separate%0Aconvolutional%20layers%20whose%20outputs%20are%20combined%20and%20passed%20to%20a%20label-wise%0Aattention%20mechanisms%20which%20obtains%20label-specific%20document%20representations%20by%0Aweighing%20the%20most%20important%20features%20for%20each%20class%20separately.%20We%20perform%0Acomprehensive%20experiments%20on%20three%20HTC%20benchmark%20datasets%20and%20show%20that%20using%0Athe%20features%20extracted%20from%20the%20topic%20model%20generally%20decreases%20classification%0Aperformance%20compared%20to%20only%20using%20the%20features%20obtained%20by%20the%20PLM.%20In%0Acontrast%20to%20previous%20work%2C%20this%20shows%20that%20the%20incorporation%20of%20features%0Aextracted%20from%20topic%20models%20for%20text%20classification%20tasks%20should%20not%20be%20assumed%0Abeneficial.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16490v1&entry.124074799=Read"},
{"title": "A Method for the Architecture of a Medical Vertical Large Language Model\n  Based on Deepseek R1", "author": "Mingda Zhang and Jianglong Qin", "abstract": "  Despite significant advances in foundation models like DeepSeek-R1 and\nChatGPT, their deployment in medical settings faces critical challenges\nincluding computational requirements and professional knowledge barriers. This\npaper presents an efficient lightweight medical large language model\narchitecture that systematically addresses these challenges through\nthree-dimensional optimization: knowledge acquisition, model compression, and\ncomputational enhancement. We design a knowledge transfer pipeline from\nDeepSeek-R1-Distill-70B to DeepSeek-R1-Distill-7B using Low-Rank Adaptation\n(LoRA) for precise medical knowledge retention. Through 4-bit quantization and\nmixed-precision strategies, we achieve substantial model compression while\npreserving medical reasoning capabilities. The inference framework incorporates\nFlash Attention acceleration and continuous batching, complemented by\nspecialized prompt templates for diverse medical queries. Experimental\nevaluation on medical benchmarks demonstrates that our approach maintains 92.1%\naccuracy on USMLE examinations while reducing memory consumption by 64.7% and\ninference latency by 12.4% compared to baseline models. This work provides a\npractical solution for deploying advanced language models in\nresource-constrained medical environments, enabling broader accessibility of\nAI-assisted healthcare.\n", "link": "http://arxiv.org/abs/2505.00025v2", "date": "2025-07-22", "relevancy": 2.2472, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5673}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Method%20for%20the%20Architecture%20of%20a%20Medical%20Vertical%20Large%20Language%20Model%0A%20%20Based%20on%20Deepseek%20R1&body=Title%3A%20A%20Method%20for%20the%20Architecture%20of%20a%20Medical%20Vertical%20Large%20Language%20Model%0A%20%20Based%20on%20Deepseek%20R1%0AAuthor%3A%20Mingda%20Zhang%20and%20Jianglong%20Qin%0AAbstract%3A%20%20%20Despite%20significant%20advances%20in%20foundation%20models%20like%20DeepSeek-R1%20and%0AChatGPT%2C%20their%20deployment%20in%20medical%20settings%20faces%20critical%20challenges%0Aincluding%20computational%20requirements%20and%20professional%20knowledge%20barriers.%20This%0Apaper%20presents%20an%20efficient%20lightweight%20medical%20large%20language%20model%0Aarchitecture%20that%20systematically%20addresses%20these%20challenges%20through%0Athree-dimensional%20optimization%3A%20knowledge%20acquisition%2C%20model%20compression%2C%20and%0Acomputational%20enhancement.%20We%20design%20a%20knowledge%20transfer%20pipeline%20from%0ADeepSeek-R1-Distill-70B%20to%20DeepSeek-R1-Distill-7B%20using%20Low-Rank%20Adaptation%0A%28LoRA%29%20for%20precise%20medical%20knowledge%20retention.%20Through%204-bit%20quantization%20and%0Amixed-precision%20strategies%2C%20we%20achieve%20substantial%20model%20compression%20while%0Apreserving%20medical%20reasoning%20capabilities.%20The%20inference%20framework%20incorporates%0AFlash%20Attention%20acceleration%20and%20continuous%20batching%2C%20complemented%20by%0Aspecialized%20prompt%20templates%20for%20diverse%20medical%20queries.%20Experimental%0Aevaluation%20on%20medical%20benchmarks%20demonstrates%20that%20our%20approach%20maintains%2092.1%25%0Aaccuracy%20on%20USMLE%20examinations%20while%20reducing%20memory%20consumption%20by%2064.7%25%20and%0Ainference%20latency%20by%2012.4%25%20compared%20to%20baseline%20models.%20This%20work%20provides%20a%0Apractical%20solution%20for%20deploying%20advanced%20language%20models%20in%0Aresource-constrained%20medical%20environments%2C%20enabling%20broader%20accessibility%20of%0AAI-assisted%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.00025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Method%2520for%2520the%2520Architecture%2520of%2520a%2520Medical%2520Vertical%2520Large%2520Language%2520Model%250A%2520%2520Based%2520on%2520Deepseek%2520R1%26entry.906535625%3DMingda%2520Zhang%2520and%2520Jianglong%2520Qin%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advances%2520in%2520foundation%2520models%2520like%2520DeepSeek-R1%2520and%250AChatGPT%252C%2520their%2520deployment%2520in%2520medical%2520settings%2520faces%2520critical%2520challenges%250Aincluding%2520computational%2520requirements%2520and%2520professional%2520knowledge%2520barriers.%2520This%250Apaper%2520presents%2520an%2520efficient%2520lightweight%2520medical%2520large%2520language%2520model%250Aarchitecture%2520that%2520systematically%2520addresses%2520these%2520challenges%2520through%250Athree-dimensional%2520optimization%253A%2520knowledge%2520acquisition%252C%2520model%2520compression%252C%2520and%250Acomputational%2520enhancement.%2520We%2520design%2520a%2520knowledge%2520transfer%2520pipeline%2520from%250ADeepSeek-R1-Distill-70B%2520to%2520DeepSeek-R1-Distill-7B%2520using%2520Low-Rank%2520Adaptation%250A%2528LoRA%2529%2520for%2520precise%2520medical%2520knowledge%2520retention.%2520Through%25204-bit%2520quantization%2520and%250Amixed-precision%2520strategies%252C%2520we%2520achieve%2520substantial%2520model%2520compression%2520while%250Apreserving%2520medical%2520reasoning%2520capabilities.%2520The%2520inference%2520framework%2520incorporates%250AFlash%2520Attention%2520acceleration%2520and%2520continuous%2520batching%252C%2520complemented%2520by%250Aspecialized%2520prompt%2520templates%2520for%2520diverse%2520medical%2520queries.%2520Experimental%250Aevaluation%2520on%2520medical%2520benchmarks%2520demonstrates%2520that%2520our%2520approach%2520maintains%252092.1%2525%250Aaccuracy%2520on%2520USMLE%2520examinations%2520while%2520reducing%2520memory%2520consumption%2520by%252064.7%2525%2520and%250Ainference%2520latency%2520by%252012.4%2525%2520compared%2520to%2520baseline%2520models.%2520This%2520work%2520provides%2520a%250Apractical%2520solution%2520for%2520deploying%2520advanced%2520language%2520models%2520in%250Aresource-constrained%2520medical%2520environments%252C%2520enabling%2520broader%2520accessibility%2520of%250AAI-assisted%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.00025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Method%20for%20the%20Architecture%20of%20a%20Medical%20Vertical%20Large%20Language%20Model%0A%20%20Based%20on%20Deepseek%20R1&entry.906535625=Mingda%20Zhang%20and%20Jianglong%20Qin&entry.1292438233=%20%20Despite%20significant%20advances%20in%20foundation%20models%20like%20DeepSeek-R1%20and%0AChatGPT%2C%20their%20deployment%20in%20medical%20settings%20faces%20critical%20challenges%0Aincluding%20computational%20requirements%20and%20professional%20knowledge%20barriers.%20This%0Apaper%20presents%20an%20efficient%20lightweight%20medical%20large%20language%20model%0Aarchitecture%20that%20systematically%20addresses%20these%20challenges%20through%0Athree-dimensional%20optimization%3A%20knowledge%20acquisition%2C%20model%20compression%2C%20and%0Acomputational%20enhancement.%20We%20design%20a%20knowledge%20transfer%20pipeline%20from%0ADeepSeek-R1-Distill-70B%20to%20DeepSeek-R1-Distill-7B%20using%20Low-Rank%20Adaptation%0A%28LoRA%29%20for%20precise%20medical%20knowledge%20retention.%20Through%204-bit%20quantization%20and%0Amixed-precision%20strategies%2C%20we%20achieve%20substantial%20model%20compression%20while%0Apreserving%20medical%20reasoning%20capabilities.%20The%20inference%20framework%20incorporates%0AFlash%20Attention%20acceleration%20and%20continuous%20batching%2C%20complemented%20by%0Aspecialized%20prompt%20templates%20for%20diverse%20medical%20queries.%20Experimental%0Aevaluation%20on%20medical%20benchmarks%20demonstrates%20that%20our%20approach%20maintains%2092.1%25%0Aaccuracy%20on%20USMLE%20examinations%20while%20reducing%20memory%20consumption%20by%2064.7%25%20and%0Ainference%20latency%20by%2012.4%25%20compared%20to%20baseline%20models.%20This%20work%20provides%20a%0Apractical%20solution%20for%20deploying%20advanced%20language%20models%20in%0Aresource-constrained%20medical%20environments%2C%20enabling%20broader%20accessibility%20of%0AAI-assisted%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.00025v2&entry.124074799=Read"},
{"title": "C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving\n  Reasoning", "author": "Xiuwei Chen and Wentao Hu and Hanhui Li and Jun Zhou and Zisheng Chen and Meng Cao and Yihan Zeng and Kui Zhang and Yu-Jie Yuan and Jianhua Han and Hang Xu and Xiaodan Liang", "abstract": "  Recent advances in multimodal large language models (MLLMs) have shown\nimpressive reasoning capabilities. However, further enhancing existing MLLMs\nnecessitates high-quality vision-language datasets with carefully curated task\ncomplexities, which are both costly and challenging to scale. Although recent\nself-improving models that iteratively refine themselves offer a feasible\nsolution, they still suffer from two core challenges: (i) most existing methods\naugment visual or textual data separately, resulting in discrepancies in data\ncomplexity (e.g., over-simplified diagrams paired with redundant textual\ndescriptions); and (ii) the evolution of data and models is also separated,\nleading to scenarios where models are exposed to tasks with mismatched\ndifficulty levels. To address these issues, we propose C2-Evo, an automatic,\nclosed-loop self-improving framework that jointly evolves both training data\nand model capabilities. Specifically, given a base dataset and a base model,\nC2-Evo enhances them by a cross-modal data evolution loop and a data-model\nevolution loop. The former loop expands the base dataset by generating complex\nmultimodal problems that combine structured textual sub-problems with\niteratively specified geometric diagrams, while the latter loop adaptively\nselects the generated problems based on the performance of the base model, to\nconduct supervised fine-tuning and reinforcement learning alternately.\nConsequently, our method continuously refines its model and training data, and\nconsistently obtains considerable performance gains across multiple\nmathematical reasoning benchmarks. Our code, models, and datasets will be\nreleased.\n", "link": "http://arxiv.org/abs/2507.16518v1", "date": "2025-07-22", "relevancy": 2.2429, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C2-Evo%3A%20Co-Evolving%20Multimodal%20Data%20and%20Model%20for%20Self-Improving%0A%20%20Reasoning&body=Title%3A%20C2-Evo%3A%20Co-Evolving%20Multimodal%20Data%20and%20Model%20for%20Self-Improving%0A%20%20Reasoning%0AAuthor%3A%20Xiuwei%20Chen%20and%20Wentao%20Hu%20and%20Hanhui%20Li%20and%20Jun%20Zhou%20and%20Zisheng%20Chen%20and%20Meng%20Cao%20and%20Yihan%20Zeng%20and%20Kui%20Zhang%20and%20Yu-Jie%20Yuan%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%0Aimpressive%20reasoning%20capabilities.%20However%2C%20further%20enhancing%20existing%20MLLMs%0Anecessitates%20high-quality%20vision-language%20datasets%20with%20carefully%20curated%20task%0Acomplexities%2C%20which%20are%20both%20costly%20and%20challenging%20to%20scale.%20Although%20recent%0Aself-improving%20models%20that%20iteratively%20refine%20themselves%20offer%20a%20feasible%0Asolution%2C%20they%20still%20suffer%20from%20two%20core%20challenges%3A%20%28i%29%20most%20existing%20methods%0Aaugment%20visual%20or%20textual%20data%20separately%2C%20resulting%20in%20discrepancies%20in%20data%0Acomplexity%20%28e.g.%2C%20over-simplified%20diagrams%20paired%20with%20redundant%20textual%0Adescriptions%29%3B%20and%20%28ii%29%20the%20evolution%20of%20data%20and%20models%20is%20also%20separated%2C%0Aleading%20to%20scenarios%20where%20models%20are%20exposed%20to%20tasks%20with%20mismatched%0Adifficulty%20levels.%20To%20address%20these%20issues%2C%20we%20propose%20C2-Evo%2C%20an%20automatic%2C%0Aclosed-loop%20self-improving%20framework%20that%20jointly%20evolves%20both%20training%20data%0Aand%20model%20capabilities.%20Specifically%2C%20given%20a%20base%20dataset%20and%20a%20base%20model%2C%0AC2-Evo%20enhances%20them%20by%20a%20cross-modal%20data%20evolution%20loop%20and%20a%20data-model%0Aevolution%20loop.%20The%20former%20loop%20expands%20the%20base%20dataset%20by%20generating%20complex%0Amultimodal%20problems%20that%20combine%20structured%20textual%20sub-problems%20with%0Aiteratively%20specified%20geometric%20diagrams%2C%20while%20the%20latter%20loop%20adaptively%0Aselects%20the%20generated%20problems%20based%20on%20the%20performance%20of%20the%20base%20model%2C%20to%0Aconduct%20supervised%20fine-tuning%20and%20reinforcement%20learning%20alternately.%0AConsequently%2C%20our%20method%20continuously%20refines%20its%20model%20and%20training%20data%2C%20and%0Aconsistently%20obtains%20considerable%20performance%20gains%20across%20multiple%0Amathematical%20reasoning%20benchmarks.%20Our%20code%2C%20models%2C%20and%20datasets%20will%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC2-Evo%253A%2520Co-Evolving%2520Multimodal%2520Data%2520and%2520Model%2520for%2520Self-Improving%250A%2520%2520Reasoning%26entry.906535625%3DXiuwei%2520Chen%2520and%2520Wentao%2520Hu%2520and%2520Hanhui%2520Li%2520and%2520Jun%2520Zhou%2520and%2520Zisheng%2520Chen%2520and%2520Meng%2520Cao%2520and%2520Yihan%2520Zeng%2520and%2520Kui%2520Zhang%2520and%2520Yu-Jie%2520Yuan%2520and%2520Jianhua%2520Han%2520and%2520Hang%2520Xu%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520shown%250Aimpressive%2520reasoning%2520capabilities.%2520However%252C%2520further%2520enhancing%2520existing%2520MLLMs%250Anecessitates%2520high-quality%2520vision-language%2520datasets%2520with%2520carefully%2520curated%2520task%250Acomplexities%252C%2520which%2520are%2520both%2520costly%2520and%2520challenging%2520to%2520scale.%2520Although%2520recent%250Aself-improving%2520models%2520that%2520iteratively%2520refine%2520themselves%2520offer%2520a%2520feasible%250Asolution%252C%2520they%2520still%2520suffer%2520from%2520two%2520core%2520challenges%253A%2520%2528i%2529%2520most%2520existing%2520methods%250Aaugment%2520visual%2520or%2520textual%2520data%2520separately%252C%2520resulting%2520in%2520discrepancies%2520in%2520data%250Acomplexity%2520%2528e.g.%252C%2520over-simplified%2520diagrams%2520paired%2520with%2520redundant%2520textual%250Adescriptions%2529%253B%2520and%2520%2528ii%2529%2520the%2520evolution%2520of%2520data%2520and%2520models%2520is%2520also%2520separated%252C%250Aleading%2520to%2520scenarios%2520where%2520models%2520are%2520exposed%2520to%2520tasks%2520with%2520mismatched%250Adifficulty%2520levels.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520C2-Evo%252C%2520an%2520automatic%252C%250Aclosed-loop%2520self-improving%2520framework%2520that%2520jointly%2520evolves%2520both%2520training%2520data%250Aand%2520model%2520capabilities.%2520Specifically%252C%2520given%2520a%2520base%2520dataset%2520and%2520a%2520base%2520model%252C%250AC2-Evo%2520enhances%2520them%2520by%2520a%2520cross-modal%2520data%2520evolution%2520loop%2520and%2520a%2520data-model%250Aevolution%2520loop.%2520The%2520former%2520loop%2520expands%2520the%2520base%2520dataset%2520by%2520generating%2520complex%250Amultimodal%2520problems%2520that%2520combine%2520structured%2520textual%2520sub-problems%2520with%250Aiteratively%2520specified%2520geometric%2520diagrams%252C%2520while%2520the%2520latter%2520loop%2520adaptively%250Aselects%2520the%2520generated%2520problems%2520based%2520on%2520the%2520performance%2520of%2520the%2520base%2520model%252C%2520to%250Aconduct%2520supervised%2520fine-tuning%2520and%2520reinforcement%2520learning%2520alternately.%250AConsequently%252C%2520our%2520method%2520continuously%2520refines%2520its%2520model%2520and%2520training%2520data%252C%2520and%250Aconsistently%2520obtains%2520considerable%2520performance%2520gains%2520across%2520multiple%250Amathematical%2520reasoning%2520benchmarks.%2520Our%2520code%252C%2520models%252C%2520and%2520datasets%2520will%2520be%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C2-Evo%3A%20Co-Evolving%20Multimodal%20Data%20and%20Model%20for%20Self-Improving%0A%20%20Reasoning&entry.906535625=Xiuwei%20Chen%20and%20Wentao%20Hu%20and%20Hanhui%20Li%20and%20Jun%20Zhou%20and%20Zisheng%20Chen%20and%20Meng%20Cao%20and%20Yihan%20Zeng%20and%20Kui%20Zhang%20and%20Yu-Jie%20Yuan%20and%20Jianhua%20Han%20and%20Hang%20Xu%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20shown%0Aimpressive%20reasoning%20capabilities.%20However%2C%20further%20enhancing%20existing%20MLLMs%0Anecessitates%20high-quality%20vision-language%20datasets%20with%20carefully%20curated%20task%0Acomplexities%2C%20which%20are%20both%20costly%20and%20challenging%20to%20scale.%20Although%20recent%0Aself-improving%20models%20that%20iteratively%20refine%20themselves%20offer%20a%20feasible%0Asolution%2C%20they%20still%20suffer%20from%20two%20core%20challenges%3A%20%28i%29%20most%20existing%20methods%0Aaugment%20visual%20or%20textual%20data%20separately%2C%20resulting%20in%20discrepancies%20in%20data%0Acomplexity%20%28e.g.%2C%20over-simplified%20diagrams%20paired%20with%20redundant%20textual%0Adescriptions%29%3B%20and%20%28ii%29%20the%20evolution%20of%20data%20and%20models%20is%20also%20separated%2C%0Aleading%20to%20scenarios%20where%20models%20are%20exposed%20to%20tasks%20with%20mismatched%0Adifficulty%20levels.%20To%20address%20these%20issues%2C%20we%20propose%20C2-Evo%2C%20an%20automatic%2C%0Aclosed-loop%20self-improving%20framework%20that%20jointly%20evolves%20both%20training%20data%0Aand%20model%20capabilities.%20Specifically%2C%20given%20a%20base%20dataset%20and%20a%20base%20model%2C%0AC2-Evo%20enhances%20them%20by%20a%20cross-modal%20data%20evolution%20loop%20and%20a%20data-model%0Aevolution%20loop.%20The%20former%20loop%20expands%20the%20base%20dataset%20by%20generating%20complex%0Amultimodal%20problems%20that%20combine%20structured%20textual%20sub-problems%20with%0Aiteratively%20specified%20geometric%20diagrams%2C%20while%20the%20latter%20loop%20adaptively%0Aselects%20the%20generated%20problems%20based%20on%20the%20performance%20of%20the%20base%20model%2C%20to%0Aconduct%20supervised%20fine-tuning%20and%20reinforcement%20learning%20alternately.%0AConsequently%2C%20our%20method%20continuously%20refines%20its%20model%20and%20training%20data%2C%20and%0Aconsistently%20obtains%20considerable%20performance%20gains%20across%20multiple%0Amathematical%20reasoning%20benchmarks.%20Our%20code%2C%20models%2C%20and%20datasets%20will%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16518v1&entry.124074799=Read"},
{"title": "Unveiling the Potential of Segment Anything Model 2 for RGB-Thermal\n  Semantic Segmentation with Language Guidance", "author": "Jiayi Zhao and Fei Teng and Kai Luo and Guoqiang Zhao and Zhiyong Li and Xu Zheng and Kailun Yang", "abstract": "  The perception capability of robotic systems relies on the richness of the\ndataset. Although Segment Anything Model 2 (SAM2), trained on large datasets,\ndemonstrates strong perception potential in perception tasks, its inherent\ntraining paradigm prevents it from being suitable for RGB-T tasks. To address\nthese challenges, we propose SHIFNet, a novel SAM2-driven Hybrid Interaction\nParadigm that unlocks the potential of SAM2 with linguistic guidance for\nefficient RGB-Thermal perception. Our framework consists of two key components:\n(1) Semantic-Aware Cross-modal Fusion (SACF) module that dynamically balances\nmodality contributions through text-guided affinity learning, overcoming SAM2's\ninherent RGB bias; (2) Heterogeneous Prompting Decoder (HPD) that enhances\nglobal semantic information through a semantic enhancement module and then\ncombined with category embeddings to amplify cross-modal semantic consistency.\nWith 32.27M trainable parameters, SHIFNet achieves state-of-the-art\nsegmentation performance on public benchmarks, reaching 89.8% on PST900 and\n67.8% on FMB, respectively. The framework facilitates the adaptation of\npre-trained large models to RGB-T segmentation tasks, effectively mitigating\nthe high costs associated with data collection while endowing robotic systems\nwith comprehensive perception capabilities. The source code will be made\npublicly available at https://github.com/iAsakiT3T/SHIFNet.\n", "link": "http://arxiv.org/abs/2503.02581v2", "date": "2025-07-22", "relevancy": 2.2362, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5728}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5576}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Potential%20of%20Segment%20Anything%20Model%202%20for%20RGB-Thermal%0A%20%20Semantic%20Segmentation%20with%20Language%20Guidance&body=Title%3A%20Unveiling%20the%20Potential%20of%20Segment%20Anything%20Model%202%20for%20RGB-Thermal%0A%20%20Semantic%20Segmentation%20with%20Language%20Guidance%0AAuthor%3A%20Jiayi%20Zhao%20and%20Fei%20Teng%20and%20Kai%20Luo%20and%20Guoqiang%20Zhao%20and%20Zhiyong%20Li%20and%20Xu%20Zheng%20and%20Kailun%20Yang%0AAbstract%3A%20%20%20The%20perception%20capability%20of%20robotic%20systems%20relies%20on%20the%20richness%20of%20the%0Adataset.%20Although%20Segment%20Anything%20Model%202%20%28SAM2%29%2C%20trained%20on%20large%20datasets%2C%0Ademonstrates%20strong%20perception%20potential%20in%20perception%20tasks%2C%20its%20inherent%0Atraining%20paradigm%20prevents%20it%20from%20being%20suitable%20for%20RGB-T%20tasks.%20To%20address%0Athese%20challenges%2C%20we%20propose%20SHIFNet%2C%20a%20novel%20SAM2-driven%20Hybrid%20Interaction%0AParadigm%20that%20unlocks%20the%20potential%20of%20SAM2%20with%20linguistic%20guidance%20for%0Aefficient%20RGB-Thermal%20perception.%20Our%20framework%20consists%20of%20two%20key%20components%3A%0A%281%29%20Semantic-Aware%20Cross-modal%20Fusion%20%28SACF%29%20module%20that%20dynamically%20balances%0Amodality%20contributions%20through%20text-guided%20affinity%20learning%2C%20overcoming%20SAM2%27s%0Ainherent%20RGB%20bias%3B%20%282%29%20Heterogeneous%20Prompting%20Decoder%20%28HPD%29%20that%20enhances%0Aglobal%20semantic%20information%20through%20a%20semantic%20enhancement%20module%20and%20then%0Acombined%20with%20category%20embeddings%20to%20amplify%20cross-modal%20semantic%20consistency.%0AWith%2032.27M%20trainable%20parameters%2C%20SHIFNet%20achieves%20state-of-the-art%0Asegmentation%20performance%20on%20public%20benchmarks%2C%20reaching%2089.8%25%20on%20PST900%20and%0A67.8%25%20on%20FMB%2C%20respectively.%20The%20framework%20facilitates%20the%20adaptation%20of%0Apre-trained%20large%20models%20to%20RGB-T%20segmentation%20tasks%2C%20effectively%20mitigating%0Athe%20high%20costs%20associated%20with%20data%20collection%20while%20endowing%20robotic%20systems%0Awith%20comprehensive%20perception%20capabilities.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20at%20https%3A//github.com/iAsakiT3T/SHIFNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.02581v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Potential%2520of%2520Segment%2520Anything%2520Model%25202%2520for%2520RGB-Thermal%250A%2520%2520Semantic%2520Segmentation%2520with%2520Language%2520Guidance%26entry.906535625%3DJiayi%2520Zhao%2520and%2520Fei%2520Teng%2520and%2520Kai%2520Luo%2520and%2520Guoqiang%2520Zhao%2520and%2520Zhiyong%2520Li%2520and%2520Xu%2520Zheng%2520and%2520Kailun%2520Yang%26entry.1292438233%3D%2520%2520The%2520perception%2520capability%2520of%2520robotic%2520systems%2520relies%2520on%2520the%2520richness%2520of%2520the%250Adataset.%2520Although%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%252C%2520trained%2520on%2520large%2520datasets%252C%250Ademonstrates%2520strong%2520perception%2520potential%2520in%2520perception%2520tasks%252C%2520its%2520inherent%250Atraining%2520paradigm%2520prevents%2520it%2520from%2520being%2520suitable%2520for%2520RGB-T%2520tasks.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520SHIFNet%252C%2520a%2520novel%2520SAM2-driven%2520Hybrid%2520Interaction%250AParadigm%2520that%2520unlocks%2520the%2520potential%2520of%2520SAM2%2520with%2520linguistic%2520guidance%2520for%250Aefficient%2520RGB-Thermal%2520perception.%2520Our%2520framework%2520consists%2520of%2520two%2520key%2520components%253A%250A%25281%2529%2520Semantic-Aware%2520Cross-modal%2520Fusion%2520%2528SACF%2529%2520module%2520that%2520dynamically%2520balances%250Amodality%2520contributions%2520through%2520text-guided%2520affinity%2520learning%252C%2520overcoming%2520SAM2%2527s%250Ainherent%2520RGB%2520bias%253B%2520%25282%2529%2520Heterogeneous%2520Prompting%2520Decoder%2520%2528HPD%2529%2520that%2520enhances%250Aglobal%2520semantic%2520information%2520through%2520a%2520semantic%2520enhancement%2520module%2520and%2520then%250Acombined%2520with%2520category%2520embeddings%2520to%2520amplify%2520cross-modal%2520semantic%2520consistency.%250AWith%252032.27M%2520trainable%2520parameters%252C%2520SHIFNet%2520achieves%2520state-of-the-art%250Asegmentation%2520performance%2520on%2520public%2520benchmarks%252C%2520reaching%252089.8%2525%2520on%2520PST900%2520and%250A67.8%2525%2520on%2520FMB%252C%2520respectively.%2520The%2520framework%2520facilitates%2520the%2520adaptation%2520of%250Apre-trained%2520large%2520models%2520to%2520RGB-T%2520segmentation%2520tasks%252C%2520effectively%2520mitigating%250Athe%2520high%2520costs%2520associated%2520with%2520data%2520collection%2520while%2520endowing%2520robotic%2520systems%250Awith%2520comprehensive%2520perception%2520capabilities.%2520The%2520source%2520code%2520will%2520be%2520made%250Apublicly%2520available%2520at%2520https%253A//github.com/iAsakiT3T/SHIFNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.02581v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Potential%20of%20Segment%20Anything%20Model%202%20for%20RGB-Thermal%0A%20%20Semantic%20Segmentation%20with%20Language%20Guidance&entry.906535625=Jiayi%20Zhao%20and%20Fei%20Teng%20and%20Kai%20Luo%20and%20Guoqiang%20Zhao%20and%20Zhiyong%20Li%20and%20Xu%20Zheng%20and%20Kailun%20Yang&entry.1292438233=%20%20The%20perception%20capability%20of%20robotic%20systems%20relies%20on%20the%20richness%20of%20the%0Adataset.%20Although%20Segment%20Anything%20Model%202%20%28SAM2%29%2C%20trained%20on%20large%20datasets%2C%0Ademonstrates%20strong%20perception%20potential%20in%20perception%20tasks%2C%20its%20inherent%0Atraining%20paradigm%20prevents%20it%20from%20being%20suitable%20for%20RGB-T%20tasks.%20To%20address%0Athese%20challenges%2C%20we%20propose%20SHIFNet%2C%20a%20novel%20SAM2-driven%20Hybrid%20Interaction%0AParadigm%20that%20unlocks%20the%20potential%20of%20SAM2%20with%20linguistic%20guidance%20for%0Aefficient%20RGB-Thermal%20perception.%20Our%20framework%20consists%20of%20two%20key%20components%3A%0A%281%29%20Semantic-Aware%20Cross-modal%20Fusion%20%28SACF%29%20module%20that%20dynamically%20balances%0Amodality%20contributions%20through%20text-guided%20affinity%20learning%2C%20overcoming%20SAM2%27s%0Ainherent%20RGB%20bias%3B%20%282%29%20Heterogeneous%20Prompting%20Decoder%20%28HPD%29%20that%20enhances%0Aglobal%20semantic%20information%20through%20a%20semantic%20enhancement%20module%20and%20then%0Acombined%20with%20category%20embeddings%20to%20amplify%20cross-modal%20semantic%20consistency.%0AWith%2032.27M%20trainable%20parameters%2C%20SHIFNet%20achieves%20state-of-the-art%0Asegmentation%20performance%20on%20public%20benchmarks%2C%20reaching%2089.8%25%20on%20PST900%20and%0A67.8%25%20on%20FMB%2C%20respectively.%20The%20framework%20facilitates%20the%20adaptation%20of%0Apre-trained%20large%20models%20to%20RGB-T%20segmentation%20tasks%2C%20effectively%20mitigating%0Athe%20high%20costs%20associated%20with%20data%20collection%20while%20endowing%20robotic%20systems%0Awith%20comprehensive%20perception%20capabilities.%20The%20source%20code%20will%20be%20made%0Apublicly%20available%20at%20https%3A//github.com/iAsakiT3T/SHIFNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.02581v2&entry.124074799=Read"},
{"title": "Conformal Predictions for Human Action Recognition with Vision-Language\n  Models", "author": "Bary Tim and Fuchs Cl\u00e9ment and Macq Beno\u00eet", "abstract": "  Human-in-the-Loop (HITL) systems are essential in high-stakes, real-world\napplications where AI must collaborate with human decision-makers. This work\ninvestigates how Conformal Prediction (CP) techniques, which provide rigorous\ncoverage guarantees, can enhance the reliability of state-of-the-art human\naction recognition (HAR) systems built upon Vision-Language Models (VLMs). We\ndemonstrate that CP can significantly reduce the average number of candidate\nclasses without modifying the underlying VLM. However, these reductions often\nresult in distributions with long tails which can hinder their practical\nutility. To mitigate this, we propose tuning the temperature of the softmax\nprediction, without using additional calibration data. This work contributes to\nongoing efforts for multi-modal human-AI interaction in dynamic real-world\nenvironments.\n", "link": "http://arxiv.org/abs/2502.06631v2", "date": "2025-07-22", "relevancy": 2.2224, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5779}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5542}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20Predictions%20for%20Human%20Action%20Recognition%20with%20Vision-Language%0A%20%20Models&body=Title%3A%20Conformal%20Predictions%20for%20Human%20Action%20Recognition%20with%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Bary%20Tim%20and%20Fuchs%20Cl%C3%A9ment%20and%20Macq%20Beno%C3%AEt%0AAbstract%3A%20%20%20Human-in-the-Loop%20%28HITL%29%20systems%20are%20essential%20in%20high-stakes%2C%20real-world%0Aapplications%20where%20AI%20must%20collaborate%20with%20human%20decision-makers.%20This%20work%0Ainvestigates%20how%20Conformal%20Prediction%20%28CP%29%20techniques%2C%20which%20provide%20rigorous%0Acoverage%20guarantees%2C%20can%20enhance%20the%20reliability%20of%20state-of-the-art%20human%0Aaction%20recognition%20%28HAR%29%20systems%20built%20upon%20Vision-Language%20Models%20%28VLMs%29.%20We%0Ademonstrate%20that%20CP%20can%20significantly%20reduce%20the%20average%20number%20of%20candidate%0Aclasses%20without%20modifying%20the%20underlying%20VLM.%20However%2C%20these%20reductions%20often%0Aresult%20in%20distributions%20with%20long%20tails%20which%20can%20hinder%20their%20practical%0Autility.%20To%20mitigate%20this%2C%20we%20propose%20tuning%20the%20temperature%20of%20the%20softmax%0Aprediction%2C%20without%20using%20additional%20calibration%20data.%20This%20work%20contributes%20to%0Aongoing%20efforts%20for%20multi-modal%20human-AI%20interaction%20in%20dynamic%20real-world%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.06631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520Predictions%2520for%2520Human%2520Action%2520Recognition%2520with%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DBary%2520Tim%2520and%2520Fuchs%2520Cl%25C3%25A9ment%2520and%2520Macq%2520Beno%25C3%25AEt%26entry.1292438233%3D%2520%2520Human-in-the-Loop%2520%2528HITL%2529%2520systems%2520are%2520essential%2520in%2520high-stakes%252C%2520real-world%250Aapplications%2520where%2520AI%2520must%2520collaborate%2520with%2520human%2520decision-makers.%2520This%2520work%250Ainvestigates%2520how%2520Conformal%2520Prediction%2520%2528CP%2529%2520techniques%252C%2520which%2520provide%2520rigorous%250Acoverage%2520guarantees%252C%2520can%2520enhance%2520the%2520reliability%2520of%2520state-of-the-art%2520human%250Aaction%2520recognition%2520%2528HAR%2529%2520systems%2520built%2520upon%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520We%250Ademonstrate%2520that%2520CP%2520can%2520significantly%2520reduce%2520the%2520average%2520number%2520of%2520candidate%250Aclasses%2520without%2520modifying%2520the%2520underlying%2520VLM.%2520However%252C%2520these%2520reductions%2520often%250Aresult%2520in%2520distributions%2520with%2520long%2520tails%2520which%2520can%2520hinder%2520their%2520practical%250Autility.%2520To%2520mitigate%2520this%252C%2520we%2520propose%2520tuning%2520the%2520temperature%2520of%2520the%2520softmax%250Aprediction%252C%2520without%2520using%2520additional%2520calibration%2520data.%2520This%2520work%2520contributes%2520to%250Aongoing%2520efforts%2520for%2520multi-modal%2520human-AI%2520interaction%2520in%2520dynamic%2520real-world%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.06631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20Predictions%20for%20Human%20Action%20Recognition%20with%20Vision-Language%0A%20%20Models&entry.906535625=Bary%20Tim%20and%20Fuchs%20Cl%C3%A9ment%20and%20Macq%20Beno%C3%AEt&entry.1292438233=%20%20Human-in-the-Loop%20%28HITL%29%20systems%20are%20essential%20in%20high-stakes%2C%20real-world%0Aapplications%20where%20AI%20must%20collaborate%20with%20human%20decision-makers.%20This%20work%0Ainvestigates%20how%20Conformal%20Prediction%20%28CP%29%20techniques%2C%20which%20provide%20rigorous%0Acoverage%20guarantees%2C%20can%20enhance%20the%20reliability%20of%20state-of-the-art%20human%0Aaction%20recognition%20%28HAR%29%20systems%20built%20upon%20Vision-Language%20Models%20%28VLMs%29.%20We%0Ademonstrate%20that%20CP%20can%20significantly%20reduce%20the%20average%20number%20of%20candidate%0Aclasses%20without%20modifying%20the%20underlying%20VLM.%20However%2C%20these%20reductions%20often%0Aresult%20in%20distributions%20with%20long%20tails%20which%20can%20hinder%20their%20practical%0Autility.%20To%20mitigate%20this%2C%20we%20propose%20tuning%20the%20temperature%20of%20the%20softmax%0Aprediction%2C%20without%20using%20additional%20calibration%20data.%20This%20work%20contributes%20to%0Aongoing%20efforts%20for%20multi-modal%20human-AI%20interaction%20in%20dynamic%20real-world%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.06631v2&entry.124074799=Read"},
{"title": "Temporally-Constrained Video Reasoning Segmentation and Automated\n  Benchmark Construction", "author": "Yiqing Shen and Chenjia Li and Chenxiao Fan and Mathias Unberath", "abstract": "  Conventional approaches to video segmentation are confined to predefined\nobject categories and cannot identify out-of-vocabulary objects, let alone\nobjects that are not identified explicitly but only referred to implicitly in\ncomplex text queries. This shortcoming limits the utility for video\nsegmentation in complex and variable scenarios, where a closed set of object\ncategories is difficult to define and where users may not know the exact object\ncategory that will appear in the video. Such scenarios can arise in operating\nroom video analysis, where different health systems may use different workflows\nand instrumentation, requiring flexible solutions for video analysis. Reasoning\nsegmentation (RS) now offers promise towards such a solution, enabling natural\nlanguage text queries as interaction for identifying object to segment.\nHowever, existing video RS formulation assume that target objects remain\ncontextually relevant throughout entire video sequences. This assumption is\ninadequate for real-world scenarios in which objects of interest appear,\ndisappear or change relevance dynamically based on temporal context, such as\nsurgical instruments that become relevant only during specific procedural\nphases or anatomical structures that gain importance at particular moments\nduring surgery. Our first contribution is the introduction of\ntemporally-constrained video reasoning segmentation, a novel task formulation\nthat requires models to implicitly infer when target objects become\ncontextually relevant based on text queries that incorporate temporal\nreasoning. Since manual annotation of temporally-constrained video RS datasets\nwould be expensive and limit scalability, our second contribution is an\ninnovative automated benchmark construction method. Finally, we present\nTCVideoRSBenchmark, a temporally-constrained video RS dataset containing 52\nsamples using the videos from the MVOR dataset.\n", "link": "http://arxiv.org/abs/2507.16718v1", "date": "2025-07-22", "relevancy": 2.2193, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5552}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporally-Constrained%20Video%20Reasoning%20Segmentation%20and%20Automated%0A%20%20Benchmark%20Construction&body=Title%3A%20Temporally-Constrained%20Video%20Reasoning%20Segmentation%20and%20Automated%0A%20%20Benchmark%20Construction%0AAuthor%3A%20Yiqing%20Shen%20and%20Chenjia%20Li%20and%20Chenxiao%20Fan%20and%20Mathias%20Unberath%0AAbstract%3A%20%20%20Conventional%20approaches%20to%20video%20segmentation%20are%20confined%20to%20predefined%0Aobject%20categories%20and%20cannot%20identify%20out-of-vocabulary%20objects%2C%20let%20alone%0Aobjects%20that%20are%20not%20identified%20explicitly%20but%20only%20referred%20to%20implicitly%20in%0Acomplex%20text%20queries.%20This%20shortcoming%20limits%20the%20utility%20for%20video%0Asegmentation%20in%20complex%20and%20variable%20scenarios%2C%20where%20a%20closed%20set%20of%20object%0Acategories%20is%20difficult%20to%20define%20and%20where%20users%20may%20not%20know%20the%20exact%20object%0Acategory%20that%20will%20appear%20in%20the%20video.%20Such%20scenarios%20can%20arise%20in%20operating%0Aroom%20video%20analysis%2C%20where%20different%20health%20systems%20may%20use%20different%20workflows%0Aand%20instrumentation%2C%20requiring%20flexible%20solutions%20for%20video%20analysis.%20Reasoning%0Asegmentation%20%28RS%29%20now%20offers%20promise%20towards%20such%20a%20solution%2C%20enabling%20natural%0Alanguage%20text%20queries%20as%20interaction%20for%20identifying%20object%20to%20segment.%0AHowever%2C%20existing%20video%20RS%20formulation%20assume%20that%20target%20objects%20remain%0Acontextually%20relevant%20throughout%20entire%20video%20sequences.%20This%20assumption%20is%0Ainadequate%20for%20real-world%20scenarios%20in%20which%20objects%20of%20interest%20appear%2C%0Adisappear%20or%20change%20relevance%20dynamically%20based%20on%20temporal%20context%2C%20such%20as%0Asurgical%20instruments%20that%20become%20relevant%20only%20during%20specific%20procedural%0Aphases%20or%20anatomical%20structures%20that%20gain%20importance%20at%20particular%20moments%0Aduring%20surgery.%20Our%20first%20contribution%20is%20the%20introduction%20of%0Atemporally-constrained%20video%20reasoning%20segmentation%2C%20a%20novel%20task%20formulation%0Athat%20requires%20models%20to%20implicitly%20infer%20when%20target%20objects%20become%0Acontextually%20relevant%20based%20on%20text%20queries%20that%20incorporate%20temporal%0Areasoning.%20Since%20manual%20annotation%20of%20temporally-constrained%20video%20RS%20datasets%0Awould%20be%20expensive%20and%20limit%20scalability%2C%20our%20second%20contribution%20is%20an%0Ainnovative%20automated%20benchmark%20construction%20method.%20Finally%2C%20we%20present%0ATCVideoRSBenchmark%2C%20a%20temporally-constrained%20video%20RS%20dataset%20containing%2052%0Asamples%20using%20the%20videos%20from%20the%20MVOR%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporally-Constrained%2520Video%2520Reasoning%2520Segmentation%2520and%2520Automated%250A%2520%2520Benchmark%2520Construction%26entry.906535625%3DYiqing%2520Shen%2520and%2520Chenjia%2520Li%2520and%2520Chenxiao%2520Fan%2520and%2520Mathias%2520Unberath%26entry.1292438233%3D%2520%2520Conventional%2520approaches%2520to%2520video%2520segmentation%2520are%2520confined%2520to%2520predefined%250Aobject%2520categories%2520and%2520cannot%2520identify%2520out-of-vocabulary%2520objects%252C%2520let%2520alone%250Aobjects%2520that%2520are%2520not%2520identified%2520explicitly%2520but%2520only%2520referred%2520to%2520implicitly%2520in%250Acomplex%2520text%2520queries.%2520This%2520shortcoming%2520limits%2520the%2520utility%2520for%2520video%250Asegmentation%2520in%2520complex%2520and%2520variable%2520scenarios%252C%2520where%2520a%2520closed%2520set%2520of%2520object%250Acategories%2520is%2520difficult%2520to%2520define%2520and%2520where%2520users%2520may%2520not%2520know%2520the%2520exact%2520object%250Acategory%2520that%2520will%2520appear%2520in%2520the%2520video.%2520Such%2520scenarios%2520can%2520arise%2520in%2520operating%250Aroom%2520video%2520analysis%252C%2520where%2520different%2520health%2520systems%2520may%2520use%2520different%2520workflows%250Aand%2520instrumentation%252C%2520requiring%2520flexible%2520solutions%2520for%2520video%2520analysis.%2520Reasoning%250Asegmentation%2520%2528RS%2529%2520now%2520offers%2520promise%2520towards%2520such%2520a%2520solution%252C%2520enabling%2520natural%250Alanguage%2520text%2520queries%2520as%2520interaction%2520for%2520identifying%2520object%2520to%2520segment.%250AHowever%252C%2520existing%2520video%2520RS%2520formulation%2520assume%2520that%2520target%2520objects%2520remain%250Acontextually%2520relevant%2520throughout%2520entire%2520video%2520sequences.%2520This%2520assumption%2520is%250Ainadequate%2520for%2520real-world%2520scenarios%2520in%2520which%2520objects%2520of%2520interest%2520appear%252C%250Adisappear%2520or%2520change%2520relevance%2520dynamically%2520based%2520on%2520temporal%2520context%252C%2520such%2520as%250Asurgical%2520instruments%2520that%2520become%2520relevant%2520only%2520during%2520specific%2520procedural%250Aphases%2520or%2520anatomical%2520structures%2520that%2520gain%2520importance%2520at%2520particular%2520moments%250Aduring%2520surgery.%2520Our%2520first%2520contribution%2520is%2520the%2520introduction%2520of%250Atemporally-constrained%2520video%2520reasoning%2520segmentation%252C%2520a%2520novel%2520task%2520formulation%250Athat%2520requires%2520models%2520to%2520implicitly%2520infer%2520when%2520target%2520objects%2520become%250Acontextually%2520relevant%2520based%2520on%2520text%2520queries%2520that%2520incorporate%2520temporal%250Areasoning.%2520Since%2520manual%2520annotation%2520of%2520temporally-constrained%2520video%2520RS%2520datasets%250Awould%2520be%2520expensive%2520and%2520limit%2520scalability%252C%2520our%2520second%2520contribution%2520is%2520an%250Ainnovative%2520automated%2520benchmark%2520construction%2520method.%2520Finally%252C%2520we%2520present%250ATCVideoRSBenchmark%252C%2520a%2520temporally-constrained%2520video%2520RS%2520dataset%2520containing%252052%250Asamples%2520using%2520the%2520videos%2520from%2520the%2520MVOR%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporally-Constrained%20Video%20Reasoning%20Segmentation%20and%20Automated%0A%20%20Benchmark%20Construction&entry.906535625=Yiqing%20Shen%20and%20Chenjia%20Li%20and%20Chenxiao%20Fan%20and%20Mathias%20Unberath&entry.1292438233=%20%20Conventional%20approaches%20to%20video%20segmentation%20are%20confined%20to%20predefined%0Aobject%20categories%20and%20cannot%20identify%20out-of-vocabulary%20objects%2C%20let%20alone%0Aobjects%20that%20are%20not%20identified%20explicitly%20but%20only%20referred%20to%20implicitly%20in%0Acomplex%20text%20queries.%20This%20shortcoming%20limits%20the%20utility%20for%20video%0Asegmentation%20in%20complex%20and%20variable%20scenarios%2C%20where%20a%20closed%20set%20of%20object%0Acategories%20is%20difficult%20to%20define%20and%20where%20users%20may%20not%20know%20the%20exact%20object%0Acategory%20that%20will%20appear%20in%20the%20video.%20Such%20scenarios%20can%20arise%20in%20operating%0Aroom%20video%20analysis%2C%20where%20different%20health%20systems%20may%20use%20different%20workflows%0Aand%20instrumentation%2C%20requiring%20flexible%20solutions%20for%20video%20analysis.%20Reasoning%0Asegmentation%20%28RS%29%20now%20offers%20promise%20towards%20such%20a%20solution%2C%20enabling%20natural%0Alanguage%20text%20queries%20as%20interaction%20for%20identifying%20object%20to%20segment.%0AHowever%2C%20existing%20video%20RS%20formulation%20assume%20that%20target%20objects%20remain%0Acontextually%20relevant%20throughout%20entire%20video%20sequences.%20This%20assumption%20is%0Ainadequate%20for%20real-world%20scenarios%20in%20which%20objects%20of%20interest%20appear%2C%0Adisappear%20or%20change%20relevance%20dynamically%20based%20on%20temporal%20context%2C%20such%20as%0Asurgical%20instruments%20that%20become%20relevant%20only%20during%20specific%20procedural%0Aphases%20or%20anatomical%20structures%20that%20gain%20importance%20at%20particular%20moments%0Aduring%20surgery.%20Our%20first%20contribution%20is%20the%20introduction%20of%0Atemporally-constrained%20video%20reasoning%20segmentation%2C%20a%20novel%20task%20formulation%0Athat%20requires%20models%20to%20implicitly%20infer%20when%20target%20objects%20become%0Acontextually%20relevant%20based%20on%20text%20queries%20that%20incorporate%20temporal%0Areasoning.%20Since%20manual%20annotation%20of%20temporally-constrained%20video%20RS%20datasets%0Awould%20be%20expensive%20and%20limit%20scalability%2C%20our%20second%20contribution%20is%20an%0Ainnovative%20automated%20benchmark%20construction%20method.%20Finally%2C%20we%20present%0ATCVideoRSBenchmark%2C%20a%20temporally-constrained%20video%20RS%20dataset%20containing%2052%0Asamples%20using%20the%20videos%20from%20the%20MVOR%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16718v1&entry.124074799=Read"},
{"title": "MCP-MedSAM: A Powerful Lightweight Medical Segment Anything Model\n  Trained with a Single GPU in Just One Day", "author": "Donghang Lyu and Ruochen Gao and Marius Staring", "abstract": "  Medical image segmentation involves partitioning medical images into\nmeaningful regions, with a focus on identifying anatomical structures and\nlesions. It has broad applications in healthcare, and deep learning methods\nhave enabled significant advancements in automating this process. Recently, the\nintroduction of the Segmentation Anything Model (SAM), the first foundation\nmodel for segmentation task, has prompted researchers to adapt it for the\nmedical domain to improve performance across various tasks. However, SAM's\nlarge model size and high GPU requirements hinder its scalability and\ndevelopment in the medical domain. In this work, we propose MCP-MedSAM, a\npowerful and lightweight medical SAM model designed to be trainable on a single\nA100 GPU with 40GB of memory within one day while delivering superior\nsegmentation performance. Recognizing the significant internal differences\nbetween modalities and the need for direct segmentation target information\nwithin bounding boxes, we introduce two kinds of prompts: the modality prompt\nand the content prompt. After passing through the prompt encoder, their\nembedding representations can further improve the segmentation performance by\nincorporating more relevant information without adding significant training\noverhead. Additionally, we adopt an effective modality-based data sampling\nstrategy to address data imbalance between modalities, ensuring more balanced\nperformance across all modalities. Our method was trained and evaluated using a\nlarge-scale challenge dataset, compared to top-ranking methods on the challenge\nleaderboard, MCP-MedSAM achieved superior performance while requiring only one\nday of training on a single GPU. The code is publicly available at\n\\textcolor{blue}{https://github.com/dong845/MCP-MedSAM}.}\n", "link": "http://arxiv.org/abs/2412.05888v3", "date": "2025-07-22", "relevancy": 2.2033, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5682}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5582}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MCP-MedSAM%3A%20A%20Powerful%20Lightweight%20Medical%20Segment%20Anything%20Model%0A%20%20Trained%20with%20a%20Single%20GPU%20in%20Just%20One%20Day&body=Title%3A%20MCP-MedSAM%3A%20A%20Powerful%20Lightweight%20Medical%20Segment%20Anything%20Model%0A%20%20Trained%20with%20a%20Single%20GPU%20in%20Just%20One%20Day%0AAuthor%3A%20Donghang%20Lyu%20and%20Ruochen%20Gao%20and%20Marius%20Staring%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20involves%20partitioning%20medical%20images%20into%0Ameaningful%20regions%2C%20with%20a%20focus%20on%20identifying%20anatomical%20structures%20and%0Alesions.%20It%20has%20broad%20applications%20in%20healthcare%2C%20and%20deep%20learning%20methods%0Ahave%20enabled%20significant%20advancements%20in%20automating%20this%20process.%20Recently%2C%20the%0Aintroduction%20of%20the%20Segmentation%20Anything%20Model%20%28SAM%29%2C%20the%20first%20foundation%0Amodel%20for%20segmentation%20task%2C%20has%20prompted%20researchers%20to%20adapt%20it%20for%20the%0Amedical%20domain%20to%20improve%20performance%20across%20various%20tasks.%20However%2C%20SAM%27s%0Alarge%20model%20size%20and%20high%20GPU%20requirements%20hinder%20its%20scalability%20and%0Adevelopment%20in%20the%20medical%20domain.%20In%20this%20work%2C%20we%20propose%20MCP-MedSAM%2C%20a%0Apowerful%20and%20lightweight%20medical%20SAM%20model%20designed%20to%20be%20trainable%20on%20a%20single%0AA100%20GPU%20with%2040GB%20of%20memory%20within%20one%20day%20while%20delivering%20superior%0Asegmentation%20performance.%20Recognizing%20the%20significant%20internal%20differences%0Abetween%20modalities%20and%20the%20need%20for%20direct%20segmentation%20target%20information%0Awithin%20bounding%20boxes%2C%20we%20introduce%20two%20kinds%20of%20prompts%3A%20the%20modality%20prompt%0Aand%20the%20content%20prompt.%20After%20passing%20through%20the%20prompt%20encoder%2C%20their%0Aembedding%20representations%20can%20further%20improve%20the%20segmentation%20performance%20by%0Aincorporating%20more%20relevant%20information%20without%20adding%20significant%20training%0Aoverhead.%20Additionally%2C%20we%20adopt%20an%20effective%20modality-based%20data%20sampling%0Astrategy%20to%20address%20data%20imbalance%20between%20modalities%2C%20ensuring%20more%20balanced%0Aperformance%20across%20all%20modalities.%20Our%20method%20was%20trained%20and%20evaluated%20using%20a%0Alarge-scale%20challenge%20dataset%2C%20compared%20to%20top-ranking%20methods%20on%20the%20challenge%0Aleaderboard%2C%20MCP-MedSAM%20achieved%20superior%20performance%20while%20requiring%20only%20one%0Aday%20of%20training%20on%20a%20single%20GPU.%20The%20code%20is%20publicly%20available%20at%0A%5Ctextcolor%7Bblue%7D%7Bhttps%3A//github.com/dong845/MCP-MedSAM%7D.%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.05888v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMCP-MedSAM%253A%2520A%2520Powerful%2520Lightweight%2520Medical%2520Segment%2520Anything%2520Model%250A%2520%2520Trained%2520with%2520a%2520Single%2520GPU%2520in%2520Just%2520One%2520Day%26entry.906535625%3DDonghang%2520Lyu%2520and%2520Ruochen%2520Gao%2520and%2520Marius%2520Staring%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520involves%2520partitioning%2520medical%2520images%2520into%250Ameaningful%2520regions%252C%2520with%2520a%2520focus%2520on%2520identifying%2520anatomical%2520structures%2520and%250Alesions.%2520It%2520has%2520broad%2520applications%2520in%2520healthcare%252C%2520and%2520deep%2520learning%2520methods%250Ahave%2520enabled%2520significant%2520advancements%2520in%2520automating%2520this%2520process.%2520Recently%252C%2520the%250Aintroduction%2520of%2520the%2520Segmentation%2520Anything%2520Model%2520%2528SAM%2529%252C%2520the%2520first%2520foundation%250Amodel%2520for%2520segmentation%2520task%252C%2520has%2520prompted%2520researchers%2520to%2520adapt%2520it%2520for%2520the%250Amedical%2520domain%2520to%2520improve%2520performance%2520across%2520various%2520tasks.%2520However%252C%2520SAM%2527s%250Alarge%2520model%2520size%2520and%2520high%2520GPU%2520requirements%2520hinder%2520its%2520scalability%2520and%250Adevelopment%2520in%2520the%2520medical%2520domain.%2520In%2520this%2520work%252C%2520we%2520propose%2520MCP-MedSAM%252C%2520a%250Apowerful%2520and%2520lightweight%2520medical%2520SAM%2520model%2520designed%2520to%2520be%2520trainable%2520on%2520a%2520single%250AA100%2520GPU%2520with%252040GB%2520of%2520memory%2520within%2520one%2520day%2520while%2520delivering%2520superior%250Asegmentation%2520performance.%2520Recognizing%2520the%2520significant%2520internal%2520differences%250Abetween%2520modalities%2520and%2520the%2520need%2520for%2520direct%2520segmentation%2520target%2520information%250Awithin%2520bounding%2520boxes%252C%2520we%2520introduce%2520two%2520kinds%2520of%2520prompts%253A%2520the%2520modality%2520prompt%250Aand%2520the%2520content%2520prompt.%2520After%2520passing%2520through%2520the%2520prompt%2520encoder%252C%2520their%250Aembedding%2520representations%2520can%2520further%2520improve%2520the%2520segmentation%2520performance%2520by%250Aincorporating%2520more%2520relevant%2520information%2520without%2520adding%2520significant%2520training%250Aoverhead.%2520Additionally%252C%2520we%2520adopt%2520an%2520effective%2520modality-based%2520data%2520sampling%250Astrategy%2520to%2520address%2520data%2520imbalance%2520between%2520modalities%252C%2520ensuring%2520more%2520balanced%250Aperformance%2520across%2520all%2520modalities.%2520Our%2520method%2520was%2520trained%2520and%2520evaluated%2520using%2520a%250Alarge-scale%2520challenge%2520dataset%252C%2520compared%2520to%2520top-ranking%2520methods%2520on%2520the%2520challenge%250Aleaderboard%252C%2520MCP-MedSAM%2520achieved%2520superior%2520performance%2520while%2520requiring%2520only%2520one%250Aday%2520of%2520training%2520on%2520a%2520single%2520GPU.%2520The%2520code%2520is%2520publicly%2520available%2520at%250A%255Ctextcolor%257Bblue%257D%257Bhttps%253A//github.com/dong845/MCP-MedSAM%257D.%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.05888v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MCP-MedSAM%3A%20A%20Powerful%20Lightweight%20Medical%20Segment%20Anything%20Model%0A%20%20Trained%20with%20a%20Single%20GPU%20in%20Just%20One%20Day&entry.906535625=Donghang%20Lyu%20and%20Ruochen%20Gao%20and%20Marius%20Staring&entry.1292438233=%20%20Medical%20image%20segmentation%20involves%20partitioning%20medical%20images%20into%0Ameaningful%20regions%2C%20with%20a%20focus%20on%20identifying%20anatomical%20structures%20and%0Alesions.%20It%20has%20broad%20applications%20in%20healthcare%2C%20and%20deep%20learning%20methods%0Ahave%20enabled%20significant%20advancements%20in%20automating%20this%20process.%20Recently%2C%20the%0Aintroduction%20of%20the%20Segmentation%20Anything%20Model%20%28SAM%29%2C%20the%20first%20foundation%0Amodel%20for%20segmentation%20task%2C%20has%20prompted%20researchers%20to%20adapt%20it%20for%20the%0Amedical%20domain%20to%20improve%20performance%20across%20various%20tasks.%20However%2C%20SAM%27s%0Alarge%20model%20size%20and%20high%20GPU%20requirements%20hinder%20its%20scalability%20and%0Adevelopment%20in%20the%20medical%20domain.%20In%20this%20work%2C%20we%20propose%20MCP-MedSAM%2C%20a%0Apowerful%20and%20lightweight%20medical%20SAM%20model%20designed%20to%20be%20trainable%20on%20a%20single%0AA100%20GPU%20with%2040GB%20of%20memory%20within%20one%20day%20while%20delivering%20superior%0Asegmentation%20performance.%20Recognizing%20the%20significant%20internal%20differences%0Abetween%20modalities%20and%20the%20need%20for%20direct%20segmentation%20target%20information%0Awithin%20bounding%20boxes%2C%20we%20introduce%20two%20kinds%20of%20prompts%3A%20the%20modality%20prompt%0Aand%20the%20content%20prompt.%20After%20passing%20through%20the%20prompt%20encoder%2C%20their%0Aembedding%20representations%20can%20further%20improve%20the%20segmentation%20performance%20by%0Aincorporating%20more%20relevant%20information%20without%20adding%20significant%20training%0Aoverhead.%20Additionally%2C%20we%20adopt%20an%20effective%20modality-based%20data%20sampling%0Astrategy%20to%20address%20data%20imbalance%20between%20modalities%2C%20ensuring%20more%20balanced%0Aperformance%20across%20all%20modalities.%20Our%20method%20was%20trained%20and%20evaluated%20using%20a%0Alarge-scale%20challenge%20dataset%2C%20compared%20to%20top-ranking%20methods%20on%20the%20challenge%0Aleaderboard%2C%20MCP-MedSAM%20achieved%20superior%20performance%20while%20requiring%20only%20one%0Aday%20of%20training%20on%20a%20single%20GPU.%20The%20code%20is%20publicly%20available%20at%0A%5Ctextcolor%7Bblue%7D%7Bhttps%3A//github.com/dong845/MCP-MedSAM%7D.%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.05888v3&entry.124074799=Read"},
{"title": "Enhancing Domain Diversity in Synthetic Data Face Recognition with\n  Dataset Fusion", "author": "Anjith George and Sebastien Marcel", "abstract": "  While the accuracy of face recognition systems has improved significantly in\nrecent years, the datasets used to train these models are often collected\nthrough web crawling without the explicit consent of users, raising ethical and\nprivacy concerns. To address this, many recent approaches have explored the use\nof synthetic data for training face recognition models. However, these models\ntypically underperform compared to those trained on real-world data. A common\nlimitation is that a single generator model is often used to create the entire\nsynthetic dataset, leading to model-specific artifacts that may cause\noverfitting to the generator's inherent biases and artifacts. In this work, we\npropose a solution by combining two state-of-the-art synthetic face datasets\ngenerated using architecturally distinct backbones. This fusion reduces\nmodel-specific artifacts, enhances diversity in pose, lighting, and\ndemographics, and implicitly regularizes the face recognition model by\nemphasizing identity-relevant features. We evaluate the performance of models\ntrained on this combined dataset using standard face recognition benchmarks and\ndemonstrate that our approach achieves superior performance across many of\nthese benchmarks.\n", "link": "http://arxiv.org/abs/2507.16790v1", "date": "2025-07-22", "relevancy": 2.2018, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.562}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5483}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Domain%20Diversity%20in%20Synthetic%20Data%20Face%20Recognition%20with%0A%20%20Dataset%20Fusion&body=Title%3A%20Enhancing%20Domain%20Diversity%20in%20Synthetic%20Data%20Face%20Recognition%20with%0A%20%20Dataset%20Fusion%0AAuthor%3A%20Anjith%20George%20and%20Sebastien%20Marcel%0AAbstract%3A%20%20%20While%20the%20accuracy%20of%20face%20recognition%20systems%20has%20improved%20significantly%20in%0Arecent%20years%2C%20the%20datasets%20used%20to%20train%20these%20models%20are%20often%20collected%0Athrough%20web%20crawling%20without%20the%20explicit%20consent%20of%20users%2C%20raising%20ethical%20and%0Aprivacy%20concerns.%20To%20address%20this%2C%20many%20recent%20approaches%20have%20explored%20the%20use%0Aof%20synthetic%20data%20for%20training%20face%20recognition%20models.%20However%2C%20these%20models%0Atypically%20underperform%20compared%20to%20those%20trained%20on%20real-world%20data.%20A%20common%0Alimitation%20is%20that%20a%20single%20generator%20model%20is%20often%20used%20to%20create%20the%20entire%0Asynthetic%20dataset%2C%20leading%20to%20model-specific%20artifacts%20that%20may%20cause%0Aoverfitting%20to%20the%20generator%27s%20inherent%20biases%20and%20artifacts.%20In%20this%20work%2C%20we%0Apropose%20a%20solution%20by%20combining%20two%20state-of-the-art%20synthetic%20face%20datasets%0Agenerated%20using%20architecturally%20distinct%20backbones.%20This%20fusion%20reduces%0Amodel-specific%20artifacts%2C%20enhances%20diversity%20in%20pose%2C%20lighting%2C%20and%0Ademographics%2C%20and%20implicitly%20regularizes%20the%20face%20recognition%20model%20by%0Aemphasizing%20identity-relevant%20features.%20We%20evaluate%20the%20performance%20of%20models%0Atrained%20on%20this%20combined%20dataset%20using%20standard%20face%20recognition%20benchmarks%20and%0Ademonstrate%20that%20our%20approach%20achieves%20superior%20performance%20across%20many%20of%0Athese%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Domain%2520Diversity%2520in%2520Synthetic%2520Data%2520Face%2520Recognition%2520with%250A%2520%2520Dataset%2520Fusion%26entry.906535625%3DAnjith%2520George%2520and%2520Sebastien%2520Marcel%26entry.1292438233%3D%2520%2520While%2520the%2520accuracy%2520of%2520face%2520recognition%2520systems%2520has%2520improved%2520significantly%2520in%250Arecent%2520years%252C%2520the%2520datasets%2520used%2520to%2520train%2520these%2520models%2520are%2520often%2520collected%250Athrough%2520web%2520crawling%2520without%2520the%2520explicit%2520consent%2520of%2520users%252C%2520raising%2520ethical%2520and%250Aprivacy%2520concerns.%2520To%2520address%2520this%252C%2520many%2520recent%2520approaches%2520have%2520explored%2520the%2520use%250Aof%2520synthetic%2520data%2520for%2520training%2520face%2520recognition%2520models.%2520However%252C%2520these%2520models%250Atypically%2520underperform%2520compared%2520to%2520those%2520trained%2520on%2520real-world%2520data.%2520A%2520common%250Alimitation%2520is%2520that%2520a%2520single%2520generator%2520model%2520is%2520often%2520used%2520to%2520create%2520the%2520entire%250Asynthetic%2520dataset%252C%2520leading%2520to%2520model-specific%2520artifacts%2520that%2520may%2520cause%250Aoverfitting%2520to%2520the%2520generator%2527s%2520inherent%2520biases%2520and%2520artifacts.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520solution%2520by%2520combining%2520two%2520state-of-the-art%2520synthetic%2520face%2520datasets%250Agenerated%2520using%2520architecturally%2520distinct%2520backbones.%2520This%2520fusion%2520reduces%250Amodel-specific%2520artifacts%252C%2520enhances%2520diversity%2520in%2520pose%252C%2520lighting%252C%2520and%250Ademographics%252C%2520and%2520implicitly%2520regularizes%2520the%2520face%2520recognition%2520model%2520by%250Aemphasizing%2520identity-relevant%2520features.%2520We%2520evaluate%2520the%2520performance%2520of%2520models%250Atrained%2520on%2520this%2520combined%2520dataset%2520using%2520standard%2520face%2520recognition%2520benchmarks%2520and%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520superior%2520performance%2520across%2520many%2520of%250Athese%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Domain%20Diversity%20in%20Synthetic%20Data%20Face%20Recognition%20with%0A%20%20Dataset%20Fusion&entry.906535625=Anjith%20George%20and%20Sebastien%20Marcel&entry.1292438233=%20%20While%20the%20accuracy%20of%20face%20recognition%20systems%20has%20improved%20significantly%20in%0Arecent%20years%2C%20the%20datasets%20used%20to%20train%20these%20models%20are%20often%20collected%0Athrough%20web%20crawling%20without%20the%20explicit%20consent%20of%20users%2C%20raising%20ethical%20and%0Aprivacy%20concerns.%20To%20address%20this%2C%20many%20recent%20approaches%20have%20explored%20the%20use%0Aof%20synthetic%20data%20for%20training%20face%20recognition%20models.%20However%2C%20these%20models%0Atypically%20underperform%20compared%20to%20those%20trained%20on%20real-world%20data.%20A%20common%0Alimitation%20is%20that%20a%20single%20generator%20model%20is%20often%20used%20to%20create%20the%20entire%0Asynthetic%20dataset%2C%20leading%20to%20model-specific%20artifacts%20that%20may%20cause%0Aoverfitting%20to%20the%20generator%27s%20inherent%20biases%20and%20artifacts.%20In%20this%20work%2C%20we%0Apropose%20a%20solution%20by%20combining%20two%20state-of-the-art%20synthetic%20face%20datasets%0Agenerated%20using%20architecturally%20distinct%20backbones.%20This%20fusion%20reduces%0Amodel-specific%20artifacts%2C%20enhances%20diversity%20in%20pose%2C%20lighting%2C%20and%0Ademographics%2C%20and%20implicitly%20regularizes%20the%20face%20recognition%20model%20by%0Aemphasizing%20identity-relevant%20features.%20We%20evaluate%20the%20performance%20of%20models%0Atrained%20on%20this%20combined%20dataset%20using%20standard%20face%20recognition%20benchmarks%20and%0Ademonstrate%20that%20our%20approach%20achieves%20superior%20performance%20across%20many%20of%0Athese%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16790v1&entry.124074799=Read"},
{"title": "CTSL: Codebook-based Temporal-Spatial Learning for Accurate Non-Contrast\n  Cardiac Risk Prediction Using Cine MRIs", "author": "Haoyang Su and Shaohao Rui and Jinyi Xiang and Lianming Wu and Xiaosong Wang", "abstract": "  Accurate and contrast-free Major Adverse Cardiac Events (MACE) prediction\nfrom Cine MRI sequences remains a critical challenge. Existing methods\ntypically necessitate supervised learning based on human-refined masks in the\nventricular myocardium, which become impractical without contrast agents. We\nintroduce a self-supervised framework, namely Codebook-based Temporal-Spatial\nLearning (CTSL), that learns dynamic, spatiotemporal representations from raw\nCine data without requiring segmentation masks. CTSL decouples temporal and\nspatial features through a multi-view distillation strategy, where the teacher\nmodel processes multiple Cine views, and the student model learns from\nreduced-dimensional Cine-SA sequences. By leveraging codebook-based feature\nrepresentations and dynamic lesion self-detection through motion cues, CTSL\ncaptures intricate temporal dependencies and motion patterns. High-confidence\nMACE risk predictions are achieved through our model, providing a rapid,\nnon-invasive solution for cardiac risk assessment that outperforms traditional\ncontrast-dependent methods, thereby enabling timely and accessible heart\ndisease diagnosis in clinical settings.\n", "link": "http://arxiv.org/abs/2507.16612v1", "date": "2025-07-22", "relevancy": 2.1961, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5688}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5381}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTSL%3A%20Codebook-based%20Temporal-Spatial%20Learning%20for%20Accurate%20Non-Contrast%0A%20%20Cardiac%20Risk%20Prediction%20Using%20Cine%20MRIs&body=Title%3A%20CTSL%3A%20Codebook-based%20Temporal-Spatial%20Learning%20for%20Accurate%20Non-Contrast%0A%20%20Cardiac%20Risk%20Prediction%20Using%20Cine%20MRIs%0AAuthor%3A%20Haoyang%20Su%20and%20Shaohao%20Rui%20and%20Jinyi%20Xiang%20and%20Lianming%20Wu%20and%20Xiaosong%20Wang%0AAbstract%3A%20%20%20Accurate%20and%20contrast-free%20Major%20Adverse%20Cardiac%20Events%20%28MACE%29%20prediction%0Afrom%20Cine%20MRI%20sequences%20remains%20a%20critical%20challenge.%20Existing%20methods%0Atypically%20necessitate%20supervised%20learning%20based%20on%20human-refined%20masks%20in%20the%0Aventricular%20myocardium%2C%20which%20become%20impractical%20without%20contrast%20agents.%20We%0Aintroduce%20a%20self-supervised%20framework%2C%20namely%20Codebook-based%20Temporal-Spatial%0ALearning%20%28CTSL%29%2C%20that%20learns%20dynamic%2C%20spatiotemporal%20representations%20from%20raw%0ACine%20data%20without%20requiring%20segmentation%20masks.%20CTSL%20decouples%20temporal%20and%0Aspatial%20features%20through%20a%20multi-view%20distillation%20strategy%2C%20where%20the%20teacher%0Amodel%20processes%20multiple%20Cine%20views%2C%20and%20the%20student%20model%20learns%20from%0Areduced-dimensional%20Cine-SA%20sequences.%20By%20leveraging%20codebook-based%20feature%0Arepresentations%20and%20dynamic%20lesion%20self-detection%20through%20motion%20cues%2C%20CTSL%0Acaptures%20intricate%20temporal%20dependencies%20and%20motion%20patterns.%20High-confidence%0AMACE%20risk%20predictions%20are%20achieved%20through%20our%20model%2C%20providing%20a%20rapid%2C%0Anon-invasive%20solution%20for%20cardiac%20risk%20assessment%20that%20outperforms%20traditional%0Acontrast-dependent%20methods%2C%20thereby%20enabling%20timely%20and%20accessible%20heart%0Adisease%20diagnosis%20in%20clinical%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16612v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTSL%253A%2520Codebook-based%2520Temporal-Spatial%2520Learning%2520for%2520Accurate%2520Non-Contrast%250A%2520%2520Cardiac%2520Risk%2520Prediction%2520Using%2520Cine%2520MRIs%26entry.906535625%3DHaoyang%2520Su%2520and%2520Shaohao%2520Rui%2520and%2520Jinyi%2520Xiang%2520and%2520Lianming%2520Wu%2520and%2520Xiaosong%2520Wang%26entry.1292438233%3D%2520%2520Accurate%2520and%2520contrast-free%2520Major%2520Adverse%2520Cardiac%2520Events%2520%2528MACE%2529%2520prediction%250Afrom%2520Cine%2520MRI%2520sequences%2520remains%2520a%2520critical%2520challenge.%2520Existing%2520methods%250Atypically%2520necessitate%2520supervised%2520learning%2520based%2520on%2520human-refined%2520masks%2520in%2520the%250Aventricular%2520myocardium%252C%2520which%2520become%2520impractical%2520without%2520contrast%2520agents.%2520We%250Aintroduce%2520a%2520self-supervised%2520framework%252C%2520namely%2520Codebook-based%2520Temporal-Spatial%250ALearning%2520%2528CTSL%2529%252C%2520that%2520learns%2520dynamic%252C%2520spatiotemporal%2520representations%2520from%2520raw%250ACine%2520data%2520without%2520requiring%2520segmentation%2520masks.%2520CTSL%2520decouples%2520temporal%2520and%250Aspatial%2520features%2520through%2520a%2520multi-view%2520distillation%2520strategy%252C%2520where%2520the%2520teacher%250Amodel%2520processes%2520multiple%2520Cine%2520views%252C%2520and%2520the%2520student%2520model%2520learns%2520from%250Areduced-dimensional%2520Cine-SA%2520sequences.%2520By%2520leveraging%2520codebook-based%2520feature%250Arepresentations%2520and%2520dynamic%2520lesion%2520self-detection%2520through%2520motion%2520cues%252C%2520CTSL%250Acaptures%2520intricate%2520temporal%2520dependencies%2520and%2520motion%2520patterns.%2520High-confidence%250AMACE%2520risk%2520predictions%2520are%2520achieved%2520through%2520our%2520model%252C%2520providing%2520a%2520rapid%252C%250Anon-invasive%2520solution%2520for%2520cardiac%2520risk%2520assessment%2520that%2520outperforms%2520traditional%250Acontrast-dependent%2520methods%252C%2520thereby%2520enabling%2520timely%2520and%2520accessible%2520heart%250Adisease%2520diagnosis%2520in%2520clinical%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16612v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTSL%3A%20Codebook-based%20Temporal-Spatial%20Learning%20for%20Accurate%20Non-Contrast%0A%20%20Cardiac%20Risk%20Prediction%20Using%20Cine%20MRIs&entry.906535625=Haoyang%20Su%20and%20Shaohao%20Rui%20and%20Jinyi%20Xiang%20and%20Lianming%20Wu%20and%20Xiaosong%20Wang&entry.1292438233=%20%20Accurate%20and%20contrast-free%20Major%20Adverse%20Cardiac%20Events%20%28MACE%29%20prediction%0Afrom%20Cine%20MRI%20sequences%20remains%20a%20critical%20challenge.%20Existing%20methods%0Atypically%20necessitate%20supervised%20learning%20based%20on%20human-refined%20masks%20in%20the%0Aventricular%20myocardium%2C%20which%20become%20impractical%20without%20contrast%20agents.%20We%0Aintroduce%20a%20self-supervised%20framework%2C%20namely%20Codebook-based%20Temporal-Spatial%0ALearning%20%28CTSL%29%2C%20that%20learns%20dynamic%2C%20spatiotemporal%20representations%20from%20raw%0ACine%20data%20without%20requiring%20segmentation%20masks.%20CTSL%20decouples%20temporal%20and%0Aspatial%20features%20through%20a%20multi-view%20distillation%20strategy%2C%20where%20the%20teacher%0Amodel%20processes%20multiple%20Cine%20views%2C%20and%20the%20student%20model%20learns%20from%0Areduced-dimensional%20Cine-SA%20sequences.%20By%20leveraging%20codebook-based%20feature%0Arepresentations%20and%20dynamic%20lesion%20self-detection%20through%20motion%20cues%2C%20CTSL%0Acaptures%20intricate%20temporal%20dependencies%20and%20motion%20patterns.%20High-confidence%0AMACE%20risk%20predictions%20are%20achieved%20through%20our%20model%2C%20providing%20a%20rapid%2C%0Anon-invasive%20solution%20for%20cardiac%20risk%20assessment%20that%20outperforms%20traditional%0Acontrast-dependent%20methods%2C%20thereby%20enabling%20timely%20and%20accessible%20heart%0Adisease%20diagnosis%20in%20clinical%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16612v1&entry.124074799=Read"},
{"title": "ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for\n  Visual Question Answering", "author": "Thuy-Duong Tran and Trung-Kien Tran and Manfred Hauswirth and Danh Le Phuoc", "abstract": "  In this paper, we propose a new dataset, ReasonVQA, for the Visual Question\nAnswering (VQA) task. Our dataset is automatically integrated with structured\nencyclopedic knowledge and constructed using a low-cost framework, which is\ncapable of generating complex, multi-hop questions. We evaluated\nstate-of-the-art VQA models on ReasonVQA, and the empirical results demonstrate\nthat ReasonVQA poses significant challenges to these models, highlighting its\npotential for benchmarking and advancing the field of VQA. Additionally, our\ndataset can be easily scaled with respect to input images; the current version\nsurpasses the largest existing datasets requiring external knowledge by more\nthan an order of magnitude.\n", "link": "http://arxiv.org/abs/2507.16403v1", "date": "2025-07-22", "relevancy": 2.1939, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5561}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReasonVQA%3A%20A%20Multi-hop%20Reasoning%20Benchmark%20with%20Structural%20Knowledge%20for%0A%20%20Visual%20Question%20Answering&body=Title%3A%20ReasonVQA%3A%20A%20Multi-hop%20Reasoning%20Benchmark%20with%20Structural%20Knowledge%20for%0A%20%20Visual%20Question%20Answering%0AAuthor%3A%20Thuy-Duong%20Tran%20and%20Trung-Kien%20Tran%20and%20Manfred%20Hauswirth%20and%20Danh%20Le%20Phuoc%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20dataset%2C%20ReasonVQA%2C%20for%20the%20Visual%20Question%0AAnswering%20%28VQA%29%20task.%20Our%20dataset%20is%20automatically%20integrated%20with%20structured%0Aencyclopedic%20knowledge%20and%20constructed%20using%20a%20low-cost%20framework%2C%20which%20is%0Acapable%20of%20generating%20complex%2C%20multi-hop%20questions.%20We%20evaluated%0Astate-of-the-art%20VQA%20models%20on%20ReasonVQA%2C%20and%20the%20empirical%20results%20demonstrate%0Athat%20ReasonVQA%20poses%20significant%20challenges%20to%20these%20models%2C%20highlighting%20its%0Apotential%20for%20benchmarking%20and%20advancing%20the%20field%20of%20VQA.%20Additionally%2C%20our%0Adataset%20can%20be%20easily%20scaled%20with%20respect%20to%20input%20images%3B%20the%20current%20version%0Asurpasses%20the%20largest%20existing%20datasets%20requiring%20external%20knowledge%20by%20more%0Athan%20an%20order%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasonVQA%253A%2520A%2520Multi-hop%2520Reasoning%2520Benchmark%2520with%2520Structural%2520Knowledge%2520for%250A%2520%2520Visual%2520Question%2520Answering%26entry.906535625%3DThuy-Duong%2520Tran%2520and%2520Trung-Kien%2520Tran%2520and%2520Manfred%2520Hauswirth%2520and%2520Danh%2520Le%2520Phuoc%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520dataset%252C%2520ReasonVQA%252C%2520for%2520the%2520Visual%2520Question%250AAnswering%2520%2528VQA%2529%2520task.%2520Our%2520dataset%2520is%2520automatically%2520integrated%2520with%2520structured%250Aencyclopedic%2520knowledge%2520and%2520constructed%2520using%2520a%2520low-cost%2520framework%252C%2520which%2520is%250Acapable%2520of%2520generating%2520complex%252C%2520multi-hop%2520questions.%2520We%2520evaluated%250Astate-of-the-art%2520VQA%2520models%2520on%2520ReasonVQA%252C%2520and%2520the%2520empirical%2520results%2520demonstrate%250Athat%2520ReasonVQA%2520poses%2520significant%2520challenges%2520to%2520these%2520models%252C%2520highlighting%2520its%250Apotential%2520for%2520benchmarking%2520and%2520advancing%2520the%2520field%2520of%2520VQA.%2520Additionally%252C%2520our%250Adataset%2520can%2520be%2520easily%2520scaled%2520with%2520respect%2520to%2520input%2520images%253B%2520the%2520current%2520version%250Asurpasses%2520the%2520largest%2520existing%2520datasets%2520requiring%2520external%2520knowledge%2520by%2520more%250Athan%2520an%2520order%2520of%2520magnitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReasonVQA%3A%20A%20Multi-hop%20Reasoning%20Benchmark%20with%20Structural%20Knowledge%20for%0A%20%20Visual%20Question%20Answering&entry.906535625=Thuy-Duong%20Tran%20and%20Trung-Kien%20Tran%20and%20Manfred%20Hauswirth%20and%20Danh%20Le%20Phuoc&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20dataset%2C%20ReasonVQA%2C%20for%20the%20Visual%20Question%0AAnswering%20%28VQA%29%20task.%20Our%20dataset%20is%20automatically%20integrated%20with%20structured%0Aencyclopedic%20knowledge%20and%20constructed%20using%20a%20low-cost%20framework%2C%20which%20is%0Acapable%20of%20generating%20complex%2C%20multi-hop%20questions.%20We%20evaluated%0Astate-of-the-art%20VQA%20models%20on%20ReasonVQA%2C%20and%20the%20empirical%20results%20demonstrate%0Athat%20ReasonVQA%20poses%20significant%20challenges%20to%20these%20models%2C%20highlighting%20its%0Apotential%20for%20benchmarking%20and%20advancing%20the%20field%20of%20VQA.%20Additionally%2C%20our%0Adataset%20can%20be%20easily%20scaled%20with%20respect%20to%20input%20images%3B%20the%20current%20version%0Asurpasses%20the%20largest%20existing%20datasets%20requiring%20external%20knowledge%20by%20more%0Athan%20an%20order%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16403v1&entry.124074799=Read"},
{"title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking\n  Reasoning", "author": "Junhao Shen and Haiteng Zhao and Yuzhe Gu and Songyang Gao and Kuikun Liu and Haian Huang and Jianfei Gao and Dahua Lin and Wenwei Zhang and Kai Chen", "abstract": "  Enhancing large vision-language models (LVLMs) with visual slow-thinking\nreasoning is crucial for solving complex multimodal tasks. However, since LVLMs\nare mainly trained with vision-language alignment, it is difficult to adopt\non-policy reinforcement learning (RL) to develop the slow thinking ability\nbecause the rollout space is restricted by its initial abilities. Off-policy RL\noffers a way to go beyond the current policy, but directly distilling\ntrajectories from external models may cause visual hallucinations due to\nmismatched visual perception abilities across models. To address these issues,\nthis paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for\nvision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy\nbehavior model by combining on-policy visual understanding from a trainable\nLVLM with off-policy slow-thinking reasoning from a language model, assigns\noutcome-based rewards to reasoning, and propagates visual rewards backward.\nThen LVLM learns slow-thinking reasoning ability from the obtained reasoning\ntrajectories using propagated rewards via off-policy RL algorithms. Extensive\nexperiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the\neffectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in\naverage, reaching state-of-the-art performance among open-source LVLMs on\nmultiple multimodal reasoning benchmarks, and even outperforms some\nclosed-source models (e.g., GPT-4.1) on the challenging MathVision and\nOlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively.\nAnalysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy\nRL methods, offering a better policy initialization for further on-policy\ntraining.\n", "link": "http://arxiv.org/abs/2507.16814v1", "date": "2025-07-22", "relevancy": 2.1768, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5468}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-off-Policy%20Reinforcement%20Learning%20for%20Vision-Language%20Slow-thinking%0A%20%20Reasoning&body=Title%3A%20Semi-off-Policy%20Reinforcement%20Learning%20for%20Vision-Language%20Slow-thinking%0A%20%20Reasoning%0AAuthor%3A%20Junhao%20Shen%20and%20Haiteng%20Zhao%20and%20Yuzhe%20Gu%20and%20Songyang%20Gao%20and%20Kuikun%20Liu%20and%20Haian%20Huang%20and%20Jianfei%20Gao%20and%20Dahua%20Lin%20and%20Wenwei%20Zhang%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Enhancing%20large%20vision-language%20models%20%28LVLMs%29%20with%20visual%20slow-thinking%0Areasoning%20is%20crucial%20for%20solving%20complex%20multimodal%20tasks.%20However%2C%20since%20LVLMs%0Aare%20mainly%20trained%20with%20vision-language%20alignment%2C%20it%20is%20difficult%20to%20adopt%0Aon-policy%20reinforcement%20learning%20%28RL%29%20to%20develop%20the%20slow%20thinking%20ability%0Abecause%20the%20rollout%20space%20is%20restricted%20by%20its%20initial%20abilities.%20Off-policy%20RL%0Aoffers%20a%20way%20to%20go%20beyond%20the%20current%20policy%2C%20but%20directly%20distilling%0Atrajectories%20from%20external%20models%20may%20cause%20visual%20hallucinations%20due%20to%0Amismatched%20visual%20perception%20abilities%20across%20models.%20To%20address%20these%20issues%2C%0Athis%20paper%20proposes%20SOPHIA%2C%20a%20simple%20and%20scalable%20Semi-Off-Policy%20RL%20for%0Avision-language%20slow-tHInking%20reAsoning.%20SOPHIA%20builds%20a%20semi-off-policy%0Abehavior%20model%20by%20combining%20on-policy%20visual%20understanding%20from%20a%20trainable%0ALVLM%20with%20off-policy%20slow-thinking%20reasoning%20from%20a%20language%20model%2C%20assigns%0Aoutcome-based%20rewards%20to%20reasoning%2C%20and%20propagates%20visual%20rewards%20backward.%0AThen%20LVLM%20learns%20slow-thinking%20reasoning%20ability%20from%20the%20obtained%20reasoning%0Atrajectories%20using%20propagated%20rewards%20via%20off-policy%20RL%20algorithms.%20Extensive%0Aexperiments%20with%20InternVL2.5%20and%20InternVL3.0%20with%208B%20and%2038B%20sizes%20show%20the%0Aeffectiveness%20of%20SOPHIA.%20Notably%2C%20SOPHIA%20improves%20InternVL3.0-38B%20by%208.50%25%20in%0Aaverage%2C%20reaching%20state-of-the-art%20performance%20among%20open-source%20LVLMs%20on%0Amultiple%20multimodal%20reasoning%20benchmarks%2C%20and%20even%20outperforms%20some%0Aclosed-source%20models%20%28e.g.%2C%20GPT-4.1%29%20on%20the%20challenging%20MathVision%20and%0AOlympiadBench%2C%20achieving%2049.08%25%20and%2049.95%25%20pass%401%20accuracy%2C%20respectively.%0AAnalysis%20shows%20SOPHIA%20outperforms%20supervised%20fine-tuning%20and%20direct%20on-policy%0ARL%20methods%2C%20offering%20a%20better%20policy%20initialization%20for%20further%20on-policy%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-off-Policy%2520Reinforcement%2520Learning%2520for%2520Vision-Language%2520Slow-thinking%250A%2520%2520Reasoning%26entry.906535625%3DJunhao%2520Shen%2520and%2520Haiteng%2520Zhao%2520and%2520Yuzhe%2520Gu%2520and%2520Songyang%2520Gao%2520and%2520Kuikun%2520Liu%2520and%2520Haian%2520Huang%2520and%2520Jianfei%2520Gao%2520and%2520Dahua%2520Lin%2520and%2520Wenwei%2520Zhang%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Enhancing%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520with%2520visual%2520slow-thinking%250Areasoning%2520is%2520crucial%2520for%2520solving%2520complex%2520multimodal%2520tasks.%2520However%252C%2520since%2520LVLMs%250Aare%2520mainly%2520trained%2520with%2520vision-language%2520alignment%252C%2520it%2520is%2520difficult%2520to%2520adopt%250Aon-policy%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520develop%2520the%2520slow%2520thinking%2520ability%250Abecause%2520the%2520rollout%2520space%2520is%2520restricted%2520by%2520its%2520initial%2520abilities.%2520Off-policy%2520RL%250Aoffers%2520a%2520way%2520to%2520go%2520beyond%2520the%2520current%2520policy%252C%2520but%2520directly%2520distilling%250Atrajectories%2520from%2520external%2520models%2520may%2520cause%2520visual%2520hallucinations%2520due%2520to%250Amismatched%2520visual%2520perception%2520abilities%2520across%2520models.%2520To%2520address%2520these%2520issues%252C%250Athis%2520paper%2520proposes%2520SOPHIA%252C%2520a%2520simple%2520and%2520scalable%2520Semi-Off-Policy%2520RL%2520for%250Avision-language%2520slow-tHInking%2520reAsoning.%2520SOPHIA%2520builds%2520a%2520semi-off-policy%250Abehavior%2520model%2520by%2520combining%2520on-policy%2520visual%2520understanding%2520from%2520a%2520trainable%250ALVLM%2520with%2520off-policy%2520slow-thinking%2520reasoning%2520from%2520a%2520language%2520model%252C%2520assigns%250Aoutcome-based%2520rewards%2520to%2520reasoning%252C%2520and%2520propagates%2520visual%2520rewards%2520backward.%250AThen%2520LVLM%2520learns%2520slow-thinking%2520reasoning%2520ability%2520from%2520the%2520obtained%2520reasoning%250Atrajectories%2520using%2520propagated%2520rewards%2520via%2520off-policy%2520RL%2520algorithms.%2520Extensive%250Aexperiments%2520with%2520InternVL2.5%2520and%2520InternVL3.0%2520with%25208B%2520and%252038B%2520sizes%2520show%2520the%250Aeffectiveness%2520of%2520SOPHIA.%2520Notably%252C%2520SOPHIA%2520improves%2520InternVL3.0-38B%2520by%25208.50%2525%2520in%250Aaverage%252C%2520reaching%2520state-of-the-art%2520performance%2520among%2520open-source%2520LVLMs%2520on%250Amultiple%2520multimodal%2520reasoning%2520benchmarks%252C%2520and%2520even%2520outperforms%2520some%250Aclosed-source%2520models%2520%2528e.g.%252C%2520GPT-4.1%2529%2520on%2520the%2520challenging%2520MathVision%2520and%250AOlympiadBench%252C%2520achieving%252049.08%2525%2520and%252049.95%2525%2520pass%25401%2520accuracy%252C%2520respectively.%250AAnalysis%2520shows%2520SOPHIA%2520outperforms%2520supervised%2520fine-tuning%2520and%2520direct%2520on-policy%250ARL%2520methods%252C%2520offering%2520a%2520better%2520policy%2520initialization%2520for%2520further%2520on-policy%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-off-Policy%20Reinforcement%20Learning%20for%20Vision-Language%20Slow-thinking%0A%20%20Reasoning&entry.906535625=Junhao%20Shen%20and%20Haiteng%20Zhao%20and%20Yuzhe%20Gu%20and%20Songyang%20Gao%20and%20Kuikun%20Liu%20and%20Haian%20Huang%20and%20Jianfei%20Gao%20and%20Dahua%20Lin%20and%20Wenwei%20Zhang%20and%20Kai%20Chen&entry.1292438233=%20%20Enhancing%20large%20vision-language%20models%20%28LVLMs%29%20with%20visual%20slow-thinking%0Areasoning%20is%20crucial%20for%20solving%20complex%20multimodal%20tasks.%20However%2C%20since%20LVLMs%0Aare%20mainly%20trained%20with%20vision-language%20alignment%2C%20it%20is%20difficult%20to%20adopt%0Aon-policy%20reinforcement%20learning%20%28RL%29%20to%20develop%20the%20slow%20thinking%20ability%0Abecause%20the%20rollout%20space%20is%20restricted%20by%20its%20initial%20abilities.%20Off-policy%20RL%0Aoffers%20a%20way%20to%20go%20beyond%20the%20current%20policy%2C%20but%20directly%20distilling%0Atrajectories%20from%20external%20models%20may%20cause%20visual%20hallucinations%20due%20to%0Amismatched%20visual%20perception%20abilities%20across%20models.%20To%20address%20these%20issues%2C%0Athis%20paper%20proposes%20SOPHIA%2C%20a%20simple%20and%20scalable%20Semi-Off-Policy%20RL%20for%0Avision-language%20slow-tHInking%20reAsoning.%20SOPHIA%20builds%20a%20semi-off-policy%0Abehavior%20model%20by%20combining%20on-policy%20visual%20understanding%20from%20a%20trainable%0ALVLM%20with%20off-policy%20slow-thinking%20reasoning%20from%20a%20language%20model%2C%20assigns%0Aoutcome-based%20rewards%20to%20reasoning%2C%20and%20propagates%20visual%20rewards%20backward.%0AThen%20LVLM%20learns%20slow-thinking%20reasoning%20ability%20from%20the%20obtained%20reasoning%0Atrajectories%20using%20propagated%20rewards%20via%20off-policy%20RL%20algorithms.%20Extensive%0Aexperiments%20with%20InternVL2.5%20and%20InternVL3.0%20with%208B%20and%2038B%20sizes%20show%20the%0Aeffectiveness%20of%20SOPHIA.%20Notably%2C%20SOPHIA%20improves%20InternVL3.0-38B%20by%208.50%25%20in%0Aaverage%2C%20reaching%20state-of-the-art%20performance%20among%20open-source%20LVLMs%20on%0Amultiple%20multimodal%20reasoning%20benchmarks%2C%20and%20even%20outperforms%20some%0Aclosed-source%20models%20%28e.g.%2C%20GPT-4.1%29%20on%20the%20challenging%20MathVision%20and%0AOlympiadBench%2C%20achieving%2049.08%25%20and%2049.95%25%20pass%401%20accuracy%2C%20respectively.%0AAnalysis%20shows%20SOPHIA%20outperforms%20supervised%20fine-tuning%20and%20direct%20on-policy%0ARL%20methods%2C%20offering%20a%20better%20policy%20initialization%20for%20further%20on-policy%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16814v1&entry.124074799=Read"},
{"title": "Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels\n  for Building Detection", "author": "Shuang Song and Yang Tang and Rongjun Qin", "abstract": "  Deep learning has significantly advanced building segmentation in remote\nsensing, yet models struggle to generalize on data of diverse geographic\nregions due to variations in city layouts and the distribution of building\ntypes, sizes and locations. However, the amount of time-consuming annotated\ndata for capturing worldwide diversity may never catch up with the demands of\nincreasingly data-hungry models. Thus, we propose a novel approach: re-training\nmodels at test time using synthetic data tailored to the target region's city\nlayout. This method generates geo-typical synthetic data that closely\nreplicates the urban structure of a target area by leveraging geospatial data\nsuch as street network from OpenStreetMap. Using procedural modeling and\nphysics-based rendering, very high-resolution synthetic images are created,\nincorporating domain randomization in building shapes, materials, and\nenvironmental illumination. This enables the generation of virtually unlimited\ntraining samples that maintain the essential characteristics of the target\nenvironment. To overcome synthetic-to-real domain gaps, our approach integrates\ngeo-typical data into an adversarial domain adaptation framework for building\nsegmentation. Experiments demonstrate significant performance enhancements,\nwith median improvements of up to 12%, depending on the domain gap. This\nscalable and cost-effective method blends partial geographic knowledge with\nsynthetic imagery, providing a promising solution to the \"model collapse\" issue\nin purely synthetic datasets. It offers a practical pathway to improving\ngeneralization in remote sensing building segmentation without extensive\nreal-world annotations.\n", "link": "http://arxiv.org/abs/2507.16657v1", "date": "2025-07-22", "relevancy": 2.1696, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5487}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.542}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Data%20Matters%3A%20Re-training%20with%20Geo-typical%20Synthetic%20Labels%0A%20%20for%20Building%20Detection&body=Title%3A%20Synthetic%20Data%20Matters%3A%20Re-training%20with%20Geo-typical%20Synthetic%20Labels%0A%20%20for%20Building%20Detection%0AAuthor%3A%20Shuang%20Song%20and%20Yang%20Tang%20and%20Rongjun%20Qin%0AAbstract%3A%20%20%20Deep%20learning%20has%20significantly%20advanced%20building%20segmentation%20in%20remote%0Asensing%2C%20yet%20models%20struggle%20to%20generalize%20on%20data%20of%20diverse%20geographic%0Aregions%20due%20to%20variations%20in%20city%20layouts%20and%20the%20distribution%20of%20building%0Atypes%2C%20sizes%20and%20locations.%20However%2C%20the%20amount%20of%20time-consuming%20annotated%0Adata%20for%20capturing%20worldwide%20diversity%20may%20never%20catch%20up%20with%20the%20demands%20of%0Aincreasingly%20data-hungry%20models.%20Thus%2C%20we%20propose%20a%20novel%20approach%3A%20re-training%0Amodels%20at%20test%20time%20using%20synthetic%20data%20tailored%20to%20the%20target%20region%27s%20city%0Alayout.%20This%20method%20generates%20geo-typical%20synthetic%20data%20that%20closely%0Areplicates%20the%20urban%20structure%20of%20a%20target%20area%20by%20leveraging%20geospatial%20data%0Asuch%20as%20street%20network%20from%20OpenStreetMap.%20Using%20procedural%20modeling%20and%0Aphysics-based%20rendering%2C%20very%20high-resolution%20synthetic%20images%20are%20created%2C%0Aincorporating%20domain%20randomization%20in%20building%20shapes%2C%20materials%2C%20and%0Aenvironmental%20illumination.%20This%20enables%20the%20generation%20of%20virtually%20unlimited%0Atraining%20samples%20that%20maintain%20the%20essential%20characteristics%20of%20the%20target%0Aenvironment.%20To%20overcome%20synthetic-to-real%20domain%20gaps%2C%20our%20approach%20integrates%0Ageo-typical%20data%20into%20an%20adversarial%20domain%20adaptation%20framework%20for%20building%0Asegmentation.%20Experiments%20demonstrate%20significant%20performance%20enhancements%2C%0Awith%20median%20improvements%20of%20up%20to%2012%25%2C%20depending%20on%20the%20domain%20gap.%20This%0Ascalable%20and%20cost-effective%20method%20blends%20partial%20geographic%20knowledge%20with%0Asynthetic%20imagery%2C%20providing%20a%20promising%20solution%20to%20the%20%22model%20collapse%22%20issue%0Ain%20purely%20synthetic%20datasets.%20It%20offers%20a%20practical%20pathway%20to%20improving%0Ageneralization%20in%20remote%20sensing%20building%20segmentation%20without%20extensive%0Areal-world%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Data%2520Matters%253A%2520Re-training%2520with%2520Geo-typical%2520Synthetic%2520Labels%250A%2520%2520for%2520Building%2520Detection%26entry.906535625%3DShuang%2520Song%2520and%2520Yang%2520Tang%2520and%2520Rongjun%2520Qin%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520significantly%2520advanced%2520building%2520segmentation%2520in%2520remote%250Asensing%252C%2520yet%2520models%2520struggle%2520to%2520generalize%2520on%2520data%2520of%2520diverse%2520geographic%250Aregions%2520due%2520to%2520variations%2520in%2520city%2520layouts%2520and%2520the%2520distribution%2520of%2520building%250Atypes%252C%2520sizes%2520and%2520locations.%2520However%252C%2520the%2520amount%2520of%2520time-consuming%2520annotated%250Adata%2520for%2520capturing%2520worldwide%2520diversity%2520may%2520never%2520catch%2520up%2520with%2520the%2520demands%2520of%250Aincreasingly%2520data-hungry%2520models.%2520Thus%252C%2520we%2520propose%2520a%2520novel%2520approach%253A%2520re-training%250Amodels%2520at%2520test%2520time%2520using%2520synthetic%2520data%2520tailored%2520to%2520the%2520target%2520region%2527s%2520city%250Alayout.%2520This%2520method%2520generates%2520geo-typical%2520synthetic%2520data%2520that%2520closely%250Areplicates%2520the%2520urban%2520structure%2520of%2520a%2520target%2520area%2520by%2520leveraging%2520geospatial%2520data%250Asuch%2520as%2520street%2520network%2520from%2520OpenStreetMap.%2520Using%2520procedural%2520modeling%2520and%250Aphysics-based%2520rendering%252C%2520very%2520high-resolution%2520synthetic%2520images%2520are%2520created%252C%250Aincorporating%2520domain%2520randomization%2520in%2520building%2520shapes%252C%2520materials%252C%2520and%250Aenvironmental%2520illumination.%2520This%2520enables%2520the%2520generation%2520of%2520virtually%2520unlimited%250Atraining%2520samples%2520that%2520maintain%2520the%2520essential%2520characteristics%2520of%2520the%2520target%250Aenvironment.%2520To%2520overcome%2520synthetic-to-real%2520domain%2520gaps%252C%2520our%2520approach%2520integrates%250Ageo-typical%2520data%2520into%2520an%2520adversarial%2520domain%2520adaptation%2520framework%2520for%2520building%250Asegmentation.%2520Experiments%2520demonstrate%2520significant%2520performance%2520enhancements%252C%250Awith%2520median%2520improvements%2520of%2520up%2520to%252012%2525%252C%2520depending%2520on%2520the%2520domain%2520gap.%2520This%250Ascalable%2520and%2520cost-effective%2520method%2520blends%2520partial%2520geographic%2520knowledge%2520with%250Asynthetic%2520imagery%252C%2520providing%2520a%2520promising%2520solution%2520to%2520the%2520%2522model%2520collapse%2522%2520issue%250Ain%2520purely%2520synthetic%2520datasets.%2520It%2520offers%2520a%2520practical%2520pathway%2520to%2520improving%250Ageneralization%2520in%2520remote%2520sensing%2520building%2520segmentation%2520without%2520extensive%250Areal-world%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Data%20Matters%3A%20Re-training%20with%20Geo-typical%20Synthetic%20Labels%0A%20%20for%20Building%20Detection&entry.906535625=Shuang%20Song%20and%20Yang%20Tang%20and%20Rongjun%20Qin&entry.1292438233=%20%20Deep%20learning%20has%20significantly%20advanced%20building%20segmentation%20in%20remote%0Asensing%2C%20yet%20models%20struggle%20to%20generalize%20on%20data%20of%20diverse%20geographic%0Aregions%20due%20to%20variations%20in%20city%20layouts%20and%20the%20distribution%20of%20building%0Atypes%2C%20sizes%20and%20locations.%20However%2C%20the%20amount%20of%20time-consuming%20annotated%0Adata%20for%20capturing%20worldwide%20diversity%20may%20never%20catch%20up%20with%20the%20demands%20of%0Aincreasingly%20data-hungry%20models.%20Thus%2C%20we%20propose%20a%20novel%20approach%3A%20re-training%0Amodels%20at%20test%20time%20using%20synthetic%20data%20tailored%20to%20the%20target%20region%27s%20city%0Alayout.%20This%20method%20generates%20geo-typical%20synthetic%20data%20that%20closely%0Areplicates%20the%20urban%20structure%20of%20a%20target%20area%20by%20leveraging%20geospatial%20data%0Asuch%20as%20street%20network%20from%20OpenStreetMap.%20Using%20procedural%20modeling%20and%0Aphysics-based%20rendering%2C%20very%20high-resolution%20synthetic%20images%20are%20created%2C%0Aincorporating%20domain%20randomization%20in%20building%20shapes%2C%20materials%2C%20and%0Aenvironmental%20illumination.%20This%20enables%20the%20generation%20of%20virtually%20unlimited%0Atraining%20samples%20that%20maintain%20the%20essential%20characteristics%20of%20the%20target%0Aenvironment.%20To%20overcome%20synthetic-to-real%20domain%20gaps%2C%20our%20approach%20integrates%0Ageo-typical%20data%20into%20an%20adversarial%20domain%20adaptation%20framework%20for%20building%0Asegmentation.%20Experiments%20demonstrate%20significant%20performance%20enhancements%2C%0Awith%20median%20improvements%20of%20up%20to%2012%25%2C%20depending%20on%20the%20domain%20gap.%20This%0Ascalable%20and%20cost-effective%20method%20blends%20partial%20geographic%20knowledge%20with%0Asynthetic%20imagery%2C%20providing%20a%20promising%20solution%20to%20the%20%22model%20collapse%22%20issue%0Ain%20purely%20synthetic%20datasets.%20It%20offers%20a%20practical%20pathway%20to%20improving%0Ageneralization%20in%20remote%20sensing%20building%20segmentation%20without%20extensive%0Areal-world%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16657v1&entry.124074799=Read"},
{"title": "Data-Driven Adaptive Gradient Recovery for Unstructured Finite Volume\n  Computations", "author": "G. de Rom\u00e9mont and F. Renac and F. Chinesta and J. Nunez and D. Gueyffier", "abstract": "  We present a novel data-driven approach for enhancing gradient reconstruction\nin unstructured finite volume methods for hyperbolic conservation laws,\nspecifically for the 2D Euler equations. Our approach extends previous\nstructured-grid methodologies to unstructured meshes through a modified\nDeepONet architecture that incorporates local geometry in the neural network.\nThe architecture employs local mesh topology to ensure rotation invariance,\nwhile also ensuring first-order constraint on the learned operator. The\ntraining methodology incorporates physics-informed regularization through\nentropy penalization, total variation diminishing penalization, and parameter\nregularization to ensure physically consistent solutions, particularly in\nshock-dominated regions. The model is trained on high-fidelity datasets\nsolutions derived from sine waves and randomized piecewise constant initial\nconditions with periodic boundary conditions, enabling robust generalization to\ncomplex flow configurations or geometries. Validation test cases from the\nliterature, including challenging geometry configuration, demonstrates\nsubstantial improvements in accuracy compared to traditional second-order\nfinite volume schemes. The method achieves gains of 20-60% in solution accuracy\nwhile enhancing computational efficiency. A convergence study has been conveyed\nand reveal improved mesh convergence rates compared to the conventional solver.\nThe proposed algorithm is faster and more accurate than the traditional\nsecond-order finite volume solver, enabling high-fidelity simulations on\ncoarser grids while preserving the stability and conservation properties\nessential for hyperbolic conservation laws. This work is a part of a new\ngeneration of solvers that are built by combining Machine-Learning (ML) tools\nwith traditional numerical schemes, all while ensuring physical constraint on\nthe results.\n", "link": "http://arxiv.org/abs/2507.16571v1", "date": "2025-07-22", "relevancy": 2.1667, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.586}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5529}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Adaptive%20Gradient%20Recovery%20for%20Unstructured%20Finite%20Volume%0A%20%20Computations&body=Title%3A%20Data-Driven%20Adaptive%20Gradient%20Recovery%20for%20Unstructured%20Finite%20Volume%0A%20%20Computations%0AAuthor%3A%20G.%20de%20Rom%C3%A9mont%20and%20F.%20Renac%20and%20F.%20Chinesta%20and%20J.%20Nunez%20and%20D.%20Gueyffier%0AAbstract%3A%20%20%20We%20present%20a%20novel%20data-driven%20approach%20for%20enhancing%20gradient%20reconstruction%0Ain%20unstructured%20finite%20volume%20methods%20for%20hyperbolic%20conservation%20laws%2C%0Aspecifically%20for%20the%202D%20Euler%20equations.%20Our%20approach%20extends%20previous%0Astructured-grid%20methodologies%20to%20unstructured%20meshes%20through%20a%20modified%0ADeepONet%20architecture%20that%20incorporates%20local%20geometry%20in%20the%20neural%20network.%0AThe%20architecture%20employs%20local%20mesh%20topology%20to%20ensure%20rotation%20invariance%2C%0Awhile%20also%20ensuring%20first-order%20constraint%20on%20the%20learned%20operator.%20The%0Atraining%20methodology%20incorporates%20physics-informed%20regularization%20through%0Aentropy%20penalization%2C%20total%20variation%20diminishing%20penalization%2C%20and%20parameter%0Aregularization%20to%20ensure%20physically%20consistent%20solutions%2C%20particularly%20in%0Ashock-dominated%20regions.%20The%20model%20is%20trained%20on%20high-fidelity%20datasets%0Asolutions%20derived%20from%20sine%20waves%20and%20randomized%20piecewise%20constant%20initial%0Aconditions%20with%20periodic%20boundary%20conditions%2C%20enabling%20robust%20generalization%20to%0Acomplex%20flow%20configurations%20or%20geometries.%20Validation%20test%20cases%20from%20the%0Aliterature%2C%20including%20challenging%20geometry%20configuration%2C%20demonstrates%0Asubstantial%20improvements%20in%20accuracy%20compared%20to%20traditional%20second-order%0Afinite%20volume%20schemes.%20The%20method%20achieves%20gains%20of%2020-60%25%20in%20solution%20accuracy%0Awhile%20enhancing%20computational%20efficiency.%20A%20convergence%20study%20has%20been%20conveyed%0Aand%20reveal%20improved%20mesh%20convergence%20rates%20compared%20to%20the%20conventional%20solver.%0AThe%20proposed%20algorithm%20is%20faster%20and%20more%20accurate%20than%20the%20traditional%0Asecond-order%20finite%20volume%20solver%2C%20enabling%20high-fidelity%20simulations%20on%0Acoarser%20grids%20while%20preserving%20the%20stability%20and%20conservation%20properties%0Aessential%20for%20hyperbolic%20conservation%20laws.%20This%20work%20is%20a%20part%20of%20a%20new%0Ageneration%20of%20solvers%20that%20are%20built%20by%20combining%20Machine-Learning%20%28ML%29%20tools%0Awith%20traditional%20numerical%20schemes%2C%20all%20while%20ensuring%20physical%20constraint%20on%0Athe%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Adaptive%2520Gradient%2520Recovery%2520for%2520Unstructured%2520Finite%2520Volume%250A%2520%2520Computations%26entry.906535625%3DG.%2520de%2520Rom%25C3%25A9mont%2520and%2520F.%2520Renac%2520and%2520F.%2520Chinesta%2520and%2520J.%2520Nunez%2520and%2520D.%2520Gueyffier%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520data-driven%2520approach%2520for%2520enhancing%2520gradient%2520reconstruction%250Ain%2520unstructured%2520finite%2520volume%2520methods%2520for%2520hyperbolic%2520conservation%2520laws%252C%250Aspecifically%2520for%2520the%25202D%2520Euler%2520equations.%2520Our%2520approach%2520extends%2520previous%250Astructured-grid%2520methodologies%2520to%2520unstructured%2520meshes%2520through%2520a%2520modified%250ADeepONet%2520architecture%2520that%2520incorporates%2520local%2520geometry%2520in%2520the%2520neural%2520network.%250AThe%2520architecture%2520employs%2520local%2520mesh%2520topology%2520to%2520ensure%2520rotation%2520invariance%252C%250Awhile%2520also%2520ensuring%2520first-order%2520constraint%2520on%2520the%2520learned%2520operator.%2520The%250Atraining%2520methodology%2520incorporates%2520physics-informed%2520regularization%2520through%250Aentropy%2520penalization%252C%2520total%2520variation%2520diminishing%2520penalization%252C%2520and%2520parameter%250Aregularization%2520to%2520ensure%2520physically%2520consistent%2520solutions%252C%2520particularly%2520in%250Ashock-dominated%2520regions.%2520The%2520model%2520is%2520trained%2520on%2520high-fidelity%2520datasets%250Asolutions%2520derived%2520from%2520sine%2520waves%2520and%2520randomized%2520piecewise%2520constant%2520initial%250Aconditions%2520with%2520periodic%2520boundary%2520conditions%252C%2520enabling%2520robust%2520generalization%2520to%250Acomplex%2520flow%2520configurations%2520or%2520geometries.%2520Validation%2520test%2520cases%2520from%2520the%250Aliterature%252C%2520including%2520challenging%2520geometry%2520configuration%252C%2520demonstrates%250Asubstantial%2520improvements%2520in%2520accuracy%2520compared%2520to%2520traditional%2520second-order%250Afinite%2520volume%2520schemes.%2520The%2520method%2520achieves%2520gains%2520of%252020-60%2525%2520in%2520solution%2520accuracy%250Awhile%2520enhancing%2520computational%2520efficiency.%2520A%2520convergence%2520study%2520has%2520been%2520conveyed%250Aand%2520reveal%2520improved%2520mesh%2520convergence%2520rates%2520compared%2520to%2520the%2520conventional%2520solver.%250AThe%2520proposed%2520algorithm%2520is%2520faster%2520and%2520more%2520accurate%2520than%2520the%2520traditional%250Asecond-order%2520finite%2520volume%2520solver%252C%2520enabling%2520high-fidelity%2520simulations%2520on%250Acoarser%2520grids%2520while%2520preserving%2520the%2520stability%2520and%2520conservation%2520properties%250Aessential%2520for%2520hyperbolic%2520conservation%2520laws.%2520This%2520work%2520is%2520a%2520part%2520of%2520a%2520new%250Ageneration%2520of%2520solvers%2520that%2520are%2520built%2520by%2520combining%2520Machine-Learning%2520%2528ML%2529%2520tools%250Awith%2520traditional%2520numerical%2520schemes%252C%2520all%2520while%2520ensuring%2520physical%2520constraint%2520on%250Athe%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Adaptive%20Gradient%20Recovery%20for%20Unstructured%20Finite%20Volume%0A%20%20Computations&entry.906535625=G.%20de%20Rom%C3%A9mont%20and%20F.%20Renac%20and%20F.%20Chinesta%20and%20J.%20Nunez%20and%20D.%20Gueyffier&entry.1292438233=%20%20We%20present%20a%20novel%20data-driven%20approach%20for%20enhancing%20gradient%20reconstruction%0Ain%20unstructured%20finite%20volume%20methods%20for%20hyperbolic%20conservation%20laws%2C%0Aspecifically%20for%20the%202D%20Euler%20equations.%20Our%20approach%20extends%20previous%0Astructured-grid%20methodologies%20to%20unstructured%20meshes%20through%20a%20modified%0ADeepONet%20architecture%20that%20incorporates%20local%20geometry%20in%20the%20neural%20network.%0AThe%20architecture%20employs%20local%20mesh%20topology%20to%20ensure%20rotation%20invariance%2C%0Awhile%20also%20ensuring%20first-order%20constraint%20on%20the%20learned%20operator.%20The%0Atraining%20methodology%20incorporates%20physics-informed%20regularization%20through%0Aentropy%20penalization%2C%20total%20variation%20diminishing%20penalization%2C%20and%20parameter%0Aregularization%20to%20ensure%20physically%20consistent%20solutions%2C%20particularly%20in%0Ashock-dominated%20regions.%20The%20model%20is%20trained%20on%20high-fidelity%20datasets%0Asolutions%20derived%20from%20sine%20waves%20and%20randomized%20piecewise%20constant%20initial%0Aconditions%20with%20periodic%20boundary%20conditions%2C%20enabling%20robust%20generalization%20to%0Acomplex%20flow%20configurations%20or%20geometries.%20Validation%20test%20cases%20from%20the%0Aliterature%2C%20including%20challenging%20geometry%20configuration%2C%20demonstrates%0Asubstantial%20improvements%20in%20accuracy%20compared%20to%20traditional%20second-order%0Afinite%20volume%20schemes.%20The%20method%20achieves%20gains%20of%2020-60%25%20in%20solution%20accuracy%0Awhile%20enhancing%20computational%20efficiency.%20A%20convergence%20study%20has%20been%20conveyed%0Aand%20reveal%20improved%20mesh%20convergence%20rates%20compared%20to%20the%20conventional%20solver.%0AThe%20proposed%20algorithm%20is%20faster%20and%20more%20accurate%20than%20the%20traditional%0Asecond-order%20finite%20volume%20solver%2C%20enabling%20high-fidelity%20simulations%20on%0Acoarser%20grids%20while%20preserving%20the%20stability%20and%20conservation%20properties%0Aessential%20for%20hyperbolic%20conservation%20laws.%20This%20work%20is%20a%20part%20of%20a%20new%0Ageneration%20of%20solvers%20that%20are%20built%20by%20combining%20Machine-Learning%20%28ML%29%20tools%0Awith%20traditional%20numerical%20schemes%2C%20all%20while%20ensuring%20physical%20constraint%20on%0Athe%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16571v1&entry.124074799=Read"},
{"title": "GASPnet: Global Agreement to Synchronize Phases", "author": "Andrea Alamiaa and Sabine Muzellec and Thomas Serre and Rufin VanRullen", "abstract": "  In recent years, Transformer architectures have revolutionized most fields of\nartificial intelligence, relying on an attentional mechanism based on the\nagreement between keys and queries to select and route information in the\nnetwork. In previous work, we introduced a novel, brain-inspired architecture\nthat leverages a similar implementation to achieve a global 'routing by\nagreement' mechanism. Such a system modulates the network's activity by\nmatching each neuron's key with a single global query, pooled across the entire\nnetwork. Acting as a global attentional system, this mechanism improves noise\nrobustness over baseline levels but is insufficient for multi-classification\ntasks. Here, we improve on this work by proposing a novel mechanism that\ncombines aspects of the Transformer attentional operations with a compelling\nneuroscience theory, namely, binding by synchrony. This theory proposes that\nthe brain binds together features by synchronizing the temporal activity of\nneurons encoding those features. This allows the binding of features from the\nsame object while efficiently disentangling those from distinct objects. We\ndrew inspiration from this theory and incorporated angular phases into all\nlayers of a convolutional network. After achieving phase alignment via Kuramoto\ndynamics, we use this approach to enhance operations between neurons with\nsimilar phases and suppresses those with opposite phases. We test the benefits\nof this mechanism on two datasets: one composed of pairs of digits and one\ncomposed of a combination of an MNIST item superimposed on a CIFAR-10 image.\nOur results reveal better accuracy than CNN networks, proving more robust to\nnoise and with better generalization abilities. Overall, we propose a novel\nmechanism that addresses the visual binding problem in neural networks by\nleveraging the synergy between neuroscience and machine learning.\n", "link": "http://arxiv.org/abs/2507.16674v1", "date": "2025-07-22", "relevancy": 2.1646, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5627}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GASPnet%3A%20Global%20Agreement%20to%20Synchronize%20Phases&body=Title%3A%20GASPnet%3A%20Global%20Agreement%20to%20Synchronize%20Phases%0AAuthor%3A%20Andrea%20Alamiaa%20and%20Sabine%20Muzellec%20and%20Thomas%20Serre%20and%20Rufin%20VanRullen%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Transformer%20architectures%20have%20revolutionized%20most%20fields%20of%0Aartificial%20intelligence%2C%20relying%20on%20an%20attentional%20mechanism%20based%20on%20the%0Aagreement%20between%20keys%20and%20queries%20to%20select%20and%20route%20information%20in%20the%0Anetwork.%20In%20previous%20work%2C%20we%20introduced%20a%20novel%2C%20brain-inspired%20architecture%0Athat%20leverages%20a%20similar%20implementation%20to%20achieve%20a%20global%20%27routing%20by%0Aagreement%27%20mechanism.%20Such%20a%20system%20modulates%20the%20network%27s%20activity%20by%0Amatching%20each%20neuron%27s%20key%20with%20a%20single%20global%20query%2C%20pooled%20across%20the%20entire%0Anetwork.%20Acting%20as%20a%20global%20attentional%20system%2C%20this%20mechanism%20improves%20noise%0Arobustness%20over%20baseline%20levels%20but%20is%20insufficient%20for%20multi-classification%0Atasks.%20Here%2C%20we%20improve%20on%20this%20work%20by%20proposing%20a%20novel%20mechanism%20that%0Acombines%20aspects%20of%20the%20Transformer%20attentional%20operations%20with%20a%20compelling%0Aneuroscience%20theory%2C%20namely%2C%20binding%20by%20synchrony.%20This%20theory%20proposes%20that%0Athe%20brain%20binds%20together%20features%20by%20synchronizing%20the%20temporal%20activity%20of%0Aneurons%20encoding%20those%20features.%20This%20allows%20the%20binding%20of%20features%20from%20the%0Asame%20object%20while%20efficiently%20disentangling%20those%20from%20distinct%20objects.%20We%0Adrew%20inspiration%20from%20this%20theory%20and%20incorporated%20angular%20phases%20into%20all%0Alayers%20of%20a%20convolutional%20network.%20After%20achieving%20phase%20alignment%20via%20Kuramoto%0Adynamics%2C%20we%20use%20this%20approach%20to%20enhance%20operations%20between%20neurons%20with%0Asimilar%20phases%20and%20suppresses%20those%20with%20opposite%20phases.%20We%20test%20the%20benefits%0Aof%20this%20mechanism%20on%20two%20datasets%3A%20one%20composed%20of%20pairs%20of%20digits%20and%20one%0Acomposed%20of%20a%20combination%20of%20an%20MNIST%20item%20superimposed%20on%20a%20CIFAR-10%20image.%0AOur%20results%20reveal%20better%20accuracy%20than%20CNN%20networks%2C%20proving%20more%20robust%20to%0Anoise%20and%20with%20better%20generalization%20abilities.%20Overall%2C%20we%20propose%20a%20novel%0Amechanism%20that%20addresses%20the%20visual%20binding%20problem%20in%20neural%20networks%20by%0Aleveraging%20the%20synergy%20between%20neuroscience%20and%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGASPnet%253A%2520Global%2520Agreement%2520to%2520Synchronize%2520Phases%26entry.906535625%3DAndrea%2520Alamiaa%2520and%2520Sabine%2520Muzellec%2520and%2520Thomas%2520Serre%2520and%2520Rufin%2520VanRullen%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Transformer%2520architectures%2520have%2520revolutionized%2520most%2520fields%2520of%250Aartificial%2520intelligence%252C%2520relying%2520on%2520an%2520attentional%2520mechanism%2520based%2520on%2520the%250Aagreement%2520between%2520keys%2520and%2520queries%2520to%2520select%2520and%2520route%2520information%2520in%2520the%250Anetwork.%2520In%2520previous%2520work%252C%2520we%2520introduced%2520a%2520novel%252C%2520brain-inspired%2520architecture%250Athat%2520leverages%2520a%2520similar%2520implementation%2520to%2520achieve%2520a%2520global%2520%2527routing%2520by%250Aagreement%2527%2520mechanism.%2520Such%2520a%2520system%2520modulates%2520the%2520network%2527s%2520activity%2520by%250Amatching%2520each%2520neuron%2527s%2520key%2520with%2520a%2520single%2520global%2520query%252C%2520pooled%2520across%2520the%2520entire%250Anetwork.%2520Acting%2520as%2520a%2520global%2520attentional%2520system%252C%2520this%2520mechanism%2520improves%2520noise%250Arobustness%2520over%2520baseline%2520levels%2520but%2520is%2520insufficient%2520for%2520multi-classification%250Atasks.%2520Here%252C%2520we%2520improve%2520on%2520this%2520work%2520by%2520proposing%2520a%2520novel%2520mechanism%2520that%250Acombines%2520aspects%2520of%2520the%2520Transformer%2520attentional%2520operations%2520with%2520a%2520compelling%250Aneuroscience%2520theory%252C%2520namely%252C%2520binding%2520by%2520synchrony.%2520This%2520theory%2520proposes%2520that%250Athe%2520brain%2520binds%2520together%2520features%2520by%2520synchronizing%2520the%2520temporal%2520activity%2520of%250Aneurons%2520encoding%2520those%2520features.%2520This%2520allows%2520the%2520binding%2520of%2520features%2520from%2520the%250Asame%2520object%2520while%2520efficiently%2520disentangling%2520those%2520from%2520distinct%2520objects.%2520We%250Adrew%2520inspiration%2520from%2520this%2520theory%2520and%2520incorporated%2520angular%2520phases%2520into%2520all%250Alayers%2520of%2520a%2520convolutional%2520network.%2520After%2520achieving%2520phase%2520alignment%2520via%2520Kuramoto%250Adynamics%252C%2520we%2520use%2520this%2520approach%2520to%2520enhance%2520operations%2520between%2520neurons%2520with%250Asimilar%2520phases%2520and%2520suppresses%2520those%2520with%2520opposite%2520phases.%2520We%2520test%2520the%2520benefits%250Aof%2520this%2520mechanism%2520on%2520two%2520datasets%253A%2520one%2520composed%2520of%2520pairs%2520of%2520digits%2520and%2520one%250Acomposed%2520of%2520a%2520combination%2520of%2520an%2520MNIST%2520item%2520superimposed%2520on%2520a%2520CIFAR-10%2520image.%250AOur%2520results%2520reveal%2520better%2520accuracy%2520than%2520CNN%2520networks%252C%2520proving%2520more%2520robust%2520to%250Anoise%2520and%2520with%2520better%2520generalization%2520abilities.%2520Overall%252C%2520we%2520propose%2520a%2520novel%250Amechanism%2520that%2520addresses%2520the%2520visual%2520binding%2520problem%2520in%2520neural%2520networks%2520by%250Aleveraging%2520the%2520synergy%2520between%2520neuroscience%2520and%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GASPnet%3A%20Global%20Agreement%20to%20Synchronize%20Phases&entry.906535625=Andrea%20Alamiaa%20and%20Sabine%20Muzellec%20and%20Thomas%20Serre%20and%20Rufin%20VanRullen&entry.1292438233=%20%20In%20recent%20years%2C%20Transformer%20architectures%20have%20revolutionized%20most%20fields%20of%0Aartificial%20intelligence%2C%20relying%20on%20an%20attentional%20mechanism%20based%20on%20the%0Aagreement%20between%20keys%20and%20queries%20to%20select%20and%20route%20information%20in%20the%0Anetwork.%20In%20previous%20work%2C%20we%20introduced%20a%20novel%2C%20brain-inspired%20architecture%0Athat%20leverages%20a%20similar%20implementation%20to%20achieve%20a%20global%20%27routing%20by%0Aagreement%27%20mechanism.%20Such%20a%20system%20modulates%20the%20network%27s%20activity%20by%0Amatching%20each%20neuron%27s%20key%20with%20a%20single%20global%20query%2C%20pooled%20across%20the%20entire%0Anetwork.%20Acting%20as%20a%20global%20attentional%20system%2C%20this%20mechanism%20improves%20noise%0Arobustness%20over%20baseline%20levels%20but%20is%20insufficient%20for%20multi-classification%0Atasks.%20Here%2C%20we%20improve%20on%20this%20work%20by%20proposing%20a%20novel%20mechanism%20that%0Acombines%20aspects%20of%20the%20Transformer%20attentional%20operations%20with%20a%20compelling%0Aneuroscience%20theory%2C%20namely%2C%20binding%20by%20synchrony.%20This%20theory%20proposes%20that%0Athe%20brain%20binds%20together%20features%20by%20synchronizing%20the%20temporal%20activity%20of%0Aneurons%20encoding%20those%20features.%20This%20allows%20the%20binding%20of%20features%20from%20the%0Asame%20object%20while%20efficiently%20disentangling%20those%20from%20distinct%20objects.%20We%0Adrew%20inspiration%20from%20this%20theory%20and%20incorporated%20angular%20phases%20into%20all%0Alayers%20of%20a%20convolutional%20network.%20After%20achieving%20phase%20alignment%20via%20Kuramoto%0Adynamics%2C%20we%20use%20this%20approach%20to%20enhance%20operations%20between%20neurons%20with%0Asimilar%20phases%20and%20suppresses%20those%20with%20opposite%20phases.%20We%20test%20the%20benefits%0Aof%20this%20mechanism%20on%20two%20datasets%3A%20one%20composed%20of%20pairs%20of%20digits%20and%20one%0Acomposed%20of%20a%20combination%20of%20an%20MNIST%20item%20superimposed%20on%20a%20CIFAR-10%20image.%0AOur%20results%20reveal%20better%20accuracy%20than%20CNN%20networks%2C%20proving%20more%20robust%20to%0Anoise%20and%20with%20better%20generalization%20abilities.%20Overall%2C%20we%20propose%20a%20novel%0Amechanism%20that%20addresses%20the%20visual%20binding%20problem%20in%20neural%20networks%20by%0Aleveraging%20the%20synergy%20between%20neuroscience%20and%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16674v1&entry.124074799=Read"},
{"title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological\n  Pathway Reasoning", "author": "Haiteng Zhao and Chang Ma and Fangzhi Xu and Lingpeng Kong and Zhi-Hong Deng", "abstract": "  The applications of large language models (LLMs) in various biological\ndomains have been explored recently, but their reasoning ability in complex\nbiological systems, such as pathways, remains underexplored, which is crucial\nfor predicting biological phenomena, formulating hypotheses, and designing\nexperiments. This work explores the potential of LLMs in pathway reasoning. We\nintroduce BioMaze, a dataset with 5.1K complex pathway problems derived from\nreal research, covering various biological contexts including natural dynamic\nchanges, disturbances, additional intervention conditions, and multi-scale\nresearch targets. Our evaluation of methods such as CoT and graph-augmented\nreasoning, shows that LLMs struggle with pathway reasoning, especially in\nperturbed systems. To address this, we propose PathSeeker, an LLM agent that\nenhances reasoning through interactive subgraph-based navigation, enabling a\nmore effective approach to handling the complexities of biological systems in a\nscientifically aligned manner. The dataset and code are available at\nhttps://github.com/zhao-ht/BioMaze.\n", "link": "http://arxiv.org/abs/2502.16660v5", "date": "2025-07-22", "relevancy": 2.1605, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BioMaze%3A%20Benchmarking%20and%20Enhancing%20Large%20Language%20Models%20for%20Biological%0A%20%20Pathway%20Reasoning&body=Title%3A%20BioMaze%3A%20Benchmarking%20and%20Enhancing%20Large%20Language%20Models%20for%20Biological%0A%20%20Pathway%20Reasoning%0AAuthor%3A%20Haiteng%20Zhao%20and%20Chang%20Ma%20and%20Fangzhi%20Xu%20and%20Lingpeng%20Kong%20and%20Zhi-Hong%20Deng%0AAbstract%3A%20%20%20The%20applications%20of%20large%20language%20models%20%28LLMs%29%20in%20various%20biological%0Adomains%20have%20been%20explored%20recently%2C%20but%20their%20reasoning%20ability%20in%20complex%0Abiological%20systems%2C%20such%20as%20pathways%2C%20remains%20underexplored%2C%20which%20is%20crucial%0Afor%20predicting%20biological%20phenomena%2C%20formulating%20hypotheses%2C%20and%20designing%0Aexperiments.%20This%20work%20explores%20the%20potential%20of%20LLMs%20in%20pathway%20reasoning.%20We%0Aintroduce%20BioMaze%2C%20a%20dataset%20with%205.1K%20complex%20pathway%20problems%20derived%20from%0Areal%20research%2C%20covering%20various%20biological%20contexts%20including%20natural%20dynamic%0Achanges%2C%20disturbances%2C%20additional%20intervention%20conditions%2C%20and%20multi-scale%0Aresearch%20targets.%20Our%20evaluation%20of%20methods%20such%20as%20CoT%20and%20graph-augmented%0Areasoning%2C%20shows%20that%20LLMs%20struggle%20with%20pathway%20reasoning%2C%20especially%20in%0Aperturbed%20systems.%20To%20address%20this%2C%20we%20propose%20PathSeeker%2C%20an%20LLM%20agent%20that%0Aenhances%20reasoning%20through%20interactive%20subgraph-based%20navigation%2C%20enabling%20a%0Amore%20effective%20approach%20to%20handling%20the%20complexities%20of%20biological%20systems%20in%20a%0Ascientifically%20aligned%20manner.%20The%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/zhao-ht/BioMaze.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16660v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBioMaze%253A%2520Benchmarking%2520and%2520Enhancing%2520Large%2520Language%2520Models%2520for%2520Biological%250A%2520%2520Pathway%2520Reasoning%26entry.906535625%3DHaiteng%2520Zhao%2520and%2520Chang%2520Ma%2520and%2520Fangzhi%2520Xu%2520and%2520Lingpeng%2520Kong%2520and%2520Zhi-Hong%2520Deng%26entry.1292438233%3D%2520%2520The%2520applications%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520various%2520biological%250Adomains%2520have%2520been%2520explored%2520recently%252C%2520but%2520their%2520reasoning%2520ability%2520in%2520complex%250Abiological%2520systems%252C%2520such%2520as%2520pathways%252C%2520remains%2520underexplored%252C%2520which%2520is%2520crucial%250Afor%2520predicting%2520biological%2520phenomena%252C%2520formulating%2520hypotheses%252C%2520and%2520designing%250Aexperiments.%2520This%2520work%2520explores%2520the%2520potential%2520of%2520LLMs%2520in%2520pathway%2520reasoning.%2520We%250Aintroduce%2520BioMaze%252C%2520a%2520dataset%2520with%25205.1K%2520complex%2520pathway%2520problems%2520derived%2520from%250Areal%2520research%252C%2520covering%2520various%2520biological%2520contexts%2520including%2520natural%2520dynamic%250Achanges%252C%2520disturbances%252C%2520additional%2520intervention%2520conditions%252C%2520and%2520multi-scale%250Aresearch%2520targets.%2520Our%2520evaluation%2520of%2520methods%2520such%2520as%2520CoT%2520and%2520graph-augmented%250Areasoning%252C%2520shows%2520that%2520LLMs%2520struggle%2520with%2520pathway%2520reasoning%252C%2520especially%2520in%250Aperturbed%2520systems.%2520To%2520address%2520this%252C%2520we%2520propose%2520PathSeeker%252C%2520an%2520LLM%2520agent%2520that%250Aenhances%2520reasoning%2520through%2520interactive%2520subgraph-based%2520navigation%252C%2520enabling%2520a%250Amore%2520effective%2520approach%2520to%2520handling%2520the%2520complexities%2520of%2520biological%2520systems%2520in%2520a%250Ascientifically%2520aligned%2520manner.%2520The%2520dataset%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/zhao-ht/BioMaze.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16660v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BioMaze%3A%20Benchmarking%20and%20Enhancing%20Large%20Language%20Models%20for%20Biological%0A%20%20Pathway%20Reasoning&entry.906535625=Haiteng%20Zhao%20and%20Chang%20Ma%20and%20Fangzhi%20Xu%20and%20Lingpeng%20Kong%20and%20Zhi-Hong%20Deng&entry.1292438233=%20%20The%20applications%20of%20large%20language%20models%20%28LLMs%29%20in%20various%20biological%0Adomains%20have%20been%20explored%20recently%2C%20but%20their%20reasoning%20ability%20in%20complex%0Abiological%20systems%2C%20such%20as%20pathways%2C%20remains%20underexplored%2C%20which%20is%20crucial%0Afor%20predicting%20biological%20phenomena%2C%20formulating%20hypotheses%2C%20and%20designing%0Aexperiments.%20This%20work%20explores%20the%20potential%20of%20LLMs%20in%20pathway%20reasoning.%20We%0Aintroduce%20BioMaze%2C%20a%20dataset%20with%205.1K%20complex%20pathway%20problems%20derived%20from%0Areal%20research%2C%20covering%20various%20biological%20contexts%20including%20natural%20dynamic%0Achanges%2C%20disturbances%2C%20additional%20intervention%20conditions%2C%20and%20multi-scale%0Aresearch%20targets.%20Our%20evaluation%20of%20methods%20such%20as%20CoT%20and%20graph-augmented%0Areasoning%2C%20shows%20that%20LLMs%20struggle%20with%20pathway%20reasoning%2C%20especially%20in%0Aperturbed%20systems.%20To%20address%20this%2C%20we%20propose%20PathSeeker%2C%20an%20LLM%20agent%20that%0Aenhances%20reasoning%20through%20interactive%20subgraph-based%20navigation%2C%20enabling%20a%0Amore%20effective%20approach%20to%20handling%20the%20complexities%20of%20biological%20systems%20in%20a%0Ascientifically%20aligned%20manner.%20The%20dataset%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/zhao-ht/BioMaze.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16660v5&entry.124074799=Read"},
{"title": "InternAgent: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification", "author": " InternAgent Team and Bo Zhang and Shiyang Feng and Xiangchao Yan and Jiakang Yuan and Runmin Ma and Yusong Hu and Zhiyin Yu and Xiaohan He and Songtao Huang and Shaowei Hou and Zheng Nie and Zhilong Wang and Jinyao Liu and Tianshuo Peng and Peng Ye and Dongzhan Zhou and Shufei Zhang and Xiaosong Wang and Yilan Zhang and Meng Li and Zhongying Tu and Xiangyu Yue and Wangli Ouyang and Bowen Zhou and Lei Bai", "abstract": "  Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce InternAgent, a unified closed-loop multi-agent\nframework to conduct Autonomous Scientific Research (ASR) across various\nscientific research fields, enabling researchers to tackle complicated problems\nin these fields with unprecedented speed and precision. InternAgent highlights\nthree key advantages: 1) Scalability: InternAgent has demonstrated its\nversatility across 12 scientific research tasks, capable of generating\ninnovative ideas to enhance the performance of baseline code. 2) Interactivity:\nInternAgent provides an interface for human expert feedback and multi-agent\ninteraction in automated end-to-end processes, allowing for the seamless\nintegration of domain expert knowledge. 3) Efficiency: InternAgent has achieved\npromising performance gains in several scientific fields with significantly\nless time cost compared to human efforts. For instance, in reaction yield\nprediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer\nactivity prediction, accuracy rose from 0.65 to 0.79 with only 4 hours of\nprocessing; and in 2D semantic segmentation, precision advanced from 78.8% to\n81.0% in a mere 30 hours.\n", "link": "http://arxiv.org/abs/2505.16938v3", "date": "2025-07-22", "relevancy": 2.159, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5513}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5355}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternAgent%3A%20When%20Agent%20Becomes%20the%20Scientist%20--%20Building%20Closed-Loop%0A%20%20System%20from%20Hypothesis%20to%20Verification&body=Title%3A%20InternAgent%3A%20When%20Agent%20Becomes%20the%20Scientist%20--%20Building%20Closed-Loop%0A%20%20System%20from%20Hypothesis%20to%20Verification%0AAuthor%3A%20%20InternAgent%20Team%20and%20Bo%20Zhang%20and%20Shiyang%20Feng%20and%20Xiangchao%20Yan%20and%20Jiakang%20Yuan%20and%20Runmin%20Ma%20and%20Yusong%20Hu%20and%20Zhiyin%20Yu%20and%20Xiaohan%20He%20and%20Songtao%20Huang%20and%20Shaowei%20Hou%20and%20Zheng%20Nie%20and%20Zhilong%20Wang%20and%20Jinyao%20Liu%20and%20Tianshuo%20Peng%20and%20Peng%20Ye%20and%20Dongzhan%20Zhou%20and%20Shufei%20Zhang%20and%20Xiaosong%20Wang%20and%20Yilan%20Zhang%20and%20Meng%20Li%20and%20Zhongying%20Tu%20and%20Xiangyu%20Yue%20and%20Wangli%20Ouyang%20and%20Bowen%20Zhou%20and%20Lei%20Bai%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20is%20accelerating%20the%20transformation%20of%20scientific%0Aresearch%20paradigms%2C%20not%20only%20enhancing%20research%20efficiency%20but%20also%20driving%0Ainnovation.%20We%20introduce%20InternAgent%2C%20a%20unified%20closed-loop%20multi-agent%0Aframework%20to%20conduct%20Autonomous%20Scientific%20Research%20%28ASR%29%20across%20various%0Ascientific%20research%20fields%2C%20enabling%20researchers%20to%20tackle%20complicated%20problems%0Ain%20these%20fields%20with%20unprecedented%20speed%20and%20precision.%20InternAgent%20highlights%0Athree%20key%20advantages%3A%201%29%20Scalability%3A%20InternAgent%20has%20demonstrated%20its%0Aversatility%20across%2012%20scientific%20research%20tasks%2C%20capable%20of%20generating%0Ainnovative%20ideas%20to%20enhance%20the%20performance%20of%20baseline%20code.%202%29%20Interactivity%3A%0AInternAgent%20provides%20an%20interface%20for%20human%20expert%20feedback%20and%20multi-agent%0Ainteraction%20in%20automated%20end-to-end%20processes%2C%20allowing%20for%20the%20seamless%0Aintegration%20of%20domain%20expert%20knowledge.%203%29%20Efficiency%3A%20InternAgent%20has%20achieved%0Apromising%20performance%20gains%20in%20several%20scientific%20fields%20with%20significantly%0Aless%20time%20cost%20compared%20to%20human%20efforts.%20For%20instance%2C%20in%20reaction%20yield%0Aprediction%2C%20it%20increased%20from%2027.6%25%20to%2035.4%25%20in%20just%2012%20hours%3B%20in%20enhancer%0Aactivity%20prediction%2C%20accuracy%20rose%20from%200.65%20to%200.79%20with%20only%204%20hours%20of%0Aprocessing%3B%20and%20in%202D%20semantic%20segmentation%2C%20precision%20advanced%20from%2078.8%25%20to%0A81.0%25%20in%20a%20mere%2030%20hours.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.16938v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternAgent%253A%2520When%2520Agent%2520Becomes%2520the%2520Scientist%2520--%2520Building%2520Closed-Loop%250A%2520%2520System%2520from%2520Hypothesis%2520to%2520Verification%26entry.906535625%3D%2520InternAgent%2520Team%2520and%2520Bo%2520Zhang%2520and%2520Shiyang%2520Feng%2520and%2520Xiangchao%2520Yan%2520and%2520Jiakang%2520Yuan%2520and%2520Runmin%2520Ma%2520and%2520Yusong%2520Hu%2520and%2520Zhiyin%2520Yu%2520and%2520Xiaohan%2520He%2520and%2520Songtao%2520Huang%2520and%2520Shaowei%2520Hou%2520and%2520Zheng%2520Nie%2520and%2520Zhilong%2520Wang%2520and%2520Jinyao%2520Liu%2520and%2520Tianshuo%2520Peng%2520and%2520Peng%2520Ye%2520and%2520Dongzhan%2520Zhou%2520and%2520Shufei%2520Zhang%2520and%2520Xiaosong%2520Wang%2520and%2520Yilan%2520Zhang%2520and%2520Meng%2520Li%2520and%2520Zhongying%2520Tu%2520and%2520Xiangyu%2520Yue%2520and%2520Wangli%2520Ouyang%2520and%2520Bowen%2520Zhou%2520and%2520Lei%2520Bai%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520is%2520accelerating%2520the%2520transformation%2520of%2520scientific%250Aresearch%2520paradigms%252C%2520not%2520only%2520enhancing%2520research%2520efficiency%2520but%2520also%2520driving%250Ainnovation.%2520We%2520introduce%2520InternAgent%252C%2520a%2520unified%2520closed-loop%2520multi-agent%250Aframework%2520to%2520conduct%2520Autonomous%2520Scientific%2520Research%2520%2528ASR%2529%2520across%2520various%250Ascientific%2520research%2520fields%252C%2520enabling%2520researchers%2520to%2520tackle%2520complicated%2520problems%250Ain%2520these%2520fields%2520with%2520unprecedented%2520speed%2520and%2520precision.%2520InternAgent%2520highlights%250Athree%2520key%2520advantages%253A%25201%2529%2520Scalability%253A%2520InternAgent%2520has%2520demonstrated%2520its%250Aversatility%2520across%252012%2520scientific%2520research%2520tasks%252C%2520capable%2520of%2520generating%250Ainnovative%2520ideas%2520to%2520enhance%2520the%2520performance%2520of%2520baseline%2520code.%25202%2529%2520Interactivity%253A%250AInternAgent%2520provides%2520an%2520interface%2520for%2520human%2520expert%2520feedback%2520and%2520multi-agent%250Ainteraction%2520in%2520automated%2520end-to-end%2520processes%252C%2520allowing%2520for%2520the%2520seamless%250Aintegration%2520of%2520domain%2520expert%2520knowledge.%25203%2529%2520Efficiency%253A%2520InternAgent%2520has%2520achieved%250Apromising%2520performance%2520gains%2520in%2520several%2520scientific%2520fields%2520with%2520significantly%250Aless%2520time%2520cost%2520compared%2520to%2520human%2520efforts.%2520For%2520instance%252C%2520in%2520reaction%2520yield%250Aprediction%252C%2520it%2520increased%2520from%252027.6%2525%2520to%252035.4%2525%2520in%2520just%252012%2520hours%253B%2520in%2520enhancer%250Aactivity%2520prediction%252C%2520accuracy%2520rose%2520from%25200.65%2520to%25200.79%2520with%2520only%25204%2520hours%2520of%250Aprocessing%253B%2520and%2520in%25202D%2520semantic%2520segmentation%252C%2520precision%2520advanced%2520from%252078.8%2525%2520to%250A81.0%2525%2520in%2520a%2520mere%252030%2520hours.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.16938v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternAgent%3A%20When%20Agent%20Becomes%20the%20Scientist%20--%20Building%20Closed-Loop%0A%20%20System%20from%20Hypothesis%20to%20Verification&entry.906535625=%20InternAgent%20Team%20and%20Bo%20Zhang%20and%20Shiyang%20Feng%20and%20Xiangchao%20Yan%20and%20Jiakang%20Yuan%20and%20Runmin%20Ma%20and%20Yusong%20Hu%20and%20Zhiyin%20Yu%20and%20Xiaohan%20He%20and%20Songtao%20Huang%20and%20Shaowei%20Hou%20and%20Zheng%20Nie%20and%20Zhilong%20Wang%20and%20Jinyao%20Liu%20and%20Tianshuo%20Peng%20and%20Peng%20Ye%20and%20Dongzhan%20Zhou%20and%20Shufei%20Zhang%20and%20Xiaosong%20Wang%20and%20Yilan%20Zhang%20and%20Meng%20Li%20and%20Zhongying%20Tu%20and%20Xiangyu%20Yue%20and%20Wangli%20Ouyang%20and%20Bowen%20Zhou%20and%20Lei%20Bai&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20is%20accelerating%20the%20transformation%20of%20scientific%0Aresearch%20paradigms%2C%20not%20only%20enhancing%20research%20efficiency%20but%20also%20driving%0Ainnovation.%20We%20introduce%20InternAgent%2C%20a%20unified%20closed-loop%20multi-agent%0Aframework%20to%20conduct%20Autonomous%20Scientific%20Research%20%28ASR%29%20across%20various%0Ascientific%20research%20fields%2C%20enabling%20researchers%20to%20tackle%20complicated%20problems%0Ain%20these%20fields%20with%20unprecedented%20speed%20and%20precision.%20InternAgent%20highlights%0Athree%20key%20advantages%3A%201%29%20Scalability%3A%20InternAgent%20has%20demonstrated%20its%0Aversatility%20across%2012%20scientific%20research%20tasks%2C%20capable%20of%20generating%0Ainnovative%20ideas%20to%20enhance%20the%20performance%20of%20baseline%20code.%202%29%20Interactivity%3A%0AInternAgent%20provides%20an%20interface%20for%20human%20expert%20feedback%20and%20multi-agent%0Ainteraction%20in%20automated%20end-to-end%20processes%2C%20allowing%20for%20the%20seamless%0Aintegration%20of%20domain%20expert%20knowledge.%203%29%20Efficiency%3A%20InternAgent%20has%20achieved%0Apromising%20performance%20gains%20in%20several%20scientific%20fields%20with%20significantly%0Aless%20time%20cost%20compared%20to%20human%20efforts.%20For%20instance%2C%20in%20reaction%20yield%0Aprediction%2C%20it%20increased%20from%2027.6%25%20to%2035.4%25%20in%20just%2012%20hours%3B%20in%20enhancer%0Aactivity%20prediction%2C%20accuracy%20rose%20from%200.65%20to%200.79%20with%20only%204%20hours%20of%0Aprocessing%3B%20and%20in%202D%20semantic%20segmentation%2C%20precision%20advanced%20from%2078.8%25%20to%0A81.0%25%20in%20a%20mere%2030%20hours.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.16938v3&entry.124074799=Read"},
{"title": "R1-Track: Direct Application of MLLMs to Visual Object Tracking via\n  Reinforcement Learning", "author": "Biao Wang and Wenwen Li and Jiawei Ge", "abstract": "  Visual single object tracking aims to continuously localize and estimate the\nscale of a target in subsequent video frames, given only its initial state in\nthe first frame. This task has traditionally been framed as a template matching\nproblem, evolving through major phases including correlation filters,\ntwo-stream networks, and one-stream networks with significant progress\nachieved. However, these methods typically require explicit classification and\nregression modeling, depend on supervised training with large-scale datasets,\nand are limited to the single task of tracking, lacking flexibility. In recent\nyears, multi-modal large language models (MLLMs) have advanced rapidly.\nOpen-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational\ncapabilities, demonstrate excellent performance in grounding tasks. This has\nspurred interest in applying such models directly to visual tracking. However,\nexperiments reveal that Qwen2.5-VL struggles with template matching between\nimage pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned\nQwen2.5-VL using the group relative policy optimization (GRPO) reinforcement\nlearning method on a small-scale dataset with a rule-based reward function. The\nresulting model, R1-Track, achieved notable performance on the GOT-10k\nbenchmark. R1-Track supports flexible initialization via bounding boxes or text\ndescriptions while retaining most of the original model's general capabilities.\nAnd we further discuss potential improvements for R1-Track. This rough\ntechnical report summarizes our findings as of May 2025.\n", "link": "http://arxiv.org/abs/2506.21980v3", "date": "2025-07-22", "relevancy": 2.1482, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5344}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R1-Track%3A%20Direct%20Application%20of%20MLLMs%20to%20Visual%20Object%20Tracking%20via%0A%20%20Reinforcement%20Learning&body=Title%3A%20R1-Track%3A%20Direct%20Application%20of%20MLLMs%20to%20Visual%20Object%20Tracking%20via%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Biao%20Wang%20and%20Wenwen%20Li%20and%20Jiawei%20Ge%0AAbstract%3A%20%20%20Visual%20single%20object%20tracking%20aims%20to%20continuously%20localize%20and%20estimate%20the%0Ascale%20of%20a%20target%20in%20subsequent%20video%20frames%2C%20given%20only%20its%20initial%20state%20in%0Athe%20first%20frame.%20This%20task%20has%20traditionally%20been%20framed%20as%20a%20template%20matching%0Aproblem%2C%20evolving%20through%20major%20phases%20including%20correlation%20filters%2C%0Atwo-stream%20networks%2C%20and%20one-stream%20networks%20with%20significant%20progress%0Aachieved.%20However%2C%20these%20methods%20typically%20require%20explicit%20classification%20and%0Aregression%20modeling%2C%20depend%20on%20supervised%20training%20with%20large-scale%20datasets%2C%0Aand%20are%20limited%20to%20the%20single%20task%20of%20tracking%2C%20lacking%20flexibility.%20In%20recent%0Ayears%2C%20multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20advanced%20rapidly.%0AOpen-source%20models%20like%20Qwen2.5-VL%2C%20a%20flagship%20MLLMs%20with%20strong%20foundational%0Acapabilities%2C%20demonstrate%20excellent%20performance%20in%20grounding%20tasks.%20This%20has%0Aspurred%20interest%20in%20applying%20such%20models%20directly%20to%20visual%20tracking.%20However%2C%0Aexperiments%20reveal%20that%20Qwen2.5-VL%20struggles%20with%20template%20matching%20between%0Aimage%20pairs%20%28i.e.%2C%20tracking%20tasks%29.%20Inspired%20by%20deepseek-R1%2C%20we%20fine-tuned%0AQwen2.5-VL%20using%20the%20group%20relative%20policy%20optimization%20%28GRPO%29%20reinforcement%0Alearning%20method%20on%20a%20small-scale%20dataset%20with%20a%20rule-based%20reward%20function.%20The%0Aresulting%20model%2C%20R1-Track%2C%20achieved%20notable%20performance%20on%20the%20GOT-10k%0Abenchmark.%20R1-Track%20supports%20flexible%20initialization%20via%20bounding%20boxes%20or%20text%0Adescriptions%20while%20retaining%20most%20of%20the%20original%20model%27s%20general%20capabilities.%0AAnd%20we%20further%20discuss%20potential%20improvements%20for%20R1-Track.%20This%20rough%0Atechnical%20report%20summarizes%20our%20findings%20as%20of%20May%202025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.21980v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR1-Track%253A%2520Direct%2520Application%2520of%2520MLLMs%2520to%2520Visual%2520Object%2520Tracking%2520via%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DBiao%2520Wang%2520and%2520Wenwen%2520Li%2520and%2520Jiawei%2520Ge%26entry.1292438233%3D%2520%2520Visual%2520single%2520object%2520tracking%2520aims%2520to%2520continuously%2520localize%2520and%2520estimate%2520the%250Ascale%2520of%2520a%2520target%2520in%2520subsequent%2520video%2520frames%252C%2520given%2520only%2520its%2520initial%2520state%2520in%250Athe%2520first%2520frame.%2520This%2520task%2520has%2520traditionally%2520been%2520framed%2520as%2520a%2520template%2520matching%250Aproblem%252C%2520evolving%2520through%2520major%2520phases%2520including%2520correlation%2520filters%252C%250Atwo-stream%2520networks%252C%2520and%2520one-stream%2520networks%2520with%2520significant%2520progress%250Aachieved.%2520However%252C%2520these%2520methods%2520typically%2520require%2520explicit%2520classification%2520and%250Aregression%2520modeling%252C%2520depend%2520on%2520supervised%2520training%2520with%2520large-scale%2520datasets%252C%250Aand%2520are%2520limited%2520to%2520the%2520single%2520task%2520of%2520tracking%252C%2520lacking%2520flexibility.%2520In%2520recent%250Ayears%252C%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520advanced%2520rapidly.%250AOpen-source%2520models%2520like%2520Qwen2.5-VL%252C%2520a%2520flagship%2520MLLMs%2520with%2520strong%2520foundational%250Acapabilities%252C%2520demonstrate%2520excellent%2520performance%2520in%2520grounding%2520tasks.%2520This%2520has%250Aspurred%2520interest%2520in%2520applying%2520such%2520models%2520directly%2520to%2520visual%2520tracking.%2520However%252C%250Aexperiments%2520reveal%2520that%2520Qwen2.5-VL%2520struggles%2520with%2520template%2520matching%2520between%250Aimage%2520pairs%2520%2528i.e.%252C%2520tracking%2520tasks%2529.%2520Inspired%2520by%2520deepseek-R1%252C%2520we%2520fine-tuned%250AQwen2.5-VL%2520using%2520the%2520group%2520relative%2520policy%2520optimization%2520%2528GRPO%2529%2520reinforcement%250Alearning%2520method%2520on%2520a%2520small-scale%2520dataset%2520with%2520a%2520rule-based%2520reward%2520function.%2520The%250Aresulting%2520model%252C%2520R1-Track%252C%2520achieved%2520notable%2520performance%2520on%2520the%2520GOT-10k%250Abenchmark.%2520R1-Track%2520supports%2520flexible%2520initialization%2520via%2520bounding%2520boxes%2520or%2520text%250Adescriptions%2520while%2520retaining%2520most%2520of%2520the%2520original%2520model%2527s%2520general%2520capabilities.%250AAnd%2520we%2520further%2520discuss%2520potential%2520improvements%2520for%2520R1-Track.%2520This%2520rough%250Atechnical%2520report%2520summarizes%2520our%2520findings%2520as%2520of%2520May%25202025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.21980v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R1-Track%3A%20Direct%20Application%20of%20MLLMs%20to%20Visual%20Object%20Tracking%20via%0A%20%20Reinforcement%20Learning&entry.906535625=Biao%20Wang%20and%20Wenwen%20Li%20and%20Jiawei%20Ge&entry.1292438233=%20%20Visual%20single%20object%20tracking%20aims%20to%20continuously%20localize%20and%20estimate%20the%0Ascale%20of%20a%20target%20in%20subsequent%20video%20frames%2C%20given%20only%20its%20initial%20state%20in%0Athe%20first%20frame.%20This%20task%20has%20traditionally%20been%20framed%20as%20a%20template%20matching%0Aproblem%2C%20evolving%20through%20major%20phases%20including%20correlation%20filters%2C%0Atwo-stream%20networks%2C%20and%20one-stream%20networks%20with%20significant%20progress%0Aachieved.%20However%2C%20these%20methods%20typically%20require%20explicit%20classification%20and%0Aregression%20modeling%2C%20depend%20on%20supervised%20training%20with%20large-scale%20datasets%2C%0Aand%20are%20limited%20to%20the%20single%20task%20of%20tracking%2C%20lacking%20flexibility.%20In%20recent%0Ayears%2C%20multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20advanced%20rapidly.%0AOpen-source%20models%20like%20Qwen2.5-VL%2C%20a%20flagship%20MLLMs%20with%20strong%20foundational%0Acapabilities%2C%20demonstrate%20excellent%20performance%20in%20grounding%20tasks.%20This%20has%0Aspurred%20interest%20in%20applying%20such%20models%20directly%20to%20visual%20tracking.%20However%2C%0Aexperiments%20reveal%20that%20Qwen2.5-VL%20struggles%20with%20template%20matching%20between%0Aimage%20pairs%20%28i.e.%2C%20tracking%20tasks%29.%20Inspired%20by%20deepseek-R1%2C%20we%20fine-tuned%0AQwen2.5-VL%20using%20the%20group%20relative%20policy%20optimization%20%28GRPO%29%20reinforcement%0Alearning%20method%20on%20a%20small-scale%20dataset%20with%20a%20rule-based%20reward%20function.%20The%0Aresulting%20model%2C%20R1-Track%2C%20achieved%20notable%20performance%20on%20the%20GOT-10k%0Abenchmark.%20R1-Track%20supports%20flexible%20initialization%20via%20bounding%20boxes%20or%20text%0Adescriptions%20while%20retaining%20most%20of%20the%20original%20model%27s%20general%20capabilities.%0AAnd%20we%20further%20discuss%20potential%20improvements%20for%20R1-Track.%20This%20rough%0Atechnical%20report%20summarizes%20our%20findings%20as%20of%20May%202025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.21980v3&entry.124074799=Read"},
{"title": "Towards a deeper GCN: Alleviate over-smoothing with iterative training\n  and fine-tuning", "author": "Furong Peng and Jinzhen Gao and Xuan Lu and Kang Liu and Yifan Huo and Sheng Wang", "abstract": "  Graph Convolutional Networks (GCNs) suffer from severe performance\ndegradation in deep architectures due to over-smoothing. While existing studies\nprimarily attribute the over-smoothing to repeated applications of graph\nLaplacian operators, our empirical analysis reveals a critical yet overlooked\nfactor: trainable linear transformations in GCNs significantly exacerbate\nfeature collapse, even at moderate depths (e.g., 8 layers). In contrast,\nSimplified Graph Convolution (SGC), which removes these transformations,\nmaintains stable feature diversity up to 32 layers, highlighting linear\ntransformations' dual role in facilitating expressive power and inducing\nover-smoothing. However, completely removing linear transformations weakens the\nmodel's expressive capacity. To address this trade-off, we propose Layer-wise\nGradual Training (LGT), a novel training strategy that progressively builds\ndeep GCNs while preserving their expressiveness. LGT integrates three\ncomplementary components: (1) layer-wise training to stabilize optimization\nfrom shallow to deep layers, (2) low-rank adaptation to fine-tune shallow\nlayers and accelerate training, and (3) identity initialization to ensure\nsmooth integration of new layers and accelerate convergence. Extensive\nexperiments on benchmark datasets demonstrate that LGT achieves\nstate-of-the-art performance on vanilla GCN, significantly improving accuracy\neven in 32-layer settings. Moreover, as a training method, LGT can be\nseamlessly combined with existing methods such as PairNorm and ContraNorm,\nfurther enhancing their performance in deeper networks. LGT offers a general,\narchitecture-agnostic training framework for scalable deep GCNs. The code is\navailable at [https://github.com/jfklasdfj/LGT_GCN].\n", "link": "http://arxiv.org/abs/2506.17576v2", "date": "2025-07-22", "relevancy": 2.1471, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5424}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5377}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20deeper%20GCN%3A%20Alleviate%20over-smoothing%20with%20iterative%20training%0A%20%20and%20fine-tuning&body=Title%3A%20Towards%20a%20deeper%20GCN%3A%20Alleviate%20over-smoothing%20with%20iterative%20training%0A%20%20and%20fine-tuning%0AAuthor%3A%20Furong%20Peng%20and%20Jinzhen%20Gao%20and%20Xuan%20Lu%20and%20Kang%20Liu%20and%20Yifan%20Huo%20and%20Sheng%20Wang%0AAbstract%3A%20%20%20Graph%20Convolutional%20Networks%20%28GCNs%29%20suffer%20from%20severe%20performance%0Adegradation%20in%20deep%20architectures%20due%20to%20over-smoothing.%20While%20existing%20studies%0Aprimarily%20attribute%20the%20over-smoothing%20to%20repeated%20applications%20of%20graph%0ALaplacian%20operators%2C%20our%20empirical%20analysis%20reveals%20a%20critical%20yet%20overlooked%0Afactor%3A%20trainable%20linear%20transformations%20in%20GCNs%20significantly%20exacerbate%0Afeature%20collapse%2C%20even%20at%20moderate%20depths%20%28e.g.%2C%208%20layers%29.%20In%20contrast%2C%0ASimplified%20Graph%20Convolution%20%28SGC%29%2C%20which%20removes%20these%20transformations%2C%0Amaintains%20stable%20feature%20diversity%20up%20to%2032%20layers%2C%20highlighting%20linear%0Atransformations%27%20dual%20role%20in%20facilitating%20expressive%20power%20and%20inducing%0Aover-smoothing.%20However%2C%20completely%20removing%20linear%20transformations%20weakens%20the%0Amodel%27s%20expressive%20capacity.%20To%20address%20this%20trade-off%2C%20we%20propose%20Layer-wise%0AGradual%20Training%20%28LGT%29%2C%20a%20novel%20training%20strategy%20that%20progressively%20builds%0Adeep%20GCNs%20while%20preserving%20their%20expressiveness.%20LGT%20integrates%20three%0Acomplementary%20components%3A%20%281%29%20layer-wise%20training%20to%20stabilize%20optimization%0Afrom%20shallow%20to%20deep%20layers%2C%20%282%29%20low-rank%20adaptation%20to%20fine-tune%20shallow%0Alayers%20and%20accelerate%20training%2C%20and%20%283%29%20identity%20initialization%20to%20ensure%0Asmooth%20integration%20of%20new%20layers%20and%20accelerate%20convergence.%20Extensive%0Aexperiments%20on%20benchmark%20datasets%20demonstrate%20that%20LGT%20achieves%0Astate-of-the-art%20performance%20on%20vanilla%20GCN%2C%20significantly%20improving%20accuracy%0Aeven%20in%2032-layer%20settings.%20Moreover%2C%20as%20a%20training%20method%2C%20LGT%20can%20be%0Aseamlessly%20combined%20with%20existing%20methods%20such%20as%20PairNorm%20and%20ContraNorm%2C%0Afurther%20enhancing%20their%20performance%20in%20deeper%20networks.%20LGT%20offers%20a%20general%2C%0Aarchitecture-agnostic%20training%20framework%20for%20scalable%20deep%20GCNs.%20The%20code%20is%0Aavailable%20at%20%5Bhttps%3A//github.com/jfklasdfj/LGT_GCN%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.17576v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520deeper%2520GCN%253A%2520Alleviate%2520over-smoothing%2520with%2520iterative%2520training%250A%2520%2520and%2520fine-tuning%26entry.906535625%3DFurong%2520Peng%2520and%2520Jinzhen%2520Gao%2520and%2520Xuan%2520Lu%2520and%2520Kang%2520Liu%2520and%2520Yifan%2520Huo%2520and%2520Sheng%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529%2520suffer%2520from%2520severe%2520performance%250Adegradation%2520in%2520deep%2520architectures%2520due%2520to%2520over-smoothing.%2520While%2520existing%2520studies%250Aprimarily%2520attribute%2520the%2520over-smoothing%2520to%2520repeated%2520applications%2520of%2520graph%250ALaplacian%2520operators%252C%2520our%2520empirical%2520analysis%2520reveals%2520a%2520critical%2520yet%2520overlooked%250Afactor%253A%2520trainable%2520linear%2520transformations%2520in%2520GCNs%2520significantly%2520exacerbate%250Afeature%2520collapse%252C%2520even%2520at%2520moderate%2520depths%2520%2528e.g.%252C%25208%2520layers%2529.%2520In%2520contrast%252C%250ASimplified%2520Graph%2520Convolution%2520%2528SGC%2529%252C%2520which%2520removes%2520these%2520transformations%252C%250Amaintains%2520stable%2520feature%2520diversity%2520up%2520to%252032%2520layers%252C%2520highlighting%2520linear%250Atransformations%2527%2520dual%2520role%2520in%2520facilitating%2520expressive%2520power%2520and%2520inducing%250Aover-smoothing.%2520However%252C%2520completely%2520removing%2520linear%2520transformations%2520weakens%2520the%250Amodel%2527s%2520expressive%2520capacity.%2520To%2520address%2520this%2520trade-off%252C%2520we%2520propose%2520Layer-wise%250AGradual%2520Training%2520%2528LGT%2529%252C%2520a%2520novel%2520training%2520strategy%2520that%2520progressively%2520builds%250Adeep%2520GCNs%2520while%2520preserving%2520their%2520expressiveness.%2520LGT%2520integrates%2520three%250Acomplementary%2520components%253A%2520%25281%2529%2520layer-wise%2520training%2520to%2520stabilize%2520optimization%250Afrom%2520shallow%2520to%2520deep%2520layers%252C%2520%25282%2529%2520low-rank%2520adaptation%2520to%2520fine-tune%2520shallow%250Alayers%2520and%2520accelerate%2520training%252C%2520and%2520%25283%2529%2520identity%2520initialization%2520to%2520ensure%250Asmooth%2520integration%2520of%2520new%2520layers%2520and%2520accelerate%2520convergence.%2520Extensive%250Aexperiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520LGT%2520achieves%250Astate-of-the-art%2520performance%2520on%2520vanilla%2520GCN%252C%2520significantly%2520improving%2520accuracy%250Aeven%2520in%252032-layer%2520settings.%2520Moreover%252C%2520as%2520a%2520training%2520method%252C%2520LGT%2520can%2520be%250Aseamlessly%2520combined%2520with%2520existing%2520methods%2520such%2520as%2520PairNorm%2520and%2520ContraNorm%252C%250Afurther%2520enhancing%2520their%2520performance%2520in%2520deeper%2520networks.%2520LGT%2520offers%2520a%2520general%252C%250Aarchitecture-agnostic%2520training%2520framework%2520for%2520scalable%2520deep%2520GCNs.%2520The%2520code%2520is%250Aavailable%2520at%2520%255Bhttps%253A//github.com/jfklasdfj/LGT_GCN%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.17576v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20deeper%20GCN%3A%20Alleviate%20over-smoothing%20with%20iterative%20training%0A%20%20and%20fine-tuning&entry.906535625=Furong%20Peng%20and%20Jinzhen%20Gao%20and%20Xuan%20Lu%20and%20Kang%20Liu%20and%20Yifan%20Huo%20and%20Sheng%20Wang&entry.1292438233=%20%20Graph%20Convolutional%20Networks%20%28GCNs%29%20suffer%20from%20severe%20performance%0Adegradation%20in%20deep%20architectures%20due%20to%20over-smoothing.%20While%20existing%20studies%0Aprimarily%20attribute%20the%20over-smoothing%20to%20repeated%20applications%20of%20graph%0ALaplacian%20operators%2C%20our%20empirical%20analysis%20reveals%20a%20critical%20yet%20overlooked%0Afactor%3A%20trainable%20linear%20transformations%20in%20GCNs%20significantly%20exacerbate%0Afeature%20collapse%2C%20even%20at%20moderate%20depths%20%28e.g.%2C%208%20layers%29.%20In%20contrast%2C%0ASimplified%20Graph%20Convolution%20%28SGC%29%2C%20which%20removes%20these%20transformations%2C%0Amaintains%20stable%20feature%20diversity%20up%20to%2032%20layers%2C%20highlighting%20linear%0Atransformations%27%20dual%20role%20in%20facilitating%20expressive%20power%20and%20inducing%0Aover-smoothing.%20However%2C%20completely%20removing%20linear%20transformations%20weakens%20the%0Amodel%27s%20expressive%20capacity.%20To%20address%20this%20trade-off%2C%20we%20propose%20Layer-wise%0AGradual%20Training%20%28LGT%29%2C%20a%20novel%20training%20strategy%20that%20progressively%20builds%0Adeep%20GCNs%20while%20preserving%20their%20expressiveness.%20LGT%20integrates%20three%0Acomplementary%20components%3A%20%281%29%20layer-wise%20training%20to%20stabilize%20optimization%0Afrom%20shallow%20to%20deep%20layers%2C%20%282%29%20low-rank%20adaptation%20to%20fine-tune%20shallow%0Alayers%20and%20accelerate%20training%2C%20and%20%283%29%20identity%20initialization%20to%20ensure%0Asmooth%20integration%20of%20new%20layers%20and%20accelerate%20convergence.%20Extensive%0Aexperiments%20on%20benchmark%20datasets%20demonstrate%20that%20LGT%20achieves%0Astate-of-the-art%20performance%20on%20vanilla%20GCN%2C%20significantly%20improving%20accuracy%0Aeven%20in%2032-layer%20settings.%20Moreover%2C%20as%20a%20training%20method%2C%20LGT%20can%20be%0Aseamlessly%20combined%20with%20existing%20methods%20such%20as%20PairNorm%20and%20ContraNorm%2C%0Afurther%20enhancing%20their%20performance%20in%20deeper%20networks.%20LGT%20offers%20a%20general%2C%0Aarchitecture-agnostic%20training%20framework%20for%20scalable%20deep%20GCNs.%20The%20code%20is%0Aavailable%20at%20%5Bhttps%3A//github.com/jfklasdfj/LGT_GCN%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.17576v2&entry.124074799=Read"},
{"title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic\n  Metamorphosis", "author": "Zhihao Xu and Bixin Li and Lulu Wang", "abstract": "  Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization.\n", "link": "http://arxiv.org/abs/2507.16808v1", "date": "2025-07-22", "relevancy": 2.1401, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.431}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.431}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20LLM-Based%20RTL%20Code%20Optimization%20Via%20Timing%20Logic%0A%20%20Metamorphosis&body=Title%3A%20Rethinking%20LLM-Based%20RTL%20Code%20Optimization%20Via%20Timing%20Logic%0A%20%20Metamorphosis%0AAuthor%3A%20Zhihao%20Xu%20and%20Bixin%20Li%20and%20Lulu%20Wang%0AAbstract%3A%20%20%20Register%20Transfer%20Level%28RTL%29%20code%20optimization%20is%20crucial%20for%20achieving%20high%0Aperformance%20and%20low%20power%20consumption%20in%20digital%20circuit%20design.%20However%2C%0Atraditional%20optimization%20methods%20often%20rely%20on%20manual%20tuning%20and%20heuristics%2C%0Awhich%20can%20be%20time-consuming%20and%20error-prone.%20Recent%20studies%20proposed%20to%0Aleverage%20Large%20Language%20Models%28LLMs%29%20to%20assist%20in%20RTL%20code%20optimization.%20LLMs%0Acan%20generate%20optimized%20code%20snippets%20based%20on%20natural%20language%20descriptions%2C%0Apotentially%20speeding%20up%20the%20optimization%20process.%20However%2C%20existing%20approaches%0Ahave%20not%20thoroughly%20evaluated%20the%20effectiveness%20of%20LLM-Based%20code%20optimization%0Amethods%20for%20RTL%20code%20with%20complex%20timing%20logic.%20To%20address%20this%20gap%2C%20we%0Aconducted%20a%20comprehensive%20empirical%20investigation%20to%20assess%20the%20capability%20of%0ALLM-Based%20RTL%20code%20optimization%20methods%20in%20handling%20RTL%20code%20with%20complex%0Atiming%20logic.%20In%20this%20study%2C%20we%20first%20propose%20a%20new%20benchmark%20for%20RTL%0Aoptimization%20evaluation.%20It%20comprises%20four%20subsets%2C%20each%20corresponding%20to%20a%0Aspecific%20area%20of%20RTL%20code%20optimization.%20Then%20we%20introduce%20a%20method%20based%20on%0Ametamorphosis%20to%20systematically%20evaluate%20the%20effectiveness%20of%20LLM-Based%20RTL%0Acode%20optimization%20methods.Our%20key%20insight%20is%20that%20the%20optimization%0Aeffectiveness%20should%20remain%20consistent%20for%20semantically%20equivalent%20but%20more%0Acomplex%20code.%20After%20intensive%20experiments%2C%20we%20revealed%20several%20key%20findings.%0A%281%29%20LLM-Based%20RTL%20optimization%20methods%20can%20effectively%20optimize%20logic%0Aoperations%20and%20outperform%20existing%20compiler-based%20methods.%20%282%29%20LLM-Based%20RTL%0Aoptimization%20methods%20do%20not%20perform%20better%20than%20existing%20compiler-based%20methods%0Aon%20RTL%20code%20with%20complex%20timing%20logic%2C%20particularly%20in%20timing%20control%20flow%0Aoptimization%20and%20clock%20domain%20optimization.%20This%20is%20primarily%20attributed%20to%20the%0Achallenges%20LLMs%20face%20in%20understanding%20timing%20logic%20in%20RTL%20code.%20Based%20on%20these%0Afindings%2C%20we%20provide%20insights%20for%20further%20research%20in%20leveraging%20LLMs%20for%20RTL%0Acode%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520LLM-Based%2520RTL%2520Code%2520Optimization%2520Via%2520Timing%2520Logic%250A%2520%2520Metamorphosis%26entry.906535625%3DZhihao%2520Xu%2520and%2520Bixin%2520Li%2520and%2520Lulu%2520Wang%26entry.1292438233%3D%2520%2520Register%2520Transfer%2520Level%2528RTL%2529%2520code%2520optimization%2520is%2520crucial%2520for%2520achieving%2520high%250Aperformance%2520and%2520low%2520power%2520consumption%2520in%2520digital%2520circuit%2520design.%2520However%252C%250Atraditional%2520optimization%2520methods%2520often%2520rely%2520on%2520manual%2520tuning%2520and%2520heuristics%252C%250Awhich%2520can%2520be%2520time-consuming%2520and%2520error-prone.%2520Recent%2520studies%2520proposed%2520to%250Aleverage%2520Large%2520Language%2520Models%2528LLMs%2529%2520to%2520assist%2520in%2520RTL%2520code%2520optimization.%2520LLMs%250Acan%2520generate%2520optimized%2520code%2520snippets%2520based%2520on%2520natural%2520language%2520descriptions%252C%250Apotentially%2520speeding%2520up%2520the%2520optimization%2520process.%2520However%252C%2520existing%2520approaches%250Ahave%2520not%2520thoroughly%2520evaluated%2520the%2520effectiveness%2520of%2520LLM-Based%2520code%2520optimization%250Amethods%2520for%2520RTL%2520code%2520with%2520complex%2520timing%2520logic.%2520To%2520address%2520this%2520gap%252C%2520we%250Aconducted%2520a%2520comprehensive%2520empirical%2520investigation%2520to%2520assess%2520the%2520capability%2520of%250ALLM-Based%2520RTL%2520code%2520optimization%2520methods%2520in%2520handling%2520RTL%2520code%2520with%2520complex%250Atiming%2520logic.%2520In%2520this%2520study%252C%2520we%2520first%2520propose%2520a%2520new%2520benchmark%2520for%2520RTL%250Aoptimization%2520evaluation.%2520It%2520comprises%2520four%2520subsets%252C%2520each%2520corresponding%2520to%2520a%250Aspecific%2520area%2520of%2520RTL%2520code%2520optimization.%2520Then%2520we%2520introduce%2520a%2520method%2520based%2520on%250Ametamorphosis%2520to%2520systematically%2520evaluate%2520the%2520effectiveness%2520of%2520LLM-Based%2520RTL%250Acode%2520optimization%2520methods.Our%2520key%2520insight%2520is%2520that%2520the%2520optimization%250Aeffectiveness%2520should%2520remain%2520consistent%2520for%2520semantically%2520equivalent%2520but%2520more%250Acomplex%2520code.%2520After%2520intensive%2520experiments%252C%2520we%2520revealed%2520several%2520key%2520findings.%250A%25281%2529%2520LLM-Based%2520RTL%2520optimization%2520methods%2520can%2520effectively%2520optimize%2520logic%250Aoperations%2520and%2520outperform%2520existing%2520compiler-based%2520methods.%2520%25282%2529%2520LLM-Based%2520RTL%250Aoptimization%2520methods%2520do%2520not%2520perform%2520better%2520than%2520existing%2520compiler-based%2520methods%250Aon%2520RTL%2520code%2520with%2520complex%2520timing%2520logic%252C%2520particularly%2520in%2520timing%2520control%2520flow%250Aoptimization%2520and%2520clock%2520domain%2520optimization.%2520This%2520is%2520primarily%2520attributed%2520to%2520the%250Achallenges%2520LLMs%2520face%2520in%2520understanding%2520timing%2520logic%2520in%2520RTL%2520code.%2520Based%2520on%2520these%250Afindings%252C%2520we%2520provide%2520insights%2520for%2520further%2520research%2520in%2520leveraging%2520LLMs%2520for%2520RTL%250Acode%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20LLM-Based%20RTL%20Code%20Optimization%20Via%20Timing%20Logic%0A%20%20Metamorphosis&entry.906535625=Zhihao%20Xu%20and%20Bixin%20Li%20and%20Lulu%20Wang&entry.1292438233=%20%20Register%20Transfer%20Level%28RTL%29%20code%20optimization%20is%20crucial%20for%20achieving%20high%0Aperformance%20and%20low%20power%20consumption%20in%20digital%20circuit%20design.%20However%2C%0Atraditional%20optimization%20methods%20often%20rely%20on%20manual%20tuning%20and%20heuristics%2C%0Awhich%20can%20be%20time-consuming%20and%20error-prone.%20Recent%20studies%20proposed%20to%0Aleverage%20Large%20Language%20Models%28LLMs%29%20to%20assist%20in%20RTL%20code%20optimization.%20LLMs%0Acan%20generate%20optimized%20code%20snippets%20based%20on%20natural%20language%20descriptions%2C%0Apotentially%20speeding%20up%20the%20optimization%20process.%20However%2C%20existing%20approaches%0Ahave%20not%20thoroughly%20evaluated%20the%20effectiveness%20of%20LLM-Based%20code%20optimization%0Amethods%20for%20RTL%20code%20with%20complex%20timing%20logic.%20To%20address%20this%20gap%2C%20we%0Aconducted%20a%20comprehensive%20empirical%20investigation%20to%20assess%20the%20capability%20of%0ALLM-Based%20RTL%20code%20optimization%20methods%20in%20handling%20RTL%20code%20with%20complex%0Atiming%20logic.%20In%20this%20study%2C%20we%20first%20propose%20a%20new%20benchmark%20for%20RTL%0Aoptimization%20evaluation.%20It%20comprises%20four%20subsets%2C%20each%20corresponding%20to%20a%0Aspecific%20area%20of%20RTL%20code%20optimization.%20Then%20we%20introduce%20a%20method%20based%20on%0Ametamorphosis%20to%20systematically%20evaluate%20the%20effectiveness%20of%20LLM-Based%20RTL%0Acode%20optimization%20methods.Our%20key%20insight%20is%20that%20the%20optimization%0Aeffectiveness%20should%20remain%20consistent%20for%20semantically%20equivalent%20but%20more%0Acomplex%20code.%20After%20intensive%20experiments%2C%20we%20revealed%20several%20key%20findings.%0A%281%29%20LLM-Based%20RTL%20optimization%20methods%20can%20effectively%20optimize%20logic%0Aoperations%20and%20outperform%20existing%20compiler-based%20methods.%20%282%29%20LLM-Based%20RTL%0Aoptimization%20methods%20do%20not%20perform%20better%20than%20existing%20compiler-based%20methods%0Aon%20RTL%20code%20with%20complex%20timing%20logic%2C%20particularly%20in%20timing%20control%20flow%0Aoptimization%20and%20clock%20domain%20optimization.%20This%20is%20primarily%20attributed%20to%20the%0Achallenges%20LLMs%20face%20in%20understanding%20timing%20logic%20in%20RTL%20code.%20Based%20on%20these%0Afindings%2C%20we%20provide%20insights%20for%20further%20research%20in%20leveraging%20LLMs%20for%20RTL%0Acode%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16808v1&entry.124074799=Read"},
{"title": "EmotiCrafter: Text-to-Emotional-Image Generation based on\n  Valence-Arousal Model", "author": "Shengqi Dang and Yi He and Long Ling and Ziqing Qian and Nanxuan Zhao and Nan Cao", "abstract": "  Recent research shows that emotions can enhance users' cognition and\ninfluence information communication. While research on visual emotion analysis\nis extensive, limited work has been done on helping users generate emotionally\nrich image content. Existing work on emotional image generation relies on\ndiscrete emotion categories, making it challenging to capture complex and\nsubtle emotional nuances accurately. Additionally, these methods struggle to\ncontrol the specific content of generated images based on text prompts. In this\nwork, we introduce the new task of continuous emotional image content\ngeneration (C-EICG) and present EmotiCrafter, an emotional image generation\nmodel that generates images based on text prompts and Valence-Arousal values.\nSpecifically, we propose a novel emotion-embedding mapping network that embeds\nValence-Arousal values into textual features, enabling the capture of specific\nemotions in alignment with intended input prompts. Additionally, we introduce a\nloss function to enhance emotion expression. The experimental results show that\nour method effectively generates images representing specific emotions with the\ndesired content and outperforms existing techniques.\n", "link": "http://arxiv.org/abs/2501.05710v2", "date": "2025-07-22", "relevancy": 2.1349, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5437}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5289}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EmotiCrafter%3A%20Text-to-Emotional-Image%20Generation%20based%20on%0A%20%20Valence-Arousal%20Model&body=Title%3A%20EmotiCrafter%3A%20Text-to-Emotional-Image%20Generation%20based%20on%0A%20%20Valence-Arousal%20Model%0AAuthor%3A%20Shengqi%20Dang%20and%20Yi%20He%20and%20Long%20Ling%20and%20Ziqing%20Qian%20and%20Nanxuan%20Zhao%20and%20Nan%20Cao%0AAbstract%3A%20%20%20Recent%20research%20shows%20that%20emotions%20can%20enhance%20users%27%20cognition%20and%0Ainfluence%20information%20communication.%20While%20research%20on%20visual%20emotion%20analysis%0Ais%20extensive%2C%20limited%20work%20has%20been%20done%20on%20helping%20users%20generate%20emotionally%0Arich%20image%20content.%20Existing%20work%20on%20emotional%20image%20generation%20relies%20on%0Adiscrete%20emotion%20categories%2C%20making%20it%20challenging%20to%20capture%20complex%20and%0Asubtle%20emotional%20nuances%20accurately.%20Additionally%2C%20these%20methods%20struggle%20to%0Acontrol%20the%20specific%20content%20of%20generated%20images%20based%20on%20text%20prompts.%20In%20this%0Awork%2C%20we%20introduce%20the%20new%20task%20of%20continuous%20emotional%20image%20content%0Ageneration%20%28C-EICG%29%20and%20present%20EmotiCrafter%2C%20an%20emotional%20image%20generation%0Amodel%20that%20generates%20images%20based%20on%20text%20prompts%20and%20Valence-Arousal%20values.%0ASpecifically%2C%20we%20propose%20a%20novel%20emotion-embedding%20mapping%20network%20that%20embeds%0AValence-Arousal%20values%20into%20textual%20features%2C%20enabling%20the%20capture%20of%20specific%0Aemotions%20in%20alignment%20with%20intended%20input%20prompts.%20Additionally%2C%20we%20introduce%20a%0Aloss%20function%20to%20enhance%20emotion%20expression.%20The%20experimental%20results%20show%20that%0Aour%20method%20effectively%20generates%20images%20representing%20specific%20emotions%20with%20the%0Adesired%20content%20and%20outperforms%20existing%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.05710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmotiCrafter%253A%2520Text-to-Emotional-Image%2520Generation%2520based%2520on%250A%2520%2520Valence-Arousal%2520Model%26entry.906535625%3DShengqi%2520Dang%2520and%2520Yi%2520He%2520and%2520Long%2520Ling%2520and%2520Ziqing%2520Qian%2520and%2520Nanxuan%2520Zhao%2520and%2520Nan%2520Cao%26entry.1292438233%3D%2520%2520Recent%2520research%2520shows%2520that%2520emotions%2520can%2520enhance%2520users%2527%2520cognition%2520and%250Ainfluence%2520information%2520communication.%2520While%2520research%2520on%2520visual%2520emotion%2520analysis%250Ais%2520extensive%252C%2520limited%2520work%2520has%2520been%2520done%2520on%2520helping%2520users%2520generate%2520emotionally%250Arich%2520image%2520content.%2520Existing%2520work%2520on%2520emotional%2520image%2520generation%2520relies%2520on%250Adiscrete%2520emotion%2520categories%252C%2520making%2520it%2520challenging%2520to%2520capture%2520complex%2520and%250Asubtle%2520emotional%2520nuances%2520accurately.%2520Additionally%252C%2520these%2520methods%2520struggle%2520to%250Acontrol%2520the%2520specific%2520content%2520of%2520generated%2520images%2520based%2520on%2520text%2520prompts.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520the%2520new%2520task%2520of%2520continuous%2520emotional%2520image%2520content%250Ageneration%2520%2528C-EICG%2529%2520and%2520present%2520EmotiCrafter%252C%2520an%2520emotional%2520image%2520generation%250Amodel%2520that%2520generates%2520images%2520based%2520on%2520text%2520prompts%2520and%2520Valence-Arousal%2520values.%250ASpecifically%252C%2520we%2520propose%2520a%2520novel%2520emotion-embedding%2520mapping%2520network%2520that%2520embeds%250AValence-Arousal%2520values%2520into%2520textual%2520features%252C%2520enabling%2520the%2520capture%2520of%2520specific%250Aemotions%2520in%2520alignment%2520with%2520intended%2520input%2520prompts.%2520Additionally%252C%2520we%2520introduce%2520a%250Aloss%2520function%2520to%2520enhance%2520emotion%2520expression.%2520The%2520experimental%2520results%2520show%2520that%250Aour%2520method%2520effectively%2520generates%2520images%2520representing%2520specific%2520emotions%2520with%2520the%250Adesired%2520content%2520and%2520outperforms%2520existing%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.05710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EmotiCrafter%3A%20Text-to-Emotional-Image%20Generation%20based%20on%0A%20%20Valence-Arousal%20Model&entry.906535625=Shengqi%20Dang%20and%20Yi%20He%20and%20Long%20Ling%20and%20Ziqing%20Qian%20and%20Nanxuan%20Zhao%20and%20Nan%20Cao&entry.1292438233=%20%20Recent%20research%20shows%20that%20emotions%20can%20enhance%20users%27%20cognition%20and%0Ainfluence%20information%20communication.%20While%20research%20on%20visual%20emotion%20analysis%0Ais%20extensive%2C%20limited%20work%20has%20been%20done%20on%20helping%20users%20generate%20emotionally%0Arich%20image%20content.%20Existing%20work%20on%20emotional%20image%20generation%20relies%20on%0Adiscrete%20emotion%20categories%2C%20making%20it%20challenging%20to%20capture%20complex%20and%0Asubtle%20emotional%20nuances%20accurately.%20Additionally%2C%20these%20methods%20struggle%20to%0Acontrol%20the%20specific%20content%20of%20generated%20images%20based%20on%20text%20prompts.%20In%20this%0Awork%2C%20we%20introduce%20the%20new%20task%20of%20continuous%20emotional%20image%20content%0Ageneration%20%28C-EICG%29%20and%20present%20EmotiCrafter%2C%20an%20emotional%20image%20generation%0Amodel%20that%20generates%20images%20based%20on%20text%20prompts%20and%20Valence-Arousal%20values.%0ASpecifically%2C%20we%20propose%20a%20novel%20emotion-embedding%20mapping%20network%20that%20embeds%0AValence-Arousal%20values%20into%20textual%20features%2C%20enabling%20the%20capture%20of%20specific%0Aemotions%20in%20alignment%20with%20intended%20input%20prompts.%20Additionally%2C%20we%20introduce%20a%0Aloss%20function%20to%20enhance%20emotion%20expression.%20The%20experimental%20results%20show%20that%0Aour%20method%20effectively%20generates%20images%20representing%20specific%20emotions%20with%20the%0Adesired%20content%20and%20outperforms%20existing%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.05710v2&entry.124074799=Read"},
{"title": "Physically Consistent Image Augmentation for Deep Learning in Mueller\n  Matrix Polarimetry", "author": "Christopher Hahne and Omar Rodriguez-Nunez and \u00c9l\u00e9a Gros and Th\u00e9otim Lucas and Ekkehard Hewer and Tatiana Novikova and Theoni Maragkou and Philippe Schucht and Richard McKinley", "abstract": "  Mueller matrix polarimetry captures essential information about polarized\nlight interactions with a sample, presenting unique challenges for data\naugmentation in deep learning due to its distinct structure. While\naugmentations are an effective and affordable way to enhance dataset diversity\nand reduce overfitting, standard transformations like rotations and flips do\nnot preserve the polarization properties in Mueller matrix images. To this end,\nwe introduce a versatile simulation framework that applies physically\nconsistent rotations and flips to Mueller matrices, tailored to maintain\npolarization fidelity. Our experimental results across multiple datasets reveal\nthat conventional augmentations can lead to falsified results when applied to\npolarimetric data, underscoring the necessity of our physics-based approach. In\nour experiments, we first compare our polarization-specific augmentations\nagainst real-world captures to validate their physical consistency. We then\napply these augmentations in a semantic segmentation task, achieving\nsubstantial improvements in model generalization and performance. This study\nunderscores the necessity of physics-informed data augmentation for\npolarimetric imaging in deep learning (DL), paving the way for broader adoption\nand more robust applications across diverse research in the field. In\nparticular, our framework unlocks the potential of DL models for polarimetric\ndatasets with limited sample sizes. Our code implementation is available at\ngithub.com/hahnec/polar_augment.\n", "link": "http://arxiv.org/abs/2411.07918v2", "date": "2025-07-22", "relevancy": 2.1326, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5398}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5289}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20Consistent%20Image%20Augmentation%20for%20Deep%20Learning%20in%20Mueller%0A%20%20Matrix%20Polarimetry&body=Title%3A%20Physically%20Consistent%20Image%20Augmentation%20for%20Deep%20Learning%20in%20Mueller%0A%20%20Matrix%20Polarimetry%0AAuthor%3A%20Christopher%20Hahne%20and%20Omar%20Rodriguez-Nunez%20and%20%C3%89l%C3%A9a%20Gros%20and%20Th%C3%A9otim%20Lucas%20and%20Ekkehard%20Hewer%20and%20Tatiana%20Novikova%20and%20Theoni%20Maragkou%20and%20Philippe%20Schucht%20and%20Richard%20McKinley%0AAbstract%3A%20%20%20Mueller%20matrix%20polarimetry%20captures%20essential%20information%20about%20polarized%0Alight%20interactions%20with%20a%20sample%2C%20presenting%20unique%20challenges%20for%20data%0Aaugmentation%20in%20deep%20learning%20due%20to%20its%20distinct%20structure.%20While%0Aaugmentations%20are%20an%20effective%20and%20affordable%20way%20to%20enhance%20dataset%20diversity%0Aand%20reduce%20overfitting%2C%20standard%20transformations%20like%20rotations%20and%20flips%20do%0Anot%20preserve%20the%20polarization%20properties%20in%20Mueller%20matrix%20images.%20To%20this%20end%2C%0Awe%20introduce%20a%20versatile%20simulation%20framework%20that%20applies%20physically%0Aconsistent%20rotations%20and%20flips%20to%20Mueller%20matrices%2C%20tailored%20to%20maintain%0Apolarization%20fidelity.%20Our%20experimental%20results%20across%20multiple%20datasets%20reveal%0Athat%20conventional%20augmentations%20can%20lead%20to%20falsified%20results%20when%20applied%20to%0Apolarimetric%20data%2C%20underscoring%20the%20necessity%20of%20our%20physics-based%20approach.%20In%0Aour%20experiments%2C%20we%20first%20compare%20our%20polarization-specific%20augmentations%0Aagainst%20real-world%20captures%20to%20validate%20their%20physical%20consistency.%20We%20then%0Aapply%20these%20augmentations%20in%20a%20semantic%20segmentation%20task%2C%20achieving%0Asubstantial%20improvements%20in%20model%20generalization%20and%20performance.%20This%20study%0Aunderscores%20the%20necessity%20of%20physics-informed%20data%20augmentation%20for%0Apolarimetric%20imaging%20in%20deep%20learning%20%28DL%29%2C%20paving%20the%20way%20for%20broader%20adoption%0Aand%20more%20robust%20applications%20across%20diverse%20research%20in%20the%20field.%20In%0Aparticular%2C%20our%20framework%20unlocks%20the%20potential%20of%20DL%20models%20for%20polarimetric%0Adatasets%20with%20limited%20sample%20sizes.%20Our%20code%20implementation%20is%20available%20at%0Agithub.com/hahnec/polar_augment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520Consistent%2520Image%2520Augmentation%2520for%2520Deep%2520Learning%2520in%2520Mueller%250A%2520%2520Matrix%2520Polarimetry%26entry.906535625%3DChristopher%2520Hahne%2520and%2520Omar%2520Rodriguez-Nunez%2520and%2520%25C3%2589l%25C3%25A9a%2520Gros%2520and%2520Th%25C3%25A9otim%2520Lucas%2520and%2520Ekkehard%2520Hewer%2520and%2520Tatiana%2520Novikova%2520and%2520Theoni%2520Maragkou%2520and%2520Philippe%2520Schucht%2520and%2520Richard%2520McKinley%26entry.1292438233%3D%2520%2520Mueller%2520matrix%2520polarimetry%2520captures%2520essential%2520information%2520about%2520polarized%250Alight%2520interactions%2520with%2520a%2520sample%252C%2520presenting%2520unique%2520challenges%2520for%2520data%250Aaugmentation%2520in%2520deep%2520learning%2520due%2520to%2520its%2520distinct%2520structure.%2520While%250Aaugmentations%2520are%2520an%2520effective%2520and%2520affordable%2520way%2520to%2520enhance%2520dataset%2520diversity%250Aand%2520reduce%2520overfitting%252C%2520standard%2520transformations%2520like%2520rotations%2520and%2520flips%2520do%250Anot%2520preserve%2520the%2520polarization%2520properties%2520in%2520Mueller%2520matrix%2520images.%2520To%2520this%2520end%252C%250Awe%2520introduce%2520a%2520versatile%2520simulation%2520framework%2520that%2520applies%2520physically%250Aconsistent%2520rotations%2520and%2520flips%2520to%2520Mueller%2520matrices%252C%2520tailored%2520to%2520maintain%250Apolarization%2520fidelity.%2520Our%2520experimental%2520results%2520across%2520multiple%2520datasets%2520reveal%250Athat%2520conventional%2520augmentations%2520can%2520lead%2520to%2520falsified%2520results%2520when%2520applied%2520to%250Apolarimetric%2520data%252C%2520underscoring%2520the%2520necessity%2520of%2520our%2520physics-based%2520approach.%2520In%250Aour%2520experiments%252C%2520we%2520first%2520compare%2520our%2520polarization-specific%2520augmentations%250Aagainst%2520real-world%2520captures%2520to%2520validate%2520their%2520physical%2520consistency.%2520We%2520then%250Aapply%2520these%2520augmentations%2520in%2520a%2520semantic%2520segmentation%2520task%252C%2520achieving%250Asubstantial%2520improvements%2520in%2520model%2520generalization%2520and%2520performance.%2520This%2520study%250Aunderscores%2520the%2520necessity%2520of%2520physics-informed%2520data%2520augmentation%2520for%250Apolarimetric%2520imaging%2520in%2520deep%2520learning%2520%2528DL%2529%252C%2520paving%2520the%2520way%2520for%2520broader%2520adoption%250Aand%2520more%2520robust%2520applications%2520across%2520diverse%2520research%2520in%2520the%2520field.%2520In%250Aparticular%252C%2520our%2520framework%2520unlocks%2520the%2520potential%2520of%2520DL%2520models%2520for%2520polarimetric%250Adatasets%2520with%2520limited%2520sample%2520sizes.%2520Our%2520code%2520implementation%2520is%2520available%2520at%250Agithub.com/hahnec/polar_augment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20Consistent%20Image%20Augmentation%20for%20Deep%20Learning%20in%20Mueller%0A%20%20Matrix%20Polarimetry&entry.906535625=Christopher%20Hahne%20and%20Omar%20Rodriguez-Nunez%20and%20%C3%89l%C3%A9a%20Gros%20and%20Th%C3%A9otim%20Lucas%20and%20Ekkehard%20Hewer%20and%20Tatiana%20Novikova%20and%20Theoni%20Maragkou%20and%20Philippe%20Schucht%20and%20Richard%20McKinley&entry.1292438233=%20%20Mueller%20matrix%20polarimetry%20captures%20essential%20information%20about%20polarized%0Alight%20interactions%20with%20a%20sample%2C%20presenting%20unique%20challenges%20for%20data%0Aaugmentation%20in%20deep%20learning%20due%20to%20its%20distinct%20structure.%20While%0Aaugmentations%20are%20an%20effective%20and%20affordable%20way%20to%20enhance%20dataset%20diversity%0Aand%20reduce%20overfitting%2C%20standard%20transformations%20like%20rotations%20and%20flips%20do%0Anot%20preserve%20the%20polarization%20properties%20in%20Mueller%20matrix%20images.%20To%20this%20end%2C%0Awe%20introduce%20a%20versatile%20simulation%20framework%20that%20applies%20physically%0Aconsistent%20rotations%20and%20flips%20to%20Mueller%20matrices%2C%20tailored%20to%20maintain%0Apolarization%20fidelity.%20Our%20experimental%20results%20across%20multiple%20datasets%20reveal%0Athat%20conventional%20augmentations%20can%20lead%20to%20falsified%20results%20when%20applied%20to%0Apolarimetric%20data%2C%20underscoring%20the%20necessity%20of%20our%20physics-based%20approach.%20In%0Aour%20experiments%2C%20we%20first%20compare%20our%20polarization-specific%20augmentations%0Aagainst%20real-world%20captures%20to%20validate%20their%20physical%20consistency.%20We%20then%0Aapply%20these%20augmentations%20in%20a%20semantic%20segmentation%20task%2C%20achieving%0Asubstantial%20improvements%20in%20model%20generalization%20and%20performance.%20This%20study%0Aunderscores%20the%20necessity%20of%20physics-informed%20data%20augmentation%20for%0Apolarimetric%20imaging%20in%20deep%20learning%20%28DL%29%2C%20paving%20the%20way%20for%20broader%20adoption%0Aand%20more%20robust%20applications%20across%20diverse%20research%20in%20the%20field.%20In%0Aparticular%2C%20our%20framework%20unlocks%20the%20potential%20of%20DL%20models%20for%20polarimetric%0Adatasets%20with%20limited%20sample%20sizes.%20Our%20code%20implementation%20is%20available%20at%0Agithub.com/hahnec/polar_augment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07918v2&entry.124074799=Read"},
{"title": "CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp\n  Segmentation", "author": "Desheng Li and Chaoliang Liu and Zhiyong Xiao", "abstract": "  Accurate segmentation of polyps from colonoscopy images is crucial for the\nearly diagnosis and treatment of colorectal cancer. Most existing deep\nlearning-based polyp segmentation methods adopt an Encoder-Decoder\narchitecture, and some utilize multi-task frameworks that incorporate auxiliary\ntasks like classification to improve segmentation. However, these methods often\nneed more labeled data and depend on task similarity, potentially limiting\ngeneralizability. To address these challenges, we propose CL-Polyp, a\ncontrastive learning-enhanced polyp segmentation network. Our method uses\ncontrastive learning to enhance the encoder's extraction of discriminative\nfeatures by contrasting positive and negative sample pairs from polyp images.\nThis self-supervised strategy improves visual representation without needing\nadditional annotations. We also introduce two efficient, lightweight modules:\nthe Modified Atrous Spatial Pyramid Pooling (MASPP) module for improved\nmulti-scale feature fusion, and the Channel Concatenate and Element Add (CA)\nmodule to merge low-level and upsampled features for {enhanced} boundary\nreconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,\nCVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-show that CL-Polyp consistently\nsurpasses state-of-the-art methods. Specifically, it enhances the IoU metric by\n0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets, respectively,\ndemonstrating its effectiveness in clinical polyp segmentation.\n", "link": "http://arxiv.org/abs/2507.07154v2", "date": "2025-07-22", "relevancy": 2.1323, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5599}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5189}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CL-Polyp%3A%20A%20Contrastive%20Learning-Enhanced%20Network%20for%20Accurate%20Polyp%0A%20%20Segmentation&body=Title%3A%20CL-Polyp%3A%20A%20Contrastive%20Learning-Enhanced%20Network%20for%20Accurate%20Polyp%0A%20%20Segmentation%0AAuthor%3A%20Desheng%20Li%20and%20Chaoliang%20Liu%20and%20Zhiyong%20Xiao%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20polyps%20from%20colonoscopy%20images%20is%20crucial%20for%20the%0Aearly%20diagnosis%20and%20treatment%20of%20colorectal%20cancer.%20Most%20existing%20deep%0Alearning-based%20polyp%20segmentation%20methods%20adopt%20an%20Encoder-Decoder%0Aarchitecture%2C%20and%20some%20utilize%20multi-task%20frameworks%20that%20incorporate%20auxiliary%0Atasks%20like%20classification%20to%20improve%20segmentation.%20However%2C%20these%20methods%20often%0Aneed%20more%20labeled%20data%20and%20depend%20on%20task%20similarity%2C%20potentially%20limiting%0Ageneralizability.%20To%20address%20these%20challenges%2C%20we%20propose%20CL-Polyp%2C%20a%0Acontrastive%20learning-enhanced%20polyp%20segmentation%20network.%20Our%20method%20uses%0Acontrastive%20learning%20to%20enhance%20the%20encoder%27s%20extraction%20of%20discriminative%0Afeatures%20by%20contrasting%20positive%20and%20negative%20sample%20pairs%20from%20polyp%20images.%0AThis%20self-supervised%20strategy%20improves%20visual%20representation%20without%20needing%0Aadditional%20annotations.%20We%20also%20introduce%20two%20efficient%2C%20lightweight%20modules%3A%0Athe%20Modified%20Atrous%20Spatial%20Pyramid%20Pooling%20%28MASPP%29%20module%20for%20improved%0Amulti-scale%20feature%20fusion%2C%20and%20the%20Channel%20Concatenate%20and%20Element%20Add%20%28CA%29%0Amodule%20to%20merge%20low-level%20and%20upsampled%20features%20for%20%7Benhanced%7D%20boundary%0Areconstruction.%20Extensive%20experiments%20on%20five%20benchmark%20datasets-Kvasir-SEG%2C%0ACVC-ClinicDB%2C%20CVC-ColonDB%2C%20CVC-300%2C%20and%20ETIS-show%20that%20CL-Polyp%20consistently%0Asurpasses%20state-of-the-art%20methods.%20Specifically%2C%20it%20enhances%20the%20IoU%20metric%20by%0A0.011%20and%200.020%20on%20the%20Kvasir-SEG%20and%20CVC-ClinicDB%20datasets%2C%20respectively%2C%0Ademonstrating%20its%20effectiveness%20in%20clinical%20polyp%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCL-Polyp%253A%2520A%2520Contrastive%2520Learning-Enhanced%2520Network%2520for%2520Accurate%2520Polyp%250A%2520%2520Segmentation%26entry.906535625%3DDesheng%2520Li%2520and%2520Chaoliang%2520Liu%2520and%2520Zhiyong%2520Xiao%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520polyps%2520from%2520colonoscopy%2520images%2520is%2520crucial%2520for%2520the%250Aearly%2520diagnosis%2520and%2520treatment%2520of%2520colorectal%2520cancer.%2520Most%2520existing%2520deep%250Alearning-based%2520polyp%2520segmentation%2520methods%2520adopt%2520an%2520Encoder-Decoder%250Aarchitecture%252C%2520and%2520some%2520utilize%2520multi-task%2520frameworks%2520that%2520incorporate%2520auxiliary%250Atasks%2520like%2520classification%2520to%2520improve%2520segmentation.%2520However%252C%2520these%2520methods%2520often%250Aneed%2520more%2520labeled%2520data%2520and%2520depend%2520on%2520task%2520similarity%252C%2520potentially%2520limiting%250Ageneralizability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520CL-Polyp%252C%2520a%250Acontrastive%2520learning-enhanced%2520polyp%2520segmentation%2520network.%2520Our%2520method%2520uses%250Acontrastive%2520learning%2520to%2520enhance%2520the%2520encoder%2527s%2520extraction%2520of%2520discriminative%250Afeatures%2520by%2520contrasting%2520positive%2520and%2520negative%2520sample%2520pairs%2520from%2520polyp%2520images.%250AThis%2520self-supervised%2520strategy%2520improves%2520visual%2520representation%2520without%2520needing%250Aadditional%2520annotations.%2520We%2520also%2520introduce%2520two%2520efficient%252C%2520lightweight%2520modules%253A%250Athe%2520Modified%2520Atrous%2520Spatial%2520Pyramid%2520Pooling%2520%2528MASPP%2529%2520module%2520for%2520improved%250Amulti-scale%2520feature%2520fusion%252C%2520and%2520the%2520Channel%2520Concatenate%2520and%2520Element%2520Add%2520%2528CA%2529%250Amodule%2520to%2520merge%2520low-level%2520and%2520upsampled%2520features%2520for%2520%257Benhanced%257D%2520boundary%250Areconstruction.%2520Extensive%2520experiments%2520on%2520five%2520benchmark%2520datasets-Kvasir-SEG%252C%250ACVC-ClinicDB%252C%2520CVC-ColonDB%252C%2520CVC-300%252C%2520and%2520ETIS-show%2520that%2520CL-Polyp%2520consistently%250Asurpasses%2520state-of-the-art%2520methods.%2520Specifically%252C%2520it%2520enhances%2520the%2520IoU%2520metric%2520by%250A0.011%2520and%25200.020%2520on%2520the%2520Kvasir-SEG%2520and%2520CVC-ClinicDB%2520datasets%252C%2520respectively%252C%250Ademonstrating%2520its%2520effectiveness%2520in%2520clinical%2520polyp%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CL-Polyp%3A%20A%20Contrastive%20Learning-Enhanced%20Network%20for%20Accurate%20Polyp%0A%20%20Segmentation&entry.906535625=Desheng%20Li%20and%20Chaoliang%20Liu%20and%20Zhiyong%20Xiao&entry.1292438233=%20%20Accurate%20segmentation%20of%20polyps%20from%20colonoscopy%20images%20is%20crucial%20for%20the%0Aearly%20diagnosis%20and%20treatment%20of%20colorectal%20cancer.%20Most%20existing%20deep%0Alearning-based%20polyp%20segmentation%20methods%20adopt%20an%20Encoder-Decoder%0Aarchitecture%2C%20and%20some%20utilize%20multi-task%20frameworks%20that%20incorporate%20auxiliary%0Atasks%20like%20classification%20to%20improve%20segmentation.%20However%2C%20these%20methods%20often%0Aneed%20more%20labeled%20data%20and%20depend%20on%20task%20similarity%2C%20potentially%20limiting%0Ageneralizability.%20To%20address%20these%20challenges%2C%20we%20propose%20CL-Polyp%2C%20a%0Acontrastive%20learning-enhanced%20polyp%20segmentation%20network.%20Our%20method%20uses%0Acontrastive%20learning%20to%20enhance%20the%20encoder%27s%20extraction%20of%20discriminative%0Afeatures%20by%20contrasting%20positive%20and%20negative%20sample%20pairs%20from%20polyp%20images.%0AThis%20self-supervised%20strategy%20improves%20visual%20representation%20without%20needing%0Aadditional%20annotations.%20We%20also%20introduce%20two%20efficient%2C%20lightweight%20modules%3A%0Athe%20Modified%20Atrous%20Spatial%20Pyramid%20Pooling%20%28MASPP%29%20module%20for%20improved%0Amulti-scale%20feature%20fusion%2C%20and%20the%20Channel%20Concatenate%20and%20Element%20Add%20%28CA%29%0Amodule%20to%20merge%20low-level%20and%20upsampled%20features%20for%20%7Benhanced%7D%20boundary%0Areconstruction.%20Extensive%20experiments%20on%20five%20benchmark%20datasets-Kvasir-SEG%2C%0ACVC-ClinicDB%2C%20CVC-ColonDB%2C%20CVC-300%2C%20and%20ETIS-show%20that%20CL-Polyp%20consistently%0Asurpasses%20state-of-the-art%20methods.%20Specifically%2C%20it%20enhances%20the%20IoU%20metric%20by%0A0.011%20and%200.020%20on%20the%20Kvasir-SEG%20and%20CVC-ClinicDB%20datasets%2C%20respectively%2C%0Ademonstrating%20its%20effectiveness%20in%20clinical%20polyp%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07154v2&entry.124074799=Read"},
{"title": "QRetinex-Net: Quaternion-Valued Retinex Decomposition for Low-Level\n  Computer Vision Applications", "author": "Sos Agaian and Vladimir Frants", "abstract": "  Images taken in low light often show color shift, low contrast, noise, and\nother artifacts that hurt computer-vision accuracy. Retinex theory addresses\nthis by viewing an image S as the pixel-wise product of reflectance R and\nillumination I, mirroring the way people perceive stable object colors under\nchanging light. The decomposition is ill-posed, and classic Retinex models have\nfour key flaws: (i) they treat the red, green, and blue channels independently;\n(ii) they lack a neuroscientific model of color vision; (iii) they cannot\nperfectly rebuild the input image; and (iv) they do not explain human color\nconstancy. We introduce the first Quaternion Retinex formulation, in which the\nscene is written as the Hamilton product of quaternion-valued reflectance and\nillumination. To gauge how well reflectance stays invariant, we propose the\nReflectance Consistency Index. Tests on low-light crack inspection, face\ndetection under varied lighting, and infrared-visible fusion show gains of 2-11\npercent over leading methods, with better color fidelity, lower noise, and\nhigher reflectance stability.\n", "link": "http://arxiv.org/abs/2507.16683v1", "date": "2025-07-22", "relevancy": 2.1238, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5398}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.528}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QRetinex-Net%3A%20Quaternion-Valued%20Retinex%20Decomposition%20for%20Low-Level%0A%20%20Computer%20Vision%20Applications&body=Title%3A%20QRetinex-Net%3A%20Quaternion-Valued%20Retinex%20Decomposition%20for%20Low-Level%0A%20%20Computer%20Vision%20Applications%0AAuthor%3A%20Sos%20Agaian%20and%20Vladimir%20Frants%0AAbstract%3A%20%20%20Images%20taken%20in%20low%20light%20often%20show%20color%20shift%2C%20low%20contrast%2C%20noise%2C%20and%0Aother%20artifacts%20that%20hurt%20computer-vision%20accuracy.%20Retinex%20theory%20addresses%0Athis%20by%20viewing%20an%20image%20S%20as%20the%20pixel-wise%20product%20of%20reflectance%20R%20and%0Aillumination%20I%2C%20mirroring%20the%20way%20people%20perceive%20stable%20object%20colors%20under%0Achanging%20light.%20The%20decomposition%20is%20ill-posed%2C%20and%20classic%20Retinex%20models%20have%0Afour%20key%20flaws%3A%20%28i%29%20they%20treat%20the%20red%2C%20green%2C%20and%20blue%20channels%20independently%3B%0A%28ii%29%20they%20lack%20a%20neuroscientific%20model%20of%20color%20vision%3B%20%28iii%29%20they%20cannot%0Aperfectly%20rebuild%20the%20input%20image%3B%20and%20%28iv%29%20they%20do%20not%20explain%20human%20color%0Aconstancy.%20We%20introduce%20the%20first%20Quaternion%20Retinex%20formulation%2C%20in%20which%20the%0Ascene%20is%20written%20as%20the%20Hamilton%20product%20of%20quaternion-valued%20reflectance%20and%0Aillumination.%20To%20gauge%20how%20well%20reflectance%20stays%20invariant%2C%20we%20propose%20the%0AReflectance%20Consistency%20Index.%20Tests%20on%20low-light%20crack%20inspection%2C%20face%0Adetection%20under%20varied%20lighting%2C%20and%20infrared-visible%20fusion%20show%20gains%20of%202-11%0Apercent%20over%20leading%20methods%2C%20with%20better%20color%20fidelity%2C%20lower%20noise%2C%20and%0Ahigher%20reflectance%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQRetinex-Net%253A%2520Quaternion-Valued%2520Retinex%2520Decomposition%2520for%2520Low-Level%250A%2520%2520Computer%2520Vision%2520Applications%26entry.906535625%3DSos%2520Agaian%2520and%2520Vladimir%2520Frants%26entry.1292438233%3D%2520%2520Images%2520taken%2520in%2520low%2520light%2520often%2520show%2520color%2520shift%252C%2520low%2520contrast%252C%2520noise%252C%2520and%250Aother%2520artifacts%2520that%2520hurt%2520computer-vision%2520accuracy.%2520Retinex%2520theory%2520addresses%250Athis%2520by%2520viewing%2520an%2520image%2520S%2520as%2520the%2520pixel-wise%2520product%2520of%2520reflectance%2520R%2520and%250Aillumination%2520I%252C%2520mirroring%2520the%2520way%2520people%2520perceive%2520stable%2520object%2520colors%2520under%250Achanging%2520light.%2520The%2520decomposition%2520is%2520ill-posed%252C%2520and%2520classic%2520Retinex%2520models%2520have%250Afour%2520key%2520flaws%253A%2520%2528i%2529%2520they%2520treat%2520the%2520red%252C%2520green%252C%2520and%2520blue%2520channels%2520independently%253B%250A%2528ii%2529%2520they%2520lack%2520a%2520neuroscientific%2520model%2520of%2520color%2520vision%253B%2520%2528iii%2529%2520they%2520cannot%250Aperfectly%2520rebuild%2520the%2520input%2520image%253B%2520and%2520%2528iv%2529%2520they%2520do%2520not%2520explain%2520human%2520color%250Aconstancy.%2520We%2520introduce%2520the%2520first%2520Quaternion%2520Retinex%2520formulation%252C%2520in%2520which%2520the%250Ascene%2520is%2520written%2520as%2520the%2520Hamilton%2520product%2520of%2520quaternion-valued%2520reflectance%2520and%250Aillumination.%2520To%2520gauge%2520how%2520well%2520reflectance%2520stays%2520invariant%252C%2520we%2520propose%2520the%250AReflectance%2520Consistency%2520Index.%2520Tests%2520on%2520low-light%2520crack%2520inspection%252C%2520face%250Adetection%2520under%2520varied%2520lighting%252C%2520and%2520infrared-visible%2520fusion%2520show%2520gains%2520of%25202-11%250Apercent%2520over%2520leading%2520methods%252C%2520with%2520better%2520color%2520fidelity%252C%2520lower%2520noise%252C%2520and%250Ahigher%2520reflectance%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QRetinex-Net%3A%20Quaternion-Valued%20Retinex%20Decomposition%20for%20Low-Level%0A%20%20Computer%20Vision%20Applications&entry.906535625=Sos%20Agaian%20and%20Vladimir%20Frants&entry.1292438233=%20%20Images%20taken%20in%20low%20light%20often%20show%20color%20shift%2C%20low%20contrast%2C%20noise%2C%20and%0Aother%20artifacts%20that%20hurt%20computer-vision%20accuracy.%20Retinex%20theory%20addresses%0Athis%20by%20viewing%20an%20image%20S%20as%20the%20pixel-wise%20product%20of%20reflectance%20R%20and%0Aillumination%20I%2C%20mirroring%20the%20way%20people%20perceive%20stable%20object%20colors%20under%0Achanging%20light.%20The%20decomposition%20is%20ill-posed%2C%20and%20classic%20Retinex%20models%20have%0Afour%20key%20flaws%3A%20%28i%29%20they%20treat%20the%20red%2C%20green%2C%20and%20blue%20channels%20independently%3B%0A%28ii%29%20they%20lack%20a%20neuroscientific%20model%20of%20color%20vision%3B%20%28iii%29%20they%20cannot%0Aperfectly%20rebuild%20the%20input%20image%3B%20and%20%28iv%29%20they%20do%20not%20explain%20human%20color%0Aconstancy.%20We%20introduce%20the%20first%20Quaternion%20Retinex%20formulation%2C%20in%20which%20the%0Ascene%20is%20written%20as%20the%20Hamilton%20product%20of%20quaternion-valued%20reflectance%20and%0Aillumination.%20To%20gauge%20how%20well%20reflectance%20stays%20invariant%2C%20we%20propose%20the%0AReflectance%20Consistency%20Index.%20Tests%20on%20low-light%20crack%20inspection%2C%20face%0Adetection%20under%20varied%20lighting%2C%20and%20infrared-visible%20fusion%20show%20gains%20of%202-11%0Apercent%20over%20leading%20methods%2C%20with%20better%20color%20fidelity%2C%20lower%20noise%2C%20and%0Ahigher%20reflectance%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16683v1&entry.124074799=Read"},
{"title": "Improving U-Net Confidence on TEM Image Data with L2-Regularization,\n  Transfer Learning, and Deep Fine-Tuning", "author": "Aiden Ochoa and Xinyuan Xu and Xing Wang", "abstract": "  With ever-increasing data volumes, it is essential to develop automated\napproaches for identifying nanoscale defects in transmission electron\nmicroscopy (TEM) images. However, compared to features in conventional\nphotographs, nanoscale defects in TEM images exhibit far greater variation due\nto the complex contrast mechanisms and intricate defect structures. These\nchallenges often result in much less labeled data and higher rates of\nannotation errors, posing significant obstacles to improving machine learning\nmodel performance for TEM image analysis. To address these limitations, we\nexamined transfer learning by leveraging large, pre-trained models used for\nnatural images.\n  We demonstrated that by using the pre-trained encoder and L2-regularization,\nsemantically complex features are ignored in favor of simpler, more reliable\ncues, substantially improving the model performance. However, this improvement\ncannot be captured by conventional evaluation metrics such as F1-score, which\ncan be skewed by human annotation errors treated as ground truth. Instead, we\nintroduced novel evaluation metrics that are independent of the annotation\naccuracy. Using grain boundary detection in UO2 TEM images as a case study, we\nfound that our approach led to a 57% improvement in defect detection rate,\nwhich is a robust and holistic measure of model performance on the TEM dataset\nused in this work. Finally, we showed that model self-confidence is only\nachieved through transfer learning and fine-tuning of very deep layers.\n", "link": "http://arxiv.org/abs/2507.16779v1", "date": "2025-07-22", "relevancy": 2.1229, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5761}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5343}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20U-Net%20Confidence%20on%20TEM%20Image%20Data%20with%20L2-Regularization%2C%0A%20%20Transfer%20Learning%2C%20and%20Deep%20Fine-Tuning&body=Title%3A%20Improving%20U-Net%20Confidence%20on%20TEM%20Image%20Data%20with%20L2-Regularization%2C%0A%20%20Transfer%20Learning%2C%20and%20Deep%20Fine-Tuning%0AAuthor%3A%20Aiden%20Ochoa%20and%20Xinyuan%20Xu%20and%20Xing%20Wang%0AAbstract%3A%20%20%20With%20ever-increasing%20data%20volumes%2C%20it%20is%20essential%20to%20develop%20automated%0Aapproaches%20for%20identifying%20nanoscale%20defects%20in%20transmission%20electron%0Amicroscopy%20%28TEM%29%20images.%20However%2C%20compared%20to%20features%20in%20conventional%0Aphotographs%2C%20nanoscale%20defects%20in%20TEM%20images%20exhibit%20far%20greater%20variation%20due%0Ato%20the%20complex%20contrast%20mechanisms%20and%20intricate%20defect%20structures.%20These%0Achallenges%20often%20result%20in%20much%20less%20labeled%20data%20and%20higher%20rates%20of%0Aannotation%20errors%2C%20posing%20significant%20obstacles%20to%20improving%20machine%20learning%0Amodel%20performance%20for%20TEM%20image%20analysis.%20To%20address%20these%20limitations%2C%20we%0Aexamined%20transfer%20learning%20by%20leveraging%20large%2C%20pre-trained%20models%20used%20for%0Anatural%20images.%0A%20%20We%20demonstrated%20that%20by%20using%20the%20pre-trained%20encoder%20and%20L2-regularization%2C%0Asemantically%20complex%20features%20are%20ignored%20in%20favor%20of%20simpler%2C%20more%20reliable%0Acues%2C%20substantially%20improving%20the%20model%20performance.%20However%2C%20this%20improvement%0Acannot%20be%20captured%20by%20conventional%20evaluation%20metrics%20such%20as%20F1-score%2C%20which%0Acan%20be%20skewed%20by%20human%20annotation%20errors%20treated%20as%20ground%20truth.%20Instead%2C%20we%0Aintroduced%20novel%20evaluation%20metrics%20that%20are%20independent%20of%20the%20annotation%0Aaccuracy.%20Using%20grain%20boundary%20detection%20in%20UO2%20TEM%20images%20as%20a%20case%20study%2C%20we%0Afound%20that%20our%20approach%20led%20to%20a%2057%25%20improvement%20in%20defect%20detection%20rate%2C%0Awhich%20is%20a%20robust%20and%20holistic%20measure%20of%20model%20performance%20on%20the%20TEM%20dataset%0Aused%20in%20this%20work.%20Finally%2C%20we%20showed%20that%20model%20self-confidence%20is%20only%0Aachieved%20through%20transfer%20learning%20and%20fine-tuning%20of%20very%20deep%20layers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520U-Net%2520Confidence%2520on%2520TEM%2520Image%2520Data%2520with%2520L2-Regularization%252C%250A%2520%2520Transfer%2520Learning%252C%2520and%2520Deep%2520Fine-Tuning%26entry.906535625%3DAiden%2520Ochoa%2520and%2520Xinyuan%2520Xu%2520and%2520Xing%2520Wang%26entry.1292438233%3D%2520%2520With%2520ever-increasing%2520data%2520volumes%252C%2520it%2520is%2520essential%2520to%2520develop%2520automated%250Aapproaches%2520for%2520identifying%2520nanoscale%2520defects%2520in%2520transmission%2520electron%250Amicroscopy%2520%2528TEM%2529%2520images.%2520However%252C%2520compared%2520to%2520features%2520in%2520conventional%250Aphotographs%252C%2520nanoscale%2520defects%2520in%2520TEM%2520images%2520exhibit%2520far%2520greater%2520variation%2520due%250Ato%2520the%2520complex%2520contrast%2520mechanisms%2520and%2520intricate%2520defect%2520structures.%2520These%250Achallenges%2520often%2520result%2520in%2520much%2520less%2520labeled%2520data%2520and%2520higher%2520rates%2520of%250Aannotation%2520errors%252C%2520posing%2520significant%2520obstacles%2520to%2520improving%2520machine%2520learning%250Amodel%2520performance%2520for%2520TEM%2520image%2520analysis.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aexamined%2520transfer%2520learning%2520by%2520leveraging%2520large%252C%2520pre-trained%2520models%2520used%2520for%250Anatural%2520images.%250A%2520%2520We%2520demonstrated%2520that%2520by%2520using%2520the%2520pre-trained%2520encoder%2520and%2520L2-regularization%252C%250Asemantically%2520complex%2520features%2520are%2520ignored%2520in%2520favor%2520of%2520simpler%252C%2520more%2520reliable%250Acues%252C%2520substantially%2520improving%2520the%2520model%2520performance.%2520However%252C%2520this%2520improvement%250Acannot%2520be%2520captured%2520by%2520conventional%2520evaluation%2520metrics%2520such%2520as%2520F1-score%252C%2520which%250Acan%2520be%2520skewed%2520by%2520human%2520annotation%2520errors%2520treated%2520as%2520ground%2520truth.%2520Instead%252C%2520we%250Aintroduced%2520novel%2520evaluation%2520metrics%2520that%2520are%2520independent%2520of%2520the%2520annotation%250Aaccuracy.%2520Using%2520grain%2520boundary%2520detection%2520in%2520UO2%2520TEM%2520images%2520as%2520a%2520case%2520study%252C%2520we%250Afound%2520that%2520our%2520approach%2520led%2520to%2520a%252057%2525%2520improvement%2520in%2520defect%2520detection%2520rate%252C%250Awhich%2520is%2520a%2520robust%2520and%2520holistic%2520measure%2520of%2520model%2520performance%2520on%2520the%2520TEM%2520dataset%250Aused%2520in%2520this%2520work.%2520Finally%252C%2520we%2520showed%2520that%2520model%2520self-confidence%2520is%2520only%250Aachieved%2520through%2520transfer%2520learning%2520and%2520fine-tuning%2520of%2520very%2520deep%2520layers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20U-Net%20Confidence%20on%20TEM%20Image%20Data%20with%20L2-Regularization%2C%0A%20%20Transfer%20Learning%2C%20and%20Deep%20Fine-Tuning&entry.906535625=Aiden%20Ochoa%20and%20Xinyuan%20Xu%20and%20Xing%20Wang&entry.1292438233=%20%20With%20ever-increasing%20data%20volumes%2C%20it%20is%20essential%20to%20develop%20automated%0Aapproaches%20for%20identifying%20nanoscale%20defects%20in%20transmission%20electron%0Amicroscopy%20%28TEM%29%20images.%20However%2C%20compared%20to%20features%20in%20conventional%0Aphotographs%2C%20nanoscale%20defects%20in%20TEM%20images%20exhibit%20far%20greater%20variation%20due%0Ato%20the%20complex%20contrast%20mechanisms%20and%20intricate%20defect%20structures.%20These%0Achallenges%20often%20result%20in%20much%20less%20labeled%20data%20and%20higher%20rates%20of%0Aannotation%20errors%2C%20posing%20significant%20obstacles%20to%20improving%20machine%20learning%0Amodel%20performance%20for%20TEM%20image%20analysis.%20To%20address%20these%20limitations%2C%20we%0Aexamined%20transfer%20learning%20by%20leveraging%20large%2C%20pre-trained%20models%20used%20for%0Anatural%20images.%0A%20%20We%20demonstrated%20that%20by%20using%20the%20pre-trained%20encoder%20and%20L2-regularization%2C%0Asemantically%20complex%20features%20are%20ignored%20in%20favor%20of%20simpler%2C%20more%20reliable%0Acues%2C%20substantially%20improving%20the%20model%20performance.%20However%2C%20this%20improvement%0Acannot%20be%20captured%20by%20conventional%20evaluation%20metrics%20such%20as%20F1-score%2C%20which%0Acan%20be%20skewed%20by%20human%20annotation%20errors%20treated%20as%20ground%20truth.%20Instead%2C%20we%0Aintroduced%20novel%20evaluation%20metrics%20that%20are%20independent%20of%20the%20annotation%0Aaccuracy.%20Using%20grain%20boundary%20detection%20in%20UO2%20TEM%20images%20as%20a%20case%20study%2C%20we%0Afound%20that%20our%20approach%20led%20to%20a%2057%25%20improvement%20in%20defect%20detection%20rate%2C%0Awhich%20is%20a%20robust%20and%20holistic%20measure%20of%20model%20performance%20on%20the%20TEM%20dataset%0Aused%20in%20this%20work.%20Finally%2C%20we%20showed%20that%20model%20self-confidence%20is%20only%0Aachieved%20through%20transfer%20learning%20and%20fine-tuning%20of%20very%20deep%20layers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16779v1&entry.124074799=Read"},
{"title": "Towards Railway Domain Adaptation for LiDAR-based 3D Detection:\n  Road-to-Rail and Sim-to-Real via SynDRA-BBox", "author": "Xavier Diaz and Gianluca D'Amico and Raul Dominguez-Sanchez and Federico Nesti and Max Ronecker and Giorgio Buttazzo", "abstract": "  In recent years, interest in automatic train operations has significantly\nincreased. To enable advanced functionalities, robust vision-based algorithms\nare essential for perceiving and understanding the surrounding environment.\nHowever, the railway sector suffers from a lack of publicly available\nreal-world annotated datasets, making it challenging to test and validate new\nperception solutions in this domain. To address this gap, we introduce\nSynDRA-BBox, a synthetic dataset designed to support object detection and other\nvision-based tasks in realistic railway scenarios. To the best of our\nknowledge, is the first synthetic dataset specifically tailored for 2D and 3D\nobject detection in the railway domain, the dataset is publicly available at\nhttps://syndra.retis.santannapisa.it. In the presented evaluation, a\nstate-of-the-art semi-supervised domain adaptation method, originally developed\nfor automotive perception, is adapted to the railway context, enabling the\ntransferability of synthetic data to 3D object detection. Experimental results\ndemonstrate promising performance, highlighting the effectiveness of synthetic\ndatasets and domain adaptation techniques in advancing perception capabilities\nfor railway environments.\n", "link": "http://arxiv.org/abs/2507.16413v1", "date": "2025-07-22", "relevancy": 2.1201, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5369}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5262}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Railway%20Domain%20Adaptation%20for%20LiDAR-based%203D%20Detection%3A%0A%20%20Road-to-Rail%20and%20Sim-to-Real%20via%20SynDRA-BBox&body=Title%3A%20Towards%20Railway%20Domain%20Adaptation%20for%20LiDAR-based%203D%20Detection%3A%0A%20%20Road-to-Rail%20and%20Sim-to-Real%20via%20SynDRA-BBox%0AAuthor%3A%20Xavier%20Diaz%20and%20Gianluca%20D%27Amico%20and%20Raul%20Dominguez-Sanchez%20and%20Federico%20Nesti%20and%20Max%20Ronecker%20and%20Giorgio%20Buttazzo%0AAbstract%3A%20%20%20In%20recent%20years%2C%20interest%20in%20automatic%20train%20operations%20has%20significantly%0Aincreased.%20To%20enable%20advanced%20functionalities%2C%20robust%20vision-based%20algorithms%0Aare%20essential%20for%20perceiving%20and%20understanding%20the%20surrounding%20environment.%0AHowever%2C%20the%20railway%20sector%20suffers%20from%20a%20lack%20of%20publicly%20available%0Areal-world%20annotated%20datasets%2C%20making%20it%20challenging%20to%20test%20and%20validate%20new%0Aperception%20solutions%20in%20this%20domain.%20To%20address%20this%20gap%2C%20we%20introduce%0ASynDRA-BBox%2C%20a%20synthetic%20dataset%20designed%20to%20support%20object%20detection%20and%20other%0Avision-based%20tasks%20in%20realistic%20railway%20scenarios.%20To%20the%20best%20of%20our%0Aknowledge%2C%20is%20the%20first%20synthetic%20dataset%20specifically%20tailored%20for%202D%20and%203D%0Aobject%20detection%20in%20the%20railway%20domain%2C%20the%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//syndra.retis.santannapisa.it.%20In%20the%20presented%20evaluation%2C%20a%0Astate-of-the-art%20semi-supervised%20domain%20adaptation%20method%2C%20originally%20developed%0Afor%20automotive%20perception%2C%20is%20adapted%20to%20the%20railway%20context%2C%20enabling%20the%0Atransferability%20of%20synthetic%20data%20to%203D%20object%20detection.%20Experimental%20results%0Ademonstrate%20promising%20performance%2C%20highlighting%20the%20effectiveness%20of%20synthetic%0Adatasets%20and%20domain%20adaptation%20techniques%20in%20advancing%20perception%20capabilities%0Afor%20railway%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Railway%2520Domain%2520Adaptation%2520for%2520LiDAR-based%25203D%2520Detection%253A%250A%2520%2520Road-to-Rail%2520and%2520Sim-to-Real%2520via%2520SynDRA-BBox%26entry.906535625%3DXavier%2520Diaz%2520and%2520Gianluca%2520D%2527Amico%2520and%2520Raul%2520Dominguez-Sanchez%2520and%2520Federico%2520Nesti%2520and%2520Max%2520Ronecker%2520and%2520Giorgio%2520Buttazzo%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520interest%2520in%2520automatic%2520train%2520operations%2520has%2520significantly%250Aincreased.%2520To%2520enable%2520advanced%2520functionalities%252C%2520robust%2520vision-based%2520algorithms%250Aare%2520essential%2520for%2520perceiving%2520and%2520understanding%2520the%2520surrounding%2520environment.%250AHowever%252C%2520the%2520railway%2520sector%2520suffers%2520from%2520a%2520lack%2520of%2520publicly%2520available%250Areal-world%2520annotated%2520datasets%252C%2520making%2520it%2520challenging%2520to%2520test%2520and%2520validate%2520new%250Aperception%2520solutions%2520in%2520this%2520domain.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250ASynDRA-BBox%252C%2520a%2520synthetic%2520dataset%2520designed%2520to%2520support%2520object%2520detection%2520and%2520other%250Avision-based%2520tasks%2520in%2520realistic%2520railway%2520scenarios.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520is%2520the%2520first%2520synthetic%2520dataset%2520specifically%2520tailored%2520for%25202D%2520and%25203D%250Aobject%2520detection%2520in%2520the%2520railway%2520domain%252C%2520the%2520dataset%2520is%2520publicly%2520available%2520at%250Ahttps%253A//syndra.retis.santannapisa.it.%2520In%2520the%2520presented%2520evaluation%252C%2520a%250Astate-of-the-art%2520semi-supervised%2520domain%2520adaptation%2520method%252C%2520originally%2520developed%250Afor%2520automotive%2520perception%252C%2520is%2520adapted%2520to%2520the%2520railway%2520context%252C%2520enabling%2520the%250Atransferability%2520of%2520synthetic%2520data%2520to%25203D%2520object%2520detection.%2520Experimental%2520results%250Ademonstrate%2520promising%2520performance%252C%2520highlighting%2520the%2520effectiveness%2520of%2520synthetic%250Adatasets%2520and%2520domain%2520adaptation%2520techniques%2520in%2520advancing%2520perception%2520capabilities%250Afor%2520railway%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Railway%20Domain%20Adaptation%20for%20LiDAR-based%203D%20Detection%3A%0A%20%20Road-to-Rail%20and%20Sim-to-Real%20via%20SynDRA-BBox&entry.906535625=Xavier%20Diaz%20and%20Gianluca%20D%27Amico%20and%20Raul%20Dominguez-Sanchez%20and%20Federico%20Nesti%20and%20Max%20Ronecker%20and%20Giorgio%20Buttazzo&entry.1292438233=%20%20In%20recent%20years%2C%20interest%20in%20automatic%20train%20operations%20has%20significantly%0Aincreased.%20To%20enable%20advanced%20functionalities%2C%20robust%20vision-based%20algorithms%0Aare%20essential%20for%20perceiving%20and%20understanding%20the%20surrounding%20environment.%0AHowever%2C%20the%20railway%20sector%20suffers%20from%20a%20lack%20of%20publicly%20available%0Areal-world%20annotated%20datasets%2C%20making%20it%20challenging%20to%20test%20and%20validate%20new%0Aperception%20solutions%20in%20this%20domain.%20To%20address%20this%20gap%2C%20we%20introduce%0ASynDRA-BBox%2C%20a%20synthetic%20dataset%20designed%20to%20support%20object%20detection%20and%20other%0Avision-based%20tasks%20in%20realistic%20railway%20scenarios.%20To%20the%20best%20of%20our%0Aknowledge%2C%20is%20the%20first%20synthetic%20dataset%20specifically%20tailored%20for%202D%20and%203D%0Aobject%20detection%20in%20the%20railway%20domain%2C%20the%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//syndra.retis.santannapisa.it.%20In%20the%20presented%20evaluation%2C%20a%0Astate-of-the-art%20semi-supervised%20domain%20adaptation%20method%2C%20originally%20developed%0Afor%20automotive%20perception%2C%20is%20adapted%20to%20the%20railway%20context%2C%20enabling%20the%0Atransferability%20of%20synthetic%20data%20to%203D%20object%20detection.%20Experimental%20results%0Ademonstrate%20promising%20performance%2C%20highlighting%20the%20effectiveness%20of%20synthetic%0Adatasets%20and%20domain%20adaptation%20techniques%20in%20advancing%20perception%20capabilities%0Afor%20railway%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16413v1&entry.124074799=Read"},
{"title": "DenseSR: Image Shadow Removal as Dense Prediction", "author": "Yu-Fan Lin and Chia-Ming Lee and Chih-Chung Hsu", "abstract": "  Shadows are a common factor degrading image quality. Single-image shadow\nremoval (SR), particularly under challenging indirect illumination, is hampered\nby non-uniform content degradation and inherent ambiguity. Consequently,\ntraditional methods often fail to simultaneously recover intra-shadow details\nand maintain sharp boundaries, resulting in inconsistent restoration and\nblurring that negatively affect both downstream applications and the overall\nviewing experience. To overcome these limitations, we propose the DenseSR,\napproaching the problem from a dense prediction perspective to emphasize\nrestoration quality. This framework uniquely synergizes two key strategies: (1)\ndeep scene understanding guided by geometric-semantic priors to resolve\nambiguity and implicitly localize shadows, and (2) high-fidelity restoration\nvia a novel Dense Fusion Block (DFB) in the decoder. The DFB employs adaptive\ncomponent processing-using an Adaptive Content Smoothing Module (ACSM) for\nconsistent appearance and a Texture-Boundary Recuperation Module (TBRM) for\nfine textures and sharp boundaries-thereby directly tackling the inconsistent\nrestoration and blurring issues. These purposefully processed components are\neffectively fused, yielding an optimized feature representation preserving both\nconsistency and fidelity. Extensive experimental results demonstrate the merits\nof our approach over existing methods. Our code can be available on\nhttps://github$.$com/VanLinLin/DenseSR\n", "link": "http://arxiv.org/abs/2507.16472v1", "date": "2025-07-22", "relevancy": 2.1071, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5566}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.524}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DenseSR%3A%20Image%20Shadow%20Removal%20as%20Dense%20Prediction&body=Title%3A%20DenseSR%3A%20Image%20Shadow%20Removal%20as%20Dense%20Prediction%0AAuthor%3A%20Yu-Fan%20Lin%20and%20Chia-Ming%20Lee%20and%20Chih-Chung%20Hsu%0AAbstract%3A%20%20%20Shadows%20are%20a%20common%20factor%20degrading%20image%20quality.%20Single-image%20shadow%0Aremoval%20%28SR%29%2C%20particularly%20under%20challenging%20indirect%20illumination%2C%20is%20hampered%0Aby%20non-uniform%20content%20degradation%20and%20inherent%20ambiguity.%20Consequently%2C%0Atraditional%20methods%20often%20fail%20to%20simultaneously%20recover%20intra-shadow%20details%0Aand%20maintain%20sharp%20boundaries%2C%20resulting%20in%20inconsistent%20restoration%20and%0Ablurring%20that%20negatively%20affect%20both%20downstream%20applications%20and%20the%20overall%0Aviewing%20experience.%20To%20overcome%20these%20limitations%2C%20we%20propose%20the%20DenseSR%2C%0Aapproaching%20the%20problem%20from%20a%20dense%20prediction%20perspective%20to%20emphasize%0Arestoration%20quality.%20This%20framework%20uniquely%20synergizes%20two%20key%20strategies%3A%20%281%29%0Adeep%20scene%20understanding%20guided%20by%20geometric-semantic%20priors%20to%20resolve%0Aambiguity%20and%20implicitly%20localize%20shadows%2C%20and%20%282%29%20high-fidelity%20restoration%0Avia%20a%20novel%20Dense%20Fusion%20Block%20%28DFB%29%20in%20the%20decoder.%20The%20DFB%20employs%20adaptive%0Acomponent%20processing-using%20an%20Adaptive%20Content%20Smoothing%20Module%20%28ACSM%29%20for%0Aconsistent%20appearance%20and%20a%20Texture-Boundary%20Recuperation%20Module%20%28TBRM%29%20for%0Afine%20textures%20and%20sharp%20boundaries-thereby%20directly%20tackling%20the%20inconsistent%0Arestoration%20and%20blurring%20issues.%20These%20purposefully%20processed%20components%20are%0Aeffectively%20fused%2C%20yielding%20an%20optimized%20feature%20representation%20preserving%20both%0Aconsistency%20and%20fidelity.%20Extensive%20experimental%20results%20demonstrate%20the%20merits%0Aof%20our%20approach%20over%20existing%20methods.%20Our%20code%20can%20be%20available%20on%0Ahttps%3A//github%24.%24com/VanLinLin/DenseSR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDenseSR%253A%2520Image%2520Shadow%2520Removal%2520as%2520Dense%2520Prediction%26entry.906535625%3DYu-Fan%2520Lin%2520and%2520Chia-Ming%2520Lee%2520and%2520Chih-Chung%2520Hsu%26entry.1292438233%3D%2520%2520Shadows%2520are%2520a%2520common%2520factor%2520degrading%2520image%2520quality.%2520Single-image%2520shadow%250Aremoval%2520%2528SR%2529%252C%2520particularly%2520under%2520challenging%2520indirect%2520illumination%252C%2520is%2520hampered%250Aby%2520non-uniform%2520content%2520degradation%2520and%2520inherent%2520ambiguity.%2520Consequently%252C%250Atraditional%2520methods%2520often%2520fail%2520to%2520simultaneously%2520recover%2520intra-shadow%2520details%250Aand%2520maintain%2520sharp%2520boundaries%252C%2520resulting%2520in%2520inconsistent%2520restoration%2520and%250Ablurring%2520that%2520negatively%2520affect%2520both%2520downstream%2520applications%2520and%2520the%2520overall%250Aviewing%2520experience.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520the%2520DenseSR%252C%250Aapproaching%2520the%2520problem%2520from%2520a%2520dense%2520prediction%2520perspective%2520to%2520emphasize%250Arestoration%2520quality.%2520This%2520framework%2520uniquely%2520synergizes%2520two%2520key%2520strategies%253A%2520%25281%2529%250Adeep%2520scene%2520understanding%2520guided%2520by%2520geometric-semantic%2520priors%2520to%2520resolve%250Aambiguity%2520and%2520implicitly%2520localize%2520shadows%252C%2520and%2520%25282%2529%2520high-fidelity%2520restoration%250Avia%2520a%2520novel%2520Dense%2520Fusion%2520Block%2520%2528DFB%2529%2520in%2520the%2520decoder.%2520The%2520DFB%2520employs%2520adaptive%250Acomponent%2520processing-using%2520an%2520Adaptive%2520Content%2520Smoothing%2520Module%2520%2528ACSM%2529%2520for%250Aconsistent%2520appearance%2520and%2520a%2520Texture-Boundary%2520Recuperation%2520Module%2520%2528TBRM%2529%2520for%250Afine%2520textures%2520and%2520sharp%2520boundaries-thereby%2520directly%2520tackling%2520the%2520inconsistent%250Arestoration%2520and%2520blurring%2520issues.%2520These%2520purposefully%2520processed%2520components%2520are%250Aeffectively%2520fused%252C%2520yielding%2520an%2520optimized%2520feature%2520representation%2520preserving%2520both%250Aconsistency%2520and%2520fidelity.%2520Extensive%2520experimental%2520results%2520demonstrate%2520the%2520merits%250Aof%2520our%2520approach%2520over%2520existing%2520methods.%2520Our%2520code%2520can%2520be%2520available%2520on%250Ahttps%253A//github%2524.%2524com/VanLinLin/DenseSR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DenseSR%3A%20Image%20Shadow%20Removal%20as%20Dense%20Prediction&entry.906535625=Yu-Fan%20Lin%20and%20Chia-Ming%20Lee%20and%20Chih-Chung%20Hsu&entry.1292438233=%20%20Shadows%20are%20a%20common%20factor%20degrading%20image%20quality.%20Single-image%20shadow%0Aremoval%20%28SR%29%2C%20particularly%20under%20challenging%20indirect%20illumination%2C%20is%20hampered%0Aby%20non-uniform%20content%20degradation%20and%20inherent%20ambiguity.%20Consequently%2C%0Atraditional%20methods%20often%20fail%20to%20simultaneously%20recover%20intra-shadow%20details%0Aand%20maintain%20sharp%20boundaries%2C%20resulting%20in%20inconsistent%20restoration%20and%0Ablurring%20that%20negatively%20affect%20both%20downstream%20applications%20and%20the%20overall%0Aviewing%20experience.%20To%20overcome%20these%20limitations%2C%20we%20propose%20the%20DenseSR%2C%0Aapproaching%20the%20problem%20from%20a%20dense%20prediction%20perspective%20to%20emphasize%0Arestoration%20quality.%20This%20framework%20uniquely%20synergizes%20two%20key%20strategies%3A%20%281%29%0Adeep%20scene%20understanding%20guided%20by%20geometric-semantic%20priors%20to%20resolve%0Aambiguity%20and%20implicitly%20localize%20shadows%2C%20and%20%282%29%20high-fidelity%20restoration%0Avia%20a%20novel%20Dense%20Fusion%20Block%20%28DFB%29%20in%20the%20decoder.%20The%20DFB%20employs%20adaptive%0Acomponent%20processing-using%20an%20Adaptive%20Content%20Smoothing%20Module%20%28ACSM%29%20for%0Aconsistent%20appearance%20and%20a%20Texture-Boundary%20Recuperation%20Module%20%28TBRM%29%20for%0Afine%20textures%20and%20sharp%20boundaries-thereby%20directly%20tackling%20the%20inconsistent%0Arestoration%20and%20blurring%20issues.%20These%20purposefully%20processed%20components%20are%0Aeffectively%20fused%2C%20yielding%20an%20optimized%20feature%20representation%20preserving%20both%0Aconsistency%20and%20fidelity.%20Extensive%20experimental%20results%20demonstrate%20the%20merits%0Aof%20our%20approach%20over%20existing%20methods.%20Our%20code%20can%20be%20available%20on%0Ahttps%3A//github%24.%24com/VanLinLin/DenseSR%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16472v1&entry.124074799=Read"},
{"title": "A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge\n  Graph Fusion Framework for Rare Disease Diagnosis", "author": "Mingda Zhang and Na Zhao and Jianglong Qin and Guoyu Ye and Ruixiang Tang", "abstract": "  Rare disease diagnosis remains challenging for medical large language models\ndue to insufficient knowledge representation, limited concept understanding,\nand constrained clinical reasoning. We propose a framework combining\nmulti-granularity sparse activation with hierarchical knowledge graphs. Our\napproach employs four complementary matching algorithms with diversity control\nand a five-level fallback strategy for precise concept activation. A\nthree-layer knowledge graph (taxonomy, clinical features, instances) provides\nstructured, up-to-date context. Experiments on the BioASQ rare disease dataset\ndemonstrate significant improvements: BLEU scores increased by up to 0.13,\nROUGE by up to 0.10, and diagnostic accuracy by up to 0.25, with the best model\nachieving 0.92 accuracy--surpassing the 0.90 clinical threshold. Expert\nevaluation confirms enhancements in information quality, reasoning, and\nprofessional expression. Our framework shows promise in reducing the diagnostic\nodyssey for rare disease patients.\n", "link": "http://arxiv.org/abs/2507.08529v2", "date": "2025-07-22", "relevancy": 2.0945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5273}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-granularity%20Concept%20Sparse%20Activation%20and%20Hierarchical%20Knowledge%0A%20%20Graph%20Fusion%20Framework%20for%20Rare%20Disease%20Diagnosis&body=Title%3A%20A%20Multi-granularity%20Concept%20Sparse%20Activation%20and%20Hierarchical%20Knowledge%0A%20%20Graph%20Fusion%20Framework%20for%20Rare%20Disease%20Diagnosis%0AAuthor%3A%20Mingda%20Zhang%20and%20Na%20Zhao%20and%20Jianglong%20Qin%20and%20Guoyu%20Ye%20and%20Ruixiang%20Tang%0AAbstract%3A%20%20%20Rare%20disease%20diagnosis%20remains%20challenging%20for%20medical%20large%20language%20models%0Adue%20to%20insufficient%20knowledge%20representation%2C%20limited%20concept%20understanding%2C%0Aand%20constrained%20clinical%20reasoning.%20We%20propose%20a%20framework%20combining%0Amulti-granularity%20sparse%20activation%20with%20hierarchical%20knowledge%20graphs.%20Our%0Aapproach%20employs%20four%20complementary%20matching%20algorithms%20with%20diversity%20control%0Aand%20a%20five-level%20fallback%20strategy%20for%20precise%20concept%20activation.%20A%0Athree-layer%20knowledge%20graph%20%28taxonomy%2C%20clinical%20features%2C%20instances%29%20provides%0Astructured%2C%20up-to-date%20context.%20Experiments%20on%20the%20BioASQ%20rare%20disease%20dataset%0Ademonstrate%20significant%20improvements%3A%20BLEU%20scores%20increased%20by%20up%20to%200.13%2C%0AROUGE%20by%20up%20to%200.10%2C%20and%20diagnostic%20accuracy%20by%20up%20to%200.25%2C%20with%20the%20best%20model%0Aachieving%200.92%20accuracy--surpassing%20the%200.90%20clinical%20threshold.%20Expert%0Aevaluation%20confirms%20enhancements%20in%20information%20quality%2C%20reasoning%2C%20and%0Aprofessional%20expression.%20Our%20framework%20shows%20promise%20in%20reducing%20the%20diagnostic%0Aodyssey%20for%20rare%20disease%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08529v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-granularity%2520Concept%2520Sparse%2520Activation%2520and%2520Hierarchical%2520Knowledge%250A%2520%2520Graph%2520Fusion%2520Framework%2520for%2520Rare%2520Disease%2520Diagnosis%26entry.906535625%3DMingda%2520Zhang%2520and%2520Na%2520Zhao%2520and%2520Jianglong%2520Qin%2520and%2520Guoyu%2520Ye%2520and%2520Ruixiang%2520Tang%26entry.1292438233%3D%2520%2520Rare%2520disease%2520diagnosis%2520remains%2520challenging%2520for%2520medical%2520large%2520language%2520models%250Adue%2520to%2520insufficient%2520knowledge%2520representation%252C%2520limited%2520concept%2520understanding%252C%250Aand%2520constrained%2520clinical%2520reasoning.%2520We%2520propose%2520a%2520framework%2520combining%250Amulti-granularity%2520sparse%2520activation%2520with%2520hierarchical%2520knowledge%2520graphs.%2520Our%250Aapproach%2520employs%2520four%2520complementary%2520matching%2520algorithms%2520with%2520diversity%2520control%250Aand%2520a%2520five-level%2520fallback%2520strategy%2520for%2520precise%2520concept%2520activation.%2520A%250Athree-layer%2520knowledge%2520graph%2520%2528taxonomy%252C%2520clinical%2520features%252C%2520instances%2529%2520provides%250Astructured%252C%2520up-to-date%2520context.%2520Experiments%2520on%2520the%2520BioASQ%2520rare%2520disease%2520dataset%250Ademonstrate%2520significant%2520improvements%253A%2520BLEU%2520scores%2520increased%2520by%2520up%2520to%25200.13%252C%250AROUGE%2520by%2520up%2520to%25200.10%252C%2520and%2520diagnostic%2520accuracy%2520by%2520up%2520to%25200.25%252C%2520with%2520the%2520best%2520model%250Aachieving%25200.92%2520accuracy--surpassing%2520the%25200.90%2520clinical%2520threshold.%2520Expert%250Aevaluation%2520confirms%2520enhancements%2520in%2520information%2520quality%252C%2520reasoning%252C%2520and%250Aprofessional%2520expression.%2520Our%2520framework%2520shows%2520promise%2520in%2520reducing%2520the%2520diagnostic%250Aodyssey%2520for%2520rare%2520disease%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08529v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-granularity%20Concept%20Sparse%20Activation%20and%20Hierarchical%20Knowledge%0A%20%20Graph%20Fusion%20Framework%20for%20Rare%20Disease%20Diagnosis&entry.906535625=Mingda%20Zhang%20and%20Na%20Zhao%20and%20Jianglong%20Qin%20and%20Guoyu%20Ye%20and%20Ruixiang%20Tang&entry.1292438233=%20%20Rare%20disease%20diagnosis%20remains%20challenging%20for%20medical%20large%20language%20models%0Adue%20to%20insufficient%20knowledge%20representation%2C%20limited%20concept%20understanding%2C%0Aand%20constrained%20clinical%20reasoning.%20We%20propose%20a%20framework%20combining%0Amulti-granularity%20sparse%20activation%20with%20hierarchical%20knowledge%20graphs.%20Our%0Aapproach%20employs%20four%20complementary%20matching%20algorithms%20with%20diversity%20control%0Aand%20a%20five-level%20fallback%20strategy%20for%20precise%20concept%20activation.%20A%0Athree-layer%20knowledge%20graph%20%28taxonomy%2C%20clinical%20features%2C%20instances%29%20provides%0Astructured%2C%20up-to-date%20context.%20Experiments%20on%20the%20BioASQ%20rare%20disease%20dataset%0Ademonstrate%20significant%20improvements%3A%20BLEU%20scores%20increased%20by%20up%20to%200.13%2C%0AROUGE%20by%20up%20to%200.10%2C%20and%20diagnostic%20accuracy%20by%20up%20to%200.25%2C%20with%20the%20best%20model%0Aachieving%200.92%20accuracy--surpassing%20the%200.90%20clinical%20threshold.%20Expert%0Aevaluation%20confirms%20enhancements%20in%20information%20quality%2C%20reasoning%2C%20and%0Aprofessional%20expression.%20Our%20framework%20shows%20promise%20in%20reducing%20the%20diagnostic%0Aodyssey%20for%20rare%20disease%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08529v2&entry.124074799=Read"},
{"title": "Improving Predictions on Highly Unbalanced Data Using Open Source\n  Synthetic Data Upsampling", "author": "Ivona Krchova and Michael Platzer and Paul Tiwald", "abstract": "  Unbalanced tabular data sets present significant challenges for predictive\nmodeling and data analysis across a wide range of applications. In many\nreal-world scenarios, such as fraud detection, medical diagnosis, and rare\nevent prediction, minority classes are vastly underrepresented, making it\ndifficult for traditional machine learning algorithms to achieve high accuracy.\nThese algorithms tend to favor the majority class, leading to biased models\nthat struggle to accurately represent minority classes. Synthetic data holds\npromise for addressing the under-representation of minority classes by\nproviding new, diverse, and highly realistic samples. This paper presents a\nbenchmark study on the use of AI-generated synthetic data for upsampling highly\nunbalanced tabular data sets.\n  We evaluate the effectiveness of an open-source solution, the Synthetic Data\nSDK by MOSTLY AI, which provides a flexible and user-friendly approach to\nsynthetic upsampling for mixed-type data. We compare predictive models trained\non data sets upsampled with synthetic records to those using standard methods,\nsuch as naive oversampling and SMOTE-NC. Our results demonstrate that synthetic\ndata can improve predictive accuracy for minority groups by generating diverse\ndata points that fill gaps in sparse regions of the feature space. We show that\nupsampled synthetic training data consistently results in top-performing\npredictive models, particularly for mixed-type data sets containing very few\nminority samples.\n", "link": "http://arxiv.org/abs/2507.16419v1", "date": "2025-07-22", "relevancy": 2.0938, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.532}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5198}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Predictions%20on%20Highly%20Unbalanced%20Data%20Using%20Open%20Source%0A%20%20Synthetic%20Data%20Upsampling&body=Title%3A%20Improving%20Predictions%20on%20Highly%20Unbalanced%20Data%20Using%20Open%20Source%0A%20%20Synthetic%20Data%20Upsampling%0AAuthor%3A%20Ivona%20Krchova%20and%20Michael%20Platzer%20and%20Paul%20Tiwald%0AAbstract%3A%20%20%20Unbalanced%20tabular%20data%20sets%20present%20significant%20challenges%20for%20predictive%0Amodeling%20and%20data%20analysis%20across%20a%20wide%20range%20of%20applications.%20In%20many%0Areal-world%20scenarios%2C%20such%20as%20fraud%20detection%2C%20medical%20diagnosis%2C%20and%20rare%0Aevent%20prediction%2C%20minority%20classes%20are%20vastly%20underrepresented%2C%20making%20it%0Adifficult%20for%20traditional%20machine%20learning%20algorithms%20to%20achieve%20high%20accuracy.%0AThese%20algorithms%20tend%20to%20favor%20the%20majority%20class%2C%20leading%20to%20biased%20models%0Athat%20struggle%20to%20accurately%20represent%20minority%20classes.%20Synthetic%20data%20holds%0Apromise%20for%20addressing%20the%20under-representation%20of%20minority%20classes%20by%0Aproviding%20new%2C%20diverse%2C%20and%20highly%20realistic%20samples.%20This%20paper%20presents%20a%0Abenchmark%20study%20on%20the%20use%20of%20AI-generated%20synthetic%20data%20for%20upsampling%20highly%0Aunbalanced%20tabular%20data%20sets.%0A%20%20We%20evaluate%20the%20effectiveness%20of%20an%20open-source%20solution%2C%20the%20Synthetic%20Data%0ASDK%20by%20MOSTLY%20AI%2C%20which%20provides%20a%20flexible%20and%20user-friendly%20approach%20to%0Asynthetic%20upsampling%20for%20mixed-type%20data.%20We%20compare%20predictive%20models%20trained%0Aon%20data%20sets%20upsampled%20with%20synthetic%20records%20to%20those%20using%20standard%20methods%2C%0Asuch%20as%20naive%20oversampling%20and%20SMOTE-NC.%20Our%20results%20demonstrate%20that%20synthetic%0Adata%20can%20improve%20predictive%20accuracy%20for%20minority%20groups%20by%20generating%20diverse%0Adata%20points%20that%20fill%20gaps%20in%20sparse%20regions%20of%20the%20feature%20space.%20We%20show%20that%0Aupsampled%20synthetic%20training%20data%20consistently%20results%20in%20top-performing%0Apredictive%20models%2C%20particularly%20for%20mixed-type%20data%20sets%20containing%20very%20few%0Aminority%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Predictions%2520on%2520Highly%2520Unbalanced%2520Data%2520Using%2520Open%2520Source%250A%2520%2520Synthetic%2520Data%2520Upsampling%26entry.906535625%3DIvona%2520Krchova%2520and%2520Michael%2520Platzer%2520and%2520Paul%2520Tiwald%26entry.1292438233%3D%2520%2520Unbalanced%2520tabular%2520data%2520sets%2520present%2520significant%2520challenges%2520for%2520predictive%250Amodeling%2520and%2520data%2520analysis%2520across%2520a%2520wide%2520range%2520of%2520applications.%2520In%2520many%250Areal-world%2520scenarios%252C%2520such%2520as%2520fraud%2520detection%252C%2520medical%2520diagnosis%252C%2520and%2520rare%250Aevent%2520prediction%252C%2520minority%2520classes%2520are%2520vastly%2520underrepresented%252C%2520making%2520it%250Adifficult%2520for%2520traditional%2520machine%2520learning%2520algorithms%2520to%2520achieve%2520high%2520accuracy.%250AThese%2520algorithms%2520tend%2520to%2520favor%2520the%2520majority%2520class%252C%2520leading%2520to%2520biased%2520models%250Athat%2520struggle%2520to%2520accurately%2520represent%2520minority%2520classes.%2520Synthetic%2520data%2520holds%250Apromise%2520for%2520addressing%2520the%2520under-representation%2520of%2520minority%2520classes%2520by%250Aproviding%2520new%252C%2520diverse%252C%2520and%2520highly%2520realistic%2520samples.%2520This%2520paper%2520presents%2520a%250Abenchmark%2520study%2520on%2520the%2520use%2520of%2520AI-generated%2520synthetic%2520data%2520for%2520upsampling%2520highly%250Aunbalanced%2520tabular%2520data%2520sets.%250A%2520%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520an%2520open-source%2520solution%252C%2520the%2520Synthetic%2520Data%250ASDK%2520by%2520MOSTLY%2520AI%252C%2520which%2520provides%2520a%2520flexible%2520and%2520user-friendly%2520approach%2520to%250Asynthetic%2520upsampling%2520for%2520mixed-type%2520data.%2520We%2520compare%2520predictive%2520models%2520trained%250Aon%2520data%2520sets%2520upsampled%2520with%2520synthetic%2520records%2520to%2520those%2520using%2520standard%2520methods%252C%250Asuch%2520as%2520naive%2520oversampling%2520and%2520SMOTE-NC.%2520Our%2520results%2520demonstrate%2520that%2520synthetic%250Adata%2520can%2520improve%2520predictive%2520accuracy%2520for%2520minority%2520groups%2520by%2520generating%2520diverse%250Adata%2520points%2520that%2520fill%2520gaps%2520in%2520sparse%2520regions%2520of%2520the%2520feature%2520space.%2520We%2520show%2520that%250Aupsampled%2520synthetic%2520training%2520data%2520consistently%2520results%2520in%2520top-performing%250Apredictive%2520models%252C%2520particularly%2520for%2520mixed-type%2520data%2520sets%2520containing%2520very%2520few%250Aminority%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Predictions%20on%20Highly%20Unbalanced%20Data%20Using%20Open%20Source%0A%20%20Synthetic%20Data%20Upsampling&entry.906535625=Ivona%20Krchova%20and%20Michael%20Platzer%20and%20Paul%20Tiwald&entry.1292438233=%20%20Unbalanced%20tabular%20data%20sets%20present%20significant%20challenges%20for%20predictive%0Amodeling%20and%20data%20analysis%20across%20a%20wide%20range%20of%20applications.%20In%20many%0Areal-world%20scenarios%2C%20such%20as%20fraud%20detection%2C%20medical%20diagnosis%2C%20and%20rare%0Aevent%20prediction%2C%20minority%20classes%20are%20vastly%20underrepresented%2C%20making%20it%0Adifficult%20for%20traditional%20machine%20learning%20algorithms%20to%20achieve%20high%20accuracy.%0AThese%20algorithms%20tend%20to%20favor%20the%20majority%20class%2C%20leading%20to%20biased%20models%0Athat%20struggle%20to%20accurately%20represent%20minority%20classes.%20Synthetic%20data%20holds%0Apromise%20for%20addressing%20the%20under-representation%20of%20minority%20classes%20by%0Aproviding%20new%2C%20diverse%2C%20and%20highly%20realistic%20samples.%20This%20paper%20presents%20a%0Abenchmark%20study%20on%20the%20use%20of%20AI-generated%20synthetic%20data%20for%20upsampling%20highly%0Aunbalanced%20tabular%20data%20sets.%0A%20%20We%20evaluate%20the%20effectiveness%20of%20an%20open-source%20solution%2C%20the%20Synthetic%20Data%0ASDK%20by%20MOSTLY%20AI%2C%20which%20provides%20a%20flexible%20and%20user-friendly%20approach%20to%0Asynthetic%20upsampling%20for%20mixed-type%20data.%20We%20compare%20predictive%20models%20trained%0Aon%20data%20sets%20upsampled%20with%20synthetic%20records%20to%20those%20using%20standard%20methods%2C%0Asuch%20as%20naive%20oversampling%20and%20SMOTE-NC.%20Our%20results%20demonstrate%20that%20synthetic%0Adata%20can%20improve%20predictive%20accuracy%20for%20minority%20groups%20by%20generating%20diverse%0Adata%20points%20that%20fill%20gaps%20in%20sparse%20regions%20of%20the%20feature%20space.%20We%20show%20that%0Aupsampled%20synthetic%20training%20data%20consistently%20results%20in%20top-performing%0Apredictive%20models%2C%20particularly%20for%20mixed-type%20data%20sets%20containing%20very%20few%0Aminority%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16419v1&entry.124074799=Read"},
{"title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning", "author": "Yanjun Zheng and Xiyang Du and Longfei Liao and Xiaoke Zhao and Zhaowen Zhou and Bo Zhang and Jiawei Liu and Xiang Qi and Zhe Li and Zhiqiang Zhang and Wang Wei and Peng Zhang", "abstract": "  Large Language Models (LLMs) demonstrate tremendous potential in the\nfinancial domain, yet existing models often fall short in scenarios demanding\nrobust reasoning capabilities, stringent trustworthiness requirements, and\nefficient adaptation to task-specific needs. We introduce the Agentar-Fin-R1\nseries of financial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task taxonomy with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage learning processes, and detailed attribution systems,\nwe achieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including FinEva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications.\n", "link": "http://arxiv.org/abs/2507.16802v1", "date": "2025-07-22", "relevancy": 2.0836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5269}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5269}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentar-Fin-R1%3A%20Enhancing%20Financial%20Intelligence%20through%20Domain%0A%20%20Expertise%2C%20Training%20Efficiency%2C%20and%20Advanced%20Reasoning&body=Title%3A%20Agentar-Fin-R1%3A%20Enhancing%20Financial%20Intelligence%20through%20Domain%0A%20%20Expertise%2C%20Training%20Efficiency%2C%20and%20Advanced%20Reasoning%0AAuthor%3A%20Yanjun%20Zheng%20and%20Xiyang%20Du%20and%20Longfei%20Liao%20and%20Xiaoke%20Zhao%20and%20Zhaowen%20Zhou%20and%20Bo%20Zhang%20and%20Jiawei%20Liu%20and%20Xiang%20Qi%20and%20Zhe%20Li%20and%20Zhiqiang%20Zhang%20and%20Wang%20Wei%20and%20Peng%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20tremendous%20potential%20in%20the%0Afinancial%20domain%2C%20yet%20existing%20models%20often%20fall%20short%20in%20scenarios%20demanding%0Arobust%20reasoning%20capabilities%2C%20stringent%20trustworthiness%20requirements%2C%20and%0Aefficient%20adaptation%20to%20task-specific%20needs.%20We%20introduce%20the%20Agentar-Fin-R1%0Aseries%20of%20financial%20large%20language%20models%20%288B%20and%2032B%20parameters%29%2C%20specifically%0Aengineered%20based%20on%20the%20Qwen3%20foundation%20model%20to%20enhance%20reasoning%0Acapabilities%2C%20reliability%2C%20and%20domain%20specialization%20for%20financial%0Aapplications.%20Our%20optimization%20approach%20integrates%20a%20high-quality%2C%20systematic%0Afinancial%20task%20taxonomy%20with%20a%20comprehensive%20multi-layered%20trustworthiness%0Aassurance%20framework.%20This%20framework%20encompasses%20high-quality%20trustworthy%0Aknowledge%20engineering%2C%20multi-agent%20trustworthy%20data%20synthesis%2C%20and%20rigorous%0Adata%20validation%20governance.%20Through%20label-guided%20automated%20difficulty-aware%0Aoptimization%2C%20tow-stage%20learning%20processes%2C%20and%20detailed%20attribution%20systems%2C%0Awe%20achieve%20substantial%20improvements%20in%20training%20efficiency.%20Our%20models%20undergo%0Acomprehensive%20evaluation%20on%20mainstream%20financial%20benchmarks%20including%20FinEva%2C%0AFinEval%2C%20and%20FinanceIQ%2C%20as%20well%20as%20general%20reasoning%20datasets%20such%20as%20MATH-500%0Aand%20GPQA.%20To%20thoroughly%20assess%20real-world%20deployment%20capabilities%2C%20we%0Ainnovatively%20propose%20the%20Finova%20evaluation%20benchmark%2C%20which%20focuses%20on%0Aagent-level%20financial%20reasoning%20and%20compliance%20verification.%20Experimental%0Aresults%20demonstrate%20that%20Agentar-Fin-R1%20not%20only%20achieves%20state-of-the-art%0Aperformance%20on%20financial%20tasks%20but%20also%20exhibits%20exceptional%20general%20reasoning%0Acapabilities%2C%20validating%20its%20effectiveness%20as%20a%20trustworthy%20solution%20for%0Ahigh-stakes%20financial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentar-Fin-R1%253A%2520Enhancing%2520Financial%2520Intelligence%2520through%2520Domain%250A%2520%2520Expertise%252C%2520Training%2520Efficiency%252C%2520and%2520Advanced%2520Reasoning%26entry.906535625%3DYanjun%2520Zheng%2520and%2520Xiyang%2520Du%2520and%2520Longfei%2520Liao%2520and%2520Xiaoke%2520Zhao%2520and%2520Zhaowen%2520Zhou%2520and%2520Bo%2520Zhang%2520and%2520Jiawei%2520Liu%2520and%2520Xiang%2520Qi%2520and%2520Zhe%2520Li%2520and%2520Zhiqiang%2520Zhang%2520and%2520Wang%2520Wei%2520and%2520Peng%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520tremendous%2520potential%2520in%2520the%250Afinancial%2520domain%252C%2520yet%2520existing%2520models%2520often%2520fall%2520short%2520in%2520scenarios%2520demanding%250Arobust%2520reasoning%2520capabilities%252C%2520stringent%2520trustworthiness%2520requirements%252C%2520and%250Aefficient%2520adaptation%2520to%2520task-specific%2520needs.%2520We%2520introduce%2520the%2520Agentar-Fin-R1%250Aseries%2520of%2520financial%2520large%2520language%2520models%2520%25288B%2520and%252032B%2520parameters%2529%252C%2520specifically%250Aengineered%2520based%2520on%2520the%2520Qwen3%2520foundation%2520model%2520to%2520enhance%2520reasoning%250Acapabilities%252C%2520reliability%252C%2520and%2520domain%2520specialization%2520for%2520financial%250Aapplications.%2520Our%2520optimization%2520approach%2520integrates%2520a%2520high-quality%252C%2520systematic%250Afinancial%2520task%2520taxonomy%2520with%2520a%2520comprehensive%2520multi-layered%2520trustworthiness%250Aassurance%2520framework.%2520This%2520framework%2520encompasses%2520high-quality%2520trustworthy%250Aknowledge%2520engineering%252C%2520multi-agent%2520trustworthy%2520data%2520synthesis%252C%2520and%2520rigorous%250Adata%2520validation%2520governance.%2520Through%2520label-guided%2520automated%2520difficulty-aware%250Aoptimization%252C%2520tow-stage%2520learning%2520processes%252C%2520and%2520detailed%2520attribution%2520systems%252C%250Awe%2520achieve%2520substantial%2520improvements%2520in%2520training%2520efficiency.%2520Our%2520models%2520undergo%250Acomprehensive%2520evaluation%2520on%2520mainstream%2520financial%2520benchmarks%2520including%2520FinEva%252C%250AFinEval%252C%2520and%2520FinanceIQ%252C%2520as%2520well%2520as%2520general%2520reasoning%2520datasets%2520such%2520as%2520MATH-500%250Aand%2520GPQA.%2520To%2520thoroughly%2520assess%2520real-world%2520deployment%2520capabilities%252C%2520we%250Ainnovatively%2520propose%2520the%2520Finova%2520evaluation%2520benchmark%252C%2520which%2520focuses%2520on%250Aagent-level%2520financial%2520reasoning%2520and%2520compliance%2520verification.%2520Experimental%250Aresults%2520demonstrate%2520that%2520Agentar-Fin-R1%2520not%2520only%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520financial%2520tasks%2520but%2520also%2520exhibits%2520exceptional%2520general%2520reasoning%250Acapabilities%252C%2520validating%2520its%2520effectiveness%2520as%2520a%2520trustworthy%2520solution%2520for%250Ahigh-stakes%2520financial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentar-Fin-R1%3A%20Enhancing%20Financial%20Intelligence%20through%20Domain%0A%20%20Expertise%2C%20Training%20Efficiency%2C%20and%20Advanced%20Reasoning&entry.906535625=Yanjun%20Zheng%20and%20Xiyang%20Du%20and%20Longfei%20Liao%20and%20Xiaoke%20Zhao%20and%20Zhaowen%20Zhou%20and%20Bo%20Zhang%20and%20Jiawei%20Liu%20and%20Xiang%20Qi%20and%20Zhe%20Li%20and%20Zhiqiang%20Zhang%20and%20Wang%20Wei%20and%20Peng%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20tremendous%20potential%20in%20the%0Afinancial%20domain%2C%20yet%20existing%20models%20often%20fall%20short%20in%20scenarios%20demanding%0Arobust%20reasoning%20capabilities%2C%20stringent%20trustworthiness%20requirements%2C%20and%0Aefficient%20adaptation%20to%20task-specific%20needs.%20We%20introduce%20the%20Agentar-Fin-R1%0Aseries%20of%20financial%20large%20language%20models%20%288B%20and%2032B%20parameters%29%2C%20specifically%0Aengineered%20based%20on%20the%20Qwen3%20foundation%20model%20to%20enhance%20reasoning%0Acapabilities%2C%20reliability%2C%20and%20domain%20specialization%20for%20financial%0Aapplications.%20Our%20optimization%20approach%20integrates%20a%20high-quality%2C%20systematic%0Afinancial%20task%20taxonomy%20with%20a%20comprehensive%20multi-layered%20trustworthiness%0Aassurance%20framework.%20This%20framework%20encompasses%20high-quality%20trustworthy%0Aknowledge%20engineering%2C%20multi-agent%20trustworthy%20data%20synthesis%2C%20and%20rigorous%0Adata%20validation%20governance.%20Through%20label-guided%20automated%20difficulty-aware%0Aoptimization%2C%20tow-stage%20learning%20processes%2C%20and%20detailed%20attribution%20systems%2C%0Awe%20achieve%20substantial%20improvements%20in%20training%20efficiency.%20Our%20models%20undergo%0Acomprehensive%20evaluation%20on%20mainstream%20financial%20benchmarks%20including%20FinEva%2C%0AFinEval%2C%20and%20FinanceIQ%2C%20as%20well%20as%20general%20reasoning%20datasets%20such%20as%20MATH-500%0Aand%20GPQA.%20To%20thoroughly%20assess%20real-world%20deployment%20capabilities%2C%20we%0Ainnovatively%20propose%20the%20Finova%20evaluation%20benchmark%2C%20which%20focuses%20on%0Aagent-level%20financial%20reasoning%20and%20compliance%20verification.%20Experimental%0Aresults%20demonstrate%20that%20Agentar-Fin-R1%20not%20only%20achieves%20state-of-the-art%0Aperformance%20on%20financial%20tasks%20but%20also%20exhibits%20exceptional%20general%20reasoning%0Acapabilities%2C%20validating%20its%20effectiveness%20as%20a%20trustworthy%20solution%20for%0Ahigh-stakes%20financial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16802v1&entry.124074799=Read"},
{"title": "PromptAL: Sample-Aware Dynamic Soft Prompts for Few-Shot Active Learning", "author": "Hui Xiang and Jinqiao Shi and Ting Zhang and Xiaojie Zhao and Yong Liu and Yong Ma", "abstract": "  Active learning (AL) aims to optimize model training and reduce annotation\ncosts by selecting the most informative samples for labeling. Typically, AL\nmethods rely on the empirical distribution of labeled data to define the\ndecision boundary and perform uncertainty or diversity estimation, subsequently\nidentifying potential high-quality samples. In few-shot scenarios, the\nempirical distribution often diverges significantly from the target\ndistribution, causing the decision boundary to shift away from its optimal\nposition. However, existing methods overlook the role of unlabeled samples in\nenhancing the empirical distribution to better align with the target\ndistribution, resulting in a suboptimal decision boundary and the selection of\nsamples that inadequately represent the target distribution. To address this,\nwe propose a hybrid AL framework, termed \\textbf{PromptAL} (Sample-Aware\nDynamic Soft \\textbf{Prompts} for Few-Shot \\textbf{A}ctive \\textbf{L}earning).\nThis framework accounts for the contribution of each unlabeled data point in\naligning the current empirical distribution with the target distribution,\nthereby optimizing the decision boundary. Specifically, PromptAL first\nleverages unlabeled data to construct sample-aware dynamic soft prompts that\nadjust the model's predictive distribution and decision boundary. Subsequently,\nbased on the adjusted decision boundary, it integrates uncertainty estimation\nwith both global and local diversity to select high-quality samples that more\naccurately represent the target distribution. Experimental results on six\nin-domain and three out-of-domain datasets show that PromptAL achieves superior\nperformance over nine baselines. Our codebase is openly accessible.\n", "link": "http://arxiv.org/abs/2507.16424v1", "date": "2025-07-22", "relevancy": 2.0819, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.53}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5198}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PromptAL%3A%20Sample-Aware%20Dynamic%20Soft%20Prompts%20for%20Few-Shot%20Active%20Learning&body=Title%3A%20PromptAL%3A%20Sample-Aware%20Dynamic%20Soft%20Prompts%20for%20Few-Shot%20Active%20Learning%0AAuthor%3A%20Hui%20Xiang%20and%20Jinqiao%20Shi%20and%20Ting%20Zhang%20and%20Xiaojie%20Zhao%20and%20Yong%20Liu%20and%20Yong%20Ma%0AAbstract%3A%20%20%20Active%20learning%20%28AL%29%20aims%20to%20optimize%20model%20training%20and%20reduce%20annotation%0Acosts%20by%20selecting%20the%20most%20informative%20samples%20for%20labeling.%20Typically%2C%20AL%0Amethods%20rely%20on%20the%20empirical%20distribution%20of%20labeled%20data%20to%20define%20the%0Adecision%20boundary%20and%20perform%20uncertainty%20or%20diversity%20estimation%2C%20subsequently%0Aidentifying%20potential%20high-quality%20samples.%20In%20few-shot%20scenarios%2C%20the%0Aempirical%20distribution%20often%20diverges%20significantly%20from%20the%20target%0Adistribution%2C%20causing%20the%20decision%20boundary%20to%20shift%20away%20from%20its%20optimal%0Aposition.%20However%2C%20existing%20methods%20overlook%20the%20role%20of%20unlabeled%20samples%20in%0Aenhancing%20the%20empirical%20distribution%20to%20better%20align%20with%20the%20target%0Adistribution%2C%20resulting%20in%20a%20suboptimal%20decision%20boundary%20and%20the%20selection%20of%0Asamples%20that%20inadequately%20represent%20the%20target%20distribution.%20To%20address%20this%2C%0Awe%20propose%20a%20hybrid%20AL%20framework%2C%20termed%20%5Ctextbf%7BPromptAL%7D%20%28Sample-Aware%0ADynamic%20Soft%20%5Ctextbf%7BPrompts%7D%20for%20Few-Shot%20%5Ctextbf%7BA%7Dctive%20%5Ctextbf%7BL%7Dearning%29.%0AThis%20framework%20accounts%20for%20the%20contribution%20of%20each%20unlabeled%20data%20point%20in%0Aaligning%20the%20current%20empirical%20distribution%20with%20the%20target%20distribution%2C%0Athereby%20optimizing%20the%20decision%20boundary.%20Specifically%2C%20PromptAL%20first%0Aleverages%20unlabeled%20data%20to%20construct%20sample-aware%20dynamic%20soft%20prompts%20that%0Aadjust%20the%20model%27s%20predictive%20distribution%20and%20decision%20boundary.%20Subsequently%2C%0Abased%20on%20the%20adjusted%20decision%20boundary%2C%20it%20integrates%20uncertainty%20estimation%0Awith%20both%20global%20and%20local%20diversity%20to%20select%20high-quality%20samples%20that%20more%0Aaccurately%20represent%20the%20target%20distribution.%20Experimental%20results%20on%20six%0Ain-domain%20and%20three%20out-of-domain%20datasets%20show%20that%20PromptAL%20achieves%20superior%0Aperformance%20over%20nine%20baselines.%20Our%20codebase%20is%20openly%20accessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPromptAL%253A%2520Sample-Aware%2520Dynamic%2520Soft%2520Prompts%2520for%2520Few-Shot%2520Active%2520Learning%26entry.906535625%3DHui%2520Xiang%2520and%2520Jinqiao%2520Shi%2520and%2520Ting%2520Zhang%2520and%2520Xiaojie%2520Zhao%2520and%2520Yong%2520Liu%2520and%2520Yong%2520Ma%26entry.1292438233%3D%2520%2520Active%2520learning%2520%2528AL%2529%2520aims%2520to%2520optimize%2520model%2520training%2520and%2520reduce%2520annotation%250Acosts%2520by%2520selecting%2520the%2520most%2520informative%2520samples%2520for%2520labeling.%2520Typically%252C%2520AL%250Amethods%2520rely%2520on%2520the%2520empirical%2520distribution%2520of%2520labeled%2520data%2520to%2520define%2520the%250Adecision%2520boundary%2520and%2520perform%2520uncertainty%2520or%2520diversity%2520estimation%252C%2520subsequently%250Aidentifying%2520potential%2520high-quality%2520samples.%2520In%2520few-shot%2520scenarios%252C%2520the%250Aempirical%2520distribution%2520often%2520diverges%2520significantly%2520from%2520the%2520target%250Adistribution%252C%2520causing%2520the%2520decision%2520boundary%2520to%2520shift%2520away%2520from%2520its%2520optimal%250Aposition.%2520However%252C%2520existing%2520methods%2520overlook%2520the%2520role%2520of%2520unlabeled%2520samples%2520in%250Aenhancing%2520the%2520empirical%2520distribution%2520to%2520better%2520align%2520with%2520the%2520target%250Adistribution%252C%2520resulting%2520in%2520a%2520suboptimal%2520decision%2520boundary%2520and%2520the%2520selection%2520of%250Asamples%2520that%2520inadequately%2520represent%2520the%2520target%2520distribution.%2520To%2520address%2520this%252C%250Awe%2520propose%2520a%2520hybrid%2520AL%2520framework%252C%2520termed%2520%255Ctextbf%257BPromptAL%257D%2520%2528Sample-Aware%250ADynamic%2520Soft%2520%255Ctextbf%257BPrompts%257D%2520for%2520Few-Shot%2520%255Ctextbf%257BA%257Dctive%2520%255Ctextbf%257BL%257Dearning%2529.%250AThis%2520framework%2520accounts%2520for%2520the%2520contribution%2520of%2520each%2520unlabeled%2520data%2520point%2520in%250Aaligning%2520the%2520current%2520empirical%2520distribution%2520with%2520the%2520target%2520distribution%252C%250Athereby%2520optimizing%2520the%2520decision%2520boundary.%2520Specifically%252C%2520PromptAL%2520first%250Aleverages%2520unlabeled%2520data%2520to%2520construct%2520sample-aware%2520dynamic%2520soft%2520prompts%2520that%250Aadjust%2520the%2520model%2527s%2520predictive%2520distribution%2520and%2520decision%2520boundary.%2520Subsequently%252C%250Abased%2520on%2520the%2520adjusted%2520decision%2520boundary%252C%2520it%2520integrates%2520uncertainty%2520estimation%250Awith%2520both%2520global%2520and%2520local%2520diversity%2520to%2520select%2520high-quality%2520samples%2520that%2520more%250Aaccurately%2520represent%2520the%2520target%2520distribution.%2520Experimental%2520results%2520on%2520six%250Ain-domain%2520and%2520three%2520out-of-domain%2520datasets%2520show%2520that%2520PromptAL%2520achieves%2520superior%250Aperformance%2520over%2520nine%2520baselines.%2520Our%2520codebase%2520is%2520openly%2520accessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PromptAL%3A%20Sample-Aware%20Dynamic%20Soft%20Prompts%20for%20Few-Shot%20Active%20Learning&entry.906535625=Hui%20Xiang%20and%20Jinqiao%20Shi%20and%20Ting%20Zhang%20and%20Xiaojie%20Zhao%20and%20Yong%20Liu%20and%20Yong%20Ma&entry.1292438233=%20%20Active%20learning%20%28AL%29%20aims%20to%20optimize%20model%20training%20and%20reduce%20annotation%0Acosts%20by%20selecting%20the%20most%20informative%20samples%20for%20labeling.%20Typically%2C%20AL%0Amethods%20rely%20on%20the%20empirical%20distribution%20of%20labeled%20data%20to%20define%20the%0Adecision%20boundary%20and%20perform%20uncertainty%20or%20diversity%20estimation%2C%20subsequently%0Aidentifying%20potential%20high-quality%20samples.%20In%20few-shot%20scenarios%2C%20the%0Aempirical%20distribution%20often%20diverges%20significantly%20from%20the%20target%0Adistribution%2C%20causing%20the%20decision%20boundary%20to%20shift%20away%20from%20its%20optimal%0Aposition.%20However%2C%20existing%20methods%20overlook%20the%20role%20of%20unlabeled%20samples%20in%0Aenhancing%20the%20empirical%20distribution%20to%20better%20align%20with%20the%20target%0Adistribution%2C%20resulting%20in%20a%20suboptimal%20decision%20boundary%20and%20the%20selection%20of%0Asamples%20that%20inadequately%20represent%20the%20target%20distribution.%20To%20address%20this%2C%0Awe%20propose%20a%20hybrid%20AL%20framework%2C%20termed%20%5Ctextbf%7BPromptAL%7D%20%28Sample-Aware%0ADynamic%20Soft%20%5Ctextbf%7BPrompts%7D%20for%20Few-Shot%20%5Ctextbf%7BA%7Dctive%20%5Ctextbf%7BL%7Dearning%29.%0AThis%20framework%20accounts%20for%20the%20contribution%20of%20each%20unlabeled%20data%20point%20in%0Aaligning%20the%20current%20empirical%20distribution%20with%20the%20target%20distribution%2C%0Athereby%20optimizing%20the%20decision%20boundary.%20Specifically%2C%20PromptAL%20first%0Aleverages%20unlabeled%20data%20to%20construct%20sample-aware%20dynamic%20soft%20prompts%20that%0Aadjust%20the%20model%27s%20predictive%20distribution%20and%20decision%20boundary.%20Subsequently%2C%0Abased%20on%20the%20adjusted%20decision%20boundary%2C%20it%20integrates%20uncertainty%20estimation%0Awith%20both%20global%20and%20local%20diversity%20to%20select%20high-quality%20samples%20that%20more%0Aaccurately%20represent%20the%20target%20distribution.%20Experimental%20results%20on%20six%0Ain-domain%20and%20three%20out-of-domain%20datasets%20show%20that%20PromptAL%20achieves%20superior%0Aperformance%20over%20nine%20baselines.%20Our%20codebase%20is%20openly%20accessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16424v1&entry.124074799=Read"},
{"title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation", "author": "Ming Yin and Fu Wang and Xujiong Ye and Yanda Meng and Zeyu Fu", "abstract": "  Surgical video segmentation is a critical task in computer-assisted surgery,\nessential for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has demonstrated remarkable\nadvancements in both image and video segmentation. However, the inherent\nlimitations of SAM2's greedy selection memory design are amplified by the\nunique properties of surgical videos-rapid instrument movement, frequent\nocclusion, and complex instrument-tissue interaction-resulting in diminished\nperformance in the segmentation of complex, long videos. To address these\nchallenges, we introduce Memory Augmented (MA)-SAM2, a training-free video\nobject segmentation strategy, featuring novel context-aware and\nocclusion-resilient memory models. MA-SAM2 exhibits strong robustness against\nocclusions and interactions arising from complex instrument movements while\nmaintaining accuracy in segmenting objects throughout videos. Employing a\nmulti-target, single-loop, one-prompt inference further enhances the efficiency\nof the tracking process in multi-instrument videos. Without introducing any\nadditional parameters or requiring further training, MA-SAM2 achieved\nperformance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and\nEndoVis2018 datasets, respectively, demonstrating its potential for practical\nsurgical applications.\n", "link": "http://arxiv.org/abs/2507.09577v2", "date": "2025-07-22", "relevancy": 2.0756, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5175}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory-Augmented%20SAM2%20for%20Training-Free%20Surgical%20Video%20Segmentation&body=Title%3A%20Memory-Augmented%20SAM2%20for%20Training-Free%20Surgical%20Video%20Segmentation%0AAuthor%3A%20Ming%20Yin%20and%20Fu%20Wang%20and%20Xujiong%20Ye%20and%20Yanda%20Meng%20and%20Zeyu%20Fu%0AAbstract%3A%20%20%20Surgical%20video%20segmentation%20is%20a%20critical%20task%20in%20computer-assisted%20surgery%2C%0Aessential%20for%20enhancing%20surgical%20quality%20and%20patient%20outcomes.%20Recently%2C%20the%0ASegment%20Anything%20Model%202%20%28SAM2%29%20framework%20has%20demonstrated%20remarkable%0Aadvancements%20in%20both%20image%20and%20video%20segmentation.%20However%2C%20the%20inherent%0Alimitations%20of%20SAM2%27s%20greedy%20selection%20memory%20design%20are%20amplified%20by%20the%0Aunique%20properties%20of%20surgical%20videos-rapid%20instrument%20movement%2C%20frequent%0Aocclusion%2C%20and%20complex%20instrument-tissue%20interaction-resulting%20in%20diminished%0Aperformance%20in%20the%20segmentation%20of%20complex%2C%20long%20videos.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Memory%20Augmented%20%28MA%29-SAM2%2C%20a%20training-free%20video%0Aobject%20segmentation%20strategy%2C%20featuring%20novel%20context-aware%20and%0Aocclusion-resilient%20memory%20models.%20MA-SAM2%20exhibits%20strong%20robustness%20against%0Aocclusions%20and%20interactions%20arising%20from%20complex%20instrument%20movements%20while%0Amaintaining%20accuracy%20in%20segmenting%20objects%20throughout%20videos.%20Employing%20a%0Amulti-target%2C%20single-loop%2C%20one-prompt%20inference%20further%20enhances%20the%20efficiency%0Aof%20the%20tracking%20process%20in%20multi-instrument%20videos.%20Without%20introducing%20any%0Aadditional%20parameters%20or%20requiring%20further%20training%2C%20MA-SAM2%20achieved%0Aperformance%20improvements%20of%204.36%25%20and%206.1%25%20over%20SAM2%20on%20the%20EndoVis2017%20and%0AEndoVis2018%20datasets%2C%20respectively%2C%20demonstrating%20its%20potential%20for%20practical%0Asurgical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory-Augmented%2520SAM2%2520for%2520Training-Free%2520Surgical%2520Video%2520Segmentation%26entry.906535625%3DMing%2520Yin%2520and%2520Fu%2520Wang%2520and%2520Xujiong%2520Ye%2520and%2520Yanda%2520Meng%2520and%2520Zeyu%2520Fu%26entry.1292438233%3D%2520%2520Surgical%2520video%2520segmentation%2520is%2520a%2520critical%2520task%2520in%2520computer-assisted%2520surgery%252C%250Aessential%2520for%2520enhancing%2520surgical%2520quality%2520and%2520patient%2520outcomes.%2520Recently%252C%2520the%250ASegment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520framework%2520has%2520demonstrated%2520remarkable%250Aadvancements%2520in%2520both%2520image%2520and%2520video%2520segmentation.%2520However%252C%2520the%2520inherent%250Alimitations%2520of%2520SAM2%2527s%2520greedy%2520selection%2520memory%2520design%2520are%2520amplified%2520by%2520the%250Aunique%2520properties%2520of%2520surgical%2520videos-rapid%2520instrument%2520movement%252C%2520frequent%250Aocclusion%252C%2520and%2520complex%2520instrument-tissue%2520interaction-resulting%2520in%2520diminished%250Aperformance%2520in%2520the%2520segmentation%2520of%2520complex%252C%2520long%2520videos.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520Memory%2520Augmented%2520%2528MA%2529-SAM2%252C%2520a%2520training-free%2520video%250Aobject%2520segmentation%2520strategy%252C%2520featuring%2520novel%2520context-aware%2520and%250Aocclusion-resilient%2520memory%2520models.%2520MA-SAM2%2520exhibits%2520strong%2520robustness%2520against%250Aocclusions%2520and%2520interactions%2520arising%2520from%2520complex%2520instrument%2520movements%2520while%250Amaintaining%2520accuracy%2520in%2520segmenting%2520objects%2520throughout%2520videos.%2520Employing%2520a%250Amulti-target%252C%2520single-loop%252C%2520one-prompt%2520inference%2520further%2520enhances%2520the%2520efficiency%250Aof%2520the%2520tracking%2520process%2520in%2520multi-instrument%2520videos.%2520Without%2520introducing%2520any%250Aadditional%2520parameters%2520or%2520requiring%2520further%2520training%252C%2520MA-SAM2%2520achieved%250Aperformance%2520improvements%2520of%25204.36%2525%2520and%25206.1%2525%2520over%2520SAM2%2520on%2520the%2520EndoVis2017%2520and%250AEndoVis2018%2520datasets%252C%2520respectively%252C%2520demonstrating%2520its%2520potential%2520for%2520practical%250Asurgical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory-Augmented%20SAM2%20for%20Training-Free%20Surgical%20Video%20Segmentation&entry.906535625=Ming%20Yin%20and%20Fu%20Wang%20and%20Xujiong%20Ye%20and%20Yanda%20Meng%20and%20Zeyu%20Fu&entry.1292438233=%20%20Surgical%20video%20segmentation%20is%20a%20critical%20task%20in%20computer-assisted%20surgery%2C%0Aessential%20for%20enhancing%20surgical%20quality%20and%20patient%20outcomes.%20Recently%2C%20the%0ASegment%20Anything%20Model%202%20%28SAM2%29%20framework%20has%20demonstrated%20remarkable%0Aadvancements%20in%20both%20image%20and%20video%20segmentation.%20However%2C%20the%20inherent%0Alimitations%20of%20SAM2%27s%20greedy%20selection%20memory%20design%20are%20amplified%20by%20the%0Aunique%20properties%20of%20surgical%20videos-rapid%20instrument%20movement%2C%20frequent%0Aocclusion%2C%20and%20complex%20instrument-tissue%20interaction-resulting%20in%20diminished%0Aperformance%20in%20the%20segmentation%20of%20complex%2C%20long%20videos.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20Memory%20Augmented%20%28MA%29-SAM2%2C%20a%20training-free%20video%0Aobject%20segmentation%20strategy%2C%20featuring%20novel%20context-aware%20and%0Aocclusion-resilient%20memory%20models.%20MA-SAM2%20exhibits%20strong%20robustness%20against%0Aocclusions%20and%20interactions%20arising%20from%20complex%20instrument%20movements%20while%0Amaintaining%20accuracy%20in%20segmenting%20objects%20throughout%20videos.%20Employing%20a%0Amulti-target%2C%20single-loop%2C%20one-prompt%20inference%20further%20enhances%20the%20efficiency%0Aof%20the%20tracking%20process%20in%20multi-instrument%20videos.%20Without%20introducing%20any%0Aadditional%20parameters%20or%20requiring%20further%20training%2C%20MA-SAM2%20achieved%0Aperformance%20improvements%20of%204.36%25%20and%206.1%25%20over%20SAM2%20on%20the%20EndoVis2017%20and%0AEndoVis2018%20datasets%2C%20respectively%2C%20demonstrating%20its%20potential%20for%20practical%0Asurgical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09577v2&entry.124074799=Read"},
{"title": "Learning Temporal Abstractions via Variational Homomorphisms in\n  Option-Induced Abstract MDPs", "author": "Chang Li and Yaren Zhang and Haoran Lv and Qiong Cao and Chao Xue and Xiaodong He", "abstract": "  Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol.\n", "link": "http://arxiv.org/abs/2507.16473v1", "date": "2025-07-22", "relevancy": 2.0752, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5736}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Temporal%20Abstractions%20via%20Variational%20Homomorphisms%20in%0A%20%20Option-Induced%20Abstract%20MDPs&body=Title%3A%20Learning%20Temporal%20Abstractions%20via%20Variational%20Homomorphisms%20in%0A%20%20Option-Induced%20Abstract%20MDPs%0AAuthor%3A%20Chang%20Li%20and%20Yaren%20Zhang%20and%20Haoran%20Lv%20and%20Qiong%20Cao%20and%20Chao%20Xue%20and%20Xiaodong%20He%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20reasoning%20ability%20through%0Aexplicit%20Chain-of-Thought%20%28CoT%29%20prompting%2C%20but%20generating%20these%20step-by-step%0Atextual%20explanations%20is%20computationally%20expensive%20and%20slow.%20To%20overcome%20this%2C%0Awe%20aim%20to%20develop%20a%20framework%20for%20efficient%2C%20implicit%20reasoning%2C%20where%20the%0Amodel%20%22thinks%22%20in%20a%20latent%20space%20without%20generating%20explicit%20text%20for%20every%0Astep.%20We%20propose%20that%20these%20latent%20thoughts%20can%20be%20modeled%20as%0Atemporally-extended%20abstract%20actions%2C%20or%20options%2C%20within%20a%20hierarchical%0Areinforcement%20learning%20framework.%20To%20effectively%20learn%20a%20diverse%20library%20of%0Aoptions%20as%20latent%20embeddings%2C%20we%20first%20introduce%20the%20Variational%20Markovian%0AOption%20Critic%20%28VMOC%29%2C%20an%20off-policy%20algorithm%20that%20uses%20variational%20inference%0Awithin%20the%20HiT-MDP%20framework.%20To%20provide%20a%20rigorous%20foundation%20for%20using%20these%0Aoptions%20as%20an%20abstract%20reasoning%20space%2C%20we%20extend%20the%20theory%20of%20continuous%20MDP%0Ahomomorphisms.%20This%20proves%20that%20learning%20a%20policy%20in%20the%20simplified%2C%20abstract%0Alatent%20space%2C%20for%20which%20VMOC%20is%20suited%2C%20preserves%20the%20optimality%20of%20the%0Asolution%20to%20the%20original%2C%20complex%20problem.%20Finally%2C%20we%20propose%20a%20cold-start%0Aprocedure%20that%20leverages%20supervised%20fine-tuning%20%28SFT%29%20data%20to%20distill%20human%0Areasoning%20demonstrations%20into%20this%20latent%20option%20space%2C%20providing%20a%20rich%0Ainitialization%20for%20the%20model%27s%20reasoning%20capabilities.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20achieves%20strong%20performance%20on%20complex%20logical%0Areasoning%20benchmarks%20and%20challenging%20locomotion%20tasks%2C%20validating%20our%20framework%0Aas%20a%20principled%20method%20for%20learning%20abstract%20skills%20for%20both%20language%20and%0Acontrol.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Temporal%2520Abstractions%2520via%2520Variational%2520Homomorphisms%2520in%250A%2520%2520Option-Induced%2520Abstract%2520MDPs%26entry.906535625%3DChang%2520Li%2520and%2520Yaren%2520Zhang%2520and%2520Haoran%2520Lv%2520and%2520Qiong%2520Cao%2520and%2520Chao%2520Xue%2520and%2520Xiaodong%2520He%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520reasoning%2520ability%2520through%250Aexplicit%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%252C%2520but%2520generating%2520these%2520step-by-step%250Atextual%2520explanations%2520is%2520computationally%2520expensive%2520and%2520slow.%2520To%2520overcome%2520this%252C%250Awe%2520aim%2520to%2520develop%2520a%2520framework%2520for%2520efficient%252C%2520implicit%2520reasoning%252C%2520where%2520the%250Amodel%2520%2522thinks%2522%2520in%2520a%2520latent%2520space%2520without%2520generating%2520explicit%2520text%2520for%2520every%250Astep.%2520We%2520propose%2520that%2520these%2520latent%2520thoughts%2520can%2520be%2520modeled%2520as%250Atemporally-extended%2520abstract%2520actions%252C%2520or%2520options%252C%2520within%2520a%2520hierarchical%250Areinforcement%2520learning%2520framework.%2520To%2520effectively%2520learn%2520a%2520diverse%2520library%2520of%250Aoptions%2520as%2520latent%2520embeddings%252C%2520we%2520first%2520introduce%2520the%2520Variational%2520Markovian%250AOption%2520Critic%2520%2528VMOC%2529%252C%2520an%2520off-policy%2520algorithm%2520that%2520uses%2520variational%2520inference%250Awithin%2520the%2520HiT-MDP%2520framework.%2520To%2520provide%2520a%2520rigorous%2520foundation%2520for%2520using%2520these%250Aoptions%2520as%2520an%2520abstract%2520reasoning%2520space%252C%2520we%2520extend%2520the%2520theory%2520of%2520continuous%2520MDP%250Ahomomorphisms.%2520This%2520proves%2520that%2520learning%2520a%2520policy%2520in%2520the%2520simplified%252C%2520abstract%250Alatent%2520space%252C%2520for%2520which%2520VMOC%2520is%2520suited%252C%2520preserves%2520the%2520optimality%2520of%2520the%250Asolution%2520to%2520the%2520original%252C%2520complex%2520problem.%2520Finally%252C%2520we%2520propose%2520a%2520cold-start%250Aprocedure%2520that%2520leverages%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520data%2520to%2520distill%2520human%250Areasoning%2520demonstrations%2520into%2520this%2520latent%2520option%2520space%252C%2520providing%2520a%2520rich%250Ainitialization%2520for%2520the%2520model%2527s%2520reasoning%2520capabilities.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520strong%2520performance%2520on%2520complex%2520logical%250Areasoning%2520benchmarks%2520and%2520challenging%2520locomotion%2520tasks%252C%2520validating%2520our%2520framework%250Aas%2520a%2520principled%2520method%2520for%2520learning%2520abstract%2520skills%2520for%2520both%2520language%2520and%250Acontrol.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Temporal%20Abstractions%20via%20Variational%20Homomorphisms%20in%0A%20%20Option-Induced%20Abstract%20MDPs&entry.906535625=Chang%20Li%20and%20Yaren%20Zhang%20and%20Haoran%20Lv%20and%20Qiong%20Cao%20and%20Chao%20Xue%20and%20Xiaodong%20He&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20reasoning%20ability%20through%0Aexplicit%20Chain-of-Thought%20%28CoT%29%20prompting%2C%20but%20generating%20these%20step-by-step%0Atextual%20explanations%20is%20computationally%20expensive%20and%20slow.%20To%20overcome%20this%2C%0Awe%20aim%20to%20develop%20a%20framework%20for%20efficient%2C%20implicit%20reasoning%2C%20where%20the%0Amodel%20%22thinks%22%20in%20a%20latent%20space%20without%20generating%20explicit%20text%20for%20every%0Astep.%20We%20propose%20that%20these%20latent%20thoughts%20can%20be%20modeled%20as%0Atemporally-extended%20abstract%20actions%2C%20or%20options%2C%20within%20a%20hierarchical%0Areinforcement%20learning%20framework.%20To%20effectively%20learn%20a%20diverse%20library%20of%0Aoptions%20as%20latent%20embeddings%2C%20we%20first%20introduce%20the%20Variational%20Markovian%0AOption%20Critic%20%28VMOC%29%2C%20an%20off-policy%20algorithm%20that%20uses%20variational%20inference%0Awithin%20the%20HiT-MDP%20framework.%20To%20provide%20a%20rigorous%20foundation%20for%20using%20these%0Aoptions%20as%20an%20abstract%20reasoning%20space%2C%20we%20extend%20the%20theory%20of%20continuous%20MDP%0Ahomomorphisms.%20This%20proves%20that%20learning%20a%20policy%20in%20the%20simplified%2C%20abstract%0Alatent%20space%2C%20for%20which%20VMOC%20is%20suited%2C%20preserves%20the%20optimality%20of%20the%0Asolution%20to%20the%20original%2C%20complex%20problem.%20Finally%2C%20we%20propose%20a%20cold-start%0Aprocedure%20that%20leverages%20supervised%20fine-tuning%20%28SFT%29%20data%20to%20distill%20human%0Areasoning%20demonstrations%20into%20this%20latent%20option%20space%2C%20providing%20a%20rich%0Ainitialization%20for%20the%20model%27s%20reasoning%20capabilities.%20Extensive%20experiments%0Ademonstrate%20that%20our%20approach%20achieves%20strong%20performance%20on%20complex%20logical%0Areasoning%20benchmarks%20and%20challenging%20locomotion%20tasks%2C%20validating%20our%20framework%0Aas%20a%20principled%20method%20for%20learning%20abstract%20skills%20for%20both%20language%20and%0Acontrol.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16473v1&entry.124074799=Read"},
{"title": "CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot\n  Segmentation", "author": "Shuai Chen and Fanman Meng and Chunjin Yang and Haoran Wei and Chenhao Wu and Qingbo Wu and Hongliang Li", "abstract": "  Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to\nlimited data and domain shifts. Recent foundation models like the Segment\nAnything Model (SAM) have shown remarkable zero-shot generalization capability\nin general segmentation tasks, making it a promising solution for few-shot\nscenarios. However, adapting SAM to CD-FSS faces two critical challenges:\nreliance on manual prompt and limited cross-domain ability. Therefore, we\npropose the Composable Meta-Prompt (CMP) framework that introduces three key\nmodules: (i) the Reference Complement and Transformation (RCT) module for\nsemantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module\nfor automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction\n(FAI) module for domain discrepancy mitigation. Evaluations across four\ncross-domain datasets demonstrate CMP's state-of-the-art performance, achieving\n71.8\\% and 74.5\\% mIoU in 1-shot and 5-shot scenarios respectively.\n", "link": "http://arxiv.org/abs/2507.16753v1", "date": "2025-07-22", "relevancy": 2.072, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5408}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5072}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMP%3A%20A%20Composable%20Meta%20Prompt%20for%20SAM-Based%20Cross-Domain%20Few-Shot%0A%20%20Segmentation&body=Title%3A%20CMP%3A%20A%20Composable%20Meta%20Prompt%20for%20SAM-Based%20Cross-Domain%20Few-Shot%0A%20%20Segmentation%0AAuthor%3A%20Shuai%20Chen%20and%20Fanman%20Meng%20and%20Chunjin%20Yang%20and%20Haoran%20Wei%20and%20Chenhao%20Wu%20and%20Qingbo%20Wu%20and%20Hongliang%20Li%0AAbstract%3A%20%20%20Cross-Domain%20Few-Shot%20Segmentation%20%28CD-FSS%29%20remains%20challenging%20due%20to%0Alimited%20data%20and%20domain%20shifts.%20Recent%20foundation%20models%20like%20the%20Segment%0AAnything%20Model%20%28SAM%29%20have%20shown%20remarkable%20zero-shot%20generalization%20capability%0Ain%20general%20segmentation%20tasks%2C%20making%20it%20a%20promising%20solution%20for%20few-shot%0Ascenarios.%20However%2C%20adapting%20SAM%20to%20CD-FSS%20faces%20two%20critical%20challenges%3A%0Areliance%20on%20manual%20prompt%20and%20limited%20cross-domain%20ability.%20Therefore%2C%20we%0Apropose%20the%20Composable%20Meta-Prompt%20%28CMP%29%20framework%20that%20introduces%20three%20key%0Amodules%3A%20%28i%29%20the%20Reference%20Complement%20and%20Transformation%20%28RCT%29%20module%20for%0Asemantic%20expansion%2C%20%28ii%29%20the%20Composable%20Meta-Prompt%20Generation%20%28CMPG%29%20module%0Afor%20automated%20meta-prompt%20synthesis%2C%20and%20%28iii%29%20the%20Frequency-Aware%20Interaction%0A%28FAI%29%20module%20for%20domain%20discrepancy%20mitigation.%20Evaluations%20across%20four%0Across-domain%20datasets%20demonstrate%20CMP%27s%20state-of-the-art%20performance%2C%20achieving%0A71.8%5C%25%20and%2074.5%5C%25%20mIoU%20in%201-shot%20and%205-shot%20scenarios%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMP%253A%2520A%2520Composable%2520Meta%2520Prompt%2520for%2520SAM-Based%2520Cross-Domain%2520Few-Shot%250A%2520%2520Segmentation%26entry.906535625%3DShuai%2520Chen%2520and%2520Fanman%2520Meng%2520and%2520Chunjin%2520Yang%2520and%2520Haoran%2520Wei%2520and%2520Chenhao%2520Wu%2520and%2520Qingbo%2520Wu%2520and%2520Hongliang%2520Li%26entry.1292438233%3D%2520%2520Cross-Domain%2520Few-Shot%2520Segmentation%2520%2528CD-FSS%2529%2520remains%2520challenging%2520due%2520to%250Alimited%2520data%2520and%2520domain%2520shifts.%2520Recent%2520foundation%2520models%2520like%2520the%2520Segment%250AAnything%2520Model%2520%2528SAM%2529%2520have%2520shown%2520remarkable%2520zero-shot%2520generalization%2520capability%250Ain%2520general%2520segmentation%2520tasks%252C%2520making%2520it%2520a%2520promising%2520solution%2520for%2520few-shot%250Ascenarios.%2520However%252C%2520adapting%2520SAM%2520to%2520CD-FSS%2520faces%2520two%2520critical%2520challenges%253A%250Areliance%2520on%2520manual%2520prompt%2520and%2520limited%2520cross-domain%2520ability.%2520Therefore%252C%2520we%250Apropose%2520the%2520Composable%2520Meta-Prompt%2520%2528CMP%2529%2520framework%2520that%2520introduces%2520three%2520key%250Amodules%253A%2520%2528i%2529%2520the%2520Reference%2520Complement%2520and%2520Transformation%2520%2528RCT%2529%2520module%2520for%250Asemantic%2520expansion%252C%2520%2528ii%2529%2520the%2520Composable%2520Meta-Prompt%2520Generation%2520%2528CMPG%2529%2520module%250Afor%2520automated%2520meta-prompt%2520synthesis%252C%2520and%2520%2528iii%2529%2520the%2520Frequency-Aware%2520Interaction%250A%2528FAI%2529%2520module%2520for%2520domain%2520discrepancy%2520mitigation.%2520Evaluations%2520across%2520four%250Across-domain%2520datasets%2520demonstrate%2520CMP%2527s%2520state-of-the-art%2520performance%252C%2520achieving%250A71.8%255C%2525%2520and%252074.5%255C%2525%2520mIoU%2520in%25201-shot%2520and%25205-shot%2520scenarios%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMP%3A%20A%20Composable%20Meta%20Prompt%20for%20SAM-Based%20Cross-Domain%20Few-Shot%0A%20%20Segmentation&entry.906535625=Shuai%20Chen%20and%20Fanman%20Meng%20and%20Chunjin%20Yang%20and%20Haoran%20Wei%20and%20Chenhao%20Wu%20and%20Qingbo%20Wu%20and%20Hongliang%20Li&entry.1292438233=%20%20Cross-Domain%20Few-Shot%20Segmentation%20%28CD-FSS%29%20remains%20challenging%20due%20to%0Alimited%20data%20and%20domain%20shifts.%20Recent%20foundation%20models%20like%20the%20Segment%0AAnything%20Model%20%28SAM%29%20have%20shown%20remarkable%20zero-shot%20generalization%20capability%0Ain%20general%20segmentation%20tasks%2C%20making%20it%20a%20promising%20solution%20for%20few-shot%0Ascenarios.%20However%2C%20adapting%20SAM%20to%20CD-FSS%20faces%20two%20critical%20challenges%3A%0Areliance%20on%20manual%20prompt%20and%20limited%20cross-domain%20ability.%20Therefore%2C%20we%0Apropose%20the%20Composable%20Meta-Prompt%20%28CMP%29%20framework%20that%20introduces%20three%20key%0Amodules%3A%20%28i%29%20the%20Reference%20Complement%20and%20Transformation%20%28RCT%29%20module%20for%0Asemantic%20expansion%2C%20%28ii%29%20the%20Composable%20Meta-Prompt%20Generation%20%28CMPG%29%20module%0Afor%20automated%20meta-prompt%20synthesis%2C%20and%20%28iii%29%20the%20Frequency-Aware%20Interaction%0A%28FAI%29%20module%20for%20domain%20discrepancy%20mitigation.%20Evaluations%20across%20four%0Across-domain%20datasets%20demonstrate%20CMP%27s%20state-of-the-art%20performance%2C%20achieving%0A71.8%5C%25%20and%2074.5%5C%25%20mIoU%20in%201-shot%20and%205-shot%20scenarios%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16753v1&entry.124074799=Read"},
{"title": "V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using\n  the iRAP Standard?", "author": "Natchapon Jongwiriyanurak and Zichao Zeng and June Moh Goo and Xinglei Wang and Ilya Ilyankou and Kerkritt Sriroongvikrai and Nicola Christie and Meihui Wang and Huanfa Chen and James Haworth", "abstract": "  Road safety assessments are critical yet costly, especially in Low- and\nMiddle-Income Countries (LMICs), where most roads remain unrated. Traditional\nmethods require expert annotation and training data, while supervised\nlearning-based approaches struggle to generalise across regions. In this paper,\nwe introduce \\textit{V-RoAst}, a zero-shot Visual Question Answering (VQA)\nframework using Vision-Language Models (VLMs) to classify road safety\nattributes defined by the iRAP standard. We introduce the first open-source\ndataset from ThaiRAP, consisting of over 2,000 curated street-level images from\nThailand annotated for this task. We evaluate Gemini-1.5-flash and GPT-4o-mini\non this dataset and benchmark their performance against VGGNet and ResNet\nbaselines. While VLMs underperform on spatial awareness, they generalise well\nto unseen classes and offer flexible prompt-based reasoning without retraining.\nOur results show that VLMs can serve as automatic road assessment tools when\nintegrated with complementary data. This work is the first to explore VLMs for\nzero-shot infrastructure risk assessment and opens new directions for\nautomatic, low-cost road safety mapping. Code and dataset:\nhttps://github.com/PongNJ/V-RoAst.\n", "link": "http://arxiv.org/abs/2408.10872v4", "date": "2025-07-22", "relevancy": 2.0714, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.531}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-RoAst%3A%20Visual%20Road%20Assessment.%20Can%20VLM%20be%20a%20Road%20Safety%20Assessor%20Using%0A%20%20the%20iRAP%20Standard%3F&body=Title%3A%20V-RoAst%3A%20Visual%20Road%20Assessment.%20Can%20VLM%20be%20a%20Road%20Safety%20Assessor%20Using%0A%20%20the%20iRAP%20Standard%3F%0AAuthor%3A%20Natchapon%20Jongwiriyanurak%20and%20Zichao%20Zeng%20and%20June%20Moh%20Goo%20and%20Xinglei%20Wang%20and%20Ilya%20Ilyankou%20and%20Kerkritt%20Sriroongvikrai%20and%20Nicola%20Christie%20and%20Meihui%20Wang%20and%20Huanfa%20Chen%20and%20James%20Haworth%0AAbstract%3A%20%20%20Road%20safety%20assessments%20are%20critical%20yet%20costly%2C%20especially%20in%20Low-%20and%0AMiddle-Income%20Countries%20%28LMICs%29%2C%20where%20most%20roads%20remain%20unrated.%20Traditional%0Amethods%20require%20expert%20annotation%20and%20training%20data%2C%20while%20supervised%0Alearning-based%20approaches%20struggle%20to%20generalise%20across%20regions.%20In%20this%20paper%2C%0Awe%20introduce%20%5Ctextit%7BV-RoAst%7D%2C%20a%20zero-shot%20Visual%20Question%20Answering%20%28VQA%29%0Aframework%20using%20Vision-Language%20Models%20%28VLMs%29%20to%20classify%20road%20safety%0Aattributes%20defined%20by%20the%20iRAP%20standard.%20We%20introduce%20the%20first%20open-source%0Adataset%20from%20ThaiRAP%2C%20consisting%20of%20over%202%2C000%20curated%20street-level%20images%20from%0AThailand%20annotated%20for%20this%20task.%20We%20evaluate%20Gemini-1.5-flash%20and%20GPT-4o-mini%0Aon%20this%20dataset%20and%20benchmark%20their%20performance%20against%20VGGNet%20and%20ResNet%0Abaselines.%20While%20VLMs%20underperform%20on%20spatial%20awareness%2C%20they%20generalise%20well%0Ato%20unseen%20classes%20and%20offer%20flexible%20prompt-based%20reasoning%20without%20retraining.%0AOur%20results%20show%20that%20VLMs%20can%20serve%20as%20automatic%20road%20assessment%20tools%20when%0Aintegrated%20with%20complementary%20data.%20This%20work%20is%20the%20first%20to%20explore%20VLMs%20for%0Azero-shot%20infrastructure%20risk%20assessment%20and%20opens%20new%20directions%20for%0Aautomatic%2C%20low-cost%20road%20safety%20mapping.%20Code%20and%20dataset%3A%0Ahttps%3A//github.com/PongNJ/V-RoAst.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10872v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-RoAst%253A%2520Visual%2520Road%2520Assessment.%2520Can%2520VLM%2520be%2520a%2520Road%2520Safety%2520Assessor%2520Using%250A%2520%2520the%2520iRAP%2520Standard%253F%26entry.906535625%3DNatchapon%2520Jongwiriyanurak%2520and%2520Zichao%2520Zeng%2520and%2520June%2520Moh%2520Goo%2520and%2520Xinglei%2520Wang%2520and%2520Ilya%2520Ilyankou%2520and%2520Kerkritt%2520Sriroongvikrai%2520and%2520Nicola%2520Christie%2520and%2520Meihui%2520Wang%2520and%2520Huanfa%2520Chen%2520and%2520James%2520Haworth%26entry.1292438233%3D%2520%2520Road%2520safety%2520assessments%2520are%2520critical%2520yet%2520costly%252C%2520especially%2520in%2520Low-%2520and%250AMiddle-Income%2520Countries%2520%2528LMICs%2529%252C%2520where%2520most%2520roads%2520remain%2520unrated.%2520Traditional%250Amethods%2520require%2520expert%2520annotation%2520and%2520training%2520data%252C%2520while%2520supervised%250Alearning-based%2520approaches%2520struggle%2520to%2520generalise%2520across%2520regions.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520%255Ctextit%257BV-RoAst%257D%252C%2520a%2520zero-shot%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%250Aframework%2520using%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520classify%2520road%2520safety%250Aattributes%2520defined%2520by%2520the%2520iRAP%2520standard.%2520We%2520introduce%2520the%2520first%2520open-source%250Adataset%2520from%2520ThaiRAP%252C%2520consisting%2520of%2520over%25202%252C000%2520curated%2520street-level%2520images%2520from%250AThailand%2520annotated%2520for%2520this%2520task.%2520We%2520evaluate%2520Gemini-1.5-flash%2520and%2520GPT-4o-mini%250Aon%2520this%2520dataset%2520and%2520benchmark%2520their%2520performance%2520against%2520VGGNet%2520and%2520ResNet%250Abaselines.%2520While%2520VLMs%2520underperform%2520on%2520spatial%2520awareness%252C%2520they%2520generalise%2520well%250Ato%2520unseen%2520classes%2520and%2520offer%2520flexible%2520prompt-based%2520reasoning%2520without%2520retraining.%250AOur%2520results%2520show%2520that%2520VLMs%2520can%2520serve%2520as%2520automatic%2520road%2520assessment%2520tools%2520when%250Aintegrated%2520with%2520complementary%2520data.%2520This%2520work%2520is%2520the%2520first%2520to%2520explore%2520VLMs%2520for%250Azero-shot%2520infrastructure%2520risk%2520assessment%2520and%2520opens%2520new%2520directions%2520for%250Aautomatic%252C%2520low-cost%2520road%2520safety%2520mapping.%2520Code%2520and%2520dataset%253A%250Ahttps%253A//github.com/PongNJ/V-RoAst.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10872v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-RoAst%3A%20Visual%20Road%20Assessment.%20Can%20VLM%20be%20a%20Road%20Safety%20Assessor%20Using%0A%20%20the%20iRAP%20Standard%3F&entry.906535625=Natchapon%20Jongwiriyanurak%20and%20Zichao%20Zeng%20and%20June%20Moh%20Goo%20and%20Xinglei%20Wang%20and%20Ilya%20Ilyankou%20and%20Kerkritt%20Sriroongvikrai%20and%20Nicola%20Christie%20and%20Meihui%20Wang%20and%20Huanfa%20Chen%20and%20James%20Haworth&entry.1292438233=%20%20Road%20safety%20assessments%20are%20critical%20yet%20costly%2C%20especially%20in%20Low-%20and%0AMiddle-Income%20Countries%20%28LMICs%29%2C%20where%20most%20roads%20remain%20unrated.%20Traditional%0Amethods%20require%20expert%20annotation%20and%20training%20data%2C%20while%20supervised%0Alearning-based%20approaches%20struggle%20to%20generalise%20across%20regions.%20In%20this%20paper%2C%0Awe%20introduce%20%5Ctextit%7BV-RoAst%7D%2C%20a%20zero-shot%20Visual%20Question%20Answering%20%28VQA%29%0Aframework%20using%20Vision-Language%20Models%20%28VLMs%29%20to%20classify%20road%20safety%0Aattributes%20defined%20by%20the%20iRAP%20standard.%20We%20introduce%20the%20first%20open-source%0Adataset%20from%20ThaiRAP%2C%20consisting%20of%20over%202%2C000%20curated%20street-level%20images%20from%0AThailand%20annotated%20for%20this%20task.%20We%20evaluate%20Gemini-1.5-flash%20and%20GPT-4o-mini%0Aon%20this%20dataset%20and%20benchmark%20their%20performance%20against%20VGGNet%20and%20ResNet%0Abaselines.%20While%20VLMs%20underperform%20on%20spatial%20awareness%2C%20they%20generalise%20well%0Ato%20unseen%20classes%20and%20offer%20flexible%20prompt-based%20reasoning%20without%20retraining.%0AOur%20results%20show%20that%20VLMs%20can%20serve%20as%20automatic%20road%20assessment%20tools%20when%0Aintegrated%20with%20complementary%20data.%20This%20work%20is%20the%20first%20to%20explore%20VLMs%20for%0Azero-shot%20infrastructure%20risk%20assessment%20and%20opens%20new%20directions%20for%0Aautomatic%2C%20low-cost%20road%20safety%20mapping.%20Code%20and%20dataset%3A%0Ahttps%3A//github.com/PongNJ/V-RoAst.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10872v4&entry.124074799=Read"},
{"title": "Identifying Pre-training Data in LLMs: A Neuron Activation-Based\n  Detection Framework", "author": "Hongyi Tang and Zhihao Zhu and Yi Yang", "abstract": "  The performance of large language models (LLMs) is closely tied to their\ntraining data, which can include copyrighted material or private information,\nraising legal and ethical concerns. Additionally, LLMs face criticism for\ndataset contamination and internalizing biases. To address these issues, the\nPre-Training Data Detection (PDD) task was proposed to identify if specific\ndata was included in an LLM's pre-training corpus. However, existing PDD\nmethods often rely on superficial features like prediction confidence and loss,\nresulting in mediocre performance. To improve this, we introduce NA-PDD, a\nnovel algorithm analyzing differential neuron activation patterns between\ntraining and non-training data in LLMs. This is based on the observation that\nthese data types activate different neurons during LLM inference. We also\nintroduce CCNewsPDD, a temporally unbiased benchmark employing rigorous data\ntransformations to ensure consistent time distributions between training and\nnon-training data. Our experiments demonstrate that NA-PDD significantly\noutperforms existing methods across three benchmarks and multiple LLMs.\n", "link": "http://arxiv.org/abs/2507.16414v1", "date": "2025-07-22", "relevancy": 2.0677, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5322}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5173}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Pre-training%20Data%20in%20LLMs%3A%20A%20Neuron%20Activation-Based%0A%20%20Detection%20Framework&body=Title%3A%20Identifying%20Pre-training%20Data%20in%20LLMs%3A%20A%20Neuron%20Activation-Based%0A%20%20Detection%20Framework%0AAuthor%3A%20Hongyi%20Tang%20and%20Zhihao%20Zhu%20and%20Yi%20Yang%0AAbstract%3A%20%20%20The%20performance%20of%20large%20language%20models%20%28LLMs%29%20is%20closely%20tied%20to%20their%0Atraining%20data%2C%20which%20can%20include%20copyrighted%20material%20or%20private%20information%2C%0Araising%20legal%20and%20ethical%20concerns.%20Additionally%2C%20LLMs%20face%20criticism%20for%0Adataset%20contamination%20and%20internalizing%20biases.%20To%20address%20these%20issues%2C%20the%0APre-Training%20Data%20Detection%20%28PDD%29%20task%20was%20proposed%20to%20identify%20if%20specific%0Adata%20was%20included%20in%20an%20LLM%27s%20pre-training%20corpus.%20However%2C%20existing%20PDD%0Amethods%20often%20rely%20on%20superficial%20features%20like%20prediction%20confidence%20and%20loss%2C%0Aresulting%20in%20mediocre%20performance.%20To%20improve%20this%2C%20we%20introduce%20NA-PDD%2C%20a%0Anovel%20algorithm%20analyzing%20differential%20neuron%20activation%20patterns%20between%0Atraining%20and%20non-training%20data%20in%20LLMs.%20This%20is%20based%20on%20the%20observation%20that%0Athese%20data%20types%20activate%20different%20neurons%20during%20LLM%20inference.%20We%20also%0Aintroduce%20CCNewsPDD%2C%20a%20temporally%20unbiased%20benchmark%20employing%20rigorous%20data%0Atransformations%20to%20ensure%20consistent%20time%20distributions%20between%20training%20and%0Anon-training%20data.%20Our%20experiments%20demonstrate%20that%20NA-PDD%20significantly%0Aoutperforms%20existing%20methods%20across%20three%20benchmarks%20and%20multiple%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Pre-training%2520Data%2520in%2520LLMs%253A%2520A%2520Neuron%2520Activation-Based%250A%2520%2520Detection%2520Framework%26entry.906535625%3DHongyi%2520Tang%2520and%2520Zhihao%2520Zhu%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520closely%2520tied%2520to%2520their%250Atraining%2520data%252C%2520which%2520can%2520include%2520copyrighted%2520material%2520or%2520private%2520information%252C%250Araising%2520legal%2520and%2520ethical%2520concerns.%2520Additionally%252C%2520LLMs%2520face%2520criticism%2520for%250Adataset%2520contamination%2520and%2520internalizing%2520biases.%2520To%2520address%2520these%2520issues%252C%2520the%250APre-Training%2520Data%2520Detection%2520%2528PDD%2529%2520task%2520was%2520proposed%2520to%2520identify%2520if%2520specific%250Adata%2520was%2520included%2520in%2520an%2520LLM%2527s%2520pre-training%2520corpus.%2520However%252C%2520existing%2520PDD%250Amethods%2520often%2520rely%2520on%2520superficial%2520features%2520like%2520prediction%2520confidence%2520and%2520loss%252C%250Aresulting%2520in%2520mediocre%2520performance.%2520To%2520improve%2520this%252C%2520we%2520introduce%2520NA-PDD%252C%2520a%250Anovel%2520algorithm%2520analyzing%2520differential%2520neuron%2520activation%2520patterns%2520between%250Atraining%2520and%2520non-training%2520data%2520in%2520LLMs.%2520This%2520is%2520based%2520on%2520the%2520observation%2520that%250Athese%2520data%2520types%2520activate%2520different%2520neurons%2520during%2520LLM%2520inference.%2520We%2520also%250Aintroduce%2520CCNewsPDD%252C%2520a%2520temporally%2520unbiased%2520benchmark%2520employing%2520rigorous%2520data%250Atransformations%2520to%2520ensure%2520consistent%2520time%2520distributions%2520between%2520training%2520and%250Anon-training%2520data.%2520Our%2520experiments%2520demonstrate%2520that%2520NA-PDD%2520significantly%250Aoutperforms%2520existing%2520methods%2520across%2520three%2520benchmarks%2520and%2520multiple%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Pre-training%20Data%20in%20LLMs%3A%20A%20Neuron%20Activation-Based%0A%20%20Detection%20Framework&entry.906535625=Hongyi%20Tang%20and%20Zhihao%20Zhu%20and%20Yi%20Yang&entry.1292438233=%20%20The%20performance%20of%20large%20language%20models%20%28LLMs%29%20is%20closely%20tied%20to%20their%0Atraining%20data%2C%20which%20can%20include%20copyrighted%20material%20or%20private%20information%2C%0Araising%20legal%20and%20ethical%20concerns.%20Additionally%2C%20LLMs%20face%20criticism%20for%0Adataset%20contamination%20and%20internalizing%20biases.%20To%20address%20these%20issues%2C%20the%0APre-Training%20Data%20Detection%20%28PDD%29%20task%20was%20proposed%20to%20identify%20if%20specific%0Adata%20was%20included%20in%20an%20LLM%27s%20pre-training%20corpus.%20However%2C%20existing%20PDD%0Amethods%20often%20rely%20on%20superficial%20features%20like%20prediction%20confidence%20and%20loss%2C%0Aresulting%20in%20mediocre%20performance.%20To%20improve%20this%2C%20we%20introduce%20NA-PDD%2C%20a%0Anovel%20algorithm%20analyzing%20differential%20neuron%20activation%20patterns%20between%0Atraining%20and%20non-training%20data%20in%20LLMs.%20This%20is%20based%20on%20the%20observation%20that%0Athese%20data%20types%20activate%20different%20neurons%20during%20LLM%20inference.%20We%20also%0Aintroduce%20CCNewsPDD%2C%20a%20temporally%20unbiased%20benchmark%20employing%20rigorous%20data%0Atransformations%20to%20ensure%20consistent%20time%20distributions%20between%20training%20and%0Anon-training%20data.%20Our%20experiments%20demonstrate%20that%20NA-PDD%20significantly%0Aoutperforms%20existing%20methods%20across%20three%20benchmarks%20and%20multiple%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16414v1&entry.124074799=Read"},
{"title": "ACT: Bridging the Gap in Code Translation through Synthetic Data\n  Generation & Adaptive Training", "author": "Shreya Saxena and Siva Prasad and Zishan Ahmad and Vishal Vaddina", "abstract": "  Code translation is a crucial process in software development and migration\nprojects, enabling interoperability between different programming languages and\nenhancing software adaptability and thus longevity. Traditional automated\ntranslation methods rely heavily on handcrafted transformation rules, which\noften lack flexibility and scalability. Meanwhile, advanced language models\npresent promising alternatives but are often limited by proprietary, API-based\nimplementations that raise concerns over data security and reliance. In this\npaper, we present Auto-Train for Code Translation (ACT), an innovative\nframework that aims to improve code translation capabilities by enabling\nin-house finetuning of open-source Large Language Models (LLMs). ACT's\nautomated pipeline significantly boosts the performance of these models,\nnarrowing the gap between open-source accessibility and the high performance of\nclosed-source solutions. Central to ACT is its synthetic data generation\nmodule, which builds extensive, high-quality datasets from initial code\nsamples, incorporating unit tests to ensure functional accuracy and diversity.\nACT's evaluation framework incorporates execution-level checks, offering a\ncomprehensive assessment of translation quality. A key feature in ACT is its\ncontroller module, which manages the entire pipeline by dynamically adjusting\nhyperparameters, orchestrating iterative data generation, and finetuning based\non real-time evaluations. This enables ACT to intelligently optimize when to\ncontinue training, generate additional targeted training data, or stop the\nprocess. Our results demonstrate that ACT consistently enhances the\neffectiveness of open-source models, offering businesses and developers a\nsecure and reliable alternative. Additionally, applying our data generation\npipeline to industry-scale migration projects has led to a notable increase in\ndeveloper acceleration.\n", "link": "http://arxiv.org/abs/2507.16478v1", "date": "2025-07-22", "relevancy": 2.0625, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.537}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5014}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACT%3A%20Bridging%20the%20Gap%20in%20Code%20Translation%20through%20Synthetic%20Data%0A%20%20Generation%20%26%20Adaptive%20Training&body=Title%3A%20ACT%3A%20Bridging%20the%20Gap%20in%20Code%20Translation%20through%20Synthetic%20Data%0A%20%20Generation%20%26%20Adaptive%20Training%0AAuthor%3A%20Shreya%20Saxena%20and%20Siva%20Prasad%20and%20Zishan%20Ahmad%20and%20Vishal%20Vaddina%0AAbstract%3A%20%20%20Code%20translation%20is%20a%20crucial%20process%20in%20software%20development%20and%20migration%0Aprojects%2C%20enabling%20interoperability%20between%20different%20programming%20languages%20and%0Aenhancing%20software%20adaptability%20and%20thus%20longevity.%20Traditional%20automated%0Atranslation%20methods%20rely%20heavily%20on%20handcrafted%20transformation%20rules%2C%20which%0Aoften%20lack%20flexibility%20and%20scalability.%20Meanwhile%2C%20advanced%20language%20models%0Apresent%20promising%20alternatives%20but%20are%20often%20limited%20by%20proprietary%2C%20API-based%0Aimplementations%20that%20raise%20concerns%20over%20data%20security%20and%20reliance.%20In%20this%0Apaper%2C%20we%20present%20Auto-Train%20for%20Code%20Translation%20%28ACT%29%2C%20an%20innovative%0Aframework%20that%20aims%20to%20improve%20code%20translation%20capabilities%20by%20enabling%0Ain-house%20finetuning%20of%20open-source%20Large%20Language%20Models%20%28LLMs%29.%20ACT%27s%0Aautomated%20pipeline%20significantly%20boosts%20the%20performance%20of%20these%20models%2C%0Anarrowing%20the%20gap%20between%20open-source%20accessibility%20and%20the%20high%20performance%20of%0Aclosed-source%20solutions.%20Central%20to%20ACT%20is%20its%20synthetic%20data%20generation%0Amodule%2C%20which%20builds%20extensive%2C%20high-quality%20datasets%20from%20initial%20code%0Asamples%2C%20incorporating%20unit%20tests%20to%20ensure%20functional%20accuracy%20and%20diversity.%0AACT%27s%20evaluation%20framework%20incorporates%20execution-level%20checks%2C%20offering%20a%0Acomprehensive%20assessment%20of%20translation%20quality.%20A%20key%20feature%20in%20ACT%20is%20its%0Acontroller%20module%2C%20which%20manages%20the%20entire%20pipeline%20by%20dynamically%20adjusting%0Ahyperparameters%2C%20orchestrating%20iterative%20data%20generation%2C%20and%20finetuning%20based%0Aon%20real-time%20evaluations.%20This%20enables%20ACT%20to%20intelligently%20optimize%20when%20to%0Acontinue%20training%2C%20generate%20additional%20targeted%20training%20data%2C%20or%20stop%20the%0Aprocess.%20Our%20results%20demonstrate%20that%20ACT%20consistently%20enhances%20the%0Aeffectiveness%20of%20open-source%20models%2C%20offering%20businesses%20and%20developers%20a%0Asecure%20and%20reliable%20alternative.%20Additionally%2C%20applying%20our%20data%20generation%0Apipeline%20to%20industry-scale%20migration%20projects%20has%20led%20to%20a%20notable%20increase%20in%0Adeveloper%20acceleration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACT%253A%2520Bridging%2520the%2520Gap%2520in%2520Code%2520Translation%2520through%2520Synthetic%2520Data%250A%2520%2520Generation%2520%2526%2520Adaptive%2520Training%26entry.906535625%3DShreya%2520Saxena%2520and%2520Siva%2520Prasad%2520and%2520Zishan%2520Ahmad%2520and%2520Vishal%2520Vaddina%26entry.1292438233%3D%2520%2520Code%2520translation%2520is%2520a%2520crucial%2520process%2520in%2520software%2520development%2520and%2520migration%250Aprojects%252C%2520enabling%2520interoperability%2520between%2520different%2520programming%2520languages%2520and%250Aenhancing%2520software%2520adaptability%2520and%2520thus%2520longevity.%2520Traditional%2520automated%250Atranslation%2520methods%2520rely%2520heavily%2520on%2520handcrafted%2520transformation%2520rules%252C%2520which%250Aoften%2520lack%2520flexibility%2520and%2520scalability.%2520Meanwhile%252C%2520advanced%2520language%2520models%250Apresent%2520promising%2520alternatives%2520but%2520are%2520often%2520limited%2520by%2520proprietary%252C%2520API-based%250Aimplementations%2520that%2520raise%2520concerns%2520over%2520data%2520security%2520and%2520reliance.%2520In%2520this%250Apaper%252C%2520we%2520present%2520Auto-Train%2520for%2520Code%2520Translation%2520%2528ACT%2529%252C%2520an%2520innovative%250Aframework%2520that%2520aims%2520to%2520improve%2520code%2520translation%2520capabilities%2520by%2520enabling%250Ain-house%2520finetuning%2520of%2520open-source%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520ACT%2527s%250Aautomated%2520pipeline%2520significantly%2520boosts%2520the%2520performance%2520of%2520these%2520models%252C%250Anarrowing%2520the%2520gap%2520between%2520open-source%2520accessibility%2520and%2520the%2520high%2520performance%2520of%250Aclosed-source%2520solutions.%2520Central%2520to%2520ACT%2520is%2520its%2520synthetic%2520data%2520generation%250Amodule%252C%2520which%2520builds%2520extensive%252C%2520high-quality%2520datasets%2520from%2520initial%2520code%250Asamples%252C%2520incorporating%2520unit%2520tests%2520to%2520ensure%2520functional%2520accuracy%2520and%2520diversity.%250AACT%2527s%2520evaluation%2520framework%2520incorporates%2520execution-level%2520checks%252C%2520offering%2520a%250Acomprehensive%2520assessment%2520of%2520translation%2520quality.%2520A%2520key%2520feature%2520in%2520ACT%2520is%2520its%250Acontroller%2520module%252C%2520which%2520manages%2520the%2520entire%2520pipeline%2520by%2520dynamically%2520adjusting%250Ahyperparameters%252C%2520orchestrating%2520iterative%2520data%2520generation%252C%2520and%2520finetuning%2520based%250Aon%2520real-time%2520evaluations.%2520This%2520enables%2520ACT%2520to%2520intelligently%2520optimize%2520when%2520to%250Acontinue%2520training%252C%2520generate%2520additional%2520targeted%2520training%2520data%252C%2520or%2520stop%2520the%250Aprocess.%2520Our%2520results%2520demonstrate%2520that%2520ACT%2520consistently%2520enhances%2520the%250Aeffectiveness%2520of%2520open-source%2520models%252C%2520offering%2520businesses%2520and%2520developers%2520a%250Asecure%2520and%2520reliable%2520alternative.%2520Additionally%252C%2520applying%2520our%2520data%2520generation%250Apipeline%2520to%2520industry-scale%2520migration%2520projects%2520has%2520led%2520to%2520a%2520notable%2520increase%2520in%250Adeveloper%2520acceleration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACT%3A%20Bridging%20the%20Gap%20in%20Code%20Translation%20through%20Synthetic%20Data%0A%20%20Generation%20%26%20Adaptive%20Training&entry.906535625=Shreya%20Saxena%20and%20Siva%20Prasad%20and%20Zishan%20Ahmad%20and%20Vishal%20Vaddina&entry.1292438233=%20%20Code%20translation%20is%20a%20crucial%20process%20in%20software%20development%20and%20migration%0Aprojects%2C%20enabling%20interoperability%20between%20different%20programming%20languages%20and%0Aenhancing%20software%20adaptability%20and%20thus%20longevity.%20Traditional%20automated%0Atranslation%20methods%20rely%20heavily%20on%20handcrafted%20transformation%20rules%2C%20which%0Aoften%20lack%20flexibility%20and%20scalability.%20Meanwhile%2C%20advanced%20language%20models%0Apresent%20promising%20alternatives%20but%20are%20often%20limited%20by%20proprietary%2C%20API-based%0Aimplementations%20that%20raise%20concerns%20over%20data%20security%20and%20reliance.%20In%20this%0Apaper%2C%20we%20present%20Auto-Train%20for%20Code%20Translation%20%28ACT%29%2C%20an%20innovative%0Aframework%20that%20aims%20to%20improve%20code%20translation%20capabilities%20by%20enabling%0Ain-house%20finetuning%20of%20open-source%20Large%20Language%20Models%20%28LLMs%29.%20ACT%27s%0Aautomated%20pipeline%20significantly%20boosts%20the%20performance%20of%20these%20models%2C%0Anarrowing%20the%20gap%20between%20open-source%20accessibility%20and%20the%20high%20performance%20of%0Aclosed-source%20solutions.%20Central%20to%20ACT%20is%20its%20synthetic%20data%20generation%0Amodule%2C%20which%20builds%20extensive%2C%20high-quality%20datasets%20from%20initial%20code%0Asamples%2C%20incorporating%20unit%20tests%20to%20ensure%20functional%20accuracy%20and%20diversity.%0AACT%27s%20evaluation%20framework%20incorporates%20execution-level%20checks%2C%20offering%20a%0Acomprehensive%20assessment%20of%20translation%20quality.%20A%20key%20feature%20in%20ACT%20is%20its%0Acontroller%20module%2C%20which%20manages%20the%20entire%20pipeline%20by%20dynamically%20adjusting%0Ahyperparameters%2C%20orchestrating%20iterative%20data%20generation%2C%20and%20finetuning%20based%0Aon%20real-time%20evaluations.%20This%20enables%20ACT%20to%20intelligently%20optimize%20when%20to%0Acontinue%20training%2C%20generate%20additional%20targeted%20training%20data%2C%20or%20stop%20the%0Aprocess.%20Our%20results%20demonstrate%20that%20ACT%20consistently%20enhances%20the%0Aeffectiveness%20of%20open-source%20models%2C%20offering%20businesses%20and%20developers%20a%0Asecure%20and%20reliable%20alternative.%20Additionally%2C%20applying%20our%20data%20generation%0Apipeline%20to%20industry-scale%20migration%20projects%20has%20led%20to%20a%20notable%20increase%20in%0Adeveloper%20acceleration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16478v1&entry.124074799=Read"},
{"title": "FLLIC: Functionally Lossless Image Compression", "author": "Xi Zhang and Xiaolin Wu", "abstract": "  Recently, DNN models for lossless image coding have surpassed their\ntraditional counterparts in compression performance, reducing the previous\nlossless bit rate by about ten percent for natural color images. But even with\nthese advances, mathematically lossless image compression (MLLIC) ratios for\nnatural images still fall short of the bandwidth and cost-effectiveness\nrequirements of most practical imaging and vision systems at present and\nbeyond. To overcome the performance barrier of MLLIC, we question the very\nnecessity of MLLIC. Considering that all digital imaging sensors suffer from\nacquisition noises, why should we insist on mathematically lossless coding,\ni.e., wasting bits to preserve noises? Instead, we propose a new paradigm of\njoint denoising and compression called functionally lossless image compression\n(FLLIC), which performs lossless compression of optimally denoised images (the\noptimality may be task-specific). Although not literally lossless with respect\nto the noisy input, FLLIC aims to achieve the best possible reconstruction of\nthe latent noise-free original image. Extensive experiments show that FLLIC\nachieves state-of-the-art performance in joint denoising and compression of\nnoisy images and does so at a lower computational cost.\n", "link": "http://arxiv.org/abs/2401.13616v4", "date": "2025-07-22", "relevancy": 2.058, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5363}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5085}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLLIC%3A%20Functionally%20Lossless%20Image%20Compression&body=Title%3A%20FLLIC%3A%20Functionally%20Lossless%20Image%20Compression%0AAuthor%3A%20Xi%20Zhang%20and%20Xiaolin%20Wu%0AAbstract%3A%20%20%20Recently%2C%20DNN%20models%20for%20lossless%20image%20coding%20have%20surpassed%20their%0Atraditional%20counterparts%20in%20compression%20performance%2C%20reducing%20the%20previous%0Alossless%20bit%20rate%20by%20about%20ten%20percent%20for%20natural%20color%20images.%20But%20even%20with%0Athese%20advances%2C%20mathematically%20lossless%20image%20compression%20%28MLLIC%29%20ratios%20for%0Anatural%20images%20still%20fall%20short%20of%20the%20bandwidth%20and%20cost-effectiveness%0Arequirements%20of%20most%20practical%20imaging%20and%20vision%20systems%20at%20present%20and%0Abeyond.%20To%20overcome%20the%20performance%20barrier%20of%20MLLIC%2C%20we%20question%20the%20very%0Anecessity%20of%20MLLIC.%20Considering%20that%20all%20digital%20imaging%20sensors%20suffer%20from%0Aacquisition%20noises%2C%20why%20should%20we%20insist%20on%20mathematically%20lossless%20coding%2C%0Ai.e.%2C%20wasting%20bits%20to%20preserve%20noises%3F%20Instead%2C%20we%20propose%20a%20new%20paradigm%20of%0Ajoint%20denoising%20and%20compression%20called%20functionally%20lossless%20image%20compression%0A%28FLLIC%29%2C%20which%20performs%20lossless%20compression%20of%20optimally%20denoised%20images%20%28the%0Aoptimality%20may%20be%20task-specific%29.%20Although%20not%20literally%20lossless%20with%20respect%0Ato%20the%20noisy%20input%2C%20FLLIC%20aims%20to%20achieve%20the%20best%20possible%20reconstruction%20of%0Athe%20latent%20noise-free%20original%20image.%20Extensive%20experiments%20show%20that%20FLLIC%0Aachieves%20state-of-the-art%20performance%20in%20joint%20denoising%20and%20compression%20of%0Anoisy%20images%20and%20does%20so%20at%20a%20lower%20computational%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13616v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLLIC%253A%2520Functionally%2520Lossless%2520Image%2520Compression%26entry.906535625%3DXi%2520Zhang%2520and%2520Xiaolin%2520Wu%26entry.1292438233%3D%2520%2520Recently%252C%2520DNN%2520models%2520for%2520lossless%2520image%2520coding%2520have%2520surpassed%2520their%250Atraditional%2520counterparts%2520in%2520compression%2520performance%252C%2520reducing%2520the%2520previous%250Alossless%2520bit%2520rate%2520by%2520about%2520ten%2520percent%2520for%2520natural%2520color%2520images.%2520But%2520even%2520with%250Athese%2520advances%252C%2520mathematically%2520lossless%2520image%2520compression%2520%2528MLLIC%2529%2520ratios%2520for%250Anatural%2520images%2520still%2520fall%2520short%2520of%2520the%2520bandwidth%2520and%2520cost-effectiveness%250Arequirements%2520of%2520most%2520practical%2520imaging%2520and%2520vision%2520systems%2520at%2520present%2520and%250Abeyond.%2520To%2520overcome%2520the%2520performance%2520barrier%2520of%2520MLLIC%252C%2520we%2520question%2520the%2520very%250Anecessity%2520of%2520MLLIC.%2520Considering%2520that%2520all%2520digital%2520imaging%2520sensors%2520suffer%2520from%250Aacquisition%2520noises%252C%2520why%2520should%2520we%2520insist%2520on%2520mathematically%2520lossless%2520coding%252C%250Ai.e.%252C%2520wasting%2520bits%2520to%2520preserve%2520noises%253F%2520Instead%252C%2520we%2520propose%2520a%2520new%2520paradigm%2520of%250Ajoint%2520denoising%2520and%2520compression%2520called%2520functionally%2520lossless%2520image%2520compression%250A%2528FLLIC%2529%252C%2520which%2520performs%2520lossless%2520compression%2520of%2520optimally%2520denoised%2520images%2520%2528the%250Aoptimality%2520may%2520be%2520task-specific%2529.%2520Although%2520not%2520literally%2520lossless%2520with%2520respect%250Ato%2520the%2520noisy%2520input%252C%2520FLLIC%2520aims%2520to%2520achieve%2520the%2520best%2520possible%2520reconstruction%2520of%250Athe%2520latent%2520noise-free%2520original%2520image.%2520Extensive%2520experiments%2520show%2520that%2520FLLIC%250Aachieves%2520state-of-the-art%2520performance%2520in%2520joint%2520denoising%2520and%2520compression%2520of%250Anoisy%2520images%2520and%2520does%2520so%2520at%2520a%2520lower%2520computational%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13616v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLLIC%3A%20Functionally%20Lossless%20Image%20Compression&entry.906535625=Xi%20Zhang%20and%20Xiaolin%20Wu&entry.1292438233=%20%20Recently%2C%20DNN%20models%20for%20lossless%20image%20coding%20have%20surpassed%20their%0Atraditional%20counterparts%20in%20compression%20performance%2C%20reducing%20the%20previous%0Alossless%20bit%20rate%20by%20about%20ten%20percent%20for%20natural%20color%20images.%20But%20even%20with%0Athese%20advances%2C%20mathematically%20lossless%20image%20compression%20%28MLLIC%29%20ratios%20for%0Anatural%20images%20still%20fall%20short%20of%20the%20bandwidth%20and%20cost-effectiveness%0Arequirements%20of%20most%20practical%20imaging%20and%20vision%20systems%20at%20present%20and%0Abeyond.%20To%20overcome%20the%20performance%20barrier%20of%20MLLIC%2C%20we%20question%20the%20very%0Anecessity%20of%20MLLIC.%20Considering%20that%20all%20digital%20imaging%20sensors%20suffer%20from%0Aacquisition%20noises%2C%20why%20should%20we%20insist%20on%20mathematically%20lossless%20coding%2C%0Ai.e.%2C%20wasting%20bits%20to%20preserve%20noises%3F%20Instead%2C%20we%20propose%20a%20new%20paradigm%20of%0Ajoint%20denoising%20and%20compression%20called%20functionally%20lossless%20image%20compression%0A%28FLLIC%29%2C%20which%20performs%20lossless%20compression%20of%20optimally%20denoised%20images%20%28the%0Aoptimality%20may%20be%20task-specific%29.%20Although%20not%20literally%20lossless%20with%20respect%0Ato%20the%20noisy%20input%2C%20FLLIC%20aims%20to%20achieve%20the%20best%20possible%20reconstruction%20of%0Athe%20latent%20noise-free%20original%20image.%20Extensive%20experiments%20show%20that%20FLLIC%0Aachieves%20state-of-the-art%20performance%20in%20joint%20denoising%20and%20compression%20of%0Anoisy%20images%20and%20does%20so%20at%20a%20lower%20computational%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13616v4&entry.124074799=Read"},
{"title": "The Joys of Categorical Conformal Prediction", "author": "Michele Caprio", "abstract": "  Conformal prediction (CP) is an Uncertainty Representation technique that\ndelivers finite-sample calibrated prediction regions for any underlying Machine\nLearning model. Its status as an Uncertainty Quantification (UQ) tool, though,\nhas remained conceptually opaque: While Conformal Prediction Regions (CPRs)\ngive an ordinal representation of uncertainty (larger regions typically\nindicate higher uncertainty), they lack the capability to cardinally quantify\nit (twice as large regions do not imply twice the uncertainty). We adopt a\ncategory-theoretic approach to CP -- framing it as a morphism, embedded in a\ncommuting diagram, of two newly-defined categories -- that brings us three\njoys. First, we show that -- under minimal assumptions -- CP is intrinsically a\nUQ mechanism, that is, its cardinal UQ capabilities are a structural feature of\nthe method. Second, we demonstrate that CP bridges (and perhaps subsumes) the\nBayesian, frequentist, and imprecise probabilistic approaches to predictive\nstatistical reasoning. Finally, we show that a CPR is the image of a covariant\nfunctor. This observation is relevant to AI privacy: It implies that privacy\nnoise added locally does not break the global coverage guarantee.\n", "link": "http://arxiv.org/abs/2507.04441v2", "date": "2025-07-22", "relevancy": 2.0553, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5225}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5187}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Joys%20of%20Categorical%20Conformal%20Prediction&body=Title%3A%20The%20Joys%20of%20Categorical%20Conformal%20Prediction%0AAuthor%3A%20Michele%20Caprio%0AAbstract%3A%20%20%20Conformal%20prediction%20%28CP%29%20is%20an%20Uncertainty%20Representation%20technique%20that%0Adelivers%20finite-sample%20calibrated%20prediction%20regions%20for%20any%20underlying%20Machine%0ALearning%20model.%20Its%20status%20as%20an%20Uncertainty%20Quantification%20%28UQ%29%20tool%2C%20though%2C%0Ahas%20remained%20conceptually%20opaque%3A%20While%20Conformal%20Prediction%20Regions%20%28CPRs%29%0Agive%20an%20ordinal%20representation%20of%20uncertainty%20%28larger%20regions%20typically%0Aindicate%20higher%20uncertainty%29%2C%20they%20lack%20the%20capability%20to%20cardinally%20quantify%0Ait%20%28twice%20as%20large%20regions%20do%20not%20imply%20twice%20the%20uncertainty%29.%20We%20adopt%20a%0Acategory-theoretic%20approach%20to%20CP%20--%20framing%20it%20as%20a%20morphism%2C%20embedded%20in%20a%0Acommuting%20diagram%2C%20of%20two%20newly-defined%20categories%20--%20that%20brings%20us%20three%0Ajoys.%20First%2C%20we%20show%20that%20--%20under%20minimal%20assumptions%20--%20CP%20is%20intrinsically%20a%0AUQ%20mechanism%2C%20that%20is%2C%20its%20cardinal%20UQ%20capabilities%20are%20a%20structural%20feature%20of%0Athe%20method.%20Second%2C%20we%20demonstrate%20that%20CP%20bridges%20%28and%20perhaps%20subsumes%29%20the%0ABayesian%2C%20frequentist%2C%20and%20imprecise%20probabilistic%20approaches%20to%20predictive%0Astatistical%20reasoning.%20Finally%2C%20we%20show%20that%20a%20CPR%20is%20the%20image%20of%20a%20covariant%0Afunctor.%20This%20observation%20is%20relevant%20to%20AI%20privacy%3A%20It%20implies%20that%20privacy%0Anoise%20added%20locally%20does%20not%20break%20the%20global%20coverage%20guarantee.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04441v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Joys%2520of%2520Categorical%2520Conformal%2520Prediction%26entry.906535625%3DMichele%2520Caprio%26entry.1292438233%3D%2520%2520Conformal%2520prediction%2520%2528CP%2529%2520is%2520an%2520Uncertainty%2520Representation%2520technique%2520that%250Adelivers%2520finite-sample%2520calibrated%2520prediction%2520regions%2520for%2520any%2520underlying%2520Machine%250ALearning%2520model.%2520Its%2520status%2520as%2520an%2520Uncertainty%2520Quantification%2520%2528UQ%2529%2520tool%252C%2520though%252C%250Ahas%2520remained%2520conceptually%2520opaque%253A%2520While%2520Conformal%2520Prediction%2520Regions%2520%2528CPRs%2529%250Agive%2520an%2520ordinal%2520representation%2520of%2520uncertainty%2520%2528larger%2520regions%2520typically%250Aindicate%2520higher%2520uncertainty%2529%252C%2520they%2520lack%2520the%2520capability%2520to%2520cardinally%2520quantify%250Ait%2520%2528twice%2520as%2520large%2520regions%2520do%2520not%2520imply%2520twice%2520the%2520uncertainty%2529.%2520We%2520adopt%2520a%250Acategory-theoretic%2520approach%2520to%2520CP%2520--%2520framing%2520it%2520as%2520a%2520morphism%252C%2520embedded%2520in%2520a%250Acommuting%2520diagram%252C%2520of%2520two%2520newly-defined%2520categories%2520--%2520that%2520brings%2520us%2520three%250Ajoys.%2520First%252C%2520we%2520show%2520that%2520--%2520under%2520minimal%2520assumptions%2520--%2520CP%2520is%2520intrinsically%2520a%250AUQ%2520mechanism%252C%2520that%2520is%252C%2520its%2520cardinal%2520UQ%2520capabilities%2520are%2520a%2520structural%2520feature%2520of%250Athe%2520method.%2520Second%252C%2520we%2520demonstrate%2520that%2520CP%2520bridges%2520%2528and%2520perhaps%2520subsumes%2529%2520the%250ABayesian%252C%2520frequentist%252C%2520and%2520imprecise%2520probabilistic%2520approaches%2520to%2520predictive%250Astatistical%2520reasoning.%2520Finally%252C%2520we%2520show%2520that%2520a%2520CPR%2520is%2520the%2520image%2520of%2520a%2520covariant%250Afunctor.%2520This%2520observation%2520is%2520relevant%2520to%2520AI%2520privacy%253A%2520It%2520implies%2520that%2520privacy%250Anoise%2520added%2520locally%2520does%2520not%2520break%2520the%2520global%2520coverage%2520guarantee.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04441v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Joys%20of%20Categorical%20Conformal%20Prediction&entry.906535625=Michele%20Caprio&entry.1292438233=%20%20Conformal%20prediction%20%28CP%29%20is%20an%20Uncertainty%20Representation%20technique%20that%0Adelivers%20finite-sample%20calibrated%20prediction%20regions%20for%20any%20underlying%20Machine%0ALearning%20model.%20Its%20status%20as%20an%20Uncertainty%20Quantification%20%28UQ%29%20tool%2C%20though%2C%0Ahas%20remained%20conceptually%20opaque%3A%20While%20Conformal%20Prediction%20Regions%20%28CPRs%29%0Agive%20an%20ordinal%20representation%20of%20uncertainty%20%28larger%20regions%20typically%0Aindicate%20higher%20uncertainty%29%2C%20they%20lack%20the%20capability%20to%20cardinally%20quantify%0Ait%20%28twice%20as%20large%20regions%20do%20not%20imply%20twice%20the%20uncertainty%29.%20We%20adopt%20a%0Acategory-theoretic%20approach%20to%20CP%20--%20framing%20it%20as%20a%20morphism%2C%20embedded%20in%20a%0Acommuting%20diagram%2C%20of%20two%20newly-defined%20categories%20--%20that%20brings%20us%20three%0Ajoys.%20First%2C%20we%20show%20that%20--%20under%20minimal%20assumptions%20--%20CP%20is%20intrinsically%20a%0AUQ%20mechanism%2C%20that%20is%2C%20its%20cardinal%20UQ%20capabilities%20are%20a%20structural%20feature%20of%0Athe%20method.%20Second%2C%20we%20demonstrate%20that%20CP%20bridges%20%28and%20perhaps%20subsumes%29%20the%0ABayesian%2C%20frequentist%2C%20and%20imprecise%20probabilistic%20approaches%20to%20predictive%0Astatistical%20reasoning.%20Finally%2C%20we%20show%20that%20a%20CPR%20is%20the%20image%20of%20a%20covariant%0Afunctor.%20This%20observation%20is%20relevant%20to%20AI%20privacy%3A%20It%20implies%20that%20privacy%0Anoise%20added%20locally%20does%20not%20break%20the%20global%20coverage%20guarantee.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04441v2&entry.124074799=Read"},
{"title": "Scaling Linear Attention with Sparse State Expansion", "author": "Yuqi Pan and Yongqi An and Zheng Li and Yuhong Chou and Ruijie Zhu and Xiaohui Wang and Mingxuan Wang and Jinqiao Wang and Guoqi Li", "abstract": "  The Transformer architecture, despite its widespread success, struggles with\nlong-context scenarios due to quadratic computation and linear memory growth.\nWhile various linear attention variants mitigate these efficiency constraints\nby compressing context into fixed-size states, they often degrade performance\nin tasks such as in-context retrieval and reasoning. To address this limitation\nand achieve more effective context compression, we propose two key innovations.\nFirst, we introduce a row-sparse update formulation for linear attention by\nconceptualizing state updating as information classification. This enables\nsparse state updates via softmax-based top-$k$ hard classification, thereby\nextending receptive fields and reducing inter-class interference. Second, we\npresent Sparse State Expansion (SSE) within the sparse framework, which expands\nthe contextual state into multiple partitions, effectively decoupling parameter\nsize from state capacity while maintaining the sparse classification paradigm.\nOur design, supported by efficient parallelized implementations, yields\neffective classification and discriminative state representations. We\nextensively validate SSE in both pure linear and hybrid (SSE-H) architectures\nacross language modeling, in-context retrieval, and mathematical reasoning\nbenchmarks. SSE demonstrates strong retrieval performance and scales favorably\nwith state size. Moreover, after reinforcement learning (RL) training, our 2B\nSSE-H model achieves state-of-the-art mathematical reasoning performance among\nsmall reasoning models, scoring 64.7 on AIME24 and 51.3 on AIME25,\nsignificantly outperforming similarly sized open-source Transformers. These\nresults highlight SSE as a promising and efficient architecture for\nlong-context modeling.\n", "link": "http://arxiv.org/abs/2507.16577v1", "date": "2025-07-22", "relevancy": 2.055, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5651}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5227}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Linear%20Attention%20with%20Sparse%20State%20Expansion&body=Title%3A%20Scaling%20Linear%20Attention%20with%20Sparse%20State%20Expansion%0AAuthor%3A%20Yuqi%20Pan%20and%20Yongqi%20An%20and%20Zheng%20Li%20and%20Yuhong%20Chou%20and%20Ruijie%20Zhu%20and%20Xiaohui%20Wang%20and%20Mingxuan%20Wang%20and%20Jinqiao%20Wang%20and%20Guoqi%20Li%0AAbstract%3A%20%20%20The%20Transformer%20architecture%2C%20despite%20its%20widespread%20success%2C%20struggles%20with%0Along-context%20scenarios%20due%20to%20quadratic%20computation%20and%20linear%20memory%20growth.%0AWhile%20various%20linear%20attention%20variants%20mitigate%20these%20efficiency%20constraints%0Aby%20compressing%20context%20into%20fixed-size%20states%2C%20they%20often%20degrade%20performance%0Ain%20tasks%20such%20as%20in-context%20retrieval%20and%20reasoning.%20To%20address%20this%20limitation%0Aand%20achieve%20more%20effective%20context%20compression%2C%20we%20propose%20two%20key%20innovations.%0AFirst%2C%20we%20introduce%20a%20row-sparse%20update%20formulation%20for%20linear%20attention%20by%0Aconceptualizing%20state%20updating%20as%20information%20classification.%20This%20enables%0Asparse%20state%20updates%20via%20softmax-based%20top-%24k%24%20hard%20classification%2C%20thereby%0Aextending%20receptive%20fields%20and%20reducing%20inter-class%20interference.%20Second%2C%20we%0Apresent%20Sparse%20State%20Expansion%20%28SSE%29%20within%20the%20sparse%20framework%2C%20which%20expands%0Athe%20contextual%20state%20into%20multiple%20partitions%2C%20effectively%20decoupling%20parameter%0Asize%20from%20state%20capacity%20while%20maintaining%20the%20sparse%20classification%20paradigm.%0AOur%20design%2C%20supported%20by%20efficient%20parallelized%20implementations%2C%20yields%0Aeffective%20classification%20and%20discriminative%20state%20representations.%20We%0Aextensively%20validate%20SSE%20in%20both%20pure%20linear%20and%20hybrid%20%28SSE-H%29%20architectures%0Aacross%20language%20modeling%2C%20in-context%20retrieval%2C%20and%20mathematical%20reasoning%0Abenchmarks.%20SSE%20demonstrates%20strong%20retrieval%20performance%20and%20scales%20favorably%0Awith%20state%20size.%20Moreover%2C%20after%20reinforcement%20learning%20%28RL%29%20training%2C%20our%202B%0ASSE-H%20model%20achieves%20state-of-the-art%20mathematical%20reasoning%20performance%20among%0Asmall%20reasoning%20models%2C%20scoring%2064.7%20on%20AIME24%20and%2051.3%20on%20AIME25%2C%0Asignificantly%20outperforming%20similarly%20sized%20open-source%20Transformers.%20These%0Aresults%20highlight%20SSE%20as%20a%20promising%20and%20efficient%20architecture%20for%0Along-context%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Linear%2520Attention%2520with%2520Sparse%2520State%2520Expansion%26entry.906535625%3DYuqi%2520Pan%2520and%2520Yongqi%2520An%2520and%2520Zheng%2520Li%2520and%2520Yuhong%2520Chou%2520and%2520Ruijie%2520Zhu%2520and%2520Xiaohui%2520Wang%2520and%2520Mingxuan%2520Wang%2520and%2520Jinqiao%2520Wang%2520and%2520Guoqi%2520Li%26entry.1292438233%3D%2520%2520The%2520Transformer%2520architecture%252C%2520despite%2520its%2520widespread%2520success%252C%2520struggles%2520with%250Along-context%2520scenarios%2520due%2520to%2520quadratic%2520computation%2520and%2520linear%2520memory%2520growth.%250AWhile%2520various%2520linear%2520attention%2520variants%2520mitigate%2520these%2520efficiency%2520constraints%250Aby%2520compressing%2520context%2520into%2520fixed-size%2520states%252C%2520they%2520often%2520degrade%2520performance%250Ain%2520tasks%2520such%2520as%2520in-context%2520retrieval%2520and%2520reasoning.%2520To%2520address%2520this%2520limitation%250Aand%2520achieve%2520more%2520effective%2520context%2520compression%252C%2520we%2520propose%2520two%2520key%2520innovations.%250AFirst%252C%2520we%2520introduce%2520a%2520row-sparse%2520update%2520formulation%2520for%2520linear%2520attention%2520by%250Aconceptualizing%2520state%2520updating%2520as%2520information%2520classification.%2520This%2520enables%250Asparse%2520state%2520updates%2520via%2520softmax-based%2520top-%2524k%2524%2520hard%2520classification%252C%2520thereby%250Aextending%2520receptive%2520fields%2520and%2520reducing%2520inter-class%2520interference.%2520Second%252C%2520we%250Apresent%2520Sparse%2520State%2520Expansion%2520%2528SSE%2529%2520within%2520the%2520sparse%2520framework%252C%2520which%2520expands%250Athe%2520contextual%2520state%2520into%2520multiple%2520partitions%252C%2520effectively%2520decoupling%2520parameter%250Asize%2520from%2520state%2520capacity%2520while%2520maintaining%2520the%2520sparse%2520classification%2520paradigm.%250AOur%2520design%252C%2520supported%2520by%2520efficient%2520parallelized%2520implementations%252C%2520yields%250Aeffective%2520classification%2520and%2520discriminative%2520state%2520representations.%2520We%250Aextensively%2520validate%2520SSE%2520in%2520both%2520pure%2520linear%2520and%2520hybrid%2520%2528SSE-H%2529%2520architectures%250Aacross%2520language%2520modeling%252C%2520in-context%2520retrieval%252C%2520and%2520mathematical%2520reasoning%250Abenchmarks.%2520SSE%2520demonstrates%2520strong%2520retrieval%2520performance%2520and%2520scales%2520favorably%250Awith%2520state%2520size.%2520Moreover%252C%2520after%2520reinforcement%2520learning%2520%2528RL%2529%2520training%252C%2520our%25202B%250ASSE-H%2520model%2520achieves%2520state-of-the-art%2520mathematical%2520reasoning%2520performance%2520among%250Asmall%2520reasoning%2520models%252C%2520scoring%252064.7%2520on%2520AIME24%2520and%252051.3%2520on%2520AIME25%252C%250Asignificantly%2520outperforming%2520similarly%2520sized%2520open-source%2520Transformers.%2520These%250Aresults%2520highlight%2520SSE%2520as%2520a%2520promising%2520and%2520efficient%2520architecture%2520for%250Along-context%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Linear%20Attention%20with%20Sparse%20State%20Expansion&entry.906535625=Yuqi%20Pan%20and%20Yongqi%20An%20and%20Zheng%20Li%20and%20Yuhong%20Chou%20and%20Ruijie%20Zhu%20and%20Xiaohui%20Wang%20and%20Mingxuan%20Wang%20and%20Jinqiao%20Wang%20and%20Guoqi%20Li&entry.1292438233=%20%20The%20Transformer%20architecture%2C%20despite%20its%20widespread%20success%2C%20struggles%20with%0Along-context%20scenarios%20due%20to%20quadratic%20computation%20and%20linear%20memory%20growth.%0AWhile%20various%20linear%20attention%20variants%20mitigate%20these%20efficiency%20constraints%0Aby%20compressing%20context%20into%20fixed-size%20states%2C%20they%20often%20degrade%20performance%0Ain%20tasks%20such%20as%20in-context%20retrieval%20and%20reasoning.%20To%20address%20this%20limitation%0Aand%20achieve%20more%20effective%20context%20compression%2C%20we%20propose%20two%20key%20innovations.%0AFirst%2C%20we%20introduce%20a%20row-sparse%20update%20formulation%20for%20linear%20attention%20by%0Aconceptualizing%20state%20updating%20as%20information%20classification.%20This%20enables%0Asparse%20state%20updates%20via%20softmax-based%20top-%24k%24%20hard%20classification%2C%20thereby%0Aextending%20receptive%20fields%20and%20reducing%20inter-class%20interference.%20Second%2C%20we%0Apresent%20Sparse%20State%20Expansion%20%28SSE%29%20within%20the%20sparse%20framework%2C%20which%20expands%0Athe%20contextual%20state%20into%20multiple%20partitions%2C%20effectively%20decoupling%20parameter%0Asize%20from%20state%20capacity%20while%20maintaining%20the%20sparse%20classification%20paradigm.%0AOur%20design%2C%20supported%20by%20efficient%20parallelized%20implementations%2C%20yields%0Aeffective%20classification%20and%20discriminative%20state%20representations.%20We%0Aextensively%20validate%20SSE%20in%20both%20pure%20linear%20and%20hybrid%20%28SSE-H%29%20architectures%0Aacross%20language%20modeling%2C%20in-context%20retrieval%2C%20and%20mathematical%20reasoning%0Abenchmarks.%20SSE%20demonstrates%20strong%20retrieval%20performance%20and%20scales%20favorably%0Awith%20state%20size.%20Moreover%2C%20after%20reinforcement%20learning%20%28RL%29%20training%2C%20our%202B%0ASSE-H%20model%20achieves%20state-of-the-art%20mathematical%20reasoning%20performance%20among%0Asmall%20reasoning%20models%2C%20scoring%2064.7%20on%20AIME24%20and%2051.3%20on%20AIME25%2C%0Asignificantly%20outperforming%20similarly%20sized%20open-source%20Transformers.%20These%0Aresults%20highlight%20SSE%20as%20a%20promising%20and%20efficient%20architecture%20for%0Along-context%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16577v1&entry.124074799=Read"},
{"title": "Can LLMs Generate Reliable Test Case Generators? A Study on\n  Competition-Level Programming Problems", "author": "Yuhan Cao and Zian Chen and Kun Quan and Ziliang Zhang and Yu Wang and Xiaoning Dong and Yeqi Feng and Guanzhong He and Jingcheng Huang and Jianhao Li and Yixuan Tan and Jiafu Tang and Yilin Tang and Junlei Wu and Qianyu Xiao and Can Zheng and Shouchen Zhou and Yuxiang Zhu and Yiming Huang and Tian Xie and Tianxing He", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation, capable of tackling complex tasks during inference. However,\nthe extent to which LLMs can be utilized for code checking or debugging through\ntest case generation remains largely unexplored. We investigate this problem\nfrom the perspective of competition-level programming (CP) programs and propose\nTCGBench, a Benchmark for (LLM generation of) Test Case Generators. This\nbenchmark comprises two tasks, aimed at studying the capabilities of LLMs in\n(1) generating valid test case generators for a given CP problem, and further\n(2) generating targeted test case generators that expose bugs in human-written\ncode. Experimental results indicate that while state-of-the-art LLMs can\ngenerate valid test case generators in most cases, most LLMs struggle to\ngenerate targeted test cases that reveal flaws in human code effectively.\nEspecially, even advanced reasoning models (e.g., o3-mini) fall significantly\nshort of human performance in the task of generating targeted generators.\nFurthermore, we construct a high-quality, manually curated dataset of\ninstructions for generating targeted generators. Analysis demonstrates that the\nperformance of LLMs can be enhanced with the aid of this dataset, by both\nprompting and fine-tuning.\n", "link": "http://arxiv.org/abs/2506.06821v3", "date": "2025-07-22", "relevancy": 1.7798, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4511}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Generate%20Reliable%20Test%20Case%20Generators%3F%20A%20Study%20on%0A%20%20Competition-Level%20Programming%20Problems&body=Title%3A%20Can%20LLMs%20Generate%20Reliable%20Test%20Case%20Generators%3F%20A%20Study%20on%0A%20%20Competition-Level%20Programming%20Problems%0AAuthor%3A%20Yuhan%20Cao%20and%20Zian%20Chen%20and%20Kun%20Quan%20and%20Ziliang%20Zhang%20and%20Yu%20Wang%20and%20Xiaoning%20Dong%20and%20Yeqi%20Feng%20and%20Guanzhong%20He%20and%20Jingcheng%20Huang%20and%20Jianhao%20Li%20and%20Yixuan%20Tan%20and%20Jiafu%20Tang%20and%20Yilin%20Tang%20and%20Junlei%20Wu%20and%20Qianyu%20Xiao%20and%20Can%20Zheng%20and%20Shouchen%20Zhou%20and%20Yuxiang%20Zhu%20and%20Yiming%20Huang%20and%20Tian%20Xie%20and%20Tianxing%20He%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Acode%20generation%2C%20capable%20of%20tackling%20complex%20tasks%20during%20inference.%20However%2C%0Athe%20extent%20to%20which%20LLMs%20can%20be%20utilized%20for%20code%20checking%20or%20debugging%20through%0Atest%20case%20generation%20remains%20largely%20unexplored.%20We%20investigate%20this%20problem%0Afrom%20the%20perspective%20of%20competition-level%20programming%20%28CP%29%20programs%20and%20propose%0ATCGBench%2C%20a%20Benchmark%20for%20%28LLM%20generation%20of%29%20Test%20Case%20Generators.%20This%0Abenchmark%20comprises%20two%20tasks%2C%20aimed%20at%20studying%20the%20capabilities%20of%20LLMs%20in%0A%281%29%20generating%20valid%20test%20case%20generators%20for%20a%20given%20CP%20problem%2C%20and%20further%0A%282%29%20generating%20targeted%20test%20case%20generators%20that%20expose%20bugs%20in%20human-written%0Acode.%20Experimental%20results%20indicate%20that%20while%20state-of-the-art%20LLMs%20can%0Agenerate%20valid%20test%20case%20generators%20in%20most%20cases%2C%20most%20LLMs%20struggle%20to%0Agenerate%20targeted%20test%20cases%20that%20reveal%20flaws%20in%20human%20code%20effectively.%0AEspecially%2C%20even%20advanced%20reasoning%20models%20%28e.g.%2C%20o3-mini%29%20fall%20significantly%0Ashort%20of%20human%20performance%20in%20the%20task%20of%20generating%20targeted%20generators.%0AFurthermore%2C%20we%20construct%20a%20high-quality%2C%20manually%20curated%20dataset%20of%0Ainstructions%20for%20generating%20targeted%20generators.%20Analysis%20demonstrates%20that%20the%0Aperformance%20of%20LLMs%20can%20be%20enhanced%20with%20the%20aid%20of%20this%20dataset%2C%20by%20both%0Aprompting%20and%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.06821v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Generate%2520Reliable%2520Test%2520Case%2520Generators%253F%2520A%2520Study%2520on%250A%2520%2520Competition-Level%2520Programming%2520Problems%26entry.906535625%3DYuhan%2520Cao%2520and%2520Zian%2520Chen%2520and%2520Kun%2520Quan%2520and%2520Ziliang%2520Zhang%2520and%2520Yu%2520Wang%2520and%2520Xiaoning%2520Dong%2520and%2520Yeqi%2520Feng%2520and%2520Guanzhong%2520He%2520and%2520Jingcheng%2520Huang%2520and%2520Jianhao%2520Li%2520and%2520Yixuan%2520Tan%2520and%2520Jiafu%2520Tang%2520and%2520Yilin%2520Tang%2520and%2520Junlei%2520Wu%2520and%2520Qianyu%2520Xiao%2520and%2520Can%2520Zheng%2520and%2520Shouchen%2520Zhou%2520and%2520Yuxiang%2520Zhu%2520and%2520Yiming%2520Huang%2520and%2520Tian%2520Xie%2520and%2520Tianxing%2520He%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Acode%2520generation%252C%2520capable%2520of%2520tackling%2520complex%2520tasks%2520during%2520inference.%2520However%252C%250Athe%2520extent%2520to%2520which%2520LLMs%2520can%2520be%2520utilized%2520for%2520code%2520checking%2520or%2520debugging%2520through%250Atest%2520case%2520generation%2520remains%2520largely%2520unexplored.%2520We%2520investigate%2520this%2520problem%250Afrom%2520the%2520perspective%2520of%2520competition-level%2520programming%2520%2528CP%2529%2520programs%2520and%2520propose%250ATCGBench%252C%2520a%2520Benchmark%2520for%2520%2528LLM%2520generation%2520of%2529%2520Test%2520Case%2520Generators.%2520This%250Abenchmark%2520comprises%2520two%2520tasks%252C%2520aimed%2520at%2520studying%2520the%2520capabilities%2520of%2520LLMs%2520in%250A%25281%2529%2520generating%2520valid%2520test%2520case%2520generators%2520for%2520a%2520given%2520CP%2520problem%252C%2520and%2520further%250A%25282%2529%2520generating%2520targeted%2520test%2520case%2520generators%2520that%2520expose%2520bugs%2520in%2520human-written%250Acode.%2520Experimental%2520results%2520indicate%2520that%2520while%2520state-of-the-art%2520LLMs%2520can%250Agenerate%2520valid%2520test%2520case%2520generators%2520in%2520most%2520cases%252C%2520most%2520LLMs%2520struggle%2520to%250Agenerate%2520targeted%2520test%2520cases%2520that%2520reveal%2520flaws%2520in%2520human%2520code%2520effectively.%250AEspecially%252C%2520even%2520advanced%2520reasoning%2520models%2520%2528e.g.%252C%2520o3-mini%2529%2520fall%2520significantly%250Ashort%2520of%2520human%2520performance%2520in%2520the%2520task%2520of%2520generating%2520targeted%2520generators.%250AFurthermore%252C%2520we%2520construct%2520a%2520high-quality%252C%2520manually%2520curated%2520dataset%2520of%250Ainstructions%2520for%2520generating%2520targeted%2520generators.%2520Analysis%2520demonstrates%2520that%2520the%250Aperformance%2520of%2520LLMs%2520can%2520be%2520enhanced%2520with%2520the%2520aid%2520of%2520this%2520dataset%252C%2520by%2520both%250Aprompting%2520and%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06821v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Generate%20Reliable%20Test%20Case%20Generators%3F%20A%20Study%20on%0A%20%20Competition-Level%20Programming%20Problems&entry.906535625=Yuhan%20Cao%20and%20Zian%20Chen%20and%20Kun%20Quan%20and%20Ziliang%20Zhang%20and%20Yu%20Wang%20and%20Xiaoning%20Dong%20and%20Yeqi%20Feng%20and%20Guanzhong%20He%20and%20Jingcheng%20Huang%20and%20Jianhao%20Li%20and%20Yixuan%20Tan%20and%20Jiafu%20Tang%20and%20Yilin%20Tang%20and%20Junlei%20Wu%20and%20Qianyu%20Xiao%20and%20Can%20Zheng%20and%20Shouchen%20Zhou%20and%20Yuxiang%20Zhu%20and%20Yiming%20Huang%20and%20Tian%20Xie%20and%20Tianxing%20He&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Acode%20generation%2C%20capable%20of%20tackling%20complex%20tasks%20during%20inference.%20However%2C%0Athe%20extent%20to%20which%20LLMs%20can%20be%20utilized%20for%20code%20checking%20or%20debugging%20through%0Atest%20case%20generation%20remains%20largely%20unexplored.%20We%20investigate%20this%20problem%0Afrom%20the%20perspective%20of%20competition-level%20programming%20%28CP%29%20programs%20and%20propose%0ATCGBench%2C%20a%20Benchmark%20for%20%28LLM%20generation%20of%29%20Test%20Case%20Generators.%20This%0Abenchmark%20comprises%20two%20tasks%2C%20aimed%20at%20studying%20the%20capabilities%20of%20LLMs%20in%0A%281%29%20generating%20valid%20test%20case%20generators%20for%20a%20given%20CP%20problem%2C%20and%20further%0A%282%29%20generating%20targeted%20test%20case%20generators%20that%20expose%20bugs%20in%20human-written%0Acode.%20Experimental%20results%20indicate%20that%20while%20state-of-the-art%20LLMs%20can%0Agenerate%20valid%20test%20case%20generators%20in%20most%20cases%2C%20most%20LLMs%20struggle%20to%0Agenerate%20targeted%20test%20cases%20that%20reveal%20flaws%20in%20human%20code%20effectively.%0AEspecially%2C%20even%20advanced%20reasoning%20models%20%28e.g.%2C%20o3-mini%29%20fall%20significantly%0Ashort%20of%20human%20performance%20in%20the%20task%20of%20generating%20targeted%20generators.%0AFurthermore%2C%20we%20construct%20a%20high-quality%2C%20manually%20curated%20dataset%20of%0Ainstructions%20for%20generating%20targeted%20generators.%20Analysis%20demonstrates%20that%20the%0Aperformance%20of%20LLMs%20can%20be%20enhanced%20with%20the%20aid%20of%20this%20dataset%2C%20by%20both%0Aprompting%20and%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.06821v3&entry.124074799=Read"},
{"title": "Distributed Oscillatory Guidance for Formation Flight of Fixed-Wing\n  Drones", "author": "Yang Xu and Jes\u00fas Bautista and Jos\u00e9 Hinojosa and H\u00e9ctor Garc\u00eda de Marina", "abstract": "  The autonomous formation flight of fixed-wing drones is hard when the\ncoordination requires the actuation over their speeds since they are critically\nbounded and aircraft are mostly designed to fly at a nominal airspeed. This\npaper proposes an algorithm to achieve formation flights of fixed-wing drones\nwithout requiring any actuation over their speed. In particular, we guide all\nthe drones to travel over specific paths, e.g., parallel straight lines, and we\nsuperpose an oscillatory behavior onto the guiding vector field that drives the\ndrones to the paths. This oscillation enables control over the average velocity\nalong the path, thereby facilitating inter-drone coordination. Each drone\nadjusts its oscillation amplitude distributively in a closed-loop manner by\ncommunicating with neighboring agents in an undirected and connected graph. A\nnovel consensus algorithm is introduced, leveraging a non-negative, asymmetric\nsaturation function. This unconventional saturation is justified since negative\namplitudes do not make drones travel backward or have a negative velocity along\nthe path. Rigorous theoretical analysis of the algorithm is complemented by\nvalidation through numerical simulations and a real-world formation flight.\n", "link": "http://arxiv.org/abs/2507.16458v1", "date": "2025-07-22", "relevancy": 1.7513, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4273}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Oscillatory%20Guidance%20for%20Formation%20Flight%20of%20Fixed-Wing%0A%20%20Drones&body=Title%3A%20Distributed%20Oscillatory%20Guidance%20for%20Formation%20Flight%20of%20Fixed-Wing%0A%20%20Drones%0AAuthor%3A%20Yang%20Xu%20and%20Jes%C3%BAs%20Bautista%20and%20Jos%C3%A9%20Hinojosa%20and%20H%C3%A9ctor%20Garc%C3%ADa%20de%20Marina%0AAbstract%3A%20%20%20The%20autonomous%20formation%20flight%20of%20fixed-wing%20drones%20is%20hard%20when%20the%0Acoordination%20requires%20the%20actuation%20over%20their%20speeds%20since%20they%20are%20critically%0Abounded%20and%20aircraft%20are%20mostly%20designed%20to%20fly%20at%20a%20nominal%20airspeed.%20This%0Apaper%20proposes%20an%20algorithm%20to%20achieve%20formation%20flights%20of%20fixed-wing%20drones%0Awithout%20requiring%20any%20actuation%20over%20their%20speed.%20In%20particular%2C%20we%20guide%20all%0Athe%20drones%20to%20travel%20over%20specific%20paths%2C%20e.g.%2C%20parallel%20straight%20lines%2C%20and%20we%0Asuperpose%20an%20oscillatory%20behavior%20onto%20the%20guiding%20vector%20field%20that%20drives%20the%0Adrones%20to%20the%20paths.%20This%20oscillation%20enables%20control%20over%20the%20average%20velocity%0Aalong%20the%20path%2C%20thereby%20facilitating%20inter-drone%20coordination.%20Each%20drone%0Aadjusts%20its%20oscillation%20amplitude%20distributively%20in%20a%20closed-loop%20manner%20by%0Acommunicating%20with%20neighboring%20agents%20in%20an%20undirected%20and%20connected%20graph.%20A%0Anovel%20consensus%20algorithm%20is%20introduced%2C%20leveraging%20a%20non-negative%2C%20asymmetric%0Asaturation%20function.%20This%20unconventional%20saturation%20is%20justified%20since%20negative%0Aamplitudes%20do%20not%20make%20drones%20travel%20backward%20or%20have%20a%20negative%20velocity%20along%0Athe%20path.%20Rigorous%20theoretical%20analysis%20of%20the%20algorithm%20is%20complemented%20by%0Avalidation%20through%20numerical%20simulations%20and%20a%20real-world%20formation%20flight.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Oscillatory%2520Guidance%2520for%2520Formation%2520Flight%2520of%2520Fixed-Wing%250A%2520%2520Drones%26entry.906535625%3DYang%2520Xu%2520and%2520Jes%25C3%25BAs%2520Bautista%2520and%2520Jos%25C3%25A9%2520Hinojosa%2520and%2520H%25C3%25A9ctor%2520Garc%25C3%25ADa%2520de%2520Marina%26entry.1292438233%3D%2520%2520The%2520autonomous%2520formation%2520flight%2520of%2520fixed-wing%2520drones%2520is%2520hard%2520when%2520the%250Acoordination%2520requires%2520the%2520actuation%2520over%2520their%2520speeds%2520since%2520they%2520are%2520critically%250Abounded%2520and%2520aircraft%2520are%2520mostly%2520designed%2520to%2520fly%2520at%2520a%2520nominal%2520airspeed.%2520This%250Apaper%2520proposes%2520an%2520algorithm%2520to%2520achieve%2520formation%2520flights%2520of%2520fixed-wing%2520drones%250Awithout%2520requiring%2520any%2520actuation%2520over%2520their%2520speed.%2520In%2520particular%252C%2520we%2520guide%2520all%250Athe%2520drones%2520to%2520travel%2520over%2520specific%2520paths%252C%2520e.g.%252C%2520parallel%2520straight%2520lines%252C%2520and%2520we%250Asuperpose%2520an%2520oscillatory%2520behavior%2520onto%2520the%2520guiding%2520vector%2520field%2520that%2520drives%2520the%250Adrones%2520to%2520the%2520paths.%2520This%2520oscillation%2520enables%2520control%2520over%2520the%2520average%2520velocity%250Aalong%2520the%2520path%252C%2520thereby%2520facilitating%2520inter-drone%2520coordination.%2520Each%2520drone%250Aadjusts%2520its%2520oscillation%2520amplitude%2520distributively%2520in%2520a%2520closed-loop%2520manner%2520by%250Acommunicating%2520with%2520neighboring%2520agents%2520in%2520an%2520undirected%2520and%2520connected%2520graph.%2520A%250Anovel%2520consensus%2520algorithm%2520is%2520introduced%252C%2520leveraging%2520a%2520non-negative%252C%2520asymmetric%250Asaturation%2520function.%2520This%2520unconventional%2520saturation%2520is%2520justified%2520since%2520negative%250Aamplitudes%2520do%2520not%2520make%2520drones%2520travel%2520backward%2520or%2520have%2520a%2520negative%2520velocity%2520along%250Athe%2520path.%2520Rigorous%2520theoretical%2520analysis%2520of%2520the%2520algorithm%2520is%2520complemented%2520by%250Avalidation%2520through%2520numerical%2520simulations%2520and%2520a%2520real-world%2520formation%2520flight.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Oscillatory%20Guidance%20for%20Formation%20Flight%20of%20Fixed-Wing%0A%20%20Drones&entry.906535625=Yang%20Xu%20and%20Jes%C3%BAs%20Bautista%20and%20Jos%C3%A9%20Hinojosa%20and%20H%C3%A9ctor%20Garc%C3%ADa%20de%20Marina&entry.1292438233=%20%20The%20autonomous%20formation%20flight%20of%20fixed-wing%20drones%20is%20hard%20when%20the%0Acoordination%20requires%20the%20actuation%20over%20their%20speeds%20since%20they%20are%20critically%0Abounded%20and%20aircraft%20are%20mostly%20designed%20to%20fly%20at%20a%20nominal%20airspeed.%20This%0Apaper%20proposes%20an%20algorithm%20to%20achieve%20formation%20flights%20of%20fixed-wing%20drones%0Awithout%20requiring%20any%20actuation%20over%20their%20speed.%20In%20particular%2C%20we%20guide%20all%0Athe%20drones%20to%20travel%20over%20specific%20paths%2C%20e.g.%2C%20parallel%20straight%20lines%2C%20and%20we%0Asuperpose%20an%20oscillatory%20behavior%20onto%20the%20guiding%20vector%20field%20that%20drives%20the%0Adrones%20to%20the%20paths.%20This%20oscillation%20enables%20control%20over%20the%20average%20velocity%0Aalong%20the%20path%2C%20thereby%20facilitating%20inter-drone%20coordination.%20Each%20drone%0Aadjusts%20its%20oscillation%20amplitude%20distributively%20in%20a%20closed-loop%20manner%20by%0Acommunicating%20with%20neighboring%20agents%20in%20an%20undirected%20and%20connected%20graph.%20A%0Anovel%20consensus%20algorithm%20is%20introduced%2C%20leveraging%20a%20non-negative%2C%20asymmetric%0Asaturation%20function.%20This%20unconventional%20saturation%20is%20justified%20since%20negative%0Aamplitudes%20do%20not%20make%20drones%20travel%20backward%20or%20have%20a%20negative%20velocity%20along%0Athe%20path.%20Rigorous%20theoretical%20analysis%20of%20the%20algorithm%20is%20complemented%20by%0Avalidation%20through%20numerical%20simulations%20and%20a%20real-world%20formation%20flight.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16458v1&entry.124074799=Read"},
{"title": "Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos\n  Networks", "author": "Marcel Kleinmann and Shashank Agnihotri and Margret Keuper", "abstract": "  Faithfulness and interpretability are essential for deploying deep neural\nnetworks (DNNs) in safety-critical domains such as medical imaging. B-cos\nnetworks offer a promising solution by replacing standard linear layers with a\nweight-input alignment mechanism, producing inherently interpretable,\nclass-specific explanations without post-hoc methods. While maintaining\ndiagnostic performance competitive with state-of-the-art DNNs, standard B-cos\nmodels suffer from severe aliasing artifacts in their explanation maps, making\nthem unsuitable for clinical use where clarity is essential. Additionally, the\noriginal B-cos formulation is limited to multi-class settings, whereas chest\nX-ray analysis often requires multi-label classification due to co-occurring\nabnormalities. In this work, we address both limitations: (1) we introduce\nanti-aliasing strategies using FLCPooling (FLC) and BlurPool (BP) to\nsignificantly improve explanation quality, and (2) we extend B-cos networks to\nsupport multi-label classification. Our experiments on chest X-ray datasets\ndemonstrate that the modified $\\text{B-cos}_\\text{FLC}$ and\n$\\text{B-cos}_\\text{BP}$ preserve strong predictive performance while providing\nfaithful and artifact-free explanations suitable for clinical application in\nmulti-label settings. Code available at:\n$\\href{https://github.com/mkleinma/B-cos-medical-paper}{GitHub repository}$.\n", "link": "http://arxiv.org/abs/2507.16761v1", "date": "2025-07-22", "relevancy": 1.4711, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5001}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4801}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faithful%2C%20Interpretable%20Chest%20X-ray%20Diagnosis%20with%20Anti-Aliased%20B-cos%0A%20%20Networks&body=Title%3A%20Faithful%2C%20Interpretable%20Chest%20X-ray%20Diagnosis%20with%20Anti-Aliased%20B-cos%0A%20%20Networks%0AAuthor%3A%20Marcel%20Kleinmann%20and%20Shashank%20Agnihotri%20and%20Margret%20Keuper%0AAbstract%3A%20%20%20Faithfulness%20and%20interpretability%20are%20essential%20for%20deploying%20deep%20neural%0Anetworks%20%28DNNs%29%20in%20safety-critical%20domains%20such%20as%20medical%20imaging.%20B-cos%0Anetworks%20offer%20a%20promising%20solution%20by%20replacing%20standard%20linear%20layers%20with%20a%0Aweight-input%20alignment%20mechanism%2C%20producing%20inherently%20interpretable%2C%0Aclass-specific%20explanations%20without%20post-hoc%20methods.%20While%20maintaining%0Adiagnostic%20performance%20competitive%20with%20state-of-the-art%20DNNs%2C%20standard%20B-cos%0Amodels%20suffer%20from%20severe%20aliasing%20artifacts%20in%20their%20explanation%20maps%2C%20making%0Athem%20unsuitable%20for%20clinical%20use%20where%20clarity%20is%20essential.%20Additionally%2C%20the%0Aoriginal%20B-cos%20formulation%20is%20limited%20to%20multi-class%20settings%2C%20whereas%20chest%0AX-ray%20analysis%20often%20requires%20multi-label%20classification%20due%20to%20co-occurring%0Aabnormalities.%20In%20this%20work%2C%20we%20address%20both%20limitations%3A%20%281%29%20we%20introduce%0Aanti-aliasing%20strategies%20using%20FLCPooling%20%28FLC%29%20and%20BlurPool%20%28BP%29%20to%0Asignificantly%20improve%20explanation%20quality%2C%20and%20%282%29%20we%20extend%20B-cos%20networks%20to%0Asupport%20multi-label%20classification.%20Our%20experiments%20on%20chest%20X-ray%20datasets%0Ademonstrate%20that%20the%20modified%20%24%5Ctext%7BB-cos%7D_%5Ctext%7BFLC%7D%24%20and%0A%24%5Ctext%7BB-cos%7D_%5Ctext%7BBP%7D%24%20preserve%20strong%20predictive%20performance%20while%20providing%0Afaithful%20and%20artifact-free%20explanations%20suitable%20for%20clinical%20application%20in%0Amulti-label%20settings.%20Code%20available%20at%3A%0A%24%5Chref%7Bhttps%3A//github.com/mkleinma/B-cos-medical-paper%7D%7BGitHub%20repository%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaithful%252C%2520Interpretable%2520Chest%2520X-ray%2520Diagnosis%2520with%2520Anti-Aliased%2520B-cos%250A%2520%2520Networks%26entry.906535625%3DMarcel%2520Kleinmann%2520and%2520Shashank%2520Agnihotri%2520and%2520Margret%2520Keuper%26entry.1292438233%3D%2520%2520Faithfulness%2520and%2520interpretability%2520are%2520essential%2520for%2520deploying%2520deep%2520neural%250Anetworks%2520%2528DNNs%2529%2520in%2520safety-critical%2520domains%2520such%2520as%2520medical%2520imaging.%2520B-cos%250Anetworks%2520offer%2520a%2520promising%2520solution%2520by%2520replacing%2520standard%2520linear%2520layers%2520with%2520a%250Aweight-input%2520alignment%2520mechanism%252C%2520producing%2520inherently%2520interpretable%252C%250Aclass-specific%2520explanations%2520without%2520post-hoc%2520methods.%2520While%2520maintaining%250Adiagnostic%2520performance%2520competitive%2520with%2520state-of-the-art%2520DNNs%252C%2520standard%2520B-cos%250Amodels%2520suffer%2520from%2520severe%2520aliasing%2520artifacts%2520in%2520their%2520explanation%2520maps%252C%2520making%250Athem%2520unsuitable%2520for%2520clinical%2520use%2520where%2520clarity%2520is%2520essential.%2520Additionally%252C%2520the%250Aoriginal%2520B-cos%2520formulation%2520is%2520limited%2520to%2520multi-class%2520settings%252C%2520whereas%2520chest%250AX-ray%2520analysis%2520often%2520requires%2520multi-label%2520classification%2520due%2520to%2520co-occurring%250Aabnormalities.%2520In%2520this%2520work%252C%2520we%2520address%2520both%2520limitations%253A%2520%25281%2529%2520we%2520introduce%250Aanti-aliasing%2520strategies%2520using%2520FLCPooling%2520%2528FLC%2529%2520and%2520BlurPool%2520%2528BP%2529%2520to%250Asignificantly%2520improve%2520explanation%2520quality%252C%2520and%2520%25282%2529%2520we%2520extend%2520B-cos%2520networks%2520to%250Asupport%2520multi-label%2520classification.%2520Our%2520experiments%2520on%2520chest%2520X-ray%2520datasets%250Ademonstrate%2520that%2520the%2520modified%2520%2524%255Ctext%257BB-cos%257D_%255Ctext%257BFLC%257D%2524%2520and%250A%2524%255Ctext%257BB-cos%257D_%255Ctext%257BBP%257D%2524%2520preserve%2520strong%2520predictive%2520performance%2520while%2520providing%250Afaithful%2520and%2520artifact-free%2520explanations%2520suitable%2520for%2520clinical%2520application%2520in%250Amulti-label%2520settings.%2520Code%2520available%2520at%253A%250A%2524%255Chref%257Bhttps%253A//github.com/mkleinma/B-cos-medical-paper%257D%257BGitHub%2520repository%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faithful%2C%20Interpretable%20Chest%20X-ray%20Diagnosis%20with%20Anti-Aliased%20B-cos%0A%20%20Networks&entry.906535625=Marcel%20Kleinmann%20and%20Shashank%20Agnihotri%20and%20Margret%20Keuper&entry.1292438233=%20%20Faithfulness%20and%20interpretability%20are%20essential%20for%20deploying%20deep%20neural%0Anetworks%20%28DNNs%29%20in%20safety-critical%20domains%20such%20as%20medical%20imaging.%20B-cos%0Anetworks%20offer%20a%20promising%20solution%20by%20replacing%20standard%20linear%20layers%20with%20a%0Aweight-input%20alignment%20mechanism%2C%20producing%20inherently%20interpretable%2C%0Aclass-specific%20explanations%20without%20post-hoc%20methods.%20While%20maintaining%0Adiagnostic%20performance%20competitive%20with%20state-of-the-art%20DNNs%2C%20standard%20B-cos%0Amodels%20suffer%20from%20severe%20aliasing%20artifacts%20in%20their%20explanation%20maps%2C%20making%0Athem%20unsuitable%20for%20clinical%20use%20where%20clarity%20is%20essential.%20Additionally%2C%20the%0Aoriginal%20B-cos%20formulation%20is%20limited%20to%20multi-class%20settings%2C%20whereas%20chest%0AX-ray%20analysis%20often%20requires%20multi-label%20classification%20due%20to%20co-occurring%0Aabnormalities.%20In%20this%20work%2C%20we%20address%20both%20limitations%3A%20%281%29%20we%20introduce%0Aanti-aliasing%20strategies%20using%20FLCPooling%20%28FLC%29%20and%20BlurPool%20%28BP%29%20to%0Asignificantly%20improve%20explanation%20quality%2C%20and%20%282%29%20we%20extend%20B-cos%20networks%20to%0Asupport%20multi-label%20classification.%20Our%20experiments%20on%20chest%20X-ray%20datasets%0Ademonstrate%20that%20the%20modified%20%24%5Ctext%7BB-cos%7D_%5Ctext%7BFLC%7D%24%20and%0A%24%5Ctext%7BB-cos%7D_%5Ctext%7BBP%7D%24%20preserve%20strong%20predictive%20performance%20while%20providing%0Afaithful%20and%20artifact-free%20explanations%20suitable%20for%20clinical%20application%20in%0Amulti-label%20settings.%20Code%20available%20at%3A%0A%24%5Chref%7Bhttps%3A//github.com/mkleinma/B-cos-medical-paper%7D%7BGitHub%20repository%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16761v1&entry.124074799=Read"},
{"title": "Families of Optimal Transport Kernels for Cell Complexes", "author": "Rahul Khorana", "abstract": "  Recent advances have discussed cell complexes as ideal learning\nrepresentations. However, there is a lack of available machine learning methods\nsuitable for learning on CW complexes. In this paper, we derive an explicit\nexpression for the Wasserstein distance between cell complex signal\ndistributions in terms of a Hodge-Laplacian matrix. This leads to a\nstructurally meaningful measure to compare CW complexes and define the optimal\ntransportation map. In order to simultaneously include both feature and\nstructure information, we extend the Fused Gromov-Wasserstein distance to CW\ncomplexes. Finally, we introduce novel kernels over the space of probability\nmeasures on CW complexes based on the dual formulation of optimal transport.\n", "link": "http://arxiv.org/abs/2507.16569v1", "date": "2025-07-22", "relevancy": 1.6954, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4217}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Families%20of%20Optimal%20Transport%20Kernels%20for%20Cell%20Complexes&body=Title%3A%20Families%20of%20Optimal%20Transport%20Kernels%20for%20Cell%20Complexes%0AAuthor%3A%20Rahul%20Khorana%0AAbstract%3A%20%20%20Recent%20advances%20have%20discussed%20cell%20complexes%20as%20ideal%20learning%0Arepresentations.%20However%2C%20there%20is%20a%20lack%20of%20available%20machine%20learning%20methods%0Asuitable%20for%20learning%20on%20CW%20complexes.%20In%20this%20paper%2C%20we%20derive%20an%20explicit%0Aexpression%20for%20the%20Wasserstein%20distance%20between%20cell%20complex%20signal%0Adistributions%20in%20terms%20of%20a%20Hodge-Laplacian%20matrix.%20This%20leads%20to%20a%0Astructurally%20meaningful%20measure%20to%20compare%20CW%20complexes%20and%20define%20the%20optimal%0Atransportation%20map.%20In%20order%20to%20simultaneously%20include%20both%20feature%20and%0Astructure%20information%2C%20we%20extend%20the%20Fused%20Gromov-Wasserstein%20distance%20to%20CW%0Acomplexes.%20Finally%2C%20we%20introduce%20novel%20kernels%20over%20the%20space%20of%20probability%0Ameasures%20on%20CW%20complexes%20based%20on%20the%20dual%20formulation%20of%20optimal%20transport.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFamilies%2520of%2520Optimal%2520Transport%2520Kernels%2520for%2520Cell%2520Complexes%26entry.906535625%3DRahul%2520Khorana%26entry.1292438233%3D%2520%2520Recent%2520advances%2520have%2520discussed%2520cell%2520complexes%2520as%2520ideal%2520learning%250Arepresentations.%2520However%252C%2520there%2520is%2520a%2520lack%2520of%2520available%2520machine%2520learning%2520methods%250Asuitable%2520for%2520learning%2520on%2520CW%2520complexes.%2520In%2520this%2520paper%252C%2520we%2520derive%2520an%2520explicit%250Aexpression%2520for%2520the%2520Wasserstein%2520distance%2520between%2520cell%2520complex%2520signal%250Adistributions%2520in%2520terms%2520of%2520a%2520Hodge-Laplacian%2520matrix.%2520This%2520leads%2520to%2520a%250Astructurally%2520meaningful%2520measure%2520to%2520compare%2520CW%2520complexes%2520and%2520define%2520the%2520optimal%250Atransportation%2520map.%2520In%2520order%2520to%2520simultaneously%2520include%2520both%2520feature%2520and%250Astructure%2520information%252C%2520we%2520extend%2520the%2520Fused%2520Gromov-Wasserstein%2520distance%2520to%2520CW%250Acomplexes.%2520Finally%252C%2520we%2520introduce%2520novel%2520kernels%2520over%2520the%2520space%2520of%2520probability%250Ameasures%2520on%2520CW%2520complexes%2520based%2520on%2520the%2520dual%2520formulation%2520of%2520optimal%2520transport.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Families%20of%20Optimal%20Transport%20Kernels%20for%20Cell%20Complexes&entry.906535625=Rahul%20Khorana&entry.1292438233=%20%20Recent%20advances%20have%20discussed%20cell%20complexes%20as%20ideal%20learning%0Arepresentations.%20However%2C%20there%20is%20a%20lack%20of%20available%20machine%20learning%20methods%0Asuitable%20for%20learning%20on%20CW%20complexes.%20In%20this%20paper%2C%20we%20derive%20an%20explicit%0Aexpression%20for%20the%20Wasserstein%20distance%20between%20cell%20complex%20signal%0Adistributions%20in%20terms%20of%20a%20Hodge-Laplacian%20matrix.%20This%20leads%20to%20a%0Astructurally%20meaningful%20measure%20to%20compare%20CW%20complexes%20and%20define%20the%20optimal%0Atransportation%20map.%20In%20order%20to%20simultaneously%20include%20both%20feature%20and%0Astructure%20information%2C%20we%20extend%20the%20Fused%20Gromov-Wasserstein%20distance%20to%20CW%0Acomplexes.%20Finally%2C%20we%20introduce%20novel%20kernels%20over%20the%20space%20of%20probability%0Ameasures%20on%20CW%20complexes%20based%20on%20the%20dual%20formulation%20of%20optimal%20transport.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16569v1&entry.124074799=Read"},
{"title": "Towards provable probabilistic safety for scalable embodied AI systems", "author": "Linxuan He and Qing-Shan Jia and Ang Li and Hongyan Sang and Ling Wang and Jiwen Lu and Tao Zhang and Jie Zhou and Yi Zhang and Yisen Wang and Peng Wei and Zhongyuan Wang and Henry X. Liu and Shuo Feng", "abstract": "  Embodied AI systems, comprising AI models and physical plants, are\nincreasingly prevalent across various applications. Due to the rarity of system\nfailures, ensuring their safety in complex operating environments remains a\nmajor challenge, which severely hinders their large-scale deployment in\nsafety-critical domains, such as autonomous vehicles, medical devices, and\nrobotics. While achieving provable deterministic safety--verifying system\nsafety across all possible scenarios--remains theoretically ideal, the rarity\nand complexity of corner cases make this approach impractical for scalable\nembodied AI systems. Instead, empirical safety evaluation is employed as an\nalternative, but the absence of provable guarantees imposes significant\nlimitations. To address these issues, we argue for a paradigm shift to provable\nprobabilistic safety that integrates provable guarantees with progressive\nachievement toward a probabilistic safety boundary on overall system\nperformance. The new paradigm better leverages statistical methods to enhance\nfeasibility and scalability, and a well-defined probabilistic safety boundary\nenables embodied AI systems to be deployed at scale. In this Perspective, we\noutline a roadmap for provable probabilistic safety, along with corresponding\nchallenges and potential solutions. By bridging the gap between theoretical\nsafety assurance and practical deployment, this Perspective offers a pathway\ntoward safer, large-scale adoption of embodied AI systems in safety-critical\napplications.\n", "link": "http://arxiv.org/abs/2506.05171v2", "date": "2025-07-22", "relevancy": 1.5122, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5671}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4899}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20provable%20probabilistic%20safety%20for%20scalable%20embodied%20AI%20systems&body=Title%3A%20Towards%20provable%20probabilistic%20safety%20for%20scalable%20embodied%20AI%20systems%0AAuthor%3A%20Linxuan%20He%20and%20Qing-Shan%20Jia%20and%20Ang%20Li%20and%20Hongyan%20Sang%20and%20Ling%20Wang%20and%20Jiwen%20Lu%20and%20Tao%20Zhang%20and%20Jie%20Zhou%20and%20Yi%20Zhang%20and%20Yisen%20Wang%20and%20Peng%20Wei%20and%20Zhongyuan%20Wang%20and%20Henry%20X.%20Liu%20and%20Shuo%20Feng%0AAbstract%3A%20%20%20Embodied%20AI%20systems%2C%20comprising%20AI%20models%20and%20physical%20plants%2C%20are%0Aincreasingly%20prevalent%20across%20various%20applications.%20Due%20to%20the%20rarity%20of%20system%0Afailures%2C%20ensuring%20their%20safety%20in%20complex%20operating%20environments%20remains%20a%0Amajor%20challenge%2C%20which%20severely%20hinders%20their%20large-scale%20deployment%20in%0Asafety-critical%20domains%2C%20such%20as%20autonomous%20vehicles%2C%20medical%20devices%2C%20and%0Arobotics.%20While%20achieving%20provable%20deterministic%20safety--verifying%20system%0Asafety%20across%20all%20possible%20scenarios--remains%20theoretically%20ideal%2C%20the%20rarity%0Aand%20complexity%20of%20corner%20cases%20make%20this%20approach%20impractical%20for%20scalable%0Aembodied%20AI%20systems.%20Instead%2C%20empirical%20safety%20evaluation%20is%20employed%20as%20an%0Aalternative%2C%20but%20the%20absence%20of%20provable%20guarantees%20imposes%20significant%0Alimitations.%20To%20address%20these%20issues%2C%20we%20argue%20for%20a%20paradigm%20shift%20to%20provable%0Aprobabilistic%20safety%20that%20integrates%20provable%20guarantees%20with%20progressive%0Aachievement%20toward%20a%20probabilistic%20safety%20boundary%20on%20overall%20system%0Aperformance.%20The%20new%20paradigm%20better%20leverages%20statistical%20methods%20to%20enhance%0Afeasibility%20and%20scalability%2C%20and%20a%20well-defined%20probabilistic%20safety%20boundary%0Aenables%20embodied%20AI%20systems%20to%20be%20deployed%20at%20scale.%20In%20this%20Perspective%2C%20we%0Aoutline%20a%20roadmap%20for%20provable%20probabilistic%20safety%2C%20along%20with%20corresponding%0Achallenges%20and%20potential%20solutions.%20By%20bridging%20the%20gap%20between%20theoretical%0Asafety%20assurance%20and%20practical%20deployment%2C%20this%20Perspective%20offers%20a%20pathway%0Atoward%20safer%2C%20large-scale%20adoption%20of%20embodied%20AI%20systems%20in%20safety-critical%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.05171v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520provable%2520probabilistic%2520safety%2520for%2520scalable%2520embodied%2520AI%2520systems%26entry.906535625%3DLinxuan%2520He%2520and%2520Qing-Shan%2520Jia%2520and%2520Ang%2520Li%2520and%2520Hongyan%2520Sang%2520and%2520Ling%2520Wang%2520and%2520Jiwen%2520Lu%2520and%2520Tao%2520Zhang%2520and%2520Jie%2520Zhou%2520and%2520Yi%2520Zhang%2520and%2520Yisen%2520Wang%2520and%2520Peng%2520Wei%2520and%2520Zhongyuan%2520Wang%2520and%2520Henry%2520X.%2520Liu%2520and%2520Shuo%2520Feng%26entry.1292438233%3D%2520%2520Embodied%2520AI%2520systems%252C%2520comprising%2520AI%2520models%2520and%2520physical%2520plants%252C%2520are%250Aincreasingly%2520prevalent%2520across%2520various%2520applications.%2520Due%2520to%2520the%2520rarity%2520of%2520system%250Afailures%252C%2520ensuring%2520their%2520safety%2520in%2520complex%2520operating%2520environments%2520remains%2520a%250Amajor%2520challenge%252C%2520which%2520severely%2520hinders%2520their%2520large-scale%2520deployment%2520in%250Asafety-critical%2520domains%252C%2520such%2520as%2520autonomous%2520vehicles%252C%2520medical%2520devices%252C%2520and%250Arobotics.%2520While%2520achieving%2520provable%2520deterministic%2520safety--verifying%2520system%250Asafety%2520across%2520all%2520possible%2520scenarios--remains%2520theoretically%2520ideal%252C%2520the%2520rarity%250Aand%2520complexity%2520of%2520corner%2520cases%2520make%2520this%2520approach%2520impractical%2520for%2520scalable%250Aembodied%2520AI%2520systems.%2520Instead%252C%2520empirical%2520safety%2520evaluation%2520is%2520employed%2520as%2520an%250Aalternative%252C%2520but%2520the%2520absence%2520of%2520provable%2520guarantees%2520imposes%2520significant%250Alimitations.%2520To%2520address%2520these%2520issues%252C%2520we%2520argue%2520for%2520a%2520paradigm%2520shift%2520to%2520provable%250Aprobabilistic%2520safety%2520that%2520integrates%2520provable%2520guarantees%2520with%2520progressive%250Aachievement%2520toward%2520a%2520probabilistic%2520safety%2520boundary%2520on%2520overall%2520system%250Aperformance.%2520The%2520new%2520paradigm%2520better%2520leverages%2520statistical%2520methods%2520to%2520enhance%250Afeasibility%2520and%2520scalability%252C%2520and%2520a%2520well-defined%2520probabilistic%2520safety%2520boundary%250Aenables%2520embodied%2520AI%2520systems%2520to%2520be%2520deployed%2520at%2520scale.%2520In%2520this%2520Perspective%252C%2520we%250Aoutline%2520a%2520roadmap%2520for%2520provable%2520probabilistic%2520safety%252C%2520along%2520with%2520corresponding%250Achallenges%2520and%2520potential%2520solutions.%2520By%2520bridging%2520the%2520gap%2520between%2520theoretical%250Asafety%2520assurance%2520and%2520practical%2520deployment%252C%2520this%2520Perspective%2520offers%2520a%2520pathway%250Atoward%2520safer%252C%2520large-scale%2520adoption%2520of%2520embodied%2520AI%2520systems%2520in%2520safety-critical%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05171v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20provable%20probabilistic%20safety%20for%20scalable%20embodied%20AI%20systems&entry.906535625=Linxuan%20He%20and%20Qing-Shan%20Jia%20and%20Ang%20Li%20and%20Hongyan%20Sang%20and%20Ling%20Wang%20and%20Jiwen%20Lu%20and%20Tao%20Zhang%20and%20Jie%20Zhou%20and%20Yi%20Zhang%20and%20Yisen%20Wang%20and%20Peng%20Wei%20and%20Zhongyuan%20Wang%20and%20Henry%20X.%20Liu%20and%20Shuo%20Feng&entry.1292438233=%20%20Embodied%20AI%20systems%2C%20comprising%20AI%20models%20and%20physical%20plants%2C%20are%0Aincreasingly%20prevalent%20across%20various%20applications.%20Due%20to%20the%20rarity%20of%20system%0Afailures%2C%20ensuring%20their%20safety%20in%20complex%20operating%20environments%20remains%20a%0Amajor%20challenge%2C%20which%20severely%20hinders%20their%20large-scale%20deployment%20in%0Asafety-critical%20domains%2C%20such%20as%20autonomous%20vehicles%2C%20medical%20devices%2C%20and%0Arobotics.%20While%20achieving%20provable%20deterministic%20safety--verifying%20system%0Asafety%20across%20all%20possible%20scenarios--remains%20theoretically%20ideal%2C%20the%20rarity%0Aand%20complexity%20of%20corner%20cases%20make%20this%20approach%20impractical%20for%20scalable%0Aembodied%20AI%20systems.%20Instead%2C%20empirical%20safety%20evaluation%20is%20employed%20as%20an%0Aalternative%2C%20but%20the%20absence%20of%20provable%20guarantees%20imposes%20significant%0Alimitations.%20To%20address%20these%20issues%2C%20we%20argue%20for%20a%20paradigm%20shift%20to%20provable%0Aprobabilistic%20safety%20that%20integrates%20provable%20guarantees%20with%20progressive%0Aachievement%20toward%20a%20probabilistic%20safety%20boundary%20on%20overall%20system%0Aperformance.%20The%20new%20paradigm%20better%20leverages%20statistical%20methods%20to%20enhance%0Afeasibility%20and%20scalability%2C%20and%20a%20well-defined%20probabilistic%20safety%20boundary%0Aenables%20embodied%20AI%20systems%20to%20be%20deployed%20at%20scale.%20In%20this%20Perspective%2C%20we%0Aoutline%20a%20roadmap%20for%20provable%20probabilistic%20safety%2C%20along%20with%20corresponding%0Achallenges%20and%20potential%20solutions.%20By%20bridging%20the%20gap%20between%20theoretical%0Asafety%20assurance%20and%20practical%20deployment%2C%20this%20Perspective%20offers%20a%20pathway%0Atoward%20safer%2C%20large-scale%20adoption%20of%20embodied%20AI%20systems%20in%20safety-critical%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.05171v2&entry.124074799=Read"},
{"title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "author": "Yichen Huang and Lin F. Yang", "abstract": "  The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly (up to a caveat discussed below). This result underscores\nthe importance of developing optimal strategies to harness the full potential\nof powerful LLMs for complex reasoning tasks.\n", "link": "http://arxiv.org/abs/2507.15855v2", "date": "2025-07-22", "relevancy": 1.5664, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3916}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3916}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemini%202.5%20Pro%20Capable%20of%20Winning%20Gold%20at%20IMO%202025&body=Title%3A%20Gemini%202.5%20Pro%20Capable%20of%20Winning%20Gold%20at%20IMO%202025%0AAuthor%3A%20Yichen%20Huang%20and%20Lin%20F.%20Yang%0AAbstract%3A%20%20%20The%20International%20Mathematical%20Olympiad%20%28IMO%29%20poses%20uniquely%20challenging%0Aproblems%20requiring%20deep%20insight%2C%20creativity%2C%20and%20formal%20reasoning.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20perform%20well%20on%20mathematical%20benchmarks%20like%20AIME%2C%20they%0Astruggle%20with%20Olympiad-level%20tasks.%20We%20use%20Google%27s%20Gemini%202.5%20Pro%20on%20the%20newly%0Areleased%20IMO%202025%20problems%2C%20avoiding%20data%20contamination.%20Using%20a%0Aself-verification%20pipeline%20with%20careful%20prompt%20design%2C%205%20%28out%20of%206%29%20problems%0Aare%20solved%20correctly%20%28up%20to%20a%20caveat%20discussed%20below%29.%20This%20result%20underscores%0Athe%20importance%20of%20developing%20optimal%20strategies%20to%20harness%20the%20full%20potential%0Aof%20powerful%20LLMs%20for%20complex%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.15855v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemini%25202.5%2520Pro%2520Capable%2520of%2520Winning%2520Gold%2520at%2520IMO%25202025%26entry.906535625%3DYichen%2520Huang%2520and%2520Lin%2520F.%2520Yang%26entry.1292438233%3D%2520%2520The%2520International%2520Mathematical%2520Olympiad%2520%2528IMO%2529%2520poses%2520uniquely%2520challenging%250Aproblems%2520requiring%2520deep%2520insight%252C%2520creativity%252C%2520and%2520formal%2520reasoning.%2520While%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520perform%2520well%2520on%2520mathematical%2520benchmarks%2520like%2520AIME%252C%2520they%250Astruggle%2520with%2520Olympiad-level%2520tasks.%2520We%2520use%2520Google%2527s%2520Gemini%25202.5%2520Pro%2520on%2520the%2520newly%250Areleased%2520IMO%25202025%2520problems%252C%2520avoiding%2520data%2520contamination.%2520Using%2520a%250Aself-verification%2520pipeline%2520with%2520careful%2520prompt%2520design%252C%25205%2520%2528out%2520of%25206%2529%2520problems%250Aare%2520solved%2520correctly%2520%2528up%2520to%2520a%2520caveat%2520discussed%2520below%2529.%2520This%2520result%2520underscores%250Athe%2520importance%2520of%2520developing%2520optimal%2520strategies%2520to%2520harness%2520the%2520full%2520potential%250Aof%2520powerful%2520LLMs%2520for%2520complex%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15855v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemini%202.5%20Pro%20Capable%20of%20Winning%20Gold%20at%20IMO%202025&entry.906535625=Yichen%20Huang%20and%20Lin%20F.%20Yang&entry.1292438233=%20%20The%20International%20Mathematical%20Olympiad%20%28IMO%29%20poses%20uniquely%20challenging%0Aproblems%20requiring%20deep%20insight%2C%20creativity%2C%20and%20formal%20reasoning.%20While%20Large%0ALanguage%20Models%20%28LLMs%29%20perform%20well%20on%20mathematical%20benchmarks%20like%20AIME%2C%20they%0Astruggle%20with%20Olympiad-level%20tasks.%20We%20use%20Google%27s%20Gemini%202.5%20Pro%20on%20the%20newly%0Areleased%20IMO%202025%20problems%2C%20avoiding%20data%20contamination.%20Using%20a%0Aself-verification%20pipeline%20with%20careful%20prompt%20design%2C%205%20%28out%20of%206%29%20problems%0Aare%20solved%20correctly%20%28up%20to%20a%20caveat%20discussed%20below%29.%20This%20result%20underscores%0Athe%20importance%20of%20developing%20optimal%20strategies%20to%20harness%20the%20full%20potential%0Aof%20powerful%20LLMs%20for%20complex%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.15855v2&entry.124074799=Read"},
{"title": "T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs", "author": "Alireza Dizaji and Benedict Aaron Tjandra and Mehrab Hamidi and Shenyang Huang and Guillaume Rabusseau", "abstract": "  Dynamic graph learning methods have recently emerged as powerful tools for\nmodelling relational data evolving through time. However, despite extensive\nbenchmarking efforts, it remains unclear whether current Temporal Graph Neural\nNetworks (TGNNs) effectively capture core temporal patterns such as\nperiodicity, cause-and-effect, and long-range dependencies. In this work, we\nintroduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set\nof synthetic tasks designed to systematically probe the capabilities of TGNNs\nto reason across time. T-GRAB provides controlled, interpretable tasks that\nisolate key temporal skills: counting/memorizing periodic repetitions,\ninferring delayed causal effects, and capturing long-range dependencies over\nboth spatial and temporal dimensions. We evaluate 11 temporal graph learning\nmethods on these tasks, revealing fundamental shortcomings in their ability to\ngeneralize temporal patterns. Our findings offer actionable insights into the\nlimitations of current models, highlight challenges hidden by traditional\nreal-world benchmarks, and motivate the development of architectures with\nstronger temporal reasoning abilities. The code for T-GRAB can be found at:\nhttps://github.com/alirezadizaji/T-GRAB.\n", "link": "http://arxiv.org/abs/2507.10183v2", "date": "2025-07-22", "relevancy": 1.909, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.469}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-GRAB%3A%20A%20Synthetic%20Diagnostic%20Benchmark%20for%20Learning%20on%20Temporal%20Graphs&body=Title%3A%20T-GRAB%3A%20A%20Synthetic%20Diagnostic%20Benchmark%20for%20Learning%20on%20Temporal%20Graphs%0AAuthor%3A%20Alireza%20Dizaji%20and%20Benedict%20Aaron%20Tjandra%20and%20Mehrab%20Hamidi%20and%20Shenyang%20Huang%20and%20Guillaume%20Rabusseau%0AAbstract%3A%20%20%20Dynamic%20graph%20learning%20methods%20have%20recently%20emerged%20as%20powerful%20tools%20for%0Amodelling%20relational%20data%20evolving%20through%20time.%20However%2C%20despite%20extensive%0Abenchmarking%20efforts%2C%20it%20remains%20unclear%20whether%20current%20Temporal%20Graph%20Neural%0ANetworks%20%28TGNNs%29%20effectively%20capture%20core%20temporal%20patterns%20such%20as%0Aperiodicity%2C%20cause-and-effect%2C%20and%20long-range%20dependencies.%20In%20this%20work%2C%20we%0Aintroduce%20the%20Temporal%20Graph%20Reasoning%20Benchmark%20%28T-GRAB%29%2C%20a%20comprehensive%20set%0Aof%20synthetic%20tasks%20designed%20to%20systematically%20probe%20the%20capabilities%20of%20TGNNs%0Ato%20reason%20across%20time.%20T-GRAB%20provides%20controlled%2C%20interpretable%20tasks%20that%0Aisolate%20key%20temporal%20skills%3A%20counting/memorizing%20periodic%20repetitions%2C%0Ainferring%20delayed%20causal%20effects%2C%20and%20capturing%20long-range%20dependencies%20over%0Aboth%20spatial%20and%20temporal%20dimensions.%20We%20evaluate%2011%20temporal%20graph%20learning%0Amethods%20on%20these%20tasks%2C%20revealing%20fundamental%20shortcomings%20in%20their%20ability%20to%0Ageneralize%20temporal%20patterns.%20Our%20findings%20offer%20actionable%20insights%20into%20the%0Alimitations%20of%20current%20models%2C%20highlight%20challenges%20hidden%20by%20traditional%0Areal-world%20benchmarks%2C%20and%20motivate%20the%20development%20of%20architectures%20with%0Astronger%20temporal%20reasoning%20abilities.%20The%20code%20for%20T-GRAB%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/alirezadizaji/T-GRAB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.10183v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-GRAB%253A%2520A%2520Synthetic%2520Diagnostic%2520Benchmark%2520for%2520Learning%2520on%2520Temporal%2520Graphs%26entry.906535625%3DAlireza%2520Dizaji%2520and%2520Benedict%2520Aaron%2520Tjandra%2520and%2520Mehrab%2520Hamidi%2520and%2520Shenyang%2520Huang%2520and%2520Guillaume%2520Rabusseau%26entry.1292438233%3D%2520%2520Dynamic%2520graph%2520learning%2520methods%2520have%2520recently%2520emerged%2520as%2520powerful%2520tools%2520for%250Amodelling%2520relational%2520data%2520evolving%2520through%2520time.%2520However%252C%2520despite%2520extensive%250Abenchmarking%2520efforts%252C%2520it%2520remains%2520unclear%2520whether%2520current%2520Temporal%2520Graph%2520Neural%250ANetworks%2520%2528TGNNs%2529%2520effectively%2520capture%2520core%2520temporal%2520patterns%2520such%2520as%250Aperiodicity%252C%2520cause-and-effect%252C%2520and%2520long-range%2520dependencies.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520Temporal%2520Graph%2520Reasoning%2520Benchmark%2520%2528T-GRAB%2529%252C%2520a%2520comprehensive%2520set%250Aof%2520synthetic%2520tasks%2520designed%2520to%2520systematically%2520probe%2520the%2520capabilities%2520of%2520TGNNs%250Ato%2520reason%2520across%2520time.%2520T-GRAB%2520provides%2520controlled%252C%2520interpretable%2520tasks%2520that%250Aisolate%2520key%2520temporal%2520skills%253A%2520counting/memorizing%2520periodic%2520repetitions%252C%250Ainferring%2520delayed%2520causal%2520effects%252C%2520and%2520capturing%2520long-range%2520dependencies%2520over%250Aboth%2520spatial%2520and%2520temporal%2520dimensions.%2520We%2520evaluate%252011%2520temporal%2520graph%2520learning%250Amethods%2520on%2520these%2520tasks%252C%2520revealing%2520fundamental%2520shortcomings%2520in%2520their%2520ability%2520to%250Ageneralize%2520temporal%2520patterns.%2520Our%2520findings%2520offer%2520actionable%2520insights%2520into%2520the%250Alimitations%2520of%2520current%2520models%252C%2520highlight%2520challenges%2520hidden%2520by%2520traditional%250Areal-world%2520benchmarks%252C%2520and%2520motivate%2520the%2520development%2520of%2520architectures%2520with%250Astronger%2520temporal%2520reasoning%2520abilities.%2520The%2520code%2520for%2520T-GRAB%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/alirezadizaji/T-GRAB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.10183v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-GRAB%3A%20A%20Synthetic%20Diagnostic%20Benchmark%20for%20Learning%20on%20Temporal%20Graphs&entry.906535625=Alireza%20Dizaji%20and%20Benedict%20Aaron%20Tjandra%20and%20Mehrab%20Hamidi%20and%20Shenyang%20Huang%20and%20Guillaume%20Rabusseau&entry.1292438233=%20%20Dynamic%20graph%20learning%20methods%20have%20recently%20emerged%20as%20powerful%20tools%20for%0Amodelling%20relational%20data%20evolving%20through%20time.%20However%2C%20despite%20extensive%0Abenchmarking%20efforts%2C%20it%20remains%20unclear%20whether%20current%20Temporal%20Graph%20Neural%0ANetworks%20%28TGNNs%29%20effectively%20capture%20core%20temporal%20patterns%20such%20as%0Aperiodicity%2C%20cause-and-effect%2C%20and%20long-range%20dependencies.%20In%20this%20work%2C%20we%0Aintroduce%20the%20Temporal%20Graph%20Reasoning%20Benchmark%20%28T-GRAB%29%2C%20a%20comprehensive%20set%0Aof%20synthetic%20tasks%20designed%20to%20systematically%20probe%20the%20capabilities%20of%20TGNNs%0Ato%20reason%20across%20time.%20T-GRAB%20provides%20controlled%2C%20interpretable%20tasks%20that%0Aisolate%20key%20temporal%20skills%3A%20counting/memorizing%20periodic%20repetitions%2C%0Ainferring%20delayed%20causal%20effects%2C%20and%20capturing%20long-range%20dependencies%20over%0Aboth%20spatial%20and%20temporal%20dimensions.%20We%20evaluate%2011%20temporal%20graph%20learning%0Amethods%20on%20these%20tasks%2C%20revealing%20fundamental%20shortcomings%20in%20their%20ability%20to%0Ageneralize%20temporal%20patterns.%20Our%20findings%20offer%20actionable%20insights%20into%20the%0Alimitations%20of%20current%20models%2C%20highlight%20challenges%20hidden%20by%20traditional%0Areal-world%20benchmarks%2C%20and%20motivate%20the%20development%20of%20architectures%20with%0Astronger%20temporal%20reasoning%20abilities.%20The%20code%20for%20T-GRAB%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/alirezadizaji/T-GRAB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.10183v2&entry.124074799=Read"},
{"title": "Pixel-Resolved Long-Context Learning for Turbulence at Exascale:\n  Resolving Small-scale Eddies Toward the Viscous Limit", "author": "Junqi Yin and Mijanur Palash and M. Paul Laiu and Muralikrishnan Gopalakrishnan Meena and John Gounley and Stephen M. de Bruyn Kops and Feiyi Wang and Ramanan Sankaran and Pei Zhang", "abstract": "  Turbulence plays a crucial role in multiphysics applications, including\naerodynamics, fusion, and combustion. Accurately capturing turbulence's\nmultiscale characteristics is essential for reliable predictions of\nmultiphysics interactions, but remains a grand challenge even for exascale\nsupercomputers and advanced deep learning models. The extreme-resolution data\nrequired to represent turbulence, ranging from billions to trillions of grid\npoints, pose prohibitive computational costs for models based on architectures\nlike vision transformers. To address this challenge, we introduce a multiscale\nhierarchical Turbulence Transformer that reduces sequence length from billions\nto a few millions and a novel RingX sequence parallelism approach that enables\nscalable long-context learning. We perform scaling and science runs on the\nFrontier supercomputer. Our approach demonstrates excellent performance up to\n1.1 EFLOPS on 32,768 AMD GPUs, with a scaling efficiency of 94%. To our\nknowledge, this is the first AI model for turbulence that can capture\nsmall-scale eddies down to the dissipative range.\n", "link": "http://arxiv.org/abs/2507.16697v1", "date": "2025-07-22", "relevancy": 1.5827, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6102}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5239}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel-Resolved%20Long-Context%20Learning%20for%20Turbulence%20at%20Exascale%3A%0A%20%20Resolving%20Small-scale%20Eddies%20Toward%20the%20Viscous%20Limit&body=Title%3A%20Pixel-Resolved%20Long-Context%20Learning%20for%20Turbulence%20at%20Exascale%3A%0A%20%20Resolving%20Small-scale%20Eddies%20Toward%20the%20Viscous%20Limit%0AAuthor%3A%20Junqi%20Yin%20and%20Mijanur%20Palash%20and%20M.%20Paul%20Laiu%20and%20Muralikrishnan%20Gopalakrishnan%20Meena%20and%20John%20Gounley%20and%20Stephen%20M.%20de%20Bruyn%20Kops%20and%20Feiyi%20Wang%20and%20Ramanan%20Sankaran%20and%20Pei%20Zhang%0AAbstract%3A%20%20%20Turbulence%20plays%20a%20crucial%20role%20in%20multiphysics%20applications%2C%20including%0Aaerodynamics%2C%20fusion%2C%20and%20combustion.%20Accurately%20capturing%20turbulence%27s%0Amultiscale%20characteristics%20is%20essential%20for%20reliable%20predictions%20of%0Amultiphysics%20interactions%2C%20but%20remains%20a%20grand%20challenge%20even%20for%20exascale%0Asupercomputers%20and%20advanced%20deep%20learning%20models.%20The%20extreme-resolution%20data%0Arequired%20to%20represent%20turbulence%2C%20ranging%20from%20billions%20to%20trillions%20of%20grid%0Apoints%2C%20pose%20prohibitive%20computational%20costs%20for%20models%20based%20on%20architectures%0Alike%20vision%20transformers.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20multiscale%0Ahierarchical%20Turbulence%20Transformer%20that%20reduces%20sequence%20length%20from%20billions%0Ato%20a%20few%20millions%20and%20a%20novel%20RingX%20sequence%20parallelism%20approach%20that%20enables%0Ascalable%20long-context%20learning.%20We%20perform%20scaling%20and%20science%20runs%20on%20the%0AFrontier%20supercomputer.%20Our%20approach%20demonstrates%20excellent%20performance%20up%20to%0A1.1%20EFLOPS%20on%2032%2C768%20AMD%20GPUs%2C%20with%20a%20scaling%20efficiency%20of%2094%25.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20AI%20model%20for%20turbulence%20that%20can%20capture%0Asmall-scale%20eddies%20down%20to%20the%20dissipative%20range.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel-Resolved%2520Long-Context%2520Learning%2520for%2520Turbulence%2520at%2520Exascale%253A%250A%2520%2520Resolving%2520Small-scale%2520Eddies%2520Toward%2520the%2520Viscous%2520Limit%26entry.906535625%3DJunqi%2520Yin%2520and%2520Mijanur%2520Palash%2520and%2520M.%2520Paul%2520Laiu%2520and%2520Muralikrishnan%2520Gopalakrishnan%2520Meena%2520and%2520John%2520Gounley%2520and%2520Stephen%2520M.%2520de%2520Bruyn%2520Kops%2520and%2520Feiyi%2520Wang%2520and%2520Ramanan%2520Sankaran%2520and%2520Pei%2520Zhang%26entry.1292438233%3D%2520%2520Turbulence%2520plays%2520a%2520crucial%2520role%2520in%2520multiphysics%2520applications%252C%2520including%250Aaerodynamics%252C%2520fusion%252C%2520and%2520combustion.%2520Accurately%2520capturing%2520turbulence%2527s%250Amultiscale%2520characteristics%2520is%2520essential%2520for%2520reliable%2520predictions%2520of%250Amultiphysics%2520interactions%252C%2520but%2520remains%2520a%2520grand%2520challenge%2520even%2520for%2520exascale%250Asupercomputers%2520and%2520advanced%2520deep%2520learning%2520models.%2520The%2520extreme-resolution%2520data%250Arequired%2520to%2520represent%2520turbulence%252C%2520ranging%2520from%2520billions%2520to%2520trillions%2520of%2520grid%250Apoints%252C%2520pose%2520prohibitive%2520computational%2520costs%2520for%2520models%2520based%2520on%2520architectures%250Alike%2520vision%2520transformers.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520a%2520multiscale%250Ahierarchical%2520Turbulence%2520Transformer%2520that%2520reduces%2520sequence%2520length%2520from%2520billions%250Ato%2520a%2520few%2520millions%2520and%2520a%2520novel%2520RingX%2520sequence%2520parallelism%2520approach%2520that%2520enables%250Ascalable%2520long-context%2520learning.%2520We%2520perform%2520scaling%2520and%2520science%2520runs%2520on%2520the%250AFrontier%2520supercomputer.%2520Our%2520approach%2520demonstrates%2520excellent%2520performance%2520up%2520to%250A1.1%2520EFLOPS%2520on%252032%252C768%2520AMD%2520GPUs%252C%2520with%2520a%2520scaling%2520efficiency%2520of%252094%2525.%2520To%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520AI%2520model%2520for%2520turbulence%2520that%2520can%2520capture%250Asmall-scale%2520eddies%2520down%2520to%2520the%2520dissipative%2520range.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-Resolved%20Long-Context%20Learning%20for%20Turbulence%20at%20Exascale%3A%0A%20%20Resolving%20Small-scale%20Eddies%20Toward%20the%20Viscous%20Limit&entry.906535625=Junqi%20Yin%20and%20Mijanur%20Palash%20and%20M.%20Paul%20Laiu%20and%20Muralikrishnan%20Gopalakrishnan%20Meena%20and%20John%20Gounley%20and%20Stephen%20M.%20de%20Bruyn%20Kops%20and%20Feiyi%20Wang%20and%20Ramanan%20Sankaran%20and%20Pei%20Zhang&entry.1292438233=%20%20Turbulence%20plays%20a%20crucial%20role%20in%20multiphysics%20applications%2C%20including%0Aaerodynamics%2C%20fusion%2C%20and%20combustion.%20Accurately%20capturing%20turbulence%27s%0Amultiscale%20characteristics%20is%20essential%20for%20reliable%20predictions%20of%0Amultiphysics%20interactions%2C%20but%20remains%20a%20grand%20challenge%20even%20for%20exascale%0Asupercomputers%20and%20advanced%20deep%20learning%20models.%20The%20extreme-resolution%20data%0Arequired%20to%20represent%20turbulence%2C%20ranging%20from%20billions%20to%20trillions%20of%20grid%0Apoints%2C%20pose%20prohibitive%20computational%20costs%20for%20models%20based%20on%20architectures%0Alike%20vision%20transformers.%20To%20address%20this%20challenge%2C%20we%20introduce%20a%20multiscale%0Ahierarchical%20Turbulence%20Transformer%20that%20reduces%20sequence%20length%20from%20billions%0Ato%20a%20few%20millions%20and%20a%20novel%20RingX%20sequence%20parallelism%20approach%20that%20enables%0Ascalable%20long-context%20learning.%20We%20perform%20scaling%20and%20science%20runs%20on%20the%0AFrontier%20supercomputer.%20Our%20approach%20demonstrates%20excellent%20performance%20up%20to%0A1.1%20EFLOPS%20on%2032%2C768%20AMD%20GPUs%2C%20with%20a%20scaling%20efficiency%20of%2094%25.%20To%20our%0Aknowledge%2C%20this%20is%20the%20first%20AI%20model%20for%20turbulence%20that%20can%20capture%0Asmall-scale%20eddies%20down%20to%20the%20dissipative%20range.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16697v1&entry.124074799=Read"},
{"title": "GG-BBQ: German Gender Bias Benchmark for Question Answering", "author": "Shalaka Satheesh and Katrin Klug and Katharina Beckh and H\u00e9ctor Allende-Cid and Sebastian Houben and Teena Hassan", "abstract": "  Within the context of Natural Language Processing (NLP), fairness evaluation\nis often associated with the assessment of bias and reduction of associated\nharm. In this regard, the evaluation is usually carried out by using a\nbenchmark dataset, for a task such as Question Answering, created for the\nmeasurement of bias in the model's predictions along various dimensions,\nincluding gender identity. In our work, we evaluate gender bias in German Large\nLanguage Models (LLMs) using the Bias Benchmark for Question Answering by\nParrish et al. (2022) as a reference. Specifically, the templates in the gender\nidentity subset of this English dataset were machine translated into German.\nThe errors in the machine translated templates were then manually reviewed and\ncorrected with the help of a language expert. We find that manual revision of\nthe translation is crucial when creating datasets for gender bias evaluation\nbecause of the limitations of machine translation from English to a language\nsuch as German with grammatical gender. Our final dataset is comprised of two\nsubsets: Subset-I, which consists of group terms related to gender identity,\nand Subset-II, where group terms are replaced with proper names. We evaluate\nseveral LLMs used for German NLP on this newly created dataset and report the\naccuracy and bias scores. The results show that all models exhibit bias, both\nalong and against existing social stereotypes.\n", "link": "http://arxiv.org/abs/2507.16410v1", "date": "2025-07-22", "relevancy": 1.6869, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4309}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4232}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GG-BBQ%3A%20German%20Gender%20Bias%20Benchmark%20for%20Question%20Answering&body=Title%3A%20GG-BBQ%3A%20German%20Gender%20Bias%20Benchmark%20for%20Question%20Answering%0AAuthor%3A%20Shalaka%20Satheesh%20and%20Katrin%20Klug%20and%20Katharina%20Beckh%20and%20H%C3%A9ctor%20Allende-Cid%20and%20Sebastian%20Houben%20and%20Teena%20Hassan%0AAbstract%3A%20%20%20Within%20the%20context%20of%20Natural%20Language%20Processing%20%28NLP%29%2C%20fairness%20evaluation%0Ais%20often%20associated%20with%20the%20assessment%20of%20bias%20and%20reduction%20of%20associated%0Aharm.%20In%20this%20regard%2C%20the%20evaluation%20is%20usually%20carried%20out%20by%20using%20a%0Abenchmark%20dataset%2C%20for%20a%20task%20such%20as%20Question%20Answering%2C%20created%20for%20the%0Ameasurement%20of%20bias%20in%20the%20model%27s%20predictions%20along%20various%20dimensions%2C%0Aincluding%20gender%20identity.%20In%20our%20work%2C%20we%20evaluate%20gender%20bias%20in%20German%20Large%0ALanguage%20Models%20%28LLMs%29%20using%20the%20Bias%20Benchmark%20for%20Question%20Answering%20by%0AParrish%20et%20al.%20%282022%29%20as%20a%20reference.%20Specifically%2C%20the%20templates%20in%20the%20gender%0Aidentity%20subset%20of%20this%20English%20dataset%20were%20machine%20translated%20into%20German.%0AThe%20errors%20in%20the%20machine%20translated%20templates%20were%20then%20manually%20reviewed%20and%0Acorrected%20with%20the%20help%20of%20a%20language%20expert.%20We%20find%20that%20manual%20revision%20of%0Athe%20translation%20is%20crucial%20when%20creating%20datasets%20for%20gender%20bias%20evaluation%0Abecause%20of%20the%20limitations%20of%20machine%20translation%20from%20English%20to%20a%20language%0Asuch%20as%20German%20with%20grammatical%20gender.%20Our%20final%20dataset%20is%20comprised%20of%20two%0Asubsets%3A%20Subset-I%2C%20which%20consists%20of%20group%20terms%20related%20to%20gender%20identity%2C%0Aand%20Subset-II%2C%20where%20group%20terms%20are%20replaced%20with%20proper%20names.%20We%20evaluate%0Aseveral%20LLMs%20used%20for%20German%20NLP%20on%20this%20newly%20created%20dataset%20and%20report%20the%0Aaccuracy%20and%20bias%20scores.%20The%20results%20show%20that%20all%20models%20exhibit%20bias%2C%20both%0Aalong%20and%20against%20existing%20social%20stereotypes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGG-BBQ%253A%2520German%2520Gender%2520Bias%2520Benchmark%2520for%2520Question%2520Answering%26entry.906535625%3DShalaka%2520Satheesh%2520and%2520Katrin%2520Klug%2520and%2520Katharina%2520Beckh%2520and%2520H%25C3%25A9ctor%2520Allende-Cid%2520and%2520Sebastian%2520Houben%2520and%2520Teena%2520Hassan%26entry.1292438233%3D%2520%2520Within%2520the%2520context%2520of%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%252C%2520fairness%2520evaluation%250Ais%2520often%2520associated%2520with%2520the%2520assessment%2520of%2520bias%2520and%2520reduction%2520of%2520associated%250Aharm.%2520In%2520this%2520regard%252C%2520the%2520evaluation%2520is%2520usually%2520carried%2520out%2520by%2520using%2520a%250Abenchmark%2520dataset%252C%2520for%2520a%2520task%2520such%2520as%2520Question%2520Answering%252C%2520created%2520for%2520the%250Ameasurement%2520of%2520bias%2520in%2520the%2520model%2527s%2520predictions%2520along%2520various%2520dimensions%252C%250Aincluding%2520gender%2520identity.%2520In%2520our%2520work%252C%2520we%2520evaluate%2520gender%2520bias%2520in%2520German%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520using%2520the%2520Bias%2520Benchmark%2520for%2520Question%2520Answering%2520by%250AParrish%2520et%2520al.%2520%25282022%2529%2520as%2520a%2520reference.%2520Specifically%252C%2520the%2520templates%2520in%2520the%2520gender%250Aidentity%2520subset%2520of%2520this%2520English%2520dataset%2520were%2520machine%2520translated%2520into%2520German.%250AThe%2520errors%2520in%2520the%2520machine%2520translated%2520templates%2520were%2520then%2520manually%2520reviewed%2520and%250Acorrected%2520with%2520the%2520help%2520of%2520a%2520language%2520expert.%2520We%2520find%2520that%2520manual%2520revision%2520of%250Athe%2520translation%2520is%2520crucial%2520when%2520creating%2520datasets%2520for%2520gender%2520bias%2520evaluation%250Abecause%2520of%2520the%2520limitations%2520of%2520machine%2520translation%2520from%2520English%2520to%2520a%2520language%250Asuch%2520as%2520German%2520with%2520grammatical%2520gender.%2520Our%2520final%2520dataset%2520is%2520comprised%2520of%2520two%250Asubsets%253A%2520Subset-I%252C%2520which%2520consists%2520of%2520group%2520terms%2520related%2520to%2520gender%2520identity%252C%250Aand%2520Subset-II%252C%2520where%2520group%2520terms%2520are%2520replaced%2520with%2520proper%2520names.%2520We%2520evaluate%250Aseveral%2520LLMs%2520used%2520for%2520German%2520NLP%2520on%2520this%2520newly%2520created%2520dataset%2520and%2520report%2520the%250Aaccuracy%2520and%2520bias%2520scores.%2520The%2520results%2520show%2520that%2520all%2520models%2520exhibit%2520bias%252C%2520both%250Aalong%2520and%2520against%2520existing%2520social%2520stereotypes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GG-BBQ%3A%20German%20Gender%20Bias%20Benchmark%20for%20Question%20Answering&entry.906535625=Shalaka%20Satheesh%20and%20Katrin%20Klug%20and%20Katharina%20Beckh%20and%20H%C3%A9ctor%20Allende-Cid%20and%20Sebastian%20Houben%20and%20Teena%20Hassan&entry.1292438233=%20%20Within%20the%20context%20of%20Natural%20Language%20Processing%20%28NLP%29%2C%20fairness%20evaluation%0Ais%20often%20associated%20with%20the%20assessment%20of%20bias%20and%20reduction%20of%20associated%0Aharm.%20In%20this%20regard%2C%20the%20evaluation%20is%20usually%20carried%20out%20by%20using%20a%0Abenchmark%20dataset%2C%20for%20a%20task%20such%20as%20Question%20Answering%2C%20created%20for%20the%0Ameasurement%20of%20bias%20in%20the%20model%27s%20predictions%20along%20various%20dimensions%2C%0Aincluding%20gender%20identity.%20In%20our%20work%2C%20we%20evaluate%20gender%20bias%20in%20German%20Large%0ALanguage%20Models%20%28LLMs%29%20using%20the%20Bias%20Benchmark%20for%20Question%20Answering%20by%0AParrish%20et%20al.%20%282022%29%20as%20a%20reference.%20Specifically%2C%20the%20templates%20in%20the%20gender%0Aidentity%20subset%20of%20this%20English%20dataset%20were%20machine%20translated%20into%20German.%0AThe%20errors%20in%20the%20machine%20translated%20templates%20were%20then%20manually%20reviewed%20and%0Acorrected%20with%20the%20help%20of%20a%20language%20expert.%20We%20find%20that%20manual%20revision%20of%0Athe%20translation%20is%20crucial%20when%20creating%20datasets%20for%20gender%20bias%20evaluation%0Abecause%20of%20the%20limitations%20of%20machine%20translation%20from%20English%20to%20a%20language%0Asuch%20as%20German%20with%20grammatical%20gender.%20Our%20final%20dataset%20is%20comprised%20of%20two%0Asubsets%3A%20Subset-I%2C%20which%20consists%20of%20group%20terms%20related%20to%20gender%20identity%2C%0Aand%20Subset-II%2C%20where%20group%20terms%20are%20replaced%20with%20proper%20names.%20We%20evaluate%0Aseveral%20LLMs%20used%20for%20German%20NLP%20on%20this%20newly%20created%20dataset%20and%20report%20the%0Aaccuracy%20and%20bias%20scores.%20The%20results%20show%20that%20all%20models%20exhibit%20bias%2C%20both%0Aalong%20and%20against%20existing%20social%20stereotypes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16410v1&entry.124074799=Read"},
{"title": "Spectral Algorithms under Covariate Shift", "author": "Jun Fan and Zheng-Chu Guo and Lei Shi", "abstract": "  Spectral algorithms leverage spectral regularization techniques to analyze\nand process data, providing a flexible framework for addressing supervised\nlearning problems. To deepen our understanding of their performance in\nreal-world scenarios where the distributions of training and test data may\ndiffer, we conduct a rigorous investigation into the convergence behavior of\nspectral algorithms under covariate shift. In this setting, the marginal\ndistributions of the input data differ between the training and test datasets,\nwhile the conditional distribution of the output given the input remains\nunchanged. Within a non-parametric regression framework over a reproducing\nkernel Hilbert space, we analyze the convergence rates of spectral algorithms\nunder covariate shift and show that they achieve minimax optimality when the\ndensity ratios between the training and test distributions are uniformly\nbounded. However, when these density ratios are unbounded, the spectral\nalgorithms may become suboptimal. To address this issue, we propose a novel\nweighted spectral algorithm with normalized weights that incorporates density\nratio information into the learning process. Our theoretical analysis shows\nthat this normalized weighted approach achieves optimal capacity-independent\nconvergence rates, but the rates will suffer from the saturation phenomenon.\nFurthermore, by introducing a weight clipping technique, we demonstrate that\nthe convergence rates of the weighted spectral algorithm with clipped weights\ncan approach the optimal capacity-dependent convergence rates arbitrarily\nclosely. This improvement resolves the suboptimality issue in unbounded density\nratio scenarios and advances the state-of-the-art by refining existing\ntheoretical results.\n", "link": "http://arxiv.org/abs/2504.12625v2", "date": "2025-07-22", "relevancy": 1.8194, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4683}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4594}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4396}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Algorithms%20under%20Covariate%20Shift&body=Title%3A%20Spectral%20Algorithms%20under%20Covariate%20Shift%0AAuthor%3A%20Jun%20Fan%20and%20Zheng-Chu%20Guo%20and%20Lei%20Shi%0AAbstract%3A%20%20%20Spectral%20algorithms%20leverage%20spectral%20regularization%20techniques%20to%20analyze%0Aand%20process%20data%2C%20providing%20a%20flexible%20framework%20for%20addressing%20supervised%0Alearning%20problems.%20To%20deepen%20our%20understanding%20of%20their%20performance%20in%0Areal-world%20scenarios%20where%20the%20distributions%20of%20training%20and%20test%20data%20may%0Adiffer%2C%20we%20conduct%20a%20rigorous%20investigation%20into%20the%20convergence%20behavior%20of%0Aspectral%20algorithms%20under%20covariate%20shift.%20In%20this%20setting%2C%20the%20marginal%0Adistributions%20of%20the%20input%20data%20differ%20between%20the%20training%20and%20test%20datasets%2C%0Awhile%20the%20conditional%20distribution%20of%20the%20output%20given%20the%20input%20remains%0Aunchanged.%20Within%20a%20non-parametric%20regression%20framework%20over%20a%20reproducing%0Akernel%20Hilbert%20space%2C%20we%20analyze%20the%20convergence%20rates%20of%20spectral%20algorithms%0Aunder%20covariate%20shift%20and%20show%20that%20they%20achieve%20minimax%20optimality%20when%20the%0Adensity%20ratios%20between%20the%20training%20and%20test%20distributions%20are%20uniformly%0Abounded.%20However%2C%20when%20these%20density%20ratios%20are%20unbounded%2C%20the%20spectral%0Aalgorithms%20may%20become%20suboptimal.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0Aweighted%20spectral%20algorithm%20with%20normalized%20weights%20that%20incorporates%20density%0Aratio%20information%20into%20the%20learning%20process.%20Our%20theoretical%20analysis%20shows%0Athat%20this%20normalized%20weighted%20approach%20achieves%20optimal%20capacity-independent%0Aconvergence%20rates%2C%20but%20the%20rates%20will%20suffer%20from%20the%20saturation%20phenomenon.%0AFurthermore%2C%20by%20introducing%20a%20weight%20clipping%20technique%2C%20we%20demonstrate%20that%0Athe%20convergence%20rates%20of%20the%20weighted%20spectral%20algorithm%20with%20clipped%20weights%0Acan%20approach%20the%20optimal%20capacity-dependent%20convergence%20rates%20arbitrarily%0Aclosely.%20This%20improvement%20resolves%20the%20suboptimality%20issue%20in%20unbounded%20density%0Aratio%20scenarios%20and%20advances%20the%20state-of-the-art%20by%20refining%20existing%0Atheoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12625v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Algorithms%2520under%2520Covariate%2520Shift%26entry.906535625%3DJun%2520Fan%2520and%2520Zheng-Chu%2520Guo%2520and%2520Lei%2520Shi%26entry.1292438233%3D%2520%2520Spectral%2520algorithms%2520leverage%2520spectral%2520regularization%2520techniques%2520to%2520analyze%250Aand%2520process%2520data%252C%2520providing%2520a%2520flexible%2520framework%2520for%2520addressing%2520supervised%250Alearning%2520problems.%2520To%2520deepen%2520our%2520understanding%2520of%2520their%2520performance%2520in%250Areal-world%2520scenarios%2520where%2520the%2520distributions%2520of%2520training%2520and%2520test%2520data%2520may%250Adiffer%252C%2520we%2520conduct%2520a%2520rigorous%2520investigation%2520into%2520the%2520convergence%2520behavior%2520of%250Aspectral%2520algorithms%2520under%2520covariate%2520shift.%2520In%2520this%2520setting%252C%2520the%2520marginal%250Adistributions%2520of%2520the%2520input%2520data%2520differ%2520between%2520the%2520training%2520and%2520test%2520datasets%252C%250Awhile%2520the%2520conditional%2520distribution%2520of%2520the%2520output%2520given%2520the%2520input%2520remains%250Aunchanged.%2520Within%2520a%2520non-parametric%2520regression%2520framework%2520over%2520a%2520reproducing%250Akernel%2520Hilbert%2520space%252C%2520we%2520analyze%2520the%2520convergence%2520rates%2520of%2520spectral%2520algorithms%250Aunder%2520covariate%2520shift%2520and%2520show%2520that%2520they%2520achieve%2520minimax%2520optimality%2520when%2520the%250Adensity%2520ratios%2520between%2520the%2520training%2520and%2520test%2520distributions%2520are%2520uniformly%250Abounded.%2520However%252C%2520when%2520these%2520density%2520ratios%2520are%2520unbounded%252C%2520the%2520spectral%250Aalgorithms%2520may%2520become%2520suboptimal.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%250Aweighted%2520spectral%2520algorithm%2520with%2520normalized%2520weights%2520that%2520incorporates%2520density%250Aratio%2520information%2520into%2520the%2520learning%2520process.%2520Our%2520theoretical%2520analysis%2520shows%250Athat%2520this%2520normalized%2520weighted%2520approach%2520achieves%2520optimal%2520capacity-independent%250Aconvergence%2520rates%252C%2520but%2520the%2520rates%2520will%2520suffer%2520from%2520the%2520saturation%2520phenomenon.%250AFurthermore%252C%2520by%2520introducing%2520a%2520weight%2520clipping%2520technique%252C%2520we%2520demonstrate%2520that%250Athe%2520convergence%2520rates%2520of%2520the%2520weighted%2520spectral%2520algorithm%2520with%2520clipped%2520weights%250Acan%2520approach%2520the%2520optimal%2520capacity-dependent%2520convergence%2520rates%2520arbitrarily%250Aclosely.%2520This%2520improvement%2520resolves%2520the%2520suboptimality%2520issue%2520in%2520unbounded%2520density%250Aratio%2520scenarios%2520and%2520advances%2520the%2520state-of-the-art%2520by%2520refining%2520existing%250Atheoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12625v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Algorithms%20under%20Covariate%20Shift&entry.906535625=Jun%20Fan%20and%20Zheng-Chu%20Guo%20and%20Lei%20Shi&entry.1292438233=%20%20Spectral%20algorithms%20leverage%20spectral%20regularization%20techniques%20to%20analyze%0Aand%20process%20data%2C%20providing%20a%20flexible%20framework%20for%20addressing%20supervised%0Alearning%20problems.%20To%20deepen%20our%20understanding%20of%20their%20performance%20in%0Areal-world%20scenarios%20where%20the%20distributions%20of%20training%20and%20test%20data%20may%0Adiffer%2C%20we%20conduct%20a%20rigorous%20investigation%20into%20the%20convergence%20behavior%20of%0Aspectral%20algorithms%20under%20covariate%20shift.%20In%20this%20setting%2C%20the%20marginal%0Adistributions%20of%20the%20input%20data%20differ%20between%20the%20training%20and%20test%20datasets%2C%0Awhile%20the%20conditional%20distribution%20of%20the%20output%20given%20the%20input%20remains%0Aunchanged.%20Within%20a%20non-parametric%20regression%20framework%20over%20a%20reproducing%0Akernel%20Hilbert%20space%2C%20we%20analyze%20the%20convergence%20rates%20of%20spectral%20algorithms%0Aunder%20covariate%20shift%20and%20show%20that%20they%20achieve%20minimax%20optimality%20when%20the%0Adensity%20ratios%20between%20the%20training%20and%20test%20distributions%20are%20uniformly%0Abounded.%20However%2C%20when%20these%20density%20ratios%20are%20unbounded%2C%20the%20spectral%0Aalgorithms%20may%20become%20suboptimal.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%0Aweighted%20spectral%20algorithm%20with%20normalized%20weights%20that%20incorporates%20density%0Aratio%20information%20into%20the%20learning%20process.%20Our%20theoretical%20analysis%20shows%0Athat%20this%20normalized%20weighted%20approach%20achieves%20optimal%20capacity-independent%0Aconvergence%20rates%2C%20but%20the%20rates%20will%20suffer%20from%20the%20saturation%20phenomenon.%0AFurthermore%2C%20by%20introducing%20a%20weight%20clipping%20technique%2C%20we%20demonstrate%20that%0Athe%20convergence%20rates%20of%20the%20weighted%20spectral%20algorithm%20with%20clipped%20weights%0Acan%20approach%20the%20optimal%20capacity-dependent%20convergence%20rates%20arbitrarily%0Aclosely.%20This%20improvement%20resolves%20the%20suboptimality%20issue%20in%20unbounded%20density%0Aratio%20scenarios%20and%20advances%20the%20state-of-the-art%20by%20refining%20existing%0Atheoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12625v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


