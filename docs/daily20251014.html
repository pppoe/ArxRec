<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251013.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view\n  Human Reconstruction", "author": "Wenyue Chen and Peng Li and Wangguandong Zheng and Chengfeng Zhao and Mengfei Li and Yaolong Zhu and Zhiyang Dou and Ronggang Wang and Yuan Liu", "abstract": "  Photorealistic 3D full-body human reconstruction from a single image is a\ncritical yet challenging task for applications in films and video games due to\ninherent ambiguities and severe self-occlusions. While recent approaches\nleverage SMPL estimation and SMPL-conditioned image generative models to\nhallucinate novel views, they suffer from inaccurate 3D priors estimated from\nSMPL meshes and have difficulty in handling difficult human poses and\nreconstructing fine details. In this paper, we propose SyncHuman, a novel\nframework that combines 2D multiview generative model and 3D native generative\nmodel for the first time, enabling high-quality clothed human mesh\nreconstruction from single-view images even under challenging human poses.\nMultiview generative model excels at capturing fine 2D details but struggles\nwith structural consistency, whereas 3D native generative model generates\ncoarse yet structurally consistent 3D shapes. By integrating the complementary\nstrengths of these two approaches, we develop a more effective generation\nframework. Specifically, we first jointly fine-tune the multiview generative\nmodel and the 3D native generative model with proposed pixel-aligned 2D-3D\nsynchronization attention to produce geometrically aligned 3D shapes and 2D\nmultiview images. To further improve details, we introduce a feature injection\nmechanism that lifts fine details from 2D multiview images onto the aligned 3D\nshapes, enabling accurate and high-fidelity reconstruction. Extensive\nexperiments demonstrate that SyncHuman achieves robust and photo-realistic 3D\nhuman reconstruction, even for images with challenging poses. Our method\noutperforms baseline methods in geometric accuracy and visual fidelity,\ndemonstrating a promising direction for future 3D generation models.\n", "link": "http://arxiv.org/abs/2510.07723v2", "date": "2025-10-13", "relevancy": 3.3898, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7217}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6875}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyncHuman%3A%20Synchronizing%202D%20and%203D%20Generative%20Models%20for%20Single-view%0A%20%20Human%20Reconstruction&body=Title%3A%20SyncHuman%3A%20Synchronizing%202D%20and%203D%20Generative%20Models%20for%20Single-view%0A%20%20Human%20Reconstruction%0AAuthor%3A%20Wenyue%20Chen%20and%20Peng%20Li%20and%20Wangguandong%20Zheng%20and%20Chengfeng%20Zhao%20and%20Mengfei%20Li%20and%20Yaolong%20Zhu%20and%20Zhiyang%20Dou%20and%20Ronggang%20Wang%20and%20Yuan%20Liu%0AAbstract%3A%20%20%20Photorealistic%203D%20full-body%20human%20reconstruction%20from%20a%20single%20image%20is%20a%0Acritical%20yet%20challenging%20task%20for%20applications%20in%20films%20and%20video%20games%20due%20to%0Ainherent%20ambiguities%20and%20severe%20self-occlusions.%20While%20recent%20approaches%0Aleverage%20SMPL%20estimation%20and%20SMPL-conditioned%20image%20generative%20models%20to%0Ahallucinate%20novel%20views%2C%20they%20suffer%20from%20inaccurate%203D%20priors%20estimated%20from%0ASMPL%20meshes%20and%20have%20difficulty%20in%20handling%20difficult%20human%20poses%20and%0Areconstructing%20fine%20details.%20In%20this%20paper%2C%20we%20propose%20SyncHuman%2C%20a%20novel%0Aframework%20that%20combines%202D%20multiview%20generative%20model%20and%203D%20native%20generative%0Amodel%20for%20the%20first%20time%2C%20enabling%20high-quality%20clothed%20human%20mesh%0Areconstruction%20from%20single-view%20images%20even%20under%20challenging%20human%20poses.%0AMultiview%20generative%20model%20excels%20at%20capturing%20fine%202D%20details%20but%20struggles%0Awith%20structural%20consistency%2C%20whereas%203D%20native%20generative%20model%20generates%0Acoarse%20yet%20structurally%20consistent%203D%20shapes.%20By%20integrating%20the%20complementary%0Astrengths%20of%20these%20two%20approaches%2C%20we%20develop%20a%20more%20effective%20generation%0Aframework.%20Specifically%2C%20we%20first%20jointly%20fine-tune%20the%20multiview%20generative%0Amodel%20and%20the%203D%20native%20generative%20model%20with%20proposed%20pixel-aligned%202D-3D%0Asynchronization%20attention%20to%20produce%20geometrically%20aligned%203D%20shapes%20and%202D%0Amultiview%20images.%20To%20further%20improve%20details%2C%20we%20introduce%20a%20feature%20injection%0Amechanism%20that%20lifts%20fine%20details%20from%202D%20multiview%20images%20onto%20the%20aligned%203D%0Ashapes%2C%20enabling%20accurate%20and%20high-fidelity%20reconstruction.%20Extensive%0Aexperiments%20demonstrate%20that%20SyncHuman%20achieves%20robust%20and%20photo-realistic%203D%0Ahuman%20reconstruction%2C%20even%20for%20images%20with%20challenging%20poses.%20Our%20method%0Aoutperforms%20baseline%20methods%20in%20geometric%20accuracy%20and%20visual%20fidelity%2C%0Ademonstrating%20a%20promising%20direction%20for%20future%203D%20generation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyncHuman%253A%2520Synchronizing%25202D%2520and%25203D%2520Generative%2520Models%2520for%2520Single-view%250A%2520%2520Human%2520Reconstruction%26entry.906535625%3DWenyue%2520Chen%2520and%2520Peng%2520Li%2520and%2520Wangguandong%2520Zheng%2520and%2520Chengfeng%2520Zhao%2520and%2520Mengfei%2520Li%2520and%2520Yaolong%2520Zhu%2520and%2520Zhiyang%2520Dou%2520and%2520Ronggang%2520Wang%2520and%2520Yuan%2520Liu%26entry.1292438233%3D%2520%2520Photorealistic%25203D%2520full-body%2520human%2520reconstruction%2520from%2520a%2520single%2520image%2520is%2520a%250Acritical%2520yet%2520challenging%2520task%2520for%2520applications%2520in%2520films%2520and%2520video%2520games%2520due%2520to%250Ainherent%2520ambiguities%2520and%2520severe%2520self-occlusions.%2520While%2520recent%2520approaches%250Aleverage%2520SMPL%2520estimation%2520and%2520SMPL-conditioned%2520image%2520generative%2520models%2520to%250Ahallucinate%2520novel%2520views%252C%2520they%2520suffer%2520from%2520inaccurate%25203D%2520priors%2520estimated%2520from%250ASMPL%2520meshes%2520and%2520have%2520difficulty%2520in%2520handling%2520difficult%2520human%2520poses%2520and%250Areconstructing%2520fine%2520details.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SyncHuman%252C%2520a%2520novel%250Aframework%2520that%2520combines%25202D%2520multiview%2520generative%2520model%2520and%25203D%2520native%2520generative%250Amodel%2520for%2520the%2520first%2520time%252C%2520enabling%2520high-quality%2520clothed%2520human%2520mesh%250Areconstruction%2520from%2520single-view%2520images%2520even%2520under%2520challenging%2520human%2520poses.%250AMultiview%2520generative%2520model%2520excels%2520at%2520capturing%2520fine%25202D%2520details%2520but%2520struggles%250Awith%2520structural%2520consistency%252C%2520whereas%25203D%2520native%2520generative%2520model%2520generates%250Acoarse%2520yet%2520structurally%2520consistent%25203D%2520shapes.%2520By%2520integrating%2520the%2520complementary%250Astrengths%2520of%2520these%2520two%2520approaches%252C%2520we%2520develop%2520a%2520more%2520effective%2520generation%250Aframework.%2520Specifically%252C%2520we%2520first%2520jointly%2520fine-tune%2520the%2520multiview%2520generative%250Amodel%2520and%2520the%25203D%2520native%2520generative%2520model%2520with%2520proposed%2520pixel-aligned%25202D-3D%250Asynchronization%2520attention%2520to%2520produce%2520geometrically%2520aligned%25203D%2520shapes%2520and%25202D%250Amultiview%2520images.%2520To%2520further%2520improve%2520details%252C%2520we%2520introduce%2520a%2520feature%2520injection%250Amechanism%2520that%2520lifts%2520fine%2520details%2520from%25202D%2520multiview%2520images%2520onto%2520the%2520aligned%25203D%250Ashapes%252C%2520enabling%2520accurate%2520and%2520high-fidelity%2520reconstruction.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520SyncHuman%2520achieves%2520robust%2520and%2520photo-realistic%25203D%250Ahuman%2520reconstruction%252C%2520even%2520for%2520images%2520with%2520challenging%2520poses.%2520Our%2520method%250Aoutperforms%2520baseline%2520methods%2520in%2520geometric%2520accuracy%2520and%2520visual%2520fidelity%252C%250Ademonstrating%2520a%2520promising%2520direction%2520for%2520future%25203D%2520generation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyncHuman%3A%20Synchronizing%202D%20and%203D%20Generative%20Models%20for%20Single-view%0A%20%20Human%20Reconstruction&entry.906535625=Wenyue%20Chen%20and%20Peng%20Li%20and%20Wangguandong%20Zheng%20and%20Chengfeng%20Zhao%20and%20Mengfei%20Li%20and%20Yaolong%20Zhu%20and%20Zhiyang%20Dou%20and%20Ronggang%20Wang%20and%20Yuan%20Liu&entry.1292438233=%20%20Photorealistic%203D%20full-body%20human%20reconstruction%20from%20a%20single%20image%20is%20a%0Acritical%20yet%20challenging%20task%20for%20applications%20in%20films%20and%20video%20games%20due%20to%0Ainherent%20ambiguities%20and%20severe%20self-occlusions.%20While%20recent%20approaches%0Aleverage%20SMPL%20estimation%20and%20SMPL-conditioned%20image%20generative%20models%20to%0Ahallucinate%20novel%20views%2C%20they%20suffer%20from%20inaccurate%203D%20priors%20estimated%20from%0ASMPL%20meshes%20and%20have%20difficulty%20in%20handling%20difficult%20human%20poses%20and%0Areconstructing%20fine%20details.%20In%20this%20paper%2C%20we%20propose%20SyncHuman%2C%20a%20novel%0Aframework%20that%20combines%202D%20multiview%20generative%20model%20and%203D%20native%20generative%0Amodel%20for%20the%20first%20time%2C%20enabling%20high-quality%20clothed%20human%20mesh%0Areconstruction%20from%20single-view%20images%20even%20under%20challenging%20human%20poses.%0AMultiview%20generative%20model%20excels%20at%20capturing%20fine%202D%20details%20but%20struggles%0Awith%20structural%20consistency%2C%20whereas%203D%20native%20generative%20model%20generates%0Acoarse%20yet%20structurally%20consistent%203D%20shapes.%20By%20integrating%20the%20complementary%0Astrengths%20of%20these%20two%20approaches%2C%20we%20develop%20a%20more%20effective%20generation%0Aframework.%20Specifically%2C%20we%20first%20jointly%20fine-tune%20the%20multiview%20generative%0Amodel%20and%20the%203D%20native%20generative%20model%20with%20proposed%20pixel-aligned%202D-3D%0Asynchronization%20attention%20to%20produce%20geometrically%20aligned%203D%20shapes%20and%202D%0Amultiview%20images.%20To%20further%20improve%20details%2C%20we%20introduce%20a%20feature%20injection%0Amechanism%20that%20lifts%20fine%20details%20from%202D%20multiview%20images%20onto%20the%20aligned%203D%0Ashapes%2C%20enabling%20accurate%20and%20high-fidelity%20reconstruction.%20Extensive%0Aexperiments%20demonstrate%20that%20SyncHuman%20achieves%20robust%20and%20photo-realistic%203D%0Ahuman%20reconstruction%2C%20even%20for%20images%20with%20challenging%20poses.%20Our%20method%0Aoutperforms%20baseline%20methods%20in%20geometric%20accuracy%20and%20visual%20fidelity%2C%0Ademonstrating%20a%20promising%20direction%20for%20future%203D%20generation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07723v2&entry.124074799=Read"},
{"title": "Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event\n  Streams", "author": "Takuya Nakabayashi and Navami Kairanda and Hideo Saito and Vladislav Golyanik", "abstract": "  Event cameras offer various advantages for novel view rendering compared to\nsynchronously operating RGB cameras, and efficient event-based techniques\nsupporting rigid scenes have been recently demonstrated in the literature. In\nthe case of non-rigid objects, however, existing approaches additionally\nrequire sparse RGB inputs, which can be a substantial practical limitation; it\nremains unknown if similar models could be learned from event streams only.\nThis paper sheds light on this challenging open question and introduces Ev4DGS,\ni.e., the first approach for novel view rendering of non-rigidly deforming\nobjects in the explicit observation space (i.e., as RGB or greyscale images)\nfrom monocular event streams. Our method regresses a deformable 3D Gaussian\nSplatting representation through 1) a loss relating the outputs of the\nestimated model with the 2D event observation space, and 2) a coarse 3D\ndeformation model trained from binary masks generated from events. We perform\nexperimental comparisons on existing synthetic and newly recorded real datasets\nwith non-rigid objects. The results demonstrate the validity of Ev4DGS and its\nsuperior performance compared to multiple naive baselines that can be applied\nin our setting. We will release our models and the datasets used in the\nevaluation for research purposes; see the project webpage:\nhttps://4dqv.mpi-inf.mpg.de/Ev4DGS/.\n", "link": "http://arxiv.org/abs/2510.11717v1", "date": "2025-10-13", "relevancy": 3.2923, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6712}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6589}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ev4DGS%3A%20Novel-view%20Rendering%20of%20Non-Rigid%20Objects%20from%20Monocular%20Event%0A%20%20Streams&body=Title%3A%20Ev4DGS%3A%20Novel-view%20Rendering%20of%20Non-Rigid%20Objects%20from%20Monocular%20Event%0A%20%20Streams%0AAuthor%3A%20Takuya%20Nakabayashi%20and%20Navami%20Kairanda%20and%20Hideo%20Saito%20and%20Vladislav%20Golyanik%0AAbstract%3A%20%20%20Event%20cameras%20offer%20various%20advantages%20for%20novel%20view%20rendering%20compared%20to%0Asynchronously%20operating%20RGB%20cameras%2C%20and%20efficient%20event-based%20techniques%0Asupporting%20rigid%20scenes%20have%20been%20recently%20demonstrated%20in%20the%20literature.%20In%0Athe%20case%20of%20non-rigid%20objects%2C%20however%2C%20existing%20approaches%20additionally%0Arequire%20sparse%20RGB%20inputs%2C%20which%20can%20be%20a%20substantial%20practical%20limitation%3B%20it%0Aremains%20unknown%20if%20similar%20models%20could%20be%20learned%20from%20event%20streams%20only.%0AThis%20paper%20sheds%20light%20on%20this%20challenging%20open%20question%20and%20introduces%20Ev4DGS%2C%0Ai.e.%2C%20the%20first%20approach%20for%20novel%20view%20rendering%20of%20non-rigidly%20deforming%0Aobjects%20in%20the%20explicit%20observation%20space%20%28i.e.%2C%20as%20RGB%20or%20greyscale%20images%29%0Afrom%20monocular%20event%20streams.%20Our%20method%20regresses%20a%20deformable%203D%20Gaussian%0ASplatting%20representation%20through%201%29%20a%20loss%20relating%20the%20outputs%20of%20the%0Aestimated%20model%20with%20the%202D%20event%20observation%20space%2C%20and%202%29%20a%20coarse%203D%0Adeformation%20model%20trained%20from%20binary%20masks%20generated%20from%20events.%20We%20perform%0Aexperimental%20comparisons%20on%20existing%20synthetic%20and%20newly%20recorded%20real%20datasets%0Awith%20non-rigid%20objects.%20The%20results%20demonstrate%20the%20validity%20of%20Ev4DGS%20and%20its%0Asuperior%20performance%20compared%20to%20multiple%20naive%20baselines%20that%20can%20be%20applied%0Ain%20our%20setting.%20We%20will%20release%20our%20models%20and%20the%20datasets%20used%20in%20the%0Aevaluation%20for%20research%20purposes%3B%20see%20the%20project%20webpage%3A%0Ahttps%3A//4dqv.mpi-inf.mpg.de/Ev4DGS/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEv4DGS%253A%2520Novel-view%2520Rendering%2520of%2520Non-Rigid%2520Objects%2520from%2520Monocular%2520Event%250A%2520%2520Streams%26entry.906535625%3DTakuya%2520Nakabayashi%2520and%2520Navami%2520Kairanda%2520and%2520Hideo%2520Saito%2520and%2520Vladislav%2520Golyanik%26entry.1292438233%3D%2520%2520Event%2520cameras%2520offer%2520various%2520advantages%2520for%2520novel%2520view%2520rendering%2520compared%2520to%250Asynchronously%2520operating%2520RGB%2520cameras%252C%2520and%2520efficient%2520event-based%2520techniques%250Asupporting%2520rigid%2520scenes%2520have%2520been%2520recently%2520demonstrated%2520in%2520the%2520literature.%2520In%250Athe%2520case%2520of%2520non-rigid%2520objects%252C%2520however%252C%2520existing%2520approaches%2520additionally%250Arequire%2520sparse%2520RGB%2520inputs%252C%2520which%2520can%2520be%2520a%2520substantial%2520practical%2520limitation%253B%2520it%250Aremains%2520unknown%2520if%2520similar%2520models%2520could%2520be%2520learned%2520from%2520event%2520streams%2520only.%250AThis%2520paper%2520sheds%2520light%2520on%2520this%2520challenging%2520open%2520question%2520and%2520introduces%2520Ev4DGS%252C%250Ai.e.%252C%2520the%2520first%2520approach%2520for%2520novel%2520view%2520rendering%2520of%2520non-rigidly%2520deforming%250Aobjects%2520in%2520the%2520explicit%2520observation%2520space%2520%2528i.e.%252C%2520as%2520RGB%2520or%2520greyscale%2520images%2529%250Afrom%2520monocular%2520event%2520streams.%2520Our%2520method%2520regresses%2520a%2520deformable%25203D%2520Gaussian%250ASplatting%2520representation%2520through%25201%2529%2520a%2520loss%2520relating%2520the%2520outputs%2520of%2520the%250Aestimated%2520model%2520with%2520the%25202D%2520event%2520observation%2520space%252C%2520and%25202%2529%2520a%2520coarse%25203D%250Adeformation%2520model%2520trained%2520from%2520binary%2520masks%2520generated%2520from%2520events.%2520We%2520perform%250Aexperimental%2520comparisons%2520on%2520existing%2520synthetic%2520and%2520newly%2520recorded%2520real%2520datasets%250Awith%2520non-rigid%2520objects.%2520The%2520results%2520demonstrate%2520the%2520validity%2520of%2520Ev4DGS%2520and%2520its%250Asuperior%2520performance%2520compared%2520to%2520multiple%2520naive%2520baselines%2520that%2520can%2520be%2520applied%250Ain%2520our%2520setting.%2520We%2520will%2520release%2520our%2520models%2520and%2520the%2520datasets%2520used%2520in%2520the%250Aevaluation%2520for%2520research%2520purposes%253B%2520see%2520the%2520project%2520webpage%253A%250Ahttps%253A//4dqv.mpi-inf.mpg.de/Ev4DGS/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ev4DGS%3A%20Novel-view%20Rendering%20of%20Non-Rigid%20Objects%20from%20Monocular%20Event%0A%20%20Streams&entry.906535625=Takuya%20Nakabayashi%20and%20Navami%20Kairanda%20and%20Hideo%20Saito%20and%20Vladislav%20Golyanik&entry.1292438233=%20%20Event%20cameras%20offer%20various%20advantages%20for%20novel%20view%20rendering%20compared%20to%0Asynchronously%20operating%20RGB%20cameras%2C%20and%20efficient%20event-based%20techniques%0Asupporting%20rigid%20scenes%20have%20been%20recently%20demonstrated%20in%20the%20literature.%20In%0Athe%20case%20of%20non-rigid%20objects%2C%20however%2C%20existing%20approaches%20additionally%0Arequire%20sparse%20RGB%20inputs%2C%20which%20can%20be%20a%20substantial%20practical%20limitation%3B%20it%0Aremains%20unknown%20if%20similar%20models%20could%20be%20learned%20from%20event%20streams%20only.%0AThis%20paper%20sheds%20light%20on%20this%20challenging%20open%20question%20and%20introduces%20Ev4DGS%2C%0Ai.e.%2C%20the%20first%20approach%20for%20novel%20view%20rendering%20of%20non-rigidly%20deforming%0Aobjects%20in%20the%20explicit%20observation%20space%20%28i.e.%2C%20as%20RGB%20or%20greyscale%20images%29%0Afrom%20monocular%20event%20streams.%20Our%20method%20regresses%20a%20deformable%203D%20Gaussian%0ASplatting%20representation%20through%201%29%20a%20loss%20relating%20the%20outputs%20of%20the%0Aestimated%20model%20with%20the%202D%20event%20observation%20space%2C%20and%202%29%20a%20coarse%203D%0Adeformation%20model%20trained%20from%20binary%20masks%20generated%20from%20events.%20We%20perform%0Aexperimental%20comparisons%20on%20existing%20synthetic%20and%20newly%20recorded%20real%20datasets%0Awith%20non-rigid%20objects.%20The%20results%20demonstrate%20the%20validity%20of%20Ev4DGS%20and%20its%0Asuperior%20performance%20compared%20to%20multiple%20naive%20baselines%20that%20can%20be%20applied%0Ain%20our%20setting.%20We%20will%20release%20our%20models%20and%20the%20datasets%20used%20in%20the%0Aevaluation%20for%20research%20purposes%3B%20see%20the%20project%20webpage%3A%0Ahttps%3A//4dqv.mpi-inf.mpg.de/Ev4DGS/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11717v1&entry.124074799=Read"},
{"title": "InfiniHuman: Infinite 3D Human Creation with Precise Control", "author": "Yuxuan Xue and Xianghui Xie and Margaret Kostyrko and Gerard Pons-Moll", "abstract": "  Generating realistic and controllable 3D human avatars is a long-standing\nchallenge, particularly when covering broad attribute ranges such as ethnicity,\nage, clothing styles, and detailed body shapes. Capturing and annotating\nlarge-scale human datasets for training generative models is prohibitively\nexpensive and limited in scale and diversity. The central question we address\nin this paper is: Can existing foundation models be distilled to generate\ntheoretically unbounded, richly annotated 3D human data? We introduce\nInfiniHuman, a framework that synergistically distills these models to produce\nrichly annotated human data at minimal cost and with theoretically unlimited\nscalability. We propose InfiniHumanData, a fully automatic pipeline that\nleverages vision-language and image generation models to create a large-scale\nmulti-modal dataset. User study shows our automatically generated identities\nare undistinguishable from scan renderings. InfiniHumanData contains 111K\nidentities spanning unprecedented diversity. Each identity is annotated with\nmulti-granularity text descriptions, multi-view RGB images, detailed clothing\nimages, and SMPL body-shape parameters. Building on this dataset, we propose\nInfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body\nshape, and clothing assets. InfiniHumanGen enables fast, realistic, and\nprecisely controllable avatar generation. Extensive experiments demonstrate\nsignificant improvements over state-of-the-art methods in visual quality,\ngeneration speed, and controllability. Our approach enables high-quality avatar\ngeneration with fine-grained control at effectively unbounded scale through a\npractical and affordable solution. We will publicly release the automatic data\ngeneration pipeline, the comprehensive InfiniHumanData dataset, and the\nInfiniHumanGen models at https://yuxuan-xue.com/infini-human.\n", "link": "http://arxiv.org/abs/2510.11650v1", "date": "2025-10-13", "relevancy": 3.1719, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6625}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6273}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiniHuman%3A%20Infinite%203D%20Human%20Creation%20with%20Precise%20Control&body=Title%3A%20InfiniHuman%3A%20Infinite%203D%20Human%20Creation%20with%20Precise%20Control%0AAuthor%3A%20Yuxuan%20Xue%20and%20Xianghui%20Xie%20and%20Margaret%20Kostyrko%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20%20%20Generating%20realistic%20and%20controllable%203D%20human%20avatars%20is%20a%20long-standing%0Achallenge%2C%20particularly%20when%20covering%20broad%20attribute%20ranges%20such%20as%20ethnicity%2C%0Aage%2C%20clothing%20styles%2C%20and%20detailed%20body%20shapes.%20Capturing%20and%20annotating%0Alarge-scale%20human%20datasets%20for%20training%20generative%20models%20is%20prohibitively%0Aexpensive%20and%20limited%20in%20scale%20and%20diversity.%20The%20central%20question%20we%20address%0Ain%20this%20paper%20is%3A%20Can%20existing%20foundation%20models%20be%20distilled%20to%20generate%0Atheoretically%20unbounded%2C%20richly%20annotated%203D%20human%20data%3F%20We%20introduce%0AInfiniHuman%2C%20a%20framework%20that%20synergistically%20distills%20these%20models%20to%20produce%0Arichly%20annotated%20human%20data%20at%20minimal%20cost%20and%20with%20theoretically%20unlimited%0Ascalability.%20We%20propose%20InfiniHumanData%2C%20a%20fully%20automatic%20pipeline%20that%0Aleverages%20vision-language%20and%20image%20generation%20models%20to%20create%20a%20large-scale%0Amulti-modal%20dataset.%20User%20study%20shows%20our%20automatically%20generated%20identities%0Aare%20undistinguishable%20from%20scan%20renderings.%20InfiniHumanData%20contains%20111K%0Aidentities%20spanning%20unprecedented%20diversity.%20Each%20identity%20is%20annotated%20with%0Amulti-granularity%20text%20descriptions%2C%20multi-view%20RGB%20images%2C%20detailed%20clothing%0Aimages%2C%20and%20SMPL%20body-shape%20parameters.%20Building%20on%20this%20dataset%2C%20we%20propose%0AInfiniHumanGen%2C%20a%20diffusion-based%20generative%20pipeline%20conditioned%20on%20text%2C%20body%0Ashape%2C%20and%20clothing%20assets.%20InfiniHumanGen%20enables%20fast%2C%20realistic%2C%20and%0Aprecisely%20controllable%20avatar%20generation.%20Extensive%20experiments%20demonstrate%0Asignificant%20improvements%20over%20state-of-the-art%20methods%20in%20visual%20quality%2C%0Ageneration%20speed%2C%20and%20controllability.%20Our%20approach%20enables%20high-quality%20avatar%0Ageneration%20with%20fine-grained%20control%20at%20effectively%20unbounded%20scale%20through%20a%0Apractical%20and%20affordable%20solution.%20We%20will%20publicly%20release%20the%20automatic%20data%0Ageneration%20pipeline%2C%20the%20comprehensive%20InfiniHumanData%20dataset%2C%20and%20the%0AInfiniHumanGen%20models%20at%20https%3A//yuxuan-xue.com/infini-human.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiniHuman%253A%2520Infinite%25203D%2520Human%2520Creation%2520with%2520Precise%2520Control%26entry.906535625%3DYuxuan%2520Xue%2520and%2520Xianghui%2520Xie%2520and%2520Margaret%2520Kostyrko%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520and%2520controllable%25203D%2520human%2520avatars%2520is%2520a%2520long-standing%250Achallenge%252C%2520particularly%2520when%2520covering%2520broad%2520attribute%2520ranges%2520such%2520as%2520ethnicity%252C%250Aage%252C%2520clothing%2520styles%252C%2520and%2520detailed%2520body%2520shapes.%2520Capturing%2520and%2520annotating%250Alarge-scale%2520human%2520datasets%2520for%2520training%2520generative%2520models%2520is%2520prohibitively%250Aexpensive%2520and%2520limited%2520in%2520scale%2520and%2520diversity.%2520The%2520central%2520question%2520we%2520address%250Ain%2520this%2520paper%2520is%253A%2520Can%2520existing%2520foundation%2520models%2520be%2520distilled%2520to%2520generate%250Atheoretically%2520unbounded%252C%2520richly%2520annotated%25203D%2520human%2520data%253F%2520We%2520introduce%250AInfiniHuman%252C%2520a%2520framework%2520that%2520synergistically%2520distills%2520these%2520models%2520to%2520produce%250Arichly%2520annotated%2520human%2520data%2520at%2520minimal%2520cost%2520and%2520with%2520theoretically%2520unlimited%250Ascalability.%2520We%2520propose%2520InfiniHumanData%252C%2520a%2520fully%2520automatic%2520pipeline%2520that%250Aleverages%2520vision-language%2520and%2520image%2520generation%2520models%2520to%2520create%2520a%2520large-scale%250Amulti-modal%2520dataset.%2520User%2520study%2520shows%2520our%2520automatically%2520generated%2520identities%250Aare%2520undistinguishable%2520from%2520scan%2520renderings.%2520InfiniHumanData%2520contains%2520111K%250Aidentities%2520spanning%2520unprecedented%2520diversity.%2520Each%2520identity%2520is%2520annotated%2520with%250Amulti-granularity%2520text%2520descriptions%252C%2520multi-view%2520RGB%2520images%252C%2520detailed%2520clothing%250Aimages%252C%2520and%2520SMPL%2520body-shape%2520parameters.%2520Building%2520on%2520this%2520dataset%252C%2520we%2520propose%250AInfiniHumanGen%252C%2520a%2520diffusion-based%2520generative%2520pipeline%2520conditioned%2520on%2520text%252C%2520body%250Ashape%252C%2520and%2520clothing%2520assets.%2520InfiniHumanGen%2520enables%2520fast%252C%2520realistic%252C%2520and%250Aprecisely%2520controllable%2520avatar%2520generation.%2520Extensive%2520experiments%2520demonstrate%250Asignificant%2520improvements%2520over%2520state-of-the-art%2520methods%2520in%2520visual%2520quality%252C%250Ageneration%2520speed%252C%2520and%2520controllability.%2520Our%2520approach%2520enables%2520high-quality%2520avatar%250Ageneration%2520with%2520fine-grained%2520control%2520at%2520effectively%2520unbounded%2520scale%2520through%2520a%250Apractical%2520and%2520affordable%2520solution.%2520We%2520will%2520publicly%2520release%2520the%2520automatic%2520data%250Ageneration%2520pipeline%252C%2520the%2520comprehensive%2520InfiniHumanData%2520dataset%252C%2520and%2520the%250AInfiniHumanGen%2520models%2520at%2520https%253A//yuxuan-xue.com/infini-human.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiniHuman%3A%20Infinite%203D%20Human%20Creation%20with%20Precise%20Control&entry.906535625=Yuxuan%20Xue%20and%20Xianghui%20Xie%20and%20Margaret%20Kostyrko%20and%20Gerard%20Pons-Moll&entry.1292438233=%20%20Generating%20realistic%20and%20controllable%203D%20human%20avatars%20is%20a%20long-standing%0Achallenge%2C%20particularly%20when%20covering%20broad%20attribute%20ranges%20such%20as%20ethnicity%2C%0Aage%2C%20clothing%20styles%2C%20and%20detailed%20body%20shapes.%20Capturing%20and%20annotating%0Alarge-scale%20human%20datasets%20for%20training%20generative%20models%20is%20prohibitively%0Aexpensive%20and%20limited%20in%20scale%20and%20diversity.%20The%20central%20question%20we%20address%0Ain%20this%20paper%20is%3A%20Can%20existing%20foundation%20models%20be%20distilled%20to%20generate%0Atheoretically%20unbounded%2C%20richly%20annotated%203D%20human%20data%3F%20We%20introduce%0AInfiniHuman%2C%20a%20framework%20that%20synergistically%20distills%20these%20models%20to%20produce%0Arichly%20annotated%20human%20data%20at%20minimal%20cost%20and%20with%20theoretically%20unlimited%0Ascalability.%20We%20propose%20InfiniHumanData%2C%20a%20fully%20automatic%20pipeline%20that%0Aleverages%20vision-language%20and%20image%20generation%20models%20to%20create%20a%20large-scale%0Amulti-modal%20dataset.%20User%20study%20shows%20our%20automatically%20generated%20identities%0Aare%20undistinguishable%20from%20scan%20renderings.%20InfiniHumanData%20contains%20111K%0Aidentities%20spanning%20unprecedented%20diversity.%20Each%20identity%20is%20annotated%20with%0Amulti-granularity%20text%20descriptions%2C%20multi-view%20RGB%20images%2C%20detailed%20clothing%0Aimages%2C%20and%20SMPL%20body-shape%20parameters.%20Building%20on%20this%20dataset%2C%20we%20propose%0AInfiniHumanGen%2C%20a%20diffusion-based%20generative%20pipeline%20conditioned%20on%20text%2C%20body%0Ashape%2C%20and%20clothing%20assets.%20InfiniHumanGen%20enables%20fast%2C%20realistic%2C%20and%0Aprecisely%20controllable%20avatar%20generation.%20Extensive%20experiments%20demonstrate%0Asignificant%20improvements%20over%20state-of-the-art%20methods%20in%20visual%20quality%2C%0Ageneration%20speed%2C%20and%20controllability.%20Our%20approach%20enables%20high-quality%20avatar%0Ageneration%20with%20fine-grained%20control%20at%20effectively%20unbounded%20scale%20through%20a%0Apractical%20and%20affordable%20solution.%20We%20will%20publicly%20release%20the%20automatic%20data%0Ageneration%20pipeline%2C%20the%20comprehensive%20InfiniHumanData%20dataset%2C%20and%20the%0AInfiniHumanGen%20models%20at%20https%3A//yuxuan-xue.com/infini-human.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11650v1&entry.124074799=Read"},
{"title": "PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from\n  a Single Image", "author": "Pradyumna Yalandur Muralidhar and Yuxuan Xue and Xianghui Xie and Margaret Kostyrko and Gerard Pons-Moll", "abstract": "  Reconstructing metrically accurate humans and their surrounding scenes from a\nsingle image is crucial for virtual reality, robotics, and comprehensive 3D\nscene understanding. However, existing methods struggle with depth ambiguity,\nocclusions, and physically inconsistent contacts. To address these challenges,\nwe introduce PhySIC, a framework for physically plausible Human-Scene\nInteraction and Contact reconstruction. PhySIC recovers metrically consistent\nSMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within\na shared coordinate frame from a single RGB image. Starting from coarse\nmonocular depth and body estimates, PhySIC performs occlusion-aware inpainting,\nfuses visible depth with unscaled geometry for a robust metric scaffold, and\nsynthesizes missing support surfaces like floors. A confidence-weighted\noptimization refines body pose, camera parameters, and global scale by jointly\nenforcing depth alignment, contact priors, interpenetration avoidance, and 2D\nreprojection consistency. Explicit occlusion masking safeguards invisible\nregions against implausible configurations. PhySIC is efficient, requiring only\n9 seconds for joint human-scene optimization and under 27 seconds end-to-end.\nIt naturally handles multiple humans, enabling reconstruction of diverse\ninteractions. Empirically, PhySIC outperforms single-image baselines, reducing\nmean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,\nand improving contact F1 from 0.09 to 0.51. Qualitative results show realistic\nfoot-floor interactions, natural seating, and plausible reconstructions of\nheavily occluded furniture. By converting a single image into a physically\nplausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.\nOur implementation is publicly available at https://yuxuan-xue.com/physic.\n", "link": "http://arxiv.org/abs/2510.11649v1", "date": "2025-10-13", "relevancy": 3.1182, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6405}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6224}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhySIC%3A%20Physically%20Plausible%203D%20Human-Scene%20Interaction%20and%20Contact%20from%0A%20%20a%20Single%20Image&body=Title%3A%20PhySIC%3A%20Physically%20Plausible%203D%20Human-Scene%20Interaction%20and%20Contact%20from%0A%20%20a%20Single%20Image%0AAuthor%3A%20Pradyumna%20Yalandur%20Muralidhar%20and%20Yuxuan%20Xue%20and%20Xianghui%20Xie%20and%20Margaret%20Kostyrko%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20%20%20Reconstructing%20metrically%20accurate%20humans%20and%20their%20surrounding%20scenes%20from%20a%0Asingle%20image%20is%20crucial%20for%20virtual%20reality%2C%20robotics%2C%20and%20comprehensive%203D%0Ascene%20understanding.%20However%2C%20existing%20methods%20struggle%20with%20depth%20ambiguity%2C%0Aocclusions%2C%20and%20physically%20inconsistent%20contacts.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20PhySIC%2C%20a%20framework%20for%20physically%20plausible%20Human-Scene%0AInteraction%20and%20Contact%20reconstruction.%20PhySIC%20recovers%20metrically%20consistent%0ASMPL-X%20human%20meshes%2C%20dense%20scene%20surfaces%2C%20and%20vertex-level%20contact%20maps%20within%0Aa%20shared%20coordinate%20frame%20from%20a%20single%20RGB%20image.%20Starting%20from%20coarse%0Amonocular%20depth%20and%20body%20estimates%2C%20PhySIC%20performs%20occlusion-aware%20inpainting%2C%0Afuses%20visible%20depth%20with%20unscaled%20geometry%20for%20a%20robust%20metric%20scaffold%2C%20and%0Asynthesizes%20missing%20support%20surfaces%20like%20floors.%20A%20confidence-weighted%0Aoptimization%20refines%20body%20pose%2C%20camera%20parameters%2C%20and%20global%20scale%20by%20jointly%0Aenforcing%20depth%20alignment%2C%20contact%20priors%2C%20interpenetration%20avoidance%2C%20and%202D%0Areprojection%20consistency.%20Explicit%20occlusion%20masking%20safeguards%20invisible%0Aregions%20against%20implausible%20configurations.%20PhySIC%20is%20efficient%2C%20requiring%20only%0A9%20seconds%20for%20joint%20human-scene%20optimization%20and%20under%2027%20seconds%20end-to-end.%0AIt%20naturally%20handles%20multiple%20humans%2C%20enabling%20reconstruction%20of%20diverse%0Ainteractions.%20Empirically%2C%20PhySIC%20outperforms%20single-image%20baselines%2C%20reducing%0Amean%20per-vertex%20scene%20error%20from%20641%20mm%20to%20227%20mm%2C%20halving%20PA-MPJPE%20to%2042%20mm%2C%0Aand%20improving%20contact%20F1%20from%200.09%20to%200.51.%20Qualitative%20results%20show%20realistic%0Afoot-floor%20interactions%2C%20natural%20seating%2C%20and%20plausible%20reconstructions%20of%0Aheavily%20occluded%20furniture.%20By%20converting%20a%20single%20image%20into%20a%20physically%0Aplausible%203D%20human-scene%20pair%2C%20PhySIC%20advances%20scalable%203D%20scene%20understanding.%0AOur%20implementation%20is%20publicly%20available%20at%20https%3A//yuxuan-xue.com/physic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhySIC%253A%2520Physically%2520Plausible%25203D%2520Human-Scene%2520Interaction%2520and%2520Contact%2520from%250A%2520%2520a%2520Single%2520Image%26entry.906535625%3DPradyumna%2520Yalandur%2520Muralidhar%2520and%2520Yuxuan%2520Xue%2520and%2520Xianghui%2520Xie%2520and%2520Margaret%2520Kostyrko%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3D%2520%2520Reconstructing%2520metrically%2520accurate%2520humans%2520and%2520their%2520surrounding%2520scenes%2520from%2520a%250Asingle%2520image%2520is%2520crucial%2520for%2520virtual%2520reality%252C%2520robotics%252C%2520and%2520comprehensive%25203D%250Ascene%2520understanding.%2520However%252C%2520existing%2520methods%2520struggle%2520with%2520depth%2520ambiguity%252C%250Aocclusions%252C%2520and%2520physically%2520inconsistent%2520contacts.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520introduce%2520PhySIC%252C%2520a%2520framework%2520for%2520physically%2520plausible%2520Human-Scene%250AInteraction%2520and%2520Contact%2520reconstruction.%2520PhySIC%2520recovers%2520metrically%2520consistent%250ASMPL-X%2520human%2520meshes%252C%2520dense%2520scene%2520surfaces%252C%2520and%2520vertex-level%2520contact%2520maps%2520within%250Aa%2520shared%2520coordinate%2520frame%2520from%2520a%2520single%2520RGB%2520image.%2520Starting%2520from%2520coarse%250Amonocular%2520depth%2520and%2520body%2520estimates%252C%2520PhySIC%2520performs%2520occlusion-aware%2520inpainting%252C%250Afuses%2520visible%2520depth%2520with%2520unscaled%2520geometry%2520for%2520a%2520robust%2520metric%2520scaffold%252C%2520and%250Asynthesizes%2520missing%2520support%2520surfaces%2520like%2520floors.%2520A%2520confidence-weighted%250Aoptimization%2520refines%2520body%2520pose%252C%2520camera%2520parameters%252C%2520and%2520global%2520scale%2520by%2520jointly%250Aenforcing%2520depth%2520alignment%252C%2520contact%2520priors%252C%2520interpenetration%2520avoidance%252C%2520and%25202D%250Areprojection%2520consistency.%2520Explicit%2520occlusion%2520masking%2520safeguards%2520invisible%250Aregions%2520against%2520implausible%2520configurations.%2520PhySIC%2520is%2520efficient%252C%2520requiring%2520only%250A9%2520seconds%2520for%2520joint%2520human-scene%2520optimization%2520and%2520under%252027%2520seconds%2520end-to-end.%250AIt%2520naturally%2520handles%2520multiple%2520humans%252C%2520enabling%2520reconstruction%2520of%2520diverse%250Ainteractions.%2520Empirically%252C%2520PhySIC%2520outperforms%2520single-image%2520baselines%252C%2520reducing%250Amean%2520per-vertex%2520scene%2520error%2520from%2520641%2520mm%2520to%2520227%2520mm%252C%2520halving%2520PA-MPJPE%2520to%252042%2520mm%252C%250Aand%2520improving%2520contact%2520F1%2520from%25200.09%2520to%25200.51.%2520Qualitative%2520results%2520show%2520realistic%250Afoot-floor%2520interactions%252C%2520natural%2520seating%252C%2520and%2520plausible%2520reconstructions%2520of%250Aheavily%2520occluded%2520furniture.%2520By%2520converting%2520a%2520single%2520image%2520into%2520a%2520physically%250Aplausible%25203D%2520human-scene%2520pair%252C%2520PhySIC%2520advances%2520scalable%25203D%2520scene%2520understanding.%250AOur%2520implementation%2520is%2520publicly%2520available%2520at%2520https%253A//yuxuan-xue.com/physic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhySIC%3A%20Physically%20Plausible%203D%20Human-Scene%20Interaction%20and%20Contact%20from%0A%20%20a%20Single%20Image&entry.906535625=Pradyumna%20Yalandur%20Muralidhar%20and%20Yuxuan%20Xue%20and%20Xianghui%20Xie%20and%20Margaret%20Kostyrko%20and%20Gerard%20Pons-Moll&entry.1292438233=%20%20Reconstructing%20metrically%20accurate%20humans%20and%20their%20surrounding%20scenes%20from%20a%0Asingle%20image%20is%20crucial%20for%20virtual%20reality%2C%20robotics%2C%20and%20comprehensive%203D%0Ascene%20understanding.%20However%2C%20existing%20methods%20struggle%20with%20depth%20ambiguity%2C%0Aocclusions%2C%20and%20physically%20inconsistent%20contacts.%20To%20address%20these%20challenges%2C%0Awe%20introduce%20PhySIC%2C%20a%20framework%20for%20physically%20plausible%20Human-Scene%0AInteraction%20and%20Contact%20reconstruction.%20PhySIC%20recovers%20metrically%20consistent%0ASMPL-X%20human%20meshes%2C%20dense%20scene%20surfaces%2C%20and%20vertex-level%20contact%20maps%20within%0Aa%20shared%20coordinate%20frame%20from%20a%20single%20RGB%20image.%20Starting%20from%20coarse%0Amonocular%20depth%20and%20body%20estimates%2C%20PhySIC%20performs%20occlusion-aware%20inpainting%2C%0Afuses%20visible%20depth%20with%20unscaled%20geometry%20for%20a%20robust%20metric%20scaffold%2C%20and%0Asynthesizes%20missing%20support%20surfaces%20like%20floors.%20A%20confidence-weighted%0Aoptimization%20refines%20body%20pose%2C%20camera%20parameters%2C%20and%20global%20scale%20by%20jointly%0Aenforcing%20depth%20alignment%2C%20contact%20priors%2C%20interpenetration%20avoidance%2C%20and%202D%0Areprojection%20consistency.%20Explicit%20occlusion%20masking%20safeguards%20invisible%0Aregions%20against%20implausible%20configurations.%20PhySIC%20is%20efficient%2C%20requiring%20only%0A9%20seconds%20for%20joint%20human-scene%20optimization%20and%20under%2027%20seconds%20end-to-end.%0AIt%20naturally%20handles%20multiple%20humans%2C%20enabling%20reconstruction%20of%20diverse%0Ainteractions.%20Empirically%2C%20PhySIC%20outperforms%20single-image%20baselines%2C%20reducing%0Amean%20per-vertex%20scene%20error%20from%20641%20mm%20to%20227%20mm%2C%20halving%20PA-MPJPE%20to%2042%20mm%2C%0Aand%20improving%20contact%20F1%20from%200.09%20to%200.51.%20Qualitative%20results%20show%20realistic%0Afoot-floor%20interactions%2C%20natural%20seating%2C%20and%20plausible%20reconstructions%20of%0Aheavily%20occluded%20furniture.%20By%20converting%20a%20single%20image%20into%20a%20physically%0Aplausible%203D%20human-scene%20pair%2C%20PhySIC%20advances%20scalable%203D%20scene%20understanding.%0AOur%20implementation%20is%20publicly%20available%20at%20https%3A//yuxuan-xue.com/physic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11649v1&entry.124074799=Read"},
{"title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models", "author": "Yolo Yunlong Tang and Jing Bi and Pinxin Liu and Zhenyu Pan and Zhangyun Tan and Qianxiang Shen and Jiani Liu and Hang Hua and Junjia Guo and Yunzhong Xiao and Chao Huang and Zhiyuan Wang and Susan Liang and Xinyi Liu and Yizhi Song and Yuhe Nie and Jia-Xing Zhong and Bozheng Li and Daiqing Qi and Ziyun Zeng and Ali Vosoughi and Luchuan Song and Zeliang Zhang and Daiki Shimada and Han Liu and Jiebo Luo and Chenliang Xu", "abstract": "  Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training\n", "link": "http://arxiv.org/abs/2510.05034v4", "date": "2025-10-13", "relevancy": 3.0364, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6234}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-LMM%20Post-Training%3A%20A%20Deep%20Dive%20into%20Video%20Reasoning%20with%20Large%0A%20%20Multimodal%20Models&body=Title%3A%20Video-LMM%20Post-Training%3A%20A%20Deep%20Dive%20into%20Video%20Reasoning%20with%20Large%0A%20%20Multimodal%20Models%0AAuthor%3A%20Yolo%20Yunlong%20Tang%20and%20Jing%20Bi%20and%20Pinxin%20Liu%20and%20Zhenyu%20Pan%20and%20Zhangyun%20Tan%20and%20Qianxiang%20Shen%20and%20Jiani%20Liu%20and%20Hang%20Hua%20and%20Junjia%20Guo%20and%20Yunzhong%20Xiao%20and%20Chao%20Huang%20and%20Zhiyuan%20Wang%20and%20Susan%20Liang%20and%20Xinyi%20Liu%20and%20Yizhi%20Song%20and%20Yuhe%20Nie%20and%20Jia-Xing%20Zhong%20and%20Bozheng%20Li%20and%20Daiqing%20Qi%20and%20Ziyun%20Zeng%20and%20Ali%20Vosoughi%20and%20Luchuan%20Song%20and%20Zeliang%20Zhang%20and%20Daiki%20Shimada%20and%20Han%20Liu%20and%20Jiebo%20Luo%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Video%20understanding%20represents%20the%20most%20challenging%20frontier%20in%20computer%0Avision%2C%20requiring%20models%20to%20reason%20about%20complex%20spatiotemporal%20relationships%2C%0Along-term%20dependencies%2C%20and%20multimodal%20evidence.%20The%20recent%20emergence%20of%0AVideo-Large%20Multimodal%20Models%20%28Video-LMMs%29%2C%20which%20integrate%20visual%20encoders%0Awith%20powerful%20decoder-based%20language%20models%2C%20has%20demonstrated%20remarkable%0Acapabilities%20in%20video%20understanding%20tasks.%20However%2C%20the%20critical%20phase%20that%0Atransforms%20these%20models%20from%20basic%20perception%20systems%20into%20sophisticated%0Areasoning%20engines%2C%20post-training%2C%20remains%20fragmented%20across%20the%20literature.%0AThis%20survey%20provides%20the%20first%20comprehensive%20examination%20of%20post-training%0Amethodologies%20for%20Video-LMMs%2C%20encompassing%20three%20fundamental%20pillars%3A%0Asupervised%20fine-tuning%20%28SFT%29%20with%20chain-of-thought%2C%20reinforcement%20learning%20%28RL%29%0Afrom%20verifiable%20objectives%2C%20and%20test-time%20scaling%20%28TTS%29%20through%20enhanced%0Ainference%20computation.%20We%20present%20a%20structured%20taxonomy%20that%20clarifies%20the%0Aroles%2C%20interconnections%2C%20and%20video-specific%20adaptations%20of%20these%20techniques%2C%0Aaddressing%20unique%20challenges%20such%20as%20temporal%20localization%2C%20spatiotemporal%0Agrounding%2C%20long%20video%20efficiency%2C%20and%20multimodal%20evidence%20integration.%20Through%0Asystematic%20analysis%20of%20representative%20methods%2C%20we%20synthesize%20key%20design%0Aprinciples%2C%20insights%2C%20and%20evaluation%20protocols%20while%20identifying%20critical%20open%0Achallenges%20in%20reward%20design%2C%20scalability%2C%20and%20cost-performance%20optimization.%20We%0Afurther%20curate%20essential%20benchmarks%2C%20datasets%2C%20and%20metrics%20to%20facilitate%0Arigorous%20assessment%20of%20post-training%20effectiveness.%20This%20survey%20aims%20to%20provide%0Aresearchers%20and%20practitioners%20with%20a%20unified%20framework%20for%20advancing%20Video-LMM%0Acapabilities.%20Additional%20resources%20and%20updates%20are%20maintained%20at%3A%0Ahttps%3A//github.com/yunlong10/Awesome-Video-LMM-Post-Training%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05034v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-LMM%2520Post-Training%253A%2520A%2520Deep%2520Dive%2520into%2520Video%2520Reasoning%2520with%2520Large%250A%2520%2520Multimodal%2520Models%26entry.906535625%3DYolo%2520Yunlong%2520Tang%2520and%2520Jing%2520Bi%2520and%2520Pinxin%2520Liu%2520and%2520Zhenyu%2520Pan%2520and%2520Zhangyun%2520Tan%2520and%2520Qianxiang%2520Shen%2520and%2520Jiani%2520Liu%2520and%2520Hang%2520Hua%2520and%2520Junjia%2520Guo%2520and%2520Yunzhong%2520Xiao%2520and%2520Chao%2520Huang%2520and%2520Zhiyuan%2520Wang%2520and%2520Susan%2520Liang%2520and%2520Xinyi%2520Liu%2520and%2520Yizhi%2520Song%2520and%2520Yuhe%2520Nie%2520and%2520Jia-Xing%2520Zhong%2520and%2520Bozheng%2520Li%2520and%2520Daiqing%2520Qi%2520and%2520Ziyun%2520Zeng%2520and%2520Ali%2520Vosoughi%2520and%2520Luchuan%2520Song%2520and%2520Zeliang%2520Zhang%2520and%2520Daiki%2520Shimada%2520and%2520Han%2520Liu%2520and%2520Jiebo%2520Luo%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520Video%2520understanding%2520represents%2520the%2520most%2520challenging%2520frontier%2520in%2520computer%250Avision%252C%2520requiring%2520models%2520to%2520reason%2520about%2520complex%2520spatiotemporal%2520relationships%252C%250Along-term%2520dependencies%252C%2520and%2520multimodal%2520evidence.%2520The%2520recent%2520emergence%2520of%250AVideo-Large%2520Multimodal%2520Models%2520%2528Video-LMMs%2529%252C%2520which%2520integrate%2520visual%2520encoders%250Awith%2520powerful%2520decoder-based%2520language%2520models%252C%2520has%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520video%2520understanding%2520tasks.%2520However%252C%2520the%2520critical%2520phase%2520that%250Atransforms%2520these%2520models%2520from%2520basic%2520perception%2520systems%2520into%2520sophisticated%250Areasoning%2520engines%252C%2520post-training%252C%2520remains%2520fragmented%2520across%2520the%2520literature.%250AThis%2520survey%2520provides%2520the%2520first%2520comprehensive%2520examination%2520of%2520post-training%250Amethodologies%2520for%2520Video-LMMs%252C%2520encompassing%2520three%2520fundamental%2520pillars%253A%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520with%2520chain-of-thought%252C%2520reinforcement%2520learning%2520%2528RL%2529%250Afrom%2520verifiable%2520objectives%252C%2520and%2520test-time%2520scaling%2520%2528TTS%2529%2520through%2520enhanced%250Ainference%2520computation.%2520We%2520present%2520a%2520structured%2520taxonomy%2520that%2520clarifies%2520the%250Aroles%252C%2520interconnections%252C%2520and%2520video-specific%2520adaptations%2520of%2520these%2520techniques%252C%250Aaddressing%2520unique%2520challenges%2520such%2520as%2520temporal%2520localization%252C%2520spatiotemporal%250Agrounding%252C%2520long%2520video%2520efficiency%252C%2520and%2520multimodal%2520evidence%2520integration.%2520Through%250Asystematic%2520analysis%2520of%2520representative%2520methods%252C%2520we%2520synthesize%2520key%2520design%250Aprinciples%252C%2520insights%252C%2520and%2520evaluation%2520protocols%2520while%2520identifying%2520critical%2520open%250Achallenges%2520in%2520reward%2520design%252C%2520scalability%252C%2520and%2520cost-performance%2520optimization.%2520We%250Afurther%2520curate%2520essential%2520benchmarks%252C%2520datasets%252C%2520and%2520metrics%2520to%2520facilitate%250Arigorous%2520assessment%2520of%2520post-training%2520effectiveness.%2520This%2520survey%2520aims%2520to%2520provide%250Aresearchers%2520and%2520practitioners%2520with%2520a%2520unified%2520framework%2520for%2520advancing%2520Video-LMM%250Acapabilities.%2520Additional%2520resources%2520and%2520updates%2520are%2520maintained%2520at%253A%250Ahttps%253A//github.com/yunlong10/Awesome-Video-LMM-Post-Training%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05034v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-LMM%20Post-Training%3A%20A%20Deep%20Dive%20into%20Video%20Reasoning%20with%20Large%0A%20%20Multimodal%20Models&entry.906535625=Yolo%20Yunlong%20Tang%20and%20Jing%20Bi%20and%20Pinxin%20Liu%20and%20Zhenyu%20Pan%20and%20Zhangyun%20Tan%20and%20Qianxiang%20Shen%20and%20Jiani%20Liu%20and%20Hang%20Hua%20and%20Junjia%20Guo%20and%20Yunzhong%20Xiao%20and%20Chao%20Huang%20and%20Zhiyuan%20Wang%20and%20Susan%20Liang%20and%20Xinyi%20Liu%20and%20Yizhi%20Song%20and%20Yuhe%20Nie%20and%20Jia-Xing%20Zhong%20and%20Bozheng%20Li%20and%20Daiqing%20Qi%20and%20Ziyun%20Zeng%20and%20Ali%20Vosoughi%20and%20Luchuan%20Song%20and%20Zeliang%20Zhang%20and%20Daiki%20Shimada%20and%20Han%20Liu%20and%20Jiebo%20Luo%20and%20Chenliang%20Xu&entry.1292438233=%20%20Video%20understanding%20represents%20the%20most%20challenging%20frontier%20in%20computer%0Avision%2C%20requiring%20models%20to%20reason%20about%20complex%20spatiotemporal%20relationships%2C%0Along-term%20dependencies%2C%20and%20multimodal%20evidence.%20The%20recent%20emergence%20of%0AVideo-Large%20Multimodal%20Models%20%28Video-LMMs%29%2C%20which%20integrate%20visual%20encoders%0Awith%20powerful%20decoder-based%20language%20models%2C%20has%20demonstrated%20remarkable%0Acapabilities%20in%20video%20understanding%20tasks.%20However%2C%20the%20critical%20phase%20that%0Atransforms%20these%20models%20from%20basic%20perception%20systems%20into%20sophisticated%0Areasoning%20engines%2C%20post-training%2C%20remains%20fragmented%20across%20the%20literature.%0AThis%20survey%20provides%20the%20first%20comprehensive%20examination%20of%20post-training%0Amethodologies%20for%20Video-LMMs%2C%20encompassing%20three%20fundamental%20pillars%3A%0Asupervised%20fine-tuning%20%28SFT%29%20with%20chain-of-thought%2C%20reinforcement%20learning%20%28RL%29%0Afrom%20verifiable%20objectives%2C%20and%20test-time%20scaling%20%28TTS%29%20through%20enhanced%0Ainference%20computation.%20We%20present%20a%20structured%20taxonomy%20that%20clarifies%20the%0Aroles%2C%20interconnections%2C%20and%20video-specific%20adaptations%20of%20these%20techniques%2C%0Aaddressing%20unique%20challenges%20such%20as%20temporal%20localization%2C%20spatiotemporal%0Agrounding%2C%20long%20video%20efficiency%2C%20and%20multimodal%20evidence%20integration.%20Through%0Asystematic%20analysis%20of%20representative%20methods%2C%20we%20synthesize%20key%20design%0Aprinciples%2C%20insights%2C%20and%20evaluation%20protocols%20while%20identifying%20critical%20open%0Achallenges%20in%20reward%20design%2C%20scalability%2C%20and%20cost-performance%20optimization.%20We%0Afurther%20curate%20essential%20benchmarks%2C%20datasets%2C%20and%20metrics%20to%20facilitate%0Arigorous%20assessment%20of%20post-training%20effectiveness.%20This%20survey%20aims%20to%20provide%0Aresearchers%20and%20practitioners%20with%20a%20unified%20framework%20for%20advancing%20Video-LMM%0Acapabilities.%20Additional%20resources%20and%20updates%20are%20maintained%20at%3A%0Ahttps%3A//github.com/yunlong10/Awesome-Video-LMM-Post-Training%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05034v4&entry.124074799=Read"},
{"title": "Scaling Language-Centric Omnimodal Representation Learning", "author": "Chenghao Xiao and Hou Pong Chan and Hao Zhang and Weiwen Xu and Mahani Aljunied and Yu Rong", "abstract": "  Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.\n", "link": "http://arxiv.org/abs/2510.11693v1", "date": "2025-10-13", "relevancy": 2.899, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.607}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Language-Centric%20Omnimodal%20Representation%20Learning&body=Title%3A%20Scaling%20Language-Centric%20Omnimodal%20Representation%20Learning%0AAuthor%3A%20Chenghao%20Xiao%20and%20Hou%20Pong%20Chan%20and%20Hao%20Zhang%20and%20Weiwen%20Xu%20and%20Mahani%20Aljunied%20and%20Yu%20Rong%0AAbstract%3A%20%20%20Recent%20multimodal%20embedding%20approaches%20leveraging%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20fine-tuned%20with%20contrastive%20learning%20%28CL%29%20have%20shown%20promising%0Aresults%2C%20yet%20the%20underlying%20reasons%20behind%20their%20superiority%20remain%0Aunderexplored.%20This%20work%20argues%20that%20a%20crucial%20advantage%20of%20MLLM-based%0Aapproaches%20stems%20from%20implicit%20cross-modal%20alignment%20achieved%20during%20generative%0Apretraining%2C%20where%20the%20language%20decoder%20learns%20to%20exploit%20multimodal%20signals%0Awithin%20a%20shared%20representation%20space%20for%20generating%20unimodal%20outputs.%20Through%0Aanalysis%20of%20anisotropy%20and%20kernel%20similarity%20structure%2C%20we%20empirically%20confirm%0Athat%20latent%20alignment%20emerges%20within%20MLLM%20representations%2C%20allowing%20CL%20to%20serve%0Aas%20a%20lightweight%20refinement%20stage.%20Leveraging%20this%20insight%2C%20we%20propose%20a%0ALanguage-Centric%20Omnimodal%20Embedding%20framework%2C%20termed%20LCO-Emb.%20Extensive%0Aexperiments%20across%20diverse%20backbones%20and%20benchmarks%20demonstrate%20its%0Aeffectiveness%2C%20achieving%20state-of-the-art%20performance%20across%20modalities.%0AFurthermore%2C%20we%20identify%20a%20Generation-Representation%20Scaling%20Law%20%28GRSL%29%2C%0Ashowing%20that%20the%20representational%20capabilities%20gained%20through%20contrastive%0Arefinement%20scales%20positively%20with%20the%20MLLM%27s%20generative%20capabilities.%20This%0Asuggests%20that%20improving%20generative%20abilities%20evolves%20as%20an%20effective%20paradigm%0Afor%20enhancing%20representation%20quality.%20We%20provide%20a%20theoretical%20explanation%20of%0AGRSL%2C%20which%20formally%20links%20the%20MLLM%27s%20generative%20quality%20to%20the%20upper%20bound%20on%0Aits%20representation%20performance%2C%20and%20validate%20it%20on%20a%20challenging%2C%20low-resource%0Avisual-document%20retrieval%20task%2C%20showing%20that%20continual%20generative%20pretraining%0Abefore%20CL%20can%20further%20enhance%20the%20potential%20of%20a%20model%27s%20embedding%0Acapabilities.%20Codes%2C%20models%2C%20and%20resources%20are%20available%20at%0Ahttps%3A//github.com/LCO-Embedding/LCO-Embedding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Language-Centric%2520Omnimodal%2520Representation%2520Learning%26entry.906535625%3DChenghao%2520Xiao%2520and%2520Hou%2520Pong%2520Chan%2520and%2520Hao%2520Zhang%2520and%2520Weiwen%2520Xu%2520and%2520Mahani%2520Aljunied%2520and%2520Yu%2520Rong%26entry.1292438233%3D%2520%2520Recent%2520multimodal%2520embedding%2520approaches%2520leveraging%2520multimodal%2520large%2520language%250Amodels%2520%2528MLLMs%2529%2520fine-tuned%2520with%2520contrastive%2520learning%2520%2528CL%2529%2520have%2520shown%2520promising%250Aresults%252C%2520yet%2520the%2520underlying%2520reasons%2520behind%2520their%2520superiority%2520remain%250Aunderexplored.%2520This%2520work%2520argues%2520that%2520a%2520crucial%2520advantage%2520of%2520MLLM-based%250Aapproaches%2520stems%2520from%2520implicit%2520cross-modal%2520alignment%2520achieved%2520during%2520generative%250Apretraining%252C%2520where%2520the%2520language%2520decoder%2520learns%2520to%2520exploit%2520multimodal%2520signals%250Awithin%2520a%2520shared%2520representation%2520space%2520for%2520generating%2520unimodal%2520outputs.%2520Through%250Aanalysis%2520of%2520anisotropy%2520and%2520kernel%2520similarity%2520structure%252C%2520we%2520empirically%2520confirm%250Athat%2520latent%2520alignment%2520emerges%2520within%2520MLLM%2520representations%252C%2520allowing%2520CL%2520to%2520serve%250Aas%2520a%2520lightweight%2520refinement%2520stage.%2520Leveraging%2520this%2520insight%252C%2520we%2520propose%2520a%250ALanguage-Centric%2520Omnimodal%2520Embedding%2520framework%252C%2520termed%2520LCO-Emb.%2520Extensive%250Aexperiments%2520across%2520diverse%2520backbones%2520and%2520benchmarks%2520demonstrate%2520its%250Aeffectiveness%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520modalities.%250AFurthermore%252C%2520we%2520identify%2520a%2520Generation-Representation%2520Scaling%2520Law%2520%2528GRSL%2529%252C%250Ashowing%2520that%2520the%2520representational%2520capabilities%2520gained%2520through%2520contrastive%250Arefinement%2520scales%2520positively%2520with%2520the%2520MLLM%2527s%2520generative%2520capabilities.%2520This%250Asuggests%2520that%2520improving%2520generative%2520abilities%2520evolves%2520as%2520an%2520effective%2520paradigm%250Afor%2520enhancing%2520representation%2520quality.%2520We%2520provide%2520a%2520theoretical%2520explanation%2520of%250AGRSL%252C%2520which%2520formally%2520links%2520the%2520MLLM%2527s%2520generative%2520quality%2520to%2520the%2520upper%2520bound%2520on%250Aits%2520representation%2520performance%252C%2520and%2520validate%2520it%2520on%2520a%2520challenging%252C%2520low-resource%250Avisual-document%2520retrieval%2520task%252C%2520showing%2520that%2520continual%2520generative%2520pretraining%250Abefore%2520CL%2520can%2520further%2520enhance%2520the%2520potential%2520of%2520a%2520model%2527s%2520embedding%250Acapabilities.%2520Codes%252C%2520models%252C%2520and%2520resources%2520are%2520available%2520at%250Ahttps%253A//github.com/LCO-Embedding/LCO-Embedding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Language-Centric%20Omnimodal%20Representation%20Learning&entry.906535625=Chenghao%20Xiao%20and%20Hou%20Pong%20Chan%20and%20Hao%20Zhang%20and%20Weiwen%20Xu%20and%20Mahani%20Aljunied%20and%20Yu%20Rong&entry.1292438233=%20%20Recent%20multimodal%20embedding%20approaches%20leveraging%20multimodal%20large%20language%0Amodels%20%28MLLMs%29%20fine-tuned%20with%20contrastive%20learning%20%28CL%29%20have%20shown%20promising%0Aresults%2C%20yet%20the%20underlying%20reasons%20behind%20their%20superiority%20remain%0Aunderexplored.%20This%20work%20argues%20that%20a%20crucial%20advantage%20of%20MLLM-based%0Aapproaches%20stems%20from%20implicit%20cross-modal%20alignment%20achieved%20during%20generative%0Apretraining%2C%20where%20the%20language%20decoder%20learns%20to%20exploit%20multimodal%20signals%0Awithin%20a%20shared%20representation%20space%20for%20generating%20unimodal%20outputs.%20Through%0Aanalysis%20of%20anisotropy%20and%20kernel%20similarity%20structure%2C%20we%20empirically%20confirm%0Athat%20latent%20alignment%20emerges%20within%20MLLM%20representations%2C%20allowing%20CL%20to%20serve%0Aas%20a%20lightweight%20refinement%20stage.%20Leveraging%20this%20insight%2C%20we%20propose%20a%0ALanguage-Centric%20Omnimodal%20Embedding%20framework%2C%20termed%20LCO-Emb.%20Extensive%0Aexperiments%20across%20diverse%20backbones%20and%20benchmarks%20demonstrate%20its%0Aeffectiveness%2C%20achieving%20state-of-the-art%20performance%20across%20modalities.%0AFurthermore%2C%20we%20identify%20a%20Generation-Representation%20Scaling%20Law%20%28GRSL%29%2C%0Ashowing%20that%20the%20representational%20capabilities%20gained%20through%20contrastive%0Arefinement%20scales%20positively%20with%20the%20MLLM%27s%20generative%20capabilities.%20This%0Asuggests%20that%20improving%20generative%20abilities%20evolves%20as%20an%20effective%20paradigm%0Afor%20enhancing%20representation%20quality.%20We%20provide%20a%20theoretical%20explanation%20of%0AGRSL%2C%20which%20formally%20links%20the%20MLLM%27s%20generative%20quality%20to%20the%20upper%20bound%20on%0Aits%20representation%20performance%2C%20and%20validate%20it%20on%20a%20challenging%2C%20low-resource%0Avisual-document%20retrieval%20task%2C%20showing%20that%20continual%20generative%20pretraining%0Abefore%20CL%20can%20further%20enhance%20the%20potential%20of%20a%20model%27s%20embedding%0Acapabilities.%20Codes%2C%20models%2C%20and%20resources%20are%20available%20at%0Ahttps%3A//github.com/LCO-Embedding/LCO-Embedding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11693v1&entry.124074799=Read"},
{"title": "SNAP: Towards Segmenting Anything in Any Point Cloud", "author": "Aniket Gupta and Hanhui Wang and Charles Saunders and Aruni RoyChowdhury and Hanumant Singh and Huaizu Jiang", "abstract": "  Interactive 3D point cloud segmentation enables efficient annotation of\ncomplex 3D scenes through user-guided prompts. However, current approaches are\ntypically restricted in scope to a single domain (indoor or outdoor), and to a\nsingle form of user interaction (either spatial clicks or textual prompts).\nMoreover, training on multiple datasets often leads to negative transfer,\nresulting in domain-specific tools that lack generalizability. To address these\nlimitations, we present \\textbf{SNAP} (\\textbf{S}egment a\\textbf{N}ything in\n\\textbf{A}ny \\textbf{P}oint cloud), a unified model for interactive 3D\nsegmentation that supports both point-based and text-based prompts across\ndiverse domains. Our approach achieves cross-domain generalizability by\ntraining on 7 datasets spanning indoor, outdoor, and aerial environments, while\nemploying domain-adaptive normalization to prevent negative transfer. For\ntext-prompted segmentation, we automatically generate mask proposals without\nhuman intervention and match them against CLIP embeddings of textual queries,\nenabling both panoptic and open-vocabulary segmentation. Extensive experiments\ndemonstrate that SNAP consistently delivers high-quality segmentation results.\nWe achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for\nspatial-prompted segmentation and demonstrate competitive results on all 5\ntext-prompted benchmarks. These results show that a unified model can match or\nexceed specialized domain-specific approaches, providing a practical tool for\nscalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/\n", "link": "http://arxiv.org/abs/2510.11565v1", "date": "2025-10-13", "relevancy": 2.8398, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5749}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5645}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SNAP%3A%20Towards%20Segmenting%20Anything%20in%20Any%20Point%20Cloud&body=Title%3A%20SNAP%3A%20Towards%20Segmenting%20Anything%20in%20Any%20Point%20Cloud%0AAuthor%3A%20Aniket%20Gupta%20and%20Hanhui%20Wang%20and%20Charles%20Saunders%20and%20Aruni%20RoyChowdhury%20and%20Hanumant%20Singh%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20Interactive%203D%20point%20cloud%20segmentation%20enables%20efficient%20annotation%20of%0Acomplex%203D%20scenes%20through%20user-guided%20prompts.%20However%2C%20current%20approaches%20are%0Atypically%20restricted%20in%20scope%20to%20a%20single%20domain%20%28indoor%20or%20outdoor%29%2C%20and%20to%20a%0Asingle%20form%20of%20user%20interaction%20%28either%20spatial%20clicks%20or%20textual%20prompts%29.%0AMoreover%2C%20training%20on%20multiple%20datasets%20often%20leads%20to%20negative%20transfer%2C%0Aresulting%20in%20domain-specific%20tools%20that%20lack%20generalizability.%20To%20address%20these%0Alimitations%2C%20we%20present%20%5Ctextbf%7BSNAP%7D%20%28%5Ctextbf%7BS%7Degment%20a%5Ctextbf%7BN%7Dything%20in%0A%5Ctextbf%7BA%7Dny%20%5Ctextbf%7BP%7Doint%20cloud%29%2C%20a%20unified%20model%20for%20interactive%203D%0Asegmentation%20that%20supports%20both%20point-based%20and%20text-based%20prompts%20across%0Adiverse%20domains.%20Our%20approach%20achieves%20cross-domain%20generalizability%20by%0Atraining%20on%207%20datasets%20spanning%20indoor%2C%20outdoor%2C%20and%20aerial%20environments%2C%20while%0Aemploying%20domain-adaptive%20normalization%20to%20prevent%20negative%20transfer.%20For%0Atext-prompted%20segmentation%2C%20we%20automatically%20generate%20mask%20proposals%20without%0Ahuman%20intervention%20and%20match%20them%20against%20CLIP%20embeddings%20of%20textual%20queries%2C%0Aenabling%20both%20panoptic%20and%20open-vocabulary%20segmentation.%20Extensive%20experiments%0Ademonstrate%20that%20SNAP%20consistently%20delivers%20high-quality%20segmentation%20results.%0AWe%20achieve%20state-of-the-art%20performance%20on%208%20out%20of%209%20zero-shot%20benchmarks%20for%0Aspatial-prompted%20segmentation%20and%20demonstrate%20competitive%20results%20on%20all%205%0Atext-prompted%20benchmarks.%20These%20results%20show%20that%20a%20unified%20model%20can%20match%20or%0Aexceed%20specialized%20domain-specific%20approaches%2C%20providing%20a%20practical%20tool%20for%0Ascalable%203D%20annotation.%20Project%20page%20is%20at%2C%20https%3A//neu-vi.github.io/SNAP/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11565v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSNAP%253A%2520Towards%2520Segmenting%2520Anything%2520in%2520Any%2520Point%2520Cloud%26entry.906535625%3DAniket%2520Gupta%2520and%2520Hanhui%2520Wang%2520and%2520Charles%2520Saunders%2520and%2520Aruni%2520RoyChowdhury%2520and%2520Hanumant%2520Singh%2520and%2520Huaizu%2520Jiang%26entry.1292438233%3D%2520%2520Interactive%25203D%2520point%2520cloud%2520segmentation%2520enables%2520efficient%2520annotation%2520of%250Acomplex%25203D%2520scenes%2520through%2520user-guided%2520prompts.%2520However%252C%2520current%2520approaches%2520are%250Atypically%2520restricted%2520in%2520scope%2520to%2520a%2520single%2520domain%2520%2528indoor%2520or%2520outdoor%2529%252C%2520and%2520to%2520a%250Asingle%2520form%2520of%2520user%2520interaction%2520%2528either%2520spatial%2520clicks%2520or%2520textual%2520prompts%2529.%250AMoreover%252C%2520training%2520on%2520multiple%2520datasets%2520often%2520leads%2520to%2520negative%2520transfer%252C%250Aresulting%2520in%2520domain-specific%2520tools%2520that%2520lack%2520generalizability.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520present%2520%255Ctextbf%257BSNAP%257D%2520%2528%255Ctextbf%257BS%257Degment%2520a%255Ctextbf%257BN%257Dything%2520in%250A%255Ctextbf%257BA%257Dny%2520%255Ctextbf%257BP%257Doint%2520cloud%2529%252C%2520a%2520unified%2520model%2520for%2520interactive%25203D%250Asegmentation%2520that%2520supports%2520both%2520point-based%2520and%2520text-based%2520prompts%2520across%250Adiverse%2520domains.%2520Our%2520approach%2520achieves%2520cross-domain%2520generalizability%2520by%250Atraining%2520on%25207%2520datasets%2520spanning%2520indoor%252C%2520outdoor%252C%2520and%2520aerial%2520environments%252C%2520while%250Aemploying%2520domain-adaptive%2520normalization%2520to%2520prevent%2520negative%2520transfer.%2520For%250Atext-prompted%2520segmentation%252C%2520we%2520automatically%2520generate%2520mask%2520proposals%2520without%250Ahuman%2520intervention%2520and%2520match%2520them%2520against%2520CLIP%2520embeddings%2520of%2520textual%2520queries%252C%250Aenabling%2520both%2520panoptic%2520and%2520open-vocabulary%2520segmentation.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520SNAP%2520consistently%2520delivers%2520high-quality%2520segmentation%2520results.%250AWe%2520achieve%2520state-of-the-art%2520performance%2520on%25208%2520out%2520of%25209%2520zero-shot%2520benchmarks%2520for%250Aspatial-prompted%2520segmentation%2520and%2520demonstrate%2520competitive%2520results%2520on%2520all%25205%250Atext-prompted%2520benchmarks.%2520These%2520results%2520show%2520that%2520a%2520unified%2520model%2520can%2520match%2520or%250Aexceed%2520specialized%2520domain-specific%2520approaches%252C%2520providing%2520a%2520practical%2520tool%2520for%250Ascalable%25203D%2520annotation.%2520Project%2520page%2520is%2520at%252C%2520https%253A//neu-vi.github.io/SNAP/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11565v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SNAP%3A%20Towards%20Segmenting%20Anything%20in%20Any%20Point%20Cloud&entry.906535625=Aniket%20Gupta%20and%20Hanhui%20Wang%20and%20Charles%20Saunders%20and%20Aruni%20RoyChowdhury%20and%20Hanumant%20Singh%20and%20Huaizu%20Jiang&entry.1292438233=%20%20Interactive%203D%20point%20cloud%20segmentation%20enables%20efficient%20annotation%20of%0Acomplex%203D%20scenes%20through%20user-guided%20prompts.%20However%2C%20current%20approaches%20are%0Atypically%20restricted%20in%20scope%20to%20a%20single%20domain%20%28indoor%20or%20outdoor%29%2C%20and%20to%20a%0Asingle%20form%20of%20user%20interaction%20%28either%20spatial%20clicks%20or%20textual%20prompts%29.%0AMoreover%2C%20training%20on%20multiple%20datasets%20often%20leads%20to%20negative%20transfer%2C%0Aresulting%20in%20domain-specific%20tools%20that%20lack%20generalizability.%20To%20address%20these%0Alimitations%2C%20we%20present%20%5Ctextbf%7BSNAP%7D%20%28%5Ctextbf%7BS%7Degment%20a%5Ctextbf%7BN%7Dything%20in%0A%5Ctextbf%7BA%7Dny%20%5Ctextbf%7BP%7Doint%20cloud%29%2C%20a%20unified%20model%20for%20interactive%203D%0Asegmentation%20that%20supports%20both%20point-based%20and%20text-based%20prompts%20across%0Adiverse%20domains.%20Our%20approach%20achieves%20cross-domain%20generalizability%20by%0Atraining%20on%207%20datasets%20spanning%20indoor%2C%20outdoor%2C%20and%20aerial%20environments%2C%20while%0Aemploying%20domain-adaptive%20normalization%20to%20prevent%20negative%20transfer.%20For%0Atext-prompted%20segmentation%2C%20we%20automatically%20generate%20mask%20proposals%20without%0Ahuman%20intervention%20and%20match%20them%20against%20CLIP%20embeddings%20of%20textual%20queries%2C%0Aenabling%20both%20panoptic%20and%20open-vocabulary%20segmentation.%20Extensive%20experiments%0Ademonstrate%20that%20SNAP%20consistently%20delivers%20high-quality%20segmentation%20results.%0AWe%20achieve%20state-of-the-art%20performance%20on%208%20out%20of%209%20zero-shot%20benchmarks%20for%0Aspatial-prompted%20segmentation%20and%20demonstrate%20competitive%20results%20on%20all%205%0Atext-prompted%20benchmarks.%20These%20results%20show%20that%20a%20unified%20model%20can%20match%20or%0Aexceed%20specialized%20domain-specific%20approaches%2C%20providing%20a%20practical%20tool%20for%0Ascalable%203D%20annotation.%20Project%20page%20is%20at%2C%20https%3A//neu-vi.github.io/SNAP/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11565v1&entry.124074799=Read"},
{"title": "InstructSAM: A Training-Free Framework for Instruction-Oriented Remote\n  Sensing Object Recognition", "author": "Yijie Zheng and Weijie Wu and Qingyun Li and Xuehui Wang and Xu Zhou and Aiai Ren and Jun Shen and Long Zhao and Guoqing Li and Xue Yang", "abstract": "  Language-Guided object recognition in remote sensing imagery is crucial for\nlarge-scale mapping and automated data annotation. However, existing\nopen-vocabulary and visual grounding methods rely on explicit category cues,\nlimiting their ability to handle complex or implicit queries that require\nadvanced reasoning. To address this issue, we introduce a new suite of tasks,\nincluding Instruction-Oriented Object Counting, Detection, and Segmentation\n(InstructCDS), covering open-vocabulary, open-ended, and open-subclass\nscenarios. We further present EarthInstruct, the first InstructCDS benchmark\nfor earth observation. It is constructed from two diverse remote sensing\ndatasets with varying spatial resolutions and annotation rules across 20\ncategories, necessitating models to interpret dataset-specific instructions.\nGiven the scarcity of semantically rich labeled data in remote sensing, we\npropose InstructSAM, a training-free framework for instruction-driven object\nrecognition. InstructSAM leverages large vision-language models to interpret\nuser instructions and estimate object counts, employs SAM2 for mask proposal,\nand formulates mask-label assignment as a binary integer programming problem.\nBy integrating semantic similarity with counting constraints, InstructSAM\nefficiently assigns categories to predicted masks without relying on confidence\nthresholds. Experiments demonstrate that InstructSAM matches or surpasses\nspecialized baselines across multiple tasks while maintaining near-constant\ninference time regardless of object count, reducing output tokens by 89% and\noverall runtime by over 32% compared to direct generation approaches. We\nbelieve the contributions of the proposed tasks, benchmark, and effective\napproach will advance future research in developing versatile object\nrecognition systems.\n", "link": "http://arxiv.org/abs/2505.15818v2", "date": "2025-10-13", "relevancy": 2.8046, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5427}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstructSAM%3A%20A%20Training-Free%20Framework%20for%20Instruction-Oriented%20Remote%0A%20%20Sensing%20Object%20Recognition&body=Title%3A%20InstructSAM%3A%20A%20Training-Free%20Framework%20for%20Instruction-Oriented%20Remote%0A%20%20Sensing%20Object%20Recognition%0AAuthor%3A%20Yijie%20Zheng%20and%20Weijie%20Wu%20and%20Qingyun%20Li%20and%20Xuehui%20Wang%20and%20Xu%20Zhou%20and%20Aiai%20Ren%20and%20Jun%20Shen%20and%20Long%20Zhao%20and%20Guoqing%20Li%20and%20Xue%20Yang%0AAbstract%3A%20%20%20Language-Guided%20object%20recognition%20in%20remote%20sensing%20imagery%20is%20crucial%20for%0Alarge-scale%20mapping%20and%20automated%20data%20annotation.%20However%2C%20existing%0Aopen-vocabulary%20and%20visual%20grounding%20methods%20rely%20on%20explicit%20category%20cues%2C%0Alimiting%20their%20ability%20to%20handle%20complex%20or%20implicit%20queries%20that%20require%0Aadvanced%20reasoning.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20new%20suite%20of%20tasks%2C%0Aincluding%20Instruction-Oriented%20Object%20Counting%2C%20Detection%2C%20and%20Segmentation%0A%28InstructCDS%29%2C%20covering%20open-vocabulary%2C%20open-ended%2C%20and%20open-subclass%0Ascenarios.%20We%20further%20present%20EarthInstruct%2C%20the%20first%20InstructCDS%20benchmark%0Afor%20earth%20observation.%20It%20is%20constructed%20from%20two%20diverse%20remote%20sensing%0Adatasets%20with%20varying%20spatial%20resolutions%20and%20annotation%20rules%20across%2020%0Acategories%2C%20necessitating%20models%20to%20interpret%20dataset-specific%20instructions.%0AGiven%20the%20scarcity%20of%20semantically%20rich%20labeled%20data%20in%20remote%20sensing%2C%20we%0Apropose%20InstructSAM%2C%20a%20training-free%20framework%20for%20instruction-driven%20object%0Arecognition.%20InstructSAM%20leverages%20large%20vision-language%20models%20to%20interpret%0Auser%20instructions%20and%20estimate%20object%20counts%2C%20employs%20SAM2%20for%20mask%20proposal%2C%0Aand%20formulates%20mask-label%20assignment%20as%20a%20binary%20integer%20programming%20problem.%0ABy%20integrating%20semantic%20similarity%20with%20counting%20constraints%2C%20InstructSAM%0Aefficiently%20assigns%20categories%20to%20predicted%20masks%20without%20relying%20on%20confidence%0Athresholds.%20Experiments%20demonstrate%20that%20InstructSAM%20matches%20or%20surpasses%0Aspecialized%20baselines%20across%20multiple%20tasks%20while%20maintaining%20near-constant%0Ainference%20time%20regardless%20of%20object%20count%2C%20reducing%20output%20tokens%20by%2089%25%20and%0Aoverall%20runtime%20by%20over%2032%25%20compared%20to%20direct%20generation%20approaches.%20We%0Abelieve%20the%20contributions%20of%20the%20proposed%20tasks%2C%20benchmark%2C%20and%20effective%0Aapproach%20will%20advance%20future%20research%20in%20developing%20versatile%20object%0Arecognition%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.15818v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstructSAM%253A%2520A%2520Training-Free%2520Framework%2520for%2520Instruction-Oriented%2520Remote%250A%2520%2520Sensing%2520Object%2520Recognition%26entry.906535625%3DYijie%2520Zheng%2520and%2520Weijie%2520Wu%2520and%2520Qingyun%2520Li%2520and%2520Xuehui%2520Wang%2520and%2520Xu%2520Zhou%2520and%2520Aiai%2520Ren%2520and%2520Jun%2520Shen%2520and%2520Long%2520Zhao%2520and%2520Guoqing%2520Li%2520and%2520Xue%2520Yang%26entry.1292438233%3D%2520%2520Language-Guided%2520object%2520recognition%2520in%2520remote%2520sensing%2520imagery%2520is%2520crucial%2520for%250Alarge-scale%2520mapping%2520and%2520automated%2520data%2520annotation.%2520However%252C%2520existing%250Aopen-vocabulary%2520and%2520visual%2520grounding%2520methods%2520rely%2520on%2520explicit%2520category%2520cues%252C%250Alimiting%2520their%2520ability%2520to%2520handle%2520complex%2520or%2520implicit%2520queries%2520that%2520require%250Aadvanced%2520reasoning.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520new%2520suite%2520of%2520tasks%252C%250Aincluding%2520Instruction-Oriented%2520Object%2520Counting%252C%2520Detection%252C%2520and%2520Segmentation%250A%2528InstructCDS%2529%252C%2520covering%2520open-vocabulary%252C%2520open-ended%252C%2520and%2520open-subclass%250Ascenarios.%2520We%2520further%2520present%2520EarthInstruct%252C%2520the%2520first%2520InstructCDS%2520benchmark%250Afor%2520earth%2520observation.%2520It%2520is%2520constructed%2520from%2520two%2520diverse%2520remote%2520sensing%250Adatasets%2520with%2520varying%2520spatial%2520resolutions%2520and%2520annotation%2520rules%2520across%252020%250Acategories%252C%2520necessitating%2520models%2520to%2520interpret%2520dataset-specific%2520instructions.%250AGiven%2520the%2520scarcity%2520of%2520semantically%2520rich%2520labeled%2520data%2520in%2520remote%2520sensing%252C%2520we%250Apropose%2520InstructSAM%252C%2520a%2520training-free%2520framework%2520for%2520instruction-driven%2520object%250Arecognition.%2520InstructSAM%2520leverages%2520large%2520vision-language%2520models%2520to%2520interpret%250Auser%2520instructions%2520and%2520estimate%2520object%2520counts%252C%2520employs%2520SAM2%2520for%2520mask%2520proposal%252C%250Aand%2520formulates%2520mask-label%2520assignment%2520as%2520a%2520binary%2520integer%2520programming%2520problem.%250ABy%2520integrating%2520semantic%2520similarity%2520with%2520counting%2520constraints%252C%2520InstructSAM%250Aefficiently%2520assigns%2520categories%2520to%2520predicted%2520masks%2520without%2520relying%2520on%2520confidence%250Athresholds.%2520Experiments%2520demonstrate%2520that%2520InstructSAM%2520matches%2520or%2520surpasses%250Aspecialized%2520baselines%2520across%2520multiple%2520tasks%2520while%2520maintaining%2520near-constant%250Ainference%2520time%2520regardless%2520of%2520object%2520count%252C%2520reducing%2520output%2520tokens%2520by%252089%2525%2520and%250Aoverall%2520runtime%2520by%2520over%252032%2525%2520compared%2520to%2520direct%2520generation%2520approaches.%2520We%250Abelieve%2520the%2520contributions%2520of%2520the%2520proposed%2520tasks%252C%2520benchmark%252C%2520and%2520effective%250Aapproach%2520will%2520advance%2520future%2520research%2520in%2520developing%2520versatile%2520object%250Arecognition%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15818v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructSAM%3A%20A%20Training-Free%20Framework%20for%20Instruction-Oriented%20Remote%0A%20%20Sensing%20Object%20Recognition&entry.906535625=Yijie%20Zheng%20and%20Weijie%20Wu%20and%20Qingyun%20Li%20and%20Xuehui%20Wang%20and%20Xu%20Zhou%20and%20Aiai%20Ren%20and%20Jun%20Shen%20and%20Long%20Zhao%20and%20Guoqing%20Li%20and%20Xue%20Yang&entry.1292438233=%20%20Language-Guided%20object%20recognition%20in%20remote%20sensing%20imagery%20is%20crucial%20for%0Alarge-scale%20mapping%20and%20automated%20data%20annotation.%20However%2C%20existing%0Aopen-vocabulary%20and%20visual%20grounding%20methods%20rely%20on%20explicit%20category%20cues%2C%0Alimiting%20their%20ability%20to%20handle%20complex%20or%20implicit%20queries%20that%20require%0Aadvanced%20reasoning.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20new%20suite%20of%20tasks%2C%0Aincluding%20Instruction-Oriented%20Object%20Counting%2C%20Detection%2C%20and%20Segmentation%0A%28InstructCDS%29%2C%20covering%20open-vocabulary%2C%20open-ended%2C%20and%20open-subclass%0Ascenarios.%20We%20further%20present%20EarthInstruct%2C%20the%20first%20InstructCDS%20benchmark%0Afor%20earth%20observation.%20It%20is%20constructed%20from%20two%20diverse%20remote%20sensing%0Adatasets%20with%20varying%20spatial%20resolutions%20and%20annotation%20rules%20across%2020%0Acategories%2C%20necessitating%20models%20to%20interpret%20dataset-specific%20instructions.%0AGiven%20the%20scarcity%20of%20semantically%20rich%20labeled%20data%20in%20remote%20sensing%2C%20we%0Apropose%20InstructSAM%2C%20a%20training-free%20framework%20for%20instruction-driven%20object%0Arecognition.%20InstructSAM%20leverages%20large%20vision-language%20models%20to%20interpret%0Auser%20instructions%20and%20estimate%20object%20counts%2C%20employs%20SAM2%20for%20mask%20proposal%2C%0Aand%20formulates%20mask-label%20assignment%20as%20a%20binary%20integer%20programming%20problem.%0ABy%20integrating%20semantic%20similarity%20with%20counting%20constraints%2C%20InstructSAM%0Aefficiently%20assigns%20categories%20to%20predicted%20masks%20without%20relying%20on%20confidence%0Athresholds.%20Experiments%20demonstrate%20that%20InstructSAM%20matches%20or%20surpasses%0Aspecialized%20baselines%20across%20multiple%20tasks%20while%20maintaining%20near-constant%0Ainference%20time%20regardless%20of%20object%20count%2C%20reducing%20output%20tokens%20by%2089%25%20and%0Aoverall%20runtime%20by%20over%2032%25%20compared%20to%20direct%20generation%20approaches.%20We%0Abelieve%20the%20contributions%20of%20the%20proposed%20tasks%2C%20benchmark%2C%20and%20effective%0Aapproach%20will%20advance%20future%20research%20in%20developing%20versatile%20object%0Arecognition%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.15818v2&entry.124074799=Read"},
{"title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images", "author": "Chengqi Duan and Kaiyue Sun and Rongyao Fang and Manyuan Zhang and Yan Feng and Ying Luo and Yufang Liu and Ke Wang and Peng Pei and Xunliang Cai and Hongsheng Li and Yi Ma and Xihui Liu", "abstract": "  Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.\n", "link": "http://arxiv.org/abs/2510.11718v1", "date": "2025-10-13", "relevancy": 2.7301, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodePlot-CoT%3A%20Mathematical%20Visual%20Reasoning%20by%20Thinking%20with%20Code-Driven%0A%20%20Images&body=Title%3A%20CodePlot-CoT%3A%20Mathematical%20Visual%20Reasoning%20by%20Thinking%20with%20Code-Driven%0A%20%20Images%0AAuthor%3A%20Chengqi%20Duan%20and%20Kaiyue%20Sun%20and%20Rongyao%20Fang%20and%20Manyuan%20Zhang%20and%20Yan%20Feng%20and%20Ying%20Luo%20and%20Yufang%20Liu%20and%20Ke%20Wang%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Hongsheng%20Li%20and%20Yi%20Ma%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision%20Language%20Models%0A%28VLMs%29%20have%20shown%20significant%20progress%20in%20mathematical%20reasoning%2C%20yet%20they%0Astill%20face%20a%20critical%20bottleneck%20with%20problems%20requiring%20visual%20assistance%2C%0Asuch%20as%20drawing%20auxiliary%20lines%20or%20plotting%20functions%20to%20solve%20the%20problems.%0AMost%20LLMs%20and%20VLMs%20are%20constrained%20to%20text-only%20reasoning%20chains%2C%20while%0Amultimodal%20unified%20models%20that%20can%20generate%20interleaved%20text%20and%20images%20lack%0Athe%20necessary%20precision%20and%20controllability%20for%20such%20tasks.%20To%20address%20this%2C%20we%0Apropose%20CodePlot-CoT%2C%20a%20code-driven%20Chain-of-Thought%20paradigm%20for%20%22thinking%0Awith%20images%22%20in%20mathematics.%20Our%20approach%20leverages%20the%20VLM%20to%20generate%20text%0Areasoning%20as%20well%20as%20executable%20plotting%20code%2C%20which%20is%20then%20rendered%20into%0Aimages%20as%20%22visual%20thought%22%2C%20to%20solve%20mathematical%20problems.%20To%20achieve%20this%2C%20we%0Afirst%20construct%20Math-VR%2C%20the%20first%20large-scale%2C%20bilingual%20dataset%20and%20benchmark%0Afor%20Mathematics%20problems%20with%20Visual%20Reasoning%2C%20comprising%20178K%20samples.%0ASecond%2C%20to%20create%20high-quality%20training%20data%2C%20we%20develop%20a%20state-of-the-art%0Aimage-to-code%20converter%20specialized%20for%20parsing%20complex%20mathematical%20figures%0Ainto%20codes.%20Finally%2C%20using%20these%20training%20data%2C%20we%20train%20the%20CodePlot-CoT%20model%0Afor%20solving%20mathematical%20problems.%20Experimental%20results%20show%20that%20our%20model%0Aachieves%20up%20to%2021%25%20increase%20over%20base%20model%20on%20our%20new%20benchmark%2C%20fully%0Avalidating%20the%20efficacy%20of%20our%20proposed%20code-driven%20reasoning%20paradigm.%20Our%0Awork%20opens%20a%20new%20direction%20for%20multimodal%20mathematical%20reasoning%20and%20provides%0Athe%20community%20with%20the%20first%20large-scale%20dataset%2C%20comprehensive%20benchmark%2C%20and%0Astrong%20approach%20for%20such%20problems.%20To%20facilitate%20future%20research%2C%20we%20make%20our%0Adatasets%2C%20code%2C%20and%20pretrained%20models%20publicly%20available%20at%0Ahttps%3A//github.com/HKU-MMLab/Math-VR-CodePlot-CoT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11718v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodePlot-CoT%253A%2520Mathematical%2520Visual%2520Reasoning%2520by%2520Thinking%2520with%2520Code-Driven%250A%2520%2520Images%26entry.906535625%3DChengqi%2520Duan%2520and%2520Kaiyue%2520Sun%2520and%2520Rongyao%2520Fang%2520and%2520Manyuan%2520Zhang%2520and%2520Yan%2520Feng%2520and%2520Ying%2520Luo%2520and%2520Yufang%2520Liu%2520and%2520Ke%2520Wang%2520and%2520Peng%2520Pei%2520and%2520Xunliang%2520Cai%2520and%2520Hongsheng%2520Li%2520and%2520Yi%2520Ma%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Vision%2520Language%2520Models%250A%2528VLMs%2529%2520have%2520shown%2520significant%2520progress%2520in%2520mathematical%2520reasoning%252C%2520yet%2520they%250Astill%2520face%2520a%2520critical%2520bottleneck%2520with%2520problems%2520requiring%2520visual%2520assistance%252C%250Asuch%2520as%2520drawing%2520auxiliary%2520lines%2520or%2520plotting%2520functions%2520to%2520solve%2520the%2520problems.%250AMost%2520LLMs%2520and%2520VLMs%2520are%2520constrained%2520to%2520text-only%2520reasoning%2520chains%252C%2520while%250Amultimodal%2520unified%2520models%2520that%2520can%2520generate%2520interleaved%2520text%2520and%2520images%2520lack%250Athe%2520necessary%2520precision%2520and%2520controllability%2520for%2520such%2520tasks.%2520To%2520address%2520this%252C%2520we%250Apropose%2520CodePlot-CoT%252C%2520a%2520code-driven%2520Chain-of-Thought%2520paradigm%2520for%2520%2522thinking%250Awith%2520images%2522%2520in%2520mathematics.%2520Our%2520approach%2520leverages%2520the%2520VLM%2520to%2520generate%2520text%250Areasoning%2520as%2520well%2520as%2520executable%2520plotting%2520code%252C%2520which%2520is%2520then%2520rendered%2520into%250Aimages%2520as%2520%2522visual%2520thought%2522%252C%2520to%2520solve%2520mathematical%2520problems.%2520To%2520achieve%2520this%252C%2520we%250Afirst%2520construct%2520Math-VR%252C%2520the%2520first%2520large-scale%252C%2520bilingual%2520dataset%2520and%2520benchmark%250Afor%2520Mathematics%2520problems%2520with%2520Visual%2520Reasoning%252C%2520comprising%2520178K%2520samples.%250ASecond%252C%2520to%2520create%2520high-quality%2520training%2520data%252C%2520we%2520develop%2520a%2520state-of-the-art%250Aimage-to-code%2520converter%2520specialized%2520for%2520parsing%2520complex%2520mathematical%2520figures%250Ainto%2520codes.%2520Finally%252C%2520using%2520these%2520training%2520data%252C%2520we%2520train%2520the%2520CodePlot-CoT%2520model%250Afor%2520solving%2520mathematical%2520problems.%2520Experimental%2520results%2520show%2520that%2520our%2520model%250Aachieves%2520up%2520to%252021%2525%2520increase%2520over%2520base%2520model%2520on%2520our%2520new%2520benchmark%252C%2520fully%250Avalidating%2520the%2520efficacy%2520of%2520our%2520proposed%2520code-driven%2520reasoning%2520paradigm.%2520Our%250Awork%2520opens%2520a%2520new%2520direction%2520for%2520multimodal%2520mathematical%2520reasoning%2520and%2520provides%250Athe%2520community%2520with%2520the%2520first%2520large-scale%2520dataset%252C%2520comprehensive%2520benchmark%252C%2520and%250Astrong%2520approach%2520for%2520such%2520problems.%2520To%2520facilitate%2520future%2520research%252C%2520we%2520make%2520our%250Adatasets%252C%2520code%252C%2520and%2520pretrained%2520models%2520publicly%2520available%2520at%250Ahttps%253A//github.com/HKU-MMLab/Math-VR-CodePlot-CoT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11718v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodePlot-CoT%3A%20Mathematical%20Visual%20Reasoning%20by%20Thinking%20with%20Code-Driven%0A%20%20Images&entry.906535625=Chengqi%20Duan%20and%20Kaiyue%20Sun%20and%20Rongyao%20Fang%20and%20Manyuan%20Zhang%20and%20Yan%20Feng%20and%20Ying%20Luo%20and%20Yufang%20Liu%20and%20Ke%20Wang%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Hongsheng%20Li%20and%20Yi%20Ma%20and%20Xihui%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20Vision%20Language%20Models%0A%28VLMs%29%20have%20shown%20significant%20progress%20in%20mathematical%20reasoning%2C%20yet%20they%0Astill%20face%20a%20critical%20bottleneck%20with%20problems%20requiring%20visual%20assistance%2C%0Asuch%20as%20drawing%20auxiliary%20lines%20or%20plotting%20functions%20to%20solve%20the%20problems.%0AMost%20LLMs%20and%20VLMs%20are%20constrained%20to%20text-only%20reasoning%20chains%2C%20while%0Amultimodal%20unified%20models%20that%20can%20generate%20interleaved%20text%20and%20images%20lack%0Athe%20necessary%20precision%20and%20controllability%20for%20such%20tasks.%20To%20address%20this%2C%20we%0Apropose%20CodePlot-CoT%2C%20a%20code-driven%20Chain-of-Thought%20paradigm%20for%20%22thinking%0Awith%20images%22%20in%20mathematics.%20Our%20approach%20leverages%20the%20VLM%20to%20generate%20text%0Areasoning%20as%20well%20as%20executable%20plotting%20code%2C%20which%20is%20then%20rendered%20into%0Aimages%20as%20%22visual%20thought%22%2C%20to%20solve%20mathematical%20problems.%20To%20achieve%20this%2C%20we%0Afirst%20construct%20Math-VR%2C%20the%20first%20large-scale%2C%20bilingual%20dataset%20and%20benchmark%0Afor%20Mathematics%20problems%20with%20Visual%20Reasoning%2C%20comprising%20178K%20samples.%0ASecond%2C%20to%20create%20high-quality%20training%20data%2C%20we%20develop%20a%20state-of-the-art%0Aimage-to-code%20converter%20specialized%20for%20parsing%20complex%20mathematical%20figures%0Ainto%20codes.%20Finally%2C%20using%20these%20training%20data%2C%20we%20train%20the%20CodePlot-CoT%20model%0Afor%20solving%20mathematical%20problems.%20Experimental%20results%20show%20that%20our%20model%0Aachieves%20up%20to%2021%25%20increase%20over%20base%20model%20on%20our%20new%20benchmark%2C%20fully%0Avalidating%20the%20efficacy%20of%20our%20proposed%20code-driven%20reasoning%20paradigm.%20Our%0Awork%20opens%20a%20new%20direction%20for%20multimodal%20mathematical%20reasoning%20and%20provides%0Athe%20community%20with%20the%20first%20large-scale%20dataset%2C%20comprehensive%20benchmark%2C%20and%0Astrong%20approach%20for%20such%20problems.%20To%20facilitate%20future%20research%2C%20we%20make%20our%0Adatasets%2C%20code%2C%20and%20pretrained%20models%20publicly%20available%20at%0Ahttps%3A//github.com/HKU-MMLab/Math-VR-CodePlot-CoT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11718v1&entry.124074799=Read"},
{"title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models", "author": "Tobias Preintner and Weixuan Yuan and Adrian K\u00f6nig and Thomas B\u00e4ck and Elena Raponi and Niki van Stein", "abstract": "  Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.\n", "link": "http://arxiv.org/abs/2510.11631v1", "date": "2025-10-13", "relevancy": 2.7099, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoCAD%3A%20Evolutionary%20CAD%20Code%20Generation%20with%20Vision%20Language%20Models&body=Title%3A%20EvoCAD%3A%20Evolutionary%20CAD%20Code%20Generation%20with%20Vision%20Language%20Models%0AAuthor%3A%20Tobias%20Preintner%20and%20Weixuan%20Yuan%20and%20Adrian%20K%C3%B6nig%20and%20Thomas%20B%C3%A4ck%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein%0AAbstract%3A%20%20%20Combining%20large%20language%20models%20with%20evolutionary%20computation%20algorithms%0Arepresents%20a%20promising%20research%20direction%20leveraging%20the%20remarkable%20generative%0Aand%20in-context%20learning%20capabilities%20of%20LLMs%20with%20the%20strengths%20of%20evolutionary%0Aalgorithms.%20In%20this%20work%2C%20we%20present%20EvoCAD%2C%20a%20method%20for%20generating%0Acomputer-aided%20design%20%28CAD%29%20objects%20through%20their%20symbolic%20representations%0Ausing%20vision%20language%20models%20and%20evolutionary%20optimization.%20Our%20method%20samples%0Amultiple%20CAD%20objects%2C%20which%20are%20then%20optimized%20using%20an%20evolutionary%20approach%0Awith%20vision%20language%20and%20reasoning%20language%20models.%20We%20assess%20our%20method%20using%0AGPT-4V%20and%20GPT-4o%2C%20evaluating%20it%20on%20the%20CADPrompt%20benchmark%20dataset%20and%0Acomparing%20it%20to%20prior%20methods.%20Additionally%2C%20we%20introduce%20two%20new%20metrics%20based%0Aon%20topological%20properties%20defined%20by%20the%20Euler%20characteristic%2C%20which%20capture%20a%0Aform%20of%20semantic%20similarity%20between%203D%20objects.%20Our%20results%20demonstrate%20that%0AEvoCAD%20outperforms%20previous%20approaches%20on%20multiple%20metrics%2C%20particularly%20in%0Agenerating%20topologically%20correct%20objects%2C%20which%20can%20be%20efficiently%20evaluated%0Ausing%20our%20two%20novel%20metrics%20that%20complement%20existing%20spatial%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoCAD%253A%2520Evolutionary%2520CAD%2520Code%2520Generation%2520with%2520Vision%2520Language%2520Models%26entry.906535625%3DTobias%2520Preintner%2520and%2520Weixuan%2520Yuan%2520and%2520Adrian%2520K%25C3%25B6nig%2520and%2520Thomas%2520B%25C3%25A4ck%2520and%2520Elena%2520Raponi%2520and%2520Niki%2520van%2520Stein%26entry.1292438233%3D%2520%2520Combining%2520large%2520language%2520models%2520with%2520evolutionary%2520computation%2520algorithms%250Arepresents%2520a%2520promising%2520research%2520direction%2520leveraging%2520the%2520remarkable%2520generative%250Aand%2520in-context%2520learning%2520capabilities%2520of%2520LLMs%2520with%2520the%2520strengths%2520of%2520evolutionary%250Aalgorithms.%2520In%2520this%2520work%252C%2520we%2520present%2520EvoCAD%252C%2520a%2520method%2520for%2520generating%250Acomputer-aided%2520design%2520%2528CAD%2529%2520objects%2520through%2520their%2520symbolic%2520representations%250Ausing%2520vision%2520language%2520models%2520and%2520evolutionary%2520optimization.%2520Our%2520method%2520samples%250Amultiple%2520CAD%2520objects%252C%2520which%2520are%2520then%2520optimized%2520using%2520an%2520evolutionary%2520approach%250Awith%2520vision%2520language%2520and%2520reasoning%2520language%2520models.%2520We%2520assess%2520our%2520method%2520using%250AGPT-4V%2520and%2520GPT-4o%252C%2520evaluating%2520it%2520on%2520the%2520CADPrompt%2520benchmark%2520dataset%2520and%250Acomparing%2520it%2520to%2520prior%2520methods.%2520Additionally%252C%2520we%2520introduce%2520two%2520new%2520metrics%2520based%250Aon%2520topological%2520properties%2520defined%2520by%2520the%2520Euler%2520characteristic%252C%2520which%2520capture%2520a%250Aform%2520of%2520semantic%2520similarity%2520between%25203D%2520objects.%2520Our%2520results%2520demonstrate%2520that%250AEvoCAD%2520outperforms%2520previous%2520approaches%2520on%2520multiple%2520metrics%252C%2520particularly%2520in%250Agenerating%2520topologically%2520correct%2520objects%252C%2520which%2520can%2520be%2520efficiently%2520evaluated%250Ausing%2520our%2520two%2520novel%2520metrics%2520that%2520complement%2520existing%2520spatial%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoCAD%3A%20Evolutionary%20CAD%20Code%20Generation%20with%20Vision%20Language%20Models&entry.906535625=Tobias%20Preintner%20and%20Weixuan%20Yuan%20and%20Adrian%20K%C3%B6nig%20and%20Thomas%20B%C3%A4ck%20and%20Elena%20Raponi%20and%20Niki%20van%20Stein&entry.1292438233=%20%20Combining%20large%20language%20models%20with%20evolutionary%20computation%20algorithms%0Arepresents%20a%20promising%20research%20direction%20leveraging%20the%20remarkable%20generative%0Aand%20in-context%20learning%20capabilities%20of%20LLMs%20with%20the%20strengths%20of%20evolutionary%0Aalgorithms.%20In%20this%20work%2C%20we%20present%20EvoCAD%2C%20a%20method%20for%20generating%0Acomputer-aided%20design%20%28CAD%29%20objects%20through%20their%20symbolic%20representations%0Ausing%20vision%20language%20models%20and%20evolutionary%20optimization.%20Our%20method%20samples%0Amultiple%20CAD%20objects%2C%20which%20are%20then%20optimized%20using%20an%20evolutionary%20approach%0Awith%20vision%20language%20and%20reasoning%20language%20models.%20We%20assess%20our%20method%20using%0AGPT-4V%20and%20GPT-4o%2C%20evaluating%20it%20on%20the%20CADPrompt%20benchmark%20dataset%20and%0Acomparing%20it%20to%20prior%20methods.%20Additionally%2C%20we%20introduce%20two%20new%20metrics%20based%0Aon%20topological%20properties%20defined%20by%20the%20Euler%20characteristic%2C%20which%20capture%20a%0Aform%20of%20semantic%20similarity%20between%203D%20objects.%20Our%20results%20demonstrate%20that%0AEvoCAD%20outperforms%20previous%20approaches%20on%20multiple%20metrics%2C%20particularly%20in%0Agenerating%20topologically%20correct%20objects%2C%20which%20can%20be%20efficiently%20evaluated%0Ausing%20our%20two%20novel%20metrics%20that%20complement%20existing%20spatial%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11631v1&entry.124074799=Read"},
{"title": "Q-Router: Agentic Video Quality Assessment with Expert Model Routing and\n  Artifact Localization", "author": "Shuo Xing and Soumik Dey and Mingyang Wu and Ashirbad Mishra and Naveen Ravipati and Binbin Li and Hansi Wu and Zhengzhong Tu", "abstract": "  Video quality assessment (VQA) is a fundamental computer vision task that\naims to predict the perceptual quality of a given video in alignment with human\njudgments. Existing performant VQA models trained with direct score supervision\nsuffer from (1) poor generalization across diverse content and tasks, ranging\nfrom user-generated content (UGC), short-form videos, to AI-generated content\n(AIGC), (2) limited interpretability, and (3) lack of extensibility to novel\nuse cases or content types. We propose Q-Router, an agentic framework for\nuniversal VQA with a multi-tier model routing system. Q-Router integrates a\ndiverse set of expert models and employs vision--language models (VLMs) as\nreal-time routers that dynamically reason and then ensemble the most\nappropriate experts conditioned on the input video semantics. We build a\nmulti-tiered routing system based on the computing budget, with the heaviest\ntier involving a specific spatiotemporal artifacts localization for\ninterpretability. This agentic design enables Q-Router to combine the\ncomplementary strengths of specialized experts, achieving both flexibility and\nrobustness in delivering consistent performance across heterogeneous video\nsources and tasks. Extensive experiments demonstrate that Q-Router matches or\nsurpasses state-of-the-art VQA models on a variety of benchmarks, while\nsubstantially improving generalization and interpretability. Moreover, Q-Router\nexcels on the quality-based question answering benchmark, Q-Bench-Video,\nhighlighting its promise as a foundation for next-generation VQA systems.\nFinally, we show that Q-Router capably localizes spatiotemporal artifacts,\nshowing potential as a reward function for post-training video generation\nmodels.\n", "link": "http://arxiv.org/abs/2510.08789v2", "date": "2025-10-13", "relevancy": 2.6716, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-Router%3A%20Agentic%20Video%20Quality%20Assessment%20with%20Expert%20Model%20Routing%20and%0A%20%20Artifact%20Localization&body=Title%3A%20Q-Router%3A%20Agentic%20Video%20Quality%20Assessment%20with%20Expert%20Model%20Routing%20and%0A%20%20Artifact%20Localization%0AAuthor%3A%20Shuo%20Xing%20and%20Soumik%20Dey%20and%20Mingyang%20Wu%20and%20Ashirbad%20Mishra%20and%20Naveen%20Ravipati%20and%20Binbin%20Li%20and%20Hansi%20Wu%20and%20Zhengzhong%20Tu%0AAbstract%3A%20%20%20Video%20quality%20assessment%20%28VQA%29%20is%20a%20fundamental%20computer%20vision%20task%20that%0Aaims%20to%20predict%20the%20perceptual%20quality%20of%20a%20given%20video%20in%20alignment%20with%20human%0Ajudgments.%20Existing%20performant%20VQA%20models%20trained%20with%20direct%20score%20supervision%0Asuffer%20from%20%281%29%20poor%20generalization%20across%20diverse%20content%20and%20tasks%2C%20ranging%0Afrom%20user-generated%20content%20%28UGC%29%2C%20short-form%20videos%2C%20to%20AI-generated%20content%0A%28AIGC%29%2C%20%282%29%20limited%20interpretability%2C%20and%20%283%29%20lack%20of%20extensibility%20to%20novel%0Ause%20cases%20or%20content%20types.%20We%20propose%20Q-Router%2C%20an%20agentic%20framework%20for%0Auniversal%20VQA%20with%20a%20multi-tier%20model%20routing%20system.%20Q-Router%20integrates%20a%0Adiverse%20set%20of%20expert%20models%20and%20employs%20vision--language%20models%20%28VLMs%29%20as%0Areal-time%20routers%20that%20dynamically%20reason%20and%20then%20ensemble%20the%20most%0Aappropriate%20experts%20conditioned%20on%20the%20input%20video%20semantics.%20We%20build%20a%0Amulti-tiered%20routing%20system%20based%20on%20the%20computing%20budget%2C%20with%20the%20heaviest%0Atier%20involving%20a%20specific%20spatiotemporal%20artifacts%20localization%20for%0Ainterpretability.%20This%20agentic%20design%20enables%20Q-Router%20to%20combine%20the%0Acomplementary%20strengths%20of%20specialized%20experts%2C%20achieving%20both%20flexibility%20and%0Arobustness%20in%20delivering%20consistent%20performance%20across%20heterogeneous%20video%0Asources%20and%20tasks.%20Extensive%20experiments%20demonstrate%20that%20Q-Router%20matches%20or%0Asurpasses%20state-of-the-art%20VQA%20models%20on%20a%20variety%20of%20benchmarks%2C%20while%0Asubstantially%20improving%20generalization%20and%20interpretability.%20Moreover%2C%20Q-Router%0Aexcels%20on%20the%20quality-based%20question%20answering%20benchmark%2C%20Q-Bench-Video%2C%0Ahighlighting%20its%20promise%20as%20a%20foundation%20for%20next-generation%20VQA%20systems.%0AFinally%2C%20we%20show%20that%20Q-Router%20capably%20localizes%20spatiotemporal%20artifacts%2C%0Ashowing%20potential%20as%20a%20reward%20function%20for%20post-training%20video%20generation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.08789v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-Router%253A%2520Agentic%2520Video%2520Quality%2520Assessment%2520with%2520Expert%2520Model%2520Routing%2520and%250A%2520%2520Artifact%2520Localization%26entry.906535625%3DShuo%2520Xing%2520and%2520Soumik%2520Dey%2520and%2520Mingyang%2520Wu%2520and%2520Ashirbad%2520Mishra%2520and%2520Naveen%2520Ravipati%2520and%2520Binbin%2520Li%2520and%2520Hansi%2520Wu%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3D%2520%2520Video%2520quality%2520assessment%2520%2528VQA%2529%2520is%2520a%2520fundamental%2520computer%2520vision%2520task%2520that%250Aaims%2520to%2520predict%2520the%2520perceptual%2520quality%2520of%2520a%2520given%2520video%2520in%2520alignment%2520with%2520human%250Ajudgments.%2520Existing%2520performant%2520VQA%2520models%2520trained%2520with%2520direct%2520score%2520supervision%250Asuffer%2520from%2520%25281%2529%2520poor%2520generalization%2520across%2520diverse%2520content%2520and%2520tasks%252C%2520ranging%250Afrom%2520user-generated%2520content%2520%2528UGC%2529%252C%2520short-form%2520videos%252C%2520to%2520AI-generated%2520content%250A%2528AIGC%2529%252C%2520%25282%2529%2520limited%2520interpretability%252C%2520and%2520%25283%2529%2520lack%2520of%2520extensibility%2520to%2520novel%250Ause%2520cases%2520or%2520content%2520types.%2520We%2520propose%2520Q-Router%252C%2520an%2520agentic%2520framework%2520for%250Auniversal%2520VQA%2520with%2520a%2520multi-tier%2520model%2520routing%2520system.%2520Q-Router%2520integrates%2520a%250Adiverse%2520set%2520of%2520expert%2520models%2520and%2520employs%2520vision--language%2520models%2520%2528VLMs%2529%2520as%250Areal-time%2520routers%2520that%2520dynamically%2520reason%2520and%2520then%2520ensemble%2520the%2520most%250Aappropriate%2520experts%2520conditioned%2520on%2520the%2520input%2520video%2520semantics.%2520We%2520build%2520a%250Amulti-tiered%2520routing%2520system%2520based%2520on%2520the%2520computing%2520budget%252C%2520with%2520the%2520heaviest%250Atier%2520involving%2520a%2520specific%2520spatiotemporal%2520artifacts%2520localization%2520for%250Ainterpretability.%2520This%2520agentic%2520design%2520enables%2520Q-Router%2520to%2520combine%2520the%250Acomplementary%2520strengths%2520of%2520specialized%2520experts%252C%2520achieving%2520both%2520flexibility%2520and%250Arobustness%2520in%2520delivering%2520consistent%2520performance%2520across%2520heterogeneous%2520video%250Asources%2520and%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520Q-Router%2520matches%2520or%250Asurpasses%2520state-of-the-art%2520VQA%2520models%2520on%2520a%2520variety%2520of%2520benchmarks%252C%2520while%250Asubstantially%2520improving%2520generalization%2520and%2520interpretability.%2520Moreover%252C%2520Q-Router%250Aexcels%2520on%2520the%2520quality-based%2520question%2520answering%2520benchmark%252C%2520Q-Bench-Video%252C%250Ahighlighting%2520its%2520promise%2520as%2520a%2520foundation%2520for%2520next-generation%2520VQA%2520systems.%250AFinally%252C%2520we%2520show%2520that%2520Q-Router%2520capably%2520localizes%2520spatiotemporal%2520artifacts%252C%250Ashowing%2520potential%2520as%2520a%2520reward%2520function%2520for%2520post-training%2520video%2520generation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.08789v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-Router%3A%20Agentic%20Video%20Quality%20Assessment%20with%20Expert%20Model%20Routing%20and%0A%20%20Artifact%20Localization&entry.906535625=Shuo%20Xing%20and%20Soumik%20Dey%20and%20Mingyang%20Wu%20and%20Ashirbad%20Mishra%20and%20Naveen%20Ravipati%20and%20Binbin%20Li%20and%20Hansi%20Wu%20and%20Zhengzhong%20Tu&entry.1292438233=%20%20Video%20quality%20assessment%20%28VQA%29%20is%20a%20fundamental%20computer%20vision%20task%20that%0Aaims%20to%20predict%20the%20perceptual%20quality%20of%20a%20given%20video%20in%20alignment%20with%20human%0Ajudgments.%20Existing%20performant%20VQA%20models%20trained%20with%20direct%20score%20supervision%0Asuffer%20from%20%281%29%20poor%20generalization%20across%20diverse%20content%20and%20tasks%2C%20ranging%0Afrom%20user-generated%20content%20%28UGC%29%2C%20short-form%20videos%2C%20to%20AI-generated%20content%0A%28AIGC%29%2C%20%282%29%20limited%20interpretability%2C%20and%20%283%29%20lack%20of%20extensibility%20to%20novel%0Ause%20cases%20or%20content%20types.%20We%20propose%20Q-Router%2C%20an%20agentic%20framework%20for%0Auniversal%20VQA%20with%20a%20multi-tier%20model%20routing%20system.%20Q-Router%20integrates%20a%0Adiverse%20set%20of%20expert%20models%20and%20employs%20vision--language%20models%20%28VLMs%29%20as%0Areal-time%20routers%20that%20dynamically%20reason%20and%20then%20ensemble%20the%20most%0Aappropriate%20experts%20conditioned%20on%20the%20input%20video%20semantics.%20We%20build%20a%0Amulti-tiered%20routing%20system%20based%20on%20the%20computing%20budget%2C%20with%20the%20heaviest%0Atier%20involving%20a%20specific%20spatiotemporal%20artifacts%20localization%20for%0Ainterpretability.%20This%20agentic%20design%20enables%20Q-Router%20to%20combine%20the%0Acomplementary%20strengths%20of%20specialized%20experts%2C%20achieving%20both%20flexibility%20and%0Arobustness%20in%20delivering%20consistent%20performance%20across%20heterogeneous%20video%0Asources%20and%20tasks.%20Extensive%20experiments%20demonstrate%20that%20Q-Router%20matches%20or%0Asurpasses%20state-of-the-art%20VQA%20models%20on%20a%20variety%20of%20benchmarks%2C%20while%0Asubstantially%20improving%20generalization%20and%20interpretability.%20Moreover%2C%20Q-Router%0Aexcels%20on%20the%20quality-based%20question%20answering%20benchmark%2C%20Q-Bench-Video%2C%0Ahighlighting%20its%20promise%20as%20a%20foundation%20for%20next-generation%20VQA%20systems.%0AFinally%2C%20we%20show%20that%20Q-Router%20capably%20localizes%20spatiotemporal%20artifacts%2C%0Ashowing%20potential%20as%20a%20reward%20function%20for%20post-training%20video%20generation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.08789v2&entry.124074799=Read"},
{"title": "Multi-Scale Manifold Alignment for Interpreting Large Language Models: A\n  Unified Information-Geometric Framework", "author": "Yukun Zhang and Qi Dong", "abstract": "  We present Multi-Scale Manifold Alignment(MSMA), an information-geometric\nframework that decomposes LLM representations into local, intermediate, and\nglobal manifolds and learns cross-scale mappings that preserve geometry and\ninformation. Across GPT-2, BERT, RoBERTa, and T5, we observe consistent\nhierarchical patterns and find that MSMA improves alignment metrics under\nmultiple estimators (e.g., relative KL reduction and MI gains with statistical\nsignificance across seeds). Controlled interventions at different scales yield\ndistinct and architecture-dependent effects on lexical diversity, sentence\nstructure, and discourse coherence. While our theoretical analysis relies on\nidealized assumptions, the empirical results suggest that multi-objective\nalignment offers a practical lens for analyzing cross-scale information flow\nand guiding representation-level control.\n", "link": "http://arxiv.org/abs/2505.20333v2", "date": "2025-10-13", "relevancy": 2.6714, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5762}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scale%20Manifold%20Alignment%20for%20Interpreting%20Large%20Language%20Models%3A%20A%0A%20%20Unified%20Information-Geometric%20Framework&body=Title%3A%20Multi-Scale%20Manifold%20Alignment%20for%20Interpreting%20Large%20Language%20Models%3A%20A%0A%20%20Unified%20Information-Geometric%20Framework%0AAuthor%3A%20Yukun%20Zhang%20and%20Qi%20Dong%0AAbstract%3A%20%20%20We%20present%20Multi-Scale%20Manifold%20Alignment%28MSMA%29%2C%20an%20information-geometric%0Aframework%20that%20decomposes%20LLM%20representations%20into%20local%2C%20intermediate%2C%20and%0Aglobal%20manifolds%20and%20learns%20cross-scale%20mappings%20that%20preserve%20geometry%20and%0Ainformation.%20Across%20GPT-2%2C%20BERT%2C%20RoBERTa%2C%20and%20T5%2C%20we%20observe%20consistent%0Ahierarchical%20patterns%20and%20find%20that%20MSMA%20improves%20alignment%20metrics%20under%0Amultiple%20estimators%20%28e.g.%2C%20relative%20KL%20reduction%20and%20MI%20gains%20with%20statistical%0Asignificance%20across%20seeds%29.%20Controlled%20interventions%20at%20different%20scales%20yield%0Adistinct%20and%20architecture-dependent%20effects%20on%20lexical%20diversity%2C%20sentence%0Astructure%2C%20and%20discourse%20coherence.%20While%20our%20theoretical%20analysis%20relies%20on%0Aidealized%20assumptions%2C%20the%20empirical%20results%20suggest%20that%20multi-objective%0Aalignment%20offers%20a%20practical%20lens%20for%20analyzing%20cross-scale%20information%20flow%0Aand%20guiding%20representation-level%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20333v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scale%2520Manifold%2520Alignment%2520for%2520Interpreting%2520Large%2520Language%2520Models%253A%2520A%250A%2520%2520Unified%2520Information-Geometric%2520Framework%26entry.906535625%3DYukun%2520Zhang%2520and%2520Qi%2520Dong%26entry.1292438233%3D%2520%2520We%2520present%2520Multi-Scale%2520Manifold%2520Alignment%2528MSMA%2529%252C%2520an%2520information-geometric%250Aframework%2520that%2520decomposes%2520LLM%2520representations%2520into%2520local%252C%2520intermediate%252C%2520and%250Aglobal%2520manifolds%2520and%2520learns%2520cross-scale%2520mappings%2520that%2520preserve%2520geometry%2520and%250Ainformation.%2520Across%2520GPT-2%252C%2520BERT%252C%2520RoBERTa%252C%2520and%2520T5%252C%2520we%2520observe%2520consistent%250Ahierarchical%2520patterns%2520and%2520find%2520that%2520MSMA%2520improves%2520alignment%2520metrics%2520under%250Amultiple%2520estimators%2520%2528e.g.%252C%2520relative%2520KL%2520reduction%2520and%2520MI%2520gains%2520with%2520statistical%250Asignificance%2520across%2520seeds%2529.%2520Controlled%2520interventions%2520at%2520different%2520scales%2520yield%250Adistinct%2520and%2520architecture-dependent%2520effects%2520on%2520lexical%2520diversity%252C%2520sentence%250Astructure%252C%2520and%2520discourse%2520coherence.%2520While%2520our%2520theoretical%2520analysis%2520relies%2520on%250Aidealized%2520assumptions%252C%2520the%2520empirical%2520results%2520suggest%2520that%2520multi-objective%250Aalignment%2520offers%2520a%2520practical%2520lens%2520for%2520analyzing%2520cross-scale%2520information%2520flow%250Aand%2520guiding%2520representation-level%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20333v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scale%20Manifold%20Alignment%20for%20Interpreting%20Large%20Language%20Models%3A%20A%0A%20%20Unified%20Information-Geometric%20Framework&entry.906535625=Yukun%20Zhang%20and%20Qi%20Dong&entry.1292438233=%20%20We%20present%20Multi-Scale%20Manifold%20Alignment%28MSMA%29%2C%20an%20information-geometric%0Aframework%20that%20decomposes%20LLM%20representations%20into%20local%2C%20intermediate%2C%20and%0Aglobal%20manifolds%20and%20learns%20cross-scale%20mappings%20that%20preserve%20geometry%20and%0Ainformation.%20Across%20GPT-2%2C%20BERT%2C%20RoBERTa%2C%20and%20T5%2C%20we%20observe%20consistent%0Ahierarchical%20patterns%20and%20find%20that%20MSMA%20improves%20alignment%20metrics%20under%0Amultiple%20estimators%20%28e.g.%2C%20relative%20KL%20reduction%20and%20MI%20gains%20with%20statistical%0Asignificance%20across%20seeds%29.%20Controlled%20interventions%20at%20different%20scales%20yield%0Adistinct%20and%20architecture-dependent%20effects%20on%20lexical%20diversity%2C%20sentence%0Astructure%2C%20and%20discourse%20coherence.%20While%20our%20theoretical%20analysis%20relies%20on%0Aidealized%20assumptions%2C%20the%20empirical%20results%20suggest%20that%20multi-objective%0Aalignment%20offers%20a%20practical%20lens%20for%20analyzing%20cross-scale%20information%20flow%0Aand%20guiding%20representation-level%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20333v2&entry.124074799=Read"},
{"title": "Bayesian Topological Convolutional Neural Nets", "author": "Sarah Harkins Dayton and Hayden Everett and Ioannis Schizas and David L. Boothe Jr. and Vasileios Maroulas", "abstract": "  Convolutional neural networks (CNNs) have been established as the main\nworkhorse in image data processing; nonetheless, they require large amounts of\ndata to train, often produce overconfident predictions, and frequently lack the\nability to quantify the uncertainty of their predictions. To address these\nconcerns, we propose a new Bayesian topological CNN that promotes a novel\ninterplay between topology-aware learning and Bayesian sampling. Specifically,\nit utilizes information from important manifolds to accelerate training while\nreducing calibration error by placing prior distributions on network parameters\nand properly learning appropriate posteriors. One important contribution of our\nwork is the inclusion of a consistency condition in the learning cost, which\ncan effectively modify the prior distributions to improve the performance of\nour novel network architecture. We evaluate the model on benchmark image\nclassification datasets and demonstrate its superiority over conventional CNNs,\nBayesian neural networks (BNNs), and topological CNNs. In particular, we supply\nevidence that our method provides an advantage in situations where training\ndata is limited or corrupted. Furthermore, we show that the new model allows\nfor better uncertainty quantification than standard BNNs since it can more\nreadily identify examples of out-of-distribution data on which it has not been\ntrained. Our results highlight the potential of our novel hybrid approach for\nmore efficient and robust image classification.\n", "link": "http://arxiv.org/abs/2510.11704v1", "date": "2025-10-13", "relevancy": 2.6127, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5328}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5256}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Topological%20Convolutional%20Neural%20Nets&body=Title%3A%20Bayesian%20Topological%20Convolutional%20Neural%20Nets%0AAuthor%3A%20Sarah%20Harkins%20Dayton%20and%20Hayden%20Everett%20and%20Ioannis%20Schizas%20and%20David%20L.%20Boothe%20Jr.%20and%20Vasileios%20Maroulas%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20have%20been%20established%20as%20the%20main%0Aworkhorse%20in%20image%20data%20processing%3B%20nonetheless%2C%20they%20require%20large%20amounts%20of%0Adata%20to%20train%2C%20often%20produce%20overconfident%20predictions%2C%20and%20frequently%20lack%20the%0Aability%20to%20quantify%20the%20uncertainty%20of%20their%20predictions.%20To%20address%20these%0Aconcerns%2C%20we%20propose%20a%20new%20Bayesian%20topological%20CNN%20that%20promotes%20a%20novel%0Ainterplay%20between%20topology-aware%20learning%20and%20Bayesian%20sampling.%20Specifically%2C%0Ait%20utilizes%20information%20from%20important%20manifolds%20to%20accelerate%20training%20while%0Areducing%20calibration%20error%20by%20placing%20prior%20distributions%20on%20network%20parameters%0Aand%20properly%20learning%20appropriate%20posteriors.%20One%20important%20contribution%20of%20our%0Awork%20is%20the%20inclusion%20of%20a%20consistency%20condition%20in%20the%20learning%20cost%2C%20which%0Acan%20effectively%20modify%20the%20prior%20distributions%20to%20improve%20the%20performance%20of%0Aour%20novel%20network%20architecture.%20We%20evaluate%20the%20model%20on%20benchmark%20image%0Aclassification%20datasets%20and%20demonstrate%20its%20superiority%20over%20conventional%20CNNs%2C%0ABayesian%20neural%20networks%20%28BNNs%29%2C%20and%20topological%20CNNs.%20In%20particular%2C%20we%20supply%0Aevidence%20that%20our%20method%20provides%20an%20advantage%20in%20situations%20where%20training%0Adata%20is%20limited%20or%20corrupted.%20Furthermore%2C%20we%20show%20that%20the%20new%20model%20allows%0Afor%20better%20uncertainty%20quantification%20than%20standard%20BNNs%20since%20it%20can%20more%0Areadily%20identify%20examples%20of%20out-of-distribution%20data%20on%20which%20it%20has%20not%20been%0Atrained.%20Our%20results%20highlight%20the%20potential%20of%20our%20novel%20hybrid%20approach%20for%0Amore%20efficient%20and%20robust%20image%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Topological%2520Convolutional%2520Neural%2520Nets%26entry.906535625%3DSarah%2520Harkins%2520Dayton%2520and%2520Hayden%2520Everett%2520and%2520Ioannis%2520Schizas%2520and%2520David%2520L.%2520Boothe%2520Jr.%2520and%2520Vasileios%2520Maroulas%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%2520been%2520established%2520as%2520the%2520main%250Aworkhorse%2520in%2520image%2520data%2520processing%253B%2520nonetheless%252C%2520they%2520require%2520large%2520amounts%2520of%250Adata%2520to%2520train%252C%2520often%2520produce%2520overconfident%2520predictions%252C%2520and%2520frequently%2520lack%2520the%250Aability%2520to%2520quantify%2520the%2520uncertainty%2520of%2520their%2520predictions.%2520To%2520address%2520these%250Aconcerns%252C%2520we%2520propose%2520a%2520new%2520Bayesian%2520topological%2520CNN%2520that%2520promotes%2520a%2520novel%250Ainterplay%2520between%2520topology-aware%2520learning%2520and%2520Bayesian%2520sampling.%2520Specifically%252C%250Ait%2520utilizes%2520information%2520from%2520important%2520manifolds%2520to%2520accelerate%2520training%2520while%250Areducing%2520calibration%2520error%2520by%2520placing%2520prior%2520distributions%2520on%2520network%2520parameters%250Aand%2520properly%2520learning%2520appropriate%2520posteriors.%2520One%2520important%2520contribution%2520of%2520our%250Awork%2520is%2520the%2520inclusion%2520of%2520a%2520consistency%2520condition%2520in%2520the%2520learning%2520cost%252C%2520which%250Acan%2520effectively%2520modify%2520the%2520prior%2520distributions%2520to%2520improve%2520the%2520performance%2520of%250Aour%2520novel%2520network%2520architecture.%2520We%2520evaluate%2520the%2520model%2520on%2520benchmark%2520image%250Aclassification%2520datasets%2520and%2520demonstrate%2520its%2520superiority%2520over%2520conventional%2520CNNs%252C%250ABayesian%2520neural%2520networks%2520%2528BNNs%2529%252C%2520and%2520topological%2520CNNs.%2520In%2520particular%252C%2520we%2520supply%250Aevidence%2520that%2520our%2520method%2520provides%2520an%2520advantage%2520in%2520situations%2520where%2520training%250Adata%2520is%2520limited%2520or%2520corrupted.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520new%2520model%2520allows%250Afor%2520better%2520uncertainty%2520quantification%2520than%2520standard%2520BNNs%2520since%2520it%2520can%2520more%250Areadily%2520identify%2520examples%2520of%2520out-of-distribution%2520data%2520on%2520which%2520it%2520has%2520not%2520been%250Atrained.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520our%2520novel%2520hybrid%2520approach%2520for%250Amore%2520efficient%2520and%2520robust%2520image%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Topological%20Convolutional%20Neural%20Nets&entry.906535625=Sarah%20Harkins%20Dayton%20and%20Hayden%20Everett%20and%20Ioannis%20Schizas%20and%20David%20L.%20Boothe%20Jr.%20and%20Vasileios%20Maroulas&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20have%20been%20established%20as%20the%20main%0Aworkhorse%20in%20image%20data%20processing%3B%20nonetheless%2C%20they%20require%20large%20amounts%20of%0Adata%20to%20train%2C%20often%20produce%20overconfident%20predictions%2C%20and%20frequently%20lack%20the%0Aability%20to%20quantify%20the%20uncertainty%20of%20their%20predictions.%20To%20address%20these%0Aconcerns%2C%20we%20propose%20a%20new%20Bayesian%20topological%20CNN%20that%20promotes%20a%20novel%0Ainterplay%20between%20topology-aware%20learning%20and%20Bayesian%20sampling.%20Specifically%2C%0Ait%20utilizes%20information%20from%20important%20manifolds%20to%20accelerate%20training%20while%0Areducing%20calibration%20error%20by%20placing%20prior%20distributions%20on%20network%20parameters%0Aand%20properly%20learning%20appropriate%20posteriors.%20One%20important%20contribution%20of%20our%0Awork%20is%20the%20inclusion%20of%20a%20consistency%20condition%20in%20the%20learning%20cost%2C%20which%0Acan%20effectively%20modify%20the%20prior%20distributions%20to%20improve%20the%20performance%20of%0Aour%20novel%20network%20architecture.%20We%20evaluate%20the%20model%20on%20benchmark%20image%0Aclassification%20datasets%20and%20demonstrate%20its%20superiority%20over%20conventional%20CNNs%2C%0ABayesian%20neural%20networks%20%28BNNs%29%2C%20and%20topological%20CNNs.%20In%20particular%2C%20we%20supply%0Aevidence%20that%20our%20method%20provides%20an%20advantage%20in%20situations%20where%20training%0Adata%20is%20limited%20or%20corrupted.%20Furthermore%2C%20we%20show%20that%20the%20new%20model%20allows%0Afor%20better%20uncertainty%20quantification%20than%20standard%20BNNs%20since%20it%20can%20more%0Areadily%20identify%20examples%20of%20out-of-distribution%20data%20on%20which%20it%20has%20not%20been%0Atrained.%20Our%20results%20highlight%20the%20potential%20of%20our%20novel%20hybrid%20approach%20for%0Amore%20efficient%20and%20robust%20image%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11704v1&entry.124074799=Read"},
{"title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment", "author": "Yinan Chen and Jiangning Zhang and Teng Hu and Yuxiang Zeng and Zhucun Xue and Qingdong He and Chengjie Wang and Yong Liu and Xiaobin Hu and Shuicheng Yan", "abstract": "  Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.\n", "link": "http://arxiv.org/abs/2510.11647v1", "date": "2025-10-13", "relevancy": 2.576, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5278}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IVEBench%3A%20Modern%20Benchmark%20Suite%20for%20Instruction-Guided%20Video%20Editing%0A%20%20Assessment&body=Title%3A%20IVEBench%3A%20Modern%20Benchmark%20Suite%20for%20Instruction-Guided%20Video%20Editing%0A%20%20Assessment%0AAuthor%3A%20Yinan%20Chen%20and%20Jiangning%20Zhang%20and%20Teng%20Hu%20and%20Yuxiang%20Zeng%20and%20Zhucun%20Xue%20and%20Qingdong%20He%20and%20Chengjie%20Wang%20and%20Yong%20Liu%20and%20Xiaobin%20Hu%20and%20Shuicheng%20Yan%0AAbstract%3A%20%20%20Instruction-guided%20video%20editing%20has%20emerged%20as%20a%20rapidly%20advancing%20research%0Adirection%2C%20offering%20new%20opportunities%20for%20intuitive%20content%20transformation%0Awhile%20also%20posing%20significant%20challenges%20for%20systematic%20evaluation.%20Existing%0Avideo%20editing%20benchmarks%20fail%20to%20support%20the%20evaluation%20of%20instruction-guided%0Avideo%20editing%20adequately%20and%20further%20suffer%20from%20limited%20source%20diversity%2C%0Anarrow%20task%20coverage%20and%20incomplete%20evaluation%20metrics.%20To%20address%20the%20above%0Alimitations%2C%20we%20introduce%20IVEBench%2C%20a%20modern%20benchmark%20suite%20specifically%0Adesigned%20for%20instruction-guided%20video%20editing%20assessment.%20IVEBench%20comprises%20a%0Adiverse%20database%20of%20600%20high-quality%20source%20videos%2C%20spanning%20seven%20semantic%0Adimensions%2C%20and%20covering%20video%20lengths%20ranging%20from%2032%20to%201%2C024%20frames.%20It%0Afurther%20includes%208%20categories%20of%20editing%20tasks%20with%2035%20subcategories%2C%20whose%0Aprompts%20are%20generated%20and%20refined%20through%20large%20language%20models%20and%20expert%0Areview.%20Crucially%2C%20IVEBench%20establishes%20a%20three-dimensional%20evaluation%20protocol%0Aencompassing%20video%20quality%2C%20instruction%20compliance%20and%20video%20fidelity%2C%0Aintegrating%20both%20traditional%20metrics%20and%20multimodal%20large%20language%20model-based%0Aassessments.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20IVEBench%20in%0Abenchmarking%20state-of-the-art%20instruction-guided%20video%20editing%20methods%2C%20showing%0Aits%20ability%20to%20provide%20comprehensive%20and%20human-aligned%20evaluation%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIVEBench%253A%2520Modern%2520Benchmark%2520Suite%2520for%2520Instruction-Guided%2520Video%2520Editing%250A%2520%2520Assessment%26entry.906535625%3DYinan%2520Chen%2520and%2520Jiangning%2520Zhang%2520and%2520Teng%2520Hu%2520and%2520Yuxiang%2520Zeng%2520and%2520Zhucun%2520Xue%2520and%2520Qingdong%2520He%2520and%2520Chengjie%2520Wang%2520and%2520Yong%2520Liu%2520and%2520Xiaobin%2520Hu%2520and%2520Shuicheng%2520Yan%26entry.1292438233%3D%2520%2520Instruction-guided%2520video%2520editing%2520has%2520emerged%2520as%2520a%2520rapidly%2520advancing%2520research%250Adirection%252C%2520offering%2520new%2520opportunities%2520for%2520intuitive%2520content%2520transformation%250Awhile%2520also%2520posing%2520significant%2520challenges%2520for%2520systematic%2520evaluation.%2520Existing%250Avideo%2520editing%2520benchmarks%2520fail%2520to%2520support%2520the%2520evaluation%2520of%2520instruction-guided%250Avideo%2520editing%2520adequately%2520and%2520further%2520suffer%2520from%2520limited%2520source%2520diversity%252C%250Anarrow%2520task%2520coverage%2520and%2520incomplete%2520evaluation%2520metrics.%2520To%2520address%2520the%2520above%250Alimitations%252C%2520we%2520introduce%2520IVEBench%252C%2520a%2520modern%2520benchmark%2520suite%2520specifically%250Adesigned%2520for%2520instruction-guided%2520video%2520editing%2520assessment.%2520IVEBench%2520comprises%2520a%250Adiverse%2520database%2520of%2520600%2520high-quality%2520source%2520videos%252C%2520spanning%2520seven%2520semantic%250Adimensions%252C%2520and%2520covering%2520video%2520lengths%2520ranging%2520from%252032%2520to%25201%252C024%2520frames.%2520It%250Afurther%2520includes%25208%2520categories%2520of%2520editing%2520tasks%2520with%252035%2520subcategories%252C%2520whose%250Aprompts%2520are%2520generated%2520and%2520refined%2520through%2520large%2520language%2520models%2520and%2520expert%250Areview.%2520Crucially%252C%2520IVEBench%2520establishes%2520a%2520three-dimensional%2520evaluation%2520protocol%250Aencompassing%2520video%2520quality%252C%2520instruction%2520compliance%2520and%2520video%2520fidelity%252C%250Aintegrating%2520both%2520traditional%2520metrics%2520and%2520multimodal%2520large%2520language%2520model-based%250Aassessments.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520IVEBench%2520in%250Abenchmarking%2520state-of-the-art%2520instruction-guided%2520video%2520editing%2520methods%252C%2520showing%250Aits%2520ability%2520to%2520provide%2520comprehensive%2520and%2520human-aligned%2520evaluation%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IVEBench%3A%20Modern%20Benchmark%20Suite%20for%20Instruction-Guided%20Video%20Editing%0A%20%20Assessment&entry.906535625=Yinan%20Chen%20and%20Jiangning%20Zhang%20and%20Teng%20Hu%20and%20Yuxiang%20Zeng%20and%20Zhucun%20Xue%20and%20Qingdong%20He%20and%20Chengjie%20Wang%20and%20Yong%20Liu%20and%20Xiaobin%20Hu%20and%20Shuicheng%20Yan&entry.1292438233=%20%20Instruction-guided%20video%20editing%20has%20emerged%20as%20a%20rapidly%20advancing%20research%0Adirection%2C%20offering%20new%20opportunities%20for%20intuitive%20content%20transformation%0Awhile%20also%20posing%20significant%20challenges%20for%20systematic%20evaluation.%20Existing%0Avideo%20editing%20benchmarks%20fail%20to%20support%20the%20evaluation%20of%20instruction-guided%0Avideo%20editing%20adequately%20and%20further%20suffer%20from%20limited%20source%20diversity%2C%0Anarrow%20task%20coverage%20and%20incomplete%20evaluation%20metrics.%20To%20address%20the%20above%0Alimitations%2C%20we%20introduce%20IVEBench%2C%20a%20modern%20benchmark%20suite%20specifically%0Adesigned%20for%20instruction-guided%20video%20editing%20assessment.%20IVEBench%20comprises%20a%0Adiverse%20database%20of%20600%20high-quality%20source%20videos%2C%20spanning%20seven%20semantic%0Adimensions%2C%20and%20covering%20video%20lengths%20ranging%20from%2032%20to%201%2C024%20frames.%20It%0Afurther%20includes%208%20categories%20of%20editing%20tasks%20with%2035%20subcategories%2C%20whose%0Aprompts%20are%20generated%20and%20refined%20through%20large%20language%20models%20and%20expert%0Areview.%20Crucially%2C%20IVEBench%20establishes%20a%20three-dimensional%20evaluation%20protocol%0Aencompassing%20video%20quality%2C%20instruction%20compliance%20and%20video%20fidelity%2C%0Aintegrating%20both%20traditional%20metrics%20and%20multimodal%20large%20language%20model-based%0Aassessments.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20IVEBench%20in%0Abenchmarking%20state-of-the-art%20instruction-guided%20video%20editing%20methods%2C%20showing%0Aits%20ability%20to%20provide%20comprehensive%20and%20human-aligned%20evaluation%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11647v1&entry.124074799=Read"},
{"title": "Benchmarking foundation models for hyperspectral image classification:\n  Application to cereal crop type mapping", "author": "Walid Elbarz and Mohamed Bourriz and Hicham Hajji and Hamd Ait Abdelali and Fran\u00e7ois Bourzeix", "abstract": "  Foundation models are transforming Earth observation, but their potential for\nhyperspectral crop mapping remains underexplored. This study benchmarks three\nfoundation models for cereal crop mapping using hyperspectral imagery:\nHyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth\ndataset (a large multitemporal hyperspectral archive). Models were fine-tuned\non manually labeled data from a training region and evaluated on an independent\ntest region. Performance was measured with overall accuracy (OA), average\naccuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),\nDOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of\n93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved\n91%, highlighting the importance of model architecture for strong\ngeneralization across geographic regions and sensor platforms. These results\nprovide a systematic evaluation of foundation models for operational\nhyperspectral crop mapping and outline directions for future model development.\n", "link": "http://arxiv.org/abs/2510.11576v1", "date": "2025-10-13", "relevancy": 2.5608, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20foundation%20models%20for%20hyperspectral%20image%20classification%3A%0A%20%20Application%20to%20cereal%20crop%20type%20mapping&body=Title%3A%20Benchmarking%20foundation%20models%20for%20hyperspectral%20image%20classification%3A%0A%20%20Application%20to%20cereal%20crop%20type%20mapping%0AAuthor%3A%20Walid%20Elbarz%20and%20Mohamed%20Bourriz%20and%20Hicham%20Hajji%20and%20Hamd%20Ait%20Abdelali%20and%20Fran%C3%A7ois%20Bourzeix%0AAbstract%3A%20%20%20Foundation%20models%20are%20transforming%20Earth%20observation%2C%20but%20their%20potential%20for%0Ahyperspectral%20crop%20mapping%20remains%20underexplored.%20This%20study%20benchmarks%20three%0Afoundation%20models%20for%20cereal%20crop%20mapping%20using%20hyperspectral%20imagery%3A%0AHyperSigma%2C%20DOFA%2C%20and%20Vision%20Transformers%20pre-trained%20on%20the%20SpectralEarth%0Adataset%20%28a%20large%20multitemporal%20hyperspectral%20archive%29.%20Models%20were%20fine-tuned%0Aon%20manually%20labeled%20data%20from%20a%20training%20region%20and%20evaluated%20on%20an%20independent%0Atest%20region.%20Performance%20was%20measured%20with%20overall%20accuracy%20%28OA%29%2C%20average%0Aaccuracy%20%28AA%29%2C%20and%20F1-score.%20HyperSigma%20achieved%20an%20OA%20of%2034.5%25%20%28%2B/-%201.8%25%29%2C%0ADOFA%20reached%2062.6%25%20%28%2B/-%203.5%25%29%2C%20and%20the%20SpectralEarth%20model%20achieved%20an%20OA%20of%0A93.5%25%20%28%2B/-%200.8%25%29.%20A%20compact%20SpectralEarth%20variant%20trained%20from%20scratch%20achieved%0A91%25%2C%20highlighting%20the%20importance%20of%20model%20architecture%20for%20strong%0Ageneralization%20across%20geographic%20regions%20and%20sensor%20platforms.%20These%20results%0Aprovide%20a%20systematic%20evaluation%20of%20foundation%20models%20for%20operational%0Ahyperspectral%20crop%20mapping%20and%20outline%20directions%20for%20future%20model%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520foundation%2520models%2520for%2520hyperspectral%2520image%2520classification%253A%250A%2520%2520Application%2520to%2520cereal%2520crop%2520type%2520mapping%26entry.906535625%3DWalid%2520Elbarz%2520and%2520Mohamed%2520Bourriz%2520and%2520Hicham%2520Hajji%2520and%2520Hamd%2520Ait%2520Abdelali%2520and%2520Fran%25C3%25A7ois%2520Bourzeix%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520transforming%2520Earth%2520observation%252C%2520but%2520their%2520potential%2520for%250Ahyperspectral%2520crop%2520mapping%2520remains%2520underexplored.%2520This%2520study%2520benchmarks%2520three%250Afoundation%2520models%2520for%2520cereal%2520crop%2520mapping%2520using%2520hyperspectral%2520imagery%253A%250AHyperSigma%252C%2520DOFA%252C%2520and%2520Vision%2520Transformers%2520pre-trained%2520on%2520the%2520SpectralEarth%250Adataset%2520%2528a%2520large%2520multitemporal%2520hyperspectral%2520archive%2529.%2520Models%2520were%2520fine-tuned%250Aon%2520manually%2520labeled%2520data%2520from%2520a%2520training%2520region%2520and%2520evaluated%2520on%2520an%2520independent%250Atest%2520region.%2520Performance%2520was%2520measured%2520with%2520overall%2520accuracy%2520%2528OA%2529%252C%2520average%250Aaccuracy%2520%2528AA%2529%252C%2520and%2520F1-score.%2520HyperSigma%2520achieved%2520an%2520OA%2520of%252034.5%2525%2520%2528%252B/-%25201.8%2525%2529%252C%250ADOFA%2520reached%252062.6%2525%2520%2528%252B/-%25203.5%2525%2529%252C%2520and%2520the%2520SpectralEarth%2520model%2520achieved%2520an%2520OA%2520of%250A93.5%2525%2520%2528%252B/-%25200.8%2525%2529.%2520A%2520compact%2520SpectralEarth%2520variant%2520trained%2520from%2520scratch%2520achieved%250A91%2525%252C%2520highlighting%2520the%2520importance%2520of%2520model%2520architecture%2520for%2520strong%250Ageneralization%2520across%2520geographic%2520regions%2520and%2520sensor%2520platforms.%2520These%2520results%250Aprovide%2520a%2520systematic%2520evaluation%2520of%2520foundation%2520models%2520for%2520operational%250Ahyperspectral%2520crop%2520mapping%2520and%2520outline%2520directions%2520for%2520future%2520model%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20foundation%20models%20for%20hyperspectral%20image%20classification%3A%0A%20%20Application%20to%20cereal%20crop%20type%20mapping&entry.906535625=Walid%20Elbarz%20and%20Mohamed%20Bourriz%20and%20Hicham%20Hajji%20and%20Hamd%20Ait%20Abdelali%20and%20Fran%C3%A7ois%20Bourzeix&entry.1292438233=%20%20Foundation%20models%20are%20transforming%20Earth%20observation%2C%20but%20their%20potential%20for%0Ahyperspectral%20crop%20mapping%20remains%20underexplored.%20This%20study%20benchmarks%20three%0Afoundation%20models%20for%20cereal%20crop%20mapping%20using%20hyperspectral%20imagery%3A%0AHyperSigma%2C%20DOFA%2C%20and%20Vision%20Transformers%20pre-trained%20on%20the%20SpectralEarth%0Adataset%20%28a%20large%20multitemporal%20hyperspectral%20archive%29.%20Models%20were%20fine-tuned%0Aon%20manually%20labeled%20data%20from%20a%20training%20region%20and%20evaluated%20on%20an%20independent%0Atest%20region.%20Performance%20was%20measured%20with%20overall%20accuracy%20%28OA%29%2C%20average%0Aaccuracy%20%28AA%29%2C%20and%20F1-score.%20HyperSigma%20achieved%20an%20OA%20of%2034.5%25%20%28%2B/-%201.8%25%29%2C%0ADOFA%20reached%2062.6%25%20%28%2B/-%203.5%25%29%2C%20and%20the%20SpectralEarth%20model%20achieved%20an%20OA%20of%0A93.5%25%20%28%2B/-%200.8%25%29.%20A%20compact%20SpectralEarth%20variant%20trained%20from%20scratch%20achieved%0A91%25%2C%20highlighting%20the%20importance%20of%20model%20architecture%20for%20strong%0Ageneralization%20across%20geographic%20regions%20and%20sensor%20platforms.%20These%20results%0Aprovide%20a%20systematic%20evaluation%20of%20foundation%20models%20for%20operational%0Ahyperspectral%20crop%20mapping%20and%20outline%20directions%20for%20future%20model%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11576v1&entry.124074799=Read"},
{"title": "The Hidden Link Between RLHF and Contrastive Learning", "author": "Xufei Lv and Kehai Chen and Haoyuan Sun and Xuefeng Bai and Min Zhang and Houde Liu and Kehai Chen", "abstract": "  Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be interpreted as methods that performing\ncontrastive learning based on the positive and negative samples derived from\nbase model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). Such paradigm further illuminates why RLHF\nmay not intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on the perspective, we replace the\nDV/MINE bound with the Jensen-Shannon (JS) MI estimator and propose the Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.\n", "link": "http://arxiv.org/abs/2506.22578v2", "date": "2025-10-13", "relevancy": 2.5477, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5203}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Hidden%20Link%20Between%20RLHF%20and%20Contrastive%20Learning&body=Title%3A%20The%20Hidden%20Link%20Between%20RLHF%20and%20Contrastive%20Learning%0AAuthor%3A%20Xufei%20Lv%20and%20Kehai%20Chen%20and%20Haoyuan%20Sun%20and%20Xuefeng%20Bai%20and%20Min%20Zhang%20and%20Houde%20Liu%20and%20Kehai%20Chen%0AAbstract%3A%20%20%20Alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20values%20has%20recently%0Agarnered%20significant%20attention%2C%20with%20prominent%20examples%20including%20the%20canonical%0Ayet%20costly%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20and%20the%20simple%0ADirect%20Preference%20Optimization%20%28DPO%29.%20In%20this%20work%2C%20we%20demonstrate%20that%20both%0ARLHF%20and%20DPO%20can%20be%20interpreted%20from%20the%20perspective%20of%20mutual%20information%20%28MI%29%0Amaximization%2C%20uncovering%20a%20profound%20connection%20to%20contrastive%20learning.%20Within%0Athis%20framework%2C%20both%20RLHF%20and%20DPO%20can%20be%20interpreted%20as%20methods%20that%20performing%0Acontrastive%20learning%20based%20on%20the%20positive%20and%20negative%20samples%20derived%20from%0Abase%20model%2C%20leveraging%20the%20Donsker-Varadhan%20%28DV%29%20lower%20bound%20on%20MI%0A%28equivalently%2C%20the%20MINE%20estimator%29.%20Such%20paradigm%20further%20illuminates%20why%20RLHF%0Amay%20not%20intrinsically%20incentivize%20reasoning%20capacities%20in%20LLMs%20beyond%20what%20is%0Aalready%20present%20in%20the%20base%20model.%20Building%20on%20the%20perspective%2C%20we%20replace%20the%0ADV/MINE%20bound%20with%20the%20Jensen-Shannon%20%28JS%29%20MI%20estimator%20and%20propose%20the%20Mutual%0AInformation%20Optimization%20%28MIO%29.%20Comprehensive%20theoretical%20analysis%20and%0Aextensive%20empirical%20evaluations%20demonstrate%20that%20MIO%20mitigates%20the%20late-stage%0Adecline%20in%20chosen-likelihood%20observed%20in%20DPO%2C%20achieving%20competitive%20or%20superior%0Aperformance%20across%20various%20challenging%20reasoning%20and%20mathematical%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.22578v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Hidden%2520Link%2520Between%2520RLHF%2520and%2520Contrastive%2520Learning%26entry.906535625%3DXufei%2520Lv%2520and%2520Kehai%2520Chen%2520and%2520Haoyuan%2520Sun%2520and%2520Xuefeng%2520Bai%2520and%2520Min%2520Zhang%2520and%2520Houde%2520Liu%2520and%2520Kehai%2520Chen%26entry.1292438233%3D%2520%2520Alignment%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520values%2520has%2520recently%250Agarnered%2520significant%2520attention%252C%2520with%2520prominent%2520examples%2520including%2520the%2520canonical%250Ayet%2520costly%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520and%2520the%2520simple%250ADirect%2520Preference%2520Optimization%2520%2528DPO%2529.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520both%250ARLHF%2520and%2520DPO%2520can%2520be%2520interpreted%2520from%2520the%2520perspective%2520of%2520mutual%2520information%2520%2528MI%2529%250Amaximization%252C%2520uncovering%2520a%2520profound%2520connection%2520to%2520contrastive%2520learning.%2520Within%250Athis%2520framework%252C%2520both%2520RLHF%2520and%2520DPO%2520can%2520be%2520interpreted%2520as%2520methods%2520that%2520performing%250Acontrastive%2520learning%2520based%2520on%2520the%2520positive%2520and%2520negative%2520samples%2520derived%2520from%250Abase%2520model%252C%2520leveraging%2520the%2520Donsker-Varadhan%2520%2528DV%2529%2520lower%2520bound%2520on%2520MI%250A%2528equivalently%252C%2520the%2520MINE%2520estimator%2529.%2520Such%2520paradigm%2520further%2520illuminates%2520why%2520RLHF%250Amay%2520not%2520intrinsically%2520incentivize%2520reasoning%2520capacities%2520in%2520LLMs%2520beyond%2520what%2520is%250Aalready%2520present%2520in%2520the%2520base%2520model.%2520Building%2520on%2520the%2520perspective%252C%2520we%2520replace%2520the%250ADV/MINE%2520bound%2520with%2520the%2520Jensen-Shannon%2520%2528JS%2529%2520MI%2520estimator%2520and%2520propose%2520the%2520Mutual%250AInformation%2520Optimization%2520%2528MIO%2529.%2520Comprehensive%2520theoretical%2520analysis%2520and%250Aextensive%2520empirical%2520evaluations%2520demonstrate%2520that%2520MIO%2520mitigates%2520the%2520late-stage%250Adecline%2520in%2520chosen-likelihood%2520observed%2520in%2520DPO%252C%2520achieving%2520competitive%2520or%2520superior%250Aperformance%2520across%2520various%2520challenging%2520reasoning%2520and%2520mathematical%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.22578v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Hidden%20Link%20Between%20RLHF%20and%20Contrastive%20Learning&entry.906535625=Xufei%20Lv%20and%20Kehai%20Chen%20and%20Haoyuan%20Sun%20and%20Xuefeng%20Bai%20and%20Min%20Zhang%20and%20Houde%20Liu%20and%20Kehai%20Chen&entry.1292438233=%20%20Alignment%20of%20large%20language%20models%20%28LLMs%29%20with%20human%20values%20has%20recently%0Agarnered%20significant%20attention%2C%20with%20prominent%20examples%20including%20the%20canonical%0Ayet%20costly%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20and%20the%20simple%0ADirect%20Preference%20Optimization%20%28DPO%29.%20In%20this%20work%2C%20we%20demonstrate%20that%20both%0ARLHF%20and%20DPO%20can%20be%20interpreted%20from%20the%20perspective%20of%20mutual%20information%20%28MI%29%0Amaximization%2C%20uncovering%20a%20profound%20connection%20to%20contrastive%20learning.%20Within%0Athis%20framework%2C%20both%20RLHF%20and%20DPO%20can%20be%20interpreted%20as%20methods%20that%20performing%0Acontrastive%20learning%20based%20on%20the%20positive%20and%20negative%20samples%20derived%20from%0Abase%20model%2C%20leveraging%20the%20Donsker-Varadhan%20%28DV%29%20lower%20bound%20on%20MI%0A%28equivalently%2C%20the%20MINE%20estimator%29.%20Such%20paradigm%20further%20illuminates%20why%20RLHF%0Amay%20not%20intrinsically%20incentivize%20reasoning%20capacities%20in%20LLMs%20beyond%20what%20is%0Aalready%20present%20in%20the%20base%20model.%20Building%20on%20the%20perspective%2C%20we%20replace%20the%0ADV/MINE%20bound%20with%20the%20Jensen-Shannon%20%28JS%29%20MI%20estimator%20and%20propose%20the%20Mutual%0AInformation%20Optimization%20%28MIO%29.%20Comprehensive%20theoretical%20analysis%20and%0Aextensive%20empirical%20evaluations%20demonstrate%20that%20MIO%20mitigates%20the%20late-stage%0Adecline%20in%20chosen-likelihood%20observed%20in%20DPO%2C%20achieving%20competitive%20or%20superior%0Aperformance%20across%20various%20challenging%20reasoning%20and%20mathematical%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.22578v2&entry.124074799=Read"},
{"title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning", "author": "Yicheng Xu and Yue Wu and Jiashuo Yu and Ziang Yan and Tianxiang Jiang and Yinan He and Qingsong Zhao and Kai Chen and Yu Qiao and Limin Wang and Manabu Okumura and Yi Wang", "abstract": "  Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.\n", "link": "http://arxiv.org/abs/2510.11606v1", "date": "2025-10-13", "relevancy": 2.4739, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6282}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExpVid%3A%20A%20Benchmark%20for%20Experiment%20Video%20Understanding%20%26%20Reasoning&body=Title%3A%20ExpVid%3A%20A%20Benchmark%20for%20Experiment%20Video%20Understanding%20%26%20Reasoning%0AAuthor%3A%20Yicheng%20Xu%20and%20Yue%20Wu%20and%20Jiashuo%20Yu%20and%20Ziang%20Yan%20and%20Tianxiang%20Jiang%20and%20Yinan%20He%20and%20Qingsong%20Zhao%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Limin%20Wang%20and%20Manabu%20Okumura%20and%20Yi%20Wang%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hold%20promise%20for%20accelerating%0Ascientific%20discovery%20by%20interpreting%20complex%20experimental%20procedures.%20However%2C%0Atheir%20true%20capabilities%20are%20poorly%20understood%2C%20as%20existing%20benchmarks%20neglect%0Athe%20fine-grained%20and%20long-horizon%20nature%20of%20authentic%20laboratory%20work%2C%0Aespecially%20in%20wet-lab%20settings.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ExpVid%2C%20the%0Afirst%20benchmark%20designed%20to%20systematically%20evaluate%20MLLMs%20on%20scientific%0Aexperiment%20videos.%20Curated%20from%20peer-reviewed%20video%20publications%2C%20ExpVid%0Afeatures%20a%20new%20three-level%20task%20hierarchy%20that%20mirrors%20the%20scientific%20process%3A%0A%281%29%20Fine-grained%20Perception%20of%20tools%2C%20materials%2C%20and%20actions%3B%20%282%29%20Procedural%0AUnderstanding%20of%20step%20order%20and%20completeness%3B%20and%20%283%29%20Scientific%20Reasoning%20that%0Aconnects%20the%20full%20experiment%20to%20its%20published%20conclusions.%20Our%20vision-centric%0Aannotation%20pipeline%2C%20combining%20automated%20generation%20with%20multi-disciplinary%0Aexpert%20validation%2C%20ensures%20that%20tasks%20require%20visual%20grounding.%20We%20evaluate%2019%0Aleading%20MLLMs%20on%20ExpVid%20and%20find%20that%20while%20they%20excel%20at%20coarse-grained%0Arecognition%2C%20they%20struggle%20with%20disambiguating%20fine%20details%2C%20tracking%20state%0Achanges%20over%20time%2C%20and%20linking%20experimental%20procedures%20to%20scientific%20outcomes.%0AOur%20results%20reveal%20a%20notable%20performance%20gap%20between%20proprietary%20and%0Aopen-source%20models%2C%20particularly%20in%20high-order%20reasoning.%20ExpVid%20not%20only%0Aprovides%20a%20diagnostic%20tool%20but%20also%20charts%20a%20roadmap%20for%20developing%20MLLMs%0Acapable%20of%20becoming%20trustworthy%20partners%20in%20scientific%20experimentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpVid%253A%2520A%2520Benchmark%2520for%2520Experiment%2520Video%2520Understanding%2520%2526%2520Reasoning%26entry.906535625%3DYicheng%2520Xu%2520and%2520Yue%2520Wu%2520and%2520Jiashuo%2520Yu%2520and%2520Ziang%2520Yan%2520and%2520Tianxiang%2520Jiang%2520and%2520Yinan%2520He%2520and%2520Qingsong%2520Zhao%2520and%2520Kai%2520Chen%2520and%2520Yu%2520Qiao%2520and%2520Limin%2520Wang%2520and%2520Manabu%2520Okumura%2520and%2520Yi%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520hold%2520promise%2520for%2520accelerating%250Ascientific%2520discovery%2520by%2520interpreting%2520complex%2520experimental%2520procedures.%2520However%252C%250Atheir%2520true%2520capabilities%2520are%2520poorly%2520understood%252C%2520as%2520existing%2520benchmarks%2520neglect%250Athe%2520fine-grained%2520and%2520long-horizon%2520nature%2520of%2520authentic%2520laboratory%2520work%252C%250Aespecially%2520in%2520wet-lab%2520settings.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520ExpVid%252C%2520the%250Afirst%2520benchmark%2520designed%2520to%2520systematically%2520evaluate%2520MLLMs%2520on%2520scientific%250Aexperiment%2520videos.%2520Curated%2520from%2520peer-reviewed%2520video%2520publications%252C%2520ExpVid%250Afeatures%2520a%2520new%2520three-level%2520task%2520hierarchy%2520that%2520mirrors%2520the%2520scientific%2520process%253A%250A%25281%2529%2520Fine-grained%2520Perception%2520of%2520tools%252C%2520materials%252C%2520and%2520actions%253B%2520%25282%2529%2520Procedural%250AUnderstanding%2520of%2520step%2520order%2520and%2520completeness%253B%2520and%2520%25283%2529%2520Scientific%2520Reasoning%2520that%250Aconnects%2520the%2520full%2520experiment%2520to%2520its%2520published%2520conclusions.%2520Our%2520vision-centric%250Aannotation%2520pipeline%252C%2520combining%2520automated%2520generation%2520with%2520multi-disciplinary%250Aexpert%2520validation%252C%2520ensures%2520that%2520tasks%2520require%2520visual%2520grounding.%2520We%2520evaluate%252019%250Aleading%2520MLLMs%2520on%2520ExpVid%2520and%2520find%2520that%2520while%2520they%2520excel%2520at%2520coarse-grained%250Arecognition%252C%2520they%2520struggle%2520with%2520disambiguating%2520fine%2520details%252C%2520tracking%2520state%250Achanges%2520over%2520time%252C%2520and%2520linking%2520experimental%2520procedures%2520to%2520scientific%2520outcomes.%250AOur%2520results%2520reveal%2520a%2520notable%2520performance%2520gap%2520between%2520proprietary%2520and%250Aopen-source%2520models%252C%2520particularly%2520in%2520high-order%2520reasoning.%2520ExpVid%2520not%2520only%250Aprovides%2520a%2520diagnostic%2520tool%2520but%2520also%2520charts%2520a%2520roadmap%2520for%2520developing%2520MLLMs%250Acapable%2520of%2520becoming%2520trustworthy%2520partners%2520in%2520scientific%2520experimentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExpVid%3A%20A%20Benchmark%20for%20Experiment%20Video%20Understanding%20%26%20Reasoning&entry.906535625=Yicheng%20Xu%20and%20Yue%20Wu%20and%20Jiashuo%20Yu%20and%20Ziang%20Yan%20and%20Tianxiang%20Jiang%20and%20Yinan%20He%20and%20Qingsong%20Zhao%20and%20Kai%20Chen%20and%20Yu%20Qiao%20and%20Limin%20Wang%20and%20Manabu%20Okumura%20and%20Yi%20Wang&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20hold%20promise%20for%20accelerating%0Ascientific%20discovery%20by%20interpreting%20complex%20experimental%20procedures.%20However%2C%0Atheir%20true%20capabilities%20are%20poorly%20understood%2C%20as%20existing%20benchmarks%20neglect%0Athe%20fine-grained%20and%20long-horizon%20nature%20of%20authentic%20laboratory%20work%2C%0Aespecially%20in%20wet-lab%20settings.%20To%20bridge%20this%20gap%2C%20we%20introduce%20ExpVid%2C%20the%0Afirst%20benchmark%20designed%20to%20systematically%20evaluate%20MLLMs%20on%20scientific%0Aexperiment%20videos.%20Curated%20from%20peer-reviewed%20video%20publications%2C%20ExpVid%0Afeatures%20a%20new%20three-level%20task%20hierarchy%20that%20mirrors%20the%20scientific%20process%3A%0A%281%29%20Fine-grained%20Perception%20of%20tools%2C%20materials%2C%20and%20actions%3B%20%282%29%20Procedural%0AUnderstanding%20of%20step%20order%20and%20completeness%3B%20and%20%283%29%20Scientific%20Reasoning%20that%0Aconnects%20the%20full%20experiment%20to%20its%20published%20conclusions.%20Our%20vision-centric%0Aannotation%20pipeline%2C%20combining%20automated%20generation%20with%20multi-disciplinary%0Aexpert%20validation%2C%20ensures%20that%20tasks%20require%20visual%20grounding.%20We%20evaluate%2019%0Aleading%20MLLMs%20on%20ExpVid%20and%20find%20that%20while%20they%20excel%20at%20coarse-grained%0Arecognition%2C%20they%20struggle%20with%20disambiguating%20fine%20details%2C%20tracking%20state%0Achanges%20over%20time%2C%20and%20linking%20experimental%20procedures%20to%20scientific%20outcomes.%0AOur%20results%20reveal%20a%20notable%20performance%20gap%20between%20proprietary%20and%0Aopen-source%20models%2C%20particularly%20in%20high-order%20reasoning.%20ExpVid%20not%20only%0Aprovides%20a%20diagnostic%20tool%20but%20also%20charts%20a%20roadmap%20for%20developing%20MLLMs%0Acapable%20of%20becoming%20trustworthy%20partners%20in%20scientific%20experimentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11606v1&entry.124074799=Read"},
{"title": "Adversarial Attacks Leverage Interference Between Features in\n  Superposition", "author": "Edward Stevinson and Lucas Prieto and Melih Barsbey and Tolga Birdal", "abstract": "  Fundamental questions remain about when and why adversarial examples arise in\nneural networks, with competing views characterising them either as artifacts\nof the irregularities in the decision landscape or as products of sensitivity\nto non-robust input features. In this paper, we instead argue that adversarial\nvulnerability can stem from efficient information encoding in neural networks.\nSpecifically, we show how superposition - where networks represent more\nfeatures than they have dimensions - creates arrangements of latent\nrepresentations that adversaries can exploit. We demonstrate that adversarial\nperturbations leverage interference between superposed features, making attack\npatterns predictable from feature arrangements. Our framework provides a\nmechanistic explanation for two known phenomena: adversarial attack\ntransferability between models with similar training regimes and class-specific\nvulnerability patterns. In synthetic settings with precisely controlled\nsuperposition, we establish that superposition suffices to create adversarial\nvulnerability. We then demonstrate that these findings persist in a ViT trained\non CIFAR-10. These findings reveal adversarial vulnerability can be a byproduct\nof networks' representational compression, rather than flaws in the learning\nprocess or non-robust inputs.\n", "link": "http://arxiv.org/abs/2510.11709v1", "date": "2025-10-13", "relevancy": 2.4085, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5035}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.472}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attacks%20Leverage%20Interference%20Between%20Features%20in%0A%20%20Superposition&body=Title%3A%20Adversarial%20Attacks%20Leverage%20Interference%20Between%20Features%20in%0A%20%20Superposition%0AAuthor%3A%20Edward%20Stevinson%20and%20Lucas%20Prieto%20and%20Melih%20Barsbey%20and%20Tolga%20Birdal%0AAbstract%3A%20%20%20Fundamental%20questions%20remain%20about%20when%20and%20why%20adversarial%20examples%20arise%20in%0Aneural%20networks%2C%20with%20competing%20views%20characterising%20them%20either%20as%20artifacts%0Aof%20the%20irregularities%20in%20the%20decision%20landscape%20or%20as%20products%20of%20sensitivity%0Ato%20non-robust%20input%20features.%20In%20this%20paper%2C%20we%20instead%20argue%20that%20adversarial%0Avulnerability%20can%20stem%20from%20efficient%20information%20encoding%20in%20neural%20networks.%0ASpecifically%2C%20we%20show%20how%20superposition%20-%20where%20networks%20represent%20more%0Afeatures%20than%20they%20have%20dimensions%20-%20creates%20arrangements%20of%20latent%0Arepresentations%20that%20adversaries%20can%20exploit.%20We%20demonstrate%20that%20adversarial%0Aperturbations%20leverage%20interference%20between%20superposed%20features%2C%20making%20attack%0Apatterns%20predictable%20from%20feature%20arrangements.%20Our%20framework%20provides%20a%0Amechanistic%20explanation%20for%20two%20known%20phenomena%3A%20adversarial%20attack%0Atransferability%20between%20models%20with%20similar%20training%20regimes%20and%20class-specific%0Avulnerability%20patterns.%20In%20synthetic%20settings%20with%20precisely%20controlled%0Asuperposition%2C%20we%20establish%20that%20superposition%20suffices%20to%20create%20adversarial%0Avulnerability.%20We%20then%20demonstrate%20that%20these%20findings%20persist%20in%20a%20ViT%20trained%0Aon%20CIFAR-10.%20These%20findings%20reveal%20adversarial%20vulnerability%20can%20be%20a%20byproduct%0Aof%20networks%27%20representational%20compression%2C%20rather%20than%20flaws%20in%20the%20learning%0Aprocess%20or%20non-robust%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Attacks%2520Leverage%2520Interference%2520Between%2520Features%2520in%250A%2520%2520Superposition%26entry.906535625%3DEdward%2520Stevinson%2520and%2520Lucas%2520Prieto%2520and%2520Melih%2520Barsbey%2520and%2520Tolga%2520Birdal%26entry.1292438233%3D%2520%2520Fundamental%2520questions%2520remain%2520about%2520when%2520and%2520why%2520adversarial%2520examples%2520arise%2520in%250Aneural%2520networks%252C%2520with%2520competing%2520views%2520characterising%2520them%2520either%2520as%2520artifacts%250Aof%2520the%2520irregularities%2520in%2520the%2520decision%2520landscape%2520or%2520as%2520products%2520of%2520sensitivity%250Ato%2520non-robust%2520input%2520features.%2520In%2520this%2520paper%252C%2520we%2520instead%2520argue%2520that%2520adversarial%250Avulnerability%2520can%2520stem%2520from%2520efficient%2520information%2520encoding%2520in%2520neural%2520networks.%250ASpecifically%252C%2520we%2520show%2520how%2520superposition%2520-%2520where%2520networks%2520represent%2520more%250Afeatures%2520than%2520they%2520have%2520dimensions%2520-%2520creates%2520arrangements%2520of%2520latent%250Arepresentations%2520that%2520adversaries%2520can%2520exploit.%2520We%2520demonstrate%2520that%2520adversarial%250Aperturbations%2520leverage%2520interference%2520between%2520superposed%2520features%252C%2520making%2520attack%250Apatterns%2520predictable%2520from%2520feature%2520arrangements.%2520Our%2520framework%2520provides%2520a%250Amechanistic%2520explanation%2520for%2520two%2520known%2520phenomena%253A%2520adversarial%2520attack%250Atransferability%2520between%2520models%2520with%2520similar%2520training%2520regimes%2520and%2520class-specific%250Avulnerability%2520patterns.%2520In%2520synthetic%2520settings%2520with%2520precisely%2520controlled%250Asuperposition%252C%2520we%2520establish%2520that%2520superposition%2520suffices%2520to%2520create%2520adversarial%250Avulnerability.%2520We%2520then%2520demonstrate%2520that%2520these%2520findings%2520persist%2520in%2520a%2520ViT%2520trained%250Aon%2520CIFAR-10.%2520These%2520findings%2520reveal%2520adversarial%2520vulnerability%2520can%2520be%2520a%2520byproduct%250Aof%2520networks%2527%2520representational%2520compression%252C%2520rather%2520than%2520flaws%2520in%2520the%2520learning%250Aprocess%2520or%2520non-robust%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attacks%20Leverage%20Interference%20Between%20Features%20in%0A%20%20Superposition&entry.906535625=Edward%20Stevinson%20and%20Lucas%20Prieto%20and%20Melih%20Barsbey%20and%20Tolga%20Birdal&entry.1292438233=%20%20Fundamental%20questions%20remain%20about%20when%20and%20why%20adversarial%20examples%20arise%20in%0Aneural%20networks%2C%20with%20competing%20views%20characterising%20them%20either%20as%20artifacts%0Aof%20the%20irregularities%20in%20the%20decision%20landscape%20or%20as%20products%20of%20sensitivity%0Ato%20non-robust%20input%20features.%20In%20this%20paper%2C%20we%20instead%20argue%20that%20adversarial%0Avulnerability%20can%20stem%20from%20efficient%20information%20encoding%20in%20neural%20networks.%0ASpecifically%2C%20we%20show%20how%20superposition%20-%20where%20networks%20represent%20more%0Afeatures%20than%20they%20have%20dimensions%20-%20creates%20arrangements%20of%20latent%0Arepresentations%20that%20adversaries%20can%20exploit.%20We%20demonstrate%20that%20adversarial%0Aperturbations%20leverage%20interference%20between%20superposed%20features%2C%20making%20attack%0Apatterns%20predictable%20from%20feature%20arrangements.%20Our%20framework%20provides%20a%0Amechanistic%20explanation%20for%20two%20known%20phenomena%3A%20adversarial%20attack%0Atransferability%20between%20models%20with%20similar%20training%20regimes%20and%20class-specific%0Avulnerability%20patterns.%20In%20synthetic%20settings%20with%20precisely%20controlled%0Asuperposition%2C%20we%20establish%20that%20superposition%20suffices%20to%20create%20adversarial%0Avulnerability.%20We%20then%20demonstrate%20that%20these%20findings%20persist%20in%20a%20ViT%20trained%0Aon%20CIFAR-10.%20These%20findings%20reveal%20adversarial%20vulnerability%20can%20be%20a%20byproduct%0Aof%20networks%27%20representational%20compression%2C%20rather%20than%20flaws%20in%20the%20learning%0Aprocess%20or%20non-robust%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11709v1&entry.124074799=Read"},
{"title": "Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape\n  Estimation from a Single View", "author": "Jinyu Zhang and Haitao Lin and Jiashu Hou and Xiangyang Xue and Yanwei Fu", "abstract": "  Estimating an object's 6D pose, size, and shape from visual input is a\nfundamental problem in computer vision, with critical applications in robotic\ngrasping and manipulation. Existing methods either rely on object-specific\npriors such as CAD models or templates, or suffer from limited generalization\nacross categories due to pose-shape entanglement and multi-stage pipelines. In\nthis work, we propose a unified, category-agnostic framework that\nsimultaneously predicts 6D pose, size, and dense shape from a single RGB-D\nimage, without requiring templates, CAD models, or category labels at test\ntime. Our model fuses dense 2D features from vision foundation models with\npartial 3D point clouds using a Transformer encoder enhanced by a\nMixture-of-Experts, and employs parallel decoders for pose-size estimation and\nshape reconstruction, achieving real-time inference at 28 FPS. Trained solely\non synthetic data from 149 categories in the SOPE dataset, our framework is\nevaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,\nspanning over 300 categories. It achieves state-of-the-art accuracy on seen\ncategories while demonstrating remarkably strong zero-shot generalization to\nunseen real-world objects, establishing a new standard for open-set 6D\nunderstanding in robotics and embodied AI.\n", "link": "http://arxiv.org/abs/2510.11687v1", "date": "2025-10-13", "relevancy": 2.4073, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6065}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6062}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20%27Templates%27%3A%20Category-Agnostic%20Object%20Pose%2C%20Size%2C%20and%20Shape%0A%20%20Estimation%20from%20a%20Single%20View&body=Title%3A%20Beyond%20%27Templates%27%3A%20Category-Agnostic%20Object%20Pose%2C%20Size%2C%20and%20Shape%0A%20%20Estimation%20from%20a%20Single%20View%0AAuthor%3A%20Jinyu%20Zhang%20and%20Haitao%20Lin%20and%20Jiashu%20Hou%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Estimating%20an%20object%27s%206D%20pose%2C%20size%2C%20and%20shape%20from%20visual%20input%20is%20a%0Afundamental%20problem%20in%20computer%20vision%2C%20with%20critical%20applications%20in%20robotic%0Agrasping%20and%20manipulation.%20Existing%20methods%20either%20rely%20on%20object-specific%0Apriors%20such%20as%20CAD%20models%20or%20templates%2C%20or%20suffer%20from%20limited%20generalization%0Aacross%20categories%20due%20to%20pose-shape%20entanglement%20and%20multi-stage%20pipelines.%20In%0Athis%20work%2C%20we%20propose%20a%20unified%2C%20category-agnostic%20framework%20that%0Asimultaneously%20predicts%206D%20pose%2C%20size%2C%20and%20dense%20shape%20from%20a%20single%20RGB-D%0Aimage%2C%20without%20requiring%20templates%2C%20CAD%20models%2C%20or%20category%20labels%20at%20test%0Atime.%20Our%20model%20fuses%20dense%202D%20features%20from%20vision%20foundation%20models%20with%0Apartial%203D%20point%20clouds%20using%20a%20Transformer%20encoder%20enhanced%20by%20a%0AMixture-of-Experts%2C%20and%20employs%20parallel%20decoders%20for%20pose-size%20estimation%20and%0Ashape%20reconstruction%2C%20achieving%20real-time%20inference%20at%2028%20FPS.%20Trained%20solely%0Aon%20synthetic%20data%20from%20149%20categories%20in%20the%20SOPE%20dataset%2C%20our%20framework%20is%0Aevaluated%20on%20four%20diverse%20benchmarks%20SOPE%2C%20ROPE%2C%20ObjaversePose%2C%20and%20HANDAL%2C%0Aspanning%20over%20300%20categories.%20It%20achieves%20state-of-the-art%20accuracy%20on%20seen%0Acategories%20while%20demonstrating%20remarkably%20strong%20zero-shot%20generalization%20to%0Aunseen%20real-world%20objects%2C%20establishing%20a%20new%20standard%20for%20open-set%206D%0Aunderstanding%20in%20robotics%20and%20embodied%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520%2527Templates%2527%253A%2520Category-Agnostic%2520Object%2520Pose%252C%2520Size%252C%2520and%2520Shape%250A%2520%2520Estimation%2520from%2520a%2520Single%2520View%26entry.906535625%3DJinyu%2520Zhang%2520and%2520Haitao%2520Lin%2520and%2520Jiashu%2520Hou%2520and%2520Xiangyang%2520Xue%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520Estimating%2520an%2520object%2527s%25206D%2520pose%252C%2520size%252C%2520and%2520shape%2520from%2520visual%2520input%2520is%2520a%250Afundamental%2520problem%2520in%2520computer%2520vision%252C%2520with%2520critical%2520applications%2520in%2520robotic%250Agrasping%2520and%2520manipulation.%2520Existing%2520methods%2520either%2520rely%2520on%2520object-specific%250Apriors%2520such%2520as%2520CAD%2520models%2520or%2520templates%252C%2520or%2520suffer%2520from%2520limited%2520generalization%250Aacross%2520categories%2520due%2520to%2520pose-shape%2520entanglement%2520and%2520multi-stage%2520pipelines.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520unified%252C%2520category-agnostic%2520framework%2520that%250Asimultaneously%2520predicts%25206D%2520pose%252C%2520size%252C%2520and%2520dense%2520shape%2520from%2520a%2520single%2520RGB-D%250Aimage%252C%2520without%2520requiring%2520templates%252C%2520CAD%2520models%252C%2520or%2520category%2520labels%2520at%2520test%250Atime.%2520Our%2520model%2520fuses%2520dense%25202D%2520features%2520from%2520vision%2520foundation%2520models%2520with%250Apartial%25203D%2520point%2520clouds%2520using%2520a%2520Transformer%2520encoder%2520enhanced%2520by%2520a%250AMixture-of-Experts%252C%2520and%2520employs%2520parallel%2520decoders%2520for%2520pose-size%2520estimation%2520and%250Ashape%2520reconstruction%252C%2520achieving%2520real-time%2520inference%2520at%252028%2520FPS.%2520Trained%2520solely%250Aon%2520synthetic%2520data%2520from%2520149%2520categories%2520in%2520the%2520SOPE%2520dataset%252C%2520our%2520framework%2520is%250Aevaluated%2520on%2520four%2520diverse%2520benchmarks%2520SOPE%252C%2520ROPE%252C%2520ObjaversePose%252C%2520and%2520HANDAL%252C%250Aspanning%2520over%2520300%2520categories.%2520It%2520achieves%2520state-of-the-art%2520accuracy%2520on%2520seen%250Acategories%2520while%2520demonstrating%2520remarkably%2520strong%2520zero-shot%2520generalization%2520to%250Aunseen%2520real-world%2520objects%252C%2520establishing%2520a%2520new%2520standard%2520for%2520open-set%25206D%250Aunderstanding%2520in%2520robotics%2520and%2520embodied%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20%27Templates%27%3A%20Category-Agnostic%20Object%20Pose%2C%20Size%2C%20and%20Shape%0A%20%20Estimation%20from%20a%20Single%20View&entry.906535625=Jinyu%20Zhang%20and%20Haitao%20Lin%20and%20Jiashu%20Hou%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu&entry.1292438233=%20%20Estimating%20an%20object%27s%206D%20pose%2C%20size%2C%20and%20shape%20from%20visual%20input%20is%20a%0Afundamental%20problem%20in%20computer%20vision%2C%20with%20critical%20applications%20in%20robotic%0Agrasping%20and%20manipulation.%20Existing%20methods%20either%20rely%20on%20object-specific%0Apriors%20such%20as%20CAD%20models%20or%20templates%2C%20or%20suffer%20from%20limited%20generalization%0Aacross%20categories%20due%20to%20pose-shape%20entanglement%20and%20multi-stage%20pipelines.%20In%0Athis%20work%2C%20we%20propose%20a%20unified%2C%20category-agnostic%20framework%20that%0Asimultaneously%20predicts%206D%20pose%2C%20size%2C%20and%20dense%20shape%20from%20a%20single%20RGB-D%0Aimage%2C%20without%20requiring%20templates%2C%20CAD%20models%2C%20or%20category%20labels%20at%20test%0Atime.%20Our%20model%20fuses%20dense%202D%20features%20from%20vision%20foundation%20models%20with%0Apartial%203D%20point%20clouds%20using%20a%20Transformer%20encoder%20enhanced%20by%20a%0AMixture-of-Experts%2C%20and%20employs%20parallel%20decoders%20for%20pose-size%20estimation%20and%0Ashape%20reconstruction%2C%20achieving%20real-time%20inference%20at%2028%20FPS.%20Trained%20solely%0Aon%20synthetic%20data%20from%20149%20categories%20in%20the%20SOPE%20dataset%2C%20our%20framework%20is%0Aevaluated%20on%20four%20diverse%20benchmarks%20SOPE%2C%20ROPE%2C%20ObjaversePose%2C%20and%20HANDAL%2C%0Aspanning%20over%20300%20categories.%20It%20achieves%20state-of-the-art%20accuracy%20on%20seen%0Acategories%20while%20demonstrating%20remarkably%20strong%20zero-shot%20generalization%20to%0Aunseen%20real-world%20objects%2C%20establishing%20a%20new%20standard%20for%20open-set%206D%0Aunderstanding%20in%20robotics%20and%20embodied%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11687v1&entry.124074799=Read"},
{"title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training", "author": "Haoran Feng and Dizhe Zhang and Xiangtai Li and Bo Du and Lu Qi", "abstract": "  In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.\n", "link": "http://arxiv.org/abs/2510.11712v1", "date": "2025-10-13", "relevancy": 2.3944, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5988}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5988}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiT360%3A%20High-Fidelity%20Panoramic%20Image%20Generation%20via%20Hybrid%20Training&body=Title%3A%20DiT360%3A%20High-Fidelity%20Panoramic%20Image%20Generation%20via%20Hybrid%20Training%0AAuthor%3A%20Haoran%20Feng%20and%20Dizhe%20Zhang%20and%20Xiangtai%20Li%20and%20Bo%20Du%20and%20Lu%20Qi%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20DiT360%2C%20a%20DiT-based%20framework%20that%20performs%20hybrid%0Atraining%20on%20perspective%20and%20panoramic%20data%20for%20panoramic%20image%20generation.%20For%0Athe%20issues%20of%20maintaining%20geometric%20fidelity%20and%20photorealism%20in%20generation%0Aquality%2C%20we%20attribute%20the%20main%20reason%20to%20the%20lack%20of%20large-scale%2C%20high-quality%2C%0Areal-world%20panoramic%20data%2C%20where%20such%20a%20data-centric%20view%20differs%20from%20prior%0Amethods%20that%20focus%20on%20model%20design.%20Basically%2C%20DiT360%20has%20several%20key%20modules%0Afor%20inter-domain%20transformation%20and%20intra-domain%20augmentation%2C%20applied%20at%20both%0Athe%20pre-VAE%20image%20level%20and%20the%20post-VAE%20token%20level.%20At%20the%20image%20level%2C%20we%0Aincorporate%20cross-domain%20knowledge%20through%20perspective%20image%20guidance%20and%0Apanoramic%20refinement%2C%20which%20enhance%20perceptual%20quality%20while%20regularizing%0Adiversity%20and%20photorealism.%20At%20the%20token%20level%2C%20hybrid%20supervision%20is%20applied%0Aacross%20multiple%20modules%2C%20which%20include%20circular%20padding%20for%20boundary%0Acontinuity%2C%20yaw%20loss%20for%20rotational%20robustness%2C%20and%20cube%20loss%20for%20distortion%0Aawareness.%20Extensive%20experiments%20on%20text-to-panorama%2C%20inpainting%2C%20and%0Aoutpainting%20tasks%20demonstrate%20that%20our%20method%20achieves%20better%20boundary%0Aconsistency%20and%20image%20fidelity%20across%20eleven%20quantitative%20metrics.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Insta360-Research-Team/DiT360.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiT360%253A%2520High-Fidelity%2520Panoramic%2520Image%2520Generation%2520via%2520Hybrid%2520Training%26entry.906535625%3DHaoran%2520Feng%2520and%2520Dizhe%2520Zhang%2520and%2520Xiangtai%2520Li%2520and%2520Bo%2520Du%2520and%2520Lu%2520Qi%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520DiT360%252C%2520a%2520DiT-based%2520framework%2520that%2520performs%2520hybrid%250Atraining%2520on%2520perspective%2520and%2520panoramic%2520data%2520for%2520panoramic%2520image%2520generation.%2520For%250Athe%2520issues%2520of%2520maintaining%2520geometric%2520fidelity%2520and%2520photorealism%2520in%2520generation%250Aquality%252C%2520we%2520attribute%2520the%2520main%2520reason%2520to%2520the%2520lack%2520of%2520large-scale%252C%2520high-quality%252C%250Areal-world%2520panoramic%2520data%252C%2520where%2520such%2520a%2520data-centric%2520view%2520differs%2520from%2520prior%250Amethods%2520that%2520focus%2520on%2520model%2520design.%2520Basically%252C%2520DiT360%2520has%2520several%2520key%2520modules%250Afor%2520inter-domain%2520transformation%2520and%2520intra-domain%2520augmentation%252C%2520applied%2520at%2520both%250Athe%2520pre-VAE%2520image%2520level%2520and%2520the%2520post-VAE%2520token%2520level.%2520At%2520the%2520image%2520level%252C%2520we%250Aincorporate%2520cross-domain%2520knowledge%2520through%2520perspective%2520image%2520guidance%2520and%250Apanoramic%2520refinement%252C%2520which%2520enhance%2520perceptual%2520quality%2520while%2520regularizing%250Adiversity%2520and%2520photorealism.%2520At%2520the%2520token%2520level%252C%2520hybrid%2520supervision%2520is%2520applied%250Aacross%2520multiple%2520modules%252C%2520which%2520include%2520circular%2520padding%2520for%2520boundary%250Acontinuity%252C%2520yaw%2520loss%2520for%2520rotational%2520robustness%252C%2520and%2520cube%2520loss%2520for%2520distortion%250Aawareness.%2520Extensive%2520experiments%2520on%2520text-to-panorama%252C%2520inpainting%252C%2520and%250Aoutpainting%2520tasks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520better%2520boundary%250Aconsistency%2520and%2520image%2520fidelity%2520across%2520eleven%2520quantitative%2520metrics.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/Insta360-Research-Team/DiT360.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiT360%3A%20High-Fidelity%20Panoramic%20Image%20Generation%20via%20Hybrid%20Training&entry.906535625=Haoran%20Feng%20and%20Dizhe%20Zhang%20and%20Xiangtai%20Li%20and%20Bo%20Du%20and%20Lu%20Qi&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20DiT360%2C%20a%20DiT-based%20framework%20that%20performs%20hybrid%0Atraining%20on%20perspective%20and%20panoramic%20data%20for%20panoramic%20image%20generation.%20For%0Athe%20issues%20of%20maintaining%20geometric%20fidelity%20and%20photorealism%20in%20generation%0Aquality%2C%20we%20attribute%20the%20main%20reason%20to%20the%20lack%20of%20large-scale%2C%20high-quality%2C%0Areal-world%20panoramic%20data%2C%20where%20such%20a%20data-centric%20view%20differs%20from%20prior%0Amethods%20that%20focus%20on%20model%20design.%20Basically%2C%20DiT360%20has%20several%20key%20modules%0Afor%20inter-domain%20transformation%20and%20intra-domain%20augmentation%2C%20applied%20at%20both%0Athe%20pre-VAE%20image%20level%20and%20the%20post-VAE%20token%20level.%20At%20the%20image%20level%2C%20we%0Aincorporate%20cross-domain%20knowledge%20through%20perspective%20image%20guidance%20and%0Apanoramic%20refinement%2C%20which%20enhance%20perceptual%20quality%20while%20regularizing%0Adiversity%20and%20photorealism.%20At%20the%20token%20level%2C%20hybrid%20supervision%20is%20applied%0Aacross%20multiple%20modules%2C%20which%20include%20circular%20padding%20for%20boundary%0Acontinuity%2C%20yaw%20loss%20for%20rotational%20robustness%2C%20and%20cube%20loss%20for%20distortion%0Aawareness.%20Extensive%20experiments%20on%20text-to-panorama%2C%20inpainting%2C%20and%0Aoutpainting%20tasks%20demonstrate%20that%20our%20method%20achieves%20better%20boundary%0Aconsistency%20and%20image%20fidelity%20across%20eleven%20quantitative%20metrics.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/Insta360-Research-Team/DiT360.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11712v1&entry.124074799=Read"},
{"title": "A Framework for Low-Effort Training Data Generation for Urban Semantic\n  Segmentation", "author": "Denis Zavadski and Damjan Kal\u0161an and Tim K\u00fcchler and Haebom Lee and Stefan Roth and Carsten Rother", "abstract": "  Synthetic datasets are widely used for training urban scene recognition\nmodels, but even highly realistic renderings show a noticeable gap to real\nimagery. This gap is particularly pronounced when adapting to a specific target\ndomain, such as Cityscapes, where differences in architecture, vegetation,\nobject appearance, and camera characteristics limit downstream performance.\nClosing this gap with more detailed 3D modelling would require expensive asset\nand scene design, defeating the purpose of low-cost labelled data. To address\nthis, we present a new framework that adapts an off-the-shelf diffusion model\nto a target domain using only imperfect pseudo-labels. Once trained, it\ngenerates high-fidelity, target-aligned images from semantic maps of any\nsynthetic dataset, including low-effort sources created in hours rather than\nmonths. The method filters suboptimal generations, rectifies image-label\nmisalignments, and standardises semantics across datasets, transforming weak\nsynthetic data into competitive real-domain training sets. Experiments on five\nsynthetic datasets and two real target datasets show segmentation gains of up\nto +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly\nconstructed synthetic datasets as effective as high-effort, time-intensive\nsynthetic datasets requiring extensive manual design. This work highlights a\nvaluable collaborative paradigm where fast semantic prototyping, combined with\ngenerative models, enables scalable, high-quality training data creation for\nurban scene understanding.\n", "link": "http://arxiv.org/abs/2510.11567v1", "date": "2025-10-13", "relevancy": 2.3444, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5912}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5874}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Framework%20for%20Low-Effort%20Training%20Data%20Generation%20for%20Urban%20Semantic%0A%20%20Segmentation&body=Title%3A%20A%20Framework%20for%20Low-Effort%20Training%20Data%20Generation%20for%20Urban%20Semantic%0A%20%20Segmentation%0AAuthor%3A%20Denis%20Zavadski%20and%20Damjan%20Kal%C5%A1an%20and%20Tim%20K%C3%BCchler%20and%20Haebom%20Lee%20and%20Stefan%20Roth%20and%20Carsten%20Rother%0AAbstract%3A%20%20%20Synthetic%20datasets%20are%20widely%20used%20for%20training%20urban%20scene%20recognition%0Amodels%2C%20but%20even%20highly%20realistic%20renderings%20show%20a%20noticeable%20gap%20to%20real%0Aimagery.%20This%20gap%20is%20particularly%20pronounced%20when%20adapting%20to%20a%20specific%20target%0Adomain%2C%20such%20as%20Cityscapes%2C%20where%20differences%20in%20architecture%2C%20vegetation%2C%0Aobject%20appearance%2C%20and%20camera%20characteristics%20limit%20downstream%20performance.%0AClosing%20this%20gap%20with%20more%20detailed%203D%20modelling%20would%20require%20expensive%20asset%0Aand%20scene%20design%2C%20defeating%20the%20purpose%20of%20low-cost%20labelled%20data.%20To%20address%0Athis%2C%20we%20present%20a%20new%20framework%20that%20adapts%20an%20off-the-shelf%20diffusion%20model%0Ato%20a%20target%20domain%20using%20only%20imperfect%20pseudo-labels.%20Once%20trained%2C%20it%0Agenerates%20high-fidelity%2C%20target-aligned%20images%20from%20semantic%20maps%20of%20any%0Asynthetic%20dataset%2C%20including%20low-effort%20sources%20created%20in%20hours%20rather%20than%0Amonths.%20The%20method%20filters%20suboptimal%20generations%2C%20rectifies%20image-label%0Amisalignments%2C%20and%20standardises%20semantics%20across%20datasets%2C%20transforming%20weak%0Asynthetic%20data%20into%20competitive%20real-domain%20training%20sets.%20Experiments%20on%20five%0Asynthetic%20datasets%20and%20two%20real%20target%20datasets%20show%20segmentation%20gains%20of%20up%0Ato%20%2B8.0%25pt.%20mIoU%20over%20state-of-the-art%20translation%20methods%2C%20making%20rapidly%0Aconstructed%20synthetic%20datasets%20as%20effective%20as%20high-effort%2C%20time-intensive%0Asynthetic%20datasets%20requiring%20extensive%20manual%20design.%20This%20work%20highlights%20a%0Avaluable%20collaborative%20paradigm%20where%20fast%20semantic%20prototyping%2C%20combined%20with%0Agenerative%20models%2C%20enables%20scalable%2C%20high-quality%20training%20data%20creation%20for%0Aurban%20scene%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Framework%2520for%2520Low-Effort%2520Training%2520Data%2520Generation%2520for%2520Urban%2520Semantic%250A%2520%2520Segmentation%26entry.906535625%3DDenis%2520Zavadski%2520and%2520Damjan%2520Kal%25C5%25A1an%2520and%2520Tim%2520K%25C3%25BCchler%2520and%2520Haebom%2520Lee%2520and%2520Stefan%2520Roth%2520and%2520Carsten%2520Rother%26entry.1292438233%3D%2520%2520Synthetic%2520datasets%2520are%2520widely%2520used%2520for%2520training%2520urban%2520scene%2520recognition%250Amodels%252C%2520but%2520even%2520highly%2520realistic%2520renderings%2520show%2520a%2520noticeable%2520gap%2520to%2520real%250Aimagery.%2520This%2520gap%2520is%2520particularly%2520pronounced%2520when%2520adapting%2520to%2520a%2520specific%2520target%250Adomain%252C%2520such%2520as%2520Cityscapes%252C%2520where%2520differences%2520in%2520architecture%252C%2520vegetation%252C%250Aobject%2520appearance%252C%2520and%2520camera%2520characteristics%2520limit%2520downstream%2520performance.%250AClosing%2520this%2520gap%2520with%2520more%2520detailed%25203D%2520modelling%2520would%2520require%2520expensive%2520asset%250Aand%2520scene%2520design%252C%2520defeating%2520the%2520purpose%2520of%2520low-cost%2520labelled%2520data.%2520To%2520address%250Athis%252C%2520we%2520present%2520a%2520new%2520framework%2520that%2520adapts%2520an%2520off-the-shelf%2520diffusion%2520model%250Ato%2520a%2520target%2520domain%2520using%2520only%2520imperfect%2520pseudo-labels.%2520Once%2520trained%252C%2520it%250Agenerates%2520high-fidelity%252C%2520target-aligned%2520images%2520from%2520semantic%2520maps%2520of%2520any%250Asynthetic%2520dataset%252C%2520including%2520low-effort%2520sources%2520created%2520in%2520hours%2520rather%2520than%250Amonths.%2520The%2520method%2520filters%2520suboptimal%2520generations%252C%2520rectifies%2520image-label%250Amisalignments%252C%2520and%2520standardises%2520semantics%2520across%2520datasets%252C%2520transforming%2520weak%250Asynthetic%2520data%2520into%2520competitive%2520real-domain%2520training%2520sets.%2520Experiments%2520on%2520five%250Asynthetic%2520datasets%2520and%2520two%2520real%2520target%2520datasets%2520show%2520segmentation%2520gains%2520of%2520up%250Ato%2520%252B8.0%2525pt.%2520mIoU%2520over%2520state-of-the-art%2520translation%2520methods%252C%2520making%2520rapidly%250Aconstructed%2520synthetic%2520datasets%2520as%2520effective%2520as%2520high-effort%252C%2520time-intensive%250Asynthetic%2520datasets%2520requiring%2520extensive%2520manual%2520design.%2520This%2520work%2520highlights%2520a%250Avaluable%2520collaborative%2520paradigm%2520where%2520fast%2520semantic%2520prototyping%252C%2520combined%2520with%250Agenerative%2520models%252C%2520enables%2520scalable%252C%2520high-quality%2520training%2520data%2520creation%2520for%250Aurban%2520scene%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Framework%20for%20Low-Effort%20Training%20Data%20Generation%20for%20Urban%20Semantic%0A%20%20Segmentation&entry.906535625=Denis%20Zavadski%20and%20Damjan%20Kal%C5%A1an%20and%20Tim%20K%C3%BCchler%20and%20Haebom%20Lee%20and%20Stefan%20Roth%20and%20Carsten%20Rother&entry.1292438233=%20%20Synthetic%20datasets%20are%20widely%20used%20for%20training%20urban%20scene%20recognition%0Amodels%2C%20but%20even%20highly%20realistic%20renderings%20show%20a%20noticeable%20gap%20to%20real%0Aimagery.%20This%20gap%20is%20particularly%20pronounced%20when%20adapting%20to%20a%20specific%20target%0Adomain%2C%20such%20as%20Cityscapes%2C%20where%20differences%20in%20architecture%2C%20vegetation%2C%0Aobject%20appearance%2C%20and%20camera%20characteristics%20limit%20downstream%20performance.%0AClosing%20this%20gap%20with%20more%20detailed%203D%20modelling%20would%20require%20expensive%20asset%0Aand%20scene%20design%2C%20defeating%20the%20purpose%20of%20low-cost%20labelled%20data.%20To%20address%0Athis%2C%20we%20present%20a%20new%20framework%20that%20adapts%20an%20off-the-shelf%20diffusion%20model%0Ato%20a%20target%20domain%20using%20only%20imperfect%20pseudo-labels.%20Once%20trained%2C%20it%0Agenerates%20high-fidelity%2C%20target-aligned%20images%20from%20semantic%20maps%20of%20any%0Asynthetic%20dataset%2C%20including%20low-effort%20sources%20created%20in%20hours%20rather%20than%0Amonths.%20The%20method%20filters%20suboptimal%20generations%2C%20rectifies%20image-label%0Amisalignments%2C%20and%20standardises%20semantics%20across%20datasets%2C%20transforming%20weak%0Asynthetic%20data%20into%20competitive%20real-domain%20training%20sets.%20Experiments%20on%20five%0Asynthetic%20datasets%20and%20two%20real%20target%20datasets%20show%20segmentation%20gains%20of%20up%0Ato%20%2B8.0%25pt.%20mIoU%20over%20state-of-the-art%20translation%20methods%2C%20making%20rapidly%0Aconstructed%20synthetic%20datasets%20as%20effective%20as%20high-effort%2C%20time-intensive%0Asynthetic%20datasets%20requiring%20extensive%20manual%20design.%20This%20work%20highlights%20a%0Avaluable%20collaborative%20paradigm%20where%20fast%20semantic%20prototyping%2C%20combined%20with%0Agenerative%20models%2C%20enables%20scalable%2C%20high-quality%20training%20data%20creation%20for%0Aurban%20scene%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11567v1&entry.124074799=Read"},
{"title": "NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object\n  Detection", "author": "Krittin Chaowakarn and Paramin Sangwongngam and Nang Htet Htet Aung and Chalie Charoenlarpnopparut", "abstract": "  Recent studies in 3D object detection for autonomous vehicles aim to enrich\nfeatures through the utilization of multi-modal setups or the extraction of\nlocal patterns within LiDAR point clouds. However, multi-modal methods face\nsignificant challenges in feature alignment, and gaining features locally can\nbe oversimplified for complex 3D object detection tasks. In this paper, we\npropose a novel model, NV3D, which utilizes local features acquired from voxel\nneighbors, as normal vectors computed per voxel basis using K-nearest neighbors\n(KNN) and principal component analysis (PCA). This informative feature enables\nNV3D to determine the relationship between the surface and pertinent target\nentities, including cars, pedestrians, or cyclists. During the normal vector\nextraction process, NV3D offers two distinct sampling strategies: normal vector\ndensity-based sampling and FOV-aware bin-based sampling, allowing elimination\nof up to 55% of data while maintaining performance. In addition, we applied\nelement-wise attention fusion, which accepts voxel features as the query and\nvalue and normal vector features as the key, similar to the attention\nmechanism. Our method is trained on the KITTI dataset and has demonstrated\nsuperior performance in car and cyclist detection owing to their spatial\nshapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%\nmean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%\nand 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in\ncar detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of\nvoxels being filtered out.\n", "link": "http://arxiv.org/abs/2510.11632v1", "date": "2025-10-13", "relevancy": 2.3422, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6039}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5761}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NV3D%3A%20Leveraging%20Spatial%20Shape%20Through%20Normal%20Vector-based%203D%20Object%0A%20%20Detection&body=Title%3A%20NV3D%3A%20Leveraging%20Spatial%20Shape%20Through%20Normal%20Vector-based%203D%20Object%0A%20%20Detection%0AAuthor%3A%20Krittin%20Chaowakarn%20and%20Paramin%20Sangwongngam%20and%20Nang%20Htet%20Htet%20Aung%20and%20Chalie%20Charoenlarpnopparut%0AAbstract%3A%20%20%20Recent%20studies%20in%203D%20object%20detection%20for%20autonomous%20vehicles%20aim%20to%20enrich%0Afeatures%20through%20the%20utilization%20of%20multi-modal%20setups%20or%20the%20extraction%20of%0Alocal%20patterns%20within%20LiDAR%20point%20clouds.%20However%2C%20multi-modal%20methods%20face%0Asignificant%20challenges%20in%20feature%20alignment%2C%20and%20gaining%20features%20locally%20can%0Abe%20oversimplified%20for%20complex%203D%20object%20detection%20tasks.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20model%2C%20NV3D%2C%20which%20utilizes%20local%20features%20acquired%20from%20voxel%0Aneighbors%2C%20as%20normal%20vectors%20computed%20per%20voxel%20basis%20using%20K-nearest%20neighbors%0A%28KNN%29%20and%20principal%20component%20analysis%20%28PCA%29.%20This%20informative%20feature%20enables%0ANV3D%20to%20determine%20the%20relationship%20between%20the%20surface%20and%20pertinent%20target%0Aentities%2C%20including%20cars%2C%20pedestrians%2C%20or%20cyclists.%20During%20the%20normal%20vector%0Aextraction%20process%2C%20NV3D%20offers%20two%20distinct%20sampling%20strategies%3A%20normal%20vector%0Adensity-based%20sampling%20and%20FOV-aware%20bin-based%20sampling%2C%20allowing%20elimination%0Aof%20up%20to%2055%25%20of%20data%20while%20maintaining%20performance.%20In%20addition%2C%20we%20applied%0Aelement-wise%20attention%20fusion%2C%20which%20accepts%20voxel%20features%20as%20the%20query%20and%0Avalue%20and%20normal%20vector%20features%20as%20the%20key%2C%20similar%20to%20the%20attention%0Amechanism.%20Our%20method%20is%20trained%20on%20the%20KITTI%20dataset%20and%20has%20demonstrated%0Asuperior%20performance%20in%20car%20and%20cyclist%20detection%20owing%20to%20their%20spatial%0Ashapes.%20In%20the%20validation%20set%2C%20NV3D%20without%20sampling%20achieves%2086.60%25%20and%2080.18%25%0Amean%20Average%20Precision%20%28mAP%29%2C%20greater%20than%20the%20baseline%20Voxel%20R-CNN%20by%202.61%25%0Aand%204.23%25%20mAP%2C%20respectively.%20With%20both%20samplings%2C%20NV3D%20achieves%2085.54%25%20mAP%20in%0Acar%20detection%2C%20exceeding%20the%20baseline%20by%201.56%25%20mAP%2C%20despite%20roughly%2055%25%20of%0Avoxels%20being%20filtered%20out.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNV3D%253A%2520Leveraging%2520Spatial%2520Shape%2520Through%2520Normal%2520Vector-based%25203D%2520Object%250A%2520%2520Detection%26entry.906535625%3DKrittin%2520Chaowakarn%2520and%2520Paramin%2520Sangwongngam%2520and%2520Nang%2520Htet%2520Htet%2520Aung%2520and%2520Chalie%2520Charoenlarpnopparut%26entry.1292438233%3D%2520%2520Recent%2520studies%2520in%25203D%2520object%2520detection%2520for%2520autonomous%2520vehicles%2520aim%2520to%2520enrich%250Afeatures%2520through%2520the%2520utilization%2520of%2520multi-modal%2520setups%2520or%2520the%2520extraction%2520of%250Alocal%2520patterns%2520within%2520LiDAR%2520point%2520clouds.%2520However%252C%2520multi-modal%2520methods%2520face%250Asignificant%2520challenges%2520in%2520feature%2520alignment%252C%2520and%2520gaining%2520features%2520locally%2520can%250Abe%2520oversimplified%2520for%2520complex%25203D%2520object%2520detection%2520tasks.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520model%252C%2520NV3D%252C%2520which%2520utilizes%2520local%2520features%2520acquired%2520from%2520voxel%250Aneighbors%252C%2520as%2520normal%2520vectors%2520computed%2520per%2520voxel%2520basis%2520using%2520K-nearest%2520neighbors%250A%2528KNN%2529%2520and%2520principal%2520component%2520analysis%2520%2528PCA%2529.%2520This%2520informative%2520feature%2520enables%250ANV3D%2520to%2520determine%2520the%2520relationship%2520between%2520the%2520surface%2520and%2520pertinent%2520target%250Aentities%252C%2520including%2520cars%252C%2520pedestrians%252C%2520or%2520cyclists.%2520During%2520the%2520normal%2520vector%250Aextraction%2520process%252C%2520NV3D%2520offers%2520two%2520distinct%2520sampling%2520strategies%253A%2520normal%2520vector%250Adensity-based%2520sampling%2520and%2520FOV-aware%2520bin-based%2520sampling%252C%2520allowing%2520elimination%250Aof%2520up%2520to%252055%2525%2520of%2520data%2520while%2520maintaining%2520performance.%2520In%2520addition%252C%2520we%2520applied%250Aelement-wise%2520attention%2520fusion%252C%2520which%2520accepts%2520voxel%2520features%2520as%2520the%2520query%2520and%250Avalue%2520and%2520normal%2520vector%2520features%2520as%2520the%2520key%252C%2520similar%2520to%2520the%2520attention%250Amechanism.%2520Our%2520method%2520is%2520trained%2520on%2520the%2520KITTI%2520dataset%2520and%2520has%2520demonstrated%250Asuperior%2520performance%2520in%2520car%2520and%2520cyclist%2520detection%2520owing%2520to%2520their%2520spatial%250Ashapes.%2520In%2520the%2520validation%2520set%252C%2520NV3D%2520without%2520sampling%2520achieves%252086.60%2525%2520and%252080.18%2525%250Amean%2520Average%2520Precision%2520%2528mAP%2529%252C%2520greater%2520than%2520the%2520baseline%2520Voxel%2520R-CNN%2520by%25202.61%2525%250Aand%25204.23%2525%2520mAP%252C%2520respectively.%2520With%2520both%2520samplings%252C%2520NV3D%2520achieves%252085.54%2525%2520mAP%2520in%250Acar%2520detection%252C%2520exceeding%2520the%2520baseline%2520by%25201.56%2525%2520mAP%252C%2520despite%2520roughly%252055%2525%2520of%250Avoxels%2520being%2520filtered%2520out.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NV3D%3A%20Leveraging%20Spatial%20Shape%20Through%20Normal%20Vector-based%203D%20Object%0A%20%20Detection&entry.906535625=Krittin%20Chaowakarn%20and%20Paramin%20Sangwongngam%20and%20Nang%20Htet%20Htet%20Aung%20and%20Chalie%20Charoenlarpnopparut&entry.1292438233=%20%20Recent%20studies%20in%203D%20object%20detection%20for%20autonomous%20vehicles%20aim%20to%20enrich%0Afeatures%20through%20the%20utilization%20of%20multi-modal%20setups%20or%20the%20extraction%20of%0Alocal%20patterns%20within%20LiDAR%20point%20clouds.%20However%2C%20multi-modal%20methods%20face%0Asignificant%20challenges%20in%20feature%20alignment%2C%20and%20gaining%20features%20locally%20can%0Abe%20oversimplified%20for%20complex%203D%20object%20detection%20tasks.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20model%2C%20NV3D%2C%20which%20utilizes%20local%20features%20acquired%20from%20voxel%0Aneighbors%2C%20as%20normal%20vectors%20computed%20per%20voxel%20basis%20using%20K-nearest%20neighbors%0A%28KNN%29%20and%20principal%20component%20analysis%20%28PCA%29.%20This%20informative%20feature%20enables%0ANV3D%20to%20determine%20the%20relationship%20between%20the%20surface%20and%20pertinent%20target%0Aentities%2C%20including%20cars%2C%20pedestrians%2C%20or%20cyclists.%20During%20the%20normal%20vector%0Aextraction%20process%2C%20NV3D%20offers%20two%20distinct%20sampling%20strategies%3A%20normal%20vector%0Adensity-based%20sampling%20and%20FOV-aware%20bin-based%20sampling%2C%20allowing%20elimination%0Aof%20up%20to%2055%25%20of%20data%20while%20maintaining%20performance.%20In%20addition%2C%20we%20applied%0Aelement-wise%20attention%20fusion%2C%20which%20accepts%20voxel%20features%20as%20the%20query%20and%0Avalue%20and%20normal%20vector%20features%20as%20the%20key%2C%20similar%20to%20the%20attention%0Amechanism.%20Our%20method%20is%20trained%20on%20the%20KITTI%20dataset%20and%20has%20demonstrated%0Asuperior%20performance%20in%20car%20and%20cyclist%20detection%20owing%20to%20their%20spatial%0Ashapes.%20In%20the%20validation%20set%2C%20NV3D%20without%20sampling%20achieves%2086.60%25%20and%2080.18%25%0Amean%20Average%20Precision%20%28mAP%29%2C%20greater%20than%20the%20baseline%20Voxel%20R-CNN%20by%202.61%25%0Aand%204.23%25%20mAP%2C%20respectively.%20With%20both%20samplings%2C%20NV3D%20achieves%2085.54%25%20mAP%20in%0Acar%20detection%2C%20exceeding%20the%20baseline%20by%201.56%25%20mAP%2C%20despite%20roughly%2055%25%20of%0Avoxels%20being%20filtered%20out.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11632v1&entry.124074799=Read"},
{"title": "High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid\n  Network", "author": "Feng Zhang and Haoyou Deng and Zhiqiang Li and Lida Li and Bin Xu and Qingbo Lu and Zisheng Cao and Minchen Wei and Changxin Gao and Nong Sang and Xiang Bai", "abstract": "  Photo enhancement plays a crucial role in augmenting the visual aesthetics of\na photograph. In recent years, photo enhancement methods have either focused on\nenhancement performance, producing powerful models that cannot be deployed on\nedge devices, or prioritized computational efficiency, resulting in inadequate\nperformance for real-world applications. To this end, this paper introduces a\npyramid network called LLF-LUT++, which integrates global and local operators\nthrough closed-form Laplacian pyramid decomposition and reconstruction. This\napproach enables fast processing of high-resolution images while also achieving\nexcellent performance. Specifically, we utilize an image-adaptive 3D LUT that\ncapitalizes on the global tonal characteristics of downsampled images, while\nincorporating two distinct weight fusion strategies to achieve coarse global\nimage enhancement. To implement this strategy, we designed a spatial-frequency\ntransformer weight predictor that effectively extracts the desired distinct\nweights by leveraging frequency features. Additionally, we apply local\nLaplacian filters to adaptively refine edge details in high-frequency\ncomponents. After meticulously redesigning the network structure and\ntransformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on\nthe HDR+ dataset, but also further reduces runtime, with 4K resolution images\nprocessed in just 13 ms on a single GPU. Extensive experimental results on two\nbenchmark datasets further show that the proposed approach performs favorably\ncompared to state-of-the-art methods. The source code will be made publicly\navailable at https://github.com/fengzhang427/LLF-LUT.\n", "link": "http://arxiv.org/abs/2510.11613v1", "date": "2025-10-13", "relevancy": 2.2994, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5782}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5726}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-resolution%20Photo%20Enhancement%20in%20Real-time%3A%20A%20Laplacian%20Pyramid%0A%20%20Network&body=Title%3A%20High-resolution%20Photo%20Enhancement%20in%20Real-time%3A%20A%20Laplacian%20Pyramid%0A%20%20Network%0AAuthor%3A%20Feng%20Zhang%20and%20Haoyou%20Deng%20and%20Zhiqiang%20Li%20and%20Lida%20Li%20and%20Bin%20Xu%20and%20Qingbo%20Lu%20and%20Zisheng%20Cao%20and%20Minchen%20Wei%20and%20Changxin%20Gao%20and%20Nong%20Sang%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Photo%20enhancement%20plays%20a%20crucial%20role%20in%20augmenting%20the%20visual%20aesthetics%20of%0Aa%20photograph.%20In%20recent%20years%2C%20photo%20enhancement%20methods%20have%20either%20focused%20on%0Aenhancement%20performance%2C%20producing%20powerful%20models%20that%20cannot%20be%20deployed%20on%0Aedge%20devices%2C%20or%20prioritized%20computational%20efficiency%2C%20resulting%20in%20inadequate%0Aperformance%20for%20real-world%20applications.%20To%20this%20end%2C%20this%20paper%20introduces%20a%0Apyramid%20network%20called%20LLF-LUT%2B%2B%2C%20which%20integrates%20global%20and%20local%20operators%0Athrough%20closed-form%20Laplacian%20pyramid%20decomposition%20and%20reconstruction.%20This%0Aapproach%20enables%20fast%20processing%20of%20high-resolution%20images%20while%20also%20achieving%0Aexcellent%20performance.%20Specifically%2C%20we%20utilize%20an%20image-adaptive%203D%20LUT%20that%0Acapitalizes%20on%20the%20global%20tonal%20characteristics%20of%20downsampled%20images%2C%20while%0Aincorporating%20two%20distinct%20weight%20fusion%20strategies%20to%20achieve%20coarse%20global%0Aimage%20enhancement.%20To%20implement%20this%20strategy%2C%20we%20designed%20a%20spatial-frequency%0Atransformer%20weight%20predictor%20that%20effectively%20extracts%20the%20desired%20distinct%0Aweights%20by%20leveraging%20frequency%20features.%20Additionally%2C%20we%20apply%20local%0ALaplacian%20filters%20to%20adaptively%20refine%20edge%20details%20in%20high-frequency%0Acomponents.%20After%20meticulously%20redesigning%20the%20network%20structure%20and%0Atransformer%20model%2C%20LLF-LUT%2B%2B%20not%20only%20achieves%20a%202.64%20dB%20improvement%20in%20PSNR%20on%0Athe%20HDR%2B%20dataset%2C%20but%20also%20further%20reduces%20runtime%2C%20with%204K%20resolution%20images%0Aprocessed%20in%20just%2013%20ms%20on%20a%20single%20GPU.%20Extensive%20experimental%20results%20on%20two%0Abenchmark%20datasets%20further%20show%20that%20the%20proposed%20approach%20performs%20favorably%0Acompared%20to%20state-of-the-art%20methods.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/fengzhang427/LLF-LUT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-resolution%2520Photo%2520Enhancement%2520in%2520Real-time%253A%2520A%2520Laplacian%2520Pyramid%250A%2520%2520Network%26entry.906535625%3DFeng%2520Zhang%2520and%2520Haoyou%2520Deng%2520and%2520Zhiqiang%2520Li%2520and%2520Lida%2520Li%2520and%2520Bin%2520Xu%2520and%2520Qingbo%2520Lu%2520and%2520Zisheng%2520Cao%2520and%2520Minchen%2520Wei%2520and%2520Changxin%2520Gao%2520and%2520Nong%2520Sang%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Photo%2520enhancement%2520plays%2520a%2520crucial%2520role%2520in%2520augmenting%2520the%2520visual%2520aesthetics%2520of%250Aa%2520photograph.%2520In%2520recent%2520years%252C%2520photo%2520enhancement%2520methods%2520have%2520either%2520focused%2520on%250Aenhancement%2520performance%252C%2520producing%2520powerful%2520models%2520that%2520cannot%2520be%2520deployed%2520on%250Aedge%2520devices%252C%2520or%2520prioritized%2520computational%2520efficiency%252C%2520resulting%2520in%2520inadequate%250Aperformance%2520for%2520real-world%2520applications.%2520To%2520this%2520end%252C%2520this%2520paper%2520introduces%2520a%250Apyramid%2520network%2520called%2520LLF-LUT%252B%252B%252C%2520which%2520integrates%2520global%2520and%2520local%2520operators%250Athrough%2520closed-form%2520Laplacian%2520pyramid%2520decomposition%2520and%2520reconstruction.%2520This%250Aapproach%2520enables%2520fast%2520processing%2520of%2520high-resolution%2520images%2520while%2520also%2520achieving%250Aexcellent%2520performance.%2520Specifically%252C%2520we%2520utilize%2520an%2520image-adaptive%25203D%2520LUT%2520that%250Acapitalizes%2520on%2520the%2520global%2520tonal%2520characteristics%2520of%2520downsampled%2520images%252C%2520while%250Aincorporating%2520two%2520distinct%2520weight%2520fusion%2520strategies%2520to%2520achieve%2520coarse%2520global%250Aimage%2520enhancement.%2520To%2520implement%2520this%2520strategy%252C%2520we%2520designed%2520a%2520spatial-frequency%250Atransformer%2520weight%2520predictor%2520that%2520effectively%2520extracts%2520the%2520desired%2520distinct%250Aweights%2520by%2520leveraging%2520frequency%2520features.%2520Additionally%252C%2520we%2520apply%2520local%250ALaplacian%2520filters%2520to%2520adaptively%2520refine%2520edge%2520details%2520in%2520high-frequency%250Acomponents.%2520After%2520meticulously%2520redesigning%2520the%2520network%2520structure%2520and%250Atransformer%2520model%252C%2520LLF-LUT%252B%252B%2520not%2520only%2520achieves%2520a%25202.64%2520dB%2520improvement%2520in%2520PSNR%2520on%250Athe%2520HDR%252B%2520dataset%252C%2520but%2520also%2520further%2520reduces%2520runtime%252C%2520with%25204K%2520resolution%2520images%250Aprocessed%2520in%2520just%252013%2520ms%2520on%2520a%2520single%2520GPU.%2520Extensive%2520experimental%2520results%2520on%2520two%250Abenchmark%2520datasets%2520further%2520show%2520that%2520the%2520proposed%2520approach%2520performs%2520favorably%250Acompared%2520to%2520state-of-the-art%2520methods.%2520The%2520source%2520code%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%2520https%253A//github.com/fengzhang427/LLF-LUT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-resolution%20Photo%20Enhancement%20in%20Real-time%3A%20A%20Laplacian%20Pyramid%0A%20%20Network&entry.906535625=Feng%20Zhang%20and%20Haoyou%20Deng%20and%20Zhiqiang%20Li%20and%20Lida%20Li%20and%20Bin%20Xu%20and%20Qingbo%20Lu%20and%20Zisheng%20Cao%20and%20Minchen%20Wei%20and%20Changxin%20Gao%20and%20Nong%20Sang%20and%20Xiang%20Bai&entry.1292438233=%20%20Photo%20enhancement%20plays%20a%20crucial%20role%20in%20augmenting%20the%20visual%20aesthetics%20of%0Aa%20photograph.%20In%20recent%20years%2C%20photo%20enhancement%20methods%20have%20either%20focused%20on%0Aenhancement%20performance%2C%20producing%20powerful%20models%20that%20cannot%20be%20deployed%20on%0Aedge%20devices%2C%20or%20prioritized%20computational%20efficiency%2C%20resulting%20in%20inadequate%0Aperformance%20for%20real-world%20applications.%20To%20this%20end%2C%20this%20paper%20introduces%20a%0Apyramid%20network%20called%20LLF-LUT%2B%2B%2C%20which%20integrates%20global%20and%20local%20operators%0Athrough%20closed-form%20Laplacian%20pyramid%20decomposition%20and%20reconstruction.%20This%0Aapproach%20enables%20fast%20processing%20of%20high-resolution%20images%20while%20also%20achieving%0Aexcellent%20performance.%20Specifically%2C%20we%20utilize%20an%20image-adaptive%203D%20LUT%20that%0Acapitalizes%20on%20the%20global%20tonal%20characteristics%20of%20downsampled%20images%2C%20while%0Aincorporating%20two%20distinct%20weight%20fusion%20strategies%20to%20achieve%20coarse%20global%0Aimage%20enhancement.%20To%20implement%20this%20strategy%2C%20we%20designed%20a%20spatial-frequency%0Atransformer%20weight%20predictor%20that%20effectively%20extracts%20the%20desired%20distinct%0Aweights%20by%20leveraging%20frequency%20features.%20Additionally%2C%20we%20apply%20local%0ALaplacian%20filters%20to%20adaptively%20refine%20edge%20details%20in%20high-frequency%0Acomponents.%20After%20meticulously%20redesigning%20the%20network%20structure%20and%0Atransformer%20model%2C%20LLF-LUT%2B%2B%20not%20only%20achieves%20a%202.64%20dB%20improvement%20in%20PSNR%20on%0Athe%20HDR%2B%20dataset%2C%20but%20also%20further%20reduces%20runtime%2C%20with%204K%20resolution%20images%0Aprocessed%20in%20just%2013%20ms%20on%20a%20single%20GPU.%20Extensive%20experimental%20results%20on%20two%0Abenchmark%20datasets%20further%20show%20that%20the%20proposed%20approach%20performs%20favorably%0Acompared%20to%20state-of-the-art%20methods.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/fengzhang427/LLF-LUT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11613v1&entry.124074799=Read"},
{"title": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence", "author": "Zhongang Cai and Yubo Wang and Qingping Sun and Ruisi Wang and Chenyang Gu and Wanqi Yin and Zhiqian Lin and Zhitao Yang and Chen Wei and Xuanke Shi and Kewang Deng and Xiaoyang Han and Zukai Chen and Jiaqi Li and Xiangyu Fan and Hanming Deng and Lewei Lu and Bo Li and Ziwei Liu and Quan Wang and Dahua Lin and Lei Yang", "abstract": "  Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We first propose a holistic taxonomy of spatial\ntasks that unifies existing benchmarks and a standardized protocol for the fair\nevaluation of state-of-the-art proprietary and open-source models across eight\nkey benchmarks, at a cost exceeding ten billion total tokens. Our empirical\nstudy then reveals that (1) GPT-5 demonstrates unprecedented strength in\nspatial intelligence (SI), yet (2) still falls short of human performance\nsignificantly across a broad spectrum of SI-tasks. Moreover, we (3) show that\nSI-tasks expose greater model capability deficiency than non-SI tasks, to the\nextent that (4) proprietary models do not exhibit a decisive advantage when\nfacing the most difficult ones. In addition, we conduct a qualitative\nevaluation across a diverse set of scenarios that are intuitive for humans, yet\nfail even the most advanced multimodal models.\n", "link": "http://arxiv.org/abs/2508.13142v2", "date": "2025-10-13", "relevancy": 2.2858, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5716}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Holistic%20Evaluation%20of%20Multimodal%20LLMs%20on%20Spatial%20Intelligence&body=Title%3A%20Holistic%20Evaluation%20of%20Multimodal%20LLMs%20on%20Spatial%20Intelligence%0AAuthor%3A%20Zhongang%20Cai%20and%20Yubo%20Wang%20and%20Qingping%20Sun%20and%20Ruisi%20Wang%20and%20Chenyang%20Gu%20and%20Wanqi%20Yin%20and%20Zhiqian%20Lin%20and%20Zhitao%20Yang%20and%20Chen%20Wei%20and%20Xuanke%20Shi%20and%20Kewang%20Deng%20and%20Xiaoyang%20Han%20and%20Zukai%20Chen%20and%20Jiaqi%20Li%20and%20Xiangyu%20Fan%20and%20Hanming%20Deng%20and%20Lewei%20Lu%20and%20Bo%20Li%20and%20Ziwei%20Liu%20and%20Quan%20Wang%20and%20Dahua%20Lin%20and%20Lei%20Yang%0AAbstract%3A%20%20%20Multimodal%20models%20have%20achieved%20remarkable%20progress%20in%20recent%20years.%0ANevertheless%2C%20they%20continue%20to%20exhibit%20notable%20limitations%20in%20spatial%0Aunderstanding%20and%20reasoning%2C%20the%20very%20capability%20that%20anchors%20artificial%0Ageneral%20intelligence%20in%20the%20physical%20world.%20With%20the%20recent%20release%20of%20GPT-5%2C%0Aallegedly%20the%20most%20powerful%20AI%20model%20to%20date%2C%20it%20is%20timely%20to%20examine%20where%20the%0Aleading%20models%20%28GPT%2C%20Gemini%2C%20Grok%2C%20Seed%2C%20Qwen%2C%20and%20Intern%29%20stand%20on%20the%20path%0Atoward%20spatial%20intelligence.%20We%20first%20propose%20a%20holistic%20taxonomy%20of%20spatial%0Atasks%20that%20unifies%20existing%20benchmarks%20and%20a%20standardized%20protocol%20for%20the%20fair%0Aevaluation%20of%20state-of-the-art%20proprietary%20and%20open-source%20models%20across%20eight%0Akey%20benchmarks%2C%20at%20a%20cost%20exceeding%20ten%20billion%20total%20tokens.%20Our%20empirical%0Astudy%20then%20reveals%20that%20%281%29%20GPT-5%20demonstrates%20unprecedented%20strength%20in%0Aspatial%20intelligence%20%28SI%29%2C%20yet%20%282%29%20still%20falls%20short%20of%20human%20performance%0Asignificantly%20across%20a%20broad%20spectrum%20of%20SI-tasks.%20Moreover%2C%20we%20%283%29%20show%20that%0ASI-tasks%20expose%20greater%20model%20capability%20deficiency%20than%20non-SI%20tasks%2C%20to%20the%0Aextent%20that%20%284%29%20proprietary%20models%20do%20not%20exhibit%20a%20decisive%20advantage%20when%0Afacing%20the%20most%20difficult%20ones.%20In%20addition%2C%20we%20conduct%20a%20qualitative%0Aevaluation%20across%20a%20diverse%20set%20of%20scenarios%20that%20are%20intuitive%20for%20humans%2C%20yet%0Afail%20even%20the%20most%20advanced%20multimodal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.13142v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHolistic%2520Evaluation%2520of%2520Multimodal%2520LLMs%2520on%2520Spatial%2520Intelligence%26entry.906535625%3DZhongang%2520Cai%2520and%2520Yubo%2520Wang%2520and%2520Qingping%2520Sun%2520and%2520Ruisi%2520Wang%2520and%2520Chenyang%2520Gu%2520and%2520Wanqi%2520Yin%2520and%2520Zhiqian%2520Lin%2520and%2520Zhitao%2520Yang%2520and%2520Chen%2520Wei%2520and%2520Xuanke%2520Shi%2520and%2520Kewang%2520Deng%2520and%2520Xiaoyang%2520Han%2520and%2520Zukai%2520Chen%2520and%2520Jiaqi%2520Li%2520and%2520Xiangyu%2520Fan%2520and%2520Hanming%2520Deng%2520and%2520Lewei%2520Lu%2520and%2520Bo%2520Li%2520and%2520Ziwei%2520Liu%2520and%2520Quan%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Lei%2520Yang%26entry.1292438233%3D%2520%2520Multimodal%2520models%2520have%2520achieved%2520remarkable%2520progress%2520in%2520recent%2520years.%250ANevertheless%252C%2520they%2520continue%2520to%2520exhibit%2520notable%2520limitations%2520in%2520spatial%250Aunderstanding%2520and%2520reasoning%252C%2520the%2520very%2520capability%2520that%2520anchors%2520artificial%250Ageneral%2520intelligence%2520in%2520the%2520physical%2520world.%2520With%2520the%2520recent%2520release%2520of%2520GPT-5%252C%250Aallegedly%2520the%2520most%2520powerful%2520AI%2520model%2520to%2520date%252C%2520it%2520is%2520timely%2520to%2520examine%2520where%2520the%250Aleading%2520models%2520%2528GPT%252C%2520Gemini%252C%2520Grok%252C%2520Seed%252C%2520Qwen%252C%2520and%2520Intern%2529%2520stand%2520on%2520the%2520path%250Atoward%2520spatial%2520intelligence.%2520We%2520first%2520propose%2520a%2520holistic%2520taxonomy%2520of%2520spatial%250Atasks%2520that%2520unifies%2520existing%2520benchmarks%2520and%2520a%2520standardized%2520protocol%2520for%2520the%2520fair%250Aevaluation%2520of%2520state-of-the-art%2520proprietary%2520and%2520open-source%2520models%2520across%2520eight%250Akey%2520benchmarks%252C%2520at%2520a%2520cost%2520exceeding%2520ten%2520billion%2520total%2520tokens.%2520Our%2520empirical%250Astudy%2520then%2520reveals%2520that%2520%25281%2529%2520GPT-5%2520demonstrates%2520unprecedented%2520strength%2520in%250Aspatial%2520intelligence%2520%2528SI%2529%252C%2520yet%2520%25282%2529%2520still%2520falls%2520short%2520of%2520human%2520performance%250Asignificantly%2520across%2520a%2520broad%2520spectrum%2520of%2520SI-tasks.%2520Moreover%252C%2520we%2520%25283%2529%2520show%2520that%250ASI-tasks%2520expose%2520greater%2520model%2520capability%2520deficiency%2520than%2520non-SI%2520tasks%252C%2520to%2520the%250Aextent%2520that%2520%25284%2529%2520proprietary%2520models%2520do%2520not%2520exhibit%2520a%2520decisive%2520advantage%2520when%250Afacing%2520the%2520most%2520difficult%2520ones.%2520In%2520addition%252C%2520we%2520conduct%2520a%2520qualitative%250Aevaluation%2520across%2520a%2520diverse%2520set%2520of%2520scenarios%2520that%2520are%2520intuitive%2520for%2520humans%252C%2520yet%250Afail%2520even%2520the%2520most%2520advanced%2520multimodal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.13142v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Holistic%20Evaluation%20of%20Multimodal%20LLMs%20on%20Spatial%20Intelligence&entry.906535625=Zhongang%20Cai%20and%20Yubo%20Wang%20and%20Qingping%20Sun%20and%20Ruisi%20Wang%20and%20Chenyang%20Gu%20and%20Wanqi%20Yin%20and%20Zhiqian%20Lin%20and%20Zhitao%20Yang%20and%20Chen%20Wei%20and%20Xuanke%20Shi%20and%20Kewang%20Deng%20and%20Xiaoyang%20Han%20and%20Zukai%20Chen%20and%20Jiaqi%20Li%20and%20Xiangyu%20Fan%20and%20Hanming%20Deng%20and%20Lewei%20Lu%20and%20Bo%20Li%20and%20Ziwei%20Liu%20and%20Quan%20Wang%20and%20Dahua%20Lin%20and%20Lei%20Yang&entry.1292438233=%20%20Multimodal%20models%20have%20achieved%20remarkable%20progress%20in%20recent%20years.%0ANevertheless%2C%20they%20continue%20to%20exhibit%20notable%20limitations%20in%20spatial%0Aunderstanding%20and%20reasoning%2C%20the%20very%20capability%20that%20anchors%20artificial%0Ageneral%20intelligence%20in%20the%20physical%20world.%20With%20the%20recent%20release%20of%20GPT-5%2C%0Aallegedly%20the%20most%20powerful%20AI%20model%20to%20date%2C%20it%20is%20timely%20to%20examine%20where%20the%0Aleading%20models%20%28GPT%2C%20Gemini%2C%20Grok%2C%20Seed%2C%20Qwen%2C%20and%20Intern%29%20stand%20on%20the%20path%0Atoward%20spatial%20intelligence.%20We%20first%20propose%20a%20holistic%20taxonomy%20of%20spatial%0Atasks%20that%20unifies%20existing%20benchmarks%20and%20a%20standardized%20protocol%20for%20the%20fair%0Aevaluation%20of%20state-of-the-art%20proprietary%20and%20open-source%20models%20across%20eight%0Akey%20benchmarks%2C%20at%20a%20cost%20exceeding%20ten%20billion%20total%20tokens.%20Our%20empirical%0Astudy%20then%20reveals%20that%20%281%29%20GPT-5%20demonstrates%20unprecedented%20strength%20in%0Aspatial%20intelligence%20%28SI%29%2C%20yet%20%282%29%20still%20falls%20short%20of%20human%20performance%0Asignificantly%20across%20a%20broad%20spectrum%20of%20SI-tasks.%20Moreover%2C%20we%20%283%29%20show%20that%0ASI-tasks%20expose%20greater%20model%20capability%20deficiency%20than%20non-SI%20tasks%2C%20to%20the%0Aextent%20that%20%284%29%20proprietary%20models%20do%20not%20exhibit%20a%20decisive%20advantage%20when%0Afacing%20the%20most%20difficult%20ones.%20In%20addition%2C%20we%20conduct%20a%20qualitative%0Aevaluation%20across%20a%20diverse%20set%20of%20scenarios%20that%20are%20intuitive%20for%20humans%2C%20yet%0Afail%20even%20the%20most%20advanced%20multimodal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.13142v2&entry.124074799=Read"},
{"title": "Representation-Based Exploration for Language Models: From Test-Time to\n  Post-Training", "author": "Jens Tuyls and Dylan J. Foster and Akshay Krishnamurthy and Jordan T. Ash", "abstract": "  Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening.\n", "link": "http://arxiv.org/abs/2510.11686v1", "date": "2025-10-13", "relevancy": 2.2065, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5584}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation-Based%20Exploration%20for%20Language%20Models%3A%20From%20Test-Time%20to%0A%20%20Post-Training&body=Title%3A%20Representation-Based%20Exploration%20for%20Language%20Models%3A%20From%20Test-Time%20to%0A%20%20Post-Training%0AAuthor%3A%20Jens%20Tuyls%20and%20Dylan%20J.%20Foster%20and%20Akshay%20Krishnamurthy%20and%20Jordan%20T.%20Ash%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20promises%20to%20expand%20the%20capabilities%20of%20language%0Amodels%2C%20but%20it%20is%20unclear%20if%20current%20RL%20techniques%20promote%20the%20discovery%20of%0Anovel%20behaviors%2C%20or%20simply%20sharpen%20those%20already%20present%20in%20the%20base%20model.%20In%0Athis%20paper%2C%20we%20investigate%20the%20value%20of%20deliberate%20exploration%20--%20explicitly%0Aincentivizing%20the%20model%20to%20discover%20novel%20and%20diverse%20behaviors%20--%20and%20aim%20to%0Aunderstand%20how%20the%20knowledge%20in%20pre-trained%20models%20can%20guide%20this%20search.%20Our%0Amain%20finding%20is%20that%20exploration%20with%20a%20simple%2C%20principled%2C%0Arepresentation-based%20bonus%20derived%20from%20the%20pre-trained%20language%20model%27s%20hidden%0Astates%20significantly%20improves%20diversity%20and%20pass%40k%20rates%20--%20both%20for%0Apost-training%2C%20and%20in%20a%20novel%20inference-time%20scaling%20setting%20we%20introduce.%20For%0Ainference-time%2C%20exploration%20with%20representation-based%20diversity%20improves%0Aefficiency%2C%20consistently%20improving%20pass%40k%20rates%20across%20a%20variety%20of%20models%20and%0Areasoning%20tasks.%20For%20example%2C%20for%20Qwen-2.5-14b-Instruct%20we%20obtain%20over%2050%25%0Aimprovement%20in%20verifier%20efficiency%20on%20almost%20all%20tasks.%20For%20post-training%2C%20we%0Ashow%20that%20integrating%20this%20exploration%20strategy%20into%20an%20RL%20pipeline%20improves%0Areasoning%20performance%20over%20that%20of%20the%20initial%20model%20and%20over%20standard%20RL%0Apost-training.%20For%20example%2C%20on%20AIME%202024%2C%20our%20post-trained%0AQwen-2.5-7b-Instruct%27s%20pass%4080%20matches%20the%20pass%40256%20of%20GRPO%20on%20the%20same%20model%2C%0Ademonstrating%20a%203x%20improvement%20in%20test-time%20sample%20efficiency.%20Overall%2C%20our%0Afindings%20suggest%20that%20deliberate%20exploration%20--%20with%20the%20right%20notion%20of%0Adiversity%20--%20is%20a%20practical%20path%20toward%20discovery%20of%20new%20behaviors%20beyond%0Asharpening.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation-Based%2520Exploration%2520for%2520Language%2520Models%253A%2520From%2520Test-Time%2520to%250A%2520%2520Post-Training%26entry.906535625%3DJens%2520Tuyls%2520and%2520Dylan%2520J.%2520Foster%2520and%2520Akshay%2520Krishnamurthy%2520and%2520Jordan%2520T.%2520Ash%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520promises%2520to%2520expand%2520the%2520capabilities%2520of%2520language%250Amodels%252C%2520but%2520it%2520is%2520unclear%2520if%2520current%2520RL%2520techniques%2520promote%2520the%2520discovery%2520of%250Anovel%2520behaviors%252C%2520or%2520simply%2520sharpen%2520those%2520already%2520present%2520in%2520the%2520base%2520model.%2520In%250Athis%2520paper%252C%2520we%2520investigate%2520the%2520value%2520of%2520deliberate%2520exploration%2520--%2520explicitly%250Aincentivizing%2520the%2520model%2520to%2520discover%2520novel%2520and%2520diverse%2520behaviors%2520--%2520and%2520aim%2520to%250Aunderstand%2520how%2520the%2520knowledge%2520in%2520pre-trained%2520models%2520can%2520guide%2520this%2520search.%2520Our%250Amain%2520finding%2520is%2520that%2520exploration%2520with%2520a%2520simple%252C%2520principled%252C%250Arepresentation-based%2520bonus%2520derived%2520from%2520the%2520pre-trained%2520language%2520model%2527s%2520hidden%250Astates%2520significantly%2520improves%2520diversity%2520and%2520pass%2540k%2520rates%2520--%2520both%2520for%250Apost-training%252C%2520and%2520in%2520a%2520novel%2520inference-time%2520scaling%2520setting%2520we%2520introduce.%2520For%250Ainference-time%252C%2520exploration%2520with%2520representation-based%2520diversity%2520improves%250Aefficiency%252C%2520consistently%2520improving%2520pass%2540k%2520rates%2520across%2520a%2520variety%2520of%2520models%2520and%250Areasoning%2520tasks.%2520For%2520example%252C%2520for%2520Qwen-2.5-14b-Instruct%2520we%2520obtain%2520over%252050%2525%250Aimprovement%2520in%2520verifier%2520efficiency%2520on%2520almost%2520all%2520tasks.%2520For%2520post-training%252C%2520we%250Ashow%2520that%2520integrating%2520this%2520exploration%2520strategy%2520into%2520an%2520RL%2520pipeline%2520improves%250Areasoning%2520performance%2520over%2520that%2520of%2520the%2520initial%2520model%2520and%2520over%2520standard%2520RL%250Apost-training.%2520For%2520example%252C%2520on%2520AIME%25202024%252C%2520our%2520post-trained%250AQwen-2.5-7b-Instruct%2527s%2520pass%254080%2520matches%2520the%2520pass%2540256%2520of%2520GRPO%2520on%2520the%2520same%2520model%252C%250Ademonstrating%2520a%25203x%2520improvement%2520in%2520test-time%2520sample%2520efficiency.%2520Overall%252C%2520our%250Afindings%2520suggest%2520that%2520deliberate%2520exploration%2520--%2520with%2520the%2520right%2520notion%2520of%250Adiversity%2520--%2520is%2520a%2520practical%2520path%2520toward%2520discovery%2520of%2520new%2520behaviors%2520beyond%250Asharpening.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation-Based%20Exploration%20for%20Language%20Models%3A%20From%20Test-Time%20to%0A%20%20Post-Training&entry.906535625=Jens%20Tuyls%20and%20Dylan%20J.%20Foster%20and%20Akshay%20Krishnamurthy%20and%20Jordan%20T.%20Ash&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20promises%20to%20expand%20the%20capabilities%20of%20language%0Amodels%2C%20but%20it%20is%20unclear%20if%20current%20RL%20techniques%20promote%20the%20discovery%20of%0Anovel%20behaviors%2C%20or%20simply%20sharpen%20those%20already%20present%20in%20the%20base%20model.%20In%0Athis%20paper%2C%20we%20investigate%20the%20value%20of%20deliberate%20exploration%20--%20explicitly%0Aincentivizing%20the%20model%20to%20discover%20novel%20and%20diverse%20behaviors%20--%20and%20aim%20to%0Aunderstand%20how%20the%20knowledge%20in%20pre-trained%20models%20can%20guide%20this%20search.%20Our%0Amain%20finding%20is%20that%20exploration%20with%20a%20simple%2C%20principled%2C%0Arepresentation-based%20bonus%20derived%20from%20the%20pre-trained%20language%20model%27s%20hidden%0Astates%20significantly%20improves%20diversity%20and%20pass%40k%20rates%20--%20both%20for%0Apost-training%2C%20and%20in%20a%20novel%20inference-time%20scaling%20setting%20we%20introduce.%20For%0Ainference-time%2C%20exploration%20with%20representation-based%20diversity%20improves%0Aefficiency%2C%20consistently%20improving%20pass%40k%20rates%20across%20a%20variety%20of%20models%20and%0Areasoning%20tasks.%20For%20example%2C%20for%20Qwen-2.5-14b-Instruct%20we%20obtain%20over%2050%25%0Aimprovement%20in%20verifier%20efficiency%20on%20almost%20all%20tasks.%20For%20post-training%2C%20we%0Ashow%20that%20integrating%20this%20exploration%20strategy%20into%20an%20RL%20pipeline%20improves%0Areasoning%20performance%20over%20that%20of%20the%20initial%20model%20and%20over%20standard%20RL%0Apost-training.%20For%20example%2C%20on%20AIME%202024%2C%20our%20post-trained%0AQwen-2.5-7b-Instruct%27s%20pass%4080%20matches%20the%20pass%40256%20of%20GRPO%20on%20the%20same%20model%2C%0Ademonstrating%20a%203x%20improvement%20in%20test-time%20sample%20efficiency.%20Overall%2C%20our%0Afindings%20suggest%20that%20deliberate%20exploration%20--%20with%20the%20right%20notion%20of%0Adiversity%20--%20is%20a%20practical%20path%20toward%20discovery%20of%20new%20behaviors%20beyond%0Asharpening.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11686v1&entry.124074799=Read"},
{"title": "Chronologically Consistent Generative AI", "author": "Songrun He and Linying Lv and Asaf Manela and Jimmy Wu", "abstract": "  We introduce a family of chronologically consistent, instruction-following\nlarge language models to eliminate lookahead bias. Each model is trained only\non data available before a clearly defined knowledge-cutoff date, ensuring\nstrict temporal separation from any post-cutoff data. The resulting framework\noffers (i) a simple, conversational chat interface, (ii) fully open, fixed\nmodel weights that guarantee replicability, and (iii) a conservative lower\nbound on forecast accuracy, isolating the share of predictability that survives\nonce training leakage is removed. Together, these features provide researchers\nwith an easy-to-use generative AI tool useful for a wide range of prediction\ntasks that is free of lookahead bias.\n", "link": "http://arxiv.org/abs/2510.11677v1", "date": "2025-10-13", "relevancy": 2.197, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5666}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5421}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chronologically%20Consistent%20Generative%20AI&body=Title%3A%20Chronologically%20Consistent%20Generative%20AI%0AAuthor%3A%20Songrun%20He%20and%20Linying%20Lv%20and%20Asaf%20Manela%20and%20Jimmy%20Wu%0AAbstract%3A%20%20%20We%20introduce%20a%20family%20of%20chronologically%20consistent%2C%20instruction-following%0Alarge%20language%20models%20to%20eliminate%20lookahead%20bias.%20Each%20model%20is%20trained%20only%0Aon%20data%20available%20before%20a%20clearly%20defined%20knowledge-cutoff%20date%2C%20ensuring%0Astrict%20temporal%20separation%20from%20any%20post-cutoff%20data.%20The%20resulting%20framework%0Aoffers%20%28i%29%20a%20simple%2C%20conversational%20chat%20interface%2C%20%28ii%29%20fully%20open%2C%20fixed%0Amodel%20weights%20that%20guarantee%20replicability%2C%20and%20%28iii%29%20a%20conservative%20lower%0Abound%20on%20forecast%20accuracy%2C%20isolating%20the%20share%20of%20predictability%20that%20survives%0Aonce%20training%20leakage%20is%20removed.%20Together%2C%20these%20features%20provide%20researchers%0Awith%20an%20easy-to-use%20generative%20AI%20tool%20useful%20for%20a%20wide%20range%20of%20prediction%0Atasks%20that%20is%20free%20of%20lookahead%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChronologically%2520Consistent%2520Generative%2520AI%26entry.906535625%3DSongrun%2520He%2520and%2520Linying%2520Lv%2520and%2520Asaf%2520Manela%2520and%2520Jimmy%2520Wu%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520family%2520of%2520chronologically%2520consistent%252C%2520instruction-following%250Alarge%2520language%2520models%2520to%2520eliminate%2520lookahead%2520bias.%2520Each%2520model%2520is%2520trained%2520only%250Aon%2520data%2520available%2520before%2520a%2520clearly%2520defined%2520knowledge-cutoff%2520date%252C%2520ensuring%250Astrict%2520temporal%2520separation%2520from%2520any%2520post-cutoff%2520data.%2520The%2520resulting%2520framework%250Aoffers%2520%2528i%2529%2520a%2520simple%252C%2520conversational%2520chat%2520interface%252C%2520%2528ii%2529%2520fully%2520open%252C%2520fixed%250Amodel%2520weights%2520that%2520guarantee%2520replicability%252C%2520and%2520%2528iii%2529%2520a%2520conservative%2520lower%250Abound%2520on%2520forecast%2520accuracy%252C%2520isolating%2520the%2520share%2520of%2520predictability%2520that%2520survives%250Aonce%2520training%2520leakage%2520is%2520removed.%2520Together%252C%2520these%2520features%2520provide%2520researchers%250Awith%2520an%2520easy-to-use%2520generative%2520AI%2520tool%2520useful%2520for%2520a%2520wide%2520range%2520of%2520prediction%250Atasks%2520that%2520is%2520free%2520of%2520lookahead%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chronologically%20Consistent%20Generative%20AI&entry.906535625=Songrun%20He%20and%20Linying%20Lv%20and%20Asaf%20Manela%20and%20Jimmy%20Wu&entry.1292438233=%20%20We%20introduce%20a%20family%20of%20chronologically%20consistent%2C%20instruction-following%0Alarge%20language%20models%20to%20eliminate%20lookahead%20bias.%20Each%20model%20is%20trained%20only%0Aon%20data%20available%20before%20a%20clearly%20defined%20knowledge-cutoff%20date%2C%20ensuring%0Astrict%20temporal%20separation%20from%20any%20post-cutoff%20data.%20The%20resulting%20framework%0Aoffers%20%28i%29%20a%20simple%2C%20conversational%20chat%20interface%2C%20%28ii%29%20fully%20open%2C%20fixed%0Amodel%20weights%20that%20guarantee%20replicability%2C%20and%20%28iii%29%20a%20conservative%20lower%0Abound%20on%20forecast%20accuracy%2C%20isolating%20the%20share%20of%20predictability%20that%20survives%0Aonce%20training%20leakage%20is%20removed.%20Together%2C%20these%20features%20provide%20researchers%0Awith%20an%20easy-to-use%20generative%20AI%20tool%20useful%20for%20a%20wide%20range%20of%20prediction%0Atasks%20that%20is%20free%20of%20lookahead%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11677v1&entry.124074799=Read"},
{"title": "Deconstructing Attention: Investigating Design Principles for Effective\n  Language Modeling", "author": "Huiyin Xue and Nafise Sadat Moosavi and Nikolaos Aletras", "abstract": "  The success of Transformer language models is widely credited to their\ndot-product attention mechanism, which interweaves a set of key design\nprinciples: mixing information across positions (enabling multi-token\ninteractions), sequence-dependent activations (where attention weights adapt to\neach input), a specific mathematical form (dot-product similarities plus\nsoftmax weighting), and coupling of queries and keys to evolving hidden states\n(grounding attention in the current layer). However, the necessity of each of\nthese principles remains largely untested. In this work, we systematically\ndeconstruct attention by designing controlled variants that selectively relax\nthese principles, applied both uniformly across all layers and in hybrid\narchitectures where only some layers retain standard attention. Our empirical\nanalysis reveals that mechanisms for mixing tokens are indispensable, as their\nabsence collapses models to near-random behavior, while the exact mathematical\nform and sequence dependency can be substantially relaxed, especially when\npreserved in just a subset of layers. Surprisingly, even variants that fail in\nisolation can achieve robust performance when interleaved with standard\nattention, highlighting a cooperative effect. These findings deepen our\nunderstanding of what truly underpins attention's effectiveness and open new\navenues for simplifying language models without sacrificing performance.\n", "link": "http://arxiv.org/abs/2510.11602v1", "date": "2025-10-13", "relevancy": 2.1917, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5663}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5638}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deconstructing%20Attention%3A%20Investigating%20Design%20Principles%20for%20Effective%0A%20%20Language%20Modeling&body=Title%3A%20Deconstructing%20Attention%3A%20Investigating%20Design%20Principles%20for%20Effective%0A%20%20Language%20Modeling%0AAuthor%3A%20Huiyin%20Xue%20and%20Nafise%20Sadat%20Moosavi%20and%20Nikolaos%20Aletras%0AAbstract%3A%20%20%20The%20success%20of%20Transformer%20language%20models%20is%20widely%20credited%20to%20their%0Adot-product%20attention%20mechanism%2C%20which%20interweaves%20a%20set%20of%20key%20design%0Aprinciples%3A%20mixing%20information%20across%20positions%20%28enabling%20multi-token%0Ainteractions%29%2C%20sequence-dependent%20activations%20%28where%20attention%20weights%20adapt%20to%0Aeach%20input%29%2C%20a%20specific%20mathematical%20form%20%28dot-product%20similarities%20plus%0Asoftmax%20weighting%29%2C%20and%20coupling%20of%20queries%20and%20keys%20to%20evolving%20hidden%20states%0A%28grounding%20attention%20in%20the%20current%20layer%29.%20However%2C%20the%20necessity%20of%20each%20of%0Athese%20principles%20remains%20largely%20untested.%20In%20this%20work%2C%20we%20systematically%0Adeconstruct%20attention%20by%20designing%20controlled%20variants%20that%20selectively%20relax%0Athese%20principles%2C%20applied%20both%20uniformly%20across%20all%20layers%20and%20in%20hybrid%0Aarchitectures%20where%20only%20some%20layers%20retain%20standard%20attention.%20Our%20empirical%0Aanalysis%20reveals%20that%20mechanisms%20for%20mixing%20tokens%20are%20indispensable%2C%20as%20their%0Aabsence%20collapses%20models%20to%20near-random%20behavior%2C%20while%20the%20exact%20mathematical%0Aform%20and%20sequence%20dependency%20can%20be%20substantially%20relaxed%2C%20especially%20when%0Apreserved%20in%20just%20a%20subset%20of%20layers.%20Surprisingly%2C%20even%20variants%20that%20fail%20in%0Aisolation%20can%20achieve%20robust%20performance%20when%20interleaved%20with%20standard%0Aattention%2C%20highlighting%20a%20cooperative%20effect.%20These%20findings%20deepen%20our%0Aunderstanding%20of%20what%20truly%20underpins%20attention%27s%20effectiveness%20and%20open%20new%0Aavenues%20for%20simplifying%20language%20models%20without%20sacrificing%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeconstructing%2520Attention%253A%2520Investigating%2520Design%2520Principles%2520for%2520Effective%250A%2520%2520Language%2520Modeling%26entry.906535625%3DHuiyin%2520Xue%2520and%2520Nafise%2520Sadat%2520Moosavi%2520and%2520Nikolaos%2520Aletras%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520Transformer%2520language%2520models%2520is%2520widely%2520credited%2520to%2520their%250Adot-product%2520attention%2520mechanism%252C%2520which%2520interweaves%2520a%2520set%2520of%2520key%2520design%250Aprinciples%253A%2520mixing%2520information%2520across%2520positions%2520%2528enabling%2520multi-token%250Ainteractions%2529%252C%2520sequence-dependent%2520activations%2520%2528where%2520attention%2520weights%2520adapt%2520to%250Aeach%2520input%2529%252C%2520a%2520specific%2520mathematical%2520form%2520%2528dot-product%2520similarities%2520plus%250Asoftmax%2520weighting%2529%252C%2520and%2520coupling%2520of%2520queries%2520and%2520keys%2520to%2520evolving%2520hidden%2520states%250A%2528grounding%2520attention%2520in%2520the%2520current%2520layer%2529.%2520However%252C%2520the%2520necessity%2520of%2520each%2520of%250Athese%2520principles%2520remains%2520largely%2520untested.%2520In%2520this%2520work%252C%2520we%2520systematically%250Adeconstruct%2520attention%2520by%2520designing%2520controlled%2520variants%2520that%2520selectively%2520relax%250Athese%2520principles%252C%2520applied%2520both%2520uniformly%2520across%2520all%2520layers%2520and%2520in%2520hybrid%250Aarchitectures%2520where%2520only%2520some%2520layers%2520retain%2520standard%2520attention.%2520Our%2520empirical%250Aanalysis%2520reveals%2520that%2520mechanisms%2520for%2520mixing%2520tokens%2520are%2520indispensable%252C%2520as%2520their%250Aabsence%2520collapses%2520models%2520to%2520near-random%2520behavior%252C%2520while%2520the%2520exact%2520mathematical%250Aform%2520and%2520sequence%2520dependency%2520can%2520be%2520substantially%2520relaxed%252C%2520especially%2520when%250Apreserved%2520in%2520just%2520a%2520subset%2520of%2520layers.%2520Surprisingly%252C%2520even%2520variants%2520that%2520fail%2520in%250Aisolation%2520can%2520achieve%2520robust%2520performance%2520when%2520interleaved%2520with%2520standard%250Aattention%252C%2520highlighting%2520a%2520cooperative%2520effect.%2520These%2520findings%2520deepen%2520our%250Aunderstanding%2520of%2520what%2520truly%2520underpins%2520attention%2527s%2520effectiveness%2520and%2520open%2520new%250Aavenues%2520for%2520simplifying%2520language%2520models%2520without%2520sacrificing%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deconstructing%20Attention%3A%20Investigating%20Design%20Principles%20for%20Effective%0A%20%20Language%20Modeling&entry.906535625=Huiyin%20Xue%20and%20Nafise%20Sadat%20Moosavi%20and%20Nikolaos%20Aletras&entry.1292438233=%20%20The%20success%20of%20Transformer%20language%20models%20is%20widely%20credited%20to%20their%0Adot-product%20attention%20mechanism%2C%20which%20interweaves%20a%20set%20of%20key%20design%0Aprinciples%3A%20mixing%20information%20across%20positions%20%28enabling%20multi-token%0Ainteractions%29%2C%20sequence-dependent%20activations%20%28where%20attention%20weights%20adapt%20to%0Aeach%20input%29%2C%20a%20specific%20mathematical%20form%20%28dot-product%20similarities%20plus%0Asoftmax%20weighting%29%2C%20and%20coupling%20of%20queries%20and%20keys%20to%20evolving%20hidden%20states%0A%28grounding%20attention%20in%20the%20current%20layer%29.%20However%2C%20the%20necessity%20of%20each%20of%0Athese%20principles%20remains%20largely%20untested.%20In%20this%20work%2C%20we%20systematically%0Adeconstruct%20attention%20by%20designing%20controlled%20variants%20that%20selectively%20relax%0Athese%20principles%2C%20applied%20both%20uniformly%20across%20all%20layers%20and%20in%20hybrid%0Aarchitectures%20where%20only%20some%20layers%20retain%20standard%20attention.%20Our%20empirical%0Aanalysis%20reveals%20that%20mechanisms%20for%20mixing%20tokens%20are%20indispensable%2C%20as%20their%0Aabsence%20collapses%20models%20to%20near-random%20behavior%2C%20while%20the%20exact%20mathematical%0Aform%20and%20sequence%20dependency%20can%20be%20substantially%20relaxed%2C%20especially%20when%0Apreserved%20in%20just%20a%20subset%20of%20layers.%20Surprisingly%2C%20even%20variants%20that%20fail%20in%0Aisolation%20can%20achieve%20robust%20performance%20when%20interleaved%20with%20standard%0Aattention%2C%20highlighting%20a%20cooperative%20effect.%20These%20findings%20deepen%20our%0Aunderstanding%20of%20what%20truly%20underpins%20attention%27s%20effectiveness%20and%20open%20new%0Aavenues%20for%20simplifying%20language%20models%20without%20sacrificing%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11602v1&entry.124074799=Read"},
{"title": "Characterizing Web Search in The Age of Generative AI", "author": "Elisabeth Kirsten and Jost Grosse Perdekamp and Mihir Upadhyay and Krishna P. Gummadi and Muhammad Bilal Zafar", "abstract": "  The advent of LLMs has given rise to a new type of web search: Generative\nsearch, where LLMs retrieve web pages related to a query and generate a single,\ncoherent text as a response. This output modality stands in stark contrast to\ntraditional web search, where results are returned as a ranked list of\nindependent web pages. In this paper, we ask: Along what dimensions do\ngenerative search outputs differ from traditional web search? We compare\nGoogle, a traditional web search engine, with four generative search engines\nfrom two providers (Google and OpenAI) across queries from four domains. Our\nanalysis reveals intriguing differences. Most generative search engines cover a\nwider range of sources compared to web search. Generative search engines vary\nin the degree to which they rely on internal knowledge contained within the\nmodel parameters v.s. external knowledge retrieved from the web. Generative\nsearch engines surface varying sets of concepts, creating new opportunities for\nenhancing search diversity and serendipity. Our results also highlight the need\nfor revisiting evaluation criteria for web search in the age of Generative AI.\n", "link": "http://arxiv.org/abs/2510.11560v1", "date": "2025-10-13", "relevancy": 2.1535, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5641}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5343}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterizing%20Web%20Search%20in%20The%20Age%20of%20Generative%20AI&body=Title%3A%20Characterizing%20Web%20Search%20in%20The%20Age%20of%20Generative%20AI%0AAuthor%3A%20Elisabeth%20Kirsten%20and%20Jost%20Grosse%20Perdekamp%20and%20Mihir%20Upadhyay%20and%20Krishna%20P.%20Gummadi%20and%20Muhammad%20Bilal%20Zafar%0AAbstract%3A%20%20%20The%20advent%20of%20LLMs%20has%20given%20rise%20to%20a%20new%20type%20of%20web%20search%3A%20Generative%0Asearch%2C%20where%20LLMs%20retrieve%20web%20pages%20related%20to%20a%20query%20and%20generate%20a%20single%2C%0Acoherent%20text%20as%20a%20response.%20This%20output%20modality%20stands%20in%20stark%20contrast%20to%0Atraditional%20web%20search%2C%20where%20results%20are%20returned%20as%20a%20ranked%20list%20of%0Aindependent%20web%20pages.%20In%20this%20paper%2C%20we%20ask%3A%20Along%20what%20dimensions%20do%0Agenerative%20search%20outputs%20differ%20from%20traditional%20web%20search%3F%20We%20compare%0AGoogle%2C%20a%20traditional%20web%20search%20engine%2C%20with%20four%20generative%20search%20engines%0Afrom%20two%20providers%20%28Google%20and%20OpenAI%29%20across%20queries%20from%20four%20domains.%20Our%0Aanalysis%20reveals%20intriguing%20differences.%20Most%20generative%20search%20engines%20cover%20a%0Awider%20range%20of%20sources%20compared%20to%20web%20search.%20Generative%20search%20engines%20vary%0Ain%20the%20degree%20to%20which%20they%20rely%20on%20internal%20knowledge%20contained%20within%20the%0Amodel%20parameters%20v.s.%20external%20knowledge%20retrieved%20from%20the%20web.%20Generative%0Asearch%20engines%20surface%20varying%20sets%20of%20concepts%2C%20creating%20new%20opportunities%20for%0Aenhancing%20search%20diversity%20and%20serendipity.%20Our%20results%20also%20highlight%20the%20need%0Afor%20revisiting%20evaluation%20criteria%20for%20web%20search%20in%20the%20age%20of%20Generative%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterizing%2520Web%2520Search%2520in%2520The%2520Age%2520of%2520Generative%2520AI%26entry.906535625%3DElisabeth%2520Kirsten%2520and%2520Jost%2520Grosse%2520Perdekamp%2520and%2520Mihir%2520Upadhyay%2520and%2520Krishna%2520P.%2520Gummadi%2520and%2520Muhammad%2520Bilal%2520Zafar%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520LLMs%2520has%2520given%2520rise%2520to%2520a%2520new%2520type%2520of%2520web%2520search%253A%2520Generative%250Asearch%252C%2520where%2520LLMs%2520retrieve%2520web%2520pages%2520related%2520to%2520a%2520query%2520and%2520generate%2520a%2520single%252C%250Acoherent%2520text%2520as%2520a%2520response.%2520This%2520output%2520modality%2520stands%2520in%2520stark%2520contrast%2520to%250Atraditional%2520web%2520search%252C%2520where%2520results%2520are%2520returned%2520as%2520a%2520ranked%2520list%2520of%250Aindependent%2520web%2520pages.%2520In%2520this%2520paper%252C%2520we%2520ask%253A%2520Along%2520what%2520dimensions%2520do%250Agenerative%2520search%2520outputs%2520differ%2520from%2520traditional%2520web%2520search%253F%2520We%2520compare%250AGoogle%252C%2520a%2520traditional%2520web%2520search%2520engine%252C%2520with%2520four%2520generative%2520search%2520engines%250Afrom%2520two%2520providers%2520%2528Google%2520and%2520OpenAI%2529%2520across%2520queries%2520from%2520four%2520domains.%2520Our%250Aanalysis%2520reveals%2520intriguing%2520differences.%2520Most%2520generative%2520search%2520engines%2520cover%2520a%250Awider%2520range%2520of%2520sources%2520compared%2520to%2520web%2520search.%2520Generative%2520search%2520engines%2520vary%250Ain%2520the%2520degree%2520to%2520which%2520they%2520rely%2520on%2520internal%2520knowledge%2520contained%2520within%2520the%250Amodel%2520parameters%2520v.s.%2520external%2520knowledge%2520retrieved%2520from%2520the%2520web.%2520Generative%250Asearch%2520engines%2520surface%2520varying%2520sets%2520of%2520concepts%252C%2520creating%2520new%2520opportunities%2520for%250Aenhancing%2520search%2520diversity%2520and%2520serendipity.%2520Our%2520results%2520also%2520highlight%2520the%2520need%250Afor%2520revisiting%2520evaluation%2520criteria%2520for%2520web%2520search%2520in%2520the%2520age%2520of%2520Generative%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterizing%20Web%20Search%20in%20The%20Age%20of%20Generative%20AI&entry.906535625=Elisabeth%20Kirsten%20and%20Jost%20Grosse%20Perdekamp%20and%20Mihir%20Upadhyay%20and%20Krishna%20P.%20Gummadi%20and%20Muhammad%20Bilal%20Zafar&entry.1292438233=%20%20The%20advent%20of%20LLMs%20has%20given%20rise%20to%20a%20new%20type%20of%20web%20search%3A%20Generative%0Asearch%2C%20where%20LLMs%20retrieve%20web%20pages%20related%20to%20a%20query%20and%20generate%20a%20single%2C%0Acoherent%20text%20as%20a%20response.%20This%20output%20modality%20stands%20in%20stark%20contrast%20to%0Atraditional%20web%20search%2C%20where%20results%20are%20returned%20as%20a%20ranked%20list%20of%0Aindependent%20web%20pages.%20In%20this%20paper%2C%20we%20ask%3A%20Along%20what%20dimensions%20do%0Agenerative%20search%20outputs%20differ%20from%20traditional%20web%20search%3F%20We%20compare%0AGoogle%2C%20a%20traditional%20web%20search%20engine%2C%20with%20four%20generative%20search%20engines%0Afrom%20two%20providers%20%28Google%20and%20OpenAI%29%20across%20queries%20from%20four%20domains.%20Our%0Aanalysis%20reveals%20intriguing%20differences.%20Most%20generative%20search%20engines%20cover%20a%0Awider%20range%20of%20sources%20compared%20to%20web%20search.%20Generative%20search%20engines%20vary%0Ain%20the%20degree%20to%20which%20they%20rely%20on%20internal%20knowledge%20contained%20within%20the%0Amodel%20parameters%20v.s.%20external%20knowledge%20retrieved%20from%20the%20web.%20Generative%0Asearch%20engines%20surface%20varying%20sets%20of%20concepts%2C%20creating%20new%20opportunities%20for%0Aenhancing%20search%20diversity%20and%20serendipity.%20Our%20results%20also%20highlight%20the%20need%0Afor%20revisiting%20evaluation%20criteria%20for%20web%20search%20in%20the%20age%20of%20Generative%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11560v1&entry.124074799=Read"},
{"title": "MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis", "author": "Hongyu Zhu and Lin Chen and Mounim A. El-Yacoubi and Mingsheng Shang", "abstract": "  Multimodal Sentiment Analysis (MSA) aims to identify and interpret human\nemotions by integrating information from heterogeneous data sources such as\ntext, video, and audio. While deep learning models have advanced in network\narchitecture design, they remain heavily limited by scarce multimodal annotated\ndata. Although Mixup-based augmentation improves generalization in unimodal\ntasks, its direct application to MSA introduces critical challenges: random\nmixing often amplifies label ambiguity and semantic inconsistency due to the\nlack of emotion-aware mixing mechanisms. To overcome these issues, we propose\nMS-Mix, an adaptive, emotion-sensitive augmentation framework that\nautomatically optimizes sample mixing in multimodal settings. The key\ncomponents of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)\nstrategy that effectively prevents semantic confusion caused by mixing samples\nwith contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module\nusing multi-head self-attention to compute modality-specific mixing ratios\ndynamically based on their respective emotional intensities. (3) a Sentiment\nAlignment Loss (SAL) that aligns the prediction distributions across\nmodalities, and incorporates the Kullback-Leibler-based loss as an additional\nregularization term to train the emotion intensity predictor and the backbone\nnetwork jointly. Extensive experiments on three benchmark datasets with six\nstate-of-the-art backbones confirm that MS-Mix consistently outperforms\nexisting methods, establishing a new standard for robust multimodal sentiment\naugmentation. The source code is available at:\nhttps://github.com/HongyuZhu-s/MS-Mix.\n", "link": "http://arxiv.org/abs/2510.11579v1", "date": "2025-10-13", "relevancy": 2.1241, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5347}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5318}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-Mix%3A%20Unveiling%20the%20Power%20of%20Mixup%20for%20Multimodal%20Sentiment%20Analysis&body=Title%3A%20MS-Mix%3A%20Unveiling%20the%20Power%20of%20Mixup%20for%20Multimodal%20Sentiment%20Analysis%0AAuthor%3A%20Hongyu%20Zhu%20and%20Lin%20Chen%20and%20Mounim%20A.%20El-Yacoubi%20and%20Mingsheng%20Shang%0AAbstract%3A%20%20%20Multimodal%20Sentiment%20Analysis%20%28MSA%29%20aims%20to%20identify%20and%20interpret%20human%0Aemotions%20by%20integrating%20information%20from%20heterogeneous%20data%20sources%20such%20as%0Atext%2C%20video%2C%20and%20audio.%20While%20deep%20learning%20models%20have%20advanced%20in%20network%0Aarchitecture%20design%2C%20they%20remain%20heavily%20limited%20by%20scarce%20multimodal%20annotated%0Adata.%20Although%20Mixup-based%20augmentation%20improves%20generalization%20in%20unimodal%0Atasks%2C%20its%20direct%20application%20to%20MSA%20introduces%20critical%20challenges%3A%20random%0Amixing%20often%20amplifies%20label%20ambiguity%20and%20semantic%20inconsistency%20due%20to%20the%0Alack%20of%20emotion-aware%20mixing%20mechanisms.%20To%20overcome%20these%20issues%2C%20we%20propose%0AMS-Mix%2C%20an%20adaptive%2C%20emotion-sensitive%20augmentation%20framework%20that%0Aautomatically%20optimizes%20sample%20mixing%20in%20multimodal%20settings.%20The%20key%0Acomponents%20of%20MS-Mix%20include%3A%20%281%29%20a%20Sentiment-Aware%20Sample%20Selection%20%28SASS%29%0Astrategy%20that%20effectively%20prevents%20semantic%20confusion%20caused%20by%20mixing%20samples%0Awith%20contradictory%20emotions.%20%282%29%20a%20Sentiment%20Intensity%20Guided%20%28SIG%29%20module%0Ausing%20multi-head%20self-attention%20to%20compute%20modality-specific%20mixing%20ratios%0Adynamically%20based%20on%20their%20respective%20emotional%20intensities.%20%283%29%20a%20Sentiment%0AAlignment%20Loss%20%28SAL%29%20that%20aligns%20the%20prediction%20distributions%20across%0Amodalities%2C%20and%20incorporates%20the%20Kullback-Leibler-based%20loss%20as%20an%20additional%0Aregularization%20term%20to%20train%20the%20emotion%20intensity%20predictor%20and%20the%20backbone%0Anetwork%20jointly.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20with%20six%0Astate-of-the-art%20backbones%20confirm%20that%20MS-Mix%20consistently%20outperforms%0Aexisting%20methods%2C%20establishing%20a%20new%20standard%20for%20robust%20multimodal%20sentiment%0Aaugmentation.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/HongyuZhu-s/MS-Mix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-Mix%253A%2520Unveiling%2520the%2520Power%2520of%2520Mixup%2520for%2520Multimodal%2520Sentiment%2520Analysis%26entry.906535625%3DHongyu%2520Zhu%2520and%2520Lin%2520Chen%2520and%2520Mounim%2520A.%2520El-Yacoubi%2520and%2520Mingsheng%2520Shang%26entry.1292438233%3D%2520%2520Multimodal%2520Sentiment%2520Analysis%2520%2528MSA%2529%2520aims%2520to%2520identify%2520and%2520interpret%2520human%250Aemotions%2520by%2520integrating%2520information%2520from%2520heterogeneous%2520data%2520sources%2520such%2520as%250Atext%252C%2520video%252C%2520and%2520audio.%2520While%2520deep%2520learning%2520models%2520have%2520advanced%2520in%2520network%250Aarchitecture%2520design%252C%2520they%2520remain%2520heavily%2520limited%2520by%2520scarce%2520multimodal%2520annotated%250Adata.%2520Although%2520Mixup-based%2520augmentation%2520improves%2520generalization%2520in%2520unimodal%250Atasks%252C%2520its%2520direct%2520application%2520to%2520MSA%2520introduces%2520critical%2520challenges%253A%2520random%250Amixing%2520often%2520amplifies%2520label%2520ambiguity%2520and%2520semantic%2520inconsistency%2520due%2520to%2520the%250Alack%2520of%2520emotion-aware%2520mixing%2520mechanisms.%2520To%2520overcome%2520these%2520issues%252C%2520we%2520propose%250AMS-Mix%252C%2520an%2520adaptive%252C%2520emotion-sensitive%2520augmentation%2520framework%2520that%250Aautomatically%2520optimizes%2520sample%2520mixing%2520in%2520multimodal%2520settings.%2520The%2520key%250Acomponents%2520of%2520MS-Mix%2520include%253A%2520%25281%2529%2520a%2520Sentiment-Aware%2520Sample%2520Selection%2520%2528SASS%2529%250Astrategy%2520that%2520effectively%2520prevents%2520semantic%2520confusion%2520caused%2520by%2520mixing%2520samples%250Awith%2520contradictory%2520emotions.%2520%25282%2529%2520a%2520Sentiment%2520Intensity%2520Guided%2520%2528SIG%2529%2520module%250Ausing%2520multi-head%2520self-attention%2520to%2520compute%2520modality-specific%2520mixing%2520ratios%250Adynamically%2520based%2520on%2520their%2520respective%2520emotional%2520intensities.%2520%25283%2529%2520a%2520Sentiment%250AAlignment%2520Loss%2520%2528SAL%2529%2520that%2520aligns%2520the%2520prediction%2520distributions%2520across%250Amodalities%252C%2520and%2520incorporates%2520the%2520Kullback-Leibler-based%2520loss%2520as%2520an%2520additional%250Aregularization%2520term%2520to%2520train%2520the%2520emotion%2520intensity%2520predictor%2520and%2520the%2520backbone%250Anetwork%2520jointly.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520with%2520six%250Astate-of-the-art%2520backbones%2520confirm%2520that%2520MS-Mix%2520consistently%2520outperforms%250Aexisting%2520methods%252C%2520establishing%2520a%2520new%2520standard%2520for%2520robust%2520multimodal%2520sentiment%250Aaugmentation.%2520The%2520source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/HongyuZhu-s/MS-Mix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-Mix%3A%20Unveiling%20the%20Power%20of%20Mixup%20for%20Multimodal%20Sentiment%20Analysis&entry.906535625=Hongyu%20Zhu%20and%20Lin%20Chen%20and%20Mounim%20A.%20El-Yacoubi%20and%20Mingsheng%20Shang&entry.1292438233=%20%20Multimodal%20Sentiment%20Analysis%20%28MSA%29%20aims%20to%20identify%20and%20interpret%20human%0Aemotions%20by%20integrating%20information%20from%20heterogeneous%20data%20sources%20such%20as%0Atext%2C%20video%2C%20and%20audio.%20While%20deep%20learning%20models%20have%20advanced%20in%20network%0Aarchitecture%20design%2C%20they%20remain%20heavily%20limited%20by%20scarce%20multimodal%20annotated%0Adata.%20Although%20Mixup-based%20augmentation%20improves%20generalization%20in%20unimodal%0Atasks%2C%20its%20direct%20application%20to%20MSA%20introduces%20critical%20challenges%3A%20random%0Amixing%20often%20amplifies%20label%20ambiguity%20and%20semantic%20inconsistency%20due%20to%20the%0Alack%20of%20emotion-aware%20mixing%20mechanisms.%20To%20overcome%20these%20issues%2C%20we%20propose%0AMS-Mix%2C%20an%20adaptive%2C%20emotion-sensitive%20augmentation%20framework%20that%0Aautomatically%20optimizes%20sample%20mixing%20in%20multimodal%20settings.%20The%20key%0Acomponents%20of%20MS-Mix%20include%3A%20%281%29%20a%20Sentiment-Aware%20Sample%20Selection%20%28SASS%29%0Astrategy%20that%20effectively%20prevents%20semantic%20confusion%20caused%20by%20mixing%20samples%0Awith%20contradictory%20emotions.%20%282%29%20a%20Sentiment%20Intensity%20Guided%20%28SIG%29%20module%0Ausing%20multi-head%20self-attention%20to%20compute%20modality-specific%20mixing%20ratios%0Adynamically%20based%20on%20their%20respective%20emotional%20intensities.%20%283%29%20a%20Sentiment%0AAlignment%20Loss%20%28SAL%29%20that%20aligns%20the%20prediction%20distributions%20across%0Amodalities%2C%20and%20incorporates%20the%20Kullback-Leibler-based%20loss%20as%20an%20additional%0Aregularization%20term%20to%20train%20the%20emotion%20intensity%20predictor%20and%20the%20backbone%0Anetwork%20jointly.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20with%20six%0Astate-of-the-art%20backbones%20confirm%20that%20MS-Mix%20consistently%20outperforms%0Aexisting%20methods%2C%20establishing%20a%20new%20standard%20for%20robust%20multimodal%20sentiment%0Aaugmentation.%20The%20source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/HongyuZhu-s/MS-Mix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11579v1&entry.124074799=Read"},
{"title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific\n  and Interpretable Scientific Domain Mapping", "author": "Marc Brinner and Sina Zarrie\u00df", "abstract": "  We propose SemCSE-Multi, a novel unsupervised framework for generating\nmultifaceted embeddings of scientific abstracts, evaluated in the domains of\ninvasion biology and medicine. These embeddings capture distinct, individually\nspecifiable aspects in isolation, thus enabling fine-grained and controllable\nsimilarity assessments as well as adaptive, user-driven visualizations of\nscientific domains. Our approach relies on an unsupervised procedure that\nproduces aspect-specific summarizing sentences and trains embedding models to\nmap semantically related summaries to nearby positions in the embedding space.\nWe then distill these aspect-specific embedding capabilities into a unified\nembedding model that directly predicts multiple aspect embeddings from a\nscientific abstract in a single, efficient forward pass. In addition, we\nintroduce an embedding decoding pipeline that decodes embeddings back into\nnatural language descriptions of their associated aspects. Notably, we show\nthat this decoding remains effective even for unoccupied regions in\nlow-dimensional visualizations, thus offering vastly improved interpretability\nin user-centric settings.\n", "link": "http://arxiv.org/abs/2510.11599v1", "date": "2025-10-13", "relevancy": 2.1131, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemCSE-Multi%3A%20Multifaceted%20and%20Decodable%20Embeddings%20for%20Aspect-Specific%0A%20%20and%20Interpretable%20Scientific%20Domain%20Mapping&body=Title%3A%20SemCSE-Multi%3A%20Multifaceted%20and%20Decodable%20Embeddings%20for%20Aspect-Specific%0A%20%20and%20Interpretable%20Scientific%20Domain%20Mapping%0AAuthor%3A%20Marc%20Brinner%20and%20Sina%20Zarrie%C3%9F%0AAbstract%3A%20%20%20We%20propose%20SemCSE-Multi%2C%20a%20novel%20unsupervised%20framework%20for%20generating%0Amultifaceted%20embeddings%20of%20scientific%20abstracts%2C%20evaluated%20in%20the%20domains%20of%0Ainvasion%20biology%20and%20medicine.%20These%20embeddings%20capture%20distinct%2C%20individually%0Aspecifiable%20aspects%20in%20isolation%2C%20thus%20enabling%20fine-grained%20and%20controllable%0Asimilarity%20assessments%20as%20well%20as%20adaptive%2C%20user-driven%20visualizations%20of%0Ascientific%20domains.%20Our%20approach%20relies%20on%20an%20unsupervised%20procedure%20that%0Aproduces%20aspect-specific%20summarizing%20sentences%20and%20trains%20embedding%20models%20to%0Amap%20semantically%20related%20summaries%20to%20nearby%20positions%20in%20the%20embedding%20space.%0AWe%20then%20distill%20these%20aspect-specific%20embedding%20capabilities%20into%20a%20unified%0Aembedding%20model%20that%20directly%20predicts%20multiple%20aspect%20embeddings%20from%20a%0Ascientific%20abstract%20in%20a%20single%2C%20efficient%20forward%20pass.%20In%20addition%2C%20we%0Aintroduce%20an%20embedding%20decoding%20pipeline%20that%20decodes%20embeddings%20back%20into%0Anatural%20language%20descriptions%20of%20their%20associated%20aspects.%20Notably%2C%20we%20show%0Athat%20this%20decoding%20remains%20effective%20even%20for%20unoccupied%20regions%20in%0Alow-dimensional%20visualizations%2C%20thus%20offering%20vastly%20improved%20interpretability%0Ain%20user-centric%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemCSE-Multi%253A%2520Multifaceted%2520and%2520Decodable%2520Embeddings%2520for%2520Aspect-Specific%250A%2520%2520and%2520Interpretable%2520Scientific%2520Domain%2520Mapping%26entry.906535625%3DMarc%2520Brinner%2520and%2520Sina%2520Zarrie%25C3%259F%26entry.1292438233%3D%2520%2520We%2520propose%2520SemCSE-Multi%252C%2520a%2520novel%2520unsupervised%2520framework%2520for%2520generating%250Amultifaceted%2520embeddings%2520of%2520scientific%2520abstracts%252C%2520evaluated%2520in%2520the%2520domains%2520of%250Ainvasion%2520biology%2520and%2520medicine.%2520These%2520embeddings%2520capture%2520distinct%252C%2520individually%250Aspecifiable%2520aspects%2520in%2520isolation%252C%2520thus%2520enabling%2520fine-grained%2520and%2520controllable%250Asimilarity%2520assessments%2520as%2520well%2520as%2520adaptive%252C%2520user-driven%2520visualizations%2520of%250Ascientific%2520domains.%2520Our%2520approach%2520relies%2520on%2520an%2520unsupervised%2520procedure%2520that%250Aproduces%2520aspect-specific%2520summarizing%2520sentences%2520and%2520trains%2520embedding%2520models%2520to%250Amap%2520semantically%2520related%2520summaries%2520to%2520nearby%2520positions%2520in%2520the%2520embedding%2520space.%250AWe%2520then%2520distill%2520these%2520aspect-specific%2520embedding%2520capabilities%2520into%2520a%2520unified%250Aembedding%2520model%2520that%2520directly%2520predicts%2520multiple%2520aspect%2520embeddings%2520from%2520a%250Ascientific%2520abstract%2520in%2520a%2520single%252C%2520efficient%2520forward%2520pass.%2520In%2520addition%252C%2520we%250Aintroduce%2520an%2520embedding%2520decoding%2520pipeline%2520that%2520decodes%2520embeddings%2520back%2520into%250Anatural%2520language%2520descriptions%2520of%2520their%2520associated%2520aspects.%2520Notably%252C%2520we%2520show%250Athat%2520this%2520decoding%2520remains%2520effective%2520even%2520for%2520unoccupied%2520regions%2520in%250Alow-dimensional%2520visualizations%252C%2520thus%2520offering%2520vastly%2520improved%2520interpretability%250Ain%2520user-centric%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemCSE-Multi%3A%20Multifaceted%20and%20Decodable%20Embeddings%20for%20Aspect-Specific%0A%20%20and%20Interpretable%20Scientific%20Domain%20Mapping&entry.906535625=Marc%20Brinner%20and%20Sina%20Zarrie%C3%9F&entry.1292438233=%20%20We%20propose%20SemCSE-Multi%2C%20a%20novel%20unsupervised%20framework%20for%20generating%0Amultifaceted%20embeddings%20of%20scientific%20abstracts%2C%20evaluated%20in%20the%20domains%20of%0Ainvasion%20biology%20and%20medicine.%20These%20embeddings%20capture%20distinct%2C%20individually%0Aspecifiable%20aspects%20in%20isolation%2C%20thus%20enabling%20fine-grained%20and%20controllable%0Asimilarity%20assessments%20as%20well%20as%20adaptive%2C%20user-driven%20visualizations%20of%0Ascientific%20domains.%20Our%20approach%20relies%20on%20an%20unsupervised%20procedure%20that%0Aproduces%20aspect-specific%20summarizing%20sentences%20and%20trains%20embedding%20models%20to%0Amap%20semantically%20related%20summaries%20to%20nearby%20positions%20in%20the%20embedding%20space.%0AWe%20then%20distill%20these%20aspect-specific%20embedding%20capabilities%20into%20a%20unified%0Aembedding%20model%20that%20directly%20predicts%20multiple%20aspect%20embeddings%20from%20a%0Ascientific%20abstract%20in%20a%20single%2C%20efficient%20forward%20pass.%20In%20addition%2C%20we%0Aintroduce%20an%20embedding%20decoding%20pipeline%20that%20decodes%20embeddings%20back%20into%0Anatural%20language%20descriptions%20of%20their%20associated%20aspects.%20Notably%2C%20we%20show%0Athat%20this%20decoding%20remains%20effective%20even%20for%20unoccupied%20regions%20in%0Alow-dimensional%20visualizations%2C%20thus%20offering%20vastly%20improved%20interpretability%0Ain%20user-centric%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11599v1&entry.124074799=Read"},
{"title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding", "author": "Haolin Yang and Feilong Tang and Lingxiao Zhao and Xiang An and Ming Hu and Huifa Li and Xinlin Zhuang and Yifan Lu and Xiaofeng Zhang and Abdalla Swikir and Junjun He and Zongyuan Ge and Imran Razzak", "abstract": "  Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.\n", "link": "http://arxiv.org/abs/2508.01875v3", "date": "2025-10-13", "relevancy": 2.1008, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamAgent%3A%20Towards%20Anticipatory%20Agents%20for%20Streaming%20Video%0A%20%20Understanding&body=Title%3A%20StreamAgent%3A%20Towards%20Anticipatory%20Agents%20for%20Streaming%20Video%0A%20%20Understanding%0AAuthor%3A%20Haolin%20Yang%20and%20Feilong%20Tang%20and%20Lingxiao%20Zhao%20and%20Xiang%20An%20and%20Ming%20Hu%20and%20Huifa%20Li%20and%20Xinlin%20Zhuang%20and%20Yifan%20Lu%20and%20Xiaofeng%20Zhang%20and%20Abdalla%20Swikir%20and%20Junjun%20He%20and%20Zongyuan%20Ge%20and%20Imran%20Razzak%0AAbstract%3A%20%20%20Real-time%20streaming%20video%20understanding%20in%20domains%20such%20as%20autonomous%20driving%0Aand%20intelligent%20surveillance%20poses%20challenges%20beyond%20conventional%20offline%20video%0Aprocessing%2C%20requiring%20continuous%20perception%2C%20proactive%20decision%20making%2C%20and%0Aresponsive%20interaction%20based%20on%20dynamically%20evolving%20visual%20content.%20However%2C%0Aexisting%20methods%20rely%20on%20alternating%20perception-reaction%20or%20asynchronous%0Atriggers%2C%20lacking%20task-driven%20planning%20and%20future%20anticipation%2C%20which%20limits%0Atheir%20real-time%20responsiveness%20and%20proactive%20decision%20making%20in%20evolving%20video%0Astreams.%20To%20this%20end%2C%20we%20propose%20a%20StreamAgent%20that%20anticipates%20the%20temporal%0Aintervals%20and%20spatial%20regions%20expected%20to%20contain%20future%20task-relevant%0Ainformation%20to%20enable%20proactive%20and%20goal-driven%20responses.%20Specifically%2C%20we%0Aintegrate%20question%20semantics%20and%20historical%20observations%20through%20prompting%20the%0Aanticipatory%20agent%20to%20anticipate%20the%20temporal%20progression%20of%20key%20events%2C%20align%0Acurrent%20observations%20with%20the%20expected%20future%20evidence%2C%20and%20subsequently%20adjust%0Athe%20perception%20action%20%28e.g.%2C%20attending%20to%20task-relevant%20regions%20or%20continuously%0Atracking%20in%20subsequent%20frames%29.%20To%20enable%20efficient%20inference%2C%20we%20design%20a%0Astreaming%20KV-cache%20memory%20mechanism%20that%20constructs%20a%20hierarchical%20memory%0Astructure%20for%20selective%20recall%20of%20relevant%20tokens%2C%20enabling%20efficient%20semantic%0Aretrieval%20while%20reducing%20the%20overhead%20of%20storing%20all%20tokens%20in%20the%20traditional%0AKV-cache.%20Extensive%20experiments%20on%20streaming%20and%20long%20video%20understanding%20tasks%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20methods%20in%20response%20accuracy%0Aand%20real-time%20efficiency%2C%20highlighting%20its%20practical%20value%20for%20real-world%0Astreaming%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.01875v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamAgent%253A%2520Towards%2520Anticipatory%2520Agents%2520for%2520Streaming%2520Video%250A%2520%2520Understanding%26entry.906535625%3DHaolin%2520Yang%2520and%2520Feilong%2520Tang%2520and%2520Lingxiao%2520Zhao%2520and%2520Xiang%2520An%2520and%2520Ming%2520Hu%2520and%2520Huifa%2520Li%2520and%2520Xinlin%2520Zhuang%2520and%2520Yifan%2520Lu%2520and%2520Xiaofeng%2520Zhang%2520and%2520Abdalla%2520Swikir%2520and%2520Junjun%2520He%2520and%2520Zongyuan%2520Ge%2520and%2520Imran%2520Razzak%26entry.1292438233%3D%2520%2520Real-time%2520streaming%2520video%2520understanding%2520in%2520domains%2520such%2520as%2520autonomous%2520driving%250Aand%2520intelligent%2520surveillance%2520poses%2520challenges%2520beyond%2520conventional%2520offline%2520video%250Aprocessing%252C%2520requiring%2520continuous%2520perception%252C%2520proactive%2520decision%2520making%252C%2520and%250Aresponsive%2520interaction%2520based%2520on%2520dynamically%2520evolving%2520visual%2520content.%2520However%252C%250Aexisting%2520methods%2520rely%2520on%2520alternating%2520perception-reaction%2520or%2520asynchronous%250Atriggers%252C%2520lacking%2520task-driven%2520planning%2520and%2520future%2520anticipation%252C%2520which%2520limits%250Atheir%2520real-time%2520responsiveness%2520and%2520proactive%2520decision%2520making%2520in%2520evolving%2520video%250Astreams.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520StreamAgent%2520that%2520anticipates%2520the%2520temporal%250Aintervals%2520and%2520spatial%2520regions%2520expected%2520to%2520contain%2520future%2520task-relevant%250Ainformation%2520to%2520enable%2520proactive%2520and%2520goal-driven%2520responses.%2520Specifically%252C%2520we%250Aintegrate%2520question%2520semantics%2520and%2520historical%2520observations%2520through%2520prompting%2520the%250Aanticipatory%2520agent%2520to%2520anticipate%2520the%2520temporal%2520progression%2520of%2520key%2520events%252C%2520align%250Acurrent%2520observations%2520with%2520the%2520expected%2520future%2520evidence%252C%2520and%2520subsequently%2520adjust%250Athe%2520perception%2520action%2520%2528e.g.%252C%2520attending%2520to%2520task-relevant%2520regions%2520or%2520continuously%250Atracking%2520in%2520subsequent%2520frames%2529.%2520To%2520enable%2520efficient%2520inference%252C%2520we%2520design%2520a%250Astreaming%2520KV-cache%2520memory%2520mechanism%2520that%2520constructs%2520a%2520hierarchical%2520memory%250Astructure%2520for%2520selective%2520recall%2520of%2520relevant%2520tokens%252C%2520enabling%2520efficient%2520semantic%250Aretrieval%2520while%2520reducing%2520the%2520overhead%2520of%2520storing%2520all%2520tokens%2520in%2520the%2520traditional%250AKV-cache.%2520Extensive%2520experiments%2520on%2520streaming%2520and%2520long%2520video%2520understanding%2520tasks%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520methods%2520in%2520response%2520accuracy%250Aand%2520real-time%2520efficiency%252C%2520highlighting%2520its%2520practical%2520value%2520for%2520real-world%250Astreaming%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01875v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamAgent%3A%20Towards%20Anticipatory%20Agents%20for%20Streaming%20Video%0A%20%20Understanding&entry.906535625=Haolin%20Yang%20and%20Feilong%20Tang%20and%20Lingxiao%20Zhao%20and%20Xiang%20An%20and%20Ming%20Hu%20and%20Huifa%20Li%20and%20Xinlin%20Zhuang%20and%20Yifan%20Lu%20and%20Xiaofeng%20Zhang%20and%20Abdalla%20Swikir%20and%20Junjun%20He%20and%20Zongyuan%20Ge%20and%20Imran%20Razzak&entry.1292438233=%20%20Real-time%20streaming%20video%20understanding%20in%20domains%20such%20as%20autonomous%20driving%0Aand%20intelligent%20surveillance%20poses%20challenges%20beyond%20conventional%20offline%20video%0Aprocessing%2C%20requiring%20continuous%20perception%2C%20proactive%20decision%20making%2C%20and%0Aresponsive%20interaction%20based%20on%20dynamically%20evolving%20visual%20content.%20However%2C%0Aexisting%20methods%20rely%20on%20alternating%20perception-reaction%20or%20asynchronous%0Atriggers%2C%20lacking%20task-driven%20planning%20and%20future%20anticipation%2C%20which%20limits%0Atheir%20real-time%20responsiveness%20and%20proactive%20decision%20making%20in%20evolving%20video%0Astreams.%20To%20this%20end%2C%20we%20propose%20a%20StreamAgent%20that%20anticipates%20the%20temporal%0Aintervals%20and%20spatial%20regions%20expected%20to%20contain%20future%20task-relevant%0Ainformation%20to%20enable%20proactive%20and%20goal-driven%20responses.%20Specifically%2C%20we%0Aintegrate%20question%20semantics%20and%20historical%20observations%20through%20prompting%20the%0Aanticipatory%20agent%20to%20anticipate%20the%20temporal%20progression%20of%20key%20events%2C%20align%0Acurrent%20observations%20with%20the%20expected%20future%20evidence%2C%20and%20subsequently%20adjust%0Athe%20perception%20action%20%28e.g.%2C%20attending%20to%20task-relevant%20regions%20or%20continuously%0Atracking%20in%20subsequent%20frames%29.%20To%20enable%20efficient%20inference%2C%20we%20design%20a%0Astreaming%20KV-cache%20memory%20mechanism%20that%20constructs%20a%20hierarchical%20memory%0Astructure%20for%20selective%20recall%20of%20relevant%20tokens%2C%20enabling%20efficient%20semantic%0Aretrieval%20while%20reducing%20the%20overhead%20of%20storing%20all%20tokens%20in%20the%20traditional%0AKV-cache.%20Extensive%20experiments%20on%20streaming%20and%20long%20video%20understanding%20tasks%0Ademonstrate%20that%20our%20method%20outperforms%20existing%20methods%20in%20response%20accuracy%0Aand%20real-time%20efficiency%2C%20highlighting%20its%20practical%20value%20for%20real-world%0Astreaming%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.01875v3&entry.124074799=Read"},
{"title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents", "author": "Jiateng Liu and Zhenhailong Wang and Xiaojiang Huang and Yingjie Li and Xing Fan and Xiang Li and Chenlei Guo and Ruhi Sarikaya and Heng Ji", "abstract": "  Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata.\n", "link": "http://arxiv.org/abs/2510.11588v1", "date": "2025-10-13", "relevancy": 2.0496, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20and%20Internalizing%20Complex%20Policy%20Documents%20for%20LLM%20Agents&body=Title%3A%20Analyzing%20and%20Internalizing%20Complex%20Policy%20Documents%20for%20LLM%20Agents%0AAuthor%3A%20Jiateng%20Liu%20and%20Zhenhailong%20Wang%20and%20Xiaojiang%20Huang%20and%20Yingjie%20Li%20and%20Xing%20Fan%20and%20Xiang%20Li%20and%20Chenlei%20Guo%20and%20Ruhi%20Sarikaya%20and%20Heng%20Ji%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29-based%20agentic%20systems%20rely%20on%20in-context%20policy%0Adocuments%20encoding%20diverse%20business%20rules.%20As%20requirements%20grow%2C%20these%0Adocuments%20expand%20rapidly%2C%20causing%20high%20computational%20overhead.%20This%20motivates%0Adeveloping%20internalization%20methods%20that%20embed%20policy%20documents%20into%20model%0Apriors%20while%20preserving%20performance.%20Prior%20prompt%20compression%20work%20targets%0Ageneric%20prompts%2C%20but%20agentic%20policy%20documents%20span%20multiple%20complexity%20levels%0Aand%20require%20deeper%20reasoning%2C%20making%20internalization%20harder.%20We%20introduce%0ACC-Gen%2C%20an%20agentic%20benchmark%20generator%20with%20Controllable%20Complexity%20across%20four%0Alevels%2C%20enabling%20systematic%20evaluation%20of%20agents%27%20ability%20to%20handle%20complexity%0Aand%20offering%20a%20unified%20framework%20for%20assessing%20policy%20internalization.%20Our%0Aanalysis%20shows%20that%20complex%20policy%20specifications%20governing%20workflows%20pose%0Amajor%20reasoning%20challenges.%20Supporting%20internalization%20with%20gold%20user%20agent%0Ainteraction%20trajectories%20containing%20chain-of-thought%20%28CoT%29%20annotations%20via%0Asupervised%20fine-tuning%20%28SFT%29%20is%20data-intensive%20and%20degrades%20sharply%20as%20policy%0Acomplexity%20increases.%20To%20mitigate%20data%20and%20reasoning%20burdens%2C%20we%20propose%0ACategory-Aware%20Policy%20Continued%20Pretraining%20%28CAP-CPT%29.%20Our%20automated%20pipeline%0Aparses%20policy%20documents%20to%20extract%20key%20specifications%2C%20grouping%20them%20into%0Afactual%2C%20behavioral%2C%20and%20conditional%20categories%2C%20and%20isolating%20complex%0Aconditions%20that%20drive%20workflow%20complexity.%20This%20guides%20targeted%20data%20synthesis%0Aand%20enables%20agents%20to%20internalize%20policy%20information%20through%20an%20autoregressive%0Apretraining%20loss.%20Experiments%20show%20CAP-CPT%20improves%20SFT%20baselines%20in%20all%0Asettings%2C%20with%20up%20to%2041%25%20and%2022%25%20gains%20on%20Qwen-3-32B%2C%20achieving%2097.3%25%20prompt%0Alength%20reduction%20on%20CC-Gen%20and%20further%20enhancing%20tau-Bench%20with%20minimal%20SFT%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520and%2520Internalizing%2520Complex%2520Policy%2520Documents%2520for%2520LLM%2520Agents%26entry.906535625%3DJiateng%2520Liu%2520and%2520Zhenhailong%2520Wang%2520and%2520Xiaojiang%2520Huang%2520and%2520Yingjie%2520Li%2520and%2520Xing%2520Fan%2520and%2520Xiang%2520Li%2520and%2520Chenlei%2520Guo%2520and%2520Ruhi%2520Sarikaya%2520and%2520Heng%2520Ji%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520agentic%2520systems%2520rely%2520on%2520in-context%2520policy%250Adocuments%2520encoding%2520diverse%2520business%2520rules.%2520As%2520requirements%2520grow%252C%2520these%250Adocuments%2520expand%2520rapidly%252C%2520causing%2520high%2520computational%2520overhead.%2520This%2520motivates%250Adeveloping%2520internalization%2520methods%2520that%2520embed%2520policy%2520documents%2520into%2520model%250Apriors%2520while%2520preserving%2520performance.%2520Prior%2520prompt%2520compression%2520work%2520targets%250Ageneric%2520prompts%252C%2520but%2520agentic%2520policy%2520documents%2520span%2520multiple%2520complexity%2520levels%250Aand%2520require%2520deeper%2520reasoning%252C%2520making%2520internalization%2520harder.%2520We%2520introduce%250ACC-Gen%252C%2520an%2520agentic%2520benchmark%2520generator%2520with%2520Controllable%2520Complexity%2520across%2520four%250Alevels%252C%2520enabling%2520systematic%2520evaluation%2520of%2520agents%2527%2520ability%2520to%2520handle%2520complexity%250Aand%2520offering%2520a%2520unified%2520framework%2520for%2520assessing%2520policy%2520internalization.%2520Our%250Aanalysis%2520shows%2520that%2520complex%2520policy%2520specifications%2520governing%2520workflows%2520pose%250Amajor%2520reasoning%2520challenges.%2520Supporting%2520internalization%2520with%2520gold%2520user%2520agent%250Ainteraction%2520trajectories%2520containing%2520chain-of-thought%2520%2528CoT%2529%2520annotations%2520via%250Asupervised%2520fine-tuning%2520%2528SFT%2529%2520is%2520data-intensive%2520and%2520degrades%2520sharply%2520as%2520policy%250Acomplexity%2520increases.%2520To%2520mitigate%2520data%2520and%2520reasoning%2520burdens%252C%2520we%2520propose%250ACategory-Aware%2520Policy%2520Continued%2520Pretraining%2520%2528CAP-CPT%2529.%2520Our%2520automated%2520pipeline%250Aparses%2520policy%2520documents%2520to%2520extract%2520key%2520specifications%252C%2520grouping%2520them%2520into%250Afactual%252C%2520behavioral%252C%2520and%2520conditional%2520categories%252C%2520and%2520isolating%2520complex%250Aconditions%2520that%2520drive%2520workflow%2520complexity.%2520This%2520guides%2520targeted%2520data%2520synthesis%250Aand%2520enables%2520agents%2520to%2520internalize%2520policy%2520information%2520through%2520an%2520autoregressive%250Apretraining%2520loss.%2520Experiments%2520show%2520CAP-CPT%2520improves%2520SFT%2520baselines%2520in%2520all%250Asettings%252C%2520with%2520up%2520to%252041%2525%2520and%252022%2525%2520gains%2520on%2520Qwen-3-32B%252C%2520achieving%252097.3%2525%2520prompt%250Alength%2520reduction%2520on%2520CC-Gen%2520and%2520further%2520enhancing%2520tau-Bench%2520with%2520minimal%2520SFT%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20and%20Internalizing%20Complex%20Policy%20Documents%20for%20LLM%20Agents&entry.906535625=Jiateng%20Liu%20and%20Zhenhailong%20Wang%20and%20Xiaojiang%20Huang%20and%20Yingjie%20Li%20and%20Xing%20Fan%20and%20Xiang%20Li%20and%20Chenlei%20Guo%20and%20Ruhi%20Sarikaya%20and%20Heng%20Ji&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29-based%20agentic%20systems%20rely%20on%20in-context%20policy%0Adocuments%20encoding%20diverse%20business%20rules.%20As%20requirements%20grow%2C%20these%0Adocuments%20expand%20rapidly%2C%20causing%20high%20computational%20overhead.%20This%20motivates%0Adeveloping%20internalization%20methods%20that%20embed%20policy%20documents%20into%20model%0Apriors%20while%20preserving%20performance.%20Prior%20prompt%20compression%20work%20targets%0Ageneric%20prompts%2C%20but%20agentic%20policy%20documents%20span%20multiple%20complexity%20levels%0Aand%20require%20deeper%20reasoning%2C%20making%20internalization%20harder.%20We%20introduce%0ACC-Gen%2C%20an%20agentic%20benchmark%20generator%20with%20Controllable%20Complexity%20across%20four%0Alevels%2C%20enabling%20systematic%20evaluation%20of%20agents%27%20ability%20to%20handle%20complexity%0Aand%20offering%20a%20unified%20framework%20for%20assessing%20policy%20internalization.%20Our%0Aanalysis%20shows%20that%20complex%20policy%20specifications%20governing%20workflows%20pose%0Amajor%20reasoning%20challenges.%20Supporting%20internalization%20with%20gold%20user%20agent%0Ainteraction%20trajectories%20containing%20chain-of-thought%20%28CoT%29%20annotations%20via%0Asupervised%20fine-tuning%20%28SFT%29%20is%20data-intensive%20and%20degrades%20sharply%20as%20policy%0Acomplexity%20increases.%20To%20mitigate%20data%20and%20reasoning%20burdens%2C%20we%20propose%0ACategory-Aware%20Policy%20Continued%20Pretraining%20%28CAP-CPT%29.%20Our%20automated%20pipeline%0Aparses%20policy%20documents%20to%20extract%20key%20specifications%2C%20grouping%20them%20into%0Afactual%2C%20behavioral%2C%20and%20conditional%20categories%2C%20and%20isolating%20complex%0Aconditions%20that%20drive%20workflow%20complexity.%20This%20guides%20targeted%20data%20synthesis%0Aand%20enables%20agents%20to%20internalize%20policy%20information%20through%20an%20autoregressive%0Apretraining%20loss.%20Experiments%20show%20CAP-CPT%20improves%20SFT%20baselines%20in%20all%0Asettings%2C%20with%20up%20to%2041%25%20and%2022%25%20gains%20on%20Qwen-3-32B%2C%20achieving%2097.3%25%20prompt%0Alength%20reduction%20on%20CC-Gen%20and%20further%20enhancing%20tau-Bench%20with%20minimal%20SFT%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11588v1&entry.124074799=Read"},
{"title": "FACE: Faithful Automatic Concept Extraction", "author": "Dipkamal Bhusal and Michael Clifford and Sara Rampazzi and Nidhi Rastogi", "abstract": "  Interpreting deep neural networks through concept-based explanations offers a\nbridge between low-level features and high-level human-understandable\nsemantics. However, existing automatic concept discovery methods often fail to\nalign these extracted concepts with the model's true decision-making process,\nthereby compromising explanation faithfulness. In this work, we propose FACE\n(Faithful Automatic Concept Extraction), a novel framework that augments\nNon-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence\nregularization term to ensure alignment between the model's original and\nconcept-based predictions. Unlike prior methods that operate solely on encoder\nactivations, FACE incorporates classifier supervision during concept learning,\nenforcing predictive consistency and enabling faithful explanations. We provide\ntheoretical guarantees showing that minimizing the KL divergence bounds the\ndeviation in predictive distributions, thereby promoting faithful local\nlinearity in the learned concept space. Systematic evaluations on ImageNet,\nCOCO, and CelebA datasets demonstrate that FACE outperforms existing methods\nacross faithfulness and sparsity metrics.\n", "link": "http://arxiv.org/abs/2510.11675v1", "date": "2025-10-13", "relevancy": 2.0313, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.51}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.51}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FACE%3A%20Faithful%20Automatic%20Concept%20Extraction&body=Title%3A%20FACE%3A%20Faithful%20Automatic%20Concept%20Extraction%0AAuthor%3A%20Dipkamal%20Bhusal%20and%20Michael%20Clifford%20and%20Sara%20Rampazzi%20and%20Nidhi%20Rastogi%0AAbstract%3A%20%20%20Interpreting%20deep%20neural%20networks%20through%20concept-based%20explanations%20offers%20a%0Abridge%20between%20low-level%20features%20and%20high-level%20human-understandable%0Asemantics.%20However%2C%20existing%20automatic%20concept%20discovery%20methods%20often%20fail%20to%0Aalign%20these%20extracted%20concepts%20with%20the%20model%27s%20true%20decision-making%20process%2C%0Athereby%20compromising%20explanation%20faithfulness.%20In%20this%20work%2C%20we%20propose%20FACE%0A%28Faithful%20Automatic%20Concept%20Extraction%29%2C%20a%20novel%20framework%20that%20augments%0ANon-negative%20Matrix%20Factorization%20%28NMF%29%20with%20a%20Kullback-Leibler%20%28KL%29%20divergence%0Aregularization%20term%20to%20ensure%20alignment%20between%20the%20model%27s%20original%20and%0Aconcept-based%20predictions.%20Unlike%20prior%20methods%20that%20operate%20solely%20on%20encoder%0Aactivations%2C%20FACE%20incorporates%20classifier%20supervision%20during%20concept%20learning%2C%0Aenforcing%20predictive%20consistency%20and%20enabling%20faithful%20explanations.%20We%20provide%0Atheoretical%20guarantees%20showing%20that%20minimizing%20the%20KL%20divergence%20bounds%20the%0Adeviation%20in%20predictive%20distributions%2C%20thereby%20promoting%20faithful%20local%0Alinearity%20in%20the%20learned%20concept%20space.%20Systematic%20evaluations%20on%20ImageNet%2C%0ACOCO%2C%20and%20CelebA%20datasets%20demonstrate%20that%20FACE%20outperforms%20existing%20methods%0Aacross%20faithfulness%20and%20sparsity%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFACE%253A%2520Faithful%2520Automatic%2520Concept%2520Extraction%26entry.906535625%3DDipkamal%2520Bhusal%2520and%2520Michael%2520Clifford%2520and%2520Sara%2520Rampazzi%2520and%2520Nidhi%2520Rastogi%26entry.1292438233%3D%2520%2520Interpreting%2520deep%2520neural%2520networks%2520through%2520concept-based%2520explanations%2520offers%2520a%250Abridge%2520between%2520low-level%2520features%2520and%2520high-level%2520human-understandable%250Asemantics.%2520However%252C%2520existing%2520automatic%2520concept%2520discovery%2520methods%2520often%2520fail%2520to%250Aalign%2520these%2520extracted%2520concepts%2520with%2520the%2520model%2527s%2520true%2520decision-making%2520process%252C%250Athereby%2520compromising%2520explanation%2520faithfulness.%2520In%2520this%2520work%252C%2520we%2520propose%2520FACE%250A%2528Faithful%2520Automatic%2520Concept%2520Extraction%2529%252C%2520a%2520novel%2520framework%2520that%2520augments%250ANon-negative%2520Matrix%2520Factorization%2520%2528NMF%2529%2520with%2520a%2520Kullback-Leibler%2520%2528KL%2529%2520divergence%250Aregularization%2520term%2520to%2520ensure%2520alignment%2520between%2520the%2520model%2527s%2520original%2520and%250Aconcept-based%2520predictions.%2520Unlike%2520prior%2520methods%2520that%2520operate%2520solely%2520on%2520encoder%250Aactivations%252C%2520FACE%2520incorporates%2520classifier%2520supervision%2520during%2520concept%2520learning%252C%250Aenforcing%2520predictive%2520consistency%2520and%2520enabling%2520faithful%2520explanations.%2520We%2520provide%250Atheoretical%2520guarantees%2520showing%2520that%2520minimizing%2520the%2520KL%2520divergence%2520bounds%2520the%250Adeviation%2520in%2520predictive%2520distributions%252C%2520thereby%2520promoting%2520faithful%2520local%250Alinearity%2520in%2520the%2520learned%2520concept%2520space.%2520Systematic%2520evaluations%2520on%2520ImageNet%252C%250ACOCO%252C%2520and%2520CelebA%2520datasets%2520demonstrate%2520that%2520FACE%2520outperforms%2520existing%2520methods%250Aacross%2520faithfulness%2520and%2520sparsity%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FACE%3A%20Faithful%20Automatic%20Concept%20Extraction&entry.906535625=Dipkamal%20Bhusal%20and%20Michael%20Clifford%20and%20Sara%20Rampazzi%20and%20Nidhi%20Rastogi&entry.1292438233=%20%20Interpreting%20deep%20neural%20networks%20through%20concept-based%20explanations%20offers%20a%0Abridge%20between%20low-level%20features%20and%20high-level%20human-understandable%0Asemantics.%20However%2C%20existing%20automatic%20concept%20discovery%20methods%20often%20fail%20to%0Aalign%20these%20extracted%20concepts%20with%20the%20model%27s%20true%20decision-making%20process%2C%0Athereby%20compromising%20explanation%20faithfulness.%20In%20this%20work%2C%20we%20propose%20FACE%0A%28Faithful%20Automatic%20Concept%20Extraction%29%2C%20a%20novel%20framework%20that%20augments%0ANon-negative%20Matrix%20Factorization%20%28NMF%29%20with%20a%20Kullback-Leibler%20%28KL%29%20divergence%0Aregularization%20term%20to%20ensure%20alignment%20between%20the%20model%27s%20original%20and%0Aconcept-based%20predictions.%20Unlike%20prior%20methods%20that%20operate%20solely%20on%20encoder%0Aactivations%2C%20FACE%20incorporates%20classifier%20supervision%20during%20concept%20learning%2C%0Aenforcing%20predictive%20consistency%20and%20enabling%20faithful%20explanations.%20We%20provide%0Atheoretical%20guarantees%20showing%20that%20minimizing%20the%20KL%20divergence%20bounds%20the%0Adeviation%20in%20predictive%20distributions%2C%20thereby%20promoting%20faithful%20local%0Alinearity%20in%20the%20learned%20concept%20space.%20Systematic%20evaluations%20on%20ImageNet%2C%0ACOCO%2C%20and%20CelebA%20datasets%20demonstrate%20that%20FACE%20outperforms%20existing%20methods%0Aacross%20faithfulness%20and%20sparsity%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11675v1&entry.124074799=Read"},
{"title": "Smooth Spatiotemporal Tube Synthesis for Prescribed-Time\n  Reach-Avoid-Stay Control", "author": "Siddhartha Upadhyay and Ratnangshu Das and Pushpak Jagtap", "abstract": "  In this work, we address the issue of controller synthesis for a\ncontrol-affine nonlinear system to meet prescribed time reach-avoid-stay\nspecifications. Our goal is to improve upon previous methods based on\nspatiotemporal tubes (STTs) by eliminating the need for circumvent functions,\nwhich often lead to abrupt tube modifications and high control effort. We\npropose an adaptive framework that constructs smooth STTs around static unsafe\nsets, enabling continuous avoidance while guiding the system toward the target\nwithin the prescribed time. A closed-form, approximation-free control law is\nderived to ensure the system trajectory remains within the tube and satisfies\nthe RAS task. The effectiveness of the proposed approach is demonstrated\nthrough a case study, showing a significant reduction in control effort\ncompared to prior methods.\n", "link": "http://arxiv.org/abs/2510.11583v1", "date": "2025-10-13", "relevancy": 2.0137, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5167}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.506}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smooth%20Spatiotemporal%20Tube%20Synthesis%20for%20Prescribed-Time%0A%20%20Reach-Avoid-Stay%20Control&body=Title%3A%20Smooth%20Spatiotemporal%20Tube%20Synthesis%20for%20Prescribed-Time%0A%20%20Reach-Avoid-Stay%20Control%0AAuthor%3A%20Siddhartha%20Upadhyay%20and%20Ratnangshu%20Das%20and%20Pushpak%20Jagtap%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20the%20issue%20of%20controller%20synthesis%20for%20a%0Acontrol-affine%20nonlinear%20system%20to%20meet%20prescribed%20time%20reach-avoid-stay%0Aspecifications.%20Our%20goal%20is%20to%20improve%20upon%20previous%20methods%20based%20on%0Aspatiotemporal%20tubes%20%28STTs%29%20by%20eliminating%20the%20need%20for%20circumvent%20functions%2C%0Awhich%20often%20lead%20to%20abrupt%20tube%20modifications%20and%20high%20control%20effort.%20We%0Apropose%20an%20adaptive%20framework%20that%20constructs%20smooth%20STTs%20around%20static%20unsafe%0Asets%2C%20enabling%20continuous%20avoidance%20while%20guiding%20the%20system%20toward%20the%20target%0Awithin%20the%20prescribed%20time.%20A%20closed-form%2C%20approximation-free%20control%20law%20is%0Aderived%20to%20ensure%20the%20system%20trajectory%20remains%20within%20the%20tube%20and%20satisfies%0Athe%20RAS%20task.%20The%20effectiveness%20of%20the%20proposed%20approach%20is%20demonstrated%0Athrough%20a%20case%20study%2C%20showing%20a%20significant%20reduction%20in%20control%20effort%0Acompared%20to%20prior%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmooth%2520Spatiotemporal%2520Tube%2520Synthesis%2520for%2520Prescribed-Time%250A%2520%2520Reach-Avoid-Stay%2520Control%26entry.906535625%3DSiddhartha%2520Upadhyay%2520and%2520Ratnangshu%2520Das%2520and%2520Pushpak%2520Jagtap%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520issue%2520of%2520controller%2520synthesis%2520for%2520a%250Acontrol-affine%2520nonlinear%2520system%2520to%2520meet%2520prescribed%2520time%2520reach-avoid-stay%250Aspecifications.%2520Our%2520goal%2520is%2520to%2520improve%2520upon%2520previous%2520methods%2520based%2520on%250Aspatiotemporal%2520tubes%2520%2528STTs%2529%2520by%2520eliminating%2520the%2520need%2520for%2520circumvent%2520functions%252C%250Awhich%2520often%2520lead%2520to%2520abrupt%2520tube%2520modifications%2520and%2520high%2520control%2520effort.%2520We%250Apropose%2520an%2520adaptive%2520framework%2520that%2520constructs%2520smooth%2520STTs%2520around%2520static%2520unsafe%250Asets%252C%2520enabling%2520continuous%2520avoidance%2520while%2520guiding%2520the%2520system%2520toward%2520the%2520target%250Awithin%2520the%2520prescribed%2520time.%2520A%2520closed-form%252C%2520approximation-free%2520control%2520law%2520is%250Aderived%2520to%2520ensure%2520the%2520system%2520trajectory%2520remains%2520within%2520the%2520tube%2520and%2520satisfies%250Athe%2520RAS%2520task.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520is%2520demonstrated%250Athrough%2520a%2520case%2520study%252C%2520showing%2520a%2520significant%2520reduction%2520in%2520control%2520effort%250Acompared%2520to%2520prior%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smooth%20Spatiotemporal%20Tube%20Synthesis%20for%20Prescribed-Time%0A%20%20Reach-Avoid-Stay%20Control&entry.906535625=Siddhartha%20Upadhyay%20and%20Ratnangshu%20Das%20and%20Pushpak%20Jagtap&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20the%20issue%20of%20controller%20synthesis%20for%20a%0Acontrol-affine%20nonlinear%20system%20to%20meet%20prescribed%20time%20reach-avoid-stay%0Aspecifications.%20Our%20goal%20is%20to%20improve%20upon%20previous%20methods%20based%20on%0Aspatiotemporal%20tubes%20%28STTs%29%20by%20eliminating%20the%20need%20for%20circumvent%20functions%2C%0Awhich%20often%20lead%20to%20abrupt%20tube%20modifications%20and%20high%20control%20effort.%20We%0Apropose%20an%20adaptive%20framework%20that%20constructs%20smooth%20STTs%20around%20static%20unsafe%0Asets%2C%20enabling%20continuous%20avoidance%20while%20guiding%20the%20system%20toward%20the%20target%0Awithin%20the%20prescribed%20time.%20A%20closed-form%2C%20approximation-free%20control%20law%20is%0Aderived%20to%20ensure%20the%20system%20trajectory%20remains%20within%20the%20tube%20and%20satisfies%0Athe%20RAS%20task.%20The%20effectiveness%20of%20the%20proposed%20approach%20is%20demonstrated%0Athrough%20a%20case%20study%2C%20showing%20a%20significant%20reduction%20in%20control%20effort%0Acompared%20to%20prior%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11583v1&entry.124074799=Read"},
{"title": "An approach for systematic decomposition of complex llm tasks", "author": "Tianle Zhou and Jiakai Xu and Guanhong Liu and Jiaxiang Liu and Haonan Wang and Eugene Wu", "abstract": "  Large Language Models (LLMs) suffer from reliability issues on complex tasks,\nas existing decomposition methods are heuristic and rely on agent or manual\ndecomposition. This work introduces a novel, systematic decomposition framework\nthat we call Analysis of CONstraint-Induced Complexity (ACONIC), which models\nthe task as a constraint problem and leveraging formal complexity measures to\nguide decomposition. On combinatorial (SATBench) and LLM database querying\ntasks (Spider), we find that by decomposing the tasks following the measure of\ncomplexity, agent can perform considerably better (10-40 percentage point).\n", "link": "http://arxiv.org/abs/2510.07772v2", "date": "2025-10-13", "relevancy": 1.9969, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4999}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20approach%20for%20systematic%20decomposition%20of%20complex%20llm%20tasks&body=Title%3A%20An%20approach%20for%20systematic%20decomposition%20of%20complex%20llm%20tasks%0AAuthor%3A%20Tianle%20Zhou%20and%20Jiakai%20Xu%20and%20Guanhong%20Liu%20and%20Jiaxiang%20Liu%20and%20Haonan%20Wang%20and%20Eugene%20Wu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20suffer%20from%20reliability%20issues%20on%20complex%20tasks%2C%0Aas%20existing%20decomposition%20methods%20are%20heuristic%20and%20rely%20on%20agent%20or%20manual%0Adecomposition.%20This%20work%20introduces%20a%20novel%2C%20systematic%20decomposition%20framework%0Athat%20we%20call%20Analysis%20of%20CONstraint-Induced%20Complexity%20%28ACONIC%29%2C%20which%20models%0Athe%20task%20as%20a%20constraint%20problem%20and%20leveraging%20formal%20complexity%20measures%20to%0Aguide%20decomposition.%20On%20combinatorial%20%28SATBench%29%20and%20LLM%20database%20querying%0Atasks%20%28Spider%29%2C%20we%20find%20that%20by%20decomposing%20the%20tasks%20following%20the%20measure%20of%0Acomplexity%2C%20agent%20can%20perform%20considerably%20better%20%2810-40%20percentage%20point%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07772v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520approach%2520for%2520systematic%2520decomposition%2520of%2520complex%2520llm%2520tasks%26entry.906535625%3DTianle%2520Zhou%2520and%2520Jiakai%2520Xu%2520and%2520Guanhong%2520Liu%2520and%2520Jiaxiang%2520Liu%2520and%2520Haonan%2520Wang%2520and%2520Eugene%2520Wu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520suffer%2520from%2520reliability%2520issues%2520on%2520complex%2520tasks%252C%250Aas%2520existing%2520decomposition%2520methods%2520are%2520heuristic%2520and%2520rely%2520on%2520agent%2520or%2520manual%250Adecomposition.%2520This%2520work%2520introduces%2520a%2520novel%252C%2520systematic%2520decomposition%2520framework%250Athat%2520we%2520call%2520Analysis%2520of%2520CONstraint-Induced%2520Complexity%2520%2528ACONIC%2529%252C%2520which%2520models%250Athe%2520task%2520as%2520a%2520constraint%2520problem%2520and%2520leveraging%2520formal%2520complexity%2520measures%2520to%250Aguide%2520decomposition.%2520On%2520combinatorial%2520%2528SATBench%2529%2520and%2520LLM%2520database%2520querying%250Atasks%2520%2528Spider%2529%252C%2520we%2520find%2520that%2520by%2520decomposing%2520the%2520tasks%2520following%2520the%2520measure%2520of%250Acomplexity%252C%2520agent%2520can%2520perform%2520considerably%2520better%2520%252810-40%2520percentage%2520point%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07772v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20approach%20for%20systematic%20decomposition%20of%20complex%20llm%20tasks&entry.906535625=Tianle%20Zhou%20and%20Jiakai%20Xu%20and%20Guanhong%20Liu%20and%20Jiaxiang%20Liu%20and%20Haonan%20Wang%20and%20Eugene%20Wu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20suffer%20from%20reliability%20issues%20on%20complex%20tasks%2C%0Aas%20existing%20decomposition%20methods%20are%20heuristic%20and%20rely%20on%20agent%20or%20manual%0Adecomposition.%20This%20work%20introduces%20a%20novel%2C%20systematic%20decomposition%20framework%0Athat%20we%20call%20Analysis%20of%20CONstraint-Induced%20Complexity%20%28ACONIC%29%2C%20which%20models%0Athe%20task%20as%20a%20constraint%20problem%20and%20leveraging%20formal%20complexity%20measures%20to%0Aguide%20decomposition.%20On%20combinatorial%20%28SATBench%29%20and%20LLM%20database%20querying%0Atasks%20%28Spider%29%2C%20we%20find%20that%20by%20decomposing%20the%20tasks%20following%20the%20measure%20of%0Acomplexity%2C%20agent%20can%20perform%20considerably%20better%20%2810-40%20percentage%20point%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07772v2&entry.124074799=Read"},
{"title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation\n  Capabilities", "author": "Zicheng Liu and Lige Huang and Jie Zhang and Dongrui Liu and Yuan Tian and Jing Shao", "abstract": "  The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models.\n", "link": "http://arxiv.org/abs/2510.11688v1", "date": "2025-10-13", "relevancy": 1.989, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4988}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4988}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PACEbench%3A%20A%20Framework%20for%20Evaluating%20Practical%20AI%20Cyber-Exploitation%0A%20%20Capabilities&body=Title%3A%20PACEbench%3A%20A%20Framework%20for%20Evaluating%20Practical%20AI%20Cyber-Exploitation%0A%20%20Capabilities%0AAuthor%3A%20Zicheng%20Liu%20and%20Lige%20Huang%20and%20Jie%20Zhang%20and%20Dongrui%20Liu%20and%20Yuan%20Tian%20and%20Jing%20Shao%0AAbstract%3A%20%20%20The%20increasing%20autonomy%20of%20Large%20Language%20Models%20%28LLMs%29%20necessitates%20a%0Arigorous%20evaluation%20of%20their%20potential%20to%20aid%20in%20cyber%20offense.%20Existing%0Abenchmarks%20often%20lack%20real-world%20complexity%20and%20are%20thus%20unable%20to%20accurately%0Aassess%20LLMs%27%20cybersecurity%20capabilities.%20To%20address%20this%20gap%2C%20we%20introduce%0APACEbench%2C%20a%20practical%20AI%20cyber-exploitation%20benchmark%20built%20on%20the%20principles%0Aof%20realistic%20vulnerability%20difficulty%2C%20environmental%20complexity%2C%20and%20cyber%0Adefenses.%20Specifically%2C%20PACEbench%20comprises%20four%20scenarios%20spanning%20single%2C%0Ablended%2C%20chained%2C%20and%20defense%20vulnerability%20exploitations.%20To%20handle%20these%0Acomplex%20challenges%2C%20we%20propose%20PACEagent%2C%20a%20novel%20agent%20that%20emulates%20human%0Apenetration%20testers%20by%20supporting%20multi-phase%20reconnaissance%2C%20analysis%2C%20and%0Aexploitation.%20Extensive%20experiments%20with%20seven%20frontier%20LLMs%20demonstrate%20that%0Acurrent%20models%20struggle%20with%20complex%20cyber%20scenarios%2C%20and%20none%20can%20bypass%0Adefenses.%20These%20findings%20suggest%20that%20current%20models%20do%20not%20yet%20pose%20a%0Ageneralized%20cyber%20offense%20threat.%20Nonetheless%2C%20our%20work%20provides%20a%20robust%0Abenchmark%20to%20guide%20the%20trustworthy%20development%20of%20future%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPACEbench%253A%2520A%2520Framework%2520for%2520Evaluating%2520Practical%2520AI%2520Cyber-Exploitation%250A%2520%2520Capabilities%26entry.906535625%3DZicheng%2520Liu%2520and%2520Lige%2520Huang%2520and%2520Jie%2520Zhang%2520and%2520Dongrui%2520Liu%2520and%2520Yuan%2520Tian%2520and%2520Jing%2520Shao%26entry.1292438233%3D%2520%2520The%2520increasing%2520autonomy%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520necessitates%2520a%250Arigorous%2520evaluation%2520of%2520their%2520potential%2520to%2520aid%2520in%2520cyber%2520offense.%2520Existing%250Abenchmarks%2520often%2520lack%2520real-world%2520complexity%2520and%2520are%2520thus%2520unable%2520to%2520accurately%250Aassess%2520LLMs%2527%2520cybersecurity%2520capabilities.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250APACEbench%252C%2520a%2520practical%2520AI%2520cyber-exploitation%2520benchmark%2520built%2520on%2520the%2520principles%250Aof%2520realistic%2520vulnerability%2520difficulty%252C%2520environmental%2520complexity%252C%2520and%2520cyber%250Adefenses.%2520Specifically%252C%2520PACEbench%2520comprises%2520four%2520scenarios%2520spanning%2520single%252C%250Ablended%252C%2520chained%252C%2520and%2520defense%2520vulnerability%2520exploitations.%2520To%2520handle%2520these%250Acomplex%2520challenges%252C%2520we%2520propose%2520PACEagent%252C%2520a%2520novel%2520agent%2520that%2520emulates%2520human%250Apenetration%2520testers%2520by%2520supporting%2520multi-phase%2520reconnaissance%252C%2520analysis%252C%2520and%250Aexploitation.%2520Extensive%2520experiments%2520with%2520seven%2520frontier%2520LLMs%2520demonstrate%2520that%250Acurrent%2520models%2520struggle%2520with%2520complex%2520cyber%2520scenarios%252C%2520and%2520none%2520can%2520bypass%250Adefenses.%2520These%2520findings%2520suggest%2520that%2520current%2520models%2520do%2520not%2520yet%2520pose%2520a%250Ageneralized%2520cyber%2520offense%2520threat.%2520Nonetheless%252C%2520our%2520work%2520provides%2520a%2520robust%250Abenchmark%2520to%2520guide%2520the%2520trustworthy%2520development%2520of%2520future%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACEbench%3A%20A%20Framework%20for%20Evaluating%20Practical%20AI%20Cyber-Exploitation%0A%20%20Capabilities&entry.906535625=Zicheng%20Liu%20and%20Lige%20Huang%20and%20Jie%20Zhang%20and%20Dongrui%20Liu%20and%20Yuan%20Tian%20and%20Jing%20Shao&entry.1292438233=%20%20The%20increasing%20autonomy%20of%20Large%20Language%20Models%20%28LLMs%29%20necessitates%20a%0Arigorous%20evaluation%20of%20their%20potential%20to%20aid%20in%20cyber%20offense.%20Existing%0Abenchmarks%20often%20lack%20real-world%20complexity%20and%20are%20thus%20unable%20to%20accurately%0Aassess%20LLMs%27%20cybersecurity%20capabilities.%20To%20address%20this%20gap%2C%20we%20introduce%0APACEbench%2C%20a%20practical%20AI%20cyber-exploitation%20benchmark%20built%20on%20the%20principles%0Aof%20realistic%20vulnerability%20difficulty%2C%20environmental%20complexity%2C%20and%20cyber%0Adefenses.%20Specifically%2C%20PACEbench%20comprises%20four%20scenarios%20spanning%20single%2C%0Ablended%2C%20chained%2C%20and%20defense%20vulnerability%20exploitations.%20To%20handle%20these%0Acomplex%20challenges%2C%20we%20propose%20PACEagent%2C%20a%20novel%20agent%20that%20emulates%20human%0Apenetration%20testers%20by%20supporting%20multi-phase%20reconnaissance%2C%20analysis%2C%20and%0Aexploitation.%20Extensive%20experiments%20with%20seven%20frontier%20LLMs%20demonstrate%20that%0Acurrent%20models%20struggle%20with%20complex%20cyber%20scenarios%2C%20and%20none%20can%20bypass%0Adefenses.%20These%20findings%20suggest%20that%20current%20models%20do%20not%20yet%20pose%20a%0Ageneralized%20cyber%20offense%20threat.%20Nonetheless%2C%20our%20work%20provides%20a%20robust%0Abenchmark%20to%20guide%20the%20trustworthy%20development%20of%20future%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11688v1&entry.124074799=Read"},
{"title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs", "author": "Wei Huang and Yi Ge and Shuai Yang and Yicheng Xiao and Huizi Mao and Yujun Lin and Hanrong Ye and Sifei Liu and Ka Chun Cheung and Hongxu Yin and Yao Lu and Xiaojuan Qi and Song Han and Yukang Chen", "abstract": "  We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.\n", "link": "http://arxiv.org/abs/2510.11696v1", "date": "2025-10-13", "relevancy": 1.9715, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.497}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4904}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QeRL%3A%20Beyond%20Efficiency%20--%20Quantization-enhanced%20Reinforcement%20Learning%0A%20%20for%20LLMs&body=Title%3A%20QeRL%3A%20Beyond%20Efficiency%20--%20Quantization-enhanced%20Reinforcement%20Learning%0A%20%20for%20LLMs%0AAuthor%3A%20Wei%20Huang%20and%20Yi%20Ge%20and%20Shuai%20Yang%20and%20Yicheng%20Xiao%20and%20Huizi%20Mao%20and%20Yujun%20Lin%20and%20Hanrong%20Ye%20and%20Sifei%20Liu%20and%20Ka%20Chun%20Cheung%20and%20Hongxu%20Yin%20and%20Yao%20Lu%20and%20Xiaojuan%20Qi%20and%20Song%20Han%20and%20Yukang%20Chen%0AAbstract%3A%20%20%20We%20propose%20QeRL%2C%20a%20Quantization-enhanced%20Reinforcement%20Learning%20framework%20for%0Alarge%20language%20models%20%28LLMs%29.%20While%20RL%20is%20essential%20for%20LLMs%27%20reasoning%0Acapabilities%2C%20it%20is%20resource-intensive%2C%20requiring%20substantial%20GPU%20memory%20and%0Along%20rollout%20durations.%20QeRL%20addresses%20these%20issues%20by%20combining%20NVFP4%0Aquantization%20with%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20accelerating%20rollout%20phase%20of%20RL%0Awhile%20reducing%20memory%20overhead.%20Beyond%20efficiency%2C%20our%20findings%20show%20that%0Aquantization%20noise%20increases%20policy%20entropy%2C%20enhancing%20exploration%2C%20and%0Aenabling%20the%20discovery%20of%20better%20strategies%20during%20RL.%20To%20further%20optimize%0Aexploration%2C%20QeRL%20introduces%20an%20Adaptive%20Quantization%20Noise%20%28AQN%29%20mechanism%2C%0Awhich%20dynamically%20adjusts%20noise%20during%20training.%20Experiments%20demonstrate%20that%0AQeRL%20delivers%20over%201.5%20times%20speedup%20in%20the%20rollout%20phase.%20Moreover%2C%20this%20is%0Athe%20first%20framework%20to%20enable%20RL%20training%20of%20a%2032B%20LLM%20on%20a%20single%20H100%2080GB%0AGPU%2C%20while%20delivering%20overall%20speedups%20for%20RL%20training.%20It%20also%20achieves%20faster%0Areward%20growth%20and%20higher%20final%20accuracy%20than%2016-bit%20LoRA%20and%20QLoRA%2C%20while%0Amatching%20the%20performance%20of%20full-parameter%20fine-tuning%20on%20mathematical%0Abenchmarks%20such%20as%20GSM8K%20%2890.8%25%29%20and%20MATH%20500%20%2877.4%25%29%20in%20the%207B%20model.%20These%0Aresults%20establish%20QeRL%20as%20an%20efficient%20and%20effective%20framework%20for%20RL%20training%0Ain%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQeRL%253A%2520Beyond%2520Efficiency%2520--%2520Quantization-enhanced%2520Reinforcement%2520Learning%250A%2520%2520for%2520LLMs%26entry.906535625%3DWei%2520Huang%2520and%2520Yi%2520Ge%2520and%2520Shuai%2520Yang%2520and%2520Yicheng%2520Xiao%2520and%2520Huizi%2520Mao%2520and%2520Yujun%2520Lin%2520and%2520Hanrong%2520Ye%2520and%2520Sifei%2520Liu%2520and%2520Ka%2520Chun%2520Cheung%2520and%2520Hongxu%2520Yin%2520and%2520Yao%2520Lu%2520and%2520Xiaojuan%2520Qi%2520and%2520Song%2520Han%2520and%2520Yukang%2520Chen%26entry.1292438233%3D%2520%2520We%2520propose%2520QeRL%252C%2520a%2520Quantization-enhanced%2520Reinforcement%2520Learning%2520framework%2520for%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520While%2520RL%2520is%2520essential%2520for%2520LLMs%2527%2520reasoning%250Acapabilities%252C%2520it%2520is%2520resource-intensive%252C%2520requiring%2520substantial%2520GPU%2520memory%2520and%250Along%2520rollout%2520durations.%2520QeRL%2520addresses%2520these%2520issues%2520by%2520combining%2520NVFP4%250Aquantization%2520with%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%252C%2520accelerating%2520rollout%2520phase%2520of%2520RL%250Awhile%2520reducing%2520memory%2520overhead.%2520Beyond%2520efficiency%252C%2520our%2520findings%2520show%2520that%250Aquantization%2520noise%2520increases%2520policy%2520entropy%252C%2520enhancing%2520exploration%252C%2520and%250Aenabling%2520the%2520discovery%2520of%2520better%2520strategies%2520during%2520RL.%2520To%2520further%2520optimize%250Aexploration%252C%2520QeRL%2520introduces%2520an%2520Adaptive%2520Quantization%2520Noise%2520%2528AQN%2529%2520mechanism%252C%250Awhich%2520dynamically%2520adjusts%2520noise%2520during%2520training.%2520Experiments%2520demonstrate%2520that%250AQeRL%2520delivers%2520over%25201.5%2520times%2520speedup%2520in%2520the%2520rollout%2520phase.%2520Moreover%252C%2520this%2520is%250Athe%2520first%2520framework%2520to%2520enable%2520RL%2520training%2520of%2520a%252032B%2520LLM%2520on%2520a%2520single%2520H100%252080GB%250AGPU%252C%2520while%2520delivering%2520overall%2520speedups%2520for%2520RL%2520training.%2520It%2520also%2520achieves%2520faster%250Areward%2520growth%2520and%2520higher%2520final%2520accuracy%2520than%252016-bit%2520LoRA%2520and%2520QLoRA%252C%2520while%250Amatching%2520the%2520performance%2520of%2520full-parameter%2520fine-tuning%2520on%2520mathematical%250Abenchmarks%2520such%2520as%2520GSM8K%2520%252890.8%2525%2529%2520and%2520MATH%2520500%2520%252877.4%2525%2529%2520in%2520the%25207B%2520model.%2520These%250Aresults%2520establish%2520QeRL%2520as%2520an%2520efficient%2520and%2520effective%2520framework%2520for%2520RL%2520training%250Ain%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QeRL%3A%20Beyond%20Efficiency%20--%20Quantization-enhanced%20Reinforcement%20Learning%0A%20%20for%20LLMs&entry.906535625=Wei%20Huang%20and%20Yi%20Ge%20and%20Shuai%20Yang%20and%20Yicheng%20Xiao%20and%20Huizi%20Mao%20and%20Yujun%20Lin%20and%20Hanrong%20Ye%20and%20Sifei%20Liu%20and%20Ka%20Chun%20Cheung%20and%20Hongxu%20Yin%20and%20Yao%20Lu%20and%20Xiaojuan%20Qi%20and%20Song%20Han%20and%20Yukang%20Chen&entry.1292438233=%20%20We%20propose%20QeRL%2C%20a%20Quantization-enhanced%20Reinforcement%20Learning%20framework%20for%0Alarge%20language%20models%20%28LLMs%29.%20While%20RL%20is%20essential%20for%20LLMs%27%20reasoning%0Acapabilities%2C%20it%20is%20resource-intensive%2C%20requiring%20substantial%20GPU%20memory%20and%0Along%20rollout%20durations.%20QeRL%20addresses%20these%20issues%20by%20combining%20NVFP4%0Aquantization%20with%20Low-Rank%20Adaptation%20%28LoRA%29%2C%20accelerating%20rollout%20phase%20of%20RL%0Awhile%20reducing%20memory%20overhead.%20Beyond%20efficiency%2C%20our%20findings%20show%20that%0Aquantization%20noise%20increases%20policy%20entropy%2C%20enhancing%20exploration%2C%20and%0Aenabling%20the%20discovery%20of%20better%20strategies%20during%20RL.%20To%20further%20optimize%0Aexploration%2C%20QeRL%20introduces%20an%20Adaptive%20Quantization%20Noise%20%28AQN%29%20mechanism%2C%0Awhich%20dynamically%20adjusts%20noise%20during%20training.%20Experiments%20demonstrate%20that%0AQeRL%20delivers%20over%201.5%20times%20speedup%20in%20the%20rollout%20phase.%20Moreover%2C%20this%20is%0Athe%20first%20framework%20to%20enable%20RL%20training%20of%20a%2032B%20LLM%20on%20a%20single%20H100%2080GB%0AGPU%2C%20while%20delivering%20overall%20speedups%20for%20RL%20training.%20It%20also%20achieves%20faster%0Areward%20growth%20and%20higher%20final%20accuracy%20than%2016-bit%20LoRA%20and%20QLoRA%2C%20while%0Amatching%20the%20performance%20of%20full-parameter%20fine-tuning%20on%20mathematical%0Abenchmarks%20such%20as%20GSM8K%20%2890.8%25%29%20and%20MATH%20500%20%2877.4%25%29%20in%20the%207B%20model.%20These%0Aresults%20establish%20QeRL%20as%20an%20efficient%20and%20effective%20framework%20for%20RL%20training%0Ain%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11696v1&entry.124074799=Read"},
{"title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model", "author": "Prasanna Mayilvahanan and Ricardo Dominguez-Olmedo and Thadd\u00e4us Wiedemer and Wieland Brendel", "abstract": "  With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.\n", "link": "http://arxiv.org/abs/2510.11653v1", "date": "2025-10-13", "relevancy": 1.9551, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MATH-Beyond%3A%20A%20Benchmark%20for%20RL%20to%20Expand%20Beyond%20the%20Base%20Model&body=Title%3A%20MATH-Beyond%3A%20A%20Benchmark%20for%20RL%20to%20Expand%20Beyond%20the%20Base%20Model%0AAuthor%3A%20Prasanna%20Mayilvahanan%20and%20Ricardo%20Dominguez-Olmedo%20and%20Thadd%C3%A4us%20Wiedemer%20and%20Wieland%20Brendel%0AAbstract%3A%20%20%20With%20the%20advent%20of%20DeepSeek-R1%2C%20a%20new%20wave%20of%20reinforcement%20learning%20%28RL%29%0Amethods%20has%20emerged%20that%20seem%20to%20unlock%20stronger%20mathematical%20reasoning.%0AHowever%2C%20a%20closer%20look%20at%20the%20open-source%20ecosystem%20reveals%20a%20critical%0Alimitation%3A%20with%20sufficiently%20many%20draws%20%28e.g.%2C%20%24%5Ctexttt%7Bpass%401024%7D%24%29%2C%20many%0Aexisting%20base%20models%20already%20solve%20nearly%20all%20questions%20on%20widely%20used%20math%0Abenchmarks%20such%20as%20MATH-500%20and%20AIME%202024.%20This%20suggests%20that%20the%20RL%0Afine-tuning%20methods%20prevalent%20in%20the%20LLM%20reasoning%20literature%20largely%20sharpen%0Aexisting%20solution%20modes%20rather%20than%20discovering%20entirely%20new%20ones.%20Such%0Asharpening%20stands%20in%20contrast%20to%20the%20broader%20promise%20of%20RL%3A%20to%20foster%0Aexploration%20and%20to%20acquire%20new%20skills.%20To%20move%20beyond%20this%20plateau%2C%20we%0Aintroduce%20MATH-Beyond%20%28MATH-B%29%2C%20a%20benchmark%20deliberately%20constructed%20to%20defeat%0Acommon%20open-source%20models%20of%20up%20to%208B%20parameters%20even%20under%20large%20sampling%0Abudgets.%20Improving%20performance%20on%20our%20benchmark%20via%20RL%20requires%20methods%20that%0Alearn%20to%20reason%20in%20ways%20that%20go%20beyond%20base%20model%20capabilities%20in%20repeated%0Asampling.%20Since%20the%20problems%20are%20drawn%20from%20subsets%20of%20DAPO-Math-17K%20and%0ADeepScaleR%20datasets%2C%20they%20remain%20topically%20equivalent%20to%20standard%20high-school%0Amath.%20Validating%20our%20premise%2C%20RL%20fine-tuned%20models%20such%20as%0ANemotron-Research-Reasoning-Qwen-1.5B%20and%20DeepScaleR-1.5B-Preview%20perform%0Apoorly%20on%20MATH-B%20at%20%24%5Ctexttt%7Bpass%401024%7D%24%2C%20showing%20how%20existing%20approaches%20fall%0Ashort%20on%20tackling%20harder%20instances.%20We%20hope%20MATH-B%20will%20catalyze%0Aexploration-driven%20RL%20approaches%20that%20elicit%20deeper%20reasoning%20capabilities.%20We%0Arelease%20MATH-B%20at%20https%3A//huggingface.co/datasets/brendel-group/MATH-Beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMATH-Beyond%253A%2520A%2520Benchmark%2520for%2520RL%2520to%2520Expand%2520Beyond%2520the%2520Base%2520Model%26entry.906535625%3DPrasanna%2520Mayilvahanan%2520and%2520Ricardo%2520Dominguez-Olmedo%2520and%2520Thadd%25C3%25A4us%2520Wiedemer%2520and%2520Wieland%2520Brendel%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520DeepSeek-R1%252C%2520a%2520new%2520wave%2520of%2520reinforcement%2520learning%2520%2528RL%2529%250Amethods%2520has%2520emerged%2520that%2520seem%2520to%2520unlock%2520stronger%2520mathematical%2520reasoning.%250AHowever%252C%2520a%2520closer%2520look%2520at%2520the%2520open-source%2520ecosystem%2520reveals%2520a%2520critical%250Alimitation%253A%2520with%2520sufficiently%2520many%2520draws%2520%2528e.g.%252C%2520%2524%255Ctexttt%257Bpass%25401024%257D%2524%2529%252C%2520many%250Aexisting%2520base%2520models%2520already%2520solve%2520nearly%2520all%2520questions%2520on%2520widely%2520used%2520math%250Abenchmarks%2520such%2520as%2520MATH-500%2520and%2520AIME%25202024.%2520This%2520suggests%2520that%2520the%2520RL%250Afine-tuning%2520methods%2520prevalent%2520in%2520the%2520LLM%2520reasoning%2520literature%2520largely%2520sharpen%250Aexisting%2520solution%2520modes%2520rather%2520than%2520discovering%2520entirely%2520new%2520ones.%2520Such%250Asharpening%2520stands%2520in%2520contrast%2520to%2520the%2520broader%2520promise%2520of%2520RL%253A%2520to%2520foster%250Aexploration%2520and%2520to%2520acquire%2520new%2520skills.%2520To%2520move%2520beyond%2520this%2520plateau%252C%2520we%250Aintroduce%2520MATH-Beyond%2520%2528MATH-B%2529%252C%2520a%2520benchmark%2520deliberately%2520constructed%2520to%2520defeat%250Acommon%2520open-source%2520models%2520of%2520up%2520to%25208B%2520parameters%2520even%2520under%2520large%2520sampling%250Abudgets.%2520Improving%2520performance%2520on%2520our%2520benchmark%2520via%2520RL%2520requires%2520methods%2520that%250Alearn%2520to%2520reason%2520in%2520ways%2520that%2520go%2520beyond%2520base%2520model%2520capabilities%2520in%2520repeated%250Asampling.%2520Since%2520the%2520problems%2520are%2520drawn%2520from%2520subsets%2520of%2520DAPO-Math-17K%2520and%250ADeepScaleR%2520datasets%252C%2520they%2520remain%2520topically%2520equivalent%2520to%2520standard%2520high-school%250Amath.%2520Validating%2520our%2520premise%252C%2520RL%2520fine-tuned%2520models%2520such%2520as%250ANemotron-Research-Reasoning-Qwen-1.5B%2520and%2520DeepScaleR-1.5B-Preview%2520perform%250Apoorly%2520on%2520MATH-B%2520at%2520%2524%255Ctexttt%257Bpass%25401024%257D%2524%252C%2520showing%2520how%2520existing%2520approaches%2520fall%250Ashort%2520on%2520tackling%2520harder%2520instances.%2520We%2520hope%2520MATH-B%2520will%2520catalyze%250Aexploration-driven%2520RL%2520approaches%2520that%2520elicit%2520deeper%2520reasoning%2520capabilities.%2520We%250Arelease%2520MATH-B%2520at%2520https%253A//huggingface.co/datasets/brendel-group/MATH-Beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MATH-Beyond%3A%20A%20Benchmark%20for%20RL%20to%20Expand%20Beyond%20the%20Base%20Model&entry.906535625=Prasanna%20Mayilvahanan%20and%20Ricardo%20Dominguez-Olmedo%20and%20Thadd%C3%A4us%20Wiedemer%20and%20Wieland%20Brendel&entry.1292438233=%20%20With%20the%20advent%20of%20DeepSeek-R1%2C%20a%20new%20wave%20of%20reinforcement%20learning%20%28RL%29%0Amethods%20has%20emerged%20that%20seem%20to%20unlock%20stronger%20mathematical%20reasoning.%0AHowever%2C%20a%20closer%20look%20at%20the%20open-source%20ecosystem%20reveals%20a%20critical%0Alimitation%3A%20with%20sufficiently%20many%20draws%20%28e.g.%2C%20%24%5Ctexttt%7Bpass%401024%7D%24%29%2C%20many%0Aexisting%20base%20models%20already%20solve%20nearly%20all%20questions%20on%20widely%20used%20math%0Abenchmarks%20such%20as%20MATH-500%20and%20AIME%202024.%20This%20suggests%20that%20the%20RL%0Afine-tuning%20methods%20prevalent%20in%20the%20LLM%20reasoning%20literature%20largely%20sharpen%0Aexisting%20solution%20modes%20rather%20than%20discovering%20entirely%20new%20ones.%20Such%0Asharpening%20stands%20in%20contrast%20to%20the%20broader%20promise%20of%20RL%3A%20to%20foster%0Aexploration%20and%20to%20acquire%20new%20skills.%20To%20move%20beyond%20this%20plateau%2C%20we%0Aintroduce%20MATH-Beyond%20%28MATH-B%29%2C%20a%20benchmark%20deliberately%20constructed%20to%20defeat%0Acommon%20open-source%20models%20of%20up%20to%208B%20parameters%20even%20under%20large%20sampling%0Abudgets.%20Improving%20performance%20on%20our%20benchmark%20via%20RL%20requires%20methods%20that%0Alearn%20to%20reason%20in%20ways%20that%20go%20beyond%20base%20model%20capabilities%20in%20repeated%0Asampling.%20Since%20the%20problems%20are%20drawn%20from%20subsets%20of%20DAPO-Math-17K%20and%0ADeepScaleR%20datasets%2C%20they%20remain%20topically%20equivalent%20to%20standard%20high-school%0Amath.%20Validating%20our%20premise%2C%20RL%20fine-tuned%20models%20such%20as%0ANemotron-Research-Reasoning-Qwen-1.5B%20and%20DeepScaleR-1.5B-Preview%20perform%0Apoorly%20on%20MATH-B%20at%20%24%5Ctexttt%7Bpass%401024%7D%24%2C%20showing%20how%20existing%20approaches%20fall%0Ashort%20on%20tackling%20harder%20instances.%20We%20hope%20MATH-B%20will%20catalyze%0Aexploration-driven%20RL%20approaches%20that%20elicit%20deeper%20reasoning%20capabilities.%20We%0Arelease%20MATH-B%20at%20https%3A//huggingface.co/datasets/brendel-group/MATH-Beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11653v1&entry.124074799=Read"},
{"title": "An Eulerian Perspective on Straight-Line Sampling", "author": "Panos Tsimpos and Youssef Marzouk", "abstract": "  We study dynamic measure transport for generative modeling: specifically,\nflows induced by stochastic processes that bridge a specified source and target\ndistribution. The conditional expectation of the process' velocity defines an\nODE whose flow map achieves the desired transport. We ask \\emph{which processes\nproduce straight-line flows} -- i.e., flows whose pointwise acceleration\nvanishes and thus are exactly integrable with a first-order method? We provide\na concise PDE characterization of straightness as a balance between conditional\nacceleration and the divergence of a weighted covariance (Reynolds) tensor.\nUsing this lens, we fully characterize affine-in-time interpolants and show\nthat straightness occurs exactly under deterministic endpoint couplings. We\nalso derive necessary conditions that constrain flow geometry for general\nprocesses, offering broad guidance for designing transports that are easier to\nintegrate.\n", "link": "http://arxiv.org/abs/2510.11657v1", "date": "2025-10-13", "relevancy": 1.9368, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5388}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4768}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Eulerian%20Perspective%20on%20Straight-Line%20Sampling&body=Title%3A%20An%20Eulerian%20Perspective%20on%20Straight-Line%20Sampling%0AAuthor%3A%20Panos%20Tsimpos%20and%20Youssef%20Marzouk%0AAbstract%3A%20%20%20We%20study%20dynamic%20measure%20transport%20for%20generative%20modeling%3A%20specifically%2C%0Aflows%20induced%20by%20stochastic%20processes%20that%20bridge%20a%20specified%20source%20and%20target%0Adistribution.%20The%20conditional%20expectation%20of%20the%20process%27%20velocity%20defines%20an%0AODE%20whose%20flow%20map%20achieves%20the%20desired%20transport.%20We%20ask%20%5Cemph%7Bwhich%20processes%0Aproduce%20straight-line%20flows%7D%20--%20i.e.%2C%20flows%20whose%20pointwise%20acceleration%0Avanishes%20and%20thus%20are%20exactly%20integrable%20with%20a%20first-order%20method%3F%20We%20provide%0Aa%20concise%20PDE%20characterization%20of%20straightness%20as%20a%20balance%20between%20conditional%0Aacceleration%20and%20the%20divergence%20of%20a%20weighted%20covariance%20%28Reynolds%29%20tensor.%0AUsing%20this%20lens%2C%20we%20fully%20characterize%20affine-in-time%20interpolants%20and%20show%0Athat%20straightness%20occurs%20exactly%20under%20deterministic%20endpoint%20couplings.%20We%0Aalso%20derive%20necessary%20conditions%20that%20constrain%20flow%20geometry%20for%20general%0Aprocesses%2C%20offering%20broad%20guidance%20for%20designing%20transports%20that%20are%20easier%20to%0Aintegrate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Eulerian%2520Perspective%2520on%2520Straight-Line%2520Sampling%26entry.906535625%3DPanos%2520Tsimpos%2520and%2520Youssef%2520Marzouk%26entry.1292438233%3D%2520%2520We%2520study%2520dynamic%2520measure%2520transport%2520for%2520generative%2520modeling%253A%2520specifically%252C%250Aflows%2520induced%2520by%2520stochastic%2520processes%2520that%2520bridge%2520a%2520specified%2520source%2520and%2520target%250Adistribution.%2520The%2520conditional%2520expectation%2520of%2520the%2520process%2527%2520velocity%2520defines%2520an%250AODE%2520whose%2520flow%2520map%2520achieves%2520the%2520desired%2520transport.%2520We%2520ask%2520%255Cemph%257Bwhich%2520processes%250Aproduce%2520straight-line%2520flows%257D%2520--%2520i.e.%252C%2520flows%2520whose%2520pointwise%2520acceleration%250Avanishes%2520and%2520thus%2520are%2520exactly%2520integrable%2520with%2520a%2520first-order%2520method%253F%2520We%2520provide%250Aa%2520concise%2520PDE%2520characterization%2520of%2520straightness%2520as%2520a%2520balance%2520between%2520conditional%250Aacceleration%2520and%2520the%2520divergence%2520of%2520a%2520weighted%2520covariance%2520%2528Reynolds%2529%2520tensor.%250AUsing%2520this%2520lens%252C%2520we%2520fully%2520characterize%2520affine-in-time%2520interpolants%2520and%2520show%250Athat%2520straightness%2520occurs%2520exactly%2520under%2520deterministic%2520endpoint%2520couplings.%2520We%250Aalso%2520derive%2520necessary%2520conditions%2520that%2520constrain%2520flow%2520geometry%2520for%2520general%250Aprocesses%252C%2520offering%2520broad%2520guidance%2520for%2520designing%2520transports%2520that%2520are%2520easier%2520to%250Aintegrate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Eulerian%20Perspective%20on%20Straight-Line%20Sampling&entry.906535625=Panos%20Tsimpos%20and%20Youssef%20Marzouk&entry.1292438233=%20%20We%20study%20dynamic%20measure%20transport%20for%20generative%20modeling%3A%20specifically%2C%0Aflows%20induced%20by%20stochastic%20processes%20that%20bridge%20a%20specified%20source%20and%20target%0Adistribution.%20The%20conditional%20expectation%20of%20the%20process%27%20velocity%20defines%20an%0AODE%20whose%20flow%20map%20achieves%20the%20desired%20transport.%20We%20ask%20%5Cemph%7Bwhich%20processes%0Aproduce%20straight-line%20flows%7D%20--%20i.e.%2C%20flows%20whose%20pointwise%20acceleration%0Avanishes%20and%20thus%20are%20exactly%20integrable%20with%20a%20first-order%20method%3F%20We%20provide%0Aa%20concise%20PDE%20characterization%20of%20straightness%20as%20a%20balance%20between%20conditional%0Aacceleration%20and%20the%20divergence%20of%20a%20weighted%20covariance%20%28Reynolds%29%20tensor.%0AUsing%20this%20lens%2C%20we%20fully%20characterize%20affine-in-time%20interpolants%20and%20show%0Athat%20straightness%20occurs%20exactly%20under%20deterministic%20endpoint%20couplings.%20We%0Aalso%20derive%20necessary%20conditions%20that%20constrain%20flow%20geometry%20for%20general%0Aprocesses%2C%20offering%20broad%20guidance%20for%20designing%20transports%20that%20are%20easier%20to%0Aintegrate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11657v1&entry.124074799=Read"},
{"title": "Reinforced sequential Monte Carlo for amortised sampling", "author": "Sanghyeok Choi and Sarthak Mittal and V\u00edctor Elvira and Jinkyoo Park and Nikolay Malkin", "abstract": "  This paper proposes a synergy of amortised and particle-based methods for\nsampling from distributions defined by unnormalised density functions. We state\na connection between sequential Monte Carlo (SMC) and neural sequential\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\nlearnt sampling policies and value functions define proposal kernels and twist\nfunctions. Exploiting this connection, we introduce an off-policy RL training\nprocedure for the sampler that uses samples from SMC -- using the learnt\nsampler as a proposal -- as a behaviour policy that better explores the target\ndistribution. We describe techniques for stable joint training of proposals and\ntwist functions and an adaptive weight tempering scheme to reduce training\nsignal variance. Furthermore, building upon past attempts to use experience\nreplay to guide the training of neural samplers, we derive a way to combine\nhistorical samples with annealed importance sampling weights within a replay\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\ndemonstrate improvements in approximating the true distribution as well as\ntraining stability compared to both amortised and Monte Carlo methods.\n", "link": "http://arxiv.org/abs/2510.11711v1", "date": "2025-10-13", "relevancy": 1.9333, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5215}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4976}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforced%20sequential%20Monte%20Carlo%20for%20amortised%20sampling&body=Title%3A%20Reinforced%20sequential%20Monte%20Carlo%20for%20amortised%20sampling%0AAuthor%3A%20Sanghyeok%20Choi%20and%20Sarthak%20Mittal%20and%20V%C3%ADctor%20Elvira%20and%20Jinkyoo%20Park%20and%20Nikolay%20Malkin%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20synergy%20of%20amortised%20and%20particle-based%20methods%20for%0Asampling%20from%20distributions%20defined%20by%20unnormalised%20density%20functions.%20We%20state%0Aa%20connection%20between%20sequential%20Monte%20Carlo%20%28SMC%29%20and%20neural%20sequential%0Asamplers%20trained%20by%20maximum-entropy%20reinforcement%20learning%20%28MaxEnt%20RL%29%2C%20wherein%0Alearnt%20sampling%20policies%20and%20value%20functions%20define%20proposal%20kernels%20and%20twist%0Afunctions.%20Exploiting%20this%20connection%2C%20we%20introduce%20an%20off-policy%20RL%20training%0Aprocedure%20for%20the%20sampler%20that%20uses%20samples%20from%20SMC%20--%20using%20the%20learnt%0Asampler%20as%20a%20proposal%20--%20as%20a%20behaviour%20policy%20that%20better%20explores%20the%20target%0Adistribution.%20We%20describe%20techniques%20for%20stable%20joint%20training%20of%20proposals%20and%0Atwist%20functions%20and%20an%20adaptive%20weight%20tempering%20scheme%20to%20reduce%20training%0Asignal%20variance.%20Furthermore%2C%20building%20upon%20past%20attempts%20to%20use%20experience%0Areplay%20to%20guide%20the%20training%20of%20neural%20samplers%2C%20we%20derive%20a%20way%20to%20combine%0Ahistorical%20samples%20with%20annealed%20importance%20sampling%20weights%20within%20a%20replay%0Abuffer.%20On%20synthetic%20multi-modal%20targets%20%28in%20both%20continuous%20and%20discrete%0Aspaces%29%20and%20the%20Boltzmann%20distribution%20of%20alanine%20dipeptide%20conformations%2C%20we%0Ademonstrate%20improvements%20in%20approximating%20the%20true%20distribution%20as%20well%20as%0Atraining%20stability%20compared%20to%20both%20amortised%20and%20Monte%20Carlo%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforced%2520sequential%2520Monte%2520Carlo%2520for%2520amortised%2520sampling%26entry.906535625%3DSanghyeok%2520Choi%2520and%2520Sarthak%2520Mittal%2520and%2520V%25C3%25ADctor%2520Elvira%2520and%2520Jinkyoo%2520Park%2520and%2520Nikolay%2520Malkin%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520synergy%2520of%2520amortised%2520and%2520particle-based%2520methods%2520for%250Asampling%2520from%2520distributions%2520defined%2520by%2520unnormalised%2520density%2520functions.%2520We%2520state%250Aa%2520connection%2520between%2520sequential%2520Monte%2520Carlo%2520%2528SMC%2529%2520and%2520neural%2520sequential%250Asamplers%2520trained%2520by%2520maximum-entropy%2520reinforcement%2520learning%2520%2528MaxEnt%2520RL%2529%252C%2520wherein%250Alearnt%2520sampling%2520policies%2520and%2520value%2520functions%2520define%2520proposal%2520kernels%2520and%2520twist%250Afunctions.%2520Exploiting%2520this%2520connection%252C%2520we%2520introduce%2520an%2520off-policy%2520RL%2520training%250Aprocedure%2520for%2520the%2520sampler%2520that%2520uses%2520samples%2520from%2520SMC%2520--%2520using%2520the%2520learnt%250Asampler%2520as%2520a%2520proposal%2520--%2520as%2520a%2520behaviour%2520policy%2520that%2520better%2520explores%2520the%2520target%250Adistribution.%2520We%2520describe%2520techniques%2520for%2520stable%2520joint%2520training%2520of%2520proposals%2520and%250Atwist%2520functions%2520and%2520an%2520adaptive%2520weight%2520tempering%2520scheme%2520to%2520reduce%2520training%250Asignal%2520variance.%2520Furthermore%252C%2520building%2520upon%2520past%2520attempts%2520to%2520use%2520experience%250Areplay%2520to%2520guide%2520the%2520training%2520of%2520neural%2520samplers%252C%2520we%2520derive%2520a%2520way%2520to%2520combine%250Ahistorical%2520samples%2520with%2520annealed%2520importance%2520sampling%2520weights%2520within%2520a%2520replay%250Abuffer.%2520On%2520synthetic%2520multi-modal%2520targets%2520%2528in%2520both%2520continuous%2520and%2520discrete%250Aspaces%2529%2520and%2520the%2520Boltzmann%2520distribution%2520of%2520alanine%2520dipeptide%2520conformations%252C%2520we%250Ademonstrate%2520improvements%2520in%2520approximating%2520the%2520true%2520distribution%2520as%2520well%2520as%250Atraining%2520stability%2520compared%2520to%2520both%2520amortised%2520and%2520Monte%2520Carlo%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforced%20sequential%20Monte%20Carlo%20for%20amortised%20sampling&entry.906535625=Sanghyeok%20Choi%20and%20Sarthak%20Mittal%20and%20V%C3%ADctor%20Elvira%20and%20Jinkyoo%20Park%20and%20Nikolay%20Malkin&entry.1292438233=%20%20This%20paper%20proposes%20a%20synergy%20of%20amortised%20and%20particle-based%20methods%20for%0Asampling%20from%20distributions%20defined%20by%20unnormalised%20density%20functions.%20We%20state%0Aa%20connection%20between%20sequential%20Monte%20Carlo%20%28SMC%29%20and%20neural%20sequential%0Asamplers%20trained%20by%20maximum-entropy%20reinforcement%20learning%20%28MaxEnt%20RL%29%2C%20wherein%0Alearnt%20sampling%20policies%20and%20value%20functions%20define%20proposal%20kernels%20and%20twist%0Afunctions.%20Exploiting%20this%20connection%2C%20we%20introduce%20an%20off-policy%20RL%20training%0Aprocedure%20for%20the%20sampler%20that%20uses%20samples%20from%20SMC%20--%20using%20the%20learnt%0Asampler%20as%20a%20proposal%20--%20as%20a%20behaviour%20policy%20that%20better%20explores%20the%20target%0Adistribution.%20We%20describe%20techniques%20for%20stable%20joint%20training%20of%20proposals%20and%0Atwist%20functions%20and%20an%20adaptive%20weight%20tempering%20scheme%20to%20reduce%20training%0Asignal%20variance.%20Furthermore%2C%20building%20upon%20past%20attempts%20to%20use%20experience%0Areplay%20to%20guide%20the%20training%20of%20neural%20samplers%2C%20we%20derive%20a%20way%20to%20combine%0Ahistorical%20samples%20with%20annealed%20importance%20sampling%20weights%20within%20a%20replay%0Abuffer.%20On%20synthetic%20multi-modal%20targets%20%28in%20both%20continuous%20and%20discrete%0Aspaces%29%20and%20the%20Boltzmann%20distribution%20of%20alanine%20dipeptide%20conformations%2C%20we%0Ademonstrate%20improvements%20in%20approximating%20the%20true%20distribution%20as%20well%20as%0Atraining%20stability%20compared%20to%20both%20amortised%20and%20Monte%20Carlo%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11711v1&entry.124074799=Read"},
{"title": "Are Large Reasoning Models Interruptible?", "author": "Tsung-Han Wu and Mihran Miroyan and David M. Chan and Trevor Darrell and Narges Norouzi and Joseph E. Gonzalez", "abstract": "  Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.\n", "link": "http://arxiv.org/abs/2510.11713v1", "date": "2025-10-13", "relevancy": 1.9076, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4858}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Reasoning%20Models%20Interruptible%3F&body=Title%3A%20Are%20Large%20Reasoning%20Models%20Interruptible%3F%0AAuthor%3A%20Tsung-Han%20Wu%20and%20Mihran%20Miroyan%20and%20David%20M.%20Chan%20and%20Trevor%20Darrell%20and%20Narges%20Norouzi%20and%20Joseph%20E.%20Gonzalez%0AAbstract%3A%20%20%20Large%20Reasoning%20Models%20%28LRMs%29%20excel%20at%20complex%20reasoning%20but%20are%0Atraditionally%20evaluated%20in%20static%2C%20%22frozen%20world%22%20settings%3A%20model%20responses%20are%0Aassumed%20to%20be%20instantaneous%2C%20and%20the%20context%20of%20a%20request%20is%20presumed%20to%20be%0Aimmutable%20over%20the%20duration%20of%20the%20response.%20While%20generally%20true%20for%0Ashort-term%20tasks%2C%20the%20%22frozen%20world%22%20assumption%20breaks%20down%20in%20modern%20reasoning%0Atasks%20such%20as%20assistive%20programming%2C%20where%20models%20may%20take%20hours%20to%20think%0Athrough%20problems%20and%20code%20may%20change%20dramatically%20from%20the%20time%20the%20model%0Astarts%20thinking%20to%20the%20model%27s%20final%20output.%20In%20this%20work%2C%20we%20challenge%20the%0Afrozen%20world%20assumption%20and%20evaluate%20LRM%20robustness%20under%20two%20realistic%20dynamic%0Ascenarios%3A%20interruptions%2C%20which%20test%20the%20quality%20of%20the%20model%27s%20partial%20outputs%0Aon%20a%20limited%20budget%2C%20and%20dynamic%20context%2C%20which%20tests%20model%20adaptation%20to%0Ain-flight%20changes.%20Across%20mathematics%20and%20programming%20benchmarks%20that%20require%0Along-form%20reasoning%2C%20static%20evaluations%20consistently%20overestimate%20robustness%3A%0Aeven%20state-of-the-art%20LRMs%2C%20which%20achieve%20high%20accuracy%20in%20static%20settings%2C%20can%0Afail%20unpredictably%20when%20interrupted%20or%20exposed%20to%20changing%20context%2C%20with%0Aperformance%20dropping%20by%20up%20to%2060%25%20when%20updates%20are%20introduced%20late%20in%20the%0Areasoning%20process.%20Our%20analysis%20further%20reveals%20several%20novel%20failure%20modes%2C%0Aincluding%20reasoning%20leakage%2C%20where%20models%20fold%20the%20reasoning%20into%20their%20final%0Aanswer%20when%20interrupted%3B%20panic%2C%20where%20under%20time%20pressure%20models%20abandon%0Areasoning%20entirely%20and%20return%20incorrect%20answers%3B%20and%20self-doubt%2C%20where%0Aperformance%20degrades%20while%20incorporating%20updated%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Large%2520Reasoning%2520Models%2520Interruptible%253F%26entry.906535625%3DTsung-Han%2520Wu%2520and%2520Mihran%2520Miroyan%2520and%2520David%2520M.%2520Chan%2520and%2520Trevor%2520Darrell%2520and%2520Narges%2520Norouzi%2520and%2520Joseph%2520E.%2520Gonzalez%26entry.1292438233%3D%2520%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529%2520excel%2520at%2520complex%2520reasoning%2520but%2520are%250Atraditionally%2520evaluated%2520in%2520static%252C%2520%2522frozen%2520world%2522%2520settings%253A%2520model%2520responses%2520are%250Aassumed%2520to%2520be%2520instantaneous%252C%2520and%2520the%2520context%2520of%2520a%2520request%2520is%2520presumed%2520to%2520be%250Aimmutable%2520over%2520the%2520duration%2520of%2520the%2520response.%2520While%2520generally%2520true%2520for%250Ashort-term%2520tasks%252C%2520the%2520%2522frozen%2520world%2522%2520assumption%2520breaks%2520down%2520in%2520modern%2520reasoning%250Atasks%2520such%2520as%2520assistive%2520programming%252C%2520where%2520models%2520may%2520take%2520hours%2520to%2520think%250Athrough%2520problems%2520and%2520code%2520may%2520change%2520dramatically%2520from%2520the%2520time%2520the%2520model%250Astarts%2520thinking%2520to%2520the%2520model%2527s%2520final%2520output.%2520In%2520this%2520work%252C%2520we%2520challenge%2520the%250Afrozen%2520world%2520assumption%2520and%2520evaluate%2520LRM%2520robustness%2520under%2520two%2520realistic%2520dynamic%250Ascenarios%253A%2520interruptions%252C%2520which%2520test%2520the%2520quality%2520of%2520the%2520model%2527s%2520partial%2520outputs%250Aon%2520a%2520limited%2520budget%252C%2520and%2520dynamic%2520context%252C%2520which%2520tests%2520model%2520adaptation%2520to%250Ain-flight%2520changes.%2520Across%2520mathematics%2520and%2520programming%2520benchmarks%2520that%2520require%250Along-form%2520reasoning%252C%2520static%2520evaluations%2520consistently%2520overestimate%2520robustness%253A%250Aeven%2520state-of-the-art%2520LRMs%252C%2520which%2520achieve%2520high%2520accuracy%2520in%2520static%2520settings%252C%2520can%250Afail%2520unpredictably%2520when%2520interrupted%2520or%2520exposed%2520to%2520changing%2520context%252C%2520with%250Aperformance%2520dropping%2520by%2520up%2520to%252060%2525%2520when%2520updates%2520are%2520introduced%2520late%2520in%2520the%250Areasoning%2520process.%2520Our%2520analysis%2520further%2520reveals%2520several%2520novel%2520failure%2520modes%252C%250Aincluding%2520reasoning%2520leakage%252C%2520where%2520models%2520fold%2520the%2520reasoning%2520into%2520their%2520final%250Aanswer%2520when%2520interrupted%253B%2520panic%252C%2520where%2520under%2520time%2520pressure%2520models%2520abandon%250Areasoning%2520entirely%2520and%2520return%2520incorrect%2520answers%253B%2520and%2520self-doubt%252C%2520where%250Aperformance%2520degrades%2520while%2520incorporating%2520updated%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Reasoning%20Models%20Interruptible%3F&entry.906535625=Tsung-Han%20Wu%20and%20Mihran%20Miroyan%20and%20David%20M.%20Chan%20and%20Trevor%20Darrell%20and%20Narges%20Norouzi%20and%20Joseph%20E.%20Gonzalez&entry.1292438233=%20%20Large%20Reasoning%20Models%20%28LRMs%29%20excel%20at%20complex%20reasoning%20but%20are%0Atraditionally%20evaluated%20in%20static%2C%20%22frozen%20world%22%20settings%3A%20model%20responses%20are%0Aassumed%20to%20be%20instantaneous%2C%20and%20the%20context%20of%20a%20request%20is%20presumed%20to%20be%0Aimmutable%20over%20the%20duration%20of%20the%20response.%20While%20generally%20true%20for%0Ashort-term%20tasks%2C%20the%20%22frozen%20world%22%20assumption%20breaks%20down%20in%20modern%20reasoning%0Atasks%20such%20as%20assistive%20programming%2C%20where%20models%20may%20take%20hours%20to%20think%0Athrough%20problems%20and%20code%20may%20change%20dramatically%20from%20the%20time%20the%20model%0Astarts%20thinking%20to%20the%20model%27s%20final%20output.%20In%20this%20work%2C%20we%20challenge%20the%0Afrozen%20world%20assumption%20and%20evaluate%20LRM%20robustness%20under%20two%20realistic%20dynamic%0Ascenarios%3A%20interruptions%2C%20which%20test%20the%20quality%20of%20the%20model%27s%20partial%20outputs%0Aon%20a%20limited%20budget%2C%20and%20dynamic%20context%2C%20which%20tests%20model%20adaptation%20to%0Ain-flight%20changes.%20Across%20mathematics%20and%20programming%20benchmarks%20that%20require%0Along-form%20reasoning%2C%20static%20evaluations%20consistently%20overestimate%20robustness%3A%0Aeven%20state-of-the-art%20LRMs%2C%20which%20achieve%20high%20accuracy%20in%20static%20settings%2C%20can%0Afail%20unpredictably%20when%20interrupted%20or%20exposed%20to%20changing%20context%2C%20with%0Aperformance%20dropping%20by%20up%20to%2060%25%20when%20updates%20are%20introduced%20late%20in%20the%0Areasoning%20process.%20Our%20analysis%20further%20reveals%20several%20novel%20failure%20modes%2C%0Aincluding%20reasoning%20leakage%2C%20where%20models%20fold%20the%20reasoning%20into%20their%20final%0Aanswer%20when%20interrupted%3B%20panic%2C%20where%20under%20time%20pressure%20models%20abandon%0Areasoning%20entirely%20and%20return%20incorrect%20answers%3B%20and%20self-doubt%2C%20where%0Aperformance%20degrades%20while%20incorporating%20updated%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11713v1&entry.124074799=Read"},
{"title": "Continual Release of Densest Subgraphs: Privacy Amplification &\n  Sublinear Space via Subsampling", "author": "Felix Zhou", "abstract": "  We study the sublinear space continual release model for edge-differentially\nprivate (DP) graph algorithms, with a focus on the densest subgraph problem\n(DSG) in the insertion-only setting. Our main result is the first continual\nrelease DSG algorithm that matches the additive error of the best static DP\nalgorithms and the space complexity of the best non-private streaming\nalgorithms, up to constants. The key idea is a refined use of subsampling that\nsimultaneously achieves privacy amplification and sparsification, a connection\nnot previously formalized in graph DP. Via a simple black-box reduction to the\nstatic setting, we obtain both pure and approximate-DP algorithms with $O(\\log\nn)$ additive error and $O(n\\log n)$ space, improving both accuracy and space\ncomplexity over the previous state of the art. Along the way, we introduce\ngraph densification in the graph DP setting, adding edges to trigger earlier\nsubsampling, which removes the extra logarithmic factors in error and space\nincurred by prior work [ELMZ25]. We believe this simple idea may be of\nindependent interest.\n", "link": "http://arxiv.org/abs/2510.11640v1", "date": "2025-10-13", "relevancy": 1.904, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4814}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4734}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Release%20of%20Densest%20Subgraphs%3A%20Privacy%20Amplification%20%26%0A%20%20Sublinear%20Space%20via%20Subsampling&body=Title%3A%20Continual%20Release%20of%20Densest%20Subgraphs%3A%20Privacy%20Amplification%20%26%0A%20%20Sublinear%20Space%20via%20Subsampling%0AAuthor%3A%20Felix%20Zhou%0AAbstract%3A%20%20%20We%20study%20the%20sublinear%20space%20continual%20release%20model%20for%20edge-differentially%0Aprivate%20%28DP%29%20graph%20algorithms%2C%20with%20a%20focus%20on%20the%20densest%20subgraph%20problem%0A%28DSG%29%20in%20the%20insertion-only%20setting.%20Our%20main%20result%20is%20the%20first%20continual%0Arelease%20DSG%20algorithm%20that%20matches%20the%20additive%20error%20of%20the%20best%20static%20DP%0Aalgorithms%20and%20the%20space%20complexity%20of%20the%20best%20non-private%20streaming%0Aalgorithms%2C%20up%20to%20constants.%20The%20key%20idea%20is%20a%20refined%20use%20of%20subsampling%20that%0Asimultaneously%20achieves%20privacy%20amplification%20and%20sparsification%2C%20a%20connection%0Anot%20previously%20formalized%20in%20graph%20DP.%20Via%20a%20simple%20black-box%20reduction%20to%20the%0Astatic%20setting%2C%20we%20obtain%20both%20pure%20and%20approximate-DP%20algorithms%20with%20%24O%28%5Clog%0An%29%24%20additive%20error%20and%20%24O%28n%5Clog%20n%29%24%20space%2C%20improving%20both%20accuracy%20and%20space%0Acomplexity%20over%20the%20previous%20state%20of%20the%20art.%20Along%20the%20way%2C%20we%20introduce%0Agraph%20densification%20in%20the%20graph%20DP%20setting%2C%20adding%20edges%20to%20trigger%20earlier%0Asubsampling%2C%20which%20removes%20the%20extra%20logarithmic%20factors%20in%20error%20and%20space%0Aincurred%20by%20prior%20work%20%5BELMZ25%5D.%20We%20believe%20this%20simple%20idea%20may%20be%20of%0Aindependent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11640v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Release%2520of%2520Densest%2520Subgraphs%253A%2520Privacy%2520Amplification%2520%2526%250A%2520%2520Sublinear%2520Space%2520via%2520Subsampling%26entry.906535625%3DFelix%2520Zhou%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520sublinear%2520space%2520continual%2520release%2520model%2520for%2520edge-differentially%250Aprivate%2520%2528DP%2529%2520graph%2520algorithms%252C%2520with%2520a%2520focus%2520on%2520the%2520densest%2520subgraph%2520problem%250A%2528DSG%2529%2520in%2520the%2520insertion-only%2520setting.%2520Our%2520main%2520result%2520is%2520the%2520first%2520continual%250Arelease%2520DSG%2520algorithm%2520that%2520matches%2520the%2520additive%2520error%2520of%2520the%2520best%2520static%2520DP%250Aalgorithms%2520and%2520the%2520space%2520complexity%2520of%2520the%2520best%2520non-private%2520streaming%250Aalgorithms%252C%2520up%2520to%2520constants.%2520The%2520key%2520idea%2520is%2520a%2520refined%2520use%2520of%2520subsampling%2520that%250Asimultaneously%2520achieves%2520privacy%2520amplification%2520and%2520sparsification%252C%2520a%2520connection%250Anot%2520previously%2520formalized%2520in%2520graph%2520DP.%2520Via%2520a%2520simple%2520black-box%2520reduction%2520to%2520the%250Astatic%2520setting%252C%2520we%2520obtain%2520both%2520pure%2520and%2520approximate-DP%2520algorithms%2520with%2520%2524O%2528%255Clog%250An%2529%2524%2520additive%2520error%2520and%2520%2524O%2528n%255Clog%2520n%2529%2524%2520space%252C%2520improving%2520both%2520accuracy%2520and%2520space%250Acomplexity%2520over%2520the%2520previous%2520state%2520of%2520the%2520art.%2520Along%2520the%2520way%252C%2520we%2520introduce%250Agraph%2520densification%2520in%2520the%2520graph%2520DP%2520setting%252C%2520adding%2520edges%2520to%2520trigger%2520earlier%250Asubsampling%252C%2520which%2520removes%2520the%2520extra%2520logarithmic%2520factors%2520in%2520error%2520and%2520space%250Aincurred%2520by%2520prior%2520work%2520%255BELMZ25%255D.%2520We%2520believe%2520this%2520simple%2520idea%2520may%2520be%2520of%250Aindependent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11640v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Release%20of%20Densest%20Subgraphs%3A%20Privacy%20Amplification%20%26%0A%20%20Sublinear%20Space%20via%20Subsampling&entry.906535625=Felix%20Zhou&entry.1292438233=%20%20We%20study%20the%20sublinear%20space%20continual%20release%20model%20for%20edge-differentially%0Aprivate%20%28DP%29%20graph%20algorithms%2C%20with%20a%20focus%20on%20the%20densest%20subgraph%20problem%0A%28DSG%29%20in%20the%20insertion-only%20setting.%20Our%20main%20result%20is%20the%20first%20continual%0Arelease%20DSG%20algorithm%20that%20matches%20the%20additive%20error%20of%20the%20best%20static%20DP%0Aalgorithms%20and%20the%20space%20complexity%20of%20the%20best%20non-private%20streaming%0Aalgorithms%2C%20up%20to%20constants.%20The%20key%20idea%20is%20a%20refined%20use%20of%20subsampling%20that%0Asimultaneously%20achieves%20privacy%20amplification%20and%20sparsification%2C%20a%20connection%0Anot%20previously%20formalized%20in%20graph%20DP.%20Via%20a%20simple%20black-box%20reduction%20to%20the%0Astatic%20setting%2C%20we%20obtain%20both%20pure%20and%20approximate-DP%20algorithms%20with%20%24O%28%5Clog%0An%29%24%20additive%20error%20and%20%24O%28n%5Clog%20n%29%24%20space%2C%20improving%20both%20accuracy%20and%20space%0Acomplexity%20over%20the%20previous%20state%20of%20the%20art.%20Along%20the%20way%2C%20we%20introduce%0Agraph%20densification%20in%20the%20graph%20DP%20setting%2C%20adding%20edges%20to%20trigger%20earlier%0Asubsampling%2C%20which%20removes%20the%20extra%20logarithmic%20factors%20in%20error%20and%20space%0Aincurred%20by%20prior%20work%20%5BELMZ25%5D.%20We%20believe%20this%20simple%20idea%20may%20be%20of%0Aindependent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11640v1&entry.124074799=Read"},
{"title": "Diffusion Transformers with Representation Autoencoders", "author": "Boyang Zheng and Nanye Ma and Shengbang Tong and Saining Xie", "abstract": "  Latent generative modeling, where a pretrained autoencoder maps pixels into a\nlatent space for the diffusion process, has become the standard strategy for\nDiffusion Transformers (DiT); however, the autoencoder component has barely\nevolved. Most DiTs continue to rely on the original VAE encoder, which\nintroduces several limitations: outdated backbones that compromise\narchitectural simplicity, low-dimensional latent spaces that restrict\ninformation capacity, and weak representations that result from purely\nreconstruction-based training and ultimately limit generative quality. In this\nwork, we explore replacing the VAE with pretrained representation encoders\n(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term\nRepresentation Autoencoders (RAEs). These models provide both high-quality\nreconstructions and semantically rich latent spaces, while allowing for a\nscalable transformer-based architecture. Since these latent spaces are\ntypically high-dimensional, a key challenge is enabling diffusion transformers\nto operate effectively within them. We analyze the sources of this difficulty,\npropose theoretically motivated solutions, and validate them empirically. Our\napproach achieves faster convergence without auxiliary representation alignment\nlosses. Using a DiT variant equipped with a lightweight, wide DDT head, we\nachieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no\nguidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers\nclear advantages and should be the new default for diffusion transformer\ntraining.\n", "link": "http://arxiv.org/abs/2510.11690v1", "date": "2025-10-13", "relevancy": 1.8686, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6913}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6389}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Transformers%20with%20Representation%20Autoencoders&body=Title%3A%20Diffusion%20Transformers%20with%20Representation%20Autoencoders%0AAuthor%3A%20Boyang%20Zheng%20and%20Nanye%20Ma%20and%20Shengbang%20Tong%20and%20Saining%20Xie%0AAbstract%3A%20%20%20Latent%20generative%20modeling%2C%20where%20a%20pretrained%20autoencoder%20maps%20pixels%20into%20a%0Alatent%20space%20for%20the%20diffusion%20process%2C%20has%20become%20the%20standard%20strategy%20for%0ADiffusion%20Transformers%20%28DiT%29%3B%20however%2C%20the%20autoencoder%20component%20has%20barely%0Aevolved.%20Most%20DiTs%20continue%20to%20rely%20on%20the%20original%20VAE%20encoder%2C%20which%0Aintroduces%20several%20limitations%3A%20outdated%20backbones%20that%20compromise%0Aarchitectural%20simplicity%2C%20low-dimensional%20latent%20spaces%20that%20restrict%0Ainformation%20capacity%2C%20and%20weak%20representations%20that%20result%20from%20purely%0Areconstruction-based%20training%20and%20ultimately%20limit%20generative%20quality.%20In%20this%0Awork%2C%20we%20explore%20replacing%20the%20VAE%20with%20pretrained%20representation%20encoders%0A%28e.g.%2C%20DINO%2C%20SigLIP%2C%20MAE%29%20paired%20with%20trained%20decoders%2C%20forming%20what%20we%20term%0ARepresentation%20Autoencoders%20%28RAEs%29.%20These%20models%20provide%20both%20high-quality%0Areconstructions%20and%20semantically%20rich%20latent%20spaces%2C%20while%20allowing%20for%20a%0Ascalable%20transformer-based%20architecture.%20Since%20these%20latent%20spaces%20are%0Atypically%20high-dimensional%2C%20a%20key%20challenge%20is%20enabling%20diffusion%20transformers%0Ato%20operate%20effectively%20within%20them.%20We%20analyze%20the%20sources%20of%20this%20difficulty%2C%0Apropose%20theoretically%20motivated%20solutions%2C%20and%20validate%20them%20empirically.%20Our%0Aapproach%20achieves%20faster%20convergence%20without%20auxiliary%20representation%20alignment%0Alosses.%20Using%20a%20DiT%20variant%20equipped%20with%20a%20lightweight%2C%20wide%20DDT%20head%2C%20we%0Aachieve%20strong%20image%20generation%20results%20on%20ImageNet%3A%201.51%20FID%20at%20256x256%20%28no%0Aguidance%29%20and%201.13%20at%20both%20256x256%20and%20512x512%20%28with%20guidance%29.%20RAE%20offers%0Aclear%20advantages%20and%20should%20be%20the%20new%20default%20for%20diffusion%20transformer%0Atraining.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11690v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Transformers%2520with%2520Representation%2520Autoencoders%26entry.906535625%3DBoyang%2520Zheng%2520and%2520Nanye%2520Ma%2520and%2520Shengbang%2520Tong%2520and%2520Saining%2520Xie%26entry.1292438233%3D%2520%2520Latent%2520generative%2520modeling%252C%2520where%2520a%2520pretrained%2520autoencoder%2520maps%2520pixels%2520into%2520a%250Alatent%2520space%2520for%2520the%2520diffusion%2520process%252C%2520has%2520become%2520the%2520standard%2520strategy%2520for%250ADiffusion%2520Transformers%2520%2528DiT%2529%253B%2520however%252C%2520the%2520autoencoder%2520component%2520has%2520barely%250Aevolved.%2520Most%2520DiTs%2520continue%2520to%2520rely%2520on%2520the%2520original%2520VAE%2520encoder%252C%2520which%250Aintroduces%2520several%2520limitations%253A%2520outdated%2520backbones%2520that%2520compromise%250Aarchitectural%2520simplicity%252C%2520low-dimensional%2520latent%2520spaces%2520that%2520restrict%250Ainformation%2520capacity%252C%2520and%2520weak%2520representations%2520that%2520result%2520from%2520purely%250Areconstruction-based%2520training%2520and%2520ultimately%2520limit%2520generative%2520quality.%2520In%2520this%250Awork%252C%2520we%2520explore%2520replacing%2520the%2520VAE%2520with%2520pretrained%2520representation%2520encoders%250A%2528e.g.%252C%2520DINO%252C%2520SigLIP%252C%2520MAE%2529%2520paired%2520with%2520trained%2520decoders%252C%2520forming%2520what%2520we%2520term%250ARepresentation%2520Autoencoders%2520%2528RAEs%2529.%2520These%2520models%2520provide%2520both%2520high-quality%250Areconstructions%2520and%2520semantically%2520rich%2520latent%2520spaces%252C%2520while%2520allowing%2520for%2520a%250Ascalable%2520transformer-based%2520architecture.%2520Since%2520these%2520latent%2520spaces%2520are%250Atypically%2520high-dimensional%252C%2520a%2520key%2520challenge%2520is%2520enabling%2520diffusion%2520transformers%250Ato%2520operate%2520effectively%2520within%2520them.%2520We%2520analyze%2520the%2520sources%2520of%2520this%2520difficulty%252C%250Apropose%2520theoretically%2520motivated%2520solutions%252C%2520and%2520validate%2520them%2520empirically.%2520Our%250Aapproach%2520achieves%2520faster%2520convergence%2520without%2520auxiliary%2520representation%2520alignment%250Alosses.%2520Using%2520a%2520DiT%2520variant%2520equipped%2520with%2520a%2520lightweight%252C%2520wide%2520DDT%2520head%252C%2520we%250Aachieve%2520strong%2520image%2520generation%2520results%2520on%2520ImageNet%253A%25201.51%2520FID%2520at%2520256x256%2520%2528no%250Aguidance%2529%2520and%25201.13%2520at%2520both%2520256x256%2520and%2520512x512%2520%2528with%2520guidance%2529.%2520RAE%2520offers%250Aclear%2520advantages%2520and%2520should%2520be%2520the%2520new%2520default%2520for%2520diffusion%2520transformer%250Atraining.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11690v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Transformers%20with%20Representation%20Autoencoders&entry.906535625=Boyang%20Zheng%20and%20Nanye%20Ma%20and%20Shengbang%20Tong%20and%20Saining%20Xie&entry.1292438233=%20%20Latent%20generative%20modeling%2C%20where%20a%20pretrained%20autoencoder%20maps%20pixels%20into%20a%0Alatent%20space%20for%20the%20diffusion%20process%2C%20has%20become%20the%20standard%20strategy%20for%0ADiffusion%20Transformers%20%28DiT%29%3B%20however%2C%20the%20autoencoder%20component%20has%20barely%0Aevolved.%20Most%20DiTs%20continue%20to%20rely%20on%20the%20original%20VAE%20encoder%2C%20which%0Aintroduces%20several%20limitations%3A%20outdated%20backbones%20that%20compromise%0Aarchitectural%20simplicity%2C%20low-dimensional%20latent%20spaces%20that%20restrict%0Ainformation%20capacity%2C%20and%20weak%20representations%20that%20result%20from%20purely%0Areconstruction-based%20training%20and%20ultimately%20limit%20generative%20quality.%20In%20this%0Awork%2C%20we%20explore%20replacing%20the%20VAE%20with%20pretrained%20representation%20encoders%0A%28e.g.%2C%20DINO%2C%20SigLIP%2C%20MAE%29%20paired%20with%20trained%20decoders%2C%20forming%20what%20we%20term%0ARepresentation%20Autoencoders%20%28RAEs%29.%20These%20models%20provide%20both%20high-quality%0Areconstructions%20and%20semantically%20rich%20latent%20spaces%2C%20while%20allowing%20for%20a%0Ascalable%20transformer-based%20architecture.%20Since%20these%20latent%20spaces%20are%0Atypically%20high-dimensional%2C%20a%20key%20challenge%20is%20enabling%20diffusion%20transformers%0Ato%20operate%20effectively%20within%20them.%20We%20analyze%20the%20sources%20of%20this%20difficulty%2C%0Apropose%20theoretically%20motivated%20solutions%2C%20and%20validate%20them%20empirically.%20Our%0Aapproach%20achieves%20faster%20convergence%20without%20auxiliary%20representation%20alignment%0Alosses.%20Using%20a%20DiT%20variant%20equipped%20with%20a%20lightweight%2C%20wide%20DDT%20head%2C%20we%0Aachieve%20strong%20image%20generation%20results%20on%20ImageNet%3A%201.51%20FID%20at%20256x256%20%28no%0Aguidance%29%20and%201.13%20at%20both%20256x256%20and%20512x512%20%28with%20guidance%29.%20RAE%20offers%0Aclear%20advantages%20and%20should%20be%20the%20new%20default%20for%20diffusion%20transformer%0Atraining.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11690v1&entry.124074799=Read"},
{"title": "Calibrated Dynamic Modeling for Force and Payload Estimation in\n  Hydraulic Machinery", "author": "Lennart Werner and Pol Eyschen and Sean Costello and Pierluigi Micarelli and Marco Hutter", "abstract": "  Accurate real-time estimation of end effector interaction forces in hydraulic\nexcavators is a key enabler for advanced automation in heavy machinery.\nAccurate knowledge of these forces allows improved, precise grading and digging\nmaneuvers. To address these challenges, we introduce a high-accuracy,\nretrofittable 2D force- and payload estimation algorithm that does not impose\nadditional requirements on the operator regarding trajectory, acceleration or\nthe use of the slew joint. The approach is designed for retrofittability,\nrequires minimal calibration and no prior knowledge of machine-specific dynamic\ncharacteristics. Specifically, we propose a method for identifying a dynamic\nmodel, necessary to estimate both end effector interaction forces and bucket\npayload during normal operation. Our optimization-based payload estimation\nachieves a full-scale payload accuracy of 1%. On a standard 25 t excavator, the\nonline force measurement from pressure and inertial measurements achieves a\ndirection accuracy of 13 degree and a magnitude accuracy of 383 N. The method's\naccuracy and generalization capability are validated on two excavator platforms\nof different type and weight classes. We benchmark our payload estimation\nagainst a classical quasistatic method and a commercially available system. Our\nsystem outperforms both in accuracy and precision.\n", "link": "http://arxiv.org/abs/2510.11574v1", "date": "2025-10-13", "relevancy": 1.8641, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4615}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrated%20Dynamic%20Modeling%20for%20Force%20and%20Payload%20Estimation%20in%0A%20%20Hydraulic%20Machinery&body=Title%3A%20Calibrated%20Dynamic%20Modeling%20for%20Force%20and%20Payload%20Estimation%20in%0A%20%20Hydraulic%20Machinery%0AAuthor%3A%20Lennart%20Werner%20and%20Pol%20Eyschen%20and%20Sean%20Costello%20and%20Pierluigi%20Micarelli%20and%20Marco%20Hutter%0AAbstract%3A%20%20%20Accurate%20real-time%20estimation%20of%20end%20effector%20interaction%20forces%20in%20hydraulic%0Aexcavators%20is%20a%20key%20enabler%20for%20advanced%20automation%20in%20heavy%20machinery.%0AAccurate%20knowledge%20of%20these%20forces%20allows%20improved%2C%20precise%20grading%20and%20digging%0Amaneuvers.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20high-accuracy%2C%0Aretrofittable%202D%20force-%20and%20payload%20estimation%20algorithm%20that%20does%20not%20impose%0Aadditional%20requirements%20on%20the%20operator%20regarding%20trajectory%2C%20acceleration%20or%0Athe%20use%20of%20the%20slew%20joint.%20The%20approach%20is%20designed%20for%20retrofittability%2C%0Arequires%20minimal%20calibration%20and%20no%20prior%20knowledge%20of%20machine-specific%20dynamic%0Acharacteristics.%20Specifically%2C%20we%20propose%20a%20method%20for%20identifying%20a%20dynamic%0Amodel%2C%20necessary%20to%20estimate%20both%20end%20effector%20interaction%20forces%20and%20bucket%0Apayload%20during%20normal%20operation.%20Our%20optimization-based%20payload%20estimation%0Aachieves%20a%20full-scale%20payload%20accuracy%20of%201%25.%20On%20a%20standard%2025%20t%20excavator%2C%20the%0Aonline%20force%20measurement%20from%20pressure%20and%20inertial%20measurements%20achieves%20a%0Adirection%20accuracy%20of%2013%20degree%20and%20a%20magnitude%20accuracy%20of%20383%20N.%20The%20method%27s%0Aaccuracy%20and%20generalization%20capability%20are%20validated%20on%20two%20excavator%20platforms%0Aof%20different%20type%20and%20weight%20classes.%20We%20benchmark%20our%20payload%20estimation%0Aagainst%20a%20classical%20quasistatic%20method%20and%20a%20commercially%20available%20system.%20Our%0Asystem%20outperforms%20both%20in%20accuracy%20and%20precision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11574v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrated%2520Dynamic%2520Modeling%2520for%2520Force%2520and%2520Payload%2520Estimation%2520in%250A%2520%2520Hydraulic%2520Machinery%26entry.906535625%3DLennart%2520Werner%2520and%2520Pol%2520Eyschen%2520and%2520Sean%2520Costello%2520and%2520Pierluigi%2520Micarelli%2520and%2520Marco%2520Hutter%26entry.1292438233%3D%2520%2520Accurate%2520real-time%2520estimation%2520of%2520end%2520effector%2520interaction%2520forces%2520in%2520hydraulic%250Aexcavators%2520is%2520a%2520key%2520enabler%2520for%2520advanced%2520automation%2520in%2520heavy%2520machinery.%250AAccurate%2520knowledge%2520of%2520these%2520forces%2520allows%2520improved%252C%2520precise%2520grading%2520and%2520digging%250Amaneuvers.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520high-accuracy%252C%250Aretrofittable%25202D%2520force-%2520and%2520payload%2520estimation%2520algorithm%2520that%2520does%2520not%2520impose%250Aadditional%2520requirements%2520on%2520the%2520operator%2520regarding%2520trajectory%252C%2520acceleration%2520or%250Athe%2520use%2520of%2520the%2520slew%2520joint.%2520The%2520approach%2520is%2520designed%2520for%2520retrofittability%252C%250Arequires%2520minimal%2520calibration%2520and%2520no%2520prior%2520knowledge%2520of%2520machine-specific%2520dynamic%250Acharacteristics.%2520Specifically%252C%2520we%2520propose%2520a%2520method%2520for%2520identifying%2520a%2520dynamic%250Amodel%252C%2520necessary%2520to%2520estimate%2520both%2520end%2520effector%2520interaction%2520forces%2520and%2520bucket%250Apayload%2520during%2520normal%2520operation.%2520Our%2520optimization-based%2520payload%2520estimation%250Aachieves%2520a%2520full-scale%2520payload%2520accuracy%2520of%25201%2525.%2520On%2520a%2520standard%252025%2520t%2520excavator%252C%2520the%250Aonline%2520force%2520measurement%2520from%2520pressure%2520and%2520inertial%2520measurements%2520achieves%2520a%250Adirection%2520accuracy%2520of%252013%2520degree%2520and%2520a%2520magnitude%2520accuracy%2520of%2520383%2520N.%2520The%2520method%2527s%250Aaccuracy%2520and%2520generalization%2520capability%2520are%2520validated%2520on%2520two%2520excavator%2520platforms%250Aof%2520different%2520type%2520and%2520weight%2520classes.%2520We%2520benchmark%2520our%2520payload%2520estimation%250Aagainst%2520a%2520classical%2520quasistatic%2520method%2520and%2520a%2520commercially%2520available%2520system.%2520Our%250Asystem%2520outperforms%2520both%2520in%2520accuracy%2520and%2520precision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11574v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrated%20Dynamic%20Modeling%20for%20Force%20and%20Payload%20Estimation%20in%0A%20%20Hydraulic%20Machinery&entry.906535625=Lennart%20Werner%20and%20Pol%20Eyschen%20and%20Sean%20Costello%20and%20Pierluigi%20Micarelli%20and%20Marco%20Hutter&entry.1292438233=%20%20Accurate%20real-time%20estimation%20of%20end%20effector%20interaction%20forces%20in%20hydraulic%0Aexcavators%20is%20a%20key%20enabler%20for%20advanced%20automation%20in%20heavy%20machinery.%0AAccurate%20knowledge%20of%20these%20forces%20allows%20improved%2C%20precise%20grading%20and%20digging%0Amaneuvers.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20high-accuracy%2C%0Aretrofittable%202D%20force-%20and%20payload%20estimation%20algorithm%20that%20does%20not%20impose%0Aadditional%20requirements%20on%20the%20operator%20regarding%20trajectory%2C%20acceleration%20or%0Athe%20use%20of%20the%20slew%20joint.%20The%20approach%20is%20designed%20for%20retrofittability%2C%0Arequires%20minimal%20calibration%20and%20no%20prior%20knowledge%20of%20machine-specific%20dynamic%0Acharacteristics.%20Specifically%2C%20we%20propose%20a%20method%20for%20identifying%20a%20dynamic%0Amodel%2C%20necessary%20to%20estimate%20both%20end%20effector%20interaction%20forces%20and%20bucket%0Apayload%20during%20normal%20operation.%20Our%20optimization-based%20payload%20estimation%0Aachieves%20a%20full-scale%20payload%20accuracy%20of%201%25.%20On%20a%20standard%2025%20t%20excavator%2C%20the%0Aonline%20force%20measurement%20from%20pressure%20and%20inertial%20measurements%20achieves%20a%0Adirection%20accuracy%20of%2013%20degree%20and%20a%20magnitude%20accuracy%20of%20383%20N.%20The%20method%27s%0Aaccuracy%20and%20generalization%20capability%20are%20validated%20on%20two%20excavator%20platforms%0Aof%20different%20type%20and%20weight%20classes.%20We%20benchmark%20our%20payload%20estimation%0Aagainst%20a%20classical%20quasistatic%20method%20and%20a%20commercially%20available%20system.%20Our%0Asystem%20outperforms%20both%20in%20accuracy%20and%20precision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11574v1&entry.124074799=Read"},
{"title": "Ego-Vision World Model for Humanoid Contact Planning", "author": "Hang Liu and Yuman Gao and Sangli Teng and Yufeng Chi and Yakun Sophia Shao and Zhongyu Li and Maani Ghaffari and Koushil Sreenath", "abstract": "  Enabling humanoid robots to exploit physical contact, rather than simply\navoid collisions, is crucial for autonomy in unstructured environments.\nTraditional optimization-based planners struggle with contact complexity, while\non-policy reinforcement learning (RL) is sample-inefficient and has limited\nmulti-task ability. We propose a framework combining a learned world model with\nsampling-based Model Predictive Control (MPC), trained on a demonstration-free\noffline dataset to predict future outcomes in a compressed latent space. To\naddress sparse contact rewards and sensor noise, the MPC uses a learned\nsurrogate value function for dense, robust planning. Our single, scalable model\nsupports contact-aware tasks, including wall support after perturbation,\nblocking incoming objects, and traversing height-limited arches, with improved\ndata efficiency and multi-task capability over on-policy RL. Deployed on a\nphysical humanoid, our system achieves robust, real-time contact planning from\nproprioception and ego-centric depth images. Website:\nhttps://ego-vcp.github.io/\n", "link": "http://arxiv.org/abs/2510.11682v1", "date": "2025-10-13", "relevancy": 1.8514, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.627}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6158}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ego-Vision%20World%20Model%20for%20Humanoid%20Contact%20Planning&body=Title%3A%20Ego-Vision%20World%20Model%20for%20Humanoid%20Contact%20Planning%0AAuthor%3A%20Hang%20Liu%20and%20Yuman%20Gao%20and%20Sangli%20Teng%20and%20Yufeng%20Chi%20and%20Yakun%20Sophia%20Shao%20and%20Zhongyu%20Li%20and%20Maani%20Ghaffari%20and%20Koushil%20Sreenath%0AAbstract%3A%20%20%20Enabling%20humanoid%20robots%20to%20exploit%20physical%20contact%2C%20rather%20than%20simply%0Aavoid%20collisions%2C%20is%20crucial%20for%20autonomy%20in%20unstructured%20environments.%0ATraditional%20optimization-based%20planners%20struggle%20with%20contact%20complexity%2C%20while%0Aon-policy%20reinforcement%20learning%20%28RL%29%20is%20sample-inefficient%20and%20has%20limited%0Amulti-task%20ability.%20We%20propose%20a%20framework%20combining%20a%20learned%20world%20model%20with%0Asampling-based%20Model%20Predictive%20Control%20%28MPC%29%2C%20trained%20on%20a%20demonstration-free%0Aoffline%20dataset%20to%20predict%20future%20outcomes%20in%20a%20compressed%20latent%20space.%20To%0Aaddress%20sparse%20contact%20rewards%20and%20sensor%20noise%2C%20the%20MPC%20uses%20a%20learned%0Asurrogate%20value%20function%20for%20dense%2C%20robust%20planning.%20Our%20single%2C%20scalable%20model%0Asupports%20contact-aware%20tasks%2C%20including%20wall%20support%20after%20perturbation%2C%0Ablocking%20incoming%20objects%2C%20and%20traversing%20height-limited%20arches%2C%20with%20improved%0Adata%20efficiency%20and%20multi-task%20capability%20over%20on-policy%20RL.%20Deployed%20on%20a%0Aphysical%20humanoid%2C%20our%20system%20achieves%20robust%2C%20real-time%20contact%20planning%20from%0Aproprioception%20and%20ego-centric%20depth%20images.%20Website%3A%0Ahttps%3A//ego-vcp.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgo-Vision%2520World%2520Model%2520for%2520Humanoid%2520Contact%2520Planning%26entry.906535625%3DHang%2520Liu%2520and%2520Yuman%2520Gao%2520and%2520Sangli%2520Teng%2520and%2520Yufeng%2520Chi%2520and%2520Yakun%2520Sophia%2520Shao%2520and%2520Zhongyu%2520Li%2520and%2520Maani%2520Ghaffari%2520and%2520Koushil%2520Sreenath%26entry.1292438233%3D%2520%2520Enabling%2520humanoid%2520robots%2520to%2520exploit%2520physical%2520contact%252C%2520rather%2520than%2520simply%250Aavoid%2520collisions%252C%2520is%2520crucial%2520for%2520autonomy%2520in%2520unstructured%2520environments.%250ATraditional%2520optimization-based%2520planners%2520struggle%2520with%2520contact%2520complexity%252C%2520while%250Aon-policy%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520sample-inefficient%2520and%2520has%2520limited%250Amulti-task%2520ability.%2520We%2520propose%2520a%2520framework%2520combining%2520a%2520learned%2520world%2520model%2520with%250Asampling-based%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%252C%2520trained%2520on%2520a%2520demonstration-free%250Aoffline%2520dataset%2520to%2520predict%2520future%2520outcomes%2520in%2520a%2520compressed%2520latent%2520space.%2520To%250Aaddress%2520sparse%2520contact%2520rewards%2520and%2520sensor%2520noise%252C%2520the%2520MPC%2520uses%2520a%2520learned%250Asurrogate%2520value%2520function%2520for%2520dense%252C%2520robust%2520planning.%2520Our%2520single%252C%2520scalable%2520model%250Asupports%2520contact-aware%2520tasks%252C%2520including%2520wall%2520support%2520after%2520perturbation%252C%250Ablocking%2520incoming%2520objects%252C%2520and%2520traversing%2520height-limited%2520arches%252C%2520with%2520improved%250Adata%2520efficiency%2520and%2520multi-task%2520capability%2520over%2520on-policy%2520RL.%2520Deployed%2520on%2520a%250Aphysical%2520humanoid%252C%2520our%2520system%2520achieves%2520robust%252C%2520real-time%2520contact%2520planning%2520from%250Aproprioception%2520and%2520ego-centric%2520depth%2520images.%2520Website%253A%250Ahttps%253A//ego-vcp.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ego-Vision%20World%20Model%20for%20Humanoid%20Contact%20Planning&entry.906535625=Hang%20Liu%20and%20Yuman%20Gao%20and%20Sangli%20Teng%20and%20Yufeng%20Chi%20and%20Yakun%20Sophia%20Shao%20and%20Zhongyu%20Li%20and%20Maani%20Ghaffari%20and%20Koushil%20Sreenath&entry.1292438233=%20%20Enabling%20humanoid%20robots%20to%20exploit%20physical%20contact%2C%20rather%20than%20simply%0Aavoid%20collisions%2C%20is%20crucial%20for%20autonomy%20in%20unstructured%20environments.%0ATraditional%20optimization-based%20planners%20struggle%20with%20contact%20complexity%2C%20while%0Aon-policy%20reinforcement%20learning%20%28RL%29%20is%20sample-inefficient%20and%20has%20limited%0Amulti-task%20ability.%20We%20propose%20a%20framework%20combining%20a%20learned%20world%20model%20with%0Asampling-based%20Model%20Predictive%20Control%20%28MPC%29%2C%20trained%20on%20a%20demonstration-free%0Aoffline%20dataset%20to%20predict%20future%20outcomes%20in%20a%20compressed%20latent%20space.%20To%0Aaddress%20sparse%20contact%20rewards%20and%20sensor%20noise%2C%20the%20MPC%20uses%20a%20learned%0Asurrogate%20value%20function%20for%20dense%2C%20robust%20planning.%20Our%20single%2C%20scalable%20model%0Asupports%20contact-aware%20tasks%2C%20including%20wall%20support%20after%20perturbation%2C%0Ablocking%20incoming%20objects%2C%20and%20traversing%20height-limited%20arches%2C%20with%20improved%0Adata%20efficiency%20and%20multi-task%20capability%20over%20on-policy%20RL.%20Deployed%20on%20a%0Aphysical%20humanoid%2C%20our%20system%20achieves%20robust%2C%20real-time%20contact%20planning%20from%0Aproprioception%20and%20ego-centric%20depth%20images.%20Website%3A%0Ahttps%3A//ego-vcp.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11682v1&entry.124074799=Read"},
{"title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for\n  Uncertainty-Aware Sim-to-Real Manipulation", "author": "Maggie Wang and Stephen Tian and Aiden Swann and Ola Shorinwa and Jiajun Wu and Mac Schwager", "abstract": "  Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ .\n", "link": "http://arxiv.org/abs/2510.11689v1", "date": "2025-10-13", "relevancy": 1.8436, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6663}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6079}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phys2Real%3A%20Fusing%20VLM%20Priors%20with%20Interactive%20Online%20Adaptation%20for%0A%20%20Uncertainty-Aware%20Sim-to-Real%20Manipulation&body=Title%3A%20Phys2Real%3A%20Fusing%20VLM%20Priors%20with%20Interactive%20Online%20Adaptation%20for%0A%20%20Uncertainty-Aware%20Sim-to-Real%20Manipulation%0AAuthor%3A%20Maggie%20Wang%20and%20Stephen%20Tian%20and%20Aiden%20Swann%20and%20Ola%20Shorinwa%20and%20Jiajun%20Wu%20and%20Mac%20Schwager%0AAbstract%3A%20%20%20Learning%20robotic%20manipulation%20policies%20directly%20in%20the%20real%20world%20can%20be%0Aexpensive%20and%20time-consuming.%20While%20reinforcement%20learning%20%28RL%29%20policies%0Atrained%20in%20simulation%20present%20a%20scalable%20alternative%2C%20effective%20sim-to-real%0Atransfer%20remains%20challenging%2C%20particularly%20for%20tasks%20that%20require%20precise%0Adynamics.%20To%20address%20this%2C%20we%20propose%20Phys2Real%2C%20a%20real-to-sim-to-real%20RL%0Apipeline%20that%20combines%20vision-language%20model%20%28VLM%29-inferred%20physical%20parameter%0Aestimates%20with%20interactive%20adaptation%20through%20uncertainty-aware%20fusion.%20Our%0Aapproach%20consists%20of%20three%20core%20components%3A%20%281%29%20high-fidelity%20geometric%0Areconstruction%20with%203D%20Gaussian%20splatting%2C%20%282%29%20VLM-inferred%20prior%20distributions%0Aover%20physical%20parameters%2C%20and%20%283%29%20online%20physical%20parameter%20estimation%20from%0Ainteraction%20data.%20Phys2Real%20conditions%20policies%20on%20interpretable%20physical%0Aparameters%2C%20refining%20VLM%20predictions%20with%20online%20estimates%20via%20ensemble-based%0Auncertainty%20quantification.%20On%20planar%20pushing%20tasks%20of%20a%20T-block%20with%20varying%0Acenter%20of%20mass%20%28CoM%29%20and%20a%20hammer%20with%20an%20off-center%20mass%20distribution%2C%0APhys2Real%20achieves%20substantial%20improvements%20over%20a%20domain%20randomization%0Abaseline%3A%20100%25%20vs%2079%25%20success%20rate%20for%20the%20bottom-weighted%20T-block%2C%2057%25%20vs%2023%25%0Ain%20the%20challenging%20top-weighted%20T-block%2C%20and%2015%25%20faster%20average%20task%20completion%0Afor%20hammer%20pushing.%20Ablation%20studies%20indicate%20that%20the%20combination%20of%20VLM%20and%0Ainteraction%20information%20is%20essential%20for%20success.%20Project%20website%3A%0Ahttps%3A//phys2real.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhys2Real%253A%2520Fusing%2520VLM%2520Priors%2520with%2520Interactive%2520Online%2520Adaptation%2520for%250A%2520%2520Uncertainty-Aware%2520Sim-to-Real%2520Manipulation%26entry.906535625%3DMaggie%2520Wang%2520and%2520Stephen%2520Tian%2520and%2520Aiden%2520Swann%2520and%2520Ola%2520Shorinwa%2520and%2520Jiajun%2520Wu%2520and%2520Mac%2520Schwager%26entry.1292438233%3D%2520%2520Learning%2520robotic%2520manipulation%2520policies%2520directly%2520in%2520the%2520real%2520world%2520can%2520be%250Aexpensive%2520and%2520time-consuming.%2520While%2520reinforcement%2520learning%2520%2528RL%2529%2520policies%250Atrained%2520in%2520simulation%2520present%2520a%2520scalable%2520alternative%252C%2520effective%2520sim-to-real%250Atransfer%2520remains%2520challenging%252C%2520particularly%2520for%2520tasks%2520that%2520require%2520precise%250Adynamics.%2520To%2520address%2520this%252C%2520we%2520propose%2520Phys2Real%252C%2520a%2520real-to-sim-to-real%2520RL%250Apipeline%2520that%2520combines%2520vision-language%2520model%2520%2528VLM%2529-inferred%2520physical%2520parameter%250Aestimates%2520with%2520interactive%2520adaptation%2520through%2520uncertainty-aware%2520fusion.%2520Our%250Aapproach%2520consists%2520of%2520three%2520core%2520components%253A%2520%25281%2529%2520high-fidelity%2520geometric%250Areconstruction%2520with%25203D%2520Gaussian%2520splatting%252C%2520%25282%2529%2520VLM-inferred%2520prior%2520distributions%250Aover%2520physical%2520parameters%252C%2520and%2520%25283%2529%2520online%2520physical%2520parameter%2520estimation%2520from%250Ainteraction%2520data.%2520Phys2Real%2520conditions%2520policies%2520on%2520interpretable%2520physical%250Aparameters%252C%2520refining%2520VLM%2520predictions%2520with%2520online%2520estimates%2520via%2520ensemble-based%250Auncertainty%2520quantification.%2520On%2520planar%2520pushing%2520tasks%2520of%2520a%2520T-block%2520with%2520varying%250Acenter%2520of%2520mass%2520%2528CoM%2529%2520and%2520a%2520hammer%2520with%2520an%2520off-center%2520mass%2520distribution%252C%250APhys2Real%2520achieves%2520substantial%2520improvements%2520over%2520a%2520domain%2520randomization%250Abaseline%253A%2520100%2525%2520vs%252079%2525%2520success%2520rate%2520for%2520the%2520bottom-weighted%2520T-block%252C%252057%2525%2520vs%252023%2525%250Ain%2520the%2520challenging%2520top-weighted%2520T-block%252C%2520and%252015%2525%2520faster%2520average%2520task%2520completion%250Afor%2520hammer%2520pushing.%2520Ablation%2520studies%2520indicate%2520that%2520the%2520combination%2520of%2520VLM%2520and%250Ainteraction%2520information%2520is%2520essential%2520for%2520success.%2520Project%2520website%253A%250Ahttps%253A//phys2real.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phys2Real%3A%20Fusing%20VLM%20Priors%20with%20Interactive%20Online%20Adaptation%20for%0A%20%20Uncertainty-Aware%20Sim-to-Real%20Manipulation&entry.906535625=Maggie%20Wang%20and%20Stephen%20Tian%20and%20Aiden%20Swann%20and%20Ola%20Shorinwa%20and%20Jiajun%20Wu%20and%20Mac%20Schwager&entry.1292438233=%20%20Learning%20robotic%20manipulation%20policies%20directly%20in%20the%20real%20world%20can%20be%0Aexpensive%20and%20time-consuming.%20While%20reinforcement%20learning%20%28RL%29%20policies%0Atrained%20in%20simulation%20present%20a%20scalable%20alternative%2C%20effective%20sim-to-real%0Atransfer%20remains%20challenging%2C%20particularly%20for%20tasks%20that%20require%20precise%0Adynamics.%20To%20address%20this%2C%20we%20propose%20Phys2Real%2C%20a%20real-to-sim-to-real%20RL%0Apipeline%20that%20combines%20vision-language%20model%20%28VLM%29-inferred%20physical%20parameter%0Aestimates%20with%20interactive%20adaptation%20through%20uncertainty-aware%20fusion.%20Our%0Aapproach%20consists%20of%20three%20core%20components%3A%20%281%29%20high-fidelity%20geometric%0Areconstruction%20with%203D%20Gaussian%20splatting%2C%20%282%29%20VLM-inferred%20prior%20distributions%0Aover%20physical%20parameters%2C%20and%20%283%29%20online%20physical%20parameter%20estimation%20from%0Ainteraction%20data.%20Phys2Real%20conditions%20policies%20on%20interpretable%20physical%0Aparameters%2C%20refining%20VLM%20predictions%20with%20online%20estimates%20via%20ensemble-based%0Auncertainty%20quantification.%20On%20planar%20pushing%20tasks%20of%20a%20T-block%20with%20varying%0Acenter%20of%20mass%20%28CoM%29%20and%20a%20hammer%20with%20an%20off-center%20mass%20distribution%2C%0APhys2Real%20achieves%20substantial%20improvements%20over%20a%20domain%20randomization%0Abaseline%3A%20100%25%20vs%2079%25%20success%20rate%20for%20the%20bottom-weighted%20T-block%2C%2057%25%20vs%2023%25%0Ain%20the%20challenging%20top-weighted%20T-block%2C%20and%2015%25%20faster%20average%20task%20completion%0Afor%20hammer%20pushing.%20Ablation%20studies%20indicate%20that%20the%20combination%20of%20VLM%20and%0Ainteraction%20information%20is%20essential%20for%20success.%20Project%20website%3A%0Ahttps%3A//phys2real.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11689v1&entry.124074799=Read"},
{"title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking\n  Agents for Financial Misinformation Detection", "author": "Daniel Berhane Araya and Duoduo Liao", "abstract": "  Financial markets face growing threats from misinformation that can trigger\nbillions in losses in minutes. Most existing approaches lack transparency in\ntheir decision-making and provide limited attribution to credible sources. We\nintroduce FinVet, a novel multi-agent framework that integrates two\nRetrieval-Augmented Generation (RAG) pipelines with external fact-checking\nthrough a confidence-weighted voting mechanism. FinVet employs adaptive\nthree-tier processing that dynamically adjusts verification strategies based on\nretrieval confidence, from direct metadata extraction to hybrid reasoning to\nfull model-based analysis. Unlike existing methods, FinVet provides\nevidence-backed verdicts, source attribution, confidence scores, and explicit\nuncertainty flags when evidence is insufficient. Experimental evaluation on the\nFinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a\n10.4% improvement over the best individual pipeline (fact-check pipeline) and\n37% improvement over standalone RAG approaches.\n", "link": "http://arxiv.org/abs/2510.11654v1", "date": "2025-10-13", "relevancy": 1.8401, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4835}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4569}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FinVet%3A%20A%20Collaborative%20Framework%20of%20RAG%20and%20External%20Fact-Checking%0A%20%20Agents%20for%20Financial%20Misinformation%20Detection&body=Title%3A%20FinVet%3A%20A%20Collaborative%20Framework%20of%20RAG%20and%20External%20Fact-Checking%0A%20%20Agents%20for%20Financial%20Misinformation%20Detection%0AAuthor%3A%20Daniel%20Berhane%20Araya%20and%20Duoduo%20Liao%0AAbstract%3A%20%20%20Financial%20markets%20face%20growing%20threats%20from%20misinformation%20that%20can%20trigger%0Abillions%20in%20losses%20in%20minutes.%20Most%20existing%20approaches%20lack%20transparency%20in%0Atheir%20decision-making%20and%20provide%20limited%20attribution%20to%20credible%20sources.%20We%0Aintroduce%20FinVet%2C%20a%20novel%20multi-agent%20framework%20that%20integrates%20two%0ARetrieval-Augmented%20Generation%20%28RAG%29%20pipelines%20with%20external%20fact-checking%0Athrough%20a%20confidence-weighted%20voting%20mechanism.%20FinVet%20employs%20adaptive%0Athree-tier%20processing%20that%20dynamically%20adjusts%20verification%20strategies%20based%20on%0Aretrieval%20confidence%2C%20from%20direct%20metadata%20extraction%20to%20hybrid%20reasoning%20to%0Afull%20model-based%20analysis.%20Unlike%20existing%20methods%2C%20FinVet%20provides%0Aevidence-backed%20verdicts%2C%20source%20attribution%2C%20confidence%20scores%2C%20and%20explicit%0Auncertainty%20flags%20when%20evidence%20is%20insufficient.%20Experimental%20evaluation%20on%20the%0AFinFact%20dataset%20shows%20that%20FinVet%20achieves%20an%20F1%20score%20of%200.85%2C%20which%20is%20a%0A10.4%25%20improvement%20over%20the%20best%20individual%20pipeline%20%28fact-check%20pipeline%29%20and%0A37%25%20improvement%20over%20standalone%20RAG%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinVet%253A%2520A%2520Collaborative%2520Framework%2520of%2520RAG%2520and%2520External%2520Fact-Checking%250A%2520%2520Agents%2520for%2520Financial%2520Misinformation%2520Detection%26entry.906535625%3DDaniel%2520Berhane%2520Araya%2520and%2520Duoduo%2520Liao%26entry.1292438233%3D%2520%2520Financial%2520markets%2520face%2520growing%2520threats%2520from%2520misinformation%2520that%2520can%2520trigger%250Abillions%2520in%2520losses%2520in%2520minutes.%2520Most%2520existing%2520approaches%2520lack%2520transparency%2520in%250Atheir%2520decision-making%2520and%2520provide%2520limited%2520attribution%2520to%2520credible%2520sources.%2520We%250Aintroduce%2520FinVet%252C%2520a%2520novel%2520multi-agent%2520framework%2520that%2520integrates%2520two%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520pipelines%2520with%2520external%2520fact-checking%250Athrough%2520a%2520confidence-weighted%2520voting%2520mechanism.%2520FinVet%2520employs%2520adaptive%250Athree-tier%2520processing%2520that%2520dynamically%2520adjusts%2520verification%2520strategies%2520based%2520on%250Aretrieval%2520confidence%252C%2520from%2520direct%2520metadata%2520extraction%2520to%2520hybrid%2520reasoning%2520to%250Afull%2520model-based%2520analysis.%2520Unlike%2520existing%2520methods%252C%2520FinVet%2520provides%250Aevidence-backed%2520verdicts%252C%2520source%2520attribution%252C%2520confidence%2520scores%252C%2520and%2520explicit%250Auncertainty%2520flags%2520when%2520evidence%2520is%2520insufficient.%2520Experimental%2520evaluation%2520on%2520the%250AFinFact%2520dataset%2520shows%2520that%2520FinVet%2520achieves%2520an%2520F1%2520score%2520of%25200.85%252C%2520which%2520is%2520a%250A10.4%2525%2520improvement%2520over%2520the%2520best%2520individual%2520pipeline%2520%2528fact-check%2520pipeline%2529%2520and%250A37%2525%2520improvement%2520over%2520standalone%2520RAG%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinVet%3A%20A%20Collaborative%20Framework%20of%20RAG%20and%20External%20Fact-Checking%0A%20%20Agents%20for%20Financial%20Misinformation%20Detection&entry.906535625=Daniel%20Berhane%20Araya%20and%20Duoduo%20Liao&entry.1292438233=%20%20Financial%20markets%20face%20growing%20threats%20from%20misinformation%20that%20can%20trigger%0Abillions%20in%20losses%20in%20minutes.%20Most%20existing%20approaches%20lack%20transparency%20in%0Atheir%20decision-making%20and%20provide%20limited%20attribution%20to%20credible%20sources.%20We%0Aintroduce%20FinVet%2C%20a%20novel%20multi-agent%20framework%20that%20integrates%20two%0ARetrieval-Augmented%20Generation%20%28RAG%29%20pipelines%20with%20external%20fact-checking%0Athrough%20a%20confidence-weighted%20voting%20mechanism.%20FinVet%20employs%20adaptive%0Athree-tier%20processing%20that%20dynamically%20adjusts%20verification%20strategies%20based%20on%0Aretrieval%20confidence%2C%20from%20direct%20metadata%20extraction%20to%20hybrid%20reasoning%20to%0Afull%20model-based%20analysis.%20Unlike%20existing%20methods%2C%20FinVet%20provides%0Aevidence-backed%20verdicts%2C%20source%20attribution%2C%20confidence%20scores%2C%20and%20explicit%0Auncertainty%20flags%20when%20evidence%20is%20insufficient.%20Experimental%20evaluation%20on%20the%0AFinFact%20dataset%20shows%20that%20FinVet%20achieves%20an%20F1%20score%20of%200.85%2C%20which%20is%20a%0A10.4%25%20improvement%20over%20the%20best%20individual%20pipeline%20%28fact-check%20pipeline%29%20and%0A37%25%20improvement%20over%20standalone%20RAG%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11654v1&entry.124074799=Read"},
{"title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent\n  Debate", "author": "Andrea Wynn and Harsh Satija and Gillian Hadfield", "abstract": "  While multi-agent debate has been proposed as a promising strategy for\nimproving AI reasoning ability, we find that debate can sometimes be harmful\nrather than helpful. Prior work has primarily focused on debates within\nhomogeneous groups of agents, whereas we explore how diversity in model\ncapabilities influences the dynamics and outcomes of multi-agent interactions.\nThrough a series of experiments, we demonstrate that debate can lead to a\ndecrease in accuracy over time - even in settings where stronger (i.e., more\ncapable) models outnumber their weaker counterparts. Our analysis reveals that\nmodels frequently shift from correct to incorrect answers in response to peer\nreasoning, favoring agreement over challenging flawed reasoning. We perform\nadditional experiments investigating various potential contributing factors to\nthese harmful shifts - including sycophancy, social conformity, and model and\ntask type. These results highlight important failure modes in the exchange of\nreasons during multi-agent debate, suggesting that naive applications of debate\nmay cause performance degradation when agents are neither incentivised nor\nadequately equipped to resist persuasive but incorrect reasoning.\n", "link": "http://arxiv.org/abs/2509.05396v2", "date": "2025-10-13", "relevancy": 1.827, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5055}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4667}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Talk%20Isn%27t%20Always%20Cheap%3A%20Understanding%20Failure%20Modes%20in%20Multi-Agent%0A%20%20Debate&body=Title%3A%20Talk%20Isn%27t%20Always%20Cheap%3A%20Understanding%20Failure%20Modes%20in%20Multi-Agent%0A%20%20Debate%0AAuthor%3A%20Andrea%20Wynn%20and%20Harsh%20Satija%20and%20Gillian%20Hadfield%0AAbstract%3A%20%20%20While%20multi-agent%20debate%20has%20been%20proposed%20as%20a%20promising%20strategy%20for%0Aimproving%20AI%20reasoning%20ability%2C%20we%20find%20that%20debate%20can%20sometimes%20be%20harmful%0Arather%20than%20helpful.%20Prior%20work%20has%20primarily%20focused%20on%20debates%20within%0Ahomogeneous%20groups%20of%20agents%2C%20whereas%20we%20explore%20how%20diversity%20in%20model%0Acapabilities%20influences%20the%20dynamics%20and%20outcomes%20of%20multi-agent%20interactions.%0AThrough%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%20debate%20can%20lead%20to%20a%0Adecrease%20in%20accuracy%20over%20time%20-%20even%20in%20settings%20where%20stronger%20%28i.e.%2C%20more%0Acapable%29%20models%20outnumber%20their%20weaker%20counterparts.%20Our%20analysis%20reveals%20that%0Amodels%20frequently%20shift%20from%20correct%20to%20incorrect%20answers%20in%20response%20to%20peer%0Areasoning%2C%20favoring%20agreement%20over%20challenging%20flawed%20reasoning.%20We%20perform%0Aadditional%20experiments%20investigating%20various%20potential%20contributing%20factors%20to%0Athese%20harmful%20shifts%20-%20including%20sycophancy%2C%20social%20conformity%2C%20and%20model%20and%0Atask%20type.%20These%20results%20highlight%20important%20failure%20modes%20in%20the%20exchange%20of%0Areasons%20during%20multi-agent%20debate%2C%20suggesting%20that%20naive%20applications%20of%20debate%0Amay%20cause%20performance%20degradation%20when%20agents%20are%20neither%20incentivised%20nor%0Aadequately%20equipped%20to%20resist%20persuasive%20but%20incorrect%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05396v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalk%2520Isn%2527t%2520Always%2520Cheap%253A%2520Understanding%2520Failure%2520Modes%2520in%2520Multi-Agent%250A%2520%2520Debate%26entry.906535625%3DAndrea%2520Wynn%2520and%2520Harsh%2520Satija%2520and%2520Gillian%2520Hadfield%26entry.1292438233%3D%2520%2520While%2520multi-agent%2520debate%2520has%2520been%2520proposed%2520as%2520a%2520promising%2520strategy%2520for%250Aimproving%2520AI%2520reasoning%2520ability%252C%2520we%2520find%2520that%2520debate%2520can%2520sometimes%2520be%2520harmful%250Arather%2520than%2520helpful.%2520Prior%2520work%2520has%2520primarily%2520focused%2520on%2520debates%2520within%250Ahomogeneous%2520groups%2520of%2520agents%252C%2520whereas%2520we%2520explore%2520how%2520diversity%2520in%2520model%250Acapabilities%2520influences%2520the%2520dynamics%2520and%2520outcomes%2520of%2520multi-agent%2520interactions.%250AThrough%2520a%2520series%2520of%2520experiments%252C%2520we%2520demonstrate%2520that%2520debate%2520can%2520lead%2520to%2520a%250Adecrease%2520in%2520accuracy%2520over%2520time%2520-%2520even%2520in%2520settings%2520where%2520stronger%2520%2528i.e.%252C%2520more%250Acapable%2529%2520models%2520outnumber%2520their%2520weaker%2520counterparts.%2520Our%2520analysis%2520reveals%2520that%250Amodels%2520frequently%2520shift%2520from%2520correct%2520to%2520incorrect%2520answers%2520in%2520response%2520to%2520peer%250Areasoning%252C%2520favoring%2520agreement%2520over%2520challenging%2520flawed%2520reasoning.%2520We%2520perform%250Aadditional%2520experiments%2520investigating%2520various%2520potential%2520contributing%2520factors%2520to%250Athese%2520harmful%2520shifts%2520-%2520including%2520sycophancy%252C%2520social%2520conformity%252C%2520and%2520model%2520and%250Atask%2520type.%2520These%2520results%2520highlight%2520important%2520failure%2520modes%2520in%2520the%2520exchange%2520of%250Areasons%2520during%2520multi-agent%2520debate%252C%2520suggesting%2520that%2520naive%2520applications%2520of%2520debate%250Amay%2520cause%2520performance%2520degradation%2520when%2520agents%2520are%2520neither%2520incentivised%2520nor%250Aadequately%2520equipped%2520to%2520resist%2520persuasive%2520but%2520incorrect%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05396v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talk%20Isn%27t%20Always%20Cheap%3A%20Understanding%20Failure%20Modes%20in%20Multi-Agent%0A%20%20Debate&entry.906535625=Andrea%20Wynn%20and%20Harsh%20Satija%20and%20Gillian%20Hadfield&entry.1292438233=%20%20While%20multi-agent%20debate%20has%20been%20proposed%20as%20a%20promising%20strategy%20for%0Aimproving%20AI%20reasoning%20ability%2C%20we%20find%20that%20debate%20can%20sometimes%20be%20harmful%0Arather%20than%20helpful.%20Prior%20work%20has%20primarily%20focused%20on%20debates%20within%0Ahomogeneous%20groups%20of%20agents%2C%20whereas%20we%20explore%20how%20diversity%20in%20model%0Acapabilities%20influences%20the%20dynamics%20and%20outcomes%20of%20multi-agent%20interactions.%0AThrough%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%20debate%20can%20lead%20to%20a%0Adecrease%20in%20accuracy%20over%20time%20-%20even%20in%20settings%20where%20stronger%20%28i.e.%2C%20more%0Acapable%29%20models%20outnumber%20their%20weaker%20counterparts.%20Our%20analysis%20reveals%20that%0Amodels%20frequently%20shift%20from%20correct%20to%20incorrect%20answers%20in%20response%20to%20peer%0Areasoning%2C%20favoring%20agreement%20over%20challenging%20flawed%20reasoning.%20We%20perform%0Aadditional%20experiments%20investigating%20various%20potential%20contributing%20factors%20to%0Athese%20harmful%20shifts%20-%20including%20sycophancy%2C%20social%20conformity%2C%20and%20model%20and%0Atask%20type.%20These%20results%20highlight%20important%20failure%20modes%20in%20the%20exchange%20of%0Areasons%20during%20multi-agent%20debate%2C%20suggesting%20that%20naive%20applications%20of%20debate%0Amay%20cause%20performance%20degradation%20when%20agents%20are%20neither%20incentivised%20nor%0Aadequately%20equipped%20to%20resist%20persuasive%20but%20incorrect%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05396v2&entry.124074799=Read"},
{"title": "Hierarchical Qubit-Merging Transformer for Quantum Error Correction", "author": "Seong-Joon Park and Hee-Youl Kwak and Yongjune Kim", "abstract": "  For reliable large-scale quantum computation, a quantum error correction\n(QEC) scheme must effectively resolve physical errors to protect logical\ninformation. Leveraging recent advances in deep learning, neural network-based\ndecoders have emerged as a promising approach to enhance the reliability of\nQEC. We propose the Hierarchical Qubit-Merging Transformer (HQMT), a novel and\ngeneral decoding framework that explicitly leverages the structural graph of\nstabilizer codes to learn error correlations across multiple scales. Our\narchitecture first computes attention locally on structurally related groups of\nstabilizers and then systematically merges these qubit-centric representations\nto build a global view of the error syndrome. The proposed HQMT achieves\nsubstantially lower logical error rates for surface codes by integrating a\ndedicated qubit-merging layer within the transformer architecture. Across\nvarious code distances, HQMT significantly outperforms previous neural\nnetwork-based QEC decoders as well as a powerful belief propagation with\nordered statistics decoding (BP+OSD) baseline. This hierarchical approach\nprovides a scalable and effective framework for surface code decoding,\nadvancing the realization of reliable quantum computing.\n", "link": "http://arxiv.org/abs/2510.11593v1", "date": "2025-10-13", "relevancy": 1.8254, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4901}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Qubit-Merging%20Transformer%20for%20Quantum%20Error%20Correction&body=Title%3A%20Hierarchical%20Qubit-Merging%20Transformer%20for%20Quantum%20Error%20Correction%0AAuthor%3A%20Seong-Joon%20Park%20and%20Hee-Youl%20Kwak%20and%20Yongjune%20Kim%0AAbstract%3A%20%20%20For%20reliable%20large-scale%20quantum%20computation%2C%20a%20quantum%20error%20correction%0A%28QEC%29%20scheme%20must%20effectively%20resolve%20physical%20errors%20to%20protect%20logical%0Ainformation.%20Leveraging%20recent%20advances%20in%20deep%20learning%2C%20neural%20network-based%0Adecoders%20have%20emerged%20as%20a%20promising%20approach%20to%20enhance%20the%20reliability%20of%0AQEC.%20We%20propose%20the%20Hierarchical%20Qubit-Merging%20Transformer%20%28HQMT%29%2C%20a%20novel%20and%0Ageneral%20decoding%20framework%20that%20explicitly%20leverages%20the%20structural%20graph%20of%0Astabilizer%20codes%20to%20learn%20error%20correlations%20across%20multiple%20scales.%20Our%0Aarchitecture%20first%20computes%20attention%20locally%20on%20structurally%20related%20groups%20of%0Astabilizers%20and%20then%20systematically%20merges%20these%20qubit-centric%20representations%0Ato%20build%20a%20global%20view%20of%20the%20error%20syndrome.%20The%20proposed%20HQMT%20achieves%0Asubstantially%20lower%20logical%20error%20rates%20for%20surface%20codes%20by%20integrating%20a%0Adedicated%20qubit-merging%20layer%20within%20the%20transformer%20architecture.%20Across%0Avarious%20code%20distances%2C%20HQMT%20significantly%20outperforms%20previous%20neural%0Anetwork-based%20QEC%20decoders%20as%20well%20as%20a%20powerful%20belief%20propagation%20with%0Aordered%20statistics%20decoding%20%28BP%2BOSD%29%20baseline.%20This%20hierarchical%20approach%0Aprovides%20a%20scalable%20and%20effective%20framework%20for%20surface%20code%20decoding%2C%0Aadvancing%20the%20realization%20of%20reliable%20quantum%20computing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Qubit-Merging%2520Transformer%2520for%2520Quantum%2520Error%2520Correction%26entry.906535625%3DSeong-Joon%2520Park%2520and%2520Hee-Youl%2520Kwak%2520and%2520Yongjune%2520Kim%26entry.1292438233%3D%2520%2520For%2520reliable%2520large-scale%2520quantum%2520computation%252C%2520a%2520quantum%2520error%2520correction%250A%2528QEC%2529%2520scheme%2520must%2520effectively%2520resolve%2520physical%2520errors%2520to%2520protect%2520logical%250Ainformation.%2520Leveraging%2520recent%2520advances%2520in%2520deep%2520learning%252C%2520neural%2520network-based%250Adecoders%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520enhance%2520the%2520reliability%2520of%250AQEC.%2520We%2520propose%2520the%2520Hierarchical%2520Qubit-Merging%2520Transformer%2520%2528HQMT%2529%252C%2520a%2520novel%2520and%250Ageneral%2520decoding%2520framework%2520that%2520explicitly%2520leverages%2520the%2520structural%2520graph%2520of%250Astabilizer%2520codes%2520to%2520learn%2520error%2520correlations%2520across%2520multiple%2520scales.%2520Our%250Aarchitecture%2520first%2520computes%2520attention%2520locally%2520on%2520structurally%2520related%2520groups%2520of%250Astabilizers%2520and%2520then%2520systematically%2520merges%2520these%2520qubit-centric%2520representations%250Ato%2520build%2520a%2520global%2520view%2520of%2520the%2520error%2520syndrome.%2520The%2520proposed%2520HQMT%2520achieves%250Asubstantially%2520lower%2520logical%2520error%2520rates%2520for%2520surface%2520codes%2520by%2520integrating%2520a%250Adedicated%2520qubit-merging%2520layer%2520within%2520the%2520transformer%2520architecture.%2520Across%250Avarious%2520code%2520distances%252C%2520HQMT%2520significantly%2520outperforms%2520previous%2520neural%250Anetwork-based%2520QEC%2520decoders%2520as%2520well%2520as%2520a%2520powerful%2520belief%2520propagation%2520with%250Aordered%2520statistics%2520decoding%2520%2528BP%252BOSD%2529%2520baseline.%2520This%2520hierarchical%2520approach%250Aprovides%2520a%2520scalable%2520and%2520effective%2520framework%2520for%2520surface%2520code%2520decoding%252C%250Aadvancing%2520the%2520realization%2520of%2520reliable%2520quantum%2520computing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Qubit-Merging%20Transformer%20for%20Quantum%20Error%20Correction&entry.906535625=Seong-Joon%20Park%20and%20Hee-Youl%20Kwak%20and%20Yongjune%20Kim&entry.1292438233=%20%20For%20reliable%20large-scale%20quantum%20computation%2C%20a%20quantum%20error%20correction%0A%28QEC%29%20scheme%20must%20effectively%20resolve%20physical%20errors%20to%20protect%20logical%0Ainformation.%20Leveraging%20recent%20advances%20in%20deep%20learning%2C%20neural%20network-based%0Adecoders%20have%20emerged%20as%20a%20promising%20approach%20to%20enhance%20the%20reliability%20of%0AQEC.%20We%20propose%20the%20Hierarchical%20Qubit-Merging%20Transformer%20%28HQMT%29%2C%20a%20novel%20and%0Ageneral%20decoding%20framework%20that%20explicitly%20leverages%20the%20structural%20graph%20of%0Astabilizer%20codes%20to%20learn%20error%20correlations%20across%20multiple%20scales.%20Our%0Aarchitecture%20first%20computes%20attention%20locally%20on%20structurally%20related%20groups%20of%0Astabilizers%20and%20then%20systematically%20merges%20these%20qubit-centric%20representations%0Ato%20build%20a%20global%20view%20of%20the%20error%20syndrome.%20The%20proposed%20HQMT%20achieves%0Asubstantially%20lower%20logical%20error%20rates%20for%20surface%20codes%20by%20integrating%20a%0Adedicated%20qubit-merging%20layer%20within%20the%20transformer%20architecture.%20Across%0Avarious%20code%20distances%2C%20HQMT%20significantly%20outperforms%20previous%20neural%0Anetwork-based%20QEC%20decoders%20as%20well%20as%20a%20powerful%20belief%20propagation%20with%0Aordered%20statistics%20decoding%20%28BP%2BOSD%29%20baseline.%20This%20hierarchical%20approach%0Aprovides%20a%20scalable%20and%20effective%20framework%20for%20surface%20code%20decoding%2C%0Aadvancing%20the%20realization%20of%20reliable%20quantum%20computing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11593v1&entry.124074799=Read"},
{"title": "Ontolearn-A Framework for Large-scale OWL Class Expression Learning in\n  Python", "author": "Caglar Demir and Alkid Baci and N'Dah Jean Kouagou and Leonie Nora Sieger and Stefan Heindorf and Simon Bin and Lukas Bl\u00fcbaum and Alexander Bigerl and Axel-Cyrille Ngonga Ngomo", "abstract": "  In this paper, we present Ontolearn-a framework for learning OWL class\nexpressions over large knowledge graphs. Ontolearn contains efficient\nimplementations of recent stateof-the-art symbolic and neuro-symbolic class\nexpression learners including EvoLearner and DRILL. A learned OWL class\nexpression can be used to classify instances in the knowledge graph.\nFurthermore, Ontolearn integrates a verbalization module based on an LLM to\ntranslate complex OWL class expressions into natural language sentences. By\nmapping OWL class expressions into respective SPARQL queries, Ontolearn can be\neasily used to operate over a remote triplestore. The source code of Ontolearn\nis available at https://github.com/dice-group/Ontolearn.\n", "link": "http://arxiv.org/abs/2510.11561v1", "date": "2025-10-13", "relevancy": 1.8074, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4558}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4558}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ontolearn-A%20Framework%20for%20Large-scale%20OWL%20Class%20Expression%20Learning%20in%0A%20%20Python&body=Title%3A%20Ontolearn-A%20Framework%20for%20Large-scale%20OWL%20Class%20Expression%20Learning%20in%0A%20%20Python%0AAuthor%3A%20Caglar%20Demir%20and%20Alkid%20Baci%20and%20N%27Dah%20Jean%20Kouagou%20and%20Leonie%20Nora%20Sieger%20and%20Stefan%20Heindorf%20and%20Simon%20Bin%20and%20Lukas%20Bl%C3%BCbaum%20and%20Alexander%20Bigerl%20and%20Axel-Cyrille%20Ngonga%20Ngomo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20Ontolearn-a%20framework%20for%20learning%20OWL%20class%0Aexpressions%20over%20large%20knowledge%20graphs.%20Ontolearn%20contains%20efficient%0Aimplementations%20of%20recent%20stateof-the-art%20symbolic%20and%20neuro-symbolic%20class%0Aexpression%20learners%20including%20EvoLearner%20and%20DRILL.%20A%20learned%20OWL%20class%0Aexpression%20can%20be%20used%20to%20classify%20instances%20in%20the%20knowledge%20graph.%0AFurthermore%2C%20Ontolearn%20integrates%20a%20verbalization%20module%20based%20on%20an%20LLM%20to%0Atranslate%20complex%20OWL%20class%20expressions%20into%20natural%20language%20sentences.%20By%0Amapping%20OWL%20class%20expressions%20into%20respective%20SPARQL%20queries%2C%20Ontolearn%20can%20be%0Aeasily%20used%20to%20operate%20over%20a%20remote%20triplestore.%20The%20source%20code%20of%20Ontolearn%0Ais%20available%20at%20https%3A//github.com/dice-group/Ontolearn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOntolearn-A%2520Framework%2520for%2520Large-scale%2520OWL%2520Class%2520Expression%2520Learning%2520in%250A%2520%2520Python%26entry.906535625%3DCaglar%2520Demir%2520and%2520Alkid%2520Baci%2520and%2520N%2527Dah%2520Jean%2520Kouagou%2520and%2520Leonie%2520Nora%2520Sieger%2520and%2520Stefan%2520Heindorf%2520and%2520Simon%2520Bin%2520and%2520Lukas%2520Bl%25C3%25BCbaum%2520and%2520Alexander%2520Bigerl%2520and%2520Axel-Cyrille%2520Ngonga%2520Ngomo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520Ontolearn-a%2520framework%2520for%2520learning%2520OWL%2520class%250Aexpressions%2520over%2520large%2520knowledge%2520graphs.%2520Ontolearn%2520contains%2520efficient%250Aimplementations%2520of%2520recent%2520stateof-the-art%2520symbolic%2520and%2520neuro-symbolic%2520class%250Aexpression%2520learners%2520including%2520EvoLearner%2520and%2520DRILL.%2520A%2520learned%2520OWL%2520class%250Aexpression%2520can%2520be%2520used%2520to%2520classify%2520instances%2520in%2520the%2520knowledge%2520graph.%250AFurthermore%252C%2520Ontolearn%2520integrates%2520a%2520verbalization%2520module%2520based%2520on%2520an%2520LLM%2520to%250Atranslate%2520complex%2520OWL%2520class%2520expressions%2520into%2520natural%2520language%2520sentences.%2520By%250Amapping%2520OWL%2520class%2520expressions%2520into%2520respective%2520SPARQL%2520queries%252C%2520Ontolearn%2520can%2520be%250Aeasily%2520used%2520to%2520operate%2520over%2520a%2520remote%2520triplestore.%2520The%2520source%2520code%2520of%2520Ontolearn%250Ais%2520available%2520at%2520https%253A//github.com/dice-group/Ontolearn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ontolearn-A%20Framework%20for%20Large-scale%20OWL%20Class%20Expression%20Learning%20in%0A%20%20Python&entry.906535625=Caglar%20Demir%20and%20Alkid%20Baci%20and%20N%27Dah%20Jean%20Kouagou%20and%20Leonie%20Nora%20Sieger%20and%20Stefan%20Heindorf%20and%20Simon%20Bin%20and%20Lukas%20Bl%C3%BCbaum%20and%20Alexander%20Bigerl%20and%20Axel-Cyrille%20Ngonga%20Ngomo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20Ontolearn-a%20framework%20for%20learning%20OWL%20class%0Aexpressions%20over%20large%20knowledge%20graphs.%20Ontolearn%20contains%20efficient%0Aimplementations%20of%20recent%20stateof-the-art%20symbolic%20and%20neuro-symbolic%20class%0Aexpression%20learners%20including%20EvoLearner%20and%20DRILL.%20A%20learned%20OWL%20class%0Aexpression%20can%20be%20used%20to%20classify%20instances%20in%20the%20knowledge%20graph.%0AFurthermore%2C%20Ontolearn%20integrates%20a%20verbalization%20module%20based%20on%20an%20LLM%20to%0Atranslate%20complex%20OWL%20class%20expressions%20into%20natural%20language%20sentences.%20By%0Amapping%20OWL%20class%20expressions%20into%20respective%20SPARQL%20queries%2C%20Ontolearn%20can%20be%0Aeasily%20used%20to%20operate%20over%20a%20remote%20triplestore.%20The%20source%20code%20of%20Ontolearn%0Ais%20available%20at%20https%3A//github.com/dice-group/Ontolearn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11561v1&entry.124074799=Read"},
{"title": "Point Prompting: Counterfactual Tracking with Video Diffusion Models", "author": "Ayush Shrivastava and Sanyam Mehta and Daniel Geng and Andrew Owens", "abstract": "  Trackers and video generators solve closely related problems: the former\nanalyze motion, while the latter synthesize it. We show that this connection\nenables pretrained video diffusion models to perform zero-shot point tracking\nby simply prompting them to visually mark points as they move over time. We\nplace a distinctively colored marker at the query point, then regenerate the\nrest of the video from an intermediate noise level. This propagates the marker\nacross frames, tracing the point's trajectory. To ensure that the marker\nremains visible in this counterfactual generation, despite such markers being\nunlikely in natural videos, we use the unedited initial frame as a negative\nprompt. Through experiments with multiple image-conditioned video diffusion\nmodels, we find that these \"emergent\" tracks outperform those of prior\nzero-shot methods and persist through occlusions, often obtaining performance\nthat is competitive with specialized self-supervised models.\n", "link": "http://arxiv.org/abs/2510.11715v1", "date": "2025-10-13", "relevancy": 1.7672, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6247}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5826}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Point%20Prompting%3A%20Counterfactual%20Tracking%20with%20Video%20Diffusion%20Models&body=Title%3A%20Point%20Prompting%3A%20Counterfactual%20Tracking%20with%20Video%20Diffusion%20Models%0AAuthor%3A%20Ayush%20Shrivastava%20and%20Sanyam%20Mehta%20and%20Daniel%20Geng%20and%20Andrew%20Owens%0AAbstract%3A%20%20%20Trackers%20and%20video%20generators%20solve%20closely%20related%20problems%3A%20the%20former%0Aanalyze%20motion%2C%20while%20the%20latter%20synthesize%20it.%20We%20show%20that%20this%20connection%0Aenables%20pretrained%20video%20diffusion%20models%20to%20perform%20zero-shot%20point%20tracking%0Aby%20simply%20prompting%20them%20to%20visually%20mark%20points%20as%20they%20move%20over%20time.%20We%0Aplace%20a%20distinctively%20colored%20marker%20at%20the%20query%20point%2C%20then%20regenerate%20the%0Arest%20of%20the%20video%20from%20an%20intermediate%20noise%20level.%20This%20propagates%20the%20marker%0Aacross%20frames%2C%20tracing%20the%20point%27s%20trajectory.%20To%20ensure%20that%20the%20marker%0Aremains%20visible%20in%20this%20counterfactual%20generation%2C%20despite%20such%20markers%20being%0Aunlikely%20in%20natural%20videos%2C%20we%20use%20the%20unedited%20initial%20frame%20as%20a%20negative%0Aprompt.%20Through%20experiments%20with%20multiple%20image-conditioned%20video%20diffusion%0Amodels%2C%20we%20find%20that%20these%20%22emergent%22%20tracks%20outperform%20those%20of%20prior%0Azero-shot%20methods%20and%20persist%20through%20occlusions%2C%20often%20obtaining%20performance%0Athat%20is%20competitive%20with%20specialized%20self-supervised%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoint%2520Prompting%253A%2520Counterfactual%2520Tracking%2520with%2520Video%2520Diffusion%2520Models%26entry.906535625%3DAyush%2520Shrivastava%2520and%2520Sanyam%2520Mehta%2520and%2520Daniel%2520Geng%2520and%2520Andrew%2520Owens%26entry.1292438233%3D%2520%2520Trackers%2520and%2520video%2520generators%2520solve%2520closely%2520related%2520problems%253A%2520the%2520former%250Aanalyze%2520motion%252C%2520while%2520the%2520latter%2520synthesize%2520it.%2520We%2520show%2520that%2520this%2520connection%250Aenables%2520pretrained%2520video%2520diffusion%2520models%2520to%2520perform%2520zero-shot%2520point%2520tracking%250Aby%2520simply%2520prompting%2520them%2520to%2520visually%2520mark%2520points%2520as%2520they%2520move%2520over%2520time.%2520We%250Aplace%2520a%2520distinctively%2520colored%2520marker%2520at%2520the%2520query%2520point%252C%2520then%2520regenerate%2520the%250Arest%2520of%2520the%2520video%2520from%2520an%2520intermediate%2520noise%2520level.%2520This%2520propagates%2520the%2520marker%250Aacross%2520frames%252C%2520tracing%2520the%2520point%2527s%2520trajectory.%2520To%2520ensure%2520that%2520the%2520marker%250Aremains%2520visible%2520in%2520this%2520counterfactual%2520generation%252C%2520despite%2520such%2520markers%2520being%250Aunlikely%2520in%2520natural%2520videos%252C%2520we%2520use%2520the%2520unedited%2520initial%2520frame%2520as%2520a%2520negative%250Aprompt.%2520Through%2520experiments%2520with%2520multiple%2520image-conditioned%2520video%2520diffusion%250Amodels%252C%2520we%2520find%2520that%2520these%2520%2522emergent%2522%2520tracks%2520outperform%2520those%2520of%2520prior%250Azero-shot%2520methods%2520and%2520persist%2520through%2520occlusions%252C%2520often%2520obtaining%2520performance%250Athat%2520is%2520competitive%2520with%2520specialized%2520self-supervised%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Prompting%3A%20Counterfactual%20Tracking%20with%20Video%20Diffusion%20Models&entry.906535625=Ayush%20Shrivastava%20and%20Sanyam%20Mehta%20and%20Daniel%20Geng%20and%20Andrew%20Owens&entry.1292438233=%20%20Trackers%20and%20video%20generators%20solve%20closely%20related%20problems%3A%20the%20former%0Aanalyze%20motion%2C%20while%20the%20latter%20synthesize%20it.%20We%20show%20that%20this%20connection%0Aenables%20pretrained%20video%20diffusion%20models%20to%20perform%20zero-shot%20point%20tracking%0Aby%20simply%20prompting%20them%20to%20visually%20mark%20points%20as%20they%20move%20over%20time.%20We%0Aplace%20a%20distinctively%20colored%20marker%20at%20the%20query%20point%2C%20then%20regenerate%20the%0Arest%20of%20the%20video%20from%20an%20intermediate%20noise%20level.%20This%20propagates%20the%20marker%0Aacross%20frames%2C%20tracing%20the%20point%27s%20trajectory.%20To%20ensure%20that%20the%20marker%0Aremains%20visible%20in%20this%20counterfactual%20generation%2C%20despite%20such%20markers%20being%0Aunlikely%20in%20natural%20videos%2C%20we%20use%20the%20unedited%20initial%20frame%20as%20a%20negative%0Aprompt.%20Through%20experiments%20with%20multiple%20image-conditioned%20video%20diffusion%0Amodels%2C%20we%20find%20that%20these%20%22emergent%22%20tracks%20outperform%20those%20of%20prior%0Azero-shot%20methods%20and%20persist%20through%20occlusions%2C%20often%20obtaining%20performance%0Athat%20is%20competitive%20with%20specialized%20self-supervised%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11715v1&entry.124074799=Read"},
{"title": "Diffusion-DFL: Decision-focused Diffusion Models for Stochastic\n  Optimization", "author": "Zihao Zhao and Christopher Yeh and Lingkai Kong and Kai Wang", "abstract": "  Decision-focused learning (DFL) integrates predictive modeling and\noptimization by training predictors to optimize the downstream decision target\nrather than merely minimizing prediction error. To date, existing DFL methods\ntypically rely on deterministic point predictions, which are often insufficient\nto capture the intrinsic stochasticity of real-world environments. To address\nthis challenge, we propose the first diffusion-based DFL approach, which trains\na diffusion model to represent the distribution of uncertain parameters and\noptimizes the decision by solving a stochastic optimization with samples drawn\nfrom the diffusion model. Our contributions are twofold. First, we formulate\ndiffusion DFL using the reparameterization trick, enabling end-to-end training\nthrough diffusion. While effective, it is memory and compute-intensive due to\nthe need to differentiate through the diffusion sampling process. Second, we\npropose a lightweight score function estimator that uses only several forward\ndiffusion passes and avoids backpropagation through the sampling. This follows\nfrom our results that backpropagating through stochastic optimization can be\napproximated by a weighted score function formulation. We empirically show that\nour diffusion DFL approach consistently outperforms strong baselines in\ndecision quality. The source code for all experiments is available at the\nproject repository: https://github.com/GT-KOALA/Diffusion_DFL.\n", "link": "http://arxiv.org/abs/2510.11590v1", "date": "2025-10-13", "relevancy": 1.7429, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6344}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5661}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-DFL%3A%20Decision-focused%20Diffusion%20Models%20for%20Stochastic%0A%20%20Optimization&body=Title%3A%20Diffusion-DFL%3A%20Decision-focused%20Diffusion%20Models%20for%20Stochastic%0A%20%20Optimization%0AAuthor%3A%20Zihao%20Zhao%20and%20Christopher%20Yeh%20and%20Lingkai%20Kong%20and%20Kai%20Wang%0AAbstract%3A%20%20%20Decision-focused%20learning%20%28DFL%29%20integrates%20predictive%20modeling%20and%0Aoptimization%20by%20training%20predictors%20to%20optimize%20the%20downstream%20decision%20target%0Arather%20than%20merely%20minimizing%20prediction%20error.%20To%20date%2C%20existing%20DFL%20methods%0Atypically%20rely%20on%20deterministic%20point%20predictions%2C%20which%20are%20often%20insufficient%0Ato%20capture%20the%20intrinsic%20stochasticity%20of%20real-world%20environments.%20To%20address%0Athis%20challenge%2C%20we%20propose%20the%20first%20diffusion-based%20DFL%20approach%2C%20which%20trains%0Aa%20diffusion%20model%20to%20represent%20the%20distribution%20of%20uncertain%20parameters%20and%0Aoptimizes%20the%20decision%20by%20solving%20a%20stochastic%20optimization%20with%20samples%20drawn%0Afrom%20the%20diffusion%20model.%20Our%20contributions%20are%20twofold.%20First%2C%20we%20formulate%0Adiffusion%20DFL%20using%20the%20reparameterization%20trick%2C%20enabling%20end-to-end%20training%0Athrough%20diffusion.%20While%20effective%2C%20it%20is%20memory%20and%20compute-intensive%20due%20to%0Athe%20need%20to%20differentiate%20through%20the%20diffusion%20sampling%20process.%20Second%2C%20we%0Apropose%20a%20lightweight%20score%20function%20estimator%20that%20uses%20only%20several%20forward%0Adiffusion%20passes%20and%20avoids%20backpropagation%20through%20the%20sampling.%20This%20follows%0Afrom%20our%20results%20that%20backpropagating%20through%20stochastic%20optimization%20can%20be%0Aapproximated%20by%20a%20weighted%20score%20function%20formulation.%20We%20empirically%20show%20that%0Aour%20diffusion%20DFL%20approach%20consistently%20outperforms%20strong%20baselines%20in%0Adecision%20quality.%20The%20source%20code%20for%20all%20experiments%20is%20available%20at%20the%0Aproject%20repository%3A%20https%3A//github.com/GT-KOALA/Diffusion_DFL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-DFL%253A%2520Decision-focused%2520Diffusion%2520Models%2520for%2520Stochastic%250A%2520%2520Optimization%26entry.906535625%3DZihao%2520Zhao%2520and%2520Christopher%2520Yeh%2520and%2520Lingkai%2520Kong%2520and%2520Kai%2520Wang%26entry.1292438233%3D%2520%2520Decision-focused%2520learning%2520%2528DFL%2529%2520integrates%2520predictive%2520modeling%2520and%250Aoptimization%2520by%2520training%2520predictors%2520to%2520optimize%2520the%2520downstream%2520decision%2520target%250Arather%2520than%2520merely%2520minimizing%2520prediction%2520error.%2520To%2520date%252C%2520existing%2520DFL%2520methods%250Atypically%2520rely%2520on%2520deterministic%2520point%2520predictions%252C%2520which%2520are%2520often%2520insufficient%250Ato%2520capture%2520the%2520intrinsic%2520stochasticity%2520of%2520real-world%2520environments.%2520To%2520address%250Athis%2520challenge%252C%2520we%2520propose%2520the%2520first%2520diffusion-based%2520DFL%2520approach%252C%2520which%2520trains%250Aa%2520diffusion%2520model%2520to%2520represent%2520the%2520distribution%2520of%2520uncertain%2520parameters%2520and%250Aoptimizes%2520the%2520decision%2520by%2520solving%2520a%2520stochastic%2520optimization%2520with%2520samples%2520drawn%250Afrom%2520the%2520diffusion%2520model.%2520Our%2520contributions%2520are%2520twofold.%2520First%252C%2520we%2520formulate%250Adiffusion%2520DFL%2520using%2520the%2520reparameterization%2520trick%252C%2520enabling%2520end-to-end%2520training%250Athrough%2520diffusion.%2520While%2520effective%252C%2520it%2520is%2520memory%2520and%2520compute-intensive%2520due%2520to%250Athe%2520need%2520to%2520differentiate%2520through%2520the%2520diffusion%2520sampling%2520process.%2520Second%252C%2520we%250Apropose%2520a%2520lightweight%2520score%2520function%2520estimator%2520that%2520uses%2520only%2520several%2520forward%250Adiffusion%2520passes%2520and%2520avoids%2520backpropagation%2520through%2520the%2520sampling.%2520This%2520follows%250Afrom%2520our%2520results%2520that%2520backpropagating%2520through%2520stochastic%2520optimization%2520can%2520be%250Aapproximated%2520by%2520a%2520weighted%2520score%2520function%2520formulation.%2520We%2520empirically%2520show%2520that%250Aour%2520diffusion%2520DFL%2520approach%2520consistently%2520outperforms%2520strong%2520baselines%2520in%250Adecision%2520quality.%2520The%2520source%2520code%2520for%2520all%2520experiments%2520is%2520available%2520at%2520the%250Aproject%2520repository%253A%2520https%253A//github.com/GT-KOALA/Diffusion_DFL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-DFL%3A%20Decision-focused%20Diffusion%20Models%20for%20Stochastic%0A%20%20Optimization&entry.906535625=Zihao%20Zhao%20and%20Christopher%20Yeh%20and%20Lingkai%20Kong%20and%20Kai%20Wang&entry.1292438233=%20%20Decision-focused%20learning%20%28DFL%29%20integrates%20predictive%20modeling%20and%0Aoptimization%20by%20training%20predictors%20to%20optimize%20the%20downstream%20decision%20target%0Arather%20than%20merely%20minimizing%20prediction%20error.%20To%20date%2C%20existing%20DFL%20methods%0Atypically%20rely%20on%20deterministic%20point%20predictions%2C%20which%20are%20often%20insufficient%0Ato%20capture%20the%20intrinsic%20stochasticity%20of%20real-world%20environments.%20To%20address%0Athis%20challenge%2C%20we%20propose%20the%20first%20diffusion-based%20DFL%20approach%2C%20which%20trains%0Aa%20diffusion%20model%20to%20represent%20the%20distribution%20of%20uncertain%20parameters%20and%0Aoptimizes%20the%20decision%20by%20solving%20a%20stochastic%20optimization%20with%20samples%20drawn%0Afrom%20the%20diffusion%20model.%20Our%20contributions%20are%20twofold.%20First%2C%20we%20formulate%0Adiffusion%20DFL%20using%20the%20reparameterization%20trick%2C%20enabling%20end-to-end%20training%0Athrough%20diffusion.%20While%20effective%2C%20it%20is%20memory%20and%20compute-intensive%20due%20to%0Athe%20need%20to%20differentiate%20through%20the%20diffusion%20sampling%20process.%20Second%2C%20we%0Apropose%20a%20lightweight%20score%20function%20estimator%20that%20uses%20only%20several%20forward%0Adiffusion%20passes%20and%20avoids%20backpropagation%20through%20the%20sampling.%20This%20follows%0Afrom%20our%20results%20that%20backpropagating%20through%20stochastic%20optimization%20can%20be%0Aapproximated%20by%20a%20weighted%20score%20function%20formulation.%20We%20empirically%20show%20that%0Aour%20diffusion%20DFL%20approach%20consistently%20outperforms%20strong%20baselines%20in%0Adecision%20quality.%20The%20source%20code%20for%20all%20experiments%20is%20available%20at%20the%0Aproject%20repository%3A%20https%3A//github.com/GT-KOALA/Diffusion_DFL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11590v1&entry.124074799=Read"},
{"title": "Provably faster randomized and quantum algorithms for $k$-means\n  clustering via uniform sampling", "author": "Tyler Chen and Archan Ray and Akshay Seshadri and Dylan Herman and Bao Bach and Pranav Deshpande and Abhishek Som and Niraj Kumar and Marco Pistoia", "abstract": "  The $k$-means algorithm (Lloyd's algorithm) is a widely used method for\nclustering unlabeled data. A key bottleneck of the $k$-means algorithm is that\neach iteration requires time linear in the number of data points, which can be\nexpensive in big data applications. This was improved in recent works proposing\nquantum and quantum-inspired classical algorithms to approximate the $k$-means\nalgorithm locally, in time depending only logarithmically on the number of data\npoints (along with data dependent parameters) [q-means: A quantum algorithm for\nunsupervised machine learning, Kerenidis, Landman, Luongo, and Prakash, NeurIPS\n2019; Do you know what $q$-means?, Cornelissen, Doriguello, Luongo, Tang, QTML\n2025]. In this work, we describe a simple randomized mini-batch $k$-means\nalgorithm and a quantum algorithm inspired by the classical algorithm. We\ndemonstrate that the worst case guarantees of these algorithms can\nsignificantly improve upon the bounds for algorithms in prior work. Our\nimprovements are due to a careful use of uniform sampling, which preserves\ncertain symmetries of the $k$-means problem that are not preserved in previous\nalgorithms that use data norm-based sampling.\n", "link": "http://arxiv.org/abs/2504.20982v3", "date": "2025-10-13", "relevancy": 1.7226, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4501}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4229}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably%20faster%20randomized%20and%20quantum%20algorithms%20for%20%24k%24-means%0A%20%20clustering%20via%20uniform%20sampling&body=Title%3A%20Provably%20faster%20randomized%20and%20quantum%20algorithms%20for%20%24k%24-means%0A%20%20clustering%20via%20uniform%20sampling%0AAuthor%3A%20Tyler%20Chen%20and%20Archan%20Ray%20and%20Akshay%20Seshadri%20and%20Dylan%20Herman%20and%20Bao%20Bach%20and%20Pranav%20Deshpande%20and%20Abhishek%20Som%20and%20Niraj%20Kumar%20and%20Marco%20Pistoia%0AAbstract%3A%20%20%20The%20%24k%24-means%20algorithm%20%28Lloyd%27s%20algorithm%29%20is%20a%20widely%20used%20method%20for%0Aclustering%20unlabeled%20data.%20A%20key%20bottleneck%20of%20the%20%24k%24-means%20algorithm%20is%20that%0Aeach%20iteration%20requires%20time%20linear%20in%20the%20number%20of%20data%20points%2C%20which%20can%20be%0Aexpensive%20in%20big%20data%20applications.%20This%20was%20improved%20in%20recent%20works%20proposing%0Aquantum%20and%20quantum-inspired%20classical%20algorithms%20to%20approximate%20the%20%24k%24-means%0Aalgorithm%20locally%2C%20in%20time%20depending%20only%20logarithmically%20on%20the%20number%20of%20data%0Apoints%20%28along%20with%20data%20dependent%20parameters%29%20%5Bq-means%3A%20A%20quantum%20algorithm%20for%0Aunsupervised%20machine%20learning%2C%20Kerenidis%2C%20Landman%2C%20Luongo%2C%20and%20Prakash%2C%20NeurIPS%0A2019%3B%20Do%20you%20know%20what%20%24q%24-means%3F%2C%20Cornelissen%2C%20Doriguello%2C%20Luongo%2C%20Tang%2C%20QTML%0A2025%5D.%20In%20this%20work%2C%20we%20describe%20a%20simple%20randomized%20mini-batch%20%24k%24-means%0Aalgorithm%20and%20a%20quantum%20algorithm%20inspired%20by%20the%20classical%20algorithm.%20We%0Ademonstrate%20that%20the%20worst%20case%20guarantees%20of%20these%20algorithms%20can%0Asignificantly%20improve%20upon%20the%20bounds%20for%20algorithms%20in%20prior%20work.%20Our%0Aimprovements%20are%20due%20to%20a%20careful%20use%20of%20uniform%20sampling%2C%20which%20preserves%0Acertain%20symmetries%20of%20the%20%24k%24-means%20problem%20that%20are%20not%20preserved%20in%20previous%0Aalgorithms%20that%20use%20data%20norm-based%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20982v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably%2520faster%2520randomized%2520and%2520quantum%2520algorithms%2520for%2520%2524k%2524-means%250A%2520%2520clustering%2520via%2520uniform%2520sampling%26entry.906535625%3DTyler%2520Chen%2520and%2520Archan%2520Ray%2520and%2520Akshay%2520Seshadri%2520and%2520Dylan%2520Herman%2520and%2520Bao%2520Bach%2520and%2520Pranav%2520Deshpande%2520and%2520Abhishek%2520Som%2520and%2520Niraj%2520Kumar%2520and%2520Marco%2520Pistoia%26entry.1292438233%3D%2520%2520The%2520%2524k%2524-means%2520algorithm%2520%2528Lloyd%2527s%2520algorithm%2529%2520is%2520a%2520widely%2520used%2520method%2520for%250Aclustering%2520unlabeled%2520data.%2520A%2520key%2520bottleneck%2520of%2520the%2520%2524k%2524-means%2520algorithm%2520is%2520that%250Aeach%2520iteration%2520requires%2520time%2520linear%2520in%2520the%2520number%2520of%2520data%2520points%252C%2520which%2520can%2520be%250Aexpensive%2520in%2520big%2520data%2520applications.%2520This%2520was%2520improved%2520in%2520recent%2520works%2520proposing%250Aquantum%2520and%2520quantum-inspired%2520classical%2520algorithms%2520to%2520approximate%2520the%2520%2524k%2524-means%250Aalgorithm%2520locally%252C%2520in%2520time%2520depending%2520only%2520logarithmically%2520on%2520the%2520number%2520of%2520data%250Apoints%2520%2528along%2520with%2520data%2520dependent%2520parameters%2529%2520%255Bq-means%253A%2520A%2520quantum%2520algorithm%2520for%250Aunsupervised%2520machine%2520learning%252C%2520Kerenidis%252C%2520Landman%252C%2520Luongo%252C%2520and%2520Prakash%252C%2520NeurIPS%250A2019%253B%2520Do%2520you%2520know%2520what%2520%2524q%2524-means%253F%252C%2520Cornelissen%252C%2520Doriguello%252C%2520Luongo%252C%2520Tang%252C%2520QTML%250A2025%255D.%2520In%2520this%2520work%252C%2520we%2520describe%2520a%2520simple%2520randomized%2520mini-batch%2520%2524k%2524-means%250Aalgorithm%2520and%2520a%2520quantum%2520algorithm%2520inspired%2520by%2520the%2520classical%2520algorithm.%2520We%250Ademonstrate%2520that%2520the%2520worst%2520case%2520guarantees%2520of%2520these%2520algorithms%2520can%250Asignificantly%2520improve%2520upon%2520the%2520bounds%2520for%2520algorithms%2520in%2520prior%2520work.%2520Our%250Aimprovements%2520are%2520due%2520to%2520a%2520careful%2520use%2520of%2520uniform%2520sampling%252C%2520which%2520preserves%250Acertain%2520symmetries%2520of%2520the%2520%2524k%2524-means%2520problem%2520that%2520are%2520not%2520preserved%2520in%2520previous%250Aalgorithms%2520that%2520use%2520data%2520norm-based%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20982v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20faster%20randomized%20and%20quantum%20algorithms%20for%20%24k%24-means%0A%20%20clustering%20via%20uniform%20sampling&entry.906535625=Tyler%20Chen%20and%20Archan%20Ray%20and%20Akshay%20Seshadri%20and%20Dylan%20Herman%20and%20Bao%20Bach%20and%20Pranav%20Deshpande%20and%20Abhishek%20Som%20and%20Niraj%20Kumar%20and%20Marco%20Pistoia&entry.1292438233=%20%20The%20%24k%24-means%20algorithm%20%28Lloyd%27s%20algorithm%29%20is%20a%20widely%20used%20method%20for%0Aclustering%20unlabeled%20data.%20A%20key%20bottleneck%20of%20the%20%24k%24-means%20algorithm%20is%20that%0Aeach%20iteration%20requires%20time%20linear%20in%20the%20number%20of%20data%20points%2C%20which%20can%20be%0Aexpensive%20in%20big%20data%20applications.%20This%20was%20improved%20in%20recent%20works%20proposing%0Aquantum%20and%20quantum-inspired%20classical%20algorithms%20to%20approximate%20the%20%24k%24-means%0Aalgorithm%20locally%2C%20in%20time%20depending%20only%20logarithmically%20on%20the%20number%20of%20data%0Apoints%20%28along%20with%20data%20dependent%20parameters%29%20%5Bq-means%3A%20A%20quantum%20algorithm%20for%0Aunsupervised%20machine%20learning%2C%20Kerenidis%2C%20Landman%2C%20Luongo%2C%20and%20Prakash%2C%20NeurIPS%0A2019%3B%20Do%20you%20know%20what%20%24q%24-means%3F%2C%20Cornelissen%2C%20Doriguello%2C%20Luongo%2C%20Tang%2C%20QTML%0A2025%5D.%20In%20this%20work%2C%20we%20describe%20a%20simple%20randomized%20mini-batch%20%24k%24-means%0Aalgorithm%20and%20a%20quantum%20algorithm%20inspired%20by%20the%20classical%20algorithm.%20We%0Ademonstrate%20that%20the%20worst%20case%20guarantees%20of%20these%20algorithms%20can%0Asignificantly%20improve%20upon%20the%20bounds%20for%20algorithms%20in%20prior%20work.%20Our%0Aimprovements%20are%20due%20to%20a%20careful%20use%20of%20uniform%20sampling%2C%20which%20preserves%0Acertain%20symmetries%20of%20the%20%24k%24-means%20problem%20that%20are%20not%20preserved%20in%20previous%0Aalgorithms%20that%20use%20data%20norm-based%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20982v3&entry.124074799=Read"},
{"title": "ManiAgent: An Agentic Framework for General Robotic Manipulation", "author": "Yi Yang and Kefan Gu and Yuqing Wen and Hebei Li and Yucheng Zhao and Tiancai Wang and Xudong Liu", "abstract": "  While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets.The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/.\n", "link": "http://arxiv.org/abs/2510.11660v1", "date": "2025-10-13", "relevancy": 1.722, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6278}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5772}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ManiAgent%3A%20An%20Agentic%20Framework%20for%20General%20Robotic%20Manipulation&body=Title%3A%20ManiAgent%3A%20An%20Agentic%20Framework%20for%20General%20Robotic%20Manipulation%0AAuthor%3A%20Yi%20Yang%20and%20Kefan%20Gu%20and%20Yuqing%20Wen%20and%20Hebei%20Li%20and%20Yucheng%20Zhao%20and%20Tiancai%20Wang%20and%20Xudong%20Liu%0AAbstract%3A%20%20%20While%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20impressive%0Acapabilities%20in%20robotic%20manipulation%2C%20their%20performance%20in%20complex%20reasoning%0Aand%20long-horizon%20task%20planning%20is%20limited%20by%20data%20scarcity%20and%20model%20capacity.%0ATo%20address%20this%2C%20we%20introduce%20ManiAgent%2C%20an%20agentic%20architecture%20for%20general%0Amanipulation%20tasks%20that%20achieves%20end-to-end%20output%20from%20task%20descriptions%20and%0Aenvironmental%20inputs%20to%20robotic%20manipulation%20actions.%20In%20this%20framework%2C%0Amultiple%20agents%20involve%20inter-agent%20communication%20to%20perform%20environmental%0Aperception%2C%20sub-task%20decomposition%20and%20action%20generation%2C%20enabling%20efficient%0Ahandling%20of%20complex%20manipulation%20scenarios.%20Evaluations%20show%20ManiAgent%20achieves%0Aan%2086.8%25%20success%20rate%20on%20the%20SimplerEnv%20benchmark%20and%2095.8%25%20on%20real-world%0Apick-and-place%20tasks%2C%20enabling%20efficient%20data%20collection%20that%20yields%20VLA%20models%0Awith%20performance%20comparable%20to%20those%20trained%20on%20human-annotated%20datasets.The%0Aproject%20webpage%20is%20available%20at%20https%3A//yi-yang929.github.io/ManiAgent/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManiAgent%253A%2520An%2520Agentic%2520Framework%2520for%2520General%2520Robotic%2520Manipulation%26entry.906535625%3DYi%2520Yang%2520and%2520Kefan%2520Gu%2520and%2520Yuqing%2520Wen%2520and%2520Hebei%2520Li%2520and%2520Yucheng%2520Zhao%2520and%2520Tiancai%2520Wang%2520and%2520Xudong%2520Liu%26entry.1292438233%3D%2520%2520While%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520demonstrated%2520impressive%250Acapabilities%2520in%2520robotic%2520manipulation%252C%2520their%2520performance%2520in%2520complex%2520reasoning%250Aand%2520long-horizon%2520task%2520planning%2520is%2520limited%2520by%2520data%2520scarcity%2520and%2520model%2520capacity.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520ManiAgent%252C%2520an%2520agentic%2520architecture%2520for%2520general%250Amanipulation%2520tasks%2520that%2520achieves%2520end-to-end%2520output%2520from%2520task%2520descriptions%2520and%250Aenvironmental%2520inputs%2520to%2520robotic%2520manipulation%2520actions.%2520In%2520this%2520framework%252C%250Amultiple%2520agents%2520involve%2520inter-agent%2520communication%2520to%2520perform%2520environmental%250Aperception%252C%2520sub-task%2520decomposition%2520and%2520action%2520generation%252C%2520enabling%2520efficient%250Ahandling%2520of%2520complex%2520manipulation%2520scenarios.%2520Evaluations%2520show%2520ManiAgent%2520achieves%250Aan%252086.8%2525%2520success%2520rate%2520on%2520the%2520SimplerEnv%2520benchmark%2520and%252095.8%2525%2520on%2520real-world%250Apick-and-place%2520tasks%252C%2520enabling%2520efficient%2520data%2520collection%2520that%2520yields%2520VLA%2520models%250Awith%2520performance%2520comparable%2520to%2520those%2520trained%2520on%2520human-annotated%2520datasets.The%250Aproject%2520webpage%2520is%2520available%2520at%2520https%253A//yi-yang929.github.io/ManiAgent/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ManiAgent%3A%20An%20Agentic%20Framework%20for%20General%20Robotic%20Manipulation&entry.906535625=Yi%20Yang%20and%20Kefan%20Gu%20and%20Yuqing%20Wen%20and%20Hebei%20Li%20and%20Yucheng%20Zhao%20and%20Tiancai%20Wang%20and%20Xudong%20Liu&entry.1292438233=%20%20While%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20impressive%0Acapabilities%20in%20robotic%20manipulation%2C%20their%20performance%20in%20complex%20reasoning%0Aand%20long-horizon%20task%20planning%20is%20limited%20by%20data%20scarcity%20and%20model%20capacity.%0ATo%20address%20this%2C%20we%20introduce%20ManiAgent%2C%20an%20agentic%20architecture%20for%20general%0Amanipulation%20tasks%20that%20achieves%20end-to-end%20output%20from%20task%20descriptions%20and%0Aenvironmental%20inputs%20to%20robotic%20manipulation%20actions.%20In%20this%20framework%2C%0Amultiple%20agents%20involve%20inter-agent%20communication%20to%20perform%20environmental%0Aperception%2C%20sub-task%20decomposition%20and%20action%20generation%2C%20enabling%20efficient%0Ahandling%20of%20complex%20manipulation%20scenarios.%20Evaluations%20show%20ManiAgent%20achieves%0Aan%2086.8%25%20success%20rate%20on%20the%20SimplerEnv%20benchmark%20and%2095.8%25%20on%20real-world%0Apick-and-place%20tasks%2C%20enabling%20efficient%20data%20collection%20that%20yields%20VLA%20models%0Awith%20performance%20comparable%20to%20those%20trained%20on%20human-annotated%20datasets.The%0Aproject%20webpage%20is%20available%20at%20https%3A//yi-yang929.github.io/ManiAgent/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11660v1&entry.124074799=Read"},
{"title": "Cost-aware Stopping for Bayesian Optimization", "author": "Qian Xie and Linda Cai and Alexander Terenin and Peter I. Frazier and Ziv Scully", "abstract": "  In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nusually matches or outperforms other acquisition-function--stopping-rule pairs\nin terms of cost-adjusted simple regret, a metric capturing trade-offs between\nsolution quality and cumulative evaluation cost.\n", "link": "http://arxiv.org/abs/2507.12453v3", "date": "2025-10-13", "relevancy": 1.7097, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4743}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4299}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cost-aware%20Stopping%20for%20Bayesian%20Optimization&body=Title%3A%20Cost-aware%20Stopping%20for%20Bayesian%20Optimization%0AAuthor%3A%20Qian%20Xie%20and%20Linda%20Cai%20and%20Alexander%20Terenin%20and%20Peter%20I.%20Frazier%20and%20Ziv%20Scully%0AAbstract%3A%20%20%20In%20automated%20machine%20learning%2C%20scientific%20discovery%2C%20and%20other%20applications%0Aof%20Bayesian%20optimization%2C%20deciding%20when%20to%20stop%20evaluating%20expensive%20black-box%0Afunctions%20is%20an%20important%20practical%20consideration.%20While%20several%20adaptive%0Astopping%20rules%20have%20been%20proposed%2C%20in%20the%20cost-aware%20setting%20they%20lack%0Aguarantees%20ensuring%20they%20stop%20before%20incurring%20excessive%20function%20evaluation%0Acosts.%20We%20propose%20a%20cost-aware%20stopping%20rule%20for%20Bayesian%20optimization%20that%0Aadapts%20to%20varying%20evaluation%20costs%20and%20is%20free%20of%20heuristic%20tuning.%20Our%20rule%20is%0Agrounded%20in%20a%20theoretical%20connection%20to%20state-of-the-art%20cost-aware%20acquisition%0Afunctions%2C%20namely%20the%20Pandora%27s%20Box%20Gittins%20Index%20%28PBGI%29%20and%20log%20expected%0Aimprovement%20per%20cost.%20We%20prove%20a%20theoretical%20guarantee%20bounding%20the%20expected%0Acumulative%20evaluation%20cost%20incurred%20by%20our%20stopping%20rule%20when%20paired%20with%20these%0Atwo%20acquisition%20functions.%20In%20experiments%20on%20synthetic%20and%20empirical%20tasks%2C%0Aincluding%20hyperparameter%20optimization%20and%20neural%20architecture%20size%20search%2C%20we%0Ashow%20that%20combining%20our%20stopping%20rule%20with%20the%20PBGI%20acquisition%20function%0Ausually%20matches%20or%20outperforms%20other%20acquisition-function--stopping-rule%20pairs%0Ain%20terms%20of%20cost-adjusted%20simple%20regret%2C%20a%20metric%20capturing%20trade-offs%20between%0Asolution%20quality%20and%20cumulative%20evaluation%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12453v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCost-aware%2520Stopping%2520for%2520Bayesian%2520Optimization%26entry.906535625%3DQian%2520Xie%2520and%2520Linda%2520Cai%2520and%2520Alexander%2520Terenin%2520and%2520Peter%2520I.%2520Frazier%2520and%2520Ziv%2520Scully%26entry.1292438233%3D%2520%2520In%2520automated%2520machine%2520learning%252C%2520scientific%2520discovery%252C%2520and%2520other%2520applications%250Aof%2520Bayesian%2520optimization%252C%2520deciding%2520when%2520to%2520stop%2520evaluating%2520expensive%2520black-box%250Afunctions%2520is%2520an%2520important%2520practical%2520consideration.%2520While%2520several%2520adaptive%250Astopping%2520rules%2520have%2520been%2520proposed%252C%2520in%2520the%2520cost-aware%2520setting%2520they%2520lack%250Aguarantees%2520ensuring%2520they%2520stop%2520before%2520incurring%2520excessive%2520function%2520evaluation%250Acosts.%2520We%2520propose%2520a%2520cost-aware%2520stopping%2520rule%2520for%2520Bayesian%2520optimization%2520that%250Aadapts%2520to%2520varying%2520evaluation%2520costs%2520and%2520is%2520free%2520of%2520heuristic%2520tuning.%2520Our%2520rule%2520is%250Agrounded%2520in%2520a%2520theoretical%2520connection%2520to%2520state-of-the-art%2520cost-aware%2520acquisition%250Afunctions%252C%2520namely%2520the%2520Pandora%2527s%2520Box%2520Gittins%2520Index%2520%2528PBGI%2529%2520and%2520log%2520expected%250Aimprovement%2520per%2520cost.%2520We%2520prove%2520a%2520theoretical%2520guarantee%2520bounding%2520the%2520expected%250Acumulative%2520evaluation%2520cost%2520incurred%2520by%2520our%2520stopping%2520rule%2520when%2520paired%2520with%2520these%250Atwo%2520acquisition%2520functions.%2520In%2520experiments%2520on%2520synthetic%2520and%2520empirical%2520tasks%252C%250Aincluding%2520hyperparameter%2520optimization%2520and%2520neural%2520architecture%2520size%2520search%252C%2520we%250Ashow%2520that%2520combining%2520our%2520stopping%2520rule%2520with%2520the%2520PBGI%2520acquisition%2520function%250Ausually%2520matches%2520or%2520outperforms%2520other%2520acquisition-function--stopping-rule%2520pairs%250Ain%2520terms%2520of%2520cost-adjusted%2520simple%2520regret%252C%2520a%2520metric%2520capturing%2520trade-offs%2520between%250Asolution%2520quality%2520and%2520cumulative%2520evaluation%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12453v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cost-aware%20Stopping%20for%20Bayesian%20Optimization&entry.906535625=Qian%20Xie%20and%20Linda%20Cai%20and%20Alexander%20Terenin%20and%20Peter%20I.%20Frazier%20and%20Ziv%20Scully&entry.1292438233=%20%20In%20automated%20machine%20learning%2C%20scientific%20discovery%2C%20and%20other%20applications%0Aof%20Bayesian%20optimization%2C%20deciding%20when%20to%20stop%20evaluating%20expensive%20black-box%0Afunctions%20is%20an%20important%20practical%20consideration.%20While%20several%20adaptive%0Astopping%20rules%20have%20been%20proposed%2C%20in%20the%20cost-aware%20setting%20they%20lack%0Aguarantees%20ensuring%20they%20stop%20before%20incurring%20excessive%20function%20evaluation%0Acosts.%20We%20propose%20a%20cost-aware%20stopping%20rule%20for%20Bayesian%20optimization%20that%0Aadapts%20to%20varying%20evaluation%20costs%20and%20is%20free%20of%20heuristic%20tuning.%20Our%20rule%20is%0Agrounded%20in%20a%20theoretical%20connection%20to%20state-of-the-art%20cost-aware%20acquisition%0Afunctions%2C%20namely%20the%20Pandora%27s%20Box%20Gittins%20Index%20%28PBGI%29%20and%20log%20expected%0Aimprovement%20per%20cost.%20We%20prove%20a%20theoretical%20guarantee%20bounding%20the%20expected%0Acumulative%20evaluation%20cost%20incurred%20by%20our%20stopping%20rule%20when%20paired%20with%20these%0Atwo%20acquisition%20functions.%20In%20experiments%20on%20synthetic%20and%20empirical%20tasks%2C%0Aincluding%20hyperparameter%20optimization%20and%20neural%20architecture%20size%20search%2C%20we%0Ashow%20that%20combining%20our%20stopping%20rule%20with%20the%20PBGI%20acquisition%20function%0Ausually%20matches%20or%20outperforms%20other%20acquisition-function--stopping-rule%20pairs%0Ain%20terms%20of%20cost-adjusted%20simple%20regret%2C%20a%20metric%20capturing%20trade-offs%20between%0Asolution%20quality%20and%20cumulative%20evaluation%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12453v3&entry.124074799=Read"},
{"title": "ACE-G: Improving Generalization of Scene Coordinate Regression Through\n  Query Pre-Training", "author": "Leonard Bruns and Axel Barroso-Laguna and Tommaso Cavallari and \u00c1ron Monszpart and Sowmya Munukutla and Victor Adrian Prisacariu and Eric Brachmann", "abstract": "  Scene coordinate regression (SCR) has established itself as a promising\nlearning-based approach to visual relocalization. After mere minutes of\nscene-specific training, SCR models estimate camera poses of query images with\nhigh accuracy. Still, SCR methods fall short of the generalization capabilities\nof more classical feature-matching approaches. When imaging conditions of query\nimages, such as lighting or viewpoint, are too different from the training\nviews, SCR models fail. Failing to generalize is an inherent limitation of\nprevious SCR frameworks, since their training objective is to encode the\ntraining views in the weights of the coordinate regressor itself. The regressor\nessentially overfits to the training views, by design. We propose to separate\nthe coordinate regressor and the map representation into a generic transformer\nand a scene-specific map code. This separation allows us to pre-train the\ntransformer on tens of thousands of scenes. More importantly, it allows us to\ntrain the transformer to generalize from mapping images to unseen query images\nduring pre-training. We demonstrate on multiple challenging relocalization\ndatasets that our method, ACE-G, leads to significantly increased robustness\nwhile keeping the computational footprint attractive.\n", "link": "http://arxiv.org/abs/2510.11605v1", "date": "2025-10-13", "relevancy": 1.7093, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6068}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5603}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACE-G%3A%20Improving%20Generalization%20of%20Scene%20Coordinate%20Regression%20Through%0A%20%20Query%20Pre-Training&body=Title%3A%20ACE-G%3A%20Improving%20Generalization%20of%20Scene%20Coordinate%20Regression%20Through%0A%20%20Query%20Pre-Training%0AAuthor%3A%20Leonard%20Bruns%20and%20Axel%20Barroso-Laguna%20and%20Tommaso%20Cavallari%20and%20%C3%81ron%20Monszpart%20and%20Sowmya%20Munukutla%20and%20Victor%20Adrian%20Prisacariu%20and%20Eric%20Brachmann%0AAbstract%3A%20%20%20Scene%20coordinate%20regression%20%28SCR%29%20has%20established%20itself%20as%20a%20promising%0Alearning-based%20approach%20to%20visual%20relocalization.%20After%20mere%20minutes%20of%0Ascene-specific%20training%2C%20SCR%20models%20estimate%20camera%20poses%20of%20query%20images%20with%0Ahigh%20accuracy.%20Still%2C%20SCR%20methods%20fall%20short%20of%20the%20generalization%20capabilities%0Aof%20more%20classical%20feature-matching%20approaches.%20When%20imaging%20conditions%20of%20query%0Aimages%2C%20such%20as%20lighting%20or%20viewpoint%2C%20are%20too%20different%20from%20the%20training%0Aviews%2C%20SCR%20models%20fail.%20Failing%20to%20generalize%20is%20an%20inherent%20limitation%20of%0Aprevious%20SCR%20frameworks%2C%20since%20their%20training%20objective%20is%20to%20encode%20the%0Atraining%20views%20in%20the%20weights%20of%20the%20coordinate%20regressor%20itself.%20The%20regressor%0Aessentially%20overfits%20to%20the%20training%20views%2C%20by%20design.%20We%20propose%20to%20separate%0Athe%20coordinate%20regressor%20and%20the%20map%20representation%20into%20a%20generic%20transformer%0Aand%20a%20scene-specific%20map%20code.%20This%20separation%20allows%20us%20to%20pre-train%20the%0Atransformer%20on%20tens%20of%20thousands%20of%20scenes.%20More%20importantly%2C%20it%20allows%20us%20to%0Atrain%20the%20transformer%20to%20generalize%20from%20mapping%20images%20to%20unseen%20query%20images%0Aduring%20pre-training.%20We%20demonstrate%20on%20multiple%20challenging%20relocalization%0Adatasets%20that%20our%20method%2C%20ACE-G%2C%20leads%20to%20significantly%20increased%20robustness%0Awhile%20keeping%20the%20computational%20footprint%20attractive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACE-G%253A%2520Improving%2520Generalization%2520of%2520Scene%2520Coordinate%2520Regression%2520Through%250A%2520%2520Query%2520Pre-Training%26entry.906535625%3DLeonard%2520Bruns%2520and%2520Axel%2520Barroso-Laguna%2520and%2520Tommaso%2520Cavallari%2520and%2520%25C3%2581ron%2520Monszpart%2520and%2520Sowmya%2520Munukutla%2520and%2520Victor%2520Adrian%2520Prisacariu%2520and%2520Eric%2520Brachmann%26entry.1292438233%3D%2520%2520Scene%2520coordinate%2520regression%2520%2528SCR%2529%2520has%2520established%2520itself%2520as%2520a%2520promising%250Alearning-based%2520approach%2520to%2520visual%2520relocalization.%2520After%2520mere%2520minutes%2520of%250Ascene-specific%2520training%252C%2520SCR%2520models%2520estimate%2520camera%2520poses%2520of%2520query%2520images%2520with%250Ahigh%2520accuracy.%2520Still%252C%2520SCR%2520methods%2520fall%2520short%2520of%2520the%2520generalization%2520capabilities%250Aof%2520more%2520classical%2520feature-matching%2520approaches.%2520When%2520imaging%2520conditions%2520of%2520query%250Aimages%252C%2520such%2520as%2520lighting%2520or%2520viewpoint%252C%2520are%2520too%2520different%2520from%2520the%2520training%250Aviews%252C%2520SCR%2520models%2520fail.%2520Failing%2520to%2520generalize%2520is%2520an%2520inherent%2520limitation%2520of%250Aprevious%2520SCR%2520frameworks%252C%2520since%2520their%2520training%2520objective%2520is%2520to%2520encode%2520the%250Atraining%2520views%2520in%2520the%2520weights%2520of%2520the%2520coordinate%2520regressor%2520itself.%2520The%2520regressor%250Aessentially%2520overfits%2520to%2520the%2520training%2520views%252C%2520by%2520design.%2520We%2520propose%2520to%2520separate%250Athe%2520coordinate%2520regressor%2520and%2520the%2520map%2520representation%2520into%2520a%2520generic%2520transformer%250Aand%2520a%2520scene-specific%2520map%2520code.%2520This%2520separation%2520allows%2520us%2520to%2520pre-train%2520the%250Atransformer%2520on%2520tens%2520of%2520thousands%2520of%2520scenes.%2520More%2520importantly%252C%2520it%2520allows%2520us%2520to%250Atrain%2520the%2520transformer%2520to%2520generalize%2520from%2520mapping%2520images%2520to%2520unseen%2520query%2520images%250Aduring%2520pre-training.%2520We%2520demonstrate%2520on%2520multiple%2520challenging%2520relocalization%250Adatasets%2520that%2520our%2520method%252C%2520ACE-G%252C%2520leads%2520to%2520significantly%2520increased%2520robustness%250Awhile%2520keeping%2520the%2520computational%2520footprint%2520attractive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACE-G%3A%20Improving%20Generalization%20of%20Scene%20Coordinate%20Regression%20Through%0A%20%20Query%20Pre-Training&entry.906535625=Leonard%20Bruns%20and%20Axel%20Barroso-Laguna%20and%20Tommaso%20Cavallari%20and%20%C3%81ron%20Monszpart%20and%20Sowmya%20Munukutla%20and%20Victor%20Adrian%20Prisacariu%20and%20Eric%20Brachmann&entry.1292438233=%20%20Scene%20coordinate%20regression%20%28SCR%29%20has%20established%20itself%20as%20a%20promising%0Alearning-based%20approach%20to%20visual%20relocalization.%20After%20mere%20minutes%20of%0Ascene-specific%20training%2C%20SCR%20models%20estimate%20camera%20poses%20of%20query%20images%20with%0Ahigh%20accuracy.%20Still%2C%20SCR%20methods%20fall%20short%20of%20the%20generalization%20capabilities%0Aof%20more%20classical%20feature-matching%20approaches.%20When%20imaging%20conditions%20of%20query%0Aimages%2C%20such%20as%20lighting%20or%20viewpoint%2C%20are%20too%20different%20from%20the%20training%0Aviews%2C%20SCR%20models%20fail.%20Failing%20to%20generalize%20is%20an%20inherent%20limitation%20of%0Aprevious%20SCR%20frameworks%2C%20since%20their%20training%20objective%20is%20to%20encode%20the%0Atraining%20views%20in%20the%20weights%20of%20the%20coordinate%20regressor%20itself.%20The%20regressor%0Aessentially%20overfits%20to%20the%20training%20views%2C%20by%20design.%20We%20propose%20to%20separate%0Athe%20coordinate%20regressor%20and%20the%20map%20representation%20into%20a%20generic%20transformer%0Aand%20a%20scene-specific%20map%20code.%20This%20separation%20allows%20us%20to%20pre-train%20the%0Atransformer%20on%20tens%20of%20thousands%20of%20scenes.%20More%20importantly%2C%20it%20allows%20us%20to%0Atrain%20the%20transformer%20to%20generalize%20from%20mapping%20images%20to%20unseen%20query%20images%0Aduring%20pre-training.%20We%20demonstrate%20on%20multiple%20challenging%20relocalization%0Adatasets%20that%20our%20method%2C%20ACE-G%2C%20leads%20to%20significantly%20increased%20robustness%0Awhile%20keeping%20the%20computational%20footprint%20attractive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11605v1&entry.124074799=Read"},
{"title": "Explainability, risk modeling, and segmentation based customer churn\n  analytics for personalized retention in e-commerce", "author": "Sanjula De Alwis and Indrajith Ekanayake", "abstract": "  In online retail, customer acquisition typically incurs higher costs than\ncustomer retention, motivating firms to invest in churn analytics. However,\nmany contemporary churn models operate as opaque black boxes, limiting insight\ninto the determinants of attrition, the timing of retention opportunities, and\nthe identification of high-risk customer segments. Accordingly, the emphasis\nshould shift from prediction alone to the design of personalized retention\nstrategies grounded in interpretable evidence. This study advances a\nthree-component framework that integrates explainable AI to quantify feature\ncontributions, survival analysis to model time-to-event churn risk, and RFM\nprofiling to segment customers by transactional behaviour. In combination,\nthese methods enable the attribution of churn drivers, estimation of\nintervention windows, and prioritization of segments for targeted actions,\nthereby supporting strategies that reduce attrition and strengthen customer\nloyalty.\n", "link": "http://arxiv.org/abs/2510.11604v1", "date": "2025-10-13", "relevancy": 1.6843, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.425}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4238}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainability%2C%20risk%20modeling%2C%20and%20segmentation%20based%20customer%20churn%0A%20%20analytics%20for%20personalized%20retention%20in%20e-commerce&body=Title%3A%20Explainability%2C%20risk%20modeling%2C%20and%20segmentation%20based%20customer%20churn%0A%20%20analytics%20for%20personalized%20retention%20in%20e-commerce%0AAuthor%3A%20Sanjula%20De%20Alwis%20and%20Indrajith%20Ekanayake%0AAbstract%3A%20%20%20In%20online%20retail%2C%20customer%20acquisition%20typically%20incurs%20higher%20costs%20than%0Acustomer%20retention%2C%20motivating%20firms%20to%20invest%20in%20churn%20analytics.%20However%2C%0Amany%20contemporary%20churn%20models%20operate%20as%20opaque%20black%20boxes%2C%20limiting%20insight%0Ainto%20the%20determinants%20of%20attrition%2C%20the%20timing%20of%20retention%20opportunities%2C%20and%0Athe%20identification%20of%20high-risk%20customer%20segments.%20Accordingly%2C%20the%20emphasis%0Ashould%20shift%20from%20prediction%20alone%20to%20the%20design%20of%20personalized%20retention%0Astrategies%20grounded%20in%20interpretable%20evidence.%20This%20study%20advances%20a%0Athree-component%20framework%20that%20integrates%20explainable%20AI%20to%20quantify%20feature%0Acontributions%2C%20survival%20analysis%20to%20model%20time-to-event%20churn%20risk%2C%20and%20RFM%0Aprofiling%20to%20segment%20customers%20by%20transactional%20behaviour.%20In%20combination%2C%0Athese%20methods%20enable%20the%20attribution%20of%20churn%20drivers%2C%20estimation%20of%0Aintervention%20windows%2C%20and%20prioritization%20of%20segments%20for%20targeted%20actions%2C%0Athereby%20supporting%20strategies%20that%20reduce%20attrition%20and%20strengthen%20customer%0Aloyalty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainability%252C%2520risk%2520modeling%252C%2520and%2520segmentation%2520based%2520customer%2520churn%250A%2520%2520analytics%2520for%2520personalized%2520retention%2520in%2520e-commerce%26entry.906535625%3DSanjula%2520De%2520Alwis%2520and%2520Indrajith%2520Ekanayake%26entry.1292438233%3D%2520%2520In%2520online%2520retail%252C%2520customer%2520acquisition%2520typically%2520incurs%2520higher%2520costs%2520than%250Acustomer%2520retention%252C%2520motivating%2520firms%2520to%2520invest%2520in%2520churn%2520analytics.%2520However%252C%250Amany%2520contemporary%2520churn%2520models%2520operate%2520as%2520opaque%2520black%2520boxes%252C%2520limiting%2520insight%250Ainto%2520the%2520determinants%2520of%2520attrition%252C%2520the%2520timing%2520of%2520retention%2520opportunities%252C%2520and%250Athe%2520identification%2520of%2520high-risk%2520customer%2520segments.%2520Accordingly%252C%2520the%2520emphasis%250Ashould%2520shift%2520from%2520prediction%2520alone%2520to%2520the%2520design%2520of%2520personalized%2520retention%250Astrategies%2520grounded%2520in%2520interpretable%2520evidence.%2520This%2520study%2520advances%2520a%250Athree-component%2520framework%2520that%2520integrates%2520explainable%2520AI%2520to%2520quantify%2520feature%250Acontributions%252C%2520survival%2520analysis%2520to%2520model%2520time-to-event%2520churn%2520risk%252C%2520and%2520RFM%250Aprofiling%2520to%2520segment%2520customers%2520by%2520transactional%2520behaviour.%2520In%2520combination%252C%250Athese%2520methods%2520enable%2520the%2520attribution%2520of%2520churn%2520drivers%252C%2520estimation%2520of%250Aintervention%2520windows%252C%2520and%2520prioritization%2520of%2520segments%2520for%2520targeted%2520actions%252C%250Athereby%2520supporting%2520strategies%2520that%2520reduce%2520attrition%2520and%2520strengthen%2520customer%250Aloyalty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainability%2C%20risk%20modeling%2C%20and%20segmentation%20based%20customer%20churn%0A%20%20analytics%20for%20personalized%20retention%20in%20e-commerce&entry.906535625=Sanjula%20De%20Alwis%20and%20Indrajith%20Ekanayake&entry.1292438233=%20%20In%20online%20retail%2C%20customer%20acquisition%20typically%20incurs%20higher%20costs%20than%0Acustomer%20retention%2C%20motivating%20firms%20to%20invest%20in%20churn%20analytics.%20However%2C%0Amany%20contemporary%20churn%20models%20operate%20as%20opaque%20black%20boxes%2C%20limiting%20insight%0Ainto%20the%20determinants%20of%20attrition%2C%20the%20timing%20of%20retention%20opportunities%2C%20and%0Athe%20identification%20of%20high-risk%20customer%20segments.%20Accordingly%2C%20the%20emphasis%0Ashould%20shift%20from%20prediction%20alone%20to%20the%20design%20of%20personalized%20retention%0Astrategies%20grounded%20in%20interpretable%20evidence.%20This%20study%20advances%20a%0Athree-component%20framework%20that%20integrates%20explainable%20AI%20to%20quantify%20feature%0Acontributions%2C%20survival%20analysis%20to%20model%20time-to-event%20churn%20risk%2C%20and%20RFM%0Aprofiling%20to%20segment%20customers%20by%20transactional%20behaviour.%20In%20combination%2C%0Athese%20methods%20enable%20the%20attribution%20of%20churn%20drivers%2C%20estimation%20of%0Aintervention%20windows%2C%20and%20prioritization%20of%20segments%20for%20targeted%20actions%2C%0Athereby%20supporting%20strategies%20that%20reduce%20attrition%20and%20strengthen%20customer%0Aloyalty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11604v1&entry.124074799=Read"},
{"title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative\n  Policy", "author": "Kuanning Wang and Yongchong Gu and Yuqian Fu and Zeyu Shangguan and Sicheng He and Xiangyang Xue and Yanwei Fu and Daniel Seita", "abstract": "  Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/.\n", "link": "http://arxiv.org/abs/2510.11566v1", "date": "2025-10-13", "relevancy": 1.6515, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6086}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5405}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCOOP%27D%3A%20Learning%20Mixed-Liquid-Solid%20Scooping%20via%20Sim2Real%20Generative%0A%20%20Policy&body=Title%3A%20SCOOP%27D%3A%20Learning%20Mixed-Liquid-Solid%20Scooping%20via%20Sim2Real%20Generative%0A%20%20Policy%0AAuthor%3A%20Kuanning%20Wang%20and%20Yongchong%20Gu%20and%20Yuqian%20Fu%20and%20Zeyu%20Shangguan%20and%20Sicheng%20He%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu%20and%20Daniel%20Seita%0AAbstract%3A%20%20%20Scooping%20items%20with%20tools%20such%20as%20spoons%20and%20ladles%20is%20common%20in%20daily%20life%2C%0Aranging%20from%20assistive%20feeding%20to%20retrieving%20items%20from%20environmental%20disaster%0Asites.%20However%2C%20developing%20a%20general%20and%20autonomous%20robotic%20scooping%20policy%20is%0Achallenging%20since%20it%20requires%20reasoning%20about%20complex%20tool-object%20interactions.%0AFurthermore%2C%20scooping%20often%20involves%20manipulating%20deformable%20objects%2C%20such%20as%0Agranular%20media%20or%20liquids%2C%20which%20is%20challenging%20due%20to%20their%0Ainfinite-dimensional%20configuration%20spaces%20and%20complex%20dynamics.%20We%20propose%20a%0Amethod%2C%20SCOOP%27D%2C%20which%20uses%20simulation%20from%20OmniGibson%20%28built%20on%20NVIDIA%0AOmniverse%29%20to%20collect%20scooping%20demonstrations%20using%20algorithmic%20procedures%20that%0Arely%20on%20privileged%20state%20information.%20Then%2C%20we%20use%20generative%20policies%20via%0Adiffusion%20to%20imitate%20demonstrations%20from%20observational%20input.%20We%20directly%20apply%0Athe%20learned%20policy%20in%20diverse%20real-world%20scenarios%2C%20testing%20its%20performance%20on%0Avarious%20item%20quantities%2C%20item%20characteristics%2C%20and%20container%20types.%20In%0Azero-shot%20deployment%2C%20our%20method%20demonstrates%20promising%20results%20across%20465%0Atrials%20in%20diverse%20scenarios%2C%20including%20objects%20of%20different%20difficulty%20levels%0Athat%20we%20categorize%20as%20%22Level%201%22%20and%20%22Level%202.%22%20SCOOP%27D%20outperforms%20all%0Abaselines%20and%20ablations%2C%20suggesting%20that%20this%20is%20a%20promising%20approach%20to%0Aacquiring%20robotic%20scooping%20skills.%20Project%20page%20is%20at%0Ahttps%3A//scoopdiff.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCOOP%2527D%253A%2520Learning%2520Mixed-Liquid-Solid%2520Scooping%2520via%2520Sim2Real%2520Generative%250A%2520%2520Policy%26entry.906535625%3DKuanning%2520Wang%2520and%2520Yongchong%2520Gu%2520and%2520Yuqian%2520Fu%2520and%2520Zeyu%2520Shangguan%2520and%2520Sicheng%2520He%2520and%2520Xiangyang%2520Xue%2520and%2520Yanwei%2520Fu%2520and%2520Daniel%2520Seita%26entry.1292438233%3D%2520%2520Scooping%2520items%2520with%2520tools%2520such%2520as%2520spoons%2520and%2520ladles%2520is%2520common%2520in%2520daily%2520life%252C%250Aranging%2520from%2520assistive%2520feeding%2520to%2520retrieving%2520items%2520from%2520environmental%2520disaster%250Asites.%2520However%252C%2520developing%2520a%2520general%2520and%2520autonomous%2520robotic%2520scooping%2520policy%2520is%250Achallenging%2520since%2520it%2520requires%2520reasoning%2520about%2520complex%2520tool-object%2520interactions.%250AFurthermore%252C%2520scooping%2520often%2520involves%2520manipulating%2520deformable%2520objects%252C%2520such%2520as%250Agranular%2520media%2520or%2520liquids%252C%2520which%2520is%2520challenging%2520due%2520to%2520their%250Ainfinite-dimensional%2520configuration%2520spaces%2520and%2520complex%2520dynamics.%2520We%2520propose%2520a%250Amethod%252C%2520SCOOP%2527D%252C%2520which%2520uses%2520simulation%2520from%2520OmniGibson%2520%2528built%2520on%2520NVIDIA%250AOmniverse%2529%2520to%2520collect%2520scooping%2520demonstrations%2520using%2520algorithmic%2520procedures%2520that%250Arely%2520on%2520privileged%2520state%2520information.%2520Then%252C%2520we%2520use%2520generative%2520policies%2520via%250Adiffusion%2520to%2520imitate%2520demonstrations%2520from%2520observational%2520input.%2520We%2520directly%2520apply%250Athe%2520learned%2520policy%2520in%2520diverse%2520real-world%2520scenarios%252C%2520testing%2520its%2520performance%2520on%250Avarious%2520item%2520quantities%252C%2520item%2520characteristics%252C%2520and%2520container%2520types.%2520In%250Azero-shot%2520deployment%252C%2520our%2520method%2520demonstrates%2520promising%2520results%2520across%2520465%250Atrials%2520in%2520diverse%2520scenarios%252C%2520including%2520objects%2520of%2520different%2520difficulty%2520levels%250Athat%2520we%2520categorize%2520as%2520%2522Level%25201%2522%2520and%2520%2522Level%25202.%2522%2520SCOOP%2527D%2520outperforms%2520all%250Abaselines%2520and%2520ablations%252C%2520suggesting%2520that%2520this%2520is%2520a%2520promising%2520approach%2520to%250Aacquiring%2520robotic%2520scooping%2520skills.%2520Project%2520page%2520is%2520at%250Ahttps%253A//scoopdiff.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCOOP%27D%3A%20Learning%20Mixed-Liquid-Solid%20Scooping%20via%20Sim2Real%20Generative%0A%20%20Policy&entry.906535625=Kuanning%20Wang%20and%20Yongchong%20Gu%20and%20Yuqian%20Fu%20and%20Zeyu%20Shangguan%20and%20Sicheng%20He%20and%20Xiangyang%20Xue%20and%20Yanwei%20Fu%20and%20Daniel%20Seita&entry.1292438233=%20%20Scooping%20items%20with%20tools%20such%20as%20spoons%20and%20ladles%20is%20common%20in%20daily%20life%2C%0Aranging%20from%20assistive%20feeding%20to%20retrieving%20items%20from%20environmental%20disaster%0Asites.%20However%2C%20developing%20a%20general%20and%20autonomous%20robotic%20scooping%20policy%20is%0Achallenging%20since%20it%20requires%20reasoning%20about%20complex%20tool-object%20interactions.%0AFurthermore%2C%20scooping%20often%20involves%20manipulating%20deformable%20objects%2C%20such%20as%0Agranular%20media%20or%20liquids%2C%20which%20is%20challenging%20due%20to%20their%0Ainfinite-dimensional%20configuration%20spaces%20and%20complex%20dynamics.%20We%20propose%20a%0Amethod%2C%20SCOOP%27D%2C%20which%20uses%20simulation%20from%20OmniGibson%20%28built%20on%20NVIDIA%0AOmniverse%29%20to%20collect%20scooping%20demonstrations%20using%20algorithmic%20procedures%20that%0Arely%20on%20privileged%20state%20information.%20Then%2C%20we%20use%20generative%20policies%20via%0Adiffusion%20to%20imitate%20demonstrations%20from%20observational%20input.%20We%20directly%20apply%0Athe%20learned%20policy%20in%20diverse%20real-world%20scenarios%2C%20testing%20its%20performance%20on%0Avarious%20item%20quantities%2C%20item%20characteristics%2C%20and%20container%20types.%20In%0Azero-shot%20deployment%2C%20our%20method%20demonstrates%20promising%20results%20across%20465%0Atrials%20in%20diverse%20scenarios%2C%20including%20objects%20of%20different%20difficulty%20levels%0Athat%20we%20categorize%20as%20%22Level%201%22%20and%20%22Level%202.%22%20SCOOP%27D%20outperforms%20all%0Abaselines%20and%20ablations%2C%20suggesting%20that%20this%20is%20a%20promising%20approach%20to%0Aacquiring%20robotic%20scooping%20skills.%20Project%20page%20is%20at%0Ahttps%3A//scoopdiff.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11566v1&entry.124074799=Read"},
{"title": "Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient\n  Delta Compression", "author": "Xiaohui Wang and Peng Ye and Chenyu Huang and Shenghe Zheng and Bo Zhang and Lei Bai and Wanli Ouyang and Tao Chen", "abstract": "  With the rise of the fine-tuned-pretrained paradigm, storing numerous\nfine-tuned models for multi-tasking creates significant storage overhead. Delta\ncompression alleviates this by storing only the pretrained model and the highly\ncompressed delta weights (the differences between fine-tuned and pretrained\nmodel weights). However, existing methods fail to maintain both high\ncompression and performance, and often rely on data. To address these\nchallenges, we propose UltraDelta, the first data-free delta compression\npipeline that achieves both ultra-high compression and strong performance.\nUltraDelta is designed to minimize redundancy, maximize information, and\nstabilize performance across inter-layer, intra-layer, and global dimensions,\nusing three key components: (1) Variance-Based Mixed Sparsity Allocation\nassigns sparsity based on variance, giving lower sparsity to high-variance\nlayers to preserve inter-layer information. (2) Distribution-Aware Compression\napplies uniform quantization and then groups parameters by value, followed by\ngroup-wise pruning, to better preserve intra-layer distribution. (3)\nTrace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a\nglobal rescaling factor, improving model stability under higher compression.\nExtensive experiments across (a) large language models (fine-tuned on LLaMA-2\n7B and 13B) with up to 50x compression, (b) general NLP models (RoBERTa-base,\nT5-base) with up to 224x compression, (c) vision models (ViT-B/32, ViT-L/14)\nwith up to 132x compression, and (d) multi-modal models (BEiT-3) with 18x\ncompression, demonstrate that UltraDelta consistently outperforms existing\nmethods, especially under ultra-high compression. Code is available at\nhttps://github.com/xiaohuiwang000/UltraDelta.\n", "link": "http://arxiv.org/abs/2505.13563v3", "date": "2025-10-13", "relevancy": 1.6182, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.555}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5447}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Compression%20Ceiling%3A%20Data-Free%20Pipeline%20for%20Ultra-Efficient%0A%20%20Delta%20Compression&body=Title%3A%20Breaking%20the%20Compression%20Ceiling%3A%20Data-Free%20Pipeline%20for%20Ultra-Efficient%0A%20%20Delta%20Compression%0AAuthor%3A%20Xiaohui%20Wang%20and%20Peng%20Ye%20and%20Chenyu%20Huang%20and%20Shenghe%20Zheng%20and%20Bo%20Zhang%20and%20Lei%20Bai%20and%20Wanli%20Ouyang%20and%20Tao%20Chen%0AAbstract%3A%20%20%20With%20the%20rise%20of%20the%20fine-tuned-pretrained%20paradigm%2C%20storing%20numerous%0Afine-tuned%20models%20for%20multi-tasking%20creates%20significant%20storage%20overhead.%20Delta%0Acompression%20alleviates%20this%20by%20storing%20only%20the%20pretrained%20model%20and%20the%20highly%0Acompressed%20delta%20weights%20%28the%20differences%20between%20fine-tuned%20and%20pretrained%0Amodel%20weights%29.%20However%2C%20existing%20methods%20fail%20to%20maintain%20both%20high%0Acompression%20and%20performance%2C%20and%20often%20rely%20on%20data.%20To%20address%20these%0Achallenges%2C%20we%20propose%20UltraDelta%2C%20the%20first%20data-free%20delta%20compression%0Apipeline%20that%20achieves%20both%20ultra-high%20compression%20and%20strong%20performance.%0AUltraDelta%20is%20designed%20to%20minimize%20redundancy%2C%20maximize%20information%2C%20and%0Astabilize%20performance%20across%20inter-layer%2C%20intra-layer%2C%20and%20global%20dimensions%2C%0Ausing%20three%20key%20components%3A%20%281%29%20Variance-Based%20Mixed%20Sparsity%20Allocation%0Aassigns%20sparsity%20based%20on%20variance%2C%20giving%20lower%20sparsity%20to%20high-variance%0Alayers%20to%20preserve%20inter-layer%20information.%20%282%29%20Distribution-Aware%20Compression%0Aapplies%20uniform%20quantization%20and%20then%20groups%20parameters%20by%20value%2C%20followed%20by%0Agroup-wise%20pruning%2C%20to%20better%20preserve%20intra-layer%20distribution.%20%283%29%0ATrace-Norm-Guided%20Rescaling%20uses%20the%20trace%20norm%20of%20delta%20weights%20to%20estimate%20a%0Aglobal%20rescaling%20factor%2C%20improving%20model%20stability%20under%20higher%20compression.%0AExtensive%20experiments%20across%20%28a%29%20large%20language%20models%20%28fine-tuned%20on%20LLaMA-2%0A7B%20and%2013B%29%20with%20up%20to%2050x%20compression%2C%20%28b%29%20general%20NLP%20models%20%28RoBERTa-base%2C%0AT5-base%29%20with%20up%20to%20224x%20compression%2C%20%28c%29%20vision%20models%20%28ViT-B/32%2C%20ViT-L/14%29%0Awith%20up%20to%20132x%20compression%2C%20and%20%28d%29%20multi-modal%20models%20%28BEiT-3%29%20with%2018x%0Acompression%2C%20demonstrate%20that%20UltraDelta%20consistently%20outperforms%20existing%0Amethods%2C%20especially%20under%20ultra-high%20compression.%20Code%20is%20available%20at%0Ahttps%3A//github.com/xiaohuiwang000/UltraDelta.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.13563v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Compression%2520Ceiling%253A%2520Data-Free%2520Pipeline%2520for%2520Ultra-Efficient%250A%2520%2520Delta%2520Compression%26entry.906535625%3DXiaohui%2520Wang%2520and%2520Peng%2520Ye%2520and%2520Chenyu%2520Huang%2520and%2520Shenghe%2520Zheng%2520and%2520Bo%2520Zhang%2520and%2520Lei%2520Bai%2520and%2520Wanli%2520Ouyang%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520With%2520the%2520rise%2520of%2520the%2520fine-tuned-pretrained%2520paradigm%252C%2520storing%2520numerous%250Afine-tuned%2520models%2520for%2520multi-tasking%2520creates%2520significant%2520storage%2520overhead.%2520Delta%250Acompression%2520alleviates%2520this%2520by%2520storing%2520only%2520the%2520pretrained%2520model%2520and%2520the%2520highly%250Acompressed%2520delta%2520weights%2520%2528the%2520differences%2520between%2520fine-tuned%2520and%2520pretrained%250Amodel%2520weights%2529.%2520However%252C%2520existing%2520methods%2520fail%2520to%2520maintain%2520both%2520high%250Acompression%2520and%2520performance%252C%2520and%2520often%2520rely%2520on%2520data.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520UltraDelta%252C%2520the%2520first%2520data-free%2520delta%2520compression%250Apipeline%2520that%2520achieves%2520both%2520ultra-high%2520compression%2520and%2520strong%2520performance.%250AUltraDelta%2520is%2520designed%2520to%2520minimize%2520redundancy%252C%2520maximize%2520information%252C%2520and%250Astabilize%2520performance%2520across%2520inter-layer%252C%2520intra-layer%252C%2520and%2520global%2520dimensions%252C%250Ausing%2520three%2520key%2520components%253A%2520%25281%2529%2520Variance-Based%2520Mixed%2520Sparsity%2520Allocation%250Aassigns%2520sparsity%2520based%2520on%2520variance%252C%2520giving%2520lower%2520sparsity%2520to%2520high-variance%250Alayers%2520to%2520preserve%2520inter-layer%2520information.%2520%25282%2529%2520Distribution-Aware%2520Compression%250Aapplies%2520uniform%2520quantization%2520and%2520then%2520groups%2520parameters%2520by%2520value%252C%2520followed%2520by%250Agroup-wise%2520pruning%252C%2520to%2520better%2520preserve%2520intra-layer%2520distribution.%2520%25283%2529%250ATrace-Norm-Guided%2520Rescaling%2520uses%2520the%2520trace%2520norm%2520of%2520delta%2520weights%2520to%2520estimate%2520a%250Aglobal%2520rescaling%2520factor%252C%2520improving%2520model%2520stability%2520under%2520higher%2520compression.%250AExtensive%2520experiments%2520across%2520%2528a%2529%2520large%2520language%2520models%2520%2528fine-tuned%2520on%2520LLaMA-2%250A7B%2520and%252013B%2529%2520with%2520up%2520to%252050x%2520compression%252C%2520%2528b%2529%2520general%2520NLP%2520models%2520%2528RoBERTa-base%252C%250AT5-base%2529%2520with%2520up%2520to%2520224x%2520compression%252C%2520%2528c%2529%2520vision%2520models%2520%2528ViT-B/32%252C%2520ViT-L/14%2529%250Awith%2520up%2520to%2520132x%2520compression%252C%2520and%2520%2528d%2529%2520multi-modal%2520models%2520%2528BEiT-3%2529%2520with%252018x%250Acompression%252C%2520demonstrate%2520that%2520UltraDelta%2520consistently%2520outperforms%2520existing%250Amethods%252C%2520especially%2520under%2520ultra-high%2520compression.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/xiaohuiwang000/UltraDelta.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.13563v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Compression%20Ceiling%3A%20Data-Free%20Pipeline%20for%20Ultra-Efficient%0A%20%20Delta%20Compression&entry.906535625=Xiaohui%20Wang%20and%20Peng%20Ye%20and%20Chenyu%20Huang%20and%20Shenghe%20Zheng%20and%20Bo%20Zhang%20and%20Lei%20Bai%20and%20Wanli%20Ouyang%20and%20Tao%20Chen&entry.1292438233=%20%20With%20the%20rise%20of%20the%20fine-tuned-pretrained%20paradigm%2C%20storing%20numerous%0Afine-tuned%20models%20for%20multi-tasking%20creates%20significant%20storage%20overhead.%20Delta%0Acompression%20alleviates%20this%20by%20storing%20only%20the%20pretrained%20model%20and%20the%20highly%0Acompressed%20delta%20weights%20%28the%20differences%20between%20fine-tuned%20and%20pretrained%0Amodel%20weights%29.%20However%2C%20existing%20methods%20fail%20to%20maintain%20both%20high%0Acompression%20and%20performance%2C%20and%20often%20rely%20on%20data.%20To%20address%20these%0Achallenges%2C%20we%20propose%20UltraDelta%2C%20the%20first%20data-free%20delta%20compression%0Apipeline%20that%20achieves%20both%20ultra-high%20compression%20and%20strong%20performance.%0AUltraDelta%20is%20designed%20to%20minimize%20redundancy%2C%20maximize%20information%2C%20and%0Astabilize%20performance%20across%20inter-layer%2C%20intra-layer%2C%20and%20global%20dimensions%2C%0Ausing%20three%20key%20components%3A%20%281%29%20Variance-Based%20Mixed%20Sparsity%20Allocation%0Aassigns%20sparsity%20based%20on%20variance%2C%20giving%20lower%20sparsity%20to%20high-variance%0Alayers%20to%20preserve%20inter-layer%20information.%20%282%29%20Distribution-Aware%20Compression%0Aapplies%20uniform%20quantization%20and%20then%20groups%20parameters%20by%20value%2C%20followed%20by%0Agroup-wise%20pruning%2C%20to%20better%20preserve%20intra-layer%20distribution.%20%283%29%0ATrace-Norm-Guided%20Rescaling%20uses%20the%20trace%20norm%20of%20delta%20weights%20to%20estimate%20a%0Aglobal%20rescaling%20factor%2C%20improving%20model%20stability%20under%20higher%20compression.%0AExtensive%20experiments%20across%20%28a%29%20large%20language%20models%20%28fine-tuned%20on%20LLaMA-2%0A7B%20and%2013B%29%20with%20up%20to%2050x%20compression%2C%20%28b%29%20general%20NLP%20models%20%28RoBERTa-base%2C%0AT5-base%29%20with%20up%20to%20224x%20compression%2C%20%28c%29%20vision%20models%20%28ViT-B/32%2C%20ViT-L/14%29%0Awith%20up%20to%20132x%20compression%2C%20and%20%28d%29%20multi-modal%20models%20%28BEiT-3%29%20with%2018x%0Acompression%2C%20demonstrate%20that%20UltraDelta%20consistently%20outperforms%20existing%0Amethods%2C%20especially%20under%20ultra-high%20compression.%20Code%20is%20available%20at%0Ahttps%3A//github.com/xiaohuiwang000/UltraDelta.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.13563v3&entry.124074799=Read"},
{"title": "Tight Regret Upper and Lower Bounds for Optimistic Hedge in Two-Player\n  Zero-Sum Games", "author": "Taira Tsuchiya", "abstract": "  In two-player zero-sum games, the learning dynamic based on optimistic Hedge\nachieves one of the best-known regret upper bounds among strongly-uncoupled\nlearning dynamics. With an appropriately chosen learning rate, the social and\nindividual regrets can be bounded by $O(\\log(mn))$ in terms of the numbers of\nactions $m$ and $n$ of the two players. This study investigates the optimality\nof the dependence on $m$ and $n$ in the regret of optimistic Hedge. To this\nend, we begin by refining existing regret analysis and show that, in the\nstrongly-uncoupled setting where the opponent's number of actions is known,\nboth the social and individual regret bounds can be improved to $O(\\sqrt{\\log m\n\\log n})$. In this analysis, we express the regret upper bound as an\noptimization problem with respect to the learning rates and the coefficients of\ncertain negative terms, enabling refined analysis of the leading constants. We\nthen show that the existing social regret bound as well as these new social and\nindividual regret upper bounds cannot be further improved for optimistic Hedge\nby providing algorithm-dependent individual regret lower bounds. Importantly,\nthese social regret upper and lower bounds match exactly including the constant\nfactor in the leading term. Finally, building on these results, we improve the\nlast-iterate convergence rate and the dynamic regret of a learning dynamic\nbased on optimistic Hedge, and complement these bounds with algorithm-dependent\ndynamic regret lower bounds that match the improved bounds.\n", "link": "http://arxiv.org/abs/2510.11691v1", "date": "2025-10-13", "relevancy": 1.6174, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4245}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3908}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tight%20Regret%20Upper%20and%20Lower%20Bounds%20for%20Optimistic%20Hedge%20in%20Two-Player%0A%20%20Zero-Sum%20Games&body=Title%3A%20Tight%20Regret%20Upper%20and%20Lower%20Bounds%20for%20Optimistic%20Hedge%20in%20Two-Player%0A%20%20Zero-Sum%20Games%0AAuthor%3A%20Taira%20Tsuchiya%0AAbstract%3A%20%20%20In%20two-player%20zero-sum%20games%2C%20the%20learning%20dynamic%20based%20on%20optimistic%20Hedge%0Aachieves%20one%20of%20the%20best-known%20regret%20upper%20bounds%20among%20strongly-uncoupled%0Alearning%20dynamics.%20With%20an%20appropriately%20chosen%20learning%20rate%2C%20the%20social%20and%0Aindividual%20regrets%20can%20be%20bounded%20by%20%24O%28%5Clog%28mn%29%29%24%20in%20terms%20of%20the%20numbers%20of%0Aactions%20%24m%24%20and%20%24n%24%20of%20the%20two%20players.%20This%20study%20investigates%20the%20optimality%0Aof%20the%20dependence%20on%20%24m%24%20and%20%24n%24%20in%20the%20regret%20of%20optimistic%20Hedge.%20To%20this%0Aend%2C%20we%20begin%20by%20refining%20existing%20regret%20analysis%20and%20show%20that%2C%20in%20the%0Astrongly-uncoupled%20setting%20where%20the%20opponent%27s%20number%20of%20actions%20is%20known%2C%0Aboth%20the%20social%20and%20individual%20regret%20bounds%20can%20be%20improved%20to%20%24O%28%5Csqrt%7B%5Clog%20m%0A%5Clog%20n%7D%29%24.%20In%20this%20analysis%2C%20we%20express%20the%20regret%20upper%20bound%20as%20an%0Aoptimization%20problem%20with%20respect%20to%20the%20learning%20rates%20and%20the%20coefficients%20of%0Acertain%20negative%20terms%2C%20enabling%20refined%20analysis%20of%20the%20leading%20constants.%20We%0Athen%20show%20that%20the%20existing%20social%20regret%20bound%20as%20well%20as%20these%20new%20social%20and%0Aindividual%20regret%20upper%20bounds%20cannot%20be%20further%20improved%20for%20optimistic%20Hedge%0Aby%20providing%20algorithm-dependent%20individual%20regret%20lower%20bounds.%20Importantly%2C%0Athese%20social%20regret%20upper%20and%20lower%20bounds%20match%20exactly%20including%20the%20constant%0Afactor%20in%20the%20leading%20term.%20Finally%2C%20building%20on%20these%20results%2C%20we%20improve%20the%0Alast-iterate%20convergence%20rate%20and%20the%20dynamic%20regret%20of%20a%20learning%20dynamic%0Abased%20on%20optimistic%20Hedge%2C%20and%20complement%20these%20bounds%20with%20algorithm-dependent%0Adynamic%20regret%20lower%20bounds%20that%20match%20the%20improved%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTight%2520Regret%2520Upper%2520and%2520Lower%2520Bounds%2520for%2520Optimistic%2520Hedge%2520in%2520Two-Player%250A%2520%2520Zero-Sum%2520Games%26entry.906535625%3DTaira%2520Tsuchiya%26entry.1292438233%3D%2520%2520In%2520two-player%2520zero-sum%2520games%252C%2520the%2520learning%2520dynamic%2520based%2520on%2520optimistic%2520Hedge%250Aachieves%2520one%2520of%2520the%2520best-known%2520regret%2520upper%2520bounds%2520among%2520strongly-uncoupled%250Alearning%2520dynamics.%2520With%2520an%2520appropriately%2520chosen%2520learning%2520rate%252C%2520the%2520social%2520and%250Aindividual%2520regrets%2520can%2520be%2520bounded%2520by%2520%2524O%2528%255Clog%2528mn%2529%2529%2524%2520in%2520terms%2520of%2520the%2520numbers%2520of%250Aactions%2520%2524m%2524%2520and%2520%2524n%2524%2520of%2520the%2520two%2520players.%2520This%2520study%2520investigates%2520the%2520optimality%250Aof%2520the%2520dependence%2520on%2520%2524m%2524%2520and%2520%2524n%2524%2520in%2520the%2520regret%2520of%2520optimistic%2520Hedge.%2520To%2520this%250Aend%252C%2520we%2520begin%2520by%2520refining%2520existing%2520regret%2520analysis%2520and%2520show%2520that%252C%2520in%2520the%250Astrongly-uncoupled%2520setting%2520where%2520the%2520opponent%2527s%2520number%2520of%2520actions%2520is%2520known%252C%250Aboth%2520the%2520social%2520and%2520individual%2520regret%2520bounds%2520can%2520be%2520improved%2520to%2520%2524O%2528%255Csqrt%257B%255Clog%2520m%250A%255Clog%2520n%257D%2529%2524.%2520In%2520this%2520analysis%252C%2520we%2520express%2520the%2520regret%2520upper%2520bound%2520as%2520an%250Aoptimization%2520problem%2520with%2520respect%2520to%2520the%2520learning%2520rates%2520and%2520the%2520coefficients%2520of%250Acertain%2520negative%2520terms%252C%2520enabling%2520refined%2520analysis%2520of%2520the%2520leading%2520constants.%2520We%250Athen%2520show%2520that%2520the%2520existing%2520social%2520regret%2520bound%2520as%2520well%2520as%2520these%2520new%2520social%2520and%250Aindividual%2520regret%2520upper%2520bounds%2520cannot%2520be%2520further%2520improved%2520for%2520optimistic%2520Hedge%250Aby%2520providing%2520algorithm-dependent%2520individual%2520regret%2520lower%2520bounds.%2520Importantly%252C%250Athese%2520social%2520regret%2520upper%2520and%2520lower%2520bounds%2520match%2520exactly%2520including%2520the%2520constant%250Afactor%2520in%2520the%2520leading%2520term.%2520Finally%252C%2520building%2520on%2520these%2520results%252C%2520we%2520improve%2520the%250Alast-iterate%2520convergence%2520rate%2520and%2520the%2520dynamic%2520regret%2520of%2520a%2520learning%2520dynamic%250Abased%2520on%2520optimistic%2520Hedge%252C%2520and%2520complement%2520these%2520bounds%2520with%2520algorithm-dependent%250Adynamic%2520regret%2520lower%2520bounds%2520that%2520match%2520the%2520improved%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tight%20Regret%20Upper%20and%20Lower%20Bounds%20for%20Optimistic%20Hedge%20in%20Two-Player%0A%20%20Zero-Sum%20Games&entry.906535625=Taira%20Tsuchiya&entry.1292438233=%20%20In%20two-player%20zero-sum%20games%2C%20the%20learning%20dynamic%20based%20on%20optimistic%20Hedge%0Aachieves%20one%20of%20the%20best-known%20regret%20upper%20bounds%20among%20strongly-uncoupled%0Alearning%20dynamics.%20With%20an%20appropriately%20chosen%20learning%20rate%2C%20the%20social%20and%0Aindividual%20regrets%20can%20be%20bounded%20by%20%24O%28%5Clog%28mn%29%29%24%20in%20terms%20of%20the%20numbers%20of%0Aactions%20%24m%24%20and%20%24n%24%20of%20the%20two%20players.%20This%20study%20investigates%20the%20optimality%0Aof%20the%20dependence%20on%20%24m%24%20and%20%24n%24%20in%20the%20regret%20of%20optimistic%20Hedge.%20To%20this%0Aend%2C%20we%20begin%20by%20refining%20existing%20regret%20analysis%20and%20show%20that%2C%20in%20the%0Astrongly-uncoupled%20setting%20where%20the%20opponent%27s%20number%20of%20actions%20is%20known%2C%0Aboth%20the%20social%20and%20individual%20regret%20bounds%20can%20be%20improved%20to%20%24O%28%5Csqrt%7B%5Clog%20m%0A%5Clog%20n%7D%29%24.%20In%20this%20analysis%2C%20we%20express%20the%20regret%20upper%20bound%20as%20an%0Aoptimization%20problem%20with%20respect%20to%20the%20learning%20rates%20and%20the%20coefficients%20of%0Acertain%20negative%20terms%2C%20enabling%20refined%20analysis%20of%20the%20leading%20constants.%20We%0Athen%20show%20that%20the%20existing%20social%20regret%20bound%20as%20well%20as%20these%20new%20social%20and%0Aindividual%20regret%20upper%20bounds%20cannot%20be%20further%20improved%20for%20optimistic%20Hedge%0Aby%20providing%20algorithm-dependent%20individual%20regret%20lower%20bounds.%20Importantly%2C%0Athese%20social%20regret%20upper%20and%20lower%20bounds%20match%20exactly%20including%20the%20constant%0Afactor%20in%20the%20leading%20term.%20Finally%2C%20building%20on%20these%20results%2C%20we%20improve%20the%0Alast-iterate%20convergence%20rate%20and%20the%20dynamic%20regret%20of%20a%20learning%20dynamic%0Abased%20on%20optimistic%20Hedge%2C%20and%20complement%20these%20bounds%20with%20algorithm-dependent%0Adynamic%20regret%20lower%20bounds%20that%20match%20the%20improved%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11691v1&entry.124074799=Read"},
{"title": "Reproducibility: The New Frontier in AI Governance", "author": "Israel Mason-Williams and Gabryel Mason-Williams", "abstract": "  AI policymakers are responsible for delivering effective governance\nmechanisms that can provide safe, aligned and trustworthy AI development.\nHowever, the information environment offered to policymakers is characterised\nby an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and\ncreating deep uncertainty and divides on which risks should be prioritised from\na governance perspective. We posit that the current publication speeds in AI\ncombined with the lack of strong scientific standards, via weak reproducibility\nprotocols, effectively erodes the power of policymakers to enact meaningful\npolicy and governance protocols. Our paper outlines how AI research could adopt\nstricter reproducibility guidelines to assist governance endeavours and improve\nconsensus on the AI risk landscape. We evaluate the forthcoming reproducibility\ncrisis within AI research through the lens of crises in other scientific\ndomains; providing a commentary on how adopting preregistration, increased\nstatistical power and negative result publication reproducibility protocols can\nenable effective AI governance. While we maintain that AI governance must be\nreactive due to AI's significant societal implications we argue that\npolicymakers and governments must consider reproducibility protocols as a core\ntool in the governance arsenal and demand higher standards for AI research.\nCode to replicate data and figures:\nhttps://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance\n", "link": "http://arxiv.org/abs/2510.11595v1", "date": "2025-10-13", "relevancy": 1.6165, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4282}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3906}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reproducibility%3A%20The%20New%20Frontier%20in%20AI%20Governance&body=Title%3A%20Reproducibility%3A%20The%20New%20Frontier%20in%20AI%20Governance%0AAuthor%3A%20Israel%20Mason-Williams%20and%20Gabryel%20Mason-Williams%0AAbstract%3A%20%20%20AI%20policymakers%20are%20responsible%20for%20delivering%20effective%20governance%0Amechanisms%20that%20can%20provide%20safe%2C%20aligned%20and%20trustworthy%20AI%20development.%0AHowever%2C%20the%20information%20environment%20offered%20to%20policymakers%20is%20characterised%0Aby%20an%20unnecessarily%20low%20Signal-To-Noise%20Ratio%2C%20favouring%20regulatory%20capture%20and%0Acreating%20deep%20uncertainty%20and%20divides%20on%20which%20risks%20should%20be%20prioritised%20from%0Aa%20governance%20perspective.%20We%20posit%20that%20the%20current%20publication%20speeds%20in%20AI%0Acombined%20with%20the%20lack%20of%20strong%20scientific%20standards%2C%20via%20weak%20reproducibility%0Aprotocols%2C%20effectively%20erodes%20the%20power%20of%20policymakers%20to%20enact%20meaningful%0Apolicy%20and%20governance%20protocols.%20Our%20paper%20outlines%20how%20AI%20research%20could%20adopt%0Astricter%20reproducibility%20guidelines%20to%20assist%20governance%20endeavours%20and%20improve%0Aconsensus%20on%20the%20AI%20risk%20landscape.%20We%20evaluate%20the%20forthcoming%20reproducibility%0Acrisis%20within%20AI%20research%20through%20the%20lens%20of%20crises%20in%20other%20scientific%0Adomains%3B%20providing%20a%20commentary%20on%20how%20adopting%20preregistration%2C%20increased%0Astatistical%20power%20and%20negative%20result%20publication%20reproducibility%20protocols%20can%0Aenable%20effective%20AI%20governance.%20While%20we%20maintain%20that%20AI%20governance%20must%20be%0Areactive%20due%20to%20AI%27s%20significant%20societal%20implications%20we%20argue%20that%0Apolicymakers%20and%20governments%20must%20consider%20reproducibility%20protocols%20as%20a%20core%0Atool%20in%20the%20governance%20arsenal%20and%20demand%20higher%20standards%20for%20AI%20research.%0ACode%20to%20replicate%20data%20and%20figures%3A%0Ahttps%3A//github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReproducibility%253A%2520The%2520New%2520Frontier%2520in%2520AI%2520Governance%26entry.906535625%3DIsrael%2520Mason-Williams%2520and%2520Gabryel%2520Mason-Williams%26entry.1292438233%3D%2520%2520AI%2520policymakers%2520are%2520responsible%2520for%2520delivering%2520effective%2520governance%250Amechanisms%2520that%2520can%2520provide%2520safe%252C%2520aligned%2520and%2520trustworthy%2520AI%2520development.%250AHowever%252C%2520the%2520information%2520environment%2520offered%2520to%2520policymakers%2520is%2520characterised%250Aby%2520an%2520unnecessarily%2520low%2520Signal-To-Noise%2520Ratio%252C%2520favouring%2520regulatory%2520capture%2520and%250Acreating%2520deep%2520uncertainty%2520and%2520divides%2520on%2520which%2520risks%2520should%2520be%2520prioritised%2520from%250Aa%2520governance%2520perspective.%2520We%2520posit%2520that%2520the%2520current%2520publication%2520speeds%2520in%2520AI%250Acombined%2520with%2520the%2520lack%2520of%2520strong%2520scientific%2520standards%252C%2520via%2520weak%2520reproducibility%250Aprotocols%252C%2520effectively%2520erodes%2520the%2520power%2520of%2520policymakers%2520to%2520enact%2520meaningful%250Apolicy%2520and%2520governance%2520protocols.%2520Our%2520paper%2520outlines%2520how%2520AI%2520research%2520could%2520adopt%250Astricter%2520reproducibility%2520guidelines%2520to%2520assist%2520governance%2520endeavours%2520and%2520improve%250Aconsensus%2520on%2520the%2520AI%2520risk%2520landscape.%2520We%2520evaluate%2520the%2520forthcoming%2520reproducibility%250Acrisis%2520within%2520AI%2520research%2520through%2520the%2520lens%2520of%2520crises%2520in%2520other%2520scientific%250Adomains%253B%2520providing%2520a%2520commentary%2520on%2520how%2520adopting%2520preregistration%252C%2520increased%250Astatistical%2520power%2520and%2520negative%2520result%2520publication%2520reproducibility%2520protocols%2520can%250Aenable%2520effective%2520AI%2520governance.%2520While%2520we%2520maintain%2520that%2520AI%2520governance%2520must%2520be%250Areactive%2520due%2520to%2520AI%2527s%2520significant%2520societal%2520implications%2520we%2520argue%2520that%250Apolicymakers%2520and%2520governments%2520must%2520consider%2520reproducibility%2520protocols%2520as%2520a%2520core%250Atool%2520in%2520the%2520governance%2520arsenal%2520and%2520demand%2520higher%2520standards%2520for%2520AI%2520research.%250ACode%2520to%2520replicate%2520data%2520and%2520figures%253A%250Ahttps%253A//github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reproducibility%3A%20The%20New%20Frontier%20in%20AI%20Governance&entry.906535625=Israel%20Mason-Williams%20and%20Gabryel%20Mason-Williams&entry.1292438233=%20%20AI%20policymakers%20are%20responsible%20for%20delivering%20effective%20governance%0Amechanisms%20that%20can%20provide%20safe%2C%20aligned%20and%20trustworthy%20AI%20development.%0AHowever%2C%20the%20information%20environment%20offered%20to%20policymakers%20is%20characterised%0Aby%20an%20unnecessarily%20low%20Signal-To-Noise%20Ratio%2C%20favouring%20regulatory%20capture%20and%0Acreating%20deep%20uncertainty%20and%20divides%20on%20which%20risks%20should%20be%20prioritised%20from%0Aa%20governance%20perspective.%20We%20posit%20that%20the%20current%20publication%20speeds%20in%20AI%0Acombined%20with%20the%20lack%20of%20strong%20scientific%20standards%2C%20via%20weak%20reproducibility%0Aprotocols%2C%20effectively%20erodes%20the%20power%20of%20policymakers%20to%20enact%20meaningful%0Apolicy%20and%20governance%20protocols.%20Our%20paper%20outlines%20how%20AI%20research%20could%20adopt%0Astricter%20reproducibility%20guidelines%20to%20assist%20governance%20endeavours%20and%20improve%0Aconsensus%20on%20the%20AI%20risk%20landscape.%20We%20evaluate%20the%20forthcoming%20reproducibility%0Acrisis%20within%20AI%20research%20through%20the%20lens%20of%20crises%20in%20other%20scientific%0Adomains%3B%20providing%20a%20commentary%20on%20how%20adopting%20preregistration%2C%20increased%0Astatistical%20power%20and%20negative%20result%20publication%20reproducibility%20protocols%20can%0Aenable%20effective%20AI%20governance.%20While%20we%20maintain%20that%20AI%20governance%20must%20be%0Areactive%20due%20to%20AI%27s%20significant%20societal%20implications%20we%20argue%20that%0Apolicymakers%20and%20governments%20must%20consider%20reproducibility%20protocols%20as%20a%20core%0Atool%20in%20the%20governance%20arsenal%20and%20demand%20higher%20standards%20for%20AI%20research.%0ACode%20to%20replicate%20data%20and%20figures%3A%0Ahttps%3A//github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11595v1&entry.124074799=Read"},
{"title": "Lecture Notes on Verifying Graph Neural Networks", "author": "Fran\u00e7ois Schwarzentruber", "abstract": "  In these lecture notes, we first recall the connection between graph neural\nnetworks, Weisfeiler-Lehman tests and logics such as first-order logic and\ngraded modal logic. We then present a modal logic in which counting modalities\nappear in linear inequalities in order to solve verification tasks on graph\nneural networks. We describe an algorithm for the satisfiability problem of\nthat logic. It is inspired from the tableau method of vanilla modal logic,\nextended with reasoning in quantifier-free fragment Boolean algebra with\nPresburger arithmetic.\n", "link": "http://arxiv.org/abs/2510.11617v1", "date": "2025-10-13", "relevancy": 1.5969, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4396}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.407}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lecture%20Notes%20on%20Verifying%20Graph%20Neural%20Networks&body=Title%3A%20Lecture%20Notes%20on%20Verifying%20Graph%20Neural%20Networks%0AAuthor%3A%20Fran%C3%A7ois%20Schwarzentruber%0AAbstract%3A%20%20%20In%20these%20lecture%20notes%2C%20we%20first%20recall%20the%20connection%20between%20graph%20neural%0Anetworks%2C%20Weisfeiler-Lehman%20tests%20and%20logics%20such%20as%20first-order%20logic%20and%0Agraded%20modal%20logic.%20We%20then%20present%20a%20modal%20logic%20in%20which%20counting%20modalities%0Aappear%20in%20linear%20inequalities%20in%20order%20to%20solve%20verification%20tasks%20on%20graph%0Aneural%20networks.%20We%20describe%20an%20algorithm%20for%20the%20satisfiability%20problem%20of%0Athat%20logic.%20It%20is%20inspired%20from%20the%20tableau%20method%20of%20vanilla%20modal%20logic%2C%0Aextended%20with%20reasoning%20in%20quantifier-free%20fragment%20Boolean%20algebra%20with%0APresburger%20arithmetic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLecture%2520Notes%2520on%2520Verifying%2520Graph%2520Neural%2520Networks%26entry.906535625%3DFran%25C3%25A7ois%2520Schwarzentruber%26entry.1292438233%3D%2520%2520In%2520these%2520lecture%2520notes%252C%2520we%2520first%2520recall%2520the%2520connection%2520between%2520graph%2520neural%250Anetworks%252C%2520Weisfeiler-Lehman%2520tests%2520and%2520logics%2520such%2520as%2520first-order%2520logic%2520and%250Agraded%2520modal%2520logic.%2520We%2520then%2520present%2520a%2520modal%2520logic%2520in%2520which%2520counting%2520modalities%250Aappear%2520in%2520linear%2520inequalities%2520in%2520order%2520to%2520solve%2520verification%2520tasks%2520on%2520graph%250Aneural%2520networks.%2520We%2520describe%2520an%2520algorithm%2520for%2520the%2520satisfiability%2520problem%2520of%250Athat%2520logic.%2520It%2520is%2520inspired%2520from%2520the%2520tableau%2520method%2520of%2520vanilla%2520modal%2520logic%252C%250Aextended%2520with%2520reasoning%2520in%2520quantifier-free%2520fragment%2520Boolean%2520algebra%2520with%250APresburger%2520arithmetic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lecture%20Notes%20on%20Verifying%20Graph%20Neural%20Networks&entry.906535625=Fran%C3%A7ois%20Schwarzentruber&entry.1292438233=%20%20In%20these%20lecture%20notes%2C%20we%20first%20recall%20the%20connection%20between%20graph%20neural%0Anetworks%2C%20Weisfeiler-Lehman%20tests%20and%20logics%20such%20as%20first-order%20logic%20and%0Agraded%20modal%20logic.%20We%20then%20present%20a%20modal%20logic%20in%20which%20counting%20modalities%0Aappear%20in%20linear%20inequalities%20in%20order%20to%20solve%20verification%20tasks%20on%20graph%0Aneural%20networks.%20We%20describe%20an%20algorithm%20for%20the%20satisfiability%20problem%20of%0Athat%20logic.%20It%20is%20inspired%20from%20the%20tableau%20method%20of%20vanilla%20modal%20logic%2C%0Aextended%20with%20reasoning%20in%20quantifier-free%20fragment%20Boolean%20algebra%20with%0APresburger%20arithmetic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11617v1&entry.124074799=Read"},
{"title": "LLM-Oriented Token-Adaptive Knowledge Distillation", "author": "Xurong Xie and Zhucun Xue and Jiafu Wu and Jian Li and Yabiao Wang and Xiaobin Hu and Yong Liu and Jiangning Zhang", "abstract": "  Knowledge distillation (KD) is a key technique for compressing large-scale\nlanguage models (LLMs), yet prevailing logit-based methods typically employ\nstatic strategies that are misaligned with the dynamic learning process of\nstudent models. These methods typically treat all tokens indiscriminately and\napply a single, fixed temperature, resulting in suboptimal knowledge transfer.\nTo address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge\nDistillation (AdaKD), a novel framework that adapts the distillation process to\nthe real-time learning state of each token. AdaKD consists of two synergistic\nmodules driven by a unified token difficulty metric. First, our Loss-Driven\nAdaptive Token Focusing (LATF) module dynamically adjusts the distillation\nfocus by monitoring the student's learning stability, concentrating\ncomputational resources on the most valuable tokens at each training phase.\nSecond, we introduce Inverse Difficulty Temperature Scaling (IDTS), a\ncounterintuitive yet effective token-level temperature strategy. It employs low\ntemperatures for difficult tokens for targeted error correction, and high\ntemperatures for easy tokens to encourage students to learn from the teacher's\ncomplete and smooth output distribution, thereby enhancing generalization. As a\nplug-and-play framework, AdaKD can consistently improve the performance of\nvarious distillation methods on multiple model architectures and benchmarks.\n", "link": "http://arxiv.org/abs/2510.11615v1", "date": "2025-10-13", "relevancy": 1.5756, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5528}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4952}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Oriented%20Token-Adaptive%20Knowledge%20Distillation&body=Title%3A%20LLM-Oriented%20Token-Adaptive%20Knowledge%20Distillation%0AAuthor%3A%20Xurong%20Xie%20and%20Zhucun%20Xue%20and%20Jiafu%20Wu%20and%20Jian%20Li%20and%20Yabiao%20Wang%20and%20Xiaobin%20Hu%20and%20Yong%20Liu%20and%20Jiangning%20Zhang%0AAbstract%3A%20%20%20Knowledge%20distillation%20%28KD%29%20is%20a%20key%20technique%20for%20compressing%20large-scale%0Alanguage%20models%20%28LLMs%29%2C%20yet%20prevailing%20logit-based%20methods%20typically%20employ%0Astatic%20strategies%20that%20are%20misaligned%20with%20the%20dynamic%20learning%20process%20of%0Astudent%20models.%20These%20methods%20typically%20treat%20all%20tokens%20indiscriminately%20and%0Aapply%20a%20single%2C%20fixed%20temperature%2C%20resulting%20in%20suboptimal%20knowledge%20transfer.%0ATo%20address%20these%20limitations%2C%20we%20propose%20LLM-Oriented%20Token-Adaptive%20Knowledge%0ADistillation%20%28AdaKD%29%2C%20a%20novel%20framework%20that%20adapts%20the%20distillation%20process%20to%0Athe%20real-time%20learning%20state%20of%20each%20token.%20AdaKD%20consists%20of%20two%20synergistic%0Amodules%20driven%20by%20a%20unified%20token%20difficulty%20metric.%20First%2C%20our%20Loss-Driven%0AAdaptive%20Token%20Focusing%20%28LATF%29%20module%20dynamically%20adjusts%20the%20distillation%0Afocus%20by%20monitoring%20the%20student%27s%20learning%20stability%2C%20concentrating%0Acomputational%20resources%20on%20the%20most%20valuable%20tokens%20at%20each%20training%20phase.%0ASecond%2C%20we%20introduce%20Inverse%20Difficulty%20Temperature%20Scaling%20%28IDTS%29%2C%20a%0Acounterintuitive%20yet%20effective%20token-level%20temperature%20strategy.%20It%20employs%20low%0Atemperatures%20for%20difficult%20tokens%20for%20targeted%20error%20correction%2C%20and%20high%0Atemperatures%20for%20easy%20tokens%20to%20encourage%20students%20to%20learn%20from%20the%20teacher%27s%0Acomplete%20and%20smooth%20output%20distribution%2C%20thereby%20enhancing%20generalization.%20As%20a%0Aplug-and-play%20framework%2C%20AdaKD%20can%20consistently%20improve%20the%20performance%20of%0Avarious%20distillation%20methods%20on%20multiple%20model%20architectures%20and%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Oriented%2520Token-Adaptive%2520Knowledge%2520Distillation%26entry.906535625%3DXurong%2520Xie%2520and%2520Zhucun%2520Xue%2520and%2520Jiafu%2520Wu%2520and%2520Jian%2520Li%2520and%2520Yabiao%2520Wang%2520and%2520Xiaobin%2520Hu%2520and%2520Yong%2520Liu%2520and%2520Jiangning%2520Zhang%26entry.1292438233%3D%2520%2520Knowledge%2520distillation%2520%2528KD%2529%2520is%2520a%2520key%2520technique%2520for%2520compressing%2520large-scale%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520yet%2520prevailing%2520logit-based%2520methods%2520typically%2520employ%250Astatic%2520strategies%2520that%2520are%2520misaligned%2520with%2520the%2520dynamic%2520learning%2520process%2520of%250Astudent%2520models.%2520These%2520methods%2520typically%2520treat%2520all%2520tokens%2520indiscriminately%2520and%250Aapply%2520a%2520single%252C%2520fixed%2520temperature%252C%2520resulting%2520in%2520suboptimal%2520knowledge%2520transfer.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520LLM-Oriented%2520Token-Adaptive%2520Knowledge%250ADistillation%2520%2528AdaKD%2529%252C%2520a%2520novel%2520framework%2520that%2520adapts%2520the%2520distillation%2520process%2520to%250Athe%2520real-time%2520learning%2520state%2520of%2520each%2520token.%2520AdaKD%2520consists%2520of%2520two%2520synergistic%250Amodules%2520driven%2520by%2520a%2520unified%2520token%2520difficulty%2520metric.%2520First%252C%2520our%2520Loss-Driven%250AAdaptive%2520Token%2520Focusing%2520%2528LATF%2529%2520module%2520dynamically%2520adjusts%2520the%2520distillation%250Afocus%2520by%2520monitoring%2520the%2520student%2527s%2520learning%2520stability%252C%2520concentrating%250Acomputational%2520resources%2520on%2520the%2520most%2520valuable%2520tokens%2520at%2520each%2520training%2520phase.%250ASecond%252C%2520we%2520introduce%2520Inverse%2520Difficulty%2520Temperature%2520Scaling%2520%2528IDTS%2529%252C%2520a%250Acounterintuitive%2520yet%2520effective%2520token-level%2520temperature%2520strategy.%2520It%2520employs%2520low%250Atemperatures%2520for%2520difficult%2520tokens%2520for%2520targeted%2520error%2520correction%252C%2520and%2520high%250Atemperatures%2520for%2520easy%2520tokens%2520to%2520encourage%2520students%2520to%2520learn%2520from%2520the%2520teacher%2527s%250Acomplete%2520and%2520smooth%2520output%2520distribution%252C%2520thereby%2520enhancing%2520generalization.%2520As%2520a%250Aplug-and-play%2520framework%252C%2520AdaKD%2520can%2520consistently%2520improve%2520the%2520performance%2520of%250Avarious%2520distillation%2520methods%2520on%2520multiple%2520model%2520architectures%2520and%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Oriented%20Token-Adaptive%20Knowledge%20Distillation&entry.906535625=Xurong%20Xie%20and%20Zhucun%20Xue%20and%20Jiafu%20Wu%20and%20Jian%20Li%20and%20Yabiao%20Wang%20and%20Xiaobin%20Hu%20and%20Yong%20Liu%20and%20Jiangning%20Zhang&entry.1292438233=%20%20Knowledge%20distillation%20%28KD%29%20is%20a%20key%20technique%20for%20compressing%20large-scale%0Alanguage%20models%20%28LLMs%29%2C%20yet%20prevailing%20logit-based%20methods%20typically%20employ%0Astatic%20strategies%20that%20are%20misaligned%20with%20the%20dynamic%20learning%20process%20of%0Astudent%20models.%20These%20methods%20typically%20treat%20all%20tokens%20indiscriminately%20and%0Aapply%20a%20single%2C%20fixed%20temperature%2C%20resulting%20in%20suboptimal%20knowledge%20transfer.%0ATo%20address%20these%20limitations%2C%20we%20propose%20LLM-Oriented%20Token-Adaptive%20Knowledge%0ADistillation%20%28AdaKD%29%2C%20a%20novel%20framework%20that%20adapts%20the%20distillation%20process%20to%0Athe%20real-time%20learning%20state%20of%20each%20token.%20AdaKD%20consists%20of%20two%20synergistic%0Amodules%20driven%20by%20a%20unified%20token%20difficulty%20metric.%20First%2C%20our%20Loss-Driven%0AAdaptive%20Token%20Focusing%20%28LATF%29%20module%20dynamically%20adjusts%20the%20distillation%0Afocus%20by%20monitoring%20the%20student%27s%20learning%20stability%2C%20concentrating%0Acomputational%20resources%20on%20the%20most%20valuable%20tokens%20at%20each%20training%20phase.%0ASecond%2C%20we%20introduce%20Inverse%20Difficulty%20Temperature%20Scaling%20%28IDTS%29%2C%20a%0Acounterintuitive%20yet%20effective%20token-level%20temperature%20strategy.%20It%20employs%20low%0Atemperatures%20for%20difficult%20tokens%20for%20targeted%20error%20correction%2C%20and%20high%0Atemperatures%20for%20easy%20tokens%20to%20encourage%20students%20to%20learn%20from%20the%20teacher%27s%0Acomplete%20and%20smooth%20output%20distribution%2C%20thereby%20enhancing%20generalization.%20As%20a%0Aplug-and-play%20framework%2C%20AdaKD%20can%20consistently%20improve%20the%20performance%20of%0Avarious%20distillation%20methods%20on%20multiple%20model%20architectures%20and%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11615v1&entry.124074799=Read"},
{"title": "Attention Factors for Statistical Arbitrage", "author": "Elliot L. Epstein and Rose Wang and Jaewon Choi and Markus Pelger", "abstract": "  Statistical arbitrage exploits temporal price differences between similar\nassets. We develop a framework to jointly identify similar assets through\nfactors, identify mispricing and form a trading policy that maximizes\nrisk-adjusted performance after trading costs. Our Attention Factors are\nconditional latent factors that are the most useful for arbitrage trading. They\nare learned from firm characteristic embeddings that allow for complex\ninteractions. We identify time-series signals from the residual portfolios of\nour factors with a general sequence model. Estimating factors and the arbitrage\ntrading strategy jointly is crucial to maximize profitability after trading\ncosts. In a comprehensive empirical study we show that our Attention Factor\nmodel achieves an out-of-sample Sharpe ratio above 4 on the largest U.S.\nequities over a 24-year period. Our one-step solution yields an unprecedented\nSharpe ratio of 2.3 net of transaction costs. We show that weak factors are\nimportant for arbitrage trading.\n", "link": "http://arxiv.org/abs/2510.11616v1", "date": "2025-10-13", "relevancy": 1.5468, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3849}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Factors%20for%20Statistical%20Arbitrage&body=Title%3A%20Attention%20Factors%20for%20Statistical%20Arbitrage%0AAuthor%3A%20Elliot%20L.%20Epstein%20and%20Rose%20Wang%20and%20Jaewon%20Choi%20and%20Markus%20Pelger%0AAbstract%3A%20%20%20Statistical%20arbitrage%20exploits%20temporal%20price%20differences%20between%20similar%0Aassets.%20We%20develop%20a%20framework%20to%20jointly%20identify%20similar%20assets%20through%0Afactors%2C%20identify%20mispricing%20and%20form%20a%20trading%20policy%20that%20maximizes%0Arisk-adjusted%20performance%20after%20trading%20costs.%20Our%20Attention%20Factors%20are%0Aconditional%20latent%20factors%20that%20are%20the%20most%20useful%20for%20arbitrage%20trading.%20They%0Aare%20learned%20from%20firm%20characteristic%20embeddings%20that%20allow%20for%20complex%0Ainteractions.%20We%20identify%20time-series%20signals%20from%20the%20residual%20portfolios%20of%0Aour%20factors%20with%20a%20general%20sequence%20model.%20Estimating%20factors%20and%20the%20arbitrage%0Atrading%20strategy%20jointly%20is%20crucial%20to%20maximize%20profitability%20after%20trading%0Acosts.%20In%20a%20comprehensive%20empirical%20study%20we%20show%20that%20our%20Attention%20Factor%0Amodel%20achieves%20an%20out-of-sample%20Sharpe%20ratio%20above%204%20on%20the%20largest%20U.S.%0Aequities%20over%20a%2024-year%20period.%20Our%20one-step%20solution%20yields%20an%20unprecedented%0ASharpe%20ratio%20of%202.3%20net%20of%20transaction%20costs.%20We%20show%20that%20weak%20factors%20are%0Aimportant%20for%20arbitrage%20trading.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11616v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Factors%2520for%2520Statistical%2520Arbitrage%26entry.906535625%3DElliot%2520L.%2520Epstein%2520and%2520Rose%2520Wang%2520and%2520Jaewon%2520Choi%2520and%2520Markus%2520Pelger%26entry.1292438233%3D%2520%2520Statistical%2520arbitrage%2520exploits%2520temporal%2520price%2520differences%2520between%2520similar%250Aassets.%2520We%2520develop%2520a%2520framework%2520to%2520jointly%2520identify%2520similar%2520assets%2520through%250Afactors%252C%2520identify%2520mispricing%2520and%2520form%2520a%2520trading%2520policy%2520that%2520maximizes%250Arisk-adjusted%2520performance%2520after%2520trading%2520costs.%2520Our%2520Attention%2520Factors%2520are%250Aconditional%2520latent%2520factors%2520that%2520are%2520the%2520most%2520useful%2520for%2520arbitrage%2520trading.%2520They%250Aare%2520learned%2520from%2520firm%2520characteristic%2520embeddings%2520that%2520allow%2520for%2520complex%250Ainteractions.%2520We%2520identify%2520time-series%2520signals%2520from%2520the%2520residual%2520portfolios%2520of%250Aour%2520factors%2520with%2520a%2520general%2520sequence%2520model.%2520Estimating%2520factors%2520and%2520the%2520arbitrage%250Atrading%2520strategy%2520jointly%2520is%2520crucial%2520to%2520maximize%2520profitability%2520after%2520trading%250Acosts.%2520In%2520a%2520comprehensive%2520empirical%2520study%2520we%2520show%2520that%2520our%2520Attention%2520Factor%250Amodel%2520achieves%2520an%2520out-of-sample%2520Sharpe%2520ratio%2520above%25204%2520on%2520the%2520largest%2520U.S.%250Aequities%2520over%2520a%252024-year%2520period.%2520Our%2520one-step%2520solution%2520yields%2520an%2520unprecedented%250ASharpe%2520ratio%2520of%25202.3%2520net%2520of%2520transaction%2520costs.%2520We%2520show%2520that%2520weak%2520factors%2520are%250Aimportant%2520for%2520arbitrage%2520trading.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11616v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Factors%20for%20Statistical%20Arbitrage&entry.906535625=Elliot%20L.%20Epstein%20and%20Rose%20Wang%20and%20Jaewon%20Choi%20and%20Markus%20Pelger&entry.1292438233=%20%20Statistical%20arbitrage%20exploits%20temporal%20price%20differences%20between%20similar%0Aassets.%20We%20develop%20a%20framework%20to%20jointly%20identify%20similar%20assets%20through%0Afactors%2C%20identify%20mispricing%20and%20form%20a%20trading%20policy%20that%20maximizes%0Arisk-adjusted%20performance%20after%20trading%20costs.%20Our%20Attention%20Factors%20are%0Aconditional%20latent%20factors%20that%20are%20the%20most%20useful%20for%20arbitrage%20trading.%20They%0Aare%20learned%20from%20firm%20characteristic%20embeddings%20that%20allow%20for%20complex%0Ainteractions.%20We%20identify%20time-series%20signals%20from%20the%20residual%20portfolios%20of%0Aour%20factors%20with%20a%20general%20sequence%20model.%20Estimating%20factors%20and%20the%20arbitrage%0Atrading%20strategy%20jointly%20is%20crucial%20to%20maximize%20profitability%20after%20trading%0Acosts.%20In%20a%20comprehensive%20empirical%20study%20we%20show%20that%20our%20Attention%20Factor%0Amodel%20achieves%20an%20out-of-sample%20Sharpe%20ratio%20above%204%20on%20the%20largest%20U.S.%0Aequities%20over%20a%2024-year%20period.%20Our%20one-step%20solution%20yields%20an%20unprecedented%0ASharpe%20ratio%20of%202.3%20net%20of%20transaction%20costs.%20We%20show%20that%20weak%20factors%20are%0Aimportant%20for%20arbitrage%20trading.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11616v1&entry.124074799=Read"},
{"title": "Measuring Physical-World Privacy Awareness of Large Language Models: An\n  Evaluation Benchmark", "author": "Xinjie Shen and Mufei Li and Pan Li", "abstract": "  The deployment of Large Language Models (LLMs) in embodied agents creates an\nurgent need to measure their privacy awareness in the physical world. Existing\nevaluation methods, however, are confined to natural language based scenarios.\nTo bridge this gap, we introduce EAPrivacy, a comprehensive evaluation\nbenchmark designed to quantify the physical-world privacy awareness of\nLLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across\nfour tiers to test an agent's ability to handle sensitive objects, adapt to\nchanging environments, balance task execution with privacy constraints, and\nresolve conflicts with social norms. Our measurements reveal a critical deficit\nin current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\\%\naccuracy in scenarios involving changing physical environments. Furthermore,\nwhen a task was accompanied by a privacy request, models prioritized completion\nover the constraint in up to 86\\% of cases. In high-stakes situations pitting\nprivacy against critical social norms, leading models like GPT-4o and\nClaude-3.5-haiku disregarded the social norm over 15\\% of the time. These\nfindings, demonstrated by our benchmark, underscore a fundamental misalignment\nin LLMs regarding physically grounded privacy and establish the need for more\nrobust, physically-aware alignment. Codes and datasets will be available at\nhttps://github.com/Graph-COM/EAPrivacy.\n", "link": "http://arxiv.org/abs/2510.02356v2", "date": "2025-10-13", "relevancy": 1.5362, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5236}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5178}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20Physical-World%20Privacy%20Awareness%20of%20Large%20Language%20Models%3A%20An%0A%20%20Evaluation%20Benchmark&body=Title%3A%20Measuring%20Physical-World%20Privacy%20Awareness%20of%20Large%20Language%20Models%3A%20An%0A%20%20Evaluation%20Benchmark%0AAuthor%3A%20Xinjie%20Shen%20and%20Mufei%20Li%20and%20Pan%20Li%0AAbstract%3A%20%20%20The%20deployment%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20embodied%20agents%20creates%20an%0Aurgent%20need%20to%20measure%20their%20privacy%20awareness%20in%20the%20physical%20world.%20Existing%0Aevaluation%20methods%2C%20however%2C%20are%20confined%20to%20natural%20language%20based%20scenarios.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20EAPrivacy%2C%20a%20comprehensive%20evaluation%0Abenchmark%20designed%20to%20quantify%20the%20physical-world%20privacy%20awareness%20of%0ALLM-powered%20agents.%20EAPrivacy%20utilizes%20procedurally%20generated%20scenarios%20across%0Afour%20tiers%20to%20test%20an%20agent%27s%20ability%20to%20handle%20sensitive%20objects%2C%20adapt%20to%0Achanging%20environments%2C%20balance%20task%20execution%20with%20privacy%20constraints%2C%20and%0Aresolve%20conflicts%20with%20social%20norms.%20Our%20measurements%20reveal%20a%20critical%20deficit%0Ain%20current%20models.%20The%20top-performing%20model%2C%20Gemini%202.5%20Pro%2C%20achieved%20only%2059%5C%25%0Aaccuracy%20in%20scenarios%20involving%20changing%20physical%20environments.%20Furthermore%2C%0Awhen%20a%20task%20was%20accompanied%20by%20a%20privacy%20request%2C%20models%20prioritized%20completion%0Aover%20the%20constraint%20in%20up%20to%2086%5C%25%20of%20cases.%20In%20high-stakes%20situations%20pitting%0Aprivacy%20against%20critical%20social%20norms%2C%20leading%20models%20like%20GPT-4o%20and%0AClaude-3.5-haiku%20disregarded%20the%20social%20norm%20over%2015%5C%25%20of%20the%20time.%20These%0Afindings%2C%20demonstrated%20by%20our%20benchmark%2C%20underscore%20a%20fundamental%20misalignment%0Ain%20LLMs%20regarding%20physically%20grounded%20privacy%20and%20establish%20the%20need%20for%20more%0Arobust%2C%20physically-aware%20alignment.%20Codes%20and%20datasets%20will%20be%20available%20at%0Ahttps%3A//github.com/Graph-COM/EAPrivacy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.02356v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520Physical-World%2520Privacy%2520Awareness%2520of%2520Large%2520Language%2520Models%253A%2520An%250A%2520%2520Evaluation%2520Benchmark%26entry.906535625%3DXinjie%2520Shen%2520and%2520Mufei%2520Li%2520and%2520Pan%2520Li%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520embodied%2520agents%2520creates%2520an%250Aurgent%2520need%2520to%2520measure%2520their%2520privacy%2520awareness%2520in%2520the%2520physical%2520world.%2520Existing%250Aevaluation%2520methods%252C%2520however%252C%2520are%2520confined%2520to%2520natural%2520language%2520based%2520scenarios.%250ATo%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520EAPrivacy%252C%2520a%2520comprehensive%2520evaluation%250Abenchmark%2520designed%2520to%2520quantify%2520the%2520physical-world%2520privacy%2520awareness%2520of%250ALLM-powered%2520agents.%2520EAPrivacy%2520utilizes%2520procedurally%2520generated%2520scenarios%2520across%250Afour%2520tiers%2520to%2520test%2520an%2520agent%2527s%2520ability%2520to%2520handle%2520sensitive%2520objects%252C%2520adapt%2520to%250Achanging%2520environments%252C%2520balance%2520task%2520execution%2520with%2520privacy%2520constraints%252C%2520and%250Aresolve%2520conflicts%2520with%2520social%2520norms.%2520Our%2520measurements%2520reveal%2520a%2520critical%2520deficit%250Ain%2520current%2520models.%2520The%2520top-performing%2520model%252C%2520Gemini%25202.5%2520Pro%252C%2520achieved%2520only%252059%255C%2525%250Aaccuracy%2520in%2520scenarios%2520involving%2520changing%2520physical%2520environments.%2520Furthermore%252C%250Awhen%2520a%2520task%2520was%2520accompanied%2520by%2520a%2520privacy%2520request%252C%2520models%2520prioritized%2520completion%250Aover%2520the%2520constraint%2520in%2520up%2520to%252086%255C%2525%2520of%2520cases.%2520In%2520high-stakes%2520situations%2520pitting%250Aprivacy%2520against%2520critical%2520social%2520norms%252C%2520leading%2520models%2520like%2520GPT-4o%2520and%250AClaude-3.5-haiku%2520disregarded%2520the%2520social%2520norm%2520over%252015%255C%2525%2520of%2520the%2520time.%2520These%250Afindings%252C%2520demonstrated%2520by%2520our%2520benchmark%252C%2520underscore%2520a%2520fundamental%2520misalignment%250Ain%2520LLMs%2520regarding%2520physically%2520grounded%2520privacy%2520and%2520establish%2520the%2520need%2520for%2520more%250Arobust%252C%2520physically-aware%2520alignment.%2520Codes%2520and%2520datasets%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/Graph-COM/EAPrivacy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02356v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Physical-World%20Privacy%20Awareness%20of%20Large%20Language%20Models%3A%20An%0A%20%20Evaluation%20Benchmark&entry.906535625=Xinjie%20Shen%20and%20Mufei%20Li%20and%20Pan%20Li&entry.1292438233=%20%20The%20deployment%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20embodied%20agents%20creates%20an%0Aurgent%20need%20to%20measure%20their%20privacy%20awareness%20in%20the%20physical%20world.%20Existing%0Aevaluation%20methods%2C%20however%2C%20are%20confined%20to%20natural%20language%20based%20scenarios.%0ATo%20bridge%20this%20gap%2C%20we%20introduce%20EAPrivacy%2C%20a%20comprehensive%20evaluation%0Abenchmark%20designed%20to%20quantify%20the%20physical-world%20privacy%20awareness%20of%0ALLM-powered%20agents.%20EAPrivacy%20utilizes%20procedurally%20generated%20scenarios%20across%0Afour%20tiers%20to%20test%20an%20agent%27s%20ability%20to%20handle%20sensitive%20objects%2C%20adapt%20to%0Achanging%20environments%2C%20balance%20task%20execution%20with%20privacy%20constraints%2C%20and%0Aresolve%20conflicts%20with%20social%20norms.%20Our%20measurements%20reveal%20a%20critical%20deficit%0Ain%20current%20models.%20The%20top-performing%20model%2C%20Gemini%202.5%20Pro%2C%20achieved%20only%2059%5C%25%0Aaccuracy%20in%20scenarios%20involving%20changing%20physical%20environments.%20Furthermore%2C%0Awhen%20a%20task%20was%20accompanied%20by%20a%20privacy%20request%2C%20models%20prioritized%20completion%0Aover%20the%20constraint%20in%20up%20to%2086%5C%25%20of%20cases.%20In%20high-stakes%20situations%20pitting%0Aprivacy%20against%20critical%20social%20norms%2C%20leading%20models%20like%20GPT-4o%20and%0AClaude-3.5-haiku%20disregarded%20the%20social%20norm%20over%2015%5C%25%20of%20the%20time.%20These%0Afindings%2C%20demonstrated%20by%20our%20benchmark%2C%20underscore%20a%20fundamental%20misalignment%0Ain%20LLMs%20regarding%20physically%20grounded%20privacy%20and%20establish%20the%20need%20for%20more%0Arobust%2C%20physically-aware%20alignment.%20Codes%20and%20datasets%20will%20be%20available%20at%0Ahttps%3A//github.com/Graph-COM/EAPrivacy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.02356v2&entry.124074799=Read"},
{"title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine\n  Learning Engineering", "author": "Arjun Sahney and Ram Gorthi and Cezary \u0141astowski and Javier Vega", "abstract": "  We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints.\n", "link": "http://arxiv.org/abs/2510.11694v1", "date": "2025-10-13", "relevancy": 1.5057, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5237}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5002}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Operand%20Quant%3A%20A%20Single-Agent%20Architecture%20for%20Autonomous%20Machine%0A%20%20Learning%20Engineering&body=Title%3A%20Operand%20Quant%3A%20A%20Single-Agent%20Architecture%20for%20Autonomous%20Machine%0A%20%20Learning%20Engineering%0AAuthor%3A%20Arjun%20Sahney%20and%20Ram%20Gorthi%20and%20Cezary%20%C5%81astowski%20and%20Javier%20Vega%0AAbstract%3A%20%20%20We%20present%20Operand%20Quant%2C%20a%20single-agent%2C%20IDE-based%20architecture%20for%0Aautonomous%20machine%20learning%20engineering%20%28MLE%29.%20Operand%20Quant%20departs%20from%0Aconventional%20multi-agent%20orchestration%20frameworks%20by%20consolidating%20all%20MLE%0Alifecycle%20stages%20--%20exploration%2C%20modeling%2C%20experimentation%2C%20and%20deployment%20--%0Awithin%20a%20single%2C%20context-aware%20agent.%20On%20the%20MLE-Benchmark%20%282025%29%2C%20Operand%0AQuant%20achieved%20a%20new%20state-of-the-art%20%28SOTA%29%20result%2C%20with%20an%20overall%20medal%20rate%0Aof%200.3956%20%2B/-%200.0565%20across%2075%20problems%20--%20the%20highest%20recorded%20performance%0Aamong%20all%20evaluated%20systems%20to%20date.%20The%20architecture%20demonstrates%20that%20a%0Alinear%2C%20non-blocking%20agent%2C%20operating%20autonomously%20within%20a%20controlled%20IDE%0Aenvironment%2C%20can%20outperform%20multi-agent%20and%20orchestrated%20systems%20under%0Aidentical%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOperand%2520Quant%253A%2520A%2520Single-Agent%2520Architecture%2520for%2520Autonomous%2520Machine%250A%2520%2520Learning%2520Engineering%26entry.906535625%3DArjun%2520Sahney%2520and%2520Ram%2520Gorthi%2520and%2520Cezary%2520%25C5%2581astowski%2520and%2520Javier%2520Vega%26entry.1292438233%3D%2520%2520We%2520present%2520Operand%2520Quant%252C%2520a%2520single-agent%252C%2520IDE-based%2520architecture%2520for%250Aautonomous%2520machine%2520learning%2520engineering%2520%2528MLE%2529.%2520Operand%2520Quant%2520departs%2520from%250Aconventional%2520multi-agent%2520orchestration%2520frameworks%2520by%2520consolidating%2520all%2520MLE%250Alifecycle%2520stages%2520--%2520exploration%252C%2520modeling%252C%2520experimentation%252C%2520and%2520deployment%2520--%250Awithin%2520a%2520single%252C%2520context-aware%2520agent.%2520On%2520the%2520MLE-Benchmark%2520%25282025%2529%252C%2520Operand%250AQuant%2520achieved%2520a%2520new%2520state-of-the-art%2520%2528SOTA%2529%2520result%252C%2520with%2520an%2520overall%2520medal%2520rate%250Aof%25200.3956%2520%252B/-%25200.0565%2520across%252075%2520problems%2520--%2520the%2520highest%2520recorded%2520performance%250Aamong%2520all%2520evaluated%2520systems%2520to%2520date.%2520The%2520architecture%2520demonstrates%2520that%2520a%250Alinear%252C%2520non-blocking%2520agent%252C%2520operating%2520autonomously%2520within%2520a%2520controlled%2520IDE%250Aenvironment%252C%2520can%2520outperform%2520multi-agent%2520and%2520orchestrated%2520systems%2520under%250Aidentical%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Operand%20Quant%3A%20A%20Single-Agent%20Architecture%20for%20Autonomous%20Machine%0A%20%20Learning%20Engineering&entry.906535625=Arjun%20Sahney%20and%20Ram%20Gorthi%20and%20Cezary%20%C5%81astowski%20and%20Javier%20Vega&entry.1292438233=%20%20We%20present%20Operand%20Quant%2C%20a%20single-agent%2C%20IDE-based%20architecture%20for%0Aautonomous%20machine%20learning%20engineering%20%28MLE%29.%20Operand%20Quant%20departs%20from%0Aconventional%20multi-agent%20orchestration%20frameworks%20by%20consolidating%20all%20MLE%0Alifecycle%20stages%20--%20exploration%2C%20modeling%2C%20experimentation%2C%20and%20deployment%20--%0Awithin%20a%20single%2C%20context-aware%20agent.%20On%20the%20MLE-Benchmark%20%282025%29%2C%20Operand%0AQuant%20achieved%20a%20new%20state-of-the-art%20%28SOTA%29%20result%2C%20with%20an%20overall%20medal%20rate%0Aof%200.3956%20%2B/-%200.0565%20across%2075%20problems%20--%20the%20highest%20recorded%20performance%0Aamong%20all%20evaluated%20systems%20to%20date.%20The%20architecture%20demonstrates%20that%20a%0Alinear%2C%20non-blocking%20agent%2C%20operating%20autonomously%20within%20a%20controlled%20IDE%0Aenvironment%2C%20can%20outperform%20multi-agent%20and%20orchestrated%20systems%20under%0Aidentical%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11694v1&entry.124074799=Read"},
{"title": "SR-Scientist: Scientific Equation Discovery With Agentic AI", "author": "Shijie Xia and Yuhan Sun and Pengfei Liu", "abstract": "  Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.\n", "link": "http://arxiv.org/abs/2510.11661v1", "date": "2025-10-13", "relevancy": 1.4971, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4965}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SR-Scientist%3A%20Scientific%20Equation%20Discovery%20With%20Agentic%20AI&body=Title%3A%20SR-Scientist%3A%20Scientific%20Equation%20Discovery%20With%20Agentic%20AI%0AAuthor%3A%20Shijie%20Xia%20and%20Yuhan%20Sun%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20applied%20to%20scientific%0Aequation%20discovery%2C%20leveraging%20their%20embedded%20scientific%20knowledge%20for%0Ahypothesis%20generation.%20However%2C%20current%20methods%20typically%20confine%20LLMs%20to%20the%0Arole%20of%20an%20equation%20proposer%20within%20search%20algorithms%20like%20genetic%20programming.%0AIn%20this%20paper%2C%20we%20present%20SR-Scientist%2C%20a%20framework%20that%20elevates%20the%20LLM%20from%0Aa%20simple%20equation%20proposer%20to%20an%20autonomous%20AI%20scientist%20that%20writes%20code%20to%0Aanalyze%20data%2C%20implements%20the%20equation%20as%20code%2C%20submits%20it%20for%20evaluation%2C%20and%0Aoptimizes%20the%20equation%20based%20on%20experimental%20feedback.%20Specifically%2C%20we%20wrap%0Athe%20code%20interpreter%20into%20a%20set%20of%20tools%20for%20data%20analysis%20and%20equation%0Aevaluation.%20The%20agent%20is%20instructed%20to%20optimize%20the%20equation%20by%20utilizing%20these%0Atools%20over%20a%20long%20horizon%20with%20minimal%20human-defined%20pipelines.%20Empirical%0Aresults%20show%20that%20SR-Scientist%20outperforms%20baseline%20methods%20by%20an%20absolute%0Amargin%20of%206%25%20to%2035%25%20on%20datasets%20covering%20four%20science%20disciplines.%0AAdditionally%2C%20we%20demonstrate%20our%20method%27s%20robustness%20to%20noise%2C%20the%0Ageneralization%20of%20the%20discovered%20equations%20to%20out-of-domain%20data%2C%20and%20their%0Asymbolic%20accuracy.%20Furthermore%2C%20we%20develop%20an%20end-to-end%20reinforcement%20learning%0Aframework%20to%20enhance%20the%20agent%27s%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSR-Scientist%253A%2520Scientific%2520Equation%2520Discovery%2520With%2520Agentic%2520AI%26entry.906535625%3DShijie%2520Xia%2520and%2520Yuhan%2520Sun%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520applied%2520to%2520scientific%250Aequation%2520discovery%252C%2520leveraging%2520their%2520embedded%2520scientific%2520knowledge%2520for%250Ahypothesis%2520generation.%2520However%252C%2520current%2520methods%2520typically%2520confine%2520LLMs%2520to%2520the%250Arole%2520of%2520an%2520equation%2520proposer%2520within%2520search%2520algorithms%2520like%2520genetic%2520programming.%250AIn%2520this%2520paper%252C%2520we%2520present%2520SR-Scientist%252C%2520a%2520framework%2520that%2520elevates%2520the%2520LLM%2520from%250Aa%2520simple%2520equation%2520proposer%2520to%2520an%2520autonomous%2520AI%2520scientist%2520that%2520writes%2520code%2520to%250Aanalyze%2520data%252C%2520implements%2520the%2520equation%2520as%2520code%252C%2520submits%2520it%2520for%2520evaluation%252C%2520and%250Aoptimizes%2520the%2520equation%2520based%2520on%2520experimental%2520feedback.%2520Specifically%252C%2520we%2520wrap%250Athe%2520code%2520interpreter%2520into%2520a%2520set%2520of%2520tools%2520for%2520data%2520analysis%2520and%2520equation%250Aevaluation.%2520The%2520agent%2520is%2520instructed%2520to%2520optimize%2520the%2520equation%2520by%2520utilizing%2520these%250Atools%2520over%2520a%2520long%2520horizon%2520with%2520minimal%2520human-defined%2520pipelines.%2520Empirical%250Aresults%2520show%2520that%2520SR-Scientist%2520outperforms%2520baseline%2520methods%2520by%2520an%2520absolute%250Amargin%2520of%25206%2525%2520to%252035%2525%2520on%2520datasets%2520covering%2520four%2520science%2520disciplines.%250AAdditionally%252C%2520we%2520demonstrate%2520our%2520method%2527s%2520robustness%2520to%2520noise%252C%2520the%250Ageneralization%2520of%2520the%2520discovered%2520equations%2520to%2520out-of-domain%2520data%252C%2520and%2520their%250Asymbolic%2520accuracy.%2520Furthermore%252C%2520we%2520develop%2520an%2520end-to-end%2520reinforcement%2520learning%250Aframework%2520to%2520enhance%2520the%2520agent%2527s%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SR-Scientist%3A%20Scientific%20Equation%20Discovery%20With%20Agentic%20AI&entry.906535625=Shijie%20Xia%20and%20Yuhan%20Sun%20and%20Pengfei%20Liu&entry.1292438233=%20%20Recently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20applied%20to%20scientific%0Aequation%20discovery%2C%20leveraging%20their%20embedded%20scientific%20knowledge%20for%0Ahypothesis%20generation.%20However%2C%20current%20methods%20typically%20confine%20LLMs%20to%20the%0Arole%20of%20an%20equation%20proposer%20within%20search%20algorithms%20like%20genetic%20programming.%0AIn%20this%20paper%2C%20we%20present%20SR-Scientist%2C%20a%20framework%20that%20elevates%20the%20LLM%20from%0Aa%20simple%20equation%20proposer%20to%20an%20autonomous%20AI%20scientist%20that%20writes%20code%20to%0Aanalyze%20data%2C%20implements%20the%20equation%20as%20code%2C%20submits%20it%20for%20evaluation%2C%20and%0Aoptimizes%20the%20equation%20based%20on%20experimental%20feedback.%20Specifically%2C%20we%20wrap%0Athe%20code%20interpreter%20into%20a%20set%20of%20tools%20for%20data%20analysis%20and%20equation%0Aevaluation.%20The%20agent%20is%20instructed%20to%20optimize%20the%20equation%20by%20utilizing%20these%0Atools%20over%20a%20long%20horizon%20with%20minimal%20human-defined%20pipelines.%20Empirical%0Aresults%20show%20that%20SR-Scientist%20outperforms%20baseline%20methods%20by%20an%20absolute%0Amargin%20of%206%25%20to%2035%25%20on%20datasets%20covering%20four%20science%20disciplines.%0AAdditionally%2C%20we%20demonstrate%20our%20method%27s%20robustness%20to%20noise%2C%20the%0Ageneralization%20of%20the%20discovered%20equations%20to%20out-of-domain%20data%2C%20and%20their%0Asymbolic%20accuracy.%20Furthermore%2C%20we%20develop%20an%20end-to-end%20reinforcement%20learning%0Aframework%20to%20enhance%20the%20agent%27s%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11661v1&entry.124074799=Read"},
{"title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion\n  Large Language Models", "author": "Nianyi Lin and Jiajie Zhang and Lei Hou and Juanzi Li", "abstract": "  A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks.\n", "link": "http://arxiv.org/abs/2510.11683v1", "date": "2025-10-13", "relevancy": 1.4891, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5186}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4984}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boundary-Guided%20Policy%20Optimization%20for%20Memory-efficient%20RL%20of%20Diffusion%0A%20%20Large%20Language%20Models&body=Title%3A%20Boundary-Guided%20Policy%20Optimization%20for%20Memory-efficient%20RL%20of%20Diffusion%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Nianyi%20Lin%20and%20Jiajie%20Zhang%20and%20Lei%20Hou%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20A%20key%20challenge%20in%20applying%20reinforcement%20learning%20%28RL%29%20to%20diffusion%20large%0Alanguage%20models%20%28dLLMs%29%20lies%20in%20the%20intractability%20of%20their%20likelihood%0Afunctions%2C%20which%20are%20essential%20for%20the%20RL%20objective%2C%20necessitating%0Acorresponding%20approximation%20in%20each%20training%20step.%20While%20existing%20methods%0Aapproximate%20the%20log-likelihoods%20by%20their%20evidence%20lower%20bounds%20%28ELBOs%29%20via%0Acustomized%20Monte%20Carlo%20%28MC%29%20sampling%2C%20the%20forward%20computational%20graphs%20of%20all%0AMC%20samples%20need%20to%20be%20retained%20for%20the%20gradient%20computation%20of%20non-linear%20terms%0Ain%20the%20RL%20objective%2C%20resulting%20in%20significant%20memory%20overhead.%20This%20constraint%0Arestricts%20feasible%20sample%20sizes%2C%20leading%20to%20imprecise%20likelihood%20approximations%0Aand%20ultimately%20distorting%20the%20RL%20objective.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20%5Cemph%7BBoundary-Guided%20Policy%20Optimization%7D%20%28BGPO%29%2C%20a%20memory-efficient%0ARL%20algorithm%20that%20maximizes%20a%20specially%20constructed%20lower%20bound%20of%20the%0AELBO-based%20objective.%20This%20lower%20bound%20is%20carefully%20designed%20to%20satisfy%20two%20key%0Aproperties%3A%20%281%29%20Linearity%3A%20it%20is%20formulated%20in%20a%20linear%20sum%20where%20each%20term%0Adepends%20only%20on%20a%20single%20MC%20sample%2C%20thereby%20enabling%20gradient%20accumulation%0Aacross%20samples%20and%20ensuring%20constant%20memory%20usage%3B%20%282%29%20Equivalence%3A%20Both%20the%0Avalue%20and%20gradient%20of%20this%20lower%20bound%20are%20equal%20to%20those%20of%20the%20ELBO-based%0Aobjective%20in%20on-policy%20training%2C%20making%20it%20also%20an%20effective%20approximation%20for%0Athe%20original%20RL%20objective.%20These%20properties%20allow%20BGPO%20to%20adopt%20a%20large%20MC%0Asample%20size%2C%20resulting%20in%20more%20accurate%20likelihood%20approximations%20and%20improved%0ARL%20objective%20estimation%2C%20which%20in%20turn%20leads%20to%20enhanced%20performance.%0AExperiments%20show%20that%20BGPO%20significantly%20outperforms%20previous%20RL%20algorithms%20for%0AdLLMs%20in%20math%20problem%20solving%2C%20code%20generation%2C%20and%20planning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoundary-Guided%2520Policy%2520Optimization%2520for%2520Memory-efficient%2520RL%2520of%2520Diffusion%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DNianyi%2520Lin%2520and%2520Jiajie%2520Zhang%2520and%2520Lei%2520Hou%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520A%2520key%2520challenge%2520in%2520applying%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520diffusion%2520large%250Alanguage%2520models%2520%2528dLLMs%2529%2520lies%2520in%2520the%2520intractability%2520of%2520their%2520likelihood%250Afunctions%252C%2520which%2520are%2520essential%2520for%2520the%2520RL%2520objective%252C%2520necessitating%250Acorresponding%2520approximation%2520in%2520each%2520training%2520step.%2520While%2520existing%2520methods%250Aapproximate%2520the%2520log-likelihoods%2520by%2520their%2520evidence%2520lower%2520bounds%2520%2528ELBOs%2529%2520via%250Acustomized%2520Monte%2520Carlo%2520%2528MC%2529%2520sampling%252C%2520the%2520forward%2520computational%2520graphs%2520of%2520all%250AMC%2520samples%2520need%2520to%2520be%2520retained%2520for%2520the%2520gradient%2520computation%2520of%2520non-linear%2520terms%250Ain%2520the%2520RL%2520objective%252C%2520resulting%2520in%2520significant%2520memory%2520overhead.%2520This%2520constraint%250Arestricts%2520feasible%2520sample%2520sizes%252C%2520leading%2520to%2520imprecise%2520likelihood%2520approximations%250Aand%2520ultimately%2520distorting%2520the%2520RL%2520objective.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Apropose%2520%255Cemph%257BBoundary-Guided%2520Policy%2520Optimization%257D%2520%2528BGPO%2529%252C%2520a%2520memory-efficient%250ARL%2520algorithm%2520that%2520maximizes%2520a%2520specially%2520constructed%2520lower%2520bound%2520of%2520the%250AELBO-based%2520objective.%2520This%2520lower%2520bound%2520is%2520carefully%2520designed%2520to%2520satisfy%2520two%2520key%250Aproperties%253A%2520%25281%2529%2520Linearity%253A%2520it%2520is%2520formulated%2520in%2520a%2520linear%2520sum%2520where%2520each%2520term%250Adepends%2520only%2520on%2520a%2520single%2520MC%2520sample%252C%2520thereby%2520enabling%2520gradient%2520accumulation%250Aacross%2520samples%2520and%2520ensuring%2520constant%2520memory%2520usage%253B%2520%25282%2529%2520Equivalence%253A%2520Both%2520the%250Avalue%2520and%2520gradient%2520of%2520this%2520lower%2520bound%2520are%2520equal%2520to%2520those%2520of%2520the%2520ELBO-based%250Aobjective%2520in%2520on-policy%2520training%252C%2520making%2520it%2520also%2520an%2520effective%2520approximation%2520for%250Athe%2520original%2520RL%2520objective.%2520These%2520properties%2520allow%2520BGPO%2520to%2520adopt%2520a%2520large%2520MC%250Asample%2520size%252C%2520resulting%2520in%2520more%2520accurate%2520likelihood%2520approximations%2520and%2520improved%250ARL%2520objective%2520estimation%252C%2520which%2520in%2520turn%2520leads%2520to%2520enhanced%2520performance.%250AExperiments%2520show%2520that%2520BGPO%2520significantly%2520outperforms%2520previous%2520RL%2520algorithms%2520for%250AdLLMs%2520in%2520math%2520problem%2520solving%252C%2520code%2520generation%252C%2520and%2520planning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boundary-Guided%20Policy%20Optimization%20for%20Memory-efficient%20RL%20of%20Diffusion%0A%20%20Large%20Language%20Models&entry.906535625=Nianyi%20Lin%20and%20Jiajie%20Zhang%20and%20Lei%20Hou%20and%20Juanzi%20Li&entry.1292438233=%20%20A%20key%20challenge%20in%20applying%20reinforcement%20learning%20%28RL%29%20to%20diffusion%20large%0Alanguage%20models%20%28dLLMs%29%20lies%20in%20the%20intractability%20of%20their%20likelihood%0Afunctions%2C%20which%20are%20essential%20for%20the%20RL%20objective%2C%20necessitating%0Acorresponding%20approximation%20in%20each%20training%20step.%20While%20existing%20methods%0Aapproximate%20the%20log-likelihoods%20by%20their%20evidence%20lower%20bounds%20%28ELBOs%29%20via%0Acustomized%20Monte%20Carlo%20%28MC%29%20sampling%2C%20the%20forward%20computational%20graphs%20of%20all%0AMC%20samples%20need%20to%20be%20retained%20for%20the%20gradient%20computation%20of%20non-linear%20terms%0Ain%20the%20RL%20objective%2C%20resulting%20in%20significant%20memory%20overhead.%20This%20constraint%0Arestricts%20feasible%20sample%20sizes%2C%20leading%20to%20imprecise%20likelihood%20approximations%0Aand%20ultimately%20distorting%20the%20RL%20objective.%20To%20overcome%20this%20limitation%2C%20we%0Apropose%20%5Cemph%7BBoundary-Guided%20Policy%20Optimization%7D%20%28BGPO%29%2C%20a%20memory-efficient%0ARL%20algorithm%20that%20maximizes%20a%20specially%20constructed%20lower%20bound%20of%20the%0AELBO-based%20objective.%20This%20lower%20bound%20is%20carefully%20designed%20to%20satisfy%20two%20key%0Aproperties%3A%20%281%29%20Linearity%3A%20it%20is%20formulated%20in%20a%20linear%20sum%20where%20each%20term%0Adepends%20only%20on%20a%20single%20MC%20sample%2C%20thereby%20enabling%20gradient%20accumulation%0Aacross%20samples%20and%20ensuring%20constant%20memory%20usage%3B%20%282%29%20Equivalence%3A%20Both%20the%0Avalue%20and%20gradient%20of%20this%20lower%20bound%20are%20equal%20to%20those%20of%20the%20ELBO-based%0Aobjective%20in%20on-policy%20training%2C%20making%20it%20also%20an%20effective%20approximation%20for%0Athe%20original%20RL%20objective.%20These%20properties%20allow%20BGPO%20to%20adopt%20a%20large%20MC%0Asample%20size%2C%20resulting%20in%20more%20accurate%20likelihood%20approximations%20and%20improved%0ARL%20objective%20estimation%2C%20which%20in%20turn%20leads%20to%20enhanced%20performance.%0AExperiments%20show%20that%20BGPO%20significantly%20outperforms%20previous%20RL%20algorithms%20for%0AdLLMs%20in%20math%20problem%20solving%2C%20code%20generation%2C%20and%20planning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11683v1&entry.124074799=Read"},
{"title": "Part-of-speech tagging for Nagamese Language using CRF", "author": "Alovi N Shohe and Chonglio Khiamungam and Teisovi Angami", "abstract": "  This paper investigates part-of-speech tagging, an important task in Natural\nLanguage Processing (NLP) for the Nagamese language. The Nagamese language,\na.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily\nas a means of communication in trade between the Nagas and people from Assam in\nnortheast India. A substantial amount of work in part-of-speech-tagging has\nbeen done for resource-rich languages like English, Hindi, etc. However, no\nwork has been done in the Nagamese language. To the best of our knowledge, this\nis the first attempt at part-of-speech tagging for the Nagamese Language. The\naim of this work is to identify the part-of-speech for a given sentence in the\nNagamese language. An annotated corpus of 16,112 tokens is created and applied\nmachine learning technique known as Conditional Random Fields (CRF). Using CRF,\nan overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score\nof 85% is achieved.\n  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF.\n", "link": "http://arxiv.org/abs/2509.19343v3", "date": "2025-10-13", "relevancy": 1.4818, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3953}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3687}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Part-of-speech%20tagging%20for%20Nagamese%20Language%20using%20CRF&body=Title%3A%20Part-of-speech%20tagging%20for%20Nagamese%20Language%20using%20CRF%0AAuthor%3A%20Alovi%20N%20Shohe%20and%20Chonglio%20Khiamungam%20and%20Teisovi%20Angami%0AAbstract%3A%20%20%20This%20paper%20investigates%20part-of-speech%20tagging%2C%20an%20important%20task%20in%20Natural%0ALanguage%20Processing%20%28NLP%29%20for%20the%20Nagamese%20language.%20The%20Nagamese%20language%2C%0Aa.k.a.%20Naga%20Pidgin%2C%20is%20an%20Assamese-lexified%20Creole%20language%20developed%20primarily%0Aas%20a%20means%20of%20communication%20in%20trade%20between%20the%20Nagas%20and%20people%20from%20Assam%20in%0Anortheast%20India.%20A%20substantial%20amount%20of%20work%20in%20part-of-speech-tagging%20has%0Abeen%20done%20for%20resource-rich%20languages%20like%20English%2C%20Hindi%2C%20etc.%20However%2C%20no%0Awork%20has%20been%20done%20in%20the%20Nagamese%20language.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20attempt%20at%20part-of-speech%20tagging%20for%20the%20Nagamese%20Language.%20The%0Aaim%20of%20this%20work%20is%20to%20identify%20the%20part-of-speech%20for%20a%20given%20sentence%20in%20the%0ANagamese%20language.%20An%20annotated%20corpus%20of%2016%2C112%20tokens%20is%20created%20and%20applied%0Amachine%20learning%20technique%20known%20as%20Conditional%20Random%20Fields%20%28CRF%29.%20Using%20CRF%2C%0Aan%20overall%20tagging%20accuracy%20of%2085.70%25%3B%20precision%2C%20recall%20of%2086%25%2C%20and%20f1-score%0Aof%2085%25%20is%20achieved.%0A%20%20Keywords.%20Nagamese%2C%20NLP%2C%20part-of-speech%2C%20machine%20learning%2C%20CRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.19343v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPart-of-speech%2520tagging%2520for%2520Nagamese%2520Language%2520using%2520CRF%26entry.906535625%3DAlovi%2520N%2520Shohe%2520and%2520Chonglio%2520Khiamungam%2520and%2520Teisovi%2520Angami%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520part-of-speech%2520tagging%252C%2520an%2520important%2520task%2520in%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529%2520for%2520the%2520Nagamese%2520language.%2520The%2520Nagamese%2520language%252C%250Aa.k.a.%2520Naga%2520Pidgin%252C%2520is%2520an%2520Assamese-lexified%2520Creole%2520language%2520developed%2520primarily%250Aas%2520a%2520means%2520of%2520communication%2520in%2520trade%2520between%2520the%2520Nagas%2520and%2520people%2520from%2520Assam%2520in%250Anortheast%2520India.%2520A%2520substantial%2520amount%2520of%2520work%2520in%2520part-of-speech-tagging%2520has%250Abeen%2520done%2520for%2520resource-rich%2520languages%2520like%2520English%252C%2520Hindi%252C%2520etc.%2520However%252C%2520no%250Awork%2520has%2520been%2520done%2520in%2520the%2520Nagamese%2520language.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520attempt%2520at%2520part-of-speech%2520tagging%2520for%2520the%2520Nagamese%2520Language.%2520The%250Aaim%2520of%2520this%2520work%2520is%2520to%2520identify%2520the%2520part-of-speech%2520for%2520a%2520given%2520sentence%2520in%2520the%250ANagamese%2520language.%2520An%2520annotated%2520corpus%2520of%252016%252C112%2520tokens%2520is%2520created%2520and%2520applied%250Amachine%2520learning%2520technique%2520known%2520as%2520Conditional%2520Random%2520Fields%2520%2528CRF%2529.%2520Using%2520CRF%252C%250Aan%2520overall%2520tagging%2520accuracy%2520of%252085.70%2525%253B%2520precision%252C%2520recall%2520of%252086%2525%252C%2520and%2520f1-score%250Aof%252085%2525%2520is%2520achieved.%250A%2520%2520Keywords.%2520Nagamese%252C%2520NLP%252C%2520part-of-speech%252C%2520machine%2520learning%252C%2520CRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19343v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Part-of-speech%20tagging%20for%20Nagamese%20Language%20using%20CRF&entry.906535625=Alovi%20N%20Shohe%20and%20Chonglio%20Khiamungam%20and%20Teisovi%20Angami&entry.1292438233=%20%20This%20paper%20investigates%20part-of-speech%20tagging%2C%20an%20important%20task%20in%20Natural%0ALanguage%20Processing%20%28NLP%29%20for%20the%20Nagamese%20language.%20The%20Nagamese%20language%2C%0Aa.k.a.%20Naga%20Pidgin%2C%20is%20an%20Assamese-lexified%20Creole%20language%20developed%20primarily%0Aas%20a%20means%20of%20communication%20in%20trade%20between%20the%20Nagas%20and%20people%20from%20Assam%20in%0Anortheast%20India.%20A%20substantial%20amount%20of%20work%20in%20part-of-speech-tagging%20has%0Abeen%20done%20for%20resource-rich%20languages%20like%20English%2C%20Hindi%2C%20etc.%20However%2C%20no%0Awork%20has%20been%20done%20in%20the%20Nagamese%20language.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20attempt%20at%20part-of-speech%20tagging%20for%20the%20Nagamese%20Language.%20The%0Aaim%20of%20this%20work%20is%20to%20identify%20the%20part-of-speech%20for%20a%20given%20sentence%20in%20the%0ANagamese%20language.%20An%20annotated%20corpus%20of%2016%2C112%20tokens%20is%20created%20and%20applied%0Amachine%20learning%20technique%20known%20as%20Conditional%20Random%20Fields%20%28CRF%29.%20Using%20CRF%2C%0Aan%20overall%20tagging%20accuracy%20of%2085.70%25%3B%20precision%2C%20recall%20of%2086%25%2C%20and%20f1-score%0Aof%2085%25%20is%20achieved.%0A%20%20Keywords.%20Nagamese%2C%20NLP%2C%20part-of-speech%2C%20machine%20learning%2C%20CRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.19343v3&entry.124074799=Read"},
{"title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems", "author": "Shiqi Zhang and Xinbei Ma and Yunqing Xu and Zouying Cao and Pengrui Lu and Haobo Yuan and Tiancheng Shen and Zhuosheng Zhang and Hai Zhao and Ming-Hsuan Yang", "abstract": "  Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.\n", "link": "http://arxiv.org/abs/2510.11608v1", "date": "2025-10-13", "relevancy": 1.3885, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4983}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4543}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ParaCook%3A%20On%20Time-Efficient%20Planning%20for%20Multi-Agent%20Systems&body=Title%3A%20ParaCook%3A%20On%20Time-Efficient%20Planning%20for%20Multi-Agent%20Systems%0AAuthor%3A%20Shiqi%20Zhang%20and%20Xinbei%20Ma%20and%20Yunqing%20Xu%20and%20Zouying%20Cao%20and%20Pengrui%20Lu%20and%20Haobo%20Yuan%20and%20Tiancheng%20Shen%20and%20Zhuosheng%20Zhang%20and%20Hai%20Zhao%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20strong%20reasoning%20abilities%20for%20planning%0Along-horizon%2C%20real-world%20tasks%2C%20yet%20existing%20agent%20benchmarks%20focus%20on%20task%0Acompletion%20while%20neglecting%20time%20efficiency%20in%20parallel%20and%20asynchronous%0Aoperations.%20To%20address%20this%2C%20we%20present%20ParaCook%2C%20a%20benchmark%20for%0Atime-efficient%20collaborative%20planning.%20Inspired%20by%20the%20Overcooked%20game%2C%0AParaCook%20provides%20an%20environment%20for%20various%20challenging%20interaction%20planning%0Aof%20multi-agent%20systems%20that%20are%20instantiated%20as%20cooking%20tasks%2C%20with%20a%0Asimplified%20action%20space%20to%20isolate%20the%20core%20challenge%20of%20strategic%20parallel%0Aplanning.%20Through%20a%20comprehensive%20evaluation%20of%20state-of-the-art%20LLMs%2C%20we%20find%0Athat%20current%20approaches%20achieve%20suboptimal%20plans%2C%20which%20struggle%20with%20parallel%0Aactions%20or%20coordination.%20Our%20analysis%20also%20reveals%20LLMs%27%20potential%20on%20abstract%0Atasks%20where%20they%20can%20focus%20on%20high-level%20parallel%20optimization.%20ParaCook%0Aprovides%20a%20scalable%20evaluation%20framework%20with%20adjustable%20complexity%2C%0Aestablishing%20a%20foundation%20for%20developing%20and%20assessing%20time%20efficiency-aware%0Amulti-agent%20planning.%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/zsq259/ParaCook.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParaCook%253A%2520On%2520Time-Efficient%2520Planning%2520for%2520Multi-Agent%2520Systems%26entry.906535625%3DShiqi%2520Zhang%2520and%2520Xinbei%2520Ma%2520and%2520Yunqing%2520Xu%2520and%2520Zouying%2520Cao%2520and%2520Pengrui%2520Lu%2520and%2520Haobo%2520Yuan%2520and%2520Tiancheng%2520Shen%2520and%2520Zhuosheng%2520Zhang%2520and%2520Hai%2520Zhao%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520strong%2520reasoning%2520abilities%2520for%2520planning%250Along-horizon%252C%2520real-world%2520tasks%252C%2520yet%2520existing%2520agent%2520benchmarks%2520focus%2520on%2520task%250Acompletion%2520while%2520neglecting%2520time%2520efficiency%2520in%2520parallel%2520and%2520asynchronous%250Aoperations.%2520To%2520address%2520this%252C%2520we%2520present%2520ParaCook%252C%2520a%2520benchmark%2520for%250Atime-efficient%2520collaborative%2520planning.%2520Inspired%2520by%2520the%2520Overcooked%2520game%252C%250AParaCook%2520provides%2520an%2520environment%2520for%2520various%2520challenging%2520interaction%2520planning%250Aof%2520multi-agent%2520systems%2520that%2520are%2520instantiated%2520as%2520cooking%2520tasks%252C%2520with%2520a%250Asimplified%2520action%2520space%2520to%2520isolate%2520the%2520core%2520challenge%2520of%2520strategic%2520parallel%250Aplanning.%2520Through%2520a%2520comprehensive%2520evaluation%2520of%2520state-of-the-art%2520LLMs%252C%2520we%2520find%250Athat%2520current%2520approaches%2520achieve%2520suboptimal%2520plans%252C%2520which%2520struggle%2520with%2520parallel%250Aactions%2520or%2520coordination.%2520Our%2520analysis%2520also%2520reveals%2520LLMs%2527%2520potential%2520on%2520abstract%250Atasks%2520where%2520they%2520can%2520focus%2520on%2520high-level%2520parallel%2520optimization.%2520ParaCook%250Aprovides%2520a%2520scalable%2520evaluation%2520framework%2520with%2520adjustable%2520complexity%252C%250Aestablishing%2520a%2520foundation%2520for%2520developing%2520and%2520assessing%2520time%2520efficiency-aware%250Amulti-agent%2520planning.%2520The%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/zsq259/ParaCook.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ParaCook%3A%20On%20Time-Efficient%20Planning%20for%20Multi-Agent%20Systems&entry.906535625=Shiqi%20Zhang%20and%20Xinbei%20Ma%20and%20Yunqing%20Xu%20and%20Zouying%20Cao%20and%20Pengrui%20Lu%20and%20Haobo%20Yuan%20and%20Tiancheng%20Shen%20and%20Zhuosheng%20Zhang%20and%20Hai%20Zhao%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20strong%20reasoning%20abilities%20for%20planning%0Along-horizon%2C%20real-world%20tasks%2C%20yet%20existing%20agent%20benchmarks%20focus%20on%20task%0Acompletion%20while%20neglecting%20time%20efficiency%20in%20parallel%20and%20asynchronous%0Aoperations.%20To%20address%20this%2C%20we%20present%20ParaCook%2C%20a%20benchmark%20for%0Atime-efficient%20collaborative%20planning.%20Inspired%20by%20the%20Overcooked%20game%2C%0AParaCook%20provides%20an%20environment%20for%20various%20challenging%20interaction%20planning%0Aof%20multi-agent%20systems%20that%20are%20instantiated%20as%20cooking%20tasks%2C%20with%20a%0Asimplified%20action%20space%20to%20isolate%20the%20core%20challenge%20of%20strategic%20parallel%0Aplanning.%20Through%20a%20comprehensive%20evaluation%20of%20state-of-the-art%20LLMs%2C%20we%20find%0Athat%20current%20approaches%20achieve%20suboptimal%20plans%2C%20which%20struggle%20with%20parallel%0Aactions%20or%20coordination.%20Our%20analysis%20also%20reveals%20LLMs%27%20potential%20on%20abstract%0Atasks%20where%20they%20can%20focus%20on%20high-level%20parallel%20optimization.%20ParaCook%0Aprovides%20a%20scalable%20evaluation%20framework%20with%20adjustable%20complexity%2C%0Aestablishing%20a%20foundation%20for%20developing%20and%20assessing%20time%20efficiency-aware%0Amulti-agent%20planning.%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/zsq259/ParaCook.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11608v1&entry.124074799=Read"},
{"title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability", "author": "Ruida Wang and Yuxin Li and Yi R. Fung and Tong Zhang", "abstract": "  Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an\nend-to-end framework designed to incorporate the FL expert into NL math\nproblem-solving. To bridge the NL and FL input format gap, we propose the NL-FL\nProblem Alignment method, which reformulates the Question-Answering (QA)\nproblems in NL as existence theorems in FL. Subsequently, the Mixed Problem\nInput technique we provide enables the FL reasoner to handle both QA and\nexistence problems concurrently. Lastly, we mitigate the NL and FL output\nformat gap in reasoning through an LLM-based Answer Extraction mechanism.\nComprehensive experiments demonstrate that the NFL-HR framework achieves\n**89.80**% and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by **4.60%** and **4.82%**,\nrespectively. Notably, some problems resolved by our framework remain unsolved\nby the NL baseline model even under a larger number of trials.\n", "link": "http://arxiv.org/abs/2505.23703v4", "date": "2025-10-13", "relevancy": 1.3882, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4646}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4625}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Reason%20Formally%3A%20Natural-Formal%20Hybrid%20Reasoning%20Enhances%20LLM%27s%0A%20%20Math%20Capability&body=Title%3A%20Let%27s%20Reason%20Formally%3A%20Natural-Formal%20Hybrid%20Reasoning%20Enhances%20LLM%27s%0A%20%20Math%20Capability%0AAuthor%3A%20Ruida%20Wang%20and%20Yuxin%20Li%20and%20Yi%20R.%20Fung%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Enhancing%20the%20mathematical%20reasoning%20capabilities%20of%20LLMs%20has%20garnered%0Asignificant%20attention%20in%20both%20the%20mathematical%20and%20computer%20science%0Acommunities.%20Recent%20works%20have%20made%20substantial%20progress%20in%20both%20Natural%0ALanguage%20%28NL%29%20reasoning%20and%20Formal%20Language%20%28FL%29%20reasoning%20by%20leveraging%20the%0Apotential%20of%20pure%20Reinforcement%20Learning%20%28RL%29%20methods%20on%20base%20models.%20However%2C%0ARL%20approaches%20struggle%20to%20impart%20new%20capabilities%20not%20presented%20in%20the%20base%0Amodel%2C%20highlighting%20the%20need%20to%20integrate%20more%20knowledge%20like%20FL%20into%20NL%20math%0Areasoning%20effectively.%20Yet%2C%20this%20integration%20is%20challenging%20due%20to%20inherent%0Adisparities%20in%20problem%20structure%20and%20reasoning%20format%20between%20NL%20and%20FL.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20%2A%2ANL-FL%20HybridReasoning%20%28NFL-HR%29%2A%2A%2C%20an%0Aend-to-end%20framework%20designed%20to%20incorporate%20the%20FL%20expert%20into%20NL%20math%0Aproblem-solving.%20To%20bridge%20the%20NL%20and%20FL%20input%20format%20gap%2C%20we%20propose%20the%20NL-FL%0AProblem%20Alignment%20method%2C%20which%20reformulates%20the%20Question-Answering%20%28QA%29%0Aproblems%20in%20NL%20as%20existence%20theorems%20in%20FL.%20Subsequently%2C%20the%20Mixed%20Problem%0AInput%20technique%20we%20provide%20enables%20the%20FL%20reasoner%20to%20handle%20both%20QA%20and%0Aexistence%20problems%20concurrently.%20Lastly%2C%20we%20mitigate%20the%20NL%20and%20FL%20output%0Aformat%20gap%20in%20reasoning%20through%20an%20LLM-based%20Answer%20Extraction%20mechanism.%0AComprehensive%20experiments%20demonstrate%20that%20the%20NFL-HR%20framework%20achieves%0A%2A%2A89.80%2A%2A%25%20and%20%2A%2A84.34%25%2A%2A%20accuracy%20rates%20on%20the%20MATH-500%20and%20the%20AMC%0Abenchmarks%2C%20surpassing%20the%20NL%20baseline%20by%20%2A%2A4.60%25%2A%2A%20and%20%2A%2A4.82%25%2A%2A%2C%0Arespectively.%20Notably%2C%20some%20problems%20resolved%20by%20our%20framework%20remain%20unsolved%0Aby%20the%20NL%20baseline%20model%20even%20under%20a%20larger%20number%20of%20trials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23703v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Reason%2520Formally%253A%2520Natural-Formal%2520Hybrid%2520Reasoning%2520Enhances%2520LLM%2527s%250A%2520%2520Math%2520Capability%26entry.906535625%3DRuida%2520Wang%2520and%2520Yuxin%2520Li%2520and%2520Yi%2520R.%2520Fung%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Enhancing%2520the%2520mathematical%2520reasoning%2520capabilities%2520of%2520LLMs%2520has%2520garnered%250Asignificant%2520attention%2520in%2520both%2520the%2520mathematical%2520and%2520computer%2520science%250Acommunities.%2520Recent%2520works%2520have%2520made%2520substantial%2520progress%2520in%2520both%2520Natural%250ALanguage%2520%2528NL%2529%2520reasoning%2520and%2520Formal%2520Language%2520%2528FL%2529%2520reasoning%2520by%2520leveraging%2520the%250Apotential%2520of%2520pure%2520Reinforcement%2520Learning%2520%2528RL%2529%2520methods%2520on%2520base%2520models.%2520However%252C%250ARL%2520approaches%2520struggle%2520to%2520impart%2520new%2520capabilities%2520not%2520presented%2520in%2520the%2520base%250Amodel%252C%2520highlighting%2520the%2520need%2520to%2520integrate%2520more%2520knowledge%2520like%2520FL%2520into%2520NL%2520math%250Areasoning%2520effectively.%2520Yet%252C%2520this%2520integration%2520is%2520challenging%2520due%2520to%2520inherent%250Adisparities%2520in%2520problem%2520structure%2520and%2520reasoning%2520format%2520between%2520NL%2520and%2520FL.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520%252A%252ANL-FL%2520HybridReasoning%2520%2528NFL-HR%2529%252A%252A%252C%2520an%250Aend-to-end%2520framework%2520designed%2520to%2520incorporate%2520the%2520FL%2520expert%2520into%2520NL%2520math%250Aproblem-solving.%2520To%2520bridge%2520the%2520NL%2520and%2520FL%2520input%2520format%2520gap%252C%2520we%2520propose%2520the%2520NL-FL%250AProblem%2520Alignment%2520method%252C%2520which%2520reformulates%2520the%2520Question-Answering%2520%2528QA%2529%250Aproblems%2520in%2520NL%2520as%2520existence%2520theorems%2520in%2520FL.%2520Subsequently%252C%2520the%2520Mixed%2520Problem%250AInput%2520technique%2520we%2520provide%2520enables%2520the%2520FL%2520reasoner%2520to%2520handle%2520both%2520QA%2520and%250Aexistence%2520problems%2520concurrently.%2520Lastly%252C%2520we%2520mitigate%2520the%2520NL%2520and%2520FL%2520output%250Aformat%2520gap%2520in%2520reasoning%2520through%2520an%2520LLM-based%2520Answer%2520Extraction%2520mechanism.%250AComprehensive%2520experiments%2520demonstrate%2520that%2520the%2520NFL-HR%2520framework%2520achieves%250A%252A%252A89.80%252A%252A%2525%2520and%2520%252A%252A84.34%2525%252A%252A%2520accuracy%2520rates%2520on%2520the%2520MATH-500%2520and%2520the%2520AMC%250Abenchmarks%252C%2520surpassing%2520the%2520NL%2520baseline%2520by%2520%252A%252A4.60%2525%252A%252A%2520and%2520%252A%252A4.82%2525%252A%252A%252C%250Arespectively.%2520Notably%252C%2520some%2520problems%2520resolved%2520by%2520our%2520framework%2520remain%2520unsolved%250Aby%2520the%2520NL%2520baseline%2520model%2520even%2520under%2520a%2520larger%2520number%2520of%2520trials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23703v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Reason%20Formally%3A%20Natural-Formal%20Hybrid%20Reasoning%20Enhances%20LLM%27s%0A%20%20Math%20Capability&entry.906535625=Ruida%20Wang%20and%20Yuxin%20Li%20and%20Yi%20R.%20Fung%20and%20Tong%20Zhang&entry.1292438233=%20%20Enhancing%20the%20mathematical%20reasoning%20capabilities%20of%20LLMs%20has%20garnered%0Asignificant%20attention%20in%20both%20the%20mathematical%20and%20computer%20science%0Acommunities.%20Recent%20works%20have%20made%20substantial%20progress%20in%20both%20Natural%0ALanguage%20%28NL%29%20reasoning%20and%20Formal%20Language%20%28FL%29%20reasoning%20by%20leveraging%20the%0Apotential%20of%20pure%20Reinforcement%20Learning%20%28RL%29%20methods%20on%20base%20models.%20However%2C%0ARL%20approaches%20struggle%20to%20impart%20new%20capabilities%20not%20presented%20in%20the%20base%0Amodel%2C%20highlighting%20the%20need%20to%20integrate%20more%20knowledge%20like%20FL%20into%20NL%20math%0Areasoning%20effectively.%20Yet%2C%20this%20integration%20is%20challenging%20due%20to%20inherent%0Adisparities%20in%20problem%20structure%20and%20reasoning%20format%20between%20NL%20and%20FL.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20%2A%2ANL-FL%20HybridReasoning%20%28NFL-HR%29%2A%2A%2C%20an%0Aend-to-end%20framework%20designed%20to%20incorporate%20the%20FL%20expert%20into%20NL%20math%0Aproblem-solving.%20To%20bridge%20the%20NL%20and%20FL%20input%20format%20gap%2C%20we%20propose%20the%20NL-FL%0AProblem%20Alignment%20method%2C%20which%20reformulates%20the%20Question-Answering%20%28QA%29%0Aproblems%20in%20NL%20as%20existence%20theorems%20in%20FL.%20Subsequently%2C%20the%20Mixed%20Problem%0AInput%20technique%20we%20provide%20enables%20the%20FL%20reasoner%20to%20handle%20both%20QA%20and%0Aexistence%20problems%20concurrently.%20Lastly%2C%20we%20mitigate%20the%20NL%20and%20FL%20output%0Aformat%20gap%20in%20reasoning%20through%20an%20LLM-based%20Answer%20Extraction%20mechanism.%0AComprehensive%20experiments%20demonstrate%20that%20the%20NFL-HR%20framework%20achieves%0A%2A%2A89.80%2A%2A%25%20and%20%2A%2A84.34%25%2A%2A%20accuracy%20rates%20on%20the%20MATH-500%20and%20the%20AMC%0Abenchmarks%2C%20surpassing%20the%20NL%20baseline%20by%20%2A%2A4.60%25%2A%2A%20and%20%2A%2A4.82%25%2A%2A%2C%0Arespectively.%20Notably%2C%20some%20problems%20resolved%20by%20our%20framework%20remain%20unsolved%0Aby%20the%20NL%20baseline%20model%20even%20under%20a%20larger%20number%20of%20trials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23703v4&entry.124074799=Read"},
{"title": "Accelerated stochastic first-order method for convex optimization under\n  heavy-tailed noise", "author": "Chuan He and Zhaosong Lu", "abstract": "  We study convex composite optimization problems, where the objective function\nis given by the sum of a prox-friendly function and a convex function whose\nsubgradients are estimated under heavy-tailed noise. Existing work often\nemploys gradient clipping or normalization techniques in stochastic first-order\nmethods to address heavy-tailed noise. In this paper, we demonstrate that a\nvanilla stochastic algorithm -- without additional modifications such as\nclipping or normalization -- can achieve optimal complexity for these problems.\nIn particular, we establish that an accelerated stochastic proximal subgradient\nmethod achieves a first-order oracle complexity that is universally optimal for\nsmooth, weakly smooth, and nonsmooth convex optimization, as well as for\nstochastic convex optimization under heavy-tailed noise. Numerical experiments\nare further provided to validate our theoretical results.\n", "link": "http://arxiv.org/abs/2510.11676v1", "date": "2025-10-13", "relevancy": 1.3009, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4407}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4359}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20stochastic%20first-order%20method%20for%20convex%20optimization%20under%0A%20%20heavy-tailed%20noise&body=Title%3A%20Accelerated%20stochastic%20first-order%20method%20for%20convex%20optimization%20under%0A%20%20heavy-tailed%20noise%0AAuthor%3A%20Chuan%20He%20and%20Zhaosong%20Lu%0AAbstract%3A%20%20%20We%20study%20convex%20composite%20optimization%20problems%2C%20where%20the%20objective%20function%0Ais%20given%20by%20the%20sum%20of%20a%20prox-friendly%20function%20and%20a%20convex%20function%20whose%0Asubgradients%20are%20estimated%20under%20heavy-tailed%20noise.%20Existing%20work%20often%0Aemploys%20gradient%20clipping%20or%20normalization%20techniques%20in%20stochastic%20first-order%0Amethods%20to%20address%20heavy-tailed%20noise.%20In%20this%20paper%2C%20we%20demonstrate%20that%20a%0Avanilla%20stochastic%20algorithm%20--%20without%20additional%20modifications%20such%20as%0Aclipping%20or%20normalization%20--%20can%20achieve%20optimal%20complexity%20for%20these%20problems.%0AIn%20particular%2C%20we%20establish%20that%20an%20accelerated%20stochastic%20proximal%20subgradient%0Amethod%20achieves%20a%20first-order%20oracle%20complexity%20that%20is%20universally%20optimal%20for%0Asmooth%2C%20weakly%20smooth%2C%20and%20nonsmooth%20convex%20optimization%2C%20as%20well%20as%20for%0Astochastic%20convex%20optimization%20under%20heavy-tailed%20noise.%20Numerical%20experiments%0Aare%20further%20provided%20to%20validate%20our%20theoretical%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.11676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520stochastic%2520first-order%2520method%2520for%2520convex%2520optimization%2520under%250A%2520%2520heavy-tailed%2520noise%26entry.906535625%3DChuan%2520He%2520and%2520Zhaosong%2520Lu%26entry.1292438233%3D%2520%2520We%2520study%2520convex%2520composite%2520optimization%2520problems%252C%2520where%2520the%2520objective%2520function%250Ais%2520given%2520by%2520the%2520sum%2520of%2520a%2520prox-friendly%2520function%2520and%2520a%2520convex%2520function%2520whose%250Asubgradients%2520are%2520estimated%2520under%2520heavy-tailed%2520noise.%2520Existing%2520work%2520often%250Aemploys%2520gradient%2520clipping%2520or%2520normalization%2520techniques%2520in%2520stochastic%2520first-order%250Amethods%2520to%2520address%2520heavy-tailed%2520noise.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520a%250Avanilla%2520stochastic%2520algorithm%2520--%2520without%2520additional%2520modifications%2520such%2520as%250Aclipping%2520or%2520normalization%2520--%2520can%2520achieve%2520optimal%2520complexity%2520for%2520these%2520problems.%250AIn%2520particular%252C%2520we%2520establish%2520that%2520an%2520accelerated%2520stochastic%2520proximal%2520subgradient%250Amethod%2520achieves%2520a%2520first-order%2520oracle%2520complexity%2520that%2520is%2520universally%2520optimal%2520for%250Asmooth%252C%2520weakly%2520smooth%252C%2520and%2520nonsmooth%2520convex%2520optimization%252C%2520as%2520well%2520as%2520for%250Astochastic%2520convex%2520optimization%2520under%2520heavy-tailed%2520noise.%2520Numerical%2520experiments%250Aare%2520further%2520provided%2520to%2520validate%2520our%2520theoretical%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20stochastic%20first-order%20method%20for%20convex%20optimization%20under%0A%20%20heavy-tailed%20noise&entry.906535625=Chuan%20He%20and%20Zhaosong%20Lu&entry.1292438233=%20%20We%20study%20convex%20composite%20optimization%20problems%2C%20where%20the%20objective%20function%0Ais%20given%20by%20the%20sum%20of%20a%20prox-friendly%20function%20and%20a%20convex%20function%20whose%0Asubgradients%20are%20estimated%20under%20heavy-tailed%20noise.%20Existing%20work%20often%0Aemploys%20gradient%20clipping%20or%20normalization%20techniques%20in%20stochastic%20first-order%0Amethods%20to%20address%20heavy-tailed%20noise.%20In%20this%20paper%2C%20we%20demonstrate%20that%20a%0Avanilla%20stochastic%20algorithm%20--%20without%20additional%20modifications%20such%20as%0Aclipping%20or%20normalization%20--%20can%20achieve%20optimal%20complexity%20for%20these%20problems.%0AIn%20particular%2C%20we%20establish%20that%20an%20accelerated%20stochastic%20proximal%20subgradient%0Amethod%20achieves%20a%20first-order%20oracle%20complexity%20that%20is%20universally%20optimal%20for%0Asmooth%2C%20weakly%20smooth%2C%20and%20nonsmooth%20convex%20optimization%2C%20as%20well%20as%20for%0Astochastic%20convex%20optimization%20under%20heavy-tailed%20noise.%20Numerical%20experiments%0Aare%20further%20provided%20to%20validate%20our%20theoretical%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.11676v1&entry.124074799=Read"},
{"title": "On Convolutions, Intrinsic Dimension, and Diffusion Models", "author": "Kin Kwan Leung and Rasa Hosseinzadeh and Gabriel Loaiza-Ganem", "abstract": "  The manifold hypothesis asserts that data of interest in high-dimensional\nambient spaces, such as image data, lies on unknown low-dimensional\nsubmanifolds. Diffusion models (DMs) -- which operate by convolving data with\nprogressively larger amounts of Gaussian noise and then learning to revert this\nprocess -- have risen to prominence as the most performant generative models,\nand are known to be able to learn distributions with low-dimensional support.\nFor a given datum in one of these submanifolds, we should thus intuitively\nexpect DMs to have implicitly learned its corresponding local intrinsic\ndimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari\net al. (2024b) recently showed that this is indeed the case by linking this LID\nto the rate of change of the log marginal densities of the DM with respect to\nthe amount of added noise, resulting in an LID estimator known as FLIPD. LID\nestimators such as FLIPD have a plethora of uses, among others they quantify\nthe complexity of a given datum, and can be used to detect outliers,\nadversarial examples and AI-generated text. FLIPD achieves state-of-the-art\nperformance at LID estimation, yet its theoretical underpinnings are incomplete\nsince Kamkari et al. (2024b) only proved its correctness under the highly\nunrealistic assumption of affine submanifolds. In this work we bridge this gap\nby formally proving the correctness of FLIPD under realistic assumptions.\nAdditionally, we show that an analogous result holds when Gaussian convolutions\nare replaced with uniform ones, and discuss the relevance of this result.\n", "link": "http://arxiv.org/abs/2506.20705v2", "date": "2025-10-13", "relevancy": 1.0834, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5667}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5445}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Convolutions%2C%20Intrinsic%20Dimension%2C%20and%20Diffusion%20Models&body=Title%3A%20On%20Convolutions%2C%20Intrinsic%20Dimension%2C%20and%20Diffusion%20Models%0AAuthor%3A%20Kin%20Kwan%20Leung%20and%20Rasa%20Hosseinzadeh%20and%20Gabriel%20Loaiza-Ganem%0AAbstract%3A%20%20%20The%20manifold%20hypothesis%20asserts%20that%20data%20of%20interest%20in%20high-dimensional%0Aambient%20spaces%2C%20such%20as%20image%20data%2C%20lies%20on%20unknown%20low-dimensional%0Asubmanifolds.%20Diffusion%20models%20%28DMs%29%20--%20which%20operate%20by%20convolving%20data%20with%0Aprogressively%20larger%20amounts%20of%20Gaussian%20noise%20and%20then%20learning%20to%20revert%20this%0Aprocess%20--%20have%20risen%20to%20prominence%20as%20the%20most%20performant%20generative%20models%2C%0Aand%20are%20known%20to%20be%20able%20to%20learn%20distributions%20with%20low-dimensional%20support.%0AFor%20a%20given%20datum%20in%20one%20of%20these%20submanifolds%2C%20we%20should%20thus%20intuitively%0Aexpect%20DMs%20to%20have%20implicitly%20learned%20its%20corresponding%20local%20intrinsic%0Adimension%20%28LID%29%2C%20i.e.%20the%20dimension%20of%20the%20submanifold%20it%20belongs%20to.%20Kamkari%0Aet%20al.%20%282024b%29%20recently%20showed%20that%20this%20is%20indeed%20the%20case%20by%20linking%20this%20LID%0Ato%20the%20rate%20of%20change%20of%20the%20log%20marginal%20densities%20of%20the%20DM%20with%20respect%20to%0Athe%20amount%20of%20added%20noise%2C%20resulting%20in%20an%20LID%20estimator%20known%20as%20FLIPD.%20LID%0Aestimators%20such%20as%20FLIPD%20have%20a%20plethora%20of%20uses%2C%20among%20others%20they%20quantify%0Athe%20complexity%20of%20a%20given%20datum%2C%20and%20can%20be%20used%20to%20detect%20outliers%2C%0Aadversarial%20examples%20and%20AI-generated%20text.%20FLIPD%20achieves%20state-of-the-art%0Aperformance%20at%20LID%20estimation%2C%20yet%20its%20theoretical%20underpinnings%20are%20incomplete%0Asince%20Kamkari%20et%20al.%20%282024b%29%20only%20proved%20its%20correctness%20under%20the%20highly%0Aunrealistic%20assumption%20of%20affine%20submanifolds.%20In%20this%20work%20we%20bridge%20this%20gap%0Aby%20formally%20proving%20the%20correctness%20of%20FLIPD%20under%20realistic%20assumptions.%0AAdditionally%2C%20we%20show%20that%20an%20analogous%20result%20holds%20when%20Gaussian%20convolutions%0Aare%20replaced%20with%20uniform%20ones%2C%20and%20discuss%20the%20relevance%20of%20this%20result.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.20705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Convolutions%252C%2520Intrinsic%2520Dimension%252C%2520and%2520Diffusion%2520Models%26entry.906535625%3DKin%2520Kwan%2520Leung%2520and%2520Rasa%2520Hosseinzadeh%2520and%2520Gabriel%2520Loaiza-Ganem%26entry.1292438233%3D%2520%2520The%2520manifold%2520hypothesis%2520asserts%2520that%2520data%2520of%2520interest%2520in%2520high-dimensional%250Aambient%2520spaces%252C%2520such%2520as%2520image%2520data%252C%2520lies%2520on%2520unknown%2520low-dimensional%250Asubmanifolds.%2520Diffusion%2520models%2520%2528DMs%2529%2520--%2520which%2520operate%2520by%2520convolving%2520data%2520with%250Aprogressively%2520larger%2520amounts%2520of%2520Gaussian%2520noise%2520and%2520then%2520learning%2520to%2520revert%2520this%250Aprocess%2520--%2520have%2520risen%2520to%2520prominence%2520as%2520the%2520most%2520performant%2520generative%2520models%252C%250Aand%2520are%2520known%2520to%2520be%2520able%2520to%2520learn%2520distributions%2520with%2520low-dimensional%2520support.%250AFor%2520a%2520given%2520datum%2520in%2520one%2520of%2520these%2520submanifolds%252C%2520we%2520should%2520thus%2520intuitively%250Aexpect%2520DMs%2520to%2520have%2520implicitly%2520learned%2520its%2520corresponding%2520local%2520intrinsic%250Adimension%2520%2528LID%2529%252C%2520i.e.%2520the%2520dimension%2520of%2520the%2520submanifold%2520it%2520belongs%2520to.%2520Kamkari%250Aet%2520al.%2520%25282024b%2529%2520recently%2520showed%2520that%2520this%2520is%2520indeed%2520the%2520case%2520by%2520linking%2520this%2520LID%250Ato%2520the%2520rate%2520of%2520change%2520of%2520the%2520log%2520marginal%2520densities%2520of%2520the%2520DM%2520with%2520respect%2520to%250Athe%2520amount%2520of%2520added%2520noise%252C%2520resulting%2520in%2520an%2520LID%2520estimator%2520known%2520as%2520FLIPD.%2520LID%250Aestimators%2520such%2520as%2520FLIPD%2520have%2520a%2520plethora%2520of%2520uses%252C%2520among%2520others%2520they%2520quantify%250Athe%2520complexity%2520of%2520a%2520given%2520datum%252C%2520and%2520can%2520be%2520used%2520to%2520detect%2520outliers%252C%250Aadversarial%2520examples%2520and%2520AI-generated%2520text.%2520FLIPD%2520achieves%2520state-of-the-art%250Aperformance%2520at%2520LID%2520estimation%252C%2520yet%2520its%2520theoretical%2520underpinnings%2520are%2520incomplete%250Asince%2520Kamkari%2520et%2520al.%2520%25282024b%2529%2520only%2520proved%2520its%2520correctness%2520under%2520the%2520highly%250Aunrealistic%2520assumption%2520of%2520affine%2520submanifolds.%2520In%2520this%2520work%2520we%2520bridge%2520this%2520gap%250Aby%2520formally%2520proving%2520the%2520correctness%2520of%2520FLIPD%2520under%2520realistic%2520assumptions.%250AAdditionally%252C%2520we%2520show%2520that%2520an%2520analogous%2520result%2520holds%2520when%2520Gaussian%2520convolutions%250Aare%2520replaced%2520with%2520uniform%2520ones%252C%2520and%2520discuss%2520the%2520relevance%2520of%2520this%2520result.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.20705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Convolutions%2C%20Intrinsic%20Dimension%2C%20and%20Diffusion%20Models&entry.906535625=Kin%20Kwan%20Leung%20and%20Rasa%20Hosseinzadeh%20and%20Gabriel%20Loaiza-Ganem&entry.1292438233=%20%20The%20manifold%20hypothesis%20asserts%20that%20data%20of%20interest%20in%20high-dimensional%0Aambient%20spaces%2C%20such%20as%20image%20data%2C%20lies%20on%20unknown%20low-dimensional%0Asubmanifolds.%20Diffusion%20models%20%28DMs%29%20--%20which%20operate%20by%20convolving%20data%20with%0Aprogressively%20larger%20amounts%20of%20Gaussian%20noise%20and%20then%20learning%20to%20revert%20this%0Aprocess%20--%20have%20risen%20to%20prominence%20as%20the%20most%20performant%20generative%20models%2C%0Aand%20are%20known%20to%20be%20able%20to%20learn%20distributions%20with%20low-dimensional%20support.%0AFor%20a%20given%20datum%20in%20one%20of%20these%20submanifolds%2C%20we%20should%20thus%20intuitively%0Aexpect%20DMs%20to%20have%20implicitly%20learned%20its%20corresponding%20local%20intrinsic%0Adimension%20%28LID%29%2C%20i.e.%20the%20dimension%20of%20the%20submanifold%20it%20belongs%20to.%20Kamkari%0Aet%20al.%20%282024b%29%20recently%20showed%20that%20this%20is%20indeed%20the%20case%20by%20linking%20this%20LID%0Ato%20the%20rate%20of%20change%20of%20the%20log%20marginal%20densities%20of%20the%20DM%20with%20respect%20to%0Athe%20amount%20of%20added%20noise%2C%20resulting%20in%20an%20LID%20estimator%20known%20as%20FLIPD.%20LID%0Aestimators%20such%20as%20FLIPD%20have%20a%20plethora%20of%20uses%2C%20among%20others%20they%20quantify%0Athe%20complexity%20of%20a%20given%20datum%2C%20and%20can%20be%20used%20to%20detect%20outliers%2C%0Aadversarial%20examples%20and%20AI-generated%20text.%20FLIPD%20achieves%20state-of-the-art%0Aperformance%20at%20LID%20estimation%2C%20yet%20its%20theoretical%20underpinnings%20are%20incomplete%0Asince%20Kamkari%20et%20al.%20%282024b%29%20only%20proved%20its%20correctness%20under%20the%20highly%0Aunrealistic%20assumption%20of%20affine%20submanifolds.%20In%20this%20work%20we%20bridge%20this%20gap%0Aby%20formally%20proving%20the%20correctness%20of%20FLIPD%20under%20realistic%20assumptions.%0AAdditionally%2C%20we%20show%20that%20an%20analogous%20result%20holds%20when%20Gaussian%20convolutions%0Aare%20replaced%20with%20uniform%20ones%2C%20and%20discuss%20the%20relevance%20of%20this%20result.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.20705v2&entry.124074799=Read"},
{"title": "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in\n  Multi-Turn Price Negotiation", "author": "Yunbo Long and Liming Xu and Lukas Beckenbauer and Yuhan Liu and Alexandra Brintrup", "abstract": "  Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.\n", "link": "http://arxiv.org/abs/2509.04310v3", "date": "2025-10-13", "relevancy": 0.9618, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4973}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4851}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoEmo%3A%20Towards%20Evolved%20Emotional%20Policies%20for%20Adversarial%20LLM%20Agents%20in%0A%20%20Multi-Turn%20Price%20Negotiation&body=Title%3A%20EvoEmo%3A%20Towards%20Evolved%20Emotional%20Policies%20for%20Adversarial%20LLM%20Agents%20in%0A%20%20Multi-Turn%20Price%20Negotiation%0AAuthor%3A%20Yunbo%20Long%20and%20Liming%20Xu%20and%20Lukas%20Beckenbauer%20and%20Yuhan%20Liu%20and%20Alexandra%20Brintrup%0AAbstract%3A%20%20%20Recent%20research%20on%20Chain-of-Thought%20%28CoT%29%20reasoning%20in%20Large%20Language%20Models%0A%28LLMs%29%20has%20demonstrated%20that%20agents%20can%20engage%20in%20%5Ctextit%7Bcomplex%7D%2C%0A%5Ctextit%7Bmulti-turn%7D%20negotiations%2C%20opening%20new%20avenues%20for%20agentic%20AI.%20However%2C%0Aexisting%20LLM%20agents%20largely%20overlook%20the%20functional%20role%20of%20emotions%20in%20such%0Anegotiations%2C%20instead%20generating%20passive%2C%20preference-driven%20emotional%20responses%0Athat%20make%20them%20vulnerable%20to%20manipulation%20and%20strategic%20exploitation%20by%0Aadversarial%20counterparts.%20To%20address%20this%20gap%2C%20we%20present%20EvoEmo%2C%20an%0Aevolutionary%20reinforcement%20learning%20framework%20that%20optimizes%20dynamic%20emotional%0Aexpression%20in%20negotiations.%20EvoEmo%20models%20emotional%20state%20transitions%20as%20a%0AMarkov%20Decision%20Process%20and%20employs%20population-based%20genetic%20optimization%20to%0Aevolve%20high-reward%20emotion%20policies%20across%20diverse%20negotiation%20scenarios.%20We%0Afurther%20propose%20an%20evaluation%20framework%20with%20two%20baselines%20--%20vanilla%0Astrategies%20and%20fixed-emotion%20strategies%20--%20for%20benchmarking%20emotion-aware%0Anegotiation.%20Extensive%20experiments%20and%20ablation%20studies%20show%20that%20EvoEmo%0Aconsistently%20outperforms%20both%20baselines%2C%20achieving%20higher%20success%20rates%2C%20higher%0Aefficiency%2C%20and%20increased%20buyer%20savings.%20This%20findings%20highlight%20the%20importance%0Aof%20adaptive%20emotional%20expression%20in%20enabling%20more%20effective%20LLM%20agents%20for%0Amulti-turn%20negotiation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.04310v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoEmo%253A%2520Towards%2520Evolved%2520Emotional%2520Policies%2520for%2520Adversarial%2520LLM%2520Agents%2520in%250A%2520%2520Multi-Turn%2520Price%2520Negotiation%26entry.906535625%3DYunbo%2520Long%2520and%2520Liming%2520Xu%2520and%2520Lukas%2520Beckenbauer%2520and%2520Yuhan%2520Liu%2520and%2520Alexandra%2520Brintrup%26entry.1292438233%3D%2520%2520Recent%2520research%2520on%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520in%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520has%2520demonstrated%2520that%2520agents%2520can%2520engage%2520in%2520%255Ctextit%257Bcomplex%257D%252C%250A%255Ctextit%257Bmulti-turn%257D%2520negotiations%252C%2520opening%2520new%2520avenues%2520for%2520agentic%2520AI.%2520However%252C%250Aexisting%2520LLM%2520agents%2520largely%2520overlook%2520the%2520functional%2520role%2520of%2520emotions%2520in%2520such%250Anegotiations%252C%2520instead%2520generating%2520passive%252C%2520preference-driven%2520emotional%2520responses%250Athat%2520make%2520them%2520vulnerable%2520to%2520manipulation%2520and%2520strategic%2520exploitation%2520by%250Aadversarial%2520counterparts.%2520To%2520address%2520this%2520gap%252C%2520we%2520present%2520EvoEmo%252C%2520an%250Aevolutionary%2520reinforcement%2520learning%2520framework%2520that%2520optimizes%2520dynamic%2520emotional%250Aexpression%2520in%2520negotiations.%2520EvoEmo%2520models%2520emotional%2520state%2520transitions%2520as%2520a%250AMarkov%2520Decision%2520Process%2520and%2520employs%2520population-based%2520genetic%2520optimization%2520to%250Aevolve%2520high-reward%2520emotion%2520policies%2520across%2520diverse%2520negotiation%2520scenarios.%2520We%250Afurther%2520propose%2520an%2520evaluation%2520framework%2520with%2520two%2520baselines%2520--%2520vanilla%250Astrategies%2520and%2520fixed-emotion%2520strategies%2520--%2520for%2520benchmarking%2520emotion-aware%250Anegotiation.%2520Extensive%2520experiments%2520and%2520ablation%2520studies%2520show%2520that%2520EvoEmo%250Aconsistently%2520outperforms%2520both%2520baselines%252C%2520achieving%2520higher%2520success%2520rates%252C%2520higher%250Aefficiency%252C%2520and%2520increased%2520buyer%2520savings.%2520This%2520findings%2520highlight%2520the%2520importance%250Aof%2520adaptive%2520emotional%2520expression%2520in%2520enabling%2520more%2520effective%2520LLM%2520agents%2520for%250Amulti-turn%2520negotiation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04310v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoEmo%3A%20Towards%20Evolved%20Emotional%20Policies%20for%20Adversarial%20LLM%20Agents%20in%0A%20%20Multi-Turn%20Price%20Negotiation&entry.906535625=Yunbo%20Long%20and%20Liming%20Xu%20and%20Lukas%20Beckenbauer%20and%20Yuhan%20Liu%20and%20Alexandra%20Brintrup&entry.1292438233=%20%20Recent%20research%20on%20Chain-of-Thought%20%28CoT%29%20reasoning%20in%20Large%20Language%20Models%0A%28LLMs%29%20has%20demonstrated%20that%20agents%20can%20engage%20in%20%5Ctextit%7Bcomplex%7D%2C%0A%5Ctextit%7Bmulti-turn%7D%20negotiations%2C%20opening%20new%20avenues%20for%20agentic%20AI.%20However%2C%0Aexisting%20LLM%20agents%20largely%20overlook%20the%20functional%20role%20of%20emotions%20in%20such%0Anegotiations%2C%20instead%20generating%20passive%2C%20preference-driven%20emotional%20responses%0Athat%20make%20them%20vulnerable%20to%20manipulation%20and%20strategic%20exploitation%20by%0Aadversarial%20counterparts.%20To%20address%20this%20gap%2C%20we%20present%20EvoEmo%2C%20an%0Aevolutionary%20reinforcement%20learning%20framework%20that%20optimizes%20dynamic%20emotional%0Aexpression%20in%20negotiations.%20EvoEmo%20models%20emotional%20state%20transitions%20as%20a%0AMarkov%20Decision%20Process%20and%20employs%20population-based%20genetic%20optimization%20to%0Aevolve%20high-reward%20emotion%20policies%20across%20diverse%20negotiation%20scenarios.%20We%0Afurther%20propose%20an%20evaluation%20framework%20with%20two%20baselines%20--%20vanilla%0Astrategies%20and%20fixed-emotion%20strategies%20--%20for%20benchmarking%20emotion-aware%0Anegotiation.%20Extensive%20experiments%20and%20ablation%20studies%20show%20that%20EvoEmo%0Aconsistently%20outperforms%20both%20baselines%2C%20achieving%20higher%20success%20rates%2C%20higher%0Aefficiency%2C%20and%20increased%20buyer%20savings.%20This%20findings%20highlight%20the%20importance%0Aof%20adaptive%20emotional%20expression%20in%20enabling%20more%20effective%20LLM%20agents%20for%0Amulti-turn%20negotiation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.04310v3&entry.124074799=Read"},
{"title": "Causal Explanation of Concept Drift -- A Truly Actionable Approach", "author": "David Komnick and Kathrin Lammers and Barbara Hammer and Valerie Vaquet and Fabian Hinder", "abstract": "  In a world that constantly changes, it is crucial to understand how those\nchanges impact different systems, such as industrial manufacturing or critical\ninfrastructure. Explaining critical changes, referred to as concept drift in\nthe field of machine learning, is the first step towards enabling targeted\ninterventions to avoid or correct model failures, as well as malfunctions and\nerrors in the physical world. Therefore, in this work, we extend model-based\ndrift explanations towards causal explanations, which increases the\nactionability of the provided explanations. We evaluate our explanation\nstrategy on a number of use cases, demonstrating the practical usefulness of\nour framework, which isolates the causally relevant features impacted by\nconcept drift and, thus, allows for targeted intervention.\n", "link": "http://arxiv.org/abs/2507.23389v2", "date": "2025-10-13", "relevancy": 0.9064, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4695}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4464}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Explanation%20of%20Concept%20Drift%20--%20A%20Truly%20Actionable%20Approach&body=Title%3A%20Causal%20Explanation%20of%20Concept%20Drift%20--%20A%20Truly%20Actionable%20Approach%0AAuthor%3A%20David%20Komnick%20and%20Kathrin%20Lammers%20and%20Barbara%20Hammer%20and%20Valerie%20Vaquet%20and%20Fabian%20Hinder%0AAbstract%3A%20%20%20In%20a%20world%20that%20constantly%20changes%2C%20it%20is%20crucial%20to%20understand%20how%20those%0Achanges%20impact%20different%20systems%2C%20such%20as%20industrial%20manufacturing%20or%20critical%0Ainfrastructure.%20Explaining%20critical%20changes%2C%20referred%20to%20as%20concept%20drift%20in%0Athe%20field%20of%20machine%20learning%2C%20is%20the%20first%20step%20towards%20enabling%20targeted%0Ainterventions%20to%20avoid%20or%20correct%20model%20failures%2C%20as%20well%20as%20malfunctions%20and%0Aerrors%20in%20the%20physical%20world.%20Therefore%2C%20in%20this%20work%2C%20we%20extend%20model-based%0Adrift%20explanations%20towards%20causal%20explanations%2C%20which%20increases%20the%0Aactionability%20of%20the%20provided%20explanations.%20We%20evaluate%20our%20explanation%0Astrategy%20on%20a%20number%20of%20use%20cases%2C%20demonstrating%20the%20practical%20usefulness%20of%0Aour%20framework%2C%20which%20isolates%20the%20causally%20relevant%20features%20impacted%20by%0Aconcept%20drift%20and%2C%20thus%2C%20allows%20for%20targeted%20intervention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23389v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Explanation%2520of%2520Concept%2520Drift%2520--%2520A%2520Truly%2520Actionable%2520Approach%26entry.906535625%3DDavid%2520Komnick%2520and%2520Kathrin%2520Lammers%2520and%2520Barbara%2520Hammer%2520and%2520Valerie%2520Vaquet%2520and%2520Fabian%2520Hinder%26entry.1292438233%3D%2520%2520In%2520a%2520world%2520that%2520constantly%2520changes%252C%2520it%2520is%2520crucial%2520to%2520understand%2520how%2520those%250Achanges%2520impact%2520different%2520systems%252C%2520such%2520as%2520industrial%2520manufacturing%2520or%2520critical%250Ainfrastructure.%2520Explaining%2520critical%2520changes%252C%2520referred%2520to%2520as%2520concept%2520drift%2520in%250Athe%2520field%2520of%2520machine%2520learning%252C%2520is%2520the%2520first%2520step%2520towards%2520enabling%2520targeted%250Ainterventions%2520to%2520avoid%2520or%2520correct%2520model%2520failures%252C%2520as%2520well%2520as%2520malfunctions%2520and%250Aerrors%2520in%2520the%2520physical%2520world.%2520Therefore%252C%2520in%2520this%2520work%252C%2520we%2520extend%2520model-based%250Adrift%2520explanations%2520towards%2520causal%2520explanations%252C%2520which%2520increases%2520the%250Aactionability%2520of%2520the%2520provided%2520explanations.%2520We%2520evaluate%2520our%2520explanation%250Astrategy%2520on%2520a%2520number%2520of%2520use%2520cases%252C%2520demonstrating%2520the%2520practical%2520usefulness%2520of%250Aour%2520framework%252C%2520which%2520isolates%2520the%2520causally%2520relevant%2520features%2520impacted%2520by%250Aconcept%2520drift%2520and%252C%2520thus%252C%2520allows%2520for%2520targeted%2520intervention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23389v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Explanation%20of%20Concept%20Drift%20--%20A%20Truly%20Actionable%20Approach&entry.906535625=David%20Komnick%20and%20Kathrin%20Lammers%20and%20Barbara%20Hammer%20and%20Valerie%20Vaquet%20and%20Fabian%20Hinder&entry.1292438233=%20%20In%20a%20world%20that%20constantly%20changes%2C%20it%20is%20crucial%20to%20understand%20how%20those%0Achanges%20impact%20different%20systems%2C%20such%20as%20industrial%20manufacturing%20or%20critical%0Ainfrastructure.%20Explaining%20critical%20changes%2C%20referred%20to%20as%20concept%20drift%20in%0Athe%20field%20of%20machine%20learning%2C%20is%20the%20first%20step%20towards%20enabling%20targeted%0Ainterventions%20to%20avoid%20or%20correct%20model%20failures%2C%20as%20well%20as%20malfunctions%20and%0Aerrors%20in%20the%20physical%20world.%20Therefore%2C%20in%20this%20work%2C%20we%20extend%20model-based%0Adrift%20explanations%20towards%20causal%20explanations%2C%20which%20increases%20the%0Aactionability%20of%20the%20provided%20explanations.%20We%20evaluate%20our%20explanation%0Astrategy%20on%20a%20number%20of%20use%20cases%2C%20demonstrating%20the%20practical%20usefulness%20of%0Aour%20framework%2C%20which%20isolates%20the%20causally%20relevant%20features%20impacted%20by%0Aconcept%20drift%20and%2C%20thus%2C%20allows%20for%20targeted%20intervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23389v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


