<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240704.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Gaussian Eigen Models for Human Heads", "author": "Wojciech Zielonka and Timo Bolkart and Thabo Beeler and Justus Thies", "abstract": "  We present personalized Gaussian Eigen Models (GEMs) for human heads, a novel\nmethod that compresses dynamic 3D Gaussians into low-dimensional linear spaces.\nOur approach is inspired by the seminal work of Blanz and Vetter, where a\nmesh-based 3D morphable model (3DMM) is constructed from registered meshes.\nBased on dynamic 3D Gaussians, we create a lower-dimensional representation of\nprimitives that applies to most 3DGS head avatars. Specifically, we propose a\nuniversal method to distill the appearance of a mesh-controlled UNet Gaussian\navatar using an ensemble of linear eigenbasis. We replace heavy CNN-based\narchitectures with a single linear layer improving speed and enabling a range\nof real-time downstream applications. To create a particular facial expression,\none simply needs to perform a dot product between the eigen coefficients and\nthe distilled basis. This efficient method removes the requirement for an input\nmesh during testing, enhancing simplicity and speed in expression generation.\nThis process is highly efficient and supports real-time rendering on everyday\ndevices, leveraging the effectiveness of standard Gaussian Splatting. In\naddition, we demonstrate how the GEM can be controlled using a ResNet-based\nregression architecture. We show and compare self-reenactment and cross-person\nreenactment to state-of-the-art 3D avatar methods, demonstrating higher quality\nand better control. A real-time demo showcases the applicability of the GEM\nrepresentation.\n", "link": "http://arxiv.org/abs/2407.04545v1", "date": "2024-07-05", "relevancy": 3.6729, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7573}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7573}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Eigen%20Models%20for%20Human%20Heads&body=Title%3A%20Gaussian%20Eigen%20Models%20for%20Human%20Heads%0AAuthor%3A%20Wojciech%20Zielonka%20and%20Timo%20Bolkart%20and%20Thabo%20Beeler%20and%20Justus%20Thies%0AAbstract%3A%20%20%20We%20present%20personalized%20Gaussian%20Eigen%20Models%20%28GEMs%29%20for%20human%20heads%2C%20a%20novel%0Amethod%20that%20compresses%20dynamic%203D%20Gaussians%20into%20low-dimensional%20linear%20spaces.%0AOur%20approach%20is%20inspired%20by%20the%20seminal%20work%20of%20Blanz%20and%20Vetter%2C%20where%20a%0Amesh-based%203D%20morphable%20model%20%283DMM%29%20is%20constructed%20from%20registered%20meshes.%0ABased%20on%20dynamic%203D%20Gaussians%2C%20we%20create%20a%20lower-dimensional%20representation%20of%0Aprimitives%20that%20applies%20to%20most%203DGS%20head%20avatars.%20Specifically%2C%20we%20propose%20a%0Auniversal%20method%20to%20distill%20the%20appearance%20of%20a%20mesh-controlled%20UNet%20Gaussian%0Aavatar%20using%20an%20ensemble%20of%20linear%20eigenbasis.%20We%20replace%20heavy%20CNN-based%0Aarchitectures%20with%20a%20single%20linear%20layer%20improving%20speed%20and%20enabling%20a%20range%0Aof%20real-time%20downstream%20applications.%20To%20create%20a%20particular%20facial%20expression%2C%0Aone%20simply%20needs%20to%20perform%20a%20dot%20product%20between%20the%20eigen%20coefficients%20and%0Athe%20distilled%20basis.%20This%20efficient%20method%20removes%20the%20requirement%20for%20an%20input%0Amesh%20during%20testing%2C%20enhancing%20simplicity%20and%20speed%20in%20expression%20generation.%0AThis%20process%20is%20highly%20efficient%20and%20supports%20real-time%20rendering%20on%20everyday%0Adevices%2C%20leveraging%20the%20effectiveness%20of%20standard%20Gaussian%20Splatting.%20In%0Aaddition%2C%20we%20demonstrate%20how%20the%20GEM%20can%20be%20controlled%20using%20a%20ResNet-based%0Aregression%20architecture.%20We%20show%20and%20compare%20self-reenactment%20and%20cross-person%0Areenactment%20to%20state-of-the-art%203D%20avatar%20methods%2C%20demonstrating%20higher%20quality%0Aand%20better%20control.%20A%20real-time%20demo%20showcases%20the%20applicability%20of%20the%20GEM%0Arepresentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Eigen%2520Models%2520for%2520Human%2520Heads%26entry.906535625%3DWojciech%2520Zielonka%2520and%2520Timo%2520Bolkart%2520and%2520Thabo%2520Beeler%2520and%2520Justus%2520Thies%26entry.1292438233%3D%2520%2520We%2520present%2520personalized%2520Gaussian%2520Eigen%2520Models%2520%2528GEMs%2529%2520for%2520human%2520heads%252C%2520a%2520novel%250Amethod%2520that%2520compresses%2520dynamic%25203D%2520Gaussians%2520into%2520low-dimensional%2520linear%2520spaces.%250AOur%2520approach%2520is%2520inspired%2520by%2520the%2520seminal%2520work%2520of%2520Blanz%2520and%2520Vetter%252C%2520where%2520a%250Amesh-based%25203D%2520morphable%2520model%2520%25283DMM%2529%2520is%2520constructed%2520from%2520registered%2520meshes.%250ABased%2520on%2520dynamic%25203D%2520Gaussians%252C%2520we%2520create%2520a%2520lower-dimensional%2520representation%2520of%250Aprimitives%2520that%2520applies%2520to%2520most%25203DGS%2520head%2520avatars.%2520Specifically%252C%2520we%2520propose%2520a%250Auniversal%2520method%2520to%2520distill%2520the%2520appearance%2520of%2520a%2520mesh-controlled%2520UNet%2520Gaussian%250Aavatar%2520using%2520an%2520ensemble%2520of%2520linear%2520eigenbasis.%2520We%2520replace%2520heavy%2520CNN-based%250Aarchitectures%2520with%2520a%2520single%2520linear%2520layer%2520improving%2520speed%2520and%2520enabling%2520a%2520range%250Aof%2520real-time%2520downstream%2520applications.%2520To%2520create%2520a%2520particular%2520facial%2520expression%252C%250Aone%2520simply%2520needs%2520to%2520perform%2520a%2520dot%2520product%2520between%2520the%2520eigen%2520coefficients%2520and%250Athe%2520distilled%2520basis.%2520This%2520efficient%2520method%2520removes%2520the%2520requirement%2520for%2520an%2520input%250Amesh%2520during%2520testing%252C%2520enhancing%2520simplicity%2520and%2520speed%2520in%2520expression%2520generation.%250AThis%2520process%2520is%2520highly%2520efficient%2520and%2520supports%2520real-time%2520rendering%2520on%2520everyday%250Adevices%252C%2520leveraging%2520the%2520effectiveness%2520of%2520standard%2520Gaussian%2520Splatting.%2520In%250Aaddition%252C%2520we%2520demonstrate%2520how%2520the%2520GEM%2520can%2520be%2520controlled%2520using%2520a%2520ResNet-based%250Aregression%2520architecture.%2520We%2520show%2520and%2520compare%2520self-reenactment%2520and%2520cross-person%250Areenactment%2520to%2520state-of-the-art%25203D%2520avatar%2520methods%252C%2520demonstrating%2520higher%2520quality%250Aand%2520better%2520control.%2520A%2520real-time%2520demo%2520showcases%2520the%2520applicability%2520of%2520the%2520GEM%250Arepresentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Eigen%20Models%20for%20Human%20Heads&entry.906535625=Wojciech%20Zielonka%20and%20Timo%20Bolkart%20and%20Thabo%20Beeler%20and%20Justus%20Thies&entry.1292438233=%20%20We%20present%20personalized%20Gaussian%20Eigen%20Models%20%28GEMs%29%20for%20human%20heads%2C%20a%20novel%0Amethod%20that%20compresses%20dynamic%203D%20Gaussians%20into%20low-dimensional%20linear%20spaces.%0AOur%20approach%20is%20inspired%20by%20the%20seminal%20work%20of%20Blanz%20and%20Vetter%2C%20where%20a%0Amesh-based%203D%20morphable%20model%20%283DMM%29%20is%20constructed%20from%20registered%20meshes.%0ABased%20on%20dynamic%203D%20Gaussians%2C%20we%20create%20a%20lower-dimensional%20representation%20of%0Aprimitives%20that%20applies%20to%20most%203DGS%20head%20avatars.%20Specifically%2C%20we%20propose%20a%0Auniversal%20method%20to%20distill%20the%20appearance%20of%20a%20mesh-controlled%20UNet%20Gaussian%0Aavatar%20using%20an%20ensemble%20of%20linear%20eigenbasis.%20We%20replace%20heavy%20CNN-based%0Aarchitectures%20with%20a%20single%20linear%20layer%20improving%20speed%20and%20enabling%20a%20range%0Aof%20real-time%20downstream%20applications.%20To%20create%20a%20particular%20facial%20expression%2C%0Aone%20simply%20needs%20to%20perform%20a%20dot%20product%20between%20the%20eigen%20coefficients%20and%0Athe%20distilled%20basis.%20This%20efficient%20method%20removes%20the%20requirement%20for%20an%20input%0Amesh%20during%20testing%2C%20enhancing%20simplicity%20and%20speed%20in%20expression%20generation.%0AThis%20process%20is%20highly%20efficient%20and%20supports%20real-time%20rendering%20on%20everyday%0Adevices%2C%20leveraging%20the%20effectiveness%20of%20standard%20Gaussian%20Splatting.%20In%0Aaddition%2C%20we%20demonstrate%20how%20the%20GEM%20can%20be%20controlled%20using%20a%20ResNet-based%0Aregression%20architecture.%20We%20show%20and%20compare%20self-reenactment%20and%20cross-person%0Areenactment%20to%20state-of-the-art%203D%20avatar%20methods%2C%20demonstrating%20higher%20quality%0Aand%20better%20control.%20A%20real-time%20demo%20showcases%20the%20applicability%20of%20the%20GEM%0Arepresentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04545v1&entry.124074799=Read"},
{"title": "Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis", "author": "Basile Van Hoorick and Rundi Wu and Ege Ozguroglu and Kyle Sargent and Ruoshi Liu and Pavel Tokmakov and Achal Dave and Changxi Zheng and Carl Vondrick", "abstract": "  Accurate reconstruction of complex dynamic scenes from just a single\nviewpoint continues to be a challenging task in computer vision. Current\ndynamic novel view synthesis methods typically require videos from many\ndifferent camera viewpoints, necessitating careful recording setups, and\nsignificantly restricting their utility in the wild as well as in terms of\nembodied AI applications. In this paper, we propose $\\textbf{GCD}$, a\ncontrollable monocular dynamic view synthesis pipeline that leverages\nlarge-scale diffusion priors to, given a video of any scene, generate a\nsynchronous video from any other chosen perspective, conditioned on a set of\nrelative camera pose parameters. Our model does not require depth as input, and\ndoes not explicitly model 3D scene geometry, instead performing end-to-end\nvideo-to-video translation in order to achieve its goal efficiently. Despite\nbeing trained on synthetic multi-view video data only, zero-shot real-world\ngeneralization experiments show promising results in multiple domains,\nincluding robotics, object permanence, and driving environments. We believe our\nframework can potentially unlock powerful applications in rich dynamic scene\nunderstanding, perception for robotics, and interactive 3D video viewing\nexperiences for virtual reality.\n", "link": "http://arxiv.org/abs/2405.14868v2", "date": "2024-07-05", "relevancy": 3.3041, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6702}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6702}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Camera%20Dolly%3A%20Extreme%20Monocular%20Dynamic%20Novel%20View%20Synthesis&body=Title%3A%20Generative%20Camera%20Dolly%3A%20Extreme%20Monocular%20Dynamic%20Novel%20View%20Synthesis%0AAuthor%3A%20Basile%20Van%20Hoorick%20and%20Rundi%20Wu%20and%20Ege%20Ozguroglu%20and%20Kyle%20Sargent%20and%20Ruoshi%20Liu%20and%20Pavel%20Tokmakov%20and%20Achal%20Dave%20and%20Changxi%20Zheng%20and%20Carl%20Vondrick%0AAbstract%3A%20%20%20Accurate%20reconstruction%20of%20complex%20dynamic%20scenes%20from%20just%20a%20single%0Aviewpoint%20continues%20to%20be%20a%20challenging%20task%20in%20computer%20vision.%20Current%0Adynamic%20novel%20view%20synthesis%20methods%20typically%20require%20videos%20from%20many%0Adifferent%20camera%20viewpoints%2C%20necessitating%20careful%20recording%20setups%2C%20and%0Asignificantly%20restricting%20their%20utility%20in%20the%20wild%20as%20well%20as%20in%20terms%20of%0Aembodied%20AI%20applications.%20In%20this%20paper%2C%20we%20propose%20%24%5Ctextbf%7BGCD%7D%24%2C%20a%0Acontrollable%20monocular%20dynamic%20view%20synthesis%20pipeline%20that%20leverages%0Alarge-scale%20diffusion%20priors%20to%2C%20given%20a%20video%20of%20any%20scene%2C%20generate%20a%0Asynchronous%20video%20from%20any%20other%20chosen%20perspective%2C%20conditioned%20on%20a%20set%20of%0Arelative%20camera%20pose%20parameters.%20Our%20model%20does%20not%20require%20depth%20as%20input%2C%20and%0Adoes%20not%20explicitly%20model%203D%20scene%20geometry%2C%20instead%20performing%20end-to-end%0Avideo-to-video%20translation%20in%20order%20to%20achieve%20its%20goal%20efficiently.%20Despite%0Abeing%20trained%20on%20synthetic%20multi-view%20video%20data%20only%2C%20zero-shot%20real-world%0Ageneralization%20experiments%20show%20promising%20results%20in%20multiple%20domains%2C%0Aincluding%20robotics%2C%20object%20permanence%2C%20and%20driving%20environments.%20We%20believe%20our%0Aframework%20can%20potentially%20unlock%20powerful%20applications%20in%20rich%20dynamic%20scene%0Aunderstanding%2C%20perception%20for%20robotics%2C%20and%20interactive%203D%20video%20viewing%0Aexperiences%20for%20virtual%20reality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Camera%2520Dolly%253A%2520Extreme%2520Monocular%2520Dynamic%2520Novel%2520View%2520Synthesis%26entry.906535625%3DBasile%2520Van%2520Hoorick%2520and%2520Rundi%2520Wu%2520and%2520Ege%2520Ozguroglu%2520and%2520Kyle%2520Sargent%2520and%2520Ruoshi%2520Liu%2520and%2520Pavel%2520Tokmakov%2520and%2520Achal%2520Dave%2520and%2520Changxi%2520Zheng%2520and%2520Carl%2520Vondrick%26entry.1292438233%3D%2520%2520Accurate%2520reconstruction%2520of%2520complex%2520dynamic%2520scenes%2520from%2520just%2520a%2520single%250Aviewpoint%2520continues%2520to%2520be%2520a%2520challenging%2520task%2520in%2520computer%2520vision.%2520Current%250Adynamic%2520novel%2520view%2520synthesis%2520methods%2520typically%2520require%2520videos%2520from%2520many%250Adifferent%2520camera%2520viewpoints%252C%2520necessitating%2520careful%2520recording%2520setups%252C%2520and%250Asignificantly%2520restricting%2520their%2520utility%2520in%2520the%2520wild%2520as%2520well%2520as%2520in%2520terms%2520of%250Aembodied%2520AI%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%2524%255Ctextbf%257BGCD%257D%2524%252C%2520a%250Acontrollable%2520monocular%2520dynamic%2520view%2520synthesis%2520pipeline%2520that%2520leverages%250Alarge-scale%2520diffusion%2520priors%2520to%252C%2520given%2520a%2520video%2520of%2520any%2520scene%252C%2520generate%2520a%250Asynchronous%2520video%2520from%2520any%2520other%2520chosen%2520perspective%252C%2520conditioned%2520on%2520a%2520set%2520of%250Arelative%2520camera%2520pose%2520parameters.%2520Our%2520model%2520does%2520not%2520require%2520depth%2520as%2520input%252C%2520and%250Adoes%2520not%2520explicitly%2520model%25203D%2520scene%2520geometry%252C%2520instead%2520performing%2520end-to-end%250Avideo-to-video%2520translation%2520in%2520order%2520to%2520achieve%2520its%2520goal%2520efficiently.%2520Despite%250Abeing%2520trained%2520on%2520synthetic%2520multi-view%2520video%2520data%2520only%252C%2520zero-shot%2520real-world%250Ageneralization%2520experiments%2520show%2520promising%2520results%2520in%2520multiple%2520domains%252C%250Aincluding%2520robotics%252C%2520object%2520permanence%252C%2520and%2520driving%2520environments.%2520We%2520believe%2520our%250Aframework%2520can%2520potentially%2520unlock%2520powerful%2520applications%2520in%2520rich%2520dynamic%2520scene%250Aunderstanding%252C%2520perception%2520for%2520robotics%252C%2520and%2520interactive%25203D%2520video%2520viewing%250Aexperiences%2520for%2520virtual%2520reality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Camera%20Dolly%3A%20Extreme%20Monocular%20Dynamic%20Novel%20View%20Synthesis&entry.906535625=Basile%20Van%20Hoorick%20and%20Rundi%20Wu%20and%20Ege%20Ozguroglu%20and%20Kyle%20Sargent%20and%20Ruoshi%20Liu%20and%20Pavel%20Tokmakov%20and%20Achal%20Dave%20and%20Changxi%20Zheng%20and%20Carl%20Vondrick&entry.1292438233=%20%20Accurate%20reconstruction%20of%20complex%20dynamic%20scenes%20from%20just%20a%20single%0Aviewpoint%20continues%20to%20be%20a%20challenging%20task%20in%20computer%20vision.%20Current%0Adynamic%20novel%20view%20synthesis%20methods%20typically%20require%20videos%20from%20many%0Adifferent%20camera%20viewpoints%2C%20necessitating%20careful%20recording%20setups%2C%20and%0Asignificantly%20restricting%20their%20utility%20in%20the%20wild%20as%20well%20as%20in%20terms%20of%0Aembodied%20AI%20applications.%20In%20this%20paper%2C%20we%20propose%20%24%5Ctextbf%7BGCD%7D%24%2C%20a%0Acontrollable%20monocular%20dynamic%20view%20synthesis%20pipeline%20that%20leverages%0Alarge-scale%20diffusion%20priors%20to%2C%20given%20a%20video%20of%20any%20scene%2C%20generate%20a%0Asynchronous%20video%20from%20any%20other%20chosen%20perspective%2C%20conditioned%20on%20a%20set%20of%0Arelative%20camera%20pose%20parameters.%20Our%20model%20does%20not%20require%20depth%20as%20input%2C%20and%0Adoes%20not%20explicitly%20model%203D%20scene%20geometry%2C%20instead%20performing%20end-to-end%0Avideo-to-video%20translation%20in%20order%20to%20achieve%20its%20goal%20efficiently.%20Despite%0Abeing%20trained%20on%20synthetic%20multi-view%20video%20data%20only%2C%20zero-shot%20real-world%0Ageneralization%20experiments%20show%20promising%20results%20in%20multiple%20domains%2C%0Aincluding%20robotics%2C%20object%20permanence%2C%20and%20driving%20environments.%20We%20believe%20our%0Aframework%20can%20potentially%20unlock%20powerful%20applications%20in%20rich%20dynamic%20scene%0Aunderstanding%2C%20perception%20for%20robotics%2C%20and%20interactive%203D%20video%20viewing%0Aexperiences%20for%20virtual%20reality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14868v2&entry.124074799=Read"},
{"title": "Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion", "author": "Ziyuan Qu and Omkar Vengurlekar and Mohamad Qadri and Kevin Zhang and Michael Kaess and Christopher Metzler and Suren Jayasuriya and Adithya Pediredla", "abstract": "  Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent\ntechnique in computer vision and graphics for reconstructing 3D scenes. GS\nrepresents a scene as a set of 3D Gaussians with varying opacities and employs\na computationally efficient splatting operation along with analytical\nderivatives to compute the 3D Gaussian parameters given scene images captured\nfrom various viewpoints. Unfortunately, capturing surround view ($360^{\\circ}$\nviewpoint) images is impossible or impractical in many real-world imaging\nscenarios, including underwater imaging, rooms inside a building, and\nautonomous navigation. In these restricted baseline imaging scenarios, the GS\nalgorithm suffers from a well-known 'missing cone' problem, which results in\npoor reconstruction along the depth axis. In this manuscript, we demonstrate\nthat using transient data (from sonars) allows us to address the missing cone\nproblem by sampling high-frequency data along the depth axis. We extend the\nGaussian splatting algorithms for two commonly used sonars and propose fusion\nalgorithms that simultaneously utilize RGB camera data and sonar data. Through\nsimulations, emulations, and hardware experiments across various imaging\nscenarios, we show that the proposed fusion algorithms lead to significantly\nbetter novel view synthesis (5 dB improvement in PSNR) and 3D geometry\nreconstruction (60% lower Chamfer distance).\n", "link": "http://arxiv.org/abs/2404.04687v2", "date": "2024-07-05", "relevancy": 3.1572, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7351}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6129}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Z-Splat%3A%20Z-Axis%20Gaussian%20Splatting%20for%20Camera-Sonar%20Fusion&body=Title%3A%20Z-Splat%3A%20Z-Axis%20Gaussian%20Splatting%20for%20Camera-Sonar%20Fusion%0AAuthor%3A%20Ziyuan%20Qu%20and%20Omkar%20Vengurlekar%20and%20Mohamad%20Qadri%20and%20Kevin%20Zhang%20and%20Michael%20Kaess%20and%20Christopher%20Metzler%20and%20Suren%20Jayasuriya%20and%20Adithya%20Pediredla%0AAbstract%3A%20%20%20Differentiable%203D-Gaussian%20splatting%20%28GS%29%20is%20emerging%20as%20a%20prominent%0Atechnique%20in%20computer%20vision%20and%20graphics%20for%20reconstructing%203D%20scenes.%20GS%0Arepresents%20a%20scene%20as%20a%20set%20of%203D%20Gaussians%20with%20varying%20opacities%20and%20employs%0Aa%20computationally%20efficient%20splatting%20operation%20along%20with%20analytical%0Aderivatives%20to%20compute%20the%203D%20Gaussian%20parameters%20given%20scene%20images%20captured%0Afrom%20various%20viewpoints.%20Unfortunately%2C%20capturing%20surround%20view%20%28%24360%5E%7B%5Ccirc%7D%24%0Aviewpoint%29%20images%20is%20impossible%20or%20impractical%20in%20many%20real-world%20imaging%0Ascenarios%2C%20including%20underwater%20imaging%2C%20rooms%20inside%20a%20building%2C%20and%0Aautonomous%20navigation.%20In%20these%20restricted%20baseline%20imaging%20scenarios%2C%20the%20GS%0Aalgorithm%20suffers%20from%20a%20well-known%20%27missing%20cone%27%20problem%2C%20which%20results%20in%0Apoor%20reconstruction%20along%20the%20depth%20axis.%20In%20this%20manuscript%2C%20we%20demonstrate%0Athat%20using%20transient%20data%20%28from%20sonars%29%20allows%20us%20to%20address%20the%20missing%20cone%0Aproblem%20by%20sampling%20high-frequency%20data%20along%20the%20depth%20axis.%20We%20extend%20the%0AGaussian%20splatting%20algorithms%20for%20two%20commonly%20used%20sonars%20and%20propose%20fusion%0Aalgorithms%20that%20simultaneously%20utilize%20RGB%20camera%20data%20and%20sonar%20data.%20Through%0Asimulations%2C%20emulations%2C%20and%20hardware%20experiments%20across%20various%20imaging%0Ascenarios%2C%20we%20show%20that%20the%20proposed%20fusion%20algorithms%20lead%20to%20significantly%0Abetter%20novel%20view%20synthesis%20%285%20dB%20improvement%20in%20PSNR%29%20and%203D%20geometry%0Areconstruction%20%2860%25%20lower%20Chamfer%20distance%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04687v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZ-Splat%253A%2520Z-Axis%2520Gaussian%2520Splatting%2520for%2520Camera-Sonar%2520Fusion%26entry.906535625%3DZiyuan%2520Qu%2520and%2520Omkar%2520Vengurlekar%2520and%2520Mohamad%2520Qadri%2520and%2520Kevin%2520Zhang%2520and%2520Michael%2520Kaess%2520and%2520Christopher%2520Metzler%2520and%2520Suren%2520Jayasuriya%2520and%2520Adithya%2520Pediredla%26entry.1292438233%3D%2520%2520Differentiable%25203D-Gaussian%2520splatting%2520%2528GS%2529%2520is%2520emerging%2520as%2520a%2520prominent%250Atechnique%2520in%2520computer%2520vision%2520and%2520graphics%2520for%2520reconstructing%25203D%2520scenes.%2520GS%250Arepresents%2520a%2520scene%2520as%2520a%2520set%2520of%25203D%2520Gaussians%2520with%2520varying%2520opacities%2520and%2520employs%250Aa%2520computationally%2520efficient%2520splatting%2520operation%2520along%2520with%2520analytical%250Aderivatives%2520to%2520compute%2520the%25203D%2520Gaussian%2520parameters%2520given%2520scene%2520images%2520captured%250Afrom%2520various%2520viewpoints.%2520Unfortunately%252C%2520capturing%2520surround%2520view%2520%2528%2524360%255E%257B%255Ccirc%257D%2524%250Aviewpoint%2529%2520images%2520is%2520impossible%2520or%2520impractical%2520in%2520many%2520real-world%2520imaging%250Ascenarios%252C%2520including%2520underwater%2520imaging%252C%2520rooms%2520inside%2520a%2520building%252C%2520and%250Aautonomous%2520navigation.%2520In%2520these%2520restricted%2520baseline%2520imaging%2520scenarios%252C%2520the%2520GS%250Aalgorithm%2520suffers%2520from%2520a%2520well-known%2520%2527missing%2520cone%2527%2520problem%252C%2520which%2520results%2520in%250Apoor%2520reconstruction%2520along%2520the%2520depth%2520axis.%2520In%2520this%2520manuscript%252C%2520we%2520demonstrate%250Athat%2520using%2520transient%2520data%2520%2528from%2520sonars%2529%2520allows%2520us%2520to%2520address%2520the%2520missing%2520cone%250Aproblem%2520by%2520sampling%2520high-frequency%2520data%2520along%2520the%2520depth%2520axis.%2520We%2520extend%2520the%250AGaussian%2520splatting%2520algorithms%2520for%2520two%2520commonly%2520used%2520sonars%2520and%2520propose%2520fusion%250Aalgorithms%2520that%2520simultaneously%2520utilize%2520RGB%2520camera%2520data%2520and%2520sonar%2520data.%2520Through%250Asimulations%252C%2520emulations%252C%2520and%2520hardware%2520experiments%2520across%2520various%2520imaging%250Ascenarios%252C%2520we%2520show%2520that%2520the%2520proposed%2520fusion%2520algorithms%2520lead%2520to%2520significantly%250Abetter%2520novel%2520view%2520synthesis%2520%25285%2520dB%2520improvement%2520in%2520PSNR%2529%2520and%25203D%2520geometry%250Areconstruction%2520%252860%2525%2520lower%2520Chamfer%2520distance%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04687v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Z-Splat%3A%20Z-Axis%20Gaussian%20Splatting%20for%20Camera-Sonar%20Fusion&entry.906535625=Ziyuan%20Qu%20and%20Omkar%20Vengurlekar%20and%20Mohamad%20Qadri%20and%20Kevin%20Zhang%20and%20Michael%20Kaess%20and%20Christopher%20Metzler%20and%20Suren%20Jayasuriya%20and%20Adithya%20Pediredla&entry.1292438233=%20%20Differentiable%203D-Gaussian%20splatting%20%28GS%29%20is%20emerging%20as%20a%20prominent%0Atechnique%20in%20computer%20vision%20and%20graphics%20for%20reconstructing%203D%20scenes.%20GS%0Arepresents%20a%20scene%20as%20a%20set%20of%203D%20Gaussians%20with%20varying%20opacities%20and%20employs%0Aa%20computationally%20efficient%20splatting%20operation%20along%20with%20analytical%0Aderivatives%20to%20compute%20the%203D%20Gaussian%20parameters%20given%20scene%20images%20captured%0Afrom%20various%20viewpoints.%20Unfortunately%2C%20capturing%20surround%20view%20%28%24360%5E%7B%5Ccirc%7D%24%0Aviewpoint%29%20images%20is%20impossible%20or%20impractical%20in%20many%20real-world%20imaging%0Ascenarios%2C%20including%20underwater%20imaging%2C%20rooms%20inside%20a%20building%2C%20and%0Aautonomous%20navigation.%20In%20these%20restricted%20baseline%20imaging%20scenarios%2C%20the%20GS%0Aalgorithm%20suffers%20from%20a%20well-known%20%27missing%20cone%27%20problem%2C%20which%20results%20in%0Apoor%20reconstruction%20along%20the%20depth%20axis.%20In%20this%20manuscript%2C%20we%20demonstrate%0Athat%20using%20transient%20data%20%28from%20sonars%29%20allows%20us%20to%20address%20the%20missing%20cone%0Aproblem%20by%20sampling%20high-frequency%20data%20along%20the%20depth%20axis.%20We%20extend%20the%0AGaussian%20splatting%20algorithms%20for%20two%20commonly%20used%20sonars%20and%20propose%20fusion%0Aalgorithms%20that%20simultaneously%20utilize%20RGB%20camera%20data%20and%20sonar%20data.%20Through%0Asimulations%2C%20emulations%2C%20and%20hardware%20experiments%20across%20various%20imaging%0Ascenarios%2C%20we%20show%20that%20the%20proposed%20fusion%20algorithms%20lead%20to%20significantly%0Abetter%20novel%20view%20synthesis%20%285%20dB%20improvement%20in%20PSNR%29%20and%203D%20geometry%0Areconstruction%20%2860%25%20lower%20Chamfer%20distance%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04687v2&entry.124074799=Read"},
{"title": "Segment Any 4D Gaussians", "author": "Shengxiang Ji and Guanjun Wu and Jiemin Fang and Jiazhong Cen and Taoran Yi and Wenyu Liu and Qi Tian and Xinggang Wang", "abstract": "  Modeling, understanding, and reconstructing the real world are crucial in\nXR/VR. Recently, 3D Gaussian Splatting (3D-GS) methods have shown remarkable\nsuccess in modeling and understanding 3D scenes. Similarly, various 4D\nrepresentations have demonstrated the ability to capture the dynamics of the 4D\nworld. However, there is a dearth of research focusing on segmentation within\n4D representations. In this paper, we propose Segment Any 4D Gaussians (SA4D),\none of the first frameworks to segment anything in the 4D digital world based\non 4D Gaussians. In SA4D, an efficient temporal identity feature field is\nintroduced to handle Gaussian drifting, with the potential to learn precise\nidentity features from noisy and sparse input. Additionally, a 4D segmentation\nrefinement process is proposed to remove artifacts. Our SA4D achieves precise,\nhigh-quality segmentation within seconds in 4D Gaussians and shows the ability\nto remove, recolor, compose, and render high-quality anything masks. More demos\nare available at: https://jsxzs.github.io/sa4d/.\n", "link": "http://arxiv.org/abs/2407.04504v1", "date": "2024-07-05", "relevancy": 3.1058, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.665}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6215}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Any%204D%20Gaussians&body=Title%3A%20Segment%20Any%204D%20Gaussians%0AAuthor%3A%20Shengxiang%20Ji%20and%20Guanjun%20Wu%20and%20Jiemin%20Fang%20and%20Jiazhong%20Cen%20and%20Taoran%20Yi%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Modeling%2C%20understanding%2C%20and%20reconstructing%20the%20real%20world%20are%20crucial%20in%0AXR/VR.%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20methods%20have%20shown%20remarkable%0Asuccess%20in%20modeling%20and%20understanding%203D%20scenes.%20Similarly%2C%20various%204D%0Arepresentations%20have%20demonstrated%20the%20ability%20to%20capture%20the%20dynamics%20of%20the%204D%0Aworld.%20However%2C%20there%20is%20a%20dearth%20of%20research%20focusing%20on%20segmentation%20within%0A4D%20representations.%20In%20this%20paper%2C%20we%20propose%20Segment%20Any%204D%20Gaussians%20%28SA4D%29%2C%0Aone%20of%20the%20first%20frameworks%20to%20segment%20anything%20in%20the%204D%20digital%20world%20based%0Aon%204D%20Gaussians.%20In%20SA4D%2C%20an%20efficient%20temporal%20identity%20feature%20field%20is%0Aintroduced%20to%20handle%20Gaussian%20drifting%2C%20with%20the%20potential%20to%20learn%20precise%0Aidentity%20features%20from%20noisy%20and%20sparse%20input.%20Additionally%2C%20a%204D%20segmentation%0Arefinement%20process%20is%20proposed%20to%20remove%20artifacts.%20Our%20SA4D%20achieves%20precise%2C%0Ahigh-quality%20segmentation%20within%20seconds%20in%204D%20Gaussians%20and%20shows%20the%20ability%0Ato%20remove%2C%20recolor%2C%20compose%2C%20and%20render%20high-quality%20anything%20masks.%20More%20demos%0Aare%20available%20at%3A%20https%3A//jsxzs.github.io/sa4d/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Any%25204D%2520Gaussians%26entry.906535625%3DShengxiang%2520Ji%2520and%2520Guanjun%2520Wu%2520and%2520Jiemin%2520Fang%2520and%2520Jiazhong%2520Cen%2520and%2520Taoran%2520Yi%2520and%2520Wenyu%2520Liu%2520and%2520Qi%2520Tian%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Modeling%252C%2520understanding%252C%2520and%2520reconstructing%2520the%2520real%2520world%2520are%2520crucial%2520in%250AXR/VR.%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529%2520methods%2520have%2520shown%2520remarkable%250Asuccess%2520in%2520modeling%2520and%2520understanding%25203D%2520scenes.%2520Similarly%252C%2520various%25204D%250Arepresentations%2520have%2520demonstrated%2520the%2520ability%2520to%2520capture%2520the%2520dynamics%2520of%2520the%25204D%250Aworld.%2520However%252C%2520there%2520is%2520a%2520dearth%2520of%2520research%2520focusing%2520on%2520segmentation%2520within%250A4D%2520representations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Segment%2520Any%25204D%2520Gaussians%2520%2528SA4D%2529%252C%250Aone%2520of%2520the%2520first%2520frameworks%2520to%2520segment%2520anything%2520in%2520the%25204D%2520digital%2520world%2520based%250Aon%25204D%2520Gaussians.%2520In%2520SA4D%252C%2520an%2520efficient%2520temporal%2520identity%2520feature%2520field%2520is%250Aintroduced%2520to%2520handle%2520Gaussian%2520drifting%252C%2520with%2520the%2520potential%2520to%2520learn%2520precise%250Aidentity%2520features%2520from%2520noisy%2520and%2520sparse%2520input.%2520Additionally%252C%2520a%25204D%2520segmentation%250Arefinement%2520process%2520is%2520proposed%2520to%2520remove%2520artifacts.%2520Our%2520SA4D%2520achieves%2520precise%252C%250Ahigh-quality%2520segmentation%2520within%2520seconds%2520in%25204D%2520Gaussians%2520and%2520shows%2520the%2520ability%250Ato%2520remove%252C%2520recolor%252C%2520compose%252C%2520and%2520render%2520high-quality%2520anything%2520masks.%2520More%2520demos%250Aare%2520available%2520at%253A%2520https%253A//jsxzs.github.io/sa4d/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Any%204D%20Gaussians&entry.906535625=Shengxiang%20Ji%20and%20Guanjun%20Wu%20and%20Jiemin%20Fang%20and%20Jiazhong%20Cen%20and%20Taoran%20Yi%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang&entry.1292438233=%20%20Modeling%2C%20understanding%2C%20and%20reconstructing%20the%20real%20world%20are%20crucial%20in%0AXR/VR.%20Recently%2C%203D%20Gaussian%20Splatting%20%283D-GS%29%20methods%20have%20shown%20remarkable%0Asuccess%20in%20modeling%20and%20understanding%203D%20scenes.%20Similarly%2C%20various%204D%0Arepresentations%20have%20demonstrated%20the%20ability%20to%20capture%20the%20dynamics%20of%20the%204D%0Aworld.%20However%2C%20there%20is%20a%20dearth%20of%20research%20focusing%20on%20segmentation%20within%0A4D%20representations.%20In%20this%20paper%2C%20we%20propose%20Segment%20Any%204D%20Gaussians%20%28SA4D%29%2C%0Aone%20of%20the%20first%20frameworks%20to%20segment%20anything%20in%20the%204D%20digital%20world%20based%0Aon%204D%20Gaussians.%20In%20SA4D%2C%20an%20efficient%20temporal%20identity%20feature%20field%20is%0Aintroduced%20to%20handle%20Gaussian%20drifting%2C%20with%20the%20potential%20to%20learn%20precise%0Aidentity%20features%20from%20noisy%20and%20sparse%20input.%20Additionally%2C%20a%204D%20segmentation%0Arefinement%20process%20is%20proposed%20to%20remove%20artifacts.%20Our%20SA4D%20achieves%20precise%2C%0Ahigh-quality%20segmentation%20within%20seconds%20in%204D%20Gaussians%20and%20shows%20the%20ability%0Ato%20remove%2C%20recolor%2C%20compose%2C%20and%20render%20high-quality%20anything%20masks.%20More%20demos%0Aare%20available%20at%3A%20https%3A//jsxzs.github.io/sa4d/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04504v1&entry.124074799=Read"},
{"title": "CBGL: Fast Monte Carlo Passive Global Localisation of 2D LIDAR Sensor", "author": "Alexandros Filotheou", "abstract": "  Navigation of a mobile robot is conditioned on the knowledge of its pose. In\nobserver-based localisation configurations its initial pose may not be knowable\nin advance, leading to the need of its estimation. Solutions to the problem of\nglobal localisation are either robust against noise and environment\narbitrariness but require motion and time, which may (need to) be economised\non, or require minimal estimation time but assume environmental structure, may\nbe sensitive to noise, and demand preprocessing and tuning. This article\nproposes a method that retains the strengths and avoids the weaknesses of the\ntwo approaches. The method leverages properties of the Cumulative Absolute\nError per Ray (CAER) metric with respect to the errors of pose hypotheses of a\n2D LIDAR sensor, and utilises scan--to--map-scan matching for fine(r) pose\nestimations. A large number of tests, in real and simulated conditions,\ninvolving disparate environments and sensor properties, illustrate that the\nproposed method outperforms state-of-the-art methods of both classes of\nsolutions in terms of pose discovery rate and execution time. The source code\nis available for download.\n", "link": "http://arxiv.org/abs/2307.14247v4", "date": "2024-07-05", "relevancy": 3.0545, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6547}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5929}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CBGL%3A%20Fast%20Monte%20Carlo%20Passive%20Global%20Localisation%20of%202D%20LIDAR%20Sensor&body=Title%3A%20CBGL%3A%20Fast%20Monte%20Carlo%20Passive%20Global%20Localisation%20of%202D%20LIDAR%20Sensor%0AAuthor%3A%20Alexandros%20Filotheou%0AAbstract%3A%20%20%20Navigation%20of%20a%20mobile%20robot%20is%20conditioned%20on%20the%20knowledge%20of%20its%20pose.%20In%0Aobserver-based%20localisation%20configurations%20its%20initial%20pose%20may%20not%20be%20knowable%0Ain%20advance%2C%20leading%20to%20the%20need%20of%20its%20estimation.%20Solutions%20to%20the%20problem%20of%0Aglobal%20localisation%20are%20either%20robust%20against%20noise%20and%20environment%0Aarbitrariness%20but%20require%20motion%20and%20time%2C%20which%20may%20%28need%20to%29%20be%20economised%0Aon%2C%20or%20require%20minimal%20estimation%20time%20but%20assume%20environmental%20structure%2C%20may%0Abe%20sensitive%20to%20noise%2C%20and%20demand%20preprocessing%20and%20tuning.%20This%20article%0Aproposes%20a%20method%20that%20retains%20the%20strengths%20and%20avoids%20the%20weaknesses%20of%20the%0Atwo%20approaches.%20The%20method%20leverages%20properties%20of%20the%20Cumulative%20Absolute%0AError%20per%20Ray%20%28CAER%29%20metric%20with%20respect%20to%20the%20errors%20of%20pose%20hypotheses%20of%20a%0A2D%20LIDAR%20sensor%2C%20and%20utilises%20scan--to--map-scan%20matching%20for%20fine%28r%29%20pose%0Aestimations.%20A%20large%20number%20of%20tests%2C%20in%20real%20and%20simulated%20conditions%2C%0Ainvolving%20disparate%20environments%20and%20sensor%20properties%2C%20illustrate%20that%20the%0Aproposed%20method%20outperforms%20state-of-the-art%20methods%20of%20both%20classes%20of%0Asolutions%20in%20terms%20of%20pose%20discovery%20rate%20and%20execution%20time.%20The%20source%20code%0Ais%20available%20for%20download.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.14247v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCBGL%253A%2520Fast%2520Monte%2520Carlo%2520Passive%2520Global%2520Localisation%2520of%25202D%2520LIDAR%2520Sensor%26entry.906535625%3DAlexandros%2520Filotheou%26entry.1292438233%3D%2520%2520Navigation%2520of%2520a%2520mobile%2520robot%2520is%2520conditioned%2520on%2520the%2520knowledge%2520of%2520its%2520pose.%2520In%250Aobserver-based%2520localisation%2520configurations%2520its%2520initial%2520pose%2520may%2520not%2520be%2520knowable%250Ain%2520advance%252C%2520leading%2520to%2520the%2520need%2520of%2520its%2520estimation.%2520Solutions%2520to%2520the%2520problem%2520of%250Aglobal%2520localisation%2520are%2520either%2520robust%2520against%2520noise%2520and%2520environment%250Aarbitrariness%2520but%2520require%2520motion%2520and%2520time%252C%2520which%2520may%2520%2528need%2520to%2529%2520be%2520economised%250Aon%252C%2520or%2520require%2520minimal%2520estimation%2520time%2520but%2520assume%2520environmental%2520structure%252C%2520may%250Abe%2520sensitive%2520to%2520noise%252C%2520and%2520demand%2520preprocessing%2520and%2520tuning.%2520This%2520article%250Aproposes%2520a%2520method%2520that%2520retains%2520the%2520strengths%2520and%2520avoids%2520the%2520weaknesses%2520of%2520the%250Atwo%2520approaches.%2520The%2520method%2520leverages%2520properties%2520of%2520the%2520Cumulative%2520Absolute%250AError%2520per%2520Ray%2520%2528CAER%2529%2520metric%2520with%2520respect%2520to%2520the%2520errors%2520of%2520pose%2520hypotheses%2520of%2520a%250A2D%2520LIDAR%2520sensor%252C%2520and%2520utilises%2520scan--to--map-scan%2520matching%2520for%2520fine%2528r%2529%2520pose%250Aestimations.%2520A%2520large%2520number%2520of%2520tests%252C%2520in%2520real%2520and%2520simulated%2520conditions%252C%250Ainvolving%2520disparate%2520environments%2520and%2520sensor%2520properties%252C%2520illustrate%2520that%2520the%250Aproposed%2520method%2520outperforms%2520state-of-the-art%2520methods%2520of%2520both%2520classes%2520of%250Asolutions%2520in%2520terms%2520of%2520pose%2520discovery%2520rate%2520and%2520execution%2520time.%2520The%2520source%2520code%250Ais%2520available%2520for%2520download.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.14247v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CBGL%3A%20Fast%20Monte%20Carlo%20Passive%20Global%20Localisation%20of%202D%20LIDAR%20Sensor&entry.906535625=Alexandros%20Filotheou&entry.1292438233=%20%20Navigation%20of%20a%20mobile%20robot%20is%20conditioned%20on%20the%20knowledge%20of%20its%20pose.%20In%0Aobserver-based%20localisation%20configurations%20its%20initial%20pose%20may%20not%20be%20knowable%0Ain%20advance%2C%20leading%20to%20the%20need%20of%20its%20estimation.%20Solutions%20to%20the%20problem%20of%0Aglobal%20localisation%20are%20either%20robust%20against%20noise%20and%20environment%0Aarbitrariness%20but%20require%20motion%20and%20time%2C%20which%20may%20%28need%20to%29%20be%20economised%0Aon%2C%20or%20require%20minimal%20estimation%20time%20but%20assume%20environmental%20structure%2C%20may%0Abe%20sensitive%20to%20noise%2C%20and%20demand%20preprocessing%20and%20tuning.%20This%20article%0Aproposes%20a%20method%20that%20retains%20the%20strengths%20and%20avoids%20the%20weaknesses%20of%20the%0Atwo%20approaches.%20The%20method%20leverages%20properties%20of%20the%20Cumulative%20Absolute%0AError%20per%20Ray%20%28CAER%29%20metric%20with%20respect%20to%20the%20errors%20of%20pose%20hypotheses%20of%20a%0A2D%20LIDAR%20sensor%2C%20and%20utilises%20scan--to--map-scan%20matching%20for%20fine%28r%29%20pose%0Aestimations.%20A%20large%20number%20of%20tests%2C%20in%20real%20and%20simulated%20conditions%2C%0Ainvolving%20disparate%20environments%20and%20sensor%20properties%2C%20illustrate%20that%20the%0Aproposed%20method%20outperforms%20state-of-the-art%20methods%20of%20both%20classes%20of%0Asolutions%20in%20terms%20of%20pose%20discovery%20rate%20and%20execution%20time.%20The%20source%20code%0Ais%20available%20for%20download.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.14247v4&entry.124074799=Read"},
{"title": "Improving Semantic Correspondence with Viewpoint-Guided Spherical Maps", "author": "Octave Mariotti and Oisin Mac Aodha and Hakan Bilen", "abstract": "  Recent progress in self-supervised representation learning has resulted in\nmodels that are capable of extracting image features that are not only\neffective at encoding image level, but also pixel-level, semantics. These\nfeatures have been shown to be effective for dense visual semantic\ncorrespondence estimation, even outperforming fully-supervised methods.\nNevertheless, current self-supervised approaches still fail in the presence of\nchallenging image characteristics such as symmetries and repeated parts. To\naddress these limitations, we propose a new approach for semantic\ncorrespondence estimation that supplements discriminative self-supervised\nfeatures with 3D understanding via a weak geometric spherical prior. Compared\nto more involved 3D pipelines, our model only requires weak viewpoint\ninformation, and the simplicity of our spherical representation enables us to\ninject informative geometric priors into the model during training. We propose\na new evaluation metric that better accounts for repeated part and\nsymmetry-induced mistakes. We present results on the challenging SPair-71k\ndataset, where we show that our approach demonstrates is capable of\ndistinguishing between symmetric views and repeated parts across many object\ncategories, and also demonstrate that we can generalize to unseen classes on\nthe AwA dataset.\n", "link": "http://arxiv.org/abs/2312.13216v2", "date": "2024-07-05", "relevancy": 2.8637, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.584}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5707}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Semantic%20Correspondence%20with%20Viewpoint-Guided%20Spherical%20Maps&body=Title%3A%20Improving%20Semantic%20Correspondence%20with%20Viewpoint-Guided%20Spherical%20Maps%0AAuthor%3A%20Octave%20Mariotti%20and%20Oisin%20Mac%20Aodha%20and%20Hakan%20Bilen%0AAbstract%3A%20%20%20Recent%20progress%20in%20self-supervised%20representation%20learning%20has%20resulted%20in%0Amodels%20that%20are%20capable%20of%20extracting%20image%20features%20that%20are%20not%20only%0Aeffective%20at%20encoding%20image%20level%2C%20but%20also%20pixel-level%2C%20semantics.%20These%0Afeatures%20have%20been%20shown%20to%20be%20effective%20for%20dense%20visual%20semantic%0Acorrespondence%20estimation%2C%20even%20outperforming%20fully-supervised%20methods.%0ANevertheless%2C%20current%20self-supervised%20approaches%20still%20fail%20in%20the%20presence%20of%0Achallenging%20image%20characteristics%20such%20as%20symmetries%20and%20repeated%20parts.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20new%20approach%20for%20semantic%0Acorrespondence%20estimation%20that%20supplements%20discriminative%20self-supervised%0Afeatures%20with%203D%20understanding%20via%20a%20weak%20geometric%20spherical%20prior.%20Compared%0Ato%20more%20involved%203D%20pipelines%2C%20our%20model%20only%20requires%20weak%20viewpoint%0Ainformation%2C%20and%20the%20simplicity%20of%20our%20spherical%20representation%20enables%20us%20to%0Ainject%20informative%20geometric%20priors%20into%20the%20model%20during%20training.%20We%20propose%0Aa%20new%20evaluation%20metric%20that%20better%20accounts%20for%20repeated%20part%20and%0Asymmetry-induced%20mistakes.%20We%20present%20results%20on%20the%20challenging%20SPair-71k%0Adataset%2C%20where%20we%20show%20that%20our%20approach%20demonstrates%20is%20capable%20of%0Adistinguishing%20between%20symmetric%20views%20and%20repeated%20parts%20across%20many%20object%0Acategories%2C%20and%20also%20demonstrate%20that%20we%20can%20generalize%20to%20unseen%20classes%20on%0Athe%20AwA%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Semantic%2520Correspondence%2520with%2520Viewpoint-Guided%2520Spherical%2520Maps%26entry.906535625%3DOctave%2520Mariotti%2520and%2520Oisin%2520Mac%2520Aodha%2520and%2520Hakan%2520Bilen%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520self-supervised%2520representation%2520learning%2520has%2520resulted%2520in%250Amodels%2520that%2520are%2520capable%2520of%2520extracting%2520image%2520features%2520that%2520are%2520not%2520only%250Aeffective%2520at%2520encoding%2520image%2520level%252C%2520but%2520also%2520pixel-level%252C%2520semantics.%2520These%250Afeatures%2520have%2520been%2520shown%2520to%2520be%2520effective%2520for%2520dense%2520visual%2520semantic%250Acorrespondence%2520estimation%252C%2520even%2520outperforming%2520fully-supervised%2520methods.%250ANevertheless%252C%2520current%2520self-supervised%2520approaches%2520still%2520fail%2520in%2520the%2520presence%2520of%250Achallenging%2520image%2520characteristics%2520such%2520as%2520symmetries%2520and%2520repeated%2520parts.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520a%2520new%2520approach%2520for%2520semantic%250Acorrespondence%2520estimation%2520that%2520supplements%2520discriminative%2520self-supervised%250Afeatures%2520with%25203D%2520understanding%2520via%2520a%2520weak%2520geometric%2520spherical%2520prior.%2520Compared%250Ato%2520more%2520involved%25203D%2520pipelines%252C%2520our%2520model%2520only%2520requires%2520weak%2520viewpoint%250Ainformation%252C%2520and%2520the%2520simplicity%2520of%2520our%2520spherical%2520representation%2520enables%2520us%2520to%250Ainject%2520informative%2520geometric%2520priors%2520into%2520the%2520model%2520during%2520training.%2520We%2520propose%250Aa%2520new%2520evaluation%2520metric%2520that%2520better%2520accounts%2520for%2520repeated%2520part%2520and%250Asymmetry-induced%2520mistakes.%2520We%2520present%2520results%2520on%2520the%2520challenging%2520SPair-71k%250Adataset%252C%2520where%2520we%2520show%2520that%2520our%2520approach%2520demonstrates%2520is%2520capable%2520of%250Adistinguishing%2520between%2520symmetric%2520views%2520and%2520repeated%2520parts%2520across%2520many%2520object%250Acategories%252C%2520and%2520also%2520demonstrate%2520that%2520we%2520can%2520generalize%2520to%2520unseen%2520classes%2520on%250Athe%2520AwA%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Semantic%20Correspondence%20with%20Viewpoint-Guided%20Spherical%20Maps&entry.906535625=Octave%20Mariotti%20and%20Oisin%20Mac%20Aodha%20and%20Hakan%20Bilen&entry.1292438233=%20%20Recent%20progress%20in%20self-supervised%20representation%20learning%20has%20resulted%20in%0Amodels%20that%20are%20capable%20of%20extracting%20image%20features%20that%20are%20not%20only%0Aeffective%20at%20encoding%20image%20level%2C%20but%20also%20pixel-level%2C%20semantics.%20These%0Afeatures%20have%20been%20shown%20to%20be%20effective%20for%20dense%20visual%20semantic%0Acorrespondence%20estimation%2C%20even%20outperforming%20fully-supervised%20methods.%0ANevertheless%2C%20current%20self-supervised%20approaches%20still%20fail%20in%20the%20presence%20of%0Achallenging%20image%20characteristics%20such%20as%20symmetries%20and%20repeated%20parts.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20new%20approach%20for%20semantic%0Acorrespondence%20estimation%20that%20supplements%20discriminative%20self-supervised%0Afeatures%20with%203D%20understanding%20via%20a%20weak%20geometric%20spherical%20prior.%20Compared%0Ato%20more%20involved%203D%20pipelines%2C%20our%20model%20only%20requires%20weak%20viewpoint%0Ainformation%2C%20and%20the%20simplicity%20of%20our%20spherical%20representation%20enables%20us%20to%0Ainject%20informative%20geometric%20priors%20into%20the%20model%20during%20training.%20We%20propose%0Aa%20new%20evaluation%20metric%20that%20better%20accounts%20for%20repeated%20part%20and%0Asymmetry-induced%20mistakes.%20We%20present%20results%20on%20the%20challenging%20SPair-71k%0Adataset%2C%20where%20we%20show%20that%20our%20approach%20demonstrates%20is%20capable%20of%0Adistinguishing%20between%20symmetric%20views%20and%20repeated%20parts%20across%20many%20object%0Acategories%2C%20and%20also%20demonstrate%20that%20we%20can%20generalize%20to%20unseen%20classes%20on%0Athe%20AwA%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13216v2&entry.124074799=Read"},
{"title": "PartCraft: Crafting Creative Objects by Parts", "author": "Kam Woh Ng and Xiatian Zhu and Yi-Zhe Song and Tao Xiang", "abstract": "  This paper propels creative control in generative visual AI by allowing users\nto \"select\". Departing from traditional text or sketch-based methods, we for\nthe first time allow users to choose visual concepts by parts for their\ncreative endeavors. The outcome is fine-grained generation that precisely\ncaptures selected visual concepts, ensuring a holistically faithful and\nplausible result. To achieve this, we first parse objects into parts through\nunsupervised feature clustering. Then, we encode parts into text tokens and\nintroduce an entropy-based normalized attention loss that operates on them.\nThis loss design enables our model to learn generic prior topology knowledge\nabout object's part composition, and further generalize to novel part\ncompositions to ensure the generation looks holistically faithful. Lastly, we\nemploy a bottleneck encoder to project the part tokens. This not only enhances\nfidelity but also accelerates learning, by leveraging shared knowledge and\nfacilitating information exchange among instances. Visual results in the paper\nand supplementary material showcase the compelling power of PartCraft in\ncrafting highly customized, innovative creations, exemplified by the \"charming\"\nand creative birds. Code is released at https://github.com/kamwoh/partcraft.\n", "link": "http://arxiv.org/abs/2407.04604v1", "date": "2024-07-05", "relevancy": 2.7021, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5599}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.548}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PartCraft%3A%20Crafting%20Creative%20Objects%20by%20Parts&body=Title%3A%20PartCraft%3A%20Crafting%20Creative%20Objects%20by%20Parts%0AAuthor%3A%20Kam%20Woh%20Ng%20and%20Xiatian%20Zhu%20and%20Yi-Zhe%20Song%20and%20Tao%20Xiang%0AAbstract%3A%20%20%20This%20paper%20propels%20creative%20control%20in%20generative%20visual%20AI%20by%20allowing%20users%0Ato%20%22select%22.%20Departing%20from%20traditional%20text%20or%20sketch-based%20methods%2C%20we%20for%0Athe%20first%20time%20allow%20users%20to%20choose%20visual%20concepts%20by%20parts%20for%20their%0Acreative%20endeavors.%20The%20outcome%20is%20fine-grained%20generation%20that%20precisely%0Acaptures%20selected%20visual%20concepts%2C%20ensuring%20a%20holistically%20faithful%20and%0Aplausible%20result.%20To%20achieve%20this%2C%20we%20first%20parse%20objects%20into%20parts%20through%0Aunsupervised%20feature%20clustering.%20Then%2C%20we%20encode%20parts%20into%20text%20tokens%20and%0Aintroduce%20an%20entropy-based%20normalized%20attention%20loss%20that%20operates%20on%20them.%0AThis%20loss%20design%20enables%20our%20model%20to%20learn%20generic%20prior%20topology%20knowledge%0Aabout%20object%27s%20part%20composition%2C%20and%20further%20generalize%20to%20novel%20part%0Acompositions%20to%20ensure%20the%20generation%20looks%20holistically%20faithful.%20Lastly%2C%20we%0Aemploy%20a%20bottleneck%20encoder%20to%20project%20the%20part%20tokens.%20This%20not%20only%20enhances%0Afidelity%20but%20also%20accelerates%20learning%2C%20by%20leveraging%20shared%20knowledge%20and%0Afacilitating%20information%20exchange%20among%20instances.%20Visual%20results%20in%20the%20paper%0Aand%20supplementary%20material%20showcase%20the%20compelling%20power%20of%20PartCraft%20in%0Acrafting%20highly%20customized%2C%20innovative%20creations%2C%20exemplified%20by%20the%20%22charming%22%0Aand%20creative%20birds.%20Code%20is%20released%20at%20https%3A//github.com/kamwoh/partcraft.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPartCraft%253A%2520Crafting%2520Creative%2520Objects%2520by%2520Parts%26entry.906535625%3DKam%2520Woh%2520Ng%2520and%2520Xiatian%2520Zhu%2520and%2520Yi-Zhe%2520Song%2520and%2520Tao%2520Xiang%26entry.1292438233%3D%2520%2520This%2520paper%2520propels%2520creative%2520control%2520in%2520generative%2520visual%2520AI%2520by%2520allowing%2520users%250Ato%2520%2522select%2522.%2520Departing%2520from%2520traditional%2520text%2520or%2520sketch-based%2520methods%252C%2520we%2520for%250Athe%2520first%2520time%2520allow%2520users%2520to%2520choose%2520visual%2520concepts%2520by%2520parts%2520for%2520their%250Acreative%2520endeavors.%2520The%2520outcome%2520is%2520fine-grained%2520generation%2520that%2520precisely%250Acaptures%2520selected%2520visual%2520concepts%252C%2520ensuring%2520a%2520holistically%2520faithful%2520and%250Aplausible%2520result.%2520To%2520achieve%2520this%252C%2520we%2520first%2520parse%2520objects%2520into%2520parts%2520through%250Aunsupervised%2520feature%2520clustering.%2520Then%252C%2520we%2520encode%2520parts%2520into%2520text%2520tokens%2520and%250Aintroduce%2520an%2520entropy-based%2520normalized%2520attention%2520loss%2520that%2520operates%2520on%2520them.%250AThis%2520loss%2520design%2520enables%2520our%2520model%2520to%2520learn%2520generic%2520prior%2520topology%2520knowledge%250Aabout%2520object%2527s%2520part%2520composition%252C%2520and%2520further%2520generalize%2520to%2520novel%2520part%250Acompositions%2520to%2520ensure%2520the%2520generation%2520looks%2520holistically%2520faithful.%2520Lastly%252C%2520we%250Aemploy%2520a%2520bottleneck%2520encoder%2520to%2520project%2520the%2520part%2520tokens.%2520This%2520not%2520only%2520enhances%250Afidelity%2520but%2520also%2520accelerates%2520learning%252C%2520by%2520leveraging%2520shared%2520knowledge%2520and%250Afacilitating%2520information%2520exchange%2520among%2520instances.%2520Visual%2520results%2520in%2520the%2520paper%250Aand%2520supplementary%2520material%2520showcase%2520the%2520compelling%2520power%2520of%2520PartCraft%2520in%250Acrafting%2520highly%2520customized%252C%2520innovative%2520creations%252C%2520exemplified%2520by%2520the%2520%2522charming%2522%250Aand%2520creative%2520birds.%2520Code%2520is%2520released%2520at%2520https%253A//github.com/kamwoh/partcraft.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PartCraft%3A%20Crafting%20Creative%20Objects%20by%20Parts&entry.906535625=Kam%20Woh%20Ng%20and%20Xiatian%20Zhu%20and%20Yi-Zhe%20Song%20and%20Tao%20Xiang&entry.1292438233=%20%20This%20paper%20propels%20creative%20control%20in%20generative%20visual%20AI%20by%20allowing%20users%0Ato%20%22select%22.%20Departing%20from%20traditional%20text%20or%20sketch-based%20methods%2C%20we%20for%0Athe%20first%20time%20allow%20users%20to%20choose%20visual%20concepts%20by%20parts%20for%20their%0Acreative%20endeavors.%20The%20outcome%20is%20fine-grained%20generation%20that%20precisely%0Acaptures%20selected%20visual%20concepts%2C%20ensuring%20a%20holistically%20faithful%20and%0Aplausible%20result.%20To%20achieve%20this%2C%20we%20first%20parse%20objects%20into%20parts%20through%0Aunsupervised%20feature%20clustering.%20Then%2C%20we%20encode%20parts%20into%20text%20tokens%20and%0Aintroduce%20an%20entropy-based%20normalized%20attention%20loss%20that%20operates%20on%20them.%0AThis%20loss%20design%20enables%20our%20model%20to%20learn%20generic%20prior%20topology%20knowledge%0Aabout%20object%27s%20part%20composition%2C%20and%20further%20generalize%20to%20novel%20part%0Acompositions%20to%20ensure%20the%20generation%20looks%20holistically%20faithful.%20Lastly%2C%20we%0Aemploy%20a%20bottleneck%20encoder%20to%20project%20the%20part%20tokens.%20This%20not%20only%20enhances%0Afidelity%20but%20also%20accelerates%20learning%2C%20by%20leveraging%20shared%20knowledge%20and%0Afacilitating%20information%20exchange%20among%20instances.%20Visual%20results%20in%20the%20paper%0Aand%20supplementary%20material%20showcase%20the%20compelling%20power%20of%20PartCraft%20in%0Acrafting%20highly%20customized%2C%20innovative%20creations%2C%20exemplified%20by%20the%20%22charming%22%0Aand%20creative%20birds.%20Code%20is%20released%20at%20https%3A//github.com/kamwoh/partcraft.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04604v1&entry.124074799=Read"},
{"title": "CountGD: Multi-Modal Open-World Counting", "author": "Niki Amini-Naieni and Tengda Han and Andrew Zisserman", "abstract": "  The goal of this paper is to improve the generality and accuracy of\nopen-vocabulary object counting in images. To improve the generality, we\nrepurpose an open-vocabulary detection foundation model (GroundingDINO) for the\ncounting task, and also extend its capabilities by introducing modules to\nenable specifying the target object to count by visual exemplars. In turn,\nthese new capabilities - being able to specify the target object by\nmulti-modalites (text and exemplars) - lead to an improvement in counting\naccuracy.\n  We make three contributions: First, we introduce the first open-world\ncounting model, CountGD, where the prompt can be specified by a text\ndescription or visual exemplars or both; Second, we show that the performance\nof the model significantly improves the state of the art on multiple counting\nbenchmarks - when using text only, CountGD is comparable to or outperforms all\nprevious text-only works, and when using both text and visual exemplars, we\noutperform all previous models; Third, we carry out a preliminary study into\ndifferent interactions between the text and visual exemplar prompts, including\nthe cases where they reinforce each other and where one restricts the other.\nThe code and an app to test the model are available at\nhttps://www.robots.ox.ac.uk/~vgg/research/countgd/.\n", "link": "http://arxiv.org/abs/2407.04619v1", "date": "2024-07-05", "relevancy": 2.7016, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5547}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5337}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5326}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CountGD%3A%20Multi-Modal%20Open-World%20Counting&body=Title%3A%20CountGD%3A%20Multi-Modal%20Open-World%20Counting%0AAuthor%3A%20Niki%20Amini-Naieni%20and%20Tengda%20Han%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20The%20goal%20of%20this%20paper%20is%20to%20improve%20the%20generality%20and%20accuracy%20of%0Aopen-vocabulary%20object%20counting%20in%20images.%20To%20improve%20the%20generality%2C%20we%0Arepurpose%20an%20open-vocabulary%20detection%20foundation%20model%20%28GroundingDINO%29%20for%20the%0Acounting%20task%2C%20and%20also%20extend%20its%20capabilities%20by%20introducing%20modules%20to%0Aenable%20specifying%20the%20target%20object%20to%20count%20by%20visual%20exemplars.%20In%20turn%2C%0Athese%20new%20capabilities%20-%20being%20able%20to%20specify%20the%20target%20object%20by%0Amulti-modalites%20%28text%20and%20exemplars%29%20-%20lead%20to%20an%20improvement%20in%20counting%0Aaccuracy.%0A%20%20We%20make%20three%20contributions%3A%20First%2C%20we%20introduce%20the%20first%20open-world%0Acounting%20model%2C%20CountGD%2C%20where%20the%20prompt%20can%20be%20specified%20by%20a%20text%0Adescription%20or%20visual%20exemplars%20or%20both%3B%20Second%2C%20we%20show%20that%20the%20performance%0Aof%20the%20model%20significantly%20improves%20the%20state%20of%20the%20art%20on%20multiple%20counting%0Abenchmarks%20-%20when%20using%20text%20only%2C%20CountGD%20is%20comparable%20to%20or%20outperforms%20all%0Aprevious%20text-only%20works%2C%20and%20when%20using%20both%20text%20and%20visual%20exemplars%2C%20we%0Aoutperform%20all%20previous%20models%3B%20Third%2C%20we%20carry%20out%20a%20preliminary%20study%20into%0Adifferent%20interactions%20between%20the%20text%20and%20visual%20exemplar%20prompts%2C%20including%0Athe%20cases%20where%20they%20reinforce%20each%20other%20and%20where%20one%20restricts%20the%20other.%0AThe%20code%20and%20an%20app%20to%20test%20the%20model%20are%20available%20at%0Ahttps%3A//www.robots.ox.ac.uk/~vgg/research/countgd/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCountGD%253A%2520Multi-Modal%2520Open-World%2520Counting%26entry.906535625%3DNiki%2520Amini-Naieni%2520and%2520Tengda%2520Han%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520The%2520goal%2520of%2520this%2520paper%2520is%2520to%2520improve%2520the%2520generality%2520and%2520accuracy%2520of%250Aopen-vocabulary%2520object%2520counting%2520in%2520images.%2520To%2520improve%2520the%2520generality%252C%2520we%250Arepurpose%2520an%2520open-vocabulary%2520detection%2520foundation%2520model%2520%2528GroundingDINO%2529%2520for%2520the%250Acounting%2520task%252C%2520and%2520also%2520extend%2520its%2520capabilities%2520by%2520introducing%2520modules%2520to%250Aenable%2520specifying%2520the%2520target%2520object%2520to%2520count%2520by%2520visual%2520exemplars.%2520In%2520turn%252C%250Athese%2520new%2520capabilities%2520-%2520being%2520able%2520to%2520specify%2520the%2520target%2520object%2520by%250Amulti-modalites%2520%2528text%2520and%2520exemplars%2529%2520-%2520lead%2520to%2520an%2520improvement%2520in%2520counting%250Aaccuracy.%250A%2520%2520We%2520make%2520three%2520contributions%253A%2520First%252C%2520we%2520introduce%2520the%2520first%2520open-world%250Acounting%2520model%252C%2520CountGD%252C%2520where%2520the%2520prompt%2520can%2520be%2520specified%2520by%2520a%2520text%250Adescription%2520or%2520visual%2520exemplars%2520or%2520both%253B%2520Second%252C%2520we%2520show%2520that%2520the%2520performance%250Aof%2520the%2520model%2520significantly%2520improves%2520the%2520state%2520of%2520the%2520art%2520on%2520multiple%2520counting%250Abenchmarks%2520-%2520when%2520using%2520text%2520only%252C%2520CountGD%2520is%2520comparable%2520to%2520or%2520outperforms%2520all%250Aprevious%2520text-only%2520works%252C%2520and%2520when%2520using%2520both%2520text%2520and%2520visual%2520exemplars%252C%2520we%250Aoutperform%2520all%2520previous%2520models%253B%2520Third%252C%2520we%2520carry%2520out%2520a%2520preliminary%2520study%2520into%250Adifferent%2520interactions%2520between%2520the%2520text%2520and%2520visual%2520exemplar%2520prompts%252C%2520including%250Athe%2520cases%2520where%2520they%2520reinforce%2520each%2520other%2520and%2520where%2520one%2520restricts%2520the%2520other.%250AThe%2520code%2520and%2520an%2520app%2520to%2520test%2520the%2520model%2520are%2520available%2520at%250Ahttps%253A//www.robots.ox.ac.uk/~vgg/research/countgd/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CountGD%3A%20Multi-Modal%20Open-World%20Counting&entry.906535625=Niki%20Amini-Naieni%20and%20Tengda%20Han%20and%20Andrew%20Zisserman&entry.1292438233=%20%20The%20goal%20of%20this%20paper%20is%20to%20improve%20the%20generality%20and%20accuracy%20of%0Aopen-vocabulary%20object%20counting%20in%20images.%20To%20improve%20the%20generality%2C%20we%0Arepurpose%20an%20open-vocabulary%20detection%20foundation%20model%20%28GroundingDINO%29%20for%20the%0Acounting%20task%2C%20and%20also%20extend%20its%20capabilities%20by%20introducing%20modules%20to%0Aenable%20specifying%20the%20target%20object%20to%20count%20by%20visual%20exemplars.%20In%20turn%2C%0Athese%20new%20capabilities%20-%20being%20able%20to%20specify%20the%20target%20object%20by%0Amulti-modalites%20%28text%20and%20exemplars%29%20-%20lead%20to%20an%20improvement%20in%20counting%0Aaccuracy.%0A%20%20We%20make%20three%20contributions%3A%20First%2C%20we%20introduce%20the%20first%20open-world%0Acounting%20model%2C%20CountGD%2C%20where%20the%20prompt%20can%20be%20specified%20by%20a%20text%0Adescription%20or%20visual%20exemplars%20or%20both%3B%20Second%2C%20we%20show%20that%20the%20performance%0Aof%20the%20model%20significantly%20improves%20the%20state%20of%20the%20art%20on%20multiple%20counting%0Abenchmarks%20-%20when%20using%20text%20only%2C%20CountGD%20is%20comparable%20to%20or%20outperforms%20all%0Aprevious%20text-only%20works%2C%20and%20when%20using%20both%20text%20and%20visual%20exemplars%2C%20we%0Aoutperform%20all%20previous%20models%3B%20Third%2C%20we%20carry%20out%20a%20preliminary%20study%20into%0Adifferent%20interactions%20between%20the%20text%20and%20visual%20exemplar%20prompts%2C%20including%0Athe%20cases%20where%20they%20reinforce%20each%20other%20and%20where%20one%20restricts%20the%20other.%0AThe%20code%20and%20an%20app%20to%20test%20the%20model%20are%20available%20at%0Ahttps%3A//www.robots.ox.ac.uk/~vgg/research/countgd/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04619v1&entry.124074799=Read"},
{"title": "Unsupervised 4D Cardiac Motion Tracking with Spatiotemporal Optical Flow\n  Networks", "author": "Long Teng and Wei Feng and Menglong Zhu and Xinchao Li", "abstract": "  Cardiac motion tracking from echocardiography can be used to estimate and\nquantify myocardial motion within a cardiac cycle. It is a cost-efficient and\neffective approach for assessing myocardial function. However, ultrasound\nimaging has the inherent characteristics of spatially low resolution and\ntemporally random noise, which leads to difficulties in obtaining reliable\nannotation. Thus it is difficult to perform supervised learning for motion\ntracking. In addition, there is no end-to-end unsupervised method currently in\nthe literature. This paper presents a motion tracking method where unsupervised\noptical flow networks are designed with spatial reconstruction loss and\ntemporal-consistency loss. Our proposed loss functions make use of the\npair-wise and temporal correlation to estimate cardiac motion from noisy\nbackground. Experiments using a synthetic 4D echocardiography dataset has shown\nthe effectiveness of our approach, and its superiority over existing methods on\nboth accuracy and running speed. To the best of our knowledge, this is the\nfirst work performed that uses unsupervised end-to-end deep learning optical\nflow network for 4D cardiac motion tracking.\n", "link": "http://arxiv.org/abs/2407.04663v1", "date": "2024-07-05", "relevancy": 2.6936, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5699}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5324}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%204D%20Cardiac%20Motion%20Tracking%20with%20Spatiotemporal%20Optical%20Flow%0A%20%20Networks&body=Title%3A%20Unsupervised%204D%20Cardiac%20Motion%20Tracking%20with%20Spatiotemporal%20Optical%20Flow%0A%20%20Networks%0AAuthor%3A%20Long%20Teng%20and%20Wei%20Feng%20and%20Menglong%20Zhu%20and%20Xinchao%20Li%0AAbstract%3A%20%20%20Cardiac%20motion%20tracking%20from%20echocardiography%20can%20be%20used%20to%20estimate%20and%0Aquantify%20myocardial%20motion%20within%20a%20cardiac%20cycle.%20It%20is%20a%20cost-efficient%20and%0Aeffective%20approach%20for%20assessing%20myocardial%20function.%20However%2C%20ultrasound%0Aimaging%20has%20the%20inherent%20characteristics%20of%20spatially%20low%20resolution%20and%0Atemporally%20random%20noise%2C%20which%20leads%20to%20difficulties%20in%20obtaining%20reliable%0Aannotation.%20Thus%20it%20is%20difficult%20to%20perform%20supervised%20learning%20for%20motion%0Atracking.%20In%20addition%2C%20there%20is%20no%20end-to-end%20unsupervised%20method%20currently%20in%0Athe%20literature.%20This%20paper%20presents%20a%20motion%20tracking%20method%20where%20unsupervised%0Aoptical%20flow%20networks%20are%20designed%20with%20spatial%20reconstruction%20loss%20and%0Atemporal-consistency%20loss.%20Our%20proposed%20loss%20functions%20make%20use%20of%20the%0Apair-wise%20and%20temporal%20correlation%20to%20estimate%20cardiac%20motion%20from%20noisy%0Abackground.%20Experiments%20using%20a%20synthetic%204D%20echocardiography%20dataset%20has%20shown%0Athe%20effectiveness%20of%20our%20approach%2C%20and%20its%20superiority%20over%20existing%20methods%20on%0Aboth%20accuracy%20and%20running%20speed.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%0Afirst%20work%20performed%20that%20uses%20unsupervised%20end-to-end%20deep%20learning%20optical%0Aflow%20network%20for%204D%20cardiac%20motion%20tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%25204D%2520Cardiac%2520Motion%2520Tracking%2520with%2520Spatiotemporal%2520Optical%2520Flow%250A%2520%2520Networks%26entry.906535625%3DLong%2520Teng%2520and%2520Wei%2520Feng%2520and%2520Menglong%2520Zhu%2520and%2520Xinchao%2520Li%26entry.1292438233%3D%2520%2520Cardiac%2520motion%2520tracking%2520from%2520echocardiography%2520can%2520be%2520used%2520to%2520estimate%2520and%250Aquantify%2520myocardial%2520motion%2520within%2520a%2520cardiac%2520cycle.%2520It%2520is%2520a%2520cost-efficient%2520and%250Aeffective%2520approach%2520for%2520assessing%2520myocardial%2520function.%2520However%252C%2520ultrasound%250Aimaging%2520has%2520the%2520inherent%2520characteristics%2520of%2520spatially%2520low%2520resolution%2520and%250Atemporally%2520random%2520noise%252C%2520which%2520leads%2520to%2520difficulties%2520in%2520obtaining%2520reliable%250Aannotation.%2520Thus%2520it%2520is%2520difficult%2520to%2520perform%2520supervised%2520learning%2520for%2520motion%250Atracking.%2520In%2520addition%252C%2520there%2520is%2520no%2520end-to-end%2520unsupervised%2520method%2520currently%2520in%250Athe%2520literature.%2520This%2520paper%2520presents%2520a%2520motion%2520tracking%2520method%2520where%2520unsupervised%250Aoptical%2520flow%2520networks%2520are%2520designed%2520with%2520spatial%2520reconstruction%2520loss%2520and%250Atemporal-consistency%2520loss.%2520Our%2520proposed%2520loss%2520functions%2520make%2520use%2520of%2520the%250Apair-wise%2520and%2520temporal%2520correlation%2520to%2520estimate%2520cardiac%2520motion%2520from%2520noisy%250Abackground.%2520Experiments%2520using%2520a%2520synthetic%25204D%2520echocardiography%2520dataset%2520has%2520shown%250Athe%2520effectiveness%2520of%2520our%2520approach%252C%2520and%2520its%2520superiority%2520over%2520existing%2520methods%2520on%250Aboth%2520accuracy%2520and%2520running%2520speed.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520work%2520performed%2520that%2520uses%2520unsupervised%2520end-to-end%2520deep%2520learning%2520optical%250Aflow%2520network%2520for%25204D%2520cardiac%2520motion%2520tracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%204D%20Cardiac%20Motion%20Tracking%20with%20Spatiotemporal%20Optical%20Flow%0A%20%20Networks&entry.906535625=Long%20Teng%20and%20Wei%20Feng%20and%20Menglong%20Zhu%20and%20Xinchao%20Li&entry.1292438233=%20%20Cardiac%20motion%20tracking%20from%20echocardiography%20can%20be%20used%20to%20estimate%20and%0Aquantify%20myocardial%20motion%20within%20a%20cardiac%20cycle.%20It%20is%20a%20cost-efficient%20and%0Aeffective%20approach%20for%20assessing%20myocardial%20function.%20However%2C%20ultrasound%0Aimaging%20has%20the%20inherent%20characteristics%20of%20spatially%20low%20resolution%20and%0Atemporally%20random%20noise%2C%20which%20leads%20to%20difficulties%20in%20obtaining%20reliable%0Aannotation.%20Thus%20it%20is%20difficult%20to%20perform%20supervised%20learning%20for%20motion%0Atracking.%20In%20addition%2C%20there%20is%20no%20end-to-end%20unsupervised%20method%20currently%20in%0Athe%20literature.%20This%20paper%20presents%20a%20motion%20tracking%20method%20where%20unsupervised%0Aoptical%20flow%20networks%20are%20designed%20with%20spatial%20reconstruction%20loss%20and%0Atemporal-consistency%20loss.%20Our%20proposed%20loss%20functions%20make%20use%20of%20the%0Apair-wise%20and%20temporal%20correlation%20to%20estimate%20cardiac%20motion%20from%20noisy%0Abackground.%20Experiments%20using%20a%20synthetic%204D%20echocardiography%20dataset%20has%20shown%0Athe%20effectiveness%20of%20our%20approach%2C%20and%20its%20superiority%20over%20existing%20methods%20on%0Aboth%20accuracy%20and%20running%20speed.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%0Afirst%20work%20performed%20that%20uses%20unsupervised%20end-to-end%20deep%20learning%20optical%0Aflow%20network%20for%204D%20cardiac%20motion%20tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04663v1&entry.124074799=Read"},
{"title": "Efficient Betti Matching Enables Topology-Aware 3D Segmentation via\n  Persistent Homology", "author": "Nico Stucki and Vincent B\u00fcrgin and Johannes C. Paetzold and Ulrich Bauer", "abstract": "  In this work, we propose an efficient algorithm for the calculation of the\nBetti matching, which can be used as a loss function to train topology aware\nsegmentation networks. Betti matching loss builds on techniques from\ntopological data analysis, specifically persistent homology. A major challenge\nis the computational cost of computing persistence barcodes. In response to\nthis challenge, we propose a new, highly optimized implementation of Betti\nmatching, implemented in C++ together with a python interface, which achieves\nsignificant speedups compared to the state-of-the-art implementation Cubical\nRipser. We use Betti matching 3D to train segmentation networks with the Betti\nmatching loss and demonstrate improved topological correctness of predicted\nsegmentations across several datasets. The source code is available at\nhttps://github.com/nstucki/Betti-Matching-3D.\n", "link": "http://arxiv.org/abs/2407.04683v1", "date": "2024-07-05", "relevancy": 2.628, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5088}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Betti%20Matching%20Enables%20Topology-Aware%203D%20Segmentation%20via%0A%20%20Persistent%20Homology&body=Title%3A%20Efficient%20Betti%20Matching%20Enables%20Topology-Aware%203D%20Segmentation%20via%0A%20%20Persistent%20Homology%0AAuthor%3A%20Nico%20Stucki%20and%20Vincent%20B%C3%BCrgin%20and%20Johannes%20C.%20Paetzold%20and%20Ulrich%20Bauer%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20an%20efficient%20algorithm%20for%20the%20calculation%20of%20the%0ABetti%20matching%2C%20which%20can%20be%20used%20as%20a%20loss%20function%20to%20train%20topology%20aware%0Asegmentation%20networks.%20Betti%20matching%20loss%20builds%20on%20techniques%20from%0Atopological%20data%20analysis%2C%20specifically%20persistent%20homology.%20A%20major%20challenge%0Ais%20the%20computational%20cost%20of%20computing%20persistence%20barcodes.%20In%20response%20to%0Athis%20challenge%2C%20we%20propose%20a%20new%2C%20highly%20optimized%20implementation%20of%20Betti%0Amatching%2C%20implemented%20in%20C%2B%2B%20together%20with%20a%20python%20interface%2C%20which%20achieves%0Asignificant%20speedups%20compared%20to%20the%20state-of-the-art%20implementation%20Cubical%0ARipser.%20We%20use%20Betti%20matching%203D%20to%20train%20segmentation%20networks%20with%20the%20Betti%0Amatching%20loss%20and%20demonstrate%20improved%20topological%20correctness%20of%20predicted%0Asegmentations%20across%20several%20datasets.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/nstucki/Betti-Matching-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Betti%2520Matching%2520Enables%2520Topology-Aware%25203D%2520Segmentation%2520via%250A%2520%2520Persistent%2520Homology%26entry.906535625%3DNico%2520Stucki%2520and%2520Vincent%2520B%25C3%25BCrgin%2520and%2520Johannes%2520C.%2520Paetzold%2520and%2520Ulrich%2520Bauer%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520efficient%2520algorithm%2520for%2520the%2520calculation%2520of%2520the%250ABetti%2520matching%252C%2520which%2520can%2520be%2520used%2520as%2520a%2520loss%2520function%2520to%2520train%2520topology%2520aware%250Asegmentation%2520networks.%2520Betti%2520matching%2520loss%2520builds%2520on%2520techniques%2520from%250Atopological%2520data%2520analysis%252C%2520specifically%2520persistent%2520homology.%2520A%2520major%2520challenge%250Ais%2520the%2520computational%2520cost%2520of%2520computing%2520persistence%2520barcodes.%2520In%2520response%2520to%250Athis%2520challenge%252C%2520we%2520propose%2520a%2520new%252C%2520highly%2520optimized%2520implementation%2520of%2520Betti%250Amatching%252C%2520implemented%2520in%2520C%252B%252B%2520together%2520with%2520a%2520python%2520interface%252C%2520which%2520achieves%250Asignificant%2520speedups%2520compared%2520to%2520the%2520state-of-the-art%2520implementation%2520Cubical%250ARipser.%2520We%2520use%2520Betti%2520matching%25203D%2520to%2520train%2520segmentation%2520networks%2520with%2520the%2520Betti%250Amatching%2520loss%2520and%2520demonstrate%2520improved%2520topological%2520correctness%2520of%2520predicted%250Asegmentations%2520across%2520several%2520datasets.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/nstucki/Betti-Matching-3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Betti%20Matching%20Enables%20Topology-Aware%203D%20Segmentation%20via%0A%20%20Persistent%20Homology&entry.906535625=Nico%20Stucki%20and%20Vincent%20B%C3%BCrgin%20and%20Johannes%20C.%20Paetzold%20and%20Ulrich%20Bauer&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20an%20efficient%20algorithm%20for%20the%20calculation%20of%20the%0ABetti%20matching%2C%20which%20can%20be%20used%20as%20a%20loss%20function%20to%20train%20topology%20aware%0Asegmentation%20networks.%20Betti%20matching%20loss%20builds%20on%20techniques%20from%0Atopological%20data%20analysis%2C%20specifically%20persistent%20homology.%20A%20major%20challenge%0Ais%20the%20computational%20cost%20of%20computing%20persistence%20barcodes.%20In%20response%20to%0Athis%20challenge%2C%20we%20propose%20a%20new%2C%20highly%20optimized%20implementation%20of%20Betti%0Amatching%2C%20implemented%20in%20C%2B%2B%20together%20with%20a%20python%20interface%2C%20which%20achieves%0Asignificant%20speedups%20compared%20to%20the%20state-of-the-art%20implementation%20Cubical%0ARipser.%20We%20use%20Betti%20matching%203D%20to%20train%20segmentation%20networks%20with%20the%20Betti%0Amatching%20loss%20and%20demonstrate%20improved%20topological%20correctness%20of%20predicted%0Asegmentations%20across%20several%20datasets.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/nstucki/Betti-Matching-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04683v1&entry.124074799=Read"},
{"title": "From Pixel to Cancer: Cellular Automata in Computed Tomography", "author": "Yuxiang Lai and Xiaoxi Chen and Angtian Wang and Alan Yuille and Zongwei Zhou", "abstract": "  AI for cancer detection encounters the bottleneck of data scarcity,\nannotation difficulty, and low prevalence of early tumors. Tumor synthesis\nseeks to create artificial tumors in medical images, which can greatly\ndiversify the data and annotations for AI training. However, current tumor\nsynthesis approaches are not applicable across different organs due to their\nneed for specific expertise and design. This paper establishes a set of generic\nrules to simulate tumor development. Each cell (pixel) is initially assigned a\nstate between zero and ten to represent the tumor population, and a tumor can\nbe developed based on three rules to describe the process of growth, invasion,\nand death. We apply these three generic rules to simulate tumor\ndevelopment--from pixel to cancer--using cellular automata. We then integrate\nthe tumor state into the original computed tomography (CT) images to generate\nsynthetic tumors across different organs. This tumor synthesis approach allows\nfor sampling tumors at multiple stages and analyzing tumor-organ interaction.\nClinically, a reader study involving three expert radiologists reveals that the\nsynthetic tumors and their developing trajectories are convincingly realistic.\nTechnically, we analyze and simulate tumor development at various stages using\n9,262 raw, unlabeled CT images sourced from 68 hospitals worldwide. The\nperformance in segmenting tumors in the liver, pancreas, and kidneys exceeds\nprevailing literature benchmarks, underlining the immense potential of tumor\nsynthesis, especially for earlier cancer detection.\n  The code and models are available at\nhttps://github.com/MrGiovanni/Pixel2Cancer\n", "link": "http://arxiv.org/abs/2403.06459v2", "date": "2024-07-05", "relevancy": 2.6058, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5449}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5449}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Pixel%20to%20Cancer%3A%20Cellular%20Automata%20in%20Computed%20Tomography&body=Title%3A%20From%20Pixel%20to%20Cancer%3A%20Cellular%20Automata%20in%20Computed%20Tomography%0AAuthor%3A%20Yuxiang%20Lai%20and%20Xiaoxi%20Chen%20and%20Angtian%20Wang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou%0AAbstract%3A%20%20%20AI%20for%20cancer%20detection%20encounters%20the%20bottleneck%20of%20data%20scarcity%2C%0Aannotation%20difficulty%2C%20and%20low%20prevalence%20of%20early%20tumors.%20Tumor%20synthesis%0Aseeks%20to%20create%20artificial%20tumors%20in%20medical%20images%2C%20which%20can%20greatly%0Adiversify%20the%20data%20and%20annotations%20for%20AI%20training.%20However%2C%20current%20tumor%0Asynthesis%20approaches%20are%20not%20applicable%20across%20different%20organs%20due%20to%20their%0Aneed%20for%20specific%20expertise%20and%20design.%20This%20paper%20establishes%20a%20set%20of%20generic%0Arules%20to%20simulate%20tumor%20development.%20Each%20cell%20%28pixel%29%20is%20initially%20assigned%20a%0Astate%20between%20zero%20and%20ten%20to%20represent%20the%20tumor%20population%2C%20and%20a%20tumor%20can%0Abe%20developed%20based%20on%20three%20rules%20to%20describe%20the%20process%20of%20growth%2C%20invasion%2C%0Aand%20death.%20We%20apply%20these%20three%20generic%20rules%20to%20simulate%20tumor%0Adevelopment--from%20pixel%20to%20cancer--using%20cellular%20automata.%20We%20then%20integrate%0Athe%20tumor%20state%20into%20the%20original%20computed%20tomography%20%28CT%29%20images%20to%20generate%0Asynthetic%20tumors%20across%20different%20organs.%20This%20tumor%20synthesis%20approach%20allows%0Afor%20sampling%20tumors%20at%20multiple%20stages%20and%20analyzing%20tumor-organ%20interaction.%0AClinically%2C%20a%20reader%20study%20involving%20three%20expert%20radiologists%20reveals%20that%20the%0Asynthetic%20tumors%20and%20their%20developing%20trajectories%20are%20convincingly%20realistic.%0ATechnically%2C%20we%20analyze%20and%20simulate%20tumor%20development%20at%20various%20stages%20using%0A9%2C262%20raw%2C%20unlabeled%20CT%20images%20sourced%20from%2068%20hospitals%20worldwide.%20The%0Aperformance%20in%20segmenting%20tumors%20in%20the%20liver%2C%20pancreas%2C%20and%20kidneys%20exceeds%0Aprevailing%20literature%20benchmarks%2C%20underlining%20the%20immense%20potential%20of%20tumor%0Asynthesis%2C%20especially%20for%20earlier%20cancer%20detection.%0A%20%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/MrGiovanni/Pixel2Cancer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Pixel%2520to%2520Cancer%253A%2520Cellular%2520Automata%2520in%2520Computed%2520Tomography%26entry.906535625%3DYuxiang%2520Lai%2520and%2520Xiaoxi%2520Chen%2520and%2520Angtian%2520Wang%2520and%2520Alan%2520Yuille%2520and%2520Zongwei%2520Zhou%26entry.1292438233%3D%2520%2520AI%2520for%2520cancer%2520detection%2520encounters%2520the%2520bottleneck%2520of%2520data%2520scarcity%252C%250Aannotation%2520difficulty%252C%2520and%2520low%2520prevalence%2520of%2520early%2520tumors.%2520Tumor%2520synthesis%250Aseeks%2520to%2520create%2520artificial%2520tumors%2520in%2520medical%2520images%252C%2520which%2520can%2520greatly%250Adiversify%2520the%2520data%2520and%2520annotations%2520for%2520AI%2520training.%2520However%252C%2520current%2520tumor%250Asynthesis%2520approaches%2520are%2520not%2520applicable%2520across%2520different%2520organs%2520due%2520to%2520their%250Aneed%2520for%2520specific%2520expertise%2520and%2520design.%2520This%2520paper%2520establishes%2520a%2520set%2520of%2520generic%250Arules%2520to%2520simulate%2520tumor%2520development.%2520Each%2520cell%2520%2528pixel%2529%2520is%2520initially%2520assigned%2520a%250Astate%2520between%2520zero%2520and%2520ten%2520to%2520represent%2520the%2520tumor%2520population%252C%2520and%2520a%2520tumor%2520can%250Abe%2520developed%2520based%2520on%2520three%2520rules%2520to%2520describe%2520the%2520process%2520of%2520growth%252C%2520invasion%252C%250Aand%2520death.%2520We%2520apply%2520these%2520three%2520generic%2520rules%2520to%2520simulate%2520tumor%250Adevelopment--from%2520pixel%2520to%2520cancer--using%2520cellular%2520automata.%2520We%2520then%2520integrate%250Athe%2520tumor%2520state%2520into%2520the%2520original%2520computed%2520tomography%2520%2528CT%2529%2520images%2520to%2520generate%250Asynthetic%2520tumors%2520across%2520different%2520organs.%2520This%2520tumor%2520synthesis%2520approach%2520allows%250Afor%2520sampling%2520tumors%2520at%2520multiple%2520stages%2520and%2520analyzing%2520tumor-organ%2520interaction.%250AClinically%252C%2520a%2520reader%2520study%2520involving%2520three%2520expert%2520radiologists%2520reveals%2520that%2520the%250Asynthetic%2520tumors%2520and%2520their%2520developing%2520trajectories%2520are%2520convincingly%2520realistic.%250ATechnically%252C%2520we%2520analyze%2520and%2520simulate%2520tumor%2520development%2520at%2520various%2520stages%2520using%250A9%252C262%2520raw%252C%2520unlabeled%2520CT%2520images%2520sourced%2520from%252068%2520hospitals%2520worldwide.%2520The%250Aperformance%2520in%2520segmenting%2520tumors%2520in%2520the%2520liver%252C%2520pancreas%252C%2520and%2520kidneys%2520exceeds%250Aprevailing%2520literature%2520benchmarks%252C%2520underlining%2520the%2520immense%2520potential%2520of%2520tumor%250Asynthesis%252C%2520especially%2520for%2520earlier%2520cancer%2520detection.%250A%2520%2520The%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/MrGiovanni/Pixel2Cancer%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Pixel%20to%20Cancer%3A%20Cellular%20Automata%20in%20Computed%20Tomography&entry.906535625=Yuxiang%20Lai%20and%20Xiaoxi%20Chen%20and%20Angtian%20Wang%20and%20Alan%20Yuille%20and%20Zongwei%20Zhou&entry.1292438233=%20%20AI%20for%20cancer%20detection%20encounters%20the%20bottleneck%20of%20data%20scarcity%2C%0Aannotation%20difficulty%2C%20and%20low%20prevalence%20of%20early%20tumors.%20Tumor%20synthesis%0Aseeks%20to%20create%20artificial%20tumors%20in%20medical%20images%2C%20which%20can%20greatly%0Adiversify%20the%20data%20and%20annotations%20for%20AI%20training.%20However%2C%20current%20tumor%0Asynthesis%20approaches%20are%20not%20applicable%20across%20different%20organs%20due%20to%20their%0Aneed%20for%20specific%20expertise%20and%20design.%20This%20paper%20establishes%20a%20set%20of%20generic%0Arules%20to%20simulate%20tumor%20development.%20Each%20cell%20%28pixel%29%20is%20initially%20assigned%20a%0Astate%20between%20zero%20and%20ten%20to%20represent%20the%20tumor%20population%2C%20and%20a%20tumor%20can%0Abe%20developed%20based%20on%20three%20rules%20to%20describe%20the%20process%20of%20growth%2C%20invasion%2C%0Aand%20death.%20We%20apply%20these%20three%20generic%20rules%20to%20simulate%20tumor%0Adevelopment--from%20pixel%20to%20cancer--using%20cellular%20automata.%20We%20then%20integrate%0Athe%20tumor%20state%20into%20the%20original%20computed%20tomography%20%28CT%29%20images%20to%20generate%0Asynthetic%20tumors%20across%20different%20organs.%20This%20tumor%20synthesis%20approach%20allows%0Afor%20sampling%20tumors%20at%20multiple%20stages%20and%20analyzing%20tumor-organ%20interaction.%0AClinically%2C%20a%20reader%20study%20involving%20three%20expert%20radiologists%20reveals%20that%20the%0Asynthetic%20tumors%20and%20their%20developing%20trajectories%20are%20convincingly%20realistic.%0ATechnically%2C%20we%20analyze%20and%20simulate%20tumor%20development%20at%20various%20stages%20using%0A9%2C262%20raw%2C%20unlabeled%20CT%20images%20sourced%20from%2068%20hospitals%20worldwide.%20The%0Aperformance%20in%20segmenting%20tumors%20in%20the%20liver%2C%20pancreas%2C%20and%20kidneys%20exceeds%0Aprevailing%20literature%20benchmarks%2C%20underlining%20the%20immense%20potential%20of%20tumor%0Asynthesis%2C%20especially%20for%20earlier%20cancer%20detection.%0A%20%20The%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/MrGiovanni/Pixel2Cancer%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06459v2&entry.124074799=Read"},
{"title": "Graph-Guided Test-Time Adaptation for Glaucoma Diagnosis using Fundus\n  Photography", "author": "Qian Zeng and Fan Zhang", "abstract": "  Glaucoma is a leading cause of irreversible blindness worldwide. While deep\nlearning approaches using fundus images have largely improved early diagnosis\nof glaucoma, variations in images from different devices and locations (known\nas domain shifts) challenge the use of pre-trained models in real-world\nsettings. To address this, we propose a novel Graph-guided Test-Time Adaptation\n(GTTA) framework to generalize glaucoma diagnosis models to unseen test\nenvironments. GTTA integrates the topological information of fundus images into\nthe model training, enhancing the model's transferability and reducing the risk\nof learning spurious correlation. During inference, GTTA introduces a novel\ntest-time training objective to make the source-trained classifier\nprogressively adapt to target patterns with reliable class conditional\nestimation and consistency regularization. Experiments on cross-domain glaucoma\ndiagnosis benchmarks demonstrate the superiority of the overall framework and\nindividual components under different backbone networks.\n", "link": "http://arxiv.org/abs/2407.04396v1", "date": "2024-07-05", "relevancy": 2.5691, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.531}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5076}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Guided%20Test-Time%20Adaptation%20for%20Glaucoma%20Diagnosis%20using%20Fundus%0A%20%20Photography&body=Title%3A%20Graph-Guided%20Test-Time%20Adaptation%20for%20Glaucoma%20Diagnosis%20using%20Fundus%0A%20%20Photography%0AAuthor%3A%20Qian%20Zeng%20and%20Fan%20Zhang%0AAbstract%3A%20%20%20Glaucoma%20is%20a%20leading%20cause%20of%20irreversible%20blindness%20worldwide.%20While%20deep%0Alearning%20approaches%20using%20fundus%20images%20have%20largely%20improved%20early%20diagnosis%0Aof%20glaucoma%2C%20variations%20in%20images%20from%20different%20devices%20and%20locations%20%28known%0Aas%20domain%20shifts%29%20challenge%20the%20use%20of%20pre-trained%20models%20in%20real-world%0Asettings.%20To%20address%20this%2C%20we%20propose%20a%20novel%20Graph-guided%20Test-Time%20Adaptation%0A%28GTTA%29%20framework%20to%20generalize%20glaucoma%20diagnosis%20models%20to%20unseen%20test%0Aenvironments.%20GTTA%20integrates%20the%20topological%20information%20of%20fundus%20images%20into%0Athe%20model%20training%2C%20enhancing%20the%20model%27s%20transferability%20and%20reducing%20the%20risk%0Aof%20learning%20spurious%20correlation.%20During%20inference%2C%20GTTA%20introduces%20a%20novel%0Atest-time%20training%20objective%20to%20make%20the%20source-trained%20classifier%0Aprogressively%20adapt%20to%20target%20patterns%20with%20reliable%20class%20conditional%0Aestimation%20and%20consistency%20regularization.%20Experiments%20on%20cross-domain%20glaucoma%0Adiagnosis%20benchmarks%20demonstrate%20the%20superiority%20of%20the%20overall%20framework%20and%0Aindividual%20components%20under%20different%20backbone%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Guided%2520Test-Time%2520Adaptation%2520for%2520Glaucoma%2520Diagnosis%2520using%2520Fundus%250A%2520%2520Photography%26entry.906535625%3DQian%2520Zeng%2520and%2520Fan%2520Zhang%26entry.1292438233%3D%2520%2520Glaucoma%2520is%2520a%2520leading%2520cause%2520of%2520irreversible%2520blindness%2520worldwide.%2520While%2520deep%250Alearning%2520approaches%2520using%2520fundus%2520images%2520have%2520largely%2520improved%2520early%2520diagnosis%250Aof%2520glaucoma%252C%2520variations%2520in%2520images%2520from%2520different%2520devices%2520and%2520locations%2520%2528known%250Aas%2520domain%2520shifts%2529%2520challenge%2520the%2520use%2520of%2520pre-trained%2520models%2520in%2520real-world%250Asettings.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520Graph-guided%2520Test-Time%2520Adaptation%250A%2528GTTA%2529%2520framework%2520to%2520generalize%2520glaucoma%2520diagnosis%2520models%2520to%2520unseen%2520test%250Aenvironments.%2520GTTA%2520integrates%2520the%2520topological%2520information%2520of%2520fundus%2520images%2520into%250Athe%2520model%2520training%252C%2520enhancing%2520the%2520model%2527s%2520transferability%2520and%2520reducing%2520the%2520risk%250Aof%2520learning%2520spurious%2520correlation.%2520During%2520inference%252C%2520GTTA%2520introduces%2520a%2520novel%250Atest-time%2520training%2520objective%2520to%2520make%2520the%2520source-trained%2520classifier%250Aprogressively%2520adapt%2520to%2520target%2520patterns%2520with%2520reliable%2520class%2520conditional%250Aestimation%2520and%2520consistency%2520regularization.%2520Experiments%2520on%2520cross-domain%2520glaucoma%250Adiagnosis%2520benchmarks%2520demonstrate%2520the%2520superiority%2520of%2520the%2520overall%2520framework%2520and%250Aindividual%2520components%2520under%2520different%2520backbone%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Guided%20Test-Time%20Adaptation%20for%20Glaucoma%20Diagnosis%20using%20Fundus%0A%20%20Photography&entry.906535625=Qian%20Zeng%20and%20Fan%20Zhang&entry.1292438233=%20%20Glaucoma%20is%20a%20leading%20cause%20of%20irreversible%20blindness%20worldwide.%20While%20deep%0Alearning%20approaches%20using%20fundus%20images%20have%20largely%20improved%20early%20diagnosis%0Aof%20glaucoma%2C%20variations%20in%20images%20from%20different%20devices%20and%20locations%20%28known%0Aas%20domain%20shifts%29%20challenge%20the%20use%20of%20pre-trained%20models%20in%20real-world%0Asettings.%20To%20address%20this%2C%20we%20propose%20a%20novel%20Graph-guided%20Test-Time%20Adaptation%0A%28GTTA%29%20framework%20to%20generalize%20glaucoma%20diagnosis%20models%20to%20unseen%20test%0Aenvironments.%20GTTA%20integrates%20the%20topological%20information%20of%20fundus%20images%20into%0Athe%20model%20training%2C%20enhancing%20the%20model%27s%20transferability%20and%20reducing%20the%20risk%0Aof%20learning%20spurious%20correlation.%20During%20inference%2C%20GTTA%20introduces%20a%20novel%0Atest-time%20training%20objective%20to%20make%20the%20source-trained%20classifier%0Aprogressively%20adapt%20to%20target%20patterns%20with%20reliable%20class%20conditional%0Aestimation%20and%20consistency%20regularization.%20Experiments%20on%20cross-domain%20glaucoma%0Adiagnosis%20benchmarks%20demonstrate%20the%20superiority%20of%20the%20overall%20framework%20and%0Aindividual%20components%20under%20different%20backbone%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04396v1&entry.124074799=Read"},
{"title": "MaTe3D: Mask-guided Text-based 3D-aware Portrait Editing", "author": "Kangneng Zhou and Daiheng Gao and Xuan Wang and Jie Zhang and Peng Zhang and Xusen Sun and Longhao Zhang and Shiqi Yang and Bang Zhang and Liefeng Bo and Yaxing Wang and Ming-Ming Cheng", "abstract": "  3D-aware portrait editing has a wide range of applications in multiple\nfields. However, current approaches are limited due that they can only perform\nmask-guided or text-based editing. Even by fusing the two procedures into a\nmodel, the editing quality and stability cannot be ensured. To address this\nlimitation, we propose \\textbf{MaTe3D}: mask-guided text-based 3D-aware\nportrait editing. In this framework, first, we introduce a new SDF-based 3D\ngenerator which learns local and global representations with proposed SDF and\ndensity consistency losses. This enhances masked-based editing in local areas;\nsecond, we present a novel distillation strategy: Conditional Distillation on\nGeometry and Texture (CDGT). Compared to exiting distillation strategies, it\nmitigates visual ambiguity and avoids mismatch between texture and geometry,\nthereby producing stable texture and convincing geometry while editing.\nAdditionally, we create the CatMask-HQ dataset, a large-scale high-resolution\ncat face annotation for exploration of model generalization and expansion. We\nperform expensive experiments on both the FFHQ and CatMask-HQ datasets to\ndemonstrate the editing quality and stability of the proposed method. Our\nmethod faithfully generates a 3D-aware edited face image based on a modified\nmask and a text prompt. Our code and models will be publicly released.\n", "link": "http://arxiv.org/abs/2312.06947v4", "date": "2024-07-05", "relevancy": 2.5623, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6427}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6427}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaTe3D%3A%20Mask-guided%20Text-based%203D-aware%20Portrait%20Editing&body=Title%3A%20MaTe3D%3A%20Mask-guided%20Text-based%203D-aware%20Portrait%20Editing%0AAuthor%3A%20Kangneng%20Zhou%20and%20Daiheng%20Gao%20and%20Xuan%20Wang%20and%20Jie%20Zhang%20and%20Peng%20Zhang%20and%20Xusen%20Sun%20and%20Longhao%20Zhang%20and%20Shiqi%20Yang%20and%20Bang%20Zhang%20and%20Liefeng%20Bo%20and%20Yaxing%20Wang%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20%20%203D-aware%20portrait%20editing%20has%20a%20wide%20range%20of%20applications%20in%20multiple%0Afields.%20However%2C%20current%20approaches%20are%20limited%20due%20that%20they%20can%20only%20perform%0Amask-guided%20or%20text-based%20editing.%20Even%20by%20fusing%20the%20two%20procedures%20into%20a%0Amodel%2C%20the%20editing%20quality%20and%20stability%20cannot%20be%20ensured.%20To%20address%20this%0Alimitation%2C%20we%20propose%20%5Ctextbf%7BMaTe3D%7D%3A%20mask-guided%20text-based%203D-aware%0Aportrait%20editing.%20In%20this%20framework%2C%20first%2C%20we%20introduce%20a%20new%20SDF-based%203D%0Agenerator%20which%20learns%20local%20and%20global%20representations%20with%20proposed%20SDF%20and%0Adensity%20consistency%20losses.%20This%20enhances%20masked-based%20editing%20in%20local%20areas%3B%0Asecond%2C%20we%20present%20a%20novel%20distillation%20strategy%3A%20Conditional%20Distillation%20on%0AGeometry%20and%20Texture%20%28CDGT%29.%20Compared%20to%20exiting%20distillation%20strategies%2C%20it%0Amitigates%20visual%20ambiguity%20and%20avoids%20mismatch%20between%20texture%20and%20geometry%2C%0Athereby%20producing%20stable%20texture%20and%20convincing%20geometry%20while%20editing.%0AAdditionally%2C%20we%20create%20the%20CatMask-HQ%20dataset%2C%20a%20large-scale%20high-resolution%0Acat%20face%20annotation%20for%20exploration%20of%20model%20generalization%20and%20expansion.%20We%0Aperform%20expensive%20experiments%20on%20both%20the%20FFHQ%20and%20CatMask-HQ%20datasets%20to%0Ademonstrate%20the%20editing%20quality%20and%20stability%20of%20the%20proposed%20method.%20Our%0Amethod%20faithfully%20generates%20a%203D-aware%20edited%20face%20image%20based%20on%20a%20modified%0Amask%20and%20a%20text%20prompt.%20Our%20code%20and%20models%20will%20be%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06947v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaTe3D%253A%2520Mask-guided%2520Text-based%25203D-aware%2520Portrait%2520Editing%26entry.906535625%3DKangneng%2520Zhou%2520and%2520Daiheng%2520Gao%2520and%2520Xuan%2520Wang%2520and%2520Jie%2520Zhang%2520and%2520Peng%2520Zhang%2520and%2520Xusen%2520Sun%2520and%2520Longhao%2520Zhang%2520and%2520Shiqi%2520Yang%2520and%2520Bang%2520Zhang%2520and%2520Liefeng%2520Bo%2520and%2520Yaxing%2520Wang%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3D%2520%25203D-aware%2520portrait%2520editing%2520has%2520a%2520wide%2520range%2520of%2520applications%2520in%2520multiple%250Afields.%2520However%252C%2520current%2520approaches%2520are%2520limited%2520due%2520that%2520they%2520can%2520only%2520perform%250Amask-guided%2520or%2520text-based%2520editing.%2520Even%2520by%2520fusing%2520the%2520two%2520procedures%2520into%2520a%250Amodel%252C%2520the%2520editing%2520quality%2520and%2520stability%2520cannot%2520be%2520ensured.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520%255Ctextbf%257BMaTe3D%257D%253A%2520mask-guided%2520text-based%25203D-aware%250Aportrait%2520editing.%2520In%2520this%2520framework%252C%2520first%252C%2520we%2520introduce%2520a%2520new%2520SDF-based%25203D%250Agenerator%2520which%2520learns%2520local%2520and%2520global%2520representations%2520with%2520proposed%2520SDF%2520and%250Adensity%2520consistency%2520losses.%2520This%2520enhances%2520masked-based%2520editing%2520in%2520local%2520areas%253B%250Asecond%252C%2520we%2520present%2520a%2520novel%2520distillation%2520strategy%253A%2520Conditional%2520Distillation%2520on%250AGeometry%2520and%2520Texture%2520%2528CDGT%2529.%2520Compared%2520to%2520exiting%2520distillation%2520strategies%252C%2520it%250Amitigates%2520visual%2520ambiguity%2520and%2520avoids%2520mismatch%2520between%2520texture%2520and%2520geometry%252C%250Athereby%2520producing%2520stable%2520texture%2520and%2520convincing%2520geometry%2520while%2520editing.%250AAdditionally%252C%2520we%2520create%2520the%2520CatMask-HQ%2520dataset%252C%2520a%2520large-scale%2520high-resolution%250Acat%2520face%2520annotation%2520for%2520exploration%2520of%2520model%2520generalization%2520and%2520expansion.%2520We%250Aperform%2520expensive%2520experiments%2520on%2520both%2520the%2520FFHQ%2520and%2520CatMask-HQ%2520datasets%2520to%250Ademonstrate%2520the%2520editing%2520quality%2520and%2520stability%2520of%2520the%2520proposed%2520method.%2520Our%250Amethod%2520faithfully%2520generates%2520a%25203D-aware%2520edited%2520face%2520image%2520based%2520on%2520a%2520modified%250Amask%2520and%2520a%2520text%2520prompt.%2520Our%2520code%2520and%2520models%2520will%2520be%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06947v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaTe3D%3A%20Mask-guided%20Text-based%203D-aware%20Portrait%20Editing&entry.906535625=Kangneng%20Zhou%20and%20Daiheng%20Gao%20and%20Xuan%20Wang%20and%20Jie%20Zhang%20and%20Peng%20Zhang%20and%20Xusen%20Sun%20and%20Longhao%20Zhang%20and%20Shiqi%20Yang%20and%20Bang%20Zhang%20and%20Liefeng%20Bo%20and%20Yaxing%20Wang%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%203D-aware%20portrait%20editing%20has%20a%20wide%20range%20of%20applications%20in%20multiple%0Afields.%20However%2C%20current%20approaches%20are%20limited%20due%20that%20they%20can%20only%20perform%0Amask-guided%20or%20text-based%20editing.%20Even%20by%20fusing%20the%20two%20procedures%20into%20a%0Amodel%2C%20the%20editing%20quality%20and%20stability%20cannot%20be%20ensured.%20To%20address%20this%0Alimitation%2C%20we%20propose%20%5Ctextbf%7BMaTe3D%7D%3A%20mask-guided%20text-based%203D-aware%0Aportrait%20editing.%20In%20this%20framework%2C%20first%2C%20we%20introduce%20a%20new%20SDF-based%203D%0Agenerator%20which%20learns%20local%20and%20global%20representations%20with%20proposed%20SDF%20and%0Adensity%20consistency%20losses.%20This%20enhances%20masked-based%20editing%20in%20local%20areas%3B%0Asecond%2C%20we%20present%20a%20novel%20distillation%20strategy%3A%20Conditional%20Distillation%20on%0AGeometry%20and%20Texture%20%28CDGT%29.%20Compared%20to%20exiting%20distillation%20strategies%2C%20it%0Amitigates%20visual%20ambiguity%20and%20avoids%20mismatch%20between%20texture%20and%20geometry%2C%0Athereby%20producing%20stable%20texture%20and%20convincing%20geometry%20while%20editing.%0AAdditionally%2C%20we%20create%20the%20CatMask-HQ%20dataset%2C%20a%20large-scale%20high-resolution%0Acat%20face%20annotation%20for%20exploration%20of%20model%20generalization%20and%20expansion.%20We%0Aperform%20expensive%20experiments%20on%20both%20the%20FFHQ%20and%20CatMask-HQ%20datasets%20to%0Ademonstrate%20the%20editing%20quality%20and%20stability%20of%20the%20proposed%20method.%20Our%0Amethod%20faithfully%20generates%20a%203D-aware%20edited%20face%20image%20based%20on%20a%20modified%0Amask%20and%20a%20text%20prompt.%20Our%20code%20and%20models%20will%20be%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06947v4&entry.124074799=Read"},
{"title": "FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer", "author": "Pavol Harar and Lukas Herrmann and Philipp Grohs and David Haselbach", "abstract": "  In cryo-electron microscopy, accurate particle localization and\nclassification are imperative. Recent deep learning solutions, though\nsuccessful, require extensive training data sets. The protracted generation\ntime of physics-based models, often employed to produce these data sets, limits\ntheir broad applicability. We introduce FakET, a method based on Neural Style\nTransfer, capable of simulating the forward operator of any cryo transmission\nelectron microscope. It can be used to adapt a synthetic training data set\naccording to reference data producing high-quality simulated micrographs or\ntilt-series. To assess the quality of our generated data, we used it to train a\nstate-of-the-art localization and classification architecture and compared its\nperformance with a counterpart trained on benchmark data. Remarkably, our\ntechnique matches the performance, boosts data generation speed 750 times, uses\n33 times less memory, and scales well to typical transmission electron\nmicroscope detector sizes. It leverages GPU acceleration and parallel\nprocessing. The source code is available at https://github.com/paloha/faket.\n", "link": "http://arxiv.org/abs/2304.02011v3", "date": "2024-07-05", "relevancy": 2.5546, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5256}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5036}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FakET%3A%20Simulating%20Cryo-Electron%20Tomograms%20with%20Neural%20Style%20Transfer&body=Title%3A%20FakET%3A%20Simulating%20Cryo-Electron%20Tomograms%20with%20Neural%20Style%20Transfer%0AAuthor%3A%20Pavol%20Harar%20and%20Lukas%20Herrmann%20and%20Philipp%20Grohs%20and%20David%20Haselbach%0AAbstract%3A%20%20%20In%20cryo-electron%20microscopy%2C%20accurate%20particle%20localization%20and%0Aclassification%20are%20imperative.%20Recent%20deep%20learning%20solutions%2C%20though%0Asuccessful%2C%20require%20extensive%20training%20data%20sets.%20The%20protracted%20generation%0Atime%20of%20physics-based%20models%2C%20often%20employed%20to%20produce%20these%20data%20sets%2C%20limits%0Atheir%20broad%20applicability.%20We%20introduce%20FakET%2C%20a%20method%20based%20on%20Neural%20Style%0ATransfer%2C%20capable%20of%20simulating%20the%20forward%20operator%20of%20any%20cryo%20transmission%0Aelectron%20microscope.%20It%20can%20be%20used%20to%20adapt%20a%20synthetic%20training%20data%20set%0Aaccording%20to%20reference%20data%20producing%20high-quality%20simulated%20micrographs%20or%0Atilt-series.%20To%20assess%20the%20quality%20of%20our%20generated%20data%2C%20we%20used%20it%20to%20train%20a%0Astate-of-the-art%20localization%20and%20classification%20architecture%20and%20compared%20its%0Aperformance%20with%20a%20counterpart%20trained%20on%20benchmark%20data.%20Remarkably%2C%20our%0Atechnique%20matches%20the%20performance%2C%20boosts%20data%20generation%20speed%20750%20times%2C%20uses%0A33%20times%20less%20memory%2C%20and%20scales%20well%20to%20typical%20transmission%20electron%0Amicroscope%20detector%20sizes.%20It%20leverages%20GPU%20acceleration%20and%20parallel%0Aprocessing.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/paloha/faket.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02011v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFakET%253A%2520Simulating%2520Cryo-Electron%2520Tomograms%2520with%2520Neural%2520Style%2520Transfer%26entry.906535625%3DPavol%2520Harar%2520and%2520Lukas%2520Herrmann%2520and%2520Philipp%2520Grohs%2520and%2520David%2520Haselbach%26entry.1292438233%3D%2520%2520In%2520cryo-electron%2520microscopy%252C%2520accurate%2520particle%2520localization%2520and%250Aclassification%2520are%2520imperative.%2520Recent%2520deep%2520learning%2520solutions%252C%2520though%250Asuccessful%252C%2520require%2520extensive%2520training%2520data%2520sets.%2520The%2520protracted%2520generation%250Atime%2520of%2520physics-based%2520models%252C%2520often%2520employed%2520to%2520produce%2520these%2520data%2520sets%252C%2520limits%250Atheir%2520broad%2520applicability.%2520We%2520introduce%2520FakET%252C%2520a%2520method%2520based%2520on%2520Neural%2520Style%250ATransfer%252C%2520capable%2520of%2520simulating%2520the%2520forward%2520operator%2520of%2520any%2520cryo%2520transmission%250Aelectron%2520microscope.%2520It%2520can%2520be%2520used%2520to%2520adapt%2520a%2520synthetic%2520training%2520data%2520set%250Aaccording%2520to%2520reference%2520data%2520producing%2520high-quality%2520simulated%2520micrographs%2520or%250Atilt-series.%2520To%2520assess%2520the%2520quality%2520of%2520our%2520generated%2520data%252C%2520we%2520used%2520it%2520to%2520train%2520a%250Astate-of-the-art%2520localization%2520and%2520classification%2520architecture%2520and%2520compared%2520its%250Aperformance%2520with%2520a%2520counterpart%2520trained%2520on%2520benchmark%2520data.%2520Remarkably%252C%2520our%250Atechnique%2520matches%2520the%2520performance%252C%2520boosts%2520data%2520generation%2520speed%2520750%2520times%252C%2520uses%250A33%2520times%2520less%2520memory%252C%2520and%2520scales%2520well%2520to%2520typical%2520transmission%2520electron%250Amicroscope%2520detector%2520sizes.%2520It%2520leverages%2520GPU%2520acceleration%2520and%2520parallel%250Aprocessing.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/paloha/faket.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.02011v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FakET%3A%20Simulating%20Cryo-Electron%20Tomograms%20with%20Neural%20Style%20Transfer&entry.906535625=Pavol%20Harar%20and%20Lukas%20Herrmann%20and%20Philipp%20Grohs%20and%20David%20Haselbach&entry.1292438233=%20%20In%20cryo-electron%20microscopy%2C%20accurate%20particle%20localization%20and%0Aclassification%20are%20imperative.%20Recent%20deep%20learning%20solutions%2C%20though%0Asuccessful%2C%20require%20extensive%20training%20data%20sets.%20The%20protracted%20generation%0Atime%20of%20physics-based%20models%2C%20often%20employed%20to%20produce%20these%20data%20sets%2C%20limits%0Atheir%20broad%20applicability.%20We%20introduce%20FakET%2C%20a%20method%20based%20on%20Neural%20Style%0ATransfer%2C%20capable%20of%20simulating%20the%20forward%20operator%20of%20any%20cryo%20transmission%0Aelectron%20microscope.%20It%20can%20be%20used%20to%20adapt%20a%20synthetic%20training%20data%20set%0Aaccording%20to%20reference%20data%20producing%20high-quality%20simulated%20micrographs%20or%0Atilt-series.%20To%20assess%20the%20quality%20of%20our%20generated%20data%2C%20we%20used%20it%20to%20train%20a%0Astate-of-the-art%20localization%20and%20classification%20architecture%20and%20compared%20its%0Aperformance%20with%20a%20counterpart%20trained%20on%20benchmark%20data.%20Remarkably%2C%20our%0Atechnique%20matches%20the%20performance%2C%20boosts%20data%20generation%20speed%20750%20times%2C%20uses%0A33%20times%20less%20memory%2C%20and%20scales%20well%20to%20typical%20transmission%20electron%0Amicroscope%20detector%20sizes.%20It%20leverages%20GPU%20acceleration%20and%20parallel%0Aprocessing.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/paloha/faket.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02011v3&entry.124074799=Read"},
{"title": "Conceptual Codebook Learning for Vision-Language Models", "author": "Yi Zhang and Ke Yu and Siqi Wu and Zhihai He", "abstract": "  In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe.\n", "link": "http://arxiv.org/abs/2407.02350v2", "date": "2024-07-05", "relevancy": 2.5533, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5228}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5125}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conceptual%20Codebook%20Learning%20for%20Vision-Language%20Models&body=Title%3A%20Conceptual%20Codebook%20Learning%20for%20Vision-Language%20Models%0AAuthor%3A%20Yi%20Zhang%20and%20Ke%20Yu%20and%20Siqi%20Wu%20and%20Zhihai%20He%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Conceptual%20Codebook%20Learning%20%28CoCoLe%29%2C%20a%20novel%0Afine-tuning%20method%20for%20vision-language%20models%20%28VLMs%29%20to%20address%20the%20challenge%0Aof%20improving%20the%20generalization%20capability%20of%20VLMs%20while%20fine-tuning%20them%20on%0Adownstream%20tasks%20in%20a%20few-shot%20setting.%20We%20recognize%20that%20visual%20concepts%2C%20such%0Aas%20textures%2C%20shapes%2C%20and%20colors%20are%20naturally%20transferable%20across%20domains%20and%0Aplay%20a%20crucial%20role%20in%20generalization%20tasks.%20Motivated%20by%20this%20interesting%0Afinding%2C%20we%20learn%20a%20conceptual%20codebook%20consisting%20of%20visual%20concepts%20as%20keys%0Aand%20conceptual%20prompts%20as%20values%2C%20which%20serves%20as%20a%20link%20between%20the%20image%0Aencoder%27s%20outputs%20and%20the%20text%20encoder%27s%20inputs.%20Specifically%2C%20for%20a%20given%0Aimage%2C%20we%20leverage%20the%20codebook%20to%20identify%20the%20most%20relevant%20conceptual%0Aprompts%20associated%20with%20the%20class%20embeddings%20to%20perform%20the%20classification.%0AAdditionally%2C%20we%20incorporate%20a%20handcrafted%20concept%20cache%20as%20a%20regularization%20to%0Aalleviate%20the%20overfitting%20issues%20in%20low-shot%20scenarios.%20We%20observe%20that%20this%0Aconceptual%20codebook%20learning%20method%20is%20able%20to%20achieve%20enhanced%20alignment%0Abetween%20visual%20and%20linguistic%20modalities.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20CoCoLe%20method%20remarkably%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20across%20various%20evaluation%20settings%2C%20including%0Abase-to-new%20generalization%2C%20cross-dataset%20evaluation%2C%20and%20domain%20generalization%0Atasks.%20Detailed%20ablation%20studies%20further%20confirm%20the%20efficacy%20of%20each%20component%0Ain%20CoCoLe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02350v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptual%2520Codebook%2520Learning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DYi%2520Zhang%2520and%2520Ke%2520Yu%2520and%2520Siqi%2520Wu%2520and%2520Zhihai%2520He%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Conceptual%2520Codebook%2520Learning%2520%2528CoCoLe%2529%252C%2520a%2520novel%250Afine-tuning%2520method%2520for%2520vision-language%2520models%2520%2528VLMs%2529%2520to%2520address%2520the%2520challenge%250Aof%2520improving%2520the%2520generalization%2520capability%2520of%2520VLMs%2520while%2520fine-tuning%2520them%2520on%250Adownstream%2520tasks%2520in%2520a%2520few-shot%2520setting.%2520We%2520recognize%2520that%2520visual%2520concepts%252C%2520such%250Aas%2520textures%252C%2520shapes%252C%2520and%2520colors%2520are%2520naturally%2520transferable%2520across%2520domains%2520and%250Aplay%2520a%2520crucial%2520role%2520in%2520generalization%2520tasks.%2520Motivated%2520by%2520this%2520interesting%250Afinding%252C%2520we%2520learn%2520a%2520conceptual%2520codebook%2520consisting%2520of%2520visual%2520concepts%2520as%2520keys%250Aand%2520conceptual%2520prompts%2520as%2520values%252C%2520which%2520serves%2520as%2520a%2520link%2520between%2520the%2520image%250Aencoder%2527s%2520outputs%2520and%2520the%2520text%2520encoder%2527s%2520inputs.%2520Specifically%252C%2520for%2520a%2520given%250Aimage%252C%2520we%2520leverage%2520the%2520codebook%2520to%2520identify%2520the%2520most%2520relevant%2520conceptual%250Aprompts%2520associated%2520with%2520the%2520class%2520embeddings%2520to%2520perform%2520the%2520classification.%250AAdditionally%252C%2520we%2520incorporate%2520a%2520handcrafted%2520concept%2520cache%2520as%2520a%2520regularization%2520to%250Aalleviate%2520the%2520overfitting%2520issues%2520in%2520low-shot%2520scenarios.%2520We%2520observe%2520that%2520this%250Aconceptual%2520codebook%2520learning%2520method%2520is%2520able%2520to%2520achieve%2520enhanced%2520alignment%250Abetween%2520visual%2520and%2520linguistic%2520modalities.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520our%2520CoCoLe%2520method%2520remarkably%2520outperforms%2520the%2520existing%250Astate-of-the-art%2520methods%2520across%2520various%2520evaluation%2520settings%252C%2520including%250Abase-to-new%2520generalization%252C%2520cross-dataset%2520evaluation%252C%2520and%2520domain%2520generalization%250Atasks.%2520Detailed%2520ablation%2520studies%2520further%2520confirm%2520the%2520efficacy%2520of%2520each%2520component%250Ain%2520CoCoLe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02350v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conceptual%20Codebook%20Learning%20for%20Vision-Language%20Models&entry.906535625=Yi%20Zhang%20and%20Ke%20Yu%20and%20Siqi%20Wu%20and%20Zhihai%20He&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Conceptual%20Codebook%20Learning%20%28CoCoLe%29%2C%20a%20novel%0Afine-tuning%20method%20for%20vision-language%20models%20%28VLMs%29%20to%20address%20the%20challenge%0Aof%20improving%20the%20generalization%20capability%20of%20VLMs%20while%20fine-tuning%20them%20on%0Adownstream%20tasks%20in%20a%20few-shot%20setting.%20We%20recognize%20that%20visual%20concepts%2C%20such%0Aas%20textures%2C%20shapes%2C%20and%20colors%20are%20naturally%20transferable%20across%20domains%20and%0Aplay%20a%20crucial%20role%20in%20generalization%20tasks.%20Motivated%20by%20this%20interesting%0Afinding%2C%20we%20learn%20a%20conceptual%20codebook%20consisting%20of%20visual%20concepts%20as%20keys%0Aand%20conceptual%20prompts%20as%20values%2C%20which%20serves%20as%20a%20link%20between%20the%20image%0Aencoder%27s%20outputs%20and%20the%20text%20encoder%27s%20inputs.%20Specifically%2C%20for%20a%20given%0Aimage%2C%20we%20leverage%20the%20codebook%20to%20identify%20the%20most%20relevant%20conceptual%0Aprompts%20associated%20with%20the%20class%20embeddings%20to%20perform%20the%20classification.%0AAdditionally%2C%20we%20incorporate%20a%20handcrafted%20concept%20cache%20as%20a%20regularization%20to%0Aalleviate%20the%20overfitting%20issues%20in%20low-shot%20scenarios.%20We%20observe%20that%20this%0Aconceptual%20codebook%20learning%20method%20is%20able%20to%20achieve%20enhanced%20alignment%0Abetween%20visual%20and%20linguistic%20modalities.%20Extensive%20experimental%20results%0Ademonstrate%20that%20our%20CoCoLe%20method%20remarkably%20outperforms%20the%20existing%0Astate-of-the-art%20methods%20across%20various%20evaluation%20settings%2C%20including%0Abase-to-new%20generalization%2C%20cross-dataset%20evaluation%2C%20and%20domain%20generalization%0Atasks.%20Detailed%20ablation%20studies%20further%20confirm%20the%20efficacy%20of%20each%20component%0Ain%20CoCoLe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02350v2&entry.124074799=Read"},
{"title": "Multi-modal Masked Siamese Network Improves Chest X-Ray Representation\n  Learning", "author": "Saeed Shurrab and Alejandro Guerra-Manzanares and Farah E. Shamout", "abstract": "  Self-supervised learning methods for medical images primarily rely on the\nimaging modality during pretraining. While such approaches deliver promising\nresults, they do not leverage associated patient or scan information collected\nwithin Electronic Health Records (EHR). Here, we propose to incorporate EHR\ndata during self-supervised pretraining with a Masked Siamese Network (MSN) to\nenhance the quality of chest X-ray representations. We investigate three types\nof EHR data, including demographic, scan metadata, and inpatient stay\ninformation. We evaluate our approach on three publicly available chest X-ray\ndatasets, MIMIC-CXR, CheXpert, and NIH-14, using two vision transformer (ViT)\nbackbones, specifically ViT-Tiny and ViT-Small. In assessing the quality of the\nrepresentations via linear evaluation, our proposed method demonstrates\nsignificant improvement compared to vanilla MSN and state-of-the-art\nself-supervised learning baselines. Our work highlights the potential of\nEHR-enhanced self-supervised pre-training for medical imaging. The code is\npublicly available at: https://github.com/nyuad-cai/CXR-EHR-MSN\n", "link": "http://arxiv.org/abs/2407.04449v1", "date": "2024-07-05", "relevancy": 2.5458, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.495}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-modal%20Masked%20Siamese%20Network%20Improves%20Chest%20X-Ray%20Representation%0A%20%20Learning&body=Title%3A%20Multi-modal%20Masked%20Siamese%20Network%20Improves%20Chest%20X-Ray%20Representation%0A%20%20Learning%0AAuthor%3A%20Saeed%20Shurrab%20and%20Alejandro%20Guerra-Manzanares%20and%20Farah%20E.%20Shamout%0AAbstract%3A%20%20%20Self-supervised%20learning%20methods%20for%20medical%20images%20primarily%20rely%20on%20the%0Aimaging%20modality%20during%20pretraining.%20While%20such%20approaches%20deliver%20promising%0Aresults%2C%20they%20do%20not%20leverage%20associated%20patient%20or%20scan%20information%20collected%0Awithin%20Electronic%20Health%20Records%20%28EHR%29.%20Here%2C%20we%20propose%20to%20incorporate%20EHR%0Adata%20during%20self-supervised%20pretraining%20with%20a%20Masked%20Siamese%20Network%20%28MSN%29%20to%0Aenhance%20the%20quality%20of%20chest%20X-ray%20representations.%20We%20investigate%20three%20types%0Aof%20EHR%20data%2C%20including%20demographic%2C%20scan%20metadata%2C%20and%20inpatient%20stay%0Ainformation.%20We%20evaluate%20our%20approach%20on%20three%20publicly%20available%20chest%20X-ray%0Adatasets%2C%20MIMIC-CXR%2C%20CheXpert%2C%20and%20NIH-14%2C%20using%20two%20vision%20transformer%20%28ViT%29%0Abackbones%2C%20specifically%20ViT-Tiny%20and%20ViT-Small.%20In%20assessing%20the%20quality%20of%20the%0Arepresentations%20via%20linear%20evaluation%2C%20our%20proposed%20method%20demonstrates%0Asignificant%20improvement%20compared%20to%20vanilla%20MSN%20and%20state-of-the-art%0Aself-supervised%20learning%20baselines.%20Our%20work%20highlights%20the%20potential%20of%0AEHR-enhanced%20self-supervised%20pre-training%20for%20medical%20imaging.%20The%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/nyuad-cai/CXR-EHR-MSN%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-modal%2520Masked%2520Siamese%2520Network%2520Improves%2520Chest%2520X-Ray%2520Representation%250A%2520%2520Learning%26entry.906535625%3DSaeed%2520Shurrab%2520and%2520Alejandro%2520Guerra-Manzanares%2520and%2520Farah%2520E.%2520Shamout%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520methods%2520for%2520medical%2520images%2520primarily%2520rely%2520on%2520the%250Aimaging%2520modality%2520during%2520pretraining.%2520While%2520such%2520approaches%2520deliver%2520promising%250Aresults%252C%2520they%2520do%2520not%2520leverage%2520associated%2520patient%2520or%2520scan%2520information%2520collected%250Awithin%2520Electronic%2520Health%2520Records%2520%2528EHR%2529.%2520Here%252C%2520we%2520propose%2520to%2520incorporate%2520EHR%250Adata%2520during%2520self-supervised%2520pretraining%2520with%2520a%2520Masked%2520Siamese%2520Network%2520%2528MSN%2529%2520to%250Aenhance%2520the%2520quality%2520of%2520chest%2520X-ray%2520representations.%2520We%2520investigate%2520three%2520types%250Aof%2520EHR%2520data%252C%2520including%2520demographic%252C%2520scan%2520metadata%252C%2520and%2520inpatient%2520stay%250Ainformation.%2520We%2520evaluate%2520our%2520approach%2520on%2520three%2520publicly%2520available%2520chest%2520X-ray%250Adatasets%252C%2520MIMIC-CXR%252C%2520CheXpert%252C%2520and%2520NIH-14%252C%2520using%2520two%2520vision%2520transformer%2520%2528ViT%2529%250Abackbones%252C%2520specifically%2520ViT-Tiny%2520and%2520ViT-Small.%2520In%2520assessing%2520the%2520quality%2520of%2520the%250Arepresentations%2520via%2520linear%2520evaluation%252C%2520our%2520proposed%2520method%2520demonstrates%250Asignificant%2520improvement%2520compared%2520to%2520vanilla%2520MSN%2520and%2520state-of-the-art%250Aself-supervised%2520learning%2520baselines.%2520Our%2520work%2520highlights%2520the%2520potential%2520of%250AEHR-enhanced%2520self-supervised%2520pre-training%2520for%2520medical%2520imaging.%2520The%2520code%2520is%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/nyuad-cai/CXR-EHR-MSN%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-modal%20Masked%20Siamese%20Network%20Improves%20Chest%20X-Ray%20Representation%0A%20%20Learning&entry.906535625=Saeed%20Shurrab%20and%20Alejandro%20Guerra-Manzanares%20and%20Farah%20E.%20Shamout&entry.1292438233=%20%20Self-supervised%20learning%20methods%20for%20medical%20images%20primarily%20rely%20on%20the%0Aimaging%20modality%20during%20pretraining.%20While%20such%20approaches%20deliver%20promising%0Aresults%2C%20they%20do%20not%20leverage%20associated%20patient%20or%20scan%20information%20collected%0Awithin%20Electronic%20Health%20Records%20%28EHR%29.%20Here%2C%20we%20propose%20to%20incorporate%20EHR%0Adata%20during%20self-supervised%20pretraining%20with%20a%20Masked%20Siamese%20Network%20%28MSN%29%20to%0Aenhance%20the%20quality%20of%20chest%20X-ray%20representations.%20We%20investigate%20three%20types%0Aof%20EHR%20data%2C%20including%20demographic%2C%20scan%20metadata%2C%20and%20inpatient%20stay%0Ainformation.%20We%20evaluate%20our%20approach%20on%20three%20publicly%20available%20chest%20X-ray%0Adatasets%2C%20MIMIC-CXR%2C%20CheXpert%2C%20and%20NIH-14%2C%20using%20two%20vision%20transformer%20%28ViT%29%0Abackbones%2C%20specifically%20ViT-Tiny%20and%20ViT-Small.%20In%20assessing%20the%20quality%20of%20the%0Arepresentations%20via%20linear%20evaluation%2C%20our%20proposed%20method%20demonstrates%0Asignificant%20improvement%20compared%20to%20vanilla%20MSN%20and%20state-of-the-art%0Aself-supervised%20learning%20baselines.%20Our%20work%20highlights%20the%20potential%20of%0AEHR-enhanced%20self-supervised%20pre-training%20for%20medical%20imaging.%20The%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/nyuad-cai/CXR-EHR-MSN%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04449v1&entry.124074799=Read"},
{"title": "Semantically Rich Local Dataset Generation for Explainable AI in\n  Genomics", "author": "Pedro Barbosa and Rosina Savisaar and Alcides Fonseca", "abstract": "  Black box deep learning models trained on genomic sequences excel at\npredicting the outcomes of different gene regulatory mechanisms. Therefore,\ninterpreting these models may provide novel insights into the underlying\nbiology, supporting downstream biomedical applications. Due to their\ncomplexity, interpretable surrogate models can only be built for local\nexplanations (e.g., a single instance). However, accomplishing this requires\ngenerating a dataset in the neighborhood of the input, which must maintain\nsyntactic similarity to the original data while introducing semantic\nvariability in the model's predictions. This task is challenging due to the\ncomplex sequence-to-function relationship of DNA.\n  We propose using Genetic Programming to generate datasets by evolving\nperturbations in sequences that contribute to their semantic diversity. Our\ncustom, domain-guided individual representation effectively constrains\nsyntactic similarity, and we provide two alternative fitness functions that\npromote diversity with no computational effort. Applied to the RNA splicing\ndomain, our approach quickly achieves good diversity and significantly\noutperforms a random baseline in exploring the search space, as shown by our\nproof-of-concept, short RNA sequence. Furthermore, we assess its\ngeneralizability and demonstrate scalability to larger sequences, resulting in\na ~30% improvement over the baseline.\n", "link": "http://arxiv.org/abs/2407.02984v2", "date": "2024-07-05", "relevancy": 2.5365, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5236}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4994}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantically%20Rich%20Local%20Dataset%20Generation%20for%20Explainable%20AI%20in%0A%20%20Genomics&body=Title%3A%20Semantically%20Rich%20Local%20Dataset%20Generation%20for%20Explainable%20AI%20in%0A%20%20Genomics%0AAuthor%3A%20Pedro%20Barbosa%20and%20Rosina%20Savisaar%20and%20Alcides%20Fonseca%0AAbstract%3A%20%20%20Black%20box%20deep%20learning%20models%20trained%20on%20genomic%20sequences%20excel%20at%0Apredicting%20the%20outcomes%20of%20different%20gene%20regulatory%20mechanisms.%20Therefore%2C%0Ainterpreting%20these%20models%20may%20provide%20novel%20insights%20into%20the%20underlying%0Abiology%2C%20supporting%20downstream%20biomedical%20applications.%20Due%20to%20their%0Acomplexity%2C%20interpretable%20surrogate%20models%20can%20only%20be%20built%20for%20local%0Aexplanations%20%28e.g.%2C%20a%20single%20instance%29.%20However%2C%20accomplishing%20this%20requires%0Agenerating%20a%20dataset%20in%20the%20neighborhood%20of%20the%20input%2C%20which%20must%20maintain%0Asyntactic%20similarity%20to%20the%20original%20data%20while%20introducing%20semantic%0Avariability%20in%20the%20model%27s%20predictions.%20This%20task%20is%20challenging%20due%20to%20the%0Acomplex%20sequence-to-function%20relationship%20of%20DNA.%0A%20%20We%20propose%20using%20Genetic%20Programming%20to%20generate%20datasets%20by%20evolving%0Aperturbations%20in%20sequences%20that%20contribute%20to%20their%20semantic%20diversity.%20Our%0Acustom%2C%20domain-guided%20individual%20representation%20effectively%20constrains%0Asyntactic%20similarity%2C%20and%20we%20provide%20two%20alternative%20fitness%20functions%20that%0Apromote%20diversity%20with%20no%20computational%20effort.%20Applied%20to%20the%20RNA%20splicing%0Adomain%2C%20our%20approach%20quickly%20achieves%20good%20diversity%20and%20significantly%0Aoutperforms%20a%20random%20baseline%20in%20exploring%20the%20search%20space%2C%20as%20shown%20by%20our%0Aproof-of-concept%2C%20short%20RNA%20sequence.%20Furthermore%2C%20we%20assess%20its%0Ageneralizability%20and%20demonstrate%20scalability%20to%20larger%20sequences%2C%20resulting%20in%0Aa%20~30%25%20improvement%20over%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02984v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantically%2520Rich%2520Local%2520Dataset%2520Generation%2520for%2520Explainable%2520AI%2520in%250A%2520%2520Genomics%26entry.906535625%3DPedro%2520Barbosa%2520and%2520Rosina%2520Savisaar%2520and%2520Alcides%2520Fonseca%26entry.1292438233%3D%2520%2520Black%2520box%2520deep%2520learning%2520models%2520trained%2520on%2520genomic%2520sequences%2520excel%2520at%250Apredicting%2520the%2520outcomes%2520of%2520different%2520gene%2520regulatory%2520mechanisms.%2520Therefore%252C%250Ainterpreting%2520these%2520models%2520may%2520provide%2520novel%2520insights%2520into%2520the%2520underlying%250Abiology%252C%2520supporting%2520downstream%2520biomedical%2520applications.%2520Due%2520to%2520their%250Acomplexity%252C%2520interpretable%2520surrogate%2520models%2520can%2520only%2520be%2520built%2520for%2520local%250Aexplanations%2520%2528e.g.%252C%2520a%2520single%2520instance%2529.%2520However%252C%2520accomplishing%2520this%2520requires%250Agenerating%2520a%2520dataset%2520in%2520the%2520neighborhood%2520of%2520the%2520input%252C%2520which%2520must%2520maintain%250Asyntactic%2520similarity%2520to%2520the%2520original%2520data%2520while%2520introducing%2520semantic%250Avariability%2520in%2520the%2520model%2527s%2520predictions.%2520This%2520task%2520is%2520challenging%2520due%2520to%2520the%250Acomplex%2520sequence-to-function%2520relationship%2520of%2520DNA.%250A%2520%2520We%2520propose%2520using%2520Genetic%2520Programming%2520to%2520generate%2520datasets%2520by%2520evolving%250Aperturbations%2520in%2520sequences%2520that%2520contribute%2520to%2520their%2520semantic%2520diversity.%2520Our%250Acustom%252C%2520domain-guided%2520individual%2520representation%2520effectively%2520constrains%250Asyntactic%2520similarity%252C%2520and%2520we%2520provide%2520two%2520alternative%2520fitness%2520functions%2520that%250Apromote%2520diversity%2520with%2520no%2520computational%2520effort.%2520Applied%2520to%2520the%2520RNA%2520splicing%250Adomain%252C%2520our%2520approach%2520quickly%2520achieves%2520good%2520diversity%2520and%2520significantly%250Aoutperforms%2520a%2520random%2520baseline%2520in%2520exploring%2520the%2520search%2520space%252C%2520as%2520shown%2520by%2520our%250Aproof-of-concept%252C%2520short%2520RNA%2520sequence.%2520Furthermore%252C%2520we%2520assess%2520its%250Ageneralizability%2520and%2520demonstrate%2520scalability%2520to%2520larger%2520sequences%252C%2520resulting%2520in%250Aa%2520~30%2525%2520improvement%2520over%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02984v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantically%20Rich%20Local%20Dataset%20Generation%20for%20Explainable%20AI%20in%0A%20%20Genomics&entry.906535625=Pedro%20Barbosa%20and%20Rosina%20Savisaar%20and%20Alcides%20Fonseca&entry.1292438233=%20%20Black%20box%20deep%20learning%20models%20trained%20on%20genomic%20sequences%20excel%20at%0Apredicting%20the%20outcomes%20of%20different%20gene%20regulatory%20mechanisms.%20Therefore%2C%0Ainterpreting%20these%20models%20may%20provide%20novel%20insights%20into%20the%20underlying%0Abiology%2C%20supporting%20downstream%20biomedical%20applications.%20Due%20to%20their%0Acomplexity%2C%20interpretable%20surrogate%20models%20can%20only%20be%20built%20for%20local%0Aexplanations%20%28e.g.%2C%20a%20single%20instance%29.%20However%2C%20accomplishing%20this%20requires%0Agenerating%20a%20dataset%20in%20the%20neighborhood%20of%20the%20input%2C%20which%20must%20maintain%0Asyntactic%20similarity%20to%20the%20original%20data%20while%20introducing%20semantic%0Avariability%20in%20the%20model%27s%20predictions.%20This%20task%20is%20challenging%20due%20to%20the%0Acomplex%20sequence-to-function%20relationship%20of%20DNA.%0A%20%20We%20propose%20using%20Genetic%20Programming%20to%20generate%20datasets%20by%20evolving%0Aperturbations%20in%20sequences%20that%20contribute%20to%20their%20semantic%20diversity.%20Our%0Acustom%2C%20domain-guided%20individual%20representation%20effectively%20constrains%0Asyntactic%20similarity%2C%20and%20we%20provide%20two%20alternative%20fitness%20functions%20that%0Apromote%20diversity%20with%20no%20computational%20effort.%20Applied%20to%20the%20RNA%20splicing%0Adomain%2C%20our%20approach%20quickly%20achieves%20good%20diversity%20and%20significantly%0Aoutperforms%20a%20random%20baseline%20in%20exploring%20the%20search%20space%2C%20as%20shown%20by%20our%0Aproof-of-concept%2C%20short%20RNA%20sequence.%20Furthermore%2C%20we%20assess%20its%0Ageneralizability%20and%20demonstrate%20scalability%20to%20larger%20sequences%2C%20resulting%20in%0Aa%20~30%25%20improvement%20over%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02984v2&entry.124074799=Read"},
{"title": "Micro-gesture Online Recognition using Learnable Query Points", "author": "Pengyu Liu and Fei Wang and Kun Li and Guoliang Chen and Yanyan Wei and Shengeng Tang and Zhiliang Wu and Dan Guo", "abstract": "  In this paper, we briefly introduce the solution developed by our team,\nHFUT-VUT, for the Micro-gesture Online Recognition track in the MiGA challenge\nat IJCAI 2024. The Micro-gesture Online Recognition task involves identifying\nthe category and locating the start and end times of micro-gestures in video\nclips. Compared to the typical Temporal Action Detection task, the\nMicro-gesture Online Recognition task focuses more on distinguishing between\nmicro-gestures and pinpointing the start and end times of actions. Our solution\nranks 2nd in the Micro-gesture Online Recognition track.\n", "link": "http://arxiv.org/abs/2407.04490v1", "date": "2024-07-05", "relevancy": 2.5214, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.52}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.516}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Micro-gesture%20Online%20Recognition%20using%20Learnable%20Query%20Points&body=Title%3A%20Micro-gesture%20Online%20Recognition%20using%20Learnable%20Query%20Points%0AAuthor%3A%20Pengyu%20Liu%20and%20Fei%20Wang%20and%20Kun%20Li%20and%20Guoliang%20Chen%20and%20Yanyan%20Wei%20and%20Shengeng%20Tang%20and%20Zhiliang%20Wu%20and%20Dan%20Guo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20briefly%20introduce%20the%20solution%20developed%20by%20our%20team%2C%0AHFUT-VUT%2C%20for%20the%20Micro-gesture%20Online%20Recognition%20track%20in%20the%20MiGA%20challenge%0Aat%20IJCAI%202024.%20The%20Micro-gesture%20Online%20Recognition%20task%20involves%20identifying%0Athe%20category%20and%20locating%20the%20start%20and%20end%20times%20of%20micro-gestures%20in%20video%0Aclips.%20Compared%20to%20the%20typical%20Temporal%20Action%20Detection%20task%2C%20the%0AMicro-gesture%20Online%20Recognition%20task%20focuses%20more%20on%20distinguishing%20between%0Amicro-gestures%20and%20pinpointing%20the%20start%20and%20end%20times%20of%20actions.%20Our%20solution%0Aranks%202nd%20in%20the%20Micro-gesture%20Online%20Recognition%20track.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMicro-gesture%2520Online%2520Recognition%2520using%2520Learnable%2520Query%2520Points%26entry.906535625%3DPengyu%2520Liu%2520and%2520Fei%2520Wang%2520and%2520Kun%2520Li%2520and%2520Guoliang%2520Chen%2520and%2520Yanyan%2520Wei%2520and%2520Shengeng%2520Tang%2520and%2520Zhiliang%2520Wu%2520and%2520Dan%2520Guo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520briefly%2520introduce%2520the%2520solution%2520developed%2520by%2520our%2520team%252C%250AHFUT-VUT%252C%2520for%2520the%2520Micro-gesture%2520Online%2520Recognition%2520track%2520in%2520the%2520MiGA%2520challenge%250Aat%2520IJCAI%25202024.%2520The%2520Micro-gesture%2520Online%2520Recognition%2520task%2520involves%2520identifying%250Athe%2520category%2520and%2520locating%2520the%2520start%2520and%2520end%2520times%2520of%2520micro-gestures%2520in%2520video%250Aclips.%2520Compared%2520to%2520the%2520typical%2520Temporal%2520Action%2520Detection%2520task%252C%2520the%250AMicro-gesture%2520Online%2520Recognition%2520task%2520focuses%2520more%2520on%2520distinguishing%2520between%250Amicro-gestures%2520and%2520pinpointing%2520the%2520start%2520and%2520end%2520times%2520of%2520actions.%2520Our%2520solution%250Aranks%25202nd%2520in%2520the%2520Micro-gesture%2520Online%2520Recognition%2520track.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Micro-gesture%20Online%20Recognition%20using%20Learnable%20Query%20Points&entry.906535625=Pengyu%20Liu%20and%20Fei%20Wang%20and%20Kun%20Li%20and%20Guoliang%20Chen%20and%20Yanyan%20Wei%20and%20Shengeng%20Tang%20and%20Zhiliang%20Wu%20and%20Dan%20Guo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20briefly%20introduce%20the%20solution%20developed%20by%20our%20team%2C%0AHFUT-VUT%2C%20for%20the%20Micro-gesture%20Online%20Recognition%20track%20in%20the%20MiGA%20challenge%0Aat%20IJCAI%202024.%20The%20Micro-gesture%20Online%20Recognition%20task%20involves%20identifying%0Athe%20category%20and%20locating%20the%20start%20and%20end%20times%20of%20micro-gestures%20in%20video%0Aclips.%20Compared%20to%20the%20typical%20Temporal%20Action%20Detection%20task%2C%20the%0AMicro-gesture%20Online%20Recognition%20task%20focuses%20more%20on%20distinguishing%20between%0Amicro-gestures%20and%20pinpointing%20the%20start%20and%20end%20times%20of%20actions.%20Our%20solution%0Aranks%202nd%20in%20the%20Micro-gesture%20Online%20Recognition%20track.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04490v1&entry.124074799=Read"},
{"title": "AnyMaker: Zero-shot General Object Customization via Decoupled\n  Dual-Level ID Injection", "author": "Lingjie Kong and Kai Wu and Xiaobin Hu and Wenhui Han and Jinlong Peng and Chengming Xu and Donghao Luo and Jiangning Zhang and Chengjie Wang and Yanwei Fu", "abstract": "  Text-to-image based object customization, aiming to generate images with the\nsame identity (ID) as objects of interest in accordance with text prompts and\nreference images, has made significant progress. However, recent customizing\nresearch is dominated by specialized tasks, such as human customization or\nvirtual try-on, leaving a gap in general object customization. To this end, we\nintroduce AnyMaker, an innovative zero-shot object customization framework\ncapable of generating general objects with high ID fidelity and flexible text\neditability. The efficacy of AnyMaker stems from its novel general ID\nextraction, dual-level ID injection, and ID-aware decoupling. Specifically, the\ngeneral ID extraction module extracts sufficient ID information with an\nensemble of self-supervised models to tackle the diverse customization tasks\nfor general objects. Then, to provide the diffusion UNet with the extracted ID\nas much while not damaging the text editability in the generation process, we\ndesign a global-local dual-level ID injection module, in which the global-level\nsemantic ID is injected into text descriptions while the local-level ID details\nare injected directly into the model through newly added cross-attention\nmodules. In addition, we propose an ID-aware decoupling module to disentangle\nID-related information from non-ID elements in the extracted representations\nfor high-fidelity generation of both identity and text descriptions. To\nvalidate our approach and boost the research of general object customization,\nwe create the first large-scale general ID dataset, Multi-Category\nID-Consistent (MC-IDC) dataset, with 315k text-image samples and 10k\ncategories. Experiments show that AnyMaker presents remarkable performance in\ngeneral object customization and outperforms specialized methods in\ncorresponding tasks. Code and dataset will be released soon.\n", "link": "http://arxiv.org/abs/2406.11643v3", "date": "2024-07-05", "relevancy": 2.52, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6573}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6181}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyMaker%3A%20Zero-shot%20General%20Object%20Customization%20via%20Decoupled%0A%20%20Dual-Level%20ID%20Injection&body=Title%3A%20AnyMaker%3A%20Zero-shot%20General%20Object%20Customization%20via%20Decoupled%0A%20%20Dual-Level%20ID%20Injection%0AAuthor%3A%20Lingjie%20Kong%20and%20Kai%20Wu%20and%20Xiaobin%20Hu%20and%20Wenhui%20Han%20and%20Jinlong%20Peng%20and%20Chengming%20Xu%20and%20Donghao%20Luo%20and%20Jiangning%20Zhang%20and%20Chengjie%20Wang%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Text-to-image%20based%20object%20customization%2C%20aiming%20to%20generate%20images%20with%20the%0Asame%20identity%20%28ID%29%20as%20objects%20of%20interest%20in%20accordance%20with%20text%20prompts%20and%0Areference%20images%2C%20has%20made%20significant%20progress.%20However%2C%20recent%20customizing%0Aresearch%20is%20dominated%20by%20specialized%20tasks%2C%20such%20as%20human%20customization%20or%0Avirtual%20try-on%2C%20leaving%20a%20gap%20in%20general%20object%20customization.%20To%20this%20end%2C%20we%0Aintroduce%20AnyMaker%2C%20an%20innovative%20zero-shot%20object%20customization%20framework%0Acapable%20of%20generating%20general%20objects%20with%20high%20ID%20fidelity%20and%20flexible%20text%0Aeditability.%20The%20efficacy%20of%20AnyMaker%20stems%20from%20its%20novel%20general%20ID%0Aextraction%2C%20dual-level%20ID%20injection%2C%20and%20ID-aware%20decoupling.%20Specifically%2C%20the%0Ageneral%20ID%20extraction%20module%20extracts%20sufficient%20ID%20information%20with%20an%0Aensemble%20of%20self-supervised%20models%20to%20tackle%20the%20diverse%20customization%20tasks%0Afor%20general%20objects.%20Then%2C%20to%20provide%20the%20diffusion%20UNet%20with%20the%20extracted%20ID%0Aas%20much%20while%20not%20damaging%20the%20text%20editability%20in%20the%20generation%20process%2C%20we%0Adesign%20a%20global-local%20dual-level%20ID%20injection%20module%2C%20in%20which%20the%20global-level%0Asemantic%20ID%20is%20injected%20into%20text%20descriptions%20while%20the%20local-level%20ID%20details%0Aare%20injected%20directly%20into%20the%20model%20through%20newly%20added%20cross-attention%0Amodules.%20In%20addition%2C%20we%20propose%20an%20ID-aware%20decoupling%20module%20to%20disentangle%0AID-related%20information%20from%20non-ID%20elements%20in%20the%20extracted%20representations%0Afor%20high-fidelity%20generation%20of%20both%20identity%20and%20text%20descriptions.%20To%0Avalidate%20our%20approach%20and%20boost%20the%20research%20of%20general%20object%20customization%2C%0Awe%20create%20the%20first%20large-scale%20general%20ID%20dataset%2C%20Multi-Category%0AID-Consistent%20%28MC-IDC%29%20dataset%2C%20with%20315k%20text-image%20samples%20and%2010k%0Acategories.%20Experiments%20show%20that%20AnyMaker%20presents%20remarkable%20performance%20in%0Ageneral%20object%20customization%20and%20outperforms%20specialized%20methods%20in%0Acorresponding%20tasks.%20Code%20and%20dataset%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11643v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyMaker%253A%2520Zero-shot%2520General%2520Object%2520Customization%2520via%2520Decoupled%250A%2520%2520Dual-Level%2520ID%2520Injection%26entry.906535625%3DLingjie%2520Kong%2520and%2520Kai%2520Wu%2520and%2520Xiaobin%2520Hu%2520and%2520Wenhui%2520Han%2520and%2520Jinlong%2520Peng%2520and%2520Chengming%2520Xu%2520and%2520Donghao%2520Luo%2520and%2520Jiangning%2520Zhang%2520and%2520Chengjie%2520Wang%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520Text-to-image%2520based%2520object%2520customization%252C%2520aiming%2520to%2520generate%2520images%2520with%2520the%250Asame%2520identity%2520%2528ID%2529%2520as%2520objects%2520of%2520interest%2520in%2520accordance%2520with%2520text%2520prompts%2520and%250Areference%2520images%252C%2520has%2520made%2520significant%2520progress.%2520However%252C%2520recent%2520customizing%250Aresearch%2520is%2520dominated%2520by%2520specialized%2520tasks%252C%2520such%2520as%2520human%2520customization%2520or%250Avirtual%2520try-on%252C%2520leaving%2520a%2520gap%2520in%2520general%2520object%2520customization.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520AnyMaker%252C%2520an%2520innovative%2520zero-shot%2520object%2520customization%2520framework%250Acapable%2520of%2520generating%2520general%2520objects%2520with%2520high%2520ID%2520fidelity%2520and%2520flexible%2520text%250Aeditability.%2520The%2520efficacy%2520of%2520AnyMaker%2520stems%2520from%2520its%2520novel%2520general%2520ID%250Aextraction%252C%2520dual-level%2520ID%2520injection%252C%2520and%2520ID-aware%2520decoupling.%2520Specifically%252C%2520the%250Ageneral%2520ID%2520extraction%2520module%2520extracts%2520sufficient%2520ID%2520information%2520with%2520an%250Aensemble%2520of%2520self-supervised%2520models%2520to%2520tackle%2520the%2520diverse%2520customization%2520tasks%250Afor%2520general%2520objects.%2520Then%252C%2520to%2520provide%2520the%2520diffusion%2520UNet%2520with%2520the%2520extracted%2520ID%250Aas%2520much%2520while%2520not%2520damaging%2520the%2520text%2520editability%2520in%2520the%2520generation%2520process%252C%2520we%250Adesign%2520a%2520global-local%2520dual-level%2520ID%2520injection%2520module%252C%2520in%2520which%2520the%2520global-level%250Asemantic%2520ID%2520is%2520injected%2520into%2520text%2520descriptions%2520while%2520the%2520local-level%2520ID%2520details%250Aare%2520injected%2520directly%2520into%2520the%2520model%2520through%2520newly%2520added%2520cross-attention%250Amodules.%2520In%2520addition%252C%2520we%2520propose%2520an%2520ID-aware%2520decoupling%2520module%2520to%2520disentangle%250AID-related%2520information%2520from%2520non-ID%2520elements%2520in%2520the%2520extracted%2520representations%250Afor%2520high-fidelity%2520generation%2520of%2520both%2520identity%2520and%2520text%2520descriptions.%2520To%250Avalidate%2520our%2520approach%2520and%2520boost%2520the%2520research%2520of%2520general%2520object%2520customization%252C%250Awe%2520create%2520the%2520first%2520large-scale%2520general%2520ID%2520dataset%252C%2520Multi-Category%250AID-Consistent%2520%2528MC-IDC%2529%2520dataset%252C%2520with%2520315k%2520text-image%2520samples%2520and%252010k%250Acategories.%2520Experiments%2520show%2520that%2520AnyMaker%2520presents%2520remarkable%2520performance%2520in%250Ageneral%2520object%2520customization%2520and%2520outperforms%2520specialized%2520methods%2520in%250Acorresponding%2520tasks.%2520Code%2520and%2520dataset%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11643v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyMaker%3A%20Zero-shot%20General%20Object%20Customization%20via%20Decoupled%0A%20%20Dual-Level%20ID%20Injection&entry.906535625=Lingjie%20Kong%20and%20Kai%20Wu%20and%20Xiaobin%20Hu%20and%20Wenhui%20Han%20and%20Jinlong%20Peng%20and%20Chengming%20Xu%20and%20Donghao%20Luo%20and%20Jiangning%20Zhang%20and%20Chengjie%20Wang%20and%20Yanwei%20Fu&entry.1292438233=%20%20Text-to-image%20based%20object%20customization%2C%20aiming%20to%20generate%20images%20with%20the%0Asame%20identity%20%28ID%29%20as%20objects%20of%20interest%20in%20accordance%20with%20text%20prompts%20and%0Areference%20images%2C%20has%20made%20significant%20progress.%20However%2C%20recent%20customizing%0Aresearch%20is%20dominated%20by%20specialized%20tasks%2C%20such%20as%20human%20customization%20or%0Avirtual%20try-on%2C%20leaving%20a%20gap%20in%20general%20object%20customization.%20To%20this%20end%2C%20we%0Aintroduce%20AnyMaker%2C%20an%20innovative%20zero-shot%20object%20customization%20framework%0Acapable%20of%20generating%20general%20objects%20with%20high%20ID%20fidelity%20and%20flexible%20text%0Aeditability.%20The%20efficacy%20of%20AnyMaker%20stems%20from%20its%20novel%20general%20ID%0Aextraction%2C%20dual-level%20ID%20injection%2C%20and%20ID-aware%20decoupling.%20Specifically%2C%20the%0Ageneral%20ID%20extraction%20module%20extracts%20sufficient%20ID%20information%20with%20an%0Aensemble%20of%20self-supervised%20models%20to%20tackle%20the%20diverse%20customization%20tasks%0Afor%20general%20objects.%20Then%2C%20to%20provide%20the%20diffusion%20UNet%20with%20the%20extracted%20ID%0Aas%20much%20while%20not%20damaging%20the%20text%20editability%20in%20the%20generation%20process%2C%20we%0Adesign%20a%20global-local%20dual-level%20ID%20injection%20module%2C%20in%20which%20the%20global-level%0Asemantic%20ID%20is%20injected%20into%20text%20descriptions%20while%20the%20local-level%20ID%20details%0Aare%20injected%20directly%20into%20the%20model%20through%20newly%20added%20cross-attention%0Amodules.%20In%20addition%2C%20we%20propose%20an%20ID-aware%20decoupling%20module%20to%20disentangle%0AID-related%20information%20from%20non-ID%20elements%20in%20the%20extracted%20representations%0Afor%20high-fidelity%20generation%20of%20both%20identity%20and%20text%20descriptions.%20To%0Avalidate%20our%20approach%20and%20boost%20the%20research%20of%20general%20object%20customization%2C%0Awe%20create%20the%20first%20large-scale%20general%20ID%20dataset%2C%20Multi-Category%0AID-Consistent%20%28MC-IDC%29%20dataset%2C%20with%20315k%20text-image%20samples%20and%2010k%0Acategories.%20Experiments%20show%20that%20AnyMaker%20presents%20remarkable%20performance%20in%0Ageneral%20object%20customization%20and%20outperforms%20specialized%20methods%20in%0Acorresponding%20tasks.%20Code%20and%20dataset%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11643v3&entry.124074799=Read"},
{"title": "Wavelet-based Temporal Attention Improves Traffic Forecasting", "author": "Yash Jakhmola and Nitish Kumar Mishra and Kripabandhu Ghosh and Tanujit Chakraborty", "abstract": "  Spatio-temporal forecasting of traffic flow data represents a typical problem\nin the field of machine learning, impacting urban traffic management systems.\nTraditional statistical and machine learning methods cannot adequately handle\nboth the temporal and spatial dependencies in these complex traffic flow\ndatasets. A prevalent approach in the field is to combine graph convolutional\nnetworks and multi-head attention mechanisms for spatio-temporal processing.\nThis paper proposes a wavelet-based temporal attention model, namely a\nwavelet-based dynamic spatio-temporal aware graph neural network (W-DSTAGNN),\nfor tackling the traffic forecasting problem. Benchmark experiments using\nseveral statistical metrics confirm that our proposal efficiently captures\nspatio-temporal correlations and outperforms ten state-of-the-art models on\nthree different real-world traffic datasets. Our proposed ensemble data-driven\nmethod can handle dynamic temporal and spatial dependencies and make long-term\nforecasts in an efficient manner.\n", "link": "http://arxiv.org/abs/2407.04440v1", "date": "2024-07-05", "relevancy": 2.515, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5248}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4942}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wavelet-based%20Temporal%20Attention%20Improves%20Traffic%20Forecasting&body=Title%3A%20Wavelet-based%20Temporal%20Attention%20Improves%20Traffic%20Forecasting%0AAuthor%3A%20Yash%20Jakhmola%20and%20Nitish%20Kumar%20Mishra%20and%20Kripabandhu%20Ghosh%20and%20Tanujit%20Chakraborty%0AAbstract%3A%20%20%20Spatio-temporal%20forecasting%20of%20traffic%20flow%20data%20represents%20a%20typical%20problem%0Ain%20the%20field%20of%20machine%20learning%2C%20impacting%20urban%20traffic%20management%20systems.%0ATraditional%20statistical%20and%20machine%20learning%20methods%20cannot%20adequately%20handle%0Aboth%20the%20temporal%20and%20spatial%20dependencies%20in%20these%20complex%20traffic%20flow%0Adatasets.%20A%20prevalent%20approach%20in%20the%20field%20is%20to%20combine%20graph%20convolutional%0Anetworks%20and%20multi-head%20attention%20mechanisms%20for%20spatio-temporal%20processing.%0AThis%20paper%20proposes%20a%20wavelet-based%20temporal%20attention%20model%2C%20namely%20a%0Awavelet-based%20dynamic%20spatio-temporal%20aware%20graph%20neural%20network%20%28W-DSTAGNN%29%2C%0Afor%20tackling%20the%20traffic%20forecasting%20problem.%20Benchmark%20experiments%20using%0Aseveral%20statistical%20metrics%20confirm%20that%20our%20proposal%20efficiently%20captures%0Aspatio-temporal%20correlations%20and%20outperforms%20ten%20state-of-the-art%20models%20on%0Athree%20different%20real-world%20traffic%20datasets.%20Our%20proposed%20ensemble%20data-driven%0Amethod%20can%20handle%20dynamic%20temporal%20and%20spatial%20dependencies%20and%20make%20long-term%0Aforecasts%20in%20an%20efficient%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavelet-based%2520Temporal%2520Attention%2520Improves%2520Traffic%2520Forecasting%26entry.906535625%3DYash%2520Jakhmola%2520and%2520Nitish%2520Kumar%2520Mishra%2520and%2520Kripabandhu%2520Ghosh%2520and%2520Tanujit%2520Chakraborty%26entry.1292438233%3D%2520%2520Spatio-temporal%2520forecasting%2520of%2520traffic%2520flow%2520data%2520represents%2520a%2520typical%2520problem%250Ain%2520the%2520field%2520of%2520machine%2520learning%252C%2520impacting%2520urban%2520traffic%2520management%2520systems.%250ATraditional%2520statistical%2520and%2520machine%2520learning%2520methods%2520cannot%2520adequately%2520handle%250Aboth%2520the%2520temporal%2520and%2520spatial%2520dependencies%2520in%2520these%2520complex%2520traffic%2520flow%250Adatasets.%2520A%2520prevalent%2520approach%2520in%2520the%2520field%2520is%2520to%2520combine%2520graph%2520convolutional%250Anetworks%2520and%2520multi-head%2520attention%2520mechanisms%2520for%2520spatio-temporal%2520processing.%250AThis%2520paper%2520proposes%2520a%2520wavelet-based%2520temporal%2520attention%2520model%252C%2520namely%2520a%250Awavelet-based%2520dynamic%2520spatio-temporal%2520aware%2520graph%2520neural%2520network%2520%2528W-DSTAGNN%2529%252C%250Afor%2520tackling%2520the%2520traffic%2520forecasting%2520problem.%2520Benchmark%2520experiments%2520using%250Aseveral%2520statistical%2520metrics%2520confirm%2520that%2520our%2520proposal%2520efficiently%2520captures%250Aspatio-temporal%2520correlations%2520and%2520outperforms%2520ten%2520state-of-the-art%2520models%2520on%250Athree%2520different%2520real-world%2520traffic%2520datasets.%2520Our%2520proposed%2520ensemble%2520data-driven%250Amethod%2520can%2520handle%2520dynamic%2520temporal%2520and%2520spatial%2520dependencies%2520and%2520make%2520long-term%250Aforecasts%2520in%2520an%2520efficient%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wavelet-based%20Temporal%20Attention%20Improves%20Traffic%20Forecasting&entry.906535625=Yash%20Jakhmola%20and%20Nitish%20Kumar%20Mishra%20and%20Kripabandhu%20Ghosh%20and%20Tanujit%20Chakraborty&entry.1292438233=%20%20Spatio-temporal%20forecasting%20of%20traffic%20flow%20data%20represents%20a%20typical%20problem%0Ain%20the%20field%20of%20machine%20learning%2C%20impacting%20urban%20traffic%20management%20systems.%0ATraditional%20statistical%20and%20machine%20learning%20methods%20cannot%20adequately%20handle%0Aboth%20the%20temporal%20and%20spatial%20dependencies%20in%20these%20complex%20traffic%20flow%0Adatasets.%20A%20prevalent%20approach%20in%20the%20field%20is%20to%20combine%20graph%20convolutional%0Anetworks%20and%20multi-head%20attention%20mechanisms%20for%20spatio-temporal%20processing.%0AThis%20paper%20proposes%20a%20wavelet-based%20temporal%20attention%20model%2C%20namely%20a%0Awavelet-based%20dynamic%20spatio-temporal%20aware%20graph%20neural%20network%20%28W-DSTAGNN%29%2C%0Afor%20tackling%20the%20traffic%20forecasting%20problem.%20Benchmark%20experiments%20using%0Aseveral%20statistical%20metrics%20confirm%20that%20our%20proposal%20efficiently%20captures%0Aspatio-temporal%20correlations%20and%20outperforms%20ten%20state-of-the-art%20models%20on%0Athree%20different%20real-world%20traffic%20datasets.%20Our%20proposed%20ensemble%20data-driven%0Amethod%20can%20handle%20dynamic%20temporal%20and%20spatial%20dependencies%20and%20make%20long-term%0Aforecasts%20in%20an%20efficient%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04440v1&entry.124074799=Read"},
{"title": "Structural Constraint Integration in Generative Model for Discovery of\n  Quantum Material Candidates", "author": "Ryotaro Okabe and Mouyang Cheng and Abhijatmedhi Chotrattanapituk and Nguyen Tuan Hung and Xiang Fu and Bowen Han and Yao Wang and Weiwei Xie and Robert J. Cava and Tommi S. Jaakkola and Yongqiang Cheng and Mingda Li", "abstract": "  Billions of organic molecules are known, but only a tiny fraction of the\nfunctional inorganic materials have been discovered, a particularly relevant\nproblem to the community searching for new quantum materials. Recent\nadvancements in machine-learning-based generative models, particularly\ndiffusion models, show great promise for generating new, stable materials.\nHowever, integrating geometric patterns into materials generation remains a\nchallenge. Here, we introduce Structural Constraint Integration in the\nGENerative model (SCIGEN). Our approach can modify any trained generative\ndiffusion model by strategic masking of the denoised structure with a diffused\nconstrained structure prior to each diffusion step to steer the generation\ntoward constrained outputs. Furthermore, we mathematically prove that SCIGEN\neffectively performs conditional sampling from the original distribution, which\nis crucial for generating stable constrained materials. We generate eight\nmillion compounds using Archimedean lattices as prototype constraints, with\nover 10% surviving a multi-staged stability pre-screening. High-throughput\ndensity functional theory (DFT) on 26,000 survived compounds shows that over\n50% passed structural optimization at the DFT level. Since the properties of\nquantum materials are closely related to geometric patterns, our results\nindicate that SCIGEN provides a general framework for generating quantum\nmaterials candidates.\n", "link": "http://arxiv.org/abs/2407.04557v1", "date": "2024-07-05", "relevancy": 2.5131, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5112}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4983}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structural%20Constraint%20Integration%20in%20Generative%20Model%20for%20Discovery%20of%0A%20%20Quantum%20Material%20Candidates&body=Title%3A%20Structural%20Constraint%20Integration%20in%20Generative%20Model%20for%20Discovery%20of%0A%20%20Quantum%20Material%20Candidates%0AAuthor%3A%20Ryotaro%20Okabe%20and%20Mouyang%20Cheng%20and%20Abhijatmedhi%20Chotrattanapituk%20and%20Nguyen%20Tuan%20Hung%20and%20Xiang%20Fu%20and%20Bowen%20Han%20and%20Yao%20Wang%20and%20Weiwei%20Xie%20and%20Robert%20J.%20Cava%20and%20Tommi%20S.%20Jaakkola%20and%20Yongqiang%20Cheng%20and%20Mingda%20Li%0AAbstract%3A%20%20%20Billions%20of%20organic%20molecules%20are%20known%2C%20but%20only%20a%20tiny%20fraction%20of%20the%0Afunctional%20inorganic%20materials%20have%20been%20discovered%2C%20a%20particularly%20relevant%0Aproblem%20to%20the%20community%20searching%20for%20new%20quantum%20materials.%20Recent%0Aadvancements%20in%20machine-learning-based%20generative%20models%2C%20particularly%0Adiffusion%20models%2C%20show%20great%20promise%20for%20generating%20new%2C%20stable%20materials.%0AHowever%2C%20integrating%20geometric%20patterns%20into%20materials%20generation%20remains%20a%0Achallenge.%20Here%2C%20we%20introduce%20Structural%20Constraint%20Integration%20in%20the%0AGENerative%20model%20%28SCIGEN%29.%20Our%20approach%20can%20modify%20any%20trained%20generative%0Adiffusion%20model%20by%20strategic%20masking%20of%20the%20denoised%20structure%20with%20a%20diffused%0Aconstrained%20structure%20prior%20to%20each%20diffusion%20step%20to%20steer%20the%20generation%0Atoward%20constrained%20outputs.%20Furthermore%2C%20we%20mathematically%20prove%20that%20SCIGEN%0Aeffectively%20performs%20conditional%20sampling%20from%20the%20original%20distribution%2C%20which%0Ais%20crucial%20for%20generating%20stable%20constrained%20materials.%20We%20generate%20eight%0Amillion%20compounds%20using%20Archimedean%20lattices%20as%20prototype%20constraints%2C%20with%0Aover%2010%25%20surviving%20a%20multi-staged%20stability%20pre-screening.%20High-throughput%0Adensity%20functional%20theory%20%28DFT%29%20on%2026%2C000%20survived%20compounds%20shows%20that%20over%0A50%25%20passed%20structural%20optimization%20at%20the%20DFT%20level.%20Since%20the%20properties%20of%0Aquantum%20materials%20are%20closely%20related%20to%20geometric%20patterns%2C%20our%20results%0Aindicate%20that%20SCIGEN%20provides%20a%20general%20framework%20for%20generating%20quantum%0Amaterials%20candidates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructural%2520Constraint%2520Integration%2520in%2520Generative%2520Model%2520for%2520Discovery%2520of%250A%2520%2520Quantum%2520Material%2520Candidates%26entry.906535625%3DRyotaro%2520Okabe%2520and%2520Mouyang%2520Cheng%2520and%2520Abhijatmedhi%2520Chotrattanapituk%2520and%2520Nguyen%2520Tuan%2520Hung%2520and%2520Xiang%2520Fu%2520and%2520Bowen%2520Han%2520and%2520Yao%2520Wang%2520and%2520Weiwei%2520Xie%2520and%2520Robert%2520J.%2520Cava%2520and%2520Tommi%2520S.%2520Jaakkola%2520and%2520Yongqiang%2520Cheng%2520and%2520Mingda%2520Li%26entry.1292438233%3D%2520%2520Billions%2520of%2520organic%2520molecules%2520are%2520known%252C%2520but%2520only%2520a%2520tiny%2520fraction%2520of%2520the%250Afunctional%2520inorganic%2520materials%2520have%2520been%2520discovered%252C%2520a%2520particularly%2520relevant%250Aproblem%2520to%2520the%2520community%2520searching%2520for%2520new%2520quantum%2520materials.%2520Recent%250Aadvancements%2520in%2520machine-learning-based%2520generative%2520models%252C%2520particularly%250Adiffusion%2520models%252C%2520show%2520great%2520promise%2520for%2520generating%2520new%252C%2520stable%2520materials.%250AHowever%252C%2520integrating%2520geometric%2520patterns%2520into%2520materials%2520generation%2520remains%2520a%250Achallenge.%2520Here%252C%2520we%2520introduce%2520Structural%2520Constraint%2520Integration%2520in%2520the%250AGENerative%2520model%2520%2528SCIGEN%2529.%2520Our%2520approach%2520can%2520modify%2520any%2520trained%2520generative%250Adiffusion%2520model%2520by%2520strategic%2520masking%2520of%2520the%2520denoised%2520structure%2520with%2520a%2520diffused%250Aconstrained%2520structure%2520prior%2520to%2520each%2520diffusion%2520step%2520to%2520steer%2520the%2520generation%250Atoward%2520constrained%2520outputs.%2520Furthermore%252C%2520we%2520mathematically%2520prove%2520that%2520SCIGEN%250Aeffectively%2520performs%2520conditional%2520sampling%2520from%2520the%2520original%2520distribution%252C%2520which%250Ais%2520crucial%2520for%2520generating%2520stable%2520constrained%2520materials.%2520We%2520generate%2520eight%250Amillion%2520compounds%2520using%2520Archimedean%2520lattices%2520as%2520prototype%2520constraints%252C%2520with%250Aover%252010%2525%2520surviving%2520a%2520multi-staged%2520stability%2520pre-screening.%2520High-throughput%250Adensity%2520functional%2520theory%2520%2528DFT%2529%2520on%252026%252C000%2520survived%2520compounds%2520shows%2520that%2520over%250A50%2525%2520passed%2520structural%2520optimization%2520at%2520the%2520DFT%2520level.%2520Since%2520the%2520properties%2520of%250Aquantum%2520materials%2520are%2520closely%2520related%2520to%2520geometric%2520patterns%252C%2520our%2520results%250Aindicate%2520that%2520SCIGEN%2520provides%2520a%2520general%2520framework%2520for%2520generating%2520quantum%250Amaterials%2520candidates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structural%20Constraint%20Integration%20in%20Generative%20Model%20for%20Discovery%20of%0A%20%20Quantum%20Material%20Candidates&entry.906535625=Ryotaro%20Okabe%20and%20Mouyang%20Cheng%20and%20Abhijatmedhi%20Chotrattanapituk%20and%20Nguyen%20Tuan%20Hung%20and%20Xiang%20Fu%20and%20Bowen%20Han%20and%20Yao%20Wang%20and%20Weiwei%20Xie%20and%20Robert%20J.%20Cava%20and%20Tommi%20S.%20Jaakkola%20and%20Yongqiang%20Cheng%20and%20Mingda%20Li&entry.1292438233=%20%20Billions%20of%20organic%20molecules%20are%20known%2C%20but%20only%20a%20tiny%20fraction%20of%20the%0Afunctional%20inorganic%20materials%20have%20been%20discovered%2C%20a%20particularly%20relevant%0Aproblem%20to%20the%20community%20searching%20for%20new%20quantum%20materials.%20Recent%0Aadvancements%20in%20machine-learning-based%20generative%20models%2C%20particularly%0Adiffusion%20models%2C%20show%20great%20promise%20for%20generating%20new%2C%20stable%20materials.%0AHowever%2C%20integrating%20geometric%20patterns%20into%20materials%20generation%20remains%20a%0Achallenge.%20Here%2C%20we%20introduce%20Structural%20Constraint%20Integration%20in%20the%0AGENerative%20model%20%28SCIGEN%29.%20Our%20approach%20can%20modify%20any%20trained%20generative%0Adiffusion%20model%20by%20strategic%20masking%20of%20the%20denoised%20structure%20with%20a%20diffused%0Aconstrained%20structure%20prior%20to%20each%20diffusion%20step%20to%20steer%20the%20generation%0Atoward%20constrained%20outputs.%20Furthermore%2C%20we%20mathematically%20prove%20that%20SCIGEN%0Aeffectively%20performs%20conditional%20sampling%20from%20the%20original%20distribution%2C%20which%0Ais%20crucial%20for%20generating%20stable%20constrained%20materials.%20We%20generate%20eight%0Amillion%20compounds%20using%20Archimedean%20lattices%20as%20prototype%20constraints%2C%20with%0Aover%2010%25%20surviving%20a%20multi-staged%20stability%20pre-screening.%20High-throughput%0Adensity%20functional%20theory%20%28DFT%29%20on%2026%2C000%20survived%20compounds%20shows%20that%20over%0A50%25%20passed%20structural%20optimization%20at%20the%20DFT%20level.%20Since%20the%20properties%20of%0Aquantum%20materials%20are%20closely%20related%20to%20geometric%20patterns%2C%20our%20results%0Aindicate%20that%20SCIGEN%20provides%20a%20general%20framework%20for%20generating%20quantum%0Amaterials%20candidates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04557v1&entry.124074799=Read"},
{"title": "Rethinking Data Input for Point Cloud Upsampling", "author": "Tongxu Zhang", "abstract": "  In recent years, point cloud upsampling has been widely applied in fields\nsuch as 3D reconstruction and surface generation. However, existing point cloud\nupsampling inputs are all patch based, and there is no research discussing the\ndifferences and principles between point cloud model full input and patch based\ninput. In order to compare with patch based point cloud input, this article\nproposes a new data input method, which divides the full point cloud model to\nensure shape integrity while training PU-GCN. This article was validated on the\nPU1K and ABC datasets, but the results showed that Patch based performance is\nbetter than model based full input i.e. Average Segment input. Therefore, this\narticle explores the data input factors and model modules that affect the\nupsampling results of point clouds.\n", "link": "http://arxiv.org/abs/2407.04476v1", "date": "2024-07-05", "relevancy": 2.5118, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5295}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4888}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Data%20Input%20for%20Point%20Cloud%20Upsampling&body=Title%3A%20Rethinking%20Data%20Input%20for%20Point%20Cloud%20Upsampling%0AAuthor%3A%20Tongxu%20Zhang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20point%20cloud%20upsampling%20has%20been%20widely%20applied%20in%20fields%0Asuch%20as%203D%20reconstruction%20and%20surface%20generation.%20However%2C%20existing%20point%20cloud%0Aupsampling%20inputs%20are%20all%20patch%20based%2C%20and%20there%20is%20no%20research%20discussing%20the%0Adifferences%20and%20principles%20between%20point%20cloud%20model%20full%20input%20and%20patch%20based%0Ainput.%20In%20order%20to%20compare%20with%20patch%20based%20point%20cloud%20input%2C%20this%20article%0Aproposes%20a%20new%20data%20input%20method%2C%20which%20divides%20the%20full%20point%20cloud%20model%20to%0Aensure%20shape%20integrity%20while%20training%20PU-GCN.%20This%20article%20was%20validated%20on%20the%0APU1K%20and%20ABC%20datasets%2C%20but%20the%20results%20showed%20that%20Patch%20based%20performance%20is%0Abetter%20than%20model%20based%20full%20input%20i.e.%20Average%20Segment%20input.%20Therefore%2C%20this%0Aarticle%20explores%20the%20data%20input%20factors%20and%20model%20modules%20that%20affect%20the%0Aupsampling%20results%20of%20point%20clouds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Data%2520Input%2520for%2520Point%2520Cloud%2520Upsampling%26entry.906535625%3DTongxu%2520Zhang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520point%2520cloud%2520upsampling%2520has%2520been%2520widely%2520applied%2520in%2520fields%250Asuch%2520as%25203D%2520reconstruction%2520and%2520surface%2520generation.%2520However%252C%2520existing%2520point%2520cloud%250Aupsampling%2520inputs%2520are%2520all%2520patch%2520based%252C%2520and%2520there%2520is%2520no%2520research%2520discussing%2520the%250Adifferences%2520and%2520principles%2520between%2520point%2520cloud%2520model%2520full%2520input%2520and%2520patch%2520based%250Ainput.%2520In%2520order%2520to%2520compare%2520with%2520patch%2520based%2520point%2520cloud%2520input%252C%2520this%2520article%250Aproposes%2520a%2520new%2520data%2520input%2520method%252C%2520which%2520divides%2520the%2520full%2520point%2520cloud%2520model%2520to%250Aensure%2520shape%2520integrity%2520while%2520training%2520PU-GCN.%2520This%2520article%2520was%2520validated%2520on%2520the%250APU1K%2520and%2520ABC%2520datasets%252C%2520but%2520the%2520results%2520showed%2520that%2520Patch%2520based%2520performance%2520is%250Abetter%2520than%2520model%2520based%2520full%2520input%2520i.e.%2520Average%2520Segment%2520input.%2520Therefore%252C%2520this%250Aarticle%2520explores%2520the%2520data%2520input%2520factors%2520and%2520model%2520modules%2520that%2520affect%2520the%250Aupsampling%2520results%2520of%2520point%2520clouds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Data%20Input%20for%20Point%20Cloud%20Upsampling&entry.906535625=Tongxu%20Zhang&entry.1292438233=%20%20In%20recent%20years%2C%20point%20cloud%20upsampling%20has%20been%20widely%20applied%20in%20fields%0Asuch%20as%203D%20reconstruction%20and%20surface%20generation.%20However%2C%20existing%20point%20cloud%0Aupsampling%20inputs%20are%20all%20patch%20based%2C%20and%20there%20is%20no%20research%20discussing%20the%0Adifferences%20and%20principles%20between%20point%20cloud%20model%20full%20input%20and%20patch%20based%0Ainput.%20In%20order%20to%20compare%20with%20patch%20based%20point%20cloud%20input%2C%20this%20article%0Aproposes%20a%20new%20data%20input%20method%2C%20which%20divides%20the%20full%20point%20cloud%20model%20to%0Aensure%20shape%20integrity%20while%20training%20PU-GCN.%20This%20article%20was%20validated%20on%20the%0APU1K%20and%20ABC%20datasets%2C%20but%20the%20results%20showed%20that%20Patch%20based%20performance%20is%0Abetter%20than%20model%20based%20full%20input%20i.e.%20Average%20Segment%20input.%20Therefore%2C%20this%0Aarticle%20explores%20the%20data%20input%20factors%20and%20model%20modules%20that%20affect%20the%0Aupsampling%20results%20of%20point%20clouds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04476v1&entry.124074799=Read"},
{"title": "EasyAnimate: A High-Performance Long Video Generation Method based on\n  Transformer Architecture", "author": "Jiaqi Xu and Xinyi Zou and Kunzhe Huang and Yunkuo Chen and Bo Liu and MengLi Cheng and Xing Shi and Jun Huang", "abstract": "  This paper presents EasyAnimate, an advanced method for video generation that\nleverages the power of transformer architecture for high-performance outcomes.\nWe have expanded the DiT framework originally designed for 2D image synthesis\nto accommodate the complexities of 3D video generation by incorporating a\nmotion module block. It is used to capture temporal dynamics, thereby ensuring\nthe production of consistent frames and seamless motion transitions. The motion\nmodule can be adapted to various DiT baseline methods to generate video with\ndifferent styles. It can also generate videos with different frame rates and\nresolutions during both training and inference phases, suitable for both images\nand videos. Moreover, we introduce slice VAE, a novel approach to condense the\ntemporal axis, facilitating the generation of long duration videos. Currently,\nEasyAnimate exhibits the proficiency to generate videos with 144 frames. We\nprovide a holistic ecosystem for video production based on DiT, encompassing\naspects such as data pre-processing, VAE training, DiT models training (both\nthe baseline model and LoRA model), and end-to-end video inference. Code is\navailable at: https://github.com/aigc-apps/EasyAnimate. We are continuously\nworking to enhance the performance of our method.\n", "link": "http://arxiv.org/abs/2405.18991v2", "date": "2024-07-05", "relevancy": 2.4703, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6185}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6172}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6163}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyAnimate%3A%20A%20High-Performance%20Long%20Video%20Generation%20Method%20based%20on%0A%20%20Transformer%20Architecture&body=Title%3A%20EasyAnimate%3A%20A%20High-Performance%20Long%20Video%20Generation%20Method%20based%20on%0A%20%20Transformer%20Architecture%0AAuthor%3A%20Jiaqi%20Xu%20and%20Xinyi%20Zou%20and%20Kunzhe%20Huang%20and%20Yunkuo%20Chen%20and%20Bo%20Liu%20and%20MengLi%20Cheng%20and%20Xing%20Shi%20and%20Jun%20Huang%0AAbstract%3A%20%20%20This%20paper%20presents%20EasyAnimate%2C%20an%20advanced%20method%20for%20video%20generation%20that%0Aleverages%20the%20power%20of%20transformer%20architecture%20for%20high-performance%20outcomes.%0AWe%20have%20expanded%20the%20DiT%20framework%20originally%20designed%20for%202D%20image%20synthesis%0Ato%20accommodate%20the%20complexities%20of%203D%20video%20generation%20by%20incorporating%20a%0Amotion%20module%20block.%20It%20is%20used%20to%20capture%20temporal%20dynamics%2C%20thereby%20ensuring%0Athe%20production%20of%20consistent%20frames%20and%20seamless%20motion%20transitions.%20The%20motion%0Amodule%20can%20be%20adapted%20to%20various%20DiT%20baseline%20methods%20to%20generate%20video%20with%0Adifferent%20styles.%20It%20can%20also%20generate%20videos%20with%20different%20frame%20rates%20and%0Aresolutions%20during%20both%20training%20and%20inference%20phases%2C%20suitable%20for%20both%20images%0Aand%20videos.%20Moreover%2C%20we%20introduce%20slice%20VAE%2C%20a%20novel%20approach%20to%20condense%20the%0Atemporal%20axis%2C%20facilitating%20the%20generation%20of%20long%20duration%20videos.%20Currently%2C%0AEasyAnimate%20exhibits%20the%20proficiency%20to%20generate%20videos%20with%20144%20frames.%20We%0Aprovide%20a%20holistic%20ecosystem%20for%20video%20production%20based%20on%20DiT%2C%20encompassing%0Aaspects%20such%20as%20data%20pre-processing%2C%20VAE%20training%2C%20DiT%20models%20training%20%28both%0Athe%20baseline%20model%20and%20LoRA%20model%29%2C%20and%20end-to-end%20video%20inference.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/aigc-apps/EasyAnimate.%20We%20are%20continuously%0Aworking%20to%20enhance%20the%20performance%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18991v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyAnimate%253A%2520A%2520High-Performance%2520Long%2520Video%2520Generation%2520Method%2520based%2520on%250A%2520%2520Transformer%2520Architecture%26entry.906535625%3DJiaqi%2520Xu%2520and%2520Xinyi%2520Zou%2520and%2520Kunzhe%2520Huang%2520and%2520Yunkuo%2520Chen%2520and%2520Bo%2520Liu%2520and%2520MengLi%2520Cheng%2520and%2520Xing%2520Shi%2520and%2520Jun%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520EasyAnimate%252C%2520an%2520advanced%2520method%2520for%2520video%2520generation%2520that%250Aleverages%2520the%2520power%2520of%2520transformer%2520architecture%2520for%2520high-performance%2520outcomes.%250AWe%2520have%2520expanded%2520the%2520DiT%2520framework%2520originally%2520designed%2520for%25202D%2520image%2520synthesis%250Ato%2520accommodate%2520the%2520complexities%2520of%25203D%2520video%2520generation%2520by%2520incorporating%2520a%250Amotion%2520module%2520block.%2520It%2520is%2520used%2520to%2520capture%2520temporal%2520dynamics%252C%2520thereby%2520ensuring%250Athe%2520production%2520of%2520consistent%2520frames%2520and%2520seamless%2520motion%2520transitions.%2520The%2520motion%250Amodule%2520can%2520be%2520adapted%2520to%2520various%2520DiT%2520baseline%2520methods%2520to%2520generate%2520video%2520with%250Adifferent%2520styles.%2520It%2520can%2520also%2520generate%2520videos%2520with%2520different%2520frame%2520rates%2520and%250Aresolutions%2520during%2520both%2520training%2520and%2520inference%2520phases%252C%2520suitable%2520for%2520both%2520images%250Aand%2520videos.%2520Moreover%252C%2520we%2520introduce%2520slice%2520VAE%252C%2520a%2520novel%2520approach%2520to%2520condense%2520the%250Atemporal%2520axis%252C%2520facilitating%2520the%2520generation%2520of%2520long%2520duration%2520videos.%2520Currently%252C%250AEasyAnimate%2520exhibits%2520the%2520proficiency%2520to%2520generate%2520videos%2520with%2520144%2520frames.%2520We%250Aprovide%2520a%2520holistic%2520ecosystem%2520for%2520video%2520production%2520based%2520on%2520DiT%252C%2520encompassing%250Aaspects%2520such%2520as%2520data%2520pre-processing%252C%2520VAE%2520training%252C%2520DiT%2520models%2520training%2520%2528both%250Athe%2520baseline%2520model%2520and%2520LoRA%2520model%2529%252C%2520and%2520end-to-end%2520video%2520inference.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/aigc-apps/EasyAnimate.%2520We%2520are%2520continuously%250Aworking%2520to%2520enhance%2520the%2520performance%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18991v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyAnimate%3A%20A%20High-Performance%20Long%20Video%20Generation%20Method%20based%20on%0A%20%20Transformer%20Architecture&entry.906535625=Jiaqi%20Xu%20and%20Xinyi%20Zou%20and%20Kunzhe%20Huang%20and%20Yunkuo%20Chen%20and%20Bo%20Liu%20and%20MengLi%20Cheng%20and%20Xing%20Shi%20and%20Jun%20Huang&entry.1292438233=%20%20This%20paper%20presents%20EasyAnimate%2C%20an%20advanced%20method%20for%20video%20generation%20that%0Aleverages%20the%20power%20of%20transformer%20architecture%20for%20high-performance%20outcomes.%0AWe%20have%20expanded%20the%20DiT%20framework%20originally%20designed%20for%202D%20image%20synthesis%0Ato%20accommodate%20the%20complexities%20of%203D%20video%20generation%20by%20incorporating%20a%0Amotion%20module%20block.%20It%20is%20used%20to%20capture%20temporal%20dynamics%2C%20thereby%20ensuring%0Athe%20production%20of%20consistent%20frames%20and%20seamless%20motion%20transitions.%20The%20motion%0Amodule%20can%20be%20adapted%20to%20various%20DiT%20baseline%20methods%20to%20generate%20video%20with%0Adifferent%20styles.%20It%20can%20also%20generate%20videos%20with%20different%20frame%20rates%20and%0Aresolutions%20during%20both%20training%20and%20inference%20phases%2C%20suitable%20for%20both%20images%0Aand%20videos.%20Moreover%2C%20we%20introduce%20slice%20VAE%2C%20a%20novel%20approach%20to%20condense%20the%0Atemporal%20axis%2C%20facilitating%20the%20generation%20of%20long%20duration%20videos.%20Currently%2C%0AEasyAnimate%20exhibits%20the%20proficiency%20to%20generate%20videos%20with%20144%20frames.%20We%0Aprovide%20a%20holistic%20ecosystem%20for%20video%20production%20based%20on%20DiT%2C%20encompassing%0Aaspects%20such%20as%20data%20pre-processing%2C%20VAE%20training%2C%20DiT%20models%20training%20%28both%0Athe%20baseline%20model%20and%20LoRA%20model%29%2C%20and%20end-to-end%20video%20inference.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/aigc-apps/EasyAnimate.%20We%20are%20continuously%0Aworking%20to%20enhance%20the%20performance%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18991v2&entry.124074799=Read"},
{"title": "VCD-Texture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided\n  Texturing", "author": "Shang Liu and Chaohui Yu and Chenjie Cao and Wen Qian and Fan Wang", "abstract": "  Recent research on texture synthesis for 3D shapes benefits a lot from\ndramatically developed 2D text-to-image diffusion models, including\ninpainting-based and optimization-based approaches. However, these methods\nignore the modal gap between the 2D diffusion model and 3D objects, which\nprimarily render 3D objects into 2D images and texture each image separately.\nIn this paper, we revisit the texture synthesis and propose a Variance\nalignment based 3D-2D Collaborative Denoising framework, dubbed VCD-Texture, to\naddress these issues. Formally, we first unify both 2D and 3D latent feature\nlearning in diffusion self-attention modules with re-projected 3D attention\nreceptive fields. Subsequently, the denoised multi-view 2D latent features are\naggregated into 3D space and then rasterized back to formulate more consistent\n2D predictions. However, the rasterization process suffers from an intractable\nvariance bias, which is theoretically addressed by the proposed variance\nalignment, achieving high-fidelity texture synthesis. Moreover, we present an\ninpainting refinement to further improve the details with conflicting regions.\nNotably, there is not a publicly available benchmark to evaluate texture\nsynthesis, which hinders its development. Thus we construct a new evaluation\nset built upon three open-source 3D datasets and propose to use four metrics to\nthoroughly validate the texturing performance. Comprehensive experiments\ndemonstrate that VCD-Texture achieves superior performance against other\ncounterparts.\n", "link": "http://arxiv.org/abs/2407.04461v1", "date": "2024-07-05", "relevancy": 2.4564, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6206}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6194}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCD-Texture%3A%20Variance%20Alignment%20based%203D-2D%20Co-Denoising%20for%20Text-Guided%0A%20%20Texturing&body=Title%3A%20VCD-Texture%3A%20Variance%20Alignment%20based%203D-2D%20Co-Denoising%20for%20Text-Guided%0A%20%20Texturing%0AAuthor%3A%20Shang%20Liu%20and%20Chaohui%20Yu%20and%20Chenjie%20Cao%20and%20Wen%20Qian%20and%20Fan%20Wang%0AAbstract%3A%20%20%20Recent%20research%20on%20texture%20synthesis%20for%203D%20shapes%20benefits%20a%20lot%20from%0Adramatically%20developed%202D%20text-to-image%20diffusion%20models%2C%20including%0Ainpainting-based%20and%20optimization-based%20approaches.%20However%2C%20these%20methods%0Aignore%20the%20modal%20gap%20between%20the%202D%20diffusion%20model%20and%203D%20objects%2C%20which%0Aprimarily%20render%203D%20objects%20into%202D%20images%20and%20texture%20each%20image%20separately.%0AIn%20this%20paper%2C%20we%20revisit%20the%20texture%20synthesis%20and%20propose%20a%20Variance%0Aalignment%20based%203D-2D%20Collaborative%20Denoising%20framework%2C%20dubbed%20VCD-Texture%2C%20to%0Aaddress%20these%20issues.%20Formally%2C%20we%20first%20unify%20both%202D%20and%203D%20latent%20feature%0Alearning%20in%20diffusion%20self-attention%20modules%20with%20re-projected%203D%20attention%0Areceptive%20fields.%20Subsequently%2C%20the%20denoised%20multi-view%202D%20latent%20features%20are%0Aaggregated%20into%203D%20space%20and%20then%20rasterized%20back%20to%20formulate%20more%20consistent%0A2D%20predictions.%20However%2C%20the%20rasterization%20process%20suffers%20from%20an%20intractable%0Avariance%20bias%2C%20which%20is%20theoretically%20addressed%20by%20the%20proposed%20variance%0Aalignment%2C%20achieving%20high-fidelity%20texture%20synthesis.%20Moreover%2C%20we%20present%20an%0Ainpainting%20refinement%20to%20further%20improve%20the%20details%20with%20conflicting%20regions.%0ANotably%2C%20there%20is%20not%20a%20publicly%20available%20benchmark%20to%20evaluate%20texture%0Asynthesis%2C%20which%20hinders%20its%20development.%20Thus%20we%20construct%20a%20new%20evaluation%0Aset%20built%20upon%20three%20open-source%203D%20datasets%20and%20propose%20to%20use%20four%20metrics%20to%0Athoroughly%20validate%20the%20texturing%20performance.%20Comprehensive%20experiments%0Ademonstrate%20that%20VCD-Texture%20achieves%20superior%20performance%20against%20other%0Acounterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCD-Texture%253A%2520Variance%2520Alignment%2520based%25203D-2D%2520Co-Denoising%2520for%2520Text-Guided%250A%2520%2520Texturing%26entry.906535625%3DShang%2520Liu%2520and%2520Chaohui%2520Yu%2520and%2520Chenjie%2520Cao%2520and%2520Wen%2520Qian%2520and%2520Fan%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520research%2520on%2520texture%2520synthesis%2520for%25203D%2520shapes%2520benefits%2520a%2520lot%2520from%250Adramatically%2520developed%25202D%2520text-to-image%2520diffusion%2520models%252C%2520including%250Ainpainting-based%2520and%2520optimization-based%2520approaches.%2520However%252C%2520these%2520methods%250Aignore%2520the%2520modal%2520gap%2520between%2520the%25202D%2520diffusion%2520model%2520and%25203D%2520objects%252C%2520which%250Aprimarily%2520render%25203D%2520objects%2520into%25202D%2520images%2520and%2520texture%2520each%2520image%2520separately.%250AIn%2520this%2520paper%252C%2520we%2520revisit%2520the%2520texture%2520synthesis%2520and%2520propose%2520a%2520Variance%250Aalignment%2520based%25203D-2D%2520Collaborative%2520Denoising%2520framework%252C%2520dubbed%2520VCD-Texture%252C%2520to%250Aaddress%2520these%2520issues.%2520Formally%252C%2520we%2520first%2520unify%2520both%25202D%2520and%25203D%2520latent%2520feature%250Alearning%2520in%2520diffusion%2520self-attention%2520modules%2520with%2520re-projected%25203D%2520attention%250Areceptive%2520fields.%2520Subsequently%252C%2520the%2520denoised%2520multi-view%25202D%2520latent%2520features%2520are%250Aaggregated%2520into%25203D%2520space%2520and%2520then%2520rasterized%2520back%2520to%2520formulate%2520more%2520consistent%250A2D%2520predictions.%2520However%252C%2520the%2520rasterization%2520process%2520suffers%2520from%2520an%2520intractable%250Avariance%2520bias%252C%2520which%2520is%2520theoretically%2520addressed%2520by%2520the%2520proposed%2520variance%250Aalignment%252C%2520achieving%2520high-fidelity%2520texture%2520synthesis.%2520Moreover%252C%2520we%2520present%2520an%250Ainpainting%2520refinement%2520to%2520further%2520improve%2520the%2520details%2520with%2520conflicting%2520regions.%250ANotably%252C%2520there%2520is%2520not%2520a%2520publicly%2520available%2520benchmark%2520to%2520evaluate%2520texture%250Asynthesis%252C%2520which%2520hinders%2520its%2520development.%2520Thus%2520we%2520construct%2520a%2520new%2520evaluation%250Aset%2520built%2520upon%2520three%2520open-source%25203D%2520datasets%2520and%2520propose%2520to%2520use%2520four%2520metrics%2520to%250Athoroughly%2520validate%2520the%2520texturing%2520performance.%2520Comprehensive%2520experiments%250Ademonstrate%2520that%2520VCD-Texture%2520achieves%2520superior%2520performance%2520against%2520other%250Acounterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCD-Texture%3A%20Variance%20Alignment%20based%203D-2D%20Co-Denoising%20for%20Text-Guided%0A%20%20Texturing&entry.906535625=Shang%20Liu%20and%20Chaohui%20Yu%20and%20Chenjie%20Cao%20and%20Wen%20Qian%20and%20Fan%20Wang&entry.1292438233=%20%20Recent%20research%20on%20texture%20synthesis%20for%203D%20shapes%20benefits%20a%20lot%20from%0Adramatically%20developed%202D%20text-to-image%20diffusion%20models%2C%20including%0Ainpainting-based%20and%20optimization-based%20approaches.%20However%2C%20these%20methods%0Aignore%20the%20modal%20gap%20between%20the%202D%20diffusion%20model%20and%203D%20objects%2C%20which%0Aprimarily%20render%203D%20objects%20into%202D%20images%20and%20texture%20each%20image%20separately.%0AIn%20this%20paper%2C%20we%20revisit%20the%20texture%20synthesis%20and%20propose%20a%20Variance%0Aalignment%20based%203D-2D%20Collaborative%20Denoising%20framework%2C%20dubbed%20VCD-Texture%2C%20to%0Aaddress%20these%20issues.%20Formally%2C%20we%20first%20unify%20both%202D%20and%203D%20latent%20feature%0Alearning%20in%20diffusion%20self-attention%20modules%20with%20re-projected%203D%20attention%0Areceptive%20fields.%20Subsequently%2C%20the%20denoised%20multi-view%202D%20latent%20features%20are%0Aaggregated%20into%203D%20space%20and%20then%20rasterized%20back%20to%20formulate%20more%20consistent%0A2D%20predictions.%20However%2C%20the%20rasterization%20process%20suffers%20from%20an%20intractable%0Avariance%20bias%2C%20which%20is%20theoretically%20addressed%20by%20the%20proposed%20variance%0Aalignment%2C%20achieving%20high-fidelity%20texture%20synthesis.%20Moreover%2C%20we%20present%20an%0Ainpainting%20refinement%20to%20further%20improve%20the%20details%20with%20conflicting%20regions.%0ANotably%2C%20there%20is%20not%20a%20publicly%20available%20benchmark%20to%20evaluate%20texture%0Asynthesis%2C%20which%20hinders%20its%20development.%20Thus%20we%20construct%20a%20new%20evaluation%0Aset%20built%20upon%20three%20open-source%203D%20datasets%20and%20propose%20to%20use%20four%20metrics%20to%0Athoroughly%20validate%20the%20texturing%20performance.%20Comprehensive%20experiments%0Ademonstrate%20that%20VCD-Texture%20achieves%20superior%20performance%20against%20other%0Acounterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04461v1&entry.124074799=Read"},
{"title": "DexDiffuser: Generating Dexterous Grasps with Diffusion Models", "author": "Zehang Weng and Haofei Lu and Danica Kragic and Jens Lundell", "abstract": "  We introduce DexDiffuser, a novel dexterous grasping method that generates,\nevaluates, and refines grasps on partial object point clouds. DexDiffuser\nincludes the conditional diffusion-based grasp sampler DexSampler and the\ndexterous grasp evaluator DexEvaluator. DexSampler generates high-quality\ngrasps conditioned on object point clouds by iterative denoising of randomly\nsampled grasps. We also introduce two grasp refinement strategies:\nEvaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR).\nThe experiment results demonstrate that DexDiffuser consistently outperforms\nthe state-of-the-art multi-finger grasp generation method FFHNet with an, on\naverage, 9.12% and 19.44% higher grasp success rate in simulation and real\nrobot experiments, respectively. Supplementary materials are available at\nhttps://yulihn.github.io/DexDiffuser_page/\n", "link": "http://arxiv.org/abs/2402.02989v2", "date": "2024-07-05", "relevancy": 2.4197, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.7236}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5601}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexDiffuser%3A%20Generating%20Dexterous%20Grasps%20with%20Diffusion%20Models&body=Title%3A%20DexDiffuser%3A%20Generating%20Dexterous%20Grasps%20with%20Diffusion%20Models%0AAuthor%3A%20Zehang%20Weng%20and%20Haofei%20Lu%20and%20Danica%20Kragic%20and%20Jens%20Lundell%0AAbstract%3A%20%20%20We%20introduce%20DexDiffuser%2C%20a%20novel%20dexterous%20grasping%20method%20that%20generates%2C%0Aevaluates%2C%20and%20refines%20grasps%20on%20partial%20object%20point%20clouds.%20DexDiffuser%0Aincludes%20the%20conditional%20diffusion-based%20grasp%20sampler%20DexSampler%20and%20the%0Adexterous%20grasp%20evaluator%20DexEvaluator.%20DexSampler%20generates%20high-quality%0Agrasps%20conditioned%20on%20object%20point%20clouds%20by%20iterative%20denoising%20of%20randomly%0Asampled%20grasps.%20We%20also%20introduce%20two%20grasp%20refinement%20strategies%3A%0AEvaluator-Guided%20Diffusion%20%28EGD%29%20and%20Evaluator-based%20Sampling%20Refinement%20%28ESR%29.%0AThe%20experiment%20results%20demonstrate%20that%20DexDiffuser%20consistently%20outperforms%0Athe%20state-of-the-art%20multi-finger%20grasp%20generation%20method%20FFHNet%20with%20an%2C%20on%0Aaverage%2C%209.12%25%20and%2019.44%25%20higher%20grasp%20success%20rate%20in%20simulation%20and%20real%0Arobot%20experiments%2C%20respectively.%20Supplementary%20materials%20are%20available%20at%0Ahttps%3A//yulihn.github.io/DexDiffuser_page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02989v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexDiffuser%253A%2520Generating%2520Dexterous%2520Grasps%2520with%2520Diffusion%2520Models%26entry.906535625%3DZehang%2520Weng%2520and%2520Haofei%2520Lu%2520and%2520Danica%2520Kragic%2520and%2520Jens%2520Lundell%26entry.1292438233%3D%2520%2520We%2520introduce%2520DexDiffuser%252C%2520a%2520novel%2520dexterous%2520grasping%2520method%2520that%2520generates%252C%250Aevaluates%252C%2520and%2520refines%2520grasps%2520on%2520partial%2520object%2520point%2520clouds.%2520DexDiffuser%250Aincludes%2520the%2520conditional%2520diffusion-based%2520grasp%2520sampler%2520DexSampler%2520and%2520the%250Adexterous%2520grasp%2520evaluator%2520DexEvaluator.%2520DexSampler%2520generates%2520high-quality%250Agrasps%2520conditioned%2520on%2520object%2520point%2520clouds%2520by%2520iterative%2520denoising%2520of%2520randomly%250Asampled%2520grasps.%2520We%2520also%2520introduce%2520two%2520grasp%2520refinement%2520strategies%253A%250AEvaluator-Guided%2520Diffusion%2520%2528EGD%2529%2520and%2520Evaluator-based%2520Sampling%2520Refinement%2520%2528ESR%2529.%250AThe%2520experiment%2520results%2520demonstrate%2520that%2520DexDiffuser%2520consistently%2520outperforms%250Athe%2520state-of-the-art%2520multi-finger%2520grasp%2520generation%2520method%2520FFHNet%2520with%2520an%252C%2520on%250Aaverage%252C%25209.12%2525%2520and%252019.44%2525%2520higher%2520grasp%2520success%2520rate%2520in%2520simulation%2520and%2520real%250Arobot%2520experiments%252C%2520respectively.%2520Supplementary%2520materials%2520are%2520available%2520at%250Ahttps%253A//yulihn.github.io/DexDiffuser_page/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02989v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexDiffuser%3A%20Generating%20Dexterous%20Grasps%20with%20Diffusion%20Models&entry.906535625=Zehang%20Weng%20and%20Haofei%20Lu%20and%20Danica%20Kragic%20and%20Jens%20Lundell&entry.1292438233=%20%20We%20introduce%20DexDiffuser%2C%20a%20novel%20dexterous%20grasping%20method%20that%20generates%2C%0Aevaluates%2C%20and%20refines%20grasps%20on%20partial%20object%20point%20clouds.%20DexDiffuser%0Aincludes%20the%20conditional%20diffusion-based%20grasp%20sampler%20DexSampler%20and%20the%0Adexterous%20grasp%20evaluator%20DexEvaluator.%20DexSampler%20generates%20high-quality%0Agrasps%20conditioned%20on%20object%20point%20clouds%20by%20iterative%20denoising%20of%20randomly%0Asampled%20grasps.%20We%20also%20introduce%20two%20grasp%20refinement%20strategies%3A%0AEvaluator-Guided%20Diffusion%20%28EGD%29%20and%20Evaluator-based%20Sampling%20Refinement%20%28ESR%29.%0AThe%20experiment%20results%20demonstrate%20that%20DexDiffuser%20consistently%20outperforms%0Athe%20state-of-the-art%20multi-finger%20grasp%20generation%20method%20FFHNet%20with%20an%2C%20on%0Aaverage%2C%209.12%25%20and%2019.44%25%20higher%20grasp%20success%20rate%20in%20simulation%20and%20real%0Arobot%20experiments%2C%20respectively.%20Supplementary%20materials%20are%20available%20at%0Ahttps%3A//yulihn.github.io/DexDiffuser_page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02989v2&entry.124074799=Read"},
{"title": "Enhancing Vehicle Re-identification and Matching for Weaving Analysis", "author": "Mei Qiu and Wei Lin and Stanley Chien and Lauren Christopher and Yaobin Chen and Shu Hu", "abstract": "  Vehicle weaving on highways contributes to traffic congestion, raises safety\nissues, and underscores the need for sophisticated traffic management systems.\nCurrent tools are inadequate in offering precise and comprehensive data on\nlane-specific weaving patterns. This paper introduces an innovative method for\ncollecting non-overlapping video data in weaving zones, enabling the generation\nof quantitative insights into lane-specific weaving behaviors. Our experimental\nresults confirm the efficacy of this approach, delivering critical data that\ncan assist transportation authorities in enhancing traffic control and roadway\ninfrastructure.\n", "link": "http://arxiv.org/abs/2407.04688v1", "date": "2024-07-05", "relevancy": 2.4149, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4997}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4841}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vehicle%20Re-identification%20and%20Matching%20for%20Weaving%20Analysis&body=Title%3A%20Enhancing%20Vehicle%20Re-identification%20and%20Matching%20for%20Weaving%20Analysis%0AAuthor%3A%20Mei%20Qiu%20and%20Wei%20Lin%20and%20Stanley%20Chien%20and%20Lauren%20Christopher%20and%20Yaobin%20Chen%20and%20Shu%20Hu%0AAbstract%3A%20%20%20Vehicle%20weaving%20on%20highways%20contributes%20to%20traffic%20congestion%2C%20raises%20safety%0Aissues%2C%20and%20underscores%20the%20need%20for%20sophisticated%20traffic%20management%20systems.%0ACurrent%20tools%20are%20inadequate%20in%20offering%20precise%20and%20comprehensive%20data%20on%0Alane-specific%20weaving%20patterns.%20This%20paper%20introduces%20an%20innovative%20method%20for%0Acollecting%20non-overlapping%20video%20data%20in%20weaving%20zones%2C%20enabling%20the%20generation%0Aof%20quantitative%20insights%20into%20lane-specific%20weaving%20behaviors.%20Our%20experimental%0Aresults%20confirm%20the%20efficacy%20of%20this%20approach%2C%20delivering%20critical%20data%20that%0Acan%20assist%20transportation%20authorities%20in%20enhancing%20traffic%20control%20and%20roadway%0Ainfrastructure.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04688v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vehicle%2520Re-identification%2520and%2520Matching%2520for%2520Weaving%2520Analysis%26entry.906535625%3DMei%2520Qiu%2520and%2520Wei%2520Lin%2520and%2520Stanley%2520Chien%2520and%2520Lauren%2520Christopher%2520and%2520Yaobin%2520Chen%2520and%2520Shu%2520Hu%26entry.1292438233%3D%2520%2520Vehicle%2520weaving%2520on%2520highways%2520contributes%2520to%2520traffic%2520congestion%252C%2520raises%2520safety%250Aissues%252C%2520and%2520underscores%2520the%2520need%2520for%2520sophisticated%2520traffic%2520management%2520systems.%250ACurrent%2520tools%2520are%2520inadequate%2520in%2520offering%2520precise%2520and%2520comprehensive%2520data%2520on%250Alane-specific%2520weaving%2520patterns.%2520This%2520paper%2520introduces%2520an%2520innovative%2520method%2520for%250Acollecting%2520non-overlapping%2520video%2520data%2520in%2520weaving%2520zones%252C%2520enabling%2520the%2520generation%250Aof%2520quantitative%2520insights%2520into%2520lane-specific%2520weaving%2520behaviors.%2520Our%2520experimental%250Aresults%2520confirm%2520the%2520efficacy%2520of%2520this%2520approach%252C%2520delivering%2520critical%2520data%2520that%250Acan%2520assist%2520transportation%2520authorities%2520in%2520enhancing%2520traffic%2520control%2520and%2520roadway%250Ainfrastructure.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04688v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vehicle%20Re-identification%20and%20Matching%20for%20Weaving%20Analysis&entry.906535625=Mei%20Qiu%20and%20Wei%20Lin%20and%20Stanley%20Chien%20and%20Lauren%20Christopher%20and%20Yaobin%20Chen%20and%20Shu%20Hu&entry.1292438233=%20%20Vehicle%20weaving%20on%20highways%20contributes%20to%20traffic%20congestion%2C%20raises%20safety%0Aissues%2C%20and%20underscores%20the%20need%20for%20sophisticated%20traffic%20management%20systems.%0ACurrent%20tools%20are%20inadequate%20in%20offering%20precise%20and%20comprehensive%20data%20on%0Alane-specific%20weaving%20patterns.%20This%20paper%20introduces%20an%20innovative%20method%20for%0Acollecting%20non-overlapping%20video%20data%20in%20weaving%20zones%2C%20enabling%20the%20generation%0Aof%20quantitative%20insights%20into%20lane-specific%20weaving%20behaviors.%20Our%20experimental%0Aresults%20confirm%20the%20efficacy%20of%20this%20approach%2C%20delivering%20critical%20data%20that%0Acan%20assist%20transportation%20authorities%20in%20enhancing%20traffic%20control%20and%20roadway%0Ainfrastructure.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04688v1&entry.124074799=Read"},
{"title": "Smart Sampling: Helping from Friendly Neighbors for Decentralized\n  Federated Learning", "author": "Lin Wang and Yang Chen and Yongxin Guo and Xiaoying Tang", "abstract": "  Federated Learning (FL) is gaining widespread interest for its ability to\nshare knowledge while preserving privacy and reducing communication costs.\nUnlike Centralized FL, Decentralized FL (DFL) employs a network architecture\nthat eliminates the need for a central server, allowing direct communication\namong clients and leading to significant communication resource savings.\nHowever, due to data heterogeneity, not all neighboring nodes contribute to\nenhancing the local client's model performance. In this work, we introduce\n\\textbf{\\emph{AFIND+}}, a simple yet efficient algorithm for sampling and\naggregating neighbors in DFL, with the aim of leveraging collaboration to\nimprove clients' model performance. AFIND+ identifies helpful neighbors,\nadaptively adjusts the number of selected neighbors, and strategically\naggregates the sampled neighbors' models based on their contributions.\nNumerical results on real-world datasets with diverse data partitions\ndemonstrate that AFIND+ outperforms other sampling algorithms in DFL and is\ncompatible with most existing DFL optimization algorithms.\n", "link": "http://arxiv.org/abs/2407.04460v1", "date": "2024-07-05", "relevancy": 2.3874, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4993}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4758}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smart%20Sampling%3A%20Helping%20from%20Friendly%20Neighbors%20for%20Decentralized%0A%20%20Federated%20Learning&body=Title%3A%20Smart%20Sampling%3A%20Helping%20from%20Friendly%20Neighbors%20for%20Decentralized%0A%20%20Federated%20Learning%0AAuthor%3A%20Lin%20Wang%20and%20Yang%20Chen%20and%20Yongxin%20Guo%20and%20Xiaoying%20Tang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20gaining%20widespread%20interest%20for%20its%20ability%20to%0Ashare%20knowledge%20while%20preserving%20privacy%20and%20reducing%20communication%20costs.%0AUnlike%20Centralized%20FL%2C%20Decentralized%20FL%20%28DFL%29%20employs%20a%20network%20architecture%0Athat%20eliminates%20the%20need%20for%20a%20central%20server%2C%20allowing%20direct%20communication%0Aamong%20clients%20and%20leading%20to%20significant%20communication%20resource%20savings.%0AHowever%2C%20due%20to%20data%20heterogeneity%2C%20not%20all%20neighboring%20nodes%20contribute%20to%0Aenhancing%20the%20local%20client%27s%20model%20performance.%20In%20this%20work%2C%20we%20introduce%0A%5Ctextbf%7B%5Cemph%7BAFIND%2B%7D%7D%2C%20a%20simple%20yet%20efficient%20algorithm%20for%20sampling%20and%0Aaggregating%20neighbors%20in%20DFL%2C%20with%20the%20aim%20of%20leveraging%20collaboration%20to%0Aimprove%20clients%27%20model%20performance.%20AFIND%2B%20identifies%20helpful%20neighbors%2C%0Aadaptively%20adjusts%20the%20number%20of%20selected%20neighbors%2C%20and%20strategically%0Aaggregates%20the%20sampled%20neighbors%27%20models%20based%20on%20their%20contributions.%0ANumerical%20results%20on%20real-world%20datasets%20with%20diverse%20data%20partitions%0Ademonstrate%20that%20AFIND%2B%20outperforms%20other%20sampling%20algorithms%20in%20DFL%20and%20is%0Acompatible%20with%20most%20existing%20DFL%20optimization%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmart%2520Sampling%253A%2520Helping%2520from%2520Friendly%2520Neighbors%2520for%2520Decentralized%250A%2520%2520Federated%2520Learning%26entry.906535625%3DLin%2520Wang%2520and%2520Yang%2520Chen%2520and%2520Yongxin%2520Guo%2520and%2520Xiaoying%2520Tang%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520gaining%2520widespread%2520interest%2520for%2520its%2520ability%2520to%250Ashare%2520knowledge%2520while%2520preserving%2520privacy%2520and%2520reducing%2520communication%2520costs.%250AUnlike%2520Centralized%2520FL%252C%2520Decentralized%2520FL%2520%2528DFL%2529%2520employs%2520a%2520network%2520architecture%250Athat%2520eliminates%2520the%2520need%2520for%2520a%2520central%2520server%252C%2520allowing%2520direct%2520communication%250Aamong%2520clients%2520and%2520leading%2520to%2520significant%2520communication%2520resource%2520savings.%250AHowever%252C%2520due%2520to%2520data%2520heterogeneity%252C%2520not%2520all%2520neighboring%2520nodes%2520contribute%2520to%250Aenhancing%2520the%2520local%2520client%2527s%2520model%2520performance.%2520In%2520this%2520work%252C%2520we%2520introduce%250A%255Ctextbf%257B%255Cemph%257BAFIND%252B%257D%257D%252C%2520a%2520simple%2520yet%2520efficient%2520algorithm%2520for%2520sampling%2520and%250Aaggregating%2520neighbors%2520in%2520DFL%252C%2520with%2520the%2520aim%2520of%2520leveraging%2520collaboration%2520to%250Aimprove%2520clients%2527%2520model%2520performance.%2520AFIND%252B%2520identifies%2520helpful%2520neighbors%252C%250Aadaptively%2520adjusts%2520the%2520number%2520of%2520selected%2520neighbors%252C%2520and%2520strategically%250Aaggregates%2520the%2520sampled%2520neighbors%2527%2520models%2520based%2520on%2520their%2520contributions.%250ANumerical%2520results%2520on%2520real-world%2520datasets%2520with%2520diverse%2520data%2520partitions%250Ademonstrate%2520that%2520AFIND%252B%2520outperforms%2520other%2520sampling%2520algorithms%2520in%2520DFL%2520and%2520is%250Acompatible%2520with%2520most%2520existing%2520DFL%2520optimization%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smart%20Sampling%3A%20Helping%20from%20Friendly%20Neighbors%20for%20Decentralized%0A%20%20Federated%20Learning&entry.906535625=Lin%20Wang%20and%20Yang%20Chen%20and%20Yongxin%20Guo%20and%20Xiaoying%20Tang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20gaining%20widespread%20interest%20for%20its%20ability%20to%0Ashare%20knowledge%20while%20preserving%20privacy%20and%20reducing%20communication%20costs.%0AUnlike%20Centralized%20FL%2C%20Decentralized%20FL%20%28DFL%29%20employs%20a%20network%20architecture%0Athat%20eliminates%20the%20need%20for%20a%20central%20server%2C%20allowing%20direct%20communication%0Aamong%20clients%20and%20leading%20to%20significant%20communication%20resource%20savings.%0AHowever%2C%20due%20to%20data%20heterogeneity%2C%20not%20all%20neighboring%20nodes%20contribute%20to%0Aenhancing%20the%20local%20client%27s%20model%20performance.%20In%20this%20work%2C%20we%20introduce%0A%5Ctextbf%7B%5Cemph%7BAFIND%2B%7D%7D%2C%20a%20simple%20yet%20efficient%20algorithm%20for%20sampling%20and%0Aaggregating%20neighbors%20in%20DFL%2C%20with%20the%20aim%20of%20leveraging%20collaboration%20to%0Aimprove%20clients%27%20model%20performance.%20AFIND%2B%20identifies%20helpful%20neighbors%2C%0Aadaptively%20adjusts%20the%20number%20of%20selected%20neighbors%2C%20and%20strategically%0Aaggregates%20the%20sampled%20neighbors%27%20models%20based%20on%20their%20contributions.%0ANumerical%20results%20on%20real-world%20datasets%20with%20diverse%20data%20partitions%0Ademonstrate%20that%20AFIND%2B%20outperforms%20other%20sampling%20algorithms%20in%20DFL%20and%20is%0Acompatible%20with%20most%20existing%20DFL%20optimization%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04460v1&entry.124074799=Read"},
{"title": "LaRa: Efficient Large-Baseline Radiance Fields", "author": "Anpei Chen and Haofei Xu and Stefano Esposito and Siyu Tang and Andreas Geiger", "abstract": "  Radiance field methods have achieved photorealistic novel view synthesis and\ngeometry reconstruction. But they are mostly applied in per-scene optimization\nor small-baseline settings. While several recent works investigate feed-forward\nreconstruction with large baselines by utilizing transformers, they all operate\nwith a standard global attention mechanism and hence ignore the local nature of\n3D reconstruction. We propose a method that unifies local and global reasoning\nin transformer layers, resulting in improved quality and faster convergence.\nOur model represents scenes as Gaussian Volumes and combines this with an image\nencoder and Group Attention Layers for efficient feed-forward reconstruction.\nExperimental results demonstrate that our model, trained for two days on four\nGPUs, demonstrates high fidelity in reconstructing 360&deg radiance fields, and\nrobustness to zero-shot and out-of-domain testing.\n", "link": "http://arxiv.org/abs/2407.04699v1", "date": "2024-07-05", "relevancy": 2.3838, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6461}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5774}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaRa%3A%20Efficient%20Large-Baseline%20Radiance%20Fields&body=Title%3A%20LaRa%3A%20Efficient%20Large-Baseline%20Radiance%20Fields%0AAuthor%3A%20Anpei%20Chen%20and%20Haofei%20Xu%20and%20Stefano%20Esposito%20and%20Siyu%20Tang%20and%20Andreas%20Geiger%0AAbstract%3A%20%20%20Radiance%20field%20methods%20have%20achieved%20photorealistic%20novel%20view%20synthesis%20and%0Ageometry%20reconstruction.%20But%20they%20are%20mostly%20applied%20in%20per-scene%20optimization%0Aor%20small-baseline%20settings.%20While%20several%20recent%20works%20investigate%20feed-forward%0Areconstruction%20with%20large%20baselines%20by%20utilizing%20transformers%2C%20they%20all%20operate%0Awith%20a%20standard%20global%20attention%20mechanism%20and%20hence%20ignore%20the%20local%20nature%20of%0A3D%20reconstruction.%20We%20propose%20a%20method%20that%20unifies%20local%20and%20global%20reasoning%0Ain%20transformer%20layers%2C%20resulting%20in%20improved%20quality%20and%20faster%20convergence.%0AOur%20model%20represents%20scenes%20as%20Gaussian%20Volumes%20and%20combines%20this%20with%20an%20image%0Aencoder%20and%20Group%20Attention%20Layers%20for%20efficient%20feed-forward%20reconstruction.%0AExperimental%20results%20demonstrate%20that%20our%20model%2C%20trained%20for%20two%20days%20on%20four%0AGPUs%2C%20demonstrates%20high%20fidelity%20in%20reconstructing%20360%26deg%20radiance%20fields%2C%20and%0Arobustness%20to%20zero-shot%20and%20out-of-domain%20testing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04699v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaRa%253A%2520Efficient%2520Large-Baseline%2520Radiance%2520Fields%26entry.906535625%3DAnpei%2520Chen%2520and%2520Haofei%2520Xu%2520and%2520Stefano%2520Esposito%2520and%2520Siyu%2520Tang%2520and%2520Andreas%2520Geiger%26entry.1292438233%3D%2520%2520Radiance%2520field%2520methods%2520have%2520achieved%2520photorealistic%2520novel%2520view%2520synthesis%2520and%250Ageometry%2520reconstruction.%2520But%2520they%2520are%2520mostly%2520applied%2520in%2520per-scene%2520optimization%250Aor%2520small-baseline%2520settings.%2520While%2520several%2520recent%2520works%2520investigate%2520feed-forward%250Areconstruction%2520with%2520large%2520baselines%2520by%2520utilizing%2520transformers%252C%2520they%2520all%2520operate%250Awith%2520a%2520standard%2520global%2520attention%2520mechanism%2520and%2520hence%2520ignore%2520the%2520local%2520nature%2520of%250A3D%2520reconstruction.%2520We%2520propose%2520a%2520method%2520that%2520unifies%2520local%2520and%2520global%2520reasoning%250Ain%2520transformer%2520layers%252C%2520resulting%2520in%2520improved%2520quality%2520and%2520faster%2520convergence.%250AOur%2520model%2520represents%2520scenes%2520as%2520Gaussian%2520Volumes%2520and%2520combines%2520this%2520with%2520an%2520image%250Aencoder%2520and%2520Group%2520Attention%2520Layers%2520for%2520efficient%2520feed-forward%2520reconstruction.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520model%252C%2520trained%2520for%2520two%2520days%2520on%2520four%250AGPUs%252C%2520demonstrates%2520high%2520fidelity%2520in%2520reconstructing%2520360%2526deg%2520radiance%2520fields%252C%2520and%250Arobustness%2520to%2520zero-shot%2520and%2520out-of-domain%2520testing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04699v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaRa%3A%20Efficient%20Large-Baseline%20Radiance%20Fields&entry.906535625=Anpei%20Chen%20and%20Haofei%20Xu%20and%20Stefano%20Esposito%20and%20Siyu%20Tang%20and%20Andreas%20Geiger&entry.1292438233=%20%20Radiance%20field%20methods%20have%20achieved%20photorealistic%20novel%20view%20synthesis%20and%0Ageometry%20reconstruction.%20But%20they%20are%20mostly%20applied%20in%20per-scene%20optimization%0Aor%20small-baseline%20settings.%20While%20several%20recent%20works%20investigate%20feed-forward%0Areconstruction%20with%20large%20baselines%20by%20utilizing%20transformers%2C%20they%20all%20operate%0Awith%20a%20standard%20global%20attention%20mechanism%20and%20hence%20ignore%20the%20local%20nature%20of%0A3D%20reconstruction.%20We%20propose%20a%20method%20that%20unifies%20local%20and%20global%20reasoning%0Ain%20transformer%20layers%2C%20resulting%20in%20improved%20quality%20and%20faster%20convergence.%0AOur%20model%20represents%20scenes%20as%20Gaussian%20Volumes%20and%20combines%20this%20with%20an%20image%0Aencoder%20and%20Group%20Attention%20Layers%20for%20efficient%20feed-forward%20reconstruction.%0AExperimental%20results%20demonstrate%20that%20our%20model%2C%20trained%20for%20two%20days%20on%20four%0AGPUs%2C%20demonstrates%20high%20fidelity%20in%20reconstructing%20360%26deg%20radiance%20fields%2C%20and%0Arobustness%20to%20zero-shot%20and%20out-of-domain%20testing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04699v1&entry.124074799=Read"},
{"title": "An autoencoder for compressing angle-resolved photoemission spectroscopy\n  data", "author": "Steinn Ymir Agustsson and Mohammad Ahsanul Haque and Thi Tam Truong and Marco Bianchi and Nikita Klyuchnikov and Davide Mottin and Panagiotis Karras and Philip Hofmann", "abstract": "  Angle-resolved photoemission spectroscopy (ARPES) is a powerful experimental\ntechnique to determine the electronic structure of solids. Advances in light\nsources for ARPES experiments are currently leading to a vast increase of data\nacquisition rates and data quantity. On the other hand, access time to the most\nadvanced ARPES instruments remains strictly limited, calling for fast,\neffective, and on-the-fly data analysis tools to exploit this time. In response\nto this need, we introduce ARPESNet, a versatile autoencoder network that\nefficiently summmarises and compresses ARPES datasets. We train ARPESNet on a\nlarge and varied dataset of 2-dimensional ARPES data extracted by cutting\nstandard 3-dimensional ARPES datasets along random directions in $\\mathbf{k}$.\nTo test the data representation capacity of ARPESNet, we compare $k$-means\nclustering quality between data compressed by ARPESNet, data compressed by\ndiscrete cosine transform, and raw data, at different noise levels. ARPESNet\ndata excels in clustering quality despite its high compression ratio.\n", "link": "http://arxiv.org/abs/2407.04631v1", "date": "2024-07-05", "relevancy": 2.3362, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5078}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.467}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20autoencoder%20for%20compressing%20angle-resolved%20photoemission%20spectroscopy%0A%20%20data&body=Title%3A%20An%20autoencoder%20for%20compressing%20angle-resolved%20photoemission%20spectroscopy%0A%20%20data%0AAuthor%3A%20Steinn%20Ymir%20Agustsson%20and%20Mohammad%20Ahsanul%20Haque%20and%20Thi%20Tam%20Truong%20and%20Marco%20Bianchi%20and%20Nikita%20Klyuchnikov%20and%20Davide%20Mottin%20and%20Panagiotis%20Karras%20and%20Philip%20Hofmann%0AAbstract%3A%20%20%20Angle-resolved%20photoemission%20spectroscopy%20%28ARPES%29%20is%20a%20powerful%20experimental%0Atechnique%20to%20determine%20the%20electronic%20structure%20of%20solids.%20Advances%20in%20light%0Asources%20for%20ARPES%20experiments%20are%20currently%20leading%20to%20a%20vast%20increase%20of%20data%0Aacquisition%20rates%20and%20data%20quantity.%20On%20the%20other%20hand%2C%20access%20time%20to%20the%20most%0Aadvanced%20ARPES%20instruments%20remains%20strictly%20limited%2C%20calling%20for%20fast%2C%0Aeffective%2C%20and%20on-the-fly%20data%20analysis%20tools%20to%20exploit%20this%20time.%20In%20response%0Ato%20this%20need%2C%20we%20introduce%20ARPESNet%2C%20a%20versatile%20autoencoder%20network%20that%0Aefficiently%20summmarises%20and%20compresses%20ARPES%20datasets.%20We%20train%20ARPESNet%20on%20a%0Alarge%20and%20varied%20dataset%20of%202-dimensional%20ARPES%20data%20extracted%20by%20cutting%0Astandard%203-dimensional%20ARPES%20datasets%20along%20random%20directions%20in%20%24%5Cmathbf%7Bk%7D%24.%0ATo%20test%20the%20data%20representation%20capacity%20of%20ARPESNet%2C%20we%20compare%20%24k%24-means%0Aclustering%20quality%20between%20data%20compressed%20by%20ARPESNet%2C%20data%20compressed%20by%0Adiscrete%20cosine%20transform%2C%20and%20raw%20data%2C%20at%20different%20noise%20levels.%20ARPESNet%0Adata%20excels%20in%20clustering%20quality%20despite%20its%20high%20compression%20ratio.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520autoencoder%2520for%2520compressing%2520angle-resolved%2520photoemission%2520spectroscopy%250A%2520%2520data%26entry.906535625%3DSteinn%2520Ymir%2520Agustsson%2520and%2520Mohammad%2520Ahsanul%2520Haque%2520and%2520Thi%2520Tam%2520Truong%2520and%2520Marco%2520Bianchi%2520and%2520Nikita%2520Klyuchnikov%2520and%2520Davide%2520Mottin%2520and%2520Panagiotis%2520Karras%2520and%2520Philip%2520Hofmann%26entry.1292438233%3D%2520%2520Angle-resolved%2520photoemission%2520spectroscopy%2520%2528ARPES%2529%2520is%2520a%2520powerful%2520experimental%250Atechnique%2520to%2520determine%2520the%2520electronic%2520structure%2520of%2520solids.%2520Advances%2520in%2520light%250Asources%2520for%2520ARPES%2520experiments%2520are%2520currently%2520leading%2520to%2520a%2520vast%2520increase%2520of%2520data%250Aacquisition%2520rates%2520and%2520data%2520quantity.%2520On%2520the%2520other%2520hand%252C%2520access%2520time%2520to%2520the%2520most%250Aadvanced%2520ARPES%2520instruments%2520remains%2520strictly%2520limited%252C%2520calling%2520for%2520fast%252C%250Aeffective%252C%2520and%2520on-the-fly%2520data%2520analysis%2520tools%2520to%2520exploit%2520this%2520time.%2520In%2520response%250Ato%2520this%2520need%252C%2520we%2520introduce%2520ARPESNet%252C%2520a%2520versatile%2520autoencoder%2520network%2520that%250Aefficiently%2520summmarises%2520and%2520compresses%2520ARPES%2520datasets.%2520We%2520train%2520ARPESNet%2520on%2520a%250Alarge%2520and%2520varied%2520dataset%2520of%25202-dimensional%2520ARPES%2520data%2520extracted%2520by%2520cutting%250Astandard%25203-dimensional%2520ARPES%2520datasets%2520along%2520random%2520directions%2520in%2520%2524%255Cmathbf%257Bk%257D%2524.%250ATo%2520test%2520the%2520data%2520representation%2520capacity%2520of%2520ARPESNet%252C%2520we%2520compare%2520%2524k%2524-means%250Aclustering%2520quality%2520between%2520data%2520compressed%2520by%2520ARPESNet%252C%2520data%2520compressed%2520by%250Adiscrete%2520cosine%2520transform%252C%2520and%2520raw%2520data%252C%2520at%2520different%2520noise%2520levels.%2520ARPESNet%250Adata%2520excels%2520in%2520clustering%2520quality%2520despite%2520its%2520high%2520compression%2520ratio.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20autoencoder%20for%20compressing%20angle-resolved%20photoemission%20spectroscopy%0A%20%20data&entry.906535625=Steinn%20Ymir%20Agustsson%20and%20Mohammad%20Ahsanul%20Haque%20and%20Thi%20Tam%20Truong%20and%20Marco%20Bianchi%20and%20Nikita%20Klyuchnikov%20and%20Davide%20Mottin%20and%20Panagiotis%20Karras%20and%20Philip%20Hofmann&entry.1292438233=%20%20Angle-resolved%20photoemission%20spectroscopy%20%28ARPES%29%20is%20a%20powerful%20experimental%0Atechnique%20to%20determine%20the%20electronic%20structure%20of%20solids.%20Advances%20in%20light%0Asources%20for%20ARPES%20experiments%20are%20currently%20leading%20to%20a%20vast%20increase%20of%20data%0Aacquisition%20rates%20and%20data%20quantity.%20On%20the%20other%20hand%2C%20access%20time%20to%20the%20most%0Aadvanced%20ARPES%20instruments%20remains%20strictly%20limited%2C%20calling%20for%20fast%2C%0Aeffective%2C%20and%20on-the-fly%20data%20analysis%20tools%20to%20exploit%20this%20time.%20In%20response%0Ato%20this%20need%2C%20we%20introduce%20ARPESNet%2C%20a%20versatile%20autoencoder%20network%20that%0Aefficiently%20summmarises%20and%20compresses%20ARPES%20datasets.%20We%20train%20ARPESNet%20on%20a%0Alarge%20and%20varied%20dataset%20of%202-dimensional%20ARPES%20data%20extracted%20by%20cutting%0Astandard%203-dimensional%20ARPES%20datasets%20along%20random%20directions%20in%20%24%5Cmathbf%7Bk%7D%24.%0ATo%20test%20the%20data%20representation%20capacity%20of%20ARPESNet%2C%20we%20compare%20%24k%24-means%0Aclustering%20quality%20between%20data%20compressed%20by%20ARPESNet%2C%20data%20compressed%20by%0Adiscrete%20cosine%20transform%2C%20and%20raw%20data%2C%20at%20different%20noise%20levels.%20ARPESNet%0Adata%20excels%20in%20clustering%20quality%20despite%20its%20high%20compression%20ratio.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04631v1&entry.124074799=Read"},
{"title": "Success or Failure? Analyzing Segmentation Refinement with Few-Shot\n  Segmentation", "author": "Seonghyeon Moon and Haein Kong and Muhammad Haris Khan", "abstract": "  The purpose of segmentation refinement is to enhance the initial coarse masks\ngenerated by segmentation algorithms. The refined masks are expected to capture\nthe details and contours of the target objects. Research on segmentation\nrefinement has developed as a response to the need for high-quality initial\nmasks. However, to our knowledge, no method has been developed that can\ndetermine the success of segmentation refinement. Such a method could ensure\nthe reliability of segmentation in applications where the outcome of the\nsegmentation is important, and fosters innovation in image processing\ntechnologies. To address this research gap, we propose JFS~(Judging From\nSupport-set), a method to identify the success of segmentation refinement\nleveraging a few-shot segmentation (FSS) model. The traditional goal of the\nproblem in FSS is to find a target object in a query image utilizing target\ninformation given by a support set. However, in our proposed method, we use the\nFSS network in a novel way to assess the segmentation refinement. When there\nare two masks, a coarse mask and a refined mask from segmentation refinement,\nthese two masks become support masks. The existing support mask works as a\nground truth mask to judge whether the quality of the refined segmentation is\nmore accurate than the coarse mask. We first obtained a coarse mask and refined\nit using SEPL (SAM Enhanced Pseduo-Labels) to get the two masks. Then, these\nbecome input to FSS model to judge whether the post-processing was successful.\nJFS is evaluated on the best and worst cases from SEPL to validate its\neffectiveness. The results showed that JFS can determine whether the SEPL is a\nsuccess or not.\n", "link": "http://arxiv.org/abs/2407.04519v1", "date": "2024-07-05", "relevancy": 2.3043, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.461}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Success%20or%20Failure%3F%20Analyzing%20Segmentation%20Refinement%20with%20Few-Shot%0A%20%20Segmentation&body=Title%3A%20Success%20or%20Failure%3F%20Analyzing%20Segmentation%20Refinement%20with%20Few-Shot%0A%20%20Segmentation%0AAuthor%3A%20Seonghyeon%20Moon%20and%20Haein%20Kong%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20The%20purpose%20of%20segmentation%20refinement%20is%20to%20enhance%20the%20initial%20coarse%20masks%0Agenerated%20by%20segmentation%20algorithms.%20The%20refined%20masks%20are%20expected%20to%20capture%0Athe%20details%20and%20contours%20of%20the%20target%20objects.%20Research%20on%20segmentation%0Arefinement%20has%20developed%20as%20a%20response%20to%20the%20need%20for%20high-quality%20initial%0Amasks.%20However%2C%20to%20our%20knowledge%2C%20no%20method%20has%20been%20developed%20that%20can%0Adetermine%20the%20success%20of%20segmentation%20refinement.%20Such%20a%20method%20could%20ensure%0Athe%20reliability%20of%20segmentation%20in%20applications%20where%20the%20outcome%20of%20the%0Asegmentation%20is%20important%2C%20and%20fosters%20innovation%20in%20image%20processing%0Atechnologies.%20To%20address%20this%20research%20gap%2C%20we%20propose%20JFS~%28Judging%20From%0ASupport-set%29%2C%20a%20method%20to%20identify%20the%20success%20of%20segmentation%20refinement%0Aleveraging%20a%20few-shot%20segmentation%20%28FSS%29%20model.%20The%20traditional%20goal%20of%20the%0Aproblem%20in%20FSS%20is%20to%20find%20a%20target%20object%20in%20a%20query%20image%20utilizing%20target%0Ainformation%20given%20by%20a%20support%20set.%20However%2C%20in%20our%20proposed%20method%2C%20we%20use%20the%0AFSS%20network%20in%20a%20novel%20way%20to%20assess%20the%20segmentation%20refinement.%20When%20there%0Aare%20two%20masks%2C%20a%20coarse%20mask%20and%20a%20refined%20mask%20from%20segmentation%20refinement%2C%0Athese%20two%20masks%20become%20support%20masks.%20The%20existing%20support%20mask%20works%20as%20a%0Aground%20truth%20mask%20to%20judge%20whether%20the%20quality%20of%20the%20refined%20segmentation%20is%0Amore%20accurate%20than%20the%20coarse%20mask.%20We%20first%20obtained%20a%20coarse%20mask%20and%20refined%0Ait%20using%20SEPL%20%28SAM%20Enhanced%20Pseduo-Labels%29%20to%20get%20the%20two%20masks.%20Then%2C%20these%0Abecome%20input%20to%20FSS%20model%20to%20judge%20whether%20the%20post-processing%20was%20successful.%0AJFS%20is%20evaluated%20on%20the%20best%20and%20worst%20cases%20from%20SEPL%20to%20validate%20its%0Aeffectiveness.%20The%20results%20showed%20that%20JFS%20can%20determine%20whether%20the%20SEPL%20is%20a%0Asuccess%20or%20not.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuccess%2520or%2520Failure%253F%2520Analyzing%2520Segmentation%2520Refinement%2520with%2520Few-Shot%250A%2520%2520Segmentation%26entry.906535625%3DSeonghyeon%2520Moon%2520and%2520Haein%2520Kong%2520and%2520Muhammad%2520Haris%2520Khan%26entry.1292438233%3D%2520%2520The%2520purpose%2520of%2520segmentation%2520refinement%2520is%2520to%2520enhance%2520the%2520initial%2520coarse%2520masks%250Agenerated%2520by%2520segmentation%2520algorithms.%2520The%2520refined%2520masks%2520are%2520expected%2520to%2520capture%250Athe%2520details%2520and%2520contours%2520of%2520the%2520target%2520objects.%2520Research%2520on%2520segmentation%250Arefinement%2520has%2520developed%2520as%2520a%2520response%2520to%2520the%2520need%2520for%2520high-quality%2520initial%250Amasks.%2520However%252C%2520to%2520our%2520knowledge%252C%2520no%2520method%2520has%2520been%2520developed%2520that%2520can%250Adetermine%2520the%2520success%2520of%2520segmentation%2520refinement.%2520Such%2520a%2520method%2520could%2520ensure%250Athe%2520reliability%2520of%2520segmentation%2520in%2520applications%2520where%2520the%2520outcome%2520of%2520the%250Asegmentation%2520is%2520important%252C%2520and%2520fosters%2520innovation%2520in%2520image%2520processing%250Atechnologies.%2520To%2520address%2520this%2520research%2520gap%252C%2520we%2520propose%2520JFS~%2528Judging%2520From%250ASupport-set%2529%252C%2520a%2520method%2520to%2520identify%2520the%2520success%2520of%2520segmentation%2520refinement%250Aleveraging%2520a%2520few-shot%2520segmentation%2520%2528FSS%2529%2520model.%2520The%2520traditional%2520goal%2520of%2520the%250Aproblem%2520in%2520FSS%2520is%2520to%2520find%2520a%2520target%2520object%2520in%2520a%2520query%2520image%2520utilizing%2520target%250Ainformation%2520given%2520by%2520a%2520support%2520set.%2520However%252C%2520in%2520our%2520proposed%2520method%252C%2520we%2520use%2520the%250AFSS%2520network%2520in%2520a%2520novel%2520way%2520to%2520assess%2520the%2520segmentation%2520refinement.%2520When%2520there%250Aare%2520two%2520masks%252C%2520a%2520coarse%2520mask%2520and%2520a%2520refined%2520mask%2520from%2520segmentation%2520refinement%252C%250Athese%2520two%2520masks%2520become%2520support%2520masks.%2520The%2520existing%2520support%2520mask%2520works%2520as%2520a%250Aground%2520truth%2520mask%2520to%2520judge%2520whether%2520the%2520quality%2520of%2520the%2520refined%2520segmentation%2520is%250Amore%2520accurate%2520than%2520the%2520coarse%2520mask.%2520We%2520first%2520obtained%2520a%2520coarse%2520mask%2520and%2520refined%250Ait%2520using%2520SEPL%2520%2528SAM%2520Enhanced%2520Pseduo-Labels%2529%2520to%2520get%2520the%2520two%2520masks.%2520Then%252C%2520these%250Abecome%2520input%2520to%2520FSS%2520model%2520to%2520judge%2520whether%2520the%2520post-processing%2520was%2520successful.%250AJFS%2520is%2520evaluated%2520on%2520the%2520best%2520and%2520worst%2520cases%2520from%2520SEPL%2520to%2520validate%2520its%250Aeffectiveness.%2520The%2520results%2520showed%2520that%2520JFS%2520can%2520determine%2520whether%2520the%2520SEPL%2520is%2520a%250Asuccess%2520or%2520not.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Success%20or%20Failure%3F%20Analyzing%20Segmentation%20Refinement%20with%20Few-Shot%0A%20%20Segmentation&entry.906535625=Seonghyeon%20Moon%20and%20Haein%20Kong%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20The%20purpose%20of%20segmentation%20refinement%20is%20to%20enhance%20the%20initial%20coarse%20masks%0Agenerated%20by%20segmentation%20algorithms.%20The%20refined%20masks%20are%20expected%20to%20capture%0Athe%20details%20and%20contours%20of%20the%20target%20objects.%20Research%20on%20segmentation%0Arefinement%20has%20developed%20as%20a%20response%20to%20the%20need%20for%20high-quality%20initial%0Amasks.%20However%2C%20to%20our%20knowledge%2C%20no%20method%20has%20been%20developed%20that%20can%0Adetermine%20the%20success%20of%20segmentation%20refinement.%20Such%20a%20method%20could%20ensure%0Athe%20reliability%20of%20segmentation%20in%20applications%20where%20the%20outcome%20of%20the%0Asegmentation%20is%20important%2C%20and%20fosters%20innovation%20in%20image%20processing%0Atechnologies.%20To%20address%20this%20research%20gap%2C%20we%20propose%20JFS~%28Judging%20From%0ASupport-set%29%2C%20a%20method%20to%20identify%20the%20success%20of%20segmentation%20refinement%0Aleveraging%20a%20few-shot%20segmentation%20%28FSS%29%20model.%20The%20traditional%20goal%20of%20the%0Aproblem%20in%20FSS%20is%20to%20find%20a%20target%20object%20in%20a%20query%20image%20utilizing%20target%0Ainformation%20given%20by%20a%20support%20set.%20However%2C%20in%20our%20proposed%20method%2C%20we%20use%20the%0AFSS%20network%20in%20a%20novel%20way%20to%20assess%20the%20segmentation%20refinement.%20When%20there%0Aare%20two%20masks%2C%20a%20coarse%20mask%20and%20a%20refined%20mask%20from%20segmentation%20refinement%2C%0Athese%20two%20masks%20become%20support%20masks.%20The%20existing%20support%20mask%20works%20as%20a%0Aground%20truth%20mask%20to%20judge%20whether%20the%20quality%20of%20the%20refined%20segmentation%20is%0Amore%20accurate%20than%20the%20coarse%20mask.%20We%20first%20obtained%20a%20coarse%20mask%20and%20refined%0Ait%20using%20SEPL%20%28SAM%20Enhanced%20Pseduo-Labels%29%20to%20get%20the%20two%20masks.%20Then%2C%20these%0Abecome%20input%20to%20FSS%20model%20to%20judge%20whether%20the%20post-processing%20was%20successful.%0AJFS%20is%20evaluated%20on%20the%20best%20and%20worst%20cases%20from%20SEPL%20to%20validate%20its%0Aeffectiveness.%20The%20results%20showed%20that%20JFS%20can%20determine%20whether%20the%20SEPL%20is%20a%0Asuccess%20or%20not.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04519v1&entry.124074799=Read"},
{"title": "Rethinking Image Compression on the Web with Generative AI", "author": "Shayan Ali Hassan and Danish Humair and Ihsan Ayyub Qazi and Zafar Ayyub Qazi", "abstract": "  The rapid growth of the Internet, driven by social media, web browsing, and\nvideo streaming, has made images central to the Web experience, resulting in\nsignificant data transfer and increased webpage sizes. Traditional image\ncompression methods, while reducing bandwidth, often degrade image quality.\nThis paper explores a novel approach using generative AI to reconstruct images\nat the edge or client-side. We develop a framework that leverages text prompts\nand provides additional conditioning inputs like Canny edges and color palettes\nto a text-to-image model, achieving up to 99.8% bandwidth savings in the best\ncases and 92.6% on average, while maintaining high perceptual similarity.\nEmpirical analysis and a user study show that our method preserves image\nmeaning and structure more effectively than traditional compression methods,\noffering a promising solution for reducing bandwidth usage and improving\nInternet affordability with minimal degradation in image quality.\n", "link": "http://arxiv.org/abs/2407.04542v1", "date": "2024-07-05", "relevancy": 2.2897, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5749}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5724}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Image%20Compression%20on%20the%20Web%20with%20Generative%20AI&body=Title%3A%20Rethinking%20Image%20Compression%20on%20the%20Web%20with%20Generative%20AI%0AAuthor%3A%20Shayan%20Ali%20Hassan%20and%20Danish%20Humair%20and%20Ihsan%20Ayyub%20Qazi%20and%20Zafar%20Ayyub%20Qazi%0AAbstract%3A%20%20%20The%20rapid%20growth%20of%20the%20Internet%2C%20driven%20by%20social%20media%2C%20web%20browsing%2C%20and%0Avideo%20streaming%2C%20has%20made%20images%20central%20to%20the%20Web%20experience%2C%20resulting%20in%0Asignificant%20data%20transfer%20and%20increased%20webpage%20sizes.%20Traditional%20image%0Acompression%20methods%2C%20while%20reducing%20bandwidth%2C%20often%20degrade%20image%20quality.%0AThis%20paper%20explores%20a%20novel%20approach%20using%20generative%20AI%20to%20reconstruct%20images%0Aat%20the%20edge%20or%20client-side.%20We%20develop%20a%20framework%20that%20leverages%20text%20prompts%0Aand%20provides%20additional%20conditioning%20inputs%20like%20Canny%20edges%20and%20color%20palettes%0Ato%20a%20text-to-image%20model%2C%20achieving%20up%20to%2099.8%25%20bandwidth%20savings%20in%20the%20best%0Acases%20and%2092.6%25%20on%20average%2C%20while%20maintaining%20high%20perceptual%20similarity.%0AEmpirical%20analysis%20and%20a%20user%20study%20show%20that%20our%20method%20preserves%20image%0Ameaning%20and%20structure%20more%20effectively%20than%20traditional%20compression%20methods%2C%0Aoffering%20a%20promising%20solution%20for%20reducing%20bandwidth%20usage%20and%20improving%0AInternet%20affordability%20with%20minimal%20degradation%20in%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Image%2520Compression%2520on%2520the%2520Web%2520with%2520Generative%2520AI%26entry.906535625%3DShayan%2520Ali%2520Hassan%2520and%2520Danish%2520Humair%2520and%2520Ihsan%2520Ayyub%2520Qazi%2520and%2520Zafar%2520Ayyub%2520Qazi%26entry.1292438233%3D%2520%2520The%2520rapid%2520growth%2520of%2520the%2520Internet%252C%2520driven%2520by%2520social%2520media%252C%2520web%2520browsing%252C%2520and%250Avideo%2520streaming%252C%2520has%2520made%2520images%2520central%2520to%2520the%2520Web%2520experience%252C%2520resulting%2520in%250Asignificant%2520data%2520transfer%2520and%2520increased%2520webpage%2520sizes.%2520Traditional%2520image%250Acompression%2520methods%252C%2520while%2520reducing%2520bandwidth%252C%2520often%2520degrade%2520image%2520quality.%250AThis%2520paper%2520explores%2520a%2520novel%2520approach%2520using%2520generative%2520AI%2520to%2520reconstruct%2520images%250Aat%2520the%2520edge%2520or%2520client-side.%2520We%2520develop%2520a%2520framework%2520that%2520leverages%2520text%2520prompts%250Aand%2520provides%2520additional%2520conditioning%2520inputs%2520like%2520Canny%2520edges%2520and%2520color%2520palettes%250Ato%2520a%2520text-to-image%2520model%252C%2520achieving%2520up%2520to%252099.8%2525%2520bandwidth%2520savings%2520in%2520the%2520best%250Acases%2520and%252092.6%2525%2520on%2520average%252C%2520while%2520maintaining%2520high%2520perceptual%2520similarity.%250AEmpirical%2520analysis%2520and%2520a%2520user%2520study%2520show%2520that%2520our%2520method%2520preserves%2520image%250Ameaning%2520and%2520structure%2520more%2520effectively%2520than%2520traditional%2520compression%2520methods%252C%250Aoffering%2520a%2520promising%2520solution%2520for%2520reducing%2520bandwidth%2520usage%2520and%2520improving%250AInternet%2520affordability%2520with%2520minimal%2520degradation%2520in%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Image%20Compression%20on%20the%20Web%20with%20Generative%20AI&entry.906535625=Shayan%20Ali%20Hassan%20and%20Danish%20Humair%20and%20Ihsan%20Ayyub%20Qazi%20and%20Zafar%20Ayyub%20Qazi&entry.1292438233=%20%20The%20rapid%20growth%20of%20the%20Internet%2C%20driven%20by%20social%20media%2C%20web%20browsing%2C%20and%0Avideo%20streaming%2C%20has%20made%20images%20central%20to%20the%20Web%20experience%2C%20resulting%20in%0Asignificant%20data%20transfer%20and%20increased%20webpage%20sizes.%20Traditional%20image%0Acompression%20methods%2C%20while%20reducing%20bandwidth%2C%20often%20degrade%20image%20quality.%0AThis%20paper%20explores%20a%20novel%20approach%20using%20generative%20AI%20to%20reconstruct%20images%0Aat%20the%20edge%20or%20client-side.%20We%20develop%20a%20framework%20that%20leverages%20text%20prompts%0Aand%20provides%20additional%20conditioning%20inputs%20like%20Canny%20edges%20and%20color%20palettes%0Ato%20a%20text-to-image%20model%2C%20achieving%20up%20to%2099.8%25%20bandwidth%20savings%20in%20the%20best%0Acases%20and%2092.6%25%20on%20average%2C%20while%20maintaining%20high%20perceptual%20similarity.%0AEmpirical%20analysis%20and%20a%20user%20study%20show%20that%20our%20method%20preserves%20image%0Ameaning%20and%20structure%20more%20effectively%20than%20traditional%20compression%20methods%2C%0Aoffering%20a%20promising%20solution%20for%20reducing%20bandwidth%20usage%20and%20improving%0AInternet%20affordability%20with%20minimal%20degradation%20in%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04542v1&entry.124074799=Read"},
{"title": "PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation", "author": "Yinghua Yao and Yuangang Pan and Jing Li and Ivor Tsang and Xin Yao", "abstract": "  Recent advancements in the realm of deep generative models focus on\ngenerating samples that satisfy multiple desired properties. However, prevalent\napproaches optimize these property functions independently, thus omitting the\ntrade-offs among them. In addition, the property optimization is often\nimproperly integrated into the generative models, resulting in an unnecessary\ncompromise on generation quality (i.e., the quality of generated samples). To\naddress these issues, we formulate a constrained optimization problem. It seeks\nto optimize generation quality while ensuring that generated samples reside at\nthe Pareto front of multiple property objectives. Such a formulation enables\nthe generation of samples that cannot be further improved simultaneously on the\nconflicting property functions and preserves good quality of generated samples.\nBuilding upon this formulation, we introduce the PaRetO-gUided Diffusion model\n(PROUD), wherein the gradients in the denoising process are dynamically\nadjusted to enhance generation quality while the generated samples adhere to\nPareto optimality. Experimental evaluations on image generation and protein\ngeneration tasks demonstrate that our PROUD consistently maintains superior\ngeneration quality while approaching Pareto optimality across multiple property\nfunctions compared to various baselines.\n", "link": "http://arxiv.org/abs/2407.04493v1", "date": "2024-07-05", "relevancy": 2.2802, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5841}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5622}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PROUD%3A%20PaRetO-gUided%20Diffusion%20Model%20for%20Multi-objective%20Generation&body=Title%3A%20PROUD%3A%20PaRetO-gUided%20Diffusion%20Model%20for%20Multi-objective%20Generation%0AAuthor%3A%20Yinghua%20Yao%20and%20Yuangang%20Pan%20and%20Jing%20Li%20and%20Ivor%20Tsang%20and%20Xin%20Yao%0AAbstract%3A%20%20%20Recent%20advancements%20in%20the%20realm%20of%20deep%20generative%20models%20focus%20on%0Agenerating%20samples%20that%20satisfy%20multiple%20desired%20properties.%20However%2C%20prevalent%0Aapproaches%20optimize%20these%20property%20functions%20independently%2C%20thus%20omitting%20the%0Atrade-offs%20among%20them.%20In%20addition%2C%20the%20property%20optimization%20is%20often%0Aimproperly%20integrated%20into%20the%20generative%20models%2C%20resulting%20in%20an%20unnecessary%0Acompromise%20on%20generation%20quality%20%28i.e.%2C%20the%20quality%20of%20generated%20samples%29.%20To%0Aaddress%20these%20issues%2C%20we%20formulate%20a%20constrained%20optimization%20problem.%20It%20seeks%0Ato%20optimize%20generation%20quality%20while%20ensuring%20that%20generated%20samples%20reside%20at%0Athe%20Pareto%20front%20of%20multiple%20property%20objectives.%20Such%20a%20formulation%20enables%0Athe%20generation%20of%20samples%20that%20cannot%20be%20further%20improved%20simultaneously%20on%20the%0Aconflicting%20property%20functions%20and%20preserves%20good%20quality%20of%20generated%20samples.%0ABuilding%20upon%20this%20formulation%2C%20we%20introduce%20the%20PaRetO-gUided%20Diffusion%20model%0A%28PROUD%29%2C%20wherein%20the%20gradients%20in%20the%20denoising%20process%20are%20dynamically%0Aadjusted%20to%20enhance%20generation%20quality%20while%20the%20generated%20samples%20adhere%20to%0APareto%20optimality.%20Experimental%20evaluations%20on%20image%20generation%20and%20protein%0Ageneration%20tasks%20demonstrate%20that%20our%20PROUD%20consistently%20maintains%20superior%0Ageneration%20quality%20while%20approaching%20Pareto%20optimality%20across%20multiple%20property%0Afunctions%20compared%20to%20various%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPROUD%253A%2520PaRetO-gUided%2520Diffusion%2520Model%2520for%2520Multi-objective%2520Generation%26entry.906535625%3DYinghua%2520Yao%2520and%2520Yuangang%2520Pan%2520and%2520Jing%2520Li%2520and%2520Ivor%2520Tsang%2520and%2520Xin%2520Yao%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520the%2520realm%2520of%2520deep%2520generative%2520models%2520focus%2520on%250Agenerating%2520samples%2520that%2520satisfy%2520multiple%2520desired%2520properties.%2520However%252C%2520prevalent%250Aapproaches%2520optimize%2520these%2520property%2520functions%2520independently%252C%2520thus%2520omitting%2520the%250Atrade-offs%2520among%2520them.%2520In%2520addition%252C%2520the%2520property%2520optimization%2520is%2520often%250Aimproperly%2520integrated%2520into%2520the%2520generative%2520models%252C%2520resulting%2520in%2520an%2520unnecessary%250Acompromise%2520on%2520generation%2520quality%2520%2528i.e.%252C%2520the%2520quality%2520of%2520generated%2520samples%2529.%2520To%250Aaddress%2520these%2520issues%252C%2520we%2520formulate%2520a%2520constrained%2520optimization%2520problem.%2520It%2520seeks%250Ato%2520optimize%2520generation%2520quality%2520while%2520ensuring%2520that%2520generated%2520samples%2520reside%2520at%250Athe%2520Pareto%2520front%2520of%2520multiple%2520property%2520objectives.%2520Such%2520a%2520formulation%2520enables%250Athe%2520generation%2520of%2520samples%2520that%2520cannot%2520be%2520further%2520improved%2520simultaneously%2520on%2520the%250Aconflicting%2520property%2520functions%2520and%2520preserves%2520good%2520quality%2520of%2520generated%2520samples.%250ABuilding%2520upon%2520this%2520formulation%252C%2520we%2520introduce%2520the%2520PaRetO-gUided%2520Diffusion%2520model%250A%2528PROUD%2529%252C%2520wherein%2520the%2520gradients%2520in%2520the%2520denoising%2520process%2520are%2520dynamically%250Aadjusted%2520to%2520enhance%2520generation%2520quality%2520while%2520the%2520generated%2520samples%2520adhere%2520to%250APareto%2520optimality.%2520Experimental%2520evaluations%2520on%2520image%2520generation%2520and%2520protein%250Ageneration%2520tasks%2520demonstrate%2520that%2520our%2520PROUD%2520consistently%2520maintains%2520superior%250Ageneration%2520quality%2520while%2520approaching%2520Pareto%2520optimality%2520across%2520multiple%2520property%250Afunctions%2520compared%2520to%2520various%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PROUD%3A%20PaRetO-gUided%20Diffusion%20Model%20for%20Multi-objective%20Generation&entry.906535625=Yinghua%20Yao%20and%20Yuangang%20Pan%20and%20Jing%20Li%20and%20Ivor%20Tsang%20and%20Xin%20Yao&entry.1292438233=%20%20Recent%20advancements%20in%20the%20realm%20of%20deep%20generative%20models%20focus%20on%0Agenerating%20samples%20that%20satisfy%20multiple%20desired%20properties.%20However%2C%20prevalent%0Aapproaches%20optimize%20these%20property%20functions%20independently%2C%20thus%20omitting%20the%0Atrade-offs%20among%20them.%20In%20addition%2C%20the%20property%20optimization%20is%20often%0Aimproperly%20integrated%20into%20the%20generative%20models%2C%20resulting%20in%20an%20unnecessary%0Acompromise%20on%20generation%20quality%20%28i.e.%2C%20the%20quality%20of%20generated%20samples%29.%20To%0Aaddress%20these%20issues%2C%20we%20formulate%20a%20constrained%20optimization%20problem.%20It%20seeks%0Ato%20optimize%20generation%20quality%20while%20ensuring%20that%20generated%20samples%20reside%20at%0Athe%20Pareto%20front%20of%20multiple%20property%20objectives.%20Such%20a%20formulation%20enables%0Athe%20generation%20of%20samples%20that%20cannot%20be%20further%20improved%20simultaneously%20on%20the%0Aconflicting%20property%20functions%20and%20preserves%20good%20quality%20of%20generated%20samples.%0ABuilding%20upon%20this%20formulation%2C%20we%20introduce%20the%20PaRetO-gUided%20Diffusion%20model%0A%28PROUD%29%2C%20wherein%20the%20gradients%20in%20the%20denoising%20process%20are%20dynamically%0Aadjusted%20to%20enhance%20generation%20quality%20while%20the%20generated%20samples%20adhere%20to%0APareto%20optimality.%20Experimental%20evaluations%20on%20image%20generation%20and%20protein%0Ageneration%20tasks%20demonstrate%20that%20our%20PROUD%20consistently%20maintains%20superior%0Ageneration%20quality%20while%20approaching%20Pareto%20optimality%20across%20multiple%20property%0Afunctions%20compared%20to%20various%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04493v1&entry.124074799=Read"},
{"title": "GOALPlace: Begin with the End in Mind", "author": "Anthony Agnesina and Rongjian Liang and Geraldo Pradipta and Anand Rajaram and Haoxing Ren", "abstract": "  Co-optimizing placement with congestion is integral to achieving high-quality\ndesigns. This paper presents GOALPlace, a new learning-based general approach\nto improving placement congestion by controlling cell density. Our method\nefficiently learns from an EDA tool's post-route optimized results and uses an\nempirical Bayes technique to adapt this goal/target to a specific placer's\nsolutions, effectively beginning with the end in mind. It enhances correlation\nwith the long-running heuristics of the tool's router and timing-opt engine --\nwhile solving placement globally without expensive incremental congestion\nestimation and mitigation methods. A statistical analysis with a new\nhierarchical netlist clustering establishes the importance of density and the\npotential for an adequate cell density target across placements. Our\nexperiments show that our method, integrated as a demonstration inside an\nacademic GPU-accelerated global placer, consistently produces macro and\nstandard cell placements of superior or comparable quality to commercial tools.\nOur empirical Bayes methodology also allows a substantial quality improvement\nover state-of-the-art academic mixed-size placers, achieving up to 10x fewer\ndesign rule check (DRC) violations, a 5% decrease in wirelength, and a 30% and\n60% reduction in worst and total negative slack (WNS/TNS).\n", "link": "http://arxiv.org/abs/2407.04579v1", "date": "2024-07-05", "relevancy": 2.2395, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4505}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4476}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GOALPlace%3A%20Begin%20with%20the%20End%20in%20Mind&body=Title%3A%20GOALPlace%3A%20Begin%20with%20the%20End%20in%20Mind%0AAuthor%3A%20Anthony%20Agnesina%20and%20Rongjian%20Liang%20and%20Geraldo%20Pradipta%20and%20Anand%20Rajaram%20and%20Haoxing%20Ren%0AAbstract%3A%20%20%20Co-optimizing%20placement%20with%20congestion%20is%20integral%20to%20achieving%20high-quality%0Adesigns.%20This%20paper%20presents%20GOALPlace%2C%20a%20new%20learning-based%20general%20approach%0Ato%20improving%20placement%20congestion%20by%20controlling%20cell%20density.%20Our%20method%0Aefficiently%20learns%20from%20an%20EDA%20tool%27s%20post-route%20optimized%20results%20and%20uses%20an%0Aempirical%20Bayes%20technique%20to%20adapt%20this%20goal/target%20to%20a%20specific%20placer%27s%0Asolutions%2C%20effectively%20beginning%20with%20the%20end%20in%20mind.%20It%20enhances%20correlation%0Awith%20the%20long-running%20heuristics%20of%20the%20tool%27s%20router%20and%20timing-opt%20engine%20--%0Awhile%20solving%20placement%20globally%20without%20expensive%20incremental%20congestion%0Aestimation%20and%20mitigation%20methods.%20A%20statistical%20analysis%20with%20a%20new%0Ahierarchical%20netlist%20clustering%20establishes%20the%20importance%20of%20density%20and%20the%0Apotential%20for%20an%20adequate%20cell%20density%20target%20across%20placements.%20Our%0Aexperiments%20show%20that%20our%20method%2C%20integrated%20as%20a%20demonstration%20inside%20an%0Aacademic%20GPU-accelerated%20global%20placer%2C%20consistently%20produces%20macro%20and%0Astandard%20cell%20placements%20of%20superior%20or%20comparable%20quality%20to%20commercial%20tools.%0AOur%20empirical%20Bayes%20methodology%20also%20allows%20a%20substantial%20quality%20improvement%0Aover%20state-of-the-art%20academic%20mixed-size%20placers%2C%20achieving%20up%20to%2010x%20fewer%0Adesign%20rule%20check%20%28DRC%29%20violations%2C%20a%205%25%20decrease%20in%20wirelength%2C%20and%20a%2030%25%20and%0A60%25%20reduction%20in%20worst%20and%20total%20negative%20slack%20%28WNS/TNS%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGOALPlace%253A%2520Begin%2520with%2520the%2520End%2520in%2520Mind%26entry.906535625%3DAnthony%2520Agnesina%2520and%2520Rongjian%2520Liang%2520and%2520Geraldo%2520Pradipta%2520and%2520Anand%2520Rajaram%2520and%2520Haoxing%2520Ren%26entry.1292438233%3D%2520%2520Co-optimizing%2520placement%2520with%2520congestion%2520is%2520integral%2520to%2520achieving%2520high-quality%250Adesigns.%2520This%2520paper%2520presents%2520GOALPlace%252C%2520a%2520new%2520learning-based%2520general%2520approach%250Ato%2520improving%2520placement%2520congestion%2520by%2520controlling%2520cell%2520density.%2520Our%2520method%250Aefficiently%2520learns%2520from%2520an%2520EDA%2520tool%2527s%2520post-route%2520optimized%2520results%2520and%2520uses%2520an%250Aempirical%2520Bayes%2520technique%2520to%2520adapt%2520this%2520goal/target%2520to%2520a%2520specific%2520placer%2527s%250Asolutions%252C%2520effectively%2520beginning%2520with%2520the%2520end%2520in%2520mind.%2520It%2520enhances%2520correlation%250Awith%2520the%2520long-running%2520heuristics%2520of%2520the%2520tool%2527s%2520router%2520and%2520timing-opt%2520engine%2520--%250Awhile%2520solving%2520placement%2520globally%2520without%2520expensive%2520incremental%2520congestion%250Aestimation%2520and%2520mitigation%2520methods.%2520A%2520statistical%2520analysis%2520with%2520a%2520new%250Ahierarchical%2520netlist%2520clustering%2520establishes%2520the%2520importance%2520of%2520density%2520and%2520the%250Apotential%2520for%2520an%2520adequate%2520cell%2520density%2520target%2520across%2520placements.%2520Our%250Aexperiments%2520show%2520that%2520our%2520method%252C%2520integrated%2520as%2520a%2520demonstration%2520inside%2520an%250Aacademic%2520GPU-accelerated%2520global%2520placer%252C%2520consistently%2520produces%2520macro%2520and%250Astandard%2520cell%2520placements%2520of%2520superior%2520or%2520comparable%2520quality%2520to%2520commercial%2520tools.%250AOur%2520empirical%2520Bayes%2520methodology%2520also%2520allows%2520a%2520substantial%2520quality%2520improvement%250Aover%2520state-of-the-art%2520academic%2520mixed-size%2520placers%252C%2520achieving%2520up%2520to%252010x%2520fewer%250Adesign%2520rule%2520check%2520%2528DRC%2529%2520violations%252C%2520a%25205%2525%2520decrease%2520in%2520wirelength%252C%2520and%2520a%252030%2525%2520and%250A60%2525%2520reduction%2520in%2520worst%2520and%2520total%2520negative%2520slack%2520%2528WNS/TNS%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GOALPlace%3A%20Begin%20with%20the%20End%20in%20Mind&entry.906535625=Anthony%20Agnesina%20and%20Rongjian%20Liang%20and%20Geraldo%20Pradipta%20and%20Anand%20Rajaram%20and%20Haoxing%20Ren&entry.1292438233=%20%20Co-optimizing%20placement%20with%20congestion%20is%20integral%20to%20achieving%20high-quality%0Adesigns.%20This%20paper%20presents%20GOALPlace%2C%20a%20new%20learning-based%20general%20approach%0Ato%20improving%20placement%20congestion%20by%20controlling%20cell%20density.%20Our%20method%0Aefficiently%20learns%20from%20an%20EDA%20tool%27s%20post-route%20optimized%20results%20and%20uses%20an%0Aempirical%20Bayes%20technique%20to%20adapt%20this%20goal/target%20to%20a%20specific%20placer%27s%0Asolutions%2C%20effectively%20beginning%20with%20the%20end%20in%20mind.%20It%20enhances%20correlation%0Awith%20the%20long-running%20heuristics%20of%20the%20tool%27s%20router%20and%20timing-opt%20engine%20--%0Awhile%20solving%20placement%20globally%20without%20expensive%20incremental%20congestion%0Aestimation%20and%20mitigation%20methods.%20A%20statistical%20analysis%20with%20a%20new%0Ahierarchical%20netlist%20clustering%20establishes%20the%20importance%20of%20density%20and%20the%0Apotential%20for%20an%20adequate%20cell%20density%20target%20across%20placements.%20Our%0Aexperiments%20show%20that%20our%20method%2C%20integrated%20as%20a%20demonstration%20inside%20an%0Aacademic%20GPU-accelerated%20global%20placer%2C%20consistently%20produces%20macro%20and%0Astandard%20cell%20placements%20of%20superior%20or%20comparable%20quality%20to%20commercial%20tools.%0AOur%20empirical%20Bayes%20methodology%20also%20allows%20a%20substantial%20quality%20improvement%0Aover%20state-of-the-art%20academic%20mixed-size%20placers%2C%20achieving%20up%20to%2010x%20fewer%0Adesign%20rule%20check%20%28DRC%29%20violations%2C%20a%205%25%20decrease%20in%20wirelength%2C%20and%20a%2030%25%20and%0A60%25%20reduction%20in%20worst%20and%20total%20negative%20slack%20%28WNS/TNS%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04579v1&entry.124074799=Read"},
{"title": "Robust Multimodal Learning via Representation Decoupling", "author": "Shicai Wei and Yang Luo and Yuji Wang and Chunbo Luo", "abstract": "  Multimodal learning robust to missing modality has attracted increasing\nattention due to its practicality. Existing methods tend to address it by\nlearning a common subspace representation for different modality combinations.\nHowever, we reveal that they are sub-optimal due to their implicit constraint\non intra-class representation. Specifically, the sample with different\nmodalities within the same class will be forced to learn representations in the\nsame direction. This hinders the model from capturing modality-specific\ninformation, resulting in insufficient learning. To this end, we propose a\nnovel Decoupled Multimodal Representation Network (DMRNet) to assist robust\nmultimodal learning. Specifically, DMRNet models the input from different\nmodality combinations as a probabilistic distribution instead of a fixed point\nin the latent space, and samples embeddings from the distribution for the\nprediction module to calculate the task loss. As a result, the direction\nconstraint from the loss minimization is blocked by the sampled representation.\nThis relaxes the constraint on the inference representation and enables the\nmodel to capture the specific information for different modality combinations.\nFurthermore, we introduce a hard combination regularizer to prevent DMRNet from\nunbalanced training by guiding it to pay more attention to hard modality\ncombinations. Finally, extensive experiments on multimodal classification and\nsegmentation tasks demonstrate that the proposed DMRNet outperforms the\nstate-of-the-art significantly.\n", "link": "http://arxiv.org/abs/2407.04458v1", "date": "2024-07-05", "relevancy": 2.2217, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5609}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Multimodal%20Learning%20via%20Representation%20Decoupling&body=Title%3A%20Robust%20Multimodal%20Learning%20via%20Representation%20Decoupling%0AAuthor%3A%20Shicai%20Wei%20and%20Yang%20Luo%20and%20Yuji%20Wang%20and%20Chunbo%20Luo%0AAbstract%3A%20%20%20Multimodal%20learning%20robust%20to%20missing%20modality%20has%20attracted%20increasing%0Aattention%20due%20to%20its%20practicality.%20Existing%20methods%20tend%20to%20address%20it%20by%0Alearning%20a%20common%20subspace%20representation%20for%20different%20modality%20combinations.%0AHowever%2C%20we%20reveal%20that%20they%20are%20sub-optimal%20due%20to%20their%20implicit%20constraint%0Aon%20intra-class%20representation.%20Specifically%2C%20the%20sample%20with%20different%0Amodalities%20within%20the%20same%20class%20will%20be%20forced%20to%20learn%20representations%20in%20the%0Asame%20direction.%20This%20hinders%20the%20model%20from%20capturing%20modality-specific%0Ainformation%2C%20resulting%20in%20insufficient%20learning.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20Decoupled%20Multimodal%20Representation%20Network%20%28DMRNet%29%20to%20assist%20robust%0Amultimodal%20learning.%20Specifically%2C%20DMRNet%20models%20the%20input%20from%20different%0Amodality%20combinations%20as%20a%20probabilistic%20distribution%20instead%20of%20a%20fixed%20point%0Ain%20the%20latent%20space%2C%20and%20samples%20embeddings%20from%20the%20distribution%20for%20the%0Aprediction%20module%20to%20calculate%20the%20task%20loss.%20As%20a%20result%2C%20the%20direction%0Aconstraint%20from%20the%20loss%20minimization%20is%20blocked%20by%20the%20sampled%20representation.%0AThis%20relaxes%20the%20constraint%20on%20the%20inference%20representation%20and%20enables%20the%0Amodel%20to%20capture%20the%20specific%20information%20for%20different%20modality%20combinations.%0AFurthermore%2C%20we%20introduce%20a%20hard%20combination%20regularizer%20to%20prevent%20DMRNet%20from%0Aunbalanced%20training%20by%20guiding%20it%20to%20pay%20more%20attention%20to%20hard%20modality%0Acombinations.%20Finally%2C%20extensive%20experiments%20on%20multimodal%20classification%20and%0Asegmentation%20tasks%20demonstrate%20that%20the%20proposed%20DMRNet%20outperforms%20the%0Astate-of-the-art%20significantly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Multimodal%2520Learning%2520via%2520Representation%2520Decoupling%26entry.906535625%3DShicai%2520Wei%2520and%2520Yang%2520Luo%2520and%2520Yuji%2520Wang%2520and%2520Chunbo%2520Luo%26entry.1292438233%3D%2520%2520Multimodal%2520learning%2520robust%2520to%2520missing%2520modality%2520has%2520attracted%2520increasing%250Aattention%2520due%2520to%2520its%2520practicality.%2520Existing%2520methods%2520tend%2520to%2520address%2520it%2520by%250Alearning%2520a%2520common%2520subspace%2520representation%2520for%2520different%2520modality%2520combinations.%250AHowever%252C%2520we%2520reveal%2520that%2520they%2520are%2520sub-optimal%2520due%2520to%2520their%2520implicit%2520constraint%250Aon%2520intra-class%2520representation.%2520Specifically%252C%2520the%2520sample%2520with%2520different%250Amodalities%2520within%2520the%2520same%2520class%2520will%2520be%2520forced%2520to%2520learn%2520representations%2520in%2520the%250Asame%2520direction.%2520This%2520hinders%2520the%2520model%2520from%2520capturing%2520modality-specific%250Ainformation%252C%2520resulting%2520in%2520insufficient%2520learning.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Anovel%2520Decoupled%2520Multimodal%2520Representation%2520Network%2520%2528DMRNet%2529%2520to%2520assist%2520robust%250Amultimodal%2520learning.%2520Specifically%252C%2520DMRNet%2520models%2520the%2520input%2520from%2520different%250Amodality%2520combinations%2520as%2520a%2520probabilistic%2520distribution%2520instead%2520of%2520a%2520fixed%2520point%250Ain%2520the%2520latent%2520space%252C%2520and%2520samples%2520embeddings%2520from%2520the%2520distribution%2520for%2520the%250Aprediction%2520module%2520to%2520calculate%2520the%2520task%2520loss.%2520As%2520a%2520result%252C%2520the%2520direction%250Aconstraint%2520from%2520the%2520loss%2520minimization%2520is%2520blocked%2520by%2520the%2520sampled%2520representation.%250AThis%2520relaxes%2520the%2520constraint%2520on%2520the%2520inference%2520representation%2520and%2520enables%2520the%250Amodel%2520to%2520capture%2520the%2520specific%2520information%2520for%2520different%2520modality%2520combinations.%250AFurthermore%252C%2520we%2520introduce%2520a%2520hard%2520combination%2520regularizer%2520to%2520prevent%2520DMRNet%2520from%250Aunbalanced%2520training%2520by%2520guiding%2520it%2520to%2520pay%2520more%2520attention%2520to%2520hard%2520modality%250Acombinations.%2520Finally%252C%2520extensive%2520experiments%2520on%2520multimodal%2520classification%2520and%250Asegmentation%2520tasks%2520demonstrate%2520that%2520the%2520proposed%2520DMRNet%2520outperforms%2520the%250Astate-of-the-art%2520significantly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Multimodal%20Learning%20via%20Representation%20Decoupling&entry.906535625=Shicai%20Wei%20and%20Yang%20Luo%20and%20Yuji%20Wang%20and%20Chunbo%20Luo&entry.1292438233=%20%20Multimodal%20learning%20robust%20to%20missing%20modality%20has%20attracted%20increasing%0Aattention%20due%20to%20its%20practicality.%20Existing%20methods%20tend%20to%20address%20it%20by%0Alearning%20a%20common%20subspace%20representation%20for%20different%20modality%20combinations.%0AHowever%2C%20we%20reveal%20that%20they%20are%20sub-optimal%20due%20to%20their%20implicit%20constraint%0Aon%20intra-class%20representation.%20Specifically%2C%20the%20sample%20with%20different%0Amodalities%20within%20the%20same%20class%20will%20be%20forced%20to%20learn%20representations%20in%20the%0Asame%20direction.%20This%20hinders%20the%20model%20from%20capturing%20modality-specific%0Ainformation%2C%20resulting%20in%20insufficient%20learning.%20To%20this%20end%2C%20we%20propose%20a%0Anovel%20Decoupled%20Multimodal%20Representation%20Network%20%28DMRNet%29%20to%20assist%20robust%0Amultimodal%20learning.%20Specifically%2C%20DMRNet%20models%20the%20input%20from%20different%0Amodality%20combinations%20as%20a%20probabilistic%20distribution%20instead%20of%20a%20fixed%20point%0Ain%20the%20latent%20space%2C%20and%20samples%20embeddings%20from%20the%20distribution%20for%20the%0Aprediction%20module%20to%20calculate%20the%20task%20loss.%20As%20a%20result%2C%20the%20direction%0Aconstraint%20from%20the%20loss%20minimization%20is%20blocked%20by%20the%20sampled%20representation.%0AThis%20relaxes%20the%20constraint%20on%20the%20inference%20representation%20and%20enables%20the%0Amodel%20to%20capture%20the%20specific%20information%20for%20different%20modality%20combinations.%0AFurthermore%2C%20we%20introduce%20a%20hard%20combination%20regularizer%20to%20prevent%20DMRNet%20from%0Aunbalanced%20training%20by%20guiding%20it%20to%20pay%20more%20attention%20to%20hard%20modality%0Acombinations.%20Finally%2C%20extensive%20experiments%20on%20multimodal%20classification%20and%0Asegmentation%20tasks%20demonstrate%20that%20the%20proposed%20DMRNet%20outperforms%20the%0Astate-of-the-art%20significantly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04458v1&entry.124074799=Read"},
{"title": "Graph Theory and GNNs to Unravel the Topographical Organization of Brain\n  Lesions in Variants of Alzheimer's Disease Progression", "author": "Gabriel Jimenez and Leopold Hebert-Stevens and Benoit Delatour and Lev Stimmer and Daniel Racoceanu", "abstract": "  In this study, we proposed and evaluated a graph-based framework to assess\nvariations in Alzheimer's disease (AD) neuropathologies, focusing on classic\n(cAD) and rapid (rpAD) progression forms. Histopathological images are\nconverted into tau-pathology-based (i.e., amyloid plaques and tau tangles)\ngraphs, and derived metrics are used in a machine-learning classifier. This\nclassifier incorporates SHAP value explainability to differentiate between cAD\nand rpAD. Furthermore, we tested graph neural networks (GNNs) to extract\ntopological embeddings from the graphs and use them in classifying the\nprogression forms of AD. The analysis demonstrated denser networks in rpAD and\na distinctive impact on brain cortical layers: rpAD predominantly affects\nmiddle layers, whereas cAD influences both superficial and deep layers of the\nsame cortical regions. These results suggest a unique neuropathological network\norganization for each AD variant.\n", "link": "http://arxiv.org/abs/2403.00636v2", "date": "2024-07-05", "relevancy": 2.1924, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4585}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4286}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Theory%20and%20GNNs%20to%20Unravel%20the%20Topographical%20Organization%20of%20Brain%0A%20%20Lesions%20in%20Variants%20of%20Alzheimer%27s%20Disease%20Progression&body=Title%3A%20Graph%20Theory%20and%20GNNs%20to%20Unravel%20the%20Topographical%20Organization%20of%20Brain%0A%20%20Lesions%20in%20Variants%20of%20Alzheimer%27s%20Disease%20Progression%0AAuthor%3A%20Gabriel%20Jimenez%20and%20Leopold%20Hebert-Stevens%20and%20Benoit%20Delatour%20and%20Lev%20Stimmer%20and%20Daniel%20Racoceanu%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20proposed%20and%20evaluated%20a%20graph-based%20framework%20to%20assess%0Avariations%20in%20Alzheimer%27s%20disease%20%28AD%29%20neuropathologies%2C%20focusing%20on%20classic%0A%28cAD%29%20and%20rapid%20%28rpAD%29%20progression%20forms.%20Histopathological%20images%20are%0Aconverted%20into%20tau-pathology-based%20%28i.e.%2C%20amyloid%20plaques%20and%20tau%20tangles%29%0Agraphs%2C%20and%20derived%20metrics%20are%20used%20in%20a%20machine-learning%20classifier.%20This%0Aclassifier%20incorporates%20SHAP%20value%20explainability%20to%20differentiate%20between%20cAD%0Aand%20rpAD.%20Furthermore%2C%20we%20tested%20graph%20neural%20networks%20%28GNNs%29%20to%20extract%0Atopological%20embeddings%20from%20the%20graphs%20and%20use%20them%20in%20classifying%20the%0Aprogression%20forms%20of%20AD.%20The%20analysis%20demonstrated%20denser%20networks%20in%20rpAD%20and%0Aa%20distinctive%20impact%20on%20brain%20cortical%20layers%3A%20rpAD%20predominantly%20affects%0Amiddle%20layers%2C%20whereas%20cAD%20influences%20both%20superficial%20and%20deep%20layers%20of%20the%0Asame%20cortical%20regions.%20These%20results%20suggest%20a%20unique%20neuropathological%20network%0Aorganization%20for%20each%20AD%20variant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00636v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Theory%2520and%2520GNNs%2520to%2520Unravel%2520the%2520Topographical%2520Organization%2520of%2520Brain%250A%2520%2520Lesions%2520in%2520Variants%2520of%2520Alzheimer%2527s%2520Disease%2520Progression%26entry.906535625%3DGabriel%2520Jimenez%2520and%2520Leopold%2520Hebert-Stevens%2520and%2520Benoit%2520Delatour%2520and%2520Lev%2520Stimmer%2520and%2520Daniel%2520Racoceanu%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520proposed%2520and%2520evaluated%2520a%2520graph-based%2520framework%2520to%2520assess%250Avariations%2520in%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520neuropathologies%252C%2520focusing%2520on%2520classic%250A%2528cAD%2529%2520and%2520rapid%2520%2528rpAD%2529%2520progression%2520forms.%2520Histopathological%2520images%2520are%250Aconverted%2520into%2520tau-pathology-based%2520%2528i.e.%252C%2520amyloid%2520plaques%2520and%2520tau%2520tangles%2529%250Agraphs%252C%2520and%2520derived%2520metrics%2520are%2520used%2520in%2520a%2520machine-learning%2520classifier.%2520This%250Aclassifier%2520incorporates%2520SHAP%2520value%2520explainability%2520to%2520differentiate%2520between%2520cAD%250Aand%2520rpAD.%2520Furthermore%252C%2520we%2520tested%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520to%2520extract%250Atopological%2520embeddings%2520from%2520the%2520graphs%2520and%2520use%2520them%2520in%2520classifying%2520the%250Aprogression%2520forms%2520of%2520AD.%2520The%2520analysis%2520demonstrated%2520denser%2520networks%2520in%2520rpAD%2520and%250Aa%2520distinctive%2520impact%2520on%2520brain%2520cortical%2520layers%253A%2520rpAD%2520predominantly%2520affects%250Amiddle%2520layers%252C%2520whereas%2520cAD%2520influences%2520both%2520superficial%2520and%2520deep%2520layers%2520of%2520the%250Asame%2520cortical%2520regions.%2520These%2520results%2520suggest%2520a%2520unique%2520neuropathological%2520network%250Aorganization%2520for%2520each%2520AD%2520variant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00636v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Theory%20and%20GNNs%20to%20Unravel%20the%20Topographical%20Organization%20of%20Brain%0A%20%20Lesions%20in%20Variants%20of%20Alzheimer%27s%20Disease%20Progression&entry.906535625=Gabriel%20Jimenez%20and%20Leopold%20Hebert-Stevens%20and%20Benoit%20Delatour%20and%20Lev%20Stimmer%20and%20Daniel%20Racoceanu&entry.1292438233=%20%20In%20this%20study%2C%20we%20proposed%20and%20evaluated%20a%20graph-based%20framework%20to%20assess%0Avariations%20in%20Alzheimer%27s%20disease%20%28AD%29%20neuropathologies%2C%20focusing%20on%20classic%0A%28cAD%29%20and%20rapid%20%28rpAD%29%20progression%20forms.%20Histopathological%20images%20are%0Aconverted%20into%20tau-pathology-based%20%28i.e.%2C%20amyloid%20plaques%20and%20tau%20tangles%29%0Agraphs%2C%20and%20derived%20metrics%20are%20used%20in%20a%20machine-learning%20classifier.%20This%0Aclassifier%20incorporates%20SHAP%20value%20explainability%20to%20differentiate%20between%20cAD%0Aand%20rpAD.%20Furthermore%2C%20we%20tested%20graph%20neural%20networks%20%28GNNs%29%20to%20extract%0Atopological%20embeddings%20from%20the%20graphs%20and%20use%20them%20in%20classifying%20the%0Aprogression%20forms%20of%20AD.%20The%20analysis%20demonstrated%20denser%20networks%20in%20rpAD%20and%0Aa%20distinctive%20impact%20on%20brain%20cortical%20layers%3A%20rpAD%20predominantly%20affects%0Amiddle%20layers%2C%20whereas%20cAD%20influences%20both%20superficial%20and%20deep%20layers%20of%20the%0Asame%20cortical%20regions.%20These%20results%20suggest%20a%20unique%20neuropathological%20network%0Aorganization%20for%20each%20AD%20variant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00636v2&entry.124074799=Read"},
{"title": "Semi-Supervised Segmentation via Embedding Matching", "author": "Weiyi Xie and Nathalie Willems and Nikolas Lessmann and Tom Gibbons and Daniele De Massari", "abstract": "  Deep convolutional neural networks are widely used in medical image\nsegmentation but require many labeled images for training. Annotating\nthree-dimensional medical images is a time-consuming and costly process. To\novercome this limitation, we propose a novel semi-supervised segmentation\nmethod that leverages mostly unlabeled images and a small set of labeled images\nin training. Our approach involves assessing prediction uncertainty to identify\nreliable predictions on unlabeled voxels from the teacher model. These voxels\nserve as pseudo-labels for training the student model. In voxels where the\nteacher model produces unreliable predictions, pseudo-labeling is carried out\nbased on voxel-wise embedding correspondence using reference voxels from\nlabeled images. We applied this method to automate hip bone segmentation in CT\nimages, achieving notable results with just 4 CT scans. The proposed approach\nyielded a Hausdorff distance with 95th percentile (HD95) of 3.30 and IoU of\n0.929, surpassing existing methods achieving HD95 (4.07) and IoU (0.927) at\ntheir best.\n", "link": "http://arxiv.org/abs/2407.04638v1", "date": "2024-07-05", "relevancy": 2.1832, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5854}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5452}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Segmentation%20via%20Embedding%20Matching&body=Title%3A%20Semi-Supervised%20Segmentation%20via%20Embedding%20Matching%0AAuthor%3A%20Weiyi%20Xie%20and%20Nathalie%20Willems%20and%20Nikolas%20Lessmann%20and%20Tom%20Gibbons%20and%20Daniele%20De%20Massari%0AAbstract%3A%20%20%20Deep%20convolutional%20neural%20networks%20are%20widely%20used%20in%20medical%20image%0Asegmentation%20but%20require%20many%20labeled%20images%20for%20training.%20Annotating%0Athree-dimensional%20medical%20images%20is%20a%20time-consuming%20and%20costly%20process.%20To%0Aovercome%20this%20limitation%2C%20we%20propose%20a%20novel%20semi-supervised%20segmentation%0Amethod%20that%20leverages%20mostly%20unlabeled%20images%20and%20a%20small%20set%20of%20labeled%20images%0Ain%20training.%20Our%20approach%20involves%20assessing%20prediction%20uncertainty%20to%20identify%0Areliable%20predictions%20on%20unlabeled%20voxels%20from%20the%20teacher%20model.%20These%20voxels%0Aserve%20as%20pseudo-labels%20for%20training%20the%20student%20model.%20In%20voxels%20where%20the%0Ateacher%20model%20produces%20unreliable%20predictions%2C%20pseudo-labeling%20is%20carried%20out%0Abased%20on%20voxel-wise%20embedding%20correspondence%20using%20reference%20voxels%20from%0Alabeled%20images.%20We%20applied%20this%20method%20to%20automate%20hip%20bone%20segmentation%20in%20CT%0Aimages%2C%20achieving%20notable%20results%20with%20just%204%20CT%20scans.%20The%20proposed%20approach%0Ayielded%20a%20Hausdorff%20distance%20with%2095th%20percentile%20%28HD95%29%20of%203.30%20and%20IoU%20of%0A0.929%2C%20surpassing%20existing%20methods%20achieving%20HD95%20%284.07%29%20and%20IoU%20%280.927%29%20at%0Atheir%20best.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Segmentation%2520via%2520Embedding%2520Matching%26entry.906535625%3DWeiyi%2520Xie%2520and%2520Nathalie%2520Willems%2520and%2520Nikolas%2520Lessmann%2520and%2520Tom%2520Gibbons%2520and%2520Daniele%2520De%2520Massari%26entry.1292438233%3D%2520%2520Deep%2520convolutional%2520neural%2520networks%2520are%2520widely%2520used%2520in%2520medical%2520image%250Asegmentation%2520but%2520require%2520many%2520labeled%2520images%2520for%2520training.%2520Annotating%250Athree-dimensional%2520medical%2520images%2520is%2520a%2520time-consuming%2520and%2520costly%2520process.%2520To%250Aovercome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520semi-supervised%2520segmentation%250Amethod%2520that%2520leverages%2520mostly%2520unlabeled%2520images%2520and%2520a%2520small%2520set%2520of%2520labeled%2520images%250Ain%2520training.%2520Our%2520approach%2520involves%2520assessing%2520prediction%2520uncertainty%2520to%2520identify%250Areliable%2520predictions%2520on%2520unlabeled%2520voxels%2520from%2520the%2520teacher%2520model.%2520These%2520voxels%250Aserve%2520as%2520pseudo-labels%2520for%2520training%2520the%2520student%2520model.%2520In%2520voxels%2520where%2520the%250Ateacher%2520model%2520produces%2520unreliable%2520predictions%252C%2520pseudo-labeling%2520is%2520carried%2520out%250Abased%2520on%2520voxel-wise%2520embedding%2520correspondence%2520using%2520reference%2520voxels%2520from%250Alabeled%2520images.%2520We%2520applied%2520this%2520method%2520to%2520automate%2520hip%2520bone%2520segmentation%2520in%2520CT%250Aimages%252C%2520achieving%2520notable%2520results%2520with%2520just%25204%2520CT%2520scans.%2520The%2520proposed%2520approach%250Ayielded%2520a%2520Hausdorff%2520distance%2520with%252095th%2520percentile%2520%2528HD95%2529%2520of%25203.30%2520and%2520IoU%2520of%250A0.929%252C%2520surpassing%2520existing%2520methods%2520achieving%2520HD95%2520%25284.07%2529%2520and%2520IoU%2520%25280.927%2529%2520at%250Atheir%2520best.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Segmentation%20via%20Embedding%20Matching&entry.906535625=Weiyi%20Xie%20and%20Nathalie%20Willems%20and%20Nikolas%20Lessmann%20and%20Tom%20Gibbons%20and%20Daniele%20De%20Massari&entry.1292438233=%20%20Deep%20convolutional%20neural%20networks%20are%20widely%20used%20in%20medical%20image%0Asegmentation%20but%20require%20many%20labeled%20images%20for%20training.%20Annotating%0Athree-dimensional%20medical%20images%20is%20a%20time-consuming%20and%20costly%20process.%20To%0Aovercome%20this%20limitation%2C%20we%20propose%20a%20novel%20semi-supervised%20segmentation%0Amethod%20that%20leverages%20mostly%20unlabeled%20images%20and%20a%20small%20set%20of%20labeled%20images%0Ain%20training.%20Our%20approach%20involves%20assessing%20prediction%20uncertainty%20to%20identify%0Areliable%20predictions%20on%20unlabeled%20voxels%20from%20the%20teacher%20model.%20These%20voxels%0Aserve%20as%20pseudo-labels%20for%20training%20the%20student%20model.%20In%20voxels%20where%20the%0Ateacher%20model%20produces%20unreliable%20predictions%2C%20pseudo-labeling%20is%20carried%20out%0Abased%20on%20voxel-wise%20embedding%20correspondence%20using%20reference%20voxels%20from%0Alabeled%20images.%20We%20applied%20this%20method%20to%20automate%20hip%20bone%20segmentation%20in%20CT%0Aimages%2C%20achieving%20notable%20results%20with%20just%204%20CT%20scans.%20The%20proposed%20approach%0Ayielded%20a%20Hausdorff%20distance%20with%2095th%20percentile%20%28HD95%29%20of%203.30%20and%20IoU%20of%0A0.929%2C%20surpassing%20existing%20methods%20achieving%20HD95%20%284.07%29%20and%20IoU%20%280.927%29%20at%0Atheir%20best.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04638v1&entry.124074799=Read"},
{"title": "G-Adaptive mesh refinement -- leveraging graph neural networks and\n  differentiable finite element solvers", "author": "James Rowbottom and Georg Maierhofer and Teo Deveney and Katharina Schratz and Pietro Li\u00f2 and Carola-Bibiane Sch\u00f6nlieb and Chris Budd", "abstract": "  We present a novel, and effective, approach to the long-standing problem of\nmesh adaptivity in finite element methods (FEM). FE solvers are powerful tools\nfor solving partial differential equations (PDEs), but their cost and accuracy\nare critically dependent on the choice of mesh points. To keep computational\ncosts low, mesh relocation (r-adaptivity) seeks to optimise the position of a\nfixed number of mesh points to obtain the best FE solution accuracy. Classical\napproaches to this problem require the solution of a separate nonlinear\n\"meshing\" PDE to find the mesh point locations. This incurs significant cost at\nremeshing and relies on certain a-priori assumptions and guiding heuristics for\noptimal mesh point location. Recent machine learning approaches to r-adaptivity\nhave mainly focused on the construction of fast surrogates for such classical\nmethods. Our new approach combines a graph neural network (GNN) powered\narchitecture, with training based on direct minimisation of the FE solution\nerror with respect to the mesh point locations. The GNN employs graph neural\ndiffusion (GRAND), closely aligning the mesh solution space to that of\nclassical meshing methodologies, thus replacing heuristics with a learnable\nstrategy, and providing a strong inductive bias. This allows for rapid and\nrobust training and results in an extremely efficient and effective GNN\napproach to online r-adaptivity. This method outperforms classical and prior ML\napproaches to r-adaptive meshing on the test problems we consider, in\nparticular achieving lower FE solution error, whilst retaining the significant\nspeed-up over classical methods observed in prior ML work.\n", "link": "http://arxiv.org/abs/2407.04516v1", "date": "2024-07-05", "relevancy": 2.1654, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5937}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5114}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-Adaptive%20mesh%20refinement%20--%20leveraging%20graph%20neural%20networks%20and%0A%20%20differentiable%20finite%20element%20solvers&body=Title%3A%20G-Adaptive%20mesh%20refinement%20--%20leveraging%20graph%20neural%20networks%20and%0A%20%20differentiable%20finite%20element%20solvers%0AAuthor%3A%20James%20Rowbottom%20and%20Georg%20Maierhofer%20and%20Teo%20Deveney%20and%20Katharina%20Schratz%20and%20Pietro%20Li%C3%B2%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Chris%20Budd%0AAbstract%3A%20%20%20We%20present%20a%20novel%2C%20and%20effective%2C%20approach%20to%20the%20long-standing%20problem%20of%0Amesh%20adaptivity%20in%20finite%20element%20methods%20%28FEM%29.%20FE%20solvers%20are%20powerful%20tools%0Afor%20solving%20partial%20differential%20equations%20%28PDEs%29%2C%20but%20their%20cost%20and%20accuracy%0Aare%20critically%20dependent%20on%20the%20choice%20of%20mesh%20points.%20To%20keep%20computational%0Acosts%20low%2C%20mesh%20relocation%20%28r-adaptivity%29%20seeks%20to%20optimise%20the%20position%20of%20a%0Afixed%20number%20of%20mesh%20points%20to%20obtain%20the%20best%20FE%20solution%20accuracy.%20Classical%0Aapproaches%20to%20this%20problem%20require%20the%20solution%20of%20a%20separate%20nonlinear%0A%22meshing%22%20PDE%20to%20find%20the%20mesh%20point%20locations.%20This%20incurs%20significant%20cost%20at%0Aremeshing%20and%20relies%20on%20certain%20a-priori%20assumptions%20and%20guiding%20heuristics%20for%0Aoptimal%20mesh%20point%20location.%20Recent%20machine%20learning%20approaches%20to%20r-adaptivity%0Ahave%20mainly%20focused%20on%20the%20construction%20of%20fast%20surrogates%20for%20such%20classical%0Amethods.%20Our%20new%20approach%20combines%20a%20graph%20neural%20network%20%28GNN%29%20powered%0Aarchitecture%2C%20with%20training%20based%20on%20direct%20minimisation%20of%20the%20FE%20solution%0Aerror%20with%20respect%20to%20the%20mesh%20point%20locations.%20The%20GNN%20employs%20graph%20neural%0Adiffusion%20%28GRAND%29%2C%20closely%20aligning%20the%20mesh%20solution%20space%20to%20that%20of%0Aclassical%20meshing%20methodologies%2C%20thus%20replacing%20heuristics%20with%20a%20learnable%0Astrategy%2C%20and%20providing%20a%20strong%20inductive%20bias.%20This%20allows%20for%20rapid%20and%0Arobust%20training%20and%20results%20in%20an%20extremely%20efficient%20and%20effective%20GNN%0Aapproach%20to%20online%20r-adaptivity.%20This%20method%20outperforms%20classical%20and%20prior%20ML%0Aapproaches%20to%20r-adaptive%20meshing%20on%20the%20test%20problems%20we%20consider%2C%20in%0Aparticular%20achieving%20lower%20FE%20solution%20error%2C%20whilst%20retaining%20the%20significant%0Aspeed-up%20over%20classical%20methods%20observed%20in%20prior%20ML%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-Adaptive%2520mesh%2520refinement%2520--%2520leveraging%2520graph%2520neural%2520networks%2520and%250A%2520%2520differentiable%2520finite%2520element%2520solvers%26entry.906535625%3DJames%2520Rowbottom%2520and%2520Georg%2520Maierhofer%2520and%2520Teo%2520Deveney%2520and%2520Katharina%2520Schratz%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Chris%2520Budd%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%252C%2520and%2520effective%252C%2520approach%2520to%2520the%2520long-standing%2520problem%2520of%250Amesh%2520adaptivity%2520in%2520finite%2520element%2520methods%2520%2528FEM%2529.%2520FE%2520solvers%2520are%2520powerful%2520tools%250Afor%2520solving%2520partial%2520differential%2520equations%2520%2528PDEs%2529%252C%2520but%2520their%2520cost%2520and%2520accuracy%250Aare%2520critically%2520dependent%2520on%2520the%2520choice%2520of%2520mesh%2520points.%2520To%2520keep%2520computational%250Acosts%2520low%252C%2520mesh%2520relocation%2520%2528r-adaptivity%2529%2520seeks%2520to%2520optimise%2520the%2520position%2520of%2520a%250Afixed%2520number%2520of%2520mesh%2520points%2520to%2520obtain%2520the%2520best%2520FE%2520solution%2520accuracy.%2520Classical%250Aapproaches%2520to%2520this%2520problem%2520require%2520the%2520solution%2520of%2520a%2520separate%2520nonlinear%250A%2522meshing%2522%2520PDE%2520to%2520find%2520the%2520mesh%2520point%2520locations.%2520This%2520incurs%2520significant%2520cost%2520at%250Aremeshing%2520and%2520relies%2520on%2520certain%2520a-priori%2520assumptions%2520and%2520guiding%2520heuristics%2520for%250Aoptimal%2520mesh%2520point%2520location.%2520Recent%2520machine%2520learning%2520approaches%2520to%2520r-adaptivity%250Ahave%2520mainly%2520focused%2520on%2520the%2520construction%2520of%2520fast%2520surrogates%2520for%2520such%2520classical%250Amethods.%2520Our%2520new%2520approach%2520combines%2520a%2520graph%2520neural%2520network%2520%2528GNN%2529%2520powered%250Aarchitecture%252C%2520with%2520training%2520based%2520on%2520direct%2520minimisation%2520of%2520the%2520FE%2520solution%250Aerror%2520with%2520respect%2520to%2520the%2520mesh%2520point%2520locations.%2520The%2520GNN%2520employs%2520graph%2520neural%250Adiffusion%2520%2528GRAND%2529%252C%2520closely%2520aligning%2520the%2520mesh%2520solution%2520space%2520to%2520that%2520of%250Aclassical%2520meshing%2520methodologies%252C%2520thus%2520replacing%2520heuristics%2520with%2520a%2520learnable%250Astrategy%252C%2520and%2520providing%2520a%2520strong%2520inductive%2520bias.%2520This%2520allows%2520for%2520rapid%2520and%250Arobust%2520training%2520and%2520results%2520in%2520an%2520extremely%2520efficient%2520and%2520effective%2520GNN%250Aapproach%2520to%2520online%2520r-adaptivity.%2520This%2520method%2520outperforms%2520classical%2520and%2520prior%2520ML%250Aapproaches%2520to%2520r-adaptive%2520meshing%2520on%2520the%2520test%2520problems%2520we%2520consider%252C%2520in%250Aparticular%2520achieving%2520lower%2520FE%2520solution%2520error%252C%2520whilst%2520retaining%2520the%2520significant%250Aspeed-up%2520over%2520classical%2520methods%2520observed%2520in%2520prior%2520ML%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-Adaptive%20mesh%20refinement%20--%20leveraging%20graph%20neural%20networks%20and%0A%20%20differentiable%20finite%20element%20solvers&entry.906535625=James%20Rowbottom%20and%20Georg%20Maierhofer%20and%20Teo%20Deveney%20and%20Katharina%20Schratz%20and%20Pietro%20Li%C3%B2%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Chris%20Budd&entry.1292438233=%20%20We%20present%20a%20novel%2C%20and%20effective%2C%20approach%20to%20the%20long-standing%20problem%20of%0Amesh%20adaptivity%20in%20finite%20element%20methods%20%28FEM%29.%20FE%20solvers%20are%20powerful%20tools%0Afor%20solving%20partial%20differential%20equations%20%28PDEs%29%2C%20but%20their%20cost%20and%20accuracy%0Aare%20critically%20dependent%20on%20the%20choice%20of%20mesh%20points.%20To%20keep%20computational%0Acosts%20low%2C%20mesh%20relocation%20%28r-adaptivity%29%20seeks%20to%20optimise%20the%20position%20of%20a%0Afixed%20number%20of%20mesh%20points%20to%20obtain%20the%20best%20FE%20solution%20accuracy.%20Classical%0Aapproaches%20to%20this%20problem%20require%20the%20solution%20of%20a%20separate%20nonlinear%0A%22meshing%22%20PDE%20to%20find%20the%20mesh%20point%20locations.%20This%20incurs%20significant%20cost%20at%0Aremeshing%20and%20relies%20on%20certain%20a-priori%20assumptions%20and%20guiding%20heuristics%20for%0Aoptimal%20mesh%20point%20location.%20Recent%20machine%20learning%20approaches%20to%20r-adaptivity%0Ahave%20mainly%20focused%20on%20the%20construction%20of%20fast%20surrogates%20for%20such%20classical%0Amethods.%20Our%20new%20approach%20combines%20a%20graph%20neural%20network%20%28GNN%29%20powered%0Aarchitecture%2C%20with%20training%20based%20on%20direct%20minimisation%20of%20the%20FE%20solution%0Aerror%20with%20respect%20to%20the%20mesh%20point%20locations.%20The%20GNN%20employs%20graph%20neural%0Adiffusion%20%28GRAND%29%2C%20closely%20aligning%20the%20mesh%20solution%20space%20to%20that%20of%0Aclassical%20meshing%20methodologies%2C%20thus%20replacing%20heuristics%20with%20a%20learnable%0Astrategy%2C%20and%20providing%20a%20strong%20inductive%20bias.%20This%20allows%20for%20rapid%20and%0Arobust%20training%20and%20results%20in%20an%20extremely%20efficient%20and%20effective%20GNN%0Aapproach%20to%20online%20r-adaptivity.%20This%20method%20outperforms%20classical%20and%20prior%20ML%0Aapproaches%20to%20r-adaptive%20meshing%20on%20the%20test%20problems%20we%20consider%2C%20in%0Aparticular%20achieving%20lower%20FE%20solution%20error%2C%20whilst%20retaining%20the%20significant%0Aspeed-up%20over%20classical%20methods%20observed%20in%20prior%20ML%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04516v1&entry.124074799=Read"},
{"title": "Advanced Smart City Monitoring: Real-Time Identification of Indian\n  Citizen Attributes", "author": "Shubham Kale and Shashank Sharma and Abhilash Khuntia", "abstract": "  This project focuses on creating a smart surveillance system for Indian\ncities that can identify and analyze people's attributes in real time. Using\nadvanced technologies like artificial intelligence and machine learning, the\nsystem can recognize attributes such as upper body color, what the person is\nwearing, accessories they are wearing, headgear, etc., and analyze behavior\nthrough cameras installed around the city.\n", "link": "http://arxiv.org/abs/2407.03305v2", "date": "2024-07-05", "relevancy": 2.1559, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4383}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4314}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20Smart%20City%20Monitoring%3A%20Real-Time%20Identification%20of%20Indian%0A%20%20Citizen%20Attributes&body=Title%3A%20Advanced%20Smart%20City%20Monitoring%3A%20Real-Time%20Identification%20of%20Indian%0A%20%20Citizen%20Attributes%0AAuthor%3A%20Shubham%20Kale%20and%20Shashank%20Sharma%20and%20Abhilash%20Khuntia%0AAbstract%3A%20%20%20This%20project%20focuses%20on%20creating%20a%20smart%20surveillance%20system%20for%20Indian%0Acities%20that%20can%20identify%20and%20analyze%20people%27s%20attributes%20in%20real%20time.%20Using%0Aadvanced%20technologies%20like%20artificial%20intelligence%20and%20machine%20learning%2C%20the%0Asystem%20can%20recognize%20attributes%20such%20as%20upper%20body%20color%2C%20what%20the%20person%20is%0Awearing%2C%20accessories%20they%20are%20wearing%2C%20headgear%2C%20etc.%2C%20and%20analyze%20behavior%0Athrough%20cameras%20installed%20around%20the%20city.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03305v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520Smart%2520City%2520Monitoring%253A%2520Real-Time%2520Identification%2520of%2520Indian%250A%2520%2520Citizen%2520Attributes%26entry.906535625%3DShubham%2520Kale%2520and%2520Shashank%2520Sharma%2520and%2520Abhilash%2520Khuntia%26entry.1292438233%3D%2520%2520This%2520project%2520focuses%2520on%2520creating%2520a%2520smart%2520surveillance%2520system%2520for%2520Indian%250Acities%2520that%2520can%2520identify%2520and%2520analyze%2520people%2527s%2520attributes%2520in%2520real%2520time.%2520Using%250Aadvanced%2520technologies%2520like%2520artificial%2520intelligence%2520and%2520machine%2520learning%252C%2520the%250Asystem%2520can%2520recognize%2520attributes%2520such%2520as%2520upper%2520body%2520color%252C%2520what%2520the%2520person%2520is%250Awearing%252C%2520accessories%2520they%2520are%2520wearing%252C%2520headgear%252C%2520etc.%252C%2520and%2520analyze%2520behavior%250Athrough%2520cameras%2520installed%2520around%2520the%2520city.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03305v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20Smart%20City%20Monitoring%3A%20Real-Time%20Identification%20of%20Indian%0A%20%20Citizen%20Attributes&entry.906535625=Shubham%20Kale%20and%20Shashank%20Sharma%20and%20Abhilash%20Khuntia&entry.1292438233=%20%20This%20project%20focuses%20on%20creating%20a%20smart%20surveillance%20system%20for%20Indian%0Acities%20that%20can%20identify%20and%20analyze%20people%27s%20attributes%20in%20real%20time.%20Using%0Aadvanced%20technologies%20like%20artificial%20intelligence%20and%20machine%20learning%2C%20the%0Asystem%20can%20recognize%20attributes%20such%20as%20upper%20body%20color%2C%20what%20the%20person%20is%0Awearing%2C%20accessories%20they%20are%20wearing%2C%20headgear%2C%20etc.%2C%20and%20analyze%20behavior%0Athrough%20cameras%20installed%20around%20the%20city.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03305v2&entry.124074799=Read"},
{"title": "Multitaper mel-spectrograms for keyword spotting", "author": "Douglas Baptista de Souza and Khaled Jamal Bakri and Fernanda Ferreira and Juliana Inacio", "abstract": "  Keyword spotting (KWS) is one of the speech recognition tasks most sensitive\nto the quality of the feature representation. However, the research on KWS has\ntraditionally focused on new model topologies, putting little emphasis on other\naspects like feature extraction. This paper investigates the use of the\nmultitaper technique to create improved features for KWS. The experimental\nstudy is carried out for different test scenarios, windows and parameters,\ndatasets, and neural networks commonly used in embedded KWS applications.\nExperiment results confirm the advantages of using the proposed improved\nfeatures.\n", "link": "http://arxiv.org/abs/2407.04662v1", "date": "2024-07-05", "relevancy": 2.1527, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4565}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4178}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multitaper%20mel-spectrograms%20for%20keyword%20spotting&body=Title%3A%20Multitaper%20mel-spectrograms%20for%20keyword%20spotting%0AAuthor%3A%20Douglas%20Baptista%20de%20Souza%20and%20Khaled%20Jamal%20Bakri%20and%20Fernanda%20Ferreira%20and%20Juliana%20Inacio%0AAbstract%3A%20%20%20Keyword%20spotting%20%28KWS%29%20is%20one%20of%20the%20speech%20recognition%20tasks%20most%20sensitive%0Ato%20the%20quality%20of%20the%20feature%20representation.%20However%2C%20the%20research%20on%20KWS%20has%0Atraditionally%20focused%20on%20new%20model%20topologies%2C%20putting%20little%20emphasis%20on%20other%0Aaspects%20like%20feature%20extraction.%20This%20paper%20investigates%20the%20use%20of%20the%0Amultitaper%20technique%20to%20create%20improved%20features%20for%20KWS.%20The%20experimental%0Astudy%20is%20carried%20out%20for%20different%20test%20scenarios%2C%20windows%20and%20parameters%2C%0Adatasets%2C%20and%20neural%20networks%20commonly%20used%20in%20embedded%20KWS%20applications.%0AExperiment%20results%20confirm%20the%20advantages%20of%20using%20the%20proposed%20improved%0Afeatures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultitaper%2520mel-spectrograms%2520for%2520keyword%2520spotting%26entry.906535625%3DDouglas%2520Baptista%2520de%2520Souza%2520and%2520Khaled%2520Jamal%2520Bakri%2520and%2520Fernanda%2520Ferreira%2520and%2520Juliana%2520Inacio%26entry.1292438233%3D%2520%2520Keyword%2520spotting%2520%2528KWS%2529%2520is%2520one%2520of%2520the%2520speech%2520recognition%2520tasks%2520most%2520sensitive%250Ato%2520the%2520quality%2520of%2520the%2520feature%2520representation.%2520However%252C%2520the%2520research%2520on%2520KWS%2520has%250Atraditionally%2520focused%2520on%2520new%2520model%2520topologies%252C%2520putting%2520little%2520emphasis%2520on%2520other%250Aaspects%2520like%2520feature%2520extraction.%2520This%2520paper%2520investigates%2520the%2520use%2520of%2520the%250Amultitaper%2520technique%2520to%2520create%2520improved%2520features%2520for%2520KWS.%2520The%2520experimental%250Astudy%2520is%2520carried%2520out%2520for%2520different%2520test%2520scenarios%252C%2520windows%2520and%2520parameters%252C%250Adatasets%252C%2520and%2520neural%2520networks%2520commonly%2520used%2520in%2520embedded%2520KWS%2520applications.%250AExperiment%2520results%2520confirm%2520the%2520advantages%2520of%2520using%2520the%2520proposed%2520improved%250Afeatures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multitaper%20mel-spectrograms%20for%20keyword%20spotting&entry.906535625=Douglas%20Baptista%20de%20Souza%20and%20Khaled%20Jamal%20Bakri%20and%20Fernanda%20Ferreira%20and%20Juliana%20Inacio&entry.1292438233=%20%20Keyword%20spotting%20%28KWS%29%20is%20one%20of%20the%20speech%20recognition%20tasks%20most%20sensitive%0Ato%20the%20quality%20of%20the%20feature%20representation.%20However%2C%20the%20research%20on%20KWS%20has%0Atraditionally%20focused%20on%20new%20model%20topologies%2C%20putting%20little%20emphasis%20on%20other%0Aaspects%20like%20feature%20extraction.%20This%20paper%20investigates%20the%20use%20of%20the%0Amultitaper%20technique%20to%20create%20improved%20features%20for%20KWS.%20The%20experimental%0Astudy%20is%20carried%20out%20for%20different%20test%20scenarios%2C%20windows%20and%20parameters%2C%0Adatasets%2C%20and%20neural%20networks%20commonly%20used%20in%20embedded%20KWS%20applications.%0AExperiment%20results%20confirm%20the%20advantages%20of%20using%20the%20proposed%20improved%0Afeatures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04662v1&entry.124074799=Read"},
{"title": "SAM Fewshot Finetuning for Anatomical Segmentation in Medical Images", "author": "Weiyi Xie and Nathalie Willems and Shubham Patil and Yang Li and Mayank Kumar", "abstract": "  We propose a straightforward yet highly effective few-shot fine-tuning\nstrategy for adapting the Segment Anything (SAM) to anatomical segmentation\ntasks in medical images. Our novel approach revolves around reformulating the\nmask decoder within SAM, leveraging few-shot embeddings derived from a limited\nset of labeled images (few-shot collection) as prompts for querying anatomical\nobjects captured in image embeddings. This innovative reformulation greatly\nreduces the need for time-consuming online user interactions for labeling\nvolumetric images, such as exhaustively marking points and bounding boxes to\nprovide prompts slice by slice. With our method, users can manually segment a\nfew 2D slices offline, and the embeddings of these annotated image regions\nserve as effective prompts for online segmentation tasks. Our method\nprioritizes the efficiency of the fine-tuning process by exclusively training\nthe mask decoder through caching mechanisms while keeping the image encoder\nfrozen. Importantly, this approach is not limited to volumetric medical images,\nbut can generically be applied to any 2D/3D segmentation task. To thoroughly\nevaluate our method, we conducted extensive validation on four datasets,\ncovering six anatomical segmentation tasks across two modalities. Furthermore,\nwe conducted a comparative analysis of different prompting options within SAM\nand the fully-supervised nnU-Net. The results demonstrate the superior\nperformance of our method compared to SAM employing only point prompts\n(approximately 50% improvement in IoU) and performs on-par with fully\nsupervised methods whilst reducing the requirement of labeled data by at least\nan order of magnitude.\n", "link": "http://arxiv.org/abs/2407.04651v1", "date": "2024-07-05", "relevancy": 2.149, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5647}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5331}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM%20Fewshot%20Finetuning%20for%20Anatomical%20Segmentation%20in%20Medical%20Images&body=Title%3A%20SAM%20Fewshot%20Finetuning%20for%20Anatomical%20Segmentation%20in%20Medical%20Images%0AAuthor%3A%20Weiyi%20Xie%20and%20Nathalie%20Willems%20and%20Shubham%20Patil%20and%20Yang%20Li%20and%20Mayank%20Kumar%0AAbstract%3A%20%20%20We%20propose%20a%20straightforward%20yet%20highly%20effective%20few-shot%20fine-tuning%0Astrategy%20for%20adapting%20the%20Segment%20Anything%20%28SAM%29%20to%20anatomical%20segmentation%0Atasks%20in%20medical%20images.%20Our%20novel%20approach%20revolves%20around%20reformulating%20the%0Amask%20decoder%20within%20SAM%2C%20leveraging%20few-shot%20embeddings%20derived%20from%20a%20limited%0Aset%20of%20labeled%20images%20%28few-shot%20collection%29%20as%20prompts%20for%20querying%20anatomical%0Aobjects%20captured%20in%20image%20embeddings.%20This%20innovative%20reformulation%20greatly%0Areduces%20the%20need%20for%20time-consuming%20online%20user%20interactions%20for%20labeling%0Avolumetric%20images%2C%20such%20as%20exhaustively%20marking%20points%20and%20bounding%20boxes%20to%0Aprovide%20prompts%20slice%20by%20slice.%20With%20our%20method%2C%20users%20can%20manually%20segment%20a%0Afew%202D%20slices%20offline%2C%20and%20the%20embeddings%20of%20these%20annotated%20image%20regions%0Aserve%20as%20effective%20prompts%20for%20online%20segmentation%20tasks.%20Our%20method%0Aprioritizes%20the%20efficiency%20of%20the%20fine-tuning%20process%20by%20exclusively%20training%0Athe%20mask%20decoder%20through%20caching%20mechanisms%20while%20keeping%20the%20image%20encoder%0Afrozen.%20Importantly%2C%20this%20approach%20is%20not%20limited%20to%20volumetric%20medical%20images%2C%0Abut%20can%20generically%20be%20applied%20to%20any%202D/3D%20segmentation%20task.%20To%20thoroughly%0Aevaluate%20our%20method%2C%20we%20conducted%20extensive%20validation%20on%20four%20datasets%2C%0Acovering%20six%20anatomical%20segmentation%20tasks%20across%20two%20modalities.%20Furthermore%2C%0Awe%20conducted%20a%20comparative%20analysis%20of%20different%20prompting%20options%20within%20SAM%0Aand%20the%20fully-supervised%20nnU-Net.%20The%20results%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method%20compared%20to%20SAM%20employing%20only%20point%20prompts%0A%28approximately%2050%25%20improvement%20in%20IoU%29%20and%20performs%20on-par%20with%20fully%0Asupervised%20methods%20whilst%20reducing%20the%20requirement%20of%20labeled%20data%20by%20at%20least%0Aan%20order%20of%20magnitude.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04651v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM%2520Fewshot%2520Finetuning%2520for%2520Anatomical%2520Segmentation%2520in%2520Medical%2520Images%26entry.906535625%3DWeiyi%2520Xie%2520and%2520Nathalie%2520Willems%2520and%2520Shubham%2520Patil%2520and%2520Yang%2520Li%2520and%2520Mayank%2520Kumar%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520straightforward%2520yet%2520highly%2520effective%2520few-shot%2520fine-tuning%250Astrategy%2520for%2520adapting%2520the%2520Segment%2520Anything%2520%2528SAM%2529%2520to%2520anatomical%2520segmentation%250Atasks%2520in%2520medical%2520images.%2520Our%2520novel%2520approach%2520revolves%2520around%2520reformulating%2520the%250Amask%2520decoder%2520within%2520SAM%252C%2520leveraging%2520few-shot%2520embeddings%2520derived%2520from%2520a%2520limited%250Aset%2520of%2520labeled%2520images%2520%2528few-shot%2520collection%2529%2520as%2520prompts%2520for%2520querying%2520anatomical%250Aobjects%2520captured%2520in%2520image%2520embeddings.%2520This%2520innovative%2520reformulation%2520greatly%250Areduces%2520the%2520need%2520for%2520time-consuming%2520online%2520user%2520interactions%2520for%2520labeling%250Avolumetric%2520images%252C%2520such%2520as%2520exhaustively%2520marking%2520points%2520and%2520bounding%2520boxes%2520to%250Aprovide%2520prompts%2520slice%2520by%2520slice.%2520With%2520our%2520method%252C%2520users%2520can%2520manually%2520segment%2520a%250Afew%25202D%2520slices%2520offline%252C%2520and%2520the%2520embeddings%2520of%2520these%2520annotated%2520image%2520regions%250Aserve%2520as%2520effective%2520prompts%2520for%2520online%2520segmentation%2520tasks.%2520Our%2520method%250Aprioritizes%2520the%2520efficiency%2520of%2520the%2520fine-tuning%2520process%2520by%2520exclusively%2520training%250Athe%2520mask%2520decoder%2520through%2520caching%2520mechanisms%2520while%2520keeping%2520the%2520image%2520encoder%250Afrozen.%2520Importantly%252C%2520this%2520approach%2520is%2520not%2520limited%2520to%2520volumetric%2520medical%2520images%252C%250Abut%2520can%2520generically%2520be%2520applied%2520to%2520any%25202D/3D%2520segmentation%2520task.%2520To%2520thoroughly%250Aevaluate%2520our%2520method%252C%2520we%2520conducted%2520extensive%2520validation%2520on%2520four%2520datasets%252C%250Acovering%2520six%2520anatomical%2520segmentation%2520tasks%2520across%2520two%2520modalities.%2520Furthermore%252C%250Awe%2520conducted%2520a%2520comparative%2520analysis%2520of%2520different%2520prompting%2520options%2520within%2520SAM%250Aand%2520the%2520fully-supervised%2520nnU-Net.%2520The%2520results%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520our%2520method%2520compared%2520to%2520SAM%2520employing%2520only%2520point%2520prompts%250A%2528approximately%252050%2525%2520improvement%2520in%2520IoU%2529%2520and%2520performs%2520on-par%2520with%2520fully%250Asupervised%2520methods%2520whilst%2520reducing%2520the%2520requirement%2520of%2520labeled%2520data%2520by%2520at%2520least%250Aan%2520order%2520of%2520magnitude.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04651v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM%20Fewshot%20Finetuning%20for%20Anatomical%20Segmentation%20in%20Medical%20Images&entry.906535625=Weiyi%20Xie%20and%20Nathalie%20Willems%20and%20Shubham%20Patil%20and%20Yang%20Li%20and%20Mayank%20Kumar&entry.1292438233=%20%20We%20propose%20a%20straightforward%20yet%20highly%20effective%20few-shot%20fine-tuning%0Astrategy%20for%20adapting%20the%20Segment%20Anything%20%28SAM%29%20to%20anatomical%20segmentation%0Atasks%20in%20medical%20images.%20Our%20novel%20approach%20revolves%20around%20reformulating%20the%0Amask%20decoder%20within%20SAM%2C%20leveraging%20few-shot%20embeddings%20derived%20from%20a%20limited%0Aset%20of%20labeled%20images%20%28few-shot%20collection%29%20as%20prompts%20for%20querying%20anatomical%0Aobjects%20captured%20in%20image%20embeddings.%20This%20innovative%20reformulation%20greatly%0Areduces%20the%20need%20for%20time-consuming%20online%20user%20interactions%20for%20labeling%0Avolumetric%20images%2C%20such%20as%20exhaustively%20marking%20points%20and%20bounding%20boxes%20to%0Aprovide%20prompts%20slice%20by%20slice.%20With%20our%20method%2C%20users%20can%20manually%20segment%20a%0Afew%202D%20slices%20offline%2C%20and%20the%20embeddings%20of%20these%20annotated%20image%20regions%0Aserve%20as%20effective%20prompts%20for%20online%20segmentation%20tasks.%20Our%20method%0Aprioritizes%20the%20efficiency%20of%20the%20fine-tuning%20process%20by%20exclusively%20training%0Athe%20mask%20decoder%20through%20caching%20mechanisms%20while%20keeping%20the%20image%20encoder%0Afrozen.%20Importantly%2C%20this%20approach%20is%20not%20limited%20to%20volumetric%20medical%20images%2C%0Abut%20can%20generically%20be%20applied%20to%20any%202D/3D%20segmentation%20task.%20To%20thoroughly%0Aevaluate%20our%20method%2C%20we%20conducted%20extensive%20validation%20on%20four%20datasets%2C%0Acovering%20six%20anatomical%20segmentation%20tasks%20across%20two%20modalities.%20Furthermore%2C%0Awe%20conducted%20a%20comparative%20analysis%20of%20different%20prompting%20options%20within%20SAM%0Aand%20the%20fully-supervised%20nnU-Net.%20The%20results%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method%20compared%20to%20SAM%20employing%20only%20point%20prompts%0A%28approximately%2050%25%20improvement%20in%20IoU%29%20and%20performs%20on-par%20with%20fully%0Asupervised%20methods%20whilst%20reducing%20the%20requirement%20of%20labeled%20data%20by%20at%20least%0Aan%20order%20of%20magnitude.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04651v1&entry.124074799=Read"},
{"title": "Multi-Attention Integrated Deep Learning Frameworks for Enhanced Breast\n  Cancer Segmentation and Identification", "author": "Pandiyaraju V and Shravan Venkatraman and Pavan Kumar S and Santhosh Malarvannan and Kannan A", "abstract": "  Breast cancer poses a profound threat to lives globally, claiming numerous\nlives each year. Therefore, timely detection is crucial for early intervention\nand improved chances of survival. Accurately diagnosing and classifying breast\ntumors using ultrasound images is a persistent challenge in medicine, demanding\ncutting-edge solutions for improved treatment strategies. This research\nintroduces multiattention-enhanced deep learning (DL) frameworks designed for\nthe classification and segmentation of breast cancer tumors from ultrasound\nimages. A spatial channel attention mechanism is proposed for segmenting tumors\nfrom ultrasound images, utilizing a novel LinkNet DL framework with an\nInceptionResNet backbone. Following this, the paper proposes a deep\nconvolutional neural network with an integrated multi-attention framework\n(DCNNIMAF) to classify the segmented tumor as benign, malignant, or normal.\nFrom experimental results, it is observed that the segmentation model has\nrecorded an accuracy of 98.1%, with a minimal loss of 0.6%. It has also\nachieved high Intersection over Union (IoU) and Dice Coefficient scores of\n96.9% and 97.2%, respectively. Similarly, the classification model has attained\nan accuracy of 99.2%, with a low loss of 0.31%. Furthermore, the classification\nframework has achieved outstanding F1-Score, precision, and recall values of\n99.1%, 99.3%, and 99.1%, respectively. By offering a robust framework for early\ndetection and accurate classification of breast cancer, this proposed work\nsignificantly advances the field of medical image analysis, potentially\nimproving diagnostic precision and patient outcomes.\n", "link": "http://arxiv.org/abs/2407.02844v2", "date": "2024-07-05", "relevancy": 2.1154, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5371}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5367}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Attention%20Integrated%20Deep%20Learning%20Frameworks%20for%20Enhanced%20Breast%0A%20%20Cancer%20Segmentation%20and%20Identification&body=Title%3A%20Multi-Attention%20Integrated%20Deep%20Learning%20Frameworks%20for%20Enhanced%20Breast%0A%20%20Cancer%20Segmentation%20and%20Identification%0AAuthor%3A%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Santhosh%20Malarvannan%20and%20Kannan%20A%0AAbstract%3A%20%20%20Breast%20cancer%20poses%20a%20profound%20threat%20to%20lives%20globally%2C%20claiming%20numerous%0Alives%20each%20year.%20Therefore%2C%20timely%20detection%20is%20crucial%20for%20early%20intervention%0Aand%20improved%20chances%20of%20survival.%20Accurately%20diagnosing%20and%20classifying%20breast%0Atumors%20using%20ultrasound%20images%20is%20a%20persistent%20challenge%20in%20medicine%2C%20demanding%0Acutting-edge%20solutions%20for%20improved%20treatment%20strategies.%20This%20research%0Aintroduces%20multiattention-enhanced%20deep%20learning%20%28DL%29%20frameworks%20designed%20for%0Athe%20classification%20and%20segmentation%20of%20breast%20cancer%20tumors%20from%20ultrasound%0Aimages.%20A%20spatial%20channel%20attention%20mechanism%20is%20proposed%20for%20segmenting%20tumors%0Afrom%20ultrasound%20images%2C%20utilizing%20a%20novel%20LinkNet%20DL%20framework%20with%20an%0AInceptionResNet%20backbone.%20Following%20this%2C%20the%20paper%20proposes%20a%20deep%0Aconvolutional%20neural%20network%20with%20an%20integrated%20multi-attention%20framework%0A%28DCNNIMAF%29%20to%20classify%20the%20segmented%20tumor%20as%20benign%2C%20malignant%2C%20or%20normal.%0AFrom%20experimental%20results%2C%20it%20is%20observed%20that%20the%20segmentation%20model%20has%0Arecorded%20an%20accuracy%20of%2098.1%25%2C%20with%20a%20minimal%20loss%20of%200.6%25.%20It%20has%20also%0Aachieved%20high%20Intersection%20over%20Union%20%28IoU%29%20and%20Dice%20Coefficient%20scores%20of%0A96.9%25%20and%2097.2%25%2C%20respectively.%20Similarly%2C%20the%20classification%20model%20has%20attained%0Aan%20accuracy%20of%2099.2%25%2C%20with%20a%20low%20loss%20of%200.31%25.%20Furthermore%2C%20the%20classification%0Aframework%20has%20achieved%20outstanding%20F1-Score%2C%20precision%2C%20and%20recall%20values%20of%0A99.1%25%2C%2099.3%25%2C%20and%2099.1%25%2C%20respectively.%20By%20offering%20a%20robust%20framework%20for%20early%0Adetection%20and%20accurate%20classification%20of%20breast%20cancer%2C%20this%20proposed%20work%0Asignificantly%20advances%20the%20field%20of%20medical%20image%20analysis%2C%20potentially%0Aimproving%20diagnostic%20precision%20and%20patient%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Attention%2520Integrated%2520Deep%2520Learning%2520Frameworks%2520for%2520Enhanced%2520Breast%250A%2520%2520Cancer%2520Segmentation%2520and%2520Identification%26entry.906535625%3DPandiyaraju%2520V%2520and%2520Shravan%2520Venkatraman%2520and%2520Pavan%2520Kumar%2520S%2520and%2520Santhosh%2520Malarvannan%2520and%2520Kannan%2520A%26entry.1292438233%3D%2520%2520Breast%2520cancer%2520poses%2520a%2520profound%2520threat%2520to%2520lives%2520globally%252C%2520claiming%2520numerous%250Alives%2520each%2520year.%2520Therefore%252C%2520timely%2520detection%2520is%2520crucial%2520for%2520early%2520intervention%250Aand%2520improved%2520chances%2520of%2520survival.%2520Accurately%2520diagnosing%2520and%2520classifying%2520breast%250Atumors%2520using%2520ultrasound%2520images%2520is%2520a%2520persistent%2520challenge%2520in%2520medicine%252C%2520demanding%250Acutting-edge%2520solutions%2520for%2520improved%2520treatment%2520strategies.%2520This%2520research%250Aintroduces%2520multiattention-enhanced%2520deep%2520learning%2520%2528DL%2529%2520frameworks%2520designed%2520for%250Athe%2520classification%2520and%2520segmentation%2520of%2520breast%2520cancer%2520tumors%2520from%2520ultrasound%250Aimages.%2520A%2520spatial%2520channel%2520attention%2520mechanism%2520is%2520proposed%2520for%2520segmenting%2520tumors%250Afrom%2520ultrasound%2520images%252C%2520utilizing%2520a%2520novel%2520LinkNet%2520DL%2520framework%2520with%2520an%250AInceptionResNet%2520backbone.%2520Following%2520this%252C%2520the%2520paper%2520proposes%2520a%2520deep%250Aconvolutional%2520neural%2520network%2520with%2520an%2520integrated%2520multi-attention%2520framework%250A%2528DCNNIMAF%2529%2520to%2520classify%2520the%2520segmented%2520tumor%2520as%2520benign%252C%2520malignant%252C%2520or%2520normal.%250AFrom%2520experimental%2520results%252C%2520it%2520is%2520observed%2520that%2520the%2520segmentation%2520model%2520has%250Arecorded%2520an%2520accuracy%2520of%252098.1%2525%252C%2520with%2520a%2520minimal%2520loss%2520of%25200.6%2525.%2520It%2520has%2520also%250Aachieved%2520high%2520Intersection%2520over%2520Union%2520%2528IoU%2529%2520and%2520Dice%2520Coefficient%2520scores%2520of%250A96.9%2525%2520and%252097.2%2525%252C%2520respectively.%2520Similarly%252C%2520the%2520classification%2520model%2520has%2520attained%250Aan%2520accuracy%2520of%252099.2%2525%252C%2520with%2520a%2520low%2520loss%2520of%25200.31%2525.%2520Furthermore%252C%2520the%2520classification%250Aframework%2520has%2520achieved%2520outstanding%2520F1-Score%252C%2520precision%252C%2520and%2520recall%2520values%2520of%250A99.1%2525%252C%252099.3%2525%252C%2520and%252099.1%2525%252C%2520respectively.%2520By%2520offering%2520a%2520robust%2520framework%2520for%2520early%250Adetection%2520and%2520accurate%2520classification%2520of%2520breast%2520cancer%252C%2520this%2520proposed%2520work%250Asignificantly%2520advances%2520the%2520field%2520of%2520medical%2520image%2520analysis%252C%2520potentially%250Aimproving%2520diagnostic%2520precision%2520and%2520patient%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Attention%20Integrated%20Deep%20Learning%20Frameworks%20for%20Enhanced%20Breast%0A%20%20Cancer%20Segmentation%20and%20Identification&entry.906535625=Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Pavan%20Kumar%20S%20and%20Santhosh%20Malarvannan%20and%20Kannan%20A&entry.1292438233=%20%20Breast%20cancer%20poses%20a%20profound%20threat%20to%20lives%20globally%2C%20claiming%20numerous%0Alives%20each%20year.%20Therefore%2C%20timely%20detection%20is%20crucial%20for%20early%20intervention%0Aand%20improved%20chances%20of%20survival.%20Accurately%20diagnosing%20and%20classifying%20breast%0Atumors%20using%20ultrasound%20images%20is%20a%20persistent%20challenge%20in%20medicine%2C%20demanding%0Acutting-edge%20solutions%20for%20improved%20treatment%20strategies.%20This%20research%0Aintroduces%20multiattention-enhanced%20deep%20learning%20%28DL%29%20frameworks%20designed%20for%0Athe%20classification%20and%20segmentation%20of%20breast%20cancer%20tumors%20from%20ultrasound%0Aimages.%20A%20spatial%20channel%20attention%20mechanism%20is%20proposed%20for%20segmenting%20tumors%0Afrom%20ultrasound%20images%2C%20utilizing%20a%20novel%20LinkNet%20DL%20framework%20with%20an%0AInceptionResNet%20backbone.%20Following%20this%2C%20the%20paper%20proposes%20a%20deep%0Aconvolutional%20neural%20network%20with%20an%20integrated%20multi-attention%20framework%0A%28DCNNIMAF%29%20to%20classify%20the%20segmented%20tumor%20as%20benign%2C%20malignant%2C%20or%20normal.%0AFrom%20experimental%20results%2C%20it%20is%20observed%20that%20the%20segmentation%20model%20has%0Arecorded%20an%20accuracy%20of%2098.1%25%2C%20with%20a%20minimal%20loss%20of%200.6%25.%20It%20has%20also%0Aachieved%20high%20Intersection%20over%20Union%20%28IoU%29%20and%20Dice%20Coefficient%20scores%20of%0A96.9%25%20and%2097.2%25%2C%20respectively.%20Similarly%2C%20the%20classification%20model%20has%20attained%0Aan%20accuracy%20of%2099.2%25%2C%20with%20a%20low%20loss%20of%200.31%25.%20Furthermore%2C%20the%20classification%0Aframework%20has%20achieved%20outstanding%20F1-Score%2C%20precision%2C%20and%20recall%20values%20of%0A99.1%25%2C%2099.3%25%2C%20and%2099.1%25%2C%20respectively.%20By%20offering%20a%20robust%20framework%20for%20early%0Adetection%20and%20accurate%20classification%20of%20breast%20cancer%2C%20this%20proposed%20work%0Asignificantly%20advances%20the%20field%20of%20medical%20image%20analysis%2C%20potentially%0Aimproving%20diagnostic%20precision%20and%20patient%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02844v2&entry.124074799=Read"},
{"title": "UltraCortex: Submillimeter Ultra-High Field 9.4 T1 Brain MR Image\n  Collection and Manual Cortical Segmentations", "author": "Lucas Mahler and Julius Steiglechner and Benjamin Bender and Tobias Lindig and Dana Ramadan and Jonas Bause and Florian Birk and Rahel Heule and Edyta Charyasz and Michael Erb and Vinod Jangir Kumar and Gisela E Hagberg and Pascal Martin and Gabriele Lohmann and Klaus Scheffler", "abstract": "  The UltraCortex repository (https://www.ultracortex.org) houses magnetic\nresonance imaging data of the human brain obtained at an ultra-high field\nstrength of 9.4 T. It contains 86 structural MR images with spatial resolutions\nranging from 0.6 to 0.8 mm. Additionally, the repository includes segmentations\nof 12 brains into gray and white matter compartments. These segmentations have\nbeen independently validated by two expert neuroradiologists, thus establishing\nthem as a reliable gold standard. This resource provides researchers with\naccess to high-quality brain imaging data and validated segmentations,\nfacilitating neuroimaging studies and advancing our understanding of brain\nstructure and function. Existing repositories do not accommodate field\nstrengths beyond 7 T, nor do they offer validated segmentations, underscoring\nthe significance of this new resource.\n", "link": "http://arxiv.org/abs/2406.18571v2", "date": "2024-07-05", "relevancy": 2.1113, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4388}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4388}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UltraCortex%3A%20Submillimeter%20Ultra-High%20Field%209.4%20T1%20Brain%20MR%20Image%0A%20%20Collection%20and%20Manual%20Cortical%20Segmentations&body=Title%3A%20UltraCortex%3A%20Submillimeter%20Ultra-High%20Field%209.4%20T1%20Brain%20MR%20Image%0A%20%20Collection%20and%20Manual%20Cortical%20Segmentations%0AAuthor%3A%20Lucas%20Mahler%20and%20Julius%20Steiglechner%20and%20Benjamin%20Bender%20and%20Tobias%20Lindig%20and%20Dana%20Ramadan%20and%20Jonas%20Bause%20and%20Florian%20Birk%20and%20Rahel%20Heule%20and%20Edyta%20Charyasz%20and%20Michael%20Erb%20and%20Vinod%20Jangir%20Kumar%20and%20Gisela%20E%20Hagberg%20and%20Pascal%20Martin%20and%20Gabriele%20Lohmann%20and%20Klaus%20Scheffler%0AAbstract%3A%20%20%20The%20UltraCortex%20repository%20%28https%3A//www.ultracortex.org%29%20houses%20magnetic%0Aresonance%20imaging%20data%20of%20the%20human%20brain%20obtained%20at%20an%20ultra-high%20field%0Astrength%20of%209.4%20T.%20It%20contains%2086%20structural%20MR%20images%20with%20spatial%20resolutions%0Aranging%20from%200.6%20to%200.8%20mm.%20Additionally%2C%20the%20repository%20includes%20segmentations%0Aof%2012%20brains%20into%20gray%20and%20white%20matter%20compartments.%20These%20segmentations%20have%0Abeen%20independently%20validated%20by%20two%20expert%20neuroradiologists%2C%20thus%20establishing%0Athem%20as%20a%20reliable%20gold%20standard.%20This%20resource%20provides%20researchers%20with%0Aaccess%20to%20high-quality%20brain%20imaging%20data%20and%20validated%20segmentations%2C%0Afacilitating%20neuroimaging%20studies%20and%20advancing%20our%20understanding%20of%20brain%0Astructure%20and%20function.%20Existing%20repositories%20do%20not%20accommodate%20field%0Astrengths%20beyond%207%20T%2C%20nor%20do%20they%20offer%20validated%20segmentations%2C%20underscoring%0Athe%20significance%20of%20this%20new%20resource.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18571v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltraCortex%253A%2520Submillimeter%2520Ultra-High%2520Field%25209.4%2520T1%2520Brain%2520MR%2520Image%250A%2520%2520Collection%2520and%2520Manual%2520Cortical%2520Segmentations%26entry.906535625%3DLucas%2520Mahler%2520and%2520Julius%2520Steiglechner%2520and%2520Benjamin%2520Bender%2520and%2520Tobias%2520Lindig%2520and%2520Dana%2520Ramadan%2520and%2520Jonas%2520Bause%2520and%2520Florian%2520Birk%2520and%2520Rahel%2520Heule%2520and%2520Edyta%2520Charyasz%2520and%2520Michael%2520Erb%2520and%2520Vinod%2520Jangir%2520Kumar%2520and%2520Gisela%2520E%2520Hagberg%2520and%2520Pascal%2520Martin%2520and%2520Gabriele%2520Lohmann%2520and%2520Klaus%2520Scheffler%26entry.1292438233%3D%2520%2520The%2520UltraCortex%2520repository%2520%2528https%253A//www.ultracortex.org%2529%2520houses%2520magnetic%250Aresonance%2520imaging%2520data%2520of%2520the%2520human%2520brain%2520obtained%2520at%2520an%2520ultra-high%2520field%250Astrength%2520of%25209.4%2520T.%2520It%2520contains%252086%2520structural%2520MR%2520images%2520with%2520spatial%2520resolutions%250Aranging%2520from%25200.6%2520to%25200.8%2520mm.%2520Additionally%252C%2520the%2520repository%2520includes%2520segmentations%250Aof%252012%2520brains%2520into%2520gray%2520and%2520white%2520matter%2520compartments.%2520These%2520segmentations%2520have%250Abeen%2520independently%2520validated%2520by%2520two%2520expert%2520neuroradiologists%252C%2520thus%2520establishing%250Athem%2520as%2520a%2520reliable%2520gold%2520standard.%2520This%2520resource%2520provides%2520researchers%2520with%250Aaccess%2520to%2520high-quality%2520brain%2520imaging%2520data%2520and%2520validated%2520segmentations%252C%250Afacilitating%2520neuroimaging%2520studies%2520and%2520advancing%2520our%2520understanding%2520of%2520brain%250Astructure%2520and%2520function.%2520Existing%2520repositories%2520do%2520not%2520accommodate%2520field%250Astrengths%2520beyond%25207%2520T%252C%2520nor%2520do%2520they%2520offer%2520validated%2520segmentations%252C%2520underscoring%250Athe%2520significance%2520of%2520this%2520new%2520resource.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18571v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UltraCortex%3A%20Submillimeter%20Ultra-High%20Field%209.4%20T1%20Brain%20MR%20Image%0A%20%20Collection%20and%20Manual%20Cortical%20Segmentations&entry.906535625=Lucas%20Mahler%20and%20Julius%20Steiglechner%20and%20Benjamin%20Bender%20and%20Tobias%20Lindig%20and%20Dana%20Ramadan%20and%20Jonas%20Bause%20and%20Florian%20Birk%20and%20Rahel%20Heule%20and%20Edyta%20Charyasz%20and%20Michael%20Erb%20and%20Vinod%20Jangir%20Kumar%20and%20Gisela%20E%20Hagberg%20and%20Pascal%20Martin%20and%20Gabriele%20Lohmann%20and%20Klaus%20Scheffler&entry.1292438233=%20%20The%20UltraCortex%20repository%20%28https%3A//www.ultracortex.org%29%20houses%20magnetic%0Aresonance%20imaging%20data%20of%20the%20human%20brain%20obtained%20at%20an%20ultra-high%20field%0Astrength%20of%209.4%20T.%20It%20contains%2086%20structural%20MR%20images%20with%20spatial%20resolutions%0Aranging%20from%200.6%20to%200.8%20mm.%20Additionally%2C%20the%20repository%20includes%20segmentations%0Aof%2012%20brains%20into%20gray%20and%20white%20matter%20compartments.%20These%20segmentations%20have%0Abeen%20independently%20validated%20by%20two%20expert%20neuroradiologists%2C%20thus%20establishing%0Athem%20as%20a%20reliable%20gold%20standard.%20This%20resource%20provides%20researchers%20with%0Aaccess%20to%20high-quality%20brain%20imaging%20data%20and%20validated%20segmentations%2C%0Afacilitating%20neuroimaging%20studies%20and%20advancing%20our%20understanding%20of%20brain%0Astructure%20and%20function.%20Existing%20repositories%20do%20not%20accommodate%20field%0Astrengths%20beyond%207%20T%2C%20nor%20do%20they%20offer%20validated%20segmentations%2C%20underscoring%0Athe%20significance%20of%20this%20new%20resource.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18571v2&entry.124074799=Read"},
{"title": "PDiscoFormer: Relaxing Part Discovery Constraints with Vision\n  Transformers", "author": "Ananthu Aniraj and Cassio F. Dantas and Dino Ienco and Diego Marcos", "abstract": "  Computer vision methods that explicitly detect object parts and reason on\nthem are a step towards inherently interpretable models. Existing approaches\nthat perform part discovery driven by a fine-grained classification task make\nvery restrictive assumptions on the geometric properties of the discovered\nparts; they should be small and compact. Although this prior is useful in some\ncases, in this paper we show that pre-trained transformer-based vision models,\nsuch as self-supervised DINOv2 ViT, enable the relaxation of these constraints.\nIn particular, we find that a total variation (TV) prior, which allows for\nmultiple connected components of any size, substantially outperforms previous\nwork. We test our approach on three fine-grained classification benchmarks:\nCUB, PartImageNet and Oxford Flowers, and compare our results to previously\npublished methods as well as a re-implementation of the state-of-the-art method\nPDiscoNet with a transformer-based backbone. We consistently obtain substantial\nimprovements across the board, both on part discovery metrics and the\ndownstream classification task, showing that the strong inductive biases in\nself-supervised ViT models require to rethink the geometric priors that can be\nused for unsupervised part discovery.\n", "link": "http://arxiv.org/abs/2407.04538v1", "date": "2024-07-05", "relevancy": 2.1105, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5385}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5265}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PDiscoFormer%3A%20Relaxing%20Part%20Discovery%20Constraints%20with%20Vision%0A%20%20Transformers&body=Title%3A%20PDiscoFormer%3A%20Relaxing%20Part%20Discovery%20Constraints%20with%20Vision%0A%20%20Transformers%0AAuthor%3A%20Ananthu%20Aniraj%20and%20Cassio%20F.%20Dantas%20and%20Dino%20Ienco%20and%20Diego%20Marcos%0AAbstract%3A%20%20%20Computer%20vision%20methods%20that%20explicitly%20detect%20object%20parts%20and%20reason%20on%0Athem%20are%20a%20step%20towards%20inherently%20interpretable%20models.%20Existing%20approaches%0Athat%20perform%20part%20discovery%20driven%20by%20a%20fine-grained%20classification%20task%20make%0Avery%20restrictive%20assumptions%20on%20the%20geometric%20properties%20of%20the%20discovered%0Aparts%3B%20they%20should%20be%20small%20and%20compact.%20Although%20this%20prior%20is%20useful%20in%20some%0Acases%2C%20in%20this%20paper%20we%20show%20that%20pre-trained%20transformer-based%20vision%20models%2C%0Asuch%20as%20self-supervised%20DINOv2%20ViT%2C%20enable%20the%20relaxation%20of%20these%20constraints.%0AIn%20particular%2C%20we%20find%20that%20a%20total%20variation%20%28TV%29%20prior%2C%20which%20allows%20for%0Amultiple%20connected%20components%20of%20any%20size%2C%20substantially%20outperforms%20previous%0Awork.%20We%20test%20our%20approach%20on%20three%20fine-grained%20classification%20benchmarks%3A%0ACUB%2C%20PartImageNet%20and%20Oxford%20Flowers%2C%20and%20compare%20our%20results%20to%20previously%0Apublished%20methods%20as%20well%20as%20a%20re-implementation%20of%20the%20state-of-the-art%20method%0APDiscoNet%20with%20a%20transformer-based%20backbone.%20We%20consistently%20obtain%20substantial%0Aimprovements%20across%20the%20board%2C%20both%20on%20part%20discovery%20metrics%20and%20the%0Adownstream%20classification%20task%2C%20showing%20that%20the%20strong%20inductive%20biases%20in%0Aself-supervised%20ViT%20models%20require%20to%20rethink%20the%20geometric%20priors%20that%20can%20be%0Aused%20for%20unsupervised%20part%20discovery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPDiscoFormer%253A%2520Relaxing%2520Part%2520Discovery%2520Constraints%2520with%2520Vision%250A%2520%2520Transformers%26entry.906535625%3DAnanthu%2520Aniraj%2520and%2520Cassio%2520F.%2520Dantas%2520and%2520Dino%2520Ienco%2520and%2520Diego%2520Marcos%26entry.1292438233%3D%2520%2520Computer%2520vision%2520methods%2520that%2520explicitly%2520detect%2520object%2520parts%2520and%2520reason%2520on%250Athem%2520are%2520a%2520step%2520towards%2520inherently%2520interpretable%2520models.%2520Existing%2520approaches%250Athat%2520perform%2520part%2520discovery%2520driven%2520by%2520a%2520fine-grained%2520classification%2520task%2520make%250Avery%2520restrictive%2520assumptions%2520on%2520the%2520geometric%2520properties%2520of%2520the%2520discovered%250Aparts%253B%2520they%2520should%2520be%2520small%2520and%2520compact.%2520Although%2520this%2520prior%2520is%2520useful%2520in%2520some%250Acases%252C%2520in%2520this%2520paper%2520we%2520show%2520that%2520pre-trained%2520transformer-based%2520vision%2520models%252C%250Asuch%2520as%2520self-supervised%2520DINOv2%2520ViT%252C%2520enable%2520the%2520relaxation%2520of%2520these%2520constraints.%250AIn%2520particular%252C%2520we%2520find%2520that%2520a%2520total%2520variation%2520%2528TV%2529%2520prior%252C%2520which%2520allows%2520for%250Amultiple%2520connected%2520components%2520of%2520any%2520size%252C%2520substantially%2520outperforms%2520previous%250Awork.%2520We%2520test%2520our%2520approach%2520on%2520three%2520fine-grained%2520classification%2520benchmarks%253A%250ACUB%252C%2520PartImageNet%2520and%2520Oxford%2520Flowers%252C%2520and%2520compare%2520our%2520results%2520to%2520previously%250Apublished%2520methods%2520as%2520well%2520as%2520a%2520re-implementation%2520of%2520the%2520state-of-the-art%2520method%250APDiscoNet%2520with%2520a%2520transformer-based%2520backbone.%2520We%2520consistently%2520obtain%2520substantial%250Aimprovements%2520across%2520the%2520board%252C%2520both%2520on%2520part%2520discovery%2520metrics%2520and%2520the%250Adownstream%2520classification%2520task%252C%2520showing%2520that%2520the%2520strong%2520inductive%2520biases%2520in%250Aself-supervised%2520ViT%2520models%2520require%2520to%2520rethink%2520the%2520geometric%2520priors%2520that%2520can%2520be%250Aused%2520for%2520unsupervised%2520part%2520discovery.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PDiscoFormer%3A%20Relaxing%20Part%20Discovery%20Constraints%20with%20Vision%0A%20%20Transformers&entry.906535625=Ananthu%20Aniraj%20and%20Cassio%20F.%20Dantas%20and%20Dino%20Ienco%20and%20Diego%20Marcos&entry.1292438233=%20%20Computer%20vision%20methods%20that%20explicitly%20detect%20object%20parts%20and%20reason%20on%0Athem%20are%20a%20step%20towards%20inherently%20interpretable%20models.%20Existing%20approaches%0Athat%20perform%20part%20discovery%20driven%20by%20a%20fine-grained%20classification%20task%20make%0Avery%20restrictive%20assumptions%20on%20the%20geometric%20properties%20of%20the%20discovered%0Aparts%3B%20they%20should%20be%20small%20and%20compact.%20Although%20this%20prior%20is%20useful%20in%20some%0Acases%2C%20in%20this%20paper%20we%20show%20that%20pre-trained%20transformer-based%20vision%20models%2C%0Asuch%20as%20self-supervised%20DINOv2%20ViT%2C%20enable%20the%20relaxation%20of%20these%20constraints.%0AIn%20particular%2C%20we%20find%20that%20a%20total%20variation%20%28TV%29%20prior%2C%20which%20allows%20for%0Amultiple%20connected%20components%20of%20any%20size%2C%20substantially%20outperforms%20previous%0Awork.%20We%20test%20our%20approach%20on%20three%20fine-grained%20classification%20benchmarks%3A%0ACUB%2C%20PartImageNet%20and%20Oxford%20Flowers%2C%20and%20compare%20our%20results%20to%20previously%0Apublished%20methods%20as%20well%20as%20a%20re-implementation%20of%20the%20state-of-the-art%20method%0APDiscoNet%20with%20a%20transformer-based%20backbone.%20We%20consistently%20obtain%20substantial%0Aimprovements%20across%20the%20board%2C%20both%20on%20part%20discovery%20metrics%20and%20the%0Adownstream%20classification%20task%2C%20showing%20that%20the%20strong%20inductive%20biases%20in%0Aself-supervised%20ViT%20models%20require%20to%20rethink%20the%20geometric%20priors%20that%20can%20be%0Aused%20for%20unsupervised%20part%20discovery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04538v1&entry.124074799=Read"},
{"title": "Generative Adversarial Networks for Spatio-Spectral Compression of\n  Hyperspectral Images", "author": "Martin Hermann Paul Fuchs and Akshara Preethy Byju and Alisa Walda and Behnood Rasti and Beg\u00fcm Demir", "abstract": "  The development of deep learning-based models for the compression of\nhyperspectral images (HSIs) has recently attracted great attention in remote\nsensing due to the sharp growing of hyperspectral data archives. Most of the\nexisting models achieve either spectral or spatial compression, and do not\njointly consider the spatio-spectral redundancies present in HSIs. To address\nthis problem, in this paper we focus our attention on the High Fidelity\nCompression (HiFiC) model (which is proven to be highly effective for spatial\ncompression problems) and adapt it to perform spatio-spectral compression of\nHSIs. In detail, we introduce two new models: i) HiFiC using Squeeze and\nExcitation (SE) blocks (denoted as HiFiC$_{SE}$); and ii) HiFiC with 3D\nconvolutions (denoted as HiFiC$_{3D}$) in the framework of compression of HSIs.\nWe analyze the effectiveness of HiFiC$_{SE}$ and HiFiC$_{3D}$ in compressing\nthe spatio-spectral redundancies with channel attention and inter-dependency\nanalysis. Experimental results show the efficacy of the proposed models in\nperforming spatio-spectral compression, while reconstructing images at reduced\nbitrates with higher reconstruction quality. The code of the proposed models is\npublicly available at https://git.tu-berlin.de/rsim/HSI-SSC .\n", "link": "http://arxiv.org/abs/2305.08514v3", "date": "2024-07-05", "relevancy": 2.1027, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5413}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5332}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Adversarial%20Networks%20for%20Spatio-Spectral%20Compression%20of%0A%20%20Hyperspectral%20Images&body=Title%3A%20Generative%20Adversarial%20Networks%20for%20Spatio-Spectral%20Compression%20of%0A%20%20Hyperspectral%20Images%0AAuthor%3A%20Martin%20Hermann%20Paul%20Fuchs%20and%20Akshara%20Preethy%20Byju%20and%20Alisa%20Walda%20and%20Behnood%20Rasti%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20The%20development%20of%20deep%20learning-based%20models%20for%20the%20compression%20of%0Ahyperspectral%20images%20%28HSIs%29%20has%20recently%20attracted%20great%20attention%20in%20remote%0Asensing%20due%20to%20the%20sharp%20growing%20of%20hyperspectral%20data%20archives.%20Most%20of%20the%0Aexisting%20models%20achieve%20either%20spectral%20or%20spatial%20compression%2C%20and%20do%20not%0Ajointly%20consider%20the%20spatio-spectral%20redundancies%20present%20in%20HSIs.%20To%20address%0Athis%20problem%2C%20in%20this%20paper%20we%20focus%20our%20attention%20on%20the%20High%20Fidelity%0ACompression%20%28HiFiC%29%20model%20%28which%20is%20proven%20to%20be%20highly%20effective%20for%20spatial%0Acompression%20problems%29%20and%20adapt%20it%20to%20perform%20spatio-spectral%20compression%20of%0AHSIs.%20In%20detail%2C%20we%20introduce%20two%20new%20models%3A%20i%29%20HiFiC%20using%20Squeeze%20and%0AExcitation%20%28SE%29%20blocks%20%28denoted%20as%20HiFiC%24_%7BSE%7D%24%29%3B%20and%20ii%29%20HiFiC%20with%203D%0Aconvolutions%20%28denoted%20as%20HiFiC%24_%7B3D%7D%24%29%20in%20the%20framework%20of%20compression%20of%20HSIs.%0AWe%20analyze%20the%20effectiveness%20of%20HiFiC%24_%7BSE%7D%24%20and%20HiFiC%24_%7B3D%7D%24%20in%20compressing%0Athe%20spatio-spectral%20redundancies%20with%20channel%20attention%20and%20inter-dependency%0Aanalysis.%20Experimental%20results%20show%20the%20efficacy%20of%20the%20proposed%20models%20in%0Aperforming%20spatio-spectral%20compression%2C%20while%20reconstructing%20images%20at%20reduced%0Abitrates%20with%20higher%20reconstruction%20quality.%20The%20code%20of%20the%20proposed%20models%20is%0Apublicly%20available%20at%20https%3A//git.tu-berlin.de/rsim/HSI-SSC%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.08514v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Adversarial%2520Networks%2520for%2520Spatio-Spectral%2520Compression%2520of%250A%2520%2520Hyperspectral%2520Images%26entry.906535625%3DMartin%2520Hermann%2520Paul%2520Fuchs%2520and%2520Akshara%2520Preethy%2520Byju%2520and%2520Alisa%2520Walda%2520and%2520Behnood%2520Rasti%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520deep%2520learning-based%2520models%2520for%2520the%2520compression%2520of%250Ahyperspectral%2520images%2520%2528HSIs%2529%2520has%2520recently%2520attracted%2520great%2520attention%2520in%2520remote%250Asensing%2520due%2520to%2520the%2520sharp%2520growing%2520of%2520hyperspectral%2520data%2520archives.%2520Most%2520of%2520the%250Aexisting%2520models%2520achieve%2520either%2520spectral%2520or%2520spatial%2520compression%252C%2520and%2520do%2520not%250Ajointly%2520consider%2520the%2520spatio-spectral%2520redundancies%2520present%2520in%2520HSIs.%2520To%2520address%250Athis%2520problem%252C%2520in%2520this%2520paper%2520we%2520focus%2520our%2520attention%2520on%2520the%2520High%2520Fidelity%250ACompression%2520%2528HiFiC%2529%2520model%2520%2528which%2520is%2520proven%2520to%2520be%2520highly%2520effective%2520for%2520spatial%250Acompression%2520problems%2529%2520and%2520adapt%2520it%2520to%2520perform%2520spatio-spectral%2520compression%2520of%250AHSIs.%2520In%2520detail%252C%2520we%2520introduce%2520two%2520new%2520models%253A%2520i%2529%2520HiFiC%2520using%2520Squeeze%2520and%250AExcitation%2520%2528SE%2529%2520blocks%2520%2528denoted%2520as%2520HiFiC%2524_%257BSE%257D%2524%2529%253B%2520and%2520ii%2529%2520HiFiC%2520with%25203D%250Aconvolutions%2520%2528denoted%2520as%2520HiFiC%2524_%257B3D%257D%2524%2529%2520in%2520the%2520framework%2520of%2520compression%2520of%2520HSIs.%250AWe%2520analyze%2520the%2520effectiveness%2520of%2520HiFiC%2524_%257BSE%257D%2524%2520and%2520HiFiC%2524_%257B3D%257D%2524%2520in%2520compressing%250Athe%2520spatio-spectral%2520redundancies%2520with%2520channel%2520attention%2520and%2520inter-dependency%250Aanalysis.%2520Experimental%2520results%2520show%2520the%2520efficacy%2520of%2520the%2520proposed%2520models%2520in%250Aperforming%2520spatio-spectral%2520compression%252C%2520while%2520reconstructing%2520images%2520at%2520reduced%250Abitrates%2520with%2520higher%2520reconstruction%2520quality.%2520The%2520code%2520of%2520the%2520proposed%2520models%2520is%250Apublicly%2520available%2520at%2520https%253A//git.tu-berlin.de/rsim/HSI-SSC%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.08514v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Adversarial%20Networks%20for%20Spatio-Spectral%20Compression%20of%0A%20%20Hyperspectral%20Images&entry.906535625=Martin%20Hermann%20Paul%20Fuchs%20and%20Akshara%20Preethy%20Byju%20and%20Alisa%20Walda%20and%20Behnood%20Rasti%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20The%20development%20of%20deep%20learning-based%20models%20for%20the%20compression%20of%0Ahyperspectral%20images%20%28HSIs%29%20has%20recently%20attracted%20great%20attention%20in%20remote%0Asensing%20due%20to%20the%20sharp%20growing%20of%20hyperspectral%20data%20archives.%20Most%20of%20the%0Aexisting%20models%20achieve%20either%20spectral%20or%20spatial%20compression%2C%20and%20do%20not%0Ajointly%20consider%20the%20spatio-spectral%20redundancies%20present%20in%20HSIs.%20To%20address%0Athis%20problem%2C%20in%20this%20paper%20we%20focus%20our%20attention%20on%20the%20High%20Fidelity%0ACompression%20%28HiFiC%29%20model%20%28which%20is%20proven%20to%20be%20highly%20effective%20for%20spatial%0Acompression%20problems%29%20and%20adapt%20it%20to%20perform%20spatio-spectral%20compression%20of%0AHSIs.%20In%20detail%2C%20we%20introduce%20two%20new%20models%3A%20i%29%20HiFiC%20using%20Squeeze%20and%0AExcitation%20%28SE%29%20blocks%20%28denoted%20as%20HiFiC%24_%7BSE%7D%24%29%3B%20and%20ii%29%20HiFiC%20with%203D%0Aconvolutions%20%28denoted%20as%20HiFiC%24_%7B3D%7D%24%29%20in%20the%20framework%20of%20compression%20of%20HSIs.%0AWe%20analyze%20the%20effectiveness%20of%20HiFiC%24_%7BSE%7D%24%20and%20HiFiC%24_%7B3D%7D%24%20in%20compressing%0Athe%20spatio-spectral%20redundancies%20with%20channel%20attention%20and%20inter-dependency%0Aanalysis.%20Experimental%20results%20show%20the%20efficacy%20of%20the%20proposed%20models%20in%0Aperforming%20spatio-spectral%20compression%2C%20while%20reconstructing%20images%20at%20reduced%0Abitrates%20with%20higher%20reconstruction%20quality.%20The%20code%20of%20the%20proposed%20models%20is%0Apublicly%20available%20at%20https%3A//git.tu-berlin.de/rsim/HSI-SSC%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.08514v3&entry.124074799=Read"},
{"title": "Optimizing the image correction pipeline for pedestrian detection in the\n  thermal-infrared domain", "author": "Christophe Karam and Jessy Matias and Xavier Breniere and Jocelyn Chanussot", "abstract": "  Infrared imagery can help in low-visibility situations such as fog and\nlow-light scenarios, but it is prone to thermal noise and requires further\nprocessing and correction. This work studies the effect of different infrared\nprocessing pipelines on the performance of a pedestrian detection in an urban\nenvironment, similar to autonomous driving scenarios. Detection on infrared\nimages is shown to outperform that on visible images, but the infrared\ncorrection pipeline is crucial since the models cannot extract information from\nraw infrared images. Two thermal correction pipelines are studied, the shutter\nand the shutterless pipes. Experiments show that some correction algorithms\nlike spatial denoising are detrimental to performance even if they increase\nvisual quality for a human observer. Other algorithms like destriping and, to a\nlesser extent, temporal denoising, increase computational time, but have some\nrole to play in increasing detection accuracy. As it stands, the optimal\ntrade-off for speed and accuracy is simply to use the shutterless pipe with a\ntonemapping algorithm only, for autonomous driving applications within varied\nenvironments.\n", "link": "http://arxiv.org/abs/2407.04484v1", "date": "2024-07-05", "relevancy": 2.0799, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5391}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5142}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20the%20image%20correction%20pipeline%20for%20pedestrian%20detection%20in%20the%0A%20%20thermal-infrared%20domain&body=Title%3A%20Optimizing%20the%20image%20correction%20pipeline%20for%20pedestrian%20detection%20in%20the%0A%20%20thermal-infrared%20domain%0AAuthor%3A%20Christophe%20Karam%20and%20Jessy%20Matias%20and%20Xavier%20Breniere%20and%20Jocelyn%20Chanussot%0AAbstract%3A%20%20%20Infrared%20imagery%20can%20help%20in%20low-visibility%20situations%20such%20as%20fog%20and%0Alow-light%20scenarios%2C%20but%20it%20is%20prone%20to%20thermal%20noise%20and%20requires%20further%0Aprocessing%20and%20correction.%20This%20work%20studies%20the%20effect%20of%20different%20infrared%0Aprocessing%20pipelines%20on%20the%20performance%20of%20a%20pedestrian%20detection%20in%20an%20urban%0Aenvironment%2C%20similar%20to%20autonomous%20driving%20scenarios.%20Detection%20on%20infrared%0Aimages%20is%20shown%20to%20outperform%20that%20on%20visible%20images%2C%20but%20the%20infrared%0Acorrection%20pipeline%20is%20crucial%20since%20the%20models%20cannot%20extract%20information%20from%0Araw%20infrared%20images.%20Two%20thermal%20correction%20pipelines%20are%20studied%2C%20the%20shutter%0Aand%20the%20shutterless%20pipes.%20Experiments%20show%20that%20some%20correction%20algorithms%0Alike%20spatial%20denoising%20are%20detrimental%20to%20performance%20even%20if%20they%20increase%0Avisual%20quality%20for%20a%20human%20observer.%20Other%20algorithms%20like%20destriping%20and%2C%20to%20a%0Alesser%20extent%2C%20temporal%20denoising%2C%20increase%20computational%20time%2C%20but%20have%20some%0Arole%20to%20play%20in%20increasing%20detection%20accuracy.%20As%20it%20stands%2C%20the%20optimal%0Atrade-off%20for%20speed%20and%20accuracy%20is%20simply%20to%20use%20the%20shutterless%20pipe%20with%20a%0Atonemapping%20algorithm%20only%2C%20for%20autonomous%20driving%20applications%20within%20varied%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520the%2520image%2520correction%2520pipeline%2520for%2520pedestrian%2520detection%2520in%2520the%250A%2520%2520thermal-infrared%2520domain%26entry.906535625%3DChristophe%2520Karam%2520and%2520Jessy%2520Matias%2520and%2520Xavier%2520Breniere%2520and%2520Jocelyn%2520Chanussot%26entry.1292438233%3D%2520%2520Infrared%2520imagery%2520can%2520help%2520in%2520low-visibility%2520situations%2520such%2520as%2520fog%2520and%250Alow-light%2520scenarios%252C%2520but%2520it%2520is%2520prone%2520to%2520thermal%2520noise%2520and%2520requires%2520further%250Aprocessing%2520and%2520correction.%2520This%2520work%2520studies%2520the%2520effect%2520of%2520different%2520infrared%250Aprocessing%2520pipelines%2520on%2520the%2520performance%2520of%2520a%2520pedestrian%2520detection%2520in%2520an%2520urban%250Aenvironment%252C%2520similar%2520to%2520autonomous%2520driving%2520scenarios.%2520Detection%2520on%2520infrared%250Aimages%2520is%2520shown%2520to%2520outperform%2520that%2520on%2520visible%2520images%252C%2520but%2520the%2520infrared%250Acorrection%2520pipeline%2520is%2520crucial%2520since%2520the%2520models%2520cannot%2520extract%2520information%2520from%250Araw%2520infrared%2520images.%2520Two%2520thermal%2520correction%2520pipelines%2520are%2520studied%252C%2520the%2520shutter%250Aand%2520the%2520shutterless%2520pipes.%2520Experiments%2520show%2520that%2520some%2520correction%2520algorithms%250Alike%2520spatial%2520denoising%2520are%2520detrimental%2520to%2520performance%2520even%2520if%2520they%2520increase%250Avisual%2520quality%2520for%2520a%2520human%2520observer.%2520Other%2520algorithms%2520like%2520destriping%2520and%252C%2520to%2520a%250Alesser%2520extent%252C%2520temporal%2520denoising%252C%2520increase%2520computational%2520time%252C%2520but%2520have%2520some%250Arole%2520to%2520play%2520in%2520increasing%2520detection%2520accuracy.%2520As%2520it%2520stands%252C%2520the%2520optimal%250Atrade-off%2520for%2520speed%2520and%2520accuracy%2520is%2520simply%2520to%2520use%2520the%2520shutterless%2520pipe%2520with%2520a%250Atonemapping%2520algorithm%2520only%252C%2520for%2520autonomous%2520driving%2520applications%2520within%2520varied%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20the%20image%20correction%20pipeline%20for%20pedestrian%20detection%20in%20the%0A%20%20thermal-infrared%20domain&entry.906535625=Christophe%20Karam%20and%20Jessy%20Matias%20and%20Xavier%20Breniere%20and%20Jocelyn%20Chanussot&entry.1292438233=%20%20Infrared%20imagery%20can%20help%20in%20low-visibility%20situations%20such%20as%20fog%20and%0Alow-light%20scenarios%2C%20but%20it%20is%20prone%20to%20thermal%20noise%20and%20requires%20further%0Aprocessing%20and%20correction.%20This%20work%20studies%20the%20effect%20of%20different%20infrared%0Aprocessing%20pipelines%20on%20the%20performance%20of%20a%20pedestrian%20detection%20in%20an%20urban%0Aenvironment%2C%20similar%20to%20autonomous%20driving%20scenarios.%20Detection%20on%20infrared%0Aimages%20is%20shown%20to%20outperform%20that%20on%20visible%20images%2C%20but%20the%20infrared%0Acorrection%20pipeline%20is%20crucial%20since%20the%20models%20cannot%20extract%20information%20from%0Araw%20infrared%20images.%20Two%20thermal%20correction%20pipelines%20are%20studied%2C%20the%20shutter%0Aand%20the%20shutterless%20pipes.%20Experiments%20show%20that%20some%20correction%20algorithms%0Alike%20spatial%20denoising%20are%20detrimental%20to%20performance%20even%20if%20they%20increase%0Avisual%20quality%20for%20a%20human%20observer.%20Other%20algorithms%20like%20destriping%20and%2C%20to%20a%0Alesser%20extent%2C%20temporal%20denoising%2C%20increase%20computational%20time%2C%20but%20have%20some%0Arole%20to%20play%20in%20increasing%20detection%20accuracy.%20As%20it%20stands%2C%20the%20optimal%0Atrade-off%20for%20speed%20and%20accuracy%20is%20simply%20to%20use%20the%20shutterless%20pipe%20with%20a%0Atonemapping%20algorithm%20only%2C%20for%20autonomous%20driving%20applications%20within%20varied%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04484v1&entry.124074799=Read"},
{"title": "Embracing Massive Medical Data", "author": "Yu-Cheng Chou and Zongwei Zhou and Alan Yuille", "abstract": "  As massive medical data become available with an increasing number of scans,\nexpanding classes, and varying sources, prevalent training paradigms -- where\nAI is trained with multiple passes over fixed, finite datasets -- face\nsignificant challenges. First, training AI all at once on such massive data is\nimpractical as new scans/sources/classes continuously arrive. Second, training\nAI continuously on new scans/sources/classes can lead to catastrophic\nforgetting, where AI forgets old data as it learns new data, and vice versa. To\naddress these two challenges, we propose an online learning method that enables\ntraining AI from massive medical data. Instead of repeatedly training AI on\nrandomly selected data samples, our method identifies the most significant\nsamples for the current AI model based on their data uniqueness and prediction\nuncertainty, then trains the AI on these selective data samples. Compared with\nprevalent training paradigms, our method not only improves data efficiency by\nenabling training on continual data streams, but also mitigates catastrophic\nforgetting by selectively training AI on significant data samples that might\notherwise be forgotten, outperforming by 15% in Dice score for multi-organ and\ntumor segmentation.\n  The code is available at https://github.com/MrGiovanni/OnlineLearning\n", "link": "http://arxiv.org/abs/2407.04687v1", "date": "2024-07-05", "relevancy": 2.0693, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5317}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5181}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embracing%20Massive%20Medical%20Data&body=Title%3A%20Embracing%20Massive%20Medical%20Data%0AAuthor%3A%20Yu-Cheng%20Chou%20and%20Zongwei%20Zhou%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20As%20massive%20medical%20data%20become%20available%20with%20an%20increasing%20number%20of%20scans%2C%0Aexpanding%20classes%2C%20and%20varying%20sources%2C%20prevalent%20training%20paradigms%20--%20where%0AAI%20is%20trained%20with%20multiple%20passes%20over%20fixed%2C%20finite%20datasets%20--%20face%0Asignificant%20challenges.%20First%2C%20training%20AI%20all%20at%20once%20on%20such%20massive%20data%20is%0Aimpractical%20as%20new%20scans/sources/classes%20continuously%20arrive.%20Second%2C%20training%0AAI%20continuously%20on%20new%20scans/sources/classes%20can%20lead%20to%20catastrophic%0Aforgetting%2C%20where%20AI%20forgets%20old%20data%20as%20it%20learns%20new%20data%2C%20and%20vice%20versa.%20To%0Aaddress%20these%20two%20challenges%2C%20we%20propose%20an%20online%20learning%20method%20that%20enables%0Atraining%20AI%20from%20massive%20medical%20data.%20Instead%20of%20repeatedly%20training%20AI%20on%0Arandomly%20selected%20data%20samples%2C%20our%20method%20identifies%20the%20most%20significant%0Asamples%20for%20the%20current%20AI%20model%20based%20on%20their%20data%20uniqueness%20and%20prediction%0Auncertainty%2C%20then%20trains%20the%20AI%20on%20these%20selective%20data%20samples.%20Compared%20with%0Aprevalent%20training%20paradigms%2C%20our%20method%20not%20only%20improves%20data%20efficiency%20by%0Aenabling%20training%20on%20continual%20data%20streams%2C%20but%20also%20mitigates%20catastrophic%0Aforgetting%20by%20selectively%20training%20AI%20on%20significant%20data%20samples%20that%20might%0Aotherwise%20be%20forgotten%2C%20outperforming%20by%2015%25%20in%20Dice%20score%20for%20multi-organ%20and%0Atumor%20segmentation.%0A%20%20The%20code%20is%20available%20at%20https%3A//github.com/MrGiovanni/OnlineLearning%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbracing%2520Massive%2520Medical%2520Data%26entry.906535625%3DYu-Cheng%2520Chou%2520and%2520Zongwei%2520Zhou%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520As%2520massive%2520medical%2520data%2520become%2520available%2520with%2520an%2520increasing%2520number%2520of%2520scans%252C%250Aexpanding%2520classes%252C%2520and%2520varying%2520sources%252C%2520prevalent%2520training%2520paradigms%2520--%2520where%250AAI%2520is%2520trained%2520with%2520multiple%2520passes%2520over%2520fixed%252C%2520finite%2520datasets%2520--%2520face%250Asignificant%2520challenges.%2520First%252C%2520training%2520AI%2520all%2520at%2520once%2520on%2520such%2520massive%2520data%2520is%250Aimpractical%2520as%2520new%2520scans/sources/classes%2520continuously%2520arrive.%2520Second%252C%2520training%250AAI%2520continuously%2520on%2520new%2520scans/sources/classes%2520can%2520lead%2520to%2520catastrophic%250Aforgetting%252C%2520where%2520AI%2520forgets%2520old%2520data%2520as%2520it%2520learns%2520new%2520data%252C%2520and%2520vice%2520versa.%2520To%250Aaddress%2520these%2520two%2520challenges%252C%2520we%2520propose%2520an%2520online%2520learning%2520method%2520that%2520enables%250Atraining%2520AI%2520from%2520massive%2520medical%2520data.%2520Instead%2520of%2520repeatedly%2520training%2520AI%2520on%250Arandomly%2520selected%2520data%2520samples%252C%2520our%2520method%2520identifies%2520the%2520most%2520significant%250Asamples%2520for%2520the%2520current%2520AI%2520model%2520based%2520on%2520their%2520data%2520uniqueness%2520and%2520prediction%250Auncertainty%252C%2520then%2520trains%2520the%2520AI%2520on%2520these%2520selective%2520data%2520samples.%2520Compared%2520with%250Aprevalent%2520training%2520paradigms%252C%2520our%2520method%2520not%2520only%2520improves%2520data%2520efficiency%2520by%250Aenabling%2520training%2520on%2520continual%2520data%2520streams%252C%2520but%2520also%2520mitigates%2520catastrophic%250Aforgetting%2520by%2520selectively%2520training%2520AI%2520on%2520significant%2520data%2520samples%2520that%2520might%250Aotherwise%2520be%2520forgotten%252C%2520outperforming%2520by%252015%2525%2520in%2520Dice%2520score%2520for%2520multi-organ%2520and%250Atumor%2520segmentation.%250A%2520%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/MrGiovanni/OnlineLearning%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embracing%20Massive%20Medical%20Data&entry.906535625=Yu-Cheng%20Chou%20and%20Zongwei%20Zhou%20and%20Alan%20Yuille&entry.1292438233=%20%20As%20massive%20medical%20data%20become%20available%20with%20an%20increasing%20number%20of%20scans%2C%0Aexpanding%20classes%2C%20and%20varying%20sources%2C%20prevalent%20training%20paradigms%20--%20where%0AAI%20is%20trained%20with%20multiple%20passes%20over%20fixed%2C%20finite%20datasets%20--%20face%0Asignificant%20challenges.%20First%2C%20training%20AI%20all%20at%20once%20on%20such%20massive%20data%20is%0Aimpractical%20as%20new%20scans/sources/classes%20continuously%20arrive.%20Second%2C%20training%0AAI%20continuously%20on%20new%20scans/sources/classes%20can%20lead%20to%20catastrophic%0Aforgetting%2C%20where%20AI%20forgets%20old%20data%20as%20it%20learns%20new%20data%2C%20and%20vice%20versa.%20To%0Aaddress%20these%20two%20challenges%2C%20we%20propose%20an%20online%20learning%20method%20that%20enables%0Atraining%20AI%20from%20massive%20medical%20data.%20Instead%20of%20repeatedly%20training%20AI%20on%0Arandomly%20selected%20data%20samples%2C%20our%20method%20identifies%20the%20most%20significant%0Asamples%20for%20the%20current%20AI%20model%20based%20on%20their%20data%20uniqueness%20and%20prediction%0Auncertainty%2C%20then%20trains%20the%20AI%20on%20these%20selective%20data%20samples.%20Compared%20with%0Aprevalent%20training%20paradigms%2C%20our%20method%20not%20only%20improves%20data%20efficiency%20by%0Aenabling%20training%20on%20continual%20data%20streams%2C%20but%20also%20mitigates%20catastrophic%0Aforgetting%20by%20selectively%20training%20AI%20on%20significant%20data%20samples%20that%20might%0Aotherwise%20be%20forgotten%2C%20outperforming%20by%2015%25%20in%20Dice%20score%20for%20multi-organ%20and%0Atumor%20segmentation.%0A%20%20The%20code%20is%20available%20at%20https%3A//github.com/MrGiovanni/OnlineLearning%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04687v1&entry.124074799=Read"},
{"title": "Trustworthy Classification through Rank-Based Conformal Prediction Sets", "author": "Rui Luo and Zhixin Zhou", "abstract": "  Machine learning classification tasks often benefit from predicting a set of\npossible labels with confidence scores to capture uncertainty. However,\nexisting methods struggle with the high-dimensional nature of the data and the\nlack of well-calibrated probabilities from modern classification models. We\npropose a novel conformal prediction method that employs a rank-based score\nfunction suitable for classification models that predict the order of labels\ncorrectly, even if not well-calibrated. Our approach constructs prediction sets\nthat achieve the desired coverage rate while managing their size. We provide a\ntheoretical analysis of the expected size of the conformal prediction sets\nbased on the rank distribution of the underlying classifier. Through extensive\nexperiments, we demonstrate that our method outperforms existing techniques on\nvarious datasets, providing reliable uncertainty quantification. Our\ncontributions include a novel conformal prediction method, theoretical\nanalysis, and empirical evaluation. This work advances the practical deployment\nof machine learning systems by enabling reliable uncertainty quantification.\n", "link": "http://arxiv.org/abs/2407.04407v1", "date": "2024-07-05", "relevancy": 2.0636, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5235}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5225}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trustworthy%20Classification%20through%20Rank-Based%20Conformal%20Prediction%20Sets&body=Title%3A%20Trustworthy%20Classification%20through%20Rank-Based%20Conformal%20Prediction%20Sets%0AAuthor%3A%20Rui%20Luo%20and%20Zhixin%20Zhou%0AAbstract%3A%20%20%20Machine%20learning%20classification%20tasks%20often%20benefit%20from%20predicting%20a%20set%20of%0Apossible%20labels%20with%20confidence%20scores%20to%20capture%20uncertainty.%20However%2C%0Aexisting%20methods%20struggle%20with%20the%20high-dimensional%20nature%20of%20the%20data%20and%20the%0Alack%20of%20well-calibrated%20probabilities%20from%20modern%20classification%20models.%20We%0Apropose%20a%20novel%20conformal%20prediction%20method%20that%20employs%20a%20rank-based%20score%0Afunction%20suitable%20for%20classification%20models%20that%20predict%20the%20order%20of%20labels%0Acorrectly%2C%20even%20if%20not%20well-calibrated.%20Our%20approach%20constructs%20prediction%20sets%0Athat%20achieve%20the%20desired%20coverage%20rate%20while%20managing%20their%20size.%20We%20provide%20a%0Atheoretical%20analysis%20of%20the%20expected%20size%20of%20the%20conformal%20prediction%20sets%0Abased%20on%20the%20rank%20distribution%20of%20the%20underlying%20classifier.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20our%20method%20outperforms%20existing%20techniques%20on%0Avarious%20datasets%2C%20providing%20reliable%20uncertainty%20quantification.%20Our%0Acontributions%20include%20a%20novel%20conformal%20prediction%20method%2C%20theoretical%0Aanalysis%2C%20and%20empirical%20evaluation.%20This%20work%20advances%20the%20practical%20deployment%0Aof%20machine%20learning%20systems%20by%20enabling%20reliable%20uncertainty%20quantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrustworthy%2520Classification%2520through%2520Rank-Based%2520Conformal%2520Prediction%2520Sets%26entry.906535625%3DRui%2520Luo%2520and%2520Zhixin%2520Zhou%26entry.1292438233%3D%2520%2520Machine%2520learning%2520classification%2520tasks%2520often%2520benefit%2520from%2520predicting%2520a%2520set%2520of%250Apossible%2520labels%2520with%2520confidence%2520scores%2520to%2520capture%2520uncertainty.%2520However%252C%250Aexisting%2520methods%2520struggle%2520with%2520the%2520high-dimensional%2520nature%2520of%2520the%2520data%2520and%2520the%250Alack%2520of%2520well-calibrated%2520probabilities%2520from%2520modern%2520classification%2520models.%2520We%250Apropose%2520a%2520novel%2520conformal%2520prediction%2520method%2520that%2520employs%2520a%2520rank-based%2520score%250Afunction%2520suitable%2520for%2520classification%2520models%2520that%2520predict%2520the%2520order%2520of%2520labels%250Acorrectly%252C%2520even%2520if%2520not%2520well-calibrated.%2520Our%2520approach%2520constructs%2520prediction%2520sets%250Athat%2520achieve%2520the%2520desired%2520coverage%2520rate%2520while%2520managing%2520their%2520size.%2520We%2520provide%2520a%250Atheoretical%2520analysis%2520of%2520the%2520expected%2520size%2520of%2520the%2520conformal%2520prediction%2520sets%250Abased%2520on%2520the%2520rank%2520distribution%2520of%2520the%2520underlying%2520classifier.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520existing%2520techniques%2520on%250Avarious%2520datasets%252C%2520providing%2520reliable%2520uncertainty%2520quantification.%2520Our%250Acontributions%2520include%2520a%2520novel%2520conformal%2520prediction%2520method%252C%2520theoretical%250Aanalysis%252C%2520and%2520empirical%2520evaluation.%2520This%2520work%2520advances%2520the%2520practical%2520deployment%250Aof%2520machine%2520learning%2520systems%2520by%2520enabling%2520reliable%2520uncertainty%2520quantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trustworthy%20Classification%20through%20Rank-Based%20Conformal%20Prediction%20Sets&entry.906535625=Rui%20Luo%20and%20Zhixin%20Zhou&entry.1292438233=%20%20Machine%20learning%20classification%20tasks%20often%20benefit%20from%20predicting%20a%20set%20of%0Apossible%20labels%20with%20confidence%20scores%20to%20capture%20uncertainty.%20However%2C%0Aexisting%20methods%20struggle%20with%20the%20high-dimensional%20nature%20of%20the%20data%20and%20the%0Alack%20of%20well-calibrated%20probabilities%20from%20modern%20classification%20models.%20We%0Apropose%20a%20novel%20conformal%20prediction%20method%20that%20employs%20a%20rank-based%20score%0Afunction%20suitable%20for%20classification%20models%20that%20predict%20the%20order%20of%20labels%0Acorrectly%2C%20even%20if%20not%20well-calibrated.%20Our%20approach%20constructs%20prediction%20sets%0Athat%20achieve%20the%20desired%20coverage%20rate%20while%20managing%20their%20size.%20We%20provide%20a%0Atheoretical%20analysis%20of%20the%20expected%20size%20of%20the%20conformal%20prediction%20sets%0Abased%20on%20the%20rank%20distribution%20of%20the%20underlying%20classifier.%20Through%20extensive%0Aexperiments%2C%20we%20demonstrate%20that%20our%20method%20outperforms%20existing%20techniques%20on%0Avarious%20datasets%2C%20providing%20reliable%20uncertainty%20quantification.%20Our%0Acontributions%20include%20a%20novel%20conformal%20prediction%20method%2C%20theoretical%0Aanalysis%2C%20and%20empirical%20evaluation.%20This%20work%20advances%20the%20practical%20deployment%0Aof%20machine%20learning%20systems%20by%20enabling%20reliable%20uncertainty%20quantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04407v1&entry.124074799=Read"},
{"title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for\n  Transformers", "author": "Jiawen Xie and Pengyu Cheng and Xiao Liang and Yong Dai and Nan Du", "abstract": "  Although dominant in natural language processing, transformer-based models\nremain challenged by the task of long-sequence processing, because the\ncomputational cost of self-attention operations in transformers swells\nquadratically with the input sequence length. To alleviate the complexity of\nlong-sequence processing, we propose a simple framework to enable the\noffthe-shelf pre-trained transformers to process much longer sequences, while\nthe computation and memory costs remain growing linearly with the input\nsequence lengths. More specifically, our method divides each long-sequence\ninput into a batch of chunks, then aligns the interchunk information during the\nencoding steps, and finally selects the most representative hidden states from\nthe encoder for the decoding process. To extract inter-chunk semantic\ninformation, we align the start and end token embeddings among chunks in each\nencoding transformer block. To learn an effective hidden selection policy, we\ndesign a dual updating scheme inspired by reinforcement learning, which regards\nthe decoders of transformers as environments, and the downstream performance\nmetrics as the rewards to evaluate the hidden selection actions. Our empirical\nresults on real-world long-text summarization and reading comprehension tasks\ndemonstrate effective improvements compared to prior longsequence processing\nbaselines.\n", "link": "http://arxiv.org/abs/2308.13191v2", "date": "2024-07-05", "relevancy": 2.0488, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5557}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chunk%2C%20Align%2C%20Select%3A%20A%20Simple%20Long-sequence%20Processing%20Method%20for%0A%20%20Transformers&body=Title%3A%20Chunk%2C%20Align%2C%20Select%3A%20A%20Simple%20Long-sequence%20Processing%20Method%20for%0A%20%20Transformers%0AAuthor%3A%20Jiawen%20Xie%20and%20Pengyu%20Cheng%20and%20Xiao%20Liang%20and%20Yong%20Dai%20and%20Nan%20Du%0AAbstract%3A%20%20%20Although%20dominant%20in%20natural%20language%20processing%2C%20transformer-based%20models%0Aremain%20challenged%20by%20the%20task%20of%20long-sequence%20processing%2C%20because%20the%0Acomputational%20cost%20of%20self-attention%20operations%20in%20transformers%20swells%0Aquadratically%20with%20the%20input%20sequence%20length.%20To%20alleviate%20the%20complexity%20of%0Along-sequence%20processing%2C%20we%20propose%20a%20simple%20framework%20to%20enable%20the%0Aoffthe-shelf%20pre-trained%20transformers%20to%20process%20much%20longer%20sequences%2C%20while%0Athe%20computation%20and%20memory%20costs%20remain%20growing%20linearly%20with%20the%20input%0Asequence%20lengths.%20More%20specifically%2C%20our%20method%20divides%20each%20long-sequence%0Ainput%20into%20a%20batch%20of%20chunks%2C%20then%20aligns%20the%20interchunk%20information%20during%20the%0Aencoding%20steps%2C%20and%20finally%20selects%20the%20most%20representative%20hidden%20states%20from%0Athe%20encoder%20for%20the%20decoding%20process.%20To%20extract%20inter-chunk%20semantic%0Ainformation%2C%20we%20align%20the%20start%20and%20end%20token%20embeddings%20among%20chunks%20in%20each%0Aencoding%20transformer%20block.%20To%20learn%20an%20effective%20hidden%20selection%20policy%2C%20we%0Adesign%20a%20dual%20updating%20scheme%20inspired%20by%20reinforcement%20learning%2C%20which%20regards%0Athe%20decoders%20of%20transformers%20as%20environments%2C%20and%20the%20downstream%20performance%0Ametrics%20as%20the%20rewards%20to%20evaluate%20the%20hidden%20selection%20actions.%20Our%20empirical%0Aresults%20on%20real-world%20long-text%20summarization%20and%20reading%20comprehension%20tasks%0Ademonstrate%20effective%20improvements%20compared%20to%20prior%20longsequence%20processing%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13191v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChunk%252C%2520Align%252C%2520Select%253A%2520A%2520Simple%2520Long-sequence%2520Processing%2520Method%2520for%250A%2520%2520Transformers%26entry.906535625%3DJiawen%2520Xie%2520and%2520Pengyu%2520Cheng%2520and%2520Xiao%2520Liang%2520and%2520Yong%2520Dai%2520and%2520Nan%2520Du%26entry.1292438233%3D%2520%2520Although%2520dominant%2520in%2520natural%2520language%2520processing%252C%2520transformer-based%2520models%250Aremain%2520challenged%2520by%2520the%2520task%2520of%2520long-sequence%2520processing%252C%2520because%2520the%250Acomputational%2520cost%2520of%2520self-attention%2520operations%2520in%2520transformers%2520swells%250Aquadratically%2520with%2520the%2520input%2520sequence%2520length.%2520To%2520alleviate%2520the%2520complexity%2520of%250Along-sequence%2520processing%252C%2520we%2520propose%2520a%2520simple%2520framework%2520to%2520enable%2520the%250Aoffthe-shelf%2520pre-trained%2520transformers%2520to%2520process%2520much%2520longer%2520sequences%252C%2520while%250Athe%2520computation%2520and%2520memory%2520costs%2520remain%2520growing%2520linearly%2520with%2520the%2520input%250Asequence%2520lengths.%2520More%2520specifically%252C%2520our%2520method%2520divides%2520each%2520long-sequence%250Ainput%2520into%2520a%2520batch%2520of%2520chunks%252C%2520then%2520aligns%2520the%2520interchunk%2520information%2520during%2520the%250Aencoding%2520steps%252C%2520and%2520finally%2520selects%2520the%2520most%2520representative%2520hidden%2520states%2520from%250Athe%2520encoder%2520for%2520the%2520decoding%2520process.%2520To%2520extract%2520inter-chunk%2520semantic%250Ainformation%252C%2520we%2520align%2520the%2520start%2520and%2520end%2520token%2520embeddings%2520among%2520chunks%2520in%2520each%250Aencoding%2520transformer%2520block.%2520To%2520learn%2520an%2520effective%2520hidden%2520selection%2520policy%252C%2520we%250Adesign%2520a%2520dual%2520updating%2520scheme%2520inspired%2520by%2520reinforcement%2520learning%252C%2520which%2520regards%250Athe%2520decoders%2520of%2520transformers%2520as%2520environments%252C%2520and%2520the%2520downstream%2520performance%250Ametrics%2520as%2520the%2520rewards%2520to%2520evaluate%2520the%2520hidden%2520selection%2520actions.%2520Our%2520empirical%250Aresults%2520on%2520real-world%2520long-text%2520summarization%2520and%2520reading%2520comprehension%2520tasks%250Ademonstrate%2520effective%2520improvements%2520compared%2520to%2520prior%2520longsequence%2520processing%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.13191v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chunk%2C%20Align%2C%20Select%3A%20A%20Simple%20Long-sequence%20Processing%20Method%20for%0A%20%20Transformers&entry.906535625=Jiawen%20Xie%20and%20Pengyu%20Cheng%20and%20Xiao%20Liang%20and%20Yong%20Dai%20and%20Nan%20Du&entry.1292438233=%20%20Although%20dominant%20in%20natural%20language%20processing%2C%20transformer-based%20models%0Aremain%20challenged%20by%20the%20task%20of%20long-sequence%20processing%2C%20because%20the%0Acomputational%20cost%20of%20self-attention%20operations%20in%20transformers%20swells%0Aquadratically%20with%20the%20input%20sequence%20length.%20To%20alleviate%20the%20complexity%20of%0Along-sequence%20processing%2C%20we%20propose%20a%20simple%20framework%20to%20enable%20the%0Aoffthe-shelf%20pre-trained%20transformers%20to%20process%20much%20longer%20sequences%2C%20while%0Athe%20computation%20and%20memory%20costs%20remain%20growing%20linearly%20with%20the%20input%0Asequence%20lengths.%20More%20specifically%2C%20our%20method%20divides%20each%20long-sequence%0Ainput%20into%20a%20batch%20of%20chunks%2C%20then%20aligns%20the%20interchunk%20information%20during%20the%0Aencoding%20steps%2C%20and%20finally%20selects%20the%20most%20representative%20hidden%20states%20from%0Athe%20encoder%20for%20the%20decoding%20process.%20To%20extract%20inter-chunk%20semantic%0Ainformation%2C%20we%20align%20the%20start%20and%20end%20token%20embeddings%20among%20chunks%20in%20each%0Aencoding%20transformer%20block.%20To%20learn%20an%20effective%20hidden%20selection%20policy%2C%20we%0Adesign%20a%20dual%20updating%20scheme%20inspired%20by%20reinforcement%20learning%2C%20which%20regards%0Athe%20decoders%20of%20transformers%20as%20environments%2C%20and%20the%20downstream%20performance%0Ametrics%20as%20the%20rewards%20to%20evaluate%20the%20hidden%20selection%20actions.%20Our%20empirical%0Aresults%20on%20real-world%20long-text%20summarization%20and%20reading%20comprehension%20tasks%0Ademonstrate%20effective%20improvements%20compared%20to%20prior%20longsequence%20processing%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13191v2&entry.124074799=Read"},
{"title": "Hard-Attention Gates with Gradient Routing for Endoscopic Image\n  Computing", "author": "Giorgio Roffo and Carlo Biffi and Pietro Salvagnini and Andrea Cherubini", "abstract": "  To address overfitting and enhance model generalization in\ngastroenterological polyp size assessment, our study introduces\nFeature-Selection Gates (FSG) or Hard-Attention Gates (HAG) alongside Gradient\nRouting (GR) for dynamic feature selection. This technique aims to boost\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs) by\npromoting sparse connectivity, thereby reducing overfitting and enhancing\ngeneralization. HAG achieves this through sparsification with learnable\nweights, serving as a regularization strategy. GR further refines this process\nby optimizing HAG parameters via dual forward passes, independently from the\nmain model, to improve feature re-weighting. Our evaluation spanned multiple\ndatasets, including CIFAR-100 for a broad impact assessment and specialized\nendoscopic datasets (REAL-Colon, Misawa, and SUN) focusing on polyp size\nestimation, covering over 200 polyps in more than 370,000 frames. The findings\nindicate that our HAG-enhanced networks substantially enhance performance in\nboth binary and triclass classification tasks related to polyp sizing.\nSpecifically, CNNs experienced an F1 Score improvement to 87.8% in binary\nclassification, while in triclass classification, the ViT-T model reached an F1\nScore of 76.5%, outperforming traditional CNNs and ViT-T models. To facilitate\nfurther research, we are releasing our codebase, which includes implementations\nfor CNNs, multistream CNNs, ViT, and HAG-augmented variants. This resource aims\nto standardize the use of endoscopic datasets, providing public\ntraining-validation-testing splits for reliable and comparable research in\ngastroenterological polyp size estimation. The codebase is available at\ngithub.com/cosmoimd/feature-selection-gates.\n", "link": "http://arxiv.org/abs/2407.04400v1", "date": "2024-07-05", "relevancy": 2.0412, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5224}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hard-Attention%20Gates%20with%20Gradient%20Routing%20for%20Endoscopic%20Image%0A%20%20Computing&body=Title%3A%20Hard-Attention%20Gates%20with%20Gradient%20Routing%20for%20Endoscopic%20Image%0A%20%20Computing%0AAuthor%3A%20Giorgio%20Roffo%20and%20Carlo%20Biffi%20and%20Pietro%20Salvagnini%20and%20Andrea%20Cherubini%0AAbstract%3A%20%20%20To%20address%20overfitting%20and%20enhance%20model%20generalization%20in%0Agastroenterological%20polyp%20size%20assessment%2C%20our%20study%20introduces%0AFeature-Selection%20Gates%20%28FSG%29%20or%20Hard-Attention%20Gates%20%28HAG%29%20alongside%20Gradient%0ARouting%20%28GR%29%20for%20dynamic%20feature%20selection.%20This%20technique%20aims%20to%20boost%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29%20by%0Apromoting%20sparse%20connectivity%2C%20thereby%20reducing%20overfitting%20and%20enhancing%0Ageneralization.%20HAG%20achieves%20this%20through%20sparsification%20with%20learnable%0Aweights%2C%20serving%20as%20a%20regularization%20strategy.%20GR%20further%20refines%20this%20process%0Aby%20optimizing%20HAG%20parameters%20via%20dual%20forward%20passes%2C%20independently%20from%20the%0Amain%20model%2C%20to%20improve%20feature%20re-weighting.%20Our%20evaluation%20spanned%20multiple%0Adatasets%2C%20including%20CIFAR-100%20for%20a%20broad%20impact%20assessment%20and%20specialized%0Aendoscopic%20datasets%20%28REAL-Colon%2C%20Misawa%2C%20and%20SUN%29%20focusing%20on%20polyp%20size%0Aestimation%2C%20covering%20over%20200%20polyps%20in%20more%20than%20370%2C000%20frames.%20The%20findings%0Aindicate%20that%20our%20HAG-enhanced%20networks%20substantially%20enhance%20performance%20in%0Aboth%20binary%20and%20triclass%20classification%20tasks%20related%20to%20polyp%20sizing.%0ASpecifically%2C%20CNNs%20experienced%20an%20F1%20Score%20improvement%20to%2087.8%25%20in%20binary%0Aclassification%2C%20while%20in%20triclass%20classification%2C%20the%20ViT-T%20model%20reached%20an%20F1%0AScore%20of%2076.5%25%2C%20outperforming%20traditional%20CNNs%20and%20ViT-T%20models.%20To%20facilitate%0Afurther%20research%2C%20we%20are%20releasing%20our%20codebase%2C%20which%20includes%20implementations%0Afor%20CNNs%2C%20multistream%20CNNs%2C%20ViT%2C%20and%20HAG-augmented%20variants.%20This%20resource%20aims%0Ato%20standardize%20the%20use%20of%20endoscopic%20datasets%2C%20providing%20public%0Atraining-validation-testing%20splits%20for%20reliable%20and%20comparable%20research%20in%0Agastroenterological%20polyp%20size%20estimation.%20The%20codebase%20is%20available%20at%0Agithub.com/cosmoimd/feature-selection-gates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHard-Attention%2520Gates%2520with%2520Gradient%2520Routing%2520for%2520Endoscopic%2520Image%250A%2520%2520Computing%26entry.906535625%3DGiorgio%2520Roffo%2520and%2520Carlo%2520Biffi%2520and%2520Pietro%2520Salvagnini%2520and%2520Andrea%2520Cherubini%26entry.1292438233%3D%2520%2520To%2520address%2520overfitting%2520and%2520enhance%2520model%2520generalization%2520in%250Agastroenterological%2520polyp%2520size%2520assessment%252C%2520our%2520study%2520introduces%250AFeature-Selection%2520Gates%2520%2528FSG%2529%2520or%2520Hard-Attention%2520Gates%2520%2528HAG%2529%2520alongside%2520Gradient%250ARouting%2520%2528GR%2529%2520for%2520dynamic%2520feature%2520selection.%2520This%2520technique%2520aims%2520to%2520boost%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Vision%2520Transformers%2520%2528ViTs%2529%2520by%250Apromoting%2520sparse%2520connectivity%252C%2520thereby%2520reducing%2520overfitting%2520and%2520enhancing%250Ageneralization.%2520HAG%2520achieves%2520this%2520through%2520sparsification%2520with%2520learnable%250Aweights%252C%2520serving%2520as%2520a%2520regularization%2520strategy.%2520GR%2520further%2520refines%2520this%2520process%250Aby%2520optimizing%2520HAG%2520parameters%2520via%2520dual%2520forward%2520passes%252C%2520independently%2520from%2520the%250Amain%2520model%252C%2520to%2520improve%2520feature%2520re-weighting.%2520Our%2520evaluation%2520spanned%2520multiple%250Adatasets%252C%2520including%2520CIFAR-100%2520for%2520a%2520broad%2520impact%2520assessment%2520and%2520specialized%250Aendoscopic%2520datasets%2520%2528REAL-Colon%252C%2520Misawa%252C%2520and%2520SUN%2529%2520focusing%2520on%2520polyp%2520size%250Aestimation%252C%2520covering%2520over%2520200%2520polyps%2520in%2520more%2520than%2520370%252C000%2520frames.%2520The%2520findings%250Aindicate%2520that%2520our%2520HAG-enhanced%2520networks%2520substantially%2520enhance%2520performance%2520in%250Aboth%2520binary%2520and%2520triclass%2520classification%2520tasks%2520related%2520to%2520polyp%2520sizing.%250ASpecifically%252C%2520CNNs%2520experienced%2520an%2520F1%2520Score%2520improvement%2520to%252087.8%2525%2520in%2520binary%250Aclassification%252C%2520while%2520in%2520triclass%2520classification%252C%2520the%2520ViT-T%2520model%2520reached%2520an%2520F1%250AScore%2520of%252076.5%2525%252C%2520outperforming%2520traditional%2520CNNs%2520and%2520ViT-T%2520models.%2520To%2520facilitate%250Afurther%2520research%252C%2520we%2520are%2520releasing%2520our%2520codebase%252C%2520which%2520includes%2520implementations%250Afor%2520CNNs%252C%2520multistream%2520CNNs%252C%2520ViT%252C%2520and%2520HAG-augmented%2520variants.%2520This%2520resource%2520aims%250Ato%2520standardize%2520the%2520use%2520of%2520endoscopic%2520datasets%252C%2520providing%2520public%250Atraining-validation-testing%2520splits%2520for%2520reliable%2520and%2520comparable%2520research%2520in%250Agastroenterological%2520polyp%2520size%2520estimation.%2520The%2520codebase%2520is%2520available%2520at%250Agithub.com/cosmoimd/feature-selection-gates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hard-Attention%20Gates%20with%20Gradient%20Routing%20for%20Endoscopic%20Image%0A%20%20Computing&entry.906535625=Giorgio%20Roffo%20and%20Carlo%20Biffi%20and%20Pietro%20Salvagnini%20and%20Andrea%20Cherubini&entry.1292438233=%20%20To%20address%20overfitting%20and%20enhance%20model%20generalization%20in%0Agastroenterological%20polyp%20size%20assessment%2C%20our%20study%20introduces%0AFeature-Selection%20Gates%20%28FSG%29%20or%20Hard-Attention%20Gates%20%28HAG%29%20alongside%20Gradient%0ARouting%20%28GR%29%20for%20dynamic%20feature%20selection.%20This%20technique%20aims%20to%20boost%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Vision%20Transformers%20%28ViTs%29%20by%0Apromoting%20sparse%20connectivity%2C%20thereby%20reducing%20overfitting%20and%20enhancing%0Ageneralization.%20HAG%20achieves%20this%20through%20sparsification%20with%20learnable%0Aweights%2C%20serving%20as%20a%20regularization%20strategy.%20GR%20further%20refines%20this%20process%0Aby%20optimizing%20HAG%20parameters%20via%20dual%20forward%20passes%2C%20independently%20from%20the%0Amain%20model%2C%20to%20improve%20feature%20re-weighting.%20Our%20evaluation%20spanned%20multiple%0Adatasets%2C%20including%20CIFAR-100%20for%20a%20broad%20impact%20assessment%20and%20specialized%0Aendoscopic%20datasets%20%28REAL-Colon%2C%20Misawa%2C%20and%20SUN%29%20focusing%20on%20polyp%20size%0Aestimation%2C%20covering%20over%20200%20polyps%20in%20more%20than%20370%2C000%20frames.%20The%20findings%0Aindicate%20that%20our%20HAG-enhanced%20networks%20substantially%20enhance%20performance%20in%0Aboth%20binary%20and%20triclass%20classification%20tasks%20related%20to%20polyp%20sizing.%0ASpecifically%2C%20CNNs%20experienced%20an%20F1%20Score%20improvement%20to%2087.8%25%20in%20binary%0Aclassification%2C%20while%20in%20triclass%20classification%2C%20the%20ViT-T%20model%20reached%20an%20F1%0AScore%20of%2076.5%25%2C%20outperforming%20traditional%20CNNs%20and%20ViT-T%20models.%20To%20facilitate%0Afurther%20research%2C%20we%20are%20releasing%20our%20codebase%2C%20which%20includes%20implementations%0Afor%20CNNs%2C%20multistream%20CNNs%2C%20ViT%2C%20and%20HAG-augmented%20variants.%20This%20resource%20aims%0Ato%20standardize%20the%20use%20of%20endoscopic%20datasets%2C%20providing%20public%0Atraining-validation-testing%20splits%20for%20reliable%20and%20comparable%20research%20in%0Agastroenterological%20polyp%20size%20estimation.%20The%20codebase%20is%20available%20at%0Agithub.com/cosmoimd/feature-selection-gates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04400v1&entry.124074799=Read"},
{"title": "Leveraging Graph Structures to Detect Hallucinations in Large Language\n  Models", "author": "Noa Nonkes and Sergei Agaronian and Evangelos Kanoulas and Roxana Petcu", "abstract": "  Large language models are extensively applied across a wide range of tasks,\nsuch as customer support, content creation, educational tutoring, and providing\nfinancial guidance. However, a well-known drawback is their predisposition to\ngenerate hallucinations. This damages the trustworthiness of the information\nthese models provide, impacting decision-making and user confidence. We propose\na method to detect hallucinations by looking at the structure of the latent\nspace and finding associations within hallucinated and non-hallucinated\ngenerations. We create a graph structure that connects generations that lie\nclosely in the embedding space. Moreover, we employ a Graph Attention Network\nwhich utilizes message passing to aggregate information from neighboring nodes\nand assigns varying degrees of importance to each neighbor based on their\nrelevance. Our findings show that 1) there exists a structure in the latent\nspace that differentiates between hallucinated and non-hallucinated\ngenerations, 2) Graph Attention Networks can learn this structure and\ngeneralize it to unseen generations, and 3) the robustness of our method is\nenhanced when incorporating contrastive learning. When evaluated against\nevidence-based benchmarks, our model performs similarly without access to\nsearch-based methods.\n", "link": "http://arxiv.org/abs/2407.04485v1", "date": "2024-07-05", "relevancy": 2.0323, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5202}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5127}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Graph%20Structures%20to%20Detect%20Hallucinations%20in%20Large%20Language%0A%20%20Models&body=Title%3A%20Leveraging%20Graph%20Structures%20to%20Detect%20Hallucinations%20in%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Noa%20Nonkes%20and%20Sergei%20Agaronian%20and%20Evangelos%20Kanoulas%20and%20Roxana%20Petcu%0AAbstract%3A%20%20%20Large%20language%20models%20are%20extensively%20applied%20across%20a%20wide%20range%20of%20tasks%2C%0Asuch%20as%20customer%20support%2C%20content%20creation%2C%20educational%20tutoring%2C%20and%20providing%0Afinancial%20guidance.%20However%2C%20a%20well-known%20drawback%20is%20their%20predisposition%20to%0Agenerate%20hallucinations.%20This%20damages%20the%20trustworthiness%20of%20the%20information%0Athese%20models%20provide%2C%20impacting%20decision-making%20and%20user%20confidence.%20We%20propose%0Aa%20method%20to%20detect%20hallucinations%20by%20looking%20at%20the%20structure%20of%20the%20latent%0Aspace%20and%20finding%20associations%20within%20hallucinated%20and%20non-hallucinated%0Agenerations.%20We%20create%20a%20graph%20structure%20that%20connects%20generations%20that%20lie%0Aclosely%20in%20the%20embedding%20space.%20Moreover%2C%20we%20employ%20a%20Graph%20Attention%20Network%0Awhich%20utilizes%20message%20passing%20to%20aggregate%20information%20from%20neighboring%20nodes%0Aand%20assigns%20varying%20degrees%20of%20importance%20to%20each%20neighbor%20based%20on%20their%0Arelevance.%20Our%20findings%20show%20that%201%29%20there%20exists%20a%20structure%20in%20the%20latent%0Aspace%20that%20differentiates%20between%20hallucinated%20and%20non-hallucinated%0Agenerations%2C%202%29%20Graph%20Attention%20Networks%20can%20learn%20this%20structure%20and%0Ageneralize%20it%20to%20unseen%20generations%2C%20and%203%29%20the%20robustness%20of%20our%20method%20is%0Aenhanced%20when%20incorporating%20contrastive%20learning.%20When%20evaluated%20against%0Aevidence-based%20benchmarks%2C%20our%20model%20performs%20similarly%20without%20access%20to%0Asearch-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Graph%2520Structures%2520to%2520Detect%2520Hallucinations%2520in%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DNoa%2520Nonkes%2520and%2520Sergei%2520Agaronian%2520and%2520Evangelos%2520Kanoulas%2520and%2520Roxana%2520Petcu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520extensively%2520applied%2520across%2520a%2520wide%2520range%2520of%2520tasks%252C%250Asuch%2520as%2520customer%2520support%252C%2520content%2520creation%252C%2520educational%2520tutoring%252C%2520and%2520providing%250Afinancial%2520guidance.%2520However%252C%2520a%2520well-known%2520drawback%2520is%2520their%2520predisposition%2520to%250Agenerate%2520hallucinations.%2520This%2520damages%2520the%2520trustworthiness%2520of%2520the%2520information%250Athese%2520models%2520provide%252C%2520impacting%2520decision-making%2520and%2520user%2520confidence.%2520We%2520propose%250Aa%2520method%2520to%2520detect%2520hallucinations%2520by%2520looking%2520at%2520the%2520structure%2520of%2520the%2520latent%250Aspace%2520and%2520finding%2520associations%2520within%2520hallucinated%2520and%2520non-hallucinated%250Agenerations.%2520We%2520create%2520a%2520graph%2520structure%2520that%2520connects%2520generations%2520that%2520lie%250Aclosely%2520in%2520the%2520embedding%2520space.%2520Moreover%252C%2520we%2520employ%2520a%2520Graph%2520Attention%2520Network%250Awhich%2520utilizes%2520message%2520passing%2520to%2520aggregate%2520information%2520from%2520neighboring%2520nodes%250Aand%2520assigns%2520varying%2520degrees%2520of%2520importance%2520to%2520each%2520neighbor%2520based%2520on%2520their%250Arelevance.%2520Our%2520findings%2520show%2520that%25201%2529%2520there%2520exists%2520a%2520structure%2520in%2520the%2520latent%250Aspace%2520that%2520differentiates%2520between%2520hallucinated%2520and%2520non-hallucinated%250Agenerations%252C%25202%2529%2520Graph%2520Attention%2520Networks%2520can%2520learn%2520this%2520structure%2520and%250Ageneralize%2520it%2520to%2520unseen%2520generations%252C%2520and%25203%2529%2520the%2520robustness%2520of%2520our%2520method%2520is%250Aenhanced%2520when%2520incorporating%2520contrastive%2520learning.%2520When%2520evaluated%2520against%250Aevidence-based%2520benchmarks%252C%2520our%2520model%2520performs%2520similarly%2520without%2520access%2520to%250Asearch-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Graph%20Structures%20to%20Detect%20Hallucinations%20in%20Large%20Language%0A%20%20Models&entry.906535625=Noa%20Nonkes%20and%20Sergei%20Agaronian%20and%20Evangelos%20Kanoulas%20and%20Roxana%20Petcu&entry.1292438233=%20%20Large%20language%20models%20are%20extensively%20applied%20across%20a%20wide%20range%20of%20tasks%2C%0Asuch%20as%20customer%20support%2C%20content%20creation%2C%20educational%20tutoring%2C%20and%20providing%0Afinancial%20guidance.%20However%2C%20a%20well-known%20drawback%20is%20their%20predisposition%20to%0Agenerate%20hallucinations.%20This%20damages%20the%20trustworthiness%20of%20the%20information%0Athese%20models%20provide%2C%20impacting%20decision-making%20and%20user%20confidence.%20We%20propose%0Aa%20method%20to%20detect%20hallucinations%20by%20looking%20at%20the%20structure%20of%20the%20latent%0Aspace%20and%20finding%20associations%20within%20hallucinated%20and%20non-hallucinated%0Agenerations.%20We%20create%20a%20graph%20structure%20that%20connects%20generations%20that%20lie%0Aclosely%20in%20the%20embedding%20space.%20Moreover%2C%20we%20employ%20a%20Graph%20Attention%20Network%0Awhich%20utilizes%20message%20passing%20to%20aggregate%20information%20from%20neighboring%20nodes%0Aand%20assigns%20varying%20degrees%20of%20importance%20to%20each%20neighbor%20based%20on%20their%0Arelevance.%20Our%20findings%20show%20that%201%29%20there%20exists%20a%20structure%20in%20the%20latent%0Aspace%20that%20differentiates%20between%20hallucinated%20and%20non-hallucinated%0Agenerations%2C%202%29%20Graph%20Attention%20Networks%20can%20learn%20this%20structure%20and%0Ageneralize%20it%20to%20unseen%20generations%2C%20and%203%29%20the%20robustness%20of%20our%20method%20is%0Aenhanced%20when%20incorporating%20contrastive%20learning.%20When%20evaluated%20against%0Aevidence-based%20benchmarks%2C%20our%20model%20performs%20similarly%20without%20access%20to%0Asearch-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04485v1&entry.124074799=Read"},
{"title": "Accelerated Parameter-Free Stochastic Optimization", "author": "Itai Kreisler and Maor Ivgi and Oliver Hinder and Yair Carmon", "abstract": "  We propose a method that achieves near-optimal rates for smooth stochastic\nconvex optimization and requires essentially no prior knowledge of problem\nparameters. This improves on prior work which requires knowing at least the\ninitial distance to optimality d0. Our method, U-DoG, combines UniXGrad (Kavis\net al., 2019) and DoG (Ivgi et al., 2023) with novel iterate stabilization\ntechniques. It requires only loose bounds on d0 and the noise magnitude,\nprovides high probability guarantees under sub-Gaussian noise, and is also\nnear-optimal in the non-smooth case. Our experiments show consistent, strong\nperformance on convex problems and mixed results on neural network training.\n", "link": "http://arxiv.org/abs/2404.00666v2", "date": "2024-07-05", "relevancy": 2.0289, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4807}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20Parameter-Free%20Stochastic%20Optimization&body=Title%3A%20Accelerated%20Parameter-Free%20Stochastic%20Optimization%0AAuthor%3A%20Itai%20Kreisler%20and%20Maor%20Ivgi%20and%20Oliver%20Hinder%20and%20Yair%20Carmon%0AAbstract%3A%20%20%20We%20propose%20a%20method%20that%20achieves%20near-optimal%20rates%20for%20smooth%20stochastic%0Aconvex%20optimization%20and%20requires%20essentially%20no%20prior%20knowledge%20of%20problem%0Aparameters.%20This%20improves%20on%20prior%20work%20which%20requires%20knowing%20at%20least%20the%0Ainitial%20distance%20to%20optimality%20d0.%20Our%20method%2C%20U-DoG%2C%20combines%20UniXGrad%20%28Kavis%0Aet%20al.%2C%202019%29%20and%20DoG%20%28Ivgi%20et%20al.%2C%202023%29%20with%20novel%20iterate%20stabilization%0Atechniques.%20It%20requires%20only%20loose%20bounds%20on%20d0%20and%20the%20noise%20magnitude%2C%0Aprovides%20high%20probability%20guarantees%20under%20sub-Gaussian%20noise%2C%20and%20is%20also%0Anear-optimal%20in%20the%20non-smooth%20case.%20Our%20experiments%20show%20consistent%2C%20strong%0Aperformance%20on%20convex%20problems%20and%20mixed%20results%20on%20neural%20network%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00666v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520Parameter-Free%2520Stochastic%2520Optimization%26entry.906535625%3DItai%2520Kreisler%2520and%2520Maor%2520Ivgi%2520and%2520Oliver%2520Hinder%2520and%2520Yair%2520Carmon%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520method%2520that%2520achieves%2520near-optimal%2520rates%2520for%2520smooth%2520stochastic%250Aconvex%2520optimization%2520and%2520requires%2520essentially%2520no%2520prior%2520knowledge%2520of%2520problem%250Aparameters.%2520This%2520improves%2520on%2520prior%2520work%2520which%2520requires%2520knowing%2520at%2520least%2520the%250Ainitial%2520distance%2520to%2520optimality%2520d0.%2520Our%2520method%252C%2520U-DoG%252C%2520combines%2520UniXGrad%2520%2528Kavis%250Aet%2520al.%252C%25202019%2529%2520and%2520DoG%2520%2528Ivgi%2520et%2520al.%252C%25202023%2529%2520with%2520novel%2520iterate%2520stabilization%250Atechniques.%2520It%2520requires%2520only%2520loose%2520bounds%2520on%2520d0%2520and%2520the%2520noise%2520magnitude%252C%250Aprovides%2520high%2520probability%2520guarantees%2520under%2520sub-Gaussian%2520noise%252C%2520and%2520is%2520also%250Anear-optimal%2520in%2520the%2520non-smooth%2520case.%2520Our%2520experiments%2520show%2520consistent%252C%2520strong%250Aperformance%2520on%2520convex%2520problems%2520and%2520mixed%2520results%2520on%2520neural%2520network%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00666v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20Parameter-Free%20Stochastic%20Optimization&entry.906535625=Itai%20Kreisler%20and%20Maor%20Ivgi%20and%20Oliver%20Hinder%20and%20Yair%20Carmon&entry.1292438233=%20%20We%20propose%20a%20method%20that%20achieves%20near-optimal%20rates%20for%20smooth%20stochastic%0Aconvex%20optimization%20and%20requires%20essentially%20no%20prior%20knowledge%20of%20problem%0Aparameters.%20This%20improves%20on%20prior%20work%20which%20requires%20knowing%20at%20least%20the%0Ainitial%20distance%20to%20optimality%20d0.%20Our%20method%2C%20U-DoG%2C%20combines%20UniXGrad%20%28Kavis%0Aet%20al.%2C%202019%29%20and%20DoG%20%28Ivgi%20et%20al.%2C%202023%29%20with%20novel%20iterate%20stabilization%0Atechniques.%20It%20requires%20only%20loose%20bounds%20on%20d0%20and%20the%20noise%20magnitude%2C%0Aprovides%20high%20probability%20guarantees%20under%20sub-Gaussian%20noise%2C%20and%20is%20also%0Anear-optimal%20in%20the%20non-smooth%20case.%20Our%20experiments%20show%20consistent%2C%20strong%0Aperformance%20on%20convex%20problems%20and%20mixed%20results%20on%20neural%20network%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00666v2&entry.124074799=Read"},
{"title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language\n  Models", "author": "Yuzhe Gu and Ziwei Ji and Wenwei Zhang and Chengqi Lyu and Dahua Lin and Kai Chen", "abstract": "  Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.\n", "link": "http://arxiv.org/abs/2407.04693v1", "date": "2024-07-05", "relevancy": 2.0266, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5766}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4931}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ANAH-v2%3A%20Scaling%20Analytical%20Hallucination%20Annotation%20of%20Large%20Language%0A%20%20Models&body=Title%3A%20ANAH-v2%3A%20Scaling%20Analytical%20Hallucination%20Annotation%20of%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yuzhe%20Gu%20and%20Ziwei%20Ji%20and%20Wenwei%20Zhang%20and%20Chengqi%20Lyu%20and%20Dahua%20Lin%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20hallucinations%20in%20long-form%0Aquestion-answering%20tasks%20across%20various%20domains%20and%20wide%20applications.%20Current%0Ahallucination%20detection%20and%20mitigation%20datasets%20are%20limited%20in%20domains%20and%0Asizes%2C%20which%20struggle%20to%20scale%20due%20to%20prohibitive%20labor%20costs%20and%20insufficient%0Areliability%20of%20existing%20hallucination%20annotators.%20To%20facilitate%20the%20scalable%0Aoversight%20of%20LLM%20hallucinations%2C%20this%20paper%20introduces%20an%20iterative%0Aself-training%20framework%20that%20simultaneously%20and%20progressively%20scales%20up%20the%0Ahallucination%20annotation%20dataset%20and%20improves%20the%20accuracy%20of%20the%20hallucination%0Aannotator.%20Based%20on%20the%20Expectation%20Maximization%20%28EM%29%20algorithm%2C%20in%20each%0Aiteration%2C%20the%20framework%20first%20applies%20a%20hallucination%20annotation%20pipeline%20to%0Aannotate%20a%20scaled%20dataset%20and%20then%20trains%20a%20more%20accurate%20hallucination%0Aannotator%20on%20the%20dataset.%20This%20new%20hallucination%20annotator%20is%20adopted%20in%20the%0Ahallucination%20annotation%20pipeline%20used%20for%20the%20next%20iteration.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20the%20finally%20obtained%20hallucination%0Aannotator%20with%20only%207B%20parameters%20surpasses%20the%20performance%20of%20GPT-4%20and%0Aobtains%20new%20state-of-the-art%20hallucination%20detection%20results%20on%20HaluEval%20and%0AHalluQA%20by%20zero-shot%20inference.%20Such%20an%20annotator%20can%20not%20only%20evaluate%20the%0Ahallucination%20levels%20of%20various%20LLMs%20on%20the%20large-scale%20dataset%20but%20also%20help%0Ato%20mitigate%20the%20hallucination%20of%20LLMs%20generations%2C%20with%20the%20Natural%20Language%0AInference%20%28NLI%29%20metric%20increasing%20from%2025%25%20to%2037%25%20on%20HaluEval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DANAH-v2%253A%2520Scaling%2520Analytical%2520Hallucination%2520Annotation%2520of%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYuzhe%2520Gu%2520and%2520Ziwei%2520Ji%2520and%2520Wenwei%2520Zhang%2520and%2520Chengqi%2520Lyu%2520and%2520Dahua%2520Lin%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520exhibit%2520hallucinations%2520in%2520long-form%250Aquestion-answering%2520tasks%2520across%2520various%2520domains%2520and%2520wide%2520applications.%2520Current%250Ahallucination%2520detection%2520and%2520mitigation%2520datasets%2520are%2520limited%2520in%2520domains%2520and%250Asizes%252C%2520which%2520struggle%2520to%2520scale%2520due%2520to%2520prohibitive%2520labor%2520costs%2520and%2520insufficient%250Areliability%2520of%2520existing%2520hallucination%2520annotators.%2520To%2520facilitate%2520the%2520scalable%250Aoversight%2520of%2520LLM%2520hallucinations%252C%2520this%2520paper%2520introduces%2520an%2520iterative%250Aself-training%2520framework%2520that%2520simultaneously%2520and%2520progressively%2520scales%2520up%2520the%250Ahallucination%2520annotation%2520dataset%2520and%2520improves%2520the%2520accuracy%2520of%2520the%2520hallucination%250Aannotator.%2520Based%2520on%2520the%2520Expectation%2520Maximization%2520%2528EM%2529%2520algorithm%252C%2520in%2520each%250Aiteration%252C%2520the%2520framework%2520first%2520applies%2520a%2520hallucination%2520annotation%2520pipeline%2520to%250Aannotate%2520a%2520scaled%2520dataset%2520and%2520then%2520trains%2520a%2520more%2520accurate%2520hallucination%250Aannotator%2520on%2520the%2520dataset.%2520This%2520new%2520hallucination%2520annotator%2520is%2520adopted%2520in%2520the%250Ahallucination%2520annotation%2520pipeline%2520used%2520for%2520the%2520next%2520iteration.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520the%2520finally%2520obtained%2520hallucination%250Aannotator%2520with%2520only%25207B%2520parameters%2520surpasses%2520the%2520performance%2520of%2520GPT-4%2520and%250Aobtains%2520new%2520state-of-the-art%2520hallucination%2520detection%2520results%2520on%2520HaluEval%2520and%250AHalluQA%2520by%2520zero-shot%2520inference.%2520Such%2520an%2520annotator%2520can%2520not%2520only%2520evaluate%2520the%250Ahallucination%2520levels%2520of%2520various%2520LLMs%2520on%2520the%2520large-scale%2520dataset%2520but%2520also%2520help%250Ato%2520mitigate%2520the%2520hallucination%2520of%2520LLMs%2520generations%252C%2520with%2520the%2520Natural%2520Language%250AInference%2520%2528NLI%2529%2520metric%2520increasing%2520from%252025%2525%2520to%252037%2525%2520on%2520HaluEval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ANAH-v2%3A%20Scaling%20Analytical%20Hallucination%20Annotation%20of%20Large%20Language%0A%20%20Models&entry.906535625=Yuzhe%20Gu%20and%20Ziwei%20Ji%20and%20Wenwei%20Zhang%20and%20Chengqi%20Lyu%20and%20Dahua%20Lin%20and%20Kai%20Chen&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20hallucinations%20in%20long-form%0Aquestion-answering%20tasks%20across%20various%20domains%20and%20wide%20applications.%20Current%0Ahallucination%20detection%20and%20mitigation%20datasets%20are%20limited%20in%20domains%20and%0Asizes%2C%20which%20struggle%20to%20scale%20due%20to%20prohibitive%20labor%20costs%20and%20insufficient%0Areliability%20of%20existing%20hallucination%20annotators.%20To%20facilitate%20the%20scalable%0Aoversight%20of%20LLM%20hallucinations%2C%20this%20paper%20introduces%20an%20iterative%0Aself-training%20framework%20that%20simultaneously%20and%20progressively%20scales%20up%20the%0Ahallucination%20annotation%20dataset%20and%20improves%20the%20accuracy%20of%20the%20hallucination%0Aannotator.%20Based%20on%20the%20Expectation%20Maximization%20%28EM%29%20algorithm%2C%20in%20each%0Aiteration%2C%20the%20framework%20first%20applies%20a%20hallucination%20annotation%20pipeline%20to%0Aannotate%20a%20scaled%20dataset%20and%20then%20trains%20a%20more%20accurate%20hallucination%0Aannotator%20on%20the%20dataset.%20This%20new%20hallucination%20annotator%20is%20adopted%20in%20the%0Ahallucination%20annotation%20pipeline%20used%20for%20the%20next%20iteration.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20the%20finally%20obtained%20hallucination%0Aannotator%20with%20only%207B%20parameters%20surpasses%20the%20performance%20of%20GPT-4%20and%0Aobtains%20new%20state-of-the-art%20hallucination%20detection%20results%20on%20HaluEval%20and%0AHalluQA%20by%20zero-shot%20inference.%20Such%20an%20annotator%20can%20not%20only%20evaluate%20the%0Ahallucination%20levels%20of%20various%20LLMs%20on%20the%20large-scale%20dataset%20but%20also%20help%0Ato%20mitigate%20the%20hallucination%20of%20LLMs%20generations%2C%20with%20the%20Natural%20Language%0AInference%20%28NLI%29%20metric%20increasing%20from%2025%25%20to%2037%25%20on%20HaluEval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04693v1&entry.124074799=Read"},
{"title": "Planning with Logical Graph-based Language Model for Instruction\n  Generation", "author": "Fan Zhang and Kebing Jin and Hankz Hankui Zhuo", "abstract": "  Despite the superior performance of large language models to generate natural\nlanguage texts, it is hard to generate texts with correct logic according to a\ngiven task, due to the difficulties for neural models to capture implied rules\nfrom free-form texts. In this paper, we propose a novel graph-based language\nmodel, Logical-GLM, to infuse logic into language models for more valid text\ngeneration and interpretability. Specifically, we first capture information\nfrom natural language instructions and construct logical bayes graphs that\ngenerally describe domains. Next, we generate logical skeletons to guide\nlanguage model training, infusing domain knowledge into language models.\nFinally, we alternately optimize the searching policy of graphs and language\nmodels until convergence. The experimental results show that Logical-GLM is\nboth effective and efficient compared with traditional language models, despite\nusing smaller-scale training data and fewer parameters. Our approach can\ngenerate instructional texts with more correct logic owing to the internalized\ndomain knowledge. Moreover, the usage of logical graphs reflects the inner\nmechanism of the language models, which improves the interpretability of\nblack-box models.\n", "link": "http://arxiv.org/abs/2308.13782v2", "date": "2024-07-05", "relevancy": 2.0212, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5205}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5002}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4921}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planning%20with%20Logical%20Graph-based%20Language%20Model%20for%20Instruction%0A%20%20Generation&body=Title%3A%20Planning%20with%20Logical%20Graph-based%20Language%20Model%20for%20Instruction%0A%20%20Generation%0AAuthor%3A%20Fan%20Zhang%20and%20Kebing%20Jin%20and%20Hankz%20Hankui%20Zhuo%0AAbstract%3A%20%20%20Despite%20the%20superior%20performance%20of%20large%20language%20models%20to%20generate%20natural%0Alanguage%20texts%2C%20it%20is%20hard%20to%20generate%20texts%20with%20correct%20logic%20according%20to%20a%0Agiven%20task%2C%20due%20to%20the%20difficulties%20for%20neural%20models%20to%20capture%20implied%20rules%0Afrom%20free-form%20texts.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20graph-based%20language%0Amodel%2C%20Logical-GLM%2C%20to%20infuse%20logic%20into%20language%20models%20for%20more%20valid%20text%0Ageneration%20and%20interpretability.%20Specifically%2C%20we%20first%20capture%20information%0Afrom%20natural%20language%20instructions%20and%20construct%20logical%20bayes%20graphs%20that%0Agenerally%20describe%20domains.%20Next%2C%20we%20generate%20logical%20skeletons%20to%20guide%0Alanguage%20model%20training%2C%20infusing%20domain%20knowledge%20into%20language%20models.%0AFinally%2C%20we%20alternately%20optimize%20the%20searching%20policy%20of%20graphs%20and%20language%0Amodels%20until%20convergence.%20The%20experimental%20results%20show%20that%20Logical-GLM%20is%0Aboth%20effective%20and%20efficient%20compared%20with%20traditional%20language%20models%2C%20despite%0Ausing%20smaller-scale%20training%20data%20and%20fewer%20parameters.%20Our%20approach%20can%0Agenerate%20instructional%20texts%20with%20more%20correct%20logic%20owing%20to%20the%20internalized%0Adomain%20knowledge.%20Moreover%2C%20the%20usage%20of%20logical%20graphs%20reflects%20the%20inner%0Amechanism%20of%20the%20language%20models%2C%20which%20improves%20the%20interpretability%20of%0Ablack-box%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13782v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanning%2520with%2520Logical%2520Graph-based%2520Language%2520Model%2520for%2520Instruction%250A%2520%2520Generation%26entry.906535625%3DFan%2520Zhang%2520and%2520Kebing%2520Jin%2520and%2520Hankz%2520Hankui%2520Zhuo%26entry.1292438233%3D%2520%2520Despite%2520the%2520superior%2520performance%2520of%2520large%2520language%2520models%2520to%2520generate%2520natural%250Alanguage%2520texts%252C%2520it%2520is%2520hard%2520to%2520generate%2520texts%2520with%2520correct%2520logic%2520according%2520to%2520a%250Agiven%2520task%252C%2520due%2520to%2520the%2520difficulties%2520for%2520neural%2520models%2520to%2520capture%2520implied%2520rules%250Afrom%2520free-form%2520texts.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520graph-based%2520language%250Amodel%252C%2520Logical-GLM%252C%2520to%2520infuse%2520logic%2520into%2520language%2520models%2520for%2520more%2520valid%2520text%250Ageneration%2520and%2520interpretability.%2520Specifically%252C%2520we%2520first%2520capture%2520information%250Afrom%2520natural%2520language%2520instructions%2520and%2520construct%2520logical%2520bayes%2520graphs%2520that%250Agenerally%2520describe%2520domains.%2520Next%252C%2520we%2520generate%2520logical%2520skeletons%2520to%2520guide%250Alanguage%2520model%2520training%252C%2520infusing%2520domain%2520knowledge%2520into%2520language%2520models.%250AFinally%252C%2520we%2520alternately%2520optimize%2520the%2520searching%2520policy%2520of%2520graphs%2520and%2520language%250Amodels%2520until%2520convergence.%2520The%2520experimental%2520results%2520show%2520that%2520Logical-GLM%2520is%250Aboth%2520effective%2520and%2520efficient%2520compared%2520with%2520traditional%2520language%2520models%252C%2520despite%250Ausing%2520smaller-scale%2520training%2520data%2520and%2520fewer%2520parameters.%2520Our%2520approach%2520can%250Agenerate%2520instructional%2520texts%2520with%2520more%2520correct%2520logic%2520owing%2520to%2520the%2520internalized%250Adomain%2520knowledge.%2520Moreover%252C%2520the%2520usage%2520of%2520logical%2520graphs%2520reflects%2520the%2520inner%250Amechanism%2520of%2520the%2520language%2520models%252C%2520which%2520improves%2520the%2520interpretability%2520of%250Ablack-box%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.13782v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planning%20with%20Logical%20Graph-based%20Language%20Model%20for%20Instruction%0A%20%20Generation&entry.906535625=Fan%20Zhang%20and%20Kebing%20Jin%20and%20Hankz%20Hankui%20Zhuo&entry.1292438233=%20%20Despite%20the%20superior%20performance%20of%20large%20language%20models%20to%20generate%20natural%0Alanguage%20texts%2C%20it%20is%20hard%20to%20generate%20texts%20with%20correct%20logic%20according%20to%20a%0Agiven%20task%2C%20due%20to%20the%20difficulties%20for%20neural%20models%20to%20capture%20implied%20rules%0Afrom%20free-form%20texts.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20graph-based%20language%0Amodel%2C%20Logical-GLM%2C%20to%20infuse%20logic%20into%20language%20models%20for%20more%20valid%20text%0Ageneration%20and%20interpretability.%20Specifically%2C%20we%20first%20capture%20information%0Afrom%20natural%20language%20instructions%20and%20construct%20logical%20bayes%20graphs%20that%0Agenerally%20describe%20domains.%20Next%2C%20we%20generate%20logical%20skeletons%20to%20guide%0Alanguage%20model%20training%2C%20infusing%20domain%20knowledge%20into%20language%20models.%0AFinally%2C%20we%20alternately%20optimize%20the%20searching%20policy%20of%20graphs%20and%20language%0Amodels%20until%20convergence.%20The%20experimental%20results%20show%20that%20Logical-GLM%20is%0Aboth%20effective%20and%20efficient%20compared%20with%20traditional%20language%20models%2C%20despite%0Ausing%20smaller-scale%20training%20data%20and%20fewer%20parameters.%20Our%20approach%20can%0Agenerate%20instructional%20texts%20with%20more%20correct%20logic%20owing%20to%20the%20internalized%0Adomain%20knowledge.%20Moreover%2C%20the%20usage%20of%20logical%20graphs%20reflects%20the%20inner%0Amechanism%20of%20the%20language%20models%2C%20which%20improves%20the%20interpretability%20of%0Ablack-box%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13782v2&entry.124074799=Read"},
{"title": "Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular\n  Data", "author": "David Holzm\u00fcller and L\u00e9o Grinsztajn and Ingo Steinwart", "abstract": "  For classification and regression on tabular data, the dominance of\ngradient-boosted decision trees (GBDTs) has recently been challenged by often\nmuch slower deep learning methods with extensive hyperparameter tuning. We\naddress this discrepancy by introducing (a) RealMLP, an improved multilayer\nperceptron (MLP), and (b) improved default parameters for GBDTs and RealMLP. We\ntune RealMLP and the default parameters on a meta-train benchmark with 71\nclassification and 47 regression datasets and compare them to\nhyperparameter-optimized versions on a disjoint meta-test benchmark with 48\nclassification and 42 regression datasets, as well as the GBDT-friendly\nbenchmark by Grinsztajn et al. (2022). Our benchmark results show that RealMLP\noffers a better time-accuracy tradeoff than other neural nets and is\ncompetitive with GBDTs. Moreover, a combination of RealMLP and GBDTs with\nimproved default parameters can achieve excellent results on medium-sized\ntabular datasets (1K--500K samples) without hyperparameter tuning.\n", "link": "http://arxiv.org/abs/2407.04491v1", "date": "2024-07-05", "relevancy": 2.019, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5564}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4706}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20by%20Default%3A%20Strong%20Pre-Tuned%20MLPs%20and%20Boosted%20Trees%20on%20Tabular%0A%20%20Data&body=Title%3A%20Better%20by%20Default%3A%20Strong%20Pre-Tuned%20MLPs%20and%20Boosted%20Trees%20on%20Tabular%0A%20%20Data%0AAuthor%3A%20David%20Holzm%C3%BCller%20and%20L%C3%A9o%20Grinsztajn%20and%20Ingo%20Steinwart%0AAbstract%3A%20%20%20For%20classification%20and%20regression%20on%20tabular%20data%2C%20the%20dominance%20of%0Agradient-boosted%20decision%20trees%20%28GBDTs%29%20has%20recently%20been%20challenged%20by%20often%0Amuch%20slower%20deep%20learning%20methods%20with%20extensive%20hyperparameter%20tuning.%20We%0Aaddress%20this%20discrepancy%20by%20introducing%20%28a%29%20RealMLP%2C%20an%20improved%20multilayer%0Aperceptron%20%28MLP%29%2C%20and%20%28b%29%20improved%20default%20parameters%20for%20GBDTs%20and%20RealMLP.%20We%0Atune%20RealMLP%20and%20the%20default%20parameters%20on%20a%20meta-train%20benchmark%20with%2071%0Aclassification%20and%2047%20regression%20datasets%20and%20compare%20them%20to%0Ahyperparameter-optimized%20versions%20on%20a%20disjoint%20meta-test%20benchmark%20with%2048%0Aclassification%20and%2042%20regression%20datasets%2C%20as%20well%20as%20the%20GBDT-friendly%0Abenchmark%20by%20Grinsztajn%20et%20al.%20%282022%29.%20Our%20benchmark%20results%20show%20that%20RealMLP%0Aoffers%20a%20better%20time-accuracy%20tradeoff%20than%20other%20neural%20nets%20and%20is%0Acompetitive%20with%20GBDTs.%20Moreover%2C%20a%20combination%20of%20RealMLP%20and%20GBDTs%20with%0Aimproved%20default%20parameters%20can%20achieve%20excellent%20results%20on%20medium-sized%0Atabular%20datasets%20%281K--500K%20samples%29%20without%20hyperparameter%20tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520by%2520Default%253A%2520Strong%2520Pre-Tuned%2520MLPs%2520and%2520Boosted%2520Trees%2520on%2520Tabular%250A%2520%2520Data%26entry.906535625%3DDavid%2520Holzm%25C3%25BCller%2520and%2520L%25C3%25A9o%2520Grinsztajn%2520and%2520Ingo%2520Steinwart%26entry.1292438233%3D%2520%2520For%2520classification%2520and%2520regression%2520on%2520tabular%2520data%252C%2520the%2520dominance%2520of%250Agradient-boosted%2520decision%2520trees%2520%2528GBDTs%2529%2520has%2520recently%2520been%2520challenged%2520by%2520often%250Amuch%2520slower%2520deep%2520learning%2520methods%2520with%2520extensive%2520hyperparameter%2520tuning.%2520We%250Aaddress%2520this%2520discrepancy%2520by%2520introducing%2520%2528a%2529%2520RealMLP%252C%2520an%2520improved%2520multilayer%250Aperceptron%2520%2528MLP%2529%252C%2520and%2520%2528b%2529%2520improved%2520default%2520parameters%2520for%2520GBDTs%2520and%2520RealMLP.%2520We%250Atune%2520RealMLP%2520and%2520the%2520default%2520parameters%2520on%2520a%2520meta-train%2520benchmark%2520with%252071%250Aclassification%2520and%252047%2520regression%2520datasets%2520and%2520compare%2520them%2520to%250Ahyperparameter-optimized%2520versions%2520on%2520a%2520disjoint%2520meta-test%2520benchmark%2520with%252048%250Aclassification%2520and%252042%2520regression%2520datasets%252C%2520as%2520well%2520as%2520the%2520GBDT-friendly%250Abenchmark%2520by%2520Grinsztajn%2520et%2520al.%2520%25282022%2529.%2520Our%2520benchmark%2520results%2520show%2520that%2520RealMLP%250Aoffers%2520a%2520better%2520time-accuracy%2520tradeoff%2520than%2520other%2520neural%2520nets%2520and%2520is%250Acompetitive%2520with%2520GBDTs.%2520Moreover%252C%2520a%2520combination%2520of%2520RealMLP%2520and%2520GBDTs%2520with%250Aimproved%2520default%2520parameters%2520can%2520achieve%2520excellent%2520results%2520on%2520medium-sized%250Atabular%2520datasets%2520%25281K--500K%2520samples%2529%2520without%2520hyperparameter%2520tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20by%20Default%3A%20Strong%20Pre-Tuned%20MLPs%20and%20Boosted%20Trees%20on%20Tabular%0A%20%20Data&entry.906535625=David%20Holzm%C3%BCller%20and%20L%C3%A9o%20Grinsztajn%20and%20Ingo%20Steinwart&entry.1292438233=%20%20For%20classification%20and%20regression%20on%20tabular%20data%2C%20the%20dominance%20of%0Agradient-boosted%20decision%20trees%20%28GBDTs%29%20has%20recently%20been%20challenged%20by%20often%0Amuch%20slower%20deep%20learning%20methods%20with%20extensive%20hyperparameter%20tuning.%20We%0Aaddress%20this%20discrepancy%20by%20introducing%20%28a%29%20RealMLP%2C%20an%20improved%20multilayer%0Aperceptron%20%28MLP%29%2C%20and%20%28b%29%20improved%20default%20parameters%20for%20GBDTs%20and%20RealMLP.%20We%0Atune%20RealMLP%20and%20the%20default%20parameters%20on%20a%20meta-train%20benchmark%20with%2071%0Aclassification%20and%2047%20regression%20datasets%20and%20compare%20them%20to%0Ahyperparameter-optimized%20versions%20on%20a%20disjoint%20meta-test%20benchmark%20with%2048%0Aclassification%20and%2042%20regression%20datasets%2C%20as%20well%20as%20the%20GBDT-friendly%0Abenchmark%20by%20Grinsztajn%20et%20al.%20%282022%29.%20Our%20benchmark%20results%20show%20that%20RealMLP%0Aoffers%20a%20better%20time-accuracy%20tradeoff%20than%20other%20neural%20nets%20and%20is%0Acompetitive%20with%20GBDTs.%20Moreover%2C%20a%20combination%20of%20RealMLP%20and%20GBDTs%20with%0Aimproved%20default%20parameters%20can%20achieve%20excellent%20results%20on%20medium-sized%0Atabular%20datasets%20%281K--500K%20samples%29%20without%20hyperparameter%20tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04491v1&entry.124074799=Read"},
{"title": "LoCo: Low-Bit Communication Adaptor for Large-scale Model Training", "author": "Xingyu Xie and Zhijie Lin and Kim-Chuan Toh and Pan Zhou", "abstract": "  To efficiently train large-scale models, low-bit gradient communication\ncompresses full-precision gradients on local GPU nodes into low-precision ones\nfor higher gradient synchronization efficiency among GPU nodes. However, it\noften degrades training quality due to compression information loss. To address\nthis, we propose the Low-bit Communication Adaptor (LoCo), which compensates\ngradients on local GPU nodes before compression, ensuring efficient\nsynchronization without compromising training quality. Specifically, LoCo\ndesigns a moving average of historical compensation errors to stably estimate\nconcurrent compression error and then adopts it to compensate for the\nconcurrent gradient compression, yielding a less lossless compression. This\nmechanism allows it to be compatible with general optimizers like Adam and\nsharding strategies like FSDP. Theoretical analysis shows that integrating LoCo\ninto full-precision optimizers like Adam and SGD does not impair their\nconvergence speed on nonconvex problems. Experimental results show that across\nlarge-scale model training frameworks like Megatron-LM and PyTorch's FSDP, LoCo\nsignificantly improves communication efficiency, e.g., improving Adam's\ntraining speed by 14% to 40% without performance degradation on large language\nmodels like LLAMAs and MoE.\n", "link": "http://arxiv.org/abs/2407.04480v1", "date": "2024-07-05", "relevancy": 2.0162, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5199}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.518}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoCo%3A%20Low-Bit%20Communication%20Adaptor%20for%20Large-scale%20Model%20Training&body=Title%3A%20LoCo%3A%20Low-Bit%20Communication%20Adaptor%20for%20Large-scale%20Model%20Training%0AAuthor%3A%20Xingyu%20Xie%20and%20Zhijie%20Lin%20and%20Kim-Chuan%20Toh%20and%20Pan%20Zhou%0AAbstract%3A%20%20%20To%20efficiently%20train%20large-scale%20models%2C%20low-bit%20gradient%20communication%0Acompresses%20full-precision%20gradients%20on%20local%20GPU%20nodes%20into%20low-precision%20ones%0Afor%20higher%20gradient%20synchronization%20efficiency%20among%20GPU%20nodes.%20However%2C%20it%0Aoften%20degrades%20training%20quality%20due%20to%20compression%20information%20loss.%20To%20address%0Athis%2C%20we%20propose%20the%20Low-bit%20Communication%20Adaptor%20%28LoCo%29%2C%20which%20compensates%0Agradients%20on%20local%20GPU%20nodes%20before%20compression%2C%20ensuring%20efficient%0Asynchronization%20without%20compromising%20training%20quality.%20Specifically%2C%20LoCo%0Adesigns%20a%20moving%20average%20of%20historical%20compensation%20errors%20to%20stably%20estimate%0Aconcurrent%20compression%20error%20and%20then%20adopts%20it%20to%20compensate%20for%20the%0Aconcurrent%20gradient%20compression%2C%20yielding%20a%20less%20lossless%20compression.%20This%0Amechanism%20allows%20it%20to%20be%20compatible%20with%20general%20optimizers%20like%20Adam%20and%0Asharding%20strategies%20like%20FSDP.%20Theoretical%20analysis%20shows%20that%20integrating%20LoCo%0Ainto%20full-precision%20optimizers%20like%20Adam%20and%20SGD%20does%20not%20impair%20their%0Aconvergence%20speed%20on%20nonconvex%20problems.%20Experimental%20results%20show%20that%20across%0Alarge-scale%20model%20training%20frameworks%20like%20Megatron-LM%20and%20PyTorch%27s%20FSDP%2C%20LoCo%0Asignificantly%20improves%20communication%20efficiency%2C%20e.g.%2C%20improving%20Adam%27s%0Atraining%20speed%20by%2014%25%20to%2040%25%20without%20performance%20degradation%20on%20large%20language%0Amodels%20like%20LLAMAs%20and%20MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoCo%253A%2520Low-Bit%2520Communication%2520Adaptor%2520for%2520Large-scale%2520Model%2520Training%26entry.906535625%3DXingyu%2520Xie%2520and%2520Zhijie%2520Lin%2520and%2520Kim-Chuan%2520Toh%2520and%2520Pan%2520Zhou%26entry.1292438233%3D%2520%2520To%2520efficiently%2520train%2520large-scale%2520models%252C%2520low-bit%2520gradient%2520communication%250Acompresses%2520full-precision%2520gradients%2520on%2520local%2520GPU%2520nodes%2520into%2520low-precision%2520ones%250Afor%2520higher%2520gradient%2520synchronization%2520efficiency%2520among%2520GPU%2520nodes.%2520However%252C%2520it%250Aoften%2520degrades%2520training%2520quality%2520due%2520to%2520compression%2520information%2520loss.%2520To%2520address%250Athis%252C%2520we%2520propose%2520the%2520Low-bit%2520Communication%2520Adaptor%2520%2528LoCo%2529%252C%2520which%2520compensates%250Agradients%2520on%2520local%2520GPU%2520nodes%2520before%2520compression%252C%2520ensuring%2520efficient%250Asynchronization%2520without%2520compromising%2520training%2520quality.%2520Specifically%252C%2520LoCo%250Adesigns%2520a%2520moving%2520average%2520of%2520historical%2520compensation%2520errors%2520to%2520stably%2520estimate%250Aconcurrent%2520compression%2520error%2520and%2520then%2520adopts%2520it%2520to%2520compensate%2520for%2520the%250Aconcurrent%2520gradient%2520compression%252C%2520yielding%2520a%2520less%2520lossless%2520compression.%2520This%250Amechanism%2520allows%2520it%2520to%2520be%2520compatible%2520with%2520general%2520optimizers%2520like%2520Adam%2520and%250Asharding%2520strategies%2520like%2520FSDP.%2520Theoretical%2520analysis%2520shows%2520that%2520integrating%2520LoCo%250Ainto%2520full-precision%2520optimizers%2520like%2520Adam%2520and%2520SGD%2520does%2520not%2520impair%2520their%250Aconvergence%2520speed%2520on%2520nonconvex%2520problems.%2520Experimental%2520results%2520show%2520that%2520across%250Alarge-scale%2520model%2520training%2520frameworks%2520like%2520Megatron-LM%2520and%2520PyTorch%2527s%2520FSDP%252C%2520LoCo%250Asignificantly%2520improves%2520communication%2520efficiency%252C%2520e.g.%252C%2520improving%2520Adam%2527s%250Atraining%2520speed%2520by%252014%2525%2520to%252040%2525%2520without%2520performance%2520degradation%2520on%2520large%2520language%250Amodels%2520like%2520LLAMAs%2520and%2520MoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoCo%3A%20Low-Bit%20Communication%20Adaptor%20for%20Large-scale%20Model%20Training&entry.906535625=Xingyu%20Xie%20and%20Zhijie%20Lin%20and%20Kim-Chuan%20Toh%20and%20Pan%20Zhou&entry.1292438233=%20%20To%20efficiently%20train%20large-scale%20models%2C%20low-bit%20gradient%20communication%0Acompresses%20full-precision%20gradients%20on%20local%20GPU%20nodes%20into%20low-precision%20ones%0Afor%20higher%20gradient%20synchronization%20efficiency%20among%20GPU%20nodes.%20However%2C%20it%0Aoften%20degrades%20training%20quality%20due%20to%20compression%20information%20loss.%20To%20address%0Athis%2C%20we%20propose%20the%20Low-bit%20Communication%20Adaptor%20%28LoCo%29%2C%20which%20compensates%0Agradients%20on%20local%20GPU%20nodes%20before%20compression%2C%20ensuring%20efficient%0Asynchronization%20without%20compromising%20training%20quality.%20Specifically%2C%20LoCo%0Adesigns%20a%20moving%20average%20of%20historical%20compensation%20errors%20to%20stably%20estimate%0Aconcurrent%20compression%20error%20and%20then%20adopts%20it%20to%20compensate%20for%20the%0Aconcurrent%20gradient%20compression%2C%20yielding%20a%20less%20lossless%20compression.%20This%0Amechanism%20allows%20it%20to%20be%20compatible%20with%20general%20optimizers%20like%20Adam%20and%0Asharding%20strategies%20like%20FSDP.%20Theoretical%20analysis%20shows%20that%20integrating%20LoCo%0Ainto%20full-precision%20optimizers%20like%20Adam%20and%20SGD%20does%20not%20impair%20their%0Aconvergence%20speed%20on%20nonconvex%20problems.%20Experimental%20results%20show%20that%20across%0Alarge-scale%20model%20training%20frameworks%20like%20Megatron-LM%20and%20PyTorch%27s%20FSDP%2C%20LoCo%0Asignificantly%20improves%20communication%20efficiency%2C%20e.g.%2C%20improving%20Adam%27s%0Atraining%20speed%20by%2014%25%20to%2040%25%20without%20performance%20degradation%20on%20large%20language%0Amodels%20like%20LLAMAs%20and%20MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04480v1&entry.124074799=Read"},
{"title": "LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing\n  Layer Execution Order", "author": "Matthias Freiberger and Peter Kun and Anders Sundnes L\u00f8vlie and Sebastian Risi", "abstract": "  Due to their architecture and how they are trained, artificial neural\nnetworks are typically not robust toward pruning, replacing, or shuffling\nlayers at test time. However, such properties would be desirable for different\napplications, such as distributed neural network architectures where the order\nof execution cannot be guaranteed or parts of the network can fail during\ninference. In this work, we address these issues through a number of proposed\ntraining approaches for vision transformers whose most important component is\nrandomizing the execution order of attention modules at training time. We show\nthat with our proposed approaches, vision transformers are indeed capable to\nadapt to arbitrary layer execution orders at test time assuming one tolerates a\nreduction (about 20\\%) in accuracy at the same model size. We also find that\nour trained models can be randomly merged with each other resulting in\nfunctional (\"Frankenstein\") models without loss of performance compared to the\nsource models. Finally, we layer-prune our models at test time and find that\ntheir performance declines gracefully.\n", "link": "http://arxiv.org/abs/2407.04513v1", "date": "2024-07-05", "relevancy": 2.0154, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5357}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5043}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LayerShuffle%3A%20Enhancing%20Robustness%20in%20Vision%20Transformers%20by%20Randomizing%0A%20%20Layer%20Execution%20Order&body=Title%3A%20LayerShuffle%3A%20Enhancing%20Robustness%20in%20Vision%20Transformers%20by%20Randomizing%0A%20%20Layer%20Execution%20Order%0AAuthor%3A%20Matthias%20Freiberger%20and%20Peter%20Kun%20and%20Anders%20Sundnes%20L%C3%B8vlie%20and%20Sebastian%20Risi%0AAbstract%3A%20%20%20Due%20to%20their%20architecture%20and%20how%20they%20are%20trained%2C%20artificial%20neural%0Anetworks%20are%20typically%20not%20robust%20toward%20pruning%2C%20replacing%2C%20or%20shuffling%0Alayers%20at%20test%20time.%20However%2C%20such%20properties%20would%20be%20desirable%20for%20different%0Aapplications%2C%20such%20as%20distributed%20neural%20network%20architectures%20where%20the%20order%0Aof%20execution%20cannot%20be%20guaranteed%20or%20parts%20of%20the%20network%20can%20fail%20during%0Ainference.%20In%20this%20work%2C%20we%20address%20these%20issues%20through%20a%20number%20of%20proposed%0Atraining%20approaches%20for%20vision%20transformers%20whose%20most%20important%20component%20is%0Arandomizing%20the%20execution%20order%20of%20attention%20modules%20at%20training%20time.%20We%20show%0Athat%20with%20our%20proposed%20approaches%2C%20vision%20transformers%20are%20indeed%20capable%20to%0Aadapt%20to%20arbitrary%20layer%20execution%20orders%20at%20test%20time%20assuming%20one%20tolerates%20a%0Areduction%20%28about%2020%5C%25%29%20in%20accuracy%20at%20the%20same%20model%20size.%20We%20also%20find%20that%0Aour%20trained%20models%20can%20be%20randomly%20merged%20with%20each%20other%20resulting%20in%0Afunctional%20%28%22Frankenstein%22%29%20models%20without%20loss%20of%20performance%20compared%20to%20the%0Asource%20models.%20Finally%2C%20we%20layer-prune%20our%20models%20at%20test%20time%20and%20find%20that%0Atheir%20performance%20declines%20gracefully.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayerShuffle%253A%2520Enhancing%2520Robustness%2520in%2520Vision%2520Transformers%2520by%2520Randomizing%250A%2520%2520Layer%2520Execution%2520Order%26entry.906535625%3DMatthias%2520Freiberger%2520and%2520Peter%2520Kun%2520and%2520Anders%2520Sundnes%2520L%25C3%25B8vlie%2520and%2520Sebastian%2520Risi%26entry.1292438233%3D%2520%2520Due%2520to%2520their%2520architecture%2520and%2520how%2520they%2520are%2520trained%252C%2520artificial%2520neural%250Anetworks%2520are%2520typically%2520not%2520robust%2520toward%2520pruning%252C%2520replacing%252C%2520or%2520shuffling%250Alayers%2520at%2520test%2520time.%2520However%252C%2520such%2520properties%2520would%2520be%2520desirable%2520for%2520different%250Aapplications%252C%2520such%2520as%2520distributed%2520neural%2520network%2520architectures%2520where%2520the%2520order%250Aof%2520execution%2520cannot%2520be%2520guaranteed%2520or%2520parts%2520of%2520the%2520network%2520can%2520fail%2520during%250Ainference.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520issues%2520through%2520a%2520number%2520of%2520proposed%250Atraining%2520approaches%2520for%2520vision%2520transformers%2520whose%2520most%2520important%2520component%2520is%250Arandomizing%2520the%2520execution%2520order%2520of%2520attention%2520modules%2520at%2520training%2520time.%2520We%2520show%250Athat%2520with%2520our%2520proposed%2520approaches%252C%2520vision%2520transformers%2520are%2520indeed%2520capable%2520to%250Aadapt%2520to%2520arbitrary%2520layer%2520execution%2520orders%2520at%2520test%2520time%2520assuming%2520one%2520tolerates%2520a%250Areduction%2520%2528about%252020%255C%2525%2529%2520in%2520accuracy%2520at%2520the%2520same%2520model%2520size.%2520We%2520also%2520find%2520that%250Aour%2520trained%2520models%2520can%2520be%2520randomly%2520merged%2520with%2520each%2520other%2520resulting%2520in%250Afunctional%2520%2528%2522Frankenstein%2522%2529%2520models%2520without%2520loss%2520of%2520performance%2520compared%2520to%2520the%250Asource%2520models.%2520Finally%252C%2520we%2520layer-prune%2520our%2520models%2520at%2520test%2520time%2520and%2520find%2520that%250Atheir%2520performance%2520declines%2520gracefully.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerShuffle%3A%20Enhancing%20Robustness%20in%20Vision%20Transformers%20by%20Randomizing%0A%20%20Layer%20Execution%20Order&entry.906535625=Matthias%20Freiberger%20and%20Peter%20Kun%20and%20Anders%20Sundnes%20L%C3%B8vlie%20and%20Sebastian%20Risi&entry.1292438233=%20%20Due%20to%20their%20architecture%20and%20how%20they%20are%20trained%2C%20artificial%20neural%0Anetworks%20are%20typically%20not%20robust%20toward%20pruning%2C%20replacing%2C%20or%20shuffling%0Alayers%20at%20test%20time.%20However%2C%20such%20properties%20would%20be%20desirable%20for%20different%0Aapplications%2C%20such%20as%20distributed%20neural%20network%20architectures%20where%20the%20order%0Aof%20execution%20cannot%20be%20guaranteed%20or%20parts%20of%20the%20network%20can%20fail%20during%0Ainference.%20In%20this%20work%2C%20we%20address%20these%20issues%20through%20a%20number%20of%20proposed%0Atraining%20approaches%20for%20vision%20transformers%20whose%20most%20important%20component%20is%0Arandomizing%20the%20execution%20order%20of%20attention%20modules%20at%20training%20time.%20We%20show%0Athat%20with%20our%20proposed%20approaches%2C%20vision%20transformers%20are%20indeed%20capable%20to%0Aadapt%20to%20arbitrary%20layer%20execution%20orders%20at%20test%20time%20assuming%20one%20tolerates%20a%0Areduction%20%28about%2020%5C%25%29%20in%20accuracy%20at%20the%20same%20model%20size.%20We%20also%20find%20that%0Aour%20trained%20models%20can%20be%20randomly%20merged%20with%20each%20other%20resulting%20in%0Afunctional%20%28%22Frankenstein%22%29%20models%20without%20loss%20of%20performance%20compared%20to%20the%0Asource%20models.%20Finally%2C%20we%20layer-prune%20our%20models%20at%20test%20time%20and%20find%20that%0Atheir%20performance%20declines%20gracefully.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04513v1&entry.124074799=Read"},
{"title": "Remember This Event That Year? Assessing Temporal Information and\n  Reasoning in Large Language Models", "author": "Himanshu Beniwal and Dishant Patel and Kowsik Nandagopan D and Hritik Ladia and Ankit Yadav and Mayank Singh", "abstract": "  Large Language Models (LLMs) are increasingly ubiquitous, yet their ability\nto retain and reason about temporal information remains limited, hindering\ntheir application in real-world scenarios where understanding the sequential\nnature of events is crucial. Our study experiments with 12 state-of-the-art\nmodels (ranging from 2B to 70B+ parameters) on a novel numerical-temporal\ndataset, \\textbf{TempUN}, spanning from 10,000 BCE to 2100 CE, to uncover\nsignificant temporal retention and comprehension limitations. We propose six\nmetrics to assess three learning paradigms to enhance temporal knowledge\nacquisition. Our findings reveal that open-source models exhibit knowledge gaps\nmore frequently, suggesting a trade-off between limited knowledge and incorrect\nresponses. Additionally, various fine-tuning approaches significantly improved\nperformance, reducing incorrect outputs and impacting the identification of\n'information not available' in the generations. The associated dataset and code\nare available at (https://github.com/lingoiitgn/TempUN).\n", "link": "http://arxiv.org/abs/2402.11997v2", "date": "2024-07-05", "relevancy": 2.0105, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5125}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5013}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Remember%20This%20Event%20That%20Year%3F%20Assessing%20Temporal%20Information%20and%0A%20%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20Remember%20This%20Event%20That%20Year%3F%20Assessing%20Temporal%20Information%20and%0A%20%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Himanshu%20Beniwal%20and%20Dishant%20Patel%20and%20Kowsik%20Nandagopan%20D%20and%20Hritik%20Ladia%20and%20Ankit%20Yadav%20and%20Mayank%20Singh%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20ubiquitous%2C%20yet%20their%20ability%0Ato%20retain%20and%20reason%20about%20temporal%20information%20remains%20limited%2C%20hindering%0Atheir%20application%20in%20real-world%20scenarios%20where%20understanding%20the%20sequential%0Anature%20of%20events%20is%20crucial.%20Our%20study%20experiments%20with%2012%20state-of-the-art%0Amodels%20%28ranging%20from%202B%20to%2070B%2B%20parameters%29%20on%20a%20novel%20numerical-temporal%0Adataset%2C%20%5Ctextbf%7BTempUN%7D%2C%20spanning%20from%2010%2C000%20BCE%20to%202100%20CE%2C%20to%20uncover%0Asignificant%20temporal%20retention%20and%20comprehension%20limitations.%20We%20propose%20six%0Ametrics%20to%20assess%20three%20learning%20paradigms%20to%20enhance%20temporal%20knowledge%0Aacquisition.%20Our%20findings%20reveal%20that%20open-source%20models%20exhibit%20knowledge%20gaps%0Amore%20frequently%2C%20suggesting%20a%20trade-off%20between%20limited%20knowledge%20and%20incorrect%0Aresponses.%20Additionally%2C%20various%20fine-tuning%20approaches%20significantly%20improved%0Aperformance%2C%20reducing%20incorrect%20outputs%20and%20impacting%20the%20identification%20of%0A%27information%20not%20available%27%20in%20the%20generations.%20The%20associated%20dataset%20and%20code%0Aare%20available%20at%20%28https%3A//github.com/lingoiitgn/TempUN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11997v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRemember%2520This%2520Event%2520That%2520Year%253F%2520Assessing%2520Temporal%2520Information%2520and%250A%2520%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DHimanshu%2520Beniwal%2520and%2520Dishant%2520Patel%2520and%2520Kowsik%2520Nandagopan%2520D%2520and%2520Hritik%2520Ladia%2520and%2520Ankit%2520Yadav%2520and%2520Mayank%2520Singh%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520ubiquitous%252C%2520yet%2520their%2520ability%250Ato%2520retain%2520and%2520reason%2520about%2520temporal%2520information%2520remains%2520limited%252C%2520hindering%250Atheir%2520application%2520in%2520real-world%2520scenarios%2520where%2520understanding%2520the%2520sequential%250Anature%2520of%2520events%2520is%2520crucial.%2520Our%2520study%2520experiments%2520with%252012%2520state-of-the-art%250Amodels%2520%2528ranging%2520from%25202B%2520to%252070B%252B%2520parameters%2529%2520on%2520a%2520novel%2520numerical-temporal%250Adataset%252C%2520%255Ctextbf%257BTempUN%257D%252C%2520spanning%2520from%252010%252C000%2520BCE%2520to%25202100%2520CE%252C%2520to%2520uncover%250Asignificant%2520temporal%2520retention%2520and%2520comprehension%2520limitations.%2520We%2520propose%2520six%250Ametrics%2520to%2520assess%2520three%2520learning%2520paradigms%2520to%2520enhance%2520temporal%2520knowledge%250Aacquisition.%2520Our%2520findings%2520reveal%2520that%2520open-source%2520models%2520exhibit%2520knowledge%2520gaps%250Amore%2520frequently%252C%2520suggesting%2520a%2520trade-off%2520between%2520limited%2520knowledge%2520and%2520incorrect%250Aresponses.%2520Additionally%252C%2520various%2520fine-tuning%2520approaches%2520significantly%2520improved%250Aperformance%252C%2520reducing%2520incorrect%2520outputs%2520and%2520impacting%2520the%2520identification%2520of%250A%2527information%2520not%2520available%2527%2520in%2520the%2520generations.%2520The%2520associated%2520dataset%2520and%2520code%250Aare%2520available%2520at%2520%2528https%253A//github.com/lingoiitgn/TempUN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11997v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Remember%20This%20Event%20That%20Year%3F%20Assessing%20Temporal%20Information%20and%0A%20%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Himanshu%20Beniwal%20and%20Dishant%20Patel%20and%20Kowsik%20Nandagopan%20D%20and%20Hritik%20Ladia%20and%20Ankit%20Yadav%20and%20Mayank%20Singh&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20ubiquitous%2C%20yet%20their%20ability%0Ato%20retain%20and%20reason%20about%20temporal%20information%20remains%20limited%2C%20hindering%0Atheir%20application%20in%20real-world%20scenarios%20where%20understanding%20the%20sequential%0Anature%20of%20events%20is%20crucial.%20Our%20study%20experiments%20with%2012%20state-of-the-art%0Amodels%20%28ranging%20from%202B%20to%2070B%2B%20parameters%29%20on%20a%20novel%20numerical-temporal%0Adataset%2C%20%5Ctextbf%7BTempUN%7D%2C%20spanning%20from%2010%2C000%20BCE%20to%202100%20CE%2C%20to%20uncover%0Asignificant%20temporal%20retention%20and%20comprehension%20limitations.%20We%20propose%20six%0Ametrics%20to%20assess%20three%20learning%20paradigms%20to%20enhance%20temporal%20knowledge%0Aacquisition.%20Our%20findings%20reveal%20that%20open-source%20models%20exhibit%20knowledge%20gaps%0Amore%20frequently%2C%20suggesting%20a%20trade-off%20between%20limited%20knowledge%20and%20incorrect%0Aresponses.%20Additionally%2C%20various%20fine-tuning%20approaches%20significantly%20improved%0Aperformance%2C%20reducing%20incorrect%20outputs%20and%20impacting%20the%20identification%20of%0A%27information%20not%20available%27%20in%20the%20generations.%20The%20associated%20dataset%20and%20code%0Aare%20available%20at%20%28https%3A//github.com/lingoiitgn/TempUN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11997v2&entry.124074799=Read"},
{"title": "Improving Low-Resource Knowledge Tracing Tasks by Supervised\n  Pre-training and Importance Mechanism Fine-tuning", "author": "Hengyuan Zhang and Zitao Liu and Shuyan Huang and Chenming Shang and Bojun Zhan and Yong Jiang", "abstract": "  Knowledge tracing (KT) aims to estimate student's knowledge mastery based on\ntheir historical interactions. Recently, the deep learning based KT (DLKT)\napproaches have achieved impressive performance in the KT task. These DLKT\nmodels heavily rely on the large number of available student interactions.\nHowever, due to various reasons such as budget constraints and privacy\nconcerns, observed interactions are very limited in many real-world scenarios,\na.k.a, low-resource KT datasets. Directly training a DLKT model on a\nlow-resource KT dataset may lead to overfitting and it is difficult to choose\nthe appropriate deep neural architecture. Therefore, in this paper, we propose\na low-resource KT framework called LoReKT to address above challenges. Inspired\nby the prevalent \"pre-training and fine-tuning\" paradigm, we aim to learn\ntransferable parameters and representations from rich-resource KT datasets\nduring the pre-training stage and subsequently facilitate effective adaptation\nto low-resource KT datasets. Specifically, we simplify existing sophisticated\nDLKT model architectures with purely a stack of transformer decoders. We design\nan encoding mechanism to incorporate student interactions from multiple KT data\nsources and develop an importance mechanism to prioritize updating parameters\nwith high importance while constraining less important ones during the\nfine-tuning stage. We evaluate LoReKT on six public KT datasets and\nexperimental results demonstrate the superiority of our approach in terms of\nAUC and Accuracy. To encourage reproducible research, we make our data and code\npublicly available at https://anonymous.4open.science/r/LoReKT-C619.\n", "link": "http://arxiv.org/abs/2403.06725v3", "date": "2024-07-05", "relevancy": 2.0026, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5162}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4918}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Low-Resource%20Knowledge%20Tracing%20Tasks%20by%20Supervised%0A%20%20Pre-training%20and%20Importance%20Mechanism%20Fine-tuning&body=Title%3A%20Improving%20Low-Resource%20Knowledge%20Tracing%20Tasks%20by%20Supervised%0A%20%20Pre-training%20and%20Importance%20Mechanism%20Fine-tuning%0AAuthor%3A%20Hengyuan%20Zhang%20and%20Zitao%20Liu%20and%20Shuyan%20Huang%20and%20Chenming%20Shang%20and%20Bojun%20Zhan%20and%20Yong%20Jiang%0AAbstract%3A%20%20%20Knowledge%20tracing%20%28KT%29%20aims%20to%20estimate%20student%27s%20knowledge%20mastery%20based%20on%0Atheir%20historical%20interactions.%20Recently%2C%20the%20deep%20learning%20based%20KT%20%28DLKT%29%0Aapproaches%20have%20achieved%20impressive%20performance%20in%20the%20KT%20task.%20These%20DLKT%0Amodels%20heavily%20rely%20on%20the%20large%20number%20of%20available%20student%20interactions.%0AHowever%2C%20due%20to%20various%20reasons%20such%20as%20budget%20constraints%20and%20privacy%0Aconcerns%2C%20observed%20interactions%20are%20very%20limited%20in%20many%20real-world%20scenarios%2C%0Aa.k.a%2C%20low-resource%20KT%20datasets.%20Directly%20training%20a%20DLKT%20model%20on%20a%0Alow-resource%20KT%20dataset%20may%20lead%20to%20overfitting%20and%20it%20is%20difficult%20to%20choose%0Athe%20appropriate%20deep%20neural%20architecture.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%0Aa%20low-resource%20KT%20framework%20called%20LoReKT%20to%20address%20above%20challenges.%20Inspired%0Aby%20the%20prevalent%20%22pre-training%20and%20fine-tuning%22%20paradigm%2C%20we%20aim%20to%20learn%0Atransferable%20parameters%20and%20representations%20from%20rich-resource%20KT%20datasets%0Aduring%20the%20pre-training%20stage%20and%20subsequently%20facilitate%20effective%20adaptation%0Ato%20low-resource%20KT%20datasets.%20Specifically%2C%20we%20simplify%20existing%20sophisticated%0ADLKT%20model%20architectures%20with%20purely%20a%20stack%20of%20transformer%20decoders.%20We%20design%0Aan%20encoding%20mechanism%20to%20incorporate%20student%20interactions%20from%20multiple%20KT%20data%0Asources%20and%20develop%20an%20importance%20mechanism%20to%20prioritize%20updating%20parameters%0Awith%20high%20importance%20while%20constraining%20less%20important%20ones%20during%20the%0Afine-tuning%20stage.%20We%20evaluate%20LoReKT%20on%20six%20public%20KT%20datasets%20and%0Aexperimental%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20in%20terms%20of%0AAUC%20and%20Accuracy.%20To%20encourage%20reproducible%20research%2C%20we%20make%20our%20data%20and%20code%0Apublicly%20available%20at%20https%3A//anonymous.4open.science/r/LoReKT-C619.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06725v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Low-Resource%2520Knowledge%2520Tracing%2520Tasks%2520by%2520Supervised%250A%2520%2520Pre-training%2520and%2520Importance%2520Mechanism%2520Fine-tuning%26entry.906535625%3DHengyuan%2520Zhang%2520and%2520Zitao%2520Liu%2520and%2520Shuyan%2520Huang%2520and%2520Chenming%2520Shang%2520and%2520Bojun%2520Zhan%2520and%2520Yong%2520Jiang%26entry.1292438233%3D%2520%2520Knowledge%2520tracing%2520%2528KT%2529%2520aims%2520to%2520estimate%2520student%2527s%2520knowledge%2520mastery%2520based%2520on%250Atheir%2520historical%2520interactions.%2520Recently%252C%2520the%2520deep%2520learning%2520based%2520KT%2520%2528DLKT%2529%250Aapproaches%2520have%2520achieved%2520impressive%2520performance%2520in%2520the%2520KT%2520task.%2520These%2520DLKT%250Amodels%2520heavily%2520rely%2520on%2520the%2520large%2520number%2520of%2520available%2520student%2520interactions.%250AHowever%252C%2520due%2520to%2520various%2520reasons%2520such%2520as%2520budget%2520constraints%2520and%2520privacy%250Aconcerns%252C%2520observed%2520interactions%2520are%2520very%2520limited%2520in%2520many%2520real-world%2520scenarios%252C%250Aa.k.a%252C%2520low-resource%2520KT%2520datasets.%2520Directly%2520training%2520a%2520DLKT%2520model%2520on%2520a%250Alow-resource%2520KT%2520dataset%2520may%2520lead%2520to%2520overfitting%2520and%2520it%2520is%2520difficult%2520to%2520choose%250Athe%2520appropriate%2520deep%2520neural%2520architecture.%2520Therefore%252C%2520in%2520this%2520paper%252C%2520we%2520propose%250Aa%2520low-resource%2520KT%2520framework%2520called%2520LoReKT%2520to%2520address%2520above%2520challenges.%2520Inspired%250Aby%2520the%2520prevalent%2520%2522pre-training%2520and%2520fine-tuning%2522%2520paradigm%252C%2520we%2520aim%2520to%2520learn%250Atransferable%2520parameters%2520and%2520representations%2520from%2520rich-resource%2520KT%2520datasets%250Aduring%2520the%2520pre-training%2520stage%2520and%2520subsequently%2520facilitate%2520effective%2520adaptation%250Ato%2520low-resource%2520KT%2520datasets.%2520Specifically%252C%2520we%2520simplify%2520existing%2520sophisticated%250ADLKT%2520model%2520architectures%2520with%2520purely%2520a%2520stack%2520of%2520transformer%2520decoders.%2520We%2520design%250Aan%2520encoding%2520mechanism%2520to%2520incorporate%2520student%2520interactions%2520from%2520multiple%2520KT%2520data%250Asources%2520and%2520develop%2520an%2520importance%2520mechanism%2520to%2520prioritize%2520updating%2520parameters%250Awith%2520high%2520importance%2520while%2520constraining%2520less%2520important%2520ones%2520during%2520the%250Afine-tuning%2520stage.%2520We%2520evaluate%2520LoReKT%2520on%2520six%2520public%2520KT%2520datasets%2520and%250Aexperimental%2520results%2520demonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520in%2520terms%2520of%250AAUC%2520and%2520Accuracy.%2520To%2520encourage%2520reproducible%2520research%252C%2520we%2520make%2520our%2520data%2520and%2520code%250Apublicly%2520available%2520at%2520https%253A//anonymous.4open.science/r/LoReKT-C619.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06725v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Low-Resource%20Knowledge%20Tracing%20Tasks%20by%20Supervised%0A%20%20Pre-training%20and%20Importance%20Mechanism%20Fine-tuning&entry.906535625=Hengyuan%20Zhang%20and%20Zitao%20Liu%20and%20Shuyan%20Huang%20and%20Chenming%20Shang%20and%20Bojun%20Zhan%20and%20Yong%20Jiang&entry.1292438233=%20%20Knowledge%20tracing%20%28KT%29%20aims%20to%20estimate%20student%27s%20knowledge%20mastery%20based%20on%0Atheir%20historical%20interactions.%20Recently%2C%20the%20deep%20learning%20based%20KT%20%28DLKT%29%0Aapproaches%20have%20achieved%20impressive%20performance%20in%20the%20KT%20task.%20These%20DLKT%0Amodels%20heavily%20rely%20on%20the%20large%20number%20of%20available%20student%20interactions.%0AHowever%2C%20due%20to%20various%20reasons%20such%20as%20budget%20constraints%20and%20privacy%0Aconcerns%2C%20observed%20interactions%20are%20very%20limited%20in%20many%20real-world%20scenarios%2C%0Aa.k.a%2C%20low-resource%20KT%20datasets.%20Directly%20training%20a%20DLKT%20model%20on%20a%0Alow-resource%20KT%20dataset%20may%20lead%20to%20overfitting%20and%20it%20is%20difficult%20to%20choose%0Athe%20appropriate%20deep%20neural%20architecture.%20Therefore%2C%20in%20this%20paper%2C%20we%20propose%0Aa%20low-resource%20KT%20framework%20called%20LoReKT%20to%20address%20above%20challenges.%20Inspired%0Aby%20the%20prevalent%20%22pre-training%20and%20fine-tuning%22%20paradigm%2C%20we%20aim%20to%20learn%0Atransferable%20parameters%20and%20representations%20from%20rich-resource%20KT%20datasets%0Aduring%20the%20pre-training%20stage%20and%20subsequently%20facilitate%20effective%20adaptation%0Ato%20low-resource%20KT%20datasets.%20Specifically%2C%20we%20simplify%20existing%20sophisticated%0ADLKT%20model%20architectures%20with%20purely%20a%20stack%20of%20transformer%20decoders.%20We%20design%0Aan%20encoding%20mechanism%20to%20incorporate%20student%20interactions%20from%20multiple%20KT%20data%0Asources%20and%20develop%20an%20importance%20mechanism%20to%20prioritize%20updating%20parameters%0Awith%20high%20importance%20while%20constraining%20less%20important%20ones%20during%20the%0Afine-tuning%20stage.%20We%20evaluate%20LoReKT%20on%20six%20public%20KT%20datasets%20and%0Aexperimental%20results%20demonstrate%20the%20superiority%20of%20our%20approach%20in%20terms%20of%0AAUC%20and%20Accuracy.%20To%20encourage%20reproducible%20research%2C%20we%20make%20our%20data%20and%20code%0Apublicly%20available%20at%20https%3A//anonymous.4open.science/r/LoReKT-C619.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06725v3&entry.124074799=Read"},
{"title": "Low-Resource Crop Classification from Multi-Spectral Time Series Using\n  Lossless Compressors", "author": "Wei Cheng and Hongrui Ye and Xiao Wen and Jiachen Zhang and Jiping Xu and Feifan Zhang", "abstract": "  Deep learning has significantly improved the accuracy of crop classification\nusing multispectral temporal data. However, these models have complex\nstructures with numerous parameters, requiring large amounts of data and costly\ntraining. In low-resource situations with fewer labeled samples, deep learning\nmodels perform poorly due to insufficient data. Conversely, compressors are\ndata-type agnostic, and non-parametric methods do not bring underlying\nassumptions. Inspired by this insight, we propose a non-training alternative to\ndeep learning models, aiming to address these situations. Specifically, the\nSymbolic Representation Module is proposed to convert the reflectivity into\nsymbolic representations. The symbolic representations are then\ncross-transformed in both the channel and time dimensions to generate symbolic\nembeddings. Next, the Multi-scale Normalised Compression Distance (MNCD) is\ndesigned to measure the correlation between any two symbolic embeddings.\nFinally, based on the MNCDs, high quality crop classification can be achieved\nusing only a k-nearest-neighbor classifier kNN. The entire framework is\nready-to-use and lightweight. Without any training, it outperformed, on\naverage, 7 advanced deep learning models trained at scale on three benchmark\ndatasets. It also outperforms more than half of these models in the few-shot\nsetting with sparse crop labels. Therefore, the high performance and robustness\nof our non-training framework makes it truly applicable to real-world crop\nmapping. Codes are available at:\nhttps://github.com/qinfengsama/Compressor-Based-Crop-Mapping.\n", "link": "http://arxiv.org/abs/2405.18119v2", "date": "2024-07-05", "relevancy": 1.9917, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5129}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4885}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low-Resource%20Crop%20Classification%20from%20Multi-Spectral%20Time%20Series%20Using%0A%20%20Lossless%20Compressors&body=Title%3A%20Low-Resource%20Crop%20Classification%20from%20Multi-Spectral%20Time%20Series%20Using%0A%20%20Lossless%20Compressors%0AAuthor%3A%20Wei%20Cheng%20and%20Hongrui%20Ye%20and%20Xiao%20Wen%20and%20Jiachen%20Zhang%20and%20Jiping%20Xu%20and%20Feifan%20Zhang%0AAbstract%3A%20%20%20Deep%20learning%20has%20significantly%20improved%20the%20accuracy%20of%20crop%20classification%0Ausing%20multispectral%20temporal%20data.%20However%2C%20these%20models%20have%20complex%0Astructures%20with%20numerous%20parameters%2C%20requiring%20large%20amounts%20of%20data%20and%20costly%0Atraining.%20In%20low-resource%20situations%20with%20fewer%20labeled%20samples%2C%20deep%20learning%0Amodels%20perform%20poorly%20due%20to%20insufficient%20data.%20Conversely%2C%20compressors%20are%0Adata-type%20agnostic%2C%20and%20non-parametric%20methods%20do%20not%20bring%20underlying%0Aassumptions.%20Inspired%20by%20this%20insight%2C%20we%20propose%20a%20non-training%20alternative%20to%0Adeep%20learning%20models%2C%20aiming%20to%20address%20these%20situations.%20Specifically%2C%20the%0ASymbolic%20Representation%20Module%20is%20proposed%20to%20convert%20the%20reflectivity%20into%0Asymbolic%20representations.%20The%20symbolic%20representations%20are%20then%0Across-transformed%20in%20both%20the%20channel%20and%20time%20dimensions%20to%20generate%20symbolic%0Aembeddings.%20Next%2C%20the%20Multi-scale%20Normalised%20Compression%20Distance%20%28MNCD%29%20is%0Adesigned%20to%20measure%20the%20correlation%20between%20any%20two%20symbolic%20embeddings.%0AFinally%2C%20based%20on%20the%20MNCDs%2C%20high%20quality%20crop%20classification%20can%20be%20achieved%0Ausing%20only%20a%20k-nearest-neighbor%20classifier%20kNN.%20The%20entire%20framework%20is%0Aready-to-use%20and%20lightweight.%20Without%20any%20training%2C%20it%20outperformed%2C%20on%0Aaverage%2C%207%20advanced%20deep%20learning%20models%20trained%20at%20scale%20on%20three%20benchmark%0Adatasets.%20It%20also%20outperforms%20more%20than%20half%20of%20these%20models%20in%20the%20few-shot%0Asetting%20with%20sparse%20crop%20labels.%20Therefore%2C%20the%20high%20performance%20and%20robustness%0Aof%20our%20non-training%20framework%20makes%20it%20truly%20applicable%20to%20real-world%20crop%0Amapping.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/qinfengsama/Compressor-Based-Crop-Mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18119v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow-Resource%2520Crop%2520Classification%2520from%2520Multi-Spectral%2520Time%2520Series%2520Using%250A%2520%2520Lossless%2520Compressors%26entry.906535625%3DWei%2520Cheng%2520and%2520Hongrui%2520Ye%2520and%2520Xiao%2520Wen%2520and%2520Jiachen%2520Zhang%2520and%2520Jiping%2520Xu%2520and%2520Feifan%2520Zhang%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520significantly%2520improved%2520the%2520accuracy%2520of%2520crop%2520classification%250Ausing%2520multispectral%2520temporal%2520data.%2520However%252C%2520these%2520models%2520have%2520complex%250Astructures%2520with%2520numerous%2520parameters%252C%2520requiring%2520large%2520amounts%2520of%2520data%2520and%2520costly%250Atraining.%2520In%2520low-resource%2520situations%2520with%2520fewer%2520labeled%2520samples%252C%2520deep%2520learning%250Amodels%2520perform%2520poorly%2520due%2520to%2520insufficient%2520data.%2520Conversely%252C%2520compressors%2520are%250Adata-type%2520agnostic%252C%2520and%2520non-parametric%2520methods%2520do%2520not%2520bring%2520underlying%250Aassumptions.%2520Inspired%2520by%2520this%2520insight%252C%2520we%2520propose%2520a%2520non-training%2520alternative%2520to%250Adeep%2520learning%2520models%252C%2520aiming%2520to%2520address%2520these%2520situations.%2520Specifically%252C%2520the%250ASymbolic%2520Representation%2520Module%2520is%2520proposed%2520to%2520convert%2520the%2520reflectivity%2520into%250Asymbolic%2520representations.%2520The%2520symbolic%2520representations%2520are%2520then%250Across-transformed%2520in%2520both%2520the%2520channel%2520and%2520time%2520dimensions%2520to%2520generate%2520symbolic%250Aembeddings.%2520Next%252C%2520the%2520Multi-scale%2520Normalised%2520Compression%2520Distance%2520%2528MNCD%2529%2520is%250Adesigned%2520to%2520measure%2520the%2520correlation%2520between%2520any%2520two%2520symbolic%2520embeddings.%250AFinally%252C%2520based%2520on%2520the%2520MNCDs%252C%2520high%2520quality%2520crop%2520classification%2520can%2520be%2520achieved%250Ausing%2520only%2520a%2520k-nearest-neighbor%2520classifier%2520kNN.%2520The%2520entire%2520framework%2520is%250Aready-to-use%2520and%2520lightweight.%2520Without%2520any%2520training%252C%2520it%2520outperformed%252C%2520on%250Aaverage%252C%25207%2520advanced%2520deep%2520learning%2520models%2520trained%2520at%2520scale%2520on%2520three%2520benchmark%250Adatasets.%2520It%2520also%2520outperforms%2520more%2520than%2520half%2520of%2520these%2520models%2520in%2520the%2520few-shot%250Asetting%2520with%2520sparse%2520crop%2520labels.%2520Therefore%252C%2520the%2520high%2520performance%2520and%2520robustness%250Aof%2520our%2520non-training%2520framework%2520makes%2520it%2520truly%2520applicable%2520to%2520real-world%2520crop%250Amapping.%2520Codes%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/qinfengsama/Compressor-Based-Crop-Mapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18119v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-Resource%20Crop%20Classification%20from%20Multi-Spectral%20Time%20Series%20Using%0A%20%20Lossless%20Compressors&entry.906535625=Wei%20Cheng%20and%20Hongrui%20Ye%20and%20Xiao%20Wen%20and%20Jiachen%20Zhang%20and%20Jiping%20Xu%20and%20Feifan%20Zhang&entry.1292438233=%20%20Deep%20learning%20has%20significantly%20improved%20the%20accuracy%20of%20crop%20classification%0Ausing%20multispectral%20temporal%20data.%20However%2C%20these%20models%20have%20complex%0Astructures%20with%20numerous%20parameters%2C%20requiring%20large%20amounts%20of%20data%20and%20costly%0Atraining.%20In%20low-resource%20situations%20with%20fewer%20labeled%20samples%2C%20deep%20learning%0Amodels%20perform%20poorly%20due%20to%20insufficient%20data.%20Conversely%2C%20compressors%20are%0Adata-type%20agnostic%2C%20and%20non-parametric%20methods%20do%20not%20bring%20underlying%0Aassumptions.%20Inspired%20by%20this%20insight%2C%20we%20propose%20a%20non-training%20alternative%20to%0Adeep%20learning%20models%2C%20aiming%20to%20address%20these%20situations.%20Specifically%2C%20the%0ASymbolic%20Representation%20Module%20is%20proposed%20to%20convert%20the%20reflectivity%20into%0Asymbolic%20representations.%20The%20symbolic%20representations%20are%20then%0Across-transformed%20in%20both%20the%20channel%20and%20time%20dimensions%20to%20generate%20symbolic%0Aembeddings.%20Next%2C%20the%20Multi-scale%20Normalised%20Compression%20Distance%20%28MNCD%29%20is%0Adesigned%20to%20measure%20the%20correlation%20between%20any%20two%20symbolic%20embeddings.%0AFinally%2C%20based%20on%20the%20MNCDs%2C%20high%20quality%20crop%20classification%20can%20be%20achieved%0Ausing%20only%20a%20k-nearest-neighbor%20classifier%20kNN.%20The%20entire%20framework%20is%0Aready-to-use%20and%20lightweight.%20Without%20any%20training%2C%20it%20outperformed%2C%20on%0Aaverage%2C%207%20advanced%20deep%20learning%20models%20trained%20at%20scale%20on%20three%20benchmark%0Adatasets.%20It%20also%20outperforms%20more%20than%20half%20of%20these%20models%20in%20the%20few-shot%0Asetting%20with%20sparse%20crop%20labels.%20Therefore%2C%20the%20high%20performance%20and%20robustness%0Aof%20our%20non-training%20framework%20makes%20it%20truly%20applicable%20to%20real-world%20crop%0Amapping.%20Codes%20are%20available%20at%3A%0Ahttps%3A//github.com/qinfengsama/Compressor-Based-Crop-Mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18119v2&entry.124074799=Read"},
{"title": "PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit\n  Posts", "author": "Ana-Cristina Rogoz and Maria Ilinca Nechita and Radu Tudor Ionescu", "abstract": "  We introduce PoPreRo, the first dataset for Popularity Prediction of Romanian\nposts collected from Reddit. The PoPreRo dataset includes a varied compilation\nof post samples from five distinct subreddits of Romania, totaling 28,107 data\nsamples. Along with our novel dataset, we introduce a set of competitive models\nto be used as baselines for future research. Interestingly, the top-scoring\nmodel achieves an accuracy of 61.35% and a macro F1 score of 60.60% on the test\nset, indicating that the popularity prediction task on PoPreRo is very\nchallenging. Further investigations based on few-shot prompting the Falcon-7B\nLarge Language Model also point in the same direction. We thus believe that\nPoPreRo is a valuable resource that can be used to evaluate models on\npredicting the popularity of social media posts in Romanian. We release our\ndataset at https://github.com/ana-rogoz/PoPreRo.\n", "link": "http://arxiv.org/abs/2407.04541v1", "date": "2024-07-05", "relevancy": 1.9719, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4061}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3985}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3785}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoPreRo%3A%20A%20New%20Dataset%20for%20Popularity%20Prediction%20of%20Romanian%20Reddit%0A%20%20Posts&body=Title%3A%20PoPreRo%3A%20A%20New%20Dataset%20for%20Popularity%20Prediction%20of%20Romanian%20Reddit%0A%20%20Posts%0AAuthor%3A%20Ana-Cristina%20Rogoz%20and%20Maria%20Ilinca%20Nechita%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20%20%20We%20introduce%20PoPreRo%2C%20the%20first%20dataset%20for%20Popularity%20Prediction%20of%20Romanian%0Aposts%20collected%20from%20Reddit.%20The%20PoPreRo%20dataset%20includes%20a%20varied%20compilation%0Aof%20post%20samples%20from%20five%20distinct%20subreddits%20of%20Romania%2C%20totaling%2028%2C107%20data%0Asamples.%20Along%20with%20our%20novel%20dataset%2C%20we%20introduce%20a%20set%20of%20competitive%20models%0Ato%20be%20used%20as%20baselines%20for%20future%20research.%20Interestingly%2C%20the%20top-scoring%0Amodel%20achieves%20an%20accuracy%20of%2061.35%25%20and%20a%20macro%20F1%20score%20of%2060.60%25%20on%20the%20test%0Aset%2C%20indicating%20that%20the%20popularity%20prediction%20task%20on%20PoPreRo%20is%20very%0Achallenging.%20Further%20investigations%20based%20on%20few-shot%20prompting%20the%20Falcon-7B%0ALarge%20Language%20Model%20also%20point%20in%20the%20same%20direction.%20We%20thus%20believe%20that%0APoPreRo%20is%20a%20valuable%20resource%20that%20can%20be%20used%20to%20evaluate%20models%20on%0Apredicting%20the%20popularity%20of%20social%20media%20posts%20in%20Romanian.%20We%20release%20our%0Adataset%20at%20https%3A//github.com/ana-rogoz/PoPreRo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoPreRo%253A%2520A%2520New%2520Dataset%2520for%2520Popularity%2520Prediction%2520of%2520Romanian%2520Reddit%250A%2520%2520Posts%26entry.906535625%3DAna-Cristina%2520Rogoz%2520and%2520Maria%2520Ilinca%2520Nechita%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3D%2520%2520We%2520introduce%2520PoPreRo%252C%2520the%2520first%2520dataset%2520for%2520Popularity%2520Prediction%2520of%2520Romanian%250Aposts%2520collected%2520from%2520Reddit.%2520The%2520PoPreRo%2520dataset%2520includes%2520a%2520varied%2520compilation%250Aof%2520post%2520samples%2520from%2520five%2520distinct%2520subreddits%2520of%2520Romania%252C%2520totaling%252028%252C107%2520data%250Asamples.%2520Along%2520with%2520our%2520novel%2520dataset%252C%2520we%2520introduce%2520a%2520set%2520of%2520competitive%2520models%250Ato%2520be%2520used%2520as%2520baselines%2520for%2520future%2520research.%2520Interestingly%252C%2520the%2520top-scoring%250Amodel%2520achieves%2520an%2520accuracy%2520of%252061.35%2525%2520and%2520a%2520macro%2520F1%2520score%2520of%252060.60%2525%2520on%2520the%2520test%250Aset%252C%2520indicating%2520that%2520the%2520popularity%2520prediction%2520task%2520on%2520PoPreRo%2520is%2520very%250Achallenging.%2520Further%2520investigations%2520based%2520on%2520few-shot%2520prompting%2520the%2520Falcon-7B%250ALarge%2520Language%2520Model%2520also%2520point%2520in%2520the%2520same%2520direction.%2520We%2520thus%2520believe%2520that%250APoPreRo%2520is%2520a%2520valuable%2520resource%2520that%2520can%2520be%2520used%2520to%2520evaluate%2520models%2520on%250Apredicting%2520the%2520popularity%2520of%2520social%2520media%2520posts%2520in%2520Romanian.%2520We%2520release%2520our%250Adataset%2520at%2520https%253A//github.com/ana-rogoz/PoPreRo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoPreRo%3A%20A%20New%20Dataset%20for%20Popularity%20Prediction%20of%20Romanian%20Reddit%0A%20%20Posts&entry.906535625=Ana-Cristina%20Rogoz%20and%20Maria%20Ilinca%20Nechita%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=%20%20We%20introduce%20PoPreRo%2C%20the%20first%20dataset%20for%20Popularity%20Prediction%20of%20Romanian%0Aposts%20collected%20from%20Reddit.%20The%20PoPreRo%20dataset%20includes%20a%20varied%20compilation%0Aof%20post%20samples%20from%20five%20distinct%20subreddits%20of%20Romania%2C%20totaling%2028%2C107%20data%0Asamples.%20Along%20with%20our%20novel%20dataset%2C%20we%20introduce%20a%20set%20of%20competitive%20models%0Ato%20be%20used%20as%20baselines%20for%20future%20research.%20Interestingly%2C%20the%20top-scoring%0Amodel%20achieves%20an%20accuracy%20of%2061.35%25%20and%20a%20macro%20F1%20score%20of%2060.60%25%20on%20the%20test%0Aset%2C%20indicating%20that%20the%20popularity%20prediction%20task%20on%20PoPreRo%20is%20very%0Achallenging.%20Further%20investigations%20based%20on%20few-shot%20prompting%20the%20Falcon-7B%0ALarge%20Language%20Model%20also%20point%20in%20the%20same%20direction.%20We%20thus%20believe%20that%0APoPreRo%20is%20a%20valuable%20resource%20that%20can%20be%20used%20to%20evaluate%20models%20on%0Apredicting%20the%20popularity%20of%20social%20media%20posts%20in%20Romanian.%20We%20release%20our%0Adataset%20at%20https%3A//github.com/ana-rogoz/PoPreRo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04541v1&entry.124074799=Read"},
{"title": "Randomized Physics-Informed Neural Networks for Bayesian Data\n  Assimilation", "author": "Yifei Zong and David Barajas-Solano and Alexandre M. Tartakovsky", "abstract": "  We propose a randomized physics-informed neural network (PINN) or rPINN\nmethod for uncertainty quantification in inverse partial differential equation\n(PDE) problems with noisy data. This method is used to quantify uncertainty in\nthe inverse PDE PINN solutions. Recently, the Bayesian PINN (BPINN) method was\nproposed, where the posterior distribution of the PINN parameters was\nformulated using the Bayes' theorem and sampled using approximate inference\nmethods such as the Hamiltonian Monte Carlo (HMC) and variational inference\n(VI) methods. In this work, we demonstrate that HMC fails to converge for\nnon-linear inverse PDE problems. As an alternative to HMC, we sample the\ndistribution by solving the stochastic optimization problem obtained by\nrandomizing the PINN loss function. The effectiveness of the rPINN method is\ntested for linear and non-linear Poisson equations, and the diffusion equation\nwith a high-dimensional space-dependent diffusion coefficient. The rPINN method\nprovides informative distributions for all considered problems. For the linear\nPoisson equation, HMC and rPINN produce similar distributions, but rPINN is on\naverage 27 times faster than HMC. For the non-linear Poison and diffusion\nequations, the HMC method fails to converge because a single HMC chain cannot\nsample multiple modes of the posterior distribution of the PINN parameters in a\nreasonable amount of time.\n", "link": "http://arxiv.org/abs/2407.04617v1", "date": "2024-07-05", "relevancy": 1.9714, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5366}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5148}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Randomized%20Physics-Informed%20Neural%20Networks%20for%20Bayesian%20Data%0A%20%20Assimilation&body=Title%3A%20Randomized%20Physics-Informed%20Neural%20Networks%20for%20Bayesian%20Data%0A%20%20Assimilation%0AAuthor%3A%20Yifei%20Zong%20and%20David%20Barajas-Solano%20and%20Alexandre%20M.%20Tartakovsky%0AAbstract%3A%20%20%20We%20propose%20a%20randomized%20physics-informed%20neural%20network%20%28PINN%29%20or%20rPINN%0Amethod%20for%20uncertainty%20quantification%20in%20inverse%20partial%20differential%20equation%0A%28PDE%29%20problems%20with%20noisy%20data.%20This%20method%20is%20used%20to%20quantify%20uncertainty%20in%0Athe%20inverse%20PDE%20PINN%20solutions.%20Recently%2C%20the%20Bayesian%20PINN%20%28BPINN%29%20method%20was%0Aproposed%2C%20where%20the%20posterior%20distribution%20of%20the%20PINN%20parameters%20was%0Aformulated%20using%20the%20Bayes%27%20theorem%20and%20sampled%20using%20approximate%20inference%0Amethods%20such%20as%20the%20Hamiltonian%20Monte%20Carlo%20%28HMC%29%20and%20variational%20inference%0A%28VI%29%20methods.%20In%20this%20work%2C%20we%20demonstrate%20that%20HMC%20fails%20to%20converge%20for%0Anon-linear%20inverse%20PDE%20problems.%20As%20an%20alternative%20to%20HMC%2C%20we%20sample%20the%0Adistribution%20by%20solving%20the%20stochastic%20optimization%20problem%20obtained%20by%0Arandomizing%20the%20PINN%20loss%20function.%20The%20effectiveness%20of%20the%20rPINN%20method%20is%0Atested%20for%20linear%20and%20non-linear%20Poisson%20equations%2C%20and%20the%20diffusion%20equation%0Awith%20a%20high-dimensional%20space-dependent%20diffusion%20coefficient.%20The%20rPINN%20method%0Aprovides%20informative%20distributions%20for%20all%20considered%20problems.%20For%20the%20linear%0APoisson%20equation%2C%20HMC%20and%20rPINN%20produce%20similar%20distributions%2C%20but%20rPINN%20is%20on%0Aaverage%2027%20times%20faster%20than%20HMC.%20For%20the%20non-linear%20Poison%20and%20diffusion%0Aequations%2C%20the%20HMC%20method%20fails%20to%20converge%20because%20a%20single%20HMC%20chain%20cannot%0Asample%20multiple%20modes%20of%20the%20posterior%20distribution%20of%20the%20PINN%20parameters%20in%20a%0Areasonable%20amount%20of%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandomized%2520Physics-Informed%2520Neural%2520Networks%2520for%2520Bayesian%2520Data%250A%2520%2520Assimilation%26entry.906535625%3DYifei%2520Zong%2520and%2520David%2520Barajas-Solano%2520and%2520Alexandre%2520M.%2520Tartakovsky%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520randomized%2520physics-informed%2520neural%2520network%2520%2528PINN%2529%2520or%2520rPINN%250Amethod%2520for%2520uncertainty%2520quantification%2520in%2520inverse%2520partial%2520differential%2520equation%250A%2528PDE%2529%2520problems%2520with%2520noisy%2520data.%2520This%2520method%2520is%2520used%2520to%2520quantify%2520uncertainty%2520in%250Athe%2520inverse%2520PDE%2520PINN%2520solutions.%2520Recently%252C%2520the%2520Bayesian%2520PINN%2520%2528BPINN%2529%2520method%2520was%250Aproposed%252C%2520where%2520the%2520posterior%2520distribution%2520of%2520the%2520PINN%2520parameters%2520was%250Aformulated%2520using%2520the%2520Bayes%2527%2520theorem%2520and%2520sampled%2520using%2520approximate%2520inference%250Amethods%2520such%2520as%2520the%2520Hamiltonian%2520Monte%2520Carlo%2520%2528HMC%2529%2520and%2520variational%2520inference%250A%2528VI%2529%2520methods.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520HMC%2520fails%2520to%2520converge%2520for%250Anon-linear%2520inverse%2520PDE%2520problems.%2520As%2520an%2520alternative%2520to%2520HMC%252C%2520we%2520sample%2520the%250Adistribution%2520by%2520solving%2520the%2520stochastic%2520optimization%2520problem%2520obtained%2520by%250Arandomizing%2520the%2520PINN%2520loss%2520function.%2520The%2520effectiveness%2520of%2520the%2520rPINN%2520method%2520is%250Atested%2520for%2520linear%2520and%2520non-linear%2520Poisson%2520equations%252C%2520and%2520the%2520diffusion%2520equation%250Awith%2520a%2520high-dimensional%2520space-dependent%2520diffusion%2520coefficient.%2520The%2520rPINN%2520method%250Aprovides%2520informative%2520distributions%2520for%2520all%2520considered%2520problems.%2520For%2520the%2520linear%250APoisson%2520equation%252C%2520HMC%2520and%2520rPINN%2520produce%2520similar%2520distributions%252C%2520but%2520rPINN%2520is%2520on%250Aaverage%252027%2520times%2520faster%2520than%2520HMC.%2520For%2520the%2520non-linear%2520Poison%2520and%2520diffusion%250Aequations%252C%2520the%2520HMC%2520method%2520fails%2520to%2520converge%2520because%2520a%2520single%2520HMC%2520chain%2520cannot%250Asample%2520multiple%2520modes%2520of%2520the%2520posterior%2520distribution%2520of%2520the%2520PINN%2520parameters%2520in%2520a%250Areasonable%2520amount%2520of%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Randomized%20Physics-Informed%20Neural%20Networks%20for%20Bayesian%20Data%0A%20%20Assimilation&entry.906535625=Yifei%20Zong%20and%20David%20Barajas-Solano%20and%20Alexandre%20M.%20Tartakovsky&entry.1292438233=%20%20We%20propose%20a%20randomized%20physics-informed%20neural%20network%20%28PINN%29%20or%20rPINN%0Amethod%20for%20uncertainty%20quantification%20in%20inverse%20partial%20differential%20equation%0A%28PDE%29%20problems%20with%20noisy%20data.%20This%20method%20is%20used%20to%20quantify%20uncertainty%20in%0Athe%20inverse%20PDE%20PINN%20solutions.%20Recently%2C%20the%20Bayesian%20PINN%20%28BPINN%29%20method%20was%0Aproposed%2C%20where%20the%20posterior%20distribution%20of%20the%20PINN%20parameters%20was%0Aformulated%20using%20the%20Bayes%27%20theorem%20and%20sampled%20using%20approximate%20inference%0Amethods%20such%20as%20the%20Hamiltonian%20Monte%20Carlo%20%28HMC%29%20and%20variational%20inference%0A%28VI%29%20methods.%20In%20this%20work%2C%20we%20demonstrate%20that%20HMC%20fails%20to%20converge%20for%0Anon-linear%20inverse%20PDE%20problems.%20As%20an%20alternative%20to%20HMC%2C%20we%20sample%20the%0Adistribution%20by%20solving%20the%20stochastic%20optimization%20problem%20obtained%20by%0Arandomizing%20the%20PINN%20loss%20function.%20The%20effectiveness%20of%20the%20rPINN%20method%20is%0Atested%20for%20linear%20and%20non-linear%20Poisson%20equations%2C%20and%20the%20diffusion%20equation%0Awith%20a%20high-dimensional%20space-dependent%20diffusion%20coefficient.%20The%20rPINN%20method%0Aprovides%20informative%20distributions%20for%20all%20considered%20problems.%20For%20the%20linear%0APoisson%20equation%2C%20HMC%20and%20rPINN%20produce%20similar%20distributions%2C%20but%20rPINN%20is%20on%0Aaverage%2027%20times%20faster%20than%20HMC.%20For%20the%20non-linear%20Poison%20and%20diffusion%0Aequations%2C%20the%20HMC%20method%20fails%20to%20converge%20because%20a%20single%20HMC%20chain%20cannot%0Asample%20multiple%20modes%20of%20the%20posterior%20distribution%20of%20the%20PINN%20parameters%20in%20a%0Areasonable%20amount%20of%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04617v1&entry.124074799=Read"},
{"title": "Research on target detection method of distracted driving behavior based\n  on improved YOLOv8", "author": "Shiquan Shen and Zhizhong Wu and Pan Zhang", "abstract": "  With the development of deep learning technology, the detection and\nclassification of distracted driving behaviour requires higher accuracy.\nExisting deep learning-based methods are computationally intensive and\nparameter redundant, limiting the efficiency and accuracy in practical\napplications. To solve this problem, this study proposes an improved YOLOv8\ndetection method based on the original YOLOv8 model by integrating the BoTNet\nmodule, GAM attention mechanism and EIoU loss function. By optimising the\nfeature extraction and multi-scale feature fusion strategies, the training and\ninference processes are simplified, and the detection accuracy and efficiency\nare significantly improved. Experimental results show that the improved model\nperforms well in both detection speed and accuracy, with an accuracy rate of\n99.4%, and the model is smaller and easy to deploy, which is able to identify\nand classify distracted driving behaviours in real time, provide timely\nwarnings, and enhance driving safety.\n", "link": "http://arxiv.org/abs/2407.01864v2", "date": "2024-07-05", "relevancy": 1.9684, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4975}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4894}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20target%20detection%20method%20of%20distracted%20driving%20behavior%20based%0A%20%20on%20improved%20YOLOv8&body=Title%3A%20Research%20on%20target%20detection%20method%20of%20distracted%20driving%20behavior%20based%0A%20%20on%20improved%20YOLOv8%0AAuthor%3A%20Shiquan%20Shen%20and%20Zhizhong%20Wu%20and%20Pan%20Zhang%0AAbstract%3A%20%20%20With%20the%20development%20of%20deep%20learning%20technology%2C%20the%20detection%20and%0Aclassification%20of%20distracted%20driving%20behaviour%20requires%20higher%20accuracy.%0AExisting%20deep%20learning-based%20methods%20are%20computationally%20intensive%20and%0Aparameter%20redundant%2C%20limiting%20the%20efficiency%20and%20accuracy%20in%20practical%0Aapplications.%20To%20solve%20this%20problem%2C%20this%20study%20proposes%20an%20improved%20YOLOv8%0Adetection%20method%20based%20on%20the%20original%20YOLOv8%20model%20by%20integrating%20the%20BoTNet%0Amodule%2C%20GAM%20attention%20mechanism%20and%20EIoU%20loss%20function.%20By%20optimising%20the%0Afeature%20extraction%20and%20multi-scale%20feature%20fusion%20strategies%2C%20the%20training%20and%0Ainference%20processes%20are%20simplified%2C%20and%20the%20detection%20accuracy%20and%20efficiency%0Aare%20significantly%20improved.%20Experimental%20results%20show%20that%20the%20improved%20model%0Aperforms%20well%20in%20both%20detection%20speed%20and%20accuracy%2C%20with%20an%20accuracy%20rate%20of%0A99.4%25%2C%20and%20the%20model%20is%20smaller%20and%20easy%20to%20deploy%2C%20which%20is%20able%20to%20identify%0Aand%20classify%20distracted%20driving%20behaviours%20in%20real%20time%2C%20provide%20timely%0Awarnings%2C%20and%20enhance%20driving%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01864v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520target%2520detection%2520method%2520of%2520distracted%2520driving%2520behavior%2520based%250A%2520%2520on%2520improved%2520YOLOv8%26entry.906535625%3DShiquan%2520Shen%2520and%2520Zhizhong%2520Wu%2520and%2520Pan%2520Zhang%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520deep%2520learning%2520technology%252C%2520the%2520detection%2520and%250Aclassification%2520of%2520distracted%2520driving%2520behaviour%2520requires%2520higher%2520accuracy.%250AExisting%2520deep%2520learning-based%2520methods%2520are%2520computationally%2520intensive%2520and%250Aparameter%2520redundant%252C%2520limiting%2520the%2520efficiency%2520and%2520accuracy%2520in%2520practical%250Aapplications.%2520To%2520solve%2520this%2520problem%252C%2520this%2520study%2520proposes%2520an%2520improved%2520YOLOv8%250Adetection%2520method%2520based%2520on%2520the%2520original%2520YOLOv8%2520model%2520by%2520integrating%2520the%2520BoTNet%250Amodule%252C%2520GAM%2520attention%2520mechanism%2520and%2520EIoU%2520loss%2520function.%2520By%2520optimising%2520the%250Afeature%2520extraction%2520and%2520multi-scale%2520feature%2520fusion%2520strategies%252C%2520the%2520training%2520and%250Ainference%2520processes%2520are%2520simplified%252C%2520and%2520the%2520detection%2520accuracy%2520and%2520efficiency%250Aare%2520significantly%2520improved.%2520Experimental%2520results%2520show%2520that%2520the%2520improved%2520model%250Aperforms%2520well%2520in%2520both%2520detection%2520speed%2520and%2520accuracy%252C%2520with%2520an%2520accuracy%2520rate%2520of%250A99.4%2525%252C%2520and%2520the%2520model%2520is%2520smaller%2520and%2520easy%2520to%2520deploy%252C%2520which%2520is%2520able%2520to%2520identify%250Aand%2520classify%2520distracted%2520driving%2520behaviours%2520in%2520real%2520time%252C%2520provide%2520timely%250Awarnings%252C%2520and%2520enhance%2520driving%2520safety.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01864v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20target%20detection%20method%20of%20distracted%20driving%20behavior%20based%0A%20%20on%20improved%20YOLOv8&entry.906535625=Shiquan%20Shen%20and%20Zhizhong%20Wu%20and%20Pan%20Zhang&entry.1292438233=%20%20With%20the%20development%20of%20deep%20learning%20technology%2C%20the%20detection%20and%0Aclassification%20of%20distracted%20driving%20behaviour%20requires%20higher%20accuracy.%0AExisting%20deep%20learning-based%20methods%20are%20computationally%20intensive%20and%0Aparameter%20redundant%2C%20limiting%20the%20efficiency%20and%20accuracy%20in%20practical%0Aapplications.%20To%20solve%20this%20problem%2C%20this%20study%20proposes%20an%20improved%20YOLOv8%0Adetection%20method%20based%20on%20the%20original%20YOLOv8%20model%20by%20integrating%20the%20BoTNet%0Amodule%2C%20GAM%20attention%20mechanism%20and%20EIoU%20loss%20function.%20By%20optimising%20the%0Afeature%20extraction%20and%20multi-scale%20feature%20fusion%20strategies%2C%20the%20training%20and%0Ainference%20processes%20are%20simplified%2C%20and%20the%20detection%20accuracy%20and%20efficiency%0Aare%20significantly%20improved.%20Experimental%20results%20show%20that%20the%20improved%20model%0Aperforms%20well%20in%20both%20detection%20speed%20and%20accuracy%2C%20with%20an%20accuracy%20rate%20of%0A99.4%25%2C%20and%20the%20model%20is%20smaller%20and%20easy%20to%20deploy%2C%20which%20is%20able%20to%20identify%0Aand%20classify%20distracted%20driving%20behaviours%20in%20real%20time%2C%20provide%20timely%0Awarnings%2C%20and%20enhance%20driving%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01864v2&entry.124074799=Read"},
{"title": "Doubly Robust Causal Effect Estimation under Networked Interference via\n  Targeted Learning", "author": "Weilin Chen and Ruichu Cai and Zeqin Yang and Jie Qiao and Yuguang Yan and Zijian Li and Zhifeng Hao", "abstract": "  Causal effect estimation under networked interference is an important but\nchallenging problem. Available parametric methods are limited in their model\nspace, while previous semiparametric methods, e.g., leveraging neural networks\nto fit only one single nuisance function, may still encounter misspecification\nproblems under networked interference without appropriate assumptions on the\ndata generation process. To mitigate bias stemming from misspecification, we\npropose a novel doubly robust causal effect estimator under networked\ninterference, by adapting the targeted learning technique to the training of\nneural networks. Specifically, we generalize the targeted learning technique\ninto the networked interference setting and establish the condition under which\nan estimator achieves double robustness. Based on the condition, we devise an\nend-to-end causal effect estimator by transforming the identified theoretical\ncondition into a targeted loss. Moreover, we provide a theoretical analysis of\nour designed estimator, revealing a faster convergence rate compared to a\nsingle nuisance model. Extensive experimental results on two real-world\nnetworks with semisynthetic data demonstrate the effectiveness of our proposed\nestimators.\n", "link": "http://arxiv.org/abs/2405.03342v3", "date": "2024-07-05", "relevancy": 1.9597, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5245}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Doubly%20Robust%20Causal%20Effect%20Estimation%20under%20Networked%20Interference%20via%0A%20%20Targeted%20Learning&body=Title%3A%20Doubly%20Robust%20Causal%20Effect%20Estimation%20under%20Networked%20Interference%20via%0A%20%20Targeted%20Learning%0AAuthor%3A%20Weilin%20Chen%20and%20Ruichu%20Cai%20and%20Zeqin%20Yang%20and%20Jie%20Qiao%20and%20Yuguang%20Yan%20and%20Zijian%20Li%20and%20Zhifeng%20Hao%0AAbstract%3A%20%20%20Causal%20effect%20estimation%20under%20networked%20interference%20is%20an%20important%20but%0Achallenging%20problem.%20Available%20parametric%20methods%20are%20limited%20in%20their%20model%0Aspace%2C%20while%20previous%20semiparametric%20methods%2C%20e.g.%2C%20leveraging%20neural%20networks%0Ato%20fit%20only%20one%20single%20nuisance%20function%2C%20may%20still%20encounter%20misspecification%0Aproblems%20under%20networked%20interference%20without%20appropriate%20assumptions%20on%20the%0Adata%20generation%20process.%20To%20mitigate%20bias%20stemming%20from%20misspecification%2C%20we%0Apropose%20a%20novel%20doubly%20robust%20causal%20effect%20estimator%20under%20networked%0Ainterference%2C%20by%20adapting%20the%20targeted%20learning%20technique%20to%20the%20training%20of%0Aneural%20networks.%20Specifically%2C%20we%20generalize%20the%20targeted%20learning%20technique%0Ainto%20the%20networked%20interference%20setting%20and%20establish%20the%20condition%20under%20which%0Aan%20estimator%20achieves%20double%20robustness.%20Based%20on%20the%20condition%2C%20we%20devise%20an%0Aend-to-end%20causal%20effect%20estimator%20by%20transforming%20the%20identified%20theoretical%0Acondition%20into%20a%20targeted%20loss.%20Moreover%2C%20we%20provide%20a%20theoretical%20analysis%20of%0Aour%20designed%20estimator%2C%20revealing%20a%20faster%20convergence%20rate%20compared%20to%20a%0Asingle%20nuisance%20model.%20Extensive%20experimental%20results%20on%20two%20real-world%0Anetworks%20with%20semisynthetic%20data%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Aestimators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03342v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoubly%2520Robust%2520Causal%2520Effect%2520Estimation%2520under%2520Networked%2520Interference%2520via%250A%2520%2520Targeted%2520Learning%26entry.906535625%3DWeilin%2520Chen%2520and%2520Ruichu%2520Cai%2520and%2520Zeqin%2520Yang%2520and%2520Jie%2520Qiao%2520and%2520Yuguang%2520Yan%2520and%2520Zijian%2520Li%2520and%2520Zhifeng%2520Hao%26entry.1292438233%3D%2520%2520Causal%2520effect%2520estimation%2520under%2520networked%2520interference%2520is%2520an%2520important%2520but%250Achallenging%2520problem.%2520Available%2520parametric%2520methods%2520are%2520limited%2520in%2520their%2520model%250Aspace%252C%2520while%2520previous%2520semiparametric%2520methods%252C%2520e.g.%252C%2520leveraging%2520neural%2520networks%250Ato%2520fit%2520only%2520one%2520single%2520nuisance%2520function%252C%2520may%2520still%2520encounter%2520misspecification%250Aproblems%2520under%2520networked%2520interference%2520without%2520appropriate%2520assumptions%2520on%2520the%250Adata%2520generation%2520process.%2520To%2520mitigate%2520bias%2520stemming%2520from%2520misspecification%252C%2520we%250Apropose%2520a%2520novel%2520doubly%2520robust%2520causal%2520effect%2520estimator%2520under%2520networked%250Ainterference%252C%2520by%2520adapting%2520the%2520targeted%2520learning%2520technique%2520to%2520the%2520training%2520of%250Aneural%2520networks.%2520Specifically%252C%2520we%2520generalize%2520the%2520targeted%2520learning%2520technique%250Ainto%2520the%2520networked%2520interference%2520setting%2520and%2520establish%2520the%2520condition%2520under%2520which%250Aan%2520estimator%2520achieves%2520double%2520robustness.%2520Based%2520on%2520the%2520condition%252C%2520we%2520devise%2520an%250Aend-to-end%2520causal%2520effect%2520estimator%2520by%2520transforming%2520the%2520identified%2520theoretical%250Acondition%2520into%2520a%2520targeted%2520loss.%2520Moreover%252C%2520we%2520provide%2520a%2520theoretical%2520analysis%2520of%250Aour%2520designed%2520estimator%252C%2520revealing%2520a%2520faster%2520convergence%2520rate%2520compared%2520to%2520a%250Asingle%2520nuisance%2520model.%2520Extensive%2520experimental%2520results%2520on%2520two%2520real-world%250Anetworks%2520with%2520semisynthetic%2520data%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Aestimators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03342v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Doubly%20Robust%20Causal%20Effect%20Estimation%20under%20Networked%20Interference%20via%0A%20%20Targeted%20Learning&entry.906535625=Weilin%20Chen%20and%20Ruichu%20Cai%20and%20Zeqin%20Yang%20and%20Jie%20Qiao%20and%20Yuguang%20Yan%20and%20Zijian%20Li%20and%20Zhifeng%20Hao&entry.1292438233=%20%20Causal%20effect%20estimation%20under%20networked%20interference%20is%20an%20important%20but%0Achallenging%20problem.%20Available%20parametric%20methods%20are%20limited%20in%20their%20model%0Aspace%2C%20while%20previous%20semiparametric%20methods%2C%20e.g.%2C%20leveraging%20neural%20networks%0Ato%20fit%20only%20one%20single%20nuisance%20function%2C%20may%20still%20encounter%20misspecification%0Aproblems%20under%20networked%20interference%20without%20appropriate%20assumptions%20on%20the%0Adata%20generation%20process.%20To%20mitigate%20bias%20stemming%20from%20misspecification%2C%20we%0Apropose%20a%20novel%20doubly%20robust%20causal%20effect%20estimator%20under%20networked%0Ainterference%2C%20by%20adapting%20the%20targeted%20learning%20technique%20to%20the%20training%20of%0Aneural%20networks.%20Specifically%2C%20we%20generalize%20the%20targeted%20learning%20technique%0Ainto%20the%20networked%20interference%20setting%20and%20establish%20the%20condition%20under%20which%0Aan%20estimator%20achieves%20double%20robustness.%20Based%20on%20the%20condition%2C%20we%20devise%20an%0Aend-to-end%20causal%20effect%20estimator%20by%20transforming%20the%20identified%20theoretical%0Acondition%20into%20a%20targeted%20loss.%20Moreover%2C%20we%20provide%20a%20theoretical%20analysis%20of%0Aour%20designed%20estimator%2C%20revealing%20a%20faster%20convergence%20rate%20compared%20to%20a%0Asingle%20nuisance%20model.%20Extensive%20experimental%20results%20on%20two%20real-world%0Anetworks%20with%20semisynthetic%20data%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Aestimators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03342v3&entry.124074799=Read"},
{"title": "Learning to (Learn at Test Time): RNNs with Expressive Hidden States", "author": "Yu Sun and Xinhao Li and Karan Dalal and Jiarui Xu and Arjun Vikram and Genghan Zhang and Yann Dubois and Xinlei Chen and Xiaolong Wang and Sanmi Koyejo and Tatsunori Hashimoto and Carlos Guestrin", "abstract": "  Self-attention performs well in long context but has quadratic complexity.\nExisting RNN layers have linear complexity, but their performance in long\ncontext is limited by the expressive power of their hidden state. We propose a\nnew class of sequence modeling layers with linear complexity and an expressive\nhidden state. The key idea is to make the hidden state a machine learning model\nitself, and the update rule a step of self-supervised learning. Since the\nhidden state is updated by training even on test sequences, our layers are\ncalled Test-Time Training (TTT) layers. We consider two instantiations:\nTTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer\nMLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B\nparameters, comparing with a strong Transformer and Mamba, a modern RNN. Both\nTTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer,\nthey can keep reducing perplexity by conditioning on more tokens, while Mamba\ncannot after 16k context. With preliminary systems optimization, TTT-Linear is\nalready faster than Transformer at 8k context and matches Mamba in wall-clock\ntime. TTT-MLP still faces challenges in memory I/O, but shows larger potential\nin long context, pointing to a promising direction for future research.\n", "link": "http://arxiv.org/abs/2407.04620v1", "date": "2024-07-05", "relevancy": 1.9298, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.493}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4825}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20%28Learn%20at%20Test%20Time%29%3A%20RNNs%20with%20Expressive%20Hidden%20States&body=Title%3A%20Learning%20to%20%28Learn%20at%20Test%20Time%29%3A%20RNNs%20with%20Expressive%20Hidden%20States%0AAuthor%3A%20Yu%20Sun%20and%20Xinhao%20Li%20and%20Karan%20Dalal%20and%20Jiarui%20Xu%20and%20Arjun%20Vikram%20and%20Genghan%20Zhang%20and%20Yann%20Dubois%20and%20Xinlei%20Chen%20and%20Xiaolong%20Wang%20and%20Sanmi%20Koyejo%20and%20Tatsunori%20Hashimoto%20and%20Carlos%20Guestrin%0AAbstract%3A%20%20%20Self-attention%20performs%20well%20in%20long%20context%20but%20has%20quadratic%20complexity.%0AExisting%20RNN%20layers%20have%20linear%20complexity%2C%20but%20their%20performance%20in%20long%0Acontext%20is%20limited%20by%20the%20expressive%20power%20of%20their%20hidden%20state.%20We%20propose%20a%0Anew%20class%20of%20sequence%20modeling%20layers%20with%20linear%20complexity%20and%20an%20expressive%0Ahidden%20state.%20The%20key%20idea%20is%20to%20make%20the%20hidden%20state%20a%20machine%20learning%20model%0Aitself%2C%20and%20the%20update%20rule%20a%20step%20of%20self-supervised%20learning.%20Since%20the%0Ahidden%20state%20is%20updated%20by%20training%20even%20on%20test%20sequences%2C%20our%20layers%20are%0Acalled%20Test-Time%20Training%20%28TTT%29%20layers.%20We%20consider%20two%20instantiations%3A%0ATTT-Linear%20and%20TTT-MLP%2C%20whose%20hidden%20state%20is%20a%20linear%20model%20and%20a%20two-layer%0AMLP%20respectively.%20We%20evaluate%20our%20instantiations%20at%20the%20scale%20of%20125M%20to%201.3B%0Aparameters%2C%20comparing%20with%20a%20strong%20Transformer%20and%20Mamba%2C%20a%20modern%20RNN.%20Both%0ATTT-Linear%20and%20TTT-MLP%20match%20or%20exceed%20the%20baselines.%20Similar%20to%20Transformer%2C%0Athey%20can%20keep%20reducing%20perplexity%20by%20conditioning%20on%20more%20tokens%2C%20while%20Mamba%0Acannot%20after%2016k%20context.%20With%20preliminary%20systems%20optimization%2C%20TTT-Linear%20is%0Aalready%20faster%20than%20Transformer%20at%208k%20context%20and%20matches%20Mamba%20in%20wall-clock%0Atime.%20TTT-MLP%20still%20faces%20challenges%20in%20memory%20I/O%2C%20but%20shows%20larger%20potential%0Ain%20long%20context%2C%20pointing%20to%20a%20promising%20direction%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520%2528Learn%2520at%2520Test%2520Time%2529%253A%2520RNNs%2520with%2520Expressive%2520Hidden%2520States%26entry.906535625%3DYu%2520Sun%2520and%2520Xinhao%2520Li%2520and%2520Karan%2520Dalal%2520and%2520Jiarui%2520Xu%2520and%2520Arjun%2520Vikram%2520and%2520Genghan%2520Zhang%2520and%2520Yann%2520Dubois%2520and%2520Xinlei%2520Chen%2520and%2520Xiaolong%2520Wang%2520and%2520Sanmi%2520Koyejo%2520and%2520Tatsunori%2520Hashimoto%2520and%2520Carlos%2520Guestrin%26entry.1292438233%3D%2520%2520Self-attention%2520performs%2520well%2520in%2520long%2520context%2520but%2520has%2520quadratic%2520complexity.%250AExisting%2520RNN%2520layers%2520have%2520linear%2520complexity%252C%2520but%2520their%2520performance%2520in%2520long%250Acontext%2520is%2520limited%2520by%2520the%2520expressive%2520power%2520of%2520their%2520hidden%2520state.%2520We%2520propose%2520a%250Anew%2520class%2520of%2520sequence%2520modeling%2520layers%2520with%2520linear%2520complexity%2520and%2520an%2520expressive%250Ahidden%2520state.%2520The%2520key%2520idea%2520is%2520to%2520make%2520the%2520hidden%2520state%2520a%2520machine%2520learning%2520model%250Aitself%252C%2520and%2520the%2520update%2520rule%2520a%2520step%2520of%2520self-supervised%2520learning.%2520Since%2520the%250Ahidden%2520state%2520is%2520updated%2520by%2520training%2520even%2520on%2520test%2520sequences%252C%2520our%2520layers%2520are%250Acalled%2520Test-Time%2520Training%2520%2528TTT%2529%2520layers.%2520We%2520consider%2520two%2520instantiations%253A%250ATTT-Linear%2520and%2520TTT-MLP%252C%2520whose%2520hidden%2520state%2520is%2520a%2520linear%2520model%2520and%2520a%2520two-layer%250AMLP%2520respectively.%2520We%2520evaluate%2520our%2520instantiations%2520at%2520the%2520scale%2520of%2520125M%2520to%25201.3B%250Aparameters%252C%2520comparing%2520with%2520a%2520strong%2520Transformer%2520and%2520Mamba%252C%2520a%2520modern%2520RNN.%2520Both%250ATTT-Linear%2520and%2520TTT-MLP%2520match%2520or%2520exceed%2520the%2520baselines.%2520Similar%2520to%2520Transformer%252C%250Athey%2520can%2520keep%2520reducing%2520perplexity%2520by%2520conditioning%2520on%2520more%2520tokens%252C%2520while%2520Mamba%250Acannot%2520after%252016k%2520context.%2520With%2520preliminary%2520systems%2520optimization%252C%2520TTT-Linear%2520is%250Aalready%2520faster%2520than%2520Transformer%2520at%25208k%2520context%2520and%2520matches%2520Mamba%2520in%2520wall-clock%250Atime.%2520TTT-MLP%2520still%2520faces%2520challenges%2520in%2520memory%2520I/O%252C%2520but%2520shows%2520larger%2520potential%250Ain%2520long%2520context%252C%2520pointing%2520to%2520a%2520promising%2520direction%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20%28Learn%20at%20Test%20Time%29%3A%20RNNs%20with%20Expressive%20Hidden%20States&entry.906535625=Yu%20Sun%20and%20Xinhao%20Li%20and%20Karan%20Dalal%20and%20Jiarui%20Xu%20and%20Arjun%20Vikram%20and%20Genghan%20Zhang%20and%20Yann%20Dubois%20and%20Xinlei%20Chen%20and%20Xiaolong%20Wang%20and%20Sanmi%20Koyejo%20and%20Tatsunori%20Hashimoto%20and%20Carlos%20Guestrin&entry.1292438233=%20%20Self-attention%20performs%20well%20in%20long%20context%20but%20has%20quadratic%20complexity.%0AExisting%20RNN%20layers%20have%20linear%20complexity%2C%20but%20their%20performance%20in%20long%0Acontext%20is%20limited%20by%20the%20expressive%20power%20of%20their%20hidden%20state.%20We%20propose%20a%0Anew%20class%20of%20sequence%20modeling%20layers%20with%20linear%20complexity%20and%20an%20expressive%0Ahidden%20state.%20The%20key%20idea%20is%20to%20make%20the%20hidden%20state%20a%20machine%20learning%20model%0Aitself%2C%20and%20the%20update%20rule%20a%20step%20of%20self-supervised%20learning.%20Since%20the%0Ahidden%20state%20is%20updated%20by%20training%20even%20on%20test%20sequences%2C%20our%20layers%20are%0Acalled%20Test-Time%20Training%20%28TTT%29%20layers.%20We%20consider%20two%20instantiations%3A%0ATTT-Linear%20and%20TTT-MLP%2C%20whose%20hidden%20state%20is%20a%20linear%20model%20and%20a%20two-layer%0AMLP%20respectively.%20We%20evaluate%20our%20instantiations%20at%20the%20scale%20of%20125M%20to%201.3B%0Aparameters%2C%20comparing%20with%20a%20strong%20Transformer%20and%20Mamba%2C%20a%20modern%20RNN.%20Both%0ATTT-Linear%20and%20TTT-MLP%20match%20or%20exceed%20the%20baselines.%20Similar%20to%20Transformer%2C%0Athey%20can%20keep%20reducing%20perplexity%20by%20conditioning%20on%20more%20tokens%2C%20while%20Mamba%0Acannot%20after%2016k%20context.%20With%20preliminary%20systems%20optimization%2C%20TTT-Linear%20is%0Aalready%20faster%20than%20Transformer%20at%208k%20context%20and%20matches%20Mamba%20in%20wall-clock%0Atime.%20TTT-MLP%20still%20faces%20challenges%20in%20memory%20I/O%2C%20but%20shows%20larger%20potential%0Ain%20long%20context%2C%20pointing%20to%20a%20promising%20direction%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04620v1&entry.124074799=Read"},
{"title": "Not (yet) the whole story: Evaluating Visual Storytelling Requires More\n  than Measuring Coherence, Grounding, and Repetition", "author": "Aditya K Surikuchi and Raquel Fern\u00e1ndez and Sandro Pezzelle", "abstract": "  Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition.\n", "link": "http://arxiv.org/abs/2407.04559v1", "date": "2024-07-05", "relevancy": 1.9274, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.472}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20%28yet%29%20the%20whole%20story%3A%20Evaluating%20Visual%20Storytelling%20Requires%20More%0A%20%20than%20Measuring%20Coherence%2C%20Grounding%2C%20and%20Repetition&body=Title%3A%20Not%20%28yet%29%20the%20whole%20story%3A%20Evaluating%20Visual%20Storytelling%20Requires%20More%0A%20%20than%20Measuring%20Coherence%2C%20Grounding%2C%20and%20Repetition%0AAuthor%3A%20Aditya%20K%20Surikuchi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle%0AAbstract%3A%20%20%20Visual%20storytelling%20consists%20in%20generating%20a%20natural%20language%20story%20given%20a%0Atemporally%20ordered%20sequence%20of%20images.%20This%20task%20is%20not%20only%20challenging%20for%0Amodels%2C%20but%20also%20very%20difficult%20to%20evaluate%20with%20automatic%20metrics%20since%20there%0Ais%20no%20consensus%20about%20what%20makes%20a%20story%20%27good%27.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20method%20that%20measures%20story%20quality%20in%20terms%20of%20human%20likeness%20regarding%0Athree%20key%20aspects%20highlighted%20in%20previous%20work%3A%20visual%20grounding%2C%20coherence%2C%0Aand%20repetitiveness.%20We%20then%20use%20this%20method%20to%20evaluate%20the%20stories%20generated%0Aby%20several%20models%2C%20showing%20that%20the%20foundation%20model%20LLaVA%20obtains%20the%20best%0Aresult%2C%20but%20only%20slightly%20so%20compared%20to%20TAPM%2C%20a%2050-times%20smaller%20visual%0Astorytelling%20model.%20Upgrading%20the%20visual%20and%20language%20components%20of%20TAPM%0Aresults%20in%20a%20model%20that%20yields%20competitive%20performance%20with%20a%20relatively%20low%0Anumber%20of%20parameters.%20Finally%2C%20we%20carry%20out%20a%20human%20evaluation%20study%2C%20whose%0Aresults%20suggest%20that%20a%20%27good%27%20story%20may%20require%20more%20than%20a%20human-like%20level%20of%0Avisual%20grounding%2C%20coherence%2C%20and%20repetition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04559v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520%2528yet%2529%2520the%2520whole%2520story%253A%2520Evaluating%2520Visual%2520Storytelling%2520Requires%2520More%250A%2520%2520than%2520Measuring%2520Coherence%252C%2520Grounding%252C%2520and%2520Repetition%26entry.906535625%3DAditya%2520K%2520Surikuchi%2520and%2520Raquel%2520Fern%25C3%25A1ndez%2520and%2520Sandro%2520Pezzelle%26entry.1292438233%3D%2520%2520Visual%2520storytelling%2520consists%2520in%2520generating%2520a%2520natural%2520language%2520story%2520given%2520a%250Atemporally%2520ordered%2520sequence%2520of%2520images.%2520This%2520task%2520is%2520not%2520only%2520challenging%2520for%250Amodels%252C%2520but%2520also%2520very%2520difficult%2520to%2520evaluate%2520with%2520automatic%2520metrics%2520since%2520there%250Ais%2520no%2520consensus%2520about%2520what%2520makes%2520a%2520story%2520%2527good%2527.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Anovel%2520method%2520that%2520measures%2520story%2520quality%2520in%2520terms%2520of%2520human%2520likeness%2520regarding%250Athree%2520key%2520aspects%2520highlighted%2520in%2520previous%2520work%253A%2520visual%2520grounding%252C%2520coherence%252C%250Aand%2520repetitiveness.%2520We%2520then%2520use%2520this%2520method%2520to%2520evaluate%2520the%2520stories%2520generated%250Aby%2520several%2520models%252C%2520showing%2520that%2520the%2520foundation%2520model%2520LLaVA%2520obtains%2520the%2520best%250Aresult%252C%2520but%2520only%2520slightly%2520so%2520compared%2520to%2520TAPM%252C%2520a%252050-times%2520smaller%2520visual%250Astorytelling%2520model.%2520Upgrading%2520the%2520visual%2520and%2520language%2520components%2520of%2520TAPM%250Aresults%2520in%2520a%2520model%2520that%2520yields%2520competitive%2520performance%2520with%2520a%2520relatively%2520low%250Anumber%2520of%2520parameters.%2520Finally%252C%2520we%2520carry%2520out%2520a%2520human%2520evaluation%2520study%252C%2520whose%250Aresults%2520suggest%2520that%2520a%2520%2527good%2527%2520story%2520may%2520require%2520more%2520than%2520a%2520human-like%2520level%2520of%250Avisual%2520grounding%252C%2520coherence%252C%2520and%2520repetition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04559v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20%28yet%29%20the%20whole%20story%3A%20Evaluating%20Visual%20Storytelling%20Requires%20More%0A%20%20than%20Measuring%20Coherence%2C%20Grounding%2C%20and%20Repetition&entry.906535625=Aditya%20K%20Surikuchi%20and%20Raquel%20Fern%C3%A1ndez%20and%20Sandro%20Pezzelle&entry.1292438233=%20%20Visual%20storytelling%20consists%20in%20generating%20a%20natural%20language%20story%20given%20a%0Atemporally%20ordered%20sequence%20of%20images.%20This%20task%20is%20not%20only%20challenging%20for%0Amodels%2C%20but%20also%20very%20difficult%20to%20evaluate%20with%20automatic%20metrics%20since%20there%0Ais%20no%20consensus%20about%20what%20makes%20a%20story%20%27good%27.%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20method%20that%20measures%20story%20quality%20in%20terms%20of%20human%20likeness%20regarding%0Athree%20key%20aspects%20highlighted%20in%20previous%20work%3A%20visual%20grounding%2C%20coherence%2C%0Aand%20repetitiveness.%20We%20then%20use%20this%20method%20to%20evaluate%20the%20stories%20generated%0Aby%20several%20models%2C%20showing%20that%20the%20foundation%20model%20LLaVA%20obtains%20the%20best%0Aresult%2C%20but%20only%20slightly%20so%20compared%20to%20TAPM%2C%20a%2050-times%20smaller%20visual%0Astorytelling%20model.%20Upgrading%20the%20visual%20and%20language%20components%20of%20TAPM%0Aresults%20in%20a%20model%20that%20yields%20competitive%20performance%20with%20a%20relatively%20low%0Anumber%20of%20parameters.%20Finally%2C%20we%20carry%20out%20a%20human%20evaluation%20study%2C%20whose%0Aresults%20suggest%20that%20a%20%27good%27%20story%20may%20require%20more%20than%20a%20human-like%20level%20of%0Avisual%20grounding%2C%20coherence%2C%20and%20repetition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04559v1&entry.124074799=Read"},
{"title": "MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under\n  Uncertainty", "author": "Tim Br\u00f6dermann and David Bruggemann and Christos Sakaridis and Kevin Ta and Odysseas Liagouris and Jason Corkill and Luc Van Gool", "abstract": "  Achieving level-5 driving automation in autonomous vehicles necessitates a\nrobust semantic visual perception system capable of parsing data from different\nsensors across diverse conditions. However, existing semantic perception\ndatasets often lack important non-camera modalities typically used in\nautonomous vehicles, or they do not exploit such modalities to aid and improve\nsemantic annotations in challenging conditions. To address this, we introduce\nMUSES, the MUlti-SEnsor Semantic perception dataset for driving in adverse\nconditions under increased uncertainty. MUSES includes synchronized multimodal\nrecordings with 2D panoptic annotations for 2500 images captured under diverse\nweather and illumination. The dataset integrates a frame camera, a lidar, a\nradar, an event camera, and an IMU/GNSS sensor. Our new two-stage panoptic\nannotation protocol captures both class-level and instance-level uncertainty in\nthe ground truth and enables the novel task of uncertainty-aware panoptic\nsegmentation we introduce, along with standard semantic and panoptic\nsegmentation. MUSES proves both effective for training and challenging for\nevaluating models under diverse visual conditions, and it opens new avenues for\nresearch in multimodal and uncertainty-aware dense semantic perception. Our\ndataset and benchmark are publicly available at\nhttps://muses.vision.ee.ethz.ch.\n", "link": "http://arxiv.org/abs/2401.12761v2", "date": "2024-07-05", "relevancy": 1.9188, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6582}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.64}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MUSES%3A%20The%20Multi-Sensor%20Semantic%20Perception%20Dataset%20for%20Driving%20under%0A%20%20Uncertainty&body=Title%3A%20MUSES%3A%20The%20Multi-Sensor%20Semantic%20Perception%20Dataset%20for%20Driving%20under%0A%20%20Uncertainty%0AAuthor%3A%20Tim%20Br%C3%B6dermann%20and%20David%20Bruggemann%20and%20Christos%20Sakaridis%20and%20Kevin%20Ta%20and%20Odysseas%20Liagouris%20and%20Jason%20Corkill%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Achieving%20level-5%20driving%20automation%20in%20autonomous%20vehicles%20necessitates%20a%0Arobust%20semantic%20visual%20perception%20system%20capable%20of%20parsing%20data%20from%20different%0Asensors%20across%20diverse%20conditions.%20However%2C%20existing%20semantic%20perception%0Adatasets%20often%20lack%20important%20non-camera%20modalities%20typically%20used%20in%0Aautonomous%20vehicles%2C%20or%20they%20do%20not%20exploit%20such%20modalities%20to%20aid%20and%20improve%0Asemantic%20annotations%20in%20challenging%20conditions.%20To%20address%20this%2C%20we%20introduce%0AMUSES%2C%20the%20MUlti-SEnsor%20Semantic%20perception%20dataset%20for%20driving%20in%20adverse%0Aconditions%20under%20increased%20uncertainty.%20MUSES%20includes%20synchronized%20multimodal%0Arecordings%20with%202D%20panoptic%20annotations%20for%202500%20images%20captured%20under%20diverse%0Aweather%20and%20illumination.%20The%20dataset%20integrates%20a%20frame%20camera%2C%20a%20lidar%2C%20a%0Aradar%2C%20an%20event%20camera%2C%20and%20an%20IMU/GNSS%20sensor.%20Our%20new%20two-stage%20panoptic%0Aannotation%20protocol%20captures%20both%20class-level%20and%20instance-level%20uncertainty%20in%0Athe%20ground%20truth%20and%20enables%20the%20novel%20task%20of%20uncertainty-aware%20panoptic%0Asegmentation%20we%20introduce%2C%20along%20with%20standard%20semantic%20and%20panoptic%0Asegmentation.%20MUSES%20proves%20both%20effective%20for%20training%20and%20challenging%20for%0Aevaluating%20models%20under%20diverse%20visual%20conditions%2C%20and%20it%20opens%20new%20avenues%20for%0Aresearch%20in%20multimodal%20and%20uncertainty-aware%20dense%20semantic%20perception.%20Our%0Adataset%20and%20benchmark%20are%20publicly%20available%20at%0Ahttps%3A//muses.vision.ee.ethz.ch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMUSES%253A%2520The%2520Multi-Sensor%2520Semantic%2520Perception%2520Dataset%2520for%2520Driving%2520under%250A%2520%2520Uncertainty%26entry.906535625%3DTim%2520Br%25C3%25B6dermann%2520and%2520David%2520Bruggemann%2520and%2520Christos%2520Sakaridis%2520and%2520Kevin%2520Ta%2520and%2520Odysseas%2520Liagouris%2520and%2520Jason%2520Corkill%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%2520Achieving%2520level-5%2520driving%2520automation%2520in%2520autonomous%2520vehicles%2520necessitates%2520a%250Arobust%2520semantic%2520visual%2520perception%2520system%2520capable%2520of%2520parsing%2520data%2520from%2520different%250Asensors%2520across%2520diverse%2520conditions.%2520However%252C%2520existing%2520semantic%2520perception%250Adatasets%2520often%2520lack%2520important%2520non-camera%2520modalities%2520typically%2520used%2520in%250Aautonomous%2520vehicles%252C%2520or%2520they%2520do%2520not%2520exploit%2520such%2520modalities%2520to%2520aid%2520and%2520improve%250Asemantic%2520annotations%2520in%2520challenging%2520conditions.%2520To%2520address%2520this%252C%2520we%2520introduce%250AMUSES%252C%2520the%2520MUlti-SEnsor%2520Semantic%2520perception%2520dataset%2520for%2520driving%2520in%2520adverse%250Aconditions%2520under%2520increased%2520uncertainty.%2520MUSES%2520includes%2520synchronized%2520multimodal%250Arecordings%2520with%25202D%2520panoptic%2520annotations%2520for%25202500%2520images%2520captured%2520under%2520diverse%250Aweather%2520and%2520illumination.%2520The%2520dataset%2520integrates%2520a%2520frame%2520camera%252C%2520a%2520lidar%252C%2520a%250Aradar%252C%2520an%2520event%2520camera%252C%2520and%2520an%2520IMU/GNSS%2520sensor.%2520Our%2520new%2520two-stage%2520panoptic%250Aannotation%2520protocol%2520captures%2520both%2520class-level%2520and%2520instance-level%2520uncertainty%2520in%250Athe%2520ground%2520truth%2520and%2520enables%2520the%2520novel%2520task%2520of%2520uncertainty-aware%2520panoptic%250Asegmentation%2520we%2520introduce%252C%2520along%2520with%2520standard%2520semantic%2520and%2520panoptic%250Asegmentation.%2520MUSES%2520proves%2520both%2520effective%2520for%2520training%2520and%2520challenging%2520for%250Aevaluating%2520models%2520under%2520diverse%2520visual%2520conditions%252C%2520and%2520it%2520opens%2520new%2520avenues%2520for%250Aresearch%2520in%2520multimodal%2520and%2520uncertainty-aware%2520dense%2520semantic%2520perception.%2520Our%250Adataset%2520and%2520benchmark%2520are%2520publicly%2520available%2520at%250Ahttps%253A//muses.vision.ee.ethz.ch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MUSES%3A%20The%20Multi-Sensor%20Semantic%20Perception%20Dataset%20for%20Driving%20under%0A%20%20Uncertainty&entry.906535625=Tim%20Br%C3%B6dermann%20and%20David%20Bruggemann%20and%20Christos%20Sakaridis%20and%20Kevin%20Ta%20and%20Odysseas%20Liagouris%20and%20Jason%20Corkill%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Achieving%20level-5%20driving%20automation%20in%20autonomous%20vehicles%20necessitates%20a%0Arobust%20semantic%20visual%20perception%20system%20capable%20of%20parsing%20data%20from%20different%0Asensors%20across%20diverse%20conditions.%20However%2C%20existing%20semantic%20perception%0Adatasets%20often%20lack%20important%20non-camera%20modalities%20typically%20used%20in%0Aautonomous%20vehicles%2C%20or%20they%20do%20not%20exploit%20such%20modalities%20to%20aid%20and%20improve%0Asemantic%20annotations%20in%20challenging%20conditions.%20To%20address%20this%2C%20we%20introduce%0AMUSES%2C%20the%20MUlti-SEnsor%20Semantic%20perception%20dataset%20for%20driving%20in%20adverse%0Aconditions%20under%20increased%20uncertainty.%20MUSES%20includes%20synchronized%20multimodal%0Arecordings%20with%202D%20panoptic%20annotations%20for%202500%20images%20captured%20under%20diverse%0Aweather%20and%20illumination.%20The%20dataset%20integrates%20a%20frame%20camera%2C%20a%20lidar%2C%20a%0Aradar%2C%20an%20event%20camera%2C%20and%20an%20IMU/GNSS%20sensor.%20Our%20new%20two-stage%20panoptic%0Aannotation%20protocol%20captures%20both%20class-level%20and%20instance-level%20uncertainty%20in%0Athe%20ground%20truth%20and%20enables%20the%20novel%20task%20of%20uncertainty-aware%20panoptic%0Asegmentation%20we%20introduce%2C%20along%20with%20standard%20semantic%20and%20panoptic%0Asegmentation.%20MUSES%20proves%20both%20effective%20for%20training%20and%20challenging%20for%0Aevaluating%20models%20under%20diverse%20visual%20conditions%2C%20and%20it%20opens%20new%20avenues%20for%0Aresearch%20in%20multimodal%20and%20uncertainty-aware%20dense%20semantic%20perception.%20Our%0Adataset%20and%20benchmark%20are%20publicly%20available%20at%0Ahttps%3A//muses.vision.ee.ethz.ch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12761v2&entry.124074799=Read"},
{"title": "XQSV: A Structurally Variable Network to Imitate Human Play in Xiangqi", "author": "Chenliang Zhou", "abstract": "  In this paper, we introduce an innovative deep learning architecture, termed\nXiangqi Structurally Variable (XQSV), designed to emulate the behavioral\npatterns of human players in Xiangqi, or Chinese Chess. The unique attribute of\nXQSV is its capacity to alter its structural configuration dynamically,\noptimizing performance for the task based on the particular subset of data on\nwhich it is trained. We have incorporated several design improvements to\nsignificantly enhance the network's predictive accuracy, including a local\nillegal move filter, an Elo range partitioning, a sequential one-dimensional\ninput, and a simulation of imperfect memory capacity. Empirical evaluations\nreveal that XQSV attains a predictive accuracy of approximately 40%, with its\nperformance peaking within the trained Elo range. This indicates the model's\nsuccess in mimicking the play behavior of individuals within that specific\nrange. A three-terminal Turing Test was employed to demonstrate that the XQSV\nmodel imitates human behavior more accurately than conventional Xiangqi\nengines, rendering it indistinguishable from actual human opponents. Given the\ninherent nondeterminism in human gameplay, we propose two supplementary relaxed\nevaluation metrics. To our knowledge, XQSV represents the first model to mimic\nXiangqi players.\n", "link": "http://arxiv.org/abs/2407.04678v1", "date": "2024-07-05", "relevancy": 1.9175, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4995}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4657}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XQSV%3A%20A%20Structurally%20Variable%20Network%20to%20Imitate%20Human%20Play%20in%20Xiangqi&body=Title%3A%20XQSV%3A%20A%20Structurally%20Variable%20Network%20to%20Imitate%20Human%20Play%20in%20Xiangqi%0AAuthor%3A%20Chenliang%20Zhou%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20innovative%20deep%20learning%20architecture%2C%20termed%0AXiangqi%20Structurally%20Variable%20%28XQSV%29%2C%20designed%20to%20emulate%20the%20behavioral%0Apatterns%20of%20human%20players%20in%20Xiangqi%2C%20or%20Chinese%20Chess.%20The%20unique%20attribute%20of%0AXQSV%20is%20its%20capacity%20to%20alter%20its%20structural%20configuration%20dynamically%2C%0Aoptimizing%20performance%20for%20the%20task%20based%20on%20the%20particular%20subset%20of%20data%20on%0Awhich%20it%20is%20trained.%20We%20have%20incorporated%20several%20design%20improvements%20to%0Asignificantly%20enhance%20the%20network%27s%20predictive%20accuracy%2C%20including%20a%20local%0Aillegal%20move%20filter%2C%20an%20Elo%20range%20partitioning%2C%20a%20sequential%20one-dimensional%0Ainput%2C%20and%20a%20simulation%20of%20imperfect%20memory%20capacity.%20Empirical%20evaluations%0Areveal%20that%20XQSV%20attains%20a%20predictive%20accuracy%20of%20approximately%2040%25%2C%20with%20its%0Aperformance%20peaking%20within%20the%20trained%20Elo%20range.%20This%20indicates%20the%20model%27s%0Asuccess%20in%20mimicking%20the%20play%20behavior%20of%20individuals%20within%20that%20specific%0Arange.%20A%20three-terminal%20Turing%20Test%20was%20employed%20to%20demonstrate%20that%20the%20XQSV%0Amodel%20imitates%20human%20behavior%20more%20accurately%20than%20conventional%20Xiangqi%0Aengines%2C%20rendering%20it%20indistinguishable%20from%20actual%20human%20opponents.%20Given%20the%0Ainherent%20nondeterminism%20in%20human%20gameplay%2C%20we%20propose%20two%20supplementary%20relaxed%0Aevaluation%20metrics.%20To%20our%20knowledge%2C%20XQSV%20represents%20the%20first%20model%20to%20mimic%0AXiangqi%20players.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXQSV%253A%2520A%2520Structurally%2520Variable%2520Network%2520to%2520Imitate%2520Human%2520Play%2520in%2520Xiangqi%26entry.906535625%3DChenliang%2520Zhou%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520innovative%2520deep%2520learning%2520architecture%252C%2520termed%250AXiangqi%2520Structurally%2520Variable%2520%2528XQSV%2529%252C%2520designed%2520to%2520emulate%2520the%2520behavioral%250Apatterns%2520of%2520human%2520players%2520in%2520Xiangqi%252C%2520or%2520Chinese%2520Chess.%2520The%2520unique%2520attribute%2520of%250AXQSV%2520is%2520its%2520capacity%2520to%2520alter%2520its%2520structural%2520configuration%2520dynamically%252C%250Aoptimizing%2520performance%2520for%2520the%2520task%2520based%2520on%2520the%2520particular%2520subset%2520of%2520data%2520on%250Awhich%2520it%2520is%2520trained.%2520We%2520have%2520incorporated%2520several%2520design%2520improvements%2520to%250Asignificantly%2520enhance%2520the%2520network%2527s%2520predictive%2520accuracy%252C%2520including%2520a%2520local%250Aillegal%2520move%2520filter%252C%2520an%2520Elo%2520range%2520partitioning%252C%2520a%2520sequential%2520one-dimensional%250Ainput%252C%2520and%2520a%2520simulation%2520of%2520imperfect%2520memory%2520capacity.%2520Empirical%2520evaluations%250Areveal%2520that%2520XQSV%2520attains%2520a%2520predictive%2520accuracy%2520of%2520approximately%252040%2525%252C%2520with%2520its%250Aperformance%2520peaking%2520within%2520the%2520trained%2520Elo%2520range.%2520This%2520indicates%2520the%2520model%2527s%250Asuccess%2520in%2520mimicking%2520the%2520play%2520behavior%2520of%2520individuals%2520within%2520that%2520specific%250Arange.%2520A%2520three-terminal%2520Turing%2520Test%2520was%2520employed%2520to%2520demonstrate%2520that%2520the%2520XQSV%250Amodel%2520imitates%2520human%2520behavior%2520more%2520accurately%2520than%2520conventional%2520Xiangqi%250Aengines%252C%2520rendering%2520it%2520indistinguishable%2520from%2520actual%2520human%2520opponents.%2520Given%2520the%250Ainherent%2520nondeterminism%2520in%2520human%2520gameplay%252C%2520we%2520propose%2520two%2520supplementary%2520relaxed%250Aevaluation%2520metrics.%2520To%2520our%2520knowledge%252C%2520XQSV%2520represents%2520the%2520first%2520model%2520to%2520mimic%250AXiangqi%2520players.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XQSV%3A%20A%20Structurally%20Variable%20Network%20to%20Imitate%20Human%20Play%20in%20Xiangqi&entry.906535625=Chenliang%20Zhou&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20innovative%20deep%20learning%20architecture%2C%20termed%0AXiangqi%20Structurally%20Variable%20%28XQSV%29%2C%20designed%20to%20emulate%20the%20behavioral%0Apatterns%20of%20human%20players%20in%20Xiangqi%2C%20or%20Chinese%20Chess.%20The%20unique%20attribute%20of%0AXQSV%20is%20its%20capacity%20to%20alter%20its%20structural%20configuration%20dynamically%2C%0Aoptimizing%20performance%20for%20the%20task%20based%20on%20the%20particular%20subset%20of%20data%20on%0Awhich%20it%20is%20trained.%20We%20have%20incorporated%20several%20design%20improvements%20to%0Asignificantly%20enhance%20the%20network%27s%20predictive%20accuracy%2C%20including%20a%20local%0Aillegal%20move%20filter%2C%20an%20Elo%20range%20partitioning%2C%20a%20sequential%20one-dimensional%0Ainput%2C%20and%20a%20simulation%20of%20imperfect%20memory%20capacity.%20Empirical%20evaluations%0Areveal%20that%20XQSV%20attains%20a%20predictive%20accuracy%20of%20approximately%2040%25%2C%20with%20its%0Aperformance%20peaking%20within%20the%20trained%20Elo%20range.%20This%20indicates%20the%20model%27s%0Asuccess%20in%20mimicking%20the%20play%20behavior%20of%20individuals%20within%20that%20specific%0Arange.%20A%20three-terminal%20Turing%20Test%20was%20employed%20to%20demonstrate%20that%20the%20XQSV%0Amodel%20imitates%20human%20behavior%20more%20accurately%20than%20conventional%20Xiangqi%0Aengines%2C%20rendering%20it%20indistinguishable%20from%20actual%20human%20opponents.%20Given%20the%0Ainherent%20nondeterminism%20in%20human%20gameplay%2C%20we%20propose%20two%20supplementary%20relaxed%0Aevaluation%20metrics.%20To%20our%20knowledge%2C%20XQSV%20represents%20the%20first%20model%20to%20mimic%0AXiangqi%20players.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04678v1&entry.124074799=Read"},
{"title": "Estimating Treatment Effects under Recommender Interference: A\n  Structured Neural Networks Approach", "author": "Ruohan Zhan and Shichao Han and Yuchen Hu and Zhenling Jiang", "abstract": "  Recommender systems are essential for content-sharing platforms by curating\npersonalized content. To evaluate updates to recommender systems targeting\ncontent creators, platforms frequently rely on creator-side randomized\nexperiments. The treatment effect measures the change in outcomes when a new\nalgorithm is implemented compared to the status quo. We show that the standard\ndifference-in-means estimator can lead to biased estimates due to recommender\ninterference that arises when treated and control creators compete for\nexposure. We propose a \"recommender choice model\" that describes which item\ngets exposed from a pool containing both treated and control items. By\ncombining a structural choice model with neural networks, this framework\ndirectly models the interference pathway while accounting for rich\nviewer-content heterogeneity. We construct a debiased estimator of the\ntreatment effect and prove it is $\\sqrt n$-consistent and asymptotically normal\nwith potentially correlated samples. We validate our estimator's empirical\nperformance with a field experiment on Weixin short-video platform. In addition\nto the standard creator-side experiment, we conduct a costly double-sided\nrandomization design to obtain a benchmark estimate free from interference\nbias. We show that the proposed estimator yields results comparable to the\nbenchmark, whereas the standard difference-in-means estimator can exhibit\nsignificant bias and even produce reversed signs.\n", "link": "http://arxiv.org/abs/2406.14380v3", "date": "2024-07-05", "relevancy": 1.915, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5127}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4657}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Treatment%20Effects%20under%20Recommender%20Interference%3A%20A%0A%20%20Structured%20Neural%20Networks%20Approach&body=Title%3A%20Estimating%20Treatment%20Effects%20under%20Recommender%20Interference%3A%20A%0A%20%20Structured%20Neural%20Networks%20Approach%0AAuthor%3A%20Ruohan%20Zhan%20and%20Shichao%20Han%20and%20Yuchen%20Hu%20and%20Zhenling%20Jiang%0AAbstract%3A%20%20%20Recommender%20systems%20are%20essential%20for%20content-sharing%20platforms%20by%20curating%0Apersonalized%20content.%20To%20evaluate%20updates%20to%20recommender%20systems%20targeting%0Acontent%20creators%2C%20platforms%20frequently%20rely%20on%20creator-side%20randomized%0Aexperiments.%20The%20treatment%20effect%20measures%20the%20change%20in%20outcomes%20when%20a%20new%0Aalgorithm%20is%20implemented%20compared%20to%20the%20status%20quo.%20We%20show%20that%20the%20standard%0Adifference-in-means%20estimator%20can%20lead%20to%20biased%20estimates%20due%20to%20recommender%0Ainterference%20that%20arises%20when%20treated%20and%20control%20creators%20compete%20for%0Aexposure.%20We%20propose%20a%20%22recommender%20choice%20model%22%20that%20describes%20which%20item%0Agets%20exposed%20from%20a%20pool%20containing%20both%20treated%20and%20control%20items.%20By%0Acombining%20a%20structural%20choice%20model%20with%20neural%20networks%2C%20this%20framework%0Adirectly%20models%20the%20interference%20pathway%20while%20accounting%20for%20rich%0Aviewer-content%20heterogeneity.%20We%20construct%20a%20debiased%20estimator%20of%20the%0Atreatment%20effect%20and%20prove%20it%20is%20%24%5Csqrt%20n%24-consistent%20and%20asymptotically%20normal%0Awith%20potentially%20correlated%20samples.%20We%20validate%20our%20estimator%27s%20empirical%0Aperformance%20with%20a%20field%20experiment%20on%20Weixin%20short-video%20platform.%20In%20addition%0Ato%20the%20standard%20creator-side%20experiment%2C%20we%20conduct%20a%20costly%20double-sided%0Arandomization%20design%20to%20obtain%20a%20benchmark%20estimate%20free%20from%20interference%0Abias.%20We%20show%20that%20the%20proposed%20estimator%20yields%20results%20comparable%20to%20the%0Abenchmark%2C%20whereas%20the%20standard%20difference-in-means%20estimator%20can%20exhibit%0Asignificant%20bias%20and%20even%20produce%20reversed%20signs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14380v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Treatment%2520Effects%2520under%2520Recommender%2520Interference%253A%2520A%250A%2520%2520Structured%2520Neural%2520Networks%2520Approach%26entry.906535625%3DRuohan%2520Zhan%2520and%2520Shichao%2520Han%2520and%2520Yuchen%2520Hu%2520and%2520Zhenling%2520Jiang%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520are%2520essential%2520for%2520content-sharing%2520platforms%2520by%2520curating%250Apersonalized%2520content.%2520To%2520evaluate%2520updates%2520to%2520recommender%2520systems%2520targeting%250Acontent%2520creators%252C%2520platforms%2520frequently%2520rely%2520on%2520creator-side%2520randomized%250Aexperiments.%2520The%2520treatment%2520effect%2520measures%2520the%2520change%2520in%2520outcomes%2520when%2520a%2520new%250Aalgorithm%2520is%2520implemented%2520compared%2520to%2520the%2520status%2520quo.%2520We%2520show%2520that%2520the%2520standard%250Adifference-in-means%2520estimator%2520can%2520lead%2520to%2520biased%2520estimates%2520due%2520to%2520recommender%250Ainterference%2520that%2520arises%2520when%2520treated%2520and%2520control%2520creators%2520compete%2520for%250Aexposure.%2520We%2520propose%2520a%2520%2522recommender%2520choice%2520model%2522%2520that%2520describes%2520which%2520item%250Agets%2520exposed%2520from%2520a%2520pool%2520containing%2520both%2520treated%2520and%2520control%2520items.%2520By%250Acombining%2520a%2520structural%2520choice%2520model%2520with%2520neural%2520networks%252C%2520this%2520framework%250Adirectly%2520models%2520the%2520interference%2520pathway%2520while%2520accounting%2520for%2520rich%250Aviewer-content%2520heterogeneity.%2520We%2520construct%2520a%2520debiased%2520estimator%2520of%2520the%250Atreatment%2520effect%2520and%2520prove%2520it%2520is%2520%2524%255Csqrt%2520n%2524-consistent%2520and%2520asymptotically%2520normal%250Awith%2520potentially%2520correlated%2520samples.%2520We%2520validate%2520our%2520estimator%2527s%2520empirical%250Aperformance%2520with%2520a%2520field%2520experiment%2520on%2520Weixin%2520short-video%2520platform.%2520In%2520addition%250Ato%2520the%2520standard%2520creator-side%2520experiment%252C%2520we%2520conduct%2520a%2520costly%2520double-sided%250Arandomization%2520design%2520to%2520obtain%2520a%2520benchmark%2520estimate%2520free%2520from%2520interference%250Abias.%2520We%2520show%2520that%2520the%2520proposed%2520estimator%2520yields%2520results%2520comparable%2520to%2520the%250Abenchmark%252C%2520whereas%2520the%2520standard%2520difference-in-means%2520estimator%2520can%2520exhibit%250Asignificant%2520bias%2520and%2520even%2520produce%2520reversed%2520signs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14380v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Treatment%20Effects%20under%20Recommender%20Interference%3A%20A%0A%20%20Structured%20Neural%20Networks%20Approach&entry.906535625=Ruohan%20Zhan%20and%20Shichao%20Han%20and%20Yuchen%20Hu%20and%20Zhenling%20Jiang&entry.1292438233=%20%20Recommender%20systems%20are%20essential%20for%20content-sharing%20platforms%20by%20curating%0Apersonalized%20content.%20To%20evaluate%20updates%20to%20recommender%20systems%20targeting%0Acontent%20creators%2C%20platforms%20frequently%20rely%20on%20creator-side%20randomized%0Aexperiments.%20The%20treatment%20effect%20measures%20the%20change%20in%20outcomes%20when%20a%20new%0Aalgorithm%20is%20implemented%20compared%20to%20the%20status%20quo.%20We%20show%20that%20the%20standard%0Adifference-in-means%20estimator%20can%20lead%20to%20biased%20estimates%20due%20to%20recommender%0Ainterference%20that%20arises%20when%20treated%20and%20control%20creators%20compete%20for%0Aexposure.%20We%20propose%20a%20%22recommender%20choice%20model%22%20that%20describes%20which%20item%0Agets%20exposed%20from%20a%20pool%20containing%20both%20treated%20and%20control%20items.%20By%0Acombining%20a%20structural%20choice%20model%20with%20neural%20networks%2C%20this%20framework%0Adirectly%20models%20the%20interference%20pathway%20while%20accounting%20for%20rich%0Aviewer-content%20heterogeneity.%20We%20construct%20a%20debiased%20estimator%20of%20the%0Atreatment%20effect%20and%20prove%20it%20is%20%24%5Csqrt%20n%24-consistent%20and%20asymptotically%20normal%0Awith%20potentially%20correlated%20samples.%20We%20validate%20our%20estimator%27s%20empirical%0Aperformance%20with%20a%20field%20experiment%20on%20Weixin%20short-video%20platform.%20In%20addition%0Ato%20the%20standard%20creator-side%20experiment%2C%20we%20conduct%20a%20costly%20double-sided%0Arandomization%20design%20to%20obtain%20a%20benchmark%20estimate%20free%20from%20interference%0Abias.%20We%20show%20that%20the%20proposed%20estimator%20yields%20results%20comparable%20to%20the%0Abenchmark%2C%20whereas%20the%20standard%20difference-in-means%20estimator%20can%20exhibit%0Asignificant%20bias%20and%20even%20produce%20reversed%20signs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14380v3&entry.124074799=Read"},
{"title": "Universal Knowledge Graph Embeddings", "author": "N'Dah Jean Kouagou and Caglar Demir and Hamada M. Zahera and Adrian Wilke and Stefan Heindorf and Jiayi Li and Axel-Cyrille Ngonga Ngomo", "abstract": "  A variety of knowledge graph embedding approaches have been developed. Most\nof them obtain embeddings by learning the structure of the knowledge graph\nwithin a link prediction setting. As a result, the embeddings reflect only the\nstructure of a single knowledge graph, and embeddings for different knowledge\ngraphs are not aligned, e.g., they cannot be used to find similar entities\nacross knowledge graphs via nearest neighbor search. However, knowledge graph\nembedding applications such as entity disambiguation require a more global\nrepresentation, i.e., a representation that is valid across multiple sources.\nWe propose to learn universal knowledge graph embeddings from large-scale\ninterlinked knowledge sources. To this end, we fuse large knowledge graphs\nbased on the owl:sameAs relation such that every entity is represented by a\nunique identity. We instantiate our idea by computing universal embeddings\nbased on DBpedia and Wikidata yielding embeddings for about 180 million\nentities, 15 thousand relations, and 1.2 billion triples. We believe our\ncomputed embeddings will support the emerging field of graph foundation models.\nMoreover, we develop a convenient API to provide embeddings as a service.\nExperiments on link prediction suggest that universal knowledge graph\nembeddings encode better semantics compared to embeddings computed on a single\nknowledge graph. For reproducibility purposes, we provide our source code and\ndatasets open access.\n", "link": "http://arxiv.org/abs/2310.14899v2", "date": "2024-07-05", "relevancy": 1.8807, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4845}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4699}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Knowledge%20Graph%20Embeddings&body=Title%3A%20Universal%20Knowledge%20Graph%20Embeddings%0AAuthor%3A%20N%27Dah%20Jean%20Kouagou%20and%20Caglar%20Demir%20and%20Hamada%20M.%20Zahera%20and%20Adrian%20Wilke%20and%20Stefan%20Heindorf%20and%20Jiayi%20Li%20and%20Axel-Cyrille%20Ngonga%20Ngomo%0AAbstract%3A%20%20%20A%20variety%20of%20knowledge%20graph%20embedding%20approaches%20have%20been%20developed.%20Most%0Aof%20them%20obtain%20embeddings%20by%20learning%20the%20structure%20of%20the%20knowledge%20graph%0Awithin%20a%20link%20prediction%20setting.%20As%20a%20result%2C%20the%20embeddings%20reflect%20only%20the%0Astructure%20of%20a%20single%20knowledge%20graph%2C%20and%20embeddings%20for%20different%20knowledge%0Agraphs%20are%20not%20aligned%2C%20e.g.%2C%20they%20cannot%20be%20used%20to%20find%20similar%20entities%0Aacross%20knowledge%20graphs%20via%20nearest%20neighbor%20search.%20However%2C%20knowledge%20graph%0Aembedding%20applications%20such%20as%20entity%20disambiguation%20require%20a%20more%20global%0Arepresentation%2C%20i.e.%2C%20a%20representation%20that%20is%20valid%20across%20multiple%20sources.%0AWe%20propose%20to%20learn%20universal%20knowledge%20graph%20embeddings%20from%20large-scale%0Ainterlinked%20knowledge%20sources.%20To%20this%20end%2C%20we%20fuse%20large%20knowledge%20graphs%0Abased%20on%20the%20owl%3AsameAs%20relation%20such%20that%20every%20entity%20is%20represented%20by%20a%0Aunique%20identity.%20We%20instantiate%20our%20idea%20by%20computing%20universal%20embeddings%0Abased%20on%20DBpedia%20and%20Wikidata%20yielding%20embeddings%20for%20about%20180%20million%0Aentities%2C%2015%20thousand%20relations%2C%20and%201.2%20billion%20triples.%20We%20believe%20our%0Acomputed%20embeddings%20will%20support%20the%20emerging%20field%20of%20graph%20foundation%20models.%0AMoreover%2C%20we%20develop%20a%20convenient%20API%20to%20provide%20embeddings%20as%20a%20service.%0AExperiments%20on%20link%20prediction%20suggest%20that%20universal%20knowledge%20graph%0Aembeddings%20encode%20better%20semantics%20compared%20to%20embeddings%20computed%20on%20a%20single%0Aknowledge%20graph.%20For%20reproducibility%20purposes%2C%20we%20provide%20our%20source%20code%20and%0Adatasets%20open%20access.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14899v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Knowledge%2520Graph%2520Embeddings%26entry.906535625%3DN%2527Dah%2520Jean%2520Kouagou%2520and%2520Caglar%2520Demir%2520and%2520Hamada%2520M.%2520Zahera%2520and%2520Adrian%2520Wilke%2520and%2520Stefan%2520Heindorf%2520and%2520Jiayi%2520Li%2520and%2520Axel-Cyrille%2520Ngonga%2520Ngomo%26entry.1292438233%3D%2520%2520A%2520variety%2520of%2520knowledge%2520graph%2520embedding%2520approaches%2520have%2520been%2520developed.%2520Most%250Aof%2520them%2520obtain%2520embeddings%2520by%2520learning%2520the%2520structure%2520of%2520the%2520knowledge%2520graph%250Awithin%2520a%2520link%2520prediction%2520setting.%2520As%2520a%2520result%252C%2520the%2520embeddings%2520reflect%2520only%2520the%250Astructure%2520of%2520a%2520single%2520knowledge%2520graph%252C%2520and%2520embeddings%2520for%2520different%2520knowledge%250Agraphs%2520are%2520not%2520aligned%252C%2520e.g.%252C%2520they%2520cannot%2520be%2520used%2520to%2520find%2520similar%2520entities%250Aacross%2520knowledge%2520graphs%2520via%2520nearest%2520neighbor%2520search.%2520However%252C%2520knowledge%2520graph%250Aembedding%2520applications%2520such%2520as%2520entity%2520disambiguation%2520require%2520a%2520more%2520global%250Arepresentation%252C%2520i.e.%252C%2520a%2520representation%2520that%2520is%2520valid%2520across%2520multiple%2520sources.%250AWe%2520propose%2520to%2520learn%2520universal%2520knowledge%2520graph%2520embeddings%2520from%2520large-scale%250Ainterlinked%2520knowledge%2520sources.%2520To%2520this%2520end%252C%2520we%2520fuse%2520large%2520knowledge%2520graphs%250Abased%2520on%2520the%2520owl%253AsameAs%2520relation%2520such%2520that%2520every%2520entity%2520is%2520represented%2520by%2520a%250Aunique%2520identity.%2520We%2520instantiate%2520our%2520idea%2520by%2520computing%2520universal%2520embeddings%250Abased%2520on%2520DBpedia%2520and%2520Wikidata%2520yielding%2520embeddings%2520for%2520about%2520180%2520million%250Aentities%252C%252015%2520thousand%2520relations%252C%2520and%25201.2%2520billion%2520triples.%2520We%2520believe%2520our%250Acomputed%2520embeddings%2520will%2520support%2520the%2520emerging%2520field%2520of%2520graph%2520foundation%2520models.%250AMoreover%252C%2520we%2520develop%2520a%2520convenient%2520API%2520to%2520provide%2520embeddings%2520as%2520a%2520service.%250AExperiments%2520on%2520link%2520prediction%2520suggest%2520that%2520universal%2520knowledge%2520graph%250Aembeddings%2520encode%2520better%2520semantics%2520compared%2520to%2520embeddings%2520computed%2520on%2520a%2520single%250Aknowledge%2520graph.%2520For%2520reproducibility%2520purposes%252C%2520we%2520provide%2520our%2520source%2520code%2520and%250Adatasets%2520open%2520access.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.14899v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Knowledge%20Graph%20Embeddings&entry.906535625=N%27Dah%20Jean%20Kouagou%20and%20Caglar%20Demir%20and%20Hamada%20M.%20Zahera%20and%20Adrian%20Wilke%20and%20Stefan%20Heindorf%20and%20Jiayi%20Li%20and%20Axel-Cyrille%20Ngonga%20Ngomo&entry.1292438233=%20%20A%20variety%20of%20knowledge%20graph%20embedding%20approaches%20have%20been%20developed.%20Most%0Aof%20them%20obtain%20embeddings%20by%20learning%20the%20structure%20of%20the%20knowledge%20graph%0Awithin%20a%20link%20prediction%20setting.%20As%20a%20result%2C%20the%20embeddings%20reflect%20only%20the%0Astructure%20of%20a%20single%20knowledge%20graph%2C%20and%20embeddings%20for%20different%20knowledge%0Agraphs%20are%20not%20aligned%2C%20e.g.%2C%20they%20cannot%20be%20used%20to%20find%20similar%20entities%0Aacross%20knowledge%20graphs%20via%20nearest%20neighbor%20search.%20However%2C%20knowledge%20graph%0Aembedding%20applications%20such%20as%20entity%20disambiguation%20require%20a%20more%20global%0Arepresentation%2C%20i.e.%2C%20a%20representation%20that%20is%20valid%20across%20multiple%20sources.%0AWe%20propose%20to%20learn%20universal%20knowledge%20graph%20embeddings%20from%20large-scale%0Ainterlinked%20knowledge%20sources.%20To%20this%20end%2C%20we%20fuse%20large%20knowledge%20graphs%0Abased%20on%20the%20owl%3AsameAs%20relation%20such%20that%20every%20entity%20is%20represented%20by%20a%0Aunique%20identity.%20We%20instantiate%20our%20idea%20by%20computing%20universal%20embeddings%0Abased%20on%20DBpedia%20and%20Wikidata%20yielding%20embeddings%20for%20about%20180%20million%0Aentities%2C%2015%20thousand%20relations%2C%20and%201.2%20billion%20triples.%20We%20believe%20our%0Acomputed%20embeddings%20will%20support%20the%20emerging%20field%20of%20graph%20foundation%20models.%0AMoreover%2C%20we%20develop%20a%20convenient%20API%20to%20provide%20embeddings%20as%20a%20service.%0AExperiments%20on%20link%20prediction%20suggest%20that%20universal%20knowledge%20graph%0Aembeddings%20encode%20better%20semantics%20compared%20to%20embeddings%20computed%20on%20a%20single%0Aknowledge%20graph.%20For%20reproducibility%20purposes%2C%20we%20provide%20our%20source%20code%20and%0Adatasets%20open%20access.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14899v2&entry.124074799=Read"},
{"title": "Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models\n  with Adaptive Expert Placement", "author": "Yongji Wu and Wenjie Qu and Tianyang Tao and Zhuang Wang and Wei Bai and Zhuohao Li and Yuan Tian and Jiaheng Zhang and Matthew Lentz and Danyang Zhuo", "abstract": "  Sparsely-activated Mixture-of-Experts (MoE) architecture has increasingly\nbeen adopted to further scale large language models (LLMs) due to its\nsub-linear scaling for computation costs. However, frequent failures still pose\nsignificant challenges as training scales. The cost of even a single failure is\nsignificant, as all GPUs need to wait idle until the failure is resolved,\npotentially losing considerable training progress as training has to restart\nfrom checkpoints. Existing solutions for efficient fault-tolerant training\neither lack elasticity or rely on building resiliency into pipeline\nparallelism, which cannot be applied to MoE models due to the expert\nparallelism strategy adopted by the MoE architecture.\n  We present Lazarus, a system for resilient and elastic training of MoE\nmodels. Lazarus adaptively allocates expert replicas to address the inherent\nimbalance in expert workload and speeds-up training, while a provably optimal\nexpert placement algorithm is developed to maximize the probability of recovery\nupon failures. Through adaptive expert placement and a flexible token\ndispatcher, Lazarus can also fully utilize all available nodes after failures,\nleaving no GPU idle. Our evaluation shows that Lazarus outperforms existing MoE\ntraining systems by up to 5.7x under frequent node failures and 3.4x on a real\nspot instance trace.\n", "link": "http://arxiv.org/abs/2407.04656v1", "date": "2024-07-05", "relevancy": 1.867, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5083}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4616}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lazarus%3A%20Resilient%20and%20Elastic%20Training%20of%20Mixture-of-Experts%20Models%0A%20%20with%20Adaptive%20Expert%20Placement&body=Title%3A%20Lazarus%3A%20Resilient%20and%20Elastic%20Training%20of%20Mixture-of-Experts%20Models%0A%20%20with%20Adaptive%20Expert%20Placement%0AAuthor%3A%20Yongji%20Wu%20and%20Wenjie%20Qu%20and%20Tianyang%20Tao%20and%20Zhuang%20Wang%20and%20Wei%20Bai%20and%20Zhuohao%20Li%20and%20Yuan%20Tian%20and%20Jiaheng%20Zhang%20and%20Matthew%20Lentz%20and%20Danyang%20Zhuo%0AAbstract%3A%20%20%20Sparsely-activated%20Mixture-of-Experts%20%28MoE%29%20architecture%20has%20increasingly%0Abeen%20adopted%20to%20further%20scale%20large%20language%20models%20%28LLMs%29%20due%20to%20its%0Asub-linear%20scaling%20for%20computation%20costs.%20However%2C%20frequent%20failures%20still%20pose%0Asignificant%20challenges%20as%20training%20scales.%20The%20cost%20of%20even%20a%20single%20failure%20is%0Asignificant%2C%20as%20all%20GPUs%20need%20to%20wait%20idle%20until%20the%20failure%20is%20resolved%2C%0Apotentially%20losing%20considerable%20training%20progress%20as%20training%20has%20to%20restart%0Afrom%20checkpoints.%20Existing%20solutions%20for%20efficient%20fault-tolerant%20training%0Aeither%20lack%20elasticity%20or%20rely%20on%20building%20resiliency%20into%20pipeline%0Aparallelism%2C%20which%20cannot%20be%20applied%20to%20MoE%20models%20due%20to%20the%20expert%0Aparallelism%20strategy%20adopted%20by%20the%20MoE%20architecture.%0A%20%20We%20present%20Lazarus%2C%20a%20system%20for%20resilient%20and%20elastic%20training%20of%20MoE%0Amodels.%20Lazarus%20adaptively%20allocates%20expert%20replicas%20to%20address%20the%20inherent%0Aimbalance%20in%20expert%20workload%20and%20speeds-up%20training%2C%20while%20a%20provably%20optimal%0Aexpert%20placement%20algorithm%20is%20developed%20to%20maximize%20the%20probability%20of%20recovery%0Aupon%20failures.%20Through%20adaptive%20expert%20placement%20and%20a%20flexible%20token%0Adispatcher%2C%20Lazarus%20can%20also%20fully%20utilize%20all%20available%20nodes%20after%20failures%2C%0Aleaving%20no%20GPU%20idle.%20Our%20evaluation%20shows%20that%20Lazarus%20outperforms%20existing%20MoE%0Atraining%20systems%20by%20up%20to%205.7x%20under%20frequent%20node%20failures%20and%203.4x%20on%20a%20real%0Aspot%20instance%20trace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLazarus%253A%2520Resilient%2520and%2520Elastic%2520Training%2520of%2520Mixture-of-Experts%2520Models%250A%2520%2520with%2520Adaptive%2520Expert%2520Placement%26entry.906535625%3DYongji%2520Wu%2520and%2520Wenjie%2520Qu%2520and%2520Tianyang%2520Tao%2520and%2520Zhuang%2520Wang%2520and%2520Wei%2520Bai%2520and%2520Zhuohao%2520Li%2520and%2520Yuan%2520Tian%2520and%2520Jiaheng%2520Zhang%2520and%2520Matthew%2520Lentz%2520and%2520Danyang%2520Zhuo%26entry.1292438233%3D%2520%2520Sparsely-activated%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture%2520has%2520increasingly%250Abeen%2520adopted%2520to%2520further%2520scale%2520large%2520language%2520models%2520%2528LLMs%2529%2520due%2520to%2520its%250Asub-linear%2520scaling%2520for%2520computation%2520costs.%2520However%252C%2520frequent%2520failures%2520still%2520pose%250Asignificant%2520challenges%2520as%2520training%2520scales.%2520The%2520cost%2520of%2520even%2520a%2520single%2520failure%2520is%250Asignificant%252C%2520as%2520all%2520GPUs%2520need%2520to%2520wait%2520idle%2520until%2520the%2520failure%2520is%2520resolved%252C%250Apotentially%2520losing%2520considerable%2520training%2520progress%2520as%2520training%2520has%2520to%2520restart%250Afrom%2520checkpoints.%2520Existing%2520solutions%2520for%2520efficient%2520fault-tolerant%2520training%250Aeither%2520lack%2520elasticity%2520or%2520rely%2520on%2520building%2520resiliency%2520into%2520pipeline%250Aparallelism%252C%2520which%2520cannot%2520be%2520applied%2520to%2520MoE%2520models%2520due%2520to%2520the%2520expert%250Aparallelism%2520strategy%2520adopted%2520by%2520the%2520MoE%2520architecture.%250A%2520%2520We%2520present%2520Lazarus%252C%2520a%2520system%2520for%2520resilient%2520and%2520elastic%2520training%2520of%2520MoE%250Amodels.%2520Lazarus%2520adaptively%2520allocates%2520expert%2520replicas%2520to%2520address%2520the%2520inherent%250Aimbalance%2520in%2520expert%2520workload%2520and%2520speeds-up%2520training%252C%2520while%2520a%2520provably%2520optimal%250Aexpert%2520placement%2520algorithm%2520is%2520developed%2520to%2520maximize%2520the%2520probability%2520of%2520recovery%250Aupon%2520failures.%2520Through%2520adaptive%2520expert%2520placement%2520and%2520a%2520flexible%2520token%250Adispatcher%252C%2520Lazarus%2520can%2520also%2520fully%2520utilize%2520all%2520available%2520nodes%2520after%2520failures%252C%250Aleaving%2520no%2520GPU%2520idle.%2520Our%2520evaluation%2520shows%2520that%2520Lazarus%2520outperforms%2520existing%2520MoE%250Atraining%2520systems%2520by%2520up%2520to%25205.7x%2520under%2520frequent%2520node%2520failures%2520and%25203.4x%2520on%2520a%2520real%250Aspot%2520instance%2520trace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lazarus%3A%20Resilient%20and%20Elastic%20Training%20of%20Mixture-of-Experts%20Models%0A%20%20with%20Adaptive%20Expert%20Placement&entry.906535625=Yongji%20Wu%20and%20Wenjie%20Qu%20and%20Tianyang%20Tao%20and%20Zhuang%20Wang%20and%20Wei%20Bai%20and%20Zhuohao%20Li%20and%20Yuan%20Tian%20and%20Jiaheng%20Zhang%20and%20Matthew%20Lentz%20and%20Danyang%20Zhuo&entry.1292438233=%20%20Sparsely-activated%20Mixture-of-Experts%20%28MoE%29%20architecture%20has%20increasingly%0Abeen%20adopted%20to%20further%20scale%20large%20language%20models%20%28LLMs%29%20due%20to%20its%0Asub-linear%20scaling%20for%20computation%20costs.%20However%2C%20frequent%20failures%20still%20pose%0Asignificant%20challenges%20as%20training%20scales.%20The%20cost%20of%20even%20a%20single%20failure%20is%0Asignificant%2C%20as%20all%20GPUs%20need%20to%20wait%20idle%20until%20the%20failure%20is%20resolved%2C%0Apotentially%20losing%20considerable%20training%20progress%20as%20training%20has%20to%20restart%0Afrom%20checkpoints.%20Existing%20solutions%20for%20efficient%20fault-tolerant%20training%0Aeither%20lack%20elasticity%20or%20rely%20on%20building%20resiliency%20into%20pipeline%0Aparallelism%2C%20which%20cannot%20be%20applied%20to%20MoE%20models%20due%20to%20the%20expert%0Aparallelism%20strategy%20adopted%20by%20the%20MoE%20architecture.%0A%20%20We%20present%20Lazarus%2C%20a%20system%20for%20resilient%20and%20elastic%20training%20of%20MoE%0Amodels.%20Lazarus%20adaptively%20allocates%20expert%20replicas%20to%20address%20the%20inherent%0Aimbalance%20in%20expert%20workload%20and%20speeds-up%20training%2C%20while%20a%20provably%20optimal%0Aexpert%20placement%20algorithm%20is%20developed%20to%20maximize%20the%20probability%20of%20recovery%0Aupon%20failures.%20Through%20adaptive%20expert%20placement%20and%20a%20flexible%20token%0Adispatcher%2C%20Lazarus%20can%20also%20fully%20utilize%20all%20available%20nodes%20after%20failures%2C%0Aleaving%20no%20GPU%20idle.%20Our%20evaluation%20shows%20that%20Lazarus%20outperforms%20existing%20MoE%0Atraining%20systems%20by%20up%20to%205.7x%20under%20frequent%20node%20failures%20and%203.4x%20on%20a%20real%0Aspot%20instance%20trace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04656v1&entry.124074799=Read"},
{"title": "Artwork Protection Against Neural Style Transfer Using Locally Adaptive\n  Adversarial Color Attack", "author": "Zhongliang Guo and Junhao Dong and Yifei Qian and Kaixuan Wang and Weiye Li and Ziheng Guo and Yuheng Wang and Yanli Li and Ognjen Arandjelovi\u0107 and Lei Fang", "abstract": "  Neural style transfer (NST) generates new images by combining the style of\none image with the content of another. However, unauthorized NST can exploit\nartwork, raising concerns about artists' rights and motivating the development\nof proactive protection methods. We propose Locally Adaptive Adversarial Color\nAttack (LAACA), empowering artists to protect their artwork from unauthorized\nstyle transfer by processing before public release. By delving into the\nintricacies of human visual perception and the role of different frequency\ncomponents, our method strategically introduces frequency-adaptive\nperturbations in the image. These perturbations significantly degrade the\ngeneration quality of NST while maintaining an acceptable level of visual\nchange in the original image, ensuring that potential infringers are\ndiscouraged from using the protected artworks, because of its bad NST\ngeneration quality. Additionally, existing metrics often overlook the\nimportance of color fidelity in evaluating color-mattered tasks, such as the\nquality of NST-generated images, which is crucial in the context of artistic\nworks. To comprehensively assess the color-mattered tasks, we propose the\nAdversarial Color Distance Metric (ACDM), designed to quantify the color\ndifference of images pre- and post-manipulations. Experimental results confirm\nthat attacking NST using LAACA results in visually inferior style transfer, and\nthe ACDM can efficiently measure color-mattered tasks. By providing artists\nwith a tool to safeguard their intellectual property, our work relieves the\nsocio-technical challenges posed by the misuse of NST in the art community.\n", "link": "http://arxiv.org/abs/2401.09673v3", "date": "2024-07-05", "relevancy": 1.858, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4868}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4609}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artwork%20Protection%20Against%20Neural%20Style%20Transfer%20Using%20Locally%20Adaptive%0A%20%20Adversarial%20Color%20Attack&body=Title%3A%20Artwork%20Protection%20Against%20Neural%20Style%20Transfer%20Using%20Locally%20Adaptive%0A%20%20Adversarial%20Color%20Attack%0AAuthor%3A%20Zhongliang%20Guo%20and%20Junhao%20Dong%20and%20Yifei%20Qian%20and%20Kaixuan%20Wang%20and%20Weiye%20Li%20and%20Ziheng%20Guo%20and%20Yuheng%20Wang%20and%20Yanli%20Li%20and%20Ognjen%20Arandjelovi%C4%87%20and%20Lei%20Fang%0AAbstract%3A%20%20%20Neural%20style%20transfer%20%28NST%29%20generates%20new%20images%20by%20combining%20the%20style%20of%0Aone%20image%20with%20the%20content%20of%20another.%20However%2C%20unauthorized%20NST%20can%20exploit%0Aartwork%2C%20raising%20concerns%20about%20artists%27%20rights%20and%20motivating%20the%20development%0Aof%20proactive%20protection%20methods.%20We%20propose%20Locally%20Adaptive%20Adversarial%20Color%0AAttack%20%28LAACA%29%2C%20empowering%20artists%20to%20protect%20their%20artwork%20from%20unauthorized%0Astyle%20transfer%20by%20processing%20before%20public%20release.%20By%20delving%20into%20the%0Aintricacies%20of%20human%20visual%20perception%20and%20the%20role%20of%20different%20frequency%0Acomponents%2C%20our%20method%20strategically%20introduces%20frequency-adaptive%0Aperturbations%20in%20the%20image.%20These%20perturbations%20significantly%20degrade%20the%0Ageneration%20quality%20of%20NST%20while%20maintaining%20an%20acceptable%20level%20of%20visual%0Achange%20in%20the%20original%20image%2C%20ensuring%20that%20potential%20infringers%20are%0Adiscouraged%20from%20using%20the%20protected%20artworks%2C%20because%20of%20its%20bad%20NST%0Ageneration%20quality.%20Additionally%2C%20existing%20metrics%20often%20overlook%20the%0Aimportance%20of%20color%20fidelity%20in%20evaluating%20color-mattered%20tasks%2C%20such%20as%20the%0Aquality%20of%20NST-generated%20images%2C%20which%20is%20crucial%20in%20the%20context%20of%20artistic%0Aworks.%20To%20comprehensively%20assess%20the%20color-mattered%20tasks%2C%20we%20propose%20the%0AAdversarial%20Color%20Distance%20Metric%20%28ACDM%29%2C%20designed%20to%20quantify%20the%20color%0Adifference%20of%20images%20pre-%20and%20post-manipulations.%20Experimental%20results%20confirm%0Athat%20attacking%20NST%20using%20LAACA%20results%20in%20visually%20inferior%20style%20transfer%2C%20and%0Athe%20ACDM%20can%20efficiently%20measure%20color-mattered%20tasks.%20By%20providing%20artists%0Awith%20a%20tool%20to%20safeguard%20their%20intellectual%20property%2C%20our%20work%20relieves%20the%0Asocio-technical%20challenges%20posed%20by%20the%20misuse%20of%20NST%20in%20the%20art%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09673v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtwork%2520Protection%2520Against%2520Neural%2520Style%2520Transfer%2520Using%2520Locally%2520Adaptive%250A%2520%2520Adversarial%2520Color%2520Attack%26entry.906535625%3DZhongliang%2520Guo%2520and%2520Junhao%2520Dong%2520and%2520Yifei%2520Qian%2520and%2520Kaixuan%2520Wang%2520and%2520Weiye%2520Li%2520and%2520Ziheng%2520Guo%2520and%2520Yuheng%2520Wang%2520and%2520Yanli%2520Li%2520and%2520Ognjen%2520Arandjelovi%25C4%2587%2520and%2520Lei%2520Fang%26entry.1292438233%3D%2520%2520Neural%2520style%2520transfer%2520%2528NST%2529%2520generates%2520new%2520images%2520by%2520combining%2520the%2520style%2520of%250Aone%2520image%2520with%2520the%2520content%2520of%2520another.%2520However%252C%2520unauthorized%2520NST%2520can%2520exploit%250Aartwork%252C%2520raising%2520concerns%2520about%2520artists%2527%2520rights%2520and%2520motivating%2520the%2520development%250Aof%2520proactive%2520protection%2520methods.%2520We%2520propose%2520Locally%2520Adaptive%2520Adversarial%2520Color%250AAttack%2520%2528LAACA%2529%252C%2520empowering%2520artists%2520to%2520protect%2520their%2520artwork%2520from%2520unauthorized%250Astyle%2520transfer%2520by%2520processing%2520before%2520public%2520release.%2520By%2520delving%2520into%2520the%250Aintricacies%2520of%2520human%2520visual%2520perception%2520and%2520the%2520role%2520of%2520different%2520frequency%250Acomponents%252C%2520our%2520method%2520strategically%2520introduces%2520frequency-adaptive%250Aperturbations%2520in%2520the%2520image.%2520These%2520perturbations%2520significantly%2520degrade%2520the%250Ageneration%2520quality%2520of%2520NST%2520while%2520maintaining%2520an%2520acceptable%2520level%2520of%2520visual%250Achange%2520in%2520the%2520original%2520image%252C%2520ensuring%2520that%2520potential%2520infringers%2520are%250Adiscouraged%2520from%2520using%2520the%2520protected%2520artworks%252C%2520because%2520of%2520its%2520bad%2520NST%250Ageneration%2520quality.%2520Additionally%252C%2520existing%2520metrics%2520often%2520overlook%2520the%250Aimportance%2520of%2520color%2520fidelity%2520in%2520evaluating%2520color-mattered%2520tasks%252C%2520such%2520as%2520the%250Aquality%2520of%2520NST-generated%2520images%252C%2520which%2520is%2520crucial%2520in%2520the%2520context%2520of%2520artistic%250Aworks.%2520To%2520comprehensively%2520assess%2520the%2520color-mattered%2520tasks%252C%2520we%2520propose%2520the%250AAdversarial%2520Color%2520Distance%2520Metric%2520%2528ACDM%2529%252C%2520designed%2520to%2520quantify%2520the%2520color%250Adifference%2520of%2520images%2520pre-%2520and%2520post-manipulations.%2520Experimental%2520results%2520confirm%250Athat%2520attacking%2520NST%2520using%2520LAACA%2520results%2520in%2520visually%2520inferior%2520style%2520transfer%252C%2520and%250Athe%2520ACDM%2520can%2520efficiently%2520measure%2520color-mattered%2520tasks.%2520By%2520providing%2520artists%250Awith%2520a%2520tool%2520to%2520safeguard%2520their%2520intellectual%2520property%252C%2520our%2520work%2520relieves%2520the%250Asocio-technical%2520challenges%2520posed%2520by%2520the%2520misuse%2520of%2520NST%2520in%2520the%2520art%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.09673v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artwork%20Protection%20Against%20Neural%20Style%20Transfer%20Using%20Locally%20Adaptive%0A%20%20Adversarial%20Color%20Attack&entry.906535625=Zhongliang%20Guo%20and%20Junhao%20Dong%20and%20Yifei%20Qian%20and%20Kaixuan%20Wang%20and%20Weiye%20Li%20and%20Ziheng%20Guo%20and%20Yuheng%20Wang%20and%20Yanli%20Li%20and%20Ognjen%20Arandjelovi%C4%87%20and%20Lei%20Fang&entry.1292438233=%20%20Neural%20style%20transfer%20%28NST%29%20generates%20new%20images%20by%20combining%20the%20style%20of%0Aone%20image%20with%20the%20content%20of%20another.%20However%2C%20unauthorized%20NST%20can%20exploit%0Aartwork%2C%20raising%20concerns%20about%20artists%27%20rights%20and%20motivating%20the%20development%0Aof%20proactive%20protection%20methods.%20We%20propose%20Locally%20Adaptive%20Adversarial%20Color%0AAttack%20%28LAACA%29%2C%20empowering%20artists%20to%20protect%20their%20artwork%20from%20unauthorized%0Astyle%20transfer%20by%20processing%20before%20public%20release.%20By%20delving%20into%20the%0Aintricacies%20of%20human%20visual%20perception%20and%20the%20role%20of%20different%20frequency%0Acomponents%2C%20our%20method%20strategically%20introduces%20frequency-adaptive%0Aperturbations%20in%20the%20image.%20These%20perturbations%20significantly%20degrade%20the%0Ageneration%20quality%20of%20NST%20while%20maintaining%20an%20acceptable%20level%20of%20visual%0Achange%20in%20the%20original%20image%2C%20ensuring%20that%20potential%20infringers%20are%0Adiscouraged%20from%20using%20the%20protected%20artworks%2C%20because%20of%20its%20bad%20NST%0Ageneration%20quality.%20Additionally%2C%20existing%20metrics%20often%20overlook%20the%0Aimportance%20of%20color%20fidelity%20in%20evaluating%20color-mattered%20tasks%2C%20such%20as%20the%0Aquality%20of%20NST-generated%20images%2C%20which%20is%20crucial%20in%20the%20context%20of%20artistic%0Aworks.%20To%20comprehensively%20assess%20the%20color-mattered%20tasks%2C%20we%20propose%20the%0AAdversarial%20Color%20Distance%20Metric%20%28ACDM%29%2C%20designed%20to%20quantify%20the%20color%0Adifference%20of%20images%20pre-%20and%20post-manipulations.%20Experimental%20results%20confirm%0Athat%20attacking%20NST%20using%20LAACA%20results%20in%20visually%20inferior%20style%20transfer%2C%20and%0Athe%20ACDM%20can%20efficiently%20measure%20color-mattered%20tasks.%20By%20providing%20artists%0Awith%20a%20tool%20to%20safeguard%20their%20intellectual%20property%2C%20our%20work%20relieves%20the%0Asocio-technical%20challenges%20posed%20by%20the%20misuse%20of%20NST%20in%20the%20art%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09673v3&entry.124074799=Read"},
{"title": "The impact of data set similarity and diversity on transfer learning\n  success in time series forecasting", "author": "Claudia Ehrig and Benedikt Sonnleitner and Ursula Neumann and Catherine Cleophas and Germain Forestier", "abstract": "  Pre-trained models have become pivotal in enhancing the efficiency and\naccuracy of time series forecasting on target data sets by leveraging transfer\nlearning. While benchmarks validate the performance of model generalization on\nvarious target data sets, there is no structured research providing similarity\nand diversity measures to explain which characteristics of source and target\ndata lead to transfer learning success. Our study pioneers in systematically\nevaluating the impact of source-target similarity and source diversity on\nzero-shot and fine-tuned forecasting outcomes in terms of accuracy, bias, and\nuncertainty estimation. We investigate these dynamics using pre-trained neural\nnetworks across five public source datasets, applied to forecasting five target\ndata sets, including real-world wholesales data. We identify two feature-based\nsimilarity and diversity measures, finding that source-target similarity\nreduces forecasting bias, while source diversity improves forecasting accuracy\nand uncertainty estimation, but increases the bias.\n", "link": "http://arxiv.org/abs/2404.06198v2", "date": "2024-07-05", "relevancy": 1.8503, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4756}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4551}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20impact%20of%20data%20set%20similarity%20and%20diversity%20on%20transfer%20learning%0A%20%20success%20in%20time%20series%20forecasting&body=Title%3A%20The%20impact%20of%20data%20set%20similarity%20and%20diversity%20on%20transfer%20learning%0A%20%20success%20in%20time%20series%20forecasting%0AAuthor%3A%20Claudia%20Ehrig%20and%20Benedikt%20Sonnleitner%20and%20Ursula%20Neumann%20and%20Catherine%20Cleophas%20and%20Germain%20Forestier%0AAbstract%3A%20%20%20Pre-trained%20models%20have%20become%20pivotal%20in%20enhancing%20the%20efficiency%20and%0Aaccuracy%20of%20time%20series%20forecasting%20on%20target%20data%20sets%20by%20leveraging%20transfer%0Alearning.%20While%20benchmarks%20validate%20the%20performance%20of%20model%20generalization%20on%0Avarious%20target%20data%20sets%2C%20there%20is%20no%20structured%20research%20providing%20similarity%0Aand%20diversity%20measures%20to%20explain%20which%20characteristics%20of%20source%20and%20target%0Adata%20lead%20to%20transfer%20learning%20success.%20Our%20study%20pioneers%20in%20systematically%0Aevaluating%20the%20impact%20of%20source-target%20similarity%20and%20source%20diversity%20on%0Azero-shot%20and%20fine-tuned%20forecasting%20outcomes%20in%20terms%20of%20accuracy%2C%20bias%2C%20and%0Auncertainty%20estimation.%20We%20investigate%20these%20dynamics%20using%20pre-trained%20neural%0Anetworks%20across%20five%20public%20source%20datasets%2C%20applied%20to%20forecasting%20five%20target%0Adata%20sets%2C%20including%20real-world%20wholesales%20data.%20We%20identify%20two%20feature-based%0Asimilarity%20and%20diversity%20measures%2C%20finding%20that%20source-target%20similarity%0Areduces%20forecasting%20bias%2C%20while%20source%20diversity%20improves%20forecasting%20accuracy%0Aand%20uncertainty%20estimation%2C%20but%20increases%20the%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.06198v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520impact%2520of%2520data%2520set%2520similarity%2520and%2520diversity%2520on%2520transfer%2520learning%250A%2520%2520success%2520in%2520time%2520series%2520forecasting%26entry.906535625%3DClaudia%2520Ehrig%2520and%2520Benedikt%2520Sonnleitner%2520and%2520Ursula%2520Neumann%2520and%2520Catherine%2520Cleophas%2520and%2520Germain%2520Forestier%26entry.1292438233%3D%2520%2520Pre-trained%2520models%2520have%2520become%2520pivotal%2520in%2520enhancing%2520the%2520efficiency%2520and%250Aaccuracy%2520of%2520time%2520series%2520forecasting%2520on%2520target%2520data%2520sets%2520by%2520leveraging%2520transfer%250Alearning.%2520While%2520benchmarks%2520validate%2520the%2520performance%2520of%2520model%2520generalization%2520on%250Avarious%2520target%2520data%2520sets%252C%2520there%2520is%2520no%2520structured%2520research%2520providing%2520similarity%250Aand%2520diversity%2520measures%2520to%2520explain%2520which%2520characteristics%2520of%2520source%2520and%2520target%250Adata%2520lead%2520to%2520transfer%2520learning%2520success.%2520Our%2520study%2520pioneers%2520in%2520systematically%250Aevaluating%2520the%2520impact%2520of%2520source-target%2520similarity%2520and%2520source%2520diversity%2520on%250Azero-shot%2520and%2520fine-tuned%2520forecasting%2520outcomes%2520in%2520terms%2520of%2520accuracy%252C%2520bias%252C%2520and%250Auncertainty%2520estimation.%2520We%2520investigate%2520these%2520dynamics%2520using%2520pre-trained%2520neural%250Anetworks%2520across%2520five%2520public%2520source%2520datasets%252C%2520applied%2520to%2520forecasting%2520five%2520target%250Adata%2520sets%252C%2520including%2520real-world%2520wholesales%2520data.%2520We%2520identify%2520two%2520feature-based%250Asimilarity%2520and%2520diversity%2520measures%252C%2520finding%2520that%2520source-target%2520similarity%250Areduces%2520forecasting%2520bias%252C%2520while%2520source%2520diversity%2520improves%2520forecasting%2520accuracy%250Aand%2520uncertainty%2520estimation%252C%2520but%2520increases%2520the%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06198v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20impact%20of%20data%20set%20similarity%20and%20diversity%20on%20transfer%20learning%0A%20%20success%20in%20time%20series%20forecasting&entry.906535625=Claudia%20Ehrig%20and%20Benedikt%20Sonnleitner%20and%20Ursula%20Neumann%20and%20Catherine%20Cleophas%20and%20Germain%20Forestier&entry.1292438233=%20%20Pre-trained%20models%20have%20become%20pivotal%20in%20enhancing%20the%20efficiency%20and%0Aaccuracy%20of%20time%20series%20forecasting%20on%20target%20data%20sets%20by%20leveraging%20transfer%0Alearning.%20While%20benchmarks%20validate%20the%20performance%20of%20model%20generalization%20on%0Avarious%20target%20data%20sets%2C%20there%20is%20no%20structured%20research%20providing%20similarity%0Aand%20diversity%20measures%20to%20explain%20which%20characteristics%20of%20source%20and%20target%0Adata%20lead%20to%20transfer%20learning%20success.%20Our%20study%20pioneers%20in%20systematically%0Aevaluating%20the%20impact%20of%20source-target%20similarity%20and%20source%20diversity%20on%0Azero-shot%20and%20fine-tuned%20forecasting%20outcomes%20in%20terms%20of%20accuracy%2C%20bias%2C%20and%0Auncertainty%20estimation.%20We%20investigate%20these%20dynamics%20using%20pre-trained%20neural%0Anetworks%20across%20five%20public%20source%20datasets%2C%20applied%20to%20forecasting%20five%20target%0Adata%20sets%2C%20including%20real-world%20wholesales%20data.%20We%20identify%20two%20feature-based%0Asimilarity%20and%20diversity%20measures%2C%20finding%20that%20source-target%20similarity%0Areduces%20forecasting%20bias%2C%20while%20source%20diversity%20improves%20forecasting%20accuracy%0Aand%20uncertainty%20estimation%2C%20but%20increases%20the%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.06198v2&entry.124074799=Read"},
{"title": "Unified continuous-time q-learning for mean-field game and mean-field\n  control problems", "author": "Xiaoli Wei and Xiang Yu and Fengyi Yuan", "abstract": "  This paper studies the continuous-time q-learning in the mean-field\njump-diffusion models from the representative agent's perspective. To overcome\nthe challenge when the population distribution may not be directly observable,\nwe introduce the integrated q-function in decoupled form (decoupled\nIq-function) and establish its martingale characterization together with the\nvalue function, which provides a unified policy evaluation rule for both\nmean-field game (MFG) and mean-field control (MFC) problems. Moreover,\ndepending on the task to solve the MFG or MFC problem, we can employ the\ndecoupled Iq-function by different means to learn the mean-field equilibrium\npolicy or the mean-field optimal policy respectively. As a result, we devise a\nunified q-learning algorithm for both MFG and MFC problems by utilizing all\ntest policies stemming from the mean-field interactions. For several examples\nin the jump-diffusion setting, within and beyond the LQ framework, we can\nobtain the exact parameterization of the decoupled Iq-functions and the value\nfunctions, and illustrate our algorithm from the representative agent's\nperspective with satisfactory performance.\n", "link": "http://arxiv.org/abs/2407.04521v1", "date": "2024-07-05", "relevancy": 1.8455, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4712}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4702}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20continuous-time%20q-learning%20for%20mean-field%20game%20and%20mean-field%0A%20%20control%20problems&body=Title%3A%20Unified%20continuous-time%20q-learning%20for%20mean-field%20game%20and%20mean-field%0A%20%20control%20problems%0AAuthor%3A%20Xiaoli%20Wei%20and%20Xiang%20Yu%20and%20Fengyi%20Yuan%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20continuous-time%20q-learning%20in%20the%20mean-field%0Ajump-diffusion%20models%20from%20the%20representative%20agent%27s%20perspective.%20To%20overcome%0Athe%20challenge%20when%20the%20population%20distribution%20may%20not%20be%20directly%20observable%2C%0Awe%20introduce%20the%20integrated%20q-function%20in%20decoupled%20form%20%28decoupled%0AIq-function%29%20and%20establish%20its%20martingale%20characterization%20together%20with%20the%0Avalue%20function%2C%20which%20provides%20a%20unified%20policy%20evaluation%20rule%20for%20both%0Amean-field%20game%20%28MFG%29%20and%20mean-field%20control%20%28MFC%29%20problems.%20Moreover%2C%0Adepending%20on%20the%20task%20to%20solve%20the%20MFG%20or%20MFC%20problem%2C%20we%20can%20employ%20the%0Adecoupled%20Iq-function%20by%20different%20means%20to%20learn%20the%20mean-field%20equilibrium%0Apolicy%20or%20the%20mean-field%20optimal%20policy%20respectively.%20As%20a%20result%2C%20we%20devise%20a%0Aunified%20q-learning%20algorithm%20for%20both%20MFG%20and%20MFC%20problems%20by%20utilizing%20all%0Atest%20policies%20stemming%20from%20the%20mean-field%20interactions.%20For%20several%20examples%0Ain%20the%20jump-diffusion%20setting%2C%20within%20and%20beyond%20the%20LQ%20framework%2C%20we%20can%0Aobtain%20the%20exact%20parameterization%20of%20the%20decoupled%20Iq-functions%20and%20the%20value%0Afunctions%2C%20and%20illustrate%20our%20algorithm%20from%20the%20representative%20agent%27s%0Aperspective%20with%20satisfactory%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520continuous-time%2520q-learning%2520for%2520mean-field%2520game%2520and%2520mean-field%250A%2520%2520control%2520problems%26entry.906535625%3DXiaoli%2520Wei%2520and%2520Xiang%2520Yu%2520and%2520Fengyi%2520Yuan%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520continuous-time%2520q-learning%2520in%2520the%2520mean-field%250Ajump-diffusion%2520models%2520from%2520the%2520representative%2520agent%2527s%2520perspective.%2520To%2520overcome%250Athe%2520challenge%2520when%2520the%2520population%2520distribution%2520may%2520not%2520be%2520directly%2520observable%252C%250Awe%2520introduce%2520the%2520integrated%2520q-function%2520in%2520decoupled%2520form%2520%2528decoupled%250AIq-function%2529%2520and%2520establish%2520its%2520martingale%2520characterization%2520together%2520with%2520the%250Avalue%2520function%252C%2520which%2520provides%2520a%2520unified%2520policy%2520evaluation%2520rule%2520for%2520both%250Amean-field%2520game%2520%2528MFG%2529%2520and%2520mean-field%2520control%2520%2528MFC%2529%2520problems.%2520Moreover%252C%250Adepending%2520on%2520the%2520task%2520to%2520solve%2520the%2520MFG%2520or%2520MFC%2520problem%252C%2520we%2520can%2520employ%2520the%250Adecoupled%2520Iq-function%2520by%2520different%2520means%2520to%2520learn%2520the%2520mean-field%2520equilibrium%250Apolicy%2520or%2520the%2520mean-field%2520optimal%2520policy%2520respectively.%2520As%2520a%2520result%252C%2520we%2520devise%2520a%250Aunified%2520q-learning%2520algorithm%2520for%2520both%2520MFG%2520and%2520MFC%2520problems%2520by%2520utilizing%2520all%250Atest%2520policies%2520stemming%2520from%2520the%2520mean-field%2520interactions.%2520For%2520several%2520examples%250Ain%2520the%2520jump-diffusion%2520setting%252C%2520within%2520and%2520beyond%2520the%2520LQ%2520framework%252C%2520we%2520can%250Aobtain%2520the%2520exact%2520parameterization%2520of%2520the%2520decoupled%2520Iq-functions%2520and%2520the%2520value%250Afunctions%252C%2520and%2520illustrate%2520our%2520algorithm%2520from%2520the%2520representative%2520agent%2527s%250Aperspective%2520with%2520satisfactory%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20continuous-time%20q-learning%20for%20mean-field%20game%20and%20mean-field%0A%20%20control%20problems&entry.906535625=Xiaoli%20Wei%20and%20Xiang%20Yu%20and%20Fengyi%20Yuan&entry.1292438233=%20%20This%20paper%20studies%20the%20continuous-time%20q-learning%20in%20the%20mean-field%0Ajump-diffusion%20models%20from%20the%20representative%20agent%27s%20perspective.%20To%20overcome%0Athe%20challenge%20when%20the%20population%20distribution%20may%20not%20be%20directly%20observable%2C%0Awe%20introduce%20the%20integrated%20q-function%20in%20decoupled%20form%20%28decoupled%0AIq-function%29%20and%20establish%20its%20martingale%20characterization%20together%20with%20the%0Avalue%20function%2C%20which%20provides%20a%20unified%20policy%20evaluation%20rule%20for%20both%0Amean-field%20game%20%28MFG%29%20and%20mean-field%20control%20%28MFC%29%20problems.%20Moreover%2C%0Adepending%20on%20the%20task%20to%20solve%20the%20MFG%20or%20MFC%20problem%2C%20we%20can%20employ%20the%0Adecoupled%20Iq-function%20by%20different%20means%20to%20learn%20the%20mean-field%20equilibrium%0Apolicy%20or%20the%20mean-field%20optimal%20policy%20respectively.%20As%20a%20result%2C%20we%20devise%20a%0Aunified%20q-learning%20algorithm%20for%20both%20MFG%20and%20MFC%20problems%20by%20utilizing%20all%0Atest%20policies%20stemming%20from%20the%20mean-field%20interactions.%20For%20several%20examples%0Ain%20the%20jump-diffusion%20setting%2C%20within%20and%20beyond%20the%20LQ%20framework%2C%20we%20can%0Aobtain%20the%20exact%20parameterization%20of%20the%20decoupled%20Iq-functions%20and%20the%20value%0Afunctions%2C%20and%20illustrate%20our%20algorithm%20from%20the%20representative%20agent%27s%0Aperspective%20with%20satisfactory%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04521v1&entry.124074799=Read"},
{"title": "Steering Llama 2 via Contrastive Activation Addition", "author": "Nina Panickssery and Nick Gabrieli and Julian Schulz and Meg Tong and Evan Hubinger and Alexander Matt Turner", "abstract": "  We introduce Contrastive Activation Addition (CAA), an innovative method for\nsteering language models by modifying their activations during forward passes.\nCAA computes \"steering vectors\" by averaging the difference in residual stream\nactivations between pairs of positive and negative examples of a particular\nbehavior, such as factual versus hallucinatory responses. During inference,\nthese steering vectors are added at all token positions after the user's prompt\nwith either a positive or negative coefficient, allowing precise control over\nthe degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2\nChat using multiple-choice behavioral question datasets and open-ended\ngeneration tasks. We demonstrate that CAA significantly alters model behavior,\nis effective over and on top of traditional methods like finetuning and system\nprompt design, and minimally reduces capabilities. Moreover, we gain deeper\ninsights into CAA's mechanisms by employing various activation space\ninterpretation methods. CAA accurately steers model outputs and sheds light on\nhow high-level concepts are represented in Large Language Models (LLMs).\n", "link": "http://arxiv.org/abs/2312.06681v4", "date": "2024-07-05", "relevancy": 1.8431, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4433}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4412}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Steering%20Llama%202%20via%20Contrastive%20Activation%20Addition&body=Title%3A%20Steering%20Llama%202%20via%20Contrastive%20Activation%20Addition%0AAuthor%3A%20Nina%20Panickssery%20and%20Nick%20Gabrieli%20and%20Julian%20Schulz%20and%20Meg%20Tong%20and%20Evan%20Hubinger%20and%20Alexander%20Matt%20Turner%0AAbstract%3A%20%20%20We%20introduce%20Contrastive%20Activation%20Addition%20%28CAA%29%2C%20an%20innovative%20method%20for%0Asteering%20language%20models%20by%20modifying%20their%20activations%20during%20forward%20passes.%0ACAA%20computes%20%22steering%20vectors%22%20by%20averaging%20the%20difference%20in%20residual%20stream%0Aactivations%20between%20pairs%20of%20positive%20and%20negative%20examples%20of%20a%20particular%0Abehavior%2C%20such%20as%20factual%20versus%20hallucinatory%20responses.%20During%20inference%2C%0Athese%20steering%20vectors%20are%20added%20at%20all%20token%20positions%20after%20the%20user%27s%20prompt%0Awith%20either%20a%20positive%20or%20negative%20coefficient%2C%20allowing%20precise%20control%20over%0Athe%20degree%20of%20the%20targeted%20behavior.%20We%20evaluate%20CAA%27s%20effectiveness%20on%20Llama%202%0AChat%20using%20multiple-choice%20behavioral%20question%20datasets%20and%20open-ended%0Ageneration%20tasks.%20We%20demonstrate%20that%20CAA%20significantly%20alters%20model%20behavior%2C%0Ais%20effective%20over%20and%20on%20top%20of%20traditional%20methods%20like%20finetuning%20and%20system%0Aprompt%20design%2C%20and%20minimally%20reduces%20capabilities.%20Moreover%2C%20we%20gain%20deeper%0Ainsights%20into%20CAA%27s%20mechanisms%20by%20employing%20various%20activation%20space%0Ainterpretation%20methods.%20CAA%20accurately%20steers%20model%20outputs%20and%20sheds%20light%20on%0Ahow%20high-level%20concepts%20are%20represented%20in%20Large%20Language%20Models%20%28LLMs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06681v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSteering%2520Llama%25202%2520via%2520Contrastive%2520Activation%2520Addition%26entry.906535625%3DNina%2520Panickssery%2520and%2520Nick%2520Gabrieli%2520and%2520Julian%2520Schulz%2520and%2520Meg%2520Tong%2520and%2520Evan%2520Hubinger%2520and%2520Alexander%2520Matt%2520Turner%26entry.1292438233%3D%2520%2520We%2520introduce%2520Contrastive%2520Activation%2520Addition%2520%2528CAA%2529%252C%2520an%2520innovative%2520method%2520for%250Asteering%2520language%2520models%2520by%2520modifying%2520their%2520activations%2520during%2520forward%2520passes.%250ACAA%2520computes%2520%2522steering%2520vectors%2522%2520by%2520averaging%2520the%2520difference%2520in%2520residual%2520stream%250Aactivations%2520between%2520pairs%2520of%2520positive%2520and%2520negative%2520examples%2520of%2520a%2520particular%250Abehavior%252C%2520such%2520as%2520factual%2520versus%2520hallucinatory%2520responses.%2520During%2520inference%252C%250Athese%2520steering%2520vectors%2520are%2520added%2520at%2520all%2520token%2520positions%2520after%2520the%2520user%2527s%2520prompt%250Awith%2520either%2520a%2520positive%2520or%2520negative%2520coefficient%252C%2520allowing%2520precise%2520control%2520over%250Athe%2520degree%2520of%2520the%2520targeted%2520behavior.%2520We%2520evaluate%2520CAA%2527s%2520effectiveness%2520on%2520Llama%25202%250AChat%2520using%2520multiple-choice%2520behavioral%2520question%2520datasets%2520and%2520open-ended%250Ageneration%2520tasks.%2520We%2520demonstrate%2520that%2520CAA%2520significantly%2520alters%2520model%2520behavior%252C%250Ais%2520effective%2520over%2520and%2520on%2520top%2520of%2520traditional%2520methods%2520like%2520finetuning%2520and%2520system%250Aprompt%2520design%252C%2520and%2520minimally%2520reduces%2520capabilities.%2520Moreover%252C%2520we%2520gain%2520deeper%250Ainsights%2520into%2520CAA%2527s%2520mechanisms%2520by%2520employing%2520various%2520activation%2520space%250Ainterpretation%2520methods.%2520CAA%2520accurately%2520steers%2520model%2520outputs%2520and%2520sheds%2520light%2520on%250Ahow%2520high-level%2520concepts%2520are%2520represented%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06681v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Steering%20Llama%202%20via%20Contrastive%20Activation%20Addition&entry.906535625=Nina%20Panickssery%20and%20Nick%20Gabrieli%20and%20Julian%20Schulz%20and%20Meg%20Tong%20and%20Evan%20Hubinger%20and%20Alexander%20Matt%20Turner&entry.1292438233=%20%20We%20introduce%20Contrastive%20Activation%20Addition%20%28CAA%29%2C%20an%20innovative%20method%20for%0Asteering%20language%20models%20by%20modifying%20their%20activations%20during%20forward%20passes.%0ACAA%20computes%20%22steering%20vectors%22%20by%20averaging%20the%20difference%20in%20residual%20stream%0Aactivations%20between%20pairs%20of%20positive%20and%20negative%20examples%20of%20a%20particular%0Abehavior%2C%20such%20as%20factual%20versus%20hallucinatory%20responses.%20During%20inference%2C%0Athese%20steering%20vectors%20are%20added%20at%20all%20token%20positions%20after%20the%20user%27s%20prompt%0Awith%20either%20a%20positive%20or%20negative%20coefficient%2C%20allowing%20precise%20control%20over%0Athe%20degree%20of%20the%20targeted%20behavior.%20We%20evaluate%20CAA%27s%20effectiveness%20on%20Llama%202%0AChat%20using%20multiple-choice%20behavioral%20question%20datasets%20and%20open-ended%0Ageneration%20tasks.%20We%20demonstrate%20that%20CAA%20significantly%20alters%20model%20behavior%2C%0Ais%20effective%20over%20and%20on%20top%20of%20traditional%20methods%20like%20finetuning%20and%20system%0Aprompt%20design%2C%20and%20minimally%20reduces%20capabilities.%20Moreover%2C%20we%20gain%20deeper%0Ainsights%20into%20CAA%27s%20mechanisms%20by%20employing%20various%20activation%20space%0Ainterpretation%20methods.%20CAA%20accurately%20steers%20model%20outputs%20and%20sheds%20light%20on%0Ahow%20high-level%20concepts%20are%20represented%20in%20Large%20Language%20Models%20%28LLMs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06681v4&entry.124074799=Read"},
{"title": "Probabilistic Rank and Reward: A Scalable Model for Slate Recommendation", "author": "Imad Aouali and Achraf Ait Sidi Hammou and Otmane Sakhi and David Rohde and Flavian Vasile", "abstract": "  We introduce Probabilistic Rank and Reward (PRR), a scalable probabilistic\nmodel for personalized slate recommendation. Our approach allows off-policy\nestimation of the reward in the scenario where the user interacts with at most\none item from a slate of K items. We show that the probability of a slate being\nsuccessful can be learned efficiently by combining the reward, whether the user\nsuccessfully interacted with the slate, and the rank, the item that was\nselected within the slate. PRR outperforms existing off-policy reward\noptimizing methods and is far more scalable to large action spaces. Moreover,\nPRR allows fast delivery of recommendations powered by maximum inner product\nsearch (MIPS), making it suitable in low latency domains such as computational\nadvertising.\n", "link": "http://arxiv.org/abs/2208.06263v3", "date": "2024-07-05", "relevancy": 1.832, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5155}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.461}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Rank%20and%20Reward%3A%20A%20Scalable%20Model%20for%20Slate%20Recommendation&body=Title%3A%20Probabilistic%20Rank%20and%20Reward%3A%20A%20Scalable%20Model%20for%20Slate%20Recommendation%0AAuthor%3A%20Imad%20Aouali%20and%20Achraf%20Ait%20Sidi%20Hammou%20and%20Otmane%20Sakhi%20and%20David%20Rohde%20and%20Flavian%20Vasile%0AAbstract%3A%20%20%20We%20introduce%20Probabilistic%20Rank%20and%20Reward%20%28PRR%29%2C%20a%20scalable%20probabilistic%0Amodel%20for%20personalized%20slate%20recommendation.%20Our%20approach%20allows%20off-policy%0Aestimation%20of%20the%20reward%20in%20the%20scenario%20where%20the%20user%20interacts%20with%20at%20most%0Aone%20item%20from%20a%20slate%20of%20K%20items.%20We%20show%20that%20the%20probability%20of%20a%20slate%20being%0Asuccessful%20can%20be%20learned%20efficiently%20by%20combining%20the%20reward%2C%20whether%20the%20user%0Asuccessfully%20interacted%20with%20the%20slate%2C%20and%20the%20rank%2C%20the%20item%20that%20was%0Aselected%20within%20the%20slate.%20PRR%20outperforms%20existing%20off-policy%20reward%0Aoptimizing%20methods%20and%20is%20far%20more%20scalable%20to%20large%20action%20spaces.%20Moreover%2C%0APRR%20allows%20fast%20delivery%20of%20recommendations%20powered%20by%20maximum%20inner%20product%0Asearch%20%28MIPS%29%2C%20making%20it%20suitable%20in%20low%20latency%20domains%20such%20as%20computational%0Aadvertising.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.06263v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Rank%2520and%2520Reward%253A%2520A%2520Scalable%2520Model%2520for%2520Slate%2520Recommendation%26entry.906535625%3DImad%2520Aouali%2520and%2520Achraf%2520Ait%2520Sidi%2520Hammou%2520and%2520Otmane%2520Sakhi%2520and%2520David%2520Rohde%2520and%2520Flavian%2520Vasile%26entry.1292438233%3D%2520%2520We%2520introduce%2520Probabilistic%2520Rank%2520and%2520Reward%2520%2528PRR%2529%252C%2520a%2520scalable%2520probabilistic%250Amodel%2520for%2520personalized%2520slate%2520recommendation.%2520Our%2520approach%2520allows%2520off-policy%250Aestimation%2520of%2520the%2520reward%2520in%2520the%2520scenario%2520where%2520the%2520user%2520interacts%2520with%2520at%2520most%250Aone%2520item%2520from%2520a%2520slate%2520of%2520K%2520items.%2520We%2520show%2520that%2520the%2520probability%2520of%2520a%2520slate%2520being%250Asuccessful%2520can%2520be%2520learned%2520efficiently%2520by%2520combining%2520the%2520reward%252C%2520whether%2520the%2520user%250Asuccessfully%2520interacted%2520with%2520the%2520slate%252C%2520and%2520the%2520rank%252C%2520the%2520item%2520that%2520was%250Aselected%2520within%2520the%2520slate.%2520PRR%2520outperforms%2520existing%2520off-policy%2520reward%250Aoptimizing%2520methods%2520and%2520is%2520far%2520more%2520scalable%2520to%2520large%2520action%2520spaces.%2520Moreover%252C%250APRR%2520allows%2520fast%2520delivery%2520of%2520recommendations%2520powered%2520by%2520maximum%2520inner%2520product%250Asearch%2520%2528MIPS%2529%252C%2520making%2520it%2520suitable%2520in%2520low%2520latency%2520domains%2520such%2520as%2520computational%250Aadvertising.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.06263v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Rank%20and%20Reward%3A%20A%20Scalable%20Model%20for%20Slate%20Recommendation&entry.906535625=Imad%20Aouali%20and%20Achraf%20Ait%20Sidi%20Hammou%20and%20Otmane%20Sakhi%20and%20David%20Rohde%20and%20Flavian%20Vasile&entry.1292438233=%20%20We%20introduce%20Probabilistic%20Rank%20and%20Reward%20%28PRR%29%2C%20a%20scalable%20probabilistic%0Amodel%20for%20personalized%20slate%20recommendation.%20Our%20approach%20allows%20off-policy%0Aestimation%20of%20the%20reward%20in%20the%20scenario%20where%20the%20user%20interacts%20with%20at%20most%0Aone%20item%20from%20a%20slate%20of%20K%20items.%20We%20show%20that%20the%20probability%20of%20a%20slate%20being%0Asuccessful%20can%20be%20learned%20efficiently%20by%20combining%20the%20reward%2C%20whether%20the%20user%0Asuccessfully%20interacted%20with%20the%20slate%2C%20and%20the%20rank%2C%20the%20item%20that%20was%0Aselected%20within%20the%20slate.%20PRR%20outperforms%20existing%20off-policy%20reward%0Aoptimizing%20methods%20and%20is%20far%20more%20scalable%20to%20large%20action%20spaces.%20Moreover%2C%0APRR%20allows%20fast%20delivery%20of%20recommendations%20powered%20by%20maximum%20inner%20product%0Asearch%20%28MIPS%29%2C%20making%20it%20suitable%20in%20low%20latency%20domains%20such%20as%20computational%0Aadvertising.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.06263v3&entry.124074799=Read"},
{"title": "Spontaneous Reward Hacking in Iterative Self-Refinement", "author": "Jane Pan and He He and Samuel R. Bowman and Shi Feng", "abstract": "  Language models are capable of iteratively improving their outputs based on\nnatural language feedback, thus enabling in-context optimization of user\npreference. In place of human users, a second language model can be used as an\nevaluator, providing feedback along with numerical ratings which the generator\nattempts to optimize. However, because the evaluator is an imperfect proxy of\nuser preference, this optimization can lead to reward hacking, where the\nevaluator's ratings improve while the generation quality remains stagnant or\neven decreases as judged by actual user preference. The concern of reward\nhacking is heightened in iterative self-refinement where the generator and the\nevaluator use the same underlying language model, in which case the\noptimization pressure can drive them to exploit shared vulnerabilities. Using\nan essay editing task, we show that iterative self-refinement leads to\ndeviation between the language model evaluator and human judgment,\ndemonstrating that reward hacking can occur spontaneously in-context with the\nuse of iterative self-refinement. In addition, we study conditions under which\nreward hacking occurs and observe two factors that affect reward hacking\nseverity: model size and context sharing between the generator and the\nevaluator.\n", "link": "http://arxiv.org/abs/2407.04549v1", "date": "2024-07-05", "relevancy": 1.8303, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4727}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.459}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spontaneous%20Reward%20Hacking%20in%20Iterative%20Self-Refinement&body=Title%3A%20Spontaneous%20Reward%20Hacking%20in%20Iterative%20Self-Refinement%0AAuthor%3A%20Jane%20Pan%20and%20He%20He%20and%20Samuel%20R.%20Bowman%20and%20Shi%20Feng%0AAbstract%3A%20%20%20Language%20models%20are%20capable%20of%20iteratively%20improving%20their%20outputs%20based%20on%0Anatural%20language%20feedback%2C%20thus%20enabling%20in-context%20optimization%20of%20user%0Apreference.%20In%20place%20of%20human%20users%2C%20a%20second%20language%20model%20can%20be%20used%20as%20an%0Aevaluator%2C%20providing%20feedback%20along%20with%20numerical%20ratings%20which%20the%20generator%0Aattempts%20to%20optimize.%20However%2C%20because%20the%20evaluator%20is%20an%20imperfect%20proxy%20of%0Auser%20preference%2C%20this%20optimization%20can%20lead%20to%20reward%20hacking%2C%20where%20the%0Aevaluator%27s%20ratings%20improve%20while%20the%20generation%20quality%20remains%20stagnant%20or%0Aeven%20decreases%20as%20judged%20by%20actual%20user%20preference.%20The%20concern%20of%20reward%0Ahacking%20is%20heightened%20in%20iterative%20self-refinement%20where%20the%20generator%20and%20the%0Aevaluator%20use%20the%20same%20underlying%20language%20model%2C%20in%20which%20case%20the%0Aoptimization%20pressure%20can%20drive%20them%20to%20exploit%20shared%20vulnerabilities.%20Using%0Aan%20essay%20editing%20task%2C%20we%20show%20that%20iterative%20self-refinement%20leads%20to%0Adeviation%20between%20the%20language%20model%20evaluator%20and%20human%20judgment%2C%0Ademonstrating%20that%20reward%20hacking%20can%20occur%20spontaneously%20in-context%20with%20the%0Ause%20of%20iterative%20self-refinement.%20In%20addition%2C%20we%20study%20conditions%20under%20which%0Areward%20hacking%20occurs%20and%20observe%20two%20factors%20that%20affect%20reward%20hacking%0Aseverity%3A%20model%20size%20and%20context%20sharing%20between%20the%20generator%20and%20the%0Aevaluator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04549v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpontaneous%2520Reward%2520Hacking%2520in%2520Iterative%2520Self-Refinement%26entry.906535625%3DJane%2520Pan%2520and%2520He%2520He%2520and%2520Samuel%2520R.%2520Bowman%2520and%2520Shi%2520Feng%26entry.1292438233%3D%2520%2520Language%2520models%2520are%2520capable%2520of%2520iteratively%2520improving%2520their%2520outputs%2520based%2520on%250Anatural%2520language%2520feedback%252C%2520thus%2520enabling%2520in-context%2520optimization%2520of%2520user%250Apreference.%2520In%2520place%2520of%2520human%2520users%252C%2520a%2520second%2520language%2520model%2520can%2520be%2520used%2520as%2520an%250Aevaluator%252C%2520providing%2520feedback%2520along%2520with%2520numerical%2520ratings%2520which%2520the%2520generator%250Aattempts%2520to%2520optimize.%2520However%252C%2520because%2520the%2520evaluator%2520is%2520an%2520imperfect%2520proxy%2520of%250Auser%2520preference%252C%2520this%2520optimization%2520can%2520lead%2520to%2520reward%2520hacking%252C%2520where%2520the%250Aevaluator%2527s%2520ratings%2520improve%2520while%2520the%2520generation%2520quality%2520remains%2520stagnant%2520or%250Aeven%2520decreases%2520as%2520judged%2520by%2520actual%2520user%2520preference.%2520The%2520concern%2520of%2520reward%250Ahacking%2520is%2520heightened%2520in%2520iterative%2520self-refinement%2520where%2520the%2520generator%2520and%2520the%250Aevaluator%2520use%2520the%2520same%2520underlying%2520language%2520model%252C%2520in%2520which%2520case%2520the%250Aoptimization%2520pressure%2520can%2520drive%2520them%2520to%2520exploit%2520shared%2520vulnerabilities.%2520Using%250Aan%2520essay%2520editing%2520task%252C%2520we%2520show%2520that%2520iterative%2520self-refinement%2520leads%2520to%250Adeviation%2520between%2520the%2520language%2520model%2520evaluator%2520and%2520human%2520judgment%252C%250Ademonstrating%2520that%2520reward%2520hacking%2520can%2520occur%2520spontaneously%2520in-context%2520with%2520the%250Ause%2520of%2520iterative%2520self-refinement.%2520In%2520addition%252C%2520we%2520study%2520conditions%2520under%2520which%250Areward%2520hacking%2520occurs%2520and%2520observe%2520two%2520factors%2520that%2520affect%2520reward%2520hacking%250Aseverity%253A%2520model%2520size%2520and%2520context%2520sharing%2520between%2520the%2520generator%2520and%2520the%250Aevaluator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04549v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spontaneous%20Reward%20Hacking%20in%20Iterative%20Self-Refinement&entry.906535625=Jane%20Pan%20and%20He%20He%20and%20Samuel%20R.%20Bowman%20and%20Shi%20Feng&entry.1292438233=%20%20Language%20models%20are%20capable%20of%20iteratively%20improving%20their%20outputs%20based%20on%0Anatural%20language%20feedback%2C%20thus%20enabling%20in-context%20optimization%20of%20user%0Apreference.%20In%20place%20of%20human%20users%2C%20a%20second%20language%20model%20can%20be%20used%20as%20an%0Aevaluator%2C%20providing%20feedback%20along%20with%20numerical%20ratings%20which%20the%20generator%0Aattempts%20to%20optimize.%20However%2C%20because%20the%20evaluator%20is%20an%20imperfect%20proxy%20of%0Auser%20preference%2C%20this%20optimization%20can%20lead%20to%20reward%20hacking%2C%20where%20the%0Aevaluator%27s%20ratings%20improve%20while%20the%20generation%20quality%20remains%20stagnant%20or%0Aeven%20decreases%20as%20judged%20by%20actual%20user%20preference.%20The%20concern%20of%20reward%0Ahacking%20is%20heightened%20in%20iterative%20self-refinement%20where%20the%20generator%20and%20the%0Aevaluator%20use%20the%20same%20underlying%20language%20model%2C%20in%20which%20case%20the%0Aoptimization%20pressure%20can%20drive%20them%20to%20exploit%20shared%20vulnerabilities.%20Using%0Aan%20essay%20editing%20task%2C%20we%20show%20that%20iterative%20self-refinement%20leads%20to%0Adeviation%20between%20the%20language%20model%20evaluator%20and%20human%20judgment%2C%0Ademonstrating%20that%20reward%20hacking%20can%20occur%20spontaneously%20in-context%20with%20the%0Ause%20of%20iterative%20self-refinement.%20In%20addition%2C%20we%20study%20conditions%20under%20which%0Areward%20hacking%20occurs%20and%20observe%20two%20factors%20that%20affect%20reward%20hacking%0Aseverity%3A%20model%20size%20and%20context%20sharing%20between%20the%20generator%20and%20the%0Aevaluator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04549v1&entry.124074799=Read"},
{"title": "Introducing 'Inside' Out of Distribution", "author": "Teddy Lazebnik", "abstract": "  Detecting and understanding out-of-distribution (OOD) samples is crucial in\nmachine learning (ML) to ensure reliable model performance. Current OOD\nstudies, in general, and in the context of ML, in particular, primarily focus\non extrapolatory OOD (outside), neglecting potential cases of interpolatory OOD\n(inside). This study introduces a novel perspective on OOD by suggesting OOD\ncan be divided into inside and outside cases. In addition, following this\nframework, we examine the inside-outside OOD profiles of datasets and their\nimpact on ML model performance. Our analysis shows that different\ninside-outside OOD profiles lead to nuanced declines in ML model performance,\nhighlighting the importance of distinguishing between these two cases for\ndeveloping effective counter-OOD methods.\n", "link": "http://arxiv.org/abs/2407.04534v1", "date": "2024-07-05", "relevancy": 1.8208, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4571}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4568}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Introducing%20%27Inside%27%20Out%20of%20Distribution&body=Title%3A%20Introducing%20%27Inside%27%20Out%20of%20Distribution%0AAuthor%3A%20Teddy%20Lazebnik%0AAbstract%3A%20%20%20Detecting%20and%20understanding%20out-of-distribution%20%28OOD%29%20samples%20is%20crucial%20in%0Amachine%20learning%20%28ML%29%20to%20ensure%20reliable%20model%20performance.%20Current%20OOD%0Astudies%2C%20in%20general%2C%20and%20in%20the%20context%20of%20ML%2C%20in%20particular%2C%20primarily%20focus%0Aon%20extrapolatory%20OOD%20%28outside%29%2C%20neglecting%20potential%20cases%20of%20interpolatory%20OOD%0A%28inside%29.%20This%20study%20introduces%20a%20novel%20perspective%20on%20OOD%20by%20suggesting%20OOD%0Acan%20be%20divided%20into%20inside%20and%20outside%20cases.%20In%20addition%2C%20following%20this%0Aframework%2C%20we%20examine%20the%20inside-outside%20OOD%20profiles%20of%20datasets%20and%20their%0Aimpact%20on%20ML%20model%20performance.%20Our%20analysis%20shows%20that%20different%0Ainside-outside%20OOD%20profiles%20lead%20to%20nuanced%20declines%20in%20ML%20model%20performance%2C%0Ahighlighting%20the%20importance%20of%20distinguishing%20between%20these%20two%20cases%20for%0Adeveloping%20effective%20counter-OOD%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntroducing%2520%2527Inside%2527%2520Out%2520of%2520Distribution%26entry.906535625%3DTeddy%2520Lazebnik%26entry.1292438233%3D%2520%2520Detecting%2520and%2520understanding%2520out-of-distribution%2520%2528OOD%2529%2520samples%2520is%2520crucial%2520in%250Amachine%2520learning%2520%2528ML%2529%2520to%2520ensure%2520reliable%2520model%2520performance.%2520Current%2520OOD%250Astudies%252C%2520in%2520general%252C%2520and%2520in%2520the%2520context%2520of%2520ML%252C%2520in%2520particular%252C%2520primarily%2520focus%250Aon%2520extrapolatory%2520OOD%2520%2528outside%2529%252C%2520neglecting%2520potential%2520cases%2520of%2520interpolatory%2520OOD%250A%2528inside%2529.%2520This%2520study%2520introduces%2520a%2520novel%2520perspective%2520on%2520OOD%2520by%2520suggesting%2520OOD%250Acan%2520be%2520divided%2520into%2520inside%2520and%2520outside%2520cases.%2520In%2520addition%252C%2520following%2520this%250Aframework%252C%2520we%2520examine%2520the%2520inside-outside%2520OOD%2520profiles%2520of%2520datasets%2520and%2520their%250Aimpact%2520on%2520ML%2520model%2520performance.%2520Our%2520analysis%2520shows%2520that%2520different%250Ainside-outside%2520OOD%2520profiles%2520lead%2520to%2520nuanced%2520declines%2520in%2520ML%2520model%2520performance%252C%250Ahighlighting%2520the%2520importance%2520of%2520distinguishing%2520between%2520these%2520two%2520cases%2520for%250Adeveloping%2520effective%2520counter-OOD%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20%27Inside%27%20Out%20of%20Distribution&entry.906535625=Teddy%20Lazebnik&entry.1292438233=%20%20Detecting%20and%20understanding%20out-of-distribution%20%28OOD%29%20samples%20is%20crucial%20in%0Amachine%20learning%20%28ML%29%20to%20ensure%20reliable%20model%20performance.%20Current%20OOD%0Astudies%2C%20in%20general%2C%20and%20in%20the%20context%20of%20ML%2C%20in%20particular%2C%20primarily%20focus%0Aon%20extrapolatory%20OOD%20%28outside%29%2C%20neglecting%20potential%20cases%20of%20interpolatory%20OOD%0A%28inside%29.%20This%20study%20introduces%20a%20novel%20perspective%20on%20OOD%20by%20suggesting%20OOD%0Acan%20be%20divided%20into%20inside%20and%20outside%20cases.%20In%20addition%2C%20following%20this%0Aframework%2C%20we%20examine%20the%20inside-outside%20OOD%20profiles%20of%20datasets%20and%20their%0Aimpact%20on%20ML%20model%20performance.%20Our%20analysis%20shows%20that%20different%0Ainside-outside%20OOD%20profiles%20lead%20to%20nuanced%20declines%20in%20ML%20model%20performance%2C%0Ahighlighting%20the%20importance%20of%20distinguishing%20between%20these%20two%20cases%20for%0Adeveloping%20effective%20counter-OOD%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04534v1&entry.124074799=Read"},
{"title": "GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning", "author": "Aleksander Ficek and Jiaqi Zeng and Oleksii Kuchaiev", "abstract": "  Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis of between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.\n", "link": "http://arxiv.org/abs/2407.04528v1", "date": "2024-07-05", "relevancy": 1.8151, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4962}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4462}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPT%20vs%20RETRO%3A%20Exploring%20the%20Intersection%20of%20Retrieval%20and%0A%20%20Parameter-Efficient%20Fine-Tuning&body=Title%3A%20GPT%20vs%20RETRO%3A%20Exploring%20the%20Intersection%20of%20Retrieval%20and%0A%20%20Parameter-Efficient%20Fine-Tuning%0AAuthor%3A%20Aleksander%20Ficek%20and%20Jiaqi%20Zeng%20and%20Oleksii%20Kuchaiev%0AAbstract%3A%20%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20and%20Retrieval-Augmented%20Generation%0A%28RAG%29%20have%20become%20popular%20methods%20for%20adapting%20large%20language%20models%20while%0Aminimizing%20compute%20requirements.%20In%20this%20paper%2C%20we%20apply%20PEFT%20methods%0A%28P-tuning%2C%20Adapters%2C%20and%20LoRA%29%20to%20a%20modified%20Retrieval-Enhanced%20Transformer%0A%28RETRO%29%20and%20a%20baseline%20GPT%20model%20across%20several%20sizes%2C%20ranging%20from%20823%20million%0Ato%2048%20billion%20parameters.%20We%20show%20that%20RETRO%20models%20outperform%20GPT%20models%20in%0Azero-shot%20settings%20due%20to%20their%20unique%20pre-training%20process%20but%20GPT%20models%20have%0Ahigher%20performance%20potential%20with%20PEFT.%20Additionally%2C%20our%20study%20indicates%20that%0A8B%20parameter%20models%20strike%20an%20optimal%20balance%20between%20cost%20and%20performance%20and%0AP-tuning%20lags%20behind%20other%20PEFT%20techniques.%20We%20further%20provide%20a%20comparative%0Aanalysis%20of%20between%20applying%20PEFT%20to%20an%20Instruction-tuned%20RETRO%20model%20and%20base%0ARETRO%20model.%20This%20work%20presents%20the%20first%20comprehensive%20comparison%20of%20various%0APEFT%20methods%20integrated%20with%20RAG%2C%20applied%20to%20both%20GPT%20and%20RETRO%20models%2C%0Ahighlighting%20their%20relative%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPT%2520vs%2520RETRO%253A%2520Exploring%2520the%2520Intersection%2520of%2520Retrieval%2520and%250A%2520%2520Parameter-Efficient%2520Fine-Tuning%26entry.906535625%3DAleksander%2520Ficek%2520and%2520Jiaqi%2520Zeng%2520and%2520Oleksii%2520Kuchaiev%26entry.1292438233%3D%2520%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520and%2520Retrieval-Augmented%2520Generation%250A%2528RAG%2529%2520have%2520become%2520popular%2520methods%2520for%2520adapting%2520large%2520language%2520models%2520while%250Aminimizing%2520compute%2520requirements.%2520In%2520this%2520paper%252C%2520we%2520apply%2520PEFT%2520methods%250A%2528P-tuning%252C%2520Adapters%252C%2520and%2520LoRA%2529%2520to%2520a%2520modified%2520Retrieval-Enhanced%2520Transformer%250A%2528RETRO%2529%2520and%2520a%2520baseline%2520GPT%2520model%2520across%2520several%2520sizes%252C%2520ranging%2520from%2520823%2520million%250Ato%252048%2520billion%2520parameters.%2520We%2520show%2520that%2520RETRO%2520models%2520outperform%2520GPT%2520models%2520in%250Azero-shot%2520settings%2520due%2520to%2520their%2520unique%2520pre-training%2520process%2520but%2520GPT%2520models%2520have%250Ahigher%2520performance%2520potential%2520with%2520PEFT.%2520Additionally%252C%2520our%2520study%2520indicates%2520that%250A8B%2520parameter%2520models%2520strike%2520an%2520optimal%2520balance%2520between%2520cost%2520and%2520performance%2520and%250AP-tuning%2520lags%2520behind%2520other%2520PEFT%2520techniques.%2520We%2520further%2520provide%2520a%2520comparative%250Aanalysis%2520of%2520between%2520applying%2520PEFT%2520to%2520an%2520Instruction-tuned%2520RETRO%2520model%2520and%2520base%250ARETRO%2520model.%2520This%2520work%2520presents%2520the%2520first%2520comprehensive%2520comparison%2520of%2520various%250APEFT%2520methods%2520integrated%2520with%2520RAG%252C%2520applied%2520to%2520both%2520GPT%2520and%2520RETRO%2520models%252C%250Ahighlighting%2520their%2520relative%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPT%20vs%20RETRO%3A%20Exploring%20the%20Intersection%20of%20Retrieval%20and%0A%20%20Parameter-Efficient%20Fine-Tuning&entry.906535625=Aleksander%20Ficek%20and%20Jiaqi%20Zeng%20and%20Oleksii%20Kuchaiev&entry.1292438233=%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20and%20Retrieval-Augmented%20Generation%0A%28RAG%29%20have%20become%20popular%20methods%20for%20adapting%20large%20language%20models%20while%0Aminimizing%20compute%20requirements.%20In%20this%20paper%2C%20we%20apply%20PEFT%20methods%0A%28P-tuning%2C%20Adapters%2C%20and%20LoRA%29%20to%20a%20modified%20Retrieval-Enhanced%20Transformer%0A%28RETRO%29%20and%20a%20baseline%20GPT%20model%20across%20several%20sizes%2C%20ranging%20from%20823%20million%0Ato%2048%20billion%20parameters.%20We%20show%20that%20RETRO%20models%20outperform%20GPT%20models%20in%0Azero-shot%20settings%20due%20to%20their%20unique%20pre-training%20process%20but%20GPT%20models%20have%0Ahigher%20performance%20potential%20with%20PEFT.%20Additionally%2C%20our%20study%20indicates%20that%0A8B%20parameter%20models%20strike%20an%20optimal%20balance%20between%20cost%20and%20performance%20and%0AP-tuning%20lags%20behind%20other%20PEFT%20techniques.%20We%20further%20provide%20a%20comparative%0Aanalysis%20of%20between%20applying%20PEFT%20to%20an%20Instruction-tuned%20RETRO%20model%20and%20base%0ARETRO%20model.%20This%20work%20presents%20the%20first%20comprehensive%20comparison%20of%20various%0APEFT%20methods%20integrated%20with%20RAG%2C%20applied%20to%20both%20GPT%20and%20RETRO%20models%2C%0Ahighlighting%20their%20relative%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04528v1&entry.124074799=Read"},
{"title": "Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image\n  Diffusion Model Training", "author": "Xinyan Chen and Jiaxin Ge and Tianjun Zhang and Jiaming Liu and Shanghang Zhang", "abstract": "  Diffusion models have shown impressive performance in many domains, including\nimage generation, time series prediction, and reinforcement learning. The\nalgorithm demonstrates superior performance over the traditional GAN and\ntransformer-based methods. However, the model's capability to follow natural\nlanguage instructions (e.g., spatial relationships between objects, generating\ncomplex scenes) is still unsatisfactory. It has been an important research area\nto enhance such capability. Prior works have shown that using Reinforcement\nLearning can effectively train diffusion models to enhance fidelity on specific\nobjectives. However, existing RL methods require collecting a large amount of\ndata to train an effective reward model. They also don't receive feedback when\nthe generated image is incorrect. In this work, we propose Iterative Prompt\nRelabeling (IPR), a novel algorithm that aligns images to text through\niterative image sampling and prompt relabeling. IPR first samples a batch of\nimages conditioned on the text then relabels the text prompts of unmatched\ntext-image pairs with classifier feedback. We conduct thorough experiments on\nSDv2 and SDXL, testing their capability to follow instructions on spatial\nrelations. With IPR, we improved up to 15.22% (absolute improvement) on the\nchallenging spatial relation VISOR benchmark, demonstrating superior\nperformance compared to previous RL methods.\n", "link": "http://arxiv.org/abs/2312.16204v2", "date": "2024-07-05", "relevancy": 1.8122, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6474}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6327}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Mistakes%3A%20Iterative%20Prompt%20Relabeling%20for%20Text-to-Image%0A%20%20Diffusion%20Model%20Training&body=Title%3A%20Learning%20from%20Mistakes%3A%20Iterative%20Prompt%20Relabeling%20for%20Text-to-Image%0A%20%20Diffusion%20Model%20Training%0AAuthor%3A%20Xinyan%20Chen%20and%20Jiaxin%20Ge%20and%20Tianjun%20Zhang%20and%20Jiaming%20Liu%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20shown%20impressive%20performance%20in%20many%20domains%2C%20including%0Aimage%20generation%2C%20time%20series%20prediction%2C%20and%20reinforcement%20learning.%20The%0Aalgorithm%20demonstrates%20superior%20performance%20over%20the%20traditional%20GAN%20and%0Atransformer-based%20methods.%20However%2C%20the%20model%27s%20capability%20to%20follow%20natural%0Alanguage%20instructions%20%28e.g.%2C%20spatial%20relationships%20between%20objects%2C%20generating%0Acomplex%20scenes%29%20is%20still%20unsatisfactory.%20It%20has%20been%20an%20important%20research%20area%0Ato%20enhance%20such%20capability.%20Prior%20works%20have%20shown%20that%20using%20Reinforcement%0ALearning%20can%20effectively%20train%20diffusion%20models%20to%20enhance%20fidelity%20on%20specific%0Aobjectives.%20However%2C%20existing%20RL%20methods%20require%20collecting%20a%20large%20amount%20of%0Adata%20to%20train%20an%20effective%20reward%20model.%20They%20also%20don%27t%20receive%20feedback%20when%0Athe%20generated%20image%20is%20incorrect.%20In%20this%20work%2C%20we%20propose%20Iterative%20Prompt%0ARelabeling%20%28IPR%29%2C%20a%20novel%20algorithm%20that%20aligns%20images%20to%20text%20through%0Aiterative%20image%20sampling%20and%20prompt%20relabeling.%20IPR%20first%20samples%20a%20batch%20of%0Aimages%20conditioned%20on%20the%20text%20then%20relabels%20the%20text%20prompts%20of%20unmatched%0Atext-image%20pairs%20with%20classifier%20feedback.%20We%20conduct%20thorough%20experiments%20on%0ASDv2%20and%20SDXL%2C%20testing%20their%20capability%20to%20follow%20instructions%20on%20spatial%0Arelations.%20With%20IPR%2C%20we%20improved%20up%20to%2015.22%25%20%28absolute%20improvement%29%20on%20the%0Achallenging%20spatial%20relation%20VISOR%20benchmark%2C%20demonstrating%20superior%0Aperformance%20compared%20to%20previous%20RL%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16204v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Mistakes%253A%2520Iterative%2520Prompt%2520Relabeling%2520for%2520Text-to-Image%250A%2520%2520Diffusion%2520Model%2520Training%26entry.906535625%3DXinyan%2520Chen%2520and%2520Jiaxin%2520Ge%2520and%2520Tianjun%2520Zhang%2520and%2520Jiaming%2520Liu%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520shown%2520impressive%2520performance%2520in%2520many%2520domains%252C%2520including%250Aimage%2520generation%252C%2520time%2520series%2520prediction%252C%2520and%2520reinforcement%2520learning.%2520The%250Aalgorithm%2520demonstrates%2520superior%2520performance%2520over%2520the%2520traditional%2520GAN%2520and%250Atransformer-based%2520methods.%2520However%252C%2520the%2520model%2527s%2520capability%2520to%2520follow%2520natural%250Alanguage%2520instructions%2520%2528e.g.%252C%2520spatial%2520relationships%2520between%2520objects%252C%2520generating%250Acomplex%2520scenes%2529%2520is%2520still%2520unsatisfactory.%2520It%2520has%2520been%2520an%2520important%2520research%2520area%250Ato%2520enhance%2520such%2520capability.%2520Prior%2520works%2520have%2520shown%2520that%2520using%2520Reinforcement%250ALearning%2520can%2520effectively%2520train%2520diffusion%2520models%2520to%2520enhance%2520fidelity%2520on%2520specific%250Aobjectives.%2520However%252C%2520existing%2520RL%2520methods%2520require%2520collecting%2520a%2520large%2520amount%2520of%250Adata%2520to%2520train%2520an%2520effective%2520reward%2520model.%2520They%2520also%2520don%2527t%2520receive%2520feedback%2520when%250Athe%2520generated%2520image%2520is%2520incorrect.%2520In%2520this%2520work%252C%2520we%2520propose%2520Iterative%2520Prompt%250ARelabeling%2520%2528IPR%2529%252C%2520a%2520novel%2520algorithm%2520that%2520aligns%2520images%2520to%2520text%2520through%250Aiterative%2520image%2520sampling%2520and%2520prompt%2520relabeling.%2520IPR%2520first%2520samples%2520a%2520batch%2520of%250Aimages%2520conditioned%2520on%2520the%2520text%2520then%2520relabels%2520the%2520text%2520prompts%2520of%2520unmatched%250Atext-image%2520pairs%2520with%2520classifier%2520feedback.%2520We%2520conduct%2520thorough%2520experiments%2520on%250ASDv2%2520and%2520SDXL%252C%2520testing%2520their%2520capability%2520to%2520follow%2520instructions%2520on%2520spatial%250Arelations.%2520With%2520IPR%252C%2520we%2520improved%2520up%2520to%252015.22%2525%2520%2528absolute%2520improvement%2529%2520on%2520the%250Achallenging%2520spatial%2520relation%2520VISOR%2520benchmark%252C%2520demonstrating%2520superior%250Aperformance%2520compared%2520to%2520previous%2520RL%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.16204v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Mistakes%3A%20Iterative%20Prompt%20Relabeling%20for%20Text-to-Image%0A%20%20Diffusion%20Model%20Training&entry.906535625=Xinyan%20Chen%20and%20Jiaxin%20Ge%20and%20Tianjun%20Zhang%20and%20Jiaming%20Liu%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Diffusion%20models%20have%20shown%20impressive%20performance%20in%20many%20domains%2C%20including%0Aimage%20generation%2C%20time%20series%20prediction%2C%20and%20reinforcement%20learning.%20The%0Aalgorithm%20demonstrates%20superior%20performance%20over%20the%20traditional%20GAN%20and%0Atransformer-based%20methods.%20However%2C%20the%20model%27s%20capability%20to%20follow%20natural%0Alanguage%20instructions%20%28e.g.%2C%20spatial%20relationships%20between%20objects%2C%20generating%0Acomplex%20scenes%29%20is%20still%20unsatisfactory.%20It%20has%20been%20an%20important%20research%20area%0Ato%20enhance%20such%20capability.%20Prior%20works%20have%20shown%20that%20using%20Reinforcement%0ALearning%20can%20effectively%20train%20diffusion%20models%20to%20enhance%20fidelity%20on%20specific%0Aobjectives.%20However%2C%20existing%20RL%20methods%20require%20collecting%20a%20large%20amount%20of%0Adata%20to%20train%20an%20effective%20reward%20model.%20They%20also%20don%27t%20receive%20feedback%20when%0Athe%20generated%20image%20is%20incorrect.%20In%20this%20work%2C%20we%20propose%20Iterative%20Prompt%0ARelabeling%20%28IPR%29%2C%20a%20novel%20algorithm%20that%20aligns%20images%20to%20text%20through%0Aiterative%20image%20sampling%20and%20prompt%20relabeling.%20IPR%20first%20samples%20a%20batch%20of%0Aimages%20conditioned%20on%20the%20text%20then%20relabels%20the%20text%20prompts%20of%20unmatched%0Atext-image%20pairs%20with%20classifier%20feedback.%20We%20conduct%20thorough%20experiments%20on%0ASDv2%20and%20SDXL%2C%20testing%20their%20capability%20to%20follow%20instructions%20on%20spatial%0Arelations.%20With%20IPR%2C%20we%20improved%20up%20to%2015.22%25%20%28absolute%20improvement%29%20on%20the%0Achallenging%20spatial%20relation%20VISOR%20benchmark%2C%20demonstrating%20superior%0Aperformance%20compared%20to%20previous%20RL%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16204v2&entry.124074799=Read"},
{"title": "Hindsight Preference Learning for Offline Preference-based Reinforcement\n  Learning", "author": "Chen-Xiao Gao and Shengjun Fang and Chenjun Xiao and Yang Yu and Zongzhang Zhang", "abstract": "  Offline preference-based reinforcement learning (RL), which focuses on\noptimizing policies using human preferences between pairs of trajectory\nsegments selected from an offline dataset, has emerged as a practical avenue\nfor RL applications. Existing works rely on extracting step-wise reward signals\nfrom trajectory-wise preference annotations, assuming that preferences\ncorrelate with the cumulative Markovian rewards. However, such methods fail to\ncapture the holistic perspective of data annotation: Humans often assess the\ndesirability of a sequence of actions by considering the overall outcome rather\nthan the immediate rewards. To address this challenge, we propose to model\nhuman preferences using rewards conditioned on future outcomes of the\ntrajectory segments, i.e. the hindsight information. For downstream RL\noptimization, the reward of each step is calculated by marginalizing over\npossible future outcomes, the distribution of which is approximated by a\nvariational auto-encoder trained using the offline dataset. Our proposed\nmethod, Hindsight Preference Learning (HPL), can facilitate credit assignment\nby taking full advantage of vast trajectory data available in massive unlabeled\ndatasets. Comprehensive empirical studies demonstrate the benefits of HPL in\ndelivering robust and advantageous rewards across various domains. Our code is\npublicly released at https://github.com/typoverflow/WiseRL.\n", "link": "http://arxiv.org/abs/2407.04451v1", "date": "2024-07-05", "relevancy": 1.8074, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4989}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4451}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hindsight%20Preference%20Learning%20for%20Offline%20Preference-based%20Reinforcement%0A%20%20Learning&body=Title%3A%20Hindsight%20Preference%20Learning%20for%20Offline%20Preference-based%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Chen-Xiao%20Gao%20and%20Shengjun%20Fang%20and%20Chenjun%20Xiao%20and%20Yang%20Yu%20and%20Zongzhang%20Zhang%0AAbstract%3A%20%20%20Offline%20preference-based%20reinforcement%20learning%20%28RL%29%2C%20which%20focuses%20on%0Aoptimizing%20policies%20using%20human%20preferences%20between%20pairs%20of%20trajectory%0Asegments%20selected%20from%20an%20offline%20dataset%2C%20has%20emerged%20as%20a%20practical%20avenue%0Afor%20RL%20applications.%20Existing%20works%20rely%20on%20extracting%20step-wise%20reward%20signals%0Afrom%20trajectory-wise%20preference%20annotations%2C%20assuming%20that%20preferences%0Acorrelate%20with%20the%20cumulative%20Markovian%20rewards.%20However%2C%20such%20methods%20fail%20to%0Acapture%20the%20holistic%20perspective%20of%20data%20annotation%3A%20Humans%20often%20assess%20the%0Adesirability%20of%20a%20sequence%20of%20actions%20by%20considering%20the%20overall%20outcome%20rather%0Athan%20the%20immediate%20rewards.%20To%20address%20this%20challenge%2C%20we%20propose%20to%20model%0Ahuman%20preferences%20using%20rewards%20conditioned%20on%20future%20outcomes%20of%20the%0Atrajectory%20segments%2C%20i.e.%20the%20hindsight%20information.%20For%20downstream%20RL%0Aoptimization%2C%20the%20reward%20of%20each%20step%20is%20calculated%20by%20marginalizing%20over%0Apossible%20future%20outcomes%2C%20the%20distribution%20of%20which%20is%20approximated%20by%20a%0Avariational%20auto-encoder%20trained%20using%20the%20offline%20dataset.%20Our%20proposed%0Amethod%2C%20Hindsight%20Preference%20Learning%20%28HPL%29%2C%20can%20facilitate%20credit%20assignment%0Aby%20taking%20full%20advantage%20of%20vast%20trajectory%20data%20available%20in%20massive%20unlabeled%0Adatasets.%20Comprehensive%20empirical%20studies%20demonstrate%20the%20benefits%20of%20HPL%20in%0Adelivering%20robust%20and%20advantageous%20rewards%20across%20various%20domains.%20Our%20code%20is%0Apublicly%20released%20at%20https%3A//github.com/typoverflow/WiseRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHindsight%2520Preference%2520Learning%2520for%2520Offline%2520Preference-based%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DChen-Xiao%2520Gao%2520and%2520Shengjun%2520Fang%2520and%2520Chenjun%2520Xiao%2520and%2520Yang%2520Yu%2520and%2520Zongzhang%2520Zhang%26entry.1292438233%3D%2520%2520Offline%2520preference-based%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520which%2520focuses%2520on%250Aoptimizing%2520policies%2520using%2520human%2520preferences%2520between%2520pairs%2520of%2520trajectory%250Asegments%2520selected%2520from%2520an%2520offline%2520dataset%252C%2520has%2520emerged%2520as%2520a%2520practical%2520avenue%250Afor%2520RL%2520applications.%2520Existing%2520works%2520rely%2520on%2520extracting%2520step-wise%2520reward%2520signals%250Afrom%2520trajectory-wise%2520preference%2520annotations%252C%2520assuming%2520that%2520preferences%250Acorrelate%2520with%2520the%2520cumulative%2520Markovian%2520rewards.%2520However%252C%2520such%2520methods%2520fail%2520to%250Acapture%2520the%2520holistic%2520perspective%2520of%2520data%2520annotation%253A%2520Humans%2520often%2520assess%2520the%250Adesirability%2520of%2520a%2520sequence%2520of%2520actions%2520by%2520considering%2520the%2520overall%2520outcome%2520rather%250Athan%2520the%2520immediate%2520rewards.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520to%2520model%250Ahuman%2520preferences%2520using%2520rewards%2520conditioned%2520on%2520future%2520outcomes%2520of%2520the%250Atrajectory%2520segments%252C%2520i.e.%2520the%2520hindsight%2520information.%2520For%2520downstream%2520RL%250Aoptimization%252C%2520the%2520reward%2520of%2520each%2520step%2520is%2520calculated%2520by%2520marginalizing%2520over%250Apossible%2520future%2520outcomes%252C%2520the%2520distribution%2520of%2520which%2520is%2520approximated%2520by%2520a%250Avariational%2520auto-encoder%2520trained%2520using%2520the%2520offline%2520dataset.%2520Our%2520proposed%250Amethod%252C%2520Hindsight%2520Preference%2520Learning%2520%2528HPL%2529%252C%2520can%2520facilitate%2520credit%2520assignment%250Aby%2520taking%2520full%2520advantage%2520of%2520vast%2520trajectory%2520data%2520available%2520in%2520massive%2520unlabeled%250Adatasets.%2520Comprehensive%2520empirical%2520studies%2520demonstrate%2520the%2520benefits%2520of%2520HPL%2520in%250Adelivering%2520robust%2520and%2520advantageous%2520rewards%2520across%2520various%2520domains.%2520Our%2520code%2520is%250Apublicly%2520released%2520at%2520https%253A//github.com/typoverflow/WiseRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hindsight%20Preference%20Learning%20for%20Offline%20Preference-based%20Reinforcement%0A%20%20Learning&entry.906535625=Chen-Xiao%20Gao%20and%20Shengjun%20Fang%20and%20Chenjun%20Xiao%20and%20Yang%20Yu%20and%20Zongzhang%20Zhang&entry.1292438233=%20%20Offline%20preference-based%20reinforcement%20learning%20%28RL%29%2C%20which%20focuses%20on%0Aoptimizing%20policies%20using%20human%20preferences%20between%20pairs%20of%20trajectory%0Asegments%20selected%20from%20an%20offline%20dataset%2C%20has%20emerged%20as%20a%20practical%20avenue%0Afor%20RL%20applications.%20Existing%20works%20rely%20on%20extracting%20step-wise%20reward%20signals%0Afrom%20trajectory-wise%20preference%20annotations%2C%20assuming%20that%20preferences%0Acorrelate%20with%20the%20cumulative%20Markovian%20rewards.%20However%2C%20such%20methods%20fail%20to%0Acapture%20the%20holistic%20perspective%20of%20data%20annotation%3A%20Humans%20often%20assess%20the%0Adesirability%20of%20a%20sequence%20of%20actions%20by%20considering%20the%20overall%20outcome%20rather%0Athan%20the%20immediate%20rewards.%20To%20address%20this%20challenge%2C%20we%20propose%20to%20model%0Ahuman%20preferences%20using%20rewards%20conditioned%20on%20future%20outcomes%20of%20the%0Atrajectory%20segments%2C%20i.e.%20the%20hindsight%20information.%20For%20downstream%20RL%0Aoptimization%2C%20the%20reward%20of%20each%20step%20is%20calculated%20by%20marginalizing%20over%0Apossible%20future%20outcomes%2C%20the%20distribution%20of%20which%20is%20approximated%20by%20a%0Avariational%20auto-encoder%20trained%20using%20the%20offline%20dataset.%20Our%20proposed%0Amethod%2C%20Hindsight%20Preference%20Learning%20%28HPL%29%2C%20can%20facilitate%20credit%20assignment%0Aby%20taking%20full%20advantage%20of%20vast%20trajectory%20data%20available%20in%20massive%20unlabeled%0Adatasets.%20Comprehensive%20empirical%20studies%20demonstrate%20the%20benefits%20of%20HPL%20in%0Adelivering%20robust%20and%20advantageous%20rewards%20across%20various%20domains.%20Our%20code%20is%0Apublicly%20released%20at%20https%3A//github.com/typoverflow/WiseRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04451v1&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Integrated\n  Satellite-Aerial-Terrestrial Networks: Recent Advances and Future Directions", "author": "Shumaila Javaid and Ruhul Amin Khalil and Nasir Saeed and Bin He and Mohamed-Slim Alouini", "abstract": "  Integrated satellite, aerial, and terrestrial networks (ISATNs) represent a\nsophisticated convergence of diverse communication technologies to ensure\nseamless connectivity across different altitudes and platforms. This paper\nexplores the transformative potential of integrating Large Language Models\n(LLMs) into ISATNs, leveraging advanced Artificial Intelligence (AI) and\nMachine Learning (ML) capabilities to enhance these networks. We outline the\ncurrent architecture of ISATNs and highlight the significant role LLMs can play\nin optimizing data flow, signal processing, and network management to advance\n5G/6G communication technologies through advanced predictive algorithms and\nreal-time decision-making. A comprehensive analysis of ISATN components is\nconducted, assessing how LLMs can effectively address traditional data\ntransmission and processing bottlenecks. The paper delves into the network\nmanagement challenges within ISATNs, emphasizing the necessity for\nsophisticated resource allocation strategies, traffic routing, and security\nmanagement to ensure seamless connectivity and optimal performance under\nvarying conditions. Furthermore, we examine the technical challenges and\nlimitations associated with integrating LLMs into ISATNs, such as data\nintegration for LLM processing, scalability issues, latency in decision-making\nprocesses, and the design of robust, fault-tolerant systems. The study also\nidentifies key future research directions for fully harnessing LLM capabilities\nin ISATNs, which is crucial for enhancing network reliability, optimizing\nperformance, and achieving a truly interconnected and intelligent global\nnetwork system.\n", "link": "http://arxiv.org/abs/2407.04581v1", "date": "2024-07-05", "relevancy": 1.8031, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4752}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4333}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Integrated%0A%20%20Satellite-Aerial-Terrestrial%20Networks%3A%20Recent%20Advances%20and%20Future%20Directions&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Integrated%0A%20%20Satellite-Aerial-Terrestrial%20Networks%3A%20Recent%20Advances%20and%20Future%20Directions%0AAuthor%3A%20Shumaila%20Javaid%20and%20Ruhul%20Amin%20Khalil%20and%20Nasir%20Saeed%20and%20Bin%20He%20and%20Mohamed-Slim%20Alouini%0AAbstract%3A%20%20%20Integrated%20satellite%2C%20aerial%2C%20and%20terrestrial%20networks%20%28ISATNs%29%20represent%20a%0Asophisticated%20convergence%20of%20diverse%20communication%20technologies%20to%20ensure%0Aseamless%20connectivity%20across%20different%20altitudes%20and%20platforms.%20This%20paper%0Aexplores%20the%20transformative%20potential%20of%20integrating%20Large%20Language%20Models%0A%28LLMs%29%20into%20ISATNs%2C%20leveraging%20advanced%20Artificial%20Intelligence%20%28AI%29%20and%0AMachine%20Learning%20%28ML%29%20capabilities%20to%20enhance%20these%20networks.%20We%20outline%20the%0Acurrent%20architecture%20of%20ISATNs%20and%20highlight%20the%20significant%20role%20LLMs%20can%20play%0Ain%20optimizing%20data%20flow%2C%20signal%20processing%2C%20and%20network%20management%20to%20advance%0A5G/6G%20communication%20technologies%20through%20advanced%20predictive%20algorithms%20and%0Areal-time%20decision-making.%20A%20comprehensive%20analysis%20of%20ISATN%20components%20is%0Aconducted%2C%20assessing%20how%20LLMs%20can%20effectively%20address%20traditional%20data%0Atransmission%20and%20processing%20bottlenecks.%20The%20paper%20delves%20into%20the%20network%0Amanagement%20challenges%20within%20ISATNs%2C%20emphasizing%20the%20necessity%20for%0Asophisticated%20resource%20allocation%20strategies%2C%20traffic%20routing%2C%20and%20security%0Amanagement%20to%20ensure%20seamless%20connectivity%20and%20optimal%20performance%20under%0Avarying%20conditions.%20Furthermore%2C%20we%20examine%20the%20technical%20challenges%20and%0Alimitations%20associated%20with%20integrating%20LLMs%20into%20ISATNs%2C%20such%20as%20data%0Aintegration%20for%20LLM%20processing%2C%20scalability%20issues%2C%20latency%20in%20decision-making%0Aprocesses%2C%20and%20the%20design%20of%20robust%2C%20fault-tolerant%20systems.%20The%20study%20also%0Aidentifies%20key%20future%20research%20directions%20for%20fully%20harnessing%20LLM%20capabilities%0Ain%20ISATNs%2C%20which%20is%20crucial%20for%20enhancing%20network%20reliability%2C%20optimizing%0Aperformance%2C%20and%20achieving%20a%20truly%20interconnected%20and%20intelligent%20global%0Anetwork%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Integrated%250A%2520%2520Satellite-Aerial-Terrestrial%2520Networks%253A%2520Recent%2520Advances%2520and%2520Future%2520Directions%26entry.906535625%3DShumaila%2520Javaid%2520and%2520Ruhul%2520Amin%2520Khalil%2520and%2520Nasir%2520Saeed%2520and%2520Bin%2520He%2520and%2520Mohamed-Slim%2520Alouini%26entry.1292438233%3D%2520%2520Integrated%2520satellite%252C%2520aerial%252C%2520and%2520terrestrial%2520networks%2520%2528ISATNs%2529%2520represent%2520a%250Asophisticated%2520convergence%2520of%2520diverse%2520communication%2520technologies%2520to%2520ensure%250Aseamless%2520connectivity%2520across%2520different%2520altitudes%2520and%2520platforms.%2520This%2520paper%250Aexplores%2520the%2520transformative%2520potential%2520of%2520integrating%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520into%2520ISATNs%252C%2520leveraging%2520advanced%2520Artificial%2520Intelligence%2520%2528AI%2529%2520and%250AMachine%2520Learning%2520%2528ML%2529%2520capabilities%2520to%2520enhance%2520these%2520networks.%2520We%2520outline%2520the%250Acurrent%2520architecture%2520of%2520ISATNs%2520and%2520highlight%2520the%2520significant%2520role%2520LLMs%2520can%2520play%250Ain%2520optimizing%2520data%2520flow%252C%2520signal%2520processing%252C%2520and%2520network%2520management%2520to%2520advance%250A5G/6G%2520communication%2520technologies%2520through%2520advanced%2520predictive%2520algorithms%2520and%250Areal-time%2520decision-making.%2520A%2520comprehensive%2520analysis%2520of%2520ISATN%2520components%2520is%250Aconducted%252C%2520assessing%2520how%2520LLMs%2520can%2520effectively%2520address%2520traditional%2520data%250Atransmission%2520and%2520processing%2520bottlenecks.%2520The%2520paper%2520delves%2520into%2520the%2520network%250Amanagement%2520challenges%2520within%2520ISATNs%252C%2520emphasizing%2520the%2520necessity%2520for%250Asophisticated%2520resource%2520allocation%2520strategies%252C%2520traffic%2520routing%252C%2520and%2520security%250Amanagement%2520to%2520ensure%2520seamless%2520connectivity%2520and%2520optimal%2520performance%2520under%250Avarying%2520conditions.%2520Furthermore%252C%2520we%2520examine%2520the%2520technical%2520challenges%2520and%250Alimitations%2520associated%2520with%2520integrating%2520LLMs%2520into%2520ISATNs%252C%2520such%2520as%2520data%250Aintegration%2520for%2520LLM%2520processing%252C%2520scalability%2520issues%252C%2520latency%2520in%2520decision-making%250Aprocesses%252C%2520and%2520the%2520design%2520of%2520robust%252C%2520fault-tolerant%2520systems.%2520The%2520study%2520also%250Aidentifies%2520key%2520future%2520research%2520directions%2520for%2520fully%2520harnessing%2520LLM%2520capabilities%250Ain%2520ISATNs%252C%2520which%2520is%2520crucial%2520for%2520enhancing%2520network%2520reliability%252C%2520optimizing%250Aperformance%252C%2520and%2520achieving%2520a%2520truly%2520interconnected%2520and%2520intelligent%2520global%250Anetwork%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Integrated%0A%20%20Satellite-Aerial-Terrestrial%20Networks%3A%20Recent%20Advances%20and%20Future%20Directions&entry.906535625=Shumaila%20Javaid%20and%20Ruhul%20Amin%20Khalil%20and%20Nasir%20Saeed%20and%20Bin%20He%20and%20Mohamed-Slim%20Alouini&entry.1292438233=%20%20Integrated%20satellite%2C%20aerial%2C%20and%20terrestrial%20networks%20%28ISATNs%29%20represent%20a%0Asophisticated%20convergence%20of%20diverse%20communication%20technologies%20to%20ensure%0Aseamless%20connectivity%20across%20different%20altitudes%20and%20platforms.%20This%20paper%0Aexplores%20the%20transformative%20potential%20of%20integrating%20Large%20Language%20Models%0A%28LLMs%29%20into%20ISATNs%2C%20leveraging%20advanced%20Artificial%20Intelligence%20%28AI%29%20and%0AMachine%20Learning%20%28ML%29%20capabilities%20to%20enhance%20these%20networks.%20We%20outline%20the%0Acurrent%20architecture%20of%20ISATNs%20and%20highlight%20the%20significant%20role%20LLMs%20can%20play%0Ain%20optimizing%20data%20flow%2C%20signal%20processing%2C%20and%20network%20management%20to%20advance%0A5G/6G%20communication%20technologies%20through%20advanced%20predictive%20algorithms%20and%0Areal-time%20decision-making.%20A%20comprehensive%20analysis%20of%20ISATN%20components%20is%0Aconducted%2C%20assessing%20how%20LLMs%20can%20effectively%20address%20traditional%20data%0Atransmission%20and%20processing%20bottlenecks.%20The%20paper%20delves%20into%20the%20network%0Amanagement%20challenges%20within%20ISATNs%2C%20emphasizing%20the%20necessity%20for%0Asophisticated%20resource%20allocation%20strategies%2C%20traffic%20routing%2C%20and%20security%0Amanagement%20to%20ensure%20seamless%20connectivity%20and%20optimal%20performance%20under%0Avarying%20conditions.%20Furthermore%2C%20we%20examine%20the%20technical%20challenges%20and%0Alimitations%20associated%20with%20integrating%20LLMs%20into%20ISATNs%2C%20such%20as%20data%0Aintegration%20for%20LLM%20processing%2C%20scalability%20issues%2C%20latency%20in%20decision-making%0Aprocesses%2C%20and%20the%20design%20of%20robust%2C%20fault-tolerant%20systems.%20The%20study%20also%0Aidentifies%20key%20future%20research%20directions%20for%20fully%20harnessing%20LLM%20capabilities%0Ain%20ISATNs%2C%20which%20is%20crucial%20for%20enhancing%20network%20reliability%2C%20optimizing%0Aperformance%2C%20and%20achieving%20a%20truly%20interconnected%20and%20intelligent%20global%0Anetwork%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04581v1&entry.124074799=Read"},
{"title": "Graph Reinforcement Learning in Power Grids: A Survey", "author": "Mohamed Hassouna and Clara Holzh\u00fcter and Pawel Lytaev and Josephine Thomas and Bernhard Sick and Christoph Scholz", "abstract": "  The challenges posed by renewable energy and distributed electricity\ngeneration motivate the development of deep learning approaches to overcome the\nlack of flexibility of traditional methods in power grids use cases. The\napplication of GNNs is particularly promising due to their ability to learn\nfrom graph-structured data present in power grids. Combined with RL, they can\nserve as control approaches to determine remedial grid actions. This review\nanalyses the ability of GRL to capture the inherent graph structure of power\ngrids to improve representation learning and decision making in different power\ngrid use cases. It distinguishes between common problems in transmission and\ndistribution grids and explores the synergy between RL and GNNs. In\ntransmission grids, GRL typically addresses automated grid management and\ntopology control, whereas on the distribution side, GRL concentrates more on\nvoltage regulation. We analyzed the selected papers based on their graph\nstructure and GNN model, the applied RL algorithm, and their overall\ncontributions. Although GRL demonstrate adaptability in the face of\nunpredictable events and noisy or incomplete data, it primarily serves as a\nproof of concept at this stage. There are multiple open challenges and\nlimitations that need to be addressed when considering the application of RL to\nreal power grid operation.\n", "link": "http://arxiv.org/abs/2407.04522v1", "date": "2024-07-05", "relevancy": 1.7918, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.459}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4546}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Reinforcement%20Learning%20in%20Power%20Grids%3A%20A%20Survey&body=Title%3A%20Graph%20Reinforcement%20Learning%20in%20Power%20Grids%3A%20A%20Survey%0AAuthor%3A%20Mohamed%20Hassouna%20and%20Clara%20Holzh%C3%BCter%20and%20Pawel%20Lytaev%20and%20Josephine%20Thomas%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20The%20challenges%20posed%20by%20renewable%20energy%20and%20distributed%20electricity%0Ageneration%20motivate%20the%20development%20of%20deep%20learning%20approaches%20to%20overcome%20the%0Alack%20of%20flexibility%20of%20traditional%20methods%20in%20power%20grids%20use%20cases.%20The%0Aapplication%20of%20GNNs%20is%20particularly%20promising%20due%20to%20their%20ability%20to%20learn%0Afrom%20graph-structured%20data%20present%20in%20power%20grids.%20Combined%20with%20RL%2C%20they%20can%0Aserve%20as%20control%20approaches%20to%20determine%20remedial%20grid%20actions.%20This%20review%0Aanalyses%20the%20ability%20of%20GRL%20to%20capture%20the%20inherent%20graph%20structure%20of%20power%0Agrids%20to%20improve%20representation%20learning%20and%20decision%20making%20in%20different%20power%0Agrid%20use%20cases.%20It%20distinguishes%20between%20common%20problems%20in%20transmission%20and%0Adistribution%20grids%20and%20explores%20the%20synergy%20between%20RL%20and%20GNNs.%20In%0Atransmission%20grids%2C%20GRL%20typically%20addresses%20automated%20grid%20management%20and%0Atopology%20control%2C%20whereas%20on%20the%20distribution%20side%2C%20GRL%20concentrates%20more%20on%0Avoltage%20regulation.%20We%20analyzed%20the%20selected%20papers%20based%20on%20their%20graph%0Astructure%20and%20GNN%20model%2C%20the%20applied%20RL%20algorithm%2C%20and%20their%20overall%0Acontributions.%20Although%20GRL%20demonstrate%20adaptability%20in%20the%20face%20of%0Aunpredictable%20events%20and%20noisy%20or%20incomplete%20data%2C%20it%20primarily%20serves%20as%20a%0Aproof%20of%20concept%20at%20this%20stage.%20There%20are%20multiple%20open%20challenges%20and%0Alimitations%20that%20need%20to%20be%20addressed%20when%20considering%20the%20application%20of%20RL%20to%0Areal%20power%20grid%20operation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Reinforcement%2520Learning%2520in%2520Power%2520Grids%253A%2520A%2520Survey%26entry.906535625%3DMohamed%2520Hassouna%2520and%2520Clara%2520Holzh%25C3%25BCter%2520and%2520Pawel%2520Lytaev%2520and%2520Josephine%2520Thomas%2520and%2520Bernhard%2520Sick%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520The%2520challenges%2520posed%2520by%2520renewable%2520energy%2520and%2520distributed%2520electricity%250Ageneration%2520motivate%2520the%2520development%2520of%2520deep%2520learning%2520approaches%2520to%2520overcome%2520the%250Alack%2520of%2520flexibility%2520of%2520traditional%2520methods%2520in%2520power%2520grids%2520use%2520cases.%2520The%250Aapplication%2520of%2520GNNs%2520is%2520particularly%2520promising%2520due%2520to%2520their%2520ability%2520to%2520learn%250Afrom%2520graph-structured%2520data%2520present%2520in%2520power%2520grids.%2520Combined%2520with%2520RL%252C%2520they%2520can%250Aserve%2520as%2520control%2520approaches%2520to%2520determine%2520remedial%2520grid%2520actions.%2520This%2520review%250Aanalyses%2520the%2520ability%2520of%2520GRL%2520to%2520capture%2520the%2520inherent%2520graph%2520structure%2520of%2520power%250Agrids%2520to%2520improve%2520representation%2520learning%2520and%2520decision%2520making%2520in%2520different%2520power%250Agrid%2520use%2520cases.%2520It%2520distinguishes%2520between%2520common%2520problems%2520in%2520transmission%2520and%250Adistribution%2520grids%2520and%2520explores%2520the%2520synergy%2520between%2520RL%2520and%2520GNNs.%2520In%250Atransmission%2520grids%252C%2520GRL%2520typically%2520addresses%2520automated%2520grid%2520management%2520and%250Atopology%2520control%252C%2520whereas%2520on%2520the%2520distribution%2520side%252C%2520GRL%2520concentrates%2520more%2520on%250Avoltage%2520regulation.%2520We%2520analyzed%2520the%2520selected%2520papers%2520based%2520on%2520their%2520graph%250Astructure%2520and%2520GNN%2520model%252C%2520the%2520applied%2520RL%2520algorithm%252C%2520and%2520their%2520overall%250Acontributions.%2520Although%2520GRL%2520demonstrate%2520adaptability%2520in%2520the%2520face%2520of%250Aunpredictable%2520events%2520and%2520noisy%2520or%2520incomplete%2520data%252C%2520it%2520primarily%2520serves%2520as%2520a%250Aproof%2520of%2520concept%2520at%2520this%2520stage.%2520There%2520are%2520multiple%2520open%2520challenges%2520and%250Alimitations%2520that%2520need%2520to%2520be%2520addressed%2520when%2520considering%2520the%2520application%2520of%2520RL%2520to%250Areal%2520power%2520grid%2520operation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Reinforcement%20Learning%20in%20Power%20Grids%3A%20A%20Survey&entry.906535625=Mohamed%20Hassouna%20and%20Clara%20Holzh%C3%BCter%20and%20Pawel%20Lytaev%20and%20Josephine%20Thomas%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz&entry.1292438233=%20%20The%20challenges%20posed%20by%20renewable%20energy%20and%20distributed%20electricity%0Ageneration%20motivate%20the%20development%20of%20deep%20learning%20approaches%20to%20overcome%20the%0Alack%20of%20flexibility%20of%20traditional%20methods%20in%20power%20grids%20use%20cases.%20The%0Aapplication%20of%20GNNs%20is%20particularly%20promising%20due%20to%20their%20ability%20to%20learn%0Afrom%20graph-structured%20data%20present%20in%20power%20grids.%20Combined%20with%20RL%2C%20they%20can%0Aserve%20as%20control%20approaches%20to%20determine%20remedial%20grid%20actions.%20This%20review%0Aanalyses%20the%20ability%20of%20GRL%20to%20capture%20the%20inherent%20graph%20structure%20of%20power%0Agrids%20to%20improve%20representation%20learning%20and%20decision%20making%20in%20different%20power%0Agrid%20use%20cases.%20It%20distinguishes%20between%20common%20problems%20in%20transmission%20and%0Adistribution%20grids%20and%20explores%20the%20synergy%20between%20RL%20and%20GNNs.%20In%0Atransmission%20grids%2C%20GRL%20typically%20addresses%20automated%20grid%20management%20and%0Atopology%20control%2C%20whereas%20on%20the%20distribution%20side%2C%20GRL%20concentrates%20more%20on%0Avoltage%20regulation.%20We%20analyzed%20the%20selected%20papers%20based%20on%20their%20graph%0Astructure%20and%20GNN%20model%2C%20the%20applied%20RL%20algorithm%2C%20and%20their%20overall%0Acontributions.%20Although%20GRL%20demonstrate%20adaptability%20in%20the%20face%20of%0Aunpredictable%20events%20and%20noisy%20or%20incomplete%20data%2C%20it%20primarily%20serves%20as%20a%0Aproof%20of%20concept%20at%20this%20stage.%20There%20are%20multiple%20open%20challenges%20and%0Alimitations%20that%20need%20to%20be%20addressed%20when%20considering%20the%20application%20of%20RL%20to%0Areal%20power%20grid%20operation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04522v1&entry.124074799=Read"},
{"title": "Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity\n  Recognition Framework", "author": "Reza Averly and Xia Ning", "abstract": "  Clinical named entity recognition (NER) aims to retrieve important entities\nwithin clinical narratives. Recent works have demonstrated that large language\nmodels (LLMs) can achieve strong performance in this task. While previous works\nfocus on proprietary LLMs, we investigate how open NER LLMs, trained\nspecifically for entity recognition, perform in clinical NER. In this paper, we\naim to improve them through a novel framework, entity decomposition with\nfiltering, or EDF. Our key idea is to decompose the entity recognition task\ninto several retrievals of sub-entity types. We also introduce a filtering\nmechanism to remove incorrect entities. Our experimental results demonstrate\nthe efficacy of our framework across all metrics, models, datasets, and entity\ntypes. Our analysis reveals that entity decomposition can recognize previously\nmissed entities with substantial improvement. We further provide a\ncomprehensive evaluation of our framework and an in-depth error analysis to\npave future works.\n", "link": "http://arxiv.org/abs/2407.04629v1", "date": "2024-07-05", "relevancy": 1.7822, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4653}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4585}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Entity%20Decomposition%20with%20Filtering%3A%20A%20Zero-Shot%20Clinical%20Named%20Entity%0A%20%20Recognition%20Framework&body=Title%3A%20Entity%20Decomposition%20with%20Filtering%3A%20A%20Zero-Shot%20Clinical%20Named%20Entity%0A%20%20Recognition%20Framework%0AAuthor%3A%20Reza%20Averly%20and%20Xia%20Ning%0AAbstract%3A%20%20%20Clinical%20named%20entity%20recognition%20%28NER%29%20aims%20to%20retrieve%20important%20entities%0Awithin%20clinical%20narratives.%20Recent%20works%20have%20demonstrated%20that%20large%20language%0Amodels%20%28LLMs%29%20can%20achieve%20strong%20performance%20in%20this%20task.%20While%20previous%20works%0Afocus%20on%20proprietary%20LLMs%2C%20we%20investigate%20how%20open%20NER%20LLMs%2C%20trained%0Aspecifically%20for%20entity%20recognition%2C%20perform%20in%20clinical%20NER.%20In%20this%20paper%2C%20we%0Aaim%20to%20improve%20them%20through%20a%20novel%20framework%2C%20entity%20decomposition%20with%0Afiltering%2C%20or%20EDF.%20Our%20key%20idea%20is%20to%20decompose%20the%20entity%20recognition%20task%0Ainto%20several%20retrievals%20of%20sub-entity%20types.%20We%20also%20introduce%20a%20filtering%0Amechanism%20to%20remove%20incorrect%20entities.%20Our%20experimental%20results%20demonstrate%0Athe%20efficacy%20of%20our%20framework%20across%20all%20metrics%2C%20models%2C%20datasets%2C%20and%20entity%0Atypes.%20Our%20analysis%20reveals%20that%20entity%20decomposition%20can%20recognize%20previously%0Amissed%20entities%20with%20substantial%20improvement.%20We%20further%20provide%20a%0Acomprehensive%20evaluation%20of%20our%20framework%20and%20an%20in-depth%20error%20analysis%20to%0Apave%20future%20works.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntity%2520Decomposition%2520with%2520Filtering%253A%2520A%2520Zero-Shot%2520Clinical%2520Named%2520Entity%250A%2520%2520Recognition%2520Framework%26entry.906535625%3DReza%2520Averly%2520and%2520Xia%2520Ning%26entry.1292438233%3D%2520%2520Clinical%2520named%2520entity%2520recognition%2520%2528NER%2529%2520aims%2520to%2520retrieve%2520important%2520entities%250Awithin%2520clinical%2520narratives.%2520Recent%2520works%2520have%2520demonstrated%2520that%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520can%2520achieve%2520strong%2520performance%2520in%2520this%2520task.%2520While%2520previous%2520works%250Afocus%2520on%2520proprietary%2520LLMs%252C%2520we%2520investigate%2520how%2520open%2520NER%2520LLMs%252C%2520trained%250Aspecifically%2520for%2520entity%2520recognition%252C%2520perform%2520in%2520clinical%2520NER.%2520In%2520this%2520paper%252C%2520we%250Aaim%2520to%2520improve%2520them%2520through%2520a%2520novel%2520framework%252C%2520entity%2520decomposition%2520with%250Afiltering%252C%2520or%2520EDF.%2520Our%2520key%2520idea%2520is%2520to%2520decompose%2520the%2520entity%2520recognition%2520task%250Ainto%2520several%2520retrievals%2520of%2520sub-entity%2520types.%2520We%2520also%2520introduce%2520a%2520filtering%250Amechanism%2520to%2520remove%2520incorrect%2520entities.%2520Our%2520experimental%2520results%2520demonstrate%250Athe%2520efficacy%2520of%2520our%2520framework%2520across%2520all%2520metrics%252C%2520models%252C%2520datasets%252C%2520and%2520entity%250Atypes.%2520Our%2520analysis%2520reveals%2520that%2520entity%2520decomposition%2520can%2520recognize%2520previously%250Amissed%2520entities%2520with%2520substantial%2520improvement.%2520We%2520further%2520provide%2520a%250Acomprehensive%2520evaluation%2520of%2520our%2520framework%2520and%2520an%2520in-depth%2520error%2520analysis%2520to%250Apave%2520future%2520works.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Entity%20Decomposition%20with%20Filtering%3A%20A%20Zero-Shot%20Clinical%20Named%20Entity%0A%20%20Recognition%20Framework&entry.906535625=Reza%20Averly%20and%20Xia%20Ning&entry.1292438233=%20%20Clinical%20named%20entity%20recognition%20%28NER%29%20aims%20to%20retrieve%20important%20entities%0Awithin%20clinical%20narratives.%20Recent%20works%20have%20demonstrated%20that%20large%20language%0Amodels%20%28LLMs%29%20can%20achieve%20strong%20performance%20in%20this%20task.%20While%20previous%20works%0Afocus%20on%20proprietary%20LLMs%2C%20we%20investigate%20how%20open%20NER%20LLMs%2C%20trained%0Aspecifically%20for%20entity%20recognition%2C%20perform%20in%20clinical%20NER.%20In%20this%20paper%2C%20we%0Aaim%20to%20improve%20them%20through%20a%20novel%20framework%2C%20entity%20decomposition%20with%0Afiltering%2C%20or%20EDF.%20Our%20key%20idea%20is%20to%20decompose%20the%20entity%20recognition%20task%0Ainto%20several%20retrievals%20of%20sub-entity%20types.%20We%20also%20introduce%20a%20filtering%0Amechanism%20to%20remove%20incorrect%20entities.%20Our%20experimental%20results%20demonstrate%0Athe%20efficacy%20of%20our%20framework%20across%20all%20metrics%2C%20models%2C%20datasets%2C%20and%20entity%0Atypes.%20Our%20analysis%20reveals%20that%20entity%20decomposition%20can%20recognize%20previously%0Amissed%20entities%20with%20substantial%20improvement.%20We%20further%20provide%20a%0Acomprehensive%20evaluation%20of%20our%20framework%20and%20an%20in-depth%20error%20analysis%20to%0Apave%20future%20works.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04629v1&entry.124074799=Read"},
{"title": "Detecting LLM-Assisted Writing in Scientific Communication: Are We There\n  Yet?", "author": "Teddy Lazebnik and Ariel Rosenfeld", "abstract": "  Large Language Models (LLMs), exemplified by ChatGPT, have significantly\nreshaped text generation, particularly in the realm of writing assistance.\nWhile ethical considerations underscore the importance of transparently\nacknowledging LLM use, especially in scientific communication, genuine\nacknowledgment remains infrequent. A potential avenue to encourage accurate\nacknowledging of LLM-assisted writing involves employing automated detectors.\nOur evaluation of four cutting-edge LLM-generated text detectors reveals their\nsuboptimal performance compared to a simple ad-hoc detector designed to\nidentify abrupt writing style changes around the time of LLM proliferation. We\ncontend that the development of specialized detectors exclusively dedicated to\nLLM-assisted writing detection is necessary. Such detectors could play a\ncrucial role in fostering more authentic recognition of LLM involvement in\nscientific communication, addressing the current challenges in acknowledgment\npractices.\n", "link": "http://arxiv.org/abs/2401.16807v2", "date": "2024-07-05", "relevancy": 1.7818, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.473}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4462}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20LLM-Assisted%20Writing%20in%20Scientific%20Communication%3A%20Are%20We%20There%0A%20%20Yet%3F&body=Title%3A%20Detecting%20LLM-Assisted%20Writing%20in%20Scientific%20Communication%3A%20Are%20We%20There%0A%20%20Yet%3F%0AAuthor%3A%20Teddy%20Lazebnik%20and%20Ariel%20Rosenfeld%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%2C%20exemplified%20by%20ChatGPT%2C%20have%20significantly%0Areshaped%20text%20generation%2C%20particularly%20in%20the%20realm%20of%20writing%20assistance.%0AWhile%20ethical%20considerations%20underscore%20the%20importance%20of%20transparently%0Aacknowledging%20LLM%20use%2C%20especially%20in%20scientific%20communication%2C%20genuine%0Aacknowledgment%20remains%20infrequent.%20A%20potential%20avenue%20to%20encourage%20accurate%0Aacknowledging%20of%20LLM-assisted%20writing%20involves%20employing%20automated%20detectors.%0AOur%20evaluation%20of%20four%20cutting-edge%20LLM-generated%20text%20detectors%20reveals%20their%0Asuboptimal%20performance%20compared%20to%20a%20simple%20ad-hoc%20detector%20designed%20to%0Aidentify%20abrupt%20writing%20style%20changes%20around%20the%20time%20of%20LLM%20proliferation.%20We%0Acontend%20that%20the%20development%20of%20specialized%20detectors%20exclusively%20dedicated%20to%0ALLM-assisted%20writing%20detection%20is%20necessary.%20Such%20detectors%20could%20play%20a%0Acrucial%20role%20in%20fostering%20more%20authentic%20recognition%20of%20LLM%20involvement%20in%0Ascientific%20communication%2C%20addressing%20the%20current%20challenges%20in%20acknowledgment%0Apractices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16807v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520LLM-Assisted%2520Writing%2520in%2520Scientific%2520Communication%253A%2520Are%2520We%2520There%250A%2520%2520Yet%253F%26entry.906535625%3DTeddy%2520Lazebnik%2520and%2520Ariel%2520Rosenfeld%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520exemplified%2520by%2520ChatGPT%252C%2520have%2520significantly%250Areshaped%2520text%2520generation%252C%2520particularly%2520in%2520the%2520realm%2520of%2520writing%2520assistance.%250AWhile%2520ethical%2520considerations%2520underscore%2520the%2520importance%2520of%2520transparently%250Aacknowledging%2520LLM%2520use%252C%2520especially%2520in%2520scientific%2520communication%252C%2520genuine%250Aacknowledgment%2520remains%2520infrequent.%2520A%2520potential%2520avenue%2520to%2520encourage%2520accurate%250Aacknowledging%2520of%2520LLM-assisted%2520writing%2520involves%2520employing%2520automated%2520detectors.%250AOur%2520evaluation%2520of%2520four%2520cutting-edge%2520LLM-generated%2520text%2520detectors%2520reveals%2520their%250Asuboptimal%2520performance%2520compared%2520to%2520a%2520simple%2520ad-hoc%2520detector%2520designed%2520to%250Aidentify%2520abrupt%2520writing%2520style%2520changes%2520around%2520the%2520time%2520of%2520LLM%2520proliferation.%2520We%250Acontend%2520that%2520the%2520development%2520of%2520specialized%2520detectors%2520exclusively%2520dedicated%2520to%250ALLM-assisted%2520writing%2520detection%2520is%2520necessary.%2520Such%2520detectors%2520could%2520play%2520a%250Acrucial%2520role%2520in%2520fostering%2520more%2520authentic%2520recognition%2520of%2520LLM%2520involvement%2520in%250Ascientific%2520communication%252C%2520addressing%2520the%2520current%2520challenges%2520in%2520acknowledgment%250Apractices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16807v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20LLM-Assisted%20Writing%20in%20Scientific%20Communication%3A%20Are%20We%20There%0A%20%20Yet%3F&entry.906535625=Teddy%20Lazebnik%20and%20Ariel%20Rosenfeld&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%2C%20exemplified%20by%20ChatGPT%2C%20have%20significantly%0Areshaped%20text%20generation%2C%20particularly%20in%20the%20realm%20of%20writing%20assistance.%0AWhile%20ethical%20considerations%20underscore%20the%20importance%20of%20transparently%0Aacknowledging%20LLM%20use%2C%20especially%20in%20scientific%20communication%2C%20genuine%0Aacknowledgment%20remains%20infrequent.%20A%20potential%20avenue%20to%20encourage%20accurate%0Aacknowledging%20of%20LLM-assisted%20writing%20involves%20employing%20automated%20detectors.%0AOur%20evaluation%20of%20four%20cutting-edge%20LLM-generated%20text%20detectors%20reveals%20their%0Asuboptimal%20performance%20compared%20to%20a%20simple%20ad-hoc%20detector%20designed%20to%0Aidentify%20abrupt%20writing%20style%20changes%20around%20the%20time%20of%20LLM%20proliferation.%20We%0Acontend%20that%20the%20development%20of%20specialized%20detectors%20exclusively%20dedicated%20to%0ALLM-assisted%20writing%20detection%20is%20necessary.%20Such%20detectors%20could%20play%20a%0Acrucial%20role%20in%20fostering%20more%20authentic%20recognition%20of%20LLM%20involvement%20in%0Ascientific%20communication%2C%20addressing%20the%20current%20challenges%20in%20acknowledgment%0Apractices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16807v2&entry.124074799=Read"},
{"title": "Adaptive proximal gradient methods are universal without approximation", "author": "Konstantinos A. Oikonomidis and Emanuel Laude and Puya Latafat and Andreas Themelis and Panagiotis Patrinos", "abstract": "  We show that adaptive proximal gradient methods for convex problems are not\nrestricted to traditional Lipschitzian assumptions. Our analysis reveals that a\nclass of linesearch-free methods is still convergent under mere local H\\\"older\ngradient continuity, covering in particular continuously differentiable\nsemi-algebraic functions. To mitigate the lack of local Lipschitz continuity,\npopular approaches revolve around $\\varepsilon$-oracles and/or linesearch\nprocedures. In contrast, we exploit plain H\\\"older inequalities not entailing\nany approximation, all while retaining the linesearch-free nature of adaptive\nschemes. Furthermore, we prove full sequence convergence without prior\nknowledge of local H\\\"older constants nor of the order of H\\\"older continuity.\nNumerical experiments make comparisons with baseline methods on diverse tasks\nfrom machine learning covering both the locally and the globally H\\\"older\nsetting.\n", "link": "http://arxiv.org/abs/2402.06271v2", "date": "2024-07-05", "relevancy": 1.7709, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4602}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20proximal%20gradient%20methods%20are%20universal%20without%20approximation&body=Title%3A%20Adaptive%20proximal%20gradient%20methods%20are%20universal%20without%20approximation%0AAuthor%3A%20Konstantinos%20A.%20Oikonomidis%20and%20Emanuel%20Laude%20and%20Puya%20Latafat%20and%20Andreas%20Themelis%20and%20Panagiotis%20Patrinos%0AAbstract%3A%20%20%20We%20show%20that%20adaptive%20proximal%20gradient%20methods%20for%20convex%20problems%20are%20not%0Arestricted%20to%20traditional%20Lipschitzian%20assumptions.%20Our%20analysis%20reveals%20that%20a%0Aclass%20of%20linesearch-free%20methods%20is%20still%20convergent%20under%20mere%20local%20H%5C%22older%0Agradient%20continuity%2C%20covering%20in%20particular%20continuously%20differentiable%0Asemi-algebraic%20functions.%20To%20mitigate%20the%20lack%20of%20local%20Lipschitz%20continuity%2C%0Apopular%20approaches%20revolve%20around%20%24%5Cvarepsilon%24-oracles%20and/or%20linesearch%0Aprocedures.%20In%20contrast%2C%20we%20exploit%20plain%20H%5C%22older%20inequalities%20not%20entailing%0Aany%20approximation%2C%20all%20while%20retaining%20the%20linesearch-free%20nature%20of%20adaptive%0Aschemes.%20Furthermore%2C%20we%20prove%20full%20sequence%20convergence%20without%20prior%0Aknowledge%20of%20local%20H%5C%22older%20constants%20nor%20of%20the%20order%20of%20H%5C%22older%20continuity.%0ANumerical%20experiments%20make%20comparisons%20with%20baseline%20methods%20on%20diverse%20tasks%0Afrom%20machine%20learning%20covering%20both%20the%20locally%20and%20the%20globally%20H%5C%22older%0Asetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.06271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520proximal%2520gradient%2520methods%2520are%2520universal%2520without%2520approximation%26entry.906535625%3DKonstantinos%2520A.%2520Oikonomidis%2520and%2520Emanuel%2520Laude%2520and%2520Puya%2520Latafat%2520and%2520Andreas%2520Themelis%2520and%2520Panagiotis%2520Patrinos%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520adaptive%2520proximal%2520gradient%2520methods%2520for%2520convex%2520problems%2520are%2520not%250Arestricted%2520to%2520traditional%2520Lipschitzian%2520assumptions.%2520Our%2520analysis%2520reveals%2520that%2520a%250Aclass%2520of%2520linesearch-free%2520methods%2520is%2520still%2520convergent%2520under%2520mere%2520local%2520H%255C%2522older%250Agradient%2520continuity%252C%2520covering%2520in%2520particular%2520continuously%2520differentiable%250Asemi-algebraic%2520functions.%2520To%2520mitigate%2520the%2520lack%2520of%2520local%2520Lipschitz%2520continuity%252C%250Apopular%2520approaches%2520revolve%2520around%2520%2524%255Cvarepsilon%2524-oracles%2520and/or%2520linesearch%250Aprocedures.%2520In%2520contrast%252C%2520we%2520exploit%2520plain%2520H%255C%2522older%2520inequalities%2520not%2520entailing%250Aany%2520approximation%252C%2520all%2520while%2520retaining%2520the%2520linesearch-free%2520nature%2520of%2520adaptive%250Aschemes.%2520Furthermore%252C%2520we%2520prove%2520full%2520sequence%2520convergence%2520without%2520prior%250Aknowledge%2520of%2520local%2520H%255C%2522older%2520constants%2520nor%2520of%2520the%2520order%2520of%2520H%255C%2522older%2520continuity.%250ANumerical%2520experiments%2520make%2520comparisons%2520with%2520baseline%2520methods%2520on%2520diverse%2520tasks%250Afrom%2520machine%2520learning%2520covering%2520both%2520the%2520locally%2520and%2520the%2520globally%2520H%255C%2522older%250Asetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.06271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20proximal%20gradient%20methods%20are%20universal%20without%20approximation&entry.906535625=Konstantinos%20A.%20Oikonomidis%20and%20Emanuel%20Laude%20and%20Puya%20Latafat%20and%20Andreas%20Themelis%20and%20Panagiotis%20Patrinos&entry.1292438233=%20%20We%20show%20that%20adaptive%20proximal%20gradient%20methods%20for%20convex%20problems%20are%20not%0Arestricted%20to%20traditional%20Lipschitzian%20assumptions.%20Our%20analysis%20reveals%20that%20a%0Aclass%20of%20linesearch-free%20methods%20is%20still%20convergent%20under%20mere%20local%20H%5C%22older%0Agradient%20continuity%2C%20covering%20in%20particular%20continuously%20differentiable%0Asemi-algebraic%20functions.%20To%20mitigate%20the%20lack%20of%20local%20Lipschitz%20continuity%2C%0Apopular%20approaches%20revolve%20around%20%24%5Cvarepsilon%24-oracles%20and/or%20linesearch%0Aprocedures.%20In%20contrast%2C%20we%20exploit%20plain%20H%5C%22older%20inequalities%20not%20entailing%0Aany%20approximation%2C%20all%20while%20retaining%20the%20linesearch-free%20nature%20of%20adaptive%0Aschemes.%20Furthermore%2C%20we%20prove%20full%20sequence%20convergence%20without%20prior%0Aknowledge%20of%20local%20H%5C%22older%20constants%20nor%20of%20the%20order%20of%20H%5C%22older%20continuity.%0ANumerical%20experiments%20make%20comparisons%20with%20baseline%20methods%20on%20diverse%20tasks%0Afrom%20machine%20learning%20covering%20both%20the%20locally%20and%20the%20globally%20H%5C%22older%0Asetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.06271v2&entry.124074799=Read"},
{"title": "AWT: Transferring Vision-Language Models via Augmentation, Weighting,\n  and Transportation", "author": "Yuhan Zhu and Yuyang Ji and Zhiyu Zhao and Gangshan Wu and Limin Wang", "abstract": "  Pre-trained vision-language models (VLMs) have shown impressive results in\nvarious visual classification tasks. However, we often fail to fully unleash\ntheir potential when adapting them for new concept understanding due to limited\ninformation on new classes. To address this limitation, we introduce a novel\nadaptation framework, AWT (Augment, Weight, then Transport). AWT comprises\nthree key components: augmenting inputs with diverse visual perspectives and\nenriched class descriptions through image transformations and language models;\ndynamically weighting inputs based on the prediction entropy; and employing\noptimal transport to mine semantic correlations in the vision-language space.\nAWT can be seamlessly integrated into various VLMs, enhancing their zero-shot\ncapabilities without additional training and facilitating few-shot learning\nthrough an integrated multimodal adapter module. We verify AWT in multiple\nchallenging scenarios, including zero-shot and few-shot image classification,\nzero-shot video action recognition, and out-of-distribution generalization. AWT\nconsistently outperforms the state-of-the-art methods in each setting. In\naddition, our extensive studies further demonstrate AWT's effectiveness and\nadaptability across different VLMs, architectures, and scales.\n", "link": "http://arxiv.org/abs/2407.04603v1", "date": "2024-07-05", "relevancy": 1.7577, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.621}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5607}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AWT%3A%20Transferring%20Vision-Language%20Models%20via%20Augmentation%2C%20Weighting%2C%0A%20%20and%20Transportation&body=Title%3A%20AWT%3A%20Transferring%20Vision-Language%20Models%20via%20Augmentation%2C%20Weighting%2C%0A%20%20and%20Transportation%0AAuthor%3A%20Yuhan%20Zhu%20and%20Yuyang%20Ji%20and%20Zhiyu%20Zhao%20and%20Gangshan%20Wu%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%20have%20shown%20impressive%20results%20in%0Avarious%20visual%20classification%20tasks.%20However%2C%20we%20often%20fail%20to%20fully%20unleash%0Atheir%20potential%20when%20adapting%20them%20for%20new%20concept%20understanding%20due%20to%20limited%0Ainformation%20on%20new%20classes.%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20novel%0Aadaptation%20framework%2C%20AWT%20%28Augment%2C%20Weight%2C%20then%20Transport%29.%20AWT%20comprises%0Athree%20key%20components%3A%20augmenting%20inputs%20with%20diverse%20visual%20perspectives%20and%0Aenriched%20class%20descriptions%20through%20image%20transformations%20and%20language%20models%3B%0Adynamically%20weighting%20inputs%20based%20on%20the%20prediction%20entropy%3B%20and%20employing%0Aoptimal%20transport%20to%20mine%20semantic%20correlations%20in%20the%20vision-language%20space.%0AAWT%20can%20be%20seamlessly%20integrated%20into%20various%20VLMs%2C%20enhancing%20their%20zero-shot%0Acapabilities%20without%20additional%20training%20and%20facilitating%20few-shot%20learning%0Athrough%20an%20integrated%20multimodal%20adapter%20module.%20We%20verify%20AWT%20in%20multiple%0Achallenging%20scenarios%2C%20including%20zero-shot%20and%20few-shot%20image%20classification%2C%0Azero-shot%20video%20action%20recognition%2C%20and%20out-of-distribution%20generalization.%20AWT%0Aconsistently%20outperforms%20the%20state-of-the-art%20methods%20in%20each%20setting.%20In%0Aaddition%2C%20our%20extensive%20studies%20further%20demonstrate%20AWT%27s%20effectiveness%20and%0Aadaptability%20across%20different%20VLMs%2C%20architectures%2C%20and%20scales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAWT%253A%2520Transferring%2520Vision-Language%2520Models%2520via%2520Augmentation%252C%2520Weighting%252C%250A%2520%2520and%2520Transportation%26entry.906535625%3DYuhan%2520Zhu%2520and%2520Yuyang%2520Ji%2520and%2520Zhiyu%2520Zhao%2520and%2520Gangshan%2520Wu%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520impressive%2520results%2520in%250Avarious%2520visual%2520classification%2520tasks.%2520However%252C%2520we%2520often%2520fail%2520to%2520fully%2520unleash%250Atheir%2520potential%2520when%2520adapting%2520them%2520for%2520new%2520concept%2520understanding%2520due%2520to%2520limited%250Ainformation%2520on%2520new%2520classes.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520novel%250Aadaptation%2520framework%252C%2520AWT%2520%2528Augment%252C%2520Weight%252C%2520then%2520Transport%2529.%2520AWT%2520comprises%250Athree%2520key%2520components%253A%2520augmenting%2520inputs%2520with%2520diverse%2520visual%2520perspectives%2520and%250Aenriched%2520class%2520descriptions%2520through%2520image%2520transformations%2520and%2520language%2520models%253B%250Adynamically%2520weighting%2520inputs%2520based%2520on%2520the%2520prediction%2520entropy%253B%2520and%2520employing%250Aoptimal%2520transport%2520to%2520mine%2520semantic%2520correlations%2520in%2520the%2520vision-language%2520space.%250AAWT%2520can%2520be%2520seamlessly%2520integrated%2520into%2520various%2520VLMs%252C%2520enhancing%2520their%2520zero-shot%250Acapabilities%2520without%2520additional%2520training%2520and%2520facilitating%2520few-shot%2520learning%250Athrough%2520an%2520integrated%2520multimodal%2520adapter%2520module.%2520We%2520verify%2520AWT%2520in%2520multiple%250Achallenging%2520scenarios%252C%2520including%2520zero-shot%2520and%2520few-shot%2520image%2520classification%252C%250Azero-shot%2520video%2520action%2520recognition%252C%2520and%2520out-of-distribution%2520generalization.%2520AWT%250Aconsistently%2520outperforms%2520the%2520state-of-the-art%2520methods%2520in%2520each%2520setting.%2520In%250Aaddition%252C%2520our%2520extensive%2520studies%2520further%2520demonstrate%2520AWT%2527s%2520effectiveness%2520and%250Aadaptability%2520across%2520different%2520VLMs%252C%2520architectures%252C%2520and%2520scales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AWT%3A%20Transferring%20Vision-Language%20Models%20via%20Augmentation%2C%20Weighting%2C%0A%20%20and%20Transportation&entry.906535625=Yuhan%20Zhu%20and%20Yuyang%20Ji%20and%20Zhiyu%20Zhao%20and%20Gangshan%20Wu%20and%20Limin%20Wang&entry.1292438233=%20%20Pre-trained%20vision-language%20models%20%28VLMs%29%20have%20shown%20impressive%20results%20in%0Avarious%20visual%20classification%20tasks.%20However%2C%20we%20often%20fail%20to%20fully%20unleash%0Atheir%20potential%20when%20adapting%20them%20for%20new%20concept%20understanding%20due%20to%20limited%0Ainformation%20on%20new%20classes.%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20novel%0Aadaptation%20framework%2C%20AWT%20%28Augment%2C%20Weight%2C%20then%20Transport%29.%20AWT%20comprises%0Athree%20key%20components%3A%20augmenting%20inputs%20with%20diverse%20visual%20perspectives%20and%0Aenriched%20class%20descriptions%20through%20image%20transformations%20and%20language%20models%3B%0Adynamically%20weighting%20inputs%20based%20on%20the%20prediction%20entropy%3B%20and%20employing%0Aoptimal%20transport%20to%20mine%20semantic%20correlations%20in%20the%20vision-language%20space.%0AAWT%20can%20be%20seamlessly%20integrated%20into%20various%20VLMs%2C%20enhancing%20their%20zero-shot%0Acapabilities%20without%20additional%20training%20and%20facilitating%20few-shot%20learning%0Athrough%20an%20integrated%20multimodal%20adapter%20module.%20We%20verify%20AWT%20in%20multiple%0Achallenging%20scenarios%2C%20including%20zero-shot%20and%20few-shot%20image%20classification%2C%0Azero-shot%20video%20action%20recognition%2C%20and%20out-of-distribution%20generalization.%20AWT%0Aconsistently%20outperforms%20the%20state-of-the-art%20methods%20in%20each%20setting.%20In%0Aaddition%2C%20our%20extensive%20studies%20further%20demonstrate%20AWT%27s%20effectiveness%20and%0Aadaptability%20across%20different%20VLMs%2C%20architectures%2C%20and%20scales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04603v1&entry.124074799=Read"},
{"title": "A Lightweight Video Anomaly Detection Model with Weak Supervision and\n  Adaptive Instance Selection", "author": "Yang Wang and Jiaogen Zhou and Jihong Guan", "abstract": "  Video anomaly detection is to determine whether there are any abnormal\nevents, behaviors or objects in a given video, which enables effective and\nintelligent public safety management. As video anomaly labeling is both\ntime-consuming and expensive, most existing works employ unsupervised or weakly\nsupervised learning methods. This paper focuses on weakly supervised video\nanomaly detection, in which the training videos are labeled whether or not they\ncontain any anomalies, but there is no information about which frames the\nanomalies are located. However, the uncertainty of weakly labeled data and the\nlarge model size prevent existing methods from wide deployment in real\nscenarios, especially the resource-limit situations such as edge-computing. In\nthis paper, we develop a lightweight video anomaly detection model. On the one\nhand, we propose an adaptive instance selection strategy, which is based on the\nmodel's current status to select confident instances, thereby mitigating the\nuncertainty of weakly labeled data and subsequently promoting the model's\nperformance. On the other hand, we design a lightweight multi-level temporal\ncorrelation attention module and an hourglass-shaped fully connected layer to\nconstruct the model, which can reduce the model parameters to only 0.56\\% of\nthe existing methods (e.g. RTFM). Our extensive experiments on two public\ndatasets UCF-Crime and ShanghaiTech show that our model can achieve comparable\nor even superior AUC score compared to the state-of-the-art methods, with a\nsignificantly reduced number of model parameters.\n", "link": "http://arxiv.org/abs/2310.05330v2", "date": "2024-07-05", "relevancy": 1.7422, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5964}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5756}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Video%20Anomaly%20Detection%20Model%20with%20Weak%20Supervision%20and%0A%20%20Adaptive%20Instance%20Selection&body=Title%3A%20A%20Lightweight%20Video%20Anomaly%20Detection%20Model%20with%20Weak%20Supervision%20and%0A%20%20Adaptive%20Instance%20Selection%0AAuthor%3A%20Yang%20Wang%20and%20Jiaogen%20Zhou%20and%20Jihong%20Guan%0AAbstract%3A%20%20%20Video%20anomaly%20detection%20is%20to%20determine%20whether%20there%20are%20any%20abnormal%0Aevents%2C%20behaviors%20or%20objects%20in%20a%20given%20video%2C%20which%20enables%20effective%20and%0Aintelligent%20public%20safety%20management.%20As%20video%20anomaly%20labeling%20is%20both%0Atime-consuming%20and%20expensive%2C%20most%20existing%20works%20employ%20unsupervised%20or%20weakly%0Asupervised%20learning%20methods.%20This%20paper%20focuses%20on%20weakly%20supervised%20video%0Aanomaly%20detection%2C%20in%20which%20the%20training%20videos%20are%20labeled%20whether%20or%20not%20they%0Acontain%20any%20anomalies%2C%20but%20there%20is%20no%20information%20about%20which%20frames%20the%0Aanomalies%20are%20located.%20However%2C%20the%20uncertainty%20of%20weakly%20labeled%20data%20and%20the%0Alarge%20model%20size%20prevent%20existing%20methods%20from%20wide%20deployment%20in%20real%0Ascenarios%2C%20especially%20the%20resource-limit%20situations%20such%20as%20edge-computing.%20In%0Athis%20paper%2C%20we%20develop%20a%20lightweight%20video%20anomaly%20detection%20model.%20On%20the%20one%0Ahand%2C%20we%20propose%20an%20adaptive%20instance%20selection%20strategy%2C%20which%20is%20based%20on%20the%0Amodel%27s%20current%20status%20to%20select%20confident%20instances%2C%20thereby%20mitigating%20the%0Auncertainty%20of%20weakly%20labeled%20data%20and%20subsequently%20promoting%20the%20model%27s%0Aperformance.%20On%20the%20other%20hand%2C%20we%20design%20a%20lightweight%20multi-level%20temporal%0Acorrelation%20attention%20module%20and%20an%20hourglass-shaped%20fully%20connected%20layer%20to%0Aconstruct%20the%20model%2C%20which%20can%20reduce%20the%20model%20parameters%20to%20only%200.56%5C%25%20of%0Athe%20existing%20methods%20%28e.g.%20RTFM%29.%20Our%20extensive%20experiments%20on%20two%20public%0Adatasets%20UCF-Crime%20and%20ShanghaiTech%20show%20that%20our%20model%20can%20achieve%20comparable%0Aor%20even%20superior%20AUC%20score%20compared%20to%20the%20state-of-the-art%20methods%2C%20with%20a%0Asignificantly%20reduced%20number%20of%20model%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05330v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Video%2520Anomaly%2520Detection%2520Model%2520with%2520Weak%2520Supervision%2520and%250A%2520%2520Adaptive%2520Instance%2520Selection%26entry.906535625%3DYang%2520Wang%2520and%2520Jiaogen%2520Zhou%2520and%2520Jihong%2520Guan%26entry.1292438233%3D%2520%2520Video%2520anomaly%2520detection%2520is%2520to%2520determine%2520whether%2520there%2520are%2520any%2520abnormal%250Aevents%252C%2520behaviors%2520or%2520objects%2520in%2520a%2520given%2520video%252C%2520which%2520enables%2520effective%2520and%250Aintelligent%2520public%2520safety%2520management.%2520As%2520video%2520anomaly%2520labeling%2520is%2520both%250Atime-consuming%2520and%2520expensive%252C%2520most%2520existing%2520works%2520employ%2520unsupervised%2520or%2520weakly%250Asupervised%2520learning%2520methods.%2520This%2520paper%2520focuses%2520on%2520weakly%2520supervised%2520video%250Aanomaly%2520detection%252C%2520in%2520which%2520the%2520training%2520videos%2520are%2520labeled%2520whether%2520or%2520not%2520they%250Acontain%2520any%2520anomalies%252C%2520but%2520there%2520is%2520no%2520information%2520about%2520which%2520frames%2520the%250Aanomalies%2520are%2520located.%2520However%252C%2520the%2520uncertainty%2520of%2520weakly%2520labeled%2520data%2520and%2520the%250Alarge%2520model%2520size%2520prevent%2520existing%2520methods%2520from%2520wide%2520deployment%2520in%2520real%250Ascenarios%252C%2520especially%2520the%2520resource-limit%2520situations%2520such%2520as%2520edge-computing.%2520In%250Athis%2520paper%252C%2520we%2520develop%2520a%2520lightweight%2520video%2520anomaly%2520detection%2520model.%2520On%2520the%2520one%250Ahand%252C%2520we%2520propose%2520an%2520adaptive%2520instance%2520selection%2520strategy%252C%2520which%2520is%2520based%2520on%2520the%250Amodel%2527s%2520current%2520status%2520to%2520select%2520confident%2520instances%252C%2520thereby%2520mitigating%2520the%250Auncertainty%2520of%2520weakly%2520labeled%2520data%2520and%2520subsequently%2520promoting%2520the%2520model%2527s%250Aperformance.%2520On%2520the%2520other%2520hand%252C%2520we%2520design%2520a%2520lightweight%2520multi-level%2520temporal%250Acorrelation%2520attention%2520module%2520and%2520an%2520hourglass-shaped%2520fully%2520connected%2520layer%2520to%250Aconstruct%2520the%2520model%252C%2520which%2520can%2520reduce%2520the%2520model%2520parameters%2520to%2520only%25200.56%255C%2525%2520of%250Athe%2520existing%2520methods%2520%2528e.g.%2520RTFM%2529.%2520Our%2520extensive%2520experiments%2520on%2520two%2520public%250Adatasets%2520UCF-Crime%2520and%2520ShanghaiTech%2520show%2520that%2520our%2520model%2520can%2520achieve%2520comparable%250Aor%2520even%2520superior%2520AUC%2520score%2520compared%2520to%2520the%2520state-of-the-art%2520methods%252C%2520with%2520a%250Asignificantly%2520reduced%2520number%2520of%2520model%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05330v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Video%20Anomaly%20Detection%20Model%20with%20Weak%20Supervision%20and%0A%20%20Adaptive%20Instance%20Selection&entry.906535625=Yang%20Wang%20and%20Jiaogen%20Zhou%20and%20Jihong%20Guan&entry.1292438233=%20%20Video%20anomaly%20detection%20is%20to%20determine%20whether%20there%20are%20any%20abnormal%0Aevents%2C%20behaviors%20or%20objects%20in%20a%20given%20video%2C%20which%20enables%20effective%20and%0Aintelligent%20public%20safety%20management.%20As%20video%20anomaly%20labeling%20is%20both%0Atime-consuming%20and%20expensive%2C%20most%20existing%20works%20employ%20unsupervised%20or%20weakly%0Asupervised%20learning%20methods.%20This%20paper%20focuses%20on%20weakly%20supervised%20video%0Aanomaly%20detection%2C%20in%20which%20the%20training%20videos%20are%20labeled%20whether%20or%20not%20they%0Acontain%20any%20anomalies%2C%20but%20there%20is%20no%20information%20about%20which%20frames%20the%0Aanomalies%20are%20located.%20However%2C%20the%20uncertainty%20of%20weakly%20labeled%20data%20and%20the%0Alarge%20model%20size%20prevent%20existing%20methods%20from%20wide%20deployment%20in%20real%0Ascenarios%2C%20especially%20the%20resource-limit%20situations%20such%20as%20edge-computing.%20In%0Athis%20paper%2C%20we%20develop%20a%20lightweight%20video%20anomaly%20detection%20model.%20On%20the%20one%0Ahand%2C%20we%20propose%20an%20adaptive%20instance%20selection%20strategy%2C%20which%20is%20based%20on%20the%0Amodel%27s%20current%20status%20to%20select%20confident%20instances%2C%20thereby%20mitigating%20the%0Auncertainty%20of%20weakly%20labeled%20data%20and%20subsequently%20promoting%20the%20model%27s%0Aperformance.%20On%20the%20other%20hand%2C%20we%20design%20a%20lightweight%20multi-level%20temporal%0Acorrelation%20attention%20module%20and%20an%20hourglass-shaped%20fully%20connected%20layer%20to%0Aconstruct%20the%20model%2C%20which%20can%20reduce%20the%20model%20parameters%20to%20only%200.56%5C%25%20of%0Athe%20existing%20methods%20%28e.g.%20RTFM%29.%20Our%20extensive%20experiments%20on%20two%20public%0Adatasets%20UCF-Crime%20and%20ShanghaiTech%20show%20that%20our%20model%20can%20achieve%20comparable%0Aor%20even%20superior%20AUC%20score%20compared%20to%20the%20state-of-the-art%20methods%2C%20with%20a%0Asignificantly%20reduced%20number%20of%20model%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05330v2&entry.124074799=Read"},
{"title": "VCoME: Verbal Video Composition with Multimodal Editing Effects", "author": "Weibo Gong and Xiaojie Jin and Xin Li and Dongliang He and Xinglong Wu", "abstract": "  Verbal videos, featuring voice-overs or text overlays, provide valuable\ncontent but present significant challenges in composition, especially when\nincorporating editing effects to enhance clarity and visual appeal. In this\npaper, we introduce the novel task of verbal video composition with editing\neffects. This task aims to generate coherent and visually appealing verbal\nvideos by integrating multimodal editing effects across textual, visual, and\naudio categories. To achieve this, we curate a large-scale dataset of video\neffects compositions from publicly available sources. We then formulate this\ntask as a generative problem, involving the identification of appropriate\npositions in the verbal content and the recommendation of editing effects for\nthese positions. To address this task, we propose VCoME, a general framework\nthat employs a large multimodal model to generate editing effects for video\ncomposition. Specifically, VCoME takes in the multimodal video context and\nautoregressively outputs where to apply effects within the verbal content and\nwhich effects are most appropriate for each position. VCoME also supports\nprompt-based control of composition density and style, providing substantial\nflexibility for diverse applications. Through extensive quantitative and\nqualitative evaluations, we clearly demonstrate the effectiveness of VCoME. A\ncomprehensive user study shows that our method produces videos of professional\nquality while being 85$\\times$ more efficient than professional editors.\n", "link": "http://arxiv.org/abs/2407.04697v1", "date": "2024-07-05", "relevancy": 1.7279, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5973}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5634}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCoME%3A%20Verbal%20Video%20Composition%20with%20Multimodal%20Editing%20Effects&body=Title%3A%20VCoME%3A%20Verbal%20Video%20Composition%20with%20Multimodal%20Editing%20Effects%0AAuthor%3A%20Weibo%20Gong%20and%20Xiaojie%20Jin%20and%20Xin%20Li%20and%20Dongliang%20He%20and%20Xinglong%20Wu%0AAbstract%3A%20%20%20Verbal%20videos%2C%20featuring%20voice-overs%20or%20text%20overlays%2C%20provide%20valuable%0Acontent%20but%20present%20significant%20challenges%20in%20composition%2C%20especially%20when%0Aincorporating%20editing%20effects%20to%20enhance%20clarity%20and%20visual%20appeal.%20In%20this%0Apaper%2C%20we%20introduce%20the%20novel%20task%20of%20verbal%20video%20composition%20with%20editing%0Aeffects.%20This%20task%20aims%20to%20generate%20coherent%20and%20visually%20appealing%20verbal%0Avideos%20by%20integrating%20multimodal%20editing%20effects%20across%20textual%2C%20visual%2C%20and%0Aaudio%20categories.%20To%20achieve%20this%2C%20we%20curate%20a%20large-scale%20dataset%20of%20video%0Aeffects%20compositions%20from%20publicly%20available%20sources.%20We%20then%20formulate%20this%0Atask%20as%20a%20generative%20problem%2C%20involving%20the%20identification%20of%20appropriate%0Apositions%20in%20the%20verbal%20content%20and%20the%20recommendation%20of%20editing%20effects%20for%0Athese%20positions.%20To%20address%20this%20task%2C%20we%20propose%20VCoME%2C%20a%20general%20framework%0Athat%20employs%20a%20large%20multimodal%20model%20to%20generate%20editing%20effects%20for%20video%0Acomposition.%20Specifically%2C%20VCoME%20takes%20in%20the%20multimodal%20video%20context%20and%0Aautoregressively%20outputs%20where%20to%20apply%20effects%20within%20the%20verbal%20content%20and%0Awhich%20effects%20are%20most%20appropriate%20for%20each%20position.%20VCoME%20also%20supports%0Aprompt-based%20control%20of%20composition%20density%20and%20style%2C%20providing%20substantial%0Aflexibility%20for%20diverse%20applications.%20Through%20extensive%20quantitative%20and%0Aqualitative%20evaluations%2C%20we%20clearly%20demonstrate%20the%20effectiveness%20of%20VCoME.%20A%0Acomprehensive%20user%20study%20shows%20that%20our%20method%20produces%20videos%20of%20professional%0Aquality%20while%20being%2085%24%5Ctimes%24%20more%20efficient%20than%20professional%20editors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCoME%253A%2520Verbal%2520Video%2520Composition%2520with%2520Multimodal%2520Editing%2520Effects%26entry.906535625%3DWeibo%2520Gong%2520and%2520Xiaojie%2520Jin%2520and%2520Xin%2520Li%2520and%2520Dongliang%2520He%2520and%2520Xinglong%2520Wu%26entry.1292438233%3D%2520%2520Verbal%2520videos%252C%2520featuring%2520voice-overs%2520or%2520text%2520overlays%252C%2520provide%2520valuable%250Acontent%2520but%2520present%2520significant%2520challenges%2520in%2520composition%252C%2520especially%2520when%250Aincorporating%2520editing%2520effects%2520to%2520enhance%2520clarity%2520and%2520visual%2520appeal.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520the%2520novel%2520task%2520of%2520verbal%2520video%2520composition%2520with%2520editing%250Aeffects.%2520This%2520task%2520aims%2520to%2520generate%2520coherent%2520and%2520visually%2520appealing%2520verbal%250Avideos%2520by%2520integrating%2520multimodal%2520editing%2520effects%2520across%2520textual%252C%2520visual%252C%2520and%250Aaudio%2520categories.%2520To%2520achieve%2520this%252C%2520we%2520curate%2520a%2520large-scale%2520dataset%2520of%2520video%250Aeffects%2520compositions%2520from%2520publicly%2520available%2520sources.%2520We%2520then%2520formulate%2520this%250Atask%2520as%2520a%2520generative%2520problem%252C%2520involving%2520the%2520identification%2520of%2520appropriate%250Apositions%2520in%2520the%2520verbal%2520content%2520and%2520the%2520recommendation%2520of%2520editing%2520effects%2520for%250Athese%2520positions.%2520To%2520address%2520this%2520task%252C%2520we%2520propose%2520VCoME%252C%2520a%2520general%2520framework%250Athat%2520employs%2520a%2520large%2520multimodal%2520model%2520to%2520generate%2520editing%2520effects%2520for%2520video%250Acomposition.%2520Specifically%252C%2520VCoME%2520takes%2520in%2520the%2520multimodal%2520video%2520context%2520and%250Aautoregressively%2520outputs%2520where%2520to%2520apply%2520effects%2520within%2520the%2520verbal%2520content%2520and%250Awhich%2520effects%2520are%2520most%2520appropriate%2520for%2520each%2520position.%2520VCoME%2520also%2520supports%250Aprompt-based%2520control%2520of%2520composition%2520density%2520and%2520style%252C%2520providing%2520substantial%250Aflexibility%2520for%2520diverse%2520applications.%2520Through%2520extensive%2520quantitative%2520and%250Aqualitative%2520evaluations%252C%2520we%2520clearly%2520demonstrate%2520the%2520effectiveness%2520of%2520VCoME.%2520A%250Acomprehensive%2520user%2520study%2520shows%2520that%2520our%2520method%2520produces%2520videos%2520of%2520professional%250Aquality%2520while%2520being%252085%2524%255Ctimes%2524%2520more%2520efficient%2520than%2520professional%2520editors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCoME%3A%20Verbal%20Video%20Composition%20with%20Multimodal%20Editing%20Effects&entry.906535625=Weibo%20Gong%20and%20Xiaojie%20Jin%20and%20Xin%20Li%20and%20Dongliang%20He%20and%20Xinglong%20Wu&entry.1292438233=%20%20Verbal%20videos%2C%20featuring%20voice-overs%20or%20text%20overlays%2C%20provide%20valuable%0Acontent%20but%20present%20significant%20challenges%20in%20composition%2C%20especially%20when%0Aincorporating%20editing%20effects%20to%20enhance%20clarity%20and%20visual%20appeal.%20In%20this%0Apaper%2C%20we%20introduce%20the%20novel%20task%20of%20verbal%20video%20composition%20with%20editing%0Aeffects.%20This%20task%20aims%20to%20generate%20coherent%20and%20visually%20appealing%20verbal%0Avideos%20by%20integrating%20multimodal%20editing%20effects%20across%20textual%2C%20visual%2C%20and%0Aaudio%20categories.%20To%20achieve%20this%2C%20we%20curate%20a%20large-scale%20dataset%20of%20video%0Aeffects%20compositions%20from%20publicly%20available%20sources.%20We%20then%20formulate%20this%0Atask%20as%20a%20generative%20problem%2C%20involving%20the%20identification%20of%20appropriate%0Apositions%20in%20the%20verbal%20content%20and%20the%20recommendation%20of%20editing%20effects%20for%0Athese%20positions.%20To%20address%20this%20task%2C%20we%20propose%20VCoME%2C%20a%20general%20framework%0Athat%20employs%20a%20large%20multimodal%20model%20to%20generate%20editing%20effects%20for%20video%0Acomposition.%20Specifically%2C%20VCoME%20takes%20in%20the%20multimodal%20video%20context%20and%0Aautoregressively%20outputs%20where%20to%20apply%20effects%20within%20the%20verbal%20content%20and%0Awhich%20effects%20are%20most%20appropriate%20for%20each%20position.%20VCoME%20also%20supports%0Aprompt-based%20control%20of%20composition%20density%20and%20style%2C%20providing%20substantial%0Aflexibility%20for%20diverse%20applications.%20Through%20extensive%20quantitative%20and%0Aqualitative%20evaluations%2C%20we%20clearly%20demonstrate%20the%20effectiveness%20of%20VCoME.%20A%0Acomprehensive%20user%20study%20shows%20that%20our%20method%20produces%20videos%20of%20professional%0Aquality%20while%20being%2085%24%5Ctimes%24%20more%20efficient%20than%20professional%20editors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04697v1&entry.124074799=Read"},
{"title": "Multimodal Classification via Modal-Aware Interactive Enhancement", "author": "Qing-Yuan Jiang and Zhouyang Chi and Yang Yang", "abstract": "  Due to the notorious modality imbalance problem, multimodal learning (MML)\nleads to the phenomenon of optimization imbalance, thus struggling to achieve\nsatisfactory performance. Recently, some representative methods have been\nproposed to boost the performance, mainly focusing on adaptive adjusting the\noptimization of each modality to rebalance the learning speed of dominant and\nnon-dominant modalities. To better facilitate the interaction of model\ninformation in multimodal learning, in this paper, we propose a novel\nmultimodal learning method, called modal-aware interactive enhancement (MIE).\nSpecifically, we first utilize an optimization strategy based on sharpness\naware minimization (SAM) to smooth the learning objective during the forward\nphase. Then, with the help of the geometry property of SAM, we propose a\ngradient modification strategy to impose the influence between different\nmodalities during the backward phase. Therefore, we can improve the\ngeneralization ability and alleviate the modality forgetting phenomenon\nsimultaneously for multimodal learning. Extensive experiments on widely used\ndatasets demonstrate that our proposed method can outperform various\nstate-of-the-art baselines to achieve the best performance.\n", "link": "http://arxiv.org/abs/2407.04587v1", "date": "2024-07-05", "relevancy": 1.7231, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5872}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5772}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Classification%20via%20Modal-Aware%20Interactive%20Enhancement&body=Title%3A%20Multimodal%20Classification%20via%20Modal-Aware%20Interactive%20Enhancement%0AAuthor%3A%20Qing-Yuan%20Jiang%20and%20Zhouyang%20Chi%20and%20Yang%20Yang%0AAbstract%3A%20%20%20Due%20to%20the%20notorious%20modality%20imbalance%20problem%2C%20multimodal%20learning%20%28MML%29%0Aleads%20to%20the%20phenomenon%20of%20optimization%20imbalance%2C%20thus%20struggling%20to%20achieve%0Asatisfactory%20performance.%20Recently%2C%20some%20representative%20methods%20have%20been%0Aproposed%20to%20boost%20the%20performance%2C%20mainly%20focusing%20on%20adaptive%20adjusting%20the%0Aoptimization%20of%20each%20modality%20to%20rebalance%20the%20learning%20speed%20of%20dominant%20and%0Anon-dominant%20modalities.%20To%20better%20facilitate%20the%20interaction%20of%20model%0Ainformation%20in%20multimodal%20learning%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Amultimodal%20learning%20method%2C%20called%20modal-aware%20interactive%20enhancement%20%28MIE%29.%0ASpecifically%2C%20we%20first%20utilize%20an%20optimization%20strategy%20based%20on%20sharpness%0Aaware%20minimization%20%28SAM%29%20to%20smooth%20the%20learning%20objective%20during%20the%20forward%0Aphase.%20Then%2C%20with%20the%20help%20of%20the%20geometry%20property%20of%20SAM%2C%20we%20propose%20a%0Agradient%20modification%20strategy%20to%20impose%20the%20influence%20between%20different%0Amodalities%20during%20the%20backward%20phase.%20Therefore%2C%20we%20can%20improve%20the%0Ageneralization%20ability%20and%20alleviate%20the%20modality%20forgetting%20phenomenon%0Asimultaneously%20for%20multimodal%20learning.%20Extensive%20experiments%20on%20widely%20used%0Adatasets%20demonstrate%20that%20our%20proposed%20method%20can%20outperform%20various%0Astate-of-the-art%20baselines%20to%20achieve%20the%20best%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Classification%2520via%2520Modal-Aware%2520Interactive%2520Enhancement%26entry.906535625%3DQing-Yuan%2520Jiang%2520and%2520Zhouyang%2520Chi%2520and%2520Yang%2520Yang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520notorious%2520modality%2520imbalance%2520problem%252C%2520multimodal%2520learning%2520%2528MML%2529%250Aleads%2520to%2520the%2520phenomenon%2520of%2520optimization%2520imbalance%252C%2520thus%2520struggling%2520to%2520achieve%250Asatisfactory%2520performance.%2520Recently%252C%2520some%2520representative%2520methods%2520have%2520been%250Aproposed%2520to%2520boost%2520the%2520performance%252C%2520mainly%2520focusing%2520on%2520adaptive%2520adjusting%2520the%250Aoptimization%2520of%2520each%2520modality%2520to%2520rebalance%2520the%2520learning%2520speed%2520of%2520dominant%2520and%250Anon-dominant%2520modalities.%2520To%2520better%2520facilitate%2520the%2520interaction%2520of%2520model%250Ainformation%2520in%2520multimodal%2520learning%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Amultimodal%2520learning%2520method%252C%2520called%2520modal-aware%2520interactive%2520enhancement%2520%2528MIE%2529.%250ASpecifically%252C%2520we%2520first%2520utilize%2520an%2520optimization%2520strategy%2520based%2520on%2520sharpness%250Aaware%2520minimization%2520%2528SAM%2529%2520to%2520smooth%2520the%2520learning%2520objective%2520during%2520the%2520forward%250Aphase.%2520Then%252C%2520with%2520the%2520help%2520of%2520the%2520geometry%2520property%2520of%2520SAM%252C%2520we%2520propose%2520a%250Agradient%2520modification%2520strategy%2520to%2520impose%2520the%2520influence%2520between%2520different%250Amodalities%2520during%2520the%2520backward%2520phase.%2520Therefore%252C%2520we%2520can%2520improve%2520the%250Ageneralization%2520ability%2520and%2520alleviate%2520the%2520modality%2520forgetting%2520phenomenon%250Asimultaneously%2520for%2520multimodal%2520learning.%2520Extensive%2520experiments%2520on%2520widely%2520used%250Adatasets%2520demonstrate%2520that%2520our%2520proposed%2520method%2520can%2520outperform%2520various%250Astate-of-the-art%2520baselines%2520to%2520achieve%2520the%2520best%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Classification%20via%20Modal-Aware%20Interactive%20Enhancement&entry.906535625=Qing-Yuan%20Jiang%20and%20Zhouyang%20Chi%20and%20Yang%20Yang&entry.1292438233=%20%20Due%20to%20the%20notorious%20modality%20imbalance%20problem%2C%20multimodal%20learning%20%28MML%29%0Aleads%20to%20the%20phenomenon%20of%20optimization%20imbalance%2C%20thus%20struggling%20to%20achieve%0Asatisfactory%20performance.%20Recently%2C%20some%20representative%20methods%20have%20been%0Aproposed%20to%20boost%20the%20performance%2C%20mainly%20focusing%20on%20adaptive%20adjusting%20the%0Aoptimization%20of%20each%20modality%20to%20rebalance%20the%20learning%20speed%20of%20dominant%20and%0Anon-dominant%20modalities.%20To%20better%20facilitate%20the%20interaction%20of%20model%0Ainformation%20in%20multimodal%20learning%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%0Amultimodal%20learning%20method%2C%20called%20modal-aware%20interactive%20enhancement%20%28MIE%29.%0ASpecifically%2C%20we%20first%20utilize%20an%20optimization%20strategy%20based%20on%20sharpness%0Aaware%20minimization%20%28SAM%29%20to%20smooth%20the%20learning%20objective%20during%20the%20forward%0Aphase.%20Then%2C%20with%20the%20help%20of%20the%20geometry%20property%20of%20SAM%2C%20we%20propose%20a%0Agradient%20modification%20strategy%20to%20impose%20the%20influence%20between%20different%0Amodalities%20during%20the%20backward%20phase.%20Therefore%2C%20we%20can%20improve%20the%0Ageneralization%20ability%20and%20alleviate%20the%20modality%20forgetting%20phenomenon%0Asimultaneously%20for%20multimodal%20learning.%20Extensive%20experiments%20on%20widely%20used%0Adatasets%20demonstrate%20that%20our%20proposed%20method%20can%20outperform%20various%0Astate-of-the-art%20baselines%20to%20achieve%20the%20best%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04587v1&entry.124074799=Read"},
{"title": "RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot\n  Robotic Manipulation", "author": "Yuxuan Kuang and Junjie Ye and Haoran Geng and Jiageng Mao and Congyue Deng and Leonidas Guibas and He Wang and Yue Wang", "abstract": "  This work proposes a retrieve-and-transfer framework for zero-shot robotic\nmanipulation, dubbed RAM, featuring generalizability across various objects,\nenvironments, and embodiments. Unlike existing approaches that learn\nmanipulation from expensive in-domain demonstrations, RAM capitalizes on a\nretrieval-based affordance transfer paradigm to acquire versatile manipulation\ncapabilities from abundant out-of-domain data. First, RAM extracts unified\naffordance at scale from diverse sources of demonstrations including robotic\ndata, human-object interaction (HOI) data, and custom data to construct a\ncomprehensive affordance memory. Then given a language instruction, RAM\nhierarchically retrieves the most similar demonstration from the affordance\nmemory and transfers such out-of-domain 2D affordance to in-domain 3D\nexecutable affordance in a zero-shot and embodiment-agnostic manner. Extensive\nsimulation and real-world evaluations demonstrate that our RAM consistently\noutperforms existing works in diverse daily tasks. Additionally, RAM shows\nsignificant potential for downstream applications such as automatic and\nefficient data collection, one-shot visual imitation, and LLM/VLM-integrated\nlong-horizon manipulation. For more details, please check our website at\nhttps://yxkryptonite.github.io/RAM/.\n", "link": "http://arxiv.org/abs/2407.04689v1", "date": "2024-07-05", "relevancy": 1.7048, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6082}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAM%3A%20Retrieval-Based%20Affordance%20Transfer%20for%20Generalizable%20Zero-Shot%0A%20%20Robotic%20Manipulation&body=Title%3A%20RAM%3A%20Retrieval-Based%20Affordance%20Transfer%20for%20Generalizable%20Zero-Shot%0A%20%20Robotic%20Manipulation%0AAuthor%3A%20Yuxuan%20Kuang%20and%20Junjie%20Ye%20and%20Haoran%20Geng%20and%20Jiageng%20Mao%20and%20Congyue%20Deng%20and%20Leonidas%20Guibas%20and%20He%20Wang%20and%20Yue%20Wang%0AAbstract%3A%20%20%20This%20work%20proposes%20a%20retrieve-and-transfer%20framework%20for%20zero-shot%20robotic%0Amanipulation%2C%20dubbed%20RAM%2C%20featuring%20generalizability%20across%20various%20objects%2C%0Aenvironments%2C%20and%20embodiments.%20Unlike%20existing%20approaches%20that%20learn%0Amanipulation%20from%20expensive%20in-domain%20demonstrations%2C%20RAM%20capitalizes%20on%20a%0Aretrieval-based%20affordance%20transfer%20paradigm%20to%20acquire%20versatile%20manipulation%0Acapabilities%20from%20abundant%20out-of-domain%20data.%20First%2C%20RAM%20extracts%20unified%0Aaffordance%20at%20scale%20from%20diverse%20sources%20of%20demonstrations%20including%20robotic%0Adata%2C%20human-object%20interaction%20%28HOI%29%20data%2C%20and%20custom%20data%20to%20construct%20a%0Acomprehensive%20affordance%20memory.%20Then%20given%20a%20language%20instruction%2C%20RAM%0Ahierarchically%20retrieves%20the%20most%20similar%20demonstration%20from%20the%20affordance%0Amemory%20and%20transfers%20such%20out-of-domain%202D%20affordance%20to%20in-domain%203D%0Aexecutable%20affordance%20in%20a%20zero-shot%20and%20embodiment-agnostic%20manner.%20Extensive%0Asimulation%20and%20real-world%20evaluations%20demonstrate%20that%20our%20RAM%20consistently%0Aoutperforms%20existing%20works%20in%20diverse%20daily%20tasks.%20Additionally%2C%20RAM%20shows%0Asignificant%20potential%20for%20downstream%20applications%20such%20as%20automatic%20and%0Aefficient%20data%20collection%2C%20one-shot%20visual%20imitation%2C%20and%20LLM/VLM-integrated%0Along-horizon%20manipulation.%20For%20more%20details%2C%20please%20check%20our%20website%20at%0Ahttps%3A//yxkryptonite.github.io/RAM/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAM%253A%2520Retrieval-Based%2520Affordance%2520Transfer%2520for%2520Generalizable%2520Zero-Shot%250A%2520%2520Robotic%2520Manipulation%26entry.906535625%3DYuxuan%2520Kuang%2520and%2520Junjie%2520Ye%2520and%2520Haoran%2520Geng%2520and%2520Jiageng%2520Mao%2520and%2520Congyue%2520Deng%2520and%2520Leonidas%2520Guibas%2520and%2520He%2520Wang%2520and%2520Yue%2520Wang%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520a%2520retrieve-and-transfer%2520framework%2520for%2520zero-shot%2520robotic%250Amanipulation%252C%2520dubbed%2520RAM%252C%2520featuring%2520generalizability%2520across%2520various%2520objects%252C%250Aenvironments%252C%2520and%2520embodiments.%2520Unlike%2520existing%2520approaches%2520that%2520learn%250Amanipulation%2520from%2520expensive%2520in-domain%2520demonstrations%252C%2520RAM%2520capitalizes%2520on%2520a%250Aretrieval-based%2520affordance%2520transfer%2520paradigm%2520to%2520acquire%2520versatile%2520manipulation%250Acapabilities%2520from%2520abundant%2520out-of-domain%2520data.%2520First%252C%2520RAM%2520extracts%2520unified%250Aaffordance%2520at%2520scale%2520from%2520diverse%2520sources%2520of%2520demonstrations%2520including%2520robotic%250Adata%252C%2520human-object%2520interaction%2520%2528HOI%2529%2520data%252C%2520and%2520custom%2520data%2520to%2520construct%2520a%250Acomprehensive%2520affordance%2520memory.%2520Then%2520given%2520a%2520language%2520instruction%252C%2520RAM%250Ahierarchically%2520retrieves%2520the%2520most%2520similar%2520demonstration%2520from%2520the%2520affordance%250Amemory%2520and%2520transfers%2520such%2520out-of-domain%25202D%2520affordance%2520to%2520in-domain%25203D%250Aexecutable%2520affordance%2520in%2520a%2520zero-shot%2520and%2520embodiment-agnostic%2520manner.%2520Extensive%250Asimulation%2520and%2520real-world%2520evaluations%2520demonstrate%2520that%2520our%2520RAM%2520consistently%250Aoutperforms%2520existing%2520works%2520in%2520diverse%2520daily%2520tasks.%2520Additionally%252C%2520RAM%2520shows%250Asignificant%2520potential%2520for%2520downstream%2520applications%2520such%2520as%2520automatic%2520and%250Aefficient%2520data%2520collection%252C%2520one-shot%2520visual%2520imitation%252C%2520and%2520LLM/VLM-integrated%250Along-horizon%2520manipulation.%2520For%2520more%2520details%252C%2520please%2520check%2520our%2520website%2520at%250Ahttps%253A//yxkryptonite.github.io/RAM/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAM%3A%20Retrieval-Based%20Affordance%20Transfer%20for%20Generalizable%20Zero-Shot%0A%20%20Robotic%20Manipulation&entry.906535625=Yuxuan%20Kuang%20and%20Junjie%20Ye%20and%20Haoran%20Geng%20and%20Jiageng%20Mao%20and%20Congyue%20Deng%20and%20Leonidas%20Guibas%20and%20He%20Wang%20and%20Yue%20Wang&entry.1292438233=%20%20This%20work%20proposes%20a%20retrieve-and-transfer%20framework%20for%20zero-shot%20robotic%0Amanipulation%2C%20dubbed%20RAM%2C%20featuring%20generalizability%20across%20various%20objects%2C%0Aenvironments%2C%20and%20embodiments.%20Unlike%20existing%20approaches%20that%20learn%0Amanipulation%20from%20expensive%20in-domain%20demonstrations%2C%20RAM%20capitalizes%20on%20a%0Aretrieval-based%20affordance%20transfer%20paradigm%20to%20acquire%20versatile%20manipulation%0Acapabilities%20from%20abundant%20out-of-domain%20data.%20First%2C%20RAM%20extracts%20unified%0Aaffordance%20at%20scale%20from%20diverse%20sources%20of%20demonstrations%20including%20robotic%0Adata%2C%20human-object%20interaction%20%28HOI%29%20data%2C%20and%20custom%20data%20to%20construct%20a%0Acomprehensive%20affordance%20memory.%20Then%20given%20a%20language%20instruction%2C%20RAM%0Ahierarchically%20retrieves%20the%20most%20similar%20demonstration%20from%20the%20affordance%0Amemory%20and%20transfers%20such%20out-of-domain%202D%20affordance%20to%20in-domain%203D%0Aexecutable%20affordance%20in%20a%20zero-shot%20and%20embodiment-agnostic%20manner.%20Extensive%0Asimulation%20and%20real-world%20evaluations%20demonstrate%20that%20our%20RAM%20consistently%0Aoutperforms%20existing%20works%20in%20diverse%20daily%20tasks.%20Additionally%2C%20RAM%20shows%0Asignificant%20potential%20for%20downstream%20applications%20such%20as%20automatic%20and%0Aefficient%20data%20collection%2C%20one-shot%20visual%20imitation%2C%20and%20LLM/VLM-integrated%0Along-horizon%20manipulation.%20For%20more%20details%2C%20please%20check%20our%20website%20at%0Ahttps%3A//yxkryptonite.github.io/RAM/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04689v1&entry.124074799=Read"},
{"title": "Hyperspectral Dataset and Deep Learning methods for Waste from Electric\n  and Electronic Equipment Identification (WEEE)", "author": "Artzai Picon and Pablo Galan and Arantza Bereciartua-Perez and Leire Benito-del-Valle", "abstract": "  Hyperspectral imaging, a rapidly evolving field, has witnessed the ascendancy\nof deep learning techniques, supplanting classical feature extraction and\nclassification methods in various applications. However, many researchers\nemploy arbitrary architectures for hyperspectral image processing, often\nwithout rigorous analysis of the interplay between spectral and spatial\ninformation. This oversight neglects the implications of combining these two\nmodalities on model performance.\n  In this paper, we evaluate the performance of diverse deep learning\narchitectures for hyperspectral image segmentation. Our analysis disentangles\nthe impact of different architectures, spanning various spectral and spatial\ngranularities. Specifically, we investigate the effects of spectral resolution\n(capturing spectral information) and spatial texture (conveying spatial\ndetails) on segmentation outcomes. Additionally, we explore the transferability\nof knowledge from large pre-trained image foundation models, originally\ndesigned for RGB images, to the hyperspectral domain.\n  Results show that incorporating spatial information alongside spectral data\nleads to improved segmentation results, and that it is essential to further\nwork on novel architectures comprising spectral and spatial information and on\nthe adaption of RGB foundation models into the hyperspectral domain.\n  Furthermore, we contribute to the field by cleaning and publicly releasing\nthe Tecnalia WEEE Hyperspectral dataset. This dataset contains different\nnon-ferrous fractions of Waste Electrical and Electronic Equipment (WEEE),\nincluding Copper, Brass, Aluminum, Stainless Steel, and White Copper, spanning\nthe range of 400 to 1000 nm.\n  We expect these conclusions can guide novel researchers in the field of\nhyperspectral imaging.\n", "link": "http://arxiv.org/abs/2407.04505v1", "date": "2024-07-05", "relevancy": 1.4343, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.494}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4765}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperspectral%20Dataset%20and%20Deep%20Learning%20methods%20for%20Waste%20from%20Electric%0A%20%20and%20Electronic%20Equipment%20Identification%20%28WEEE%29&body=Title%3A%20Hyperspectral%20Dataset%20and%20Deep%20Learning%20methods%20for%20Waste%20from%20Electric%0A%20%20and%20Electronic%20Equipment%20Identification%20%28WEEE%29%0AAuthor%3A%20Artzai%20Picon%20and%20Pablo%20Galan%20and%20Arantza%20Bereciartua-Perez%20and%20Leire%20Benito-del-Valle%0AAbstract%3A%20%20%20Hyperspectral%20imaging%2C%20a%20rapidly%20evolving%20field%2C%20has%20witnessed%20the%20ascendancy%0Aof%20deep%20learning%20techniques%2C%20supplanting%20classical%20feature%20extraction%20and%0Aclassification%20methods%20in%20various%20applications.%20However%2C%20many%20researchers%0Aemploy%20arbitrary%20architectures%20for%20hyperspectral%20image%20processing%2C%20often%0Awithout%20rigorous%20analysis%20of%20the%20interplay%20between%20spectral%20and%20spatial%0Ainformation.%20This%20oversight%20neglects%20the%20implications%20of%20combining%20these%20two%0Amodalities%20on%20model%20performance.%0A%20%20In%20this%20paper%2C%20we%20evaluate%20the%20performance%20of%20diverse%20deep%20learning%0Aarchitectures%20for%20hyperspectral%20image%20segmentation.%20Our%20analysis%20disentangles%0Athe%20impact%20of%20different%20architectures%2C%20spanning%20various%20spectral%20and%20spatial%0Agranularities.%20Specifically%2C%20we%20investigate%20the%20effects%20of%20spectral%20resolution%0A%28capturing%20spectral%20information%29%20and%20spatial%20texture%20%28conveying%20spatial%0Adetails%29%20on%20segmentation%20outcomes.%20Additionally%2C%20we%20explore%20the%20transferability%0Aof%20knowledge%20from%20large%20pre-trained%20image%20foundation%20models%2C%20originally%0Adesigned%20for%20RGB%20images%2C%20to%20the%20hyperspectral%20domain.%0A%20%20Results%20show%20that%20incorporating%20spatial%20information%20alongside%20spectral%20data%0Aleads%20to%20improved%20segmentation%20results%2C%20and%20that%20it%20is%20essential%20to%20further%0Awork%20on%20novel%20architectures%20comprising%20spectral%20and%20spatial%20information%20and%20on%0Athe%20adaption%20of%20RGB%20foundation%20models%20into%20the%20hyperspectral%20domain.%0A%20%20Furthermore%2C%20we%20contribute%20to%20the%20field%20by%20cleaning%20and%20publicly%20releasing%0Athe%20Tecnalia%20WEEE%20Hyperspectral%20dataset.%20This%20dataset%20contains%20different%0Anon-ferrous%20fractions%20of%20Waste%20Electrical%20and%20Electronic%20Equipment%20%28WEEE%29%2C%0Aincluding%20Copper%2C%20Brass%2C%20Aluminum%2C%20Stainless%20Steel%2C%20and%20White%20Copper%2C%20spanning%0Athe%20range%20of%20400%20to%201000%20nm.%0A%20%20We%20expect%20these%20conclusions%20can%20guide%20novel%20researchers%20in%20the%20field%20of%0Ahyperspectral%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperspectral%2520Dataset%2520and%2520Deep%2520Learning%2520methods%2520for%2520Waste%2520from%2520Electric%250A%2520%2520and%2520Electronic%2520Equipment%2520Identification%2520%2528WEEE%2529%26entry.906535625%3DArtzai%2520Picon%2520and%2520Pablo%2520Galan%2520and%2520Arantza%2520Bereciartua-Perez%2520and%2520Leire%2520Benito-del-Valle%26entry.1292438233%3D%2520%2520Hyperspectral%2520imaging%252C%2520a%2520rapidly%2520evolving%2520field%252C%2520has%2520witnessed%2520the%2520ascendancy%250Aof%2520deep%2520learning%2520techniques%252C%2520supplanting%2520classical%2520feature%2520extraction%2520and%250Aclassification%2520methods%2520in%2520various%2520applications.%2520However%252C%2520many%2520researchers%250Aemploy%2520arbitrary%2520architectures%2520for%2520hyperspectral%2520image%2520processing%252C%2520often%250Awithout%2520rigorous%2520analysis%2520of%2520the%2520interplay%2520between%2520spectral%2520and%2520spatial%250Ainformation.%2520This%2520oversight%2520neglects%2520the%2520implications%2520of%2520combining%2520these%2520two%250Amodalities%2520on%2520model%2520performance.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520evaluate%2520the%2520performance%2520of%2520diverse%2520deep%2520learning%250Aarchitectures%2520for%2520hyperspectral%2520image%2520segmentation.%2520Our%2520analysis%2520disentangles%250Athe%2520impact%2520of%2520different%2520architectures%252C%2520spanning%2520various%2520spectral%2520and%2520spatial%250Agranularities.%2520Specifically%252C%2520we%2520investigate%2520the%2520effects%2520of%2520spectral%2520resolution%250A%2528capturing%2520spectral%2520information%2529%2520and%2520spatial%2520texture%2520%2528conveying%2520spatial%250Adetails%2529%2520on%2520segmentation%2520outcomes.%2520Additionally%252C%2520we%2520explore%2520the%2520transferability%250Aof%2520knowledge%2520from%2520large%2520pre-trained%2520image%2520foundation%2520models%252C%2520originally%250Adesigned%2520for%2520RGB%2520images%252C%2520to%2520the%2520hyperspectral%2520domain.%250A%2520%2520Results%2520show%2520that%2520incorporating%2520spatial%2520information%2520alongside%2520spectral%2520data%250Aleads%2520to%2520improved%2520segmentation%2520results%252C%2520and%2520that%2520it%2520is%2520essential%2520to%2520further%250Awork%2520on%2520novel%2520architectures%2520comprising%2520spectral%2520and%2520spatial%2520information%2520and%2520on%250Athe%2520adaption%2520of%2520RGB%2520foundation%2520models%2520into%2520the%2520hyperspectral%2520domain.%250A%2520%2520Furthermore%252C%2520we%2520contribute%2520to%2520the%2520field%2520by%2520cleaning%2520and%2520publicly%2520releasing%250Athe%2520Tecnalia%2520WEEE%2520Hyperspectral%2520dataset.%2520This%2520dataset%2520contains%2520different%250Anon-ferrous%2520fractions%2520of%2520Waste%2520Electrical%2520and%2520Electronic%2520Equipment%2520%2528WEEE%2529%252C%250Aincluding%2520Copper%252C%2520Brass%252C%2520Aluminum%252C%2520Stainless%2520Steel%252C%2520and%2520White%2520Copper%252C%2520spanning%250Athe%2520range%2520of%2520400%2520to%25201000%2520nm.%250A%2520%2520We%2520expect%2520these%2520conclusions%2520can%2520guide%2520novel%2520researchers%2520in%2520the%2520field%2520of%250Ahyperspectral%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperspectral%20Dataset%20and%20Deep%20Learning%20methods%20for%20Waste%20from%20Electric%0A%20%20and%20Electronic%20Equipment%20Identification%20%28WEEE%29&entry.906535625=Artzai%20Picon%20and%20Pablo%20Galan%20and%20Arantza%20Bereciartua-Perez%20and%20Leire%20Benito-del-Valle&entry.1292438233=%20%20Hyperspectral%20imaging%2C%20a%20rapidly%20evolving%20field%2C%20has%20witnessed%20the%20ascendancy%0Aof%20deep%20learning%20techniques%2C%20supplanting%20classical%20feature%20extraction%20and%0Aclassification%20methods%20in%20various%20applications.%20However%2C%20many%20researchers%0Aemploy%20arbitrary%20architectures%20for%20hyperspectral%20image%20processing%2C%20often%0Awithout%20rigorous%20analysis%20of%20the%20interplay%20between%20spectral%20and%20spatial%0Ainformation.%20This%20oversight%20neglects%20the%20implications%20of%20combining%20these%20two%0Amodalities%20on%20model%20performance.%0A%20%20In%20this%20paper%2C%20we%20evaluate%20the%20performance%20of%20diverse%20deep%20learning%0Aarchitectures%20for%20hyperspectral%20image%20segmentation.%20Our%20analysis%20disentangles%0Athe%20impact%20of%20different%20architectures%2C%20spanning%20various%20spectral%20and%20spatial%0Agranularities.%20Specifically%2C%20we%20investigate%20the%20effects%20of%20spectral%20resolution%0A%28capturing%20spectral%20information%29%20and%20spatial%20texture%20%28conveying%20spatial%0Adetails%29%20on%20segmentation%20outcomes.%20Additionally%2C%20we%20explore%20the%20transferability%0Aof%20knowledge%20from%20large%20pre-trained%20image%20foundation%20models%2C%20originally%0Adesigned%20for%20RGB%20images%2C%20to%20the%20hyperspectral%20domain.%0A%20%20Results%20show%20that%20incorporating%20spatial%20information%20alongside%20spectral%20data%0Aleads%20to%20improved%20segmentation%20results%2C%20and%20that%20it%20is%20essential%20to%20further%0Awork%20on%20novel%20architectures%20comprising%20spectral%20and%20spatial%20information%20and%20on%0Athe%20adaption%20of%20RGB%20foundation%20models%20into%20the%20hyperspectral%20domain.%0A%20%20Furthermore%2C%20we%20contribute%20to%20the%20field%20by%20cleaning%20and%20publicly%20releasing%0Athe%20Tecnalia%20WEEE%20Hyperspectral%20dataset.%20This%20dataset%20contains%20different%0Anon-ferrous%20fractions%20of%20Waste%20Electrical%20and%20Electronic%20Equipment%20%28WEEE%29%2C%0Aincluding%20Copper%2C%20Brass%2C%20Aluminum%2C%20Stainless%20Steel%2C%20and%20White%20Copper%2C%20spanning%0Athe%20range%20of%20400%20to%201000%20nm.%0A%20%20We%20expect%20these%20conclusions%20can%20guide%20novel%20researchers%20in%20the%20field%20of%0Ahyperspectral%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04505v1&entry.124074799=Read"},
{"title": "The False Dawn: Reevaluating Google's Reinforcement Learning for Chip\n  Macro Placement", "author": "Igor L. Markov", "abstract": "  Reinforcement learning (RL) for physical design of silicon chips in a Google\n2021 Nature paper stirred controversy due to poorly documented claims that\nraised eyebrows and drew critical media coverage. The paper withheld critical\nmethodology steps and most inputs needed to reproduce results. Our\nmeta-analysis shows how two separate evaluations filled in the gaps and\ndemonstrated that Google RL lags behind (i) human designers, (ii) a well-known\nalgorithm (Simulated Annealing), and (iii) generally-available commercial\nsoftware, while being slower; and in a 2023 open research contest, RL methods\nweren't in top 5. Crosschecked data indicate that the integrity of the Nature\npaper is substantially undermined owing to errors in conduct, analysis and\nreporting. Before publishing, Google rebuffed internal allegations of fraud,\nwhich still stand. We note policy implications and conclusions for chip design.\n", "link": "http://arxiv.org/abs/2306.09633v9", "date": "2024-07-05", "relevancy": 1.6882, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4419}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4303}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20False%20Dawn%3A%20Reevaluating%20Google%27s%20Reinforcement%20Learning%20for%20Chip%0A%20%20Macro%20Placement&body=Title%3A%20The%20False%20Dawn%3A%20Reevaluating%20Google%27s%20Reinforcement%20Learning%20for%20Chip%0A%20%20Macro%20Placement%0AAuthor%3A%20Igor%20L.%20Markov%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20for%20physical%20design%20of%20silicon%20chips%20in%20a%20Google%0A2021%20Nature%20paper%20stirred%20controversy%20due%20to%20poorly%20documented%20claims%20that%0Araised%20eyebrows%20and%20drew%20critical%20media%20coverage.%20The%20paper%20withheld%20critical%0Amethodology%20steps%20and%20most%20inputs%20needed%20to%20reproduce%20results.%20Our%0Ameta-analysis%20shows%20how%20two%20separate%20evaluations%20filled%20in%20the%20gaps%20and%0Ademonstrated%20that%20Google%20RL%20lags%20behind%20%28i%29%20human%20designers%2C%20%28ii%29%20a%20well-known%0Aalgorithm%20%28Simulated%20Annealing%29%2C%20and%20%28iii%29%20generally-available%20commercial%0Asoftware%2C%20while%20being%20slower%3B%20and%20in%20a%202023%20open%20research%20contest%2C%20RL%20methods%0Aweren%27t%20in%20top%205.%20Crosschecked%20data%20indicate%20that%20the%20integrity%20of%20the%20Nature%0Apaper%20is%20substantially%20undermined%20owing%20to%20errors%20in%20conduct%2C%20analysis%20and%0Areporting.%20Before%20publishing%2C%20Google%20rebuffed%20internal%20allegations%20of%20fraud%2C%0Awhich%20still%20stand.%20We%20note%20policy%20implications%20and%20conclusions%20for%20chip%20design.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.09633v9%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520False%2520Dawn%253A%2520Reevaluating%2520Google%2527s%2520Reinforcement%2520Learning%2520for%2520Chip%250A%2520%2520Macro%2520Placement%26entry.906535625%3DIgor%2520L.%2520Markov%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520for%2520physical%2520design%2520of%2520silicon%2520chips%2520in%2520a%2520Google%250A2021%2520Nature%2520paper%2520stirred%2520controversy%2520due%2520to%2520poorly%2520documented%2520claims%2520that%250Araised%2520eyebrows%2520and%2520drew%2520critical%2520media%2520coverage.%2520The%2520paper%2520withheld%2520critical%250Amethodology%2520steps%2520and%2520most%2520inputs%2520needed%2520to%2520reproduce%2520results.%2520Our%250Ameta-analysis%2520shows%2520how%2520two%2520separate%2520evaluations%2520filled%2520in%2520the%2520gaps%2520and%250Ademonstrated%2520that%2520Google%2520RL%2520lags%2520behind%2520%2528i%2529%2520human%2520designers%252C%2520%2528ii%2529%2520a%2520well-known%250Aalgorithm%2520%2528Simulated%2520Annealing%2529%252C%2520and%2520%2528iii%2529%2520generally-available%2520commercial%250Asoftware%252C%2520while%2520being%2520slower%253B%2520and%2520in%2520a%25202023%2520open%2520research%2520contest%252C%2520RL%2520methods%250Aweren%2527t%2520in%2520top%25205.%2520Crosschecked%2520data%2520indicate%2520that%2520the%2520integrity%2520of%2520the%2520Nature%250Apaper%2520is%2520substantially%2520undermined%2520owing%2520to%2520errors%2520in%2520conduct%252C%2520analysis%2520and%250Areporting.%2520Before%2520publishing%252C%2520Google%2520rebuffed%2520internal%2520allegations%2520of%2520fraud%252C%250Awhich%2520still%2520stand.%2520We%2520note%2520policy%2520implications%2520and%2520conclusions%2520for%2520chip%2520design.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.09633v9%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20False%20Dawn%3A%20Reevaluating%20Google%27s%20Reinforcement%20Learning%20for%20Chip%0A%20%20Macro%20Placement&entry.906535625=Igor%20L.%20Markov&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20for%20physical%20design%20of%20silicon%20chips%20in%20a%20Google%0A2021%20Nature%20paper%20stirred%20controversy%20due%20to%20poorly%20documented%20claims%20that%0Araised%20eyebrows%20and%20drew%20critical%20media%20coverage.%20The%20paper%20withheld%20critical%0Amethodology%20steps%20and%20most%20inputs%20needed%20to%20reproduce%20results.%20Our%0Ameta-analysis%20shows%20how%20two%20separate%20evaluations%20filled%20in%20the%20gaps%20and%0Ademonstrated%20that%20Google%20RL%20lags%20behind%20%28i%29%20human%20designers%2C%20%28ii%29%20a%20well-known%0Aalgorithm%20%28Simulated%20Annealing%29%2C%20and%20%28iii%29%20generally-available%20commercial%0Asoftware%2C%20while%20being%20slower%3B%20and%20in%20a%202023%20open%20research%20contest%2C%20RL%20methods%0Aweren%27t%20in%20top%205.%20Crosschecked%20data%20indicate%20that%20the%20integrity%20of%20the%20Nature%0Apaper%20is%20substantially%20undermined%20owing%20to%20errors%20in%20conduct%2C%20analysis%20and%0Areporting.%20Before%20publishing%2C%20Google%20rebuffed%20internal%20allegations%20of%20fraud%2C%0Awhich%20still%20stand.%20We%20note%20policy%20implications%20and%20conclusions%20for%20chip%20design.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.09633v9&entry.124074799=Read"},
{"title": "EventChat: Implementation and user-centric evaluation of a large\n  language model-driven conversational recommender system for exploring leisure\n  events in an SME context", "author": "Hannes Kunstmann and Joseph Ollier and Joel Persson and Florian von Wangenheim", "abstract": "  Large language models (LLMs) present an enormous evolution in the strategic\npotential of conversational recommender systems (CRS). Yet to date, research\nhas predominantly focused upon technical frameworks to implement LLM-driven\nCRS, rather than end-user evaluations or strategic implications for firms,\nparticularly from the perspective of a small to medium enterprises (SME) that\nmakeup the bedrock of the global economy. In the current paper, we detail the\ndesign of an LLM-driven CRS in an SME setting, and its subsequent performance\nin the field using both objective system metrics and subjective user\nevaluations. While doing so, we additionally outline a short-form revised\nResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly\nevolving field. Our results reveal good system performance from a user\nexperience perspective (85.5% recommendation accuracy) but underscore latency,\ncost, and quality issues challenging business viability. Notably, with a median\ncost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and\nresponse time emerge as crucial areas for achieving a more user-friendly and\neconomically viable LLM-driven CRS for SME settings. One major driver of these\ncosts is the use of an advanced LLM as a ranker within the retrieval-augmented\ngeneration (RAG) technique. Our results additionally indicate that relying\nsolely on approaches such as Prompt-based learning with ChatGPT as the\nunderlying LLM makes it challenging to achieve satisfying quality in a\nproduction environment. Strategic considerations for SMEs deploying an\nLLM-driven CRS are outlined, particularly considering trade-offs in the current\ntechnical landscape.\n", "link": "http://arxiv.org/abs/2407.04472v1", "date": "2024-07-05", "relevancy": 1.2804, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4834}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4247}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventChat%3A%20Implementation%20and%20user-centric%20evaluation%20of%20a%20large%0A%20%20language%20model-driven%20conversational%20recommender%20system%20for%20exploring%20leisure%0A%20%20events%20in%20an%20SME%20context&body=Title%3A%20EventChat%3A%20Implementation%20and%20user-centric%20evaluation%20of%20a%20large%0A%20%20language%20model-driven%20conversational%20recommender%20system%20for%20exploring%20leisure%0A%20%20events%20in%20an%20SME%20context%0AAuthor%3A%20Hannes%20Kunstmann%20and%20Joseph%20Ollier%20and%20Joel%20Persson%20and%20Florian%20von%20Wangenheim%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20present%20an%20enormous%20evolution%20in%20the%20strategic%0Apotential%20of%20conversational%20recommender%20systems%20%28CRS%29.%20Yet%20to%20date%2C%20research%0Ahas%20predominantly%20focused%20upon%20technical%20frameworks%20to%20implement%20LLM-driven%0ACRS%2C%20rather%20than%20end-user%20evaluations%20or%20strategic%20implications%20for%20firms%2C%0Aparticularly%20from%20the%20perspective%20of%20a%20small%20to%20medium%20enterprises%20%28SME%29%20that%0Amakeup%20the%20bedrock%20of%20the%20global%20economy.%20In%20the%20current%20paper%2C%20we%20detail%20the%0Adesign%20of%20an%20LLM-driven%20CRS%20in%20an%20SME%20setting%2C%20and%20its%20subsequent%20performance%0Ain%20the%20field%20using%20both%20objective%20system%20metrics%20and%20subjective%20user%0Aevaluations.%20While%20doing%20so%2C%20we%20additionally%20outline%20a%20short-form%20revised%0AResQue%20model%20for%20evaluating%20LLM-driven%20CRS%2C%20enabling%20replicability%20in%20a%20rapidly%0Aevolving%20field.%20Our%20results%20reveal%20good%20system%20performance%20from%20a%20user%0Aexperience%20perspective%20%2885.5%25%20recommendation%20accuracy%29%20but%20underscore%20latency%2C%0Acost%2C%20and%20quality%20issues%20challenging%20business%20viability.%20Notably%2C%20with%20a%20median%0Acost%20of%20%240.04%20per%20interaction%20and%20a%20latency%20of%205.7s%2C%20cost-effectiveness%20and%0Aresponse%20time%20emerge%20as%20crucial%20areas%20for%20achieving%20a%20more%20user-friendly%20and%0Aeconomically%20viable%20LLM-driven%20CRS%20for%20SME%20settings.%20One%20major%20driver%20of%20these%0Acosts%20is%20the%20use%20of%20an%20advanced%20LLM%20as%20a%20ranker%20within%20the%20retrieval-augmented%0Ageneration%20%28RAG%29%20technique.%20Our%20results%20additionally%20indicate%20that%20relying%0Asolely%20on%20approaches%20such%20as%20Prompt-based%20learning%20with%20ChatGPT%20as%20the%0Aunderlying%20LLM%20makes%20it%20challenging%20to%20achieve%20satisfying%20quality%20in%20a%0Aproduction%20environment.%20Strategic%20considerations%20for%20SMEs%20deploying%20an%0ALLM-driven%20CRS%20are%20outlined%2C%20particularly%20considering%20trade-offs%20in%20the%20current%0Atechnical%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventChat%253A%2520Implementation%2520and%2520user-centric%2520evaluation%2520of%2520a%2520large%250A%2520%2520language%2520model-driven%2520conversational%2520recommender%2520system%2520for%2520exploring%2520leisure%250A%2520%2520events%2520in%2520an%2520SME%2520context%26entry.906535625%3DHannes%2520Kunstmann%2520and%2520Joseph%2520Ollier%2520and%2520Joel%2520Persson%2520and%2520Florian%2520von%2520Wangenheim%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520present%2520an%2520enormous%2520evolution%2520in%2520the%2520strategic%250Apotential%2520of%2520conversational%2520recommender%2520systems%2520%2528CRS%2529.%2520Yet%2520to%2520date%252C%2520research%250Ahas%2520predominantly%2520focused%2520upon%2520technical%2520frameworks%2520to%2520implement%2520LLM-driven%250ACRS%252C%2520rather%2520than%2520end-user%2520evaluations%2520or%2520strategic%2520implications%2520for%2520firms%252C%250Aparticularly%2520from%2520the%2520perspective%2520of%2520a%2520small%2520to%2520medium%2520enterprises%2520%2528SME%2529%2520that%250Amakeup%2520the%2520bedrock%2520of%2520the%2520global%2520economy.%2520In%2520the%2520current%2520paper%252C%2520we%2520detail%2520the%250Adesign%2520of%2520an%2520LLM-driven%2520CRS%2520in%2520an%2520SME%2520setting%252C%2520and%2520its%2520subsequent%2520performance%250Ain%2520the%2520field%2520using%2520both%2520objective%2520system%2520metrics%2520and%2520subjective%2520user%250Aevaluations.%2520While%2520doing%2520so%252C%2520we%2520additionally%2520outline%2520a%2520short-form%2520revised%250AResQue%2520model%2520for%2520evaluating%2520LLM-driven%2520CRS%252C%2520enabling%2520replicability%2520in%2520a%2520rapidly%250Aevolving%2520field.%2520Our%2520results%2520reveal%2520good%2520system%2520performance%2520from%2520a%2520user%250Aexperience%2520perspective%2520%252885.5%2525%2520recommendation%2520accuracy%2529%2520but%2520underscore%2520latency%252C%250Acost%252C%2520and%2520quality%2520issues%2520challenging%2520business%2520viability.%2520Notably%252C%2520with%2520a%2520median%250Acost%2520of%2520%25240.04%2520per%2520interaction%2520and%2520a%2520latency%2520of%25205.7s%252C%2520cost-effectiveness%2520and%250Aresponse%2520time%2520emerge%2520as%2520crucial%2520areas%2520for%2520achieving%2520a%2520more%2520user-friendly%2520and%250Aeconomically%2520viable%2520LLM-driven%2520CRS%2520for%2520SME%2520settings.%2520One%2520major%2520driver%2520of%2520these%250Acosts%2520is%2520the%2520use%2520of%2520an%2520advanced%2520LLM%2520as%2520a%2520ranker%2520within%2520the%2520retrieval-augmented%250Ageneration%2520%2528RAG%2529%2520technique.%2520Our%2520results%2520additionally%2520indicate%2520that%2520relying%250Asolely%2520on%2520approaches%2520such%2520as%2520Prompt-based%2520learning%2520with%2520ChatGPT%2520as%2520the%250Aunderlying%2520LLM%2520makes%2520it%2520challenging%2520to%2520achieve%2520satisfying%2520quality%2520in%2520a%250Aproduction%2520environment.%2520Strategic%2520considerations%2520for%2520SMEs%2520deploying%2520an%250ALLM-driven%2520CRS%2520are%2520outlined%252C%2520particularly%2520considering%2520trade-offs%2520in%2520the%2520current%250Atechnical%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventChat%3A%20Implementation%20and%20user-centric%20evaluation%20of%20a%20large%0A%20%20language%20model-driven%20conversational%20recommender%20system%20for%20exploring%20leisure%0A%20%20events%20in%20an%20SME%20context&entry.906535625=Hannes%20Kunstmann%20and%20Joseph%20Ollier%20and%20Joel%20Persson%20and%20Florian%20von%20Wangenheim&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20present%20an%20enormous%20evolution%20in%20the%20strategic%0Apotential%20of%20conversational%20recommender%20systems%20%28CRS%29.%20Yet%20to%20date%2C%20research%0Ahas%20predominantly%20focused%20upon%20technical%20frameworks%20to%20implement%20LLM-driven%0ACRS%2C%20rather%20than%20end-user%20evaluations%20or%20strategic%20implications%20for%20firms%2C%0Aparticularly%20from%20the%20perspective%20of%20a%20small%20to%20medium%20enterprises%20%28SME%29%20that%0Amakeup%20the%20bedrock%20of%20the%20global%20economy.%20In%20the%20current%20paper%2C%20we%20detail%20the%0Adesign%20of%20an%20LLM-driven%20CRS%20in%20an%20SME%20setting%2C%20and%20its%20subsequent%20performance%0Ain%20the%20field%20using%20both%20objective%20system%20metrics%20and%20subjective%20user%0Aevaluations.%20While%20doing%20so%2C%20we%20additionally%20outline%20a%20short-form%20revised%0AResQue%20model%20for%20evaluating%20LLM-driven%20CRS%2C%20enabling%20replicability%20in%20a%20rapidly%0Aevolving%20field.%20Our%20results%20reveal%20good%20system%20performance%20from%20a%20user%0Aexperience%20perspective%20%2885.5%25%20recommendation%20accuracy%29%20but%20underscore%20latency%2C%0Acost%2C%20and%20quality%20issues%20challenging%20business%20viability.%20Notably%2C%20with%20a%20median%0Acost%20of%20%240.04%20per%20interaction%20and%20a%20latency%20of%205.7s%2C%20cost-effectiveness%20and%0Aresponse%20time%20emerge%20as%20crucial%20areas%20for%20achieving%20a%20more%20user-friendly%20and%0Aeconomically%20viable%20LLM-driven%20CRS%20for%20SME%20settings.%20One%20major%20driver%20of%20these%0Acosts%20is%20the%20use%20of%20an%20advanced%20LLM%20as%20a%20ranker%20within%20the%20retrieval-augmented%0Ageneration%20%28RAG%29%20technique.%20Our%20results%20additionally%20indicate%20that%20relying%0Asolely%20on%20approaches%20such%20as%20Prompt-based%20learning%20with%20ChatGPT%20as%20the%0Aunderlying%20LLM%20makes%20it%20challenging%20to%20achieve%20satisfying%20quality%20in%20a%0Aproduction%20environment.%20Strategic%20considerations%20for%20SMEs%20deploying%20an%0ALLM-driven%20CRS%20are%20outlined%2C%20particularly%20considering%20trade-offs%20in%20the%20current%0Atechnical%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04472v1&entry.124074799=Read"},
{"title": "Enhancing learning in artificial neural networks through cellular\n  heterogeneity and neuromodulatory signaling", "author": "Alejandro Rodriguez-Garcia and Jie Mei and Srikanth Ramaswamy", "abstract": "  Recent progress in artificial intelligence (AI) has been driven by insights\nfrom neuroscience, particularly with the development of artificial neural\nnetworks (ANNs). This has significantly enhanced the replication of complex\ncognitive tasks such as vision and natural language processing. Despite these\nadvances, ANNs struggle with continual learning, adaptable knowledge transfer,\nrobustness, and resource efficiency - capabilities that biological systems\nhandle seamlessly. Specifically, ANNs often overlook the functional and\nmorphological diversity of the brain, hindering their computational\ncapabilities. Furthermore, incorporating cell-type specific neuromodulatory\neffects into ANNs with neuronal heterogeneity could enable learning at two\nspatial scales: spiking behavior at the neuronal level, and synaptic plasticity\nat the circuit level, thereby potentially enhancing their learning abilities.\nIn this article, we summarize recent bio-inspired models, learning rules and\narchitectures and propose a biologically-informed framework for enhancing ANNs.\nOur proposed dual-framework approach highlights the potential of spiking neural\nnetworks (SNNs) for emulating diverse spiking behaviors and dendritic\ncompartments to simulate morphological and functional diversity of neuronal\ncomputations. Finally, we outline how the proposed approach integrates\nbrain-inspired compartmental models and task-driven SNNs, balances\nbioinspiration and complexity, and provides scalable solutions for pressing AI\nchallenges, such as continual learning, adaptability, robustness, and\nresource-efficiency.\n", "link": "http://arxiv.org/abs/2407.04525v1", "date": "2024-07-05", "relevancy": 1.5394, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4942}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20learning%20in%20artificial%20neural%20networks%20through%20cellular%0A%20%20heterogeneity%20and%20neuromodulatory%20signaling&body=Title%3A%20Enhancing%20learning%20in%20artificial%20neural%20networks%20through%20cellular%0A%20%20heterogeneity%20and%20neuromodulatory%20signaling%0AAuthor%3A%20Alejandro%20Rodriguez-Garcia%20and%20Jie%20Mei%20and%20Srikanth%20Ramaswamy%0AAbstract%3A%20%20%20Recent%20progress%20in%20artificial%20intelligence%20%28AI%29%20has%20been%20driven%20by%20insights%0Afrom%20neuroscience%2C%20particularly%20with%20the%20development%20of%20artificial%20neural%0Anetworks%20%28ANNs%29.%20This%20has%20significantly%20enhanced%20the%20replication%20of%20complex%0Acognitive%20tasks%20such%20as%20vision%20and%20natural%20language%20processing.%20Despite%20these%0Aadvances%2C%20ANNs%20struggle%20with%20continual%20learning%2C%20adaptable%20knowledge%20transfer%2C%0Arobustness%2C%20and%20resource%20efficiency%20-%20capabilities%20that%20biological%20systems%0Ahandle%20seamlessly.%20Specifically%2C%20ANNs%20often%20overlook%20the%20functional%20and%0Amorphological%20diversity%20of%20the%20brain%2C%20hindering%20their%20computational%0Acapabilities.%20Furthermore%2C%20incorporating%20cell-type%20specific%20neuromodulatory%0Aeffects%20into%20ANNs%20with%20neuronal%20heterogeneity%20could%20enable%20learning%20at%20two%0Aspatial%20scales%3A%20spiking%20behavior%20at%20the%20neuronal%20level%2C%20and%20synaptic%20plasticity%0Aat%20the%20circuit%20level%2C%20thereby%20potentially%20enhancing%20their%20learning%20abilities.%0AIn%20this%20article%2C%20we%20summarize%20recent%20bio-inspired%20models%2C%20learning%20rules%20and%0Aarchitectures%20and%20propose%20a%20biologically-informed%20framework%20for%20enhancing%20ANNs.%0AOur%20proposed%20dual-framework%20approach%20highlights%20the%20potential%20of%20spiking%20neural%0Anetworks%20%28SNNs%29%20for%20emulating%20diverse%20spiking%20behaviors%20and%20dendritic%0Acompartments%20to%20simulate%20morphological%20and%20functional%20diversity%20of%20neuronal%0Acomputations.%20Finally%2C%20we%20outline%20how%20the%20proposed%20approach%20integrates%0Abrain-inspired%20compartmental%20models%20and%20task-driven%20SNNs%2C%20balances%0Abioinspiration%20and%20complexity%2C%20and%20provides%20scalable%20solutions%20for%20pressing%20AI%0Achallenges%2C%20such%20as%20continual%20learning%2C%20adaptability%2C%20robustness%2C%20and%0Aresource-efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520learning%2520in%2520artificial%2520neural%2520networks%2520through%2520cellular%250A%2520%2520heterogeneity%2520and%2520neuromodulatory%2520signaling%26entry.906535625%3DAlejandro%2520Rodriguez-Garcia%2520and%2520Jie%2520Mei%2520and%2520Srikanth%2520Ramaswamy%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520artificial%2520intelligence%2520%2528AI%2529%2520has%2520been%2520driven%2520by%2520insights%250Afrom%2520neuroscience%252C%2520particularly%2520with%2520the%2520development%2520of%2520artificial%2520neural%250Anetworks%2520%2528ANNs%2529.%2520This%2520has%2520significantly%2520enhanced%2520the%2520replication%2520of%2520complex%250Acognitive%2520tasks%2520such%2520as%2520vision%2520and%2520natural%2520language%2520processing.%2520Despite%2520these%250Aadvances%252C%2520ANNs%2520struggle%2520with%2520continual%2520learning%252C%2520adaptable%2520knowledge%2520transfer%252C%250Arobustness%252C%2520and%2520resource%2520efficiency%2520-%2520capabilities%2520that%2520biological%2520systems%250Ahandle%2520seamlessly.%2520Specifically%252C%2520ANNs%2520often%2520overlook%2520the%2520functional%2520and%250Amorphological%2520diversity%2520of%2520the%2520brain%252C%2520hindering%2520their%2520computational%250Acapabilities.%2520Furthermore%252C%2520incorporating%2520cell-type%2520specific%2520neuromodulatory%250Aeffects%2520into%2520ANNs%2520with%2520neuronal%2520heterogeneity%2520could%2520enable%2520learning%2520at%2520two%250Aspatial%2520scales%253A%2520spiking%2520behavior%2520at%2520the%2520neuronal%2520level%252C%2520and%2520synaptic%2520plasticity%250Aat%2520the%2520circuit%2520level%252C%2520thereby%2520potentially%2520enhancing%2520their%2520learning%2520abilities.%250AIn%2520this%2520article%252C%2520we%2520summarize%2520recent%2520bio-inspired%2520models%252C%2520learning%2520rules%2520and%250Aarchitectures%2520and%2520propose%2520a%2520biologically-informed%2520framework%2520for%2520enhancing%2520ANNs.%250AOur%2520proposed%2520dual-framework%2520approach%2520highlights%2520the%2520potential%2520of%2520spiking%2520neural%250Anetworks%2520%2528SNNs%2529%2520for%2520emulating%2520diverse%2520spiking%2520behaviors%2520and%2520dendritic%250Acompartments%2520to%2520simulate%2520morphological%2520and%2520functional%2520diversity%2520of%2520neuronal%250Acomputations.%2520Finally%252C%2520we%2520outline%2520how%2520the%2520proposed%2520approach%2520integrates%250Abrain-inspired%2520compartmental%2520models%2520and%2520task-driven%2520SNNs%252C%2520balances%250Abioinspiration%2520and%2520complexity%252C%2520and%2520provides%2520scalable%2520solutions%2520for%2520pressing%2520AI%250Achallenges%252C%2520such%2520as%2520continual%2520learning%252C%2520adaptability%252C%2520robustness%252C%2520and%250Aresource-efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20learning%20in%20artificial%20neural%20networks%20through%20cellular%0A%20%20heterogeneity%20and%20neuromodulatory%20signaling&entry.906535625=Alejandro%20Rodriguez-Garcia%20and%20Jie%20Mei%20and%20Srikanth%20Ramaswamy&entry.1292438233=%20%20Recent%20progress%20in%20artificial%20intelligence%20%28AI%29%20has%20been%20driven%20by%20insights%0Afrom%20neuroscience%2C%20particularly%20with%20the%20development%20of%20artificial%20neural%0Anetworks%20%28ANNs%29.%20This%20has%20significantly%20enhanced%20the%20replication%20of%20complex%0Acognitive%20tasks%20such%20as%20vision%20and%20natural%20language%20processing.%20Despite%20these%0Aadvances%2C%20ANNs%20struggle%20with%20continual%20learning%2C%20adaptable%20knowledge%20transfer%2C%0Arobustness%2C%20and%20resource%20efficiency%20-%20capabilities%20that%20biological%20systems%0Ahandle%20seamlessly.%20Specifically%2C%20ANNs%20often%20overlook%20the%20functional%20and%0Amorphological%20diversity%20of%20the%20brain%2C%20hindering%20their%20computational%0Acapabilities.%20Furthermore%2C%20incorporating%20cell-type%20specific%20neuromodulatory%0Aeffects%20into%20ANNs%20with%20neuronal%20heterogeneity%20could%20enable%20learning%20at%20two%0Aspatial%20scales%3A%20spiking%20behavior%20at%20the%20neuronal%20level%2C%20and%20synaptic%20plasticity%0Aat%20the%20circuit%20level%2C%20thereby%20potentially%20enhancing%20their%20learning%20abilities.%0AIn%20this%20article%2C%20we%20summarize%20recent%20bio-inspired%20models%2C%20learning%20rules%20and%0Aarchitectures%20and%20propose%20a%20biologically-informed%20framework%20for%20enhancing%20ANNs.%0AOur%20proposed%20dual-framework%20approach%20highlights%20the%20potential%20of%20spiking%20neural%0Anetworks%20%28SNNs%29%20for%20emulating%20diverse%20spiking%20behaviors%20and%20dendritic%0Acompartments%20to%20simulate%20morphological%20and%20functional%20diversity%20of%20neuronal%0Acomputations.%20Finally%2C%20we%20outline%20how%20the%20proposed%20approach%20integrates%0Abrain-inspired%20compartmental%20models%20and%20task-driven%20SNNs%2C%20balances%0Abioinspiration%20and%20complexity%2C%20and%20provides%20scalable%20solutions%20for%20pressing%20AI%0Achallenges%2C%20such%20as%20continual%20learning%2C%20adaptability%2C%20robustness%2C%20and%0Aresource-efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04525v1&entry.124074799=Read"},
{"title": "Dude: Dual Distribution-Aware Context Prompt Learning For Large\n  Vision-Language Model", "author": "Duy M. H. Nguyen and An T. Le and Trung Q. Nguyen and Nghiem T. Diep and Tai Nguyen and Duy Duong-Tran and Jan Peters and Li Shen and Mathias Niepert and Daniel Sonntag", "abstract": "  Prompt learning methods are gaining increasing attention due to their ability\nto customize large vision-language models to new domains using pre-trained\ncontextual knowledge and minimal training data. However, existing works\ntypically rely on optimizing unified prompt inputs, often struggling with\nfine-grained classification tasks due to insufficient discriminative\nattributes. To tackle this, we consider a new framework based on a dual context\nof both domain-shared and class-specific contexts, where the latter is\ngenerated by Large Language Models (LLMs) such as GPTs. Such dual prompt\nmethods enhance the model's feature representation by joining implicit and\nexplicit factors encoded in LLM knowledge. Moreover, we formulate the\nUnbalanced Optimal Transport (UOT) theory to quantify the relationships between\nconstructed prompts and visual tokens. Through partial matching, UOT can\nproperly align discrete sets of visual tokens and prompt embeddings under\ndifferent mass distributions, which is particularly valuable for handling\nirrelevant or noisy elements, ensuring that the preservation of mass does not\nrestrict transport solutions. Furthermore, UOT's characteristics integrate\nseamlessly with image augmentation, expanding the training sample pool while\nmaintaining a reasonable distance between perturbed images and prompt inputs.\nExtensive experiments across few-shot classification and adapter settings\nsubstantiate the superiority of our model over current state-of-the-art\nbaselines.\n", "link": "http://arxiv.org/abs/2407.04489v1", "date": "2024-07-05", "relevancy": 1.5989, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5396}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5295}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dude%3A%20Dual%20Distribution-Aware%20Context%20Prompt%20Learning%20For%20Large%0A%20%20Vision-Language%20Model&body=Title%3A%20Dude%3A%20Dual%20Distribution-Aware%20Context%20Prompt%20Learning%20For%20Large%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Duy%20M.%20H.%20Nguyen%20and%20An%20T.%20Le%20and%20Trung%20Q.%20Nguyen%20and%20Nghiem%20T.%20Diep%20and%20Tai%20Nguyen%20and%20Duy%20Duong-Tran%20and%20Jan%20Peters%20and%20Li%20Shen%20and%20Mathias%20Niepert%20and%20Daniel%20Sonntag%0AAbstract%3A%20%20%20Prompt%20learning%20methods%20are%20gaining%20increasing%20attention%20due%20to%20their%20ability%0Ato%20customize%20large%20vision-language%20models%20to%20new%20domains%20using%20pre-trained%0Acontextual%20knowledge%20and%20minimal%20training%20data.%20However%2C%20existing%20works%0Atypically%20rely%20on%20optimizing%20unified%20prompt%20inputs%2C%20often%20struggling%20with%0Afine-grained%20classification%20tasks%20due%20to%20insufficient%20discriminative%0Aattributes.%20To%20tackle%20this%2C%20we%20consider%20a%20new%20framework%20based%20on%20a%20dual%20context%0Aof%20both%20domain-shared%20and%20class-specific%20contexts%2C%20where%20the%20latter%20is%0Agenerated%20by%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20GPTs.%20Such%20dual%20prompt%0Amethods%20enhance%20the%20model%27s%20feature%20representation%20by%20joining%20implicit%20and%0Aexplicit%20factors%20encoded%20in%20LLM%20knowledge.%20Moreover%2C%20we%20formulate%20the%0AUnbalanced%20Optimal%20Transport%20%28UOT%29%20theory%20to%20quantify%20the%20relationships%20between%0Aconstructed%20prompts%20and%20visual%20tokens.%20Through%20partial%20matching%2C%20UOT%20can%0Aproperly%20align%20discrete%20sets%20of%20visual%20tokens%20and%20prompt%20embeddings%20under%0Adifferent%20mass%20distributions%2C%20which%20is%20particularly%20valuable%20for%20handling%0Airrelevant%20or%20noisy%20elements%2C%20ensuring%20that%20the%20preservation%20of%20mass%20does%20not%0Arestrict%20transport%20solutions.%20Furthermore%2C%20UOT%27s%20characteristics%20integrate%0Aseamlessly%20with%20image%20augmentation%2C%20expanding%20the%20training%20sample%20pool%20while%0Amaintaining%20a%20reasonable%20distance%20between%20perturbed%20images%20and%20prompt%20inputs.%0AExtensive%20experiments%20across%20few-shot%20classification%20and%20adapter%20settings%0Asubstantiate%20the%20superiority%20of%20our%20model%20over%20current%20state-of-the-art%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDude%253A%2520Dual%2520Distribution-Aware%2520Context%2520Prompt%2520Learning%2520For%2520Large%250A%2520%2520Vision-Language%2520Model%26entry.906535625%3DDuy%2520M.%2520H.%2520Nguyen%2520and%2520An%2520T.%2520Le%2520and%2520Trung%2520Q.%2520Nguyen%2520and%2520Nghiem%2520T.%2520Diep%2520and%2520Tai%2520Nguyen%2520and%2520Duy%2520Duong-Tran%2520and%2520Jan%2520Peters%2520and%2520Li%2520Shen%2520and%2520Mathias%2520Niepert%2520and%2520Daniel%2520Sonntag%26entry.1292438233%3D%2520%2520Prompt%2520learning%2520methods%2520are%2520gaining%2520increasing%2520attention%2520due%2520to%2520their%2520ability%250Ato%2520customize%2520large%2520vision-language%2520models%2520to%2520new%2520domains%2520using%2520pre-trained%250Acontextual%2520knowledge%2520and%2520minimal%2520training%2520data.%2520However%252C%2520existing%2520works%250Atypically%2520rely%2520on%2520optimizing%2520unified%2520prompt%2520inputs%252C%2520often%2520struggling%2520with%250Afine-grained%2520classification%2520tasks%2520due%2520to%2520insufficient%2520discriminative%250Aattributes.%2520To%2520tackle%2520this%252C%2520we%2520consider%2520a%2520new%2520framework%2520based%2520on%2520a%2520dual%2520context%250Aof%2520both%2520domain-shared%2520and%2520class-specific%2520contexts%252C%2520where%2520the%2520latter%2520is%250Agenerated%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520such%2520as%2520GPTs.%2520Such%2520dual%2520prompt%250Amethods%2520enhance%2520the%2520model%2527s%2520feature%2520representation%2520by%2520joining%2520implicit%2520and%250Aexplicit%2520factors%2520encoded%2520in%2520LLM%2520knowledge.%2520Moreover%252C%2520we%2520formulate%2520the%250AUnbalanced%2520Optimal%2520Transport%2520%2528UOT%2529%2520theory%2520to%2520quantify%2520the%2520relationships%2520between%250Aconstructed%2520prompts%2520and%2520visual%2520tokens.%2520Through%2520partial%2520matching%252C%2520UOT%2520can%250Aproperly%2520align%2520discrete%2520sets%2520of%2520visual%2520tokens%2520and%2520prompt%2520embeddings%2520under%250Adifferent%2520mass%2520distributions%252C%2520which%2520is%2520particularly%2520valuable%2520for%2520handling%250Airrelevant%2520or%2520noisy%2520elements%252C%2520ensuring%2520that%2520the%2520preservation%2520of%2520mass%2520does%2520not%250Arestrict%2520transport%2520solutions.%2520Furthermore%252C%2520UOT%2527s%2520characteristics%2520integrate%250Aseamlessly%2520with%2520image%2520augmentation%252C%2520expanding%2520the%2520training%2520sample%2520pool%2520while%250Amaintaining%2520a%2520reasonable%2520distance%2520between%2520perturbed%2520images%2520and%2520prompt%2520inputs.%250AExtensive%2520experiments%2520across%2520few-shot%2520classification%2520and%2520adapter%2520settings%250Asubstantiate%2520the%2520superiority%2520of%2520our%2520model%2520over%2520current%2520state-of-the-art%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dude%3A%20Dual%20Distribution-Aware%20Context%20Prompt%20Learning%20For%20Large%0A%20%20Vision-Language%20Model&entry.906535625=Duy%20M.%20H.%20Nguyen%20and%20An%20T.%20Le%20and%20Trung%20Q.%20Nguyen%20and%20Nghiem%20T.%20Diep%20and%20Tai%20Nguyen%20and%20Duy%20Duong-Tran%20and%20Jan%20Peters%20and%20Li%20Shen%20and%20Mathias%20Niepert%20and%20Daniel%20Sonntag&entry.1292438233=%20%20Prompt%20learning%20methods%20are%20gaining%20increasing%20attention%20due%20to%20their%20ability%0Ato%20customize%20large%20vision-language%20models%20to%20new%20domains%20using%20pre-trained%0Acontextual%20knowledge%20and%20minimal%20training%20data.%20However%2C%20existing%20works%0Atypically%20rely%20on%20optimizing%20unified%20prompt%20inputs%2C%20often%20struggling%20with%0Afine-grained%20classification%20tasks%20due%20to%20insufficient%20discriminative%0Aattributes.%20To%20tackle%20this%2C%20we%20consider%20a%20new%20framework%20based%20on%20a%20dual%20context%0Aof%20both%20domain-shared%20and%20class-specific%20contexts%2C%20where%20the%20latter%20is%0Agenerated%20by%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20GPTs.%20Such%20dual%20prompt%0Amethods%20enhance%20the%20model%27s%20feature%20representation%20by%20joining%20implicit%20and%0Aexplicit%20factors%20encoded%20in%20LLM%20knowledge.%20Moreover%2C%20we%20formulate%20the%0AUnbalanced%20Optimal%20Transport%20%28UOT%29%20theory%20to%20quantify%20the%20relationships%20between%0Aconstructed%20prompts%20and%20visual%20tokens.%20Through%20partial%20matching%2C%20UOT%20can%0Aproperly%20align%20discrete%20sets%20of%20visual%20tokens%20and%20prompt%20embeddings%20under%0Adifferent%20mass%20distributions%2C%20which%20is%20particularly%20valuable%20for%20handling%0Airrelevant%20or%20noisy%20elements%2C%20ensuring%20that%20the%20preservation%20of%20mass%20does%20not%0Arestrict%20transport%20solutions.%20Furthermore%2C%20UOT%27s%20characteristics%20integrate%0Aseamlessly%20with%20image%20augmentation%2C%20expanding%20the%20training%20sample%20pool%20while%0Amaintaining%20a%20reasonable%20distance%20between%20perturbed%20images%20and%20prompt%20inputs.%0AExtensive%20experiments%20across%20few-shot%20classification%20and%20adapter%20settings%0Asubstantiate%20the%20superiority%20of%20our%20model%20over%20current%20state-of-the-art%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04489v1&entry.124074799=Read"},
{"title": "Lost in Translation: The Algorithmic Gap Between LMs and the Brain", "author": "Tommaso Tosato and Pascal Jr Tikeng Notsawo and Saskia Helbling and Irina Rish and Guillaume Dumas", "abstract": "  Language Models (LMs) have achieved impressive performance on various\nlinguistic tasks, but their relationship to human language processing in the\nbrain remains unclear. This paper examines the gaps and overlaps between LMs\nand the brain at different levels of analysis, emphasizing the importance of\nlooking beyond input-output behavior to examine and compare the internal\nprocesses of these systems. We discuss how insights from neuroscience, such as\nsparsity, modularity, internal states, and interactive learning, can inform the\ndevelopment of more biologically plausible language models. Furthermore, we\nexplore the role of scaling laws in bridging the gap between LMs and human\ncognition, highlighting the need for efficiency constraints analogous to those\nin biological systems. By developing LMs that more closely mimic brain\nfunction, we aim to advance both artificial intelligence and our understanding\nof human cognition.\n", "link": "http://arxiv.org/abs/2407.04680v1", "date": "2024-07-05", "relevancy": 1.4599, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4911}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4859}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20Translation%3A%20The%20Algorithmic%20Gap%20Between%20LMs%20and%20the%20Brain&body=Title%3A%20Lost%20in%20Translation%3A%20The%20Algorithmic%20Gap%20Between%20LMs%20and%20the%20Brain%0AAuthor%3A%20Tommaso%20Tosato%20and%20Pascal%20Jr%20Tikeng%20Notsawo%20and%20Saskia%20Helbling%20and%20Irina%20Rish%20and%20Guillaume%20Dumas%0AAbstract%3A%20%20%20Language%20Models%20%28LMs%29%20have%20achieved%20impressive%20performance%20on%20various%0Alinguistic%20tasks%2C%20but%20their%20relationship%20to%20human%20language%20processing%20in%20the%0Abrain%20remains%20unclear.%20This%20paper%20examines%20the%20gaps%20and%20overlaps%20between%20LMs%0Aand%20the%20brain%20at%20different%20levels%20of%20analysis%2C%20emphasizing%20the%20importance%20of%0Alooking%20beyond%20input-output%20behavior%20to%20examine%20and%20compare%20the%20internal%0Aprocesses%20of%20these%20systems.%20We%20discuss%20how%20insights%20from%20neuroscience%2C%20such%20as%0Asparsity%2C%20modularity%2C%20internal%20states%2C%20and%20interactive%20learning%2C%20can%20inform%20the%0Adevelopment%20of%20more%20biologically%20plausible%20language%20models.%20Furthermore%2C%20we%0Aexplore%20the%20role%20of%20scaling%20laws%20in%20bridging%20the%20gap%20between%20LMs%20and%20human%0Acognition%2C%20highlighting%20the%20need%20for%20efficiency%20constraints%20analogous%20to%20those%0Ain%20biological%20systems.%20By%20developing%20LMs%20that%20more%20closely%20mimic%20brain%0Afunction%2C%20we%20aim%20to%20advance%20both%20artificial%20intelligence%20and%20our%20understanding%0Aof%20human%20cognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520Translation%253A%2520The%2520Algorithmic%2520Gap%2520Between%2520LMs%2520and%2520the%2520Brain%26entry.906535625%3DTommaso%2520Tosato%2520and%2520Pascal%2520Jr%2520Tikeng%2520Notsawo%2520and%2520Saskia%2520Helbling%2520and%2520Irina%2520Rish%2520and%2520Guillaume%2520Dumas%26entry.1292438233%3D%2520%2520Language%2520Models%2520%2528LMs%2529%2520have%2520achieved%2520impressive%2520performance%2520on%2520various%250Alinguistic%2520tasks%252C%2520but%2520their%2520relationship%2520to%2520human%2520language%2520processing%2520in%2520the%250Abrain%2520remains%2520unclear.%2520This%2520paper%2520examines%2520the%2520gaps%2520and%2520overlaps%2520between%2520LMs%250Aand%2520the%2520brain%2520at%2520different%2520levels%2520of%2520analysis%252C%2520emphasizing%2520the%2520importance%2520of%250Alooking%2520beyond%2520input-output%2520behavior%2520to%2520examine%2520and%2520compare%2520the%2520internal%250Aprocesses%2520of%2520these%2520systems.%2520We%2520discuss%2520how%2520insights%2520from%2520neuroscience%252C%2520such%2520as%250Asparsity%252C%2520modularity%252C%2520internal%2520states%252C%2520and%2520interactive%2520learning%252C%2520can%2520inform%2520the%250Adevelopment%2520of%2520more%2520biologically%2520plausible%2520language%2520models.%2520Furthermore%252C%2520we%250Aexplore%2520the%2520role%2520of%2520scaling%2520laws%2520in%2520bridging%2520the%2520gap%2520between%2520LMs%2520and%2520human%250Acognition%252C%2520highlighting%2520the%2520need%2520for%2520efficiency%2520constraints%2520analogous%2520to%2520those%250Ain%2520biological%2520systems.%2520By%2520developing%2520LMs%2520that%2520more%2520closely%2520mimic%2520brain%250Afunction%252C%2520we%2520aim%2520to%2520advance%2520both%2520artificial%2520intelligence%2520and%2520our%2520understanding%250Aof%2520human%2520cognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20Translation%3A%20The%20Algorithmic%20Gap%20Between%20LMs%20and%20the%20Brain&entry.906535625=Tommaso%20Tosato%20and%20Pascal%20Jr%20Tikeng%20Notsawo%20and%20Saskia%20Helbling%20and%20Irina%20Rish%20and%20Guillaume%20Dumas&entry.1292438233=%20%20Language%20Models%20%28LMs%29%20have%20achieved%20impressive%20performance%20on%20various%0Alinguistic%20tasks%2C%20but%20their%20relationship%20to%20human%20language%20processing%20in%20the%0Abrain%20remains%20unclear.%20This%20paper%20examines%20the%20gaps%20and%20overlaps%20between%20LMs%0Aand%20the%20brain%20at%20different%20levels%20of%20analysis%2C%20emphasizing%20the%20importance%20of%0Alooking%20beyond%20input-output%20behavior%20to%20examine%20and%20compare%20the%20internal%0Aprocesses%20of%20these%20systems.%20We%20discuss%20how%20insights%20from%20neuroscience%2C%20such%20as%0Asparsity%2C%20modularity%2C%20internal%20states%2C%20and%20interactive%20learning%2C%20can%20inform%20the%0Adevelopment%20of%20more%20biologically%20plausible%20language%20models.%20Furthermore%2C%20we%0Aexplore%20the%20role%20of%20scaling%20laws%20in%20bridging%20the%20gap%20between%20LMs%20and%20human%0Acognition%2C%20highlighting%20the%20need%20for%20efficiency%20constraints%20analogous%20to%20those%0Ain%20biological%20systems.%20By%20developing%20LMs%20that%20more%20closely%20mimic%20brain%0Afunction%2C%20we%20aim%20to%20advance%20both%20artificial%20intelligence%20and%20our%20understanding%0Aof%20human%20cognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04680v1&entry.124074799=Read"},
{"title": "Understanding the Gains from Repeated Self-Distillation", "author": "Divyansh Pareek and Simon S. Du and Sewoong Oh", "abstract": "  Self-Distillation is a special type of knowledge distillation where the\nstudent model has the same architecture as the teacher model. Despite using the\nsame architecture and the same training data, self-distillation has been\nempirically observed to improve performance, especially when applied\nrepeatedly. For such a process, there is a fundamental question of interest:\nHow much gain is possible by applying multiple steps of self-distillation? To\ninvestigate this relative gain, we propose studying the simple but canonical\ntask of linear regression. Our analysis shows that the excess risk achieved by\nmulti-step self-distillation can significantly improve upon a single step of\nself-distillation, reducing the excess risk by a factor as large as $d$, where\n$d$ is the input dimension. Empirical results on regression tasks from the UCI\nrepository show a reduction in the learnt model's risk (MSE) by up to 47%.\n", "link": "http://arxiv.org/abs/2407.04600v1", "date": "2024-07-05", "relevancy": 1.4412, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4822}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4813}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20the%20Gains%20from%20Repeated%20Self-Distillation&body=Title%3A%20Understanding%20the%20Gains%20from%20Repeated%20Self-Distillation%0AAuthor%3A%20Divyansh%20Pareek%20and%20Simon%20S.%20Du%20and%20Sewoong%20Oh%0AAbstract%3A%20%20%20Self-Distillation%20is%20a%20special%20type%20of%20knowledge%20distillation%20where%20the%0Astudent%20model%20has%20the%20same%20architecture%20as%20the%20teacher%20model.%20Despite%20using%20the%0Asame%20architecture%20and%20the%20same%20training%20data%2C%20self-distillation%20has%20been%0Aempirically%20observed%20to%20improve%20performance%2C%20especially%20when%20applied%0Arepeatedly.%20For%20such%20a%20process%2C%20there%20is%20a%20fundamental%20question%20of%20interest%3A%0AHow%20much%20gain%20is%20possible%20by%20applying%20multiple%20steps%20of%20self-distillation%3F%20To%0Ainvestigate%20this%20relative%20gain%2C%20we%20propose%20studying%20the%20simple%20but%20canonical%0Atask%20of%20linear%20regression.%20Our%20analysis%20shows%20that%20the%20excess%20risk%20achieved%20by%0Amulti-step%20self-distillation%20can%20significantly%20improve%20upon%20a%20single%20step%20of%0Aself-distillation%2C%20reducing%20the%20excess%20risk%20by%20a%20factor%20as%20large%20as%20%24d%24%2C%20where%0A%24d%24%20is%20the%20input%20dimension.%20Empirical%20results%20on%20regression%20tasks%20from%20the%20UCI%0Arepository%20show%20a%20reduction%20in%20the%20learnt%20model%27s%20risk%20%28MSE%29%20by%20up%20to%2047%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520the%2520Gains%2520from%2520Repeated%2520Self-Distillation%26entry.906535625%3DDivyansh%2520Pareek%2520and%2520Simon%2520S.%2520Du%2520and%2520Sewoong%2520Oh%26entry.1292438233%3D%2520%2520Self-Distillation%2520is%2520a%2520special%2520type%2520of%2520knowledge%2520distillation%2520where%2520the%250Astudent%2520model%2520has%2520the%2520same%2520architecture%2520as%2520the%2520teacher%2520model.%2520Despite%2520using%2520the%250Asame%2520architecture%2520and%2520the%2520same%2520training%2520data%252C%2520self-distillation%2520has%2520been%250Aempirically%2520observed%2520to%2520improve%2520performance%252C%2520especially%2520when%2520applied%250Arepeatedly.%2520For%2520such%2520a%2520process%252C%2520there%2520is%2520a%2520fundamental%2520question%2520of%2520interest%253A%250AHow%2520much%2520gain%2520is%2520possible%2520by%2520applying%2520multiple%2520steps%2520of%2520self-distillation%253F%2520To%250Ainvestigate%2520this%2520relative%2520gain%252C%2520we%2520propose%2520studying%2520the%2520simple%2520but%2520canonical%250Atask%2520of%2520linear%2520regression.%2520Our%2520analysis%2520shows%2520that%2520the%2520excess%2520risk%2520achieved%2520by%250Amulti-step%2520self-distillation%2520can%2520significantly%2520improve%2520upon%2520a%2520single%2520step%2520of%250Aself-distillation%252C%2520reducing%2520the%2520excess%2520risk%2520by%2520a%2520factor%2520as%2520large%2520as%2520%2524d%2524%252C%2520where%250A%2524d%2524%2520is%2520the%2520input%2520dimension.%2520Empirical%2520results%2520on%2520regression%2520tasks%2520from%2520the%2520UCI%250Arepository%2520show%2520a%2520reduction%2520in%2520the%2520learnt%2520model%2527s%2520risk%2520%2528MSE%2529%2520by%2520up%2520to%252047%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20the%20Gains%20from%20Repeated%20Self-Distillation&entry.906535625=Divyansh%20Pareek%20and%20Simon%20S.%20Du%20and%20Sewoong%20Oh&entry.1292438233=%20%20Self-Distillation%20is%20a%20special%20type%20of%20knowledge%20distillation%20where%20the%0Astudent%20model%20has%20the%20same%20architecture%20as%20the%20teacher%20model.%20Despite%20using%20the%0Asame%20architecture%20and%20the%20same%20training%20data%2C%20self-distillation%20has%20been%0Aempirically%20observed%20to%20improve%20performance%2C%20especially%20when%20applied%0Arepeatedly.%20For%20such%20a%20process%2C%20there%20is%20a%20fundamental%20question%20of%20interest%3A%0AHow%20much%20gain%20is%20possible%20by%20applying%20multiple%20steps%20of%20self-distillation%3F%20To%0Ainvestigate%20this%20relative%20gain%2C%20we%20propose%20studying%20the%20simple%20but%20canonical%0Atask%20of%20linear%20regression.%20Our%20analysis%20shows%20that%20the%20excess%20risk%20achieved%20by%0Amulti-step%20self-distillation%20can%20significantly%20improve%20upon%20a%20single%20step%20of%0Aself-distillation%2C%20reducing%20the%20excess%20risk%20by%20a%20factor%20as%20large%20as%20%24d%24%2C%20where%0A%24d%24%20is%20the%20input%20dimension.%20Empirical%20results%20on%20regression%20tasks%20from%20the%20UCI%0Arepository%20show%20a%20reduction%20in%20the%20learnt%20model%27s%20risk%20%28MSE%29%20by%20up%20to%2047%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04600v1&entry.124074799=Read"},
{"title": "Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics\n  Instability Detection", "author": "Mohammod N. I. Suvon and Prasun C. Tripathi and Wenrui Fan and Shuo Zhou and Xianyuan Liu and Samer Alabed and Venet Osmani and Andrew J. Swift and Chen Chen and Haiping Lu", "abstract": "  Recent advancements in non-invasive detection of cardiac hemodynamic\ninstability (CHDI) primarily focus on applying machine learning techniques to a\nsingle data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite\ntheir potential, these approaches often fall short especially when the size of\nlabeled patient data is limited, a common challenge in the medical domain.\nFurthermore, only a few studies have explored multimodal methods to study CHDI,\nwhich mostly rely on costly modalities such as cardiac MRI and echocardiogram.\nIn response to these limitations, we propose a novel multimodal variational\nautoencoder ($\\text{CardioVAE}_\\text{X,G}$) to integrate low-cost chest X-ray\n(CXR) and electrocardiogram (ECG) modalities with pre-training on a large\nunlabeled dataset. Specifically, $\\text{CardioVAE}_\\text{X,G}$ introduces a\nnovel tri-stream pre-training strategy to learn both shared and\nmodality-specific features, thus enabling fine-tuning with both unimodal and\nmultimodal datasets. We pre-train $\\text{CardioVAE}_\\text{X,G}$ on a large,\nunlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then\nfine-tune the pre-trained model on a labeled dataset of $795$ subjects from the\nASPIRE registry. Comprehensive evaluations against existing methods show that\n$\\text{CardioVAE}_\\text{X,G}$ offers promising performance (AUROC $=0.79$ and\nAccuracy $=0.77$), representing a significant step forward in non-invasive\nprediction of CHDI. Our model also excels in producing fine interpretations of\npredictions directly associated with clinical features, thereby supporting\nclinical decision-making.\n", "link": "http://arxiv.org/abs/2403.13658v3", "date": "2024-07-05", "relevancy": 1.5047, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5091}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5006}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Variational%20Autoencoder%20for%20Low-cost%20Cardiac%20Hemodynamics%0A%20%20Instability%20Detection&body=Title%3A%20Multimodal%20Variational%20Autoencoder%20for%20Low-cost%20Cardiac%20Hemodynamics%0A%20%20Instability%20Detection%0AAuthor%3A%20Mohammod%20N.%20I.%20Suvon%20and%20Prasun%20C.%20Tripathi%20and%20Wenrui%20Fan%20and%20Shuo%20Zhou%20and%20Xianyuan%20Liu%20and%20Samer%20Alabed%20and%20Venet%20Osmani%20and%20Andrew%20J.%20Swift%20and%20Chen%20Chen%20and%20Haiping%20Lu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20non-invasive%20detection%20of%20cardiac%20hemodynamic%0Ainstability%20%28CHDI%29%20primarily%20focus%20on%20applying%20machine%20learning%20techniques%20to%20a%0Asingle%20data%20modality%2C%20e.g.%20cardiac%20magnetic%20resonance%20imaging%20%28MRI%29.%20Despite%0Atheir%20potential%2C%20these%20approaches%20often%20fall%20short%20especially%20when%20the%20size%20of%0Alabeled%20patient%20data%20is%20limited%2C%20a%20common%20challenge%20in%20the%20medical%20domain.%0AFurthermore%2C%20only%20a%20few%20studies%20have%20explored%20multimodal%20methods%20to%20study%20CHDI%2C%0Awhich%20mostly%20rely%20on%20costly%20modalities%20such%20as%20cardiac%20MRI%20and%20echocardiogram.%0AIn%20response%20to%20these%20limitations%2C%20we%20propose%20a%20novel%20multimodal%20variational%0Aautoencoder%20%28%24%5Ctext%7BCardioVAE%7D_%5Ctext%7BX%2CG%7D%24%29%20to%20integrate%20low-cost%20chest%20X-ray%0A%28CXR%29%20and%20electrocardiogram%20%28ECG%29%20modalities%20with%20pre-training%20on%20a%20large%0Aunlabeled%20dataset.%20Specifically%2C%20%24%5Ctext%7BCardioVAE%7D_%5Ctext%7BX%2CG%7D%24%20introduces%20a%0Anovel%20tri-stream%20pre-training%20strategy%20to%20learn%20both%20shared%20and%0Amodality-specific%20features%2C%20thus%20enabling%20fine-tuning%20with%20both%20unimodal%20and%0Amultimodal%20datasets.%20We%20pre-train%20%24%5Ctext%7BCardioVAE%7D_%5Ctext%7BX%2CG%7D%24%20on%20a%20large%2C%0Aunlabeled%20dataset%20of%20%2450%2C982%24%20subjects%20from%20a%20subset%20of%20MIMIC%20database%20and%20then%0Afine-tune%20the%20pre-trained%20model%20on%20a%20labeled%20dataset%20of%20%24795%24%20subjects%20from%20the%0AASPIRE%20registry.%20Comprehensive%20evaluations%20against%20existing%20methods%20show%20that%0A%24%5Ctext%7BCardioVAE%7D_%5Ctext%7BX%2CG%7D%24%20offers%20promising%20performance%20%28AUROC%20%24%3D0.79%24%20and%0AAccuracy%20%24%3D0.77%24%29%2C%20representing%20a%20significant%20step%20forward%20in%20non-invasive%0Aprediction%20of%20CHDI.%20Our%20model%20also%20excels%20in%20producing%20fine%20interpretations%20of%0Apredictions%20directly%20associated%20with%20clinical%20features%2C%20thereby%20supporting%0Aclinical%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13658v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Variational%2520Autoencoder%2520for%2520Low-cost%2520Cardiac%2520Hemodynamics%250A%2520%2520Instability%2520Detection%26entry.906535625%3DMohammod%2520N.%2520I.%2520Suvon%2520and%2520Prasun%2520C.%2520Tripathi%2520and%2520Wenrui%2520Fan%2520and%2520Shuo%2520Zhou%2520and%2520Xianyuan%2520Liu%2520and%2520Samer%2520Alabed%2520and%2520Venet%2520Osmani%2520and%2520Andrew%2520J.%2520Swift%2520and%2520Chen%2520Chen%2520and%2520Haiping%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520non-invasive%2520detection%2520of%2520cardiac%2520hemodynamic%250Ainstability%2520%2528CHDI%2529%2520primarily%2520focus%2520on%2520applying%2520machine%2520learning%2520techniques%2520to%2520a%250Asingle%2520data%2520modality%252C%2520e.g.%2520cardiac%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529.%2520Despite%250Atheir%2520potential%252C%2520these%2520approaches%2520often%2520fall%2520short%2520especially%2520when%2520the%2520size%2520of%250Alabeled%2520patient%2520data%2520is%2520limited%252C%2520a%2520common%2520challenge%2520in%2520the%2520medical%2520domain.%250AFurthermore%252C%2520only%2520a%2520few%2520studies%2520have%2520explored%2520multimodal%2520methods%2520to%2520study%2520CHDI%252C%250Awhich%2520mostly%2520rely%2520on%2520costly%2520modalities%2520such%2520as%2520cardiac%2520MRI%2520and%2520echocardiogram.%250AIn%2520response%2520to%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520multimodal%2520variational%250Aautoencoder%2520%2528%2524%255Ctext%257BCardioVAE%257D_%255Ctext%257BX%252CG%257D%2524%2529%2520to%2520integrate%2520low-cost%2520chest%2520X-ray%250A%2528CXR%2529%2520and%2520electrocardiogram%2520%2528ECG%2529%2520modalities%2520with%2520pre-training%2520on%2520a%2520large%250Aunlabeled%2520dataset.%2520Specifically%252C%2520%2524%255Ctext%257BCardioVAE%257D_%255Ctext%257BX%252CG%257D%2524%2520introduces%2520a%250Anovel%2520tri-stream%2520pre-training%2520strategy%2520to%2520learn%2520both%2520shared%2520and%250Amodality-specific%2520features%252C%2520thus%2520enabling%2520fine-tuning%2520with%2520both%2520unimodal%2520and%250Amultimodal%2520datasets.%2520We%2520pre-train%2520%2524%255Ctext%257BCardioVAE%257D_%255Ctext%257BX%252CG%257D%2524%2520on%2520a%2520large%252C%250Aunlabeled%2520dataset%2520of%2520%252450%252C982%2524%2520subjects%2520from%2520a%2520subset%2520of%2520MIMIC%2520database%2520and%2520then%250Afine-tune%2520the%2520pre-trained%2520model%2520on%2520a%2520labeled%2520dataset%2520of%2520%2524795%2524%2520subjects%2520from%2520the%250AASPIRE%2520registry.%2520Comprehensive%2520evaluations%2520against%2520existing%2520methods%2520show%2520that%250A%2524%255Ctext%257BCardioVAE%257D_%255Ctext%257BX%252CG%257D%2524%2520offers%2520promising%2520performance%2520%2528AUROC%2520%2524%253D0.79%2524%2520and%250AAccuracy%2520%2524%253D0.77%2524%2529%252C%2520representing%2520a%2520significant%2520step%2520forward%2520in%2520non-invasive%250Aprediction%2520of%2520CHDI.%2520Our%2520model%2520also%2520excels%2520in%2520producing%2520fine%2520interpretations%2520of%250Apredictions%2520directly%2520associated%2520with%2520clinical%2520features%252C%2520thereby%2520supporting%250Aclinical%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13658v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Variational%20Autoencoder%20for%20Low-cost%20Cardiac%20Hemodynamics%0A%20%20Instability%20Detection&entry.906535625=Mohammod%20N.%20I.%20Suvon%20and%20Prasun%20C.%20Tripathi%20and%20Wenrui%20Fan%20and%20Shuo%20Zhou%20and%20Xianyuan%20Liu%20and%20Samer%20Alabed%20and%20Venet%20Osmani%20and%20Andrew%20J.%20Swift%20and%20Chen%20Chen%20and%20Haiping%20Lu&entry.1292438233=%20%20Recent%20advancements%20in%20non-invasive%20detection%20of%20cardiac%20hemodynamic%0Ainstability%20%28CHDI%29%20primarily%20focus%20on%20applying%20machine%20learning%20techniques%20to%20a%0Asingle%20data%20modality%2C%20e.g.%20cardiac%20magnetic%20resonance%20imaging%20%28MRI%29.%20Despite%0Atheir%20potential%2C%20these%20approaches%20often%20fall%20short%20especially%20when%20the%20size%20of%0Alabeled%20patient%20data%20is%20limited%2C%20a%20common%20challenge%20in%20the%20medical%20domain.%0AFurthermore%2C%20only%20a%20few%20studies%20have%20explored%20multimodal%20methods%20to%20study%20CHDI%2C%0Awhich%20mostly%20rely%20on%20costly%20modalities%20such%20as%20cardiac%20MRI%20and%20echocardiogram.%0AIn%20response%20to%20these%20limitations%2C%20we%20propose%20a%20novel%20multimodal%20variational%0Aautoencoder%20%28%24%5Ctext%7BCardioVAE%7D_%5Ctext%7BX%2CG%7D%24%29%20to%20integrate%20low-cost%20chest%20X-ray%0A%28CXR%29%20and%20electrocardiogram%20%28ECG%29%20modalities%20with%20pre-training%20on%20a%20large%0Aunlabeled%20dataset.%20Specifically%2C%20%24%5Ctext%7BCardioVAE%7D_%5Ctext%7BX%2CG%7D%24%20introduces%20a%0Anovel%20tri-stream%20pre-training%20strategy%20to%20learn%20both%20shared%20and%0Amodality-specific%20features%2C%20thus%20enabling%20fine-tuning%20with%20both%20unimodal%20and%0Amultimodal%20datasets.%20We%20pre-train%20%24%5Ctext%7BCardioVAE%7D_%5Ctext%7BX%2CG%7D%24%20on%20a%20large%2C%0Aunlabeled%20dataset%20of%20%2450%2C982%24%20subjects%20from%20a%20subset%20of%20MIMIC%20database%20and%20then%0Afine-tune%20the%20pre-trained%20model%20on%20a%20labeled%20dataset%20of%20%24795%24%20subjects%20from%20the%0AASPIRE%20registry.%20Comprehensive%20evaluations%20against%20existing%20methods%20show%20that%0A%24%5Ctext%7BCardioVAE%7D_%5Ctext%7BX%2CG%7D%24%20offers%20promising%20performance%20%28AUROC%20%24%3D0.79%24%20and%0AAccuracy%20%24%3D0.77%24%29%2C%20representing%20a%20significant%20step%20forward%20in%20non-invasive%0Aprediction%20of%20CHDI.%20Our%20model%20also%20excels%20in%20producing%20fine%20interpretations%20of%0Apredictions%20directly%20associated%20with%20clinical%20features%2C%20thereby%20supporting%0Aclinical%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13658v3&entry.124074799=Read"},
{"title": "The Complexity of Symmetry Breaking Beyond Lex-Leader", "author": "Markus Anders and Sofia Brenner and Gaurav Rattan", "abstract": "  Symmetry breaking is a widely popular approach to enhance solvers in\nconstraint programming, such as those for SAT or MIP. Symmetry breaking\npredicates (SBPs) typically impose an order on variables and single out the\nlexicographic leader (lex-leader) in each orbit of assignments. Although it is\nNP-hard to find complete lex-leader SBPs, incomplete lex-leader SBPs are widely\nused in practice.\n  In this paper, we investigate the complexity of computing complete SBPs,\nlex-leader or otherwise, for SAT. Our main result proves a natural barrier for\nefficiently computing SBPs: efficient certification of graph non-isomorphism.\nOur results explain the difficulty of obtaining short SBPs for important CP\nproblems, such as matrix-models with row-column symmetries and graph generation\nproblems. Our results hold even when SBPs are allowed to introduce additional\nvariables. We show polynomial upper bounds for breaking certain symmetry\ngroups, namely automorphism groups of trees and wreath products of groups with\nefficient SBPs.\n", "link": "http://arxiv.org/abs/2407.04419v1", "date": "2024-07-05", "relevancy": 1.0888, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3779}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3705}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Complexity%20of%20Symmetry%20Breaking%20Beyond%20Lex-Leader&body=Title%3A%20The%20Complexity%20of%20Symmetry%20Breaking%20Beyond%20Lex-Leader%0AAuthor%3A%20Markus%20Anders%20and%20Sofia%20Brenner%20and%20Gaurav%20Rattan%0AAbstract%3A%20%20%20Symmetry%20breaking%20is%20a%20widely%20popular%20approach%20to%20enhance%20solvers%20in%0Aconstraint%20programming%2C%20such%20as%20those%20for%20SAT%20or%20MIP.%20Symmetry%20breaking%0Apredicates%20%28SBPs%29%20typically%20impose%20an%20order%20on%20variables%20and%20single%20out%20the%0Alexicographic%20leader%20%28lex-leader%29%20in%20each%20orbit%20of%20assignments.%20Although%20it%20is%0ANP-hard%20to%20find%20complete%20lex-leader%20SBPs%2C%20incomplete%20lex-leader%20SBPs%20are%20widely%0Aused%20in%20practice.%0A%20%20In%20this%20paper%2C%20we%20investigate%20the%20complexity%20of%20computing%20complete%20SBPs%2C%0Alex-leader%20or%20otherwise%2C%20for%20SAT.%20Our%20main%20result%20proves%20a%20natural%20barrier%20for%0Aefficiently%20computing%20SBPs%3A%20efficient%20certification%20of%20graph%20non-isomorphism.%0AOur%20results%20explain%20the%20difficulty%20of%20obtaining%20short%20SBPs%20for%20important%20CP%0Aproblems%2C%20such%20as%20matrix-models%20with%20row-column%20symmetries%20and%20graph%20generation%0Aproblems.%20Our%20results%20hold%20even%20when%20SBPs%20are%20allowed%20to%20introduce%20additional%0Avariables.%20We%20show%20polynomial%20upper%20bounds%20for%20breaking%20certain%20symmetry%0Agroups%2C%20namely%20automorphism%20groups%20of%20trees%20and%20wreath%20products%20of%20groups%20with%0Aefficient%20SBPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Complexity%2520of%2520Symmetry%2520Breaking%2520Beyond%2520Lex-Leader%26entry.906535625%3DMarkus%2520Anders%2520and%2520Sofia%2520Brenner%2520and%2520Gaurav%2520Rattan%26entry.1292438233%3D%2520%2520Symmetry%2520breaking%2520is%2520a%2520widely%2520popular%2520approach%2520to%2520enhance%2520solvers%2520in%250Aconstraint%2520programming%252C%2520such%2520as%2520those%2520for%2520SAT%2520or%2520MIP.%2520Symmetry%2520breaking%250Apredicates%2520%2528SBPs%2529%2520typically%2520impose%2520an%2520order%2520on%2520variables%2520and%2520single%2520out%2520the%250Alexicographic%2520leader%2520%2528lex-leader%2529%2520in%2520each%2520orbit%2520of%2520assignments.%2520Although%2520it%2520is%250ANP-hard%2520to%2520find%2520complete%2520lex-leader%2520SBPs%252C%2520incomplete%2520lex-leader%2520SBPs%2520are%2520widely%250Aused%2520in%2520practice.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520complexity%2520of%2520computing%2520complete%2520SBPs%252C%250Alex-leader%2520or%2520otherwise%252C%2520for%2520SAT.%2520Our%2520main%2520result%2520proves%2520a%2520natural%2520barrier%2520for%250Aefficiently%2520computing%2520SBPs%253A%2520efficient%2520certification%2520of%2520graph%2520non-isomorphism.%250AOur%2520results%2520explain%2520the%2520difficulty%2520of%2520obtaining%2520short%2520SBPs%2520for%2520important%2520CP%250Aproblems%252C%2520such%2520as%2520matrix-models%2520with%2520row-column%2520symmetries%2520and%2520graph%2520generation%250Aproblems.%2520Our%2520results%2520hold%2520even%2520when%2520SBPs%2520are%2520allowed%2520to%2520introduce%2520additional%250Avariables.%2520We%2520show%2520polynomial%2520upper%2520bounds%2520for%2520breaking%2520certain%2520symmetry%250Agroups%252C%2520namely%2520automorphism%2520groups%2520of%2520trees%2520and%2520wreath%2520products%2520of%2520groups%2520with%250Aefficient%2520SBPs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Complexity%20of%20Symmetry%20Breaking%20Beyond%20Lex-Leader&entry.906535625=Markus%20Anders%20and%20Sofia%20Brenner%20and%20Gaurav%20Rattan&entry.1292438233=%20%20Symmetry%20breaking%20is%20a%20widely%20popular%20approach%20to%20enhance%20solvers%20in%0Aconstraint%20programming%2C%20such%20as%20those%20for%20SAT%20or%20MIP.%20Symmetry%20breaking%0Apredicates%20%28SBPs%29%20typically%20impose%20an%20order%20on%20variables%20and%20single%20out%20the%0Alexicographic%20leader%20%28lex-leader%29%20in%20each%20orbit%20of%20assignments.%20Although%20it%20is%0ANP-hard%20to%20find%20complete%20lex-leader%20SBPs%2C%20incomplete%20lex-leader%20SBPs%20are%20widely%0Aused%20in%20practice.%0A%20%20In%20this%20paper%2C%20we%20investigate%20the%20complexity%20of%20computing%20complete%20SBPs%2C%0Alex-leader%20or%20otherwise%2C%20for%20SAT.%20Our%20main%20result%20proves%20a%20natural%20barrier%20for%0Aefficiently%20computing%20SBPs%3A%20efficient%20certification%20of%20graph%20non-isomorphism.%0AOur%20results%20explain%20the%20difficulty%20of%20obtaining%20short%20SBPs%20for%20important%20CP%0Aproblems%2C%20such%20as%20matrix-models%20with%20row-column%20symmetries%20and%20graph%20generation%0Aproblems.%20Our%20results%20hold%20even%20when%20SBPs%20are%20allowed%20to%20introduce%20additional%0Avariables.%20We%20show%20polynomial%20upper%20bounds%20for%20breaking%20certain%20symmetry%0Agroups%2C%20namely%20automorphism%20groups%20of%20trees%20and%20wreath%20products%20of%20groups%20with%0Aefficient%20SBPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04419v1&entry.124074799=Read"},
{"title": "Planetary Causal Inference: Implications for the Geography of Poverty", "author": "Kazuki Sakamoto and Connor T. Jerzak and Adel Daoud", "abstract": "  Earth observation data such as satellite imagery can, when combined with\nmachine learning, can have far-reaching impacts on our understanding of the\ngeography of poverty through the prediction of living conditions, especially\nwhere government-derived economic indicators are either unavailable or\npotentially untrustworthy. Recent work has progressed in using Earth\nObservation (EO) data not only to predict spatial economic outcomes but also to\nexplore cause and effect, an understanding which is critical for downstream\npolicy analysis. In this review, we first document the growth of interest in\nusing satellite images together with EO data in causal analysis. We then trace\nthe relationship between spatial statistics and machine learning methods before\ndiscussing four ways in which EO data has been used in causal machine learning\npipelines -- (1.) poverty outcome imputation for downstream causal analysis,\n(2.) EO image deconfounding, (3.) EO-based treatment effect heterogeneity, and\n(4.) EO-based transportability analysis. We conclude by providing a\nstep-by-step workflow for how researchers can incorporate EO data in causal ML\nanalysis going forward, outlining major choices of data, models, and evaluation\nmetrics.\n", "link": "http://arxiv.org/abs/2406.02584v2", "date": "2024-07-05", "relevancy": 1.3645, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4781}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4675}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planetary%20Causal%20Inference%3A%20Implications%20for%20the%20Geography%20of%20Poverty&body=Title%3A%20Planetary%20Causal%20Inference%3A%20Implications%20for%20the%20Geography%20of%20Poverty%0AAuthor%3A%20Kazuki%20Sakamoto%20and%20Connor%20T.%20Jerzak%20and%20Adel%20Daoud%0AAbstract%3A%20%20%20Earth%20observation%20data%20such%20as%20satellite%20imagery%20can%2C%20when%20combined%20with%0Amachine%20learning%2C%20can%20have%20far-reaching%20impacts%20on%20our%20understanding%20of%20the%0Ageography%20of%20poverty%20through%20the%20prediction%20of%20living%20conditions%2C%20especially%0Awhere%20government-derived%20economic%20indicators%20are%20either%20unavailable%20or%0Apotentially%20untrustworthy.%20Recent%20work%20has%20progressed%20in%20using%20Earth%0AObservation%20%28EO%29%20data%20not%20only%20to%20predict%20spatial%20economic%20outcomes%20but%20also%20to%0Aexplore%20cause%20and%20effect%2C%20an%20understanding%20which%20is%20critical%20for%20downstream%0Apolicy%20analysis.%20In%20this%20review%2C%20we%20first%20document%20the%20growth%20of%20interest%20in%0Ausing%20satellite%20images%20together%20with%20EO%20data%20in%20causal%20analysis.%20We%20then%20trace%0Athe%20relationship%20between%20spatial%20statistics%20and%20machine%20learning%20methods%20before%0Adiscussing%20four%20ways%20in%20which%20EO%20data%20has%20been%20used%20in%20causal%20machine%20learning%0Apipelines%20--%20%281.%29%20poverty%20outcome%20imputation%20for%20downstream%20causal%20analysis%2C%0A%282.%29%20EO%20image%20deconfounding%2C%20%283.%29%20EO-based%20treatment%20effect%20heterogeneity%2C%20and%0A%284.%29%20EO-based%20transportability%20analysis.%20We%20conclude%20by%20providing%20a%0Astep-by-step%20workflow%20for%20how%20researchers%20can%20incorporate%20EO%20data%20in%20causal%20ML%0Aanalysis%20going%20forward%2C%20outlining%20major%20choices%20of%20data%2C%20models%2C%20and%20evaluation%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanetary%2520Causal%2520Inference%253A%2520Implications%2520for%2520the%2520Geography%2520of%2520Poverty%26entry.906535625%3DKazuki%2520Sakamoto%2520and%2520Connor%2520T.%2520Jerzak%2520and%2520Adel%2520Daoud%26entry.1292438233%3D%2520%2520Earth%2520observation%2520data%2520such%2520as%2520satellite%2520imagery%2520can%252C%2520when%2520combined%2520with%250Amachine%2520learning%252C%2520can%2520have%2520far-reaching%2520impacts%2520on%2520our%2520understanding%2520of%2520the%250Ageography%2520of%2520poverty%2520through%2520the%2520prediction%2520of%2520living%2520conditions%252C%2520especially%250Awhere%2520government-derived%2520economic%2520indicators%2520are%2520either%2520unavailable%2520or%250Apotentially%2520untrustworthy.%2520Recent%2520work%2520has%2520progressed%2520in%2520using%2520Earth%250AObservation%2520%2528EO%2529%2520data%2520not%2520only%2520to%2520predict%2520spatial%2520economic%2520outcomes%2520but%2520also%2520to%250Aexplore%2520cause%2520and%2520effect%252C%2520an%2520understanding%2520which%2520is%2520critical%2520for%2520downstream%250Apolicy%2520analysis.%2520In%2520this%2520review%252C%2520we%2520first%2520document%2520the%2520growth%2520of%2520interest%2520in%250Ausing%2520satellite%2520images%2520together%2520with%2520EO%2520data%2520in%2520causal%2520analysis.%2520We%2520then%2520trace%250Athe%2520relationship%2520between%2520spatial%2520statistics%2520and%2520machine%2520learning%2520methods%2520before%250Adiscussing%2520four%2520ways%2520in%2520which%2520EO%2520data%2520has%2520been%2520used%2520in%2520causal%2520machine%2520learning%250Apipelines%2520--%2520%25281.%2529%2520poverty%2520outcome%2520imputation%2520for%2520downstream%2520causal%2520analysis%252C%250A%25282.%2529%2520EO%2520image%2520deconfounding%252C%2520%25283.%2529%2520EO-based%2520treatment%2520effect%2520heterogeneity%252C%2520and%250A%25284.%2529%2520EO-based%2520transportability%2520analysis.%2520We%2520conclude%2520by%2520providing%2520a%250Astep-by-step%2520workflow%2520for%2520how%2520researchers%2520can%2520incorporate%2520EO%2520data%2520in%2520causal%2520ML%250Aanalysis%2520going%2520forward%252C%2520outlining%2520major%2520choices%2520of%2520data%252C%2520models%252C%2520and%2520evaluation%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planetary%20Causal%20Inference%3A%20Implications%20for%20the%20Geography%20of%20Poverty&entry.906535625=Kazuki%20Sakamoto%20and%20Connor%20T.%20Jerzak%20and%20Adel%20Daoud&entry.1292438233=%20%20Earth%20observation%20data%20such%20as%20satellite%20imagery%20can%2C%20when%20combined%20with%0Amachine%20learning%2C%20can%20have%20far-reaching%20impacts%20on%20our%20understanding%20of%20the%0Ageography%20of%20poverty%20through%20the%20prediction%20of%20living%20conditions%2C%20especially%0Awhere%20government-derived%20economic%20indicators%20are%20either%20unavailable%20or%0Apotentially%20untrustworthy.%20Recent%20work%20has%20progressed%20in%20using%20Earth%0AObservation%20%28EO%29%20data%20not%20only%20to%20predict%20spatial%20economic%20outcomes%20but%20also%20to%0Aexplore%20cause%20and%20effect%2C%20an%20understanding%20which%20is%20critical%20for%20downstream%0Apolicy%20analysis.%20In%20this%20review%2C%20we%20first%20document%20the%20growth%20of%20interest%20in%0Ausing%20satellite%20images%20together%20with%20EO%20data%20in%20causal%20analysis.%20We%20then%20trace%0Athe%20relationship%20between%20spatial%20statistics%20and%20machine%20learning%20methods%20before%0Adiscussing%20four%20ways%20in%20which%20EO%20data%20has%20been%20used%20in%20causal%20machine%20learning%0Apipelines%20--%20%281.%29%20poverty%20outcome%20imputation%20for%20downstream%20causal%20analysis%2C%0A%282.%29%20EO%20image%20deconfounding%2C%20%283.%29%20EO-based%20treatment%20effect%20heterogeneity%2C%20and%0A%284.%29%20EO-based%20transportability%20analysis.%20We%20conclude%20by%20providing%20a%0Astep-by-step%20workflow%20for%20how%20researchers%20can%20incorporate%20EO%20data%20in%20causal%20ML%0Aanalysis%20going%20forward%2C%20outlining%20major%20choices%20of%20data%2C%20models%2C%20and%20evaluation%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02584v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


