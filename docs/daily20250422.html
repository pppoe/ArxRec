<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250421.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image", "author": "Fei Yin and Mallikarjun B R and Chun-Han Yao and Rafa\u0142 Mantiuk and Varun Jampani", "abstract": "  We present a novel framework for generating high-quality, animatable 4D\navatar from a single image. While recent advances have shown promising results\nin 4D avatar creation, existing methods either require extensive multiview data\nor struggle with shape accuracy and identity consistency. To address these\nlimitations, we propose a comprehensive system that leverages shape, image, and\nvideo priors to create full-view, animatable avatars. Our approach first\nobtains initial coarse shape through 3D-GAN inversion. Then, it enhances\nmultiview textures using depth-guided warping signals for cross-view\nconsistency with the help of the image diffusion model. To handle expression\nanimation, we incorporate a video prior with synchronized driving signals\nacross viewpoints. We further introduce a Consistent-Inconsistent training to\neffectively handle data inconsistencies during 4D reconstruction. Experimental\nresults demonstrate that our method achieves superior quality compared to the\nprior art, while maintaining consistency across different viewpoints and\nexpressions.\n", "link": "http://arxiv.org/abs/2504.15179v1", "date": "2025-04-21", "relevancy": 3.426, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6908}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6908}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaceCraft4D%3A%20Animated%203D%20Facial%20Avatar%20Generation%20from%20a%20Single%20Image&body=Title%3A%20FaceCraft4D%3A%20Animated%203D%20Facial%20Avatar%20Generation%20from%20a%20Single%20Image%0AAuthor%3A%20Fei%20Yin%20and%20Mallikarjun%20B%20R%20and%20Chun-Han%20Yao%20and%20Rafa%C5%82%20Mantiuk%20and%20Varun%20Jampani%0AAbstract%3A%20%20%20We%20present%20a%20novel%20framework%20for%20generating%20high-quality%2C%20animatable%204D%0Aavatar%20from%20a%20single%20image.%20While%20recent%20advances%20have%20shown%20promising%20results%0Ain%204D%20avatar%20creation%2C%20existing%20methods%20either%20require%20extensive%20multiview%20data%0Aor%20struggle%20with%20shape%20accuracy%20and%20identity%20consistency.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20comprehensive%20system%20that%20leverages%20shape%2C%20image%2C%20and%0Avideo%20priors%20to%20create%20full-view%2C%20animatable%20avatars.%20Our%20approach%20first%0Aobtains%20initial%20coarse%20shape%20through%203D-GAN%20inversion.%20Then%2C%20it%20enhances%0Amultiview%20textures%20using%20depth-guided%20warping%20signals%20for%20cross-view%0Aconsistency%20with%20the%20help%20of%20the%20image%20diffusion%20model.%20To%20handle%20expression%0Aanimation%2C%20we%20incorporate%20a%20video%20prior%20with%20synchronized%20driving%20signals%0Aacross%20viewpoints.%20We%20further%20introduce%20a%20Consistent-Inconsistent%20training%20to%0Aeffectively%20handle%20data%20inconsistencies%20during%204D%20reconstruction.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20achieves%20superior%20quality%20compared%20to%20the%0Aprior%20art%2C%20while%20maintaining%20consistency%20across%20different%20viewpoints%20and%0Aexpressions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaceCraft4D%253A%2520Animated%25203D%2520Facial%2520Avatar%2520Generation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DFei%2520Yin%2520and%2520Mallikarjun%2520B%2520R%2520and%2520Chun-Han%2520Yao%2520and%2520Rafa%25C5%2582%2520Mantiuk%2520and%2520Varun%2520Jampani%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520framework%2520for%2520generating%2520high-quality%252C%2520animatable%25204D%250Aavatar%2520from%2520a%2520single%2520image.%2520While%2520recent%2520advances%2520have%2520shown%2520promising%2520results%250Ain%25204D%2520avatar%2520creation%252C%2520existing%2520methods%2520either%2520require%2520extensive%2520multiview%2520data%250Aor%2520struggle%2520with%2520shape%2520accuracy%2520and%2520identity%2520consistency.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520comprehensive%2520system%2520that%2520leverages%2520shape%252C%2520image%252C%2520and%250Avideo%2520priors%2520to%2520create%2520full-view%252C%2520animatable%2520avatars.%2520Our%2520approach%2520first%250Aobtains%2520initial%2520coarse%2520shape%2520through%25203D-GAN%2520inversion.%2520Then%252C%2520it%2520enhances%250Amultiview%2520textures%2520using%2520depth-guided%2520warping%2520signals%2520for%2520cross-view%250Aconsistency%2520with%2520the%2520help%2520of%2520the%2520image%2520diffusion%2520model.%2520To%2520handle%2520expression%250Aanimation%252C%2520we%2520incorporate%2520a%2520video%2520prior%2520with%2520synchronized%2520driving%2520signals%250Aacross%2520viewpoints.%2520We%2520further%2520introduce%2520a%2520Consistent-Inconsistent%2520training%2520to%250Aeffectively%2520handle%2520data%2520inconsistencies%2520during%25204D%2520reconstruction.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520quality%2520compared%2520to%2520the%250Aprior%2520art%252C%2520while%2520maintaining%2520consistency%2520across%2520different%2520viewpoints%2520and%250Aexpressions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaceCraft4D%3A%20Animated%203D%20Facial%20Avatar%20Generation%20from%20a%20Single%20Image&entry.906535625=Fei%20Yin%20and%20Mallikarjun%20B%20R%20and%20Chun-Han%20Yao%20and%20Rafa%C5%82%20Mantiuk%20and%20Varun%20Jampani&entry.1292438233=%20%20We%20present%20a%20novel%20framework%20for%20generating%20high-quality%2C%20animatable%204D%0Aavatar%20from%20a%20single%20image.%20While%20recent%20advances%20have%20shown%20promising%20results%0Ain%204D%20avatar%20creation%2C%20existing%20methods%20either%20require%20extensive%20multiview%20data%0Aor%20struggle%20with%20shape%20accuracy%20and%20identity%20consistency.%20To%20address%20these%0Alimitations%2C%20we%20propose%20a%20comprehensive%20system%20that%20leverages%20shape%2C%20image%2C%20and%0Avideo%20priors%20to%20create%20full-view%2C%20animatable%20avatars.%20Our%20approach%20first%0Aobtains%20initial%20coarse%20shape%20through%203D-GAN%20inversion.%20Then%2C%20it%20enhances%0Amultiview%20textures%20using%20depth-guided%20warping%20signals%20for%20cross-view%0Aconsistency%20with%20the%20help%20of%20the%20image%20diffusion%20model.%20To%20handle%20expression%0Aanimation%2C%20we%20incorporate%20a%20video%20prior%20with%20synchronized%20driving%20signals%0Aacross%20viewpoints.%20We%20further%20introduce%20a%20Consistent-Inconsistent%20training%20to%0Aeffectively%20handle%20data%20inconsistencies%20during%204D%20reconstruction.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20achieves%20superior%20quality%20compared%20to%20the%0Aprior%20art%2C%20while%20maintaining%20consistency%20across%20different%20viewpoints%20and%0Aexpressions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15179v1&entry.124074799=Read"},
{"title": "Direct Learning of Mesh and Appearance via 3D Gaussian Splatting", "author": "Ancheng Lin and Yusheng Xiang and Paul Kennedy and Jun Li", "abstract": "  Accurately reconstructing a 3D scene including explicit geometry information\nis both attractive and challenging. Geometry reconstruction can benefit from\nincorporating differentiable appearance models, such as Neural Radiance Fields\nand 3D Gaussian Splatting (3DGS). However, existing methods encounter\nefficiency issues due to indirect geometry learning and the paradigm of\nseparately modeling geometry and surface appearance. In this work, we propose a\nlearnable scene model that incorporates 3DGS with an explicit geometry\nrepresentation, namely a mesh. Our model learns the mesh and appearance in an\nend-to-end manner, where we bind 3D Gaussians to the mesh faces and perform\ndifferentiable rendering of 3DGS to obtain photometric supervision. The model\ncreates an effective information pathway to supervise the learning of both 3DGS\nand mesh. Experimental results demonstrate that the learned scene model not\nonly improves efficiency and rendering quality but also enables manipulation\nvia the explicit mesh. In addition, our model has a unique advantage in\nadapting to scene updates, thanks to the end-to-end learning of both mesh and\nappearance.\n", "link": "http://arxiv.org/abs/2405.06945v3", "date": "2025-04-21", "relevancy": 3.3354, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7013}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6529}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct%20Learning%20of%20Mesh%20and%20Appearance%20via%203D%20Gaussian%20Splatting&body=Title%3A%20Direct%20Learning%20of%20Mesh%20and%20Appearance%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Ancheng%20Lin%20and%20Yusheng%20Xiang%20and%20Paul%20Kennedy%20and%20Jun%20Li%0AAbstract%3A%20%20%20Accurately%20reconstructing%20a%203D%20scene%20including%20explicit%20geometry%20information%0Ais%20both%20attractive%20and%20challenging.%20Geometry%20reconstruction%20can%20benefit%20from%0Aincorporating%20differentiable%20appearance%20models%2C%20such%20as%20Neural%20Radiance%20Fields%0Aand%203D%20Gaussian%20Splatting%20%283DGS%29.%20However%2C%20existing%20methods%20encounter%0Aefficiency%20issues%20due%20to%20indirect%20geometry%20learning%20and%20the%20paradigm%20of%0Aseparately%20modeling%20geometry%20and%20surface%20appearance.%20In%20this%20work%2C%20we%20propose%20a%0Alearnable%20scene%20model%20that%20incorporates%203DGS%20with%20an%20explicit%20geometry%0Arepresentation%2C%20namely%20a%20mesh.%20Our%20model%20learns%20the%20mesh%20and%20appearance%20in%20an%0Aend-to-end%20manner%2C%20where%20we%20bind%203D%20Gaussians%20to%20the%20mesh%20faces%20and%20perform%0Adifferentiable%20rendering%20of%203DGS%20to%20obtain%20photometric%20supervision.%20The%20model%0Acreates%20an%20effective%20information%20pathway%20to%20supervise%20the%20learning%20of%20both%203DGS%0Aand%20mesh.%20Experimental%20results%20demonstrate%20that%20the%20learned%20scene%20model%20not%0Aonly%20improves%20efficiency%20and%20rendering%20quality%20but%20also%20enables%20manipulation%0Avia%20the%20explicit%20mesh.%20In%20addition%2C%20our%20model%20has%20a%20unique%20advantage%20in%0Aadapting%20to%20scene%20updates%2C%20thanks%20to%20the%20end-to-end%20learning%20of%20both%20mesh%20and%0Aappearance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06945v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect%2520Learning%2520of%2520Mesh%2520and%2520Appearance%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DAncheng%2520Lin%2520and%2520Yusheng%2520Xiang%2520and%2520Paul%2520Kennedy%2520and%2520Jun%2520Li%26entry.1292438233%3D%2520%2520Accurately%2520reconstructing%2520a%25203D%2520scene%2520including%2520explicit%2520geometry%2520information%250Ais%2520both%2520attractive%2520and%2520challenging.%2520Geometry%2520reconstruction%2520can%2520benefit%2520from%250Aincorporating%2520differentiable%2520appearance%2520models%252C%2520such%2520as%2520Neural%2520Radiance%2520Fields%250Aand%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520However%252C%2520existing%2520methods%2520encounter%250Aefficiency%2520issues%2520due%2520to%2520indirect%2520geometry%2520learning%2520and%2520the%2520paradigm%2520of%250Aseparately%2520modeling%2520geometry%2520and%2520surface%2520appearance.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%250Alearnable%2520scene%2520model%2520that%2520incorporates%25203DGS%2520with%2520an%2520explicit%2520geometry%250Arepresentation%252C%2520namely%2520a%2520mesh.%2520Our%2520model%2520learns%2520the%2520mesh%2520and%2520appearance%2520in%2520an%250Aend-to-end%2520manner%252C%2520where%2520we%2520bind%25203D%2520Gaussians%2520to%2520the%2520mesh%2520faces%2520and%2520perform%250Adifferentiable%2520rendering%2520of%25203DGS%2520to%2520obtain%2520photometric%2520supervision.%2520The%2520model%250Acreates%2520an%2520effective%2520information%2520pathway%2520to%2520supervise%2520the%2520learning%2520of%2520both%25203DGS%250Aand%2520mesh.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520learned%2520scene%2520model%2520not%250Aonly%2520improves%2520efficiency%2520and%2520rendering%2520quality%2520but%2520also%2520enables%2520manipulation%250Avia%2520the%2520explicit%2520mesh.%2520In%2520addition%252C%2520our%2520model%2520has%2520a%2520unique%2520advantage%2520in%250Aadapting%2520to%2520scene%2520updates%252C%2520thanks%2520to%2520the%2520end-to-end%2520learning%2520of%2520both%2520mesh%2520and%250Aappearance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06945v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Learning%20of%20Mesh%20and%20Appearance%20via%203D%20Gaussian%20Splatting&entry.906535625=Ancheng%20Lin%20and%20Yusheng%20Xiang%20and%20Paul%20Kennedy%20and%20Jun%20Li&entry.1292438233=%20%20Accurately%20reconstructing%20a%203D%20scene%20including%20explicit%20geometry%20information%0Ais%20both%20attractive%20and%20challenging.%20Geometry%20reconstruction%20can%20benefit%20from%0Aincorporating%20differentiable%20appearance%20models%2C%20such%20as%20Neural%20Radiance%20Fields%0Aand%203D%20Gaussian%20Splatting%20%283DGS%29.%20However%2C%20existing%20methods%20encounter%0Aefficiency%20issues%20due%20to%20indirect%20geometry%20learning%20and%20the%20paradigm%20of%0Aseparately%20modeling%20geometry%20and%20surface%20appearance.%20In%20this%20work%2C%20we%20propose%20a%0Alearnable%20scene%20model%20that%20incorporates%203DGS%20with%20an%20explicit%20geometry%0Arepresentation%2C%20namely%20a%20mesh.%20Our%20model%20learns%20the%20mesh%20and%20appearance%20in%20an%0Aend-to-end%20manner%2C%20where%20we%20bind%203D%20Gaussians%20to%20the%20mesh%20faces%20and%20perform%0Adifferentiable%20rendering%20of%203DGS%20to%20obtain%20photometric%20supervision.%20The%20model%0Acreates%20an%20effective%20information%20pathway%20to%20supervise%20the%20learning%20of%20both%203DGS%0Aand%20mesh.%20Experimental%20results%20demonstrate%20that%20the%20learned%20scene%20model%20not%0Aonly%20improves%20efficiency%20and%20rendering%20quality%20but%20also%20enables%20manipulation%0Avia%20the%20explicit%20mesh.%20In%20addition%2C%20our%20model%20has%20a%20unique%20advantage%20in%0Aadapting%20to%20scene%20updates%2C%20thanks%20to%20the%20end-to-end%20learning%20of%20both%20mesh%20and%0Aappearance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06945v3&entry.124074799=Read"},
{"title": "MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry\n  Monocular Video", "author": "Minh-Quan Viet Bui and Jongmin Park and Juan Luis Gonzalez Bello and Jaeho Moon and Jihyong Oh and Munchurl Kim", "abstract": "  We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)\nframework capable of reconstructing sharp and high-quality novel\nspatio-temporal views from blurry monocular videos in an end-to-end manner.\nExisting dynamic novel view synthesis (NVS) methods are highly sensitive to\nmotion blur in casually captured videos, resulting in significant degradation\nof rendering quality. While recent approaches address motion-blurred inputs for\nNVS, they primarily focus on static scene reconstruction and lack dedicated\nmotion modeling for dynamic objects. To overcome these limitations, our MoBGS\nintroduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for\neffective latent camera trajectory estimation, improving global camera motion\ndeblurring. In addition, we propose a physically-inspired Latent Camera-induced\nExposure Estimation (LCEE) method to ensure consistent deblurring of both\nglobal camera and local object motion. Our MoBGS framework ensures the temporal\nconsistency of unseen latent timestamps and robust motion decomposition of\nstatic and dynamic regions. Extensive experiments on the Stereo Blur dataset\nand real-world blurry videos show that our MoBGS significantly outperforms the\nvery recent advanced methods (DyBluRF and Deblur4DGS), achieving\nstate-of-the-art performance for dynamic NVS under motion blur.\n", "link": "http://arxiv.org/abs/2504.15122v1", "date": "2025-04-21", "relevancy": 3.2171, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6841}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6447}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoBGS%3A%20Motion%20Deblurring%20Dynamic%203D%20Gaussian%20Splatting%20for%20Blurry%0A%20%20Monocular%20Video&body=Title%3A%20MoBGS%3A%20Motion%20Deblurring%20Dynamic%203D%20Gaussian%20Splatting%20for%20Blurry%0A%20%20Monocular%20Video%0AAuthor%3A%20Minh-Quan%20Viet%20Bui%20and%20Jongmin%20Park%20and%20Juan%20Luis%20Gonzalez%20Bello%20and%20Jaeho%20Moon%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim%0AAbstract%3A%20%20%20We%20present%20MoBGS%2C%20a%20novel%20deblurring%20dynamic%203D%20Gaussian%20Splatting%20%283DGS%29%0Aframework%20capable%20of%20reconstructing%20sharp%20and%20high-quality%20novel%0Aspatio-temporal%20views%20from%20blurry%20monocular%20videos%20in%20an%20end-to-end%20manner.%0AExisting%20dynamic%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20highly%20sensitive%20to%0Amotion%20blur%20in%20casually%20captured%20videos%2C%20resulting%20in%20significant%20degradation%0Aof%20rendering%20quality.%20While%20recent%20approaches%20address%20motion-blurred%20inputs%20for%0ANVS%2C%20they%20primarily%20focus%20on%20static%20scene%20reconstruction%20and%20lack%20dedicated%0Amotion%20modeling%20for%20dynamic%20objects.%20To%20overcome%20these%20limitations%2C%20our%20MoBGS%0Aintroduces%20a%20novel%20Blur-adaptive%20Latent%20Camera%20Estimation%20%28BLCE%29%20method%20for%0Aeffective%20latent%20camera%20trajectory%20estimation%2C%20improving%20global%20camera%20motion%0Adeblurring.%20In%20addition%2C%20we%20propose%20a%20physically-inspired%20Latent%20Camera-induced%0AExposure%20Estimation%20%28LCEE%29%20method%20to%20ensure%20consistent%20deblurring%20of%20both%0Aglobal%20camera%20and%20local%20object%20motion.%20Our%20MoBGS%20framework%20ensures%20the%20temporal%0Aconsistency%20of%20unseen%20latent%20timestamps%20and%20robust%20motion%20decomposition%20of%0Astatic%20and%20dynamic%20regions.%20Extensive%20experiments%20on%20the%20Stereo%20Blur%20dataset%0Aand%20real-world%20blurry%20videos%20show%20that%20our%20MoBGS%20significantly%20outperforms%20the%0Avery%20recent%20advanced%20methods%20%28DyBluRF%20and%20Deblur4DGS%29%2C%20achieving%0Astate-of-the-art%20performance%20for%20dynamic%20NVS%20under%20motion%20blur.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoBGS%253A%2520Motion%2520Deblurring%2520Dynamic%25203D%2520Gaussian%2520Splatting%2520for%2520Blurry%250A%2520%2520Monocular%2520Video%26entry.906535625%3DMinh-Quan%2520Viet%2520Bui%2520and%2520Jongmin%2520Park%2520and%2520Juan%2520Luis%2520Gonzalez%2520Bello%2520and%2520Jaeho%2520Moon%2520and%2520Jihyong%2520Oh%2520and%2520Munchurl%2520Kim%26entry.1292438233%3D%2520%2520We%2520present%2520MoBGS%252C%2520a%2520novel%2520deblurring%2520dynamic%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%250Aframework%2520capable%2520of%2520reconstructing%2520sharp%2520and%2520high-quality%2520novel%250Aspatio-temporal%2520views%2520from%2520blurry%2520monocular%2520videos%2520in%2520an%2520end-to-end%2520manner.%250AExisting%2520dynamic%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520methods%2520are%2520highly%2520sensitive%2520to%250Amotion%2520blur%2520in%2520casually%2520captured%2520videos%252C%2520resulting%2520in%2520significant%2520degradation%250Aof%2520rendering%2520quality.%2520While%2520recent%2520approaches%2520address%2520motion-blurred%2520inputs%2520for%250ANVS%252C%2520they%2520primarily%2520focus%2520on%2520static%2520scene%2520reconstruction%2520and%2520lack%2520dedicated%250Amotion%2520modeling%2520for%2520dynamic%2520objects.%2520To%2520overcome%2520these%2520limitations%252C%2520our%2520MoBGS%250Aintroduces%2520a%2520novel%2520Blur-adaptive%2520Latent%2520Camera%2520Estimation%2520%2528BLCE%2529%2520method%2520for%250Aeffective%2520latent%2520camera%2520trajectory%2520estimation%252C%2520improving%2520global%2520camera%2520motion%250Adeblurring.%2520In%2520addition%252C%2520we%2520propose%2520a%2520physically-inspired%2520Latent%2520Camera-induced%250AExposure%2520Estimation%2520%2528LCEE%2529%2520method%2520to%2520ensure%2520consistent%2520deblurring%2520of%2520both%250Aglobal%2520camera%2520and%2520local%2520object%2520motion.%2520Our%2520MoBGS%2520framework%2520ensures%2520the%2520temporal%250Aconsistency%2520of%2520unseen%2520latent%2520timestamps%2520and%2520robust%2520motion%2520decomposition%2520of%250Astatic%2520and%2520dynamic%2520regions.%2520Extensive%2520experiments%2520on%2520the%2520Stereo%2520Blur%2520dataset%250Aand%2520real-world%2520blurry%2520videos%2520show%2520that%2520our%2520MoBGS%2520significantly%2520outperforms%2520the%250Avery%2520recent%2520advanced%2520methods%2520%2528DyBluRF%2520and%2520Deblur4DGS%2529%252C%2520achieving%250Astate-of-the-art%2520performance%2520for%2520dynamic%2520NVS%2520under%2520motion%2520blur.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoBGS%3A%20Motion%20Deblurring%20Dynamic%203D%20Gaussian%20Splatting%20for%20Blurry%0A%20%20Monocular%20Video&entry.906535625=Minh-Quan%20Viet%20Bui%20and%20Jongmin%20Park%20and%20Juan%20Luis%20Gonzalez%20Bello%20and%20Jaeho%20Moon%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim&entry.1292438233=%20%20We%20present%20MoBGS%2C%20a%20novel%20deblurring%20dynamic%203D%20Gaussian%20Splatting%20%283DGS%29%0Aframework%20capable%20of%20reconstructing%20sharp%20and%20high-quality%20novel%0Aspatio-temporal%20views%20from%20blurry%20monocular%20videos%20in%20an%20end-to-end%20manner.%0AExisting%20dynamic%20novel%20view%20synthesis%20%28NVS%29%20methods%20are%20highly%20sensitive%20to%0Amotion%20blur%20in%20casually%20captured%20videos%2C%20resulting%20in%20significant%20degradation%0Aof%20rendering%20quality.%20While%20recent%20approaches%20address%20motion-blurred%20inputs%20for%0ANVS%2C%20they%20primarily%20focus%20on%20static%20scene%20reconstruction%20and%20lack%20dedicated%0Amotion%20modeling%20for%20dynamic%20objects.%20To%20overcome%20these%20limitations%2C%20our%20MoBGS%0Aintroduces%20a%20novel%20Blur-adaptive%20Latent%20Camera%20Estimation%20%28BLCE%29%20method%20for%0Aeffective%20latent%20camera%20trajectory%20estimation%2C%20improving%20global%20camera%20motion%0Adeblurring.%20In%20addition%2C%20we%20propose%20a%20physically-inspired%20Latent%20Camera-induced%0AExposure%20Estimation%20%28LCEE%29%20method%20to%20ensure%20consistent%20deblurring%20of%20both%0Aglobal%20camera%20and%20local%20object%20motion.%20Our%20MoBGS%20framework%20ensures%20the%20temporal%0Aconsistency%20of%20unseen%20latent%20timestamps%20and%20robust%20motion%20decomposition%20of%0Astatic%20and%20dynamic%20regions.%20Extensive%20experiments%20on%20the%20Stereo%20Blur%20dataset%0Aand%20real-world%20blurry%20videos%20show%20that%20our%20MoBGS%20significantly%20outperforms%20the%0Avery%20recent%20advanced%20methods%20%28DyBluRF%20and%20Deblur4DGS%29%2C%20achieving%0Astate-of-the-art%20performance%20for%20dynamic%20NVS%20under%20motion%20blur.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15122v1&entry.124074799=Read"},
{"title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs", "author": "Chun-Hsiao Yeh and Chenyu Wang and Shengbang Tong and Ta-Ying Cheng and Rouyu Wang and Tianzhe Chu and Yuexiang Zhai and Yubei Chen and Shenghua Gao and Yi Ma", "abstract": "  Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.\n", "link": "http://arxiv.org/abs/2504.15280v1", "date": "2025-04-21", "relevancy": 3.1761, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.642}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.642}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20from%20Another%20Perspective%3A%20Evaluating%20Multi-View%20Understanding%20in%0A%20%20MLLMs&body=Title%3A%20Seeing%20from%20Another%20Perspective%3A%20Evaluating%20Multi-View%20Understanding%20in%0A%20%20MLLMs%0AAuthor%3A%20Chun-Hsiao%20Yeh%20and%20Chenyu%20Wang%20and%20Shengbang%20Tong%20and%20Ta-Ying%20Cheng%20and%20Rouyu%20Wang%20and%20Tianzhe%20Chu%20and%20Yuexiang%20Zhai%20and%20Yubei%20Chen%20and%20Shenghua%20Gao%20and%20Yi%20Ma%0AAbstract%3A%20%20%20Multi-view%20understanding%2C%20the%20ability%20to%20reconcile%20visual%20information%20across%0Adiverse%20viewpoints%20for%20effective%20navigation%2C%20manipulation%2C%20and%203D%20scene%0Acomprehension%2C%20is%20a%20fundamental%20challenge%20in%20Multi-Modal%20Large%20Language%20Models%0A%28MLLMs%29%20to%20be%20used%20as%20embodied%20agents.%20While%20recent%20MLLMs%20have%20shown%20impressive%0Aadvances%20in%20high-level%20reasoning%20and%20planning%2C%20they%20frequently%20fall%20short%20when%0Aconfronted%20with%20multi-view%20geometric%20consistency%20and%20cross-view%20correspondence.%0ATo%20comprehensively%20evaluate%20the%20challenges%20of%20MLLMs%20in%20multi-view%20scene%0Areasoning%2C%20we%20propose%20All-Angles%20Bench%2C%20a%20benchmark%20of%20over%202%2C100%20human%0Acarefully%20annotated%20multi-view%20question-answer%20pairs%20across%2090%20diverse%0Areal-world%20scenes.%20Our%20six%20tasks%20%28counting%2C%20attribute%20identification%2C%20relative%0Adistance%2C%20relative%20direction%2C%20object%20manipulation%2C%20and%20camera%20pose%20estimation%29%0Aspecifically%20test%20model%27s%20geometric%20correspondence%20and%20the%20capacity%20to%20align%0Ainformation%20consistently%20across%20views.%20Our%20extensive%20experiments%2C%20benchmark%20on%0A27%20representative%20MLLMs%20including%20Gemini-2.0-Flash%2C%20Claude-3.7-Sonnet%2C%20and%0AGPT-4o%20against%20human%20evaluators%20reveals%20a%20substantial%20performance%20gap%2C%0Aindicating%20that%20current%20MLLMs%20remain%20far%20from%20human-level%20proficiency.%20Through%0Ain-depth%20analysis%2C%20we%20show%20that%20MLLMs%20are%20particularly%20underperforming%20under%0Atwo%20aspects%3A%20%281%29%20cross-view%20correspondence%20for%20partially%20occluded%20views%20and%20%282%29%0Aestablishing%20the%20coarse%20camera%20poses.%20These%20findings%20highlight%20the%20necessity%20of%0Adomain-specific%20refinements%20or%20modules%20that%20embed%20stronger%20multi-view%0Aawareness.%20We%20believe%20that%20our%20All-Angles%20Bench%20offers%20valuable%20insights%20and%0Acontribute%20to%20bridging%20the%20gap%20between%20MLLMs%20and%20human-level%20multi-view%0Aunderstanding.%20The%20project%20and%20benchmark%20are%20publicly%20available%20at%0Ahttps%3A//danielchyeh.github.io/All-Angles-Bench/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15280v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520from%2520Another%2520Perspective%253A%2520Evaluating%2520Multi-View%2520Understanding%2520in%250A%2520%2520MLLMs%26entry.906535625%3DChun-Hsiao%2520Yeh%2520and%2520Chenyu%2520Wang%2520and%2520Shengbang%2520Tong%2520and%2520Ta-Ying%2520Cheng%2520and%2520Rouyu%2520Wang%2520and%2520Tianzhe%2520Chu%2520and%2520Yuexiang%2520Zhai%2520and%2520Yubei%2520Chen%2520and%2520Shenghua%2520Gao%2520and%2520Yi%2520Ma%26entry.1292438233%3D%2520%2520Multi-view%2520understanding%252C%2520the%2520ability%2520to%2520reconcile%2520visual%2520information%2520across%250Adiverse%2520viewpoints%2520for%2520effective%2520navigation%252C%2520manipulation%252C%2520and%25203D%2520scene%250Acomprehension%252C%2520is%2520a%2520fundamental%2520challenge%2520in%2520Multi-Modal%2520Large%2520Language%2520Models%250A%2528MLLMs%2529%2520to%2520be%2520used%2520as%2520embodied%2520agents.%2520While%2520recent%2520MLLMs%2520have%2520shown%2520impressive%250Aadvances%2520in%2520high-level%2520reasoning%2520and%2520planning%252C%2520they%2520frequently%2520fall%2520short%2520when%250Aconfronted%2520with%2520multi-view%2520geometric%2520consistency%2520and%2520cross-view%2520correspondence.%250ATo%2520comprehensively%2520evaluate%2520the%2520challenges%2520of%2520MLLMs%2520in%2520multi-view%2520scene%250Areasoning%252C%2520we%2520propose%2520All-Angles%2520Bench%252C%2520a%2520benchmark%2520of%2520over%25202%252C100%2520human%250Acarefully%2520annotated%2520multi-view%2520question-answer%2520pairs%2520across%252090%2520diverse%250Areal-world%2520scenes.%2520Our%2520six%2520tasks%2520%2528counting%252C%2520attribute%2520identification%252C%2520relative%250Adistance%252C%2520relative%2520direction%252C%2520object%2520manipulation%252C%2520and%2520camera%2520pose%2520estimation%2529%250Aspecifically%2520test%2520model%2527s%2520geometric%2520correspondence%2520and%2520the%2520capacity%2520to%2520align%250Ainformation%2520consistently%2520across%2520views.%2520Our%2520extensive%2520experiments%252C%2520benchmark%2520on%250A27%2520representative%2520MLLMs%2520including%2520Gemini-2.0-Flash%252C%2520Claude-3.7-Sonnet%252C%2520and%250AGPT-4o%2520against%2520human%2520evaluators%2520reveals%2520a%2520substantial%2520performance%2520gap%252C%250Aindicating%2520that%2520current%2520MLLMs%2520remain%2520far%2520from%2520human-level%2520proficiency.%2520Through%250Ain-depth%2520analysis%252C%2520we%2520show%2520that%2520MLLMs%2520are%2520particularly%2520underperforming%2520under%250Atwo%2520aspects%253A%2520%25281%2529%2520cross-view%2520correspondence%2520for%2520partially%2520occluded%2520views%2520and%2520%25282%2529%250Aestablishing%2520the%2520coarse%2520camera%2520poses.%2520These%2520findings%2520highlight%2520the%2520necessity%2520of%250Adomain-specific%2520refinements%2520or%2520modules%2520that%2520embed%2520stronger%2520multi-view%250Aawareness.%2520We%2520believe%2520that%2520our%2520All-Angles%2520Bench%2520offers%2520valuable%2520insights%2520and%250Acontribute%2520to%2520bridging%2520the%2520gap%2520between%2520MLLMs%2520and%2520human-level%2520multi-view%250Aunderstanding.%2520The%2520project%2520and%2520benchmark%2520are%2520publicly%2520available%2520at%250Ahttps%253A//danielchyeh.github.io/All-Angles-Bench/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15280v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20from%20Another%20Perspective%3A%20Evaluating%20Multi-View%20Understanding%20in%0A%20%20MLLMs&entry.906535625=Chun-Hsiao%20Yeh%20and%20Chenyu%20Wang%20and%20Shengbang%20Tong%20and%20Ta-Ying%20Cheng%20and%20Rouyu%20Wang%20and%20Tianzhe%20Chu%20and%20Yuexiang%20Zhai%20and%20Yubei%20Chen%20and%20Shenghua%20Gao%20and%20Yi%20Ma&entry.1292438233=%20%20Multi-view%20understanding%2C%20the%20ability%20to%20reconcile%20visual%20information%20across%0Adiverse%20viewpoints%20for%20effective%20navigation%2C%20manipulation%2C%20and%203D%20scene%0Acomprehension%2C%20is%20a%20fundamental%20challenge%20in%20Multi-Modal%20Large%20Language%20Models%0A%28MLLMs%29%20to%20be%20used%20as%20embodied%20agents.%20While%20recent%20MLLMs%20have%20shown%20impressive%0Aadvances%20in%20high-level%20reasoning%20and%20planning%2C%20they%20frequently%20fall%20short%20when%0Aconfronted%20with%20multi-view%20geometric%20consistency%20and%20cross-view%20correspondence.%0ATo%20comprehensively%20evaluate%20the%20challenges%20of%20MLLMs%20in%20multi-view%20scene%0Areasoning%2C%20we%20propose%20All-Angles%20Bench%2C%20a%20benchmark%20of%20over%202%2C100%20human%0Acarefully%20annotated%20multi-view%20question-answer%20pairs%20across%2090%20diverse%0Areal-world%20scenes.%20Our%20six%20tasks%20%28counting%2C%20attribute%20identification%2C%20relative%0Adistance%2C%20relative%20direction%2C%20object%20manipulation%2C%20and%20camera%20pose%20estimation%29%0Aspecifically%20test%20model%27s%20geometric%20correspondence%20and%20the%20capacity%20to%20align%0Ainformation%20consistently%20across%20views.%20Our%20extensive%20experiments%2C%20benchmark%20on%0A27%20representative%20MLLMs%20including%20Gemini-2.0-Flash%2C%20Claude-3.7-Sonnet%2C%20and%0AGPT-4o%20against%20human%20evaluators%20reveals%20a%20substantial%20performance%20gap%2C%0Aindicating%20that%20current%20MLLMs%20remain%20far%20from%20human-level%20proficiency.%20Through%0Ain-depth%20analysis%2C%20we%20show%20that%20MLLMs%20are%20particularly%20underperforming%20under%0Atwo%20aspects%3A%20%281%29%20cross-view%20correspondence%20for%20partially%20occluded%20views%20and%20%282%29%0Aestablishing%20the%20coarse%20camera%20poses.%20These%20findings%20highlight%20the%20necessity%20of%0Adomain-specific%20refinements%20or%20modules%20that%20embed%20stronger%20multi-view%0Aawareness.%20We%20believe%20that%20our%20All-Angles%20Bench%20offers%20valuable%20insights%20and%0Acontribute%20to%20bridging%20the%20gap%20between%20MLLMs%20and%20human-level%20multi-view%0Aunderstanding.%20The%20project%20and%20benchmark%20are%20publicly%20available%20at%0Ahttps%3A//danielchyeh.github.io/All-Angles-Bench/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15280v1&entry.124074799=Read"},
{"title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians", "author": "Cailin Zhuang and Yaoqi Hu and Xuanyang Zhang and Wei Cheng and Jiacheng Bao and Shengqi Liu and Yiying Yang and Xianfang Zeng and Gang Yu and Ming Li", "abstract": "  3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.\n", "link": "http://arxiv.org/abs/2504.15281v1", "date": "2025-04-21", "relevancy": 3.0917, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6517}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6043}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StyleMe3D%3A%20Stylization%20with%20Disentangled%20Priors%20by%20Multiple%20Encoders%20on%0A%20%203D%20Gaussians&body=Title%3A%20StyleMe3D%3A%20Stylization%20with%20Disentangled%20Priors%20by%20Multiple%20Encoders%20on%0A%20%203D%20Gaussians%0AAuthor%3A%20Cailin%20Zhuang%20and%20Yaoqi%20Hu%20and%20Xuanyang%20Zhang%20and%20Wei%20Cheng%20and%20Jiacheng%20Bao%20and%20Shengqi%20Liu%20and%20Yiying%20Yang%20and%20Xianfang%20Zeng%20and%20Gang%20Yu%20and%20Ming%20Li%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20excels%20in%20photorealistic%20scene%20reconstruction%0Abut%20struggles%20with%20stylized%20scenarios%20%28e.g.%2C%20cartoons%2C%20games%29%20due%20to%20fragmented%0Atextures%2C%20semantic%20misalignment%2C%20and%20limited%20adaptability%20to%20abstract%0Aaesthetics.%20We%20propose%20StyleMe3D%2C%20a%20holistic%20framework%20for%203D%20GS%20style%20transfer%0Athat%20integrates%20multi-modal%20style%20conditioning%2C%20multi-level%20semantic%20alignment%2C%0Aand%20perceptual%20quality%20enhancement.%20Our%20key%20insights%20include%3A%20%281%29%20optimizing%0Aonly%20RGB%20attributes%20preserves%20geometric%20integrity%20during%20stylization%3B%20%282%29%0Adisentangling%20low-%2C%20medium-%2C%20and%20high-level%20semantics%20is%20critical%20for%20coherent%0Astyle%20transfer%3B%20%283%29%20scalability%20across%20isolated%20objects%20and%20complex%20scenes%20is%0Aessential%20for%20practical%20deployment.%20StyleMe3D%20introduces%20four%20novel%20components%3A%0ADynamic%20Style%20Score%20Distillation%20%28DSSD%29%2C%20leveraging%20Stable%20Diffusion%27s%20latent%0Aspace%20for%20semantic%20alignment%3B%20Contrastive%20Style%20Descriptor%20%28CSD%29%20for%20localized%2C%0Acontent-aware%20texture%20transfer%3B%20Simultaneously%20Optimized%20Scale%20%28SOS%29%20to%0Adecouple%20style%20details%20and%20structural%20coherence%3B%20and%203D%20Gaussian%20Quality%0AAssessment%20%283DG-QA%29%2C%20a%20differentiable%20aesthetic%20prior%20trained%20on%20human-rated%0Adata%20to%20suppress%20artifacts%20and%20enhance%20visual%20harmony.%20Evaluated%20on%20NeRF%0Asynthetic%20dataset%20%28objects%29%20and%20tandt%20db%20%28scenes%29%20datasets%2C%20StyleMe3D%0Aoutperforms%20state-of-the-art%20methods%20in%20preserving%20geometric%20details%20%28e.g.%2C%0Acarvings%20on%20sculptures%29%20and%20ensuring%20stylistic%20consistency%20across%20scenes%20%28e.g.%2C%0Acoherent%20lighting%20in%20landscapes%29%2C%20while%20maintaining%20real-time%20rendering.%20This%0Awork%20bridges%20photorealistic%203D%20GS%20and%20artistic%20stylization%2C%20unlocking%0Aapplications%20in%20gaming%2C%20virtual%20worlds%2C%20and%20digital%20art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyleMe3D%253A%2520Stylization%2520with%2520Disentangled%2520Priors%2520by%2520Multiple%2520Encoders%2520on%250A%2520%25203D%2520Gaussians%26entry.906535625%3DCailin%2520Zhuang%2520and%2520Yaoqi%2520Hu%2520and%2520Xuanyang%2520Zhang%2520and%2520Wei%2520Cheng%2520and%2520Jiacheng%2520Bao%2520and%2520Shengqi%2520Liu%2520and%2520Yiying%2520Yang%2520and%2520Xianfang%2520Zeng%2520and%2520Gang%2520Yu%2520and%2520Ming%2520Li%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520excels%2520in%2520photorealistic%2520scene%2520reconstruction%250Abut%2520struggles%2520with%2520stylized%2520scenarios%2520%2528e.g.%252C%2520cartoons%252C%2520games%2529%2520due%2520to%2520fragmented%250Atextures%252C%2520semantic%2520misalignment%252C%2520and%2520limited%2520adaptability%2520to%2520abstract%250Aaesthetics.%2520We%2520propose%2520StyleMe3D%252C%2520a%2520holistic%2520framework%2520for%25203D%2520GS%2520style%2520transfer%250Athat%2520integrates%2520multi-modal%2520style%2520conditioning%252C%2520multi-level%2520semantic%2520alignment%252C%250Aand%2520perceptual%2520quality%2520enhancement.%2520Our%2520key%2520insights%2520include%253A%2520%25281%2529%2520optimizing%250Aonly%2520RGB%2520attributes%2520preserves%2520geometric%2520integrity%2520during%2520stylization%253B%2520%25282%2529%250Adisentangling%2520low-%252C%2520medium-%252C%2520and%2520high-level%2520semantics%2520is%2520critical%2520for%2520coherent%250Astyle%2520transfer%253B%2520%25283%2529%2520scalability%2520across%2520isolated%2520objects%2520and%2520complex%2520scenes%2520is%250Aessential%2520for%2520practical%2520deployment.%2520StyleMe3D%2520introduces%2520four%2520novel%2520components%253A%250ADynamic%2520Style%2520Score%2520Distillation%2520%2528DSSD%2529%252C%2520leveraging%2520Stable%2520Diffusion%2527s%2520latent%250Aspace%2520for%2520semantic%2520alignment%253B%2520Contrastive%2520Style%2520Descriptor%2520%2528CSD%2529%2520for%2520localized%252C%250Acontent-aware%2520texture%2520transfer%253B%2520Simultaneously%2520Optimized%2520Scale%2520%2528SOS%2529%2520to%250Adecouple%2520style%2520details%2520and%2520structural%2520coherence%253B%2520and%25203D%2520Gaussian%2520Quality%250AAssessment%2520%25283DG-QA%2529%252C%2520a%2520differentiable%2520aesthetic%2520prior%2520trained%2520on%2520human-rated%250Adata%2520to%2520suppress%2520artifacts%2520and%2520enhance%2520visual%2520harmony.%2520Evaluated%2520on%2520NeRF%250Asynthetic%2520dataset%2520%2528objects%2529%2520and%2520tandt%2520db%2520%2528scenes%2529%2520datasets%252C%2520StyleMe3D%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520preserving%2520geometric%2520details%2520%2528e.g.%252C%250Acarvings%2520on%2520sculptures%2529%2520and%2520ensuring%2520stylistic%2520consistency%2520across%2520scenes%2520%2528e.g.%252C%250Acoherent%2520lighting%2520in%2520landscapes%2529%252C%2520while%2520maintaining%2520real-time%2520rendering.%2520This%250Awork%2520bridges%2520photorealistic%25203D%2520GS%2520and%2520artistic%2520stylization%252C%2520unlocking%250Aapplications%2520in%2520gaming%252C%2520virtual%2520worlds%252C%2520and%2520digital%2520art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StyleMe3D%3A%20Stylization%20with%20Disentangled%20Priors%20by%20Multiple%20Encoders%20on%0A%20%203D%20Gaussians&entry.906535625=Cailin%20Zhuang%20and%20Yaoqi%20Hu%20and%20Xuanyang%20Zhang%20and%20Wei%20Cheng%20and%20Jiacheng%20Bao%20and%20Shengqi%20Liu%20and%20Yiying%20Yang%20and%20Xianfang%20Zeng%20and%20Gang%20Yu%20and%20Ming%20Li&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20excels%20in%20photorealistic%20scene%20reconstruction%0Abut%20struggles%20with%20stylized%20scenarios%20%28e.g.%2C%20cartoons%2C%20games%29%20due%20to%20fragmented%0Atextures%2C%20semantic%20misalignment%2C%20and%20limited%20adaptability%20to%20abstract%0Aaesthetics.%20We%20propose%20StyleMe3D%2C%20a%20holistic%20framework%20for%203D%20GS%20style%20transfer%0Athat%20integrates%20multi-modal%20style%20conditioning%2C%20multi-level%20semantic%20alignment%2C%0Aand%20perceptual%20quality%20enhancement.%20Our%20key%20insights%20include%3A%20%281%29%20optimizing%0Aonly%20RGB%20attributes%20preserves%20geometric%20integrity%20during%20stylization%3B%20%282%29%0Adisentangling%20low-%2C%20medium-%2C%20and%20high-level%20semantics%20is%20critical%20for%20coherent%0Astyle%20transfer%3B%20%283%29%20scalability%20across%20isolated%20objects%20and%20complex%20scenes%20is%0Aessential%20for%20practical%20deployment.%20StyleMe3D%20introduces%20four%20novel%20components%3A%0ADynamic%20Style%20Score%20Distillation%20%28DSSD%29%2C%20leveraging%20Stable%20Diffusion%27s%20latent%0Aspace%20for%20semantic%20alignment%3B%20Contrastive%20Style%20Descriptor%20%28CSD%29%20for%20localized%2C%0Acontent-aware%20texture%20transfer%3B%20Simultaneously%20Optimized%20Scale%20%28SOS%29%20to%0Adecouple%20style%20details%20and%20structural%20coherence%3B%20and%203D%20Gaussian%20Quality%0AAssessment%20%283DG-QA%29%2C%20a%20differentiable%20aesthetic%20prior%20trained%20on%20human-rated%0Adata%20to%20suppress%20artifacts%20and%20enhance%20visual%20harmony.%20Evaluated%20on%20NeRF%0Asynthetic%20dataset%20%28objects%29%20and%20tandt%20db%20%28scenes%29%20datasets%2C%20StyleMe3D%0Aoutperforms%20state-of-the-art%20methods%20in%20preserving%20geometric%20details%20%28e.g.%2C%0Acarvings%20on%20sculptures%29%20and%20ensuring%20stylistic%20consistency%20across%20scenes%20%28e.g.%2C%0Acoherent%20lighting%20in%20landscapes%29%2C%20while%20maintaining%20real-time%20rendering.%20This%0Awork%20bridges%20photorealistic%203D%20GS%20and%20artistic%20stylization%2C%20unlocking%0Aapplications%20in%20gaming%2C%20virtual%20worlds%2C%20and%20digital%20art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15281v1&entry.124074799=Read"},
{"title": "DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial\n  Basis Functions", "author": "Vishagar Arunan and Saeedha Nazar and Hashiru Pramuditha and Vinasirajan Viruthshaan and Sameera Ramasinghe and Simon Lucey and Ranga Rodrigo", "abstract": "  Splatting-based 3D reconstruction methods have gained popularity with the\nadvent of 3D Gaussian Splatting, efficiently synthesizing high-quality novel\nviews. These methods commonly resort to using exponential family functions,\nsuch as the Gaussian function, as reconstruction kernels due to their\nanisotropic nature, ease of projection, and differentiability in rasterization.\nHowever, the field remains restricted to variations within the exponential\nfamily, leaving generalized reconstruction kernels largely underexplored,\npartly due to the lack of easy integrability in 3D to 2D projections. In this\nlight, we show that a class of decaying anisotropic radial basis functions\n(DARBFs), which are non-negative functions of the Mahalanobis distance,\nsupports splatting by approximating the Gaussian function's closed-form\nintegration advantage. With this fresh perspective, we demonstrate up to 34%\nfaster convergence during training and a 45% reduction in memory consumption\nacross various DARB reconstruction kernels, while maintaining comparable PSNR,\nSSIM, and LPIPS results. We will make the code available.\n", "link": "http://arxiv.org/abs/2501.12369v2", "date": "2025-04-21", "relevancy": 3.0134, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.648}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5855}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DARB-Splatting%3A%20Generalizing%20Splatting%20with%20Decaying%20Anisotropic%20Radial%0A%20%20Basis%20Functions&body=Title%3A%20DARB-Splatting%3A%20Generalizing%20Splatting%20with%20Decaying%20Anisotropic%20Radial%0A%20%20Basis%20Functions%0AAuthor%3A%20Vishagar%20Arunan%20and%20Saeedha%20Nazar%20and%20Hashiru%20Pramuditha%20and%20Vinasirajan%20Viruthshaan%20and%20Sameera%20Ramasinghe%20and%20Simon%20Lucey%20and%20Ranga%20Rodrigo%0AAbstract%3A%20%20%20Splatting-based%203D%20reconstruction%20methods%20have%20gained%20popularity%20with%20the%0Aadvent%20of%203D%20Gaussian%20Splatting%2C%20efficiently%20synthesizing%20high-quality%20novel%0Aviews.%20These%20methods%20commonly%20resort%20to%20using%20exponential%20family%20functions%2C%0Asuch%20as%20the%20Gaussian%20function%2C%20as%20reconstruction%20kernels%20due%20to%20their%0Aanisotropic%20nature%2C%20ease%20of%20projection%2C%20and%20differentiability%20in%20rasterization.%0AHowever%2C%20the%20field%20remains%20restricted%20to%20variations%20within%20the%20exponential%0Afamily%2C%20leaving%20generalized%20reconstruction%20kernels%20largely%20underexplored%2C%0Apartly%20due%20to%20the%20lack%20of%20easy%20integrability%20in%203D%20to%202D%20projections.%20In%20this%0Alight%2C%20we%20show%20that%20a%20class%20of%20decaying%20anisotropic%20radial%20basis%20functions%0A%28DARBFs%29%2C%20which%20are%20non-negative%20functions%20of%20the%20Mahalanobis%20distance%2C%0Asupports%20splatting%20by%20approximating%20the%20Gaussian%20function%27s%20closed-form%0Aintegration%20advantage.%20With%20this%20fresh%20perspective%2C%20we%20demonstrate%20up%20to%2034%25%0Afaster%20convergence%20during%20training%20and%20a%2045%25%20reduction%20in%20memory%20consumption%0Aacross%20various%20DARB%20reconstruction%20kernels%2C%20while%20maintaining%20comparable%20PSNR%2C%0ASSIM%2C%20and%20LPIPS%20results.%20We%20will%20make%20the%20code%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12369v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDARB-Splatting%253A%2520Generalizing%2520Splatting%2520with%2520Decaying%2520Anisotropic%2520Radial%250A%2520%2520Basis%2520Functions%26entry.906535625%3DVishagar%2520Arunan%2520and%2520Saeedha%2520Nazar%2520and%2520Hashiru%2520Pramuditha%2520and%2520Vinasirajan%2520Viruthshaan%2520and%2520Sameera%2520Ramasinghe%2520and%2520Simon%2520Lucey%2520and%2520Ranga%2520Rodrigo%26entry.1292438233%3D%2520%2520Splatting-based%25203D%2520reconstruction%2520methods%2520have%2520gained%2520popularity%2520with%2520the%250Aadvent%2520of%25203D%2520Gaussian%2520Splatting%252C%2520efficiently%2520synthesizing%2520high-quality%2520novel%250Aviews.%2520These%2520methods%2520commonly%2520resort%2520to%2520using%2520exponential%2520family%2520functions%252C%250Asuch%2520as%2520the%2520Gaussian%2520function%252C%2520as%2520reconstruction%2520kernels%2520due%2520to%2520their%250Aanisotropic%2520nature%252C%2520ease%2520of%2520projection%252C%2520and%2520differentiability%2520in%2520rasterization.%250AHowever%252C%2520the%2520field%2520remains%2520restricted%2520to%2520variations%2520within%2520the%2520exponential%250Afamily%252C%2520leaving%2520generalized%2520reconstruction%2520kernels%2520largely%2520underexplored%252C%250Apartly%2520due%2520to%2520the%2520lack%2520of%2520easy%2520integrability%2520in%25203D%2520to%25202D%2520projections.%2520In%2520this%250Alight%252C%2520we%2520show%2520that%2520a%2520class%2520of%2520decaying%2520anisotropic%2520radial%2520basis%2520functions%250A%2528DARBFs%2529%252C%2520which%2520are%2520non-negative%2520functions%2520of%2520the%2520Mahalanobis%2520distance%252C%250Asupports%2520splatting%2520by%2520approximating%2520the%2520Gaussian%2520function%2527s%2520closed-form%250Aintegration%2520advantage.%2520With%2520this%2520fresh%2520perspective%252C%2520we%2520demonstrate%2520up%2520to%252034%2525%250Afaster%2520convergence%2520during%2520training%2520and%2520a%252045%2525%2520reduction%2520in%2520memory%2520consumption%250Aacross%2520various%2520DARB%2520reconstruction%2520kernels%252C%2520while%2520maintaining%2520comparable%2520PSNR%252C%250ASSIM%252C%2520and%2520LPIPS%2520results.%2520We%2520will%2520make%2520the%2520code%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12369v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DARB-Splatting%3A%20Generalizing%20Splatting%20with%20Decaying%20Anisotropic%20Radial%0A%20%20Basis%20Functions&entry.906535625=Vishagar%20Arunan%20and%20Saeedha%20Nazar%20and%20Hashiru%20Pramuditha%20and%20Vinasirajan%20Viruthshaan%20and%20Sameera%20Ramasinghe%20and%20Simon%20Lucey%20and%20Ranga%20Rodrigo&entry.1292438233=%20%20Splatting-based%203D%20reconstruction%20methods%20have%20gained%20popularity%20with%20the%0Aadvent%20of%203D%20Gaussian%20Splatting%2C%20efficiently%20synthesizing%20high-quality%20novel%0Aviews.%20These%20methods%20commonly%20resort%20to%20using%20exponential%20family%20functions%2C%0Asuch%20as%20the%20Gaussian%20function%2C%20as%20reconstruction%20kernels%20due%20to%20their%0Aanisotropic%20nature%2C%20ease%20of%20projection%2C%20and%20differentiability%20in%20rasterization.%0AHowever%2C%20the%20field%20remains%20restricted%20to%20variations%20within%20the%20exponential%0Afamily%2C%20leaving%20generalized%20reconstruction%20kernels%20largely%20underexplored%2C%0Apartly%20due%20to%20the%20lack%20of%20easy%20integrability%20in%203D%20to%202D%20projections.%20In%20this%0Alight%2C%20we%20show%20that%20a%20class%20of%20decaying%20anisotropic%20radial%20basis%20functions%0A%28DARBFs%29%2C%20which%20are%20non-negative%20functions%20of%20the%20Mahalanobis%20distance%2C%0Asupports%20splatting%20by%20approximating%20the%20Gaussian%20function%27s%20closed-form%0Aintegration%20advantage.%20With%20this%20fresh%20perspective%2C%20we%20demonstrate%20up%20to%2034%25%0Afaster%20convergence%20during%20training%20and%20a%2045%25%20reduction%20in%20memory%20consumption%0Aacross%20various%20DARB%20reconstruction%20kernels%2C%20while%20maintaining%20comparable%20PSNR%2C%0ASSIM%2C%20and%20LPIPS%20results.%20We%20will%20make%20the%20code%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12369v2&entry.124074799=Read"},
{"title": "Instance-Adaptive Keypoint Learning with Local-to-Global Geometric\n  Aggregation for Category-Level Object Pose Estimation", "author": "Xiao Zhang and Lu Zou and Tao Lu and Yuan Yao and Zhangjin Huang and Guoping Wang", "abstract": "  Category-level object pose estimation aims to predict the 6D pose and size of\npreviously unseen instances from predefined categories, requiring strong\ngeneralization across diverse object instances. Although many previous methods\nattempt to mitigate intra-class variations, they often struggle with instances\nexhibiting complex geometries or significant deviations from canonical shapes.\nTo address this challenge, we propose INKL-Pose, a novel category-level object\npose estimation framework that enables INstance-adaptive Keypoint Learning with\nlocal-to-global geometric aggregation. Specifically, our approach first\npredicts semantically consistent and geometric informative keypoints through an\nInstance-Adaptive Keypoint Generator, then refines them with: (1) a Local\nKeypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global\nKeypoint Feature Aggregator using bidirectional Mamba for structural\nconsistency. To enable bidirectional modeling in Mamba, we introduce a Feature\nSequence Flipping strategy that preserves spatial coherence while constructing\nbackward feature sequences. Additionally, we design a surface loss and a\nseparation loss to enforce uniform coverage and spatial diversity in keypoint\ndistribution. The generated keypoints are finally mapped to a canonical space\nfor regressing the object's 6D pose and size. Extensive experiments on\nCAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves\nstate-of-the-art performance and significantly outperforms existing methods.\n", "link": "http://arxiv.org/abs/2504.15134v1", "date": "2025-04-21", "relevancy": 2.9216, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6306}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5634}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-Adaptive%20Keypoint%20Learning%20with%20Local-to-Global%20Geometric%0A%20%20Aggregation%20for%20Category-Level%20Object%20Pose%20Estimation&body=Title%3A%20Instance-Adaptive%20Keypoint%20Learning%20with%20Local-to-Global%20Geometric%0A%20%20Aggregation%20for%20Category-Level%20Object%20Pose%20Estimation%0AAuthor%3A%20Xiao%20Zhang%20and%20Lu%20Zou%20and%20Tao%20Lu%20and%20Yuan%20Yao%20and%20Zhangjin%20Huang%20and%20Guoping%20Wang%0AAbstract%3A%20%20%20Category-level%20object%20pose%20estimation%20aims%20to%20predict%20the%206D%20pose%20and%20size%20of%0Apreviously%20unseen%20instances%20from%20predefined%20categories%2C%20requiring%20strong%0Ageneralization%20across%20diverse%20object%20instances.%20Although%20many%20previous%20methods%0Aattempt%20to%20mitigate%20intra-class%20variations%2C%20they%20often%20struggle%20with%20instances%0Aexhibiting%20complex%20geometries%20or%20significant%20deviations%20from%20canonical%20shapes.%0ATo%20address%20this%20challenge%2C%20we%20propose%20INKL-Pose%2C%20a%20novel%20category-level%20object%0Apose%20estimation%20framework%20that%20enables%20INstance-adaptive%20Keypoint%20Learning%20with%0Alocal-to-global%20geometric%20aggregation.%20Specifically%2C%20our%20approach%20first%0Apredicts%20semantically%20consistent%20and%20geometric%20informative%20keypoints%20through%20an%0AInstance-Adaptive%20Keypoint%20Generator%2C%20then%20refines%20them%20with%3A%20%281%29%20a%20Local%0AKeypoint%20Feature%20Aggregator%20capturing%20fine-grained%20geometries%2C%20and%20%282%29%20a%20Global%0AKeypoint%20Feature%20Aggregator%20using%20bidirectional%20Mamba%20for%20structural%0Aconsistency.%20To%20enable%20bidirectional%20modeling%20in%20Mamba%2C%20we%20introduce%20a%20Feature%0ASequence%20Flipping%20strategy%20that%20preserves%20spatial%20coherence%20while%20constructing%0Abackward%20feature%20sequences.%20Additionally%2C%20we%20design%20a%20surface%20loss%20and%20a%0Aseparation%20loss%20to%20enforce%20uniform%20coverage%20and%20spatial%20diversity%20in%20keypoint%0Adistribution.%20The%20generated%20keypoints%20are%20finally%20mapped%20to%20a%20canonical%20space%0Afor%20regressing%20the%20object%27s%206D%20pose%20and%20size.%20Extensive%20experiments%20on%0ACAMERA25%2C%20REAL275%2C%20and%20HouseCat6D%20demonstrate%20that%20INKL-Pose%20achieves%0Astate-of-the-art%20performance%20and%20significantly%20outperforms%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-Adaptive%2520Keypoint%2520Learning%2520with%2520Local-to-Global%2520Geometric%250A%2520%2520Aggregation%2520for%2520Category-Level%2520Object%2520Pose%2520Estimation%26entry.906535625%3DXiao%2520Zhang%2520and%2520Lu%2520Zou%2520and%2520Tao%2520Lu%2520and%2520Yuan%2520Yao%2520and%2520Zhangjin%2520Huang%2520and%2520Guoping%2520Wang%26entry.1292438233%3D%2520%2520Category-level%2520object%2520pose%2520estimation%2520aims%2520to%2520predict%2520the%25206D%2520pose%2520and%2520size%2520of%250Apreviously%2520unseen%2520instances%2520from%2520predefined%2520categories%252C%2520requiring%2520strong%250Ageneralization%2520across%2520diverse%2520object%2520instances.%2520Although%2520many%2520previous%2520methods%250Aattempt%2520to%2520mitigate%2520intra-class%2520variations%252C%2520they%2520often%2520struggle%2520with%2520instances%250Aexhibiting%2520complex%2520geometries%2520or%2520significant%2520deviations%2520from%2520canonical%2520shapes.%250ATo%2520address%2520this%2520challenge%252C%2520we%2520propose%2520INKL-Pose%252C%2520a%2520novel%2520category-level%2520object%250Apose%2520estimation%2520framework%2520that%2520enables%2520INstance-adaptive%2520Keypoint%2520Learning%2520with%250Alocal-to-global%2520geometric%2520aggregation.%2520Specifically%252C%2520our%2520approach%2520first%250Apredicts%2520semantically%2520consistent%2520and%2520geometric%2520informative%2520keypoints%2520through%2520an%250AInstance-Adaptive%2520Keypoint%2520Generator%252C%2520then%2520refines%2520them%2520with%253A%2520%25281%2529%2520a%2520Local%250AKeypoint%2520Feature%2520Aggregator%2520capturing%2520fine-grained%2520geometries%252C%2520and%2520%25282%2529%2520a%2520Global%250AKeypoint%2520Feature%2520Aggregator%2520using%2520bidirectional%2520Mamba%2520for%2520structural%250Aconsistency.%2520To%2520enable%2520bidirectional%2520modeling%2520in%2520Mamba%252C%2520we%2520introduce%2520a%2520Feature%250ASequence%2520Flipping%2520strategy%2520that%2520preserves%2520spatial%2520coherence%2520while%2520constructing%250Abackward%2520feature%2520sequences.%2520Additionally%252C%2520we%2520design%2520a%2520surface%2520loss%2520and%2520a%250Aseparation%2520loss%2520to%2520enforce%2520uniform%2520coverage%2520and%2520spatial%2520diversity%2520in%2520keypoint%250Adistribution.%2520The%2520generated%2520keypoints%2520are%2520finally%2520mapped%2520to%2520a%2520canonical%2520space%250Afor%2520regressing%2520the%2520object%2527s%25206D%2520pose%2520and%2520size.%2520Extensive%2520experiments%2520on%250ACAMERA25%252C%2520REAL275%252C%2520and%2520HouseCat6D%2520demonstrate%2520that%2520INKL-Pose%2520achieves%250Astate-of-the-art%2520performance%2520and%2520significantly%2520outperforms%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-Adaptive%20Keypoint%20Learning%20with%20Local-to-Global%20Geometric%0A%20%20Aggregation%20for%20Category-Level%20Object%20Pose%20Estimation&entry.906535625=Xiao%20Zhang%20and%20Lu%20Zou%20and%20Tao%20Lu%20and%20Yuan%20Yao%20and%20Zhangjin%20Huang%20and%20Guoping%20Wang&entry.1292438233=%20%20Category-level%20object%20pose%20estimation%20aims%20to%20predict%20the%206D%20pose%20and%20size%20of%0Apreviously%20unseen%20instances%20from%20predefined%20categories%2C%20requiring%20strong%0Ageneralization%20across%20diverse%20object%20instances.%20Although%20many%20previous%20methods%0Aattempt%20to%20mitigate%20intra-class%20variations%2C%20they%20often%20struggle%20with%20instances%0Aexhibiting%20complex%20geometries%20or%20significant%20deviations%20from%20canonical%20shapes.%0ATo%20address%20this%20challenge%2C%20we%20propose%20INKL-Pose%2C%20a%20novel%20category-level%20object%0Apose%20estimation%20framework%20that%20enables%20INstance-adaptive%20Keypoint%20Learning%20with%0Alocal-to-global%20geometric%20aggregation.%20Specifically%2C%20our%20approach%20first%0Apredicts%20semantically%20consistent%20and%20geometric%20informative%20keypoints%20through%20an%0AInstance-Adaptive%20Keypoint%20Generator%2C%20then%20refines%20them%20with%3A%20%281%29%20a%20Local%0AKeypoint%20Feature%20Aggregator%20capturing%20fine-grained%20geometries%2C%20and%20%282%29%20a%20Global%0AKeypoint%20Feature%20Aggregator%20using%20bidirectional%20Mamba%20for%20structural%0Aconsistency.%20To%20enable%20bidirectional%20modeling%20in%20Mamba%2C%20we%20introduce%20a%20Feature%0ASequence%20Flipping%20strategy%20that%20preserves%20spatial%20coherence%20while%20constructing%0Abackward%20feature%20sequences.%20Additionally%2C%20we%20design%20a%20surface%20loss%20and%20a%0Aseparation%20loss%20to%20enforce%20uniform%20coverage%20and%20spatial%20diversity%20in%20keypoint%0Adistribution.%20The%20generated%20keypoints%20are%20finally%20mapped%20to%20a%20canonical%20space%0Afor%20regressing%20the%20object%27s%206D%20pose%20and%20size.%20Extensive%20experiments%20on%0ACAMERA25%2C%20REAL275%2C%20and%20HouseCat6D%20demonstrate%20that%20INKL-Pose%20achieves%0Astate-of-the-art%20performance%20and%20significantly%20outperforms%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15134v1&entry.124074799=Read"},
{"title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models", "author": "Guo Chen and Zhiqi Li and Shihao Wang and Jindong Jiang and Yicheng Liu and Lidong Lu and De-An Huang and Wonmin Byeon and Matthieu Le and Tuomas Rintamaki and Tyler Poon and Max Ehrlich and Tuomas Rintamaki and Tyler Poon and Tong Lu and Limin Wang and Bryan Catanzaro and Jan Kautz and Andrew Tao and Zhiding Yu and Guilin Liu", "abstract": "  We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.\n", "link": "http://arxiv.org/abs/2504.15271v1", "date": "2025-04-21", "relevancy": 2.8611, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5959}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eagle%202.5%3A%20Boosting%20Long-Context%20Post-Training%20for%20Frontier%0A%20%20Vision-Language%20Models&body=Title%3A%20Eagle%202.5%3A%20Boosting%20Long-Context%20Post-Training%20for%20Frontier%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Guo%20Chen%20and%20Zhiqi%20Li%20and%20Shihao%20Wang%20and%20Jindong%20Jiang%20and%20Yicheng%20Liu%20and%20Lidong%20Lu%20and%20De-An%20Huang%20and%20Wonmin%20Byeon%20and%20Matthieu%20Le%20and%20Tuomas%20Rintamaki%20and%20Tyler%20Poon%20and%20Max%20Ehrlich%20and%20Tuomas%20Rintamaki%20and%20Tyler%20Poon%20and%20Tong%20Lu%20and%20Limin%20Wang%20and%20Bryan%20Catanzaro%20and%20Jan%20Kautz%20and%20Andrew%20Tao%20and%20Zhiding%20Yu%20and%20Guilin%20Liu%0AAbstract%3A%20%20%20We%20introduce%20Eagle%202.5%2C%20a%20family%20of%20frontier%20vision-language%20models%20%28VLMs%29%0Afor%20long-context%20multimodal%20learning.%20Our%20work%20addresses%20the%20challenges%20in%20long%0Avideo%20comprehension%20and%20high-resolution%20image%20understanding%2C%20introducing%20a%0Ageneralist%20framework%20for%20both%20tasks.%20The%20proposed%20training%20framework%0Aincorporates%20Automatic%20Degrade%20Sampling%20and%20Image%20Area%20Preservation%2C%20two%0Atechniques%20that%20preserve%20contextual%20integrity%20and%20visual%20details.%20The%20framework%0Aalso%20includes%20numerous%20efficiency%20optimizations%20in%20the%20pipeline%20for%0Along-context%20data%20training.%20Finally%2C%20we%20propose%20Eagle-Video-110K%2C%20a%20novel%0Adataset%20that%20integrates%20both%20story-level%20and%20clip-level%20annotations%2C%0Afacilitating%20long-video%20understanding.%20Eagle%202.5%20demonstrates%20substantial%0Aimprovements%20on%20long-context%20multimodal%20benchmarks%2C%20providing%20a%20robust%20solution%0Ato%20the%20limitations%20of%20existing%20VLMs.%20Notably%2C%20our%20best%20model%20Eagle%202.5-8B%0Aachieves%2072.4%25%20on%20Video-MME%20with%20512%20input%20frames%2C%20matching%20the%20results%20of%0Atop-tier%20commercial%20model%20such%20as%20GPT-4o%20and%20large-scale%20open-source%20models%0Alike%20Qwen2.5-VL-72B%20and%20InternVL2.5-78B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEagle%25202.5%253A%2520Boosting%2520Long-Context%2520Post-Training%2520for%2520Frontier%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DGuo%2520Chen%2520and%2520Zhiqi%2520Li%2520and%2520Shihao%2520Wang%2520and%2520Jindong%2520Jiang%2520and%2520Yicheng%2520Liu%2520and%2520Lidong%2520Lu%2520and%2520De-An%2520Huang%2520and%2520Wonmin%2520Byeon%2520and%2520Matthieu%2520Le%2520and%2520Tuomas%2520Rintamaki%2520and%2520Tyler%2520Poon%2520and%2520Max%2520Ehrlich%2520and%2520Tuomas%2520Rintamaki%2520and%2520Tyler%2520Poon%2520and%2520Tong%2520Lu%2520and%2520Limin%2520Wang%2520and%2520Bryan%2520Catanzaro%2520and%2520Jan%2520Kautz%2520and%2520Andrew%2520Tao%2520and%2520Zhiding%2520Yu%2520and%2520Guilin%2520Liu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Eagle%25202.5%252C%2520a%2520family%2520of%2520frontier%2520vision-language%2520models%2520%2528VLMs%2529%250Afor%2520long-context%2520multimodal%2520learning.%2520Our%2520work%2520addresses%2520the%2520challenges%2520in%2520long%250Avideo%2520comprehension%2520and%2520high-resolution%2520image%2520understanding%252C%2520introducing%2520a%250Ageneralist%2520framework%2520for%2520both%2520tasks.%2520The%2520proposed%2520training%2520framework%250Aincorporates%2520Automatic%2520Degrade%2520Sampling%2520and%2520Image%2520Area%2520Preservation%252C%2520two%250Atechniques%2520that%2520preserve%2520contextual%2520integrity%2520and%2520visual%2520details.%2520The%2520framework%250Aalso%2520includes%2520numerous%2520efficiency%2520optimizations%2520in%2520the%2520pipeline%2520for%250Along-context%2520data%2520training.%2520Finally%252C%2520we%2520propose%2520Eagle-Video-110K%252C%2520a%2520novel%250Adataset%2520that%2520integrates%2520both%2520story-level%2520and%2520clip-level%2520annotations%252C%250Afacilitating%2520long-video%2520understanding.%2520Eagle%25202.5%2520demonstrates%2520substantial%250Aimprovements%2520on%2520long-context%2520multimodal%2520benchmarks%252C%2520providing%2520a%2520robust%2520solution%250Ato%2520the%2520limitations%2520of%2520existing%2520VLMs.%2520Notably%252C%2520our%2520best%2520model%2520Eagle%25202.5-8B%250Aachieves%252072.4%2525%2520on%2520Video-MME%2520with%2520512%2520input%2520frames%252C%2520matching%2520the%2520results%2520of%250Atop-tier%2520commercial%2520model%2520such%2520as%2520GPT-4o%2520and%2520large-scale%2520open-source%2520models%250Alike%2520Qwen2.5-VL-72B%2520and%2520InternVL2.5-78B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eagle%202.5%3A%20Boosting%20Long-Context%20Post-Training%20for%20Frontier%0A%20%20Vision-Language%20Models&entry.906535625=Guo%20Chen%20and%20Zhiqi%20Li%20and%20Shihao%20Wang%20and%20Jindong%20Jiang%20and%20Yicheng%20Liu%20and%20Lidong%20Lu%20and%20De-An%20Huang%20and%20Wonmin%20Byeon%20and%20Matthieu%20Le%20and%20Tuomas%20Rintamaki%20and%20Tyler%20Poon%20and%20Max%20Ehrlich%20and%20Tuomas%20Rintamaki%20and%20Tyler%20Poon%20and%20Tong%20Lu%20and%20Limin%20Wang%20and%20Bryan%20Catanzaro%20and%20Jan%20Kautz%20and%20Andrew%20Tao%20and%20Zhiding%20Yu%20and%20Guilin%20Liu&entry.1292438233=%20%20We%20introduce%20Eagle%202.5%2C%20a%20family%20of%20frontier%20vision-language%20models%20%28VLMs%29%0Afor%20long-context%20multimodal%20learning.%20Our%20work%20addresses%20the%20challenges%20in%20long%0Avideo%20comprehension%20and%20high-resolution%20image%20understanding%2C%20introducing%20a%0Ageneralist%20framework%20for%20both%20tasks.%20The%20proposed%20training%20framework%0Aincorporates%20Automatic%20Degrade%20Sampling%20and%20Image%20Area%20Preservation%2C%20two%0Atechniques%20that%20preserve%20contextual%20integrity%20and%20visual%20details.%20The%20framework%0Aalso%20includes%20numerous%20efficiency%20optimizations%20in%20the%20pipeline%20for%0Along-context%20data%20training.%20Finally%2C%20we%20propose%20Eagle-Video-110K%2C%20a%20novel%0Adataset%20that%20integrates%20both%20story-level%20and%20clip-level%20annotations%2C%0Afacilitating%20long-video%20understanding.%20Eagle%202.5%20demonstrates%20substantial%0Aimprovements%20on%20long-context%20multimodal%20benchmarks%2C%20providing%20a%20robust%20solution%0Ato%20the%20limitations%20of%20existing%20VLMs.%20Notably%2C%20our%20best%20model%20Eagle%202.5-8B%0Aachieves%2072.4%25%20on%20Video-MME%20with%20512%20input%20frames%2C%20matching%20the%20results%20of%0Atop-tier%20commercial%20model%20such%20as%20GPT-4o%20and%20large-scale%20open-source%20models%0Alike%20Qwen2.5-VL-72B%20and%20InternVL2.5-78B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15271v1&entry.124074799=Read"},
{"title": "A Call for New Recipes to Enhance Spatial Reasoning in MLLMs", "author": "Huanyu Zhang and Chengzu Li and Wenshan Wu and Shaoguang Mao and Yan xia and Ivan Vuli\u0107 and Zhang Zhang and Liang Wang and Tieniu Tan and Furu Wei", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in general vision-language tasks. However, recent studies have\nexposed critical limitations in their spatial reasoning capabilities. This\ndeficiency in spatial reasoning significantly constrains MLLMs' ability to\ninteract effectively with the physical world, thereby limiting their broader\napplications. We argue that spatial reasoning capabilities will not naturally\nemerge from merely scaling existing architectures and training methodologies.\nInstead, this challenge demands dedicated attention to fundamental\nmodifications in the current MLLM development approach. In this position paper,\nwe first establish a comprehensive framework for spatial reasoning within the\ncontext of MLLMs. We then elaborate on its pivotal role in real-world\napplications. Through systematic analysis, we examine how individual components\nof the current methodology-from training data to reasoning mechanisms-influence\nspatial reasoning capabilities. This examination reveals critical limitations\nwhile simultaneously identifying promising avenues for advancement. Our work\naims to direct the AI research community's attention toward these crucial yet\nunderexplored aspects. By highlighting these challenges and opportunities, we\nseek to catalyze progress toward achieving human-like spatial reasoning\ncapabilities in MLLMs.\n", "link": "http://arxiv.org/abs/2504.15037v1", "date": "2025-04-21", "relevancy": 2.7852, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Call%20for%20New%20Recipes%20to%20Enhance%20Spatial%20Reasoning%20in%20MLLMs&body=Title%3A%20A%20Call%20for%20New%20Recipes%20to%20Enhance%20Spatial%20Reasoning%20in%20MLLMs%0AAuthor%3A%20Huanyu%20Zhang%20and%20Chengzu%20Li%20and%20Wenshan%20Wu%20and%20Shaoguang%20Mao%20and%20Yan%20xia%20and%20Ivan%20Vuli%C4%87%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Tieniu%20Tan%20and%20Furu%20Wei%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%0Aperformance%20in%20general%20vision-language%20tasks.%20However%2C%20recent%20studies%20have%0Aexposed%20critical%20limitations%20in%20their%20spatial%20reasoning%20capabilities.%20This%0Adeficiency%20in%20spatial%20reasoning%20significantly%20constrains%20MLLMs%27%20ability%20to%0Ainteract%20effectively%20with%20the%20physical%20world%2C%20thereby%20limiting%20their%20broader%0Aapplications.%20We%20argue%20that%20spatial%20reasoning%20capabilities%20will%20not%20naturally%0Aemerge%20from%20merely%20scaling%20existing%20architectures%20and%20training%20methodologies.%0AInstead%2C%20this%20challenge%20demands%20dedicated%20attention%20to%20fundamental%0Amodifications%20in%20the%20current%20MLLM%20development%20approach.%20In%20this%20position%20paper%2C%0Awe%20first%20establish%20a%20comprehensive%20framework%20for%20spatial%20reasoning%20within%20the%0Acontext%20of%20MLLMs.%20We%20then%20elaborate%20on%20its%20pivotal%20role%20in%20real-world%0Aapplications.%20Through%20systematic%20analysis%2C%20we%20examine%20how%20individual%20components%0Aof%20the%20current%20methodology-from%20training%20data%20to%20reasoning%20mechanisms-influence%0Aspatial%20reasoning%20capabilities.%20This%20examination%20reveals%20critical%20limitations%0Awhile%20simultaneously%20identifying%20promising%20avenues%20for%20advancement.%20Our%20work%0Aaims%20to%20direct%20the%20AI%20research%20community%27s%20attention%20toward%20these%20crucial%20yet%0Aunderexplored%20aspects.%20By%20highlighting%20these%20challenges%20and%20opportunities%2C%20we%0Aseek%20to%20catalyze%20progress%20toward%20achieving%20human-like%20spatial%20reasoning%0Acapabilities%20in%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Call%2520for%2520New%2520Recipes%2520to%2520Enhance%2520Spatial%2520Reasoning%2520in%2520MLLMs%26entry.906535625%3DHuanyu%2520Zhang%2520and%2520Chengzu%2520Li%2520and%2520Wenshan%2520Wu%2520and%2520Shaoguang%2520Mao%2520and%2520Yan%2520xia%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Zhang%2520Zhang%2520and%2520Liang%2520Wang%2520and%2520Tieniu%2520Tan%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520impressive%250Aperformance%2520in%2520general%2520vision-language%2520tasks.%2520However%252C%2520recent%2520studies%2520have%250Aexposed%2520critical%2520limitations%2520in%2520their%2520spatial%2520reasoning%2520capabilities.%2520This%250Adeficiency%2520in%2520spatial%2520reasoning%2520significantly%2520constrains%2520MLLMs%2527%2520ability%2520to%250Ainteract%2520effectively%2520with%2520the%2520physical%2520world%252C%2520thereby%2520limiting%2520their%2520broader%250Aapplications.%2520We%2520argue%2520that%2520spatial%2520reasoning%2520capabilities%2520will%2520not%2520naturally%250Aemerge%2520from%2520merely%2520scaling%2520existing%2520architectures%2520and%2520training%2520methodologies.%250AInstead%252C%2520this%2520challenge%2520demands%2520dedicated%2520attention%2520to%2520fundamental%250Amodifications%2520in%2520the%2520current%2520MLLM%2520development%2520approach.%2520In%2520this%2520position%2520paper%252C%250Awe%2520first%2520establish%2520a%2520comprehensive%2520framework%2520for%2520spatial%2520reasoning%2520within%2520the%250Acontext%2520of%2520MLLMs.%2520We%2520then%2520elaborate%2520on%2520its%2520pivotal%2520role%2520in%2520real-world%250Aapplications.%2520Through%2520systematic%2520analysis%252C%2520we%2520examine%2520how%2520individual%2520components%250Aof%2520the%2520current%2520methodology-from%2520training%2520data%2520to%2520reasoning%2520mechanisms-influence%250Aspatial%2520reasoning%2520capabilities.%2520This%2520examination%2520reveals%2520critical%2520limitations%250Awhile%2520simultaneously%2520identifying%2520promising%2520avenues%2520for%2520advancement.%2520Our%2520work%250Aaims%2520to%2520direct%2520the%2520AI%2520research%2520community%2527s%2520attention%2520toward%2520these%2520crucial%2520yet%250Aunderexplored%2520aspects.%2520By%2520highlighting%2520these%2520challenges%2520and%2520opportunities%252C%2520we%250Aseek%2520to%2520catalyze%2520progress%2520toward%2520achieving%2520human-like%2520spatial%2520reasoning%250Acapabilities%2520in%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Call%20for%20New%20Recipes%20to%20Enhance%20Spatial%20Reasoning%20in%20MLLMs&entry.906535625=Huanyu%20Zhang%20and%20Chengzu%20Li%20and%20Wenshan%20Wu%20and%20Shaoguang%20Mao%20and%20Yan%20xia%20and%20Ivan%20Vuli%C4%87%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Tieniu%20Tan%20and%20Furu%20Wei&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20impressive%0Aperformance%20in%20general%20vision-language%20tasks.%20However%2C%20recent%20studies%20have%0Aexposed%20critical%20limitations%20in%20their%20spatial%20reasoning%20capabilities.%20This%0Adeficiency%20in%20spatial%20reasoning%20significantly%20constrains%20MLLMs%27%20ability%20to%0Ainteract%20effectively%20with%20the%20physical%20world%2C%20thereby%20limiting%20their%20broader%0Aapplications.%20We%20argue%20that%20spatial%20reasoning%20capabilities%20will%20not%20naturally%0Aemerge%20from%20merely%20scaling%20existing%20architectures%20and%20training%20methodologies.%0AInstead%2C%20this%20challenge%20demands%20dedicated%20attention%20to%20fundamental%0Amodifications%20in%20the%20current%20MLLM%20development%20approach.%20In%20this%20position%20paper%2C%0Awe%20first%20establish%20a%20comprehensive%20framework%20for%20spatial%20reasoning%20within%20the%0Acontext%20of%20MLLMs.%20We%20then%20elaborate%20on%20its%20pivotal%20role%20in%20real-world%0Aapplications.%20Through%20systematic%20analysis%2C%20we%20examine%20how%20individual%20components%0Aof%20the%20current%20methodology-from%20training%20data%20to%20reasoning%20mechanisms-influence%0Aspatial%20reasoning%20capabilities.%20This%20examination%20reveals%20critical%20limitations%0Awhile%20simultaneously%20identifying%20promising%20avenues%20for%20advancement.%20Our%20work%0Aaims%20to%20direct%20the%20AI%20research%20community%27s%20attention%20toward%20these%20crucial%20yet%0Aunderexplored%20aspects.%20By%20highlighting%20these%20challenges%20and%20opportunities%2C%20we%0Aseek%20to%20catalyze%20progress%20toward%20achieving%20human-like%20spatial%20reasoning%0Acapabilities%20in%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15037v1&entry.124074799=Read"},
{"title": "Tree of Attributes Prompt Learning for Vision-Language Models", "author": "Tong Ding and Wanhua Li and Zhongqi Miao and Hanspeter Pfister", "abstract": "  Prompt learning has proven effective in adapting vision language models for\ndownstream tasks. However, existing methods usually append learnable prompt\ntokens solely with the category names to obtain textual features, which fails\nto fully leverage the rich context indicated in the category name. To address\nthis issue, we propose the Tree of Attributes Prompt learning (TAP), which\nfirst instructs LLMs to generate a tree of attributes with a \"concept -\nattribute - description\" structure for each category, and then learn the\nhierarchy with vision and text prompt tokens. Unlike existing methods that\nmerely augment category names with a set of unstructured descriptions, our\napproach essentially distills structured knowledge graphs associated with class\nnames from LLMs. Furthermore, our approach introduces text and vision prompts\ndesigned to explicitly learn the corresponding visual attributes, effectively\nserving as domain experts. Additionally, the general and diverse descriptions\ngenerated based on the class names may be wrong or absent in the specific given\nimages. To address this misalignment, we further introduce a vision-conditional\npooling module to extract instance-specific text features. Extensive\nexperimental results demonstrate that our approach outperforms state-of-the-art\nmethods on the zero-shot base-to-novel generalization, cross-dataset transfer,\nas well as few-shot classification across 11 diverse datasets. Code is\navailable at https://github.com/HHenryD/TAP.\n", "link": "http://arxiv.org/abs/2410.11201v2", "date": "2025-04-21", "relevancy": 2.7785, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.607}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.531}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree%20of%20Attributes%20Prompt%20Learning%20for%20Vision-Language%20Models&body=Title%3A%20Tree%20of%20Attributes%20Prompt%20Learning%20for%20Vision-Language%20Models%0AAuthor%3A%20Tong%20Ding%20and%20Wanhua%20Li%20and%20Zhongqi%20Miao%20and%20Hanspeter%20Pfister%0AAbstract%3A%20%20%20Prompt%20learning%20has%20proven%20effective%20in%20adapting%20vision%20language%20models%20for%0Adownstream%20tasks.%20However%2C%20existing%20methods%20usually%20append%20learnable%20prompt%0Atokens%20solely%20with%20the%20category%20names%20to%20obtain%20textual%20features%2C%20which%20fails%0Ato%20fully%20leverage%20the%20rich%20context%20indicated%20in%20the%20category%20name.%20To%20address%0Athis%20issue%2C%20we%20propose%20the%20Tree%20of%20Attributes%20Prompt%20learning%20%28TAP%29%2C%20which%0Afirst%20instructs%20LLMs%20to%20generate%20a%20tree%20of%20attributes%20with%20a%20%22concept%20-%0Aattribute%20-%20description%22%20structure%20for%20each%20category%2C%20and%20then%20learn%20the%0Ahierarchy%20with%20vision%20and%20text%20prompt%20tokens.%20Unlike%20existing%20methods%20that%0Amerely%20augment%20category%20names%20with%20a%20set%20of%20unstructured%20descriptions%2C%20our%0Aapproach%20essentially%20distills%20structured%20knowledge%20graphs%20associated%20with%20class%0Anames%20from%20LLMs.%20Furthermore%2C%20our%20approach%20introduces%20text%20and%20vision%20prompts%0Adesigned%20to%20explicitly%20learn%20the%20corresponding%20visual%20attributes%2C%20effectively%0Aserving%20as%20domain%20experts.%20Additionally%2C%20the%20general%20and%20diverse%20descriptions%0Agenerated%20based%20on%20the%20class%20names%20may%20be%20wrong%20or%20absent%20in%20the%20specific%20given%0Aimages.%20To%20address%20this%20misalignment%2C%20we%20further%20introduce%20a%20vision-conditional%0Apooling%20module%20to%20extract%20instance-specific%20text%20features.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%0Amethods%20on%20the%20zero-shot%20base-to-novel%20generalization%2C%20cross-dataset%20transfer%2C%0Aas%20well%20as%20few-shot%20classification%20across%2011%20diverse%20datasets.%20Code%20is%0Aavailable%20at%20https%3A//github.com/HHenryD/TAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11201v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree%2520of%2520Attributes%2520Prompt%2520Learning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DTong%2520Ding%2520and%2520Wanhua%2520Li%2520and%2520Zhongqi%2520Miao%2520and%2520Hanspeter%2520Pfister%26entry.1292438233%3D%2520%2520Prompt%2520learning%2520has%2520proven%2520effective%2520in%2520adapting%2520vision%2520language%2520models%2520for%250Adownstream%2520tasks.%2520However%252C%2520existing%2520methods%2520usually%2520append%2520learnable%2520prompt%250Atokens%2520solely%2520with%2520the%2520category%2520names%2520to%2520obtain%2520textual%2520features%252C%2520which%2520fails%250Ato%2520fully%2520leverage%2520the%2520rich%2520context%2520indicated%2520in%2520the%2520category%2520name.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520the%2520Tree%2520of%2520Attributes%2520Prompt%2520learning%2520%2528TAP%2529%252C%2520which%250Afirst%2520instructs%2520LLMs%2520to%2520generate%2520a%2520tree%2520of%2520attributes%2520with%2520a%2520%2522concept%2520-%250Aattribute%2520-%2520description%2522%2520structure%2520for%2520each%2520category%252C%2520and%2520then%2520learn%2520the%250Ahierarchy%2520with%2520vision%2520and%2520text%2520prompt%2520tokens.%2520Unlike%2520existing%2520methods%2520that%250Amerely%2520augment%2520category%2520names%2520with%2520a%2520set%2520of%2520unstructured%2520descriptions%252C%2520our%250Aapproach%2520essentially%2520distills%2520structured%2520knowledge%2520graphs%2520associated%2520with%2520class%250Anames%2520from%2520LLMs.%2520Furthermore%252C%2520our%2520approach%2520introduces%2520text%2520and%2520vision%2520prompts%250Adesigned%2520to%2520explicitly%2520learn%2520the%2520corresponding%2520visual%2520attributes%252C%2520effectively%250Aserving%2520as%2520domain%2520experts.%2520Additionally%252C%2520the%2520general%2520and%2520diverse%2520descriptions%250Agenerated%2520based%2520on%2520the%2520class%2520names%2520may%2520be%2520wrong%2520or%2520absent%2520in%2520the%2520specific%2520given%250Aimages.%2520To%2520address%2520this%2520misalignment%252C%2520we%2520further%2520introduce%2520a%2520vision-conditional%250Apooling%2520module%2520to%2520extract%2520instance-specific%2520text%2520features.%2520Extensive%250Aexperimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520state-of-the-art%250Amethods%2520on%2520the%2520zero-shot%2520base-to-novel%2520generalization%252C%2520cross-dataset%2520transfer%252C%250Aas%2520well%2520as%2520few-shot%2520classification%2520across%252011%2520diverse%2520datasets.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/HHenryD/TAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11201v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree%20of%20Attributes%20Prompt%20Learning%20for%20Vision-Language%20Models&entry.906535625=Tong%20Ding%20and%20Wanhua%20Li%20and%20Zhongqi%20Miao%20and%20Hanspeter%20Pfister&entry.1292438233=%20%20Prompt%20learning%20has%20proven%20effective%20in%20adapting%20vision%20language%20models%20for%0Adownstream%20tasks.%20However%2C%20existing%20methods%20usually%20append%20learnable%20prompt%0Atokens%20solely%20with%20the%20category%20names%20to%20obtain%20textual%20features%2C%20which%20fails%0Ato%20fully%20leverage%20the%20rich%20context%20indicated%20in%20the%20category%20name.%20To%20address%0Athis%20issue%2C%20we%20propose%20the%20Tree%20of%20Attributes%20Prompt%20learning%20%28TAP%29%2C%20which%0Afirst%20instructs%20LLMs%20to%20generate%20a%20tree%20of%20attributes%20with%20a%20%22concept%20-%0Aattribute%20-%20description%22%20structure%20for%20each%20category%2C%20and%20then%20learn%20the%0Ahierarchy%20with%20vision%20and%20text%20prompt%20tokens.%20Unlike%20existing%20methods%20that%0Amerely%20augment%20category%20names%20with%20a%20set%20of%20unstructured%20descriptions%2C%20our%0Aapproach%20essentially%20distills%20structured%20knowledge%20graphs%20associated%20with%20class%0Anames%20from%20LLMs.%20Furthermore%2C%20our%20approach%20introduces%20text%20and%20vision%20prompts%0Adesigned%20to%20explicitly%20learn%20the%20corresponding%20visual%20attributes%2C%20effectively%0Aserving%20as%20domain%20experts.%20Additionally%2C%20the%20general%20and%20diverse%20descriptions%0Agenerated%20based%20on%20the%20class%20names%20may%20be%20wrong%20or%20absent%20in%20the%20specific%20given%0Aimages.%20To%20address%20this%20misalignment%2C%20we%20further%20introduce%20a%20vision-conditional%0Apooling%20module%20to%20extract%20instance-specific%20text%20features.%20Extensive%0Aexperimental%20results%20demonstrate%20that%20our%20approach%20outperforms%20state-of-the-art%0Amethods%20on%20the%20zero-shot%20base-to-novel%20generalization%2C%20cross-dataset%20transfer%2C%0Aas%20well%20as%20few-shot%20classification%20across%2011%20diverse%20datasets.%20Code%20is%0Aavailable%20at%20https%3A//github.com/HHenryD/TAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11201v2&entry.124074799=Read"},
{"title": "Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's\n  LLM-CLIP Framework for Image Captioning", "author": "Yassir Benhammou and Alessandro Tiberio and Gabriel Trautmann and Suman Kalyan", "abstract": "  MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models.\n", "link": "http://arxiv.org/abs/2504.15199v1", "date": "2025-04-21", "relevancy": 2.7747, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5615}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%2C%20But%20at%20What%20Cost%3F%20Unveiling%20the%20Hidden%20Overhead%20of%20MILS%27s%0A%20%20LLM-CLIP%20Framework%20for%20Image%20Captioning&body=Title%3A%20Zero-Shot%2C%20But%20at%20What%20Cost%3F%20Unveiling%20the%20Hidden%20Overhead%20of%20MILS%27s%0A%20%20LLM-CLIP%20Framework%20for%20Image%20Captioning%0AAuthor%3A%20Yassir%20Benhammou%20and%20Alessandro%20Tiberio%20and%20Gabriel%20Trautmann%20and%20Suman%20Kalyan%0AAbstract%3A%20%20%20MILS%20%28Multimodal%20Iterative%20LLM%20Solver%29%20is%20a%20recently%20published%20framework%20that%0Aclaims%20%22LLMs%20can%20see%20and%20hear%20without%20any%20training%22%20by%20leveraging%20an%20iterative%2C%0ALLM-CLIP%20based%20approach%20for%20zero-shot%20image%20captioning.%20While%20this%20MILS%0Aapproach%20demonstrates%20good%20performance%2C%20our%20investigation%20reveals%20that%20this%0Asuccess%20comes%20at%20a%20hidden%2C%20substantial%20computational%20cost%20due%20to%20its%20expensive%0Amulti-step%20refinement%20process.%20In%20contrast%2C%20alternative%20models%20such%20as%20BLIP-2%0Aand%20GPT-4V%20achieve%20competitive%20results%20through%20a%20streamlined%2C%20single-pass%0Aapproach.%20We%20hypothesize%20that%20the%20significant%20overhead%20inherent%20in%20MILS%27s%0Aiterative%20process%20may%20undermine%20its%20practical%20benefits%2C%20thereby%20challenging%20the%0Anarrative%20that%20zero-shot%20performance%20can%20be%20attained%20without%20incurring%20heavy%0Aresource%20demands.%20This%20work%20is%20the%20first%20to%20expose%20and%20quantify%20the%20trade-offs%0Abetween%20output%20quality%20and%20computational%20cost%20in%20MILS%2C%20providing%20critical%0Ainsights%20for%20the%20design%20of%20more%20efficient%20multimodal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%252C%2520But%2520at%2520What%2520Cost%253F%2520Unveiling%2520the%2520Hidden%2520Overhead%2520of%2520MILS%2527s%250A%2520%2520LLM-CLIP%2520Framework%2520for%2520Image%2520Captioning%26entry.906535625%3DYassir%2520Benhammou%2520and%2520Alessandro%2520Tiberio%2520and%2520Gabriel%2520Trautmann%2520and%2520Suman%2520Kalyan%26entry.1292438233%3D%2520%2520MILS%2520%2528Multimodal%2520Iterative%2520LLM%2520Solver%2529%2520is%2520a%2520recently%2520published%2520framework%2520that%250Aclaims%2520%2522LLMs%2520can%2520see%2520and%2520hear%2520without%2520any%2520training%2522%2520by%2520leveraging%2520an%2520iterative%252C%250ALLM-CLIP%2520based%2520approach%2520for%2520zero-shot%2520image%2520captioning.%2520While%2520this%2520MILS%250Aapproach%2520demonstrates%2520good%2520performance%252C%2520our%2520investigation%2520reveals%2520that%2520this%250Asuccess%2520comes%2520at%2520a%2520hidden%252C%2520substantial%2520computational%2520cost%2520due%2520to%2520its%2520expensive%250Amulti-step%2520refinement%2520process.%2520In%2520contrast%252C%2520alternative%2520models%2520such%2520as%2520BLIP-2%250Aand%2520GPT-4V%2520achieve%2520competitive%2520results%2520through%2520a%2520streamlined%252C%2520single-pass%250Aapproach.%2520We%2520hypothesize%2520that%2520the%2520significant%2520overhead%2520inherent%2520in%2520MILS%2527s%250Aiterative%2520process%2520may%2520undermine%2520its%2520practical%2520benefits%252C%2520thereby%2520challenging%2520the%250Anarrative%2520that%2520zero-shot%2520performance%2520can%2520be%2520attained%2520without%2520incurring%2520heavy%250Aresource%2520demands.%2520This%2520work%2520is%2520the%2520first%2520to%2520expose%2520and%2520quantify%2520the%2520trade-offs%250Abetween%2520output%2520quality%2520and%2520computational%2520cost%2520in%2520MILS%252C%2520providing%2520critical%250Ainsights%2520for%2520the%2520design%2520of%2520more%2520efficient%2520multimodal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%2C%20But%20at%20What%20Cost%3F%20Unveiling%20the%20Hidden%20Overhead%20of%20MILS%27s%0A%20%20LLM-CLIP%20Framework%20for%20Image%20Captioning&entry.906535625=Yassir%20Benhammou%20and%20Alessandro%20Tiberio%20and%20Gabriel%20Trautmann%20and%20Suman%20Kalyan&entry.1292438233=%20%20MILS%20%28Multimodal%20Iterative%20LLM%20Solver%29%20is%20a%20recently%20published%20framework%20that%0Aclaims%20%22LLMs%20can%20see%20and%20hear%20without%20any%20training%22%20by%20leveraging%20an%20iterative%2C%0ALLM-CLIP%20based%20approach%20for%20zero-shot%20image%20captioning.%20While%20this%20MILS%0Aapproach%20demonstrates%20good%20performance%2C%20our%20investigation%20reveals%20that%20this%0Asuccess%20comes%20at%20a%20hidden%2C%20substantial%20computational%20cost%20due%20to%20its%20expensive%0Amulti-step%20refinement%20process.%20In%20contrast%2C%20alternative%20models%20such%20as%20BLIP-2%0Aand%20GPT-4V%20achieve%20competitive%20results%20through%20a%20streamlined%2C%20single-pass%0Aapproach.%20We%20hypothesize%20that%20the%20significant%20overhead%20inherent%20in%20MILS%27s%0Aiterative%20process%20may%20undermine%20its%20practical%20benefits%2C%20thereby%20challenging%20the%0Anarrative%20that%20zero-shot%20performance%20can%20be%20attained%20without%20incurring%20heavy%0Aresource%20demands.%20This%20work%20is%20the%20first%20to%20expose%20and%20quantify%20the%20trade-offs%0Abetween%20output%20quality%20and%20computational%20cost%20in%20MILS%2C%20providing%20critical%0Ainsights%20for%20the%20design%20of%20more%20efficient%20multimodal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15199v1&entry.124074799=Read"},
{"title": "Robust and Real-time Surface Normal Estimation from Stereo Disparities\n  using Affine Transformations", "author": "Csongor Csanad Kariko and Muhammad Rafi Faisal and Levente Hajder", "abstract": "  This work introduces a novel method for surface normal estimation from\nrectified stereo image pairs, leveraging affine transformations derived from\ndisparity values to achieve fast and accurate results. We demonstrate how the\nrectification of stereo image pairs simplifies the process of surface normal\nestimation by reducing computational complexity. To address noise reduction, we\ndevelop a custom algorithm inspired by convolutional operations, tailored to\nprocess disparity data efficiently. We also introduce adaptive heuristic\ntechniques for efficiently detecting connected surface components within the\nimages, further improving the robustness of the method. By integrating these\nmethods, we construct a surface normal estimator that is both fast and\naccurate, producing a dense, oriented point cloud as the final output. Our\nmethod is validated using both simulated environments and real-world stereo\nimages from the Middlebury and Cityscapes datasets, demonstrating significant\nimprovements in real-time performance and accuracy when implemented on a GPU.\nUpon acceptance, the shader source code will be made publicly available to\nfacilitate further research and reproducibility.\n", "link": "http://arxiv.org/abs/2504.15121v1", "date": "2025-04-21", "relevancy": 2.7693, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5756}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5491}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20and%20Real-time%20Surface%20Normal%20Estimation%20from%20Stereo%20Disparities%0A%20%20using%20Affine%20Transformations&body=Title%3A%20Robust%20and%20Real-time%20Surface%20Normal%20Estimation%20from%20Stereo%20Disparities%0A%20%20using%20Affine%20Transformations%0AAuthor%3A%20Csongor%20Csanad%20Kariko%20and%20Muhammad%20Rafi%20Faisal%20and%20Levente%20Hajder%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20novel%20method%20for%20surface%20normal%20estimation%20from%0Arectified%20stereo%20image%20pairs%2C%20leveraging%20affine%20transformations%20derived%20from%0Adisparity%20values%20to%20achieve%20fast%20and%20accurate%20results.%20We%20demonstrate%20how%20the%0Arectification%20of%20stereo%20image%20pairs%20simplifies%20the%20process%20of%20surface%20normal%0Aestimation%20by%20reducing%20computational%20complexity.%20To%20address%20noise%20reduction%2C%20we%0Adevelop%20a%20custom%20algorithm%20inspired%20by%20convolutional%20operations%2C%20tailored%20to%0Aprocess%20disparity%20data%20efficiently.%20We%20also%20introduce%20adaptive%20heuristic%0Atechniques%20for%20efficiently%20detecting%20connected%20surface%20components%20within%20the%0Aimages%2C%20further%20improving%20the%20robustness%20of%20the%20method.%20By%20integrating%20these%0Amethods%2C%20we%20construct%20a%20surface%20normal%20estimator%20that%20is%20both%20fast%20and%0Aaccurate%2C%20producing%20a%20dense%2C%20oriented%20point%20cloud%20as%20the%20final%20output.%20Our%0Amethod%20is%20validated%20using%20both%20simulated%20environments%20and%20real-world%20stereo%0Aimages%20from%20the%20Middlebury%20and%20Cityscapes%20datasets%2C%20demonstrating%20significant%0Aimprovements%20in%20real-time%20performance%20and%20accuracy%20when%20implemented%20on%20a%20GPU.%0AUpon%20acceptance%2C%20the%20shader%20source%20code%20will%20be%20made%20publicly%20available%20to%0Afacilitate%20further%20research%20and%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520and%2520Real-time%2520Surface%2520Normal%2520Estimation%2520from%2520Stereo%2520Disparities%250A%2520%2520using%2520Affine%2520Transformations%26entry.906535625%3DCsongor%2520Csanad%2520Kariko%2520and%2520Muhammad%2520Rafi%2520Faisal%2520and%2520Levente%2520Hajder%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520novel%2520method%2520for%2520surface%2520normal%2520estimation%2520from%250Arectified%2520stereo%2520image%2520pairs%252C%2520leveraging%2520affine%2520transformations%2520derived%2520from%250Adisparity%2520values%2520to%2520achieve%2520fast%2520and%2520accurate%2520results.%2520We%2520demonstrate%2520how%2520the%250Arectification%2520of%2520stereo%2520image%2520pairs%2520simplifies%2520the%2520process%2520of%2520surface%2520normal%250Aestimation%2520by%2520reducing%2520computational%2520complexity.%2520To%2520address%2520noise%2520reduction%252C%2520we%250Adevelop%2520a%2520custom%2520algorithm%2520inspired%2520by%2520convolutional%2520operations%252C%2520tailored%2520to%250Aprocess%2520disparity%2520data%2520efficiently.%2520We%2520also%2520introduce%2520adaptive%2520heuristic%250Atechniques%2520for%2520efficiently%2520detecting%2520connected%2520surface%2520components%2520within%2520the%250Aimages%252C%2520further%2520improving%2520the%2520robustness%2520of%2520the%2520method.%2520By%2520integrating%2520these%250Amethods%252C%2520we%2520construct%2520a%2520surface%2520normal%2520estimator%2520that%2520is%2520both%2520fast%2520and%250Aaccurate%252C%2520producing%2520a%2520dense%252C%2520oriented%2520point%2520cloud%2520as%2520the%2520final%2520output.%2520Our%250Amethod%2520is%2520validated%2520using%2520both%2520simulated%2520environments%2520and%2520real-world%2520stereo%250Aimages%2520from%2520the%2520Middlebury%2520and%2520Cityscapes%2520datasets%252C%2520demonstrating%2520significant%250Aimprovements%2520in%2520real-time%2520performance%2520and%2520accuracy%2520when%2520implemented%2520on%2520a%2520GPU.%250AUpon%2520acceptance%252C%2520the%2520shader%2520source%2520code%2520will%2520be%2520made%2520publicly%2520available%2520to%250Afacilitate%2520further%2520research%2520and%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20and%20Real-time%20Surface%20Normal%20Estimation%20from%20Stereo%20Disparities%0A%20%20using%20Affine%20Transformations&entry.906535625=Csongor%20Csanad%20Kariko%20and%20Muhammad%20Rafi%20Faisal%20and%20Levente%20Hajder&entry.1292438233=%20%20This%20work%20introduces%20a%20novel%20method%20for%20surface%20normal%20estimation%20from%0Arectified%20stereo%20image%20pairs%2C%20leveraging%20affine%20transformations%20derived%20from%0Adisparity%20values%20to%20achieve%20fast%20and%20accurate%20results.%20We%20demonstrate%20how%20the%0Arectification%20of%20stereo%20image%20pairs%20simplifies%20the%20process%20of%20surface%20normal%0Aestimation%20by%20reducing%20computational%20complexity.%20To%20address%20noise%20reduction%2C%20we%0Adevelop%20a%20custom%20algorithm%20inspired%20by%20convolutional%20operations%2C%20tailored%20to%0Aprocess%20disparity%20data%20efficiently.%20We%20also%20introduce%20adaptive%20heuristic%0Atechniques%20for%20efficiently%20detecting%20connected%20surface%20components%20within%20the%0Aimages%2C%20further%20improving%20the%20robustness%20of%20the%20method.%20By%20integrating%20these%0Amethods%2C%20we%20construct%20a%20surface%20normal%20estimator%20that%20is%20both%20fast%20and%0Aaccurate%2C%20producing%20a%20dense%2C%20oriented%20point%20cloud%20as%20the%20final%20output.%20Our%0Amethod%20is%20validated%20using%20both%20simulated%20environments%20and%20real-world%20stereo%0Aimages%20from%20the%20Middlebury%20and%20Cityscapes%20datasets%2C%20demonstrating%20significant%0Aimprovements%20in%20real-time%20performance%20and%20accuracy%20when%20implemented%20on%20a%20GPU.%0AUpon%20acceptance%2C%20the%20shader%20source%20code%20will%20be%20made%20publicly%20available%20to%0Afacilitate%20further%20research%20and%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15121v1&entry.124074799=Read"},
{"title": "Explorable INR: An Implicit Neural Representation for Ensemble\n  Simulation Enabling Efficient Spatial and Parameter Exploration", "author": "Yi-Tang Chen and Haoyu Li and Neng Shi and Xihaier Luo and Wei Xu and Han-Wei Shen", "abstract": "  With the growing computational power available for high-resolution ensemble\nsimulations in scientific fields such as cosmology and oceanology, storage and\ncomputational demands present significant challenges. Current surrogate models\nfall short in the flexibility of point- or region-based predictions as the\nentire field reconstruction is required for each parameter setting, hence\nhindering the efficiency of parameter space exploration. Limitations exist in\ncapturing physical attribute distributions and pinpointing optimal parameter\nconfigurations. In this work, we propose Explorable INR, a novel implicit\nneural representation-based surrogate model, designed to facilitate exploration\nand allow point-based spatial queries without computing full-scale field data.\nIn addition, to further address computational bottlenecks of spatial\nexploration, we utilize probabilistic affine forms (PAFs) for uncertainty\npropagation through Explorable INR to obtain statistical summaries,\nfacilitating various ensemble analysis and visualization tasks that are\nexpensive with existing models. Furthermore, we reformulate the parameter\nexploration problem as optimization tasks using gradient descent and KL\ndivergence minimization that ensures scalability. We demonstrate that the\nExplorable INR with the proposed approach for spatial and parameter exploration\ncan significantly reduce computation and memory costs while providing effective\nensemble analysis.\n", "link": "http://arxiv.org/abs/2504.00904v2", "date": "2025-04-21", "relevancy": 2.7216, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5527}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5507}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explorable%20INR%3A%20An%20Implicit%20Neural%20Representation%20for%20Ensemble%0A%20%20Simulation%20Enabling%20Efficient%20Spatial%20and%20Parameter%20Exploration&body=Title%3A%20Explorable%20INR%3A%20An%20Implicit%20Neural%20Representation%20for%20Ensemble%0A%20%20Simulation%20Enabling%20Efficient%20Spatial%20and%20Parameter%20Exploration%0AAuthor%3A%20Yi-Tang%20Chen%20and%20Haoyu%20Li%20and%20Neng%20Shi%20and%20Xihaier%20Luo%20and%20Wei%20Xu%20and%20Han-Wei%20Shen%0AAbstract%3A%20%20%20With%20the%20growing%20computational%20power%20available%20for%20high-resolution%20ensemble%0Asimulations%20in%20scientific%20fields%20such%20as%20cosmology%20and%20oceanology%2C%20storage%20and%0Acomputational%20demands%20present%20significant%20challenges.%20Current%20surrogate%20models%0Afall%20short%20in%20the%20flexibility%20of%20point-%20or%20region-based%20predictions%20as%20the%0Aentire%20field%20reconstruction%20is%20required%20for%20each%20parameter%20setting%2C%20hence%0Ahindering%20the%20efficiency%20of%20parameter%20space%20exploration.%20Limitations%20exist%20in%0Acapturing%20physical%20attribute%20distributions%20and%20pinpointing%20optimal%20parameter%0Aconfigurations.%20In%20this%20work%2C%20we%20propose%20Explorable%20INR%2C%20a%20novel%20implicit%0Aneural%20representation-based%20surrogate%20model%2C%20designed%20to%20facilitate%20exploration%0Aand%20allow%20point-based%20spatial%20queries%20without%20computing%20full-scale%20field%20data.%0AIn%20addition%2C%20to%20further%20address%20computational%20bottlenecks%20of%20spatial%0Aexploration%2C%20we%20utilize%20probabilistic%20affine%20forms%20%28PAFs%29%20for%20uncertainty%0Apropagation%20through%20Explorable%20INR%20to%20obtain%20statistical%20summaries%2C%0Afacilitating%20various%20ensemble%20analysis%20and%20visualization%20tasks%20that%20are%0Aexpensive%20with%20existing%20models.%20Furthermore%2C%20we%20reformulate%20the%20parameter%0Aexploration%20problem%20as%20optimization%20tasks%20using%20gradient%20descent%20and%20KL%0Adivergence%20minimization%20that%20ensures%20scalability.%20We%20demonstrate%20that%20the%0AExplorable%20INR%20with%20the%20proposed%20approach%20for%20spatial%20and%20parameter%20exploration%0Acan%20significantly%20reduce%20computation%20and%20memory%20costs%20while%20providing%20effective%0Aensemble%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.00904v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplorable%2520INR%253A%2520An%2520Implicit%2520Neural%2520Representation%2520for%2520Ensemble%250A%2520%2520Simulation%2520Enabling%2520Efficient%2520Spatial%2520and%2520Parameter%2520Exploration%26entry.906535625%3DYi-Tang%2520Chen%2520and%2520Haoyu%2520Li%2520and%2520Neng%2520Shi%2520and%2520Xihaier%2520Luo%2520and%2520Wei%2520Xu%2520and%2520Han-Wei%2520Shen%26entry.1292438233%3D%2520%2520With%2520the%2520growing%2520computational%2520power%2520available%2520for%2520high-resolution%2520ensemble%250Asimulations%2520in%2520scientific%2520fields%2520such%2520as%2520cosmology%2520and%2520oceanology%252C%2520storage%2520and%250Acomputational%2520demands%2520present%2520significant%2520challenges.%2520Current%2520surrogate%2520models%250Afall%2520short%2520in%2520the%2520flexibility%2520of%2520point-%2520or%2520region-based%2520predictions%2520as%2520the%250Aentire%2520field%2520reconstruction%2520is%2520required%2520for%2520each%2520parameter%2520setting%252C%2520hence%250Ahindering%2520the%2520efficiency%2520of%2520parameter%2520space%2520exploration.%2520Limitations%2520exist%2520in%250Acapturing%2520physical%2520attribute%2520distributions%2520and%2520pinpointing%2520optimal%2520parameter%250Aconfigurations.%2520In%2520this%2520work%252C%2520we%2520propose%2520Explorable%2520INR%252C%2520a%2520novel%2520implicit%250Aneural%2520representation-based%2520surrogate%2520model%252C%2520designed%2520to%2520facilitate%2520exploration%250Aand%2520allow%2520point-based%2520spatial%2520queries%2520without%2520computing%2520full-scale%2520field%2520data.%250AIn%2520addition%252C%2520to%2520further%2520address%2520computational%2520bottlenecks%2520of%2520spatial%250Aexploration%252C%2520we%2520utilize%2520probabilistic%2520affine%2520forms%2520%2528PAFs%2529%2520for%2520uncertainty%250Apropagation%2520through%2520Explorable%2520INR%2520to%2520obtain%2520statistical%2520summaries%252C%250Afacilitating%2520various%2520ensemble%2520analysis%2520and%2520visualization%2520tasks%2520that%2520are%250Aexpensive%2520with%2520existing%2520models.%2520Furthermore%252C%2520we%2520reformulate%2520the%2520parameter%250Aexploration%2520problem%2520as%2520optimization%2520tasks%2520using%2520gradient%2520descent%2520and%2520KL%250Adivergence%2520minimization%2520that%2520ensures%2520scalability.%2520We%2520demonstrate%2520that%2520the%250AExplorable%2520INR%2520with%2520the%2520proposed%2520approach%2520for%2520spatial%2520and%2520parameter%2520exploration%250Acan%2520significantly%2520reduce%2520computation%2520and%2520memory%2520costs%2520while%2520providing%2520effective%250Aensemble%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.00904v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explorable%20INR%3A%20An%20Implicit%20Neural%20Representation%20for%20Ensemble%0A%20%20Simulation%20Enabling%20Efficient%20Spatial%20and%20Parameter%20Exploration&entry.906535625=Yi-Tang%20Chen%20and%20Haoyu%20Li%20and%20Neng%20Shi%20and%20Xihaier%20Luo%20and%20Wei%20Xu%20and%20Han-Wei%20Shen&entry.1292438233=%20%20With%20the%20growing%20computational%20power%20available%20for%20high-resolution%20ensemble%0Asimulations%20in%20scientific%20fields%20such%20as%20cosmology%20and%20oceanology%2C%20storage%20and%0Acomputational%20demands%20present%20significant%20challenges.%20Current%20surrogate%20models%0Afall%20short%20in%20the%20flexibility%20of%20point-%20or%20region-based%20predictions%20as%20the%0Aentire%20field%20reconstruction%20is%20required%20for%20each%20parameter%20setting%2C%20hence%0Ahindering%20the%20efficiency%20of%20parameter%20space%20exploration.%20Limitations%20exist%20in%0Acapturing%20physical%20attribute%20distributions%20and%20pinpointing%20optimal%20parameter%0Aconfigurations.%20In%20this%20work%2C%20we%20propose%20Explorable%20INR%2C%20a%20novel%20implicit%0Aneural%20representation-based%20surrogate%20model%2C%20designed%20to%20facilitate%20exploration%0Aand%20allow%20point-based%20spatial%20queries%20without%20computing%20full-scale%20field%20data.%0AIn%20addition%2C%20to%20further%20address%20computational%20bottlenecks%20of%20spatial%0Aexploration%2C%20we%20utilize%20probabilistic%20affine%20forms%20%28PAFs%29%20for%20uncertainty%0Apropagation%20through%20Explorable%20INR%20to%20obtain%20statistical%20summaries%2C%0Afacilitating%20various%20ensemble%20analysis%20and%20visualization%20tasks%20that%20are%0Aexpensive%20with%20existing%20models.%20Furthermore%2C%20we%20reformulate%20the%20parameter%0Aexploration%20problem%20as%20optimization%20tasks%20using%20gradient%20descent%20and%20KL%0Adivergence%20minimization%20that%20ensures%20scalability.%20We%20demonstrate%20that%20the%0AExplorable%20INR%20with%20the%20proposed%20approach%20for%20spatial%20and%20parameter%20exploration%0Acan%20significantly%20reduce%20computation%20and%20memory%20costs%20while%20providing%20effective%0Aensemble%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.00904v2&entry.124074799=Read"},
{"title": "DataComp-LM: In search of the next generation of training sets for\n  language models", "author": "Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei Koh and Jenia Jitsev and Thomas Kollar and Alexandros G. Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar", "abstract": "  We introduce DataComp for Language Models (DCLM), a testbed for controlled\ndataset experiments with the goal of improving language models. As part of\nDCLM, we provide a standardized corpus of 240T tokens extracted from Common\nCrawl, effective pretraining recipes based on the OpenLM framework, and a broad\nsuite of 53 downstream evaluations. Participants in the DCLM benchmark can\nexperiment with data curation strategies such as deduplication, filtering, and\ndata mixing at model scales ranging from 412M to 7B parameters. As a baseline\nfor DCLM, we conduct extensive experiments and find that model-based filtering\nis key to assembling a high-quality training set. The resulting dataset,\nDCLM-Baseline enables training a 7B parameter language model from scratch to\n64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the\nprevious state-of-the-art in open-data language models, DCLM-Baseline\nrepresents a 6.6 percentage point improvement on MMLU while being trained with\n40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and\nLlama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53\nnatural language understanding tasks while being trained with 6.6x less compute\nthan Llama 3 8B. Our results highlight the importance of dataset design for\ntraining language models and offer a starting point for further research on\ndata curation.\n", "link": "http://arxiv.org/abs/2406.11794v4", "date": "2025-04-21", "relevancy": 2.692, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataComp-LM%3A%20In%20search%20of%20the%20next%20generation%20of%20training%20sets%20for%0A%20%20language%20models&body=Title%3A%20DataComp-LM%3A%20In%20search%20of%20the%20next%20generation%20of%20training%20sets%20for%0A%20%20language%20models%0AAuthor%3A%20Jeffrey%20Li%20and%20Alex%20Fang%20and%20Georgios%20Smyrnis%20and%20Maor%20Ivgi%20and%20Matt%20Jordan%20and%20Samir%20Gadre%20and%20Hritik%20Bansal%20and%20Etash%20Guha%20and%20Sedrick%20Keh%20and%20Kushal%20Arora%20and%20Saurabh%20Garg%20and%20Rui%20Xin%20and%20Niklas%20Muennighoff%20and%20Reinhard%20Heckel%20and%20Jean%20Mercat%20and%20Mayee%20Chen%20and%20Suchin%20Gururangan%20and%20Mitchell%20Wortsman%20and%20Alon%20Albalak%20and%20Yonatan%20Bitton%20and%20Marianna%20Nezhurina%20and%20Amro%20Abbas%20and%20Cheng-Yu%20Hsieh%20and%20Dhruba%20Ghosh%20and%20Josh%20Gardner%20and%20Maciej%20Kilian%20and%20Hanlin%20Zhang%20and%20Rulin%20Shao%20and%20Sarah%20Pratt%20and%20Sunny%20Sanyal%20and%20Gabriel%20Ilharco%20and%20Giannis%20Daras%20and%20Kalyani%20Marathe%20and%20Aaron%20Gokaslan%20and%20Jieyu%20Zhang%20and%20Khyathi%20Chandu%20and%20Thao%20Nguyen%20and%20Igor%20Vasiljevic%20and%20Sham%20Kakade%20and%20Shuran%20Song%20and%20Sujay%20Sanghavi%20and%20Fartash%20Faghri%20and%20Sewoong%20Oh%20and%20Luke%20Zettlemoyer%20and%20Kyle%20Lo%20and%20Alaaeldin%20El-Nouby%20and%20Hadi%20Pouransari%20and%20Alexander%20Toshev%20and%20Stephanie%20Wang%20and%20Dirk%20Groeneveld%20and%20Luca%20Soldaini%20and%20Pang%20Wei%20Koh%20and%20Jenia%20Jitsev%20and%20Thomas%20Kollar%20and%20Alexandros%20G.%20Dimakis%20and%20Yair%20Carmon%20and%20Achal%20Dave%20and%20Ludwig%20Schmidt%20and%20Vaishaal%20Shankar%0AAbstract%3A%20%20%20We%20introduce%20DataComp%20for%20Language%20Models%20%28DCLM%29%2C%20a%20testbed%20for%20controlled%0Adataset%20experiments%20with%20the%20goal%20of%20improving%20language%20models.%20As%20part%20of%0ADCLM%2C%20we%20provide%20a%20standardized%20corpus%20of%20240T%20tokens%20extracted%20from%20Common%0ACrawl%2C%20effective%20pretraining%20recipes%20based%20on%20the%20OpenLM%20framework%2C%20and%20a%20broad%0Asuite%20of%2053%20downstream%20evaluations.%20Participants%20in%20the%20DCLM%20benchmark%20can%0Aexperiment%20with%20data%20curation%20strategies%20such%20as%20deduplication%2C%20filtering%2C%20and%0Adata%20mixing%20at%20model%20scales%20ranging%20from%20412M%20to%207B%20parameters.%20As%20a%20baseline%0Afor%20DCLM%2C%20we%20conduct%20extensive%20experiments%20and%20find%20that%20model-based%20filtering%0Ais%20key%20to%20assembling%20a%20high-quality%20training%20set.%20The%20resulting%20dataset%2C%0ADCLM-Baseline%20enables%20training%20a%207B%20parameter%20language%20model%20from%20scratch%20to%0A64%25%205-shot%20accuracy%20on%20MMLU%20with%202.6T%20training%20tokens.%20Compared%20to%20MAP-Neo%2C%20the%0Aprevious%20state-of-the-art%20in%20open-data%20language%20models%2C%20DCLM-Baseline%0Arepresents%20a%206.6%20percentage%20point%20improvement%20on%20MMLU%20while%20being%20trained%20with%0A40%25%20less%20compute.%20Our%20baseline%20model%20is%20also%20comparable%20to%20Mistral-7B-v0.3%20and%0ALlama%203%208B%20on%20MMLU%20%2863%25%20%26%2066%25%29%2C%20and%20performs%20similarly%20on%20an%20average%20of%2053%0Anatural%20language%20understanding%20tasks%20while%20being%20trained%20with%206.6x%20less%20compute%0Athan%20Llama%203%208B.%20Our%20results%20highlight%20the%20importance%20of%20dataset%20design%20for%0Atraining%20language%20models%20and%20offer%20a%20starting%20point%20for%20further%20research%20on%0Adata%20curation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11794v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataComp-LM%253A%2520In%2520search%2520of%2520the%2520next%2520generation%2520of%2520training%2520sets%2520for%250A%2520%2520language%2520models%26entry.906535625%3DJeffrey%2520Li%2520and%2520Alex%2520Fang%2520and%2520Georgios%2520Smyrnis%2520and%2520Maor%2520Ivgi%2520and%2520Matt%2520Jordan%2520and%2520Samir%2520Gadre%2520and%2520Hritik%2520Bansal%2520and%2520Etash%2520Guha%2520and%2520Sedrick%2520Keh%2520and%2520Kushal%2520Arora%2520and%2520Saurabh%2520Garg%2520and%2520Rui%2520Xin%2520and%2520Niklas%2520Muennighoff%2520and%2520Reinhard%2520Heckel%2520and%2520Jean%2520Mercat%2520and%2520Mayee%2520Chen%2520and%2520Suchin%2520Gururangan%2520and%2520Mitchell%2520Wortsman%2520and%2520Alon%2520Albalak%2520and%2520Yonatan%2520Bitton%2520and%2520Marianna%2520Nezhurina%2520and%2520Amro%2520Abbas%2520and%2520Cheng-Yu%2520Hsieh%2520and%2520Dhruba%2520Ghosh%2520and%2520Josh%2520Gardner%2520and%2520Maciej%2520Kilian%2520and%2520Hanlin%2520Zhang%2520and%2520Rulin%2520Shao%2520and%2520Sarah%2520Pratt%2520and%2520Sunny%2520Sanyal%2520and%2520Gabriel%2520Ilharco%2520and%2520Giannis%2520Daras%2520and%2520Kalyani%2520Marathe%2520and%2520Aaron%2520Gokaslan%2520and%2520Jieyu%2520Zhang%2520and%2520Khyathi%2520Chandu%2520and%2520Thao%2520Nguyen%2520and%2520Igor%2520Vasiljevic%2520and%2520Sham%2520Kakade%2520and%2520Shuran%2520Song%2520and%2520Sujay%2520Sanghavi%2520and%2520Fartash%2520Faghri%2520and%2520Sewoong%2520Oh%2520and%2520Luke%2520Zettlemoyer%2520and%2520Kyle%2520Lo%2520and%2520Alaaeldin%2520El-Nouby%2520and%2520Hadi%2520Pouransari%2520and%2520Alexander%2520Toshev%2520and%2520Stephanie%2520Wang%2520and%2520Dirk%2520Groeneveld%2520and%2520Luca%2520Soldaini%2520and%2520Pang%2520Wei%2520Koh%2520and%2520Jenia%2520Jitsev%2520and%2520Thomas%2520Kollar%2520and%2520Alexandros%2520G.%2520Dimakis%2520and%2520Yair%2520Carmon%2520and%2520Achal%2520Dave%2520and%2520Ludwig%2520Schmidt%2520and%2520Vaishaal%2520Shankar%26entry.1292438233%3D%2520%2520We%2520introduce%2520DataComp%2520for%2520Language%2520Models%2520%2528DCLM%2529%252C%2520a%2520testbed%2520for%2520controlled%250Adataset%2520experiments%2520with%2520the%2520goal%2520of%2520improving%2520language%2520models.%2520As%2520part%2520of%250ADCLM%252C%2520we%2520provide%2520a%2520standardized%2520corpus%2520of%2520240T%2520tokens%2520extracted%2520from%2520Common%250ACrawl%252C%2520effective%2520pretraining%2520recipes%2520based%2520on%2520the%2520OpenLM%2520framework%252C%2520and%2520a%2520broad%250Asuite%2520of%252053%2520downstream%2520evaluations.%2520Participants%2520in%2520the%2520DCLM%2520benchmark%2520can%250Aexperiment%2520with%2520data%2520curation%2520strategies%2520such%2520as%2520deduplication%252C%2520filtering%252C%2520and%250Adata%2520mixing%2520at%2520model%2520scales%2520ranging%2520from%2520412M%2520to%25207B%2520parameters.%2520As%2520a%2520baseline%250Afor%2520DCLM%252C%2520we%2520conduct%2520extensive%2520experiments%2520and%2520find%2520that%2520model-based%2520filtering%250Ais%2520key%2520to%2520assembling%2520a%2520high-quality%2520training%2520set.%2520The%2520resulting%2520dataset%252C%250ADCLM-Baseline%2520enables%2520training%2520a%25207B%2520parameter%2520language%2520model%2520from%2520scratch%2520to%250A64%2525%25205-shot%2520accuracy%2520on%2520MMLU%2520with%25202.6T%2520training%2520tokens.%2520Compared%2520to%2520MAP-Neo%252C%2520the%250Aprevious%2520state-of-the-art%2520in%2520open-data%2520language%2520models%252C%2520DCLM-Baseline%250Arepresents%2520a%25206.6%2520percentage%2520point%2520improvement%2520on%2520MMLU%2520while%2520being%2520trained%2520with%250A40%2525%2520less%2520compute.%2520Our%2520baseline%2520model%2520is%2520also%2520comparable%2520to%2520Mistral-7B-v0.3%2520and%250ALlama%25203%25208B%2520on%2520MMLU%2520%252863%2525%2520%2526%252066%2525%2529%252C%2520and%2520performs%2520similarly%2520on%2520an%2520average%2520of%252053%250Anatural%2520language%2520understanding%2520tasks%2520while%2520being%2520trained%2520with%25206.6x%2520less%2520compute%250Athan%2520Llama%25203%25208B.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520dataset%2520design%2520for%250Atraining%2520language%2520models%2520and%2520offer%2520a%2520starting%2520point%2520for%2520further%2520research%2520on%250Adata%2520curation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11794v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataComp-LM%3A%20In%20search%20of%20the%20next%20generation%20of%20training%20sets%20for%0A%20%20language%20models&entry.906535625=Jeffrey%20Li%20and%20Alex%20Fang%20and%20Georgios%20Smyrnis%20and%20Maor%20Ivgi%20and%20Matt%20Jordan%20and%20Samir%20Gadre%20and%20Hritik%20Bansal%20and%20Etash%20Guha%20and%20Sedrick%20Keh%20and%20Kushal%20Arora%20and%20Saurabh%20Garg%20and%20Rui%20Xin%20and%20Niklas%20Muennighoff%20and%20Reinhard%20Heckel%20and%20Jean%20Mercat%20and%20Mayee%20Chen%20and%20Suchin%20Gururangan%20and%20Mitchell%20Wortsman%20and%20Alon%20Albalak%20and%20Yonatan%20Bitton%20and%20Marianna%20Nezhurina%20and%20Amro%20Abbas%20and%20Cheng-Yu%20Hsieh%20and%20Dhruba%20Ghosh%20and%20Josh%20Gardner%20and%20Maciej%20Kilian%20and%20Hanlin%20Zhang%20and%20Rulin%20Shao%20and%20Sarah%20Pratt%20and%20Sunny%20Sanyal%20and%20Gabriel%20Ilharco%20and%20Giannis%20Daras%20and%20Kalyani%20Marathe%20and%20Aaron%20Gokaslan%20and%20Jieyu%20Zhang%20and%20Khyathi%20Chandu%20and%20Thao%20Nguyen%20and%20Igor%20Vasiljevic%20and%20Sham%20Kakade%20and%20Shuran%20Song%20and%20Sujay%20Sanghavi%20and%20Fartash%20Faghri%20and%20Sewoong%20Oh%20and%20Luke%20Zettlemoyer%20and%20Kyle%20Lo%20and%20Alaaeldin%20El-Nouby%20and%20Hadi%20Pouransari%20and%20Alexander%20Toshev%20and%20Stephanie%20Wang%20and%20Dirk%20Groeneveld%20and%20Luca%20Soldaini%20and%20Pang%20Wei%20Koh%20and%20Jenia%20Jitsev%20and%20Thomas%20Kollar%20and%20Alexandros%20G.%20Dimakis%20and%20Yair%20Carmon%20and%20Achal%20Dave%20and%20Ludwig%20Schmidt%20and%20Vaishaal%20Shankar&entry.1292438233=%20%20We%20introduce%20DataComp%20for%20Language%20Models%20%28DCLM%29%2C%20a%20testbed%20for%20controlled%0Adataset%20experiments%20with%20the%20goal%20of%20improving%20language%20models.%20As%20part%20of%0ADCLM%2C%20we%20provide%20a%20standardized%20corpus%20of%20240T%20tokens%20extracted%20from%20Common%0ACrawl%2C%20effective%20pretraining%20recipes%20based%20on%20the%20OpenLM%20framework%2C%20and%20a%20broad%0Asuite%20of%2053%20downstream%20evaluations.%20Participants%20in%20the%20DCLM%20benchmark%20can%0Aexperiment%20with%20data%20curation%20strategies%20such%20as%20deduplication%2C%20filtering%2C%20and%0Adata%20mixing%20at%20model%20scales%20ranging%20from%20412M%20to%207B%20parameters.%20As%20a%20baseline%0Afor%20DCLM%2C%20we%20conduct%20extensive%20experiments%20and%20find%20that%20model-based%20filtering%0Ais%20key%20to%20assembling%20a%20high-quality%20training%20set.%20The%20resulting%20dataset%2C%0ADCLM-Baseline%20enables%20training%20a%207B%20parameter%20language%20model%20from%20scratch%20to%0A64%25%205-shot%20accuracy%20on%20MMLU%20with%202.6T%20training%20tokens.%20Compared%20to%20MAP-Neo%2C%20the%0Aprevious%20state-of-the-art%20in%20open-data%20language%20models%2C%20DCLM-Baseline%0Arepresents%20a%206.6%20percentage%20point%20improvement%20on%20MMLU%20while%20being%20trained%20with%0A40%25%20less%20compute.%20Our%20baseline%20model%20is%20also%20comparable%20to%20Mistral-7B-v0.3%20and%0ALlama%203%208B%20on%20MMLU%20%2863%25%20%26%2066%25%29%2C%20and%20performs%20similarly%20on%20an%20average%20of%2053%0Anatural%20language%20understanding%20tasks%20while%20being%20trained%20with%206.6x%20less%20compute%0Athan%20Llama%203%208B.%20Our%20results%20highlight%20the%20importance%20of%20dataset%20design%20for%0Atraining%20language%20models%20and%20offer%20a%20starting%20point%20for%20further%20research%20on%0Adata%20curation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11794v4&entry.124074799=Read"},
{"title": "Empowering AI to Generate Better AI Code: Guided Generation of Deep\n  Learning Projects with LLMs", "author": "Chen Xie and Mingsheng Jiao and Xiaodong Gu and Beijun Shen", "abstract": "  While large language models (LLMs) have been widely applied to code\ngeneration, they struggle with generating entire deep learning projects, which\nare characterized by complex structures, longer functions, and stronger\nreliance on domain knowledge than general-purpose code. An open-domain LLM\noften lacks coherent contextual guidance and domain expertise for specific\nprojects, making it challenging to produce complete code that fully meets user\nrequirements.\n  In this paper, we propose a novel planning-guided code generation method,\nDLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a\nstructured solution plan, offering global guidance for LLMs to generate the\nproject. The generated plan is then leveraged to retrieve semantically\nanalogous code samples and subsequently abstract a code template. To\neffectively integrate these multiple retrieval-augmented techniques, a\ncomparative learning mechanism is designed to generate the final code. We\nvalidate the effectiveness of our approach on a dataset we build for deep\nlearning code generation. Experimental results demonstrate that DLCodeGen\noutperforms other baselines, achieving improvements of 9.7% in CodeBLEU and\n3.6% in human evaluation metrics.\n", "link": "http://arxiv.org/abs/2504.15080v1", "date": "2025-04-21", "relevancy": 2.6842, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5576}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5418}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20AI%20to%20Generate%20Better%20AI%20Code%3A%20Guided%20Generation%20of%20Deep%0A%20%20Learning%20Projects%20with%20LLMs&body=Title%3A%20Empowering%20AI%20to%20Generate%20Better%20AI%20Code%3A%20Guided%20Generation%20of%20Deep%0A%20%20Learning%20Projects%20with%20LLMs%0AAuthor%3A%20Chen%20Xie%20and%20Mingsheng%20Jiao%20and%20Xiaodong%20Gu%20and%20Beijun%20Shen%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20have%20been%20widely%20applied%20to%20code%0Ageneration%2C%20they%20struggle%20with%20generating%20entire%20deep%20learning%20projects%2C%20which%0Aare%20characterized%20by%20complex%20structures%2C%20longer%20functions%2C%20and%20stronger%0Areliance%20on%20domain%20knowledge%20than%20general-purpose%20code.%20An%20open-domain%20LLM%0Aoften%20lacks%20coherent%20contextual%20guidance%20and%20domain%20expertise%20for%20specific%0Aprojects%2C%20making%20it%20challenging%20to%20produce%20complete%20code%20that%20fully%20meets%20user%0Arequirements.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20planning-guided%20code%20generation%20method%2C%0ADLCodeGen%2C%20tailored%20for%20generating%20deep%20learning%20projects.%20DLCodeGen%20predicts%20a%0Astructured%20solution%20plan%2C%20offering%20global%20guidance%20for%20LLMs%20to%20generate%20the%0Aproject.%20The%20generated%20plan%20is%20then%20leveraged%20to%20retrieve%20semantically%0Aanalogous%20code%20samples%20and%20subsequently%20abstract%20a%20code%20template.%20To%0Aeffectively%20integrate%20these%20multiple%20retrieval-augmented%20techniques%2C%20a%0Acomparative%20learning%20mechanism%20is%20designed%20to%20generate%20the%20final%20code.%20We%0Avalidate%20the%20effectiveness%20of%20our%20approach%20on%20a%20dataset%20we%20build%20for%20deep%0Alearning%20code%20generation.%20Experimental%20results%20demonstrate%20that%20DLCodeGen%0Aoutperforms%20other%20baselines%2C%20achieving%20improvements%20of%209.7%25%20in%20CodeBLEU%20and%0A3.6%25%20in%20human%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520AI%2520to%2520Generate%2520Better%2520AI%2520Code%253A%2520Guided%2520Generation%2520of%2520Deep%250A%2520%2520Learning%2520Projects%2520with%2520LLMs%26entry.906535625%3DChen%2520Xie%2520and%2520Mingsheng%2520Jiao%2520and%2520Xiaodong%2520Gu%2520and%2520Beijun%2520Shen%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520been%2520widely%2520applied%2520to%2520code%250Ageneration%252C%2520they%2520struggle%2520with%2520generating%2520entire%2520deep%2520learning%2520projects%252C%2520which%250Aare%2520characterized%2520by%2520complex%2520structures%252C%2520longer%2520functions%252C%2520and%2520stronger%250Areliance%2520on%2520domain%2520knowledge%2520than%2520general-purpose%2520code.%2520An%2520open-domain%2520LLM%250Aoften%2520lacks%2520coherent%2520contextual%2520guidance%2520and%2520domain%2520expertise%2520for%2520specific%250Aprojects%252C%2520making%2520it%2520challenging%2520to%2520produce%2520complete%2520code%2520that%2520fully%2520meets%2520user%250Arequirements.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520planning-guided%2520code%2520generation%2520method%252C%250ADLCodeGen%252C%2520tailored%2520for%2520generating%2520deep%2520learning%2520projects.%2520DLCodeGen%2520predicts%2520a%250Astructured%2520solution%2520plan%252C%2520offering%2520global%2520guidance%2520for%2520LLMs%2520to%2520generate%2520the%250Aproject.%2520The%2520generated%2520plan%2520is%2520then%2520leveraged%2520to%2520retrieve%2520semantically%250Aanalogous%2520code%2520samples%2520and%2520subsequently%2520abstract%2520a%2520code%2520template.%2520To%250Aeffectively%2520integrate%2520these%2520multiple%2520retrieval-augmented%2520techniques%252C%2520a%250Acomparative%2520learning%2520mechanism%2520is%2520designed%2520to%2520generate%2520the%2520final%2520code.%2520We%250Avalidate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520a%2520dataset%2520we%2520build%2520for%2520deep%250Alearning%2520code%2520generation.%2520Experimental%2520results%2520demonstrate%2520that%2520DLCodeGen%250Aoutperforms%2520other%2520baselines%252C%2520achieving%2520improvements%2520of%25209.7%2525%2520in%2520CodeBLEU%2520and%250A3.6%2525%2520in%2520human%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20AI%20to%20Generate%20Better%20AI%20Code%3A%20Guided%20Generation%20of%20Deep%0A%20%20Learning%20Projects%20with%20LLMs&entry.906535625=Chen%20Xie%20and%20Mingsheng%20Jiao%20and%20Xiaodong%20Gu%20and%20Beijun%20Shen&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20have%20been%20widely%20applied%20to%20code%0Ageneration%2C%20they%20struggle%20with%20generating%20entire%20deep%20learning%20projects%2C%20which%0Aare%20characterized%20by%20complex%20structures%2C%20longer%20functions%2C%20and%20stronger%0Areliance%20on%20domain%20knowledge%20than%20general-purpose%20code.%20An%20open-domain%20LLM%0Aoften%20lacks%20coherent%20contextual%20guidance%20and%20domain%20expertise%20for%20specific%0Aprojects%2C%20making%20it%20challenging%20to%20produce%20complete%20code%20that%20fully%20meets%20user%0Arequirements.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20planning-guided%20code%20generation%20method%2C%0ADLCodeGen%2C%20tailored%20for%20generating%20deep%20learning%20projects.%20DLCodeGen%20predicts%20a%0Astructured%20solution%20plan%2C%20offering%20global%20guidance%20for%20LLMs%20to%20generate%20the%0Aproject.%20The%20generated%20plan%20is%20then%20leveraged%20to%20retrieve%20semantically%0Aanalogous%20code%20samples%20and%20subsequently%20abstract%20a%20code%20template.%20To%0Aeffectively%20integrate%20these%20multiple%20retrieval-augmented%20techniques%2C%20a%0Acomparative%20learning%20mechanism%20is%20designed%20to%20generate%20the%20final%20code.%20We%0Avalidate%20the%20effectiveness%20of%20our%20approach%20on%20a%20dataset%20we%20build%20for%20deep%0Alearning%20code%20generation.%20Experimental%20results%20demonstrate%20that%20DLCodeGen%0Aoutperforms%20other%20baselines%2C%20achieving%20improvements%20of%209.7%25%20in%20CodeBLEU%20and%0A3.6%25%20in%20human%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15080v1&entry.124074799=Read"},
{"title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws", "author": "Zhixuan Pan and Shaowen Wang and Jian Li", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.\n", "link": "http://arxiv.org/abs/2504.09597v3", "date": "2025-04-21", "relevancy": 2.6836, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws&body=Title%3A%20Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws%0AAuthor%3A%20Zhixuan%20Pan%20and%20Shaowen%20Wang%20and%20Jian%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Anumerous%20tasks%2C%20yet%20principled%20explanations%20for%20their%20underlying%20mechanisms%20and%0Aseveral%20phenomena%2C%20such%20as%20scaling%20laws%2C%20hallucinations%2C%20and%20related%20behaviors%2C%0Aremain%20elusive.%20In%20this%20work%2C%20we%20revisit%20the%20classical%20relationship%20between%0Acompression%20and%20prediction%2C%20grounded%20in%20Kolmogorov%20complexity%20and%20Shannon%0Ainformation%20theory%2C%20to%20provide%20deeper%20insights%20into%20LLM%20behaviors.%20By%0Aleveraging%20the%20Kolmogorov%20Structure%20Function%20and%20interpreting%20LLM%20compression%0Aas%20a%20two-part%20coding%20process%2C%20we%20offer%20a%20detailed%20view%20of%20how%20LLMs%20acquire%20and%0Astore%20information%20across%20increasing%20model%20and%20data%20scales%20--%20from%20pervasive%0Asyntactic%20patterns%20to%20progressively%20rarer%20knowledge%20elements.%20Motivated%20by%20this%0Atheoretical%20perspective%20and%20natural%20assumptions%20inspired%20by%20Heap%27s%20and%20Zipf%27s%0Alaws%2C%20we%20introduce%20a%20simplified%20yet%20representative%20hierarchical%20data-generation%0Aframework%20called%20the%20Syntax-Knowledge%20model.%20Under%20the%20Bayesian%20setting%2C%20we%0Ashow%20that%20prediction%20and%20compression%20within%20this%20model%20naturally%20lead%20to%0Adiverse%20learning%20and%20scaling%20behaviors%20of%20LLMs.%20In%20particular%2C%20our%20theoretical%0Aanalysis%20offers%20intuitive%20and%20principled%20explanations%20for%20both%20data%20and%20model%0Ascaling%20laws%2C%20the%20dynamics%20of%20knowledge%20acquisition%20during%20training%20and%0Afine-tuning%2C%20factual%20knowledge%20hallucinations%20in%20LLMs.%20The%20experimental%20results%0Avalidate%20our%20theoretical%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09597v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520LLM%2520Behaviors%2520via%2520Compression%253A%2520Data%2520Generation%252C%2520Knowledge%250A%2520%2520Acquisition%2520and%2520Scaling%2520Laws%26entry.906535625%3DZhixuan%2520Pan%2520and%2520Shaowen%2520Wang%2520and%2520Jian%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Anumerous%2520tasks%252C%2520yet%2520principled%2520explanations%2520for%2520their%2520underlying%2520mechanisms%2520and%250Aseveral%2520phenomena%252C%2520such%2520as%2520scaling%2520laws%252C%2520hallucinations%252C%2520and%2520related%2520behaviors%252C%250Aremain%2520elusive.%2520In%2520this%2520work%252C%2520we%2520revisit%2520the%2520classical%2520relationship%2520between%250Acompression%2520and%2520prediction%252C%2520grounded%2520in%2520Kolmogorov%2520complexity%2520and%2520Shannon%250Ainformation%2520theory%252C%2520to%2520provide%2520deeper%2520insights%2520into%2520LLM%2520behaviors.%2520By%250Aleveraging%2520the%2520Kolmogorov%2520Structure%2520Function%2520and%2520interpreting%2520LLM%2520compression%250Aas%2520a%2520two-part%2520coding%2520process%252C%2520we%2520offer%2520a%2520detailed%2520view%2520of%2520how%2520LLMs%2520acquire%2520and%250Astore%2520information%2520across%2520increasing%2520model%2520and%2520data%2520scales%2520--%2520from%2520pervasive%250Asyntactic%2520patterns%2520to%2520progressively%2520rarer%2520knowledge%2520elements.%2520Motivated%2520by%2520this%250Atheoretical%2520perspective%2520and%2520natural%2520assumptions%2520inspired%2520by%2520Heap%2527s%2520and%2520Zipf%2527s%250Alaws%252C%2520we%2520introduce%2520a%2520simplified%2520yet%2520representative%2520hierarchical%2520data-generation%250Aframework%2520called%2520the%2520Syntax-Knowledge%2520model.%2520Under%2520the%2520Bayesian%2520setting%252C%2520we%250Ashow%2520that%2520prediction%2520and%2520compression%2520within%2520this%2520model%2520naturally%2520lead%2520to%250Adiverse%2520learning%2520and%2520scaling%2520behaviors%2520of%2520LLMs.%2520In%2520particular%252C%2520our%2520theoretical%250Aanalysis%2520offers%2520intuitive%2520and%2520principled%2520explanations%2520for%2520both%2520data%2520and%2520model%250Ascaling%2520laws%252C%2520the%2520dynamics%2520of%2520knowledge%2520acquisition%2520during%2520training%2520and%250Afine-tuning%252C%2520factual%2520knowledge%2520hallucinations%2520in%2520LLMs.%2520The%2520experimental%2520results%250Avalidate%2520our%2520theoretical%2520predictions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09597v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20LLM%20Behaviors%20via%20Compression%3A%20Data%20Generation%2C%20Knowledge%0A%20%20Acquisition%20and%20Scaling%20Laws&entry.906535625=Zhixuan%20Pan%20and%20Shaowen%20Wang%20and%20Jian%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Anumerous%20tasks%2C%20yet%20principled%20explanations%20for%20their%20underlying%20mechanisms%20and%0Aseveral%20phenomena%2C%20such%20as%20scaling%20laws%2C%20hallucinations%2C%20and%20related%20behaviors%2C%0Aremain%20elusive.%20In%20this%20work%2C%20we%20revisit%20the%20classical%20relationship%20between%0Acompression%20and%20prediction%2C%20grounded%20in%20Kolmogorov%20complexity%20and%20Shannon%0Ainformation%20theory%2C%20to%20provide%20deeper%20insights%20into%20LLM%20behaviors.%20By%0Aleveraging%20the%20Kolmogorov%20Structure%20Function%20and%20interpreting%20LLM%20compression%0Aas%20a%20two-part%20coding%20process%2C%20we%20offer%20a%20detailed%20view%20of%20how%20LLMs%20acquire%20and%0Astore%20information%20across%20increasing%20model%20and%20data%20scales%20--%20from%20pervasive%0Asyntactic%20patterns%20to%20progressively%20rarer%20knowledge%20elements.%20Motivated%20by%20this%0Atheoretical%20perspective%20and%20natural%20assumptions%20inspired%20by%20Heap%27s%20and%20Zipf%27s%0Alaws%2C%20we%20introduce%20a%20simplified%20yet%20representative%20hierarchical%20data-generation%0Aframework%20called%20the%20Syntax-Knowledge%20model.%20Under%20the%20Bayesian%20setting%2C%20we%0Ashow%20that%20prediction%20and%20compression%20within%20this%20model%20naturally%20lead%20to%0Adiverse%20learning%20and%20scaling%20behaviors%20of%20LLMs.%20In%20particular%2C%20our%20theoretical%0Aanalysis%20offers%20intuitive%20and%20principled%20explanations%20for%20both%20data%20and%20model%0Ascaling%20laws%2C%20the%20dynamics%20of%20knowledge%20acquisition%20during%20training%20and%0Afine-tuning%2C%20factual%20knowledge%20hallucinations%20in%20LLMs.%20The%20experimental%20results%0Avalidate%20our%20theoretical%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09597v3&entry.124074799=Read"},
{"title": "Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong\n  Person Re-identification", "author": "Shiben Liu and Huijie Fan and Qiang Wang and Baojie Fan and Yandong Tang and Liangqiong Qu", "abstract": "  Lifelong Person Re-identification (LReID) suffers from a key challenge in\npreserving old knowledge while adapting to new information. The existing\nsolutions include rehearsal-based and rehearsal-free methods to address this\nchallenge. Rehearsal-based approaches rely on knowledge distillation,\ncontinuously accumulating forgetting during the distillation process.\nRehearsal-free methods insufficiently learn the distribution of each domain,\nleading to forgetfulness over time. To solve these issues, we propose a novel\nDistribution-aware Forgetting Compensation (DAFC) model that explores\ncross-domain shared representation learning and domain-specific distribution\nintegration without using old exemplars or knowledge distillation. We propose a\nText-driven Prompt Aggregation (TPA) that utilizes text features to enrich\nprompt elements and guide the prompt model to learn fine-grained\nrepresentations for each instance. This can enhance the differentiation of\nidentity information and establish the foundation for domain distribution\nawareness. Then, Distribution-based Awareness and Integration (DAI) is designed\nto capture each domain-specific distribution by a dedicated expert network and\nadaptively consolidate them into a shared region in high-dimensional space. In\nthis manner, DAI can consolidate and enhance cross-domain shared representation\nlearning while alleviating catastrophic forgetting. Furthermore, we develop a\nKnowledge Consolidation Mechanism (KCM) that comprises instance-level\ndiscrimination and cross-domain consistency alignment strategies to facilitate\nmodel adaptive learning of new knowledge from the current domain and promote\nknowledge consolidation learning between acquired domain-specific\ndistributions, respectively. Experimental results show that our DAFC outperform\nstate-of-the-art methods by at least 9.8\\%/6.6\\% and 6.4\\%/6.2\\% of average\nmAP/R@1 on two training orders.\n", "link": "http://arxiv.org/abs/2504.15041v1", "date": "2025-04-21", "relevancy": 2.6818, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5431}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5348}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution-aware%20Forgetting%20Compensation%20for%20Exemplar-Free%20Lifelong%0A%20%20Person%20Re-identification&body=Title%3A%20Distribution-aware%20Forgetting%20Compensation%20for%20Exemplar-Free%20Lifelong%0A%20%20Person%20Re-identification%0AAuthor%3A%20Shiben%20Liu%20and%20Huijie%20Fan%20and%20Qiang%20Wang%20and%20Baojie%20Fan%20and%20Yandong%20Tang%20and%20Liangqiong%20Qu%0AAbstract%3A%20%20%20Lifelong%20Person%20Re-identification%20%28LReID%29%20suffers%20from%20a%20key%20challenge%20in%0Apreserving%20old%20knowledge%20while%20adapting%20to%20new%20information.%20The%20existing%0Asolutions%20include%20rehearsal-based%20and%20rehearsal-free%20methods%20to%20address%20this%0Achallenge.%20Rehearsal-based%20approaches%20rely%20on%20knowledge%20distillation%2C%0Acontinuously%20accumulating%20forgetting%20during%20the%20distillation%20process.%0ARehearsal-free%20methods%20insufficiently%20learn%20the%20distribution%20of%20each%20domain%2C%0Aleading%20to%20forgetfulness%20over%20time.%20To%20solve%20these%20issues%2C%20we%20propose%20a%20novel%0ADistribution-aware%20Forgetting%20Compensation%20%28DAFC%29%20model%20that%20explores%0Across-domain%20shared%20representation%20learning%20and%20domain-specific%20distribution%0Aintegration%20without%20using%20old%20exemplars%20or%20knowledge%20distillation.%20We%20propose%20a%0AText-driven%20Prompt%20Aggregation%20%28TPA%29%20that%20utilizes%20text%20features%20to%20enrich%0Aprompt%20elements%20and%20guide%20the%20prompt%20model%20to%20learn%20fine-grained%0Arepresentations%20for%20each%20instance.%20This%20can%20enhance%20the%20differentiation%20of%0Aidentity%20information%20and%20establish%20the%20foundation%20for%20domain%20distribution%0Aawareness.%20Then%2C%20Distribution-based%20Awareness%20and%20Integration%20%28DAI%29%20is%20designed%0Ato%20capture%20each%20domain-specific%20distribution%20by%20a%20dedicated%20expert%20network%20and%0Aadaptively%20consolidate%20them%20into%20a%20shared%20region%20in%20high-dimensional%20space.%20In%0Athis%20manner%2C%20DAI%20can%20consolidate%20and%20enhance%20cross-domain%20shared%20representation%0Alearning%20while%20alleviating%20catastrophic%20forgetting.%20Furthermore%2C%20we%20develop%20a%0AKnowledge%20Consolidation%20Mechanism%20%28KCM%29%20that%20comprises%20instance-level%0Adiscrimination%20and%20cross-domain%20consistency%20alignment%20strategies%20to%20facilitate%0Amodel%20adaptive%20learning%20of%20new%20knowledge%20from%20the%20current%20domain%20and%20promote%0Aknowledge%20consolidation%20learning%20between%20acquired%20domain-specific%0Adistributions%2C%20respectively.%20Experimental%20results%20show%20that%20our%20DAFC%20outperform%0Astate-of-the-art%20methods%20by%20at%20least%209.8%5C%25/6.6%5C%25%20and%206.4%5C%25/6.2%5C%25%20of%20average%0AmAP/R%401%20on%20two%20training%20orders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution-aware%2520Forgetting%2520Compensation%2520for%2520Exemplar-Free%2520Lifelong%250A%2520%2520Person%2520Re-identification%26entry.906535625%3DShiben%2520Liu%2520and%2520Huijie%2520Fan%2520and%2520Qiang%2520Wang%2520and%2520Baojie%2520Fan%2520and%2520Yandong%2520Tang%2520and%2520Liangqiong%2520Qu%26entry.1292438233%3D%2520%2520Lifelong%2520Person%2520Re-identification%2520%2528LReID%2529%2520suffers%2520from%2520a%2520key%2520challenge%2520in%250Apreserving%2520old%2520knowledge%2520while%2520adapting%2520to%2520new%2520information.%2520The%2520existing%250Asolutions%2520include%2520rehearsal-based%2520and%2520rehearsal-free%2520methods%2520to%2520address%2520this%250Achallenge.%2520Rehearsal-based%2520approaches%2520rely%2520on%2520knowledge%2520distillation%252C%250Acontinuously%2520accumulating%2520forgetting%2520during%2520the%2520distillation%2520process.%250ARehearsal-free%2520methods%2520insufficiently%2520learn%2520the%2520distribution%2520of%2520each%2520domain%252C%250Aleading%2520to%2520forgetfulness%2520over%2520time.%2520To%2520solve%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%250ADistribution-aware%2520Forgetting%2520Compensation%2520%2528DAFC%2529%2520model%2520that%2520explores%250Across-domain%2520shared%2520representation%2520learning%2520and%2520domain-specific%2520distribution%250Aintegration%2520without%2520using%2520old%2520exemplars%2520or%2520knowledge%2520distillation.%2520We%2520propose%2520a%250AText-driven%2520Prompt%2520Aggregation%2520%2528TPA%2529%2520that%2520utilizes%2520text%2520features%2520to%2520enrich%250Aprompt%2520elements%2520and%2520guide%2520the%2520prompt%2520model%2520to%2520learn%2520fine-grained%250Arepresentations%2520for%2520each%2520instance.%2520This%2520can%2520enhance%2520the%2520differentiation%2520of%250Aidentity%2520information%2520and%2520establish%2520the%2520foundation%2520for%2520domain%2520distribution%250Aawareness.%2520Then%252C%2520Distribution-based%2520Awareness%2520and%2520Integration%2520%2528DAI%2529%2520is%2520designed%250Ato%2520capture%2520each%2520domain-specific%2520distribution%2520by%2520a%2520dedicated%2520expert%2520network%2520and%250Aadaptively%2520consolidate%2520them%2520into%2520a%2520shared%2520region%2520in%2520high-dimensional%2520space.%2520In%250Athis%2520manner%252C%2520DAI%2520can%2520consolidate%2520and%2520enhance%2520cross-domain%2520shared%2520representation%250Alearning%2520while%2520alleviating%2520catastrophic%2520forgetting.%2520Furthermore%252C%2520we%2520develop%2520a%250AKnowledge%2520Consolidation%2520Mechanism%2520%2528KCM%2529%2520that%2520comprises%2520instance-level%250Adiscrimination%2520and%2520cross-domain%2520consistency%2520alignment%2520strategies%2520to%2520facilitate%250Amodel%2520adaptive%2520learning%2520of%2520new%2520knowledge%2520from%2520the%2520current%2520domain%2520and%2520promote%250Aknowledge%2520consolidation%2520learning%2520between%2520acquired%2520domain-specific%250Adistributions%252C%2520respectively.%2520Experimental%2520results%2520show%2520that%2520our%2520DAFC%2520outperform%250Astate-of-the-art%2520methods%2520by%2520at%2520least%25209.8%255C%2525/6.6%255C%2525%2520and%25206.4%255C%2525/6.2%255C%2525%2520of%2520average%250AmAP/R%25401%2520on%2520two%2520training%2520orders.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution-aware%20Forgetting%20Compensation%20for%20Exemplar-Free%20Lifelong%0A%20%20Person%20Re-identification&entry.906535625=Shiben%20Liu%20and%20Huijie%20Fan%20and%20Qiang%20Wang%20and%20Baojie%20Fan%20and%20Yandong%20Tang%20and%20Liangqiong%20Qu&entry.1292438233=%20%20Lifelong%20Person%20Re-identification%20%28LReID%29%20suffers%20from%20a%20key%20challenge%20in%0Apreserving%20old%20knowledge%20while%20adapting%20to%20new%20information.%20The%20existing%0Asolutions%20include%20rehearsal-based%20and%20rehearsal-free%20methods%20to%20address%20this%0Achallenge.%20Rehearsal-based%20approaches%20rely%20on%20knowledge%20distillation%2C%0Acontinuously%20accumulating%20forgetting%20during%20the%20distillation%20process.%0ARehearsal-free%20methods%20insufficiently%20learn%20the%20distribution%20of%20each%20domain%2C%0Aleading%20to%20forgetfulness%20over%20time.%20To%20solve%20these%20issues%2C%20we%20propose%20a%20novel%0ADistribution-aware%20Forgetting%20Compensation%20%28DAFC%29%20model%20that%20explores%0Across-domain%20shared%20representation%20learning%20and%20domain-specific%20distribution%0Aintegration%20without%20using%20old%20exemplars%20or%20knowledge%20distillation.%20We%20propose%20a%0AText-driven%20Prompt%20Aggregation%20%28TPA%29%20that%20utilizes%20text%20features%20to%20enrich%0Aprompt%20elements%20and%20guide%20the%20prompt%20model%20to%20learn%20fine-grained%0Arepresentations%20for%20each%20instance.%20This%20can%20enhance%20the%20differentiation%20of%0Aidentity%20information%20and%20establish%20the%20foundation%20for%20domain%20distribution%0Aawareness.%20Then%2C%20Distribution-based%20Awareness%20and%20Integration%20%28DAI%29%20is%20designed%0Ato%20capture%20each%20domain-specific%20distribution%20by%20a%20dedicated%20expert%20network%20and%0Aadaptively%20consolidate%20them%20into%20a%20shared%20region%20in%20high-dimensional%20space.%20In%0Athis%20manner%2C%20DAI%20can%20consolidate%20and%20enhance%20cross-domain%20shared%20representation%0Alearning%20while%20alleviating%20catastrophic%20forgetting.%20Furthermore%2C%20we%20develop%20a%0AKnowledge%20Consolidation%20Mechanism%20%28KCM%29%20that%20comprises%20instance-level%0Adiscrimination%20and%20cross-domain%20consistency%20alignment%20strategies%20to%20facilitate%0Amodel%20adaptive%20learning%20of%20new%20knowledge%20from%20the%20current%20domain%20and%20promote%0Aknowledge%20consolidation%20learning%20between%20acquired%20domain-specific%0Adistributions%2C%20respectively.%20Experimental%20results%20show%20that%20our%20DAFC%20outperform%0Astate-of-the-art%20methods%20by%20at%20least%209.8%5C%25/6.6%5C%25%20and%206.4%5C%25/6.2%5C%25%20of%20average%0AmAP/R%401%20on%20two%20training%20orders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15041v1&entry.124074799=Read"},
{"title": "Transferable Adversarial Attacks on SAM and Its Downstream Models", "author": "Song Xia and Wenhan Yang and Yi Yu and Xun Lin and Henghui Ding and Ling-Yu Duan and Xudong Jiang", "abstract": "  The utilization of large foundational models has a dilemma: while fine-tuning\ndownstream tasks from them holds promise for making use of the well-generalized\nknowledge in practical applications, their open accessibility also poses\nthreats of adverse usage. This paper, for the first time, explores the\nfeasibility of adversarial attacking various downstream models fine-tuned from\nthe segment anything model (SAM), by solely utilizing the information from the\nopen-sourced SAM. In contrast to prevailing transfer-based adversarial attacks,\nwe demonstrate the existence of adversarial dangers even without accessing the\ndownstream task and dataset to train a similar surrogate model. To enhance the\neffectiveness of the adversarial attack towards models fine-tuned on unknown\ndatasets, we propose a universal meta-initialization (UMI) algorithm to extract\nthe intrinsic vulnerability inherent in the foundation model, which is then\nutilized as the prior knowledge to guide the generation of adversarial\nperturbations. Moreover, by formulating the gradient difference in the\nattacking process between the open-sourced SAM and its fine-tuned downstream\nmodels, we theoretically demonstrate that a deviation occurs in the adversarial\nupdate direction by directly maximizing the distance of encoded feature\nembeddings in the open-sourced SAM. Consequently, we propose a gradient robust\nloss that simulates the associated uncertainty with gradient-based noise\naugmentation to enhance the robustness of generated adversarial examples (AEs)\ntowards this deviation, thus improving the transferability. Extensive\nexperiments demonstrate the effectiveness of the proposed universal\nmeta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs\nand their downstream models. Code is available at\nhttps://github.com/xiasong0501/GRAT.\n", "link": "http://arxiv.org/abs/2410.20197v3", "date": "2025-04-21", "relevancy": 2.6417, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5492}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5297}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferable%20Adversarial%20Attacks%20on%20SAM%20and%20Its%20Downstream%20Models&body=Title%3A%20Transferable%20Adversarial%20Attacks%20on%20SAM%20and%20Its%20Downstream%20Models%0AAuthor%3A%20Song%20Xia%20and%20Wenhan%20Yang%20and%20Yi%20Yu%20and%20Xun%20Lin%20and%20Henghui%20Ding%20and%20Ling-Yu%20Duan%20and%20Xudong%20Jiang%0AAbstract%3A%20%20%20The%20utilization%20of%20large%20foundational%20models%20has%20a%20dilemma%3A%20while%20fine-tuning%0Adownstream%20tasks%20from%20them%20holds%20promise%20for%20making%20use%20of%20the%20well-generalized%0Aknowledge%20in%20practical%20applications%2C%20their%20open%20accessibility%20also%20poses%0Athreats%20of%20adverse%20usage.%20This%20paper%2C%20for%20the%20first%20time%2C%20explores%20the%0Afeasibility%20of%20adversarial%20attacking%20various%20downstream%20models%20fine-tuned%20from%0Athe%20segment%20anything%20model%20%28SAM%29%2C%20by%20solely%20utilizing%20the%20information%20from%20the%0Aopen-sourced%20SAM.%20In%20contrast%20to%20prevailing%20transfer-based%20adversarial%20attacks%2C%0Awe%20demonstrate%20the%20existence%20of%20adversarial%20dangers%20even%20without%20accessing%20the%0Adownstream%20task%20and%20dataset%20to%20train%20a%20similar%20surrogate%20model.%20To%20enhance%20the%0Aeffectiveness%20of%20the%20adversarial%20attack%20towards%20models%20fine-tuned%20on%20unknown%0Adatasets%2C%20we%20propose%20a%20universal%20meta-initialization%20%28UMI%29%20algorithm%20to%20extract%0Athe%20intrinsic%20vulnerability%20inherent%20in%20the%20foundation%20model%2C%20which%20is%20then%0Autilized%20as%20the%20prior%20knowledge%20to%20guide%20the%20generation%20of%20adversarial%0Aperturbations.%20Moreover%2C%20by%20formulating%20the%20gradient%20difference%20in%20the%0Aattacking%20process%20between%20the%20open-sourced%20SAM%20and%20its%20fine-tuned%20downstream%0Amodels%2C%20we%20theoretically%20demonstrate%20that%20a%20deviation%20occurs%20in%20the%20adversarial%0Aupdate%20direction%20by%20directly%20maximizing%20the%20distance%20of%20encoded%20feature%0Aembeddings%20in%20the%20open-sourced%20SAM.%20Consequently%2C%20we%20propose%20a%20gradient%20robust%0Aloss%20that%20simulates%20the%20associated%20uncertainty%20with%20gradient-based%20noise%0Aaugmentation%20to%20enhance%20the%20robustness%20of%20generated%20adversarial%20examples%20%28AEs%29%0Atowards%20this%20deviation%2C%20thus%20improving%20the%20transferability.%20Extensive%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20universal%0Ameta-initialized%20and%20gradient%20robust%20adversarial%20attack%20%28UMI-GRAT%29%20toward%20SAMs%0Aand%20their%20downstream%20models.%20Code%20is%20available%20at%0Ahttps%3A//github.com/xiasong0501/GRAT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20197v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferable%2520Adversarial%2520Attacks%2520on%2520SAM%2520and%2520Its%2520Downstream%2520Models%26entry.906535625%3DSong%2520Xia%2520and%2520Wenhan%2520Yang%2520and%2520Yi%2520Yu%2520and%2520Xun%2520Lin%2520and%2520Henghui%2520Ding%2520and%2520Ling-Yu%2520Duan%2520and%2520Xudong%2520Jiang%26entry.1292438233%3D%2520%2520The%2520utilization%2520of%2520large%2520foundational%2520models%2520has%2520a%2520dilemma%253A%2520while%2520fine-tuning%250Adownstream%2520tasks%2520from%2520them%2520holds%2520promise%2520for%2520making%2520use%2520of%2520the%2520well-generalized%250Aknowledge%2520in%2520practical%2520applications%252C%2520their%2520open%2520accessibility%2520also%2520poses%250Athreats%2520of%2520adverse%2520usage.%2520This%2520paper%252C%2520for%2520the%2520first%2520time%252C%2520explores%2520the%250Afeasibility%2520of%2520adversarial%2520attacking%2520various%2520downstream%2520models%2520fine-tuned%2520from%250Athe%2520segment%2520anything%2520model%2520%2528SAM%2529%252C%2520by%2520solely%2520utilizing%2520the%2520information%2520from%2520the%250Aopen-sourced%2520SAM.%2520In%2520contrast%2520to%2520prevailing%2520transfer-based%2520adversarial%2520attacks%252C%250Awe%2520demonstrate%2520the%2520existence%2520of%2520adversarial%2520dangers%2520even%2520without%2520accessing%2520the%250Adownstream%2520task%2520and%2520dataset%2520to%2520train%2520a%2520similar%2520surrogate%2520model.%2520To%2520enhance%2520the%250Aeffectiveness%2520of%2520the%2520adversarial%2520attack%2520towards%2520models%2520fine-tuned%2520on%2520unknown%250Adatasets%252C%2520we%2520propose%2520a%2520universal%2520meta-initialization%2520%2528UMI%2529%2520algorithm%2520to%2520extract%250Athe%2520intrinsic%2520vulnerability%2520inherent%2520in%2520the%2520foundation%2520model%252C%2520which%2520is%2520then%250Autilized%2520as%2520the%2520prior%2520knowledge%2520to%2520guide%2520the%2520generation%2520of%2520adversarial%250Aperturbations.%2520Moreover%252C%2520by%2520formulating%2520the%2520gradient%2520difference%2520in%2520the%250Aattacking%2520process%2520between%2520the%2520open-sourced%2520SAM%2520and%2520its%2520fine-tuned%2520downstream%250Amodels%252C%2520we%2520theoretically%2520demonstrate%2520that%2520a%2520deviation%2520occurs%2520in%2520the%2520adversarial%250Aupdate%2520direction%2520by%2520directly%2520maximizing%2520the%2520distance%2520of%2520encoded%2520feature%250Aembeddings%2520in%2520the%2520open-sourced%2520SAM.%2520Consequently%252C%2520we%2520propose%2520a%2520gradient%2520robust%250Aloss%2520that%2520simulates%2520the%2520associated%2520uncertainty%2520with%2520gradient-based%2520noise%250Aaugmentation%2520to%2520enhance%2520the%2520robustness%2520of%2520generated%2520adversarial%2520examples%2520%2528AEs%2529%250Atowards%2520this%2520deviation%252C%2520thus%2520improving%2520the%2520transferability.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520universal%250Ameta-initialized%2520and%2520gradient%2520robust%2520adversarial%2520attack%2520%2528UMI-GRAT%2529%2520toward%2520SAMs%250Aand%2520their%2520downstream%2520models.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/xiasong0501/GRAT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20197v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferable%20Adversarial%20Attacks%20on%20SAM%20and%20Its%20Downstream%20Models&entry.906535625=Song%20Xia%20and%20Wenhan%20Yang%20and%20Yi%20Yu%20and%20Xun%20Lin%20and%20Henghui%20Ding%20and%20Ling-Yu%20Duan%20and%20Xudong%20Jiang&entry.1292438233=%20%20The%20utilization%20of%20large%20foundational%20models%20has%20a%20dilemma%3A%20while%20fine-tuning%0Adownstream%20tasks%20from%20them%20holds%20promise%20for%20making%20use%20of%20the%20well-generalized%0Aknowledge%20in%20practical%20applications%2C%20their%20open%20accessibility%20also%20poses%0Athreats%20of%20adverse%20usage.%20This%20paper%2C%20for%20the%20first%20time%2C%20explores%20the%0Afeasibility%20of%20adversarial%20attacking%20various%20downstream%20models%20fine-tuned%20from%0Athe%20segment%20anything%20model%20%28SAM%29%2C%20by%20solely%20utilizing%20the%20information%20from%20the%0Aopen-sourced%20SAM.%20In%20contrast%20to%20prevailing%20transfer-based%20adversarial%20attacks%2C%0Awe%20demonstrate%20the%20existence%20of%20adversarial%20dangers%20even%20without%20accessing%20the%0Adownstream%20task%20and%20dataset%20to%20train%20a%20similar%20surrogate%20model.%20To%20enhance%20the%0Aeffectiveness%20of%20the%20adversarial%20attack%20towards%20models%20fine-tuned%20on%20unknown%0Adatasets%2C%20we%20propose%20a%20universal%20meta-initialization%20%28UMI%29%20algorithm%20to%20extract%0Athe%20intrinsic%20vulnerability%20inherent%20in%20the%20foundation%20model%2C%20which%20is%20then%0Autilized%20as%20the%20prior%20knowledge%20to%20guide%20the%20generation%20of%20adversarial%0Aperturbations.%20Moreover%2C%20by%20formulating%20the%20gradient%20difference%20in%20the%0Aattacking%20process%20between%20the%20open-sourced%20SAM%20and%20its%20fine-tuned%20downstream%0Amodels%2C%20we%20theoretically%20demonstrate%20that%20a%20deviation%20occurs%20in%20the%20adversarial%0Aupdate%20direction%20by%20directly%20maximizing%20the%20distance%20of%20encoded%20feature%0Aembeddings%20in%20the%20open-sourced%20SAM.%20Consequently%2C%20we%20propose%20a%20gradient%20robust%0Aloss%20that%20simulates%20the%20associated%20uncertainty%20with%20gradient-based%20noise%0Aaugmentation%20to%20enhance%20the%20robustness%20of%20generated%20adversarial%20examples%20%28AEs%29%0Atowards%20this%20deviation%2C%20thus%20improving%20the%20transferability.%20Extensive%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20universal%0Ameta-initialized%20and%20gradient%20robust%20adversarial%20attack%20%28UMI-GRAT%29%20toward%20SAMs%0Aand%20their%20downstream%20models.%20Code%20is%20available%20at%0Ahttps%3A//github.com/xiasong0501/GRAT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20197v3&entry.124074799=Read"},
{"title": "Landmark-Free Preoperative-to-Intraoperative Registration in\n  Laparoscopic Liver Resection", "author": "Jun Zhou and Bingchen Gao and Kai Wang and Jialun Pei and Pheng-Ann Heng and Jing Qin", "abstract": "  Liver registration by overlaying preoperative 3D models onto intraoperative\n2D frames can assist surgeons in perceiving the spatial anatomy of the liver\nclearly for a higher surgical success rate. Existing registration methods rely\nheavily on anatomical landmark-based workflows, which encounter two major\nlimitations: 1) ambiguous landmark definitions fail to provide efficient\nmarkers for registration; 2) insufficient integration of intraoperative liver\nvisual information in shape deformation modeling. To address these challenges,\nin this paper, we propose a landmark-free preoperative-to-intraoperative\nregistration framework utilizing effective self-supervised learning, termed\n\\ourmodel. This framework transforms the conventional 3D-2D workflow into a\n3D-3D registration pipeline, which is then decoupled into rigid and non-rigid\nregistration subtasks. \\ourmodel~first introduces a feature-disentangled\ntransformer to learn robust correspondences for recovering rigid\ntransformations. Further, a structure-regularized deformation network is\ndesigned to adjust the preoperative model to align with the intraoperative\nliver surface. This network captures structural correlations through geometry\nsimilarity modeling in a low-rank transformer network. To facilitate the\nvalidation of the registration performance, we also construct an in-vivo\nregistration dataset containing liver resection videos of 21 patients, called\n\\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the\nliver together with liver mask annotations and calibrated camera intrinsic\nparameters. Extensive experiments and user studies on both synthetic and\nin-vivo datasets demonstrate the superiority and potential clinical\napplicability of our method.\n", "link": "http://arxiv.org/abs/2504.15152v1", "date": "2025-04-21", "relevancy": 2.6398, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5431}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5275}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Landmark-Free%20Preoperative-to-Intraoperative%20Registration%20in%0A%20%20Laparoscopic%20Liver%20Resection&body=Title%3A%20Landmark-Free%20Preoperative-to-Intraoperative%20Registration%20in%0A%20%20Laparoscopic%20Liver%20Resection%0AAuthor%3A%20Jun%20Zhou%20and%20Bingchen%20Gao%20and%20Kai%20Wang%20and%20Jialun%20Pei%20and%20Pheng-Ann%20Heng%20and%20Jing%20Qin%0AAbstract%3A%20%20%20Liver%20registration%20by%20overlaying%20preoperative%203D%20models%20onto%20intraoperative%0A2D%20frames%20can%20assist%20surgeons%20in%20perceiving%20the%20spatial%20anatomy%20of%20the%20liver%0Aclearly%20for%20a%20higher%20surgical%20success%20rate.%20Existing%20registration%20methods%20rely%0Aheavily%20on%20anatomical%20landmark-based%20workflows%2C%20which%20encounter%20two%20major%0Alimitations%3A%201%29%20ambiguous%20landmark%20definitions%20fail%20to%20provide%20efficient%0Amarkers%20for%20registration%3B%202%29%20insufficient%20integration%20of%20intraoperative%20liver%0Avisual%20information%20in%20shape%20deformation%20modeling.%20To%20address%20these%20challenges%2C%0Ain%20this%20paper%2C%20we%20propose%20a%20landmark-free%20preoperative-to-intraoperative%0Aregistration%20framework%20utilizing%20effective%20self-supervised%20learning%2C%20termed%0A%5Courmodel.%20This%20framework%20transforms%20the%20conventional%203D-2D%20workflow%20into%20a%0A3D-3D%20registration%20pipeline%2C%20which%20is%20then%20decoupled%20into%20rigid%20and%20non-rigid%0Aregistration%20subtasks.%20%5Courmodel~first%20introduces%20a%20feature-disentangled%0Atransformer%20to%20learn%20robust%20correspondences%20for%20recovering%20rigid%0Atransformations.%20Further%2C%20a%20structure-regularized%20deformation%20network%20is%0Adesigned%20to%20adjust%20the%20preoperative%20model%20to%20align%20with%20the%20intraoperative%0Aliver%20surface.%20This%20network%20captures%20structural%20correlations%20through%20geometry%0Asimilarity%20modeling%20in%20a%20low-rank%20transformer%20network.%20To%20facilitate%20the%0Avalidation%20of%20the%20registration%20performance%2C%20we%20also%20construct%20an%20in-vivo%0Aregistration%20dataset%20containing%20liver%20resection%20videos%20of%2021%20patients%2C%20called%0A%5Cemph%7BP2I-LReg%7D%2C%20which%20contains%20346%20keyframes%20that%20provide%20a%20global%20view%20of%20the%0Aliver%20together%20with%20liver%20mask%20annotations%20and%20calibrated%20camera%20intrinsic%0Aparameters.%20Extensive%20experiments%20and%20user%20studies%20on%20both%20synthetic%20and%0Ain-vivo%20datasets%20demonstrate%20the%20superiority%20and%20potential%20clinical%0Aapplicability%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLandmark-Free%2520Preoperative-to-Intraoperative%2520Registration%2520in%250A%2520%2520Laparoscopic%2520Liver%2520Resection%26entry.906535625%3DJun%2520Zhou%2520and%2520Bingchen%2520Gao%2520and%2520Kai%2520Wang%2520and%2520Jialun%2520Pei%2520and%2520Pheng-Ann%2520Heng%2520and%2520Jing%2520Qin%26entry.1292438233%3D%2520%2520Liver%2520registration%2520by%2520overlaying%2520preoperative%25203D%2520models%2520onto%2520intraoperative%250A2D%2520frames%2520can%2520assist%2520surgeons%2520in%2520perceiving%2520the%2520spatial%2520anatomy%2520of%2520the%2520liver%250Aclearly%2520for%2520a%2520higher%2520surgical%2520success%2520rate.%2520Existing%2520registration%2520methods%2520rely%250Aheavily%2520on%2520anatomical%2520landmark-based%2520workflows%252C%2520which%2520encounter%2520two%2520major%250Alimitations%253A%25201%2529%2520ambiguous%2520landmark%2520definitions%2520fail%2520to%2520provide%2520efficient%250Amarkers%2520for%2520registration%253B%25202%2529%2520insufficient%2520integration%2520of%2520intraoperative%2520liver%250Avisual%2520information%2520in%2520shape%2520deformation%2520modeling.%2520To%2520address%2520these%2520challenges%252C%250Ain%2520this%2520paper%252C%2520we%2520propose%2520a%2520landmark-free%2520preoperative-to-intraoperative%250Aregistration%2520framework%2520utilizing%2520effective%2520self-supervised%2520learning%252C%2520termed%250A%255Courmodel.%2520This%2520framework%2520transforms%2520the%2520conventional%25203D-2D%2520workflow%2520into%2520a%250A3D-3D%2520registration%2520pipeline%252C%2520which%2520is%2520then%2520decoupled%2520into%2520rigid%2520and%2520non-rigid%250Aregistration%2520subtasks.%2520%255Courmodel~first%2520introduces%2520a%2520feature-disentangled%250Atransformer%2520to%2520learn%2520robust%2520correspondences%2520for%2520recovering%2520rigid%250Atransformations.%2520Further%252C%2520a%2520structure-regularized%2520deformation%2520network%2520is%250Adesigned%2520to%2520adjust%2520the%2520preoperative%2520model%2520to%2520align%2520with%2520the%2520intraoperative%250Aliver%2520surface.%2520This%2520network%2520captures%2520structural%2520correlations%2520through%2520geometry%250Asimilarity%2520modeling%2520in%2520a%2520low-rank%2520transformer%2520network.%2520To%2520facilitate%2520the%250Avalidation%2520of%2520the%2520registration%2520performance%252C%2520we%2520also%2520construct%2520an%2520in-vivo%250Aregistration%2520dataset%2520containing%2520liver%2520resection%2520videos%2520of%252021%2520patients%252C%2520called%250A%255Cemph%257BP2I-LReg%257D%252C%2520which%2520contains%2520346%2520keyframes%2520that%2520provide%2520a%2520global%2520view%2520of%2520the%250Aliver%2520together%2520with%2520liver%2520mask%2520annotations%2520and%2520calibrated%2520camera%2520intrinsic%250Aparameters.%2520Extensive%2520experiments%2520and%2520user%2520studies%2520on%2520both%2520synthetic%2520and%250Ain-vivo%2520datasets%2520demonstrate%2520the%2520superiority%2520and%2520potential%2520clinical%250Aapplicability%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Landmark-Free%20Preoperative-to-Intraoperative%20Registration%20in%0A%20%20Laparoscopic%20Liver%20Resection&entry.906535625=Jun%20Zhou%20and%20Bingchen%20Gao%20and%20Kai%20Wang%20and%20Jialun%20Pei%20and%20Pheng-Ann%20Heng%20and%20Jing%20Qin&entry.1292438233=%20%20Liver%20registration%20by%20overlaying%20preoperative%203D%20models%20onto%20intraoperative%0A2D%20frames%20can%20assist%20surgeons%20in%20perceiving%20the%20spatial%20anatomy%20of%20the%20liver%0Aclearly%20for%20a%20higher%20surgical%20success%20rate.%20Existing%20registration%20methods%20rely%0Aheavily%20on%20anatomical%20landmark-based%20workflows%2C%20which%20encounter%20two%20major%0Alimitations%3A%201%29%20ambiguous%20landmark%20definitions%20fail%20to%20provide%20efficient%0Amarkers%20for%20registration%3B%202%29%20insufficient%20integration%20of%20intraoperative%20liver%0Avisual%20information%20in%20shape%20deformation%20modeling.%20To%20address%20these%20challenges%2C%0Ain%20this%20paper%2C%20we%20propose%20a%20landmark-free%20preoperative-to-intraoperative%0Aregistration%20framework%20utilizing%20effective%20self-supervised%20learning%2C%20termed%0A%5Courmodel.%20This%20framework%20transforms%20the%20conventional%203D-2D%20workflow%20into%20a%0A3D-3D%20registration%20pipeline%2C%20which%20is%20then%20decoupled%20into%20rigid%20and%20non-rigid%0Aregistration%20subtasks.%20%5Courmodel~first%20introduces%20a%20feature-disentangled%0Atransformer%20to%20learn%20robust%20correspondences%20for%20recovering%20rigid%0Atransformations.%20Further%2C%20a%20structure-regularized%20deformation%20network%20is%0Adesigned%20to%20adjust%20the%20preoperative%20model%20to%20align%20with%20the%20intraoperative%0Aliver%20surface.%20This%20network%20captures%20structural%20correlations%20through%20geometry%0Asimilarity%20modeling%20in%20a%20low-rank%20transformer%20network.%20To%20facilitate%20the%0Avalidation%20of%20the%20registration%20performance%2C%20we%20also%20construct%20an%20in-vivo%0Aregistration%20dataset%20containing%20liver%20resection%20videos%20of%2021%20patients%2C%20called%0A%5Cemph%7BP2I-LReg%7D%2C%20which%20contains%20346%20keyframes%20that%20provide%20a%20global%20view%20of%20the%0Aliver%20together%20with%20liver%20mask%20annotations%20and%20calibrated%20camera%20intrinsic%0Aparameters.%20Extensive%20experiments%20and%20user%20studies%20on%20both%20synthetic%20and%0Ain-vivo%20datasets%20demonstrate%20the%20superiority%20and%20potential%20clinical%0Aapplicability%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15152v1&entry.124074799=Read"},
{"title": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL", "author": "Simone Papicchio and Simone Rossi and Luca Cagliero and Paolo Papotti", "abstract": "  Large Language Models (LLMs) have shown impressive capabilities in\ntransforming natural language questions about relational databases into SQL\nqueries. Despite recent improvements, small LLMs struggle to handle questions\ninvolving multiple tables and complex SQL patterns under a Zero-Shot Learning\n(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge\ndeficits in pretrained models but falls short while dealing with queries\ninvolving multi-hop reasoning. To bridge this gap, different LLM training\nstrategies to reinforce reasoning capabilities have been proposed, ranging from\nleveraging a thinking process within ZSL, including reasoning traces in SFT, or\nadopt Reinforcement Learning (RL) strategies. However, the influence of\nreasoning on Text2SQL performance is still largely unexplored. This paper\ninvestigates to what extent LLM reasoning capabilities influence their Text2SQL\nperformance on four benchmark datasets. To this end, it considers the following\nLLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,\nwith and without task-specific reasoning traces; (3) RL, leveraging execution\naccuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that\ncombines SFT and RL. The results show that general-purpose reasoning under ZSL\nproves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit\nfrom SFT with reasoning much more than larger ones, bridging the gap of their\n(weaker) model pretraining. RL is generally beneficial across all tested models\nand datasets, particularly when SQL queries involve multi-hop reasoning and\nmultiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks\nto a strategic balance between generality of the reasoning process and\noptimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5\nmodel performs on par with 100+ Billion ones on the Bird dataset.\n", "link": "http://arxiv.org/abs/2504.15077v1", "date": "2025-04-21", "relevancy": 2.6005, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5367}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Think2SQL%3A%20Reinforce%20LLM%20Reasoning%20Capabilities%20for%20Text2SQL&body=Title%3A%20Think2SQL%3A%20Reinforce%20LLM%20Reasoning%20Capabilities%20for%20Text2SQL%0AAuthor%3A%20Simone%20Papicchio%20and%20Simone%20Rossi%20and%20Luca%20Cagliero%20and%20Paolo%20Papotti%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20in%0Atransforming%20natural%20language%20questions%20about%20relational%20databases%20into%20SQL%0Aqueries.%20Despite%20recent%20improvements%2C%20small%20LLMs%20struggle%20to%20handle%20questions%0Ainvolving%20multiple%20tables%20and%20complex%20SQL%20patterns%20under%20a%20Zero-Shot%20Learning%0A%28ZSL%29%20setting.%20Supervised%20Fine-Tuning%20%28SFT%29%20partially%20compensate%20the%20knowledge%0Adeficits%20in%20pretrained%20models%20but%20falls%20short%20while%20dealing%20with%20queries%0Ainvolving%20multi-hop%20reasoning.%20To%20bridge%20this%20gap%2C%20different%20LLM%20training%0Astrategies%20to%20reinforce%20reasoning%20capabilities%20have%20been%20proposed%2C%20ranging%20from%0Aleveraging%20a%20thinking%20process%20within%20ZSL%2C%20including%20reasoning%20traces%20in%20SFT%2C%20or%0Aadopt%20Reinforcement%20Learning%20%28RL%29%20strategies.%20However%2C%20the%20influence%20of%0Areasoning%20on%20Text2SQL%20performance%20is%20still%20largely%20unexplored.%20This%20paper%0Ainvestigates%20to%20what%20extent%20LLM%20reasoning%20capabilities%20influence%20their%20Text2SQL%0Aperformance%20on%20four%20benchmark%20datasets.%20To%20this%20end%2C%20it%20considers%20the%20following%0ALLM%20settings%3A%20%281%29%20ZSL%2C%20including%20general-purpose%20reasoning%20or%20not%3B%20%282%29%20SFT%2C%0Awith%20and%20without%20task-specific%20reasoning%20traces%3B%20%283%29%20RL%2C%20leveraging%20execution%0Aaccuracy%20as%20primary%20reward%20function%3B%20%284%29%20SFT%2BRL%2C%20i.e%2C%20a%20two-stage%20approach%20that%0Acombines%20SFT%20and%20RL.%20The%20results%20show%20that%20general-purpose%20reasoning%20under%20ZSL%0Aproves%20to%20be%20ineffective%20in%20tackling%20complex%20Text2SQL%20cases.%20Small%20LLMs%20benefit%0Afrom%20SFT%20with%20reasoning%20much%20more%20than%20larger%20ones%2C%20bridging%20the%20gap%20of%20their%0A%28weaker%29%20model%20pretraining.%20RL%20is%20generally%20beneficial%20across%20all%20tested%20models%0Aand%20datasets%2C%20particularly%20when%20SQL%20queries%20involve%20multi-hop%20reasoning%20and%0Amultiple%20tables.%20Small%20LLMs%20with%20SFT%2BRL%20excel%20on%20most%20complex%20datasets%20thanks%0Ato%20a%20strategic%20balance%20between%20generality%20of%20the%20reasoning%20process%20and%0Aoptimization%20of%20the%20execution%20accuracy.%20Thanks%20to%20RL%2C%20the7B%20Qwen-Coder-2.5%0Amodel%20performs%20on%20par%20with%20100%2B%20Billion%20ones%20on%20the%20Bird%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThink2SQL%253A%2520Reinforce%2520LLM%2520Reasoning%2520Capabilities%2520for%2520Text2SQL%26entry.906535625%3DSimone%2520Papicchio%2520and%2520Simone%2520Rossi%2520and%2520Luca%2520Cagliero%2520and%2520Paolo%2520Papotti%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520impressive%2520capabilities%2520in%250Atransforming%2520natural%2520language%2520questions%2520about%2520relational%2520databases%2520into%2520SQL%250Aqueries.%2520Despite%2520recent%2520improvements%252C%2520small%2520LLMs%2520struggle%2520to%2520handle%2520questions%250Ainvolving%2520multiple%2520tables%2520and%2520complex%2520SQL%2520patterns%2520under%2520a%2520Zero-Shot%2520Learning%250A%2528ZSL%2529%2520setting.%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520partially%2520compensate%2520the%2520knowledge%250Adeficits%2520in%2520pretrained%2520models%2520but%2520falls%2520short%2520while%2520dealing%2520with%2520queries%250Ainvolving%2520multi-hop%2520reasoning.%2520To%2520bridge%2520this%2520gap%252C%2520different%2520LLM%2520training%250Astrategies%2520to%2520reinforce%2520reasoning%2520capabilities%2520have%2520been%2520proposed%252C%2520ranging%2520from%250Aleveraging%2520a%2520thinking%2520process%2520within%2520ZSL%252C%2520including%2520reasoning%2520traces%2520in%2520SFT%252C%2520or%250Aadopt%2520Reinforcement%2520Learning%2520%2528RL%2529%2520strategies.%2520However%252C%2520the%2520influence%2520of%250Areasoning%2520on%2520Text2SQL%2520performance%2520is%2520still%2520largely%2520unexplored.%2520This%2520paper%250Ainvestigates%2520to%2520what%2520extent%2520LLM%2520reasoning%2520capabilities%2520influence%2520their%2520Text2SQL%250Aperformance%2520on%2520four%2520benchmark%2520datasets.%2520To%2520this%2520end%252C%2520it%2520considers%2520the%2520following%250ALLM%2520settings%253A%2520%25281%2529%2520ZSL%252C%2520including%2520general-purpose%2520reasoning%2520or%2520not%253B%2520%25282%2529%2520SFT%252C%250Awith%2520and%2520without%2520task-specific%2520reasoning%2520traces%253B%2520%25283%2529%2520RL%252C%2520leveraging%2520execution%250Aaccuracy%2520as%2520primary%2520reward%2520function%253B%2520%25284%2529%2520SFT%252BRL%252C%2520i.e%252C%2520a%2520two-stage%2520approach%2520that%250Acombines%2520SFT%2520and%2520RL.%2520The%2520results%2520show%2520that%2520general-purpose%2520reasoning%2520under%2520ZSL%250Aproves%2520to%2520be%2520ineffective%2520in%2520tackling%2520complex%2520Text2SQL%2520cases.%2520Small%2520LLMs%2520benefit%250Afrom%2520SFT%2520with%2520reasoning%2520much%2520more%2520than%2520larger%2520ones%252C%2520bridging%2520the%2520gap%2520of%2520their%250A%2528weaker%2529%2520model%2520pretraining.%2520RL%2520is%2520generally%2520beneficial%2520across%2520all%2520tested%2520models%250Aand%2520datasets%252C%2520particularly%2520when%2520SQL%2520queries%2520involve%2520multi-hop%2520reasoning%2520and%250Amultiple%2520tables.%2520Small%2520LLMs%2520with%2520SFT%252BRL%2520excel%2520on%2520most%2520complex%2520datasets%2520thanks%250Ato%2520a%2520strategic%2520balance%2520between%2520generality%2520of%2520the%2520reasoning%2520process%2520and%250Aoptimization%2520of%2520the%2520execution%2520accuracy.%2520Thanks%2520to%2520RL%252C%2520the7B%2520Qwen-Coder-2.5%250Amodel%2520performs%2520on%2520par%2520with%2520100%252B%2520Billion%2520ones%2520on%2520the%2520Bird%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Think2SQL%3A%20Reinforce%20LLM%20Reasoning%20Capabilities%20for%20Text2SQL&entry.906535625=Simone%20Papicchio%20and%20Simone%20Rossi%20and%20Luca%20Cagliero%20and%20Paolo%20Papotti&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20impressive%20capabilities%20in%0Atransforming%20natural%20language%20questions%20about%20relational%20databases%20into%20SQL%0Aqueries.%20Despite%20recent%20improvements%2C%20small%20LLMs%20struggle%20to%20handle%20questions%0Ainvolving%20multiple%20tables%20and%20complex%20SQL%20patterns%20under%20a%20Zero-Shot%20Learning%0A%28ZSL%29%20setting.%20Supervised%20Fine-Tuning%20%28SFT%29%20partially%20compensate%20the%20knowledge%0Adeficits%20in%20pretrained%20models%20but%20falls%20short%20while%20dealing%20with%20queries%0Ainvolving%20multi-hop%20reasoning.%20To%20bridge%20this%20gap%2C%20different%20LLM%20training%0Astrategies%20to%20reinforce%20reasoning%20capabilities%20have%20been%20proposed%2C%20ranging%20from%0Aleveraging%20a%20thinking%20process%20within%20ZSL%2C%20including%20reasoning%20traces%20in%20SFT%2C%20or%0Aadopt%20Reinforcement%20Learning%20%28RL%29%20strategies.%20However%2C%20the%20influence%20of%0Areasoning%20on%20Text2SQL%20performance%20is%20still%20largely%20unexplored.%20This%20paper%0Ainvestigates%20to%20what%20extent%20LLM%20reasoning%20capabilities%20influence%20their%20Text2SQL%0Aperformance%20on%20four%20benchmark%20datasets.%20To%20this%20end%2C%20it%20considers%20the%20following%0ALLM%20settings%3A%20%281%29%20ZSL%2C%20including%20general-purpose%20reasoning%20or%20not%3B%20%282%29%20SFT%2C%0Awith%20and%20without%20task-specific%20reasoning%20traces%3B%20%283%29%20RL%2C%20leveraging%20execution%0Aaccuracy%20as%20primary%20reward%20function%3B%20%284%29%20SFT%2BRL%2C%20i.e%2C%20a%20two-stage%20approach%20that%0Acombines%20SFT%20and%20RL.%20The%20results%20show%20that%20general-purpose%20reasoning%20under%20ZSL%0Aproves%20to%20be%20ineffective%20in%20tackling%20complex%20Text2SQL%20cases.%20Small%20LLMs%20benefit%0Afrom%20SFT%20with%20reasoning%20much%20more%20than%20larger%20ones%2C%20bridging%20the%20gap%20of%20their%0A%28weaker%29%20model%20pretraining.%20RL%20is%20generally%20beneficial%20across%20all%20tested%20models%0Aand%20datasets%2C%20particularly%20when%20SQL%20queries%20involve%20multi-hop%20reasoning%20and%0Amultiple%20tables.%20Small%20LLMs%20with%20SFT%2BRL%20excel%20on%20most%20complex%20datasets%20thanks%0Ato%20a%20strategic%20balance%20between%20generality%20of%20the%20reasoning%20process%20and%0Aoptimization%20of%20the%20execution%20accuracy.%20Thanks%20to%20RL%2C%20the7B%20Qwen-Coder-2.5%0Amodel%20performs%20on%20par%20with%20100%2B%20Billion%20ones%20on%20the%20Bird%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15077v1&entry.124074799=Read"},
{"title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention", "author": "Shang Yang and Junxian Guo and Haotian Tang and Qinghao Hu and Guangxuan Xiao and Jiaming Tang and Yujun Lin and Zhijian Liu and Yao Lu and Song Han", "abstract": "  Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.\n", "link": "http://arxiv.org/abs/2502.14866v2", "date": "2025-04-21", "relevancy": 2.5867, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5341}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5341}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LServe%3A%20Efficient%20Long-sequence%20LLM%20Serving%20with%20Unified%20Sparse%0A%20%20Attention&body=Title%3A%20LServe%3A%20Efficient%20Long-sequence%20LLM%20Serving%20with%20Unified%20Sparse%0A%20%20Attention%0AAuthor%3A%20Shang%20Yang%20and%20Junxian%20Guo%20and%20Haotian%20Tang%20and%20Qinghao%20Hu%20and%20Guangxuan%20Xiao%20and%20Jiaming%20Tang%20and%20Yujun%20Lin%20and%20Zhijian%20Liu%20and%20Yao%20Lu%20and%20Song%20Han%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20potential%20in%20processing%0Along%20sequences%20and%20complex%20reasoning%20tasks%2C%20yet%20efficiently%20serving%20these%0Amodels%20remains%20challenging%20due%20to%20the%20quadratic%20computational%20complexity%20of%0Aattention%20in%20the%20prefilling%20stage%20and%20the%20large%20memory%20footprint%20of%20the%20KV%0Acache%20in%20the%20decoding%20stage.%20To%20address%20these%20issues%2C%20we%20introduce%20LServe%2C%20an%0Aefficient%20system%20that%20accelerates%20long-sequence%20LLM%20serving%20via%20hybrid%20sparse%0Aattention.%20This%20method%20unifies%20different%20hardware-friendly%2C%20structured%20sparsity%0Apatterns%20for%20both%20prefilling%20and%20decoding%20attention%20into%20a%20single%20framework%2C%0Awhere%20computations%20on%20less%20important%20tokens%20are%20skipped%20block-wise.%20LServe%0Ademonstrates%20the%20compatibility%20of%20static%20and%20dynamic%20sparsity%20in%20long-context%0ALLM%20attention.%20This%20design%20enables%20multiplicative%20speedups%20by%20combining%20these%0Aoptimizations.%20Specifically%2C%20we%20convert%20half%20of%20the%20attention%20heads%20to%20nearly%0Afree%20streaming%20heads%20in%20both%20the%20prefilling%20and%20decoding%20stages.%20Additionally%2C%0Awe%20find%20that%20only%20a%20constant%20number%20of%20KV%20pages%20is%20required%20to%20preserve%0Along-context%20and%20reasoning%20capabilities%2C%20irrespective%20of%20context%20length.%20We%0Athen%20design%20a%20hierarchical%20KV%20page%20selection%20policy%20that%20dynamically%20prunes%20KV%0Apages%20based%20on%20query-centric%20similarity.%20On%20average%2C%20LServe%20accelerates%20LLM%0Aprefilling%20by%20up%20to%202.9x%20and%20decoding%20by%201.3-2.1x%20over%20vLLM%2C%20maintaining%0Along-context%20accuracy.%20Code%20is%20released%20at%0Ahttps%3A//github.com/mit-han-lab/omniserve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.14866v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLServe%253A%2520Efficient%2520Long-sequence%2520LLM%2520Serving%2520with%2520Unified%2520Sparse%250A%2520%2520Attention%26entry.906535625%3DShang%2520Yang%2520and%2520Junxian%2520Guo%2520and%2520Haotian%2520Tang%2520and%2520Qinghao%2520Hu%2520and%2520Guangxuan%2520Xiao%2520and%2520Jiaming%2520Tang%2520and%2520Yujun%2520Lin%2520and%2520Zhijian%2520Liu%2520and%2520Yao%2520Lu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520potential%2520in%2520processing%250Along%2520sequences%2520and%2520complex%2520reasoning%2520tasks%252C%2520yet%2520efficiently%2520serving%2520these%250Amodels%2520remains%2520challenging%2520due%2520to%2520the%2520quadratic%2520computational%2520complexity%2520of%250Aattention%2520in%2520the%2520prefilling%2520stage%2520and%2520the%2520large%2520memory%2520footprint%2520of%2520the%2520KV%250Acache%2520in%2520the%2520decoding%2520stage.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520LServe%252C%2520an%250Aefficient%2520system%2520that%2520accelerates%2520long-sequence%2520LLM%2520serving%2520via%2520hybrid%2520sparse%250Aattention.%2520This%2520method%2520unifies%2520different%2520hardware-friendly%252C%2520structured%2520sparsity%250Apatterns%2520for%2520both%2520prefilling%2520and%2520decoding%2520attention%2520into%2520a%2520single%2520framework%252C%250Awhere%2520computations%2520on%2520less%2520important%2520tokens%2520are%2520skipped%2520block-wise.%2520LServe%250Ademonstrates%2520the%2520compatibility%2520of%2520static%2520and%2520dynamic%2520sparsity%2520in%2520long-context%250ALLM%2520attention.%2520This%2520design%2520enables%2520multiplicative%2520speedups%2520by%2520combining%2520these%250Aoptimizations.%2520Specifically%252C%2520we%2520convert%2520half%2520of%2520the%2520attention%2520heads%2520to%2520nearly%250Afree%2520streaming%2520heads%2520in%2520both%2520the%2520prefilling%2520and%2520decoding%2520stages.%2520Additionally%252C%250Awe%2520find%2520that%2520only%2520a%2520constant%2520number%2520of%2520KV%2520pages%2520is%2520required%2520to%2520preserve%250Along-context%2520and%2520reasoning%2520capabilities%252C%2520irrespective%2520of%2520context%2520length.%2520We%250Athen%2520design%2520a%2520hierarchical%2520KV%2520page%2520selection%2520policy%2520that%2520dynamically%2520prunes%2520KV%250Apages%2520based%2520on%2520query-centric%2520similarity.%2520On%2520average%252C%2520LServe%2520accelerates%2520LLM%250Aprefilling%2520by%2520up%2520to%25202.9x%2520and%2520decoding%2520by%25201.3-2.1x%2520over%2520vLLM%252C%2520maintaining%250Along-context%2520accuracy.%2520Code%2520is%2520released%2520at%250Ahttps%253A//github.com/mit-han-lab/omniserve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.14866v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LServe%3A%20Efficient%20Long-sequence%20LLM%20Serving%20with%20Unified%20Sparse%0A%20%20Attention&entry.906535625=Shang%20Yang%20and%20Junxian%20Guo%20and%20Haotian%20Tang%20and%20Qinghao%20Hu%20and%20Guangxuan%20Xiao%20and%20Jiaming%20Tang%20and%20Yujun%20Lin%20and%20Zhijian%20Liu%20and%20Yao%20Lu%20and%20Song%20Han&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20potential%20in%20processing%0Along%20sequences%20and%20complex%20reasoning%20tasks%2C%20yet%20efficiently%20serving%20these%0Amodels%20remains%20challenging%20due%20to%20the%20quadratic%20computational%20complexity%20of%0Aattention%20in%20the%20prefilling%20stage%20and%20the%20large%20memory%20footprint%20of%20the%20KV%0Acache%20in%20the%20decoding%20stage.%20To%20address%20these%20issues%2C%20we%20introduce%20LServe%2C%20an%0Aefficient%20system%20that%20accelerates%20long-sequence%20LLM%20serving%20via%20hybrid%20sparse%0Aattention.%20This%20method%20unifies%20different%20hardware-friendly%2C%20structured%20sparsity%0Apatterns%20for%20both%20prefilling%20and%20decoding%20attention%20into%20a%20single%20framework%2C%0Awhere%20computations%20on%20less%20important%20tokens%20are%20skipped%20block-wise.%20LServe%0Ademonstrates%20the%20compatibility%20of%20static%20and%20dynamic%20sparsity%20in%20long-context%0ALLM%20attention.%20This%20design%20enables%20multiplicative%20speedups%20by%20combining%20these%0Aoptimizations.%20Specifically%2C%20we%20convert%20half%20of%20the%20attention%20heads%20to%20nearly%0Afree%20streaming%20heads%20in%20both%20the%20prefilling%20and%20decoding%20stages.%20Additionally%2C%0Awe%20find%20that%20only%20a%20constant%20number%20of%20KV%20pages%20is%20required%20to%20preserve%0Along-context%20and%20reasoning%20capabilities%2C%20irrespective%20of%20context%20length.%20We%0Athen%20design%20a%20hierarchical%20KV%20page%20selection%20policy%20that%20dynamically%20prunes%20KV%0Apages%20based%20on%20query-centric%20similarity.%20On%20average%2C%20LServe%20accelerates%20LLM%0Aprefilling%20by%20up%20to%202.9x%20and%20decoding%20by%201.3-2.1x%20over%20vLLM%2C%20maintaining%0Along-context%20accuracy.%20Code%20is%20released%20at%0Ahttps%3A//github.com/mit-han-lab/omniserve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.14866v2&entry.124074799=Read"},
{"title": "A Deep Learning Framework for Sequence Mining with Bidirectional LSTM\n  and Multi-Scale Attention", "author": "Tao Yang and Yu Cheng and Yaokun Ren and Yujia Lou and Minggu Wei and Honghui Xin", "abstract": "  This paper addresses the challenges of mining latent patterns and modeling\ncontextual dependencies in complex sequence data. A sequence pattern mining\nalgorithm is proposed by integrating Bidirectional Long Short-Term Memory\n(BiLSTM) with a multi-scale attention mechanism. The BiLSTM captures both\nforward and backward dependencies in sequences, enhancing the model's ability\nto perceive global contextual structures. At the same time, the multi-scale\nattention module assigns adaptive weights to key feature regions under\ndifferent window sizes. This improves the model's responsiveness to both local\nand global important information. Extensive experiments are conducted on a\npublicly available multivariate time series dataset. The proposed model is\ncompared with several mainstream sequence modeling methods. Results show that\nit outperforms existing models in terms of accuracy, precision, and recall.\nThis confirms the effectiveness and robustness of the proposed architecture in\ncomplex pattern recognition tasks. Further ablation studies and sensitivity\nanalyses are carried out to investigate the effects of attention scale and\ninput sequence length on model performance. These results provide empirical\nsupport for structural optimization of the model.\n", "link": "http://arxiv.org/abs/2504.15223v1", "date": "2025-04-21", "relevancy": 2.5539, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5263}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Deep%20Learning%20Framework%20for%20Sequence%20Mining%20with%20Bidirectional%20LSTM%0A%20%20and%20Multi-Scale%20Attention&body=Title%3A%20A%20Deep%20Learning%20Framework%20for%20Sequence%20Mining%20with%20Bidirectional%20LSTM%0A%20%20and%20Multi-Scale%20Attention%0AAuthor%3A%20Tao%20Yang%20and%20Yu%20Cheng%20and%20Yaokun%20Ren%20and%20Yujia%20Lou%20and%20Minggu%20Wei%20and%20Honghui%20Xin%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenges%20of%20mining%20latent%20patterns%20and%20modeling%0Acontextual%20dependencies%20in%20complex%20sequence%20data.%20A%20sequence%20pattern%20mining%0Aalgorithm%20is%20proposed%20by%20integrating%20Bidirectional%20Long%20Short-Term%20Memory%0A%28BiLSTM%29%20with%20a%20multi-scale%20attention%20mechanism.%20The%20BiLSTM%20captures%20both%0Aforward%20and%20backward%20dependencies%20in%20sequences%2C%20enhancing%20the%20model%27s%20ability%0Ato%20perceive%20global%20contextual%20structures.%20At%20the%20same%20time%2C%20the%20multi-scale%0Aattention%20module%20assigns%20adaptive%20weights%20to%20key%20feature%20regions%20under%0Adifferent%20window%20sizes.%20This%20improves%20the%20model%27s%20responsiveness%20to%20both%20local%0Aand%20global%20important%20information.%20Extensive%20experiments%20are%20conducted%20on%20a%0Apublicly%20available%20multivariate%20time%20series%20dataset.%20The%20proposed%20model%20is%0Acompared%20with%20several%20mainstream%20sequence%20modeling%20methods.%20Results%20show%20that%0Ait%20outperforms%20existing%20models%20in%20terms%20of%20accuracy%2C%20precision%2C%20and%20recall.%0AThis%20confirms%20the%20effectiveness%20and%20robustness%20of%20the%20proposed%20architecture%20in%0Acomplex%20pattern%20recognition%20tasks.%20Further%20ablation%20studies%20and%20sensitivity%0Aanalyses%20are%20carried%20out%20to%20investigate%20the%20effects%20of%20attention%20scale%20and%0Ainput%20sequence%20length%20on%20model%20performance.%20These%20results%20provide%20empirical%0Asupport%20for%20structural%20optimization%20of%20the%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Deep%2520Learning%2520Framework%2520for%2520Sequence%2520Mining%2520with%2520Bidirectional%2520LSTM%250A%2520%2520and%2520Multi-Scale%2520Attention%26entry.906535625%3DTao%2520Yang%2520and%2520Yu%2520Cheng%2520and%2520Yaokun%2520Ren%2520and%2520Yujia%2520Lou%2520and%2520Minggu%2520Wei%2520and%2520Honghui%2520Xin%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenges%2520of%2520mining%2520latent%2520patterns%2520and%2520modeling%250Acontextual%2520dependencies%2520in%2520complex%2520sequence%2520data.%2520A%2520sequence%2520pattern%2520mining%250Aalgorithm%2520is%2520proposed%2520by%2520integrating%2520Bidirectional%2520Long%2520Short-Term%2520Memory%250A%2528BiLSTM%2529%2520with%2520a%2520multi-scale%2520attention%2520mechanism.%2520The%2520BiLSTM%2520captures%2520both%250Aforward%2520and%2520backward%2520dependencies%2520in%2520sequences%252C%2520enhancing%2520the%2520model%2527s%2520ability%250Ato%2520perceive%2520global%2520contextual%2520structures.%2520At%2520the%2520same%2520time%252C%2520the%2520multi-scale%250Aattention%2520module%2520assigns%2520adaptive%2520weights%2520to%2520key%2520feature%2520regions%2520under%250Adifferent%2520window%2520sizes.%2520This%2520improves%2520the%2520model%2527s%2520responsiveness%2520to%2520both%2520local%250Aand%2520global%2520important%2520information.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520a%250Apublicly%2520available%2520multivariate%2520time%2520series%2520dataset.%2520The%2520proposed%2520model%2520is%250Acompared%2520with%2520several%2520mainstream%2520sequence%2520modeling%2520methods.%2520Results%2520show%2520that%250Ait%2520outperforms%2520existing%2520models%2520in%2520terms%2520of%2520accuracy%252C%2520precision%252C%2520and%2520recall.%250AThis%2520confirms%2520the%2520effectiveness%2520and%2520robustness%2520of%2520the%2520proposed%2520architecture%2520in%250Acomplex%2520pattern%2520recognition%2520tasks.%2520Further%2520ablation%2520studies%2520and%2520sensitivity%250Aanalyses%2520are%2520carried%2520out%2520to%2520investigate%2520the%2520effects%2520of%2520attention%2520scale%2520and%250Ainput%2520sequence%2520length%2520on%2520model%2520performance.%2520These%2520results%2520provide%2520empirical%250Asupport%2520for%2520structural%2520optimization%2520of%2520the%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Deep%20Learning%20Framework%20for%20Sequence%20Mining%20with%20Bidirectional%20LSTM%0A%20%20and%20Multi-Scale%20Attention&entry.906535625=Tao%20Yang%20and%20Yu%20Cheng%20and%20Yaokun%20Ren%20and%20Yujia%20Lou%20and%20Minggu%20Wei%20and%20Honghui%20Xin&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenges%20of%20mining%20latent%20patterns%20and%20modeling%0Acontextual%20dependencies%20in%20complex%20sequence%20data.%20A%20sequence%20pattern%20mining%0Aalgorithm%20is%20proposed%20by%20integrating%20Bidirectional%20Long%20Short-Term%20Memory%0A%28BiLSTM%29%20with%20a%20multi-scale%20attention%20mechanism.%20The%20BiLSTM%20captures%20both%0Aforward%20and%20backward%20dependencies%20in%20sequences%2C%20enhancing%20the%20model%27s%20ability%0Ato%20perceive%20global%20contextual%20structures.%20At%20the%20same%20time%2C%20the%20multi-scale%0Aattention%20module%20assigns%20adaptive%20weights%20to%20key%20feature%20regions%20under%0Adifferent%20window%20sizes.%20This%20improves%20the%20model%27s%20responsiveness%20to%20both%20local%0Aand%20global%20important%20information.%20Extensive%20experiments%20are%20conducted%20on%20a%0Apublicly%20available%20multivariate%20time%20series%20dataset.%20The%20proposed%20model%20is%0Acompared%20with%20several%20mainstream%20sequence%20modeling%20methods.%20Results%20show%20that%0Ait%20outperforms%20existing%20models%20in%20terms%20of%20accuracy%2C%20precision%2C%20and%20recall.%0AThis%20confirms%20the%20effectiveness%20and%20robustness%20of%20the%20proposed%20architecture%20in%0Acomplex%20pattern%20recognition%20tasks.%20Further%20ablation%20studies%20and%20sensitivity%0Aanalyses%20are%20carried%20out%20to%20investigate%20the%20effects%20of%20attention%20scale%20and%0Ainput%20sequence%20length%20on%20model%20performance.%20These%20results%20provide%20empirical%0Asupport%20for%20structural%20optimization%20of%20the%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15223v1&entry.124074799=Read"},
{"title": "HSANET: A Hybrid Self-Cross Attention Network For Remote Sensing Change\n  Detection", "author": "Chengxi Han and Xiaoyu Su and Zhiqiang Wei and Meiqi Hu and Yichu Xu", "abstract": "  The remote sensing image change detection task is an essential method for\nlarge-scale monitoring. We propose HSANet, a network that uses hierarchical\nconvolution to extract multi-scale features. It incorporates hybrid\nself-attention and cross-attention mechanisms to learn and fuse global and\ncross-scale information. This enables HSANet to capture global context at\ndifferent scales and integrate cross-scale features, refining edge details and\nimproving detection performance. We will also open-source our model code:\nhttps://github.com/ChengxiHAN/HSANet.\n", "link": "http://arxiv.org/abs/2504.15170v1", "date": "2025-04-21", "relevancy": 2.5188, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5128}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5029}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HSANET%3A%20A%20Hybrid%20Self-Cross%20Attention%20Network%20For%20Remote%20Sensing%20Change%0A%20%20Detection&body=Title%3A%20HSANET%3A%20A%20Hybrid%20Self-Cross%20Attention%20Network%20For%20Remote%20Sensing%20Change%0A%20%20Detection%0AAuthor%3A%20Chengxi%20Han%20and%20Xiaoyu%20Su%20and%20Zhiqiang%20Wei%20and%20Meiqi%20Hu%20and%20Yichu%20Xu%0AAbstract%3A%20%20%20The%20remote%20sensing%20image%20change%20detection%20task%20is%20an%20essential%20method%20for%0Alarge-scale%20monitoring.%20We%20propose%20HSANet%2C%20a%20network%20that%20uses%20hierarchical%0Aconvolution%20to%20extract%20multi-scale%20features.%20It%20incorporates%20hybrid%0Aself-attention%20and%20cross-attention%20mechanisms%20to%20learn%20and%20fuse%20global%20and%0Across-scale%20information.%20This%20enables%20HSANet%20to%20capture%20global%20context%20at%0Adifferent%20scales%20and%20integrate%20cross-scale%20features%2C%20refining%20edge%20details%20and%0Aimproving%20detection%20performance.%20We%20will%20also%20open-source%20our%20model%20code%3A%0Ahttps%3A//github.com/ChengxiHAN/HSANet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHSANET%253A%2520A%2520Hybrid%2520Self-Cross%2520Attention%2520Network%2520For%2520Remote%2520Sensing%2520Change%250A%2520%2520Detection%26entry.906535625%3DChengxi%2520Han%2520and%2520Xiaoyu%2520Su%2520and%2520Zhiqiang%2520Wei%2520and%2520Meiqi%2520Hu%2520and%2520Yichu%2520Xu%26entry.1292438233%3D%2520%2520The%2520remote%2520sensing%2520image%2520change%2520detection%2520task%2520is%2520an%2520essential%2520method%2520for%250Alarge-scale%2520monitoring.%2520We%2520propose%2520HSANet%252C%2520a%2520network%2520that%2520uses%2520hierarchical%250Aconvolution%2520to%2520extract%2520multi-scale%2520features.%2520It%2520incorporates%2520hybrid%250Aself-attention%2520and%2520cross-attention%2520mechanisms%2520to%2520learn%2520and%2520fuse%2520global%2520and%250Across-scale%2520information.%2520This%2520enables%2520HSANet%2520to%2520capture%2520global%2520context%2520at%250Adifferent%2520scales%2520and%2520integrate%2520cross-scale%2520features%252C%2520refining%2520edge%2520details%2520and%250Aimproving%2520detection%2520performance.%2520We%2520will%2520also%2520open-source%2520our%2520model%2520code%253A%250Ahttps%253A//github.com/ChengxiHAN/HSANet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HSANET%3A%20A%20Hybrid%20Self-Cross%20Attention%20Network%20For%20Remote%20Sensing%20Change%0A%20%20Detection&entry.906535625=Chengxi%20Han%20and%20Xiaoyu%20Su%20and%20Zhiqiang%20Wei%20and%20Meiqi%20Hu%20and%20Yichu%20Xu&entry.1292438233=%20%20The%20remote%20sensing%20image%20change%20detection%20task%20is%20an%20essential%20method%20for%0Alarge-scale%20monitoring.%20We%20propose%20HSANet%2C%20a%20network%20that%20uses%20hierarchical%0Aconvolution%20to%20extract%20multi-scale%20features.%20It%20incorporates%20hybrid%0Aself-attention%20and%20cross-attention%20mechanisms%20to%20learn%20and%20fuse%20global%20and%0Across-scale%20information.%20This%20enables%20HSANet%20to%20capture%20global%20context%20at%0Adifferent%20scales%20and%20integrate%20cross-scale%20features%2C%20refining%20edge%20details%20and%0Aimproving%20detection%20performance.%20We%20will%20also%20open-source%20our%20model%20code%3A%0Ahttps%3A//github.com/ChengxiHAN/HSANet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15170v1&entry.124074799=Read"},
{"title": "Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset\n  Generation", "author": "Yunxuan Cai and Sitao Xiang and Zongjian Li and Haiwei Chen and Yajie Zhao", "abstract": "  Digital modeling and reconstruction of human faces serve various\napplications. However, its availability is often hindered by the requirements\nof data capturing devices, manual labor, and suitable actors. This situation\nrestricts the diversity, expressiveness, and control over the resulting models.\nThis work aims to demonstrate that a semantically controllable generative\nnetwork can provide enhanced control over the digital face modeling process. To\nenhance diversity beyond the limited human faces scanned in a controlled\nsetting, we introduce a novel data generation pipeline that creates a\nhigh-quality 3D face database using a pre-trained diffusion model. Our proposed\nnormalization module converts synthesized data from the diffusion model into\nhigh-quality scanned data. Using the 44,000 face models we obtained, we further\ndeveloped an efficient GAN-based generator. This generator accepts semantic\nattributes as input, and generates geometry and albedo. It also allows\ncontinuous post-editing of attributes in the latent space. Our asset refinement\ncomponent subsequently creates physically-based facial assets. We introduce a\ncomprehensive system designed for creating and editing high-quality face\nassets. Our proposed model has undergone extensive experiment, comparison and\nevaluation. We also integrate everything into a web-based interactive tool. We\naim to make this tool publicly available with the release of the paper.\n", "link": "http://arxiv.org/abs/2504.15259v1", "date": "2025-04-21", "relevancy": 2.5007, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6341}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6328}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bringing%20Diversity%20from%20Diffusion%20Models%20to%20Semantic-Guided%20Face%20Asset%0A%20%20Generation&body=Title%3A%20Bringing%20Diversity%20from%20Diffusion%20Models%20to%20Semantic-Guided%20Face%20Asset%0A%20%20Generation%0AAuthor%3A%20Yunxuan%20Cai%20and%20Sitao%20Xiang%20and%20Zongjian%20Li%20and%20Haiwei%20Chen%20and%20Yajie%20Zhao%0AAbstract%3A%20%20%20Digital%20modeling%20and%20reconstruction%20of%20human%20faces%20serve%20various%0Aapplications.%20However%2C%20its%20availability%20is%20often%20hindered%20by%20the%20requirements%0Aof%20data%20capturing%20devices%2C%20manual%20labor%2C%20and%20suitable%20actors.%20This%20situation%0Arestricts%20the%20diversity%2C%20expressiveness%2C%20and%20control%20over%20the%20resulting%20models.%0AThis%20work%20aims%20to%20demonstrate%20that%20a%20semantically%20controllable%20generative%0Anetwork%20can%20provide%20enhanced%20control%20over%20the%20digital%20face%20modeling%20process.%20To%0Aenhance%20diversity%20beyond%20the%20limited%20human%20faces%20scanned%20in%20a%20controlled%0Asetting%2C%20we%20introduce%20a%20novel%20data%20generation%20pipeline%20that%20creates%20a%0Ahigh-quality%203D%20face%20database%20using%20a%20pre-trained%20diffusion%20model.%20Our%20proposed%0Anormalization%20module%20converts%20synthesized%20data%20from%20the%20diffusion%20model%20into%0Ahigh-quality%20scanned%20data.%20Using%20the%2044%2C000%20face%20models%20we%20obtained%2C%20we%20further%0Adeveloped%20an%20efficient%20GAN-based%20generator.%20This%20generator%20accepts%20semantic%0Aattributes%20as%20input%2C%20and%20generates%20geometry%20and%20albedo.%20It%20also%20allows%0Acontinuous%20post-editing%20of%20attributes%20in%20the%20latent%20space.%20Our%20asset%20refinement%0Acomponent%20subsequently%20creates%20physically-based%20facial%20assets.%20We%20introduce%20a%0Acomprehensive%20system%20designed%20for%20creating%20and%20editing%20high-quality%20face%0Aassets.%20Our%20proposed%20model%20has%20undergone%20extensive%20experiment%2C%20comparison%20and%0Aevaluation.%20We%20also%20integrate%20everything%20into%20a%20web-based%20interactive%20tool.%20We%0Aaim%20to%20make%20this%20tool%20publicly%20available%20with%20the%20release%20of%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15259v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBringing%2520Diversity%2520from%2520Diffusion%2520Models%2520to%2520Semantic-Guided%2520Face%2520Asset%250A%2520%2520Generation%26entry.906535625%3DYunxuan%2520Cai%2520and%2520Sitao%2520Xiang%2520and%2520Zongjian%2520Li%2520and%2520Haiwei%2520Chen%2520and%2520Yajie%2520Zhao%26entry.1292438233%3D%2520%2520Digital%2520modeling%2520and%2520reconstruction%2520of%2520human%2520faces%2520serve%2520various%250Aapplications.%2520However%252C%2520its%2520availability%2520is%2520often%2520hindered%2520by%2520the%2520requirements%250Aof%2520data%2520capturing%2520devices%252C%2520manual%2520labor%252C%2520and%2520suitable%2520actors.%2520This%2520situation%250Arestricts%2520the%2520diversity%252C%2520expressiveness%252C%2520and%2520control%2520over%2520the%2520resulting%2520models.%250AThis%2520work%2520aims%2520to%2520demonstrate%2520that%2520a%2520semantically%2520controllable%2520generative%250Anetwork%2520can%2520provide%2520enhanced%2520control%2520over%2520the%2520digital%2520face%2520modeling%2520process.%2520To%250Aenhance%2520diversity%2520beyond%2520the%2520limited%2520human%2520faces%2520scanned%2520in%2520a%2520controlled%250Asetting%252C%2520we%2520introduce%2520a%2520novel%2520data%2520generation%2520pipeline%2520that%2520creates%2520a%250Ahigh-quality%25203D%2520face%2520database%2520using%2520a%2520pre-trained%2520diffusion%2520model.%2520Our%2520proposed%250Anormalization%2520module%2520converts%2520synthesized%2520data%2520from%2520the%2520diffusion%2520model%2520into%250Ahigh-quality%2520scanned%2520data.%2520Using%2520the%252044%252C000%2520face%2520models%2520we%2520obtained%252C%2520we%2520further%250Adeveloped%2520an%2520efficient%2520GAN-based%2520generator.%2520This%2520generator%2520accepts%2520semantic%250Aattributes%2520as%2520input%252C%2520and%2520generates%2520geometry%2520and%2520albedo.%2520It%2520also%2520allows%250Acontinuous%2520post-editing%2520of%2520attributes%2520in%2520the%2520latent%2520space.%2520Our%2520asset%2520refinement%250Acomponent%2520subsequently%2520creates%2520physically-based%2520facial%2520assets.%2520We%2520introduce%2520a%250Acomprehensive%2520system%2520designed%2520for%2520creating%2520and%2520editing%2520high-quality%2520face%250Aassets.%2520Our%2520proposed%2520model%2520has%2520undergone%2520extensive%2520experiment%252C%2520comparison%2520and%250Aevaluation.%2520We%2520also%2520integrate%2520everything%2520into%2520a%2520web-based%2520interactive%2520tool.%2520We%250Aaim%2520to%2520make%2520this%2520tool%2520publicly%2520available%2520with%2520the%2520release%2520of%2520the%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15259v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bringing%20Diversity%20from%20Diffusion%20Models%20to%20Semantic-Guided%20Face%20Asset%0A%20%20Generation&entry.906535625=Yunxuan%20Cai%20and%20Sitao%20Xiang%20and%20Zongjian%20Li%20and%20Haiwei%20Chen%20and%20Yajie%20Zhao&entry.1292438233=%20%20Digital%20modeling%20and%20reconstruction%20of%20human%20faces%20serve%20various%0Aapplications.%20However%2C%20its%20availability%20is%20often%20hindered%20by%20the%20requirements%0Aof%20data%20capturing%20devices%2C%20manual%20labor%2C%20and%20suitable%20actors.%20This%20situation%0Arestricts%20the%20diversity%2C%20expressiveness%2C%20and%20control%20over%20the%20resulting%20models.%0AThis%20work%20aims%20to%20demonstrate%20that%20a%20semantically%20controllable%20generative%0Anetwork%20can%20provide%20enhanced%20control%20over%20the%20digital%20face%20modeling%20process.%20To%0Aenhance%20diversity%20beyond%20the%20limited%20human%20faces%20scanned%20in%20a%20controlled%0Asetting%2C%20we%20introduce%20a%20novel%20data%20generation%20pipeline%20that%20creates%20a%0Ahigh-quality%203D%20face%20database%20using%20a%20pre-trained%20diffusion%20model.%20Our%20proposed%0Anormalization%20module%20converts%20synthesized%20data%20from%20the%20diffusion%20model%20into%0Ahigh-quality%20scanned%20data.%20Using%20the%2044%2C000%20face%20models%20we%20obtained%2C%20we%20further%0Adeveloped%20an%20efficient%20GAN-based%20generator.%20This%20generator%20accepts%20semantic%0Aattributes%20as%20input%2C%20and%20generates%20geometry%20and%20albedo.%20It%20also%20allows%0Acontinuous%20post-editing%20of%20attributes%20in%20the%20latent%20space.%20Our%20asset%20refinement%0Acomponent%20subsequently%20creates%20physically-based%20facial%20assets.%20We%20introduce%20a%0Acomprehensive%20system%20designed%20for%20creating%20and%20editing%20high-quality%20face%0Aassets.%20Our%20proposed%20model%20has%20undergone%20extensive%20experiment%2C%20comparison%20and%0Aevaluation.%20We%20also%20integrate%20everything%20into%20a%20web-based%20interactive%20tool.%20We%0Aaim%20to%20make%20this%20tool%20publicly%20available%20with%20the%20release%20of%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15259v1&entry.124074799=Read"},
{"title": "A Cognitive Paradigm Approach to Probe the Perception-Reasoning\n  Interface in VLMs", "author": "Mohit Vaishnav and Tanel Tammet", "abstract": "  A fundamental challenge in artificial intelligence involves understanding the\ncognitive processes underlying visual reasoning in sophisticated models like\nVision-Language Models (VLMs). How do these models integrate visual perception\nwith abstract thought, especially when reasoning across multiple images?\nDrawing inspiration from cognitive science, this paper introduces a structured\nevaluation framework using Bongard Problems (BPs) - a classic test of visual\nabstraction to dissect the perception-reasoning interface in VLMs. We propose\nthree distinct evaluation paradigms, mirroring human problem-solving\nstrategies: Direct Visual Rule Learning (DVRL; holistic processing), Deductive\nRule Learning (DRL; rule extraction and application), and Componential Analysis\n(CA; analytical decomposition via textual descriptions). These paradigms allow\nus to systematically vary the cognitive load and probe specific processing\nstages. Notably, the CA paradigm enables the evaluation of multi-image\nreasoning even in VLMs architecturally limited to single images and facilitates\nthe isolation of reasoning capabilities from perceptual limitations by\ncontrolling the descriptive input. Ablation studies further confirm that\nreasoning abilities improve significantly when perceptual challenges are\nmitigated. Our framework provides a valuable diagnostic tool, highlighting the\nneed to enhance visual processing fidelity for achieving more robust and\nhuman-like visual intelligence in AI.\n", "link": "http://arxiv.org/abs/2501.13620v3", "date": "2025-04-21", "relevancy": 2.4986, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6442}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Cognitive%20Paradigm%20Approach%20to%20Probe%20the%20Perception-Reasoning%0A%20%20Interface%20in%20VLMs&body=Title%3A%20A%20Cognitive%20Paradigm%20Approach%20to%20Probe%20the%20Perception-Reasoning%0A%20%20Interface%20in%20VLMs%0AAuthor%3A%20Mohit%20Vaishnav%20and%20Tanel%20Tammet%0AAbstract%3A%20%20%20A%20fundamental%20challenge%20in%20artificial%20intelligence%20involves%20understanding%20the%0Acognitive%20processes%20underlying%20visual%20reasoning%20in%20sophisticated%20models%20like%0AVision-Language%20Models%20%28VLMs%29.%20How%20do%20these%20models%20integrate%20visual%20perception%0Awith%20abstract%20thought%2C%20especially%20when%20reasoning%20across%20multiple%20images%3F%0ADrawing%20inspiration%20from%20cognitive%20science%2C%20this%20paper%20introduces%20a%20structured%0Aevaluation%20framework%20using%20Bongard%20Problems%20%28BPs%29%20-%20a%20classic%20test%20of%20visual%0Aabstraction%20to%20dissect%20the%20perception-reasoning%20interface%20in%20VLMs.%20We%20propose%0Athree%20distinct%20evaluation%20paradigms%2C%20mirroring%20human%20problem-solving%0Astrategies%3A%20Direct%20Visual%20Rule%20Learning%20%28DVRL%3B%20holistic%20processing%29%2C%20Deductive%0ARule%20Learning%20%28DRL%3B%20rule%20extraction%20and%20application%29%2C%20and%20Componential%20Analysis%0A%28CA%3B%20analytical%20decomposition%20via%20textual%20descriptions%29.%20These%20paradigms%20allow%0Aus%20to%20systematically%20vary%20the%20cognitive%20load%20and%20probe%20specific%20processing%0Astages.%20Notably%2C%20the%20CA%20paradigm%20enables%20the%20evaluation%20of%20multi-image%0Areasoning%20even%20in%20VLMs%20architecturally%20limited%20to%20single%20images%20and%20facilitates%0Athe%20isolation%20of%20reasoning%20capabilities%20from%20perceptual%20limitations%20by%0Acontrolling%20the%20descriptive%20input.%20Ablation%20studies%20further%20confirm%20that%0Areasoning%20abilities%20improve%20significantly%20when%20perceptual%20challenges%20are%0Amitigated.%20Our%20framework%20provides%20a%20valuable%20diagnostic%20tool%2C%20highlighting%20the%0Aneed%20to%20enhance%20visual%20processing%20fidelity%20for%20achieving%20more%20robust%20and%0Ahuman-like%20visual%20intelligence%20in%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13620v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Cognitive%2520Paradigm%2520Approach%2520to%2520Probe%2520the%2520Perception-Reasoning%250A%2520%2520Interface%2520in%2520VLMs%26entry.906535625%3DMohit%2520Vaishnav%2520and%2520Tanel%2520Tammet%26entry.1292438233%3D%2520%2520A%2520fundamental%2520challenge%2520in%2520artificial%2520intelligence%2520involves%2520understanding%2520the%250Acognitive%2520processes%2520underlying%2520visual%2520reasoning%2520in%2520sophisticated%2520models%2520like%250AVision-Language%2520Models%2520%2528VLMs%2529.%2520How%2520do%2520these%2520models%2520integrate%2520visual%2520perception%250Awith%2520abstract%2520thought%252C%2520especially%2520when%2520reasoning%2520across%2520multiple%2520images%253F%250ADrawing%2520inspiration%2520from%2520cognitive%2520science%252C%2520this%2520paper%2520introduces%2520a%2520structured%250Aevaluation%2520framework%2520using%2520Bongard%2520Problems%2520%2528BPs%2529%2520-%2520a%2520classic%2520test%2520of%2520visual%250Aabstraction%2520to%2520dissect%2520the%2520perception-reasoning%2520interface%2520in%2520VLMs.%2520We%2520propose%250Athree%2520distinct%2520evaluation%2520paradigms%252C%2520mirroring%2520human%2520problem-solving%250Astrategies%253A%2520Direct%2520Visual%2520Rule%2520Learning%2520%2528DVRL%253B%2520holistic%2520processing%2529%252C%2520Deductive%250ARule%2520Learning%2520%2528DRL%253B%2520rule%2520extraction%2520and%2520application%2529%252C%2520and%2520Componential%2520Analysis%250A%2528CA%253B%2520analytical%2520decomposition%2520via%2520textual%2520descriptions%2529.%2520These%2520paradigms%2520allow%250Aus%2520to%2520systematically%2520vary%2520the%2520cognitive%2520load%2520and%2520probe%2520specific%2520processing%250Astages.%2520Notably%252C%2520the%2520CA%2520paradigm%2520enables%2520the%2520evaluation%2520of%2520multi-image%250Areasoning%2520even%2520in%2520VLMs%2520architecturally%2520limited%2520to%2520single%2520images%2520and%2520facilitates%250Athe%2520isolation%2520of%2520reasoning%2520capabilities%2520from%2520perceptual%2520limitations%2520by%250Acontrolling%2520the%2520descriptive%2520input.%2520Ablation%2520studies%2520further%2520confirm%2520that%250Areasoning%2520abilities%2520improve%2520significantly%2520when%2520perceptual%2520challenges%2520are%250Amitigated.%2520Our%2520framework%2520provides%2520a%2520valuable%2520diagnostic%2520tool%252C%2520highlighting%2520the%250Aneed%2520to%2520enhance%2520visual%2520processing%2520fidelity%2520for%2520achieving%2520more%2520robust%2520and%250Ahuman-like%2520visual%2520intelligence%2520in%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13620v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Cognitive%20Paradigm%20Approach%20to%20Probe%20the%20Perception-Reasoning%0A%20%20Interface%20in%20VLMs&entry.906535625=Mohit%20Vaishnav%20and%20Tanel%20Tammet&entry.1292438233=%20%20A%20fundamental%20challenge%20in%20artificial%20intelligence%20involves%20understanding%20the%0Acognitive%20processes%20underlying%20visual%20reasoning%20in%20sophisticated%20models%20like%0AVision-Language%20Models%20%28VLMs%29.%20How%20do%20these%20models%20integrate%20visual%20perception%0Awith%20abstract%20thought%2C%20especially%20when%20reasoning%20across%20multiple%20images%3F%0ADrawing%20inspiration%20from%20cognitive%20science%2C%20this%20paper%20introduces%20a%20structured%0Aevaluation%20framework%20using%20Bongard%20Problems%20%28BPs%29%20-%20a%20classic%20test%20of%20visual%0Aabstraction%20to%20dissect%20the%20perception-reasoning%20interface%20in%20VLMs.%20We%20propose%0Athree%20distinct%20evaluation%20paradigms%2C%20mirroring%20human%20problem-solving%0Astrategies%3A%20Direct%20Visual%20Rule%20Learning%20%28DVRL%3B%20holistic%20processing%29%2C%20Deductive%0ARule%20Learning%20%28DRL%3B%20rule%20extraction%20and%20application%29%2C%20and%20Componential%20Analysis%0A%28CA%3B%20analytical%20decomposition%20via%20textual%20descriptions%29.%20These%20paradigms%20allow%0Aus%20to%20systematically%20vary%20the%20cognitive%20load%20and%20probe%20specific%20processing%0Astages.%20Notably%2C%20the%20CA%20paradigm%20enables%20the%20evaluation%20of%20multi-image%0Areasoning%20even%20in%20VLMs%20architecturally%20limited%20to%20single%20images%20and%20facilitates%0Athe%20isolation%20of%20reasoning%20capabilities%20from%20perceptual%20limitations%20by%0Acontrolling%20the%20descriptive%20input.%20Ablation%20studies%20further%20confirm%20that%0Areasoning%20abilities%20improve%20significantly%20when%20perceptual%20challenges%20are%0Amitigated.%20Our%20framework%20provides%20a%20valuable%20diagnostic%20tool%2C%20highlighting%20the%0Aneed%20to%20enhance%20visual%20processing%20fidelity%20for%20achieving%20more%20robust%20and%0Ahuman-like%20visual%20intelligence%20in%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13620v3&entry.124074799=Read"},
{"title": "Chinese-LiPS: A Chinese audio-visual speech recognition dataset with\n  Lip-reading and Presentation Slides", "author": "Jinghua Zhao and Yuhang Jia and Shiyao Wang and Jiaming Zhou and Hui Wang and Yong Qin", "abstract": "  Incorporating visual modalities to assist Automatic Speech Recognition (ASR)\ntasks has led to significant improvements. However, existing Audio-Visual\nSpeech Recognition (AVSR) datasets and methods typically rely solely on\nlip-reading information or speaking contextual video, neglecting the potential\nof combining these different valuable visual cues within the speaking context.\nIn this paper, we release a multimodal Chinese AVSR dataset, Chinese-LiPS,\ncomprising 100 hours of speech, video, and corresponding manual transcription,\nwith the visual modality encompassing both lip-reading information and the\npresentation slides used by the speaker. Based on Chinese-LiPS, we develop a\nsimple yet effective pipeline, LiPS-AVSR, which leverages both lip-reading and\npresentation slide information as visual modalities for AVSR tasks. Experiments\nshow that lip-reading and presentation slide information improve ASR\nperformance by approximately 8\\% and 25\\%, respectively, with a combined\nperformance improvement of about 35\\%. The dataset is available at\nhttps://kiri0824.github.io/Chinese-LiPS/\n", "link": "http://arxiv.org/abs/2504.15066v1", "date": "2025-04-21", "relevancy": 2.4953, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5102}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chinese-LiPS%3A%20A%20Chinese%20audio-visual%20speech%20recognition%20dataset%20with%0A%20%20Lip-reading%20and%20Presentation%20Slides&body=Title%3A%20Chinese-LiPS%3A%20A%20Chinese%20audio-visual%20speech%20recognition%20dataset%20with%0A%20%20Lip-reading%20and%20Presentation%20Slides%0AAuthor%3A%20Jinghua%20Zhao%20and%20Yuhang%20Jia%20and%20Shiyao%20Wang%20and%20Jiaming%20Zhou%20and%20Hui%20Wang%20and%20Yong%20Qin%0AAbstract%3A%20%20%20Incorporating%20visual%20modalities%20to%20assist%20Automatic%20Speech%20Recognition%20%28ASR%29%0Atasks%20has%20led%20to%20significant%20improvements.%20However%2C%20existing%20Audio-Visual%0ASpeech%20Recognition%20%28AVSR%29%20datasets%20and%20methods%20typically%20rely%20solely%20on%0Alip-reading%20information%20or%20speaking%20contextual%20video%2C%20neglecting%20the%20potential%0Aof%20combining%20these%20different%20valuable%20visual%20cues%20within%20the%20speaking%20context.%0AIn%20this%20paper%2C%20we%20release%20a%20multimodal%20Chinese%20AVSR%20dataset%2C%20Chinese-LiPS%2C%0Acomprising%20100%20hours%20of%20speech%2C%20video%2C%20and%20corresponding%20manual%20transcription%2C%0Awith%20the%20visual%20modality%20encompassing%20both%20lip-reading%20information%20and%20the%0Apresentation%20slides%20used%20by%20the%20speaker.%20Based%20on%20Chinese-LiPS%2C%20we%20develop%20a%0Asimple%20yet%20effective%20pipeline%2C%20LiPS-AVSR%2C%20which%20leverages%20both%20lip-reading%20and%0Apresentation%20slide%20information%20as%20visual%20modalities%20for%20AVSR%20tasks.%20Experiments%0Ashow%20that%20lip-reading%20and%20presentation%20slide%20information%20improve%20ASR%0Aperformance%20by%20approximately%208%5C%25%20and%2025%5C%25%2C%20respectively%2C%20with%20a%20combined%0Aperformance%20improvement%20of%20about%2035%5C%25.%20The%20dataset%20is%20available%20at%0Ahttps%3A//kiri0824.github.io/Chinese-LiPS/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChinese-LiPS%253A%2520A%2520Chinese%2520audio-visual%2520speech%2520recognition%2520dataset%2520with%250A%2520%2520Lip-reading%2520and%2520Presentation%2520Slides%26entry.906535625%3DJinghua%2520Zhao%2520and%2520Yuhang%2520Jia%2520and%2520Shiyao%2520Wang%2520and%2520Jiaming%2520Zhou%2520and%2520Hui%2520Wang%2520and%2520Yong%2520Qin%26entry.1292438233%3D%2520%2520Incorporating%2520visual%2520modalities%2520to%2520assist%2520Automatic%2520Speech%2520Recognition%2520%2528ASR%2529%250Atasks%2520has%2520led%2520to%2520significant%2520improvements.%2520However%252C%2520existing%2520Audio-Visual%250ASpeech%2520Recognition%2520%2528AVSR%2529%2520datasets%2520and%2520methods%2520typically%2520rely%2520solely%2520on%250Alip-reading%2520information%2520or%2520speaking%2520contextual%2520video%252C%2520neglecting%2520the%2520potential%250Aof%2520combining%2520these%2520different%2520valuable%2520visual%2520cues%2520within%2520the%2520speaking%2520context.%250AIn%2520this%2520paper%252C%2520we%2520release%2520a%2520multimodal%2520Chinese%2520AVSR%2520dataset%252C%2520Chinese-LiPS%252C%250Acomprising%2520100%2520hours%2520of%2520speech%252C%2520video%252C%2520and%2520corresponding%2520manual%2520transcription%252C%250Awith%2520the%2520visual%2520modality%2520encompassing%2520both%2520lip-reading%2520information%2520and%2520the%250Apresentation%2520slides%2520used%2520by%2520the%2520speaker.%2520Based%2520on%2520Chinese-LiPS%252C%2520we%2520develop%2520a%250Asimple%2520yet%2520effective%2520pipeline%252C%2520LiPS-AVSR%252C%2520which%2520leverages%2520both%2520lip-reading%2520and%250Apresentation%2520slide%2520information%2520as%2520visual%2520modalities%2520for%2520AVSR%2520tasks.%2520Experiments%250Ashow%2520that%2520lip-reading%2520and%2520presentation%2520slide%2520information%2520improve%2520ASR%250Aperformance%2520by%2520approximately%25208%255C%2525%2520and%252025%255C%2525%252C%2520respectively%252C%2520with%2520a%2520combined%250Aperformance%2520improvement%2520of%2520about%252035%255C%2525.%2520The%2520dataset%2520is%2520available%2520at%250Ahttps%253A//kiri0824.github.io/Chinese-LiPS/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chinese-LiPS%3A%20A%20Chinese%20audio-visual%20speech%20recognition%20dataset%20with%0A%20%20Lip-reading%20and%20Presentation%20Slides&entry.906535625=Jinghua%20Zhao%20and%20Yuhang%20Jia%20and%20Shiyao%20Wang%20and%20Jiaming%20Zhou%20and%20Hui%20Wang%20and%20Yong%20Qin&entry.1292438233=%20%20Incorporating%20visual%20modalities%20to%20assist%20Automatic%20Speech%20Recognition%20%28ASR%29%0Atasks%20has%20led%20to%20significant%20improvements.%20However%2C%20existing%20Audio-Visual%0ASpeech%20Recognition%20%28AVSR%29%20datasets%20and%20methods%20typically%20rely%20solely%20on%0Alip-reading%20information%20or%20speaking%20contextual%20video%2C%20neglecting%20the%20potential%0Aof%20combining%20these%20different%20valuable%20visual%20cues%20within%20the%20speaking%20context.%0AIn%20this%20paper%2C%20we%20release%20a%20multimodal%20Chinese%20AVSR%20dataset%2C%20Chinese-LiPS%2C%0Acomprising%20100%20hours%20of%20speech%2C%20video%2C%20and%20corresponding%20manual%20transcription%2C%0Awith%20the%20visual%20modality%20encompassing%20both%20lip-reading%20information%20and%20the%0Apresentation%20slides%20used%20by%20the%20speaker.%20Based%20on%20Chinese-LiPS%2C%20we%20develop%20a%0Asimple%20yet%20effective%20pipeline%2C%20LiPS-AVSR%2C%20which%20leverages%20both%20lip-reading%20and%0Apresentation%20slide%20information%20as%20visual%20modalities%20for%20AVSR%20tasks.%20Experiments%0Ashow%20that%20lip-reading%20and%20presentation%20slide%20information%20improve%20ASR%0Aperformance%20by%20approximately%208%5C%25%20and%2025%5C%25%2C%20respectively%2C%20with%20a%20combined%0Aperformance%20improvement%20of%20about%2035%5C%25.%20The%20dataset%20is%20available%20at%0Ahttps%3A//kiri0824.github.io/Chinese-LiPS/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15066v1&entry.124074799=Read"},
{"title": "ASIDE: Architectural Separation of Instructions and Data in Language\n  Models", "author": "Egor Zverev and Evgenii Kortukov and Alexander Panfilov and Alexandra Volkova and Soroush Tabesh and Sebastian Lapuschkin and Wojciech Samek and Christoph H. Lampert", "abstract": "  Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose a method, ASIDE, that allows\nthe model to clearly separate between instructions and data on the level of\nembeddings. ASIDE applies a fixed orthogonal rotation to the embeddings of data\ntokens, thus creating distinct representations of instructions and data tokens\nwithout introducing any additional parameters. We demonstrate the effectiveness\nof our method by instruct-tuning LLMs with ASIDE and showing (1) highly\nincreased instruction-data separation scores without a loss in model\ncapabilities and (2) competitive results on prompt injection benchmarks, even\nwithout dedicated safety training. Additionally, we study the working mechanism\nbehind our method through an analysis of model representations.\n", "link": "http://arxiv.org/abs/2503.10566v2", "date": "2025-04-21", "relevancy": 2.4941, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5082}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASIDE%3A%20Architectural%20Separation%20of%20Instructions%20and%20Data%20in%20Language%0A%20%20Models&body=Title%3A%20ASIDE%3A%20Architectural%20Separation%20of%20Instructions%20and%20Data%20in%20Language%0A%20%20Models%0AAuthor%3A%20Egor%20Zverev%20and%20Evgenii%20Kortukov%20and%20Alexander%20Panfilov%20and%20Alexandra%20Volkova%20and%20Soroush%20Tabesh%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek%20and%20Christoph%20H.%20Lampert%0AAbstract%3A%20%20%20Despite%20their%20remarkable%20performance%2C%20large%20language%20models%20lack%20elementary%0Asafety%20features%2C%20and%20this%20makes%20them%20susceptible%20to%20numerous%20malicious%20attacks.%0AIn%20particular%2C%20previous%20work%20has%20identified%20the%20absence%20of%20an%20intrinsic%0Aseparation%20between%20instructions%20and%20data%20as%20a%20root%20cause%20for%20the%20success%20of%0Aprompt%20injection%20attacks.%20In%20this%20work%2C%20we%20propose%20a%20method%2C%20ASIDE%2C%20that%20allows%0Athe%20model%20to%20clearly%20separate%20between%20instructions%20and%20data%20on%20the%20level%20of%0Aembeddings.%20ASIDE%20applies%20a%20fixed%20orthogonal%20rotation%20to%20the%20embeddings%20of%20data%0Atokens%2C%20thus%20creating%20distinct%20representations%20of%20instructions%20and%20data%20tokens%0Awithout%20introducing%20any%20additional%20parameters.%20We%20demonstrate%20the%20effectiveness%0Aof%20our%20method%20by%20instruct-tuning%20LLMs%20with%20ASIDE%20and%20showing%20%281%29%20highly%0Aincreased%20instruction-data%20separation%20scores%20without%20a%20loss%20in%20model%0Acapabilities%20and%20%282%29%20competitive%20results%20on%20prompt%20injection%20benchmarks%2C%20even%0Awithout%20dedicated%20safety%20training.%20Additionally%2C%20we%20study%20the%20working%20mechanism%0Abehind%20our%20method%20through%20an%20analysis%20of%20model%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10566v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASIDE%253A%2520Architectural%2520Separation%2520of%2520Instructions%2520and%2520Data%2520in%2520Language%250A%2520%2520Models%26entry.906535625%3DEgor%2520Zverev%2520and%2520Evgenii%2520Kortukov%2520and%2520Alexander%2520Panfilov%2520and%2520Alexandra%2520Volkova%2520and%2520Soroush%2520Tabesh%2520and%2520Sebastian%2520Lapuschkin%2520and%2520Wojciech%2520Samek%2520and%2520Christoph%2520H.%2520Lampert%26entry.1292438233%3D%2520%2520Despite%2520their%2520remarkable%2520performance%252C%2520large%2520language%2520models%2520lack%2520elementary%250Asafety%2520features%252C%2520and%2520this%2520makes%2520them%2520susceptible%2520to%2520numerous%2520malicious%2520attacks.%250AIn%2520particular%252C%2520previous%2520work%2520has%2520identified%2520the%2520absence%2520of%2520an%2520intrinsic%250Aseparation%2520between%2520instructions%2520and%2520data%2520as%2520a%2520root%2520cause%2520for%2520the%2520success%2520of%250Aprompt%2520injection%2520attacks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%252C%2520ASIDE%252C%2520that%2520allows%250Athe%2520model%2520to%2520clearly%2520separate%2520between%2520instructions%2520and%2520data%2520on%2520the%2520level%2520of%250Aembeddings.%2520ASIDE%2520applies%2520a%2520fixed%2520orthogonal%2520rotation%2520to%2520the%2520embeddings%2520of%2520data%250Atokens%252C%2520thus%2520creating%2520distinct%2520representations%2520of%2520instructions%2520and%2520data%2520tokens%250Awithout%2520introducing%2520any%2520additional%2520parameters.%2520We%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520method%2520by%2520instruct-tuning%2520LLMs%2520with%2520ASIDE%2520and%2520showing%2520%25281%2529%2520highly%250Aincreased%2520instruction-data%2520separation%2520scores%2520without%2520a%2520loss%2520in%2520model%250Acapabilities%2520and%2520%25282%2529%2520competitive%2520results%2520on%2520prompt%2520injection%2520benchmarks%252C%2520even%250Awithout%2520dedicated%2520safety%2520training.%2520Additionally%252C%2520we%2520study%2520the%2520working%2520mechanism%250Abehind%2520our%2520method%2520through%2520an%2520analysis%2520of%2520model%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10566v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASIDE%3A%20Architectural%20Separation%20of%20Instructions%20and%20Data%20in%20Language%0A%20%20Models&entry.906535625=Egor%20Zverev%20and%20Evgenii%20Kortukov%20and%20Alexander%20Panfilov%20and%20Alexandra%20Volkova%20and%20Soroush%20Tabesh%20and%20Sebastian%20Lapuschkin%20and%20Wojciech%20Samek%20and%20Christoph%20H.%20Lampert&entry.1292438233=%20%20Despite%20their%20remarkable%20performance%2C%20large%20language%20models%20lack%20elementary%0Asafety%20features%2C%20and%20this%20makes%20them%20susceptible%20to%20numerous%20malicious%20attacks.%0AIn%20particular%2C%20previous%20work%20has%20identified%20the%20absence%20of%20an%20intrinsic%0Aseparation%20between%20instructions%20and%20data%20as%20a%20root%20cause%20for%20the%20success%20of%0Aprompt%20injection%20attacks.%20In%20this%20work%2C%20we%20propose%20a%20method%2C%20ASIDE%2C%20that%20allows%0Athe%20model%20to%20clearly%20separate%20between%20instructions%20and%20data%20on%20the%20level%20of%0Aembeddings.%20ASIDE%20applies%20a%20fixed%20orthogonal%20rotation%20to%20the%20embeddings%20of%20data%0Atokens%2C%20thus%20creating%20distinct%20representations%20of%20instructions%20and%20data%20tokens%0Awithout%20introducing%20any%20additional%20parameters.%20We%20demonstrate%20the%20effectiveness%0Aof%20our%20method%20by%20instruct-tuning%20LLMs%20with%20ASIDE%20and%20showing%20%281%29%20highly%0Aincreased%20instruction-data%20separation%20scores%20without%20a%20loss%20in%20model%0Acapabilities%20and%20%282%29%20competitive%20results%20on%20prompt%20injection%20benchmarks%2C%20even%0Awithout%20dedicated%20safety%20training.%20Additionally%2C%20we%20study%20the%20working%20mechanism%0Abehind%20our%20method%20through%20an%20analysis%20of%20model%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10566v2&entry.124074799=Read"},
{"title": "Embedding Ontologies via Incorporating Extensional and Intensional\n  Knowledge", "author": "Keyu Wang and Guilin Qi and Jiaoyan Chen and Yi Huang and Tianxing Wu", "abstract": "  Ontologies contain rich knowledge within domain, which can be divided into\ntwo categories, namely extensional knowledge and intensional knowledge.\nExtensional knowledge provides information about the concrete instances that\nbelong to specific concepts in the ontology, while intensional knowledge\ndetails inherent properties, characteristics, and semantic associations among\nconcepts. However, existing ontology embedding approaches fail to take both\nextensional knowledge and intensional knowledge into fine consideration\nsimultaneously. In this paper, we propose a novel ontology embedding approach\nnamed EIKE (Extensional and Intensional Knowledge Embedding) by representing\nontologies in two spaces, called extensional space and intensional space. EIKE\npresents a unified framework for embedding instances, concepts and their\nrelations in an ontology, applying a geometry-based method to model extensional\nknowledge and a pretrained language model to model intensional knowledge, which\ncan capture both structure information and textual information. Experimental\nresults show that EIKE significantly outperforms state-of-the-art methods in\nthree datasets for both triple classification and link prediction, indicating\nthat EIKE provides a more comprehensive and representative perspective of the\ndomain.\n", "link": "http://arxiv.org/abs/2402.01677v5", "date": "2025-04-21", "relevancy": 2.4931, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5136}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedding%20Ontologies%20via%20Incorporating%20Extensional%20and%20Intensional%0A%20%20Knowledge&body=Title%3A%20Embedding%20Ontologies%20via%20Incorporating%20Extensional%20and%20Intensional%0A%20%20Knowledge%0AAuthor%3A%20Keyu%20Wang%20and%20Guilin%20Qi%20and%20Jiaoyan%20Chen%20and%20Yi%20Huang%20and%20Tianxing%20Wu%0AAbstract%3A%20%20%20Ontologies%20contain%20rich%20knowledge%20within%20domain%2C%20which%20can%20be%20divided%20into%0Atwo%20categories%2C%20namely%20extensional%20knowledge%20and%20intensional%20knowledge.%0AExtensional%20knowledge%20provides%20information%20about%20the%20concrete%20instances%20that%0Abelong%20to%20specific%20concepts%20in%20the%20ontology%2C%20while%20intensional%20knowledge%0Adetails%20inherent%20properties%2C%20characteristics%2C%20and%20semantic%20associations%20among%0Aconcepts.%20However%2C%20existing%20ontology%20embedding%20approaches%20fail%20to%20take%20both%0Aextensional%20knowledge%20and%20intensional%20knowledge%20into%20fine%20consideration%0Asimultaneously.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20ontology%20embedding%20approach%0Anamed%20EIKE%20%28Extensional%20and%20Intensional%20Knowledge%20Embedding%29%20by%20representing%0Aontologies%20in%20two%20spaces%2C%20called%20extensional%20space%20and%20intensional%20space.%20EIKE%0Apresents%20a%20unified%20framework%20for%20embedding%20instances%2C%20concepts%20and%20their%0Arelations%20in%20an%20ontology%2C%20applying%20a%20geometry-based%20method%20to%20model%20extensional%0Aknowledge%20and%20a%20pretrained%20language%20model%20to%20model%20intensional%20knowledge%2C%20which%0Acan%20capture%20both%20structure%20information%20and%20textual%20information.%20Experimental%0Aresults%20show%20that%20EIKE%20significantly%20outperforms%20state-of-the-art%20methods%20in%0Athree%20datasets%20for%20both%20triple%20classification%20and%20link%20prediction%2C%20indicating%0Athat%20EIKE%20provides%20a%20more%20comprehensive%20and%20representative%20perspective%20of%20the%0Adomain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01677v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedding%2520Ontologies%2520via%2520Incorporating%2520Extensional%2520and%2520Intensional%250A%2520%2520Knowledge%26entry.906535625%3DKeyu%2520Wang%2520and%2520Guilin%2520Qi%2520and%2520Jiaoyan%2520Chen%2520and%2520Yi%2520Huang%2520and%2520Tianxing%2520Wu%26entry.1292438233%3D%2520%2520Ontologies%2520contain%2520rich%2520knowledge%2520within%2520domain%252C%2520which%2520can%2520be%2520divided%2520into%250Atwo%2520categories%252C%2520namely%2520extensional%2520knowledge%2520and%2520intensional%2520knowledge.%250AExtensional%2520knowledge%2520provides%2520information%2520about%2520the%2520concrete%2520instances%2520that%250Abelong%2520to%2520specific%2520concepts%2520in%2520the%2520ontology%252C%2520while%2520intensional%2520knowledge%250Adetails%2520inherent%2520properties%252C%2520characteristics%252C%2520and%2520semantic%2520associations%2520among%250Aconcepts.%2520However%252C%2520existing%2520ontology%2520embedding%2520approaches%2520fail%2520to%2520take%2520both%250Aextensional%2520knowledge%2520and%2520intensional%2520knowledge%2520into%2520fine%2520consideration%250Asimultaneously.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520ontology%2520embedding%2520approach%250Anamed%2520EIKE%2520%2528Extensional%2520and%2520Intensional%2520Knowledge%2520Embedding%2529%2520by%2520representing%250Aontologies%2520in%2520two%2520spaces%252C%2520called%2520extensional%2520space%2520and%2520intensional%2520space.%2520EIKE%250Apresents%2520a%2520unified%2520framework%2520for%2520embedding%2520instances%252C%2520concepts%2520and%2520their%250Arelations%2520in%2520an%2520ontology%252C%2520applying%2520a%2520geometry-based%2520method%2520to%2520model%2520extensional%250Aknowledge%2520and%2520a%2520pretrained%2520language%2520model%2520to%2520model%2520intensional%2520knowledge%252C%2520which%250Acan%2520capture%2520both%2520structure%2520information%2520and%2520textual%2520information.%2520Experimental%250Aresults%2520show%2520that%2520EIKE%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%250Athree%2520datasets%2520for%2520both%2520triple%2520classification%2520and%2520link%2520prediction%252C%2520indicating%250Athat%2520EIKE%2520provides%2520a%2520more%2520comprehensive%2520and%2520representative%2520perspective%2520of%2520the%250Adomain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01677v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedding%20Ontologies%20via%20Incorporating%20Extensional%20and%20Intensional%0A%20%20Knowledge&entry.906535625=Keyu%20Wang%20and%20Guilin%20Qi%20and%20Jiaoyan%20Chen%20and%20Yi%20Huang%20and%20Tianxing%20Wu&entry.1292438233=%20%20Ontologies%20contain%20rich%20knowledge%20within%20domain%2C%20which%20can%20be%20divided%20into%0Atwo%20categories%2C%20namely%20extensional%20knowledge%20and%20intensional%20knowledge.%0AExtensional%20knowledge%20provides%20information%20about%20the%20concrete%20instances%20that%0Abelong%20to%20specific%20concepts%20in%20the%20ontology%2C%20while%20intensional%20knowledge%0Adetails%20inherent%20properties%2C%20characteristics%2C%20and%20semantic%20associations%20among%0Aconcepts.%20However%2C%20existing%20ontology%20embedding%20approaches%20fail%20to%20take%20both%0Aextensional%20knowledge%20and%20intensional%20knowledge%20into%20fine%20consideration%0Asimultaneously.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20ontology%20embedding%20approach%0Anamed%20EIKE%20%28Extensional%20and%20Intensional%20Knowledge%20Embedding%29%20by%20representing%0Aontologies%20in%20two%20spaces%2C%20called%20extensional%20space%20and%20intensional%20space.%20EIKE%0Apresents%20a%20unified%20framework%20for%20embedding%20instances%2C%20concepts%20and%20their%0Arelations%20in%20an%20ontology%2C%20applying%20a%20geometry-based%20method%20to%20model%20extensional%0Aknowledge%20and%20a%20pretrained%20language%20model%20to%20model%20intensional%20knowledge%2C%20which%0Acan%20capture%20both%20structure%20information%20and%20textual%20information.%20Experimental%0Aresults%20show%20that%20EIKE%20significantly%20outperforms%20state-of-the-art%20methods%20in%0Athree%20datasets%20for%20both%20triple%20classification%20and%20link%20prediction%2C%20indicating%0Athat%20EIKE%20provides%20a%20more%20comprehensive%20and%20representative%20perspective%20of%20the%0Adomain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01677v5&entry.124074799=Read"},
{"title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models", "author": "Weiye Xu and Jiahao Wang and Weiyun Wang and Zhe Chen and Wengang Zhou and Aijun Yang and Lewei Lu and Houqiang Li and Xiaohua Wang and Xizhou Zhu and Wenhai Wang and Jifeng Dai and Jinguo Zhu", "abstract": "  Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress.\n", "link": "http://arxiv.org/abs/2504.15279v1", "date": "2025-04-21", "relevancy": 2.4801, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6327}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisuLogic%3A%20A%20Benchmark%20for%20Evaluating%20Visual%20Reasoning%20in%20Multi-modal%0A%20%20Large%20Language%20Models&body=Title%3A%20VisuLogic%3A%20A%20Benchmark%20for%20Evaluating%20Visual%20Reasoning%20in%20Multi-modal%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Weiye%20Xu%20and%20Jiahao%20Wang%20and%20Weiyun%20Wang%20and%20Zhe%20Chen%20and%20Wengang%20Zhou%20and%20Aijun%20Yang%20and%20Lewei%20Lu%20and%20Houqiang%20Li%20and%20Xiaohua%20Wang%20and%20Xizhou%20Zhu%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Jinguo%20Zhu%0AAbstract%3A%20%20%20Visual%20reasoning%20is%20a%20core%20component%20of%20human%20intelligence%20and%20a%20critical%0Acapability%20for%20advanced%20multimodal%20models.%20Yet%20current%20reasoning%20evaluations%20of%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20often%20rely%20on%20text%20descriptions%20and%0Aallow%20language-based%20reasoning%20shortcuts%2C%20failing%20to%20measure%20genuine%0Avision-centric%20reasoning.%20To%20address%20this%2C%20we%20introduce%20VisuLogic%3A%20a%20benchmark%0Aof%201%2C000%20human-verified%20problems%20across%20six%20categories%20%28e.g.%2C%20quantitative%0Ashifts%2C%20spatial%20relations%2C%20attribute%20comparisons%29.%20These%20various%20types%20of%0Aquestions%20can%20be%20evaluated%20to%20assess%20the%20visual%20reasoning%20capabilities%20of%20MLLMs%0Afrom%20multiple%20perspectives.%20We%20evaluate%20leading%20MLLMs%20on%20this%20benchmark%20and%0Aanalyze%20their%20results%20to%20identify%20common%20failure%20modes.%20Most%20models%20score%20below%0A30%25%20accuracy-only%20slightly%20above%20the%2025%25%20random%20baseline%20and%20far%20below%20the%0A51.4%25%20achieved%20by%20humans-revealing%20significant%20gaps%20in%20visual%20reasoning.%0AFurthermore%2C%20we%20provide%20a%20supplementary%20training%20dataset%20and%20a%0Areinforcement-learning%20baseline%20to%20support%20further%20progress.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisuLogic%253A%2520A%2520Benchmark%2520for%2520Evaluating%2520Visual%2520Reasoning%2520in%2520Multi-modal%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DWeiye%2520Xu%2520and%2520Jiahao%2520Wang%2520and%2520Weiyun%2520Wang%2520and%2520Zhe%2520Chen%2520and%2520Wengang%2520Zhou%2520and%2520Aijun%2520Yang%2520and%2520Lewei%2520Lu%2520and%2520Houqiang%2520Li%2520and%2520Xiaohua%2520Wang%2520and%2520Xizhou%2520Zhu%2520and%2520Wenhai%2520Wang%2520and%2520Jifeng%2520Dai%2520and%2520Jinguo%2520Zhu%26entry.1292438233%3D%2520%2520Visual%2520reasoning%2520is%2520a%2520core%2520component%2520of%2520human%2520intelligence%2520and%2520a%2520critical%250Acapability%2520for%2520advanced%2520multimodal%2520models.%2520Yet%2520current%2520reasoning%2520evaluations%2520of%250Amultimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520often%2520rely%2520on%2520text%2520descriptions%2520and%250Aallow%2520language-based%2520reasoning%2520shortcuts%252C%2520failing%2520to%2520measure%2520genuine%250Avision-centric%2520reasoning.%2520To%2520address%2520this%252C%2520we%2520introduce%2520VisuLogic%253A%2520a%2520benchmark%250Aof%25201%252C000%2520human-verified%2520problems%2520across%2520six%2520categories%2520%2528e.g.%252C%2520quantitative%250Ashifts%252C%2520spatial%2520relations%252C%2520attribute%2520comparisons%2529.%2520These%2520various%2520types%2520of%250Aquestions%2520can%2520be%2520evaluated%2520to%2520assess%2520the%2520visual%2520reasoning%2520capabilities%2520of%2520MLLMs%250Afrom%2520multiple%2520perspectives.%2520We%2520evaluate%2520leading%2520MLLMs%2520on%2520this%2520benchmark%2520and%250Aanalyze%2520their%2520results%2520to%2520identify%2520common%2520failure%2520modes.%2520Most%2520models%2520score%2520below%250A30%2525%2520accuracy-only%2520slightly%2520above%2520the%252025%2525%2520random%2520baseline%2520and%2520far%2520below%2520the%250A51.4%2525%2520achieved%2520by%2520humans-revealing%2520significant%2520gaps%2520in%2520visual%2520reasoning.%250AFurthermore%252C%2520we%2520provide%2520a%2520supplementary%2520training%2520dataset%2520and%2520a%250Areinforcement-learning%2520baseline%2520to%2520support%2520further%2520progress.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisuLogic%3A%20A%20Benchmark%20for%20Evaluating%20Visual%20Reasoning%20in%20Multi-modal%0A%20%20Large%20Language%20Models&entry.906535625=Weiye%20Xu%20and%20Jiahao%20Wang%20and%20Weiyun%20Wang%20and%20Zhe%20Chen%20and%20Wengang%20Zhou%20and%20Aijun%20Yang%20and%20Lewei%20Lu%20and%20Houqiang%20Li%20and%20Xiaohua%20Wang%20and%20Xizhou%20Zhu%20and%20Wenhai%20Wang%20and%20Jifeng%20Dai%20and%20Jinguo%20Zhu&entry.1292438233=%20%20Visual%20reasoning%20is%20a%20core%20component%20of%20human%20intelligence%20and%20a%20critical%0Acapability%20for%20advanced%20multimodal%20models.%20Yet%20current%20reasoning%20evaluations%20of%0Amultimodal%20large%20language%20models%20%28MLLMs%29%20often%20rely%20on%20text%20descriptions%20and%0Aallow%20language-based%20reasoning%20shortcuts%2C%20failing%20to%20measure%20genuine%0Avision-centric%20reasoning.%20To%20address%20this%2C%20we%20introduce%20VisuLogic%3A%20a%20benchmark%0Aof%201%2C000%20human-verified%20problems%20across%20six%20categories%20%28e.g.%2C%20quantitative%0Ashifts%2C%20spatial%20relations%2C%20attribute%20comparisons%29.%20These%20various%20types%20of%0Aquestions%20can%20be%20evaluated%20to%20assess%20the%20visual%20reasoning%20capabilities%20of%20MLLMs%0Afrom%20multiple%20perspectives.%20We%20evaluate%20leading%20MLLMs%20on%20this%20benchmark%20and%0Aanalyze%20their%20results%20to%20identify%20common%20failure%20modes.%20Most%20models%20score%20below%0A30%25%20accuracy-only%20slightly%20above%20the%2025%25%20random%20baseline%20and%20far%20below%20the%0A51.4%25%20achieved%20by%20humans-revealing%20significant%20gaps%20in%20visual%20reasoning.%0AFurthermore%2C%20we%20provide%20a%20supplementary%20training%20dataset%20and%20a%0Areinforcement-learning%20baseline%20to%20support%20further%20progress.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15279v1&entry.124074799=Read"},
{"title": "Synergistic Weak-Strong Collaboration by Aligning Preferences", "author": "Yizhu Jiao and Xuchao Zhang and Zhaoyang Wang and Yubo Ma and Zhun Deng and Rujia Wang and Chetan Bansal and Saravan Rajmohan and Jiawei Han and Huaxiu Yao", "abstract": "  Current Large Language Models (LLMs) excel in general reasoning yet struggle\nwith specialized tasks requiring proprietary or domain-specific knowledge.\nFine-tuning large models for every niche application is often infeasible due to\nblack-box constraints and high computational overhead. To address this, we\npropose a collaborative framework that pairs a specialized weak model with a\ngeneral strong model. The weak model, tailored to specific domains, produces\ninitial drafts and background information, while the strong model leverages its\nadvanced reasoning to refine these drafts, extending LLMs' capabilities to\ncritical yet specialized tasks. To optimize this collaboration, we introduce a\ncollaborative feedback to fine-tunes the weak model, which quantifies the\ninfluence of the weak model's contributions in the collaboration procedure and\nestablishes preference pairs to guide preference tuning of the weak model. We\nvalidate our framework through experiments on three domains. We find that the\ncollaboration significantly outperforms each model alone by leveraging\ncomplementary strengths. Moreover, aligning the weak model with the\ncollaborative preference further enhances overall performance.\n", "link": "http://arxiv.org/abs/2504.15188v1", "date": "2025-04-21", "relevancy": 2.4613, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4986}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synergistic%20Weak-Strong%20Collaboration%20by%20Aligning%20Preferences&body=Title%3A%20Synergistic%20Weak-Strong%20Collaboration%20by%20Aligning%20Preferences%0AAuthor%3A%20Yizhu%20Jiao%20and%20Xuchao%20Zhang%20and%20Zhaoyang%20Wang%20and%20Yubo%20Ma%20and%20Zhun%20Deng%20and%20Rujia%20Wang%20and%20Chetan%20Bansal%20and%20Saravan%20Rajmohan%20and%20Jiawei%20Han%20and%20Huaxiu%20Yao%0AAbstract%3A%20%20%20Current%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20general%20reasoning%20yet%20struggle%0Awith%20specialized%20tasks%20requiring%20proprietary%20or%20domain-specific%20knowledge.%0AFine-tuning%20large%20models%20for%20every%20niche%20application%20is%20often%20infeasible%20due%20to%0Ablack-box%20constraints%20and%20high%20computational%20overhead.%20To%20address%20this%2C%20we%0Apropose%20a%20collaborative%20framework%20that%20pairs%20a%20specialized%20weak%20model%20with%20a%0Ageneral%20strong%20model.%20The%20weak%20model%2C%20tailored%20to%20specific%20domains%2C%20produces%0Ainitial%20drafts%20and%20background%20information%2C%20while%20the%20strong%20model%20leverages%20its%0Aadvanced%20reasoning%20to%20refine%20these%20drafts%2C%20extending%20LLMs%27%20capabilities%20to%0Acritical%20yet%20specialized%20tasks.%20To%20optimize%20this%20collaboration%2C%20we%20introduce%20a%0Acollaborative%20feedback%20to%20fine-tunes%20the%20weak%20model%2C%20which%20quantifies%20the%0Ainfluence%20of%20the%20weak%20model%27s%20contributions%20in%20the%20collaboration%20procedure%20and%0Aestablishes%20preference%20pairs%20to%20guide%20preference%20tuning%20of%20the%20weak%20model.%20We%0Avalidate%20our%20framework%20through%20experiments%20on%20three%20domains.%20We%20find%20that%20the%0Acollaboration%20significantly%20outperforms%20each%20model%20alone%20by%20leveraging%0Acomplementary%20strengths.%20Moreover%2C%20aligning%20the%20weak%20model%20with%20the%0Acollaborative%20preference%20further%20enhances%20overall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynergistic%2520Weak-Strong%2520Collaboration%2520by%2520Aligning%2520Preferences%26entry.906535625%3DYizhu%2520Jiao%2520and%2520Xuchao%2520Zhang%2520and%2520Zhaoyang%2520Wang%2520and%2520Yubo%2520Ma%2520and%2520Zhun%2520Deng%2520and%2520Rujia%2520Wang%2520and%2520Chetan%2520Bansal%2520and%2520Saravan%2520Rajmohan%2520and%2520Jiawei%2520Han%2520and%2520Huaxiu%2520Yao%26entry.1292438233%3D%2520%2520Current%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520general%2520reasoning%2520yet%2520struggle%250Awith%2520specialized%2520tasks%2520requiring%2520proprietary%2520or%2520domain-specific%2520knowledge.%250AFine-tuning%2520large%2520models%2520for%2520every%2520niche%2520application%2520is%2520often%2520infeasible%2520due%2520to%250Ablack-box%2520constraints%2520and%2520high%2520computational%2520overhead.%2520To%2520address%2520this%252C%2520we%250Apropose%2520a%2520collaborative%2520framework%2520that%2520pairs%2520a%2520specialized%2520weak%2520model%2520with%2520a%250Ageneral%2520strong%2520model.%2520The%2520weak%2520model%252C%2520tailored%2520to%2520specific%2520domains%252C%2520produces%250Ainitial%2520drafts%2520and%2520background%2520information%252C%2520while%2520the%2520strong%2520model%2520leverages%2520its%250Aadvanced%2520reasoning%2520to%2520refine%2520these%2520drafts%252C%2520extending%2520LLMs%2527%2520capabilities%2520to%250Acritical%2520yet%2520specialized%2520tasks.%2520To%2520optimize%2520this%2520collaboration%252C%2520we%2520introduce%2520a%250Acollaborative%2520feedback%2520to%2520fine-tunes%2520the%2520weak%2520model%252C%2520which%2520quantifies%2520the%250Ainfluence%2520of%2520the%2520weak%2520model%2527s%2520contributions%2520in%2520the%2520collaboration%2520procedure%2520and%250Aestablishes%2520preference%2520pairs%2520to%2520guide%2520preference%2520tuning%2520of%2520the%2520weak%2520model.%2520We%250Avalidate%2520our%2520framework%2520through%2520experiments%2520on%2520three%2520domains.%2520We%2520find%2520that%2520the%250Acollaboration%2520significantly%2520outperforms%2520each%2520model%2520alone%2520by%2520leveraging%250Acomplementary%2520strengths.%2520Moreover%252C%2520aligning%2520the%2520weak%2520model%2520with%2520the%250Acollaborative%2520preference%2520further%2520enhances%2520overall%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synergistic%20Weak-Strong%20Collaboration%20by%20Aligning%20Preferences&entry.906535625=Yizhu%20Jiao%20and%20Xuchao%20Zhang%20and%20Zhaoyang%20Wang%20and%20Yubo%20Ma%20and%20Zhun%20Deng%20and%20Rujia%20Wang%20and%20Chetan%20Bansal%20and%20Saravan%20Rajmohan%20and%20Jiawei%20Han%20and%20Huaxiu%20Yao&entry.1292438233=%20%20Current%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20general%20reasoning%20yet%20struggle%0Awith%20specialized%20tasks%20requiring%20proprietary%20or%20domain-specific%20knowledge.%0AFine-tuning%20large%20models%20for%20every%20niche%20application%20is%20often%20infeasible%20due%20to%0Ablack-box%20constraints%20and%20high%20computational%20overhead.%20To%20address%20this%2C%20we%0Apropose%20a%20collaborative%20framework%20that%20pairs%20a%20specialized%20weak%20model%20with%20a%0Ageneral%20strong%20model.%20The%20weak%20model%2C%20tailored%20to%20specific%20domains%2C%20produces%0Ainitial%20drafts%20and%20background%20information%2C%20while%20the%20strong%20model%20leverages%20its%0Aadvanced%20reasoning%20to%20refine%20these%20drafts%2C%20extending%20LLMs%27%20capabilities%20to%0Acritical%20yet%20specialized%20tasks.%20To%20optimize%20this%20collaboration%2C%20we%20introduce%20a%0Acollaborative%20feedback%20to%20fine-tunes%20the%20weak%20model%2C%20which%20quantifies%20the%0Ainfluence%20of%20the%20weak%20model%27s%20contributions%20in%20the%20collaboration%20procedure%20and%0Aestablishes%20preference%20pairs%20to%20guide%20preference%20tuning%20of%20the%20weak%20model.%20We%0Avalidate%20our%20framework%20through%20experiments%20on%20three%20domains.%20We%20find%20that%20the%0Acollaboration%20significantly%20outperforms%20each%20model%20alone%20by%20leveraging%0Acomplementary%20strengths.%20Moreover%2C%20aligning%20the%20weak%20model%20with%20the%0Acollaborative%20preference%20further%20enhances%20overall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15188v1&entry.124074799=Read"},
{"title": "Diffusion Bridge Models for 3D Medical Image Translation", "author": "Shaorong Zhang and Tamoghna Chattopadhyay and Sophia I. Thomopoulos and Jose-Luis Ambite and Paul M. Thompson and Greg Ver Steeg", "abstract": "  Diffusion tensor imaging (DTI) provides crucial insights into the\nmicrostructure of the human brain, but it can be time-consuming to acquire\ncompared to more readily available T1-weighted (T1w) magnetic resonance imaging\n(MRI). To address this challenge, we propose a diffusion bridge model for 3D\nbrain image translation between T1w MRI and DTI modalities. Our model learns to\ngenerate high-quality DTI fractional anisotropy (FA) images from T1w images and\nvice versa, enabling cross-modality data augmentation and reducing the need for\nextensive DTI acquisition. We evaluate our approach using perceptual\nsimilarity, pixel-level agreement, and distributional consistency metrics,\ndemonstrating strong performance in capturing anatomical structures and\npreserving information on white matter integrity. The practical utility of the\nsynthetic data is validated through sex classification and Alzheimer's disease\nclassification tasks, where the generated images achieve comparable performance\nto real data. Our diffusion bridge model offers a promising solution for\nimproving neuroimaging datasets and supporting clinical decision-making, with\nthe potential to significantly impact neuroimaging research and clinical\npractice.\n", "link": "http://arxiv.org/abs/2504.15267v1", "date": "2025-04-21", "relevancy": 2.4502, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6147}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6147}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Bridge%20Models%20for%203D%20Medical%20Image%20Translation&body=Title%3A%20Diffusion%20Bridge%20Models%20for%203D%20Medical%20Image%20Translation%0AAuthor%3A%20Shaorong%20Zhang%20and%20Tamoghna%20Chattopadhyay%20and%20Sophia%20I.%20Thomopoulos%20and%20Jose-Luis%20Ambite%20and%20Paul%20M.%20Thompson%20and%20Greg%20Ver%20Steeg%0AAbstract%3A%20%20%20Diffusion%20tensor%20imaging%20%28DTI%29%20provides%20crucial%20insights%20into%20the%0Amicrostructure%20of%20the%20human%20brain%2C%20but%20it%20can%20be%20time-consuming%20to%20acquire%0Acompared%20to%20more%20readily%20available%20T1-weighted%20%28T1w%29%20magnetic%20resonance%20imaging%0A%28MRI%29.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20diffusion%20bridge%20model%20for%203D%0Abrain%20image%20translation%20between%20T1w%20MRI%20and%20DTI%20modalities.%20Our%20model%20learns%20to%0Agenerate%20high-quality%20DTI%20fractional%20anisotropy%20%28FA%29%20images%20from%20T1w%20images%20and%0Avice%20versa%2C%20enabling%20cross-modality%20data%20augmentation%20and%20reducing%20the%20need%20for%0Aextensive%20DTI%20acquisition.%20We%20evaluate%20our%20approach%20using%20perceptual%0Asimilarity%2C%20pixel-level%20agreement%2C%20and%20distributional%20consistency%20metrics%2C%0Ademonstrating%20strong%20performance%20in%20capturing%20anatomical%20structures%20and%0Apreserving%20information%20on%20white%20matter%20integrity.%20The%20practical%20utility%20of%20the%0Asynthetic%20data%20is%20validated%20through%20sex%20classification%20and%20Alzheimer%27s%20disease%0Aclassification%20tasks%2C%20where%20the%20generated%20images%20achieve%20comparable%20performance%0Ato%20real%20data.%20Our%20diffusion%20bridge%20model%20offers%20a%20promising%20solution%20for%0Aimproving%20neuroimaging%20datasets%20and%20supporting%20clinical%20decision-making%2C%20with%0Athe%20potential%20to%20significantly%20impact%20neuroimaging%20research%20and%20clinical%0Apractice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Bridge%2520Models%2520for%25203D%2520Medical%2520Image%2520Translation%26entry.906535625%3DShaorong%2520Zhang%2520and%2520Tamoghna%2520Chattopadhyay%2520and%2520Sophia%2520I.%2520Thomopoulos%2520and%2520Jose-Luis%2520Ambite%2520and%2520Paul%2520M.%2520Thompson%2520and%2520Greg%2520Ver%2520Steeg%26entry.1292438233%3D%2520%2520Diffusion%2520tensor%2520imaging%2520%2528DTI%2529%2520provides%2520crucial%2520insights%2520into%2520the%250Amicrostructure%2520of%2520the%2520human%2520brain%252C%2520but%2520it%2520can%2520be%2520time-consuming%2520to%2520acquire%250Acompared%2520to%2520more%2520readily%2520available%2520T1-weighted%2520%2528T1w%2529%2520magnetic%2520resonance%2520imaging%250A%2528MRI%2529.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520diffusion%2520bridge%2520model%2520for%25203D%250Abrain%2520image%2520translation%2520between%2520T1w%2520MRI%2520and%2520DTI%2520modalities.%2520Our%2520model%2520learns%2520to%250Agenerate%2520high-quality%2520DTI%2520fractional%2520anisotropy%2520%2528FA%2529%2520images%2520from%2520T1w%2520images%2520and%250Avice%2520versa%252C%2520enabling%2520cross-modality%2520data%2520augmentation%2520and%2520reducing%2520the%2520need%2520for%250Aextensive%2520DTI%2520acquisition.%2520We%2520evaluate%2520our%2520approach%2520using%2520perceptual%250Asimilarity%252C%2520pixel-level%2520agreement%252C%2520and%2520distributional%2520consistency%2520metrics%252C%250Ademonstrating%2520strong%2520performance%2520in%2520capturing%2520anatomical%2520structures%2520and%250Apreserving%2520information%2520on%2520white%2520matter%2520integrity.%2520The%2520practical%2520utility%2520of%2520the%250Asynthetic%2520data%2520is%2520validated%2520through%2520sex%2520classification%2520and%2520Alzheimer%2527s%2520disease%250Aclassification%2520tasks%252C%2520where%2520the%2520generated%2520images%2520achieve%2520comparable%2520performance%250Ato%2520real%2520data.%2520Our%2520diffusion%2520bridge%2520model%2520offers%2520a%2520promising%2520solution%2520for%250Aimproving%2520neuroimaging%2520datasets%2520and%2520supporting%2520clinical%2520decision-making%252C%2520with%250Athe%2520potential%2520to%2520significantly%2520impact%2520neuroimaging%2520research%2520and%2520clinical%250Apractice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Bridge%20Models%20for%203D%20Medical%20Image%20Translation&entry.906535625=Shaorong%20Zhang%20and%20Tamoghna%20Chattopadhyay%20and%20Sophia%20I.%20Thomopoulos%20and%20Jose-Luis%20Ambite%20and%20Paul%20M.%20Thompson%20and%20Greg%20Ver%20Steeg&entry.1292438233=%20%20Diffusion%20tensor%20imaging%20%28DTI%29%20provides%20crucial%20insights%20into%20the%0Amicrostructure%20of%20the%20human%20brain%2C%20but%20it%20can%20be%20time-consuming%20to%20acquire%0Acompared%20to%20more%20readily%20available%20T1-weighted%20%28T1w%29%20magnetic%20resonance%20imaging%0A%28MRI%29.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20diffusion%20bridge%20model%20for%203D%0Abrain%20image%20translation%20between%20T1w%20MRI%20and%20DTI%20modalities.%20Our%20model%20learns%20to%0Agenerate%20high-quality%20DTI%20fractional%20anisotropy%20%28FA%29%20images%20from%20T1w%20images%20and%0Avice%20versa%2C%20enabling%20cross-modality%20data%20augmentation%20and%20reducing%20the%20need%20for%0Aextensive%20DTI%20acquisition.%20We%20evaluate%20our%20approach%20using%20perceptual%0Asimilarity%2C%20pixel-level%20agreement%2C%20and%20distributional%20consistency%20metrics%2C%0Ademonstrating%20strong%20performance%20in%20capturing%20anatomical%20structures%20and%0Apreserving%20information%20on%20white%20matter%20integrity.%20The%20practical%20utility%20of%20the%0Asynthetic%20data%20is%20validated%20through%20sex%20classification%20and%20Alzheimer%27s%20disease%0Aclassification%20tasks%2C%20where%20the%20generated%20images%20achieve%20comparable%20performance%0Ato%20real%20data.%20Our%20diffusion%20bridge%20model%20offers%20a%20promising%20solution%20for%0Aimproving%20neuroimaging%20datasets%20and%20supporting%20clinical%20decision-making%2C%20with%0Athe%20potential%20to%20significantly%20impact%20neuroimaging%20research%20and%20clinical%0Apractice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15267v1&entry.124074799=Read"},
{"title": "SkyReels-V2: Infinite-length Film Generative Model", "author": "Guibin Chen and Dixuan Lin and Jiangping Yang and Chunze Lin and Junchen Zhu and Mingyuan Fan and Hao Zhang and Sheng Chen and Zheng Chen and Chengcheng Ma and Weiming Xiong and Wei Wang and Nuo Pang and Kang Kang and Zhiheng Xu and Yuzhe Jin and Yupeng Liang and Yubing Song and Peng Zhao and Boyuan Xu and Di Qiu and Debang Li and Zhengcong Fei and Yang Li and Yahui Zhou", "abstract": "  Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.\n", "link": "http://arxiv.org/abs/2504.13074v3", "date": "2025-04-21", "relevancy": 2.4485, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6298}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6179}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkyReels-V2%3A%20Infinite-length%20Film%20Generative%20Model&body=Title%3A%20SkyReels-V2%3A%20Infinite-length%20Film%20Generative%20Model%0AAuthor%3A%20Guibin%20Chen%20and%20Dixuan%20Lin%20and%20Jiangping%20Yang%20and%20Chunze%20Lin%20and%20Junchen%20Zhu%20and%20Mingyuan%20Fan%20and%20Hao%20Zhang%20and%20Sheng%20Chen%20and%20Zheng%20Chen%20and%20Chengcheng%20Ma%20and%20Weiming%20Xiong%20and%20Wei%20Wang%20and%20Nuo%20Pang%20and%20Kang%20Kang%20and%20Zhiheng%20Xu%20and%20Yuzhe%20Jin%20and%20Yupeng%20Liang%20and%20Yubing%20Song%20and%20Peng%20Zhao%20and%20Boyuan%20Xu%20and%20Di%20Qiu%20and%20Debang%20Li%20and%20Zhengcong%20Fei%20and%20Yang%20Li%20and%20Yahui%20Zhou%0AAbstract%3A%20%20%20Recent%20advances%20in%20video%20generation%20have%20been%20driven%20by%20diffusion%20models%20and%0Aautoregressive%20frameworks%2C%20yet%20critical%20challenges%20persist%20in%20harmonizing%0Aprompt%20adherence%2C%20visual%20quality%2C%20motion%20dynamics%2C%20and%20duration%3A%20compromises%20in%0Amotion%20dynamics%20to%20enhance%20temporal%20visual%20quality%2C%20constrained%20video%20duration%0A%285-10%20seconds%29%20to%20prioritize%20resolution%2C%20and%20inadequate%20shot-aware%20generation%0Astemming%20from%20general-purpose%20MLLMs%27%20inability%20to%20interpret%20cinematic%20grammar%2C%0Asuch%20as%20shot%20composition%2C%20actor%20expressions%2C%20and%20camera%20motions.%20These%0Aintertwined%20limitations%20hinder%20realistic%20long-form%20synthesis%20and%20professional%0Afilm-style%20generation.%20To%20address%20these%20limitations%2C%20we%20propose%20SkyReels-V2%2C%20an%0AInfinite-length%20Film%20Generative%20Model%2C%20that%20synergizes%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%2C%20Multi-stage%20Pretraining%2C%20Reinforcement%20Learning%2C%20and%0ADiffusion%20Forcing%20Framework.%20Firstly%2C%20we%20design%20a%20comprehensive%20structural%0Arepresentation%20of%20video%20that%20combines%20the%20general%20descriptions%20by%20the%0AMulti-modal%20LLM%20and%20the%20detailed%20shot%20language%20by%20sub-expert%20models.%20Aided%20with%0Ahuman%20annotation%2C%20we%20then%20train%20a%20unified%20Video%20Captioner%2C%20named%0ASkyCaptioner-V1%2C%20to%20efficiently%20label%20the%20video%20data.%20Secondly%2C%20we%20establish%0Aprogressive-resolution%20pretraining%20for%20the%20fundamental%20video%20generation%2C%0Afollowed%20by%20a%20four-stage%20post-training%20enhancement%3A%20Initial%20concept-balanced%0ASupervised%20Fine-Tuning%20%28SFT%29%20improves%20baseline%20quality%3B%20Motion-specific%0AReinforcement%20Learning%20%28RL%29%20training%20with%20human-annotated%20and%20synthetic%0Adistortion%20data%20addresses%20dynamic%20artifacts%3B%20Our%20diffusion%20forcing%20framework%0Awith%20non-decreasing%20noise%20schedules%20enables%20long-video%20synthesis%20in%20an%0Aefficient%20search%20space%3B%20Final%20high-quality%20SFT%20refines%20visual%20fidelity.%20All%20the%0Acode%20and%20models%20are%20available%20at%20https%3A//github.com/SkyworkAI/SkyReels-V2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13074v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkyReels-V2%253A%2520Infinite-length%2520Film%2520Generative%2520Model%26entry.906535625%3DGuibin%2520Chen%2520and%2520Dixuan%2520Lin%2520and%2520Jiangping%2520Yang%2520and%2520Chunze%2520Lin%2520and%2520Junchen%2520Zhu%2520and%2520Mingyuan%2520Fan%2520and%2520Hao%2520Zhang%2520and%2520Sheng%2520Chen%2520and%2520Zheng%2520Chen%2520and%2520Chengcheng%2520Ma%2520and%2520Weiming%2520Xiong%2520and%2520Wei%2520Wang%2520and%2520Nuo%2520Pang%2520and%2520Kang%2520Kang%2520and%2520Zhiheng%2520Xu%2520and%2520Yuzhe%2520Jin%2520and%2520Yupeng%2520Liang%2520and%2520Yubing%2520Song%2520and%2520Peng%2520Zhao%2520and%2520Boyuan%2520Xu%2520and%2520Di%2520Qiu%2520and%2520Debang%2520Li%2520and%2520Zhengcong%2520Fei%2520and%2520Yang%2520Li%2520and%2520Yahui%2520Zhou%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520video%2520generation%2520have%2520been%2520driven%2520by%2520diffusion%2520models%2520and%250Aautoregressive%2520frameworks%252C%2520yet%2520critical%2520challenges%2520persist%2520in%2520harmonizing%250Aprompt%2520adherence%252C%2520visual%2520quality%252C%2520motion%2520dynamics%252C%2520and%2520duration%253A%2520compromises%2520in%250Amotion%2520dynamics%2520to%2520enhance%2520temporal%2520visual%2520quality%252C%2520constrained%2520video%2520duration%250A%25285-10%2520seconds%2529%2520to%2520prioritize%2520resolution%252C%2520and%2520inadequate%2520shot-aware%2520generation%250Astemming%2520from%2520general-purpose%2520MLLMs%2527%2520inability%2520to%2520interpret%2520cinematic%2520grammar%252C%250Asuch%2520as%2520shot%2520composition%252C%2520actor%2520expressions%252C%2520and%2520camera%2520motions.%2520These%250Aintertwined%2520limitations%2520hinder%2520realistic%2520long-form%2520synthesis%2520and%2520professional%250Afilm-style%2520generation.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520SkyReels-V2%252C%2520an%250AInfinite-length%2520Film%2520Generative%2520Model%252C%2520that%2520synergizes%2520Multi-modal%2520Large%250ALanguage%2520Model%2520%2528MLLM%2529%252C%2520Multi-stage%2520Pretraining%252C%2520Reinforcement%2520Learning%252C%2520and%250ADiffusion%2520Forcing%2520Framework.%2520Firstly%252C%2520we%2520design%2520a%2520comprehensive%2520structural%250Arepresentation%2520of%2520video%2520that%2520combines%2520the%2520general%2520descriptions%2520by%2520the%250AMulti-modal%2520LLM%2520and%2520the%2520detailed%2520shot%2520language%2520by%2520sub-expert%2520models.%2520Aided%2520with%250Ahuman%2520annotation%252C%2520we%2520then%2520train%2520a%2520unified%2520Video%2520Captioner%252C%2520named%250ASkyCaptioner-V1%252C%2520to%2520efficiently%2520label%2520the%2520video%2520data.%2520Secondly%252C%2520we%2520establish%250Aprogressive-resolution%2520pretraining%2520for%2520the%2520fundamental%2520video%2520generation%252C%250Afollowed%2520by%2520a%2520four-stage%2520post-training%2520enhancement%253A%2520Initial%2520concept-balanced%250ASupervised%2520Fine-Tuning%2520%2528SFT%2529%2520improves%2520baseline%2520quality%253B%2520Motion-specific%250AReinforcement%2520Learning%2520%2528RL%2529%2520training%2520with%2520human-annotated%2520and%2520synthetic%250Adistortion%2520data%2520addresses%2520dynamic%2520artifacts%253B%2520Our%2520diffusion%2520forcing%2520framework%250Awith%2520non-decreasing%2520noise%2520schedules%2520enables%2520long-video%2520synthesis%2520in%2520an%250Aefficient%2520search%2520space%253B%2520Final%2520high-quality%2520SFT%2520refines%2520visual%2520fidelity.%2520All%2520the%250Acode%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/SkyworkAI/SkyReels-V2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13074v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkyReels-V2%3A%20Infinite-length%20Film%20Generative%20Model&entry.906535625=Guibin%20Chen%20and%20Dixuan%20Lin%20and%20Jiangping%20Yang%20and%20Chunze%20Lin%20and%20Junchen%20Zhu%20and%20Mingyuan%20Fan%20and%20Hao%20Zhang%20and%20Sheng%20Chen%20and%20Zheng%20Chen%20and%20Chengcheng%20Ma%20and%20Weiming%20Xiong%20and%20Wei%20Wang%20and%20Nuo%20Pang%20and%20Kang%20Kang%20and%20Zhiheng%20Xu%20and%20Yuzhe%20Jin%20and%20Yupeng%20Liang%20and%20Yubing%20Song%20and%20Peng%20Zhao%20and%20Boyuan%20Xu%20and%20Di%20Qiu%20and%20Debang%20Li%20and%20Zhengcong%20Fei%20and%20Yang%20Li%20and%20Yahui%20Zhou&entry.1292438233=%20%20Recent%20advances%20in%20video%20generation%20have%20been%20driven%20by%20diffusion%20models%20and%0Aautoregressive%20frameworks%2C%20yet%20critical%20challenges%20persist%20in%20harmonizing%0Aprompt%20adherence%2C%20visual%20quality%2C%20motion%20dynamics%2C%20and%20duration%3A%20compromises%20in%0Amotion%20dynamics%20to%20enhance%20temporal%20visual%20quality%2C%20constrained%20video%20duration%0A%285-10%20seconds%29%20to%20prioritize%20resolution%2C%20and%20inadequate%20shot-aware%20generation%0Astemming%20from%20general-purpose%20MLLMs%27%20inability%20to%20interpret%20cinematic%20grammar%2C%0Asuch%20as%20shot%20composition%2C%20actor%20expressions%2C%20and%20camera%20motions.%20These%0Aintertwined%20limitations%20hinder%20realistic%20long-form%20synthesis%20and%20professional%0Afilm-style%20generation.%20To%20address%20these%20limitations%2C%20we%20propose%20SkyReels-V2%2C%20an%0AInfinite-length%20Film%20Generative%20Model%2C%20that%20synergizes%20Multi-modal%20Large%0ALanguage%20Model%20%28MLLM%29%2C%20Multi-stage%20Pretraining%2C%20Reinforcement%20Learning%2C%20and%0ADiffusion%20Forcing%20Framework.%20Firstly%2C%20we%20design%20a%20comprehensive%20structural%0Arepresentation%20of%20video%20that%20combines%20the%20general%20descriptions%20by%20the%0AMulti-modal%20LLM%20and%20the%20detailed%20shot%20language%20by%20sub-expert%20models.%20Aided%20with%0Ahuman%20annotation%2C%20we%20then%20train%20a%20unified%20Video%20Captioner%2C%20named%0ASkyCaptioner-V1%2C%20to%20efficiently%20label%20the%20video%20data.%20Secondly%2C%20we%20establish%0Aprogressive-resolution%20pretraining%20for%20the%20fundamental%20video%20generation%2C%0Afollowed%20by%20a%20four-stage%20post-training%20enhancement%3A%20Initial%20concept-balanced%0ASupervised%20Fine-Tuning%20%28SFT%29%20improves%20baseline%20quality%3B%20Motion-specific%0AReinforcement%20Learning%20%28RL%29%20training%20with%20human-annotated%20and%20synthetic%0Adistortion%20data%20addresses%20dynamic%20artifacts%3B%20Our%20diffusion%20forcing%20framework%0Awith%20non-decreasing%20noise%20schedules%20enables%20long-video%20synthesis%20in%20an%0Aefficient%20search%20space%3B%20Final%20high-quality%20SFT%20refines%20visual%20fidelity.%20All%20the%0Acode%20and%20models%20are%20available%20at%20https%3A//github.com/SkyworkAI/SkyReels-V2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13074v3&entry.124074799=Read"},
{"title": "ScanEdit: Hierarchically-Guided Functional 3D Scan Editing", "author": "Mohamed el amine Boudjoghra and Ivan Laptev and Angela Dai", "abstract": "  With the fast pace of 3D capture technology and resulting abundance of 3D\ndata, effective 3D scene editing becomes essential for a variety of graphics\napplications. In this work we present ScanEdit, an instruction-driven method\nfor functional editing of complex, real-world 3D scans. To model large and\ninterdependent sets of ob- jectswe propose a hierarchically-guided approach.\nGiven a 3D scan decomposed into its object instances, we first construct a\nhierarchical scene graph representation to enable effective, tractable editing.\nWe then leverage reason- ing capabilities of Large Language Models (LLMs) and\ntranslate high-level language instructions into actionable commands applied\nhierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based\nguidance with ex- plicit physical constraints and generates realistic scenes\nwhere object arrangements obey both physics and common sense. In our extensive\nexperimental evaluation ScanEdit outperforms state of the art and demonstrates\nexcellent re- sults for a variety of real-world scenes and input instruc-\ntions.\n", "link": "http://arxiv.org/abs/2504.15049v1", "date": "2025-04-21", "relevancy": 2.4323, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6115}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6115}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScanEdit%3A%20Hierarchically-Guided%20Functional%203D%20Scan%20Editing&body=Title%3A%20ScanEdit%3A%20Hierarchically-Guided%20Functional%203D%20Scan%20Editing%0AAuthor%3A%20Mohamed%20el%20amine%20Boudjoghra%20and%20Ivan%20Laptev%20and%20Angela%20Dai%0AAbstract%3A%20%20%20With%20the%20fast%20pace%20of%203D%20capture%20technology%20and%20resulting%20abundance%20of%203D%0Adata%2C%20effective%203D%20scene%20editing%20becomes%20essential%20for%20a%20variety%20of%20graphics%0Aapplications.%20In%20this%20work%20we%20present%20ScanEdit%2C%20an%20instruction-driven%20method%0Afor%20functional%20editing%20of%20complex%2C%20real-world%203D%20scans.%20To%20model%20large%20and%0Ainterdependent%20sets%20of%20ob-%20jectswe%20propose%20a%20hierarchically-guided%20approach.%0AGiven%20a%203D%20scan%20decomposed%20into%20its%20object%20instances%2C%20we%20first%20construct%20a%0Ahierarchical%20scene%20graph%20representation%20to%20enable%20effective%2C%20tractable%20editing.%0AWe%20then%20leverage%20reason-%20ing%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20and%0Atranslate%20high-level%20language%20instructions%20into%20actionable%20commands%20applied%0Ahierarchically%20to%20the%20scene%20graph.%20Fi-%20nally%2C%20ScanEdit%20integrates%20LLM-based%0Aguidance%20with%20ex-%20plicit%20physical%20constraints%20and%20generates%20realistic%20scenes%0Awhere%20object%20arrangements%20obey%20both%20physics%20and%20common%20sense.%20In%20our%20extensive%0Aexperimental%20evaluation%20ScanEdit%20outperforms%20state%20of%20the%20art%20and%20demonstrates%0Aexcellent%20re-%20sults%20for%20a%20variety%20of%20real-world%20scenes%20and%20input%20instruc-%0Ations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScanEdit%253A%2520Hierarchically-Guided%2520Functional%25203D%2520Scan%2520Editing%26entry.906535625%3DMohamed%2520el%2520amine%2520Boudjoghra%2520and%2520Ivan%2520Laptev%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520With%2520the%2520fast%2520pace%2520of%25203D%2520capture%2520technology%2520and%2520resulting%2520abundance%2520of%25203D%250Adata%252C%2520effective%25203D%2520scene%2520editing%2520becomes%2520essential%2520for%2520a%2520variety%2520of%2520graphics%250Aapplications.%2520In%2520this%2520work%2520we%2520present%2520ScanEdit%252C%2520an%2520instruction-driven%2520method%250Afor%2520functional%2520editing%2520of%2520complex%252C%2520real-world%25203D%2520scans.%2520To%2520model%2520large%2520and%250Ainterdependent%2520sets%2520of%2520ob-%2520jectswe%2520propose%2520a%2520hierarchically-guided%2520approach.%250AGiven%2520a%25203D%2520scan%2520decomposed%2520into%2520its%2520object%2520instances%252C%2520we%2520first%2520construct%2520a%250Ahierarchical%2520scene%2520graph%2520representation%2520to%2520enable%2520effective%252C%2520tractable%2520editing.%250AWe%2520then%2520leverage%2520reason-%2520ing%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%250Atranslate%2520high-level%2520language%2520instructions%2520into%2520actionable%2520commands%2520applied%250Ahierarchically%2520to%2520the%2520scene%2520graph.%2520Fi-%2520nally%252C%2520ScanEdit%2520integrates%2520LLM-based%250Aguidance%2520with%2520ex-%2520plicit%2520physical%2520constraints%2520and%2520generates%2520realistic%2520scenes%250Awhere%2520object%2520arrangements%2520obey%2520both%2520physics%2520and%2520common%2520sense.%2520In%2520our%2520extensive%250Aexperimental%2520evaluation%2520ScanEdit%2520outperforms%2520state%2520of%2520the%2520art%2520and%2520demonstrates%250Aexcellent%2520re-%2520sults%2520for%2520a%2520variety%2520of%2520real-world%2520scenes%2520and%2520input%2520instruc-%250Ations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScanEdit%3A%20Hierarchically-Guided%20Functional%203D%20Scan%20Editing&entry.906535625=Mohamed%20el%20amine%20Boudjoghra%20and%20Ivan%20Laptev%20and%20Angela%20Dai&entry.1292438233=%20%20With%20the%20fast%20pace%20of%203D%20capture%20technology%20and%20resulting%20abundance%20of%203D%0Adata%2C%20effective%203D%20scene%20editing%20becomes%20essential%20for%20a%20variety%20of%20graphics%0Aapplications.%20In%20this%20work%20we%20present%20ScanEdit%2C%20an%20instruction-driven%20method%0Afor%20functional%20editing%20of%20complex%2C%20real-world%203D%20scans.%20To%20model%20large%20and%0Ainterdependent%20sets%20of%20ob-%20jectswe%20propose%20a%20hierarchically-guided%20approach.%0AGiven%20a%203D%20scan%20decomposed%20into%20its%20object%20instances%2C%20we%20first%20construct%20a%0Ahierarchical%20scene%20graph%20representation%20to%20enable%20effective%2C%20tractable%20editing.%0AWe%20then%20leverage%20reason-%20ing%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20and%0Atranslate%20high-level%20language%20instructions%20into%20actionable%20commands%20applied%0Ahierarchically%20to%20the%20scene%20graph.%20Fi-%20nally%2C%20ScanEdit%20integrates%20LLM-based%0Aguidance%20with%20ex-%20plicit%20physical%20constraints%20and%20generates%20realistic%20scenes%0Awhere%20object%20arrangements%20obey%20both%20physics%20and%20common%20sense.%20In%20our%20extensive%0Aexperimental%20evaluation%20ScanEdit%20outperforms%20state%20of%20the%20art%20and%20demonstrates%0Aexcellent%20re-%20sults%20for%20a%20variety%20of%20real-world%20scenes%20and%20input%20instruc-%0Ations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15049v1&entry.124074799=Read"},
{"title": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional\n  Text-to-Video Generation", "author": "Weijie He and Mushui Liu and Yunlong Yu and Zhao Wang and Chao Wu", "abstract": "  Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving the precise control\nover individual entities; and (3) An Entity-Consistency Constraint strategy\nthat propagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis.\n", "link": "http://arxiv.org/abs/2504.15032v1", "date": "2025-04-21", "relevancy": 2.4266, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6265}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.604}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyST-XL%3A%20Dynamic%20Layout%20Planning%20and%20Content%20Control%20for%20Compositional%0A%20%20Text-to-Video%20Generation&body=Title%3A%20DyST-XL%3A%20Dynamic%20Layout%20Planning%20and%20Content%20Control%20for%20Compositional%0A%20%20Text-to-Video%20Generation%0AAuthor%3A%20Weijie%20He%20and%20Mushui%20Liu%20and%20Yunlong%20Yu%20and%20Zhao%20Wang%20and%20Chao%20Wu%0AAbstract%3A%20%20%20Compositional%20text-to-video%20generation%2C%20which%20requires%20synthesizing%20dynamic%0Ascenes%20with%20multiple%20interacting%20entities%20and%20precise%20spatial-temporal%0Arelationships%2C%20remains%20a%20critical%20challenge%20for%20diffusion-based%20models.%0AExisting%20methods%20struggle%20with%20layout%20discontinuity%2C%20entity%20identity%20drift%2C%20and%0Aimplausible%20interaction%20dynamics%20due%20to%20unconstrained%20cross-attention%0Amechanisms%20and%20inadequate%20physics-aware%20reasoning.%20To%20address%20these%0Alimitations%2C%20we%20propose%20DyST-XL%2C%20a%20%5Ctextbf%7Btraining-free%7D%20framework%20that%0Aenhances%20off-the-shelf%20text-to-video%20models%20%28e.g.%2C%20CogVideoX-5B%29%20through%0Aframe-aware%20control.%20DyST-XL%20integrates%20three%20key%20innovations%3A%20%281%29%20A%20Dynamic%0ALayout%20Planner%20that%20leverages%20large%20language%20models%20%28LLMs%29%20to%20parse%20input%0Aprompts%20into%20entity-attribute%20graphs%20and%20generates%20physics-aware%20keyframe%0Alayouts%2C%20with%20intermediate%20frames%20interpolated%20via%20trajectory%20optimization%3B%20%282%29%0AA%20Dual-Prompt%20Controlled%20Attention%20Mechanism%20that%20enforces%20localized%20text-video%0Aalignment%20through%20frame-aware%20attention%20masking%2C%20achieving%20the%20precise%20control%0Aover%20individual%20entities%3B%20and%20%283%29%20An%20Entity-Consistency%20Constraint%20strategy%0Athat%20propagates%20first-frame%20feature%20embeddings%20to%20subsequent%20frames%20during%0Adenoising%2C%20preserving%20object%20identity%20without%20manual%20annotation.%20Experiments%0Ademonstrate%20that%20DyST-XL%20excels%20in%20compositional%20text-to-video%20generation%2C%0Asignificantly%20improving%20performance%20on%20complex%20prompts%20and%20bridging%20a%20crucial%0Agap%20in%20training-free%20video%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyST-XL%253A%2520Dynamic%2520Layout%2520Planning%2520and%2520Content%2520Control%2520for%2520Compositional%250A%2520%2520Text-to-Video%2520Generation%26entry.906535625%3DWeijie%2520He%2520and%2520Mushui%2520Liu%2520and%2520Yunlong%2520Yu%2520and%2520Zhao%2520Wang%2520and%2520Chao%2520Wu%26entry.1292438233%3D%2520%2520Compositional%2520text-to-video%2520generation%252C%2520which%2520requires%2520synthesizing%2520dynamic%250Ascenes%2520with%2520multiple%2520interacting%2520entities%2520and%2520precise%2520spatial-temporal%250Arelationships%252C%2520remains%2520a%2520critical%2520challenge%2520for%2520diffusion-based%2520models.%250AExisting%2520methods%2520struggle%2520with%2520layout%2520discontinuity%252C%2520entity%2520identity%2520drift%252C%2520and%250Aimplausible%2520interaction%2520dynamics%2520due%2520to%2520unconstrained%2520cross-attention%250Amechanisms%2520and%2520inadequate%2520physics-aware%2520reasoning.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520DyST-XL%252C%2520a%2520%255Ctextbf%257Btraining-free%257D%2520framework%2520that%250Aenhances%2520off-the-shelf%2520text-to-video%2520models%2520%2528e.g.%252C%2520CogVideoX-5B%2529%2520through%250Aframe-aware%2520control.%2520DyST-XL%2520integrates%2520three%2520key%2520innovations%253A%2520%25281%2529%2520A%2520Dynamic%250ALayout%2520Planner%2520that%2520leverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520parse%2520input%250Aprompts%2520into%2520entity-attribute%2520graphs%2520and%2520generates%2520physics-aware%2520keyframe%250Alayouts%252C%2520with%2520intermediate%2520frames%2520interpolated%2520via%2520trajectory%2520optimization%253B%2520%25282%2529%250AA%2520Dual-Prompt%2520Controlled%2520Attention%2520Mechanism%2520that%2520enforces%2520localized%2520text-video%250Aalignment%2520through%2520frame-aware%2520attention%2520masking%252C%2520achieving%2520the%2520precise%2520control%250Aover%2520individual%2520entities%253B%2520and%2520%25283%2529%2520An%2520Entity-Consistency%2520Constraint%2520strategy%250Athat%2520propagates%2520first-frame%2520feature%2520embeddings%2520to%2520subsequent%2520frames%2520during%250Adenoising%252C%2520preserving%2520object%2520identity%2520without%2520manual%2520annotation.%2520Experiments%250Ademonstrate%2520that%2520DyST-XL%2520excels%2520in%2520compositional%2520text-to-video%2520generation%252C%250Asignificantly%2520improving%2520performance%2520on%2520complex%2520prompts%2520and%2520bridging%2520a%2520crucial%250Agap%2520in%2520training-free%2520video%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyST-XL%3A%20Dynamic%20Layout%20Planning%20and%20Content%20Control%20for%20Compositional%0A%20%20Text-to-Video%20Generation&entry.906535625=Weijie%20He%20and%20Mushui%20Liu%20and%20Yunlong%20Yu%20and%20Zhao%20Wang%20and%20Chao%20Wu&entry.1292438233=%20%20Compositional%20text-to-video%20generation%2C%20which%20requires%20synthesizing%20dynamic%0Ascenes%20with%20multiple%20interacting%20entities%20and%20precise%20spatial-temporal%0Arelationships%2C%20remains%20a%20critical%20challenge%20for%20diffusion-based%20models.%0AExisting%20methods%20struggle%20with%20layout%20discontinuity%2C%20entity%20identity%20drift%2C%20and%0Aimplausible%20interaction%20dynamics%20due%20to%20unconstrained%20cross-attention%0Amechanisms%20and%20inadequate%20physics-aware%20reasoning.%20To%20address%20these%0Alimitations%2C%20we%20propose%20DyST-XL%2C%20a%20%5Ctextbf%7Btraining-free%7D%20framework%20that%0Aenhances%20off-the-shelf%20text-to-video%20models%20%28e.g.%2C%20CogVideoX-5B%29%20through%0Aframe-aware%20control.%20DyST-XL%20integrates%20three%20key%20innovations%3A%20%281%29%20A%20Dynamic%0ALayout%20Planner%20that%20leverages%20large%20language%20models%20%28LLMs%29%20to%20parse%20input%0Aprompts%20into%20entity-attribute%20graphs%20and%20generates%20physics-aware%20keyframe%0Alayouts%2C%20with%20intermediate%20frames%20interpolated%20via%20trajectory%20optimization%3B%20%282%29%0AA%20Dual-Prompt%20Controlled%20Attention%20Mechanism%20that%20enforces%20localized%20text-video%0Aalignment%20through%20frame-aware%20attention%20masking%2C%20achieving%20the%20precise%20control%0Aover%20individual%20entities%3B%20and%20%283%29%20An%20Entity-Consistency%20Constraint%20strategy%0Athat%20propagates%20first-frame%20feature%20embeddings%20to%20subsequent%20frames%20during%0Adenoising%2C%20preserving%20object%20identity%20without%20manual%20annotation.%20Experiments%0Ademonstrate%20that%20DyST-XL%20excels%20in%20compositional%20text-to-video%20generation%2C%0Asignificantly%20improving%20performance%20on%20complex%20prompts%20and%20bridging%20a%20crucial%0Agap%20in%20training-free%20video%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15032v1&entry.124074799=Read"},
{"title": "Shape-Guided Clothing Warping for Virtual Try-On", "author": "Xiaoyu Han and Shunyuan Zheng and Zonglin Li and Chenyang Wang and Xin Sun and Quanling Meng", "abstract": "  Image-based virtual try-on aims to seamlessly fit in-shop clothing to a\nperson image while maintaining pose consistency. Existing methods commonly\nemploy the thin plate spline (TPS) transformation or appearance flow to deform\nin-shop clothing for aligning with the person's body. Despite their promising\nperformance, these methods often lack precise control over fine details,\nleading to inconsistencies in shape between clothing and the person's body as\nwell as distortions in exposed limb regions. To tackle these challenges, we\npropose a novel shape-guided clothing warping method for virtual try-on, dubbed\nSCW-VTON, which incorporates global shape constraints and additional limb\ntextures to enhance the realism and consistency of the warped clothing and\ntry-on results. To integrate global shape constraints for clothing warping, we\ndevise a dual-path clothing warping module comprising a shape path and a flow\npath. The former path captures the clothing shape aligned with the person's\nbody, while the latter path leverages the mapping between the pre- and\npost-deformation of the clothing shape to guide the estimation of appearance\nflow. Furthermore, to alleviate distortions in limb regions of try-on results,\nwe integrate detailed limb guidance by developing a limb reconstruction network\nbased on masked image modeling. Through the utilization of SCW-VTON, we are\nable to generate try-on results with enhanced clothing shape consistency and\nprecise control over details. Extensive experiments demonstrate the superiority\nof our approach over state-of-the-art methods both qualitatively and\nquantitatively. The code is available at https://github.com/xyhanHIT/SCW-VTON.\n", "link": "http://arxiv.org/abs/2504.15232v1", "date": "2025-04-21", "relevancy": 2.4204, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6127}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.605}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shape-Guided%20Clothing%20Warping%20for%20Virtual%20Try-On&body=Title%3A%20Shape-Guided%20Clothing%20Warping%20for%20Virtual%20Try-On%0AAuthor%3A%20Xiaoyu%20Han%20and%20Shunyuan%20Zheng%20and%20Zonglin%20Li%20and%20Chenyang%20Wang%20and%20Xin%20Sun%20and%20Quanling%20Meng%0AAbstract%3A%20%20%20Image-based%20virtual%20try-on%20aims%20to%20seamlessly%20fit%20in-shop%20clothing%20to%20a%0Aperson%20image%20while%20maintaining%20pose%20consistency.%20Existing%20methods%20commonly%0Aemploy%20the%20thin%20plate%20spline%20%28TPS%29%20transformation%20or%20appearance%20flow%20to%20deform%0Ain-shop%20clothing%20for%20aligning%20with%20the%20person%27s%20body.%20Despite%20their%20promising%0Aperformance%2C%20these%20methods%20often%20lack%20precise%20control%20over%20fine%20details%2C%0Aleading%20to%20inconsistencies%20in%20shape%20between%20clothing%20and%20the%20person%27s%20body%20as%0Awell%20as%20distortions%20in%20exposed%20limb%20regions.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20a%20novel%20shape-guided%20clothing%20warping%20method%20for%20virtual%20try-on%2C%20dubbed%0ASCW-VTON%2C%20which%20incorporates%20global%20shape%20constraints%20and%20additional%20limb%0Atextures%20to%20enhance%20the%20realism%20and%20consistency%20of%20the%20warped%20clothing%20and%0Atry-on%20results.%20To%20integrate%20global%20shape%20constraints%20for%20clothing%20warping%2C%20we%0Adevise%20a%20dual-path%20clothing%20warping%20module%20comprising%20a%20shape%20path%20and%20a%20flow%0Apath.%20The%20former%20path%20captures%20the%20clothing%20shape%20aligned%20with%20the%20person%27s%0Abody%2C%20while%20the%20latter%20path%20leverages%20the%20mapping%20between%20the%20pre-%20and%0Apost-deformation%20of%20the%20clothing%20shape%20to%20guide%20the%20estimation%20of%20appearance%0Aflow.%20Furthermore%2C%20to%20alleviate%20distortions%20in%20limb%20regions%20of%20try-on%20results%2C%0Awe%20integrate%20detailed%20limb%20guidance%20by%20developing%20a%20limb%20reconstruction%20network%0Abased%20on%20masked%20image%20modeling.%20Through%20the%20utilization%20of%20SCW-VTON%2C%20we%20are%0Aable%20to%20generate%20try-on%20results%20with%20enhanced%20clothing%20shape%20consistency%20and%0Aprecise%20control%20over%20details.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20our%20approach%20over%20state-of-the-art%20methods%20both%20qualitatively%20and%0Aquantitatively.%20The%20code%20is%20available%20at%20https%3A//github.com/xyhanHIT/SCW-VTON.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShape-Guided%2520Clothing%2520Warping%2520for%2520Virtual%2520Try-On%26entry.906535625%3DXiaoyu%2520Han%2520and%2520Shunyuan%2520Zheng%2520and%2520Zonglin%2520Li%2520and%2520Chenyang%2520Wang%2520and%2520Xin%2520Sun%2520and%2520Quanling%2520Meng%26entry.1292438233%3D%2520%2520Image-based%2520virtual%2520try-on%2520aims%2520to%2520seamlessly%2520fit%2520in-shop%2520clothing%2520to%2520a%250Aperson%2520image%2520while%2520maintaining%2520pose%2520consistency.%2520Existing%2520methods%2520commonly%250Aemploy%2520the%2520thin%2520plate%2520spline%2520%2528TPS%2529%2520transformation%2520or%2520appearance%2520flow%2520to%2520deform%250Ain-shop%2520clothing%2520for%2520aligning%2520with%2520the%2520person%2527s%2520body.%2520Despite%2520their%2520promising%250Aperformance%252C%2520these%2520methods%2520often%2520lack%2520precise%2520control%2520over%2520fine%2520details%252C%250Aleading%2520to%2520inconsistencies%2520in%2520shape%2520between%2520clothing%2520and%2520the%2520person%2527s%2520body%2520as%250Awell%2520as%2520distortions%2520in%2520exposed%2520limb%2520regions.%2520To%2520tackle%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520novel%2520shape-guided%2520clothing%2520warping%2520method%2520for%2520virtual%2520try-on%252C%2520dubbed%250ASCW-VTON%252C%2520which%2520incorporates%2520global%2520shape%2520constraints%2520and%2520additional%2520limb%250Atextures%2520to%2520enhance%2520the%2520realism%2520and%2520consistency%2520of%2520the%2520warped%2520clothing%2520and%250Atry-on%2520results.%2520To%2520integrate%2520global%2520shape%2520constraints%2520for%2520clothing%2520warping%252C%2520we%250Adevise%2520a%2520dual-path%2520clothing%2520warping%2520module%2520comprising%2520a%2520shape%2520path%2520and%2520a%2520flow%250Apath.%2520The%2520former%2520path%2520captures%2520the%2520clothing%2520shape%2520aligned%2520with%2520the%2520person%2527s%250Abody%252C%2520while%2520the%2520latter%2520path%2520leverages%2520the%2520mapping%2520between%2520the%2520pre-%2520and%250Apost-deformation%2520of%2520the%2520clothing%2520shape%2520to%2520guide%2520the%2520estimation%2520of%2520appearance%250Aflow.%2520Furthermore%252C%2520to%2520alleviate%2520distortions%2520in%2520limb%2520regions%2520of%2520try-on%2520results%252C%250Awe%2520integrate%2520detailed%2520limb%2520guidance%2520by%2520developing%2520a%2520limb%2520reconstruction%2520network%250Abased%2520on%2520masked%2520image%2520modeling.%2520Through%2520the%2520utilization%2520of%2520SCW-VTON%252C%2520we%2520are%250Aable%2520to%2520generate%2520try-on%2520results%2520with%2520enhanced%2520clothing%2520shape%2520consistency%2520and%250Aprecise%2520control%2520over%2520details.%2520Extensive%2520experiments%2520demonstrate%2520the%2520superiority%250Aof%2520our%2520approach%2520over%2520state-of-the-art%2520methods%2520both%2520qualitatively%2520and%250Aquantitatively.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/xyhanHIT/SCW-VTON.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shape-Guided%20Clothing%20Warping%20for%20Virtual%20Try-On&entry.906535625=Xiaoyu%20Han%20and%20Shunyuan%20Zheng%20and%20Zonglin%20Li%20and%20Chenyang%20Wang%20and%20Xin%20Sun%20and%20Quanling%20Meng&entry.1292438233=%20%20Image-based%20virtual%20try-on%20aims%20to%20seamlessly%20fit%20in-shop%20clothing%20to%20a%0Aperson%20image%20while%20maintaining%20pose%20consistency.%20Existing%20methods%20commonly%0Aemploy%20the%20thin%20plate%20spline%20%28TPS%29%20transformation%20or%20appearance%20flow%20to%20deform%0Ain-shop%20clothing%20for%20aligning%20with%20the%20person%27s%20body.%20Despite%20their%20promising%0Aperformance%2C%20these%20methods%20often%20lack%20precise%20control%20over%20fine%20details%2C%0Aleading%20to%20inconsistencies%20in%20shape%20between%20clothing%20and%20the%20person%27s%20body%20as%0Awell%20as%20distortions%20in%20exposed%20limb%20regions.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20a%20novel%20shape-guided%20clothing%20warping%20method%20for%20virtual%20try-on%2C%20dubbed%0ASCW-VTON%2C%20which%20incorporates%20global%20shape%20constraints%20and%20additional%20limb%0Atextures%20to%20enhance%20the%20realism%20and%20consistency%20of%20the%20warped%20clothing%20and%0Atry-on%20results.%20To%20integrate%20global%20shape%20constraints%20for%20clothing%20warping%2C%20we%0Adevise%20a%20dual-path%20clothing%20warping%20module%20comprising%20a%20shape%20path%20and%20a%20flow%0Apath.%20The%20former%20path%20captures%20the%20clothing%20shape%20aligned%20with%20the%20person%27s%0Abody%2C%20while%20the%20latter%20path%20leverages%20the%20mapping%20between%20the%20pre-%20and%0Apost-deformation%20of%20the%20clothing%20shape%20to%20guide%20the%20estimation%20of%20appearance%0Aflow.%20Furthermore%2C%20to%20alleviate%20distortions%20in%20limb%20regions%20of%20try-on%20results%2C%0Awe%20integrate%20detailed%20limb%20guidance%20by%20developing%20a%20limb%20reconstruction%20network%0Abased%20on%20masked%20image%20modeling.%20Through%20the%20utilization%20of%20SCW-VTON%2C%20we%20are%0Aable%20to%20generate%20try-on%20results%20with%20enhanced%20clothing%20shape%20consistency%20and%0Aprecise%20control%20over%20details.%20Extensive%20experiments%20demonstrate%20the%20superiority%0Aof%20our%20approach%20over%20state-of-the-art%20methods%20both%20qualitatively%20and%0Aquantitatively.%20The%20code%20is%20available%20at%20https%3A//github.com/xyhanHIT/SCW-VTON.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15232v1&entry.124074799=Read"},
{"title": "Improving Sound Source Localization with Joint Slot Attention on Image\n  and Audio", "author": "Inho Kim and Youngkil Song and Jicheol Park and Won Hwa Kim and Suha Kwak", "abstract": "  Sound source localization (SSL) is the task of locating the source of sound\nwithin an image. Due to the lack of localization labels, the de facto standard\nin SSL has been to represent an image and audio as a single embedding vector\neach, and use them to learn SSL via contrastive learning. To this end, previous\nwork samples one of local image features as the image embedding and aggregates\nall local audio features to obtain the audio embedding, which is far from\noptimal due to the presence of noise and background irrelevant to the actual\ntarget in the input. We present a novel SSL method that addresses this chronic\nissue by joint slot attention on image and audio. To be specific, two slots\ncompetitively attend image and audio features to decompose them into target and\noff-target representations, and only target representations of image and audio\nare used for contrastive learning. Also, we introduce cross-modal attention\nmatching to further align local features of image and audio. Our method\nachieved the best in almost all settings on three public benchmarks for SSL,\nand substantially outperformed all the prior work in cross-modal retrieval.\n", "link": "http://arxiv.org/abs/2504.15118v1", "date": "2025-04-21", "relevancy": 2.4061, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4903}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Sound%20Source%20Localization%20with%20Joint%20Slot%20Attention%20on%20Image%0A%20%20and%20Audio&body=Title%3A%20Improving%20Sound%20Source%20Localization%20with%20Joint%20Slot%20Attention%20on%20Image%0A%20%20and%20Audio%0AAuthor%3A%20Inho%20Kim%20and%20Youngkil%20Song%20and%20Jicheol%20Park%20and%20Won%20Hwa%20Kim%20and%20Suha%20Kwak%0AAbstract%3A%20%20%20Sound%20source%20localization%20%28SSL%29%20is%20the%20task%20of%20locating%20the%20source%20of%20sound%0Awithin%20an%20image.%20Due%20to%20the%20lack%20of%20localization%20labels%2C%20the%20de%20facto%20standard%0Ain%20SSL%20has%20been%20to%20represent%20an%20image%20and%20audio%20as%20a%20single%20embedding%20vector%0Aeach%2C%20and%20use%20them%20to%20learn%20SSL%20via%20contrastive%20learning.%20To%20this%20end%2C%20previous%0Awork%20samples%20one%20of%20local%20image%20features%20as%20the%20image%20embedding%20and%20aggregates%0Aall%20local%20audio%20features%20to%20obtain%20the%20audio%20embedding%2C%20which%20is%20far%20from%0Aoptimal%20due%20to%20the%20presence%20of%20noise%20and%20background%20irrelevant%20to%20the%20actual%0Atarget%20in%20the%20input.%20We%20present%20a%20novel%20SSL%20method%20that%20addresses%20this%20chronic%0Aissue%20by%20joint%20slot%20attention%20on%20image%20and%20audio.%20To%20be%20specific%2C%20two%20slots%0Acompetitively%20attend%20image%20and%20audio%20features%20to%20decompose%20them%20into%20target%20and%0Aoff-target%20representations%2C%20and%20only%20target%20representations%20of%20image%20and%20audio%0Aare%20used%20for%20contrastive%20learning.%20Also%2C%20we%20introduce%20cross-modal%20attention%0Amatching%20to%20further%20align%20local%20features%20of%20image%20and%20audio.%20Our%20method%0Aachieved%20the%20best%20in%20almost%20all%20settings%20on%20three%20public%20benchmarks%20for%20SSL%2C%0Aand%20substantially%20outperformed%20all%20the%20prior%20work%20in%20cross-modal%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Sound%2520Source%2520Localization%2520with%2520Joint%2520Slot%2520Attention%2520on%2520Image%250A%2520%2520and%2520Audio%26entry.906535625%3DInho%2520Kim%2520and%2520Youngkil%2520Song%2520and%2520Jicheol%2520Park%2520and%2520Won%2520Hwa%2520Kim%2520and%2520Suha%2520Kwak%26entry.1292438233%3D%2520%2520Sound%2520source%2520localization%2520%2528SSL%2529%2520is%2520the%2520task%2520of%2520locating%2520the%2520source%2520of%2520sound%250Awithin%2520an%2520image.%2520Due%2520to%2520the%2520lack%2520of%2520localization%2520labels%252C%2520the%2520de%2520facto%2520standard%250Ain%2520SSL%2520has%2520been%2520to%2520represent%2520an%2520image%2520and%2520audio%2520as%2520a%2520single%2520embedding%2520vector%250Aeach%252C%2520and%2520use%2520them%2520to%2520learn%2520SSL%2520via%2520contrastive%2520learning.%2520To%2520this%2520end%252C%2520previous%250Awork%2520samples%2520one%2520of%2520local%2520image%2520features%2520as%2520the%2520image%2520embedding%2520and%2520aggregates%250Aall%2520local%2520audio%2520features%2520to%2520obtain%2520the%2520audio%2520embedding%252C%2520which%2520is%2520far%2520from%250Aoptimal%2520due%2520to%2520the%2520presence%2520of%2520noise%2520and%2520background%2520irrelevant%2520to%2520the%2520actual%250Atarget%2520in%2520the%2520input.%2520We%2520present%2520a%2520novel%2520SSL%2520method%2520that%2520addresses%2520this%2520chronic%250Aissue%2520by%2520joint%2520slot%2520attention%2520on%2520image%2520and%2520audio.%2520To%2520be%2520specific%252C%2520two%2520slots%250Acompetitively%2520attend%2520image%2520and%2520audio%2520features%2520to%2520decompose%2520them%2520into%2520target%2520and%250Aoff-target%2520representations%252C%2520and%2520only%2520target%2520representations%2520of%2520image%2520and%2520audio%250Aare%2520used%2520for%2520contrastive%2520learning.%2520Also%252C%2520we%2520introduce%2520cross-modal%2520attention%250Amatching%2520to%2520further%2520align%2520local%2520features%2520of%2520image%2520and%2520audio.%2520Our%2520method%250Aachieved%2520the%2520best%2520in%2520almost%2520all%2520settings%2520on%2520three%2520public%2520benchmarks%2520for%2520SSL%252C%250Aand%2520substantially%2520outperformed%2520all%2520the%2520prior%2520work%2520in%2520cross-modal%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Sound%20Source%20Localization%20with%20Joint%20Slot%20Attention%20on%20Image%0A%20%20and%20Audio&entry.906535625=Inho%20Kim%20and%20Youngkil%20Song%20and%20Jicheol%20Park%20and%20Won%20Hwa%20Kim%20and%20Suha%20Kwak&entry.1292438233=%20%20Sound%20source%20localization%20%28SSL%29%20is%20the%20task%20of%20locating%20the%20source%20of%20sound%0Awithin%20an%20image.%20Due%20to%20the%20lack%20of%20localization%20labels%2C%20the%20de%20facto%20standard%0Ain%20SSL%20has%20been%20to%20represent%20an%20image%20and%20audio%20as%20a%20single%20embedding%20vector%0Aeach%2C%20and%20use%20them%20to%20learn%20SSL%20via%20contrastive%20learning.%20To%20this%20end%2C%20previous%0Awork%20samples%20one%20of%20local%20image%20features%20as%20the%20image%20embedding%20and%20aggregates%0Aall%20local%20audio%20features%20to%20obtain%20the%20audio%20embedding%2C%20which%20is%20far%20from%0Aoptimal%20due%20to%20the%20presence%20of%20noise%20and%20background%20irrelevant%20to%20the%20actual%0Atarget%20in%20the%20input.%20We%20present%20a%20novel%20SSL%20method%20that%20addresses%20this%20chronic%0Aissue%20by%20joint%20slot%20attention%20on%20image%20and%20audio.%20To%20be%20specific%2C%20two%20slots%0Acompetitively%20attend%20image%20and%20audio%20features%20to%20decompose%20them%20into%20target%20and%0Aoff-target%20representations%2C%20and%20only%20target%20representations%20of%20image%20and%20audio%0Aare%20used%20for%20contrastive%20learning.%20Also%2C%20we%20introduce%20cross-modal%20attention%0Amatching%20to%20further%20align%20local%20features%20of%20image%20and%20audio.%20Our%20method%0Aachieved%20the%20best%20in%20almost%20all%20settings%20on%20three%20public%20benchmarks%20for%20SSL%2C%0Aand%20substantially%20outperformed%20all%20the%20prior%20work%20in%20cross-modal%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15118v1&entry.124074799=Read"},
{"title": "Audio-Visual Class-Incremental Learning for Fish Feeding intensity\n  Assessment in Aquaculture", "author": "Meng Cui and Xianghu Yue and Xinyuan Qian and Jinzheng Zhao and Haohe Liu and Xubo Liu and Daoliang Li and Wenwu Wang", "abstract": "  Fish Feeding Intensity Assessment (FFIA) is crucial in industrial aquaculture\nmanagement. Recent multi-modal approaches have shown promise in improving FFIA\nrobustness and efficiency. However, these methods face significant challenges\nwhen adapting to new fish species or environments due to catastrophic\nforgetting and the lack of suitable datasets. To address these limitations, we\nfirst introduce AV-CIL-FFIA, a new dataset comprising 81,932 labelled\naudio-visual clips capturing feeding intensities across six different fish\nspecies in real aquaculture environments. Then, we pioneer audio-visual class\nincremental learning (CIL) for FFIA and demonstrate through benchmarking on\nAV-CIL-FFIA that it significantly outperforms single-modality methods. Existing\nCIL methods rely heavily on historical data. Exemplar-based approaches store\nraw samples, creating storage challenges, while exemplar-free methods avoid\ndata storage but struggle to distinguish subtle feeding intensity variations\nacross different fish species. To overcome these limitations, we introduce\nHAIL-FFIA, a novel audio-visual class-incremental learning framework that\nbridges this gap with a prototype-based approach that achieves exemplar-free\nefficiency while preserving essential knowledge through compact feature\nrepresentations. Specifically, HAIL-FFIA employs hierarchical representation\nlearning with a dual-path knowledge preservation mechanism that separates\ngeneral intensity knowledge from fish-specific characteristics. Additionally,\nit features a dynamic modality balancing system that adaptively adjusts the\nimportance of audio versus visual information based on feeding behaviour\nstages. Experimental results show that HAIL-FFIA is superior to SOTA methods on\nAV-CIL-FFIA, achieving higher accuracy with lower storage needs while\neffectively mitigating catastrophic forgetting in incremental fish species\nlearning.\n", "link": "http://arxiv.org/abs/2504.15171v1", "date": "2025-04-21", "relevancy": 2.3826, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4774}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4774}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio-Visual%20Class-Incremental%20Learning%20for%20Fish%20Feeding%20intensity%0A%20%20Assessment%20in%20Aquaculture&body=Title%3A%20Audio-Visual%20Class-Incremental%20Learning%20for%20Fish%20Feeding%20intensity%0A%20%20Assessment%20in%20Aquaculture%0AAuthor%3A%20Meng%20Cui%20and%20Xianghu%20Yue%20and%20Xinyuan%20Qian%20and%20Jinzheng%20Zhao%20and%20Haohe%20Liu%20and%20Xubo%20Liu%20and%20Daoliang%20Li%20and%20Wenwu%20Wang%0AAbstract%3A%20%20%20Fish%20Feeding%20Intensity%20Assessment%20%28FFIA%29%20is%20crucial%20in%20industrial%20aquaculture%0Amanagement.%20Recent%20multi-modal%20approaches%20have%20shown%20promise%20in%20improving%20FFIA%0Arobustness%20and%20efficiency.%20However%2C%20these%20methods%20face%20significant%20challenges%0Awhen%20adapting%20to%20new%20fish%20species%20or%20environments%20due%20to%20catastrophic%0Aforgetting%20and%20the%20lack%20of%20suitable%20datasets.%20To%20address%20these%20limitations%2C%20we%0Afirst%20introduce%20AV-CIL-FFIA%2C%20a%20new%20dataset%20comprising%2081%2C932%20labelled%0Aaudio-visual%20clips%20capturing%20feeding%20intensities%20across%20six%20different%20fish%0Aspecies%20in%20real%20aquaculture%20environments.%20Then%2C%20we%20pioneer%20audio-visual%20class%0Aincremental%20learning%20%28CIL%29%20for%20FFIA%20and%20demonstrate%20through%20benchmarking%20on%0AAV-CIL-FFIA%20that%20it%20significantly%20outperforms%20single-modality%20methods.%20Existing%0ACIL%20methods%20rely%20heavily%20on%20historical%20data.%20Exemplar-based%20approaches%20store%0Araw%20samples%2C%20creating%20storage%20challenges%2C%20while%20exemplar-free%20methods%20avoid%0Adata%20storage%20but%20struggle%20to%20distinguish%20subtle%20feeding%20intensity%20variations%0Aacross%20different%20fish%20species.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0AHAIL-FFIA%2C%20a%20novel%20audio-visual%20class-incremental%20learning%20framework%20that%0Abridges%20this%20gap%20with%20a%20prototype-based%20approach%20that%20achieves%20exemplar-free%0Aefficiency%20while%20preserving%20essential%20knowledge%20through%20compact%20feature%0Arepresentations.%20Specifically%2C%20HAIL-FFIA%20employs%20hierarchical%20representation%0Alearning%20with%20a%20dual-path%20knowledge%20preservation%20mechanism%20that%20separates%0Ageneral%20intensity%20knowledge%20from%20fish-specific%20characteristics.%20Additionally%2C%0Ait%20features%20a%20dynamic%20modality%20balancing%20system%20that%20adaptively%20adjusts%20the%0Aimportance%20of%20audio%20versus%20visual%20information%20based%20on%20feeding%20behaviour%0Astages.%20Experimental%20results%20show%20that%20HAIL-FFIA%20is%20superior%20to%20SOTA%20methods%20on%0AAV-CIL-FFIA%2C%20achieving%20higher%20accuracy%20with%20lower%20storage%20needs%20while%0Aeffectively%20mitigating%20catastrophic%20forgetting%20in%20incremental%20fish%20species%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio-Visual%2520Class-Incremental%2520Learning%2520for%2520Fish%2520Feeding%2520intensity%250A%2520%2520Assessment%2520in%2520Aquaculture%26entry.906535625%3DMeng%2520Cui%2520and%2520Xianghu%2520Yue%2520and%2520Xinyuan%2520Qian%2520and%2520Jinzheng%2520Zhao%2520and%2520Haohe%2520Liu%2520and%2520Xubo%2520Liu%2520and%2520Daoliang%2520Li%2520and%2520Wenwu%2520Wang%26entry.1292438233%3D%2520%2520Fish%2520Feeding%2520Intensity%2520Assessment%2520%2528FFIA%2529%2520is%2520crucial%2520in%2520industrial%2520aquaculture%250Amanagement.%2520Recent%2520multi-modal%2520approaches%2520have%2520shown%2520promise%2520in%2520improving%2520FFIA%250Arobustness%2520and%2520efficiency.%2520However%252C%2520these%2520methods%2520face%2520significant%2520challenges%250Awhen%2520adapting%2520to%2520new%2520fish%2520species%2520or%2520environments%2520due%2520to%2520catastrophic%250Aforgetting%2520and%2520the%2520lack%2520of%2520suitable%2520datasets.%2520To%2520address%2520these%2520limitations%252C%2520we%250Afirst%2520introduce%2520AV-CIL-FFIA%252C%2520a%2520new%2520dataset%2520comprising%252081%252C932%2520labelled%250Aaudio-visual%2520clips%2520capturing%2520feeding%2520intensities%2520across%2520six%2520different%2520fish%250Aspecies%2520in%2520real%2520aquaculture%2520environments.%2520Then%252C%2520we%2520pioneer%2520audio-visual%2520class%250Aincremental%2520learning%2520%2528CIL%2529%2520for%2520FFIA%2520and%2520demonstrate%2520through%2520benchmarking%2520on%250AAV-CIL-FFIA%2520that%2520it%2520significantly%2520outperforms%2520single-modality%2520methods.%2520Existing%250ACIL%2520methods%2520rely%2520heavily%2520on%2520historical%2520data.%2520Exemplar-based%2520approaches%2520store%250Araw%2520samples%252C%2520creating%2520storage%2520challenges%252C%2520while%2520exemplar-free%2520methods%2520avoid%250Adata%2520storage%2520but%2520struggle%2520to%2520distinguish%2520subtle%2520feeding%2520intensity%2520variations%250Aacross%2520different%2520fish%2520species.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%250AHAIL-FFIA%252C%2520a%2520novel%2520audio-visual%2520class-incremental%2520learning%2520framework%2520that%250Abridges%2520this%2520gap%2520with%2520a%2520prototype-based%2520approach%2520that%2520achieves%2520exemplar-free%250Aefficiency%2520while%2520preserving%2520essential%2520knowledge%2520through%2520compact%2520feature%250Arepresentations.%2520Specifically%252C%2520HAIL-FFIA%2520employs%2520hierarchical%2520representation%250Alearning%2520with%2520a%2520dual-path%2520knowledge%2520preservation%2520mechanism%2520that%2520separates%250Ageneral%2520intensity%2520knowledge%2520from%2520fish-specific%2520characteristics.%2520Additionally%252C%250Ait%2520features%2520a%2520dynamic%2520modality%2520balancing%2520system%2520that%2520adaptively%2520adjusts%2520the%250Aimportance%2520of%2520audio%2520versus%2520visual%2520information%2520based%2520on%2520feeding%2520behaviour%250Astages.%2520Experimental%2520results%2520show%2520that%2520HAIL-FFIA%2520is%2520superior%2520to%2520SOTA%2520methods%2520on%250AAV-CIL-FFIA%252C%2520achieving%2520higher%2520accuracy%2520with%2520lower%2520storage%2520needs%2520while%250Aeffectively%2520mitigating%2520catastrophic%2520forgetting%2520in%2520incremental%2520fish%2520species%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio-Visual%20Class-Incremental%20Learning%20for%20Fish%20Feeding%20intensity%0A%20%20Assessment%20in%20Aquaculture&entry.906535625=Meng%20Cui%20and%20Xianghu%20Yue%20and%20Xinyuan%20Qian%20and%20Jinzheng%20Zhao%20and%20Haohe%20Liu%20and%20Xubo%20Liu%20and%20Daoliang%20Li%20and%20Wenwu%20Wang&entry.1292438233=%20%20Fish%20Feeding%20Intensity%20Assessment%20%28FFIA%29%20is%20crucial%20in%20industrial%20aquaculture%0Amanagement.%20Recent%20multi-modal%20approaches%20have%20shown%20promise%20in%20improving%20FFIA%0Arobustness%20and%20efficiency.%20However%2C%20these%20methods%20face%20significant%20challenges%0Awhen%20adapting%20to%20new%20fish%20species%20or%20environments%20due%20to%20catastrophic%0Aforgetting%20and%20the%20lack%20of%20suitable%20datasets.%20To%20address%20these%20limitations%2C%20we%0Afirst%20introduce%20AV-CIL-FFIA%2C%20a%20new%20dataset%20comprising%2081%2C932%20labelled%0Aaudio-visual%20clips%20capturing%20feeding%20intensities%20across%20six%20different%20fish%0Aspecies%20in%20real%20aquaculture%20environments.%20Then%2C%20we%20pioneer%20audio-visual%20class%0Aincremental%20learning%20%28CIL%29%20for%20FFIA%20and%20demonstrate%20through%20benchmarking%20on%0AAV-CIL-FFIA%20that%20it%20significantly%20outperforms%20single-modality%20methods.%20Existing%0ACIL%20methods%20rely%20heavily%20on%20historical%20data.%20Exemplar-based%20approaches%20store%0Araw%20samples%2C%20creating%20storage%20challenges%2C%20while%20exemplar-free%20methods%20avoid%0Adata%20storage%20but%20struggle%20to%20distinguish%20subtle%20feeding%20intensity%20variations%0Aacross%20different%20fish%20species.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0AHAIL-FFIA%2C%20a%20novel%20audio-visual%20class-incremental%20learning%20framework%20that%0Abridges%20this%20gap%20with%20a%20prototype-based%20approach%20that%20achieves%20exemplar-free%0Aefficiency%20while%20preserving%20essential%20knowledge%20through%20compact%20feature%0Arepresentations.%20Specifically%2C%20HAIL-FFIA%20employs%20hierarchical%20representation%0Alearning%20with%20a%20dual-path%20knowledge%20preservation%20mechanism%20that%20separates%0Ageneral%20intensity%20knowledge%20from%20fish-specific%20characteristics.%20Additionally%2C%0Ait%20features%20a%20dynamic%20modality%20balancing%20system%20that%20adaptively%20adjusts%20the%0Aimportance%20of%20audio%20versus%20visual%20information%20based%20on%20feeding%20behaviour%0Astages.%20Experimental%20results%20show%20that%20HAIL-FFIA%20is%20superior%20to%20SOTA%20methods%20on%0AAV-CIL-FFIA%2C%20achieving%20higher%20accuracy%20with%20lower%20storage%20needs%20while%0Aeffectively%20mitigating%20catastrophic%20forgetting%20in%20incremental%20fish%20species%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15171v1&entry.124074799=Read"},
{"title": "On Learning Parallel Pancakes with Mostly Uniform Weights", "author": "Ilias Diakonikolas and Daniel M. Kane and Sushrut Karmalkar and Jasper C. H. Lee and Thanasis Pittas", "abstract": "  We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on\n$\\mathbb{R}^d$. This task is known to have complexity $d^{\\Omega(k)}$ in full\ngenerality. To circumvent this exponential lower bound on the number of\ncomponents, research has focused on learning families of GMMs satisfying\nadditional structural properties. A natural assumption posits that the\ncomponent weights are not exponentially small and that the components have the\nsame unknown covariance. Recent work gave a $d^{O(\\log(1/w_{\\min}))}$-time\nalgorithm for this class of GMMs, where $w_{\\min}$ is the minimum weight. Our\nfirst main result is a Statistical Query (SQ) lower bound showing that this\nquasi-polynomial upper bound is essentially best possible, even for the special\ncase of uniform weights. Specifically, we show that it is SQ-hard to\ndistinguish between such a mixture and the standard Gaussian. We further\nexplore how the distribution of weights affects the complexity of this task.\nOur second main result is a quasi-polynomial upper bound for the aforementioned\ntesting task when most of the weights are uniform while a small fraction of the\nweights are potentially arbitrary.\n", "link": "http://arxiv.org/abs/2504.15251v1", "date": "2025-04-21", "relevancy": 2.3659, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4839}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4812}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Learning%20Parallel%20Pancakes%20with%20Mostly%20Uniform%20Weights&body=Title%3A%20On%20Learning%20Parallel%20Pancakes%20with%20Mostly%20Uniform%20Weights%0AAuthor%3A%20Ilias%20Diakonikolas%20and%20Daniel%20M.%20Kane%20and%20Sushrut%20Karmalkar%20and%20Jasper%20C.%20H.%20Lee%20and%20Thanasis%20Pittas%0AAbstract%3A%20%20%20We%20study%20the%20complexity%20of%20learning%20%24k%24-mixtures%20of%20Gaussians%20%28%24k%24-GMMs%29%20on%0A%24%5Cmathbb%7BR%7D%5Ed%24.%20This%20task%20is%20known%20to%20have%20complexity%20%24d%5E%7B%5COmega%28k%29%7D%24%20in%20full%0Agenerality.%20To%20circumvent%20this%20exponential%20lower%20bound%20on%20the%20number%20of%0Acomponents%2C%20research%20has%20focused%20on%20learning%20families%20of%20GMMs%20satisfying%0Aadditional%20structural%20properties.%20A%20natural%20assumption%20posits%20that%20the%0Acomponent%20weights%20are%20not%20exponentially%20small%20and%20that%20the%20components%20have%20the%0Asame%20unknown%20covariance.%20Recent%20work%20gave%20a%20%24d%5E%7BO%28%5Clog%281/w_%7B%5Cmin%7D%29%29%7D%24-time%0Aalgorithm%20for%20this%20class%20of%20GMMs%2C%20where%20%24w_%7B%5Cmin%7D%24%20is%20the%20minimum%20weight.%20Our%0Afirst%20main%20result%20is%20a%20Statistical%20Query%20%28SQ%29%20lower%20bound%20showing%20that%20this%0Aquasi-polynomial%20upper%20bound%20is%20essentially%20best%20possible%2C%20even%20for%20the%20special%0Acase%20of%20uniform%20weights.%20Specifically%2C%20we%20show%20that%20it%20is%20SQ-hard%20to%0Adistinguish%20between%20such%20a%20mixture%20and%20the%20standard%20Gaussian.%20We%20further%0Aexplore%20how%20the%20distribution%20of%20weights%20affects%20the%20complexity%20of%20this%20task.%0AOur%20second%20main%20result%20is%20a%20quasi-polynomial%20upper%20bound%20for%20the%20aforementioned%0Atesting%20task%20when%20most%20of%20the%20weights%20are%20uniform%20while%20a%20small%20fraction%20of%20the%0Aweights%20are%20potentially%20arbitrary.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Learning%2520Parallel%2520Pancakes%2520with%2520Mostly%2520Uniform%2520Weights%26entry.906535625%3DIlias%2520Diakonikolas%2520and%2520Daniel%2520M.%2520Kane%2520and%2520Sushrut%2520Karmalkar%2520and%2520Jasper%2520C.%2520H.%2520Lee%2520and%2520Thanasis%2520Pittas%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520complexity%2520of%2520learning%2520%2524k%2524-mixtures%2520of%2520Gaussians%2520%2528%2524k%2524-GMMs%2529%2520on%250A%2524%255Cmathbb%257BR%257D%255Ed%2524.%2520This%2520task%2520is%2520known%2520to%2520have%2520complexity%2520%2524d%255E%257B%255COmega%2528k%2529%257D%2524%2520in%2520full%250Agenerality.%2520To%2520circumvent%2520this%2520exponential%2520lower%2520bound%2520on%2520the%2520number%2520of%250Acomponents%252C%2520research%2520has%2520focused%2520on%2520learning%2520families%2520of%2520GMMs%2520satisfying%250Aadditional%2520structural%2520properties.%2520A%2520natural%2520assumption%2520posits%2520that%2520the%250Acomponent%2520weights%2520are%2520not%2520exponentially%2520small%2520and%2520that%2520the%2520components%2520have%2520the%250Asame%2520unknown%2520covariance.%2520Recent%2520work%2520gave%2520a%2520%2524d%255E%257BO%2528%255Clog%25281/w_%257B%255Cmin%257D%2529%2529%257D%2524-time%250Aalgorithm%2520for%2520this%2520class%2520of%2520GMMs%252C%2520where%2520%2524w_%257B%255Cmin%257D%2524%2520is%2520the%2520minimum%2520weight.%2520Our%250Afirst%2520main%2520result%2520is%2520a%2520Statistical%2520Query%2520%2528SQ%2529%2520lower%2520bound%2520showing%2520that%2520this%250Aquasi-polynomial%2520upper%2520bound%2520is%2520essentially%2520best%2520possible%252C%2520even%2520for%2520the%2520special%250Acase%2520of%2520uniform%2520weights.%2520Specifically%252C%2520we%2520show%2520that%2520it%2520is%2520SQ-hard%2520to%250Adistinguish%2520between%2520such%2520a%2520mixture%2520and%2520the%2520standard%2520Gaussian.%2520We%2520further%250Aexplore%2520how%2520the%2520distribution%2520of%2520weights%2520affects%2520the%2520complexity%2520of%2520this%2520task.%250AOur%2520second%2520main%2520result%2520is%2520a%2520quasi-polynomial%2520upper%2520bound%2520for%2520the%2520aforementioned%250Atesting%2520task%2520when%2520most%2520of%2520the%2520weights%2520are%2520uniform%2520while%2520a%2520small%2520fraction%2520of%2520the%250Aweights%2520are%2520potentially%2520arbitrary.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Learning%20Parallel%20Pancakes%20with%20Mostly%20Uniform%20Weights&entry.906535625=Ilias%20Diakonikolas%20and%20Daniel%20M.%20Kane%20and%20Sushrut%20Karmalkar%20and%20Jasper%20C.%20H.%20Lee%20and%20Thanasis%20Pittas&entry.1292438233=%20%20We%20study%20the%20complexity%20of%20learning%20%24k%24-mixtures%20of%20Gaussians%20%28%24k%24-GMMs%29%20on%0A%24%5Cmathbb%7BR%7D%5Ed%24.%20This%20task%20is%20known%20to%20have%20complexity%20%24d%5E%7B%5COmega%28k%29%7D%24%20in%20full%0Agenerality.%20To%20circumvent%20this%20exponential%20lower%20bound%20on%20the%20number%20of%0Acomponents%2C%20research%20has%20focused%20on%20learning%20families%20of%20GMMs%20satisfying%0Aadditional%20structural%20properties.%20A%20natural%20assumption%20posits%20that%20the%0Acomponent%20weights%20are%20not%20exponentially%20small%20and%20that%20the%20components%20have%20the%0Asame%20unknown%20covariance.%20Recent%20work%20gave%20a%20%24d%5E%7BO%28%5Clog%281/w_%7B%5Cmin%7D%29%29%7D%24-time%0Aalgorithm%20for%20this%20class%20of%20GMMs%2C%20where%20%24w_%7B%5Cmin%7D%24%20is%20the%20minimum%20weight.%20Our%0Afirst%20main%20result%20is%20a%20Statistical%20Query%20%28SQ%29%20lower%20bound%20showing%20that%20this%0Aquasi-polynomial%20upper%20bound%20is%20essentially%20best%20possible%2C%20even%20for%20the%20special%0Acase%20of%20uniform%20weights.%20Specifically%2C%20we%20show%20that%20it%20is%20SQ-hard%20to%0Adistinguish%20between%20such%20a%20mixture%20and%20the%20standard%20Gaussian.%20We%20further%0Aexplore%20how%20the%20distribution%20of%20weights%20affects%20the%20complexity%20of%20this%20task.%0AOur%20second%20main%20result%20is%20a%20quasi-polynomial%20upper%20bound%20for%20the%20aforementioned%0Atesting%20task%20when%20most%20of%20the%20weights%20are%20uniform%20while%20a%20small%20fraction%20of%20the%0Aweights%20are%20potentially%20arbitrary.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15251v1&entry.124074799=Read"},
{"title": "Inverse Constitutional AI: Compressing Preferences into Principles", "author": "Arduin Findeis and Timo Kaufmann and Eyke H\u00fcllermeier and Samuel Albanie and Robert Mullins", "abstract": "  Feedback data is widely used for fine-tuning and evaluating state-of-the-art\nAI models. Pairwise text preferences, where human or AI annotators select the\n\"better\" of two options, are particularly common. Such preferences are used to\ntrain (reward) models or to rank models with aggregate statistics. For many\napplications it is desirable to understand annotator preferences in addition to\nmodelling them - not least because extensive prior work has shown various\nunintended biases in preference datasets. Yet, preference datasets remain\nchallenging to interpret. Neither black-box reward models nor statistics can\nanswer why one text is preferred over another. Manual interpretation of the\nnumerous (long) response pairs is usually equally infeasible. In this paper, we\nintroduce the Inverse Constitutional AI (ICAI) problem, formulating the\ninterpretation of pairwise text preference data as a compression task. In\nconstitutional AI, a set of principles (a constitution) is used to provide\nfeedback and fine-tune AI models. ICAI inverts this process: given a feedback\ndataset, we aim to extract a constitution that best enables a large language\nmodel (LLM) to reconstruct the original annotations. We propose a corresponding\nICAI algorithm and validate its generated constitutions quantitatively based on\nannotation reconstruction accuracy on several datasets: (a) synthetic feedback\ndata with known principles; (b) AlpacaEval cross-annotated human feedback data;\n(c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse\ndemographic groups. As a short and interpretable representation of the original\ndataset, generated constitutions have many potential use cases: help identify\nundesirable annotator biases, understand model performance better, scale\nfeedback to unseen data, or adapt models to individual user or group\npreferences. We release the source code at https://github.com/rdnfn/icai.\n", "link": "http://arxiv.org/abs/2406.06560v2", "date": "2025-04-21", "relevancy": 2.3589, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4973}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Constitutional%20AI%3A%20Compressing%20Preferences%20into%20Principles&body=Title%3A%20Inverse%20Constitutional%20AI%3A%20Compressing%20Preferences%20into%20Principles%0AAuthor%3A%20Arduin%20Findeis%20and%20Timo%20Kaufmann%20and%20Eyke%20H%C3%BCllermeier%20and%20Samuel%20Albanie%20and%20Robert%20Mullins%0AAbstract%3A%20%20%20Feedback%20data%20is%20widely%20used%20for%20fine-tuning%20and%20evaluating%20state-of-the-art%0AAI%20models.%20Pairwise%20text%20preferences%2C%20where%20human%20or%20AI%20annotators%20select%20the%0A%22better%22%20of%20two%20options%2C%20are%20particularly%20common.%20Such%20preferences%20are%20used%20to%0Atrain%20%28reward%29%20models%20or%20to%20rank%20models%20with%20aggregate%20statistics.%20For%20many%0Aapplications%20it%20is%20desirable%20to%20understand%20annotator%20preferences%20in%20addition%20to%0Amodelling%20them%20-%20not%20least%20because%20extensive%20prior%20work%20has%20shown%20various%0Aunintended%20biases%20in%20preference%20datasets.%20Yet%2C%20preference%20datasets%20remain%0Achallenging%20to%20interpret.%20Neither%20black-box%20reward%20models%20nor%20statistics%20can%0Aanswer%20why%20one%20text%20is%20preferred%20over%20another.%20Manual%20interpretation%20of%20the%0Anumerous%20%28long%29%20response%20pairs%20is%20usually%20equally%20infeasible.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20Inverse%20Constitutional%20AI%20%28ICAI%29%20problem%2C%20formulating%20the%0Ainterpretation%20of%20pairwise%20text%20preference%20data%20as%20a%20compression%20task.%20In%0Aconstitutional%20AI%2C%20a%20set%20of%20principles%20%28a%20constitution%29%20is%20used%20to%20provide%0Afeedback%20and%20fine-tune%20AI%20models.%20ICAI%20inverts%20this%20process%3A%20given%20a%20feedback%0Adataset%2C%20we%20aim%20to%20extract%20a%20constitution%20that%20best%20enables%20a%20large%20language%0Amodel%20%28LLM%29%20to%20reconstruct%20the%20original%20annotations.%20We%20propose%20a%20corresponding%0AICAI%20algorithm%20and%20validate%20its%20generated%20constitutions%20quantitatively%20based%20on%0Aannotation%20reconstruction%20accuracy%20on%20several%20datasets%3A%20%28a%29%20synthetic%20feedback%0Adata%20with%20known%20principles%3B%20%28b%29%20AlpacaEval%20cross-annotated%20human%20feedback%20data%3B%0A%28c%29%20crowdsourced%20Chatbot%20Arena%20data%3B%20and%20%28d%29%20PRISM%20data%20from%20diverse%0Ademographic%20groups.%20As%20a%20short%20and%20interpretable%20representation%20of%20the%20original%0Adataset%2C%20generated%20constitutions%20have%20many%20potential%20use%20cases%3A%20help%20identify%0Aundesirable%20annotator%20biases%2C%20understand%20model%20performance%20better%2C%20scale%0Afeedback%20to%20unseen%20data%2C%20or%20adapt%20models%20to%20individual%20user%20or%20group%0Apreferences.%20We%20release%20the%20source%20code%20at%20https%3A//github.com/rdnfn/icai.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06560v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Constitutional%2520AI%253A%2520Compressing%2520Preferences%2520into%2520Principles%26entry.906535625%3DArduin%2520Findeis%2520and%2520Timo%2520Kaufmann%2520and%2520Eyke%2520H%25C3%25BCllermeier%2520and%2520Samuel%2520Albanie%2520and%2520Robert%2520Mullins%26entry.1292438233%3D%2520%2520Feedback%2520data%2520is%2520widely%2520used%2520for%2520fine-tuning%2520and%2520evaluating%2520state-of-the-art%250AAI%2520models.%2520Pairwise%2520text%2520preferences%252C%2520where%2520human%2520or%2520AI%2520annotators%2520select%2520the%250A%2522better%2522%2520of%2520two%2520options%252C%2520are%2520particularly%2520common.%2520Such%2520preferences%2520are%2520used%2520to%250Atrain%2520%2528reward%2529%2520models%2520or%2520to%2520rank%2520models%2520with%2520aggregate%2520statistics.%2520For%2520many%250Aapplications%2520it%2520is%2520desirable%2520to%2520understand%2520annotator%2520preferences%2520in%2520addition%2520to%250Amodelling%2520them%2520-%2520not%2520least%2520because%2520extensive%2520prior%2520work%2520has%2520shown%2520various%250Aunintended%2520biases%2520in%2520preference%2520datasets.%2520Yet%252C%2520preference%2520datasets%2520remain%250Achallenging%2520to%2520interpret.%2520Neither%2520black-box%2520reward%2520models%2520nor%2520statistics%2520can%250Aanswer%2520why%2520one%2520text%2520is%2520preferred%2520over%2520another.%2520Manual%2520interpretation%2520of%2520the%250Anumerous%2520%2528long%2529%2520response%2520pairs%2520is%2520usually%2520equally%2520infeasible.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520the%2520Inverse%2520Constitutional%2520AI%2520%2528ICAI%2529%2520problem%252C%2520formulating%2520the%250Ainterpretation%2520of%2520pairwise%2520text%2520preference%2520data%2520as%2520a%2520compression%2520task.%2520In%250Aconstitutional%2520AI%252C%2520a%2520set%2520of%2520principles%2520%2528a%2520constitution%2529%2520is%2520used%2520to%2520provide%250Afeedback%2520and%2520fine-tune%2520AI%2520models.%2520ICAI%2520inverts%2520this%2520process%253A%2520given%2520a%2520feedback%250Adataset%252C%2520we%2520aim%2520to%2520extract%2520a%2520constitution%2520that%2520best%2520enables%2520a%2520large%2520language%250Amodel%2520%2528LLM%2529%2520to%2520reconstruct%2520the%2520original%2520annotations.%2520We%2520propose%2520a%2520corresponding%250AICAI%2520algorithm%2520and%2520validate%2520its%2520generated%2520constitutions%2520quantitatively%2520based%2520on%250Aannotation%2520reconstruction%2520accuracy%2520on%2520several%2520datasets%253A%2520%2528a%2529%2520synthetic%2520feedback%250Adata%2520with%2520known%2520principles%253B%2520%2528b%2529%2520AlpacaEval%2520cross-annotated%2520human%2520feedback%2520data%253B%250A%2528c%2529%2520crowdsourced%2520Chatbot%2520Arena%2520data%253B%2520and%2520%2528d%2529%2520PRISM%2520data%2520from%2520diverse%250Ademographic%2520groups.%2520As%2520a%2520short%2520and%2520interpretable%2520representation%2520of%2520the%2520original%250Adataset%252C%2520generated%2520constitutions%2520have%2520many%2520potential%2520use%2520cases%253A%2520help%2520identify%250Aundesirable%2520annotator%2520biases%252C%2520understand%2520model%2520performance%2520better%252C%2520scale%250Afeedback%2520to%2520unseen%2520data%252C%2520or%2520adapt%2520models%2520to%2520individual%2520user%2520or%2520group%250Apreferences.%2520We%2520release%2520the%2520source%2520code%2520at%2520https%253A//github.com/rdnfn/icai.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06560v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Constitutional%20AI%3A%20Compressing%20Preferences%20into%20Principles&entry.906535625=Arduin%20Findeis%20and%20Timo%20Kaufmann%20and%20Eyke%20H%C3%BCllermeier%20and%20Samuel%20Albanie%20and%20Robert%20Mullins&entry.1292438233=%20%20Feedback%20data%20is%20widely%20used%20for%20fine-tuning%20and%20evaluating%20state-of-the-art%0AAI%20models.%20Pairwise%20text%20preferences%2C%20where%20human%20or%20AI%20annotators%20select%20the%0A%22better%22%20of%20two%20options%2C%20are%20particularly%20common.%20Such%20preferences%20are%20used%20to%0Atrain%20%28reward%29%20models%20or%20to%20rank%20models%20with%20aggregate%20statistics.%20For%20many%0Aapplications%20it%20is%20desirable%20to%20understand%20annotator%20preferences%20in%20addition%20to%0Amodelling%20them%20-%20not%20least%20because%20extensive%20prior%20work%20has%20shown%20various%0Aunintended%20biases%20in%20preference%20datasets.%20Yet%2C%20preference%20datasets%20remain%0Achallenging%20to%20interpret.%20Neither%20black-box%20reward%20models%20nor%20statistics%20can%0Aanswer%20why%20one%20text%20is%20preferred%20over%20another.%20Manual%20interpretation%20of%20the%0Anumerous%20%28long%29%20response%20pairs%20is%20usually%20equally%20infeasible.%20In%20this%20paper%2C%20we%0Aintroduce%20the%20Inverse%20Constitutional%20AI%20%28ICAI%29%20problem%2C%20formulating%20the%0Ainterpretation%20of%20pairwise%20text%20preference%20data%20as%20a%20compression%20task.%20In%0Aconstitutional%20AI%2C%20a%20set%20of%20principles%20%28a%20constitution%29%20is%20used%20to%20provide%0Afeedback%20and%20fine-tune%20AI%20models.%20ICAI%20inverts%20this%20process%3A%20given%20a%20feedback%0Adataset%2C%20we%20aim%20to%20extract%20a%20constitution%20that%20best%20enables%20a%20large%20language%0Amodel%20%28LLM%29%20to%20reconstruct%20the%20original%20annotations.%20We%20propose%20a%20corresponding%0AICAI%20algorithm%20and%20validate%20its%20generated%20constitutions%20quantitatively%20based%20on%0Aannotation%20reconstruction%20accuracy%20on%20several%20datasets%3A%20%28a%29%20synthetic%20feedback%0Adata%20with%20known%20principles%3B%20%28b%29%20AlpacaEval%20cross-annotated%20human%20feedback%20data%3B%0A%28c%29%20crowdsourced%20Chatbot%20Arena%20data%3B%20and%20%28d%29%20PRISM%20data%20from%20diverse%0Ademographic%20groups.%20As%20a%20short%20and%20interpretable%20representation%20of%20the%20original%0Adataset%2C%20generated%20constitutions%20have%20many%20potential%20use%20cases%3A%20help%20identify%0Aundesirable%20annotator%20biases%2C%20understand%20model%20performance%20better%2C%20scale%0Afeedback%20to%20unseen%20data%2C%20or%20adapt%20models%20to%20individual%20user%20or%20group%0Apreferences.%20We%20release%20the%20source%20code%20at%20https%3A//github.com/rdnfn/icai.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06560v2&entry.124074799=Read"},
{"title": "Trainable Quantum Neural Network for Multiclass Image Classification\n  with the Power of Pre-trained Tree Tensor Networks", "author": "Keisuke Murota and Takumi Kobori", "abstract": "  Tree tensor networks (TTNs) offer powerful models for image classification.\nWhile these TTN image classifiers already show excellent performance on\nclassical hardware, embedding them into quantum neural networks (QNNs) may\nfurther improve the performance by leveraging quantum resources. However,\nembedding TTN classifiers into QNNs for multiclass classification remains\nchallenging. Key obstacles are the highorder gate operations required for large\nbond dimensions and the mid-circuit postselection with exponentially low\nsuccess rates necessary for the exact embedding. In this work, to address these\nchallenges, we propose forest tensor network (FTN)-classifiers, which aggregate\nmultiple small-bond-dimension TTNs. This allows us to handle multiclass\nclassification without requiring large gates in the embedded circuits. We then\nremove the overhead of mid-circuit postselection by extending the adiabatic\nencoding framework to our setting and smoothly encode the FTN-classifiers into\na quantum forest tensor network (qFTN)- classifiers. Numerical experiments on\nMNIST and CIFAR-10 demonstrate that we can successfully train FTN-classifiers\nand encode them into qFTN-classifiers, while maintaining or even improving the\nperformance of the pre-trained FTN-classifiers. These results suggest that\nsynergy between TTN classification models and QNNs can provide a robust and\nscalable framework for multiclass quantum-enhanced image classification.\n", "link": "http://arxiv.org/abs/2504.14995v1", "date": "2025-04-21", "relevancy": 2.35, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4736}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4691}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trainable%20Quantum%20Neural%20Network%20for%20Multiclass%20Image%20Classification%0A%20%20with%20the%20Power%20of%20Pre-trained%20Tree%20Tensor%20Networks&body=Title%3A%20Trainable%20Quantum%20Neural%20Network%20for%20Multiclass%20Image%20Classification%0A%20%20with%20the%20Power%20of%20Pre-trained%20Tree%20Tensor%20Networks%0AAuthor%3A%20Keisuke%20Murota%20and%20Takumi%20Kobori%0AAbstract%3A%20%20%20Tree%20tensor%20networks%20%28TTNs%29%20offer%20powerful%20models%20for%20image%20classification.%0AWhile%20these%20TTN%20image%20classifiers%20already%20show%20excellent%20performance%20on%0Aclassical%20hardware%2C%20embedding%20them%20into%20quantum%20neural%20networks%20%28QNNs%29%20may%0Afurther%20improve%20the%20performance%20by%20leveraging%20quantum%20resources.%20However%2C%0Aembedding%20TTN%20classifiers%20into%20QNNs%20for%20multiclass%20classification%20remains%0Achallenging.%20Key%20obstacles%20are%20the%20highorder%20gate%20operations%20required%20for%20large%0Abond%20dimensions%20and%20the%20mid-circuit%20postselection%20with%20exponentially%20low%0Asuccess%20rates%20necessary%20for%20the%20exact%20embedding.%20In%20this%20work%2C%20to%20address%20these%0Achallenges%2C%20we%20propose%20forest%20tensor%20network%20%28FTN%29-classifiers%2C%20which%20aggregate%0Amultiple%20small-bond-dimension%20TTNs.%20This%20allows%20us%20to%20handle%20multiclass%0Aclassification%20without%20requiring%20large%20gates%20in%20the%20embedded%20circuits.%20We%20then%0Aremove%20the%20overhead%20of%20mid-circuit%20postselection%20by%20extending%20the%20adiabatic%0Aencoding%20framework%20to%20our%20setting%20and%20smoothly%20encode%20the%20FTN-classifiers%20into%0Aa%20quantum%20forest%20tensor%20network%20%28qFTN%29-%20classifiers.%20Numerical%20experiments%20on%0AMNIST%20and%20CIFAR-10%20demonstrate%20that%20we%20can%20successfully%20train%20FTN-classifiers%0Aand%20encode%20them%20into%20qFTN-classifiers%2C%20while%20maintaining%20or%20even%20improving%20the%0Aperformance%20of%20the%20pre-trained%20FTN-classifiers.%20These%20results%20suggest%20that%0Asynergy%20between%20TTN%20classification%20models%20and%20QNNs%20can%20provide%20a%20robust%20and%0Ascalable%20framework%20for%20multiclass%20quantum-enhanced%20image%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrainable%2520Quantum%2520Neural%2520Network%2520for%2520Multiclass%2520Image%2520Classification%250A%2520%2520with%2520the%2520Power%2520of%2520Pre-trained%2520Tree%2520Tensor%2520Networks%26entry.906535625%3DKeisuke%2520Murota%2520and%2520Takumi%2520Kobori%26entry.1292438233%3D%2520%2520Tree%2520tensor%2520networks%2520%2528TTNs%2529%2520offer%2520powerful%2520models%2520for%2520image%2520classification.%250AWhile%2520these%2520TTN%2520image%2520classifiers%2520already%2520show%2520excellent%2520performance%2520on%250Aclassical%2520hardware%252C%2520embedding%2520them%2520into%2520quantum%2520neural%2520networks%2520%2528QNNs%2529%2520may%250Afurther%2520improve%2520the%2520performance%2520by%2520leveraging%2520quantum%2520resources.%2520However%252C%250Aembedding%2520TTN%2520classifiers%2520into%2520QNNs%2520for%2520multiclass%2520classification%2520remains%250Achallenging.%2520Key%2520obstacles%2520are%2520the%2520highorder%2520gate%2520operations%2520required%2520for%2520large%250Abond%2520dimensions%2520and%2520the%2520mid-circuit%2520postselection%2520with%2520exponentially%2520low%250Asuccess%2520rates%2520necessary%2520for%2520the%2520exact%2520embedding.%2520In%2520this%2520work%252C%2520to%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520forest%2520tensor%2520network%2520%2528FTN%2529-classifiers%252C%2520which%2520aggregate%250Amultiple%2520small-bond-dimension%2520TTNs.%2520This%2520allows%2520us%2520to%2520handle%2520multiclass%250Aclassification%2520without%2520requiring%2520large%2520gates%2520in%2520the%2520embedded%2520circuits.%2520We%2520then%250Aremove%2520the%2520overhead%2520of%2520mid-circuit%2520postselection%2520by%2520extending%2520the%2520adiabatic%250Aencoding%2520framework%2520to%2520our%2520setting%2520and%2520smoothly%2520encode%2520the%2520FTN-classifiers%2520into%250Aa%2520quantum%2520forest%2520tensor%2520network%2520%2528qFTN%2529-%2520classifiers.%2520Numerical%2520experiments%2520on%250AMNIST%2520and%2520CIFAR-10%2520demonstrate%2520that%2520we%2520can%2520successfully%2520train%2520FTN-classifiers%250Aand%2520encode%2520them%2520into%2520qFTN-classifiers%252C%2520while%2520maintaining%2520or%2520even%2520improving%2520the%250Aperformance%2520of%2520the%2520pre-trained%2520FTN-classifiers.%2520These%2520results%2520suggest%2520that%250Asynergy%2520between%2520TTN%2520classification%2520models%2520and%2520QNNs%2520can%2520provide%2520a%2520robust%2520and%250Ascalable%2520framework%2520for%2520multiclass%2520quantum-enhanced%2520image%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trainable%20Quantum%20Neural%20Network%20for%20Multiclass%20Image%20Classification%0A%20%20with%20the%20Power%20of%20Pre-trained%20Tree%20Tensor%20Networks&entry.906535625=Keisuke%20Murota%20and%20Takumi%20Kobori&entry.1292438233=%20%20Tree%20tensor%20networks%20%28TTNs%29%20offer%20powerful%20models%20for%20image%20classification.%0AWhile%20these%20TTN%20image%20classifiers%20already%20show%20excellent%20performance%20on%0Aclassical%20hardware%2C%20embedding%20them%20into%20quantum%20neural%20networks%20%28QNNs%29%20may%0Afurther%20improve%20the%20performance%20by%20leveraging%20quantum%20resources.%20However%2C%0Aembedding%20TTN%20classifiers%20into%20QNNs%20for%20multiclass%20classification%20remains%0Achallenging.%20Key%20obstacles%20are%20the%20highorder%20gate%20operations%20required%20for%20large%0Abond%20dimensions%20and%20the%20mid-circuit%20postselection%20with%20exponentially%20low%0Asuccess%20rates%20necessary%20for%20the%20exact%20embedding.%20In%20this%20work%2C%20to%20address%20these%0Achallenges%2C%20we%20propose%20forest%20tensor%20network%20%28FTN%29-classifiers%2C%20which%20aggregate%0Amultiple%20small-bond-dimension%20TTNs.%20This%20allows%20us%20to%20handle%20multiclass%0Aclassification%20without%20requiring%20large%20gates%20in%20the%20embedded%20circuits.%20We%20then%0Aremove%20the%20overhead%20of%20mid-circuit%20postselection%20by%20extending%20the%20adiabatic%0Aencoding%20framework%20to%20our%20setting%20and%20smoothly%20encode%20the%20FTN-classifiers%20into%0Aa%20quantum%20forest%20tensor%20network%20%28qFTN%29-%20classifiers.%20Numerical%20experiments%20on%0AMNIST%20and%20CIFAR-10%20demonstrate%20that%20we%20can%20successfully%20train%20FTN-classifiers%0Aand%20encode%20them%20into%20qFTN-classifiers%2C%20while%20maintaining%20or%20even%20improving%20the%0Aperformance%20of%20the%20pre-trained%20FTN-classifiers.%20These%20results%20suggest%20that%0Asynergy%20between%20TTN%20classification%20models%20and%20QNNs%20can%20provide%20a%20robust%20and%0Ascalable%20framework%20for%20multiclass%20quantum-enhanced%20image%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14995v1&entry.124074799=Read"},
{"title": "DRAWER: Digital Reconstruction and Articulation With Environment Realism", "author": "Hongchi Xia and Entong Su and Marius Memmel and Arhan Jain and Raymond Yu and Numfor Mbiziwo-Tiapo and Ali Farhadi and Abhishek Gupta and Shenlong Wang and Wei-Chiu Ma", "abstract": "  Creating virtual digital replicas from real-world data unlocks significant\npotential across domains like gaming and robotics. In this paper, we present\nDRAWER, a novel framework that converts a video of a static indoor scene into a\nphotorealistic and interactive digital environment. Our approach centers on two\nmain contributions: (i) a reconstruction module based on a dual scene\nrepresentation that reconstructs the scene with fine-grained geometric details,\nand (ii) an articulation module that identifies articulation types and hinge\npositions, reconstructs simulatable shapes and appearances and integrates them\ninto the scene. The resulting virtual environment is photorealistic,\ninteractive, and runs in real time, with compatibility for game engines and\nrobotic simulation platforms. We demonstrate the potential of DRAWER by using\nit to automatically create an interactive game in Unreal Engine and to enable\nreal-to-sim-to-real transfer for robotics applications.\n", "link": "http://arxiv.org/abs/2504.15278v1", "date": "2025-04-21", "relevancy": 2.3247, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5829}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5829}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRAWER%3A%20Digital%20Reconstruction%20and%20Articulation%20With%20Environment%20Realism&body=Title%3A%20DRAWER%3A%20Digital%20Reconstruction%20and%20Articulation%20With%20Environment%20Realism%0AAuthor%3A%20Hongchi%20Xia%20and%20Entong%20Su%20and%20Marius%20Memmel%20and%20Arhan%20Jain%20and%20Raymond%20Yu%20and%20Numfor%20Mbiziwo-Tiapo%20and%20Ali%20Farhadi%20and%20Abhishek%20Gupta%20and%20Shenlong%20Wang%20and%20Wei-Chiu%20Ma%0AAbstract%3A%20%20%20Creating%20virtual%20digital%20replicas%20from%20real-world%20data%20unlocks%20significant%0Apotential%20across%20domains%20like%20gaming%20and%20robotics.%20In%20this%20paper%2C%20we%20present%0ADRAWER%2C%20a%20novel%20framework%20that%20converts%20a%20video%20of%20a%20static%20indoor%20scene%20into%20a%0Aphotorealistic%20and%20interactive%20digital%20environment.%20Our%20approach%20centers%20on%20two%0Amain%20contributions%3A%20%28i%29%20a%20reconstruction%20module%20based%20on%20a%20dual%20scene%0Arepresentation%20that%20reconstructs%20the%20scene%20with%20fine-grained%20geometric%20details%2C%0Aand%20%28ii%29%20an%20articulation%20module%20that%20identifies%20articulation%20types%20and%20hinge%0Apositions%2C%20reconstructs%20simulatable%20shapes%20and%20appearances%20and%20integrates%20them%0Ainto%20the%20scene.%20The%20resulting%20virtual%20environment%20is%20photorealistic%2C%0Ainteractive%2C%20and%20runs%20in%20real%20time%2C%20with%20compatibility%20for%20game%20engines%20and%0Arobotic%20simulation%20platforms.%20We%20demonstrate%20the%20potential%20of%20DRAWER%20by%20using%0Ait%20to%20automatically%20create%20an%20interactive%20game%20in%20Unreal%20Engine%20and%20to%20enable%0Areal-to-sim-to-real%20transfer%20for%20robotics%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15278v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRAWER%253A%2520Digital%2520Reconstruction%2520and%2520Articulation%2520With%2520Environment%2520Realism%26entry.906535625%3DHongchi%2520Xia%2520and%2520Entong%2520Su%2520and%2520Marius%2520Memmel%2520and%2520Arhan%2520Jain%2520and%2520Raymond%2520Yu%2520and%2520Numfor%2520Mbiziwo-Tiapo%2520and%2520Ali%2520Farhadi%2520and%2520Abhishek%2520Gupta%2520and%2520Shenlong%2520Wang%2520and%2520Wei-Chiu%2520Ma%26entry.1292438233%3D%2520%2520Creating%2520virtual%2520digital%2520replicas%2520from%2520real-world%2520data%2520unlocks%2520significant%250Apotential%2520across%2520domains%2520like%2520gaming%2520and%2520robotics.%2520In%2520this%2520paper%252C%2520we%2520present%250ADRAWER%252C%2520a%2520novel%2520framework%2520that%2520converts%2520a%2520video%2520of%2520a%2520static%2520indoor%2520scene%2520into%2520a%250Aphotorealistic%2520and%2520interactive%2520digital%2520environment.%2520Our%2520approach%2520centers%2520on%2520two%250Amain%2520contributions%253A%2520%2528i%2529%2520a%2520reconstruction%2520module%2520based%2520on%2520a%2520dual%2520scene%250Arepresentation%2520that%2520reconstructs%2520the%2520scene%2520with%2520fine-grained%2520geometric%2520details%252C%250Aand%2520%2528ii%2529%2520an%2520articulation%2520module%2520that%2520identifies%2520articulation%2520types%2520and%2520hinge%250Apositions%252C%2520reconstructs%2520simulatable%2520shapes%2520and%2520appearances%2520and%2520integrates%2520them%250Ainto%2520the%2520scene.%2520The%2520resulting%2520virtual%2520environment%2520is%2520photorealistic%252C%250Ainteractive%252C%2520and%2520runs%2520in%2520real%2520time%252C%2520with%2520compatibility%2520for%2520game%2520engines%2520and%250Arobotic%2520simulation%2520platforms.%2520We%2520demonstrate%2520the%2520potential%2520of%2520DRAWER%2520by%2520using%250Ait%2520to%2520automatically%2520create%2520an%2520interactive%2520game%2520in%2520Unreal%2520Engine%2520and%2520to%2520enable%250Areal-to-sim-to-real%2520transfer%2520for%2520robotics%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15278v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRAWER%3A%20Digital%20Reconstruction%20and%20Articulation%20With%20Environment%20Realism&entry.906535625=Hongchi%20Xia%20and%20Entong%20Su%20and%20Marius%20Memmel%20and%20Arhan%20Jain%20and%20Raymond%20Yu%20and%20Numfor%20Mbiziwo-Tiapo%20and%20Ali%20Farhadi%20and%20Abhishek%20Gupta%20and%20Shenlong%20Wang%20and%20Wei-Chiu%20Ma&entry.1292438233=%20%20Creating%20virtual%20digital%20replicas%20from%20real-world%20data%20unlocks%20significant%0Apotential%20across%20domains%20like%20gaming%20and%20robotics.%20In%20this%20paper%2C%20we%20present%0ADRAWER%2C%20a%20novel%20framework%20that%20converts%20a%20video%20of%20a%20static%20indoor%20scene%20into%20a%0Aphotorealistic%20and%20interactive%20digital%20environment.%20Our%20approach%20centers%20on%20two%0Amain%20contributions%3A%20%28i%29%20a%20reconstruction%20module%20based%20on%20a%20dual%20scene%0Arepresentation%20that%20reconstructs%20the%20scene%20with%20fine-grained%20geometric%20details%2C%0Aand%20%28ii%29%20an%20articulation%20module%20that%20identifies%20articulation%20types%20and%20hinge%0Apositions%2C%20reconstructs%20simulatable%20shapes%20and%20appearances%20and%20integrates%20them%0Ainto%20the%20scene.%20The%20resulting%20virtual%20environment%20is%20photorealistic%2C%0Ainteractive%2C%20and%20runs%20in%20real%20time%2C%20with%20compatibility%20for%20game%20engines%20and%0Arobotic%20simulation%20platforms.%20We%20demonstrate%20the%20potential%20of%20DRAWER%20by%20using%0Ait%20to%20automatically%20create%20an%20interactive%20game%20in%20Unreal%20Engine%20and%20to%20enable%0Areal-to-sim-to-real%20transfer%20for%20robotics%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15278v1&entry.124074799=Read"},
{"title": "Analysing the Robustness of Vision-Language-Models to Common Corruptions", "author": "Muhammad Usama and Syeda Aishah Asim and Syed Bilal Ali and Syed Talal Wasim and Umair Bin Mansoor", "abstract": "  Vision-language models (VLMs) have demonstrated impressive capabilities in\nunderstanding and reasoning about visual and textual content. However, their\nrobustness to common image corruptions remains under-explored. In this work, we\npresent the first comprehensive analysis of VLM robustness across 19 corruption\ntypes from the ImageNet-C benchmark, spanning four categories: noise, blur,\nweather, and digital distortions. We introduce two new benchmarks, TextVQA-C\nand GQA-C, to systematically evaluate how corruptions affect scene text\nunderstanding and object-based reasoning, respectively. Our analysis reveals\nthat transformer-based VLMs exhibit distinct vulnerability patterns across\ntasks: text recognition deteriorates most severely under blur and snow\ncorruptions, while object reasoning shows higher sensitivity to corruptions\nsuch as frost and impulse noise. We connect these observations to the\nfrequency-domain characteristics of different corruptions, revealing how\ntransformers' inherent bias toward low-frequency processing explains their\ndifferential robustness patterns. Our findings provide valuable insights for\ndeveloping more corruption-robust vision-language models for real-world\napplications.\n", "link": "http://arxiv.org/abs/2504.13690v2", "date": "2025-04-21", "relevancy": 2.3045, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5817}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysing%20the%20Robustness%20of%20Vision-Language-Models%20to%20Common%20Corruptions&body=Title%3A%20Analysing%20the%20Robustness%20of%20Vision-Language-Models%20to%20Common%20Corruptions%0AAuthor%3A%20Muhammad%20Usama%20and%20Syeda%20Aishah%20Asim%20and%20Syed%20Bilal%20Ali%20and%20Syed%20Talal%20Wasim%20and%20Umair%20Bin%20Mansoor%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Aunderstanding%20and%20reasoning%20about%20visual%20and%20textual%20content.%20However%2C%20their%0Arobustness%20to%20common%20image%20corruptions%20remains%20under-explored.%20In%20this%20work%2C%20we%0Apresent%20the%20first%20comprehensive%20analysis%20of%20VLM%20robustness%20across%2019%20corruption%0Atypes%20from%20the%20ImageNet-C%20benchmark%2C%20spanning%20four%20categories%3A%20noise%2C%20blur%2C%0Aweather%2C%20and%20digital%20distortions.%20We%20introduce%20two%20new%20benchmarks%2C%20TextVQA-C%0Aand%20GQA-C%2C%20to%20systematically%20evaluate%20how%20corruptions%20affect%20scene%20text%0Aunderstanding%20and%20object-based%20reasoning%2C%20respectively.%20Our%20analysis%20reveals%0Athat%20transformer-based%20VLMs%20exhibit%20distinct%20vulnerability%20patterns%20across%0Atasks%3A%20text%20recognition%20deteriorates%20most%20severely%20under%20blur%20and%20snow%0Acorruptions%2C%20while%20object%20reasoning%20shows%20higher%20sensitivity%20to%20corruptions%0Asuch%20as%20frost%20and%20impulse%20noise.%20We%20connect%20these%20observations%20to%20the%0Afrequency-domain%20characteristics%20of%20different%20corruptions%2C%20revealing%20how%0Atransformers%27%20inherent%20bias%20toward%20low-frequency%20processing%20explains%20their%0Adifferential%20robustness%20patterns.%20Our%20findings%20provide%20valuable%20insights%20for%0Adeveloping%20more%20corruption-robust%20vision-language%20models%20for%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysing%2520the%2520Robustness%2520of%2520Vision-Language-Models%2520to%2520Common%2520Corruptions%26entry.906535625%3DMuhammad%2520Usama%2520and%2520Syeda%2520Aishah%2520Asim%2520and%2520Syed%2520Bilal%2520Ali%2520and%2520Syed%2520Talal%2520Wasim%2520and%2520Umair%2520Bin%2520Mansoor%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520have%2520demonstrated%2520impressive%2520capabilities%2520in%250Aunderstanding%2520and%2520reasoning%2520about%2520visual%2520and%2520textual%2520content.%2520However%252C%2520their%250Arobustness%2520to%2520common%2520image%2520corruptions%2520remains%2520under-explored.%2520In%2520this%2520work%252C%2520we%250Apresent%2520the%2520first%2520comprehensive%2520analysis%2520of%2520VLM%2520robustness%2520across%252019%2520corruption%250Atypes%2520from%2520the%2520ImageNet-C%2520benchmark%252C%2520spanning%2520four%2520categories%253A%2520noise%252C%2520blur%252C%250Aweather%252C%2520and%2520digital%2520distortions.%2520We%2520introduce%2520two%2520new%2520benchmarks%252C%2520TextVQA-C%250Aand%2520GQA-C%252C%2520to%2520systematically%2520evaluate%2520how%2520corruptions%2520affect%2520scene%2520text%250Aunderstanding%2520and%2520object-based%2520reasoning%252C%2520respectively.%2520Our%2520analysis%2520reveals%250Athat%2520transformer-based%2520VLMs%2520exhibit%2520distinct%2520vulnerability%2520patterns%2520across%250Atasks%253A%2520text%2520recognition%2520deteriorates%2520most%2520severely%2520under%2520blur%2520and%2520snow%250Acorruptions%252C%2520while%2520object%2520reasoning%2520shows%2520higher%2520sensitivity%2520to%2520corruptions%250Asuch%2520as%2520frost%2520and%2520impulse%2520noise.%2520We%2520connect%2520these%2520observations%2520to%2520the%250Afrequency-domain%2520characteristics%2520of%2520different%2520corruptions%252C%2520revealing%2520how%250Atransformers%2527%2520inherent%2520bias%2520toward%2520low-frequency%2520processing%2520explains%2520their%250Adifferential%2520robustness%2520patterns.%2520Our%2520findings%2520provide%2520valuable%2520insights%2520for%250Adeveloping%2520more%2520corruption-robust%2520vision-language%2520models%2520for%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysing%20the%20Robustness%20of%20Vision-Language-Models%20to%20Common%20Corruptions&entry.906535625=Muhammad%20Usama%20and%20Syeda%20Aishah%20Asim%20and%20Syed%20Bilal%20Ali%20and%20Syed%20Talal%20Wasim%20and%20Umair%20Bin%20Mansoor&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20have%20demonstrated%20impressive%20capabilities%20in%0Aunderstanding%20and%20reasoning%20about%20visual%20and%20textual%20content.%20However%2C%20their%0Arobustness%20to%20common%20image%20corruptions%20remains%20under-explored.%20In%20this%20work%2C%20we%0Apresent%20the%20first%20comprehensive%20analysis%20of%20VLM%20robustness%20across%2019%20corruption%0Atypes%20from%20the%20ImageNet-C%20benchmark%2C%20spanning%20four%20categories%3A%20noise%2C%20blur%2C%0Aweather%2C%20and%20digital%20distortions.%20We%20introduce%20two%20new%20benchmarks%2C%20TextVQA-C%0Aand%20GQA-C%2C%20to%20systematically%20evaluate%20how%20corruptions%20affect%20scene%20text%0Aunderstanding%20and%20object-based%20reasoning%2C%20respectively.%20Our%20analysis%20reveals%0Athat%20transformer-based%20VLMs%20exhibit%20distinct%20vulnerability%20patterns%20across%0Atasks%3A%20text%20recognition%20deteriorates%20most%20severely%20under%20blur%20and%20snow%0Acorruptions%2C%20while%20object%20reasoning%20shows%20higher%20sensitivity%20to%20corruptions%0Asuch%20as%20frost%20and%20impulse%20noise.%20We%20connect%20these%20observations%20to%20the%0Afrequency-domain%20characteristics%20of%20different%20corruptions%2C%20revealing%20how%0Atransformers%27%20inherent%20bias%20toward%20low-frequency%20processing%20explains%20their%0Adifferential%20robustness%20patterns.%20Our%20findings%20provide%20valuable%20insights%20for%0Adeveloping%20more%20corruption-robust%20vision-language%20models%20for%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13690v2&entry.124074799=Read"},
{"title": "Continuous Locomotive Crowd Behavior Generation", "author": "Inhwan Bae and Junoh Lee and Hae-Gon Jeon", "abstract": "  Modeling and reproducing crowd behaviors are important in various domains\nincluding psychology, robotics, transport engineering and virtual environments.\nConventional methods have focused on synthesizing momentary scenes, which have\ndifficulty in replicating the continuous nature of real-world crowds. In this\npaper, we introduce a novel method for automatically generating continuous,\nrealistic crowd trajectories with heterogeneous behaviors and interactions\namong individuals. We first design a crowd emitter model. To do this, we obtain\nspatial layouts from single input images, including a segmentation map,\nappearance map, population density map and population probability, prior to\ncrowd generation. The emitter then continually places individuals on the\ntimeline by assigning independent behavior characteristics such as agents'\ntype, pace, and start/end positions using diffusion models. Next, our crowd\nsimulator produces their long-term locomotions. To simulate diverse actions, it\ncan augment their behaviors based on a Markov chain. As a result, our overall\nframework populates the scenes with heterogeneous crowd behaviors by\nalternating between the proposed emitter and simulator. Note that all the\ncomponents in the proposed framework are user-controllable. Lastly, we propose\na benchmark protocol to evaluate the realism and quality of the generated\ncrowds in terms of the scene-level population dynamics and the individual-level\ntrajectory accuracy. We demonstrate that our approach effectively models\ndiverse crowd behavior patterns and generalizes well across different\ngeographical environments. Code is publicly available at\nhttps://github.com/InhwanBae/CrowdES .\n", "link": "http://arxiv.org/abs/2504.04756v2", "date": "2025-04-21", "relevancy": 2.2877, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5908}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.56}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20Locomotive%20Crowd%20Behavior%20Generation&body=Title%3A%20Continuous%20Locomotive%20Crowd%20Behavior%20Generation%0AAuthor%3A%20Inhwan%20Bae%20and%20Junoh%20Lee%20and%20Hae-Gon%20Jeon%0AAbstract%3A%20%20%20Modeling%20and%20reproducing%20crowd%20behaviors%20are%20important%20in%20various%20domains%0Aincluding%20psychology%2C%20robotics%2C%20transport%20engineering%20and%20virtual%20environments.%0AConventional%20methods%20have%20focused%20on%20synthesizing%20momentary%20scenes%2C%20which%20have%0Adifficulty%20in%20replicating%20the%20continuous%20nature%20of%20real-world%20crowds.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20method%20for%20automatically%20generating%20continuous%2C%0Arealistic%20crowd%20trajectories%20with%20heterogeneous%20behaviors%20and%20interactions%0Aamong%20individuals.%20We%20first%20design%20a%20crowd%20emitter%20model.%20To%20do%20this%2C%20we%20obtain%0Aspatial%20layouts%20from%20single%20input%20images%2C%20including%20a%20segmentation%20map%2C%0Aappearance%20map%2C%20population%20density%20map%20and%20population%20probability%2C%20prior%20to%0Acrowd%20generation.%20The%20emitter%20then%20continually%20places%20individuals%20on%20the%0Atimeline%20by%20assigning%20independent%20behavior%20characteristics%20such%20as%20agents%27%0Atype%2C%20pace%2C%20and%20start/end%20positions%20using%20diffusion%20models.%20Next%2C%20our%20crowd%0Asimulator%20produces%20their%20long-term%20locomotions.%20To%20simulate%20diverse%20actions%2C%20it%0Acan%20augment%20their%20behaviors%20based%20on%20a%20Markov%20chain.%20As%20a%20result%2C%20our%20overall%0Aframework%20populates%20the%20scenes%20with%20heterogeneous%20crowd%20behaviors%20by%0Aalternating%20between%20the%20proposed%20emitter%20and%20simulator.%20Note%20that%20all%20the%0Acomponents%20in%20the%20proposed%20framework%20are%20user-controllable.%20Lastly%2C%20we%20propose%0Aa%20benchmark%20protocol%20to%20evaluate%20the%20realism%20and%20quality%20of%20the%20generated%0Acrowds%20in%20terms%20of%20the%20scene-level%20population%20dynamics%20and%20the%20individual-level%0Atrajectory%20accuracy.%20We%20demonstrate%20that%20our%20approach%20effectively%20models%0Adiverse%20crowd%20behavior%20patterns%20and%20generalizes%20well%20across%20different%0Ageographical%20environments.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/InhwanBae/CrowdES%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04756v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520Locomotive%2520Crowd%2520Behavior%2520Generation%26entry.906535625%3DInhwan%2520Bae%2520and%2520Junoh%2520Lee%2520and%2520Hae-Gon%2520Jeon%26entry.1292438233%3D%2520%2520Modeling%2520and%2520reproducing%2520crowd%2520behaviors%2520are%2520important%2520in%2520various%2520domains%250Aincluding%2520psychology%252C%2520robotics%252C%2520transport%2520engineering%2520and%2520virtual%2520environments.%250AConventional%2520methods%2520have%2520focused%2520on%2520synthesizing%2520momentary%2520scenes%252C%2520which%2520have%250Adifficulty%2520in%2520replicating%2520the%2520continuous%2520nature%2520of%2520real-world%2520crowds.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520method%2520for%2520automatically%2520generating%2520continuous%252C%250Arealistic%2520crowd%2520trajectories%2520with%2520heterogeneous%2520behaviors%2520and%2520interactions%250Aamong%2520individuals.%2520We%2520first%2520design%2520a%2520crowd%2520emitter%2520model.%2520To%2520do%2520this%252C%2520we%2520obtain%250Aspatial%2520layouts%2520from%2520single%2520input%2520images%252C%2520including%2520a%2520segmentation%2520map%252C%250Aappearance%2520map%252C%2520population%2520density%2520map%2520and%2520population%2520probability%252C%2520prior%2520to%250Acrowd%2520generation.%2520The%2520emitter%2520then%2520continually%2520places%2520individuals%2520on%2520the%250Atimeline%2520by%2520assigning%2520independent%2520behavior%2520characteristics%2520such%2520as%2520agents%2527%250Atype%252C%2520pace%252C%2520and%2520start/end%2520positions%2520using%2520diffusion%2520models.%2520Next%252C%2520our%2520crowd%250Asimulator%2520produces%2520their%2520long-term%2520locomotions.%2520To%2520simulate%2520diverse%2520actions%252C%2520it%250Acan%2520augment%2520their%2520behaviors%2520based%2520on%2520a%2520Markov%2520chain.%2520As%2520a%2520result%252C%2520our%2520overall%250Aframework%2520populates%2520the%2520scenes%2520with%2520heterogeneous%2520crowd%2520behaviors%2520by%250Aalternating%2520between%2520the%2520proposed%2520emitter%2520and%2520simulator.%2520Note%2520that%2520all%2520the%250Acomponents%2520in%2520the%2520proposed%2520framework%2520are%2520user-controllable.%2520Lastly%252C%2520we%2520propose%250Aa%2520benchmark%2520protocol%2520to%2520evaluate%2520the%2520realism%2520and%2520quality%2520of%2520the%2520generated%250Acrowds%2520in%2520terms%2520of%2520the%2520scene-level%2520population%2520dynamics%2520and%2520the%2520individual-level%250Atrajectory%2520accuracy.%2520We%2520demonstrate%2520that%2520our%2520approach%2520effectively%2520models%250Adiverse%2520crowd%2520behavior%2520patterns%2520and%2520generalizes%2520well%2520across%2520different%250Ageographical%2520environments.%2520Code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/InhwanBae/CrowdES%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04756v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20Locomotive%20Crowd%20Behavior%20Generation&entry.906535625=Inhwan%20Bae%20and%20Junoh%20Lee%20and%20Hae-Gon%20Jeon&entry.1292438233=%20%20Modeling%20and%20reproducing%20crowd%20behaviors%20are%20important%20in%20various%20domains%0Aincluding%20psychology%2C%20robotics%2C%20transport%20engineering%20and%20virtual%20environments.%0AConventional%20methods%20have%20focused%20on%20synthesizing%20momentary%20scenes%2C%20which%20have%0Adifficulty%20in%20replicating%20the%20continuous%20nature%20of%20real-world%20crowds.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20method%20for%20automatically%20generating%20continuous%2C%0Arealistic%20crowd%20trajectories%20with%20heterogeneous%20behaviors%20and%20interactions%0Aamong%20individuals.%20We%20first%20design%20a%20crowd%20emitter%20model.%20To%20do%20this%2C%20we%20obtain%0Aspatial%20layouts%20from%20single%20input%20images%2C%20including%20a%20segmentation%20map%2C%0Aappearance%20map%2C%20population%20density%20map%20and%20population%20probability%2C%20prior%20to%0Acrowd%20generation.%20The%20emitter%20then%20continually%20places%20individuals%20on%20the%0Atimeline%20by%20assigning%20independent%20behavior%20characteristics%20such%20as%20agents%27%0Atype%2C%20pace%2C%20and%20start/end%20positions%20using%20diffusion%20models.%20Next%2C%20our%20crowd%0Asimulator%20produces%20their%20long-term%20locomotions.%20To%20simulate%20diverse%20actions%2C%20it%0Acan%20augment%20their%20behaviors%20based%20on%20a%20Markov%20chain.%20As%20a%20result%2C%20our%20overall%0Aframework%20populates%20the%20scenes%20with%20heterogeneous%20crowd%20behaviors%20by%0Aalternating%20between%20the%20proposed%20emitter%20and%20simulator.%20Note%20that%20all%20the%0Acomponents%20in%20the%20proposed%20framework%20are%20user-controllable.%20Lastly%2C%20we%20propose%0Aa%20benchmark%20protocol%20to%20evaluate%20the%20realism%20and%20quality%20of%20the%20generated%0Acrowds%20in%20terms%20of%20the%20scene-level%20population%20dynamics%20and%20the%20individual-level%0Atrajectory%20accuracy.%20We%20demonstrate%20that%20our%20approach%20effectively%20models%0Adiverse%20crowd%20behavior%20patterns%20and%20generalizes%20well%20across%20different%0Ageographical%20environments.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/InhwanBae/CrowdES%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04756v2&entry.124074799=Read"},
{"title": "Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs", "author": "Marina Sakharova and Abhinav Anand and Mira Mezini", "abstract": "  Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark.\n", "link": "http://arxiv.org/abs/2504.15210v1", "date": "2025-04-21", "relevancy": 2.2849, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4554}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Symbolic%20Execution%20into%20the%20Fine-Tuning%20of%20Code-Generating%0A%20%20LLMs&body=Title%3A%20Integrating%20Symbolic%20Execution%20into%20the%20Fine-Tuning%20of%20Code-Generating%0A%20%20LLMs%0AAuthor%3A%20Marina%20Sakharova%20and%20Abhinav%20Anand%20and%20Mira%20Mezini%0AAbstract%3A%20%20%20Code-generating%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20essential%20tools%20in%0Amodern%20software%20development%2C%20enhancing%20productivity%20and%20accelerating%0Adevelopment.%20This%20paper%20aims%20to%20investigate%20the%20fine-tuning%20of%20code-generating%0ALLMs%20using%20Reinforcement%20Learning%20and%20Direct%20Preference%20Optimization%2C%20further%0Aimproving%20their%20performance.%20To%20achieve%20this%2C%20we%20enhance%20the%20training%20data%20for%0Athe%20reward%20model%20with%20the%20help%20of%20symbolic%20execution%20techniques%2C%20ensuring%20more%0Acomprehensive%20and%20objective%20data.%20With%20symbolic%20execution%2C%20we%20create%20a%20custom%0Adataset%20that%20better%20captures%20the%20nuances%20in%20code%20evaluation.%20Our%20reward%20models%2C%0Afine-tuned%20on%20this%20dataset%2C%20demonstrate%20significant%20improvements%20over%20the%0Abaseline%2C%20CodeRL%2C%20in%20estimating%20the%20quality%20of%20generated%20code.%20Our%0Acode-generating%20LLMs%2C%20trained%20with%20the%20help%20of%20reward%20model%20feedback%2C%20achieve%0Asimilar%20results%20compared%20to%20the%20CodeRL%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Symbolic%2520Execution%2520into%2520the%2520Fine-Tuning%2520of%2520Code-Generating%250A%2520%2520LLMs%26entry.906535625%3DMarina%2520Sakharova%2520and%2520Abhinav%2520Anand%2520and%2520Mira%2520Mezini%26entry.1292438233%3D%2520%2520Code-generating%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520essential%2520tools%2520in%250Amodern%2520software%2520development%252C%2520enhancing%2520productivity%2520and%2520accelerating%250Adevelopment.%2520This%2520paper%2520aims%2520to%2520investigate%2520the%2520fine-tuning%2520of%2520code-generating%250ALLMs%2520using%2520Reinforcement%2520Learning%2520and%2520Direct%2520Preference%2520Optimization%252C%2520further%250Aimproving%2520their%2520performance.%2520To%2520achieve%2520this%252C%2520we%2520enhance%2520the%2520training%2520data%2520for%250Athe%2520reward%2520model%2520with%2520the%2520help%2520of%2520symbolic%2520execution%2520techniques%252C%2520ensuring%2520more%250Acomprehensive%2520and%2520objective%2520data.%2520With%2520symbolic%2520execution%252C%2520we%2520create%2520a%2520custom%250Adataset%2520that%2520better%2520captures%2520the%2520nuances%2520in%2520code%2520evaluation.%2520Our%2520reward%2520models%252C%250Afine-tuned%2520on%2520this%2520dataset%252C%2520demonstrate%2520significant%2520improvements%2520over%2520the%250Abaseline%252C%2520CodeRL%252C%2520in%2520estimating%2520the%2520quality%2520of%2520generated%2520code.%2520Our%250Acode-generating%2520LLMs%252C%2520trained%2520with%2520the%2520help%2520of%2520reward%2520model%2520feedback%252C%2520achieve%250Asimilar%2520results%2520compared%2520to%2520the%2520CodeRL%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Symbolic%20Execution%20into%20the%20Fine-Tuning%20of%20Code-Generating%0A%20%20LLMs&entry.906535625=Marina%20Sakharova%20and%20Abhinav%20Anand%20and%20Mira%20Mezini&entry.1292438233=%20%20Code-generating%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20essential%20tools%20in%0Amodern%20software%20development%2C%20enhancing%20productivity%20and%20accelerating%0Adevelopment.%20This%20paper%20aims%20to%20investigate%20the%20fine-tuning%20of%20code-generating%0ALLMs%20using%20Reinforcement%20Learning%20and%20Direct%20Preference%20Optimization%2C%20further%0Aimproving%20their%20performance.%20To%20achieve%20this%2C%20we%20enhance%20the%20training%20data%20for%0Athe%20reward%20model%20with%20the%20help%20of%20symbolic%20execution%20techniques%2C%20ensuring%20more%0Acomprehensive%20and%20objective%20data.%20With%20symbolic%20execution%2C%20we%20create%20a%20custom%0Adataset%20that%20better%20captures%20the%20nuances%20in%20code%20evaluation.%20Our%20reward%20models%2C%0Afine-tuned%20on%20this%20dataset%2C%20demonstrate%20significant%20improvements%20over%20the%0Abaseline%2C%20CodeRL%2C%20in%20estimating%20the%20quality%20of%20generated%20code.%20Our%0Acode-generating%20LLMs%2C%20trained%20with%20the%20help%20of%20reward%20model%20feedback%2C%20achieve%0Asimilar%20results%20compared%20to%20the%20CodeRL%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15210v1&entry.124074799=Read"},
{"title": "An LMM for Efficient Video Understanding via Reinforced Compression of\n  Video Cubes", "author": "Ji Qi and Yuan Yao and Yushi Bai and Bin Xu and Juanzi Li and Zhiyuan Liu and Tat-Seng Chua", "abstract": "  Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present \\textbf{Quicksviewer}, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.\n", "link": "http://arxiv.org/abs/2504.15270v1", "date": "2025-04-21", "relevancy": 2.2769, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5729}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20LMM%20for%20Efficient%20Video%20Understanding%20via%20Reinforced%20Compression%20of%0A%20%20Video%20Cubes&body=Title%3A%20An%20LMM%20for%20Efficient%20Video%20Understanding%20via%20Reinforced%20Compression%20of%0A%20%20Video%20Cubes%0AAuthor%3A%20Ji%20Qi%20and%20Yuan%20Yao%20and%20Yushi%20Bai%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Zhiyuan%20Liu%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28LMMs%29%20uniformly%20perceive%20video%20frames%2C%20creating%0Acomputational%20inefficiency%20for%20videos%20with%20inherently%20varying%20temporal%0Ainformation%20density.%20This%20paper%20present%20%5Ctextbf%7BQuicksviewer%7D%2C%20an%20LMM%20with%20new%0Aperceiving%20paradigm%20that%20partitions%20a%20video%20of%20nonuniform%20density%20into%20varying%0Acubes%20using%20Gumbel%20Softmax%2C%20followed%20by%20a%20unified%20resampling%20for%20each%20cube%20to%0Aachieve%20efficient%20video%20understanding.%20This%20simple%20and%20intuitive%20approach%0Adynamically%20compress%20video%20online%20based%20on%20its%20temporal%20density%2C%20significantly%0Areducing%20spatiotemporal%20redundancy%20%28overall%2045%24%5Ctimes%24%20compression%20rate%29%2C%20while%0Aenabling%20efficient%20training%20with%20large%20receptive%20field.%20We%20train%20the%20model%20from%0Aa%20language%20backbone%20through%20three%20progressive%20stages%2C%20each%20incorporating%0Alengthy%20videos%20on%20average%20of%20420s/1fps%20thanks%20to%20the%20perceiving%20efficiency.%0AWith%20only%200.8M%20total%20video-text%20samples%20for%20training%2C%20our%20model%20outperforms%20the%0Adirect%20baseline%20employing%20a%20fixed%20partitioning%20strategy%20by%20a%20maximum%20of%208.72%20in%0Aaccuracy%2C%20demonstrating%20the%20effectiveness%20in%20performance.%20On%20Video-MME%2C%0AQuicksviewer%20achieves%20SOTA%20under%20modest%20sequence%20lengths%20using%20just%20up%20to%205%5C%25%0Aof%20tokens%20per%20frame%20required%20by%20baselines.%20With%20this%20paradigm%2C%20scaling%20up%20the%0Anumber%20of%20input%20frames%20reveals%20a%20clear%20power%20law%20of%20the%20model%20capabilities.%20It%0Ais%20also%20empirically%20verified%20that%20the%20segments%20generated%20by%20the%20cubing%20network%0Acan%20help%20for%20analyzing%20continuous%20events%20in%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520LMM%2520for%2520Efficient%2520Video%2520Understanding%2520via%2520Reinforced%2520Compression%2520of%250A%2520%2520Video%2520Cubes%26entry.906535625%3DJi%2520Qi%2520and%2520Yuan%2520Yao%2520and%2520Yushi%2520Bai%2520and%2520Bin%2520Xu%2520and%2520Juanzi%2520Li%2520and%2520Zhiyuan%2520Liu%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520uniformly%2520perceive%2520video%2520frames%252C%2520creating%250Acomputational%2520inefficiency%2520for%2520videos%2520with%2520inherently%2520varying%2520temporal%250Ainformation%2520density.%2520This%2520paper%2520present%2520%255Ctextbf%257BQuicksviewer%257D%252C%2520an%2520LMM%2520with%2520new%250Aperceiving%2520paradigm%2520that%2520partitions%2520a%2520video%2520of%2520nonuniform%2520density%2520into%2520varying%250Acubes%2520using%2520Gumbel%2520Softmax%252C%2520followed%2520by%2520a%2520unified%2520resampling%2520for%2520each%2520cube%2520to%250Aachieve%2520efficient%2520video%2520understanding.%2520This%2520simple%2520and%2520intuitive%2520approach%250Adynamically%2520compress%2520video%2520online%2520based%2520on%2520its%2520temporal%2520density%252C%2520significantly%250Areducing%2520spatiotemporal%2520redundancy%2520%2528overall%252045%2524%255Ctimes%2524%2520compression%2520rate%2529%252C%2520while%250Aenabling%2520efficient%2520training%2520with%2520large%2520receptive%2520field.%2520We%2520train%2520the%2520model%2520from%250Aa%2520language%2520backbone%2520through%2520three%2520progressive%2520stages%252C%2520each%2520incorporating%250Alengthy%2520videos%2520on%2520average%2520of%2520420s/1fps%2520thanks%2520to%2520the%2520perceiving%2520efficiency.%250AWith%2520only%25200.8M%2520total%2520video-text%2520samples%2520for%2520training%252C%2520our%2520model%2520outperforms%2520the%250Adirect%2520baseline%2520employing%2520a%2520fixed%2520partitioning%2520strategy%2520by%2520a%2520maximum%2520of%25208.72%2520in%250Aaccuracy%252C%2520demonstrating%2520the%2520effectiveness%2520in%2520performance.%2520On%2520Video-MME%252C%250AQuicksviewer%2520achieves%2520SOTA%2520under%2520modest%2520sequence%2520lengths%2520using%2520just%2520up%2520to%25205%255C%2525%250Aof%2520tokens%2520per%2520frame%2520required%2520by%2520baselines.%2520With%2520this%2520paradigm%252C%2520scaling%2520up%2520the%250Anumber%2520of%2520input%2520frames%2520reveals%2520a%2520clear%2520power%2520law%2520of%2520the%2520model%2520capabilities.%2520It%250Ais%2520also%2520empirically%2520verified%2520that%2520the%2520segments%2520generated%2520by%2520the%2520cubing%2520network%250Acan%2520help%2520for%2520analyzing%2520continuous%2520events%2520in%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20LMM%20for%20Efficient%20Video%20Understanding%20via%20Reinforced%20Compression%20of%0A%20%20Video%20Cubes&entry.906535625=Ji%20Qi%20and%20Yuan%20Yao%20and%20Yushi%20Bai%20and%20Bin%20Xu%20and%20Juanzi%20Li%20and%20Zhiyuan%20Liu%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28LMMs%29%20uniformly%20perceive%20video%20frames%2C%20creating%0Acomputational%20inefficiency%20for%20videos%20with%20inherently%20varying%20temporal%0Ainformation%20density.%20This%20paper%20present%20%5Ctextbf%7BQuicksviewer%7D%2C%20an%20LMM%20with%20new%0Aperceiving%20paradigm%20that%20partitions%20a%20video%20of%20nonuniform%20density%20into%20varying%0Acubes%20using%20Gumbel%20Softmax%2C%20followed%20by%20a%20unified%20resampling%20for%20each%20cube%20to%0Aachieve%20efficient%20video%20understanding.%20This%20simple%20and%20intuitive%20approach%0Adynamically%20compress%20video%20online%20based%20on%20its%20temporal%20density%2C%20significantly%0Areducing%20spatiotemporal%20redundancy%20%28overall%2045%24%5Ctimes%24%20compression%20rate%29%2C%20while%0Aenabling%20efficient%20training%20with%20large%20receptive%20field.%20We%20train%20the%20model%20from%0Aa%20language%20backbone%20through%20three%20progressive%20stages%2C%20each%20incorporating%0Alengthy%20videos%20on%20average%20of%20420s/1fps%20thanks%20to%20the%20perceiving%20efficiency.%0AWith%20only%200.8M%20total%20video-text%20samples%20for%20training%2C%20our%20model%20outperforms%20the%0Adirect%20baseline%20employing%20a%20fixed%20partitioning%20strategy%20by%20a%20maximum%20of%208.72%20in%0Aaccuracy%2C%20demonstrating%20the%20effectiveness%20in%20performance.%20On%20Video-MME%2C%0AQuicksviewer%20achieves%20SOTA%20under%20modest%20sequence%20lengths%20using%20just%20up%20to%205%5C%25%0Aof%20tokens%20per%20frame%20required%20by%20baselines.%20With%20this%20paradigm%2C%20scaling%20up%20the%0Anumber%20of%20input%20frames%20reveals%20a%20clear%20power%20law%20of%20the%20model%20capabilities.%20It%0Ais%20also%20empirically%20verified%20that%20the%20segments%20generated%20by%20the%20cubing%20network%0Acan%20help%20for%20analyzing%20continuous%20events%20in%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15270v1&entry.124074799=Read"},
{"title": "STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World\n  Understanding?", "author": "Yun Li and Yiming Zhang and Tao Lin and XiangRui Liu and Wenxiao Cai and Zheng Liu and Bo Zhao", "abstract": "  The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution\nfor Embodied AI and Autonomous Driving has become a prevailing trend. While\nMLLMs have been extensively studied for visual semantic understanding tasks,\ntheir ability to perform precise and quantitative spatial-temporal\nunderstanding in real-world applications remains largely unexamined, leading to\nuncertain prospects. To evaluate models' Spatial-Temporal Intelligence, we\nintroduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporal\nunderstanding through challenging tasks such as estimating and predicting the\nappearance, pose, displacement, and motion of objects. Our benchmark\nencompasses a wide range of robot and vehicle operations across desktop,\nindoor, and outdoor scenarios. The extensive experiments reveals that the\nstate-of-the-art MLLMs still struggle in real-world spatial-temporal\nunderstanding, especially in tasks requiring precise distance estimation and\nmotion analysis.\n", "link": "http://arxiv.org/abs/2503.23765v3", "date": "2025-04-21", "relevancy": 2.2711, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5853}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STI-Bench%3A%20Are%20MLLMs%20Ready%20for%20Precise%20Spatial-Temporal%20World%0A%20%20Understanding%3F&body=Title%3A%20STI-Bench%3A%20Are%20MLLMs%20Ready%20for%20Precise%20Spatial-Temporal%20World%0A%20%20Understanding%3F%0AAuthor%3A%20Yun%20Li%20and%20Yiming%20Zhang%20and%20Tao%20Lin%20and%20XiangRui%20Liu%20and%20Wenxiao%20Cai%20and%20Zheng%20Liu%20and%20Bo%20Zhao%0AAbstract%3A%20%20%20The%20use%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20as%20an%20end-to-end%20solution%0Afor%20Embodied%20AI%20and%20Autonomous%20Driving%20has%20become%20a%20prevailing%20trend.%20While%0AMLLMs%20have%20been%20extensively%20studied%20for%20visual%20semantic%20understanding%20tasks%2C%0Atheir%20ability%20to%20perform%20precise%20and%20quantitative%20spatial-temporal%0Aunderstanding%20in%20real-world%20applications%20remains%20largely%20unexamined%2C%20leading%20to%0Auncertain%20prospects.%20To%20evaluate%20models%27%20Spatial-Temporal%20Intelligence%2C%20we%0Aintroduce%20STI-Bench%2C%20a%20benchmark%20designed%20to%20evaluate%20MLLMs%27%20spatial-temporal%0Aunderstanding%20through%20challenging%20tasks%20such%20as%20estimating%20and%20predicting%20the%0Aappearance%2C%20pose%2C%20displacement%2C%20and%20motion%20of%20objects.%20Our%20benchmark%0Aencompasses%20a%20wide%20range%20of%20robot%20and%20vehicle%20operations%20across%20desktop%2C%0Aindoor%2C%20and%20outdoor%20scenarios.%20The%20extensive%20experiments%20reveals%20that%20the%0Astate-of-the-art%20MLLMs%20still%20struggle%20in%20real-world%20spatial-temporal%0Aunderstanding%2C%20especially%20in%20tasks%20requiring%20precise%20distance%20estimation%20and%0Amotion%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23765v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTI-Bench%253A%2520Are%2520MLLMs%2520Ready%2520for%2520Precise%2520Spatial-Temporal%2520World%250A%2520%2520Understanding%253F%26entry.906535625%3DYun%2520Li%2520and%2520Yiming%2520Zhang%2520and%2520Tao%2520Lin%2520and%2520XiangRui%2520Liu%2520and%2520Wenxiao%2520Cai%2520and%2520Zheng%2520Liu%2520and%2520Bo%2520Zhao%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520as%2520an%2520end-to-end%2520solution%250Afor%2520Embodied%2520AI%2520and%2520Autonomous%2520Driving%2520has%2520become%2520a%2520prevailing%2520trend.%2520While%250AMLLMs%2520have%2520been%2520extensively%2520studied%2520for%2520visual%2520semantic%2520understanding%2520tasks%252C%250Atheir%2520ability%2520to%2520perform%2520precise%2520and%2520quantitative%2520spatial-temporal%250Aunderstanding%2520in%2520real-world%2520applications%2520remains%2520largely%2520unexamined%252C%2520leading%2520to%250Auncertain%2520prospects.%2520To%2520evaluate%2520models%2527%2520Spatial-Temporal%2520Intelligence%252C%2520we%250Aintroduce%2520STI-Bench%252C%2520a%2520benchmark%2520designed%2520to%2520evaluate%2520MLLMs%2527%2520spatial-temporal%250Aunderstanding%2520through%2520challenging%2520tasks%2520such%2520as%2520estimating%2520and%2520predicting%2520the%250Aappearance%252C%2520pose%252C%2520displacement%252C%2520and%2520motion%2520of%2520objects.%2520Our%2520benchmark%250Aencompasses%2520a%2520wide%2520range%2520of%2520robot%2520and%2520vehicle%2520operations%2520across%2520desktop%252C%250Aindoor%252C%2520and%2520outdoor%2520scenarios.%2520The%2520extensive%2520experiments%2520reveals%2520that%2520the%250Astate-of-the-art%2520MLLMs%2520still%2520struggle%2520in%2520real-world%2520spatial-temporal%250Aunderstanding%252C%2520especially%2520in%2520tasks%2520requiring%2520precise%2520distance%2520estimation%2520and%250Amotion%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23765v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STI-Bench%3A%20Are%20MLLMs%20Ready%20for%20Precise%20Spatial-Temporal%20World%0A%20%20Understanding%3F&entry.906535625=Yun%20Li%20and%20Yiming%20Zhang%20and%20Tao%20Lin%20and%20XiangRui%20Liu%20and%20Wenxiao%20Cai%20and%20Zheng%20Liu%20and%20Bo%20Zhao&entry.1292438233=%20%20The%20use%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20as%20an%20end-to-end%20solution%0Afor%20Embodied%20AI%20and%20Autonomous%20Driving%20has%20become%20a%20prevailing%20trend.%20While%0AMLLMs%20have%20been%20extensively%20studied%20for%20visual%20semantic%20understanding%20tasks%2C%0Atheir%20ability%20to%20perform%20precise%20and%20quantitative%20spatial-temporal%0Aunderstanding%20in%20real-world%20applications%20remains%20largely%20unexamined%2C%20leading%20to%0Auncertain%20prospects.%20To%20evaluate%20models%27%20Spatial-Temporal%20Intelligence%2C%20we%0Aintroduce%20STI-Bench%2C%20a%20benchmark%20designed%20to%20evaluate%20MLLMs%27%20spatial-temporal%0Aunderstanding%20through%20challenging%20tasks%20such%20as%20estimating%20and%20predicting%20the%0Aappearance%2C%20pose%2C%20displacement%2C%20and%20motion%20of%20objects.%20Our%20benchmark%0Aencompasses%20a%20wide%20range%20of%20robot%20and%20vehicle%20operations%20across%20desktop%2C%0Aindoor%2C%20and%20outdoor%20scenarios.%20The%20extensive%20experiments%20reveals%20that%20the%0Astate-of-the-art%20MLLMs%20still%20struggle%20in%20real-world%20spatial-temporal%0Aunderstanding%2C%20especially%20in%20tasks%20requiring%20precise%20distance%20estimation%20and%0Amotion%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23765v3&entry.124074799=Read"},
{"title": "Immersive Teleoperation Framework for Locomanipulation Tasks", "author": "Takuya Boehringer and Jonathan Embley-Riches and Karim Hammoud and Valerio Modugno and Dimitrios Kanoulas", "abstract": "  Recent advancements in robotic loco-manipulation have leveraged Virtual\nReality (VR) to enhance the precision and immersiveness of teleoperation\nsystems, significantly outperforming traditional methods reliant on 2D camera\nfeeds and joystick controls. Despite these advancements, challenges remain,\nparticularly concerning user experience across different setups. This paper\nintroduces a novel VR-based teleoperation framework designed for a robotic\nmanipulator integrated onto a mobile platform. Central to our approach is the\napplication of Gaussian splatting, a technique that abstracts the manipulable\nscene into a VR environment, thereby enabling more intuitive and immersive\ninteractions. Users can navigate and manipulate within the virtual scene as if\ninteracting with a real robot, enhancing both the engagement and efficacy of\nteleoperation tasks. An extensive user study validates our approach,\ndemonstrating significant usability and efficiency improvements. Two-thirds\n(66%) of participants completed tasks faster, achieving an average time\nreduction of 43%. Additionally, 93% preferred the Gaussian Splat interface\noverall, with unanimous (100%) recommendations for future use, highlighting\nimprovements in precision, responsiveness, and situational awareness. Finally,\nwe demonstrate the effectiveness of our framework through real-world\nexperiments in two distinct application scenarios, showcasing the practical\ncapabilities and versatility of the Splat-based VR interface.\n", "link": "http://arxiv.org/abs/2504.15229v1", "date": "2025-04-21", "relevancy": 2.2549, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6012}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5586}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Immersive%20Teleoperation%20Framework%20for%20Locomanipulation%20Tasks&body=Title%3A%20Immersive%20Teleoperation%20Framework%20for%20Locomanipulation%20Tasks%0AAuthor%3A%20Takuya%20Boehringer%20and%20Jonathan%20Embley-Riches%20and%20Karim%20Hammoud%20and%20Valerio%20Modugno%20and%20Dimitrios%20Kanoulas%0AAbstract%3A%20%20%20Recent%20advancements%20in%20robotic%20loco-manipulation%20have%20leveraged%20Virtual%0AReality%20%28VR%29%20to%20enhance%20the%20precision%20and%20immersiveness%20of%20teleoperation%0Asystems%2C%20significantly%20outperforming%20traditional%20methods%20reliant%20on%202D%20camera%0Afeeds%20and%20joystick%20controls.%20Despite%20these%20advancements%2C%20challenges%20remain%2C%0Aparticularly%20concerning%20user%20experience%20across%20different%20setups.%20This%20paper%0Aintroduces%20a%20novel%20VR-based%20teleoperation%20framework%20designed%20for%20a%20robotic%0Amanipulator%20integrated%20onto%20a%20mobile%20platform.%20Central%20to%20our%20approach%20is%20the%0Aapplication%20of%20Gaussian%20splatting%2C%20a%20technique%20that%20abstracts%20the%20manipulable%0Ascene%20into%20a%20VR%20environment%2C%20thereby%20enabling%20more%20intuitive%20and%20immersive%0Ainteractions.%20Users%20can%20navigate%20and%20manipulate%20within%20the%20virtual%20scene%20as%20if%0Ainteracting%20with%20a%20real%20robot%2C%20enhancing%20both%20the%20engagement%20and%20efficacy%20of%0Ateleoperation%20tasks.%20An%20extensive%20user%20study%20validates%20our%20approach%2C%0Ademonstrating%20significant%20usability%20and%20efficiency%20improvements.%20Two-thirds%0A%2866%25%29%20of%20participants%20completed%20tasks%20faster%2C%20achieving%20an%20average%20time%0Areduction%20of%2043%25.%20Additionally%2C%2093%25%20preferred%20the%20Gaussian%20Splat%20interface%0Aoverall%2C%20with%20unanimous%20%28100%25%29%20recommendations%20for%20future%20use%2C%20highlighting%0Aimprovements%20in%20precision%2C%20responsiveness%2C%20and%20situational%20awareness.%20Finally%2C%0Awe%20demonstrate%20the%20effectiveness%20of%20our%20framework%20through%20real-world%0Aexperiments%20in%20two%20distinct%20application%20scenarios%2C%20showcasing%20the%20practical%0Acapabilities%20and%20versatility%20of%20the%20Splat-based%20VR%20interface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImmersive%2520Teleoperation%2520Framework%2520for%2520Locomanipulation%2520Tasks%26entry.906535625%3DTakuya%2520Boehringer%2520and%2520Jonathan%2520Embley-Riches%2520and%2520Karim%2520Hammoud%2520and%2520Valerio%2520Modugno%2520and%2520Dimitrios%2520Kanoulas%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520robotic%2520loco-manipulation%2520have%2520leveraged%2520Virtual%250AReality%2520%2528VR%2529%2520to%2520enhance%2520the%2520precision%2520and%2520immersiveness%2520of%2520teleoperation%250Asystems%252C%2520significantly%2520outperforming%2520traditional%2520methods%2520reliant%2520on%25202D%2520camera%250Afeeds%2520and%2520joystick%2520controls.%2520Despite%2520these%2520advancements%252C%2520challenges%2520remain%252C%250Aparticularly%2520concerning%2520user%2520experience%2520across%2520different%2520setups.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520VR-based%2520teleoperation%2520framework%2520designed%2520for%2520a%2520robotic%250Amanipulator%2520integrated%2520onto%2520a%2520mobile%2520platform.%2520Central%2520to%2520our%2520approach%2520is%2520the%250Aapplication%2520of%2520Gaussian%2520splatting%252C%2520a%2520technique%2520that%2520abstracts%2520the%2520manipulable%250Ascene%2520into%2520a%2520VR%2520environment%252C%2520thereby%2520enabling%2520more%2520intuitive%2520and%2520immersive%250Ainteractions.%2520Users%2520can%2520navigate%2520and%2520manipulate%2520within%2520the%2520virtual%2520scene%2520as%2520if%250Ainteracting%2520with%2520a%2520real%2520robot%252C%2520enhancing%2520both%2520the%2520engagement%2520and%2520efficacy%2520of%250Ateleoperation%2520tasks.%2520An%2520extensive%2520user%2520study%2520validates%2520our%2520approach%252C%250Ademonstrating%2520significant%2520usability%2520and%2520efficiency%2520improvements.%2520Two-thirds%250A%252866%2525%2529%2520of%2520participants%2520completed%2520tasks%2520faster%252C%2520achieving%2520an%2520average%2520time%250Areduction%2520of%252043%2525.%2520Additionally%252C%252093%2525%2520preferred%2520the%2520Gaussian%2520Splat%2520interface%250Aoverall%252C%2520with%2520unanimous%2520%2528100%2525%2529%2520recommendations%2520for%2520future%2520use%252C%2520highlighting%250Aimprovements%2520in%2520precision%252C%2520responsiveness%252C%2520and%2520situational%2520awareness.%2520Finally%252C%250Awe%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework%2520through%2520real-world%250Aexperiments%2520in%2520two%2520distinct%2520application%2520scenarios%252C%2520showcasing%2520the%2520practical%250Acapabilities%2520and%2520versatility%2520of%2520the%2520Splat-based%2520VR%2520interface.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Immersive%20Teleoperation%20Framework%20for%20Locomanipulation%20Tasks&entry.906535625=Takuya%20Boehringer%20and%20Jonathan%20Embley-Riches%20and%20Karim%20Hammoud%20and%20Valerio%20Modugno%20and%20Dimitrios%20Kanoulas&entry.1292438233=%20%20Recent%20advancements%20in%20robotic%20loco-manipulation%20have%20leveraged%20Virtual%0AReality%20%28VR%29%20to%20enhance%20the%20precision%20and%20immersiveness%20of%20teleoperation%0Asystems%2C%20significantly%20outperforming%20traditional%20methods%20reliant%20on%202D%20camera%0Afeeds%20and%20joystick%20controls.%20Despite%20these%20advancements%2C%20challenges%20remain%2C%0Aparticularly%20concerning%20user%20experience%20across%20different%20setups.%20This%20paper%0Aintroduces%20a%20novel%20VR-based%20teleoperation%20framework%20designed%20for%20a%20robotic%0Amanipulator%20integrated%20onto%20a%20mobile%20platform.%20Central%20to%20our%20approach%20is%20the%0Aapplication%20of%20Gaussian%20splatting%2C%20a%20technique%20that%20abstracts%20the%20manipulable%0Ascene%20into%20a%20VR%20environment%2C%20thereby%20enabling%20more%20intuitive%20and%20immersive%0Ainteractions.%20Users%20can%20navigate%20and%20manipulate%20within%20the%20virtual%20scene%20as%20if%0Ainteracting%20with%20a%20real%20robot%2C%20enhancing%20both%20the%20engagement%20and%20efficacy%20of%0Ateleoperation%20tasks.%20An%20extensive%20user%20study%20validates%20our%20approach%2C%0Ademonstrating%20significant%20usability%20and%20efficiency%20improvements.%20Two-thirds%0A%2866%25%29%20of%20participants%20completed%20tasks%20faster%2C%20achieving%20an%20average%20time%0Areduction%20of%2043%25.%20Additionally%2C%2093%25%20preferred%20the%20Gaussian%20Splat%20interface%0Aoverall%2C%20with%20unanimous%20%28100%25%29%20recommendations%20for%20future%20use%2C%20highlighting%0Aimprovements%20in%20precision%2C%20responsiveness%2C%20and%20situational%20awareness.%20Finally%2C%0Awe%20demonstrate%20the%20effectiveness%20of%20our%20framework%20through%20real-world%0Aexperiments%20in%20two%20distinct%20application%20scenarios%2C%20showcasing%20the%20practical%0Acapabilities%20and%20versatility%20of%20the%20Splat-based%20VR%20interface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15229v1&entry.124074799=Read"},
{"title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding", "author": "Rui Hu and Lianghui Zhu and Yuxuan Zhang and Tianheng Cheng and Lei Liu and Heng Liu and Longjin Ran and Xiaoxin Chen and Wenyu Liu and Xinggang Wang", "abstract": "  Pixel grounding, encompassing tasks such as Referring Expression Segmentation\n(RES), has garnered considerable attention due to its immense potential for\nbridging the gap between vision and language modalities. However, advancements\nin this domain are currently constrained by limitations inherent in existing\ndatasets, including limited object categories, insufficient textual diversity,\nand a scarcity of high-quality annotations. To mitigate these limitations, we\nintroduce GroundingSuite, which comprises: (1) an automated data annotation\nframework leveraging multiple Vision-Language Model (VLM) agents; (2) a\nlarge-scale training dataset encompassing 9.56 million diverse referring\nexpressions and their corresponding segmentations; and (3) a meticulously\ncurated evaluation benchmark consisting of 3,800 images. The GroundingSuite\ntraining dataset facilitates substantial performance improvements, enabling\nmodels trained on it to achieve state-of-the-art results. Specifically, a cIoU\nof 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the\nGroundingSuite annotation framework demonstrates superior efficiency compared\nto the current leading data annotation method, i.e., $4.5 \\times$ faster than\nthe GLaMM.\n", "link": "http://arxiv.org/abs/2503.10596v2", "date": "2025-04-21", "relevancy": 2.2494, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5704}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GroundingSuite%3A%20Measuring%20Complex%20Multi-Granular%20Pixel%20Grounding&body=Title%3A%20GroundingSuite%3A%20Measuring%20Complex%20Multi-Granular%20Pixel%20Grounding%0AAuthor%3A%20Rui%20Hu%20and%20Lianghui%20Zhu%20and%20Yuxuan%20Zhang%20and%20Tianheng%20Cheng%20and%20Lei%20Liu%20and%20Heng%20Liu%20and%20Longjin%20Ran%20and%20Xiaoxin%20Chen%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20Pixel%20grounding%2C%20encompassing%20tasks%20such%20as%20Referring%20Expression%20Segmentation%0A%28RES%29%2C%20has%20garnered%20considerable%20attention%20due%20to%20its%20immense%20potential%20for%0Abridging%20the%20gap%20between%20vision%20and%20language%20modalities.%20However%2C%20advancements%0Ain%20this%20domain%20are%20currently%20constrained%20by%20limitations%20inherent%20in%20existing%0Adatasets%2C%20including%20limited%20object%20categories%2C%20insufficient%20textual%20diversity%2C%0Aand%20a%20scarcity%20of%20high-quality%20annotations.%20To%20mitigate%20these%20limitations%2C%20we%0Aintroduce%20GroundingSuite%2C%20which%20comprises%3A%20%281%29%20an%20automated%20data%20annotation%0Aframework%20leveraging%20multiple%20Vision-Language%20Model%20%28VLM%29%20agents%3B%20%282%29%20a%0Alarge-scale%20training%20dataset%20encompassing%209.56%20million%20diverse%20referring%0Aexpressions%20and%20their%20corresponding%20segmentations%3B%20and%20%283%29%20a%20meticulously%0Acurated%20evaluation%20benchmark%20consisting%20of%203%2C800%20images.%20The%20GroundingSuite%0Atraining%20dataset%20facilitates%20substantial%20performance%20improvements%2C%20enabling%0Amodels%20trained%20on%20it%20to%20achieve%20state-of-the-art%20results.%20Specifically%2C%20a%20cIoU%0Aof%2068.9%20on%20gRefCOCO%20and%20a%20gIoU%20of%2055.3%20on%20RefCOCOm.%20Moreover%2C%20the%0AGroundingSuite%20annotation%20framework%20demonstrates%20superior%20efficiency%20compared%0Ato%20the%20current%20leading%20data%20annotation%20method%2C%20i.e.%2C%20%244.5%20%5Ctimes%24%20faster%20than%0Athe%20GLaMM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10596v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroundingSuite%253A%2520Measuring%2520Complex%2520Multi-Granular%2520Pixel%2520Grounding%26entry.906535625%3DRui%2520Hu%2520and%2520Lianghui%2520Zhu%2520and%2520Yuxuan%2520Zhang%2520and%2520Tianheng%2520Cheng%2520and%2520Lei%2520Liu%2520and%2520Heng%2520Liu%2520and%2520Longjin%2520Ran%2520and%2520Xiaoxin%2520Chen%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520Pixel%2520grounding%252C%2520encompassing%2520tasks%2520such%2520as%2520Referring%2520Expression%2520Segmentation%250A%2528RES%2529%252C%2520has%2520garnered%2520considerable%2520attention%2520due%2520to%2520its%2520immense%2520potential%2520for%250Abridging%2520the%2520gap%2520between%2520vision%2520and%2520language%2520modalities.%2520However%252C%2520advancements%250Ain%2520this%2520domain%2520are%2520currently%2520constrained%2520by%2520limitations%2520inherent%2520in%2520existing%250Adatasets%252C%2520including%2520limited%2520object%2520categories%252C%2520insufficient%2520textual%2520diversity%252C%250Aand%2520a%2520scarcity%2520of%2520high-quality%2520annotations.%2520To%2520mitigate%2520these%2520limitations%252C%2520we%250Aintroduce%2520GroundingSuite%252C%2520which%2520comprises%253A%2520%25281%2529%2520an%2520automated%2520data%2520annotation%250Aframework%2520leveraging%2520multiple%2520Vision-Language%2520Model%2520%2528VLM%2529%2520agents%253B%2520%25282%2529%2520a%250Alarge-scale%2520training%2520dataset%2520encompassing%25209.56%2520million%2520diverse%2520referring%250Aexpressions%2520and%2520their%2520corresponding%2520segmentations%253B%2520and%2520%25283%2529%2520a%2520meticulously%250Acurated%2520evaluation%2520benchmark%2520consisting%2520of%25203%252C800%2520images.%2520The%2520GroundingSuite%250Atraining%2520dataset%2520facilitates%2520substantial%2520performance%2520improvements%252C%2520enabling%250Amodels%2520trained%2520on%2520it%2520to%2520achieve%2520state-of-the-art%2520results.%2520Specifically%252C%2520a%2520cIoU%250Aof%252068.9%2520on%2520gRefCOCO%2520and%2520a%2520gIoU%2520of%252055.3%2520on%2520RefCOCOm.%2520Moreover%252C%2520the%250AGroundingSuite%2520annotation%2520framework%2520demonstrates%2520superior%2520efficiency%2520compared%250Ato%2520the%2520current%2520leading%2520data%2520annotation%2520method%252C%2520i.e.%252C%2520%25244.5%2520%255Ctimes%2524%2520faster%2520than%250Athe%2520GLaMM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10596v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GroundingSuite%3A%20Measuring%20Complex%20Multi-Granular%20Pixel%20Grounding&entry.906535625=Rui%20Hu%20and%20Lianghui%20Zhu%20and%20Yuxuan%20Zhang%20and%20Tianheng%20Cheng%20and%20Lei%20Liu%20and%20Heng%20Liu%20and%20Longjin%20Ran%20and%20Xiaoxin%20Chen%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=%20%20Pixel%20grounding%2C%20encompassing%20tasks%20such%20as%20Referring%20Expression%20Segmentation%0A%28RES%29%2C%20has%20garnered%20considerable%20attention%20due%20to%20its%20immense%20potential%20for%0Abridging%20the%20gap%20between%20vision%20and%20language%20modalities.%20However%2C%20advancements%0Ain%20this%20domain%20are%20currently%20constrained%20by%20limitations%20inherent%20in%20existing%0Adatasets%2C%20including%20limited%20object%20categories%2C%20insufficient%20textual%20diversity%2C%0Aand%20a%20scarcity%20of%20high-quality%20annotations.%20To%20mitigate%20these%20limitations%2C%20we%0Aintroduce%20GroundingSuite%2C%20which%20comprises%3A%20%281%29%20an%20automated%20data%20annotation%0Aframework%20leveraging%20multiple%20Vision-Language%20Model%20%28VLM%29%20agents%3B%20%282%29%20a%0Alarge-scale%20training%20dataset%20encompassing%209.56%20million%20diverse%20referring%0Aexpressions%20and%20their%20corresponding%20segmentations%3B%20and%20%283%29%20a%20meticulously%0Acurated%20evaluation%20benchmark%20consisting%20of%203%2C800%20images.%20The%20GroundingSuite%0Atraining%20dataset%20facilitates%20substantial%20performance%20improvements%2C%20enabling%0Amodels%20trained%20on%20it%20to%20achieve%20state-of-the-art%20results.%20Specifically%2C%20a%20cIoU%0Aof%2068.9%20on%20gRefCOCO%20and%20a%20gIoU%20of%2055.3%20on%20RefCOCOm.%20Moreover%2C%20the%0AGroundingSuite%20annotation%20framework%20demonstrates%20superior%20efficiency%20compared%0Ato%20the%20current%20leading%20data%20annotation%20method%2C%20i.e.%2C%20%244.5%20%5Ctimes%24%20faster%20than%0Athe%20GLaMM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10596v2&entry.124074799=Read"},
{"title": "Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC\n  Platform", "author": "Xianpan Zhou", "abstract": "  The recent surge in open-source text-to-video generation models has\nsignificantly energized the research community, yet their dependence on\nproprietary training datasets remains a key constraint. While existing open\ndatasets like Koala-36M employ algorithmic filtering of web-scraped videos from\nearly platforms, they still lack the quality required for fine-tuning advanced\nvideo generation models. We present Tiger200K, a manually curated high visual\nquality video dataset sourced from User-Generated Content (UGC) platforms. By\nprioritizing visual fidelity and aesthetic quality, Tiger200K underscores the\ncritical role of human expertise in data curation, and providing high-quality,\ntemporally consistent video-text pairs for fine-tuning and optimizing video\ngeneration architectures through a simple but effective pipeline including shot\nboundary detection, OCR, border detecting, motion filter and fine bilingual\ncaption. The dataset will undergo ongoing expansion and be released as an\nopen-source initiative to advance research and applications in video generative\nmodels. Project page: https://tinytigerpan.github.io/tiger200k/\n", "link": "http://arxiv.org/abs/2504.15182v1", "date": "2025-04-21", "relevancy": 2.2457, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5751}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5709}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5465}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tiger200K%3A%20Manually%20Curated%20High%20Visual%20Quality%20Video%20Dataset%20from%20UGC%0A%20%20Platform&body=Title%3A%20Tiger200K%3A%20Manually%20Curated%20High%20Visual%20Quality%20Video%20Dataset%20from%20UGC%0A%20%20Platform%0AAuthor%3A%20Xianpan%20Zhou%0AAbstract%3A%20%20%20The%20recent%20surge%20in%20open-source%20text-to-video%20generation%20models%20has%0Asignificantly%20energized%20the%20research%20community%2C%20yet%20their%20dependence%20on%0Aproprietary%20training%20datasets%20remains%20a%20key%20constraint.%20While%20existing%20open%0Adatasets%20like%20Koala-36M%20employ%20algorithmic%20filtering%20of%20web-scraped%20videos%20from%0Aearly%20platforms%2C%20they%20still%20lack%20the%20quality%20required%20for%20fine-tuning%20advanced%0Avideo%20generation%20models.%20We%20present%20Tiger200K%2C%20a%20manually%20curated%20high%20visual%0Aquality%20video%20dataset%20sourced%20from%20User-Generated%20Content%20%28UGC%29%20platforms.%20By%0Aprioritizing%20visual%20fidelity%20and%20aesthetic%20quality%2C%20Tiger200K%20underscores%20the%0Acritical%20role%20of%20human%20expertise%20in%20data%20curation%2C%20and%20providing%20high-quality%2C%0Atemporally%20consistent%20video-text%20pairs%20for%20fine-tuning%20and%20optimizing%20video%0Ageneration%20architectures%20through%20a%20simple%20but%20effective%20pipeline%20including%20shot%0Aboundary%20detection%2C%20OCR%2C%20border%20detecting%2C%20motion%20filter%20and%20fine%20bilingual%0Acaption.%20The%20dataset%20will%20undergo%20ongoing%20expansion%20and%20be%20released%20as%20an%0Aopen-source%20initiative%20to%20advance%20research%20and%20applications%20in%20video%20generative%0Amodels.%20Project%20page%3A%20https%3A//tinytigerpan.github.io/tiger200k/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15182v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiger200K%253A%2520Manually%2520Curated%2520High%2520Visual%2520Quality%2520Video%2520Dataset%2520from%2520UGC%250A%2520%2520Platform%26entry.906535625%3DXianpan%2520Zhou%26entry.1292438233%3D%2520%2520The%2520recent%2520surge%2520in%2520open-source%2520text-to-video%2520generation%2520models%2520has%250Asignificantly%2520energized%2520the%2520research%2520community%252C%2520yet%2520their%2520dependence%2520on%250Aproprietary%2520training%2520datasets%2520remains%2520a%2520key%2520constraint.%2520While%2520existing%2520open%250Adatasets%2520like%2520Koala-36M%2520employ%2520algorithmic%2520filtering%2520of%2520web-scraped%2520videos%2520from%250Aearly%2520platforms%252C%2520they%2520still%2520lack%2520the%2520quality%2520required%2520for%2520fine-tuning%2520advanced%250Avideo%2520generation%2520models.%2520We%2520present%2520Tiger200K%252C%2520a%2520manually%2520curated%2520high%2520visual%250Aquality%2520video%2520dataset%2520sourced%2520from%2520User-Generated%2520Content%2520%2528UGC%2529%2520platforms.%2520By%250Aprioritizing%2520visual%2520fidelity%2520and%2520aesthetic%2520quality%252C%2520Tiger200K%2520underscores%2520the%250Acritical%2520role%2520of%2520human%2520expertise%2520in%2520data%2520curation%252C%2520and%2520providing%2520high-quality%252C%250Atemporally%2520consistent%2520video-text%2520pairs%2520for%2520fine-tuning%2520and%2520optimizing%2520video%250Ageneration%2520architectures%2520through%2520a%2520simple%2520but%2520effective%2520pipeline%2520including%2520shot%250Aboundary%2520detection%252C%2520OCR%252C%2520border%2520detecting%252C%2520motion%2520filter%2520and%2520fine%2520bilingual%250Acaption.%2520The%2520dataset%2520will%2520undergo%2520ongoing%2520expansion%2520and%2520be%2520released%2520as%2520an%250Aopen-source%2520initiative%2520to%2520advance%2520research%2520and%2520applications%2520in%2520video%2520generative%250Amodels.%2520Project%2520page%253A%2520https%253A//tinytigerpan.github.io/tiger200k/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15182v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tiger200K%3A%20Manually%20Curated%20High%20Visual%20Quality%20Video%20Dataset%20from%20UGC%0A%20%20Platform&entry.906535625=Xianpan%20Zhou&entry.1292438233=%20%20The%20recent%20surge%20in%20open-source%20text-to-video%20generation%20models%20has%0Asignificantly%20energized%20the%20research%20community%2C%20yet%20their%20dependence%20on%0Aproprietary%20training%20datasets%20remains%20a%20key%20constraint.%20While%20existing%20open%0Adatasets%20like%20Koala-36M%20employ%20algorithmic%20filtering%20of%20web-scraped%20videos%20from%0Aearly%20platforms%2C%20they%20still%20lack%20the%20quality%20required%20for%20fine-tuning%20advanced%0Avideo%20generation%20models.%20We%20present%20Tiger200K%2C%20a%20manually%20curated%20high%20visual%0Aquality%20video%20dataset%20sourced%20from%20User-Generated%20Content%20%28UGC%29%20platforms.%20By%0Aprioritizing%20visual%20fidelity%20and%20aesthetic%20quality%2C%20Tiger200K%20underscores%20the%0Acritical%20role%20of%20human%20expertise%20in%20data%20curation%2C%20and%20providing%20high-quality%2C%0Atemporally%20consistent%20video-text%20pairs%20for%20fine-tuning%20and%20optimizing%20video%0Ageneration%20architectures%20through%20a%20simple%20but%20effective%20pipeline%20including%20shot%0Aboundary%20detection%2C%20OCR%2C%20border%20detecting%2C%20motion%20filter%20and%20fine%20bilingual%0Acaption.%20The%20dataset%20will%20undergo%20ongoing%20expansion%20and%20be%20released%20as%20an%0Aopen-source%20initiative%20to%20advance%20research%20and%20applications%20in%20video%20generative%0Amodels.%20Project%20page%3A%20https%3A//tinytigerpan.github.io/tiger200k/%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15182v1&entry.124074799=Read"},
{"title": "DSPO: Direct Semantic Preference Optimization for Real-World Image\n  Super-Resolution", "author": "Miaomiao Cai and Simiao Li and Wei Li and Xudong Huang and Hanting Chen and Jie Hu and Yunhe Wang", "abstract": "  Recent advances in diffusion models have improved Real-World Image\nSuper-Resolution (Real-ISR), but existing methods lack human feedback\nintegration, risking misalignment with human preference and may leading to\nartifacts, hallucinations and harmful content generation. To this end, we are\nthe first to introduce human preference alignment into Real-ISR, a technique\nthat has been successfully applied in Large Language Models and Text-to-Image\ntasks to effectively enhance the alignment of generated outputs with human\npreferences. Specifically, we introduce Direct Preference Optimization (DPO)\ninto Real-ISR to achieve alignment, where DPO serves as a general alignment\ntechnique that directly learns from the human preference dataset. Nevertheless,\nunlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR\nare difficult to reconcile with the image-level preferences of DPO, which can\nlead to the DPO being overly sensitive to local anomalies, leading to reduced\ngeneration quality. To resolve this dichotomy, we propose Direct Semantic\nPreference Optimization (DSPO) to align instance-level human preferences by\nincorporating semantic guidance, which is through two strategies: (a) semantic\ninstance alignment strategy, implementing instance-level alignment to ensure\nfine-grained perceptual consistency, and (b) user description feedback\nstrategy, mitigating hallucinations through semantic textual feedback on\ninstance-level images. As a plug-and-play solution, DSPO proves highly\neffective in both one-step and multi-step SR frameworks.\n", "link": "http://arxiv.org/abs/2504.15176v1", "date": "2025-04-21", "relevancy": 2.1971, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5672}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.55}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSPO%3A%20Direct%20Semantic%20Preference%20Optimization%20for%20Real-World%20Image%0A%20%20Super-Resolution&body=Title%3A%20DSPO%3A%20Direct%20Semantic%20Preference%20Optimization%20for%20Real-World%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Miaomiao%20Cai%20and%20Simiao%20Li%20and%20Wei%20Li%20and%20Xudong%20Huang%20and%20Hanting%20Chen%20and%20Jie%20Hu%20and%20Yunhe%20Wang%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20have%20improved%20Real-World%20Image%0ASuper-Resolution%20%28Real-ISR%29%2C%20but%20existing%20methods%20lack%20human%20feedback%0Aintegration%2C%20risking%20misalignment%20with%20human%20preference%20and%20may%20leading%20to%0Aartifacts%2C%20hallucinations%20and%20harmful%20content%20generation.%20To%20this%20end%2C%20we%20are%0Athe%20first%20to%20introduce%20human%20preference%20alignment%20into%20Real-ISR%2C%20a%20technique%0Athat%20has%20been%20successfully%20applied%20in%20Large%20Language%20Models%20and%20Text-to-Image%0Atasks%20to%20effectively%20enhance%20the%20alignment%20of%20generated%20outputs%20with%20human%0Apreferences.%20Specifically%2C%20we%20introduce%20Direct%20Preference%20Optimization%20%28DPO%29%0Ainto%20Real-ISR%20to%20achieve%20alignment%2C%20where%20DPO%20serves%20as%20a%20general%20alignment%0Atechnique%20that%20directly%20learns%20from%20the%20human%20preference%20dataset.%20Nevertheless%2C%0Aunlike%20high-level%20tasks%2C%20the%20pixel-level%20reconstruction%20objectives%20of%20Real-ISR%0Aare%20difficult%20to%20reconcile%20with%20the%20image-level%20preferences%20of%20DPO%2C%20which%20can%0Alead%20to%20the%20DPO%20being%20overly%20sensitive%20to%20local%20anomalies%2C%20leading%20to%20reduced%0Ageneration%20quality.%20To%20resolve%20this%20dichotomy%2C%20we%20propose%20Direct%20Semantic%0APreference%20Optimization%20%28DSPO%29%20to%20align%20instance-level%20human%20preferences%20by%0Aincorporating%20semantic%20guidance%2C%20which%20is%20through%20two%20strategies%3A%20%28a%29%20semantic%0Ainstance%20alignment%20strategy%2C%20implementing%20instance-level%20alignment%20to%20ensure%0Afine-grained%20perceptual%20consistency%2C%20and%20%28b%29%20user%20description%20feedback%0Astrategy%2C%20mitigating%20hallucinations%20through%20semantic%20textual%20feedback%20on%0Ainstance-level%20images.%20As%20a%20plug-and-play%20solution%2C%20DSPO%20proves%20highly%0Aeffective%20in%20both%20one-step%20and%20multi-step%20SR%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSPO%253A%2520Direct%2520Semantic%2520Preference%2520Optimization%2520for%2520Real-World%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DMiaomiao%2520Cai%2520and%2520Simiao%2520Li%2520and%2520Wei%2520Li%2520and%2520Xudong%2520Huang%2520and%2520Hanting%2520Chen%2520and%2520Jie%2520Hu%2520and%2520Yunhe%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520diffusion%2520models%2520have%2520improved%2520Real-World%2520Image%250ASuper-Resolution%2520%2528Real-ISR%2529%252C%2520but%2520existing%2520methods%2520lack%2520human%2520feedback%250Aintegration%252C%2520risking%2520misalignment%2520with%2520human%2520preference%2520and%2520may%2520leading%2520to%250Aartifacts%252C%2520hallucinations%2520and%2520harmful%2520content%2520generation.%2520To%2520this%2520end%252C%2520we%2520are%250Athe%2520first%2520to%2520introduce%2520human%2520preference%2520alignment%2520into%2520Real-ISR%252C%2520a%2520technique%250Athat%2520has%2520been%2520successfully%2520applied%2520in%2520Large%2520Language%2520Models%2520and%2520Text-to-Image%250Atasks%2520to%2520effectively%2520enhance%2520the%2520alignment%2520of%2520generated%2520outputs%2520with%2520human%250Apreferences.%2520Specifically%252C%2520we%2520introduce%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%250Ainto%2520Real-ISR%2520to%2520achieve%2520alignment%252C%2520where%2520DPO%2520serves%2520as%2520a%2520general%2520alignment%250Atechnique%2520that%2520directly%2520learns%2520from%2520the%2520human%2520preference%2520dataset.%2520Nevertheless%252C%250Aunlike%2520high-level%2520tasks%252C%2520the%2520pixel-level%2520reconstruction%2520objectives%2520of%2520Real-ISR%250Aare%2520difficult%2520to%2520reconcile%2520with%2520the%2520image-level%2520preferences%2520of%2520DPO%252C%2520which%2520can%250Alead%2520to%2520the%2520DPO%2520being%2520overly%2520sensitive%2520to%2520local%2520anomalies%252C%2520leading%2520to%2520reduced%250Ageneration%2520quality.%2520To%2520resolve%2520this%2520dichotomy%252C%2520we%2520propose%2520Direct%2520Semantic%250APreference%2520Optimization%2520%2528DSPO%2529%2520to%2520align%2520instance-level%2520human%2520preferences%2520by%250Aincorporating%2520semantic%2520guidance%252C%2520which%2520is%2520through%2520two%2520strategies%253A%2520%2528a%2529%2520semantic%250Ainstance%2520alignment%2520strategy%252C%2520implementing%2520instance-level%2520alignment%2520to%2520ensure%250Afine-grained%2520perceptual%2520consistency%252C%2520and%2520%2528b%2529%2520user%2520description%2520feedback%250Astrategy%252C%2520mitigating%2520hallucinations%2520through%2520semantic%2520textual%2520feedback%2520on%250Ainstance-level%2520images.%2520As%2520a%2520plug-and-play%2520solution%252C%2520DSPO%2520proves%2520highly%250Aeffective%2520in%2520both%2520one-step%2520and%2520multi-step%2520SR%2520frameworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSPO%3A%20Direct%20Semantic%20Preference%20Optimization%20for%20Real-World%20Image%0A%20%20Super-Resolution&entry.906535625=Miaomiao%20Cai%20and%20Simiao%20Li%20and%20Wei%20Li%20and%20Xudong%20Huang%20and%20Hanting%20Chen%20and%20Jie%20Hu%20and%20Yunhe%20Wang&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20have%20improved%20Real-World%20Image%0ASuper-Resolution%20%28Real-ISR%29%2C%20but%20existing%20methods%20lack%20human%20feedback%0Aintegration%2C%20risking%20misalignment%20with%20human%20preference%20and%20may%20leading%20to%0Aartifacts%2C%20hallucinations%20and%20harmful%20content%20generation.%20To%20this%20end%2C%20we%20are%0Athe%20first%20to%20introduce%20human%20preference%20alignment%20into%20Real-ISR%2C%20a%20technique%0Athat%20has%20been%20successfully%20applied%20in%20Large%20Language%20Models%20and%20Text-to-Image%0Atasks%20to%20effectively%20enhance%20the%20alignment%20of%20generated%20outputs%20with%20human%0Apreferences.%20Specifically%2C%20we%20introduce%20Direct%20Preference%20Optimization%20%28DPO%29%0Ainto%20Real-ISR%20to%20achieve%20alignment%2C%20where%20DPO%20serves%20as%20a%20general%20alignment%0Atechnique%20that%20directly%20learns%20from%20the%20human%20preference%20dataset.%20Nevertheless%2C%0Aunlike%20high-level%20tasks%2C%20the%20pixel-level%20reconstruction%20objectives%20of%20Real-ISR%0Aare%20difficult%20to%20reconcile%20with%20the%20image-level%20preferences%20of%20DPO%2C%20which%20can%0Alead%20to%20the%20DPO%20being%20overly%20sensitive%20to%20local%20anomalies%2C%20leading%20to%20reduced%0Ageneration%20quality.%20To%20resolve%20this%20dichotomy%2C%20we%20propose%20Direct%20Semantic%0APreference%20Optimization%20%28DSPO%29%20to%20align%20instance-level%20human%20preferences%20by%0Aincorporating%20semantic%20guidance%2C%20which%20is%20through%20two%20strategies%3A%20%28a%29%20semantic%0Ainstance%20alignment%20strategy%2C%20implementing%20instance-level%20alignment%20to%20ensure%0Afine-grained%20perceptual%20consistency%2C%20and%20%28b%29%20user%20description%20feedback%0Astrategy%2C%20mitigating%20hallucinations%20through%20semantic%20textual%20feedback%20on%0Ainstance-level%20images.%20As%20a%20plug-and-play%20solution%2C%20DSPO%20proves%20highly%0Aeffective%20in%20both%20one-step%20and%20multi-step%20SR%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15176v1&entry.124074799=Read"},
{"title": "Revealing the 3D Cosmic Web through Gravitationally Constrained Neural\n  Fields", "author": "Brandon Zhao and Aviad Levis and Liam Connor and Pratul P. Srinivasan and Katherine L. Bouman", "abstract": "  Weak gravitational lensing is the slight distortion of galaxy shapes caused\nprimarily by the gravitational effects of dark matter in the universe. In our\nwork, we seek to invert the weak lensing signal from 2D telescope images to\nreconstruct a 3D map of the universe's dark matter field. While inversion\ntypically yields a 2D projection of the dark matter field, accurate 3D maps of\nthe dark matter distribution are essential for localizing structures of\ninterest and testing theories of our universe. However, 3D inversion poses\nsignificant challenges. First, unlike standard 3D reconstruction that relies on\nmultiple viewpoints, in this case, images are only observed from a single\nviewpoint. This challenge can be partially addressed by observing how galaxy\nemitters throughout the volume are lensed. However, this leads to the second\nchallenge: the shapes and exact locations of unlensed galaxies are unknown, and\ncan only be estimated with a very large degree of uncertainty. This introduces\nan overwhelming amount of noise which nearly drowns out the lensing signal\ncompletely. Previous approaches tackle this by imposing strong assumptions\nabout the structures in the volume. We instead propose a methodology using a\ngravitationally-constrained neural field to flexibly model the continuous\nmatter distribution. We take an analysis-by-synthesis approach, optimizing the\nweights of the neural network through a fully differentiable physical forward\nmodel to reproduce the lensing signal present in image measurements. We\nshowcase our method on simulations, including realistic simulated measurements\nof dark matter distributions that mimic data from upcoming telescope surveys.\nOur results show that our method can not only outperform previous methods, but\nimportantly is also able to recover potentially surprising dark matter\nstructures.\n", "link": "http://arxiv.org/abs/2504.15262v1", "date": "2025-04-21", "relevancy": 2.1969, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5829}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5434}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20the%203D%20Cosmic%20Web%20through%20Gravitationally%20Constrained%20Neural%0A%20%20Fields&body=Title%3A%20Revealing%20the%203D%20Cosmic%20Web%20through%20Gravitationally%20Constrained%20Neural%0A%20%20Fields%0AAuthor%3A%20Brandon%20Zhao%20and%20Aviad%20Levis%20and%20Liam%20Connor%20and%20Pratul%20P.%20Srinivasan%20and%20Katherine%20L.%20Bouman%0AAbstract%3A%20%20%20Weak%20gravitational%20lensing%20is%20the%20slight%20distortion%20of%20galaxy%20shapes%20caused%0Aprimarily%20by%20the%20gravitational%20effects%20of%20dark%20matter%20in%20the%20universe.%20In%20our%0Awork%2C%20we%20seek%20to%20invert%20the%20weak%20lensing%20signal%20from%202D%20telescope%20images%20to%0Areconstruct%20a%203D%20map%20of%20the%20universe%27s%20dark%20matter%20field.%20While%20inversion%0Atypically%20yields%20a%202D%20projection%20of%20the%20dark%20matter%20field%2C%20accurate%203D%20maps%20of%0Athe%20dark%20matter%20distribution%20are%20essential%20for%20localizing%20structures%20of%0Ainterest%20and%20testing%20theories%20of%20our%20universe.%20However%2C%203D%20inversion%20poses%0Asignificant%20challenges.%20First%2C%20unlike%20standard%203D%20reconstruction%20that%20relies%20on%0Amultiple%20viewpoints%2C%20in%20this%20case%2C%20images%20are%20only%20observed%20from%20a%20single%0Aviewpoint.%20This%20challenge%20can%20be%20partially%20addressed%20by%20observing%20how%20galaxy%0Aemitters%20throughout%20the%20volume%20are%20lensed.%20However%2C%20this%20leads%20to%20the%20second%0Achallenge%3A%20the%20shapes%20and%20exact%20locations%20of%20unlensed%20galaxies%20are%20unknown%2C%20and%0Acan%20only%20be%20estimated%20with%20a%20very%20large%20degree%20of%20uncertainty.%20This%20introduces%0Aan%20overwhelming%20amount%20of%20noise%20which%20nearly%20drowns%20out%20the%20lensing%20signal%0Acompletely.%20Previous%20approaches%20tackle%20this%20by%20imposing%20strong%20assumptions%0Aabout%20the%20structures%20in%20the%20volume.%20We%20instead%20propose%20a%20methodology%20using%20a%0Agravitationally-constrained%20neural%20field%20to%20flexibly%20model%20the%20continuous%0Amatter%20distribution.%20We%20take%20an%20analysis-by-synthesis%20approach%2C%20optimizing%20the%0Aweights%20of%20the%20neural%20network%20through%20a%20fully%20differentiable%20physical%20forward%0Amodel%20to%20reproduce%20the%20lensing%20signal%20present%20in%20image%20measurements.%20We%0Ashowcase%20our%20method%20on%20simulations%2C%20including%20realistic%20simulated%20measurements%0Aof%20dark%20matter%20distributions%20that%20mimic%20data%20from%20upcoming%20telescope%20surveys.%0AOur%20results%20show%20that%20our%20method%20can%20not%20only%20outperform%20previous%20methods%2C%20but%0Aimportantly%20is%20also%20able%20to%20recover%20potentially%20surprising%20dark%20matter%0Astructures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520the%25203D%2520Cosmic%2520Web%2520through%2520Gravitationally%2520Constrained%2520Neural%250A%2520%2520Fields%26entry.906535625%3DBrandon%2520Zhao%2520and%2520Aviad%2520Levis%2520and%2520Liam%2520Connor%2520and%2520Pratul%2520P.%2520Srinivasan%2520and%2520Katherine%2520L.%2520Bouman%26entry.1292438233%3D%2520%2520Weak%2520gravitational%2520lensing%2520is%2520the%2520slight%2520distortion%2520of%2520galaxy%2520shapes%2520caused%250Aprimarily%2520by%2520the%2520gravitational%2520effects%2520of%2520dark%2520matter%2520in%2520the%2520universe.%2520In%2520our%250Awork%252C%2520we%2520seek%2520to%2520invert%2520the%2520weak%2520lensing%2520signal%2520from%25202D%2520telescope%2520images%2520to%250Areconstruct%2520a%25203D%2520map%2520of%2520the%2520universe%2527s%2520dark%2520matter%2520field.%2520While%2520inversion%250Atypically%2520yields%2520a%25202D%2520projection%2520of%2520the%2520dark%2520matter%2520field%252C%2520accurate%25203D%2520maps%2520of%250Athe%2520dark%2520matter%2520distribution%2520are%2520essential%2520for%2520localizing%2520structures%2520of%250Ainterest%2520and%2520testing%2520theories%2520of%2520our%2520universe.%2520However%252C%25203D%2520inversion%2520poses%250Asignificant%2520challenges.%2520First%252C%2520unlike%2520standard%25203D%2520reconstruction%2520that%2520relies%2520on%250Amultiple%2520viewpoints%252C%2520in%2520this%2520case%252C%2520images%2520are%2520only%2520observed%2520from%2520a%2520single%250Aviewpoint.%2520This%2520challenge%2520can%2520be%2520partially%2520addressed%2520by%2520observing%2520how%2520galaxy%250Aemitters%2520throughout%2520the%2520volume%2520are%2520lensed.%2520However%252C%2520this%2520leads%2520to%2520the%2520second%250Achallenge%253A%2520the%2520shapes%2520and%2520exact%2520locations%2520of%2520unlensed%2520galaxies%2520are%2520unknown%252C%2520and%250Acan%2520only%2520be%2520estimated%2520with%2520a%2520very%2520large%2520degree%2520of%2520uncertainty.%2520This%2520introduces%250Aan%2520overwhelming%2520amount%2520of%2520noise%2520which%2520nearly%2520drowns%2520out%2520the%2520lensing%2520signal%250Acompletely.%2520Previous%2520approaches%2520tackle%2520this%2520by%2520imposing%2520strong%2520assumptions%250Aabout%2520the%2520structures%2520in%2520the%2520volume.%2520We%2520instead%2520propose%2520a%2520methodology%2520using%2520a%250Agravitationally-constrained%2520neural%2520field%2520to%2520flexibly%2520model%2520the%2520continuous%250Amatter%2520distribution.%2520We%2520take%2520an%2520analysis-by-synthesis%2520approach%252C%2520optimizing%2520the%250Aweights%2520of%2520the%2520neural%2520network%2520through%2520a%2520fully%2520differentiable%2520physical%2520forward%250Amodel%2520to%2520reproduce%2520the%2520lensing%2520signal%2520present%2520in%2520image%2520measurements.%2520We%250Ashowcase%2520our%2520method%2520on%2520simulations%252C%2520including%2520realistic%2520simulated%2520measurements%250Aof%2520dark%2520matter%2520distributions%2520that%2520mimic%2520data%2520from%2520upcoming%2520telescope%2520surveys.%250AOur%2520results%2520show%2520that%2520our%2520method%2520can%2520not%2520only%2520outperform%2520previous%2520methods%252C%2520but%250Aimportantly%2520is%2520also%2520able%2520to%2520recover%2520potentially%2520surprising%2520dark%2520matter%250Astructures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20the%203D%20Cosmic%20Web%20through%20Gravitationally%20Constrained%20Neural%0A%20%20Fields&entry.906535625=Brandon%20Zhao%20and%20Aviad%20Levis%20and%20Liam%20Connor%20and%20Pratul%20P.%20Srinivasan%20and%20Katherine%20L.%20Bouman&entry.1292438233=%20%20Weak%20gravitational%20lensing%20is%20the%20slight%20distortion%20of%20galaxy%20shapes%20caused%0Aprimarily%20by%20the%20gravitational%20effects%20of%20dark%20matter%20in%20the%20universe.%20In%20our%0Awork%2C%20we%20seek%20to%20invert%20the%20weak%20lensing%20signal%20from%202D%20telescope%20images%20to%0Areconstruct%20a%203D%20map%20of%20the%20universe%27s%20dark%20matter%20field.%20While%20inversion%0Atypically%20yields%20a%202D%20projection%20of%20the%20dark%20matter%20field%2C%20accurate%203D%20maps%20of%0Athe%20dark%20matter%20distribution%20are%20essential%20for%20localizing%20structures%20of%0Ainterest%20and%20testing%20theories%20of%20our%20universe.%20However%2C%203D%20inversion%20poses%0Asignificant%20challenges.%20First%2C%20unlike%20standard%203D%20reconstruction%20that%20relies%20on%0Amultiple%20viewpoints%2C%20in%20this%20case%2C%20images%20are%20only%20observed%20from%20a%20single%0Aviewpoint.%20This%20challenge%20can%20be%20partially%20addressed%20by%20observing%20how%20galaxy%0Aemitters%20throughout%20the%20volume%20are%20lensed.%20However%2C%20this%20leads%20to%20the%20second%0Achallenge%3A%20the%20shapes%20and%20exact%20locations%20of%20unlensed%20galaxies%20are%20unknown%2C%20and%0Acan%20only%20be%20estimated%20with%20a%20very%20large%20degree%20of%20uncertainty.%20This%20introduces%0Aan%20overwhelming%20amount%20of%20noise%20which%20nearly%20drowns%20out%20the%20lensing%20signal%0Acompletely.%20Previous%20approaches%20tackle%20this%20by%20imposing%20strong%20assumptions%0Aabout%20the%20structures%20in%20the%20volume.%20We%20instead%20propose%20a%20methodology%20using%20a%0Agravitationally-constrained%20neural%20field%20to%20flexibly%20model%20the%20continuous%0Amatter%20distribution.%20We%20take%20an%20analysis-by-synthesis%20approach%2C%20optimizing%20the%0Aweights%20of%20the%20neural%20network%20through%20a%20fully%20differentiable%20physical%20forward%0Amodel%20to%20reproduce%20the%20lensing%20signal%20present%20in%20image%20measurements.%20We%0Ashowcase%20our%20method%20on%20simulations%2C%20including%20realistic%20simulated%20measurements%0Aof%20dark%20matter%20distributions%20that%20mimic%20data%20from%20upcoming%20telescope%20surveys.%0AOur%20results%20show%20that%20our%20method%20can%20not%20only%20outperform%20previous%20methods%2C%20but%0Aimportantly%20is%20also%20able%20to%20recover%20potentially%20surprising%20dark%20matter%0Astructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15262v1&entry.124074799=Read"},
{"title": "Barren plateaus are amplified by the dimension of qudits", "author": "Lucas Friedrich and Tiago de Souza Farias and Jonas Maziero", "abstract": "  Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for\nattaining quantum advantage in diverse scientific and technological domains,\nnotably within Quantum Neural Networks. However, despite their potential, VQAs\nencounter significant obstacles, chief among them being the vanishing gradient\nproblem, commonly referred to as barren plateaus. In this article, through\nmeticulous analysis, we demonstrate that existing literature implicitly\nsuggests the intrinsic influence of qudit dimensionality on barren plateaus. To\ninstantiate these findings, we present numerical results that exemplify the\nimpact of qudit dimensionality on barren plateaus. Therefore, despite the\nproposition of various error mitigation techniques, our results call for\nfurther scrutiny about their efficacy in the context of VQAs with qudits.\n", "link": "http://arxiv.org/abs/2405.08190v3", "date": "2025-04-21", "relevancy": 2.1867, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4332}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Barren%20plateaus%20are%20amplified%20by%20the%20dimension%20of%20qudits&body=Title%3A%20Barren%20plateaus%20are%20amplified%20by%20the%20dimension%20of%20qudits%0AAuthor%3A%20Lucas%20Friedrich%20and%20Tiago%20de%20Souza%20Farias%20and%20Jonas%20Maziero%0AAbstract%3A%20%20%20Variational%20Quantum%20Algorithms%20%28VQAs%29%20have%20emerged%20as%20pivotal%20strategies%20for%0Aattaining%20quantum%20advantage%20in%20diverse%20scientific%20and%20technological%20domains%2C%0Anotably%20within%20Quantum%20Neural%20Networks.%20However%2C%20despite%20their%20potential%2C%20VQAs%0Aencounter%20significant%20obstacles%2C%20chief%20among%20them%20being%20the%20vanishing%20gradient%0Aproblem%2C%20commonly%20referred%20to%20as%20barren%20plateaus.%20In%20this%20article%2C%20through%0Ameticulous%20analysis%2C%20we%20demonstrate%20that%20existing%20literature%20implicitly%0Asuggests%20the%20intrinsic%20influence%20of%20qudit%20dimensionality%20on%20barren%20plateaus.%20To%0Ainstantiate%20these%20findings%2C%20we%20present%20numerical%20results%20that%20exemplify%20the%0Aimpact%20of%20qudit%20dimensionality%20on%20barren%20plateaus.%20Therefore%2C%20despite%20the%0Aproposition%20of%20various%20error%20mitigation%20techniques%2C%20our%20results%20call%20for%0Afurther%20scrutiny%20about%20their%20efficacy%20in%20the%20context%20of%20VQAs%20with%20qudits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08190v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBarren%2520plateaus%2520are%2520amplified%2520by%2520the%2520dimension%2520of%2520qudits%26entry.906535625%3DLucas%2520Friedrich%2520and%2520Tiago%2520de%2520Souza%2520Farias%2520and%2520Jonas%2520Maziero%26entry.1292438233%3D%2520%2520Variational%2520Quantum%2520Algorithms%2520%2528VQAs%2529%2520have%2520emerged%2520as%2520pivotal%2520strategies%2520for%250Aattaining%2520quantum%2520advantage%2520in%2520diverse%2520scientific%2520and%2520technological%2520domains%252C%250Anotably%2520within%2520Quantum%2520Neural%2520Networks.%2520However%252C%2520despite%2520their%2520potential%252C%2520VQAs%250Aencounter%2520significant%2520obstacles%252C%2520chief%2520among%2520them%2520being%2520the%2520vanishing%2520gradient%250Aproblem%252C%2520commonly%2520referred%2520to%2520as%2520barren%2520plateaus.%2520In%2520this%2520article%252C%2520through%250Ameticulous%2520analysis%252C%2520we%2520demonstrate%2520that%2520existing%2520literature%2520implicitly%250Asuggests%2520the%2520intrinsic%2520influence%2520of%2520qudit%2520dimensionality%2520on%2520barren%2520plateaus.%2520To%250Ainstantiate%2520these%2520findings%252C%2520we%2520present%2520numerical%2520results%2520that%2520exemplify%2520the%250Aimpact%2520of%2520qudit%2520dimensionality%2520on%2520barren%2520plateaus.%2520Therefore%252C%2520despite%2520the%250Aproposition%2520of%2520various%2520error%2520mitigation%2520techniques%252C%2520our%2520results%2520call%2520for%250Afurther%2520scrutiny%2520about%2520their%2520efficacy%2520in%2520the%2520context%2520of%2520VQAs%2520with%2520qudits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08190v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Barren%20plateaus%20are%20amplified%20by%20the%20dimension%20of%20qudits&entry.906535625=Lucas%20Friedrich%20and%20Tiago%20de%20Souza%20Farias%20and%20Jonas%20Maziero&entry.1292438233=%20%20Variational%20Quantum%20Algorithms%20%28VQAs%29%20have%20emerged%20as%20pivotal%20strategies%20for%0Aattaining%20quantum%20advantage%20in%20diverse%20scientific%20and%20technological%20domains%2C%0Anotably%20within%20Quantum%20Neural%20Networks.%20However%2C%20despite%20their%20potential%2C%20VQAs%0Aencounter%20significant%20obstacles%2C%20chief%20among%20them%20being%20the%20vanishing%20gradient%0Aproblem%2C%20commonly%20referred%20to%20as%20barren%20plateaus.%20In%20this%20article%2C%20through%0Ameticulous%20analysis%2C%20we%20demonstrate%20that%20existing%20literature%20implicitly%0Asuggests%20the%20intrinsic%20influence%20of%20qudit%20dimensionality%20on%20barren%20plateaus.%20To%0Ainstantiate%20these%20findings%2C%20we%20present%20numerical%20results%20that%20exemplify%20the%0Aimpact%20of%20qudit%20dimensionality%20on%20barren%20plateaus.%20Therefore%2C%20despite%20the%0Aproposition%20of%20various%20error%20mitigation%20techniques%2C%20our%20results%20call%20for%0Afurther%20scrutiny%20about%20their%20efficacy%20in%20the%20context%20of%20VQAs%20with%20qudits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08190v3&entry.124074799=Read"},
{"title": "\"I Know It When I See It\": Mood Spaces for Connecting and Expressing\n  Visual Concepts", "author": "Huzheng Yang and Katherine Xu and Michael D. Grossberg and Yutong Bai and Jianbo Shi", "abstract": "  Expressing complex concepts is easy when they can be labeled or quantified,\nbut many ideas are hard to define yet instantly recognizable. We propose a Mood\nBoard, where users convey abstract concepts with examples that hint at the\nintended direction of attribute changes. We compute an underlying Mood Space\nthat 1) factors out irrelevant features and 2) finds the connections between\nimages, thus bringing relevant concepts closer. We invent a fibration\ncomputation to compress/decompress pre-trained features into/from a compact\nspace, 50-100x smaller. The main innovation is learning to mimic the pairwise\naffinity relationship of the image tokens across exemplars. To focus on the\ncoarse-to-fine hierarchical structures in the Mood Space, we compute the top\neigenvector structure from the affinity matrix and define a loss in the\neigenvector space. The resulting Mood Space is locally linear and compact,\nallowing image-level operations, such as object averaging, visual analogy, and\npose transfer, to be performed as a simple vector operation in Mood Space. Our\nlearning is efficient in computation without any fine-tuning, needs only a few\n(2-20) exemplars, and takes less than a minute to learn.\n", "link": "http://arxiv.org/abs/2504.15145v1", "date": "2025-04-21", "relevancy": 2.1736, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5548}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22I%20Know%20It%20When%20I%20See%20It%22%3A%20Mood%20Spaces%20for%20Connecting%20and%20Expressing%0A%20%20Visual%20Concepts&body=Title%3A%20%22I%20Know%20It%20When%20I%20See%20It%22%3A%20Mood%20Spaces%20for%20Connecting%20and%20Expressing%0A%20%20Visual%20Concepts%0AAuthor%3A%20Huzheng%20Yang%20and%20Katherine%20Xu%20and%20Michael%20D.%20Grossberg%20and%20Yutong%20Bai%20and%20Jianbo%20Shi%0AAbstract%3A%20%20%20Expressing%20complex%20concepts%20is%20easy%20when%20they%20can%20be%20labeled%20or%20quantified%2C%0Abut%20many%20ideas%20are%20hard%20to%20define%20yet%20instantly%20recognizable.%20We%20propose%20a%20Mood%0ABoard%2C%20where%20users%20convey%20abstract%20concepts%20with%20examples%20that%20hint%20at%20the%0Aintended%20direction%20of%20attribute%20changes.%20We%20compute%20an%20underlying%20Mood%20Space%0Athat%201%29%20factors%20out%20irrelevant%20features%20and%202%29%20finds%20the%20connections%20between%0Aimages%2C%20thus%20bringing%20relevant%20concepts%20closer.%20We%20invent%20a%20fibration%0Acomputation%20to%20compress/decompress%20pre-trained%20features%20into/from%20a%20compact%0Aspace%2C%2050-100x%20smaller.%20The%20main%20innovation%20is%20learning%20to%20mimic%20the%20pairwise%0Aaffinity%20relationship%20of%20the%20image%20tokens%20across%20exemplars.%20To%20focus%20on%20the%0Acoarse-to-fine%20hierarchical%20structures%20in%20the%20Mood%20Space%2C%20we%20compute%20the%20top%0Aeigenvector%20structure%20from%20the%20affinity%20matrix%20and%20define%20a%20loss%20in%20the%0Aeigenvector%20space.%20The%20resulting%20Mood%20Space%20is%20locally%20linear%20and%20compact%2C%0Aallowing%20image-level%20operations%2C%20such%20as%20object%20averaging%2C%20visual%20analogy%2C%20and%0Apose%20transfer%2C%20to%20be%20performed%20as%20a%20simple%20vector%20operation%20in%20Mood%20Space.%20Our%0Alearning%20is%20efficient%20in%20computation%20without%20any%20fine-tuning%2C%20needs%20only%20a%20few%0A%282-20%29%20exemplars%2C%20and%20takes%20less%20than%20a%20minute%20to%20learn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522I%2520Know%2520It%2520When%2520I%2520See%2520It%2522%253A%2520Mood%2520Spaces%2520for%2520Connecting%2520and%2520Expressing%250A%2520%2520Visual%2520Concepts%26entry.906535625%3DHuzheng%2520Yang%2520and%2520Katherine%2520Xu%2520and%2520Michael%2520D.%2520Grossberg%2520and%2520Yutong%2520Bai%2520and%2520Jianbo%2520Shi%26entry.1292438233%3D%2520%2520Expressing%2520complex%2520concepts%2520is%2520easy%2520when%2520they%2520can%2520be%2520labeled%2520or%2520quantified%252C%250Abut%2520many%2520ideas%2520are%2520hard%2520to%2520define%2520yet%2520instantly%2520recognizable.%2520We%2520propose%2520a%2520Mood%250ABoard%252C%2520where%2520users%2520convey%2520abstract%2520concepts%2520with%2520examples%2520that%2520hint%2520at%2520the%250Aintended%2520direction%2520of%2520attribute%2520changes.%2520We%2520compute%2520an%2520underlying%2520Mood%2520Space%250Athat%25201%2529%2520factors%2520out%2520irrelevant%2520features%2520and%25202%2529%2520finds%2520the%2520connections%2520between%250Aimages%252C%2520thus%2520bringing%2520relevant%2520concepts%2520closer.%2520We%2520invent%2520a%2520fibration%250Acomputation%2520to%2520compress/decompress%2520pre-trained%2520features%2520into/from%2520a%2520compact%250Aspace%252C%252050-100x%2520smaller.%2520The%2520main%2520innovation%2520is%2520learning%2520to%2520mimic%2520the%2520pairwise%250Aaffinity%2520relationship%2520of%2520the%2520image%2520tokens%2520across%2520exemplars.%2520To%2520focus%2520on%2520the%250Acoarse-to-fine%2520hierarchical%2520structures%2520in%2520the%2520Mood%2520Space%252C%2520we%2520compute%2520the%2520top%250Aeigenvector%2520structure%2520from%2520the%2520affinity%2520matrix%2520and%2520define%2520a%2520loss%2520in%2520the%250Aeigenvector%2520space.%2520The%2520resulting%2520Mood%2520Space%2520is%2520locally%2520linear%2520and%2520compact%252C%250Aallowing%2520image-level%2520operations%252C%2520such%2520as%2520object%2520averaging%252C%2520visual%2520analogy%252C%2520and%250Apose%2520transfer%252C%2520to%2520be%2520performed%2520as%2520a%2520simple%2520vector%2520operation%2520in%2520Mood%2520Space.%2520Our%250Alearning%2520is%2520efficient%2520in%2520computation%2520without%2520any%2520fine-tuning%252C%2520needs%2520only%2520a%2520few%250A%25282-20%2529%2520exemplars%252C%2520and%2520takes%2520less%2520than%2520a%2520minute%2520to%2520learn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22I%20Know%20It%20When%20I%20See%20It%22%3A%20Mood%20Spaces%20for%20Connecting%20and%20Expressing%0A%20%20Visual%20Concepts&entry.906535625=Huzheng%20Yang%20and%20Katherine%20Xu%20and%20Michael%20D.%20Grossberg%20and%20Yutong%20Bai%20and%20Jianbo%20Shi&entry.1292438233=%20%20Expressing%20complex%20concepts%20is%20easy%20when%20they%20can%20be%20labeled%20or%20quantified%2C%0Abut%20many%20ideas%20are%20hard%20to%20define%20yet%20instantly%20recognizable.%20We%20propose%20a%20Mood%0ABoard%2C%20where%20users%20convey%20abstract%20concepts%20with%20examples%20that%20hint%20at%20the%0Aintended%20direction%20of%20attribute%20changes.%20We%20compute%20an%20underlying%20Mood%20Space%0Athat%201%29%20factors%20out%20irrelevant%20features%20and%202%29%20finds%20the%20connections%20between%0Aimages%2C%20thus%20bringing%20relevant%20concepts%20closer.%20We%20invent%20a%20fibration%0Acomputation%20to%20compress/decompress%20pre-trained%20features%20into/from%20a%20compact%0Aspace%2C%2050-100x%20smaller.%20The%20main%20innovation%20is%20learning%20to%20mimic%20the%20pairwise%0Aaffinity%20relationship%20of%20the%20image%20tokens%20across%20exemplars.%20To%20focus%20on%20the%0Acoarse-to-fine%20hierarchical%20structures%20in%20the%20Mood%20Space%2C%20we%20compute%20the%20top%0Aeigenvector%20structure%20from%20the%20affinity%20matrix%20and%20define%20a%20loss%20in%20the%0Aeigenvector%20space.%20The%20resulting%20Mood%20Space%20is%20locally%20linear%20and%20compact%2C%0Aallowing%20image-level%20operations%2C%20such%20as%20object%20averaging%2C%20visual%20analogy%2C%20and%0Apose%20transfer%2C%20to%20be%20performed%20as%20a%20simple%20vector%20operation%20in%20Mood%20Space.%20Our%0Alearning%20is%20efficient%20in%20computation%20without%20any%20fine-tuning%2C%20needs%20only%20a%20few%0A%282-20%29%20exemplars%2C%20and%20takes%20less%20than%20a%20minute%20to%20learn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15145v1&entry.124074799=Read"},
{"title": "A triple-branch network for latent fingerprint enhancement guided by\n  orientation fields and minutiae", "author": "Yurun Wang and Zerong Qi and Shujun Fu and Mingzheng Hu", "abstract": "  Latent fingerprint enhancement is a critical step in the process of latent\nfingerprint identification. Existing deep learning-based enhancement methods\nstill fall short of practical application requirements, particularly in\nrestoring low-quality fingerprint regions. Recognizing that different regions\nof latent fingerprints require distinct enhancement strategies, we propose a\nTriple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances\ndifferent regions of the image using tailored strategies. Furthermore, to\nimprove the generalization capability of the network, we integrate orientation\nfield and minutiae-related modules into TBSFNet and introduce a Multi-Level\nFeature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST\ndatasets demonstrate that MLFGNet outperforms existing enhancement algorithms.\n", "link": "http://arxiv.org/abs/2504.15105v1", "date": "2025-04-21", "relevancy": 2.1611, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5614}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5385}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20triple-branch%20network%20for%20latent%20fingerprint%20enhancement%20guided%20by%0A%20%20orientation%20fields%20and%20minutiae&body=Title%3A%20A%20triple-branch%20network%20for%20latent%20fingerprint%20enhancement%20guided%20by%0A%20%20orientation%20fields%20and%20minutiae%0AAuthor%3A%20Yurun%20Wang%20and%20Zerong%20Qi%20and%20Shujun%20Fu%20and%20Mingzheng%20Hu%0AAbstract%3A%20%20%20Latent%20fingerprint%20enhancement%20is%20a%20critical%20step%20in%20the%20process%20of%20latent%0Afingerprint%20identification.%20Existing%20deep%20learning-based%20enhancement%20methods%0Astill%20fall%20short%20of%20practical%20application%20requirements%2C%20particularly%20in%0Arestoring%20low-quality%20fingerprint%20regions.%20Recognizing%20that%20different%20regions%0Aof%20latent%20fingerprints%20require%20distinct%20enhancement%20strategies%2C%20we%20propose%20a%0ATriple%20Branch%20Spatial%20Fusion%20Network%20%28TBSFNet%29%2C%20which%20simultaneously%20enhances%0Adifferent%20regions%20of%20the%20image%20using%20tailored%20strategies.%20Furthermore%2C%20to%0Aimprove%20the%20generalization%20capability%20of%20the%20network%2C%20we%20integrate%20orientation%0Afield%20and%20minutiae-related%20modules%20into%20TBSFNet%20and%20introduce%20a%20Multi-Level%0AFeature%20Guidance%20Network%20%28MLFGNet%29.%20Experimental%20results%20on%20the%20MOLF%20and%20MUST%0Adatasets%20demonstrate%20that%20MLFGNet%20outperforms%20existing%20enhancement%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520triple-branch%2520network%2520for%2520latent%2520fingerprint%2520enhancement%2520guided%2520by%250A%2520%2520orientation%2520fields%2520and%2520minutiae%26entry.906535625%3DYurun%2520Wang%2520and%2520Zerong%2520Qi%2520and%2520Shujun%2520Fu%2520and%2520Mingzheng%2520Hu%26entry.1292438233%3D%2520%2520Latent%2520fingerprint%2520enhancement%2520is%2520a%2520critical%2520step%2520in%2520the%2520process%2520of%2520latent%250Afingerprint%2520identification.%2520Existing%2520deep%2520learning-based%2520enhancement%2520methods%250Astill%2520fall%2520short%2520of%2520practical%2520application%2520requirements%252C%2520particularly%2520in%250Arestoring%2520low-quality%2520fingerprint%2520regions.%2520Recognizing%2520that%2520different%2520regions%250Aof%2520latent%2520fingerprints%2520require%2520distinct%2520enhancement%2520strategies%252C%2520we%2520propose%2520a%250ATriple%2520Branch%2520Spatial%2520Fusion%2520Network%2520%2528TBSFNet%2529%252C%2520which%2520simultaneously%2520enhances%250Adifferent%2520regions%2520of%2520the%2520image%2520using%2520tailored%2520strategies.%2520Furthermore%252C%2520to%250Aimprove%2520the%2520generalization%2520capability%2520of%2520the%2520network%252C%2520we%2520integrate%2520orientation%250Afield%2520and%2520minutiae-related%2520modules%2520into%2520TBSFNet%2520and%2520introduce%2520a%2520Multi-Level%250AFeature%2520Guidance%2520Network%2520%2528MLFGNet%2529.%2520Experimental%2520results%2520on%2520the%2520MOLF%2520and%2520MUST%250Adatasets%2520demonstrate%2520that%2520MLFGNet%2520outperforms%2520existing%2520enhancement%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20triple-branch%20network%20for%20latent%20fingerprint%20enhancement%20guided%20by%0A%20%20orientation%20fields%20and%20minutiae&entry.906535625=Yurun%20Wang%20and%20Zerong%20Qi%20and%20Shujun%20Fu%20and%20Mingzheng%20Hu&entry.1292438233=%20%20Latent%20fingerprint%20enhancement%20is%20a%20critical%20step%20in%20the%20process%20of%20latent%0Afingerprint%20identification.%20Existing%20deep%20learning-based%20enhancement%20methods%0Astill%20fall%20short%20of%20practical%20application%20requirements%2C%20particularly%20in%0Arestoring%20low-quality%20fingerprint%20regions.%20Recognizing%20that%20different%20regions%0Aof%20latent%20fingerprints%20require%20distinct%20enhancement%20strategies%2C%20we%20propose%20a%0ATriple%20Branch%20Spatial%20Fusion%20Network%20%28TBSFNet%29%2C%20which%20simultaneously%20enhances%0Adifferent%20regions%20of%20the%20image%20using%20tailored%20strategies.%20Furthermore%2C%20to%0Aimprove%20the%20generalization%20capability%20of%20the%20network%2C%20we%20integrate%20orientation%0Afield%20and%20minutiae-related%20modules%20into%20TBSFNet%20and%20introduce%20a%20Multi-Level%0AFeature%20Guidance%20Network%20%28MLFGNet%29.%20Experimental%20results%20on%20the%20MOLF%20and%20MUST%0Adatasets%20demonstrate%20that%20MLFGNet%20outperforms%20existing%20enhancement%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15105v1&entry.124074799=Read"},
{"title": "Automated Measurement of Eczema Severity with Self-Supervised Learning", "author": "Neelesh Kumar and Oya Aran", "abstract": "  Automated diagnosis of eczema using images acquired from digital camera can\nenable individuals to self-monitor their recovery. The process entails first\nsegmenting out the eczema region from the image and then measuring the severity\nof eczema in the segmented region. The state-of-the-art methods for automated\neczema diagnosis rely on deep neural networks such as convolutional neural\nnetwork (CNN) and have shown impressive performance in accurately measuring the\nseverity of eczema. However, these methods require massive volume of annotated\ndata to train which can be hard to obtain. In this paper, we propose a\nself-supervised learning framework for automated eczema diagnosis under limited\ntraining data regime. Our framework consists of two stages: i) Segmentation,\nwhere we use an in-context learning based algorithm called SegGPT for few-shot\nsegmentation of eczema region from the image; ii) Feature extraction and\nclassification, where we extract DINO features from the segmented regions and\nfeed it to a multi-layered perceptron (MLP) for 4-class classification of\neczema severity. When evaluated on a dataset of annotated \"in-the-wild\" eczema\nimages, we show that our method outperforms (Weighted F1: 0.67 $\\pm$ 0.01) the\nstate-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted\nF1: 0.44 $\\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\\pm$ 0.22). Our\nresults show that self-supervised learning can be a viable solution for\nautomated skin diagnosis where labeled data is scarce.\n", "link": "http://arxiv.org/abs/2504.15193v1", "date": "2025-04-21", "relevancy": 2.1591, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5755}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.528}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Measurement%20of%20Eczema%20Severity%20with%20Self-Supervised%20Learning&body=Title%3A%20Automated%20Measurement%20of%20Eczema%20Severity%20with%20Self-Supervised%20Learning%0AAuthor%3A%20Neelesh%20Kumar%20and%20Oya%20Aran%0AAbstract%3A%20%20%20Automated%20diagnosis%20of%20eczema%20using%20images%20acquired%20from%20digital%20camera%20can%0Aenable%20individuals%20to%20self-monitor%20their%20recovery.%20The%20process%20entails%20first%0Asegmenting%20out%20the%20eczema%20region%20from%20the%20image%20and%20then%20measuring%20the%20severity%0Aof%20eczema%20in%20the%20segmented%20region.%20The%20state-of-the-art%20methods%20for%20automated%0Aeczema%20diagnosis%20rely%20on%20deep%20neural%20networks%20such%20as%20convolutional%20neural%0Anetwork%20%28CNN%29%20and%20have%20shown%20impressive%20performance%20in%20accurately%20measuring%20the%0Aseverity%20of%20eczema.%20However%2C%20these%20methods%20require%20massive%20volume%20of%20annotated%0Adata%20to%20train%20which%20can%20be%20hard%20to%20obtain.%20In%20this%20paper%2C%20we%20propose%20a%0Aself-supervised%20learning%20framework%20for%20automated%20eczema%20diagnosis%20under%20limited%0Atraining%20data%20regime.%20Our%20framework%20consists%20of%20two%20stages%3A%20i%29%20Segmentation%2C%0Awhere%20we%20use%20an%20in-context%20learning%20based%20algorithm%20called%20SegGPT%20for%20few-shot%0Asegmentation%20of%20eczema%20region%20from%20the%20image%3B%20ii%29%20Feature%20extraction%20and%0Aclassification%2C%20where%20we%20extract%20DINO%20features%20from%20the%20segmented%20regions%20and%0Afeed%20it%20to%20a%20multi-layered%20perceptron%20%28MLP%29%20for%204-class%20classification%20of%0Aeczema%20severity.%20When%20evaluated%20on%20a%20dataset%20of%20annotated%20%22in-the-wild%22%20eczema%0Aimages%2C%20we%20show%20that%20our%20method%20outperforms%20%28Weighted%20F1%3A%200.67%20%24%5Cpm%24%200.01%29%20the%0Astate-of-the-art%20deep%20learning%20methods%20such%20as%20finetuned%20Resnet-18%20%28Weighted%0AF1%3A%200.44%20%24%5Cpm%24%200.16%29%20and%20Vision%20Transformer%20%28Weighted%20F1%3A%200.40%20%24%5Cpm%24%200.22%29.%20Our%0Aresults%20show%20that%20self-supervised%20learning%20can%20be%20a%20viable%20solution%20for%0Aautomated%20skin%20diagnosis%20where%20labeled%20data%20is%20scarce.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15193v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Measurement%2520of%2520Eczema%2520Severity%2520with%2520Self-Supervised%2520Learning%26entry.906535625%3DNeelesh%2520Kumar%2520and%2520Oya%2520Aran%26entry.1292438233%3D%2520%2520Automated%2520diagnosis%2520of%2520eczema%2520using%2520images%2520acquired%2520from%2520digital%2520camera%2520can%250Aenable%2520individuals%2520to%2520self-monitor%2520their%2520recovery.%2520The%2520process%2520entails%2520first%250Asegmenting%2520out%2520the%2520eczema%2520region%2520from%2520the%2520image%2520and%2520then%2520measuring%2520the%2520severity%250Aof%2520eczema%2520in%2520the%2520segmented%2520region.%2520The%2520state-of-the-art%2520methods%2520for%2520automated%250Aeczema%2520diagnosis%2520rely%2520on%2520deep%2520neural%2520networks%2520such%2520as%2520convolutional%2520neural%250Anetwork%2520%2528CNN%2529%2520and%2520have%2520shown%2520impressive%2520performance%2520in%2520accurately%2520measuring%2520the%250Aseverity%2520of%2520eczema.%2520However%252C%2520these%2520methods%2520require%2520massive%2520volume%2520of%2520annotated%250Adata%2520to%2520train%2520which%2520can%2520be%2520hard%2520to%2520obtain.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Aself-supervised%2520learning%2520framework%2520for%2520automated%2520eczema%2520diagnosis%2520under%2520limited%250Atraining%2520data%2520regime.%2520Our%2520framework%2520consists%2520of%2520two%2520stages%253A%2520i%2529%2520Segmentation%252C%250Awhere%2520we%2520use%2520an%2520in-context%2520learning%2520based%2520algorithm%2520called%2520SegGPT%2520for%2520few-shot%250Asegmentation%2520of%2520eczema%2520region%2520from%2520the%2520image%253B%2520ii%2529%2520Feature%2520extraction%2520and%250Aclassification%252C%2520where%2520we%2520extract%2520DINO%2520features%2520from%2520the%2520segmented%2520regions%2520and%250Afeed%2520it%2520to%2520a%2520multi-layered%2520perceptron%2520%2528MLP%2529%2520for%25204-class%2520classification%2520of%250Aeczema%2520severity.%2520When%2520evaluated%2520on%2520a%2520dataset%2520of%2520annotated%2520%2522in-the-wild%2522%2520eczema%250Aimages%252C%2520we%2520show%2520that%2520our%2520method%2520outperforms%2520%2528Weighted%2520F1%253A%25200.67%2520%2524%255Cpm%2524%25200.01%2529%2520the%250Astate-of-the-art%2520deep%2520learning%2520methods%2520such%2520as%2520finetuned%2520Resnet-18%2520%2528Weighted%250AF1%253A%25200.44%2520%2524%255Cpm%2524%25200.16%2529%2520and%2520Vision%2520Transformer%2520%2528Weighted%2520F1%253A%25200.40%2520%2524%255Cpm%2524%25200.22%2529.%2520Our%250Aresults%2520show%2520that%2520self-supervised%2520learning%2520can%2520be%2520a%2520viable%2520solution%2520for%250Aautomated%2520skin%2520diagnosis%2520where%2520labeled%2520data%2520is%2520scarce.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15193v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Measurement%20of%20Eczema%20Severity%20with%20Self-Supervised%20Learning&entry.906535625=Neelesh%20Kumar%20and%20Oya%20Aran&entry.1292438233=%20%20Automated%20diagnosis%20of%20eczema%20using%20images%20acquired%20from%20digital%20camera%20can%0Aenable%20individuals%20to%20self-monitor%20their%20recovery.%20The%20process%20entails%20first%0Asegmenting%20out%20the%20eczema%20region%20from%20the%20image%20and%20then%20measuring%20the%20severity%0Aof%20eczema%20in%20the%20segmented%20region.%20The%20state-of-the-art%20methods%20for%20automated%0Aeczema%20diagnosis%20rely%20on%20deep%20neural%20networks%20such%20as%20convolutional%20neural%0Anetwork%20%28CNN%29%20and%20have%20shown%20impressive%20performance%20in%20accurately%20measuring%20the%0Aseverity%20of%20eczema.%20However%2C%20these%20methods%20require%20massive%20volume%20of%20annotated%0Adata%20to%20train%20which%20can%20be%20hard%20to%20obtain.%20In%20this%20paper%2C%20we%20propose%20a%0Aself-supervised%20learning%20framework%20for%20automated%20eczema%20diagnosis%20under%20limited%0Atraining%20data%20regime.%20Our%20framework%20consists%20of%20two%20stages%3A%20i%29%20Segmentation%2C%0Awhere%20we%20use%20an%20in-context%20learning%20based%20algorithm%20called%20SegGPT%20for%20few-shot%0Asegmentation%20of%20eczema%20region%20from%20the%20image%3B%20ii%29%20Feature%20extraction%20and%0Aclassification%2C%20where%20we%20extract%20DINO%20features%20from%20the%20segmented%20regions%20and%0Afeed%20it%20to%20a%20multi-layered%20perceptron%20%28MLP%29%20for%204-class%20classification%20of%0Aeczema%20severity.%20When%20evaluated%20on%20a%20dataset%20of%20annotated%20%22in-the-wild%22%20eczema%0Aimages%2C%20we%20show%20that%20our%20method%20outperforms%20%28Weighted%20F1%3A%200.67%20%24%5Cpm%24%200.01%29%20the%0Astate-of-the-art%20deep%20learning%20methods%20such%20as%20finetuned%20Resnet-18%20%28Weighted%0AF1%3A%200.44%20%24%5Cpm%24%200.16%29%20and%20Vision%20Transformer%20%28Weighted%20F1%3A%200.40%20%24%5Cpm%24%200.22%29.%20Our%0Aresults%20show%20that%20self-supervised%20learning%20can%20be%20a%20viable%20solution%20for%0Aautomated%20skin%20diagnosis%20where%20labeled%20data%20is%20scarce.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15193v1&entry.124074799=Read"},
{"title": "Potential Societal Biases of ChatGPT in Higher Education: A Scoping\n  Review", "author": "Ming Li and Ariunaa Enkhtur and Beverley Anne Yamamoto and Fei Cheng and Lilan Chen", "abstract": "  Purpose:Generative Artificial Intelligence (GAI) models, such as ChatGPT, may\ninherit or amplify societal biases due to their training on extensive datasets.\nWith the increasing usage of GAI by students, faculty, and staff in higher\neducation institutions (HEIs), it is urgent to examine the ethical issues and\npotential biases associated with these technologies.\nDesign/Approach/Methods:This scoping review aims to elucidate how biases\nrelated to GAI in HEIs have been researched and discussed in recent academic\npublications. We categorized the potential societal biases that GAI might cause\nin the field of higher education. Our review includes articles written in\nEnglish, Chinese, and Japanese across four main databases, focusing on GAI\nusage in higher education and bias. Findings:Our findings reveal that while\nthere is meaningful scholarly discussion around bias and discrimination\nconcerning LLMs in the AI field, most articles addressing higher education\napproach the issue superficially. Few articles identify specific types of bias\nunder different circumstances, and there is a notable lack of empirical\nresearch. Most papers in our review focus primarily on educational and research\nfields related to medicine and engineering, with some addressing English\neducation. However, there is almost no discussion regarding the humanities and\nsocial sciences. Additionally, a significant portion of the current discourse\nis in English and primarily addresses English-speaking contexts.\nOriginality/Value:To the best of our knowledge, our study is the first to\nsummarize the potential societal biases in higher education. This review\nhighlights the need for more in-depth studies and empirical work to understand\nthe specific biases that GAI might introduce or amplify in educational\nsettings, guiding the development of more ethical AI applications in higher\neducation.\n", "link": "http://arxiv.org/abs/2311.14381v4", "date": "2025-04-21", "relevancy": 2.1582, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4485}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4426}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Potential%20Societal%20Biases%20of%20ChatGPT%20in%20Higher%20Education%3A%20A%20Scoping%0A%20%20Review&body=Title%3A%20Potential%20Societal%20Biases%20of%20ChatGPT%20in%20Higher%20Education%3A%20A%20Scoping%0A%20%20Review%0AAuthor%3A%20Ming%20Li%20and%20Ariunaa%20Enkhtur%20and%20Beverley%20Anne%20Yamamoto%20and%20Fei%20Cheng%20and%20Lilan%20Chen%0AAbstract%3A%20%20%20Purpose%3AGenerative%20Artificial%20Intelligence%20%28GAI%29%20models%2C%20such%20as%20ChatGPT%2C%20may%0Ainherit%20or%20amplify%20societal%20biases%20due%20to%20their%20training%20on%20extensive%20datasets.%0AWith%20the%20increasing%20usage%20of%20GAI%20by%20students%2C%20faculty%2C%20and%20staff%20in%20higher%0Aeducation%20institutions%20%28HEIs%29%2C%20it%20is%20urgent%20to%20examine%20the%20ethical%20issues%20and%0Apotential%20biases%20associated%20with%20these%20technologies.%0ADesign/Approach/Methods%3AThis%20scoping%20review%20aims%20to%20elucidate%20how%20biases%0Arelated%20to%20GAI%20in%20HEIs%20have%20been%20researched%20and%20discussed%20in%20recent%20academic%0Apublications.%20We%20categorized%20the%20potential%20societal%20biases%20that%20GAI%20might%20cause%0Ain%20the%20field%20of%20higher%20education.%20Our%20review%20includes%20articles%20written%20in%0AEnglish%2C%20Chinese%2C%20and%20Japanese%20across%20four%20main%20databases%2C%20focusing%20on%20GAI%0Ausage%20in%20higher%20education%20and%20bias.%20Findings%3AOur%20findings%20reveal%20that%20while%0Athere%20is%20meaningful%20scholarly%20discussion%20around%20bias%20and%20discrimination%0Aconcerning%20LLMs%20in%20the%20AI%20field%2C%20most%20articles%20addressing%20higher%20education%0Aapproach%20the%20issue%20superficially.%20Few%20articles%20identify%20specific%20types%20of%20bias%0Aunder%20different%20circumstances%2C%20and%20there%20is%20a%20notable%20lack%20of%20empirical%0Aresearch.%20Most%20papers%20in%20our%20review%20focus%20primarily%20on%20educational%20and%20research%0Afields%20related%20to%20medicine%20and%20engineering%2C%20with%20some%20addressing%20English%0Aeducation.%20However%2C%20there%20is%20almost%20no%20discussion%20regarding%20the%20humanities%20and%0Asocial%20sciences.%20Additionally%2C%20a%20significant%20portion%20of%20the%20current%20discourse%0Ais%20in%20English%20and%20primarily%20addresses%20English-speaking%20contexts.%0AOriginality/Value%3ATo%20the%20best%20of%20our%20knowledge%2C%20our%20study%20is%20the%20first%20to%0Asummarize%20the%20potential%20societal%20biases%20in%20higher%20education.%20This%20review%0Ahighlights%20the%20need%20for%20more%20in-depth%20studies%20and%20empirical%20work%20to%20understand%0Athe%20specific%20biases%20that%20GAI%20might%20introduce%20or%20amplify%20in%20educational%0Asettings%2C%20guiding%20the%20development%20of%20more%20ethical%20AI%20applications%20in%20higher%0Aeducation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14381v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPotential%2520Societal%2520Biases%2520of%2520ChatGPT%2520in%2520Higher%2520Education%253A%2520A%2520Scoping%250A%2520%2520Review%26entry.906535625%3DMing%2520Li%2520and%2520Ariunaa%2520Enkhtur%2520and%2520Beverley%2520Anne%2520Yamamoto%2520and%2520Fei%2520Cheng%2520and%2520Lilan%2520Chen%26entry.1292438233%3D%2520%2520Purpose%253AGenerative%2520Artificial%2520Intelligence%2520%2528GAI%2529%2520models%252C%2520such%2520as%2520ChatGPT%252C%2520may%250Ainherit%2520or%2520amplify%2520societal%2520biases%2520due%2520to%2520their%2520training%2520on%2520extensive%2520datasets.%250AWith%2520the%2520increasing%2520usage%2520of%2520GAI%2520by%2520students%252C%2520faculty%252C%2520and%2520staff%2520in%2520higher%250Aeducation%2520institutions%2520%2528HEIs%2529%252C%2520it%2520is%2520urgent%2520to%2520examine%2520the%2520ethical%2520issues%2520and%250Apotential%2520biases%2520associated%2520with%2520these%2520technologies.%250ADesign/Approach/Methods%253AThis%2520scoping%2520review%2520aims%2520to%2520elucidate%2520how%2520biases%250Arelated%2520to%2520GAI%2520in%2520HEIs%2520have%2520been%2520researched%2520and%2520discussed%2520in%2520recent%2520academic%250Apublications.%2520We%2520categorized%2520the%2520potential%2520societal%2520biases%2520that%2520GAI%2520might%2520cause%250Ain%2520the%2520field%2520of%2520higher%2520education.%2520Our%2520review%2520includes%2520articles%2520written%2520in%250AEnglish%252C%2520Chinese%252C%2520and%2520Japanese%2520across%2520four%2520main%2520databases%252C%2520focusing%2520on%2520GAI%250Ausage%2520in%2520higher%2520education%2520and%2520bias.%2520Findings%253AOur%2520findings%2520reveal%2520that%2520while%250Athere%2520is%2520meaningful%2520scholarly%2520discussion%2520around%2520bias%2520and%2520discrimination%250Aconcerning%2520LLMs%2520in%2520the%2520AI%2520field%252C%2520most%2520articles%2520addressing%2520higher%2520education%250Aapproach%2520the%2520issue%2520superficially.%2520Few%2520articles%2520identify%2520specific%2520types%2520of%2520bias%250Aunder%2520different%2520circumstances%252C%2520and%2520there%2520is%2520a%2520notable%2520lack%2520of%2520empirical%250Aresearch.%2520Most%2520papers%2520in%2520our%2520review%2520focus%2520primarily%2520on%2520educational%2520and%2520research%250Afields%2520related%2520to%2520medicine%2520and%2520engineering%252C%2520with%2520some%2520addressing%2520English%250Aeducation.%2520However%252C%2520there%2520is%2520almost%2520no%2520discussion%2520regarding%2520the%2520humanities%2520and%250Asocial%2520sciences.%2520Additionally%252C%2520a%2520significant%2520portion%2520of%2520the%2520current%2520discourse%250Ais%2520in%2520English%2520and%2520primarily%2520addresses%2520English-speaking%2520contexts.%250AOriginality/Value%253ATo%2520the%2520best%2520of%2520our%2520knowledge%252C%2520our%2520study%2520is%2520the%2520first%2520to%250Asummarize%2520the%2520potential%2520societal%2520biases%2520in%2520higher%2520education.%2520This%2520review%250Ahighlights%2520the%2520need%2520for%2520more%2520in-depth%2520studies%2520and%2520empirical%2520work%2520to%2520understand%250Athe%2520specific%2520biases%2520that%2520GAI%2520might%2520introduce%2520or%2520amplify%2520in%2520educational%250Asettings%252C%2520guiding%2520the%2520development%2520of%2520more%2520ethical%2520AI%2520applications%2520in%2520higher%250Aeducation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14381v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Potential%20Societal%20Biases%20of%20ChatGPT%20in%20Higher%20Education%3A%20A%20Scoping%0A%20%20Review&entry.906535625=Ming%20Li%20and%20Ariunaa%20Enkhtur%20and%20Beverley%20Anne%20Yamamoto%20and%20Fei%20Cheng%20and%20Lilan%20Chen&entry.1292438233=%20%20Purpose%3AGenerative%20Artificial%20Intelligence%20%28GAI%29%20models%2C%20such%20as%20ChatGPT%2C%20may%0Ainherit%20or%20amplify%20societal%20biases%20due%20to%20their%20training%20on%20extensive%20datasets.%0AWith%20the%20increasing%20usage%20of%20GAI%20by%20students%2C%20faculty%2C%20and%20staff%20in%20higher%0Aeducation%20institutions%20%28HEIs%29%2C%20it%20is%20urgent%20to%20examine%20the%20ethical%20issues%20and%0Apotential%20biases%20associated%20with%20these%20technologies.%0ADesign/Approach/Methods%3AThis%20scoping%20review%20aims%20to%20elucidate%20how%20biases%0Arelated%20to%20GAI%20in%20HEIs%20have%20been%20researched%20and%20discussed%20in%20recent%20academic%0Apublications.%20We%20categorized%20the%20potential%20societal%20biases%20that%20GAI%20might%20cause%0Ain%20the%20field%20of%20higher%20education.%20Our%20review%20includes%20articles%20written%20in%0AEnglish%2C%20Chinese%2C%20and%20Japanese%20across%20four%20main%20databases%2C%20focusing%20on%20GAI%0Ausage%20in%20higher%20education%20and%20bias.%20Findings%3AOur%20findings%20reveal%20that%20while%0Athere%20is%20meaningful%20scholarly%20discussion%20around%20bias%20and%20discrimination%0Aconcerning%20LLMs%20in%20the%20AI%20field%2C%20most%20articles%20addressing%20higher%20education%0Aapproach%20the%20issue%20superficially.%20Few%20articles%20identify%20specific%20types%20of%20bias%0Aunder%20different%20circumstances%2C%20and%20there%20is%20a%20notable%20lack%20of%20empirical%0Aresearch.%20Most%20papers%20in%20our%20review%20focus%20primarily%20on%20educational%20and%20research%0Afields%20related%20to%20medicine%20and%20engineering%2C%20with%20some%20addressing%20English%0Aeducation.%20However%2C%20there%20is%20almost%20no%20discussion%20regarding%20the%20humanities%20and%0Asocial%20sciences.%20Additionally%2C%20a%20significant%20portion%20of%20the%20current%20discourse%0Ais%20in%20English%20and%20primarily%20addresses%20English-speaking%20contexts.%0AOriginality/Value%3ATo%20the%20best%20of%20our%20knowledge%2C%20our%20study%20is%20the%20first%20to%0Asummarize%20the%20potential%20societal%20biases%20in%20higher%20education.%20This%20review%0Ahighlights%20the%20need%20for%20more%20in-depth%20studies%20and%20empirical%20work%20to%20understand%0Athe%20specific%20biases%20that%20GAI%20might%20introduce%20or%20amplify%20in%20educational%0Asettings%2C%20guiding%20the%20development%20of%20more%20ethical%20AI%20applications%20in%20higher%0Aeducation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14381v4&entry.124074799=Read"},
{"title": "Text-to-Decision Agent: Learning Generalist Policies from Natural\n  Language Supervision", "author": "Shilin Zhang and Zican Hu and Wenhao Wu and Xinyi Xie and Jianxiang Tang and Chunlin Chen and Daoyi Dong and Yu Cheng and Zhenhong Sun and Zhi Wang", "abstract": "  RL systems usually tackle generalization by inferring task beliefs from\nhigh-quality samples or warmup explorations. The restricted form limits their\ngenerality and usability since these supervision signals are expensive and even\ninfeasible to acquire in advance for unseen tasks. Learning directly from the\nraw text about decision tasks is a promising alternative to leverage a much\nbroader source of supervision. In the paper, we propose Text-to-Decision Agent\n(T2DA), a simple and scalable framework that supervises generalist policy\nlearning with natural language. We first introduce a generalized world model to\nencode multi-task decision data into a dynamics-aware embedding space. Then,\ninspired by CLIP, we predict which textual description goes with which decision\nembedding, effectively bridging their semantic gap via contrastive\nlanguage-decision pre-training and aligning the text embeddings to comprehend\nthe environment dynamics. After training the text-conditioned generalist\npolicy, the agent can directly realize zero-shot text-to-decision generation in\nresponse to language instructions. Comprehensive experiments on MuJoCo and\nMeta-World benchmarks show that T2DA facilitates high-capacity zero-shot\ngeneralization and outperforms various types of baselines.\n", "link": "http://arxiv.org/abs/2504.15046v1", "date": "2025-04-21", "relevancy": 2.1565, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5537}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5355}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-Decision%20Agent%3A%20Learning%20Generalist%20Policies%20from%20Natural%0A%20%20Language%20Supervision&body=Title%3A%20Text-to-Decision%20Agent%3A%20Learning%20Generalist%20Policies%20from%20Natural%0A%20%20Language%20Supervision%0AAuthor%3A%20Shilin%20Zhang%20and%20Zican%20Hu%20and%20Wenhao%20Wu%20and%20Xinyi%20Xie%20and%20Jianxiang%20Tang%20and%20Chunlin%20Chen%20and%20Daoyi%20Dong%20and%20Yu%20Cheng%20and%20Zhenhong%20Sun%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20RL%20systems%20usually%20tackle%20generalization%20by%20inferring%20task%20beliefs%20from%0Ahigh-quality%20samples%20or%20warmup%20explorations.%20The%20restricted%20form%20limits%20their%0Agenerality%20and%20usability%20since%20these%20supervision%20signals%20are%20expensive%20and%20even%0Ainfeasible%20to%20acquire%20in%20advance%20for%20unseen%20tasks.%20Learning%20directly%20from%20the%0Araw%20text%20about%20decision%20tasks%20is%20a%20promising%20alternative%20to%20leverage%20a%20much%0Abroader%20source%20of%20supervision.%20In%20the%20paper%2C%20we%20propose%20Text-to-Decision%20Agent%0A%28T2DA%29%2C%20a%20simple%20and%20scalable%20framework%20that%20supervises%20generalist%20policy%0Alearning%20with%20natural%20language.%20We%20first%20introduce%20a%20generalized%20world%20model%20to%0Aencode%20multi-task%20decision%20data%20into%20a%20dynamics-aware%20embedding%20space.%20Then%2C%0Ainspired%20by%20CLIP%2C%20we%20predict%20which%20textual%20description%20goes%20with%20which%20decision%0Aembedding%2C%20effectively%20bridging%20their%20semantic%20gap%20via%20contrastive%0Alanguage-decision%20pre-training%20and%20aligning%20the%20text%20embeddings%20to%20comprehend%0Athe%20environment%20dynamics.%20After%20training%20the%20text-conditioned%20generalist%0Apolicy%2C%20the%20agent%20can%20directly%20realize%20zero-shot%20text-to-decision%20generation%20in%0Aresponse%20to%20language%20instructions.%20Comprehensive%20experiments%20on%20MuJoCo%20and%0AMeta-World%20benchmarks%20show%20that%20T2DA%20facilitates%20high-capacity%20zero-shot%0Ageneralization%20and%20outperforms%20various%20types%20of%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-Decision%2520Agent%253A%2520Learning%2520Generalist%2520Policies%2520from%2520Natural%250A%2520%2520Language%2520Supervision%26entry.906535625%3DShilin%2520Zhang%2520and%2520Zican%2520Hu%2520and%2520Wenhao%2520Wu%2520and%2520Xinyi%2520Xie%2520and%2520Jianxiang%2520Tang%2520and%2520Chunlin%2520Chen%2520and%2520Daoyi%2520Dong%2520and%2520Yu%2520Cheng%2520and%2520Zhenhong%2520Sun%2520and%2520Zhi%2520Wang%26entry.1292438233%3D%2520%2520RL%2520systems%2520usually%2520tackle%2520generalization%2520by%2520inferring%2520task%2520beliefs%2520from%250Ahigh-quality%2520samples%2520or%2520warmup%2520explorations.%2520The%2520restricted%2520form%2520limits%2520their%250Agenerality%2520and%2520usability%2520since%2520these%2520supervision%2520signals%2520are%2520expensive%2520and%2520even%250Ainfeasible%2520to%2520acquire%2520in%2520advance%2520for%2520unseen%2520tasks.%2520Learning%2520directly%2520from%2520the%250Araw%2520text%2520about%2520decision%2520tasks%2520is%2520a%2520promising%2520alternative%2520to%2520leverage%2520a%2520much%250Abroader%2520source%2520of%2520supervision.%2520In%2520the%2520paper%252C%2520we%2520propose%2520Text-to-Decision%2520Agent%250A%2528T2DA%2529%252C%2520a%2520simple%2520and%2520scalable%2520framework%2520that%2520supervises%2520generalist%2520policy%250Alearning%2520with%2520natural%2520language.%2520We%2520first%2520introduce%2520a%2520generalized%2520world%2520model%2520to%250Aencode%2520multi-task%2520decision%2520data%2520into%2520a%2520dynamics-aware%2520embedding%2520space.%2520Then%252C%250Ainspired%2520by%2520CLIP%252C%2520we%2520predict%2520which%2520textual%2520description%2520goes%2520with%2520which%2520decision%250Aembedding%252C%2520effectively%2520bridging%2520their%2520semantic%2520gap%2520via%2520contrastive%250Alanguage-decision%2520pre-training%2520and%2520aligning%2520the%2520text%2520embeddings%2520to%2520comprehend%250Athe%2520environment%2520dynamics.%2520After%2520training%2520the%2520text-conditioned%2520generalist%250Apolicy%252C%2520the%2520agent%2520can%2520directly%2520realize%2520zero-shot%2520text-to-decision%2520generation%2520in%250Aresponse%2520to%2520language%2520instructions.%2520Comprehensive%2520experiments%2520on%2520MuJoCo%2520and%250AMeta-World%2520benchmarks%2520show%2520that%2520T2DA%2520facilitates%2520high-capacity%2520zero-shot%250Ageneralization%2520and%2520outperforms%2520various%2520types%2520of%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-Decision%20Agent%3A%20Learning%20Generalist%20Policies%20from%20Natural%0A%20%20Language%20Supervision&entry.906535625=Shilin%20Zhang%20and%20Zican%20Hu%20and%20Wenhao%20Wu%20and%20Xinyi%20Xie%20and%20Jianxiang%20Tang%20and%20Chunlin%20Chen%20and%20Daoyi%20Dong%20and%20Yu%20Cheng%20and%20Zhenhong%20Sun%20and%20Zhi%20Wang&entry.1292438233=%20%20RL%20systems%20usually%20tackle%20generalization%20by%20inferring%20task%20beliefs%20from%0Ahigh-quality%20samples%20or%20warmup%20explorations.%20The%20restricted%20form%20limits%20their%0Agenerality%20and%20usability%20since%20these%20supervision%20signals%20are%20expensive%20and%20even%0Ainfeasible%20to%20acquire%20in%20advance%20for%20unseen%20tasks.%20Learning%20directly%20from%20the%0Araw%20text%20about%20decision%20tasks%20is%20a%20promising%20alternative%20to%20leverage%20a%20much%0Abroader%20source%20of%20supervision.%20In%20the%20paper%2C%20we%20propose%20Text-to-Decision%20Agent%0A%28T2DA%29%2C%20a%20simple%20and%20scalable%20framework%20that%20supervises%20generalist%20policy%0Alearning%20with%20natural%20language.%20We%20first%20introduce%20a%20generalized%20world%20model%20to%0Aencode%20multi-task%20decision%20data%20into%20a%20dynamics-aware%20embedding%20space.%20Then%2C%0Ainspired%20by%20CLIP%2C%20we%20predict%20which%20textual%20description%20goes%20with%20which%20decision%0Aembedding%2C%20effectively%20bridging%20their%20semantic%20gap%20via%20contrastive%0Alanguage-decision%20pre-training%20and%20aligning%20the%20text%20embeddings%20to%20comprehend%0Athe%20environment%20dynamics.%20After%20training%20the%20text-conditioned%20generalist%0Apolicy%2C%20the%20agent%20can%20directly%20realize%20zero-shot%20text-to-decision%20generation%20in%0Aresponse%20to%20language%20instructions.%20Comprehensive%20experiments%20on%20MuJoCo%20and%0AMeta-World%20benchmarks%20show%20that%20T2DA%20facilitates%20high-capacity%20zero-shot%0Ageneralization%20and%20outperforms%20various%20types%20of%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15046v1&entry.124074799=Read"},
{"title": "Rethinking the Potential of Multimodality in Collaborative Problem\n  Solving Diagnosis with Large Language Models", "author": "K. Wong and B. Wu and S. Bulathwela and M. Cukurova", "abstract": "  Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts.\n", "link": "http://arxiv.org/abs/2504.15093v1", "date": "2025-04-21", "relevancy": 2.1376, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5365}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5341}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20Potential%20of%20Multimodality%20in%20Collaborative%20Problem%0A%20%20Solving%20Diagnosis%20with%20Large%20Language%20Models&body=Title%3A%20Rethinking%20the%20Potential%20of%20Multimodality%20in%20Collaborative%20Problem%0A%20%20Solving%20Diagnosis%20with%20Large%20Language%20Models%0AAuthor%3A%20K.%20Wong%20and%20B.%20Wu%20and%20S.%20Bulathwela%20and%20M.%20Cukurova%0AAbstract%3A%20%20%20Detecting%20collaborative%20and%20problem-solving%20behaviours%20from%20digital%20traces%20to%0Ainterpret%20students%27%20collaborative%20problem%20solving%20%28CPS%29%20competency%20is%20a%0Along-term%20goal%20in%20the%20Artificial%20Intelligence%20in%20Education%20%28AIEd%29%20field.%0AAlthough%20multimodal%20data%20and%20advanced%20models%20are%20argued%20to%20have%20the%20potential%0Ato%20detect%20complex%20CPS%20behaviours%2C%20empirical%20evidence%20on%20their%20value%20remains%0Alimited%20with%20some%20contrasting%20evidence.%20In%20this%20study%2C%20we%20investigated%20the%0Apotential%20of%20multimodal%20data%20to%20improve%20model%20performance%20in%20diagnosing%2078%0Asecondary%20school%20students%27%20CPS%20subskills%20and%20indicators%20in%20authentic%0Aeducational%20settings.%20In%20particular%2C%20text%20embeddings%20from%20verbal%20data%20and%0Aacoustic%20embeddings%20from%20audio%20data%20were%20used%20in%20a%20multimodal%20classification%0Amodel%20for%20CPS%20diagnosis.%20Both%20unimodal%20and%20multimodal%20transformer-based%20models%0Aoutperformed%20traditional%20models%20in%20detecting%20CPS%20classes.%20Although%20the%0Ainclusion%20of%20multimodality%20did%20not%20improve%20the%20performance%20of%20traditional%0Aunimodal%20models%2C%20its%20integration%20into%20transformer-based%20models%20demonstrated%0Aimproved%20performance%20for%20diagnosing%20social-cognitive%20CPS%20classes%20compared%20to%0Aunimodal%20transformer-based%20models.%20Based%20on%20the%20results%2C%20the%20paper%20argues%20that%0Amultimodality%20and%20the%20selection%20of%20a%20particular%20modelling%20technique%20should%20not%0Abe%20taken%20for%20granted%20to%20achieve%20the%20best%20performance%20in%20the%20automated%20detection%0Aof%20every%20CPS%20subskill%20and%20indicator.%20Rather%2C%20their%20value%20is%20limited%20to%20certain%0Atypes%20of%20CPS%20indicators%2C%20affected%20by%20the%20complexity%20of%20the%20labels%2C%20and%0Adependent%20on%20the%20composition%20of%20indicators%20in%20the%20dataset.%20We%20conclude%20the%0Apaper%20by%20discussing%20the%20required%20nuance%20when%20considering%20the%20value%20of%20LLMs%20and%0Amultimodality%20in%20automated%20CPS%20diagnosis%2C%20highlighting%20the%20need%20for%20human-AI%0Acomplementarity%2C%20and%20proposing%20the%20exploration%20of%20relevant%20model%20architectures%0Aand%20techniques%20to%20improve%20CPS%20diagnosis%20in%20authentic%20educational%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520Potential%2520of%2520Multimodality%2520in%2520Collaborative%2520Problem%250A%2520%2520Solving%2520Diagnosis%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DK.%2520Wong%2520and%2520B.%2520Wu%2520and%2520S.%2520Bulathwela%2520and%2520M.%2520Cukurova%26entry.1292438233%3D%2520%2520Detecting%2520collaborative%2520and%2520problem-solving%2520behaviours%2520from%2520digital%2520traces%2520to%250Ainterpret%2520students%2527%2520collaborative%2520problem%2520solving%2520%2528CPS%2529%2520competency%2520is%2520a%250Along-term%2520goal%2520in%2520the%2520Artificial%2520Intelligence%2520in%2520Education%2520%2528AIEd%2529%2520field.%250AAlthough%2520multimodal%2520data%2520and%2520advanced%2520models%2520are%2520argued%2520to%2520have%2520the%2520potential%250Ato%2520detect%2520complex%2520CPS%2520behaviours%252C%2520empirical%2520evidence%2520on%2520their%2520value%2520remains%250Alimited%2520with%2520some%2520contrasting%2520evidence.%2520In%2520this%2520study%252C%2520we%2520investigated%2520the%250Apotential%2520of%2520multimodal%2520data%2520to%2520improve%2520model%2520performance%2520in%2520diagnosing%252078%250Asecondary%2520school%2520students%2527%2520CPS%2520subskills%2520and%2520indicators%2520in%2520authentic%250Aeducational%2520settings.%2520In%2520particular%252C%2520text%2520embeddings%2520from%2520verbal%2520data%2520and%250Aacoustic%2520embeddings%2520from%2520audio%2520data%2520were%2520used%2520in%2520a%2520multimodal%2520classification%250Amodel%2520for%2520CPS%2520diagnosis.%2520Both%2520unimodal%2520and%2520multimodal%2520transformer-based%2520models%250Aoutperformed%2520traditional%2520models%2520in%2520detecting%2520CPS%2520classes.%2520Although%2520the%250Ainclusion%2520of%2520multimodality%2520did%2520not%2520improve%2520the%2520performance%2520of%2520traditional%250Aunimodal%2520models%252C%2520its%2520integration%2520into%2520transformer-based%2520models%2520demonstrated%250Aimproved%2520performance%2520for%2520diagnosing%2520social-cognitive%2520CPS%2520classes%2520compared%2520to%250Aunimodal%2520transformer-based%2520models.%2520Based%2520on%2520the%2520results%252C%2520the%2520paper%2520argues%2520that%250Amultimodality%2520and%2520the%2520selection%2520of%2520a%2520particular%2520modelling%2520technique%2520should%2520not%250Abe%2520taken%2520for%2520granted%2520to%2520achieve%2520the%2520best%2520performance%2520in%2520the%2520automated%2520detection%250Aof%2520every%2520CPS%2520subskill%2520and%2520indicator.%2520Rather%252C%2520their%2520value%2520is%2520limited%2520to%2520certain%250Atypes%2520of%2520CPS%2520indicators%252C%2520affected%2520by%2520the%2520complexity%2520of%2520the%2520labels%252C%2520and%250Adependent%2520on%2520the%2520composition%2520of%2520indicators%2520in%2520the%2520dataset.%2520We%2520conclude%2520the%250Apaper%2520by%2520discussing%2520the%2520required%2520nuance%2520when%2520considering%2520the%2520value%2520of%2520LLMs%2520and%250Amultimodality%2520in%2520automated%2520CPS%2520diagnosis%252C%2520highlighting%2520the%2520need%2520for%2520human-AI%250Acomplementarity%252C%2520and%2520proposing%2520the%2520exploration%2520of%2520relevant%2520model%2520architectures%250Aand%2520techniques%2520to%2520improve%2520CPS%2520diagnosis%2520in%2520authentic%2520educational%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20Potential%20of%20Multimodality%20in%20Collaborative%20Problem%0A%20%20Solving%20Diagnosis%20with%20Large%20Language%20Models&entry.906535625=K.%20Wong%20and%20B.%20Wu%20and%20S.%20Bulathwela%20and%20M.%20Cukurova&entry.1292438233=%20%20Detecting%20collaborative%20and%20problem-solving%20behaviours%20from%20digital%20traces%20to%0Ainterpret%20students%27%20collaborative%20problem%20solving%20%28CPS%29%20competency%20is%20a%0Along-term%20goal%20in%20the%20Artificial%20Intelligence%20in%20Education%20%28AIEd%29%20field.%0AAlthough%20multimodal%20data%20and%20advanced%20models%20are%20argued%20to%20have%20the%20potential%0Ato%20detect%20complex%20CPS%20behaviours%2C%20empirical%20evidence%20on%20their%20value%20remains%0Alimited%20with%20some%20contrasting%20evidence.%20In%20this%20study%2C%20we%20investigated%20the%0Apotential%20of%20multimodal%20data%20to%20improve%20model%20performance%20in%20diagnosing%2078%0Asecondary%20school%20students%27%20CPS%20subskills%20and%20indicators%20in%20authentic%0Aeducational%20settings.%20In%20particular%2C%20text%20embeddings%20from%20verbal%20data%20and%0Aacoustic%20embeddings%20from%20audio%20data%20were%20used%20in%20a%20multimodal%20classification%0Amodel%20for%20CPS%20diagnosis.%20Both%20unimodal%20and%20multimodal%20transformer-based%20models%0Aoutperformed%20traditional%20models%20in%20detecting%20CPS%20classes.%20Although%20the%0Ainclusion%20of%20multimodality%20did%20not%20improve%20the%20performance%20of%20traditional%0Aunimodal%20models%2C%20its%20integration%20into%20transformer-based%20models%20demonstrated%0Aimproved%20performance%20for%20diagnosing%20social-cognitive%20CPS%20classes%20compared%20to%0Aunimodal%20transformer-based%20models.%20Based%20on%20the%20results%2C%20the%20paper%20argues%20that%0Amultimodality%20and%20the%20selection%20of%20a%20particular%20modelling%20technique%20should%20not%0Abe%20taken%20for%20granted%20to%20achieve%20the%20best%20performance%20in%20the%20automated%20detection%0Aof%20every%20CPS%20subskill%20and%20indicator.%20Rather%2C%20their%20value%20is%20limited%20to%20certain%0Atypes%20of%20CPS%20indicators%2C%20affected%20by%20the%20complexity%20of%20the%20labels%2C%20and%0Adependent%20on%20the%20composition%20of%20indicators%20in%20the%20dataset.%20We%20conclude%20the%0Apaper%20by%20discussing%20the%20required%20nuance%20when%20considering%20the%20value%20of%20LLMs%20and%0Amultimodality%20in%20automated%20CPS%20diagnosis%2C%20highlighting%20the%20need%20for%20human-AI%0Acomplementarity%2C%20and%20proposing%20the%20exploration%20of%20relevant%20model%20architectures%0Aand%20techniques%20to%20improve%20CPS%20diagnosis%20in%20authentic%20educational%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15093v1&entry.124074799=Read"},
{"title": "Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism", "author": "Timo Aukusti Laine", "abstract": "  Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding.\n", "link": "http://arxiv.org/abs/2503.10664v2", "date": "2025-04-21", "relevancy": 2.1348, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5369}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Wave%20Functions%3A%20Exploring%20Meaning%20in%20Large%20Language%20Models%0A%20%20Through%20Quantum%20Formalism&body=Title%3A%20Semantic%20Wave%20Functions%3A%20Exploring%20Meaning%20in%20Large%20Language%20Models%0A%20%20Through%20Quantum%20Formalism%0AAuthor%3A%20Timo%20Aukusti%20Laine%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20encode%20semantic%20relationships%20in%0Ahigh-dimensional%20vector%20embeddings.%20This%20paper%20explores%20the%20analogy%20between%20LLM%0Aembedding%20spaces%20and%20quantum%20mechanics%2C%20positing%20that%20LLMs%20operate%20within%20a%0Aquantized%20semantic%20space%20where%20words%20and%20phrases%20behave%20as%20quantum%20states.%20To%0Acapture%20nuanced%20semantic%20interference%20effects%2C%20we%20extend%20the%20standard%0Areal-valued%20embedding%20space%20to%20the%20complex%20domain%2C%20drawing%20parallels%20to%20the%0Adouble-slit%20experiment.%20We%20introduce%20a%20%22semantic%20wave%20function%22%20to%20formalize%0Athis%20quantum-derived%20representation%20and%20utilize%20potential%20landscapes%2C%20such%20as%0Athe%20double-well%20potential%2C%20to%20model%20semantic%20ambiguity.%20Furthermore%2C%20we%20propose%0Aa%20complex-valued%20similarity%20measure%20that%20incorporates%20both%20magnitude%20and%20phase%0Ainformation%2C%20enabling%20a%20more%20sensitive%20comparison%20of%20semantic%20representations.%0AWe%20develop%20a%20path%20integral%20formalism%2C%20based%20on%20a%20nonlinear%20Schr%5C%22odinger%0Aequation%20with%20a%20gauge%20field%20and%20Mexican%20hat%20potential%2C%20to%20model%20the%20dynamic%0Aevolution%20of%20LLM%20behavior.%20This%20interdisciplinary%20approach%20offers%20a%20new%0Atheoretical%20framework%20for%20understanding%20and%20potentially%20manipulating%20LLMs%2C%20with%0Athe%20goal%20of%20advancing%20both%20artificial%20and%20natural%20language%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10664v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Wave%2520Functions%253A%2520Exploring%2520Meaning%2520in%2520Large%2520Language%2520Models%250A%2520%2520Through%2520Quantum%2520Formalism%26entry.906535625%3DTimo%2520Aukusti%2520Laine%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520encode%2520semantic%2520relationships%2520in%250Ahigh-dimensional%2520vector%2520embeddings.%2520This%2520paper%2520explores%2520the%2520analogy%2520between%2520LLM%250Aembedding%2520spaces%2520and%2520quantum%2520mechanics%252C%2520positing%2520that%2520LLMs%2520operate%2520within%2520a%250Aquantized%2520semantic%2520space%2520where%2520words%2520and%2520phrases%2520behave%2520as%2520quantum%2520states.%2520To%250Acapture%2520nuanced%2520semantic%2520interference%2520effects%252C%2520we%2520extend%2520the%2520standard%250Areal-valued%2520embedding%2520space%2520to%2520the%2520complex%2520domain%252C%2520drawing%2520parallels%2520to%2520the%250Adouble-slit%2520experiment.%2520We%2520introduce%2520a%2520%2522semantic%2520wave%2520function%2522%2520to%2520formalize%250Athis%2520quantum-derived%2520representation%2520and%2520utilize%2520potential%2520landscapes%252C%2520such%2520as%250Athe%2520double-well%2520potential%252C%2520to%2520model%2520semantic%2520ambiguity.%2520Furthermore%252C%2520we%2520propose%250Aa%2520complex-valued%2520similarity%2520measure%2520that%2520incorporates%2520both%2520magnitude%2520and%2520phase%250Ainformation%252C%2520enabling%2520a%2520more%2520sensitive%2520comparison%2520of%2520semantic%2520representations.%250AWe%2520develop%2520a%2520path%2520integral%2520formalism%252C%2520based%2520on%2520a%2520nonlinear%2520Schr%255C%2522odinger%250Aequation%2520with%2520a%2520gauge%2520field%2520and%2520Mexican%2520hat%2520potential%252C%2520to%2520model%2520the%2520dynamic%250Aevolution%2520of%2520LLM%2520behavior.%2520This%2520interdisciplinary%2520approach%2520offers%2520a%2520new%250Atheoretical%2520framework%2520for%2520understanding%2520and%2520potentially%2520manipulating%2520LLMs%252C%2520with%250Athe%2520goal%2520of%2520advancing%2520both%2520artificial%2520and%2520natural%2520language%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10664v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Wave%20Functions%3A%20Exploring%20Meaning%20in%20Large%20Language%20Models%0A%20%20Through%20Quantum%20Formalism&entry.906535625=Timo%20Aukusti%20Laine&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20encode%20semantic%20relationships%20in%0Ahigh-dimensional%20vector%20embeddings.%20This%20paper%20explores%20the%20analogy%20between%20LLM%0Aembedding%20spaces%20and%20quantum%20mechanics%2C%20positing%20that%20LLMs%20operate%20within%20a%0Aquantized%20semantic%20space%20where%20words%20and%20phrases%20behave%20as%20quantum%20states.%20To%0Acapture%20nuanced%20semantic%20interference%20effects%2C%20we%20extend%20the%20standard%0Areal-valued%20embedding%20space%20to%20the%20complex%20domain%2C%20drawing%20parallels%20to%20the%0Adouble-slit%20experiment.%20We%20introduce%20a%20%22semantic%20wave%20function%22%20to%20formalize%0Athis%20quantum-derived%20representation%20and%20utilize%20potential%20landscapes%2C%20such%20as%0Athe%20double-well%20potential%2C%20to%20model%20semantic%20ambiguity.%20Furthermore%2C%20we%20propose%0Aa%20complex-valued%20similarity%20measure%20that%20incorporates%20both%20magnitude%20and%20phase%0Ainformation%2C%20enabling%20a%20more%20sensitive%20comparison%20of%20semantic%20representations.%0AWe%20develop%20a%20path%20integral%20formalism%2C%20based%20on%20a%20nonlinear%20Schr%5C%22odinger%0Aequation%20with%20a%20gauge%20field%20and%20Mexican%20hat%20potential%2C%20to%20model%20the%20dynamic%0Aevolution%20of%20LLM%20behavior.%20This%20interdisciplinary%20approach%20offers%20a%20new%0Atheoretical%20framework%20for%20understanding%20and%20potentially%20manipulating%20LLMs%2C%20with%0Athe%20goal%20of%20advancing%20both%20artificial%20and%20natural%20language%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10664v2&entry.124074799=Read"},
{"title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models", "author": "Yatong Bai and Jonah Casebeer and Somayeh Sojoudi and Nicholas J. Bryan", "abstract": "  We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.\n", "link": "http://arxiv.org/abs/2504.15217v1", "date": "2025-04-21", "relevancy": 2.1279, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5683}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5255}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRAGON%3A%20Distributional%20Rewards%20Optimize%20Diffusion%20Generative%20Models&body=Title%3A%20DRAGON%3A%20Distributional%20Rewards%20Optimize%20Diffusion%20Generative%20Models%0AAuthor%3A%20Yatong%20Bai%20and%20Jonah%20Casebeer%20and%20Somayeh%20Sojoudi%20and%20Nicholas%20J.%20Bryan%0AAbstract%3A%20%20%20We%20present%20Distributional%20RewArds%20for%20Generative%20OptimizatioN%20%28DRAGON%29%2C%20a%0Aversatile%20framework%20for%20fine-tuning%20media%20generation%20models%20towards%20a%20desired%0Aoutcome.%20Compared%20with%20traditional%20reinforcement%20learning%20with%20human%20feedback%0A%28RLHF%29%20or%20pairwise%20preference%20approaches%20such%20as%20direct%20preference%20optimization%0A%28DPO%29%2C%20DRAGON%20is%20more%20flexible.%20It%20can%20optimize%20reward%20functions%20that%20evaluate%0Aeither%20individual%20examples%20or%20distributions%20of%20them%2C%20making%20it%20compatible%20with%0Aa%20broad%20spectrum%20of%20instance-wise%2C%20instance-to-distribution%2C%20and%0Adistribution-to-distribution%20rewards.%20Leveraging%20this%20versatility%2C%20we%20construct%0Anovel%20reward%20functions%20by%20selecting%20an%20encoder%20and%20a%20set%20of%20reference%20examples%0Ato%20create%20an%20exemplar%20distribution.%20When%20cross-modality%20encoders%20such%20as%20CLAP%0Aare%20used%2C%20the%20reference%20examples%20may%20be%20of%20a%20different%20modality%20%28e.g.%2C%20text%0Aversus%20audio%29.%20Then%2C%20DRAGON%20gathers%20online%20and%20on-policy%20generations%2C%20scores%0Athem%20to%20construct%20a%20positive%20demonstration%20set%20and%20a%20negative%20set%2C%20and%0Aleverages%20the%20contrast%20between%20the%20two%20sets%20to%20maximize%20the%20reward.%20For%0Aevaluation%2C%20we%20fine-tune%20an%20audio-domain%20text-to-music%20diffusion%20model%20with%2020%0Adifferent%20reward%20functions%2C%20including%20a%20custom%20music%20aesthetics%20model%2C%20CLAP%0Ascore%2C%20Vendi%20diversity%2C%20and%20Frechet%20audio%20distance%20%28FAD%29.%20We%20further%20compare%0Ainstance-wise%20%28per-song%29%20and%20full-dataset%20FAD%20settings%20while%20ablating%20multiple%0AFAD%20encoders%20and%20reference%20sets.%20Over%20all%2020%20target%20rewards%2C%20DRAGON%20achieves%20an%0A81.45%25%20average%20win%20rate.%20Moreover%2C%20reward%20functions%20based%20on%20exemplar%20sets%0Aindeed%20enhance%20generations%20and%20are%20comparable%20to%20model-based%20rewards.%20With%20an%0Aappropriate%20exemplar%20set%2C%20DRAGON%20achieves%20a%2060.95%25%20human-voted%20music%20quality%0Awin%20rate%20without%20training%20on%20human%20preference%20annotations.%20As%20such%2C%20DRAGON%0Aexhibits%20a%20new%20approach%20to%20designing%20and%20optimizing%20reward%20functions%20for%0Aimproving%20human-perceived%20quality.%20Sound%20examples%20at%0Ahttps%3A//ml-dragon.github.io/web.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRAGON%253A%2520Distributional%2520Rewards%2520Optimize%2520Diffusion%2520Generative%2520Models%26entry.906535625%3DYatong%2520Bai%2520and%2520Jonah%2520Casebeer%2520and%2520Somayeh%2520Sojoudi%2520and%2520Nicholas%2520J.%2520Bryan%26entry.1292438233%3D%2520%2520We%2520present%2520Distributional%2520RewArds%2520for%2520Generative%2520OptimizatioN%2520%2528DRAGON%2529%252C%2520a%250Aversatile%2520framework%2520for%2520fine-tuning%2520media%2520generation%2520models%2520towards%2520a%2520desired%250Aoutcome.%2520Compared%2520with%2520traditional%2520reinforcement%2520learning%2520with%2520human%2520feedback%250A%2528RLHF%2529%2520or%2520pairwise%2520preference%2520approaches%2520such%2520as%2520direct%2520preference%2520optimization%250A%2528DPO%2529%252C%2520DRAGON%2520is%2520more%2520flexible.%2520It%2520can%2520optimize%2520reward%2520functions%2520that%2520evaluate%250Aeither%2520individual%2520examples%2520or%2520distributions%2520of%2520them%252C%2520making%2520it%2520compatible%2520with%250Aa%2520broad%2520spectrum%2520of%2520instance-wise%252C%2520instance-to-distribution%252C%2520and%250Adistribution-to-distribution%2520rewards.%2520Leveraging%2520this%2520versatility%252C%2520we%2520construct%250Anovel%2520reward%2520functions%2520by%2520selecting%2520an%2520encoder%2520and%2520a%2520set%2520of%2520reference%2520examples%250Ato%2520create%2520an%2520exemplar%2520distribution.%2520When%2520cross-modality%2520encoders%2520such%2520as%2520CLAP%250Aare%2520used%252C%2520the%2520reference%2520examples%2520may%2520be%2520of%2520a%2520different%2520modality%2520%2528e.g.%252C%2520text%250Aversus%2520audio%2529.%2520Then%252C%2520DRAGON%2520gathers%2520online%2520and%2520on-policy%2520generations%252C%2520scores%250Athem%2520to%2520construct%2520a%2520positive%2520demonstration%2520set%2520and%2520a%2520negative%2520set%252C%2520and%250Aleverages%2520the%2520contrast%2520between%2520the%2520two%2520sets%2520to%2520maximize%2520the%2520reward.%2520For%250Aevaluation%252C%2520we%2520fine-tune%2520an%2520audio-domain%2520text-to-music%2520diffusion%2520model%2520with%252020%250Adifferent%2520reward%2520functions%252C%2520including%2520a%2520custom%2520music%2520aesthetics%2520model%252C%2520CLAP%250Ascore%252C%2520Vendi%2520diversity%252C%2520and%2520Frechet%2520audio%2520distance%2520%2528FAD%2529.%2520We%2520further%2520compare%250Ainstance-wise%2520%2528per-song%2529%2520and%2520full-dataset%2520FAD%2520settings%2520while%2520ablating%2520multiple%250AFAD%2520encoders%2520and%2520reference%2520sets.%2520Over%2520all%252020%2520target%2520rewards%252C%2520DRAGON%2520achieves%2520an%250A81.45%2525%2520average%2520win%2520rate.%2520Moreover%252C%2520reward%2520functions%2520based%2520on%2520exemplar%2520sets%250Aindeed%2520enhance%2520generations%2520and%2520are%2520comparable%2520to%2520model-based%2520rewards.%2520With%2520an%250Aappropriate%2520exemplar%2520set%252C%2520DRAGON%2520achieves%2520a%252060.95%2525%2520human-voted%2520music%2520quality%250Awin%2520rate%2520without%2520training%2520on%2520human%2520preference%2520annotations.%2520As%2520such%252C%2520DRAGON%250Aexhibits%2520a%2520new%2520approach%2520to%2520designing%2520and%2520optimizing%2520reward%2520functions%2520for%250Aimproving%2520human-perceived%2520quality.%2520Sound%2520examples%2520at%250Ahttps%253A//ml-dragon.github.io/web.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRAGON%3A%20Distributional%20Rewards%20Optimize%20Diffusion%20Generative%20Models&entry.906535625=Yatong%20Bai%20and%20Jonah%20Casebeer%20and%20Somayeh%20Sojoudi%20and%20Nicholas%20J.%20Bryan&entry.1292438233=%20%20We%20present%20Distributional%20RewArds%20for%20Generative%20OptimizatioN%20%28DRAGON%29%2C%20a%0Aversatile%20framework%20for%20fine-tuning%20media%20generation%20models%20towards%20a%20desired%0Aoutcome.%20Compared%20with%20traditional%20reinforcement%20learning%20with%20human%20feedback%0A%28RLHF%29%20or%20pairwise%20preference%20approaches%20such%20as%20direct%20preference%20optimization%0A%28DPO%29%2C%20DRAGON%20is%20more%20flexible.%20It%20can%20optimize%20reward%20functions%20that%20evaluate%0Aeither%20individual%20examples%20or%20distributions%20of%20them%2C%20making%20it%20compatible%20with%0Aa%20broad%20spectrum%20of%20instance-wise%2C%20instance-to-distribution%2C%20and%0Adistribution-to-distribution%20rewards.%20Leveraging%20this%20versatility%2C%20we%20construct%0Anovel%20reward%20functions%20by%20selecting%20an%20encoder%20and%20a%20set%20of%20reference%20examples%0Ato%20create%20an%20exemplar%20distribution.%20When%20cross-modality%20encoders%20such%20as%20CLAP%0Aare%20used%2C%20the%20reference%20examples%20may%20be%20of%20a%20different%20modality%20%28e.g.%2C%20text%0Aversus%20audio%29.%20Then%2C%20DRAGON%20gathers%20online%20and%20on-policy%20generations%2C%20scores%0Athem%20to%20construct%20a%20positive%20demonstration%20set%20and%20a%20negative%20set%2C%20and%0Aleverages%20the%20contrast%20between%20the%20two%20sets%20to%20maximize%20the%20reward.%20For%0Aevaluation%2C%20we%20fine-tune%20an%20audio-domain%20text-to-music%20diffusion%20model%20with%2020%0Adifferent%20reward%20functions%2C%20including%20a%20custom%20music%20aesthetics%20model%2C%20CLAP%0Ascore%2C%20Vendi%20diversity%2C%20and%20Frechet%20audio%20distance%20%28FAD%29.%20We%20further%20compare%0Ainstance-wise%20%28per-song%29%20and%20full-dataset%20FAD%20settings%20while%20ablating%20multiple%0AFAD%20encoders%20and%20reference%20sets.%20Over%20all%2020%20target%20rewards%2C%20DRAGON%20achieves%20an%0A81.45%25%20average%20win%20rate.%20Moreover%2C%20reward%20functions%20based%20on%20exemplar%20sets%0Aindeed%20enhance%20generations%20and%20are%20comparable%20to%20model-based%20rewards.%20With%20an%0Aappropriate%20exemplar%20set%2C%20DRAGON%20achieves%20a%2060.95%25%20human-voted%20music%20quality%0Awin%20rate%20without%20training%20on%20human%20preference%20annotations.%20As%20such%2C%20DRAGON%0Aexhibits%20a%20new%20approach%20to%20designing%20and%20optimizing%20reward%20functions%20for%0Aimproving%20human-perceived%20quality.%20Sound%20examples%20at%0Ahttps%3A//ml-dragon.github.io/web.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15217v1&entry.124074799=Read"},
{"title": "Shifting Attention to You: Personalized Brain-Inspired AI Models", "author": "Stephen Chong Zhao and Yang Hu and Jason Lee and Andrew Bender and Trisha Mazumdar and Mark Wallace and David A. Tovar", "abstract": "  The integration of human and artificial intelligence offers a powerful avenue\nfor advancing our understanding of information processing, as each system\nprovides unique computational insights. However, despite the promise of\nhuman-AI integration, current AI models are largely trained on massive\ndatasets, optimized for population-level performance, lacking mechanisms to\nalign their computations with individual users' perceptual semantics and neural\ndynamics. Here we show that integrating human behavioral insights and\nmillisecond scale neural data within a fine tuned CLIP based model not only\ncaptures generalized and individualized aspects of perception but also over\ndoubles behavioral performance compared to the unmodified CLIP baseline. By\nembedding human inductive biases and mirroring dynamic neural processes during\ntraining, personalized neural fine tuning improves predictions of human\nsimilarity judgments and tracks the temporal evolution of individual neural\nresponses. Our work establishes a novel, interpretable framework for designing\nadaptive AI systems, with broad implications for neuroscience, personalized\nmedicine, and human-computer interaction.\n", "link": "http://arxiv.org/abs/2502.04658v2", "date": "2025-04-21", "relevancy": 2.1236, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shifting%20Attention%20to%20You%3A%20Personalized%20Brain-Inspired%20AI%20Models&body=Title%3A%20Shifting%20Attention%20to%20You%3A%20Personalized%20Brain-Inspired%20AI%20Models%0AAuthor%3A%20Stephen%20Chong%20Zhao%20and%20Yang%20Hu%20and%20Jason%20Lee%20and%20Andrew%20Bender%20and%20Trisha%20Mazumdar%20and%20Mark%20Wallace%20and%20David%20A.%20Tovar%0AAbstract%3A%20%20%20The%20integration%20of%20human%20and%20artificial%20intelligence%20offers%20a%20powerful%20avenue%0Afor%20advancing%20our%20understanding%20of%20information%20processing%2C%20as%20each%20system%0Aprovides%20unique%20computational%20insights.%20However%2C%20despite%20the%20promise%20of%0Ahuman-AI%20integration%2C%20current%20AI%20models%20are%20largely%20trained%20on%20massive%0Adatasets%2C%20optimized%20for%20population-level%20performance%2C%20lacking%20mechanisms%20to%0Aalign%20their%20computations%20with%20individual%20users%27%20perceptual%20semantics%20and%20neural%0Adynamics.%20Here%20we%20show%20that%20integrating%20human%20behavioral%20insights%20and%0Amillisecond%20scale%20neural%20data%20within%20a%20fine%20tuned%20CLIP%20based%20model%20not%20only%0Acaptures%20generalized%20and%20individualized%20aspects%20of%20perception%20but%20also%20over%0Adoubles%20behavioral%20performance%20compared%20to%20the%20unmodified%20CLIP%20baseline.%20By%0Aembedding%20human%20inductive%20biases%20and%20mirroring%20dynamic%20neural%20processes%20during%0Atraining%2C%20personalized%20neural%20fine%20tuning%20improves%20predictions%20of%20human%0Asimilarity%20judgments%20and%20tracks%20the%20temporal%20evolution%20of%20individual%20neural%0Aresponses.%20Our%20work%20establishes%20a%20novel%2C%20interpretable%20framework%20for%20designing%0Aadaptive%20AI%20systems%2C%20with%20broad%20implications%20for%20neuroscience%2C%20personalized%0Amedicine%2C%20and%20human-computer%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04658v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShifting%2520Attention%2520to%2520You%253A%2520Personalized%2520Brain-Inspired%2520AI%2520Models%26entry.906535625%3DStephen%2520Chong%2520Zhao%2520and%2520Yang%2520Hu%2520and%2520Jason%2520Lee%2520and%2520Andrew%2520Bender%2520and%2520Trisha%2520Mazumdar%2520and%2520Mark%2520Wallace%2520and%2520David%2520A.%2520Tovar%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520human%2520and%2520artificial%2520intelligence%2520offers%2520a%2520powerful%2520avenue%250Afor%2520advancing%2520our%2520understanding%2520of%2520information%2520processing%252C%2520as%2520each%2520system%250Aprovides%2520unique%2520computational%2520insights.%2520However%252C%2520despite%2520the%2520promise%2520of%250Ahuman-AI%2520integration%252C%2520current%2520AI%2520models%2520are%2520largely%2520trained%2520on%2520massive%250Adatasets%252C%2520optimized%2520for%2520population-level%2520performance%252C%2520lacking%2520mechanisms%2520to%250Aalign%2520their%2520computations%2520with%2520individual%2520users%2527%2520perceptual%2520semantics%2520and%2520neural%250Adynamics.%2520Here%2520we%2520show%2520that%2520integrating%2520human%2520behavioral%2520insights%2520and%250Amillisecond%2520scale%2520neural%2520data%2520within%2520a%2520fine%2520tuned%2520CLIP%2520based%2520model%2520not%2520only%250Acaptures%2520generalized%2520and%2520individualized%2520aspects%2520of%2520perception%2520but%2520also%2520over%250Adoubles%2520behavioral%2520performance%2520compared%2520to%2520the%2520unmodified%2520CLIP%2520baseline.%2520By%250Aembedding%2520human%2520inductive%2520biases%2520and%2520mirroring%2520dynamic%2520neural%2520processes%2520during%250Atraining%252C%2520personalized%2520neural%2520fine%2520tuning%2520improves%2520predictions%2520of%2520human%250Asimilarity%2520judgments%2520and%2520tracks%2520the%2520temporal%2520evolution%2520of%2520individual%2520neural%250Aresponses.%2520Our%2520work%2520establishes%2520a%2520novel%252C%2520interpretable%2520framework%2520for%2520designing%250Aadaptive%2520AI%2520systems%252C%2520with%2520broad%2520implications%2520for%2520neuroscience%252C%2520personalized%250Amedicine%252C%2520and%2520human-computer%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04658v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shifting%20Attention%20to%20You%3A%20Personalized%20Brain-Inspired%20AI%20Models&entry.906535625=Stephen%20Chong%20Zhao%20and%20Yang%20Hu%20and%20Jason%20Lee%20and%20Andrew%20Bender%20and%20Trisha%20Mazumdar%20and%20Mark%20Wallace%20and%20David%20A.%20Tovar&entry.1292438233=%20%20The%20integration%20of%20human%20and%20artificial%20intelligence%20offers%20a%20powerful%20avenue%0Afor%20advancing%20our%20understanding%20of%20information%20processing%2C%20as%20each%20system%0Aprovides%20unique%20computational%20insights.%20However%2C%20despite%20the%20promise%20of%0Ahuman-AI%20integration%2C%20current%20AI%20models%20are%20largely%20trained%20on%20massive%0Adatasets%2C%20optimized%20for%20population-level%20performance%2C%20lacking%20mechanisms%20to%0Aalign%20their%20computations%20with%20individual%20users%27%20perceptual%20semantics%20and%20neural%0Adynamics.%20Here%20we%20show%20that%20integrating%20human%20behavioral%20insights%20and%0Amillisecond%20scale%20neural%20data%20within%20a%20fine%20tuned%20CLIP%20based%20model%20not%20only%0Acaptures%20generalized%20and%20individualized%20aspects%20of%20perception%20but%20also%20over%0Adoubles%20behavioral%20performance%20compared%20to%20the%20unmodified%20CLIP%20baseline.%20By%0Aembedding%20human%20inductive%20biases%20and%20mirroring%20dynamic%20neural%20processes%20during%0Atraining%2C%20personalized%20neural%20fine%20tuning%20improves%20predictions%20of%20human%0Asimilarity%20judgments%20and%20tracks%20the%20temporal%20evolution%20of%20individual%20neural%0Aresponses.%20Our%20work%20establishes%20a%20novel%2C%20interpretable%20framework%20for%20designing%0Aadaptive%20AI%20systems%2C%20with%20broad%20implications%20for%20neuroscience%2C%20personalized%0Amedicine%2C%20and%20human-computer%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04658v2&entry.124074799=Read"},
{"title": "Dynamic 3D KAN Convolution with Adaptive Grid Optimization for\n  Hyperspectral Image Classification", "author": "Guandong Li and Mengxia Ye", "abstract": "  Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more efficiently adapt to\nground object distributions while extracting image features without introducing\nexcessive parameters and skipping redundant information, this paper proposes\nKANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an\nadaptive grid update mechanism. By introducing learnable univariate B-spline\nfunctions on network edges, specifically by flattening three-dimensional\nneighborhoods into vectors and applying B-spline-parameterized nonlinear\nactivation functions to replace the fixed linear weights of traditional 3D\nconvolutional kernels, we precisely capture complex spectral-spatial nonlinear\nrelationships in hyperspectral data. Simultaneously, through a dynamic grid\nadjustment mechanism, we adaptively update the grid point positions of\nB-splines based on the statistical characteristics of input data, optimizing\nthe resolution of spline functions to match the non-uniform distribution of\nspectral features, significantly improving the model's accuracy in\nhigh-dimensional data modeling and parameter efficiency, effectively\nalleviating the curse of dimensionality. This characteristic demonstrates\nsuperior neural scaling laws compared to traditional convolutional neural\nnetworks and reduces overfitting risks in small-sample and high-noise\nscenarios. KANet enhances model representation capability through a 3D dynamic\nexpert convolution system without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches.\n", "link": "http://arxiv.org/abs/2504.15155v1", "date": "2025-04-21", "relevancy": 2.1119, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5528}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5102}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%203D%20KAN%20Convolution%20with%20Adaptive%20Grid%20Optimization%20for%0A%20%20Hyperspectral%20Image%20Classification&body=Title%3A%20Dynamic%203D%20KAN%20Convolution%20with%20Adaptive%20Grid%20Optimization%20for%0A%20%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Guandong%20Li%20and%20Mengxia%20Ye%0AAbstract%3A%20%20%20Deep%20neural%20networks%20face%20several%20challenges%20in%20hyperspectral%20image%0Aclassification%2C%20including%20high-dimensional%20data%2C%20sparse%20distribution%20of%20ground%0Aobjects%2C%20and%20spectral%20redundancy%2C%20which%20often%20lead%20to%20classification%0Aoverfitting%20and%20limited%20generalization%20capability.%20To%20more%20efficiently%20adapt%20to%0Aground%20object%20distributions%20while%20extracting%20image%20features%20without%20introducing%0Aexcessive%20parameters%20and%20skipping%20redundant%20information%2C%20this%20paper%20proposes%0AKANet%20based%20on%20an%20improved%203D-DenseNet%20model%2C%20consisting%20of%203D%20KAN%20Conv%20and%20an%0Aadaptive%20grid%20update%20mechanism.%20By%20introducing%20learnable%20univariate%20B-spline%0Afunctions%20on%20network%20edges%2C%20specifically%20by%20flattening%20three-dimensional%0Aneighborhoods%20into%20vectors%20and%20applying%20B-spline-parameterized%20nonlinear%0Aactivation%20functions%20to%20replace%20the%20fixed%20linear%20weights%20of%20traditional%203D%0Aconvolutional%20kernels%2C%20we%20precisely%20capture%20complex%20spectral-spatial%20nonlinear%0Arelationships%20in%20hyperspectral%20data.%20Simultaneously%2C%20through%20a%20dynamic%20grid%0Aadjustment%20mechanism%2C%20we%20adaptively%20update%20the%20grid%20point%20positions%20of%0AB-splines%20based%20on%20the%20statistical%20characteristics%20of%20input%20data%2C%20optimizing%0Athe%20resolution%20of%20spline%20functions%20to%20match%20the%20non-uniform%20distribution%20of%0Aspectral%20features%2C%20significantly%20improving%20the%20model%27s%20accuracy%20in%0Ahigh-dimensional%20data%20modeling%20and%20parameter%20efficiency%2C%20effectively%0Aalleviating%20the%20curse%20of%20dimensionality.%20This%20characteristic%20demonstrates%0Asuperior%20neural%20scaling%20laws%20compared%20to%20traditional%20convolutional%20neural%0Anetworks%20and%20reduces%20overfitting%20risks%20in%20small-sample%20and%20high-noise%0Ascenarios.%20KANet%20enhances%20model%20representation%20capability%20through%20a%203D%20dynamic%0Aexpert%20convolution%20system%20without%20increasing%20network%20depth%20or%20width.%20The%0Aproposed%20method%20demonstrates%20superior%20performance%20on%20IN%2C%20UP%2C%20and%20KSC%20datasets%2C%0Aoutperforming%20mainstream%20hyperspectral%20image%20classification%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%25203D%2520KAN%2520Convolution%2520with%2520Adaptive%2520Grid%2520Optimization%2520for%250A%2520%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DGuandong%2520Li%2520and%2520Mengxia%2520Ye%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520face%2520several%2520challenges%2520in%2520hyperspectral%2520image%250Aclassification%252C%2520including%2520high-dimensional%2520data%252C%2520sparse%2520distribution%2520of%2520ground%250Aobjects%252C%2520and%2520spectral%2520redundancy%252C%2520which%2520often%2520lead%2520to%2520classification%250Aoverfitting%2520and%2520limited%2520generalization%2520capability.%2520To%2520more%2520efficiently%2520adapt%2520to%250Aground%2520object%2520distributions%2520while%2520extracting%2520image%2520features%2520without%2520introducing%250Aexcessive%2520parameters%2520and%2520skipping%2520redundant%2520information%252C%2520this%2520paper%2520proposes%250AKANet%2520based%2520on%2520an%2520improved%25203D-DenseNet%2520model%252C%2520consisting%2520of%25203D%2520KAN%2520Conv%2520and%2520an%250Aadaptive%2520grid%2520update%2520mechanism.%2520By%2520introducing%2520learnable%2520univariate%2520B-spline%250Afunctions%2520on%2520network%2520edges%252C%2520specifically%2520by%2520flattening%2520three-dimensional%250Aneighborhoods%2520into%2520vectors%2520and%2520applying%2520B-spline-parameterized%2520nonlinear%250Aactivation%2520functions%2520to%2520replace%2520the%2520fixed%2520linear%2520weights%2520of%2520traditional%25203D%250Aconvolutional%2520kernels%252C%2520we%2520precisely%2520capture%2520complex%2520spectral-spatial%2520nonlinear%250Arelationships%2520in%2520hyperspectral%2520data.%2520Simultaneously%252C%2520through%2520a%2520dynamic%2520grid%250Aadjustment%2520mechanism%252C%2520we%2520adaptively%2520update%2520the%2520grid%2520point%2520positions%2520of%250AB-splines%2520based%2520on%2520the%2520statistical%2520characteristics%2520of%2520input%2520data%252C%2520optimizing%250Athe%2520resolution%2520of%2520spline%2520functions%2520to%2520match%2520the%2520non-uniform%2520distribution%2520of%250Aspectral%2520features%252C%2520significantly%2520improving%2520the%2520model%2527s%2520accuracy%2520in%250Ahigh-dimensional%2520data%2520modeling%2520and%2520parameter%2520efficiency%252C%2520effectively%250Aalleviating%2520the%2520curse%2520of%2520dimensionality.%2520This%2520characteristic%2520demonstrates%250Asuperior%2520neural%2520scaling%2520laws%2520compared%2520to%2520traditional%2520convolutional%2520neural%250Anetworks%2520and%2520reduces%2520overfitting%2520risks%2520in%2520small-sample%2520and%2520high-noise%250Ascenarios.%2520KANet%2520enhances%2520model%2520representation%2520capability%2520through%2520a%25203D%2520dynamic%250Aexpert%2520convolution%2520system%2520without%2520increasing%2520network%2520depth%2520or%2520width.%2520The%250Aproposed%2520method%2520demonstrates%2520superior%2520performance%2520on%2520IN%252C%2520UP%252C%2520and%2520KSC%2520datasets%252C%250Aoutperforming%2520mainstream%2520hyperspectral%2520image%2520classification%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%203D%20KAN%20Convolution%20with%20Adaptive%20Grid%20Optimization%20for%0A%20%20Hyperspectral%20Image%20Classification&entry.906535625=Guandong%20Li%20and%20Mengxia%20Ye&entry.1292438233=%20%20Deep%20neural%20networks%20face%20several%20challenges%20in%20hyperspectral%20image%0Aclassification%2C%20including%20high-dimensional%20data%2C%20sparse%20distribution%20of%20ground%0Aobjects%2C%20and%20spectral%20redundancy%2C%20which%20often%20lead%20to%20classification%0Aoverfitting%20and%20limited%20generalization%20capability.%20To%20more%20efficiently%20adapt%20to%0Aground%20object%20distributions%20while%20extracting%20image%20features%20without%20introducing%0Aexcessive%20parameters%20and%20skipping%20redundant%20information%2C%20this%20paper%20proposes%0AKANet%20based%20on%20an%20improved%203D-DenseNet%20model%2C%20consisting%20of%203D%20KAN%20Conv%20and%20an%0Aadaptive%20grid%20update%20mechanism.%20By%20introducing%20learnable%20univariate%20B-spline%0Afunctions%20on%20network%20edges%2C%20specifically%20by%20flattening%20three-dimensional%0Aneighborhoods%20into%20vectors%20and%20applying%20B-spline-parameterized%20nonlinear%0Aactivation%20functions%20to%20replace%20the%20fixed%20linear%20weights%20of%20traditional%203D%0Aconvolutional%20kernels%2C%20we%20precisely%20capture%20complex%20spectral-spatial%20nonlinear%0Arelationships%20in%20hyperspectral%20data.%20Simultaneously%2C%20through%20a%20dynamic%20grid%0Aadjustment%20mechanism%2C%20we%20adaptively%20update%20the%20grid%20point%20positions%20of%0AB-splines%20based%20on%20the%20statistical%20characteristics%20of%20input%20data%2C%20optimizing%0Athe%20resolution%20of%20spline%20functions%20to%20match%20the%20non-uniform%20distribution%20of%0Aspectral%20features%2C%20significantly%20improving%20the%20model%27s%20accuracy%20in%0Ahigh-dimensional%20data%20modeling%20and%20parameter%20efficiency%2C%20effectively%0Aalleviating%20the%20curse%20of%20dimensionality.%20This%20characteristic%20demonstrates%0Asuperior%20neural%20scaling%20laws%20compared%20to%20traditional%20convolutional%20neural%0Anetworks%20and%20reduces%20overfitting%20risks%20in%20small-sample%20and%20high-noise%0Ascenarios.%20KANet%20enhances%20model%20representation%20capability%20through%20a%203D%20dynamic%0Aexpert%20convolution%20system%20without%20increasing%20network%20depth%20or%20width.%20The%0Aproposed%20method%20demonstrates%20superior%20performance%20on%20IN%2C%20UP%2C%20and%20KSC%20datasets%2C%0Aoutperforming%20mainstream%20hyperspectral%20image%20classification%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15155v1&entry.124074799=Read"},
{"title": "Exploring Commonalities in Explanation Frameworks: A Multi-Domain Survey\n  Analysis", "author": "Eduard Barbu and Marharyta Domnich and Raul Vicente and Nikos Sakkas and Andr\u00e9 Morim", "abstract": "  This study presents insights gathered from surveys and discussions with\nspecialists in three domains, aiming to find essential elements for a universal\nexplanation framework that could be applied to these and other similar use\ncases. The insights are incorporated into a software tool that utilizes GP\nalgorithms, known for their interpretability. The applications analyzed include\na medical scenario (involving predictive ML), a retail use case (involving\nprescriptive ML), and an energy use case (also involving predictive ML). We\ninterviewed professionals from each sector, transcribing their conversations\nfor further analysis. Additionally, experts and non-experts in these fields\nfilled out questionnaires designed to probe various dimensions of explanatory\nmethods. The findings indicate a universal preference for sacrificing a degree\nof accuracy in favor of greater explainability. Additionally, we highlight the\nsignificance of feature importance and counterfactual explanations as critical\ncomponents of such a framework. Our questionnaires are publicly available to\nfacilitate the dissemination of knowledge in the field of XAI.\n", "link": "http://arxiv.org/abs/2405.11958v2", "date": "2025-04-21", "relevancy": 2.0943, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Commonalities%20in%20Explanation%20Frameworks%3A%20A%20Multi-Domain%20Survey%0A%20%20Analysis&body=Title%3A%20Exploring%20Commonalities%20in%20Explanation%20Frameworks%3A%20A%20Multi-Domain%20Survey%0A%20%20Analysis%0AAuthor%3A%20Eduard%20Barbu%20and%20Marharyta%20Domnich%20and%20Raul%20Vicente%20and%20Nikos%20Sakkas%20and%20Andr%C3%A9%20Morim%0AAbstract%3A%20%20%20This%20study%20presents%20insights%20gathered%20from%20surveys%20and%20discussions%20with%0Aspecialists%20in%20three%20domains%2C%20aiming%20to%20find%20essential%20elements%20for%20a%20universal%0Aexplanation%20framework%20that%20could%20be%20applied%20to%20these%20and%20other%20similar%20use%0Acases.%20The%20insights%20are%20incorporated%20into%20a%20software%20tool%20that%20utilizes%20GP%0Aalgorithms%2C%20known%20for%20their%20interpretability.%20The%20applications%20analyzed%20include%0Aa%20medical%20scenario%20%28involving%20predictive%20ML%29%2C%20a%20retail%20use%20case%20%28involving%0Aprescriptive%20ML%29%2C%20and%20an%20energy%20use%20case%20%28also%20involving%20predictive%20ML%29.%20We%0Ainterviewed%20professionals%20from%20each%20sector%2C%20transcribing%20their%20conversations%0Afor%20further%20analysis.%20Additionally%2C%20experts%20and%20non-experts%20in%20these%20fields%0Afilled%20out%20questionnaires%20designed%20to%20probe%20various%20dimensions%20of%20explanatory%0Amethods.%20The%20findings%20indicate%20a%20universal%20preference%20for%20sacrificing%20a%20degree%0Aof%20accuracy%20in%20favor%20of%20greater%20explainability.%20Additionally%2C%20we%20highlight%20the%0Asignificance%20of%20feature%20importance%20and%20counterfactual%20explanations%20as%20critical%0Acomponents%20of%20such%20a%20framework.%20Our%20questionnaires%20are%20publicly%20available%20to%0Afacilitate%20the%20dissemination%20of%20knowledge%20in%20the%20field%20of%20XAI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11958v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Commonalities%2520in%2520Explanation%2520Frameworks%253A%2520A%2520Multi-Domain%2520Survey%250A%2520%2520Analysis%26entry.906535625%3DEduard%2520Barbu%2520and%2520Marharyta%2520Domnich%2520and%2520Raul%2520Vicente%2520and%2520Nikos%2520Sakkas%2520and%2520Andr%25C3%25A9%2520Morim%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520insights%2520gathered%2520from%2520surveys%2520and%2520discussions%2520with%250Aspecialists%2520in%2520three%2520domains%252C%2520aiming%2520to%2520find%2520essential%2520elements%2520for%2520a%2520universal%250Aexplanation%2520framework%2520that%2520could%2520be%2520applied%2520to%2520these%2520and%2520other%2520similar%2520use%250Acases.%2520The%2520insights%2520are%2520incorporated%2520into%2520a%2520software%2520tool%2520that%2520utilizes%2520GP%250Aalgorithms%252C%2520known%2520for%2520their%2520interpretability.%2520The%2520applications%2520analyzed%2520include%250Aa%2520medical%2520scenario%2520%2528involving%2520predictive%2520ML%2529%252C%2520a%2520retail%2520use%2520case%2520%2528involving%250Aprescriptive%2520ML%2529%252C%2520and%2520an%2520energy%2520use%2520case%2520%2528also%2520involving%2520predictive%2520ML%2529.%2520We%250Ainterviewed%2520professionals%2520from%2520each%2520sector%252C%2520transcribing%2520their%2520conversations%250Afor%2520further%2520analysis.%2520Additionally%252C%2520experts%2520and%2520non-experts%2520in%2520these%2520fields%250Afilled%2520out%2520questionnaires%2520designed%2520to%2520probe%2520various%2520dimensions%2520of%2520explanatory%250Amethods.%2520The%2520findings%2520indicate%2520a%2520universal%2520preference%2520for%2520sacrificing%2520a%2520degree%250Aof%2520accuracy%2520in%2520favor%2520of%2520greater%2520explainability.%2520Additionally%252C%2520we%2520highlight%2520the%250Asignificance%2520of%2520feature%2520importance%2520and%2520counterfactual%2520explanations%2520as%2520critical%250Acomponents%2520of%2520such%2520a%2520framework.%2520Our%2520questionnaires%2520are%2520publicly%2520available%2520to%250Afacilitate%2520the%2520dissemination%2520of%2520knowledge%2520in%2520the%2520field%2520of%2520XAI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11958v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Commonalities%20in%20Explanation%20Frameworks%3A%20A%20Multi-Domain%20Survey%0A%20%20Analysis&entry.906535625=Eduard%20Barbu%20and%20Marharyta%20Domnich%20and%20Raul%20Vicente%20and%20Nikos%20Sakkas%20and%20Andr%C3%A9%20Morim&entry.1292438233=%20%20This%20study%20presents%20insights%20gathered%20from%20surveys%20and%20discussions%20with%0Aspecialists%20in%20three%20domains%2C%20aiming%20to%20find%20essential%20elements%20for%20a%20universal%0Aexplanation%20framework%20that%20could%20be%20applied%20to%20these%20and%20other%20similar%20use%0Acases.%20The%20insights%20are%20incorporated%20into%20a%20software%20tool%20that%20utilizes%20GP%0Aalgorithms%2C%20known%20for%20their%20interpretability.%20The%20applications%20analyzed%20include%0Aa%20medical%20scenario%20%28involving%20predictive%20ML%29%2C%20a%20retail%20use%20case%20%28involving%0Aprescriptive%20ML%29%2C%20and%20an%20energy%20use%20case%20%28also%20involving%20predictive%20ML%29.%20We%0Ainterviewed%20professionals%20from%20each%20sector%2C%20transcribing%20their%20conversations%0Afor%20further%20analysis.%20Additionally%2C%20experts%20and%20non-experts%20in%20these%20fields%0Afilled%20out%20questionnaires%20designed%20to%20probe%20various%20dimensions%20of%20explanatory%0Amethods.%20The%20findings%20indicate%20a%20universal%20preference%20for%20sacrificing%20a%20degree%0Aof%20accuracy%20in%20favor%20of%20greater%20explainability.%20Additionally%2C%20we%20highlight%20the%0Asignificance%20of%20feature%20importance%20and%20counterfactual%20explanations%20as%20critical%0Acomponents%20of%20such%20a%20framework.%20Our%20questionnaires%20are%20publicly%20available%20to%0Afacilitate%20the%20dissemination%20of%20knowledge%20in%20the%20field%20of%20XAI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11958v2&entry.124074799=Read"},
{"title": "Histogram-based Parameter-efficient Tuning for Passive Sonar\n  Classification", "author": "Amirmohammad Mohammadi and Davelle Carreiro and Alexandra Van Dine and Joshua Peeples", "abstract": "  Parameter-efficient transfer learning (PETL) methods adapt large artificial\nneural networks to downstream tasks without fine-tuning the entire model.\nHowever, existing additive methods, such as adapters, sometimes struggle to\ncapture distributional shifts in intermediate feature embeddings. We propose a\nnovel histogram-based parameter-efficient tuning (HPT) technique that captures\nthe statistics of the target domain and modulates the embeddings. Experimental\nresults on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)\ndemonstrate that HPT outperforms conventional adapters. Notably, HPT achieves\n91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields\nfeature representations closer to those of fully fine-tuned models. Overall,\nHPT balances parameter savings and performance, providing a distribution-aware\nalternative to existing adapters and shows a promising direction for scalable\ntransfer learning in resource-constrained environments. The code is publicly\navailable:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.\n", "link": "http://arxiv.org/abs/2504.15214v1", "date": "2025-04-21", "relevancy": 2.0912, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5314}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5201}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Histogram-based%20Parameter-efficient%20Tuning%20for%20Passive%20Sonar%0A%20%20Classification&body=Title%3A%20Histogram-based%20Parameter-efficient%20Tuning%20for%20Passive%20Sonar%0A%20%20Classification%0AAuthor%3A%20Amirmohammad%20Mohammadi%20and%20Davelle%20Carreiro%20and%20Alexandra%20Van%20Dine%20and%20Joshua%20Peeples%0AAbstract%3A%20%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20methods%20adapt%20large%20artificial%0Aneural%20networks%20to%20downstream%20tasks%20without%20fine-tuning%20the%20entire%20model.%0AHowever%2C%20existing%20additive%20methods%2C%20such%20as%20adapters%2C%20sometimes%20struggle%20to%0Acapture%20distributional%20shifts%20in%20intermediate%20feature%20embeddings.%20We%20propose%20a%0Anovel%20histogram-based%20parameter-efficient%20tuning%20%28HPT%29%20technique%20that%20captures%0Athe%20statistics%20of%20the%20target%20domain%20and%20modulates%20the%20embeddings.%20Experimental%0Aresults%20on%20three%20downstream%20passive%20sonar%20datasets%20%28ShipsEar%2C%20DeepShip%2C%20VTUAD%29%0Ademonstrate%20that%20HPT%20outperforms%20conventional%20adapters.%20Notably%2C%20HPT%20achieves%0A91.8%25%20vs.%2089.8%25%20accuracy%20on%20VTUAD.%20Furthermore%2C%20HPT%20trains%20faster%20and%20yields%0Afeature%20representations%20closer%20to%20those%20of%20fully%20fine-tuned%20models.%20Overall%2C%0AHPT%20balances%20parameter%20savings%20and%20performance%2C%20providing%20a%20distribution-aware%0Aalternative%20to%20existing%20adapters%20and%20shows%20a%20promising%20direction%20for%20scalable%0Atransfer%20learning%20in%20resource-constrained%20environments.%20The%20code%20is%20publicly%0Aavailable%3A%0Ahttps%3A//github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHistogram-based%2520Parameter-efficient%2520Tuning%2520for%2520Passive%2520Sonar%250A%2520%2520Classification%26entry.906535625%3DAmirmohammad%2520Mohammadi%2520and%2520Davelle%2520Carreiro%2520and%2520Alexandra%2520Van%2520Dine%2520and%2520Joshua%2520Peeples%26entry.1292438233%3D%2520%2520Parameter-efficient%2520transfer%2520learning%2520%2528PETL%2529%2520methods%2520adapt%2520large%2520artificial%250Aneural%2520networks%2520to%2520downstream%2520tasks%2520without%2520fine-tuning%2520the%2520entire%2520model.%250AHowever%252C%2520existing%2520additive%2520methods%252C%2520such%2520as%2520adapters%252C%2520sometimes%2520struggle%2520to%250Acapture%2520distributional%2520shifts%2520in%2520intermediate%2520feature%2520embeddings.%2520We%2520propose%2520a%250Anovel%2520histogram-based%2520parameter-efficient%2520tuning%2520%2528HPT%2529%2520technique%2520that%2520captures%250Athe%2520statistics%2520of%2520the%2520target%2520domain%2520and%2520modulates%2520the%2520embeddings.%2520Experimental%250Aresults%2520on%2520three%2520downstream%2520passive%2520sonar%2520datasets%2520%2528ShipsEar%252C%2520DeepShip%252C%2520VTUAD%2529%250Ademonstrate%2520that%2520HPT%2520outperforms%2520conventional%2520adapters.%2520Notably%252C%2520HPT%2520achieves%250A91.8%2525%2520vs.%252089.8%2525%2520accuracy%2520on%2520VTUAD.%2520Furthermore%252C%2520HPT%2520trains%2520faster%2520and%2520yields%250Afeature%2520representations%2520closer%2520to%2520those%2520of%2520fully%2520fine-tuned%2520models.%2520Overall%252C%250AHPT%2520balances%2520parameter%2520savings%2520and%2520performance%252C%2520providing%2520a%2520distribution-aware%250Aalternative%2520to%2520existing%2520adapters%2520and%2520shows%2520a%2520promising%2520direction%2520for%2520scalable%250Atransfer%2520learning%2520in%2520resource-constrained%2520environments.%2520The%2520code%2520is%2520publicly%250Aavailable%253A%250Ahttps%253A//github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Histogram-based%20Parameter-efficient%20Tuning%20for%20Passive%20Sonar%0A%20%20Classification&entry.906535625=Amirmohammad%20Mohammadi%20and%20Davelle%20Carreiro%20and%20Alexandra%20Van%20Dine%20and%20Joshua%20Peeples&entry.1292438233=%20%20Parameter-efficient%20transfer%20learning%20%28PETL%29%20methods%20adapt%20large%20artificial%0Aneural%20networks%20to%20downstream%20tasks%20without%20fine-tuning%20the%20entire%20model.%0AHowever%2C%20existing%20additive%20methods%2C%20such%20as%20adapters%2C%20sometimes%20struggle%20to%0Acapture%20distributional%20shifts%20in%20intermediate%20feature%20embeddings.%20We%20propose%20a%0Anovel%20histogram-based%20parameter-efficient%20tuning%20%28HPT%29%20technique%20that%20captures%0Athe%20statistics%20of%20the%20target%20domain%20and%20modulates%20the%20embeddings.%20Experimental%0Aresults%20on%20three%20downstream%20passive%20sonar%20datasets%20%28ShipsEar%2C%20DeepShip%2C%20VTUAD%29%0Ademonstrate%20that%20HPT%20outperforms%20conventional%20adapters.%20Notably%2C%20HPT%20achieves%0A91.8%25%20vs.%2089.8%25%20accuracy%20on%20VTUAD.%20Furthermore%2C%20HPT%20trains%20faster%20and%20yields%0Afeature%20representations%20closer%20to%20those%20of%20fully%20fine-tuned%20models.%20Overall%2C%0AHPT%20balances%20parameter%20savings%20and%20performance%2C%20providing%20a%20distribution-aware%0Aalternative%20to%20existing%20adapters%20and%20shows%20a%20promising%20direction%20for%20scalable%0Atransfer%20learning%20in%20resource-constrained%20environments.%20The%20code%20is%20publicly%0Aavailable%3A%0Ahttps%3A//github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15214v1&entry.124074799=Read"},
{"title": "Conformalized-KANs: Uncertainty Quantification with Coverage Guarantees\n  for Kolmogorov-Arnold Networks (KANs) in Scientific Machine Learning", "author": "Amirhossein Mollaali and Christian Bolivar Moya and Amanda A. Howard and Alexander Heinlein and Panos Stinis and Guang Lin", "abstract": "  This paper explores uncertainty quantification (UQ) methods in the context of\nKolmogorov-Arnold Networks (KANs). We apply an ensemble approach to KANs to\nobtain a heuristic measure of UQ, enhancing interpretability and robustness in\nmodeling complex functions. Building on this, we introduce Conformalized-KANs,\nwhich integrate conformal prediction, a distribution-free UQ technique, with\nKAN ensembles to generate calibrated prediction intervals with guaranteed\ncoverage. Extensive numerical experiments are conducted to evaluate the\neffectiveness of these methods, focusing particularly on the robustness and\naccuracy of the prediction intervals under various hyperparameter settings. We\nshow that the conformal KAN predictions can be applied to recent extensions of\nKANs, including Finite Basis KANs (FBKANs) and multifideilty KANs (MFKANs). The\nresults demonstrate the potential of our approaches to improve the reliability\nand applicability of KANs in scientific machine learning.\n", "link": "http://arxiv.org/abs/2504.15240v1", "date": "2025-04-21", "relevancy": 2.0896, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5563}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5334}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformalized-KANs%3A%20Uncertainty%20Quantification%20with%20Coverage%20Guarantees%0A%20%20for%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20in%20Scientific%20Machine%20Learning&body=Title%3A%20Conformalized-KANs%3A%20Uncertainty%20Quantification%20with%20Coverage%20Guarantees%0A%20%20for%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20in%20Scientific%20Machine%20Learning%0AAuthor%3A%20Amirhossein%20Mollaali%20and%20Christian%20Bolivar%20Moya%20and%20Amanda%20A.%20Howard%20and%20Alexander%20Heinlein%20and%20Panos%20Stinis%20and%20Guang%20Lin%0AAbstract%3A%20%20%20This%20paper%20explores%20uncertainty%20quantification%20%28UQ%29%20methods%20in%20the%20context%20of%0AKolmogorov-Arnold%20Networks%20%28KANs%29.%20We%20apply%20an%20ensemble%20approach%20to%20KANs%20to%0Aobtain%20a%20heuristic%20measure%20of%20UQ%2C%20enhancing%20interpretability%20and%20robustness%20in%0Amodeling%20complex%20functions.%20Building%20on%20this%2C%20we%20introduce%20Conformalized-KANs%2C%0Awhich%20integrate%20conformal%20prediction%2C%20a%20distribution-free%20UQ%20technique%2C%20with%0AKAN%20ensembles%20to%20generate%20calibrated%20prediction%20intervals%20with%20guaranteed%0Acoverage.%20Extensive%20numerical%20experiments%20are%20conducted%20to%20evaluate%20the%0Aeffectiveness%20of%20these%20methods%2C%20focusing%20particularly%20on%20the%20robustness%20and%0Aaccuracy%20of%20the%20prediction%20intervals%20under%20various%20hyperparameter%20settings.%20We%0Ashow%20that%20the%20conformal%20KAN%20predictions%20can%20be%20applied%20to%20recent%20extensions%20of%0AKANs%2C%20including%20Finite%20Basis%20KANs%20%28FBKANs%29%20and%20multifideilty%20KANs%20%28MFKANs%29.%20The%0Aresults%20demonstrate%20the%20potential%20of%20our%20approaches%20to%20improve%20the%20reliability%0Aand%20applicability%20of%20KANs%20in%20scientific%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformalized-KANs%253A%2520Uncertainty%2520Quantification%2520with%2520Coverage%2520Guarantees%250A%2520%2520for%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520in%2520Scientific%2520Machine%2520Learning%26entry.906535625%3DAmirhossein%2520Mollaali%2520and%2520Christian%2520Bolivar%2520Moya%2520and%2520Amanda%2520A.%2520Howard%2520and%2520Alexander%2520Heinlein%2520and%2520Panos%2520Stinis%2520and%2520Guang%2520Lin%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520uncertainty%2520quantification%2520%2528UQ%2529%2520methods%2520in%2520the%2520context%2520of%250AKolmogorov-Arnold%2520Networks%2520%2528KANs%2529.%2520We%2520apply%2520an%2520ensemble%2520approach%2520to%2520KANs%2520to%250Aobtain%2520a%2520heuristic%2520measure%2520of%2520UQ%252C%2520enhancing%2520interpretability%2520and%2520robustness%2520in%250Amodeling%2520complex%2520functions.%2520Building%2520on%2520this%252C%2520we%2520introduce%2520Conformalized-KANs%252C%250Awhich%2520integrate%2520conformal%2520prediction%252C%2520a%2520distribution-free%2520UQ%2520technique%252C%2520with%250AKAN%2520ensembles%2520to%2520generate%2520calibrated%2520prediction%2520intervals%2520with%2520guaranteed%250Acoverage.%2520Extensive%2520numerical%2520experiments%2520are%2520conducted%2520to%2520evaluate%2520the%250Aeffectiveness%2520of%2520these%2520methods%252C%2520focusing%2520particularly%2520on%2520the%2520robustness%2520and%250Aaccuracy%2520of%2520the%2520prediction%2520intervals%2520under%2520various%2520hyperparameter%2520settings.%2520We%250Ashow%2520that%2520the%2520conformal%2520KAN%2520predictions%2520can%2520be%2520applied%2520to%2520recent%2520extensions%2520of%250AKANs%252C%2520including%2520Finite%2520Basis%2520KANs%2520%2528FBKANs%2529%2520and%2520multifideilty%2520KANs%2520%2528MFKANs%2529.%2520The%250Aresults%2520demonstrate%2520the%2520potential%2520of%2520our%2520approaches%2520to%2520improve%2520the%2520reliability%250Aand%2520applicability%2520of%2520KANs%2520in%2520scientific%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformalized-KANs%3A%20Uncertainty%20Quantification%20with%20Coverage%20Guarantees%0A%20%20for%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20in%20Scientific%20Machine%20Learning&entry.906535625=Amirhossein%20Mollaali%20and%20Christian%20Bolivar%20Moya%20and%20Amanda%20A.%20Howard%20and%20Alexander%20Heinlein%20and%20Panos%20Stinis%20and%20Guang%20Lin&entry.1292438233=%20%20This%20paper%20explores%20uncertainty%20quantification%20%28UQ%29%20methods%20in%20the%20context%20of%0AKolmogorov-Arnold%20Networks%20%28KANs%29.%20We%20apply%20an%20ensemble%20approach%20to%20KANs%20to%0Aobtain%20a%20heuristic%20measure%20of%20UQ%2C%20enhancing%20interpretability%20and%20robustness%20in%0Amodeling%20complex%20functions.%20Building%20on%20this%2C%20we%20introduce%20Conformalized-KANs%2C%0Awhich%20integrate%20conformal%20prediction%2C%20a%20distribution-free%20UQ%20technique%2C%20with%0AKAN%20ensembles%20to%20generate%20calibrated%20prediction%20intervals%20with%20guaranteed%0Acoverage.%20Extensive%20numerical%20experiments%20are%20conducted%20to%20evaluate%20the%0Aeffectiveness%20of%20these%20methods%2C%20focusing%20particularly%20on%20the%20robustness%20and%0Aaccuracy%20of%20the%20prediction%20intervals%20under%20various%20hyperparameter%20settings.%20We%0Ashow%20that%20the%20conformal%20KAN%20predictions%20can%20be%20applied%20to%20recent%20extensions%20of%0AKANs%2C%20including%20Finite%20Basis%20KANs%20%28FBKANs%29%20and%20multifideilty%20KANs%20%28MFKANs%29.%20The%0Aresults%20demonstrate%20the%20potential%20of%20our%20approaches%20to%20improve%20the%20reliability%0Aand%20applicability%20of%20KANs%20in%20scientific%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15240v1&entry.124074799=Read"},
{"title": "Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training\n  of GAN", "author": "Lin Wang and Xiancheng Wang and Rui Wang and Zhibo Zhang and Minghang Zhao", "abstract": "  Up to now, the training processes of typical Generative Adversarial Networks\n(GANs) are still particularly sensitive to data properties and hyperparameters,\nwhich may lead to severe oscillations, difficulties in convergence, or even\nfailures to converge, especially when the overall variances of the training\nsets are large. These phenomena are often attributed to the training\ncharacteristics of such networks. Aiming at the problem, this paper develops a\nnew intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which\nemploys reinforcement learning in the training process of GANs to make training\neasier. Specifically, this paper allows the training step size to be controlled\nby an agent to improve training stability, and makes the training process more\nintelligent with variable learning rates, making GANs less sensitive to step\nsize. Experiments have been conducted on three benchmark datasets to verify the\neffectiveness of the developed FSCO.\n", "link": "http://arxiv.org/abs/2504.15099v1", "date": "2025-04-21", "relevancy": 2.0849, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5305}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5168}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast-Slow%20Co-advancing%20Optimizer%3A%20Toward%20Harmonious%20Adversarial%20Training%0A%20%20of%20GAN&body=Title%3A%20Fast-Slow%20Co-advancing%20Optimizer%3A%20Toward%20Harmonious%20Adversarial%20Training%0A%20%20of%20GAN%0AAuthor%3A%20Lin%20Wang%20and%20Xiancheng%20Wang%20and%20Rui%20Wang%20and%20Zhibo%20Zhang%20and%20Minghang%20Zhao%0AAbstract%3A%20%20%20Up%20to%20now%2C%20the%20training%20processes%20of%20typical%20Generative%20Adversarial%20Networks%0A%28GANs%29%20are%20still%20particularly%20sensitive%20to%20data%20properties%20and%20hyperparameters%2C%0Awhich%20may%20lead%20to%20severe%20oscillations%2C%20difficulties%20in%20convergence%2C%20or%20even%0Afailures%20to%20converge%2C%20especially%20when%20the%20overall%20variances%20of%20the%20training%0Asets%20are%20large.%20These%20phenomena%20are%20often%20attributed%20to%20the%20training%0Acharacteristics%20of%20such%20networks.%20Aiming%20at%20the%20problem%2C%20this%20paper%20develops%20a%0Anew%20intelligent%20optimizer%2C%20Fast-Slow%20Co-advancing%20Optimizer%20%28FSCO%29%2C%20which%0Aemploys%20reinforcement%20learning%20in%20the%20training%20process%20of%20GANs%20to%20make%20training%0Aeasier.%20Specifically%2C%20this%20paper%20allows%20the%20training%20step%20size%20to%20be%20controlled%0Aby%20an%20agent%20to%20improve%20training%20stability%2C%20and%20makes%20the%20training%20process%20more%0Aintelligent%20with%20variable%20learning%20rates%2C%20making%20GANs%20less%20sensitive%20to%20step%0Asize.%20Experiments%20have%20been%20conducted%20on%20three%20benchmark%20datasets%20to%20verify%20the%0Aeffectiveness%20of%20the%20developed%20FSCO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast-Slow%2520Co-advancing%2520Optimizer%253A%2520Toward%2520Harmonious%2520Adversarial%2520Training%250A%2520%2520of%2520GAN%26entry.906535625%3DLin%2520Wang%2520and%2520Xiancheng%2520Wang%2520and%2520Rui%2520Wang%2520and%2520Zhibo%2520Zhang%2520and%2520Minghang%2520Zhao%26entry.1292438233%3D%2520%2520Up%2520to%2520now%252C%2520the%2520training%2520processes%2520of%2520typical%2520Generative%2520Adversarial%2520Networks%250A%2528GANs%2529%2520are%2520still%2520particularly%2520sensitive%2520to%2520data%2520properties%2520and%2520hyperparameters%252C%250Awhich%2520may%2520lead%2520to%2520severe%2520oscillations%252C%2520difficulties%2520in%2520convergence%252C%2520or%2520even%250Afailures%2520to%2520converge%252C%2520especially%2520when%2520the%2520overall%2520variances%2520of%2520the%2520training%250Asets%2520are%2520large.%2520These%2520phenomena%2520are%2520often%2520attributed%2520to%2520the%2520training%250Acharacteristics%2520of%2520such%2520networks.%2520Aiming%2520at%2520the%2520problem%252C%2520this%2520paper%2520develops%2520a%250Anew%2520intelligent%2520optimizer%252C%2520Fast-Slow%2520Co-advancing%2520Optimizer%2520%2528FSCO%2529%252C%2520which%250Aemploys%2520reinforcement%2520learning%2520in%2520the%2520training%2520process%2520of%2520GANs%2520to%2520make%2520training%250Aeasier.%2520Specifically%252C%2520this%2520paper%2520allows%2520the%2520training%2520step%2520size%2520to%2520be%2520controlled%250Aby%2520an%2520agent%2520to%2520improve%2520training%2520stability%252C%2520and%2520makes%2520the%2520training%2520process%2520more%250Aintelligent%2520with%2520variable%2520learning%2520rates%252C%2520making%2520GANs%2520less%2520sensitive%2520to%2520step%250Asize.%2520Experiments%2520have%2520been%2520conducted%2520on%2520three%2520benchmark%2520datasets%2520to%2520verify%2520the%250Aeffectiveness%2520of%2520the%2520developed%2520FSCO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast-Slow%20Co-advancing%20Optimizer%3A%20Toward%20Harmonious%20Adversarial%20Training%0A%20%20of%20GAN&entry.906535625=Lin%20Wang%20and%20Xiancheng%20Wang%20and%20Rui%20Wang%20and%20Zhibo%20Zhang%20and%20Minghang%20Zhao&entry.1292438233=%20%20Up%20to%20now%2C%20the%20training%20processes%20of%20typical%20Generative%20Adversarial%20Networks%0A%28GANs%29%20are%20still%20particularly%20sensitive%20to%20data%20properties%20and%20hyperparameters%2C%0Awhich%20may%20lead%20to%20severe%20oscillations%2C%20difficulties%20in%20convergence%2C%20or%20even%0Afailures%20to%20converge%2C%20especially%20when%20the%20overall%20variances%20of%20the%20training%0Asets%20are%20large.%20These%20phenomena%20are%20often%20attributed%20to%20the%20training%0Acharacteristics%20of%20such%20networks.%20Aiming%20at%20the%20problem%2C%20this%20paper%20develops%20a%0Anew%20intelligent%20optimizer%2C%20Fast-Slow%20Co-advancing%20Optimizer%20%28FSCO%29%2C%20which%0Aemploys%20reinforcement%20learning%20in%20the%20training%20process%20of%20GANs%20to%20make%20training%0Aeasier.%20Specifically%2C%20this%20paper%20allows%20the%20training%20step%20size%20to%20be%20controlled%0Aby%20an%20agent%20to%20improve%20training%20stability%2C%20and%20makes%20the%20training%20process%20more%0Aintelligent%20with%20variable%20learning%20rates%2C%20making%20GANs%20less%20sensitive%20to%20step%0Asize.%20Experiments%20have%20been%20conducted%20on%20three%20benchmark%20datasets%20to%20verify%20the%0Aeffectiveness%20of%20the%20developed%20FSCO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15099v1&entry.124074799=Read"},
{"title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "author": "Shijie Xia and Yiwei Qin and Xuefeng Li and Yan Ma and Run-Ze Fan and Steffi Chern and Haoyang Zou and Fan Zhou and Xiangkun Hu and Jiahe Jin and Yanheng He and Yixin Ye and Yixiu Liu and Pengfei Liu", "abstract": "  The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations such as\nknowledge latency, shallow reasoning, and constrained cognitive processes.\nDuring this era, prompt engineering emerged as our primary interface with AI,\nenabling dialogue-level communication through natural language. We now witness\nthe emergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering\n", "link": "http://arxiv.org/abs/2504.13828v2", "date": "2025-04-21", "relevancy": 2.0839, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5579}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4998}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20Act%20II%3A%20Test%20Time%20Scaling%20Drives%20Cognition%20Engineering&body=Title%3A%20Generative%20AI%20Act%20II%3A%20Test%20Time%20Scaling%20Drives%20Cognition%20Engineering%0AAuthor%3A%20Shijie%20Xia%20and%20Yiwei%20Qin%20and%20Xuefeng%20Li%20and%20Yan%20Ma%20and%20Run-Ze%20Fan%20and%20Steffi%20Chern%20and%20Haoyang%20Zou%20and%20Fan%20Zhou%20and%20Xiangkun%20Hu%20and%20Jiahe%20Jin%20and%20Yanheng%20He%20and%20Yixin%20Ye%20and%20Yixiu%20Liu%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20The%20first%20generation%20of%20Large%20Language%20Models%20-%20what%20might%20be%20called%20%22Act%20I%22%0Aof%20generative%20AI%20%282020-2023%29%20-%20achieved%20remarkable%20success%20through%20massive%0Aparameter%20and%20data%20scaling%2C%20yet%20exhibited%20fundamental%20limitations%20such%20as%0Aknowledge%20latency%2C%20shallow%20reasoning%2C%20and%20constrained%20cognitive%20processes.%0ADuring%20this%20era%2C%20prompt%20engineering%20emerged%20as%20our%20primary%20interface%20with%20AI%2C%0Aenabling%20dialogue-level%20communication%20through%20natural%20language.%20We%20now%20witness%0Athe%20emergence%20of%20%22Act%20II%22%20%282024-present%29%2C%20where%20models%20are%20transitioning%20from%0Aknowledge-retrieval%20systems%20%28in%20latent%20space%29%20to%20thought-construction%20engines%0Athrough%20test-time%20scaling%20techniques.%20This%20new%20paradigm%20establishes%20a%0Amind-level%20connection%20with%20AI%20through%20language-based%20thoughts.%20In%20this%20paper%2C%0Awe%20clarify%20the%20conceptual%20foundations%20of%20cognition%20engineering%20and%20explain%20why%0Athis%20moment%20is%20critical%20for%20its%20development.%20We%20systematically%20break%20down%20these%0Aadvanced%20approaches%20through%20comprehensive%20tutorials%20and%20optimized%0Aimplementations%2C%20democratizing%20access%20to%20cognition%20engineering%20and%20enabling%0Aevery%20practitioner%20to%20participate%20in%20AI%27s%20second%20act.%20We%20provide%20a%20regularly%0Aupdated%20collection%20of%20papers%20on%20test-time%20scaling%20in%20the%20GitHub%20Repository%3A%0Ahttps%3A//github.com/GAIR-NLP/cognition-engineering%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520Act%2520II%253A%2520Test%2520Time%2520Scaling%2520Drives%2520Cognition%2520Engineering%26entry.906535625%3DShijie%2520Xia%2520and%2520Yiwei%2520Qin%2520and%2520Xuefeng%2520Li%2520and%2520Yan%2520Ma%2520and%2520Run-Ze%2520Fan%2520and%2520Steffi%2520Chern%2520and%2520Haoyang%2520Zou%2520and%2520Fan%2520Zhou%2520and%2520Xiangkun%2520Hu%2520and%2520Jiahe%2520Jin%2520and%2520Yanheng%2520He%2520and%2520Yixin%2520Ye%2520and%2520Yixiu%2520Liu%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520The%2520first%2520generation%2520of%2520Large%2520Language%2520Models%2520-%2520what%2520might%2520be%2520called%2520%2522Act%2520I%2522%250Aof%2520generative%2520AI%2520%25282020-2023%2529%2520-%2520achieved%2520remarkable%2520success%2520through%2520massive%250Aparameter%2520and%2520data%2520scaling%252C%2520yet%2520exhibited%2520fundamental%2520limitations%2520such%2520as%250Aknowledge%2520latency%252C%2520shallow%2520reasoning%252C%2520and%2520constrained%2520cognitive%2520processes.%250ADuring%2520this%2520era%252C%2520prompt%2520engineering%2520emerged%2520as%2520our%2520primary%2520interface%2520with%2520AI%252C%250Aenabling%2520dialogue-level%2520communication%2520through%2520natural%2520language.%2520We%2520now%2520witness%250Athe%2520emergence%2520of%2520%2522Act%2520II%2522%2520%25282024-present%2529%252C%2520where%2520models%2520are%2520transitioning%2520from%250Aknowledge-retrieval%2520systems%2520%2528in%2520latent%2520space%2529%2520to%2520thought-construction%2520engines%250Athrough%2520test-time%2520scaling%2520techniques.%2520This%2520new%2520paradigm%2520establishes%2520a%250Amind-level%2520connection%2520with%2520AI%2520through%2520language-based%2520thoughts.%2520In%2520this%2520paper%252C%250Awe%2520clarify%2520the%2520conceptual%2520foundations%2520of%2520cognition%2520engineering%2520and%2520explain%2520why%250Athis%2520moment%2520is%2520critical%2520for%2520its%2520development.%2520We%2520systematically%2520break%2520down%2520these%250Aadvanced%2520approaches%2520through%2520comprehensive%2520tutorials%2520and%2520optimized%250Aimplementations%252C%2520democratizing%2520access%2520to%2520cognition%2520engineering%2520and%2520enabling%250Aevery%2520practitioner%2520to%2520participate%2520in%2520AI%2527s%2520second%2520act.%2520We%2520provide%2520a%2520regularly%250Aupdated%2520collection%2520of%2520papers%2520on%2520test-time%2520scaling%2520in%2520the%2520GitHub%2520Repository%253A%250Ahttps%253A//github.com/GAIR-NLP/cognition-engineering%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20Act%20II%3A%20Test%20Time%20Scaling%20Drives%20Cognition%20Engineering&entry.906535625=Shijie%20Xia%20and%20Yiwei%20Qin%20and%20Xuefeng%20Li%20and%20Yan%20Ma%20and%20Run-Ze%20Fan%20and%20Steffi%20Chern%20and%20Haoyang%20Zou%20and%20Fan%20Zhou%20and%20Xiangkun%20Hu%20and%20Jiahe%20Jin%20and%20Yanheng%20He%20and%20Yixin%20Ye%20and%20Yixiu%20Liu%20and%20Pengfei%20Liu&entry.1292438233=%20%20The%20first%20generation%20of%20Large%20Language%20Models%20-%20what%20might%20be%20called%20%22Act%20I%22%0Aof%20generative%20AI%20%282020-2023%29%20-%20achieved%20remarkable%20success%20through%20massive%0Aparameter%20and%20data%20scaling%2C%20yet%20exhibited%20fundamental%20limitations%20such%20as%0Aknowledge%20latency%2C%20shallow%20reasoning%2C%20and%20constrained%20cognitive%20processes.%0ADuring%20this%20era%2C%20prompt%20engineering%20emerged%20as%20our%20primary%20interface%20with%20AI%2C%0Aenabling%20dialogue-level%20communication%20through%20natural%20language.%20We%20now%20witness%0Athe%20emergence%20of%20%22Act%20II%22%20%282024-present%29%2C%20where%20models%20are%20transitioning%20from%0Aknowledge-retrieval%20systems%20%28in%20latent%20space%29%20to%20thought-construction%20engines%0Athrough%20test-time%20scaling%20techniques.%20This%20new%20paradigm%20establishes%20a%0Amind-level%20connection%20with%20AI%20through%20language-based%20thoughts.%20In%20this%20paper%2C%0Awe%20clarify%20the%20conceptual%20foundations%20of%20cognition%20engineering%20and%20explain%20why%0Athis%20moment%20is%20critical%20for%20its%20development.%20We%20systematically%20break%20down%20these%0Aadvanced%20approaches%20through%20comprehensive%20tutorials%20and%20optimized%0Aimplementations%2C%20democratizing%20access%20to%20cognition%20engineering%20and%20enabling%0Aevery%20practitioner%20to%20participate%20in%20AI%27s%20second%20act.%20We%20provide%20a%20regularly%0Aupdated%20collection%20of%20papers%20on%20test-time%20scaling%20in%20the%20GitHub%20Repository%3A%0Ahttps%3A//github.com/GAIR-NLP/cognition-engineering%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13828v2&entry.124074799=Read"},
{"title": "Mitigating Degree Bias in Graph Representation Learning with Learnable\n  Structural Augmentation and Structural Self-Attention", "author": "Van Thuy Hoang and Hyeon-Ju Jeon and O-Joun Lee", "abstract": "  Graph Neural Networks (GNNs) update node representations through message\npassing, which is primarily based on the homophily principle, assuming that\nadjacent nodes share similar features. However, in real-world graphs with\nlong-tailed degree distributions, high-degree nodes dominate message passing,\ncausing a degree bias where low-degree nodes remain under-represented due to\ninadequate messages. The main challenge in addressing degree bias is how to\ndiscover non-adjacent nodes to provide additional messages to low-degree nodes\nwhile reducing excessive messages for high-degree nodes. Nevertheless,\nexploiting non-adjacent nodes to provide valuable messages is challenging, as\nit could generate noisy information and disrupt the original graph structures.\nTo solve it, we propose a novel Degree Fairness Graph Transformer, named\nDegFairGT, to mitigate degree bias by discovering structural similarities\nbetween non-adjacent nodes through learnable structural augmentation and\nstructural self-attention. Our key idea is to exploit non-adjacent nodes with\nsimilar roles in the same community to generate informative edges under our\naugmentation, which could provide informative messages between nodes with\nsimilar roles while ensuring that the homophily principle is maintained within\nthe community. To enable DegFairGT to learn such structural similarities, we\nthen propose a structural self-attention to capture the similarities between\nnode pairs. To preserve global graph structures and prevent graph augmentation\nfrom hindering graph structure, we propose a Self-Supervised Learning task to\npreserve p-step transition probability and regularize graph augmentation.\nExtensive experiments on six datasets showed that DegFairGT outperformed\nstate-of-the-art baselines in degree fairness analysis, node classification,\nand node clustering tasks.\n", "link": "http://arxiv.org/abs/2504.15075v1", "date": "2025-04-21", "relevancy": 2.0777, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5404}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5062}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Degree%20Bias%20in%20Graph%20Representation%20Learning%20with%20Learnable%0A%20%20Structural%20Augmentation%20and%20Structural%20Self-Attention&body=Title%3A%20Mitigating%20Degree%20Bias%20in%20Graph%20Representation%20Learning%20with%20Learnable%0A%20%20Structural%20Augmentation%20and%20Structural%20Self-Attention%0AAuthor%3A%20Van%20Thuy%20Hoang%20and%20Hyeon-Ju%20Jeon%20and%20O-Joun%20Lee%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20update%20node%20representations%20through%20message%0Apassing%2C%20which%20is%20primarily%20based%20on%20the%20homophily%20principle%2C%20assuming%20that%0Aadjacent%20nodes%20share%20similar%20features.%20However%2C%20in%20real-world%20graphs%20with%0Along-tailed%20degree%20distributions%2C%20high-degree%20nodes%20dominate%20message%20passing%2C%0Acausing%20a%20degree%20bias%20where%20low-degree%20nodes%20remain%20under-represented%20due%20to%0Ainadequate%20messages.%20The%20main%20challenge%20in%20addressing%20degree%20bias%20is%20how%20to%0Adiscover%20non-adjacent%20nodes%20to%20provide%20additional%20messages%20to%20low-degree%20nodes%0Awhile%20reducing%20excessive%20messages%20for%20high-degree%20nodes.%20Nevertheless%2C%0Aexploiting%20non-adjacent%20nodes%20to%20provide%20valuable%20messages%20is%20challenging%2C%20as%0Ait%20could%20generate%20noisy%20information%20and%20disrupt%20the%20original%20graph%20structures.%0ATo%20solve%20it%2C%20we%20propose%20a%20novel%20Degree%20Fairness%20Graph%20Transformer%2C%20named%0ADegFairGT%2C%20to%20mitigate%20degree%20bias%20by%20discovering%20structural%20similarities%0Abetween%20non-adjacent%20nodes%20through%20learnable%20structural%20augmentation%20and%0Astructural%20self-attention.%20Our%20key%20idea%20is%20to%20exploit%20non-adjacent%20nodes%20with%0Asimilar%20roles%20in%20the%20same%20community%20to%20generate%20informative%20edges%20under%20our%0Aaugmentation%2C%20which%20could%20provide%20informative%20messages%20between%20nodes%20with%0Asimilar%20roles%20while%20ensuring%20that%20the%20homophily%20principle%20is%20maintained%20within%0Athe%20community.%20To%20enable%20DegFairGT%20to%20learn%20such%20structural%20similarities%2C%20we%0Athen%20propose%20a%20structural%20self-attention%20to%20capture%20the%20similarities%20between%0Anode%20pairs.%20To%20preserve%20global%20graph%20structures%20and%20prevent%20graph%20augmentation%0Afrom%20hindering%20graph%20structure%2C%20we%20propose%20a%20Self-Supervised%20Learning%20task%20to%0Apreserve%20p-step%20transition%20probability%20and%20regularize%20graph%20augmentation.%0AExtensive%20experiments%20on%20six%20datasets%20showed%20that%20DegFairGT%20outperformed%0Astate-of-the-art%20baselines%20in%20degree%20fairness%20analysis%2C%20node%20classification%2C%0Aand%20node%20clustering%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Degree%2520Bias%2520in%2520Graph%2520Representation%2520Learning%2520with%2520Learnable%250A%2520%2520Structural%2520Augmentation%2520and%2520Structural%2520Self-Attention%26entry.906535625%3DVan%2520Thuy%2520Hoang%2520and%2520Hyeon-Ju%2520Jeon%2520and%2520O-Joun%2520Lee%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520update%2520node%2520representations%2520through%2520message%250Apassing%252C%2520which%2520is%2520primarily%2520based%2520on%2520the%2520homophily%2520principle%252C%2520assuming%2520that%250Aadjacent%2520nodes%2520share%2520similar%2520features.%2520However%252C%2520in%2520real-world%2520graphs%2520with%250Along-tailed%2520degree%2520distributions%252C%2520high-degree%2520nodes%2520dominate%2520message%2520passing%252C%250Acausing%2520a%2520degree%2520bias%2520where%2520low-degree%2520nodes%2520remain%2520under-represented%2520due%2520to%250Ainadequate%2520messages.%2520The%2520main%2520challenge%2520in%2520addressing%2520degree%2520bias%2520is%2520how%2520to%250Adiscover%2520non-adjacent%2520nodes%2520to%2520provide%2520additional%2520messages%2520to%2520low-degree%2520nodes%250Awhile%2520reducing%2520excessive%2520messages%2520for%2520high-degree%2520nodes.%2520Nevertheless%252C%250Aexploiting%2520non-adjacent%2520nodes%2520to%2520provide%2520valuable%2520messages%2520is%2520challenging%252C%2520as%250Ait%2520could%2520generate%2520noisy%2520information%2520and%2520disrupt%2520the%2520original%2520graph%2520structures.%250ATo%2520solve%2520it%252C%2520we%2520propose%2520a%2520novel%2520Degree%2520Fairness%2520Graph%2520Transformer%252C%2520named%250ADegFairGT%252C%2520to%2520mitigate%2520degree%2520bias%2520by%2520discovering%2520structural%2520similarities%250Abetween%2520non-adjacent%2520nodes%2520through%2520learnable%2520structural%2520augmentation%2520and%250Astructural%2520self-attention.%2520Our%2520key%2520idea%2520is%2520to%2520exploit%2520non-adjacent%2520nodes%2520with%250Asimilar%2520roles%2520in%2520the%2520same%2520community%2520to%2520generate%2520informative%2520edges%2520under%2520our%250Aaugmentation%252C%2520which%2520could%2520provide%2520informative%2520messages%2520between%2520nodes%2520with%250Asimilar%2520roles%2520while%2520ensuring%2520that%2520the%2520homophily%2520principle%2520is%2520maintained%2520within%250Athe%2520community.%2520To%2520enable%2520DegFairGT%2520to%2520learn%2520such%2520structural%2520similarities%252C%2520we%250Athen%2520propose%2520a%2520structural%2520self-attention%2520to%2520capture%2520the%2520similarities%2520between%250Anode%2520pairs.%2520To%2520preserve%2520global%2520graph%2520structures%2520and%2520prevent%2520graph%2520augmentation%250Afrom%2520hindering%2520graph%2520structure%252C%2520we%2520propose%2520a%2520Self-Supervised%2520Learning%2520task%2520to%250Apreserve%2520p-step%2520transition%2520probability%2520and%2520regularize%2520graph%2520augmentation.%250AExtensive%2520experiments%2520on%2520six%2520datasets%2520showed%2520that%2520DegFairGT%2520outperformed%250Astate-of-the-art%2520baselines%2520in%2520degree%2520fairness%2520analysis%252C%2520node%2520classification%252C%250Aand%2520node%2520clustering%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Degree%20Bias%20in%20Graph%20Representation%20Learning%20with%20Learnable%0A%20%20Structural%20Augmentation%20and%20Structural%20Self-Attention&entry.906535625=Van%20Thuy%20Hoang%20and%20Hyeon-Ju%20Jeon%20and%20O-Joun%20Lee&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20update%20node%20representations%20through%20message%0Apassing%2C%20which%20is%20primarily%20based%20on%20the%20homophily%20principle%2C%20assuming%20that%0Aadjacent%20nodes%20share%20similar%20features.%20However%2C%20in%20real-world%20graphs%20with%0Along-tailed%20degree%20distributions%2C%20high-degree%20nodes%20dominate%20message%20passing%2C%0Acausing%20a%20degree%20bias%20where%20low-degree%20nodes%20remain%20under-represented%20due%20to%0Ainadequate%20messages.%20The%20main%20challenge%20in%20addressing%20degree%20bias%20is%20how%20to%0Adiscover%20non-adjacent%20nodes%20to%20provide%20additional%20messages%20to%20low-degree%20nodes%0Awhile%20reducing%20excessive%20messages%20for%20high-degree%20nodes.%20Nevertheless%2C%0Aexploiting%20non-adjacent%20nodes%20to%20provide%20valuable%20messages%20is%20challenging%2C%20as%0Ait%20could%20generate%20noisy%20information%20and%20disrupt%20the%20original%20graph%20structures.%0ATo%20solve%20it%2C%20we%20propose%20a%20novel%20Degree%20Fairness%20Graph%20Transformer%2C%20named%0ADegFairGT%2C%20to%20mitigate%20degree%20bias%20by%20discovering%20structural%20similarities%0Abetween%20non-adjacent%20nodes%20through%20learnable%20structural%20augmentation%20and%0Astructural%20self-attention.%20Our%20key%20idea%20is%20to%20exploit%20non-adjacent%20nodes%20with%0Asimilar%20roles%20in%20the%20same%20community%20to%20generate%20informative%20edges%20under%20our%0Aaugmentation%2C%20which%20could%20provide%20informative%20messages%20between%20nodes%20with%0Asimilar%20roles%20while%20ensuring%20that%20the%20homophily%20principle%20is%20maintained%20within%0Athe%20community.%20To%20enable%20DegFairGT%20to%20learn%20such%20structural%20similarities%2C%20we%0Athen%20propose%20a%20structural%20self-attention%20to%20capture%20the%20similarities%20between%0Anode%20pairs.%20To%20preserve%20global%20graph%20structures%20and%20prevent%20graph%20augmentation%0Afrom%20hindering%20graph%20structure%2C%20we%20propose%20a%20Self-Supervised%20Learning%20task%20to%0Apreserve%20p-step%20transition%20probability%20and%20regularize%20graph%20augmentation.%0AExtensive%20experiments%20on%20six%20datasets%20showed%20that%20DegFairGT%20outperformed%0Astate-of-the-art%20baselines%20in%20degree%20fairness%20analysis%2C%20node%20classification%2C%0Aand%20node%20clustering%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15075v1&entry.124074799=Read"},
{"title": "SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank\n  Adaptation", "author": "Yue Li and Weizhi Liu and Dongdong Lin", "abstract": "  The accelerated advancement of speech generative models has given rise to\nsecurity issues, including model infringement and unauthorized abuse of\ncontent. Although existing generative watermarking techniques have proposed\ncorresponding solutions, most methods require substantial computational\noverhead and training costs. In addition, some methods have limitations in\nrobustness when handling variable-length inputs. To tackle these challenges, we\npropose \\textsc{SOLIDO}, a novel generative watermarking method that integrates\nparameter-efficient fine-tuning with speech watermarking through low-rank\nadaptation (LoRA) for speech diffusion models. Concretely, the watermark\nencoder converts the watermark to align with the input of diffusion models. To\nachieve precise watermark extraction from variable-length inputs, the watermark\ndecoder based on depthwise separable convolution is designed for watermark\nrecovery. To further enhance speech generation performance and watermark\nextraction capability, we propose a speech-driven lightweight fine-tuning\nstrategy, which reduces computational overhead through LoRA. Comprehensive\nexperiments demonstrate that the proposed method ensures high-fidelity\nwatermarked speech even at a large capacity of 2000 bps. Furthermore, against\ncommon individual and compound speech attacks, our SOLIDO achieves a maximum\naverage extraction accuracy of 99.20\\% and 98.43\\%, respectively. It surpasses\nother state-of-the-art methods by nearly 23\\% in resisting time-stretching\nattacks.\n", "link": "http://arxiv.org/abs/2504.15035v1", "date": "2025-04-21", "relevancy": 2.0776, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5332}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5219}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOLIDO%3A%20A%20Robust%20Watermarking%20Method%20for%20Speech%20Synthesis%20via%20Low-Rank%0A%20%20Adaptation&body=Title%3A%20SOLIDO%3A%20A%20Robust%20Watermarking%20Method%20for%20Speech%20Synthesis%20via%20Low-Rank%0A%20%20Adaptation%0AAuthor%3A%20Yue%20Li%20and%20Weizhi%20Liu%20and%20Dongdong%20Lin%0AAbstract%3A%20%20%20The%20accelerated%20advancement%20of%20speech%20generative%20models%20has%20given%20rise%20to%0Asecurity%20issues%2C%20including%20model%20infringement%20and%20unauthorized%20abuse%20of%0Acontent.%20Although%20existing%20generative%20watermarking%20techniques%20have%20proposed%0Acorresponding%20solutions%2C%20most%20methods%20require%20substantial%20computational%0Aoverhead%20and%20training%20costs.%20In%20addition%2C%20some%20methods%20have%20limitations%20in%0Arobustness%20when%20handling%20variable-length%20inputs.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20%5Ctextsc%7BSOLIDO%7D%2C%20a%20novel%20generative%20watermarking%20method%20that%20integrates%0Aparameter-efficient%20fine-tuning%20with%20speech%20watermarking%20through%20low-rank%0Aadaptation%20%28LoRA%29%20for%20speech%20diffusion%20models.%20Concretely%2C%20the%20watermark%0Aencoder%20converts%20the%20watermark%20to%20align%20with%20the%20input%20of%20diffusion%20models.%20To%0Aachieve%20precise%20watermark%20extraction%20from%20variable-length%20inputs%2C%20the%20watermark%0Adecoder%20based%20on%20depthwise%20separable%20convolution%20is%20designed%20for%20watermark%0Arecovery.%20To%20further%20enhance%20speech%20generation%20performance%20and%20watermark%0Aextraction%20capability%2C%20we%20propose%20a%20speech-driven%20lightweight%20fine-tuning%0Astrategy%2C%20which%20reduces%20computational%20overhead%20through%20LoRA.%20Comprehensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20method%20ensures%20high-fidelity%0Awatermarked%20speech%20even%20at%20a%20large%20capacity%20of%202000%20bps.%20Furthermore%2C%20against%0Acommon%20individual%20and%20compound%20speech%20attacks%2C%20our%20SOLIDO%20achieves%20a%20maximum%0Aaverage%20extraction%20accuracy%20of%2099.20%5C%25%20and%2098.43%5C%25%2C%20respectively.%20It%20surpasses%0Aother%20state-of-the-art%20methods%20by%20nearly%2023%5C%25%20in%20resisting%20time-stretching%0Aattacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOLIDO%253A%2520A%2520Robust%2520Watermarking%2520Method%2520for%2520Speech%2520Synthesis%2520via%2520Low-Rank%250A%2520%2520Adaptation%26entry.906535625%3DYue%2520Li%2520and%2520Weizhi%2520Liu%2520and%2520Dongdong%2520Lin%26entry.1292438233%3D%2520%2520The%2520accelerated%2520advancement%2520of%2520speech%2520generative%2520models%2520has%2520given%2520rise%2520to%250Asecurity%2520issues%252C%2520including%2520model%2520infringement%2520and%2520unauthorized%2520abuse%2520of%250Acontent.%2520Although%2520existing%2520generative%2520watermarking%2520techniques%2520have%2520proposed%250Acorresponding%2520solutions%252C%2520most%2520methods%2520require%2520substantial%2520computational%250Aoverhead%2520and%2520training%2520costs.%2520In%2520addition%252C%2520some%2520methods%2520have%2520limitations%2520in%250Arobustness%2520when%2520handling%2520variable-length%2520inputs.%2520To%2520tackle%2520these%2520challenges%252C%2520we%250Apropose%2520%255Ctextsc%257BSOLIDO%257D%252C%2520a%2520novel%2520generative%2520watermarking%2520method%2520that%2520integrates%250Aparameter-efficient%2520fine-tuning%2520with%2520speech%2520watermarking%2520through%2520low-rank%250Aadaptation%2520%2528LoRA%2529%2520for%2520speech%2520diffusion%2520models.%2520Concretely%252C%2520the%2520watermark%250Aencoder%2520converts%2520the%2520watermark%2520to%2520align%2520with%2520the%2520input%2520of%2520diffusion%2520models.%2520To%250Aachieve%2520precise%2520watermark%2520extraction%2520from%2520variable-length%2520inputs%252C%2520the%2520watermark%250Adecoder%2520based%2520on%2520depthwise%2520separable%2520convolution%2520is%2520designed%2520for%2520watermark%250Arecovery.%2520To%2520further%2520enhance%2520speech%2520generation%2520performance%2520and%2520watermark%250Aextraction%2520capability%252C%2520we%2520propose%2520a%2520speech-driven%2520lightweight%2520fine-tuning%250Astrategy%252C%2520which%2520reduces%2520computational%2520overhead%2520through%2520LoRA.%2520Comprehensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520method%2520ensures%2520high-fidelity%250Awatermarked%2520speech%2520even%2520at%2520a%2520large%2520capacity%2520of%25202000%2520bps.%2520Furthermore%252C%2520against%250Acommon%2520individual%2520and%2520compound%2520speech%2520attacks%252C%2520our%2520SOLIDO%2520achieves%2520a%2520maximum%250Aaverage%2520extraction%2520accuracy%2520of%252099.20%255C%2525%2520and%252098.43%255C%2525%252C%2520respectively.%2520It%2520surpasses%250Aother%2520state-of-the-art%2520methods%2520by%2520nearly%252023%255C%2525%2520in%2520resisting%2520time-stretching%250Aattacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOLIDO%3A%20A%20Robust%20Watermarking%20Method%20for%20Speech%20Synthesis%20via%20Low-Rank%0A%20%20Adaptation&entry.906535625=Yue%20Li%20and%20Weizhi%20Liu%20and%20Dongdong%20Lin&entry.1292438233=%20%20The%20accelerated%20advancement%20of%20speech%20generative%20models%20has%20given%20rise%20to%0Asecurity%20issues%2C%20including%20model%20infringement%20and%20unauthorized%20abuse%20of%0Acontent.%20Although%20existing%20generative%20watermarking%20techniques%20have%20proposed%0Acorresponding%20solutions%2C%20most%20methods%20require%20substantial%20computational%0Aoverhead%20and%20training%20costs.%20In%20addition%2C%20some%20methods%20have%20limitations%20in%0Arobustness%20when%20handling%20variable-length%20inputs.%20To%20tackle%20these%20challenges%2C%20we%0Apropose%20%5Ctextsc%7BSOLIDO%7D%2C%20a%20novel%20generative%20watermarking%20method%20that%20integrates%0Aparameter-efficient%20fine-tuning%20with%20speech%20watermarking%20through%20low-rank%0Aadaptation%20%28LoRA%29%20for%20speech%20diffusion%20models.%20Concretely%2C%20the%20watermark%0Aencoder%20converts%20the%20watermark%20to%20align%20with%20the%20input%20of%20diffusion%20models.%20To%0Aachieve%20precise%20watermark%20extraction%20from%20variable-length%20inputs%2C%20the%20watermark%0Adecoder%20based%20on%20depthwise%20separable%20convolution%20is%20designed%20for%20watermark%0Arecovery.%20To%20further%20enhance%20speech%20generation%20performance%20and%20watermark%0Aextraction%20capability%2C%20we%20propose%20a%20speech-driven%20lightweight%20fine-tuning%0Astrategy%2C%20which%20reduces%20computational%20overhead%20through%20LoRA.%20Comprehensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20method%20ensures%20high-fidelity%0Awatermarked%20speech%20even%20at%20a%20large%20capacity%20of%202000%20bps.%20Furthermore%2C%20against%0Acommon%20individual%20and%20compound%20speech%20attacks%2C%20our%20SOLIDO%20achieves%20a%20maximum%0Aaverage%20extraction%20accuracy%20of%2099.20%5C%25%20and%2098.43%5C%25%2C%20respectively.%20It%20surpasses%0Aother%20state-of-the-art%20methods%20by%20nearly%2023%5C%25%20in%20resisting%20time-stretching%0Aattacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15035v1&entry.124074799=Read"},
{"title": "Training on the Test Task Confounds Evaluation and Emergence", "author": "Ricardo Dominguez-Olmedo and Florian E. Dorner and Moritz Hardt", "abstract": "  We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of practices that\nutilize knowledge about evaluation tasks at training time. We demonstrate that\ntraining on the test task confounds both relative model evaluations and claims\nabout emergent capabilities. We argue that the seeming superiority of one model\nfamily over another may be explained by a different degree of training on the\ntest task. To this end, we propose an effective method to adjust for the effect\nof training on the test task on benchmark evaluations. Put simply, to fine-tune\neach model under comparison on the same task-relevant data prior to evaluation.\nWe then show that instances of emergent behavior disappear gradually as models\ntrain on the test task. Our work promotes a new perspective on the evaluation\nof large language models, with broad implications for benchmarking and the\nstudy of emergent capabilities.\n", "link": "http://arxiv.org/abs/2407.07890v3", "date": "2025-04-21", "relevancy": 2.0775, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5234}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20on%20the%20Test%20Task%20Confounds%20Evaluation%20and%20Emergence&body=Title%3A%20Training%20on%20the%20Test%20Task%20Confounds%20Evaluation%20and%20Emergence%0AAuthor%3A%20Ricardo%20Dominguez-Olmedo%20and%20Florian%20E.%20Dorner%20and%20Moritz%20Hardt%0AAbstract%3A%20%20%20We%20study%20a%20fundamental%20problem%20in%20the%20evaluation%20of%20large%20language%20models%0Athat%20we%20call%20training%20on%20the%20test%20task.%20Unlike%20wrongful%20practices%20like%20training%0Aon%20the%20test%20data%2C%20leakage%2C%20or%20data%20contamination%2C%20training%20on%20the%20test%20task%20is%0Anot%20a%20malpractice.%20Rather%2C%20the%20term%20describes%20a%20growing%20set%20of%20practices%20that%0Autilize%20knowledge%20about%20evaluation%20tasks%20at%20training%20time.%20We%20demonstrate%20that%0Atraining%20on%20the%20test%20task%20confounds%20both%20relative%20model%20evaluations%20and%20claims%0Aabout%20emergent%20capabilities.%20We%20argue%20that%20the%20seeming%20superiority%20of%20one%20model%0Afamily%20over%20another%20may%20be%20explained%20by%20a%20different%20degree%20of%20training%20on%20the%0Atest%20task.%20To%20this%20end%2C%20we%20propose%20an%20effective%20method%20to%20adjust%20for%20the%20effect%0Aof%20training%20on%20the%20test%20task%20on%20benchmark%20evaluations.%20Put%20simply%2C%20to%20fine-tune%0Aeach%20model%20under%20comparison%20on%20the%20same%20task-relevant%20data%20prior%20to%20evaluation.%0AWe%20then%20show%20that%20instances%20of%20emergent%20behavior%20disappear%20gradually%20as%20models%0Atrain%20on%20the%20test%20task.%20Our%20work%20promotes%20a%20new%20perspective%20on%20the%20evaluation%0Aof%20large%20language%20models%2C%20with%20broad%20implications%20for%20benchmarking%20and%20the%0Astudy%20of%20emergent%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07890v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520on%2520the%2520Test%2520Task%2520Confounds%2520Evaluation%2520and%2520Emergence%26entry.906535625%3DRicardo%2520Dominguez-Olmedo%2520and%2520Florian%2520E.%2520Dorner%2520and%2520Moritz%2520Hardt%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520fundamental%2520problem%2520in%2520the%2520evaluation%2520of%2520large%2520language%2520models%250Athat%2520we%2520call%2520training%2520on%2520the%2520test%2520task.%2520Unlike%2520wrongful%2520practices%2520like%2520training%250Aon%2520the%2520test%2520data%252C%2520leakage%252C%2520or%2520data%2520contamination%252C%2520training%2520on%2520the%2520test%2520task%2520is%250Anot%2520a%2520malpractice.%2520Rather%252C%2520the%2520term%2520describes%2520a%2520growing%2520set%2520of%2520practices%2520that%250Autilize%2520knowledge%2520about%2520evaluation%2520tasks%2520at%2520training%2520time.%2520We%2520demonstrate%2520that%250Atraining%2520on%2520the%2520test%2520task%2520confounds%2520both%2520relative%2520model%2520evaluations%2520and%2520claims%250Aabout%2520emergent%2520capabilities.%2520We%2520argue%2520that%2520the%2520seeming%2520superiority%2520of%2520one%2520model%250Afamily%2520over%2520another%2520may%2520be%2520explained%2520by%2520a%2520different%2520degree%2520of%2520training%2520on%2520the%250Atest%2520task.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%2520effective%2520method%2520to%2520adjust%2520for%2520the%2520effect%250Aof%2520training%2520on%2520the%2520test%2520task%2520on%2520benchmark%2520evaluations.%2520Put%2520simply%252C%2520to%2520fine-tune%250Aeach%2520model%2520under%2520comparison%2520on%2520the%2520same%2520task-relevant%2520data%2520prior%2520to%2520evaluation.%250AWe%2520then%2520show%2520that%2520instances%2520of%2520emergent%2520behavior%2520disappear%2520gradually%2520as%2520models%250Atrain%2520on%2520the%2520test%2520task.%2520Our%2520work%2520promotes%2520a%2520new%2520perspective%2520on%2520the%2520evaluation%250Aof%2520large%2520language%2520models%252C%2520with%2520broad%2520implications%2520for%2520benchmarking%2520and%2520the%250Astudy%2520of%2520emergent%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07890v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20on%20the%20Test%20Task%20Confounds%20Evaluation%20and%20Emergence&entry.906535625=Ricardo%20Dominguez-Olmedo%20and%20Florian%20E.%20Dorner%20and%20Moritz%20Hardt&entry.1292438233=%20%20We%20study%20a%20fundamental%20problem%20in%20the%20evaluation%20of%20large%20language%20models%0Athat%20we%20call%20training%20on%20the%20test%20task.%20Unlike%20wrongful%20practices%20like%20training%0Aon%20the%20test%20data%2C%20leakage%2C%20or%20data%20contamination%2C%20training%20on%20the%20test%20task%20is%0Anot%20a%20malpractice.%20Rather%2C%20the%20term%20describes%20a%20growing%20set%20of%20practices%20that%0Autilize%20knowledge%20about%20evaluation%20tasks%20at%20training%20time.%20We%20demonstrate%20that%0Atraining%20on%20the%20test%20task%20confounds%20both%20relative%20model%20evaluations%20and%20claims%0Aabout%20emergent%20capabilities.%20We%20argue%20that%20the%20seeming%20superiority%20of%20one%20model%0Afamily%20over%20another%20may%20be%20explained%20by%20a%20different%20degree%20of%20training%20on%20the%0Atest%20task.%20To%20this%20end%2C%20we%20propose%20an%20effective%20method%20to%20adjust%20for%20the%20effect%0Aof%20training%20on%20the%20test%20task%20on%20benchmark%20evaluations.%20Put%20simply%2C%20to%20fine-tune%0Aeach%20model%20under%20comparison%20on%20the%20same%20task-relevant%20data%20prior%20to%20evaluation.%0AWe%20then%20show%20that%20instances%20of%20emergent%20behavior%20disappear%20gradually%20as%20models%0Atrain%20on%20the%20test%20task.%20Our%20work%20promotes%20a%20new%20perspective%20on%20the%20evaluation%0Aof%20large%20language%20models%2C%20with%20broad%20implications%20for%20benchmarking%20and%20the%0Astudy%20of%20emergent%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07890v3&entry.124074799=Read"},
{"title": "KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking", "author": "Juyeon Kim and Geon Lee and Taeuk Kim and Kijung Shin", "abstract": "  Entity linking (EL) aligns textual mentions with their corresponding entities\nin a knowledge base, facilitating various applications such as semantic search\nand question answering. Recent advances in multimodal entity linking (MEL) have\nshown that combining text and images can reduce ambiguity and improve alignment\naccuracy. However, most existing MEL methods overlook the rich structural\ninformation available in the form of knowledge-graph (KG) triples. In this\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\nhigh-quality triples for each mention by employing vision-language models based\non its text and images. (2) Retrieval: Learns joint mention-entity\nrepresentations, via contrastive learning, that integrate text, images, and\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\nReranking: Refines the KG triples of the candidate entities and employs large\nlanguage models to identify the best-matching entity for the mention. Extensive\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\nmethods. Our code and datasets are available at:\nhttps://github.com/juyeonnn/KGMEL.\n", "link": "http://arxiv.org/abs/2504.15135v1", "date": "2025-04-21", "relevancy": 2.0554, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.534}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5194}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KGMEL%3A%20Knowledge%20Graph-Enhanced%20Multimodal%20Entity%20Linking&body=Title%3A%20KGMEL%3A%20Knowledge%20Graph-Enhanced%20Multimodal%20Entity%20Linking%0AAuthor%3A%20Juyeon%20Kim%20and%20Geon%20Lee%20and%20Taeuk%20Kim%20and%20Kijung%20Shin%0AAbstract%3A%20%20%20Entity%20linking%20%28EL%29%20aligns%20textual%20mentions%20with%20their%20corresponding%20entities%0Ain%20a%20knowledge%20base%2C%20facilitating%20various%20applications%20such%20as%20semantic%20search%0Aand%20question%20answering.%20Recent%20advances%20in%20multimodal%20entity%20linking%20%28MEL%29%20have%0Ashown%20that%20combining%20text%20and%20images%20can%20reduce%20ambiguity%20and%20improve%20alignment%0Aaccuracy.%20However%2C%20most%20existing%20MEL%20methods%20overlook%20the%20rich%20structural%0Ainformation%20available%20in%20the%20form%20of%20knowledge-graph%20%28KG%29%20triples.%20In%20this%0Apaper%2C%20we%20propose%20KGMEL%2C%20a%20novel%20framework%20that%20leverages%20KG%20triples%20to%20enhance%0AMEL.%20Specifically%2C%20it%20operates%20in%20three%20stages%3A%20%281%29%20Generation%3A%20Produces%0Ahigh-quality%20triples%20for%20each%20mention%20by%20employing%20vision-language%20models%20based%0Aon%20its%20text%20and%20images.%20%282%29%20Retrieval%3A%20Learns%20joint%20mention-entity%0Arepresentations%2C%20via%20contrastive%20learning%2C%20that%20integrate%20text%2C%20images%2C%20and%0A%28generated%20or%20KG%29%20triples%20to%20retrieve%20candidate%20entities%20for%20each%20mention.%20%283%29%0AReranking%3A%20Refines%20the%20KG%20triples%20of%20the%20candidate%20entities%20and%20employs%20large%0Alanguage%20models%20to%20identify%20the%20best-matching%20entity%20for%20the%20mention.%20Extensive%0Aexperiments%20on%20benchmark%20datasets%20demonstrate%20that%20KGMEL%20outperforms%20existing%0Amethods.%20Our%20code%20and%20datasets%20are%20available%20at%3A%0Ahttps%3A//github.com/juyeonnn/KGMEL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKGMEL%253A%2520Knowledge%2520Graph-Enhanced%2520Multimodal%2520Entity%2520Linking%26entry.906535625%3DJuyeon%2520Kim%2520and%2520Geon%2520Lee%2520and%2520Taeuk%2520Kim%2520and%2520Kijung%2520Shin%26entry.1292438233%3D%2520%2520Entity%2520linking%2520%2528EL%2529%2520aligns%2520textual%2520mentions%2520with%2520their%2520corresponding%2520entities%250Ain%2520a%2520knowledge%2520base%252C%2520facilitating%2520various%2520applications%2520such%2520as%2520semantic%2520search%250Aand%2520question%2520answering.%2520Recent%2520advances%2520in%2520multimodal%2520entity%2520linking%2520%2528MEL%2529%2520have%250Ashown%2520that%2520combining%2520text%2520and%2520images%2520can%2520reduce%2520ambiguity%2520and%2520improve%2520alignment%250Aaccuracy.%2520However%252C%2520most%2520existing%2520MEL%2520methods%2520overlook%2520the%2520rich%2520structural%250Ainformation%2520available%2520in%2520the%2520form%2520of%2520knowledge-graph%2520%2528KG%2529%2520triples.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520KGMEL%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520KG%2520triples%2520to%2520enhance%250AMEL.%2520Specifically%252C%2520it%2520operates%2520in%2520three%2520stages%253A%2520%25281%2529%2520Generation%253A%2520Produces%250Ahigh-quality%2520triples%2520for%2520each%2520mention%2520by%2520employing%2520vision-language%2520models%2520based%250Aon%2520its%2520text%2520and%2520images.%2520%25282%2529%2520Retrieval%253A%2520Learns%2520joint%2520mention-entity%250Arepresentations%252C%2520via%2520contrastive%2520learning%252C%2520that%2520integrate%2520text%252C%2520images%252C%2520and%250A%2528generated%2520or%2520KG%2529%2520triples%2520to%2520retrieve%2520candidate%2520entities%2520for%2520each%2520mention.%2520%25283%2529%250AReranking%253A%2520Refines%2520the%2520KG%2520triples%2520of%2520the%2520candidate%2520entities%2520and%2520employs%2520large%250Alanguage%2520models%2520to%2520identify%2520the%2520best-matching%2520entity%2520for%2520the%2520mention.%2520Extensive%250Aexperiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520KGMEL%2520outperforms%2520existing%250Amethods.%2520Our%2520code%2520and%2520datasets%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/juyeonnn/KGMEL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KGMEL%3A%20Knowledge%20Graph-Enhanced%20Multimodal%20Entity%20Linking&entry.906535625=Juyeon%20Kim%20and%20Geon%20Lee%20and%20Taeuk%20Kim%20and%20Kijung%20Shin&entry.1292438233=%20%20Entity%20linking%20%28EL%29%20aligns%20textual%20mentions%20with%20their%20corresponding%20entities%0Ain%20a%20knowledge%20base%2C%20facilitating%20various%20applications%20such%20as%20semantic%20search%0Aand%20question%20answering.%20Recent%20advances%20in%20multimodal%20entity%20linking%20%28MEL%29%20have%0Ashown%20that%20combining%20text%20and%20images%20can%20reduce%20ambiguity%20and%20improve%20alignment%0Aaccuracy.%20However%2C%20most%20existing%20MEL%20methods%20overlook%20the%20rich%20structural%0Ainformation%20available%20in%20the%20form%20of%20knowledge-graph%20%28KG%29%20triples.%20In%20this%0Apaper%2C%20we%20propose%20KGMEL%2C%20a%20novel%20framework%20that%20leverages%20KG%20triples%20to%20enhance%0AMEL.%20Specifically%2C%20it%20operates%20in%20three%20stages%3A%20%281%29%20Generation%3A%20Produces%0Ahigh-quality%20triples%20for%20each%20mention%20by%20employing%20vision-language%20models%20based%0Aon%20its%20text%20and%20images.%20%282%29%20Retrieval%3A%20Learns%20joint%20mention-entity%0Arepresentations%2C%20via%20contrastive%20learning%2C%20that%20integrate%20text%2C%20images%2C%20and%0A%28generated%20or%20KG%29%20triples%20to%20retrieve%20candidate%20entities%20for%20each%20mention.%20%283%29%0AReranking%3A%20Refines%20the%20KG%20triples%20of%20the%20candidate%20entities%20and%20employs%20large%0Alanguage%20models%20to%20identify%20the%20best-matching%20entity%20for%20the%20mention.%20Extensive%0Aexperiments%20on%20benchmark%20datasets%20demonstrate%20that%20KGMEL%20outperforms%20existing%0Amethods.%20Our%20code%20and%20datasets%20are%20available%20at%3A%0Ahttps%3A//github.com/juyeonnn/KGMEL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15135v1&entry.124074799=Read"},
{"title": "RILe: Reinforced Imitation Learning", "author": "Mert Albaba and Sammy Christen and Thomas Langarek and Christoph Gebhardt and Otmar Hilliges and Michael J. Black", "abstract": "  Acquiring complex behaviors is essential for artificially intelligent agents,\nyet learning these behaviors in high-dimensional settings poses a significant\nchallenge due to the vast search space. Traditional reinforcement learning (RL)\nrequires extensive manual effort for reward function engineering. Inverse\nreinforcement learning (IRL) uncovers reward functions from expert\ndemonstrations but relies on an iterative process that is often computationally\nexpensive. Imitation learning (IL) provides a more efficient alternative by\ndirectly comparing an agent's actions to expert demonstrations; however, in\nhigh-dimensional environments, such direct comparisons often offer insufficient\nfeedback for effective learning. We introduce RILe (Reinforced Imitation\nLearning), a framework that combines the strengths of imitation learning and\ninverse reinforcement learning to learn a dense reward function efficiently and\nachieve strong performance in high-dimensional tasks. RILe employs a novel\ntrainer-student framework: the trainer learns an adaptive reward function, and\nthe student uses this reward signal to imitate expert behaviors. By dynamically\nadjusting its guidance as the student evolves, the trainer provides nuanced\nfeedback across different phases of learning. Our framework produces\nhigh-performing policies in high-dimensional tasks where direct imitation fails\nto replicate complex behaviors. We validate RILe in challenging robotic\nlocomotion tasks, demonstrating that it significantly outperforms existing\nmethods and achieves near-expert performance across multiple settings.\n", "link": "http://arxiv.org/abs/2406.08472v4", "date": "2025-04-21", "relevancy": 2.0505, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5183}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5104}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RILe%3A%20Reinforced%20Imitation%20Learning&body=Title%3A%20RILe%3A%20Reinforced%20Imitation%20Learning%0AAuthor%3A%20Mert%20Albaba%20and%20Sammy%20Christen%20and%20Thomas%20Langarek%20and%20Christoph%20Gebhardt%20and%20Otmar%20Hilliges%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20Acquiring%20complex%20behaviors%20is%20essential%20for%20artificially%20intelligent%20agents%2C%0Ayet%20learning%20these%20behaviors%20in%20high-dimensional%20settings%20poses%20a%20significant%0Achallenge%20due%20to%20the%20vast%20search%20space.%20Traditional%20reinforcement%20learning%20%28RL%29%0Arequires%20extensive%20manual%20effort%20for%20reward%20function%20engineering.%20Inverse%0Areinforcement%20learning%20%28IRL%29%20uncovers%20reward%20functions%20from%20expert%0Ademonstrations%20but%20relies%20on%20an%20iterative%20process%20that%20is%20often%20computationally%0Aexpensive.%20Imitation%20learning%20%28IL%29%20provides%20a%20more%20efficient%20alternative%20by%0Adirectly%20comparing%20an%20agent%27s%20actions%20to%20expert%20demonstrations%3B%20however%2C%20in%0Ahigh-dimensional%20environments%2C%20such%20direct%20comparisons%20often%20offer%20insufficient%0Afeedback%20for%20effective%20learning.%20We%20introduce%20RILe%20%28Reinforced%20Imitation%0ALearning%29%2C%20a%20framework%20that%20combines%20the%20strengths%20of%20imitation%20learning%20and%0Ainverse%20reinforcement%20learning%20to%20learn%20a%20dense%20reward%20function%20efficiently%20and%0Aachieve%20strong%20performance%20in%20high-dimensional%20tasks.%20RILe%20employs%20a%20novel%0Atrainer-student%20framework%3A%20the%20trainer%20learns%20an%20adaptive%20reward%20function%2C%20and%0Athe%20student%20uses%20this%20reward%20signal%20to%20imitate%20expert%20behaviors.%20By%20dynamically%0Aadjusting%20its%20guidance%20as%20the%20student%20evolves%2C%20the%20trainer%20provides%20nuanced%0Afeedback%20across%20different%20phases%20of%20learning.%20Our%20framework%20produces%0Ahigh-performing%20policies%20in%20high-dimensional%20tasks%20where%20direct%20imitation%20fails%0Ato%20replicate%20complex%20behaviors.%20We%20validate%20RILe%20in%20challenging%20robotic%0Alocomotion%20tasks%2C%20demonstrating%20that%20it%20significantly%20outperforms%20existing%0Amethods%20and%20achieves%20near-expert%20performance%20across%20multiple%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08472v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRILe%253A%2520Reinforced%2520Imitation%2520Learning%26entry.906535625%3DMert%2520Albaba%2520and%2520Sammy%2520Christen%2520and%2520Thomas%2520Langarek%2520and%2520Christoph%2520Gebhardt%2520and%2520Otmar%2520Hilliges%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520Acquiring%2520complex%2520behaviors%2520is%2520essential%2520for%2520artificially%2520intelligent%2520agents%252C%250Ayet%2520learning%2520these%2520behaviors%2520in%2520high-dimensional%2520settings%2520poses%2520a%2520significant%250Achallenge%2520due%2520to%2520the%2520vast%2520search%2520space.%2520Traditional%2520reinforcement%2520learning%2520%2528RL%2529%250Arequires%2520extensive%2520manual%2520effort%2520for%2520reward%2520function%2520engineering.%2520Inverse%250Areinforcement%2520learning%2520%2528IRL%2529%2520uncovers%2520reward%2520functions%2520from%2520expert%250Ademonstrations%2520but%2520relies%2520on%2520an%2520iterative%2520process%2520that%2520is%2520often%2520computationally%250Aexpensive.%2520Imitation%2520learning%2520%2528IL%2529%2520provides%2520a%2520more%2520efficient%2520alternative%2520by%250Adirectly%2520comparing%2520an%2520agent%2527s%2520actions%2520to%2520expert%2520demonstrations%253B%2520however%252C%2520in%250Ahigh-dimensional%2520environments%252C%2520such%2520direct%2520comparisons%2520often%2520offer%2520insufficient%250Afeedback%2520for%2520effective%2520learning.%2520We%2520introduce%2520RILe%2520%2528Reinforced%2520Imitation%250ALearning%2529%252C%2520a%2520framework%2520that%2520combines%2520the%2520strengths%2520of%2520imitation%2520learning%2520and%250Ainverse%2520reinforcement%2520learning%2520to%2520learn%2520a%2520dense%2520reward%2520function%2520efficiently%2520and%250Aachieve%2520strong%2520performance%2520in%2520high-dimensional%2520tasks.%2520RILe%2520employs%2520a%2520novel%250Atrainer-student%2520framework%253A%2520the%2520trainer%2520learns%2520an%2520adaptive%2520reward%2520function%252C%2520and%250Athe%2520student%2520uses%2520this%2520reward%2520signal%2520to%2520imitate%2520expert%2520behaviors.%2520By%2520dynamically%250Aadjusting%2520its%2520guidance%2520as%2520the%2520student%2520evolves%252C%2520the%2520trainer%2520provides%2520nuanced%250Afeedback%2520across%2520different%2520phases%2520of%2520learning.%2520Our%2520framework%2520produces%250Ahigh-performing%2520policies%2520in%2520high-dimensional%2520tasks%2520where%2520direct%2520imitation%2520fails%250Ato%2520replicate%2520complex%2520behaviors.%2520We%2520validate%2520RILe%2520in%2520challenging%2520robotic%250Alocomotion%2520tasks%252C%2520demonstrating%2520that%2520it%2520significantly%2520outperforms%2520existing%250Amethods%2520and%2520achieves%2520near-expert%2520performance%2520across%2520multiple%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08472v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RILe%3A%20Reinforced%20Imitation%20Learning&entry.906535625=Mert%20Albaba%20and%20Sammy%20Christen%20and%20Thomas%20Langarek%20and%20Christoph%20Gebhardt%20and%20Otmar%20Hilliges%20and%20Michael%20J.%20Black&entry.1292438233=%20%20Acquiring%20complex%20behaviors%20is%20essential%20for%20artificially%20intelligent%20agents%2C%0Ayet%20learning%20these%20behaviors%20in%20high-dimensional%20settings%20poses%20a%20significant%0Achallenge%20due%20to%20the%20vast%20search%20space.%20Traditional%20reinforcement%20learning%20%28RL%29%0Arequires%20extensive%20manual%20effort%20for%20reward%20function%20engineering.%20Inverse%0Areinforcement%20learning%20%28IRL%29%20uncovers%20reward%20functions%20from%20expert%0Ademonstrations%20but%20relies%20on%20an%20iterative%20process%20that%20is%20often%20computationally%0Aexpensive.%20Imitation%20learning%20%28IL%29%20provides%20a%20more%20efficient%20alternative%20by%0Adirectly%20comparing%20an%20agent%27s%20actions%20to%20expert%20demonstrations%3B%20however%2C%20in%0Ahigh-dimensional%20environments%2C%20such%20direct%20comparisons%20often%20offer%20insufficient%0Afeedback%20for%20effective%20learning.%20We%20introduce%20RILe%20%28Reinforced%20Imitation%0ALearning%29%2C%20a%20framework%20that%20combines%20the%20strengths%20of%20imitation%20learning%20and%0Ainverse%20reinforcement%20learning%20to%20learn%20a%20dense%20reward%20function%20efficiently%20and%0Aachieve%20strong%20performance%20in%20high-dimensional%20tasks.%20RILe%20employs%20a%20novel%0Atrainer-student%20framework%3A%20the%20trainer%20learns%20an%20adaptive%20reward%20function%2C%20and%0Athe%20student%20uses%20this%20reward%20signal%20to%20imitate%20expert%20behaviors.%20By%20dynamically%0Aadjusting%20its%20guidance%20as%20the%20student%20evolves%2C%20the%20trainer%20provides%20nuanced%0Afeedback%20across%20different%20phases%20of%20learning.%20Our%20framework%20produces%0Ahigh-performing%20policies%20in%20high-dimensional%20tasks%20where%20direct%20imitation%20fails%0Ato%20replicate%20complex%20behaviors.%20We%20validate%20RILe%20in%20challenging%20robotic%0Alocomotion%20tasks%2C%20demonstrating%20that%20it%20significantly%20outperforms%20existing%0Amethods%20and%20achieves%20near-expert%20performance%20across%20multiple%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08472v4&entry.124074799=Read"},
{"title": "Accelerating Goal-Conditioned RL Algorithms and Research", "author": "Micha\u0142 Bortkiewicz and W\u0142adys\u0142aw Pa\u0142ucki and Vivek Myers and Tadeusz Dziarmaga and Tomasz Arczewski and \u0141ukasz Kuci\u0144ski and Benjamin Eysenbach", "abstract": "  Self-supervision has the potential to transform reinforcement learning (RL),\nparalleling the breakthroughs it has enabled in other areas of machine\nlearning. While self-supervised learning in other domains aims to find patterns\nin a fixed dataset, self-supervised goal-conditioned reinforcement learning\n(GCRL) agents discover new behaviors by learning from the goals achieved during\nunstructured interaction with the environment. However, these methods have\nfailed to see similar success, both due to a lack of data from slow environment\nsimulations as well as a lack of stable algorithms. We take a step toward\naddressing both of these issues by releasing a high-performance codebase and\nbenchmark (JaxGCRL) for self-supervised GCRL, enabling researchers to train\nagents for millions of environment steps in minutes on a single GPU. By\nutilizing GPU-accelerated replay buffers, environments, and a stable\ncontrastive RL algorithm, we reduce training time by up to $22\\times$.\nAdditionally, we assess key design choices in contrastive RL, identifying those\nthat most effectively stabilize and enhance training performance. With this\napproach, we provide a foundation for future research in self-supervised GCRL,\nenabling researchers to quickly iterate on new ideas and evaluate them in\ndiverse and challenging environments. Website + Code:\nhttps://github.com/MichalBortkiewicz/JaxGCRL\n", "link": "http://arxiv.org/abs/2408.11052v3", "date": "2025-04-21", "relevancy": 2.044, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5263}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5098}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Goal-Conditioned%20RL%20Algorithms%20and%20Research&body=Title%3A%20Accelerating%20Goal-Conditioned%20RL%20Algorithms%20and%20Research%0AAuthor%3A%20Micha%C5%82%20Bortkiewicz%20and%20W%C5%82adys%C5%82aw%20Pa%C5%82ucki%20and%20Vivek%20Myers%20and%20Tadeusz%20Dziarmaga%20and%20Tomasz%20Arczewski%20and%20%C5%81ukasz%20Kuci%C5%84ski%20and%20Benjamin%20Eysenbach%0AAbstract%3A%20%20%20Self-supervision%20has%20the%20potential%20to%20transform%20reinforcement%20learning%20%28RL%29%2C%0Aparalleling%20the%20breakthroughs%20it%20has%20enabled%20in%20other%20areas%20of%20machine%0Alearning.%20While%20self-supervised%20learning%20in%20other%20domains%20aims%20to%20find%20patterns%0Ain%20a%20fixed%20dataset%2C%20self-supervised%20goal-conditioned%20reinforcement%20learning%0A%28GCRL%29%20agents%20discover%20new%20behaviors%20by%20learning%20from%20the%20goals%20achieved%20during%0Aunstructured%20interaction%20with%20the%20environment.%20However%2C%20these%20methods%20have%0Afailed%20to%20see%20similar%20success%2C%20both%20due%20to%20a%20lack%20of%20data%20from%20slow%20environment%0Asimulations%20as%20well%20as%20a%20lack%20of%20stable%20algorithms.%20We%20take%20a%20step%20toward%0Aaddressing%20both%20of%20these%20issues%20by%20releasing%20a%20high-performance%20codebase%20and%0Abenchmark%20%28JaxGCRL%29%20for%20self-supervised%20GCRL%2C%20enabling%20researchers%20to%20train%0Aagents%20for%20millions%20of%20environment%20steps%20in%20minutes%20on%20a%20single%20GPU.%20By%0Autilizing%20GPU-accelerated%20replay%20buffers%2C%20environments%2C%20and%20a%20stable%0Acontrastive%20RL%20algorithm%2C%20we%20reduce%20training%20time%20by%20up%20to%20%2422%5Ctimes%24.%0AAdditionally%2C%20we%20assess%20key%20design%20choices%20in%20contrastive%20RL%2C%20identifying%20those%0Athat%20most%20effectively%20stabilize%20and%20enhance%20training%20performance.%20With%20this%0Aapproach%2C%20we%20provide%20a%20foundation%20for%20future%20research%20in%20self-supervised%20GCRL%2C%0Aenabling%20researchers%20to%20quickly%20iterate%20on%20new%20ideas%20and%20evaluate%20them%20in%0Adiverse%20and%20challenging%20environments.%20Website%20%2B%20Code%3A%0Ahttps%3A//github.com/MichalBortkiewicz/JaxGCRL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11052v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Goal-Conditioned%2520RL%2520Algorithms%2520and%2520Research%26entry.906535625%3DMicha%25C5%2582%2520Bortkiewicz%2520and%2520W%25C5%2582adys%25C5%2582aw%2520Pa%25C5%2582ucki%2520and%2520Vivek%2520Myers%2520and%2520Tadeusz%2520Dziarmaga%2520and%2520Tomasz%2520Arczewski%2520and%2520%25C5%2581ukasz%2520Kuci%25C5%2584ski%2520and%2520Benjamin%2520Eysenbach%26entry.1292438233%3D%2520%2520Self-supervision%2520has%2520the%2520potential%2520to%2520transform%2520reinforcement%2520learning%2520%2528RL%2529%252C%250Aparalleling%2520the%2520breakthroughs%2520it%2520has%2520enabled%2520in%2520other%2520areas%2520of%2520machine%250Alearning.%2520While%2520self-supervised%2520learning%2520in%2520other%2520domains%2520aims%2520to%2520find%2520patterns%250Ain%2520a%2520fixed%2520dataset%252C%2520self-supervised%2520goal-conditioned%2520reinforcement%2520learning%250A%2528GCRL%2529%2520agents%2520discover%2520new%2520behaviors%2520by%2520learning%2520from%2520the%2520goals%2520achieved%2520during%250Aunstructured%2520interaction%2520with%2520the%2520environment.%2520However%252C%2520these%2520methods%2520have%250Afailed%2520to%2520see%2520similar%2520success%252C%2520both%2520due%2520to%2520a%2520lack%2520of%2520data%2520from%2520slow%2520environment%250Asimulations%2520as%2520well%2520as%2520a%2520lack%2520of%2520stable%2520algorithms.%2520We%2520take%2520a%2520step%2520toward%250Aaddressing%2520both%2520of%2520these%2520issues%2520by%2520releasing%2520a%2520high-performance%2520codebase%2520and%250Abenchmark%2520%2528JaxGCRL%2529%2520for%2520self-supervised%2520GCRL%252C%2520enabling%2520researchers%2520to%2520train%250Aagents%2520for%2520millions%2520of%2520environment%2520steps%2520in%2520minutes%2520on%2520a%2520single%2520GPU.%2520By%250Autilizing%2520GPU-accelerated%2520replay%2520buffers%252C%2520environments%252C%2520and%2520a%2520stable%250Acontrastive%2520RL%2520algorithm%252C%2520we%2520reduce%2520training%2520time%2520by%2520up%2520to%2520%252422%255Ctimes%2524.%250AAdditionally%252C%2520we%2520assess%2520key%2520design%2520choices%2520in%2520contrastive%2520RL%252C%2520identifying%2520those%250Athat%2520most%2520effectively%2520stabilize%2520and%2520enhance%2520training%2520performance.%2520With%2520this%250Aapproach%252C%2520we%2520provide%2520a%2520foundation%2520for%2520future%2520research%2520in%2520self-supervised%2520GCRL%252C%250Aenabling%2520researchers%2520to%2520quickly%2520iterate%2520on%2520new%2520ideas%2520and%2520evaluate%2520them%2520in%250Adiverse%2520and%2520challenging%2520environments.%2520Website%2520%252B%2520Code%253A%250Ahttps%253A//github.com/MichalBortkiewicz/JaxGCRL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11052v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Goal-Conditioned%20RL%20Algorithms%20and%20Research&entry.906535625=Micha%C5%82%20Bortkiewicz%20and%20W%C5%82adys%C5%82aw%20Pa%C5%82ucki%20and%20Vivek%20Myers%20and%20Tadeusz%20Dziarmaga%20and%20Tomasz%20Arczewski%20and%20%C5%81ukasz%20Kuci%C5%84ski%20and%20Benjamin%20Eysenbach&entry.1292438233=%20%20Self-supervision%20has%20the%20potential%20to%20transform%20reinforcement%20learning%20%28RL%29%2C%0Aparalleling%20the%20breakthroughs%20it%20has%20enabled%20in%20other%20areas%20of%20machine%0Alearning.%20While%20self-supervised%20learning%20in%20other%20domains%20aims%20to%20find%20patterns%0Ain%20a%20fixed%20dataset%2C%20self-supervised%20goal-conditioned%20reinforcement%20learning%0A%28GCRL%29%20agents%20discover%20new%20behaviors%20by%20learning%20from%20the%20goals%20achieved%20during%0Aunstructured%20interaction%20with%20the%20environment.%20However%2C%20these%20methods%20have%0Afailed%20to%20see%20similar%20success%2C%20both%20due%20to%20a%20lack%20of%20data%20from%20slow%20environment%0Asimulations%20as%20well%20as%20a%20lack%20of%20stable%20algorithms.%20We%20take%20a%20step%20toward%0Aaddressing%20both%20of%20these%20issues%20by%20releasing%20a%20high-performance%20codebase%20and%0Abenchmark%20%28JaxGCRL%29%20for%20self-supervised%20GCRL%2C%20enabling%20researchers%20to%20train%0Aagents%20for%20millions%20of%20environment%20steps%20in%20minutes%20on%20a%20single%20GPU.%20By%0Autilizing%20GPU-accelerated%20replay%20buffers%2C%20environments%2C%20and%20a%20stable%0Acontrastive%20RL%20algorithm%2C%20we%20reduce%20training%20time%20by%20up%20to%20%2422%5Ctimes%24.%0AAdditionally%2C%20we%20assess%20key%20design%20choices%20in%20contrastive%20RL%2C%20identifying%20those%0Athat%20most%20effectively%20stabilize%20and%20enhance%20training%20performance.%20With%20this%0Aapproach%2C%20we%20provide%20a%20foundation%20for%20future%20research%20in%20self-supervised%20GCRL%2C%0Aenabling%20researchers%20to%20quickly%20iterate%20on%20new%20ideas%20and%20evaluate%20them%20in%0Adiverse%20and%20challenging%20environments.%20Website%20%2B%20Code%3A%0Ahttps%3A//github.com/MichalBortkiewicz/JaxGCRL%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11052v3&entry.124074799=Read"},
{"title": "MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal\n  Retrieval-Augmented Multimodal Generation", "author": "Qinhan Yu and Zhiyou Xiao and Binghui Li and Zhengren Wang and Chong Chen and Wentao Zhang", "abstract": "  Recent advances in Retrieval-Augmented Generation (RAG) have significantly\nimproved response accuracy and relevance by incorporating external knowledge\ninto Large Language Models (LLMs). However, existing RAG methods primarily\nfocus on generating text-only answers, even in Multimodal Retrieval-Augmented\nGeneration (MRAG) scenarios, where multimodal elements are retrieved to assist\nin generating text answers. To address this, we introduce the Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) task, in which we aim to\ngenerate multimodal answers that combine both text and images, fully leveraging\nthe multimodal data within a corpus. Despite growing attention to this\nchallenging task, a notable lack of a comprehensive benchmark persists for\neffectively evaluating its performance. To bridge this gap, we provide\nMRAMG-Bench, a meticulously curated, human-annotated benchmark comprising 4,346\ndocuments, 14,190 images, and 4,800 QA pairs, distributed across six distinct\ndatasets and spanning three domains: Web, Academia, and Lifestyle. The datasets\nincorporate diverse difficulty levels and complex multi-image scenarios,\nproviding a robust foundation for evaluating the MRAMG task. To facilitate\nrigorous evaluation, MRAMG-Bench incorporates a comprehensive suite of both\nstatistical and LLM-based metrics, enabling a thorough analysis of the\nperformance of generative models in the MRAMG task. Additionally, we propose an\nefficient and flexible multimodal answer generation framework that can leverage\nLLMs/MLLMs to generate multimodal responses. Our datasets and complete\nevaluation results for 11 popular generative models are available at\nhttps://github.com/MRAMG-Bench/MRAMG.\n", "link": "http://arxiv.org/abs/2502.04176v2", "date": "2025-04-21", "relevancy": 2.0432, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5197}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5102}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRAMG-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20Advancing%20Multimodal%0A%20%20Retrieval-Augmented%20Multimodal%20Generation&body=Title%3A%20MRAMG-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20Advancing%20Multimodal%0A%20%20Retrieval-Augmented%20Multimodal%20Generation%0AAuthor%3A%20Qinhan%20Yu%20and%20Zhiyou%20Xiao%20and%20Binghui%20Li%20and%20Zhengren%20Wang%20and%20Chong%20Chen%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Recent%20advances%20in%20Retrieval-Augmented%20Generation%20%28RAG%29%20have%20significantly%0Aimproved%20response%20accuracy%20and%20relevance%20by%20incorporating%20external%20knowledge%0Ainto%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20existing%20RAG%20methods%20primarily%0Afocus%20on%20generating%20text-only%20answers%2C%20even%20in%20Multimodal%20Retrieval-Augmented%0AGeneration%20%28MRAG%29%20scenarios%2C%20where%20multimodal%20elements%20are%20retrieved%20to%20assist%0Ain%20generating%20text%20answers.%20To%20address%20this%2C%20we%20introduce%20the%20Multimodal%0ARetrieval-Augmented%20Multimodal%20Generation%20%28MRAMG%29%20task%2C%20in%20which%20we%20aim%20to%0Agenerate%20multimodal%20answers%20that%20combine%20both%20text%20and%20images%2C%20fully%20leveraging%0Athe%20multimodal%20data%20within%20a%20corpus.%20Despite%20growing%20attention%20to%20this%0Achallenging%20task%2C%20a%20notable%20lack%20of%20a%20comprehensive%20benchmark%20persists%20for%0Aeffectively%20evaluating%20its%20performance.%20To%20bridge%20this%20gap%2C%20we%20provide%0AMRAMG-Bench%2C%20a%20meticulously%20curated%2C%20human-annotated%20benchmark%20comprising%204%2C346%0Adocuments%2C%2014%2C190%20images%2C%20and%204%2C800%20QA%20pairs%2C%20distributed%20across%20six%20distinct%0Adatasets%20and%20spanning%20three%20domains%3A%20Web%2C%20Academia%2C%20and%20Lifestyle.%20The%20datasets%0Aincorporate%20diverse%20difficulty%20levels%20and%20complex%20multi-image%20scenarios%2C%0Aproviding%20a%20robust%20foundation%20for%20evaluating%20the%20MRAMG%20task.%20To%20facilitate%0Arigorous%20evaluation%2C%20MRAMG-Bench%20incorporates%20a%20comprehensive%20suite%20of%20both%0Astatistical%20and%20LLM-based%20metrics%2C%20enabling%20a%20thorough%20analysis%20of%20the%0Aperformance%20of%20generative%20models%20in%20the%20MRAMG%20task.%20Additionally%2C%20we%20propose%20an%0Aefficient%20and%20flexible%20multimodal%20answer%20generation%20framework%20that%20can%20leverage%0ALLMs/MLLMs%20to%20generate%20multimodal%20responses.%20Our%20datasets%20and%20complete%0Aevaluation%20results%20for%2011%20popular%20generative%20models%20are%20available%20at%0Ahttps%3A//github.com/MRAMG-Bench/MRAMG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRAMG-Bench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Advancing%2520Multimodal%250A%2520%2520Retrieval-Augmented%2520Multimodal%2520Generation%26entry.906535625%3DQinhan%2520Yu%2520and%2520Zhiyou%2520Xiao%2520and%2520Binghui%2520Li%2520and%2520Zhengren%2520Wang%2520and%2520Chong%2520Chen%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520have%2520significantly%250Aimproved%2520response%2520accuracy%2520and%2520relevance%2520by%2520incorporating%2520external%2520knowledge%250Ainto%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520However%252C%2520existing%2520RAG%2520methods%2520primarily%250Afocus%2520on%2520generating%2520text-only%2520answers%252C%2520even%2520in%2520Multimodal%2520Retrieval-Augmented%250AGeneration%2520%2528MRAG%2529%2520scenarios%252C%2520where%2520multimodal%2520elements%2520are%2520retrieved%2520to%2520assist%250Ain%2520generating%2520text%2520answers.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Multimodal%250ARetrieval-Augmented%2520Multimodal%2520Generation%2520%2528MRAMG%2529%2520task%252C%2520in%2520which%2520we%2520aim%2520to%250Agenerate%2520multimodal%2520answers%2520that%2520combine%2520both%2520text%2520and%2520images%252C%2520fully%2520leveraging%250Athe%2520multimodal%2520data%2520within%2520a%2520corpus.%2520Despite%2520growing%2520attention%2520to%2520this%250Achallenging%2520task%252C%2520a%2520notable%2520lack%2520of%2520a%2520comprehensive%2520benchmark%2520persists%2520for%250Aeffectively%2520evaluating%2520its%2520performance.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520provide%250AMRAMG-Bench%252C%2520a%2520meticulously%2520curated%252C%2520human-annotated%2520benchmark%2520comprising%25204%252C346%250Adocuments%252C%252014%252C190%2520images%252C%2520and%25204%252C800%2520QA%2520pairs%252C%2520distributed%2520across%2520six%2520distinct%250Adatasets%2520and%2520spanning%2520three%2520domains%253A%2520Web%252C%2520Academia%252C%2520and%2520Lifestyle.%2520The%2520datasets%250Aincorporate%2520diverse%2520difficulty%2520levels%2520and%2520complex%2520multi-image%2520scenarios%252C%250Aproviding%2520a%2520robust%2520foundation%2520for%2520evaluating%2520the%2520MRAMG%2520task.%2520To%2520facilitate%250Arigorous%2520evaluation%252C%2520MRAMG-Bench%2520incorporates%2520a%2520comprehensive%2520suite%2520of%2520both%250Astatistical%2520and%2520LLM-based%2520metrics%252C%2520enabling%2520a%2520thorough%2520analysis%2520of%2520the%250Aperformance%2520of%2520generative%2520models%2520in%2520the%2520MRAMG%2520task.%2520Additionally%252C%2520we%2520propose%2520an%250Aefficient%2520and%2520flexible%2520multimodal%2520answer%2520generation%2520framework%2520that%2520can%2520leverage%250ALLMs/MLLMs%2520to%2520generate%2520multimodal%2520responses.%2520Our%2520datasets%2520and%2520complete%250Aevaluation%2520results%2520for%252011%2520popular%2520generative%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/MRAMG-Bench/MRAMG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRAMG-Bench%3A%20A%20Comprehensive%20Benchmark%20for%20Advancing%20Multimodal%0A%20%20Retrieval-Augmented%20Multimodal%20Generation&entry.906535625=Qinhan%20Yu%20and%20Zhiyou%20Xiao%20and%20Binghui%20Li%20and%20Zhengren%20Wang%20and%20Chong%20Chen%20and%20Wentao%20Zhang&entry.1292438233=%20%20Recent%20advances%20in%20Retrieval-Augmented%20Generation%20%28RAG%29%20have%20significantly%0Aimproved%20response%20accuracy%20and%20relevance%20by%20incorporating%20external%20knowledge%0Ainto%20Large%20Language%20Models%20%28LLMs%29.%20However%2C%20existing%20RAG%20methods%20primarily%0Afocus%20on%20generating%20text-only%20answers%2C%20even%20in%20Multimodal%20Retrieval-Augmented%0AGeneration%20%28MRAG%29%20scenarios%2C%20where%20multimodal%20elements%20are%20retrieved%20to%20assist%0Ain%20generating%20text%20answers.%20To%20address%20this%2C%20we%20introduce%20the%20Multimodal%0ARetrieval-Augmented%20Multimodal%20Generation%20%28MRAMG%29%20task%2C%20in%20which%20we%20aim%20to%0Agenerate%20multimodal%20answers%20that%20combine%20both%20text%20and%20images%2C%20fully%20leveraging%0Athe%20multimodal%20data%20within%20a%20corpus.%20Despite%20growing%20attention%20to%20this%0Achallenging%20task%2C%20a%20notable%20lack%20of%20a%20comprehensive%20benchmark%20persists%20for%0Aeffectively%20evaluating%20its%20performance.%20To%20bridge%20this%20gap%2C%20we%20provide%0AMRAMG-Bench%2C%20a%20meticulously%20curated%2C%20human-annotated%20benchmark%20comprising%204%2C346%0Adocuments%2C%2014%2C190%20images%2C%20and%204%2C800%20QA%20pairs%2C%20distributed%20across%20six%20distinct%0Adatasets%20and%20spanning%20three%20domains%3A%20Web%2C%20Academia%2C%20and%20Lifestyle.%20The%20datasets%0Aincorporate%20diverse%20difficulty%20levels%20and%20complex%20multi-image%20scenarios%2C%0Aproviding%20a%20robust%20foundation%20for%20evaluating%20the%20MRAMG%20task.%20To%20facilitate%0Arigorous%20evaluation%2C%20MRAMG-Bench%20incorporates%20a%20comprehensive%20suite%20of%20both%0Astatistical%20and%20LLM-based%20metrics%2C%20enabling%20a%20thorough%20analysis%20of%20the%0Aperformance%20of%20generative%20models%20in%20the%20MRAMG%20task.%20Additionally%2C%20we%20propose%20an%0Aefficient%20and%20flexible%20multimodal%20answer%20generation%20framework%20that%20can%20leverage%0ALLMs/MLLMs%20to%20generate%20multimodal%20responses.%20Our%20datasets%20and%20complete%0Aevaluation%20results%20for%2011%20popular%20generative%20models%20are%20available%20at%0Ahttps%3A//github.com/MRAMG-Bench/MRAMG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04176v2&entry.124074799=Read"},
{"title": "Existing Industry Practice for the EU AI Act's General-Purpose AI Code\n  of Practice Safety and Security Measures", "author": "Lily Stelling and Mick Yang and Rokas Gipi\u0161kis and Leon Staufer and Ze Shen Chin and Sim\u00e9on Campos and Michael Chen", "abstract": "  This report provides a detailed comparison between the measures proposed in\nthe EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and\ncurrent practices adopted by leading AI companies. As the EU moves toward\nenforcing binding obligations for GPAI model providers, the Code of Practice\nwill be key to bridging legal requirements with concrete technical commitments.\nOur analysis focuses on the draft's Safety and Security section which is only\nrelevant for the providers of the most advanced models (Commitments II.1-II.16)\nand excerpts from current public-facing documents quotes that are relevant to\neach individual measure.\n  We systematically reviewed different document types - including companies'\nfrontier safety frameworks and model cards - from over a dozen companies,\nincluding OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and\nothers. This report is not meant to be an indication of legal compliance nor\ndoes it take any prescriptive viewpoint about the Code of Practice or\ncompanies' policies. Instead, it aims to inform the ongoing dialogue between\nregulators and GPAI model providers by surfacing evidence of precedent.\n", "link": "http://arxiv.org/abs/2504.15181v1", "date": "2025-04-21", "relevancy": 2.0351, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Existing%20Industry%20Practice%20for%20the%20EU%20AI%20Act%27s%20General-Purpose%20AI%20Code%0A%20%20of%20Practice%20Safety%20and%20Security%20Measures&body=Title%3A%20Existing%20Industry%20Practice%20for%20the%20EU%20AI%20Act%27s%20General-Purpose%20AI%20Code%0A%20%20of%20Practice%20Safety%20and%20Security%20Measures%0AAuthor%3A%20Lily%20Stelling%20and%20Mick%20Yang%20and%20Rokas%20Gipi%C5%A1kis%20and%20Leon%20Staufer%20and%20Ze%20Shen%20Chin%20and%20Sim%C3%A9on%20Campos%20and%20Michael%20Chen%0AAbstract%3A%20%20%20This%20report%20provides%20a%20detailed%20comparison%20between%20the%20measures%20proposed%20in%0Athe%20EU%20AI%20Act%27s%20General-Purpose%20AI%20%28GPAI%29%20Code%20of%20Practice%20%28Third%20Draft%29%20and%0Acurrent%20practices%20adopted%20by%20leading%20AI%20companies.%20As%20the%20EU%20moves%20toward%0Aenforcing%20binding%20obligations%20for%20GPAI%20model%20providers%2C%20the%20Code%20of%20Practice%0Awill%20be%20key%20to%20bridging%20legal%20requirements%20with%20concrete%20technical%20commitments.%0AOur%20analysis%20focuses%20on%20the%20draft%27s%20Safety%20and%20Security%20section%20which%20is%20only%0Arelevant%20for%20the%20providers%20of%20the%20most%20advanced%20models%20%28Commitments%20II.1-II.16%29%0Aand%20excerpts%20from%20current%20public-facing%20documents%20quotes%20that%20are%20relevant%20to%0Aeach%20individual%20measure.%0A%20%20We%20systematically%20reviewed%20different%20document%20types%20-%20including%20companies%27%0Afrontier%20safety%20frameworks%20and%20model%20cards%20-%20from%20over%20a%20dozen%20companies%2C%0Aincluding%20OpenAI%2C%20Anthropic%2C%20Google%20DeepMind%2C%20Microsoft%2C%20Meta%2C%20Amazon%2C%20and%0Aothers.%20This%20report%20is%20not%20meant%20to%20be%20an%20indication%20of%20legal%20compliance%20nor%0Adoes%20it%20take%20any%20prescriptive%20viewpoint%20about%20the%20Code%20of%20Practice%20or%0Acompanies%27%20policies.%20Instead%2C%20it%20aims%20to%20inform%20the%20ongoing%20dialogue%20between%0Aregulators%20and%20GPAI%20model%20providers%20by%20surfacing%20evidence%20of%20precedent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExisting%2520Industry%2520Practice%2520for%2520the%2520EU%2520AI%2520Act%2527s%2520General-Purpose%2520AI%2520Code%250A%2520%2520of%2520Practice%2520Safety%2520and%2520Security%2520Measures%26entry.906535625%3DLily%2520Stelling%2520and%2520Mick%2520Yang%2520and%2520Rokas%2520Gipi%25C5%25A1kis%2520and%2520Leon%2520Staufer%2520and%2520Ze%2520Shen%2520Chin%2520and%2520Sim%25C3%25A9on%2520Campos%2520and%2520Michael%2520Chen%26entry.1292438233%3D%2520%2520This%2520report%2520provides%2520a%2520detailed%2520comparison%2520between%2520the%2520measures%2520proposed%2520in%250Athe%2520EU%2520AI%2520Act%2527s%2520General-Purpose%2520AI%2520%2528GPAI%2529%2520Code%2520of%2520Practice%2520%2528Third%2520Draft%2529%2520and%250Acurrent%2520practices%2520adopted%2520by%2520leading%2520AI%2520companies.%2520As%2520the%2520EU%2520moves%2520toward%250Aenforcing%2520binding%2520obligations%2520for%2520GPAI%2520model%2520providers%252C%2520the%2520Code%2520of%2520Practice%250Awill%2520be%2520key%2520to%2520bridging%2520legal%2520requirements%2520with%2520concrete%2520technical%2520commitments.%250AOur%2520analysis%2520focuses%2520on%2520the%2520draft%2527s%2520Safety%2520and%2520Security%2520section%2520which%2520is%2520only%250Arelevant%2520for%2520the%2520providers%2520of%2520the%2520most%2520advanced%2520models%2520%2528Commitments%2520II.1-II.16%2529%250Aand%2520excerpts%2520from%2520current%2520public-facing%2520documents%2520quotes%2520that%2520are%2520relevant%2520to%250Aeach%2520individual%2520measure.%250A%2520%2520We%2520systematically%2520reviewed%2520different%2520document%2520types%2520-%2520including%2520companies%2527%250Afrontier%2520safety%2520frameworks%2520and%2520model%2520cards%2520-%2520from%2520over%2520a%2520dozen%2520companies%252C%250Aincluding%2520OpenAI%252C%2520Anthropic%252C%2520Google%2520DeepMind%252C%2520Microsoft%252C%2520Meta%252C%2520Amazon%252C%2520and%250Aothers.%2520This%2520report%2520is%2520not%2520meant%2520to%2520be%2520an%2520indication%2520of%2520legal%2520compliance%2520nor%250Adoes%2520it%2520take%2520any%2520prescriptive%2520viewpoint%2520about%2520the%2520Code%2520of%2520Practice%2520or%250Acompanies%2527%2520policies.%2520Instead%252C%2520it%2520aims%2520to%2520inform%2520the%2520ongoing%2520dialogue%2520between%250Aregulators%2520and%2520GPAI%2520model%2520providers%2520by%2520surfacing%2520evidence%2520of%2520precedent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Existing%20Industry%20Practice%20for%20the%20EU%20AI%20Act%27s%20General-Purpose%20AI%20Code%0A%20%20of%20Practice%20Safety%20and%20Security%20Measures&entry.906535625=Lily%20Stelling%20and%20Mick%20Yang%20and%20Rokas%20Gipi%C5%A1kis%20and%20Leon%20Staufer%20and%20Ze%20Shen%20Chin%20and%20Sim%C3%A9on%20Campos%20and%20Michael%20Chen&entry.1292438233=%20%20This%20report%20provides%20a%20detailed%20comparison%20between%20the%20measures%20proposed%20in%0Athe%20EU%20AI%20Act%27s%20General-Purpose%20AI%20%28GPAI%29%20Code%20of%20Practice%20%28Third%20Draft%29%20and%0Acurrent%20practices%20adopted%20by%20leading%20AI%20companies.%20As%20the%20EU%20moves%20toward%0Aenforcing%20binding%20obligations%20for%20GPAI%20model%20providers%2C%20the%20Code%20of%20Practice%0Awill%20be%20key%20to%20bridging%20legal%20requirements%20with%20concrete%20technical%20commitments.%0AOur%20analysis%20focuses%20on%20the%20draft%27s%20Safety%20and%20Security%20section%20which%20is%20only%0Arelevant%20for%20the%20providers%20of%20the%20most%20advanced%20models%20%28Commitments%20II.1-II.16%29%0Aand%20excerpts%20from%20current%20public-facing%20documents%20quotes%20that%20are%20relevant%20to%0Aeach%20individual%20measure.%0A%20%20We%20systematically%20reviewed%20different%20document%20types%20-%20including%20companies%27%0Afrontier%20safety%20frameworks%20and%20model%20cards%20-%20from%20over%20a%20dozen%20companies%2C%0Aincluding%20OpenAI%2C%20Anthropic%2C%20Google%20DeepMind%2C%20Microsoft%2C%20Meta%2C%20Amazon%2C%20and%0Aothers.%20This%20report%20is%20not%20meant%20to%20be%20an%20indication%20of%20legal%20compliance%20nor%0Adoes%20it%20take%20any%20prescriptive%20viewpoint%20about%20the%20Code%20of%20Practice%20or%0Acompanies%27%20policies.%20Instead%2C%20it%20aims%20to%20inform%20the%20ongoing%20dialogue%20between%0Aregulators%20and%20GPAI%20model%20providers%20by%20surfacing%20evidence%20of%20precedent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15181v1&entry.124074799=Read"},
{"title": "Enhancing Efficiency in Multidevice Federated Learning through Data\n  Selection", "author": "Fan Mo and Mohammad Malekzadeh and Soumyajit Chatterjee and Fahim Kawsar and Akhil Mathur", "abstract": "  Ubiquitous wearable and mobile devices provide access to a diverse set of\ndata. However, the mobility demand for our devices naturally imposes\nconstraints on their computational and communication capabilities. A solution\nis to locally learn knowledge from data captured by ubiquitous devices, rather\nthan to store and transmit the data in its original form. In this paper, we\ndevelop a federated learning framework, called Centaur, to incorporate\non-device data selection at the edge, which allows partition-based training of\na deep neural nets through collaboration between constrained and resourceful\ndevices within the multidevice ecosystem of the same user. We benchmark on five\nneural net architecture and six datasets that include image data and wearable\nsensor time series. On average, Centaur achieves ~19% higher classification\naccuracy and ~58% lower federated training latency, compared to the baseline.\nWe also evaluate Centaur when dealing with imbalanced non-iid data, client\nparticipation heterogeneity, and different mobility patterns. To encourage\nfurther research in this area, we release our code at\nhttps://github.com/nokia-bell-labs/data-centric-federated-learning\n", "link": "http://arxiv.org/abs/2211.04175v5", "date": "2025-04-21", "relevancy": 2.0196, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.511}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.505}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Efficiency%20in%20Multidevice%20Federated%20Learning%20through%20Data%0A%20%20Selection&body=Title%3A%20Enhancing%20Efficiency%20in%20Multidevice%20Federated%20Learning%20through%20Data%0A%20%20Selection%0AAuthor%3A%20Fan%20Mo%20and%20Mohammad%20Malekzadeh%20and%20Soumyajit%20Chatterjee%20and%20Fahim%20Kawsar%20and%20Akhil%20Mathur%0AAbstract%3A%20%20%20Ubiquitous%20wearable%20and%20mobile%20devices%20provide%20access%20to%20a%20diverse%20set%20of%0Adata.%20However%2C%20the%20mobility%20demand%20for%20our%20devices%20naturally%20imposes%0Aconstraints%20on%20their%20computational%20and%20communication%20capabilities.%20A%20solution%0Ais%20to%20locally%20learn%20knowledge%20from%20data%20captured%20by%20ubiquitous%20devices%2C%20rather%0Athan%20to%20store%20and%20transmit%20the%20data%20in%20its%20original%20form.%20In%20this%20paper%2C%20we%0Adevelop%20a%20federated%20learning%20framework%2C%20called%20Centaur%2C%20to%20incorporate%0Aon-device%20data%20selection%20at%20the%20edge%2C%20which%20allows%20partition-based%20training%20of%0Aa%20deep%20neural%20nets%20through%20collaboration%20between%20constrained%20and%20resourceful%0Adevices%20within%20the%20multidevice%20ecosystem%20of%20the%20same%20user.%20We%20benchmark%20on%20five%0Aneural%20net%20architecture%20and%20six%20datasets%20that%20include%20image%20data%20and%20wearable%0Asensor%20time%20series.%20On%20average%2C%20Centaur%20achieves%20~19%25%20higher%20classification%0Aaccuracy%20and%20~58%25%20lower%20federated%20training%20latency%2C%20compared%20to%20the%20baseline.%0AWe%20also%20evaluate%20Centaur%20when%20dealing%20with%20imbalanced%20non-iid%20data%2C%20client%0Aparticipation%20heterogeneity%2C%20and%20different%20mobility%20patterns.%20To%20encourage%0Afurther%20research%20in%20this%20area%2C%20we%20release%20our%20code%20at%0Ahttps%3A//github.com/nokia-bell-labs/data-centric-federated-learning%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.04175v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Efficiency%2520in%2520Multidevice%2520Federated%2520Learning%2520through%2520Data%250A%2520%2520Selection%26entry.906535625%3DFan%2520Mo%2520and%2520Mohammad%2520Malekzadeh%2520and%2520Soumyajit%2520Chatterjee%2520and%2520Fahim%2520Kawsar%2520and%2520Akhil%2520Mathur%26entry.1292438233%3D%2520%2520Ubiquitous%2520wearable%2520and%2520mobile%2520devices%2520provide%2520access%2520to%2520a%2520diverse%2520set%2520of%250Adata.%2520However%252C%2520the%2520mobility%2520demand%2520for%2520our%2520devices%2520naturally%2520imposes%250Aconstraints%2520on%2520their%2520computational%2520and%2520communication%2520capabilities.%2520A%2520solution%250Ais%2520to%2520locally%2520learn%2520knowledge%2520from%2520data%2520captured%2520by%2520ubiquitous%2520devices%252C%2520rather%250Athan%2520to%2520store%2520and%2520transmit%2520the%2520data%2520in%2520its%2520original%2520form.%2520In%2520this%2520paper%252C%2520we%250Adevelop%2520a%2520federated%2520learning%2520framework%252C%2520called%2520Centaur%252C%2520to%2520incorporate%250Aon-device%2520data%2520selection%2520at%2520the%2520edge%252C%2520which%2520allows%2520partition-based%2520training%2520of%250Aa%2520deep%2520neural%2520nets%2520through%2520collaboration%2520between%2520constrained%2520and%2520resourceful%250Adevices%2520within%2520the%2520multidevice%2520ecosystem%2520of%2520the%2520same%2520user.%2520We%2520benchmark%2520on%2520five%250Aneural%2520net%2520architecture%2520and%2520six%2520datasets%2520that%2520include%2520image%2520data%2520and%2520wearable%250Asensor%2520time%2520series.%2520On%2520average%252C%2520Centaur%2520achieves%2520~19%2525%2520higher%2520classification%250Aaccuracy%2520and%2520~58%2525%2520lower%2520federated%2520training%2520latency%252C%2520compared%2520to%2520the%2520baseline.%250AWe%2520also%2520evaluate%2520Centaur%2520when%2520dealing%2520with%2520imbalanced%2520non-iid%2520data%252C%2520client%250Aparticipation%2520heterogeneity%252C%2520and%2520different%2520mobility%2520patterns.%2520To%2520encourage%250Afurther%2520research%2520in%2520this%2520area%252C%2520we%2520release%2520our%2520code%2520at%250Ahttps%253A//github.com/nokia-bell-labs/data-centric-federated-learning%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.04175v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Efficiency%20in%20Multidevice%20Federated%20Learning%20through%20Data%0A%20%20Selection&entry.906535625=Fan%20Mo%20and%20Mohammad%20Malekzadeh%20and%20Soumyajit%20Chatterjee%20and%20Fahim%20Kawsar%20and%20Akhil%20Mathur&entry.1292438233=%20%20Ubiquitous%20wearable%20and%20mobile%20devices%20provide%20access%20to%20a%20diverse%20set%20of%0Adata.%20However%2C%20the%20mobility%20demand%20for%20our%20devices%20naturally%20imposes%0Aconstraints%20on%20their%20computational%20and%20communication%20capabilities.%20A%20solution%0Ais%20to%20locally%20learn%20knowledge%20from%20data%20captured%20by%20ubiquitous%20devices%2C%20rather%0Athan%20to%20store%20and%20transmit%20the%20data%20in%20its%20original%20form.%20In%20this%20paper%2C%20we%0Adevelop%20a%20federated%20learning%20framework%2C%20called%20Centaur%2C%20to%20incorporate%0Aon-device%20data%20selection%20at%20the%20edge%2C%20which%20allows%20partition-based%20training%20of%0Aa%20deep%20neural%20nets%20through%20collaboration%20between%20constrained%20and%20resourceful%0Adevices%20within%20the%20multidevice%20ecosystem%20of%20the%20same%20user.%20We%20benchmark%20on%20five%0Aneural%20net%20architecture%20and%20six%20datasets%20that%20include%20image%20data%20and%20wearable%0Asensor%20time%20series.%20On%20average%2C%20Centaur%20achieves%20~19%25%20higher%20classification%0Aaccuracy%20and%20~58%25%20lower%20federated%20training%20latency%2C%20compared%20to%20the%20baseline.%0AWe%20also%20evaluate%20Centaur%20when%20dealing%20with%20imbalanced%20non-iid%20data%2C%20client%0Aparticipation%20heterogeneity%2C%20and%20different%20mobility%20patterns.%20To%20encourage%0Afurther%20research%20in%20this%20area%2C%20we%20release%20our%20code%20at%0Ahttps%3A//github.com/nokia-bell-labs/data-centric-federated-learning%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.04175v5&entry.124074799=Read"},
{"title": "A Self-Improving Coding Agent", "author": "Maxime Robeyns and Martin Szummer and Laurence Aitchison", "abstract": "  We demonstrate that an LLM coding agent, equipped with basic coding tools,\ncan autonomously edit itself, and thereby improve its performance on benchmark\ntasks. We find performance gains from 17% to 53% on a random subset of SWE\nBench Verified, with additional performance gains on LiveCodeBench, as well as\nsynthetically generated agent benchmarks. Our work represents an advancement in\nthe automated and open-ended design of agentic systems, and provides a\nreference agent framework for those seeking to post-train LLMs on tool use and\nother agentic tasks.\n", "link": "http://arxiv.org/abs/2504.15228v1", "date": "2025-04-21", "relevancy": 2.0047, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5349}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4781}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Self-Improving%20Coding%20Agent&body=Title%3A%20A%20Self-Improving%20Coding%20Agent%0AAuthor%3A%20Maxime%20Robeyns%20and%20Martin%20Szummer%20and%20Laurence%20Aitchison%0AAbstract%3A%20%20%20We%20demonstrate%20that%20an%20LLM%20coding%20agent%2C%20equipped%20with%20basic%20coding%20tools%2C%0Acan%20autonomously%20edit%20itself%2C%20and%20thereby%20improve%20its%20performance%20on%20benchmark%0Atasks.%20We%20find%20performance%20gains%20from%2017%25%20to%2053%25%20on%20a%20random%20subset%20of%20SWE%0ABench%20Verified%2C%20with%20additional%20performance%20gains%20on%20LiveCodeBench%2C%20as%20well%20as%0Asynthetically%20generated%20agent%20benchmarks.%20Our%20work%20represents%20an%20advancement%20in%0Athe%20automated%20and%20open-ended%20design%20of%20agentic%20systems%2C%20and%20provides%20a%0Areference%20agent%20framework%20for%20those%20seeking%20to%20post-train%20LLMs%20on%20tool%20use%20and%0Aother%20agentic%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Self-Improving%2520Coding%2520Agent%26entry.906535625%3DMaxime%2520Robeyns%2520and%2520Martin%2520Szummer%2520and%2520Laurence%2520Aitchison%26entry.1292438233%3D%2520%2520We%2520demonstrate%2520that%2520an%2520LLM%2520coding%2520agent%252C%2520equipped%2520with%2520basic%2520coding%2520tools%252C%250Acan%2520autonomously%2520edit%2520itself%252C%2520and%2520thereby%2520improve%2520its%2520performance%2520on%2520benchmark%250Atasks.%2520We%2520find%2520performance%2520gains%2520from%252017%2525%2520to%252053%2525%2520on%2520a%2520random%2520subset%2520of%2520SWE%250ABench%2520Verified%252C%2520with%2520additional%2520performance%2520gains%2520on%2520LiveCodeBench%252C%2520as%2520well%2520as%250Asynthetically%2520generated%2520agent%2520benchmarks.%2520Our%2520work%2520represents%2520an%2520advancement%2520in%250Athe%2520automated%2520and%2520open-ended%2520design%2520of%2520agentic%2520systems%252C%2520and%2520provides%2520a%250Areference%2520agent%2520framework%2520for%2520those%2520seeking%2520to%2520post-train%2520LLMs%2520on%2520tool%2520use%2520and%250Aother%2520agentic%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Self-Improving%20Coding%20Agent&entry.906535625=Maxime%20Robeyns%20and%20Martin%20Szummer%20and%20Laurence%20Aitchison&entry.1292438233=%20%20We%20demonstrate%20that%20an%20LLM%20coding%20agent%2C%20equipped%20with%20basic%20coding%20tools%2C%0Acan%20autonomously%20edit%20itself%2C%20and%20thereby%20improve%20its%20performance%20on%20benchmark%0Atasks.%20We%20find%20performance%20gains%20from%2017%25%20to%2053%25%20on%20a%20random%20subset%20of%20SWE%0ABench%20Verified%2C%20with%20additional%20performance%20gains%20on%20LiveCodeBench%2C%20as%20well%20as%0Asynthetically%20generated%20agent%20benchmarks.%20Our%20work%20represents%20an%20advancement%20in%0Athe%20automated%20and%20open-ended%20design%20of%20agentic%20systems%2C%20and%20provides%20a%0Areference%20agent%20framework%20for%20those%20seeking%20to%20post-train%20LLMs%20on%20tool%20use%20and%0Aother%20agentic%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15228v1&entry.124074799=Read"},
{"title": "Robust Planning and Control of Omnidirectional MRAVs for Aerial\n  Communications in Wireless Networks", "author": "Giuseppe Silano and Daniel Bonilla Licea and Hajar El Hammouti and Mounir Ghogho and and Martin Saska", "abstract": "  A new class of Multi-Rotor Aerial Vehicles (MRAVs), known as omnidirectional\nMRAVs (o-MRAVs), has gained attention for their ability to independently\ncontrol 3D position and orientation. This capability enhances robust planning\nand control in aerial communication networks, enabling more adaptive trajectory\nplanning and precise antenna alignment without additional mechanical\ncomponents. These features are particularly valuable in uncertain environments,\nwhere disturbances such as wind and interference affect communication\nstability. This paper examines o-MRAVs in the context of robust aerial network\nplanning, comparing them with the more common under-actuated MRAVs (u-MRAVs).\nKey applications, including physical layer security, optical communications,\nand network densification, are highlighted, demonstrating the potential of\no-MRAVs to improve reliability and efficiency in dynamic communication\nscenarios.\n", "link": "http://arxiv.org/abs/2504.15089v1", "date": "2025-04-21", "relevancy": 1.9984, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5082}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4936}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Planning%20and%20Control%20of%20Omnidirectional%20MRAVs%20for%20Aerial%0A%20%20Communications%20in%20Wireless%20Networks&body=Title%3A%20Robust%20Planning%20and%20Control%20of%20Omnidirectional%20MRAVs%20for%20Aerial%0A%20%20Communications%20in%20Wireless%20Networks%0AAuthor%3A%20Giuseppe%20Silano%20and%20Daniel%20Bonilla%20Licea%20and%20Hajar%20El%20Hammouti%20and%20Mounir%20Ghogho%20and%20and%20Martin%20Saska%0AAbstract%3A%20%20%20A%20new%20class%20of%20Multi-Rotor%20Aerial%20Vehicles%20%28MRAVs%29%2C%20known%20as%20omnidirectional%0AMRAVs%20%28o-MRAVs%29%2C%20has%20gained%20attention%20for%20their%20ability%20to%20independently%0Acontrol%203D%20position%20and%20orientation.%20This%20capability%20enhances%20robust%20planning%0Aand%20control%20in%20aerial%20communication%20networks%2C%20enabling%20more%20adaptive%20trajectory%0Aplanning%20and%20precise%20antenna%20alignment%20without%20additional%20mechanical%0Acomponents.%20These%20features%20are%20particularly%20valuable%20in%20uncertain%20environments%2C%0Awhere%20disturbances%20such%20as%20wind%20and%20interference%20affect%20communication%0Astability.%20This%20paper%20examines%20o-MRAVs%20in%20the%20context%20of%20robust%20aerial%20network%0Aplanning%2C%20comparing%20them%20with%20the%20more%20common%20under-actuated%20MRAVs%20%28u-MRAVs%29.%0AKey%20applications%2C%20including%20physical%20layer%20security%2C%20optical%20communications%2C%0Aand%20network%20densification%2C%20are%20highlighted%2C%20demonstrating%20the%20potential%20of%0Ao-MRAVs%20to%20improve%20reliability%20and%20efficiency%20in%20dynamic%20communication%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Planning%2520and%2520Control%2520of%2520Omnidirectional%2520MRAVs%2520for%2520Aerial%250A%2520%2520Communications%2520in%2520Wireless%2520Networks%26entry.906535625%3DGiuseppe%2520Silano%2520and%2520Daniel%2520Bonilla%2520Licea%2520and%2520Hajar%2520El%2520Hammouti%2520and%2520Mounir%2520Ghogho%2520and%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520A%2520new%2520class%2520of%2520Multi-Rotor%2520Aerial%2520Vehicles%2520%2528MRAVs%2529%252C%2520known%2520as%2520omnidirectional%250AMRAVs%2520%2528o-MRAVs%2529%252C%2520has%2520gained%2520attention%2520for%2520their%2520ability%2520to%2520independently%250Acontrol%25203D%2520position%2520and%2520orientation.%2520This%2520capability%2520enhances%2520robust%2520planning%250Aand%2520control%2520in%2520aerial%2520communication%2520networks%252C%2520enabling%2520more%2520adaptive%2520trajectory%250Aplanning%2520and%2520precise%2520antenna%2520alignment%2520without%2520additional%2520mechanical%250Acomponents.%2520These%2520features%2520are%2520particularly%2520valuable%2520in%2520uncertain%2520environments%252C%250Awhere%2520disturbances%2520such%2520as%2520wind%2520and%2520interference%2520affect%2520communication%250Astability.%2520This%2520paper%2520examines%2520o-MRAVs%2520in%2520the%2520context%2520of%2520robust%2520aerial%2520network%250Aplanning%252C%2520comparing%2520them%2520with%2520the%2520more%2520common%2520under-actuated%2520MRAVs%2520%2528u-MRAVs%2529.%250AKey%2520applications%252C%2520including%2520physical%2520layer%2520security%252C%2520optical%2520communications%252C%250Aand%2520network%2520densification%252C%2520are%2520highlighted%252C%2520demonstrating%2520the%2520potential%2520of%250Ao-MRAVs%2520to%2520improve%2520reliability%2520and%2520efficiency%2520in%2520dynamic%2520communication%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Planning%20and%20Control%20of%20Omnidirectional%20MRAVs%20for%20Aerial%0A%20%20Communications%20in%20Wireless%20Networks&entry.906535625=Giuseppe%20Silano%20and%20Daniel%20Bonilla%20Licea%20and%20Hajar%20El%20Hammouti%20and%20Mounir%20Ghogho%20and%20and%20Martin%20Saska&entry.1292438233=%20%20A%20new%20class%20of%20Multi-Rotor%20Aerial%20Vehicles%20%28MRAVs%29%2C%20known%20as%20omnidirectional%0AMRAVs%20%28o-MRAVs%29%2C%20has%20gained%20attention%20for%20their%20ability%20to%20independently%0Acontrol%203D%20position%20and%20orientation.%20This%20capability%20enhances%20robust%20planning%0Aand%20control%20in%20aerial%20communication%20networks%2C%20enabling%20more%20adaptive%20trajectory%0Aplanning%20and%20precise%20antenna%20alignment%20without%20additional%20mechanical%0Acomponents.%20These%20features%20are%20particularly%20valuable%20in%20uncertain%20environments%2C%0Awhere%20disturbances%20such%20as%20wind%20and%20interference%20affect%20communication%0Astability.%20This%20paper%20examines%20o-MRAVs%20in%20the%20context%20of%20robust%20aerial%20network%0Aplanning%2C%20comparing%20them%20with%20the%20more%20common%20under-actuated%20MRAVs%20%28u-MRAVs%29.%0AKey%20applications%2C%20including%20physical%20layer%20security%2C%20optical%20communications%2C%0Aand%20network%20densification%2C%20are%20highlighted%2C%20demonstrating%20the%20potential%20of%0Ao-MRAVs%20to%20improve%20reliability%20and%20efficiency%20in%20dynamic%20communication%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15089v1&entry.124074799=Read"},
{"title": "Decidability of Querying First-Order Theories via Countermodels of\n  Finite Width", "author": "Thomas Feller and Tim S. Lyon and Piotr Ostropolski-Nalewaja and Sebastian Rudolph", "abstract": "  We propose a generic framework for establishing the decidability of a wide\nrange of logical entailment problems (briefly called querying), based on the\nexistence of countermodels that are structurally simple, gauged by certain\ntypes of width measures (with treewidth and cliquewidth as popular examples).\nAs an important special case of our framework, we identify logics exhibiting\nwidth-finite finitely universal model sets, warranting decidable entailment for\na wide range of homomorphism-closed queries, subsuming a diverse set of\npractically relevant query languages. As a particularly powerful width measure,\nwe propose to employ Blumensath's partitionwidth, which subsumes various other\ncommonly considered width measures and exhibits highly favorable computational\nand structural properties. Focusing on the formalism of existential rules as a\npopular showcase, we explain how finite partitionwidth sets of rules subsume\nother known abstract decidable classes but - leveraging existing notions of\nstratification - also cover a wide range of new rulesets. We expose natural\nlimitations for fitting the class of finite unification sets into our picture\nand suggest several options for remedy.\n", "link": "http://arxiv.org/abs/2304.06348v5", "date": "2025-04-21", "relevancy": 1.9823, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decidability%20of%20Querying%20First-Order%20Theories%20via%20Countermodels%20of%0A%20%20Finite%20Width&body=Title%3A%20Decidability%20of%20Querying%20First-Order%20Theories%20via%20Countermodels%20of%0A%20%20Finite%20Width%0AAuthor%3A%20Thomas%20Feller%20and%20Tim%20S.%20Lyon%20and%20Piotr%20Ostropolski-Nalewaja%20and%20Sebastian%20Rudolph%0AAbstract%3A%20%20%20We%20propose%20a%20generic%20framework%20for%20establishing%20the%20decidability%20of%20a%20wide%0Arange%20of%20logical%20entailment%20problems%20%28briefly%20called%20querying%29%2C%20based%20on%20the%0Aexistence%20of%20countermodels%20that%20are%20structurally%20simple%2C%20gauged%20by%20certain%0Atypes%20of%20width%20measures%20%28with%20treewidth%20and%20cliquewidth%20as%20popular%20examples%29.%0AAs%20an%20important%20special%20case%20of%20our%20framework%2C%20we%20identify%20logics%20exhibiting%0Awidth-finite%20finitely%20universal%20model%20sets%2C%20warranting%20decidable%20entailment%20for%0Aa%20wide%20range%20of%20homomorphism-closed%20queries%2C%20subsuming%20a%20diverse%20set%20of%0Apractically%20relevant%20query%20languages.%20As%20a%20particularly%20powerful%20width%20measure%2C%0Awe%20propose%20to%20employ%20Blumensath%27s%20partitionwidth%2C%20which%20subsumes%20various%20other%0Acommonly%20considered%20width%20measures%20and%20exhibits%20highly%20favorable%20computational%0Aand%20structural%20properties.%20Focusing%20on%20the%20formalism%20of%20existential%20rules%20as%20a%0Apopular%20showcase%2C%20we%20explain%20how%20finite%20partitionwidth%20sets%20of%20rules%20subsume%0Aother%20known%20abstract%20decidable%20classes%20but%20-%20leveraging%20existing%20notions%20of%0Astratification%20-%20also%20cover%20a%20wide%20range%20of%20new%20rulesets.%20We%20expose%20natural%0Alimitations%20for%20fitting%20the%20class%20of%20finite%20unification%20sets%20into%20our%20picture%0Aand%20suggest%20several%20options%20for%20remedy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.06348v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecidability%2520of%2520Querying%2520First-Order%2520Theories%2520via%2520Countermodels%2520of%250A%2520%2520Finite%2520Width%26entry.906535625%3DThomas%2520Feller%2520and%2520Tim%2520S.%2520Lyon%2520and%2520Piotr%2520Ostropolski-Nalewaja%2520and%2520Sebastian%2520Rudolph%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520generic%2520framework%2520for%2520establishing%2520the%2520decidability%2520of%2520a%2520wide%250Arange%2520of%2520logical%2520entailment%2520problems%2520%2528briefly%2520called%2520querying%2529%252C%2520based%2520on%2520the%250Aexistence%2520of%2520countermodels%2520that%2520are%2520structurally%2520simple%252C%2520gauged%2520by%2520certain%250Atypes%2520of%2520width%2520measures%2520%2528with%2520treewidth%2520and%2520cliquewidth%2520as%2520popular%2520examples%2529.%250AAs%2520an%2520important%2520special%2520case%2520of%2520our%2520framework%252C%2520we%2520identify%2520logics%2520exhibiting%250Awidth-finite%2520finitely%2520universal%2520model%2520sets%252C%2520warranting%2520decidable%2520entailment%2520for%250Aa%2520wide%2520range%2520of%2520homomorphism-closed%2520queries%252C%2520subsuming%2520a%2520diverse%2520set%2520of%250Apractically%2520relevant%2520query%2520languages.%2520As%2520a%2520particularly%2520powerful%2520width%2520measure%252C%250Awe%2520propose%2520to%2520employ%2520Blumensath%2527s%2520partitionwidth%252C%2520which%2520subsumes%2520various%2520other%250Acommonly%2520considered%2520width%2520measures%2520and%2520exhibits%2520highly%2520favorable%2520computational%250Aand%2520structural%2520properties.%2520Focusing%2520on%2520the%2520formalism%2520of%2520existential%2520rules%2520as%2520a%250Apopular%2520showcase%252C%2520we%2520explain%2520how%2520finite%2520partitionwidth%2520sets%2520of%2520rules%2520subsume%250Aother%2520known%2520abstract%2520decidable%2520classes%2520but%2520-%2520leveraging%2520existing%2520notions%2520of%250Astratification%2520-%2520also%2520cover%2520a%2520wide%2520range%2520of%2520new%2520rulesets.%2520We%2520expose%2520natural%250Alimitations%2520for%2520fitting%2520the%2520class%2520of%2520finite%2520unification%2520sets%2520into%2520our%2520picture%250Aand%2520suggest%2520several%2520options%2520for%2520remedy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.06348v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decidability%20of%20Querying%20First-Order%20Theories%20via%20Countermodels%20of%0A%20%20Finite%20Width&entry.906535625=Thomas%20Feller%20and%20Tim%20S.%20Lyon%20and%20Piotr%20Ostropolski-Nalewaja%20and%20Sebastian%20Rudolph&entry.1292438233=%20%20We%20propose%20a%20generic%20framework%20for%20establishing%20the%20decidability%20of%20a%20wide%0Arange%20of%20logical%20entailment%20problems%20%28briefly%20called%20querying%29%2C%20based%20on%20the%0Aexistence%20of%20countermodels%20that%20are%20structurally%20simple%2C%20gauged%20by%20certain%0Atypes%20of%20width%20measures%20%28with%20treewidth%20and%20cliquewidth%20as%20popular%20examples%29.%0AAs%20an%20important%20special%20case%20of%20our%20framework%2C%20we%20identify%20logics%20exhibiting%0Awidth-finite%20finitely%20universal%20model%20sets%2C%20warranting%20decidable%20entailment%20for%0Aa%20wide%20range%20of%20homomorphism-closed%20queries%2C%20subsuming%20a%20diverse%20set%20of%0Apractically%20relevant%20query%20languages.%20As%20a%20particularly%20powerful%20width%20measure%2C%0Awe%20propose%20to%20employ%20Blumensath%27s%20partitionwidth%2C%20which%20subsumes%20various%20other%0Acommonly%20considered%20width%20measures%20and%20exhibits%20highly%20favorable%20computational%0Aand%20structural%20properties.%20Focusing%20on%20the%20formalism%20of%20existential%20rules%20as%20a%0Apopular%20showcase%2C%20we%20explain%20how%20finite%20partitionwidth%20sets%20of%20rules%20subsume%0Aother%20known%20abstract%20decidable%20classes%20but%20-%20leveraging%20existing%20notions%20of%0Astratification%20-%20also%20cover%20a%20wide%20range%20of%20new%20rulesets.%20We%20expose%20natural%0Alimitations%20for%20fitting%20the%20class%20of%20finite%20unification%20sets%20into%20our%20picture%0Aand%20suggest%20several%20options%20for%20remedy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.06348v5&entry.124074799=Read"},
{"title": "CTINexus: Automatic Cyber Threat Intelligence Knowledge Graph\n  Construction Using Large Language Models", "author": "Yutong Cheng and Osama Bajaber and Saimon Amanuel Tsegai and Dawn Song and Peng Gao", "abstract": "  Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI knowledge extraction methods lack flexibility\nand generalizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through: (1) a carefully designed automatic prompt\nconstruction strategy with optimal demonstration retrieval for extracting a\nwide range of cybersecurity entities and relations; (2) a hierarchical entity\nalignment technique that canonicalizes the extracted knowledge and removes\nredundancy; (3) an long-distance relation prediction technique to further\ncomplete the CSKG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKG, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape.\n", "link": "http://arxiv.org/abs/2410.21060v2", "date": "2025-04-21", "relevancy": 1.9819, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CTINexus%3A%20Automatic%20Cyber%20Threat%20Intelligence%20Knowledge%20Graph%0A%20%20Construction%20Using%20Large%20Language%20Models&body=Title%3A%20CTINexus%3A%20Automatic%20Cyber%20Threat%20Intelligence%20Knowledge%20Graph%0A%20%20Construction%20Using%20Large%20Language%20Models%0AAuthor%3A%20Yutong%20Cheng%20and%20Osama%20Bajaber%20and%20Saimon%20Amanuel%20Tsegai%20and%20Dawn%20Song%20and%20Peng%20Gao%0AAbstract%3A%20%20%20Textual%20descriptions%20in%20cyber%20threat%20intelligence%20%28CTI%29%20reports%2C%20such%20as%0Asecurity%20articles%20and%20news%2C%20are%20rich%20sources%20of%20knowledge%20about%20cyber%20threats%2C%0Acrucial%20for%20organizations%20to%20stay%20informed%20about%20the%20rapidly%20evolving%20threat%0Alandscape.%20However%2C%20current%20CTI%20knowledge%20extraction%20methods%20lack%20flexibility%0Aand%20generalizability%2C%20often%20resulting%20in%20inaccurate%20and%20incomplete%20knowledge%0Aextraction.%20Syntax%20parsing%20relies%20on%20fixed%20rules%20and%20dictionaries%2C%20while%20model%0Afine-tuning%20requires%20large%20annotated%20datasets%2C%20making%20both%20paradigms%0Achallenging%20to%20adapt%20to%20new%20threats%20and%20ontologies.%20To%20bridge%20the%20gap%2C%20we%0Apropose%20CTINexus%2C%20a%20novel%20framework%20leveraging%20optimized%20in-context%20learning%0A%28ICL%29%20of%20large%20language%20models%20%28LLMs%29%20for%20data-efficient%20CTI%20knowledge%0Aextraction%20and%20high-quality%20cybersecurity%20knowledge%20graph%20%28CSKG%29%20construction.%0AUnlike%20existing%20methods%2C%20CTINexus%20requires%20neither%20extensive%20data%20nor%20parameter%0Atuning%20and%20can%20adapt%20to%20various%20ontologies%20with%20minimal%20annotated%20examples.%0AThis%20is%20achieved%20through%3A%20%281%29%20a%20carefully%20designed%20automatic%20prompt%0Aconstruction%20strategy%20with%20optimal%20demonstration%20retrieval%20for%20extracting%20a%0Awide%20range%20of%20cybersecurity%20entities%20and%20relations%3B%20%282%29%20a%20hierarchical%20entity%0Aalignment%20technique%20that%20canonicalizes%20the%20extracted%20knowledge%20and%20removes%0Aredundancy%3B%20%283%29%20an%20long-distance%20relation%20prediction%20technique%20to%20further%0Acomplete%20the%20CSKG%20with%20missing%20links.%20Our%20extensive%20evaluations%20using%20150%0Areal-world%20CTI%20reports%20collected%20from%2010%20platforms%20demonstrate%20that%20CTINexus%0Asignificantly%20outperforms%20existing%20methods%20in%20constructing%20accurate%20and%0Acomplete%20CSKG%2C%20highlighting%20its%20potential%20to%20transform%20CTI%20analysis%20with%20an%0Aefficient%20and%20adaptable%20solution%20for%20the%20dynamic%20threat%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21060v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCTINexus%253A%2520Automatic%2520Cyber%2520Threat%2520Intelligence%2520Knowledge%2520Graph%250A%2520%2520Construction%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DYutong%2520Cheng%2520and%2520Osama%2520Bajaber%2520and%2520Saimon%2520Amanuel%2520Tsegai%2520and%2520Dawn%2520Song%2520and%2520Peng%2520Gao%26entry.1292438233%3D%2520%2520Textual%2520descriptions%2520in%2520cyber%2520threat%2520intelligence%2520%2528CTI%2529%2520reports%252C%2520such%2520as%250Asecurity%2520articles%2520and%2520news%252C%2520are%2520rich%2520sources%2520of%2520knowledge%2520about%2520cyber%2520threats%252C%250Acrucial%2520for%2520organizations%2520to%2520stay%2520informed%2520about%2520the%2520rapidly%2520evolving%2520threat%250Alandscape.%2520However%252C%2520current%2520CTI%2520knowledge%2520extraction%2520methods%2520lack%2520flexibility%250Aand%2520generalizability%252C%2520often%2520resulting%2520in%2520inaccurate%2520and%2520incomplete%2520knowledge%250Aextraction.%2520Syntax%2520parsing%2520relies%2520on%2520fixed%2520rules%2520and%2520dictionaries%252C%2520while%2520model%250Afine-tuning%2520requires%2520large%2520annotated%2520datasets%252C%2520making%2520both%2520paradigms%250Achallenging%2520to%2520adapt%2520to%2520new%2520threats%2520and%2520ontologies.%2520To%2520bridge%2520the%2520gap%252C%2520we%250Apropose%2520CTINexus%252C%2520a%2520novel%2520framework%2520leveraging%2520optimized%2520in-context%2520learning%250A%2528ICL%2529%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520data-efficient%2520CTI%2520knowledge%250Aextraction%2520and%2520high-quality%2520cybersecurity%2520knowledge%2520graph%2520%2528CSKG%2529%2520construction.%250AUnlike%2520existing%2520methods%252C%2520CTINexus%2520requires%2520neither%2520extensive%2520data%2520nor%2520parameter%250Atuning%2520and%2520can%2520adapt%2520to%2520various%2520ontologies%2520with%2520minimal%2520annotated%2520examples.%250AThis%2520is%2520achieved%2520through%253A%2520%25281%2529%2520a%2520carefully%2520designed%2520automatic%2520prompt%250Aconstruction%2520strategy%2520with%2520optimal%2520demonstration%2520retrieval%2520for%2520extracting%2520a%250Awide%2520range%2520of%2520cybersecurity%2520entities%2520and%2520relations%253B%2520%25282%2529%2520a%2520hierarchical%2520entity%250Aalignment%2520technique%2520that%2520canonicalizes%2520the%2520extracted%2520knowledge%2520and%2520removes%250Aredundancy%253B%2520%25283%2529%2520an%2520long-distance%2520relation%2520prediction%2520technique%2520to%2520further%250Acomplete%2520the%2520CSKG%2520with%2520missing%2520links.%2520Our%2520extensive%2520evaluations%2520using%2520150%250Areal-world%2520CTI%2520reports%2520collected%2520from%252010%2520platforms%2520demonstrate%2520that%2520CTINexus%250Asignificantly%2520outperforms%2520existing%2520methods%2520in%2520constructing%2520accurate%2520and%250Acomplete%2520CSKG%252C%2520highlighting%2520its%2520potential%2520to%2520transform%2520CTI%2520analysis%2520with%2520an%250Aefficient%2520and%2520adaptable%2520solution%2520for%2520the%2520dynamic%2520threat%2520landscape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21060v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CTINexus%3A%20Automatic%20Cyber%20Threat%20Intelligence%20Knowledge%20Graph%0A%20%20Construction%20Using%20Large%20Language%20Models&entry.906535625=Yutong%20Cheng%20and%20Osama%20Bajaber%20and%20Saimon%20Amanuel%20Tsegai%20and%20Dawn%20Song%20and%20Peng%20Gao&entry.1292438233=%20%20Textual%20descriptions%20in%20cyber%20threat%20intelligence%20%28CTI%29%20reports%2C%20such%20as%0Asecurity%20articles%20and%20news%2C%20are%20rich%20sources%20of%20knowledge%20about%20cyber%20threats%2C%0Acrucial%20for%20organizations%20to%20stay%20informed%20about%20the%20rapidly%20evolving%20threat%0Alandscape.%20However%2C%20current%20CTI%20knowledge%20extraction%20methods%20lack%20flexibility%0Aand%20generalizability%2C%20often%20resulting%20in%20inaccurate%20and%20incomplete%20knowledge%0Aextraction.%20Syntax%20parsing%20relies%20on%20fixed%20rules%20and%20dictionaries%2C%20while%20model%0Afine-tuning%20requires%20large%20annotated%20datasets%2C%20making%20both%20paradigms%0Achallenging%20to%20adapt%20to%20new%20threats%20and%20ontologies.%20To%20bridge%20the%20gap%2C%20we%0Apropose%20CTINexus%2C%20a%20novel%20framework%20leveraging%20optimized%20in-context%20learning%0A%28ICL%29%20of%20large%20language%20models%20%28LLMs%29%20for%20data-efficient%20CTI%20knowledge%0Aextraction%20and%20high-quality%20cybersecurity%20knowledge%20graph%20%28CSKG%29%20construction.%0AUnlike%20existing%20methods%2C%20CTINexus%20requires%20neither%20extensive%20data%20nor%20parameter%0Atuning%20and%20can%20adapt%20to%20various%20ontologies%20with%20minimal%20annotated%20examples.%0AThis%20is%20achieved%20through%3A%20%281%29%20a%20carefully%20designed%20automatic%20prompt%0Aconstruction%20strategy%20with%20optimal%20demonstration%20retrieval%20for%20extracting%20a%0Awide%20range%20of%20cybersecurity%20entities%20and%20relations%3B%20%282%29%20a%20hierarchical%20entity%0Aalignment%20technique%20that%20canonicalizes%20the%20extracted%20knowledge%20and%20removes%0Aredundancy%3B%20%283%29%20an%20long-distance%20relation%20prediction%20technique%20to%20further%0Acomplete%20the%20CSKG%20with%20missing%20links.%20Our%20extensive%20evaluations%20using%20150%0Areal-world%20CTI%20reports%20collected%20from%2010%20platforms%20demonstrate%20that%20CTINexus%0Asignificantly%20outperforms%20existing%20methods%20in%20constructing%20accurate%20and%0Acomplete%20CSKG%2C%20highlighting%20its%20potential%20to%20transform%20CTI%20analysis%20with%20an%0Aefficient%20and%20adaptable%20solution%20for%20the%20dynamic%20threat%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21060v2&entry.124074799=Read"},
{"title": "VeLU: Variance-enhanced Learning Unit for Deep Neural Networks", "author": "Ashkan Shakarami and Yousef Yeganeh and Azade Farshad and Lorenzo Nicol\u00e8 and Stefano Ghidoni and Nassir Navab", "abstract": "  Activation functions are fundamental in deep neural networks and directly\nimpact gradient flow, optimization stability, and generalization. Although ReLU\nremains standard because of its simplicity, it suffers from vanishing gradients\nand lacks adaptability. Alternatives like Swish and GELU introduce smooth\ntransitions, but fail to dynamically adjust to input statistics. We propose\nVeLU, a Variance-enhanced Learning Unit as an activation function that\ndynamically scales based on input variance by integrating ArcTan-Sin\ntransformations and Wasserstein-2 regularization, effectively mitigating\ncovariate shifts and stabilizing optimization. Extensive experiments on\nViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm\nVeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.\nThe codes of VeLU are publicly available on GitHub.\n", "link": "http://arxiv.org/abs/2504.15051v1", "date": "2025-04-21", "relevancy": 1.9735, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5093}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4971}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VeLU%3A%20Variance-enhanced%20Learning%20Unit%20for%20Deep%20Neural%20Networks&body=Title%3A%20VeLU%3A%20Variance-enhanced%20Learning%20Unit%20for%20Deep%20Neural%20Networks%0AAuthor%3A%20Ashkan%20Shakarami%20and%20Yousef%20Yeganeh%20and%20Azade%20Farshad%20and%20Lorenzo%20Nicol%C3%A8%20and%20Stefano%20Ghidoni%20and%20Nassir%20Navab%0AAbstract%3A%20%20%20Activation%20functions%20are%20fundamental%20in%20deep%20neural%20networks%20and%20directly%0Aimpact%20gradient%20flow%2C%20optimization%20stability%2C%20and%20generalization.%20Although%20ReLU%0Aremains%20standard%20because%20of%20its%20simplicity%2C%20it%20suffers%20from%20vanishing%20gradients%0Aand%20lacks%20adaptability.%20Alternatives%20like%20Swish%20and%20GELU%20introduce%20smooth%0Atransitions%2C%20but%20fail%20to%20dynamically%20adjust%20to%20input%20statistics.%20We%20propose%0AVeLU%2C%20a%20Variance-enhanced%20Learning%20Unit%20as%20an%20activation%20function%20that%0Adynamically%20scales%20based%20on%20input%20variance%20by%20integrating%20ArcTan-Sin%0Atransformations%20and%20Wasserstein-2%20regularization%2C%20effectively%20mitigating%0Acovariate%20shifts%20and%20stabilizing%20optimization.%20Extensive%20experiments%20on%0AViT_B16%2C%20VGG19%2C%20ResNet50%2C%20DenseNet121%2C%20MobileNetV2%2C%20and%20EfficientNetB3%20confirm%0AVeLU%27s%20superiority%20over%20ReLU%2C%20ReLU6%2C%20Swish%2C%20and%20GELU%20on%20six%20vision%20benchmarks.%0AThe%20codes%20of%20VeLU%20are%20publicly%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVeLU%253A%2520Variance-enhanced%2520Learning%2520Unit%2520for%2520Deep%2520Neural%2520Networks%26entry.906535625%3DAshkan%2520Shakarami%2520and%2520Yousef%2520Yeganeh%2520and%2520Azade%2520Farshad%2520and%2520Lorenzo%2520Nicol%25C3%25A8%2520and%2520Stefano%2520Ghidoni%2520and%2520Nassir%2520Navab%26entry.1292438233%3D%2520%2520Activation%2520functions%2520are%2520fundamental%2520in%2520deep%2520neural%2520networks%2520and%2520directly%250Aimpact%2520gradient%2520flow%252C%2520optimization%2520stability%252C%2520and%2520generalization.%2520Although%2520ReLU%250Aremains%2520standard%2520because%2520of%2520its%2520simplicity%252C%2520it%2520suffers%2520from%2520vanishing%2520gradients%250Aand%2520lacks%2520adaptability.%2520Alternatives%2520like%2520Swish%2520and%2520GELU%2520introduce%2520smooth%250Atransitions%252C%2520but%2520fail%2520to%2520dynamically%2520adjust%2520to%2520input%2520statistics.%2520We%2520propose%250AVeLU%252C%2520a%2520Variance-enhanced%2520Learning%2520Unit%2520as%2520an%2520activation%2520function%2520that%250Adynamically%2520scales%2520based%2520on%2520input%2520variance%2520by%2520integrating%2520ArcTan-Sin%250Atransformations%2520and%2520Wasserstein-2%2520regularization%252C%2520effectively%2520mitigating%250Acovariate%2520shifts%2520and%2520stabilizing%2520optimization.%2520Extensive%2520experiments%2520on%250AViT_B16%252C%2520VGG19%252C%2520ResNet50%252C%2520DenseNet121%252C%2520MobileNetV2%252C%2520and%2520EfficientNetB3%2520confirm%250AVeLU%2527s%2520superiority%2520over%2520ReLU%252C%2520ReLU6%252C%2520Swish%252C%2520and%2520GELU%2520on%2520six%2520vision%2520benchmarks.%250AThe%2520codes%2520of%2520VeLU%2520are%2520publicly%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VeLU%3A%20Variance-enhanced%20Learning%20Unit%20for%20Deep%20Neural%20Networks&entry.906535625=Ashkan%20Shakarami%20and%20Yousef%20Yeganeh%20and%20Azade%20Farshad%20and%20Lorenzo%20Nicol%C3%A8%20and%20Stefano%20Ghidoni%20and%20Nassir%20Navab&entry.1292438233=%20%20Activation%20functions%20are%20fundamental%20in%20deep%20neural%20networks%20and%20directly%0Aimpact%20gradient%20flow%2C%20optimization%20stability%2C%20and%20generalization.%20Although%20ReLU%0Aremains%20standard%20because%20of%20its%20simplicity%2C%20it%20suffers%20from%20vanishing%20gradients%0Aand%20lacks%20adaptability.%20Alternatives%20like%20Swish%20and%20GELU%20introduce%20smooth%0Atransitions%2C%20but%20fail%20to%20dynamically%20adjust%20to%20input%20statistics.%20We%20propose%0AVeLU%2C%20a%20Variance-enhanced%20Learning%20Unit%20as%20an%20activation%20function%20that%0Adynamically%20scales%20based%20on%20input%20variance%20by%20integrating%20ArcTan-Sin%0Atransformations%20and%20Wasserstein-2%20regularization%2C%20effectively%20mitigating%0Acovariate%20shifts%20and%20stabilizing%20optimization.%20Extensive%20experiments%20on%0AViT_B16%2C%20VGG19%2C%20ResNet50%2C%20DenseNet121%2C%20MobileNetV2%2C%20and%20EfficientNetB3%20confirm%0AVeLU%27s%20superiority%20over%20ReLU%2C%20ReLU6%2C%20Swish%2C%20and%20GELU%20on%20six%20vision%20benchmarks.%0AThe%20codes%20of%20VeLU%20are%20publicly%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15051v1&entry.124074799=Read"},
{"title": "Values in the Wild: Discovering and Analyzing Values in Real-World\n  Language Model Interactions", "author": "Saffron Huang and Esin Durmus and Miles McCain and Kunal Handa and Alex Tamkin and Jerry Hong and Michael Stern and Arushi Somani and Xiuruo Zhang and Deep Ganguli", "abstract": "  AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems.\n", "link": "http://arxiv.org/abs/2504.15236v1", "date": "2025-04-21", "relevancy": 1.9731, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Values%20in%20the%20Wild%3A%20Discovering%20and%20Analyzing%20Values%20in%20Real-World%0A%20%20Language%20Model%20Interactions&body=Title%3A%20Values%20in%20the%20Wild%3A%20Discovering%20and%20Analyzing%20Values%20in%20Real-World%0A%20%20Language%20Model%20Interactions%0AAuthor%3A%20Saffron%20Huang%20and%20Esin%20Durmus%20and%20Miles%20McCain%20and%20Kunal%20Handa%20and%20Alex%20Tamkin%20and%20Jerry%20Hong%20and%20Michael%20Stern%20and%20Arushi%20Somani%20and%20Xiuruo%20Zhang%20and%20Deep%20Ganguli%0AAbstract%3A%20%20%20AI%20assistants%20can%20impart%20value%20judgments%20that%20shape%20people%27s%20decisions%20and%0Aworldviews%2C%20yet%20little%20is%20known%20empirically%20about%20what%20values%20these%20systems%0Arely%20on%20in%20practice.%20To%20address%20this%2C%20we%20develop%20a%20bottom-up%2C%0Aprivacy-preserving%20method%20to%20extract%20the%20values%20%28normative%20considerations%0Astated%20or%20demonstrated%20in%20model%20responses%29%20that%20Claude%203%20and%203.5%20models%20exhibit%0Ain%20hundreds%20of%20thousands%20of%20real-world%20interactions.%20We%20empirically%20discover%0Aand%20taxonomize%203%2C307%20AI%20values%20and%20study%20how%20they%20vary%20by%20context.%20We%20find%20that%0AClaude%20expresses%20many%20practical%20and%20epistemic%20values%2C%20and%20typically%20supports%0Aprosocial%20human%20values%20while%20resisting%20values%20like%20%22moral%20nihilism%22.%20While%20some%0Avalues%20appear%20consistently%20across%20contexts%20%28e.g.%20%22transparency%22%29%2C%20many%20are%20more%0Aspecialized%20and%20context-dependent%2C%20reflecting%20the%20diversity%20of%20human%0Ainterlocutors%20and%20their%20varied%20contexts.%20For%20example%2C%20%22harm%20prevention%22%20emerges%0Awhen%20Claude%20resists%20users%2C%20%22historical%20accuracy%22%20when%20responding%20to%20queries%0Aabout%20controversial%20events%2C%20%22healthy%20boundaries%22%20when%20asked%20for%20relationship%0Aadvice%2C%20and%20%22human%20agency%22%20in%20technology%20ethics%20discussions.%20By%20providing%20the%0Afirst%20large-scale%20empirical%20mapping%20of%20AI%20values%20in%20deployment%2C%20our%20work%0Acreates%20a%20foundation%20for%20more%20grounded%20evaluation%20and%20design%20of%20values%20in%20AI%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValues%2520in%2520the%2520Wild%253A%2520Discovering%2520and%2520Analyzing%2520Values%2520in%2520Real-World%250A%2520%2520Language%2520Model%2520Interactions%26entry.906535625%3DSaffron%2520Huang%2520and%2520Esin%2520Durmus%2520and%2520Miles%2520McCain%2520and%2520Kunal%2520Handa%2520and%2520Alex%2520Tamkin%2520and%2520Jerry%2520Hong%2520and%2520Michael%2520Stern%2520and%2520Arushi%2520Somani%2520and%2520Xiuruo%2520Zhang%2520and%2520Deep%2520Ganguli%26entry.1292438233%3D%2520%2520AI%2520assistants%2520can%2520impart%2520value%2520judgments%2520that%2520shape%2520people%2527s%2520decisions%2520and%250Aworldviews%252C%2520yet%2520little%2520is%2520known%2520empirically%2520about%2520what%2520values%2520these%2520systems%250Arely%2520on%2520in%2520practice.%2520To%2520address%2520this%252C%2520we%2520develop%2520a%2520bottom-up%252C%250Aprivacy-preserving%2520method%2520to%2520extract%2520the%2520values%2520%2528normative%2520considerations%250Astated%2520or%2520demonstrated%2520in%2520model%2520responses%2529%2520that%2520Claude%25203%2520and%25203.5%2520models%2520exhibit%250Ain%2520hundreds%2520of%2520thousands%2520of%2520real-world%2520interactions.%2520We%2520empirically%2520discover%250Aand%2520taxonomize%25203%252C307%2520AI%2520values%2520and%2520study%2520how%2520they%2520vary%2520by%2520context.%2520We%2520find%2520that%250AClaude%2520expresses%2520many%2520practical%2520and%2520epistemic%2520values%252C%2520and%2520typically%2520supports%250Aprosocial%2520human%2520values%2520while%2520resisting%2520values%2520like%2520%2522moral%2520nihilism%2522.%2520While%2520some%250Avalues%2520appear%2520consistently%2520across%2520contexts%2520%2528e.g.%2520%2522transparency%2522%2529%252C%2520many%2520are%2520more%250Aspecialized%2520and%2520context-dependent%252C%2520reflecting%2520the%2520diversity%2520of%2520human%250Ainterlocutors%2520and%2520their%2520varied%2520contexts.%2520For%2520example%252C%2520%2522harm%2520prevention%2522%2520emerges%250Awhen%2520Claude%2520resists%2520users%252C%2520%2522historical%2520accuracy%2522%2520when%2520responding%2520to%2520queries%250Aabout%2520controversial%2520events%252C%2520%2522healthy%2520boundaries%2522%2520when%2520asked%2520for%2520relationship%250Aadvice%252C%2520and%2520%2522human%2520agency%2522%2520in%2520technology%2520ethics%2520discussions.%2520By%2520providing%2520the%250Afirst%2520large-scale%2520empirical%2520mapping%2520of%2520AI%2520values%2520in%2520deployment%252C%2520our%2520work%250Acreates%2520a%2520foundation%2520for%2520more%2520grounded%2520evaluation%2520and%2520design%2520of%2520values%2520in%2520AI%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Values%20in%20the%20Wild%3A%20Discovering%20and%20Analyzing%20Values%20in%20Real-World%0A%20%20Language%20Model%20Interactions&entry.906535625=Saffron%20Huang%20and%20Esin%20Durmus%20and%20Miles%20McCain%20and%20Kunal%20Handa%20and%20Alex%20Tamkin%20and%20Jerry%20Hong%20and%20Michael%20Stern%20and%20Arushi%20Somani%20and%20Xiuruo%20Zhang%20and%20Deep%20Ganguli&entry.1292438233=%20%20AI%20assistants%20can%20impart%20value%20judgments%20that%20shape%20people%27s%20decisions%20and%0Aworldviews%2C%20yet%20little%20is%20known%20empirically%20about%20what%20values%20these%20systems%0Arely%20on%20in%20practice.%20To%20address%20this%2C%20we%20develop%20a%20bottom-up%2C%0Aprivacy-preserving%20method%20to%20extract%20the%20values%20%28normative%20considerations%0Astated%20or%20demonstrated%20in%20model%20responses%29%20that%20Claude%203%20and%203.5%20models%20exhibit%0Ain%20hundreds%20of%20thousands%20of%20real-world%20interactions.%20We%20empirically%20discover%0Aand%20taxonomize%203%2C307%20AI%20values%20and%20study%20how%20they%20vary%20by%20context.%20We%20find%20that%0AClaude%20expresses%20many%20practical%20and%20epistemic%20values%2C%20and%20typically%20supports%0Aprosocial%20human%20values%20while%20resisting%20values%20like%20%22moral%20nihilism%22.%20While%20some%0Avalues%20appear%20consistently%20across%20contexts%20%28e.g.%20%22transparency%22%29%2C%20many%20are%20more%0Aspecialized%20and%20context-dependent%2C%20reflecting%20the%20diversity%20of%20human%0Ainterlocutors%20and%20their%20varied%20contexts.%20For%20example%2C%20%22harm%20prevention%22%20emerges%0Awhen%20Claude%20resists%20users%2C%20%22historical%20accuracy%22%20when%20responding%20to%20queries%0Aabout%20controversial%20events%2C%20%22healthy%20boundaries%22%20when%20asked%20for%20relationship%0Aadvice%2C%20and%20%22human%20agency%22%20in%20technology%20ethics%20discussions.%20By%20providing%20the%0Afirst%20large-scale%20empirical%20mapping%20of%20AI%20values%20in%20deployment%2C%20our%20work%0Acreates%20a%20foundation%20for%20more%20grounded%20evaluation%20and%20design%20of%20values%20in%20AI%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15236v1&entry.124074799=Read"},
{"title": "Cascade IPG Observer for Underwater Robot State Estimation", "author": "Kaustubh Joshi and Tianchen Liu and Nikhil Chopra", "abstract": "  This paper presents a novel cascade nonlinear observer framework for inertial\nstate estimation. It tackles the problem of intermediate state estimation when\nexternal localization is unavailable or in the event of a sensor outage. The\nproposed observer comprises two nonlinear observers based on a recently\ndeveloped iteratively preconditioned gradient descent (IPG) algorithm. It takes\nthe inputs via an IMU preintegration model where the first observer is a\nquaternion-based IPG. The output for the first observer is the input for the\nsecond observer, estimating the velocity and, consequently, the position. The\nproposed observer is validated on a public underwater dataset and a real-world\nexperiment using our robot platform. The estimation is compared with an\nextended Kalman filter (EKF) and an invariant extended Kalman filter (InEKF).\nResults demonstrate that our method outperforms these methods regarding better\npositional accuracy and lower variance.\n", "link": "http://arxiv.org/abs/2504.15235v1", "date": "2025-04-21", "relevancy": 1.9717, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5308}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5101}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascade%20IPG%20Observer%20for%20Underwater%20Robot%20State%20Estimation&body=Title%3A%20Cascade%20IPG%20Observer%20for%20Underwater%20Robot%20State%20Estimation%0AAuthor%3A%20Kaustubh%20Joshi%20and%20Tianchen%20Liu%20and%20Nikhil%20Chopra%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20cascade%20nonlinear%20observer%20framework%20for%20inertial%0Astate%20estimation.%20It%20tackles%20the%20problem%20of%20intermediate%20state%20estimation%20when%0Aexternal%20localization%20is%20unavailable%20or%20in%20the%20event%20of%20a%20sensor%20outage.%20The%0Aproposed%20observer%20comprises%20two%20nonlinear%20observers%20based%20on%20a%20recently%0Adeveloped%20iteratively%20preconditioned%20gradient%20descent%20%28IPG%29%20algorithm.%20It%20takes%0Athe%20inputs%20via%20an%20IMU%20preintegration%20model%20where%20the%20first%20observer%20is%20a%0Aquaternion-based%20IPG.%20The%20output%20for%20the%20first%20observer%20is%20the%20input%20for%20the%0Asecond%20observer%2C%20estimating%20the%20velocity%20and%2C%20consequently%2C%20the%20position.%20The%0Aproposed%20observer%20is%20validated%20on%20a%20public%20underwater%20dataset%20and%20a%20real-world%0Aexperiment%20using%20our%20robot%20platform.%20The%20estimation%20is%20compared%20with%20an%0Aextended%20Kalman%20filter%20%28EKF%29%20and%20an%20invariant%20extended%20Kalman%20filter%20%28InEKF%29.%0AResults%20demonstrate%20that%20our%20method%20outperforms%20these%20methods%20regarding%20better%0Apositional%20accuracy%20and%20lower%20variance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15235v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascade%2520IPG%2520Observer%2520for%2520Underwater%2520Robot%2520State%2520Estimation%26entry.906535625%3DKaustubh%2520Joshi%2520and%2520Tianchen%2520Liu%2520and%2520Nikhil%2520Chopra%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520cascade%2520nonlinear%2520observer%2520framework%2520for%2520inertial%250Astate%2520estimation.%2520It%2520tackles%2520the%2520problem%2520of%2520intermediate%2520state%2520estimation%2520when%250Aexternal%2520localization%2520is%2520unavailable%2520or%2520in%2520the%2520event%2520of%2520a%2520sensor%2520outage.%2520The%250Aproposed%2520observer%2520comprises%2520two%2520nonlinear%2520observers%2520based%2520on%2520a%2520recently%250Adeveloped%2520iteratively%2520preconditioned%2520gradient%2520descent%2520%2528IPG%2529%2520algorithm.%2520It%2520takes%250Athe%2520inputs%2520via%2520an%2520IMU%2520preintegration%2520model%2520where%2520the%2520first%2520observer%2520is%2520a%250Aquaternion-based%2520IPG.%2520The%2520output%2520for%2520the%2520first%2520observer%2520is%2520the%2520input%2520for%2520the%250Asecond%2520observer%252C%2520estimating%2520the%2520velocity%2520and%252C%2520consequently%252C%2520the%2520position.%2520The%250Aproposed%2520observer%2520is%2520validated%2520on%2520a%2520public%2520underwater%2520dataset%2520and%2520a%2520real-world%250Aexperiment%2520using%2520our%2520robot%2520platform.%2520The%2520estimation%2520is%2520compared%2520with%2520an%250Aextended%2520Kalman%2520filter%2520%2528EKF%2529%2520and%2520an%2520invariant%2520extended%2520Kalman%2520filter%2520%2528InEKF%2529.%250AResults%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520these%2520methods%2520regarding%2520better%250Apositional%2520accuracy%2520and%2520lower%2520variance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15235v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascade%20IPG%20Observer%20for%20Underwater%20Robot%20State%20Estimation&entry.906535625=Kaustubh%20Joshi%20and%20Tianchen%20Liu%20and%20Nikhil%20Chopra&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20cascade%20nonlinear%20observer%20framework%20for%20inertial%0Astate%20estimation.%20It%20tackles%20the%20problem%20of%20intermediate%20state%20estimation%20when%0Aexternal%20localization%20is%20unavailable%20or%20in%20the%20event%20of%20a%20sensor%20outage.%20The%0Aproposed%20observer%20comprises%20two%20nonlinear%20observers%20based%20on%20a%20recently%0Adeveloped%20iteratively%20preconditioned%20gradient%20descent%20%28IPG%29%20algorithm.%20It%20takes%0Athe%20inputs%20via%20an%20IMU%20preintegration%20model%20where%20the%20first%20observer%20is%20a%0Aquaternion-based%20IPG.%20The%20output%20for%20the%20first%20observer%20is%20the%20input%20for%20the%0Asecond%20observer%2C%20estimating%20the%20velocity%20and%2C%20consequently%2C%20the%20position.%20The%0Aproposed%20observer%20is%20validated%20on%20a%20public%20underwater%20dataset%20and%20a%20real-world%0Aexperiment%20using%20our%20robot%20platform.%20The%20estimation%20is%20compared%20with%20an%0Aextended%20Kalman%20filter%20%28EKF%29%20and%20an%20invariant%20extended%20Kalman%20filter%20%28InEKF%29.%0AResults%20demonstrate%20that%20our%20method%20outperforms%20these%20methods%20regarding%20better%0Apositional%20accuracy%20and%20lower%20variance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15235v1&entry.124074799=Read"},
{"title": "M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global\n  Scoring and Calibrated Thresholding", "author": "Sarah Alnegheimish and Zelin He and Matthew Reimherr and Akash Chandrayan and Abhinav Pradhan and Luca D'Angelo", "abstract": "  With the widespread availability of sensor data across industrial and\noperational systems, we frequently encounter heterogeneous time series from\nmultiple systems. Anomaly detection is crucial for such systems to facilitate\npredictive maintenance. However, most existing anomaly detection methods are\ndesigned for either univariate or single-system multivariate data, making them\ninsufficient for these complex scenarios. To address this, we introduce\nM$^2$AD, a framework for unsupervised anomaly detection in multivariate time\nseries data from multiple systems. M$^2$AD employs deep models to capture\nexpected behavior under normal conditions, using the residuals as indicators of\npotential anomalies. These residuals are then aggregated into a global anomaly\nscore through a Gaussian Mixture Model and Gamma calibration. We theoretically\ndemonstrate that this framework can effectively address heterogeneity and\ndependencies across sensors and systems. Empirically, M$^2$AD outperforms\nexisting methods in extensive evaluations by 21% on average, and its\neffectiveness is demonstrated on a large-scale real-world case study on 130\nassets in Amazon Fulfillment Centers. Our code and results are available at\nhttps://github.com/sarahmish/M2AD.\n", "link": "http://arxiv.org/abs/2504.15225v1", "date": "2025-04-21", "relevancy": 1.9548, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.533}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20M%24%5E2%24AD%3A%20Multi-Sensor%20Multi-System%20Anomaly%20Detection%20through%20Global%0A%20%20Scoring%20and%20Calibrated%20Thresholding&body=Title%3A%20M%24%5E2%24AD%3A%20Multi-Sensor%20Multi-System%20Anomaly%20Detection%20through%20Global%0A%20%20Scoring%20and%20Calibrated%20Thresholding%0AAuthor%3A%20Sarah%20Alnegheimish%20and%20Zelin%20He%20and%20Matthew%20Reimherr%20and%20Akash%20Chandrayan%20and%20Abhinav%20Pradhan%20and%20Luca%20D%27Angelo%0AAbstract%3A%20%20%20With%20the%20widespread%20availability%20of%20sensor%20data%20across%20industrial%20and%0Aoperational%20systems%2C%20we%20frequently%20encounter%20heterogeneous%20time%20series%20from%0Amultiple%20systems.%20Anomaly%20detection%20is%20crucial%20for%20such%20systems%20to%20facilitate%0Apredictive%20maintenance.%20However%2C%20most%20existing%20anomaly%20detection%20methods%20are%0Adesigned%20for%20either%20univariate%20or%20single-system%20multivariate%20data%2C%20making%20them%0Ainsufficient%20for%20these%20complex%20scenarios.%20To%20address%20this%2C%20we%20introduce%0AM%24%5E2%24AD%2C%20a%20framework%20for%20unsupervised%20anomaly%20detection%20in%20multivariate%20time%0Aseries%20data%20from%20multiple%20systems.%20M%24%5E2%24AD%20employs%20deep%20models%20to%20capture%0Aexpected%20behavior%20under%20normal%20conditions%2C%20using%20the%20residuals%20as%20indicators%20of%0Apotential%20anomalies.%20These%20residuals%20are%20then%20aggregated%20into%20a%20global%20anomaly%0Ascore%20through%20a%20Gaussian%20Mixture%20Model%20and%20Gamma%20calibration.%20We%20theoretically%0Ademonstrate%20that%20this%20framework%20can%20effectively%20address%20heterogeneity%20and%0Adependencies%20across%20sensors%20and%20systems.%20Empirically%2C%20M%24%5E2%24AD%20outperforms%0Aexisting%20methods%20in%20extensive%20evaluations%20by%2021%25%20on%20average%2C%20and%20its%0Aeffectiveness%20is%20demonstrated%20on%20a%20large-scale%20real-world%20case%20study%20on%20130%0Aassets%20in%20Amazon%20Fulfillment%20Centers.%20Our%20code%20and%20results%20are%20available%20at%0Ahttps%3A//github.com/sarahmish/M2AD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DM%2524%255E2%2524AD%253A%2520Multi-Sensor%2520Multi-System%2520Anomaly%2520Detection%2520through%2520Global%250A%2520%2520Scoring%2520and%2520Calibrated%2520Thresholding%26entry.906535625%3DSarah%2520Alnegheimish%2520and%2520Zelin%2520He%2520and%2520Matthew%2520Reimherr%2520and%2520Akash%2520Chandrayan%2520and%2520Abhinav%2520Pradhan%2520and%2520Luca%2520D%2527Angelo%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520availability%2520of%2520sensor%2520data%2520across%2520industrial%2520and%250Aoperational%2520systems%252C%2520we%2520frequently%2520encounter%2520heterogeneous%2520time%2520series%2520from%250Amultiple%2520systems.%2520Anomaly%2520detection%2520is%2520crucial%2520for%2520such%2520systems%2520to%2520facilitate%250Apredictive%2520maintenance.%2520However%252C%2520most%2520existing%2520anomaly%2520detection%2520methods%2520are%250Adesigned%2520for%2520either%2520univariate%2520or%2520single-system%2520multivariate%2520data%252C%2520making%2520them%250Ainsufficient%2520for%2520these%2520complex%2520scenarios.%2520To%2520address%2520this%252C%2520we%2520introduce%250AM%2524%255E2%2524AD%252C%2520a%2520framework%2520for%2520unsupervised%2520anomaly%2520detection%2520in%2520multivariate%2520time%250Aseries%2520data%2520from%2520multiple%2520systems.%2520M%2524%255E2%2524AD%2520employs%2520deep%2520models%2520to%2520capture%250Aexpected%2520behavior%2520under%2520normal%2520conditions%252C%2520using%2520the%2520residuals%2520as%2520indicators%2520of%250Apotential%2520anomalies.%2520These%2520residuals%2520are%2520then%2520aggregated%2520into%2520a%2520global%2520anomaly%250Ascore%2520through%2520a%2520Gaussian%2520Mixture%2520Model%2520and%2520Gamma%2520calibration.%2520We%2520theoretically%250Ademonstrate%2520that%2520this%2520framework%2520can%2520effectively%2520address%2520heterogeneity%2520and%250Adependencies%2520across%2520sensors%2520and%2520systems.%2520Empirically%252C%2520M%2524%255E2%2524AD%2520outperforms%250Aexisting%2520methods%2520in%2520extensive%2520evaluations%2520by%252021%2525%2520on%2520average%252C%2520and%2520its%250Aeffectiveness%2520is%2520demonstrated%2520on%2520a%2520large-scale%2520real-world%2520case%2520study%2520on%2520130%250Aassets%2520in%2520Amazon%2520Fulfillment%2520Centers.%2520Our%2520code%2520and%2520results%2520are%2520available%2520at%250Ahttps%253A//github.com/sarahmish/M2AD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=M%24%5E2%24AD%3A%20Multi-Sensor%20Multi-System%20Anomaly%20Detection%20through%20Global%0A%20%20Scoring%20and%20Calibrated%20Thresholding&entry.906535625=Sarah%20Alnegheimish%20and%20Zelin%20He%20and%20Matthew%20Reimherr%20and%20Akash%20Chandrayan%20and%20Abhinav%20Pradhan%20and%20Luca%20D%27Angelo&entry.1292438233=%20%20With%20the%20widespread%20availability%20of%20sensor%20data%20across%20industrial%20and%0Aoperational%20systems%2C%20we%20frequently%20encounter%20heterogeneous%20time%20series%20from%0Amultiple%20systems.%20Anomaly%20detection%20is%20crucial%20for%20such%20systems%20to%20facilitate%0Apredictive%20maintenance.%20However%2C%20most%20existing%20anomaly%20detection%20methods%20are%0Adesigned%20for%20either%20univariate%20or%20single-system%20multivariate%20data%2C%20making%20them%0Ainsufficient%20for%20these%20complex%20scenarios.%20To%20address%20this%2C%20we%20introduce%0AM%24%5E2%24AD%2C%20a%20framework%20for%20unsupervised%20anomaly%20detection%20in%20multivariate%20time%0Aseries%20data%20from%20multiple%20systems.%20M%24%5E2%24AD%20employs%20deep%20models%20to%20capture%0Aexpected%20behavior%20under%20normal%20conditions%2C%20using%20the%20residuals%20as%20indicators%20of%0Apotential%20anomalies.%20These%20residuals%20are%20then%20aggregated%20into%20a%20global%20anomaly%0Ascore%20through%20a%20Gaussian%20Mixture%20Model%20and%20Gamma%20calibration.%20We%20theoretically%0Ademonstrate%20that%20this%20framework%20can%20effectively%20address%20heterogeneity%20and%0Adependencies%20across%20sensors%20and%20systems.%20Empirically%2C%20M%24%5E2%24AD%20outperforms%0Aexisting%20methods%20in%20extensive%20evaluations%20by%2021%25%20on%20average%2C%20and%20its%0Aeffectiveness%20is%20demonstrated%20on%20a%20large-scale%20real-world%20case%20study%20on%20130%0Aassets%20in%20Amazon%20Fulfillment%20Centers.%20Our%20code%20and%20results%20are%20available%20at%0Ahttps%3A//github.com/sarahmish/M2AD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15225v1&entry.124074799=Read"},
{"title": "Learning Compositional Transferability of Time Series for Source-Free\n  Domain Adaptation", "author": "Hankang Sun and Guiming Li and Su Yang and Baoqi Li", "abstract": "  Domain adaptation is challenging for time series classification due to the\nhighly dynamic nature. This study tackles the most difficult subtask when both\ntarget labels and source data are inaccessible, namely, source-free domain\nadaptation. To reuse the classification backbone pre-trained on source data,\ntime series reconstruction is a sound solution that aligns target and source\ntime series by minimizing the reconstruction errors of both. However, simply\nfine-tuning the source pre-trained reconstruction model on target data may lose\nthe learnt priori, and it struggles to accommodate domain varying temporal\npatterns in a single encoder-decoder. Therefore, this paper tries to\ndisentangle the composition of domain transferability by using a compositional\narchitecture for time series reconstruction. Here, the preceding component is a\nU-net frozen since pre-trained, the output of which during adaptation is the\ninitial reconstruction of a given target time series, acting as a coarse step\nto prompt the subsequent finer adaptation. The following pipeline for finer\nadaptation includes two parallel branches: The source replay branch using a\nresidual link to preserve the output of U-net, and the offset compensation\nbranch that applies an additional autoencoder (AE) to further warp U-net's\noutput. By deploying a learnable factor on either branch to scale their\ncomposition in the final output of reconstruction, the data transferability is\ndisentangled and the learnt reconstructive capability from source data is\nretained. During inference, aside from the batch-level optimization in the\ntraining, we search at test time stability-aware rescaling of source replay\nbranch to tolerate instance-wise variation. The experimental results show that\nsuch compositional architecture of time series reconstruction leads to SOTA\nperformance on 3 widely used benchmarks.\n", "link": "http://arxiv.org/abs/2504.14994v1", "date": "2025-04-21", "relevancy": 1.9546, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.494}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4889}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Compositional%20Transferability%20of%20Time%20Series%20for%20Source-Free%0A%20%20Domain%20Adaptation&body=Title%3A%20Learning%20Compositional%20Transferability%20of%20Time%20Series%20for%20Source-Free%0A%20%20Domain%20Adaptation%0AAuthor%3A%20Hankang%20Sun%20and%20Guiming%20Li%20and%20Su%20Yang%20and%20Baoqi%20Li%0AAbstract%3A%20%20%20Domain%20adaptation%20is%20challenging%20for%20time%20series%20classification%20due%20to%20the%0Ahighly%20dynamic%20nature.%20This%20study%20tackles%20the%20most%20difficult%20subtask%20when%20both%0Atarget%20labels%20and%20source%20data%20are%20inaccessible%2C%20namely%2C%20source-free%20domain%0Aadaptation.%20To%20reuse%20the%20classification%20backbone%20pre-trained%20on%20source%20data%2C%0Atime%20series%20reconstruction%20is%20a%20sound%20solution%20that%20aligns%20target%20and%20source%0Atime%20series%20by%20minimizing%20the%20reconstruction%20errors%20of%20both.%20However%2C%20simply%0Afine-tuning%20the%20source%20pre-trained%20reconstruction%20model%20on%20target%20data%20may%20lose%0Athe%20learnt%20priori%2C%20and%20it%20struggles%20to%20accommodate%20domain%20varying%20temporal%0Apatterns%20in%20a%20single%20encoder-decoder.%20Therefore%2C%20this%20paper%20tries%20to%0Adisentangle%20the%20composition%20of%20domain%20transferability%20by%20using%20a%20compositional%0Aarchitecture%20for%20time%20series%20reconstruction.%20Here%2C%20the%20preceding%20component%20is%20a%0AU-net%20frozen%20since%20pre-trained%2C%20the%20output%20of%20which%20during%20adaptation%20is%20the%0Ainitial%20reconstruction%20of%20a%20given%20target%20time%20series%2C%20acting%20as%20a%20coarse%20step%0Ato%20prompt%20the%20subsequent%20finer%20adaptation.%20The%20following%20pipeline%20for%20finer%0Aadaptation%20includes%20two%20parallel%20branches%3A%20The%20source%20replay%20branch%20using%20a%0Aresidual%20link%20to%20preserve%20the%20output%20of%20U-net%2C%20and%20the%20offset%20compensation%0Abranch%20that%20applies%20an%20additional%20autoencoder%20%28AE%29%20to%20further%20warp%20U-net%27s%0Aoutput.%20By%20deploying%20a%20learnable%20factor%20on%20either%20branch%20to%20scale%20their%0Acomposition%20in%20the%20final%20output%20of%20reconstruction%2C%20the%20data%20transferability%20is%0Adisentangled%20and%20the%20learnt%20reconstructive%20capability%20from%20source%20data%20is%0Aretained.%20During%20inference%2C%20aside%20from%20the%20batch-level%20optimization%20in%20the%0Atraining%2C%20we%20search%20at%20test%20time%20stability-aware%20rescaling%20of%20source%20replay%0Abranch%20to%20tolerate%20instance-wise%20variation.%20The%20experimental%20results%20show%20that%0Asuch%20compositional%20architecture%20of%20time%20series%20reconstruction%20leads%20to%20SOTA%0Aperformance%20on%203%20widely%20used%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Compositional%2520Transferability%2520of%2520Time%2520Series%2520for%2520Source-Free%250A%2520%2520Domain%2520Adaptation%26entry.906535625%3DHankang%2520Sun%2520and%2520Guiming%2520Li%2520and%2520Su%2520Yang%2520and%2520Baoqi%2520Li%26entry.1292438233%3D%2520%2520Domain%2520adaptation%2520is%2520challenging%2520for%2520time%2520series%2520classification%2520due%2520to%2520the%250Ahighly%2520dynamic%2520nature.%2520This%2520study%2520tackles%2520the%2520most%2520difficult%2520subtask%2520when%2520both%250Atarget%2520labels%2520and%2520source%2520data%2520are%2520inaccessible%252C%2520namely%252C%2520source-free%2520domain%250Aadaptation.%2520To%2520reuse%2520the%2520classification%2520backbone%2520pre-trained%2520on%2520source%2520data%252C%250Atime%2520series%2520reconstruction%2520is%2520a%2520sound%2520solution%2520that%2520aligns%2520target%2520and%2520source%250Atime%2520series%2520by%2520minimizing%2520the%2520reconstruction%2520errors%2520of%2520both.%2520However%252C%2520simply%250Afine-tuning%2520the%2520source%2520pre-trained%2520reconstruction%2520model%2520on%2520target%2520data%2520may%2520lose%250Athe%2520learnt%2520priori%252C%2520and%2520it%2520struggles%2520to%2520accommodate%2520domain%2520varying%2520temporal%250Apatterns%2520in%2520a%2520single%2520encoder-decoder.%2520Therefore%252C%2520this%2520paper%2520tries%2520to%250Adisentangle%2520the%2520composition%2520of%2520domain%2520transferability%2520by%2520using%2520a%2520compositional%250Aarchitecture%2520for%2520time%2520series%2520reconstruction.%2520Here%252C%2520the%2520preceding%2520component%2520is%2520a%250AU-net%2520frozen%2520since%2520pre-trained%252C%2520the%2520output%2520of%2520which%2520during%2520adaptation%2520is%2520the%250Ainitial%2520reconstruction%2520of%2520a%2520given%2520target%2520time%2520series%252C%2520acting%2520as%2520a%2520coarse%2520step%250Ato%2520prompt%2520the%2520subsequent%2520finer%2520adaptation.%2520The%2520following%2520pipeline%2520for%2520finer%250Aadaptation%2520includes%2520two%2520parallel%2520branches%253A%2520The%2520source%2520replay%2520branch%2520using%2520a%250Aresidual%2520link%2520to%2520preserve%2520the%2520output%2520of%2520U-net%252C%2520and%2520the%2520offset%2520compensation%250Abranch%2520that%2520applies%2520an%2520additional%2520autoencoder%2520%2528AE%2529%2520to%2520further%2520warp%2520U-net%2527s%250Aoutput.%2520By%2520deploying%2520a%2520learnable%2520factor%2520on%2520either%2520branch%2520to%2520scale%2520their%250Acomposition%2520in%2520the%2520final%2520output%2520of%2520reconstruction%252C%2520the%2520data%2520transferability%2520is%250Adisentangled%2520and%2520the%2520learnt%2520reconstructive%2520capability%2520from%2520source%2520data%2520is%250Aretained.%2520During%2520inference%252C%2520aside%2520from%2520the%2520batch-level%2520optimization%2520in%2520the%250Atraining%252C%2520we%2520search%2520at%2520test%2520time%2520stability-aware%2520rescaling%2520of%2520source%2520replay%250Abranch%2520to%2520tolerate%2520instance-wise%2520variation.%2520The%2520experimental%2520results%2520show%2520that%250Asuch%2520compositional%2520architecture%2520of%2520time%2520series%2520reconstruction%2520leads%2520to%2520SOTA%250Aperformance%2520on%25203%2520widely%2520used%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Compositional%20Transferability%20of%20Time%20Series%20for%20Source-Free%0A%20%20Domain%20Adaptation&entry.906535625=Hankang%20Sun%20and%20Guiming%20Li%20and%20Su%20Yang%20and%20Baoqi%20Li&entry.1292438233=%20%20Domain%20adaptation%20is%20challenging%20for%20time%20series%20classification%20due%20to%20the%0Ahighly%20dynamic%20nature.%20This%20study%20tackles%20the%20most%20difficult%20subtask%20when%20both%0Atarget%20labels%20and%20source%20data%20are%20inaccessible%2C%20namely%2C%20source-free%20domain%0Aadaptation.%20To%20reuse%20the%20classification%20backbone%20pre-trained%20on%20source%20data%2C%0Atime%20series%20reconstruction%20is%20a%20sound%20solution%20that%20aligns%20target%20and%20source%0Atime%20series%20by%20minimizing%20the%20reconstruction%20errors%20of%20both.%20However%2C%20simply%0Afine-tuning%20the%20source%20pre-trained%20reconstruction%20model%20on%20target%20data%20may%20lose%0Athe%20learnt%20priori%2C%20and%20it%20struggles%20to%20accommodate%20domain%20varying%20temporal%0Apatterns%20in%20a%20single%20encoder-decoder.%20Therefore%2C%20this%20paper%20tries%20to%0Adisentangle%20the%20composition%20of%20domain%20transferability%20by%20using%20a%20compositional%0Aarchitecture%20for%20time%20series%20reconstruction.%20Here%2C%20the%20preceding%20component%20is%20a%0AU-net%20frozen%20since%20pre-trained%2C%20the%20output%20of%20which%20during%20adaptation%20is%20the%0Ainitial%20reconstruction%20of%20a%20given%20target%20time%20series%2C%20acting%20as%20a%20coarse%20step%0Ato%20prompt%20the%20subsequent%20finer%20adaptation.%20The%20following%20pipeline%20for%20finer%0Aadaptation%20includes%20two%20parallel%20branches%3A%20The%20source%20replay%20branch%20using%20a%0Aresidual%20link%20to%20preserve%20the%20output%20of%20U-net%2C%20and%20the%20offset%20compensation%0Abranch%20that%20applies%20an%20additional%20autoencoder%20%28AE%29%20to%20further%20warp%20U-net%27s%0Aoutput.%20By%20deploying%20a%20learnable%20factor%20on%20either%20branch%20to%20scale%20their%0Acomposition%20in%20the%20final%20output%20of%20reconstruction%2C%20the%20data%20transferability%20is%0Adisentangled%20and%20the%20learnt%20reconstructive%20capability%20from%20source%20data%20is%0Aretained.%20During%20inference%2C%20aside%20from%20the%20batch-level%20optimization%20in%20the%0Atraining%2C%20we%20search%20at%20test%20time%20stability-aware%20rescaling%20of%20source%20replay%0Abranch%20to%20tolerate%20instance-wise%20variation.%20The%20experimental%20results%20show%20that%0Asuch%20compositional%20architecture%20of%20time%20series%20reconstruction%20leads%20to%20SOTA%0Aperformance%20on%203%20widely%20used%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14994v1&entry.124074799=Read"},
{"title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models", "author": "Ziwen Xu and Shuxun Wang and Kewei Xu and Haoming Xu and Mengru Wang and Xinle Deng and Yunzhi Yao and Guozhou Zheng and Huajun Chen and Ningyu Zhang", "abstract": "  In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.\n", "link": "http://arxiv.org/abs/2504.15133v1", "date": "2025-04-21", "relevancy": 1.9312, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EasyEdit2%3A%20An%20Easy-to-use%20Steering%20Framework%20for%20Editing%20Large%20Language%0A%20%20Models&body=Title%3A%20EasyEdit2%3A%20An%20Easy-to-use%20Steering%20Framework%20for%20Editing%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Ziwen%20Xu%20and%20Shuxun%20Wang%20and%20Kewei%20Xu%20and%20Haoming%20Xu%20and%20Mengru%20Wang%20and%20Xinle%20Deng%20and%20Yunzhi%20Yao%20and%20Guozhou%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20EasyEdit2%2C%20a%20framework%20designed%20to%20enable%0Aplug-and-play%20adjustability%20for%20controlling%20Large%20Language%20Model%20%28LLM%29%0Abehaviors.%20EasyEdit2%20supports%20a%20wide%20range%20of%20test-time%20interventions%2C%0Aincluding%20safety%2C%20sentiment%2C%20personality%2C%20reasoning%20patterns%2C%20factuality%2C%20and%0Alanguage%20features.%20Unlike%20its%20predecessor%2C%20EasyEdit2%20features%20a%20new%0Aarchitecture%20specifically%20designed%20for%20seamless%20model%20steering.%20It%20comprises%0Akey%20modules%20such%20as%20the%20steering%20vector%20generator%20and%20the%20steering%20vector%0Aapplier%2C%20which%20enable%20automatic%20generation%20and%20application%20of%20steering%20vectors%0Ato%20influence%20the%20model%27s%20behavior%20without%20modifying%20its%20parameters.%20One%20of%20the%0Amain%20advantages%20of%20EasyEdit2%20is%20its%20ease%20of%20use-users%20do%20not%20need%20extensive%0Atechnical%20knowledge.%20With%20just%20a%20single%20example%2C%20they%20can%20effectively%20guide%20and%0Aadjust%20the%20model%27s%20responses%2C%20making%20precise%20control%20both%20accessible%20and%0Aefficient.%20Empirically%2C%20we%20report%20model%20steering%20performance%20across%20different%0ALLMs%2C%20demonstrating%20the%20effectiveness%20of%20these%20techniques.%20We%20have%20released%20the%0Asource%20code%20on%20GitHub%20at%20https%3A//github.com/zjunlp/EasyEdit%20along%20with%20a%0Ademonstration%20notebook.%20In%20addition%2C%20we%20provide%20a%20demo%20video%20at%0Ahttps%3A//zjunlp.github.io/project/EasyEdit2/video%20for%20a%20quick%20introduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasyEdit2%253A%2520An%2520Easy-to-use%2520Steering%2520Framework%2520for%2520Editing%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DZiwen%2520Xu%2520and%2520Shuxun%2520Wang%2520and%2520Kewei%2520Xu%2520and%2520Haoming%2520Xu%2520and%2520Mengru%2520Wang%2520and%2520Xinle%2520Deng%2520and%2520Yunzhi%2520Yao%2520and%2520Guozhou%2520Zheng%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520EasyEdit2%252C%2520a%2520framework%2520designed%2520to%2520enable%250Aplug-and-play%2520adjustability%2520for%2520controlling%2520Large%2520Language%2520Model%2520%2528LLM%2529%250Abehaviors.%2520EasyEdit2%2520supports%2520a%2520wide%2520range%2520of%2520test-time%2520interventions%252C%250Aincluding%2520safety%252C%2520sentiment%252C%2520personality%252C%2520reasoning%2520patterns%252C%2520factuality%252C%2520and%250Alanguage%2520features.%2520Unlike%2520its%2520predecessor%252C%2520EasyEdit2%2520features%2520a%2520new%250Aarchitecture%2520specifically%2520designed%2520for%2520seamless%2520model%2520steering.%2520It%2520comprises%250Akey%2520modules%2520such%2520as%2520the%2520steering%2520vector%2520generator%2520and%2520the%2520steering%2520vector%250Aapplier%252C%2520which%2520enable%2520automatic%2520generation%2520and%2520application%2520of%2520steering%2520vectors%250Ato%2520influence%2520the%2520model%2527s%2520behavior%2520without%2520modifying%2520its%2520parameters.%2520One%2520of%2520the%250Amain%2520advantages%2520of%2520EasyEdit2%2520is%2520its%2520ease%2520of%2520use-users%2520do%2520not%2520need%2520extensive%250Atechnical%2520knowledge.%2520With%2520just%2520a%2520single%2520example%252C%2520they%2520can%2520effectively%2520guide%2520and%250Aadjust%2520the%2520model%2527s%2520responses%252C%2520making%2520precise%2520control%2520both%2520accessible%2520and%250Aefficient.%2520Empirically%252C%2520we%2520report%2520model%2520steering%2520performance%2520across%2520different%250ALLMs%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520these%2520techniques.%2520We%2520have%2520released%2520the%250Asource%2520code%2520on%2520GitHub%2520at%2520https%253A//github.com/zjunlp/EasyEdit%2520along%2520with%2520a%250Ademonstration%2520notebook.%2520In%2520addition%252C%2520we%2520provide%2520a%2520demo%2520video%2520at%250Ahttps%253A//zjunlp.github.io/project/EasyEdit2/video%2520for%2520a%2520quick%2520introduction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EasyEdit2%3A%20An%20Easy-to-use%20Steering%20Framework%20for%20Editing%20Large%20Language%0A%20%20Models&entry.906535625=Ziwen%20Xu%20and%20Shuxun%20Wang%20and%20Kewei%20Xu%20and%20Haoming%20Xu%20and%20Mengru%20Wang%20and%20Xinle%20Deng%20and%20Yunzhi%20Yao%20and%20Guozhou%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20EasyEdit2%2C%20a%20framework%20designed%20to%20enable%0Aplug-and-play%20adjustability%20for%20controlling%20Large%20Language%20Model%20%28LLM%29%0Abehaviors.%20EasyEdit2%20supports%20a%20wide%20range%20of%20test-time%20interventions%2C%0Aincluding%20safety%2C%20sentiment%2C%20personality%2C%20reasoning%20patterns%2C%20factuality%2C%20and%0Alanguage%20features.%20Unlike%20its%20predecessor%2C%20EasyEdit2%20features%20a%20new%0Aarchitecture%20specifically%20designed%20for%20seamless%20model%20steering.%20It%20comprises%0Akey%20modules%20such%20as%20the%20steering%20vector%20generator%20and%20the%20steering%20vector%0Aapplier%2C%20which%20enable%20automatic%20generation%20and%20application%20of%20steering%20vectors%0Ato%20influence%20the%20model%27s%20behavior%20without%20modifying%20its%20parameters.%20One%20of%20the%0Amain%20advantages%20of%20EasyEdit2%20is%20its%20ease%20of%20use-users%20do%20not%20need%20extensive%0Atechnical%20knowledge.%20With%20just%20a%20single%20example%2C%20they%20can%20effectively%20guide%20and%0Aadjust%20the%20model%27s%20responses%2C%20making%20precise%20control%20both%20accessible%20and%0Aefficient.%20Empirically%2C%20we%20report%20model%20steering%20performance%20across%20different%0ALLMs%2C%20demonstrating%20the%20effectiveness%20of%20these%20techniques.%20We%20have%20released%20the%0Asource%20code%20on%20GitHub%20at%20https%3A//github.com/zjunlp/EasyEdit%20along%20with%20a%0Ademonstration%20notebook.%20In%20addition%2C%20we%20provide%20a%20demo%20video%20at%0Ahttps%3A//zjunlp.github.io/project/EasyEdit2/video%20for%20a%20quick%20introduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15133v1&entry.124074799=Read"},
{"title": "Communication and Energy-Aware Multi-UAV Coverage Path Planning for\n  Networked Operations", "author": "Mohamed Samshad and Ketan Rajawat", "abstract": "  This paper presents a communication and energy-aware multi-UAV Coverage Path\nPlanning (mCPP) method for scenarios requiring continuous inter-UAV\ncommunication, such as cooperative search and rescue and surveillance missions.\nUnlike existing mCPP solutions that focus on energy, time, or coverage\nefficiency, the proposed method generates coverage paths that minimize a\nspecified combination of energy and inter-UAV connectivity radius. Key features\nof the proposed algorithm include a simplified and validated energy consumption\nmodel, an efficient connectivity radius estimator, and an optimization\nframework that enables us to search for the optimal paths over irregular and\nobstacle-rich regions. The effectiveness and utility of the proposed algorithm\nis validated through simulations on various test regions with and without\nno-fly-zones. Real-world experiments on a three-UAV system demonstrate the\nremarkably high 99% match between the estimated and actual communication range\nrequirement.\n", "link": "http://arxiv.org/abs/2411.02772v3", "date": "2025-04-21", "relevancy": 1.9276, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5252}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4766}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication%20and%20Energy-Aware%20Multi-UAV%20Coverage%20Path%20Planning%20for%0A%20%20Networked%20Operations&body=Title%3A%20Communication%20and%20Energy-Aware%20Multi-UAV%20Coverage%20Path%20Planning%20for%0A%20%20Networked%20Operations%0AAuthor%3A%20Mohamed%20Samshad%20and%20Ketan%20Rajawat%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20communication%20and%20energy-aware%20multi-UAV%20Coverage%20Path%0APlanning%20%28mCPP%29%20method%20for%20scenarios%20requiring%20continuous%20inter-UAV%0Acommunication%2C%20such%20as%20cooperative%20search%20and%20rescue%20and%20surveillance%20missions.%0AUnlike%20existing%20mCPP%20solutions%20that%20focus%20on%20energy%2C%20time%2C%20or%20coverage%0Aefficiency%2C%20the%20proposed%20method%20generates%20coverage%20paths%20that%20minimize%20a%0Aspecified%20combination%20of%20energy%20and%20inter-UAV%20connectivity%20radius.%20Key%20features%0Aof%20the%20proposed%20algorithm%20include%20a%20simplified%20and%20validated%20energy%20consumption%0Amodel%2C%20an%20efficient%20connectivity%20radius%20estimator%2C%20and%20an%20optimization%0Aframework%20that%20enables%20us%20to%20search%20for%20the%20optimal%20paths%20over%20irregular%20and%0Aobstacle-rich%20regions.%20The%20effectiveness%20and%20utility%20of%20the%20proposed%20algorithm%0Ais%20validated%20through%20simulations%20on%20various%20test%20regions%20with%20and%20without%0Ano-fly-zones.%20Real-world%20experiments%20on%20a%20three-UAV%20system%20demonstrate%20the%0Aremarkably%20high%2099%25%20match%20between%20the%20estimated%20and%20actual%20communication%20range%0Arequirement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02772v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication%2520and%2520Energy-Aware%2520Multi-UAV%2520Coverage%2520Path%2520Planning%2520for%250A%2520%2520Networked%2520Operations%26entry.906535625%3DMohamed%2520Samshad%2520and%2520Ketan%2520Rajawat%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520communication%2520and%2520energy-aware%2520multi-UAV%2520Coverage%2520Path%250APlanning%2520%2528mCPP%2529%2520method%2520for%2520scenarios%2520requiring%2520continuous%2520inter-UAV%250Acommunication%252C%2520such%2520as%2520cooperative%2520search%2520and%2520rescue%2520and%2520surveillance%2520missions.%250AUnlike%2520existing%2520mCPP%2520solutions%2520that%2520focus%2520on%2520energy%252C%2520time%252C%2520or%2520coverage%250Aefficiency%252C%2520the%2520proposed%2520method%2520generates%2520coverage%2520paths%2520that%2520minimize%2520a%250Aspecified%2520combination%2520of%2520energy%2520and%2520inter-UAV%2520connectivity%2520radius.%2520Key%2520features%250Aof%2520the%2520proposed%2520algorithm%2520include%2520a%2520simplified%2520and%2520validated%2520energy%2520consumption%250Amodel%252C%2520an%2520efficient%2520connectivity%2520radius%2520estimator%252C%2520and%2520an%2520optimization%250Aframework%2520that%2520enables%2520us%2520to%2520search%2520for%2520the%2520optimal%2520paths%2520over%2520irregular%2520and%250Aobstacle-rich%2520regions.%2520The%2520effectiveness%2520and%2520utility%2520of%2520the%2520proposed%2520algorithm%250Ais%2520validated%2520through%2520simulations%2520on%2520various%2520test%2520regions%2520with%2520and%2520without%250Ano-fly-zones.%2520Real-world%2520experiments%2520on%2520a%2520three-UAV%2520system%2520demonstrate%2520the%250Aremarkably%2520high%252099%2525%2520match%2520between%2520the%2520estimated%2520and%2520actual%2520communication%2520range%250Arequirement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02772v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication%20and%20Energy-Aware%20Multi-UAV%20Coverage%20Path%20Planning%20for%0A%20%20Networked%20Operations&entry.906535625=Mohamed%20Samshad%20and%20Ketan%20Rajawat&entry.1292438233=%20%20This%20paper%20presents%20a%20communication%20and%20energy-aware%20multi-UAV%20Coverage%20Path%0APlanning%20%28mCPP%29%20method%20for%20scenarios%20requiring%20continuous%20inter-UAV%0Acommunication%2C%20such%20as%20cooperative%20search%20and%20rescue%20and%20surveillance%20missions.%0AUnlike%20existing%20mCPP%20solutions%20that%20focus%20on%20energy%2C%20time%2C%20or%20coverage%0Aefficiency%2C%20the%20proposed%20method%20generates%20coverage%20paths%20that%20minimize%20a%0Aspecified%20combination%20of%20energy%20and%20inter-UAV%20connectivity%20radius.%20Key%20features%0Aof%20the%20proposed%20algorithm%20include%20a%20simplified%20and%20validated%20energy%20consumption%0Amodel%2C%20an%20efficient%20connectivity%20radius%20estimator%2C%20and%20an%20optimization%0Aframework%20that%20enables%20us%20to%20search%20for%20the%20optimal%20paths%20over%20irregular%20and%0Aobstacle-rich%20regions.%20The%20effectiveness%20and%20utility%20of%20the%20proposed%20algorithm%0Ais%20validated%20through%20simulations%20on%20various%20test%20regions%20with%20and%20without%0Ano-fly-zones.%20Real-world%20experiments%20on%20a%20three-UAV%20system%20demonstrate%20the%0Aremarkably%20high%2099%25%20match%20between%20the%20estimated%20and%20actual%20communication%20range%0Arequirement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02772v3&entry.124074799=Read"},
{"title": "Application of Sensitivity Analysis Methods for Studying Neural Network\n  Models", "author": "Jiaxuan Miao and Sergey Matveev", "abstract": "  This study demonstrates the capabilities of several methods for analyzing the\nsensitivity of neural networks to perturbations of the input data and\ninterpreting their underlying mechanisms. The investigated approaches include\nthe Sobol global sensitivity analysis, the local sensitivity method for input\npixel perturbations and the activation maximization technique. As examples, in\nthis study we consider a small feedforward neural network for analyzing an open\ntabular dataset of clinical diabetes data, as well as two classical\nconvolutional architectures, VGG-16 and ResNet-18, which are widely used in\nimage processing and classification. Utilization of the global sensitivity\nanalysis allows us to identify the leading input parameters of the chosen tiny\nneural network and reduce their number without significant loss of the\naccuracy. As far as global sensitivity analysis is not applicable to larger\nmodels we try the local sensitivity analysis and activation maximization method\nin application to the convolutional neural networks. These methods show\ninteresting patterns for the convolutional models solving the image\nclassification problem. All in all, we compare the results of the activation\nmaximization method with popular Grad-CAM technique in the context of\nultrasound data analysis.\n", "link": "http://arxiv.org/abs/2504.15100v1", "date": "2025-04-21", "relevancy": 1.9266, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4871}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4869}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20Sensitivity%20Analysis%20Methods%20for%20Studying%20Neural%20Network%0A%20%20Models&body=Title%3A%20Application%20of%20Sensitivity%20Analysis%20Methods%20for%20Studying%20Neural%20Network%0A%20%20Models%0AAuthor%3A%20Jiaxuan%20Miao%20and%20Sergey%20Matveev%0AAbstract%3A%20%20%20This%20study%20demonstrates%20the%20capabilities%20of%20several%20methods%20for%20analyzing%20the%0Asensitivity%20of%20neural%20networks%20to%20perturbations%20of%20the%20input%20data%20and%0Ainterpreting%20their%20underlying%20mechanisms.%20The%20investigated%20approaches%20include%0Athe%20Sobol%20global%20sensitivity%20analysis%2C%20the%20local%20sensitivity%20method%20for%20input%0Apixel%20perturbations%20and%20the%20activation%20maximization%20technique.%20As%20examples%2C%20in%0Athis%20study%20we%20consider%20a%20small%20feedforward%20neural%20network%20for%20analyzing%20an%20open%0Atabular%20dataset%20of%20clinical%20diabetes%20data%2C%20as%20well%20as%20two%20classical%0Aconvolutional%20architectures%2C%20VGG-16%20and%20ResNet-18%2C%20which%20are%20widely%20used%20in%0Aimage%20processing%20and%20classification.%20Utilization%20of%20the%20global%20sensitivity%0Aanalysis%20allows%20us%20to%20identify%20the%20leading%20input%20parameters%20of%20the%20chosen%20tiny%0Aneural%20network%20and%20reduce%20their%20number%20without%20significant%20loss%20of%20the%0Aaccuracy.%20As%20far%20as%20global%20sensitivity%20analysis%20is%20not%20applicable%20to%20larger%0Amodels%20we%20try%20the%20local%20sensitivity%20analysis%20and%20activation%20maximization%20method%0Ain%20application%20to%20the%20convolutional%20neural%20networks.%20These%20methods%20show%0Ainteresting%20patterns%20for%20the%20convolutional%20models%20solving%20the%20image%0Aclassification%20problem.%20All%20in%20all%2C%20we%20compare%20the%20results%20of%20the%20activation%0Amaximization%20method%20with%20popular%20Grad-CAM%20technique%20in%20the%20context%20of%0Aultrasound%20data%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520Sensitivity%2520Analysis%2520Methods%2520for%2520Studying%2520Neural%2520Network%250A%2520%2520Models%26entry.906535625%3DJiaxuan%2520Miao%2520and%2520Sergey%2520Matveev%26entry.1292438233%3D%2520%2520This%2520study%2520demonstrates%2520the%2520capabilities%2520of%2520several%2520methods%2520for%2520analyzing%2520the%250Asensitivity%2520of%2520neural%2520networks%2520to%2520perturbations%2520of%2520the%2520input%2520data%2520and%250Ainterpreting%2520their%2520underlying%2520mechanisms.%2520The%2520investigated%2520approaches%2520include%250Athe%2520Sobol%2520global%2520sensitivity%2520analysis%252C%2520the%2520local%2520sensitivity%2520method%2520for%2520input%250Apixel%2520perturbations%2520and%2520the%2520activation%2520maximization%2520technique.%2520As%2520examples%252C%2520in%250Athis%2520study%2520we%2520consider%2520a%2520small%2520feedforward%2520neural%2520network%2520for%2520analyzing%2520an%2520open%250Atabular%2520dataset%2520of%2520clinical%2520diabetes%2520data%252C%2520as%2520well%2520as%2520two%2520classical%250Aconvolutional%2520architectures%252C%2520VGG-16%2520and%2520ResNet-18%252C%2520which%2520are%2520widely%2520used%2520in%250Aimage%2520processing%2520and%2520classification.%2520Utilization%2520of%2520the%2520global%2520sensitivity%250Aanalysis%2520allows%2520us%2520to%2520identify%2520the%2520leading%2520input%2520parameters%2520of%2520the%2520chosen%2520tiny%250Aneural%2520network%2520and%2520reduce%2520their%2520number%2520without%2520significant%2520loss%2520of%2520the%250Aaccuracy.%2520As%2520far%2520as%2520global%2520sensitivity%2520analysis%2520is%2520not%2520applicable%2520to%2520larger%250Amodels%2520we%2520try%2520the%2520local%2520sensitivity%2520analysis%2520and%2520activation%2520maximization%2520method%250Ain%2520application%2520to%2520the%2520convolutional%2520neural%2520networks.%2520These%2520methods%2520show%250Ainteresting%2520patterns%2520for%2520the%2520convolutional%2520models%2520solving%2520the%2520image%250Aclassification%2520problem.%2520All%2520in%2520all%252C%2520we%2520compare%2520the%2520results%2520of%2520the%2520activation%250Amaximization%2520method%2520with%2520popular%2520Grad-CAM%2520technique%2520in%2520the%2520context%2520of%250Aultrasound%2520data%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20Sensitivity%20Analysis%20Methods%20for%20Studying%20Neural%20Network%0A%20%20Models&entry.906535625=Jiaxuan%20Miao%20and%20Sergey%20Matveev&entry.1292438233=%20%20This%20study%20demonstrates%20the%20capabilities%20of%20several%20methods%20for%20analyzing%20the%0Asensitivity%20of%20neural%20networks%20to%20perturbations%20of%20the%20input%20data%20and%0Ainterpreting%20their%20underlying%20mechanisms.%20The%20investigated%20approaches%20include%0Athe%20Sobol%20global%20sensitivity%20analysis%2C%20the%20local%20sensitivity%20method%20for%20input%0Apixel%20perturbations%20and%20the%20activation%20maximization%20technique.%20As%20examples%2C%20in%0Athis%20study%20we%20consider%20a%20small%20feedforward%20neural%20network%20for%20analyzing%20an%20open%0Atabular%20dataset%20of%20clinical%20diabetes%20data%2C%20as%20well%20as%20two%20classical%0Aconvolutional%20architectures%2C%20VGG-16%20and%20ResNet-18%2C%20which%20are%20widely%20used%20in%0Aimage%20processing%20and%20classification.%20Utilization%20of%20the%20global%20sensitivity%0Aanalysis%20allows%20us%20to%20identify%20the%20leading%20input%20parameters%20of%20the%20chosen%20tiny%0Aneural%20network%20and%20reduce%20their%20number%20without%20significant%20loss%20of%20the%0Aaccuracy.%20As%20far%20as%20global%20sensitivity%20analysis%20is%20not%20applicable%20to%20larger%0Amodels%20we%20try%20the%20local%20sensitivity%20analysis%20and%20activation%20maximization%20method%0Ain%20application%20to%20the%20convolutional%20neural%20networks.%20These%20methods%20show%0Ainteresting%20patterns%20for%20the%20convolutional%20models%20solving%20the%20image%0Aclassification%20problem.%20All%20in%20all%2C%20we%20compare%20the%20results%20of%20the%20activation%0Amaximization%20method%20with%20popular%20Grad-CAM%20technique%20in%20the%20context%20of%0Aultrasound%20data%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15100v1&entry.124074799=Read"},
{"title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and\n  Enhancement: KwaiSR Dataset and Study", "author": "Xin Li and Xijun Wang and Bingchen Li and Kun Yuan and Yizhen Shao and Suhang Yao and Ming Sun and Chao Zhou and Radu Timofte and Zhibo Chen", "abstract": "  In this work, we build the first benchmark dataset for short-form UGC Image\nSuper-resolution in the wild, termed KwaiSR, intending to advance the research\non developing image super-resolution algorithms for short-form UGC platforms.\nThis dataset is collected from the Kwai Platform, which is composed of two\nparts, i.e., synthetic and wild parts. Among them, the synthetic dataset,\nincluding 1,900 image pairs, is produced by simulating the degradation\nfollowing the distribution of real-world low-quality short-form UGC images,\naiming to provide the ground truth for training and objective comparison in the\nvalidation/testing. The wild dataset contains low-quality images collected\ndirectly from the Kwai Platform, which are filtered using the quality\nassessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset\ncontains 1800 synthetic image pairs and 1900 wild images, which are divided\ninto training, validation, and testing parts with a ratio of 8:1:1. Based on\nthe KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form\nUGC Video quality assessment and enhancement, which attracts lots of\nresearchers to develop the algorithm for it. The results of this competition\nhave revealed that our KwaiSR dataset is pretty challenging for existing Image\nSR methods, which is expected to lead to a new direction in the image\nsuper-resolution field. The dataset can be found from\nhttps://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.\n", "link": "http://arxiv.org/abs/2504.15003v1", "date": "2025-04-21", "relevancy": 1.9218, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4945}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4705}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NTIRE%202025%20Challenge%20on%20Short-form%20UGC%20Video%20Quality%20Assessment%20and%0A%20%20Enhancement%3A%20KwaiSR%20Dataset%20and%20Study&body=Title%3A%20NTIRE%202025%20Challenge%20on%20Short-form%20UGC%20Video%20Quality%20Assessment%20and%0A%20%20Enhancement%3A%20KwaiSR%20Dataset%20and%20Study%0AAuthor%3A%20Xin%20Li%20and%20Xijun%20Wang%20and%20Bingchen%20Li%20and%20Kun%20Yuan%20and%20Yizhen%20Shao%20and%20Suhang%20Yao%20and%20Ming%20Sun%20and%20Chao%20Zhou%20and%20Radu%20Timofte%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20build%20the%20first%20benchmark%20dataset%20for%20short-form%20UGC%20Image%0ASuper-resolution%20in%20the%20wild%2C%20termed%20KwaiSR%2C%20intending%20to%20advance%20the%20research%0Aon%20developing%20image%20super-resolution%20algorithms%20for%20short-form%20UGC%20platforms.%0AThis%20dataset%20is%20collected%20from%20the%20Kwai%20Platform%2C%20which%20is%20composed%20of%20two%0Aparts%2C%20i.e.%2C%20synthetic%20and%20wild%20parts.%20Among%20them%2C%20the%20synthetic%20dataset%2C%0Aincluding%201%2C900%20image%20pairs%2C%20is%20produced%20by%20simulating%20the%20degradation%0Afollowing%20the%20distribution%20of%20real-world%20low-quality%20short-form%20UGC%20images%2C%0Aaiming%20to%20provide%20the%20ground%20truth%20for%20training%20and%20objective%20comparison%20in%20the%0Avalidation/testing.%20The%20wild%20dataset%20contains%20low-quality%20images%20collected%0Adirectly%20from%20the%20Kwai%20Platform%2C%20which%20are%20filtered%20using%20the%20quality%0Aassessment%20method%20KVQ%20from%20the%20Kwai%20Platform.%20As%20a%20result%2C%20the%20KwaiSR%20dataset%0Acontains%201800%20synthetic%20image%20pairs%20and%201900%20wild%20images%2C%20which%20are%20divided%0Ainto%20training%2C%20validation%2C%20and%20testing%20parts%20with%20a%20ratio%20of%208%3A1%3A1.%20Based%20on%0Athe%20KwaiSR%20dataset%2C%20we%20organize%20the%20NTIRE%202025%20challenge%20on%20a%20second%20short-form%0AUGC%20Video%20quality%20assessment%20and%20enhancement%2C%20which%20attracts%20lots%20of%0Aresearchers%20to%20develop%20the%20algorithm%20for%20it.%20The%20results%20of%20this%20competition%0Ahave%20revealed%20that%20our%20KwaiSR%20dataset%20is%20pretty%20challenging%20for%20existing%20Image%0ASR%20methods%2C%20which%20is%20expected%20to%20lead%20to%20a%20new%20direction%20in%20the%20image%0Asuper-resolution%20field.%20The%20dataset%20can%20be%20found%20from%0Ahttps%3A//lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNTIRE%25202025%2520Challenge%2520on%2520Short-form%2520UGC%2520Video%2520Quality%2520Assessment%2520and%250A%2520%2520Enhancement%253A%2520KwaiSR%2520Dataset%2520and%2520Study%26entry.906535625%3DXin%2520Li%2520and%2520Xijun%2520Wang%2520and%2520Bingchen%2520Li%2520and%2520Kun%2520Yuan%2520and%2520Yizhen%2520Shao%2520and%2520Suhang%2520Yao%2520and%2520Ming%2520Sun%2520and%2520Chao%2520Zhou%2520and%2520Radu%2520Timofte%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520build%2520the%2520first%2520benchmark%2520dataset%2520for%2520short-form%2520UGC%2520Image%250ASuper-resolution%2520in%2520the%2520wild%252C%2520termed%2520KwaiSR%252C%2520intending%2520to%2520advance%2520the%2520research%250Aon%2520developing%2520image%2520super-resolution%2520algorithms%2520for%2520short-form%2520UGC%2520platforms.%250AThis%2520dataset%2520is%2520collected%2520from%2520the%2520Kwai%2520Platform%252C%2520which%2520is%2520composed%2520of%2520two%250Aparts%252C%2520i.e.%252C%2520synthetic%2520and%2520wild%2520parts.%2520Among%2520them%252C%2520the%2520synthetic%2520dataset%252C%250Aincluding%25201%252C900%2520image%2520pairs%252C%2520is%2520produced%2520by%2520simulating%2520the%2520degradation%250Afollowing%2520the%2520distribution%2520of%2520real-world%2520low-quality%2520short-form%2520UGC%2520images%252C%250Aaiming%2520to%2520provide%2520the%2520ground%2520truth%2520for%2520training%2520and%2520objective%2520comparison%2520in%2520the%250Avalidation/testing.%2520The%2520wild%2520dataset%2520contains%2520low-quality%2520images%2520collected%250Adirectly%2520from%2520the%2520Kwai%2520Platform%252C%2520which%2520are%2520filtered%2520using%2520the%2520quality%250Aassessment%2520method%2520KVQ%2520from%2520the%2520Kwai%2520Platform.%2520As%2520a%2520result%252C%2520the%2520KwaiSR%2520dataset%250Acontains%25201800%2520synthetic%2520image%2520pairs%2520and%25201900%2520wild%2520images%252C%2520which%2520are%2520divided%250Ainto%2520training%252C%2520validation%252C%2520and%2520testing%2520parts%2520with%2520a%2520ratio%2520of%25208%253A1%253A1.%2520Based%2520on%250Athe%2520KwaiSR%2520dataset%252C%2520we%2520organize%2520the%2520NTIRE%25202025%2520challenge%2520on%2520a%2520second%2520short-form%250AUGC%2520Video%2520quality%2520assessment%2520and%2520enhancement%252C%2520which%2520attracts%2520lots%2520of%250Aresearchers%2520to%2520develop%2520the%2520algorithm%2520for%2520it.%2520The%2520results%2520of%2520this%2520competition%250Ahave%2520revealed%2520that%2520our%2520KwaiSR%2520dataset%2520is%2520pretty%2520challenging%2520for%2520existing%2520Image%250ASR%2520methods%252C%2520which%2520is%2520expected%2520to%2520lead%2520to%2520a%2520new%2520direction%2520in%2520the%2520image%250Asuper-resolution%2520field.%2520The%2520dataset%2520can%2520be%2520found%2520from%250Ahttps%253A//lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTIRE%202025%20Challenge%20on%20Short-form%20UGC%20Video%20Quality%20Assessment%20and%0A%20%20Enhancement%3A%20KwaiSR%20Dataset%20and%20Study&entry.906535625=Xin%20Li%20and%20Xijun%20Wang%20and%20Bingchen%20Li%20and%20Kun%20Yuan%20and%20Yizhen%20Shao%20and%20Suhang%20Yao%20and%20Ming%20Sun%20and%20Chao%20Zhou%20and%20Radu%20Timofte%20and%20Zhibo%20Chen&entry.1292438233=%20%20In%20this%20work%2C%20we%20build%20the%20first%20benchmark%20dataset%20for%20short-form%20UGC%20Image%0ASuper-resolution%20in%20the%20wild%2C%20termed%20KwaiSR%2C%20intending%20to%20advance%20the%20research%0Aon%20developing%20image%20super-resolution%20algorithms%20for%20short-form%20UGC%20platforms.%0AThis%20dataset%20is%20collected%20from%20the%20Kwai%20Platform%2C%20which%20is%20composed%20of%20two%0Aparts%2C%20i.e.%2C%20synthetic%20and%20wild%20parts.%20Among%20them%2C%20the%20synthetic%20dataset%2C%0Aincluding%201%2C900%20image%20pairs%2C%20is%20produced%20by%20simulating%20the%20degradation%0Afollowing%20the%20distribution%20of%20real-world%20low-quality%20short-form%20UGC%20images%2C%0Aaiming%20to%20provide%20the%20ground%20truth%20for%20training%20and%20objective%20comparison%20in%20the%0Avalidation/testing.%20The%20wild%20dataset%20contains%20low-quality%20images%20collected%0Adirectly%20from%20the%20Kwai%20Platform%2C%20which%20are%20filtered%20using%20the%20quality%0Aassessment%20method%20KVQ%20from%20the%20Kwai%20Platform.%20As%20a%20result%2C%20the%20KwaiSR%20dataset%0Acontains%201800%20synthetic%20image%20pairs%20and%201900%20wild%20images%2C%20which%20are%20divided%0Ainto%20training%2C%20validation%2C%20and%20testing%20parts%20with%20a%20ratio%20of%208%3A1%3A1.%20Based%20on%0Athe%20KwaiSR%20dataset%2C%20we%20organize%20the%20NTIRE%202025%20challenge%20on%20a%20second%20short-form%0AUGC%20Video%20quality%20assessment%20and%20enhancement%2C%20which%20attracts%20lots%20of%0Aresearchers%20to%20develop%20the%20algorithm%20for%20it.%20The%20results%20of%20this%20competition%0Ahave%20revealed%20that%20our%20KwaiSR%20dataset%20is%20pretty%20challenging%20for%20existing%20Image%0ASR%20methods%2C%20which%20is%20expected%20to%20lead%20to%20a%20new%20direction%20in%20the%20image%0Asuper-resolution%20field.%20The%20dataset%20can%20be%20found%20from%0Ahttps%3A//lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15003v1&entry.124074799=Read"},
{"title": "Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical\n  Images", "author": "David C Wong and Bin Wang and Gorkem Durak and Marouane Tliba and Mohamed Amine Kerkouri and Aladine Chetouani and Ahmet Enis Cetin and Cagdas Topel and Nicolo Gennaro and Camila Vendrami and Tugce Agirlar Trabzonlu and Amir Ali Rahsepar and Laetitia Perronne and Matthew Antalek and Onural Ozturk and Gokcan Okur and Andrew C. Gordon and Ayis Pyrros and Frank H Miller and Amir A Borhani and Hatice Savas and Eric M. Hart", "abstract": "  Eye-tracking analysis plays a vital role in medical imaging, providing key\ninsights into how radiologists visually interpret and diagnose clinical cases.\nIn this work, we first analyze radiologists' attention and agreement by\nmeasuring the distribution of various eye-movement patterns, including saccades\ndirection, amplitude, and their joint distribution. These metrics help uncover\npatterns in attention allocation and diagnostic strategies. Furthermore, we\ninvestigate whether and how doctors' gaze behavior shifts when viewing\nauthentic (Real) versus deep-learning-generated (Fake) images. To achieve this,\nwe examine fixation bias maps, focusing on first, last, short, and longest\nfixations independently, along with detailed saccades patterns, to quantify\ndifferences in gaze distribution and visual saliency between authentic and\nsynthetic images.\n", "link": "http://arxiv.org/abs/2504.15007v1", "date": "2025-04-21", "relevancy": 1.9184, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.489}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4782}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shifts%20in%20Doctors%27%20Eye%20Movements%20Between%20Real%20and%20AI-Generated%20Medical%0A%20%20Images&body=Title%3A%20Shifts%20in%20Doctors%27%20Eye%20Movements%20Between%20Real%20and%20AI-Generated%20Medical%0A%20%20Images%0AAuthor%3A%20David%20C%20Wong%20and%20Bin%20Wang%20and%20Gorkem%20Durak%20and%20Marouane%20Tliba%20and%20Mohamed%20Amine%20Kerkouri%20and%20Aladine%20Chetouani%20and%20Ahmet%20Enis%20Cetin%20and%20Cagdas%20Topel%20and%20Nicolo%20Gennaro%20and%20Camila%20Vendrami%20and%20Tugce%20Agirlar%20Trabzonlu%20and%20Amir%20Ali%20Rahsepar%20and%20Laetitia%20Perronne%20and%20Matthew%20Antalek%20and%20Onural%20Ozturk%20and%20Gokcan%20Okur%20and%20Andrew%20C.%20Gordon%20and%20Ayis%20Pyrros%20and%20Frank%20H%20Miller%20and%20Amir%20A%20Borhani%20and%20Hatice%20Savas%20and%20Eric%20M.%20Hart%0AAbstract%3A%20%20%20Eye-tracking%20analysis%20plays%20a%20vital%20role%20in%20medical%20imaging%2C%20providing%20key%0Ainsights%20into%20how%20radiologists%20visually%20interpret%20and%20diagnose%20clinical%20cases.%0AIn%20this%20work%2C%20we%20first%20analyze%20radiologists%27%20attention%20and%20agreement%20by%0Ameasuring%20the%20distribution%20of%20various%20eye-movement%20patterns%2C%20including%20saccades%0Adirection%2C%20amplitude%2C%20and%20their%20joint%20distribution.%20These%20metrics%20help%20uncover%0Apatterns%20in%20attention%20allocation%20and%20diagnostic%20strategies.%20Furthermore%2C%20we%0Ainvestigate%20whether%20and%20how%20doctors%27%20gaze%20behavior%20shifts%20when%20viewing%0Aauthentic%20%28Real%29%20versus%20deep-learning-generated%20%28Fake%29%20images.%20To%20achieve%20this%2C%0Awe%20examine%20fixation%20bias%20maps%2C%20focusing%20on%20first%2C%20last%2C%20short%2C%20and%20longest%0Afixations%20independently%2C%20along%20with%20detailed%20saccades%20patterns%2C%20to%20quantify%0Adifferences%20in%20gaze%20distribution%20and%20visual%20saliency%20between%20authentic%20and%0Asynthetic%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShifts%2520in%2520Doctors%2527%2520Eye%2520Movements%2520Between%2520Real%2520and%2520AI-Generated%2520Medical%250A%2520%2520Images%26entry.906535625%3DDavid%2520C%2520Wong%2520and%2520Bin%2520Wang%2520and%2520Gorkem%2520Durak%2520and%2520Marouane%2520Tliba%2520and%2520Mohamed%2520Amine%2520Kerkouri%2520and%2520Aladine%2520Chetouani%2520and%2520Ahmet%2520Enis%2520Cetin%2520and%2520Cagdas%2520Topel%2520and%2520Nicolo%2520Gennaro%2520and%2520Camila%2520Vendrami%2520and%2520Tugce%2520Agirlar%2520Trabzonlu%2520and%2520Amir%2520Ali%2520Rahsepar%2520and%2520Laetitia%2520Perronne%2520and%2520Matthew%2520Antalek%2520and%2520Onural%2520Ozturk%2520and%2520Gokcan%2520Okur%2520and%2520Andrew%2520C.%2520Gordon%2520and%2520Ayis%2520Pyrros%2520and%2520Frank%2520H%2520Miller%2520and%2520Amir%2520A%2520Borhani%2520and%2520Hatice%2520Savas%2520and%2520Eric%2520M.%2520Hart%26entry.1292438233%3D%2520%2520Eye-tracking%2520analysis%2520plays%2520a%2520vital%2520role%2520in%2520medical%2520imaging%252C%2520providing%2520key%250Ainsights%2520into%2520how%2520radiologists%2520visually%2520interpret%2520and%2520diagnose%2520clinical%2520cases.%250AIn%2520this%2520work%252C%2520we%2520first%2520analyze%2520radiologists%2527%2520attention%2520and%2520agreement%2520by%250Ameasuring%2520the%2520distribution%2520of%2520various%2520eye-movement%2520patterns%252C%2520including%2520saccades%250Adirection%252C%2520amplitude%252C%2520and%2520their%2520joint%2520distribution.%2520These%2520metrics%2520help%2520uncover%250Apatterns%2520in%2520attention%2520allocation%2520and%2520diagnostic%2520strategies.%2520Furthermore%252C%2520we%250Ainvestigate%2520whether%2520and%2520how%2520doctors%2527%2520gaze%2520behavior%2520shifts%2520when%2520viewing%250Aauthentic%2520%2528Real%2529%2520versus%2520deep-learning-generated%2520%2528Fake%2529%2520images.%2520To%2520achieve%2520this%252C%250Awe%2520examine%2520fixation%2520bias%2520maps%252C%2520focusing%2520on%2520first%252C%2520last%252C%2520short%252C%2520and%2520longest%250Afixations%2520independently%252C%2520along%2520with%2520detailed%2520saccades%2520patterns%252C%2520to%2520quantify%250Adifferences%2520in%2520gaze%2520distribution%2520and%2520visual%2520saliency%2520between%2520authentic%2520and%250Asynthetic%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shifts%20in%20Doctors%27%20Eye%20Movements%20Between%20Real%20and%20AI-Generated%20Medical%0A%20%20Images&entry.906535625=David%20C%20Wong%20and%20Bin%20Wang%20and%20Gorkem%20Durak%20and%20Marouane%20Tliba%20and%20Mohamed%20Amine%20Kerkouri%20and%20Aladine%20Chetouani%20and%20Ahmet%20Enis%20Cetin%20and%20Cagdas%20Topel%20and%20Nicolo%20Gennaro%20and%20Camila%20Vendrami%20and%20Tugce%20Agirlar%20Trabzonlu%20and%20Amir%20Ali%20Rahsepar%20and%20Laetitia%20Perronne%20and%20Matthew%20Antalek%20and%20Onural%20Ozturk%20and%20Gokcan%20Okur%20and%20Andrew%20C.%20Gordon%20and%20Ayis%20Pyrros%20and%20Frank%20H%20Miller%20and%20Amir%20A%20Borhani%20and%20Hatice%20Savas%20and%20Eric%20M.%20Hart&entry.1292438233=%20%20Eye-tracking%20analysis%20plays%20a%20vital%20role%20in%20medical%20imaging%2C%20providing%20key%0Ainsights%20into%20how%20radiologists%20visually%20interpret%20and%20diagnose%20clinical%20cases.%0AIn%20this%20work%2C%20we%20first%20analyze%20radiologists%27%20attention%20and%20agreement%20by%0Ameasuring%20the%20distribution%20of%20various%20eye-movement%20patterns%2C%20including%20saccades%0Adirection%2C%20amplitude%2C%20and%20their%20joint%20distribution.%20These%20metrics%20help%20uncover%0Apatterns%20in%20attention%20allocation%20and%20diagnostic%20strategies.%20Furthermore%2C%20we%0Ainvestigate%20whether%20and%20how%20doctors%27%20gaze%20behavior%20shifts%20when%20viewing%0Aauthentic%20%28Real%29%20versus%20deep-learning-generated%20%28Fake%29%20images.%20To%20achieve%20this%2C%0Awe%20examine%20fixation%20bias%20maps%2C%20focusing%20on%20first%2C%20last%2C%20short%2C%20and%20longest%0Afixations%20independently%2C%20along%20with%20detailed%20saccades%20patterns%2C%20to%20quantify%0Adifferences%20in%20gaze%20distribution%20and%20visual%20saliency%20between%20authentic%20and%0Asynthetic%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15007v1&entry.124074799=Read"},
{"title": "GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities\n  for Language-Goal Aerial Navigation", "author": "Haotian Xu and Yue Hu and Chen Gao and Zhengqiu Zhu and Yong Zhao and Yong Li and Quanjun Yin", "abstract": "  Language-goal aerial navigation is a critical challenge in embodied AI,\nrequiring UAVs to localize targets in complex environments such as urban blocks\nbased on textual specification. Existing methods, often adapted from indoor\nnavigation, struggle to scale due to limited field of view, semantic ambiguity\namong objects, and lack of structured spatial reasoning. In this work, we\npropose GeoNav, a geospatially aware multimodal agent to enable long-range\nnavigation. GeoNav operates in three phases-landmark navigation, target search,\nand precise localization-mimicking human coarse-to-fine spatial strategies. To\nsupport such reasoning, it dynamically builds two different types of spatial\nmemory. The first is a global but schematic cognitive map, which fuses prior\ntextual geographic knowledge and embodied visual cues into a top-down,\nannotated form for fast navigation to the landmark region. The second is a\nlocal but delicate scene graph representing hierarchical spatial relationships\nbetween blocks, landmarks, and objects, which is used for definite target\nlocalization. On top of this structured representation, GeoNav employs a\nspatially aware, multimodal chain-of-thought prompting mechanism to enable\nmultimodal large language models with efficient and interpretable\ndecision-making across stages. On the CityNav urban navigation benchmark,\nGeoNav surpasses the current state-of-the-art by up to 12.53% in success rate\nand significantly improves navigation efficiency, even in hard-level tasks.\nAblation studies highlight the importance of each module, showcasing how\ngeospatial representations and coarse-to-fine reasoning enhance UAV navigation.\n", "link": "http://arxiv.org/abs/2504.09587v2", "date": "2025-04-21", "relevancy": 1.7392, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6009}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5591}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoNav%3A%20Empowering%20MLLMs%20with%20Explicit%20Geospatial%20Reasoning%20Abilities%0A%20%20for%20Language-Goal%20Aerial%20Navigation&body=Title%3A%20GeoNav%3A%20Empowering%20MLLMs%20with%20Explicit%20Geospatial%20Reasoning%20Abilities%0A%20%20for%20Language-Goal%20Aerial%20Navigation%0AAuthor%3A%20Haotian%20Xu%20and%20Yue%20Hu%20and%20Chen%20Gao%20and%20Zhengqiu%20Zhu%20and%20Yong%20Zhao%20and%20Yong%20Li%20and%20Quanjun%20Yin%0AAbstract%3A%20%20%20Language-goal%20aerial%20navigation%20is%20a%20critical%20challenge%20in%20embodied%20AI%2C%0Arequiring%20UAVs%20to%20localize%20targets%20in%20complex%20environments%20such%20as%20urban%20blocks%0Abased%20on%20textual%20specification.%20Existing%20methods%2C%20often%20adapted%20from%20indoor%0Anavigation%2C%20struggle%20to%20scale%20due%20to%20limited%20field%20of%20view%2C%20semantic%20ambiguity%0Aamong%20objects%2C%20and%20lack%20of%20structured%20spatial%20reasoning.%20In%20this%20work%2C%20we%0Apropose%20GeoNav%2C%20a%20geospatially%20aware%20multimodal%20agent%20to%20enable%20long-range%0Anavigation.%20GeoNav%20operates%20in%20three%20phases-landmark%20navigation%2C%20target%20search%2C%0Aand%20precise%20localization-mimicking%20human%20coarse-to-fine%20spatial%20strategies.%20To%0Asupport%20such%20reasoning%2C%20it%20dynamically%20builds%20two%20different%20types%20of%20spatial%0Amemory.%20The%20first%20is%20a%20global%20but%20schematic%20cognitive%20map%2C%20which%20fuses%20prior%0Atextual%20geographic%20knowledge%20and%20embodied%20visual%20cues%20into%20a%20top-down%2C%0Aannotated%20form%20for%20fast%20navigation%20to%20the%20landmark%20region.%20The%20second%20is%20a%0Alocal%20but%20delicate%20scene%20graph%20representing%20hierarchical%20spatial%20relationships%0Abetween%20blocks%2C%20landmarks%2C%20and%20objects%2C%20which%20is%20used%20for%20definite%20target%0Alocalization.%20On%20top%20of%20this%20structured%20representation%2C%20GeoNav%20employs%20a%0Aspatially%20aware%2C%20multimodal%20chain-of-thought%20prompting%20mechanism%20to%20enable%0Amultimodal%20large%20language%20models%20with%20efficient%20and%20interpretable%0Adecision-making%20across%20stages.%20On%20the%20CityNav%20urban%20navigation%20benchmark%2C%0AGeoNav%20surpasses%20the%20current%20state-of-the-art%20by%20up%20to%2012.53%25%20in%20success%20rate%0Aand%20significantly%20improves%20navigation%20efficiency%2C%20even%20in%20hard-level%20tasks.%0AAblation%20studies%20highlight%20the%20importance%20of%20each%20module%2C%20showcasing%20how%0Ageospatial%20representations%20and%20coarse-to-fine%20reasoning%20enhance%20UAV%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoNav%253A%2520Empowering%2520MLLMs%2520with%2520Explicit%2520Geospatial%2520Reasoning%2520Abilities%250A%2520%2520for%2520Language-Goal%2520Aerial%2520Navigation%26entry.906535625%3DHaotian%2520Xu%2520and%2520Yue%2520Hu%2520and%2520Chen%2520Gao%2520and%2520Zhengqiu%2520Zhu%2520and%2520Yong%2520Zhao%2520and%2520Yong%2520Li%2520and%2520Quanjun%2520Yin%26entry.1292438233%3D%2520%2520Language-goal%2520aerial%2520navigation%2520is%2520a%2520critical%2520challenge%2520in%2520embodied%2520AI%252C%250Arequiring%2520UAVs%2520to%2520localize%2520targets%2520in%2520complex%2520environments%2520such%2520as%2520urban%2520blocks%250Abased%2520on%2520textual%2520specification.%2520Existing%2520methods%252C%2520often%2520adapted%2520from%2520indoor%250Anavigation%252C%2520struggle%2520to%2520scale%2520due%2520to%2520limited%2520field%2520of%2520view%252C%2520semantic%2520ambiguity%250Aamong%2520objects%252C%2520and%2520lack%2520of%2520structured%2520spatial%2520reasoning.%2520In%2520this%2520work%252C%2520we%250Apropose%2520GeoNav%252C%2520a%2520geospatially%2520aware%2520multimodal%2520agent%2520to%2520enable%2520long-range%250Anavigation.%2520GeoNav%2520operates%2520in%2520three%2520phases-landmark%2520navigation%252C%2520target%2520search%252C%250Aand%2520precise%2520localization-mimicking%2520human%2520coarse-to-fine%2520spatial%2520strategies.%2520To%250Asupport%2520such%2520reasoning%252C%2520it%2520dynamically%2520builds%2520two%2520different%2520types%2520of%2520spatial%250Amemory.%2520The%2520first%2520is%2520a%2520global%2520but%2520schematic%2520cognitive%2520map%252C%2520which%2520fuses%2520prior%250Atextual%2520geographic%2520knowledge%2520and%2520embodied%2520visual%2520cues%2520into%2520a%2520top-down%252C%250Aannotated%2520form%2520for%2520fast%2520navigation%2520to%2520the%2520landmark%2520region.%2520The%2520second%2520is%2520a%250Alocal%2520but%2520delicate%2520scene%2520graph%2520representing%2520hierarchical%2520spatial%2520relationships%250Abetween%2520blocks%252C%2520landmarks%252C%2520and%2520objects%252C%2520which%2520is%2520used%2520for%2520definite%2520target%250Alocalization.%2520On%2520top%2520of%2520this%2520structured%2520representation%252C%2520GeoNav%2520employs%2520a%250Aspatially%2520aware%252C%2520multimodal%2520chain-of-thought%2520prompting%2520mechanism%2520to%2520enable%250Amultimodal%2520large%2520language%2520models%2520with%2520efficient%2520and%2520interpretable%250Adecision-making%2520across%2520stages.%2520On%2520the%2520CityNav%2520urban%2520navigation%2520benchmark%252C%250AGeoNav%2520surpasses%2520the%2520current%2520state-of-the-art%2520by%2520up%2520to%252012.53%2525%2520in%2520success%2520rate%250Aand%2520significantly%2520improves%2520navigation%2520efficiency%252C%2520even%2520in%2520hard-level%2520tasks.%250AAblation%2520studies%2520highlight%2520the%2520importance%2520of%2520each%2520module%252C%2520showcasing%2520how%250Ageospatial%2520representations%2520and%2520coarse-to-fine%2520reasoning%2520enhance%2520UAV%2520navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoNav%3A%20Empowering%20MLLMs%20with%20Explicit%20Geospatial%20Reasoning%20Abilities%0A%20%20for%20Language-Goal%20Aerial%20Navigation&entry.906535625=Haotian%20Xu%20and%20Yue%20Hu%20and%20Chen%20Gao%20and%20Zhengqiu%20Zhu%20and%20Yong%20Zhao%20and%20Yong%20Li%20and%20Quanjun%20Yin&entry.1292438233=%20%20Language-goal%20aerial%20navigation%20is%20a%20critical%20challenge%20in%20embodied%20AI%2C%0Arequiring%20UAVs%20to%20localize%20targets%20in%20complex%20environments%20such%20as%20urban%20blocks%0Abased%20on%20textual%20specification.%20Existing%20methods%2C%20often%20adapted%20from%20indoor%0Anavigation%2C%20struggle%20to%20scale%20due%20to%20limited%20field%20of%20view%2C%20semantic%20ambiguity%0Aamong%20objects%2C%20and%20lack%20of%20structured%20spatial%20reasoning.%20In%20this%20work%2C%20we%0Apropose%20GeoNav%2C%20a%20geospatially%20aware%20multimodal%20agent%20to%20enable%20long-range%0Anavigation.%20GeoNav%20operates%20in%20three%20phases-landmark%20navigation%2C%20target%20search%2C%0Aand%20precise%20localization-mimicking%20human%20coarse-to-fine%20spatial%20strategies.%20To%0Asupport%20such%20reasoning%2C%20it%20dynamically%20builds%20two%20different%20types%20of%20spatial%0Amemory.%20The%20first%20is%20a%20global%20but%20schematic%20cognitive%20map%2C%20which%20fuses%20prior%0Atextual%20geographic%20knowledge%20and%20embodied%20visual%20cues%20into%20a%20top-down%2C%0Aannotated%20form%20for%20fast%20navigation%20to%20the%20landmark%20region.%20The%20second%20is%20a%0Alocal%20but%20delicate%20scene%20graph%20representing%20hierarchical%20spatial%20relationships%0Abetween%20blocks%2C%20landmarks%2C%20and%20objects%2C%20which%20is%20used%20for%20definite%20target%0Alocalization.%20On%20top%20of%20this%20structured%20representation%2C%20GeoNav%20employs%20a%0Aspatially%20aware%2C%20multimodal%20chain-of-thought%20prompting%20mechanism%20to%20enable%0Amultimodal%20large%20language%20models%20with%20efficient%20and%20interpretable%0Adecision-making%20across%20stages.%20On%20the%20CityNav%20urban%20navigation%20benchmark%2C%0AGeoNav%20surpasses%20the%20current%20state-of-the-art%20by%20up%20to%2012.53%25%20in%20success%20rate%0Aand%20significantly%20improves%20navigation%20efficiency%2C%20even%20in%20hard-level%20tasks.%0AAblation%20studies%20highlight%20the%20importance%20of%20each%20module%2C%20showcasing%20how%0Ageospatial%20representations%20and%20coarse-to-fine%20reasoning%20enhance%20UAV%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09587v2&entry.124074799=Read"},
{"title": "Hierarchical Attention Fusion of Visual and Textual Representations for\n  Cross-Domain Sequential Recommendation", "author": "Wangyu Wu and Zhenhong Chen and Siqi Song and Xianglin Qiua and Xiaowei Huang and Fei Ma and Jimin Xiao", "abstract": "  Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences through intra- and inter-sequence item\nrelationships. Inspired by human cognitive processes, we propose Hierarchical\nAttention Fusion of Visual and Textual Representations (HAF-VT), a novel\napproach integrating visual and textual data to enhance cognitive modeling.\nUsing the frozen CLIP model, we generate image and text embeddings, enriching\nitem representations with multimodal data. A hierarchical attention mechanism\njointly learns single-domain and cross-domain preferences, mimicking human\ninformation integration. Evaluated on four e-commerce datasets, HAF-VT\noutperforms existing methods in capturing cross-domain user interests, bridging\ncognitive principles with computational models and highlighting the role of\nmultimodal data in sequential decision-making.\n", "link": "http://arxiv.org/abs/2504.15085v1", "date": "2025-04-21", "relevancy": 1.5863, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5285}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Attention%20Fusion%20of%20Visual%20and%20Textual%20Representations%20for%0A%20%20Cross-Domain%20Sequential%20Recommendation&body=Title%3A%20Hierarchical%20Attention%20Fusion%20of%20Visual%20and%20Textual%20Representations%20for%0A%20%20Cross-Domain%20Sequential%20Recommendation%0AAuthor%3A%20Wangyu%20Wu%20and%20Zhenhong%20Chen%20and%20Siqi%20Song%20and%20Xianglin%20Qiua%20and%20Xiaowei%20Huang%20and%20Fei%20Ma%20and%20Jimin%20Xiao%0AAbstract%3A%20%20%20Cross-Domain%20Sequential%20Recommendation%20%28CDSR%29%20predicts%20user%20behavior%20by%0Aleveraging%20historical%20interactions%20across%20multiple%20domains%2C%20focusing%20on%0Amodeling%20cross-domain%20preferences%20through%20intra-%20and%20inter-sequence%20item%0Arelationships.%20Inspired%20by%20human%20cognitive%20processes%2C%20we%20propose%20Hierarchical%0AAttention%20Fusion%20of%20Visual%20and%20Textual%20Representations%20%28HAF-VT%29%2C%20a%20novel%0Aapproach%20integrating%20visual%20and%20textual%20data%20to%20enhance%20cognitive%20modeling.%0AUsing%20the%20frozen%20CLIP%20model%2C%20we%20generate%20image%20and%20text%20embeddings%2C%20enriching%0Aitem%20representations%20with%20multimodal%20data.%20A%20hierarchical%20attention%20mechanism%0Ajointly%20learns%20single-domain%20and%20cross-domain%20preferences%2C%20mimicking%20human%0Ainformation%20integration.%20Evaluated%20on%20four%20e-commerce%20datasets%2C%20HAF-VT%0Aoutperforms%20existing%20methods%20in%20capturing%20cross-domain%20user%20interests%2C%20bridging%0Acognitive%20principles%20with%20computational%20models%20and%20highlighting%20the%20role%20of%0Amultimodal%20data%20in%20sequential%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15085v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Attention%2520Fusion%2520of%2520Visual%2520and%2520Textual%2520Representations%2520for%250A%2520%2520Cross-Domain%2520Sequential%2520Recommendation%26entry.906535625%3DWangyu%2520Wu%2520and%2520Zhenhong%2520Chen%2520and%2520Siqi%2520Song%2520and%2520Xianglin%2520Qiua%2520and%2520Xiaowei%2520Huang%2520and%2520Fei%2520Ma%2520and%2520Jimin%2520Xiao%26entry.1292438233%3D%2520%2520Cross-Domain%2520Sequential%2520Recommendation%2520%2528CDSR%2529%2520predicts%2520user%2520behavior%2520by%250Aleveraging%2520historical%2520interactions%2520across%2520multiple%2520domains%252C%2520focusing%2520on%250Amodeling%2520cross-domain%2520preferences%2520through%2520intra-%2520and%2520inter-sequence%2520item%250Arelationships.%2520Inspired%2520by%2520human%2520cognitive%2520processes%252C%2520we%2520propose%2520Hierarchical%250AAttention%2520Fusion%2520of%2520Visual%2520and%2520Textual%2520Representations%2520%2528HAF-VT%2529%252C%2520a%2520novel%250Aapproach%2520integrating%2520visual%2520and%2520textual%2520data%2520to%2520enhance%2520cognitive%2520modeling.%250AUsing%2520the%2520frozen%2520CLIP%2520model%252C%2520we%2520generate%2520image%2520and%2520text%2520embeddings%252C%2520enriching%250Aitem%2520representations%2520with%2520multimodal%2520data.%2520A%2520hierarchical%2520attention%2520mechanism%250Ajointly%2520learns%2520single-domain%2520and%2520cross-domain%2520preferences%252C%2520mimicking%2520human%250Ainformation%2520integration.%2520Evaluated%2520on%2520four%2520e-commerce%2520datasets%252C%2520HAF-VT%250Aoutperforms%2520existing%2520methods%2520in%2520capturing%2520cross-domain%2520user%2520interests%252C%2520bridging%250Acognitive%2520principles%2520with%2520computational%2520models%2520and%2520highlighting%2520the%2520role%2520of%250Amultimodal%2520data%2520in%2520sequential%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15085v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Attention%20Fusion%20of%20Visual%20and%20Textual%20Representations%20for%0A%20%20Cross-Domain%20Sequential%20Recommendation&entry.906535625=Wangyu%20Wu%20and%20Zhenhong%20Chen%20and%20Siqi%20Song%20and%20Xianglin%20Qiua%20and%20Xiaowei%20Huang%20and%20Fei%20Ma%20and%20Jimin%20Xiao&entry.1292438233=%20%20Cross-Domain%20Sequential%20Recommendation%20%28CDSR%29%20predicts%20user%20behavior%20by%0Aleveraging%20historical%20interactions%20across%20multiple%20domains%2C%20focusing%20on%0Amodeling%20cross-domain%20preferences%20through%20intra-%20and%20inter-sequence%20item%0Arelationships.%20Inspired%20by%20human%20cognitive%20processes%2C%20we%20propose%20Hierarchical%0AAttention%20Fusion%20of%20Visual%20and%20Textual%20Representations%20%28HAF-VT%29%2C%20a%20novel%0Aapproach%20integrating%20visual%20and%20textual%20data%20to%20enhance%20cognitive%20modeling.%0AUsing%20the%20frozen%20CLIP%20model%2C%20we%20generate%20image%20and%20text%20embeddings%2C%20enriching%0Aitem%20representations%20with%20multimodal%20data.%20A%20hierarchical%20attention%20mechanism%0Ajointly%20learns%20single-domain%20and%20cross-domain%20preferences%2C%20mimicking%20human%0Ainformation%20integration.%20Evaluated%20on%20four%20e-commerce%20datasets%2C%20HAF-VT%0Aoutperforms%20existing%20methods%20in%20capturing%20cross-domain%20user%20interests%2C%20bridging%0Acognitive%20principles%20with%20computational%20models%20and%20highlighting%20the%20role%20of%0Amultimodal%20data%20in%20sequential%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15085v1&entry.124074799=Read"},
{"title": "A Controllable Appearance Representation for Flexible Transfer and\n  Editing", "author": "Santiago Jimenez-Navarro and Julia Guerrero-Viu and Belen Masia", "abstract": "  We present a method that computes an interpretable representation of material\nappearance within a highly compact, disentangled latent space. This\nrepresentation is learned in a self-supervised fashion using an adapted\nFactorVAE. We train our model with a carefully designed unlabeled dataset,\navoiding possible biases induced by human-generated labels. Our model\ndemonstrates strong disentanglement and interpretability by effectively\nencoding material appearance and illumination, despite the absence of explicit\nsupervision. Then, we use our representation as guidance for training a\nlightweight IP-Adapter to condition a diffusion pipeline that transfers the\nappearance of one or more images onto a target geometry, and allows the user to\nfurther edit the resulting appearance. Our approach offers fine-grained control\nover the generated results: thanks to the well-structured compact latent space,\nusers can intuitively manipulate attributes such as hue or glossiness in image\nspace to achieve the desired final appearance.\n", "link": "http://arxiv.org/abs/2504.15028v1", "date": "2025-04-21", "relevancy": 1.7877, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6221}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6025}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Controllable%20Appearance%20Representation%20for%20Flexible%20Transfer%20and%0A%20%20Editing&body=Title%3A%20A%20Controllable%20Appearance%20Representation%20for%20Flexible%20Transfer%20and%0A%20%20Editing%0AAuthor%3A%20Santiago%20Jimenez-Navarro%20and%20Julia%20Guerrero-Viu%20and%20Belen%20Masia%0AAbstract%3A%20%20%20We%20present%20a%20method%20that%20computes%20an%20interpretable%20representation%20of%20material%0Aappearance%20within%20a%20highly%20compact%2C%20disentangled%20latent%20space.%20This%0Arepresentation%20is%20learned%20in%20a%20self-supervised%20fashion%20using%20an%20adapted%0AFactorVAE.%20We%20train%20our%20model%20with%20a%20carefully%20designed%20unlabeled%20dataset%2C%0Aavoiding%20possible%20biases%20induced%20by%20human-generated%20labels.%20Our%20model%0Ademonstrates%20strong%20disentanglement%20and%20interpretability%20by%20effectively%0Aencoding%20material%20appearance%20and%20illumination%2C%20despite%20the%20absence%20of%20explicit%0Asupervision.%20Then%2C%20we%20use%20our%20representation%20as%20guidance%20for%20training%20a%0Alightweight%20IP-Adapter%20to%20condition%20a%20diffusion%20pipeline%20that%20transfers%20the%0Aappearance%20of%20one%20or%20more%20images%20onto%20a%20target%20geometry%2C%20and%20allows%20the%20user%20to%0Afurther%20edit%20the%20resulting%20appearance.%20Our%20approach%20offers%20fine-grained%20control%0Aover%20the%20generated%20results%3A%20thanks%20to%20the%20well-structured%20compact%20latent%20space%2C%0Ausers%20can%20intuitively%20manipulate%20attributes%20such%20as%20hue%20or%20glossiness%20in%20image%0Aspace%20to%20achieve%20the%20desired%20final%20appearance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Controllable%2520Appearance%2520Representation%2520for%2520Flexible%2520Transfer%2520and%250A%2520%2520Editing%26entry.906535625%3DSantiago%2520Jimenez-Navarro%2520and%2520Julia%2520Guerrero-Viu%2520and%2520Belen%2520Masia%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520that%2520computes%2520an%2520interpretable%2520representation%2520of%2520material%250Aappearance%2520within%2520a%2520highly%2520compact%252C%2520disentangled%2520latent%2520space.%2520This%250Arepresentation%2520is%2520learned%2520in%2520a%2520self-supervised%2520fashion%2520using%2520an%2520adapted%250AFactorVAE.%2520We%2520train%2520our%2520model%2520with%2520a%2520carefully%2520designed%2520unlabeled%2520dataset%252C%250Aavoiding%2520possible%2520biases%2520induced%2520by%2520human-generated%2520labels.%2520Our%2520model%250Ademonstrates%2520strong%2520disentanglement%2520and%2520interpretability%2520by%2520effectively%250Aencoding%2520material%2520appearance%2520and%2520illumination%252C%2520despite%2520the%2520absence%2520of%2520explicit%250Asupervision.%2520Then%252C%2520we%2520use%2520our%2520representation%2520as%2520guidance%2520for%2520training%2520a%250Alightweight%2520IP-Adapter%2520to%2520condition%2520a%2520diffusion%2520pipeline%2520that%2520transfers%2520the%250Aappearance%2520of%2520one%2520or%2520more%2520images%2520onto%2520a%2520target%2520geometry%252C%2520and%2520allows%2520the%2520user%2520to%250Afurther%2520edit%2520the%2520resulting%2520appearance.%2520Our%2520approach%2520offers%2520fine-grained%2520control%250Aover%2520the%2520generated%2520results%253A%2520thanks%2520to%2520the%2520well-structured%2520compact%2520latent%2520space%252C%250Ausers%2520can%2520intuitively%2520manipulate%2520attributes%2520such%2520as%2520hue%2520or%2520glossiness%2520in%2520image%250Aspace%2520to%2520achieve%2520the%2520desired%2520final%2520appearance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Controllable%20Appearance%20Representation%20for%20Flexible%20Transfer%20and%0A%20%20Editing&entry.906535625=Santiago%20Jimenez-Navarro%20and%20Julia%20Guerrero-Viu%20and%20Belen%20Masia&entry.1292438233=%20%20We%20present%20a%20method%20that%20computes%20an%20interpretable%20representation%20of%20material%0Aappearance%20within%20a%20highly%20compact%2C%20disentangled%20latent%20space.%20This%0Arepresentation%20is%20learned%20in%20a%20self-supervised%20fashion%20using%20an%20adapted%0AFactorVAE.%20We%20train%20our%20model%20with%20a%20carefully%20designed%20unlabeled%20dataset%2C%0Aavoiding%20possible%20biases%20induced%20by%20human-generated%20labels.%20Our%20model%0Ademonstrates%20strong%20disentanglement%20and%20interpretability%20by%20effectively%0Aencoding%20material%20appearance%20and%20illumination%2C%20despite%20the%20absence%20of%20explicit%0Asupervision.%20Then%2C%20we%20use%20our%20representation%20as%20guidance%20for%20training%20a%0Alightweight%20IP-Adapter%20to%20condition%20a%20diffusion%20pipeline%20that%20transfers%20the%0Aappearance%20of%20one%20or%20more%20images%20onto%20a%20target%20geometry%2C%20and%20allows%20the%20user%20to%0Afurther%20edit%20the%20resulting%20appearance.%20Our%20approach%20offers%20fine-grained%20control%0Aover%20the%20generated%20results%3A%20thanks%20to%20the%20well-structured%20compact%20latent%20space%2C%0Ausers%20can%20intuitively%20manipulate%20attributes%20such%20as%20hue%20or%20glossiness%20in%20image%0Aspace%20to%20achieve%20the%20desired%20final%20appearance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15028v1&entry.124074799=Read"},
{"title": "Position: Bayesian Statistics Facilitates Stakeholder Participation in\n  Evaluation of Generative AI", "author": "Yanan Long", "abstract": "  The evaluation of Generative AI (GenAI) systems plays a critical role in\npublic policy and decision-making, yet existing methods are often limited by\nreliance on benchmark-driven, point-estimate comparisons that fail to capture\nuncertainty and broader societal impacts. This paper argues for the use of\nBayesian statistics as a principled framework to address these challenges.\nBayesian methods enable the integration of domain expertise through prior\nelicitation, allow for continuous learning from new data, and provide robust\nuncertainty quantification via posterior inference. We demonstrate how Bayesian\ninference can be applied to GenAI evaluation, particularly in incorporating\nstakeholder perspectives to enhance fairness, transparency, and reliability.\nFurthermore, we discuss Bayesian workflows as an iterative process for model\nvalidation and refinement, ensuring robust assessments of GenAI systems in\ndynamic, real-world contexts.\n", "link": "http://arxiv.org/abs/2504.15211v1", "date": "2025-04-21", "relevancy": 1.5671, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5412}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5387}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Bayesian%20Statistics%20Facilitates%20Stakeholder%20Participation%20in%0A%20%20Evaluation%20of%20Generative%20AI&body=Title%3A%20Position%3A%20Bayesian%20Statistics%20Facilitates%20Stakeholder%20Participation%20in%0A%20%20Evaluation%20of%20Generative%20AI%0AAuthor%3A%20Yanan%20Long%0AAbstract%3A%20%20%20The%20evaluation%20of%20Generative%20AI%20%28GenAI%29%20systems%20plays%20a%20critical%20role%20in%0Apublic%20policy%20and%20decision-making%2C%20yet%20existing%20methods%20are%20often%20limited%20by%0Areliance%20on%20benchmark-driven%2C%20point-estimate%20comparisons%20that%20fail%20to%20capture%0Auncertainty%20and%20broader%20societal%20impacts.%20This%20paper%20argues%20for%20the%20use%20of%0ABayesian%20statistics%20as%20a%20principled%20framework%20to%20address%20these%20challenges.%0ABayesian%20methods%20enable%20the%20integration%20of%20domain%20expertise%20through%20prior%0Aelicitation%2C%20allow%20for%20continuous%20learning%20from%20new%20data%2C%20and%20provide%20robust%0Auncertainty%20quantification%20via%20posterior%20inference.%20We%20demonstrate%20how%20Bayesian%0Ainference%20can%20be%20applied%20to%20GenAI%20evaluation%2C%20particularly%20in%20incorporating%0Astakeholder%20perspectives%20to%20enhance%20fairness%2C%20transparency%2C%20and%20reliability.%0AFurthermore%2C%20we%20discuss%20Bayesian%20workflows%20as%20an%20iterative%20process%20for%20model%0Avalidation%20and%20refinement%2C%20ensuring%20robust%20assessments%20of%20GenAI%20systems%20in%0Adynamic%2C%20real-world%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Bayesian%2520Statistics%2520Facilitates%2520Stakeholder%2520Participation%2520in%250A%2520%2520Evaluation%2520of%2520Generative%2520AI%26entry.906535625%3DYanan%2520Long%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520Generative%2520AI%2520%2528GenAI%2529%2520systems%2520plays%2520a%2520critical%2520role%2520in%250Apublic%2520policy%2520and%2520decision-making%252C%2520yet%2520existing%2520methods%2520are%2520often%2520limited%2520by%250Areliance%2520on%2520benchmark-driven%252C%2520point-estimate%2520comparisons%2520that%2520fail%2520to%2520capture%250Auncertainty%2520and%2520broader%2520societal%2520impacts.%2520This%2520paper%2520argues%2520for%2520the%2520use%2520of%250ABayesian%2520statistics%2520as%2520a%2520principled%2520framework%2520to%2520address%2520these%2520challenges.%250ABayesian%2520methods%2520enable%2520the%2520integration%2520of%2520domain%2520expertise%2520through%2520prior%250Aelicitation%252C%2520allow%2520for%2520continuous%2520learning%2520from%2520new%2520data%252C%2520and%2520provide%2520robust%250Auncertainty%2520quantification%2520via%2520posterior%2520inference.%2520We%2520demonstrate%2520how%2520Bayesian%250Ainference%2520can%2520be%2520applied%2520to%2520GenAI%2520evaluation%252C%2520particularly%2520in%2520incorporating%250Astakeholder%2520perspectives%2520to%2520enhance%2520fairness%252C%2520transparency%252C%2520and%2520reliability.%250AFurthermore%252C%2520we%2520discuss%2520Bayesian%2520workflows%2520as%2520an%2520iterative%2520process%2520for%2520model%250Avalidation%2520and%2520refinement%252C%2520ensuring%2520robust%2520assessments%2520of%2520GenAI%2520systems%2520in%250Adynamic%252C%2520real-world%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Bayesian%20Statistics%20Facilitates%20Stakeholder%20Participation%20in%0A%20%20Evaluation%20of%20Generative%20AI&entry.906535625=Yanan%20Long&entry.1292438233=%20%20The%20evaluation%20of%20Generative%20AI%20%28GenAI%29%20systems%20plays%20a%20critical%20role%20in%0Apublic%20policy%20and%20decision-making%2C%20yet%20existing%20methods%20are%20often%20limited%20by%0Areliance%20on%20benchmark-driven%2C%20point-estimate%20comparisons%20that%20fail%20to%20capture%0Auncertainty%20and%20broader%20societal%20impacts.%20This%20paper%20argues%20for%20the%20use%20of%0ABayesian%20statistics%20as%20a%20principled%20framework%20to%20address%20these%20challenges.%0ABayesian%20methods%20enable%20the%20integration%20of%20domain%20expertise%20through%20prior%0Aelicitation%2C%20allow%20for%20continuous%20learning%20from%20new%20data%2C%20and%20provide%20robust%0Auncertainty%20quantification%20via%20posterior%20inference.%20We%20demonstrate%20how%20Bayesian%0Ainference%20can%20be%20applied%20to%20GenAI%20evaluation%2C%20particularly%20in%20incorporating%0Astakeholder%20perspectives%20to%20enhance%20fairness%2C%20transparency%2C%20and%20reliability.%0AFurthermore%2C%20we%20discuss%20Bayesian%20workflows%20as%20an%20iterative%20process%20for%20model%0Avalidation%20and%20refinement%2C%20ensuring%20robust%20assessments%20of%20GenAI%20systems%20in%0Adynamic%2C%20real-world%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15211v1&entry.124074799=Read"},
{"title": "Fully Bayesian Approaches to Topics over Time", "author": "Juli\u00e1n Cendrero and Julio Gonzalo and Ivar Zapata", "abstract": "  The Topics over Time (ToT) model captures thematic changes in timestamped\ndatasets by explicitly modeling publication dates jointly with word\nco-occurrence patterns. However, ToT was not approached in a fully Bayesian\nfashion, a flaw that makes it susceptible to stability problems. To address\nthis issue, we propose a fully Bayesian Topics over Time (BToT) model via the\nintroduction of a conjugate prior to the Beta distribution. This prior acts as\na regularization that prevents the online version of the algorithm from\nunstable updates when a topic is poorly represented in a mini-batch. The\ncharacteristics of this prior to the Beta distribution are studied here for the\nfirst time. Still, this model suffers from a difference in scale between the\nsingle-time observations and the multiplicity of words per document. A\nvariation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a\nsolution. In WBToT, publication dates are repeated a certain number of times\nper document, which balances the relative influence of words and timestamps\nalong the inference process. We have tested our models on two datasets: a\ncollection of over 200 years of US state-of-the-union (SOTU) addresses and a\nlarge-scale COVID-19 Twitter corpus of 10 million tweets. The results show that\nWBToT captures events better than Latent Dirichlet Allocation and other SOTA\ntopic models like BERTopic: the median absolute deviation of the topic presence\nover time is reduced by $51\\%$ and $34\\%$, respectively. Our experiments also\ndemonstrate the superior coherence of WBToT over BToT, which highlights the\nimportance of balancing the time and word modalities. Finally, we illustrate\nthe stability of the online optimization algorithm in WBToT, which allows the\napplication of WBToT to problems that are intractable for standard ToT.\n", "link": "http://arxiv.org/abs/2504.15220v1", "date": "2025-04-21", "relevancy": 1.4517, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4893}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4877}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20Bayesian%20Approaches%20to%20Topics%20over%20Time&body=Title%3A%20Fully%20Bayesian%20Approaches%20to%20Topics%20over%20Time%0AAuthor%3A%20Juli%C3%A1n%20Cendrero%20and%20Julio%20Gonzalo%20and%20Ivar%20Zapata%0AAbstract%3A%20%20%20The%20Topics%20over%20Time%20%28ToT%29%20model%20captures%20thematic%20changes%20in%20timestamped%0Adatasets%20by%20explicitly%20modeling%20publication%20dates%20jointly%20with%20word%0Aco-occurrence%20patterns.%20However%2C%20ToT%20was%20not%20approached%20in%20a%20fully%20Bayesian%0Afashion%2C%20a%20flaw%20that%20makes%20it%20susceptible%20to%20stability%20problems.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20fully%20Bayesian%20Topics%20over%20Time%20%28BToT%29%20model%20via%20the%0Aintroduction%20of%20a%20conjugate%20prior%20to%20the%20Beta%20distribution.%20This%20prior%20acts%20as%0Aa%20regularization%20that%20prevents%20the%20online%20version%20of%20the%20algorithm%20from%0Aunstable%20updates%20when%20a%20topic%20is%20poorly%20represented%20in%20a%20mini-batch.%20The%0Acharacteristics%20of%20this%20prior%20to%20the%20Beta%20distribution%20are%20studied%20here%20for%20the%0Afirst%20time.%20Still%2C%20this%20model%20suffers%20from%20a%20difference%20in%20scale%20between%20the%0Asingle-time%20observations%20and%20the%20multiplicity%20of%20words%20per%20document.%20A%0Avariation%20of%20BToT%2C%20Weighted%20Bayesian%20Topics%20over%20Time%20%28WBToT%29%2C%20is%20proposed%20as%20a%0Asolution.%20In%20WBToT%2C%20publication%20dates%20are%20repeated%20a%20certain%20number%20of%20times%0Aper%20document%2C%20which%20balances%20the%20relative%20influence%20of%20words%20and%20timestamps%0Aalong%20the%20inference%20process.%20We%20have%20tested%20our%20models%20on%20two%20datasets%3A%20a%0Acollection%20of%20over%20200%20years%20of%20US%20state-of-the-union%20%28SOTU%29%20addresses%20and%20a%0Alarge-scale%20COVID-19%20Twitter%20corpus%20of%2010%20million%20tweets.%20The%20results%20show%20that%0AWBToT%20captures%20events%20better%20than%20Latent%20Dirichlet%20Allocation%20and%20other%20SOTA%0Atopic%20models%20like%20BERTopic%3A%20the%20median%20absolute%20deviation%20of%20the%20topic%20presence%0Aover%20time%20is%20reduced%20by%20%2451%5C%25%24%20and%20%2434%5C%25%24%2C%20respectively.%20Our%20experiments%20also%0Ademonstrate%20the%20superior%20coherence%20of%20WBToT%20over%20BToT%2C%20which%20highlights%20the%0Aimportance%20of%20balancing%20the%20time%20and%20word%20modalities.%20Finally%2C%20we%20illustrate%0Athe%20stability%20of%20the%20online%20optimization%20algorithm%20in%20WBToT%2C%20which%20allows%20the%0Aapplication%20of%20WBToT%20to%20problems%20that%20are%20intractable%20for%20standard%20ToT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15220v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520Bayesian%2520Approaches%2520to%2520Topics%2520over%2520Time%26entry.906535625%3DJuli%25C3%25A1n%2520Cendrero%2520and%2520Julio%2520Gonzalo%2520and%2520Ivar%2520Zapata%26entry.1292438233%3D%2520%2520The%2520Topics%2520over%2520Time%2520%2528ToT%2529%2520model%2520captures%2520thematic%2520changes%2520in%2520timestamped%250Adatasets%2520by%2520explicitly%2520modeling%2520publication%2520dates%2520jointly%2520with%2520word%250Aco-occurrence%2520patterns.%2520However%252C%2520ToT%2520was%2520not%2520approached%2520in%2520a%2520fully%2520Bayesian%250Afashion%252C%2520a%2520flaw%2520that%2520makes%2520it%2520susceptible%2520to%2520stability%2520problems.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520a%2520fully%2520Bayesian%2520Topics%2520over%2520Time%2520%2528BToT%2529%2520model%2520via%2520the%250Aintroduction%2520of%2520a%2520conjugate%2520prior%2520to%2520the%2520Beta%2520distribution.%2520This%2520prior%2520acts%2520as%250Aa%2520regularization%2520that%2520prevents%2520the%2520online%2520version%2520of%2520the%2520algorithm%2520from%250Aunstable%2520updates%2520when%2520a%2520topic%2520is%2520poorly%2520represented%2520in%2520a%2520mini-batch.%2520The%250Acharacteristics%2520of%2520this%2520prior%2520to%2520the%2520Beta%2520distribution%2520are%2520studied%2520here%2520for%2520the%250Afirst%2520time.%2520Still%252C%2520this%2520model%2520suffers%2520from%2520a%2520difference%2520in%2520scale%2520between%2520the%250Asingle-time%2520observations%2520and%2520the%2520multiplicity%2520of%2520words%2520per%2520document.%2520A%250Avariation%2520of%2520BToT%252C%2520Weighted%2520Bayesian%2520Topics%2520over%2520Time%2520%2528WBToT%2529%252C%2520is%2520proposed%2520as%2520a%250Asolution.%2520In%2520WBToT%252C%2520publication%2520dates%2520are%2520repeated%2520a%2520certain%2520number%2520of%2520times%250Aper%2520document%252C%2520which%2520balances%2520the%2520relative%2520influence%2520of%2520words%2520and%2520timestamps%250Aalong%2520the%2520inference%2520process.%2520We%2520have%2520tested%2520our%2520models%2520on%2520two%2520datasets%253A%2520a%250Acollection%2520of%2520over%2520200%2520years%2520of%2520US%2520state-of-the-union%2520%2528SOTU%2529%2520addresses%2520and%2520a%250Alarge-scale%2520COVID-19%2520Twitter%2520corpus%2520of%252010%2520million%2520tweets.%2520The%2520results%2520show%2520that%250AWBToT%2520captures%2520events%2520better%2520than%2520Latent%2520Dirichlet%2520Allocation%2520and%2520other%2520SOTA%250Atopic%2520models%2520like%2520BERTopic%253A%2520the%2520median%2520absolute%2520deviation%2520of%2520the%2520topic%2520presence%250Aover%2520time%2520is%2520reduced%2520by%2520%252451%255C%2525%2524%2520and%2520%252434%255C%2525%2524%252C%2520respectively.%2520Our%2520experiments%2520also%250Ademonstrate%2520the%2520superior%2520coherence%2520of%2520WBToT%2520over%2520BToT%252C%2520which%2520highlights%2520the%250Aimportance%2520of%2520balancing%2520the%2520time%2520and%2520word%2520modalities.%2520Finally%252C%2520we%2520illustrate%250Athe%2520stability%2520of%2520the%2520online%2520optimization%2520algorithm%2520in%2520WBToT%252C%2520which%2520allows%2520the%250Aapplication%2520of%2520WBToT%2520to%2520problems%2520that%2520are%2520intractable%2520for%2520standard%2520ToT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15220v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Bayesian%20Approaches%20to%20Topics%20over%20Time&entry.906535625=Juli%C3%A1n%20Cendrero%20and%20Julio%20Gonzalo%20and%20Ivar%20Zapata&entry.1292438233=%20%20The%20Topics%20over%20Time%20%28ToT%29%20model%20captures%20thematic%20changes%20in%20timestamped%0Adatasets%20by%20explicitly%20modeling%20publication%20dates%20jointly%20with%20word%0Aco-occurrence%20patterns.%20However%2C%20ToT%20was%20not%20approached%20in%20a%20fully%20Bayesian%0Afashion%2C%20a%20flaw%20that%20makes%20it%20susceptible%20to%20stability%20problems.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20fully%20Bayesian%20Topics%20over%20Time%20%28BToT%29%20model%20via%20the%0Aintroduction%20of%20a%20conjugate%20prior%20to%20the%20Beta%20distribution.%20This%20prior%20acts%20as%0Aa%20regularization%20that%20prevents%20the%20online%20version%20of%20the%20algorithm%20from%0Aunstable%20updates%20when%20a%20topic%20is%20poorly%20represented%20in%20a%20mini-batch.%20The%0Acharacteristics%20of%20this%20prior%20to%20the%20Beta%20distribution%20are%20studied%20here%20for%20the%0Afirst%20time.%20Still%2C%20this%20model%20suffers%20from%20a%20difference%20in%20scale%20between%20the%0Asingle-time%20observations%20and%20the%20multiplicity%20of%20words%20per%20document.%20A%0Avariation%20of%20BToT%2C%20Weighted%20Bayesian%20Topics%20over%20Time%20%28WBToT%29%2C%20is%20proposed%20as%20a%0Asolution.%20In%20WBToT%2C%20publication%20dates%20are%20repeated%20a%20certain%20number%20of%20times%0Aper%20document%2C%20which%20balances%20the%20relative%20influence%20of%20words%20and%20timestamps%0Aalong%20the%20inference%20process.%20We%20have%20tested%20our%20models%20on%20two%20datasets%3A%20a%0Acollection%20of%20over%20200%20years%20of%20US%20state-of-the-union%20%28SOTU%29%20addresses%20and%20a%0Alarge-scale%20COVID-19%20Twitter%20corpus%20of%2010%20million%20tweets.%20The%20results%20show%20that%0AWBToT%20captures%20events%20better%20than%20Latent%20Dirichlet%20Allocation%20and%20other%20SOTA%0Atopic%20models%20like%20BERTopic%3A%20the%20median%20absolute%20deviation%20of%20the%20topic%20presence%0Aover%20time%20is%20reduced%20by%20%2451%5C%25%24%20and%20%2434%5C%25%24%2C%20respectively.%20Our%20experiments%20also%0Ademonstrate%20the%20superior%20coherence%20of%20WBToT%20over%20BToT%2C%20which%20highlights%20the%0Aimportance%20of%20balancing%20the%20time%20and%20word%20modalities.%20Finally%2C%20we%20illustrate%0Athe%20stability%20of%20the%20online%20optimization%20algorithm%20in%20WBToT%2C%20which%20allows%20the%0Aapplication%20of%20WBToT%20to%20problems%20that%20are%20intractable%20for%20standard%20ToT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15220v1&entry.124074799=Read"},
{"title": "A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space\n  Servicing", "author": "Nathan Steffen and Wilhelm Louw and Nicholas Ernest and Timothy Arnett and Kelly Cohen", "abstract": "  Automation of robotic systems for servicing in cislunar space is becoming\nextremely important as the number of satellites in orbit increases. Safety is\ncritical in performing satellite maintenance, so the control techniques\nutilized must be trusted in addition to being highly efficient. In this work,\nGenetic Fuzzy Trees are combined with the widely used LQR control scheme via\nThales' TrUE AI Toolkit to create a trusted and efficient controller for a\ntwo-degree-of-freedom planar robotic manipulator that would theoretically be\nused to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is\n18.5% more performant than optimal LQR on average, and that it is incredibly\nrobust to uncertainty.\n", "link": "http://arxiv.org/abs/2504.15226v1", "date": "2025-04-21", "relevancy": 1.5956, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5585}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Genetic%20Fuzzy-Enabled%20Framework%20on%20Robotic%20Manipulation%20for%20In-Space%0A%20%20Servicing&body=Title%3A%20A%20Genetic%20Fuzzy-Enabled%20Framework%20on%20Robotic%20Manipulation%20for%20In-Space%0A%20%20Servicing%0AAuthor%3A%20Nathan%20Steffen%20and%20Wilhelm%20Louw%20and%20Nicholas%20Ernest%20and%20Timothy%20Arnett%20and%20Kelly%20Cohen%0AAbstract%3A%20%20%20Automation%20of%20robotic%20systems%20for%20servicing%20in%20cislunar%20space%20is%20becoming%0Aextremely%20important%20as%20the%20number%20of%20satellites%20in%20orbit%20increases.%20Safety%20is%0Acritical%20in%20performing%20satellite%20maintenance%2C%20so%20the%20control%20techniques%0Autilized%20must%20be%20trusted%20in%20addition%20to%20being%20highly%20efficient.%20In%20this%20work%2C%0AGenetic%20Fuzzy%20Trees%20are%20combined%20with%20the%20widely%20used%20LQR%20control%20scheme%20via%0AThales%27%20TrUE%20AI%20Toolkit%20to%20create%20a%20trusted%20and%20efficient%20controller%20for%20a%0Atwo-degree-of-freedom%20planar%20robotic%20manipulator%20that%20would%20theoretically%20be%0Aused%20to%20perform%20satellite%20maintenance.%20It%20was%20found%20that%20Genetic%20Fuzzy-LQR%20is%0A18.5%25%20more%20performant%20than%20optimal%20LQR%20on%20average%2C%20and%20that%20it%20is%20incredibly%0Arobust%20to%20uncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Genetic%2520Fuzzy-Enabled%2520Framework%2520on%2520Robotic%2520Manipulation%2520for%2520In-Space%250A%2520%2520Servicing%26entry.906535625%3DNathan%2520Steffen%2520and%2520Wilhelm%2520Louw%2520and%2520Nicholas%2520Ernest%2520and%2520Timothy%2520Arnett%2520and%2520Kelly%2520Cohen%26entry.1292438233%3D%2520%2520Automation%2520of%2520robotic%2520systems%2520for%2520servicing%2520in%2520cislunar%2520space%2520is%2520becoming%250Aextremely%2520important%2520as%2520the%2520number%2520of%2520satellites%2520in%2520orbit%2520increases.%2520Safety%2520is%250Acritical%2520in%2520performing%2520satellite%2520maintenance%252C%2520so%2520the%2520control%2520techniques%250Autilized%2520must%2520be%2520trusted%2520in%2520addition%2520to%2520being%2520highly%2520efficient.%2520In%2520this%2520work%252C%250AGenetic%2520Fuzzy%2520Trees%2520are%2520combined%2520with%2520the%2520widely%2520used%2520LQR%2520control%2520scheme%2520via%250AThales%2527%2520TrUE%2520AI%2520Toolkit%2520to%2520create%2520a%2520trusted%2520and%2520efficient%2520controller%2520for%2520a%250Atwo-degree-of-freedom%2520planar%2520robotic%2520manipulator%2520that%2520would%2520theoretically%2520be%250Aused%2520to%2520perform%2520satellite%2520maintenance.%2520It%2520was%2520found%2520that%2520Genetic%2520Fuzzy-LQR%2520is%250A18.5%2525%2520more%2520performant%2520than%2520optimal%2520LQR%2520on%2520average%252C%2520and%2520that%2520it%2520is%2520incredibly%250Arobust%2520to%2520uncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Genetic%20Fuzzy-Enabled%20Framework%20on%20Robotic%20Manipulation%20for%20In-Space%0A%20%20Servicing&entry.906535625=Nathan%20Steffen%20and%20Wilhelm%20Louw%20and%20Nicholas%20Ernest%20and%20Timothy%20Arnett%20and%20Kelly%20Cohen&entry.1292438233=%20%20Automation%20of%20robotic%20systems%20for%20servicing%20in%20cislunar%20space%20is%20becoming%0Aextremely%20important%20as%20the%20number%20of%20satellites%20in%20orbit%20increases.%20Safety%20is%0Acritical%20in%20performing%20satellite%20maintenance%2C%20so%20the%20control%20techniques%0Autilized%20must%20be%20trusted%20in%20addition%20to%20being%20highly%20efficient.%20In%20this%20work%2C%0AGenetic%20Fuzzy%20Trees%20are%20combined%20with%20the%20widely%20used%20LQR%20control%20scheme%20via%0AThales%27%20TrUE%20AI%20Toolkit%20to%20create%20a%20trusted%20and%20efficient%20controller%20for%20a%0Atwo-degree-of-freedom%20planar%20robotic%20manipulator%20that%20would%20theoretically%20be%0Aused%20to%20perform%20satellite%20maintenance.%20It%20was%20found%20that%20Genetic%20Fuzzy-LQR%20is%0A18.5%25%20more%20performant%20than%20optimal%20LQR%20on%20average%2C%20and%20that%20it%20is%20incredibly%0Arobust%20to%20uncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15226v1&entry.124074799=Read"},
{"title": "How Global Calibration Strengthens Multiaccuracy", "author": "S\u00edlvia Casacuberta and Parikshit Gopalan and Varun Kanade and Omer Reingold", "abstract": "  Multiaccuracy and multicalibration are multigroup fairness notions for\nprediction that have found numerous applications in learning and computational\ncomplexity. They can be achieved from a single learning primitive: weak\nagnostic learning. Here we investigate the power of multiaccuracy as a learning\nprimitive, both with and without the additional assumption of calibration. We\nfind that multiaccuracy in itself is rather weak, but that the addition of\nglobal calibration (this notion is called calibrated multiaccuracy) boosts its\npower substantially, enough to recover implications that were previously known\nonly assuming the stronger notion of multicalibration.\n  We give evidence that multiaccuracy might not be as powerful as standard weak\nagnostic learning, by showing that there is no way to post-process a\nmultiaccurate predictor to get a weak learner, even assuming the best\nhypothesis has correlation $1/2$. Rather, we show that it yields a restricted\nform of weak agnostic learning, which requires some concept in the class to\nhave correlation greater than $1/2$ with the labels. However, by also requiring\nthe predictor to be calibrated, we recover not just weak, but strong agnostic\nlearning.\n  A similar picture emerges when we consider the derivation of hardcore\nmeasures from predictors satisfying multigroup fairness notions. On the one\nhand, while multiaccuracy only yields hardcore measures of density half the\noptimal, we show that (a weighted version of) calibrated multiaccuracy achieves\noptimal density.\n  Our results yield new insights into the complementary roles played by\nmultiaccuracy and calibration in each setting. They shed light on why\nmultiaccuracy and global calibration, although not particularly powerful by\nthemselves, together yield considerably stronger notions.\n", "link": "http://arxiv.org/abs/2504.15206v1", "date": "2025-04-21", "relevancy": 1.8559, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4953}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4615}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Global%20Calibration%20Strengthens%20Multiaccuracy&body=Title%3A%20How%20Global%20Calibration%20Strengthens%20Multiaccuracy%0AAuthor%3A%20S%C3%ADlvia%20Casacuberta%20and%20Parikshit%20Gopalan%20and%20Varun%20Kanade%20and%20Omer%20Reingold%0AAbstract%3A%20%20%20Multiaccuracy%20and%20multicalibration%20are%20multigroup%20fairness%20notions%20for%0Aprediction%20that%20have%20found%20numerous%20applications%20in%20learning%20and%20computational%0Acomplexity.%20They%20can%20be%20achieved%20from%20a%20single%20learning%20primitive%3A%20weak%0Aagnostic%20learning.%20Here%20we%20investigate%20the%20power%20of%20multiaccuracy%20as%20a%20learning%0Aprimitive%2C%20both%20with%20and%20without%20the%20additional%20assumption%20of%20calibration.%20We%0Afind%20that%20multiaccuracy%20in%20itself%20is%20rather%20weak%2C%20but%20that%20the%20addition%20of%0Aglobal%20calibration%20%28this%20notion%20is%20called%20calibrated%20multiaccuracy%29%20boosts%20its%0Apower%20substantially%2C%20enough%20to%20recover%20implications%20that%20were%20previously%20known%0Aonly%20assuming%20the%20stronger%20notion%20of%20multicalibration.%0A%20%20We%20give%20evidence%20that%20multiaccuracy%20might%20not%20be%20as%20powerful%20as%20standard%20weak%0Aagnostic%20learning%2C%20by%20showing%20that%20there%20is%20no%20way%20to%20post-process%20a%0Amultiaccurate%20predictor%20to%20get%20a%20weak%20learner%2C%20even%20assuming%20the%20best%0Ahypothesis%20has%20correlation%20%241/2%24.%20Rather%2C%20we%20show%20that%20it%20yields%20a%20restricted%0Aform%20of%20weak%20agnostic%20learning%2C%20which%20requires%20some%20concept%20in%20the%20class%20to%0Ahave%20correlation%20greater%20than%20%241/2%24%20with%20the%20labels.%20However%2C%20by%20also%20requiring%0Athe%20predictor%20to%20be%20calibrated%2C%20we%20recover%20not%20just%20weak%2C%20but%20strong%20agnostic%0Alearning.%0A%20%20A%20similar%20picture%20emerges%20when%20we%20consider%20the%20derivation%20of%20hardcore%0Ameasures%20from%20predictors%20satisfying%20multigroup%20fairness%20notions.%20On%20the%20one%0Ahand%2C%20while%20multiaccuracy%20only%20yields%20hardcore%20measures%20of%20density%20half%20the%0Aoptimal%2C%20we%20show%20that%20%28a%20weighted%20version%20of%29%20calibrated%20multiaccuracy%20achieves%0Aoptimal%20density.%0A%20%20Our%20results%20yield%20new%20insights%20into%20the%20complementary%20roles%20played%20by%0Amultiaccuracy%20and%20calibration%20in%20each%20setting.%20They%20shed%20light%20on%20why%0Amultiaccuracy%20and%20global%20calibration%2C%20although%20not%20particularly%20powerful%20by%0Athemselves%2C%20together%20yield%20considerably%20stronger%20notions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Global%2520Calibration%2520Strengthens%2520Multiaccuracy%26entry.906535625%3DS%25C3%25ADlvia%2520Casacuberta%2520and%2520Parikshit%2520Gopalan%2520and%2520Varun%2520Kanade%2520and%2520Omer%2520Reingold%26entry.1292438233%3D%2520%2520Multiaccuracy%2520and%2520multicalibration%2520are%2520multigroup%2520fairness%2520notions%2520for%250Aprediction%2520that%2520have%2520found%2520numerous%2520applications%2520in%2520learning%2520and%2520computational%250Acomplexity.%2520They%2520can%2520be%2520achieved%2520from%2520a%2520single%2520learning%2520primitive%253A%2520weak%250Aagnostic%2520learning.%2520Here%2520we%2520investigate%2520the%2520power%2520of%2520multiaccuracy%2520as%2520a%2520learning%250Aprimitive%252C%2520both%2520with%2520and%2520without%2520the%2520additional%2520assumption%2520of%2520calibration.%2520We%250Afind%2520that%2520multiaccuracy%2520in%2520itself%2520is%2520rather%2520weak%252C%2520but%2520that%2520the%2520addition%2520of%250Aglobal%2520calibration%2520%2528this%2520notion%2520is%2520called%2520calibrated%2520multiaccuracy%2529%2520boosts%2520its%250Apower%2520substantially%252C%2520enough%2520to%2520recover%2520implications%2520that%2520were%2520previously%2520known%250Aonly%2520assuming%2520the%2520stronger%2520notion%2520of%2520multicalibration.%250A%2520%2520We%2520give%2520evidence%2520that%2520multiaccuracy%2520might%2520not%2520be%2520as%2520powerful%2520as%2520standard%2520weak%250Aagnostic%2520learning%252C%2520by%2520showing%2520that%2520there%2520is%2520no%2520way%2520to%2520post-process%2520a%250Amultiaccurate%2520predictor%2520to%2520get%2520a%2520weak%2520learner%252C%2520even%2520assuming%2520the%2520best%250Ahypothesis%2520has%2520correlation%2520%25241/2%2524.%2520Rather%252C%2520we%2520show%2520that%2520it%2520yields%2520a%2520restricted%250Aform%2520of%2520weak%2520agnostic%2520learning%252C%2520which%2520requires%2520some%2520concept%2520in%2520the%2520class%2520to%250Ahave%2520correlation%2520greater%2520than%2520%25241/2%2524%2520with%2520the%2520labels.%2520However%252C%2520by%2520also%2520requiring%250Athe%2520predictor%2520to%2520be%2520calibrated%252C%2520we%2520recover%2520not%2520just%2520weak%252C%2520but%2520strong%2520agnostic%250Alearning.%250A%2520%2520A%2520similar%2520picture%2520emerges%2520when%2520we%2520consider%2520the%2520derivation%2520of%2520hardcore%250Ameasures%2520from%2520predictors%2520satisfying%2520multigroup%2520fairness%2520notions.%2520On%2520the%2520one%250Ahand%252C%2520while%2520multiaccuracy%2520only%2520yields%2520hardcore%2520measures%2520of%2520density%2520half%2520the%250Aoptimal%252C%2520we%2520show%2520that%2520%2528a%2520weighted%2520version%2520of%2529%2520calibrated%2520multiaccuracy%2520achieves%250Aoptimal%2520density.%250A%2520%2520Our%2520results%2520yield%2520new%2520insights%2520into%2520the%2520complementary%2520roles%2520played%2520by%250Amultiaccuracy%2520and%2520calibration%2520in%2520each%2520setting.%2520They%2520shed%2520light%2520on%2520why%250Amultiaccuracy%2520and%2520global%2520calibration%252C%2520although%2520not%2520particularly%2520powerful%2520by%250Athemselves%252C%2520together%2520yield%2520considerably%2520stronger%2520notions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Global%20Calibration%20Strengthens%20Multiaccuracy&entry.906535625=S%C3%ADlvia%20Casacuberta%20and%20Parikshit%20Gopalan%20and%20Varun%20Kanade%20and%20Omer%20Reingold&entry.1292438233=%20%20Multiaccuracy%20and%20multicalibration%20are%20multigroup%20fairness%20notions%20for%0Aprediction%20that%20have%20found%20numerous%20applications%20in%20learning%20and%20computational%0Acomplexity.%20They%20can%20be%20achieved%20from%20a%20single%20learning%20primitive%3A%20weak%0Aagnostic%20learning.%20Here%20we%20investigate%20the%20power%20of%20multiaccuracy%20as%20a%20learning%0Aprimitive%2C%20both%20with%20and%20without%20the%20additional%20assumption%20of%20calibration.%20We%0Afind%20that%20multiaccuracy%20in%20itself%20is%20rather%20weak%2C%20but%20that%20the%20addition%20of%0Aglobal%20calibration%20%28this%20notion%20is%20called%20calibrated%20multiaccuracy%29%20boosts%20its%0Apower%20substantially%2C%20enough%20to%20recover%20implications%20that%20were%20previously%20known%0Aonly%20assuming%20the%20stronger%20notion%20of%20multicalibration.%0A%20%20We%20give%20evidence%20that%20multiaccuracy%20might%20not%20be%20as%20powerful%20as%20standard%20weak%0Aagnostic%20learning%2C%20by%20showing%20that%20there%20is%20no%20way%20to%20post-process%20a%0Amultiaccurate%20predictor%20to%20get%20a%20weak%20learner%2C%20even%20assuming%20the%20best%0Ahypothesis%20has%20correlation%20%241/2%24.%20Rather%2C%20we%20show%20that%20it%20yields%20a%20restricted%0Aform%20of%20weak%20agnostic%20learning%2C%20which%20requires%20some%20concept%20in%20the%20class%20to%0Ahave%20correlation%20greater%20than%20%241/2%24%20with%20the%20labels.%20However%2C%20by%20also%20requiring%0Athe%20predictor%20to%20be%20calibrated%2C%20we%20recover%20not%20just%20weak%2C%20but%20strong%20agnostic%0Alearning.%0A%20%20A%20similar%20picture%20emerges%20when%20we%20consider%20the%20derivation%20of%20hardcore%0Ameasures%20from%20predictors%20satisfying%20multigroup%20fairness%20notions.%20On%20the%20one%0Ahand%2C%20while%20multiaccuracy%20only%20yields%20hardcore%20measures%20of%20density%20half%20the%0Aoptimal%2C%20we%20show%20that%20%28a%20weighted%20version%20of%29%20calibrated%20multiaccuracy%20achieves%0Aoptimal%20density.%0A%20%20Our%20results%20yield%20new%20insights%20into%20the%20complementary%20roles%20played%20by%0Amultiaccuracy%20and%20calibration%20in%20each%20setting.%20They%20shed%20light%20on%20why%0Amultiaccuracy%20and%20global%20calibration%2C%20although%20not%20particularly%20powerful%20by%0Athemselves%2C%20together%20yield%20considerably%20stronger%20notions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15206v1&entry.124074799=Read"},
{"title": "TinyML NLP Scheme for Semantic Wireless Sentiment Classification with\n  Privacy Preservation", "author": "Ahmed Y. Radwan and Mohammad Shehab and Mohamed-Slim Alouini", "abstract": "  Natural Language Processing (NLP) operations, such as semantic sentiment\nanalysis and text synthesis, often raise privacy concerns and demand\nsignificant on-device computational resources. Centralized learning (CL) on the\nedge provides an energy-efficient alternative but requires collecting raw data,\ncompromising user privacy. While federated learning (FL) enhances privacy, it\nimposes high computational energy demands on resource-constrained devices. This\nstudy provides insights into deploying privacy-preserving, energy-efficient NLP\nmodels on edge devices. We introduce semantic split learning (SL) as an\nenergy-efficient, privacy-preserving tiny machine learning (TinyML) framework\nand compare it to FL and CL in the presence of Rayleigh fading and additive\nnoise. Our results show that SL significantly reduces computational power and\nCO2 emissions while enhancing privacy, as evidenced by a fourfold increase in\nreconstruction error compared to FL and nearly eighteen times that of CL. In\ncontrast, FL offers a balanced trade-off between privacy and efficiency. Our\ncode is available for replication at our GitHub repository:\nhttps://github.com/AhmedRadwan02/TinyEco2AI-NLP.\n", "link": "http://arxiv.org/abs/2411.06291v3", "date": "2025-04-21", "relevancy": 0.8938, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4621}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4396}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TinyML%20NLP%20Scheme%20for%20Semantic%20Wireless%20Sentiment%20Classification%20with%0A%20%20Privacy%20Preservation&body=Title%3A%20TinyML%20NLP%20Scheme%20for%20Semantic%20Wireless%20Sentiment%20Classification%20with%0A%20%20Privacy%20Preservation%0AAuthor%3A%20Ahmed%20Y.%20Radwan%20and%20Mohammad%20Shehab%20and%20Mohamed-Slim%20Alouini%0AAbstract%3A%20%20%20Natural%20Language%20Processing%20%28NLP%29%20operations%2C%20such%20as%20semantic%20sentiment%0Aanalysis%20and%20text%20synthesis%2C%20often%20raise%20privacy%20concerns%20and%20demand%0Asignificant%20on-device%20computational%20resources.%20Centralized%20learning%20%28CL%29%20on%20the%0Aedge%20provides%20an%20energy-efficient%20alternative%20but%20requires%20collecting%20raw%20data%2C%0Acompromising%20user%20privacy.%20While%20federated%20learning%20%28FL%29%20enhances%20privacy%2C%20it%0Aimposes%20high%20computational%20energy%20demands%20on%20resource-constrained%20devices.%20This%0Astudy%20provides%20insights%20into%20deploying%20privacy-preserving%2C%20energy-efficient%20NLP%0Amodels%20on%20edge%20devices.%20We%20introduce%20semantic%20split%20learning%20%28SL%29%20as%20an%0Aenergy-efficient%2C%20privacy-preserving%20tiny%20machine%20learning%20%28TinyML%29%20framework%0Aand%20compare%20it%20to%20FL%20and%20CL%20in%20the%20presence%20of%20Rayleigh%20fading%20and%20additive%0Anoise.%20Our%20results%20show%20that%20SL%20significantly%20reduces%20computational%20power%20and%0ACO2%20emissions%20while%20enhancing%20privacy%2C%20as%20evidenced%20by%20a%20fourfold%20increase%20in%0Areconstruction%20error%20compared%20to%20FL%20and%20nearly%20eighteen%20times%20that%20of%20CL.%20In%0Acontrast%2C%20FL%20offers%20a%20balanced%20trade-off%20between%20privacy%20and%20efficiency.%20Our%0Acode%20is%20available%20for%20replication%20at%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/AhmedRadwan02/TinyEco2AI-NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.06291v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTinyML%2520NLP%2520Scheme%2520for%2520Semantic%2520Wireless%2520Sentiment%2520Classification%2520with%250A%2520%2520Privacy%2520Preservation%26entry.906535625%3DAhmed%2520Y.%2520Radwan%2520and%2520Mohammad%2520Shehab%2520and%2520Mohamed-Slim%2520Alouini%26entry.1292438233%3D%2520%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520operations%252C%2520such%2520as%2520semantic%2520sentiment%250Aanalysis%2520and%2520text%2520synthesis%252C%2520often%2520raise%2520privacy%2520concerns%2520and%2520demand%250Asignificant%2520on-device%2520computational%2520resources.%2520Centralized%2520learning%2520%2528CL%2529%2520on%2520the%250Aedge%2520provides%2520an%2520energy-efficient%2520alternative%2520but%2520requires%2520collecting%2520raw%2520data%252C%250Acompromising%2520user%2520privacy.%2520While%2520federated%2520learning%2520%2528FL%2529%2520enhances%2520privacy%252C%2520it%250Aimposes%2520high%2520computational%2520energy%2520demands%2520on%2520resource-constrained%2520devices.%2520This%250Astudy%2520provides%2520insights%2520into%2520deploying%2520privacy-preserving%252C%2520energy-efficient%2520NLP%250Amodels%2520on%2520edge%2520devices.%2520We%2520introduce%2520semantic%2520split%2520learning%2520%2528SL%2529%2520as%2520an%250Aenergy-efficient%252C%2520privacy-preserving%2520tiny%2520machine%2520learning%2520%2528TinyML%2529%2520framework%250Aand%2520compare%2520it%2520to%2520FL%2520and%2520CL%2520in%2520the%2520presence%2520of%2520Rayleigh%2520fading%2520and%2520additive%250Anoise.%2520Our%2520results%2520show%2520that%2520SL%2520significantly%2520reduces%2520computational%2520power%2520and%250ACO2%2520emissions%2520while%2520enhancing%2520privacy%252C%2520as%2520evidenced%2520by%2520a%2520fourfold%2520increase%2520in%250Areconstruction%2520error%2520compared%2520to%2520FL%2520and%2520nearly%2520eighteen%2520times%2520that%2520of%2520CL.%2520In%250Acontrast%252C%2520FL%2520offers%2520a%2520balanced%2520trade-off%2520between%2520privacy%2520and%2520efficiency.%2520Our%250Acode%2520is%2520available%2520for%2520replication%2520at%2520our%2520GitHub%2520repository%253A%250Ahttps%253A//github.com/AhmedRadwan02/TinyEco2AI-NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.06291v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TinyML%20NLP%20Scheme%20for%20Semantic%20Wireless%20Sentiment%20Classification%20with%0A%20%20Privacy%20Preservation&entry.906535625=Ahmed%20Y.%20Radwan%20and%20Mohammad%20Shehab%20and%20Mohamed-Slim%20Alouini&entry.1292438233=%20%20Natural%20Language%20Processing%20%28NLP%29%20operations%2C%20such%20as%20semantic%20sentiment%0Aanalysis%20and%20text%20synthesis%2C%20often%20raise%20privacy%20concerns%20and%20demand%0Asignificant%20on-device%20computational%20resources.%20Centralized%20learning%20%28CL%29%20on%20the%0Aedge%20provides%20an%20energy-efficient%20alternative%20but%20requires%20collecting%20raw%20data%2C%0Acompromising%20user%20privacy.%20While%20federated%20learning%20%28FL%29%20enhances%20privacy%2C%20it%0Aimposes%20high%20computational%20energy%20demands%20on%20resource-constrained%20devices.%20This%0Astudy%20provides%20insights%20into%20deploying%20privacy-preserving%2C%20energy-efficient%20NLP%0Amodels%20on%20edge%20devices.%20We%20introduce%20semantic%20split%20learning%20%28SL%29%20as%20an%0Aenergy-efficient%2C%20privacy-preserving%20tiny%20machine%20learning%20%28TinyML%29%20framework%0Aand%20compare%20it%20to%20FL%20and%20CL%20in%20the%20presence%20of%20Rayleigh%20fading%20and%20additive%0Anoise.%20Our%20results%20show%20that%20SL%20significantly%20reduces%20computational%20power%20and%0ACO2%20emissions%20while%20enhancing%20privacy%2C%20as%20evidenced%20by%20a%20fourfold%20increase%20in%0Areconstruction%20error%20compared%20to%20FL%20and%20nearly%20eighteen%20times%20that%20of%20CL.%20In%0Acontrast%2C%20FL%20offers%20a%20balanced%20trade-off%20between%20privacy%20and%20efficiency.%20Our%0Acode%20is%20available%20for%20replication%20at%20our%20GitHub%20repository%3A%0Ahttps%3A//github.com/AhmedRadwan02/TinyEco2AI-NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.06291v3&entry.124074799=Read"},
{"title": "SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam", "author": "Tue Vo and Lakshay Sharma and Tuan Dinh and Khuong Dinh and Trang Nguyen and Trung Phan and Minh Do and Duong Vu", "abstract": "  Understanding and monitoring aquatic biodiversity is critical for ecological\nhealth and conservation efforts. This paper proposes SuoiAI, an end-to-end\npipeline for building a dataset of aquatic invertebrates in Vietnam and\nemploying machine learning (ML) techniques for species classification. We\noutline the methods for data collection, annotation, and model training,\nfocusing on reducing annotation effort through semi-supervised learning and\nleveraging state-of-the-art object detection and classification models. Our\napproach aims to overcome challenges such as data scarcity, fine-grained\nclassification, and deployment in diverse environmental conditions.\n", "link": "http://arxiv.org/abs/2504.15252v1", "date": "2025-04-21", "relevancy": 1.7175, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4397}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuoiAI%3A%20Building%20a%20Dataset%20for%20Aquatic%20Invertebrates%20in%20Vietnam&body=Title%3A%20SuoiAI%3A%20Building%20a%20Dataset%20for%20Aquatic%20Invertebrates%20in%20Vietnam%0AAuthor%3A%20Tue%20Vo%20and%20Lakshay%20Sharma%20and%20Tuan%20Dinh%20and%20Khuong%20Dinh%20and%20Trang%20Nguyen%20and%20Trung%20Phan%20and%20Minh%20Do%20and%20Duong%20Vu%0AAbstract%3A%20%20%20Understanding%20and%20monitoring%20aquatic%20biodiversity%20is%20critical%20for%20ecological%0Ahealth%20and%20conservation%20efforts.%20This%20paper%20proposes%20SuoiAI%2C%20an%20end-to-end%0Apipeline%20for%20building%20a%20dataset%20of%20aquatic%20invertebrates%20in%20Vietnam%20and%0Aemploying%20machine%20learning%20%28ML%29%20techniques%20for%20species%20classification.%20We%0Aoutline%20the%20methods%20for%20data%20collection%2C%20annotation%2C%20and%20model%20training%2C%0Afocusing%20on%20reducing%20annotation%20effort%20through%20semi-supervised%20learning%20and%0Aleveraging%20state-of-the-art%20object%20detection%20and%20classification%20models.%20Our%0Aapproach%20aims%20to%20overcome%20challenges%20such%20as%20data%20scarcity%2C%20fine-grained%0Aclassification%2C%20and%20deployment%20in%20diverse%20environmental%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuoiAI%253A%2520Building%2520a%2520Dataset%2520for%2520Aquatic%2520Invertebrates%2520in%2520Vietnam%26entry.906535625%3DTue%2520Vo%2520and%2520Lakshay%2520Sharma%2520and%2520Tuan%2520Dinh%2520and%2520Khuong%2520Dinh%2520and%2520Trang%2520Nguyen%2520and%2520Trung%2520Phan%2520and%2520Minh%2520Do%2520and%2520Duong%2520Vu%26entry.1292438233%3D%2520%2520Understanding%2520and%2520monitoring%2520aquatic%2520biodiversity%2520is%2520critical%2520for%2520ecological%250Ahealth%2520and%2520conservation%2520efforts.%2520This%2520paper%2520proposes%2520SuoiAI%252C%2520an%2520end-to-end%250Apipeline%2520for%2520building%2520a%2520dataset%2520of%2520aquatic%2520invertebrates%2520in%2520Vietnam%2520and%250Aemploying%2520machine%2520learning%2520%2528ML%2529%2520techniques%2520for%2520species%2520classification.%2520We%250Aoutline%2520the%2520methods%2520for%2520data%2520collection%252C%2520annotation%252C%2520and%2520model%2520training%252C%250Afocusing%2520on%2520reducing%2520annotation%2520effort%2520through%2520semi-supervised%2520learning%2520and%250Aleveraging%2520state-of-the-art%2520object%2520detection%2520and%2520classification%2520models.%2520Our%250Aapproach%2520aims%2520to%2520overcome%2520challenges%2520such%2520as%2520data%2520scarcity%252C%2520fine-grained%250Aclassification%252C%2520and%2520deployment%2520in%2520diverse%2520environmental%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuoiAI%3A%20Building%20a%20Dataset%20for%20Aquatic%20Invertebrates%20in%20Vietnam&entry.906535625=Tue%20Vo%20and%20Lakshay%20Sharma%20and%20Tuan%20Dinh%20and%20Khuong%20Dinh%20and%20Trang%20Nguyen%20and%20Trung%20Phan%20and%20Minh%20Do%20and%20Duong%20Vu&entry.1292438233=%20%20Understanding%20and%20monitoring%20aquatic%20biodiversity%20is%20critical%20for%20ecological%0Ahealth%20and%20conservation%20efforts.%20This%20paper%20proposes%20SuoiAI%2C%20an%20end-to-end%0Apipeline%20for%20building%20a%20dataset%20of%20aquatic%20invertebrates%20in%20Vietnam%20and%0Aemploying%20machine%20learning%20%28ML%29%20techniques%20for%20species%20classification.%20We%0Aoutline%20the%20methods%20for%20data%20collection%2C%20annotation%2C%20and%20model%20training%2C%0Afocusing%20on%20reducing%20annotation%20effort%20through%20semi-supervised%20learning%20and%0Aleveraging%20state-of-the-art%20object%20detection%20and%20classification%20models.%20Our%0Aapproach%20aims%20to%20overcome%20challenges%20such%20as%20data%20scarcity%2C%20fine-grained%0Aclassification%2C%20and%20deployment%20in%20diverse%20environmental%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15252v1&entry.124074799=Read"},
{"title": "Faster Algorithms for Agnostically Learning Disjunctions and their\n  Implications", "author": "Ilias Diakonikolas and Daniel M. Kane and Lisheng Ren", "abstract": "  We study the algorithmic task of learning Boolean disjunctions in the\ndistribution-free agnostic PAC model. The best known agnostic learner for the\nclass of disjunctions over $\\{0, 1\\}^n$ is the $L_1$-polynomial regression\nalgorithm, achieving complexity $2^{\\tilde{O}(n^{1/2})}$. This complexity bound\nis known to be nearly best possible within the class of Correlational\nStatistical Query (CSQ) algorithms. In this work, we develop an agnostic\nlearner for this concept class with complexity $2^{\\tilde{O}(n^{1/3})}$. Our\nalgorithm can be implemented in the Statistical Query (SQ) model, providing the\nfirst separation between the SQ and CSQ models in distribution-free agnostic\nlearning.\n", "link": "http://arxiv.org/abs/2504.15244v1", "date": "2025-04-21", "relevancy": 1.2806, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4398}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4194}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Algorithms%20for%20Agnostically%20Learning%20Disjunctions%20and%20their%0A%20%20Implications&body=Title%3A%20Faster%20Algorithms%20for%20Agnostically%20Learning%20Disjunctions%20and%20their%0A%20%20Implications%0AAuthor%3A%20Ilias%20Diakonikolas%20and%20Daniel%20M.%20Kane%20and%20Lisheng%20Ren%0AAbstract%3A%20%20%20We%20study%20the%20algorithmic%20task%20of%20learning%20Boolean%20disjunctions%20in%20the%0Adistribution-free%20agnostic%20PAC%20model.%20The%20best%20known%20agnostic%20learner%20for%20the%0Aclass%20of%20disjunctions%20over%20%24%5C%7B0%2C%201%5C%7D%5En%24%20is%20the%20%24L_1%24-polynomial%20regression%0Aalgorithm%2C%20achieving%20complexity%20%242%5E%7B%5Ctilde%7BO%7D%28n%5E%7B1/2%7D%29%7D%24.%20This%20complexity%20bound%0Ais%20known%20to%20be%20nearly%20best%20possible%20within%20the%20class%20of%20Correlational%0AStatistical%20Query%20%28CSQ%29%20algorithms.%20In%20this%20work%2C%20we%20develop%20an%20agnostic%0Alearner%20for%20this%20concept%20class%20with%20complexity%20%242%5E%7B%5Ctilde%7BO%7D%28n%5E%7B1/3%7D%29%7D%24.%20Our%0Aalgorithm%20can%20be%20implemented%20in%20the%20Statistical%20Query%20%28SQ%29%20model%2C%20providing%20the%0Afirst%20separation%20between%20the%20SQ%20and%20CSQ%20models%20in%20distribution-free%20agnostic%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.15244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Algorithms%2520for%2520Agnostically%2520Learning%2520Disjunctions%2520and%2520their%250A%2520%2520Implications%26entry.906535625%3DIlias%2520Diakonikolas%2520and%2520Daniel%2520M.%2520Kane%2520and%2520Lisheng%2520Ren%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520algorithmic%2520task%2520of%2520learning%2520Boolean%2520disjunctions%2520in%2520the%250Adistribution-free%2520agnostic%2520PAC%2520model.%2520The%2520best%2520known%2520agnostic%2520learner%2520for%2520the%250Aclass%2520of%2520disjunctions%2520over%2520%2524%255C%257B0%252C%25201%255C%257D%255En%2524%2520is%2520the%2520%2524L_1%2524-polynomial%2520regression%250Aalgorithm%252C%2520achieving%2520complexity%2520%25242%255E%257B%255Ctilde%257BO%257D%2528n%255E%257B1/2%257D%2529%257D%2524.%2520This%2520complexity%2520bound%250Ais%2520known%2520to%2520be%2520nearly%2520best%2520possible%2520within%2520the%2520class%2520of%2520Correlational%250AStatistical%2520Query%2520%2528CSQ%2529%2520algorithms.%2520In%2520this%2520work%252C%2520we%2520develop%2520an%2520agnostic%250Alearner%2520for%2520this%2520concept%2520class%2520with%2520complexity%2520%25242%255E%257B%255Ctilde%257BO%257D%2528n%255E%257B1/3%257D%2529%257D%2524.%2520Our%250Aalgorithm%2520can%2520be%2520implemented%2520in%2520the%2520Statistical%2520Query%2520%2528SQ%2529%2520model%252C%2520providing%2520the%250Afirst%2520separation%2520between%2520the%2520SQ%2520and%2520CSQ%2520models%2520in%2520distribution-free%2520agnostic%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.15244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Algorithms%20for%20Agnostically%20Learning%20Disjunctions%20and%20their%0A%20%20Implications&entry.906535625=Ilias%20Diakonikolas%20and%20Daniel%20M.%20Kane%20and%20Lisheng%20Ren&entry.1292438233=%20%20We%20study%20the%20algorithmic%20task%20of%20learning%20Boolean%20disjunctions%20in%20the%0Adistribution-free%20agnostic%20PAC%20model.%20The%20best%20known%20agnostic%20learner%20for%20the%0Aclass%20of%20disjunctions%20over%20%24%5C%7B0%2C%201%5C%7D%5En%24%20is%20the%20%24L_1%24-polynomial%20regression%0Aalgorithm%2C%20achieving%20complexity%20%242%5E%7B%5Ctilde%7BO%7D%28n%5E%7B1/2%7D%29%7D%24.%20This%20complexity%20bound%0Ais%20known%20to%20be%20nearly%20best%20possible%20within%20the%20class%20of%20Correlational%0AStatistical%20Query%20%28CSQ%29%20algorithms.%20In%20this%20work%2C%20we%20develop%20an%20agnostic%0Alearner%20for%20this%20concept%20class%20with%20complexity%20%242%5E%7B%5Ctilde%7BO%7D%28n%5E%7B1/3%7D%29%7D%24.%20Our%0Aalgorithm%20can%20be%20implemented%20in%20the%20Statistical%20Query%20%28SQ%29%20model%2C%20providing%20the%0Afirst%20separation%20between%20the%20SQ%20and%20CSQ%20models%20in%20distribution-free%20agnostic%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.15244v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


