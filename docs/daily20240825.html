<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240822.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Segment anything model 2: an application to 2D and 3D medical images", "author": "Haoyu Dong and Hanxue Gu and Yaqian Chen and Jichen Yang and Yuwen Chen and Maciej A. Mazurowski", "abstract": "  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment various objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we extensively evaluate SAM 2's ability\nto segment both 2D and 3D medical images by first collecting 21 medical imaging\ndatasets, including surgical videos, common 3D modalities such as computed\ntomography (CT), magnetic resonance imaging (MRI), and positron emission\ntomography (PET) as well as 2D modalities such as X-ray and ultrasound. Two\nevaluation settings of SAM 2 are considered: (1) multi-frame 3D segmentation,\nwhere prompts are provided to one or multiple slice(s) selected from the\nvolume, and (2) single-frame 2D segmentation, where prompts are provided to\neach slice. The former only applies to videos and 3D modalities, while the\nlatter applies to all datasets. Our results show that SAM 2 exhibits similar\nperformance as SAM under single-frame 2D segmentation, and has variable\nperformance under multi-frame 3D segmentation depending on the choices of\nslices to annotate, the direction of the propagation, the predictions utilized\nduring the propagation, etc. We believe our work enhances the understanding of\nSAM 2's behavior in the medical field and provides directions for future work\nin adapting SAM 2 to this domain. Our code is available at:\nhttps://github.com/mazurowski-lab/segment-anything2-medical-evaluation.\n", "link": "http://arxiv.org/abs/2408.00756v3", "date": "2024-08-22", "relevancy": 2.9778, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5963}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5963}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20anything%20model%202%3A%20an%20application%20to%202D%20and%203D%20medical%20images&body=Title%3A%20Segment%20anything%20model%202%3A%20an%20application%20to%202D%20and%203D%20medical%20images%0AAuthor%3A%20Haoyu%20Dong%20and%20Hanxue%20Gu%20and%20Yaqian%20Chen%20and%20Jichen%20Yang%20and%20Yuwen%20Chen%20and%20Maciej%20A.%20Mazurowski%0AAbstract%3A%20%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20gained%20significant%20attention%20because%20of%20its%0Aability%20to%20segment%20various%20objects%20in%20images%20given%20a%20prompt.%20The%20recently%0Adeveloped%20SAM%202%20has%20extended%20this%20ability%20to%20video%20inputs.%20This%20opens%20an%0Aopportunity%20to%20apply%20SAM%20to%203D%20images%2C%20one%20of%20the%20fundamental%20tasks%20in%20the%0Amedical%20imaging%20field.%20In%20this%20paper%2C%20we%20extensively%20evaluate%20SAM%202%27s%20ability%0Ato%20segment%20both%202D%20and%203D%20medical%20images%20by%20first%20collecting%2021%20medical%20imaging%0Adatasets%2C%20including%20surgical%20videos%2C%20common%203D%20modalities%20such%20as%20computed%0Atomography%20%28CT%29%2C%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20positron%20emission%0Atomography%20%28PET%29%20as%20well%20as%202D%20modalities%20such%20as%20X-ray%20and%20ultrasound.%20Two%0Aevaluation%20settings%20of%20SAM%202%20are%20considered%3A%20%281%29%20multi-frame%203D%20segmentation%2C%0Awhere%20prompts%20are%20provided%20to%20one%20or%20multiple%20slice%28s%29%20selected%20from%20the%0Avolume%2C%20and%20%282%29%20single-frame%202D%20segmentation%2C%20where%20prompts%20are%20provided%20to%0Aeach%20slice.%20The%20former%20only%20applies%20to%20videos%20and%203D%20modalities%2C%20while%20the%0Alatter%20applies%20to%20all%20datasets.%20Our%20results%20show%20that%20SAM%202%20exhibits%20similar%0Aperformance%20as%20SAM%20under%20single-frame%202D%20segmentation%2C%20and%20has%20variable%0Aperformance%20under%20multi-frame%203D%20segmentation%20depending%20on%20the%20choices%20of%0Aslices%20to%20annotate%2C%20the%20direction%20of%20the%20propagation%2C%20the%20predictions%20utilized%0Aduring%20the%20propagation%2C%20etc.%20We%20believe%20our%20work%20enhances%20the%20understanding%20of%0ASAM%202%27s%20behavior%20in%20the%20medical%20field%20and%20provides%20directions%20for%20future%20work%0Ain%20adapting%20SAM%202%20to%20this%20domain.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/mazurowski-lab/segment-anything2-medical-evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.00756v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520anything%2520model%25202%253A%2520an%2520application%2520to%25202D%2520and%25203D%2520medical%2520images%26entry.906535625%3DHaoyu%2520Dong%2520and%2520Hanxue%2520Gu%2520and%2520Yaqian%2520Chen%2520and%2520Jichen%2520Yang%2520and%2520Yuwen%2520Chen%2520and%2520Maciej%2520A.%2520Mazurowski%26entry.1292438233%3D%2520%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%2520gained%2520significant%2520attention%2520because%2520of%2520its%250Aability%2520to%2520segment%2520various%2520objects%2520in%2520images%2520given%2520a%2520prompt.%2520The%2520recently%250Adeveloped%2520SAM%25202%2520has%2520extended%2520this%2520ability%2520to%2520video%2520inputs.%2520This%2520opens%2520an%250Aopportunity%2520to%2520apply%2520SAM%2520to%25203D%2520images%252C%2520one%2520of%2520the%2520fundamental%2520tasks%2520in%2520the%250Amedical%2520imaging%2520field.%2520In%2520this%2520paper%252C%2520we%2520extensively%2520evaluate%2520SAM%25202%2527s%2520ability%250Ato%2520segment%2520both%25202D%2520and%25203D%2520medical%2520images%2520by%2520first%2520collecting%252021%2520medical%2520imaging%250Adatasets%252C%2520including%2520surgical%2520videos%252C%2520common%25203D%2520modalities%2520such%2520as%2520computed%250Atomography%2520%2528CT%2529%252C%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%252C%2520and%2520positron%2520emission%250Atomography%2520%2528PET%2529%2520as%2520well%2520as%25202D%2520modalities%2520such%2520as%2520X-ray%2520and%2520ultrasound.%2520Two%250Aevaluation%2520settings%2520of%2520SAM%25202%2520are%2520considered%253A%2520%25281%2529%2520multi-frame%25203D%2520segmentation%252C%250Awhere%2520prompts%2520are%2520provided%2520to%2520one%2520or%2520multiple%2520slice%2528s%2529%2520selected%2520from%2520the%250Avolume%252C%2520and%2520%25282%2529%2520single-frame%25202D%2520segmentation%252C%2520where%2520prompts%2520are%2520provided%2520to%250Aeach%2520slice.%2520The%2520former%2520only%2520applies%2520to%2520videos%2520and%25203D%2520modalities%252C%2520while%2520the%250Alatter%2520applies%2520to%2520all%2520datasets.%2520Our%2520results%2520show%2520that%2520SAM%25202%2520exhibits%2520similar%250Aperformance%2520as%2520SAM%2520under%2520single-frame%25202D%2520segmentation%252C%2520and%2520has%2520variable%250Aperformance%2520under%2520multi-frame%25203D%2520segmentation%2520depending%2520on%2520the%2520choices%2520of%250Aslices%2520to%2520annotate%252C%2520the%2520direction%2520of%2520the%2520propagation%252C%2520the%2520predictions%2520utilized%250Aduring%2520the%2520propagation%252C%2520etc.%2520We%2520believe%2520our%2520work%2520enhances%2520the%2520understanding%2520of%250ASAM%25202%2527s%2520behavior%2520in%2520the%2520medical%2520field%2520and%2520provides%2520directions%2520for%2520future%2520work%250Ain%2520adapting%2520SAM%25202%2520to%2520this%2520domain.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/mazurowski-lab/segment-anything2-medical-evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.00756v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20anything%20model%202%3A%20an%20application%20to%202D%20and%203D%20medical%20images&entry.906535625=Haoyu%20Dong%20and%20Hanxue%20Gu%20and%20Yaqian%20Chen%20and%20Jichen%20Yang%20and%20Yuwen%20Chen%20and%20Maciej%20A.%20Mazurowski&entry.1292438233=%20%20Segment%20Anything%20Model%20%28SAM%29%20has%20gained%20significant%20attention%20because%20of%20its%0Aability%20to%20segment%20various%20objects%20in%20images%20given%20a%20prompt.%20The%20recently%0Adeveloped%20SAM%202%20has%20extended%20this%20ability%20to%20video%20inputs.%20This%20opens%20an%0Aopportunity%20to%20apply%20SAM%20to%203D%20images%2C%20one%20of%20the%20fundamental%20tasks%20in%20the%0Amedical%20imaging%20field.%20In%20this%20paper%2C%20we%20extensively%20evaluate%20SAM%202%27s%20ability%0Ato%20segment%20both%202D%20and%203D%20medical%20images%20by%20first%20collecting%2021%20medical%20imaging%0Adatasets%2C%20including%20surgical%20videos%2C%20common%203D%20modalities%20such%20as%20computed%0Atomography%20%28CT%29%2C%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20positron%20emission%0Atomography%20%28PET%29%20as%20well%20as%202D%20modalities%20such%20as%20X-ray%20and%20ultrasound.%20Two%0Aevaluation%20settings%20of%20SAM%202%20are%20considered%3A%20%281%29%20multi-frame%203D%20segmentation%2C%0Awhere%20prompts%20are%20provided%20to%20one%20or%20multiple%20slice%28s%29%20selected%20from%20the%0Avolume%2C%20and%20%282%29%20single-frame%202D%20segmentation%2C%20where%20prompts%20are%20provided%20to%0Aeach%20slice.%20The%20former%20only%20applies%20to%20videos%20and%203D%20modalities%2C%20while%20the%0Alatter%20applies%20to%20all%20datasets.%20Our%20results%20show%20that%20SAM%202%20exhibits%20similar%0Aperformance%20as%20SAM%20under%20single-frame%202D%20segmentation%2C%20and%20has%20variable%0Aperformance%20under%20multi-frame%203D%20segmentation%20depending%20on%20the%20choices%20of%0Aslices%20to%20annotate%2C%20the%20direction%20of%20the%20propagation%2C%20the%20predictions%20utilized%0Aduring%20the%20propagation%2C%20etc.%20We%20believe%20our%20work%20enhances%20the%20understanding%20of%0ASAM%202%27s%20behavior%20in%20the%20medical%20field%20and%20provides%20directions%20for%20future%20work%0Ain%20adapting%20SAM%202%20to%20this%20domain.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/mazurowski-lab/segment-anything2-medical-evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.00756v3&entry.124074799=Read"},
{"title": "DreamCinema: Cinematic Transfer with Free Camera and 3D Character", "author": "Weiliang Chen and Fangfu Liu and Diankun Wu and Haowen Sun and Haixu Song and Yueqi Duan", "abstract": "  We are living in a flourishing era of digital media, where everyone has the\npotential to become a personal filmmaker. Current research on cinematic\ntransfer empowers filmmakers to reproduce and manipulate the visual elements\n(e.g., cinematography and character behaviors) from classic shots. However,\ncharacters in the reimagined films still rely on manual crafting, which\ninvolves significant technical complexity and high costs, making it\nunattainable for ordinary users. Furthermore, their estimated cinematography\nlacks smoothness due to inadequate capturing of inter-frame motion and modeling\nof physical trajectories. Fortunately, the remarkable success of 2D and 3D AIGC\nhas opened up the possibility of efficiently generating characters tailored to\nusers' needs, diversifying cinematography. In this paper, we propose\nDreamCinema, a novel cinematic transfer framework that pioneers generative AI\ninto the film production paradigm, aiming at facilitating user-friendly film\ncreation. Specifically, we first extract cinematic elements (i.e., human and\ncamera pose) and optimize the camera trajectory. Then, we apply a character\ngenerator to efficiently create 3D high-quality characters with a human\nstructure prior. Finally, we develop a structure-guided motion transfer\nstrategy to incorporate generated characters into film creation and transfer it\nvia 3D graphics engines smoothly. Extensive experiments demonstrate the\neffectiveness of our method for creating high-quality films with free camera\nand 3D characters.\n", "link": "http://arxiv.org/abs/2408.12601v1", "date": "2024-08-22", "relevancy": 2.9504, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5915}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5915}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamCinema%3A%20Cinematic%20Transfer%20with%20Free%20Camera%20and%203D%20Character&body=Title%3A%20DreamCinema%3A%20Cinematic%20Transfer%20with%20Free%20Camera%20and%203D%20Character%0AAuthor%3A%20Weiliang%20Chen%20and%20Fangfu%20Liu%20and%20Diankun%20Wu%20and%20Haowen%20Sun%20and%20Haixu%20Song%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20We%20are%20living%20in%20a%20flourishing%20era%20of%20digital%20media%2C%20where%20everyone%20has%20the%0Apotential%20to%20become%20a%20personal%20filmmaker.%20Current%20research%20on%20cinematic%0Atransfer%20empowers%20filmmakers%20to%20reproduce%20and%20manipulate%20the%20visual%20elements%0A%28e.g.%2C%20cinematography%20and%20character%20behaviors%29%20from%20classic%20shots.%20However%2C%0Acharacters%20in%20the%20reimagined%20films%20still%20rely%20on%20manual%20crafting%2C%20which%0Ainvolves%20significant%20technical%20complexity%20and%20high%20costs%2C%20making%20it%0Aunattainable%20for%20ordinary%20users.%20Furthermore%2C%20their%20estimated%20cinematography%0Alacks%20smoothness%20due%20to%20inadequate%20capturing%20of%20inter-frame%20motion%20and%20modeling%0Aof%20physical%20trajectories.%20Fortunately%2C%20the%20remarkable%20success%20of%202D%20and%203D%20AIGC%0Ahas%20opened%20up%20the%20possibility%20of%20efficiently%20generating%20characters%20tailored%20to%0Ausers%27%20needs%2C%20diversifying%20cinematography.%20In%20this%20paper%2C%20we%20propose%0ADreamCinema%2C%20a%20novel%20cinematic%20transfer%20framework%20that%20pioneers%20generative%20AI%0Ainto%20the%20film%20production%20paradigm%2C%20aiming%20at%20facilitating%20user-friendly%20film%0Acreation.%20Specifically%2C%20we%20first%20extract%20cinematic%20elements%20%28i.e.%2C%20human%20and%0Acamera%20pose%29%20and%20optimize%20the%20camera%20trajectory.%20Then%2C%20we%20apply%20a%20character%0Agenerator%20to%20efficiently%20create%203D%20high-quality%20characters%20with%20a%20human%0Astructure%20prior.%20Finally%2C%20we%20develop%20a%20structure-guided%20motion%20transfer%0Astrategy%20to%20incorporate%20generated%20characters%20into%20film%20creation%20and%20transfer%20it%0Avia%203D%20graphics%20engines%20smoothly.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20for%20creating%20high-quality%20films%20with%20free%20camera%0Aand%203D%20characters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamCinema%253A%2520Cinematic%2520Transfer%2520with%2520Free%2520Camera%2520and%25203D%2520Character%26entry.906535625%3DWeiliang%2520Chen%2520and%2520Fangfu%2520Liu%2520and%2520Diankun%2520Wu%2520and%2520Haowen%2520Sun%2520and%2520Haixu%2520Song%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520We%2520are%2520living%2520in%2520a%2520flourishing%2520era%2520of%2520digital%2520media%252C%2520where%2520everyone%2520has%2520the%250Apotential%2520to%2520become%2520a%2520personal%2520filmmaker.%2520Current%2520research%2520on%2520cinematic%250Atransfer%2520empowers%2520filmmakers%2520to%2520reproduce%2520and%2520manipulate%2520the%2520visual%2520elements%250A%2528e.g.%252C%2520cinematography%2520and%2520character%2520behaviors%2529%2520from%2520classic%2520shots.%2520However%252C%250Acharacters%2520in%2520the%2520reimagined%2520films%2520still%2520rely%2520on%2520manual%2520crafting%252C%2520which%250Ainvolves%2520significant%2520technical%2520complexity%2520and%2520high%2520costs%252C%2520making%2520it%250Aunattainable%2520for%2520ordinary%2520users.%2520Furthermore%252C%2520their%2520estimated%2520cinematography%250Alacks%2520smoothness%2520due%2520to%2520inadequate%2520capturing%2520of%2520inter-frame%2520motion%2520and%2520modeling%250Aof%2520physical%2520trajectories.%2520Fortunately%252C%2520the%2520remarkable%2520success%2520of%25202D%2520and%25203D%2520AIGC%250Ahas%2520opened%2520up%2520the%2520possibility%2520of%2520efficiently%2520generating%2520characters%2520tailored%2520to%250Ausers%2527%2520needs%252C%2520diversifying%2520cinematography.%2520In%2520this%2520paper%252C%2520we%2520propose%250ADreamCinema%252C%2520a%2520novel%2520cinematic%2520transfer%2520framework%2520that%2520pioneers%2520generative%2520AI%250Ainto%2520the%2520film%2520production%2520paradigm%252C%2520aiming%2520at%2520facilitating%2520user-friendly%2520film%250Acreation.%2520Specifically%252C%2520we%2520first%2520extract%2520cinematic%2520elements%2520%2528i.e.%252C%2520human%2520and%250Acamera%2520pose%2529%2520and%2520optimize%2520the%2520camera%2520trajectory.%2520Then%252C%2520we%2520apply%2520a%2520character%250Agenerator%2520to%2520efficiently%2520create%25203D%2520high-quality%2520characters%2520with%2520a%2520human%250Astructure%2520prior.%2520Finally%252C%2520we%2520develop%2520a%2520structure-guided%2520motion%2520transfer%250Astrategy%2520to%2520incorporate%2520generated%2520characters%2520into%2520film%2520creation%2520and%2520transfer%2520it%250Avia%25203D%2520graphics%2520engines%2520smoothly.%2520Extensive%2520experiments%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520for%2520creating%2520high-quality%2520films%2520with%2520free%2520camera%250Aand%25203D%2520characters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamCinema%3A%20Cinematic%20Transfer%20with%20Free%20Camera%20and%203D%20Character&entry.906535625=Weiliang%20Chen%20and%20Fangfu%20Liu%20and%20Diankun%20Wu%20and%20Haowen%20Sun%20and%20Haixu%20Song%20and%20Yueqi%20Duan&entry.1292438233=%20%20We%20are%20living%20in%20a%20flourishing%20era%20of%20digital%20media%2C%20where%20everyone%20has%20the%0Apotential%20to%20become%20a%20personal%20filmmaker.%20Current%20research%20on%20cinematic%0Atransfer%20empowers%20filmmakers%20to%20reproduce%20and%20manipulate%20the%20visual%20elements%0A%28e.g.%2C%20cinematography%20and%20character%20behaviors%29%20from%20classic%20shots.%20However%2C%0Acharacters%20in%20the%20reimagined%20films%20still%20rely%20on%20manual%20crafting%2C%20which%0Ainvolves%20significant%20technical%20complexity%20and%20high%20costs%2C%20making%20it%0Aunattainable%20for%20ordinary%20users.%20Furthermore%2C%20their%20estimated%20cinematography%0Alacks%20smoothness%20due%20to%20inadequate%20capturing%20of%20inter-frame%20motion%20and%20modeling%0Aof%20physical%20trajectories.%20Fortunately%2C%20the%20remarkable%20success%20of%202D%20and%203D%20AIGC%0Ahas%20opened%20up%20the%20possibility%20of%20efficiently%20generating%20characters%20tailored%20to%0Ausers%27%20needs%2C%20diversifying%20cinematography.%20In%20this%20paper%2C%20we%20propose%0ADreamCinema%2C%20a%20novel%20cinematic%20transfer%20framework%20that%20pioneers%20generative%20AI%0Ainto%20the%20film%20production%20paradigm%2C%20aiming%20at%20facilitating%20user-friendly%20film%0Acreation.%20Specifically%2C%20we%20first%20extract%20cinematic%20elements%20%28i.e.%2C%20human%20and%0Acamera%20pose%29%20and%20optimize%20the%20camera%20trajectory.%20Then%2C%20we%20apply%20a%20character%0Agenerator%20to%20efficiently%20create%203D%20high-quality%20characters%20with%20a%20human%0Astructure%20prior.%20Finally%2C%20we%20develop%20a%20structure-guided%20motion%20transfer%0Astrategy%20to%20incorporate%20generated%20characters%20into%20film%20creation%20and%20transfer%20it%0Avia%203D%20graphics%20engines%20smoothly.%20Extensive%20experiments%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20for%20creating%20high-quality%20films%20with%20free%20camera%0Aand%203D%20characters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12601v1&entry.124074799=Read"},
{"title": "Unrolled Decomposed Unpaired Learning for Controllable Low-Light Video\n  Enhancement", "author": "Lingyu Zhu and Wenhan Yang and Baoliang Chen and Hanwei Zhu and Zhangkai Ni and Qi Mao and Shiqi Wang", "abstract": "  Obtaining pairs of low/normal-light videos, with motions, is more challenging\nthan still images, which raises technical issues and poses the technical route\nof unpaired learning as a critical role. This paper makes endeavors in the\ndirection of learning for low-light video enhancement without using paired\nground truth. Compared to low-light image enhancement, enhancing low-light\nvideos is more difficult due to the intertwined effects of noise, exposure, and\ncontrast in the spatial domain, jointly with the need for temporal coherence.\nTo address the above challenge, we propose the Unrolled Decomposed Unpaired\nNetwork (UDU-Net) for enhancing low-light videos by unrolling the optimization\nfunctions into a deep network to decompose the signal into spatial and\ntemporal-related factors, which are updated iteratively. Firstly, we formulate\nlow-light video enhancement as a Maximum A Posteriori estimation (MAP) problem\nwith carefully designed spatial and temporal visual regularization. Then, via\nunrolling the problem, the optimization of the spatial and temporal constraints\ncan be decomposed into different steps and updated in a stage-wise manner. From\nthe spatial perspective, the designed Intra subnet leverages unpair prior\ninformation from expert photography retouched skills to adjust the statistical\ndistribution. Additionally, we introduce a novel mechanism that integrates\nhuman perception feedback to guide network optimization, suppressing\nover/under-exposure conditions. Meanwhile, to address the issue from the\ntemporal perspective, the designed Inter subnet fully exploits temporal cues in\nprogressive optimization, which helps achieve improved temporal consistency in\nenhancement results. Consequently, the proposed method achieves superior\nperformance to state-of-the-art methods in video illumination, noise\nsuppression, and temporal consistency across outdoor and indoor scenes.\n", "link": "http://arxiv.org/abs/2408.12316v1", "date": "2024-08-22", "relevancy": 2.8285, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5727}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5629}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unrolled%20Decomposed%20Unpaired%20Learning%20for%20Controllable%20Low-Light%20Video%0A%20%20Enhancement&body=Title%3A%20Unrolled%20Decomposed%20Unpaired%20Learning%20for%20Controllable%20Low-Light%20Video%0A%20%20Enhancement%0AAuthor%3A%20Lingyu%20Zhu%20and%20Wenhan%20Yang%20and%20Baoliang%20Chen%20and%20Hanwei%20Zhu%20and%20Zhangkai%20Ni%20and%20Qi%20Mao%20and%20Shiqi%20Wang%0AAbstract%3A%20%20%20Obtaining%20pairs%20of%20low/normal-light%20videos%2C%20with%20motions%2C%20is%20more%20challenging%0Athan%20still%20images%2C%20which%20raises%20technical%20issues%20and%20poses%20the%20technical%20route%0Aof%20unpaired%20learning%20as%20a%20critical%20role.%20This%20paper%20makes%20endeavors%20in%20the%0Adirection%20of%20learning%20for%20low-light%20video%20enhancement%20without%20using%20paired%0Aground%20truth.%20Compared%20to%20low-light%20image%20enhancement%2C%20enhancing%20low-light%0Avideos%20is%20more%20difficult%20due%20to%20the%20intertwined%20effects%20of%20noise%2C%20exposure%2C%20and%0Acontrast%20in%20the%20spatial%20domain%2C%20jointly%20with%20the%20need%20for%20temporal%20coherence.%0ATo%20address%20the%20above%20challenge%2C%20we%20propose%20the%20Unrolled%20Decomposed%20Unpaired%0ANetwork%20%28UDU-Net%29%20for%20enhancing%20low-light%20videos%20by%20unrolling%20the%20optimization%0Afunctions%20into%20a%20deep%20network%20to%20decompose%20the%20signal%20into%20spatial%20and%0Atemporal-related%20factors%2C%20which%20are%20updated%20iteratively.%20Firstly%2C%20we%20formulate%0Alow-light%20video%20enhancement%20as%20a%20Maximum%20A%20Posteriori%20estimation%20%28MAP%29%20problem%0Awith%20carefully%20designed%20spatial%20and%20temporal%20visual%20regularization.%20Then%2C%20via%0Aunrolling%20the%20problem%2C%20the%20optimization%20of%20the%20spatial%20and%20temporal%20constraints%0Acan%20be%20decomposed%20into%20different%20steps%20and%20updated%20in%20a%20stage-wise%20manner.%20From%0Athe%20spatial%20perspective%2C%20the%20designed%20Intra%20subnet%20leverages%20unpair%20prior%0Ainformation%20from%20expert%20photography%20retouched%20skills%20to%20adjust%20the%20statistical%0Adistribution.%20Additionally%2C%20we%20introduce%20a%20novel%20mechanism%20that%20integrates%0Ahuman%20perception%20feedback%20to%20guide%20network%20optimization%2C%20suppressing%0Aover/under-exposure%20conditions.%20Meanwhile%2C%20to%20address%20the%20issue%20from%20the%0Atemporal%20perspective%2C%20the%20designed%20Inter%20subnet%20fully%20exploits%20temporal%20cues%20in%0Aprogressive%20optimization%2C%20which%20helps%20achieve%20improved%20temporal%20consistency%20in%0Aenhancement%20results.%20Consequently%2C%20the%20proposed%20method%20achieves%20superior%0Aperformance%20to%20state-of-the-art%20methods%20in%20video%20illumination%2C%20noise%0Asuppression%2C%20and%20temporal%20consistency%20across%20outdoor%20and%20indoor%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnrolled%2520Decomposed%2520Unpaired%2520Learning%2520for%2520Controllable%2520Low-Light%2520Video%250A%2520%2520Enhancement%26entry.906535625%3DLingyu%2520Zhu%2520and%2520Wenhan%2520Yang%2520and%2520Baoliang%2520Chen%2520and%2520Hanwei%2520Zhu%2520and%2520Zhangkai%2520Ni%2520and%2520Qi%2520Mao%2520and%2520Shiqi%2520Wang%26entry.1292438233%3D%2520%2520Obtaining%2520pairs%2520of%2520low/normal-light%2520videos%252C%2520with%2520motions%252C%2520is%2520more%2520challenging%250Athan%2520still%2520images%252C%2520which%2520raises%2520technical%2520issues%2520and%2520poses%2520the%2520technical%2520route%250Aof%2520unpaired%2520learning%2520as%2520a%2520critical%2520role.%2520This%2520paper%2520makes%2520endeavors%2520in%2520the%250Adirection%2520of%2520learning%2520for%2520low-light%2520video%2520enhancement%2520without%2520using%2520paired%250Aground%2520truth.%2520Compared%2520to%2520low-light%2520image%2520enhancement%252C%2520enhancing%2520low-light%250Avideos%2520is%2520more%2520difficult%2520due%2520to%2520the%2520intertwined%2520effects%2520of%2520noise%252C%2520exposure%252C%2520and%250Acontrast%2520in%2520the%2520spatial%2520domain%252C%2520jointly%2520with%2520the%2520need%2520for%2520temporal%2520coherence.%250ATo%2520address%2520the%2520above%2520challenge%252C%2520we%2520propose%2520the%2520Unrolled%2520Decomposed%2520Unpaired%250ANetwork%2520%2528UDU-Net%2529%2520for%2520enhancing%2520low-light%2520videos%2520by%2520unrolling%2520the%2520optimization%250Afunctions%2520into%2520a%2520deep%2520network%2520to%2520decompose%2520the%2520signal%2520into%2520spatial%2520and%250Atemporal-related%2520factors%252C%2520which%2520are%2520updated%2520iteratively.%2520Firstly%252C%2520we%2520formulate%250Alow-light%2520video%2520enhancement%2520as%2520a%2520Maximum%2520A%2520Posteriori%2520estimation%2520%2528MAP%2529%2520problem%250Awith%2520carefully%2520designed%2520spatial%2520and%2520temporal%2520visual%2520regularization.%2520Then%252C%2520via%250Aunrolling%2520the%2520problem%252C%2520the%2520optimization%2520of%2520the%2520spatial%2520and%2520temporal%2520constraints%250Acan%2520be%2520decomposed%2520into%2520different%2520steps%2520and%2520updated%2520in%2520a%2520stage-wise%2520manner.%2520From%250Athe%2520spatial%2520perspective%252C%2520the%2520designed%2520Intra%2520subnet%2520leverages%2520unpair%2520prior%250Ainformation%2520from%2520expert%2520photography%2520retouched%2520skills%2520to%2520adjust%2520the%2520statistical%250Adistribution.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520mechanism%2520that%2520integrates%250Ahuman%2520perception%2520feedback%2520to%2520guide%2520network%2520optimization%252C%2520suppressing%250Aover/under-exposure%2520conditions.%2520Meanwhile%252C%2520to%2520address%2520the%2520issue%2520from%2520the%250Atemporal%2520perspective%252C%2520the%2520designed%2520Inter%2520subnet%2520fully%2520exploits%2520temporal%2520cues%2520in%250Aprogressive%2520optimization%252C%2520which%2520helps%2520achieve%2520improved%2520temporal%2520consistency%2520in%250Aenhancement%2520results.%2520Consequently%252C%2520the%2520proposed%2520method%2520achieves%2520superior%250Aperformance%2520to%2520state-of-the-art%2520methods%2520in%2520video%2520illumination%252C%2520noise%250Asuppression%252C%2520and%2520temporal%2520consistency%2520across%2520outdoor%2520and%2520indoor%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unrolled%20Decomposed%20Unpaired%20Learning%20for%20Controllable%20Low-Light%20Video%0A%20%20Enhancement&entry.906535625=Lingyu%20Zhu%20and%20Wenhan%20Yang%20and%20Baoliang%20Chen%20and%20Hanwei%20Zhu%20and%20Zhangkai%20Ni%20and%20Qi%20Mao%20and%20Shiqi%20Wang&entry.1292438233=%20%20Obtaining%20pairs%20of%20low/normal-light%20videos%2C%20with%20motions%2C%20is%20more%20challenging%0Athan%20still%20images%2C%20which%20raises%20technical%20issues%20and%20poses%20the%20technical%20route%0Aof%20unpaired%20learning%20as%20a%20critical%20role.%20This%20paper%20makes%20endeavors%20in%20the%0Adirection%20of%20learning%20for%20low-light%20video%20enhancement%20without%20using%20paired%0Aground%20truth.%20Compared%20to%20low-light%20image%20enhancement%2C%20enhancing%20low-light%0Avideos%20is%20more%20difficult%20due%20to%20the%20intertwined%20effects%20of%20noise%2C%20exposure%2C%20and%0Acontrast%20in%20the%20spatial%20domain%2C%20jointly%20with%20the%20need%20for%20temporal%20coherence.%0ATo%20address%20the%20above%20challenge%2C%20we%20propose%20the%20Unrolled%20Decomposed%20Unpaired%0ANetwork%20%28UDU-Net%29%20for%20enhancing%20low-light%20videos%20by%20unrolling%20the%20optimization%0Afunctions%20into%20a%20deep%20network%20to%20decompose%20the%20signal%20into%20spatial%20and%0Atemporal-related%20factors%2C%20which%20are%20updated%20iteratively.%20Firstly%2C%20we%20formulate%0Alow-light%20video%20enhancement%20as%20a%20Maximum%20A%20Posteriori%20estimation%20%28MAP%29%20problem%0Awith%20carefully%20designed%20spatial%20and%20temporal%20visual%20regularization.%20Then%2C%20via%0Aunrolling%20the%20problem%2C%20the%20optimization%20of%20the%20spatial%20and%20temporal%20constraints%0Acan%20be%20decomposed%20into%20different%20steps%20and%20updated%20in%20a%20stage-wise%20manner.%20From%0Athe%20spatial%20perspective%2C%20the%20designed%20Intra%20subnet%20leverages%20unpair%20prior%0Ainformation%20from%20expert%20photography%20retouched%20skills%20to%20adjust%20the%20statistical%0Adistribution.%20Additionally%2C%20we%20introduce%20a%20novel%20mechanism%20that%20integrates%0Ahuman%20perception%20feedback%20to%20guide%20network%20optimization%2C%20suppressing%0Aover/under-exposure%20conditions.%20Meanwhile%2C%20to%20address%20the%20issue%20from%20the%0Atemporal%20perspective%2C%20the%20designed%20Inter%20subnet%20fully%20exploits%20temporal%20cues%20in%0Aprogressive%20optimization%2C%20which%20helps%20achieve%20improved%20temporal%20consistency%20in%0Aenhancement%20results.%20Consequently%2C%20the%20proposed%20method%20achieves%20superior%0Aperformance%20to%20state-of-the-art%20methods%20in%20video%20illumination%2C%20noise%0Asuppression%2C%20and%20temporal%20consistency%20across%20outdoor%20and%20indoor%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12316v1&entry.124074799=Read"},
{"title": "Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action\n  Recognition", "author": "Bozheng Li and Mushui Liu and Gaoang Wang and Yunlong Yu", "abstract": "  In this paper, we propose a novel Temporal Sequence-Aware Model (TSAM) for\nfew-shot action recognition (FSAR), which incorporates a sequential perceiver\nadapter into the pre-training framework, to integrate both the spatial\ninformation and the sequential temporal dynamics into the feature embeddings.\nDifferent from the existing fine-tuning approaches that capture temporal\ninformation by exploring the relationships among all the frames, our\nperceiver-based adapter recurrently captures the sequential dynamics alongside\nthe timeline, which could perceive the order change. To obtain the\ndiscriminative representations for each class, we extend a textual corpus for\neach class derived from the large language models (LLMs) and enrich the visual\nprototypes by integrating the contextual semantic information. Besides, We\nintroduce an unbalanced optimal transport strategy for feature matching that\nmitigates the impact of class-unrelated features, thereby facilitating more\neffective decision-making. Experimental results on five FSAR datasets\ndemonstrate that our method set a new benchmark, beating the second-best\ncompetitors with large margins.\n", "link": "http://arxiv.org/abs/2408.12475v1", "date": "2024-08-22", "relevancy": 2.8222, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.598}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5684}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frame%20Order%20Matters%3A%20A%20Temporal%20Sequence-Aware%20Model%20for%20Few-Shot%20Action%0A%20%20Recognition&body=Title%3A%20Frame%20Order%20Matters%3A%20A%20Temporal%20Sequence-Aware%20Model%20for%20Few-Shot%20Action%0A%20%20Recognition%0AAuthor%3A%20Bozheng%20Li%20and%20Mushui%20Liu%20and%20Gaoang%20Wang%20and%20Yunlong%20Yu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Temporal%20Sequence-Aware%20Model%20%28TSAM%29%20for%0Afew-shot%20action%20recognition%20%28FSAR%29%2C%20which%20incorporates%20a%20sequential%20perceiver%0Aadapter%20into%20the%20pre-training%20framework%2C%20to%20integrate%20both%20the%20spatial%0Ainformation%20and%20the%20sequential%20temporal%20dynamics%20into%20the%20feature%20embeddings.%0ADifferent%20from%20the%20existing%20fine-tuning%20approaches%20that%20capture%20temporal%0Ainformation%20by%20exploring%20the%20relationships%20among%20all%20the%20frames%2C%20our%0Aperceiver-based%20adapter%20recurrently%20captures%20the%20sequential%20dynamics%20alongside%0Athe%20timeline%2C%20which%20could%20perceive%20the%20order%20change.%20To%20obtain%20the%0Adiscriminative%20representations%20for%20each%20class%2C%20we%20extend%20a%20textual%20corpus%20for%0Aeach%20class%20derived%20from%20the%20large%20language%20models%20%28LLMs%29%20and%20enrich%20the%20visual%0Aprototypes%20by%20integrating%20the%20contextual%20semantic%20information.%20Besides%2C%20We%0Aintroduce%20an%20unbalanced%20optimal%20transport%20strategy%20for%20feature%20matching%20that%0Amitigates%20the%20impact%20of%20class-unrelated%20features%2C%20thereby%20facilitating%20more%0Aeffective%20decision-making.%20Experimental%20results%20on%20five%20FSAR%20datasets%0Ademonstrate%20that%20our%20method%20set%20a%20new%20benchmark%2C%20beating%20the%20second-best%0Acompetitors%20with%20large%20margins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrame%2520Order%2520Matters%253A%2520A%2520Temporal%2520Sequence-Aware%2520Model%2520for%2520Few-Shot%2520Action%250A%2520%2520Recognition%26entry.906535625%3DBozheng%2520Li%2520and%2520Mushui%2520Liu%2520and%2520Gaoang%2520Wang%2520and%2520Yunlong%2520Yu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Temporal%2520Sequence-Aware%2520Model%2520%2528TSAM%2529%2520for%250Afew-shot%2520action%2520recognition%2520%2528FSAR%2529%252C%2520which%2520incorporates%2520a%2520sequential%2520perceiver%250Aadapter%2520into%2520the%2520pre-training%2520framework%252C%2520to%2520integrate%2520both%2520the%2520spatial%250Ainformation%2520and%2520the%2520sequential%2520temporal%2520dynamics%2520into%2520the%2520feature%2520embeddings.%250ADifferent%2520from%2520the%2520existing%2520fine-tuning%2520approaches%2520that%2520capture%2520temporal%250Ainformation%2520by%2520exploring%2520the%2520relationships%2520among%2520all%2520the%2520frames%252C%2520our%250Aperceiver-based%2520adapter%2520recurrently%2520captures%2520the%2520sequential%2520dynamics%2520alongside%250Athe%2520timeline%252C%2520which%2520could%2520perceive%2520the%2520order%2520change.%2520To%2520obtain%2520the%250Adiscriminative%2520representations%2520for%2520each%2520class%252C%2520we%2520extend%2520a%2520textual%2520corpus%2520for%250Aeach%2520class%2520derived%2520from%2520the%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520enrich%2520the%2520visual%250Aprototypes%2520by%2520integrating%2520the%2520contextual%2520semantic%2520information.%2520Besides%252C%2520We%250Aintroduce%2520an%2520unbalanced%2520optimal%2520transport%2520strategy%2520for%2520feature%2520matching%2520that%250Amitigates%2520the%2520impact%2520of%2520class-unrelated%2520features%252C%2520thereby%2520facilitating%2520more%250Aeffective%2520decision-making.%2520Experimental%2520results%2520on%2520five%2520FSAR%2520datasets%250Ademonstrate%2520that%2520our%2520method%2520set%2520a%2520new%2520benchmark%252C%2520beating%2520the%2520second-best%250Acompetitors%2520with%2520large%2520margins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frame%20Order%20Matters%3A%20A%20Temporal%20Sequence-Aware%20Model%20for%20Few-Shot%20Action%0A%20%20Recognition&entry.906535625=Bozheng%20Li%20and%20Mushui%20Liu%20and%20Gaoang%20Wang%20and%20Yunlong%20Yu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Temporal%20Sequence-Aware%20Model%20%28TSAM%29%20for%0Afew-shot%20action%20recognition%20%28FSAR%29%2C%20which%20incorporates%20a%20sequential%20perceiver%0Aadapter%20into%20the%20pre-training%20framework%2C%20to%20integrate%20both%20the%20spatial%0Ainformation%20and%20the%20sequential%20temporal%20dynamics%20into%20the%20feature%20embeddings.%0ADifferent%20from%20the%20existing%20fine-tuning%20approaches%20that%20capture%20temporal%0Ainformation%20by%20exploring%20the%20relationships%20among%20all%20the%20frames%2C%20our%0Aperceiver-based%20adapter%20recurrently%20captures%20the%20sequential%20dynamics%20alongside%0Athe%20timeline%2C%20which%20could%20perceive%20the%20order%20change.%20To%20obtain%20the%0Adiscriminative%20representations%20for%20each%20class%2C%20we%20extend%20a%20textual%20corpus%20for%0Aeach%20class%20derived%20from%20the%20large%20language%20models%20%28LLMs%29%20and%20enrich%20the%20visual%0Aprototypes%20by%20integrating%20the%20contextual%20semantic%20information.%20Besides%2C%20We%0Aintroduce%20an%20unbalanced%20optimal%20transport%20strategy%20for%20feature%20matching%20that%0Amitigates%20the%20impact%20of%20class-unrelated%20features%2C%20thereby%20facilitating%20more%0Aeffective%20decision-making.%20Experimental%20results%20on%20five%20FSAR%20datasets%0Ademonstrate%20that%20our%20method%20set%20a%20new%20benchmark%2C%20beating%20the%20second-best%0Acompetitors%20with%20large%20margins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12475v1&entry.124074799=Read"},
{"title": "Real-Time Video Generation with Pyramid Attention Broadcast", "author": "Xuanlei Zhao and Xiaolong Jin and Kai Wang and Yang You", "abstract": "  We present Pyramid Attention Broadcast (PAB), a real-time, high quality and\ntraining-free approach for DiT-based video generation. Our method is founded on\nthe observation that attention difference in the diffusion process exhibits a\nU-shaped pattern, indicating significant redundancy. We mitigate this by\nbroadcasting attention outputs to subsequent steps in a pyramid style. It\napplies different broadcast strategies to each attention based on their\nvariance for best efficiency. We further introduce broadcast sequence parallel\nfor more efficient distributed inference. PAB demonstrates superior results\nacross three models compared to baselines, achieving real-time generation for\nup to 720p videos. We anticipate that our simple yet effective method will\nserve as a robust baseline and facilitate future research and application for\nvideo generation.\n", "link": "http://arxiv.org/abs/2408.12588v1", "date": "2024-08-22", "relevancy": 2.8206, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5786}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5597}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Video%20Generation%20with%20Pyramid%20Attention%20Broadcast&body=Title%3A%20Real-Time%20Video%20Generation%20with%20Pyramid%20Attention%20Broadcast%0AAuthor%3A%20Xuanlei%20Zhao%20and%20Xiaolong%20Jin%20and%20Kai%20Wang%20and%20Yang%20You%0AAbstract%3A%20%20%20We%20present%20Pyramid%20Attention%20Broadcast%20%28PAB%29%2C%20a%20real-time%2C%20high%20quality%20and%0Atraining-free%20approach%20for%20DiT-based%20video%20generation.%20Our%20method%20is%20founded%20on%0Athe%20observation%20that%20attention%20difference%20in%20the%20diffusion%20process%20exhibits%20a%0AU-shaped%20pattern%2C%20indicating%20significant%20redundancy.%20We%20mitigate%20this%20by%0Abroadcasting%20attention%20outputs%20to%20subsequent%20steps%20in%20a%20pyramid%20style.%20It%0Aapplies%20different%20broadcast%20strategies%20to%20each%20attention%20based%20on%20their%0Avariance%20for%20best%20efficiency.%20We%20further%20introduce%20broadcast%20sequence%20parallel%0Afor%20more%20efficient%20distributed%20inference.%20PAB%20demonstrates%20superior%20results%0Aacross%20three%20models%20compared%20to%20baselines%2C%20achieving%20real-time%20generation%20for%0Aup%20to%20720p%20videos.%20We%20anticipate%20that%20our%20simple%20yet%20effective%20method%20will%0Aserve%20as%20a%20robust%20baseline%20and%20facilitate%20future%20research%20and%20application%20for%0Avideo%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Video%2520Generation%2520with%2520Pyramid%2520Attention%2520Broadcast%26entry.906535625%3DXuanlei%2520Zhao%2520and%2520Xiaolong%2520Jin%2520and%2520Kai%2520Wang%2520and%2520Yang%2520You%26entry.1292438233%3D%2520%2520We%2520present%2520Pyramid%2520Attention%2520Broadcast%2520%2528PAB%2529%252C%2520a%2520real-time%252C%2520high%2520quality%2520and%250Atraining-free%2520approach%2520for%2520DiT-based%2520video%2520generation.%2520Our%2520method%2520is%2520founded%2520on%250Athe%2520observation%2520that%2520attention%2520difference%2520in%2520the%2520diffusion%2520process%2520exhibits%2520a%250AU-shaped%2520pattern%252C%2520indicating%2520significant%2520redundancy.%2520We%2520mitigate%2520this%2520by%250Abroadcasting%2520attention%2520outputs%2520to%2520subsequent%2520steps%2520in%2520a%2520pyramid%2520style.%2520It%250Aapplies%2520different%2520broadcast%2520strategies%2520to%2520each%2520attention%2520based%2520on%2520their%250Avariance%2520for%2520best%2520efficiency.%2520We%2520further%2520introduce%2520broadcast%2520sequence%2520parallel%250Afor%2520more%2520efficient%2520distributed%2520inference.%2520PAB%2520demonstrates%2520superior%2520results%250Aacross%2520three%2520models%2520compared%2520to%2520baselines%252C%2520achieving%2520real-time%2520generation%2520for%250Aup%2520to%2520720p%2520videos.%2520We%2520anticipate%2520that%2520our%2520simple%2520yet%2520effective%2520method%2520will%250Aserve%2520as%2520a%2520robust%2520baseline%2520and%2520facilitate%2520future%2520research%2520and%2520application%2520for%250Avideo%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Video%20Generation%20with%20Pyramid%20Attention%20Broadcast&entry.906535625=Xuanlei%20Zhao%20and%20Xiaolong%20Jin%20and%20Kai%20Wang%20and%20Yang%20You&entry.1292438233=%20%20We%20present%20Pyramid%20Attention%20Broadcast%20%28PAB%29%2C%20a%20real-time%2C%20high%20quality%20and%0Atraining-free%20approach%20for%20DiT-based%20video%20generation.%20Our%20method%20is%20founded%20on%0Athe%20observation%20that%20attention%20difference%20in%20the%20diffusion%20process%20exhibits%20a%0AU-shaped%20pattern%2C%20indicating%20significant%20redundancy.%20We%20mitigate%20this%20by%0Abroadcasting%20attention%20outputs%20to%20subsequent%20steps%20in%20a%20pyramid%20style.%20It%0Aapplies%20different%20broadcast%20strategies%20to%20each%20attention%20based%20on%20their%0Avariance%20for%20best%20efficiency.%20We%20further%20introduce%20broadcast%20sequence%20parallel%0Afor%20more%20efficient%20distributed%20inference.%20PAB%20demonstrates%20superior%20results%0Aacross%20three%20models%20compared%20to%20baselines%2C%20achieving%20real-time%20generation%20for%0Aup%20to%20720p%20videos.%20We%20anticipate%20that%20our%20simple%20yet%20effective%20method%20will%0Aserve%20as%20a%20robust%20baseline%20and%20facilitate%20future%20research%20and%20application%20for%0Avideo%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12588v1&entry.124074799=Read"},
{"title": "Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method", "author": "Jie Tian and Ran Ji and Lingxiao Yang and Yuexin Ma and Lan Xu and Jingyi Yu and Ye Shi and Jingya Wang", "abstract": "  Gaze plays a crucial role in revealing human attention and intention,\nparticularly in hand-object interaction scenarios, where it guides and\nsynchronizes complex tasks that require precise coordination between the brain,\nhand, and object. Motivated by this, we introduce a novel task: Gaze-Guided\nHand-Object Interaction Synthesis, with potential applications in augmented\nreality, virtual reality, and assistive technologies. To support this task, we\npresent GazeHOI, the first dataset to capture simultaneous 3D modeling of gaze,\nhand, and object interactions. This task poses significant challenges due to\nthe inherent sparsity and noise in gaze data, as well as the need for high\nconsistency and physical plausibility in generating hand and object motions. To\ntackle these issues, we propose a stacked gaze-guided hand-object interaction\ndiffusion model, named GHO-Diffusion. The stacked design effectively reduces\nthe complexity of motion generation. We also introduce HOI-Manifold Guidance\nduring the sampling stage of GHO-Diffusion, enabling fine-grained control over\ngenerated motions while maintaining the data manifold. Additionally, we propose\na spatial-temporal gaze feature encoding for the diffusion condition and select\ndiffusion results based on consistency scores between gaze-contact maps and\ngaze-interaction trajectories. Extensive experiments highlight the\neffectiveness of our method and the unique contributions of our dataset.\n", "link": "http://arxiv.org/abs/2403.16169v4", "date": "2024-08-22", "relevancy": 2.8192, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5852}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5808}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaze-guided%20Hand-Object%20Interaction%20Synthesis%3A%20Dataset%20and%20Method&body=Title%3A%20Gaze-guided%20Hand-Object%20Interaction%20Synthesis%3A%20Dataset%20and%20Method%0AAuthor%3A%20Jie%20Tian%20and%20Ran%20Ji%20and%20Lingxiao%20Yang%20and%20Yuexin%20Ma%20and%20Lan%20Xu%20and%20Jingyi%20Yu%20and%20Ye%20Shi%20and%20Jingya%20Wang%0AAbstract%3A%20%20%20Gaze%20plays%20a%20crucial%20role%20in%20revealing%20human%20attention%20and%20intention%2C%0Aparticularly%20in%20hand-object%20interaction%20scenarios%2C%20where%20it%20guides%20and%0Asynchronizes%20complex%20tasks%20that%20require%20precise%20coordination%20between%20the%20brain%2C%0Ahand%2C%20and%20object.%20Motivated%20by%20this%2C%20we%20introduce%20a%20novel%20task%3A%20Gaze-Guided%0AHand-Object%20Interaction%20Synthesis%2C%20with%20potential%20applications%20in%20augmented%0Areality%2C%20virtual%20reality%2C%20and%20assistive%20technologies.%20To%20support%20this%20task%2C%20we%0Apresent%20GazeHOI%2C%20the%20first%20dataset%20to%20capture%20simultaneous%203D%20modeling%20of%20gaze%2C%0Ahand%2C%20and%20object%20interactions.%20This%20task%20poses%20significant%20challenges%20due%20to%0Athe%20inherent%20sparsity%20and%20noise%20in%20gaze%20data%2C%20as%20well%20as%20the%20need%20for%20high%0Aconsistency%20and%20physical%20plausibility%20in%20generating%20hand%20and%20object%20motions.%20To%0Atackle%20these%20issues%2C%20we%20propose%20a%20stacked%20gaze-guided%20hand-object%20interaction%0Adiffusion%20model%2C%20named%20GHO-Diffusion.%20The%20stacked%20design%20effectively%20reduces%0Athe%20complexity%20of%20motion%20generation.%20We%20also%20introduce%20HOI-Manifold%20Guidance%0Aduring%20the%20sampling%20stage%20of%20GHO-Diffusion%2C%20enabling%20fine-grained%20control%20over%0Agenerated%20motions%20while%20maintaining%20the%20data%20manifold.%20Additionally%2C%20we%20propose%0Aa%20spatial-temporal%20gaze%20feature%20encoding%20for%20the%20diffusion%20condition%20and%20select%0Adiffusion%20results%20based%20on%20consistency%20scores%20between%20gaze-contact%20maps%20and%0Agaze-interaction%20trajectories.%20Extensive%20experiments%20highlight%20the%0Aeffectiveness%20of%20our%20method%20and%20the%20unique%20contributions%20of%20our%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16169v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaze-guided%2520Hand-Object%2520Interaction%2520Synthesis%253A%2520Dataset%2520and%2520Method%26entry.906535625%3DJie%2520Tian%2520and%2520Ran%2520Ji%2520and%2520Lingxiao%2520Yang%2520and%2520Yuexin%2520Ma%2520and%2520Lan%2520Xu%2520and%2520Jingyi%2520Yu%2520and%2520Ye%2520Shi%2520and%2520Jingya%2520Wang%26entry.1292438233%3D%2520%2520Gaze%2520plays%2520a%2520crucial%2520role%2520in%2520revealing%2520human%2520attention%2520and%2520intention%252C%250Aparticularly%2520in%2520hand-object%2520interaction%2520scenarios%252C%2520where%2520it%2520guides%2520and%250Asynchronizes%2520complex%2520tasks%2520that%2520require%2520precise%2520coordination%2520between%2520the%2520brain%252C%250Ahand%252C%2520and%2520object.%2520Motivated%2520by%2520this%252C%2520we%2520introduce%2520a%2520novel%2520task%253A%2520Gaze-Guided%250AHand-Object%2520Interaction%2520Synthesis%252C%2520with%2520potential%2520applications%2520in%2520augmented%250Areality%252C%2520virtual%2520reality%252C%2520and%2520assistive%2520technologies.%2520To%2520support%2520this%2520task%252C%2520we%250Apresent%2520GazeHOI%252C%2520the%2520first%2520dataset%2520to%2520capture%2520simultaneous%25203D%2520modeling%2520of%2520gaze%252C%250Ahand%252C%2520and%2520object%2520interactions.%2520This%2520task%2520poses%2520significant%2520challenges%2520due%2520to%250Athe%2520inherent%2520sparsity%2520and%2520noise%2520in%2520gaze%2520data%252C%2520as%2520well%2520as%2520the%2520need%2520for%2520high%250Aconsistency%2520and%2520physical%2520plausibility%2520in%2520generating%2520hand%2520and%2520object%2520motions.%2520To%250Atackle%2520these%2520issues%252C%2520we%2520propose%2520a%2520stacked%2520gaze-guided%2520hand-object%2520interaction%250Adiffusion%2520model%252C%2520named%2520GHO-Diffusion.%2520The%2520stacked%2520design%2520effectively%2520reduces%250Athe%2520complexity%2520of%2520motion%2520generation.%2520We%2520also%2520introduce%2520HOI-Manifold%2520Guidance%250Aduring%2520the%2520sampling%2520stage%2520of%2520GHO-Diffusion%252C%2520enabling%2520fine-grained%2520control%2520over%250Agenerated%2520motions%2520while%2520maintaining%2520the%2520data%2520manifold.%2520Additionally%252C%2520we%2520propose%250Aa%2520spatial-temporal%2520gaze%2520feature%2520encoding%2520for%2520the%2520diffusion%2520condition%2520and%2520select%250Adiffusion%2520results%2520based%2520on%2520consistency%2520scores%2520between%2520gaze-contact%2520maps%2520and%250Agaze-interaction%2520trajectories.%2520Extensive%2520experiments%2520highlight%2520the%250Aeffectiveness%2520of%2520our%2520method%2520and%2520the%2520unique%2520contributions%2520of%2520our%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16169v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaze-guided%20Hand-Object%20Interaction%20Synthesis%3A%20Dataset%20and%20Method&entry.906535625=Jie%20Tian%20and%20Ran%20Ji%20and%20Lingxiao%20Yang%20and%20Yuexin%20Ma%20and%20Lan%20Xu%20and%20Jingyi%20Yu%20and%20Ye%20Shi%20and%20Jingya%20Wang&entry.1292438233=%20%20Gaze%20plays%20a%20crucial%20role%20in%20revealing%20human%20attention%20and%20intention%2C%0Aparticularly%20in%20hand-object%20interaction%20scenarios%2C%20where%20it%20guides%20and%0Asynchronizes%20complex%20tasks%20that%20require%20precise%20coordination%20between%20the%20brain%2C%0Ahand%2C%20and%20object.%20Motivated%20by%20this%2C%20we%20introduce%20a%20novel%20task%3A%20Gaze-Guided%0AHand-Object%20Interaction%20Synthesis%2C%20with%20potential%20applications%20in%20augmented%0Areality%2C%20virtual%20reality%2C%20and%20assistive%20technologies.%20To%20support%20this%20task%2C%20we%0Apresent%20GazeHOI%2C%20the%20first%20dataset%20to%20capture%20simultaneous%203D%20modeling%20of%20gaze%2C%0Ahand%2C%20and%20object%20interactions.%20This%20task%20poses%20significant%20challenges%20due%20to%0Athe%20inherent%20sparsity%20and%20noise%20in%20gaze%20data%2C%20as%20well%20as%20the%20need%20for%20high%0Aconsistency%20and%20physical%20plausibility%20in%20generating%20hand%20and%20object%20motions.%20To%0Atackle%20these%20issues%2C%20we%20propose%20a%20stacked%20gaze-guided%20hand-object%20interaction%0Adiffusion%20model%2C%20named%20GHO-Diffusion.%20The%20stacked%20design%20effectively%20reduces%0Athe%20complexity%20of%20motion%20generation.%20We%20also%20introduce%20HOI-Manifold%20Guidance%0Aduring%20the%20sampling%20stage%20of%20GHO-Diffusion%2C%20enabling%20fine-grained%20control%20over%0Agenerated%20motions%20while%20maintaining%20the%20data%20manifold.%20Additionally%2C%20we%20propose%0Aa%20spatial-temporal%20gaze%20feature%20encoding%20for%20the%20diffusion%20condition%20and%20select%0Adiffusion%20results%20based%20on%20consistency%20scores%20between%20gaze-contact%20maps%20and%0Agaze-interaction%20trajectories.%20Extensive%20experiments%20highlight%20the%0Aeffectiveness%20of%20our%20method%20and%20the%20unique%20contributions%20of%20our%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16169v4&entry.124074799=Read"},
{"title": "ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor\n  Reconstruction", "author": "Ziyu Tang and Weicai Ye and Yifan Wang and Di Huang and Hujun Bao and Tong He and Guofeng Zhang", "abstract": "  Neural implicit reconstruction via volume rendering has demonstrated its\neffectiveness in recovering dense 3D surfaces. However, it is non-trivial to\nsimultaneously recover meticulous geometry and preserve smoothness across\nregions with differing characteristics. To address this issue, previous methods\ntypically employ geometric priors, which are often constrained by the\nperformance of the prior models. In this paper, we propose ND-SDF, which learns\na Normal Ddeflection field to represent the angular deviation between the scene\nnormal and the prior normal. Unlike previous methods that uniformly apply\ngeometric priors on all samples, introducing significant bias in accuracy, our\nproposed normal deflection field dynamically learns and adapts the utilization\nof samples based on their specific characteristics, thereby improving both the\naccuracy and effectiveness of the model. Our method not only obtains smooth\nweakly textured regions such as walls and floors but also preserves the\ngeometric details of complex structures. In addition, we introduce a novel ray\nsampling strategy based on the deflection angle to facilitate the unbiased\nrendering process, which significantly improves the quality and accuracy of\nintricate surfaces, especially on thin structures. Consistent improvements on\nvarious challenging datasets demonstrate the superiority of our method.\n", "link": "http://arxiv.org/abs/2408.12598v1", "date": "2024-08-22", "relevancy": 2.8157, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5824}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5625}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ND-SDF%3A%20Learning%20Normal%20Deflection%20Fields%20for%20High-Fidelity%20Indoor%0A%20%20Reconstruction&body=Title%3A%20ND-SDF%3A%20Learning%20Normal%20Deflection%20Fields%20for%20High-Fidelity%20Indoor%0A%20%20Reconstruction%0AAuthor%3A%20Ziyu%20Tang%20and%20Weicai%20Ye%20and%20Yifan%20Wang%20and%20Di%20Huang%20and%20Hujun%20Bao%20and%20Tong%20He%20and%20Guofeng%20Zhang%0AAbstract%3A%20%20%20Neural%20implicit%20reconstruction%20via%20volume%20rendering%20has%20demonstrated%20its%0Aeffectiveness%20in%20recovering%20dense%203D%20surfaces.%20However%2C%20it%20is%20non-trivial%20to%0Asimultaneously%20recover%20meticulous%20geometry%20and%20preserve%20smoothness%20across%0Aregions%20with%20differing%20characteristics.%20To%20address%20this%20issue%2C%20previous%20methods%0Atypically%20employ%20geometric%20priors%2C%20which%20are%20often%20constrained%20by%20the%0Aperformance%20of%20the%20prior%20models.%20In%20this%20paper%2C%20we%20propose%20ND-SDF%2C%20which%20learns%0Aa%20Normal%20Ddeflection%20field%20to%20represent%20the%20angular%20deviation%20between%20the%20scene%0Anormal%20and%20the%20prior%20normal.%20Unlike%20previous%20methods%20that%20uniformly%20apply%0Ageometric%20priors%20on%20all%20samples%2C%20introducing%20significant%20bias%20in%20accuracy%2C%20our%0Aproposed%20normal%20deflection%20field%20dynamically%20learns%20and%20adapts%20the%20utilization%0Aof%20samples%20based%20on%20their%20specific%20characteristics%2C%20thereby%20improving%20both%20the%0Aaccuracy%20and%20effectiveness%20of%20the%20model.%20Our%20method%20not%20only%20obtains%20smooth%0Aweakly%20textured%20regions%20such%20as%20walls%20and%20floors%20but%20also%20preserves%20the%0Ageometric%20details%20of%20complex%20structures.%20In%20addition%2C%20we%20introduce%20a%20novel%20ray%0Asampling%20strategy%20based%20on%20the%20deflection%20angle%20to%20facilitate%20the%20unbiased%0Arendering%20process%2C%20which%20significantly%20improves%20the%20quality%20and%20accuracy%20of%0Aintricate%20surfaces%2C%20especially%20on%20thin%20structures.%20Consistent%20improvements%20on%0Avarious%20challenging%20datasets%20demonstrate%20the%20superiority%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DND-SDF%253A%2520Learning%2520Normal%2520Deflection%2520Fields%2520for%2520High-Fidelity%2520Indoor%250A%2520%2520Reconstruction%26entry.906535625%3DZiyu%2520Tang%2520and%2520Weicai%2520Ye%2520and%2520Yifan%2520Wang%2520and%2520Di%2520Huang%2520and%2520Hujun%2520Bao%2520and%2520Tong%2520He%2520and%2520Guofeng%2520Zhang%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520reconstruction%2520via%2520volume%2520rendering%2520has%2520demonstrated%2520its%250Aeffectiveness%2520in%2520recovering%2520dense%25203D%2520surfaces.%2520However%252C%2520it%2520is%2520non-trivial%2520to%250Asimultaneously%2520recover%2520meticulous%2520geometry%2520and%2520preserve%2520smoothness%2520across%250Aregions%2520with%2520differing%2520characteristics.%2520To%2520address%2520this%2520issue%252C%2520previous%2520methods%250Atypically%2520employ%2520geometric%2520priors%252C%2520which%2520are%2520often%2520constrained%2520by%2520the%250Aperformance%2520of%2520the%2520prior%2520models.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ND-SDF%252C%2520which%2520learns%250Aa%2520Normal%2520Ddeflection%2520field%2520to%2520represent%2520the%2520angular%2520deviation%2520between%2520the%2520scene%250Anormal%2520and%2520the%2520prior%2520normal.%2520Unlike%2520previous%2520methods%2520that%2520uniformly%2520apply%250Ageometric%2520priors%2520on%2520all%2520samples%252C%2520introducing%2520significant%2520bias%2520in%2520accuracy%252C%2520our%250Aproposed%2520normal%2520deflection%2520field%2520dynamically%2520learns%2520and%2520adapts%2520the%2520utilization%250Aof%2520samples%2520based%2520on%2520their%2520specific%2520characteristics%252C%2520thereby%2520improving%2520both%2520the%250Aaccuracy%2520and%2520effectiveness%2520of%2520the%2520model.%2520Our%2520method%2520not%2520only%2520obtains%2520smooth%250Aweakly%2520textured%2520regions%2520such%2520as%2520walls%2520and%2520floors%2520but%2520also%2520preserves%2520the%250Ageometric%2520details%2520of%2520complex%2520structures.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520novel%2520ray%250Asampling%2520strategy%2520based%2520on%2520the%2520deflection%2520angle%2520to%2520facilitate%2520the%2520unbiased%250Arendering%2520process%252C%2520which%2520significantly%2520improves%2520the%2520quality%2520and%2520accuracy%2520of%250Aintricate%2520surfaces%252C%2520especially%2520on%2520thin%2520structures.%2520Consistent%2520improvements%2520on%250Avarious%2520challenging%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ND-SDF%3A%20Learning%20Normal%20Deflection%20Fields%20for%20High-Fidelity%20Indoor%0A%20%20Reconstruction&entry.906535625=Ziyu%20Tang%20and%20Weicai%20Ye%20and%20Yifan%20Wang%20and%20Di%20Huang%20and%20Hujun%20Bao%20and%20Tong%20He%20and%20Guofeng%20Zhang&entry.1292438233=%20%20Neural%20implicit%20reconstruction%20via%20volume%20rendering%20has%20demonstrated%20its%0Aeffectiveness%20in%20recovering%20dense%203D%20surfaces.%20However%2C%20it%20is%20non-trivial%20to%0Asimultaneously%20recover%20meticulous%20geometry%20and%20preserve%20smoothness%20across%0Aregions%20with%20differing%20characteristics.%20To%20address%20this%20issue%2C%20previous%20methods%0Atypically%20employ%20geometric%20priors%2C%20which%20are%20often%20constrained%20by%20the%0Aperformance%20of%20the%20prior%20models.%20In%20this%20paper%2C%20we%20propose%20ND-SDF%2C%20which%20learns%0Aa%20Normal%20Ddeflection%20field%20to%20represent%20the%20angular%20deviation%20between%20the%20scene%0Anormal%20and%20the%20prior%20normal.%20Unlike%20previous%20methods%20that%20uniformly%20apply%0Ageometric%20priors%20on%20all%20samples%2C%20introducing%20significant%20bias%20in%20accuracy%2C%20our%0Aproposed%20normal%20deflection%20field%20dynamically%20learns%20and%20adapts%20the%20utilization%0Aof%20samples%20based%20on%20their%20specific%20characteristics%2C%20thereby%20improving%20both%20the%0Aaccuracy%20and%20effectiveness%20of%20the%20model.%20Our%20method%20not%20only%20obtains%20smooth%0Aweakly%20textured%20regions%20such%20as%20walls%20and%20floors%20but%20also%20preserves%20the%0Ageometric%20details%20of%20complex%20structures.%20In%20addition%2C%20we%20introduce%20a%20novel%20ray%0Asampling%20strategy%20based%20on%20the%20deflection%20angle%20to%20facilitate%20the%20unbiased%0Arendering%20process%2C%20which%20significantly%20improves%20the%20quality%20and%20accuracy%20of%0Aintricate%20surfaces%2C%20especially%20on%20thin%20structures.%20Consistent%20improvements%20on%0Avarious%20challenging%20datasets%20demonstrate%20the%20superiority%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12598v1&entry.124074799=Read"},
{"title": "Object Re-identification via Spatial-temporal Fusion Networks and Causal\n  Identity Matching", "author": "Hye-Geun Kim and Yong-Hyuk Moon and Yeong-Jun Cho", "abstract": "  Object re-identification (ReID) in large camera networks faces numerous\nchallenges. First, the similar appearances of objects degrade ReID performance,\na challenge that needs to be addressed by existing appearance-based ReID\nmethods. Second, most ReID studies are performed in laboratory settings and do\nnot consider real-world scenarios. To overcome these challenges, we introduce a\nnovel ReID framework that leverages a spatial-temporal fusion network and\ncausal identity matching (CIM). Our framework estimates camera network topology\nusing a proposed adaptive Parzen window and combines appearance features with\nspatial-temporal cues within the fusion network. This approach has demonstrated\noutstanding performance across several datasets, including VeRi776, Vehicle-3I,\nand Market-1501, achieving up to 99.70% rank-1 accuracy and 95.5% mAP.\nFurthermore, the proposed CIM approach, which dynamically assigns gallery sets\nbased on camera network topology, has further improved ReID accuracy and\nrobustness in real-world settings, evidenced by a 94.95% mAP and a 95.19% F1\nscore on the Vehicle-3I dataset. The experimental results support the\neffectiveness of incorporating spatial-temporal information and CIM for\nreal-world ReID scenarios, regardless of the data domain (e.g., vehicle,\nperson).\n", "link": "http://arxiv.org/abs/2408.05558v2", "date": "2024-08-22", "relevancy": 2.7652, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5599}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.55}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Re-identification%20via%20Spatial-temporal%20Fusion%20Networks%20and%20Causal%0A%20%20Identity%20Matching&body=Title%3A%20Object%20Re-identification%20via%20Spatial-temporal%20Fusion%20Networks%20and%20Causal%0A%20%20Identity%20Matching%0AAuthor%3A%20Hye-Geun%20Kim%20and%20Yong-Hyuk%20Moon%20and%20Yeong-Jun%20Cho%0AAbstract%3A%20%20%20Object%20re-identification%20%28ReID%29%20in%20large%20camera%20networks%20faces%20numerous%0Achallenges.%20First%2C%20the%20similar%20appearances%20of%20objects%20degrade%20ReID%20performance%2C%0Aa%20challenge%20that%20needs%20to%20be%20addressed%20by%20existing%20appearance-based%20ReID%0Amethods.%20Second%2C%20most%20ReID%20studies%20are%20performed%20in%20laboratory%20settings%20and%20do%0Anot%20consider%20real-world%20scenarios.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20a%0Anovel%20ReID%20framework%20that%20leverages%20a%20spatial-temporal%20fusion%20network%20and%0Acausal%20identity%20matching%20%28CIM%29.%20Our%20framework%20estimates%20camera%20network%20topology%0Ausing%20a%20proposed%20adaptive%20Parzen%20window%20and%20combines%20appearance%20features%20with%0Aspatial-temporal%20cues%20within%20the%20fusion%20network.%20This%20approach%20has%20demonstrated%0Aoutstanding%20performance%20across%20several%20datasets%2C%20including%20VeRi776%2C%20Vehicle-3I%2C%0Aand%20Market-1501%2C%20achieving%20up%20to%2099.70%25%20rank-1%20accuracy%20and%2095.5%25%20mAP.%0AFurthermore%2C%20the%20proposed%20CIM%20approach%2C%20which%20dynamically%20assigns%20gallery%20sets%0Abased%20on%20camera%20network%20topology%2C%20has%20further%20improved%20ReID%20accuracy%20and%0Arobustness%20in%20real-world%20settings%2C%20evidenced%20by%20a%2094.95%25%20mAP%20and%20a%2095.19%25%20F1%0Ascore%20on%20the%20Vehicle-3I%20dataset.%20The%20experimental%20results%20support%20the%0Aeffectiveness%20of%20incorporating%20spatial-temporal%20information%20and%20CIM%20for%0Areal-world%20ReID%20scenarios%2C%20regardless%20of%20the%20data%20domain%20%28e.g.%2C%20vehicle%2C%0Aperson%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05558v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Re-identification%2520via%2520Spatial-temporal%2520Fusion%2520Networks%2520and%2520Causal%250A%2520%2520Identity%2520Matching%26entry.906535625%3DHye-Geun%2520Kim%2520and%2520Yong-Hyuk%2520Moon%2520and%2520Yeong-Jun%2520Cho%26entry.1292438233%3D%2520%2520Object%2520re-identification%2520%2528ReID%2529%2520in%2520large%2520camera%2520networks%2520faces%2520numerous%250Achallenges.%2520First%252C%2520the%2520similar%2520appearances%2520of%2520objects%2520degrade%2520ReID%2520performance%252C%250Aa%2520challenge%2520that%2520needs%2520to%2520be%2520addressed%2520by%2520existing%2520appearance-based%2520ReID%250Amethods.%2520Second%252C%2520most%2520ReID%2520studies%2520are%2520performed%2520in%2520laboratory%2520settings%2520and%2520do%250Anot%2520consider%2520real-world%2520scenarios.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520a%250Anovel%2520ReID%2520framework%2520that%2520leverages%2520a%2520spatial-temporal%2520fusion%2520network%2520and%250Acausal%2520identity%2520matching%2520%2528CIM%2529.%2520Our%2520framework%2520estimates%2520camera%2520network%2520topology%250Ausing%2520a%2520proposed%2520adaptive%2520Parzen%2520window%2520and%2520combines%2520appearance%2520features%2520with%250Aspatial-temporal%2520cues%2520within%2520the%2520fusion%2520network.%2520This%2520approach%2520has%2520demonstrated%250Aoutstanding%2520performance%2520across%2520several%2520datasets%252C%2520including%2520VeRi776%252C%2520Vehicle-3I%252C%250Aand%2520Market-1501%252C%2520achieving%2520up%2520to%252099.70%2525%2520rank-1%2520accuracy%2520and%252095.5%2525%2520mAP.%250AFurthermore%252C%2520the%2520proposed%2520CIM%2520approach%252C%2520which%2520dynamically%2520assigns%2520gallery%2520sets%250Abased%2520on%2520camera%2520network%2520topology%252C%2520has%2520further%2520improved%2520ReID%2520accuracy%2520and%250Arobustness%2520in%2520real-world%2520settings%252C%2520evidenced%2520by%2520a%252094.95%2525%2520mAP%2520and%2520a%252095.19%2525%2520F1%250Ascore%2520on%2520the%2520Vehicle-3I%2520dataset.%2520The%2520experimental%2520results%2520support%2520the%250Aeffectiveness%2520of%2520incorporating%2520spatial-temporal%2520information%2520and%2520CIM%2520for%250Areal-world%2520ReID%2520scenarios%252C%2520regardless%2520of%2520the%2520data%2520domain%2520%2528e.g.%252C%2520vehicle%252C%250Aperson%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05558v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Re-identification%20via%20Spatial-temporal%20Fusion%20Networks%20and%20Causal%0A%20%20Identity%20Matching&entry.906535625=Hye-Geun%20Kim%20and%20Yong-Hyuk%20Moon%20and%20Yeong-Jun%20Cho&entry.1292438233=%20%20Object%20re-identification%20%28ReID%29%20in%20large%20camera%20networks%20faces%20numerous%0Achallenges.%20First%2C%20the%20similar%20appearances%20of%20objects%20degrade%20ReID%20performance%2C%0Aa%20challenge%20that%20needs%20to%20be%20addressed%20by%20existing%20appearance-based%20ReID%0Amethods.%20Second%2C%20most%20ReID%20studies%20are%20performed%20in%20laboratory%20settings%20and%20do%0Anot%20consider%20real-world%20scenarios.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20a%0Anovel%20ReID%20framework%20that%20leverages%20a%20spatial-temporal%20fusion%20network%20and%0Acausal%20identity%20matching%20%28CIM%29.%20Our%20framework%20estimates%20camera%20network%20topology%0Ausing%20a%20proposed%20adaptive%20Parzen%20window%20and%20combines%20appearance%20features%20with%0Aspatial-temporal%20cues%20within%20the%20fusion%20network.%20This%20approach%20has%20demonstrated%0Aoutstanding%20performance%20across%20several%20datasets%2C%20including%20VeRi776%2C%20Vehicle-3I%2C%0Aand%20Market-1501%2C%20achieving%20up%20to%2099.70%25%20rank-1%20accuracy%20and%2095.5%25%20mAP.%0AFurthermore%2C%20the%20proposed%20CIM%20approach%2C%20which%20dynamically%20assigns%20gallery%20sets%0Abased%20on%20camera%20network%20topology%2C%20has%20further%20improved%20ReID%20accuracy%20and%0Arobustness%20in%20real-world%20settings%2C%20evidenced%20by%20a%2094.95%25%20mAP%20and%20a%2095.19%25%20F1%0Ascore%20on%20the%20Vehicle-3I%20dataset.%20The%20experimental%20results%20support%20the%0Aeffectiveness%20of%20incorporating%20spatial-temporal%20information%20and%20CIM%20for%0Areal-world%20ReID%20scenarios%2C%20regardless%20of%20the%20data%20domain%20%28e.g.%2C%20vehicle%2C%0Aperson%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05558v2&entry.124074799=Read"},
{"title": "StreamLTS: Query-based Temporal-Spatial LiDAR Fusion for Cooperative\n  Object Detection", "author": "Yunshuang Yuan and Monika Sester", "abstract": "  Cooperative perception via communication among intelligent traffic agents has\ngreat potential to improve the safety of autonomous driving. However, limited\ncommunication bandwidth, localization errors and asynchronized capturing time\nof sensor data, all introduce difficulties to the data fusion of different\nagents. To some extend, previous works have attempted to reduce the shared data\nsize, mitigate the spatial feature misalignment caused by localization errors\nand communication delay. However, none of them have considered the\nasynchronized sensor ticking times, which can lead to dynamic object\nmisplacement of more than one meter during data fusion. In this work, we\npropose Time-Aligned COoperative Object Detection (TA-COOD), for which we adapt\nwidely used dataset OPV2V and DairV2X with considering asynchronous LiDAR\nsensor ticking times and build an efficient fully sparse framework with\nmodeling the temporal information of individual objects with query-based\ntechniques. The experiment results confirmed the superior efficiency of our\nfully sparse framework compared to the state-of-the-art dense models. More\nimportantly, they show that the point-wise observation timestamps of the\ndynamic objects are crucial for accurate modeling the object temporal context\nand the predictability of their time-related locations. The official code is\navailable at \\url{https://github.com/YuanYunshuang/CoSense3D}.\n", "link": "http://arxiv.org/abs/2407.03825v2", "date": "2024-08-22", "relevancy": 2.7514, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.559}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5497}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamLTS%3A%20Query-based%20Temporal-Spatial%20LiDAR%20Fusion%20for%20Cooperative%0A%20%20Object%20Detection&body=Title%3A%20StreamLTS%3A%20Query-based%20Temporal-Spatial%20LiDAR%20Fusion%20for%20Cooperative%0A%20%20Object%20Detection%0AAuthor%3A%20Yunshuang%20Yuan%20and%20Monika%20Sester%0AAbstract%3A%20%20%20Cooperative%20perception%20via%20communication%20among%20intelligent%20traffic%20agents%20has%0Agreat%20potential%20to%20improve%20the%20safety%20of%20autonomous%20driving.%20However%2C%20limited%0Acommunication%20bandwidth%2C%20localization%20errors%20and%20asynchronized%20capturing%20time%0Aof%20sensor%20data%2C%20all%20introduce%20difficulties%20to%20the%20data%20fusion%20of%20different%0Aagents.%20To%20some%20extend%2C%20previous%20works%20have%20attempted%20to%20reduce%20the%20shared%20data%0Asize%2C%20mitigate%20the%20spatial%20feature%20misalignment%20caused%20by%20localization%20errors%0Aand%20communication%20delay.%20However%2C%20none%20of%20them%20have%20considered%20the%0Aasynchronized%20sensor%20ticking%20times%2C%20which%20can%20lead%20to%20dynamic%20object%0Amisplacement%20of%20more%20than%20one%20meter%20during%20data%20fusion.%20In%20this%20work%2C%20we%0Apropose%20Time-Aligned%20COoperative%20Object%20Detection%20%28TA-COOD%29%2C%20for%20which%20we%20adapt%0Awidely%20used%20dataset%20OPV2V%20and%20DairV2X%20with%20considering%20asynchronous%20LiDAR%0Asensor%20ticking%20times%20and%20build%20an%20efficient%20fully%20sparse%20framework%20with%0Amodeling%20the%20temporal%20information%20of%20individual%20objects%20with%20query-based%0Atechniques.%20The%20experiment%20results%20confirmed%20the%20superior%20efficiency%20of%20our%0Afully%20sparse%20framework%20compared%20to%20the%20state-of-the-art%20dense%20models.%20More%0Aimportantly%2C%20they%20show%20that%20the%20point-wise%20observation%20timestamps%20of%20the%0Adynamic%20objects%20are%20crucial%20for%20accurate%20modeling%20the%20object%20temporal%20context%0Aand%20the%20predictability%20of%20their%20time-related%20locations.%20The%20official%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/YuanYunshuang/CoSense3D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03825v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamLTS%253A%2520Query-based%2520Temporal-Spatial%2520LiDAR%2520Fusion%2520for%2520Cooperative%250A%2520%2520Object%2520Detection%26entry.906535625%3DYunshuang%2520Yuan%2520and%2520Monika%2520Sester%26entry.1292438233%3D%2520%2520Cooperative%2520perception%2520via%2520communication%2520among%2520intelligent%2520traffic%2520agents%2520has%250Agreat%2520potential%2520to%2520improve%2520the%2520safety%2520of%2520autonomous%2520driving.%2520However%252C%2520limited%250Acommunication%2520bandwidth%252C%2520localization%2520errors%2520and%2520asynchronized%2520capturing%2520time%250Aof%2520sensor%2520data%252C%2520all%2520introduce%2520difficulties%2520to%2520the%2520data%2520fusion%2520of%2520different%250Aagents.%2520To%2520some%2520extend%252C%2520previous%2520works%2520have%2520attempted%2520to%2520reduce%2520the%2520shared%2520data%250Asize%252C%2520mitigate%2520the%2520spatial%2520feature%2520misalignment%2520caused%2520by%2520localization%2520errors%250Aand%2520communication%2520delay.%2520However%252C%2520none%2520of%2520them%2520have%2520considered%2520the%250Aasynchronized%2520sensor%2520ticking%2520times%252C%2520which%2520can%2520lead%2520to%2520dynamic%2520object%250Amisplacement%2520of%2520more%2520than%2520one%2520meter%2520during%2520data%2520fusion.%2520In%2520this%2520work%252C%2520we%250Apropose%2520Time-Aligned%2520COoperative%2520Object%2520Detection%2520%2528TA-COOD%2529%252C%2520for%2520which%2520we%2520adapt%250Awidely%2520used%2520dataset%2520OPV2V%2520and%2520DairV2X%2520with%2520considering%2520asynchronous%2520LiDAR%250Asensor%2520ticking%2520times%2520and%2520build%2520an%2520efficient%2520fully%2520sparse%2520framework%2520with%250Amodeling%2520the%2520temporal%2520information%2520of%2520individual%2520objects%2520with%2520query-based%250Atechniques.%2520The%2520experiment%2520results%2520confirmed%2520the%2520superior%2520efficiency%2520of%2520our%250Afully%2520sparse%2520framework%2520compared%2520to%2520the%2520state-of-the-art%2520dense%2520models.%2520More%250Aimportantly%252C%2520they%2520show%2520that%2520the%2520point-wise%2520observation%2520timestamps%2520of%2520the%250Adynamic%2520objects%2520are%2520crucial%2520for%2520accurate%2520modeling%2520the%2520object%2520temporal%2520context%250Aand%2520the%2520predictability%2520of%2520their%2520time-related%2520locations.%2520The%2520official%2520code%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/YuanYunshuang/CoSense3D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03825v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamLTS%3A%20Query-based%20Temporal-Spatial%20LiDAR%20Fusion%20for%20Cooperative%0A%20%20Object%20Detection&entry.906535625=Yunshuang%20Yuan%20and%20Monika%20Sester&entry.1292438233=%20%20Cooperative%20perception%20via%20communication%20among%20intelligent%20traffic%20agents%20has%0Agreat%20potential%20to%20improve%20the%20safety%20of%20autonomous%20driving.%20However%2C%20limited%0Acommunication%20bandwidth%2C%20localization%20errors%20and%20asynchronized%20capturing%20time%0Aof%20sensor%20data%2C%20all%20introduce%20difficulties%20to%20the%20data%20fusion%20of%20different%0Aagents.%20To%20some%20extend%2C%20previous%20works%20have%20attempted%20to%20reduce%20the%20shared%20data%0Asize%2C%20mitigate%20the%20spatial%20feature%20misalignment%20caused%20by%20localization%20errors%0Aand%20communication%20delay.%20However%2C%20none%20of%20them%20have%20considered%20the%0Aasynchronized%20sensor%20ticking%20times%2C%20which%20can%20lead%20to%20dynamic%20object%0Amisplacement%20of%20more%20than%20one%20meter%20during%20data%20fusion.%20In%20this%20work%2C%20we%0Apropose%20Time-Aligned%20COoperative%20Object%20Detection%20%28TA-COOD%29%2C%20for%20which%20we%20adapt%0Awidely%20used%20dataset%20OPV2V%20and%20DairV2X%20with%20considering%20asynchronous%20LiDAR%0Asensor%20ticking%20times%20and%20build%20an%20efficient%20fully%20sparse%20framework%20with%0Amodeling%20the%20temporal%20information%20of%20individual%20objects%20with%20query-based%0Atechniques.%20The%20experiment%20results%20confirmed%20the%20superior%20efficiency%20of%20our%0Afully%20sparse%20framework%20compared%20to%20the%20state-of-the-art%20dense%20models.%20More%0Aimportantly%2C%20they%20show%20that%20the%20point-wise%20observation%20timestamps%20of%20the%0Adynamic%20objects%20are%20crucial%20for%20accurate%20modeling%20the%20object%20temporal%20context%0Aand%20the%20predictability%20of%20their%20time-related%20locations.%20The%20official%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/YuanYunshuang/CoSense3D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03825v2&entry.124074799=Read"},
{"title": "Envisioning Class Entity Reasoning by Large Language Models for Few-shot\n  Learning", "author": "Mushui Liu and Fangtai Wu and Bozheng Li and Ziqian Lu and Yunlong Yu and Xi Li", "abstract": "  Few-shot learning (FSL) aims to recognize new concepts using a limited number\nof visual samples. Existing approaches attempt to incorporate semantic\ninformation into the limited visual data for category understanding. However,\nthese methods often enrich class-level feature representations with abstract\ncategory names, failing to capture the nuanced features essential for effective\ngeneralization. To address this issue, we propose a novel framework for FSL,\nwhich incorporates both the abstract class semantics and the concrete class\nentities extracted from Large Language Models (LLMs), to enhance the\nrepresentation of the class prototypes. Specifically, our framework composes a\nSemantic-guided Visual Pattern Extraction (SVPE) module and a\nPrototype-Calibration (PC) module, where the SVPE meticulously extracts\nsemantic-aware visual patterns across diverse scales, while the PC module\nseamlessly integrates these patterns to refine the visual prototype, enhancing\nits representativeness. Extensive experiments on four few-shot classification\nbenchmarks and the BSCD-FSL cross-domain benchmarks showcase remarkable\nadvancements over the current state-of-the-art methods. Notably, for the\nchallenging one-shot setting, our approach, utilizing the ResNet-12 backbone,\nachieves an impressive average improvement of 1.95% over the second-best\ncompetitor.\n", "link": "http://arxiv.org/abs/2408.12469v1", "date": "2024-08-22", "relevancy": 2.7057, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5487}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5389}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Envisioning%20Class%20Entity%20Reasoning%20by%20Large%20Language%20Models%20for%20Few-shot%0A%20%20Learning&body=Title%3A%20Envisioning%20Class%20Entity%20Reasoning%20by%20Large%20Language%20Models%20for%20Few-shot%0A%20%20Learning%0AAuthor%3A%20Mushui%20Liu%20and%20Fangtai%20Wu%20and%20Bozheng%20Li%20and%20Ziqian%20Lu%20and%20Yunlong%20Yu%20and%20Xi%20Li%0AAbstract%3A%20%20%20Few-shot%20learning%20%28FSL%29%20aims%20to%20recognize%20new%20concepts%20using%20a%20limited%20number%0Aof%20visual%20samples.%20Existing%20approaches%20attempt%20to%20incorporate%20semantic%0Ainformation%20into%20the%20limited%20visual%20data%20for%20category%20understanding.%20However%2C%0Athese%20methods%20often%20enrich%20class-level%20feature%20representations%20with%20abstract%0Acategory%20names%2C%20failing%20to%20capture%20the%20nuanced%20features%20essential%20for%20effective%0Ageneralization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20for%20FSL%2C%0Awhich%20incorporates%20both%20the%20abstract%20class%20semantics%20and%20the%20concrete%20class%0Aentities%20extracted%20from%20Large%20Language%20Models%20%28LLMs%29%2C%20to%20enhance%20the%0Arepresentation%20of%20the%20class%20prototypes.%20Specifically%2C%20our%20framework%20composes%20a%0ASemantic-guided%20Visual%20Pattern%20Extraction%20%28SVPE%29%20module%20and%20a%0APrototype-Calibration%20%28PC%29%20module%2C%20where%20the%20SVPE%20meticulously%20extracts%0Asemantic-aware%20visual%20patterns%20across%20diverse%20scales%2C%20while%20the%20PC%20module%0Aseamlessly%20integrates%20these%20patterns%20to%20refine%20the%20visual%20prototype%2C%20enhancing%0Aits%20representativeness.%20Extensive%20experiments%20on%20four%20few-shot%20classification%0Abenchmarks%20and%20the%20BSCD-FSL%20cross-domain%20benchmarks%20showcase%20remarkable%0Aadvancements%20over%20the%20current%20state-of-the-art%20methods.%20Notably%2C%20for%20the%0Achallenging%20one-shot%20setting%2C%20our%20approach%2C%20utilizing%20the%20ResNet-12%20backbone%2C%0Aachieves%20an%20impressive%20average%20improvement%20of%201.95%25%20over%20the%20second-best%0Acompetitor.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvisioning%2520Class%2520Entity%2520Reasoning%2520by%2520Large%2520Language%2520Models%2520for%2520Few-shot%250A%2520%2520Learning%26entry.906535625%3DMushui%2520Liu%2520and%2520Fangtai%2520Wu%2520and%2520Bozheng%2520Li%2520and%2520Ziqian%2520Lu%2520and%2520Yunlong%2520Yu%2520and%2520Xi%2520Li%26entry.1292438233%3D%2520%2520Few-shot%2520learning%2520%2528FSL%2529%2520aims%2520to%2520recognize%2520new%2520concepts%2520using%2520a%2520limited%2520number%250Aof%2520visual%2520samples.%2520Existing%2520approaches%2520attempt%2520to%2520incorporate%2520semantic%250Ainformation%2520into%2520the%2520limited%2520visual%2520data%2520for%2520category%2520understanding.%2520However%252C%250Athese%2520methods%2520often%2520enrich%2520class-level%2520feature%2520representations%2520with%2520abstract%250Acategory%2520names%252C%2520failing%2520to%2520capture%2520the%2520nuanced%2520features%2520essential%2520for%2520effective%250Ageneralization.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520FSL%252C%250Awhich%2520incorporates%2520both%2520the%2520abstract%2520class%2520semantics%2520and%2520the%2520concrete%2520class%250Aentities%2520extracted%2520from%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520to%2520enhance%2520the%250Arepresentation%2520of%2520the%2520class%2520prototypes.%2520Specifically%252C%2520our%2520framework%2520composes%2520a%250ASemantic-guided%2520Visual%2520Pattern%2520Extraction%2520%2528SVPE%2529%2520module%2520and%2520a%250APrototype-Calibration%2520%2528PC%2529%2520module%252C%2520where%2520the%2520SVPE%2520meticulously%2520extracts%250Asemantic-aware%2520visual%2520patterns%2520across%2520diverse%2520scales%252C%2520while%2520the%2520PC%2520module%250Aseamlessly%2520integrates%2520these%2520patterns%2520to%2520refine%2520the%2520visual%2520prototype%252C%2520enhancing%250Aits%2520representativeness.%2520Extensive%2520experiments%2520on%2520four%2520few-shot%2520classification%250Abenchmarks%2520and%2520the%2520BSCD-FSL%2520cross-domain%2520benchmarks%2520showcase%2520remarkable%250Aadvancements%2520over%2520the%2520current%2520state-of-the-art%2520methods.%2520Notably%252C%2520for%2520the%250Achallenging%2520one-shot%2520setting%252C%2520our%2520approach%252C%2520utilizing%2520the%2520ResNet-12%2520backbone%252C%250Aachieves%2520an%2520impressive%2520average%2520improvement%2520of%25201.95%2525%2520over%2520the%2520second-best%250Acompetitor.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Envisioning%20Class%20Entity%20Reasoning%20by%20Large%20Language%20Models%20for%20Few-shot%0A%20%20Learning&entry.906535625=Mushui%20Liu%20and%20Fangtai%20Wu%20and%20Bozheng%20Li%20and%20Ziqian%20Lu%20and%20Yunlong%20Yu%20and%20Xi%20Li&entry.1292438233=%20%20Few-shot%20learning%20%28FSL%29%20aims%20to%20recognize%20new%20concepts%20using%20a%20limited%20number%0Aof%20visual%20samples.%20Existing%20approaches%20attempt%20to%20incorporate%20semantic%0Ainformation%20into%20the%20limited%20visual%20data%20for%20category%20understanding.%20However%2C%0Athese%20methods%20often%20enrich%20class-level%20feature%20representations%20with%20abstract%0Acategory%20names%2C%20failing%20to%20capture%20the%20nuanced%20features%20essential%20for%20effective%0Ageneralization.%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20framework%20for%20FSL%2C%0Awhich%20incorporates%20both%20the%20abstract%20class%20semantics%20and%20the%20concrete%20class%0Aentities%20extracted%20from%20Large%20Language%20Models%20%28LLMs%29%2C%20to%20enhance%20the%0Arepresentation%20of%20the%20class%20prototypes.%20Specifically%2C%20our%20framework%20composes%20a%0ASemantic-guided%20Visual%20Pattern%20Extraction%20%28SVPE%29%20module%20and%20a%0APrototype-Calibration%20%28PC%29%20module%2C%20where%20the%20SVPE%20meticulously%20extracts%0Asemantic-aware%20visual%20patterns%20across%20diverse%20scales%2C%20while%20the%20PC%20module%0Aseamlessly%20integrates%20these%20patterns%20to%20refine%20the%20visual%20prototype%2C%20enhancing%0Aits%20representativeness.%20Extensive%20experiments%20on%20four%20few-shot%20classification%0Abenchmarks%20and%20the%20BSCD-FSL%20cross-domain%20benchmarks%20showcase%20remarkable%0Aadvancements%20over%20the%20current%20state-of-the-art%20methods.%20Notably%2C%20for%20the%0Achallenging%20one-shot%20setting%2C%20our%20approach%2C%20utilizing%20the%20ResNet-12%20backbone%2C%0Aachieves%20an%20impressive%20average%20improvement%20of%201.95%25%20over%20the%20second-best%0Acompetitor.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12469v1&entry.124074799=Read"},
{"title": "Enhanced Expressivity in Graph Neural Networks with Lanczos-Based Linear\n  Constraints", "author": "Niloofar Azizi and Nils Kriege and Horst Bischof", "abstract": "  Graph Neural Networks (GNNs) excel in handling graph-structured data but\noften underperform in link prediction tasks compared to classical methods,\nmainly due to the limitations of the commonly used Message Passing GNNs\n(MPNNs). Notably, their ability to distinguish non-isomorphic graphs is limited\nby the 1-dimensional Weisfeiler-Lehman test. Our study presents a novel method\nto enhance the expressivity of GNNs by embedding induced subgraphs into the\ngraph Laplacian matrix's eigenbasis. We introduce a Learnable Lanczos algorithm\nwith Linear Constraints (LLwLC), proposing two novel subgraph extraction\nstrategies: encoding vertex-deleted subgraphs and applying Neumann eigenvalue\nconstraints. For the former, we conjecture that LLwLC establishes a universal\napproximator, offering efficient time complexity. The latter focuses on link\nrepresentations enabling differentiation between $k$-regular graphs and node\nautomorphism, a vital aspect for link prediction tasks. Our approach results in\nan extremely lightweight architecture, reducing the need for extensive training\ndatasets. Empirically, our method improves performance in challenging link\nprediction tasks across benchmark datasets, establishing its practical utility\nand supporting our theoretical findings. Notably, LLwLC achieves 20x and 10x\nspeedup by only requiring 5% and 10% data from the PubMed and OGBL-Vessel\ndatasets while comparing to the state-of-the-art.\n", "link": "http://arxiv.org/abs/2408.12334v1", "date": "2024-08-22", "relevancy": 2.6756, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5513}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5354}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Expressivity%20in%20Graph%20Neural%20Networks%20with%20Lanczos-Based%20Linear%0A%20%20Constraints&body=Title%3A%20Enhanced%20Expressivity%20in%20Graph%20Neural%20Networks%20with%20Lanczos-Based%20Linear%0A%20%20Constraints%0AAuthor%3A%20Niloofar%20Azizi%20and%20Nils%20Kriege%20and%20Horst%20Bischof%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20in%20handling%20graph-structured%20data%20but%0Aoften%20underperform%20in%20link%20prediction%20tasks%20compared%20to%20classical%20methods%2C%0Amainly%20due%20to%20the%20limitations%20of%20the%20commonly%20used%20Message%20Passing%20GNNs%0A%28MPNNs%29.%20Notably%2C%20their%20ability%20to%20distinguish%20non-isomorphic%20graphs%20is%20limited%0Aby%20the%201-dimensional%20Weisfeiler-Lehman%20test.%20Our%20study%20presents%20a%20novel%20method%0Ato%20enhance%20the%20expressivity%20of%20GNNs%20by%20embedding%20induced%20subgraphs%20into%20the%0Agraph%20Laplacian%20matrix%27s%20eigenbasis.%20We%20introduce%20a%20Learnable%20Lanczos%20algorithm%0Awith%20Linear%20Constraints%20%28LLwLC%29%2C%20proposing%20two%20novel%20subgraph%20extraction%0Astrategies%3A%20encoding%20vertex-deleted%20subgraphs%20and%20applying%20Neumann%20eigenvalue%0Aconstraints.%20For%20the%20former%2C%20we%20conjecture%20that%20LLwLC%20establishes%20a%20universal%0Aapproximator%2C%20offering%20efficient%20time%20complexity.%20The%20latter%20focuses%20on%20link%0Arepresentations%20enabling%20differentiation%20between%20%24k%24-regular%20graphs%20and%20node%0Aautomorphism%2C%20a%20vital%20aspect%20for%20link%20prediction%20tasks.%20Our%20approach%20results%20in%0Aan%20extremely%20lightweight%20architecture%2C%20reducing%20the%20need%20for%20extensive%20training%0Adatasets.%20Empirically%2C%20our%20method%20improves%20performance%20in%20challenging%20link%0Aprediction%20tasks%20across%20benchmark%20datasets%2C%20establishing%20its%20practical%20utility%0Aand%20supporting%20our%20theoretical%20findings.%20Notably%2C%20LLwLC%20achieves%2020x%20and%2010x%0Aspeedup%20by%20only%20requiring%205%25%20and%2010%25%20data%20from%20the%20PubMed%20and%20OGBL-Vessel%0Adatasets%20while%20comparing%20to%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Expressivity%2520in%2520Graph%2520Neural%2520Networks%2520with%2520Lanczos-Based%2520Linear%250A%2520%2520Constraints%26entry.906535625%3DNiloofar%2520Azizi%2520and%2520Nils%2520Kriege%2520and%2520Horst%2520Bischof%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520excel%2520in%2520handling%2520graph-structured%2520data%2520but%250Aoften%2520underperform%2520in%2520link%2520prediction%2520tasks%2520compared%2520to%2520classical%2520methods%252C%250Amainly%2520due%2520to%2520the%2520limitations%2520of%2520the%2520commonly%2520used%2520Message%2520Passing%2520GNNs%250A%2528MPNNs%2529.%2520Notably%252C%2520their%2520ability%2520to%2520distinguish%2520non-isomorphic%2520graphs%2520is%2520limited%250Aby%2520the%25201-dimensional%2520Weisfeiler-Lehman%2520test.%2520Our%2520study%2520presents%2520a%2520novel%2520method%250Ato%2520enhance%2520the%2520expressivity%2520of%2520GNNs%2520by%2520embedding%2520induced%2520subgraphs%2520into%2520the%250Agraph%2520Laplacian%2520matrix%2527s%2520eigenbasis.%2520We%2520introduce%2520a%2520Learnable%2520Lanczos%2520algorithm%250Awith%2520Linear%2520Constraints%2520%2528LLwLC%2529%252C%2520proposing%2520two%2520novel%2520subgraph%2520extraction%250Astrategies%253A%2520encoding%2520vertex-deleted%2520subgraphs%2520and%2520applying%2520Neumann%2520eigenvalue%250Aconstraints.%2520For%2520the%2520former%252C%2520we%2520conjecture%2520that%2520LLwLC%2520establishes%2520a%2520universal%250Aapproximator%252C%2520offering%2520efficient%2520time%2520complexity.%2520The%2520latter%2520focuses%2520on%2520link%250Arepresentations%2520enabling%2520differentiation%2520between%2520%2524k%2524-regular%2520graphs%2520and%2520node%250Aautomorphism%252C%2520a%2520vital%2520aspect%2520for%2520link%2520prediction%2520tasks.%2520Our%2520approach%2520results%2520in%250Aan%2520extremely%2520lightweight%2520architecture%252C%2520reducing%2520the%2520need%2520for%2520extensive%2520training%250Adatasets.%2520Empirically%252C%2520our%2520method%2520improves%2520performance%2520in%2520challenging%2520link%250Aprediction%2520tasks%2520across%2520benchmark%2520datasets%252C%2520establishing%2520its%2520practical%2520utility%250Aand%2520supporting%2520our%2520theoretical%2520findings.%2520Notably%252C%2520LLwLC%2520achieves%252020x%2520and%252010x%250Aspeedup%2520by%2520only%2520requiring%25205%2525%2520and%252010%2525%2520data%2520from%2520the%2520PubMed%2520and%2520OGBL-Vessel%250Adatasets%2520while%2520comparing%2520to%2520the%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Expressivity%20in%20Graph%20Neural%20Networks%20with%20Lanczos-Based%20Linear%0A%20%20Constraints&entry.906535625=Niloofar%20Azizi%20and%20Nils%20Kriege%20and%20Horst%20Bischof&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20in%20handling%20graph-structured%20data%20but%0Aoften%20underperform%20in%20link%20prediction%20tasks%20compared%20to%20classical%20methods%2C%0Amainly%20due%20to%20the%20limitations%20of%20the%20commonly%20used%20Message%20Passing%20GNNs%0A%28MPNNs%29.%20Notably%2C%20their%20ability%20to%20distinguish%20non-isomorphic%20graphs%20is%20limited%0Aby%20the%201-dimensional%20Weisfeiler-Lehman%20test.%20Our%20study%20presents%20a%20novel%20method%0Ato%20enhance%20the%20expressivity%20of%20GNNs%20by%20embedding%20induced%20subgraphs%20into%20the%0Agraph%20Laplacian%20matrix%27s%20eigenbasis.%20We%20introduce%20a%20Learnable%20Lanczos%20algorithm%0Awith%20Linear%20Constraints%20%28LLwLC%29%2C%20proposing%20two%20novel%20subgraph%20extraction%0Astrategies%3A%20encoding%20vertex-deleted%20subgraphs%20and%20applying%20Neumann%20eigenvalue%0Aconstraints.%20For%20the%20former%2C%20we%20conjecture%20that%20LLwLC%20establishes%20a%20universal%0Aapproximator%2C%20offering%20efficient%20time%20complexity.%20The%20latter%20focuses%20on%20link%0Arepresentations%20enabling%20differentiation%20between%20%24k%24-regular%20graphs%20and%20node%0Aautomorphism%2C%20a%20vital%20aspect%20for%20link%20prediction%20tasks.%20Our%20approach%20results%20in%0Aan%20extremely%20lightweight%20architecture%2C%20reducing%20the%20need%20for%20extensive%20training%0Adatasets.%20Empirically%2C%20our%20method%20improves%20performance%20in%20challenging%20link%0Aprediction%20tasks%20across%20benchmark%20datasets%2C%20establishing%20its%20practical%20utility%0Aand%20supporting%20our%20theoretical%20findings.%20Notably%2C%20LLwLC%20achieves%2020x%20and%2010x%0Aspeedup%20by%20only%20requiring%205%25%20and%2010%25%20data%20from%20the%20PubMed%20and%20OGBL-Vessel%0Adatasets%20while%20comparing%20to%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12334v1&entry.124074799=Read"},
{"title": "A Riemannian Approach for Spatiotemporal Analysis and Generation of 4D\n  Tree-shaped Structures", "author": "Tahmina Khanam and Hamid Laga and Mohammed Bennamoun and Guanjin Wang and Ferdous Sohel and Farid Boussaid and Guan Wang and Anuj Srivastava", "abstract": "  We propose the first comprehensive approach for modeling and analyzing the\nspatiotemporal shape variability in tree-like 4D objects, i.e., 3D objects\nwhose shapes bend, stretch, and change in their branching structure over time\nas they deform, grow, and interact with their environment. Our key contribution\nis the representation of tree-like 3D shapes using Square Root Velocity\nFunction Trees (SRVFT). By solving the spatial registration in the SRVFT space,\nwhich is equipped with an L2 metric, 4D tree-shaped structures become\ntime-parameterized trajectories in this space. This reduces the problem of\nmodeling and analyzing 4D tree-like shapes to that of modeling and analyzing\nelastic trajectories in the SRVFT space, where elasticity refers to time\nwarping. In this paper, we propose a novel mathematical representation of the\nshape space of such trajectories, a Riemannian metric on that space, and\ncomputational tools for fast and accurate spatiotemporal registration and\ngeodesics computation between 4D tree-shaped structures. Leveraging these\nbuilding blocks, we develop a full framework for modelling the spatiotemporal\nvariability using statistical models and generating novel 4D tree-like\nstructures from a set of exemplars. We demonstrate and validate the proposed\nframework using real 4D plant data.\n", "link": "http://arxiv.org/abs/2408.12443v1", "date": "2024-08-22", "relevancy": 2.6557, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5323}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5306}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Riemannian%20Approach%20for%20Spatiotemporal%20Analysis%20and%20Generation%20of%204D%0A%20%20Tree-shaped%20Structures&body=Title%3A%20A%20Riemannian%20Approach%20for%20Spatiotemporal%20Analysis%20and%20Generation%20of%204D%0A%20%20Tree-shaped%20Structures%0AAuthor%3A%20Tahmina%20Khanam%20and%20Hamid%20Laga%20and%20Mohammed%20Bennamoun%20and%20Guanjin%20Wang%20and%20Ferdous%20Sohel%20and%20Farid%20Boussaid%20and%20Guan%20Wang%20and%20Anuj%20Srivastava%0AAbstract%3A%20%20%20We%20propose%20the%20first%20comprehensive%20approach%20for%20modeling%20and%20analyzing%20the%0Aspatiotemporal%20shape%20variability%20in%20tree-like%204D%20objects%2C%20i.e.%2C%203D%20objects%0Awhose%20shapes%20bend%2C%20stretch%2C%20and%20change%20in%20their%20branching%20structure%20over%20time%0Aas%20they%20deform%2C%20grow%2C%20and%20interact%20with%20their%20environment.%20Our%20key%20contribution%0Ais%20the%20representation%20of%20tree-like%203D%20shapes%20using%20Square%20Root%20Velocity%0AFunction%20Trees%20%28SRVFT%29.%20By%20solving%20the%20spatial%20registration%20in%20the%20SRVFT%20space%2C%0Awhich%20is%20equipped%20with%20an%20L2%20metric%2C%204D%20tree-shaped%20structures%20become%0Atime-parameterized%20trajectories%20in%20this%20space.%20This%20reduces%20the%20problem%20of%0Amodeling%20and%20analyzing%204D%20tree-like%20shapes%20to%20that%20of%20modeling%20and%20analyzing%0Aelastic%20trajectories%20in%20the%20SRVFT%20space%2C%20where%20elasticity%20refers%20to%20time%0Awarping.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20mathematical%20representation%20of%20the%0Ashape%20space%20of%20such%20trajectories%2C%20a%20Riemannian%20metric%20on%20that%20space%2C%20and%0Acomputational%20tools%20for%20fast%20and%20accurate%20spatiotemporal%20registration%20and%0Ageodesics%20computation%20between%204D%20tree-shaped%20structures.%20Leveraging%20these%0Abuilding%20blocks%2C%20we%20develop%20a%20full%20framework%20for%20modelling%20the%20spatiotemporal%0Avariability%20using%20statistical%20models%20and%20generating%20novel%204D%20tree-like%0Astructures%20from%20a%20set%20of%20exemplars.%20We%20demonstrate%20and%20validate%20the%20proposed%0Aframework%20using%20real%204D%20plant%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Riemannian%2520Approach%2520for%2520Spatiotemporal%2520Analysis%2520and%2520Generation%2520of%25204D%250A%2520%2520Tree-shaped%2520Structures%26entry.906535625%3DTahmina%2520Khanam%2520and%2520Hamid%2520Laga%2520and%2520Mohammed%2520Bennamoun%2520and%2520Guanjin%2520Wang%2520and%2520Ferdous%2520Sohel%2520and%2520Farid%2520Boussaid%2520and%2520Guan%2520Wang%2520and%2520Anuj%2520Srivastava%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520first%2520comprehensive%2520approach%2520for%2520modeling%2520and%2520analyzing%2520the%250Aspatiotemporal%2520shape%2520variability%2520in%2520tree-like%25204D%2520objects%252C%2520i.e.%252C%25203D%2520objects%250Awhose%2520shapes%2520bend%252C%2520stretch%252C%2520and%2520change%2520in%2520their%2520branching%2520structure%2520over%2520time%250Aas%2520they%2520deform%252C%2520grow%252C%2520and%2520interact%2520with%2520their%2520environment.%2520Our%2520key%2520contribution%250Ais%2520the%2520representation%2520of%2520tree-like%25203D%2520shapes%2520using%2520Square%2520Root%2520Velocity%250AFunction%2520Trees%2520%2528SRVFT%2529.%2520By%2520solving%2520the%2520spatial%2520registration%2520in%2520the%2520SRVFT%2520space%252C%250Awhich%2520is%2520equipped%2520with%2520an%2520L2%2520metric%252C%25204D%2520tree-shaped%2520structures%2520become%250Atime-parameterized%2520trajectories%2520in%2520this%2520space.%2520This%2520reduces%2520the%2520problem%2520of%250Amodeling%2520and%2520analyzing%25204D%2520tree-like%2520shapes%2520to%2520that%2520of%2520modeling%2520and%2520analyzing%250Aelastic%2520trajectories%2520in%2520the%2520SRVFT%2520space%252C%2520where%2520elasticity%2520refers%2520to%2520time%250Awarping.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520mathematical%2520representation%2520of%2520the%250Ashape%2520space%2520of%2520such%2520trajectories%252C%2520a%2520Riemannian%2520metric%2520on%2520that%2520space%252C%2520and%250Acomputational%2520tools%2520for%2520fast%2520and%2520accurate%2520spatiotemporal%2520registration%2520and%250Ageodesics%2520computation%2520between%25204D%2520tree-shaped%2520structures.%2520Leveraging%2520these%250Abuilding%2520blocks%252C%2520we%2520develop%2520a%2520full%2520framework%2520for%2520modelling%2520the%2520spatiotemporal%250Avariability%2520using%2520statistical%2520models%2520and%2520generating%2520novel%25204D%2520tree-like%250Astructures%2520from%2520a%2520set%2520of%2520exemplars.%2520We%2520demonstrate%2520and%2520validate%2520the%2520proposed%250Aframework%2520using%2520real%25204D%2520plant%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Riemannian%20Approach%20for%20Spatiotemporal%20Analysis%20and%20Generation%20of%204D%0A%20%20Tree-shaped%20Structures&entry.906535625=Tahmina%20Khanam%20and%20Hamid%20Laga%20and%20Mohammed%20Bennamoun%20and%20Guanjin%20Wang%20and%20Ferdous%20Sohel%20and%20Farid%20Boussaid%20and%20Guan%20Wang%20and%20Anuj%20Srivastava&entry.1292438233=%20%20We%20propose%20the%20first%20comprehensive%20approach%20for%20modeling%20and%20analyzing%20the%0Aspatiotemporal%20shape%20variability%20in%20tree-like%204D%20objects%2C%20i.e.%2C%203D%20objects%0Awhose%20shapes%20bend%2C%20stretch%2C%20and%20change%20in%20their%20branching%20structure%20over%20time%0Aas%20they%20deform%2C%20grow%2C%20and%20interact%20with%20their%20environment.%20Our%20key%20contribution%0Ais%20the%20representation%20of%20tree-like%203D%20shapes%20using%20Square%20Root%20Velocity%0AFunction%20Trees%20%28SRVFT%29.%20By%20solving%20the%20spatial%20registration%20in%20the%20SRVFT%20space%2C%0Awhich%20is%20equipped%20with%20an%20L2%20metric%2C%204D%20tree-shaped%20structures%20become%0Atime-parameterized%20trajectories%20in%20this%20space.%20This%20reduces%20the%20problem%20of%0Amodeling%20and%20analyzing%204D%20tree-like%20shapes%20to%20that%20of%20modeling%20and%20analyzing%0Aelastic%20trajectories%20in%20the%20SRVFT%20space%2C%20where%20elasticity%20refers%20to%20time%0Awarping.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20mathematical%20representation%20of%20the%0Ashape%20space%20of%20such%20trajectories%2C%20a%20Riemannian%20metric%20on%20that%20space%2C%20and%0Acomputational%20tools%20for%20fast%20and%20accurate%20spatiotemporal%20registration%20and%0Ageodesics%20computation%20between%204D%20tree-shaped%20structures.%20Leveraging%20these%0Abuilding%20blocks%2C%20we%20develop%20a%20full%20framework%20for%20modelling%20the%20spatiotemporal%0Avariability%20using%20statistical%20models%20and%20generating%20novel%204D%20tree-like%0Astructures%20from%20a%20set%20of%20exemplars.%20We%20demonstrate%20and%20validate%20the%20proposed%0Aframework%20using%20real%204D%20plant%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12443v1&entry.124074799=Read"},
{"title": "Scribbles for All: Benchmarking Scribble Supervised Segmentation Across\n  Datasets", "author": "Wolfgang Boettcher and Lukas Hoyer and Ozan Unal and Jan Eric Lenssen and Bernt Schiele", "abstract": "  In this work, we introduce Scribbles for All, a label and training data\ngeneration algorithm for semantic segmentation trained on scribble labels.\nTraining or fine-tuning semantic segmentation models with weak supervision has\nbecome an important topic recently and was subject to significant advances in\nmodel quality. In this setting, scribbles are a promising label type to achieve\nhigh quality segmentation results while requiring a much lower annotation\neffort than usual pixel-wise dense semantic segmentation annotations. The main\nlimitation of scribbles as source for weak supervision is the lack of\nchallenging datasets for scribble segmentation, which hinders the development\nof novel methods and conclusive evaluations. To overcome this limitation,\nScribbles for All provides scribble labels for several popular segmentation\ndatasets and provides an algorithm to automatically generate scribble labels\nfor any dataset with dense annotations, paving the way for new insights and\nmodel advancements in the field of weakly supervised segmentation. In addition\nto providing datasets and algorithm, we evaluate state-of-the-art segmentation\nmodels on our datasets and show that models trained with our synthetic labels\nperform competitively with respect to models trained on manual labels. Thus,\nour datasets enable state-of-the-art research into methods for scribble-labeled\nsemantic segmentation. The datasets, scribble generation algorithm, and\nbaselines are publicly available at https://github.com/wbkit/Scribbles4All\n", "link": "http://arxiv.org/abs/2408.12489v1", "date": "2024-08-22", "relevancy": 2.5117, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5193}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5036}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scribbles%20for%20All%3A%20Benchmarking%20Scribble%20Supervised%20Segmentation%20Across%0A%20%20Datasets&body=Title%3A%20Scribbles%20for%20All%3A%20Benchmarking%20Scribble%20Supervised%20Segmentation%20Across%0A%20%20Datasets%0AAuthor%3A%20Wolfgang%20Boettcher%20and%20Lukas%20Hoyer%20and%20Ozan%20Unal%20and%20Jan%20Eric%20Lenssen%20and%20Bernt%20Schiele%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20Scribbles%20for%20All%2C%20a%20label%20and%20training%20data%0Ageneration%20algorithm%20for%20semantic%20segmentation%20trained%20on%20scribble%20labels.%0ATraining%20or%20fine-tuning%20semantic%20segmentation%20models%20with%20weak%20supervision%20has%0Abecome%20an%20important%20topic%20recently%20and%20was%20subject%20to%20significant%20advances%20in%0Amodel%20quality.%20In%20this%20setting%2C%20scribbles%20are%20a%20promising%20label%20type%20to%20achieve%0Ahigh%20quality%20segmentation%20results%20while%20requiring%20a%20much%20lower%20annotation%0Aeffort%20than%20usual%20pixel-wise%20dense%20semantic%20segmentation%20annotations.%20The%20main%0Alimitation%20of%20scribbles%20as%20source%20for%20weak%20supervision%20is%20the%20lack%20of%0Achallenging%20datasets%20for%20scribble%20segmentation%2C%20which%20hinders%20the%20development%0Aof%20novel%20methods%20and%20conclusive%20evaluations.%20To%20overcome%20this%20limitation%2C%0AScribbles%20for%20All%20provides%20scribble%20labels%20for%20several%20popular%20segmentation%0Adatasets%20and%20provides%20an%20algorithm%20to%20automatically%20generate%20scribble%20labels%0Afor%20any%20dataset%20with%20dense%20annotations%2C%20paving%20the%20way%20for%20new%20insights%20and%0Amodel%20advancements%20in%20the%20field%20of%20weakly%20supervised%20segmentation.%20In%20addition%0Ato%20providing%20datasets%20and%20algorithm%2C%20we%20evaluate%20state-of-the-art%20segmentation%0Amodels%20on%20our%20datasets%20and%20show%20that%20models%20trained%20with%20our%20synthetic%20labels%0Aperform%20competitively%20with%20respect%20to%20models%20trained%20on%20manual%20labels.%20Thus%2C%0Aour%20datasets%20enable%20state-of-the-art%20research%20into%20methods%20for%20scribble-labeled%0Asemantic%20segmentation.%20The%20datasets%2C%20scribble%20generation%20algorithm%2C%20and%0Abaselines%20are%20publicly%20available%20at%20https%3A//github.com/wbkit/Scribbles4All%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScribbles%2520for%2520All%253A%2520Benchmarking%2520Scribble%2520Supervised%2520Segmentation%2520Across%250A%2520%2520Datasets%26entry.906535625%3DWolfgang%2520Boettcher%2520and%2520Lukas%2520Hoyer%2520and%2520Ozan%2520Unal%2520and%2520Jan%2520Eric%2520Lenssen%2520and%2520Bernt%2520Schiele%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Scribbles%2520for%2520All%252C%2520a%2520label%2520and%2520training%2520data%250Ageneration%2520algorithm%2520for%2520semantic%2520segmentation%2520trained%2520on%2520scribble%2520labels.%250ATraining%2520or%2520fine-tuning%2520semantic%2520segmentation%2520models%2520with%2520weak%2520supervision%2520has%250Abecome%2520an%2520important%2520topic%2520recently%2520and%2520was%2520subject%2520to%2520significant%2520advances%2520in%250Amodel%2520quality.%2520In%2520this%2520setting%252C%2520scribbles%2520are%2520a%2520promising%2520label%2520type%2520to%2520achieve%250Ahigh%2520quality%2520segmentation%2520results%2520while%2520requiring%2520a%2520much%2520lower%2520annotation%250Aeffort%2520than%2520usual%2520pixel-wise%2520dense%2520semantic%2520segmentation%2520annotations.%2520The%2520main%250Alimitation%2520of%2520scribbles%2520as%2520source%2520for%2520weak%2520supervision%2520is%2520the%2520lack%2520of%250Achallenging%2520datasets%2520for%2520scribble%2520segmentation%252C%2520which%2520hinders%2520the%2520development%250Aof%2520novel%2520methods%2520and%2520conclusive%2520evaluations.%2520To%2520overcome%2520this%2520limitation%252C%250AScribbles%2520for%2520All%2520provides%2520scribble%2520labels%2520for%2520several%2520popular%2520segmentation%250Adatasets%2520and%2520provides%2520an%2520algorithm%2520to%2520automatically%2520generate%2520scribble%2520labels%250Afor%2520any%2520dataset%2520with%2520dense%2520annotations%252C%2520paving%2520the%2520way%2520for%2520new%2520insights%2520and%250Amodel%2520advancements%2520in%2520the%2520field%2520of%2520weakly%2520supervised%2520segmentation.%2520In%2520addition%250Ato%2520providing%2520datasets%2520and%2520algorithm%252C%2520we%2520evaluate%2520state-of-the-art%2520segmentation%250Amodels%2520on%2520our%2520datasets%2520and%2520show%2520that%2520models%2520trained%2520with%2520our%2520synthetic%2520labels%250Aperform%2520competitively%2520with%2520respect%2520to%2520models%2520trained%2520on%2520manual%2520labels.%2520Thus%252C%250Aour%2520datasets%2520enable%2520state-of-the-art%2520research%2520into%2520methods%2520for%2520scribble-labeled%250Asemantic%2520segmentation.%2520The%2520datasets%252C%2520scribble%2520generation%2520algorithm%252C%2520and%250Abaselines%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/wbkit/Scribbles4All%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scribbles%20for%20All%3A%20Benchmarking%20Scribble%20Supervised%20Segmentation%20Across%0A%20%20Datasets&entry.906535625=Wolfgang%20Boettcher%20and%20Lukas%20Hoyer%20and%20Ozan%20Unal%20and%20Jan%20Eric%20Lenssen%20and%20Bernt%20Schiele&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20Scribbles%20for%20All%2C%20a%20label%20and%20training%20data%0Ageneration%20algorithm%20for%20semantic%20segmentation%20trained%20on%20scribble%20labels.%0ATraining%20or%20fine-tuning%20semantic%20segmentation%20models%20with%20weak%20supervision%20has%0Abecome%20an%20important%20topic%20recently%20and%20was%20subject%20to%20significant%20advances%20in%0Amodel%20quality.%20In%20this%20setting%2C%20scribbles%20are%20a%20promising%20label%20type%20to%20achieve%0Ahigh%20quality%20segmentation%20results%20while%20requiring%20a%20much%20lower%20annotation%0Aeffort%20than%20usual%20pixel-wise%20dense%20semantic%20segmentation%20annotations.%20The%20main%0Alimitation%20of%20scribbles%20as%20source%20for%20weak%20supervision%20is%20the%20lack%20of%0Achallenging%20datasets%20for%20scribble%20segmentation%2C%20which%20hinders%20the%20development%0Aof%20novel%20methods%20and%20conclusive%20evaluations.%20To%20overcome%20this%20limitation%2C%0AScribbles%20for%20All%20provides%20scribble%20labels%20for%20several%20popular%20segmentation%0Adatasets%20and%20provides%20an%20algorithm%20to%20automatically%20generate%20scribble%20labels%0Afor%20any%20dataset%20with%20dense%20annotations%2C%20paving%20the%20way%20for%20new%20insights%20and%0Amodel%20advancements%20in%20the%20field%20of%20weakly%20supervised%20segmentation.%20In%20addition%0Ato%20providing%20datasets%20and%20algorithm%2C%20we%20evaluate%20state-of-the-art%20segmentation%0Amodels%20on%20our%20datasets%20and%20show%20that%20models%20trained%20with%20our%20synthetic%20labels%0Aperform%20competitively%20with%20respect%20to%20models%20trained%20on%20manual%20labels.%20Thus%2C%0Aour%20datasets%20enable%20state-of-the-art%20research%20into%20methods%20for%20scribble-labeled%0Asemantic%20segmentation.%20The%20datasets%2C%20scribble%20generation%20algorithm%2C%20and%0Abaselines%20are%20publicly%20available%20at%20https%3A//github.com/wbkit/Scribbles4All%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12489v1&entry.124074799=Read"},
{"title": "GarmentAligner: Text-to-Garment Generation via Retrieval-augmented\n  Multi-level Corrections", "author": "Shiyue Zhang and Zheng Chong and Xujie Zhang and Hanhui Li and Yuhao Cheng and Yiqiang Yan and Xiaodan Liang", "abstract": "  General text-to-image models bring revolutionary innovation to the fields of\narts, design, and media. However, when applied to garment generation, even the\nstate-of-the-art text-to-image models suffer from fine-grained semantic\nmisalignment, particularly concerning the quantity, position, and\ninterrelations of garment components. Addressing this, we propose\nGarmentAligner, a text-to-garment diffusion model trained with\nretrieval-augmented multi-level corrections. To achieve semantic alignment at\nthe component level, we introduce an automatic component extraction pipeline to\nobtain spatial and quantitative information of garment components from\ncorresponding images and captions. Subsequently, to exploit component\nrelationships within the garment images, we construct retrieval subsets for\neach garment by retrieval augmentation based on component-level similarity\nranking and conduct contrastive learning to enhance the model perception of\ncomponents from positive and negative samples. To further enhance the alignment\nof components across semantic, spatial, and quantitative granularities, we\npropose the utilization of multi-level correction losses that leverage detailed\ncomponent information. The experimental findings demonstrate that\nGarmentAligner achieves superior fidelity and fine-grained semantic alignment\nwhen compared to existing competitors.\n", "link": "http://arxiv.org/abs/2408.12352v1", "date": "2024-08-22", "relevancy": 2.4963, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7036}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6129}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GarmentAligner%3A%20Text-to-Garment%20Generation%20via%20Retrieval-augmented%0A%20%20Multi-level%20Corrections&body=Title%3A%20GarmentAligner%3A%20Text-to-Garment%20Generation%20via%20Retrieval-augmented%0A%20%20Multi-level%20Corrections%0AAuthor%3A%20Shiyue%20Zhang%20and%20Zheng%20Chong%20and%20Xujie%20Zhang%20and%20Hanhui%20Li%20and%20Yuhao%20Cheng%20and%20Yiqiang%20Yan%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20General%20text-to-image%20models%20bring%20revolutionary%20innovation%20to%20the%20fields%20of%0Aarts%2C%20design%2C%20and%20media.%20However%2C%20when%20applied%20to%20garment%20generation%2C%20even%20the%0Astate-of-the-art%20text-to-image%20models%20suffer%20from%20fine-grained%20semantic%0Amisalignment%2C%20particularly%20concerning%20the%20quantity%2C%20position%2C%20and%0Ainterrelations%20of%20garment%20components.%20Addressing%20this%2C%20we%20propose%0AGarmentAligner%2C%20a%20text-to-garment%20diffusion%20model%20trained%20with%0Aretrieval-augmented%20multi-level%20corrections.%20To%20achieve%20semantic%20alignment%20at%0Athe%20component%20level%2C%20we%20introduce%20an%20automatic%20component%20extraction%20pipeline%20to%0Aobtain%20spatial%20and%20quantitative%20information%20of%20garment%20components%20from%0Acorresponding%20images%20and%20captions.%20Subsequently%2C%20to%20exploit%20component%0Arelationships%20within%20the%20garment%20images%2C%20we%20construct%20retrieval%20subsets%20for%0Aeach%20garment%20by%20retrieval%20augmentation%20based%20on%20component-level%20similarity%0Aranking%20and%20conduct%20contrastive%20learning%20to%20enhance%20the%20model%20perception%20of%0Acomponents%20from%20positive%20and%20negative%20samples.%20To%20further%20enhance%20the%20alignment%0Aof%20components%20across%20semantic%2C%20spatial%2C%20and%20quantitative%20granularities%2C%20we%0Apropose%20the%20utilization%20of%20multi-level%20correction%20losses%20that%20leverage%20detailed%0Acomponent%20information.%20The%20experimental%20findings%20demonstrate%20that%0AGarmentAligner%20achieves%20superior%20fidelity%20and%20fine-grained%20semantic%20alignment%0Awhen%20compared%20to%20existing%20competitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGarmentAligner%253A%2520Text-to-Garment%2520Generation%2520via%2520Retrieval-augmented%250A%2520%2520Multi-level%2520Corrections%26entry.906535625%3DShiyue%2520Zhang%2520and%2520Zheng%2520Chong%2520and%2520Xujie%2520Zhang%2520and%2520Hanhui%2520Li%2520and%2520Yuhao%2520Cheng%2520and%2520Yiqiang%2520Yan%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3D%2520%2520General%2520text-to-image%2520models%2520bring%2520revolutionary%2520innovation%2520to%2520the%2520fields%2520of%250Aarts%252C%2520design%252C%2520and%2520media.%2520However%252C%2520when%2520applied%2520to%2520garment%2520generation%252C%2520even%2520the%250Astate-of-the-art%2520text-to-image%2520models%2520suffer%2520from%2520fine-grained%2520semantic%250Amisalignment%252C%2520particularly%2520concerning%2520the%2520quantity%252C%2520position%252C%2520and%250Ainterrelations%2520of%2520garment%2520components.%2520Addressing%2520this%252C%2520we%2520propose%250AGarmentAligner%252C%2520a%2520text-to-garment%2520diffusion%2520model%2520trained%2520with%250Aretrieval-augmented%2520multi-level%2520corrections.%2520To%2520achieve%2520semantic%2520alignment%2520at%250Athe%2520component%2520level%252C%2520we%2520introduce%2520an%2520automatic%2520component%2520extraction%2520pipeline%2520to%250Aobtain%2520spatial%2520and%2520quantitative%2520information%2520of%2520garment%2520components%2520from%250Acorresponding%2520images%2520and%2520captions.%2520Subsequently%252C%2520to%2520exploit%2520component%250Arelationships%2520within%2520the%2520garment%2520images%252C%2520we%2520construct%2520retrieval%2520subsets%2520for%250Aeach%2520garment%2520by%2520retrieval%2520augmentation%2520based%2520on%2520component-level%2520similarity%250Aranking%2520and%2520conduct%2520contrastive%2520learning%2520to%2520enhance%2520the%2520model%2520perception%2520of%250Acomponents%2520from%2520positive%2520and%2520negative%2520samples.%2520To%2520further%2520enhance%2520the%2520alignment%250Aof%2520components%2520across%2520semantic%252C%2520spatial%252C%2520and%2520quantitative%2520granularities%252C%2520we%250Apropose%2520the%2520utilization%2520of%2520multi-level%2520correction%2520losses%2520that%2520leverage%2520detailed%250Acomponent%2520information.%2520The%2520experimental%2520findings%2520demonstrate%2520that%250AGarmentAligner%2520achieves%2520superior%2520fidelity%2520and%2520fine-grained%2520semantic%2520alignment%250Awhen%2520compared%2520to%2520existing%2520competitors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GarmentAligner%3A%20Text-to-Garment%20Generation%20via%20Retrieval-augmented%0A%20%20Multi-level%20Corrections&entry.906535625=Shiyue%20Zhang%20and%20Zheng%20Chong%20and%20Xujie%20Zhang%20and%20Hanhui%20Li%20and%20Yuhao%20Cheng%20and%20Yiqiang%20Yan%20and%20Xiaodan%20Liang&entry.1292438233=%20%20General%20text-to-image%20models%20bring%20revolutionary%20innovation%20to%20the%20fields%20of%0Aarts%2C%20design%2C%20and%20media.%20However%2C%20when%20applied%20to%20garment%20generation%2C%20even%20the%0Astate-of-the-art%20text-to-image%20models%20suffer%20from%20fine-grained%20semantic%0Amisalignment%2C%20particularly%20concerning%20the%20quantity%2C%20position%2C%20and%0Ainterrelations%20of%20garment%20components.%20Addressing%20this%2C%20we%20propose%0AGarmentAligner%2C%20a%20text-to-garment%20diffusion%20model%20trained%20with%0Aretrieval-augmented%20multi-level%20corrections.%20To%20achieve%20semantic%20alignment%20at%0Athe%20component%20level%2C%20we%20introduce%20an%20automatic%20component%20extraction%20pipeline%20to%0Aobtain%20spatial%20and%20quantitative%20information%20of%20garment%20components%20from%0Acorresponding%20images%20and%20captions.%20Subsequently%2C%20to%20exploit%20component%0Arelationships%20within%20the%20garment%20images%2C%20we%20construct%20retrieval%20subsets%20for%0Aeach%20garment%20by%20retrieval%20augmentation%20based%20on%20component-level%20similarity%0Aranking%20and%20conduct%20contrastive%20learning%20to%20enhance%20the%20model%20perception%20of%0Acomponents%20from%20positive%20and%20negative%20samples.%20To%20further%20enhance%20the%20alignment%0Aof%20components%20across%20semantic%2C%20spatial%2C%20and%20quantitative%20granularities%2C%20we%0Apropose%20the%20utilization%20of%20multi-level%20correction%20losses%20that%20leverage%20detailed%0Acomponent%20information.%20The%20experimental%20findings%20demonstrate%20that%0AGarmentAligner%20achieves%20superior%20fidelity%20and%20fine-grained%20semantic%20alignment%0Awhen%20compared%20to%20existing%20competitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12352v1&entry.124074799=Read"},
{"title": "Beyond Shortsighted Navigation: Merging Best View Trajectory Planning\n  with Robot Navigation", "author": "Srinath Tankasala and Roberto Mart\u00edn-Mart\u00edn and Mitch Pryor", "abstract": "  Gathering visual information effectively to monitor known environments is a\nkey challenge in robotics. To be as efficient as human surveyors, robotic\nsystems must continuously collect observational data required to complete their\nsurvey task. Inspection personnel instinctively know to look at relevant\nequipment that happens to be ``along the way.'' In this paper, we introduce a\nnovel framework for continuous long-horizon viewpoint planning, for ground\nrobots, applied to tasks involving patrolling, monitoring or visual data\ngathering in known environments. Our approach to Long Horizon Viewpoint\nPlanning (LHVP), enables the robot to autonomously navigate and collect\nenvironmental data optimizing for coverage over the horizon of the patrol.\nLeveraging a quadruped's mobility and sensory capabilities, our LHVP framework\nplans patrol paths that account for coupling the viewpoint planner for the arm\ncamera with the mobile base's navigation planner. The viewpath optimization\nalgorithm seeks a balance between comprehensive environmental coverage and\ndynamically feasible movements, thus ensuring prolonged and effective operation\nin scenarios including monitoring, security surveillance, and disaster\nresponse. We validate our approach through simulations and in the real world\nand show that our LHVP significantly outperforms naive patrolling methods in\nterms of area coverage generating information-gathering trajectories for the\nrobot arm. Our results indicate a promising direction for the deployment of\nmobile robots in long-term, autonomous surveying, and environmental data\ncollection tasks, highlighting the potential of intelligent robotic systems in\nchallenging real-world applications.\n", "link": "http://arxiv.org/abs/2408.12513v1", "date": "2024-08-22", "relevancy": 2.4803, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6486}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6007}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Shortsighted%20Navigation%3A%20Merging%20Best%20View%20Trajectory%20Planning%0A%20%20with%20Robot%20Navigation&body=Title%3A%20Beyond%20Shortsighted%20Navigation%3A%20Merging%20Best%20View%20Trajectory%20Planning%0A%20%20with%20Robot%20Navigation%0AAuthor%3A%20Srinath%20Tankasala%20and%20Roberto%20Mart%C3%ADn-Mart%C3%ADn%20and%20Mitch%20Pryor%0AAbstract%3A%20%20%20Gathering%20visual%20information%20effectively%20to%20monitor%20known%20environments%20is%20a%0Akey%20challenge%20in%20robotics.%20To%20be%20as%20efficient%20as%20human%20surveyors%2C%20robotic%0Asystems%20must%20continuously%20collect%20observational%20data%20required%20to%20complete%20their%0Asurvey%20task.%20Inspection%20personnel%20instinctively%20know%20to%20look%20at%20relevant%0Aequipment%20that%20happens%20to%20be%20%60%60along%20the%20way.%27%27%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20framework%20for%20continuous%20long-horizon%20viewpoint%20planning%2C%20for%20ground%0Arobots%2C%20applied%20to%20tasks%20involving%20patrolling%2C%20monitoring%20or%20visual%20data%0Agathering%20in%20known%20environments.%20Our%20approach%20to%20Long%20Horizon%20Viewpoint%0APlanning%20%28LHVP%29%2C%20enables%20the%20robot%20to%20autonomously%20navigate%20and%20collect%0Aenvironmental%20data%20optimizing%20for%20coverage%20over%20the%20horizon%20of%20the%20patrol.%0ALeveraging%20a%20quadruped%27s%20mobility%20and%20sensory%20capabilities%2C%20our%20LHVP%20framework%0Aplans%20patrol%20paths%20that%20account%20for%20coupling%20the%20viewpoint%20planner%20for%20the%20arm%0Acamera%20with%20the%20mobile%20base%27s%20navigation%20planner.%20The%20viewpath%20optimization%0Aalgorithm%20seeks%20a%20balance%20between%20comprehensive%20environmental%20coverage%20and%0Adynamically%20feasible%20movements%2C%20thus%20ensuring%20prolonged%20and%20effective%20operation%0Ain%20scenarios%20including%20monitoring%2C%20security%20surveillance%2C%20and%20disaster%0Aresponse.%20We%20validate%20our%20approach%20through%20simulations%20and%20in%20the%20real%20world%0Aand%20show%20that%20our%20LHVP%20significantly%20outperforms%20naive%20patrolling%20methods%20in%0Aterms%20of%20area%20coverage%20generating%20information-gathering%20trajectories%20for%20the%0Arobot%20arm.%20Our%20results%20indicate%20a%20promising%20direction%20for%20the%20deployment%20of%0Amobile%20robots%20in%20long-term%2C%20autonomous%20surveying%2C%20and%20environmental%20data%0Acollection%20tasks%2C%20highlighting%20the%20potential%20of%20intelligent%20robotic%20systems%20in%0Achallenging%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Shortsighted%2520Navigation%253A%2520Merging%2520Best%2520View%2520Trajectory%2520Planning%250A%2520%2520with%2520Robot%2520Navigation%26entry.906535625%3DSrinath%2520Tankasala%2520and%2520Roberto%2520Mart%25C3%25ADn-Mart%25C3%25ADn%2520and%2520Mitch%2520Pryor%26entry.1292438233%3D%2520%2520Gathering%2520visual%2520information%2520effectively%2520to%2520monitor%2520known%2520environments%2520is%2520a%250Akey%2520challenge%2520in%2520robotics.%2520To%2520be%2520as%2520efficient%2520as%2520human%2520surveyors%252C%2520robotic%250Asystems%2520must%2520continuously%2520collect%2520observational%2520data%2520required%2520to%2520complete%2520their%250Asurvey%2520task.%2520Inspection%2520personnel%2520instinctively%2520know%2520to%2520look%2520at%2520relevant%250Aequipment%2520that%2520happens%2520to%2520be%2520%2560%2560along%2520the%2520way.%2527%2527%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Anovel%2520framework%2520for%2520continuous%2520long-horizon%2520viewpoint%2520planning%252C%2520for%2520ground%250Arobots%252C%2520applied%2520to%2520tasks%2520involving%2520patrolling%252C%2520monitoring%2520or%2520visual%2520data%250Agathering%2520in%2520known%2520environments.%2520Our%2520approach%2520to%2520Long%2520Horizon%2520Viewpoint%250APlanning%2520%2528LHVP%2529%252C%2520enables%2520the%2520robot%2520to%2520autonomously%2520navigate%2520and%2520collect%250Aenvironmental%2520data%2520optimizing%2520for%2520coverage%2520over%2520the%2520horizon%2520of%2520the%2520patrol.%250ALeveraging%2520a%2520quadruped%2527s%2520mobility%2520and%2520sensory%2520capabilities%252C%2520our%2520LHVP%2520framework%250Aplans%2520patrol%2520paths%2520that%2520account%2520for%2520coupling%2520the%2520viewpoint%2520planner%2520for%2520the%2520arm%250Acamera%2520with%2520the%2520mobile%2520base%2527s%2520navigation%2520planner.%2520The%2520viewpath%2520optimization%250Aalgorithm%2520seeks%2520a%2520balance%2520between%2520comprehensive%2520environmental%2520coverage%2520and%250Adynamically%2520feasible%2520movements%252C%2520thus%2520ensuring%2520prolonged%2520and%2520effective%2520operation%250Ain%2520scenarios%2520including%2520monitoring%252C%2520security%2520surveillance%252C%2520and%2520disaster%250Aresponse.%2520We%2520validate%2520our%2520approach%2520through%2520simulations%2520and%2520in%2520the%2520real%2520world%250Aand%2520show%2520that%2520our%2520LHVP%2520significantly%2520outperforms%2520naive%2520patrolling%2520methods%2520in%250Aterms%2520of%2520area%2520coverage%2520generating%2520information-gathering%2520trajectories%2520for%2520the%250Arobot%2520arm.%2520Our%2520results%2520indicate%2520a%2520promising%2520direction%2520for%2520the%2520deployment%2520of%250Amobile%2520robots%2520in%2520long-term%252C%2520autonomous%2520surveying%252C%2520and%2520environmental%2520data%250Acollection%2520tasks%252C%2520highlighting%2520the%2520potential%2520of%2520intelligent%2520robotic%2520systems%2520in%250Achallenging%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Shortsighted%20Navigation%3A%20Merging%20Best%20View%20Trajectory%20Planning%0A%20%20with%20Robot%20Navigation&entry.906535625=Srinath%20Tankasala%20and%20Roberto%20Mart%C3%ADn-Mart%C3%ADn%20and%20Mitch%20Pryor&entry.1292438233=%20%20Gathering%20visual%20information%20effectively%20to%20monitor%20known%20environments%20is%20a%0Akey%20challenge%20in%20robotics.%20To%20be%20as%20efficient%20as%20human%20surveyors%2C%20robotic%0Asystems%20must%20continuously%20collect%20observational%20data%20required%20to%20complete%20their%0Asurvey%20task.%20Inspection%20personnel%20instinctively%20know%20to%20look%20at%20relevant%0Aequipment%20that%20happens%20to%20be%20%60%60along%20the%20way.%27%27%20In%20this%20paper%2C%20we%20introduce%20a%0Anovel%20framework%20for%20continuous%20long-horizon%20viewpoint%20planning%2C%20for%20ground%0Arobots%2C%20applied%20to%20tasks%20involving%20patrolling%2C%20monitoring%20or%20visual%20data%0Agathering%20in%20known%20environments.%20Our%20approach%20to%20Long%20Horizon%20Viewpoint%0APlanning%20%28LHVP%29%2C%20enables%20the%20robot%20to%20autonomously%20navigate%20and%20collect%0Aenvironmental%20data%20optimizing%20for%20coverage%20over%20the%20horizon%20of%20the%20patrol.%0ALeveraging%20a%20quadruped%27s%20mobility%20and%20sensory%20capabilities%2C%20our%20LHVP%20framework%0Aplans%20patrol%20paths%20that%20account%20for%20coupling%20the%20viewpoint%20planner%20for%20the%20arm%0Acamera%20with%20the%20mobile%20base%27s%20navigation%20planner.%20The%20viewpath%20optimization%0Aalgorithm%20seeks%20a%20balance%20between%20comprehensive%20environmental%20coverage%20and%0Adynamically%20feasible%20movements%2C%20thus%20ensuring%20prolonged%20and%20effective%20operation%0Ain%20scenarios%20including%20monitoring%2C%20security%20surveillance%2C%20and%20disaster%0Aresponse.%20We%20validate%20our%20approach%20through%20simulations%20and%20in%20the%20real%20world%0Aand%20show%20that%20our%20LHVP%20significantly%20outperforms%20naive%20patrolling%20methods%20in%0Aterms%20of%20area%20coverage%20generating%20information-gathering%20trajectories%20for%20the%0Arobot%20arm.%20Our%20results%20indicate%20a%20promising%20direction%20for%20the%20deployment%20of%0Amobile%20robots%20in%20long-term%2C%20autonomous%20surveying%2C%20and%20environmental%20data%0Acollection%20tasks%2C%20highlighting%20the%20potential%20of%20intelligent%20robotic%20systems%20in%0Achallenging%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12513v1&entry.124074799=Read"},
{"title": "Non-Homophilic Graph Pre-Training and Prompt Learning", "author": "Xingtong Yu and Jie Zhang and Yuan Fang and Renhe Jiang", "abstract": "  Graphs are ubiquitous for modeling complex relationships between objects\nacross various fields. Graph neural networks (GNNs) have become a mainstream\ntechnique for graph-based applications, but their performance heavily relies on\nabundant labeled data. To reduce labeling requirement, pre-training and prompt\nlearning has become a popular alternative. However, most existing prompt\nmethods do not differentiate homophilic and heterophilic characteristics of\nreal-world graphs. In particular, many real-world graphs are non-homophilic,\nnot strictly or uniformly homophilic with mixing homophilic and heterophilic\npatterns, exhibiting varying non-homophilic characteristics across graphs and\nnodes. In this paper, we propose ProNoG, a novel pre-training and prompt\nlearning framework for such non-homophilic graphs. First, we analyze existing\ngraph pre-training methods, providing theoretical insights into the choice of\npre-training tasks. Second, recognizing that each node exhibits unique\nnon-homophilic characteristics, we propose a conditional network to\ncharacterize the node-specific patterns in downstream tasks. Finally, we\nthoroughly evaluate and analyze ProNoG through extensive experiments on ten\npublic datasets.\n", "link": "http://arxiv.org/abs/2408.12594v1", "date": "2024-08-22", "relevancy": 2.4702, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5198}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4949}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-Homophilic%20Graph%20Pre-Training%20and%20Prompt%20Learning&body=Title%3A%20Non-Homophilic%20Graph%20Pre-Training%20and%20Prompt%20Learning%0AAuthor%3A%20Xingtong%20Yu%20and%20Jie%20Zhang%20and%20Yuan%20Fang%20and%20Renhe%20Jiang%0AAbstract%3A%20%20%20Graphs%20are%20ubiquitous%20for%20modeling%20complex%20relationships%20between%20objects%0Aacross%20various%20fields.%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20a%20mainstream%0Atechnique%20for%20graph-based%20applications%2C%20but%20their%20performance%20heavily%20relies%20on%0Aabundant%20labeled%20data.%20To%20reduce%20labeling%20requirement%2C%20pre-training%20and%20prompt%0Alearning%20has%20become%20a%20popular%20alternative.%20However%2C%20most%20existing%20prompt%0Amethods%20do%20not%20differentiate%20homophilic%20and%20heterophilic%20characteristics%20of%0Areal-world%20graphs.%20In%20particular%2C%20many%20real-world%20graphs%20are%20non-homophilic%2C%0Anot%20strictly%20or%20uniformly%20homophilic%20with%20mixing%20homophilic%20and%20heterophilic%0Apatterns%2C%20exhibiting%20varying%20non-homophilic%20characteristics%20across%20graphs%20and%0Anodes.%20In%20this%20paper%2C%20we%20propose%20ProNoG%2C%20a%20novel%20pre-training%20and%20prompt%0Alearning%20framework%20for%20such%20non-homophilic%20graphs.%20First%2C%20we%20analyze%20existing%0Agraph%20pre-training%20methods%2C%20providing%20theoretical%20insights%20into%20the%20choice%20of%0Apre-training%20tasks.%20Second%2C%20recognizing%20that%20each%20node%20exhibits%20unique%0Anon-homophilic%20characteristics%2C%20we%20propose%20a%20conditional%20network%20to%0Acharacterize%20the%20node-specific%20patterns%20in%20downstream%20tasks.%20Finally%2C%20we%0Athoroughly%20evaluate%20and%20analyze%20ProNoG%20through%20extensive%20experiments%20on%20ten%0Apublic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-Homophilic%2520Graph%2520Pre-Training%2520and%2520Prompt%2520Learning%26entry.906535625%3DXingtong%2520Yu%2520and%2520Jie%2520Zhang%2520and%2520Yuan%2520Fang%2520and%2520Renhe%2520Jiang%26entry.1292438233%3D%2520%2520Graphs%2520are%2520ubiquitous%2520for%2520modeling%2520complex%2520relationships%2520between%2520objects%250Aacross%2520various%2520fields.%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520become%2520a%2520mainstream%250Atechnique%2520for%2520graph-based%2520applications%252C%2520but%2520their%2520performance%2520heavily%2520relies%2520on%250Aabundant%2520labeled%2520data.%2520To%2520reduce%2520labeling%2520requirement%252C%2520pre-training%2520and%2520prompt%250Alearning%2520has%2520become%2520a%2520popular%2520alternative.%2520However%252C%2520most%2520existing%2520prompt%250Amethods%2520do%2520not%2520differentiate%2520homophilic%2520and%2520heterophilic%2520characteristics%2520of%250Areal-world%2520graphs.%2520In%2520particular%252C%2520many%2520real-world%2520graphs%2520are%2520non-homophilic%252C%250Anot%2520strictly%2520or%2520uniformly%2520homophilic%2520with%2520mixing%2520homophilic%2520and%2520heterophilic%250Apatterns%252C%2520exhibiting%2520varying%2520non-homophilic%2520characteristics%2520across%2520graphs%2520and%250Anodes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ProNoG%252C%2520a%2520novel%2520pre-training%2520and%2520prompt%250Alearning%2520framework%2520for%2520such%2520non-homophilic%2520graphs.%2520First%252C%2520we%2520analyze%2520existing%250Agraph%2520pre-training%2520methods%252C%2520providing%2520theoretical%2520insights%2520into%2520the%2520choice%2520of%250Apre-training%2520tasks.%2520Second%252C%2520recognizing%2520that%2520each%2520node%2520exhibits%2520unique%250Anon-homophilic%2520characteristics%252C%2520we%2520propose%2520a%2520conditional%2520network%2520to%250Acharacterize%2520the%2520node-specific%2520patterns%2520in%2520downstream%2520tasks.%2520Finally%252C%2520we%250Athoroughly%2520evaluate%2520and%2520analyze%2520ProNoG%2520through%2520extensive%2520experiments%2520on%2520ten%250Apublic%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-Homophilic%20Graph%20Pre-Training%20and%20Prompt%20Learning&entry.906535625=Xingtong%20Yu%20and%20Jie%20Zhang%20and%20Yuan%20Fang%20and%20Renhe%20Jiang&entry.1292438233=%20%20Graphs%20are%20ubiquitous%20for%20modeling%20complex%20relationships%20between%20objects%0Aacross%20various%20fields.%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20a%20mainstream%0Atechnique%20for%20graph-based%20applications%2C%20but%20their%20performance%20heavily%20relies%20on%0Aabundant%20labeled%20data.%20To%20reduce%20labeling%20requirement%2C%20pre-training%20and%20prompt%0Alearning%20has%20become%20a%20popular%20alternative.%20However%2C%20most%20existing%20prompt%0Amethods%20do%20not%20differentiate%20homophilic%20and%20heterophilic%20characteristics%20of%0Areal-world%20graphs.%20In%20particular%2C%20many%20real-world%20graphs%20are%20non-homophilic%2C%0Anot%20strictly%20or%20uniformly%20homophilic%20with%20mixing%20homophilic%20and%20heterophilic%0Apatterns%2C%20exhibiting%20varying%20non-homophilic%20characteristics%20across%20graphs%20and%0Anodes.%20In%20this%20paper%2C%20we%20propose%20ProNoG%2C%20a%20novel%20pre-training%20and%20prompt%0Alearning%20framework%20for%20such%20non-homophilic%20graphs.%20First%2C%20we%20analyze%20existing%0Agraph%20pre-training%20methods%2C%20providing%20theoretical%20insights%20into%20the%20choice%20of%0Apre-training%20tasks.%20Second%2C%20recognizing%20that%20each%20node%20exhibits%20unique%0Anon-homophilic%20characteristics%2C%20we%20propose%20a%20conditional%20network%20to%0Acharacterize%20the%20node-specific%20patterns%20in%20downstream%20tasks.%20Finally%2C%20we%0Athoroughly%20evaluate%20and%20analyze%20ProNoG%20through%20extensive%20experiments%20on%20ten%0Apublic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12594v1&entry.124074799=Read"},
{"title": "Automating Deformable Gasket Assembly", "author": "Simeon Adebola and Tara Sadjadpour and Karim El-Refai and Will Panitch and Zehan Ma and Roy Lin and Tianshuang Qiu and Shreya Ganti and Charlotte Le and Jaimyn Drake and Ken Goldberg", "abstract": "  In Gasket Assembly, a deformable gasket must be aligned and pressed into a\nnarrow channel. This task is common for sealing surfaces in the manufacturing\nof automobiles, appliances, electronics, and other products. Gasket Assembly is\na long-horizon, high-precision task and the gasket must align with the channel\nand be fully pressed in to achieve a secure fit. To compare approaches, we\npresent 4 methods for Gasket Assembly: one policy from deep imitation learning\nand three procedural algorithms. We evaluate these methods with 100 physical\ntrials. Results suggest that the Binary+ algorithm succeeds in 10/10 on the\nstraight channel whereas the learned policy based on 250 human teleoperated\ndemonstrations succeeds in 8/10 trials and is significantly slower. Code, CAD\nmodels, videos, and data can be found at\nhttps://berkeleyautomation.github.io/robot-gasket/\n", "link": "http://arxiv.org/abs/2408.12593v1", "date": "2024-08-22", "relevancy": 2.4514, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4938}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4905}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automating%20Deformable%20Gasket%20Assembly&body=Title%3A%20Automating%20Deformable%20Gasket%20Assembly%0AAuthor%3A%20Simeon%20Adebola%20and%20Tara%20Sadjadpour%20and%20Karim%20El-Refai%20and%20Will%20Panitch%20and%20Zehan%20Ma%20and%20Roy%20Lin%20and%20Tianshuang%20Qiu%20and%20Shreya%20Ganti%20and%20Charlotte%20Le%20and%20Jaimyn%20Drake%20and%20Ken%20Goldberg%0AAbstract%3A%20%20%20In%20Gasket%20Assembly%2C%20a%20deformable%20gasket%20must%20be%20aligned%20and%20pressed%20into%20a%0Anarrow%20channel.%20This%20task%20is%20common%20for%20sealing%20surfaces%20in%20the%20manufacturing%0Aof%20automobiles%2C%20appliances%2C%20electronics%2C%20and%20other%20products.%20Gasket%20Assembly%20is%0Aa%20long-horizon%2C%20high-precision%20task%20and%20the%20gasket%20must%20align%20with%20the%20channel%0Aand%20be%20fully%20pressed%20in%20to%20achieve%20a%20secure%20fit.%20To%20compare%20approaches%2C%20we%0Apresent%204%20methods%20for%20Gasket%20Assembly%3A%20one%20policy%20from%20deep%20imitation%20learning%0Aand%20three%20procedural%20algorithms.%20We%20evaluate%20these%20methods%20with%20100%20physical%0Atrials.%20Results%20suggest%20that%20the%20Binary%2B%20algorithm%20succeeds%20in%2010/10%20on%20the%0Astraight%20channel%20whereas%20the%20learned%20policy%20based%20on%20250%20human%20teleoperated%0Ademonstrations%20succeeds%20in%208/10%20trials%20and%20is%20significantly%20slower.%20Code%2C%20CAD%0Amodels%2C%20videos%2C%20and%20data%20can%20be%20found%20at%0Ahttps%3A//berkeleyautomation.github.io/robot-gasket/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomating%2520Deformable%2520Gasket%2520Assembly%26entry.906535625%3DSimeon%2520Adebola%2520and%2520Tara%2520Sadjadpour%2520and%2520Karim%2520El-Refai%2520and%2520Will%2520Panitch%2520and%2520Zehan%2520Ma%2520and%2520Roy%2520Lin%2520and%2520Tianshuang%2520Qiu%2520and%2520Shreya%2520Ganti%2520and%2520Charlotte%2520Le%2520and%2520Jaimyn%2520Drake%2520and%2520Ken%2520Goldberg%26entry.1292438233%3D%2520%2520In%2520Gasket%2520Assembly%252C%2520a%2520deformable%2520gasket%2520must%2520be%2520aligned%2520and%2520pressed%2520into%2520a%250Anarrow%2520channel.%2520This%2520task%2520is%2520common%2520for%2520sealing%2520surfaces%2520in%2520the%2520manufacturing%250Aof%2520automobiles%252C%2520appliances%252C%2520electronics%252C%2520and%2520other%2520products.%2520Gasket%2520Assembly%2520is%250Aa%2520long-horizon%252C%2520high-precision%2520task%2520and%2520the%2520gasket%2520must%2520align%2520with%2520the%2520channel%250Aand%2520be%2520fully%2520pressed%2520in%2520to%2520achieve%2520a%2520secure%2520fit.%2520To%2520compare%2520approaches%252C%2520we%250Apresent%25204%2520methods%2520for%2520Gasket%2520Assembly%253A%2520one%2520policy%2520from%2520deep%2520imitation%2520learning%250Aand%2520three%2520procedural%2520algorithms.%2520We%2520evaluate%2520these%2520methods%2520with%2520100%2520physical%250Atrials.%2520Results%2520suggest%2520that%2520the%2520Binary%252B%2520algorithm%2520succeeds%2520in%252010/10%2520on%2520the%250Astraight%2520channel%2520whereas%2520the%2520learned%2520policy%2520based%2520on%2520250%2520human%2520teleoperated%250Ademonstrations%2520succeeds%2520in%25208/10%2520trials%2520and%2520is%2520significantly%2520slower.%2520Code%252C%2520CAD%250Amodels%252C%2520videos%252C%2520and%2520data%2520can%2520be%2520found%2520at%250Ahttps%253A//berkeleyautomation.github.io/robot-gasket/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automating%20Deformable%20Gasket%20Assembly&entry.906535625=Simeon%20Adebola%20and%20Tara%20Sadjadpour%20and%20Karim%20El-Refai%20and%20Will%20Panitch%20and%20Zehan%20Ma%20and%20Roy%20Lin%20and%20Tianshuang%20Qiu%20and%20Shreya%20Ganti%20and%20Charlotte%20Le%20and%20Jaimyn%20Drake%20and%20Ken%20Goldberg&entry.1292438233=%20%20In%20Gasket%20Assembly%2C%20a%20deformable%20gasket%20must%20be%20aligned%20and%20pressed%20into%20a%0Anarrow%20channel.%20This%20task%20is%20common%20for%20sealing%20surfaces%20in%20the%20manufacturing%0Aof%20automobiles%2C%20appliances%2C%20electronics%2C%20and%20other%20products.%20Gasket%20Assembly%20is%0Aa%20long-horizon%2C%20high-precision%20task%20and%20the%20gasket%20must%20align%20with%20the%20channel%0Aand%20be%20fully%20pressed%20in%20to%20achieve%20a%20secure%20fit.%20To%20compare%20approaches%2C%20we%0Apresent%204%20methods%20for%20Gasket%20Assembly%3A%20one%20policy%20from%20deep%20imitation%20learning%0Aand%20three%20procedural%20algorithms.%20We%20evaluate%20these%20methods%20with%20100%20physical%0Atrials.%20Results%20suggest%20that%20the%20Binary%2B%20algorithm%20succeeds%20in%2010/10%20on%20the%0Astraight%20channel%20whereas%20the%20learned%20policy%20based%20on%20250%20human%20teleoperated%0Ademonstrations%20succeeds%20in%208/10%20trials%20and%20is%20significantly%20slower.%20Code%2C%20CAD%0Amodels%2C%20videos%2C%20and%20data%20can%20be%20found%20at%0Ahttps%3A//berkeleyautomation.github.io/robot-gasket/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12593v1&entry.124074799=Read"},
{"title": "A Personalized Zero-Shot ECG Arrhythmia Monitoring System: From Sparse\n  Representation Based Domain Adaption to Energy Efficient Abnormal Beat\n  Detection for Practical ECG Surveillance", "author": "Mehmet Yama\u00e7 and Mert Duman and \u0130lke Adal\u0131o\u011flu and Serkan Kiranyaz and Moncef Gabbouj", "abstract": "  This paper proposes a low-cost and highly accurate ECG-monitoring system\nintended for personalized early arrhythmia detection for wearable mobile\nsensors. Earlier supervised approaches for personalized ECG monitoring require\nboth abnormal and normal heartbeats for the training of the dedicated\nclassifier. However, in a real-world scenario where the personalized algorithm\nis embedded in a wearable device, such training data is not available for\nhealthy people with no cardiac disorder history. In this study, (i) we propose\na null space analysis on the healthy signal space obtained via sparse\ndictionary learning, and investigate how a simple null space projection or\nalternatively regularized least squares-based classification methods can reduce\nthe computational complexity, without sacrificing the detection accuracy, when\ncompared to sparse representation-based classification. (ii) Then we introduce\na sparse representation-based domain adaptation technique in order to project\nother existing users' abnormal and normal signals onto the new user's signal\nspace, enabling us to train the dedicated classifier without having any\nabnormal heartbeat of the new user. Therefore, zero-shot learning can be\nachieved without the need for synthetic abnormal heartbeat generation. An\nextensive set of experiments performed on the benchmark MIT-BIH ECG dataset\nshows that when this domain adaptation-based training data generator is used\nwith a simple 1-D CNN classifier, the method outperforms the prior work by a\nsignificant margin. (iii) Then, by combining (i) and (ii), we propose an\nensemble classifier that further improves the performance. This approach for\nzero-shot arrhythmia detection achieves an average accuracy level of 98.2% and\nan F1-Score of 92.8%. Finally, a personalized energy-efficient ECG monitoring\nscheme is proposed using the above-mentioned innovations.\n", "link": "http://arxiv.org/abs/2207.07089v2", "date": "2024-08-22", "relevancy": 2.4445, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5235}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4744}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Personalized%20Zero-Shot%20ECG%20Arrhythmia%20Monitoring%20System%3A%20From%20Sparse%0A%20%20Representation%20Based%20Domain%20Adaption%20to%20Energy%20Efficient%20Abnormal%20Beat%0A%20%20Detection%20for%20Practical%20ECG%20Surveillance&body=Title%3A%20A%20Personalized%20Zero-Shot%20ECG%20Arrhythmia%20Monitoring%20System%3A%20From%20Sparse%0A%20%20Representation%20Based%20Domain%20Adaption%20to%20Energy%20Efficient%20Abnormal%20Beat%0A%20%20Detection%20for%20Practical%20ECG%20Surveillance%0AAuthor%3A%20Mehmet%20Yama%C3%A7%20and%20Mert%20Duman%20and%20%C4%B0lke%20Adal%C4%B1o%C4%9Flu%20and%20Serkan%20Kiranyaz%20and%20Moncef%20Gabbouj%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20low-cost%20and%20highly%20accurate%20ECG-monitoring%20system%0Aintended%20for%20personalized%20early%20arrhythmia%20detection%20for%20wearable%20mobile%0Asensors.%20Earlier%20supervised%20approaches%20for%20personalized%20ECG%20monitoring%20require%0Aboth%20abnormal%20and%20normal%20heartbeats%20for%20the%20training%20of%20the%20dedicated%0Aclassifier.%20However%2C%20in%20a%20real-world%20scenario%20where%20the%20personalized%20algorithm%0Ais%20embedded%20in%20a%20wearable%20device%2C%20such%20training%20data%20is%20not%20available%20for%0Ahealthy%20people%20with%20no%20cardiac%20disorder%20history.%20In%20this%20study%2C%20%28i%29%20we%20propose%0Aa%20null%20space%20analysis%20on%20the%20healthy%20signal%20space%20obtained%20via%20sparse%0Adictionary%20learning%2C%20and%20investigate%20how%20a%20simple%20null%20space%20projection%20or%0Aalternatively%20regularized%20least%20squares-based%20classification%20methods%20can%20reduce%0Athe%20computational%20complexity%2C%20without%20sacrificing%20the%20detection%20accuracy%2C%20when%0Acompared%20to%20sparse%20representation-based%20classification.%20%28ii%29%20Then%20we%20introduce%0Aa%20sparse%20representation-based%20domain%20adaptation%20technique%20in%20order%20to%20project%0Aother%20existing%20users%27%20abnormal%20and%20normal%20signals%20onto%20the%20new%20user%27s%20signal%0Aspace%2C%20enabling%20us%20to%20train%20the%20dedicated%20classifier%20without%20having%20any%0Aabnormal%20heartbeat%20of%20the%20new%20user.%20Therefore%2C%20zero-shot%20learning%20can%20be%0Aachieved%20without%20the%20need%20for%20synthetic%20abnormal%20heartbeat%20generation.%20An%0Aextensive%20set%20of%20experiments%20performed%20on%20the%20benchmark%20MIT-BIH%20ECG%20dataset%0Ashows%20that%20when%20this%20domain%20adaptation-based%20training%20data%20generator%20is%20used%0Awith%20a%20simple%201-D%20CNN%20classifier%2C%20the%20method%20outperforms%20the%20prior%20work%20by%20a%0Asignificant%20margin.%20%28iii%29%20Then%2C%20by%20combining%20%28i%29%20and%20%28ii%29%2C%20we%20propose%20an%0Aensemble%20classifier%20that%20further%20improves%20the%20performance.%20This%20approach%20for%0Azero-shot%20arrhythmia%20detection%20achieves%20an%20average%20accuracy%20level%20of%2098.2%25%20and%0Aan%20F1-Score%20of%2092.8%25.%20Finally%2C%20a%20personalized%20energy-efficient%20ECG%20monitoring%0Ascheme%20is%20proposed%20using%20the%20above-mentioned%20innovations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.07089v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Personalized%2520Zero-Shot%2520ECG%2520Arrhythmia%2520Monitoring%2520System%253A%2520From%2520Sparse%250A%2520%2520Representation%2520Based%2520Domain%2520Adaption%2520to%2520Energy%2520Efficient%2520Abnormal%2520Beat%250A%2520%2520Detection%2520for%2520Practical%2520ECG%2520Surveillance%26entry.906535625%3DMehmet%2520Yama%25C3%25A7%2520and%2520Mert%2520Duman%2520and%2520%25C4%25B0lke%2520Adal%25C4%25B1o%25C4%259Flu%2520and%2520Serkan%2520Kiranyaz%2520and%2520Moncef%2520Gabbouj%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520low-cost%2520and%2520highly%2520accurate%2520ECG-monitoring%2520system%250Aintended%2520for%2520personalized%2520early%2520arrhythmia%2520detection%2520for%2520wearable%2520mobile%250Asensors.%2520Earlier%2520supervised%2520approaches%2520for%2520personalized%2520ECG%2520monitoring%2520require%250Aboth%2520abnormal%2520and%2520normal%2520heartbeats%2520for%2520the%2520training%2520of%2520the%2520dedicated%250Aclassifier.%2520However%252C%2520in%2520a%2520real-world%2520scenario%2520where%2520the%2520personalized%2520algorithm%250Ais%2520embedded%2520in%2520a%2520wearable%2520device%252C%2520such%2520training%2520data%2520is%2520not%2520available%2520for%250Ahealthy%2520people%2520with%2520no%2520cardiac%2520disorder%2520history.%2520In%2520this%2520study%252C%2520%2528i%2529%2520we%2520propose%250Aa%2520null%2520space%2520analysis%2520on%2520the%2520healthy%2520signal%2520space%2520obtained%2520via%2520sparse%250Adictionary%2520learning%252C%2520and%2520investigate%2520how%2520a%2520simple%2520null%2520space%2520projection%2520or%250Aalternatively%2520regularized%2520least%2520squares-based%2520classification%2520methods%2520can%2520reduce%250Athe%2520computational%2520complexity%252C%2520without%2520sacrificing%2520the%2520detection%2520accuracy%252C%2520when%250Acompared%2520to%2520sparse%2520representation-based%2520classification.%2520%2528ii%2529%2520Then%2520we%2520introduce%250Aa%2520sparse%2520representation-based%2520domain%2520adaptation%2520technique%2520in%2520order%2520to%2520project%250Aother%2520existing%2520users%2527%2520abnormal%2520and%2520normal%2520signals%2520onto%2520the%2520new%2520user%2527s%2520signal%250Aspace%252C%2520enabling%2520us%2520to%2520train%2520the%2520dedicated%2520classifier%2520without%2520having%2520any%250Aabnormal%2520heartbeat%2520of%2520the%2520new%2520user.%2520Therefore%252C%2520zero-shot%2520learning%2520can%2520be%250Aachieved%2520without%2520the%2520need%2520for%2520synthetic%2520abnormal%2520heartbeat%2520generation.%2520An%250Aextensive%2520set%2520of%2520experiments%2520performed%2520on%2520the%2520benchmark%2520MIT-BIH%2520ECG%2520dataset%250Ashows%2520that%2520when%2520this%2520domain%2520adaptation-based%2520training%2520data%2520generator%2520is%2520used%250Awith%2520a%2520simple%25201-D%2520CNN%2520classifier%252C%2520the%2520method%2520outperforms%2520the%2520prior%2520work%2520by%2520a%250Asignificant%2520margin.%2520%2528iii%2529%2520Then%252C%2520by%2520combining%2520%2528i%2529%2520and%2520%2528ii%2529%252C%2520we%2520propose%2520an%250Aensemble%2520classifier%2520that%2520further%2520improves%2520the%2520performance.%2520This%2520approach%2520for%250Azero-shot%2520arrhythmia%2520detection%2520achieves%2520an%2520average%2520accuracy%2520level%2520of%252098.2%2525%2520and%250Aan%2520F1-Score%2520of%252092.8%2525.%2520Finally%252C%2520a%2520personalized%2520energy-efficient%2520ECG%2520monitoring%250Ascheme%2520is%2520proposed%2520using%2520the%2520above-mentioned%2520innovations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.07089v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Personalized%20Zero-Shot%20ECG%20Arrhythmia%20Monitoring%20System%3A%20From%20Sparse%0A%20%20Representation%20Based%20Domain%20Adaption%20to%20Energy%20Efficient%20Abnormal%20Beat%0A%20%20Detection%20for%20Practical%20ECG%20Surveillance&entry.906535625=Mehmet%20Yama%C3%A7%20and%20Mert%20Duman%20and%20%C4%B0lke%20Adal%C4%B1o%C4%9Flu%20and%20Serkan%20Kiranyaz%20and%20Moncef%20Gabbouj&entry.1292438233=%20%20This%20paper%20proposes%20a%20low-cost%20and%20highly%20accurate%20ECG-monitoring%20system%0Aintended%20for%20personalized%20early%20arrhythmia%20detection%20for%20wearable%20mobile%0Asensors.%20Earlier%20supervised%20approaches%20for%20personalized%20ECG%20monitoring%20require%0Aboth%20abnormal%20and%20normal%20heartbeats%20for%20the%20training%20of%20the%20dedicated%0Aclassifier.%20However%2C%20in%20a%20real-world%20scenario%20where%20the%20personalized%20algorithm%0Ais%20embedded%20in%20a%20wearable%20device%2C%20such%20training%20data%20is%20not%20available%20for%0Ahealthy%20people%20with%20no%20cardiac%20disorder%20history.%20In%20this%20study%2C%20%28i%29%20we%20propose%0Aa%20null%20space%20analysis%20on%20the%20healthy%20signal%20space%20obtained%20via%20sparse%0Adictionary%20learning%2C%20and%20investigate%20how%20a%20simple%20null%20space%20projection%20or%0Aalternatively%20regularized%20least%20squares-based%20classification%20methods%20can%20reduce%0Athe%20computational%20complexity%2C%20without%20sacrificing%20the%20detection%20accuracy%2C%20when%0Acompared%20to%20sparse%20representation-based%20classification.%20%28ii%29%20Then%20we%20introduce%0Aa%20sparse%20representation-based%20domain%20adaptation%20technique%20in%20order%20to%20project%0Aother%20existing%20users%27%20abnormal%20and%20normal%20signals%20onto%20the%20new%20user%27s%20signal%0Aspace%2C%20enabling%20us%20to%20train%20the%20dedicated%20classifier%20without%20having%20any%0Aabnormal%20heartbeat%20of%20the%20new%20user.%20Therefore%2C%20zero-shot%20learning%20can%20be%0Aachieved%20without%20the%20need%20for%20synthetic%20abnormal%20heartbeat%20generation.%20An%0Aextensive%20set%20of%20experiments%20performed%20on%20the%20benchmark%20MIT-BIH%20ECG%20dataset%0Ashows%20that%20when%20this%20domain%20adaptation-based%20training%20data%20generator%20is%20used%0Awith%20a%20simple%201-D%20CNN%20classifier%2C%20the%20method%20outperforms%20the%20prior%20work%20by%20a%0Asignificant%20margin.%20%28iii%29%20Then%2C%20by%20combining%20%28i%29%20and%20%28ii%29%2C%20we%20propose%20an%0Aensemble%20classifier%20that%20further%20improves%20the%20performance.%20This%20approach%20for%0Azero-shot%20arrhythmia%20detection%20achieves%20an%20average%20accuracy%20level%20of%2098.2%25%20and%0Aan%20F1-Score%20of%2092.8%25.%20Finally%2C%20a%20personalized%20energy-efficient%20ECG%20monitoring%0Ascheme%20is%20proposed%20using%20the%20above-mentioned%20innovations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.07089v2&entry.124074799=Read"},
{"title": "Deep Learning with CNNs: A Compact Holistic Tutorial with Focus on\n  Supervised Regression (Preprint)", "author": "Yansel Gonzalez Tejeda and Helmut A. Mayer", "abstract": "  In this tutorial, we present a compact and holistic discussion of Deep\nLearning with a focus on Convolutional Neural Networks (CNNs) and supervised\nregression. While there are numerous books and articles on the individual\ntopics we cover, comprehensive and detailed tutorials that address Deep\nLearning from a foundational yet rigorous and accessible perspective are rare.\nMost resources on CNNs are either too advanced, focusing on cutting-edge\narchitectures, or too narrow, addressing only specific applications like image\nclassification.This tutorial not only summarizes the most relevant concepts but\nalso provides an in-depth exploration of each, offering a complete yet agile\nset of ideas. Moreover, we highlight the powerful synergy between learning\ntheory, statistic, and machine learning, which together underpin the Deep\nLearning and CNN frameworks. We aim for this tutorial to serve as an optimal\nresource for students, professors, and anyone interested in understanding the\nfoundations of Deep Learning. Upon acceptance we will provide an accompanying\nrepository under\n\\href{https://github.com/neoglez/deep-learning-tutorial}{https://github.com/neoglez/deep-learning-tutorial}\n  Keywords: Tutorial, Deep Learning, Convolutional Neural Networks, Machine\nLearning.\n", "link": "http://arxiv.org/abs/2408.12308v1", "date": "2024-08-22", "relevancy": 2.4431, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5104}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4823}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20with%20CNNs%3A%20A%20Compact%20Holistic%20Tutorial%20with%20Focus%20on%0A%20%20Supervised%20Regression%20%28Preprint%29&body=Title%3A%20Deep%20Learning%20with%20CNNs%3A%20A%20Compact%20Holistic%20Tutorial%20with%20Focus%20on%0A%20%20Supervised%20Regression%20%28Preprint%29%0AAuthor%3A%20Yansel%20Gonzalez%20Tejeda%20and%20Helmut%20A.%20Mayer%0AAbstract%3A%20%20%20In%20this%20tutorial%2C%20we%20present%20a%20compact%20and%20holistic%20discussion%20of%20Deep%0ALearning%20with%20a%20focus%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20supervised%0Aregression.%20While%20there%20are%20numerous%20books%20and%20articles%20on%20the%20individual%0Atopics%20we%20cover%2C%20comprehensive%20and%20detailed%20tutorials%20that%20address%20Deep%0ALearning%20from%20a%20foundational%20yet%20rigorous%20and%20accessible%20perspective%20are%20rare.%0AMost%20resources%20on%20CNNs%20are%20either%20too%20advanced%2C%20focusing%20on%20cutting-edge%0Aarchitectures%2C%20or%20too%20narrow%2C%20addressing%20only%20specific%20applications%20like%20image%0Aclassification.This%20tutorial%20not%20only%20summarizes%20the%20most%20relevant%20concepts%20but%0Aalso%20provides%20an%20in-depth%20exploration%20of%20each%2C%20offering%20a%20complete%20yet%20agile%0Aset%20of%20ideas.%20Moreover%2C%20we%20highlight%20the%20powerful%20synergy%20between%20learning%0Atheory%2C%20statistic%2C%20and%20machine%20learning%2C%20which%20together%20underpin%20the%20Deep%0ALearning%20and%20CNN%20frameworks.%20We%20aim%20for%20this%20tutorial%20to%20serve%20as%20an%20optimal%0Aresource%20for%20students%2C%20professors%2C%20and%20anyone%20interested%20in%20understanding%20the%0Afoundations%20of%20Deep%20Learning.%20Upon%20acceptance%20we%20will%20provide%20an%20accompanying%0Arepository%20under%0A%5Chref%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%0A%20%20Keywords%3A%20Tutorial%2C%20Deep%20Learning%2C%20Convolutional%20Neural%20Networks%2C%20Machine%0ALearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520with%2520CNNs%253A%2520A%2520Compact%2520Holistic%2520Tutorial%2520with%2520Focus%2520on%250A%2520%2520Supervised%2520Regression%2520%2528Preprint%2529%26entry.906535625%3DYansel%2520Gonzalez%2520Tejeda%2520and%2520Helmut%2520A.%2520Mayer%26entry.1292438233%3D%2520%2520In%2520this%2520tutorial%252C%2520we%2520present%2520a%2520compact%2520and%2520holistic%2520discussion%2520of%2520Deep%250ALearning%2520with%2520a%2520focus%2520on%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520supervised%250Aregression.%2520While%2520there%2520are%2520numerous%2520books%2520and%2520articles%2520on%2520the%2520individual%250Atopics%2520we%2520cover%252C%2520comprehensive%2520and%2520detailed%2520tutorials%2520that%2520address%2520Deep%250ALearning%2520from%2520a%2520foundational%2520yet%2520rigorous%2520and%2520accessible%2520perspective%2520are%2520rare.%250AMost%2520resources%2520on%2520CNNs%2520are%2520either%2520too%2520advanced%252C%2520focusing%2520on%2520cutting-edge%250Aarchitectures%252C%2520or%2520too%2520narrow%252C%2520addressing%2520only%2520specific%2520applications%2520like%2520image%250Aclassification.This%2520tutorial%2520not%2520only%2520summarizes%2520the%2520most%2520relevant%2520concepts%2520but%250Aalso%2520provides%2520an%2520in-depth%2520exploration%2520of%2520each%252C%2520offering%2520a%2520complete%2520yet%2520agile%250Aset%2520of%2520ideas.%2520Moreover%252C%2520we%2520highlight%2520the%2520powerful%2520synergy%2520between%2520learning%250Atheory%252C%2520statistic%252C%2520and%2520machine%2520learning%252C%2520which%2520together%2520underpin%2520the%2520Deep%250ALearning%2520and%2520CNN%2520frameworks.%2520We%2520aim%2520for%2520this%2520tutorial%2520to%2520serve%2520as%2520an%2520optimal%250Aresource%2520for%2520students%252C%2520professors%252C%2520and%2520anyone%2520interested%2520in%2520understanding%2520the%250Afoundations%2520of%2520Deep%2520Learning.%2520Upon%2520acceptance%2520we%2520will%2520provide%2520an%2520accompanying%250Arepository%2520under%250A%255Chref%257Bhttps%253A//github.com/neoglez/deep-learning-tutorial%257D%257Bhttps%253A//github.com/neoglez/deep-learning-tutorial%257D%250A%2520%2520Keywords%253A%2520Tutorial%252C%2520Deep%2520Learning%252C%2520Convolutional%2520Neural%2520Networks%252C%2520Machine%250ALearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20with%20CNNs%3A%20A%20Compact%20Holistic%20Tutorial%20with%20Focus%20on%0A%20%20Supervised%20Regression%20%28Preprint%29&entry.906535625=Yansel%20Gonzalez%20Tejeda%20and%20Helmut%20A.%20Mayer&entry.1292438233=%20%20In%20this%20tutorial%2C%20we%20present%20a%20compact%20and%20holistic%20discussion%20of%20Deep%0ALearning%20with%20a%20focus%20on%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20supervised%0Aregression.%20While%20there%20are%20numerous%20books%20and%20articles%20on%20the%20individual%0Atopics%20we%20cover%2C%20comprehensive%20and%20detailed%20tutorials%20that%20address%20Deep%0ALearning%20from%20a%20foundational%20yet%20rigorous%20and%20accessible%20perspective%20are%20rare.%0AMost%20resources%20on%20CNNs%20are%20either%20too%20advanced%2C%20focusing%20on%20cutting-edge%0Aarchitectures%2C%20or%20too%20narrow%2C%20addressing%20only%20specific%20applications%20like%20image%0Aclassification.This%20tutorial%20not%20only%20summarizes%20the%20most%20relevant%20concepts%20but%0Aalso%20provides%20an%20in-depth%20exploration%20of%20each%2C%20offering%20a%20complete%20yet%20agile%0Aset%20of%20ideas.%20Moreover%2C%20we%20highlight%20the%20powerful%20synergy%20between%20learning%0Atheory%2C%20statistic%2C%20and%20machine%20learning%2C%20which%20together%20underpin%20the%20Deep%0ALearning%20and%20CNN%20frameworks.%20We%20aim%20for%20this%20tutorial%20to%20serve%20as%20an%20optimal%0Aresource%20for%20students%2C%20professors%2C%20and%20anyone%20interested%20in%20understanding%20the%0Afoundations%20of%20Deep%20Learning.%20Upon%20acceptance%20we%20will%20provide%20an%20accompanying%0Arepository%20under%0A%5Chref%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%7Bhttps%3A//github.com/neoglez/deep-learning-tutorial%7D%0A%20%20Keywords%3A%20Tutorial%2C%20Deep%20Learning%2C%20Convolutional%20Neural%20Networks%2C%20Machine%0ALearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12308v1&entry.124074799=Read"},
{"title": "Finding Closure: A Closer Look at the Gestalt Law of Closure in\n  Convolutional Neural Networks", "author": "Yuyan Zhang and Derya Soydaner and Lisa Ko\u00dfmann and Fatemeh Behrad and Johan Wagemans", "abstract": "  The human brain has an inherent ability to fill in gaps to perceive figures\nas complete wholes, even when parts are missing or fragmented. This phenomenon\nis known as Closure in psychology, one of the Gestalt laws of perceptual\norganization, explaining how the human brain interprets visual stimuli. Given\nthe importance of Closure for human object recognition, we investigate whether\nneural networks rely on a similar mechanism. Exploring this crucial human\nvisual skill in neural networks has the potential to highlight their\ncomparability to humans. Recent studies have examined the Closure effect in\nneural networks. However, they typically focus on a limited selection of\nConvolutional Neural Networks (CNNs) and have not reached a consensus on their\ncapability to perform Closure. To address these gaps, we present a systematic\nframework for investigating the Closure principle in neural networks. We\nintroduce well-curated datasets designed to test for Closure effects, including\nboth modal and amodal completion. We then conduct experiments on various CNNs\nemploying different measurements. Our comprehensive analysis reveals that VGG16\nand DenseNet-121 exhibit the Closure effect, while other CNNs show variable\nresults. We interpret these findings by blending insights from psychology and\nneural network research, offering a unique perspective that enhances\ntransparency in understanding neural networks. Our code and dataset will be\nmade available on GitHub.\n", "link": "http://arxiv.org/abs/2408.12460v1", "date": "2024-08-22", "relevancy": 2.4261, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4978}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4892}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finding%20Closure%3A%20A%20Closer%20Look%20at%20the%20Gestalt%20Law%20of%20Closure%20in%0A%20%20Convolutional%20Neural%20Networks&body=Title%3A%20Finding%20Closure%3A%20A%20Closer%20Look%20at%20the%20Gestalt%20Law%20of%20Closure%20in%0A%20%20Convolutional%20Neural%20Networks%0AAuthor%3A%20Yuyan%20Zhang%20and%20Derya%20Soydaner%20and%20Lisa%20Ko%C3%9Fmann%20and%20Fatemeh%20Behrad%20and%20Johan%20Wagemans%0AAbstract%3A%20%20%20The%20human%20brain%20has%20an%20inherent%20ability%20to%20fill%20in%20gaps%20to%20perceive%20figures%0Aas%20complete%20wholes%2C%20even%20when%20parts%20are%20missing%20or%20fragmented.%20This%20phenomenon%0Ais%20known%20as%20Closure%20in%20psychology%2C%20one%20of%20the%20Gestalt%20laws%20of%20perceptual%0Aorganization%2C%20explaining%20how%20the%20human%20brain%20interprets%20visual%20stimuli.%20Given%0Athe%20importance%20of%20Closure%20for%20human%20object%20recognition%2C%20we%20investigate%20whether%0Aneural%20networks%20rely%20on%20a%20similar%20mechanism.%20Exploring%20this%20crucial%20human%0Avisual%20skill%20in%20neural%20networks%20has%20the%20potential%20to%20highlight%20their%0Acomparability%20to%20humans.%20Recent%20studies%20have%20examined%20the%20Closure%20effect%20in%0Aneural%20networks.%20However%2C%20they%20typically%20focus%20on%20a%20limited%20selection%20of%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20have%20not%20reached%20a%20consensus%20on%20their%0Acapability%20to%20perform%20Closure.%20To%20address%20these%20gaps%2C%20we%20present%20a%20systematic%0Aframework%20for%20investigating%20the%20Closure%20principle%20in%20neural%20networks.%20We%0Aintroduce%20well-curated%20datasets%20designed%20to%20test%20for%20Closure%20effects%2C%20including%0Aboth%20modal%20and%20amodal%20completion.%20We%20then%20conduct%20experiments%20on%20various%20CNNs%0Aemploying%20different%20measurements.%20Our%20comprehensive%20analysis%20reveals%20that%20VGG16%0Aand%20DenseNet-121%20exhibit%20the%20Closure%20effect%2C%20while%20other%20CNNs%20show%20variable%0Aresults.%20We%20interpret%20these%20findings%20by%20blending%20insights%20from%20psychology%20and%0Aneural%20network%20research%2C%20offering%20a%20unique%20perspective%20that%20enhances%0Atransparency%20in%20understanding%20neural%20networks.%20Our%20code%20and%20dataset%20will%20be%0Amade%20available%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinding%2520Closure%253A%2520A%2520Closer%2520Look%2520at%2520the%2520Gestalt%2520Law%2520of%2520Closure%2520in%250A%2520%2520Convolutional%2520Neural%2520Networks%26entry.906535625%3DYuyan%2520Zhang%2520and%2520Derya%2520Soydaner%2520and%2520Lisa%2520Ko%25C3%259Fmann%2520and%2520Fatemeh%2520Behrad%2520and%2520Johan%2520Wagemans%26entry.1292438233%3D%2520%2520The%2520human%2520brain%2520has%2520an%2520inherent%2520ability%2520to%2520fill%2520in%2520gaps%2520to%2520perceive%2520figures%250Aas%2520complete%2520wholes%252C%2520even%2520when%2520parts%2520are%2520missing%2520or%2520fragmented.%2520This%2520phenomenon%250Ais%2520known%2520as%2520Closure%2520in%2520psychology%252C%2520one%2520of%2520the%2520Gestalt%2520laws%2520of%2520perceptual%250Aorganization%252C%2520explaining%2520how%2520the%2520human%2520brain%2520interprets%2520visual%2520stimuli.%2520Given%250Athe%2520importance%2520of%2520Closure%2520for%2520human%2520object%2520recognition%252C%2520we%2520investigate%2520whether%250Aneural%2520networks%2520rely%2520on%2520a%2520similar%2520mechanism.%2520Exploring%2520this%2520crucial%2520human%250Avisual%2520skill%2520in%2520neural%2520networks%2520has%2520the%2520potential%2520to%2520highlight%2520their%250Acomparability%2520to%2520humans.%2520Recent%2520studies%2520have%2520examined%2520the%2520Closure%2520effect%2520in%250Aneural%2520networks.%2520However%252C%2520they%2520typically%2520focus%2520on%2520a%2520limited%2520selection%2520of%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520have%2520not%2520reached%2520a%2520consensus%2520on%2520their%250Acapability%2520to%2520perform%2520Closure.%2520To%2520address%2520these%2520gaps%252C%2520we%2520present%2520a%2520systematic%250Aframework%2520for%2520investigating%2520the%2520Closure%2520principle%2520in%2520neural%2520networks.%2520We%250Aintroduce%2520well-curated%2520datasets%2520designed%2520to%2520test%2520for%2520Closure%2520effects%252C%2520including%250Aboth%2520modal%2520and%2520amodal%2520completion.%2520We%2520then%2520conduct%2520experiments%2520on%2520various%2520CNNs%250Aemploying%2520different%2520measurements.%2520Our%2520comprehensive%2520analysis%2520reveals%2520that%2520VGG16%250Aand%2520DenseNet-121%2520exhibit%2520the%2520Closure%2520effect%252C%2520while%2520other%2520CNNs%2520show%2520variable%250Aresults.%2520We%2520interpret%2520these%2520findings%2520by%2520blending%2520insights%2520from%2520psychology%2520and%250Aneural%2520network%2520research%252C%2520offering%2520a%2520unique%2520perspective%2520that%2520enhances%250Atransparency%2520in%2520understanding%2520neural%2520networks.%2520Our%2520code%2520and%2520dataset%2520will%2520be%250Amade%2520available%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20Closure%3A%20A%20Closer%20Look%20at%20the%20Gestalt%20Law%20of%20Closure%20in%0A%20%20Convolutional%20Neural%20Networks&entry.906535625=Yuyan%20Zhang%20and%20Derya%20Soydaner%20and%20Lisa%20Ko%C3%9Fmann%20and%20Fatemeh%20Behrad%20and%20Johan%20Wagemans&entry.1292438233=%20%20The%20human%20brain%20has%20an%20inherent%20ability%20to%20fill%20in%20gaps%20to%20perceive%20figures%0Aas%20complete%20wholes%2C%20even%20when%20parts%20are%20missing%20or%20fragmented.%20This%20phenomenon%0Ais%20known%20as%20Closure%20in%20psychology%2C%20one%20of%20the%20Gestalt%20laws%20of%20perceptual%0Aorganization%2C%20explaining%20how%20the%20human%20brain%20interprets%20visual%20stimuli.%20Given%0Athe%20importance%20of%20Closure%20for%20human%20object%20recognition%2C%20we%20investigate%20whether%0Aneural%20networks%20rely%20on%20a%20similar%20mechanism.%20Exploring%20this%20crucial%20human%0Avisual%20skill%20in%20neural%20networks%20has%20the%20potential%20to%20highlight%20their%0Acomparability%20to%20humans.%20Recent%20studies%20have%20examined%20the%20Closure%20effect%20in%0Aneural%20networks.%20However%2C%20they%20typically%20focus%20on%20a%20limited%20selection%20of%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20have%20not%20reached%20a%20consensus%20on%20their%0Acapability%20to%20perform%20Closure.%20To%20address%20these%20gaps%2C%20we%20present%20a%20systematic%0Aframework%20for%20investigating%20the%20Closure%20principle%20in%20neural%20networks.%20We%0Aintroduce%20well-curated%20datasets%20designed%20to%20test%20for%20Closure%20effects%2C%20including%0Aboth%20modal%20and%20amodal%20completion.%20We%20then%20conduct%20experiments%20on%20various%20CNNs%0Aemploying%20different%20measurements.%20Our%20comprehensive%20analysis%20reveals%20that%20VGG16%0Aand%20DenseNet-121%20exhibit%20the%20Closure%20effect%2C%20while%20other%20CNNs%20show%20variable%0Aresults.%20We%20interpret%20these%20findings%20by%20blending%20insights%20from%20psychology%20and%0Aneural%20network%20research%2C%20offering%20a%20unique%20perspective%20that%20enhances%0Atransparency%20in%20understanding%20neural%20networks.%20Our%20code%20and%20dataset%20will%20be%0Amade%20available%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12460v1&entry.124074799=Read"},
{"title": "Urban Region Pre-training and Prompting: A Graph-based Approach", "author": "Jiahui Jin and Yifan Song and Dong Kan and Haojia Zhu and Xiangguo Sun and Zhicheng Li and Xigang Sun and Jinghui Zhang", "abstract": "  Urban region representation is crucial for various urban downstream tasks.\nHowever, despite the proliferation of methods and their success, acquiring\ngeneral urban region knowledge and adapting to different tasks remains\nchallenging. Previous work often neglects the spatial structures and functional\nlayouts between entities, limiting their ability to capture transferable\nknowledge across regions. Further, these methods struggle to adapt effectively\nto specific downstream tasks, as they do not adequately address the unique\nfeatures and relationships required for different downstream tasks. In this\npaper, we propose a $\\textbf{G}$raph-based $\\textbf{U}$rban $\\textbf{R}$egion\n$\\textbf{P}$re-training and $\\textbf{P}$rompting framework ($\\textbf{GURPP}$)\nfor region representation learning. Specifically, we first construct an urban\nregion graph that integrates detailed spatial entity data for more effective\nurban region representation. Then, we develop a subgraph-centric urban region\npre-training model to capture the heterogeneous and transferable patterns of\ninteractions among entities. To further enhance the adaptability of these\nembeddings to different tasks, we design two graph-based prompting methods to\nincorporate explicit/hidden task knowledge. Extensive experiments on various\nurban region prediction tasks and different cities demonstrate the superior\nperformance of our GURPP framework. We wil release code and data upon paper\nnotification.\n", "link": "http://arxiv.org/abs/2408.05920v2", "date": "2024-08-22", "relevancy": 2.4077, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4916}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.479}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Urban%20Region%20Pre-training%20and%20Prompting%3A%20A%20Graph-based%20Approach&body=Title%3A%20Urban%20Region%20Pre-training%20and%20Prompting%3A%20A%20Graph-based%20Approach%0AAuthor%3A%20Jiahui%20Jin%20and%20Yifan%20Song%20and%20Dong%20Kan%20and%20Haojia%20Zhu%20and%20Xiangguo%20Sun%20and%20Zhicheng%20Li%20and%20Xigang%20Sun%20and%20Jinghui%20Zhang%0AAbstract%3A%20%20%20Urban%20region%20representation%20is%20crucial%20for%20various%20urban%20downstream%20tasks.%0AHowever%2C%20despite%20the%20proliferation%20of%20methods%20and%20their%20success%2C%20acquiring%0Ageneral%20urban%20region%20knowledge%20and%20adapting%20to%20different%20tasks%20remains%0Achallenging.%20Previous%20work%20often%20neglects%20the%20spatial%20structures%20and%20functional%0Alayouts%20between%20entities%2C%20limiting%20their%20ability%20to%20capture%20transferable%0Aknowledge%20across%20regions.%20Further%2C%20these%20methods%20struggle%20to%20adapt%20effectively%0Ato%20specific%20downstream%20tasks%2C%20as%20they%20do%20not%20adequately%20address%20the%20unique%0Afeatures%20and%20relationships%20required%20for%20different%20downstream%20tasks.%20In%20this%0Apaper%2C%20we%20propose%20a%20%24%5Ctextbf%7BG%7D%24raph-based%20%24%5Ctextbf%7BU%7D%24rban%20%24%5Ctextbf%7BR%7D%24egion%0A%24%5Ctextbf%7BP%7D%24re-training%20and%20%24%5Ctextbf%7BP%7D%24rompting%20framework%20%28%24%5Ctextbf%7BGURPP%7D%24%29%0Afor%20region%20representation%20learning.%20Specifically%2C%20we%20first%20construct%20an%20urban%0Aregion%20graph%20that%20integrates%20detailed%20spatial%20entity%20data%20for%20more%20effective%0Aurban%20region%20representation.%20Then%2C%20we%20develop%20a%20subgraph-centric%20urban%20region%0Apre-training%20model%20to%20capture%20the%20heterogeneous%20and%20transferable%20patterns%20of%0Ainteractions%20among%20entities.%20To%20further%20enhance%20the%20adaptability%20of%20these%0Aembeddings%20to%20different%20tasks%2C%20we%20design%20two%20graph-based%20prompting%20methods%20to%0Aincorporate%20explicit/hidden%20task%20knowledge.%20Extensive%20experiments%20on%20various%0Aurban%20region%20prediction%20tasks%20and%20different%20cities%20demonstrate%20the%20superior%0Aperformance%20of%20our%20GURPP%20framework.%20We%20wil%20release%20code%20and%20data%20upon%20paper%0Anotification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05920v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrban%2520Region%2520Pre-training%2520and%2520Prompting%253A%2520A%2520Graph-based%2520Approach%26entry.906535625%3DJiahui%2520Jin%2520and%2520Yifan%2520Song%2520and%2520Dong%2520Kan%2520and%2520Haojia%2520Zhu%2520and%2520Xiangguo%2520Sun%2520and%2520Zhicheng%2520Li%2520and%2520Xigang%2520Sun%2520and%2520Jinghui%2520Zhang%26entry.1292438233%3D%2520%2520Urban%2520region%2520representation%2520is%2520crucial%2520for%2520various%2520urban%2520downstream%2520tasks.%250AHowever%252C%2520despite%2520the%2520proliferation%2520of%2520methods%2520and%2520their%2520success%252C%2520acquiring%250Ageneral%2520urban%2520region%2520knowledge%2520and%2520adapting%2520to%2520different%2520tasks%2520remains%250Achallenging.%2520Previous%2520work%2520often%2520neglects%2520the%2520spatial%2520structures%2520and%2520functional%250Alayouts%2520between%2520entities%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520transferable%250Aknowledge%2520across%2520regions.%2520Further%252C%2520these%2520methods%2520struggle%2520to%2520adapt%2520effectively%250Ato%2520specific%2520downstream%2520tasks%252C%2520as%2520they%2520do%2520not%2520adequately%2520address%2520the%2520unique%250Afeatures%2520and%2520relationships%2520required%2520for%2520different%2520downstream%2520tasks.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520%2524%255Ctextbf%257BG%257D%2524raph-based%2520%2524%255Ctextbf%257BU%257D%2524rban%2520%2524%255Ctextbf%257BR%257D%2524egion%250A%2524%255Ctextbf%257BP%257D%2524re-training%2520and%2520%2524%255Ctextbf%257BP%257D%2524rompting%2520framework%2520%2528%2524%255Ctextbf%257BGURPP%257D%2524%2529%250Afor%2520region%2520representation%2520learning.%2520Specifically%252C%2520we%2520first%2520construct%2520an%2520urban%250Aregion%2520graph%2520that%2520integrates%2520detailed%2520spatial%2520entity%2520data%2520for%2520more%2520effective%250Aurban%2520region%2520representation.%2520Then%252C%2520we%2520develop%2520a%2520subgraph-centric%2520urban%2520region%250Apre-training%2520model%2520to%2520capture%2520the%2520heterogeneous%2520and%2520transferable%2520patterns%2520of%250Ainteractions%2520among%2520entities.%2520To%2520further%2520enhance%2520the%2520adaptability%2520of%2520these%250Aembeddings%2520to%2520different%2520tasks%252C%2520we%2520design%2520two%2520graph-based%2520prompting%2520methods%2520to%250Aincorporate%2520explicit/hidden%2520task%2520knowledge.%2520Extensive%2520experiments%2520on%2520various%250Aurban%2520region%2520prediction%2520tasks%2520and%2520different%2520cities%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520our%2520GURPP%2520framework.%2520We%2520wil%2520release%2520code%2520and%2520data%2520upon%2520paper%250Anotification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05920v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Urban%20Region%20Pre-training%20and%20Prompting%3A%20A%20Graph-based%20Approach&entry.906535625=Jiahui%20Jin%20and%20Yifan%20Song%20and%20Dong%20Kan%20and%20Haojia%20Zhu%20and%20Xiangguo%20Sun%20and%20Zhicheng%20Li%20and%20Xigang%20Sun%20and%20Jinghui%20Zhang&entry.1292438233=%20%20Urban%20region%20representation%20is%20crucial%20for%20various%20urban%20downstream%20tasks.%0AHowever%2C%20despite%20the%20proliferation%20of%20methods%20and%20their%20success%2C%20acquiring%0Ageneral%20urban%20region%20knowledge%20and%20adapting%20to%20different%20tasks%20remains%0Achallenging.%20Previous%20work%20often%20neglects%20the%20spatial%20structures%20and%20functional%0Alayouts%20between%20entities%2C%20limiting%20their%20ability%20to%20capture%20transferable%0Aknowledge%20across%20regions.%20Further%2C%20these%20methods%20struggle%20to%20adapt%20effectively%0Ato%20specific%20downstream%20tasks%2C%20as%20they%20do%20not%20adequately%20address%20the%20unique%0Afeatures%20and%20relationships%20required%20for%20different%20downstream%20tasks.%20In%20this%0Apaper%2C%20we%20propose%20a%20%24%5Ctextbf%7BG%7D%24raph-based%20%24%5Ctextbf%7BU%7D%24rban%20%24%5Ctextbf%7BR%7D%24egion%0A%24%5Ctextbf%7BP%7D%24re-training%20and%20%24%5Ctextbf%7BP%7D%24rompting%20framework%20%28%24%5Ctextbf%7BGURPP%7D%24%29%0Afor%20region%20representation%20learning.%20Specifically%2C%20we%20first%20construct%20an%20urban%0Aregion%20graph%20that%20integrates%20detailed%20spatial%20entity%20data%20for%20more%20effective%0Aurban%20region%20representation.%20Then%2C%20we%20develop%20a%20subgraph-centric%20urban%20region%0Apre-training%20model%20to%20capture%20the%20heterogeneous%20and%20transferable%20patterns%20of%0Ainteractions%20among%20entities.%20To%20further%20enhance%20the%20adaptability%20of%20these%0Aembeddings%20to%20different%20tasks%2C%20we%20design%20two%20graph-based%20prompting%20methods%20to%0Aincorporate%20explicit/hidden%20task%20knowledge.%20Extensive%20experiments%20on%20various%0Aurban%20region%20prediction%20tasks%20and%20different%20cities%20demonstrate%20the%20superior%0Aperformance%20of%20our%20GURPP%20framework.%20We%20wil%20release%20code%20and%20data%20upon%20paper%0Anotification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05920v2&entry.124074799=Read"},
{"title": "Enhanced Parking Perception by Multi-Task Fisheye Cross-view\n  Transformers", "author": "Antonyo Musabini and Ivan Novikov and Sana Soula and Christel Leonet and Lihao Wang and Rachid Benmokhtar and Fabian Burger and Thomas Boulay and Xavier Perrotton", "abstract": "  Current parking area perception algorithms primarily focus on detecting\nvacant slots within a limited range, relying on error-prone homographic\nprojection for both labeling and inference. However, recent advancements in\nAdvanced Driver Assistance System (ADAS) require interaction with end-users\nthrough comprehensive and intelligent Human-Machine Interfaces (HMIs). These\ninterfaces should present a complete perception of the parking area going from\ndistinguishing vacant slots' entry lines to the orientation of other parked\nvehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT\nF-CVT), which leverages features from a four-camera fisheye Surround-view\nCamera System (SVCS) with multihead attentions to create a detailed Bird-Eye\nView (BEV) grid feature map. Features are processed by both a segmentation\ndecoder and a Polygon-Yolo based object detection decoder for parking slots and\nvehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects\nwithin a 25m x 25m real open-road scenes with an average error of only 20 cm.\nOur larger model achieves an F-1 score of 0.89. Moreover the smaller model\noperates at 16 fps on an Nvidia Jetson Orin embedded board, with similar\ndetection results to the larger one. MT F-CVT demonstrates robust\ngeneralization capability across different vehicles and camera rig\nconfigurations. A demo video from an unseen vehicle and camera rig is available\nat: https://streamable.com/jjw54x.\n", "link": "http://arxiv.org/abs/2408.12575v1", "date": "2024-08-22", "relevancy": 2.3329, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5939}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5777}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Parking%20Perception%20by%20Multi-Task%20Fisheye%20Cross-view%0A%20%20Transformers&body=Title%3A%20Enhanced%20Parking%20Perception%20by%20Multi-Task%20Fisheye%20Cross-view%0A%20%20Transformers%0AAuthor%3A%20Antonyo%20Musabini%20and%20Ivan%20Novikov%20and%20Sana%20Soula%20and%20Christel%20Leonet%20and%20Lihao%20Wang%20and%20Rachid%20Benmokhtar%20and%20Fabian%20Burger%20and%20Thomas%20Boulay%20and%20Xavier%20Perrotton%0AAbstract%3A%20%20%20Current%20parking%20area%20perception%20algorithms%20primarily%20focus%20on%20detecting%0Avacant%20slots%20within%20a%20limited%20range%2C%20relying%20on%20error-prone%20homographic%0Aprojection%20for%20both%20labeling%20and%20inference.%20However%2C%20recent%20advancements%20in%0AAdvanced%20Driver%20Assistance%20System%20%28ADAS%29%20require%20interaction%20with%20end-users%0Athrough%20comprehensive%20and%20intelligent%20Human-Machine%20Interfaces%20%28HMIs%29.%20These%0Ainterfaces%20should%20present%20a%20complete%20perception%20of%20the%20parking%20area%20going%20from%0Adistinguishing%20vacant%20slots%27%20entry%20lines%20to%20the%20orientation%20of%20other%20parked%0Avehicles.%20This%20paper%20introduces%20Multi-Task%20Fisheye%20Cross%20View%20Transformers%20%28MT%0AF-CVT%29%2C%20which%20leverages%20features%20from%20a%20four-camera%20fisheye%20Surround-view%0ACamera%20System%20%28SVCS%29%20with%20multihead%20attentions%20to%20create%20a%20detailed%20Bird-Eye%0AView%20%28BEV%29%20grid%20feature%20map.%20Features%20are%20processed%20by%20both%20a%20segmentation%0Adecoder%20and%20a%20Polygon-Yolo%20based%20object%20detection%20decoder%20for%20parking%20slots%20and%0Avehicles.%20Trained%20on%20data%20labeled%20using%20LiDAR%2C%20MT%20F-CVT%20positions%20objects%0Awithin%20a%2025m%20x%2025m%20real%20open-road%20scenes%20with%20an%20average%20error%20of%20only%2020%20cm.%0AOur%20larger%20model%20achieves%20an%20F-1%20score%20of%200.89.%20Moreover%20the%20smaller%20model%0Aoperates%20at%2016%20fps%20on%20an%20Nvidia%20Jetson%20Orin%20embedded%20board%2C%20with%20similar%0Adetection%20results%20to%20the%20larger%20one.%20MT%20F-CVT%20demonstrates%20robust%0Ageneralization%20capability%20across%20different%20vehicles%20and%20camera%20rig%0Aconfigurations.%20A%20demo%20video%20from%20an%20unseen%20vehicle%20and%20camera%20rig%20is%20available%0Aat%3A%20https%3A//streamable.com/jjw54x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Parking%2520Perception%2520by%2520Multi-Task%2520Fisheye%2520Cross-view%250A%2520%2520Transformers%26entry.906535625%3DAntonyo%2520Musabini%2520and%2520Ivan%2520Novikov%2520and%2520Sana%2520Soula%2520and%2520Christel%2520Leonet%2520and%2520Lihao%2520Wang%2520and%2520Rachid%2520Benmokhtar%2520and%2520Fabian%2520Burger%2520and%2520Thomas%2520Boulay%2520and%2520Xavier%2520Perrotton%26entry.1292438233%3D%2520%2520Current%2520parking%2520area%2520perception%2520algorithms%2520primarily%2520focus%2520on%2520detecting%250Avacant%2520slots%2520within%2520a%2520limited%2520range%252C%2520relying%2520on%2520error-prone%2520homographic%250Aprojection%2520for%2520both%2520labeling%2520and%2520inference.%2520However%252C%2520recent%2520advancements%2520in%250AAdvanced%2520Driver%2520Assistance%2520System%2520%2528ADAS%2529%2520require%2520interaction%2520with%2520end-users%250Athrough%2520comprehensive%2520and%2520intelligent%2520Human-Machine%2520Interfaces%2520%2528HMIs%2529.%2520These%250Ainterfaces%2520should%2520present%2520a%2520complete%2520perception%2520of%2520the%2520parking%2520area%2520going%2520from%250Adistinguishing%2520vacant%2520slots%2527%2520entry%2520lines%2520to%2520the%2520orientation%2520of%2520other%2520parked%250Avehicles.%2520This%2520paper%2520introduces%2520Multi-Task%2520Fisheye%2520Cross%2520View%2520Transformers%2520%2528MT%250AF-CVT%2529%252C%2520which%2520leverages%2520features%2520from%2520a%2520four-camera%2520fisheye%2520Surround-view%250ACamera%2520System%2520%2528SVCS%2529%2520with%2520multihead%2520attentions%2520to%2520create%2520a%2520detailed%2520Bird-Eye%250AView%2520%2528BEV%2529%2520grid%2520feature%2520map.%2520Features%2520are%2520processed%2520by%2520both%2520a%2520segmentation%250Adecoder%2520and%2520a%2520Polygon-Yolo%2520based%2520object%2520detection%2520decoder%2520for%2520parking%2520slots%2520and%250Avehicles.%2520Trained%2520on%2520data%2520labeled%2520using%2520LiDAR%252C%2520MT%2520F-CVT%2520positions%2520objects%250Awithin%2520a%252025m%2520x%252025m%2520real%2520open-road%2520scenes%2520with%2520an%2520average%2520error%2520of%2520only%252020%2520cm.%250AOur%2520larger%2520model%2520achieves%2520an%2520F-1%2520score%2520of%25200.89.%2520Moreover%2520the%2520smaller%2520model%250Aoperates%2520at%252016%2520fps%2520on%2520an%2520Nvidia%2520Jetson%2520Orin%2520embedded%2520board%252C%2520with%2520similar%250Adetection%2520results%2520to%2520the%2520larger%2520one.%2520MT%2520F-CVT%2520demonstrates%2520robust%250Ageneralization%2520capability%2520across%2520different%2520vehicles%2520and%2520camera%2520rig%250Aconfigurations.%2520A%2520demo%2520video%2520from%2520an%2520unseen%2520vehicle%2520and%2520camera%2520rig%2520is%2520available%250Aat%253A%2520https%253A//streamable.com/jjw54x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Parking%20Perception%20by%20Multi-Task%20Fisheye%20Cross-view%0A%20%20Transformers&entry.906535625=Antonyo%20Musabini%20and%20Ivan%20Novikov%20and%20Sana%20Soula%20and%20Christel%20Leonet%20and%20Lihao%20Wang%20and%20Rachid%20Benmokhtar%20and%20Fabian%20Burger%20and%20Thomas%20Boulay%20and%20Xavier%20Perrotton&entry.1292438233=%20%20Current%20parking%20area%20perception%20algorithms%20primarily%20focus%20on%20detecting%0Avacant%20slots%20within%20a%20limited%20range%2C%20relying%20on%20error-prone%20homographic%0Aprojection%20for%20both%20labeling%20and%20inference.%20However%2C%20recent%20advancements%20in%0AAdvanced%20Driver%20Assistance%20System%20%28ADAS%29%20require%20interaction%20with%20end-users%0Athrough%20comprehensive%20and%20intelligent%20Human-Machine%20Interfaces%20%28HMIs%29.%20These%0Ainterfaces%20should%20present%20a%20complete%20perception%20of%20the%20parking%20area%20going%20from%0Adistinguishing%20vacant%20slots%27%20entry%20lines%20to%20the%20orientation%20of%20other%20parked%0Avehicles.%20This%20paper%20introduces%20Multi-Task%20Fisheye%20Cross%20View%20Transformers%20%28MT%0AF-CVT%29%2C%20which%20leverages%20features%20from%20a%20four-camera%20fisheye%20Surround-view%0ACamera%20System%20%28SVCS%29%20with%20multihead%20attentions%20to%20create%20a%20detailed%20Bird-Eye%0AView%20%28BEV%29%20grid%20feature%20map.%20Features%20are%20processed%20by%20both%20a%20segmentation%0Adecoder%20and%20a%20Polygon-Yolo%20based%20object%20detection%20decoder%20for%20parking%20slots%20and%0Avehicles.%20Trained%20on%20data%20labeled%20using%20LiDAR%2C%20MT%20F-CVT%20positions%20objects%0Awithin%20a%2025m%20x%2025m%20real%20open-road%20scenes%20with%20an%20average%20error%20of%20only%2020%20cm.%0AOur%20larger%20model%20achieves%20an%20F-1%20score%20of%200.89.%20Moreover%20the%20smaller%20model%0Aoperates%20at%2016%20fps%20on%20an%20Nvidia%20Jetson%20Orin%20embedded%20board%2C%20with%20similar%0Adetection%20results%20to%20the%20larger%20one.%20MT%20F-CVT%20demonstrates%20robust%0Ageneralization%20capability%20across%20different%20vehicles%20and%20camera%20rig%0Aconfigurations.%20A%20demo%20video%20from%20an%20unseen%20vehicle%20and%20camera%20rig%20is%20available%0Aat%3A%20https%3A//streamable.com/jjw54x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12575v1&entry.124074799=Read"},
{"title": "Sampling Strategies based on Wisdom of Crowds for Amazon Deforestation\n  Detection", "author": "Hugo Resende and Eduardo B. Neto and Fabio A. M. Cappabianco and Alvaro L. Fazenda and Fabio A. Faria", "abstract": "  Conserving tropical forests is highly relevant socially and ecologically\nbecause of their critical role in the global ecosystem. However, the ongoing\ndeforestation and degradation affect millions of hectares each year,\nnecessitating government or private initiatives to ensure effective forest\nmonitoring. In April 2019, a project based on Citizen Science and Machine\nLearning models called ForestEyes (FE) was launched with the aim of providing\nsupplementary data to assist experts from government and non-profit\norganizations in their deforestation monitoring efforts. Recent research has\nshown that labeling FE project volunteers/citizen scientists helps tailor\nmachine learning models. In this sense, we adopt the FE project to create\ndifferent sampling strategies based on the wisdom of crowds to select the most\nsuitable samples from the training set to learn an SVM technique and obtain\nbetter classification results in deforestation detection tasks. In our\nexperiments, we can show that our strategy based on user entropy-increasing\nachieved the best classification results in the deforestation detection task\nwhen compared with the random sampling strategies, as well as, reducing the\nconvergence time of the SVM technique.\n", "link": "http://arxiv.org/abs/2408.12381v1", "date": "2024-08-22", "relevancy": 2.2895, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5027}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4376}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling%20Strategies%20based%20on%20Wisdom%20of%20Crowds%20for%20Amazon%20Deforestation%0A%20%20Detection&body=Title%3A%20Sampling%20Strategies%20based%20on%20Wisdom%20of%20Crowds%20for%20Amazon%20Deforestation%0A%20%20Detection%0AAuthor%3A%20Hugo%20Resende%20and%20Eduardo%20B.%20Neto%20and%20Fabio%20A.%20M.%20Cappabianco%20and%20Alvaro%20L.%20Fazenda%20and%20Fabio%20A.%20Faria%0AAbstract%3A%20%20%20Conserving%20tropical%20forests%20is%20highly%20relevant%20socially%20and%20ecologically%0Abecause%20of%20their%20critical%20role%20in%20the%20global%20ecosystem.%20However%2C%20the%20ongoing%0Adeforestation%20and%20degradation%20affect%20millions%20of%20hectares%20each%20year%2C%0Anecessitating%20government%20or%20private%20initiatives%20to%20ensure%20effective%20forest%0Amonitoring.%20In%20April%202019%2C%20a%20project%20based%20on%20Citizen%20Science%20and%20Machine%0ALearning%20models%20called%20ForestEyes%20%28FE%29%20was%20launched%20with%20the%20aim%20of%20providing%0Asupplementary%20data%20to%20assist%20experts%20from%20government%20and%20non-profit%0Aorganizations%20in%20their%20deforestation%20monitoring%20efforts.%20Recent%20research%20has%0Ashown%20that%20labeling%20FE%20project%20volunteers/citizen%20scientists%20helps%20tailor%0Amachine%20learning%20models.%20In%20this%20sense%2C%20we%20adopt%20the%20FE%20project%20to%20create%0Adifferent%20sampling%20strategies%20based%20on%20the%20wisdom%20of%20crowds%20to%20select%20the%20most%0Asuitable%20samples%20from%20the%20training%20set%20to%20learn%20an%20SVM%20technique%20and%20obtain%0Abetter%20classification%20results%20in%20deforestation%20detection%20tasks.%20In%20our%0Aexperiments%2C%20we%20can%20show%20that%20our%20strategy%20based%20on%20user%20entropy-increasing%0Aachieved%20the%20best%20classification%20results%20in%20the%20deforestation%20detection%20task%0Awhen%20compared%20with%20the%20random%20sampling%20strategies%2C%20as%20well%20as%2C%20reducing%20the%0Aconvergence%20time%20of%20the%20SVM%20technique.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling%2520Strategies%2520based%2520on%2520Wisdom%2520of%2520Crowds%2520for%2520Amazon%2520Deforestation%250A%2520%2520Detection%26entry.906535625%3DHugo%2520Resende%2520and%2520Eduardo%2520B.%2520Neto%2520and%2520Fabio%2520A.%2520M.%2520Cappabianco%2520and%2520Alvaro%2520L.%2520Fazenda%2520and%2520Fabio%2520A.%2520Faria%26entry.1292438233%3D%2520%2520Conserving%2520tropical%2520forests%2520is%2520highly%2520relevant%2520socially%2520and%2520ecologically%250Abecause%2520of%2520their%2520critical%2520role%2520in%2520the%2520global%2520ecosystem.%2520However%252C%2520the%2520ongoing%250Adeforestation%2520and%2520degradation%2520affect%2520millions%2520of%2520hectares%2520each%2520year%252C%250Anecessitating%2520government%2520or%2520private%2520initiatives%2520to%2520ensure%2520effective%2520forest%250Amonitoring.%2520In%2520April%25202019%252C%2520a%2520project%2520based%2520on%2520Citizen%2520Science%2520and%2520Machine%250ALearning%2520models%2520called%2520ForestEyes%2520%2528FE%2529%2520was%2520launched%2520with%2520the%2520aim%2520of%2520providing%250Asupplementary%2520data%2520to%2520assist%2520experts%2520from%2520government%2520and%2520non-profit%250Aorganizations%2520in%2520their%2520deforestation%2520monitoring%2520efforts.%2520Recent%2520research%2520has%250Ashown%2520that%2520labeling%2520FE%2520project%2520volunteers/citizen%2520scientists%2520helps%2520tailor%250Amachine%2520learning%2520models.%2520In%2520this%2520sense%252C%2520we%2520adopt%2520the%2520FE%2520project%2520to%2520create%250Adifferent%2520sampling%2520strategies%2520based%2520on%2520the%2520wisdom%2520of%2520crowds%2520to%2520select%2520the%2520most%250Asuitable%2520samples%2520from%2520the%2520training%2520set%2520to%2520learn%2520an%2520SVM%2520technique%2520and%2520obtain%250Abetter%2520classification%2520results%2520in%2520deforestation%2520detection%2520tasks.%2520In%2520our%250Aexperiments%252C%2520we%2520can%2520show%2520that%2520our%2520strategy%2520based%2520on%2520user%2520entropy-increasing%250Aachieved%2520the%2520best%2520classification%2520results%2520in%2520the%2520deforestation%2520detection%2520task%250Awhen%2520compared%2520with%2520the%2520random%2520sampling%2520strategies%252C%2520as%2520well%2520as%252C%2520reducing%2520the%250Aconvergence%2520time%2520of%2520the%2520SVM%2520technique.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%20Strategies%20based%20on%20Wisdom%20of%20Crowds%20for%20Amazon%20Deforestation%0A%20%20Detection&entry.906535625=Hugo%20Resende%20and%20Eduardo%20B.%20Neto%20and%20Fabio%20A.%20M.%20Cappabianco%20and%20Alvaro%20L.%20Fazenda%20and%20Fabio%20A.%20Faria&entry.1292438233=%20%20Conserving%20tropical%20forests%20is%20highly%20relevant%20socially%20and%20ecologically%0Abecause%20of%20their%20critical%20role%20in%20the%20global%20ecosystem.%20However%2C%20the%20ongoing%0Adeforestation%20and%20degradation%20affect%20millions%20of%20hectares%20each%20year%2C%0Anecessitating%20government%20or%20private%20initiatives%20to%20ensure%20effective%20forest%0Amonitoring.%20In%20April%202019%2C%20a%20project%20based%20on%20Citizen%20Science%20and%20Machine%0ALearning%20models%20called%20ForestEyes%20%28FE%29%20was%20launched%20with%20the%20aim%20of%20providing%0Asupplementary%20data%20to%20assist%20experts%20from%20government%20and%20non-profit%0Aorganizations%20in%20their%20deforestation%20monitoring%20efforts.%20Recent%20research%20has%0Ashown%20that%20labeling%20FE%20project%20volunteers/citizen%20scientists%20helps%20tailor%0Amachine%20learning%20models.%20In%20this%20sense%2C%20we%20adopt%20the%20FE%20project%20to%20create%0Adifferent%20sampling%20strategies%20based%20on%20the%20wisdom%20of%20crowds%20to%20select%20the%20most%0Asuitable%20samples%20from%20the%20training%20set%20to%20learn%20an%20SVM%20technique%20and%20obtain%0Abetter%20classification%20results%20in%20deforestation%20detection%20tasks.%20In%20our%0Aexperiments%2C%20we%20can%20show%20that%20our%20strategy%20based%20on%20user%20entropy-increasing%0Aachieved%20the%20best%20classification%20results%20in%20the%20deforestation%20detection%20task%0Awhen%20compared%20with%20the%20random%20sampling%20strategies%2C%20as%20well%20as%2C%20reducing%20the%0Aconvergence%20time%20of%20the%20SVM%20technique.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12381v1&entry.124074799=Read"},
{"title": "UMERegRobust -- Universal Manifold Embedding Compatible Features for\n  Robust Point Cloud Registration", "author": "Yuval Haitman and Amit Efraim and Joseph M. Francos", "abstract": "  In this paper, we adopt the Universal Manifold Embedding (UME) framework for\nthe estimation of rigid transformations and extend it, so that it can\naccommodate scenarios involving partial overlap and differently sampled point\nclouds. UME is a methodology designed for mapping observations of the same\nobject, related by rigid transformations, into a single low-dimensional linear\nsubspace. This process yields a transformation-invariant representation of the\nobservations, with its matrix form representation being covariant (i.e.\nequivariant) with the transformation. We extend the UME framework by\nintroducing a UME-compatible feature extraction method augmented with a unique\nUME contrastive loss and a sampling equalizer. These components are integrated\ninto a comprehensive and robust registration pipeline, named UMERegRobust. We\npropose the RotKITTI registration benchmark, specifically tailored to evaluate\nregistration methods for scenarios involving large rotations. UMERegRobust\nachieves better than state-of-the-art performance on the KITTI benchmark,\nespecially when strict precision of (1{\\deg}, 10cm) is considered (with an\naverage gain of +9%), and notably outperform SOTA methods on the RotKITTI\nbenchmark (with +45% gain compared the most recent SOTA method). Our code is\navailable at https://github.com/yuvalH9/UMERegRobust.\n", "link": "http://arxiv.org/abs/2408.12380v1", "date": "2024-08-22", "relevancy": 2.2725, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6081}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5673}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UMERegRobust%20--%20Universal%20Manifold%20Embedding%20Compatible%20Features%20for%0A%20%20Robust%20Point%20Cloud%20Registration&body=Title%3A%20UMERegRobust%20--%20Universal%20Manifold%20Embedding%20Compatible%20Features%20for%0A%20%20Robust%20Point%20Cloud%20Registration%0AAuthor%3A%20Yuval%20Haitman%20and%20Amit%20Efraim%20and%20Joseph%20M.%20Francos%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20adopt%20the%20Universal%20Manifold%20Embedding%20%28UME%29%20framework%20for%0Athe%20estimation%20of%20rigid%20transformations%20and%20extend%20it%2C%20so%20that%20it%20can%0Aaccommodate%20scenarios%20involving%20partial%20overlap%20and%20differently%20sampled%20point%0Aclouds.%20UME%20is%20a%20methodology%20designed%20for%20mapping%20observations%20of%20the%20same%0Aobject%2C%20related%20by%20rigid%20transformations%2C%20into%20a%20single%20low-dimensional%20linear%0Asubspace.%20This%20process%20yields%20a%20transformation-invariant%20representation%20of%20the%0Aobservations%2C%20with%20its%20matrix%20form%20representation%20being%20covariant%20%28i.e.%0Aequivariant%29%20with%20the%20transformation.%20We%20extend%20the%20UME%20framework%20by%0Aintroducing%20a%20UME-compatible%20feature%20extraction%20method%20augmented%20with%20a%20unique%0AUME%20contrastive%20loss%20and%20a%20sampling%20equalizer.%20These%20components%20are%20integrated%0Ainto%20a%20comprehensive%20and%20robust%20registration%20pipeline%2C%20named%20UMERegRobust.%20We%0Apropose%20the%20RotKITTI%20registration%20benchmark%2C%20specifically%20tailored%20to%20evaluate%0Aregistration%20methods%20for%20scenarios%20involving%20large%20rotations.%20UMERegRobust%0Aachieves%20better%20than%20state-of-the-art%20performance%20on%20the%20KITTI%20benchmark%2C%0Aespecially%20when%20strict%20precision%20of%20%281%7B%5Cdeg%7D%2C%2010cm%29%20is%20considered%20%28with%20an%0Aaverage%20gain%20of%20%2B9%25%29%2C%20and%20notably%20outperform%20SOTA%20methods%20on%20the%20RotKITTI%0Abenchmark%20%28with%20%2B45%25%20gain%20compared%20the%20most%20recent%20SOTA%20method%29.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/yuvalH9/UMERegRobust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUMERegRobust%2520--%2520Universal%2520Manifold%2520Embedding%2520Compatible%2520Features%2520for%250A%2520%2520Robust%2520Point%2520Cloud%2520Registration%26entry.906535625%3DYuval%2520Haitman%2520and%2520Amit%2520Efraim%2520and%2520Joseph%2520M.%2520Francos%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520adopt%2520the%2520Universal%2520Manifold%2520Embedding%2520%2528UME%2529%2520framework%2520for%250Athe%2520estimation%2520of%2520rigid%2520transformations%2520and%2520extend%2520it%252C%2520so%2520that%2520it%2520can%250Aaccommodate%2520scenarios%2520involving%2520partial%2520overlap%2520and%2520differently%2520sampled%2520point%250Aclouds.%2520UME%2520is%2520a%2520methodology%2520designed%2520for%2520mapping%2520observations%2520of%2520the%2520same%250Aobject%252C%2520related%2520by%2520rigid%2520transformations%252C%2520into%2520a%2520single%2520low-dimensional%2520linear%250Asubspace.%2520This%2520process%2520yields%2520a%2520transformation-invariant%2520representation%2520of%2520the%250Aobservations%252C%2520with%2520its%2520matrix%2520form%2520representation%2520being%2520covariant%2520%2528i.e.%250Aequivariant%2529%2520with%2520the%2520transformation.%2520We%2520extend%2520the%2520UME%2520framework%2520by%250Aintroducing%2520a%2520UME-compatible%2520feature%2520extraction%2520method%2520augmented%2520with%2520a%2520unique%250AUME%2520contrastive%2520loss%2520and%2520a%2520sampling%2520equalizer.%2520These%2520components%2520are%2520integrated%250Ainto%2520a%2520comprehensive%2520and%2520robust%2520registration%2520pipeline%252C%2520named%2520UMERegRobust.%2520We%250Apropose%2520the%2520RotKITTI%2520registration%2520benchmark%252C%2520specifically%2520tailored%2520to%2520evaluate%250Aregistration%2520methods%2520for%2520scenarios%2520involving%2520large%2520rotations.%2520UMERegRobust%250Aachieves%2520better%2520than%2520state-of-the-art%2520performance%2520on%2520the%2520KITTI%2520benchmark%252C%250Aespecially%2520when%2520strict%2520precision%2520of%2520%25281%257B%255Cdeg%257D%252C%252010cm%2529%2520is%2520considered%2520%2528with%2520an%250Aaverage%2520gain%2520of%2520%252B9%2525%2529%252C%2520and%2520notably%2520outperform%2520SOTA%2520methods%2520on%2520the%2520RotKITTI%250Abenchmark%2520%2528with%2520%252B45%2525%2520gain%2520compared%2520the%2520most%2520recent%2520SOTA%2520method%2529.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/yuvalH9/UMERegRobust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UMERegRobust%20--%20Universal%20Manifold%20Embedding%20Compatible%20Features%20for%0A%20%20Robust%20Point%20Cloud%20Registration&entry.906535625=Yuval%20Haitman%20and%20Amit%20Efraim%20and%20Joseph%20M.%20Francos&entry.1292438233=%20%20In%20this%20paper%2C%20we%20adopt%20the%20Universal%20Manifold%20Embedding%20%28UME%29%20framework%20for%0Athe%20estimation%20of%20rigid%20transformations%20and%20extend%20it%2C%20so%20that%20it%20can%0Aaccommodate%20scenarios%20involving%20partial%20overlap%20and%20differently%20sampled%20point%0Aclouds.%20UME%20is%20a%20methodology%20designed%20for%20mapping%20observations%20of%20the%20same%0Aobject%2C%20related%20by%20rigid%20transformations%2C%20into%20a%20single%20low-dimensional%20linear%0Asubspace.%20This%20process%20yields%20a%20transformation-invariant%20representation%20of%20the%0Aobservations%2C%20with%20its%20matrix%20form%20representation%20being%20covariant%20%28i.e.%0Aequivariant%29%20with%20the%20transformation.%20We%20extend%20the%20UME%20framework%20by%0Aintroducing%20a%20UME-compatible%20feature%20extraction%20method%20augmented%20with%20a%20unique%0AUME%20contrastive%20loss%20and%20a%20sampling%20equalizer.%20These%20components%20are%20integrated%0Ainto%20a%20comprehensive%20and%20robust%20registration%20pipeline%2C%20named%20UMERegRobust.%20We%0Apropose%20the%20RotKITTI%20registration%20benchmark%2C%20specifically%20tailored%20to%20evaluate%0Aregistration%20methods%20for%20scenarios%20involving%20large%20rotations.%20UMERegRobust%0Aachieves%20better%20than%20state-of-the-art%20performance%20on%20the%20KITTI%20benchmark%2C%0Aespecially%20when%20strict%20precision%20of%20%281%7B%5Cdeg%7D%2C%2010cm%29%20is%20considered%20%28with%20an%0Aaverage%20gain%20of%20%2B9%25%29%2C%20and%20notably%20outperform%20SOTA%20methods%20on%20the%20RotKITTI%0Abenchmark%20%28with%20%2B45%25%20gain%20compared%20the%20most%20recent%20SOTA%20method%29.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/yuvalH9/UMERegRobust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12380v1&entry.124074799=Read"},
{"title": "VTON-HandFit: Virtual Try-on for Arbitrary Hand Pose Guided by Hand\n  Priors Embedding", "author": "Yujie Liang and Xiaobin Hu and Boyuan Jiang and Donghao Luo and Kai WU and Wenhui Han and Taisong Jin and Chengjie Wang", "abstract": "  Although diffusion-based image virtual try-on has made considerable progress,\nemerging approaches still struggle to effectively address the issue of hand\nocclusion (i.e., clothing regions occluded by the hand part), leading to a\nnotable degradation of the try-on performance. To tackle this issue widely\nexisting in real-world scenarios, we propose VTON-HandFit, leveraging the power\nof hand priors to reconstruct the appearance and structure for hand occlusion\ncases. Firstly, we tailor a Handpose Aggregation Net using the ControlNet-based\nstructure explicitly and adaptively encoding the global hand and pose priors.\nBesides, to fully exploit the hand-related structure and appearance\ninformation, we propose Hand-feature Disentanglement Embedding module to\ndisentangle the hand priors into the hand structure-parametric and\nvisual-appearance features, and customize a masked cross attention for further\ndecoupled feature embedding. Lastly, we customize a hand-canny constraint loss\nto better learn the structure edge knowledge from the hand template of model\nimage. VTON-HandFit outperforms the baselines in qualitative and quantitative\nevaluations on the public dataset and our self-collected hand-occlusion\nHandfit-3K dataset particularly for the arbitrary hand pose occlusion cases in\nreal-world scenarios. Code and dataset will be made publicly available.\n", "link": "http://arxiv.org/abs/2408.12340v1", "date": "2024-08-22", "relevancy": 2.2723, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5836}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5757}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VTON-HandFit%3A%20Virtual%20Try-on%20for%20Arbitrary%20Hand%20Pose%20Guided%20by%20Hand%0A%20%20Priors%20Embedding&body=Title%3A%20VTON-HandFit%3A%20Virtual%20Try-on%20for%20Arbitrary%20Hand%20Pose%20Guided%20by%20Hand%0A%20%20Priors%20Embedding%0AAuthor%3A%20Yujie%20Liang%20and%20Xiaobin%20Hu%20and%20Boyuan%20Jiang%20and%20Donghao%20Luo%20and%20Kai%20WU%20and%20Wenhui%20Han%20and%20Taisong%20Jin%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Although%20diffusion-based%20image%20virtual%20try-on%20has%20made%20considerable%20progress%2C%0Aemerging%20approaches%20still%20struggle%20to%20effectively%20address%20the%20issue%20of%20hand%0Aocclusion%20%28i.e.%2C%20clothing%20regions%20occluded%20by%20the%20hand%20part%29%2C%20leading%20to%20a%0Anotable%20degradation%20of%20the%20try-on%20performance.%20To%20tackle%20this%20issue%20widely%0Aexisting%20in%20real-world%20scenarios%2C%20we%20propose%20VTON-HandFit%2C%20leveraging%20the%20power%0Aof%20hand%20priors%20to%20reconstruct%20the%20appearance%20and%20structure%20for%20hand%20occlusion%0Acases.%20Firstly%2C%20we%20tailor%20a%20Handpose%20Aggregation%20Net%20using%20the%20ControlNet-based%0Astructure%20explicitly%20and%20adaptively%20encoding%20the%20global%20hand%20and%20pose%20priors.%0ABesides%2C%20to%20fully%20exploit%20the%20hand-related%20structure%20and%20appearance%0Ainformation%2C%20we%20propose%20Hand-feature%20Disentanglement%20Embedding%20module%20to%0Adisentangle%20the%20hand%20priors%20into%20the%20hand%20structure-parametric%20and%0Avisual-appearance%20features%2C%20and%20customize%20a%20masked%20cross%20attention%20for%20further%0Adecoupled%20feature%20embedding.%20Lastly%2C%20we%20customize%20a%20hand-canny%20constraint%20loss%0Ato%20better%20learn%20the%20structure%20edge%20knowledge%20from%20the%20hand%20template%20of%20model%0Aimage.%20VTON-HandFit%20outperforms%20the%20baselines%20in%20qualitative%20and%20quantitative%0Aevaluations%20on%20the%20public%20dataset%20and%20our%20self-collected%20hand-occlusion%0AHandfit-3K%20dataset%20particularly%20for%20the%20arbitrary%20hand%20pose%20occlusion%20cases%20in%0Areal-world%20scenarios.%20Code%20and%20dataset%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVTON-HandFit%253A%2520Virtual%2520Try-on%2520for%2520Arbitrary%2520Hand%2520Pose%2520Guided%2520by%2520Hand%250A%2520%2520Priors%2520Embedding%26entry.906535625%3DYujie%2520Liang%2520and%2520Xiaobin%2520Hu%2520and%2520Boyuan%2520Jiang%2520and%2520Donghao%2520Luo%2520and%2520Kai%2520WU%2520and%2520Wenhui%2520Han%2520and%2520Taisong%2520Jin%2520and%2520Chengjie%2520Wang%26entry.1292438233%3D%2520%2520Although%2520diffusion-based%2520image%2520virtual%2520try-on%2520has%2520made%2520considerable%2520progress%252C%250Aemerging%2520approaches%2520still%2520struggle%2520to%2520effectively%2520address%2520the%2520issue%2520of%2520hand%250Aocclusion%2520%2528i.e.%252C%2520clothing%2520regions%2520occluded%2520by%2520the%2520hand%2520part%2529%252C%2520leading%2520to%2520a%250Anotable%2520degradation%2520of%2520the%2520try-on%2520performance.%2520To%2520tackle%2520this%2520issue%2520widely%250Aexisting%2520in%2520real-world%2520scenarios%252C%2520we%2520propose%2520VTON-HandFit%252C%2520leveraging%2520the%2520power%250Aof%2520hand%2520priors%2520to%2520reconstruct%2520the%2520appearance%2520and%2520structure%2520for%2520hand%2520occlusion%250Acases.%2520Firstly%252C%2520we%2520tailor%2520a%2520Handpose%2520Aggregation%2520Net%2520using%2520the%2520ControlNet-based%250Astructure%2520explicitly%2520and%2520adaptively%2520encoding%2520the%2520global%2520hand%2520and%2520pose%2520priors.%250ABesides%252C%2520to%2520fully%2520exploit%2520the%2520hand-related%2520structure%2520and%2520appearance%250Ainformation%252C%2520we%2520propose%2520Hand-feature%2520Disentanglement%2520Embedding%2520module%2520to%250Adisentangle%2520the%2520hand%2520priors%2520into%2520the%2520hand%2520structure-parametric%2520and%250Avisual-appearance%2520features%252C%2520and%2520customize%2520a%2520masked%2520cross%2520attention%2520for%2520further%250Adecoupled%2520feature%2520embedding.%2520Lastly%252C%2520we%2520customize%2520a%2520hand-canny%2520constraint%2520loss%250Ato%2520better%2520learn%2520the%2520structure%2520edge%2520knowledge%2520from%2520the%2520hand%2520template%2520of%2520model%250Aimage.%2520VTON-HandFit%2520outperforms%2520the%2520baselines%2520in%2520qualitative%2520and%2520quantitative%250Aevaluations%2520on%2520the%2520public%2520dataset%2520and%2520our%2520self-collected%2520hand-occlusion%250AHandfit-3K%2520dataset%2520particularly%2520for%2520the%2520arbitrary%2520hand%2520pose%2520occlusion%2520cases%2520in%250Areal-world%2520scenarios.%2520Code%2520and%2520dataset%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VTON-HandFit%3A%20Virtual%20Try-on%20for%20Arbitrary%20Hand%20Pose%20Guided%20by%20Hand%0A%20%20Priors%20Embedding&entry.906535625=Yujie%20Liang%20and%20Xiaobin%20Hu%20and%20Boyuan%20Jiang%20and%20Donghao%20Luo%20and%20Kai%20WU%20and%20Wenhui%20Han%20and%20Taisong%20Jin%20and%20Chengjie%20Wang&entry.1292438233=%20%20Although%20diffusion-based%20image%20virtual%20try-on%20has%20made%20considerable%20progress%2C%0Aemerging%20approaches%20still%20struggle%20to%20effectively%20address%20the%20issue%20of%20hand%0Aocclusion%20%28i.e.%2C%20clothing%20regions%20occluded%20by%20the%20hand%20part%29%2C%20leading%20to%20a%0Anotable%20degradation%20of%20the%20try-on%20performance.%20To%20tackle%20this%20issue%20widely%0Aexisting%20in%20real-world%20scenarios%2C%20we%20propose%20VTON-HandFit%2C%20leveraging%20the%20power%0Aof%20hand%20priors%20to%20reconstruct%20the%20appearance%20and%20structure%20for%20hand%20occlusion%0Acases.%20Firstly%2C%20we%20tailor%20a%20Handpose%20Aggregation%20Net%20using%20the%20ControlNet-based%0Astructure%20explicitly%20and%20adaptively%20encoding%20the%20global%20hand%20and%20pose%20priors.%0ABesides%2C%20to%20fully%20exploit%20the%20hand-related%20structure%20and%20appearance%0Ainformation%2C%20we%20propose%20Hand-feature%20Disentanglement%20Embedding%20module%20to%0Adisentangle%20the%20hand%20priors%20into%20the%20hand%20structure-parametric%20and%0Avisual-appearance%20features%2C%20and%20customize%20a%20masked%20cross%20attention%20for%20further%0Adecoupled%20feature%20embedding.%20Lastly%2C%20we%20customize%20a%20hand-canny%20constraint%20loss%0Ato%20better%20learn%20the%20structure%20edge%20knowledge%20from%20the%20hand%20template%20of%20model%0Aimage.%20VTON-HandFit%20outperforms%20the%20baselines%20in%20qualitative%20and%20quantitative%0Aevaluations%20on%20the%20public%20dataset%20and%20our%20self-collected%20hand-occlusion%0AHandfit-3K%20dataset%20particularly%20for%20the%20arbitrary%20hand%20pose%20occlusion%20cases%20in%0Areal-world%20scenarios.%20Code%20and%20dataset%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12340v1&entry.124074799=Read"},
{"title": "PCGRL+: Scaling, Control and Generalization in Reinforcement Learning\n  Level Generators", "author": "Sam Earle and Zehua Jiang and Julian Togelius", "abstract": "  Procedural Content Generation via Reinforcement Learning (PCGRL) has been\nintroduced as a means by which controllable designer agents can be trained\nbased only on a set of computable metrics acting as a proxy for the level's\nquality and key characteristics. While PCGRL offers a unique set of affordances\nfor game designers, it is constrained by the compute-intensive process of\ntraining RL agents, and has so far been limited to generating relatively small\nlevels. To address this issue of scale, we implement several PCGRL environments\nin Jax so that all aspects of learning and simulation happen in parallel on the\nGPU, resulting in faster environment simulation; removing the CPU-GPU transfer\nof information bottleneck during RL training; and ultimately resulting in\nsignificantly improved training speed. We replicate several key results from\nprior works in this new framework, letting models train for much longer than\npreviously studied, and evaluating their behavior after 1 billion timesteps.\nAiming for greater control for human designers, we introduce randomized level\nsizes and frozen \"pinpoints\" of pivotal game tiles as further ways of\ncountering overfitting. To test the generalization ability of learned\ngenerators, we evaluate models on large, out-of-distribution map sizes, and\nfind that partial observation sizes learn more robust design strategies.\n", "link": "http://arxiv.org/abs/2408.12525v1", "date": "2024-08-22", "relevancy": 2.2623, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5978}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5471}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5408}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCGRL%2B%3A%20Scaling%2C%20Control%20and%20Generalization%20in%20Reinforcement%20Learning%0A%20%20Level%20Generators&body=Title%3A%20PCGRL%2B%3A%20Scaling%2C%20Control%20and%20Generalization%20in%20Reinforcement%20Learning%0A%20%20Level%20Generators%0AAuthor%3A%20Sam%20Earle%20and%20Zehua%20Jiang%20and%20Julian%20Togelius%0AAbstract%3A%20%20%20Procedural%20Content%20Generation%20via%20Reinforcement%20Learning%20%28PCGRL%29%20has%20been%0Aintroduced%20as%20a%20means%20by%20which%20controllable%20designer%20agents%20can%20be%20trained%0Abased%20only%20on%20a%20set%20of%20computable%20metrics%20acting%20as%20a%20proxy%20for%20the%20level%27s%0Aquality%20and%20key%20characteristics.%20While%20PCGRL%20offers%20a%20unique%20set%20of%20affordances%0Afor%20game%20designers%2C%20it%20is%20constrained%20by%20the%20compute-intensive%20process%20of%0Atraining%20RL%20agents%2C%20and%20has%20so%20far%20been%20limited%20to%20generating%20relatively%20small%0Alevels.%20To%20address%20this%20issue%20of%20scale%2C%20we%20implement%20several%20PCGRL%20environments%0Ain%20Jax%20so%20that%20all%20aspects%20of%20learning%20and%20simulation%20happen%20in%20parallel%20on%20the%0AGPU%2C%20resulting%20in%20faster%20environment%20simulation%3B%20removing%20the%20CPU-GPU%20transfer%0Aof%20information%20bottleneck%20during%20RL%20training%3B%20and%20ultimately%20resulting%20in%0Asignificantly%20improved%20training%20speed.%20We%20replicate%20several%20key%20results%20from%0Aprior%20works%20in%20this%20new%20framework%2C%20letting%20models%20train%20for%20much%20longer%20than%0Apreviously%20studied%2C%20and%20evaluating%20their%20behavior%20after%201%20billion%20timesteps.%0AAiming%20for%20greater%20control%20for%20human%20designers%2C%20we%20introduce%20randomized%20level%0Asizes%20and%20frozen%20%22pinpoints%22%20of%20pivotal%20game%20tiles%20as%20further%20ways%20of%0Acountering%20overfitting.%20To%20test%20the%20generalization%20ability%20of%20learned%0Agenerators%2C%20we%20evaluate%20models%20on%20large%2C%20out-of-distribution%20map%20sizes%2C%20and%0Afind%20that%20partial%20observation%20sizes%20learn%20more%20robust%20design%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCGRL%252B%253A%2520Scaling%252C%2520Control%2520and%2520Generalization%2520in%2520Reinforcement%2520Learning%250A%2520%2520Level%2520Generators%26entry.906535625%3DSam%2520Earle%2520and%2520Zehua%2520Jiang%2520and%2520Julian%2520Togelius%26entry.1292438233%3D%2520%2520Procedural%2520Content%2520Generation%2520via%2520Reinforcement%2520Learning%2520%2528PCGRL%2529%2520has%2520been%250Aintroduced%2520as%2520a%2520means%2520by%2520which%2520controllable%2520designer%2520agents%2520can%2520be%2520trained%250Abased%2520only%2520on%2520a%2520set%2520of%2520computable%2520metrics%2520acting%2520as%2520a%2520proxy%2520for%2520the%2520level%2527s%250Aquality%2520and%2520key%2520characteristics.%2520While%2520PCGRL%2520offers%2520a%2520unique%2520set%2520of%2520affordances%250Afor%2520game%2520designers%252C%2520it%2520is%2520constrained%2520by%2520the%2520compute-intensive%2520process%2520of%250Atraining%2520RL%2520agents%252C%2520and%2520has%2520so%2520far%2520been%2520limited%2520to%2520generating%2520relatively%2520small%250Alevels.%2520To%2520address%2520this%2520issue%2520of%2520scale%252C%2520we%2520implement%2520several%2520PCGRL%2520environments%250Ain%2520Jax%2520so%2520that%2520all%2520aspects%2520of%2520learning%2520and%2520simulation%2520happen%2520in%2520parallel%2520on%2520the%250AGPU%252C%2520resulting%2520in%2520faster%2520environment%2520simulation%253B%2520removing%2520the%2520CPU-GPU%2520transfer%250Aof%2520information%2520bottleneck%2520during%2520RL%2520training%253B%2520and%2520ultimately%2520resulting%2520in%250Asignificantly%2520improved%2520training%2520speed.%2520We%2520replicate%2520several%2520key%2520results%2520from%250Aprior%2520works%2520in%2520this%2520new%2520framework%252C%2520letting%2520models%2520train%2520for%2520much%2520longer%2520than%250Apreviously%2520studied%252C%2520and%2520evaluating%2520their%2520behavior%2520after%25201%2520billion%2520timesteps.%250AAiming%2520for%2520greater%2520control%2520for%2520human%2520designers%252C%2520we%2520introduce%2520randomized%2520level%250Asizes%2520and%2520frozen%2520%2522pinpoints%2522%2520of%2520pivotal%2520game%2520tiles%2520as%2520further%2520ways%2520of%250Acountering%2520overfitting.%2520To%2520test%2520the%2520generalization%2520ability%2520of%2520learned%250Agenerators%252C%2520we%2520evaluate%2520models%2520on%2520large%252C%2520out-of-distribution%2520map%2520sizes%252C%2520and%250Afind%2520that%2520partial%2520observation%2520sizes%2520learn%2520more%2520robust%2520design%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCGRL%2B%3A%20Scaling%2C%20Control%20and%20Generalization%20in%20Reinforcement%20Learning%0A%20%20Level%20Generators&entry.906535625=Sam%20Earle%20and%20Zehua%20Jiang%20and%20Julian%20Togelius&entry.1292438233=%20%20Procedural%20Content%20Generation%20via%20Reinforcement%20Learning%20%28PCGRL%29%20has%20been%0Aintroduced%20as%20a%20means%20by%20which%20controllable%20designer%20agents%20can%20be%20trained%0Abased%20only%20on%20a%20set%20of%20computable%20metrics%20acting%20as%20a%20proxy%20for%20the%20level%27s%0Aquality%20and%20key%20characteristics.%20While%20PCGRL%20offers%20a%20unique%20set%20of%20affordances%0Afor%20game%20designers%2C%20it%20is%20constrained%20by%20the%20compute-intensive%20process%20of%0Atraining%20RL%20agents%2C%20and%20has%20so%20far%20been%20limited%20to%20generating%20relatively%20small%0Alevels.%20To%20address%20this%20issue%20of%20scale%2C%20we%20implement%20several%20PCGRL%20environments%0Ain%20Jax%20so%20that%20all%20aspects%20of%20learning%20and%20simulation%20happen%20in%20parallel%20on%20the%0AGPU%2C%20resulting%20in%20faster%20environment%20simulation%3B%20removing%20the%20CPU-GPU%20transfer%0Aof%20information%20bottleneck%20during%20RL%20training%3B%20and%20ultimately%20resulting%20in%0Asignificantly%20improved%20training%20speed.%20We%20replicate%20several%20key%20results%20from%0Aprior%20works%20in%20this%20new%20framework%2C%20letting%20models%20train%20for%20much%20longer%20than%0Apreviously%20studied%2C%20and%20evaluating%20their%20behavior%20after%201%20billion%20timesteps.%0AAiming%20for%20greater%20control%20for%20human%20designers%2C%20we%20introduce%20randomized%20level%0Asizes%20and%20frozen%20%22pinpoints%22%20of%20pivotal%20game%20tiles%20as%20further%20ways%20of%0Acountering%20overfitting.%20To%20test%20the%20generalization%20ability%20of%20learned%0Agenerators%2C%20we%20evaluate%20models%20on%20large%2C%20out-of-distribution%20map%20sizes%2C%20and%0Afind%20that%20partial%20observation%20sizes%20learn%20more%20robust%20design%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12525v1&entry.124074799=Read"},
{"title": "Efficient Sensor Placement from Regression with Sparse Gaussian\n  Processes in Continuous and Discrete Spaces", "author": "Kalvik Jakkala and Srinivas Akella", "abstract": "  The sensor placement problem is a common problem that arises when monitoring\ncorrelated phenomena, such as temperature, precipitation, and salinity.\nExisting approaches to this problem typically formulate it as the maximization\nof information metrics, such as mutual information~(MI), and use optimization\nmethods such as greedy algorithms in discrete domains, and derivative-free\noptimization methods such as genetic algorithms in continuous domains. However,\ncomputing MI for sensor placement requires discretizing the environment, and\nits computation cost depends on the size of the discretized environment. These\nlimitations restrict these approaches from scaling to large problems.\n  We present a novel formulation to the SP problem based on variational\napproximation that can be optimized using gradient descent, allowing us to\nefficiently find solutions in continuous domains. We generalize our method to\nalso handle discrete environments. Our experimental results on four real-world\ndatasets demonstrate that our approach generates sensor placements consistently\non par with or better than the prior state-of-the-art approaches in terms of\nboth MI and reconstruction quality, all while being significantly faster. Our\ncomputationally efficient approach enables both large-scale sensor placement\nand fast robotic sensor placement for informative path planning algorithms.\n", "link": "http://arxiv.org/abs/2303.00028v7", "date": "2024-08-22", "relevancy": 2.2172, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5881}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Sensor%20Placement%20from%20Regression%20with%20Sparse%20Gaussian%0A%20%20Processes%20in%20Continuous%20and%20Discrete%20Spaces&body=Title%3A%20Efficient%20Sensor%20Placement%20from%20Regression%20with%20Sparse%20Gaussian%0A%20%20Processes%20in%20Continuous%20and%20Discrete%20Spaces%0AAuthor%3A%20Kalvik%20Jakkala%20and%20Srinivas%20Akella%0AAbstract%3A%20%20%20The%20sensor%20placement%20problem%20is%20a%20common%20problem%20that%20arises%20when%20monitoring%0Acorrelated%20phenomena%2C%20such%20as%20temperature%2C%20precipitation%2C%20and%20salinity.%0AExisting%20approaches%20to%20this%20problem%20typically%20formulate%20it%20as%20the%20maximization%0Aof%20information%20metrics%2C%20such%20as%20mutual%20information~%28MI%29%2C%20and%20use%20optimization%0Amethods%20such%20as%20greedy%20algorithms%20in%20discrete%20domains%2C%20and%20derivative-free%0Aoptimization%20methods%20such%20as%20genetic%20algorithms%20in%20continuous%20domains.%20However%2C%0Acomputing%20MI%20for%20sensor%20placement%20requires%20discretizing%20the%20environment%2C%20and%0Aits%20computation%20cost%20depends%20on%20the%20size%20of%20the%20discretized%20environment.%20These%0Alimitations%20restrict%20these%20approaches%20from%20scaling%20to%20large%20problems.%0A%20%20We%20present%20a%20novel%20formulation%20to%20the%20SP%20problem%20based%20on%20variational%0Aapproximation%20that%20can%20be%20optimized%20using%20gradient%20descent%2C%20allowing%20us%20to%0Aefficiently%20find%20solutions%20in%20continuous%20domains.%20We%20generalize%20our%20method%20to%0Aalso%20handle%20discrete%20environments.%20Our%20experimental%20results%20on%20four%20real-world%0Adatasets%20demonstrate%20that%20our%20approach%20generates%20sensor%20placements%20consistently%0Aon%20par%20with%20or%20better%20than%20the%20prior%20state-of-the-art%20approaches%20in%20terms%20of%0Aboth%20MI%20and%20reconstruction%20quality%2C%20all%20while%20being%20significantly%20faster.%20Our%0Acomputationally%20efficient%20approach%20enables%20both%20large-scale%20sensor%20placement%0Aand%20fast%20robotic%20sensor%20placement%20for%20informative%20path%20planning%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.00028v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Sensor%2520Placement%2520from%2520Regression%2520with%2520Sparse%2520Gaussian%250A%2520%2520Processes%2520in%2520Continuous%2520and%2520Discrete%2520Spaces%26entry.906535625%3DKalvik%2520Jakkala%2520and%2520Srinivas%2520Akella%26entry.1292438233%3D%2520%2520The%2520sensor%2520placement%2520problem%2520is%2520a%2520common%2520problem%2520that%2520arises%2520when%2520monitoring%250Acorrelated%2520phenomena%252C%2520such%2520as%2520temperature%252C%2520precipitation%252C%2520and%2520salinity.%250AExisting%2520approaches%2520to%2520this%2520problem%2520typically%2520formulate%2520it%2520as%2520the%2520maximization%250Aof%2520information%2520metrics%252C%2520such%2520as%2520mutual%2520information~%2528MI%2529%252C%2520and%2520use%2520optimization%250Amethods%2520such%2520as%2520greedy%2520algorithms%2520in%2520discrete%2520domains%252C%2520and%2520derivative-free%250Aoptimization%2520methods%2520such%2520as%2520genetic%2520algorithms%2520in%2520continuous%2520domains.%2520However%252C%250Acomputing%2520MI%2520for%2520sensor%2520placement%2520requires%2520discretizing%2520the%2520environment%252C%2520and%250Aits%2520computation%2520cost%2520depends%2520on%2520the%2520size%2520of%2520the%2520discretized%2520environment.%2520These%250Alimitations%2520restrict%2520these%2520approaches%2520from%2520scaling%2520to%2520large%2520problems.%250A%2520%2520We%2520present%2520a%2520novel%2520formulation%2520to%2520the%2520SP%2520problem%2520based%2520on%2520variational%250Aapproximation%2520that%2520can%2520be%2520optimized%2520using%2520gradient%2520descent%252C%2520allowing%2520us%2520to%250Aefficiently%2520find%2520solutions%2520in%2520continuous%2520domains.%2520We%2520generalize%2520our%2520method%2520to%250Aalso%2520handle%2520discrete%2520environments.%2520Our%2520experimental%2520results%2520on%2520four%2520real-world%250Adatasets%2520demonstrate%2520that%2520our%2520approach%2520generates%2520sensor%2520placements%2520consistently%250Aon%2520par%2520with%2520or%2520better%2520than%2520the%2520prior%2520state-of-the-art%2520approaches%2520in%2520terms%2520of%250Aboth%2520MI%2520and%2520reconstruction%2520quality%252C%2520all%2520while%2520being%2520significantly%2520faster.%2520Our%250Acomputationally%2520efficient%2520approach%2520enables%2520both%2520large-scale%2520sensor%2520placement%250Aand%2520fast%2520robotic%2520sensor%2520placement%2520for%2520informative%2520path%2520planning%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.00028v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Sensor%20Placement%20from%20Regression%20with%20Sparse%20Gaussian%0A%20%20Processes%20in%20Continuous%20and%20Discrete%20Spaces&entry.906535625=Kalvik%20Jakkala%20and%20Srinivas%20Akella&entry.1292438233=%20%20The%20sensor%20placement%20problem%20is%20a%20common%20problem%20that%20arises%20when%20monitoring%0Acorrelated%20phenomena%2C%20such%20as%20temperature%2C%20precipitation%2C%20and%20salinity.%0AExisting%20approaches%20to%20this%20problem%20typically%20formulate%20it%20as%20the%20maximization%0Aof%20information%20metrics%2C%20such%20as%20mutual%20information~%28MI%29%2C%20and%20use%20optimization%0Amethods%20such%20as%20greedy%20algorithms%20in%20discrete%20domains%2C%20and%20derivative-free%0Aoptimization%20methods%20such%20as%20genetic%20algorithms%20in%20continuous%20domains.%20However%2C%0Acomputing%20MI%20for%20sensor%20placement%20requires%20discretizing%20the%20environment%2C%20and%0Aits%20computation%20cost%20depends%20on%20the%20size%20of%20the%20discretized%20environment.%20These%0Alimitations%20restrict%20these%20approaches%20from%20scaling%20to%20large%20problems.%0A%20%20We%20present%20a%20novel%20formulation%20to%20the%20SP%20problem%20based%20on%20variational%0Aapproximation%20that%20can%20be%20optimized%20using%20gradient%20descent%2C%20allowing%20us%20to%0Aefficiently%20find%20solutions%20in%20continuous%20domains.%20We%20generalize%20our%20method%20to%0Aalso%20handle%20discrete%20environments.%20Our%20experimental%20results%20on%20four%20real-world%0Adatasets%20demonstrate%20that%20our%20approach%20generates%20sensor%20placements%20consistently%0Aon%20par%20with%20or%20better%20than%20the%20prior%20state-of-the-art%20approaches%20in%20terms%20of%0Aboth%20MI%20and%20reconstruction%20quality%2C%20all%20while%20being%20significantly%20faster.%20Our%0Acomputationally%20efficient%20approach%20enables%20both%20large-scale%20sensor%20placement%0Aand%20fast%20robotic%20sensor%20placement%20for%20informative%20path%20planning%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.00028v7&entry.124074799=Read"},
{"title": "FOUND: Foot Optimization with Uncertain Normals for Surface Deformation\n  Using Synthetic Data", "author": "Oliver Boyne and Gwangbin Bae and James Charles and Roberto Cipolla", "abstract": "  Surface reconstruction from multi-view images is a challenging task, with\nsolutions often requiring a large number of sampled images with high overlap.\nWe seek to develop a method for few-view reconstruction, for the case of the\nhuman foot. To solve this task, we must extract rich geometric cues from RGB\nimages, before carefully fusing them into a final 3D object. Our FOUND approach\ntackles this, with 4 main contributions: (i) SynFoot, a synthetic dataset of\n50,000 photorealistic foot images, paired with ground truth surface normals and\nkeypoints; (ii) an uncertainty-aware surface normal predictor trained on our\nsynthetic dataset; (iii) an optimization scheme for fitting a generative foot\nmodel to a series of images; and (iv) a benchmark dataset of calibrated images\nand high resolution ground truth geometry. We show that our normal predictor\noutperforms all off-the-shelf equivalents significantly on real images, and our\noptimization scheme outperforms state-of-the-art photogrammetry pipelines,\nespecially for a few-view setting. We release our synthetic dataset and\nbaseline 3D scans to the research community.\n", "link": "http://arxiv.org/abs/2310.18279v2", "date": "2024-08-22", "relevancy": 2.1905, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5482}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5475}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOUND%3A%20Foot%20Optimization%20with%20Uncertain%20Normals%20for%20Surface%20Deformation%0A%20%20Using%20Synthetic%20Data&body=Title%3A%20FOUND%3A%20Foot%20Optimization%20with%20Uncertain%20Normals%20for%20Surface%20Deformation%0A%20%20Using%20Synthetic%20Data%0AAuthor%3A%20Oliver%20Boyne%20and%20Gwangbin%20Bae%20and%20James%20Charles%20and%20Roberto%20Cipolla%0AAbstract%3A%20%20%20Surface%20reconstruction%20from%20multi-view%20images%20is%20a%20challenging%20task%2C%20with%0Asolutions%20often%20requiring%20a%20large%20number%20of%20sampled%20images%20with%20high%20overlap.%0AWe%20seek%20to%20develop%20a%20method%20for%20few-view%20reconstruction%2C%20for%20the%20case%20of%20the%0Ahuman%20foot.%20To%20solve%20this%20task%2C%20we%20must%20extract%20rich%20geometric%20cues%20from%20RGB%0Aimages%2C%20before%20carefully%20fusing%20them%20into%20a%20final%203D%20object.%20Our%20FOUND%20approach%0Atackles%20this%2C%20with%204%20main%20contributions%3A%20%28i%29%20SynFoot%2C%20a%20synthetic%20dataset%20of%0A50%2C000%20photorealistic%20foot%20images%2C%20paired%20with%20ground%20truth%20surface%20normals%20and%0Akeypoints%3B%20%28ii%29%20an%20uncertainty-aware%20surface%20normal%20predictor%20trained%20on%20our%0Asynthetic%20dataset%3B%20%28iii%29%20an%20optimization%20scheme%20for%20fitting%20a%20generative%20foot%0Amodel%20to%20a%20series%20of%20images%3B%20and%20%28iv%29%20a%20benchmark%20dataset%20of%20calibrated%20images%0Aand%20high%20resolution%20ground%20truth%20geometry.%20We%20show%20that%20our%20normal%20predictor%0Aoutperforms%20all%20off-the-shelf%20equivalents%20significantly%20on%20real%20images%2C%20and%20our%0Aoptimization%20scheme%20outperforms%20state-of-the-art%20photogrammetry%20pipelines%2C%0Aespecially%20for%20a%20few-view%20setting.%20We%20release%20our%20synthetic%20dataset%20and%0Abaseline%203D%20scans%20to%20the%20research%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOUND%253A%2520Foot%2520Optimization%2520with%2520Uncertain%2520Normals%2520for%2520Surface%2520Deformation%250A%2520%2520Using%2520Synthetic%2520Data%26entry.906535625%3DOliver%2520Boyne%2520and%2520Gwangbin%2520Bae%2520and%2520James%2520Charles%2520and%2520Roberto%2520Cipolla%26entry.1292438233%3D%2520%2520Surface%2520reconstruction%2520from%2520multi-view%2520images%2520is%2520a%2520challenging%2520task%252C%2520with%250Asolutions%2520often%2520requiring%2520a%2520large%2520number%2520of%2520sampled%2520images%2520with%2520high%2520overlap.%250AWe%2520seek%2520to%2520develop%2520a%2520method%2520for%2520few-view%2520reconstruction%252C%2520for%2520the%2520case%2520of%2520the%250Ahuman%2520foot.%2520To%2520solve%2520this%2520task%252C%2520we%2520must%2520extract%2520rich%2520geometric%2520cues%2520from%2520RGB%250Aimages%252C%2520before%2520carefully%2520fusing%2520them%2520into%2520a%2520final%25203D%2520object.%2520Our%2520FOUND%2520approach%250Atackles%2520this%252C%2520with%25204%2520main%2520contributions%253A%2520%2528i%2529%2520SynFoot%252C%2520a%2520synthetic%2520dataset%2520of%250A50%252C000%2520photorealistic%2520foot%2520images%252C%2520paired%2520with%2520ground%2520truth%2520surface%2520normals%2520and%250Akeypoints%253B%2520%2528ii%2529%2520an%2520uncertainty-aware%2520surface%2520normal%2520predictor%2520trained%2520on%2520our%250Asynthetic%2520dataset%253B%2520%2528iii%2529%2520an%2520optimization%2520scheme%2520for%2520fitting%2520a%2520generative%2520foot%250Amodel%2520to%2520a%2520series%2520of%2520images%253B%2520and%2520%2528iv%2529%2520a%2520benchmark%2520dataset%2520of%2520calibrated%2520images%250Aand%2520high%2520resolution%2520ground%2520truth%2520geometry.%2520We%2520show%2520that%2520our%2520normal%2520predictor%250Aoutperforms%2520all%2520off-the-shelf%2520equivalents%2520significantly%2520on%2520real%2520images%252C%2520and%2520our%250Aoptimization%2520scheme%2520outperforms%2520state-of-the-art%2520photogrammetry%2520pipelines%252C%250Aespecially%2520for%2520a%2520few-view%2520setting.%2520We%2520release%2520our%2520synthetic%2520dataset%2520and%250Abaseline%25203D%2520scans%2520to%2520the%2520research%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOUND%3A%20Foot%20Optimization%20with%20Uncertain%20Normals%20for%20Surface%20Deformation%0A%20%20Using%20Synthetic%20Data&entry.906535625=Oliver%20Boyne%20and%20Gwangbin%20Bae%20and%20James%20Charles%20and%20Roberto%20Cipolla&entry.1292438233=%20%20Surface%20reconstruction%20from%20multi-view%20images%20is%20a%20challenging%20task%2C%20with%0Asolutions%20often%20requiring%20a%20large%20number%20of%20sampled%20images%20with%20high%20overlap.%0AWe%20seek%20to%20develop%20a%20method%20for%20few-view%20reconstruction%2C%20for%20the%20case%20of%20the%0Ahuman%20foot.%20To%20solve%20this%20task%2C%20we%20must%20extract%20rich%20geometric%20cues%20from%20RGB%0Aimages%2C%20before%20carefully%20fusing%20them%20into%20a%20final%203D%20object.%20Our%20FOUND%20approach%0Atackles%20this%2C%20with%204%20main%20contributions%3A%20%28i%29%20SynFoot%2C%20a%20synthetic%20dataset%20of%0A50%2C000%20photorealistic%20foot%20images%2C%20paired%20with%20ground%20truth%20surface%20normals%20and%0Akeypoints%3B%20%28ii%29%20an%20uncertainty-aware%20surface%20normal%20predictor%20trained%20on%20our%0Asynthetic%20dataset%3B%20%28iii%29%20an%20optimization%20scheme%20for%20fitting%20a%20generative%20foot%0Amodel%20to%20a%20series%20of%20images%3B%20and%20%28iv%29%20a%20benchmark%20dataset%20of%20calibrated%20images%0Aand%20high%20resolution%20ground%20truth%20geometry.%20We%20show%20that%20our%20normal%20predictor%0Aoutperforms%20all%20off-the-shelf%20equivalents%20significantly%20on%20real%20images%2C%20and%20our%0Aoptimization%20scheme%20outperforms%20state-of-the-art%20photogrammetry%20pipelines%2C%0Aespecially%20for%20a%20few-view%20setting.%20We%20release%20our%20synthetic%20dataset%20and%0Abaseline%203D%20scans%20to%20the%20research%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18279v2&entry.124074799=Read"},
{"title": "ssProp: Energy-Efficient Training for Convolutional Neural Networks with\n  Scheduled Sparse Back Propagation", "author": "Lujia Zhong and Shuo Huang and Yonggang Shi", "abstract": "  Recently, deep learning has made remarkable strides, especially with\ngenerative modeling, such as large language models and probabilistic diffusion\nmodels. However, training these models often involves significant computational\nresources, requiring billions of petaFLOPs. This high resource consumption\nresults in substantial energy usage and a large carbon footprint, raising\ncritical environmental concerns. Back-propagation (BP) is a major source of\ncomputational expense during training deep learning models. To advance research\non energy-efficient training and allow for sparse learning on any machine and\ndevice, we propose a general, energy-efficient convolution module that can be\nseamlessly integrated into any deep learning architecture. Specifically, we\nintroduce channel-wise sparsity with additional gradient selection schedulers\nduring backward based on the assumption that BP is often dense and inefficient,\nwhich can lead to over-fitting and high computational consumption. Our\nexperiments demonstrate that our approach reduces 40\\% computations while\npotentially improving model performance, validated on image classification and\ngeneration tasks. This reduction can lead to significant energy savings and a\nlower carbon footprint during the research and development phases of\nlarge-scale AI systems. Additionally, our method mitigates over-fitting in a\nmanner distinct from Dropout, allowing it to be combined with Dropout to\nfurther enhance model performance and reduce computational resource usage.\nExtensive experiments validate that our method generalizes to a variety of\ndatasets and tasks and is compatible with a wide range of deep learning\narchitectures and modules. Code is publicly available at\nhttps://github.com/lujiazho/ssProp.\n", "link": "http://arxiv.org/abs/2408.12561v1", "date": "2024-08-22", "relevancy": 2.1807, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5523}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5428}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ssProp%3A%20Energy-Efficient%20Training%20for%20Convolutional%20Neural%20Networks%20with%0A%20%20Scheduled%20Sparse%20Back%20Propagation&body=Title%3A%20ssProp%3A%20Energy-Efficient%20Training%20for%20Convolutional%20Neural%20Networks%20with%0A%20%20Scheduled%20Sparse%20Back%20Propagation%0AAuthor%3A%20Lujia%20Zhong%20and%20Shuo%20Huang%20and%20Yonggang%20Shi%0AAbstract%3A%20%20%20Recently%2C%20deep%20learning%20has%20made%20remarkable%20strides%2C%20especially%20with%0Agenerative%20modeling%2C%20such%20as%20large%20language%20models%20and%20probabilistic%20diffusion%0Amodels.%20However%2C%20training%20these%20models%20often%20involves%20significant%20computational%0Aresources%2C%20requiring%20billions%20of%20petaFLOPs.%20This%20high%20resource%20consumption%0Aresults%20in%20substantial%20energy%20usage%20and%20a%20large%20carbon%20footprint%2C%20raising%0Acritical%20environmental%20concerns.%20Back-propagation%20%28BP%29%20is%20a%20major%20source%20of%0Acomputational%20expense%20during%20training%20deep%20learning%20models.%20To%20advance%20research%0Aon%20energy-efficient%20training%20and%20allow%20for%20sparse%20learning%20on%20any%20machine%20and%0Adevice%2C%20we%20propose%20a%20general%2C%20energy-efficient%20convolution%20module%20that%20can%20be%0Aseamlessly%20integrated%20into%20any%20deep%20learning%20architecture.%20Specifically%2C%20we%0Aintroduce%20channel-wise%20sparsity%20with%20additional%20gradient%20selection%20schedulers%0Aduring%20backward%20based%20on%20the%20assumption%20that%20BP%20is%20often%20dense%20and%20inefficient%2C%0Awhich%20can%20lead%20to%20over-fitting%20and%20high%20computational%20consumption.%20Our%0Aexperiments%20demonstrate%20that%20our%20approach%20reduces%2040%5C%25%20computations%20while%0Apotentially%20improving%20model%20performance%2C%20validated%20on%20image%20classification%20and%0Ageneration%20tasks.%20This%20reduction%20can%20lead%20to%20significant%20energy%20savings%20and%20a%0Alower%20carbon%20footprint%20during%20the%20research%20and%20development%20phases%20of%0Alarge-scale%20AI%20systems.%20Additionally%2C%20our%20method%20mitigates%20over-fitting%20in%20a%0Amanner%20distinct%20from%20Dropout%2C%20allowing%20it%20to%20be%20combined%20with%20Dropout%20to%0Afurther%20enhance%20model%20performance%20and%20reduce%20computational%20resource%20usage.%0AExtensive%20experiments%20validate%20that%20our%20method%20generalizes%20to%20a%20variety%20of%0Adatasets%20and%20tasks%20and%20is%20compatible%20with%20a%20wide%20range%20of%20deep%20learning%0Aarchitectures%20and%20modules.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/lujiazho/ssProp.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DssProp%253A%2520Energy-Efficient%2520Training%2520for%2520Convolutional%2520Neural%2520Networks%2520with%250A%2520%2520Scheduled%2520Sparse%2520Back%2520Propagation%26entry.906535625%3DLujia%2520Zhong%2520and%2520Shuo%2520Huang%2520and%2520Yonggang%2520Shi%26entry.1292438233%3D%2520%2520Recently%252C%2520deep%2520learning%2520has%2520made%2520remarkable%2520strides%252C%2520especially%2520with%250Agenerative%2520modeling%252C%2520such%2520as%2520large%2520language%2520models%2520and%2520probabilistic%2520diffusion%250Amodels.%2520However%252C%2520training%2520these%2520models%2520often%2520involves%2520significant%2520computational%250Aresources%252C%2520requiring%2520billions%2520of%2520petaFLOPs.%2520This%2520high%2520resource%2520consumption%250Aresults%2520in%2520substantial%2520energy%2520usage%2520and%2520a%2520large%2520carbon%2520footprint%252C%2520raising%250Acritical%2520environmental%2520concerns.%2520Back-propagation%2520%2528BP%2529%2520is%2520a%2520major%2520source%2520of%250Acomputational%2520expense%2520during%2520training%2520deep%2520learning%2520models.%2520To%2520advance%2520research%250Aon%2520energy-efficient%2520training%2520and%2520allow%2520for%2520sparse%2520learning%2520on%2520any%2520machine%2520and%250Adevice%252C%2520we%2520propose%2520a%2520general%252C%2520energy-efficient%2520convolution%2520module%2520that%2520can%2520be%250Aseamlessly%2520integrated%2520into%2520any%2520deep%2520learning%2520architecture.%2520Specifically%252C%2520we%250Aintroduce%2520channel-wise%2520sparsity%2520with%2520additional%2520gradient%2520selection%2520schedulers%250Aduring%2520backward%2520based%2520on%2520the%2520assumption%2520that%2520BP%2520is%2520often%2520dense%2520and%2520inefficient%252C%250Awhich%2520can%2520lead%2520to%2520over-fitting%2520and%2520high%2520computational%2520consumption.%2520Our%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520reduces%252040%255C%2525%2520computations%2520while%250Apotentially%2520improving%2520model%2520performance%252C%2520validated%2520on%2520image%2520classification%2520and%250Ageneration%2520tasks.%2520This%2520reduction%2520can%2520lead%2520to%2520significant%2520energy%2520savings%2520and%2520a%250Alower%2520carbon%2520footprint%2520during%2520the%2520research%2520and%2520development%2520phases%2520of%250Alarge-scale%2520AI%2520systems.%2520Additionally%252C%2520our%2520method%2520mitigates%2520over-fitting%2520in%2520a%250Amanner%2520distinct%2520from%2520Dropout%252C%2520allowing%2520it%2520to%2520be%2520combined%2520with%2520Dropout%2520to%250Afurther%2520enhance%2520model%2520performance%2520and%2520reduce%2520computational%2520resource%2520usage.%250AExtensive%2520experiments%2520validate%2520that%2520our%2520method%2520generalizes%2520to%2520a%2520variety%2520of%250Adatasets%2520and%2520tasks%2520and%2520is%2520compatible%2520with%2520a%2520wide%2520range%2520of%2520deep%2520learning%250Aarchitectures%2520and%2520modules.%2520Code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/lujiazho/ssProp.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ssProp%3A%20Energy-Efficient%20Training%20for%20Convolutional%20Neural%20Networks%20with%0A%20%20Scheduled%20Sparse%20Back%20Propagation&entry.906535625=Lujia%20Zhong%20and%20Shuo%20Huang%20and%20Yonggang%20Shi&entry.1292438233=%20%20Recently%2C%20deep%20learning%20has%20made%20remarkable%20strides%2C%20especially%20with%0Agenerative%20modeling%2C%20such%20as%20large%20language%20models%20and%20probabilistic%20diffusion%0Amodels.%20However%2C%20training%20these%20models%20often%20involves%20significant%20computational%0Aresources%2C%20requiring%20billions%20of%20petaFLOPs.%20This%20high%20resource%20consumption%0Aresults%20in%20substantial%20energy%20usage%20and%20a%20large%20carbon%20footprint%2C%20raising%0Acritical%20environmental%20concerns.%20Back-propagation%20%28BP%29%20is%20a%20major%20source%20of%0Acomputational%20expense%20during%20training%20deep%20learning%20models.%20To%20advance%20research%0Aon%20energy-efficient%20training%20and%20allow%20for%20sparse%20learning%20on%20any%20machine%20and%0Adevice%2C%20we%20propose%20a%20general%2C%20energy-efficient%20convolution%20module%20that%20can%20be%0Aseamlessly%20integrated%20into%20any%20deep%20learning%20architecture.%20Specifically%2C%20we%0Aintroduce%20channel-wise%20sparsity%20with%20additional%20gradient%20selection%20schedulers%0Aduring%20backward%20based%20on%20the%20assumption%20that%20BP%20is%20often%20dense%20and%20inefficient%2C%0Awhich%20can%20lead%20to%20over-fitting%20and%20high%20computational%20consumption.%20Our%0Aexperiments%20demonstrate%20that%20our%20approach%20reduces%2040%5C%25%20computations%20while%0Apotentially%20improving%20model%20performance%2C%20validated%20on%20image%20classification%20and%0Ageneration%20tasks.%20This%20reduction%20can%20lead%20to%20significant%20energy%20savings%20and%20a%0Alower%20carbon%20footprint%20during%20the%20research%20and%20development%20phases%20of%0Alarge-scale%20AI%20systems.%20Additionally%2C%20our%20method%20mitigates%20over-fitting%20in%20a%0Amanner%20distinct%20from%20Dropout%2C%20allowing%20it%20to%20be%20combined%20with%20Dropout%20to%0Afurther%20enhance%20model%20performance%20and%20reduce%20computational%20resource%20usage.%0AExtensive%20experiments%20validate%20that%20our%20method%20generalizes%20to%20a%20variety%20of%0Adatasets%20and%20tasks%20and%20is%20compatible%20with%20a%20wide%20range%20of%20deep%20learning%0Aarchitectures%20and%20modules.%20Code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/lujiazho/ssProp.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12561v1&entry.124074799=Read"},
{"title": "Deep Learning Improvements for Sparse Spatial Field Reconstruction", "author": "Robert Sunderhaft and Logan Frank and Jim Davis", "abstract": "  Accurately reconstructing a global spatial field from sparse data has been a\nlongstanding problem in several domains, such as Earth Sciences and Fluid\nDynamics. Historically, scientists have approached this problem by employing\ncomplex physics models to reconstruct the spatial fields. However, these\nmethods are often computationally intensive. With the increase in popularity of\nmachine learning (ML), several researchers have applied ML to the spatial field\nreconstruction task and observed improvements in computational efficiency. One\nsuch method in arXiv:2101.00554 utilizes a sparse mask of sensor locations and\na Voronoi tessellation with sensor measurements as inputs to a convolutional\nneural network for reconstructing the global spatial field. In this work, we\npropose multiple adjustments to the aforementioned approach and show\nimprovements on geoscience and fluid dynamics simulation datasets. We identify\nand discuss scenarios that benefit the most using the proposed ML-based spatial\nfield reconstruction approach.\n", "link": "http://arxiv.org/abs/2408.12531v1", "date": "2024-08-22", "relevancy": 2.1803, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5503}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5466}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Improvements%20for%20Sparse%20Spatial%20Field%20Reconstruction&body=Title%3A%20Deep%20Learning%20Improvements%20for%20Sparse%20Spatial%20Field%20Reconstruction%0AAuthor%3A%20Robert%20Sunderhaft%20and%20Logan%20Frank%20and%20Jim%20Davis%0AAbstract%3A%20%20%20Accurately%20reconstructing%20a%20global%20spatial%20field%20from%20sparse%20data%20has%20been%20a%0Alongstanding%20problem%20in%20several%20domains%2C%20such%20as%20Earth%20Sciences%20and%20Fluid%0ADynamics.%20Historically%2C%20scientists%20have%20approached%20this%20problem%20by%20employing%0Acomplex%20physics%20models%20to%20reconstruct%20the%20spatial%20fields.%20However%2C%20these%0Amethods%20are%20often%20computationally%20intensive.%20With%20the%20increase%20in%20popularity%20of%0Amachine%20learning%20%28ML%29%2C%20several%20researchers%20have%20applied%20ML%20to%20the%20spatial%20field%0Areconstruction%20task%20and%20observed%20improvements%20in%20computational%20efficiency.%20One%0Asuch%20method%20in%20arXiv%3A2101.00554%20utilizes%20a%20sparse%20mask%20of%20sensor%20locations%20and%0Aa%20Voronoi%20tessellation%20with%20sensor%20measurements%20as%20inputs%20to%20a%20convolutional%0Aneural%20network%20for%20reconstructing%20the%20global%20spatial%20field.%20In%20this%20work%2C%20we%0Apropose%20multiple%20adjustments%20to%20the%20aforementioned%20approach%20and%20show%0Aimprovements%20on%20geoscience%20and%20fluid%20dynamics%20simulation%20datasets.%20We%20identify%0Aand%20discuss%20scenarios%20that%20benefit%20the%20most%20using%20the%20proposed%20ML-based%20spatial%0Afield%20reconstruction%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Improvements%2520for%2520Sparse%2520Spatial%2520Field%2520Reconstruction%26entry.906535625%3DRobert%2520Sunderhaft%2520and%2520Logan%2520Frank%2520and%2520Jim%2520Davis%26entry.1292438233%3D%2520%2520Accurately%2520reconstructing%2520a%2520global%2520spatial%2520field%2520from%2520sparse%2520data%2520has%2520been%2520a%250Alongstanding%2520problem%2520in%2520several%2520domains%252C%2520such%2520as%2520Earth%2520Sciences%2520and%2520Fluid%250ADynamics.%2520Historically%252C%2520scientists%2520have%2520approached%2520this%2520problem%2520by%2520employing%250Acomplex%2520physics%2520models%2520to%2520reconstruct%2520the%2520spatial%2520fields.%2520However%252C%2520these%250Amethods%2520are%2520often%2520computationally%2520intensive.%2520With%2520the%2520increase%2520in%2520popularity%2520of%250Amachine%2520learning%2520%2528ML%2529%252C%2520several%2520researchers%2520have%2520applied%2520ML%2520to%2520the%2520spatial%2520field%250Areconstruction%2520task%2520and%2520observed%2520improvements%2520in%2520computational%2520efficiency.%2520One%250Asuch%2520method%2520in%2520arXiv%253A2101.00554%2520utilizes%2520a%2520sparse%2520mask%2520of%2520sensor%2520locations%2520and%250Aa%2520Voronoi%2520tessellation%2520with%2520sensor%2520measurements%2520as%2520inputs%2520to%2520a%2520convolutional%250Aneural%2520network%2520for%2520reconstructing%2520the%2520global%2520spatial%2520field.%2520In%2520this%2520work%252C%2520we%250Apropose%2520multiple%2520adjustments%2520to%2520the%2520aforementioned%2520approach%2520and%2520show%250Aimprovements%2520on%2520geoscience%2520and%2520fluid%2520dynamics%2520simulation%2520datasets.%2520We%2520identify%250Aand%2520discuss%2520scenarios%2520that%2520benefit%2520the%2520most%2520using%2520the%2520proposed%2520ML-based%2520spatial%250Afield%2520reconstruction%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Improvements%20for%20Sparse%20Spatial%20Field%20Reconstruction&entry.906535625=Robert%20Sunderhaft%20and%20Logan%20Frank%20and%20Jim%20Davis&entry.1292438233=%20%20Accurately%20reconstructing%20a%20global%20spatial%20field%20from%20sparse%20data%20has%20been%20a%0Alongstanding%20problem%20in%20several%20domains%2C%20such%20as%20Earth%20Sciences%20and%20Fluid%0ADynamics.%20Historically%2C%20scientists%20have%20approached%20this%20problem%20by%20employing%0Acomplex%20physics%20models%20to%20reconstruct%20the%20spatial%20fields.%20However%2C%20these%0Amethods%20are%20often%20computationally%20intensive.%20With%20the%20increase%20in%20popularity%20of%0Amachine%20learning%20%28ML%29%2C%20several%20researchers%20have%20applied%20ML%20to%20the%20spatial%20field%0Areconstruction%20task%20and%20observed%20improvements%20in%20computational%20efficiency.%20One%0Asuch%20method%20in%20arXiv%3A2101.00554%20utilizes%20a%20sparse%20mask%20of%20sensor%20locations%20and%0Aa%20Voronoi%20tessellation%20with%20sensor%20measurements%20as%20inputs%20to%20a%20convolutional%0Aneural%20network%20for%20reconstructing%20the%20global%20spatial%20field.%20In%20this%20work%2C%20we%0Apropose%20multiple%20adjustments%20to%20the%20aforementioned%20approach%20and%20show%0Aimprovements%20on%20geoscience%20and%20fluid%20dynamics%20simulation%20datasets.%20We%20identify%0Aand%20discuss%20scenarios%20that%20benefit%20the%20most%20using%20the%20proposed%20ML-based%20spatial%0Afield%20reconstruction%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12531v1&entry.124074799=Read"},
{"title": "Adapt CLIP as Aggregation Instructor for Image Dehazing", "author": "Xiaozhe Zhang and Fengying Xie and Haidong Ding and Linpeng Pan and Zhenwei Shi", "abstract": "  Most dehazing methods suffer from limited receptive field and do not explore\nthe rich semantic prior encapsulated in vision-language models, which have\nproven effective in downstream tasks. In this paper, we introduce CLIPHaze, a\npioneering hybrid framework that synergizes the efficient global modeling of\nMamba with the prior knowledge and zero-shot capabilities of CLIP to address\nboth issues simultaneously. Specifically, our method employs parallel state\nspace model and window-based self-attention to obtain global contextual\ndependency and local fine-grained perception, respectively. To seamlessly\naggregate information from both paths, we introduce CLIP-instructed Aggregation\nModule (CAM). For non-homogeneous and homogeneous haze, CAM leverages zero-shot\nestimated haze density map and high-quality image embedding without degradation\ninformation to explicitly and implicitly determine the optimal neural operation\nrange for each pixel, thereby adaptively fusing two paths with different\nreceptive fields. Extensive experiments on various benchmarks demonstrate that\nCLIPHaze achieves state-of-the-art (SOTA) performance, particularly in\nnon-homogeneous haze. Code will be publicly after acceptance.\n", "link": "http://arxiv.org/abs/2408.12317v1", "date": "2024-08-22", "relevancy": 2.1775, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.575}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5373}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapt%20CLIP%20as%20Aggregation%20Instructor%20for%20Image%20Dehazing&body=Title%3A%20Adapt%20CLIP%20as%20Aggregation%20Instructor%20for%20Image%20Dehazing%0AAuthor%3A%20Xiaozhe%20Zhang%20and%20Fengying%20Xie%20and%20Haidong%20Ding%20and%20Linpeng%20Pan%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20Most%20dehazing%20methods%20suffer%20from%20limited%20receptive%20field%20and%20do%20not%20explore%0Athe%20rich%20semantic%20prior%20encapsulated%20in%20vision-language%20models%2C%20which%20have%0Aproven%20effective%20in%20downstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20CLIPHaze%2C%20a%0Apioneering%20hybrid%20framework%20that%20synergizes%20the%20efficient%20global%20modeling%20of%0AMamba%20with%20the%20prior%20knowledge%20and%20zero-shot%20capabilities%20of%20CLIP%20to%20address%0Aboth%20issues%20simultaneously.%20Specifically%2C%20our%20method%20employs%20parallel%20state%0Aspace%20model%20and%20window-based%20self-attention%20to%20obtain%20global%20contextual%0Adependency%20and%20local%20fine-grained%20perception%2C%20respectively.%20To%20seamlessly%0Aaggregate%20information%20from%20both%20paths%2C%20we%20introduce%20CLIP-instructed%20Aggregation%0AModule%20%28CAM%29.%20For%20non-homogeneous%20and%20homogeneous%20haze%2C%20CAM%20leverages%20zero-shot%0Aestimated%20haze%20density%20map%20and%20high-quality%20image%20embedding%20without%20degradation%0Ainformation%20to%20explicitly%20and%20implicitly%20determine%20the%20optimal%20neural%20operation%0Arange%20for%20each%20pixel%2C%20thereby%20adaptively%20fusing%20two%20paths%20with%20different%0Areceptive%20fields.%20Extensive%20experiments%20on%20various%20benchmarks%20demonstrate%20that%0ACLIPHaze%20achieves%20state-of-the-art%20%28SOTA%29%20performance%2C%20particularly%20in%0Anon-homogeneous%20haze.%20Code%20will%20be%20publicly%20after%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapt%2520CLIP%2520as%2520Aggregation%2520Instructor%2520for%2520Image%2520Dehazing%26entry.906535625%3DXiaozhe%2520Zhang%2520and%2520Fengying%2520Xie%2520and%2520Haidong%2520Ding%2520and%2520Linpeng%2520Pan%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3D%2520%2520Most%2520dehazing%2520methods%2520suffer%2520from%2520limited%2520receptive%2520field%2520and%2520do%2520not%2520explore%250Athe%2520rich%2520semantic%2520prior%2520encapsulated%2520in%2520vision-language%2520models%252C%2520which%2520have%250Aproven%2520effective%2520in%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CLIPHaze%252C%2520a%250Apioneering%2520hybrid%2520framework%2520that%2520synergizes%2520the%2520efficient%2520global%2520modeling%2520of%250AMamba%2520with%2520the%2520prior%2520knowledge%2520and%2520zero-shot%2520capabilities%2520of%2520CLIP%2520to%2520address%250Aboth%2520issues%2520simultaneously.%2520Specifically%252C%2520our%2520method%2520employs%2520parallel%2520state%250Aspace%2520model%2520and%2520window-based%2520self-attention%2520to%2520obtain%2520global%2520contextual%250Adependency%2520and%2520local%2520fine-grained%2520perception%252C%2520respectively.%2520To%2520seamlessly%250Aaggregate%2520information%2520from%2520both%2520paths%252C%2520we%2520introduce%2520CLIP-instructed%2520Aggregation%250AModule%2520%2528CAM%2529.%2520For%2520non-homogeneous%2520and%2520homogeneous%2520haze%252C%2520CAM%2520leverages%2520zero-shot%250Aestimated%2520haze%2520density%2520map%2520and%2520high-quality%2520image%2520embedding%2520without%2520degradation%250Ainformation%2520to%2520explicitly%2520and%2520implicitly%2520determine%2520the%2520optimal%2520neural%2520operation%250Arange%2520for%2520each%2520pixel%252C%2520thereby%2520adaptively%2520fusing%2520two%2520paths%2520with%2520different%250Areceptive%2520fields.%2520Extensive%2520experiments%2520on%2520various%2520benchmarks%2520demonstrate%2520that%250ACLIPHaze%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%2520performance%252C%2520particularly%2520in%250Anon-homogeneous%2520haze.%2520Code%2520will%2520be%2520publicly%2520after%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapt%20CLIP%20as%20Aggregation%20Instructor%20for%20Image%20Dehazing&entry.906535625=Xiaozhe%20Zhang%20and%20Fengying%20Xie%20and%20Haidong%20Ding%20and%20Linpeng%20Pan%20and%20Zhenwei%20Shi&entry.1292438233=%20%20Most%20dehazing%20methods%20suffer%20from%20limited%20receptive%20field%20and%20do%20not%20explore%0Athe%20rich%20semantic%20prior%20encapsulated%20in%20vision-language%20models%2C%20which%20have%0Aproven%20effective%20in%20downstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20CLIPHaze%2C%20a%0Apioneering%20hybrid%20framework%20that%20synergizes%20the%20efficient%20global%20modeling%20of%0AMamba%20with%20the%20prior%20knowledge%20and%20zero-shot%20capabilities%20of%20CLIP%20to%20address%0Aboth%20issues%20simultaneously.%20Specifically%2C%20our%20method%20employs%20parallel%20state%0Aspace%20model%20and%20window-based%20self-attention%20to%20obtain%20global%20contextual%0Adependency%20and%20local%20fine-grained%20perception%2C%20respectively.%20To%20seamlessly%0Aaggregate%20information%20from%20both%20paths%2C%20we%20introduce%20CLIP-instructed%20Aggregation%0AModule%20%28CAM%29.%20For%20non-homogeneous%20and%20homogeneous%20haze%2C%20CAM%20leverages%20zero-shot%0Aestimated%20haze%20density%20map%20and%20high-quality%20image%20embedding%20without%20degradation%0Ainformation%20to%20explicitly%20and%20implicitly%20determine%20the%20optimal%20neural%20operation%0Arange%20for%20each%20pixel%2C%20thereby%20adaptively%20fusing%20two%20paths%20with%20different%0Areceptive%20fields.%20Extensive%20experiments%20on%20various%20benchmarks%20demonstrate%20that%0ACLIPHaze%20achieves%20state-of-the-art%20%28SOTA%29%20performance%2C%20particularly%20in%0Anon-homogeneous%20haze.%20Code%20will%20be%20publicly%20after%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12317v1&entry.124074799=Read"},
{"title": "Sapiens: Foundation for Human Vision Models", "author": "Rawal Khirodkar and Timur Bagautdinov and Julieta Martinez and Su Zhaoen and Austin James and Peter Selednik and Stuart Anderson and Shunsuke Saito", "abstract": "  We present Sapiens, a family of models for four fundamental human-centric\nvision tasks - 2D pose estimation, body-part segmentation, depth estimation,\nand surface normal prediction. Our models natively support 1K high-resolution\ninference and are extremely easy to adapt for individual tasks by simply\nfine-tuning models pretrained on over 300 million in-the-wild human images. We\nobserve that, given the same computational budget, self-supervised pretraining\non a curated dataset of human images significantly boosts the performance for a\ndiverse set of human-centric tasks. The resulting models exhibit remarkable\ngeneralization to in-the-wild data, even when labeled data is scarce or\nentirely synthetic. Our simple model design also brings scalability - model\nperformance across tasks improves as we scale the number of parameters from 0.3\nto 2 billion. Sapiens consistently surpasses existing baselines across various\nhuman-centric benchmarks. We achieve significant improvements over the prior\nstate-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1\nmIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5%\nrelative angular error.\n", "link": "http://arxiv.org/abs/2408.12569v1", "date": "2024-08-22", "relevancy": 2.1684, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.542}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sapiens%3A%20Foundation%20for%20Human%20Vision%20Models&body=Title%3A%20Sapiens%3A%20Foundation%20for%20Human%20Vision%20Models%0AAuthor%3A%20Rawal%20Khirodkar%20and%20Timur%20Bagautdinov%20and%20Julieta%20Martinez%20and%20Su%20Zhaoen%20and%20Austin%20James%20and%20Peter%20Selednik%20and%20Stuart%20Anderson%20and%20Shunsuke%20Saito%0AAbstract%3A%20%20%20We%20present%20Sapiens%2C%20a%20family%20of%20models%20for%20four%20fundamental%20human-centric%0Avision%20tasks%20-%202D%20pose%20estimation%2C%20body-part%20segmentation%2C%20depth%20estimation%2C%0Aand%20surface%20normal%20prediction.%20Our%20models%20natively%20support%201K%20high-resolution%0Ainference%20and%20are%20extremely%20easy%20to%20adapt%20for%20individual%20tasks%20by%20simply%0Afine-tuning%20models%20pretrained%20on%20over%20300%20million%20in-the-wild%20human%20images.%20We%0Aobserve%20that%2C%20given%20the%20same%20computational%20budget%2C%20self-supervised%20pretraining%0Aon%20a%20curated%20dataset%20of%20human%20images%20significantly%20boosts%20the%20performance%20for%20a%0Adiverse%20set%20of%20human-centric%20tasks.%20The%20resulting%20models%20exhibit%20remarkable%0Ageneralization%20to%20in-the-wild%20data%2C%20even%20when%20labeled%20data%20is%20scarce%20or%0Aentirely%20synthetic.%20Our%20simple%20model%20design%20also%20brings%20scalability%20-%20model%0Aperformance%20across%20tasks%20improves%20as%20we%20scale%20the%20number%20of%20parameters%20from%200.3%0Ato%202%20billion.%20Sapiens%20consistently%20surpasses%20existing%20baselines%20across%20various%0Ahuman-centric%20benchmarks.%20We%20achieve%20significant%20improvements%20over%20the%20prior%0Astate-of-the-art%20on%20Humans-5K%20%28pose%29%20by%207.6%20mAP%2C%20Humans-2K%20%28part-seg%29%20by%2017.1%0AmIoU%2C%20Hi4D%20%28depth%29%20by%2022.4%25%20relative%20RMSE%2C%20and%20THuman2%20%28normal%29%20by%2053.5%25%0Arelative%20angular%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSapiens%253A%2520Foundation%2520for%2520Human%2520Vision%2520Models%26entry.906535625%3DRawal%2520Khirodkar%2520and%2520Timur%2520Bagautdinov%2520and%2520Julieta%2520Martinez%2520and%2520Su%2520Zhaoen%2520and%2520Austin%2520James%2520and%2520Peter%2520Selednik%2520and%2520Stuart%2520Anderson%2520and%2520Shunsuke%2520Saito%26entry.1292438233%3D%2520%2520We%2520present%2520Sapiens%252C%2520a%2520family%2520of%2520models%2520for%2520four%2520fundamental%2520human-centric%250Avision%2520tasks%2520-%25202D%2520pose%2520estimation%252C%2520body-part%2520segmentation%252C%2520depth%2520estimation%252C%250Aand%2520surface%2520normal%2520prediction.%2520Our%2520models%2520natively%2520support%25201K%2520high-resolution%250Ainference%2520and%2520are%2520extremely%2520easy%2520to%2520adapt%2520for%2520individual%2520tasks%2520by%2520simply%250Afine-tuning%2520models%2520pretrained%2520on%2520over%2520300%2520million%2520in-the-wild%2520human%2520images.%2520We%250Aobserve%2520that%252C%2520given%2520the%2520same%2520computational%2520budget%252C%2520self-supervised%2520pretraining%250Aon%2520a%2520curated%2520dataset%2520of%2520human%2520images%2520significantly%2520boosts%2520the%2520performance%2520for%2520a%250Adiverse%2520set%2520of%2520human-centric%2520tasks.%2520The%2520resulting%2520models%2520exhibit%2520remarkable%250Ageneralization%2520to%2520in-the-wild%2520data%252C%2520even%2520when%2520labeled%2520data%2520is%2520scarce%2520or%250Aentirely%2520synthetic.%2520Our%2520simple%2520model%2520design%2520also%2520brings%2520scalability%2520-%2520model%250Aperformance%2520across%2520tasks%2520improves%2520as%2520we%2520scale%2520the%2520number%2520of%2520parameters%2520from%25200.3%250Ato%25202%2520billion.%2520Sapiens%2520consistently%2520surpasses%2520existing%2520baselines%2520across%2520various%250Ahuman-centric%2520benchmarks.%2520We%2520achieve%2520significant%2520improvements%2520over%2520the%2520prior%250Astate-of-the-art%2520on%2520Humans-5K%2520%2528pose%2529%2520by%25207.6%2520mAP%252C%2520Humans-2K%2520%2528part-seg%2529%2520by%252017.1%250AmIoU%252C%2520Hi4D%2520%2528depth%2529%2520by%252022.4%2525%2520relative%2520RMSE%252C%2520and%2520THuman2%2520%2528normal%2529%2520by%252053.5%2525%250Arelative%2520angular%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sapiens%3A%20Foundation%20for%20Human%20Vision%20Models&entry.906535625=Rawal%20Khirodkar%20and%20Timur%20Bagautdinov%20and%20Julieta%20Martinez%20and%20Su%20Zhaoen%20and%20Austin%20James%20and%20Peter%20Selednik%20and%20Stuart%20Anderson%20and%20Shunsuke%20Saito&entry.1292438233=%20%20We%20present%20Sapiens%2C%20a%20family%20of%20models%20for%20four%20fundamental%20human-centric%0Avision%20tasks%20-%202D%20pose%20estimation%2C%20body-part%20segmentation%2C%20depth%20estimation%2C%0Aand%20surface%20normal%20prediction.%20Our%20models%20natively%20support%201K%20high-resolution%0Ainference%20and%20are%20extremely%20easy%20to%20adapt%20for%20individual%20tasks%20by%20simply%0Afine-tuning%20models%20pretrained%20on%20over%20300%20million%20in-the-wild%20human%20images.%20We%0Aobserve%20that%2C%20given%20the%20same%20computational%20budget%2C%20self-supervised%20pretraining%0Aon%20a%20curated%20dataset%20of%20human%20images%20significantly%20boosts%20the%20performance%20for%20a%0Adiverse%20set%20of%20human-centric%20tasks.%20The%20resulting%20models%20exhibit%20remarkable%0Ageneralization%20to%20in-the-wild%20data%2C%20even%20when%20labeled%20data%20is%20scarce%20or%0Aentirely%20synthetic.%20Our%20simple%20model%20design%20also%20brings%20scalability%20-%20model%0Aperformance%20across%20tasks%20improves%20as%20we%20scale%20the%20number%20of%20parameters%20from%200.3%0Ato%202%20billion.%20Sapiens%20consistently%20surpasses%20existing%20baselines%20across%20various%0Ahuman-centric%20benchmarks.%20We%20achieve%20significant%20improvements%20over%20the%20prior%0Astate-of-the-art%20on%20Humans-5K%20%28pose%29%20by%207.6%20mAP%2C%20Humans-2K%20%28part-seg%29%20by%2017.1%0AmIoU%2C%20Hi4D%20%28depth%29%20by%2022.4%25%20relative%20RMSE%2C%20and%20THuman2%20%28normal%29%20by%2053.5%25%0Arelative%20angular%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12569v1&entry.124074799=Read"},
{"title": "Generalized SAM: Efficient Fine-Tuning of SAM for Variable Input Image\n  Sizes", "author": "Sota Kato and Hinako Mitsuoka and Kazuhiro Hotta", "abstract": "  There has been a lot of recent research on improving the efficiency of\nfine-tuning foundation models. In this paper, we propose a novel efficient\nfine-tuning method that allows the input image size of Segment Anything Model\n(SAM) to be variable. SAM is a powerful foundational model for image\nsegmentation trained on huge datasets, but it requires fine-tuning to recognize\narbitrary classes. The input image size of SAM is fixed at 1024 x 1024,\nresulting in substantial computational demands during training. Furthermore,\nthe fixed input image size may result in the loss of image information, e.g.\ndue to fixed aspect ratios. To address this problem, we propose Generalized SAM\n(GSAM). Different from the previous methods, GSAM is the first to apply random\ncropping during training with SAM, thereby significantly reducing the\ncomputational cost of training. Experiments on datasets of various types and\nvarious pixel counts have shown that GSAM can train more efficiently than SAM\nand other fine-tuning methods for SAM, achieving comparable or higher accuracy.\n", "link": "http://arxiv.org/abs/2408.12406v1", "date": "2024-08-22", "relevancy": 2.1651, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5603}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5342}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20SAM%3A%20Efficient%20Fine-Tuning%20of%20SAM%20for%20Variable%20Input%20Image%0A%20%20Sizes&body=Title%3A%20Generalized%20SAM%3A%20Efficient%20Fine-Tuning%20of%20SAM%20for%20Variable%20Input%20Image%0A%20%20Sizes%0AAuthor%3A%20Sota%20Kato%20and%20Hinako%20Mitsuoka%20and%20Kazuhiro%20Hotta%0AAbstract%3A%20%20%20There%20has%20been%20a%20lot%20of%20recent%20research%20on%20improving%20the%20efficiency%20of%0Afine-tuning%20foundation%20models.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20efficient%0Afine-tuning%20method%20that%20allows%20the%20input%20image%20size%20of%20Segment%20Anything%20Model%0A%28SAM%29%20to%20be%20variable.%20SAM%20is%20a%20powerful%20foundational%20model%20for%20image%0Asegmentation%20trained%20on%20huge%20datasets%2C%20but%20it%20requires%20fine-tuning%20to%20recognize%0Aarbitrary%20classes.%20The%20input%20image%20size%20of%20SAM%20is%20fixed%20at%201024%20x%201024%2C%0Aresulting%20in%20substantial%20computational%20demands%20during%20training.%20Furthermore%2C%0Athe%20fixed%20input%20image%20size%20may%20result%20in%20the%20loss%20of%20image%20information%2C%20e.g.%0Adue%20to%20fixed%20aspect%20ratios.%20To%20address%20this%20problem%2C%20we%20propose%20Generalized%20SAM%0A%28GSAM%29.%20Different%20from%20the%20previous%20methods%2C%20GSAM%20is%20the%20first%20to%20apply%20random%0Acropping%20during%20training%20with%20SAM%2C%20thereby%20significantly%20reducing%20the%0Acomputational%20cost%20of%20training.%20Experiments%20on%20datasets%20of%20various%20types%20and%0Avarious%20pixel%20counts%20have%20shown%20that%20GSAM%20can%20train%20more%20efficiently%20than%20SAM%0Aand%20other%20fine-tuning%20methods%20for%20SAM%2C%20achieving%20comparable%20or%20higher%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520SAM%253A%2520Efficient%2520Fine-Tuning%2520of%2520SAM%2520for%2520Variable%2520Input%2520Image%250A%2520%2520Sizes%26entry.906535625%3DSota%2520Kato%2520and%2520Hinako%2520Mitsuoka%2520and%2520Kazuhiro%2520Hotta%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520a%2520lot%2520of%2520recent%2520research%2520on%2520improving%2520the%2520efficiency%2520of%250Afine-tuning%2520foundation%2520models.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520efficient%250Afine-tuning%2520method%2520that%2520allows%2520the%2520input%2520image%2520size%2520of%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%2520to%2520be%2520variable.%2520SAM%2520is%2520a%2520powerful%2520foundational%2520model%2520for%2520image%250Asegmentation%2520trained%2520on%2520huge%2520datasets%252C%2520but%2520it%2520requires%2520fine-tuning%2520to%2520recognize%250Aarbitrary%2520classes.%2520The%2520input%2520image%2520size%2520of%2520SAM%2520is%2520fixed%2520at%25201024%2520x%25201024%252C%250Aresulting%2520in%2520substantial%2520computational%2520demands%2520during%2520training.%2520Furthermore%252C%250Athe%2520fixed%2520input%2520image%2520size%2520may%2520result%2520in%2520the%2520loss%2520of%2520image%2520information%252C%2520e.g.%250Adue%2520to%2520fixed%2520aspect%2520ratios.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520Generalized%2520SAM%250A%2528GSAM%2529.%2520Different%2520from%2520the%2520previous%2520methods%252C%2520GSAM%2520is%2520the%2520first%2520to%2520apply%2520random%250Acropping%2520during%2520training%2520with%2520SAM%252C%2520thereby%2520significantly%2520reducing%2520the%250Acomputational%2520cost%2520of%2520training.%2520Experiments%2520on%2520datasets%2520of%2520various%2520types%2520and%250Avarious%2520pixel%2520counts%2520have%2520shown%2520that%2520GSAM%2520can%2520train%2520more%2520efficiently%2520than%2520SAM%250Aand%2520other%2520fine-tuning%2520methods%2520for%2520SAM%252C%2520achieving%2520comparable%2520or%2520higher%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20SAM%3A%20Efficient%20Fine-Tuning%20of%20SAM%20for%20Variable%20Input%20Image%0A%20%20Sizes&entry.906535625=Sota%20Kato%20and%20Hinako%20Mitsuoka%20and%20Kazuhiro%20Hotta&entry.1292438233=%20%20There%20has%20been%20a%20lot%20of%20recent%20research%20on%20improving%20the%20efficiency%20of%0Afine-tuning%20foundation%20models.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20efficient%0Afine-tuning%20method%20that%20allows%20the%20input%20image%20size%20of%20Segment%20Anything%20Model%0A%28SAM%29%20to%20be%20variable.%20SAM%20is%20a%20powerful%20foundational%20model%20for%20image%0Asegmentation%20trained%20on%20huge%20datasets%2C%20but%20it%20requires%20fine-tuning%20to%20recognize%0Aarbitrary%20classes.%20The%20input%20image%20size%20of%20SAM%20is%20fixed%20at%201024%20x%201024%2C%0Aresulting%20in%20substantial%20computational%20demands%20during%20training.%20Furthermore%2C%0Athe%20fixed%20input%20image%20size%20may%20result%20in%20the%20loss%20of%20image%20information%2C%20e.g.%0Adue%20to%20fixed%20aspect%20ratios.%20To%20address%20this%20problem%2C%20we%20propose%20Generalized%20SAM%0A%28GSAM%29.%20Different%20from%20the%20previous%20methods%2C%20GSAM%20is%20the%20first%20to%20apply%20random%0Acropping%20during%20training%20with%20SAM%2C%20thereby%20significantly%20reducing%20the%0Acomputational%20cost%20of%20training.%20Experiments%20on%20datasets%20of%20various%20types%20and%0Avarious%20pixel%20counts%20have%20shown%20that%20GSAM%20can%20train%20more%20efficiently%20than%20SAM%0Aand%20other%20fine-tuning%20methods%20for%20SAM%2C%20achieving%20comparable%20or%20higher%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12406v1&entry.124074799=Read"},
{"title": "Recursive Distributed Collaborative Aided Inertial Navigation", "author": "Roland Jung", "abstract": "  In this dissertation, we investigate the issue of robust localization in\nswarms of heterogeneous mobile agents with multiple and time-varying sensing\nmodalities. Our focus is the development of filter-based and decoupled\nestimators under the assumption that agents possess communication and\nprocessing capabilities. Based on the findings from Distributed Collaborative\nState Estimation and modular sensor fusion, we propose a novel Kalman filter\ndecoupling paradigm, which is termed Isolated Kalman Filtering (IKF). This\nparadigm is formally discussed and the treatment of delayed measurement is\nstudied. The impact of approximation made was investigated on different\nobservation graphs and the filter credibility was evaluated on a linear system\nin a Monte Carlo simulation. Finally, we propose a multi-agent modular sensor\nfusion approach based on the IKF paradigm, in order to cooperatively estimate\nthe global state of a multi-agent system in a distributed way and fuse\ninformation provided by different on-board sensors in a computationally\nefficient way. As a consequence, this approach can be performed distributed\namong agents, while (i) communication between agents is only required at the\nmoment of inter-agent joint observations, (ii) one agent acts as interim master\nto process state corrections isolated, (iii) agents can be added and removed\nfrom the swarm, (iv) each agent's full state can vary during mission (each\nlocal sensor suite can be truly modular), and (v) delayed and multi-rate sensor\nupdates are supported. Extensive evaluation on realistic simulated and\nreal-world data sets show that the proposed Isolated Kalman Filtering (IKF)\nparadigm, is applicable for both, truly modular single agent estimation and\ndistributed collaborative multi-agent estimation problems.\n", "link": "http://arxiv.org/abs/2408.12360v1", "date": "2024-08-22", "relevancy": 2.165, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5746}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5552}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5023}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Distributed%20Collaborative%20Aided%20Inertial%20Navigation&body=Title%3A%20Recursive%20Distributed%20Collaborative%20Aided%20Inertial%20Navigation%0AAuthor%3A%20Roland%20Jung%0AAbstract%3A%20%20%20In%20this%20dissertation%2C%20we%20investigate%20the%20issue%20of%20robust%20localization%20in%0Aswarms%20of%20heterogeneous%20mobile%20agents%20with%20multiple%20and%20time-varying%20sensing%0Amodalities.%20Our%20focus%20is%20the%20development%20of%20filter-based%20and%20decoupled%0Aestimators%20under%20the%20assumption%20that%20agents%20possess%20communication%20and%0Aprocessing%20capabilities.%20Based%20on%20the%20findings%20from%20Distributed%20Collaborative%0AState%20Estimation%20and%20modular%20sensor%20fusion%2C%20we%20propose%20a%20novel%20Kalman%20filter%0Adecoupling%20paradigm%2C%20which%20is%20termed%20Isolated%20Kalman%20Filtering%20%28IKF%29.%20This%0Aparadigm%20is%20formally%20discussed%20and%20the%20treatment%20of%20delayed%20measurement%20is%0Astudied.%20The%20impact%20of%20approximation%20made%20was%20investigated%20on%20different%0Aobservation%20graphs%20and%20the%20filter%20credibility%20was%20evaluated%20on%20a%20linear%20system%0Ain%20a%20Monte%20Carlo%20simulation.%20Finally%2C%20we%20propose%20a%20multi-agent%20modular%20sensor%0Afusion%20approach%20based%20on%20the%20IKF%20paradigm%2C%20in%20order%20to%20cooperatively%20estimate%0Athe%20global%20state%20of%20a%20multi-agent%20system%20in%20a%20distributed%20way%20and%20fuse%0Ainformation%20provided%20by%20different%20on-board%20sensors%20in%20a%20computationally%0Aefficient%20way.%20As%20a%20consequence%2C%20this%20approach%20can%20be%20performed%20distributed%0Aamong%20agents%2C%20while%20%28i%29%20communication%20between%20agents%20is%20only%20required%20at%20the%0Amoment%20of%20inter-agent%20joint%20observations%2C%20%28ii%29%20one%20agent%20acts%20as%20interim%20master%0Ato%20process%20state%20corrections%20isolated%2C%20%28iii%29%20agents%20can%20be%20added%20and%20removed%0Afrom%20the%20swarm%2C%20%28iv%29%20each%20agent%27s%20full%20state%20can%20vary%20during%20mission%20%28each%0Alocal%20sensor%20suite%20can%20be%20truly%20modular%29%2C%20and%20%28v%29%20delayed%20and%20multi-rate%20sensor%0Aupdates%20are%20supported.%20Extensive%20evaluation%20on%20realistic%20simulated%20and%0Areal-world%20data%20sets%20show%20that%20the%20proposed%20Isolated%20Kalman%20Filtering%20%28IKF%29%0Aparadigm%2C%20is%20applicable%20for%20both%2C%20truly%20modular%20single%20agent%20estimation%20and%0Adistributed%20collaborative%20multi-agent%20estimation%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Distributed%2520Collaborative%2520Aided%2520Inertial%2520Navigation%26entry.906535625%3DRoland%2520Jung%26entry.1292438233%3D%2520%2520In%2520this%2520dissertation%252C%2520we%2520investigate%2520the%2520issue%2520of%2520robust%2520localization%2520in%250Aswarms%2520of%2520heterogeneous%2520mobile%2520agents%2520with%2520multiple%2520and%2520time-varying%2520sensing%250Amodalities.%2520Our%2520focus%2520is%2520the%2520development%2520of%2520filter-based%2520and%2520decoupled%250Aestimators%2520under%2520the%2520assumption%2520that%2520agents%2520possess%2520communication%2520and%250Aprocessing%2520capabilities.%2520Based%2520on%2520the%2520findings%2520from%2520Distributed%2520Collaborative%250AState%2520Estimation%2520and%2520modular%2520sensor%2520fusion%252C%2520we%2520propose%2520a%2520novel%2520Kalman%2520filter%250Adecoupling%2520paradigm%252C%2520which%2520is%2520termed%2520Isolated%2520Kalman%2520Filtering%2520%2528IKF%2529.%2520This%250Aparadigm%2520is%2520formally%2520discussed%2520and%2520the%2520treatment%2520of%2520delayed%2520measurement%2520is%250Astudied.%2520The%2520impact%2520of%2520approximation%2520made%2520was%2520investigated%2520on%2520different%250Aobservation%2520graphs%2520and%2520the%2520filter%2520credibility%2520was%2520evaluated%2520on%2520a%2520linear%2520system%250Ain%2520a%2520Monte%2520Carlo%2520simulation.%2520Finally%252C%2520we%2520propose%2520a%2520multi-agent%2520modular%2520sensor%250Afusion%2520approach%2520based%2520on%2520the%2520IKF%2520paradigm%252C%2520in%2520order%2520to%2520cooperatively%2520estimate%250Athe%2520global%2520state%2520of%2520a%2520multi-agent%2520system%2520in%2520a%2520distributed%2520way%2520and%2520fuse%250Ainformation%2520provided%2520by%2520different%2520on-board%2520sensors%2520in%2520a%2520computationally%250Aefficient%2520way.%2520As%2520a%2520consequence%252C%2520this%2520approach%2520can%2520be%2520performed%2520distributed%250Aamong%2520agents%252C%2520while%2520%2528i%2529%2520communication%2520between%2520agents%2520is%2520only%2520required%2520at%2520the%250Amoment%2520of%2520inter-agent%2520joint%2520observations%252C%2520%2528ii%2529%2520one%2520agent%2520acts%2520as%2520interim%2520master%250Ato%2520process%2520state%2520corrections%2520isolated%252C%2520%2528iii%2529%2520agents%2520can%2520be%2520added%2520and%2520removed%250Afrom%2520the%2520swarm%252C%2520%2528iv%2529%2520each%2520agent%2527s%2520full%2520state%2520can%2520vary%2520during%2520mission%2520%2528each%250Alocal%2520sensor%2520suite%2520can%2520be%2520truly%2520modular%2529%252C%2520and%2520%2528v%2529%2520delayed%2520and%2520multi-rate%2520sensor%250Aupdates%2520are%2520supported.%2520Extensive%2520evaluation%2520on%2520realistic%2520simulated%2520and%250Areal-world%2520data%2520sets%2520show%2520that%2520the%2520proposed%2520Isolated%2520Kalman%2520Filtering%2520%2528IKF%2529%250Aparadigm%252C%2520is%2520applicable%2520for%2520both%252C%2520truly%2520modular%2520single%2520agent%2520estimation%2520and%250Adistributed%2520collaborative%2520multi-agent%2520estimation%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Distributed%20Collaborative%20Aided%20Inertial%20Navigation&entry.906535625=Roland%20Jung&entry.1292438233=%20%20In%20this%20dissertation%2C%20we%20investigate%20the%20issue%20of%20robust%20localization%20in%0Aswarms%20of%20heterogeneous%20mobile%20agents%20with%20multiple%20and%20time-varying%20sensing%0Amodalities.%20Our%20focus%20is%20the%20development%20of%20filter-based%20and%20decoupled%0Aestimators%20under%20the%20assumption%20that%20agents%20possess%20communication%20and%0Aprocessing%20capabilities.%20Based%20on%20the%20findings%20from%20Distributed%20Collaborative%0AState%20Estimation%20and%20modular%20sensor%20fusion%2C%20we%20propose%20a%20novel%20Kalman%20filter%0Adecoupling%20paradigm%2C%20which%20is%20termed%20Isolated%20Kalman%20Filtering%20%28IKF%29.%20This%0Aparadigm%20is%20formally%20discussed%20and%20the%20treatment%20of%20delayed%20measurement%20is%0Astudied.%20The%20impact%20of%20approximation%20made%20was%20investigated%20on%20different%0Aobservation%20graphs%20and%20the%20filter%20credibility%20was%20evaluated%20on%20a%20linear%20system%0Ain%20a%20Monte%20Carlo%20simulation.%20Finally%2C%20we%20propose%20a%20multi-agent%20modular%20sensor%0Afusion%20approach%20based%20on%20the%20IKF%20paradigm%2C%20in%20order%20to%20cooperatively%20estimate%0Athe%20global%20state%20of%20a%20multi-agent%20system%20in%20a%20distributed%20way%20and%20fuse%0Ainformation%20provided%20by%20different%20on-board%20sensors%20in%20a%20computationally%0Aefficient%20way.%20As%20a%20consequence%2C%20this%20approach%20can%20be%20performed%20distributed%0Aamong%20agents%2C%20while%20%28i%29%20communication%20between%20agents%20is%20only%20required%20at%20the%0Amoment%20of%20inter-agent%20joint%20observations%2C%20%28ii%29%20one%20agent%20acts%20as%20interim%20master%0Ato%20process%20state%20corrections%20isolated%2C%20%28iii%29%20agents%20can%20be%20added%20and%20removed%0Afrom%20the%20swarm%2C%20%28iv%29%20each%20agent%27s%20full%20state%20can%20vary%20during%20mission%20%28each%0Alocal%20sensor%20suite%20can%20be%20truly%20modular%29%2C%20and%20%28v%29%20delayed%20and%20multi-rate%20sensor%0Aupdates%20are%20supported.%20Extensive%20evaluation%20on%20realistic%20simulated%20and%0Areal-world%20data%20sets%20show%20that%20the%20proposed%20Isolated%20Kalman%20Filtering%20%28IKF%29%0Aparadigm%2C%20is%20applicable%20for%20both%2C%20truly%20modular%20single%20agent%20estimation%20and%0Adistributed%20collaborative%20multi-agent%20estimation%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12360v1&entry.124074799=Read"},
{"title": "4D Diffusion for Dynamic Protein Structure Prediction with Reference\n  Guided Motion Alignment", "author": "Kaihui Cheng and Ce Liu and Qingkun Su and Jun Wang and Liwei Zhang and Yining Tang and Yao Yao and Siyu Zhu and Yuan Qi", "abstract": "  Protein structure prediction is pivotal for understanding the\nstructure-function relationship of proteins, advancing biological research, and\nfacilitating pharmaceutical development and experimental design. While deep\nlearning methods and the expanded availability of experimental 3D protein\nstructures have accelerated structure prediction, the dynamic nature of protein\nstructures has received limited attention. This study introduces an innovative\n4D diffusion model incorporating molecular dynamics (MD) simulation data to\nlearn dynamic protein structures. Our approach is distinguished by the\nfollowing components: (1) a unified diffusion model capable of generating\ndynamic protein structures, including both the backbone and side chains,\nutilizing atomic grouping and side-chain dihedral angle predictions; (2) a\nreference network that enhances structural consistency by integrating the\nlatent embeddings of the initial 3D protein structures; and (3) a motion\nalignment module aimed at improving temporal structural coherence across\nmultiple time steps. To our knowledge, this is the first diffusion-based model\naimed at predicting protein trajectories across multiple time steps\nsimultaneously. Validation on benchmark datasets demonstrates that our model\nexhibits high accuracy in predicting dynamic 3D structures of proteins\ncontaining up to 256 amino acids over 32 time steps, effectively capturing both\nlocal flexibility in stable states and significant conformational changes.\n", "link": "http://arxiv.org/abs/2408.12419v1", "date": "2024-08-22", "relevancy": 2.1416, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5378}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5378}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D%20Diffusion%20for%20Dynamic%20Protein%20Structure%20Prediction%20with%20Reference%0A%20%20Guided%20Motion%20Alignment&body=Title%3A%204D%20Diffusion%20for%20Dynamic%20Protein%20Structure%20Prediction%20with%20Reference%0A%20%20Guided%20Motion%20Alignment%0AAuthor%3A%20Kaihui%20Cheng%20and%20Ce%20Liu%20and%20Qingkun%20Su%20and%20Jun%20Wang%20and%20Liwei%20Zhang%20and%20Yining%20Tang%20and%20Yao%20Yao%20and%20Siyu%20Zhu%20and%20Yuan%20Qi%0AAbstract%3A%20%20%20Protein%20structure%20prediction%20is%20pivotal%20for%20understanding%20the%0Astructure-function%20relationship%20of%20proteins%2C%20advancing%20biological%20research%2C%20and%0Afacilitating%20pharmaceutical%20development%20and%20experimental%20design.%20While%20deep%0Alearning%20methods%20and%20the%20expanded%20availability%20of%20experimental%203D%20protein%0Astructures%20have%20accelerated%20structure%20prediction%2C%20the%20dynamic%20nature%20of%20protein%0Astructures%20has%20received%20limited%20attention.%20This%20study%20introduces%20an%20innovative%0A4D%20diffusion%20model%20incorporating%20molecular%20dynamics%20%28MD%29%20simulation%20data%20to%0Alearn%20dynamic%20protein%20structures.%20Our%20approach%20is%20distinguished%20by%20the%0Afollowing%20components%3A%20%281%29%20a%20unified%20diffusion%20model%20capable%20of%20generating%0Adynamic%20protein%20structures%2C%20including%20both%20the%20backbone%20and%20side%20chains%2C%0Autilizing%20atomic%20grouping%20and%20side-chain%20dihedral%20angle%20predictions%3B%20%282%29%20a%0Areference%20network%20that%20enhances%20structural%20consistency%20by%20integrating%20the%0Alatent%20embeddings%20of%20the%20initial%203D%20protein%20structures%3B%20and%20%283%29%20a%20motion%0Aalignment%20module%20aimed%20at%20improving%20temporal%20structural%20coherence%20across%0Amultiple%20time%20steps.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20diffusion-based%20model%0Aaimed%20at%20predicting%20protein%20trajectories%20across%20multiple%20time%20steps%0Asimultaneously.%20Validation%20on%20benchmark%20datasets%20demonstrates%20that%20our%20model%0Aexhibits%20high%20accuracy%20in%20predicting%20dynamic%203D%20structures%20of%20proteins%0Acontaining%20up%20to%20256%20amino%20acids%20over%2032%20time%20steps%2C%20effectively%20capturing%20both%0Alocal%20flexibility%20in%20stable%20states%20and%20significant%20conformational%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D%2520Diffusion%2520for%2520Dynamic%2520Protein%2520Structure%2520Prediction%2520with%2520Reference%250A%2520%2520Guided%2520Motion%2520Alignment%26entry.906535625%3DKaihui%2520Cheng%2520and%2520Ce%2520Liu%2520and%2520Qingkun%2520Su%2520and%2520Jun%2520Wang%2520and%2520Liwei%2520Zhang%2520and%2520Yining%2520Tang%2520and%2520Yao%2520Yao%2520and%2520Siyu%2520Zhu%2520and%2520Yuan%2520Qi%26entry.1292438233%3D%2520%2520Protein%2520structure%2520prediction%2520is%2520pivotal%2520for%2520understanding%2520the%250Astructure-function%2520relationship%2520of%2520proteins%252C%2520advancing%2520biological%2520research%252C%2520and%250Afacilitating%2520pharmaceutical%2520development%2520and%2520experimental%2520design.%2520While%2520deep%250Alearning%2520methods%2520and%2520the%2520expanded%2520availability%2520of%2520experimental%25203D%2520protein%250Astructures%2520have%2520accelerated%2520structure%2520prediction%252C%2520the%2520dynamic%2520nature%2520of%2520protein%250Astructures%2520has%2520received%2520limited%2520attention.%2520This%2520study%2520introduces%2520an%2520innovative%250A4D%2520diffusion%2520model%2520incorporating%2520molecular%2520dynamics%2520%2528MD%2529%2520simulation%2520data%2520to%250Alearn%2520dynamic%2520protein%2520structures.%2520Our%2520approach%2520is%2520distinguished%2520by%2520the%250Afollowing%2520components%253A%2520%25281%2529%2520a%2520unified%2520diffusion%2520model%2520capable%2520of%2520generating%250Adynamic%2520protein%2520structures%252C%2520including%2520both%2520the%2520backbone%2520and%2520side%2520chains%252C%250Autilizing%2520atomic%2520grouping%2520and%2520side-chain%2520dihedral%2520angle%2520predictions%253B%2520%25282%2529%2520a%250Areference%2520network%2520that%2520enhances%2520structural%2520consistency%2520by%2520integrating%2520the%250Alatent%2520embeddings%2520of%2520the%2520initial%25203D%2520protein%2520structures%253B%2520and%2520%25283%2529%2520a%2520motion%250Aalignment%2520module%2520aimed%2520at%2520improving%2520temporal%2520structural%2520coherence%2520across%250Amultiple%2520time%2520steps.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520diffusion-based%2520model%250Aaimed%2520at%2520predicting%2520protein%2520trajectories%2520across%2520multiple%2520time%2520steps%250Asimultaneously.%2520Validation%2520on%2520benchmark%2520datasets%2520demonstrates%2520that%2520our%2520model%250Aexhibits%2520high%2520accuracy%2520in%2520predicting%2520dynamic%25203D%2520structures%2520of%2520proteins%250Acontaining%2520up%2520to%2520256%2520amino%2520acids%2520over%252032%2520time%2520steps%252C%2520effectively%2520capturing%2520both%250Alocal%2520flexibility%2520in%2520stable%2520states%2520and%2520significant%2520conformational%2520changes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D%20Diffusion%20for%20Dynamic%20Protein%20Structure%20Prediction%20with%20Reference%0A%20%20Guided%20Motion%20Alignment&entry.906535625=Kaihui%20Cheng%20and%20Ce%20Liu%20and%20Qingkun%20Su%20and%20Jun%20Wang%20and%20Liwei%20Zhang%20and%20Yining%20Tang%20and%20Yao%20Yao%20and%20Siyu%20Zhu%20and%20Yuan%20Qi&entry.1292438233=%20%20Protein%20structure%20prediction%20is%20pivotal%20for%20understanding%20the%0Astructure-function%20relationship%20of%20proteins%2C%20advancing%20biological%20research%2C%20and%0Afacilitating%20pharmaceutical%20development%20and%20experimental%20design.%20While%20deep%0Alearning%20methods%20and%20the%20expanded%20availability%20of%20experimental%203D%20protein%0Astructures%20have%20accelerated%20structure%20prediction%2C%20the%20dynamic%20nature%20of%20protein%0Astructures%20has%20received%20limited%20attention.%20This%20study%20introduces%20an%20innovative%0A4D%20diffusion%20model%20incorporating%20molecular%20dynamics%20%28MD%29%20simulation%20data%20to%0Alearn%20dynamic%20protein%20structures.%20Our%20approach%20is%20distinguished%20by%20the%0Afollowing%20components%3A%20%281%29%20a%20unified%20diffusion%20model%20capable%20of%20generating%0Adynamic%20protein%20structures%2C%20including%20both%20the%20backbone%20and%20side%20chains%2C%0Autilizing%20atomic%20grouping%20and%20side-chain%20dihedral%20angle%20predictions%3B%20%282%29%20a%0Areference%20network%20that%20enhances%20structural%20consistency%20by%20integrating%20the%0Alatent%20embeddings%20of%20the%20initial%203D%20protein%20structures%3B%20and%20%283%29%20a%20motion%0Aalignment%20module%20aimed%20at%20improving%20temporal%20structural%20coherence%20across%0Amultiple%20time%20steps.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20diffusion-based%20model%0Aaimed%20at%20predicting%20protein%20trajectories%20across%20multiple%20time%20steps%0Asimultaneously.%20Validation%20on%20benchmark%20datasets%20demonstrates%20that%20our%20model%0Aexhibits%20high%20accuracy%20in%20predicting%20dynamic%203D%20structures%20of%20proteins%0Acontaining%20up%20to%20256%20amino%20acids%20over%2032%20time%20steps%2C%20effectively%20capturing%20both%0Alocal%20flexibility%20in%20stable%20states%20and%20significant%20conformational%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12419v1&entry.124074799=Read"},
{"title": "Large Language Models Are Self-Taught Reasoners: Enhancing LLM\n  Applications via Tailored Problem-Solving Demonstrations", "author": "Kai Tzu-iunn Ong and Taeyoon Kwon and Jinyoung Yeo", "abstract": "  Guiding large language models with a selected set of human-authored\ndemonstrations is a common practice for improving LLM applications. However,\nhuman effort can be costly, especially in specialized domains (e.g., clinical\ndiagnosis), and does not guarantee optimal performance due to the potential\ndiscrepancy of target skills between selected demonstrations and real test\ninstances. Motivated by these, this paper explores the automatic creation of\ncustomized demonstrations, whose target skills align with the given target\ninstance. We present SELF-TAUGHT, a problem-solving framework, which\nfacilitates demonstrations that are \"tailored\" to the target problem and\n\"filtered\" for better quality (i.e., correctness) in a zero-shot manner. In 15\ntasks of multiple-choice questions of diverse domains and the diagnosis of\nAlzheimer's disease (AD) with real-world patients, SELF-TAUGHT achieves\nsuperior performance to strong baselines (e.g., Few-shot CoT, Plan-and-Solve,\nAuto-CoT). We conduct comprehensive analyses on SELF-TAUGHT, including its\ngeneralizability to existing prompting methods and different LLMs, the quality\nof its intermediate generation, and more.\n", "link": "http://arxiv.org/abs/2408.12315v1", "date": "2024-08-22", "relevancy": 2.1313, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5349}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5322}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Are%20Self-Taught%20Reasoners%3A%20Enhancing%20LLM%0A%20%20Applications%20via%20Tailored%20Problem-Solving%20Demonstrations&body=Title%3A%20Large%20Language%20Models%20Are%20Self-Taught%20Reasoners%3A%20Enhancing%20LLM%0A%20%20Applications%20via%20Tailored%20Problem-Solving%20Demonstrations%0AAuthor%3A%20Kai%20Tzu-iunn%20Ong%20and%20Taeyoon%20Kwon%20and%20Jinyoung%20Yeo%0AAbstract%3A%20%20%20Guiding%20large%20language%20models%20with%20a%20selected%20set%20of%20human-authored%0Ademonstrations%20is%20a%20common%20practice%20for%20improving%20LLM%20applications.%20However%2C%0Ahuman%20effort%20can%20be%20costly%2C%20especially%20in%20specialized%20domains%20%28e.g.%2C%20clinical%0Adiagnosis%29%2C%20and%20does%20not%20guarantee%20optimal%20performance%20due%20to%20the%20potential%0Adiscrepancy%20of%20target%20skills%20between%20selected%20demonstrations%20and%20real%20test%0Ainstances.%20Motivated%20by%20these%2C%20this%20paper%20explores%20the%20automatic%20creation%20of%0Acustomized%20demonstrations%2C%20whose%20target%20skills%20align%20with%20the%20given%20target%0Ainstance.%20We%20present%20SELF-TAUGHT%2C%20a%20problem-solving%20framework%2C%20which%0Afacilitates%20demonstrations%20that%20are%20%22tailored%22%20to%20the%20target%20problem%20and%0A%22filtered%22%20for%20better%20quality%20%28i.e.%2C%20correctness%29%20in%20a%20zero-shot%20manner.%20In%2015%0Atasks%20of%20multiple-choice%20questions%20of%20diverse%20domains%20and%20the%20diagnosis%20of%0AAlzheimer%27s%20disease%20%28AD%29%20with%20real-world%20patients%2C%20SELF-TAUGHT%20achieves%0Asuperior%20performance%20to%20strong%20baselines%20%28e.g.%2C%20Few-shot%20CoT%2C%20Plan-and-Solve%2C%0AAuto-CoT%29.%20We%20conduct%20comprehensive%20analyses%20on%20SELF-TAUGHT%2C%20including%20its%0Ageneralizability%20to%20existing%20prompting%20methods%20and%20different%20LLMs%2C%20the%20quality%0Aof%20its%20intermediate%20generation%2C%20and%20more.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Are%2520Self-Taught%2520Reasoners%253A%2520Enhancing%2520LLM%250A%2520%2520Applications%2520via%2520Tailored%2520Problem-Solving%2520Demonstrations%26entry.906535625%3DKai%2520Tzu-iunn%2520Ong%2520and%2520Taeyoon%2520Kwon%2520and%2520Jinyoung%2520Yeo%26entry.1292438233%3D%2520%2520Guiding%2520large%2520language%2520models%2520with%2520a%2520selected%2520set%2520of%2520human-authored%250Ademonstrations%2520is%2520a%2520common%2520practice%2520for%2520improving%2520LLM%2520applications.%2520However%252C%250Ahuman%2520effort%2520can%2520be%2520costly%252C%2520especially%2520in%2520specialized%2520domains%2520%2528e.g.%252C%2520clinical%250Adiagnosis%2529%252C%2520and%2520does%2520not%2520guarantee%2520optimal%2520performance%2520due%2520to%2520the%2520potential%250Adiscrepancy%2520of%2520target%2520skills%2520between%2520selected%2520demonstrations%2520and%2520real%2520test%250Ainstances.%2520Motivated%2520by%2520these%252C%2520this%2520paper%2520explores%2520the%2520automatic%2520creation%2520of%250Acustomized%2520demonstrations%252C%2520whose%2520target%2520skills%2520align%2520with%2520the%2520given%2520target%250Ainstance.%2520We%2520present%2520SELF-TAUGHT%252C%2520a%2520problem-solving%2520framework%252C%2520which%250Afacilitates%2520demonstrations%2520that%2520are%2520%2522tailored%2522%2520to%2520the%2520target%2520problem%2520and%250A%2522filtered%2522%2520for%2520better%2520quality%2520%2528i.e.%252C%2520correctness%2529%2520in%2520a%2520zero-shot%2520manner.%2520In%252015%250Atasks%2520of%2520multiple-choice%2520questions%2520of%2520diverse%2520domains%2520and%2520the%2520diagnosis%2520of%250AAlzheimer%2527s%2520disease%2520%2528AD%2529%2520with%2520real-world%2520patients%252C%2520SELF-TAUGHT%2520achieves%250Asuperior%2520performance%2520to%2520strong%2520baselines%2520%2528e.g.%252C%2520Few-shot%2520CoT%252C%2520Plan-and-Solve%252C%250AAuto-CoT%2529.%2520We%2520conduct%2520comprehensive%2520analyses%2520on%2520SELF-TAUGHT%252C%2520including%2520its%250Ageneralizability%2520to%2520existing%2520prompting%2520methods%2520and%2520different%2520LLMs%252C%2520the%2520quality%250Aof%2520its%2520intermediate%2520generation%252C%2520and%2520more.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Are%20Self-Taught%20Reasoners%3A%20Enhancing%20LLM%0A%20%20Applications%20via%20Tailored%20Problem-Solving%20Demonstrations&entry.906535625=Kai%20Tzu-iunn%20Ong%20and%20Taeyoon%20Kwon%20and%20Jinyoung%20Yeo&entry.1292438233=%20%20Guiding%20large%20language%20models%20with%20a%20selected%20set%20of%20human-authored%0Ademonstrations%20is%20a%20common%20practice%20for%20improving%20LLM%20applications.%20However%2C%0Ahuman%20effort%20can%20be%20costly%2C%20especially%20in%20specialized%20domains%20%28e.g.%2C%20clinical%0Adiagnosis%29%2C%20and%20does%20not%20guarantee%20optimal%20performance%20due%20to%20the%20potential%0Adiscrepancy%20of%20target%20skills%20between%20selected%20demonstrations%20and%20real%20test%0Ainstances.%20Motivated%20by%20these%2C%20this%20paper%20explores%20the%20automatic%20creation%20of%0Acustomized%20demonstrations%2C%20whose%20target%20skills%20align%20with%20the%20given%20target%0Ainstance.%20We%20present%20SELF-TAUGHT%2C%20a%20problem-solving%20framework%2C%20which%0Afacilitates%20demonstrations%20that%20are%20%22tailored%22%20to%20the%20target%20problem%20and%0A%22filtered%22%20for%20better%20quality%20%28i.e.%2C%20correctness%29%20in%20a%20zero-shot%20manner.%20In%2015%0Atasks%20of%20multiple-choice%20questions%20of%20diverse%20domains%20and%20the%20diagnosis%20of%0AAlzheimer%27s%20disease%20%28AD%29%20with%20real-world%20patients%2C%20SELF-TAUGHT%20achieves%0Asuperior%20performance%20to%20strong%20baselines%20%28e.g.%2C%20Few-shot%20CoT%2C%20Plan-and-Solve%2C%0AAuto-CoT%29.%20We%20conduct%20comprehensive%20analyses%20on%20SELF-TAUGHT%2C%20including%20its%0Ageneralizability%20to%20existing%20prompting%20methods%20and%20different%20LLMs%2C%20the%20quality%0Aof%20its%20intermediate%20generation%2C%20and%20more.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12315v1&entry.124074799=Read"},
{"title": "Cross-Domain Foundation Model Adaptation: Pioneering Computer Vision\n  Models for Geophysical Data Analysis", "author": "Zhixiang Guo and Xinming Wu and Luming Liang and Hanlin Sheng and Nuo Chen and Zhengfa Bi", "abstract": "  We explore adapting foundation models (FMs) from the computer vision domain\nto geoscience. FMs, large neural networks trained on massive datasets, excel in\ndiverse tasks with remarkable adaptability and generality. However, geoscience\nfaces challenges like lacking curated training datasets and high computational\ncosts for developing specialized FMs. This study considers adapting FMs from\ncomputer vision to geoscience, analyzing their scale, adaptability, and\ngenerality for geoscientific data analysis. We introduce a workflow that\nleverages existing computer vision FMs, fine-tuning them for geoscientific\ntasks, reducing development costs while enhancing accuracy. Through\nexperiments, we demonstrate this workflow's effectiveness in broad applications\nto process and interpret geoscientific data of lunar images, seismic data, DAS\narrays and so on. Our findings introduce advanced ML techniques to geoscience,\nproving the feasibility and advantages of cross-domain FMs adaptation, driving\nfurther advancements in geoscientific data analysis and offering valuable\ninsights for FMs applications in other scientific domains.\n", "link": "http://arxiv.org/abs/2408.12396v1", "date": "2024-08-22", "relevancy": 2.0982, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5433}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5274}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Domain%20Foundation%20Model%20Adaptation%3A%20Pioneering%20Computer%20Vision%0A%20%20Models%20for%20Geophysical%20Data%20Analysis&body=Title%3A%20Cross-Domain%20Foundation%20Model%20Adaptation%3A%20Pioneering%20Computer%20Vision%0A%20%20Models%20for%20Geophysical%20Data%20Analysis%0AAuthor%3A%20Zhixiang%20Guo%20and%20Xinming%20Wu%20and%20Luming%20Liang%20and%20Hanlin%20Sheng%20and%20Nuo%20Chen%20and%20Zhengfa%20Bi%0AAbstract%3A%20%20%20We%20explore%20adapting%20foundation%20models%20%28FMs%29%20from%20the%20computer%20vision%20domain%0Ato%20geoscience.%20FMs%2C%20large%20neural%20networks%20trained%20on%20massive%20datasets%2C%20excel%20in%0Adiverse%20tasks%20with%20remarkable%20adaptability%20and%20generality.%20However%2C%20geoscience%0Afaces%20challenges%20like%20lacking%20curated%20training%20datasets%20and%20high%20computational%0Acosts%20for%20developing%20specialized%20FMs.%20This%20study%20considers%20adapting%20FMs%20from%0Acomputer%20vision%20to%20geoscience%2C%20analyzing%20their%20scale%2C%20adaptability%2C%20and%0Agenerality%20for%20geoscientific%20data%20analysis.%20We%20introduce%20a%20workflow%20that%0Aleverages%20existing%20computer%20vision%20FMs%2C%20fine-tuning%20them%20for%20geoscientific%0Atasks%2C%20reducing%20development%20costs%20while%20enhancing%20accuracy.%20Through%0Aexperiments%2C%20we%20demonstrate%20this%20workflow%27s%20effectiveness%20in%20broad%20applications%0Ato%20process%20and%20interpret%20geoscientific%20data%20of%20lunar%20images%2C%20seismic%20data%2C%20DAS%0Aarrays%20and%20so%20on.%20Our%20findings%20introduce%20advanced%20ML%20techniques%20to%20geoscience%2C%0Aproving%20the%20feasibility%20and%20advantages%20of%20cross-domain%20FMs%20adaptation%2C%20driving%0Afurther%20advancements%20in%20geoscientific%20data%20analysis%20and%20offering%20valuable%0Ainsights%20for%20FMs%20applications%20in%20other%20scientific%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Domain%2520Foundation%2520Model%2520Adaptation%253A%2520Pioneering%2520Computer%2520Vision%250A%2520%2520Models%2520for%2520Geophysical%2520Data%2520Analysis%26entry.906535625%3DZhixiang%2520Guo%2520and%2520Xinming%2520Wu%2520and%2520Luming%2520Liang%2520and%2520Hanlin%2520Sheng%2520and%2520Nuo%2520Chen%2520and%2520Zhengfa%2520Bi%26entry.1292438233%3D%2520%2520We%2520explore%2520adapting%2520foundation%2520models%2520%2528FMs%2529%2520from%2520the%2520computer%2520vision%2520domain%250Ato%2520geoscience.%2520FMs%252C%2520large%2520neural%2520networks%2520trained%2520on%2520massive%2520datasets%252C%2520excel%2520in%250Adiverse%2520tasks%2520with%2520remarkable%2520adaptability%2520and%2520generality.%2520However%252C%2520geoscience%250Afaces%2520challenges%2520like%2520lacking%2520curated%2520training%2520datasets%2520and%2520high%2520computational%250Acosts%2520for%2520developing%2520specialized%2520FMs.%2520This%2520study%2520considers%2520adapting%2520FMs%2520from%250Acomputer%2520vision%2520to%2520geoscience%252C%2520analyzing%2520their%2520scale%252C%2520adaptability%252C%2520and%250Agenerality%2520for%2520geoscientific%2520data%2520analysis.%2520We%2520introduce%2520a%2520workflow%2520that%250Aleverages%2520existing%2520computer%2520vision%2520FMs%252C%2520fine-tuning%2520them%2520for%2520geoscientific%250Atasks%252C%2520reducing%2520development%2520costs%2520while%2520enhancing%2520accuracy.%2520Through%250Aexperiments%252C%2520we%2520demonstrate%2520this%2520workflow%2527s%2520effectiveness%2520in%2520broad%2520applications%250Ato%2520process%2520and%2520interpret%2520geoscientific%2520data%2520of%2520lunar%2520images%252C%2520seismic%2520data%252C%2520DAS%250Aarrays%2520and%2520so%2520on.%2520Our%2520findings%2520introduce%2520advanced%2520ML%2520techniques%2520to%2520geoscience%252C%250Aproving%2520the%2520feasibility%2520and%2520advantages%2520of%2520cross-domain%2520FMs%2520adaptation%252C%2520driving%250Afurther%2520advancements%2520in%2520geoscientific%2520data%2520analysis%2520and%2520offering%2520valuable%250Ainsights%2520for%2520FMs%2520applications%2520in%2520other%2520scientific%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Domain%20Foundation%20Model%20Adaptation%3A%20Pioneering%20Computer%20Vision%0A%20%20Models%20for%20Geophysical%20Data%20Analysis&entry.906535625=Zhixiang%20Guo%20and%20Xinming%20Wu%20and%20Luming%20Liang%20and%20Hanlin%20Sheng%20and%20Nuo%20Chen%20and%20Zhengfa%20Bi&entry.1292438233=%20%20We%20explore%20adapting%20foundation%20models%20%28FMs%29%20from%20the%20computer%20vision%20domain%0Ato%20geoscience.%20FMs%2C%20large%20neural%20networks%20trained%20on%20massive%20datasets%2C%20excel%20in%0Adiverse%20tasks%20with%20remarkable%20adaptability%20and%20generality.%20However%2C%20geoscience%0Afaces%20challenges%20like%20lacking%20curated%20training%20datasets%20and%20high%20computational%0Acosts%20for%20developing%20specialized%20FMs.%20This%20study%20considers%20adapting%20FMs%20from%0Acomputer%20vision%20to%20geoscience%2C%20analyzing%20their%20scale%2C%20adaptability%2C%20and%0Agenerality%20for%20geoscientific%20data%20analysis.%20We%20introduce%20a%20workflow%20that%0Aleverages%20existing%20computer%20vision%20FMs%2C%20fine-tuning%20them%20for%20geoscientific%0Atasks%2C%20reducing%20development%20costs%20while%20enhancing%20accuracy.%20Through%0Aexperiments%2C%20we%20demonstrate%20this%20workflow%27s%20effectiveness%20in%20broad%20applications%0Ato%20process%20and%20interpret%20geoscientific%20data%20of%20lunar%20images%2C%20seismic%20data%2C%20DAS%0Aarrays%20and%20so%20on.%20Our%20findings%20introduce%20advanced%20ML%20techniques%20to%20geoscience%2C%0Aproving%20the%20feasibility%20and%20advantages%20of%20cross-domain%20FMs%20adaptation%2C%20driving%0Afurther%20advancements%20in%20geoscientific%20data%20analysis%20and%20offering%20valuable%0Ainsights%20for%20FMs%20applications%20in%20other%20scientific%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12396v1&entry.124074799=Read"},
{"title": "SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast\n  and Reliable Surface Defect Detection", "author": "Bla\u017e Rolih and Matic Fu\u010dka and Danijel Sko\u010daj", "abstract": "  The aim of surface defect detection is to identify and localise abnormal\nregions on the surfaces of captured objects, a task that's increasingly\ndemanded across various industries. Current approaches frequently fail to\nfulfil the extensive demands of these industries, which encompass high\nperformance, consistency, and fast operation, along with the capacity to\nleverage the entirety of the available training data. Addressing these gaps, we\nintroduce SuperSimpleNet, an innovative discriminative model that evolved from\nSimpleNet. This advanced model significantly enhances its predecessor's\ntraining consistency, inference time, as well as detection performance.\nSuperSimpleNet operates in an unsupervised manner using only normal training\nimages but also benefits from labelled abnormal training images when they are\navailable. SuperSimpleNet achieves state-of-the-art results in both the\nsupervised and the unsupervised settings, as demonstrated by experiments across\nfour challenging benchmark datasets. Code:\nhttps://github.com/blaz-r/SuperSimpleNet .\n", "link": "http://arxiv.org/abs/2408.03143v2", "date": "2024-08-22", "relevancy": 2.0981, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5341}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5008}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperSimpleNet%3A%20Unifying%20Unsupervised%20and%20Supervised%20Learning%20for%20Fast%0A%20%20and%20Reliable%20Surface%20Defect%20Detection&body=Title%3A%20SuperSimpleNet%3A%20Unifying%20Unsupervised%20and%20Supervised%20Learning%20for%20Fast%0A%20%20and%20Reliable%20Surface%20Defect%20Detection%0AAuthor%3A%20Bla%C5%BE%20Rolih%20and%20Matic%20Fu%C4%8Dka%20and%20Danijel%20Sko%C4%8Daj%0AAbstract%3A%20%20%20The%20aim%20of%20surface%20defect%20detection%20is%20to%20identify%20and%20localise%20abnormal%0Aregions%20on%20the%20surfaces%20of%20captured%20objects%2C%20a%20task%20that%27s%20increasingly%0Ademanded%20across%20various%20industries.%20Current%20approaches%20frequently%20fail%20to%0Afulfil%20the%20extensive%20demands%20of%20these%20industries%2C%20which%20encompass%20high%0Aperformance%2C%20consistency%2C%20and%20fast%20operation%2C%20along%20with%20the%20capacity%20to%0Aleverage%20the%20entirety%20of%20the%20available%20training%20data.%20Addressing%20these%20gaps%2C%20we%0Aintroduce%20SuperSimpleNet%2C%20an%20innovative%20discriminative%20model%20that%20evolved%20from%0ASimpleNet.%20This%20advanced%20model%20significantly%20enhances%20its%20predecessor%27s%0Atraining%20consistency%2C%20inference%20time%2C%20as%20well%20as%20detection%20performance.%0ASuperSimpleNet%20operates%20in%20an%20unsupervised%20manner%20using%20only%20normal%20training%0Aimages%20but%20also%20benefits%20from%20labelled%20abnormal%20training%20images%20when%20they%20are%0Aavailable.%20SuperSimpleNet%20achieves%20state-of-the-art%20results%20in%20both%20the%0Asupervised%20and%20the%20unsupervised%20settings%2C%20as%20demonstrated%20by%20experiments%20across%0Afour%20challenging%20benchmark%20datasets.%20Code%3A%0Ahttps%3A//github.com/blaz-r/SuperSimpleNet%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03143v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperSimpleNet%253A%2520Unifying%2520Unsupervised%2520and%2520Supervised%2520Learning%2520for%2520Fast%250A%2520%2520and%2520Reliable%2520Surface%2520Defect%2520Detection%26entry.906535625%3DBla%25C5%25BE%2520Rolih%2520and%2520Matic%2520Fu%25C4%258Dka%2520and%2520Danijel%2520Sko%25C4%258Daj%26entry.1292438233%3D%2520%2520The%2520aim%2520of%2520surface%2520defect%2520detection%2520is%2520to%2520identify%2520and%2520localise%2520abnormal%250Aregions%2520on%2520the%2520surfaces%2520of%2520captured%2520objects%252C%2520a%2520task%2520that%2527s%2520increasingly%250Ademanded%2520across%2520various%2520industries.%2520Current%2520approaches%2520frequently%2520fail%2520to%250Afulfil%2520the%2520extensive%2520demands%2520of%2520these%2520industries%252C%2520which%2520encompass%2520high%250Aperformance%252C%2520consistency%252C%2520and%2520fast%2520operation%252C%2520along%2520with%2520the%2520capacity%2520to%250Aleverage%2520the%2520entirety%2520of%2520the%2520available%2520training%2520data.%2520Addressing%2520these%2520gaps%252C%2520we%250Aintroduce%2520SuperSimpleNet%252C%2520an%2520innovative%2520discriminative%2520model%2520that%2520evolved%2520from%250ASimpleNet.%2520This%2520advanced%2520model%2520significantly%2520enhances%2520its%2520predecessor%2527s%250Atraining%2520consistency%252C%2520inference%2520time%252C%2520as%2520well%2520as%2520detection%2520performance.%250ASuperSimpleNet%2520operates%2520in%2520an%2520unsupervised%2520manner%2520using%2520only%2520normal%2520training%250Aimages%2520but%2520also%2520benefits%2520from%2520labelled%2520abnormal%2520training%2520images%2520when%2520they%2520are%250Aavailable.%2520SuperSimpleNet%2520achieves%2520state-of-the-art%2520results%2520in%2520both%2520the%250Asupervised%2520and%2520the%2520unsupervised%2520settings%252C%2520as%2520demonstrated%2520by%2520experiments%2520across%250Afour%2520challenging%2520benchmark%2520datasets.%2520Code%253A%250Ahttps%253A//github.com/blaz-r/SuperSimpleNet%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03143v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperSimpleNet%3A%20Unifying%20Unsupervised%20and%20Supervised%20Learning%20for%20Fast%0A%20%20and%20Reliable%20Surface%20Defect%20Detection&entry.906535625=Bla%C5%BE%20Rolih%20and%20Matic%20Fu%C4%8Dka%20and%20Danijel%20Sko%C4%8Daj&entry.1292438233=%20%20The%20aim%20of%20surface%20defect%20detection%20is%20to%20identify%20and%20localise%20abnormal%0Aregions%20on%20the%20surfaces%20of%20captured%20objects%2C%20a%20task%20that%27s%20increasingly%0Ademanded%20across%20various%20industries.%20Current%20approaches%20frequently%20fail%20to%0Afulfil%20the%20extensive%20demands%20of%20these%20industries%2C%20which%20encompass%20high%0Aperformance%2C%20consistency%2C%20and%20fast%20operation%2C%20along%20with%20the%20capacity%20to%0Aleverage%20the%20entirety%20of%20the%20available%20training%20data.%20Addressing%20these%20gaps%2C%20we%0Aintroduce%20SuperSimpleNet%2C%20an%20innovative%20discriminative%20model%20that%20evolved%20from%0ASimpleNet.%20This%20advanced%20model%20significantly%20enhances%20its%20predecessor%27s%0Atraining%20consistency%2C%20inference%20time%2C%20as%20well%20as%20detection%20performance.%0ASuperSimpleNet%20operates%20in%20an%20unsupervised%20manner%20using%20only%20normal%20training%0Aimages%20but%20also%20benefits%20from%20labelled%20abnormal%20training%20images%20when%20they%20are%0Aavailable.%20SuperSimpleNet%20achieves%20state-of-the-art%20results%20in%20both%20the%0Asupervised%20and%20the%20unsupervised%20settings%2C%20as%20demonstrated%20by%20experiments%20across%0Afour%20challenging%20benchmark%20datasets.%20Code%3A%0Ahttps%3A//github.com/blaz-r/SuperSimpleNet%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03143v2&entry.124074799=Read"},
{"title": "U-KAN Makes Strong Backbone for Medical Image Segmentation and\n  Generation", "author": "Chenxin Li and Xinyu Liu and Wuyang Li and Cheng Wang and Hengyu Liu and Yifan Liu and Zhen Chen and Yixuan Yuan", "abstract": "  U-Net has become a cornerstone in various visual applications such as image\nsegmentation and diffusion probability models. While numerous innovative\ndesigns and improvements have been introduced by incorporating transformers or\nMLPs, the networks are still limited to linearly modeling patterns as well as\nthe deficient interpretability. To address these challenges, our intuition is\ninspired by the impressive results of the Kolmogorov-Arnold Networks (KANs) in\nterms of accuracy and interpretability, which reshape the neural network\nlearning via the stack of non-linear learnable activation functions derived\nfrom the Kolmogorov-Anold representation theorem. Specifically, in this paper,\nwe explore the untapped potential of KANs in improving backbones for vision\ntasks. We investigate, modify and re-design the established U-Net pipeline by\nintegrating the dedicated KAN layers on the tokenized intermediate\nrepresentation, termed U-KAN. Rigorous medical image segmentation benchmarks\nverify the superiority of U-KAN by higher accuracy even with less computation\ncost. We further delved into the potential of U-KAN as an alternative U-Net\nnoise predictor in diffusion models, demonstrating its applicability in\ngenerating task-oriented model architectures. These endeavours unveil valuable\ninsights and sheds light on the prospect that with U-KAN, you can make strong\nbackbone for medical image segmentation and generation. Project\npage:\\url{https://yes-u-kan.github.io/}.\n", "link": "http://arxiv.org/abs/2406.02918v3", "date": "2024-08-22", "relevancy": 2.0931, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.541}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5356}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20U-KAN%20Makes%20Strong%20Backbone%20for%20Medical%20Image%20Segmentation%20and%0A%20%20Generation&body=Title%3A%20U-KAN%20Makes%20Strong%20Backbone%20for%20Medical%20Image%20Segmentation%20and%0A%20%20Generation%0AAuthor%3A%20Chenxin%20Li%20and%20Xinyu%20Liu%20and%20Wuyang%20Li%20and%20Cheng%20Wang%20and%20Hengyu%20Liu%20and%20Yifan%20Liu%20and%20Zhen%20Chen%20and%20Yixuan%20Yuan%0AAbstract%3A%20%20%20U-Net%20has%20become%20a%20cornerstone%20in%20various%20visual%20applications%20such%20as%20image%0Asegmentation%20and%20diffusion%20probability%20models.%20While%20numerous%20innovative%0Adesigns%20and%20improvements%20have%20been%20introduced%20by%20incorporating%20transformers%20or%0AMLPs%2C%20the%20networks%20are%20still%20limited%20to%20linearly%20modeling%20patterns%20as%20well%20as%0Athe%20deficient%20interpretability.%20To%20address%20these%20challenges%2C%20our%20intuition%20is%0Ainspired%20by%20the%20impressive%20results%20of%20the%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20in%0Aterms%20of%20accuracy%20and%20interpretability%2C%20which%20reshape%20the%20neural%20network%0Alearning%20via%20the%20stack%20of%20non-linear%20learnable%20activation%20functions%20derived%0Afrom%20the%20Kolmogorov-Anold%20representation%20theorem.%20Specifically%2C%20in%20this%20paper%2C%0Awe%20explore%20the%20untapped%20potential%20of%20KANs%20in%20improving%20backbones%20for%20vision%0Atasks.%20We%20investigate%2C%20modify%20and%20re-design%20the%20established%20U-Net%20pipeline%20by%0Aintegrating%20the%20dedicated%20KAN%20layers%20on%20the%20tokenized%20intermediate%0Arepresentation%2C%20termed%20U-KAN.%20Rigorous%20medical%20image%20segmentation%20benchmarks%0Averify%20the%20superiority%20of%20U-KAN%20by%20higher%20accuracy%20even%20with%20less%20computation%0Acost.%20We%20further%20delved%20into%20the%20potential%20of%20U-KAN%20as%20an%20alternative%20U-Net%0Anoise%20predictor%20in%20diffusion%20models%2C%20demonstrating%20its%20applicability%20in%0Agenerating%20task-oriented%20model%20architectures.%20These%20endeavours%20unveil%20valuable%0Ainsights%20and%20sheds%20light%20on%20the%20prospect%20that%20with%20U-KAN%2C%20you%20can%20make%20strong%0Abackbone%20for%20medical%20image%20segmentation%20and%20generation.%20Project%0Apage%3A%5Curl%7Bhttps%3A//yes-u-kan.github.io/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02918v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DU-KAN%2520Makes%2520Strong%2520Backbone%2520for%2520Medical%2520Image%2520Segmentation%2520and%250A%2520%2520Generation%26entry.906535625%3DChenxin%2520Li%2520and%2520Xinyu%2520Liu%2520and%2520Wuyang%2520Li%2520and%2520Cheng%2520Wang%2520and%2520Hengyu%2520Liu%2520and%2520Yifan%2520Liu%2520and%2520Zhen%2520Chen%2520and%2520Yixuan%2520Yuan%26entry.1292438233%3D%2520%2520U-Net%2520has%2520become%2520a%2520cornerstone%2520in%2520various%2520visual%2520applications%2520such%2520as%2520image%250Asegmentation%2520and%2520diffusion%2520probability%2520models.%2520While%2520numerous%2520innovative%250Adesigns%2520and%2520improvements%2520have%2520been%2520introduced%2520by%2520incorporating%2520transformers%2520or%250AMLPs%252C%2520the%2520networks%2520are%2520still%2520limited%2520to%2520linearly%2520modeling%2520patterns%2520as%2520well%2520as%250Athe%2520deficient%2520interpretability.%2520To%2520address%2520these%2520challenges%252C%2520our%2520intuition%2520is%250Ainspired%2520by%2520the%2520impressive%2520results%2520of%2520the%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520in%250Aterms%2520of%2520accuracy%2520and%2520interpretability%252C%2520which%2520reshape%2520the%2520neural%2520network%250Alearning%2520via%2520the%2520stack%2520of%2520non-linear%2520learnable%2520activation%2520functions%2520derived%250Afrom%2520the%2520Kolmogorov-Anold%2520representation%2520theorem.%2520Specifically%252C%2520in%2520this%2520paper%252C%250Awe%2520explore%2520the%2520untapped%2520potential%2520of%2520KANs%2520in%2520improving%2520backbones%2520for%2520vision%250Atasks.%2520We%2520investigate%252C%2520modify%2520and%2520re-design%2520the%2520established%2520U-Net%2520pipeline%2520by%250Aintegrating%2520the%2520dedicated%2520KAN%2520layers%2520on%2520the%2520tokenized%2520intermediate%250Arepresentation%252C%2520termed%2520U-KAN.%2520Rigorous%2520medical%2520image%2520segmentation%2520benchmarks%250Averify%2520the%2520superiority%2520of%2520U-KAN%2520by%2520higher%2520accuracy%2520even%2520with%2520less%2520computation%250Acost.%2520We%2520further%2520delved%2520into%2520the%2520potential%2520of%2520U-KAN%2520as%2520an%2520alternative%2520U-Net%250Anoise%2520predictor%2520in%2520diffusion%2520models%252C%2520demonstrating%2520its%2520applicability%2520in%250Agenerating%2520task-oriented%2520model%2520architectures.%2520These%2520endeavours%2520unveil%2520valuable%250Ainsights%2520and%2520sheds%2520light%2520on%2520the%2520prospect%2520that%2520with%2520U-KAN%252C%2520you%2520can%2520make%2520strong%250Abackbone%2520for%2520medical%2520image%2520segmentation%2520and%2520generation.%2520Project%250Apage%253A%255Curl%257Bhttps%253A//yes-u-kan.github.io/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02918v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=U-KAN%20Makes%20Strong%20Backbone%20for%20Medical%20Image%20Segmentation%20and%0A%20%20Generation&entry.906535625=Chenxin%20Li%20and%20Xinyu%20Liu%20and%20Wuyang%20Li%20and%20Cheng%20Wang%20and%20Hengyu%20Liu%20and%20Yifan%20Liu%20and%20Zhen%20Chen%20and%20Yixuan%20Yuan&entry.1292438233=%20%20U-Net%20has%20become%20a%20cornerstone%20in%20various%20visual%20applications%20such%20as%20image%0Asegmentation%20and%20diffusion%20probability%20models.%20While%20numerous%20innovative%0Adesigns%20and%20improvements%20have%20been%20introduced%20by%20incorporating%20transformers%20or%0AMLPs%2C%20the%20networks%20are%20still%20limited%20to%20linearly%20modeling%20patterns%20as%20well%20as%0Athe%20deficient%20interpretability.%20To%20address%20these%20challenges%2C%20our%20intuition%20is%0Ainspired%20by%20the%20impressive%20results%20of%20the%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20in%0Aterms%20of%20accuracy%20and%20interpretability%2C%20which%20reshape%20the%20neural%20network%0Alearning%20via%20the%20stack%20of%20non-linear%20learnable%20activation%20functions%20derived%0Afrom%20the%20Kolmogorov-Anold%20representation%20theorem.%20Specifically%2C%20in%20this%20paper%2C%0Awe%20explore%20the%20untapped%20potential%20of%20KANs%20in%20improving%20backbones%20for%20vision%0Atasks.%20We%20investigate%2C%20modify%20and%20re-design%20the%20established%20U-Net%20pipeline%20by%0Aintegrating%20the%20dedicated%20KAN%20layers%20on%20the%20tokenized%20intermediate%0Arepresentation%2C%20termed%20U-KAN.%20Rigorous%20medical%20image%20segmentation%20benchmarks%0Averify%20the%20superiority%20of%20U-KAN%20by%20higher%20accuracy%20even%20with%20less%20computation%0Acost.%20We%20further%20delved%20into%20the%20potential%20of%20U-KAN%20as%20an%20alternative%20U-Net%0Anoise%20predictor%20in%20diffusion%20models%2C%20demonstrating%20its%20applicability%20in%0Agenerating%20task-oriented%20model%20architectures.%20These%20endeavours%20unveil%20valuable%0Ainsights%20and%20sheds%20light%20on%20the%20prospect%20that%20with%20U-KAN%2C%20you%20can%20make%20strong%0Abackbone%20for%20medical%20image%20segmentation%20and%20generation.%20Project%0Apage%3A%5Curl%7Bhttps%3A//yes-u-kan.github.io/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02918v3&entry.124074799=Read"},
{"title": "Class-balanced Open-set Semi-supervised Object Detection for Medical\n  Images", "author": "Zhanyun Lu and Renshu Gu and Huimin Cheng and Siyu Pang and Mingyu Xu and Peifang Xu and Yaqi Wang and Yuichiro Kinoshita and Juan Ye and Gangyong Jia and Qing Wu", "abstract": "  Medical image datasets in the real world are often unlabeled and imbalanced,\nand Semi-Supervised Object Detection (SSOD) can utilize unlabeled data to\nimprove an object detector. However, existing approaches predominantly assumed\nthat the unlabeled data and test data do not contain out-of-distribution (OOD)\nclasses. The few open-set semi-supervised object detection methods have two\nweaknesses: first, the class imbalance is not considered; second, the OOD\ninstances are distinguished and simply discarded during pseudo-labeling. In\nthis paper, we consider the open-set semi-supervised object detection problem\nwhich leverages unlabeled data that contain OOD classes to improve object\ndetection for medical images. Our study incorporates two key innovations:\nCategory Control Embed (CCE) and out-of-distribution Detection Fusion\nClassifier (OODFC). CCE is designed to tackle dataset imbalance by constructing\na Foreground information Library, while OODFC tackles open-set challenges by\nintegrating the ``unknown'' information into basic pseudo-labels. Our method\noutperforms the state-of-the-art SSOD performance, achieving a 4.25 mAP\nimprovement on the public Parasite dataset.\n", "link": "http://arxiv.org/abs/2408.12355v1", "date": "2024-08-22", "relevancy": 2.0798, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5484}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5253}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class-balanced%20Open-set%20Semi-supervised%20Object%20Detection%20for%20Medical%0A%20%20Images&body=Title%3A%20Class-balanced%20Open-set%20Semi-supervised%20Object%20Detection%20for%20Medical%0A%20%20Images%0AAuthor%3A%20Zhanyun%20Lu%20and%20Renshu%20Gu%20and%20Huimin%20Cheng%20and%20Siyu%20Pang%20and%20Mingyu%20Xu%20and%20Peifang%20Xu%20and%20Yaqi%20Wang%20and%20Yuichiro%20Kinoshita%20and%20Juan%20Ye%20and%20Gangyong%20Jia%20and%20Qing%20Wu%0AAbstract%3A%20%20%20Medical%20image%20datasets%20in%20the%20real%20world%20are%20often%20unlabeled%20and%20imbalanced%2C%0Aand%20Semi-Supervised%20Object%20Detection%20%28SSOD%29%20can%20utilize%20unlabeled%20data%20to%0Aimprove%20an%20object%20detector.%20However%2C%20existing%20approaches%20predominantly%20assumed%0Athat%20the%20unlabeled%20data%20and%20test%20data%20do%20not%20contain%20out-of-distribution%20%28OOD%29%0Aclasses.%20The%20few%20open-set%20semi-supervised%20object%20detection%20methods%20have%20two%0Aweaknesses%3A%20first%2C%20the%20class%20imbalance%20is%20not%20considered%3B%20second%2C%20the%20OOD%0Ainstances%20are%20distinguished%20and%20simply%20discarded%20during%20pseudo-labeling.%20In%0Athis%20paper%2C%20we%20consider%20the%20open-set%20semi-supervised%20object%20detection%20problem%0Awhich%20leverages%20unlabeled%20data%20that%20contain%20OOD%20classes%20to%20improve%20object%0Adetection%20for%20medical%20images.%20Our%20study%20incorporates%20two%20key%20innovations%3A%0ACategory%20Control%20Embed%20%28CCE%29%20and%20out-of-distribution%20Detection%20Fusion%0AClassifier%20%28OODFC%29.%20CCE%20is%20designed%20to%20tackle%20dataset%20imbalance%20by%20constructing%0Aa%20Foreground%20information%20Library%2C%20while%20OODFC%20tackles%20open-set%20challenges%20by%0Aintegrating%20the%20%60%60unknown%27%27%20information%20into%20basic%20pseudo-labels.%20Our%20method%0Aoutperforms%20the%20state-of-the-art%20SSOD%20performance%2C%20achieving%20a%204.25%20mAP%0Aimprovement%20on%20the%20public%20Parasite%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12355v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass-balanced%2520Open-set%2520Semi-supervised%2520Object%2520Detection%2520for%2520Medical%250A%2520%2520Images%26entry.906535625%3DZhanyun%2520Lu%2520and%2520Renshu%2520Gu%2520and%2520Huimin%2520Cheng%2520and%2520Siyu%2520Pang%2520and%2520Mingyu%2520Xu%2520and%2520Peifang%2520Xu%2520and%2520Yaqi%2520Wang%2520and%2520Yuichiro%2520Kinoshita%2520and%2520Juan%2520Ye%2520and%2520Gangyong%2520Jia%2520and%2520Qing%2520Wu%26entry.1292438233%3D%2520%2520Medical%2520image%2520datasets%2520in%2520the%2520real%2520world%2520are%2520often%2520unlabeled%2520and%2520imbalanced%252C%250Aand%2520Semi-Supervised%2520Object%2520Detection%2520%2528SSOD%2529%2520can%2520utilize%2520unlabeled%2520data%2520to%250Aimprove%2520an%2520object%2520detector.%2520However%252C%2520existing%2520approaches%2520predominantly%2520assumed%250Athat%2520the%2520unlabeled%2520data%2520and%2520test%2520data%2520do%2520not%2520contain%2520out-of-distribution%2520%2528OOD%2529%250Aclasses.%2520The%2520few%2520open-set%2520semi-supervised%2520object%2520detection%2520methods%2520have%2520two%250Aweaknesses%253A%2520first%252C%2520the%2520class%2520imbalance%2520is%2520not%2520considered%253B%2520second%252C%2520the%2520OOD%250Ainstances%2520are%2520distinguished%2520and%2520simply%2520discarded%2520during%2520pseudo-labeling.%2520In%250Athis%2520paper%252C%2520we%2520consider%2520the%2520open-set%2520semi-supervised%2520object%2520detection%2520problem%250Awhich%2520leverages%2520unlabeled%2520data%2520that%2520contain%2520OOD%2520classes%2520to%2520improve%2520object%250Adetection%2520for%2520medical%2520images.%2520Our%2520study%2520incorporates%2520two%2520key%2520innovations%253A%250ACategory%2520Control%2520Embed%2520%2528CCE%2529%2520and%2520out-of-distribution%2520Detection%2520Fusion%250AClassifier%2520%2528OODFC%2529.%2520CCE%2520is%2520designed%2520to%2520tackle%2520dataset%2520imbalance%2520by%2520constructing%250Aa%2520Foreground%2520information%2520Library%252C%2520while%2520OODFC%2520tackles%2520open-set%2520challenges%2520by%250Aintegrating%2520the%2520%2560%2560unknown%2527%2527%2520information%2520into%2520basic%2520pseudo-labels.%2520Our%2520method%250Aoutperforms%2520the%2520state-of-the-art%2520SSOD%2520performance%252C%2520achieving%2520a%25204.25%2520mAP%250Aimprovement%2520on%2520the%2520public%2520Parasite%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12355v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class-balanced%20Open-set%20Semi-supervised%20Object%20Detection%20for%20Medical%0A%20%20Images&entry.906535625=Zhanyun%20Lu%20and%20Renshu%20Gu%20and%20Huimin%20Cheng%20and%20Siyu%20Pang%20and%20Mingyu%20Xu%20and%20Peifang%20Xu%20and%20Yaqi%20Wang%20and%20Yuichiro%20Kinoshita%20and%20Juan%20Ye%20and%20Gangyong%20Jia%20and%20Qing%20Wu&entry.1292438233=%20%20Medical%20image%20datasets%20in%20the%20real%20world%20are%20often%20unlabeled%20and%20imbalanced%2C%0Aand%20Semi-Supervised%20Object%20Detection%20%28SSOD%29%20can%20utilize%20unlabeled%20data%20to%0Aimprove%20an%20object%20detector.%20However%2C%20existing%20approaches%20predominantly%20assumed%0Athat%20the%20unlabeled%20data%20and%20test%20data%20do%20not%20contain%20out-of-distribution%20%28OOD%29%0Aclasses.%20The%20few%20open-set%20semi-supervised%20object%20detection%20methods%20have%20two%0Aweaknesses%3A%20first%2C%20the%20class%20imbalance%20is%20not%20considered%3B%20second%2C%20the%20OOD%0Ainstances%20are%20distinguished%20and%20simply%20discarded%20during%20pseudo-labeling.%20In%0Athis%20paper%2C%20we%20consider%20the%20open-set%20semi-supervised%20object%20detection%20problem%0Awhich%20leverages%20unlabeled%20data%20that%20contain%20OOD%20classes%20to%20improve%20object%0Adetection%20for%20medical%20images.%20Our%20study%20incorporates%20two%20key%20innovations%3A%0ACategory%20Control%20Embed%20%28CCE%29%20and%20out-of-distribution%20Detection%20Fusion%0AClassifier%20%28OODFC%29.%20CCE%20is%20designed%20to%20tackle%20dataset%20imbalance%20by%20constructing%0Aa%20Foreground%20information%20Library%2C%20while%20OODFC%20tackles%20open-set%20challenges%20by%0Aintegrating%20the%20%60%60unknown%27%27%20information%20into%20basic%20pseudo-labels.%20Our%20method%0Aoutperforms%20the%20state-of-the-art%20SSOD%20performance%2C%20achieving%20a%204.25%20mAP%0Aimprovement%20on%20the%20public%20Parasite%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12355v1&entry.124074799=Read"},
{"title": "Dual-path Frequency Discriminators for Few-shot Anomaly Detection", "author": "Yuhu Bai and Jiangning Zhang and Zhaofeng Chen and Yuhang Dong and Yunkang Cao and Guanzhong Tian", "abstract": "  Few-shot anomaly detection (FSAD) plays a crucial role in industrial\nmanufacturing. However, existing FSAD methods encounter difficulties leveraging\na limited number of normal samples, frequently failing to detect and locate\ninconspicuous anomalies in the spatial domain. We have further discovered that\nthese subtle anomalies would be more noticeable in the frequency domain. In\nthis paper, we propose a Dual-Path Frequency Discriminators (DFD) network from\na frequency perspective to tackle these issues. The original spatial images are\ntransformed into multi-frequency images, making them more conducive to the\ntailored discriminators in detecting anomalies. Additionally, the\ndiscriminators learn a joint representation with forms of pseudo-anomalies.\nExtensive experiments conducted on MVTec AD and VisA benchmarks demonstrate\nthat our DFD surpasses current state-of-the-art methods. The code is available\nat \\url{https://github.com/yuhbai/DFD}.\n", "link": "http://arxiv.org/abs/2403.04151v4", "date": "2024-08-22", "relevancy": 2.0683, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5281}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5177}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-path%20Frequency%20Discriminators%20for%20Few-shot%20Anomaly%20Detection&body=Title%3A%20Dual-path%20Frequency%20Discriminators%20for%20Few-shot%20Anomaly%20Detection%0AAuthor%3A%20Yuhu%20Bai%20and%20Jiangning%20Zhang%20and%20Zhaofeng%20Chen%20and%20Yuhang%20Dong%20and%20Yunkang%20Cao%20and%20Guanzhong%20Tian%0AAbstract%3A%20%20%20Few-shot%20anomaly%20detection%20%28FSAD%29%20plays%20a%20crucial%20role%20in%20industrial%0Amanufacturing.%20However%2C%20existing%20FSAD%20methods%20encounter%20difficulties%20leveraging%0Aa%20limited%20number%20of%20normal%20samples%2C%20frequently%20failing%20to%20detect%20and%20locate%0Ainconspicuous%20anomalies%20in%20the%20spatial%20domain.%20We%20have%20further%20discovered%20that%0Athese%20subtle%20anomalies%20would%20be%20more%20noticeable%20in%20the%20frequency%20domain.%20In%0Athis%20paper%2C%20we%20propose%20a%20Dual-Path%20Frequency%20Discriminators%20%28DFD%29%20network%20from%0Aa%20frequency%20perspective%20to%20tackle%20these%20issues.%20The%20original%20spatial%20images%20are%0Atransformed%20into%20multi-frequency%20images%2C%20making%20them%20more%20conducive%20to%20the%0Atailored%20discriminators%20in%20detecting%20anomalies.%20Additionally%2C%20the%0Adiscriminators%20learn%20a%20joint%20representation%20with%20forms%20of%20pseudo-anomalies.%0AExtensive%20experiments%20conducted%20on%20MVTec%20AD%20and%20VisA%20benchmarks%20demonstrate%0Athat%20our%20DFD%20surpasses%20current%20state-of-the-art%20methods.%20The%20code%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/yuhbai/DFD%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04151v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-path%2520Frequency%2520Discriminators%2520for%2520Few-shot%2520Anomaly%2520Detection%26entry.906535625%3DYuhu%2520Bai%2520and%2520Jiangning%2520Zhang%2520and%2520Zhaofeng%2520Chen%2520and%2520Yuhang%2520Dong%2520and%2520Yunkang%2520Cao%2520and%2520Guanzhong%2520Tian%26entry.1292438233%3D%2520%2520Few-shot%2520anomaly%2520detection%2520%2528FSAD%2529%2520plays%2520a%2520crucial%2520role%2520in%2520industrial%250Amanufacturing.%2520However%252C%2520existing%2520FSAD%2520methods%2520encounter%2520difficulties%2520leveraging%250Aa%2520limited%2520number%2520of%2520normal%2520samples%252C%2520frequently%2520failing%2520to%2520detect%2520and%2520locate%250Ainconspicuous%2520anomalies%2520in%2520the%2520spatial%2520domain.%2520We%2520have%2520further%2520discovered%2520that%250Athese%2520subtle%2520anomalies%2520would%2520be%2520more%2520noticeable%2520in%2520the%2520frequency%2520domain.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520Dual-Path%2520Frequency%2520Discriminators%2520%2528DFD%2529%2520network%2520from%250Aa%2520frequency%2520perspective%2520to%2520tackle%2520these%2520issues.%2520The%2520original%2520spatial%2520images%2520are%250Atransformed%2520into%2520multi-frequency%2520images%252C%2520making%2520them%2520more%2520conducive%2520to%2520the%250Atailored%2520discriminators%2520in%2520detecting%2520anomalies.%2520Additionally%252C%2520the%250Adiscriminators%2520learn%2520a%2520joint%2520representation%2520with%2520forms%2520of%2520pseudo-anomalies.%250AExtensive%2520experiments%2520conducted%2520on%2520MVTec%2520AD%2520and%2520VisA%2520benchmarks%2520demonstrate%250Athat%2520our%2520DFD%2520surpasses%2520current%2520state-of-the-art%2520methods.%2520The%2520code%2520is%2520available%250Aat%2520%255Curl%257Bhttps%253A//github.com/yuhbai/DFD%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04151v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-path%20Frequency%20Discriminators%20for%20Few-shot%20Anomaly%20Detection&entry.906535625=Yuhu%20Bai%20and%20Jiangning%20Zhang%20and%20Zhaofeng%20Chen%20and%20Yuhang%20Dong%20and%20Yunkang%20Cao%20and%20Guanzhong%20Tian&entry.1292438233=%20%20Few-shot%20anomaly%20detection%20%28FSAD%29%20plays%20a%20crucial%20role%20in%20industrial%0Amanufacturing.%20However%2C%20existing%20FSAD%20methods%20encounter%20difficulties%20leveraging%0Aa%20limited%20number%20of%20normal%20samples%2C%20frequently%20failing%20to%20detect%20and%20locate%0Ainconspicuous%20anomalies%20in%20the%20spatial%20domain.%20We%20have%20further%20discovered%20that%0Athese%20subtle%20anomalies%20would%20be%20more%20noticeable%20in%20the%20frequency%20domain.%20In%0Athis%20paper%2C%20we%20propose%20a%20Dual-Path%20Frequency%20Discriminators%20%28DFD%29%20network%20from%0Aa%20frequency%20perspective%20to%20tackle%20these%20issues.%20The%20original%20spatial%20images%20are%0Atransformed%20into%20multi-frequency%20images%2C%20making%20them%20more%20conducive%20to%20the%0Atailored%20discriminators%20in%20detecting%20anomalies.%20Additionally%2C%20the%0Adiscriminators%20learn%20a%20joint%20representation%20with%20forms%20of%20pseudo-anomalies.%0AExtensive%20experiments%20conducted%20on%20MVTec%20AD%20and%20VisA%20benchmarks%20demonstrate%0Athat%20our%20DFD%20surpasses%20current%20state-of-the-art%20methods.%20The%20code%20is%20available%0Aat%20%5Curl%7Bhttps%3A//github.com/yuhbai/DFD%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04151v4&entry.124074799=Read"},
{"title": "LightFF: Lightweight Inference for Forward-Forward Algorithm", "author": "Amin Aminifar and Baichuan Huang and Azra Abtahi and Amir Aminifar", "abstract": "  The human brain performs tasks with an outstanding energy efficiency, i.e.,\nwith approximately 20 Watts. The state-of-the-art Artificial/Deep Neural\nNetworks (ANN/DNN), on the other hand, have recently been shown to consume\nmassive amounts of energy. The training of these ANNs/DNNs is done almost\nexclusively based on the back-propagation algorithm, which is known to be\nbiologically implausible. This has led to a new generation of forward-only\ntechniques, including the Forward-Forward algorithm. In this paper, we propose\na lightweight inference scheme specifically designed for DNNs trained using the\nForward-Forward algorithm. We have evaluated our proposed lightweight inference\nscheme in the case of the MNIST and CIFAR datasets, as well as two real-world\napplications, namely, epileptic seizure detection and cardiac arrhythmia\nclassification using wearable technologies, where complexity overheads/energy\nconsumption is a major constraint, and demonstrate its relevance. Our code is\navailable at https://github.com/AminAminifar/LightFF.\n", "link": "http://arxiv.org/abs/2404.05241v5", "date": "2024-08-22", "relevancy": 2.051, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5195}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5081}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightFF%3A%20Lightweight%20Inference%20for%20Forward-Forward%20Algorithm&body=Title%3A%20LightFF%3A%20Lightweight%20Inference%20for%20Forward-Forward%20Algorithm%0AAuthor%3A%20Amin%20Aminifar%20and%20Baichuan%20Huang%20and%20Azra%20Abtahi%20and%20Amir%20Aminifar%0AAbstract%3A%20%20%20The%20human%20brain%20performs%20tasks%20with%20an%20outstanding%20energy%20efficiency%2C%20i.e.%2C%0Awith%20approximately%2020%20Watts.%20The%20state-of-the-art%20Artificial/Deep%20Neural%0ANetworks%20%28ANN/DNN%29%2C%20on%20the%20other%20hand%2C%20have%20recently%20been%20shown%20to%20consume%0Amassive%20amounts%20of%20energy.%20The%20training%20of%20these%20ANNs/DNNs%20is%20done%20almost%0Aexclusively%20based%20on%20the%20back-propagation%20algorithm%2C%20which%20is%20known%20to%20be%0Abiologically%20implausible.%20This%20has%20led%20to%20a%20new%20generation%20of%20forward-only%0Atechniques%2C%20including%20the%20Forward-Forward%20algorithm.%20In%20this%20paper%2C%20we%20propose%0Aa%20lightweight%20inference%20scheme%20specifically%20designed%20for%20DNNs%20trained%20using%20the%0AForward-Forward%20algorithm.%20We%20have%20evaluated%20our%20proposed%20lightweight%20inference%0Ascheme%20in%20the%20case%20of%20the%20MNIST%20and%20CIFAR%20datasets%2C%20as%20well%20as%20two%20real-world%0Aapplications%2C%20namely%2C%20epileptic%20seizure%20detection%20and%20cardiac%20arrhythmia%0Aclassification%20using%20wearable%20technologies%2C%20where%20complexity%20overheads/energy%0Aconsumption%20is%20a%20major%20constraint%2C%20and%20demonstrate%20its%20relevance.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/AminAminifar/LightFF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05241v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightFF%253A%2520Lightweight%2520Inference%2520for%2520Forward-Forward%2520Algorithm%26entry.906535625%3DAmin%2520Aminifar%2520and%2520Baichuan%2520Huang%2520and%2520Azra%2520Abtahi%2520and%2520Amir%2520Aminifar%26entry.1292438233%3D%2520%2520The%2520human%2520brain%2520performs%2520tasks%2520with%2520an%2520outstanding%2520energy%2520efficiency%252C%2520i.e.%252C%250Awith%2520approximately%252020%2520Watts.%2520The%2520state-of-the-art%2520Artificial/Deep%2520Neural%250ANetworks%2520%2528ANN/DNN%2529%252C%2520on%2520the%2520other%2520hand%252C%2520have%2520recently%2520been%2520shown%2520to%2520consume%250Amassive%2520amounts%2520of%2520energy.%2520The%2520training%2520of%2520these%2520ANNs/DNNs%2520is%2520done%2520almost%250Aexclusively%2520based%2520on%2520the%2520back-propagation%2520algorithm%252C%2520which%2520is%2520known%2520to%2520be%250Abiologically%2520implausible.%2520This%2520has%2520led%2520to%2520a%2520new%2520generation%2520of%2520forward-only%250Atechniques%252C%2520including%2520the%2520Forward-Forward%2520algorithm.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520lightweight%2520inference%2520scheme%2520specifically%2520designed%2520for%2520DNNs%2520trained%2520using%2520the%250AForward-Forward%2520algorithm.%2520We%2520have%2520evaluated%2520our%2520proposed%2520lightweight%2520inference%250Ascheme%2520in%2520the%2520case%2520of%2520the%2520MNIST%2520and%2520CIFAR%2520datasets%252C%2520as%2520well%2520as%2520two%2520real-world%250Aapplications%252C%2520namely%252C%2520epileptic%2520seizure%2520detection%2520and%2520cardiac%2520arrhythmia%250Aclassification%2520using%2520wearable%2520technologies%252C%2520where%2520complexity%2520overheads/energy%250Aconsumption%2520is%2520a%2520major%2520constraint%252C%2520and%2520demonstrate%2520its%2520relevance.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/AminAminifar/LightFF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05241v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightFF%3A%20Lightweight%20Inference%20for%20Forward-Forward%20Algorithm&entry.906535625=Amin%20Aminifar%20and%20Baichuan%20Huang%20and%20Azra%20Abtahi%20and%20Amir%20Aminifar&entry.1292438233=%20%20The%20human%20brain%20performs%20tasks%20with%20an%20outstanding%20energy%20efficiency%2C%20i.e.%2C%0Awith%20approximately%2020%20Watts.%20The%20state-of-the-art%20Artificial/Deep%20Neural%0ANetworks%20%28ANN/DNN%29%2C%20on%20the%20other%20hand%2C%20have%20recently%20been%20shown%20to%20consume%0Amassive%20amounts%20of%20energy.%20The%20training%20of%20these%20ANNs/DNNs%20is%20done%20almost%0Aexclusively%20based%20on%20the%20back-propagation%20algorithm%2C%20which%20is%20known%20to%20be%0Abiologically%20implausible.%20This%20has%20led%20to%20a%20new%20generation%20of%20forward-only%0Atechniques%2C%20including%20the%20Forward-Forward%20algorithm.%20In%20this%20paper%2C%20we%20propose%0Aa%20lightweight%20inference%20scheme%20specifically%20designed%20for%20DNNs%20trained%20using%20the%0AForward-Forward%20algorithm.%20We%20have%20evaluated%20our%20proposed%20lightweight%20inference%0Ascheme%20in%20the%20case%20of%20the%20MNIST%20and%20CIFAR%20datasets%2C%20as%20well%20as%20two%20real-world%0Aapplications%2C%20namely%2C%20epileptic%20seizure%20detection%20and%20cardiac%20arrhythmia%0Aclassification%20using%20wearable%20technologies%2C%20where%20complexity%20overheads/energy%0Aconsumption%20is%20a%20major%20constraint%2C%20and%20demonstrate%20its%20relevance.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/AminAminifar/LightFF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05241v5&entry.124074799=Read"},
{"title": "Smartphone-based Eye Tracking System using Edge Intelligence and Model\n  Optimisation", "author": "Nishan Gunawardena and Gough Yumu Lui and Jeewani Anupama Ginige and Bahman Javadi", "abstract": "  A significant limitation of current smartphone-based eye-tracking algorithms\nis their low accuracy when applied to video-type visual stimuli, as they are\ntypically trained on static images. Also, the increasing demand for real-time\ninteractive applications like games, VR, and AR on smartphones requires\novercoming the limitations posed by resource constraints such as limited\ncomputational power, battery life, and network bandwidth. Therefore, we\ndeveloped two new smartphone eye-tracking techniques for video-type visuals by\ncombining Convolutional Neural Networks (CNN) with two different Recurrent\nNeural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent\nUnit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean\nSquare Error of 0.955cm and 1.091cm, respectively. To address the computational\nconstraints of smartphones, we developed an edge intelligence architecture to\nenhance the performance of smartphone-based eye tracking. We applied various\noptimisation methods like quantisation and pruning to deep learning models for\nbetter energy, CPU, and memory usage on edge devices, focusing on real-time\nprocessing. Using model quantisation, the model inference time in the CNN+LSTM\nand CNN+GRU models was reduced by 21.72% and 19.50%, respectively, on edge\ndevices.\n", "link": "http://arxiv.org/abs/2408.12463v1", "date": "2024-08-22", "relevancy": 2.0447, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5143}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5109}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smartphone-based%20Eye%20Tracking%20System%20using%20Edge%20Intelligence%20and%20Model%0A%20%20Optimisation&body=Title%3A%20Smartphone-based%20Eye%20Tracking%20System%20using%20Edge%20Intelligence%20and%20Model%0A%20%20Optimisation%0AAuthor%3A%20Nishan%20Gunawardena%20and%20Gough%20Yumu%20Lui%20and%20Jeewani%20Anupama%20Ginige%20and%20Bahman%20Javadi%0AAbstract%3A%20%20%20A%20significant%20limitation%20of%20current%20smartphone-based%20eye-tracking%20algorithms%0Ais%20their%20low%20accuracy%20when%20applied%20to%20video-type%20visual%20stimuli%2C%20as%20they%20are%0Atypically%20trained%20on%20static%20images.%20Also%2C%20the%20increasing%20demand%20for%20real-time%0Ainteractive%20applications%20like%20games%2C%20VR%2C%20and%20AR%20on%20smartphones%20requires%0Aovercoming%20the%20limitations%20posed%20by%20resource%20constraints%20such%20as%20limited%0Acomputational%20power%2C%20battery%20life%2C%20and%20network%20bandwidth.%20Therefore%2C%20we%0Adeveloped%20two%20new%20smartphone%20eye-tracking%20techniques%20for%20video-type%20visuals%20by%0Acombining%20Convolutional%20Neural%20Networks%20%28CNN%29%20with%20two%20different%20Recurrent%0ANeural%20Networks%20%28RNN%29%2C%20namely%20Long%20Short%20Term%20Memory%20%28LSTM%29%20and%20Gated%20Recurrent%0AUnit%20%28GRU%29.%20Our%20CNN%2BLSTM%20and%20CNN%2BGRU%20models%20achieved%20an%20average%20Root%20Mean%0ASquare%20Error%20of%200.955cm%20and%201.091cm%2C%20respectively.%20To%20address%20the%20computational%0Aconstraints%20of%20smartphones%2C%20we%20developed%20an%20edge%20intelligence%20architecture%20to%0Aenhance%20the%20performance%20of%20smartphone-based%20eye%20tracking.%20We%20applied%20various%0Aoptimisation%20methods%20like%20quantisation%20and%20pruning%20to%20deep%20learning%20models%20for%0Abetter%20energy%2C%20CPU%2C%20and%20memory%20usage%20on%20edge%20devices%2C%20focusing%20on%20real-time%0Aprocessing.%20Using%20model%20quantisation%2C%20the%20model%20inference%20time%20in%20the%20CNN%2BLSTM%0Aand%20CNN%2BGRU%20models%20was%20reduced%20by%2021.72%25%20and%2019.50%25%2C%20respectively%2C%20on%20edge%0Adevices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmartphone-based%2520Eye%2520Tracking%2520System%2520using%2520Edge%2520Intelligence%2520and%2520Model%250A%2520%2520Optimisation%26entry.906535625%3DNishan%2520Gunawardena%2520and%2520Gough%2520Yumu%2520Lui%2520and%2520Jeewani%2520Anupama%2520Ginige%2520and%2520Bahman%2520Javadi%26entry.1292438233%3D%2520%2520A%2520significant%2520limitation%2520of%2520current%2520smartphone-based%2520eye-tracking%2520algorithms%250Ais%2520their%2520low%2520accuracy%2520when%2520applied%2520to%2520video-type%2520visual%2520stimuli%252C%2520as%2520they%2520are%250Atypically%2520trained%2520on%2520static%2520images.%2520Also%252C%2520the%2520increasing%2520demand%2520for%2520real-time%250Ainteractive%2520applications%2520like%2520games%252C%2520VR%252C%2520and%2520AR%2520on%2520smartphones%2520requires%250Aovercoming%2520the%2520limitations%2520posed%2520by%2520resource%2520constraints%2520such%2520as%2520limited%250Acomputational%2520power%252C%2520battery%2520life%252C%2520and%2520network%2520bandwidth.%2520Therefore%252C%2520we%250Adeveloped%2520two%2520new%2520smartphone%2520eye-tracking%2520techniques%2520for%2520video-type%2520visuals%2520by%250Acombining%2520Convolutional%2520Neural%2520Networks%2520%2528CNN%2529%2520with%2520two%2520different%2520Recurrent%250ANeural%2520Networks%2520%2528RNN%2529%252C%2520namely%2520Long%2520Short%2520Term%2520Memory%2520%2528LSTM%2529%2520and%2520Gated%2520Recurrent%250AUnit%2520%2528GRU%2529.%2520Our%2520CNN%252BLSTM%2520and%2520CNN%252BGRU%2520models%2520achieved%2520an%2520average%2520Root%2520Mean%250ASquare%2520Error%2520of%25200.955cm%2520and%25201.091cm%252C%2520respectively.%2520To%2520address%2520the%2520computational%250Aconstraints%2520of%2520smartphones%252C%2520we%2520developed%2520an%2520edge%2520intelligence%2520architecture%2520to%250Aenhance%2520the%2520performance%2520of%2520smartphone-based%2520eye%2520tracking.%2520We%2520applied%2520various%250Aoptimisation%2520methods%2520like%2520quantisation%2520and%2520pruning%2520to%2520deep%2520learning%2520models%2520for%250Abetter%2520energy%252C%2520CPU%252C%2520and%2520memory%2520usage%2520on%2520edge%2520devices%252C%2520focusing%2520on%2520real-time%250Aprocessing.%2520Using%2520model%2520quantisation%252C%2520the%2520model%2520inference%2520time%2520in%2520the%2520CNN%252BLSTM%250Aand%2520CNN%252BGRU%2520models%2520was%2520reduced%2520by%252021.72%2525%2520and%252019.50%2525%252C%2520respectively%252C%2520on%2520edge%250Adevices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smartphone-based%20Eye%20Tracking%20System%20using%20Edge%20Intelligence%20and%20Model%0A%20%20Optimisation&entry.906535625=Nishan%20Gunawardena%20and%20Gough%20Yumu%20Lui%20and%20Jeewani%20Anupama%20Ginige%20and%20Bahman%20Javadi&entry.1292438233=%20%20A%20significant%20limitation%20of%20current%20smartphone-based%20eye-tracking%20algorithms%0Ais%20their%20low%20accuracy%20when%20applied%20to%20video-type%20visual%20stimuli%2C%20as%20they%20are%0Atypically%20trained%20on%20static%20images.%20Also%2C%20the%20increasing%20demand%20for%20real-time%0Ainteractive%20applications%20like%20games%2C%20VR%2C%20and%20AR%20on%20smartphones%20requires%0Aovercoming%20the%20limitations%20posed%20by%20resource%20constraints%20such%20as%20limited%0Acomputational%20power%2C%20battery%20life%2C%20and%20network%20bandwidth.%20Therefore%2C%20we%0Adeveloped%20two%20new%20smartphone%20eye-tracking%20techniques%20for%20video-type%20visuals%20by%0Acombining%20Convolutional%20Neural%20Networks%20%28CNN%29%20with%20two%20different%20Recurrent%0ANeural%20Networks%20%28RNN%29%2C%20namely%20Long%20Short%20Term%20Memory%20%28LSTM%29%20and%20Gated%20Recurrent%0AUnit%20%28GRU%29.%20Our%20CNN%2BLSTM%20and%20CNN%2BGRU%20models%20achieved%20an%20average%20Root%20Mean%0ASquare%20Error%20of%200.955cm%20and%201.091cm%2C%20respectively.%20To%20address%20the%20computational%0Aconstraints%20of%20smartphones%2C%20we%20developed%20an%20edge%20intelligence%20architecture%20to%0Aenhance%20the%20performance%20of%20smartphone-based%20eye%20tracking.%20We%20applied%20various%0Aoptimisation%20methods%20like%20quantisation%20and%20pruning%20to%20deep%20learning%20models%20for%0Abetter%20energy%2C%20CPU%2C%20and%20memory%20usage%20on%20edge%20devices%2C%20focusing%20on%20real-time%0Aprocessing.%20Using%20model%20quantisation%2C%20the%20model%20inference%20time%20in%20the%20CNN%2BLSTM%0Aand%20CNN%2BGRU%20models%20was%20reduced%20by%2021.72%25%20and%2019.50%25%2C%20respectively%2C%20on%20edge%0Adevices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12463v1&entry.124074799=Read"},
{"title": "Contextual Tuning of Model Predictive Control for Autonomous Racing", "author": "Lukas P. Fr\u00f6hlich and Christian K\u00fcttel and Elena Arcari and Lukas Hewing and Melanie N. Zeilinger and Andrea Carron", "abstract": "  Learning-based model predictive control has been widely applied in autonomous\nracing to improve the closed-loop behaviour of vehicles in a data-driven\nmanner. When environmental conditions change, e.g., due to rain, often only the\npredictive model is adapted, but the controller parameters are kept constant.\nHowever, this can lead to suboptimal behaviour. In this paper, we address the\nproblem of data-efficient controller tuning, adapting both the model and\nobjective simultaneously. The key novelty of the proposed approach is that we\nleverage a learned dynamics model to encode the environmental condition as a\nso-called context. This insight allows us to employ contextual Bayesian\noptimization to efficiently transfer knowledge across different environmental\nconditions. Consequently, we require fewer data to find the optimal controller\nconfiguration for each context. The proposed framework is extensively evaluated\nwith more than 3'000 laps driven on an experimental platform with 1:28 scale RC\nrace cars. The results show that our approach successfully optimizes the lap\ntime across different contexts requiring fewer data compared to other\napproaches based on standard Bayesian optimization.\n", "link": "http://arxiv.org/abs/2110.02710v2", "date": "2024-08-22", "relevancy": 2.043, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5459}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5229}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Tuning%20of%20Model%20Predictive%20Control%20for%20Autonomous%20Racing&body=Title%3A%20Contextual%20Tuning%20of%20Model%20Predictive%20Control%20for%20Autonomous%20Racing%0AAuthor%3A%20Lukas%20P.%20Fr%C3%B6hlich%20and%20Christian%20K%C3%BCttel%20and%20Elena%20Arcari%20and%20Lukas%20Hewing%20and%20Melanie%20N.%20Zeilinger%20and%20Andrea%20Carron%0AAbstract%3A%20%20%20Learning-based%20model%20predictive%20control%20has%20been%20widely%20applied%20in%20autonomous%0Aracing%20to%20improve%20the%20closed-loop%20behaviour%20of%20vehicles%20in%20a%20data-driven%0Amanner.%20When%20environmental%20conditions%20change%2C%20e.g.%2C%20due%20to%20rain%2C%20often%20only%20the%0Apredictive%20model%20is%20adapted%2C%20but%20the%20controller%20parameters%20are%20kept%20constant.%0AHowever%2C%20this%20can%20lead%20to%20suboptimal%20behaviour.%20In%20this%20paper%2C%20we%20address%20the%0Aproblem%20of%20data-efficient%20controller%20tuning%2C%20adapting%20both%20the%20model%20and%0Aobjective%20simultaneously.%20The%20key%20novelty%20of%20the%20proposed%20approach%20is%20that%20we%0Aleverage%20a%20learned%20dynamics%20model%20to%20encode%20the%20environmental%20condition%20as%20a%0Aso-called%20context.%20This%20insight%20allows%20us%20to%20employ%20contextual%20Bayesian%0Aoptimization%20to%20efficiently%20transfer%20knowledge%20across%20different%20environmental%0Aconditions.%20Consequently%2C%20we%20require%20fewer%20data%20to%20find%20the%20optimal%20controller%0Aconfiguration%20for%20each%20context.%20The%20proposed%20framework%20is%20extensively%20evaluated%0Awith%20more%20than%203%27000%20laps%20driven%20on%20an%20experimental%20platform%20with%201%3A28%20scale%20RC%0Arace%20cars.%20The%20results%20show%20that%20our%20approach%20successfully%20optimizes%20the%20lap%0Atime%20across%20different%20contexts%20requiring%20fewer%20data%20compared%20to%20other%0Aapproaches%20based%20on%20standard%20Bayesian%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2110.02710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Tuning%2520of%2520Model%2520Predictive%2520Control%2520for%2520Autonomous%2520Racing%26entry.906535625%3DLukas%2520P.%2520Fr%25C3%25B6hlich%2520and%2520Christian%2520K%25C3%25BCttel%2520and%2520Elena%2520Arcari%2520and%2520Lukas%2520Hewing%2520and%2520Melanie%2520N.%2520Zeilinger%2520and%2520Andrea%2520Carron%26entry.1292438233%3D%2520%2520Learning-based%2520model%2520predictive%2520control%2520has%2520been%2520widely%2520applied%2520in%2520autonomous%250Aracing%2520to%2520improve%2520the%2520closed-loop%2520behaviour%2520of%2520vehicles%2520in%2520a%2520data-driven%250Amanner.%2520When%2520environmental%2520conditions%2520change%252C%2520e.g.%252C%2520due%2520to%2520rain%252C%2520often%2520only%2520the%250Apredictive%2520model%2520is%2520adapted%252C%2520but%2520the%2520controller%2520parameters%2520are%2520kept%2520constant.%250AHowever%252C%2520this%2520can%2520lead%2520to%2520suboptimal%2520behaviour.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%250Aproblem%2520of%2520data-efficient%2520controller%2520tuning%252C%2520adapting%2520both%2520the%2520model%2520and%250Aobjective%2520simultaneously.%2520The%2520key%2520novelty%2520of%2520the%2520proposed%2520approach%2520is%2520that%2520we%250Aleverage%2520a%2520learned%2520dynamics%2520model%2520to%2520encode%2520the%2520environmental%2520condition%2520as%2520a%250Aso-called%2520context.%2520This%2520insight%2520allows%2520us%2520to%2520employ%2520contextual%2520Bayesian%250Aoptimization%2520to%2520efficiently%2520transfer%2520knowledge%2520across%2520different%2520environmental%250Aconditions.%2520Consequently%252C%2520we%2520require%2520fewer%2520data%2520to%2520find%2520the%2520optimal%2520controller%250Aconfiguration%2520for%2520each%2520context.%2520The%2520proposed%2520framework%2520is%2520extensively%2520evaluated%250Awith%2520more%2520than%25203%2527000%2520laps%2520driven%2520on%2520an%2520experimental%2520platform%2520with%25201%253A28%2520scale%2520RC%250Arace%2520cars.%2520The%2520results%2520show%2520that%2520our%2520approach%2520successfully%2520optimizes%2520the%2520lap%250Atime%2520across%2520different%2520contexts%2520requiring%2520fewer%2520data%2520compared%2520to%2520other%250Aapproaches%2520based%2520on%2520standard%2520Bayesian%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2110.02710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Tuning%20of%20Model%20Predictive%20Control%20for%20Autonomous%20Racing&entry.906535625=Lukas%20P.%20Fr%C3%B6hlich%20and%20Christian%20K%C3%BCttel%20and%20Elena%20Arcari%20and%20Lukas%20Hewing%20and%20Melanie%20N.%20Zeilinger%20and%20Andrea%20Carron&entry.1292438233=%20%20Learning-based%20model%20predictive%20control%20has%20been%20widely%20applied%20in%20autonomous%0Aracing%20to%20improve%20the%20closed-loop%20behaviour%20of%20vehicles%20in%20a%20data-driven%0Amanner.%20When%20environmental%20conditions%20change%2C%20e.g.%2C%20due%20to%20rain%2C%20often%20only%20the%0Apredictive%20model%20is%20adapted%2C%20but%20the%20controller%20parameters%20are%20kept%20constant.%0AHowever%2C%20this%20can%20lead%20to%20suboptimal%20behaviour.%20In%20this%20paper%2C%20we%20address%20the%0Aproblem%20of%20data-efficient%20controller%20tuning%2C%20adapting%20both%20the%20model%20and%0Aobjective%20simultaneously.%20The%20key%20novelty%20of%20the%20proposed%20approach%20is%20that%20we%0Aleverage%20a%20learned%20dynamics%20model%20to%20encode%20the%20environmental%20condition%20as%20a%0Aso-called%20context.%20This%20insight%20allows%20us%20to%20employ%20contextual%20Bayesian%0Aoptimization%20to%20efficiently%20transfer%20knowledge%20across%20different%20environmental%0Aconditions.%20Consequently%2C%20we%20require%20fewer%20data%20to%20find%20the%20optimal%20controller%0Aconfiguration%20for%20each%20context.%20The%20proposed%20framework%20is%20extensively%20evaluated%0Awith%20more%20than%203%27000%20laps%20driven%20on%20an%20experimental%20platform%20with%201%3A28%20scale%20RC%0Arace%20cars.%20The%20results%20show%20that%20our%20approach%20successfully%20optimizes%20the%20lap%0Atime%20across%20different%20contexts%20requiring%20fewer%20data%20compared%20to%20other%0Aapproaches%20based%20on%20standard%20Bayesian%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2110.02710v2&entry.124074799=Read"},
{"title": "A Percolation Model of Emergence: Analyzing Transformers Trained on a\n  Formal Language", "author": "Ekdeep Singh Lubana and Kyogo Kawaguchi and Robert P. Dick and Hidenori Tanaka", "abstract": "  Increase in data, size, or compute can lead to sudden learning of specific\ncapabilities by a neural network -- a phenomenon often called \"emergence\".\nBeyond scientific understanding, establishing the causal factors underlying\nsuch emergent capabilities is crucial to enable risk regulation frameworks for\nAI. In this work, we seek inspiration from study of emergent properties in\nother fields and propose a phenomenological definition for the concept in the\ncontext of neural networks. Our definition implicates the acquisition of\nspecific structures underlying the data-generating process as a cause of sudden\nperformance growth for specific, narrower tasks. We empirically investigate\nthis definition by proposing an experimental system grounded in a\ncontext-sensitive formal language and find that Transformers trained to perform\ntasks on top of strings from this language indeed exhibit emergent\ncapabilities. Specifically, we show that once the language's underlying grammar\nand context-sensitivity inducing structures are learned by the model,\nperformance on narrower tasks suddenly begins to improve. We then analogize our\nnetwork's learning dynamics with the process of percolation on a bipartite\ngraph, establishing a formal phase transition model that predicts the shift in\nthe point of emergence observed in experiment when changing the data structure.\nOverall, our experimental and theoretical frameworks yield a step towards\nbetter defining, characterizing, and predicting emergence in neural networks.\n", "link": "http://arxiv.org/abs/2408.12578v1", "date": "2024-08-22", "relevancy": 2.0422, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5876}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5107}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Percolation%20Model%20of%20Emergence%3A%20Analyzing%20Transformers%20Trained%20on%20a%0A%20%20Formal%20Language&body=Title%3A%20A%20Percolation%20Model%20of%20Emergence%3A%20Analyzing%20Transformers%20Trained%20on%20a%0A%20%20Formal%20Language%0AAuthor%3A%20Ekdeep%20Singh%20Lubana%20and%20Kyogo%20Kawaguchi%20and%20Robert%20P.%20Dick%20and%20Hidenori%20Tanaka%0AAbstract%3A%20%20%20Increase%20in%20data%2C%20size%2C%20or%20compute%20can%20lead%20to%20sudden%20learning%20of%20specific%0Acapabilities%20by%20a%20neural%20network%20--%20a%20phenomenon%20often%20called%20%22emergence%22.%0ABeyond%20scientific%20understanding%2C%20establishing%20the%20causal%20factors%20underlying%0Asuch%20emergent%20capabilities%20is%20crucial%20to%20enable%20risk%20regulation%20frameworks%20for%0AAI.%20In%20this%20work%2C%20we%20seek%20inspiration%20from%20study%20of%20emergent%20properties%20in%0Aother%20fields%20and%20propose%20a%20phenomenological%20definition%20for%20the%20concept%20in%20the%0Acontext%20of%20neural%20networks.%20Our%20definition%20implicates%20the%20acquisition%20of%0Aspecific%20structures%20underlying%20the%20data-generating%20process%20as%20a%20cause%20of%20sudden%0Aperformance%20growth%20for%20specific%2C%20narrower%20tasks.%20We%20empirically%20investigate%0Athis%20definition%20by%20proposing%20an%20experimental%20system%20grounded%20in%20a%0Acontext-sensitive%20formal%20language%20and%20find%20that%20Transformers%20trained%20to%20perform%0Atasks%20on%20top%20of%20strings%20from%20this%20language%20indeed%20exhibit%20emergent%0Acapabilities.%20Specifically%2C%20we%20show%20that%20once%20the%20language%27s%20underlying%20grammar%0Aand%20context-sensitivity%20inducing%20structures%20are%20learned%20by%20the%20model%2C%0Aperformance%20on%20narrower%20tasks%20suddenly%20begins%20to%20improve.%20We%20then%20analogize%20our%0Anetwork%27s%20learning%20dynamics%20with%20the%20process%20of%20percolation%20on%20a%20bipartite%0Agraph%2C%20establishing%20a%20formal%20phase%20transition%20model%20that%20predicts%20the%20shift%20in%0Athe%20point%20of%20emergence%20observed%20in%20experiment%20when%20changing%20the%20data%20structure.%0AOverall%2C%20our%20experimental%20and%20theoretical%20frameworks%20yield%20a%20step%20towards%0Abetter%20defining%2C%20characterizing%2C%20and%20predicting%20emergence%20in%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Percolation%2520Model%2520of%2520Emergence%253A%2520Analyzing%2520Transformers%2520Trained%2520on%2520a%250A%2520%2520Formal%2520Language%26entry.906535625%3DEkdeep%2520Singh%2520Lubana%2520and%2520Kyogo%2520Kawaguchi%2520and%2520Robert%2520P.%2520Dick%2520and%2520Hidenori%2520Tanaka%26entry.1292438233%3D%2520%2520Increase%2520in%2520data%252C%2520size%252C%2520or%2520compute%2520can%2520lead%2520to%2520sudden%2520learning%2520of%2520specific%250Acapabilities%2520by%2520a%2520neural%2520network%2520--%2520a%2520phenomenon%2520often%2520called%2520%2522emergence%2522.%250ABeyond%2520scientific%2520understanding%252C%2520establishing%2520the%2520causal%2520factors%2520underlying%250Asuch%2520emergent%2520capabilities%2520is%2520crucial%2520to%2520enable%2520risk%2520regulation%2520frameworks%2520for%250AAI.%2520In%2520this%2520work%252C%2520we%2520seek%2520inspiration%2520from%2520study%2520of%2520emergent%2520properties%2520in%250Aother%2520fields%2520and%2520propose%2520a%2520phenomenological%2520definition%2520for%2520the%2520concept%2520in%2520the%250Acontext%2520of%2520neural%2520networks.%2520Our%2520definition%2520implicates%2520the%2520acquisition%2520of%250Aspecific%2520structures%2520underlying%2520the%2520data-generating%2520process%2520as%2520a%2520cause%2520of%2520sudden%250Aperformance%2520growth%2520for%2520specific%252C%2520narrower%2520tasks.%2520We%2520empirically%2520investigate%250Athis%2520definition%2520by%2520proposing%2520an%2520experimental%2520system%2520grounded%2520in%2520a%250Acontext-sensitive%2520formal%2520language%2520and%2520find%2520that%2520Transformers%2520trained%2520to%2520perform%250Atasks%2520on%2520top%2520of%2520strings%2520from%2520this%2520language%2520indeed%2520exhibit%2520emergent%250Acapabilities.%2520Specifically%252C%2520we%2520show%2520that%2520once%2520the%2520language%2527s%2520underlying%2520grammar%250Aand%2520context-sensitivity%2520inducing%2520structures%2520are%2520learned%2520by%2520the%2520model%252C%250Aperformance%2520on%2520narrower%2520tasks%2520suddenly%2520begins%2520to%2520improve.%2520We%2520then%2520analogize%2520our%250Anetwork%2527s%2520learning%2520dynamics%2520with%2520the%2520process%2520of%2520percolation%2520on%2520a%2520bipartite%250Agraph%252C%2520establishing%2520a%2520formal%2520phase%2520transition%2520model%2520that%2520predicts%2520the%2520shift%2520in%250Athe%2520point%2520of%2520emergence%2520observed%2520in%2520experiment%2520when%2520changing%2520the%2520data%2520structure.%250AOverall%252C%2520our%2520experimental%2520and%2520theoretical%2520frameworks%2520yield%2520a%2520step%2520towards%250Abetter%2520defining%252C%2520characterizing%252C%2520and%2520predicting%2520emergence%2520in%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Percolation%20Model%20of%20Emergence%3A%20Analyzing%20Transformers%20Trained%20on%20a%0A%20%20Formal%20Language&entry.906535625=Ekdeep%20Singh%20Lubana%20and%20Kyogo%20Kawaguchi%20and%20Robert%20P.%20Dick%20and%20Hidenori%20Tanaka&entry.1292438233=%20%20Increase%20in%20data%2C%20size%2C%20or%20compute%20can%20lead%20to%20sudden%20learning%20of%20specific%0Acapabilities%20by%20a%20neural%20network%20--%20a%20phenomenon%20often%20called%20%22emergence%22.%0ABeyond%20scientific%20understanding%2C%20establishing%20the%20causal%20factors%20underlying%0Asuch%20emergent%20capabilities%20is%20crucial%20to%20enable%20risk%20regulation%20frameworks%20for%0AAI.%20In%20this%20work%2C%20we%20seek%20inspiration%20from%20study%20of%20emergent%20properties%20in%0Aother%20fields%20and%20propose%20a%20phenomenological%20definition%20for%20the%20concept%20in%20the%0Acontext%20of%20neural%20networks.%20Our%20definition%20implicates%20the%20acquisition%20of%0Aspecific%20structures%20underlying%20the%20data-generating%20process%20as%20a%20cause%20of%20sudden%0Aperformance%20growth%20for%20specific%2C%20narrower%20tasks.%20We%20empirically%20investigate%0Athis%20definition%20by%20proposing%20an%20experimental%20system%20grounded%20in%20a%0Acontext-sensitive%20formal%20language%20and%20find%20that%20Transformers%20trained%20to%20perform%0Atasks%20on%20top%20of%20strings%20from%20this%20language%20indeed%20exhibit%20emergent%0Acapabilities.%20Specifically%2C%20we%20show%20that%20once%20the%20language%27s%20underlying%20grammar%0Aand%20context-sensitivity%20inducing%20structures%20are%20learned%20by%20the%20model%2C%0Aperformance%20on%20narrower%20tasks%20suddenly%20begins%20to%20improve.%20We%20then%20analogize%20our%0Anetwork%27s%20learning%20dynamics%20with%20the%20process%20of%20percolation%20on%20a%20bipartite%0Agraph%2C%20establishing%20a%20formal%20phase%20transition%20model%20that%20predicts%20the%20shift%20in%0Athe%20point%20of%20emergence%20observed%20in%20experiment%20when%20changing%20the%20data%20structure.%0AOverall%2C%20our%20experimental%20and%20theoretical%20frameworks%20yield%20a%20step%20towards%0Abetter%20defining%2C%20characterizing%2C%20and%20predicting%20emergence%20in%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12578v1&entry.124074799=Read"},
{"title": "Tactile Perception in Upper Limb Prostheses: Mechanical\n  Characterization, Human Experiments, and Computational Findings", "author": "Alessia Silvia Ivani and Manuel G. Catalano and Giorgio Grioli and Matteo Bianchi and Yon Visell and Antonio Bicchi", "abstract": "  Our research investigates vibrotactile perception in four prosthetic hands\nwith distinct kinematics and mechanical characteristics. We found that rigid\nand simple socket-based prosthetic devices can transmit tactile information and\nsurprisingly enable users to identify the stimulated finger with high\nreliability. This ability decreases with more advanced prosthetic hands with\nadditional articulations and softer mechanics. We conducted experiments to\nunderstand the underlying mechanisms. We assessed a prosthetic user's ability\nto discriminate finger contacts based on vibrations transmitted through the\nfour prosthetic hands. We also performed numerical and mechanical vibration\ntests on the prostheses and used a machine learning classifier to identify the\ncontacted finger. Our results show that simpler and rigid prosthetic hands\nfacilitate contact discrimination (for instance, a user of a purely cosmetic\nhand can distinguish a contact on the index finger from other fingers with 83%\naccuracy), but all tested hands, including soft advanced ones, performed above\nchance level. Despite advanced hands reducing vibration transmission, a machine\nlearning algorithm still exceeded human performance in discriminating finger\ncontacts. These findings suggest the potential for enhancing vibrotactile\nfeedback in advanced prosthetic hands and lay the groundwork for future\nintegration of such feedback in prosthetic devices.\n", "link": "http://arxiv.org/abs/2402.12989v2", "date": "2024-08-22", "relevancy": 2.0351, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5427}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5327}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tactile%20Perception%20in%20Upper%20Limb%20Prostheses%3A%20Mechanical%0A%20%20Characterization%2C%20Human%20Experiments%2C%20and%20Computational%20Findings&body=Title%3A%20Tactile%20Perception%20in%20Upper%20Limb%20Prostheses%3A%20Mechanical%0A%20%20Characterization%2C%20Human%20Experiments%2C%20and%20Computational%20Findings%0AAuthor%3A%20Alessia%20Silvia%20Ivani%20and%20Manuel%20G.%20Catalano%20and%20Giorgio%20Grioli%20and%20Matteo%20Bianchi%20and%20Yon%20Visell%20and%20Antonio%20Bicchi%0AAbstract%3A%20%20%20Our%20research%20investigates%20vibrotactile%20perception%20in%20four%20prosthetic%20hands%0Awith%20distinct%20kinematics%20and%20mechanical%20characteristics.%20We%20found%20that%20rigid%0Aand%20simple%20socket-based%20prosthetic%20devices%20can%20transmit%20tactile%20information%20and%0Asurprisingly%20enable%20users%20to%20identify%20the%20stimulated%20finger%20with%20high%0Areliability.%20This%20ability%20decreases%20with%20more%20advanced%20prosthetic%20hands%20with%0Aadditional%20articulations%20and%20softer%20mechanics.%20We%20conducted%20experiments%20to%0Aunderstand%20the%20underlying%20mechanisms.%20We%20assessed%20a%20prosthetic%20user%27s%20ability%0Ato%20discriminate%20finger%20contacts%20based%20on%20vibrations%20transmitted%20through%20the%0Afour%20prosthetic%20hands.%20We%20also%20performed%20numerical%20and%20mechanical%20vibration%0Atests%20on%20the%20prostheses%20and%20used%20a%20machine%20learning%20classifier%20to%20identify%20the%0Acontacted%20finger.%20Our%20results%20show%20that%20simpler%20and%20rigid%20prosthetic%20hands%0Afacilitate%20contact%20discrimination%20%28for%20instance%2C%20a%20user%20of%20a%20purely%20cosmetic%0Ahand%20can%20distinguish%20a%20contact%20on%20the%20index%20finger%20from%20other%20fingers%20with%2083%25%0Aaccuracy%29%2C%20but%20all%20tested%20hands%2C%20including%20soft%20advanced%20ones%2C%20performed%20above%0Achance%20level.%20Despite%20advanced%20hands%20reducing%20vibration%20transmission%2C%20a%20machine%0Alearning%20algorithm%20still%20exceeded%20human%20performance%20in%20discriminating%20finger%0Acontacts.%20These%20findings%20suggest%20the%20potential%20for%20enhancing%20vibrotactile%0Afeedback%20in%20advanced%20prosthetic%20hands%20and%20lay%20the%20groundwork%20for%20future%0Aintegration%20of%20such%20feedback%20in%20prosthetic%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12989v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTactile%2520Perception%2520in%2520Upper%2520Limb%2520Prostheses%253A%2520Mechanical%250A%2520%2520Characterization%252C%2520Human%2520Experiments%252C%2520and%2520Computational%2520Findings%26entry.906535625%3DAlessia%2520Silvia%2520Ivani%2520and%2520Manuel%2520G.%2520Catalano%2520and%2520Giorgio%2520Grioli%2520and%2520Matteo%2520Bianchi%2520and%2520Yon%2520Visell%2520and%2520Antonio%2520Bicchi%26entry.1292438233%3D%2520%2520Our%2520research%2520investigates%2520vibrotactile%2520perception%2520in%2520four%2520prosthetic%2520hands%250Awith%2520distinct%2520kinematics%2520and%2520mechanical%2520characteristics.%2520We%2520found%2520that%2520rigid%250Aand%2520simple%2520socket-based%2520prosthetic%2520devices%2520can%2520transmit%2520tactile%2520information%2520and%250Asurprisingly%2520enable%2520users%2520to%2520identify%2520the%2520stimulated%2520finger%2520with%2520high%250Areliability.%2520This%2520ability%2520decreases%2520with%2520more%2520advanced%2520prosthetic%2520hands%2520with%250Aadditional%2520articulations%2520and%2520softer%2520mechanics.%2520We%2520conducted%2520experiments%2520to%250Aunderstand%2520the%2520underlying%2520mechanisms.%2520We%2520assessed%2520a%2520prosthetic%2520user%2527s%2520ability%250Ato%2520discriminate%2520finger%2520contacts%2520based%2520on%2520vibrations%2520transmitted%2520through%2520the%250Afour%2520prosthetic%2520hands.%2520We%2520also%2520performed%2520numerical%2520and%2520mechanical%2520vibration%250Atests%2520on%2520the%2520prostheses%2520and%2520used%2520a%2520machine%2520learning%2520classifier%2520to%2520identify%2520the%250Acontacted%2520finger.%2520Our%2520results%2520show%2520that%2520simpler%2520and%2520rigid%2520prosthetic%2520hands%250Afacilitate%2520contact%2520discrimination%2520%2528for%2520instance%252C%2520a%2520user%2520of%2520a%2520purely%2520cosmetic%250Ahand%2520can%2520distinguish%2520a%2520contact%2520on%2520the%2520index%2520finger%2520from%2520other%2520fingers%2520with%252083%2525%250Aaccuracy%2529%252C%2520but%2520all%2520tested%2520hands%252C%2520including%2520soft%2520advanced%2520ones%252C%2520performed%2520above%250Achance%2520level.%2520Despite%2520advanced%2520hands%2520reducing%2520vibration%2520transmission%252C%2520a%2520machine%250Alearning%2520algorithm%2520still%2520exceeded%2520human%2520performance%2520in%2520discriminating%2520finger%250Acontacts.%2520These%2520findings%2520suggest%2520the%2520potential%2520for%2520enhancing%2520vibrotactile%250Afeedback%2520in%2520advanced%2520prosthetic%2520hands%2520and%2520lay%2520the%2520groundwork%2520for%2520future%250Aintegration%2520of%2520such%2520feedback%2520in%2520prosthetic%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12989v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tactile%20Perception%20in%20Upper%20Limb%20Prostheses%3A%20Mechanical%0A%20%20Characterization%2C%20Human%20Experiments%2C%20and%20Computational%20Findings&entry.906535625=Alessia%20Silvia%20Ivani%20and%20Manuel%20G.%20Catalano%20and%20Giorgio%20Grioli%20and%20Matteo%20Bianchi%20and%20Yon%20Visell%20and%20Antonio%20Bicchi&entry.1292438233=%20%20Our%20research%20investigates%20vibrotactile%20perception%20in%20four%20prosthetic%20hands%0Awith%20distinct%20kinematics%20and%20mechanical%20characteristics.%20We%20found%20that%20rigid%0Aand%20simple%20socket-based%20prosthetic%20devices%20can%20transmit%20tactile%20information%20and%0Asurprisingly%20enable%20users%20to%20identify%20the%20stimulated%20finger%20with%20high%0Areliability.%20This%20ability%20decreases%20with%20more%20advanced%20prosthetic%20hands%20with%0Aadditional%20articulations%20and%20softer%20mechanics.%20We%20conducted%20experiments%20to%0Aunderstand%20the%20underlying%20mechanisms.%20We%20assessed%20a%20prosthetic%20user%27s%20ability%0Ato%20discriminate%20finger%20contacts%20based%20on%20vibrations%20transmitted%20through%20the%0Afour%20prosthetic%20hands.%20We%20also%20performed%20numerical%20and%20mechanical%20vibration%0Atests%20on%20the%20prostheses%20and%20used%20a%20machine%20learning%20classifier%20to%20identify%20the%0Acontacted%20finger.%20Our%20results%20show%20that%20simpler%20and%20rigid%20prosthetic%20hands%0Afacilitate%20contact%20discrimination%20%28for%20instance%2C%20a%20user%20of%20a%20purely%20cosmetic%0Ahand%20can%20distinguish%20a%20contact%20on%20the%20index%20finger%20from%20other%20fingers%20with%2083%25%0Aaccuracy%29%2C%20but%20all%20tested%20hands%2C%20including%20soft%20advanced%20ones%2C%20performed%20above%0Achance%20level.%20Despite%20advanced%20hands%20reducing%20vibration%20transmission%2C%20a%20machine%0Alearning%20algorithm%20still%20exceeded%20human%20performance%20in%20discriminating%20finger%0Acontacts.%20These%20findings%20suggest%20the%20potential%20for%20enhancing%20vibrotactile%0Afeedback%20in%20advanced%20prosthetic%20hands%20and%20lay%20the%20groundwork%20for%20future%0Aintegration%20of%20such%20feedback%20in%20prosthetic%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12989v2&entry.124074799=Read"},
{"title": "Distributed quasi-Newton robust estimation under differential privacy", "author": "Chuhan Wang and Lixing Zhu and Xuehu Zhu", "abstract": "  For distributed computing with Byzantine machines under Privacy Protection\n(PP) constraints, this paper develops a robust PP distributed quasi-Newton\nestimation, which only requires the node machines to transmit five vectors to\nthe central processor with high asymptotic relative efficiency. Compared with\nthe gradient descent strategy which requires more rounds of transmission and\nthe Newton iteration strategy which requires the entire Hessian matrix to be\ntransmitted, the novel quasi-Newton iteration has advantages in reducing\nprivacy budgeting and transmission cost. Moreover, our PP algorithm does not\ndepend on the boundedness of gradients and second-order derivatives. When\ngradients and second-order derivatives follow sub-exponential distributions, we\noffer a mechanism that can ensure PP with a sufficiently high probability.\nFurthermore, this novel estimator can achieve the optimal convergence rate and\nthe asymptotic normality. The numerical studies on synthetic and real data sets\nevaluate the performance of the proposed algorithm.\n", "link": "http://arxiv.org/abs/2408.12353v1", "date": "2024-08-22", "relevancy": 2.0349, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4149}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4035}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4026}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20quasi-Newton%20robust%20estimation%20under%20differential%20privacy&body=Title%3A%20Distributed%20quasi-Newton%20robust%20estimation%20under%20differential%20privacy%0AAuthor%3A%20Chuhan%20Wang%20and%20Lixing%20Zhu%20and%20Xuehu%20Zhu%0AAbstract%3A%20%20%20For%20distributed%20computing%20with%20Byzantine%20machines%20under%20Privacy%20Protection%0A%28PP%29%20constraints%2C%20this%20paper%20develops%20a%20robust%20PP%20distributed%20quasi-Newton%0Aestimation%2C%20which%20only%20requires%20the%20node%20machines%20to%20transmit%20five%20vectors%20to%0Athe%20central%20processor%20with%20high%20asymptotic%20relative%20efficiency.%20Compared%20with%0Athe%20gradient%20descent%20strategy%20which%20requires%20more%20rounds%20of%20transmission%20and%0Athe%20Newton%20iteration%20strategy%20which%20requires%20the%20entire%20Hessian%20matrix%20to%20be%0Atransmitted%2C%20the%20novel%20quasi-Newton%20iteration%20has%20advantages%20in%20reducing%0Aprivacy%20budgeting%20and%20transmission%20cost.%20Moreover%2C%20our%20PP%20algorithm%20does%20not%0Adepend%20on%20the%20boundedness%20of%20gradients%20and%20second-order%20derivatives.%20When%0Agradients%20and%20second-order%20derivatives%20follow%20sub-exponential%20distributions%2C%20we%0Aoffer%20a%20mechanism%20that%20can%20ensure%20PP%20with%20a%20sufficiently%20high%20probability.%0AFurthermore%2C%20this%20novel%20estimator%20can%20achieve%20the%20optimal%20convergence%20rate%20and%0Athe%20asymptotic%20normality.%20The%20numerical%20studies%20on%20synthetic%20and%20real%20data%20sets%0Aevaluate%20the%20performance%20of%20the%20proposed%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520quasi-Newton%2520robust%2520estimation%2520under%2520differential%2520privacy%26entry.906535625%3DChuhan%2520Wang%2520and%2520Lixing%2520Zhu%2520and%2520Xuehu%2520Zhu%26entry.1292438233%3D%2520%2520For%2520distributed%2520computing%2520with%2520Byzantine%2520machines%2520under%2520Privacy%2520Protection%250A%2528PP%2529%2520constraints%252C%2520this%2520paper%2520develops%2520a%2520robust%2520PP%2520distributed%2520quasi-Newton%250Aestimation%252C%2520which%2520only%2520requires%2520the%2520node%2520machines%2520to%2520transmit%2520five%2520vectors%2520to%250Athe%2520central%2520processor%2520with%2520high%2520asymptotic%2520relative%2520efficiency.%2520Compared%2520with%250Athe%2520gradient%2520descent%2520strategy%2520which%2520requires%2520more%2520rounds%2520of%2520transmission%2520and%250Athe%2520Newton%2520iteration%2520strategy%2520which%2520requires%2520the%2520entire%2520Hessian%2520matrix%2520to%2520be%250Atransmitted%252C%2520the%2520novel%2520quasi-Newton%2520iteration%2520has%2520advantages%2520in%2520reducing%250Aprivacy%2520budgeting%2520and%2520transmission%2520cost.%2520Moreover%252C%2520our%2520PP%2520algorithm%2520does%2520not%250Adepend%2520on%2520the%2520boundedness%2520of%2520gradients%2520and%2520second-order%2520derivatives.%2520When%250Agradients%2520and%2520second-order%2520derivatives%2520follow%2520sub-exponential%2520distributions%252C%2520we%250Aoffer%2520a%2520mechanism%2520that%2520can%2520ensure%2520PP%2520with%2520a%2520sufficiently%2520high%2520probability.%250AFurthermore%252C%2520this%2520novel%2520estimator%2520can%2520achieve%2520the%2520optimal%2520convergence%2520rate%2520and%250Athe%2520asymptotic%2520normality.%2520The%2520numerical%2520studies%2520on%2520synthetic%2520and%2520real%2520data%2520sets%250Aevaluate%2520the%2520performance%2520of%2520the%2520proposed%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20quasi-Newton%20robust%20estimation%20under%20differential%20privacy&entry.906535625=Chuhan%20Wang%20and%20Lixing%20Zhu%20and%20Xuehu%20Zhu&entry.1292438233=%20%20For%20distributed%20computing%20with%20Byzantine%20machines%20under%20Privacy%20Protection%0A%28PP%29%20constraints%2C%20this%20paper%20develops%20a%20robust%20PP%20distributed%20quasi-Newton%0Aestimation%2C%20which%20only%20requires%20the%20node%20machines%20to%20transmit%20five%20vectors%20to%0Athe%20central%20processor%20with%20high%20asymptotic%20relative%20efficiency.%20Compared%20with%0Athe%20gradient%20descent%20strategy%20which%20requires%20more%20rounds%20of%20transmission%20and%0Athe%20Newton%20iteration%20strategy%20which%20requires%20the%20entire%20Hessian%20matrix%20to%20be%0Atransmitted%2C%20the%20novel%20quasi-Newton%20iteration%20has%20advantages%20in%20reducing%0Aprivacy%20budgeting%20and%20transmission%20cost.%20Moreover%2C%20our%20PP%20algorithm%20does%20not%0Adepend%20on%20the%20boundedness%20of%20gradients%20and%20second-order%20derivatives.%20When%0Agradients%20and%20second-order%20derivatives%20follow%20sub-exponential%20distributions%2C%20we%0Aoffer%20a%20mechanism%20that%20can%20ensure%20PP%20with%20a%20sufficiently%20high%20probability.%0AFurthermore%2C%20this%20novel%20estimator%20can%20achieve%20the%20optimal%20convergence%20rate%20and%0Athe%20asymptotic%20normality.%20The%20numerical%20studies%20on%20synthetic%20and%20real%20data%20sets%0Aevaluate%20the%20performance%20of%20the%20proposed%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12353v1&entry.124074799=Read"},
{"title": "Dynamic PDB: A New Dataset and a SE(3) Model Extension by Integrating\n  Dynamic Behaviors and Physical Properties in Protein Structures", "author": "Ce Liu and Jun Wang and Zhiqiang Cai and Yingxu Wang and Huizhen Kuang and Kaihui Cheng and Liwei Zhang and Qingkun Su and Yining Tang and Fenglei Cao and Limei Han and Siyu Zhu and Yuan Qi", "abstract": "  Despite significant progress in static protein structure collection and\nprediction, the dynamic behavior of proteins, one of their most vital\ncharacteristics, has been largely overlooked in prior research. This oversight\ncan be attributed to the limited availability, diversity, and heterogeneity of\ndynamic protein datasets. To address this gap, we propose to enhance existing\nprestigious static 3D protein structural databases, such as the Protein Data\nBank (PDB), by integrating dynamic data and additional physical properties.\nSpecifically, we introduce a large-scale dataset, Dynamic PDB, encompassing\napproximately 12.6K proteins, each subjected to all-atom molecular dynamics\n(MD) simulations lasting 1 microsecond to capture conformational changes.\nFurthermore, we provide a comprehensive suite of physical properties, including\natomic velocities and forces, potential and kinetic energies of proteins, and\nthe temperature of the simulation environment, recorded at 1 picosecond\nintervals throughout the simulations. For benchmarking purposes, we evaluate\nstate-of-the-art methods on the proposed dataset for the task of trajectory\nprediction. To demonstrate the value of integrating richer physical properties\nin the study of protein dynamics and related model design, we base our approach\non the SE(3) diffusion model and incorporate these physical properties into the\ntrajectory prediction process. Preliminary results indicate that this\nstraightforward extension of the SE(3) model yields improved accuracy, as\nmeasured by MAE and RMSD, when the proposed physical properties are taken into\nconsideration.\n", "link": "http://arxiv.org/abs/2408.12413v1", "date": "2024-08-22", "relevancy": 2.0297, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5568}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4975}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20PDB%3A%20A%20New%20Dataset%20and%20a%20SE%283%29%20Model%20Extension%20by%20Integrating%0A%20%20Dynamic%20Behaviors%20and%20Physical%20Properties%20in%20Protein%20Structures&body=Title%3A%20Dynamic%20PDB%3A%20A%20New%20Dataset%20and%20a%20SE%283%29%20Model%20Extension%20by%20Integrating%0A%20%20Dynamic%20Behaviors%20and%20Physical%20Properties%20in%20Protein%20Structures%0AAuthor%3A%20Ce%20Liu%20and%20Jun%20Wang%20and%20Zhiqiang%20Cai%20and%20Yingxu%20Wang%20and%20Huizhen%20Kuang%20and%20Kaihui%20Cheng%20and%20Liwei%20Zhang%20and%20Qingkun%20Su%20and%20Yining%20Tang%20and%20Fenglei%20Cao%20and%20Limei%20Han%20and%20Siyu%20Zhu%20and%20Yuan%20Qi%0AAbstract%3A%20%20%20Despite%20significant%20progress%20in%20static%20protein%20structure%20collection%20and%0Aprediction%2C%20the%20dynamic%20behavior%20of%20proteins%2C%20one%20of%20their%20most%20vital%0Acharacteristics%2C%20has%20been%20largely%20overlooked%20in%20prior%20research.%20This%20oversight%0Acan%20be%20attributed%20to%20the%20limited%20availability%2C%20diversity%2C%20and%20heterogeneity%20of%0Adynamic%20protein%20datasets.%20To%20address%20this%20gap%2C%20we%20propose%20to%20enhance%20existing%0Aprestigious%20static%203D%20protein%20structural%20databases%2C%20such%20as%20the%20Protein%20Data%0ABank%20%28PDB%29%2C%20by%20integrating%20dynamic%20data%20and%20additional%20physical%20properties.%0ASpecifically%2C%20we%20introduce%20a%20large-scale%20dataset%2C%20Dynamic%20PDB%2C%20encompassing%0Aapproximately%2012.6K%20proteins%2C%20each%20subjected%20to%20all-atom%20molecular%20dynamics%0A%28MD%29%20simulations%20lasting%201%20microsecond%20to%20capture%20conformational%20changes.%0AFurthermore%2C%20we%20provide%20a%20comprehensive%20suite%20of%20physical%20properties%2C%20including%0Aatomic%20velocities%20and%20forces%2C%20potential%20and%20kinetic%20energies%20of%20proteins%2C%20and%0Athe%20temperature%20of%20the%20simulation%20environment%2C%20recorded%20at%201%20picosecond%0Aintervals%20throughout%20the%20simulations.%20For%20benchmarking%20purposes%2C%20we%20evaluate%0Astate-of-the-art%20methods%20on%20the%20proposed%20dataset%20for%20the%20task%20of%20trajectory%0Aprediction.%20To%20demonstrate%20the%20value%20of%20integrating%20richer%20physical%20properties%0Ain%20the%20study%20of%20protein%20dynamics%20and%20related%20model%20design%2C%20we%20base%20our%20approach%0Aon%20the%20SE%283%29%20diffusion%20model%20and%20incorporate%20these%20physical%20properties%20into%20the%0Atrajectory%20prediction%20process.%20Preliminary%20results%20indicate%20that%20this%0Astraightforward%20extension%20of%20the%20SE%283%29%20model%20yields%20improved%20accuracy%2C%20as%0Ameasured%20by%20MAE%20and%20RMSD%2C%20when%20the%20proposed%20physical%20properties%20are%20taken%20into%0Aconsideration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12413v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520PDB%253A%2520A%2520New%2520Dataset%2520and%2520a%2520SE%25283%2529%2520Model%2520Extension%2520by%2520Integrating%250A%2520%2520Dynamic%2520Behaviors%2520and%2520Physical%2520Properties%2520in%2520Protein%2520Structures%26entry.906535625%3DCe%2520Liu%2520and%2520Jun%2520Wang%2520and%2520Zhiqiang%2520Cai%2520and%2520Yingxu%2520Wang%2520and%2520Huizhen%2520Kuang%2520and%2520Kaihui%2520Cheng%2520and%2520Liwei%2520Zhang%2520and%2520Qingkun%2520Su%2520and%2520Yining%2520Tang%2520and%2520Fenglei%2520Cao%2520and%2520Limei%2520Han%2520and%2520Siyu%2520Zhu%2520and%2520Yuan%2520Qi%26entry.1292438233%3D%2520%2520Despite%2520significant%2520progress%2520in%2520static%2520protein%2520structure%2520collection%2520and%250Aprediction%252C%2520the%2520dynamic%2520behavior%2520of%2520proteins%252C%2520one%2520of%2520their%2520most%2520vital%250Acharacteristics%252C%2520has%2520been%2520largely%2520overlooked%2520in%2520prior%2520research.%2520This%2520oversight%250Acan%2520be%2520attributed%2520to%2520the%2520limited%2520availability%252C%2520diversity%252C%2520and%2520heterogeneity%2520of%250Adynamic%2520protein%2520datasets.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520to%2520enhance%2520existing%250Aprestigious%2520static%25203D%2520protein%2520structural%2520databases%252C%2520such%2520as%2520the%2520Protein%2520Data%250ABank%2520%2528PDB%2529%252C%2520by%2520integrating%2520dynamic%2520data%2520and%2520additional%2520physical%2520properties.%250ASpecifically%252C%2520we%2520introduce%2520a%2520large-scale%2520dataset%252C%2520Dynamic%2520PDB%252C%2520encompassing%250Aapproximately%252012.6K%2520proteins%252C%2520each%2520subjected%2520to%2520all-atom%2520molecular%2520dynamics%250A%2528MD%2529%2520simulations%2520lasting%25201%2520microsecond%2520to%2520capture%2520conformational%2520changes.%250AFurthermore%252C%2520we%2520provide%2520a%2520comprehensive%2520suite%2520of%2520physical%2520properties%252C%2520including%250Aatomic%2520velocities%2520and%2520forces%252C%2520potential%2520and%2520kinetic%2520energies%2520of%2520proteins%252C%2520and%250Athe%2520temperature%2520of%2520the%2520simulation%2520environment%252C%2520recorded%2520at%25201%2520picosecond%250Aintervals%2520throughout%2520the%2520simulations.%2520For%2520benchmarking%2520purposes%252C%2520we%2520evaluate%250Astate-of-the-art%2520methods%2520on%2520the%2520proposed%2520dataset%2520for%2520the%2520task%2520of%2520trajectory%250Aprediction.%2520To%2520demonstrate%2520the%2520value%2520of%2520integrating%2520richer%2520physical%2520properties%250Ain%2520the%2520study%2520of%2520protein%2520dynamics%2520and%2520related%2520model%2520design%252C%2520we%2520base%2520our%2520approach%250Aon%2520the%2520SE%25283%2529%2520diffusion%2520model%2520and%2520incorporate%2520these%2520physical%2520properties%2520into%2520the%250Atrajectory%2520prediction%2520process.%2520Preliminary%2520results%2520indicate%2520that%2520this%250Astraightforward%2520extension%2520of%2520the%2520SE%25283%2529%2520model%2520yields%2520improved%2520accuracy%252C%2520as%250Ameasured%2520by%2520MAE%2520and%2520RMSD%252C%2520when%2520the%2520proposed%2520physical%2520properties%2520are%2520taken%2520into%250Aconsideration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12413v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20PDB%3A%20A%20New%20Dataset%20and%20a%20SE%283%29%20Model%20Extension%20by%20Integrating%0A%20%20Dynamic%20Behaviors%20and%20Physical%20Properties%20in%20Protein%20Structures&entry.906535625=Ce%20Liu%20and%20Jun%20Wang%20and%20Zhiqiang%20Cai%20and%20Yingxu%20Wang%20and%20Huizhen%20Kuang%20and%20Kaihui%20Cheng%20and%20Liwei%20Zhang%20and%20Qingkun%20Su%20and%20Yining%20Tang%20and%20Fenglei%20Cao%20and%20Limei%20Han%20and%20Siyu%20Zhu%20and%20Yuan%20Qi&entry.1292438233=%20%20Despite%20significant%20progress%20in%20static%20protein%20structure%20collection%20and%0Aprediction%2C%20the%20dynamic%20behavior%20of%20proteins%2C%20one%20of%20their%20most%20vital%0Acharacteristics%2C%20has%20been%20largely%20overlooked%20in%20prior%20research.%20This%20oversight%0Acan%20be%20attributed%20to%20the%20limited%20availability%2C%20diversity%2C%20and%20heterogeneity%20of%0Adynamic%20protein%20datasets.%20To%20address%20this%20gap%2C%20we%20propose%20to%20enhance%20existing%0Aprestigious%20static%203D%20protein%20structural%20databases%2C%20such%20as%20the%20Protein%20Data%0ABank%20%28PDB%29%2C%20by%20integrating%20dynamic%20data%20and%20additional%20physical%20properties.%0ASpecifically%2C%20we%20introduce%20a%20large-scale%20dataset%2C%20Dynamic%20PDB%2C%20encompassing%0Aapproximately%2012.6K%20proteins%2C%20each%20subjected%20to%20all-atom%20molecular%20dynamics%0A%28MD%29%20simulations%20lasting%201%20microsecond%20to%20capture%20conformational%20changes.%0AFurthermore%2C%20we%20provide%20a%20comprehensive%20suite%20of%20physical%20properties%2C%20including%0Aatomic%20velocities%20and%20forces%2C%20potential%20and%20kinetic%20energies%20of%20proteins%2C%20and%0Athe%20temperature%20of%20the%20simulation%20environment%2C%20recorded%20at%201%20picosecond%0Aintervals%20throughout%20the%20simulations.%20For%20benchmarking%20purposes%2C%20we%20evaluate%0Astate-of-the-art%20methods%20on%20the%20proposed%20dataset%20for%20the%20task%20of%20trajectory%0Aprediction.%20To%20demonstrate%20the%20value%20of%20integrating%20richer%20physical%20properties%0Ain%20the%20study%20of%20protein%20dynamics%20and%20related%20model%20design%2C%20we%20base%20our%20approach%0Aon%20the%20SE%283%29%20diffusion%20model%20and%20incorporate%20these%20physical%20properties%20into%20the%0Atrajectory%20prediction%20process.%20Preliminary%20results%20indicate%20that%20this%0Astraightforward%20extension%20of%20the%20SE%283%29%20model%20yields%20improved%20accuracy%2C%20as%0Ameasured%20by%20MAE%20and%20RMSD%2C%20when%20the%20proposed%20physical%20properties%20are%20taken%20into%0Aconsideration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12413v1&entry.124074799=Read"},
{"title": "Domain Generalization through Meta-Learning: A Survey", "author": "Arsham Gholamzadeh Khoee and Yinan Yu and Robert Feldt", "abstract": "  Deep neural networks (DNNs) have revolutionized artificial intelligence but\noften lack performance when faced with out-of-distribution (OOD) data, a common\nscenario due to the inevitable domain shifts in real-world applications. This\nlimitation stems from the common assumption that training and testing data\nshare the same distribution--an assumption frequently violated in practice.\nDespite their effectiveness with large amounts of data and computational power,\nDNNs struggle with distributional shifts and limited labeled data, leading to\noverfitting and poor generalization across various tasks and domains.\nMeta-learning presents a promising approach by employing algorithms that\nacquire transferable knowledge across various tasks for fast adaptation,\neliminating the need to learn each task from scratch. This survey paper delves\ninto the realm of meta-learning with a focus on its contribution to domain\ngeneralization. We first clarify the concept of meta-learning for domain\ngeneralization and introduce a novel taxonomy based on the feature extraction\nstrategy and the classifier learning methodology, offering a granular view of\nmethodologies. Additionally, we present a decision graph to assist readers in\nnavigating the taxonomy based on data availability and domain shifts, enabling\nthem to select and develop a proper model tailored to their specific problem\nrequirements. Through an exhaustive review of existing methods and underlying\ntheories, we map out the fundamentals of the field. Our survey provides\npractical insights and an informed discussion on promising research directions.\n", "link": "http://arxiv.org/abs/2404.02785v3", "date": "2024-08-22", "relevancy": 2.0126, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5227}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4907}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalization%20through%20Meta-Learning%3A%20A%20Survey&body=Title%3A%20Domain%20Generalization%20through%20Meta-Learning%3A%20A%20Survey%0AAuthor%3A%20Arsham%20Gholamzadeh%20Khoee%20and%20Yinan%20Yu%20and%20Robert%20Feldt%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20revolutionized%20artificial%20intelligence%20but%0Aoften%20lack%20performance%20when%20faced%20with%20out-of-distribution%20%28OOD%29%20data%2C%20a%20common%0Ascenario%20due%20to%20the%20inevitable%20domain%20shifts%20in%20real-world%20applications.%20This%0Alimitation%20stems%20from%20the%20common%20assumption%20that%20training%20and%20testing%20data%0Ashare%20the%20same%20distribution--an%20assumption%20frequently%20violated%20in%20practice.%0ADespite%20their%20effectiveness%20with%20large%20amounts%20of%20data%20and%20computational%20power%2C%0ADNNs%20struggle%20with%20distributional%20shifts%20and%20limited%20labeled%20data%2C%20leading%20to%0Aoverfitting%20and%20poor%20generalization%20across%20various%20tasks%20and%20domains.%0AMeta-learning%20presents%20a%20promising%20approach%20by%20employing%20algorithms%20that%0Aacquire%20transferable%20knowledge%20across%20various%20tasks%20for%20fast%20adaptation%2C%0Aeliminating%20the%20need%20to%20learn%20each%20task%20from%20scratch.%20This%20survey%20paper%20delves%0Ainto%20the%20realm%20of%20meta-learning%20with%20a%20focus%20on%20its%20contribution%20to%20domain%0Ageneralization.%20We%20first%20clarify%20the%20concept%20of%20meta-learning%20for%20domain%0Ageneralization%20and%20introduce%20a%20novel%20taxonomy%20based%20on%20the%20feature%20extraction%0Astrategy%20and%20the%20classifier%20learning%20methodology%2C%20offering%20a%20granular%20view%20of%0Amethodologies.%20Additionally%2C%20we%20present%20a%20decision%20graph%20to%20assist%20readers%20in%0Anavigating%20the%20taxonomy%20based%20on%20data%20availability%20and%20domain%20shifts%2C%20enabling%0Athem%20to%20select%20and%20develop%20a%20proper%20model%20tailored%20to%20their%20specific%20problem%0Arequirements.%20Through%20an%20exhaustive%20review%20of%20existing%20methods%20and%20underlying%0Atheories%2C%20we%20map%20out%20the%20fundamentals%20of%20the%20field.%20Our%20survey%20provides%0Apractical%20insights%20and%20an%20informed%20discussion%20on%20promising%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02785v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Generalization%2520through%2520Meta-Learning%253A%2520A%2520Survey%26entry.906535625%3DArsham%2520Gholamzadeh%2520Khoee%2520and%2520Yinan%2520Yu%2520and%2520Robert%2520Feldt%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520have%2520revolutionized%2520artificial%2520intelligence%2520but%250Aoften%2520lack%2520performance%2520when%2520faced%2520with%2520out-of-distribution%2520%2528OOD%2529%2520data%252C%2520a%2520common%250Ascenario%2520due%2520to%2520the%2520inevitable%2520domain%2520shifts%2520in%2520real-world%2520applications.%2520This%250Alimitation%2520stems%2520from%2520the%2520common%2520assumption%2520that%2520training%2520and%2520testing%2520data%250Ashare%2520the%2520same%2520distribution--an%2520assumption%2520frequently%2520violated%2520in%2520practice.%250ADespite%2520their%2520effectiveness%2520with%2520large%2520amounts%2520of%2520data%2520and%2520computational%2520power%252C%250ADNNs%2520struggle%2520with%2520distributional%2520shifts%2520and%2520limited%2520labeled%2520data%252C%2520leading%2520to%250Aoverfitting%2520and%2520poor%2520generalization%2520across%2520various%2520tasks%2520and%2520domains.%250AMeta-learning%2520presents%2520a%2520promising%2520approach%2520by%2520employing%2520algorithms%2520that%250Aacquire%2520transferable%2520knowledge%2520across%2520various%2520tasks%2520for%2520fast%2520adaptation%252C%250Aeliminating%2520the%2520need%2520to%2520learn%2520each%2520task%2520from%2520scratch.%2520This%2520survey%2520paper%2520delves%250Ainto%2520the%2520realm%2520of%2520meta-learning%2520with%2520a%2520focus%2520on%2520its%2520contribution%2520to%2520domain%250Ageneralization.%2520We%2520first%2520clarify%2520the%2520concept%2520of%2520meta-learning%2520for%2520domain%250Ageneralization%2520and%2520introduce%2520a%2520novel%2520taxonomy%2520based%2520on%2520the%2520feature%2520extraction%250Astrategy%2520and%2520the%2520classifier%2520learning%2520methodology%252C%2520offering%2520a%2520granular%2520view%2520of%250Amethodologies.%2520Additionally%252C%2520we%2520present%2520a%2520decision%2520graph%2520to%2520assist%2520readers%2520in%250Anavigating%2520the%2520taxonomy%2520based%2520on%2520data%2520availability%2520and%2520domain%2520shifts%252C%2520enabling%250Athem%2520to%2520select%2520and%2520develop%2520a%2520proper%2520model%2520tailored%2520to%2520their%2520specific%2520problem%250Arequirements.%2520Through%2520an%2520exhaustive%2520review%2520of%2520existing%2520methods%2520and%2520underlying%250Atheories%252C%2520we%2520map%2520out%2520the%2520fundamentals%2520of%2520the%2520field.%2520Our%2520survey%2520provides%250Apractical%2520insights%2520and%2520an%2520informed%2520discussion%2520on%2520promising%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02785v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalization%20through%20Meta-Learning%3A%20A%20Survey&entry.906535625=Arsham%20Gholamzadeh%20Khoee%20and%20Yinan%20Yu%20and%20Robert%20Feldt&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20have%20revolutionized%20artificial%20intelligence%20but%0Aoften%20lack%20performance%20when%20faced%20with%20out-of-distribution%20%28OOD%29%20data%2C%20a%20common%0Ascenario%20due%20to%20the%20inevitable%20domain%20shifts%20in%20real-world%20applications.%20This%0Alimitation%20stems%20from%20the%20common%20assumption%20that%20training%20and%20testing%20data%0Ashare%20the%20same%20distribution--an%20assumption%20frequently%20violated%20in%20practice.%0ADespite%20their%20effectiveness%20with%20large%20amounts%20of%20data%20and%20computational%20power%2C%0ADNNs%20struggle%20with%20distributional%20shifts%20and%20limited%20labeled%20data%2C%20leading%20to%0Aoverfitting%20and%20poor%20generalization%20across%20various%20tasks%20and%20domains.%0AMeta-learning%20presents%20a%20promising%20approach%20by%20employing%20algorithms%20that%0Aacquire%20transferable%20knowledge%20across%20various%20tasks%20for%20fast%20adaptation%2C%0Aeliminating%20the%20need%20to%20learn%20each%20task%20from%20scratch.%20This%20survey%20paper%20delves%0Ainto%20the%20realm%20of%20meta-learning%20with%20a%20focus%20on%20its%20contribution%20to%20domain%0Ageneralization.%20We%20first%20clarify%20the%20concept%20of%20meta-learning%20for%20domain%0Ageneralization%20and%20introduce%20a%20novel%20taxonomy%20based%20on%20the%20feature%20extraction%0Astrategy%20and%20the%20classifier%20learning%20methodology%2C%20offering%20a%20granular%20view%20of%0Amethodologies.%20Additionally%2C%20we%20present%20a%20decision%20graph%20to%20assist%20readers%20in%0Anavigating%20the%20taxonomy%20based%20on%20data%20availability%20and%20domain%20shifts%2C%20enabling%0Athem%20to%20select%20and%20develop%20a%20proper%20model%20tailored%20to%20their%20specific%20problem%0Arequirements.%20Through%20an%20exhaustive%20review%20of%20existing%20methods%20and%20underlying%0Atheories%2C%20we%20map%20out%20the%20fundamentals%20of%20the%20field.%20Our%20survey%20provides%0Apractical%20insights%20and%20an%20informed%20discussion%20on%20promising%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02785v3&entry.124074799=Read"},
{"title": "Sharper Bounds for Chebyshev Moment Matching with Applications to\n  Differential Privacy and Beyond", "author": "Cameron Musco and Christopher Musco and Lucas Rosenblatt and Apoorv Vikram Singh", "abstract": "  We study the problem of approximately recovering a probability distribution\ngiven noisy measurements of its Chebyshev polynomial moments. We sharpen prior\nwork, proving that accurate recovery in the Wasserstein distance is possible\nwith more noise than previously known.\n  As a main application, our result yields a simple \"linear query\" algorithm\nfor constructing a differentially private synthetic data distribution with\nWasserstein-1 error $\\tilde{O}(1/n)$ based on a dataset of $n$ points in\n$[-1,1]$. This bound is optimal up to log factors and matches a recent\nbreakthrough of Boedihardjo, Strohmer, and Vershynin [Probab. Theory. Rel.,\n2024], which uses a more complex \"superregular random walk\" method to beat an\n$O(1/\\sqrt{n})$ accuracy barrier inherent to earlier approaches.\n  We illustrate a second application of our new moment-based recovery bound in\nnumerical linear algebra: by improving an approach of Braverman, Krishnan, and\nMusco [STOC 2022], our result yields a faster algorithm for estimating the\nspectral density of a symmetric matrix up to small error in the Wasserstein\ndistance.\n", "link": "http://arxiv.org/abs/2408.12385v1", "date": "2024-08-22", "relevancy": 2.0117, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.409}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4067}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharper%20Bounds%20for%20Chebyshev%20Moment%20Matching%20with%20Applications%20to%0A%20%20Differential%20Privacy%20and%20Beyond&body=Title%3A%20Sharper%20Bounds%20for%20Chebyshev%20Moment%20Matching%20with%20Applications%20to%0A%20%20Differential%20Privacy%20and%20Beyond%0AAuthor%3A%20Cameron%20Musco%20and%20Christopher%20Musco%20and%20Lucas%20Rosenblatt%20and%20Apoorv%20Vikram%20Singh%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20approximately%20recovering%20a%20probability%20distribution%0Agiven%20noisy%20measurements%20of%20its%20Chebyshev%20polynomial%20moments.%20We%20sharpen%20prior%0Awork%2C%20proving%20that%20accurate%20recovery%20in%20the%20Wasserstein%20distance%20is%20possible%0Awith%20more%20noise%20than%20previously%20known.%0A%20%20As%20a%20main%20application%2C%20our%20result%20yields%20a%20simple%20%22linear%20query%22%20algorithm%0Afor%20constructing%20a%20differentially%20private%20synthetic%20data%20distribution%20with%0AWasserstein-1%20error%20%24%5Ctilde%7BO%7D%281/n%29%24%20based%20on%20a%20dataset%20of%20%24n%24%20points%20in%0A%24%5B-1%2C1%5D%24.%20This%20bound%20is%20optimal%20up%20to%20log%20factors%20and%20matches%20a%20recent%0Abreakthrough%20of%20Boedihardjo%2C%20Strohmer%2C%20and%20Vershynin%20%5BProbab.%20Theory.%20Rel.%2C%0A2024%5D%2C%20which%20uses%20a%20more%20complex%20%22superregular%20random%20walk%22%20method%20to%20beat%20an%0A%24O%281/%5Csqrt%7Bn%7D%29%24%20accuracy%20barrier%20inherent%20to%20earlier%20approaches.%0A%20%20We%20illustrate%20a%20second%20application%20of%20our%20new%20moment-based%20recovery%20bound%20in%0Anumerical%20linear%20algebra%3A%20by%20improving%20an%20approach%20of%20Braverman%2C%20Krishnan%2C%20and%0AMusco%20%5BSTOC%202022%5D%2C%20our%20result%20yields%20a%20faster%20algorithm%20for%20estimating%20the%0Aspectral%20density%20of%20a%20symmetric%20matrix%20up%20to%20small%20error%20in%20the%20Wasserstein%0Adistance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12385v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharper%2520Bounds%2520for%2520Chebyshev%2520Moment%2520Matching%2520with%2520Applications%2520to%250A%2520%2520Differential%2520Privacy%2520and%2520Beyond%26entry.906535625%3DCameron%2520Musco%2520and%2520Christopher%2520Musco%2520and%2520Lucas%2520Rosenblatt%2520and%2520Apoorv%2520Vikram%2520Singh%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520approximately%2520recovering%2520a%2520probability%2520distribution%250Agiven%2520noisy%2520measurements%2520of%2520its%2520Chebyshev%2520polynomial%2520moments.%2520We%2520sharpen%2520prior%250Awork%252C%2520proving%2520that%2520accurate%2520recovery%2520in%2520the%2520Wasserstein%2520distance%2520is%2520possible%250Awith%2520more%2520noise%2520than%2520previously%2520known.%250A%2520%2520As%2520a%2520main%2520application%252C%2520our%2520result%2520yields%2520a%2520simple%2520%2522linear%2520query%2522%2520algorithm%250Afor%2520constructing%2520a%2520differentially%2520private%2520synthetic%2520data%2520distribution%2520with%250AWasserstein-1%2520error%2520%2524%255Ctilde%257BO%257D%25281/n%2529%2524%2520based%2520on%2520a%2520dataset%2520of%2520%2524n%2524%2520points%2520in%250A%2524%255B-1%252C1%255D%2524.%2520This%2520bound%2520is%2520optimal%2520up%2520to%2520log%2520factors%2520and%2520matches%2520a%2520recent%250Abreakthrough%2520of%2520Boedihardjo%252C%2520Strohmer%252C%2520and%2520Vershynin%2520%255BProbab.%2520Theory.%2520Rel.%252C%250A2024%255D%252C%2520which%2520uses%2520a%2520more%2520complex%2520%2522superregular%2520random%2520walk%2522%2520method%2520to%2520beat%2520an%250A%2524O%25281/%255Csqrt%257Bn%257D%2529%2524%2520accuracy%2520barrier%2520inherent%2520to%2520earlier%2520approaches.%250A%2520%2520We%2520illustrate%2520a%2520second%2520application%2520of%2520our%2520new%2520moment-based%2520recovery%2520bound%2520in%250Anumerical%2520linear%2520algebra%253A%2520by%2520improving%2520an%2520approach%2520of%2520Braverman%252C%2520Krishnan%252C%2520and%250AMusco%2520%255BSTOC%25202022%255D%252C%2520our%2520result%2520yields%2520a%2520faster%2520algorithm%2520for%2520estimating%2520the%250Aspectral%2520density%2520of%2520a%2520symmetric%2520matrix%2520up%2520to%2520small%2520error%2520in%2520the%2520Wasserstein%250Adistance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12385v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharper%20Bounds%20for%20Chebyshev%20Moment%20Matching%20with%20Applications%20to%0A%20%20Differential%20Privacy%20and%20Beyond&entry.906535625=Cameron%20Musco%20and%20Christopher%20Musco%20and%20Lucas%20Rosenblatt%20and%20Apoorv%20Vikram%20Singh&entry.1292438233=%20%20We%20study%20the%20problem%20of%20approximately%20recovering%20a%20probability%20distribution%0Agiven%20noisy%20measurements%20of%20its%20Chebyshev%20polynomial%20moments.%20We%20sharpen%20prior%0Awork%2C%20proving%20that%20accurate%20recovery%20in%20the%20Wasserstein%20distance%20is%20possible%0Awith%20more%20noise%20than%20previously%20known.%0A%20%20As%20a%20main%20application%2C%20our%20result%20yields%20a%20simple%20%22linear%20query%22%20algorithm%0Afor%20constructing%20a%20differentially%20private%20synthetic%20data%20distribution%20with%0AWasserstein-1%20error%20%24%5Ctilde%7BO%7D%281/n%29%24%20based%20on%20a%20dataset%20of%20%24n%24%20points%20in%0A%24%5B-1%2C1%5D%24.%20This%20bound%20is%20optimal%20up%20to%20log%20factors%20and%20matches%20a%20recent%0Abreakthrough%20of%20Boedihardjo%2C%20Strohmer%2C%20and%20Vershynin%20%5BProbab.%20Theory.%20Rel.%2C%0A2024%5D%2C%20which%20uses%20a%20more%20complex%20%22superregular%20random%20walk%22%20method%20to%20beat%20an%0A%24O%281/%5Csqrt%7Bn%7D%29%24%20accuracy%20barrier%20inherent%20to%20earlier%20approaches.%0A%20%20We%20illustrate%20a%20second%20application%20of%20our%20new%20moment-based%20recovery%20bound%20in%0Anumerical%20linear%20algebra%3A%20by%20improving%20an%20approach%20of%20Braverman%2C%20Krishnan%2C%20and%0AMusco%20%5BSTOC%202022%5D%2C%20our%20result%20yields%20a%20faster%20algorithm%20for%20estimating%20the%0Aspectral%20density%20of%20a%20symmetric%20matrix%20up%20to%20small%20error%20in%20the%20Wasserstein%0Adistance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12385v1&entry.124074799=Read"},
{"title": "Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune\n  CNNs and Transformers", "author": "Sayed Mohammad Vakilzadeh Hatefi and Maximilian Dreyer and Reduan Achtibat and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin", "abstract": "  To solve ever more complex problems, Deep Neural Networks are scaled to\nbillions of parameters, leading to huge computational costs. An effective\napproach to reduce computational requirements and increase efficiency is to\nprune unnecessary components of these often over-parameterized networks.\nPrevious work has shown that attribution methods from the field of eXplainable\nAI serve as effective means to extract and prune the least relevant network\ncomponents in a few-shot fashion. We extend the current state by proposing to\nexplicitly optimize hyperparameters of attribution methods for the task of\npruning, and further include transformer-based networks in our analysis. Our\napproach yields higher model compression rates of large transformer- and\nconvolutional architectures (VGG, ResNet, ViT) compared to previous works,\nwhile still attaining high performance on ImageNet classification tasks. Here,\nour experiments indicate that transformers have a higher degree of\nover-parameterization compared to convolutional neural networks. Code is\navailable at\n$\\href{https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch}{\\text{this\nhttps link}}$.\n", "link": "http://arxiv.org/abs/2408.12568v1", "date": "2024-08-22", "relevancy": 2.0104, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5259}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5059}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20By%20Explaining%20Revisited%3A%20Optimizing%20Attribution%20Methods%20to%20Prune%0A%20%20CNNs%20and%20Transformers&body=Title%3A%20Pruning%20By%20Explaining%20Revisited%3A%20Optimizing%20Attribution%20Methods%20to%20Prune%0A%20%20CNNs%20and%20Transformers%0AAuthor%3A%20Sayed%20Mohammad%20Vakilzadeh%20Hatefi%20and%20Maximilian%20Dreyer%20and%20Reduan%20Achtibat%20and%20Thomas%20Wiegand%20and%20Wojciech%20Samek%20and%20Sebastian%20Lapuschkin%0AAbstract%3A%20%20%20To%20solve%20ever%20more%20complex%20problems%2C%20Deep%20Neural%20Networks%20are%20scaled%20to%0Abillions%20of%20parameters%2C%20leading%20to%20huge%20computational%20costs.%20An%20effective%0Aapproach%20to%20reduce%20computational%20requirements%20and%20increase%20efficiency%20is%20to%0Aprune%20unnecessary%20components%20of%20these%20often%20over-parameterized%20networks.%0APrevious%20work%20has%20shown%20that%20attribution%20methods%20from%20the%20field%20of%20eXplainable%0AAI%20serve%20as%20effective%20means%20to%20extract%20and%20prune%20the%20least%20relevant%20network%0Acomponents%20in%20a%20few-shot%20fashion.%20We%20extend%20the%20current%20state%20by%20proposing%20to%0Aexplicitly%20optimize%20hyperparameters%20of%20attribution%20methods%20for%20the%20task%20of%0Apruning%2C%20and%20further%20include%20transformer-based%20networks%20in%20our%20analysis.%20Our%0Aapproach%20yields%20higher%20model%20compression%20rates%20of%20large%20transformer-%20and%0Aconvolutional%20architectures%20%28VGG%2C%20ResNet%2C%20ViT%29%20compared%20to%20previous%20works%2C%0Awhile%20still%20attaining%20high%20performance%20on%20ImageNet%20classification%20tasks.%20Here%2C%0Aour%20experiments%20indicate%20that%20transformers%20have%20a%20higher%20degree%20of%0Aover-parameterization%20compared%20to%20convolutional%20neural%20networks.%20Code%20is%0Aavailable%20at%0A%24%5Chref%7Bhttps%3A//github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch%7D%7B%5Ctext%7Bthis%0Ahttps%20link%7D%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520By%2520Explaining%2520Revisited%253A%2520Optimizing%2520Attribution%2520Methods%2520to%2520Prune%250A%2520%2520CNNs%2520and%2520Transformers%26entry.906535625%3DSayed%2520Mohammad%2520Vakilzadeh%2520Hatefi%2520and%2520Maximilian%2520Dreyer%2520and%2520Reduan%2520Achtibat%2520and%2520Thomas%2520Wiegand%2520and%2520Wojciech%2520Samek%2520and%2520Sebastian%2520Lapuschkin%26entry.1292438233%3D%2520%2520To%2520solve%2520ever%2520more%2520complex%2520problems%252C%2520Deep%2520Neural%2520Networks%2520are%2520scaled%2520to%250Abillions%2520of%2520parameters%252C%2520leading%2520to%2520huge%2520computational%2520costs.%2520An%2520effective%250Aapproach%2520to%2520reduce%2520computational%2520requirements%2520and%2520increase%2520efficiency%2520is%2520to%250Aprune%2520unnecessary%2520components%2520of%2520these%2520often%2520over-parameterized%2520networks.%250APrevious%2520work%2520has%2520shown%2520that%2520attribution%2520methods%2520from%2520the%2520field%2520of%2520eXplainable%250AAI%2520serve%2520as%2520effective%2520means%2520to%2520extract%2520and%2520prune%2520the%2520least%2520relevant%2520network%250Acomponents%2520in%2520a%2520few-shot%2520fashion.%2520We%2520extend%2520the%2520current%2520state%2520by%2520proposing%2520to%250Aexplicitly%2520optimize%2520hyperparameters%2520of%2520attribution%2520methods%2520for%2520the%2520task%2520of%250Apruning%252C%2520and%2520further%2520include%2520transformer-based%2520networks%2520in%2520our%2520analysis.%2520Our%250Aapproach%2520yields%2520higher%2520model%2520compression%2520rates%2520of%2520large%2520transformer-%2520and%250Aconvolutional%2520architectures%2520%2528VGG%252C%2520ResNet%252C%2520ViT%2529%2520compared%2520to%2520previous%2520works%252C%250Awhile%2520still%2520attaining%2520high%2520performance%2520on%2520ImageNet%2520classification%2520tasks.%2520Here%252C%250Aour%2520experiments%2520indicate%2520that%2520transformers%2520have%2520a%2520higher%2520degree%2520of%250Aover-parameterization%2520compared%2520to%2520convolutional%2520neural%2520networks.%2520Code%2520is%250Aavailable%2520at%250A%2524%255Chref%257Bhttps%253A//github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch%257D%257B%255Ctext%257Bthis%250Ahttps%2520link%257D%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20By%20Explaining%20Revisited%3A%20Optimizing%20Attribution%20Methods%20to%20Prune%0A%20%20CNNs%20and%20Transformers&entry.906535625=Sayed%20Mohammad%20Vakilzadeh%20Hatefi%20and%20Maximilian%20Dreyer%20and%20Reduan%20Achtibat%20and%20Thomas%20Wiegand%20and%20Wojciech%20Samek%20and%20Sebastian%20Lapuschkin&entry.1292438233=%20%20To%20solve%20ever%20more%20complex%20problems%2C%20Deep%20Neural%20Networks%20are%20scaled%20to%0Abillions%20of%20parameters%2C%20leading%20to%20huge%20computational%20costs.%20An%20effective%0Aapproach%20to%20reduce%20computational%20requirements%20and%20increase%20efficiency%20is%20to%0Aprune%20unnecessary%20components%20of%20these%20often%20over-parameterized%20networks.%0APrevious%20work%20has%20shown%20that%20attribution%20methods%20from%20the%20field%20of%20eXplainable%0AAI%20serve%20as%20effective%20means%20to%20extract%20and%20prune%20the%20least%20relevant%20network%0Acomponents%20in%20a%20few-shot%20fashion.%20We%20extend%20the%20current%20state%20by%20proposing%20to%0Aexplicitly%20optimize%20hyperparameters%20of%20attribution%20methods%20for%20the%20task%20of%0Apruning%2C%20and%20further%20include%20transformer-based%20networks%20in%20our%20analysis.%20Our%0Aapproach%20yields%20higher%20model%20compression%20rates%20of%20large%20transformer-%20and%0Aconvolutional%20architectures%20%28VGG%2C%20ResNet%2C%20ViT%29%20compared%20to%20previous%20works%2C%0Awhile%20still%20attaining%20high%20performance%20on%20ImageNet%20classification%20tasks.%20Here%2C%0Aour%20experiments%20indicate%20that%20transformers%20have%20a%20higher%20degree%20of%0Aover-parameterization%20compared%20to%20convolutional%20neural%20networks.%20Code%20is%0Aavailable%20at%0A%24%5Chref%7Bhttps%3A//github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch%7D%7B%5Ctext%7Bthis%0Ahttps%20link%7D%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12568v1&entry.124074799=Read"},
{"title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed\n  Representations", "author": "Can Qin and Congying Xia and Krithika Ramakrishnan and Michael Ryoo and Lifu Tu and Yihao Feng and Manli Shu and Honglu Zhou and Anas Awadalla and Jun Wang and Senthil Purushwalkam and Le Xue and Yingbo Zhou and Huan Wang and Silvio Savarese and Juan Carlos Niebles and Zeyuan Chen and Ran Xu and Caiming Xiong", "abstract": "  We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of\nproducing realistic scenes from textual descriptions. Building on recent\nadvancements, such as OpenAI's Sora, we explore the latent diffusion model\n(LDM) architecture and introduce a video variational autoencoder (VidVAE).\nVidVAE compresses video data both spatially and temporally, significantly\nreducing the length of visual tokens and the computational demands associated\nwith generating long-sequence videos. To further address the computational\ncosts, we propose a divide-and-merge strategy that maintains temporal\nconsistency across video segments. Our Diffusion Transformer (DiT) model\nincorporates spatial and temporal self-attention layers, enabling robust\ngeneralization across different timeframes and aspect ratios. We have devised a\ndata processing pipeline from the very beginning and collected over 13M\nhigh-quality video-text pairs. The pipeline includes multiple steps such as\nclipping, text detection, motion estimation, aesthetics scoring, and dense\ncaptioning based on our in-house video-LLM model. Training the VidVAE and DiT\nmodels required approximately 40 and 642 H100 days, respectively. Our model\nsupports over 14-second 720p video generation in an end-to-end way and\ndemonstrates competitive performance against state-of-the-art T2V models.\n", "link": "http://arxiv.org/abs/2408.12590v1", "date": "2024-08-22", "relevancy": 2.0049, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6814}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6805}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xGen-VideoSyn-1%3A%20High-fidelity%20Text-to-Video%20Synthesis%20with%20Compressed%0A%20%20Representations&body=Title%3A%20xGen-VideoSyn-1%3A%20High-fidelity%20Text-to-Video%20Synthesis%20with%20Compressed%0A%20%20Representations%0AAuthor%3A%20Can%20Qin%20and%20Congying%20Xia%20and%20Krithika%20Ramakrishnan%20and%20Michael%20Ryoo%20and%20Lifu%20Tu%20and%20Yihao%20Feng%20and%20Manli%20Shu%20and%20Honglu%20Zhou%20and%20Anas%20Awadalla%20and%20Jun%20Wang%20and%20Senthil%20Purushwalkam%20and%20Le%20Xue%20and%20Yingbo%20Zhou%20and%20Huan%20Wang%20and%20Silvio%20Savarese%20and%20Juan%20Carlos%20Niebles%20and%20Zeyuan%20Chen%20and%20Ran%20Xu%20and%20Caiming%20Xiong%0AAbstract%3A%20%20%20We%20present%20xGen-VideoSyn-1%2C%20a%20text-to-video%20%28T2V%29%20generation%20model%20capable%20of%0Aproducing%20realistic%20scenes%20from%20textual%20descriptions.%20Building%20on%20recent%0Aadvancements%2C%20such%20as%20OpenAI%27s%20Sora%2C%20we%20explore%20the%20latent%20diffusion%20model%0A%28LDM%29%20architecture%20and%20introduce%20a%20video%20variational%20autoencoder%20%28VidVAE%29.%0AVidVAE%20compresses%20video%20data%20both%20spatially%20and%20temporally%2C%20significantly%0Areducing%20the%20length%20of%20visual%20tokens%20and%20the%20computational%20demands%20associated%0Awith%20generating%20long-sequence%20videos.%20To%20further%20address%20the%20computational%0Acosts%2C%20we%20propose%20a%20divide-and-merge%20strategy%20that%20maintains%20temporal%0Aconsistency%20across%20video%20segments.%20Our%20Diffusion%20Transformer%20%28DiT%29%20model%0Aincorporates%20spatial%20and%20temporal%20self-attention%20layers%2C%20enabling%20robust%0Ageneralization%20across%20different%20timeframes%20and%20aspect%20ratios.%20We%20have%20devised%20a%0Adata%20processing%20pipeline%20from%20the%20very%20beginning%20and%20collected%20over%2013M%0Ahigh-quality%20video-text%20pairs.%20The%20pipeline%20includes%20multiple%20steps%20such%20as%0Aclipping%2C%20text%20detection%2C%20motion%20estimation%2C%20aesthetics%20scoring%2C%20and%20dense%0Acaptioning%20based%20on%20our%20in-house%20video-LLM%20model.%20Training%20the%20VidVAE%20and%20DiT%0Amodels%20required%20approximately%2040%20and%20642%20H100%20days%2C%20respectively.%20Our%20model%0Asupports%20over%2014-second%20720p%20video%20generation%20in%20an%20end-to-end%20way%20and%0Ademonstrates%20competitive%20performance%20against%20state-of-the-art%20T2V%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxGen-VideoSyn-1%253A%2520High-fidelity%2520Text-to-Video%2520Synthesis%2520with%2520Compressed%250A%2520%2520Representations%26entry.906535625%3DCan%2520Qin%2520and%2520Congying%2520Xia%2520and%2520Krithika%2520Ramakrishnan%2520and%2520Michael%2520Ryoo%2520and%2520Lifu%2520Tu%2520and%2520Yihao%2520Feng%2520and%2520Manli%2520Shu%2520and%2520Honglu%2520Zhou%2520and%2520Anas%2520Awadalla%2520and%2520Jun%2520Wang%2520and%2520Senthil%2520Purushwalkam%2520and%2520Le%2520Xue%2520and%2520Yingbo%2520Zhou%2520and%2520Huan%2520Wang%2520and%2520Silvio%2520Savarese%2520and%2520Juan%2520Carlos%2520Niebles%2520and%2520Zeyuan%2520Chen%2520and%2520Ran%2520Xu%2520and%2520Caiming%2520Xiong%26entry.1292438233%3D%2520%2520We%2520present%2520xGen-VideoSyn-1%252C%2520a%2520text-to-video%2520%2528T2V%2529%2520generation%2520model%2520capable%2520of%250Aproducing%2520realistic%2520scenes%2520from%2520textual%2520descriptions.%2520Building%2520on%2520recent%250Aadvancements%252C%2520such%2520as%2520OpenAI%2527s%2520Sora%252C%2520we%2520explore%2520the%2520latent%2520diffusion%2520model%250A%2528LDM%2529%2520architecture%2520and%2520introduce%2520a%2520video%2520variational%2520autoencoder%2520%2528VidVAE%2529.%250AVidVAE%2520compresses%2520video%2520data%2520both%2520spatially%2520and%2520temporally%252C%2520significantly%250Areducing%2520the%2520length%2520of%2520visual%2520tokens%2520and%2520the%2520computational%2520demands%2520associated%250Awith%2520generating%2520long-sequence%2520videos.%2520To%2520further%2520address%2520the%2520computational%250Acosts%252C%2520we%2520propose%2520a%2520divide-and-merge%2520strategy%2520that%2520maintains%2520temporal%250Aconsistency%2520across%2520video%2520segments.%2520Our%2520Diffusion%2520Transformer%2520%2528DiT%2529%2520model%250Aincorporates%2520spatial%2520and%2520temporal%2520self-attention%2520layers%252C%2520enabling%2520robust%250Ageneralization%2520across%2520different%2520timeframes%2520and%2520aspect%2520ratios.%2520We%2520have%2520devised%2520a%250Adata%2520processing%2520pipeline%2520from%2520the%2520very%2520beginning%2520and%2520collected%2520over%252013M%250Ahigh-quality%2520video-text%2520pairs.%2520The%2520pipeline%2520includes%2520multiple%2520steps%2520such%2520as%250Aclipping%252C%2520text%2520detection%252C%2520motion%2520estimation%252C%2520aesthetics%2520scoring%252C%2520and%2520dense%250Acaptioning%2520based%2520on%2520our%2520in-house%2520video-LLM%2520model.%2520Training%2520the%2520VidVAE%2520and%2520DiT%250Amodels%2520required%2520approximately%252040%2520and%2520642%2520H100%2520days%252C%2520respectively.%2520Our%2520model%250Asupports%2520over%252014-second%2520720p%2520video%2520generation%2520in%2520an%2520end-to-end%2520way%2520and%250Ademonstrates%2520competitive%2520performance%2520against%2520state-of-the-art%2520T2V%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xGen-VideoSyn-1%3A%20High-fidelity%20Text-to-Video%20Synthesis%20with%20Compressed%0A%20%20Representations&entry.906535625=Can%20Qin%20and%20Congying%20Xia%20and%20Krithika%20Ramakrishnan%20and%20Michael%20Ryoo%20and%20Lifu%20Tu%20and%20Yihao%20Feng%20and%20Manli%20Shu%20and%20Honglu%20Zhou%20and%20Anas%20Awadalla%20and%20Jun%20Wang%20and%20Senthil%20Purushwalkam%20and%20Le%20Xue%20and%20Yingbo%20Zhou%20and%20Huan%20Wang%20and%20Silvio%20Savarese%20and%20Juan%20Carlos%20Niebles%20and%20Zeyuan%20Chen%20and%20Ran%20Xu%20and%20Caiming%20Xiong&entry.1292438233=%20%20We%20present%20xGen-VideoSyn-1%2C%20a%20text-to-video%20%28T2V%29%20generation%20model%20capable%20of%0Aproducing%20realistic%20scenes%20from%20textual%20descriptions.%20Building%20on%20recent%0Aadvancements%2C%20such%20as%20OpenAI%27s%20Sora%2C%20we%20explore%20the%20latent%20diffusion%20model%0A%28LDM%29%20architecture%20and%20introduce%20a%20video%20variational%20autoencoder%20%28VidVAE%29.%0AVidVAE%20compresses%20video%20data%20both%20spatially%20and%20temporally%2C%20significantly%0Areducing%20the%20length%20of%20visual%20tokens%20and%20the%20computational%20demands%20associated%0Awith%20generating%20long-sequence%20videos.%20To%20further%20address%20the%20computational%0Acosts%2C%20we%20propose%20a%20divide-and-merge%20strategy%20that%20maintains%20temporal%0Aconsistency%20across%20video%20segments.%20Our%20Diffusion%20Transformer%20%28DiT%29%20model%0Aincorporates%20spatial%20and%20temporal%20self-attention%20layers%2C%20enabling%20robust%0Ageneralization%20across%20different%20timeframes%20and%20aspect%20ratios.%20We%20have%20devised%20a%0Adata%20processing%20pipeline%20from%20the%20very%20beginning%20and%20collected%20over%2013M%0Ahigh-quality%20video-text%20pairs.%20The%20pipeline%20includes%20multiple%20steps%20such%20as%0Aclipping%2C%20text%20detection%2C%20motion%20estimation%2C%20aesthetics%20scoring%2C%20and%20dense%0Acaptioning%20based%20on%20our%20in-house%20video-LLM%20model.%20Training%20the%20VidVAE%20and%20DiT%0Amodels%20required%20approximately%2040%20and%20642%20H100%20days%2C%20respectively.%20Our%20model%0Asupports%20over%2014-second%20720p%20video%20generation%20in%20an%20end-to-end%20way%20and%0Ademonstrates%20competitive%20performance%20against%20state-of-the-art%20T2V%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12590v1&entry.124074799=Read"},
{"title": "Probabilistic Homotopy Optimization for Dynamic Motion Planning", "author": "Shayan Pardis and Matthew Chignoli and Sangbae Kim", "abstract": "  We present a homotopic approach to solving challenging, optimization-based\nmotion planning problems. The approach uses Homotopy Optimization, which,\nunlike standard continuation methods for solving homotopy problems, solves a\nsequence of constrained optimization problems rather than a sequence of\nnonlinear systems of equations. The insight behind our proposed algorithm is\nformulating the discovery of this sequence of optimization problems as a search\nproblem in a multidimensional homotopy parameter space. Our proposed algorithm,\nthe Probabilistic Homotopy Optimization algorithm, switches between solve and\nsample phases, using solutions to easy problems as initial guesses to more\nchallenging problems. We analyze how our algorithm performs in the presence of\ncommon challenges to homotopy methods, such as bifurcation, folding, and\ndisconnectedness of the homotopy solution manifold. Finally, we demonstrate its\nutility via a case study on two dynamic motion planning problems: the cart-pole\nand the MIT Humanoid.\n", "link": "http://arxiv.org/abs/2408.12490v1", "date": "2024-08-22", "relevancy": 1.9965, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5461}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4919}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Homotopy%20Optimization%20for%20Dynamic%20Motion%20Planning&body=Title%3A%20Probabilistic%20Homotopy%20Optimization%20for%20Dynamic%20Motion%20Planning%0AAuthor%3A%20Shayan%20Pardis%20and%20Matthew%20Chignoli%20and%20Sangbae%20Kim%0AAbstract%3A%20%20%20We%20present%20a%20homotopic%20approach%20to%20solving%20challenging%2C%20optimization-based%0Amotion%20planning%20problems.%20The%20approach%20uses%20Homotopy%20Optimization%2C%20which%2C%0Aunlike%20standard%20continuation%20methods%20for%20solving%20homotopy%20problems%2C%20solves%20a%0Asequence%20of%20constrained%20optimization%20problems%20rather%20than%20a%20sequence%20of%0Anonlinear%20systems%20of%20equations.%20The%20insight%20behind%20our%20proposed%20algorithm%20is%0Aformulating%20the%20discovery%20of%20this%20sequence%20of%20optimization%20problems%20as%20a%20search%0Aproblem%20in%20a%20multidimensional%20homotopy%20parameter%20space.%20Our%20proposed%20algorithm%2C%0Athe%20Probabilistic%20Homotopy%20Optimization%20algorithm%2C%20switches%20between%20solve%20and%0Asample%20phases%2C%20using%20solutions%20to%20easy%20problems%20as%20initial%20guesses%20to%20more%0Achallenging%20problems.%20We%20analyze%20how%20our%20algorithm%20performs%20in%20the%20presence%20of%0Acommon%20challenges%20to%20homotopy%20methods%2C%20such%20as%20bifurcation%2C%20folding%2C%20and%0Adisconnectedness%20of%20the%20homotopy%20solution%20manifold.%20Finally%2C%20we%20demonstrate%20its%0Autility%20via%20a%20case%20study%20on%20two%20dynamic%20motion%20planning%20problems%3A%20the%20cart-pole%0Aand%20the%20MIT%20Humanoid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Homotopy%2520Optimization%2520for%2520Dynamic%2520Motion%2520Planning%26entry.906535625%3DShayan%2520Pardis%2520and%2520Matthew%2520Chignoli%2520and%2520Sangbae%2520Kim%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520homotopic%2520approach%2520to%2520solving%2520challenging%252C%2520optimization-based%250Amotion%2520planning%2520problems.%2520The%2520approach%2520uses%2520Homotopy%2520Optimization%252C%2520which%252C%250Aunlike%2520standard%2520continuation%2520methods%2520for%2520solving%2520homotopy%2520problems%252C%2520solves%2520a%250Asequence%2520of%2520constrained%2520optimization%2520problems%2520rather%2520than%2520a%2520sequence%2520of%250Anonlinear%2520systems%2520of%2520equations.%2520The%2520insight%2520behind%2520our%2520proposed%2520algorithm%2520is%250Aformulating%2520the%2520discovery%2520of%2520this%2520sequence%2520of%2520optimization%2520problems%2520as%2520a%2520search%250Aproblem%2520in%2520a%2520multidimensional%2520homotopy%2520parameter%2520space.%2520Our%2520proposed%2520algorithm%252C%250Athe%2520Probabilistic%2520Homotopy%2520Optimization%2520algorithm%252C%2520switches%2520between%2520solve%2520and%250Asample%2520phases%252C%2520using%2520solutions%2520to%2520easy%2520problems%2520as%2520initial%2520guesses%2520to%2520more%250Achallenging%2520problems.%2520We%2520analyze%2520how%2520our%2520algorithm%2520performs%2520in%2520the%2520presence%2520of%250Acommon%2520challenges%2520to%2520homotopy%2520methods%252C%2520such%2520as%2520bifurcation%252C%2520folding%252C%2520and%250Adisconnectedness%2520of%2520the%2520homotopy%2520solution%2520manifold.%2520Finally%252C%2520we%2520demonstrate%2520its%250Autility%2520via%2520a%2520case%2520study%2520on%2520two%2520dynamic%2520motion%2520planning%2520problems%253A%2520the%2520cart-pole%250Aand%2520the%2520MIT%2520Humanoid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Homotopy%20Optimization%20for%20Dynamic%20Motion%20Planning&entry.906535625=Shayan%20Pardis%20and%20Matthew%20Chignoli%20and%20Sangbae%20Kim&entry.1292438233=%20%20We%20present%20a%20homotopic%20approach%20to%20solving%20challenging%2C%20optimization-based%0Amotion%20planning%20problems.%20The%20approach%20uses%20Homotopy%20Optimization%2C%20which%2C%0Aunlike%20standard%20continuation%20methods%20for%20solving%20homotopy%20problems%2C%20solves%20a%0Asequence%20of%20constrained%20optimization%20problems%20rather%20than%20a%20sequence%20of%0Anonlinear%20systems%20of%20equations.%20The%20insight%20behind%20our%20proposed%20algorithm%20is%0Aformulating%20the%20discovery%20of%20this%20sequence%20of%20optimization%20problems%20as%20a%20search%0Aproblem%20in%20a%20multidimensional%20homotopy%20parameter%20space.%20Our%20proposed%20algorithm%2C%0Athe%20Probabilistic%20Homotopy%20Optimization%20algorithm%2C%20switches%20between%20solve%20and%0Asample%20phases%2C%20using%20solutions%20to%20easy%20problems%20as%20initial%20guesses%20to%20more%0Achallenging%20problems.%20We%20analyze%20how%20our%20algorithm%20performs%20in%20the%20presence%20of%0Acommon%20challenges%20to%20homotopy%20methods%2C%20such%20as%20bifurcation%2C%20folding%2C%20and%0Adisconnectedness%20of%20the%20homotopy%20solution%20manifold.%20Finally%2C%20we%20demonstrate%20its%0Autility%20via%20a%20case%20study%20on%20two%20dynamic%20motion%20planning%20problems%3A%20the%20cart-pole%0Aand%20the%20MIT%20Humanoid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12490v1&entry.124074799=Read"},
{"title": "SAM-SP: Self-Prompting Makes SAM Great Again", "author": "Chunpeng Zhou and Kangjie Ning and Qianqian Shen and Sheng Zhou and Zhi Yu and Haishuai Wang", "abstract": "  The recently introduced Segment Anything Model (SAM), a Visual Foundation\nModel (VFM), has demonstrated impressive capabilities in zero-shot segmentation\ntasks across diverse natural image datasets. Despite its success, SAM\nencounters noticeably performance degradation when applied to specific domains,\nsuch as medical images. Current efforts to address this issue have involved\nfine-tuning strategies, intended to bolster the generalizability of the vanilla\nSAM. However, these approaches still predominantly necessitate the utilization\nof domain specific expert-level prompts during the evaluation phase, which\nseverely constrains the model's practicality.\n  To overcome this limitation, we introduce a novel self-prompting based\nfine-tuning approach, called SAM-SP, tailored for extending the vanilla SAM\nmodel. Specifically, SAM-SP leverages the output from the previous iteration of\nthe model itself as prompts to guide subsequent iteration of the model. This\nself-prompting module endeavors to learn how to generate useful prompts\nautonomously and alleviates the dependence on expert prompts during the\nevaluation phase, significantly broadening SAM's applicability. Additionally,\nwe integrate a self-distillation module to enhance the self-prompting process\nfurther. Extensive experiments across various domain specific datasets validate\nthe effectiveness of the proposed SAM-SP. Our SAM-SP not only alleviates the\nreliance on expert prompts but also exhibits superior segmentation performance\ncomparing to the state-of-the-art task-specific segmentation approaches, the\nvanilla SAM, and SAM-based approaches.\n", "link": "http://arxiv.org/abs/2408.12364v1", "date": "2024-08-22", "relevancy": 1.9954, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5194}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4849}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM-SP%3A%20Self-Prompting%20Makes%20SAM%20Great%20Again&body=Title%3A%20SAM-SP%3A%20Self-Prompting%20Makes%20SAM%20Great%20Again%0AAuthor%3A%20Chunpeng%20Zhou%20and%20Kangjie%20Ning%20and%20Qianqian%20Shen%20and%20Sheng%20Zhou%20and%20Zhi%20Yu%20and%20Haishuai%20Wang%0AAbstract%3A%20%20%20The%20recently%20introduced%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20Visual%20Foundation%0AModel%20%28VFM%29%2C%20has%20demonstrated%20impressive%20capabilities%20in%20zero-shot%20segmentation%0Atasks%20across%20diverse%20natural%20image%20datasets.%20Despite%20its%20success%2C%20SAM%0Aencounters%20noticeably%20performance%20degradation%20when%20applied%20to%20specific%20domains%2C%0Asuch%20as%20medical%20images.%20Current%20efforts%20to%20address%20this%20issue%20have%20involved%0Afine-tuning%20strategies%2C%20intended%20to%20bolster%20the%20generalizability%20of%20the%20vanilla%0ASAM.%20However%2C%20these%20approaches%20still%20predominantly%20necessitate%20the%20utilization%0Aof%20domain%20specific%20expert-level%20prompts%20during%20the%20evaluation%20phase%2C%20which%0Aseverely%20constrains%20the%20model%27s%20practicality.%0A%20%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20novel%20self-prompting%20based%0Afine-tuning%20approach%2C%20called%20SAM-SP%2C%20tailored%20for%20extending%20the%20vanilla%20SAM%0Amodel.%20Specifically%2C%20SAM-SP%20leverages%20the%20output%20from%20the%20previous%20iteration%20of%0Athe%20model%20itself%20as%20prompts%20to%20guide%20subsequent%20iteration%20of%20the%20model.%20This%0Aself-prompting%20module%20endeavors%20to%20learn%20how%20to%20generate%20useful%20prompts%0Aautonomously%20and%20alleviates%20the%20dependence%20on%20expert%20prompts%20during%20the%0Aevaluation%20phase%2C%20significantly%20broadening%20SAM%27s%20applicability.%20Additionally%2C%0Awe%20integrate%20a%20self-distillation%20module%20to%20enhance%20the%20self-prompting%20process%0Afurther.%20Extensive%20experiments%20across%20various%20domain%20specific%20datasets%20validate%0Athe%20effectiveness%20of%20the%20proposed%20SAM-SP.%20Our%20SAM-SP%20not%20only%20alleviates%20the%0Areliance%20on%20expert%20prompts%20but%20also%20exhibits%20superior%20segmentation%20performance%0Acomparing%20to%20the%20state-of-the-art%20task-specific%20segmentation%20approaches%2C%20the%0Avanilla%20SAM%2C%20and%20SAM-based%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM-SP%253A%2520Self-Prompting%2520Makes%2520SAM%2520Great%2520Again%26entry.906535625%3DChunpeng%2520Zhou%2520and%2520Kangjie%2520Ning%2520and%2520Qianqian%2520Shen%2520and%2520Sheng%2520Zhou%2520and%2520Zhi%2520Yu%2520and%2520Haishuai%2520Wang%26entry.1292438233%3D%2520%2520The%2520recently%2520introduced%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520a%2520Visual%2520Foundation%250AModel%2520%2528VFM%2529%252C%2520has%2520demonstrated%2520impressive%2520capabilities%2520in%2520zero-shot%2520segmentation%250Atasks%2520across%2520diverse%2520natural%2520image%2520datasets.%2520Despite%2520its%2520success%252C%2520SAM%250Aencounters%2520noticeably%2520performance%2520degradation%2520when%2520applied%2520to%2520specific%2520domains%252C%250Asuch%2520as%2520medical%2520images.%2520Current%2520efforts%2520to%2520address%2520this%2520issue%2520have%2520involved%250Afine-tuning%2520strategies%252C%2520intended%2520to%2520bolster%2520the%2520generalizability%2520of%2520the%2520vanilla%250ASAM.%2520However%252C%2520these%2520approaches%2520still%2520predominantly%2520necessitate%2520the%2520utilization%250Aof%2520domain%2520specific%2520expert-level%2520prompts%2520during%2520the%2520evaluation%2520phase%252C%2520which%250Aseverely%2520constrains%2520the%2520model%2527s%2520practicality.%250A%2520%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520novel%2520self-prompting%2520based%250Afine-tuning%2520approach%252C%2520called%2520SAM-SP%252C%2520tailored%2520for%2520extending%2520the%2520vanilla%2520SAM%250Amodel.%2520Specifically%252C%2520SAM-SP%2520leverages%2520the%2520output%2520from%2520the%2520previous%2520iteration%2520of%250Athe%2520model%2520itself%2520as%2520prompts%2520to%2520guide%2520subsequent%2520iteration%2520of%2520the%2520model.%2520This%250Aself-prompting%2520module%2520endeavors%2520to%2520learn%2520how%2520to%2520generate%2520useful%2520prompts%250Aautonomously%2520and%2520alleviates%2520the%2520dependence%2520on%2520expert%2520prompts%2520during%2520the%250Aevaluation%2520phase%252C%2520significantly%2520broadening%2520SAM%2527s%2520applicability.%2520Additionally%252C%250Awe%2520integrate%2520a%2520self-distillation%2520module%2520to%2520enhance%2520the%2520self-prompting%2520process%250Afurther.%2520Extensive%2520experiments%2520across%2520various%2520domain%2520specific%2520datasets%2520validate%250Athe%2520effectiveness%2520of%2520the%2520proposed%2520SAM-SP.%2520Our%2520SAM-SP%2520not%2520only%2520alleviates%2520the%250Areliance%2520on%2520expert%2520prompts%2520but%2520also%2520exhibits%2520superior%2520segmentation%2520performance%250Acomparing%2520to%2520the%2520state-of-the-art%2520task-specific%2520segmentation%2520approaches%252C%2520the%250Avanilla%2520SAM%252C%2520and%2520SAM-based%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM-SP%3A%20Self-Prompting%20Makes%20SAM%20Great%20Again&entry.906535625=Chunpeng%20Zhou%20and%20Kangjie%20Ning%20and%20Qianqian%20Shen%20and%20Sheng%20Zhou%20and%20Zhi%20Yu%20and%20Haishuai%20Wang&entry.1292438233=%20%20The%20recently%20introduced%20Segment%20Anything%20Model%20%28SAM%29%2C%20a%20Visual%20Foundation%0AModel%20%28VFM%29%2C%20has%20demonstrated%20impressive%20capabilities%20in%20zero-shot%20segmentation%0Atasks%20across%20diverse%20natural%20image%20datasets.%20Despite%20its%20success%2C%20SAM%0Aencounters%20noticeably%20performance%20degradation%20when%20applied%20to%20specific%20domains%2C%0Asuch%20as%20medical%20images.%20Current%20efforts%20to%20address%20this%20issue%20have%20involved%0Afine-tuning%20strategies%2C%20intended%20to%20bolster%20the%20generalizability%20of%20the%20vanilla%0ASAM.%20However%2C%20these%20approaches%20still%20predominantly%20necessitate%20the%20utilization%0Aof%20domain%20specific%20expert-level%20prompts%20during%20the%20evaluation%20phase%2C%20which%0Aseverely%20constrains%20the%20model%27s%20practicality.%0A%20%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20novel%20self-prompting%20based%0Afine-tuning%20approach%2C%20called%20SAM-SP%2C%20tailored%20for%20extending%20the%20vanilla%20SAM%0Amodel.%20Specifically%2C%20SAM-SP%20leverages%20the%20output%20from%20the%20previous%20iteration%20of%0Athe%20model%20itself%20as%20prompts%20to%20guide%20subsequent%20iteration%20of%20the%20model.%20This%0Aself-prompting%20module%20endeavors%20to%20learn%20how%20to%20generate%20useful%20prompts%0Aautonomously%20and%20alleviates%20the%20dependence%20on%20expert%20prompts%20during%20the%0Aevaluation%20phase%2C%20significantly%20broadening%20SAM%27s%20applicability.%20Additionally%2C%0Awe%20integrate%20a%20self-distillation%20module%20to%20enhance%20the%20self-prompting%20process%0Afurther.%20Extensive%20experiments%20across%20various%20domain%20specific%20datasets%20validate%0Athe%20effectiveness%20of%20the%20proposed%20SAM-SP.%20Our%20SAM-SP%20not%20only%20alleviates%20the%0Areliance%20on%20expert%20prompts%20but%20also%20exhibits%20superior%20segmentation%20performance%0Acomparing%20to%20the%20state-of-the-art%20task-specific%20segmentation%20approaches%2C%20the%0Avanilla%20SAM%2C%20and%20SAM-based%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12364v1&entry.124074799=Read"},
{"title": "Mixstyle-Entropy: Domain Generalization with Causal Intervention and\n  Perturbation", "author": "Luyao Tang and Yuxuan Yuan and Chaoqi Chen and Xinghao Ding and Yue Huang", "abstract": "  Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer.\n", "link": "http://arxiv.org/abs/2408.03608v2", "date": "2024-08-22", "relevancy": 1.9936, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5045}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5026}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixstyle-Entropy%3A%20Domain%20Generalization%20with%20Causal%20Intervention%20and%0A%20%20Perturbation&body=Title%3A%20Mixstyle-Entropy%3A%20Domain%20Generalization%20with%20Causal%20Intervention%20and%0A%20%20Perturbation%0AAuthor%3A%20Luyao%20Tang%20and%20Yuxuan%20Yuan%20and%20Chaoqi%20Chen%20and%20Xinghao%20Ding%20and%20Yue%20Huang%0AAbstract%3A%20%20%20Despite%20the%20considerable%20advancements%20achieved%20by%20deep%20neural%20networks%2C%20their%0Aperformance%20tends%20to%20degenerate%20when%20the%20test%20environment%20diverges%20from%20the%0Atraining%20ones.%20Domain%20generalization%20%28DG%29%20solves%20this%20issue%20by%20learning%0Arepresentations%20independent%20of%20domain-related%20information%2C%20thus%20facilitating%0Aextrapolation%20to%20unseen%20environments.%20Existing%20approaches%20typically%20focus%20on%0Aformulating%20tailored%20training%20objectives%20to%20extract%20shared%20features%20from%20the%0Asource%20data.%20However%2C%20the%20disjointed%20training%20and%20testing%20procedures%20may%0Acompromise%20robustness%2C%20particularly%20in%20the%20face%20of%20unforeseen%20variations%20during%0Adeployment.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20holistic%20framework%20based%20on%0Acausality%2C%20named%20InPer%2C%20designed%20to%20enhance%20model%20generalization%20by%0Aincorporating%20causal%20intervention%20during%20training%20and%20causal%20perturbation%0Aduring%20testing.%20Specifically%2C%20during%20the%20training%20phase%2C%20we%20employ%0Aentropy-based%20causal%20intervention%20%28EnIn%29%20to%20refine%20the%20selection%20of%20causal%0Avariables.%20To%20identify%20samples%20with%20anti-interference%20causal%20variables%20from%20the%0Atarget%20domain%2C%20we%20propose%20a%20novel%20metric%2C%20homeostatic%20score%2C%20through%20causal%0Aperturbation%20%28HoPer%29%20to%20construct%20a%20prototype%20classifier%20in%20test%20time.%0AExperimental%20results%20across%20multiple%20cross-domain%20tasks%20confirm%20the%20efficacy%20of%0AInPer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03608v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixstyle-Entropy%253A%2520Domain%2520Generalization%2520with%2520Causal%2520Intervention%2520and%250A%2520%2520Perturbation%26entry.906535625%3DLuyao%2520Tang%2520and%2520Yuxuan%2520Yuan%2520and%2520Chaoqi%2520Chen%2520and%2520Xinghao%2520Ding%2520and%2520Yue%2520Huang%26entry.1292438233%3D%2520%2520Despite%2520the%2520considerable%2520advancements%2520achieved%2520by%2520deep%2520neural%2520networks%252C%2520their%250Aperformance%2520tends%2520to%2520degenerate%2520when%2520the%2520test%2520environment%2520diverges%2520from%2520the%250Atraining%2520ones.%2520Domain%2520generalization%2520%2528DG%2529%2520solves%2520this%2520issue%2520by%2520learning%250Arepresentations%2520independent%2520of%2520domain-related%2520information%252C%2520thus%2520facilitating%250Aextrapolation%2520to%2520unseen%2520environments.%2520Existing%2520approaches%2520typically%2520focus%2520on%250Aformulating%2520tailored%2520training%2520objectives%2520to%2520extract%2520shared%2520features%2520from%2520the%250Asource%2520data.%2520However%252C%2520the%2520disjointed%2520training%2520and%2520testing%2520procedures%2520may%250Acompromise%2520robustness%252C%2520particularly%2520in%2520the%2520face%2520of%2520unforeseen%2520variations%2520during%250Adeployment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520and%2520holistic%2520framework%2520based%2520on%250Acausality%252C%2520named%2520InPer%252C%2520designed%2520to%2520enhance%2520model%2520generalization%2520by%250Aincorporating%2520causal%2520intervention%2520during%2520training%2520and%2520causal%2520perturbation%250Aduring%2520testing.%2520Specifically%252C%2520during%2520the%2520training%2520phase%252C%2520we%2520employ%250Aentropy-based%2520causal%2520intervention%2520%2528EnIn%2529%2520to%2520refine%2520the%2520selection%2520of%2520causal%250Avariables.%2520To%2520identify%2520samples%2520with%2520anti-interference%2520causal%2520variables%2520from%2520the%250Atarget%2520domain%252C%2520we%2520propose%2520a%2520novel%2520metric%252C%2520homeostatic%2520score%252C%2520through%2520causal%250Aperturbation%2520%2528HoPer%2529%2520to%2520construct%2520a%2520prototype%2520classifier%2520in%2520test%2520time.%250AExperimental%2520results%2520across%2520multiple%2520cross-domain%2520tasks%2520confirm%2520the%2520efficacy%2520of%250AInPer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03608v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixstyle-Entropy%3A%20Domain%20Generalization%20with%20Causal%20Intervention%20and%0A%20%20Perturbation&entry.906535625=Luyao%20Tang%20and%20Yuxuan%20Yuan%20and%20Chaoqi%20Chen%20and%20Xinghao%20Ding%20and%20Yue%20Huang&entry.1292438233=%20%20Despite%20the%20considerable%20advancements%20achieved%20by%20deep%20neural%20networks%2C%20their%0Aperformance%20tends%20to%20degenerate%20when%20the%20test%20environment%20diverges%20from%20the%0Atraining%20ones.%20Domain%20generalization%20%28DG%29%20solves%20this%20issue%20by%20learning%0Arepresentations%20independent%20of%20domain-related%20information%2C%20thus%20facilitating%0Aextrapolation%20to%20unseen%20environments.%20Existing%20approaches%20typically%20focus%20on%0Aformulating%20tailored%20training%20objectives%20to%20extract%20shared%20features%20from%20the%0Asource%20data.%20However%2C%20the%20disjointed%20training%20and%20testing%20procedures%20may%0Acompromise%20robustness%2C%20particularly%20in%20the%20face%20of%20unforeseen%20variations%20during%0Adeployment.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20and%20holistic%20framework%20based%20on%0Acausality%2C%20named%20InPer%2C%20designed%20to%20enhance%20model%20generalization%20by%0Aincorporating%20causal%20intervention%20during%20training%20and%20causal%20perturbation%0Aduring%20testing.%20Specifically%2C%20during%20the%20training%20phase%2C%20we%20employ%0Aentropy-based%20causal%20intervention%20%28EnIn%29%20to%20refine%20the%20selection%20of%20causal%0Avariables.%20To%20identify%20samples%20with%20anti-interference%20causal%20variables%20from%20the%0Atarget%20domain%2C%20we%20propose%20a%20novel%20metric%2C%20homeostatic%20score%2C%20through%20causal%0Aperturbation%20%28HoPer%29%20to%20construct%20a%20prototype%20classifier%20in%20test%20time.%0AExperimental%20results%20across%20multiple%20cross-domain%20tasks%20confirm%20the%20efficacy%20of%0AInPer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03608v2&entry.124074799=Read"},
{"title": "SiNGR: Brain Tumor Segmentation via Signed Normalized Geodesic Transform\n  Regression", "author": "Trung Dang and Huy Hoang Nguyen and Aleksei Tiulpin", "abstract": "  One of the primary challenges in brain tumor segmentation arises from the\nuncertainty of voxels close to tumor boundaries. However, the conventional\nprocess of generating ground truth segmentation masks fails to treat such\nuncertainties properly. Those \"hard labels\" with 0s and 1s conceptually\ninfluenced the majority of prior studies on brain image segmentation. As a\nresult, tumor segmentation is often solved through voxel classification. In\nthis work, we instead view this problem as a voxel-level regression, where the\nground truth represents a certainty mapping from any pixel to the border of the\ntumor. We propose a novel ground truth label transformation, which is based on\na signed geodesic transform, to capture the uncertainty in brain tumors'\nvicinity. We combine this idea with a Focal-like regression L1-loss that\nenables effective regression learning in high-dimensional output space by\nappropriately weighting voxels according to their difficulty. We thoroughly\nconduct an experimental evaluation to validate the components of our proposed\nmethod, compare it to a diverse array of state-of-the-art segmentation models,\nand show that it is architecture-agnostic. The code of our method is made\npublicly available (\\url{https://github.com/Oulu-IMEDS/SiNGR/}).\n", "link": "http://arxiv.org/abs/2405.16813v4", "date": "2024-08-22", "relevancy": 1.9801, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5095}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4851}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SiNGR%3A%20Brain%20Tumor%20Segmentation%20via%20Signed%20Normalized%20Geodesic%20Transform%0A%20%20Regression&body=Title%3A%20SiNGR%3A%20Brain%20Tumor%20Segmentation%20via%20Signed%20Normalized%20Geodesic%20Transform%0A%20%20Regression%0AAuthor%3A%20Trung%20Dang%20and%20Huy%20Hoang%20Nguyen%20and%20Aleksei%20Tiulpin%0AAbstract%3A%20%20%20One%20of%20the%20primary%20challenges%20in%20brain%20tumor%20segmentation%20arises%20from%20the%0Auncertainty%20of%20voxels%20close%20to%20tumor%20boundaries.%20However%2C%20the%20conventional%0Aprocess%20of%20generating%20ground%20truth%20segmentation%20masks%20fails%20to%20treat%20such%0Auncertainties%20properly.%20Those%20%22hard%20labels%22%20with%200s%20and%201s%20conceptually%0Ainfluenced%20the%20majority%20of%20prior%20studies%20on%20brain%20image%20segmentation.%20As%20a%0Aresult%2C%20tumor%20segmentation%20is%20often%20solved%20through%20voxel%20classification.%20In%0Athis%20work%2C%20we%20instead%20view%20this%20problem%20as%20a%20voxel-level%20regression%2C%20where%20the%0Aground%20truth%20represents%20a%20certainty%20mapping%20from%20any%20pixel%20to%20the%20border%20of%20the%0Atumor.%20We%20propose%20a%20novel%20ground%20truth%20label%20transformation%2C%20which%20is%20based%20on%0Aa%20signed%20geodesic%20transform%2C%20to%20capture%20the%20uncertainty%20in%20brain%20tumors%27%0Avicinity.%20We%20combine%20this%20idea%20with%20a%20Focal-like%20regression%20L1-loss%20that%0Aenables%20effective%20regression%20learning%20in%20high-dimensional%20output%20space%20by%0Aappropriately%20weighting%20voxels%20according%20to%20their%20difficulty.%20We%20thoroughly%0Aconduct%20an%20experimental%20evaluation%20to%20validate%20the%20components%20of%20our%20proposed%0Amethod%2C%20compare%20it%20to%20a%20diverse%20array%20of%20state-of-the-art%20segmentation%20models%2C%0Aand%20show%20that%20it%20is%20architecture-agnostic.%20The%20code%20of%20our%20method%20is%20made%0Apublicly%20available%20%28%5Curl%7Bhttps%3A//github.com/Oulu-IMEDS/SiNGR/%7D%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16813v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSiNGR%253A%2520Brain%2520Tumor%2520Segmentation%2520via%2520Signed%2520Normalized%2520Geodesic%2520Transform%250A%2520%2520Regression%26entry.906535625%3DTrung%2520Dang%2520and%2520Huy%2520Hoang%2520Nguyen%2520and%2520Aleksei%2520Tiulpin%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520primary%2520challenges%2520in%2520brain%2520tumor%2520segmentation%2520arises%2520from%2520the%250Auncertainty%2520of%2520voxels%2520close%2520to%2520tumor%2520boundaries.%2520However%252C%2520the%2520conventional%250Aprocess%2520of%2520generating%2520ground%2520truth%2520segmentation%2520masks%2520fails%2520to%2520treat%2520such%250Auncertainties%2520properly.%2520Those%2520%2522hard%2520labels%2522%2520with%25200s%2520and%25201s%2520conceptually%250Ainfluenced%2520the%2520majority%2520of%2520prior%2520studies%2520on%2520brain%2520image%2520segmentation.%2520As%2520a%250Aresult%252C%2520tumor%2520segmentation%2520is%2520often%2520solved%2520through%2520voxel%2520classification.%2520In%250Athis%2520work%252C%2520we%2520instead%2520view%2520this%2520problem%2520as%2520a%2520voxel-level%2520regression%252C%2520where%2520the%250Aground%2520truth%2520represents%2520a%2520certainty%2520mapping%2520from%2520any%2520pixel%2520to%2520the%2520border%2520of%2520the%250Atumor.%2520We%2520propose%2520a%2520novel%2520ground%2520truth%2520label%2520transformation%252C%2520which%2520is%2520based%2520on%250Aa%2520signed%2520geodesic%2520transform%252C%2520to%2520capture%2520the%2520uncertainty%2520in%2520brain%2520tumors%2527%250Avicinity.%2520We%2520combine%2520this%2520idea%2520with%2520a%2520Focal-like%2520regression%2520L1-loss%2520that%250Aenables%2520effective%2520regression%2520learning%2520in%2520high-dimensional%2520output%2520space%2520by%250Aappropriately%2520weighting%2520voxels%2520according%2520to%2520their%2520difficulty.%2520We%2520thoroughly%250Aconduct%2520an%2520experimental%2520evaluation%2520to%2520validate%2520the%2520components%2520of%2520our%2520proposed%250Amethod%252C%2520compare%2520it%2520to%2520a%2520diverse%2520array%2520of%2520state-of-the-art%2520segmentation%2520models%252C%250Aand%2520show%2520that%2520it%2520is%2520architecture-agnostic.%2520The%2520code%2520of%2520our%2520method%2520is%2520made%250Apublicly%2520available%2520%2528%255Curl%257Bhttps%253A//github.com/Oulu-IMEDS/SiNGR/%257D%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16813v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SiNGR%3A%20Brain%20Tumor%20Segmentation%20via%20Signed%20Normalized%20Geodesic%20Transform%0A%20%20Regression&entry.906535625=Trung%20Dang%20and%20Huy%20Hoang%20Nguyen%20and%20Aleksei%20Tiulpin&entry.1292438233=%20%20One%20of%20the%20primary%20challenges%20in%20brain%20tumor%20segmentation%20arises%20from%20the%0Auncertainty%20of%20voxels%20close%20to%20tumor%20boundaries.%20However%2C%20the%20conventional%0Aprocess%20of%20generating%20ground%20truth%20segmentation%20masks%20fails%20to%20treat%20such%0Auncertainties%20properly.%20Those%20%22hard%20labels%22%20with%200s%20and%201s%20conceptually%0Ainfluenced%20the%20majority%20of%20prior%20studies%20on%20brain%20image%20segmentation.%20As%20a%0Aresult%2C%20tumor%20segmentation%20is%20often%20solved%20through%20voxel%20classification.%20In%0Athis%20work%2C%20we%20instead%20view%20this%20problem%20as%20a%20voxel-level%20regression%2C%20where%20the%0Aground%20truth%20represents%20a%20certainty%20mapping%20from%20any%20pixel%20to%20the%20border%20of%20the%0Atumor.%20We%20propose%20a%20novel%20ground%20truth%20label%20transformation%2C%20which%20is%20based%20on%0Aa%20signed%20geodesic%20transform%2C%20to%20capture%20the%20uncertainty%20in%20brain%20tumors%27%0Avicinity.%20We%20combine%20this%20idea%20with%20a%20Focal-like%20regression%20L1-loss%20that%0Aenables%20effective%20regression%20learning%20in%20high-dimensional%20output%20space%20by%0Aappropriately%20weighting%20voxels%20according%20to%20their%20difficulty.%20We%20thoroughly%0Aconduct%20an%20experimental%20evaluation%20to%20validate%20the%20components%20of%20our%20proposed%0Amethod%2C%20compare%20it%20to%20a%20diverse%20array%20of%20state-of-the-art%20segmentation%20models%2C%0Aand%20show%20that%20it%20is%20architecture-agnostic.%20The%20code%20of%20our%20method%20is%20made%0Apublicly%20available%20%28%5Curl%7Bhttps%3A//github.com/Oulu-IMEDS/SiNGR/%7D%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16813v4&entry.124074799=Read"},
{"title": "Multi-Source Knowledge-Based Hybrid Neural Framework for Time Series\n  Representation Learning", "author": "Sagar Srinivas Sakhinana and Krishna Sai Sudhir Aripirala and Shivam Gupta and Venkataramana Runkana", "abstract": "  Accurately predicting the behavior of complex dynamical systems,\ncharacterized by high-dimensional multivariate time series(MTS) in\ninterconnected sensor networks, is crucial for informed decision-making in\nvarious applications to minimize risk. While graph forecasting networks(GFNs)\nare ideal for forecasting MTS data that exhibit spatio-temporal dependencies,\nprior works rely solely on the domain-specific knowledge of time-series\nvariables inter-relationships to model the nonlinear dynamics, neglecting\ninherent relational structural dependencies among the variables within the MTS\ndata. In contrast, contemporary works infer relational structures from MTS data\nbut neglect domain-specific knowledge. The proposed hybrid architecture\naddresses these limitations by combining both domain-specific knowledge and\nimplicit knowledge of the relational structure underlying the MTS data using\nKnowledge-Based Compositional Generalization. The hybrid architecture shows\npromising results on multiple benchmark datasets, outperforming\nstate-of-the-art forecasting methods. Additionally, the architecture models the\ntime varying uncertainty of multi-horizon forecasts.\n", "link": "http://arxiv.org/abs/2408.12409v1", "date": "2024-08-22", "relevancy": 1.9713, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5506}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.497}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Source%20Knowledge-Based%20Hybrid%20Neural%20Framework%20for%20Time%20Series%0A%20%20Representation%20Learning&body=Title%3A%20Multi-Source%20Knowledge-Based%20Hybrid%20Neural%20Framework%20for%20Time%20Series%0A%20%20Representation%20Learning%0AAuthor%3A%20Sagar%20Srinivas%20Sakhinana%20and%20Krishna%20Sai%20Sudhir%20Aripirala%20and%20Shivam%20Gupta%20and%20Venkataramana%20Runkana%0AAbstract%3A%20%20%20Accurately%20predicting%20the%20behavior%20of%20complex%20dynamical%20systems%2C%0Acharacterized%20by%20high-dimensional%20multivariate%20time%20series%28MTS%29%20in%0Ainterconnected%20sensor%20networks%2C%20is%20crucial%20for%20informed%20decision-making%20in%0Avarious%20applications%20to%20minimize%20risk.%20While%20graph%20forecasting%20networks%28GFNs%29%0Aare%20ideal%20for%20forecasting%20MTS%20data%20that%20exhibit%20spatio-temporal%20dependencies%2C%0Aprior%20works%20rely%20solely%20on%20the%20domain-specific%20knowledge%20of%20time-series%0Avariables%20inter-relationships%20to%20model%20the%20nonlinear%20dynamics%2C%20neglecting%0Ainherent%20relational%20structural%20dependencies%20among%20the%20variables%20within%20the%20MTS%0Adata.%20In%20contrast%2C%20contemporary%20works%20infer%20relational%20structures%20from%20MTS%20data%0Abut%20neglect%20domain-specific%20knowledge.%20The%20proposed%20hybrid%20architecture%0Aaddresses%20these%20limitations%20by%20combining%20both%20domain-specific%20knowledge%20and%0Aimplicit%20knowledge%20of%20the%20relational%20structure%20underlying%20the%20MTS%20data%20using%0AKnowledge-Based%20Compositional%20Generalization.%20The%20hybrid%20architecture%20shows%0Apromising%20results%20on%20multiple%20benchmark%20datasets%2C%20outperforming%0Astate-of-the-art%20forecasting%20methods.%20Additionally%2C%20the%20architecture%20models%20the%0Atime%20varying%20uncertainty%20of%20multi-horizon%20forecasts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Source%2520Knowledge-Based%2520Hybrid%2520Neural%2520Framework%2520for%2520Time%2520Series%250A%2520%2520Representation%2520Learning%26entry.906535625%3DSagar%2520Srinivas%2520Sakhinana%2520and%2520Krishna%2520Sai%2520Sudhir%2520Aripirala%2520and%2520Shivam%2520Gupta%2520and%2520Venkataramana%2520Runkana%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520the%2520behavior%2520of%2520complex%2520dynamical%2520systems%252C%250Acharacterized%2520by%2520high-dimensional%2520multivariate%2520time%2520series%2528MTS%2529%2520in%250Ainterconnected%2520sensor%2520networks%252C%2520is%2520crucial%2520for%2520informed%2520decision-making%2520in%250Avarious%2520applications%2520to%2520minimize%2520risk.%2520While%2520graph%2520forecasting%2520networks%2528GFNs%2529%250Aare%2520ideal%2520for%2520forecasting%2520MTS%2520data%2520that%2520exhibit%2520spatio-temporal%2520dependencies%252C%250Aprior%2520works%2520rely%2520solely%2520on%2520the%2520domain-specific%2520knowledge%2520of%2520time-series%250Avariables%2520inter-relationships%2520to%2520model%2520the%2520nonlinear%2520dynamics%252C%2520neglecting%250Ainherent%2520relational%2520structural%2520dependencies%2520among%2520the%2520variables%2520within%2520the%2520MTS%250Adata.%2520In%2520contrast%252C%2520contemporary%2520works%2520infer%2520relational%2520structures%2520from%2520MTS%2520data%250Abut%2520neglect%2520domain-specific%2520knowledge.%2520The%2520proposed%2520hybrid%2520architecture%250Aaddresses%2520these%2520limitations%2520by%2520combining%2520both%2520domain-specific%2520knowledge%2520and%250Aimplicit%2520knowledge%2520of%2520the%2520relational%2520structure%2520underlying%2520the%2520MTS%2520data%2520using%250AKnowledge-Based%2520Compositional%2520Generalization.%2520The%2520hybrid%2520architecture%2520shows%250Apromising%2520results%2520on%2520multiple%2520benchmark%2520datasets%252C%2520outperforming%250Astate-of-the-art%2520forecasting%2520methods.%2520Additionally%252C%2520the%2520architecture%2520models%2520the%250Atime%2520varying%2520uncertainty%2520of%2520multi-horizon%2520forecasts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Source%20Knowledge-Based%20Hybrid%20Neural%20Framework%20for%20Time%20Series%0A%20%20Representation%20Learning&entry.906535625=Sagar%20Srinivas%20Sakhinana%20and%20Krishna%20Sai%20Sudhir%20Aripirala%20and%20Shivam%20Gupta%20and%20Venkataramana%20Runkana&entry.1292438233=%20%20Accurately%20predicting%20the%20behavior%20of%20complex%20dynamical%20systems%2C%0Acharacterized%20by%20high-dimensional%20multivariate%20time%20series%28MTS%29%20in%0Ainterconnected%20sensor%20networks%2C%20is%20crucial%20for%20informed%20decision-making%20in%0Avarious%20applications%20to%20minimize%20risk.%20While%20graph%20forecasting%20networks%28GFNs%29%0Aare%20ideal%20for%20forecasting%20MTS%20data%20that%20exhibit%20spatio-temporal%20dependencies%2C%0Aprior%20works%20rely%20solely%20on%20the%20domain-specific%20knowledge%20of%20time-series%0Avariables%20inter-relationships%20to%20model%20the%20nonlinear%20dynamics%2C%20neglecting%0Ainherent%20relational%20structural%20dependencies%20among%20the%20variables%20within%20the%20MTS%0Adata.%20In%20contrast%2C%20contemporary%20works%20infer%20relational%20structures%20from%20MTS%20data%0Abut%20neglect%20domain-specific%20knowledge.%20The%20proposed%20hybrid%20architecture%0Aaddresses%20these%20limitations%20by%20combining%20both%20domain-specific%20knowledge%20and%0Aimplicit%20knowledge%20of%20the%20relational%20structure%20underlying%20the%20MTS%20data%20using%0AKnowledge-Based%20Compositional%20Generalization.%20The%20hybrid%20architecture%20shows%0Apromising%20results%20on%20multiple%20benchmark%20datasets%2C%20outperforming%0Astate-of-the-art%20forecasting%20methods.%20Additionally%2C%20the%20architecture%20models%20the%0Atime%20varying%20uncertainty%20of%20multi-horizon%20forecasts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12409v1&entry.124074799=Read"},
{"title": "Robust Principal Component Analysis via Discriminant Sample Weight\n  Learning", "author": "Yingzhuo Deng and Ke Hu and Bo Li and Yao Zhang", "abstract": "  Principal component analysis (PCA) is a classical feature extraction method,\nbut it may be adversely affected by outliers, resulting in inaccurate learning\nof the projection matrix. This paper proposes a robust method to estimate both\nthe data mean and the PCA projection matrix by learning discriminant sample\nweights from data containing outliers. Each sample in the dataset is assigned a\nweight, and the proposed algorithm iteratively learns the weights, the mean,\nand the projection matrix, respectively. Specifically, when the mean and the\nprojection matrix are available, via fine-grained analysis of outliers, a\nweight for each sample is learned hierarchically so that outliers have small\nweights while normal samples have large weights. With the learned weights\navailable, a weighted optimization problem is solved to estimate both the data\nmean and the projection matrix. Because the learned weights discriminate\noutliers from normal samples, the adverse influence of outliers is mitigated\ndue to the corresponding small weights. Experiments on toy data, UCI dataset,\nand face dataset demonstrate the effectiveness of the proposed method in\nestimating the mean and the projection matrix from the data containing\noutliers.\n", "link": "http://arxiv.org/abs/2408.12366v1", "date": "2024-08-22", "relevancy": 1.9595, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5028}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4899}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Principal%20Component%20Analysis%20via%20Discriminant%20Sample%20Weight%0A%20%20Learning&body=Title%3A%20Robust%20Principal%20Component%20Analysis%20via%20Discriminant%20Sample%20Weight%0A%20%20Learning%0AAuthor%3A%20Yingzhuo%20Deng%20and%20Ke%20Hu%20and%20Bo%20Li%20and%20Yao%20Zhang%0AAbstract%3A%20%20%20Principal%20component%20analysis%20%28PCA%29%20is%20a%20classical%20feature%20extraction%20method%2C%0Abut%20it%20may%20be%20adversely%20affected%20by%20outliers%2C%20resulting%20in%20inaccurate%20learning%0Aof%20the%20projection%20matrix.%20This%20paper%20proposes%20a%20robust%20method%20to%20estimate%20both%0Athe%20data%20mean%20and%20the%20PCA%20projection%20matrix%20by%20learning%20discriminant%20sample%0Aweights%20from%20data%20containing%20outliers.%20Each%20sample%20in%20the%20dataset%20is%20assigned%20a%0Aweight%2C%20and%20the%20proposed%20algorithm%20iteratively%20learns%20the%20weights%2C%20the%20mean%2C%0Aand%20the%20projection%20matrix%2C%20respectively.%20Specifically%2C%20when%20the%20mean%20and%20the%0Aprojection%20matrix%20are%20available%2C%20via%20fine-grained%20analysis%20of%20outliers%2C%20a%0Aweight%20for%20each%20sample%20is%20learned%20hierarchically%20so%20that%20outliers%20have%20small%0Aweights%20while%20normal%20samples%20have%20large%20weights.%20With%20the%20learned%20weights%0Aavailable%2C%20a%20weighted%20optimization%20problem%20is%20solved%20to%20estimate%20both%20the%20data%0Amean%20and%20the%20projection%20matrix.%20Because%20the%20learned%20weights%20discriminate%0Aoutliers%20from%20normal%20samples%2C%20the%20adverse%20influence%20of%20outliers%20is%20mitigated%0Adue%20to%20the%20corresponding%20small%20weights.%20Experiments%20on%20toy%20data%2C%20UCI%20dataset%2C%0Aand%20face%20dataset%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20in%0Aestimating%20the%20mean%20and%20the%20projection%20matrix%20from%20the%20data%20containing%0Aoutliers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Principal%2520Component%2520Analysis%2520via%2520Discriminant%2520Sample%2520Weight%250A%2520%2520Learning%26entry.906535625%3DYingzhuo%2520Deng%2520and%2520Ke%2520Hu%2520and%2520Bo%2520Li%2520and%2520Yao%2520Zhang%26entry.1292438233%3D%2520%2520Principal%2520component%2520analysis%2520%2528PCA%2529%2520is%2520a%2520classical%2520feature%2520extraction%2520method%252C%250Abut%2520it%2520may%2520be%2520adversely%2520affected%2520by%2520outliers%252C%2520resulting%2520in%2520inaccurate%2520learning%250Aof%2520the%2520projection%2520matrix.%2520This%2520paper%2520proposes%2520a%2520robust%2520method%2520to%2520estimate%2520both%250Athe%2520data%2520mean%2520and%2520the%2520PCA%2520projection%2520matrix%2520by%2520learning%2520discriminant%2520sample%250Aweights%2520from%2520data%2520containing%2520outliers.%2520Each%2520sample%2520in%2520the%2520dataset%2520is%2520assigned%2520a%250Aweight%252C%2520and%2520the%2520proposed%2520algorithm%2520iteratively%2520learns%2520the%2520weights%252C%2520the%2520mean%252C%250Aand%2520the%2520projection%2520matrix%252C%2520respectively.%2520Specifically%252C%2520when%2520the%2520mean%2520and%2520the%250Aprojection%2520matrix%2520are%2520available%252C%2520via%2520fine-grained%2520analysis%2520of%2520outliers%252C%2520a%250Aweight%2520for%2520each%2520sample%2520is%2520learned%2520hierarchically%2520so%2520that%2520outliers%2520have%2520small%250Aweights%2520while%2520normal%2520samples%2520have%2520large%2520weights.%2520With%2520the%2520learned%2520weights%250Aavailable%252C%2520a%2520weighted%2520optimization%2520problem%2520is%2520solved%2520to%2520estimate%2520both%2520the%2520data%250Amean%2520and%2520the%2520projection%2520matrix.%2520Because%2520the%2520learned%2520weights%2520discriminate%250Aoutliers%2520from%2520normal%2520samples%252C%2520the%2520adverse%2520influence%2520of%2520outliers%2520is%2520mitigated%250Adue%2520to%2520the%2520corresponding%2520small%2520weights.%2520Experiments%2520on%2520toy%2520data%252C%2520UCI%2520dataset%252C%250Aand%2520face%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520in%250Aestimating%2520the%2520mean%2520and%2520the%2520projection%2520matrix%2520from%2520the%2520data%2520containing%250Aoutliers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Principal%20Component%20Analysis%20via%20Discriminant%20Sample%20Weight%0A%20%20Learning&entry.906535625=Yingzhuo%20Deng%20and%20Ke%20Hu%20and%20Bo%20Li%20and%20Yao%20Zhang&entry.1292438233=%20%20Principal%20component%20analysis%20%28PCA%29%20is%20a%20classical%20feature%20extraction%20method%2C%0Abut%20it%20may%20be%20adversely%20affected%20by%20outliers%2C%20resulting%20in%20inaccurate%20learning%0Aof%20the%20projection%20matrix.%20This%20paper%20proposes%20a%20robust%20method%20to%20estimate%20both%0Athe%20data%20mean%20and%20the%20PCA%20projection%20matrix%20by%20learning%20discriminant%20sample%0Aweights%20from%20data%20containing%20outliers.%20Each%20sample%20in%20the%20dataset%20is%20assigned%20a%0Aweight%2C%20and%20the%20proposed%20algorithm%20iteratively%20learns%20the%20weights%2C%20the%20mean%2C%0Aand%20the%20projection%20matrix%2C%20respectively.%20Specifically%2C%20when%20the%20mean%20and%20the%0Aprojection%20matrix%20are%20available%2C%20via%20fine-grained%20analysis%20of%20outliers%2C%20a%0Aweight%20for%20each%20sample%20is%20learned%20hierarchically%20so%20that%20outliers%20have%20small%0Aweights%20while%20normal%20samples%20have%20large%20weights.%20With%20the%20learned%20weights%0Aavailable%2C%20a%20weighted%20optimization%20problem%20is%20solved%20to%20estimate%20both%20the%20data%0Amean%20and%20the%20projection%20matrix.%20Because%20the%20learned%20weights%20discriminate%0Aoutliers%20from%20normal%20samples%2C%20the%20adverse%20influence%20of%20outliers%20is%20mitigated%0Adue%20to%20the%20corresponding%20small%20weights.%20Experiments%20on%20toy%20data%2C%20UCI%20dataset%2C%0Aand%20face%20dataset%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20in%0Aestimating%20the%20mean%20and%20the%20projection%20matrix%20from%20the%20data%20containing%0Aoutliers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12366v1&entry.124074799=Read"},
{"title": "Relaxed Rotational Equivariance via $G$-Biases in Vision", "author": "Zhiqiang Wu and Licheng Sun and Yingjie Liu and Jian Yang and Hanlin Dong and Shing-Ho J. Lin and Xuan Tang and Jinpeng Mi and Bo Jin and Xian Wei", "abstract": "  Group Equivariant Convolution (GConv) can effectively handle rotational\nsymmetry data. They assume uniform and strict rotational symmetry across all\nfeatures, as the transformations under the specific group. However, real-world\ndata rarely conforms to strict rotational symmetry commonly referred to as\nRotational Symmetry-Breaking in the system or dataset, making GConv unable to\nadapt effectively to this phenomenon. Motivated by this, we propose a simple\nbut highly effective method to address this problem, which utilizes a set of\nlearnable biases called the $G$-Biases under the group order to break strict\ngroup constraints and achieve \\textbf{R}elaxed \\textbf{R}otational\n\\textbf{E}quivarant \\textbf{Conv}olution (RREConv). We conduct extensive\nexperiments to validate Relaxed Rotational Equivariance on rotational symmetry\ngroups $\\mathcal{C}_n$ (e.g. $\\mathcal{C}_2$, $\\mathcal{C}_4$, and\n$\\mathcal{C}_6$ groups). Further experiments demonstrate that our proposed\nRREConv-based methods achieve excellent performance, compared to existing\nGConv-based methods in classification and detection tasks on natural image\ndatasets.\n", "link": "http://arxiv.org/abs/2408.12454v1", "date": "2024-08-22", "relevancy": 1.9575, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4977}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4925}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relaxed%20Rotational%20Equivariance%20via%20%24G%24-Biases%20in%20Vision&body=Title%3A%20Relaxed%20Rotational%20Equivariance%20via%20%24G%24-Biases%20in%20Vision%0AAuthor%3A%20Zhiqiang%20Wu%20and%20Licheng%20Sun%20and%20Yingjie%20Liu%20and%20Jian%20Yang%20and%20Hanlin%20Dong%20and%20Shing-Ho%20J.%20Lin%20and%20Xuan%20Tang%20and%20Jinpeng%20Mi%20and%20Bo%20Jin%20and%20Xian%20Wei%0AAbstract%3A%20%20%20Group%20Equivariant%20Convolution%20%28GConv%29%20can%20effectively%20handle%20rotational%0Asymmetry%20data.%20They%20assume%20uniform%20and%20strict%20rotational%20symmetry%20across%20all%0Afeatures%2C%20as%20the%20transformations%20under%20the%20specific%20group.%20However%2C%20real-world%0Adata%20rarely%20conforms%20to%20strict%20rotational%20symmetry%20commonly%20referred%20to%20as%0ARotational%20Symmetry-Breaking%20in%20the%20system%20or%20dataset%2C%20making%20GConv%20unable%20to%0Aadapt%20effectively%20to%20this%20phenomenon.%20Motivated%20by%20this%2C%20we%20propose%20a%20simple%0Abut%20highly%20effective%20method%20to%20address%20this%20problem%2C%20which%20utilizes%20a%20set%20of%0Alearnable%20biases%20called%20the%20%24G%24-Biases%20under%20the%20group%20order%20to%20break%20strict%0Agroup%20constraints%20and%20achieve%20%5Ctextbf%7BR%7Delaxed%20%5Ctextbf%7BR%7Dotational%0A%5Ctextbf%7BE%7Dquivarant%20%5Ctextbf%7BConv%7Dolution%20%28RREConv%29.%20We%20conduct%20extensive%0Aexperiments%20to%20validate%20Relaxed%20Rotational%20Equivariance%20on%20rotational%20symmetry%0Agroups%20%24%5Cmathcal%7BC%7D_n%24%20%28e.g.%20%24%5Cmathcal%7BC%7D_2%24%2C%20%24%5Cmathcal%7BC%7D_4%24%2C%20and%0A%24%5Cmathcal%7BC%7D_6%24%20groups%29.%20Further%20experiments%20demonstrate%20that%20our%20proposed%0ARREConv-based%20methods%20achieve%20excellent%20performance%2C%20compared%20to%20existing%0AGConv-based%20methods%20in%20classification%20and%20detection%20tasks%20on%20natural%20image%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelaxed%2520Rotational%2520Equivariance%2520via%2520%2524G%2524-Biases%2520in%2520Vision%26entry.906535625%3DZhiqiang%2520Wu%2520and%2520Licheng%2520Sun%2520and%2520Yingjie%2520Liu%2520and%2520Jian%2520Yang%2520and%2520Hanlin%2520Dong%2520and%2520Shing-Ho%2520J.%2520Lin%2520and%2520Xuan%2520Tang%2520and%2520Jinpeng%2520Mi%2520and%2520Bo%2520Jin%2520and%2520Xian%2520Wei%26entry.1292438233%3D%2520%2520Group%2520Equivariant%2520Convolution%2520%2528GConv%2529%2520can%2520effectively%2520handle%2520rotational%250Asymmetry%2520data.%2520They%2520assume%2520uniform%2520and%2520strict%2520rotational%2520symmetry%2520across%2520all%250Afeatures%252C%2520as%2520the%2520transformations%2520under%2520the%2520specific%2520group.%2520However%252C%2520real-world%250Adata%2520rarely%2520conforms%2520to%2520strict%2520rotational%2520symmetry%2520commonly%2520referred%2520to%2520as%250ARotational%2520Symmetry-Breaking%2520in%2520the%2520system%2520or%2520dataset%252C%2520making%2520GConv%2520unable%2520to%250Aadapt%2520effectively%2520to%2520this%2520phenomenon.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520simple%250Abut%2520highly%2520effective%2520method%2520to%2520address%2520this%2520problem%252C%2520which%2520utilizes%2520a%2520set%2520of%250Alearnable%2520biases%2520called%2520the%2520%2524G%2524-Biases%2520under%2520the%2520group%2520order%2520to%2520break%2520strict%250Agroup%2520constraints%2520and%2520achieve%2520%255Ctextbf%257BR%257Delaxed%2520%255Ctextbf%257BR%257Dotational%250A%255Ctextbf%257BE%257Dquivarant%2520%255Ctextbf%257BConv%257Dolution%2520%2528RREConv%2529.%2520We%2520conduct%2520extensive%250Aexperiments%2520to%2520validate%2520Relaxed%2520Rotational%2520Equivariance%2520on%2520rotational%2520symmetry%250Agroups%2520%2524%255Cmathcal%257BC%257D_n%2524%2520%2528e.g.%2520%2524%255Cmathcal%257BC%257D_2%2524%252C%2520%2524%255Cmathcal%257BC%257D_4%2524%252C%2520and%250A%2524%255Cmathcal%257BC%257D_6%2524%2520groups%2529.%2520Further%2520experiments%2520demonstrate%2520that%2520our%2520proposed%250ARREConv-based%2520methods%2520achieve%2520excellent%2520performance%252C%2520compared%2520to%2520existing%250AGConv-based%2520methods%2520in%2520classification%2520and%2520detection%2520tasks%2520on%2520natural%2520image%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relaxed%20Rotational%20Equivariance%20via%20%24G%24-Biases%20in%20Vision&entry.906535625=Zhiqiang%20Wu%20and%20Licheng%20Sun%20and%20Yingjie%20Liu%20and%20Jian%20Yang%20and%20Hanlin%20Dong%20and%20Shing-Ho%20J.%20Lin%20and%20Xuan%20Tang%20and%20Jinpeng%20Mi%20and%20Bo%20Jin%20and%20Xian%20Wei&entry.1292438233=%20%20Group%20Equivariant%20Convolution%20%28GConv%29%20can%20effectively%20handle%20rotational%0Asymmetry%20data.%20They%20assume%20uniform%20and%20strict%20rotational%20symmetry%20across%20all%0Afeatures%2C%20as%20the%20transformations%20under%20the%20specific%20group.%20However%2C%20real-world%0Adata%20rarely%20conforms%20to%20strict%20rotational%20symmetry%20commonly%20referred%20to%20as%0ARotational%20Symmetry-Breaking%20in%20the%20system%20or%20dataset%2C%20making%20GConv%20unable%20to%0Aadapt%20effectively%20to%20this%20phenomenon.%20Motivated%20by%20this%2C%20we%20propose%20a%20simple%0Abut%20highly%20effective%20method%20to%20address%20this%20problem%2C%20which%20utilizes%20a%20set%20of%0Alearnable%20biases%20called%20the%20%24G%24-Biases%20under%20the%20group%20order%20to%20break%20strict%0Agroup%20constraints%20and%20achieve%20%5Ctextbf%7BR%7Delaxed%20%5Ctextbf%7BR%7Dotational%0A%5Ctextbf%7BE%7Dquivarant%20%5Ctextbf%7BConv%7Dolution%20%28RREConv%29.%20We%20conduct%20extensive%0Aexperiments%20to%20validate%20Relaxed%20Rotational%20Equivariance%20on%20rotational%20symmetry%0Agroups%20%24%5Cmathcal%7BC%7D_n%24%20%28e.g.%20%24%5Cmathcal%7BC%7D_2%24%2C%20%24%5Cmathcal%7BC%7D_4%24%2C%20and%0A%24%5Cmathcal%7BC%7D_6%24%20groups%29.%20Further%20experiments%20demonstrate%20that%20our%20proposed%0ARREConv-based%20methods%20achieve%20excellent%20performance%2C%20compared%20to%20existing%0AGConv-based%20methods%20in%20classification%20and%20detection%20tasks%20on%20natural%20image%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12454v1&entry.124074799=Read"},
{"title": "Automatic Organ and Pan-cancer Segmentation in Abdomen CT: the FLARE\n  2023 Challenge", "author": "Jun Ma and Yao Zhang and Song Gu and Cheng Ge and Ershuai Wang and Qin Zhou and Ziyan Huang and Pengju Lyu and Jian He and Bo Wang", "abstract": "  Organ and cancer segmentation in abdomen Computed Tomography (CT) scans is\nthe prerequisite for precise cancer diagnosis and treatment. Most existing\nbenchmarks and algorithms are tailored to specific cancer types, limiting their\nability to provide comprehensive cancer analysis. This work presents the first\ninternational competition on abdominal organ and pan-cancer segmentation by\nproviding a large-scale and diverse dataset, including 4650 CT scans with\nvarious cancer types from over 40 medical centers. The winning team established\na new state-of-the-art with a deep learning-based cascaded framework, achieving\naverage Dice Similarity Coefficient scores of 92.3% for organs and 64.9% for\nlesions on the hidden multi-national testing set. The dataset and code of top\nteams are publicly available, offering a benchmark platform to drive further\ninnovations https://codalab.lisn.upsaclay.fr/competitions/12239.\n", "link": "http://arxiv.org/abs/2408.12534v1", "date": "2024-08-22", "relevancy": 1.952, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5037}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4788}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Organ%20and%20Pan-cancer%20Segmentation%20in%20Abdomen%20CT%3A%20the%20FLARE%0A%20%202023%20Challenge&body=Title%3A%20Automatic%20Organ%20and%20Pan-cancer%20Segmentation%20in%20Abdomen%20CT%3A%20the%20FLARE%0A%20%202023%20Challenge%0AAuthor%3A%20Jun%20Ma%20and%20Yao%20Zhang%20and%20Song%20Gu%20and%20Cheng%20Ge%20and%20Ershuai%20Wang%20and%20Qin%20Zhou%20and%20Ziyan%20Huang%20and%20Pengju%20Lyu%20and%20Jian%20He%20and%20Bo%20Wang%0AAbstract%3A%20%20%20Organ%20and%20cancer%20segmentation%20in%20abdomen%20Computed%20Tomography%20%28CT%29%20scans%20is%0Athe%20prerequisite%20for%20precise%20cancer%20diagnosis%20and%20treatment.%20Most%20existing%0Abenchmarks%20and%20algorithms%20are%20tailored%20to%20specific%20cancer%20types%2C%20limiting%20their%0Aability%20to%20provide%20comprehensive%20cancer%20analysis.%20This%20work%20presents%20the%20first%0Ainternational%20competition%20on%20abdominal%20organ%20and%20pan-cancer%20segmentation%20by%0Aproviding%20a%20large-scale%20and%20diverse%20dataset%2C%20including%204650%20CT%20scans%20with%0Avarious%20cancer%20types%20from%20over%2040%20medical%20centers.%20The%20winning%20team%20established%0Aa%20new%20state-of-the-art%20with%20a%20deep%20learning-based%20cascaded%20framework%2C%20achieving%0Aaverage%20Dice%20Similarity%20Coefficient%20scores%20of%2092.3%25%20for%20organs%20and%2064.9%25%20for%0Alesions%20on%20the%20hidden%20multi-national%20testing%20set.%20The%20dataset%20and%20code%20of%20top%0Ateams%20are%20publicly%20available%2C%20offering%20a%20benchmark%20platform%20to%20drive%20further%0Ainnovations%20https%3A//codalab.lisn.upsaclay.fr/competitions/12239.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Organ%2520and%2520Pan-cancer%2520Segmentation%2520in%2520Abdomen%2520CT%253A%2520the%2520FLARE%250A%2520%25202023%2520Challenge%26entry.906535625%3DJun%2520Ma%2520and%2520Yao%2520Zhang%2520and%2520Song%2520Gu%2520and%2520Cheng%2520Ge%2520and%2520Ershuai%2520Wang%2520and%2520Qin%2520Zhou%2520and%2520Ziyan%2520Huang%2520and%2520Pengju%2520Lyu%2520and%2520Jian%2520He%2520and%2520Bo%2520Wang%26entry.1292438233%3D%2520%2520Organ%2520and%2520cancer%2520segmentation%2520in%2520abdomen%2520Computed%2520Tomography%2520%2528CT%2529%2520scans%2520is%250Athe%2520prerequisite%2520for%2520precise%2520cancer%2520diagnosis%2520and%2520treatment.%2520Most%2520existing%250Abenchmarks%2520and%2520algorithms%2520are%2520tailored%2520to%2520specific%2520cancer%2520types%252C%2520limiting%2520their%250Aability%2520to%2520provide%2520comprehensive%2520cancer%2520analysis.%2520This%2520work%2520presents%2520the%2520first%250Ainternational%2520competition%2520on%2520abdominal%2520organ%2520and%2520pan-cancer%2520segmentation%2520by%250Aproviding%2520a%2520large-scale%2520and%2520diverse%2520dataset%252C%2520including%25204650%2520CT%2520scans%2520with%250Avarious%2520cancer%2520types%2520from%2520over%252040%2520medical%2520centers.%2520The%2520winning%2520team%2520established%250Aa%2520new%2520state-of-the-art%2520with%2520a%2520deep%2520learning-based%2520cascaded%2520framework%252C%2520achieving%250Aaverage%2520Dice%2520Similarity%2520Coefficient%2520scores%2520of%252092.3%2525%2520for%2520organs%2520and%252064.9%2525%2520for%250Alesions%2520on%2520the%2520hidden%2520multi-national%2520testing%2520set.%2520The%2520dataset%2520and%2520code%2520of%2520top%250Ateams%2520are%2520publicly%2520available%252C%2520offering%2520a%2520benchmark%2520platform%2520to%2520drive%2520further%250Ainnovations%2520https%253A//codalab.lisn.upsaclay.fr/competitions/12239.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Organ%20and%20Pan-cancer%20Segmentation%20in%20Abdomen%20CT%3A%20the%20FLARE%0A%20%202023%20Challenge&entry.906535625=Jun%20Ma%20and%20Yao%20Zhang%20and%20Song%20Gu%20and%20Cheng%20Ge%20and%20Ershuai%20Wang%20and%20Qin%20Zhou%20and%20Ziyan%20Huang%20and%20Pengju%20Lyu%20and%20Jian%20He%20and%20Bo%20Wang&entry.1292438233=%20%20Organ%20and%20cancer%20segmentation%20in%20abdomen%20Computed%20Tomography%20%28CT%29%20scans%20is%0Athe%20prerequisite%20for%20precise%20cancer%20diagnosis%20and%20treatment.%20Most%20existing%0Abenchmarks%20and%20algorithms%20are%20tailored%20to%20specific%20cancer%20types%2C%20limiting%20their%0Aability%20to%20provide%20comprehensive%20cancer%20analysis.%20This%20work%20presents%20the%20first%0Ainternational%20competition%20on%20abdominal%20organ%20and%20pan-cancer%20segmentation%20by%0Aproviding%20a%20large-scale%20and%20diverse%20dataset%2C%20including%204650%20CT%20scans%20with%0Avarious%20cancer%20types%20from%20over%2040%20medical%20centers.%20The%20winning%20team%20established%0Aa%20new%20state-of-the-art%20with%20a%20deep%20learning-based%20cascaded%20framework%2C%20achieving%0Aaverage%20Dice%20Similarity%20Coefficient%20scores%20of%2092.3%25%20for%20organs%20and%2064.9%25%20for%0Alesions%20on%20the%20hidden%20multi-national%20testing%20set.%20The%20dataset%20and%20code%20of%20top%0Ateams%20are%20publicly%20available%2C%20offering%20a%20benchmark%20platform%20to%20drive%20further%0Ainnovations%20https%3A//codalab.lisn.upsaclay.fr/competitions/12239.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12534v1&entry.124074799=Read"},
{"title": "The 2nd Solution for LSVOS Challenge RVOS Track: Spatial-temporal\n  Refinement for Consistent Semantic Segmentation", "author": "Tuyen Tran", "abstract": "  Referring Video Object Segmentation (RVOS) is a challenging task due to its\nrequirement for temporal understanding. Due to the obstacle of computational\ncomplexity, many state-of-the-art models are trained on short time intervals.\nDuring testing, while these models can effectively process information over\nshort time steps, they struggle to maintain consistent perception over\nprolonged time sequences, leading to inconsistencies in the resulting semantic\nsegmentation masks. To address this challenge, we take a step further in this\nwork by leveraging the tracking capabilities of the newly introduced Segment\nAnything Model version 2 (SAM-v2) to enhance the temporal consistency of the\nreferring object segmentation model. Our method achieved a score of 60.40\n\\mathcal{J\\text{\\&}F} on the test set of the MeViS dataset, placing 2nd place\nin the final ranking of the RVOS Track at the ECCV 2024 LSVOS Challenge.\n", "link": "http://arxiv.org/abs/2408.12447v1", "date": "2024-08-22", "relevancy": 1.9504, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4995}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.481}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%202nd%20Solution%20for%20LSVOS%20Challenge%20RVOS%20Track%3A%20Spatial-temporal%0A%20%20Refinement%20for%20Consistent%20Semantic%20Segmentation&body=Title%3A%20The%202nd%20Solution%20for%20LSVOS%20Challenge%20RVOS%20Track%3A%20Spatial-temporal%0A%20%20Refinement%20for%20Consistent%20Semantic%20Segmentation%0AAuthor%3A%20Tuyen%20Tran%0AAbstract%3A%20%20%20Referring%20Video%20Object%20Segmentation%20%28RVOS%29%20is%20a%20challenging%20task%20due%20to%20its%0Arequirement%20for%20temporal%20understanding.%20Due%20to%20the%20obstacle%20of%20computational%0Acomplexity%2C%20many%20state-of-the-art%20models%20are%20trained%20on%20short%20time%20intervals.%0ADuring%20testing%2C%20while%20these%20models%20can%20effectively%20process%20information%20over%0Ashort%20time%20steps%2C%20they%20struggle%20to%20maintain%20consistent%20perception%20over%0Aprolonged%20time%20sequences%2C%20leading%20to%20inconsistencies%20in%20the%20resulting%20semantic%0Asegmentation%20masks.%20To%20address%20this%20challenge%2C%20we%20take%20a%20step%20further%20in%20this%0Awork%20by%20leveraging%20the%20tracking%20capabilities%20of%20the%20newly%20introduced%20Segment%0AAnything%20Model%20version%202%20%28SAM-v2%29%20to%20enhance%20the%20temporal%20consistency%20of%20the%0Areferring%20object%20segmentation%20model.%20Our%20method%20achieved%20a%20score%20of%2060.40%0A%5Cmathcal%7BJ%5Ctext%7B%5C%26%7DF%7D%20on%20the%20test%20set%20of%20the%20MeViS%20dataset%2C%20placing%202nd%20place%0Ain%20the%20final%20ranking%20of%20the%20RVOS%20Track%20at%20the%20ECCV%202024%20LSVOS%20Challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%25202nd%2520Solution%2520for%2520LSVOS%2520Challenge%2520RVOS%2520Track%253A%2520Spatial-temporal%250A%2520%2520Refinement%2520for%2520Consistent%2520Semantic%2520Segmentation%26entry.906535625%3DTuyen%2520Tran%26entry.1292438233%3D%2520%2520Referring%2520Video%2520Object%2520Segmentation%2520%2528RVOS%2529%2520is%2520a%2520challenging%2520task%2520due%2520to%2520its%250Arequirement%2520for%2520temporal%2520understanding.%2520Due%2520to%2520the%2520obstacle%2520of%2520computational%250Acomplexity%252C%2520many%2520state-of-the-art%2520models%2520are%2520trained%2520on%2520short%2520time%2520intervals.%250ADuring%2520testing%252C%2520while%2520these%2520models%2520can%2520effectively%2520process%2520information%2520over%250Ashort%2520time%2520steps%252C%2520they%2520struggle%2520to%2520maintain%2520consistent%2520perception%2520over%250Aprolonged%2520time%2520sequences%252C%2520leading%2520to%2520inconsistencies%2520in%2520the%2520resulting%2520semantic%250Asegmentation%2520masks.%2520To%2520address%2520this%2520challenge%252C%2520we%2520take%2520a%2520step%2520further%2520in%2520this%250Awork%2520by%2520leveraging%2520the%2520tracking%2520capabilities%2520of%2520the%2520newly%2520introduced%2520Segment%250AAnything%2520Model%2520version%25202%2520%2528SAM-v2%2529%2520to%2520enhance%2520the%2520temporal%2520consistency%2520of%2520the%250Areferring%2520object%2520segmentation%2520model.%2520Our%2520method%2520achieved%2520a%2520score%2520of%252060.40%250A%255Cmathcal%257BJ%255Ctext%257B%255C%2526%257DF%257D%2520on%2520the%2520test%2520set%2520of%2520the%2520MeViS%2520dataset%252C%2520placing%25202nd%2520place%250Ain%2520the%2520final%2520ranking%2520of%2520the%2520RVOS%2520Track%2520at%2520the%2520ECCV%25202024%2520LSVOS%2520Challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%202nd%20Solution%20for%20LSVOS%20Challenge%20RVOS%20Track%3A%20Spatial-temporal%0A%20%20Refinement%20for%20Consistent%20Semantic%20Segmentation&entry.906535625=Tuyen%20Tran&entry.1292438233=%20%20Referring%20Video%20Object%20Segmentation%20%28RVOS%29%20is%20a%20challenging%20task%20due%20to%20its%0Arequirement%20for%20temporal%20understanding.%20Due%20to%20the%20obstacle%20of%20computational%0Acomplexity%2C%20many%20state-of-the-art%20models%20are%20trained%20on%20short%20time%20intervals.%0ADuring%20testing%2C%20while%20these%20models%20can%20effectively%20process%20information%20over%0Ashort%20time%20steps%2C%20they%20struggle%20to%20maintain%20consistent%20perception%20over%0Aprolonged%20time%20sequences%2C%20leading%20to%20inconsistencies%20in%20the%20resulting%20semantic%0Asegmentation%20masks.%20To%20address%20this%20challenge%2C%20we%20take%20a%20step%20further%20in%20this%0Awork%20by%20leveraging%20the%20tracking%20capabilities%20of%20the%20newly%20introduced%20Segment%0AAnything%20Model%20version%202%20%28SAM-v2%29%20to%20enhance%20the%20temporal%20consistency%20of%20the%0Areferring%20object%20segmentation%20model.%20Our%20method%20achieved%20a%20score%20of%2060.40%0A%5Cmathcal%7BJ%5Ctext%7B%5C%26%7DF%7D%20on%20the%20test%20set%20of%20the%20MeViS%20dataset%2C%20placing%202nd%20place%0Ain%20the%20final%20ranking%20of%20the%20RVOS%20Track%20at%20the%20ECCV%202024%20LSVOS%20Challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12447v1&entry.124074799=Read"},
{"title": "Language Agents as Optimizable Graphs", "author": "Mingchen Zhuge and Wenyi Wang and Louis Kirsch and Francesco Faccio and Dmitrii Khizbullin and J\u00fcrgen Schmidhuber", "abstract": "  Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm.\n", "link": "http://arxiv.org/abs/2402.16823v3", "date": "2024-08-22", "relevancy": 1.95, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5345}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4833}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Agents%20as%20Optimizable%20Graphs&body=Title%3A%20Language%20Agents%20as%20Optimizable%20Graphs%0AAuthor%3A%20Mingchen%20Zhuge%20and%20Wenyi%20Wang%20and%20Louis%20Kirsch%20and%20Francesco%20Faccio%20and%20Dmitrii%20Khizbullin%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Various%20human-designed%20prompt%20engineering%20techniques%20have%20been%20proposed%20to%0Aimprove%20problem%20solvers%20based%20on%20Large%20Language%20Models%20%28LLMs%29%2C%20yielding%20many%0Adisparate%20code%20bases.%20We%20unify%20these%20approaches%20by%20describing%20LLM-based%20agents%0Aas%20computational%20graphs.%20The%20nodes%20implement%20functions%20to%20process%20multimodal%0Adata%20or%20query%20LLMs%2C%20and%20the%20edges%20describe%20the%20information%20flow%20between%0Aoperations.%20Graphs%20can%20be%20recursively%20combined%20into%20larger%20composite%20graphs%0Arepresenting%20hierarchies%20of%20inter-agent%20collaboration%20%28where%20edges%20connect%0Aoperations%20of%20different%20agents%29.%20Our%20novel%20automatic%20graph%20optimizers%20%281%29%0Arefine%20node-level%20LLM%20prompts%20%28node%20optimization%29%20and%20%282%29%20improve%20agent%0Aorchestration%20by%20changing%20graph%20connectivity%20%28edge%20optimization%29.%20Experiments%0Ademonstrate%20that%20our%20framework%20can%20be%20used%20to%20efficiently%20develop%2C%20integrate%2C%0Aand%20automatically%20improve%20various%20LLM%20agents.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/metauto-ai/gptswarm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16823v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Agents%2520as%2520Optimizable%2520Graphs%26entry.906535625%3DMingchen%2520Zhuge%2520and%2520Wenyi%2520Wang%2520and%2520Louis%2520Kirsch%2520and%2520Francesco%2520Faccio%2520and%2520Dmitrii%2520Khizbullin%2520and%2520J%25C3%25BCrgen%2520Schmidhuber%26entry.1292438233%3D%2520%2520Various%2520human-designed%2520prompt%2520engineering%2520techniques%2520have%2520been%2520proposed%2520to%250Aimprove%2520problem%2520solvers%2520based%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520yielding%2520many%250Adisparate%2520code%2520bases.%2520We%2520unify%2520these%2520approaches%2520by%2520describing%2520LLM-based%2520agents%250Aas%2520computational%2520graphs.%2520The%2520nodes%2520implement%2520functions%2520to%2520process%2520multimodal%250Adata%2520or%2520query%2520LLMs%252C%2520and%2520the%2520edges%2520describe%2520the%2520information%2520flow%2520between%250Aoperations.%2520Graphs%2520can%2520be%2520recursively%2520combined%2520into%2520larger%2520composite%2520graphs%250Arepresenting%2520hierarchies%2520of%2520inter-agent%2520collaboration%2520%2528where%2520edges%2520connect%250Aoperations%2520of%2520different%2520agents%2529.%2520Our%2520novel%2520automatic%2520graph%2520optimizers%2520%25281%2529%250Arefine%2520node-level%2520LLM%2520prompts%2520%2528node%2520optimization%2529%2520and%2520%25282%2529%2520improve%2520agent%250Aorchestration%2520by%2520changing%2520graph%2520connectivity%2520%2528edge%2520optimization%2529.%2520Experiments%250Ademonstrate%2520that%2520our%2520framework%2520can%2520be%2520used%2520to%2520efficiently%2520develop%252C%2520integrate%252C%250Aand%2520automatically%2520improve%2520various%2520LLM%2520agents.%2520The%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/metauto-ai/gptswarm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16823v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Agents%20as%20Optimizable%20Graphs&entry.906535625=Mingchen%20Zhuge%20and%20Wenyi%20Wang%20and%20Louis%20Kirsch%20and%20Francesco%20Faccio%20and%20Dmitrii%20Khizbullin%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Various%20human-designed%20prompt%20engineering%20techniques%20have%20been%20proposed%20to%0Aimprove%20problem%20solvers%20based%20on%20Large%20Language%20Models%20%28LLMs%29%2C%20yielding%20many%0Adisparate%20code%20bases.%20We%20unify%20these%20approaches%20by%20describing%20LLM-based%20agents%0Aas%20computational%20graphs.%20The%20nodes%20implement%20functions%20to%20process%20multimodal%0Adata%20or%20query%20LLMs%2C%20and%20the%20edges%20describe%20the%20information%20flow%20between%0Aoperations.%20Graphs%20can%20be%20recursively%20combined%20into%20larger%20composite%20graphs%0Arepresenting%20hierarchies%20of%20inter-agent%20collaboration%20%28where%20edges%20connect%0Aoperations%20of%20different%20agents%29.%20Our%20novel%20automatic%20graph%20optimizers%20%281%29%0Arefine%20node-level%20LLM%20prompts%20%28node%20optimization%29%20and%20%282%29%20improve%20agent%0Aorchestration%20by%20changing%20graph%20connectivity%20%28edge%20optimization%29.%20Experiments%0Ademonstrate%20that%20our%20framework%20can%20be%20used%20to%20efficiently%20develop%2C%20integrate%2C%0Aand%20automatically%20improve%20various%20LLM%20agents.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/metauto-ai/gptswarm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16823v3&entry.124074799=Read"},
{"title": "Enhancing Uncertainty Communication in Time Series Predictions: Insights\n  and Recommendations", "author": "Apoorva Karagappa and Pawandeep Kaur Betz and Jonas Gilg and Moritz Zeumer and Andreas Gerndt and Bernhard Preim", "abstract": "  As the world increasingly relies on mathematical models for forecasts in\ndifferent areas, effective communication of uncertainty in time series\npredictions is important for informed decision making. This study explores how\nusers estimate probabilistic uncertainty in time series predictions under\ndifferent variants of line charts depicting uncertainty. It examines the role\nof individual characteristics and the influence of user-reported metrics on\nuncertainty estimations. By addressing these aspects, this paper aims to\nenhance the understanding of uncertainty visualization and for improving\ncommunication in time series forecast visualizations and the design of\nprediction data dashboards.As the world increasingly relies on mathematical\nmodels for forecasts in different areas, effective communication of uncertainty\nin time series predictions is important for informed decision making. This\nstudy explores how users estimate probabilistic uncertainty in time series\npredictions under different variants of line charts depicting uncertainty. It\nexamines the role of individual characteristics and the influence of\nuser-reported metrics on uncertainty estimations. By addressing these aspects,\nthis paper aims to enhance the understanding of uncertainty visualization and\nfor improving communication in time series forecast visualizations and the\ndesign of prediction data dashboards.\n", "link": "http://arxiv.org/abs/2408.12365v1", "date": "2024-08-22", "relevancy": 1.9491, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.493}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4849}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Uncertainty%20Communication%20in%20Time%20Series%20Predictions%3A%20Insights%0A%20%20and%20Recommendations&body=Title%3A%20Enhancing%20Uncertainty%20Communication%20in%20Time%20Series%20Predictions%3A%20Insights%0A%20%20and%20Recommendations%0AAuthor%3A%20Apoorva%20Karagappa%20and%20Pawandeep%20Kaur%20Betz%20and%20Jonas%20Gilg%20and%20Moritz%20Zeumer%20and%20Andreas%20Gerndt%20and%20Bernhard%20Preim%0AAbstract%3A%20%20%20As%20the%20world%20increasingly%20relies%20on%20mathematical%20models%20for%20forecasts%20in%0Adifferent%20areas%2C%20effective%20communication%20of%20uncertainty%20in%20time%20series%0Apredictions%20is%20important%20for%20informed%20decision%20making.%20This%20study%20explores%20how%0Ausers%20estimate%20probabilistic%20uncertainty%20in%20time%20series%20predictions%20under%0Adifferent%20variants%20of%20line%20charts%20depicting%20uncertainty.%20It%20examines%20the%20role%0Aof%20individual%20characteristics%20and%20the%20influence%20of%20user-reported%20metrics%20on%0Auncertainty%20estimations.%20By%20addressing%20these%20aspects%2C%20this%20paper%20aims%20to%0Aenhance%20the%20understanding%20of%20uncertainty%20visualization%20and%20for%20improving%0Acommunication%20in%20time%20series%20forecast%20visualizations%20and%20the%20design%20of%0Aprediction%20data%20dashboards.As%20the%20world%20increasingly%20relies%20on%20mathematical%0Amodels%20for%20forecasts%20in%20different%20areas%2C%20effective%20communication%20of%20uncertainty%0Ain%20time%20series%20predictions%20is%20important%20for%20informed%20decision%20making.%20This%0Astudy%20explores%20how%20users%20estimate%20probabilistic%20uncertainty%20in%20time%20series%0Apredictions%20under%20different%20variants%20of%20line%20charts%20depicting%20uncertainty.%20It%0Aexamines%20the%20role%20of%20individual%20characteristics%20and%20the%20influence%20of%0Auser-reported%20metrics%20on%20uncertainty%20estimations.%20By%20addressing%20these%20aspects%2C%0Athis%20paper%20aims%20to%20enhance%20the%20understanding%20of%20uncertainty%20visualization%20and%0Afor%20improving%20communication%20in%20time%20series%20forecast%20visualizations%20and%20the%0Adesign%20of%20prediction%20data%20dashboards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Uncertainty%2520Communication%2520in%2520Time%2520Series%2520Predictions%253A%2520Insights%250A%2520%2520and%2520Recommendations%26entry.906535625%3DApoorva%2520Karagappa%2520and%2520Pawandeep%2520Kaur%2520Betz%2520and%2520Jonas%2520Gilg%2520and%2520Moritz%2520Zeumer%2520and%2520Andreas%2520Gerndt%2520and%2520Bernhard%2520Preim%26entry.1292438233%3D%2520%2520As%2520the%2520world%2520increasingly%2520relies%2520on%2520mathematical%2520models%2520for%2520forecasts%2520in%250Adifferent%2520areas%252C%2520effective%2520communication%2520of%2520uncertainty%2520in%2520time%2520series%250Apredictions%2520is%2520important%2520for%2520informed%2520decision%2520making.%2520This%2520study%2520explores%2520how%250Ausers%2520estimate%2520probabilistic%2520uncertainty%2520in%2520time%2520series%2520predictions%2520under%250Adifferent%2520variants%2520of%2520line%2520charts%2520depicting%2520uncertainty.%2520It%2520examines%2520the%2520role%250Aof%2520individual%2520characteristics%2520and%2520the%2520influence%2520of%2520user-reported%2520metrics%2520on%250Auncertainty%2520estimations.%2520By%2520addressing%2520these%2520aspects%252C%2520this%2520paper%2520aims%2520to%250Aenhance%2520the%2520understanding%2520of%2520uncertainty%2520visualization%2520and%2520for%2520improving%250Acommunication%2520in%2520time%2520series%2520forecast%2520visualizations%2520and%2520the%2520design%2520of%250Aprediction%2520data%2520dashboards.As%2520the%2520world%2520increasingly%2520relies%2520on%2520mathematical%250Amodels%2520for%2520forecasts%2520in%2520different%2520areas%252C%2520effective%2520communication%2520of%2520uncertainty%250Ain%2520time%2520series%2520predictions%2520is%2520important%2520for%2520informed%2520decision%2520making.%2520This%250Astudy%2520explores%2520how%2520users%2520estimate%2520probabilistic%2520uncertainty%2520in%2520time%2520series%250Apredictions%2520under%2520different%2520variants%2520of%2520line%2520charts%2520depicting%2520uncertainty.%2520It%250Aexamines%2520the%2520role%2520of%2520individual%2520characteristics%2520and%2520the%2520influence%2520of%250Auser-reported%2520metrics%2520on%2520uncertainty%2520estimations.%2520By%2520addressing%2520these%2520aspects%252C%250Athis%2520paper%2520aims%2520to%2520enhance%2520the%2520understanding%2520of%2520uncertainty%2520visualization%2520and%250Afor%2520improving%2520communication%2520in%2520time%2520series%2520forecast%2520visualizations%2520and%2520the%250Adesign%2520of%2520prediction%2520data%2520dashboards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Uncertainty%20Communication%20in%20Time%20Series%20Predictions%3A%20Insights%0A%20%20and%20Recommendations&entry.906535625=Apoorva%20Karagappa%20and%20Pawandeep%20Kaur%20Betz%20and%20Jonas%20Gilg%20and%20Moritz%20Zeumer%20and%20Andreas%20Gerndt%20and%20Bernhard%20Preim&entry.1292438233=%20%20As%20the%20world%20increasingly%20relies%20on%20mathematical%20models%20for%20forecasts%20in%0Adifferent%20areas%2C%20effective%20communication%20of%20uncertainty%20in%20time%20series%0Apredictions%20is%20important%20for%20informed%20decision%20making.%20This%20study%20explores%20how%0Ausers%20estimate%20probabilistic%20uncertainty%20in%20time%20series%20predictions%20under%0Adifferent%20variants%20of%20line%20charts%20depicting%20uncertainty.%20It%20examines%20the%20role%0Aof%20individual%20characteristics%20and%20the%20influence%20of%20user-reported%20metrics%20on%0Auncertainty%20estimations.%20By%20addressing%20these%20aspects%2C%20this%20paper%20aims%20to%0Aenhance%20the%20understanding%20of%20uncertainty%20visualization%20and%20for%20improving%0Acommunication%20in%20time%20series%20forecast%20visualizations%20and%20the%20design%20of%0Aprediction%20data%20dashboards.As%20the%20world%20increasingly%20relies%20on%20mathematical%0Amodels%20for%20forecasts%20in%20different%20areas%2C%20effective%20communication%20of%20uncertainty%0Ain%20time%20series%20predictions%20is%20important%20for%20informed%20decision%20making.%20This%0Astudy%20explores%20how%20users%20estimate%20probabilistic%20uncertainty%20in%20time%20series%0Apredictions%20under%20different%20variants%20of%20line%20charts%20depicting%20uncertainty.%20It%0Aexamines%20the%20role%20of%20individual%20characteristics%20and%20the%20influence%20of%0Auser-reported%20metrics%20on%20uncertainty%20estimations.%20By%20addressing%20these%20aspects%2C%0Athis%20paper%20aims%20to%20enhance%20the%20understanding%20of%20uncertainty%20visualization%20and%0Afor%20improving%20communication%20in%20time%20series%20forecast%20visualizations%20and%20the%0Adesign%20of%20prediction%20data%20dashboards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12365v1&entry.124074799=Read"},
{"title": "Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese", "author": "Khang T. Doan and Bao G. Huynh and Dung T. Hoang and Thuc D. Pham and Nhat H. Pham and Quan T. M. Nguyen and Bang Q. Vo and Suong N. Hoang", "abstract": "  In this report, we introduce Vintern-1B, a reliable 1-billion-parameters\nmultimodal large language model (MLLM) for Vietnamese language tasks. By\nintegrating the Qwen2-0.5B-Instruct language model with the\nInternViT-300M-448px visual model, Vintern-1B is optimized for a range of\napplications, including optical character recognition (OCR), document\nextraction, and general question-answering in Vietnamese context. The model is\nfine-tuned on an extensive dataset of over 3 million image-question-answer\npairs, achieving robust performance and reliable results across multiple\nVietnamese language benchmarks like OpenViVQA and ViTextVQA. Vintern-1B is\nsmall enough to fit into various on-device applications easily. Additionally,\nwe have open-sourced several Vietnamese vision question answering (VQA)\ndatasets for text and diagrams, created with Gemini 1.5 Flash. Our models are\navailable at: https://huggingface.co/5CD-AI/Vintern-1B-v2.\n", "link": "http://arxiv.org/abs/2408.12480v1", "date": "2024-08-22", "relevancy": 1.9436, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4932}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4863}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vintern-1B%3A%20An%20Efficient%20Multimodal%20Large%20Language%20Model%20for%20Vietnamese&body=Title%3A%20Vintern-1B%3A%20An%20Efficient%20Multimodal%20Large%20Language%20Model%20for%20Vietnamese%0AAuthor%3A%20Khang%20T.%20Doan%20and%20Bao%20G.%20Huynh%20and%20Dung%20T.%20Hoang%20and%20Thuc%20D.%20Pham%20and%20Nhat%20H.%20Pham%20and%20Quan%20T.%20M.%20Nguyen%20and%20Bang%20Q.%20Vo%20and%20Suong%20N.%20Hoang%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20introduce%20Vintern-1B%2C%20a%20reliable%201-billion-parameters%0Amultimodal%20large%20language%20model%20%28MLLM%29%20for%20Vietnamese%20language%20tasks.%20By%0Aintegrating%20the%20Qwen2-0.5B-Instruct%20language%20model%20with%20the%0AInternViT-300M-448px%20visual%20model%2C%20Vintern-1B%20is%20optimized%20for%20a%20range%20of%0Aapplications%2C%20including%20optical%20character%20recognition%20%28OCR%29%2C%20document%0Aextraction%2C%20and%20general%20question-answering%20in%20Vietnamese%20context.%20The%20model%20is%0Afine-tuned%20on%20an%20extensive%20dataset%20of%20over%203%20million%20image-question-answer%0Apairs%2C%20achieving%20robust%20performance%20and%20reliable%20results%20across%20multiple%0AVietnamese%20language%20benchmarks%20like%20OpenViVQA%20and%20ViTextVQA.%20Vintern-1B%20is%0Asmall%20enough%20to%20fit%20into%20various%20on-device%20applications%20easily.%20Additionally%2C%0Awe%20have%20open-sourced%20several%20Vietnamese%20vision%20question%20answering%20%28VQA%29%0Adatasets%20for%20text%20and%20diagrams%2C%20created%20with%20Gemini%201.5%20Flash.%20Our%20models%20are%0Aavailable%20at%3A%20https%3A//huggingface.co/5CD-AI/Vintern-1B-v2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVintern-1B%253A%2520An%2520Efficient%2520Multimodal%2520Large%2520Language%2520Model%2520for%2520Vietnamese%26entry.906535625%3DKhang%2520T.%2520Doan%2520and%2520Bao%2520G.%2520Huynh%2520and%2520Dung%2520T.%2520Hoang%2520and%2520Thuc%2520D.%2520Pham%2520and%2520Nhat%2520H.%2520Pham%2520and%2520Quan%2520T.%2520M.%2520Nguyen%2520and%2520Bang%2520Q.%2520Vo%2520and%2520Suong%2520N.%2520Hoang%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520introduce%2520Vintern-1B%252C%2520a%2520reliable%25201-billion-parameters%250Amultimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520for%2520Vietnamese%2520language%2520tasks.%2520By%250Aintegrating%2520the%2520Qwen2-0.5B-Instruct%2520language%2520model%2520with%2520the%250AInternViT-300M-448px%2520visual%2520model%252C%2520Vintern-1B%2520is%2520optimized%2520for%2520a%2520range%2520of%250Aapplications%252C%2520including%2520optical%2520character%2520recognition%2520%2528OCR%2529%252C%2520document%250Aextraction%252C%2520and%2520general%2520question-answering%2520in%2520Vietnamese%2520context.%2520The%2520model%2520is%250Afine-tuned%2520on%2520an%2520extensive%2520dataset%2520of%2520over%25203%2520million%2520image-question-answer%250Apairs%252C%2520achieving%2520robust%2520performance%2520and%2520reliable%2520results%2520across%2520multiple%250AVietnamese%2520language%2520benchmarks%2520like%2520OpenViVQA%2520and%2520ViTextVQA.%2520Vintern-1B%2520is%250Asmall%2520enough%2520to%2520fit%2520into%2520various%2520on-device%2520applications%2520easily.%2520Additionally%252C%250Awe%2520have%2520open-sourced%2520several%2520Vietnamese%2520vision%2520question%2520answering%2520%2528VQA%2529%250Adatasets%2520for%2520text%2520and%2520diagrams%252C%2520created%2520with%2520Gemini%25201.5%2520Flash.%2520Our%2520models%2520are%250Aavailable%2520at%253A%2520https%253A//huggingface.co/5CD-AI/Vintern-1B-v2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vintern-1B%3A%20An%20Efficient%20Multimodal%20Large%20Language%20Model%20for%20Vietnamese&entry.906535625=Khang%20T.%20Doan%20and%20Bao%20G.%20Huynh%20and%20Dung%20T.%20Hoang%20and%20Thuc%20D.%20Pham%20and%20Nhat%20H.%20Pham%20and%20Quan%20T.%20M.%20Nguyen%20and%20Bang%20Q.%20Vo%20and%20Suong%20N.%20Hoang&entry.1292438233=%20%20In%20this%20report%2C%20we%20introduce%20Vintern-1B%2C%20a%20reliable%201-billion-parameters%0Amultimodal%20large%20language%20model%20%28MLLM%29%20for%20Vietnamese%20language%20tasks.%20By%0Aintegrating%20the%20Qwen2-0.5B-Instruct%20language%20model%20with%20the%0AInternViT-300M-448px%20visual%20model%2C%20Vintern-1B%20is%20optimized%20for%20a%20range%20of%0Aapplications%2C%20including%20optical%20character%20recognition%20%28OCR%29%2C%20document%0Aextraction%2C%20and%20general%20question-answering%20in%20Vietnamese%20context.%20The%20model%20is%0Afine-tuned%20on%20an%20extensive%20dataset%20of%20over%203%20million%20image-question-answer%0Apairs%2C%20achieving%20robust%20performance%20and%20reliable%20results%20across%20multiple%0AVietnamese%20language%20benchmarks%20like%20OpenViVQA%20and%20ViTextVQA.%20Vintern-1B%20is%0Asmall%20enough%20to%20fit%20into%20various%20on-device%20applications%20easily.%20Additionally%2C%0Awe%20have%20open-sourced%20several%20Vietnamese%20vision%20question%20answering%20%28VQA%29%0Adatasets%20for%20text%20and%20diagrams%2C%20created%20with%20Gemini%201.5%20Flash.%20Our%20models%20are%0Aavailable%20at%3A%20https%3A//huggingface.co/5CD-AI/Vintern-1B-v2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12480v1&entry.124074799=Read"},
{"title": "Generalizing Visual Question Answering from Synthetic to Human-Written\n  Questions via a Chain of QA with a Large Language Model", "author": "Taehee Kim and Yeongjae Cho and Heejun Shin and Yohan Jo and Dongmyung Shin", "abstract": "  Visual question answering (VQA) is a task where an image is given, and a\nseries of questions are asked about the image. To build an efficient VQA\nalgorithm, a large amount of QA data is required which is very expensive.\nGenerating synthetic QA pairs based on templates is a practical way to obtain\ndata. However, VQA models trained on those data do not perform well on complex,\nhuman-written questions. To address this issue, we propose a new method called\n{\\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a\nsequence of QA interactions between a large language model and a VQA model\ntrained on synthetic data to reason and derive logical answers for\nhuman-written questions. We tested the effectiveness of CoQAH on two types of\nhuman-written VQA datasets for 3D-rendered and chest X-ray images and found\nthat it achieved state-of-the-art accuracy in both types of data. Notably,\nCoQAH outperformed general vision-language models, VQA models, and medical\nfoundation models with no finetuning.\n", "link": "http://arxiv.org/abs/2401.06400v3", "date": "2024-08-22", "relevancy": 1.9411, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4868}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizing%20Visual%20Question%20Answering%20from%20Synthetic%20to%20Human-Written%0A%20%20Questions%20via%20a%20Chain%20of%20QA%20with%20a%20Large%20Language%20Model&body=Title%3A%20Generalizing%20Visual%20Question%20Answering%20from%20Synthetic%20to%20Human-Written%0A%20%20Questions%20via%20a%20Chain%20of%20QA%20with%20a%20Large%20Language%20Model%0AAuthor%3A%20Taehee%20Kim%20and%20Yeongjae%20Cho%20and%20Heejun%20Shin%20and%20Yohan%20Jo%20and%20Dongmyung%20Shin%0AAbstract%3A%20%20%20Visual%20question%20answering%20%28VQA%29%20is%20a%20task%20where%20an%20image%20is%20given%2C%20and%20a%0Aseries%20of%20questions%20are%20asked%20about%20the%20image.%20To%20build%20an%20efficient%20VQA%0Aalgorithm%2C%20a%20large%20amount%20of%20QA%20data%20is%20required%20which%20is%20very%20expensive.%0AGenerating%20synthetic%20QA%20pairs%20based%20on%20templates%20is%20a%20practical%20way%20to%20obtain%0Adata.%20However%2C%20VQA%20models%20trained%20on%20those%20data%20do%20not%20perform%20well%20on%20complex%2C%0Ahuman-written%20questions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20method%20called%0A%7B%5Cit%20chain%20of%20QA%20for%20human-written%20questions%7D%20%28CoQAH%29.%20CoQAH%20utilizes%20a%0Asequence%20of%20QA%20interactions%20between%20a%20large%20language%20model%20and%20a%20VQA%20model%0Atrained%20on%20synthetic%20data%20to%20reason%20and%20derive%20logical%20answers%20for%0Ahuman-written%20questions.%20We%20tested%20the%20effectiveness%20of%20CoQAH%20on%20two%20types%20of%0Ahuman-written%20VQA%20datasets%20for%203D-rendered%20and%20chest%20X-ray%20images%20and%20found%0Athat%20it%20achieved%20state-of-the-art%20accuracy%20in%20both%20types%20of%20data.%20Notably%2C%0ACoQAH%20outperformed%20general%20vision-language%20models%2C%20VQA%20models%2C%20and%20medical%0Afoundation%20models%20with%20no%20finetuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06400v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizing%2520Visual%2520Question%2520Answering%2520from%2520Synthetic%2520to%2520Human-Written%250A%2520%2520Questions%2520via%2520a%2520Chain%2520of%2520QA%2520with%2520a%2520Large%2520Language%2520Model%26entry.906535625%3DTaehee%2520Kim%2520and%2520Yeongjae%2520Cho%2520and%2520Heejun%2520Shin%2520and%2520Yohan%2520Jo%2520and%2520Dongmyung%2520Shin%26entry.1292438233%3D%2520%2520Visual%2520question%2520answering%2520%2528VQA%2529%2520is%2520a%2520task%2520where%2520an%2520image%2520is%2520given%252C%2520and%2520a%250Aseries%2520of%2520questions%2520are%2520asked%2520about%2520the%2520image.%2520To%2520build%2520an%2520efficient%2520VQA%250Aalgorithm%252C%2520a%2520large%2520amount%2520of%2520QA%2520data%2520is%2520required%2520which%2520is%2520very%2520expensive.%250AGenerating%2520synthetic%2520QA%2520pairs%2520based%2520on%2520templates%2520is%2520a%2520practical%2520way%2520to%2520obtain%250Adata.%2520However%252C%2520VQA%2520models%2520trained%2520on%2520those%2520data%2520do%2520not%2520perform%2520well%2520on%2520complex%252C%250Ahuman-written%2520questions.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520new%2520method%2520called%250A%257B%255Cit%2520chain%2520of%2520QA%2520for%2520human-written%2520questions%257D%2520%2528CoQAH%2529.%2520CoQAH%2520utilizes%2520a%250Asequence%2520of%2520QA%2520interactions%2520between%2520a%2520large%2520language%2520model%2520and%2520a%2520VQA%2520model%250Atrained%2520on%2520synthetic%2520data%2520to%2520reason%2520and%2520derive%2520logical%2520answers%2520for%250Ahuman-written%2520questions.%2520We%2520tested%2520the%2520effectiveness%2520of%2520CoQAH%2520on%2520two%2520types%2520of%250Ahuman-written%2520VQA%2520datasets%2520for%25203D-rendered%2520and%2520chest%2520X-ray%2520images%2520and%2520found%250Athat%2520it%2520achieved%2520state-of-the-art%2520accuracy%2520in%2520both%2520types%2520of%2520data.%2520Notably%252C%250ACoQAH%2520outperformed%2520general%2520vision-language%2520models%252C%2520VQA%2520models%252C%2520and%2520medical%250Afoundation%2520models%2520with%2520no%2520finetuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06400v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20Visual%20Question%20Answering%20from%20Synthetic%20to%20Human-Written%0A%20%20Questions%20via%20a%20Chain%20of%20QA%20with%20a%20Large%20Language%20Model&entry.906535625=Taehee%20Kim%20and%20Yeongjae%20Cho%20and%20Heejun%20Shin%20and%20Yohan%20Jo%20and%20Dongmyung%20Shin&entry.1292438233=%20%20Visual%20question%20answering%20%28VQA%29%20is%20a%20task%20where%20an%20image%20is%20given%2C%20and%20a%0Aseries%20of%20questions%20are%20asked%20about%20the%20image.%20To%20build%20an%20efficient%20VQA%0Aalgorithm%2C%20a%20large%20amount%20of%20QA%20data%20is%20required%20which%20is%20very%20expensive.%0AGenerating%20synthetic%20QA%20pairs%20based%20on%20templates%20is%20a%20practical%20way%20to%20obtain%0Adata.%20However%2C%20VQA%20models%20trained%20on%20those%20data%20do%20not%20perform%20well%20on%20complex%2C%0Ahuman-written%20questions.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20method%20called%0A%7B%5Cit%20chain%20of%20QA%20for%20human-written%20questions%7D%20%28CoQAH%29.%20CoQAH%20utilizes%20a%0Asequence%20of%20QA%20interactions%20between%20a%20large%20language%20model%20and%20a%20VQA%20model%0Atrained%20on%20synthetic%20data%20to%20reason%20and%20derive%20logical%20answers%20for%0Ahuman-written%20questions.%20We%20tested%20the%20effectiveness%20of%20CoQAH%20on%20two%20types%20of%0Ahuman-written%20VQA%20datasets%20for%203D-rendered%20and%20chest%20X-ray%20images%20and%20found%0Athat%20it%20achieved%20state-of-the-art%20accuracy%20in%20both%20types%20of%20data.%20Notably%2C%0ACoQAH%20outperformed%20general%20vision-language%20models%2C%20VQA%20models%2C%20and%20medical%0Afoundation%20models%20with%20no%20finetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06400v3&entry.124074799=Read"},
{"title": "Assessing Lower Limb Strength using Internet-of-Things Enabled Chair", "author": "Hudson Kaleb Dy and Chelsea Yeh and Hanna Kaitlin Dy and Phillip Schodinger", "abstract": "  This project describes the application of the technologies of Machine\nLearning and Internet-of-Things to assess the lower limb strength of\nindividuals undergoing rehabilitation or therapy. Specifically, it seeks to\nmeasure and assess the progress of individuals by sensors attached to chairs\nand processing the data through Google GPU Tensorflow CoLab. Pressure sensors\nare attached to various locations on a chair, including but not limited to the\nseating area, backrest, hand rests, and legs. Sensor data from the individual\nperforming both sit-to-stand transition and stand-to-sit transition provides a\ntime series dataset regarding the pressure distribution and vibratory motion on\nthe chair. The dataset and timing information can then be fed into a machine\nlearning model to estimate the relative strength and weakness during various\nphases of the movement.\n", "link": "http://arxiv.org/abs/2209.04042v2", "date": "2024-08-22", "relevancy": 1.9318, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5269}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5247}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Lower%20Limb%20Strength%20using%20Internet-of-Things%20Enabled%20Chair&body=Title%3A%20Assessing%20Lower%20Limb%20Strength%20using%20Internet-of-Things%20Enabled%20Chair%0AAuthor%3A%20Hudson%20Kaleb%20Dy%20and%20Chelsea%20Yeh%20and%20Hanna%20Kaitlin%20Dy%20and%20Phillip%20Schodinger%0AAbstract%3A%20%20%20This%20project%20describes%20the%20application%20of%20the%20technologies%20of%20Machine%0ALearning%20and%20Internet-of-Things%20to%20assess%20the%20lower%20limb%20strength%20of%0Aindividuals%20undergoing%20rehabilitation%20or%20therapy.%20Specifically%2C%20it%20seeks%20to%0Ameasure%20and%20assess%20the%20progress%20of%20individuals%20by%20sensors%20attached%20to%20chairs%0Aand%20processing%20the%20data%20through%20Google%20GPU%20Tensorflow%20CoLab.%20Pressure%20sensors%0Aare%20attached%20to%20various%20locations%20on%20a%20chair%2C%20including%20but%20not%20limited%20to%20the%0Aseating%20area%2C%20backrest%2C%20hand%20rests%2C%20and%20legs.%20Sensor%20data%20from%20the%20individual%0Aperforming%20both%20sit-to-stand%20transition%20and%20stand-to-sit%20transition%20provides%20a%0Atime%20series%20dataset%20regarding%20the%20pressure%20distribution%20and%20vibratory%20motion%20on%0Athe%20chair.%20The%20dataset%20and%20timing%20information%20can%20then%20be%20fed%20into%20a%20machine%0Alearning%20model%20to%20estimate%20the%20relative%20strength%20and%20weakness%20during%20various%0Aphases%20of%20the%20movement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.04042v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Lower%2520Limb%2520Strength%2520using%2520Internet-of-Things%2520Enabled%2520Chair%26entry.906535625%3DHudson%2520Kaleb%2520Dy%2520and%2520Chelsea%2520Yeh%2520and%2520Hanna%2520Kaitlin%2520Dy%2520and%2520Phillip%2520Schodinger%26entry.1292438233%3D%2520%2520This%2520project%2520describes%2520the%2520application%2520of%2520the%2520technologies%2520of%2520Machine%250ALearning%2520and%2520Internet-of-Things%2520to%2520assess%2520the%2520lower%2520limb%2520strength%2520of%250Aindividuals%2520undergoing%2520rehabilitation%2520or%2520therapy.%2520Specifically%252C%2520it%2520seeks%2520to%250Ameasure%2520and%2520assess%2520the%2520progress%2520of%2520individuals%2520by%2520sensors%2520attached%2520to%2520chairs%250Aand%2520processing%2520the%2520data%2520through%2520Google%2520GPU%2520Tensorflow%2520CoLab.%2520Pressure%2520sensors%250Aare%2520attached%2520to%2520various%2520locations%2520on%2520a%2520chair%252C%2520including%2520but%2520not%2520limited%2520to%2520the%250Aseating%2520area%252C%2520backrest%252C%2520hand%2520rests%252C%2520and%2520legs.%2520Sensor%2520data%2520from%2520the%2520individual%250Aperforming%2520both%2520sit-to-stand%2520transition%2520and%2520stand-to-sit%2520transition%2520provides%2520a%250Atime%2520series%2520dataset%2520regarding%2520the%2520pressure%2520distribution%2520and%2520vibratory%2520motion%2520on%250Athe%2520chair.%2520The%2520dataset%2520and%2520timing%2520information%2520can%2520then%2520be%2520fed%2520into%2520a%2520machine%250Alearning%2520model%2520to%2520estimate%2520the%2520relative%2520strength%2520and%2520weakness%2520during%2520various%250Aphases%2520of%2520the%2520movement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.04042v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Lower%20Limb%20Strength%20using%20Internet-of-Things%20Enabled%20Chair&entry.906535625=Hudson%20Kaleb%20Dy%20and%20Chelsea%20Yeh%20and%20Hanna%20Kaitlin%20Dy%20and%20Phillip%20Schodinger&entry.1292438233=%20%20This%20project%20describes%20the%20application%20of%20the%20technologies%20of%20Machine%0ALearning%20and%20Internet-of-Things%20to%20assess%20the%20lower%20limb%20strength%20of%0Aindividuals%20undergoing%20rehabilitation%20or%20therapy.%20Specifically%2C%20it%20seeks%20to%0Ameasure%20and%20assess%20the%20progress%20of%20individuals%20by%20sensors%20attached%20to%20chairs%0Aand%20processing%20the%20data%20through%20Google%20GPU%20Tensorflow%20CoLab.%20Pressure%20sensors%0Aare%20attached%20to%20various%20locations%20on%20a%20chair%2C%20including%20but%20not%20limited%20to%20the%0Aseating%20area%2C%20backrest%2C%20hand%20rests%2C%20and%20legs.%20Sensor%20data%20from%20the%20individual%0Aperforming%20both%20sit-to-stand%20transition%20and%20stand-to-sit%20transition%20provides%20a%0Atime%20series%20dataset%20regarding%20the%20pressure%20distribution%20and%20vibratory%20motion%20on%0Athe%20chair.%20The%20dataset%20and%20timing%20information%20can%20then%20be%20fed%20into%20a%20machine%0Alearning%20model%20to%20estimate%20the%20relative%20strength%20and%20weakness%20during%20various%0Aphases%20of%20the%20movement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.04042v2&entry.124074799=Read"},
{"title": "Robotic Eye-in-hand Visual Servo Axially Aligning Nasopharyngeal Swabs\n  with the Nasal Cavity", "author": "Peter Q. Lee and John S. Zelek and Katja Mombaur", "abstract": "  The nasopharyngeal (NP) swab test is a method for collecting cultures to\ndiagnose for different types of respiratory illnesses, including COVID-19.\nDelegating this task to robots would be beneficial in terms of reducing\ninfection risks and bolstering the healthcare system, but a critical component\nof the NP swab test is having the swab aligned properly with the nasal cavity\nso that it does not cause excessive discomfort or injury by traveling down the\nwrong passage. Existing research towards robotic NP swabbing typically assumes\nthe patient's head is held within a fixture. This simplifies the alignment\nproblem, but is also dissimilar to clinical scenarios where patients are\ntypically free-standing. Consequently, our work creates a vision-guided\npipeline to allow an instrumented robot arm to properly position and orient NP\nswabs with respect to the nostrils of free-standing patients. The first\ncomponent of the pipeline is a precomputed joint lookup table to allow the arm\nto meet the patient's arbitrary position in the designated workspace, while\navoiding joint limits. Our pipeline leverages semantic face models from\ncomputer vision to estimate the Euclidean pose of the face with respect to a\nmonocular RGB-D camera placed on the end-effector. These estimates are passed\ninto an unscented Kalman filter on manifolds state estimator and a pose based\nvisual servo control loop to move the swab to the designated pose in front of\nthe nostril. Our pipeline was validated with human trials, featuring a cohort\nof 25 participants. The system is effective, reaching the nostril for 84% of\nparticipants, and our statistical analysis did not find significant demographic\nbiases within the cohort.\n", "link": "http://arxiv.org/abs/2408.12437v1", "date": "2024-08-22", "relevancy": 1.9314, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5259}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.456}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robotic%20Eye-in-hand%20Visual%20Servo%20Axially%20Aligning%20Nasopharyngeal%20Swabs%0A%20%20with%20the%20Nasal%20Cavity&body=Title%3A%20Robotic%20Eye-in-hand%20Visual%20Servo%20Axially%20Aligning%20Nasopharyngeal%20Swabs%0A%20%20with%20the%20Nasal%20Cavity%0AAuthor%3A%20Peter%20Q.%20Lee%20and%20John%20S.%20Zelek%20and%20Katja%20Mombaur%0AAbstract%3A%20%20%20The%20nasopharyngeal%20%28NP%29%20swab%20test%20is%20a%20method%20for%20collecting%20cultures%20to%0Adiagnose%20for%20different%20types%20of%20respiratory%20illnesses%2C%20including%20COVID-19.%0ADelegating%20this%20task%20to%20robots%20would%20be%20beneficial%20in%20terms%20of%20reducing%0Ainfection%20risks%20and%20bolstering%20the%20healthcare%20system%2C%20but%20a%20critical%20component%0Aof%20the%20NP%20swab%20test%20is%20having%20the%20swab%20aligned%20properly%20with%20the%20nasal%20cavity%0Aso%20that%20it%20does%20not%20cause%20excessive%20discomfort%20or%20injury%20by%20traveling%20down%20the%0Awrong%20passage.%20Existing%20research%20towards%20robotic%20NP%20swabbing%20typically%20assumes%0Athe%20patient%27s%20head%20is%20held%20within%20a%20fixture.%20This%20simplifies%20the%20alignment%0Aproblem%2C%20but%20is%20also%20dissimilar%20to%20clinical%20scenarios%20where%20patients%20are%0Atypically%20free-standing.%20Consequently%2C%20our%20work%20creates%20a%20vision-guided%0Apipeline%20to%20allow%20an%20instrumented%20robot%20arm%20to%20properly%20position%20and%20orient%20NP%0Aswabs%20with%20respect%20to%20the%20nostrils%20of%20free-standing%20patients.%20The%20first%0Acomponent%20of%20the%20pipeline%20is%20a%20precomputed%20joint%20lookup%20table%20to%20allow%20the%20arm%0Ato%20meet%20the%20patient%27s%20arbitrary%20position%20in%20the%20designated%20workspace%2C%20while%0Aavoiding%20joint%20limits.%20Our%20pipeline%20leverages%20semantic%20face%20models%20from%0Acomputer%20vision%20to%20estimate%20the%20Euclidean%20pose%20of%20the%20face%20with%20respect%20to%20a%0Amonocular%20RGB-D%20camera%20placed%20on%20the%20end-effector.%20These%20estimates%20are%20passed%0Ainto%20an%20unscented%20Kalman%20filter%20on%20manifolds%20state%20estimator%20and%20a%20pose%20based%0Avisual%20servo%20control%20loop%20to%20move%20the%20swab%20to%20the%20designated%20pose%20in%20front%20of%0Athe%20nostril.%20Our%20pipeline%20was%20validated%20with%20human%20trials%2C%20featuring%20a%20cohort%0Aof%2025%20participants.%20The%20system%20is%20effective%2C%20reaching%20the%20nostril%20for%2084%25%20of%0Aparticipants%2C%20and%20our%20statistical%20analysis%20did%20not%20find%20significant%20demographic%0Abiases%20within%20the%20cohort.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobotic%2520Eye-in-hand%2520Visual%2520Servo%2520Axially%2520Aligning%2520Nasopharyngeal%2520Swabs%250A%2520%2520with%2520the%2520Nasal%2520Cavity%26entry.906535625%3DPeter%2520Q.%2520Lee%2520and%2520John%2520S.%2520Zelek%2520and%2520Katja%2520Mombaur%26entry.1292438233%3D%2520%2520The%2520nasopharyngeal%2520%2528NP%2529%2520swab%2520test%2520is%2520a%2520method%2520for%2520collecting%2520cultures%2520to%250Adiagnose%2520for%2520different%2520types%2520of%2520respiratory%2520illnesses%252C%2520including%2520COVID-19.%250ADelegating%2520this%2520task%2520to%2520robots%2520would%2520be%2520beneficial%2520in%2520terms%2520of%2520reducing%250Ainfection%2520risks%2520and%2520bolstering%2520the%2520healthcare%2520system%252C%2520but%2520a%2520critical%2520component%250Aof%2520the%2520NP%2520swab%2520test%2520is%2520having%2520the%2520swab%2520aligned%2520properly%2520with%2520the%2520nasal%2520cavity%250Aso%2520that%2520it%2520does%2520not%2520cause%2520excessive%2520discomfort%2520or%2520injury%2520by%2520traveling%2520down%2520the%250Awrong%2520passage.%2520Existing%2520research%2520towards%2520robotic%2520NP%2520swabbing%2520typically%2520assumes%250Athe%2520patient%2527s%2520head%2520is%2520held%2520within%2520a%2520fixture.%2520This%2520simplifies%2520the%2520alignment%250Aproblem%252C%2520but%2520is%2520also%2520dissimilar%2520to%2520clinical%2520scenarios%2520where%2520patients%2520are%250Atypically%2520free-standing.%2520Consequently%252C%2520our%2520work%2520creates%2520a%2520vision-guided%250Apipeline%2520to%2520allow%2520an%2520instrumented%2520robot%2520arm%2520to%2520properly%2520position%2520and%2520orient%2520NP%250Aswabs%2520with%2520respect%2520to%2520the%2520nostrils%2520of%2520free-standing%2520patients.%2520The%2520first%250Acomponent%2520of%2520the%2520pipeline%2520is%2520a%2520precomputed%2520joint%2520lookup%2520table%2520to%2520allow%2520the%2520arm%250Ato%2520meet%2520the%2520patient%2527s%2520arbitrary%2520position%2520in%2520the%2520designated%2520workspace%252C%2520while%250Aavoiding%2520joint%2520limits.%2520Our%2520pipeline%2520leverages%2520semantic%2520face%2520models%2520from%250Acomputer%2520vision%2520to%2520estimate%2520the%2520Euclidean%2520pose%2520of%2520the%2520face%2520with%2520respect%2520to%2520a%250Amonocular%2520RGB-D%2520camera%2520placed%2520on%2520the%2520end-effector.%2520These%2520estimates%2520are%2520passed%250Ainto%2520an%2520unscented%2520Kalman%2520filter%2520on%2520manifolds%2520state%2520estimator%2520and%2520a%2520pose%2520based%250Avisual%2520servo%2520control%2520loop%2520to%2520move%2520the%2520swab%2520to%2520the%2520designated%2520pose%2520in%2520front%2520of%250Athe%2520nostril.%2520Our%2520pipeline%2520was%2520validated%2520with%2520human%2520trials%252C%2520featuring%2520a%2520cohort%250Aof%252025%2520participants.%2520The%2520system%2520is%2520effective%252C%2520reaching%2520the%2520nostril%2520for%252084%2525%2520of%250Aparticipants%252C%2520and%2520our%2520statistical%2520analysis%2520did%2520not%2520find%2520significant%2520demographic%250Abiases%2520within%2520the%2520cohort.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robotic%20Eye-in-hand%20Visual%20Servo%20Axially%20Aligning%20Nasopharyngeal%20Swabs%0A%20%20with%20the%20Nasal%20Cavity&entry.906535625=Peter%20Q.%20Lee%20and%20John%20S.%20Zelek%20and%20Katja%20Mombaur&entry.1292438233=%20%20The%20nasopharyngeal%20%28NP%29%20swab%20test%20is%20a%20method%20for%20collecting%20cultures%20to%0Adiagnose%20for%20different%20types%20of%20respiratory%20illnesses%2C%20including%20COVID-19.%0ADelegating%20this%20task%20to%20robots%20would%20be%20beneficial%20in%20terms%20of%20reducing%0Ainfection%20risks%20and%20bolstering%20the%20healthcare%20system%2C%20but%20a%20critical%20component%0Aof%20the%20NP%20swab%20test%20is%20having%20the%20swab%20aligned%20properly%20with%20the%20nasal%20cavity%0Aso%20that%20it%20does%20not%20cause%20excessive%20discomfort%20or%20injury%20by%20traveling%20down%20the%0Awrong%20passage.%20Existing%20research%20towards%20robotic%20NP%20swabbing%20typically%20assumes%0Athe%20patient%27s%20head%20is%20held%20within%20a%20fixture.%20This%20simplifies%20the%20alignment%0Aproblem%2C%20but%20is%20also%20dissimilar%20to%20clinical%20scenarios%20where%20patients%20are%0Atypically%20free-standing.%20Consequently%2C%20our%20work%20creates%20a%20vision-guided%0Apipeline%20to%20allow%20an%20instrumented%20robot%20arm%20to%20properly%20position%20and%20orient%20NP%0Aswabs%20with%20respect%20to%20the%20nostrils%20of%20free-standing%20patients.%20The%20first%0Acomponent%20of%20the%20pipeline%20is%20a%20precomputed%20joint%20lookup%20table%20to%20allow%20the%20arm%0Ato%20meet%20the%20patient%27s%20arbitrary%20position%20in%20the%20designated%20workspace%2C%20while%0Aavoiding%20joint%20limits.%20Our%20pipeline%20leverages%20semantic%20face%20models%20from%0Acomputer%20vision%20to%20estimate%20the%20Euclidean%20pose%20of%20the%20face%20with%20respect%20to%20a%0Amonocular%20RGB-D%20camera%20placed%20on%20the%20end-effector.%20These%20estimates%20are%20passed%0Ainto%20an%20unscented%20Kalman%20filter%20on%20manifolds%20state%20estimator%20and%20a%20pose%20based%0Avisual%20servo%20control%20loop%20to%20move%20the%20swab%20to%20the%20designated%20pose%20in%20front%20of%0Athe%20nostril.%20Our%20pipeline%20was%20validated%20with%20human%20trials%2C%20featuring%20a%20cohort%0Aof%2025%20participants.%20The%20system%20is%20effective%2C%20reaching%20the%20nostril%20for%2084%25%20of%0Aparticipants%2C%20and%20our%20statistical%20analysis%20did%20not%20find%20significant%20demographic%0Abiases%20within%20the%20cohort.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12437v1&entry.124074799=Read"},
{"title": "MakeupAttack: Feature Space Black-box Backdoor Attack on Face\n  Recognition via Makeup Transfer", "author": "Ming Sun and Lihua Jing and Zixuan Zhu and Rui Wang", "abstract": "  Backdoor attacks pose a significant threat to the training process of deep\nneural networks (DNNs). As a widely-used DNN-based application in real-world\nscenarios, face recognition systems once implanted into the backdoor, may cause\nserious consequences. Backdoor research on face recognition is still in its\nearly stages, and the existing backdoor triggers are relatively simple and\nvisible. Furthermore, due to the perceptibility, diversity, and similarity of\nfacial datasets, many state-of-the-art backdoor attacks lose effectiveness on\nface recognition tasks. In this work, we propose a novel feature space backdoor\nattack against face recognition via makeup transfer, dubbed MakeupAttack. In\ncontrast to many feature space attacks that demand full access to target\nmodels, our method only requires model queries, adhering to black-box attack\nprinciples. In our attack, we design an iterative training paradigm to learn\nthe subtle features of the proposed makeup-style trigger. Additionally,\nMakeupAttack promotes trigger diversity using the adaptive selection method,\ndispersing the feature distribution of malicious samples to bypass existing\ndefense methods. Extensive experiments were conducted on two widely-used facial\ndatasets targeting multiple models. The results demonstrate that our proposed\nattack method can bypass existing state-of-the-art defenses while maintaining\neffectiveness, robustness, naturalness, and stealthiness, without compromising\nmodel performance.\n", "link": "http://arxiv.org/abs/2408.12312v1", "date": "2024-08-22", "relevancy": 1.9254, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4892}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4884}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MakeupAttack%3A%20Feature%20Space%20Black-box%20Backdoor%20Attack%20on%20Face%0A%20%20Recognition%20via%20Makeup%20Transfer&body=Title%3A%20MakeupAttack%3A%20Feature%20Space%20Black-box%20Backdoor%20Attack%20on%20Face%0A%20%20Recognition%20via%20Makeup%20Transfer%0AAuthor%3A%20Ming%20Sun%20and%20Lihua%20Jing%20and%20Zixuan%20Zhu%20and%20Rui%20Wang%0AAbstract%3A%20%20%20Backdoor%20attacks%20pose%20a%20significant%20threat%20to%20the%20training%20process%20of%20deep%0Aneural%20networks%20%28DNNs%29.%20As%20a%20widely-used%20DNN-based%20application%20in%20real-world%0Ascenarios%2C%20face%20recognition%20systems%20once%20implanted%20into%20the%20backdoor%2C%20may%20cause%0Aserious%20consequences.%20Backdoor%20research%20on%20face%20recognition%20is%20still%20in%20its%0Aearly%20stages%2C%20and%20the%20existing%20backdoor%20triggers%20are%20relatively%20simple%20and%0Avisible.%20Furthermore%2C%20due%20to%20the%20perceptibility%2C%20diversity%2C%20and%20similarity%20of%0Afacial%20datasets%2C%20many%20state-of-the-art%20backdoor%20attacks%20lose%20effectiveness%20on%0Aface%20recognition%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20novel%20feature%20space%20backdoor%0Aattack%20against%20face%20recognition%20via%20makeup%20transfer%2C%20dubbed%20MakeupAttack.%20In%0Acontrast%20to%20many%20feature%20space%20attacks%20that%20demand%20full%20access%20to%20target%0Amodels%2C%20our%20method%20only%20requires%20model%20queries%2C%20adhering%20to%20black-box%20attack%0Aprinciples.%20In%20our%20attack%2C%20we%20design%20an%20iterative%20training%20paradigm%20to%20learn%0Athe%20subtle%20features%20of%20the%20proposed%20makeup-style%20trigger.%20Additionally%2C%0AMakeupAttack%20promotes%20trigger%20diversity%20using%20the%20adaptive%20selection%20method%2C%0Adispersing%20the%20feature%20distribution%20of%20malicious%20samples%20to%20bypass%20existing%0Adefense%20methods.%20Extensive%20experiments%20were%20conducted%20on%20two%20widely-used%20facial%0Adatasets%20targeting%20multiple%20models.%20The%20results%20demonstrate%20that%20our%20proposed%0Aattack%20method%20can%20bypass%20existing%20state-of-the-art%20defenses%20while%20maintaining%0Aeffectiveness%2C%20robustness%2C%20naturalness%2C%20and%20stealthiness%2C%20without%20compromising%0Amodel%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMakeupAttack%253A%2520Feature%2520Space%2520Black-box%2520Backdoor%2520Attack%2520on%2520Face%250A%2520%2520Recognition%2520via%2520Makeup%2520Transfer%26entry.906535625%3DMing%2520Sun%2520and%2520Lihua%2520Jing%2520and%2520Zixuan%2520Zhu%2520and%2520Rui%2520Wang%26entry.1292438233%3D%2520%2520Backdoor%2520attacks%2520pose%2520a%2520significant%2520threat%2520to%2520the%2520training%2520process%2520of%2520deep%250Aneural%2520networks%2520%2528DNNs%2529.%2520As%2520a%2520widely-used%2520DNN-based%2520application%2520in%2520real-world%250Ascenarios%252C%2520face%2520recognition%2520systems%2520once%2520implanted%2520into%2520the%2520backdoor%252C%2520may%2520cause%250Aserious%2520consequences.%2520Backdoor%2520research%2520on%2520face%2520recognition%2520is%2520still%2520in%2520its%250Aearly%2520stages%252C%2520and%2520the%2520existing%2520backdoor%2520triggers%2520are%2520relatively%2520simple%2520and%250Avisible.%2520Furthermore%252C%2520due%2520to%2520the%2520perceptibility%252C%2520diversity%252C%2520and%2520similarity%2520of%250Afacial%2520datasets%252C%2520many%2520state-of-the-art%2520backdoor%2520attacks%2520lose%2520effectiveness%2520on%250Aface%2520recognition%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520feature%2520space%2520backdoor%250Aattack%2520against%2520face%2520recognition%2520via%2520makeup%2520transfer%252C%2520dubbed%2520MakeupAttack.%2520In%250Acontrast%2520to%2520many%2520feature%2520space%2520attacks%2520that%2520demand%2520full%2520access%2520to%2520target%250Amodels%252C%2520our%2520method%2520only%2520requires%2520model%2520queries%252C%2520adhering%2520to%2520black-box%2520attack%250Aprinciples.%2520In%2520our%2520attack%252C%2520we%2520design%2520an%2520iterative%2520training%2520paradigm%2520to%2520learn%250Athe%2520subtle%2520features%2520of%2520the%2520proposed%2520makeup-style%2520trigger.%2520Additionally%252C%250AMakeupAttack%2520promotes%2520trigger%2520diversity%2520using%2520the%2520adaptive%2520selection%2520method%252C%250Adispersing%2520the%2520feature%2520distribution%2520of%2520malicious%2520samples%2520to%2520bypass%2520existing%250Adefense%2520methods.%2520Extensive%2520experiments%2520were%2520conducted%2520on%2520two%2520widely-used%2520facial%250Adatasets%2520targeting%2520multiple%2520models.%2520The%2520results%2520demonstrate%2520that%2520our%2520proposed%250Aattack%2520method%2520can%2520bypass%2520existing%2520state-of-the-art%2520defenses%2520while%2520maintaining%250Aeffectiveness%252C%2520robustness%252C%2520naturalness%252C%2520and%2520stealthiness%252C%2520without%2520compromising%250Amodel%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MakeupAttack%3A%20Feature%20Space%20Black-box%20Backdoor%20Attack%20on%20Face%0A%20%20Recognition%20via%20Makeup%20Transfer&entry.906535625=Ming%20Sun%20and%20Lihua%20Jing%20and%20Zixuan%20Zhu%20and%20Rui%20Wang&entry.1292438233=%20%20Backdoor%20attacks%20pose%20a%20significant%20threat%20to%20the%20training%20process%20of%20deep%0Aneural%20networks%20%28DNNs%29.%20As%20a%20widely-used%20DNN-based%20application%20in%20real-world%0Ascenarios%2C%20face%20recognition%20systems%20once%20implanted%20into%20the%20backdoor%2C%20may%20cause%0Aserious%20consequences.%20Backdoor%20research%20on%20face%20recognition%20is%20still%20in%20its%0Aearly%20stages%2C%20and%20the%20existing%20backdoor%20triggers%20are%20relatively%20simple%20and%0Avisible.%20Furthermore%2C%20due%20to%20the%20perceptibility%2C%20diversity%2C%20and%20similarity%20of%0Afacial%20datasets%2C%20many%20state-of-the-art%20backdoor%20attacks%20lose%20effectiveness%20on%0Aface%20recognition%20tasks.%20In%20this%20work%2C%20we%20propose%20a%20novel%20feature%20space%20backdoor%0Aattack%20against%20face%20recognition%20via%20makeup%20transfer%2C%20dubbed%20MakeupAttack.%20In%0Acontrast%20to%20many%20feature%20space%20attacks%20that%20demand%20full%20access%20to%20target%0Amodels%2C%20our%20method%20only%20requires%20model%20queries%2C%20adhering%20to%20black-box%20attack%0Aprinciples.%20In%20our%20attack%2C%20we%20design%20an%20iterative%20training%20paradigm%20to%20learn%0Athe%20subtle%20features%20of%20the%20proposed%20makeup-style%20trigger.%20Additionally%2C%0AMakeupAttack%20promotes%20trigger%20diversity%20using%20the%20adaptive%20selection%20method%2C%0Adispersing%20the%20feature%20distribution%20of%20malicious%20samples%20to%20bypass%20existing%0Adefense%20methods.%20Extensive%20experiments%20were%20conducted%20on%20two%20widely-used%20facial%0Adatasets%20targeting%20multiple%20models.%20The%20results%20demonstrate%20that%20our%20proposed%0Aattack%20method%20can%20bypass%20existing%20state-of-the-art%20defenses%20while%20maintaining%0Aeffectiveness%2C%20robustness%2C%20naturalness%2C%20and%20stealthiness%2C%20without%20compromising%0Amodel%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12312v1&entry.124074799=Read"},
{"title": "Unlearning Trojans in Large Language Models: A Comparison Between\n  Natural Language and Source Code", "author": "Mahdi Kazemi and Aftab Hussain and Md Rafiqul Islam Rabin and Mohammad Amin Alipour and Sen Lin", "abstract": "  This work investigates the application of Machine Unlearning (MU) for\nmitigating the impact of trojans embedded in conventional large language models\nof natural language (Text-LLMs) and large language models of code (Code-LLMs)\nWe propose a novel unlearning approach, LYA, that leverages both gradient\nascent and elastic weight consolidation, a Fisher Information Matrix (FIM)\nbased regularization technique, to unlearn trojans from poisoned models. We\ncompare the effectiveness of LYA against conventional techniques like\nfine-tuning, retraining, and vanilla gradient ascent. The subject models we\ninvestigate are BERT and CodeBERT, for sentiment analysis and code defect\ndetection tasks, respectively. Our findings demonstrate that the combination of\ngradient ascent and FIM-based regularization, as done in LYA, outperforms\nexisting methods in removing the trojan's influence from the poisoned model,\nwhile preserving its original functionality. To the best of our knowledge, this\nis the first work that compares and contrasts MU of trojans in LLMs, in the NL\nand Coding domain.\n", "link": "http://arxiv.org/abs/2408.12416v1", "date": "2024-08-22", "relevancy": 1.924, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5115}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlearning%20Trojans%20in%20Large%20Language%20Models%3A%20A%20Comparison%20Between%0A%20%20Natural%20Language%20and%20Source%20Code&body=Title%3A%20Unlearning%20Trojans%20in%20Large%20Language%20Models%3A%20A%20Comparison%20Between%0A%20%20Natural%20Language%20and%20Source%20Code%0AAuthor%3A%20Mahdi%20Kazemi%20and%20Aftab%20Hussain%20and%20Md%20Rafiqul%20Islam%20Rabin%20and%20Mohammad%20Amin%20Alipour%20and%20Sen%20Lin%0AAbstract%3A%20%20%20This%20work%20investigates%20the%20application%20of%20Machine%20Unlearning%20%28MU%29%20for%0Amitigating%20the%20impact%20of%20trojans%20embedded%20in%20conventional%20large%20language%20models%0Aof%20natural%20language%20%28Text-LLMs%29%20and%20large%20language%20models%20of%20code%20%28Code-LLMs%29%0AWe%20propose%20a%20novel%20unlearning%20approach%2C%20LYA%2C%20that%20leverages%20both%20gradient%0Aascent%20and%20elastic%20weight%20consolidation%2C%20a%20Fisher%20Information%20Matrix%20%28FIM%29%0Abased%20regularization%20technique%2C%20to%20unlearn%20trojans%20from%20poisoned%20models.%20We%0Acompare%20the%20effectiveness%20of%20LYA%20against%20conventional%20techniques%20like%0Afine-tuning%2C%20retraining%2C%20and%20vanilla%20gradient%20ascent.%20The%20subject%20models%20we%0Ainvestigate%20are%20BERT%20and%20CodeBERT%2C%20for%20sentiment%20analysis%20and%20code%20defect%0Adetection%20tasks%2C%20respectively.%20Our%20findings%20demonstrate%20that%20the%20combination%20of%0Agradient%20ascent%20and%20FIM-based%20regularization%2C%20as%20done%20in%20LYA%2C%20outperforms%0Aexisting%20methods%20in%20removing%20the%20trojan%27s%20influence%20from%20the%20poisoned%20model%2C%0Awhile%20preserving%20its%20original%20functionality.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20work%20that%20compares%20and%20contrasts%20MU%20of%20trojans%20in%20LLMs%2C%20in%20the%20NL%0Aand%20Coding%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlearning%2520Trojans%2520in%2520Large%2520Language%2520Models%253A%2520A%2520Comparison%2520Between%250A%2520%2520Natural%2520Language%2520and%2520Source%2520Code%26entry.906535625%3DMahdi%2520Kazemi%2520and%2520Aftab%2520Hussain%2520and%2520Md%2520Rafiqul%2520Islam%2520Rabin%2520and%2520Mohammad%2520Amin%2520Alipour%2520and%2520Sen%2520Lin%26entry.1292438233%3D%2520%2520This%2520work%2520investigates%2520the%2520application%2520of%2520Machine%2520Unlearning%2520%2528MU%2529%2520for%250Amitigating%2520the%2520impact%2520of%2520trojans%2520embedded%2520in%2520conventional%2520large%2520language%2520models%250Aof%2520natural%2520language%2520%2528Text-LLMs%2529%2520and%2520large%2520language%2520models%2520of%2520code%2520%2528Code-LLMs%2529%250AWe%2520propose%2520a%2520novel%2520unlearning%2520approach%252C%2520LYA%252C%2520that%2520leverages%2520both%2520gradient%250Aascent%2520and%2520elastic%2520weight%2520consolidation%252C%2520a%2520Fisher%2520Information%2520Matrix%2520%2528FIM%2529%250Abased%2520regularization%2520technique%252C%2520to%2520unlearn%2520trojans%2520from%2520poisoned%2520models.%2520We%250Acompare%2520the%2520effectiveness%2520of%2520LYA%2520against%2520conventional%2520techniques%2520like%250Afine-tuning%252C%2520retraining%252C%2520and%2520vanilla%2520gradient%2520ascent.%2520The%2520subject%2520models%2520we%250Ainvestigate%2520are%2520BERT%2520and%2520CodeBERT%252C%2520for%2520sentiment%2520analysis%2520and%2520code%2520defect%250Adetection%2520tasks%252C%2520respectively.%2520Our%2520findings%2520demonstrate%2520that%2520the%2520combination%2520of%250Agradient%2520ascent%2520and%2520FIM-based%2520regularization%252C%2520as%2520done%2520in%2520LYA%252C%2520outperforms%250Aexisting%2520methods%2520in%2520removing%2520the%2520trojan%2527s%2520influence%2520from%2520the%2520poisoned%2520model%252C%250Awhile%2520preserving%2520its%2520original%2520functionality.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520work%2520that%2520compares%2520and%2520contrasts%2520MU%2520of%2520trojans%2520in%2520LLMs%252C%2520in%2520the%2520NL%250Aand%2520Coding%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlearning%20Trojans%20in%20Large%20Language%20Models%3A%20A%20Comparison%20Between%0A%20%20Natural%20Language%20and%20Source%20Code&entry.906535625=Mahdi%20Kazemi%20and%20Aftab%20Hussain%20and%20Md%20Rafiqul%20Islam%20Rabin%20and%20Mohammad%20Amin%20Alipour%20and%20Sen%20Lin&entry.1292438233=%20%20This%20work%20investigates%20the%20application%20of%20Machine%20Unlearning%20%28MU%29%20for%0Amitigating%20the%20impact%20of%20trojans%20embedded%20in%20conventional%20large%20language%20models%0Aof%20natural%20language%20%28Text-LLMs%29%20and%20large%20language%20models%20of%20code%20%28Code-LLMs%29%0AWe%20propose%20a%20novel%20unlearning%20approach%2C%20LYA%2C%20that%20leverages%20both%20gradient%0Aascent%20and%20elastic%20weight%20consolidation%2C%20a%20Fisher%20Information%20Matrix%20%28FIM%29%0Abased%20regularization%20technique%2C%20to%20unlearn%20trojans%20from%20poisoned%20models.%20We%0Acompare%20the%20effectiveness%20of%20LYA%20against%20conventional%20techniques%20like%0Afine-tuning%2C%20retraining%2C%20and%20vanilla%20gradient%20ascent.%20The%20subject%20models%20we%0Ainvestigate%20are%20BERT%20and%20CodeBERT%2C%20for%20sentiment%20analysis%20and%20code%20defect%0Adetection%20tasks%2C%20respectively.%20Our%20findings%20demonstrate%20that%20the%20combination%20of%0Agradient%20ascent%20and%20FIM-based%20regularization%2C%20as%20done%20in%20LYA%2C%20outperforms%0Aexisting%20methods%20in%20removing%20the%20trojan%27s%20influence%20from%20the%20poisoned%20model%2C%0Awhile%20preserving%20its%20original%20functionality.%20To%20the%20best%20of%20our%20knowledge%2C%20this%0Ais%20the%20first%20work%20that%20compares%20and%20contrasts%20MU%20of%20trojans%20in%20LLMs%2C%20in%20the%20NL%0Aand%20Coding%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12416v1&entry.124074799=Read"},
{"title": "An Efficient and Explainable Transformer-Based Few-Shot Learning for\n  Modeling Electricity Consumption Profiles Across Thousands of Domains", "author": "Weijie Xia and Gao Peng and Chenguang Wang and Peter Palensky and Eric Pauwels and Pedro P. Vergara", "abstract": "  Electricity Consumption Profiles (ECPs) are crucial for operating and\nplanning power distribution systems, especially with the increasing numbers of\nvarious low-carbon technologies such as solar panels and electric vehicles.\nTraditional ECP modeling methods typically assume the availability of\nsufficient ECP data. However, in practice, the accessibility of ECP data is\nlimited due to privacy issues or the absence of metering devices. Few-shot\nlearning (FSL) has emerged as a promising solution for ECP modeling in\ndata-scarce scenarios. Nevertheless, standard FSL methods, such as those used\nfor images, are unsuitable for ECP modeling because (1) these methods usually\nassume several source domains with sufficient data and several target domains.\nHowever, in the context of ECP modeling, there may be thousands of source\ndomains with a moderate amount of data and thousands of target domains. (2)\nStandard FSL methods usually involve cumbersome knowledge transfer mechanisms,\nsuch as pre-training and fine-tuning, whereas ECP modeling requires more\nlightweight methods. (3) Deep learning models often lack explainability,\nhindering their application in industry. This paper proposes a novel FSL method\nthat exploits Transformers and Gaussian Mixture Models (GMMs) for ECP modeling\nto address the above-described issues. Results show that our method can\naccurately restore the complex ECP distribution with a minimal amount of ECP\ndata (e.g., only 1.6\\% of the complete domain dataset) while it outperforms\nstate-of-the-art time series modeling methods, maintaining the advantages of\nbeing both lightweight and interpretable. The project is open-sourced at\nhttps://github.com/xiaweijie1996/TransformerEM-GMM.git.\n", "link": "http://arxiv.org/abs/2408.08399v2", "date": "2024-08-22", "relevancy": 1.9074, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5408}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4699}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20and%20Explainable%20Transformer-Based%20Few-Shot%20Learning%20for%0A%20%20Modeling%20Electricity%20Consumption%20Profiles%20Across%20Thousands%20of%20Domains&body=Title%3A%20An%20Efficient%20and%20Explainable%20Transformer-Based%20Few-Shot%20Learning%20for%0A%20%20Modeling%20Electricity%20Consumption%20Profiles%20Across%20Thousands%20of%20Domains%0AAuthor%3A%20Weijie%20Xia%20and%20Gao%20Peng%20and%20Chenguang%20Wang%20and%20Peter%20Palensky%20and%20Eric%20Pauwels%20and%20Pedro%20P.%20Vergara%0AAbstract%3A%20%20%20Electricity%20Consumption%20Profiles%20%28ECPs%29%20are%20crucial%20for%20operating%20and%0Aplanning%20power%20distribution%20systems%2C%20especially%20with%20the%20increasing%20numbers%20of%0Avarious%20low-carbon%20technologies%20such%20as%20solar%20panels%20and%20electric%20vehicles.%0ATraditional%20ECP%20modeling%20methods%20typically%20assume%20the%20availability%20of%0Asufficient%20ECP%20data.%20However%2C%20in%20practice%2C%20the%20accessibility%20of%20ECP%20data%20is%0Alimited%20due%20to%20privacy%20issues%20or%20the%20absence%20of%20metering%20devices.%20Few-shot%0Alearning%20%28FSL%29%20has%20emerged%20as%20a%20promising%20solution%20for%20ECP%20modeling%20in%0Adata-scarce%20scenarios.%20Nevertheless%2C%20standard%20FSL%20methods%2C%20such%20as%20those%20used%0Afor%20images%2C%20are%20unsuitable%20for%20ECP%20modeling%20because%20%281%29%20these%20methods%20usually%0Aassume%20several%20source%20domains%20with%20sufficient%20data%20and%20several%20target%20domains.%0AHowever%2C%20in%20the%20context%20of%20ECP%20modeling%2C%20there%20may%20be%20thousands%20of%20source%0Adomains%20with%20a%20moderate%20amount%20of%20data%20and%20thousands%20of%20target%20domains.%20%282%29%0AStandard%20FSL%20methods%20usually%20involve%20cumbersome%20knowledge%20transfer%20mechanisms%2C%0Asuch%20as%20pre-training%20and%20fine-tuning%2C%20whereas%20ECP%20modeling%20requires%20more%0Alightweight%20methods.%20%283%29%20Deep%20learning%20models%20often%20lack%20explainability%2C%0Ahindering%20their%20application%20in%20industry.%20This%20paper%20proposes%20a%20novel%20FSL%20method%0Athat%20exploits%20Transformers%20and%20Gaussian%20Mixture%20Models%20%28GMMs%29%20for%20ECP%20modeling%0Ato%20address%20the%20above-described%20issues.%20Results%20show%20that%20our%20method%20can%0Aaccurately%20restore%20the%20complex%20ECP%20distribution%20with%20a%20minimal%20amount%20of%20ECP%0Adata%20%28e.g.%2C%20only%201.6%5C%25%20of%20the%20complete%20domain%20dataset%29%20while%20it%20outperforms%0Astate-of-the-art%20time%20series%20modeling%20methods%2C%20maintaining%20the%20advantages%20of%0Abeing%20both%20lightweight%20and%20interpretable.%20The%20project%20is%20open-sourced%20at%0Ahttps%3A//github.com/xiaweijie1996/TransformerEM-GMM.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08399v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520and%2520Explainable%2520Transformer-Based%2520Few-Shot%2520Learning%2520for%250A%2520%2520Modeling%2520Electricity%2520Consumption%2520Profiles%2520Across%2520Thousands%2520of%2520Domains%26entry.906535625%3DWeijie%2520Xia%2520and%2520Gao%2520Peng%2520and%2520Chenguang%2520Wang%2520and%2520Peter%2520Palensky%2520and%2520Eric%2520Pauwels%2520and%2520Pedro%2520P.%2520Vergara%26entry.1292438233%3D%2520%2520Electricity%2520Consumption%2520Profiles%2520%2528ECPs%2529%2520are%2520crucial%2520for%2520operating%2520and%250Aplanning%2520power%2520distribution%2520systems%252C%2520especially%2520with%2520the%2520increasing%2520numbers%2520of%250Avarious%2520low-carbon%2520technologies%2520such%2520as%2520solar%2520panels%2520and%2520electric%2520vehicles.%250ATraditional%2520ECP%2520modeling%2520methods%2520typically%2520assume%2520the%2520availability%2520of%250Asufficient%2520ECP%2520data.%2520However%252C%2520in%2520practice%252C%2520the%2520accessibility%2520of%2520ECP%2520data%2520is%250Alimited%2520due%2520to%2520privacy%2520issues%2520or%2520the%2520absence%2520of%2520metering%2520devices.%2520Few-shot%250Alearning%2520%2528FSL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520solution%2520for%2520ECP%2520modeling%2520in%250Adata-scarce%2520scenarios.%2520Nevertheless%252C%2520standard%2520FSL%2520methods%252C%2520such%2520as%2520those%2520used%250Afor%2520images%252C%2520are%2520unsuitable%2520for%2520ECP%2520modeling%2520because%2520%25281%2529%2520these%2520methods%2520usually%250Aassume%2520several%2520source%2520domains%2520with%2520sufficient%2520data%2520and%2520several%2520target%2520domains.%250AHowever%252C%2520in%2520the%2520context%2520of%2520ECP%2520modeling%252C%2520there%2520may%2520be%2520thousands%2520of%2520source%250Adomains%2520with%2520a%2520moderate%2520amount%2520of%2520data%2520and%2520thousands%2520of%2520target%2520domains.%2520%25282%2529%250AStandard%2520FSL%2520methods%2520usually%2520involve%2520cumbersome%2520knowledge%2520transfer%2520mechanisms%252C%250Asuch%2520as%2520pre-training%2520and%2520fine-tuning%252C%2520whereas%2520ECP%2520modeling%2520requires%2520more%250Alightweight%2520methods.%2520%25283%2529%2520Deep%2520learning%2520models%2520often%2520lack%2520explainability%252C%250Ahindering%2520their%2520application%2520in%2520industry.%2520This%2520paper%2520proposes%2520a%2520novel%2520FSL%2520method%250Athat%2520exploits%2520Transformers%2520and%2520Gaussian%2520Mixture%2520Models%2520%2528GMMs%2529%2520for%2520ECP%2520modeling%250Ato%2520address%2520the%2520above-described%2520issues.%2520Results%2520show%2520that%2520our%2520method%2520can%250Aaccurately%2520restore%2520the%2520complex%2520ECP%2520distribution%2520with%2520a%2520minimal%2520amount%2520of%2520ECP%250Adata%2520%2528e.g.%252C%2520only%25201.6%255C%2525%2520of%2520the%2520complete%2520domain%2520dataset%2529%2520while%2520it%2520outperforms%250Astate-of-the-art%2520time%2520series%2520modeling%2520methods%252C%2520maintaining%2520the%2520advantages%2520of%250Abeing%2520both%2520lightweight%2520and%2520interpretable.%2520The%2520project%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/xiaweijie1996/TransformerEM-GMM.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08399v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20and%20Explainable%20Transformer-Based%20Few-Shot%20Learning%20for%0A%20%20Modeling%20Electricity%20Consumption%20Profiles%20Across%20Thousands%20of%20Domains&entry.906535625=Weijie%20Xia%20and%20Gao%20Peng%20and%20Chenguang%20Wang%20and%20Peter%20Palensky%20and%20Eric%20Pauwels%20and%20Pedro%20P.%20Vergara&entry.1292438233=%20%20Electricity%20Consumption%20Profiles%20%28ECPs%29%20are%20crucial%20for%20operating%20and%0Aplanning%20power%20distribution%20systems%2C%20especially%20with%20the%20increasing%20numbers%20of%0Avarious%20low-carbon%20technologies%20such%20as%20solar%20panels%20and%20electric%20vehicles.%0ATraditional%20ECP%20modeling%20methods%20typically%20assume%20the%20availability%20of%0Asufficient%20ECP%20data.%20However%2C%20in%20practice%2C%20the%20accessibility%20of%20ECP%20data%20is%0Alimited%20due%20to%20privacy%20issues%20or%20the%20absence%20of%20metering%20devices.%20Few-shot%0Alearning%20%28FSL%29%20has%20emerged%20as%20a%20promising%20solution%20for%20ECP%20modeling%20in%0Adata-scarce%20scenarios.%20Nevertheless%2C%20standard%20FSL%20methods%2C%20such%20as%20those%20used%0Afor%20images%2C%20are%20unsuitable%20for%20ECP%20modeling%20because%20%281%29%20these%20methods%20usually%0Aassume%20several%20source%20domains%20with%20sufficient%20data%20and%20several%20target%20domains.%0AHowever%2C%20in%20the%20context%20of%20ECP%20modeling%2C%20there%20may%20be%20thousands%20of%20source%0Adomains%20with%20a%20moderate%20amount%20of%20data%20and%20thousands%20of%20target%20domains.%20%282%29%0AStandard%20FSL%20methods%20usually%20involve%20cumbersome%20knowledge%20transfer%20mechanisms%2C%0Asuch%20as%20pre-training%20and%20fine-tuning%2C%20whereas%20ECP%20modeling%20requires%20more%0Alightweight%20methods.%20%283%29%20Deep%20learning%20models%20often%20lack%20explainability%2C%0Ahindering%20their%20application%20in%20industry.%20This%20paper%20proposes%20a%20novel%20FSL%20method%0Athat%20exploits%20Transformers%20and%20Gaussian%20Mixture%20Models%20%28GMMs%29%20for%20ECP%20modeling%0Ato%20address%20the%20above-described%20issues.%20Results%20show%20that%20our%20method%20can%0Aaccurately%20restore%20the%20complex%20ECP%20distribution%20with%20a%20minimal%20amount%20of%20ECP%0Adata%20%28e.g.%2C%20only%201.6%5C%25%20of%20the%20complete%20domain%20dataset%29%20while%20it%20outperforms%0Astate-of-the-art%20time%20series%20modeling%20methods%2C%20maintaining%20the%20advantages%20of%0Abeing%20both%20lightweight%20and%20interpretable.%20The%20project%20is%20open-sourced%20at%0Ahttps%3A//github.com/xiaweijie1996/TransformerEM-GMM.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08399v2&entry.124074799=Read"},
{"title": "Differentiable Logic Programming for Distant Supervision", "author": "Akihiro Takemura and Katsumi Inoue", "abstract": "  We introduce a new method for integrating neural networks with logic\nprogramming in Neural-Symbolic AI (NeSy), aimed at learning with distant\nsupervision, in which direct labels are unavailable. Unlike prior methods, our\napproach does not depend on symbolic solvers for reasoning about missing\nlabels. Instead, it evaluates logical implications and constraints in a\ndifferentiable manner by embedding both neural network outputs and logic\nprograms into matrices. This method facilitates more efficient learning under\ndistant supervision. We evaluated our approach against existing methods while\nmaintaining a constant volume of training data. The findings indicate that our\nmethod not only matches or exceeds the accuracy of other methods across various\ntasks but also speeds up the learning process. These results highlight the\npotential of our approach to enhance both accuracy and learning efficiency in\nNeSy applications.\n", "link": "http://arxiv.org/abs/2408.12591v1", "date": "2024-08-22", "relevancy": 1.8941, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4876}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4814}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.46}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Logic%20Programming%20for%20Distant%20Supervision&body=Title%3A%20Differentiable%20Logic%20Programming%20for%20Distant%20Supervision%0AAuthor%3A%20Akihiro%20Takemura%20and%20Katsumi%20Inoue%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20method%20for%20integrating%20neural%20networks%20with%20logic%0Aprogramming%20in%20Neural-Symbolic%20AI%20%28NeSy%29%2C%20aimed%20at%20learning%20with%20distant%0Asupervision%2C%20in%20which%20direct%20labels%20are%20unavailable.%20Unlike%20prior%20methods%2C%20our%0Aapproach%20does%20not%20depend%20on%20symbolic%20solvers%20for%20reasoning%20about%20missing%0Alabels.%20Instead%2C%20it%20evaluates%20logical%20implications%20and%20constraints%20in%20a%0Adifferentiable%20manner%20by%20embedding%20both%20neural%20network%20outputs%20and%20logic%0Aprograms%20into%20matrices.%20This%20method%20facilitates%20more%20efficient%20learning%20under%0Adistant%20supervision.%20We%20evaluated%20our%20approach%20against%20existing%20methods%20while%0Amaintaining%20a%20constant%20volume%20of%20training%20data.%20The%20findings%20indicate%20that%20our%0Amethod%20not%20only%20matches%20or%20exceeds%20the%20accuracy%20of%20other%20methods%20across%20various%0Atasks%20but%20also%20speeds%20up%20the%20learning%20process.%20These%20results%20highlight%20the%0Apotential%20of%20our%20approach%20to%20enhance%20both%20accuracy%20and%20learning%20efficiency%20in%0ANeSy%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Logic%2520Programming%2520for%2520Distant%2520Supervision%26entry.906535625%3DAkihiro%2520Takemura%2520and%2520Katsumi%2520Inoue%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520method%2520for%2520integrating%2520neural%2520networks%2520with%2520logic%250Aprogramming%2520in%2520Neural-Symbolic%2520AI%2520%2528NeSy%2529%252C%2520aimed%2520at%2520learning%2520with%2520distant%250Asupervision%252C%2520in%2520which%2520direct%2520labels%2520are%2520unavailable.%2520Unlike%2520prior%2520methods%252C%2520our%250Aapproach%2520does%2520not%2520depend%2520on%2520symbolic%2520solvers%2520for%2520reasoning%2520about%2520missing%250Alabels.%2520Instead%252C%2520it%2520evaluates%2520logical%2520implications%2520and%2520constraints%2520in%2520a%250Adifferentiable%2520manner%2520by%2520embedding%2520both%2520neural%2520network%2520outputs%2520and%2520logic%250Aprograms%2520into%2520matrices.%2520This%2520method%2520facilitates%2520more%2520efficient%2520learning%2520under%250Adistant%2520supervision.%2520We%2520evaluated%2520our%2520approach%2520against%2520existing%2520methods%2520while%250Amaintaining%2520a%2520constant%2520volume%2520of%2520training%2520data.%2520The%2520findings%2520indicate%2520that%2520our%250Amethod%2520not%2520only%2520matches%2520or%2520exceeds%2520the%2520accuracy%2520of%2520other%2520methods%2520across%2520various%250Atasks%2520but%2520also%2520speeds%2520up%2520the%2520learning%2520process.%2520These%2520results%2520highlight%2520the%250Apotential%2520of%2520our%2520approach%2520to%2520enhance%2520both%2520accuracy%2520and%2520learning%2520efficiency%2520in%250ANeSy%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Logic%20Programming%20for%20Distant%20Supervision&entry.906535625=Akihiro%20Takemura%20and%20Katsumi%20Inoue&entry.1292438233=%20%20We%20introduce%20a%20new%20method%20for%20integrating%20neural%20networks%20with%20logic%0Aprogramming%20in%20Neural-Symbolic%20AI%20%28NeSy%29%2C%20aimed%20at%20learning%20with%20distant%0Asupervision%2C%20in%20which%20direct%20labels%20are%20unavailable.%20Unlike%20prior%20methods%2C%20our%0Aapproach%20does%20not%20depend%20on%20symbolic%20solvers%20for%20reasoning%20about%20missing%0Alabels.%20Instead%2C%20it%20evaluates%20logical%20implications%20and%20constraints%20in%20a%0Adifferentiable%20manner%20by%20embedding%20both%20neural%20network%20outputs%20and%20logic%0Aprograms%20into%20matrices.%20This%20method%20facilitates%20more%20efficient%20learning%20under%0Adistant%20supervision.%20We%20evaluated%20our%20approach%20against%20existing%20methods%20while%0Amaintaining%20a%20constant%20volume%20of%20training%20data.%20The%20findings%20indicate%20that%20our%0Amethod%20not%20only%20matches%20or%20exceeds%20the%20accuracy%20of%20other%20methods%20across%20various%0Atasks%20but%20also%20speeds%20up%20the%20learning%20process.%20These%20results%20highlight%20the%0Apotential%20of%20our%20approach%20to%20enhance%20both%20accuracy%20and%20learning%20efficiency%20in%0ANeSy%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12591v1&entry.124074799=Read"},
{"title": "Stochastic Compositional Minimax Optimization with Provable Convergence\n  Guarantees", "author": "Yuyang Deng and Fuli Qiao and Mehrdad Mahdavi", "abstract": "  Stochastic compositional minimax problems are prevalent in machine learning,\nyet there are only limited established on the convergence of this class of\nproblems. In this paper, we propose a formal definition of the stochastic\ncompositional minimax problem, which involves optimizing a minimax loss with a\ncompositional structure either in primal , dual, or both primal and dual\nvariables. We introduce a simple yet effective algorithm, stochastically\nCorrected stOchastic gradient Descent Ascent (CODA), which is a descent ascent\ntype algorithm with compositional correction steps, and establish its\nconvergence rate in aforementioned three settings. In the presence of the\ncompositional structure in primal, the objective function typically becomes\nnonconvex in primal due to function composition. Thus, we consider the\nnonconvex-strongly-concave and nonconvex-concave settings and show that CODA\ncan efficiently converge to a stationary point. In the case of composition on\nthe dual, the objective function becomes nonconcave in the dual variable, and\nwe demonstrate convergence in the strongly-convex-nonconcave and\nconvex-nonconcave setting. In the case of composition on both variables, the\nprimal and dual variables may lose convexity and concavity, respectively.\nTherefore, we anaylze the convergence in weakly-convex-weakly-concave setting.\nWe also give a variance reduction version algorithm, CODA+, which achieves the\nbest known rate on nonconvex-strongly-concave and nonconvex-concave\ncompositional minimax problem. This work initiates the theoretical study of the\nstochastic compositional minimax problem on various settings and may inform\nmodern machine learning scenarios such as domain adaptation or robust\nmodel-agnostic meta-learning.\n", "link": "http://arxiv.org/abs/2408.12505v1", "date": "2024-08-22", "relevancy": 1.879, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4988}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4752}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stochastic%20Compositional%20Minimax%20Optimization%20with%20Provable%20Convergence%0A%20%20Guarantees&body=Title%3A%20Stochastic%20Compositional%20Minimax%20Optimization%20with%20Provable%20Convergence%0A%20%20Guarantees%0AAuthor%3A%20Yuyang%20Deng%20and%20Fuli%20Qiao%20and%20Mehrdad%20Mahdavi%0AAbstract%3A%20%20%20Stochastic%20compositional%20minimax%20problems%20are%20prevalent%20in%20machine%20learning%2C%0Ayet%20there%20are%20only%20limited%20established%20on%20the%20convergence%20of%20this%20class%20of%0Aproblems.%20In%20this%20paper%2C%20we%20propose%20a%20formal%20definition%20of%20the%20stochastic%0Acompositional%20minimax%20problem%2C%20which%20involves%20optimizing%20a%20minimax%20loss%20with%20a%0Acompositional%20structure%20either%20in%20primal%20%2C%20dual%2C%20or%20both%20primal%20and%20dual%0Avariables.%20We%20introduce%20a%20simple%20yet%20effective%20algorithm%2C%20stochastically%0ACorrected%20stOchastic%20gradient%20Descent%20Ascent%20%28CODA%29%2C%20which%20is%20a%20descent%20ascent%0Atype%20algorithm%20with%20compositional%20correction%20steps%2C%20and%20establish%20its%0Aconvergence%20rate%20in%20aforementioned%20three%20settings.%20In%20the%20presence%20of%20the%0Acompositional%20structure%20in%20primal%2C%20the%20objective%20function%20typically%20becomes%0Anonconvex%20in%20primal%20due%20to%20function%20composition.%20Thus%2C%20we%20consider%20the%0Anonconvex-strongly-concave%20and%20nonconvex-concave%20settings%20and%20show%20that%20CODA%0Acan%20efficiently%20converge%20to%20a%20stationary%20point.%20In%20the%20case%20of%20composition%20on%0Athe%20dual%2C%20the%20objective%20function%20becomes%20nonconcave%20in%20the%20dual%20variable%2C%20and%0Awe%20demonstrate%20convergence%20in%20the%20strongly-convex-nonconcave%20and%0Aconvex-nonconcave%20setting.%20In%20the%20case%20of%20composition%20on%20both%20variables%2C%20the%0Aprimal%20and%20dual%20variables%20may%20lose%20convexity%20and%20concavity%2C%20respectively.%0ATherefore%2C%20we%20anaylze%20the%20convergence%20in%20weakly-convex-weakly-concave%20setting.%0AWe%20also%20give%20a%20variance%20reduction%20version%20algorithm%2C%20CODA%2B%2C%20which%20achieves%20the%0Abest%20known%20rate%20on%20nonconvex-strongly-concave%20and%20nonconvex-concave%0Acompositional%20minimax%20problem.%20This%20work%20initiates%20the%20theoretical%20study%20of%20the%0Astochastic%20compositional%20minimax%20problem%20on%20various%20settings%20and%20may%20inform%0Amodern%20machine%20learning%20scenarios%20such%20as%20domain%20adaptation%20or%20robust%0Amodel-agnostic%20meta-learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12505v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStochastic%2520Compositional%2520Minimax%2520Optimization%2520with%2520Provable%2520Convergence%250A%2520%2520Guarantees%26entry.906535625%3DYuyang%2520Deng%2520and%2520Fuli%2520Qiao%2520and%2520Mehrdad%2520Mahdavi%26entry.1292438233%3D%2520%2520Stochastic%2520compositional%2520minimax%2520problems%2520are%2520prevalent%2520in%2520machine%2520learning%252C%250Ayet%2520there%2520are%2520only%2520limited%2520established%2520on%2520the%2520convergence%2520of%2520this%2520class%2520of%250Aproblems.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520formal%2520definition%2520of%2520the%2520stochastic%250Acompositional%2520minimax%2520problem%252C%2520which%2520involves%2520optimizing%2520a%2520minimax%2520loss%2520with%2520a%250Acompositional%2520structure%2520either%2520in%2520primal%2520%252C%2520dual%252C%2520or%2520both%2520primal%2520and%2520dual%250Avariables.%2520We%2520introduce%2520a%2520simple%2520yet%2520effective%2520algorithm%252C%2520stochastically%250ACorrected%2520stOchastic%2520gradient%2520Descent%2520Ascent%2520%2528CODA%2529%252C%2520which%2520is%2520a%2520descent%2520ascent%250Atype%2520algorithm%2520with%2520compositional%2520correction%2520steps%252C%2520and%2520establish%2520its%250Aconvergence%2520rate%2520in%2520aforementioned%2520three%2520settings.%2520In%2520the%2520presence%2520of%2520the%250Acompositional%2520structure%2520in%2520primal%252C%2520the%2520objective%2520function%2520typically%2520becomes%250Anonconvex%2520in%2520primal%2520due%2520to%2520function%2520composition.%2520Thus%252C%2520we%2520consider%2520the%250Anonconvex-strongly-concave%2520and%2520nonconvex-concave%2520settings%2520and%2520show%2520that%2520CODA%250Acan%2520efficiently%2520converge%2520to%2520a%2520stationary%2520point.%2520In%2520the%2520case%2520of%2520composition%2520on%250Athe%2520dual%252C%2520the%2520objective%2520function%2520becomes%2520nonconcave%2520in%2520the%2520dual%2520variable%252C%2520and%250Awe%2520demonstrate%2520convergence%2520in%2520the%2520strongly-convex-nonconcave%2520and%250Aconvex-nonconcave%2520setting.%2520In%2520the%2520case%2520of%2520composition%2520on%2520both%2520variables%252C%2520the%250Aprimal%2520and%2520dual%2520variables%2520may%2520lose%2520convexity%2520and%2520concavity%252C%2520respectively.%250ATherefore%252C%2520we%2520anaylze%2520the%2520convergence%2520in%2520weakly-convex-weakly-concave%2520setting.%250AWe%2520also%2520give%2520a%2520variance%2520reduction%2520version%2520algorithm%252C%2520CODA%252B%252C%2520which%2520achieves%2520the%250Abest%2520known%2520rate%2520on%2520nonconvex-strongly-concave%2520and%2520nonconvex-concave%250Acompositional%2520minimax%2520problem.%2520This%2520work%2520initiates%2520the%2520theoretical%2520study%2520of%2520the%250Astochastic%2520compositional%2520minimax%2520problem%2520on%2520various%2520settings%2520and%2520may%2520inform%250Amodern%2520machine%2520learning%2520scenarios%2520such%2520as%2520domain%2520adaptation%2520or%2520robust%250Amodel-agnostic%2520meta-learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12505v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stochastic%20Compositional%20Minimax%20Optimization%20with%20Provable%20Convergence%0A%20%20Guarantees&entry.906535625=Yuyang%20Deng%20and%20Fuli%20Qiao%20and%20Mehrdad%20Mahdavi&entry.1292438233=%20%20Stochastic%20compositional%20minimax%20problems%20are%20prevalent%20in%20machine%20learning%2C%0Ayet%20there%20are%20only%20limited%20established%20on%20the%20convergence%20of%20this%20class%20of%0Aproblems.%20In%20this%20paper%2C%20we%20propose%20a%20formal%20definition%20of%20the%20stochastic%0Acompositional%20minimax%20problem%2C%20which%20involves%20optimizing%20a%20minimax%20loss%20with%20a%0Acompositional%20structure%20either%20in%20primal%20%2C%20dual%2C%20or%20both%20primal%20and%20dual%0Avariables.%20We%20introduce%20a%20simple%20yet%20effective%20algorithm%2C%20stochastically%0ACorrected%20stOchastic%20gradient%20Descent%20Ascent%20%28CODA%29%2C%20which%20is%20a%20descent%20ascent%0Atype%20algorithm%20with%20compositional%20correction%20steps%2C%20and%20establish%20its%0Aconvergence%20rate%20in%20aforementioned%20three%20settings.%20In%20the%20presence%20of%20the%0Acompositional%20structure%20in%20primal%2C%20the%20objective%20function%20typically%20becomes%0Anonconvex%20in%20primal%20due%20to%20function%20composition.%20Thus%2C%20we%20consider%20the%0Anonconvex-strongly-concave%20and%20nonconvex-concave%20settings%20and%20show%20that%20CODA%0Acan%20efficiently%20converge%20to%20a%20stationary%20point.%20In%20the%20case%20of%20composition%20on%0Athe%20dual%2C%20the%20objective%20function%20becomes%20nonconcave%20in%20the%20dual%20variable%2C%20and%0Awe%20demonstrate%20convergence%20in%20the%20strongly-convex-nonconcave%20and%0Aconvex-nonconcave%20setting.%20In%20the%20case%20of%20composition%20on%20both%20variables%2C%20the%0Aprimal%20and%20dual%20variables%20may%20lose%20convexity%20and%20concavity%2C%20respectively.%0ATherefore%2C%20we%20anaylze%20the%20convergence%20in%20weakly-convex-weakly-concave%20setting.%0AWe%20also%20give%20a%20variance%20reduction%20version%20algorithm%2C%20CODA%2B%2C%20which%20achieves%20the%0Abest%20known%20rate%20on%20nonconvex-strongly-concave%20and%20nonconvex-concave%0Acompositional%20minimax%20problem.%20This%20work%20initiates%20the%20theoretical%20study%20of%20the%0Astochastic%20compositional%20minimax%20problem%20on%20various%20settings%20and%20may%20inform%0Amodern%20machine%20learning%20scenarios%20such%20as%20domain%20adaptation%20or%20robust%0Amodel-agnostic%20meta-learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12505v1&entry.124074799=Read"},
{"title": "Dynamics of Meta-learning Representation in the Teacher-student Scenario", "author": "Hui Wang and Cho Tung Yip and Bo Li", "abstract": "  Gradient-based meta-learning algorithms have gained popularity for their\nability to train models on new tasks using limited data. Empirical observations\nindicate that such algorithms are able to learn a shared representation across\ntasks, which is regarded as a key factor in their success. However, the\nin-depth theoretical understanding of the learning dynamics and the origin of\nthe shared representation remains underdeveloped. In this work, we investigate\nthe meta-learning dynamics of the non-linear two-layer neural networks trained\non streaming tasks in the teach-student scenario. Through the lens of\nstatistical physics analysis, we characterize the macroscopic behavior of the\nmeta-training processes, the formation of the shared representation, and the\ngeneralization ability of the model on new tasks. The analysis also points to\nthe importance of the choice of certain hyper-parameters of the learning\nalgorithms.\n", "link": "http://arxiv.org/abs/2408.12545v1", "date": "2024-08-22", "relevancy": 1.8671, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4687}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4685}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamics%20of%20Meta-learning%20Representation%20in%20the%20Teacher-student%20Scenario&body=Title%3A%20Dynamics%20of%20Meta-learning%20Representation%20in%20the%20Teacher-student%20Scenario%0AAuthor%3A%20Hui%20Wang%20and%20Cho%20Tung%20Yip%20and%20Bo%20Li%0AAbstract%3A%20%20%20Gradient-based%20meta-learning%20algorithms%20have%20gained%20popularity%20for%20their%0Aability%20to%20train%20models%20on%20new%20tasks%20using%20limited%20data.%20Empirical%20observations%0Aindicate%20that%20such%20algorithms%20are%20able%20to%20learn%20a%20shared%20representation%20across%0Atasks%2C%20which%20is%20regarded%20as%20a%20key%20factor%20in%20their%20success.%20However%2C%20the%0Ain-depth%20theoretical%20understanding%20of%20the%20learning%20dynamics%20and%20the%20origin%20of%0Athe%20shared%20representation%20remains%20underdeveloped.%20In%20this%20work%2C%20we%20investigate%0Athe%20meta-learning%20dynamics%20of%20the%20non-linear%20two-layer%20neural%20networks%20trained%0Aon%20streaming%20tasks%20in%20the%20teach-student%20scenario.%20Through%20the%20lens%20of%0Astatistical%20physics%20analysis%2C%20we%20characterize%20the%20macroscopic%20behavior%20of%20the%0Ameta-training%20processes%2C%20the%20formation%20of%20the%20shared%20representation%2C%20and%20the%0Ageneralization%20ability%20of%20the%20model%20on%20new%20tasks.%20The%20analysis%20also%20points%20to%0Athe%20importance%20of%20the%20choice%20of%20certain%20hyper-parameters%20of%20the%20learning%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamics%2520of%2520Meta-learning%2520Representation%2520in%2520the%2520Teacher-student%2520Scenario%26entry.906535625%3DHui%2520Wang%2520and%2520Cho%2520Tung%2520Yip%2520and%2520Bo%2520Li%26entry.1292438233%3D%2520%2520Gradient-based%2520meta-learning%2520algorithms%2520have%2520gained%2520popularity%2520for%2520their%250Aability%2520to%2520train%2520models%2520on%2520new%2520tasks%2520using%2520limited%2520data.%2520Empirical%2520observations%250Aindicate%2520that%2520such%2520algorithms%2520are%2520able%2520to%2520learn%2520a%2520shared%2520representation%2520across%250Atasks%252C%2520which%2520is%2520regarded%2520as%2520a%2520key%2520factor%2520in%2520their%2520success.%2520However%252C%2520the%250Ain-depth%2520theoretical%2520understanding%2520of%2520the%2520learning%2520dynamics%2520and%2520the%2520origin%2520of%250Athe%2520shared%2520representation%2520remains%2520underdeveloped.%2520In%2520this%2520work%252C%2520we%2520investigate%250Athe%2520meta-learning%2520dynamics%2520of%2520the%2520non-linear%2520two-layer%2520neural%2520networks%2520trained%250Aon%2520streaming%2520tasks%2520in%2520the%2520teach-student%2520scenario.%2520Through%2520the%2520lens%2520of%250Astatistical%2520physics%2520analysis%252C%2520we%2520characterize%2520the%2520macroscopic%2520behavior%2520of%2520the%250Ameta-training%2520processes%252C%2520the%2520formation%2520of%2520the%2520shared%2520representation%252C%2520and%2520the%250Ageneralization%2520ability%2520of%2520the%2520model%2520on%2520new%2520tasks.%2520The%2520analysis%2520also%2520points%2520to%250Athe%2520importance%2520of%2520the%2520choice%2520of%2520certain%2520hyper-parameters%2520of%2520the%2520learning%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamics%20of%20Meta-learning%20Representation%20in%20the%20Teacher-student%20Scenario&entry.906535625=Hui%20Wang%20and%20Cho%20Tung%20Yip%20and%20Bo%20Li&entry.1292438233=%20%20Gradient-based%20meta-learning%20algorithms%20have%20gained%20popularity%20for%20their%0Aability%20to%20train%20models%20on%20new%20tasks%20using%20limited%20data.%20Empirical%20observations%0Aindicate%20that%20such%20algorithms%20are%20able%20to%20learn%20a%20shared%20representation%20across%0Atasks%2C%20which%20is%20regarded%20as%20a%20key%20factor%20in%20their%20success.%20However%2C%20the%0Ain-depth%20theoretical%20understanding%20of%20the%20learning%20dynamics%20and%20the%20origin%20of%0Athe%20shared%20representation%20remains%20underdeveloped.%20In%20this%20work%2C%20we%20investigate%0Athe%20meta-learning%20dynamics%20of%20the%20non-linear%20two-layer%20neural%20networks%20trained%0Aon%20streaming%20tasks%20in%20the%20teach-student%20scenario.%20Through%20the%20lens%20of%0Astatistical%20physics%20analysis%2C%20we%20characterize%20the%20macroscopic%20behavior%20of%20the%0Ameta-training%20processes%2C%20the%20formation%20of%20the%20shared%20representation%2C%20and%20the%0Ageneralization%20ability%20of%20the%20model%20on%20new%20tasks.%20The%20analysis%20also%20points%20to%0Athe%20importance%20of%20the%20choice%20of%20certain%20hyper-parameters%20of%20the%20learning%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12545v1&entry.124074799=Read"},
{"title": "Characterization, Experimental Validation and Pilot User Study of the\n  Vibro-Inertial Bionic Enhancement System (VIBES)", "author": "Alessia S. Ivani and Federica Barontini and Manuel G. Catalano and Giorgio Grioli and Matteo Bianchi and Antonio Bicchi", "abstract": "  This study presents the characterization and validation of the VIBES, a\nwearable vibrotactile device that provides high-frequency tactile information\nembedded in a prosthetic socket. A psychophysical characterization involving\nten able-bodied participants is performed to compute the Just Noticeable\nDifference (JND) related to the discrimination of vibrotactile cues delivered\non the skin in two forearm positions, with the goal of optimising vibrotactile\nactuator position to maximise perceptual response. Furthermore, system\nperformance is validated and tested both with ten able-bodied participants and\none prosthesis user considering three tasks. More specifically, in the Active\nTexture Identification, Slippage and Fragile Object Experiments, we investigate\nif the VIBES could enhance users' roughness discrimination and manual usability\nand dexterity. Finally, we test the effect of the vibrotactile system on\nprosthetic embodiment in a Rubber Hand Illusion (RHI) task. Results show the\nsystem's effectiveness in conveying contact and texture cues, making it a\npotential tool to restore sensory feedback and enhance the embodiment in\nprosthetic users.\n", "link": "http://arxiv.org/abs/2408.12375v1", "date": "2024-08-22", "relevancy": 1.8657, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4985}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4899}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterization%2C%20Experimental%20Validation%20and%20Pilot%20User%20Study%20of%20the%0A%20%20Vibro-Inertial%20Bionic%20Enhancement%20System%20%28VIBES%29&body=Title%3A%20Characterization%2C%20Experimental%20Validation%20and%20Pilot%20User%20Study%20of%20the%0A%20%20Vibro-Inertial%20Bionic%20Enhancement%20System%20%28VIBES%29%0AAuthor%3A%20Alessia%20S.%20Ivani%20and%20Federica%20Barontini%20and%20Manuel%20G.%20Catalano%20and%20Giorgio%20Grioli%20and%20Matteo%20Bianchi%20and%20Antonio%20Bicchi%0AAbstract%3A%20%20%20This%20study%20presents%20the%20characterization%20and%20validation%20of%20the%20VIBES%2C%20a%0Awearable%20vibrotactile%20device%20that%20provides%20high-frequency%20tactile%20information%0Aembedded%20in%20a%20prosthetic%20socket.%20A%20psychophysical%20characterization%20involving%0Aten%20able-bodied%20participants%20is%20performed%20to%20compute%20the%20Just%20Noticeable%0ADifference%20%28JND%29%20related%20to%20the%20discrimination%20of%20vibrotactile%20cues%20delivered%0Aon%20the%20skin%20in%20two%20forearm%20positions%2C%20with%20the%20goal%20of%20optimising%20vibrotactile%0Aactuator%20position%20to%20maximise%20perceptual%20response.%20Furthermore%2C%20system%0Aperformance%20is%20validated%20and%20tested%20both%20with%20ten%20able-bodied%20participants%20and%0Aone%20prosthesis%20user%20considering%20three%20tasks.%20More%20specifically%2C%20in%20the%20Active%0ATexture%20Identification%2C%20Slippage%20and%20Fragile%20Object%20Experiments%2C%20we%20investigate%0Aif%20the%20VIBES%20could%20enhance%20users%27%20roughness%20discrimination%20and%20manual%20usability%0Aand%20dexterity.%20Finally%2C%20we%20test%20the%20effect%20of%20the%20vibrotactile%20system%20on%0Aprosthetic%20embodiment%20in%20a%20Rubber%20Hand%20Illusion%20%28RHI%29%20task.%20Results%20show%20the%0Asystem%27s%20effectiveness%20in%20conveying%20contact%20and%20texture%20cues%2C%20making%20it%20a%0Apotential%20tool%20to%20restore%20sensory%20feedback%20and%20enhance%20the%20embodiment%20in%0Aprosthetic%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12375v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterization%252C%2520Experimental%2520Validation%2520and%2520Pilot%2520User%2520Study%2520of%2520the%250A%2520%2520Vibro-Inertial%2520Bionic%2520Enhancement%2520System%2520%2528VIBES%2529%26entry.906535625%3DAlessia%2520S.%2520Ivani%2520and%2520Federica%2520Barontini%2520and%2520Manuel%2520G.%2520Catalano%2520and%2520Giorgio%2520Grioli%2520and%2520Matteo%2520Bianchi%2520and%2520Antonio%2520Bicchi%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520the%2520characterization%2520and%2520validation%2520of%2520the%2520VIBES%252C%2520a%250Awearable%2520vibrotactile%2520device%2520that%2520provides%2520high-frequency%2520tactile%2520information%250Aembedded%2520in%2520a%2520prosthetic%2520socket.%2520A%2520psychophysical%2520characterization%2520involving%250Aten%2520able-bodied%2520participants%2520is%2520performed%2520to%2520compute%2520the%2520Just%2520Noticeable%250ADifference%2520%2528JND%2529%2520related%2520to%2520the%2520discrimination%2520of%2520vibrotactile%2520cues%2520delivered%250Aon%2520the%2520skin%2520in%2520two%2520forearm%2520positions%252C%2520with%2520the%2520goal%2520of%2520optimising%2520vibrotactile%250Aactuator%2520position%2520to%2520maximise%2520perceptual%2520response.%2520Furthermore%252C%2520system%250Aperformance%2520is%2520validated%2520and%2520tested%2520both%2520with%2520ten%2520able-bodied%2520participants%2520and%250Aone%2520prosthesis%2520user%2520considering%2520three%2520tasks.%2520More%2520specifically%252C%2520in%2520the%2520Active%250ATexture%2520Identification%252C%2520Slippage%2520and%2520Fragile%2520Object%2520Experiments%252C%2520we%2520investigate%250Aif%2520the%2520VIBES%2520could%2520enhance%2520users%2527%2520roughness%2520discrimination%2520and%2520manual%2520usability%250Aand%2520dexterity.%2520Finally%252C%2520we%2520test%2520the%2520effect%2520of%2520the%2520vibrotactile%2520system%2520on%250Aprosthetic%2520embodiment%2520in%2520a%2520Rubber%2520Hand%2520Illusion%2520%2528RHI%2529%2520task.%2520Results%2520show%2520the%250Asystem%2527s%2520effectiveness%2520in%2520conveying%2520contact%2520and%2520texture%2520cues%252C%2520making%2520it%2520a%250Apotential%2520tool%2520to%2520restore%2520sensory%2520feedback%2520and%2520enhance%2520the%2520embodiment%2520in%250Aprosthetic%2520users.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12375v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterization%2C%20Experimental%20Validation%20and%20Pilot%20User%20Study%20of%20the%0A%20%20Vibro-Inertial%20Bionic%20Enhancement%20System%20%28VIBES%29&entry.906535625=Alessia%20S.%20Ivani%20and%20Federica%20Barontini%20and%20Manuel%20G.%20Catalano%20and%20Giorgio%20Grioli%20and%20Matteo%20Bianchi%20and%20Antonio%20Bicchi&entry.1292438233=%20%20This%20study%20presents%20the%20characterization%20and%20validation%20of%20the%20VIBES%2C%20a%0Awearable%20vibrotactile%20device%20that%20provides%20high-frequency%20tactile%20information%0Aembedded%20in%20a%20prosthetic%20socket.%20A%20psychophysical%20characterization%20involving%0Aten%20able-bodied%20participants%20is%20performed%20to%20compute%20the%20Just%20Noticeable%0ADifference%20%28JND%29%20related%20to%20the%20discrimination%20of%20vibrotactile%20cues%20delivered%0Aon%20the%20skin%20in%20two%20forearm%20positions%2C%20with%20the%20goal%20of%20optimising%20vibrotactile%0Aactuator%20position%20to%20maximise%20perceptual%20response.%20Furthermore%2C%20system%0Aperformance%20is%20validated%20and%20tested%20both%20with%20ten%20able-bodied%20participants%20and%0Aone%20prosthesis%20user%20considering%20three%20tasks.%20More%20specifically%2C%20in%20the%20Active%0ATexture%20Identification%2C%20Slippage%20and%20Fragile%20Object%20Experiments%2C%20we%20investigate%0Aif%20the%20VIBES%20could%20enhance%20users%27%20roughness%20discrimination%20and%20manual%20usability%0Aand%20dexterity.%20Finally%2C%20we%20test%20the%20effect%20of%20the%20vibrotactile%20system%20on%0Aprosthetic%20embodiment%20in%20a%20Rubber%20Hand%20Illusion%20%28RHI%29%20task.%20Results%20show%20the%0Asystem%27s%20effectiveness%20in%20conveying%20contact%20and%20texture%20cues%2C%20making%20it%20a%0Apotential%20tool%20to%20restore%20sensory%20feedback%20and%20enhance%20the%20embodiment%20in%0Aprosthetic%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12375v1&entry.124074799=Read"},
{"title": "WCEbleedGen: A wireless capsule endoscopy dataset and its benchmarking\n  for automatic bleeding classification, detection, and segmentation", "author": "Palak Handa and Manas Dhir and Amirreza Mahbod and Florian Schwarzhans and Ramona Woitek and Nidhi Goel and Deepak Gunjan", "abstract": "  Computer-based analysis of Wireless Capsule Endoscopy (WCE) is crucial.\nHowever, a medically annotated WCE dataset for training and evaluation of\nautomatic classification, detection, and segmentation of bleeding and\nnon-bleeding frames is currently lacking. The present work focused on\ndevelopment of a medically annotated WCE dataset called WCEbleedGen for\nautomatic classification, detection, and segmentation of bleeding and\nnon-bleeding frames. It comprises 2,618 WCE bleeding and non-bleeding frames\nwhich were collected from various internet resources and existing WCE datasets.\nA comprehensive benchmarking and evaluation of the developed dataset was done\nusing nine classification-based, three detection-based, and three\nsegmentation-based deep learning models. The dataset is of high-quality, is\nclass-balanced and contains single and multiple bleeding sites. Overall, our\nstandard benchmark results show that Visual Geometric Group (VGG) 19, You Only\nLook Once version 8 nano (YOLOv8n), and Link network (Linknet) performed best\nin automatic classification, detection, and segmentation-based evaluations,\nrespectively. Automatic bleeding diagnosis is crucial for WCE video\ninterpretations. This diverse dataset will aid in developing of real-time,\nmulti-task learning-based innovative solutions for automatic bleeding diagnosis\nin WCE. The dataset and code are publicly available at\nhttps://zenodo.org/records/10156571 and\nhttps://github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset.\n", "link": "http://arxiv.org/abs/2408.12466v1", "date": "2024-08-22", "relevancy": 1.8642, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4905}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4679}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WCEbleedGen%3A%20A%20wireless%20capsule%20endoscopy%20dataset%20and%20its%20benchmarking%0A%20%20for%20automatic%20bleeding%20classification%2C%20detection%2C%20and%20segmentation&body=Title%3A%20WCEbleedGen%3A%20A%20wireless%20capsule%20endoscopy%20dataset%20and%20its%20benchmarking%0A%20%20for%20automatic%20bleeding%20classification%2C%20detection%2C%20and%20segmentation%0AAuthor%3A%20Palak%20Handa%20and%20Manas%20Dhir%20and%20Amirreza%20Mahbod%20and%20Florian%20Schwarzhans%20and%20Ramona%20Woitek%20and%20Nidhi%20Goel%20and%20Deepak%20Gunjan%0AAbstract%3A%20%20%20Computer-based%20analysis%20of%20Wireless%20Capsule%20Endoscopy%20%28WCE%29%20is%20crucial.%0AHowever%2C%20a%20medically%20annotated%20WCE%20dataset%20for%20training%20and%20evaluation%20of%0Aautomatic%20classification%2C%20detection%2C%20and%20segmentation%20of%20bleeding%20and%0Anon-bleeding%20frames%20is%20currently%20lacking.%20The%20present%20work%20focused%20on%0Adevelopment%20of%20a%20medically%20annotated%20WCE%20dataset%20called%20WCEbleedGen%20for%0Aautomatic%20classification%2C%20detection%2C%20and%20segmentation%20of%20bleeding%20and%0Anon-bleeding%20frames.%20It%20comprises%202%2C618%20WCE%20bleeding%20and%20non-bleeding%20frames%0Awhich%20were%20collected%20from%20various%20internet%20resources%20and%20existing%20WCE%20datasets.%0AA%20comprehensive%20benchmarking%20and%20evaluation%20of%20the%20developed%20dataset%20was%20done%0Ausing%20nine%20classification-based%2C%20three%20detection-based%2C%20and%20three%0Asegmentation-based%20deep%20learning%20models.%20The%20dataset%20is%20of%20high-quality%2C%20is%0Aclass-balanced%20and%20contains%20single%20and%20multiple%20bleeding%20sites.%20Overall%2C%20our%0Astandard%20benchmark%20results%20show%20that%20Visual%20Geometric%20Group%20%28VGG%29%2019%2C%20You%20Only%0ALook%20Once%20version%208%20nano%20%28YOLOv8n%29%2C%20and%20Link%20network%20%28Linknet%29%20performed%20best%0Ain%20automatic%20classification%2C%20detection%2C%20and%20segmentation-based%20evaluations%2C%0Arespectively.%20Automatic%20bleeding%20diagnosis%20is%20crucial%20for%20WCE%20video%0Ainterpretations.%20This%20diverse%20dataset%20will%20aid%20in%20developing%20of%20real-time%2C%0Amulti-task%20learning-based%20innovative%20solutions%20for%20automatic%20bleeding%20diagnosis%0Ain%20WCE.%20The%20dataset%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//zenodo.org/records/10156571%20and%0Ahttps%3A//github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWCEbleedGen%253A%2520A%2520wireless%2520capsule%2520endoscopy%2520dataset%2520and%2520its%2520benchmarking%250A%2520%2520for%2520automatic%2520bleeding%2520classification%252C%2520detection%252C%2520and%2520segmentation%26entry.906535625%3DPalak%2520Handa%2520and%2520Manas%2520Dhir%2520and%2520Amirreza%2520Mahbod%2520and%2520Florian%2520Schwarzhans%2520and%2520Ramona%2520Woitek%2520and%2520Nidhi%2520Goel%2520and%2520Deepak%2520Gunjan%26entry.1292438233%3D%2520%2520Computer-based%2520analysis%2520of%2520Wireless%2520Capsule%2520Endoscopy%2520%2528WCE%2529%2520is%2520crucial.%250AHowever%252C%2520a%2520medically%2520annotated%2520WCE%2520dataset%2520for%2520training%2520and%2520evaluation%2520of%250Aautomatic%2520classification%252C%2520detection%252C%2520and%2520segmentation%2520of%2520bleeding%2520and%250Anon-bleeding%2520frames%2520is%2520currently%2520lacking.%2520The%2520present%2520work%2520focused%2520on%250Adevelopment%2520of%2520a%2520medically%2520annotated%2520WCE%2520dataset%2520called%2520WCEbleedGen%2520for%250Aautomatic%2520classification%252C%2520detection%252C%2520and%2520segmentation%2520of%2520bleeding%2520and%250Anon-bleeding%2520frames.%2520It%2520comprises%25202%252C618%2520WCE%2520bleeding%2520and%2520non-bleeding%2520frames%250Awhich%2520were%2520collected%2520from%2520various%2520internet%2520resources%2520and%2520existing%2520WCE%2520datasets.%250AA%2520comprehensive%2520benchmarking%2520and%2520evaluation%2520of%2520the%2520developed%2520dataset%2520was%2520done%250Ausing%2520nine%2520classification-based%252C%2520three%2520detection-based%252C%2520and%2520three%250Asegmentation-based%2520deep%2520learning%2520models.%2520The%2520dataset%2520is%2520of%2520high-quality%252C%2520is%250Aclass-balanced%2520and%2520contains%2520single%2520and%2520multiple%2520bleeding%2520sites.%2520Overall%252C%2520our%250Astandard%2520benchmark%2520results%2520show%2520that%2520Visual%2520Geometric%2520Group%2520%2528VGG%2529%252019%252C%2520You%2520Only%250ALook%2520Once%2520version%25208%2520nano%2520%2528YOLOv8n%2529%252C%2520and%2520Link%2520network%2520%2528Linknet%2529%2520performed%2520best%250Ain%2520automatic%2520classification%252C%2520detection%252C%2520and%2520segmentation-based%2520evaluations%252C%250Arespectively.%2520Automatic%2520bleeding%2520diagnosis%2520is%2520crucial%2520for%2520WCE%2520video%250Ainterpretations.%2520This%2520diverse%2520dataset%2520will%2520aid%2520in%2520developing%2520of%2520real-time%252C%250Amulti-task%2520learning-based%2520innovative%2520solutions%2520for%2520automatic%2520bleeding%2520diagnosis%250Ain%2520WCE.%2520The%2520dataset%2520and%2520code%2520are%2520publicly%2520available%2520at%250Ahttps%253A//zenodo.org/records/10156571%2520and%250Ahttps%253A//github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WCEbleedGen%3A%20A%20wireless%20capsule%20endoscopy%20dataset%20and%20its%20benchmarking%0A%20%20for%20automatic%20bleeding%20classification%2C%20detection%2C%20and%20segmentation&entry.906535625=Palak%20Handa%20and%20Manas%20Dhir%20and%20Amirreza%20Mahbod%20and%20Florian%20Schwarzhans%20and%20Ramona%20Woitek%20and%20Nidhi%20Goel%20and%20Deepak%20Gunjan&entry.1292438233=%20%20Computer-based%20analysis%20of%20Wireless%20Capsule%20Endoscopy%20%28WCE%29%20is%20crucial.%0AHowever%2C%20a%20medically%20annotated%20WCE%20dataset%20for%20training%20and%20evaluation%20of%0Aautomatic%20classification%2C%20detection%2C%20and%20segmentation%20of%20bleeding%20and%0Anon-bleeding%20frames%20is%20currently%20lacking.%20The%20present%20work%20focused%20on%0Adevelopment%20of%20a%20medically%20annotated%20WCE%20dataset%20called%20WCEbleedGen%20for%0Aautomatic%20classification%2C%20detection%2C%20and%20segmentation%20of%20bleeding%20and%0Anon-bleeding%20frames.%20It%20comprises%202%2C618%20WCE%20bleeding%20and%20non-bleeding%20frames%0Awhich%20were%20collected%20from%20various%20internet%20resources%20and%20existing%20WCE%20datasets.%0AA%20comprehensive%20benchmarking%20and%20evaluation%20of%20the%20developed%20dataset%20was%20done%0Ausing%20nine%20classification-based%2C%20three%20detection-based%2C%20and%20three%0Asegmentation-based%20deep%20learning%20models.%20The%20dataset%20is%20of%20high-quality%2C%20is%0Aclass-balanced%20and%20contains%20single%20and%20multiple%20bleeding%20sites.%20Overall%2C%20our%0Astandard%20benchmark%20results%20show%20that%20Visual%20Geometric%20Group%20%28VGG%29%2019%2C%20You%20Only%0ALook%20Once%20version%208%20nano%20%28YOLOv8n%29%2C%20and%20Link%20network%20%28Linknet%29%20performed%20best%0Ain%20automatic%20classification%2C%20detection%2C%20and%20segmentation-based%20evaluations%2C%0Arespectively.%20Automatic%20bleeding%20diagnosis%20is%20crucial%20for%20WCE%20video%0Ainterpretations.%20This%20diverse%20dataset%20will%20aid%20in%20developing%20of%20real-time%2C%0Amulti-task%20learning-based%20innovative%20solutions%20for%20automatic%20bleeding%20diagnosis%0Ain%20WCE.%20The%20dataset%20and%20code%20are%20publicly%20available%20at%0Ahttps%3A//zenodo.org/records/10156571%20and%0Ahttps%3A//github.com/misahub2023/Benchmarking-Codes-of-the-WCEBleedGen-dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12466v1&entry.124074799=Read"},
{"title": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender\n  Bias in Large Language Models", "author": "Kunsheng Tang and Wenbo Zhou and Jie Zhang and Aishan Liu and Gelei Deng and Shuai Li and Peigui Qi and Weiming Zhang and Tianwei Zhang and Nenghai Yu", "abstract": "  Large language models (LLMs) have exhibited remarkable capabilities in\nnatural language generation, but they have also been observed to magnify\nsocietal biases, particularly those related to gender. In response to this\nissue, several benchmarks have been proposed to assess gender bias in LLMs.\nHowever, these benchmarks often lack practical flexibility or inadvertently\nintroduce biases. To address these shortcomings, we introduce GenderCARE, a\ncomprehensive framework that encompasses innovative Criteria, bias Assessment,\nReduction techniques, and Evaluation metrics for quantifying and mitigating\ngender bias in LLMs. To begin, we establish pioneering criteria for gender\nequality benchmarks, spanning dimensions such as inclusivity, diversity,\nexplainability, objectivity, robustness, and realisticity. Guided by these\ncriteria, we construct GenderPair, a novel pair-based benchmark designed to\nassess gender bias in LLMs comprehensively. Our benchmark provides standardized\nand realistic evaluations, including previously overlooked gender groups such\nas transgender and non-binary individuals. Furthermore, we develop effective\ndebiasing techniques that incorporate counterfactual data augmentation and\nspecialized fine-tuning strategies to reduce gender bias in LLMs without\ncompromising their overall performance. Extensive experiments demonstrate a\nsignificant reduction in various gender bias benchmarks, with reductions\npeaking at over 90% and averaging above 35% across 17 different LLMs.\nImportantly, these reductions come with minimal variability in mainstream\nlanguage tasks, remaining below 2%. By offering a realistic assessment and\ntailored reduction of gender biases, we hope that our GenderCARE can represent\na significant step towards achieving fairness and equity in LLMs. More details\nare available at https://github.com/kstanghere/GenderCARE-ccs24.\n", "link": "http://arxiv.org/abs/2408.12494v1", "date": "2024-08-22", "relevancy": 1.864, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4819}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4597}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenderCARE%3A%20A%20Comprehensive%20Framework%20for%20Assessing%20and%20Reducing%20Gender%0A%20%20Bias%20in%20Large%20Language%20Models&body=Title%3A%20GenderCARE%3A%20A%20Comprehensive%20Framework%20for%20Assessing%20and%20Reducing%20Gender%0A%20%20Bias%20in%20Large%20Language%20Models%0AAuthor%3A%20Kunsheng%20Tang%20and%20Wenbo%20Zhou%20and%20Jie%20Zhang%20and%20Aishan%20Liu%20and%20Gelei%20Deng%20and%20Shuai%20Li%20and%20Peigui%20Qi%20and%20Weiming%20Zhang%20and%20Tianwei%20Zhang%20and%20Nenghai%20Yu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20remarkable%20capabilities%20in%0Anatural%20language%20generation%2C%20but%20they%20have%20also%20been%20observed%20to%20magnify%0Asocietal%20biases%2C%20particularly%20those%20related%20to%20gender.%20In%20response%20to%20this%0Aissue%2C%20several%20benchmarks%20have%20been%20proposed%20to%20assess%20gender%20bias%20in%20LLMs.%0AHowever%2C%20these%20benchmarks%20often%20lack%20practical%20flexibility%20or%20inadvertently%0Aintroduce%20biases.%20To%20address%20these%20shortcomings%2C%20we%20introduce%20GenderCARE%2C%20a%0Acomprehensive%20framework%20that%20encompasses%20innovative%20Criteria%2C%20bias%20Assessment%2C%0AReduction%20techniques%2C%20and%20Evaluation%20metrics%20for%20quantifying%20and%20mitigating%0Agender%20bias%20in%20LLMs.%20To%20begin%2C%20we%20establish%20pioneering%20criteria%20for%20gender%0Aequality%20benchmarks%2C%20spanning%20dimensions%20such%20as%20inclusivity%2C%20diversity%2C%0Aexplainability%2C%20objectivity%2C%20robustness%2C%20and%20realisticity.%20Guided%20by%20these%0Acriteria%2C%20we%20construct%20GenderPair%2C%20a%20novel%20pair-based%20benchmark%20designed%20to%0Aassess%20gender%20bias%20in%20LLMs%20comprehensively.%20Our%20benchmark%20provides%20standardized%0Aand%20realistic%20evaluations%2C%20including%20previously%20overlooked%20gender%20groups%20such%0Aas%20transgender%20and%20non-binary%20individuals.%20Furthermore%2C%20we%20develop%20effective%0Adebiasing%20techniques%20that%20incorporate%20counterfactual%20data%20augmentation%20and%0Aspecialized%20fine-tuning%20strategies%20to%20reduce%20gender%20bias%20in%20LLMs%20without%0Acompromising%20their%20overall%20performance.%20Extensive%20experiments%20demonstrate%20a%0Asignificant%20reduction%20in%20various%20gender%20bias%20benchmarks%2C%20with%20reductions%0Apeaking%20at%20over%2090%25%20and%20averaging%20above%2035%25%20across%2017%20different%20LLMs.%0AImportantly%2C%20these%20reductions%20come%20with%20minimal%20variability%20in%20mainstream%0Alanguage%20tasks%2C%20remaining%20below%202%25.%20By%20offering%20a%20realistic%20assessment%20and%0Atailored%20reduction%20of%20gender%20biases%2C%20we%20hope%20that%20our%20GenderCARE%20can%20represent%0Aa%20significant%20step%20towards%20achieving%20fairness%20and%20equity%20in%20LLMs.%20More%20details%0Aare%20available%20at%20https%3A//github.com/kstanghere/GenderCARE-ccs24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenderCARE%253A%2520A%2520Comprehensive%2520Framework%2520for%2520Assessing%2520and%2520Reducing%2520Gender%250A%2520%2520Bias%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DKunsheng%2520Tang%2520and%2520Wenbo%2520Zhou%2520and%2520Jie%2520Zhang%2520and%2520Aishan%2520Liu%2520and%2520Gelei%2520Deng%2520and%2520Shuai%2520Li%2520and%2520Peigui%2520Qi%2520and%2520Weiming%2520Zhang%2520and%2520Tianwei%2520Zhang%2520and%2520Nenghai%2520Yu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520exhibited%2520remarkable%2520capabilities%2520in%250Anatural%2520language%2520generation%252C%2520but%2520they%2520have%2520also%2520been%2520observed%2520to%2520magnify%250Asocietal%2520biases%252C%2520particularly%2520those%2520related%2520to%2520gender.%2520In%2520response%2520to%2520this%250Aissue%252C%2520several%2520benchmarks%2520have%2520been%2520proposed%2520to%2520assess%2520gender%2520bias%2520in%2520LLMs.%250AHowever%252C%2520these%2520benchmarks%2520often%2520lack%2520practical%2520flexibility%2520or%2520inadvertently%250Aintroduce%2520biases.%2520To%2520address%2520these%2520shortcomings%252C%2520we%2520introduce%2520GenderCARE%252C%2520a%250Acomprehensive%2520framework%2520that%2520encompasses%2520innovative%2520Criteria%252C%2520bias%2520Assessment%252C%250AReduction%2520techniques%252C%2520and%2520Evaluation%2520metrics%2520for%2520quantifying%2520and%2520mitigating%250Agender%2520bias%2520in%2520LLMs.%2520To%2520begin%252C%2520we%2520establish%2520pioneering%2520criteria%2520for%2520gender%250Aequality%2520benchmarks%252C%2520spanning%2520dimensions%2520such%2520as%2520inclusivity%252C%2520diversity%252C%250Aexplainability%252C%2520objectivity%252C%2520robustness%252C%2520and%2520realisticity.%2520Guided%2520by%2520these%250Acriteria%252C%2520we%2520construct%2520GenderPair%252C%2520a%2520novel%2520pair-based%2520benchmark%2520designed%2520to%250Aassess%2520gender%2520bias%2520in%2520LLMs%2520comprehensively.%2520Our%2520benchmark%2520provides%2520standardized%250Aand%2520realistic%2520evaluations%252C%2520including%2520previously%2520overlooked%2520gender%2520groups%2520such%250Aas%2520transgender%2520and%2520non-binary%2520individuals.%2520Furthermore%252C%2520we%2520develop%2520effective%250Adebiasing%2520techniques%2520that%2520incorporate%2520counterfactual%2520data%2520augmentation%2520and%250Aspecialized%2520fine-tuning%2520strategies%2520to%2520reduce%2520gender%2520bias%2520in%2520LLMs%2520without%250Acompromising%2520their%2520overall%2520performance.%2520Extensive%2520experiments%2520demonstrate%2520a%250Asignificant%2520reduction%2520in%2520various%2520gender%2520bias%2520benchmarks%252C%2520with%2520reductions%250Apeaking%2520at%2520over%252090%2525%2520and%2520averaging%2520above%252035%2525%2520across%252017%2520different%2520LLMs.%250AImportantly%252C%2520these%2520reductions%2520come%2520with%2520minimal%2520variability%2520in%2520mainstream%250Alanguage%2520tasks%252C%2520remaining%2520below%25202%2525.%2520By%2520offering%2520a%2520realistic%2520assessment%2520and%250Atailored%2520reduction%2520of%2520gender%2520biases%252C%2520we%2520hope%2520that%2520our%2520GenderCARE%2520can%2520represent%250Aa%2520significant%2520step%2520towards%2520achieving%2520fairness%2520and%2520equity%2520in%2520LLMs.%2520More%2520details%250Aare%2520available%2520at%2520https%253A//github.com/kstanghere/GenderCARE-ccs24.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenderCARE%3A%20A%20Comprehensive%20Framework%20for%20Assessing%20and%20Reducing%20Gender%0A%20%20Bias%20in%20Large%20Language%20Models&entry.906535625=Kunsheng%20Tang%20and%20Wenbo%20Zhou%20and%20Jie%20Zhang%20and%20Aishan%20Liu%20and%20Gelei%20Deng%20and%20Shuai%20Li%20and%20Peigui%20Qi%20and%20Weiming%20Zhang%20and%20Tianwei%20Zhang%20and%20Nenghai%20Yu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20remarkable%20capabilities%20in%0Anatural%20language%20generation%2C%20but%20they%20have%20also%20been%20observed%20to%20magnify%0Asocietal%20biases%2C%20particularly%20those%20related%20to%20gender.%20In%20response%20to%20this%0Aissue%2C%20several%20benchmarks%20have%20been%20proposed%20to%20assess%20gender%20bias%20in%20LLMs.%0AHowever%2C%20these%20benchmarks%20often%20lack%20practical%20flexibility%20or%20inadvertently%0Aintroduce%20biases.%20To%20address%20these%20shortcomings%2C%20we%20introduce%20GenderCARE%2C%20a%0Acomprehensive%20framework%20that%20encompasses%20innovative%20Criteria%2C%20bias%20Assessment%2C%0AReduction%20techniques%2C%20and%20Evaluation%20metrics%20for%20quantifying%20and%20mitigating%0Agender%20bias%20in%20LLMs.%20To%20begin%2C%20we%20establish%20pioneering%20criteria%20for%20gender%0Aequality%20benchmarks%2C%20spanning%20dimensions%20such%20as%20inclusivity%2C%20diversity%2C%0Aexplainability%2C%20objectivity%2C%20robustness%2C%20and%20realisticity.%20Guided%20by%20these%0Acriteria%2C%20we%20construct%20GenderPair%2C%20a%20novel%20pair-based%20benchmark%20designed%20to%0Aassess%20gender%20bias%20in%20LLMs%20comprehensively.%20Our%20benchmark%20provides%20standardized%0Aand%20realistic%20evaluations%2C%20including%20previously%20overlooked%20gender%20groups%20such%0Aas%20transgender%20and%20non-binary%20individuals.%20Furthermore%2C%20we%20develop%20effective%0Adebiasing%20techniques%20that%20incorporate%20counterfactual%20data%20augmentation%20and%0Aspecialized%20fine-tuning%20strategies%20to%20reduce%20gender%20bias%20in%20LLMs%20without%0Acompromising%20their%20overall%20performance.%20Extensive%20experiments%20demonstrate%20a%0Asignificant%20reduction%20in%20various%20gender%20bias%20benchmarks%2C%20with%20reductions%0Apeaking%20at%20over%2090%25%20and%20averaging%20above%2035%25%20across%2017%20different%20LLMs.%0AImportantly%2C%20these%20reductions%20come%20with%20minimal%20variability%20in%20mainstream%0Alanguage%20tasks%2C%20remaining%20below%202%25.%20By%20offering%20a%20realistic%20assessment%20and%0Atailored%20reduction%20of%20gender%20biases%2C%20we%20hope%20that%20our%20GenderCARE%20can%20represent%0Aa%20significant%20step%20towards%20achieving%20fairness%20and%20equity%20in%20LLMs.%20More%20details%0Aare%20available%20at%20https%3A//github.com/kstanghere/GenderCARE-ccs24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12494v1&entry.124074799=Read"},
{"title": "RuleAlign: Making Large Language Models Better Physicians with\n  Diagnostic Rule Alignment", "author": "Xiaohan Wang and Xiaoyan Yang and Yuqi Zhu and Yue Shen and Jian Wang and Peng Wei and Lei Liang and Jinjie Gu and Huajun Chen and Ningyu Zhang", "abstract": "  Large Language Models (LLMs) like GPT-4, MedPaLM-2, and Med-Gemini achieve\nperformance competitively with human experts across various medical benchmarks.\nHowever, they still face challenges in making professional diagnoses akin to\nphysicians, particularly in efficiently gathering patient information and\nreasoning the final diagnosis. To this end, we introduce the RuleAlign\nframework, designed to align LLMs with specific diagnostic rules. We develop a\nmedical dialogue dataset comprising rule-based communications between patients\nand physicians and design an alignment learning approach through preference\nlearning. Experimental results demonstrate the effectiveness of the proposed\napproach. We hope that our work can serve as an inspiration for exploring the\npotential of LLMs as AI physicians.\n", "link": "http://arxiv.org/abs/2408.12579v1", "date": "2024-08-22", "relevancy": 1.8436, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4887}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4847}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RuleAlign%3A%20Making%20Large%20Language%20Models%20Better%20Physicians%20with%0A%20%20Diagnostic%20Rule%20Alignment&body=Title%3A%20RuleAlign%3A%20Making%20Large%20Language%20Models%20Better%20Physicians%20with%0A%20%20Diagnostic%20Rule%20Alignment%0AAuthor%3A%20Xiaohan%20Wang%20and%20Xiaoyan%20Yang%20and%20Yuqi%20Zhu%20and%20Yue%20Shen%20and%20Jian%20Wang%20and%20Peng%20Wei%20and%20Lei%20Liang%20and%20Jinjie%20Gu%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20like%20GPT-4%2C%20MedPaLM-2%2C%20and%20Med-Gemini%20achieve%0Aperformance%20competitively%20with%20human%20experts%20across%20various%20medical%20benchmarks.%0AHowever%2C%20they%20still%20face%20challenges%20in%20making%20professional%20diagnoses%20akin%20to%0Aphysicians%2C%20particularly%20in%20efficiently%20gathering%20patient%20information%20and%0Areasoning%20the%20final%20diagnosis.%20To%20this%20end%2C%20we%20introduce%20the%20RuleAlign%0Aframework%2C%20designed%20to%20align%20LLMs%20with%20specific%20diagnostic%20rules.%20We%20develop%20a%0Amedical%20dialogue%20dataset%20comprising%20rule-based%20communications%20between%20patients%0Aand%20physicians%20and%20design%20an%20alignment%20learning%20approach%20through%20preference%0Alearning.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aapproach.%20We%20hope%20that%20our%20work%20can%20serve%20as%20an%20inspiration%20for%20exploring%20the%0Apotential%20of%20LLMs%20as%20AI%20physicians.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRuleAlign%253A%2520Making%2520Large%2520Language%2520Models%2520Better%2520Physicians%2520with%250A%2520%2520Diagnostic%2520Rule%2520Alignment%26entry.906535625%3DXiaohan%2520Wang%2520and%2520Xiaoyan%2520Yang%2520and%2520Yuqi%2520Zhu%2520and%2520Yue%2520Shen%2520and%2520Jian%2520Wang%2520and%2520Peng%2520Wei%2520and%2520Lei%2520Liang%2520and%2520Jinjie%2520Gu%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520like%2520GPT-4%252C%2520MedPaLM-2%252C%2520and%2520Med-Gemini%2520achieve%250Aperformance%2520competitively%2520with%2520human%2520experts%2520across%2520various%2520medical%2520benchmarks.%250AHowever%252C%2520they%2520still%2520face%2520challenges%2520in%2520making%2520professional%2520diagnoses%2520akin%2520to%250Aphysicians%252C%2520particularly%2520in%2520efficiently%2520gathering%2520patient%2520information%2520and%250Areasoning%2520the%2520final%2520diagnosis.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520RuleAlign%250Aframework%252C%2520designed%2520to%2520align%2520LLMs%2520with%2520specific%2520diagnostic%2520rules.%2520We%2520develop%2520a%250Amedical%2520dialogue%2520dataset%2520comprising%2520rule-based%2520communications%2520between%2520patients%250Aand%2520physicians%2520and%2520design%2520an%2520alignment%2520learning%2520approach%2520through%2520preference%250Alearning.%2520Experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Aapproach.%2520We%2520hope%2520that%2520our%2520work%2520can%2520serve%2520as%2520an%2520inspiration%2520for%2520exploring%2520the%250Apotential%2520of%2520LLMs%2520as%2520AI%2520physicians.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RuleAlign%3A%20Making%20Large%20Language%20Models%20Better%20Physicians%20with%0A%20%20Diagnostic%20Rule%20Alignment&entry.906535625=Xiaohan%20Wang%20and%20Xiaoyan%20Yang%20and%20Yuqi%20Zhu%20and%20Yue%20Shen%20and%20Jian%20Wang%20and%20Peng%20Wei%20and%20Lei%20Liang%20and%20Jinjie%20Gu%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20like%20GPT-4%2C%20MedPaLM-2%2C%20and%20Med-Gemini%20achieve%0Aperformance%20competitively%20with%20human%20experts%20across%20various%20medical%20benchmarks.%0AHowever%2C%20they%20still%20face%20challenges%20in%20making%20professional%20diagnoses%20akin%20to%0Aphysicians%2C%20particularly%20in%20efficiently%20gathering%20patient%20information%20and%0Areasoning%20the%20final%20diagnosis.%20To%20this%20end%2C%20we%20introduce%20the%20RuleAlign%0Aframework%2C%20designed%20to%20align%20LLMs%20with%20specific%20diagnostic%20rules.%20We%20develop%20a%0Amedical%20dialogue%20dataset%20comprising%20rule-based%20communications%20between%20patients%0Aand%20physicians%20and%20design%20an%20alignment%20learning%20approach%20through%20preference%0Alearning.%20Experimental%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%0Aapproach.%20We%20hope%20that%20our%20work%20can%20serve%20as%20an%20inspiration%20for%20exploring%20the%0Apotential%20of%20LLMs%20as%20AI%20physicians.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12579v1&entry.124074799=Read"},
{"title": "PolyRouter: A Multi-LLM Querying System", "author": "Dimitris Stripelis and Zijian Hu and Jipeng Zhang and Zhaozhuo Xu and Alay Shah and Han Jin and Yuhang Yao and Salman Avestimehr and Chaoyang He", "abstract": "  With the rapid growth of Large Language Models (LLMs) across various domains,\nnumerous new LLMs have emerged, each possessing domain-specific expertise. This\nproliferation has highlighted the need for quick, high-quality, and\ncost-effective LLM query response methods. Yet, no single LLM exists to\nefficiently balance this trilemma. Some models are powerful but extremely\ncostly, while others are fast and inexpensive but qualitatively inferior. To\naddress this challenge, we present PolyRouter, a non-monolithic LLM querying\nsystem that seamlessly integrates various LLM experts into a single query\ninterface and dynamically routes incoming queries to the most high-performant\nexpert based on query's requirements. Through extensive experiments, we\ndemonstrate that when compared to standalone expert models, PolyRouter improves\nquery efficiency by up to 40%, and leads to significant cost reductions of up\nto 30%, while maintaining or enhancing model performance by up to 10%.\n", "link": "http://arxiv.org/abs/2408.12320v1", "date": "2024-08-22", "relevancy": 1.837, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4672}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolyRouter%3A%20A%20Multi-LLM%20Querying%20System&body=Title%3A%20PolyRouter%3A%20A%20Multi-LLM%20Querying%20System%0AAuthor%3A%20Dimitris%20Stripelis%20and%20Zijian%20Hu%20and%20Jipeng%20Zhang%20and%20Zhaozhuo%20Xu%20and%20Alay%20Shah%20and%20Han%20Jin%20and%20Yuhang%20Yao%20and%20Salman%20Avestimehr%20and%20Chaoyang%20He%0AAbstract%3A%20%20%20With%20the%20rapid%20growth%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%20domains%2C%0Anumerous%20new%20LLMs%20have%20emerged%2C%20each%20possessing%20domain-specific%20expertise.%20This%0Aproliferation%20has%20highlighted%20the%20need%20for%20quick%2C%20high-quality%2C%20and%0Acost-effective%20LLM%20query%20response%20methods.%20Yet%2C%20no%20single%20LLM%20exists%20to%0Aefficiently%20balance%20this%20trilemma.%20Some%20models%20are%20powerful%20but%20extremely%0Acostly%2C%20while%20others%20are%20fast%20and%20inexpensive%20but%20qualitatively%20inferior.%20To%0Aaddress%20this%20challenge%2C%20we%20present%20PolyRouter%2C%20a%20non-monolithic%20LLM%20querying%0Asystem%20that%20seamlessly%20integrates%20various%20LLM%20experts%20into%20a%20single%20query%0Ainterface%20and%20dynamically%20routes%20incoming%20queries%20to%20the%20most%20high-performant%0Aexpert%20based%20on%20query%27s%20requirements.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20when%20compared%20to%20standalone%20expert%20models%2C%20PolyRouter%20improves%0Aquery%20efficiency%20by%20up%20to%2040%25%2C%20and%20leads%20to%20significant%20cost%20reductions%20of%20up%0Ato%2030%25%2C%20while%20maintaining%20or%20enhancing%20model%20performance%20by%20up%20to%2010%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolyRouter%253A%2520A%2520Multi-LLM%2520Querying%2520System%26entry.906535625%3DDimitris%2520Stripelis%2520and%2520Zijian%2520Hu%2520and%2520Jipeng%2520Zhang%2520and%2520Zhaozhuo%2520Xu%2520and%2520Alay%2520Shah%2520and%2520Han%2520Jin%2520and%2520Yuhang%2520Yao%2520and%2520Salman%2520Avestimehr%2520and%2520Chaoyang%2520He%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520growth%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520across%2520various%2520domains%252C%250Anumerous%2520new%2520LLMs%2520have%2520emerged%252C%2520each%2520possessing%2520domain-specific%2520expertise.%2520This%250Aproliferation%2520has%2520highlighted%2520the%2520need%2520for%2520quick%252C%2520high-quality%252C%2520and%250Acost-effective%2520LLM%2520query%2520response%2520methods.%2520Yet%252C%2520no%2520single%2520LLM%2520exists%2520to%250Aefficiently%2520balance%2520this%2520trilemma.%2520Some%2520models%2520are%2520powerful%2520but%2520extremely%250Acostly%252C%2520while%2520others%2520are%2520fast%2520and%2520inexpensive%2520but%2520qualitatively%2520inferior.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520present%2520PolyRouter%252C%2520a%2520non-monolithic%2520LLM%2520querying%250Asystem%2520that%2520seamlessly%2520integrates%2520various%2520LLM%2520experts%2520into%2520a%2520single%2520query%250Ainterface%2520and%2520dynamically%2520routes%2520incoming%2520queries%2520to%2520the%2520most%2520high-performant%250Aexpert%2520based%2520on%2520query%2527s%2520requirements.%2520Through%2520extensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520when%2520compared%2520to%2520standalone%2520expert%2520models%252C%2520PolyRouter%2520improves%250Aquery%2520efficiency%2520by%2520up%2520to%252040%2525%252C%2520and%2520leads%2520to%2520significant%2520cost%2520reductions%2520of%2520up%250Ato%252030%2525%252C%2520while%2520maintaining%2520or%2520enhancing%2520model%2520performance%2520by%2520up%2520to%252010%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolyRouter%3A%20A%20Multi-LLM%20Querying%20System&entry.906535625=Dimitris%20Stripelis%20and%20Zijian%20Hu%20and%20Jipeng%20Zhang%20and%20Zhaozhuo%20Xu%20and%20Alay%20Shah%20and%20Han%20Jin%20and%20Yuhang%20Yao%20and%20Salman%20Avestimehr%20and%20Chaoyang%20He&entry.1292438233=%20%20With%20the%20rapid%20growth%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%20domains%2C%0Anumerous%20new%20LLMs%20have%20emerged%2C%20each%20possessing%20domain-specific%20expertise.%20This%0Aproliferation%20has%20highlighted%20the%20need%20for%20quick%2C%20high-quality%2C%20and%0Acost-effective%20LLM%20query%20response%20methods.%20Yet%2C%20no%20single%20LLM%20exists%20to%0Aefficiently%20balance%20this%20trilemma.%20Some%20models%20are%20powerful%20but%20extremely%0Acostly%2C%20while%20others%20are%20fast%20and%20inexpensive%20but%20qualitatively%20inferior.%20To%0Aaddress%20this%20challenge%2C%20we%20present%20PolyRouter%2C%20a%20non-monolithic%20LLM%20querying%0Asystem%20that%20seamlessly%20integrates%20various%20LLM%20experts%20into%20a%20single%20query%0Ainterface%20and%20dynamically%20routes%20incoming%20queries%20to%20the%20most%20high-performant%0Aexpert%20based%20on%20query%27s%20requirements.%20Through%20extensive%20experiments%2C%20we%0Ademonstrate%20that%20when%20compared%20to%20standalone%20expert%20models%2C%20PolyRouter%20improves%0Aquery%20efficiency%20by%20up%20to%2040%25%2C%20and%20leads%20to%20significant%20cost%20reductions%20of%20up%0Ato%2030%25%2C%20while%20maintaining%20or%20enhancing%20model%20performance%20by%20up%20to%2010%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12320v1&entry.124074799=Read"},
{"title": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend\n  Against Jailbreak Attacks", "author": "Jiawei Zhao and Kejiang Chen and Xiaojian Yuan and Weiming Zhang", "abstract": "  In recent years, the rapid development of large language models (LLMs) has\nachieved remarkable performance across various tasks. However, research\nindicates that LLMs are vulnerable to jailbreak attacks, where adversaries can\ninduce the generation of harmful content through meticulously crafted prompts.\nThis vulnerability poses significant challenges to the secure use and promotion\nof LLMs. Existing defense methods offer protection from different perspectives\nbut often suffer from insufficient effectiveness or a significant impact on the\nmodel's capabilities. In this paper, we propose a plug-and-play and\neasy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which\nguides the model to identify harmful prompts by directly setting the first few\ntokens of the model's output. This approach combines the model's inherent\nsecurity capabilities with an external classifier to defend against jailbreak\nattacks. We demonstrate the effectiveness of PG across three models and five\nattack methods. Compared to baselines, our approach is generally more effective\non average. Additionally, results on the Just-Eval benchmark further confirm\nPG's superiority to preserve the model's performance. our code is available at\nhttps://github.com/weiyezhimeng/Prefix-Guidance.\n", "link": "http://arxiv.org/abs/2408.08924v2", "date": "2024-08-22", "relevancy": 1.8344, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4666}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4532}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prefix%20Guidance%3A%20A%20Steering%20Wheel%20for%20Large%20Language%20Models%20to%20Defend%0A%20%20Against%20Jailbreak%20Attacks&body=Title%3A%20Prefix%20Guidance%3A%20A%20Steering%20Wheel%20for%20Large%20Language%20Models%20to%20Defend%0A%20%20Against%20Jailbreak%20Attacks%0AAuthor%3A%20Jiawei%20Zhao%20and%20Kejiang%20Chen%20and%20Xiaojian%20Yuan%20and%20Weiming%20Zhang%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%0Aachieved%20remarkable%20performance%20across%20various%20tasks.%20However%2C%20research%0Aindicates%20that%20LLMs%20are%20vulnerable%20to%20jailbreak%20attacks%2C%20where%20adversaries%20can%0Ainduce%20the%20generation%20of%20harmful%20content%20through%20meticulously%20crafted%20prompts.%0AThis%20vulnerability%20poses%20significant%20challenges%20to%20the%20secure%20use%20and%20promotion%0Aof%20LLMs.%20Existing%20defense%20methods%20offer%20protection%20from%20different%20perspectives%0Abut%20often%20suffer%20from%20insufficient%20effectiveness%20or%20a%20significant%20impact%20on%20the%0Amodel%27s%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%20plug-and-play%20and%0Aeasy-to-deploy%20jailbreak%20defense%20framework%2C%20namely%20Prefix%20Guidance%20%28PG%29%2C%20which%0Aguides%20the%20model%20to%20identify%20harmful%20prompts%20by%20directly%20setting%20the%20first%20few%0Atokens%20of%20the%20model%27s%20output.%20This%20approach%20combines%20the%20model%27s%20inherent%0Asecurity%20capabilities%20with%20an%20external%20classifier%20to%20defend%20against%20jailbreak%0Aattacks.%20We%20demonstrate%20the%20effectiveness%20of%20PG%20across%20three%20models%20and%20five%0Aattack%20methods.%20Compared%20to%20baselines%2C%20our%20approach%20is%20generally%20more%20effective%0Aon%20average.%20Additionally%2C%20results%20on%20the%20Just-Eval%20benchmark%20further%20confirm%0APG%27s%20superiority%20to%20preserve%20the%20model%27s%20performance.%20our%20code%20is%20available%20at%0Ahttps%3A//github.com/weiyezhimeng/Prefix-Guidance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08924v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrefix%2520Guidance%253A%2520A%2520Steering%2520Wheel%2520for%2520Large%2520Language%2520Models%2520to%2520Defend%250A%2520%2520Against%2520Jailbreak%2520Attacks%26entry.906535625%3DJiawei%2520Zhao%2520and%2520Kejiang%2520Chen%2520and%2520Xiaojian%2520Yuan%2520and%2520Weiming%2520Zhang%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520rapid%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%250Aachieved%2520remarkable%2520performance%2520across%2520various%2520tasks.%2520However%252C%2520research%250Aindicates%2520that%2520LLMs%2520are%2520vulnerable%2520to%2520jailbreak%2520attacks%252C%2520where%2520adversaries%2520can%250Ainduce%2520the%2520generation%2520of%2520harmful%2520content%2520through%2520meticulously%2520crafted%2520prompts.%250AThis%2520vulnerability%2520poses%2520significant%2520challenges%2520to%2520the%2520secure%2520use%2520and%2520promotion%250Aof%2520LLMs.%2520Existing%2520defense%2520methods%2520offer%2520protection%2520from%2520different%2520perspectives%250Abut%2520often%2520suffer%2520from%2520insufficient%2520effectiveness%2520or%2520a%2520significant%2520impact%2520on%2520the%250Amodel%2527s%2520capabilities.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520plug-and-play%2520and%250Aeasy-to-deploy%2520jailbreak%2520defense%2520framework%252C%2520namely%2520Prefix%2520Guidance%2520%2528PG%2529%252C%2520which%250Aguides%2520the%2520model%2520to%2520identify%2520harmful%2520prompts%2520by%2520directly%2520setting%2520the%2520first%2520few%250Atokens%2520of%2520the%2520model%2527s%2520output.%2520This%2520approach%2520combines%2520the%2520model%2527s%2520inherent%250Asecurity%2520capabilities%2520with%2520an%2520external%2520classifier%2520to%2520defend%2520against%2520jailbreak%250Aattacks.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520PG%2520across%2520three%2520models%2520and%2520five%250Aattack%2520methods.%2520Compared%2520to%2520baselines%252C%2520our%2520approach%2520is%2520generally%2520more%2520effective%250Aon%2520average.%2520Additionally%252C%2520results%2520on%2520the%2520Just-Eval%2520benchmark%2520further%2520confirm%250APG%2527s%2520superiority%2520to%2520preserve%2520the%2520model%2527s%2520performance.%2520our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/weiyezhimeng/Prefix-Guidance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08924v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prefix%20Guidance%3A%20A%20Steering%20Wheel%20for%20Large%20Language%20Models%20to%20Defend%0A%20%20Against%20Jailbreak%20Attacks&entry.906535625=Jiawei%20Zhao%20and%20Kejiang%20Chen%20and%20Xiaojian%20Yuan%20and%20Weiming%20Zhang&entry.1292438233=%20%20In%20recent%20years%2C%20the%20rapid%20development%20of%20large%20language%20models%20%28LLMs%29%20has%0Aachieved%20remarkable%20performance%20across%20various%20tasks.%20However%2C%20research%0Aindicates%20that%20LLMs%20are%20vulnerable%20to%20jailbreak%20attacks%2C%20where%20adversaries%20can%0Ainduce%20the%20generation%20of%20harmful%20content%20through%20meticulously%20crafted%20prompts.%0AThis%20vulnerability%20poses%20significant%20challenges%20to%20the%20secure%20use%20and%20promotion%0Aof%20LLMs.%20Existing%20defense%20methods%20offer%20protection%20from%20different%20perspectives%0Abut%20often%20suffer%20from%20insufficient%20effectiveness%20or%20a%20significant%20impact%20on%20the%0Amodel%27s%20capabilities.%20In%20this%20paper%2C%20we%20propose%20a%20plug-and-play%20and%0Aeasy-to-deploy%20jailbreak%20defense%20framework%2C%20namely%20Prefix%20Guidance%20%28PG%29%2C%20which%0Aguides%20the%20model%20to%20identify%20harmful%20prompts%20by%20directly%20setting%20the%20first%20few%0Atokens%20of%20the%20model%27s%20output.%20This%20approach%20combines%20the%20model%27s%20inherent%0Asecurity%20capabilities%20with%20an%20external%20classifier%20to%20defend%20against%20jailbreak%0Aattacks.%20We%20demonstrate%20the%20effectiveness%20of%20PG%20across%20three%20models%20and%20five%0Aattack%20methods.%20Compared%20to%20baselines%2C%20our%20approach%20is%20generally%20more%20effective%0Aon%20average.%20Additionally%2C%20results%20on%20the%20Just-Eval%20benchmark%20further%20confirm%0APG%27s%20superiority%20to%20preserve%20the%20model%27s%20performance.%20our%20code%20is%20available%20at%0Ahttps%3A//github.com/weiyezhimeng/Prefix-Guidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08924v2&entry.124074799=Read"},
{"title": "CODE: Confident Ordinary Differential Editing", "author": "Bastien van Delft and Tommaso Martorella and Alexandre Alahi", "abstract": "  Conditioning image generation facilitates seamless editing and the creation\nof photorealistic images. However, conditioning on noisy or Out-of-Distribution\n(OoD) images poses significant challenges, particularly in balancing fidelity\nto the input and realism of the output. We introduce Confident Ordinary\nDifferential Editing (CODE), a novel approach for image synthesis that\neffectively handles OoD guidance images. Utilizing a diffusion model as a\ngenerative prior, CODE enhances images through score-based updates along the\nprobability-flow Ordinary Differential Equation (ODE) trajectory. This method\nrequires no task-specific training, no handcrafted modules, and no assumptions\nregarding the corruptions affecting the conditioning image. Our method is\ncompatible with any diffusion model. Positioned at the intersection of\nconditional image generation and blind image restoration, CODE operates in a\nfully blind manner, relying solely on a pre-trained generative model. Our\nmethod introduces an alternative approach to blind restoration: instead of\ntargeting a specific ground truth image based on assumptions about the\nunderlying corruption, CODE aims to increase the likelihood of the input image\nwhile maintaining fidelity. This results in the most probable in-distribution\nimage around the input. Our contributions are twofold. First, CODE introduces a\nnovel editing method based on ODE, providing enhanced control, realism, and\nfidelity compared to its SDE-based counterpart. Second, we introduce a\nconfidence interval-based clipping method, which improves CODE's effectiveness\nby allowing it to disregard certain pixels or information, thus enhancing the\nrestoration process in a blind manner. Experimental results demonstrate CODE's\neffectiveness over existing methods, particularly in scenarios involving severe\ndegradation or OoD inputs.\n", "link": "http://arxiv.org/abs/2408.12418v1", "date": "2024-08-22", "relevancy": 1.8244, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6126}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6073}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CODE%3A%20Confident%20Ordinary%20Differential%20Editing&body=Title%3A%20CODE%3A%20Confident%20Ordinary%20Differential%20Editing%0AAuthor%3A%20Bastien%20van%20Delft%20and%20Tommaso%20Martorella%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Conditioning%20image%20generation%20facilitates%20seamless%20editing%20and%20the%20creation%0Aof%20photorealistic%20images.%20However%2C%20conditioning%20on%20noisy%20or%20Out-of-Distribution%0A%28OoD%29%20images%20poses%20significant%20challenges%2C%20particularly%20in%20balancing%20fidelity%0Ato%20the%20input%20and%20realism%20of%20the%20output.%20We%20introduce%20Confident%20Ordinary%0ADifferential%20Editing%20%28CODE%29%2C%20a%20novel%20approach%20for%20image%20synthesis%20that%0Aeffectively%20handles%20OoD%20guidance%20images.%20Utilizing%20a%20diffusion%20model%20as%20a%0Agenerative%20prior%2C%20CODE%20enhances%20images%20through%20score-based%20updates%20along%20the%0Aprobability-flow%20Ordinary%20Differential%20Equation%20%28ODE%29%20trajectory.%20This%20method%0Arequires%20no%20task-specific%20training%2C%20no%20handcrafted%20modules%2C%20and%20no%20assumptions%0Aregarding%20the%20corruptions%20affecting%20the%20conditioning%20image.%20Our%20method%20is%0Acompatible%20with%20any%20diffusion%20model.%20Positioned%20at%20the%20intersection%20of%0Aconditional%20image%20generation%20and%20blind%20image%20restoration%2C%20CODE%20operates%20in%20a%0Afully%20blind%20manner%2C%20relying%20solely%20on%20a%20pre-trained%20generative%20model.%20Our%0Amethod%20introduces%20an%20alternative%20approach%20to%20blind%20restoration%3A%20instead%20of%0Atargeting%20a%20specific%20ground%20truth%20image%20based%20on%20assumptions%20about%20the%0Aunderlying%20corruption%2C%20CODE%20aims%20to%20increase%20the%20likelihood%20of%20the%20input%20image%0Awhile%20maintaining%20fidelity.%20This%20results%20in%20the%20most%20probable%20in-distribution%0Aimage%20around%20the%20input.%20Our%20contributions%20are%20twofold.%20First%2C%20CODE%20introduces%20a%0Anovel%20editing%20method%20based%20on%20ODE%2C%20providing%20enhanced%20control%2C%20realism%2C%20and%0Afidelity%20compared%20to%20its%20SDE-based%20counterpart.%20Second%2C%20we%20introduce%20a%0Aconfidence%20interval-based%20clipping%20method%2C%20which%20improves%20CODE%27s%20effectiveness%0Aby%20allowing%20it%20to%20disregard%20certain%20pixels%20or%20information%2C%20thus%20enhancing%20the%0Arestoration%20process%20in%20a%20blind%20manner.%20Experimental%20results%20demonstrate%20CODE%27s%0Aeffectiveness%20over%20existing%20methods%2C%20particularly%20in%20scenarios%20involving%20severe%0Adegradation%20or%20OoD%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCODE%253A%2520Confident%2520Ordinary%2520Differential%2520Editing%26entry.906535625%3DBastien%2520van%2520Delft%2520and%2520Tommaso%2520Martorella%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3D%2520%2520Conditioning%2520image%2520generation%2520facilitates%2520seamless%2520editing%2520and%2520the%2520creation%250Aof%2520photorealistic%2520images.%2520However%252C%2520conditioning%2520on%2520noisy%2520or%2520Out-of-Distribution%250A%2528OoD%2529%2520images%2520poses%2520significant%2520challenges%252C%2520particularly%2520in%2520balancing%2520fidelity%250Ato%2520the%2520input%2520and%2520realism%2520of%2520the%2520output.%2520We%2520introduce%2520Confident%2520Ordinary%250ADifferential%2520Editing%2520%2528CODE%2529%252C%2520a%2520novel%2520approach%2520for%2520image%2520synthesis%2520that%250Aeffectively%2520handles%2520OoD%2520guidance%2520images.%2520Utilizing%2520a%2520diffusion%2520model%2520as%2520a%250Agenerative%2520prior%252C%2520CODE%2520enhances%2520images%2520through%2520score-based%2520updates%2520along%2520the%250Aprobability-flow%2520Ordinary%2520Differential%2520Equation%2520%2528ODE%2529%2520trajectory.%2520This%2520method%250Arequires%2520no%2520task-specific%2520training%252C%2520no%2520handcrafted%2520modules%252C%2520and%2520no%2520assumptions%250Aregarding%2520the%2520corruptions%2520affecting%2520the%2520conditioning%2520image.%2520Our%2520method%2520is%250Acompatible%2520with%2520any%2520diffusion%2520model.%2520Positioned%2520at%2520the%2520intersection%2520of%250Aconditional%2520image%2520generation%2520and%2520blind%2520image%2520restoration%252C%2520CODE%2520operates%2520in%2520a%250Afully%2520blind%2520manner%252C%2520relying%2520solely%2520on%2520a%2520pre-trained%2520generative%2520model.%2520Our%250Amethod%2520introduces%2520an%2520alternative%2520approach%2520to%2520blind%2520restoration%253A%2520instead%2520of%250Atargeting%2520a%2520specific%2520ground%2520truth%2520image%2520based%2520on%2520assumptions%2520about%2520the%250Aunderlying%2520corruption%252C%2520CODE%2520aims%2520to%2520increase%2520the%2520likelihood%2520of%2520the%2520input%2520image%250Awhile%2520maintaining%2520fidelity.%2520This%2520results%2520in%2520the%2520most%2520probable%2520in-distribution%250Aimage%2520around%2520the%2520input.%2520Our%2520contributions%2520are%2520twofold.%2520First%252C%2520CODE%2520introduces%2520a%250Anovel%2520editing%2520method%2520based%2520on%2520ODE%252C%2520providing%2520enhanced%2520control%252C%2520realism%252C%2520and%250Afidelity%2520compared%2520to%2520its%2520SDE-based%2520counterpart.%2520Second%252C%2520we%2520introduce%2520a%250Aconfidence%2520interval-based%2520clipping%2520method%252C%2520which%2520improves%2520CODE%2527s%2520effectiveness%250Aby%2520allowing%2520it%2520to%2520disregard%2520certain%2520pixels%2520or%2520information%252C%2520thus%2520enhancing%2520the%250Arestoration%2520process%2520in%2520a%2520blind%2520manner.%2520Experimental%2520results%2520demonstrate%2520CODE%2527s%250Aeffectiveness%2520over%2520existing%2520methods%252C%2520particularly%2520in%2520scenarios%2520involving%2520severe%250Adegradation%2520or%2520OoD%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CODE%3A%20Confident%20Ordinary%20Differential%20Editing&entry.906535625=Bastien%20van%20Delft%20and%20Tommaso%20Martorella%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Conditioning%20image%20generation%20facilitates%20seamless%20editing%20and%20the%20creation%0Aof%20photorealistic%20images.%20However%2C%20conditioning%20on%20noisy%20or%20Out-of-Distribution%0A%28OoD%29%20images%20poses%20significant%20challenges%2C%20particularly%20in%20balancing%20fidelity%0Ato%20the%20input%20and%20realism%20of%20the%20output.%20We%20introduce%20Confident%20Ordinary%0ADifferential%20Editing%20%28CODE%29%2C%20a%20novel%20approach%20for%20image%20synthesis%20that%0Aeffectively%20handles%20OoD%20guidance%20images.%20Utilizing%20a%20diffusion%20model%20as%20a%0Agenerative%20prior%2C%20CODE%20enhances%20images%20through%20score-based%20updates%20along%20the%0Aprobability-flow%20Ordinary%20Differential%20Equation%20%28ODE%29%20trajectory.%20This%20method%0Arequires%20no%20task-specific%20training%2C%20no%20handcrafted%20modules%2C%20and%20no%20assumptions%0Aregarding%20the%20corruptions%20affecting%20the%20conditioning%20image.%20Our%20method%20is%0Acompatible%20with%20any%20diffusion%20model.%20Positioned%20at%20the%20intersection%20of%0Aconditional%20image%20generation%20and%20blind%20image%20restoration%2C%20CODE%20operates%20in%20a%0Afully%20blind%20manner%2C%20relying%20solely%20on%20a%20pre-trained%20generative%20model.%20Our%0Amethod%20introduces%20an%20alternative%20approach%20to%20blind%20restoration%3A%20instead%20of%0Atargeting%20a%20specific%20ground%20truth%20image%20based%20on%20assumptions%20about%20the%0Aunderlying%20corruption%2C%20CODE%20aims%20to%20increase%20the%20likelihood%20of%20the%20input%20image%0Awhile%20maintaining%20fidelity.%20This%20results%20in%20the%20most%20probable%20in-distribution%0Aimage%20around%20the%20input.%20Our%20contributions%20are%20twofold.%20First%2C%20CODE%20introduces%20a%0Anovel%20editing%20method%20based%20on%20ODE%2C%20providing%20enhanced%20control%2C%20realism%2C%20and%0Afidelity%20compared%20to%20its%20SDE-based%20counterpart.%20Second%2C%20we%20introduce%20a%0Aconfidence%20interval-based%20clipping%20method%2C%20which%20improves%20CODE%27s%20effectiveness%0Aby%20allowing%20it%20to%20disregard%20certain%20pixels%20or%20information%2C%20thus%20enhancing%20the%0Arestoration%20process%20in%20a%20blind%20manner.%20Experimental%20results%20demonstrate%20CODE%27s%0Aeffectiveness%20over%20existing%20methods%2C%20particularly%20in%20scenarios%20involving%20severe%0Adegradation%20or%20OoD%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12418v1&entry.124074799=Read"},
{"title": "Neural interval-censored survival regression with feature selection", "author": "Carlos Garc\u00eda Meixide and Marcos Matabuena and Louis Abraham and Michael R. Kosorok", "abstract": "  Survival analysis is a fundamental area of focus in biomedical research,\nparticularly in the context of personalized medicine. This prominence is due to\nthe increasing prevalence of large and high-dimensional datasets, such as omics\nand medical image data. However, the literature on non-linear regression\nalgorithms and variable selection techniques for interval-censoring is either\nlimited or non-existent, particularly in the context of neural networks. Our\nobjective is to introduce a novel predictive framework tailored for\ninterval-censored regression tasks, rooted in Accelerated Failure Time (AFT)\nmodels. Our strategy comprises two key components: i) a variable selection\nphase leveraging recent advances on sparse neural network architectures, ii) a\nregression model targeting prediction of the interval-censored response. To\nassess the performance of our novel algorithm, we conducted a comprehensive\nevaluation through both numerical experiments and real-world applications that\nencompass scenarios related to diabetes and physical activity. Our results\noutperform traditional AFT algorithms, particularly in scenarios featuring\nnon-linear relationships.\n", "link": "http://arxiv.org/abs/2206.06885v3", "date": "2024-08-22", "relevancy": 1.814, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4729}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4399}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20interval-censored%20survival%20regression%20with%20feature%20selection&body=Title%3A%20Neural%20interval-censored%20survival%20regression%20with%20feature%20selection%0AAuthor%3A%20Carlos%20Garc%C3%ADa%20Meixide%20and%20Marcos%20Matabuena%20and%20Louis%20Abraham%20and%20Michael%20R.%20Kosorok%0AAbstract%3A%20%20%20Survival%20analysis%20is%20a%20fundamental%20area%20of%20focus%20in%20biomedical%20research%2C%0Aparticularly%20in%20the%20context%20of%20personalized%20medicine.%20This%20prominence%20is%20due%20to%0Athe%20increasing%20prevalence%20of%20large%20and%20high-dimensional%20datasets%2C%20such%20as%20omics%0Aand%20medical%20image%20data.%20However%2C%20the%20literature%20on%20non-linear%20regression%0Aalgorithms%20and%20variable%20selection%20techniques%20for%20interval-censoring%20is%20either%0Alimited%20or%20non-existent%2C%20particularly%20in%20the%20context%20of%20neural%20networks.%20Our%0Aobjective%20is%20to%20introduce%20a%20novel%20predictive%20framework%20tailored%20for%0Ainterval-censored%20regression%20tasks%2C%20rooted%20in%20Accelerated%20Failure%20Time%20%28AFT%29%0Amodels.%20Our%20strategy%20comprises%20two%20key%20components%3A%20i%29%20a%20variable%20selection%0Aphase%20leveraging%20recent%20advances%20on%20sparse%20neural%20network%20architectures%2C%20ii%29%20a%0Aregression%20model%20targeting%20prediction%20of%20the%20interval-censored%20response.%20To%0Aassess%20the%20performance%20of%20our%20novel%20algorithm%2C%20we%20conducted%20a%20comprehensive%0Aevaluation%20through%20both%20numerical%20experiments%20and%20real-world%20applications%20that%0Aencompass%20scenarios%20related%20to%20diabetes%20and%20physical%20activity.%20Our%20results%0Aoutperform%20traditional%20AFT%20algorithms%2C%20particularly%20in%20scenarios%20featuring%0Anon-linear%20relationships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.06885v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520interval-censored%2520survival%2520regression%2520with%2520feature%2520selection%26entry.906535625%3DCarlos%2520Garc%25C3%25ADa%2520Meixide%2520and%2520Marcos%2520Matabuena%2520and%2520Louis%2520Abraham%2520and%2520Michael%2520R.%2520Kosorok%26entry.1292438233%3D%2520%2520Survival%2520analysis%2520is%2520a%2520fundamental%2520area%2520of%2520focus%2520in%2520biomedical%2520research%252C%250Aparticularly%2520in%2520the%2520context%2520of%2520personalized%2520medicine.%2520This%2520prominence%2520is%2520due%2520to%250Athe%2520increasing%2520prevalence%2520of%2520large%2520and%2520high-dimensional%2520datasets%252C%2520such%2520as%2520omics%250Aand%2520medical%2520image%2520data.%2520However%252C%2520the%2520literature%2520on%2520non-linear%2520regression%250Aalgorithms%2520and%2520variable%2520selection%2520techniques%2520for%2520interval-censoring%2520is%2520either%250Alimited%2520or%2520non-existent%252C%2520particularly%2520in%2520the%2520context%2520of%2520neural%2520networks.%2520Our%250Aobjective%2520is%2520to%2520introduce%2520a%2520novel%2520predictive%2520framework%2520tailored%2520for%250Ainterval-censored%2520regression%2520tasks%252C%2520rooted%2520in%2520Accelerated%2520Failure%2520Time%2520%2528AFT%2529%250Amodels.%2520Our%2520strategy%2520comprises%2520two%2520key%2520components%253A%2520i%2529%2520a%2520variable%2520selection%250Aphase%2520leveraging%2520recent%2520advances%2520on%2520sparse%2520neural%2520network%2520architectures%252C%2520ii%2529%2520a%250Aregression%2520model%2520targeting%2520prediction%2520of%2520the%2520interval-censored%2520response.%2520To%250Aassess%2520the%2520performance%2520of%2520our%2520novel%2520algorithm%252C%2520we%2520conducted%2520a%2520comprehensive%250Aevaluation%2520through%2520both%2520numerical%2520experiments%2520and%2520real-world%2520applications%2520that%250Aencompass%2520scenarios%2520related%2520to%2520diabetes%2520and%2520physical%2520activity.%2520Our%2520results%250Aoutperform%2520traditional%2520AFT%2520algorithms%252C%2520particularly%2520in%2520scenarios%2520featuring%250Anon-linear%2520relationships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.06885v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20interval-censored%20survival%20regression%20with%20feature%20selection&entry.906535625=Carlos%20Garc%C3%ADa%20Meixide%20and%20Marcos%20Matabuena%20and%20Louis%20Abraham%20and%20Michael%20R.%20Kosorok&entry.1292438233=%20%20Survival%20analysis%20is%20a%20fundamental%20area%20of%20focus%20in%20biomedical%20research%2C%0Aparticularly%20in%20the%20context%20of%20personalized%20medicine.%20This%20prominence%20is%20due%20to%0Athe%20increasing%20prevalence%20of%20large%20and%20high-dimensional%20datasets%2C%20such%20as%20omics%0Aand%20medical%20image%20data.%20However%2C%20the%20literature%20on%20non-linear%20regression%0Aalgorithms%20and%20variable%20selection%20techniques%20for%20interval-censoring%20is%20either%0Alimited%20or%20non-existent%2C%20particularly%20in%20the%20context%20of%20neural%20networks.%20Our%0Aobjective%20is%20to%20introduce%20a%20novel%20predictive%20framework%20tailored%20for%0Ainterval-censored%20regression%20tasks%2C%20rooted%20in%20Accelerated%20Failure%20Time%20%28AFT%29%0Amodels.%20Our%20strategy%20comprises%20two%20key%20components%3A%20i%29%20a%20variable%20selection%0Aphase%20leveraging%20recent%20advances%20on%20sparse%20neural%20network%20architectures%2C%20ii%29%20a%0Aregression%20model%20targeting%20prediction%20of%20the%20interval-censored%20response.%20To%0Aassess%20the%20performance%20of%20our%20novel%20algorithm%2C%20we%20conducted%20a%20comprehensive%0Aevaluation%20through%20both%20numerical%20experiments%20and%20real-world%20applications%20that%0Aencompass%20scenarios%20related%20to%20diabetes%20and%20physical%20activity.%20Our%20results%0Aoutperform%20traditional%20AFT%20algorithms%2C%20particularly%20in%20scenarios%20featuring%0Anon-linear%20relationships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.06885v3&entry.124074799=Read"},
{"title": "Advanced atom-level representations for protein flexibility prediction\n  utilizing graph neural networks", "author": "Sina Sarparast and Aldo Zaimi and Maximilian Ebert and Michael-Rock Goldsmith", "abstract": "  Protein dynamics play a crucial role in many biological processes and drug\ninteractions. However, measuring, and simulating protein dynamics is\nchallenging and time-consuming. While machine learning holds promise in\ndeciphering the determinants of protein dynamics from structural information,\nmost existing methods for protein representation learning operate at the\nresidue level, ignoring the finer details of atomic interactions. In this work,\nwe propose for the first time to use graph neural networks (GNNs) to learn\nprotein representations at the atomic level and predict B-factors from protein\n3D structures. The B-factor reflects the atomic displacement of atoms in\nproteins, and can serve as a surrogate for protein flexibility. We compared\ndifferent GNN architectures to assess their performance. The Meta-GNN model\nachieves a correlation coefficient of 0.71 on a large and diverse test set of\nover 4k proteins (17M atoms) from the Protein Data Bank (PDB), outperforming\nprevious methods by a large margin. Our work demonstrates the potential of\nrepresentations learned by GNNs for protein flexibility prediction and other\nrelated tasks.\n", "link": "http://arxiv.org/abs/2408.12519v1", "date": "2024-08-22", "relevancy": 1.8114, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4574}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4502}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advanced%20atom-level%20representations%20for%20protein%20flexibility%20prediction%0A%20%20utilizing%20graph%20neural%20networks&body=Title%3A%20Advanced%20atom-level%20representations%20for%20protein%20flexibility%20prediction%0A%20%20utilizing%20graph%20neural%20networks%0AAuthor%3A%20Sina%20Sarparast%20and%20Aldo%20Zaimi%20and%20Maximilian%20Ebert%20and%20Michael-Rock%20Goldsmith%0AAbstract%3A%20%20%20Protein%20dynamics%20play%20a%20crucial%20role%20in%20many%20biological%20processes%20and%20drug%0Ainteractions.%20However%2C%20measuring%2C%20and%20simulating%20protein%20dynamics%20is%0Achallenging%20and%20time-consuming.%20While%20machine%20learning%20holds%20promise%20in%0Adeciphering%20the%20determinants%20of%20protein%20dynamics%20from%20structural%20information%2C%0Amost%20existing%20methods%20for%20protein%20representation%20learning%20operate%20at%20the%0Aresidue%20level%2C%20ignoring%20the%20finer%20details%20of%20atomic%20interactions.%20In%20this%20work%2C%0Awe%20propose%20for%20the%20first%20time%20to%20use%20graph%20neural%20networks%20%28GNNs%29%20to%20learn%0Aprotein%20representations%20at%20the%20atomic%20level%20and%20predict%20B-factors%20from%20protein%0A3D%20structures.%20The%20B-factor%20reflects%20the%20atomic%20displacement%20of%20atoms%20in%0Aproteins%2C%20and%20can%20serve%20as%20a%20surrogate%20for%20protein%20flexibility.%20We%20compared%0Adifferent%20GNN%20architectures%20to%20assess%20their%20performance.%20The%20Meta-GNN%20model%0Aachieves%20a%20correlation%20coefficient%20of%200.71%20on%20a%20large%20and%20diverse%20test%20set%20of%0Aover%204k%20proteins%20%2817M%20atoms%29%20from%20the%20Protein%20Data%20Bank%20%28PDB%29%2C%20outperforming%0Aprevious%20methods%20by%20a%20large%20margin.%20Our%20work%20demonstrates%20the%20potential%20of%0Arepresentations%20learned%20by%20GNNs%20for%20protein%20flexibility%20prediction%20and%20other%0Arelated%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvanced%2520atom-level%2520representations%2520for%2520protein%2520flexibility%2520prediction%250A%2520%2520utilizing%2520graph%2520neural%2520networks%26entry.906535625%3DSina%2520Sarparast%2520and%2520Aldo%2520Zaimi%2520and%2520Maximilian%2520Ebert%2520and%2520Michael-Rock%2520Goldsmith%26entry.1292438233%3D%2520%2520Protein%2520dynamics%2520play%2520a%2520crucial%2520role%2520in%2520many%2520biological%2520processes%2520and%2520drug%250Ainteractions.%2520However%252C%2520measuring%252C%2520and%2520simulating%2520protein%2520dynamics%2520is%250Achallenging%2520and%2520time-consuming.%2520While%2520machine%2520learning%2520holds%2520promise%2520in%250Adeciphering%2520the%2520determinants%2520of%2520protein%2520dynamics%2520from%2520structural%2520information%252C%250Amost%2520existing%2520methods%2520for%2520protein%2520representation%2520learning%2520operate%2520at%2520the%250Aresidue%2520level%252C%2520ignoring%2520the%2520finer%2520details%2520of%2520atomic%2520interactions.%2520In%2520this%2520work%252C%250Awe%2520propose%2520for%2520the%2520first%2520time%2520to%2520use%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520to%2520learn%250Aprotein%2520representations%2520at%2520the%2520atomic%2520level%2520and%2520predict%2520B-factors%2520from%2520protein%250A3D%2520structures.%2520The%2520B-factor%2520reflects%2520the%2520atomic%2520displacement%2520of%2520atoms%2520in%250Aproteins%252C%2520and%2520can%2520serve%2520as%2520a%2520surrogate%2520for%2520protein%2520flexibility.%2520We%2520compared%250Adifferent%2520GNN%2520architectures%2520to%2520assess%2520their%2520performance.%2520The%2520Meta-GNN%2520model%250Aachieves%2520a%2520correlation%2520coefficient%2520of%25200.71%2520on%2520a%2520large%2520and%2520diverse%2520test%2520set%2520of%250Aover%25204k%2520proteins%2520%252817M%2520atoms%2529%2520from%2520the%2520Protein%2520Data%2520Bank%2520%2528PDB%2529%252C%2520outperforming%250Aprevious%2520methods%2520by%2520a%2520large%2520margin.%2520Our%2520work%2520demonstrates%2520the%2520potential%2520of%250Arepresentations%2520learned%2520by%2520GNNs%2520for%2520protein%2520flexibility%2520prediction%2520and%2520other%250Arelated%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advanced%20atom-level%20representations%20for%20protein%20flexibility%20prediction%0A%20%20utilizing%20graph%20neural%20networks&entry.906535625=Sina%20Sarparast%20and%20Aldo%20Zaimi%20and%20Maximilian%20Ebert%20and%20Michael-Rock%20Goldsmith&entry.1292438233=%20%20Protein%20dynamics%20play%20a%20crucial%20role%20in%20many%20biological%20processes%20and%20drug%0Ainteractions.%20However%2C%20measuring%2C%20and%20simulating%20protein%20dynamics%20is%0Achallenging%20and%20time-consuming.%20While%20machine%20learning%20holds%20promise%20in%0Adeciphering%20the%20determinants%20of%20protein%20dynamics%20from%20structural%20information%2C%0Amost%20existing%20methods%20for%20protein%20representation%20learning%20operate%20at%20the%0Aresidue%20level%2C%20ignoring%20the%20finer%20details%20of%20atomic%20interactions.%20In%20this%20work%2C%0Awe%20propose%20for%20the%20first%20time%20to%20use%20graph%20neural%20networks%20%28GNNs%29%20to%20learn%0Aprotein%20representations%20at%20the%20atomic%20level%20and%20predict%20B-factors%20from%20protein%0A3D%20structures.%20The%20B-factor%20reflects%20the%20atomic%20displacement%20of%20atoms%20in%0Aproteins%2C%20and%20can%20serve%20as%20a%20surrogate%20for%20protein%20flexibility.%20We%20compared%0Adifferent%20GNN%20architectures%20to%20assess%20their%20performance.%20The%20Meta-GNN%20model%0Aachieves%20a%20correlation%20coefficient%20of%200.71%20on%20a%20large%20and%20diverse%20test%20set%20of%0Aover%204k%20proteins%20%2817M%20atoms%29%20from%20the%20Protein%20Data%20Bank%20%28PDB%29%2C%20outperforming%0Aprevious%20methods%20by%20a%20large%20margin.%20Our%20work%20demonstrates%20the%20potential%20of%0Arepresentations%20learned%20by%20GNNs%20for%20protein%20flexibility%20prediction%20and%20other%0Arelated%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12519v1&entry.124074799=Read"},
{"title": "EX-DRL: Hedging Against Heavy Losses with EXtreme Distributional\n  Reinforcement Learning", "author": "Parvin Malekzadeh and Zissis Poulos and Jacky Chen and Zeyu Wang and Konstantinos N. Plataniotis", "abstract": "  Recent advancements in Distributional Reinforcement Learning (DRL) for\nmodeling loss distributions have shown promise in developing hedging strategies\nin derivatives markets. A common approach in DRL involves learning the\nquantiles of loss distributions at specified levels using Quantile Regression\n(QR). This method is particularly effective in option hedging due to its direct\nquantile-based risk assessment, such as Value at Risk (VaR) and Conditional\nValue at Risk (CVaR). However, these risk measures depend on the accurate\nestimation of extreme quantiles in the loss distribution's tail, which can be\nimprecise in QR-based DRL due to the rarity and extremity of tail data, as\nhighlighted in the literature. To address this issue, we propose EXtreme DRL\n(EX-DRL), which enhances extreme quantile prediction by modeling the tail of\nthe loss distribution with a Generalized Pareto Distribution (GPD). This method\nintroduces supplementary data to mitigate the scarcity of extreme quantile\nobservations, thereby improving estimation accuracy through QR. Comprehensive\nexperiments on gamma hedging options demonstrate that EX-DRL improves existing\nQR-based models by providing more precise estimates of extreme quantiles,\nthereby improving the computation and reliability of risk metrics for complex\nfinancial risk management.\n", "link": "http://arxiv.org/abs/2408.12446v1", "date": "2024-08-22", "relevancy": 1.7693, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4598}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4491}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EX-DRL%3A%20Hedging%20Against%20Heavy%20Losses%20with%20EXtreme%20Distributional%0A%20%20Reinforcement%20Learning&body=Title%3A%20EX-DRL%3A%20Hedging%20Against%20Heavy%20Losses%20with%20EXtreme%20Distributional%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Parvin%20Malekzadeh%20and%20Zissis%20Poulos%20and%20Jacky%20Chen%20and%20Zeyu%20Wang%20and%20Konstantinos%20N.%20Plataniotis%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Distributional%20Reinforcement%20Learning%20%28DRL%29%20for%0Amodeling%20loss%20distributions%20have%20shown%20promise%20in%20developing%20hedging%20strategies%0Ain%20derivatives%20markets.%20A%20common%20approach%20in%20DRL%20involves%20learning%20the%0Aquantiles%20of%20loss%20distributions%20at%20specified%20levels%20using%20Quantile%20Regression%0A%28QR%29.%20This%20method%20is%20particularly%20effective%20in%20option%20hedging%20due%20to%20its%20direct%0Aquantile-based%20risk%20assessment%2C%20such%20as%20Value%20at%20Risk%20%28VaR%29%20and%20Conditional%0AValue%20at%20Risk%20%28CVaR%29.%20However%2C%20these%20risk%20measures%20depend%20on%20the%20accurate%0Aestimation%20of%20extreme%20quantiles%20in%20the%20loss%20distribution%27s%20tail%2C%20which%20can%20be%0Aimprecise%20in%20QR-based%20DRL%20due%20to%20the%20rarity%20and%20extremity%20of%20tail%20data%2C%20as%0Ahighlighted%20in%20the%20literature.%20To%20address%20this%20issue%2C%20we%20propose%20EXtreme%20DRL%0A%28EX-DRL%29%2C%20which%20enhances%20extreme%20quantile%20prediction%20by%20modeling%20the%20tail%20of%0Athe%20loss%20distribution%20with%20a%20Generalized%20Pareto%20Distribution%20%28GPD%29.%20This%20method%0Aintroduces%20supplementary%20data%20to%20mitigate%20the%20scarcity%20of%20extreme%20quantile%0Aobservations%2C%20thereby%20improving%20estimation%20accuracy%20through%20QR.%20Comprehensive%0Aexperiments%20on%20gamma%20hedging%20options%20demonstrate%20that%20EX-DRL%20improves%20existing%0AQR-based%20models%20by%20providing%20more%20precise%20estimates%20of%20extreme%20quantiles%2C%0Athereby%20improving%20the%20computation%20and%20reliability%20of%20risk%20metrics%20for%20complex%0Afinancial%20risk%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12446v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEX-DRL%253A%2520Hedging%2520Against%2520Heavy%2520Losses%2520with%2520EXtreme%2520Distributional%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DParvin%2520Malekzadeh%2520and%2520Zissis%2520Poulos%2520and%2520Jacky%2520Chen%2520and%2520Zeyu%2520Wang%2520and%2520Konstantinos%2520N.%2520Plataniotis%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Distributional%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520for%250Amodeling%2520loss%2520distributions%2520have%2520shown%2520promise%2520in%2520developing%2520hedging%2520strategies%250Ain%2520derivatives%2520markets.%2520A%2520common%2520approach%2520in%2520DRL%2520involves%2520learning%2520the%250Aquantiles%2520of%2520loss%2520distributions%2520at%2520specified%2520levels%2520using%2520Quantile%2520Regression%250A%2528QR%2529.%2520This%2520method%2520is%2520particularly%2520effective%2520in%2520option%2520hedging%2520due%2520to%2520its%2520direct%250Aquantile-based%2520risk%2520assessment%252C%2520such%2520as%2520Value%2520at%2520Risk%2520%2528VaR%2529%2520and%2520Conditional%250AValue%2520at%2520Risk%2520%2528CVaR%2529.%2520However%252C%2520these%2520risk%2520measures%2520depend%2520on%2520the%2520accurate%250Aestimation%2520of%2520extreme%2520quantiles%2520in%2520the%2520loss%2520distribution%2527s%2520tail%252C%2520which%2520can%2520be%250Aimprecise%2520in%2520QR-based%2520DRL%2520due%2520to%2520the%2520rarity%2520and%2520extremity%2520of%2520tail%2520data%252C%2520as%250Ahighlighted%2520in%2520the%2520literature.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520EXtreme%2520DRL%250A%2528EX-DRL%2529%252C%2520which%2520enhances%2520extreme%2520quantile%2520prediction%2520by%2520modeling%2520the%2520tail%2520of%250Athe%2520loss%2520distribution%2520with%2520a%2520Generalized%2520Pareto%2520Distribution%2520%2528GPD%2529.%2520This%2520method%250Aintroduces%2520supplementary%2520data%2520to%2520mitigate%2520the%2520scarcity%2520of%2520extreme%2520quantile%250Aobservations%252C%2520thereby%2520improving%2520estimation%2520accuracy%2520through%2520QR.%2520Comprehensive%250Aexperiments%2520on%2520gamma%2520hedging%2520options%2520demonstrate%2520that%2520EX-DRL%2520improves%2520existing%250AQR-based%2520models%2520by%2520providing%2520more%2520precise%2520estimates%2520of%2520extreme%2520quantiles%252C%250Athereby%2520improving%2520the%2520computation%2520and%2520reliability%2520of%2520risk%2520metrics%2520for%2520complex%250Afinancial%2520risk%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12446v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EX-DRL%3A%20Hedging%20Against%20Heavy%20Losses%20with%20EXtreme%20Distributional%0A%20%20Reinforcement%20Learning&entry.906535625=Parvin%20Malekzadeh%20and%20Zissis%20Poulos%20and%20Jacky%20Chen%20and%20Zeyu%20Wang%20and%20Konstantinos%20N.%20Plataniotis&entry.1292438233=%20%20Recent%20advancements%20in%20Distributional%20Reinforcement%20Learning%20%28DRL%29%20for%0Amodeling%20loss%20distributions%20have%20shown%20promise%20in%20developing%20hedging%20strategies%0Ain%20derivatives%20markets.%20A%20common%20approach%20in%20DRL%20involves%20learning%20the%0Aquantiles%20of%20loss%20distributions%20at%20specified%20levels%20using%20Quantile%20Regression%0A%28QR%29.%20This%20method%20is%20particularly%20effective%20in%20option%20hedging%20due%20to%20its%20direct%0Aquantile-based%20risk%20assessment%2C%20such%20as%20Value%20at%20Risk%20%28VaR%29%20and%20Conditional%0AValue%20at%20Risk%20%28CVaR%29.%20However%2C%20these%20risk%20measures%20depend%20on%20the%20accurate%0Aestimation%20of%20extreme%20quantiles%20in%20the%20loss%20distribution%27s%20tail%2C%20which%20can%20be%0Aimprecise%20in%20QR-based%20DRL%20due%20to%20the%20rarity%20and%20extremity%20of%20tail%20data%2C%20as%0Ahighlighted%20in%20the%20literature.%20To%20address%20this%20issue%2C%20we%20propose%20EXtreme%20DRL%0A%28EX-DRL%29%2C%20which%20enhances%20extreme%20quantile%20prediction%20by%20modeling%20the%20tail%20of%0Athe%20loss%20distribution%20with%20a%20Generalized%20Pareto%20Distribution%20%28GPD%29.%20This%20method%0Aintroduces%20supplementary%20data%20to%20mitigate%20the%20scarcity%20of%20extreme%20quantile%0Aobservations%2C%20thereby%20improving%20estimation%20accuracy%20through%20QR.%20Comprehensive%0Aexperiments%20on%20gamma%20hedging%20options%20demonstrate%20that%20EX-DRL%20improves%20existing%0AQR-based%20models%20by%20providing%20more%20precise%20estimates%20of%20extreme%20quantiles%2C%0Athereby%20improving%20the%20computation%20and%20reliability%20of%20risk%20metrics%20for%20complex%0Afinancial%20risk%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12446v1&entry.124074799=Read"},
{"title": "OPTDTALS: Approximate Logic Synthesis via Optimal Decision Trees\n  Approach", "author": "Hao Hu and Shaowei Cai", "abstract": "  The growing interest in Explainable Artificial Intelligence (XAI) motivates\npromising studies of computing optimal Interpretable Machine Learning models,\nespecially decision trees. Such models generally provide optimality in compact\nsize or empirical accuracy. Recent works focus on improving efficiency due to\nthe natural scalability issue. The application of such models to practical\nproblems is quite limited. As an emerging problem in circuit design,\nApproximate Logic Synthesis (ALS) aims to reduce circuit complexity by\nsacrificing correctness. Recently, multiple heuristic machine learning methods\nhave been applied in ALS, which learns approximated circuits from samples of\ninput-output pairs.\n  In this paper, we propose a new ALS methodology realizing the approximation\nvia learning optimal decision trees in empirical accuracy. Compared to previous\nheuristic ALS methods, the guarantee of optimality achieves a more controllable\ntrade-off between circuit complexity and accuracy. Experimental results show\nclear improvements in our methodology in the quality of approximated designs\n(circuit complexity and accuracy) compared to the state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2408.12304v1", "date": "2024-08-22", "relevancy": 1.7626, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4505}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.438}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPTDTALS%3A%20Approximate%20Logic%20Synthesis%20via%20Optimal%20Decision%20Trees%0A%20%20Approach&body=Title%3A%20OPTDTALS%3A%20Approximate%20Logic%20Synthesis%20via%20Optimal%20Decision%20Trees%0A%20%20Approach%0AAuthor%3A%20Hao%20Hu%20and%20Shaowei%20Cai%0AAbstract%3A%20%20%20The%20growing%20interest%20in%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20motivates%0Apromising%20studies%20of%20computing%20optimal%20Interpretable%20Machine%20Learning%20models%2C%0Aespecially%20decision%20trees.%20Such%20models%20generally%20provide%20optimality%20in%20compact%0Asize%20or%20empirical%20accuracy.%20Recent%20works%20focus%20on%20improving%20efficiency%20due%20to%0Athe%20natural%20scalability%20issue.%20The%20application%20of%20such%20models%20to%20practical%0Aproblems%20is%20quite%20limited.%20As%20an%20emerging%20problem%20in%20circuit%20design%2C%0AApproximate%20Logic%20Synthesis%20%28ALS%29%20aims%20to%20reduce%20circuit%20complexity%20by%0Asacrificing%20correctness.%20Recently%2C%20multiple%20heuristic%20machine%20learning%20methods%0Ahave%20been%20applied%20in%20ALS%2C%20which%20learns%20approximated%20circuits%20from%20samples%20of%0Ainput-output%20pairs.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20ALS%20methodology%20realizing%20the%20approximation%0Avia%20learning%20optimal%20decision%20trees%20in%20empirical%20accuracy.%20Compared%20to%20previous%0Aheuristic%20ALS%20methods%2C%20the%20guarantee%20of%20optimality%20achieves%20a%20more%20controllable%0Atrade-off%20between%20circuit%20complexity%20and%20accuracy.%20Experimental%20results%20show%0Aclear%20improvements%20in%20our%20methodology%20in%20the%20quality%20of%20approximated%20designs%0A%28circuit%20complexity%20and%20accuracy%29%20compared%20to%20the%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPTDTALS%253A%2520Approximate%2520Logic%2520Synthesis%2520via%2520Optimal%2520Decision%2520Trees%250A%2520%2520Approach%26entry.906535625%3DHao%2520Hu%2520and%2520Shaowei%2520Cai%26entry.1292438233%3D%2520%2520The%2520growing%2520interest%2520in%2520Explainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%2520motivates%250Apromising%2520studies%2520of%2520computing%2520optimal%2520Interpretable%2520Machine%2520Learning%2520models%252C%250Aespecially%2520decision%2520trees.%2520Such%2520models%2520generally%2520provide%2520optimality%2520in%2520compact%250Asize%2520or%2520empirical%2520accuracy.%2520Recent%2520works%2520focus%2520on%2520improving%2520efficiency%2520due%2520to%250Athe%2520natural%2520scalability%2520issue.%2520The%2520application%2520of%2520such%2520models%2520to%2520practical%250Aproblems%2520is%2520quite%2520limited.%2520As%2520an%2520emerging%2520problem%2520in%2520circuit%2520design%252C%250AApproximate%2520Logic%2520Synthesis%2520%2528ALS%2529%2520aims%2520to%2520reduce%2520circuit%2520complexity%2520by%250Asacrificing%2520correctness.%2520Recently%252C%2520multiple%2520heuristic%2520machine%2520learning%2520methods%250Ahave%2520been%2520applied%2520in%2520ALS%252C%2520which%2520learns%2520approximated%2520circuits%2520from%2520samples%2520of%250Ainput-output%2520pairs.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520ALS%2520methodology%2520realizing%2520the%2520approximation%250Avia%2520learning%2520optimal%2520decision%2520trees%2520in%2520empirical%2520accuracy.%2520Compared%2520to%2520previous%250Aheuristic%2520ALS%2520methods%252C%2520the%2520guarantee%2520of%2520optimality%2520achieves%2520a%2520more%2520controllable%250Atrade-off%2520between%2520circuit%2520complexity%2520and%2520accuracy.%2520Experimental%2520results%2520show%250Aclear%2520improvements%2520in%2520our%2520methodology%2520in%2520the%2520quality%2520of%2520approximated%2520designs%250A%2528circuit%2520complexity%2520and%2520accuracy%2529%2520compared%2520to%2520the%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPTDTALS%3A%20Approximate%20Logic%20Synthesis%20via%20Optimal%20Decision%20Trees%0A%20%20Approach&entry.906535625=Hao%20Hu%20and%20Shaowei%20Cai&entry.1292438233=%20%20The%20growing%20interest%20in%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20motivates%0Apromising%20studies%20of%20computing%20optimal%20Interpretable%20Machine%20Learning%20models%2C%0Aespecially%20decision%20trees.%20Such%20models%20generally%20provide%20optimality%20in%20compact%0Asize%20or%20empirical%20accuracy.%20Recent%20works%20focus%20on%20improving%20efficiency%20due%20to%0Athe%20natural%20scalability%20issue.%20The%20application%20of%20such%20models%20to%20practical%0Aproblems%20is%20quite%20limited.%20As%20an%20emerging%20problem%20in%20circuit%20design%2C%0AApproximate%20Logic%20Synthesis%20%28ALS%29%20aims%20to%20reduce%20circuit%20complexity%20by%0Asacrificing%20correctness.%20Recently%2C%20multiple%20heuristic%20machine%20learning%20methods%0Ahave%20been%20applied%20in%20ALS%2C%20which%20learns%20approximated%20circuits%20from%20samples%20of%0Ainput-output%20pairs.%0A%20%20In%20this%20paper%2C%20we%20propose%20a%20new%20ALS%20methodology%20realizing%20the%20approximation%0Avia%20learning%20optimal%20decision%20trees%20in%20empirical%20accuracy.%20Compared%20to%20previous%0Aheuristic%20ALS%20methods%2C%20the%20guarantee%20of%20optimality%20achieves%20a%20more%20controllable%0Atrade-off%20between%20circuit%20complexity%20and%20accuracy.%20Experimental%20results%20show%0Aclear%20improvements%20in%20our%20methodology%20in%20the%20quality%20of%20approximated%20designs%0A%28circuit%20complexity%20and%20accuracy%29%20compared%20to%20the%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12304v1&entry.124074799=Read"},
{"title": "Comparing YOLOv5 Variants for Vehicle Detection: A Performance Analysis", "author": "Athulya Sundaresan Geetha", "abstract": "  Vehicle detection is an important task in the management of traffic and\nautomatic vehicles. This study provides a comparative analysis of five YOLOv5\nvariants, YOLOv5n6s, YOLOv5s6s, YOLOv5m6s, YOLOv5l6s, and YOLOv5x6s, for\nvehicle detection in various environments. The research focuses on evaluating\nthe effectiveness of these models in detecting different types of vehicles,\nsuch as Car, Bus, Truck, Bicycle, and Motorcycle, under varying conditions\nincluding lighting, occlusion, and weather. Performance metrics such as\nprecision, recall, F1-score, and mean Average Precision are utilized to assess\nthe accuracy and reliability of each model. YOLOv5n6s demonstrated a strong\nbalance between precision and recall, particularly in detecting Cars. YOLOv5s6s\nand YOLOv5m6s showed improvements in recall, enhancing their ability to detect\nall relevant objects. YOLOv5l6s, with its larger capacity, provided robust\nperformance, especially in detecting Cars, but not good with identifying\nMotorcycles and Bicycles. YOLOv5x6s was effective in recognizing Buses and Cars\nbut faced challenges with Motorcycle class.\n", "link": "http://arxiv.org/abs/2408.12550v1", "date": "2024-08-22", "relevancy": 1.7531, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4445}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4377}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20YOLOv5%20Variants%20for%20Vehicle%20Detection%3A%20A%20Performance%20Analysis&body=Title%3A%20Comparing%20YOLOv5%20Variants%20for%20Vehicle%20Detection%3A%20A%20Performance%20Analysis%0AAuthor%3A%20Athulya%20Sundaresan%20Geetha%0AAbstract%3A%20%20%20Vehicle%20detection%20is%20an%20important%20task%20in%20the%20management%20of%20traffic%20and%0Aautomatic%20vehicles.%20This%20study%20provides%20a%20comparative%20analysis%20of%20five%20YOLOv5%0Avariants%2C%20YOLOv5n6s%2C%20YOLOv5s6s%2C%20YOLOv5m6s%2C%20YOLOv5l6s%2C%20and%20YOLOv5x6s%2C%20for%0Avehicle%20detection%20in%20various%20environments.%20The%20research%20focuses%20on%20evaluating%0Athe%20effectiveness%20of%20these%20models%20in%20detecting%20different%20types%20of%20vehicles%2C%0Asuch%20as%20Car%2C%20Bus%2C%20Truck%2C%20Bicycle%2C%20and%20Motorcycle%2C%20under%20varying%20conditions%0Aincluding%20lighting%2C%20occlusion%2C%20and%20weather.%20Performance%20metrics%20such%20as%0Aprecision%2C%20recall%2C%20F1-score%2C%20and%20mean%20Average%20Precision%20are%20utilized%20to%20assess%0Athe%20accuracy%20and%20reliability%20of%20each%20model.%20YOLOv5n6s%20demonstrated%20a%20strong%0Abalance%20between%20precision%20and%20recall%2C%20particularly%20in%20detecting%20Cars.%20YOLOv5s6s%0Aand%20YOLOv5m6s%20showed%20improvements%20in%20recall%2C%20enhancing%20their%20ability%20to%20detect%0Aall%20relevant%20objects.%20YOLOv5l6s%2C%20with%20its%20larger%20capacity%2C%20provided%20robust%0Aperformance%2C%20especially%20in%20detecting%20Cars%2C%20but%20not%20good%20with%20identifying%0AMotorcycles%20and%20Bicycles.%20YOLOv5x6s%20was%20effective%20in%20recognizing%20Buses%20and%20Cars%0Abut%20faced%20challenges%20with%20Motorcycle%20class.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12550v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520YOLOv5%2520Variants%2520for%2520Vehicle%2520Detection%253A%2520A%2520Performance%2520Analysis%26entry.906535625%3DAthulya%2520Sundaresan%2520Geetha%26entry.1292438233%3D%2520%2520Vehicle%2520detection%2520is%2520an%2520important%2520task%2520in%2520the%2520management%2520of%2520traffic%2520and%250Aautomatic%2520vehicles.%2520This%2520study%2520provides%2520a%2520comparative%2520analysis%2520of%2520five%2520YOLOv5%250Avariants%252C%2520YOLOv5n6s%252C%2520YOLOv5s6s%252C%2520YOLOv5m6s%252C%2520YOLOv5l6s%252C%2520and%2520YOLOv5x6s%252C%2520for%250Avehicle%2520detection%2520in%2520various%2520environments.%2520The%2520research%2520focuses%2520on%2520evaluating%250Athe%2520effectiveness%2520of%2520these%2520models%2520in%2520detecting%2520different%2520types%2520of%2520vehicles%252C%250Asuch%2520as%2520Car%252C%2520Bus%252C%2520Truck%252C%2520Bicycle%252C%2520and%2520Motorcycle%252C%2520under%2520varying%2520conditions%250Aincluding%2520lighting%252C%2520occlusion%252C%2520and%2520weather.%2520Performance%2520metrics%2520such%2520as%250Aprecision%252C%2520recall%252C%2520F1-score%252C%2520and%2520mean%2520Average%2520Precision%2520are%2520utilized%2520to%2520assess%250Athe%2520accuracy%2520and%2520reliability%2520of%2520each%2520model.%2520YOLOv5n6s%2520demonstrated%2520a%2520strong%250Abalance%2520between%2520precision%2520and%2520recall%252C%2520particularly%2520in%2520detecting%2520Cars.%2520YOLOv5s6s%250Aand%2520YOLOv5m6s%2520showed%2520improvements%2520in%2520recall%252C%2520enhancing%2520their%2520ability%2520to%2520detect%250Aall%2520relevant%2520objects.%2520YOLOv5l6s%252C%2520with%2520its%2520larger%2520capacity%252C%2520provided%2520robust%250Aperformance%252C%2520especially%2520in%2520detecting%2520Cars%252C%2520but%2520not%2520good%2520with%2520identifying%250AMotorcycles%2520and%2520Bicycles.%2520YOLOv5x6s%2520was%2520effective%2520in%2520recognizing%2520Buses%2520and%2520Cars%250Abut%2520faced%2520challenges%2520with%2520Motorcycle%2520class.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12550v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20YOLOv5%20Variants%20for%20Vehicle%20Detection%3A%20A%20Performance%20Analysis&entry.906535625=Athulya%20Sundaresan%20Geetha&entry.1292438233=%20%20Vehicle%20detection%20is%20an%20important%20task%20in%20the%20management%20of%20traffic%20and%0Aautomatic%20vehicles.%20This%20study%20provides%20a%20comparative%20analysis%20of%20five%20YOLOv5%0Avariants%2C%20YOLOv5n6s%2C%20YOLOv5s6s%2C%20YOLOv5m6s%2C%20YOLOv5l6s%2C%20and%20YOLOv5x6s%2C%20for%0Avehicle%20detection%20in%20various%20environments.%20The%20research%20focuses%20on%20evaluating%0Athe%20effectiveness%20of%20these%20models%20in%20detecting%20different%20types%20of%20vehicles%2C%0Asuch%20as%20Car%2C%20Bus%2C%20Truck%2C%20Bicycle%2C%20and%20Motorcycle%2C%20under%20varying%20conditions%0Aincluding%20lighting%2C%20occlusion%2C%20and%20weather.%20Performance%20metrics%20such%20as%0Aprecision%2C%20recall%2C%20F1-score%2C%20and%20mean%20Average%20Precision%20are%20utilized%20to%20assess%0Athe%20accuracy%20and%20reliability%20of%20each%20model.%20YOLOv5n6s%20demonstrated%20a%20strong%0Abalance%20between%20precision%20and%20recall%2C%20particularly%20in%20detecting%20Cars.%20YOLOv5s6s%0Aand%20YOLOv5m6s%20showed%20improvements%20in%20recall%2C%20enhancing%20their%20ability%20to%20detect%0Aall%20relevant%20objects.%20YOLOv5l6s%2C%20with%20its%20larger%20capacity%2C%20provided%20robust%0Aperformance%2C%20especially%20in%20detecting%20Cars%2C%20but%20not%20good%20with%20identifying%0AMotorcycles%20and%20Bicycles.%20YOLOv5x6s%20was%20effective%20in%20recognizing%20Buses%20and%20Cars%0Abut%20faced%20challenges%20with%20Motorcycle%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12550v1&entry.124074799=Read"},
{"title": "ManipVQA: Injecting Robotic Affordance and Physically Grounded\n  Information into Multi-Modal Large Language Models", "author": "Siyuan Huang and Iaroslav Ponomarenko and Zhengkai Jiang and Xiaoqi Li and Xiaobin Hu and Peng Gao and Hongsheng Li and Hao Dong", "abstract": "  While the integration of Multi-modal Large Language Models (MLLMs) with\nrobotic systems has significantly improved robots' ability to understand and\nexecute natural language instructions, their performance in manipulation tasks\nremains limited due to a lack of robotics-specific knowledge. Conventional\nMLLMs are typically trained on generic image-text pairs, leaving them deficient\nin understanding affordances and physical concepts crucial for manipulation. To\naddress this gap, we propose ManipVQA, a novel framework that infuses MLLMs\nwith manipulation-centric knowledge through a Visual Question-Answering (VQA)\nformat. This approach encompasses tool detection, affordance recognition, and a\nbroader understanding of physical concepts. We curated a diverse dataset of\nimages depicting interactive objects, to challenge robotic understanding in\ntool detection, affordance prediction, and physical concept comprehension. To\neffectively integrate this robotics-specific knowledge with the inherent\nvision-reasoning capabilities of MLLMs, we leverage a unified VQA format and\ndevise a fine-tuning strategy. This strategy preserves the original\nvision-reasoning abilities while incorporating the newly acquired robotic\ninsights. Empirical evaluations conducted in robotic simulators and across\nvarious vision task benchmarks demonstrate the robust performance of ManipVQA.\nThe code and dataset are publicly available at\nhttps://github.com/SiyuanHuang95/ManipVQA.\n", "link": "http://arxiv.org/abs/2403.11289v2", "date": "2024-08-22", "relevancy": 1.7454, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6327}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6004}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ManipVQA%3A%20Injecting%20Robotic%20Affordance%20and%20Physically%20Grounded%0A%20%20Information%20into%20Multi-Modal%20Large%20Language%20Models&body=Title%3A%20ManipVQA%3A%20Injecting%20Robotic%20Affordance%20and%20Physically%20Grounded%0A%20%20Information%20into%20Multi-Modal%20Large%20Language%20Models%0AAuthor%3A%20Siyuan%20Huang%20and%20Iaroslav%20Ponomarenko%20and%20Zhengkai%20Jiang%20and%20Xiaoqi%20Li%20and%20Xiaobin%20Hu%20and%20Peng%20Gao%20and%20Hongsheng%20Li%20and%20Hao%20Dong%0AAbstract%3A%20%20%20While%20the%20integration%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20with%0Arobotic%20systems%20has%20significantly%20improved%20robots%27%20ability%20to%20understand%20and%0Aexecute%20natural%20language%20instructions%2C%20their%20performance%20in%20manipulation%20tasks%0Aremains%20limited%20due%20to%20a%20lack%20of%20robotics-specific%20knowledge.%20Conventional%0AMLLMs%20are%20typically%20trained%20on%20generic%20image-text%20pairs%2C%20leaving%20them%20deficient%0Ain%20understanding%20affordances%20and%20physical%20concepts%20crucial%20for%20manipulation.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20ManipVQA%2C%20a%20novel%20framework%20that%20infuses%20MLLMs%0Awith%20manipulation-centric%20knowledge%20through%20a%20Visual%20Question-Answering%20%28VQA%29%0Aformat.%20This%20approach%20encompasses%20tool%20detection%2C%20affordance%20recognition%2C%20and%20a%0Abroader%20understanding%20of%20physical%20concepts.%20We%20curated%20a%20diverse%20dataset%20of%0Aimages%20depicting%20interactive%20objects%2C%20to%20challenge%20robotic%20understanding%20in%0Atool%20detection%2C%20affordance%20prediction%2C%20and%20physical%20concept%20comprehension.%20To%0Aeffectively%20integrate%20this%20robotics-specific%20knowledge%20with%20the%20inherent%0Avision-reasoning%20capabilities%20of%20MLLMs%2C%20we%20leverage%20a%20unified%20VQA%20format%20and%0Adevise%20a%20fine-tuning%20strategy.%20This%20strategy%20preserves%20the%20original%0Avision-reasoning%20abilities%20while%20incorporating%20the%20newly%20acquired%20robotic%0Ainsights.%20Empirical%20evaluations%20conducted%20in%20robotic%20simulators%20and%20across%0Avarious%20vision%20task%20benchmarks%20demonstrate%20the%20robust%20performance%20of%20ManipVQA.%0AThe%20code%20and%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/SiyuanHuang95/ManipVQA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManipVQA%253A%2520Injecting%2520Robotic%2520Affordance%2520and%2520Physically%2520Grounded%250A%2520%2520Information%2520into%2520Multi-Modal%2520Large%2520Language%2520Models%26entry.906535625%3DSiyuan%2520Huang%2520and%2520Iaroslav%2520Ponomarenko%2520and%2520Zhengkai%2520Jiang%2520and%2520Xiaoqi%2520Li%2520and%2520Xiaobin%2520Hu%2520and%2520Peng%2520Gao%2520and%2520Hongsheng%2520Li%2520and%2520Hao%2520Dong%26entry.1292438233%3D%2520%2520While%2520the%2520integration%2520of%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520with%250Arobotic%2520systems%2520has%2520significantly%2520improved%2520robots%2527%2520ability%2520to%2520understand%2520and%250Aexecute%2520natural%2520language%2520instructions%252C%2520their%2520performance%2520in%2520manipulation%2520tasks%250Aremains%2520limited%2520due%2520to%2520a%2520lack%2520of%2520robotics-specific%2520knowledge.%2520Conventional%250AMLLMs%2520are%2520typically%2520trained%2520on%2520generic%2520image-text%2520pairs%252C%2520leaving%2520them%2520deficient%250Ain%2520understanding%2520affordances%2520and%2520physical%2520concepts%2520crucial%2520for%2520manipulation.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520propose%2520ManipVQA%252C%2520a%2520novel%2520framework%2520that%2520infuses%2520MLLMs%250Awith%2520manipulation-centric%2520knowledge%2520through%2520a%2520Visual%2520Question-Answering%2520%2528VQA%2529%250Aformat.%2520This%2520approach%2520encompasses%2520tool%2520detection%252C%2520affordance%2520recognition%252C%2520and%2520a%250Abroader%2520understanding%2520of%2520physical%2520concepts.%2520We%2520curated%2520a%2520diverse%2520dataset%2520of%250Aimages%2520depicting%2520interactive%2520objects%252C%2520to%2520challenge%2520robotic%2520understanding%2520in%250Atool%2520detection%252C%2520affordance%2520prediction%252C%2520and%2520physical%2520concept%2520comprehension.%2520To%250Aeffectively%2520integrate%2520this%2520robotics-specific%2520knowledge%2520with%2520the%2520inherent%250Avision-reasoning%2520capabilities%2520of%2520MLLMs%252C%2520we%2520leverage%2520a%2520unified%2520VQA%2520format%2520and%250Adevise%2520a%2520fine-tuning%2520strategy.%2520This%2520strategy%2520preserves%2520the%2520original%250Avision-reasoning%2520abilities%2520while%2520incorporating%2520the%2520newly%2520acquired%2520robotic%250Ainsights.%2520Empirical%2520evaluations%2520conducted%2520in%2520robotic%2520simulators%2520and%2520across%250Avarious%2520vision%2520task%2520benchmarks%2520demonstrate%2520the%2520robust%2520performance%2520of%2520ManipVQA.%250AThe%2520code%2520and%2520dataset%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/SiyuanHuang95/ManipVQA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ManipVQA%3A%20Injecting%20Robotic%20Affordance%20and%20Physically%20Grounded%0A%20%20Information%20into%20Multi-Modal%20Large%20Language%20Models&entry.906535625=Siyuan%20Huang%20and%20Iaroslav%20Ponomarenko%20and%20Zhengkai%20Jiang%20and%20Xiaoqi%20Li%20and%20Xiaobin%20Hu%20and%20Peng%20Gao%20and%20Hongsheng%20Li%20and%20Hao%20Dong&entry.1292438233=%20%20While%20the%20integration%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20with%0Arobotic%20systems%20has%20significantly%20improved%20robots%27%20ability%20to%20understand%20and%0Aexecute%20natural%20language%20instructions%2C%20their%20performance%20in%20manipulation%20tasks%0Aremains%20limited%20due%20to%20a%20lack%20of%20robotics-specific%20knowledge.%20Conventional%0AMLLMs%20are%20typically%20trained%20on%20generic%20image-text%20pairs%2C%20leaving%20them%20deficient%0Ain%20understanding%20affordances%20and%20physical%20concepts%20crucial%20for%20manipulation.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20ManipVQA%2C%20a%20novel%20framework%20that%20infuses%20MLLMs%0Awith%20manipulation-centric%20knowledge%20through%20a%20Visual%20Question-Answering%20%28VQA%29%0Aformat.%20This%20approach%20encompasses%20tool%20detection%2C%20affordance%20recognition%2C%20and%20a%0Abroader%20understanding%20of%20physical%20concepts.%20We%20curated%20a%20diverse%20dataset%20of%0Aimages%20depicting%20interactive%20objects%2C%20to%20challenge%20robotic%20understanding%20in%0Atool%20detection%2C%20affordance%20prediction%2C%20and%20physical%20concept%20comprehension.%20To%0Aeffectively%20integrate%20this%20robotics-specific%20knowledge%20with%20the%20inherent%0Avision-reasoning%20capabilities%20of%20MLLMs%2C%20we%20leverage%20a%20unified%20VQA%20format%20and%0Adevise%20a%20fine-tuning%20strategy.%20This%20strategy%20preserves%20the%20original%0Avision-reasoning%20abilities%20while%20incorporating%20the%20newly%20acquired%20robotic%0Ainsights.%20Empirical%20evaluations%20conducted%20in%20robotic%20simulators%20and%20across%0Avarious%20vision%20task%20benchmarks%20demonstrate%20the%20robust%20performance%20of%20ManipVQA.%0AThe%20code%20and%20dataset%20are%20publicly%20available%20at%0Ahttps%3A//github.com/SiyuanHuang95/ManipVQA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11289v2&entry.124074799=Read"},
{"title": "A New Chinese Landscape Paintings Generation Model based on Stable\n  Diffusion using DreamBooth", "author": "Yujia Gu and Xinyu Fang and Xueyuan Deng and Zihan Peng and Yinan Peng", "abstract": "  This study mainly introduces a method combining the Stable Diffusion Model\n(SDM) and Parameter-Efficient Fine-Tuning method for generating Chinese\nLandscape Paintings. This training process is accelerated by combining LoRA\nwith pre-trained SDM and DreamBooth with pre-trained SDM, respectively. On the\nChinese Landscape Paintings Internet dataset used in this paper, this study\nfinds that SDM combined with DreamBooth exhibits superior performance,\noutperforming other models, including the generic pre-trained SDM and\nLoRA-based fine-tuning SDM. The SDM combined with DreamBooth achieves a FID of\n12.75 on the dataset and outperforms all other models in terms of expert\nevaluation, highlighting the model's versatility in the field of Chinese\nLandscape Paintings given the unique identifier, high fidelity and high\nquality. This study illustrates the potential of specialised fine-tuning method\nto improve the performance of SDM on domain-specific tasks, particularly in the\ndomain of Landscape Paintings.\n", "link": "http://arxiv.org/abs/2408.08561v3", "date": "2024-08-22", "relevancy": 1.744, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.611}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5795}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Chinese%20Landscape%20Paintings%20Generation%20Model%20based%20on%20Stable%0A%20%20Diffusion%20using%20DreamBooth&body=Title%3A%20A%20New%20Chinese%20Landscape%20Paintings%20Generation%20Model%20based%20on%20Stable%0A%20%20Diffusion%20using%20DreamBooth%0AAuthor%3A%20Yujia%20Gu%20and%20Xinyu%20Fang%20and%20Xueyuan%20Deng%20and%20Zihan%20Peng%20and%20Yinan%20Peng%0AAbstract%3A%20%20%20This%20study%20mainly%20introduces%20a%20method%20combining%20the%20Stable%20Diffusion%20Model%0A%28SDM%29%20and%20Parameter-Efficient%20Fine-Tuning%20method%20for%20generating%20Chinese%0ALandscape%20Paintings.%20This%20training%20process%20is%20accelerated%20by%20combining%20LoRA%0Awith%20pre-trained%20SDM%20and%20DreamBooth%20with%20pre-trained%20SDM%2C%20respectively.%20On%20the%0AChinese%20Landscape%20Paintings%20Internet%20dataset%20used%20in%20this%20paper%2C%20this%20study%0Afinds%20that%20SDM%20combined%20with%20DreamBooth%20exhibits%20superior%20performance%2C%0Aoutperforming%20other%20models%2C%20including%20the%20generic%20pre-trained%20SDM%20and%0ALoRA-based%20fine-tuning%20SDM.%20The%20SDM%20combined%20with%20DreamBooth%20achieves%20a%20FID%20of%0A12.75%20on%20the%20dataset%20and%20outperforms%20all%20other%20models%20in%20terms%20of%20expert%0Aevaluation%2C%20highlighting%20the%20model%27s%20versatility%20in%20the%20field%20of%20Chinese%0ALandscape%20Paintings%20given%20the%20unique%20identifier%2C%20high%20fidelity%20and%20high%0Aquality.%20This%20study%20illustrates%20the%20potential%20of%20specialised%20fine-tuning%20method%0Ato%20improve%20the%20performance%20of%20SDM%20on%20domain-specific%20tasks%2C%20particularly%20in%20the%0Adomain%20of%20Landscape%20Paintings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08561v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Chinese%2520Landscape%2520Paintings%2520Generation%2520Model%2520based%2520on%2520Stable%250A%2520%2520Diffusion%2520using%2520DreamBooth%26entry.906535625%3DYujia%2520Gu%2520and%2520Xinyu%2520Fang%2520and%2520Xueyuan%2520Deng%2520and%2520Zihan%2520Peng%2520and%2520Yinan%2520Peng%26entry.1292438233%3D%2520%2520This%2520study%2520mainly%2520introduces%2520a%2520method%2520combining%2520the%2520Stable%2520Diffusion%2520Model%250A%2528SDM%2529%2520and%2520Parameter-Efficient%2520Fine-Tuning%2520method%2520for%2520generating%2520Chinese%250ALandscape%2520Paintings.%2520This%2520training%2520process%2520is%2520accelerated%2520by%2520combining%2520LoRA%250Awith%2520pre-trained%2520SDM%2520and%2520DreamBooth%2520with%2520pre-trained%2520SDM%252C%2520respectively.%2520On%2520the%250AChinese%2520Landscape%2520Paintings%2520Internet%2520dataset%2520used%2520in%2520this%2520paper%252C%2520this%2520study%250Afinds%2520that%2520SDM%2520combined%2520with%2520DreamBooth%2520exhibits%2520superior%2520performance%252C%250Aoutperforming%2520other%2520models%252C%2520including%2520the%2520generic%2520pre-trained%2520SDM%2520and%250ALoRA-based%2520fine-tuning%2520SDM.%2520The%2520SDM%2520combined%2520with%2520DreamBooth%2520achieves%2520a%2520FID%2520of%250A12.75%2520on%2520the%2520dataset%2520and%2520outperforms%2520all%2520other%2520models%2520in%2520terms%2520of%2520expert%250Aevaluation%252C%2520highlighting%2520the%2520model%2527s%2520versatility%2520in%2520the%2520field%2520of%2520Chinese%250ALandscape%2520Paintings%2520given%2520the%2520unique%2520identifier%252C%2520high%2520fidelity%2520and%2520high%250Aquality.%2520This%2520study%2520illustrates%2520the%2520potential%2520of%2520specialised%2520fine-tuning%2520method%250Ato%2520improve%2520the%2520performance%2520of%2520SDM%2520on%2520domain-specific%2520tasks%252C%2520particularly%2520in%2520the%250Adomain%2520of%2520Landscape%2520Paintings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08561v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Chinese%20Landscape%20Paintings%20Generation%20Model%20based%20on%20Stable%0A%20%20Diffusion%20using%20DreamBooth&entry.906535625=Yujia%20Gu%20and%20Xinyu%20Fang%20and%20Xueyuan%20Deng%20and%20Zihan%20Peng%20and%20Yinan%20Peng&entry.1292438233=%20%20This%20study%20mainly%20introduces%20a%20method%20combining%20the%20Stable%20Diffusion%20Model%0A%28SDM%29%20and%20Parameter-Efficient%20Fine-Tuning%20method%20for%20generating%20Chinese%0ALandscape%20Paintings.%20This%20training%20process%20is%20accelerated%20by%20combining%20LoRA%0Awith%20pre-trained%20SDM%20and%20DreamBooth%20with%20pre-trained%20SDM%2C%20respectively.%20On%20the%0AChinese%20Landscape%20Paintings%20Internet%20dataset%20used%20in%20this%20paper%2C%20this%20study%0Afinds%20that%20SDM%20combined%20with%20DreamBooth%20exhibits%20superior%20performance%2C%0Aoutperforming%20other%20models%2C%20including%20the%20generic%20pre-trained%20SDM%20and%0ALoRA-based%20fine-tuning%20SDM.%20The%20SDM%20combined%20with%20DreamBooth%20achieves%20a%20FID%20of%0A12.75%20on%20the%20dataset%20and%20outperforms%20all%20other%20models%20in%20terms%20of%20expert%0Aevaluation%2C%20highlighting%20the%20model%27s%20versatility%20in%20the%20field%20of%20Chinese%0ALandscape%20Paintings%20given%20the%20unique%20identifier%2C%20high%20fidelity%20and%20high%0Aquality.%20This%20study%20illustrates%20the%20potential%20of%20specialised%20fine-tuning%20method%0Ato%20improve%20the%20performance%20of%20SDM%20on%20domain-specific%20tasks%2C%20particularly%20in%20the%0Adomain%20of%20Landscape%20Paintings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08561v3&entry.124074799=Read"},
{"title": "Neural-ANOVA: Model Decomposition for Interpretable Machine Learning", "author": "Steffen Limmer and Steffen Udluft and Clemens Otte", "abstract": "  The analysis of variance (ANOVA) decomposition offers a systematic method to\nunderstand the interaction effects that contribute to a specific decision\noutput. In this paper we introduce Neural-ANOVA, an approach to decompose\nneural networks into glassbox models using the ANOVA decomposition. Our\napproach formulates a learning problem, which enables rapid and closed-form\nevaluation of integrals over subspaces that appear in the calculation of the\nANOVA decomposition. Finally, we conduct numerical experiments to illustrate\nthe advantages of enhanced interpretability and model validation by a\ndecomposition of the learned interaction effects.\n", "link": "http://arxiv.org/abs/2408.12319v1", "date": "2024-08-22", "relevancy": 1.7318, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4346}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4345}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural-ANOVA%3A%20Model%20Decomposition%20for%20Interpretable%20Machine%20Learning&body=Title%3A%20Neural-ANOVA%3A%20Model%20Decomposition%20for%20Interpretable%20Machine%20Learning%0AAuthor%3A%20Steffen%20Limmer%20and%20Steffen%20Udluft%20and%20Clemens%20Otte%0AAbstract%3A%20%20%20The%20analysis%20of%20variance%20%28ANOVA%29%20decomposition%20offers%20a%20systematic%20method%20to%0Aunderstand%20the%20interaction%20effects%20that%20contribute%20to%20a%20specific%20decision%0Aoutput.%20In%20this%20paper%20we%20introduce%20Neural-ANOVA%2C%20an%20approach%20to%20decompose%0Aneural%20networks%20into%20glassbox%20models%20using%20the%20ANOVA%20decomposition.%20Our%0Aapproach%20formulates%20a%20learning%20problem%2C%20which%20enables%20rapid%20and%20closed-form%0Aevaluation%20of%20integrals%20over%20subspaces%20that%20appear%20in%20the%20calculation%20of%20the%0AANOVA%20decomposition.%20Finally%2C%20we%20conduct%20numerical%20experiments%20to%20illustrate%0Athe%20advantages%20of%20enhanced%20interpretability%20and%20model%20validation%20by%20a%0Adecomposition%20of%20the%20learned%20interaction%20effects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural-ANOVA%253A%2520Model%2520Decomposition%2520for%2520Interpretable%2520Machine%2520Learning%26entry.906535625%3DSteffen%2520Limmer%2520and%2520Steffen%2520Udluft%2520and%2520Clemens%2520Otte%26entry.1292438233%3D%2520%2520The%2520analysis%2520of%2520variance%2520%2528ANOVA%2529%2520decomposition%2520offers%2520a%2520systematic%2520method%2520to%250Aunderstand%2520the%2520interaction%2520effects%2520that%2520contribute%2520to%2520a%2520specific%2520decision%250Aoutput.%2520In%2520this%2520paper%2520we%2520introduce%2520Neural-ANOVA%252C%2520an%2520approach%2520to%2520decompose%250Aneural%2520networks%2520into%2520glassbox%2520models%2520using%2520the%2520ANOVA%2520decomposition.%2520Our%250Aapproach%2520formulates%2520a%2520learning%2520problem%252C%2520which%2520enables%2520rapid%2520and%2520closed-form%250Aevaluation%2520of%2520integrals%2520over%2520subspaces%2520that%2520appear%2520in%2520the%2520calculation%2520of%2520the%250AANOVA%2520decomposition.%2520Finally%252C%2520we%2520conduct%2520numerical%2520experiments%2520to%2520illustrate%250Athe%2520advantages%2520of%2520enhanced%2520interpretability%2520and%2520model%2520validation%2520by%2520a%250Adecomposition%2520of%2520the%2520learned%2520interaction%2520effects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural-ANOVA%3A%20Model%20Decomposition%20for%20Interpretable%20Machine%20Learning&entry.906535625=Steffen%20Limmer%20and%20Steffen%20Udluft%20and%20Clemens%20Otte&entry.1292438233=%20%20The%20analysis%20of%20variance%20%28ANOVA%29%20decomposition%20offers%20a%20systematic%20method%20to%0Aunderstand%20the%20interaction%20effects%20that%20contribute%20to%20a%20specific%20decision%0Aoutput.%20In%20this%20paper%20we%20introduce%20Neural-ANOVA%2C%20an%20approach%20to%20decompose%0Aneural%20networks%20into%20glassbox%20models%20using%20the%20ANOVA%20decomposition.%20Our%0Aapproach%20formulates%20a%20learning%20problem%2C%20which%20enables%20rapid%20and%20closed-form%0Aevaluation%20of%20integrals%20over%20subspaces%20that%20appear%20in%20the%20calculation%20of%20the%0AANOVA%20decomposition.%20Finally%2C%20we%20conduct%20numerical%20experiments%20to%20illustrate%0Athe%20advantages%20of%20enhanced%20interpretability%20and%20model%20validation%20by%20a%0Adecomposition%20of%20the%20learned%20interaction%20effects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12319v1&entry.124074799=Read"},
{"title": "Show-o: One Single Transformer to Unify Multimodal Understanding and\n  Generation", "author": "Jinheng Xie and Weijia Mao and Zechen Bai and David Junhao Zhang and Weihao Wang and Kevin Qinghong Lin and Yuchao Gu and Zhijie Chen and Zhenheng Yang and Mike Zheng Shou", "abstract": "  We present a unified transformer, i.e., Show-o, that unifies multimodal\nunderstanding and generation. Unlike fully autoregressive models, Show-o\nunifies autoregressive and (discrete) diffusion modeling to adaptively handle\ninputs and outputs of various and mixed modalities. The unified model flexibly\nsupports a wide range of vision-language tasks including visual\nquestion-answering, text-to-image generation, text-guided\ninpainting/extrapolation, and mixed-modality generation. Across various\nbenchmarks, it demonstrates comparable or superior performance to existing\nindividual models with an equivalent or larger number of parameters tailored\nfor understanding or generation. This significantly highlights its potential as\na next-generation foundation model. Code and models are released at\nhttps://github.com/showlab/Show-o.\n", "link": "http://arxiv.org/abs/2408.12528v1", "date": "2024-08-22", "relevancy": 1.7142, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6043}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.563}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Show-o%3A%20One%20Single%20Transformer%20to%20Unify%20Multimodal%20Understanding%20and%0A%20%20Generation&body=Title%3A%20Show-o%3A%20One%20Single%20Transformer%20to%20Unify%20Multimodal%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Jinheng%20Xie%20and%20Weijia%20Mao%20and%20Zechen%20Bai%20and%20David%20Junhao%20Zhang%20and%20Weihao%20Wang%20and%20Kevin%20Qinghong%20Lin%20and%20Yuchao%20Gu%20and%20Zhijie%20Chen%20and%20Zhenheng%20Yang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20We%20present%20a%20unified%20transformer%2C%20i.e.%2C%20Show-o%2C%20that%20unifies%20multimodal%0Aunderstanding%20and%20generation.%20Unlike%20fully%20autoregressive%20models%2C%20Show-o%0Aunifies%20autoregressive%20and%20%28discrete%29%20diffusion%20modeling%20to%20adaptively%20handle%0Ainputs%20and%20outputs%20of%20various%20and%20mixed%20modalities.%20The%20unified%20model%20flexibly%0Asupports%20a%20wide%20range%20of%20vision-language%20tasks%20including%20visual%0Aquestion-answering%2C%20text-to-image%20generation%2C%20text-guided%0Ainpainting/extrapolation%2C%20and%20mixed-modality%20generation.%20Across%20various%0Abenchmarks%2C%20it%20demonstrates%20comparable%20or%20superior%20performance%20to%20existing%0Aindividual%20models%20with%20an%20equivalent%20or%20larger%20number%20of%20parameters%20tailored%0Afor%20understanding%20or%20generation.%20This%20significantly%20highlights%20its%20potential%20as%0Aa%20next-generation%20foundation%20model.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/showlab/Show-o.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShow-o%253A%2520One%2520Single%2520Transformer%2520to%2520Unify%2520Multimodal%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DJinheng%2520Xie%2520and%2520Weijia%2520Mao%2520and%2520Zechen%2520Bai%2520and%2520David%2520Junhao%2520Zhang%2520and%2520Weihao%2520Wang%2520and%2520Kevin%2520Qinghong%2520Lin%2520and%2520Yuchao%2520Gu%2520and%2520Zhijie%2520Chen%2520and%2520Zhenheng%2520Yang%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520unified%2520transformer%252C%2520i.e.%252C%2520Show-o%252C%2520that%2520unifies%2520multimodal%250Aunderstanding%2520and%2520generation.%2520Unlike%2520fully%2520autoregressive%2520models%252C%2520Show-o%250Aunifies%2520autoregressive%2520and%2520%2528discrete%2529%2520diffusion%2520modeling%2520to%2520adaptively%2520handle%250Ainputs%2520and%2520outputs%2520of%2520various%2520and%2520mixed%2520modalities.%2520The%2520unified%2520model%2520flexibly%250Asupports%2520a%2520wide%2520range%2520of%2520vision-language%2520tasks%2520including%2520visual%250Aquestion-answering%252C%2520text-to-image%2520generation%252C%2520text-guided%250Ainpainting/extrapolation%252C%2520and%2520mixed-modality%2520generation.%2520Across%2520various%250Abenchmarks%252C%2520it%2520demonstrates%2520comparable%2520or%2520superior%2520performance%2520to%2520existing%250Aindividual%2520models%2520with%2520an%2520equivalent%2520or%2520larger%2520number%2520of%2520parameters%2520tailored%250Afor%2520understanding%2520or%2520generation.%2520This%2520significantly%2520highlights%2520its%2520potential%2520as%250Aa%2520next-generation%2520foundation%2520model.%2520Code%2520and%2520models%2520are%2520released%2520at%250Ahttps%253A//github.com/showlab/Show-o.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Show-o%3A%20One%20Single%20Transformer%20to%20Unify%20Multimodal%20Understanding%20and%0A%20%20Generation&entry.906535625=Jinheng%20Xie%20and%20Weijia%20Mao%20and%20Zechen%20Bai%20and%20David%20Junhao%20Zhang%20and%20Weihao%20Wang%20and%20Kevin%20Qinghong%20Lin%20and%20Yuchao%20Gu%20and%20Zhijie%20Chen%20and%20Zhenheng%20Yang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20We%20present%20a%20unified%20transformer%2C%20i.e.%2C%20Show-o%2C%20that%20unifies%20multimodal%0Aunderstanding%20and%20generation.%20Unlike%20fully%20autoregressive%20models%2C%20Show-o%0Aunifies%20autoregressive%20and%20%28discrete%29%20diffusion%20modeling%20to%20adaptively%20handle%0Ainputs%20and%20outputs%20of%20various%20and%20mixed%20modalities.%20The%20unified%20model%20flexibly%0Asupports%20a%20wide%20range%20of%20vision-language%20tasks%20including%20visual%0Aquestion-answering%2C%20text-to-image%20generation%2C%20text-guided%0Ainpainting/extrapolation%2C%20and%20mixed-modality%20generation.%20Across%20various%0Abenchmarks%2C%20it%20demonstrates%20comparable%20or%20superior%20performance%20to%20existing%0Aindividual%20models%20with%20an%20equivalent%20or%20larger%20number%20of%20parameters%20tailored%0Afor%20understanding%20or%20generation.%20This%20significantly%20highlights%20its%20potential%20as%0Aa%20next-generation%20foundation%20model.%20Code%20and%20models%20are%20released%20at%0Ahttps%3A//github.com/showlab/Show-o.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12528v1&entry.124074799=Read"},
{"title": "Multimodal Foundational Models for Unsupervised 3D General Obstacle\n  Detection", "author": "Tam\u00e1s Matuszka and P\u00e9ter Hajas and D\u00e1vid Szeghy", "abstract": "  Current autonomous driving perception models primarily rely on supervised\nlearning with predefined categories. However, these models struggle to detect\ngeneral obstacles not included in the fixed category set due to their\nvariability and numerous edge cases. To address this issue, we propose a\ncombination of multimodal foundational model-based obstacle segmentation with\ntraditional unsupervised computational geometry-based outlier detection. Our\napproach operates offline, allowing us to leverage non-causality, and utilizes\ntraining-free methods. This enables the detection of general obstacles in 3D\nwithout the need for expensive retraining. To overcome the limitations of\npublicly available obstacle detection datasets, we collected and annotated our\ndataset, which includes various obstacles even in distant regions.\n", "link": "http://arxiv.org/abs/2408.12322v1", "date": "2024-08-22", "relevancy": 1.7095, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5761}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5711}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Foundational%20Models%20for%20Unsupervised%203D%20General%20Obstacle%0A%20%20Detection&body=Title%3A%20Multimodal%20Foundational%20Models%20for%20Unsupervised%203D%20General%20Obstacle%0A%20%20Detection%0AAuthor%3A%20Tam%C3%A1s%20Matuszka%20and%20P%C3%A9ter%20Hajas%20and%20D%C3%A1vid%20Szeghy%0AAbstract%3A%20%20%20Current%20autonomous%20driving%20perception%20models%20primarily%20rely%20on%20supervised%0Alearning%20with%20predefined%20categories.%20However%2C%20these%20models%20struggle%20to%20detect%0Ageneral%20obstacles%20not%20included%20in%20the%20fixed%20category%20set%20due%20to%20their%0Avariability%20and%20numerous%20edge%20cases.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Acombination%20of%20multimodal%20foundational%20model-based%20obstacle%20segmentation%20with%0Atraditional%20unsupervised%20computational%20geometry-based%20outlier%20detection.%20Our%0Aapproach%20operates%20offline%2C%20allowing%20us%20to%20leverage%20non-causality%2C%20and%20utilizes%0Atraining-free%20methods.%20This%20enables%20the%20detection%20of%20general%20obstacles%20in%203D%0Awithout%20the%20need%20for%20expensive%20retraining.%20To%20overcome%20the%20limitations%20of%0Apublicly%20available%20obstacle%20detection%20datasets%2C%20we%20collected%20and%20annotated%20our%0Adataset%2C%20which%20includes%20various%20obstacles%20even%20in%20distant%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Foundational%2520Models%2520for%2520Unsupervised%25203D%2520General%2520Obstacle%250A%2520%2520Detection%26entry.906535625%3DTam%25C3%25A1s%2520Matuszka%2520and%2520P%25C3%25A9ter%2520Hajas%2520and%2520D%25C3%25A1vid%2520Szeghy%26entry.1292438233%3D%2520%2520Current%2520autonomous%2520driving%2520perception%2520models%2520primarily%2520rely%2520on%2520supervised%250Alearning%2520with%2520predefined%2520categories.%2520However%252C%2520these%2520models%2520struggle%2520to%2520detect%250Ageneral%2520obstacles%2520not%2520included%2520in%2520the%2520fixed%2520category%2520set%2520due%2520to%2520their%250Avariability%2520and%2520numerous%2520edge%2520cases.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Acombination%2520of%2520multimodal%2520foundational%2520model-based%2520obstacle%2520segmentation%2520with%250Atraditional%2520unsupervised%2520computational%2520geometry-based%2520outlier%2520detection.%2520Our%250Aapproach%2520operates%2520offline%252C%2520allowing%2520us%2520to%2520leverage%2520non-causality%252C%2520and%2520utilizes%250Atraining-free%2520methods.%2520This%2520enables%2520the%2520detection%2520of%2520general%2520obstacles%2520in%25203D%250Awithout%2520the%2520need%2520for%2520expensive%2520retraining.%2520To%2520overcome%2520the%2520limitations%2520of%250Apublicly%2520available%2520obstacle%2520detection%2520datasets%252C%2520we%2520collected%2520and%2520annotated%2520our%250Adataset%252C%2520which%2520includes%2520various%2520obstacles%2520even%2520in%2520distant%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Foundational%20Models%20for%20Unsupervised%203D%20General%20Obstacle%0A%20%20Detection&entry.906535625=Tam%C3%A1s%20Matuszka%20and%20P%C3%A9ter%20Hajas%20and%20D%C3%A1vid%20Szeghy&entry.1292438233=%20%20Current%20autonomous%20driving%20perception%20models%20primarily%20rely%20on%20supervised%0Alearning%20with%20predefined%20categories.%20However%2C%20these%20models%20struggle%20to%20detect%0Ageneral%20obstacles%20not%20included%20in%20the%20fixed%20category%20set%20due%20to%20their%0Avariability%20and%20numerous%20edge%20cases.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Acombination%20of%20multimodal%20foundational%20model-based%20obstacle%20segmentation%20with%0Atraditional%20unsupervised%20computational%20geometry-based%20outlier%20detection.%20Our%0Aapproach%20operates%20offline%2C%20allowing%20us%20to%20leverage%20non-causality%2C%20and%20utilizes%0Atraining-free%20methods.%20This%20enables%20the%20detection%20of%20general%20obstacles%20in%203D%0Awithout%20the%20need%20for%20expensive%20retraining.%20To%20overcome%20the%20limitations%20of%0Apublicly%20available%20obstacle%20detection%20datasets%2C%20we%20collected%20and%20annotated%20our%0Adataset%2C%20which%20includes%20various%20obstacles%20even%20in%20distant%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12322v1&entry.124074799=Read"},
{"title": "Verifiable Homomorphic Linear Combinations in Multi-Instance Time-Lock\n  Puzzles", "author": "Aydin Abadi", "abstract": "  Time-Lock Puzzles (TLPs) have been developed to securely transmit sensitive\ninformation into the future without relying on a trusted third party.\nMulti-instance TLP is a scalable variant of TLP that enables a server to\nefficiently find solutions to different puzzles provided by a client at once.\nNevertheless, existing multi-instance TLPs lack support for (verifiable)\nhomomorphic computation. To address this limitation, we introduce the\n\"Multi-Instance partially Homomorphic TLP\" (MH-TLP), a multi-instance TLP\nsupporting efficient verifiable homomorphic linear combinations of puzzles\nbelonging to a client. It ensures anyone can verify the correctness of\ncomputations and solutions. Building on MH-TLP, we further propose the\n\"Multi-instance Multi-client verifiable partially Homomorphic TLP\" (MMH-TLP).\nIt not only supports all the features of MH-TLP but also allows for verifiable\nhomomorphic linear combinations of puzzles from different clients. Our schemes\nrefrain from using asymmetric-key cryptography for verification and, unlike\nmost homomorphic TLPs, do not require a trusted third party. A comprehensive\ncost analysis demonstrates that our schemes scale linearly with the number of\nclients and puzzles.\n", "link": "http://arxiv.org/abs/2408.12444v1", "date": "2024-08-22", "relevancy": 1.5454, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4047}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3827}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verifiable%20Homomorphic%20Linear%20Combinations%20in%20Multi-Instance%20Time-Lock%0A%20%20Puzzles&body=Title%3A%20Verifiable%20Homomorphic%20Linear%20Combinations%20in%20Multi-Instance%20Time-Lock%0A%20%20Puzzles%0AAuthor%3A%20Aydin%20Abadi%0AAbstract%3A%20%20%20Time-Lock%20Puzzles%20%28TLPs%29%20have%20been%20developed%20to%20securely%20transmit%20sensitive%0Ainformation%20into%20the%20future%20without%20relying%20on%20a%20trusted%20third%20party.%0AMulti-instance%20TLP%20is%20a%20scalable%20variant%20of%20TLP%20that%20enables%20a%20server%20to%0Aefficiently%20find%20solutions%20to%20different%20puzzles%20provided%20by%20a%20client%20at%20once.%0ANevertheless%2C%20existing%20multi-instance%20TLPs%20lack%20support%20for%20%28verifiable%29%0Ahomomorphic%20computation.%20To%20address%20this%20limitation%2C%20we%20introduce%20the%0A%22Multi-Instance%20partially%20Homomorphic%20TLP%22%20%28MH-TLP%29%2C%20a%20multi-instance%20TLP%0Asupporting%20efficient%20verifiable%20homomorphic%20linear%20combinations%20of%20puzzles%0Abelonging%20to%20a%20client.%20It%20ensures%20anyone%20can%20verify%20the%20correctness%20of%0Acomputations%20and%20solutions.%20Building%20on%20MH-TLP%2C%20we%20further%20propose%20the%0A%22Multi-instance%20Multi-client%20verifiable%20partially%20Homomorphic%20TLP%22%20%28MMH-TLP%29.%0AIt%20not%20only%20supports%20all%20the%20features%20of%20MH-TLP%20but%20also%20allows%20for%20verifiable%0Ahomomorphic%20linear%20combinations%20of%20puzzles%20from%20different%20clients.%20Our%20schemes%0Arefrain%20from%20using%20asymmetric-key%20cryptography%20for%20verification%20and%2C%20unlike%0Amost%20homomorphic%20TLPs%2C%20do%20not%20require%20a%20trusted%20third%20party.%20A%20comprehensive%0Acost%20analysis%20demonstrates%20that%20our%20schemes%20scale%20linearly%20with%20the%20number%20of%0Aclients%20and%20puzzles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerifiable%2520Homomorphic%2520Linear%2520Combinations%2520in%2520Multi-Instance%2520Time-Lock%250A%2520%2520Puzzles%26entry.906535625%3DAydin%2520Abadi%26entry.1292438233%3D%2520%2520Time-Lock%2520Puzzles%2520%2528TLPs%2529%2520have%2520been%2520developed%2520to%2520securely%2520transmit%2520sensitive%250Ainformation%2520into%2520the%2520future%2520without%2520relying%2520on%2520a%2520trusted%2520third%2520party.%250AMulti-instance%2520TLP%2520is%2520a%2520scalable%2520variant%2520of%2520TLP%2520that%2520enables%2520a%2520server%2520to%250Aefficiently%2520find%2520solutions%2520to%2520different%2520puzzles%2520provided%2520by%2520a%2520client%2520at%2520once.%250ANevertheless%252C%2520existing%2520multi-instance%2520TLPs%2520lack%2520support%2520for%2520%2528verifiable%2529%250Ahomomorphic%2520computation.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520the%250A%2522Multi-Instance%2520partially%2520Homomorphic%2520TLP%2522%2520%2528MH-TLP%2529%252C%2520a%2520multi-instance%2520TLP%250Asupporting%2520efficient%2520verifiable%2520homomorphic%2520linear%2520combinations%2520of%2520puzzles%250Abelonging%2520to%2520a%2520client.%2520It%2520ensures%2520anyone%2520can%2520verify%2520the%2520correctness%2520of%250Acomputations%2520and%2520solutions.%2520Building%2520on%2520MH-TLP%252C%2520we%2520further%2520propose%2520the%250A%2522Multi-instance%2520Multi-client%2520verifiable%2520partially%2520Homomorphic%2520TLP%2522%2520%2528MMH-TLP%2529.%250AIt%2520not%2520only%2520supports%2520all%2520the%2520features%2520of%2520MH-TLP%2520but%2520also%2520allows%2520for%2520verifiable%250Ahomomorphic%2520linear%2520combinations%2520of%2520puzzles%2520from%2520different%2520clients.%2520Our%2520schemes%250Arefrain%2520from%2520using%2520asymmetric-key%2520cryptography%2520for%2520verification%2520and%252C%2520unlike%250Amost%2520homomorphic%2520TLPs%252C%2520do%2520not%2520require%2520a%2520trusted%2520third%2520party.%2520A%2520comprehensive%250Acost%2520analysis%2520demonstrates%2520that%2520our%2520schemes%2520scale%2520linearly%2520with%2520the%2520number%2520of%250Aclients%2520and%2520puzzles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verifiable%20Homomorphic%20Linear%20Combinations%20in%20Multi-Instance%20Time-Lock%0A%20%20Puzzles&entry.906535625=Aydin%20Abadi&entry.1292438233=%20%20Time-Lock%20Puzzles%20%28TLPs%29%20have%20been%20developed%20to%20securely%20transmit%20sensitive%0Ainformation%20into%20the%20future%20without%20relying%20on%20a%20trusted%20third%20party.%0AMulti-instance%20TLP%20is%20a%20scalable%20variant%20of%20TLP%20that%20enables%20a%20server%20to%0Aefficiently%20find%20solutions%20to%20different%20puzzles%20provided%20by%20a%20client%20at%20once.%0ANevertheless%2C%20existing%20multi-instance%20TLPs%20lack%20support%20for%20%28verifiable%29%0Ahomomorphic%20computation.%20To%20address%20this%20limitation%2C%20we%20introduce%20the%0A%22Multi-Instance%20partially%20Homomorphic%20TLP%22%20%28MH-TLP%29%2C%20a%20multi-instance%20TLP%0Asupporting%20efficient%20verifiable%20homomorphic%20linear%20combinations%20of%20puzzles%0Abelonging%20to%20a%20client.%20It%20ensures%20anyone%20can%20verify%20the%20correctness%20of%0Acomputations%20and%20solutions.%20Building%20on%20MH-TLP%2C%20we%20further%20propose%20the%0A%22Multi-instance%20Multi-client%20verifiable%20partially%20Homomorphic%20TLP%22%20%28MMH-TLP%29.%0AIt%20not%20only%20supports%20all%20the%20features%20of%20MH-TLP%20but%20also%20allows%20for%20verifiable%0Ahomomorphic%20linear%20combinations%20of%20puzzles%20from%20different%20clients.%20Our%20schemes%0Arefrain%20from%20using%20asymmetric-key%20cryptography%20for%20verification%20and%2C%20unlike%0Amost%20homomorphic%20TLPs%2C%20do%20not%20require%20a%20trusted%20third%20party.%20A%20comprehensive%0Acost%20analysis%20demonstrates%20that%20our%20schemes%20scale%20linearly%20with%20the%20number%20of%0Aclients%20and%20puzzles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12444v1&entry.124074799=Read"},
{"title": "Multi-Style Facial Sketch Synthesis through Masked Generative Modeling", "author": "Bowen Sun and Guo Lu and Shibao Zheng", "abstract": "  The facial sketch synthesis (FSS) model, capable of generating sketch\nportraits from given facial photographs, holds profound implications across\nmultiple domains, encompassing cross-modal face recognition, entertainment,\nart, media, among others. However, the production of high-quality sketches\nremains a formidable task, primarily due to the challenges and flaws associated\nwith three key factors: (1) the scarcity of artist-drawn data, (2) the\nconstraints imposed by limited style types, and (3) the deficiencies of\nprocessing input information in existing models. To address these difficulties,\nwe propose a lightweight end-to-end synthesis model that efficiently converts\nimages to corresponding multi-stylized sketches, obviating the necessity for\nany supplementary inputs (\\eg, 3D geometry). In this study, we overcome the\nissue of data insufficiency by incorporating semi-supervised learning into the\ntraining process. Additionally, we employ a feature extraction module and style\nembeddings to proficiently steer the generative transformer during the\niterative prediction of masked image tokens, thus achieving a continuous\nstylized output that retains facial features accurately in sketches. The\nextensive experiments demonstrate that our method consistently outperforms\nprevious algorithms across multiple benchmarks, exhibiting a discernible\ndisparity.\n", "link": "http://arxiv.org/abs/2408.12400v1", "date": "2024-08-22", "relevancy": 1.114, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.57}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5521}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Style%20Facial%20Sketch%20Synthesis%20through%20Masked%20Generative%20Modeling&body=Title%3A%20Multi-Style%20Facial%20Sketch%20Synthesis%20through%20Masked%20Generative%20Modeling%0AAuthor%3A%20Bowen%20Sun%20and%20Guo%20Lu%20and%20Shibao%20Zheng%0AAbstract%3A%20%20%20The%20facial%20sketch%20synthesis%20%28FSS%29%20model%2C%20capable%20of%20generating%20sketch%0Aportraits%20from%20given%20facial%20photographs%2C%20holds%20profound%20implications%20across%0Amultiple%20domains%2C%20encompassing%20cross-modal%20face%20recognition%2C%20entertainment%2C%0Aart%2C%20media%2C%20among%20others.%20However%2C%20the%20production%20of%20high-quality%20sketches%0Aremains%20a%20formidable%20task%2C%20primarily%20due%20to%20the%20challenges%20and%20flaws%20associated%0Awith%20three%20key%20factors%3A%20%281%29%20the%20scarcity%20of%20artist-drawn%20data%2C%20%282%29%20the%0Aconstraints%20imposed%20by%20limited%20style%20types%2C%20and%20%283%29%20the%20deficiencies%20of%0Aprocessing%20input%20information%20in%20existing%20models.%20To%20address%20these%20difficulties%2C%0Awe%20propose%20a%20lightweight%20end-to-end%20synthesis%20model%20that%20efficiently%20converts%0Aimages%20to%20corresponding%20multi-stylized%20sketches%2C%20obviating%20the%20necessity%20for%0Aany%20supplementary%20inputs%20%28%5Ceg%2C%203D%20geometry%29.%20In%20this%20study%2C%20we%20overcome%20the%0Aissue%20of%20data%20insufficiency%20by%20incorporating%20semi-supervised%20learning%20into%20the%0Atraining%20process.%20Additionally%2C%20we%20employ%20a%20feature%20extraction%20module%20and%20style%0Aembeddings%20to%20proficiently%20steer%20the%20generative%20transformer%20during%20the%0Aiterative%20prediction%20of%20masked%20image%20tokens%2C%20thus%20achieving%20a%20continuous%0Astylized%20output%20that%20retains%20facial%20features%20accurately%20in%20sketches.%20The%0Aextensive%20experiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%0Aprevious%20algorithms%20across%20multiple%20benchmarks%2C%20exhibiting%20a%20discernible%0Adisparity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Style%2520Facial%2520Sketch%2520Synthesis%2520through%2520Masked%2520Generative%2520Modeling%26entry.906535625%3DBowen%2520Sun%2520and%2520Guo%2520Lu%2520and%2520Shibao%2520Zheng%26entry.1292438233%3D%2520%2520The%2520facial%2520sketch%2520synthesis%2520%2528FSS%2529%2520model%252C%2520capable%2520of%2520generating%2520sketch%250Aportraits%2520from%2520given%2520facial%2520photographs%252C%2520holds%2520profound%2520implications%2520across%250Amultiple%2520domains%252C%2520encompassing%2520cross-modal%2520face%2520recognition%252C%2520entertainment%252C%250Aart%252C%2520media%252C%2520among%2520others.%2520However%252C%2520the%2520production%2520of%2520high-quality%2520sketches%250Aremains%2520a%2520formidable%2520task%252C%2520primarily%2520due%2520to%2520the%2520challenges%2520and%2520flaws%2520associated%250Awith%2520three%2520key%2520factors%253A%2520%25281%2529%2520the%2520scarcity%2520of%2520artist-drawn%2520data%252C%2520%25282%2529%2520the%250Aconstraints%2520imposed%2520by%2520limited%2520style%2520types%252C%2520and%2520%25283%2529%2520the%2520deficiencies%2520of%250Aprocessing%2520input%2520information%2520in%2520existing%2520models.%2520To%2520address%2520these%2520difficulties%252C%250Awe%2520propose%2520a%2520lightweight%2520end-to-end%2520synthesis%2520model%2520that%2520efficiently%2520converts%250Aimages%2520to%2520corresponding%2520multi-stylized%2520sketches%252C%2520obviating%2520the%2520necessity%2520for%250Aany%2520supplementary%2520inputs%2520%2528%255Ceg%252C%25203D%2520geometry%2529.%2520In%2520this%2520study%252C%2520we%2520overcome%2520the%250Aissue%2520of%2520data%2520insufficiency%2520by%2520incorporating%2520semi-supervised%2520learning%2520into%2520the%250Atraining%2520process.%2520Additionally%252C%2520we%2520employ%2520a%2520feature%2520extraction%2520module%2520and%2520style%250Aembeddings%2520to%2520proficiently%2520steer%2520the%2520generative%2520transformer%2520during%2520the%250Aiterative%2520prediction%2520of%2520masked%2520image%2520tokens%252C%2520thus%2520achieving%2520a%2520continuous%250Astylized%2520output%2520that%2520retains%2520facial%2520features%2520accurately%2520in%2520sketches.%2520The%250Aextensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%250Aprevious%2520algorithms%2520across%2520multiple%2520benchmarks%252C%2520exhibiting%2520a%2520discernible%250Adisparity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Style%20Facial%20Sketch%20Synthesis%20through%20Masked%20Generative%20Modeling&entry.906535625=Bowen%20Sun%20and%20Guo%20Lu%20and%20Shibao%20Zheng&entry.1292438233=%20%20The%20facial%20sketch%20synthesis%20%28FSS%29%20model%2C%20capable%20of%20generating%20sketch%0Aportraits%20from%20given%20facial%20photographs%2C%20holds%20profound%20implications%20across%0Amultiple%20domains%2C%20encompassing%20cross-modal%20face%20recognition%2C%20entertainment%2C%0Aart%2C%20media%2C%20among%20others.%20However%2C%20the%20production%20of%20high-quality%20sketches%0Aremains%20a%20formidable%20task%2C%20primarily%20due%20to%20the%20challenges%20and%20flaws%20associated%0Awith%20three%20key%20factors%3A%20%281%29%20the%20scarcity%20of%20artist-drawn%20data%2C%20%282%29%20the%0Aconstraints%20imposed%20by%20limited%20style%20types%2C%20and%20%283%29%20the%20deficiencies%20of%0Aprocessing%20input%20information%20in%20existing%20models.%20To%20address%20these%20difficulties%2C%0Awe%20propose%20a%20lightweight%20end-to-end%20synthesis%20model%20that%20efficiently%20converts%0Aimages%20to%20corresponding%20multi-stylized%20sketches%2C%20obviating%20the%20necessity%20for%0Aany%20supplementary%20inputs%20%28%5Ceg%2C%203D%20geometry%29.%20In%20this%20study%2C%20we%20overcome%20the%0Aissue%20of%20data%20insufficiency%20by%20incorporating%20semi-supervised%20learning%20into%20the%0Atraining%20process.%20Additionally%2C%20we%20employ%20a%20feature%20extraction%20module%20and%20style%0Aembeddings%20to%20proficiently%20steer%20the%20generative%20transformer%20during%20the%0Aiterative%20prediction%20of%20masked%20image%20tokens%2C%20thus%20achieving%20a%20continuous%0Astylized%20output%20that%20retains%20facial%20features%20accurately%20in%20sketches.%20The%0Aextensive%20experiments%20demonstrate%20that%20our%20method%20consistently%20outperforms%0Aprevious%20algorithms%20across%20multiple%20benchmarks%2C%20exhibiting%20a%20discernible%0Adisparity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12400v1&entry.124074799=Read"},
{"title": "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework", "author": "Hao Wei and Jianing Qiu and Haibao Yu and Wu Yuan", "abstract": "  Large language models (LLMs) have had a significant impact on diverse\nresearch domains, including medicine and healthcare. However, the potential of\nLLMs as copilots in medical education remains underexplored. Current\nAI-assisted educational tools are limited by their solitary learning approach\nand inability to simulate the multi-disciplinary and interactive nature of\nactual medical training. To address these limitations, we propose MEDCO\n(Medical EDucation COpilots), a novel multi-agent-based copilot system\nspecially developed to emulate real-world medical training environments. MEDCO\nincorporates three primary agents: an agentic patient, an expert doctor, and a\nradiologist, facilitating a multi-modal and interactive learning environment.\nOur framework emphasizes the learning of proficient question-asking skills,\nmulti-disciplinary collaboration, and peer discussions between students. Our\nexperiments show that simulated virtual students who underwent training with\nMEDCO not only achieved substantial performance enhancements comparable to\nthose of advanced models, but also demonstrated human-like learning behaviors\nand improvements, coupled with an increase in the number of learning samples.\nThis work contributes to medical education by introducing a copilot that\nimplements an interactive and collaborative learning approach. It also provides\nvaluable insights into the effectiveness of AI-integrated training paradigms.\n", "link": "http://arxiv.org/abs/2408.12496v1", "date": "2024-08-22", "relevancy": 1.6082, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.56}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5298}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEDCO%3A%20Medical%20Education%20Copilots%20Based%20on%20A%20Multi-Agent%20Framework&body=Title%3A%20MEDCO%3A%20Medical%20Education%20Copilots%20Based%20on%20A%20Multi-Agent%20Framework%0AAuthor%3A%20Hao%20Wei%20and%20Jianing%20Qiu%20and%20Haibao%20Yu%20and%20Wu%20Yuan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20had%20a%20significant%20impact%20on%20diverse%0Aresearch%20domains%2C%20including%20medicine%20and%20healthcare.%20However%2C%20the%20potential%20of%0ALLMs%20as%20copilots%20in%20medical%20education%20remains%20underexplored.%20Current%0AAI-assisted%20educational%20tools%20are%20limited%20by%20their%20solitary%20learning%20approach%0Aand%20inability%20to%20simulate%20the%20multi-disciplinary%20and%20interactive%20nature%20of%0Aactual%20medical%20training.%20To%20address%20these%20limitations%2C%20we%20propose%20MEDCO%0A%28Medical%20EDucation%20COpilots%29%2C%20a%20novel%20multi-agent-based%20copilot%20system%0Aspecially%20developed%20to%20emulate%20real-world%20medical%20training%20environments.%20MEDCO%0Aincorporates%20three%20primary%20agents%3A%20an%20agentic%20patient%2C%20an%20expert%20doctor%2C%20and%20a%0Aradiologist%2C%20facilitating%20a%20multi-modal%20and%20interactive%20learning%20environment.%0AOur%20framework%20emphasizes%20the%20learning%20of%20proficient%20question-asking%20skills%2C%0Amulti-disciplinary%20collaboration%2C%20and%20peer%20discussions%20between%20students.%20Our%0Aexperiments%20show%20that%20simulated%20virtual%20students%20who%20underwent%20training%20with%0AMEDCO%20not%20only%20achieved%20substantial%20performance%20enhancements%20comparable%20to%0Athose%20of%20advanced%20models%2C%20but%20also%20demonstrated%20human-like%20learning%20behaviors%0Aand%20improvements%2C%20coupled%20with%20an%20increase%20in%20the%20number%20of%20learning%20samples.%0AThis%20work%20contributes%20to%20medical%20education%20by%20introducing%20a%20copilot%20that%0Aimplements%20an%20interactive%20and%20collaborative%20learning%20approach.%20It%20also%20provides%0Avaluable%20insights%20into%20the%20effectiveness%20of%20AI-integrated%20training%20paradigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEDCO%253A%2520Medical%2520Education%2520Copilots%2520Based%2520on%2520A%2520Multi-Agent%2520Framework%26entry.906535625%3DHao%2520Wei%2520and%2520Jianing%2520Qiu%2520and%2520Haibao%2520Yu%2520and%2520Wu%2520Yuan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520had%2520a%2520significant%2520impact%2520on%2520diverse%250Aresearch%2520domains%252C%2520including%2520medicine%2520and%2520healthcare.%2520However%252C%2520the%2520potential%2520of%250ALLMs%2520as%2520copilots%2520in%2520medical%2520education%2520remains%2520underexplored.%2520Current%250AAI-assisted%2520educational%2520tools%2520are%2520limited%2520by%2520their%2520solitary%2520learning%2520approach%250Aand%2520inability%2520to%2520simulate%2520the%2520multi-disciplinary%2520and%2520interactive%2520nature%2520of%250Aactual%2520medical%2520training.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520MEDCO%250A%2528Medical%2520EDucation%2520COpilots%2529%252C%2520a%2520novel%2520multi-agent-based%2520copilot%2520system%250Aspecially%2520developed%2520to%2520emulate%2520real-world%2520medical%2520training%2520environments.%2520MEDCO%250Aincorporates%2520three%2520primary%2520agents%253A%2520an%2520agentic%2520patient%252C%2520an%2520expert%2520doctor%252C%2520and%2520a%250Aradiologist%252C%2520facilitating%2520a%2520multi-modal%2520and%2520interactive%2520learning%2520environment.%250AOur%2520framework%2520emphasizes%2520the%2520learning%2520of%2520proficient%2520question-asking%2520skills%252C%250Amulti-disciplinary%2520collaboration%252C%2520and%2520peer%2520discussions%2520between%2520students.%2520Our%250Aexperiments%2520show%2520that%2520simulated%2520virtual%2520students%2520who%2520underwent%2520training%2520with%250AMEDCO%2520not%2520only%2520achieved%2520substantial%2520performance%2520enhancements%2520comparable%2520to%250Athose%2520of%2520advanced%2520models%252C%2520but%2520also%2520demonstrated%2520human-like%2520learning%2520behaviors%250Aand%2520improvements%252C%2520coupled%2520with%2520an%2520increase%2520in%2520the%2520number%2520of%2520learning%2520samples.%250AThis%2520work%2520contributes%2520to%2520medical%2520education%2520by%2520introducing%2520a%2520copilot%2520that%250Aimplements%2520an%2520interactive%2520and%2520collaborative%2520learning%2520approach.%2520It%2520also%2520provides%250Avaluable%2520insights%2520into%2520the%2520effectiveness%2520of%2520AI-integrated%2520training%2520paradigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEDCO%3A%20Medical%20Education%20Copilots%20Based%20on%20A%20Multi-Agent%20Framework&entry.906535625=Hao%20Wei%20and%20Jianing%20Qiu%20and%20Haibao%20Yu%20and%20Wu%20Yuan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20had%20a%20significant%20impact%20on%20diverse%0Aresearch%20domains%2C%20including%20medicine%20and%20healthcare.%20However%2C%20the%20potential%20of%0ALLMs%20as%20copilots%20in%20medical%20education%20remains%20underexplored.%20Current%0AAI-assisted%20educational%20tools%20are%20limited%20by%20their%20solitary%20learning%20approach%0Aand%20inability%20to%20simulate%20the%20multi-disciplinary%20and%20interactive%20nature%20of%0Aactual%20medical%20training.%20To%20address%20these%20limitations%2C%20we%20propose%20MEDCO%0A%28Medical%20EDucation%20COpilots%29%2C%20a%20novel%20multi-agent-based%20copilot%20system%0Aspecially%20developed%20to%20emulate%20real-world%20medical%20training%20environments.%20MEDCO%0Aincorporates%20three%20primary%20agents%3A%20an%20agentic%20patient%2C%20an%20expert%20doctor%2C%20and%20a%0Aradiologist%2C%20facilitating%20a%20multi-modal%20and%20interactive%20learning%20environment.%0AOur%20framework%20emphasizes%20the%20learning%20of%20proficient%20question-asking%20skills%2C%0Amulti-disciplinary%20collaboration%2C%20and%20peer%20discussions%20between%20students.%20Our%0Aexperiments%20show%20that%20simulated%20virtual%20students%20who%20underwent%20training%20with%0AMEDCO%20not%20only%20achieved%20substantial%20performance%20enhancements%20comparable%20to%0Athose%20of%20advanced%20models%2C%20but%20also%20demonstrated%20human-like%20learning%20behaviors%0Aand%20improvements%2C%20coupled%20with%20an%20increase%20in%20the%20number%20of%20learning%20samples.%0AThis%20work%20contributes%20to%20medical%20education%20by%20introducing%20a%20copilot%20that%0Aimplements%20an%20interactive%20and%20collaborative%20learning%20approach.%20It%20also%20provides%0Avaluable%20insights%20into%20the%20effectiveness%20of%20AI-integrated%20training%20paradigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12496v1&entry.124074799=Read"},
{"title": "Human-In-The-Loop Machine Learning for Safe and Ethical Autonomous\n  Vehicles: Principles, Challenges, and Opportunities", "author": "Yousef Emami and Kai Li and Luis Almeida and Wei Ni and Zhu Han", "abstract": "  Rapid advances in Machine Learning (ML) have triggered new trends in\nAutonomous Vehicles (AVs). ML algorithms play a crucial role in interpreting\nsensor data, predicting potential hazards, and optimizing navigation\nstrategies. However, achieving full autonomy in cluttered and complex\nsituations, such as intricate intersections, diverse sceneries, varied\ntrajectories, and complex missions, is still challenging, and the cost of data\nlabeling remains a significant bottleneck. The adaptability and robustness of\nhumans in complex scenarios motivate the inclusion of humans in ML process,\nleveraging their creativity, ethical power, and emotional intelligence to\nimprove ML effectiveness. The scientific community knows this approach as\nHuman-In-The-Loop Machine Learning (HITL-ML). Towards safe and ethical\nautonomy, we present a review of HITL-ML for AVs, focusing on Curriculum\nLearning (CL), Human-In-The-Loop Reinforcement Learning (HITL-RL), Active\nLearning (AL), and ethical principles. In CL, human experts systematically\ntrain ML models by starting with simple tasks and gradually progressing to more\ndifficult ones. HITL-RL significantly enhances the RL process by incorporating\nhuman input through techniques like reward shaping, action injection, and\ninteractive learning. AL streamlines the annotation process by targeting\nspecific instances that need to be labeled with human oversight, reducing the\noverall time and cost associated with training. Ethical principles must be\nembedded in AVs to align their behavior with societal values and norms. In\naddition, we provide insights and specify future research directions.\n", "link": "http://arxiv.org/abs/2408.12548v1", "date": "2024-08-22", "relevancy": 1.6013, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5384}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5285}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-In-The-Loop%20Machine%20Learning%20for%20Safe%20and%20Ethical%20Autonomous%0A%20%20Vehicles%3A%20Principles%2C%20Challenges%2C%20and%20Opportunities&body=Title%3A%20Human-In-The-Loop%20Machine%20Learning%20for%20Safe%20and%20Ethical%20Autonomous%0A%20%20Vehicles%3A%20Principles%2C%20Challenges%2C%20and%20Opportunities%0AAuthor%3A%20Yousef%20Emami%20and%20Kai%20Li%20and%20Luis%20Almeida%20and%20Wei%20Ni%20and%20Zhu%20Han%0AAbstract%3A%20%20%20Rapid%20advances%20in%20Machine%20Learning%20%28ML%29%20have%20triggered%20new%20trends%20in%0AAutonomous%20Vehicles%20%28AVs%29.%20ML%20algorithms%20play%20a%20crucial%20role%20in%20interpreting%0Asensor%20data%2C%20predicting%20potential%20hazards%2C%20and%20optimizing%20navigation%0Astrategies.%20However%2C%20achieving%20full%20autonomy%20in%20cluttered%20and%20complex%0Asituations%2C%20such%20as%20intricate%20intersections%2C%20diverse%20sceneries%2C%20varied%0Atrajectories%2C%20and%20complex%20missions%2C%20is%20still%20challenging%2C%20and%20the%20cost%20of%20data%0Alabeling%20remains%20a%20significant%20bottleneck.%20The%20adaptability%20and%20robustness%20of%0Ahumans%20in%20complex%20scenarios%20motivate%20the%20inclusion%20of%20humans%20in%20ML%20process%2C%0Aleveraging%20their%20creativity%2C%20ethical%20power%2C%20and%20emotional%20intelligence%20to%0Aimprove%20ML%20effectiveness.%20The%20scientific%20community%20knows%20this%20approach%20as%0AHuman-In-The-Loop%20Machine%20Learning%20%28HITL-ML%29.%20Towards%20safe%20and%20ethical%0Aautonomy%2C%20we%20present%20a%20review%20of%20HITL-ML%20for%20AVs%2C%20focusing%20on%20Curriculum%0ALearning%20%28CL%29%2C%20Human-In-The-Loop%20Reinforcement%20Learning%20%28HITL-RL%29%2C%20Active%0ALearning%20%28AL%29%2C%20and%20ethical%20principles.%20In%20CL%2C%20human%20experts%20systematically%0Atrain%20ML%20models%20by%20starting%20with%20simple%20tasks%20and%20gradually%20progressing%20to%20more%0Adifficult%20ones.%20HITL-RL%20significantly%20enhances%20the%20RL%20process%20by%20incorporating%0Ahuman%20input%20through%20techniques%20like%20reward%20shaping%2C%20action%20injection%2C%20and%0Ainteractive%20learning.%20AL%20streamlines%20the%20annotation%20process%20by%20targeting%0Aspecific%20instances%20that%20need%20to%20be%20labeled%20with%20human%20oversight%2C%20reducing%20the%0Aoverall%20time%20and%20cost%20associated%20with%20training.%20Ethical%20principles%20must%20be%0Aembedded%20in%20AVs%20to%20align%20their%20behavior%20with%20societal%20values%20and%20norms.%20In%0Aaddition%2C%20we%20provide%20insights%20and%20specify%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-In-The-Loop%2520Machine%2520Learning%2520for%2520Safe%2520and%2520Ethical%2520Autonomous%250A%2520%2520Vehicles%253A%2520Principles%252C%2520Challenges%252C%2520and%2520Opportunities%26entry.906535625%3DYousef%2520Emami%2520and%2520Kai%2520Li%2520and%2520Luis%2520Almeida%2520and%2520Wei%2520Ni%2520and%2520Zhu%2520Han%26entry.1292438233%3D%2520%2520Rapid%2520advances%2520in%2520Machine%2520Learning%2520%2528ML%2529%2520have%2520triggered%2520new%2520trends%2520in%250AAutonomous%2520Vehicles%2520%2528AVs%2529.%2520ML%2520algorithms%2520play%2520a%2520crucial%2520role%2520in%2520interpreting%250Asensor%2520data%252C%2520predicting%2520potential%2520hazards%252C%2520and%2520optimizing%2520navigation%250Astrategies.%2520However%252C%2520achieving%2520full%2520autonomy%2520in%2520cluttered%2520and%2520complex%250Asituations%252C%2520such%2520as%2520intricate%2520intersections%252C%2520diverse%2520sceneries%252C%2520varied%250Atrajectories%252C%2520and%2520complex%2520missions%252C%2520is%2520still%2520challenging%252C%2520and%2520the%2520cost%2520of%2520data%250Alabeling%2520remains%2520a%2520significant%2520bottleneck.%2520The%2520adaptability%2520and%2520robustness%2520of%250Ahumans%2520in%2520complex%2520scenarios%2520motivate%2520the%2520inclusion%2520of%2520humans%2520in%2520ML%2520process%252C%250Aleveraging%2520their%2520creativity%252C%2520ethical%2520power%252C%2520and%2520emotional%2520intelligence%2520to%250Aimprove%2520ML%2520effectiveness.%2520The%2520scientific%2520community%2520knows%2520this%2520approach%2520as%250AHuman-In-The-Loop%2520Machine%2520Learning%2520%2528HITL-ML%2529.%2520Towards%2520safe%2520and%2520ethical%250Aautonomy%252C%2520we%2520present%2520a%2520review%2520of%2520HITL-ML%2520for%2520AVs%252C%2520focusing%2520on%2520Curriculum%250ALearning%2520%2528CL%2529%252C%2520Human-In-The-Loop%2520Reinforcement%2520Learning%2520%2528HITL-RL%2529%252C%2520Active%250ALearning%2520%2528AL%2529%252C%2520and%2520ethical%2520principles.%2520In%2520CL%252C%2520human%2520experts%2520systematically%250Atrain%2520ML%2520models%2520by%2520starting%2520with%2520simple%2520tasks%2520and%2520gradually%2520progressing%2520to%2520more%250Adifficult%2520ones.%2520HITL-RL%2520significantly%2520enhances%2520the%2520RL%2520process%2520by%2520incorporating%250Ahuman%2520input%2520through%2520techniques%2520like%2520reward%2520shaping%252C%2520action%2520injection%252C%2520and%250Ainteractive%2520learning.%2520AL%2520streamlines%2520the%2520annotation%2520process%2520by%2520targeting%250Aspecific%2520instances%2520that%2520need%2520to%2520be%2520labeled%2520with%2520human%2520oversight%252C%2520reducing%2520the%250Aoverall%2520time%2520and%2520cost%2520associated%2520with%2520training.%2520Ethical%2520principles%2520must%2520be%250Aembedded%2520in%2520AVs%2520to%2520align%2520their%2520behavior%2520with%2520societal%2520values%2520and%2520norms.%2520In%250Aaddition%252C%2520we%2520provide%2520insights%2520and%2520specify%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-In-The-Loop%20Machine%20Learning%20for%20Safe%20and%20Ethical%20Autonomous%0A%20%20Vehicles%3A%20Principles%2C%20Challenges%2C%20and%20Opportunities&entry.906535625=Yousef%20Emami%20and%20Kai%20Li%20and%20Luis%20Almeida%20and%20Wei%20Ni%20and%20Zhu%20Han&entry.1292438233=%20%20Rapid%20advances%20in%20Machine%20Learning%20%28ML%29%20have%20triggered%20new%20trends%20in%0AAutonomous%20Vehicles%20%28AVs%29.%20ML%20algorithms%20play%20a%20crucial%20role%20in%20interpreting%0Asensor%20data%2C%20predicting%20potential%20hazards%2C%20and%20optimizing%20navigation%0Astrategies.%20However%2C%20achieving%20full%20autonomy%20in%20cluttered%20and%20complex%0Asituations%2C%20such%20as%20intricate%20intersections%2C%20diverse%20sceneries%2C%20varied%0Atrajectories%2C%20and%20complex%20missions%2C%20is%20still%20challenging%2C%20and%20the%20cost%20of%20data%0Alabeling%20remains%20a%20significant%20bottleneck.%20The%20adaptability%20and%20robustness%20of%0Ahumans%20in%20complex%20scenarios%20motivate%20the%20inclusion%20of%20humans%20in%20ML%20process%2C%0Aleveraging%20their%20creativity%2C%20ethical%20power%2C%20and%20emotional%20intelligence%20to%0Aimprove%20ML%20effectiveness.%20The%20scientific%20community%20knows%20this%20approach%20as%0AHuman-In-The-Loop%20Machine%20Learning%20%28HITL-ML%29.%20Towards%20safe%20and%20ethical%0Aautonomy%2C%20we%20present%20a%20review%20of%20HITL-ML%20for%20AVs%2C%20focusing%20on%20Curriculum%0ALearning%20%28CL%29%2C%20Human-In-The-Loop%20Reinforcement%20Learning%20%28HITL-RL%29%2C%20Active%0ALearning%20%28AL%29%2C%20and%20ethical%20principles.%20In%20CL%2C%20human%20experts%20systematically%0Atrain%20ML%20models%20by%20starting%20with%20simple%20tasks%20and%20gradually%20progressing%20to%20more%0Adifficult%20ones.%20HITL-RL%20significantly%20enhances%20the%20RL%20process%20by%20incorporating%0Ahuman%20input%20through%20techniques%20like%20reward%20shaping%2C%20action%20injection%2C%20and%0Ainteractive%20learning.%20AL%20streamlines%20the%20annotation%20process%20by%20targeting%0Aspecific%20instances%20that%20need%20to%20be%20labeled%20with%20human%20oversight%2C%20reducing%20the%0Aoverall%20time%20and%20cost%20associated%20with%20training.%20Ethical%20principles%20must%20be%0Aembedded%20in%20AVs%20to%20align%20their%20behavior%20with%20societal%20values%20and%20norms.%20In%0Aaddition%2C%20we%20provide%20insights%20and%20specify%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12548v1&entry.124074799=Read"},
{"title": "Integrated Hardware and Software Architecture for Industrial AGV with\n  Manual Override Capability", "author": "Pietro Iob and Mauro Schiavo and Angelo Cenedese", "abstract": "  This paper presents a study on transforming a traditional human-operated\nvehicle into a fully autonomous device. By leveraging previous research and\nstate-of-the-art technologies, the study addresses autonomy, safety, and\noperational efficiency in industrial environments. Motivated by the demand for\nautomation in hazardous and complex industries, the autonomous system\nintegrates sensors, actuators, advanced control algorithms, and communication\nsystems to enhance safety, streamline processes, and improve productivity. The\npaper covers system requirements, hardware architecture, software framework and\npreliminary results. This research offers insights into designing and\nimplementing autonomous capabilities in human-operated vehicles, with\nimplications for improving safety and efficiency in various industrial sectors.\n", "link": "http://arxiv.org/abs/2408.12499v1", "date": "2024-08-22", "relevancy": 1.575, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5624}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4811}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrated%20Hardware%20and%20Software%20Architecture%20for%20Industrial%20AGV%20with%0A%20%20Manual%20Override%20Capability&body=Title%3A%20Integrated%20Hardware%20and%20Software%20Architecture%20for%20Industrial%20AGV%20with%0A%20%20Manual%20Override%20Capability%0AAuthor%3A%20Pietro%20Iob%20and%20Mauro%20Schiavo%20and%20Angelo%20Cenedese%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20study%20on%20transforming%20a%20traditional%20human-operated%0Avehicle%20into%20a%20fully%20autonomous%20device.%20By%20leveraging%20previous%20research%20and%0Astate-of-the-art%20technologies%2C%20the%20study%20addresses%20autonomy%2C%20safety%2C%20and%0Aoperational%20efficiency%20in%20industrial%20environments.%20Motivated%20by%20the%20demand%20for%0Aautomation%20in%20hazardous%20and%20complex%20industries%2C%20the%20autonomous%20system%0Aintegrates%20sensors%2C%20actuators%2C%20advanced%20control%20algorithms%2C%20and%20communication%0Asystems%20to%20enhance%20safety%2C%20streamline%20processes%2C%20and%20improve%20productivity.%20The%0Apaper%20covers%20system%20requirements%2C%20hardware%20architecture%2C%20software%20framework%20and%0Apreliminary%20results.%20This%20research%20offers%20insights%20into%20designing%20and%0Aimplementing%20autonomous%20capabilities%20in%20human-operated%20vehicles%2C%20with%0Aimplications%20for%20improving%20safety%20and%20efficiency%20in%20various%20industrial%20sectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrated%2520Hardware%2520and%2520Software%2520Architecture%2520for%2520Industrial%2520AGV%2520with%250A%2520%2520Manual%2520Override%2520Capability%26entry.906535625%3DPietro%2520Iob%2520and%2520Mauro%2520Schiavo%2520and%2520Angelo%2520Cenedese%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520study%2520on%2520transforming%2520a%2520traditional%2520human-operated%250Avehicle%2520into%2520a%2520fully%2520autonomous%2520device.%2520By%2520leveraging%2520previous%2520research%2520and%250Astate-of-the-art%2520technologies%252C%2520the%2520study%2520addresses%2520autonomy%252C%2520safety%252C%2520and%250Aoperational%2520efficiency%2520in%2520industrial%2520environments.%2520Motivated%2520by%2520the%2520demand%2520for%250Aautomation%2520in%2520hazardous%2520and%2520complex%2520industries%252C%2520the%2520autonomous%2520system%250Aintegrates%2520sensors%252C%2520actuators%252C%2520advanced%2520control%2520algorithms%252C%2520and%2520communication%250Asystems%2520to%2520enhance%2520safety%252C%2520streamline%2520processes%252C%2520and%2520improve%2520productivity.%2520The%250Apaper%2520covers%2520system%2520requirements%252C%2520hardware%2520architecture%252C%2520software%2520framework%2520and%250Apreliminary%2520results.%2520This%2520research%2520offers%2520insights%2520into%2520designing%2520and%250Aimplementing%2520autonomous%2520capabilities%2520in%2520human-operated%2520vehicles%252C%2520with%250Aimplications%2520for%2520improving%2520safety%2520and%2520efficiency%2520in%2520various%2520industrial%2520sectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrated%20Hardware%20and%20Software%20Architecture%20for%20Industrial%20AGV%20with%0A%20%20Manual%20Override%20Capability&entry.906535625=Pietro%20Iob%20and%20Mauro%20Schiavo%20and%20Angelo%20Cenedese&entry.1292438233=%20%20This%20paper%20presents%20a%20study%20on%20transforming%20a%20traditional%20human-operated%0Avehicle%20into%20a%20fully%20autonomous%20device.%20By%20leveraging%20previous%20research%20and%0Astate-of-the-art%20technologies%2C%20the%20study%20addresses%20autonomy%2C%20safety%2C%20and%0Aoperational%20efficiency%20in%20industrial%20environments.%20Motivated%20by%20the%20demand%20for%0Aautomation%20in%20hazardous%20and%20complex%20industries%2C%20the%20autonomous%20system%0Aintegrates%20sensors%2C%20actuators%2C%20advanced%20control%20algorithms%2C%20and%20communication%0Asystems%20to%20enhance%20safety%2C%20streamline%20processes%2C%20and%20improve%20productivity.%20The%0Apaper%20covers%20system%20requirements%2C%20hardware%20architecture%2C%20software%20framework%20and%0Apreliminary%20results.%20This%20research%20offers%20insights%20into%20designing%20and%0Aimplementing%20autonomous%20capabilities%20in%20human-operated%20vehicles%2C%20with%0Aimplications%20for%20improving%20safety%20and%20efficiency%20in%20various%20industrial%20sectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12499v1&entry.124074799=Read"},
{"title": "Fine-tuning Smaller Language Models for Question Answering over\n  Financial Documents", "author": "Karmvir Singh Phogat and Sai Akhil Puranam and Sridhar Dasaratha and Chetan Harsha and Shashishekar Ramakrishna", "abstract": "  Recent research has shown that smaller language models can acquire\nsubstantial reasoning abilities when fine-tuned with reasoning exemplars\ncrafted by a significantly larger teacher model. We explore this paradigm for\nthe financial domain, focusing on the challenge of answering questions that\nrequire multi-hop numerical reasoning over financial texts. We assess the\nperformance of several smaller models that have been fine-tuned to generate\nprograms that encode the required financial reasoning and calculations. Our\nfindings demonstrate that these fine-tuned smaller models approach the\nperformance of the teacher model.\n  To provide a granular analysis of model performance, we propose an approach\nto investigate the specific student model capabilities that are enhanced by\nfine-tuning. Our empirical analysis indicates that fine-tuning refines the\nstudent models ability to express and apply the required financial concepts\nalong with adapting the entity extraction for the specific data format. In\naddition, we hypothesize and demonstrate that comparable financial reasoning\ncapability can be induced using relatively smaller datasets.\n", "link": "http://arxiv.org/abs/2408.12337v1", "date": "2024-08-22", "relevancy": 1.3615, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4591}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4567}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-tuning%20Smaller%20Language%20Models%20for%20Question%20Answering%20over%0A%20%20Financial%20Documents&body=Title%3A%20Fine-tuning%20Smaller%20Language%20Models%20for%20Question%20Answering%20over%0A%20%20Financial%20Documents%0AAuthor%3A%20Karmvir%20Singh%20Phogat%20and%20Sai%20Akhil%20Puranam%20and%20Sridhar%20Dasaratha%20and%20Chetan%20Harsha%20and%20Shashishekar%20Ramakrishna%0AAbstract%3A%20%20%20Recent%20research%20has%20shown%20that%20smaller%20language%20models%20can%20acquire%0Asubstantial%20reasoning%20abilities%20when%20fine-tuned%20with%20reasoning%20exemplars%0Acrafted%20by%20a%20significantly%20larger%20teacher%20model.%20We%20explore%20this%20paradigm%20for%0Athe%20financial%20domain%2C%20focusing%20on%20the%20challenge%20of%20answering%20questions%20that%0Arequire%20multi-hop%20numerical%20reasoning%20over%20financial%20texts.%20We%20assess%20the%0Aperformance%20of%20several%20smaller%20models%20that%20have%20been%20fine-tuned%20to%20generate%0Aprograms%20that%20encode%20the%20required%20financial%20reasoning%20and%20calculations.%20Our%0Afindings%20demonstrate%20that%20these%20fine-tuned%20smaller%20models%20approach%20the%0Aperformance%20of%20the%20teacher%20model.%0A%20%20To%20provide%20a%20granular%20analysis%20of%20model%20performance%2C%20we%20propose%20an%20approach%0Ato%20investigate%20the%20specific%20student%20model%20capabilities%20that%20are%20enhanced%20by%0Afine-tuning.%20Our%20empirical%20analysis%20indicates%20that%20fine-tuning%20refines%20the%0Astudent%20models%20ability%20to%20express%20and%20apply%20the%20required%20financial%20concepts%0Aalong%20with%20adapting%20the%20entity%20extraction%20for%20the%20specific%20data%20format.%20In%0Aaddition%2C%20we%20hypothesize%20and%20demonstrate%20that%20comparable%20financial%20reasoning%0Acapability%20can%20be%20induced%20using%20relatively%20smaller%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-tuning%2520Smaller%2520Language%2520Models%2520for%2520Question%2520Answering%2520over%250A%2520%2520Financial%2520Documents%26entry.906535625%3DKarmvir%2520Singh%2520Phogat%2520and%2520Sai%2520Akhil%2520Puranam%2520and%2520Sridhar%2520Dasaratha%2520and%2520Chetan%2520Harsha%2520and%2520Shashishekar%2520Ramakrishna%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520shown%2520that%2520smaller%2520language%2520models%2520can%2520acquire%250Asubstantial%2520reasoning%2520abilities%2520when%2520fine-tuned%2520with%2520reasoning%2520exemplars%250Acrafted%2520by%2520a%2520significantly%2520larger%2520teacher%2520model.%2520We%2520explore%2520this%2520paradigm%2520for%250Athe%2520financial%2520domain%252C%2520focusing%2520on%2520the%2520challenge%2520of%2520answering%2520questions%2520that%250Arequire%2520multi-hop%2520numerical%2520reasoning%2520over%2520financial%2520texts.%2520We%2520assess%2520the%250Aperformance%2520of%2520several%2520smaller%2520models%2520that%2520have%2520been%2520fine-tuned%2520to%2520generate%250Aprograms%2520that%2520encode%2520the%2520required%2520financial%2520reasoning%2520and%2520calculations.%2520Our%250Afindings%2520demonstrate%2520that%2520these%2520fine-tuned%2520smaller%2520models%2520approach%2520the%250Aperformance%2520of%2520the%2520teacher%2520model.%250A%2520%2520To%2520provide%2520a%2520granular%2520analysis%2520of%2520model%2520performance%252C%2520we%2520propose%2520an%2520approach%250Ato%2520investigate%2520the%2520specific%2520student%2520model%2520capabilities%2520that%2520are%2520enhanced%2520by%250Afine-tuning.%2520Our%2520empirical%2520analysis%2520indicates%2520that%2520fine-tuning%2520refines%2520the%250Astudent%2520models%2520ability%2520to%2520express%2520and%2520apply%2520the%2520required%2520financial%2520concepts%250Aalong%2520with%2520adapting%2520the%2520entity%2520extraction%2520for%2520the%2520specific%2520data%2520format.%2520In%250Aaddition%252C%2520we%2520hypothesize%2520and%2520demonstrate%2520that%2520comparable%2520financial%2520reasoning%250Acapability%2520can%2520be%2520induced%2520using%2520relatively%2520smaller%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20Smaller%20Language%20Models%20for%20Question%20Answering%20over%0A%20%20Financial%20Documents&entry.906535625=Karmvir%20Singh%20Phogat%20and%20Sai%20Akhil%20Puranam%20and%20Sridhar%20Dasaratha%20and%20Chetan%20Harsha%20and%20Shashishekar%20Ramakrishna&entry.1292438233=%20%20Recent%20research%20has%20shown%20that%20smaller%20language%20models%20can%20acquire%0Asubstantial%20reasoning%20abilities%20when%20fine-tuned%20with%20reasoning%20exemplars%0Acrafted%20by%20a%20significantly%20larger%20teacher%20model.%20We%20explore%20this%20paradigm%20for%0Athe%20financial%20domain%2C%20focusing%20on%20the%20challenge%20of%20answering%20questions%20that%0Arequire%20multi-hop%20numerical%20reasoning%20over%20financial%20texts.%20We%20assess%20the%0Aperformance%20of%20several%20smaller%20models%20that%20have%20been%20fine-tuned%20to%20generate%0Aprograms%20that%20encode%20the%20required%20financial%20reasoning%20and%20calculations.%20Our%0Afindings%20demonstrate%20that%20these%20fine-tuned%20smaller%20models%20approach%20the%0Aperformance%20of%20the%20teacher%20model.%0A%20%20To%20provide%20a%20granular%20analysis%20of%20model%20performance%2C%20we%20propose%20an%20approach%0Ato%20investigate%20the%20specific%20student%20model%20capabilities%20that%20are%20enhanced%20by%0Afine-tuning.%20Our%20empirical%20analysis%20indicates%20that%20fine-tuning%20refines%20the%0Astudent%20models%20ability%20to%20express%20and%20apply%20the%20required%20financial%20concepts%0Aalong%20with%20adapting%20the%20entity%20extraction%20for%20the%20specific%20data%20format.%20In%0Aaddition%2C%20we%20hypothesize%20and%20demonstrate%20that%20comparable%20financial%20reasoning%0Acapability%20can%20be%20induced%20using%20relatively%20smaller%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12337v1&entry.124074799=Read"},
{"title": "Diff-Cleanse: Identifying and Mitigating Backdoor Attacks in Diffusion\n  Models", "author": "Jiang Hao and Xiao Jin and Hu Xiaoguang and Chen Tianyou and Zhao Jiajia", "abstract": "  Diffusion models (DMs) are regarded as one of the most advanced generative\nmodels today, yet recent studies suggest that they are vulnerable to backdoor\nattacks, which establish hidden associations between particular input patterns\nand model behaviors, compromising model integrity by causing undesirable\nactions with manipulated inputs. This vulnerability poses substantial risks,\nincluding reputational damage to model owners and the dissemination of harmful\ncontent. To mitigate the threat of backdoor attacks, there have been some\ninvestigations on backdoor detection and model repair. However, previous work\nfails to reliably purify the models backdoored by state-of-the-art attack\nmethods, rendering the field much underexplored. To bridge this gap, we\nintroduce Diff-Cleanse, a novel two-stage backdoor defense framework\nspecifically designed for DMs. The first stage employs a novel trigger\ninversion technique to reconstruct the trigger and detect the backdoor, and the\nsecond stage utilizes a structural pruning method to eliminate the backdoor. We\nevaluate our framework on hundreds of DMs that are attacked by three existing\nbackdoor attack methods with a wide range of hyperparameter settings. Extensive\nexperiments demonstrate that Diff-Cleanse achieves nearly 100\\% detection\naccuracy and effectively mitigates backdoor impacts, preserving the model's\nbenign performance with minimal compromise. Our code is avaliable at\nhttps://github.com/shymuel/diff-cleanse.\n", "link": "http://arxiv.org/abs/2407.21316v2", "date": "2024-08-22", "relevancy": 1.5415, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5587}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5119}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4966}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diff-Cleanse%3A%20Identifying%20and%20Mitigating%20Backdoor%20Attacks%20in%20Diffusion%0A%20%20Models&body=Title%3A%20Diff-Cleanse%3A%20Identifying%20and%20Mitigating%20Backdoor%20Attacks%20in%20Diffusion%0A%20%20Models%0AAuthor%3A%20Jiang%20Hao%20and%20Xiao%20Jin%20and%20Hu%20Xiaoguang%20and%20Chen%20Tianyou%20and%20Zhao%20Jiajia%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20are%20regarded%20as%20one%20of%20the%20most%20advanced%20generative%0Amodels%20today%2C%20yet%20recent%20studies%20suggest%20that%20they%20are%20vulnerable%20to%20backdoor%0Aattacks%2C%20which%20establish%20hidden%20associations%20between%20particular%20input%20patterns%0Aand%20model%20behaviors%2C%20compromising%20model%20integrity%20by%20causing%20undesirable%0Aactions%20with%20manipulated%20inputs.%20This%20vulnerability%20poses%20substantial%20risks%2C%0Aincluding%20reputational%20damage%20to%20model%20owners%20and%20the%20dissemination%20of%20harmful%0Acontent.%20To%20mitigate%20the%20threat%20of%20backdoor%20attacks%2C%20there%20have%20been%20some%0Ainvestigations%20on%20backdoor%20detection%20and%20model%20repair.%20However%2C%20previous%20work%0Afails%20to%20reliably%20purify%20the%20models%20backdoored%20by%20state-of-the-art%20attack%0Amethods%2C%20rendering%20the%20field%20much%20underexplored.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20Diff-Cleanse%2C%20a%20novel%20two-stage%20backdoor%20defense%20framework%0Aspecifically%20designed%20for%20DMs.%20The%20first%20stage%20employs%20a%20novel%20trigger%0Ainversion%20technique%20to%20reconstruct%20the%20trigger%20and%20detect%20the%20backdoor%2C%20and%20the%0Asecond%20stage%20utilizes%20a%20structural%20pruning%20method%20to%20eliminate%20the%20backdoor.%20We%0Aevaluate%20our%20framework%20on%20hundreds%20of%20DMs%20that%20are%20attacked%20by%20three%20existing%0Abackdoor%20attack%20methods%20with%20a%20wide%20range%20of%20hyperparameter%20settings.%20Extensive%0Aexperiments%20demonstrate%20that%20Diff-Cleanse%20achieves%20nearly%20100%5C%25%20detection%0Aaccuracy%20and%20effectively%20mitigates%20backdoor%20impacts%2C%20preserving%20the%20model%27s%0Abenign%20performance%20with%20minimal%20compromise.%20Our%20code%20is%20avaliable%20at%0Ahttps%3A//github.com/shymuel/diff-cleanse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21316v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiff-Cleanse%253A%2520Identifying%2520and%2520Mitigating%2520Backdoor%2520Attacks%2520in%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DJiang%2520Hao%2520and%2520Xiao%2520Jin%2520and%2520Hu%2520Xiaoguang%2520and%2520Chen%2520Tianyou%2520and%2520Zhao%2520Jiajia%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520are%2520regarded%2520as%2520one%2520of%2520the%2520most%2520advanced%2520generative%250Amodels%2520today%252C%2520yet%2520recent%2520studies%2520suggest%2520that%2520they%2520are%2520vulnerable%2520to%2520backdoor%250Aattacks%252C%2520which%2520establish%2520hidden%2520associations%2520between%2520particular%2520input%2520patterns%250Aand%2520model%2520behaviors%252C%2520compromising%2520model%2520integrity%2520by%2520causing%2520undesirable%250Aactions%2520with%2520manipulated%2520inputs.%2520This%2520vulnerability%2520poses%2520substantial%2520risks%252C%250Aincluding%2520reputational%2520damage%2520to%2520model%2520owners%2520and%2520the%2520dissemination%2520of%2520harmful%250Acontent.%2520To%2520mitigate%2520the%2520threat%2520of%2520backdoor%2520attacks%252C%2520there%2520have%2520been%2520some%250Ainvestigations%2520on%2520backdoor%2520detection%2520and%2520model%2520repair.%2520However%252C%2520previous%2520work%250Afails%2520to%2520reliably%2520purify%2520the%2520models%2520backdoored%2520by%2520state-of-the-art%2520attack%250Amethods%252C%2520rendering%2520the%2520field%2520much%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Aintroduce%2520Diff-Cleanse%252C%2520a%2520novel%2520two-stage%2520backdoor%2520defense%2520framework%250Aspecifically%2520designed%2520for%2520DMs.%2520The%2520first%2520stage%2520employs%2520a%2520novel%2520trigger%250Ainversion%2520technique%2520to%2520reconstruct%2520the%2520trigger%2520and%2520detect%2520the%2520backdoor%252C%2520and%2520the%250Asecond%2520stage%2520utilizes%2520a%2520structural%2520pruning%2520method%2520to%2520eliminate%2520the%2520backdoor.%2520We%250Aevaluate%2520our%2520framework%2520on%2520hundreds%2520of%2520DMs%2520that%2520are%2520attacked%2520by%2520three%2520existing%250Abackdoor%2520attack%2520methods%2520with%2520a%2520wide%2520range%2520of%2520hyperparameter%2520settings.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520Diff-Cleanse%2520achieves%2520nearly%2520100%255C%2525%2520detection%250Aaccuracy%2520and%2520effectively%2520mitigates%2520backdoor%2520impacts%252C%2520preserving%2520the%2520model%2527s%250Abenign%2520performance%2520with%2520minimal%2520compromise.%2520Our%2520code%2520is%2520avaliable%2520at%250Ahttps%253A//github.com/shymuel/diff-cleanse.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21316v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diff-Cleanse%3A%20Identifying%20and%20Mitigating%20Backdoor%20Attacks%20in%20Diffusion%0A%20%20Models&entry.906535625=Jiang%20Hao%20and%20Xiao%20Jin%20and%20Hu%20Xiaoguang%20and%20Chen%20Tianyou%20and%20Zhao%20Jiajia&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20are%20regarded%20as%20one%20of%20the%20most%20advanced%20generative%0Amodels%20today%2C%20yet%20recent%20studies%20suggest%20that%20they%20are%20vulnerable%20to%20backdoor%0Aattacks%2C%20which%20establish%20hidden%20associations%20between%20particular%20input%20patterns%0Aand%20model%20behaviors%2C%20compromising%20model%20integrity%20by%20causing%20undesirable%0Aactions%20with%20manipulated%20inputs.%20This%20vulnerability%20poses%20substantial%20risks%2C%0Aincluding%20reputational%20damage%20to%20model%20owners%20and%20the%20dissemination%20of%20harmful%0Acontent.%20To%20mitigate%20the%20threat%20of%20backdoor%20attacks%2C%20there%20have%20been%20some%0Ainvestigations%20on%20backdoor%20detection%20and%20model%20repair.%20However%2C%20previous%20work%0Afails%20to%20reliably%20purify%20the%20models%20backdoored%20by%20state-of-the-art%20attack%0Amethods%2C%20rendering%20the%20field%20much%20underexplored.%20To%20bridge%20this%20gap%2C%20we%0Aintroduce%20Diff-Cleanse%2C%20a%20novel%20two-stage%20backdoor%20defense%20framework%0Aspecifically%20designed%20for%20DMs.%20The%20first%20stage%20employs%20a%20novel%20trigger%0Ainversion%20technique%20to%20reconstruct%20the%20trigger%20and%20detect%20the%20backdoor%2C%20and%20the%0Asecond%20stage%20utilizes%20a%20structural%20pruning%20method%20to%20eliminate%20the%20backdoor.%20We%0Aevaluate%20our%20framework%20on%20hundreds%20of%20DMs%20that%20are%20attacked%20by%20three%20existing%0Abackdoor%20attack%20methods%20with%20a%20wide%20range%20of%20hyperparameter%20settings.%20Extensive%0Aexperiments%20demonstrate%20that%20Diff-Cleanse%20achieves%20nearly%20100%5C%25%20detection%0Aaccuracy%20and%20effectively%20mitigates%20backdoor%20impacts%2C%20preserving%20the%20model%27s%0Abenign%20performance%20with%20minimal%20compromise.%20Our%20code%20is%20avaliable%20at%0Ahttps%3A//github.com/shymuel/diff-cleanse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21316v2&entry.124074799=Read"},
{"title": "EUIS-Net: A Convolutional Neural Network for Efficient Ultrasound Image\n  Segmentation", "author": "Shahzaib Iqbal and Hasnat Ahmed and Muhammad Sharif and Madiha Hena and Tariq M. Khan and Imran Razzak", "abstract": "  Segmenting ultrasound images is critical for various medical applications,\nbut it offers significant challenges due to ultrasound images' inherent noise\nand unpredictability. To address these challenges, we proposed EUIS-Net, a CNN\nnetwork designed to segment ultrasound images efficiently and precisely. The\nproposed EUIS-Net utilises four encoder-decoder blocks, resulting in a notable\ndecrease in computational complexity while achieving excellent performance. The\nproposed EUIS-Net integrates both channel and spatial attention mechanisms into\nthe bottleneck to improve feature representation and collect significant\ncontextual information. In addition, EUIS-Net incorporates a region-aware\nattention module in skip connections, which enhances the ability to concentrate\non the region of the injury. To enable thorough information exchange across\nvarious network blocks, skip connection aggregation is employed from the\nnetwork's lowermost to the uppermost block. Comprehensive evaluations are\nconducted on two publicly available ultrasound image segmentation datasets. The\nproposed EUIS-Net achieved mean IoU and dice scores of 78. 12\\%, 85. 42\\% and\n84. 73\\%, 89. 01\\% in the BUSI and DDTI datasets, respectively. The findings of\nour study showcase the substantial capabilities of EUIS-Net for immediate use\nin clinical settings and its versatility in various ultrasound imaging tasks.\n", "link": "http://arxiv.org/abs/2408.12323v1", "date": "2024-08-22", "relevancy": 1.517, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5259}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5045}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EUIS-Net%3A%20A%20Convolutional%20Neural%20Network%20for%20Efficient%20Ultrasound%20Image%0A%20%20Segmentation&body=Title%3A%20EUIS-Net%3A%20A%20Convolutional%20Neural%20Network%20for%20Efficient%20Ultrasound%20Image%0A%20%20Segmentation%0AAuthor%3A%20Shahzaib%20Iqbal%20and%20Hasnat%20Ahmed%20and%20Muhammad%20Sharif%20and%20Madiha%20Hena%20and%20Tariq%20M.%20Khan%20and%20Imran%20Razzak%0AAbstract%3A%20%20%20Segmenting%20ultrasound%20images%20is%20critical%20for%20various%20medical%20applications%2C%0Abut%20it%20offers%20significant%20challenges%20due%20to%20ultrasound%20images%27%20inherent%20noise%0Aand%20unpredictability.%20To%20address%20these%20challenges%2C%20we%20proposed%20EUIS-Net%2C%20a%20CNN%0Anetwork%20designed%20to%20segment%20ultrasound%20images%20efficiently%20and%20precisely.%20The%0Aproposed%20EUIS-Net%20utilises%20four%20encoder-decoder%20blocks%2C%20resulting%20in%20a%20notable%0Adecrease%20in%20computational%20complexity%20while%20achieving%20excellent%20performance.%20The%0Aproposed%20EUIS-Net%20integrates%20both%20channel%20and%20spatial%20attention%20mechanisms%20into%0Athe%20bottleneck%20to%20improve%20feature%20representation%20and%20collect%20significant%0Acontextual%20information.%20In%20addition%2C%20EUIS-Net%20incorporates%20a%20region-aware%0Aattention%20module%20in%20skip%20connections%2C%20which%20enhances%20the%20ability%20to%20concentrate%0Aon%20the%20region%20of%20the%20injury.%20To%20enable%20thorough%20information%20exchange%20across%0Avarious%20network%20blocks%2C%20skip%20connection%20aggregation%20is%20employed%20from%20the%0Anetwork%27s%20lowermost%20to%20the%20uppermost%20block.%20Comprehensive%20evaluations%20are%0Aconducted%20on%20two%20publicly%20available%20ultrasound%20image%20segmentation%20datasets.%20The%0Aproposed%20EUIS-Net%20achieved%20mean%20IoU%20and%20dice%20scores%20of%2078.%2012%5C%25%2C%2085.%2042%5C%25%20and%0A84.%2073%5C%25%2C%2089.%2001%5C%25%20in%20the%20BUSI%20and%20DDTI%20datasets%2C%20respectively.%20The%20findings%20of%0Aour%20study%20showcase%20the%20substantial%20capabilities%20of%20EUIS-Net%20for%20immediate%20use%0Ain%20clinical%20settings%20and%20its%20versatility%20in%20various%20ultrasound%20imaging%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEUIS-Net%253A%2520A%2520Convolutional%2520Neural%2520Network%2520for%2520Efficient%2520Ultrasound%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DShahzaib%2520Iqbal%2520and%2520Hasnat%2520Ahmed%2520and%2520Muhammad%2520Sharif%2520and%2520Madiha%2520Hena%2520and%2520Tariq%2520M.%2520Khan%2520and%2520Imran%2520Razzak%26entry.1292438233%3D%2520%2520Segmenting%2520ultrasound%2520images%2520is%2520critical%2520for%2520various%2520medical%2520applications%252C%250Abut%2520it%2520offers%2520significant%2520challenges%2520due%2520to%2520ultrasound%2520images%2527%2520inherent%2520noise%250Aand%2520unpredictability.%2520To%2520address%2520these%2520challenges%252C%2520we%2520proposed%2520EUIS-Net%252C%2520a%2520CNN%250Anetwork%2520designed%2520to%2520segment%2520ultrasound%2520images%2520efficiently%2520and%2520precisely.%2520The%250Aproposed%2520EUIS-Net%2520utilises%2520four%2520encoder-decoder%2520blocks%252C%2520resulting%2520in%2520a%2520notable%250Adecrease%2520in%2520computational%2520complexity%2520while%2520achieving%2520excellent%2520performance.%2520The%250Aproposed%2520EUIS-Net%2520integrates%2520both%2520channel%2520and%2520spatial%2520attention%2520mechanisms%2520into%250Athe%2520bottleneck%2520to%2520improve%2520feature%2520representation%2520and%2520collect%2520significant%250Acontextual%2520information.%2520In%2520addition%252C%2520EUIS-Net%2520incorporates%2520a%2520region-aware%250Aattention%2520module%2520in%2520skip%2520connections%252C%2520which%2520enhances%2520the%2520ability%2520to%2520concentrate%250Aon%2520the%2520region%2520of%2520the%2520injury.%2520To%2520enable%2520thorough%2520information%2520exchange%2520across%250Avarious%2520network%2520blocks%252C%2520skip%2520connection%2520aggregation%2520is%2520employed%2520from%2520the%250Anetwork%2527s%2520lowermost%2520to%2520the%2520uppermost%2520block.%2520Comprehensive%2520evaluations%2520are%250Aconducted%2520on%2520two%2520publicly%2520available%2520ultrasound%2520image%2520segmentation%2520datasets.%2520The%250Aproposed%2520EUIS-Net%2520achieved%2520mean%2520IoU%2520and%2520dice%2520scores%2520of%252078.%252012%255C%2525%252C%252085.%252042%255C%2525%2520and%250A84.%252073%255C%2525%252C%252089.%252001%255C%2525%2520in%2520the%2520BUSI%2520and%2520DDTI%2520datasets%252C%2520respectively.%2520The%2520findings%2520of%250Aour%2520study%2520showcase%2520the%2520substantial%2520capabilities%2520of%2520EUIS-Net%2520for%2520immediate%2520use%250Ain%2520clinical%2520settings%2520and%2520its%2520versatility%2520in%2520various%2520ultrasound%2520imaging%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EUIS-Net%3A%20A%20Convolutional%20Neural%20Network%20for%20Efficient%20Ultrasound%20Image%0A%20%20Segmentation&entry.906535625=Shahzaib%20Iqbal%20and%20Hasnat%20Ahmed%20and%20Muhammad%20Sharif%20and%20Madiha%20Hena%20and%20Tariq%20M.%20Khan%20and%20Imran%20Razzak&entry.1292438233=%20%20Segmenting%20ultrasound%20images%20is%20critical%20for%20various%20medical%20applications%2C%0Abut%20it%20offers%20significant%20challenges%20due%20to%20ultrasound%20images%27%20inherent%20noise%0Aand%20unpredictability.%20To%20address%20these%20challenges%2C%20we%20proposed%20EUIS-Net%2C%20a%20CNN%0Anetwork%20designed%20to%20segment%20ultrasound%20images%20efficiently%20and%20precisely.%20The%0Aproposed%20EUIS-Net%20utilises%20four%20encoder-decoder%20blocks%2C%20resulting%20in%20a%20notable%0Adecrease%20in%20computational%20complexity%20while%20achieving%20excellent%20performance.%20The%0Aproposed%20EUIS-Net%20integrates%20both%20channel%20and%20spatial%20attention%20mechanisms%20into%0Athe%20bottleneck%20to%20improve%20feature%20representation%20and%20collect%20significant%0Acontextual%20information.%20In%20addition%2C%20EUIS-Net%20incorporates%20a%20region-aware%0Aattention%20module%20in%20skip%20connections%2C%20which%20enhances%20the%20ability%20to%20concentrate%0Aon%20the%20region%20of%20the%20injury.%20To%20enable%20thorough%20information%20exchange%20across%0Avarious%20network%20blocks%2C%20skip%20connection%20aggregation%20is%20employed%20from%20the%0Anetwork%27s%20lowermost%20to%20the%20uppermost%20block.%20Comprehensive%20evaluations%20are%0Aconducted%20on%20two%20publicly%20available%20ultrasound%20image%20segmentation%20datasets.%20The%0Aproposed%20EUIS-Net%20achieved%20mean%20IoU%20and%20dice%20scores%20of%2078.%2012%5C%25%2C%2085.%2042%5C%25%20and%0A84.%2073%5C%25%2C%2089.%2001%5C%25%20in%20the%20BUSI%20and%20DDTI%20datasets%2C%20respectively.%20The%20findings%20of%0Aour%20study%20showcase%20the%20substantial%20capabilities%20of%20EUIS-Net%20for%20immediate%20use%0Ain%20clinical%20settings%20and%20its%20versatility%20in%20various%20ultrasound%20imaging%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12323v1&entry.124074799=Read"},
{"title": "API-guided Dataset Synthesis to Finetune Large Code Models", "author": "Zongjie Li and Daoyuan Wu and Shuai Wang and Zhendong Su", "abstract": "  Large code models (LCMs), pre-trained on vast code corpora, have demonstrated\nremarkable performance across a wide array of code-related tasks. Supervised\nfine-tuning (SFT) plays a vital role in aligning these models with specific\nrequirements and enhancing their performance in particular domains. However,\nsynthesizing high-quality SFT datasets poses a significant challenge due to the\nuneven quality of datasets and the scarcity of domain-specific datasets.\n  Inspired by APIs as high-level abstractions of code that encapsulate rich\nsemantic information in a concise structure, we propose DataScope, an\nAPI-guided dataset synthesis framework designed to enhance the SFT process for\nLCMs in both general and domain-specific scenarios. DataScope comprises two\nmain components: Dsel and Dgen. On one hand, Dsel employs API coverage as a\ncore metric, enabling efficient dataset synthesis in general scenarios by\nselecting subsets of existing (uneven-quality) datasets with higher API\ncoverage. On the other hand, Dgen recasts domain dataset synthesis as a process\nof using API-specified high-level functionality and deliberately-constituted\ncode skeletons to synthesize concrete code.\n  Extensive experiments demonstrate DataScope's effectiveness, with models\nfine-tuned on its synthesized datasets outperforming those tuned on unoptimized\ndatasets five times larger. Furthermore, a series of analyses on model\ninternals, relevant hyperparameters, and case studies provide additional\nevidence for the efficacy of our proposed methods. These findings underscore\nthe significance of dataset quality in SFT and advance the field of LCMs by\nproviding an efficient, cost-effective framework for constructing high-quality\ndatasets. This contribution enhances performance across both general and\ndomain-specific scenarios, paving the way for more powerful and tailored LCMs.\n", "link": "http://arxiv.org/abs/2408.08343v2", "date": "2024-08-22", "relevancy": 1.4256, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.509}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4664}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20API-guided%20Dataset%20Synthesis%20to%20Finetune%20Large%20Code%20Models&body=Title%3A%20API-guided%20Dataset%20Synthesis%20to%20Finetune%20Large%20Code%20Models%0AAuthor%3A%20Zongjie%20Li%20and%20Daoyuan%20Wu%20and%20Shuai%20Wang%20and%20Zhendong%20Su%0AAbstract%3A%20%20%20Large%20code%20models%20%28LCMs%29%2C%20pre-trained%20on%20vast%20code%20corpora%2C%20have%20demonstrated%0Aremarkable%20performance%20across%20a%20wide%20array%20of%20code-related%20tasks.%20Supervised%0Afine-tuning%20%28SFT%29%20plays%20a%20vital%20role%20in%20aligning%20these%20models%20with%20specific%0Arequirements%20and%20enhancing%20their%20performance%20in%20particular%20domains.%20However%2C%0Asynthesizing%20high-quality%20SFT%20datasets%20poses%20a%20significant%20challenge%20due%20to%20the%0Auneven%20quality%20of%20datasets%20and%20the%20scarcity%20of%20domain-specific%20datasets.%0A%20%20Inspired%20by%20APIs%20as%20high-level%20abstractions%20of%20code%20that%20encapsulate%20rich%0Asemantic%20information%20in%20a%20concise%20structure%2C%20we%20propose%20DataScope%2C%20an%0AAPI-guided%20dataset%20synthesis%20framework%20designed%20to%20enhance%20the%20SFT%20process%20for%0ALCMs%20in%20both%20general%20and%20domain-specific%20scenarios.%20DataScope%20comprises%20two%0Amain%20components%3A%20Dsel%20and%20Dgen.%20On%20one%20hand%2C%20Dsel%20employs%20API%20coverage%20as%20a%0Acore%20metric%2C%20enabling%20efficient%20dataset%20synthesis%20in%20general%20scenarios%20by%0Aselecting%20subsets%20of%20existing%20%28uneven-quality%29%20datasets%20with%20higher%20API%0Acoverage.%20On%20the%20other%20hand%2C%20Dgen%20recasts%20domain%20dataset%20synthesis%20as%20a%20process%0Aof%20using%20API-specified%20high-level%20functionality%20and%20deliberately-constituted%0Acode%20skeletons%20to%20synthesize%20concrete%20code.%0A%20%20Extensive%20experiments%20demonstrate%20DataScope%27s%20effectiveness%2C%20with%20models%0Afine-tuned%20on%20its%20synthesized%20datasets%20outperforming%20those%20tuned%20on%20unoptimized%0Adatasets%20five%20times%20larger.%20Furthermore%2C%20a%20series%20of%20analyses%20on%20model%0Ainternals%2C%20relevant%20hyperparameters%2C%20and%20case%20studies%20provide%20additional%0Aevidence%20for%20the%20efficacy%20of%20our%20proposed%20methods.%20These%20findings%20underscore%0Athe%20significance%20of%20dataset%20quality%20in%20SFT%20and%20advance%20the%20field%20of%20LCMs%20by%0Aproviding%20an%20efficient%2C%20cost-effective%20framework%20for%20constructing%20high-quality%0Adatasets.%20This%20contribution%20enhances%20performance%20across%20both%20general%20and%0Adomain-specific%20scenarios%2C%20paving%20the%20way%20for%20more%20powerful%20and%20tailored%20LCMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPI-guided%2520Dataset%2520Synthesis%2520to%2520Finetune%2520Large%2520Code%2520Models%26entry.906535625%3DZongjie%2520Li%2520and%2520Daoyuan%2520Wu%2520and%2520Shuai%2520Wang%2520and%2520Zhendong%2520Su%26entry.1292438233%3D%2520%2520Large%2520code%2520models%2520%2528LCMs%2529%252C%2520pre-trained%2520on%2520vast%2520code%2520corpora%252C%2520have%2520demonstrated%250Aremarkable%2520performance%2520across%2520a%2520wide%2520array%2520of%2520code-related%2520tasks.%2520Supervised%250Afine-tuning%2520%2528SFT%2529%2520plays%2520a%2520vital%2520role%2520in%2520aligning%2520these%2520models%2520with%2520specific%250Arequirements%2520and%2520enhancing%2520their%2520performance%2520in%2520particular%2520domains.%2520However%252C%250Asynthesizing%2520high-quality%2520SFT%2520datasets%2520poses%2520a%2520significant%2520challenge%2520due%2520to%2520the%250Auneven%2520quality%2520of%2520datasets%2520and%2520the%2520scarcity%2520of%2520domain-specific%2520datasets.%250A%2520%2520Inspired%2520by%2520APIs%2520as%2520high-level%2520abstractions%2520of%2520code%2520that%2520encapsulate%2520rich%250Asemantic%2520information%2520in%2520a%2520concise%2520structure%252C%2520we%2520propose%2520DataScope%252C%2520an%250AAPI-guided%2520dataset%2520synthesis%2520framework%2520designed%2520to%2520enhance%2520the%2520SFT%2520process%2520for%250ALCMs%2520in%2520both%2520general%2520and%2520domain-specific%2520scenarios.%2520DataScope%2520comprises%2520two%250Amain%2520components%253A%2520Dsel%2520and%2520Dgen.%2520On%2520one%2520hand%252C%2520Dsel%2520employs%2520API%2520coverage%2520as%2520a%250Acore%2520metric%252C%2520enabling%2520efficient%2520dataset%2520synthesis%2520in%2520general%2520scenarios%2520by%250Aselecting%2520subsets%2520of%2520existing%2520%2528uneven-quality%2529%2520datasets%2520with%2520higher%2520API%250Acoverage.%2520On%2520the%2520other%2520hand%252C%2520Dgen%2520recasts%2520domain%2520dataset%2520synthesis%2520as%2520a%2520process%250Aof%2520using%2520API-specified%2520high-level%2520functionality%2520and%2520deliberately-constituted%250Acode%2520skeletons%2520to%2520synthesize%2520concrete%2520code.%250A%2520%2520Extensive%2520experiments%2520demonstrate%2520DataScope%2527s%2520effectiveness%252C%2520with%2520models%250Afine-tuned%2520on%2520its%2520synthesized%2520datasets%2520outperforming%2520those%2520tuned%2520on%2520unoptimized%250Adatasets%2520five%2520times%2520larger.%2520Furthermore%252C%2520a%2520series%2520of%2520analyses%2520on%2520model%250Ainternals%252C%2520relevant%2520hyperparameters%252C%2520and%2520case%2520studies%2520provide%2520additional%250Aevidence%2520for%2520the%2520efficacy%2520of%2520our%2520proposed%2520methods.%2520These%2520findings%2520underscore%250Athe%2520significance%2520of%2520dataset%2520quality%2520in%2520SFT%2520and%2520advance%2520the%2520field%2520of%2520LCMs%2520by%250Aproviding%2520an%2520efficient%252C%2520cost-effective%2520framework%2520for%2520constructing%2520high-quality%250Adatasets.%2520This%2520contribution%2520enhances%2520performance%2520across%2520both%2520general%2520and%250Adomain-specific%2520scenarios%252C%2520paving%2520the%2520way%2520for%2520more%2520powerful%2520and%2520tailored%2520LCMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=API-guided%20Dataset%20Synthesis%20to%20Finetune%20Large%20Code%20Models&entry.906535625=Zongjie%20Li%20and%20Daoyuan%20Wu%20and%20Shuai%20Wang%20and%20Zhendong%20Su&entry.1292438233=%20%20Large%20code%20models%20%28LCMs%29%2C%20pre-trained%20on%20vast%20code%20corpora%2C%20have%20demonstrated%0Aremarkable%20performance%20across%20a%20wide%20array%20of%20code-related%20tasks.%20Supervised%0Afine-tuning%20%28SFT%29%20plays%20a%20vital%20role%20in%20aligning%20these%20models%20with%20specific%0Arequirements%20and%20enhancing%20their%20performance%20in%20particular%20domains.%20However%2C%0Asynthesizing%20high-quality%20SFT%20datasets%20poses%20a%20significant%20challenge%20due%20to%20the%0Auneven%20quality%20of%20datasets%20and%20the%20scarcity%20of%20domain-specific%20datasets.%0A%20%20Inspired%20by%20APIs%20as%20high-level%20abstractions%20of%20code%20that%20encapsulate%20rich%0Asemantic%20information%20in%20a%20concise%20structure%2C%20we%20propose%20DataScope%2C%20an%0AAPI-guided%20dataset%20synthesis%20framework%20designed%20to%20enhance%20the%20SFT%20process%20for%0ALCMs%20in%20both%20general%20and%20domain-specific%20scenarios.%20DataScope%20comprises%20two%0Amain%20components%3A%20Dsel%20and%20Dgen.%20On%20one%20hand%2C%20Dsel%20employs%20API%20coverage%20as%20a%0Acore%20metric%2C%20enabling%20efficient%20dataset%20synthesis%20in%20general%20scenarios%20by%0Aselecting%20subsets%20of%20existing%20%28uneven-quality%29%20datasets%20with%20higher%20API%0Acoverage.%20On%20the%20other%20hand%2C%20Dgen%20recasts%20domain%20dataset%20synthesis%20as%20a%20process%0Aof%20using%20API-specified%20high-level%20functionality%20and%20deliberately-constituted%0Acode%20skeletons%20to%20synthesize%20concrete%20code.%0A%20%20Extensive%20experiments%20demonstrate%20DataScope%27s%20effectiveness%2C%20with%20models%0Afine-tuned%20on%20its%20synthesized%20datasets%20outperforming%20those%20tuned%20on%20unoptimized%0Adatasets%20five%20times%20larger.%20Furthermore%2C%20a%20series%20of%20analyses%20on%20model%0Ainternals%2C%20relevant%20hyperparameters%2C%20and%20case%20studies%20provide%20additional%0Aevidence%20for%20the%20efficacy%20of%20our%20proposed%20methods.%20These%20findings%20underscore%0Athe%20significance%20of%20dataset%20quality%20in%20SFT%20and%20advance%20the%20field%20of%20LCMs%20by%0Aproviding%20an%20efficient%2C%20cost-effective%20framework%20for%20constructing%20high-quality%0Adatasets.%20This%20contribution%20enhances%20performance%20across%20both%20general%20and%0Adomain-specific%20scenarios%2C%20paving%20the%20way%20for%20more%20powerful%20and%20tailored%20LCMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08343v2&entry.124074799=Read"},
{"title": "Data Quality Antipatterns for Software Analytics", "author": "Aaditya Bhatia and Dayi Lin and Gopi Krishnan Rajbahadur and Bram Adams and Ahmed E. Hassan", "abstract": "  Background: Data quality is vital in software analytics, particularly for\nmachine learning (ML) applications like software defect prediction (SDP).\nDespite the widespread use of ML in software engineering, the effect of data\nquality antipatterns on these models remains underexplored.\n  Objective: This study develops a taxonomy of ML-specific data quality\nantipatterns and assesses their impact on software analytics models'\nperformance and interpretation.\n  Methods: We identified eight types and 14 sub-types of ML-specific data\nquality antipatterns through a literature review. We conducted experiments to\ndetermine the prevalence of these antipatterns in SDP data (RQ1), assess how\ncleaning order affects model performance (RQ2), evaluate the impact of\nantipattern removal on performance (RQ3), and examine the consistency of\ninterpretation from models built with different antipatterns (RQ4).\n  Results: In our SDP case study, we identified nine antipatterns. Over 90% of\nthese overlapped at both row and column levels, complicating cleaning\nprioritization and risking excessive data removal. The order of cleaning\nsignificantly impacts ML model performance, with neural networks being more\nresilient to cleaning order changes than simpler models like logistic\nregression. Antipatterns such as Tailed Distributions and Class Overlap show a\nstatistically significant correlation with performance metrics when other\nantipatterns are cleaned. Models built with different antipatterns showed\nmoderate consistency in interpretation results.\n  Conclusion: The cleaning order of different antipatterns impacts ML model\nperformance. Five antipatterns have a statistically significant correlation\nwith model performance when others are cleaned. Additionally, model\ninterpretation is moderately affected by different data quality antipatterns.\n", "link": "http://arxiv.org/abs/2408.12560v1", "date": "2024-08-22", "relevancy": 1.2443, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4302}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4112}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Quality%20Antipatterns%20for%20Software%20Analytics&body=Title%3A%20Data%20Quality%20Antipatterns%20for%20Software%20Analytics%0AAuthor%3A%20Aaditya%20Bhatia%20and%20Dayi%20Lin%20and%20Gopi%20Krishnan%20Rajbahadur%20and%20Bram%20Adams%20and%20Ahmed%20E.%20Hassan%0AAbstract%3A%20%20%20Background%3A%20Data%20quality%20is%20vital%20in%20software%20analytics%2C%20particularly%20for%0Amachine%20learning%20%28ML%29%20applications%20like%20software%20defect%20prediction%20%28SDP%29.%0ADespite%20the%20widespread%20use%20of%20ML%20in%20software%20engineering%2C%20the%20effect%20of%20data%0Aquality%20antipatterns%20on%20these%20models%20remains%20underexplored.%0A%20%20Objective%3A%20This%20study%20develops%20a%20taxonomy%20of%20ML-specific%20data%20quality%0Aantipatterns%20and%20assesses%20their%20impact%20on%20software%20analytics%20models%27%0Aperformance%20and%20interpretation.%0A%20%20Methods%3A%20We%20identified%20eight%20types%20and%2014%20sub-types%20of%20ML-specific%20data%0Aquality%20antipatterns%20through%20a%20literature%20review.%20We%20conducted%20experiments%20to%0Adetermine%20the%20prevalence%20of%20these%20antipatterns%20in%20SDP%20data%20%28RQ1%29%2C%20assess%20how%0Acleaning%20order%20affects%20model%20performance%20%28RQ2%29%2C%20evaluate%20the%20impact%20of%0Aantipattern%20removal%20on%20performance%20%28RQ3%29%2C%20and%20examine%20the%20consistency%20of%0Ainterpretation%20from%20models%20built%20with%20different%20antipatterns%20%28RQ4%29.%0A%20%20Results%3A%20In%20our%20SDP%20case%20study%2C%20we%20identified%20nine%20antipatterns.%20Over%2090%25%20of%0Athese%20overlapped%20at%20both%20row%20and%20column%20levels%2C%20complicating%20cleaning%0Aprioritization%20and%20risking%20excessive%20data%20removal.%20The%20order%20of%20cleaning%0Asignificantly%20impacts%20ML%20model%20performance%2C%20with%20neural%20networks%20being%20more%0Aresilient%20to%20cleaning%20order%20changes%20than%20simpler%20models%20like%20logistic%0Aregression.%20Antipatterns%20such%20as%20Tailed%20Distributions%20and%20Class%20Overlap%20show%20a%0Astatistically%20significant%20correlation%20with%20performance%20metrics%20when%20other%0Aantipatterns%20are%20cleaned.%20Models%20built%20with%20different%20antipatterns%20showed%0Amoderate%20consistency%20in%20interpretation%20results.%0A%20%20Conclusion%3A%20The%20cleaning%20order%20of%20different%20antipatterns%20impacts%20ML%20model%0Aperformance.%20Five%20antipatterns%20have%20a%20statistically%20significant%20correlation%0Awith%20model%20performance%20when%20others%20are%20cleaned.%20Additionally%2C%20model%0Ainterpretation%20is%20moderately%20affected%20by%20different%20data%20quality%20antipatterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Quality%2520Antipatterns%2520for%2520Software%2520Analytics%26entry.906535625%3DAaditya%2520Bhatia%2520and%2520Dayi%2520Lin%2520and%2520Gopi%2520Krishnan%2520Rajbahadur%2520and%2520Bram%2520Adams%2520and%2520Ahmed%2520E.%2520Hassan%26entry.1292438233%3D%2520%2520Background%253A%2520Data%2520quality%2520is%2520vital%2520in%2520software%2520analytics%252C%2520particularly%2520for%250Amachine%2520learning%2520%2528ML%2529%2520applications%2520like%2520software%2520defect%2520prediction%2520%2528SDP%2529.%250ADespite%2520the%2520widespread%2520use%2520of%2520ML%2520in%2520software%2520engineering%252C%2520the%2520effect%2520of%2520data%250Aquality%2520antipatterns%2520on%2520these%2520models%2520remains%2520underexplored.%250A%2520%2520Objective%253A%2520This%2520study%2520develops%2520a%2520taxonomy%2520of%2520ML-specific%2520data%2520quality%250Aantipatterns%2520and%2520assesses%2520their%2520impact%2520on%2520software%2520analytics%2520models%2527%250Aperformance%2520and%2520interpretation.%250A%2520%2520Methods%253A%2520We%2520identified%2520eight%2520types%2520and%252014%2520sub-types%2520of%2520ML-specific%2520data%250Aquality%2520antipatterns%2520through%2520a%2520literature%2520review.%2520We%2520conducted%2520experiments%2520to%250Adetermine%2520the%2520prevalence%2520of%2520these%2520antipatterns%2520in%2520SDP%2520data%2520%2528RQ1%2529%252C%2520assess%2520how%250Acleaning%2520order%2520affects%2520model%2520performance%2520%2528RQ2%2529%252C%2520evaluate%2520the%2520impact%2520of%250Aantipattern%2520removal%2520on%2520performance%2520%2528RQ3%2529%252C%2520and%2520examine%2520the%2520consistency%2520of%250Ainterpretation%2520from%2520models%2520built%2520with%2520different%2520antipatterns%2520%2528RQ4%2529.%250A%2520%2520Results%253A%2520In%2520our%2520SDP%2520case%2520study%252C%2520we%2520identified%2520nine%2520antipatterns.%2520Over%252090%2525%2520of%250Athese%2520overlapped%2520at%2520both%2520row%2520and%2520column%2520levels%252C%2520complicating%2520cleaning%250Aprioritization%2520and%2520risking%2520excessive%2520data%2520removal.%2520The%2520order%2520of%2520cleaning%250Asignificantly%2520impacts%2520ML%2520model%2520performance%252C%2520with%2520neural%2520networks%2520being%2520more%250Aresilient%2520to%2520cleaning%2520order%2520changes%2520than%2520simpler%2520models%2520like%2520logistic%250Aregression.%2520Antipatterns%2520such%2520as%2520Tailed%2520Distributions%2520and%2520Class%2520Overlap%2520show%2520a%250Astatistically%2520significant%2520correlation%2520with%2520performance%2520metrics%2520when%2520other%250Aantipatterns%2520are%2520cleaned.%2520Models%2520built%2520with%2520different%2520antipatterns%2520showed%250Amoderate%2520consistency%2520in%2520interpretation%2520results.%250A%2520%2520Conclusion%253A%2520The%2520cleaning%2520order%2520of%2520different%2520antipatterns%2520impacts%2520ML%2520model%250Aperformance.%2520Five%2520antipatterns%2520have%2520a%2520statistically%2520significant%2520correlation%250Awith%2520model%2520performance%2520when%2520others%2520are%2520cleaned.%2520Additionally%252C%2520model%250Ainterpretation%2520is%2520moderately%2520affected%2520by%2520different%2520data%2520quality%2520antipatterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Quality%20Antipatterns%20for%20Software%20Analytics&entry.906535625=Aaditya%20Bhatia%20and%20Dayi%20Lin%20and%20Gopi%20Krishnan%20Rajbahadur%20and%20Bram%20Adams%20and%20Ahmed%20E.%20Hassan&entry.1292438233=%20%20Background%3A%20Data%20quality%20is%20vital%20in%20software%20analytics%2C%20particularly%20for%0Amachine%20learning%20%28ML%29%20applications%20like%20software%20defect%20prediction%20%28SDP%29.%0ADespite%20the%20widespread%20use%20of%20ML%20in%20software%20engineering%2C%20the%20effect%20of%20data%0Aquality%20antipatterns%20on%20these%20models%20remains%20underexplored.%0A%20%20Objective%3A%20This%20study%20develops%20a%20taxonomy%20of%20ML-specific%20data%20quality%0Aantipatterns%20and%20assesses%20their%20impact%20on%20software%20analytics%20models%27%0Aperformance%20and%20interpretation.%0A%20%20Methods%3A%20We%20identified%20eight%20types%20and%2014%20sub-types%20of%20ML-specific%20data%0Aquality%20antipatterns%20through%20a%20literature%20review.%20We%20conducted%20experiments%20to%0Adetermine%20the%20prevalence%20of%20these%20antipatterns%20in%20SDP%20data%20%28RQ1%29%2C%20assess%20how%0Acleaning%20order%20affects%20model%20performance%20%28RQ2%29%2C%20evaluate%20the%20impact%20of%0Aantipattern%20removal%20on%20performance%20%28RQ3%29%2C%20and%20examine%20the%20consistency%20of%0Ainterpretation%20from%20models%20built%20with%20different%20antipatterns%20%28RQ4%29.%0A%20%20Results%3A%20In%20our%20SDP%20case%20study%2C%20we%20identified%20nine%20antipatterns.%20Over%2090%25%20of%0Athese%20overlapped%20at%20both%20row%20and%20column%20levels%2C%20complicating%20cleaning%0Aprioritization%20and%20risking%20excessive%20data%20removal.%20The%20order%20of%20cleaning%0Asignificantly%20impacts%20ML%20model%20performance%2C%20with%20neural%20networks%20being%20more%0Aresilient%20to%20cleaning%20order%20changes%20than%20simpler%20models%20like%20logistic%0Aregression.%20Antipatterns%20such%20as%20Tailed%20Distributions%20and%20Class%20Overlap%20show%20a%0Astatistically%20significant%20correlation%20with%20performance%20metrics%20when%20other%0Aantipatterns%20are%20cleaned.%20Models%20built%20with%20different%20antipatterns%20showed%0Amoderate%20consistency%20in%20interpretation%20results.%0A%20%20Conclusion%3A%20The%20cleaning%20order%20of%20different%20antipatterns%20impacts%20ML%20model%0Aperformance.%20Five%20antipatterns%20have%20a%20statistically%20significant%20correlation%0Awith%20model%20performance%20when%20others%20are%20cleaned.%20Additionally%2C%20model%0Ainterpretation%20is%20moderately%20affected%20by%20different%20data%20quality%20antipatterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12560v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


