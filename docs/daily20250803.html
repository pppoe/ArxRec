<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250731.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar\n  Reconstruction", "author": "Zijian Dong and Longteng Duan and Jie Song and Michael J. Black and Andreas Geiger", "abstract": "  We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian\navatars from a single-view image. The main challenge lies in inferring unseen\nappearance and geometric details while ensuring 3D consistency and realism.\nMost previous methods rely on 2D diffusion models to synthesize unseen views;\nhowever, these generated views are sparse and inconsistent, resulting in\nunrealistic 3D artifacts and blurred appearance. To address these limitations,\nwe leverage a generative avatar model, that can generate diverse 3D avatars by\nsampling deformed Gaussians from a learned prior distribution. Due to the\nlimited amount of 3D training data such a 3D model alone cannot capture all\nimage details of unseen identities. Consequently, we integrate it as a prior,\nensuring 3D consistency by projecting input images into its latent space and\nenforcing additional 3D appearance and geometric constraints. Our novel\napproach formulates Gaussian avatar creation as a model inversion process by\nfitting the generative avatar to synthetic views from 2D diffusion models. The\ngenerative avatar provides a meaningful initialization for model fitting,\nenforces 3D regularization, and helps in refining pose estimation. Experiments\nshow that our method surpasses state-of-the-art techniques and generalizes well\nto real-world scenarios. Our Gaussian avatars are also inherently animatable\n", "link": "http://arxiv.org/abs/2507.23597v1", "date": "2025-07-31", "relevancy": 3.6279, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7438}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7438}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoGA%3A%203D%20Generative%20Avatar%20Prior%20for%20Monocular%20Gaussian%20Avatar%0A%20%20Reconstruction&body=Title%3A%20MoGA%3A%203D%20Generative%20Avatar%20Prior%20for%20Monocular%20Gaussian%20Avatar%0A%20%20Reconstruction%0AAuthor%3A%20Zijian%20Dong%20and%20Longteng%20Duan%20and%20Jie%20Song%20and%20Michael%20J.%20Black%20and%20Andreas%20Geiger%0AAbstract%3A%20%20%20We%20present%20MoGA%2C%20a%20novel%20method%20to%20reconstruct%20high-fidelity%203D%20Gaussian%0Aavatars%20from%20a%20single-view%20image.%20The%20main%20challenge%20lies%20in%20inferring%20unseen%0Aappearance%20and%20geometric%20details%20while%20ensuring%203D%20consistency%20and%20realism.%0AMost%20previous%20methods%20rely%20on%202D%20diffusion%20models%20to%20synthesize%20unseen%20views%3B%0Ahowever%2C%20these%20generated%20views%20are%20sparse%20and%20inconsistent%2C%20resulting%20in%0Aunrealistic%203D%20artifacts%20and%20blurred%20appearance.%20To%20address%20these%20limitations%2C%0Awe%20leverage%20a%20generative%20avatar%20model%2C%20that%20can%20generate%20diverse%203D%20avatars%20by%0Asampling%20deformed%20Gaussians%20from%20a%20learned%20prior%20distribution.%20Due%20to%20the%0Alimited%20amount%20of%203D%20training%20data%20such%20a%203D%20model%20alone%20cannot%20capture%20all%0Aimage%20details%20of%20unseen%20identities.%20Consequently%2C%20we%20integrate%20it%20as%20a%20prior%2C%0Aensuring%203D%20consistency%20by%20projecting%20input%20images%20into%20its%20latent%20space%20and%0Aenforcing%20additional%203D%20appearance%20and%20geometric%20constraints.%20Our%20novel%0Aapproach%20formulates%20Gaussian%20avatar%20creation%20as%20a%20model%20inversion%20process%20by%0Afitting%20the%20generative%20avatar%20to%20synthetic%20views%20from%202D%20diffusion%20models.%20The%0Agenerative%20avatar%20provides%20a%20meaningful%20initialization%20for%20model%20fitting%2C%0Aenforces%203D%20regularization%2C%20and%20helps%20in%20refining%20pose%20estimation.%20Experiments%0Ashow%20that%20our%20method%20surpasses%20state-of-the-art%20techniques%20and%20generalizes%20well%0Ato%20real-world%20scenarios.%20Our%20Gaussian%20avatars%20are%20also%20inherently%20animatable%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoGA%253A%25203D%2520Generative%2520Avatar%2520Prior%2520for%2520Monocular%2520Gaussian%2520Avatar%250A%2520%2520Reconstruction%26entry.906535625%3DZijian%2520Dong%2520and%2520Longteng%2520Duan%2520and%2520Jie%2520Song%2520and%2520Michael%2520J.%2520Black%2520and%2520Andreas%2520Geiger%26entry.1292438233%3D%2520%2520We%2520present%2520MoGA%252C%2520a%2520novel%2520method%2520to%2520reconstruct%2520high-fidelity%25203D%2520Gaussian%250Aavatars%2520from%2520a%2520single-view%2520image.%2520The%2520main%2520challenge%2520lies%2520in%2520inferring%2520unseen%250Aappearance%2520and%2520geometric%2520details%2520while%2520ensuring%25203D%2520consistency%2520and%2520realism.%250AMost%2520previous%2520methods%2520rely%2520on%25202D%2520diffusion%2520models%2520to%2520synthesize%2520unseen%2520views%253B%250Ahowever%252C%2520these%2520generated%2520views%2520are%2520sparse%2520and%2520inconsistent%252C%2520resulting%2520in%250Aunrealistic%25203D%2520artifacts%2520and%2520blurred%2520appearance.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520leverage%2520a%2520generative%2520avatar%2520model%252C%2520that%2520can%2520generate%2520diverse%25203D%2520avatars%2520by%250Asampling%2520deformed%2520Gaussians%2520from%2520a%2520learned%2520prior%2520distribution.%2520Due%2520to%2520the%250Alimited%2520amount%2520of%25203D%2520training%2520data%2520such%2520a%25203D%2520model%2520alone%2520cannot%2520capture%2520all%250Aimage%2520details%2520of%2520unseen%2520identities.%2520Consequently%252C%2520we%2520integrate%2520it%2520as%2520a%2520prior%252C%250Aensuring%25203D%2520consistency%2520by%2520projecting%2520input%2520images%2520into%2520its%2520latent%2520space%2520and%250Aenforcing%2520additional%25203D%2520appearance%2520and%2520geometric%2520constraints.%2520Our%2520novel%250Aapproach%2520formulates%2520Gaussian%2520avatar%2520creation%2520as%2520a%2520model%2520inversion%2520process%2520by%250Afitting%2520the%2520generative%2520avatar%2520to%2520synthetic%2520views%2520from%25202D%2520diffusion%2520models.%2520The%250Agenerative%2520avatar%2520provides%2520a%2520meaningful%2520initialization%2520for%2520model%2520fitting%252C%250Aenforces%25203D%2520regularization%252C%2520and%2520helps%2520in%2520refining%2520pose%2520estimation.%2520Experiments%250Ashow%2520that%2520our%2520method%2520surpasses%2520state-of-the-art%2520techniques%2520and%2520generalizes%2520well%250Ato%2520real-world%2520scenarios.%2520Our%2520Gaussian%2520avatars%2520are%2520also%2520inherently%2520animatable%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoGA%3A%203D%20Generative%20Avatar%20Prior%20for%20Monocular%20Gaussian%20Avatar%0A%20%20Reconstruction&entry.906535625=Zijian%20Dong%20and%20Longteng%20Duan%20and%20Jie%20Song%20and%20Michael%20J.%20Black%20and%20Andreas%20Geiger&entry.1292438233=%20%20We%20present%20MoGA%2C%20a%20novel%20method%20to%20reconstruct%20high-fidelity%203D%20Gaussian%0Aavatars%20from%20a%20single-view%20image.%20The%20main%20challenge%20lies%20in%20inferring%20unseen%0Aappearance%20and%20geometric%20details%20while%20ensuring%203D%20consistency%20and%20realism.%0AMost%20previous%20methods%20rely%20on%202D%20diffusion%20models%20to%20synthesize%20unseen%20views%3B%0Ahowever%2C%20these%20generated%20views%20are%20sparse%20and%20inconsistent%2C%20resulting%20in%0Aunrealistic%203D%20artifacts%20and%20blurred%20appearance.%20To%20address%20these%20limitations%2C%0Awe%20leverage%20a%20generative%20avatar%20model%2C%20that%20can%20generate%20diverse%203D%20avatars%20by%0Asampling%20deformed%20Gaussians%20from%20a%20learned%20prior%20distribution.%20Due%20to%20the%0Alimited%20amount%20of%203D%20training%20data%20such%20a%203D%20model%20alone%20cannot%20capture%20all%0Aimage%20details%20of%20unseen%20identities.%20Consequently%2C%20we%20integrate%20it%20as%20a%20prior%2C%0Aensuring%203D%20consistency%20by%20projecting%20input%20images%20into%20its%20latent%20space%20and%0Aenforcing%20additional%203D%20appearance%20and%20geometric%20constraints.%20Our%20novel%0Aapproach%20formulates%20Gaussian%20avatar%20creation%20as%20a%20model%20inversion%20process%20by%0Afitting%20the%20generative%20avatar%20to%20synthetic%20views%20from%202D%20diffusion%20models.%20The%0Agenerative%20avatar%20provides%20a%20meaningful%20initialization%20for%20model%20fitting%2C%0Aenforces%203D%20regularization%2C%20and%20helps%20in%20refining%20pose%20estimation.%20Experiments%0Ashow%20that%20our%20method%20surpasses%20state-of-the-art%20techniques%20and%20generalizes%20well%0Ato%20real-world%20scenarios.%20Our%20Gaussian%20avatars%20are%20also%20inherently%20animatable%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23597v1&entry.124074799=Read"},
{"title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D\n  Content Creation from a Single Image", "author": "DongFu Yin and Xiaotian Chen and Fei Richard Yu and Xuanchen Li and Xinhao Zhang", "abstract": "  Advances in generative modeling have significantly enhanced digital content\ncreation, extending from 2D images to complex 3D and 4D scenes. Despite\nsubstantial progress, producing high-fidelity and temporally consistent dynamic\n4D content remains a challenge. In this paper, we propose MVG4D, a novel\nframework that generates dynamic 4D content from a single still image by\ncombining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,\nMVG4D employs an image matrix module that synthesizes temporally coherent and\nspatially diverse multi-view images, providing rich supervisory signals for\ndownstream 3D and 4D reconstruction. These multi-view images are used to\noptimize a 3D Gaussian point cloud, which is further extended into the temporal\ndomain via a lightweight deformation network. Our method effectively enhances\ntemporal consistency, geometric fidelity, and visual realism, addressing key\nchallenges in motion discontinuity and background degradation that affect prior\n4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate\nthat MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and\ntime efficiency. Notably, it reduces flickering artifacts and sharpens\nstructural details across views and time, enabling more immersive AR/VR\nexperiences. MVG4D sets a new direction for efficient and controllable 4D\ngeneration from minimal inputs.\n", "link": "http://arxiv.org/abs/2507.18371v2", "date": "2025-07-31", "relevancy": 3.4163, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6976}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6976}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVG4D%3A%20Image%20Matrix-Based%20Multi-View%20and%20Motion%20Generation%20for%204D%0A%20%20Content%20Creation%20from%20a%20Single%20Image&body=Title%3A%20MVG4D%3A%20Image%20Matrix-Based%20Multi-View%20and%20Motion%20Generation%20for%204D%0A%20%20Content%20Creation%20from%20a%20Single%20Image%0AAuthor%3A%20DongFu%20Yin%20and%20Xiaotian%20Chen%20and%20Fei%20Richard%20Yu%20and%20Xuanchen%20Li%20and%20Xinhao%20Zhang%0AAbstract%3A%20%20%20Advances%20in%20generative%20modeling%20have%20significantly%20enhanced%20digital%20content%0Acreation%2C%20extending%20from%202D%20images%20to%20complex%203D%20and%204D%20scenes.%20Despite%0Asubstantial%20progress%2C%20producing%20high-fidelity%20and%20temporally%20consistent%20dynamic%0A4D%20content%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20propose%20MVG4D%2C%20a%20novel%0Aframework%20that%20generates%20dynamic%204D%20content%20from%20a%20single%20still%20image%20by%0Acombining%20multi-view%20synthesis%20with%204D%20Gaussian%20Splatting%20%284D%20GS%29.%20At%20its%20core%2C%0AMVG4D%20employs%20an%20image%20matrix%20module%20that%20synthesizes%20temporally%20coherent%20and%0Aspatially%20diverse%20multi-view%20images%2C%20providing%20rich%20supervisory%20signals%20for%0Adownstream%203D%20and%204D%20reconstruction.%20These%20multi-view%20images%20are%20used%20to%0Aoptimize%20a%203D%20Gaussian%20point%20cloud%2C%20which%20is%20further%20extended%20into%20the%20temporal%0Adomain%20via%20a%20lightweight%20deformation%20network.%20Our%20method%20effectively%20enhances%0Atemporal%20consistency%2C%20geometric%20fidelity%2C%20and%20visual%20realism%2C%20addressing%20key%0Achallenges%20in%20motion%20discontinuity%20and%20background%20degradation%20that%20affect%20prior%0A4D%20GS-based%20methods.%20Extensive%20experiments%20on%20the%20Objaverse%20dataset%20demonstrate%0Athat%20MVG4D%20outperforms%20state-of-the-art%20baselines%20in%20CLIP-I%2C%20PSNR%2C%20FVD%2C%20and%0Atime%20efficiency.%20Notably%2C%20it%20reduces%20flickering%20artifacts%20and%20sharpens%0Astructural%20details%20across%20views%20and%20time%2C%20enabling%20more%20immersive%20AR/VR%0Aexperiences.%20MVG4D%20sets%20a%20new%20direction%20for%20efficient%20and%20controllable%204D%0Ageneration%20from%20minimal%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.18371v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVG4D%253A%2520Image%2520Matrix-Based%2520Multi-View%2520and%2520Motion%2520Generation%2520for%25204D%250A%2520%2520Content%2520Creation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DDongFu%2520Yin%2520and%2520Xiaotian%2520Chen%2520and%2520Fei%2520Richard%2520Yu%2520and%2520Xuanchen%2520Li%2520and%2520Xinhao%2520Zhang%26entry.1292438233%3D%2520%2520Advances%2520in%2520generative%2520modeling%2520have%2520significantly%2520enhanced%2520digital%2520content%250Acreation%252C%2520extending%2520from%25202D%2520images%2520to%2520complex%25203D%2520and%25204D%2520scenes.%2520Despite%250Asubstantial%2520progress%252C%2520producing%2520high-fidelity%2520and%2520temporally%2520consistent%2520dynamic%250A4D%2520content%2520remains%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520MVG4D%252C%2520a%2520novel%250Aframework%2520that%2520generates%2520dynamic%25204D%2520content%2520from%2520a%2520single%2520still%2520image%2520by%250Acombining%2520multi-view%2520synthesis%2520with%25204D%2520Gaussian%2520Splatting%2520%25284D%2520GS%2529.%2520At%2520its%2520core%252C%250AMVG4D%2520employs%2520an%2520image%2520matrix%2520module%2520that%2520synthesizes%2520temporally%2520coherent%2520and%250Aspatially%2520diverse%2520multi-view%2520images%252C%2520providing%2520rich%2520supervisory%2520signals%2520for%250Adownstream%25203D%2520and%25204D%2520reconstruction.%2520These%2520multi-view%2520images%2520are%2520used%2520to%250Aoptimize%2520a%25203D%2520Gaussian%2520point%2520cloud%252C%2520which%2520is%2520further%2520extended%2520into%2520the%2520temporal%250Adomain%2520via%2520a%2520lightweight%2520deformation%2520network.%2520Our%2520method%2520effectively%2520enhances%250Atemporal%2520consistency%252C%2520geometric%2520fidelity%252C%2520and%2520visual%2520realism%252C%2520addressing%2520key%250Achallenges%2520in%2520motion%2520discontinuity%2520and%2520background%2520degradation%2520that%2520affect%2520prior%250A4D%2520GS-based%2520methods.%2520Extensive%2520experiments%2520on%2520the%2520Objaverse%2520dataset%2520demonstrate%250Athat%2520MVG4D%2520outperforms%2520state-of-the-art%2520baselines%2520in%2520CLIP-I%252C%2520PSNR%252C%2520FVD%252C%2520and%250Atime%2520efficiency.%2520Notably%252C%2520it%2520reduces%2520flickering%2520artifacts%2520and%2520sharpens%250Astructural%2520details%2520across%2520views%2520and%2520time%252C%2520enabling%2520more%2520immersive%2520AR/VR%250Aexperiences.%2520MVG4D%2520sets%2520a%2520new%2520direction%2520for%2520efficient%2520and%2520controllable%25204D%250Ageneration%2520from%2520minimal%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18371v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVG4D%3A%20Image%20Matrix-Based%20Multi-View%20and%20Motion%20Generation%20for%204D%0A%20%20Content%20Creation%20from%20a%20Single%20Image&entry.906535625=DongFu%20Yin%20and%20Xiaotian%20Chen%20and%20Fei%20Richard%20Yu%20and%20Xuanchen%20Li%20and%20Xinhao%20Zhang&entry.1292438233=%20%20Advances%20in%20generative%20modeling%20have%20significantly%20enhanced%20digital%20content%0Acreation%2C%20extending%20from%202D%20images%20to%20complex%203D%20and%204D%20scenes.%20Despite%0Asubstantial%20progress%2C%20producing%20high-fidelity%20and%20temporally%20consistent%20dynamic%0A4D%20content%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20propose%20MVG4D%2C%20a%20novel%0Aframework%20that%20generates%20dynamic%204D%20content%20from%20a%20single%20still%20image%20by%0Acombining%20multi-view%20synthesis%20with%204D%20Gaussian%20Splatting%20%284D%20GS%29.%20At%20its%20core%2C%0AMVG4D%20employs%20an%20image%20matrix%20module%20that%20synthesizes%20temporally%20coherent%20and%0Aspatially%20diverse%20multi-view%20images%2C%20providing%20rich%20supervisory%20signals%20for%0Adownstream%203D%20and%204D%20reconstruction.%20These%20multi-view%20images%20are%20used%20to%0Aoptimize%20a%203D%20Gaussian%20point%20cloud%2C%20which%20is%20further%20extended%20into%20the%20temporal%0Adomain%20via%20a%20lightweight%20deformation%20network.%20Our%20method%20effectively%20enhances%0Atemporal%20consistency%2C%20geometric%20fidelity%2C%20and%20visual%20realism%2C%20addressing%20key%0Achallenges%20in%20motion%20discontinuity%20and%20background%20degradation%20that%20affect%20prior%0A4D%20GS-based%20methods.%20Extensive%20experiments%20on%20the%20Objaverse%20dataset%20demonstrate%0Athat%20MVG4D%20outperforms%20state-of-the-art%20baselines%20in%20CLIP-I%2C%20PSNR%2C%20FVD%2C%20and%0Atime%20efficiency.%20Notably%2C%20it%20reduces%20flickering%20artifacts%20and%20sharpens%0Astructural%20details%20across%20views%20and%20time%2C%20enabling%20more%20immersive%20AR/VR%0Aexperiences.%20MVG4D%20sets%20a%20new%20direction%20for%20efficient%20and%20controllable%204D%0Ageneration%20from%20minimal%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.18371v2&entry.124074799=Read"},
{"title": "Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with\n  Two-Stage Depth Diffusion", "author": "Mutian Xu and Chongjie Ye and Haolin Liu and Yushuang Wu and Jiahao Chang and Xiaoguang Han", "abstract": "  3D data simulation aims to bridge the gap between simulated and real-captured\n3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D\ndata simulation methods inject predefined physical priors but struggle to\ncapture the full complexity of real data. An optimal approach involves learning\nan implicit mapping from synthetic to realistic data in a data-driven manner,\nbut progress in this solution has met stagnation in recent studies. This work\nexplores a new solution path of data-driven 3D simulation, called\nStable-Sim2Real, based on a novel two-stage depth diffusion model. The initial\nstage finetunes Stable-Diffusion to generate the residual between the real and\nsynthetic paired depth, producing a stable but coarse depth, where some local\nregions may deviate from realistic patterns. To enhance this, both the\nsynthetic and initial output depth are fed into a second-stage diffusion, where\ndiffusion loss is adjusted to prioritize these distinct areas identified by a\n3D discriminator. We provide a new benchmark scheme to evaluate 3D data\nsimulation methods. Extensive experiments show that training the network with\nthe 3D simulated data derived from our method significantly enhances\nperformance in real-world 3D visual tasks. Moreover, the evaluation\ndemonstrates the high similarity between our 3D simulated data and\nreal-captured patterns. Project page:\nhttps://mutianxu.github.io/stable-sim2real/.\n", "link": "http://arxiv.org/abs/2507.23483v1", "date": "2025-07-31", "relevancy": 3.3773, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6799}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6799}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable-Sim2Real%3A%20Exploring%20Simulation%20of%20Real-Captured%203D%20Data%20with%0A%20%20Two-Stage%20Depth%20Diffusion&body=Title%3A%20Stable-Sim2Real%3A%20Exploring%20Simulation%20of%20Real-Captured%203D%20Data%20with%0A%20%20Two-Stage%20Depth%20Diffusion%0AAuthor%3A%20Mutian%20Xu%20and%20Chongjie%20Ye%20and%20Haolin%20Liu%20and%20Yushuang%20Wu%20and%20Jiahao%20Chang%20and%20Xiaoguang%20Han%0AAbstract%3A%20%20%203D%20data%20simulation%20aims%20to%20bridge%20the%20gap%20between%20simulated%20and%20real-captured%0A3D%20data%2C%20which%20is%20a%20fundamental%20problem%20for%20real-world%203D%20visual%20tasks.%20Most%203D%0Adata%20simulation%20methods%20inject%20predefined%20physical%20priors%20but%20struggle%20to%0Acapture%20the%20full%20complexity%20of%20real%20data.%20An%20optimal%20approach%20involves%20learning%0Aan%20implicit%20mapping%20from%20synthetic%20to%20realistic%20data%20in%20a%20data-driven%20manner%2C%0Abut%20progress%20in%20this%20solution%20has%20met%20stagnation%20in%20recent%20studies.%20This%20work%0Aexplores%20a%20new%20solution%20path%20of%20data-driven%203D%20simulation%2C%20called%0AStable-Sim2Real%2C%20based%20on%20a%20novel%20two-stage%20depth%20diffusion%20model.%20The%20initial%0Astage%20finetunes%20Stable-Diffusion%20to%20generate%20the%20residual%20between%20the%20real%20and%0Asynthetic%20paired%20depth%2C%20producing%20a%20stable%20but%20coarse%20depth%2C%20where%20some%20local%0Aregions%20may%20deviate%20from%20realistic%20patterns.%20To%20enhance%20this%2C%20both%20the%0Asynthetic%20and%20initial%20output%20depth%20are%20fed%20into%20a%20second-stage%20diffusion%2C%20where%0Adiffusion%20loss%20is%20adjusted%20to%20prioritize%20these%20distinct%20areas%20identified%20by%20a%0A3D%20discriminator.%20We%20provide%20a%20new%20benchmark%20scheme%20to%20evaluate%203D%20data%0Asimulation%20methods.%20Extensive%20experiments%20show%20that%20training%20the%20network%20with%0Athe%203D%20simulated%20data%20derived%20from%20our%20method%20significantly%20enhances%0Aperformance%20in%20real-world%203D%20visual%20tasks.%20Moreover%2C%20the%20evaluation%0Ademonstrates%20the%20high%20similarity%20between%20our%203D%20simulated%20data%20and%0Areal-captured%20patterns.%20Project%20page%3A%0Ahttps%3A//mutianxu.github.io/stable-sim2real/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable-Sim2Real%253A%2520Exploring%2520Simulation%2520of%2520Real-Captured%25203D%2520Data%2520with%250A%2520%2520Two-Stage%2520Depth%2520Diffusion%26entry.906535625%3DMutian%2520Xu%2520and%2520Chongjie%2520Ye%2520and%2520Haolin%2520Liu%2520and%2520Yushuang%2520Wu%2520and%2520Jiahao%2520Chang%2520and%2520Xiaoguang%2520Han%26entry.1292438233%3D%2520%25203D%2520data%2520simulation%2520aims%2520to%2520bridge%2520the%2520gap%2520between%2520simulated%2520and%2520real-captured%250A3D%2520data%252C%2520which%2520is%2520a%2520fundamental%2520problem%2520for%2520real-world%25203D%2520visual%2520tasks.%2520Most%25203D%250Adata%2520simulation%2520methods%2520inject%2520predefined%2520physical%2520priors%2520but%2520struggle%2520to%250Acapture%2520the%2520full%2520complexity%2520of%2520real%2520data.%2520An%2520optimal%2520approach%2520involves%2520learning%250Aan%2520implicit%2520mapping%2520from%2520synthetic%2520to%2520realistic%2520data%2520in%2520a%2520data-driven%2520manner%252C%250Abut%2520progress%2520in%2520this%2520solution%2520has%2520met%2520stagnation%2520in%2520recent%2520studies.%2520This%2520work%250Aexplores%2520a%2520new%2520solution%2520path%2520of%2520data-driven%25203D%2520simulation%252C%2520called%250AStable-Sim2Real%252C%2520based%2520on%2520a%2520novel%2520two-stage%2520depth%2520diffusion%2520model.%2520The%2520initial%250Astage%2520finetunes%2520Stable-Diffusion%2520to%2520generate%2520the%2520residual%2520between%2520the%2520real%2520and%250Asynthetic%2520paired%2520depth%252C%2520producing%2520a%2520stable%2520but%2520coarse%2520depth%252C%2520where%2520some%2520local%250Aregions%2520may%2520deviate%2520from%2520realistic%2520patterns.%2520To%2520enhance%2520this%252C%2520both%2520the%250Asynthetic%2520and%2520initial%2520output%2520depth%2520are%2520fed%2520into%2520a%2520second-stage%2520diffusion%252C%2520where%250Adiffusion%2520loss%2520is%2520adjusted%2520to%2520prioritize%2520these%2520distinct%2520areas%2520identified%2520by%2520a%250A3D%2520discriminator.%2520We%2520provide%2520a%2520new%2520benchmark%2520scheme%2520to%2520evaluate%25203D%2520data%250Asimulation%2520methods.%2520Extensive%2520experiments%2520show%2520that%2520training%2520the%2520network%2520with%250Athe%25203D%2520simulated%2520data%2520derived%2520from%2520our%2520method%2520significantly%2520enhances%250Aperformance%2520in%2520real-world%25203D%2520visual%2520tasks.%2520Moreover%252C%2520the%2520evaluation%250Ademonstrates%2520the%2520high%2520similarity%2520between%2520our%25203D%2520simulated%2520data%2520and%250Areal-captured%2520patterns.%2520Project%2520page%253A%250Ahttps%253A//mutianxu.github.io/stable-sim2real/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable-Sim2Real%3A%20Exploring%20Simulation%20of%20Real-Captured%203D%20Data%20with%0A%20%20Two-Stage%20Depth%20Diffusion&entry.906535625=Mutian%20Xu%20and%20Chongjie%20Ye%20and%20Haolin%20Liu%20and%20Yushuang%20Wu%20and%20Jiahao%20Chang%20and%20Xiaoguang%20Han&entry.1292438233=%20%203D%20data%20simulation%20aims%20to%20bridge%20the%20gap%20between%20simulated%20and%20real-captured%0A3D%20data%2C%20which%20is%20a%20fundamental%20problem%20for%20real-world%203D%20visual%20tasks.%20Most%203D%0Adata%20simulation%20methods%20inject%20predefined%20physical%20priors%20but%20struggle%20to%0Acapture%20the%20full%20complexity%20of%20real%20data.%20An%20optimal%20approach%20involves%20learning%0Aan%20implicit%20mapping%20from%20synthetic%20to%20realistic%20data%20in%20a%20data-driven%20manner%2C%0Abut%20progress%20in%20this%20solution%20has%20met%20stagnation%20in%20recent%20studies.%20This%20work%0Aexplores%20a%20new%20solution%20path%20of%20data-driven%203D%20simulation%2C%20called%0AStable-Sim2Real%2C%20based%20on%20a%20novel%20two-stage%20depth%20diffusion%20model.%20The%20initial%0Astage%20finetunes%20Stable-Diffusion%20to%20generate%20the%20residual%20between%20the%20real%20and%0Asynthetic%20paired%20depth%2C%20producing%20a%20stable%20but%20coarse%20depth%2C%20where%20some%20local%0Aregions%20may%20deviate%20from%20realistic%20patterns.%20To%20enhance%20this%2C%20both%20the%0Asynthetic%20and%20initial%20output%20depth%20are%20fed%20into%20a%20second-stage%20diffusion%2C%20where%0Adiffusion%20loss%20is%20adjusted%20to%20prioritize%20these%20distinct%20areas%20identified%20by%20a%0A3D%20discriminator.%20We%20provide%20a%20new%20benchmark%20scheme%20to%20evaluate%203D%20data%0Asimulation%20methods.%20Extensive%20experiments%20show%20that%20training%20the%20network%20with%0Athe%203D%20simulated%20data%20derived%20from%20our%20method%20significantly%20enhances%0Aperformance%20in%20real-world%203D%20visual%20tasks.%20Moreover%2C%20the%20evaluation%0Ademonstrates%20the%20high%20similarity%20between%20our%203D%20simulated%20data%20and%0Areal-captured%20patterns.%20Project%20page%3A%0Ahttps%3A//mutianxu.github.io/stable-sim2real/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23483v1&entry.124074799=Read"},
{"title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding", "author": "Ting Huang and Zeyu Zhang and Hao Tang", "abstract": "  Large vision-language models (VLMs) have made significant strides in 2D\nvisual understanding tasks, sparking interest in extending these capabilities\nto 3D scene understanding. However, current 3D VLMs often struggle with robust\nreasoning and generalization due to limitations in high-quality spatial data\nand the static nature of viewpoint assumptions. To address these challenges, we\npropose 3D-R1, a foundation model that enhances the reasoning capabilities of\n3D VLMs. Specifically, we first construct a high-quality synthetic dataset with\nCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine\nbased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.\nMoreover, we leverage RLHF policy such as GRPO in the reinforcement learning\ntraining process to enhance reasoning capabilities and introduce three reward\nfunctions: a perception reward, a semantic similarity reward and a format\nreward to maintain detection accuracy and answer semantic precision.\nFurthermore, we introduce a dynamic view selection strategy that adaptively\nchooses the most informative perspectives for 3D scene understanding. Extensive\nexperiments demonstrate that 3D-R1 delivers an average improvement of 10%\nacross various 3D scene benchmarks, highlighting its effectiveness in enhancing\nreasoning and generalization in 3D scene understanding. Code:\nhttps://github.com/AIGeeksGroup/3D-R1. Website:\nhttps://aigeeksgroup.github.io/3D-R1.\n", "link": "http://arxiv.org/abs/2507.23478v1", "date": "2025-07-31", "relevancy": 3.3692, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.7031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-R1%3A%20Enhancing%20Reasoning%20in%203D%20VLMs%20for%20Unified%20Scene%20Understanding&body=Title%3A%203D-R1%3A%20Enhancing%20Reasoning%20in%203D%20VLMs%20for%20Unified%20Scene%20Understanding%0AAuthor%3A%20Ting%20Huang%20and%20Zeyu%20Zhang%20and%20Hao%20Tang%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28VLMs%29%20have%20made%20significant%20strides%20in%202D%0Avisual%20understanding%20tasks%2C%20sparking%20interest%20in%20extending%20these%20capabilities%0Ato%203D%20scene%20understanding.%20However%2C%20current%203D%20VLMs%20often%20struggle%20with%20robust%0Areasoning%20and%20generalization%20due%20to%20limitations%20in%20high-quality%20spatial%20data%0Aand%20the%20static%20nature%20of%20viewpoint%20assumptions.%20To%20address%20these%20challenges%2C%20we%0Apropose%203D-R1%2C%20a%20foundation%20model%20that%20enhances%20the%20reasoning%20capabilities%20of%0A3D%20VLMs.%20Specifically%2C%20we%20first%20construct%20a%20high-quality%20synthetic%20dataset%20with%0ACoT%2C%20named%20Scene-30K%2C%20leveraging%20existing%203D-VL%20datasets%20and%20a%20data%20engine%0Abased%20on%20Gemini%202.5%20Pro.%20It%20serves%20as%20cold-start%20initialization%20data%20for%203D-R1.%0AMoreover%2C%20we%20leverage%20RLHF%20policy%20such%20as%20GRPO%20in%20the%20reinforcement%20learning%0Atraining%20process%20to%20enhance%20reasoning%20capabilities%20and%20introduce%20three%20reward%0Afunctions%3A%20a%20perception%20reward%2C%20a%20semantic%20similarity%20reward%20and%20a%20format%0Areward%20to%20maintain%20detection%20accuracy%20and%20answer%20semantic%20precision.%0AFurthermore%2C%20we%20introduce%20a%20dynamic%20view%20selection%20strategy%20that%20adaptively%0Achooses%20the%20most%20informative%20perspectives%20for%203D%20scene%20understanding.%20Extensive%0Aexperiments%20demonstrate%20that%203D-R1%20delivers%20an%20average%20improvement%20of%2010%25%0Aacross%20various%203D%20scene%20benchmarks%2C%20highlighting%20its%20effectiveness%20in%20enhancing%0Areasoning%20and%20generalization%20in%203D%20scene%20understanding.%20Code%3A%0Ahttps%3A//github.com/AIGeeksGroup/3D-R1.%20Website%3A%0Ahttps%3A//aigeeksgroup.github.io/3D-R1.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-R1%253A%2520Enhancing%2520Reasoning%2520in%25203D%2520VLMs%2520for%2520Unified%2520Scene%2520Understanding%26entry.906535625%3DTing%2520Huang%2520and%2520Zeyu%2520Zhang%2520and%2520Hao%2520Tang%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520made%2520significant%2520strides%2520in%25202D%250Avisual%2520understanding%2520tasks%252C%2520sparking%2520interest%2520in%2520extending%2520these%2520capabilities%250Ato%25203D%2520scene%2520understanding.%2520However%252C%2520current%25203D%2520VLMs%2520often%2520struggle%2520with%2520robust%250Areasoning%2520and%2520generalization%2520due%2520to%2520limitations%2520in%2520high-quality%2520spatial%2520data%250Aand%2520the%2520static%2520nature%2520of%2520viewpoint%2520assumptions.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%25203D-R1%252C%2520a%2520foundation%2520model%2520that%2520enhances%2520the%2520reasoning%2520capabilities%2520of%250A3D%2520VLMs.%2520Specifically%252C%2520we%2520first%2520construct%2520a%2520high-quality%2520synthetic%2520dataset%2520with%250ACoT%252C%2520named%2520Scene-30K%252C%2520leveraging%2520existing%25203D-VL%2520datasets%2520and%2520a%2520data%2520engine%250Abased%2520on%2520Gemini%25202.5%2520Pro.%2520It%2520serves%2520as%2520cold-start%2520initialization%2520data%2520for%25203D-R1.%250AMoreover%252C%2520we%2520leverage%2520RLHF%2520policy%2520such%2520as%2520GRPO%2520in%2520the%2520reinforcement%2520learning%250Atraining%2520process%2520to%2520enhance%2520reasoning%2520capabilities%2520and%2520introduce%2520three%2520reward%250Afunctions%253A%2520a%2520perception%2520reward%252C%2520a%2520semantic%2520similarity%2520reward%2520and%2520a%2520format%250Areward%2520to%2520maintain%2520detection%2520accuracy%2520and%2520answer%2520semantic%2520precision.%250AFurthermore%252C%2520we%2520introduce%2520a%2520dynamic%2520view%2520selection%2520strategy%2520that%2520adaptively%250Achooses%2520the%2520most%2520informative%2520perspectives%2520for%25203D%2520scene%2520understanding.%2520Extensive%250Aexperiments%2520demonstrate%2520that%25203D-R1%2520delivers%2520an%2520average%2520improvement%2520of%252010%2525%250Aacross%2520various%25203D%2520scene%2520benchmarks%252C%2520highlighting%2520its%2520effectiveness%2520in%2520enhancing%250Areasoning%2520and%2520generalization%2520in%25203D%2520scene%2520understanding.%2520Code%253A%250Ahttps%253A//github.com/AIGeeksGroup/3D-R1.%2520Website%253A%250Ahttps%253A//aigeeksgroup.github.io/3D-R1.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-R1%3A%20Enhancing%20Reasoning%20in%203D%20VLMs%20for%20Unified%20Scene%20Understanding&entry.906535625=Ting%20Huang%20and%20Zeyu%20Zhang%20and%20Hao%20Tang&entry.1292438233=%20%20Large%20vision-language%20models%20%28VLMs%29%20have%20made%20significant%20strides%20in%202D%0Avisual%20understanding%20tasks%2C%20sparking%20interest%20in%20extending%20these%20capabilities%0Ato%203D%20scene%20understanding.%20However%2C%20current%203D%20VLMs%20often%20struggle%20with%20robust%0Areasoning%20and%20generalization%20due%20to%20limitations%20in%20high-quality%20spatial%20data%0Aand%20the%20static%20nature%20of%20viewpoint%20assumptions.%20To%20address%20these%20challenges%2C%20we%0Apropose%203D-R1%2C%20a%20foundation%20model%20that%20enhances%20the%20reasoning%20capabilities%20of%0A3D%20VLMs.%20Specifically%2C%20we%20first%20construct%20a%20high-quality%20synthetic%20dataset%20with%0ACoT%2C%20named%20Scene-30K%2C%20leveraging%20existing%203D-VL%20datasets%20and%20a%20data%20engine%0Abased%20on%20Gemini%202.5%20Pro.%20It%20serves%20as%20cold-start%20initialization%20data%20for%203D-R1.%0AMoreover%2C%20we%20leverage%20RLHF%20policy%20such%20as%20GRPO%20in%20the%20reinforcement%20learning%0Atraining%20process%20to%20enhance%20reasoning%20capabilities%20and%20introduce%20three%20reward%0Afunctions%3A%20a%20perception%20reward%2C%20a%20semantic%20similarity%20reward%20and%20a%20format%0Areward%20to%20maintain%20detection%20accuracy%20and%20answer%20semantic%20precision.%0AFurthermore%2C%20we%20introduce%20a%20dynamic%20view%20selection%20strategy%20that%20adaptively%0Achooses%20the%20most%20informative%20perspectives%20for%203D%20scene%20understanding.%20Extensive%0Aexperiments%20demonstrate%20that%203D-R1%20delivers%20an%20average%20improvement%20of%2010%25%0Aacross%20various%203D%20scene%20benchmarks%2C%20highlighting%20its%20effectiveness%20in%20enhancing%0Areasoning%20and%20generalization%20in%203D%20scene%20understanding.%20Code%3A%0Ahttps%3A//github.com/AIGeeksGroup/3D-R1.%20Website%3A%0Ahttps%3A//aigeeksgroup.github.io/3D-R1.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23478v1&entry.124074799=Read"},
{"title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis", "author": "Bowen Zhang and Sicheng Xu and Chuxin Wang and Jiaolong Yang and Feng Zhao and Dong Chen and Baining Guo", "abstract": "  In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.\n", "link": "http://arxiv.org/abs/2507.23785v1", "date": "2025-07-31", "relevancy": 3.3481, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6972}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6653}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Variation%20Field%20Diffusion%20for%20High-fidelity%20Video-to-4D%0A%20%20Synthesis&body=Title%3A%20Gaussian%20Variation%20Field%20Diffusion%20for%20High-fidelity%20Video-to-4D%0A%20%20Synthesis%0AAuthor%3A%20Bowen%20Zhang%20and%20Sicheng%20Xu%20and%20Chuxin%20Wang%20and%20Jiaolong%20Yang%20and%20Feng%20Zhao%20and%20Dong%20Chen%20and%20Baining%20Guo%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%20for%20video-to-4D%20generation%20that%0Acreates%20high-quality%20dynamic%203D%20content%20from%20single%20video%20inputs.%20Direct%204D%0Adiffusion%20modeling%20is%20extremely%20challenging%20due%20to%20costly%20data%20construction%20and%0Athe%20high-dimensional%20nature%20of%20jointly%20representing%203D%20shape%2C%20appearance%2C%20and%0Amotion.%20We%20address%20these%20challenges%20by%20introducing%20a%20Direct%204DMesh-to-GS%0AVariation%20Field%20VAE%20that%20directly%20encodes%20canonical%20Gaussian%20Splats%20%28GS%29%20and%0Atheir%20temporal%20variations%20from%203D%20animation%20data%20without%20per-instance%20fitting%2C%0Aand%20compresses%20high-dimensional%20animations%20into%20a%20compact%20latent%20space.%0ABuilding%20upon%20this%20efficient%20representation%2C%20we%20train%20a%20Gaussian%20Variation%0AField%20diffusion%20model%20with%20temporal-aware%20Diffusion%20Transformer%20conditioned%20on%0Ainput%20videos%20and%20canonical%20GS.%20Trained%20on%20carefully-curated%20animatable%203D%0Aobjects%20from%20the%20Objaverse%20dataset%2C%20our%20model%20demonstrates%20superior%20generation%0Aquality%20compared%20to%20existing%20methods.%20It%20also%20exhibits%20remarkable%0Ageneralization%20to%20in-the-wild%20video%20inputs%20despite%20being%20trained%20exclusively%20on%0Asynthetic%20data%2C%20paving%20the%20way%20for%20generating%20high-quality%20animated%203D%20content.%0AProject%20page%3A%20https%3A//gvfdiffusion.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Variation%2520Field%2520Diffusion%2520for%2520High-fidelity%2520Video-to-4D%250A%2520%2520Synthesis%26entry.906535625%3DBowen%2520Zhang%2520and%2520Sicheng%2520Xu%2520and%2520Chuxin%2520Wang%2520and%2520Jiaolong%2520Yang%2520and%2520Feng%2520Zhao%2520and%2520Dong%2520Chen%2520and%2520Baining%2520Guo%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520framework%2520for%2520video-to-4D%2520generation%2520that%250Acreates%2520high-quality%2520dynamic%25203D%2520content%2520from%2520single%2520video%2520inputs.%2520Direct%25204D%250Adiffusion%2520modeling%2520is%2520extremely%2520challenging%2520due%2520to%2520costly%2520data%2520construction%2520and%250Athe%2520high-dimensional%2520nature%2520of%2520jointly%2520representing%25203D%2520shape%252C%2520appearance%252C%2520and%250Amotion.%2520We%2520address%2520these%2520challenges%2520by%2520introducing%2520a%2520Direct%25204DMesh-to-GS%250AVariation%2520Field%2520VAE%2520that%2520directly%2520encodes%2520canonical%2520Gaussian%2520Splats%2520%2528GS%2529%2520and%250Atheir%2520temporal%2520variations%2520from%25203D%2520animation%2520data%2520without%2520per-instance%2520fitting%252C%250Aand%2520compresses%2520high-dimensional%2520animations%2520into%2520a%2520compact%2520latent%2520space.%250ABuilding%2520upon%2520this%2520efficient%2520representation%252C%2520we%2520train%2520a%2520Gaussian%2520Variation%250AField%2520diffusion%2520model%2520with%2520temporal-aware%2520Diffusion%2520Transformer%2520conditioned%2520on%250Ainput%2520videos%2520and%2520canonical%2520GS.%2520Trained%2520on%2520carefully-curated%2520animatable%25203D%250Aobjects%2520from%2520the%2520Objaverse%2520dataset%252C%2520our%2520model%2520demonstrates%2520superior%2520generation%250Aquality%2520compared%2520to%2520existing%2520methods.%2520It%2520also%2520exhibits%2520remarkable%250Ageneralization%2520to%2520in-the-wild%2520video%2520inputs%2520despite%2520being%2520trained%2520exclusively%2520on%250Asynthetic%2520data%252C%2520paving%2520the%2520way%2520for%2520generating%2520high-quality%2520animated%25203D%2520content.%250AProject%2520page%253A%2520https%253A//gvfdiffusion.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Variation%20Field%20Diffusion%20for%20High-fidelity%20Video-to-4D%0A%20%20Synthesis&entry.906535625=Bowen%20Zhang%20and%20Sicheng%20Xu%20and%20Chuxin%20Wang%20and%20Jiaolong%20Yang%20and%20Feng%20Zhao%20and%20Dong%20Chen%20and%20Baining%20Guo&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%20for%20video-to-4D%20generation%20that%0Acreates%20high-quality%20dynamic%203D%20content%20from%20single%20video%20inputs.%20Direct%204D%0Adiffusion%20modeling%20is%20extremely%20challenging%20due%20to%20costly%20data%20construction%20and%0Athe%20high-dimensional%20nature%20of%20jointly%20representing%203D%20shape%2C%20appearance%2C%20and%0Amotion.%20We%20address%20these%20challenges%20by%20introducing%20a%20Direct%204DMesh-to-GS%0AVariation%20Field%20VAE%20that%20directly%20encodes%20canonical%20Gaussian%20Splats%20%28GS%29%20and%0Atheir%20temporal%20variations%20from%203D%20animation%20data%20without%20per-instance%20fitting%2C%0Aand%20compresses%20high-dimensional%20animations%20into%20a%20compact%20latent%20space.%0ABuilding%20upon%20this%20efficient%20representation%2C%20we%20train%20a%20Gaussian%20Variation%0AField%20diffusion%20model%20with%20temporal-aware%20Diffusion%20Transformer%20conditioned%20on%0Ainput%20videos%20and%20canonical%20GS.%20Trained%20on%20carefully-curated%20animatable%203D%0Aobjects%20from%20the%20Objaverse%20dataset%2C%20our%20model%20demonstrates%20superior%20generation%0Aquality%20compared%20to%20existing%20methods.%20It%20also%20exhibits%20remarkable%0Ageneralization%20to%20in-the-wild%20video%20inputs%20despite%20being%20trained%20exclusively%20on%0Asynthetic%20data%2C%20paving%20the%20way%20for%20generating%20high-quality%20animated%203D%20content.%0AProject%20page%3A%20https%3A//gvfdiffusion.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23785v1&entry.124074799=Read"},
{"title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "author": "Xiaohan Li and Ziren Gong and Fabio Tosi and Matteo Poggi and Stefano Mattoccia and Dong Liu and Jun Wu", "abstract": "  3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM\napplications due to its fast rendering and high-fidelity representation.\nHowever, existing 3DGS-SLAM systems have predominantly focused on indoor\nenvironments and relied on active depth sensors, leaving a gap for large-scale\noutdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian\nSplatting SLAM system designed for outdoor scenarios. Our approach uses only\nRGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages\ndepth estimates from pre-trained deep stereo networks to guide 3D Gaussian\noptimization with a multi-loss strategy enhancing both geometric consistency\nand visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM\nachieves superior tracking accuracy and mapping performance compared to other\n3DGS-based solutions in complex outdoor environments.\n", "link": "http://arxiv.org/abs/2507.23677v1", "date": "2025-07-31", "relevancy": 3.3219, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7538}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6463}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stereo%203D%20Gaussian%20Splatting%20SLAM%20for%20Outdoor%20Urban%20Scenes&body=Title%3A%20Stereo%203D%20Gaussian%20Splatting%20SLAM%20for%20Outdoor%20Urban%20Scenes%0AAuthor%3A%20Xiaohan%20Li%20and%20Ziren%20Gong%20and%20Fabio%20Tosi%20and%20Matteo%20Poggi%20and%20Stefano%20Mattoccia%20and%20Dong%20Liu%20and%20Jun%20Wu%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20gained%20popularity%20in%20SLAM%0Aapplications%20due%20to%20its%20fast%20rendering%20and%20high-fidelity%20representation.%0AHowever%2C%20existing%203DGS-SLAM%20systems%20have%20predominantly%20focused%20on%20indoor%0Aenvironments%20and%20relied%20on%20active%20depth%20sensors%2C%20leaving%20a%20gap%20for%20large-scale%0Aoutdoor%20applications.%20We%20present%20BGS-SLAM%2C%20the%20first%20binocular%203D%20Gaussian%0ASplatting%20SLAM%20system%20designed%20for%20outdoor%20scenarios.%20Our%20approach%20uses%20only%0ARGB%20stereo%20pairs%20without%20requiring%20LiDAR%20or%20active%20sensors.%20BGS-SLAM%20leverages%0Adepth%20estimates%20from%20pre-trained%20deep%20stereo%20networks%20to%20guide%203D%20Gaussian%0Aoptimization%20with%20a%20multi-loss%20strategy%20enhancing%20both%20geometric%20consistency%0Aand%20visual%20quality.%20Experiments%20on%20multiple%20datasets%20demonstrate%20that%20BGS-SLAM%0Aachieves%20superior%20tracking%20accuracy%20and%20mapping%20performance%20compared%20to%20other%0A3DGS-based%20solutions%20in%20complex%20outdoor%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23677v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereo%25203D%2520Gaussian%2520Splatting%2520SLAM%2520for%2520Outdoor%2520Urban%2520Scenes%26entry.906535625%3DXiaohan%2520Li%2520and%2520Ziren%2520Gong%2520and%2520Fabio%2520Tosi%2520and%2520Matteo%2520Poggi%2520and%2520Stefano%2520Mattoccia%2520and%2520Dong%2520Liu%2520and%2520Jun%2520Wu%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520gained%2520popularity%2520in%2520SLAM%250Aapplications%2520due%2520to%2520its%2520fast%2520rendering%2520and%2520high-fidelity%2520representation.%250AHowever%252C%2520existing%25203DGS-SLAM%2520systems%2520have%2520predominantly%2520focused%2520on%2520indoor%250Aenvironments%2520and%2520relied%2520on%2520active%2520depth%2520sensors%252C%2520leaving%2520a%2520gap%2520for%2520large-scale%250Aoutdoor%2520applications.%2520We%2520present%2520BGS-SLAM%252C%2520the%2520first%2520binocular%25203D%2520Gaussian%250ASplatting%2520SLAM%2520system%2520designed%2520for%2520outdoor%2520scenarios.%2520Our%2520approach%2520uses%2520only%250ARGB%2520stereo%2520pairs%2520without%2520requiring%2520LiDAR%2520or%2520active%2520sensors.%2520BGS-SLAM%2520leverages%250Adepth%2520estimates%2520from%2520pre-trained%2520deep%2520stereo%2520networks%2520to%2520guide%25203D%2520Gaussian%250Aoptimization%2520with%2520a%2520multi-loss%2520strategy%2520enhancing%2520both%2520geometric%2520consistency%250Aand%2520visual%2520quality.%2520Experiments%2520on%2520multiple%2520datasets%2520demonstrate%2520that%2520BGS-SLAM%250Aachieves%2520superior%2520tracking%2520accuracy%2520and%2520mapping%2520performance%2520compared%2520to%2520other%250A3DGS-based%2520solutions%2520in%2520complex%2520outdoor%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23677v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stereo%203D%20Gaussian%20Splatting%20SLAM%20for%20Outdoor%20Urban%20Scenes&entry.906535625=Xiaohan%20Li%20and%20Ziren%20Gong%20and%20Fabio%20Tosi%20and%20Matteo%20Poggi%20and%20Stefano%20Mattoccia%20and%20Dong%20Liu%20and%20Jun%20Wu&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20gained%20popularity%20in%20SLAM%0Aapplications%20due%20to%20its%20fast%20rendering%20and%20high-fidelity%20representation.%0AHowever%2C%20existing%203DGS-SLAM%20systems%20have%20predominantly%20focused%20on%20indoor%0Aenvironments%20and%20relied%20on%20active%20depth%20sensors%2C%20leaving%20a%20gap%20for%20large-scale%0Aoutdoor%20applications.%20We%20present%20BGS-SLAM%2C%20the%20first%20binocular%203D%20Gaussian%0ASplatting%20SLAM%20system%20designed%20for%20outdoor%20scenarios.%20Our%20approach%20uses%20only%0ARGB%20stereo%20pairs%20without%20requiring%20LiDAR%20or%20active%20sensors.%20BGS-SLAM%20leverages%0Adepth%20estimates%20from%20pre-trained%20deep%20stereo%20networks%20to%20guide%203D%20Gaussian%0Aoptimization%20with%20a%20multi-loss%20strategy%20enhancing%20both%20geometric%20consistency%0Aand%20visual%20quality.%20Experiments%20on%20multiple%20datasets%20demonstrate%20that%20BGS-SLAM%0Aachieves%20superior%20tracking%20accuracy%20and%20mapping%20performance%20compared%20to%20other%0A3DGS-based%20solutions%20in%20complex%20outdoor%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23677v1&entry.124074799=Read"},
{"title": "Gaussian Splatting Feature Fields for Privacy-Preserving Visual\n  Localization", "author": "Maxime Pietrantoni and Gabriela Csurka and Torsten Sattler", "abstract": "  Visual localization is the task of estimating a camera pose in a known\nenvironment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based\nrepresentations for accurate and privacy-preserving visual localization. We\npropose Gaussian Splatting Feature Fields (GSFFs), a scene representation for\nvisual localization that combines an explicit geometry model (3DGS) with an\nimplicit feature field. We leverage the dense geometric information and\ndifferentiable rasterization algorithm from 3DGS to learn robust feature\nrepresentations grounded in 3D. In particular, we align a 3D scale-aware\nfeature field and a 2D feature encoder in a common embedding space through a\ncontrastive framework. Using a 3D structure-informed clustering procedure, we\nfurther regularize the representation learning and seamlessly convert the\nfeatures to segmentations, which can be used for privacy-preserving visual\nlocalization. Pose refinement, which involves aligning either feature maps or\nsegmentations from a query image with those rendered from the GSFFs scene\nrepresentation, is used to achieve localization. The resulting privacy- and\nnon-privacy-preserving localization pipelines, evaluated on multiple real-world\ndatasets, show state-of-the-art performances.\n", "link": "http://arxiv.org/abs/2507.23569v1", "date": "2025-07-31", "relevancy": 3.3127, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6938}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6761}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Splatting%20Feature%20Fields%20for%20Privacy-Preserving%20Visual%0A%20%20Localization&body=Title%3A%20Gaussian%20Splatting%20Feature%20Fields%20for%20Privacy-Preserving%20Visual%0A%20%20Localization%0AAuthor%3A%20Maxime%20Pietrantoni%20and%20Gabriela%20Csurka%20and%20Torsten%20Sattler%0AAbstract%3A%20%20%20Visual%20localization%20is%20the%20task%20of%20estimating%20a%20camera%20pose%20in%20a%20known%0Aenvironment.%20In%20this%20paper%2C%20we%20utilize%203D%20Gaussian%20Splatting%20%283DGS%29-based%0Arepresentations%20for%20accurate%20and%20privacy-preserving%20visual%20localization.%20We%0Apropose%20Gaussian%20Splatting%20Feature%20Fields%20%28GSFFs%29%2C%20a%20scene%20representation%20for%0Avisual%20localization%20that%20combines%20an%20explicit%20geometry%20model%20%283DGS%29%20with%20an%0Aimplicit%20feature%20field.%20We%20leverage%20the%20dense%20geometric%20information%20and%0Adifferentiable%20rasterization%20algorithm%20from%203DGS%20to%20learn%20robust%20feature%0Arepresentations%20grounded%20in%203D.%20In%20particular%2C%20we%20align%20a%203D%20scale-aware%0Afeature%20field%20and%20a%202D%20feature%20encoder%20in%20a%20common%20embedding%20space%20through%20a%0Acontrastive%20framework.%20Using%20a%203D%20structure-informed%20clustering%20procedure%2C%20we%0Afurther%20regularize%20the%20representation%20learning%20and%20seamlessly%20convert%20the%0Afeatures%20to%20segmentations%2C%20which%20can%20be%20used%20for%20privacy-preserving%20visual%0Alocalization.%20Pose%20refinement%2C%20which%20involves%20aligning%20either%20feature%20maps%20or%0Asegmentations%20from%20a%20query%20image%20with%20those%20rendered%20from%20the%20GSFFs%20scene%0Arepresentation%2C%20is%20used%20to%20achieve%20localization.%20The%20resulting%20privacy-%20and%0Anon-privacy-preserving%20localization%20pipelines%2C%20evaluated%20on%20multiple%20real-world%0Adatasets%2C%20show%20state-of-the-art%20performances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Splatting%2520Feature%2520Fields%2520for%2520Privacy-Preserving%2520Visual%250A%2520%2520Localization%26entry.906535625%3DMaxime%2520Pietrantoni%2520and%2520Gabriela%2520Csurka%2520and%2520Torsten%2520Sattler%26entry.1292438233%3D%2520%2520Visual%2520localization%2520is%2520the%2520task%2520of%2520estimating%2520a%2520camera%2520pose%2520in%2520a%2520known%250Aenvironment.%2520In%2520this%2520paper%252C%2520we%2520utilize%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529-based%250Arepresentations%2520for%2520accurate%2520and%2520privacy-preserving%2520visual%2520localization.%2520We%250Apropose%2520Gaussian%2520Splatting%2520Feature%2520Fields%2520%2528GSFFs%2529%252C%2520a%2520scene%2520representation%2520for%250Avisual%2520localization%2520that%2520combines%2520an%2520explicit%2520geometry%2520model%2520%25283DGS%2529%2520with%2520an%250Aimplicit%2520feature%2520field.%2520We%2520leverage%2520the%2520dense%2520geometric%2520information%2520and%250Adifferentiable%2520rasterization%2520algorithm%2520from%25203DGS%2520to%2520learn%2520robust%2520feature%250Arepresentations%2520grounded%2520in%25203D.%2520In%2520particular%252C%2520we%2520align%2520a%25203D%2520scale-aware%250Afeature%2520field%2520and%2520a%25202D%2520feature%2520encoder%2520in%2520a%2520common%2520embedding%2520space%2520through%2520a%250Acontrastive%2520framework.%2520Using%2520a%25203D%2520structure-informed%2520clustering%2520procedure%252C%2520we%250Afurther%2520regularize%2520the%2520representation%2520learning%2520and%2520seamlessly%2520convert%2520the%250Afeatures%2520to%2520segmentations%252C%2520which%2520can%2520be%2520used%2520for%2520privacy-preserving%2520visual%250Alocalization.%2520Pose%2520refinement%252C%2520which%2520involves%2520aligning%2520either%2520feature%2520maps%2520or%250Asegmentations%2520from%2520a%2520query%2520image%2520with%2520those%2520rendered%2520from%2520the%2520GSFFs%2520scene%250Arepresentation%252C%2520is%2520used%2520to%2520achieve%2520localization.%2520The%2520resulting%2520privacy-%2520and%250Anon-privacy-preserving%2520localization%2520pipelines%252C%2520evaluated%2520on%2520multiple%2520real-world%250Adatasets%252C%2520show%2520state-of-the-art%2520performances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Splatting%20Feature%20Fields%20for%20Privacy-Preserving%20Visual%0A%20%20Localization&entry.906535625=Maxime%20Pietrantoni%20and%20Gabriela%20Csurka%20and%20Torsten%20Sattler&entry.1292438233=%20%20Visual%20localization%20is%20the%20task%20of%20estimating%20a%20camera%20pose%20in%20a%20known%0Aenvironment.%20In%20this%20paper%2C%20we%20utilize%203D%20Gaussian%20Splatting%20%283DGS%29-based%0Arepresentations%20for%20accurate%20and%20privacy-preserving%20visual%20localization.%20We%0Apropose%20Gaussian%20Splatting%20Feature%20Fields%20%28GSFFs%29%2C%20a%20scene%20representation%20for%0Avisual%20localization%20that%20combines%20an%20explicit%20geometry%20model%20%283DGS%29%20with%20an%0Aimplicit%20feature%20field.%20We%20leverage%20the%20dense%20geometric%20information%20and%0Adifferentiable%20rasterization%20algorithm%20from%203DGS%20to%20learn%20robust%20feature%0Arepresentations%20grounded%20in%203D.%20In%20particular%2C%20we%20align%20a%203D%20scale-aware%0Afeature%20field%20and%20a%202D%20feature%20encoder%20in%20a%20common%20embedding%20space%20through%20a%0Acontrastive%20framework.%20Using%20a%203D%20structure-informed%20clustering%20procedure%2C%20we%0Afurther%20regularize%20the%20representation%20learning%20and%20seamlessly%20convert%20the%0Afeatures%20to%20segmentations%2C%20which%20can%20be%20used%20for%20privacy-preserving%20visual%0Alocalization.%20Pose%20refinement%2C%20which%20involves%20aligning%20either%20feature%20maps%20or%0Asegmentations%20from%20a%20query%20image%20with%20those%20rendered%20from%20the%20GSFFs%20scene%0Arepresentation%2C%20is%20used%20to%20achieve%20localization.%20The%20resulting%20privacy-%20and%0Anon-privacy-preserving%20localization%20pipelines%2C%20evaluated%20on%20multiple%20real-world%0Adatasets%2C%20show%20state-of-the-art%20performances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23569v1&entry.124074799=Read"},
{"title": "I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian\n  Splatting for Autonomous Driving Data Generation", "author": "Jialei Chen and Wuhao Xu and Sipeng He and Baoru Huang and Dongchun Ren", "abstract": "  Vast and high-quality data are essential for end-to-end autonomous driving\nsystems. However, current driving data is mainly collected by vehicles, which\nis expensive and inefficient. A potential solution lies in synthesizing data\nfrom real-world images. Recent advancements in 3D reconstruction demonstrate\nphotorealistic novel view synthesis, highlighting the potential of generating\ndriving data from images captured on the road. This paper introduces a novel\nmethod, I2V-GS, to transfer the Infrastructure view To the Vehicle view with\nGaussian Splatting. Reconstruction from sparse infrastructure viewpoints and\nrendering under large view transformations is a challenging problem. We adopt\nthe adaptive depth warp to generate dense training views. To further expand the\nrange of views, we employ a cascade strategy to inpaint warped images, which\nalso ensures inpainting content is consistent across views. To further ensure\nthe reliability of the diffusion model, we utilize the cross-view information\nto perform a confidenceguided optimization. Moreover, we introduce RoadSight, a\nmulti-modality, multi-view dataset from real scenarios in infrastructure views.\nTo our knowledge, I2V-GS is the first framework to generate autonomous driving\ndatasets with infrastructure-vehicle view transformation. Experimental results\ndemonstrate that I2V-GS significantly improves synthesis quality under vehicle\nview, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%,\n34.2%, and 14.9%, respectively.\n", "link": "http://arxiv.org/abs/2507.23683v1", "date": "2025-07-31", "relevancy": 3.2415, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6696}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6566}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I2V-GS%3A%20Infrastructure-to-Vehicle%20View%20Transformation%20with%20Gaussian%0A%20%20Splatting%20for%20Autonomous%20Driving%20Data%20Generation&body=Title%3A%20I2V-GS%3A%20Infrastructure-to-Vehicle%20View%20Transformation%20with%20Gaussian%0A%20%20Splatting%20for%20Autonomous%20Driving%20Data%20Generation%0AAuthor%3A%20Jialei%20Chen%20and%20Wuhao%20Xu%20and%20Sipeng%20He%20and%20Baoru%20Huang%20and%20Dongchun%20Ren%0AAbstract%3A%20%20%20Vast%20and%20high-quality%20data%20are%20essential%20for%20end-to-end%20autonomous%20driving%0Asystems.%20However%2C%20current%20driving%20data%20is%20mainly%20collected%20by%20vehicles%2C%20which%0Ais%20expensive%20and%20inefficient.%20A%20potential%20solution%20lies%20in%20synthesizing%20data%0Afrom%20real-world%20images.%20Recent%20advancements%20in%203D%20reconstruction%20demonstrate%0Aphotorealistic%20novel%20view%20synthesis%2C%20highlighting%20the%20potential%20of%20generating%0Adriving%20data%20from%20images%20captured%20on%20the%20road.%20This%20paper%20introduces%20a%20novel%0Amethod%2C%20I2V-GS%2C%20to%20transfer%20the%20Infrastructure%20view%20To%20the%20Vehicle%20view%20with%0AGaussian%20Splatting.%20Reconstruction%20from%20sparse%20infrastructure%20viewpoints%20and%0Arendering%20under%20large%20view%20transformations%20is%20a%20challenging%20problem.%20We%20adopt%0Athe%20adaptive%20depth%20warp%20to%20generate%20dense%20training%20views.%20To%20further%20expand%20the%0Arange%20of%20views%2C%20we%20employ%20a%20cascade%20strategy%20to%20inpaint%20warped%20images%2C%20which%0Aalso%20ensures%20inpainting%20content%20is%20consistent%20across%20views.%20To%20further%20ensure%0Athe%20reliability%20of%20the%20diffusion%20model%2C%20we%20utilize%20the%20cross-view%20information%0Ato%20perform%20a%20confidenceguided%20optimization.%20Moreover%2C%20we%20introduce%20RoadSight%2C%20a%0Amulti-modality%2C%20multi-view%20dataset%20from%20real%20scenarios%20in%20infrastructure%20views.%0ATo%20our%20knowledge%2C%20I2V-GS%20is%20the%20first%20framework%20to%20generate%20autonomous%20driving%0Adatasets%20with%20infrastructure-vehicle%20view%20transformation.%20Experimental%20results%0Ademonstrate%20that%20I2V-GS%20significantly%20improves%20synthesis%20quality%20under%20vehicle%0Aview%2C%20outperforming%20StreetGaussian%20in%20NTA-Iou%2C%20NTL-Iou%2C%20and%20FID%20by%2045.7%25%2C%0A34.2%25%2C%20and%2014.9%25%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI2V-GS%253A%2520Infrastructure-to-Vehicle%2520View%2520Transformation%2520with%2520Gaussian%250A%2520%2520Splatting%2520for%2520Autonomous%2520Driving%2520Data%2520Generation%26entry.906535625%3DJialei%2520Chen%2520and%2520Wuhao%2520Xu%2520and%2520Sipeng%2520He%2520and%2520Baoru%2520Huang%2520and%2520Dongchun%2520Ren%26entry.1292438233%3D%2520%2520Vast%2520and%2520high-quality%2520data%2520are%2520essential%2520for%2520end-to-end%2520autonomous%2520driving%250Asystems.%2520However%252C%2520current%2520driving%2520data%2520is%2520mainly%2520collected%2520by%2520vehicles%252C%2520which%250Ais%2520expensive%2520and%2520inefficient.%2520A%2520potential%2520solution%2520lies%2520in%2520synthesizing%2520data%250Afrom%2520real-world%2520images.%2520Recent%2520advancements%2520in%25203D%2520reconstruction%2520demonstrate%250Aphotorealistic%2520novel%2520view%2520synthesis%252C%2520highlighting%2520the%2520potential%2520of%2520generating%250Adriving%2520data%2520from%2520images%2520captured%2520on%2520the%2520road.%2520This%2520paper%2520introduces%2520a%2520novel%250Amethod%252C%2520I2V-GS%252C%2520to%2520transfer%2520the%2520Infrastructure%2520view%2520To%2520the%2520Vehicle%2520view%2520with%250AGaussian%2520Splatting.%2520Reconstruction%2520from%2520sparse%2520infrastructure%2520viewpoints%2520and%250Arendering%2520under%2520large%2520view%2520transformations%2520is%2520a%2520challenging%2520problem.%2520We%2520adopt%250Athe%2520adaptive%2520depth%2520warp%2520to%2520generate%2520dense%2520training%2520views.%2520To%2520further%2520expand%2520the%250Arange%2520of%2520views%252C%2520we%2520employ%2520a%2520cascade%2520strategy%2520to%2520inpaint%2520warped%2520images%252C%2520which%250Aalso%2520ensures%2520inpainting%2520content%2520is%2520consistent%2520across%2520views.%2520To%2520further%2520ensure%250Athe%2520reliability%2520of%2520the%2520diffusion%2520model%252C%2520we%2520utilize%2520the%2520cross-view%2520information%250Ato%2520perform%2520a%2520confidenceguided%2520optimization.%2520Moreover%252C%2520we%2520introduce%2520RoadSight%252C%2520a%250Amulti-modality%252C%2520multi-view%2520dataset%2520from%2520real%2520scenarios%2520in%2520infrastructure%2520views.%250ATo%2520our%2520knowledge%252C%2520I2V-GS%2520is%2520the%2520first%2520framework%2520to%2520generate%2520autonomous%2520driving%250Adatasets%2520with%2520infrastructure-vehicle%2520view%2520transformation.%2520Experimental%2520results%250Ademonstrate%2520that%2520I2V-GS%2520significantly%2520improves%2520synthesis%2520quality%2520under%2520vehicle%250Aview%252C%2520outperforming%2520StreetGaussian%2520in%2520NTA-Iou%252C%2520NTL-Iou%252C%2520and%2520FID%2520by%252045.7%2525%252C%250A34.2%2525%252C%2520and%252014.9%2525%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I2V-GS%3A%20Infrastructure-to-Vehicle%20View%20Transformation%20with%20Gaussian%0A%20%20Splatting%20for%20Autonomous%20Driving%20Data%20Generation&entry.906535625=Jialei%20Chen%20and%20Wuhao%20Xu%20and%20Sipeng%20He%20and%20Baoru%20Huang%20and%20Dongchun%20Ren&entry.1292438233=%20%20Vast%20and%20high-quality%20data%20are%20essential%20for%20end-to-end%20autonomous%20driving%0Asystems.%20However%2C%20current%20driving%20data%20is%20mainly%20collected%20by%20vehicles%2C%20which%0Ais%20expensive%20and%20inefficient.%20A%20potential%20solution%20lies%20in%20synthesizing%20data%0Afrom%20real-world%20images.%20Recent%20advancements%20in%203D%20reconstruction%20demonstrate%0Aphotorealistic%20novel%20view%20synthesis%2C%20highlighting%20the%20potential%20of%20generating%0Adriving%20data%20from%20images%20captured%20on%20the%20road.%20This%20paper%20introduces%20a%20novel%0Amethod%2C%20I2V-GS%2C%20to%20transfer%20the%20Infrastructure%20view%20To%20the%20Vehicle%20view%20with%0AGaussian%20Splatting.%20Reconstruction%20from%20sparse%20infrastructure%20viewpoints%20and%0Arendering%20under%20large%20view%20transformations%20is%20a%20challenging%20problem.%20We%20adopt%0Athe%20adaptive%20depth%20warp%20to%20generate%20dense%20training%20views.%20To%20further%20expand%20the%0Arange%20of%20views%2C%20we%20employ%20a%20cascade%20strategy%20to%20inpaint%20warped%20images%2C%20which%0Aalso%20ensures%20inpainting%20content%20is%20consistent%20across%20views.%20To%20further%20ensure%0Athe%20reliability%20of%20the%20diffusion%20model%2C%20we%20utilize%20the%20cross-view%20information%0Ato%20perform%20a%20confidenceguided%20optimization.%20Moreover%2C%20we%20introduce%20RoadSight%2C%20a%0Amulti-modality%2C%20multi-view%20dataset%20from%20real%20scenarios%20in%20infrastructure%20views.%0ATo%20our%20knowledge%2C%20I2V-GS%20is%20the%20first%20framework%20to%20generate%20autonomous%20driving%0Adatasets%20with%20infrastructure-vehicle%20view%20transformation.%20Experimental%20results%0Ademonstrate%20that%20I2V-GS%20significantly%20improves%20synthesis%20quality%20under%20vehicle%0Aview%2C%20outperforming%20StreetGaussian%20in%20NTA-Iou%2C%20NTL-Iou%2C%20and%20FID%20by%2045.7%25%2C%0A34.2%25%2C%20and%2014.9%25%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23683v1&entry.124074799=Read"},
{"title": "Enhanced Velocity Field Modeling for Gaussian Video Reconstruction", "author": "Zhenyang Li and Xiaoyang Bai and Tongchen Zhang and Pengfei Shen and Weiwei Xu and Yifan Peng", "abstract": "  High-fidelity 3D video reconstruction is essential for enabling real-time\nrendering of dynamic scenes with realistic motion in virtual and augmented\nreality (VR/AR). The deformation field paradigm of 3D Gaussian splatting has\nachieved near-photorealistic results in video reconstruction due to the great\nrepresentation capability of deep deformation networks. However, in videos with\ncomplex motion and significant scale variations, deformation networks often\noverfit to irregular Gaussian trajectories, leading to suboptimal visual\nquality. Moreover, the gradient-based densification strategy designed for\nstatic scene reconstruction proves inadequate to address the absence of dynamic\ncontent. In light of these challenges, we propose a flow-empowered velocity\nfield modeling scheme tailored for Gaussian video reconstruction, dubbed\nFlowGaussian-VR. It consists of two core components: a velocity field rendering\n(VFR) pipeline which enables optical flow-based optimization, and a\nflow-assisted adaptive densification (FAD) strategy that adjusts the number and\nsize of Gaussians in dynamic regions. We validate our model's effectiveness on\nmulti-view dynamic reconstruction and novel view synthesis with multiple\nreal-world datasets containing challenging motion scenarios, demonstrating not\nonly notable visual improvements (over 2.5 dB gain in PSNR) and less blurry\nartifacts in dynamic textures, but also regularized and trackable per-Gaussian\ntrajectories.\n", "link": "http://arxiv.org/abs/2507.23704v1", "date": "2025-07-31", "relevancy": 3.1978, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6821}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6296}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Velocity%20Field%20Modeling%20for%20Gaussian%20Video%20Reconstruction&body=Title%3A%20Enhanced%20Velocity%20Field%20Modeling%20for%20Gaussian%20Video%20Reconstruction%0AAuthor%3A%20Zhenyang%20Li%20and%20Xiaoyang%20Bai%20and%20Tongchen%20Zhang%20and%20Pengfei%20Shen%20and%20Weiwei%20Xu%20and%20Yifan%20Peng%0AAbstract%3A%20%20%20High-fidelity%203D%20video%20reconstruction%20is%20essential%20for%20enabling%20real-time%0Arendering%20of%20dynamic%20scenes%20with%20realistic%20motion%20in%20virtual%20and%20augmented%0Areality%20%28VR/AR%29.%20The%20deformation%20field%20paradigm%20of%203D%20Gaussian%20splatting%20has%0Aachieved%20near-photorealistic%20results%20in%20video%20reconstruction%20due%20to%20the%20great%0Arepresentation%20capability%20of%20deep%20deformation%20networks.%20However%2C%20in%20videos%20with%0Acomplex%20motion%20and%20significant%20scale%20variations%2C%20deformation%20networks%20often%0Aoverfit%20to%20irregular%20Gaussian%20trajectories%2C%20leading%20to%20suboptimal%20visual%0Aquality.%20Moreover%2C%20the%20gradient-based%20densification%20strategy%20designed%20for%0Astatic%20scene%20reconstruction%20proves%20inadequate%20to%20address%20the%20absence%20of%20dynamic%0Acontent.%20In%20light%20of%20these%20challenges%2C%20we%20propose%20a%20flow-empowered%20velocity%0Afield%20modeling%20scheme%20tailored%20for%20Gaussian%20video%20reconstruction%2C%20dubbed%0AFlowGaussian-VR.%20It%20consists%20of%20two%20core%20components%3A%20a%20velocity%20field%20rendering%0A%28VFR%29%20pipeline%20which%20enables%20optical%20flow-based%20optimization%2C%20and%20a%0Aflow-assisted%20adaptive%20densification%20%28FAD%29%20strategy%20that%20adjusts%20the%20number%20and%0Asize%20of%20Gaussians%20in%20dynamic%20regions.%20We%20validate%20our%20model%27s%20effectiveness%20on%0Amulti-view%20dynamic%20reconstruction%20and%20novel%20view%20synthesis%20with%20multiple%0Areal-world%20datasets%20containing%20challenging%20motion%20scenarios%2C%20demonstrating%20not%0Aonly%20notable%20visual%20improvements%20%28over%202.5%20dB%20gain%20in%20PSNR%29%20and%20less%20blurry%0Aartifacts%20in%20dynamic%20textures%2C%20but%20also%20regularized%20and%20trackable%20per-Gaussian%0Atrajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Velocity%2520Field%2520Modeling%2520for%2520Gaussian%2520Video%2520Reconstruction%26entry.906535625%3DZhenyang%2520Li%2520and%2520Xiaoyang%2520Bai%2520and%2520Tongchen%2520Zhang%2520and%2520Pengfei%2520Shen%2520and%2520Weiwei%2520Xu%2520and%2520Yifan%2520Peng%26entry.1292438233%3D%2520%2520High-fidelity%25203D%2520video%2520reconstruction%2520is%2520essential%2520for%2520enabling%2520real-time%250Arendering%2520of%2520dynamic%2520scenes%2520with%2520realistic%2520motion%2520in%2520virtual%2520and%2520augmented%250Areality%2520%2528VR/AR%2529.%2520The%2520deformation%2520field%2520paradigm%2520of%25203D%2520Gaussian%2520splatting%2520has%250Aachieved%2520near-photorealistic%2520results%2520in%2520video%2520reconstruction%2520due%2520to%2520the%2520great%250Arepresentation%2520capability%2520of%2520deep%2520deformation%2520networks.%2520However%252C%2520in%2520videos%2520with%250Acomplex%2520motion%2520and%2520significant%2520scale%2520variations%252C%2520deformation%2520networks%2520often%250Aoverfit%2520to%2520irregular%2520Gaussian%2520trajectories%252C%2520leading%2520to%2520suboptimal%2520visual%250Aquality.%2520Moreover%252C%2520the%2520gradient-based%2520densification%2520strategy%2520designed%2520for%250Astatic%2520scene%2520reconstruction%2520proves%2520inadequate%2520to%2520address%2520the%2520absence%2520of%2520dynamic%250Acontent.%2520In%2520light%2520of%2520these%2520challenges%252C%2520we%2520propose%2520a%2520flow-empowered%2520velocity%250Afield%2520modeling%2520scheme%2520tailored%2520for%2520Gaussian%2520video%2520reconstruction%252C%2520dubbed%250AFlowGaussian-VR.%2520It%2520consists%2520of%2520two%2520core%2520components%253A%2520a%2520velocity%2520field%2520rendering%250A%2528VFR%2529%2520pipeline%2520which%2520enables%2520optical%2520flow-based%2520optimization%252C%2520and%2520a%250Aflow-assisted%2520adaptive%2520densification%2520%2528FAD%2529%2520strategy%2520that%2520adjusts%2520the%2520number%2520and%250Asize%2520of%2520Gaussians%2520in%2520dynamic%2520regions.%2520We%2520validate%2520our%2520model%2527s%2520effectiveness%2520on%250Amulti-view%2520dynamic%2520reconstruction%2520and%2520novel%2520view%2520synthesis%2520with%2520multiple%250Areal-world%2520datasets%2520containing%2520challenging%2520motion%2520scenarios%252C%2520demonstrating%2520not%250Aonly%2520notable%2520visual%2520improvements%2520%2528over%25202.5%2520dB%2520gain%2520in%2520PSNR%2529%2520and%2520less%2520blurry%250Aartifacts%2520in%2520dynamic%2520textures%252C%2520but%2520also%2520regularized%2520and%2520trackable%2520per-Gaussian%250Atrajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Velocity%20Field%20Modeling%20for%20Gaussian%20Video%20Reconstruction&entry.906535625=Zhenyang%20Li%20and%20Xiaoyang%20Bai%20and%20Tongchen%20Zhang%20and%20Pengfei%20Shen%20and%20Weiwei%20Xu%20and%20Yifan%20Peng&entry.1292438233=%20%20High-fidelity%203D%20video%20reconstruction%20is%20essential%20for%20enabling%20real-time%0Arendering%20of%20dynamic%20scenes%20with%20realistic%20motion%20in%20virtual%20and%20augmented%0Areality%20%28VR/AR%29.%20The%20deformation%20field%20paradigm%20of%203D%20Gaussian%20splatting%20has%0Aachieved%20near-photorealistic%20results%20in%20video%20reconstruction%20due%20to%20the%20great%0Arepresentation%20capability%20of%20deep%20deformation%20networks.%20However%2C%20in%20videos%20with%0Acomplex%20motion%20and%20significant%20scale%20variations%2C%20deformation%20networks%20often%0Aoverfit%20to%20irregular%20Gaussian%20trajectories%2C%20leading%20to%20suboptimal%20visual%0Aquality.%20Moreover%2C%20the%20gradient-based%20densification%20strategy%20designed%20for%0Astatic%20scene%20reconstruction%20proves%20inadequate%20to%20address%20the%20absence%20of%20dynamic%0Acontent.%20In%20light%20of%20these%20challenges%2C%20we%20propose%20a%20flow-empowered%20velocity%0Afield%20modeling%20scheme%20tailored%20for%20Gaussian%20video%20reconstruction%2C%20dubbed%0AFlowGaussian-VR.%20It%20consists%20of%20two%20core%20components%3A%20a%20velocity%20field%20rendering%0A%28VFR%29%20pipeline%20which%20enables%20optical%20flow-based%20optimization%2C%20and%20a%0Aflow-assisted%20adaptive%20densification%20%28FAD%29%20strategy%20that%20adjusts%20the%20number%20and%0Asize%20of%20Gaussians%20in%20dynamic%20regions.%20We%20validate%20our%20model%27s%20effectiveness%20on%0Amulti-view%20dynamic%20reconstruction%20and%20novel%20view%20synthesis%20with%20multiple%0Areal-world%20datasets%20containing%20challenging%20motion%20scenarios%2C%20demonstrating%20not%0Aonly%20notable%20visual%20improvements%20%28over%202.5%20dB%20gain%20in%20PSNR%29%20and%20less%20blurry%0Aartifacts%20in%20dynamic%20textures%2C%20but%20also%20regularized%20and%20trackable%20per-Gaussian%0Atrajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23704v1&entry.124074799=Read"},
{"title": "MonoFusion: Sparse-View 4D Reconstruction via Monocular Fusion", "author": "Zihan Wang and Jeff Tan and Tarasha Khurana and Neehar Peri and Deva Ramanan", "abstract": "  We address the problem of dynamic scene reconstruction from sparse-view\nvideos. Prior work often requires dense multi-view captures with hundreds of\ncalibrated cameras (e.g. Panoptic Studio). Such multi-view setups are\nprohibitively expensive to build and cannot capture diverse scenes in-the-wild.\nIn contrast, we aim to reconstruct dynamic human behaviors, such as repairing a\nbike or dancing, from a small set of sparse-view cameras with complete scene\ncoverage (e.g. four equidistant inward-facing static cameras). We find that\ndense multi-view reconstruction methods struggle to adapt to this sparse-view\nsetup due to limited overlap between viewpoints. To address these limitations,\nwe carefully align independent monocular reconstructions of each camera to\nproduce time- and view-consistent dynamic scene reconstructions. Extensive\nexperiments on PanopticStudio and Ego-Exo4D demonstrate that our method\nachieves higher quality reconstructions than prior art, particularly when\nrendering novel views. Code, data, and data-processing scripts are available on\nhttps://github.com/ImNotPrepared/MonoFusion.\n", "link": "http://arxiv.org/abs/2507.23782v1", "date": "2025-07-31", "relevancy": 3.1337, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6304}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6249}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoFusion%3A%20Sparse-View%204D%20Reconstruction%20via%20Monocular%20Fusion&body=Title%3A%20MonoFusion%3A%20Sparse-View%204D%20Reconstruction%20via%20Monocular%20Fusion%0AAuthor%3A%20Zihan%20Wang%20and%20Jeff%20Tan%20and%20Tarasha%20Khurana%20and%20Neehar%20Peri%20and%20Deva%20Ramanan%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20dynamic%20scene%20reconstruction%20from%20sparse-view%0Avideos.%20Prior%20work%20often%20requires%20dense%20multi-view%20captures%20with%20hundreds%20of%0Acalibrated%20cameras%20%28e.g.%20Panoptic%20Studio%29.%20Such%20multi-view%20setups%20are%0Aprohibitively%20expensive%20to%20build%20and%20cannot%20capture%20diverse%20scenes%20in-the-wild.%0AIn%20contrast%2C%20we%20aim%20to%20reconstruct%20dynamic%20human%20behaviors%2C%20such%20as%20repairing%20a%0Abike%20or%20dancing%2C%20from%20a%20small%20set%20of%20sparse-view%20cameras%20with%20complete%20scene%0Acoverage%20%28e.g.%20four%20equidistant%20inward-facing%20static%20cameras%29.%20We%20find%20that%0Adense%20multi-view%20reconstruction%20methods%20struggle%20to%20adapt%20to%20this%20sparse-view%0Asetup%20due%20to%20limited%20overlap%20between%20viewpoints.%20To%20address%20these%20limitations%2C%0Awe%20carefully%20align%20independent%20monocular%20reconstructions%20of%20each%20camera%20to%0Aproduce%20time-%20and%20view-consistent%20dynamic%20scene%20reconstructions.%20Extensive%0Aexperiments%20on%20PanopticStudio%20and%20Ego-Exo4D%20demonstrate%20that%20our%20method%0Aachieves%20higher%20quality%20reconstructions%20than%20prior%20art%2C%20particularly%20when%0Arendering%20novel%20views.%20Code%2C%20data%2C%20and%20data-processing%20scripts%20are%20available%20on%0Ahttps%3A//github.com/ImNotPrepared/MonoFusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoFusion%253A%2520Sparse-View%25204D%2520Reconstruction%2520via%2520Monocular%2520Fusion%26entry.906535625%3DZihan%2520Wang%2520and%2520Jeff%2520Tan%2520and%2520Tarasha%2520Khurana%2520and%2520Neehar%2520Peri%2520and%2520Deva%2520Ramanan%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520dynamic%2520scene%2520reconstruction%2520from%2520sparse-view%250Avideos.%2520Prior%2520work%2520often%2520requires%2520dense%2520multi-view%2520captures%2520with%2520hundreds%2520of%250Acalibrated%2520cameras%2520%2528e.g.%2520Panoptic%2520Studio%2529.%2520Such%2520multi-view%2520setups%2520are%250Aprohibitively%2520expensive%2520to%2520build%2520and%2520cannot%2520capture%2520diverse%2520scenes%2520in-the-wild.%250AIn%2520contrast%252C%2520we%2520aim%2520to%2520reconstruct%2520dynamic%2520human%2520behaviors%252C%2520such%2520as%2520repairing%2520a%250Abike%2520or%2520dancing%252C%2520from%2520a%2520small%2520set%2520of%2520sparse-view%2520cameras%2520with%2520complete%2520scene%250Acoverage%2520%2528e.g.%2520four%2520equidistant%2520inward-facing%2520static%2520cameras%2529.%2520We%2520find%2520that%250Adense%2520multi-view%2520reconstruction%2520methods%2520struggle%2520to%2520adapt%2520to%2520this%2520sparse-view%250Asetup%2520due%2520to%2520limited%2520overlap%2520between%2520viewpoints.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520carefully%2520align%2520independent%2520monocular%2520reconstructions%2520of%2520each%2520camera%2520to%250Aproduce%2520time-%2520and%2520view-consistent%2520dynamic%2520scene%2520reconstructions.%2520Extensive%250Aexperiments%2520on%2520PanopticStudio%2520and%2520Ego-Exo4D%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520higher%2520quality%2520reconstructions%2520than%2520prior%2520art%252C%2520particularly%2520when%250Arendering%2520novel%2520views.%2520Code%252C%2520data%252C%2520and%2520data-processing%2520scripts%2520are%2520available%2520on%250Ahttps%253A//github.com/ImNotPrepared/MonoFusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoFusion%3A%20Sparse-View%204D%20Reconstruction%20via%20Monocular%20Fusion&entry.906535625=Zihan%20Wang%20and%20Jeff%20Tan%20and%20Tarasha%20Khurana%20and%20Neehar%20Peri%20and%20Deva%20Ramanan&entry.1292438233=%20%20We%20address%20the%20problem%20of%20dynamic%20scene%20reconstruction%20from%20sparse-view%0Avideos.%20Prior%20work%20often%20requires%20dense%20multi-view%20captures%20with%20hundreds%20of%0Acalibrated%20cameras%20%28e.g.%20Panoptic%20Studio%29.%20Such%20multi-view%20setups%20are%0Aprohibitively%20expensive%20to%20build%20and%20cannot%20capture%20diverse%20scenes%20in-the-wild.%0AIn%20contrast%2C%20we%20aim%20to%20reconstruct%20dynamic%20human%20behaviors%2C%20such%20as%20repairing%20a%0Abike%20or%20dancing%2C%20from%20a%20small%20set%20of%20sparse-view%20cameras%20with%20complete%20scene%0Acoverage%20%28e.g.%20four%20equidistant%20inward-facing%20static%20cameras%29.%20We%20find%20that%0Adense%20multi-view%20reconstruction%20methods%20struggle%20to%20adapt%20to%20this%20sparse-view%0Asetup%20due%20to%20limited%20overlap%20between%20viewpoints.%20To%20address%20these%20limitations%2C%0Awe%20carefully%20align%20independent%20monocular%20reconstructions%20of%20each%20camera%20to%0Aproduce%20time-%20and%20view-consistent%20dynamic%20scene%20reconstructions.%20Extensive%0Aexperiments%20on%20PanopticStudio%20and%20Ego-Exo4D%20demonstrate%20that%20our%20method%0Aachieves%20higher%20quality%20reconstructions%20than%20prior%20art%2C%20particularly%20when%0Arendering%20novel%20views.%20Code%2C%20data%2C%20and%20data-processing%20scripts%20are%20available%20on%0Ahttps%3A//github.com/ImNotPrepared/MonoFusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23782v1&entry.124074799=Read"},
{"title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention", "author": "Yiwen Chen and Zhihao Li and Yikai Wang and Hu Zhang and Qin Li and Chi Zhang and Guosheng Lin", "abstract": "  Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.\n", "link": "http://arxiv.org/abs/2507.17745v3", "date": "2025-07-31", "relevancy": 3.1257, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6322}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra3D%3A%20Efficient%20and%20High-Fidelity%203D%20Generation%20with%20Part%20Attention&body=Title%3A%20Ultra3D%3A%20Efficient%20and%20High-Fidelity%203D%20Generation%20with%20Part%20Attention%0AAuthor%3A%20Yiwen%20Chen%20and%20Zhihao%20Li%20and%20Yikai%20Wang%20and%20Hu%20Zhang%20and%20Qin%20Li%20and%20Chi%20Zhang%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20Recent%20advances%20in%20sparse%20voxel%20representations%20have%20significantly%20improved%0Athe%20quality%20of%203D%20content%20generation%2C%20enabling%20high-resolution%20modeling%20with%0Afine-grained%20geometry.%20However%2C%20existing%20frameworks%20suffer%20from%20severe%0Acomputational%20inefficiencies%20due%20to%20the%20quadratic%20complexity%20of%20attention%0Amechanisms%20in%20their%20two-stage%20diffusion%20pipelines.%20In%20this%20work%2C%20we%20propose%0AUltra3D%2C%20an%20efficient%203D%20generation%20framework%20that%20significantly%20accelerates%0Asparse%20voxel%20modeling%20without%20compromising%20quality.%20Our%20method%20leverages%20the%0Acompact%20VecSet%20representation%20to%20efficiently%20generate%20a%20coarse%20object%20layout%20in%0Athe%20first%20stage%2C%20reducing%20token%20count%20and%20accelerating%20voxel%20coordinate%0Aprediction.%20To%20refine%20per-voxel%20latent%20features%20in%20the%20second%20stage%2C%20we%0Aintroduce%20Part%20Attention%2C%20a%20geometry-aware%20localized%20attention%20mechanism%20that%0Arestricts%20attention%20computation%20within%20semantically%20consistent%20part%20regions.%0AThis%20design%20preserves%20structural%20continuity%20while%20avoiding%20unnecessary%20global%0Aattention%2C%20achieving%20up%20to%206.7x%20speed-up%20in%20latent%20generation.%20To%20support%20this%0Amechanism%2C%20we%20construct%20a%20scalable%20part%20annotation%20pipeline%20that%20converts%20raw%0Ameshes%20into%20part-labeled%20sparse%20voxels.%20Extensive%20experiments%20demonstrate%20that%0AUltra3D%20supports%20high-resolution%203D%20generation%20at%201024%20resolution%20and%20achieves%0Astate-of-the-art%20performance%20in%20both%20visual%20fidelity%20and%20user%20preference.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.17745v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra3D%253A%2520Efficient%2520and%2520High-Fidelity%25203D%2520Generation%2520with%2520Part%2520Attention%26entry.906535625%3DYiwen%2520Chen%2520and%2520Zhihao%2520Li%2520and%2520Yikai%2520Wang%2520and%2520Hu%2520Zhang%2520and%2520Qin%2520Li%2520and%2520Chi%2520Zhang%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520sparse%2520voxel%2520representations%2520have%2520significantly%2520improved%250Athe%2520quality%2520of%25203D%2520content%2520generation%252C%2520enabling%2520high-resolution%2520modeling%2520with%250Afine-grained%2520geometry.%2520However%252C%2520existing%2520frameworks%2520suffer%2520from%2520severe%250Acomputational%2520inefficiencies%2520due%2520to%2520the%2520quadratic%2520complexity%2520of%2520attention%250Amechanisms%2520in%2520their%2520two-stage%2520diffusion%2520pipelines.%2520In%2520this%2520work%252C%2520we%2520propose%250AUltra3D%252C%2520an%2520efficient%25203D%2520generation%2520framework%2520that%2520significantly%2520accelerates%250Asparse%2520voxel%2520modeling%2520without%2520compromising%2520quality.%2520Our%2520method%2520leverages%2520the%250Acompact%2520VecSet%2520representation%2520to%2520efficiently%2520generate%2520a%2520coarse%2520object%2520layout%2520in%250Athe%2520first%2520stage%252C%2520reducing%2520token%2520count%2520and%2520accelerating%2520voxel%2520coordinate%250Aprediction.%2520To%2520refine%2520per-voxel%2520latent%2520features%2520in%2520the%2520second%2520stage%252C%2520we%250Aintroduce%2520Part%2520Attention%252C%2520a%2520geometry-aware%2520localized%2520attention%2520mechanism%2520that%250Arestricts%2520attention%2520computation%2520within%2520semantically%2520consistent%2520part%2520regions.%250AThis%2520design%2520preserves%2520structural%2520continuity%2520while%2520avoiding%2520unnecessary%2520global%250Aattention%252C%2520achieving%2520up%2520to%25206.7x%2520speed-up%2520in%2520latent%2520generation.%2520To%2520support%2520this%250Amechanism%252C%2520we%2520construct%2520a%2520scalable%2520part%2520annotation%2520pipeline%2520that%2520converts%2520raw%250Ameshes%2520into%2520part-labeled%2520sparse%2520voxels.%2520Extensive%2520experiments%2520demonstrate%2520that%250AUltra3D%2520supports%2520high-resolution%25203D%2520generation%2520at%25201024%2520resolution%2520and%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520visual%2520fidelity%2520and%2520user%2520preference.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17745v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra3D%3A%20Efficient%20and%20High-Fidelity%203D%20Generation%20with%20Part%20Attention&entry.906535625=Yiwen%20Chen%20and%20Zhihao%20Li%20and%20Yikai%20Wang%20and%20Hu%20Zhang%20and%20Qin%20Li%20and%20Chi%20Zhang%20and%20Guosheng%20Lin&entry.1292438233=%20%20Recent%20advances%20in%20sparse%20voxel%20representations%20have%20significantly%20improved%0Athe%20quality%20of%203D%20content%20generation%2C%20enabling%20high-resolution%20modeling%20with%0Afine-grained%20geometry.%20However%2C%20existing%20frameworks%20suffer%20from%20severe%0Acomputational%20inefficiencies%20due%20to%20the%20quadratic%20complexity%20of%20attention%0Amechanisms%20in%20their%20two-stage%20diffusion%20pipelines.%20In%20this%20work%2C%20we%20propose%0AUltra3D%2C%20an%20efficient%203D%20generation%20framework%20that%20significantly%20accelerates%0Asparse%20voxel%20modeling%20without%20compromising%20quality.%20Our%20method%20leverages%20the%0Acompact%20VecSet%20representation%20to%20efficiently%20generate%20a%20coarse%20object%20layout%20in%0Athe%20first%20stage%2C%20reducing%20token%20count%20and%20accelerating%20voxel%20coordinate%0Aprediction.%20To%20refine%20per-voxel%20latent%20features%20in%20the%20second%20stage%2C%20we%0Aintroduce%20Part%20Attention%2C%20a%20geometry-aware%20localized%20attention%20mechanism%20that%0Arestricts%20attention%20computation%20within%20semantically%20consistent%20part%20regions.%0AThis%20design%20preserves%20structural%20continuity%20while%20avoiding%20unnecessary%20global%0Aattention%2C%20achieving%20up%20to%206.7x%20speed-up%20in%20latent%20generation.%20To%20support%20this%0Amechanism%2C%20we%20construct%20a%20scalable%20part%20annotation%20pipeline%20that%20converts%20raw%0Ameshes%20into%20part-labeled%20sparse%20voxels.%20Extensive%20experiments%20demonstrate%20that%0AUltra3D%20supports%20high-resolution%203D%20generation%20at%201024%20resolution%20and%20achieves%0Astate-of-the-art%20performance%20in%20both%20visual%20fidelity%20and%20user%20preference.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.17745v3&entry.124074799=Read"},
{"title": "Vector-Quantized Vision Foundation Models for Object-Centric Learning", "author": "Rongzhen Zhao and Vivienne Wang and Juho Kannala and Joni Pajarinen", "abstract": "  Perceiving visual scenes as objects and background--like humans\ndo--Object-Centric Learning (OCL) aggregates image or video feature maps into\nobject-level feature vectors, termed \\textit{slots}. OCL's self-supervision of\nreconstructing the input from these aggregated slots struggles with complex\nobject textures, thus Vision Foundation Model (VFM) representations are used as\nthe aggregation input and reconstruction target. However, existing methods\nleverage VFM representations in diverse ways and often fail to fully exploit\ntheir potential. In response, we propose a clean architecture--Vector-Quantized\nVFMs for OCL (VQ-VFM-OCL, or VVO)--that unifies mainstream OCL methods. The key\nto our unification is simple yet effective, just shared quantizing the same VFM\nrepresentation as the reconstruction target. Through mathematical modeling and\nstatistical verification, we further analyze why VFM representations facilitate\nOCL aggregation and how their shared quantization as reconstruction targets\nstrengthens OCL supervision. Experiments show that across different VFMs,\naggregators and decoders, our VVO consistently outperforms baselines in object\ndiscovery and recognition, as well as downstream visual prediction and\nreasoning. The implementation and model checkpoints are available on\nhttps://github.com/Genera1Z/VQ-VFM-OCL.\n", "link": "http://arxiv.org/abs/2502.20263v4", "date": "2025-07-31", "relevancy": 3.1123, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6456}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vector-Quantized%20Vision%20Foundation%20Models%20for%20Object-Centric%20Learning&body=Title%3A%20Vector-Quantized%20Vision%20Foundation%20Models%20for%20Object-Centric%20Learning%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Perceiving%20visual%20scenes%20as%20objects%20and%20background--like%20humans%0Ado--Object-Centric%20Learning%20%28OCL%29%20aggregates%20image%20or%20video%20feature%20maps%20into%0Aobject-level%20feature%20vectors%2C%20termed%20%5Ctextit%7Bslots%7D.%20OCL%27s%20self-supervision%20of%0Areconstructing%20the%20input%20from%20these%20aggregated%20slots%20struggles%20with%20complex%0Aobject%20textures%2C%20thus%20Vision%20Foundation%20Model%20%28VFM%29%20representations%20are%20used%20as%0Athe%20aggregation%20input%20and%20reconstruction%20target.%20However%2C%20existing%20methods%0Aleverage%20VFM%20representations%20in%20diverse%20ways%20and%20often%20fail%20to%20fully%20exploit%0Atheir%20potential.%20In%20response%2C%20we%20propose%20a%20clean%20architecture--Vector-Quantized%0AVFMs%20for%20OCL%20%28VQ-VFM-OCL%2C%20or%20VVO%29--that%20unifies%20mainstream%20OCL%20methods.%20The%20key%0Ato%20our%20unification%20is%20simple%20yet%20effective%2C%20just%20shared%20quantizing%20the%20same%20VFM%0Arepresentation%20as%20the%20reconstruction%20target.%20Through%20mathematical%20modeling%20and%0Astatistical%20verification%2C%20we%20further%20analyze%20why%20VFM%20representations%20facilitate%0AOCL%20aggregation%20and%20how%20their%20shared%20quantization%20as%20reconstruction%20targets%0Astrengthens%20OCL%20supervision.%20Experiments%20show%20that%20across%20different%20VFMs%2C%0Aaggregators%20and%20decoders%2C%20our%20VVO%20consistently%20outperforms%20baselines%20in%20object%0Adiscovery%20and%20recognition%2C%20as%20well%20as%20downstream%20visual%20prediction%20and%0Areasoning.%20The%20implementation%20and%20model%20checkpoints%20are%20available%20on%0Ahttps%3A//github.com/Genera1Z/VQ-VFM-OCL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.20263v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVector-Quantized%2520Vision%2520Foundation%2520Models%2520for%2520Object-Centric%2520Learning%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Vivienne%2520Wang%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Perceiving%2520visual%2520scenes%2520as%2520objects%2520and%2520background--like%2520humans%250Ado--Object-Centric%2520Learning%2520%2528OCL%2529%2520aggregates%2520image%2520or%2520video%2520feature%2520maps%2520into%250Aobject-level%2520feature%2520vectors%252C%2520termed%2520%255Ctextit%257Bslots%257D.%2520OCL%2527s%2520self-supervision%2520of%250Areconstructing%2520the%2520input%2520from%2520these%2520aggregated%2520slots%2520struggles%2520with%2520complex%250Aobject%2520textures%252C%2520thus%2520Vision%2520Foundation%2520Model%2520%2528VFM%2529%2520representations%2520are%2520used%2520as%250Athe%2520aggregation%2520input%2520and%2520reconstruction%2520target.%2520However%252C%2520existing%2520methods%250Aleverage%2520VFM%2520representations%2520in%2520diverse%2520ways%2520and%2520often%2520fail%2520to%2520fully%2520exploit%250Atheir%2520potential.%2520In%2520response%252C%2520we%2520propose%2520a%2520clean%2520architecture--Vector-Quantized%250AVFMs%2520for%2520OCL%2520%2528VQ-VFM-OCL%252C%2520or%2520VVO%2529--that%2520unifies%2520mainstream%2520OCL%2520methods.%2520The%2520key%250Ato%2520our%2520unification%2520is%2520simple%2520yet%2520effective%252C%2520just%2520shared%2520quantizing%2520the%2520same%2520VFM%250Arepresentation%2520as%2520the%2520reconstruction%2520target.%2520Through%2520mathematical%2520modeling%2520and%250Astatistical%2520verification%252C%2520we%2520further%2520analyze%2520why%2520VFM%2520representations%2520facilitate%250AOCL%2520aggregation%2520and%2520how%2520their%2520shared%2520quantization%2520as%2520reconstruction%2520targets%250Astrengthens%2520OCL%2520supervision.%2520Experiments%2520show%2520that%2520across%2520different%2520VFMs%252C%250Aaggregators%2520and%2520decoders%252C%2520our%2520VVO%2520consistently%2520outperforms%2520baselines%2520in%2520object%250Adiscovery%2520and%2520recognition%252C%2520as%2520well%2520as%2520downstream%2520visual%2520prediction%2520and%250Areasoning.%2520The%2520implementation%2520and%2520model%2520checkpoints%2520are%2520available%2520on%250Ahttps%253A//github.com/Genera1Z/VQ-VFM-OCL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20263v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vector-Quantized%20Vision%20Foundation%20Models%20for%20Object-Centric%20Learning&entry.906535625=Rongzhen%20Zhao%20and%20Vivienne%20Wang%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Perceiving%20visual%20scenes%20as%20objects%20and%20background--like%20humans%0Ado--Object-Centric%20Learning%20%28OCL%29%20aggregates%20image%20or%20video%20feature%20maps%20into%0Aobject-level%20feature%20vectors%2C%20termed%20%5Ctextit%7Bslots%7D.%20OCL%27s%20self-supervision%20of%0Areconstructing%20the%20input%20from%20these%20aggregated%20slots%20struggles%20with%20complex%0Aobject%20textures%2C%20thus%20Vision%20Foundation%20Model%20%28VFM%29%20representations%20are%20used%20as%0Athe%20aggregation%20input%20and%20reconstruction%20target.%20However%2C%20existing%20methods%0Aleverage%20VFM%20representations%20in%20diverse%20ways%20and%20often%20fail%20to%20fully%20exploit%0Atheir%20potential.%20In%20response%2C%20we%20propose%20a%20clean%20architecture--Vector-Quantized%0AVFMs%20for%20OCL%20%28VQ-VFM-OCL%2C%20or%20VVO%29--that%20unifies%20mainstream%20OCL%20methods.%20The%20key%0Ato%20our%20unification%20is%20simple%20yet%20effective%2C%20just%20shared%20quantizing%20the%20same%20VFM%0Arepresentation%20as%20the%20reconstruction%20target.%20Through%20mathematical%20modeling%20and%0Astatistical%20verification%2C%20we%20further%20analyze%20why%20VFM%20representations%20facilitate%0AOCL%20aggregation%20and%20how%20their%20shared%20quantization%20as%20reconstruction%20targets%0Astrengthens%20OCL%20supervision.%20Experiments%20show%20that%20across%20different%20VFMs%2C%0Aaggregators%20and%20decoders%2C%20our%20VVO%20consistently%20outperforms%20baselines%20in%20object%0Adiscovery%20and%20recognition%2C%20as%20well%20as%20downstream%20visual%20prediction%20and%0Areasoning.%20The%20implementation%20and%20model%20checkpoints%20are%20available%20on%0Ahttps%3A//github.com/Genera1Z/VQ-VFM-OCL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.20263v4&entry.124074799=Read"},
{"title": "UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic\n  LiDAR Global Localization", "author": "Hongming Shen and Xun Chen and Yulin Hui and Zhenyu Wu and Wei Wang and Qiyang Lyu and Tianchen Deng and Danwei Wang", "abstract": "  Existing LGL methods typically consider only partial information (e.g.,\ngeometric features) from LiDAR observations or are designed for homogeneous\nLiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL\nmethod is proposed, termed UniLGL, which simultaneously achieves spatial and\nmaterial uniformity, as well as sensor-type uniformity. The key idea of the\nproposed method is to encode the complete point cloud, which contains both\ngeometric and material information, into a pair of BEV images (i.e., a spatial\nBEV image and an intensity BEV image). An end-to-end multi-BEV fusion network\nis designed to extract uniform features, equipping UniLGL with spatial and\nmaterial uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a\nviewpoint invariance hypothesis is introduced, which replaces the conventional\ntranslation equivariance assumption commonly used in existing LPR networks and\nsupervises UniLGL to achieve sensor-type uniformity in both global descriptors\nand local feature representations. Finally, based on the mapping between local\nfeatures on the 2D BEV image and the point cloud, a robust global pose\nestimator is derived that determines the global minimum of the global pose on\nSE(3) without requiring additional registration. To validate the effectiveness\nof the proposed uniform LGL, extensive benchmarks are conducted in real-world\nenvironments, and the results show that the proposed UniLGL is demonstratively\ncompetitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL\nhas been deployed on diverse platforms, including full-size trucks and agile\nMicro Aerial Vehicles (MAVs), to enable high-precision localization and mapping\nas well as multi-MAV collaborative exploration in port and forest environments,\ndemonstrating the applicability of UniLGL in industrial and field scenarios.\n", "link": "http://arxiv.org/abs/2507.12194v2", "date": "2025-07-31", "relevancy": 3.1026, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6708}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6125}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniLGL%3A%20Learning%20Uniform%20Place%20Recognition%20for%20FOV-limited/Panoramic%0A%20%20LiDAR%20Global%20Localization&body=Title%3A%20UniLGL%3A%20Learning%20Uniform%20Place%20Recognition%20for%20FOV-limited/Panoramic%0A%20%20LiDAR%20Global%20Localization%0AAuthor%3A%20Hongming%20Shen%20and%20Xun%20Chen%20and%20Yulin%20Hui%20and%20Zhenyu%20Wu%20and%20Wei%20Wang%20and%20Qiyang%20Lyu%20and%20Tianchen%20Deng%20and%20Danwei%20Wang%0AAbstract%3A%20%20%20Existing%20LGL%20methods%20typically%20consider%20only%20partial%20information%20%28e.g.%2C%0Ageometric%20features%29%20from%20LiDAR%20observations%20or%20are%20designed%20for%20homogeneous%0ALiDAR%20sensors%2C%20overlooking%20the%20uniformity%20in%20LGL.%20In%20this%20work%2C%20a%20uniform%20LGL%0Amethod%20is%20proposed%2C%20termed%20UniLGL%2C%20which%20simultaneously%20achieves%20spatial%20and%0Amaterial%20uniformity%2C%20as%20well%20as%20sensor-type%20uniformity.%20The%20key%20idea%20of%20the%0Aproposed%20method%20is%20to%20encode%20the%20complete%20point%20cloud%2C%20which%20contains%20both%0Ageometric%20and%20material%20information%2C%20into%20a%20pair%20of%20BEV%20images%20%28i.e.%2C%20a%20spatial%0ABEV%20image%20and%20an%20intensity%20BEV%20image%29.%20An%20end-to-end%20multi-BEV%20fusion%20network%0Ais%20designed%20to%20extract%20uniform%20features%2C%20equipping%20UniLGL%20with%20spatial%20and%0Amaterial%20uniformity.%20To%20ensure%20robust%20LGL%20across%20heterogeneous%20LiDAR%20sensors%2C%20a%0Aviewpoint%20invariance%20hypothesis%20is%20introduced%2C%20which%20replaces%20the%20conventional%0Atranslation%20equivariance%20assumption%20commonly%20used%20in%20existing%20LPR%20networks%20and%0Asupervises%20UniLGL%20to%20achieve%20sensor-type%20uniformity%20in%20both%20global%20descriptors%0Aand%20local%20feature%20representations.%20Finally%2C%20based%20on%20the%20mapping%20between%20local%0Afeatures%20on%20the%202D%20BEV%20image%20and%20the%20point%20cloud%2C%20a%20robust%20global%20pose%0Aestimator%20is%20derived%20that%20determines%20the%20global%20minimum%20of%20the%20global%20pose%20on%0ASE%283%29%20without%20requiring%20additional%20registration.%20To%20validate%20the%20effectiveness%0Aof%20the%20proposed%20uniform%20LGL%2C%20extensive%20benchmarks%20are%20conducted%20in%20real-world%0Aenvironments%2C%20and%20the%20results%20show%20that%20the%20proposed%20UniLGL%20is%20demonstratively%0Acompetitive%20compared%20to%20other%20State-of-the-Art%20LGL%20methods.%20Furthermore%2C%20UniLGL%0Ahas%20been%20deployed%20on%20diverse%20platforms%2C%20including%20full-size%20trucks%20and%20agile%0AMicro%20Aerial%20Vehicles%20%28MAVs%29%2C%20to%20enable%20high-precision%20localization%20and%20mapping%0Aas%20well%20as%20multi-MAV%20collaborative%20exploration%20in%20port%20and%20forest%20environments%2C%0Ademonstrating%20the%20applicability%20of%20UniLGL%20in%20industrial%20and%20field%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.12194v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniLGL%253A%2520Learning%2520Uniform%2520Place%2520Recognition%2520for%2520FOV-limited/Panoramic%250A%2520%2520LiDAR%2520Global%2520Localization%26entry.906535625%3DHongming%2520Shen%2520and%2520Xun%2520Chen%2520and%2520Yulin%2520Hui%2520and%2520Zhenyu%2520Wu%2520and%2520Wei%2520Wang%2520and%2520Qiyang%2520Lyu%2520and%2520Tianchen%2520Deng%2520and%2520Danwei%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520LGL%2520methods%2520typically%2520consider%2520only%2520partial%2520information%2520%2528e.g.%252C%250Ageometric%2520features%2529%2520from%2520LiDAR%2520observations%2520or%2520are%2520designed%2520for%2520homogeneous%250ALiDAR%2520sensors%252C%2520overlooking%2520the%2520uniformity%2520in%2520LGL.%2520In%2520this%2520work%252C%2520a%2520uniform%2520LGL%250Amethod%2520is%2520proposed%252C%2520termed%2520UniLGL%252C%2520which%2520simultaneously%2520achieves%2520spatial%2520and%250Amaterial%2520uniformity%252C%2520as%2520well%2520as%2520sensor-type%2520uniformity.%2520The%2520key%2520idea%2520of%2520the%250Aproposed%2520method%2520is%2520to%2520encode%2520the%2520complete%2520point%2520cloud%252C%2520which%2520contains%2520both%250Ageometric%2520and%2520material%2520information%252C%2520into%2520a%2520pair%2520of%2520BEV%2520images%2520%2528i.e.%252C%2520a%2520spatial%250ABEV%2520image%2520and%2520an%2520intensity%2520BEV%2520image%2529.%2520An%2520end-to-end%2520multi-BEV%2520fusion%2520network%250Ais%2520designed%2520to%2520extract%2520uniform%2520features%252C%2520equipping%2520UniLGL%2520with%2520spatial%2520and%250Amaterial%2520uniformity.%2520To%2520ensure%2520robust%2520LGL%2520across%2520heterogeneous%2520LiDAR%2520sensors%252C%2520a%250Aviewpoint%2520invariance%2520hypothesis%2520is%2520introduced%252C%2520which%2520replaces%2520the%2520conventional%250Atranslation%2520equivariance%2520assumption%2520commonly%2520used%2520in%2520existing%2520LPR%2520networks%2520and%250Asupervises%2520UniLGL%2520to%2520achieve%2520sensor-type%2520uniformity%2520in%2520both%2520global%2520descriptors%250Aand%2520local%2520feature%2520representations.%2520Finally%252C%2520based%2520on%2520the%2520mapping%2520between%2520local%250Afeatures%2520on%2520the%25202D%2520BEV%2520image%2520and%2520the%2520point%2520cloud%252C%2520a%2520robust%2520global%2520pose%250Aestimator%2520is%2520derived%2520that%2520determines%2520the%2520global%2520minimum%2520of%2520the%2520global%2520pose%2520on%250ASE%25283%2529%2520without%2520requiring%2520additional%2520registration.%2520To%2520validate%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520uniform%2520LGL%252C%2520extensive%2520benchmarks%2520are%2520conducted%2520in%2520real-world%250Aenvironments%252C%2520and%2520the%2520results%2520show%2520that%2520the%2520proposed%2520UniLGL%2520is%2520demonstratively%250Acompetitive%2520compared%2520to%2520other%2520State-of-the-Art%2520LGL%2520methods.%2520Furthermore%252C%2520UniLGL%250Ahas%2520been%2520deployed%2520on%2520diverse%2520platforms%252C%2520including%2520full-size%2520trucks%2520and%2520agile%250AMicro%2520Aerial%2520Vehicles%2520%2528MAVs%2529%252C%2520to%2520enable%2520high-precision%2520localization%2520and%2520mapping%250Aas%2520well%2520as%2520multi-MAV%2520collaborative%2520exploration%2520in%2520port%2520and%2520forest%2520environments%252C%250Ademonstrating%2520the%2520applicability%2520of%2520UniLGL%2520in%2520industrial%2520and%2520field%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12194v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniLGL%3A%20Learning%20Uniform%20Place%20Recognition%20for%20FOV-limited/Panoramic%0A%20%20LiDAR%20Global%20Localization&entry.906535625=Hongming%20Shen%20and%20Xun%20Chen%20and%20Yulin%20Hui%20and%20Zhenyu%20Wu%20and%20Wei%20Wang%20and%20Qiyang%20Lyu%20and%20Tianchen%20Deng%20and%20Danwei%20Wang&entry.1292438233=%20%20Existing%20LGL%20methods%20typically%20consider%20only%20partial%20information%20%28e.g.%2C%0Ageometric%20features%29%20from%20LiDAR%20observations%20or%20are%20designed%20for%20homogeneous%0ALiDAR%20sensors%2C%20overlooking%20the%20uniformity%20in%20LGL.%20In%20this%20work%2C%20a%20uniform%20LGL%0Amethod%20is%20proposed%2C%20termed%20UniLGL%2C%20which%20simultaneously%20achieves%20spatial%20and%0Amaterial%20uniformity%2C%20as%20well%20as%20sensor-type%20uniformity.%20The%20key%20idea%20of%20the%0Aproposed%20method%20is%20to%20encode%20the%20complete%20point%20cloud%2C%20which%20contains%20both%0Ageometric%20and%20material%20information%2C%20into%20a%20pair%20of%20BEV%20images%20%28i.e.%2C%20a%20spatial%0ABEV%20image%20and%20an%20intensity%20BEV%20image%29.%20An%20end-to-end%20multi-BEV%20fusion%20network%0Ais%20designed%20to%20extract%20uniform%20features%2C%20equipping%20UniLGL%20with%20spatial%20and%0Amaterial%20uniformity.%20To%20ensure%20robust%20LGL%20across%20heterogeneous%20LiDAR%20sensors%2C%20a%0Aviewpoint%20invariance%20hypothesis%20is%20introduced%2C%20which%20replaces%20the%20conventional%0Atranslation%20equivariance%20assumption%20commonly%20used%20in%20existing%20LPR%20networks%20and%0Asupervises%20UniLGL%20to%20achieve%20sensor-type%20uniformity%20in%20both%20global%20descriptors%0Aand%20local%20feature%20representations.%20Finally%2C%20based%20on%20the%20mapping%20between%20local%0Afeatures%20on%20the%202D%20BEV%20image%20and%20the%20point%20cloud%2C%20a%20robust%20global%20pose%0Aestimator%20is%20derived%20that%20determines%20the%20global%20minimum%20of%20the%20global%20pose%20on%0ASE%283%29%20without%20requiring%20additional%20registration.%20To%20validate%20the%20effectiveness%0Aof%20the%20proposed%20uniform%20LGL%2C%20extensive%20benchmarks%20are%20conducted%20in%20real-world%0Aenvironments%2C%20and%20the%20results%20show%20that%20the%20proposed%20UniLGL%20is%20demonstratively%0Acompetitive%20compared%20to%20other%20State-of-the-Art%20LGL%20methods.%20Furthermore%2C%20UniLGL%0Ahas%20been%20deployed%20on%20diverse%20platforms%2C%20including%20full-size%20trucks%20and%20agile%0AMicro%20Aerial%20Vehicles%20%28MAVs%29%2C%20to%20enable%20high-precision%20localization%20and%20mapping%0Aas%20well%20as%20multi-MAV%20collaborative%20exploration%20in%20port%20and%20forest%20environments%2C%0Ademonstrating%20the%20applicability%20of%20UniLGL%20in%20industrial%20and%20field%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.12194v2&entry.124074799=Read"},
{"title": "Collaborative Perceiver: Elevating Vision-based 3D Object Detection via\n  Local Density-Aware Spatial Occupancy", "author": "Jicheng Yuan and Manh Nguyen Duc and Qian Liu and Manfred Hauswirth and Danh Le Phuoc", "abstract": "  Vision-based bird's-eye-view (BEV) 3D object detection has advanced\nsignificantly in autonomous driving by offering cost-effectiveness and rich\ncontextual information. However, existing methods often construct BEV\nrepresentations by collapsing extracted object features, neglecting intrinsic\nenvironmental contexts, such as roads and pavements. This hinders detectors\nfrom comprehensively perceiving the characteristics of the physical world. To\nalleviate this, we introduce a multi-task learning framework, Collaborative\nPerceiver (CoP), that leverages spatial occupancy as auxiliary information to\nmine consistent structural and conceptual similarities shared between 3D object\ndetection and occupancy prediction tasks, bridging gaps in spatial\nrepresentations and feature refinement. To this end, we first propose a\npipeline to generate dense occupancy ground truths incorporating local density\ninformation (LDO) for reconstructing detailed environmental information. Next,\nwe employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained\nlocal features according to distinct object properties. Furthermore, we develop\na global-local collaborative feature fusion (CFF) module that seamlessly\nintegrates complementary knowledge between both tasks, thus composing more\nrobust BEV representations. Extensive experiments on the nuScenes benchmark\ndemonstrate that CoP outperforms existing vision-based frameworks, achieving\n49.5\\% mAP and 59.2\\% NDS on the test set. Code and supplementary materials are\navailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.\n", "link": "http://arxiv.org/abs/2507.21358v3", "date": "2025-07-31", "relevancy": 3.0559, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6186}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Perceiver%3A%20Elevating%20Vision-based%203D%20Object%20Detection%20via%0A%20%20Local%20Density-Aware%20Spatial%20Occupancy&body=Title%3A%20Collaborative%20Perceiver%3A%20Elevating%20Vision-based%203D%20Object%20Detection%20via%0A%20%20Local%20Density-Aware%20Spatial%20Occupancy%0AAuthor%3A%20Jicheng%20Yuan%20and%20Manh%20Nguyen%20Duc%20and%20Qian%20Liu%20and%20Manfred%20Hauswirth%20and%20Danh%20Le%20Phuoc%0AAbstract%3A%20%20%20Vision-based%20bird%27s-eye-view%20%28BEV%29%203D%20object%20detection%20has%20advanced%0Asignificantly%20in%20autonomous%20driving%20by%20offering%20cost-effectiveness%20and%20rich%0Acontextual%20information.%20However%2C%20existing%20methods%20often%20construct%20BEV%0Arepresentations%20by%20collapsing%20extracted%20object%20features%2C%20neglecting%20intrinsic%0Aenvironmental%20contexts%2C%20such%20as%20roads%20and%20pavements.%20This%20hinders%20detectors%0Afrom%20comprehensively%20perceiving%20the%20characteristics%20of%20the%20physical%20world.%20To%0Aalleviate%20this%2C%20we%20introduce%20a%20multi-task%20learning%20framework%2C%20Collaborative%0APerceiver%20%28CoP%29%2C%20that%20leverages%20spatial%20occupancy%20as%20auxiliary%20information%20to%0Amine%20consistent%20structural%20and%20conceptual%20similarities%20shared%20between%203D%20object%0Adetection%20and%20occupancy%20prediction%20tasks%2C%20bridging%20gaps%20in%20spatial%0Arepresentations%20and%20feature%20refinement.%20To%20this%20end%2C%20we%20first%20propose%20a%0Apipeline%20to%20generate%20dense%20occupancy%20ground%20truths%20incorporating%20local%20density%0Ainformation%20%28LDO%29%20for%20reconstructing%20detailed%20environmental%20information.%20Next%2C%0Awe%20employ%20a%20voxel-height-guided%20sampling%20%28VHS%29%20strategy%20to%20distill%20fine-grained%0Alocal%20features%20according%20to%20distinct%20object%20properties.%20Furthermore%2C%20we%20develop%0Aa%20global-local%20collaborative%20feature%20fusion%20%28CFF%29%20module%20that%20seamlessly%0Aintegrates%20complementary%20knowledge%20between%20both%20tasks%2C%20thus%20composing%20more%0Arobust%20BEV%20representations.%20Extensive%20experiments%20on%20the%20nuScenes%20benchmark%0Ademonstrate%20that%20CoP%20outperforms%20existing%20vision-based%20frameworks%2C%20achieving%0A49.5%5C%25%20mAP%20and%2059.2%5C%25%20NDS%20on%20the%20test%20set.%20Code%20and%20supplementary%20materials%20are%0Aavailable%20at%20this%20link%20https%3A//github.com/jichengyuan/Collaborative-Perceiver.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21358v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Perceiver%253A%2520Elevating%2520Vision-based%25203D%2520Object%2520Detection%2520via%250A%2520%2520Local%2520Density-Aware%2520Spatial%2520Occupancy%26entry.906535625%3DJicheng%2520Yuan%2520and%2520Manh%2520Nguyen%2520Duc%2520and%2520Qian%2520Liu%2520and%2520Manfred%2520Hauswirth%2520and%2520Danh%2520Le%2520Phuoc%26entry.1292438233%3D%2520%2520Vision-based%2520bird%2527s-eye-view%2520%2528BEV%2529%25203D%2520object%2520detection%2520has%2520advanced%250Asignificantly%2520in%2520autonomous%2520driving%2520by%2520offering%2520cost-effectiveness%2520and%2520rich%250Acontextual%2520information.%2520However%252C%2520existing%2520methods%2520often%2520construct%2520BEV%250Arepresentations%2520by%2520collapsing%2520extracted%2520object%2520features%252C%2520neglecting%2520intrinsic%250Aenvironmental%2520contexts%252C%2520such%2520as%2520roads%2520and%2520pavements.%2520This%2520hinders%2520detectors%250Afrom%2520comprehensively%2520perceiving%2520the%2520characteristics%2520of%2520the%2520physical%2520world.%2520To%250Aalleviate%2520this%252C%2520we%2520introduce%2520a%2520multi-task%2520learning%2520framework%252C%2520Collaborative%250APerceiver%2520%2528CoP%2529%252C%2520that%2520leverages%2520spatial%2520occupancy%2520as%2520auxiliary%2520information%2520to%250Amine%2520consistent%2520structural%2520and%2520conceptual%2520similarities%2520shared%2520between%25203D%2520object%250Adetection%2520and%2520occupancy%2520prediction%2520tasks%252C%2520bridging%2520gaps%2520in%2520spatial%250Arepresentations%2520and%2520feature%2520refinement.%2520To%2520this%2520end%252C%2520we%2520first%2520propose%2520a%250Apipeline%2520to%2520generate%2520dense%2520occupancy%2520ground%2520truths%2520incorporating%2520local%2520density%250Ainformation%2520%2528LDO%2529%2520for%2520reconstructing%2520detailed%2520environmental%2520information.%2520Next%252C%250Awe%2520employ%2520a%2520voxel-height-guided%2520sampling%2520%2528VHS%2529%2520strategy%2520to%2520distill%2520fine-grained%250Alocal%2520features%2520according%2520to%2520distinct%2520object%2520properties.%2520Furthermore%252C%2520we%2520develop%250Aa%2520global-local%2520collaborative%2520feature%2520fusion%2520%2528CFF%2529%2520module%2520that%2520seamlessly%250Aintegrates%2520complementary%2520knowledge%2520between%2520both%2520tasks%252C%2520thus%2520composing%2520more%250Arobust%2520BEV%2520representations.%2520Extensive%2520experiments%2520on%2520the%2520nuScenes%2520benchmark%250Ademonstrate%2520that%2520CoP%2520outperforms%2520existing%2520vision-based%2520frameworks%252C%2520achieving%250A49.5%255C%2525%2520mAP%2520and%252059.2%255C%2525%2520NDS%2520on%2520the%2520test%2520set.%2520Code%2520and%2520supplementary%2520materials%2520are%250Aavailable%2520at%2520this%2520link%2520https%253A//github.com/jichengyuan/Collaborative-Perceiver.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21358v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Perceiver%3A%20Elevating%20Vision-based%203D%20Object%20Detection%20via%0A%20%20Local%20Density-Aware%20Spatial%20Occupancy&entry.906535625=Jicheng%20Yuan%20and%20Manh%20Nguyen%20Duc%20and%20Qian%20Liu%20and%20Manfred%20Hauswirth%20and%20Danh%20Le%20Phuoc&entry.1292438233=%20%20Vision-based%20bird%27s-eye-view%20%28BEV%29%203D%20object%20detection%20has%20advanced%0Asignificantly%20in%20autonomous%20driving%20by%20offering%20cost-effectiveness%20and%20rich%0Acontextual%20information.%20However%2C%20existing%20methods%20often%20construct%20BEV%0Arepresentations%20by%20collapsing%20extracted%20object%20features%2C%20neglecting%20intrinsic%0Aenvironmental%20contexts%2C%20such%20as%20roads%20and%20pavements.%20This%20hinders%20detectors%0Afrom%20comprehensively%20perceiving%20the%20characteristics%20of%20the%20physical%20world.%20To%0Aalleviate%20this%2C%20we%20introduce%20a%20multi-task%20learning%20framework%2C%20Collaborative%0APerceiver%20%28CoP%29%2C%20that%20leverages%20spatial%20occupancy%20as%20auxiliary%20information%20to%0Amine%20consistent%20structural%20and%20conceptual%20similarities%20shared%20between%203D%20object%0Adetection%20and%20occupancy%20prediction%20tasks%2C%20bridging%20gaps%20in%20spatial%0Arepresentations%20and%20feature%20refinement.%20To%20this%20end%2C%20we%20first%20propose%20a%0Apipeline%20to%20generate%20dense%20occupancy%20ground%20truths%20incorporating%20local%20density%0Ainformation%20%28LDO%29%20for%20reconstructing%20detailed%20environmental%20information.%20Next%2C%0Awe%20employ%20a%20voxel-height-guided%20sampling%20%28VHS%29%20strategy%20to%20distill%20fine-grained%0Alocal%20features%20according%20to%20distinct%20object%20properties.%20Furthermore%2C%20we%20develop%0Aa%20global-local%20collaborative%20feature%20fusion%20%28CFF%29%20module%20that%20seamlessly%0Aintegrates%20complementary%20knowledge%20between%20both%20tasks%2C%20thus%20composing%20more%0Arobust%20BEV%20representations.%20Extensive%20experiments%20on%20the%20nuScenes%20benchmark%0Ademonstrate%20that%20CoP%20outperforms%20existing%20vision-based%20frameworks%2C%20achieving%0A49.5%5C%25%20mAP%20and%2059.2%5C%25%20NDS%20on%20the%20test%20set.%20Code%20and%20supplementary%20materials%20are%0Aavailable%20at%20this%20link%20https%3A//github.com/jichengyuan/Collaborative-Perceiver.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21358v3&entry.124074799=Read"},
{"title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for\n  Enhanced Visual Instruction Tuning", "author": "Federico Cocchi and Nicholas Moratelli and Davide Caffagni and Sara Sarto and Lorenzo Baraldi and Marcella Cornia and Rita Cucchiara", "abstract": "  Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.\n", "link": "http://arxiv.org/abs/2503.15621v2", "date": "2025-07-31", "relevancy": 3.0194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6097}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaVA-MORE%3A%20A%20Comparative%20Study%20of%20LLMs%20and%20Visual%20Backbones%20for%0A%20%20Enhanced%20Visual%20Instruction%20Tuning&body=Title%3A%20LLaVA-MORE%3A%20A%20Comparative%20Study%20of%20LLMs%20and%20Visual%20Backbones%20for%0A%20%20Enhanced%20Visual%20Instruction%20Tuning%0AAuthor%3A%20Federico%20Cocchi%20and%20Nicholas%20Moratelli%20and%20Davide%20Caffagni%20and%20Sara%20Sarto%20and%20Lorenzo%20Baraldi%20and%20Marcella%20Cornia%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20highlighted%0Athe%20critical%20roles%20of%20both%20the%20visual%20backbone%20and%20the%20underlying%20language%0Amodel.%20While%20prior%20work%20has%20primarily%20focused%20on%20scaling%20these%20components%20to%0Abillions%20of%20parameters%2C%20the%20trade-offs%20between%20model%20size%2C%20architecture%2C%20and%0Aperformance%20remain%20underexplored.%20Additionally%2C%20inconsistencies%20in%20training%0Adata%20and%20evaluation%20protocols%20have%20hindered%20direct%20comparisons%2C%20making%20it%0Adifficult%20to%20derive%20optimal%20design%20choices.%20In%20this%20paper%2C%20we%20introduce%0ALLaVA-MORE%2C%20a%20new%20family%20of%20MLLMs%20that%20integrates%20recent%20language%20models%20with%0Adiverse%20visual%20backbones.%20To%20ensure%20fair%20comparisons%2C%20we%20employ%20a%20unified%0Atraining%20protocol%20applied%20consistently%20across%20all%20architectures.%20Our%20analysis%0Asystematically%20explores%20both%20small-%20and%20medium-scale%20LLMs%20--%20including%20Phi-4%2C%0ALLaMA-3.1%2C%20and%20Gemma-2%20--%20to%20evaluate%20multimodal%20reasoning%2C%20generation%2C%20and%0Ainstruction%20following%2C%20while%20examining%20the%20relationship%20between%20model%20size%20and%0Aperformance.%20Beyond%20evaluating%20the%20LLM%20impact%20on%20final%20results%2C%20we%20conduct%20a%0Acomprehensive%20study%20of%20various%20visual%20encoders%2C%20ranging%20from%20CLIP-based%0Aarchitectures%20to%20alternatives%20such%20as%20DINOv2%2C%20SigLIP%2C%20and%20SigLIP2.%20Additional%0Aexperiments%20investigate%20the%20effects%20of%20increased%20image%20resolution%20and%0Avariations%20in%20pre-training%20datasets.%20Overall%2C%20our%20results%20provide%20insights%20into%0Athe%20design%20of%20more%20effective%20MLLMs%2C%20offering%20a%20reproducible%20evaluation%0Aframework%20that%20facilitates%20direct%20comparisons%20and%20can%20guide%20future%20model%0Adevelopment.%20Our%20source%20code%20and%20trained%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/aimagelab/LLaVA-MORE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.15621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaVA-MORE%253A%2520A%2520Comparative%2520Study%2520of%2520LLMs%2520and%2520Visual%2520Backbones%2520for%250A%2520%2520Enhanced%2520Visual%2520Instruction%2520Tuning%26entry.906535625%3DFederico%2520Cocchi%2520and%2520Nicholas%2520Moratelli%2520and%2520Davide%2520Caffagni%2520and%2520Sara%2520Sarto%2520and%2520Lorenzo%2520Baraldi%2520and%2520Marcella%2520Cornia%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520highlighted%250Athe%2520critical%2520roles%2520of%2520both%2520the%2520visual%2520backbone%2520and%2520the%2520underlying%2520language%250Amodel.%2520While%2520prior%2520work%2520has%2520primarily%2520focused%2520on%2520scaling%2520these%2520components%2520to%250Abillions%2520of%2520parameters%252C%2520the%2520trade-offs%2520between%2520model%2520size%252C%2520architecture%252C%2520and%250Aperformance%2520remain%2520underexplored.%2520Additionally%252C%2520inconsistencies%2520in%2520training%250Adata%2520and%2520evaluation%2520protocols%2520have%2520hindered%2520direct%2520comparisons%252C%2520making%2520it%250Adifficult%2520to%2520derive%2520optimal%2520design%2520choices.%2520In%2520this%2520paper%252C%2520we%2520introduce%250ALLaVA-MORE%252C%2520a%2520new%2520family%2520of%2520MLLMs%2520that%2520integrates%2520recent%2520language%2520models%2520with%250Adiverse%2520visual%2520backbones.%2520To%2520ensure%2520fair%2520comparisons%252C%2520we%2520employ%2520a%2520unified%250Atraining%2520protocol%2520applied%2520consistently%2520across%2520all%2520architectures.%2520Our%2520analysis%250Asystematically%2520explores%2520both%2520small-%2520and%2520medium-scale%2520LLMs%2520--%2520including%2520Phi-4%252C%250ALLaMA-3.1%252C%2520and%2520Gemma-2%2520--%2520to%2520evaluate%2520multimodal%2520reasoning%252C%2520generation%252C%2520and%250Ainstruction%2520following%252C%2520while%2520examining%2520the%2520relationship%2520between%2520model%2520size%2520and%250Aperformance.%2520Beyond%2520evaluating%2520the%2520LLM%2520impact%2520on%2520final%2520results%252C%2520we%2520conduct%2520a%250Acomprehensive%2520study%2520of%2520various%2520visual%2520encoders%252C%2520ranging%2520from%2520CLIP-based%250Aarchitectures%2520to%2520alternatives%2520such%2520as%2520DINOv2%252C%2520SigLIP%252C%2520and%2520SigLIP2.%2520Additional%250Aexperiments%2520investigate%2520the%2520effects%2520of%2520increased%2520image%2520resolution%2520and%250Avariations%2520in%2520pre-training%2520datasets.%2520Overall%252C%2520our%2520results%2520provide%2520insights%2520into%250Athe%2520design%2520of%2520more%2520effective%2520MLLMs%252C%2520offering%2520a%2520reproducible%2520evaluation%250Aframework%2520that%2520facilitates%2520direct%2520comparisons%2520and%2520can%2520guide%2520future%2520model%250Adevelopment.%2520Our%2520source%2520code%2520and%2520trained%2520models%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//github.com/aimagelab/LLaVA-MORE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.15621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-MORE%3A%20A%20Comparative%20Study%20of%20LLMs%20and%20Visual%20Backbones%20for%0A%20%20Enhanced%20Visual%20Instruction%20Tuning&entry.906535625=Federico%20Cocchi%20and%20Nicholas%20Moratelli%20and%20Davide%20Caffagni%20and%20Sara%20Sarto%20and%20Lorenzo%20Baraldi%20and%20Marcella%20Cornia%20and%20Rita%20Cucchiara&entry.1292438233=%20%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20highlighted%0Athe%20critical%20roles%20of%20both%20the%20visual%20backbone%20and%20the%20underlying%20language%0Amodel.%20While%20prior%20work%20has%20primarily%20focused%20on%20scaling%20these%20components%20to%0Abillions%20of%20parameters%2C%20the%20trade-offs%20between%20model%20size%2C%20architecture%2C%20and%0Aperformance%20remain%20underexplored.%20Additionally%2C%20inconsistencies%20in%20training%0Adata%20and%20evaluation%20protocols%20have%20hindered%20direct%20comparisons%2C%20making%20it%0Adifficult%20to%20derive%20optimal%20design%20choices.%20In%20this%20paper%2C%20we%20introduce%0ALLaVA-MORE%2C%20a%20new%20family%20of%20MLLMs%20that%20integrates%20recent%20language%20models%20with%0Adiverse%20visual%20backbones.%20To%20ensure%20fair%20comparisons%2C%20we%20employ%20a%20unified%0Atraining%20protocol%20applied%20consistently%20across%20all%20architectures.%20Our%20analysis%0Asystematically%20explores%20both%20small-%20and%20medium-scale%20LLMs%20--%20including%20Phi-4%2C%0ALLaMA-3.1%2C%20and%20Gemma-2%20--%20to%20evaluate%20multimodal%20reasoning%2C%20generation%2C%20and%0Ainstruction%20following%2C%20while%20examining%20the%20relationship%20between%20model%20size%20and%0Aperformance.%20Beyond%20evaluating%20the%20LLM%20impact%20on%20final%20results%2C%20we%20conduct%20a%0Acomprehensive%20study%20of%20various%20visual%20encoders%2C%20ranging%20from%20CLIP-based%0Aarchitectures%20to%20alternatives%20such%20as%20DINOv2%2C%20SigLIP%2C%20and%20SigLIP2.%20Additional%0Aexperiments%20investigate%20the%20effects%20of%20increased%20image%20resolution%20and%0Avariations%20in%20pre-training%20datasets.%20Overall%2C%20our%20results%20provide%20insights%20into%0Athe%20design%20of%20more%20effective%20MLLMs%2C%20offering%20a%20reproducible%20evaluation%0Aframework%20that%20facilitates%20direct%20comparisons%20and%20can%20guide%20future%20model%0Adevelopment.%20Our%20source%20code%20and%20trained%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//github.com/aimagelab/LLaVA-MORE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.15621v2&entry.124074799=Read"},
{"title": "SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single\n  Image", "author": "Dimitrije Anti\u0107 and Georgios Paschalidis and Shashank Tripathi and Theo Gevers and Sai Kumar Dwivedi and Dimitrios Tzionas", "abstract": "  Recovering 3D object pose and shape from a single image is a challenging and\nill-posed problem. This is due to strong (self-)occlusions, depth ambiguities,\nthe vast intra- and inter-class shape variance, and the lack of 3D ground truth\nfor natural images. Existing deep-network methods are trained on synthetic\ndatasets to predict 3D shapes, so they often struggle generalizing to\nreal-world images. Moreover, they lack an explicit feedback loop for refining\nnoisy estimates, and primarily focus on geometry without directly considering\npixel alignment. To tackle these limitations, we develop a novel\nrender-and-compare optimization framework, called SDFit. This has three key\ninnovations: First, it uses a learned category-specific and morphable\nsigned-distance-function (mSDF) model, and fits this to an image by iteratively\nrefining both 3D pose and shape. The mSDF robustifies inference by constraining\nthe search on the manifold of valid shapes, while allowing for arbitrary shape\ntopologies. Second, SDFit retrieves an initial 3D shape that likely matches the\nimage, by exploiting foundational models for efficient look-up into 3D shape\ndatabases. Third, SDFit initializes pose by establishing rich 2D-3D\ncorrespondences between the image and the mSDF through foundational features.\nWe evaluate SDFit on three image datasets, i.e., Pix3D, Pascal3D+, and COMIC.\nSDFit performs on par with SotA feed-forward networks for unoccluded images and\ncommon poses, but is uniquely robust to occlusions and uncommon poses.\nMoreover, it requires no retraining for unseen images. Thus, SDFit contributes\nnew insights for generalizing in the wild. Code is available at\nhttps://anticdimi.github.io/sdfit.\n", "link": "http://arxiv.org/abs/2409.16178v3", "date": "2025-07-31", "relevancy": 2.9015, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5889}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5872}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDFit%3A%203D%20Object%20Pose%20and%20Shape%20by%20Fitting%20a%20Morphable%20SDF%20to%20a%20Single%0A%20%20Image&body=Title%3A%20SDFit%3A%203D%20Object%20Pose%20and%20Shape%20by%20Fitting%20a%20Morphable%20SDF%20to%20a%20Single%0A%20%20Image%0AAuthor%3A%20Dimitrije%20Anti%C4%87%20and%20Georgios%20Paschalidis%20and%20Shashank%20Tripathi%20and%20Theo%20Gevers%20and%20Sai%20Kumar%20Dwivedi%20and%20Dimitrios%20Tzionas%0AAbstract%3A%20%20%20Recovering%203D%20object%20pose%20and%20shape%20from%20a%20single%20image%20is%20a%20challenging%20and%0Aill-posed%20problem.%20This%20is%20due%20to%20strong%20%28self-%29occlusions%2C%20depth%20ambiguities%2C%0Athe%20vast%20intra-%20and%20inter-class%20shape%20variance%2C%20and%20the%20lack%20of%203D%20ground%20truth%0Afor%20natural%20images.%20Existing%20deep-network%20methods%20are%20trained%20on%20synthetic%0Adatasets%20to%20predict%203D%20shapes%2C%20so%20they%20often%20struggle%20generalizing%20to%0Areal-world%20images.%20Moreover%2C%20they%20lack%20an%20explicit%20feedback%20loop%20for%20refining%0Anoisy%20estimates%2C%20and%20primarily%20focus%20on%20geometry%20without%20directly%20considering%0Apixel%20alignment.%20To%20tackle%20these%20limitations%2C%20we%20develop%20a%20novel%0Arender-and-compare%20optimization%20framework%2C%20called%20SDFit.%20This%20has%20three%20key%0Ainnovations%3A%20First%2C%20it%20uses%20a%20learned%20category-specific%20and%20morphable%0Asigned-distance-function%20%28mSDF%29%20model%2C%20and%20fits%20this%20to%20an%20image%20by%20iteratively%0Arefining%20both%203D%20pose%20and%20shape.%20The%20mSDF%20robustifies%20inference%20by%20constraining%0Athe%20search%20on%20the%20manifold%20of%20valid%20shapes%2C%20while%20allowing%20for%20arbitrary%20shape%0Atopologies.%20Second%2C%20SDFit%20retrieves%20an%20initial%203D%20shape%20that%20likely%20matches%20the%0Aimage%2C%20by%20exploiting%20foundational%20models%20for%20efficient%20look-up%20into%203D%20shape%0Adatabases.%20Third%2C%20SDFit%20initializes%20pose%20by%20establishing%20rich%202D-3D%0Acorrespondences%20between%20the%20image%20and%20the%20mSDF%20through%20foundational%20features.%0AWe%20evaluate%20SDFit%20on%20three%20image%20datasets%2C%20i.e.%2C%20Pix3D%2C%20Pascal3D%2B%2C%20and%20COMIC.%0ASDFit%20performs%20on%20par%20with%20SotA%20feed-forward%20networks%20for%20unoccluded%20images%20and%0Acommon%20poses%2C%20but%20is%20uniquely%20robust%20to%20occlusions%20and%20uncommon%20poses.%0AMoreover%2C%20it%20requires%20no%20retraining%20for%20unseen%20images.%20Thus%2C%20SDFit%20contributes%0Anew%20insights%20for%20generalizing%20in%20the%20wild.%20Code%20is%20available%20at%0Ahttps%3A//anticdimi.github.io/sdfit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16178v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDFit%253A%25203D%2520Object%2520Pose%2520and%2520Shape%2520by%2520Fitting%2520a%2520Morphable%2520SDF%2520to%2520a%2520Single%250A%2520%2520Image%26entry.906535625%3DDimitrije%2520Anti%25C4%2587%2520and%2520Georgios%2520Paschalidis%2520and%2520Shashank%2520Tripathi%2520and%2520Theo%2520Gevers%2520and%2520Sai%2520Kumar%2520Dwivedi%2520and%2520Dimitrios%2520Tzionas%26entry.1292438233%3D%2520%2520Recovering%25203D%2520object%2520pose%2520and%2520shape%2520from%2520a%2520single%2520image%2520is%2520a%2520challenging%2520and%250Aill-posed%2520problem.%2520This%2520is%2520due%2520to%2520strong%2520%2528self-%2529occlusions%252C%2520depth%2520ambiguities%252C%250Athe%2520vast%2520intra-%2520and%2520inter-class%2520shape%2520variance%252C%2520and%2520the%2520lack%2520of%25203D%2520ground%2520truth%250Afor%2520natural%2520images.%2520Existing%2520deep-network%2520methods%2520are%2520trained%2520on%2520synthetic%250Adatasets%2520to%2520predict%25203D%2520shapes%252C%2520so%2520they%2520often%2520struggle%2520generalizing%2520to%250Areal-world%2520images.%2520Moreover%252C%2520they%2520lack%2520an%2520explicit%2520feedback%2520loop%2520for%2520refining%250Anoisy%2520estimates%252C%2520and%2520primarily%2520focus%2520on%2520geometry%2520without%2520directly%2520considering%250Apixel%2520alignment.%2520To%2520tackle%2520these%2520limitations%252C%2520we%2520develop%2520a%2520novel%250Arender-and-compare%2520optimization%2520framework%252C%2520called%2520SDFit.%2520This%2520has%2520three%2520key%250Ainnovations%253A%2520First%252C%2520it%2520uses%2520a%2520learned%2520category-specific%2520and%2520morphable%250Asigned-distance-function%2520%2528mSDF%2529%2520model%252C%2520and%2520fits%2520this%2520to%2520an%2520image%2520by%2520iteratively%250Arefining%2520both%25203D%2520pose%2520and%2520shape.%2520The%2520mSDF%2520robustifies%2520inference%2520by%2520constraining%250Athe%2520search%2520on%2520the%2520manifold%2520of%2520valid%2520shapes%252C%2520while%2520allowing%2520for%2520arbitrary%2520shape%250Atopologies.%2520Second%252C%2520SDFit%2520retrieves%2520an%2520initial%25203D%2520shape%2520that%2520likely%2520matches%2520the%250Aimage%252C%2520by%2520exploiting%2520foundational%2520models%2520for%2520efficient%2520look-up%2520into%25203D%2520shape%250Adatabases.%2520Third%252C%2520SDFit%2520initializes%2520pose%2520by%2520establishing%2520rich%25202D-3D%250Acorrespondences%2520between%2520the%2520image%2520and%2520the%2520mSDF%2520through%2520foundational%2520features.%250AWe%2520evaluate%2520SDFit%2520on%2520three%2520image%2520datasets%252C%2520i.e.%252C%2520Pix3D%252C%2520Pascal3D%252B%252C%2520and%2520COMIC.%250ASDFit%2520performs%2520on%2520par%2520with%2520SotA%2520feed-forward%2520networks%2520for%2520unoccluded%2520images%2520and%250Acommon%2520poses%252C%2520but%2520is%2520uniquely%2520robust%2520to%2520occlusions%2520and%2520uncommon%2520poses.%250AMoreover%252C%2520it%2520requires%2520no%2520retraining%2520for%2520unseen%2520images.%2520Thus%252C%2520SDFit%2520contributes%250Anew%2520insights%2520for%2520generalizing%2520in%2520the%2520wild.%2520Code%2520is%2520available%2520at%250Ahttps%253A//anticdimi.github.io/sdfit.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16178v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDFit%3A%203D%20Object%20Pose%20and%20Shape%20by%20Fitting%20a%20Morphable%20SDF%20to%20a%20Single%0A%20%20Image&entry.906535625=Dimitrije%20Anti%C4%87%20and%20Georgios%20Paschalidis%20and%20Shashank%20Tripathi%20and%20Theo%20Gevers%20and%20Sai%20Kumar%20Dwivedi%20and%20Dimitrios%20Tzionas&entry.1292438233=%20%20Recovering%203D%20object%20pose%20and%20shape%20from%20a%20single%20image%20is%20a%20challenging%20and%0Aill-posed%20problem.%20This%20is%20due%20to%20strong%20%28self-%29occlusions%2C%20depth%20ambiguities%2C%0Athe%20vast%20intra-%20and%20inter-class%20shape%20variance%2C%20and%20the%20lack%20of%203D%20ground%20truth%0Afor%20natural%20images.%20Existing%20deep-network%20methods%20are%20trained%20on%20synthetic%0Adatasets%20to%20predict%203D%20shapes%2C%20so%20they%20often%20struggle%20generalizing%20to%0Areal-world%20images.%20Moreover%2C%20they%20lack%20an%20explicit%20feedback%20loop%20for%20refining%0Anoisy%20estimates%2C%20and%20primarily%20focus%20on%20geometry%20without%20directly%20considering%0Apixel%20alignment.%20To%20tackle%20these%20limitations%2C%20we%20develop%20a%20novel%0Arender-and-compare%20optimization%20framework%2C%20called%20SDFit.%20This%20has%20three%20key%0Ainnovations%3A%20First%2C%20it%20uses%20a%20learned%20category-specific%20and%20morphable%0Asigned-distance-function%20%28mSDF%29%20model%2C%20and%20fits%20this%20to%20an%20image%20by%20iteratively%0Arefining%20both%203D%20pose%20and%20shape.%20The%20mSDF%20robustifies%20inference%20by%20constraining%0Athe%20search%20on%20the%20manifold%20of%20valid%20shapes%2C%20while%20allowing%20for%20arbitrary%20shape%0Atopologies.%20Second%2C%20SDFit%20retrieves%20an%20initial%203D%20shape%20that%20likely%20matches%20the%0Aimage%2C%20by%20exploiting%20foundational%20models%20for%20efficient%20look-up%20into%203D%20shape%0Adatabases.%20Third%2C%20SDFit%20initializes%20pose%20by%20establishing%20rich%202D-3D%0Acorrespondences%20between%20the%20image%20and%20the%20mSDF%20through%20foundational%20features.%0AWe%20evaluate%20SDFit%20on%20three%20image%20datasets%2C%20i.e.%2C%20Pix3D%2C%20Pascal3D%2B%2C%20and%20COMIC.%0ASDFit%20performs%20on%20par%20with%20SotA%20feed-forward%20networks%20for%20unoccluded%20images%20and%0Acommon%20poses%2C%20but%20is%20uniquely%20robust%20to%20occlusions%20and%20uncommon%20poses.%0AMoreover%2C%20it%20requires%20no%20retraining%20for%20unseen%20images.%20Thus%2C%20SDFit%20contributes%0Anew%20insights%20for%20generalizing%20in%20the%20wild.%20Code%20is%20available%20at%0Ahttps%3A//anticdimi.github.io/sdfit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16178v3&entry.124074799=Read"},
{"title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and\n  Explanation", "author": "Haiquan Wen and Yiwei He and Zhenglin Huang and Tianxiao Li and Zihan Yu and Xingru Huang and Lu Qi and Baoyuan Wu and Xiangtai Li and Guangliang Cheng", "abstract": "  Advances in AI generative models facilitate super-realistic video synthesis,\namplifying misinformation risks via social media and eroding trust in digital\ncontent. Several research works have explored new deepfake detection methods on\nAI-generated images to alleviate these risks. However, with the fast\ndevelopment of video generation models, such as Sora and WanX, there is\ncurrently a lack of large-scale, high-quality AI-generated video datasets for\nforgery detection. In addition, existing detection approaches predominantly\ntreat the task as binary classification, lacking explainability in model\ndecision-making and failing to provide actionable insights or guidance for the\npublic. To address these challenges, we propose \\textbf{GenBuster-200K}, a\nlarge-scale AI-generated video dataset featuring 200K high-resolution video\nclips, diverse latest generative techniques, and real-world scenes. We further\nintroduce \\textbf{BusterX}, a novel AI-generated video detection and\nexplanation framework leveraging multimodal large language model (MLLM) and\nreinforcement learning for authenticity determination and explainable\nrationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}}\nlarge-scale, high-quality AI-generated video dataset that incorporates the\nlatest generative techniques for real-world scenarios. BusterX is the {\\it\n\\textbf{first}} framework to integrate MLLM with reinforcement learning for\nexplainable AI-generated video detection. Extensive comparisons with\nstate-of-the-art methods and ablation studies validate the effectiveness and\ngeneralizability of BusterX. The code, models, and datasets will be released.\n", "link": "http://arxiv.org/abs/2505.12620v4", "date": "2025-07-31", "relevancy": 2.8253, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5736}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5731}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BusterX%3A%20MLLM-Powered%20AI-Generated%20Video%20Forgery%20Detection%20and%0A%20%20Explanation&body=Title%3A%20BusterX%3A%20MLLM-Powered%20AI-Generated%20Video%20Forgery%20Detection%20and%0A%20%20Explanation%0AAuthor%3A%20Haiquan%20Wen%20and%20Yiwei%20He%20and%20Zhenglin%20Huang%20and%20Tianxiao%20Li%20and%20Zihan%20Yu%20and%20Xingru%20Huang%20and%20Lu%20Qi%20and%20Baoyuan%20Wu%20and%20Xiangtai%20Li%20and%20Guangliang%20Cheng%0AAbstract%3A%20%20%20Advances%20in%20AI%20generative%20models%20facilitate%20super-realistic%20video%20synthesis%2C%0Aamplifying%20misinformation%20risks%20via%20social%20media%20and%20eroding%20trust%20in%20digital%0Acontent.%20Several%20research%20works%20have%20explored%20new%20deepfake%20detection%20methods%20on%0AAI-generated%20images%20to%20alleviate%20these%20risks.%20However%2C%20with%20the%20fast%0Adevelopment%20of%20video%20generation%20models%2C%20such%20as%20Sora%20and%20WanX%2C%20there%20is%0Acurrently%20a%20lack%20of%20large-scale%2C%20high-quality%20AI-generated%20video%20datasets%20for%0Aforgery%20detection.%20In%20addition%2C%20existing%20detection%20approaches%20predominantly%0Atreat%20the%20task%20as%20binary%20classification%2C%20lacking%20explainability%20in%20model%0Adecision-making%20and%20failing%20to%20provide%20actionable%20insights%20or%20guidance%20for%20the%0Apublic.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BGenBuster-200K%7D%2C%20a%0Alarge-scale%20AI-generated%20video%20dataset%20featuring%20200K%20high-resolution%20video%0Aclips%2C%20diverse%20latest%20generative%20techniques%2C%20and%20real-world%20scenes.%20We%20further%0Aintroduce%20%5Ctextbf%7BBusterX%7D%2C%20a%20novel%20AI-generated%20video%20detection%20and%0Aexplanation%20framework%20leveraging%20multimodal%20large%20language%20model%20%28MLLM%29%20and%0Areinforcement%20learning%20for%20authenticity%20determination%20and%20explainable%0Arationale.%20To%20our%20knowledge%2C%20GenBuster-200K%20is%20the%20%7B%5Cit%20%5Ctextbf%7Bfirst%7D%7D%0Alarge-scale%2C%20high-quality%20AI-generated%20video%20dataset%20that%20incorporates%20the%0Alatest%20generative%20techniques%20for%20real-world%20scenarios.%20BusterX%20is%20the%20%7B%5Cit%0A%5Ctextbf%7Bfirst%7D%7D%20framework%20to%20integrate%20MLLM%20with%20reinforcement%20learning%20for%0Aexplainable%20AI-generated%20video%20detection.%20Extensive%20comparisons%20with%0Astate-of-the-art%20methods%20and%20ablation%20studies%20validate%20the%20effectiveness%20and%0Ageneralizability%20of%20BusterX.%20The%20code%2C%20models%2C%20and%20datasets%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.12620v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBusterX%253A%2520MLLM-Powered%2520AI-Generated%2520Video%2520Forgery%2520Detection%2520and%250A%2520%2520Explanation%26entry.906535625%3DHaiquan%2520Wen%2520and%2520Yiwei%2520He%2520and%2520Zhenglin%2520Huang%2520and%2520Tianxiao%2520Li%2520and%2520Zihan%2520Yu%2520and%2520Xingru%2520Huang%2520and%2520Lu%2520Qi%2520and%2520Baoyuan%2520Wu%2520and%2520Xiangtai%2520Li%2520and%2520Guangliang%2520Cheng%26entry.1292438233%3D%2520%2520Advances%2520in%2520AI%2520generative%2520models%2520facilitate%2520super-realistic%2520video%2520synthesis%252C%250Aamplifying%2520misinformation%2520risks%2520via%2520social%2520media%2520and%2520eroding%2520trust%2520in%2520digital%250Acontent.%2520Several%2520research%2520works%2520have%2520explored%2520new%2520deepfake%2520detection%2520methods%2520on%250AAI-generated%2520images%2520to%2520alleviate%2520these%2520risks.%2520However%252C%2520with%2520the%2520fast%250Adevelopment%2520of%2520video%2520generation%2520models%252C%2520such%2520as%2520Sora%2520and%2520WanX%252C%2520there%2520is%250Acurrently%2520a%2520lack%2520of%2520large-scale%252C%2520high-quality%2520AI-generated%2520video%2520datasets%2520for%250Aforgery%2520detection.%2520In%2520addition%252C%2520existing%2520detection%2520approaches%2520predominantly%250Atreat%2520the%2520task%2520as%2520binary%2520classification%252C%2520lacking%2520explainability%2520in%2520model%250Adecision-making%2520and%2520failing%2520to%2520provide%2520actionable%2520insights%2520or%2520guidance%2520for%2520the%250Apublic.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%255Ctextbf%257BGenBuster-200K%257D%252C%2520a%250Alarge-scale%2520AI-generated%2520video%2520dataset%2520featuring%2520200K%2520high-resolution%2520video%250Aclips%252C%2520diverse%2520latest%2520generative%2520techniques%252C%2520and%2520real-world%2520scenes.%2520We%2520further%250Aintroduce%2520%255Ctextbf%257BBusterX%257D%252C%2520a%2520novel%2520AI-generated%2520video%2520detection%2520and%250Aexplanation%2520framework%2520leveraging%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%2520and%250Areinforcement%2520learning%2520for%2520authenticity%2520determination%2520and%2520explainable%250Arationale.%2520To%2520our%2520knowledge%252C%2520GenBuster-200K%2520is%2520the%2520%257B%255Cit%2520%255Ctextbf%257Bfirst%257D%257D%250Alarge-scale%252C%2520high-quality%2520AI-generated%2520video%2520dataset%2520that%2520incorporates%2520the%250Alatest%2520generative%2520techniques%2520for%2520real-world%2520scenarios.%2520BusterX%2520is%2520the%2520%257B%255Cit%250A%255Ctextbf%257Bfirst%257D%257D%2520framework%2520to%2520integrate%2520MLLM%2520with%2520reinforcement%2520learning%2520for%250Aexplainable%2520AI-generated%2520video%2520detection.%2520Extensive%2520comparisons%2520with%250Astate-of-the-art%2520methods%2520and%2520ablation%2520studies%2520validate%2520the%2520effectiveness%2520and%250Ageneralizability%2520of%2520BusterX.%2520The%2520code%252C%2520models%252C%2520and%2520datasets%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12620v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BusterX%3A%20MLLM-Powered%20AI-Generated%20Video%20Forgery%20Detection%20and%0A%20%20Explanation&entry.906535625=Haiquan%20Wen%20and%20Yiwei%20He%20and%20Zhenglin%20Huang%20and%20Tianxiao%20Li%20and%20Zihan%20Yu%20and%20Xingru%20Huang%20and%20Lu%20Qi%20and%20Baoyuan%20Wu%20and%20Xiangtai%20Li%20and%20Guangliang%20Cheng&entry.1292438233=%20%20Advances%20in%20AI%20generative%20models%20facilitate%20super-realistic%20video%20synthesis%2C%0Aamplifying%20misinformation%20risks%20via%20social%20media%20and%20eroding%20trust%20in%20digital%0Acontent.%20Several%20research%20works%20have%20explored%20new%20deepfake%20detection%20methods%20on%0AAI-generated%20images%20to%20alleviate%20these%20risks.%20However%2C%20with%20the%20fast%0Adevelopment%20of%20video%20generation%20models%2C%20such%20as%20Sora%20and%20WanX%2C%20there%20is%0Acurrently%20a%20lack%20of%20large-scale%2C%20high-quality%20AI-generated%20video%20datasets%20for%0Aforgery%20detection.%20In%20addition%2C%20existing%20detection%20approaches%20predominantly%0Atreat%20the%20task%20as%20binary%20classification%2C%20lacking%20explainability%20in%20model%0Adecision-making%20and%20failing%20to%20provide%20actionable%20insights%20or%20guidance%20for%20the%0Apublic.%20To%20address%20these%20challenges%2C%20we%20propose%20%5Ctextbf%7BGenBuster-200K%7D%2C%20a%0Alarge-scale%20AI-generated%20video%20dataset%20featuring%20200K%20high-resolution%20video%0Aclips%2C%20diverse%20latest%20generative%20techniques%2C%20and%20real-world%20scenes.%20We%20further%0Aintroduce%20%5Ctextbf%7BBusterX%7D%2C%20a%20novel%20AI-generated%20video%20detection%20and%0Aexplanation%20framework%20leveraging%20multimodal%20large%20language%20model%20%28MLLM%29%20and%0Areinforcement%20learning%20for%20authenticity%20determination%20and%20explainable%0Arationale.%20To%20our%20knowledge%2C%20GenBuster-200K%20is%20the%20%7B%5Cit%20%5Ctextbf%7Bfirst%7D%7D%0Alarge-scale%2C%20high-quality%20AI-generated%20video%20dataset%20that%20incorporates%20the%0Alatest%20generative%20techniques%20for%20real-world%20scenarios.%20BusterX%20is%20the%20%7B%5Cit%0A%5Ctextbf%7Bfirst%7D%7D%20framework%20to%20integrate%20MLLM%20with%20reinforcement%20learning%20for%0Aexplainable%20AI-generated%20video%20detection.%20Extensive%20comparisons%20with%0Astate-of-the-art%20methods%20and%20ablation%20studies%20validate%20the%20effectiveness%20and%0Ageneralizability%20of%20BusterX.%20The%20code%2C%20models%2C%20and%20datasets%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.12620v4&entry.124074799=Read"},
{"title": "Efficient Masked Attention Transformer for Few-Shot Classification and\n  Segmentation", "author": "Dustin Carri\u00f3n-Ojeda and Stefan Roth and Simone Schaub-Meyer", "abstract": "  Few-shot classification and segmentation (FS-CS) focuses on jointly\nperforming multi-label classification and multi-class segmentation using few\nannotated examples. Although the current state of the art (SOTA) achieves high\naccuracy in both tasks, it struggles with small objects. To overcome this, we\npropose the Efficient Masked Attention Transformer (EMAT), which improves\nclassification and segmentation accuracy, especially for small objects. EMAT\nintroduces three modifications: a novel memory-efficient masked attention\nmechanism, a learnable downscaling strategy, and parameter-efficiency\nenhancements. EMAT outperforms all FS-CS methods on the PASCAL-5$^i$ and\nCOCO-20$^i$ datasets, using at least four times fewer trainable parameters.\nMoreover, as the current FS-CS evaluation setting discards available\nannotations, despite their costly collection, we introduce two novel evaluation\nsettings that consider these annotations to better reflect practical scenarios.\n", "link": "http://arxiv.org/abs/2507.23642v1", "date": "2025-07-31", "relevancy": 2.755, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5622}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5547}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Masked%20Attention%20Transformer%20for%20Few-Shot%20Classification%20and%0A%20%20Segmentation&body=Title%3A%20Efficient%20Masked%20Attention%20Transformer%20for%20Few-Shot%20Classification%20and%0A%20%20Segmentation%0AAuthor%3A%20Dustin%20Carri%C3%B3n-Ojeda%20and%20Stefan%20Roth%20and%20Simone%20Schaub-Meyer%0AAbstract%3A%20%20%20Few-shot%20classification%20and%20segmentation%20%28FS-CS%29%20focuses%20on%20jointly%0Aperforming%20multi-label%20classification%20and%20multi-class%20segmentation%20using%20few%0Aannotated%20examples.%20Although%20the%20current%20state%20of%20the%20art%20%28SOTA%29%20achieves%20high%0Aaccuracy%20in%20both%20tasks%2C%20it%20struggles%20with%20small%20objects.%20To%20overcome%20this%2C%20we%0Apropose%20the%20Efficient%20Masked%20Attention%20Transformer%20%28EMAT%29%2C%20which%20improves%0Aclassification%20and%20segmentation%20accuracy%2C%20especially%20for%20small%20objects.%20EMAT%0Aintroduces%20three%20modifications%3A%20a%20novel%20memory-efficient%20masked%20attention%0Amechanism%2C%20a%20learnable%20downscaling%20strategy%2C%20and%20parameter-efficiency%0Aenhancements.%20EMAT%20outperforms%20all%20FS-CS%20methods%20on%20the%20PASCAL-5%24%5Ei%24%20and%0ACOCO-20%24%5Ei%24%20datasets%2C%20using%20at%20least%20four%20times%20fewer%20trainable%20parameters.%0AMoreover%2C%20as%20the%20current%20FS-CS%20evaluation%20setting%20discards%20available%0Aannotations%2C%20despite%20their%20costly%20collection%2C%20we%20introduce%20two%20novel%20evaluation%0Asettings%20that%20consider%20these%20annotations%20to%20better%20reflect%20practical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Masked%2520Attention%2520Transformer%2520for%2520Few-Shot%2520Classification%2520and%250A%2520%2520Segmentation%26entry.906535625%3DDustin%2520Carri%25C3%25B3n-Ojeda%2520and%2520Stefan%2520Roth%2520and%2520Simone%2520Schaub-Meyer%26entry.1292438233%3D%2520%2520Few-shot%2520classification%2520and%2520segmentation%2520%2528FS-CS%2529%2520focuses%2520on%2520jointly%250Aperforming%2520multi-label%2520classification%2520and%2520multi-class%2520segmentation%2520using%2520few%250Aannotated%2520examples.%2520Although%2520the%2520current%2520state%2520of%2520the%2520art%2520%2528SOTA%2529%2520achieves%2520high%250Aaccuracy%2520in%2520both%2520tasks%252C%2520it%2520struggles%2520with%2520small%2520objects.%2520To%2520overcome%2520this%252C%2520we%250Apropose%2520the%2520Efficient%2520Masked%2520Attention%2520Transformer%2520%2528EMAT%2529%252C%2520which%2520improves%250Aclassification%2520and%2520segmentation%2520accuracy%252C%2520especially%2520for%2520small%2520objects.%2520EMAT%250Aintroduces%2520three%2520modifications%253A%2520a%2520novel%2520memory-efficient%2520masked%2520attention%250Amechanism%252C%2520a%2520learnable%2520downscaling%2520strategy%252C%2520and%2520parameter-efficiency%250Aenhancements.%2520EMAT%2520outperforms%2520all%2520FS-CS%2520methods%2520on%2520the%2520PASCAL-5%2524%255Ei%2524%2520and%250ACOCO-20%2524%255Ei%2524%2520datasets%252C%2520using%2520at%2520least%2520four%2520times%2520fewer%2520trainable%2520parameters.%250AMoreover%252C%2520as%2520the%2520current%2520FS-CS%2520evaluation%2520setting%2520discards%2520available%250Aannotations%252C%2520despite%2520their%2520costly%2520collection%252C%2520we%2520introduce%2520two%2520novel%2520evaluation%250Asettings%2520that%2520consider%2520these%2520annotations%2520to%2520better%2520reflect%2520practical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Masked%20Attention%20Transformer%20for%20Few-Shot%20Classification%20and%0A%20%20Segmentation&entry.906535625=Dustin%20Carri%C3%B3n-Ojeda%20and%20Stefan%20Roth%20and%20Simone%20Schaub-Meyer&entry.1292438233=%20%20Few-shot%20classification%20and%20segmentation%20%28FS-CS%29%20focuses%20on%20jointly%0Aperforming%20multi-label%20classification%20and%20multi-class%20segmentation%20using%20few%0Aannotated%20examples.%20Although%20the%20current%20state%20of%20the%20art%20%28SOTA%29%20achieves%20high%0Aaccuracy%20in%20both%20tasks%2C%20it%20struggles%20with%20small%20objects.%20To%20overcome%20this%2C%20we%0Apropose%20the%20Efficient%20Masked%20Attention%20Transformer%20%28EMAT%29%2C%20which%20improves%0Aclassification%20and%20segmentation%20accuracy%2C%20especially%20for%20small%20objects.%20EMAT%0Aintroduces%20three%20modifications%3A%20a%20novel%20memory-efficient%20masked%20attention%0Amechanism%2C%20a%20learnable%20downscaling%20strategy%2C%20and%20parameter-efficiency%0Aenhancements.%20EMAT%20outperforms%20all%20FS-CS%20methods%20on%20the%20PASCAL-5%24%5Ei%24%20and%0ACOCO-20%24%5Ei%24%20datasets%2C%20using%20at%20least%20four%20times%20fewer%20trainable%20parameters.%0AMoreover%2C%20as%20the%20current%20FS-CS%20evaluation%20setting%20discards%20available%0Aannotations%2C%20despite%20their%20costly%20collection%2C%20we%20introduce%20two%20novel%20evaluation%0Asettings%20that%20consider%20these%20annotations%20to%20better%20reflect%20practical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23642v1&entry.124074799=Read"},
{"title": "Slot Attention with Re-Initialization and Self-Distillation", "author": "Rongzhen Zhao and Yi Zhao and Juho Kannala and Joni Pajarinen", "abstract": "  Unlike popular solutions based on dense feature maps, Object-Centric Learning\n(OCL) represents visual scenes as sub-symbolic object-level feature vectors,\ntermed slots, which are highly versatile for tasks involving visual modalities.\nOCL typically aggregates object superpixels into slots by iteratively applying\ncompetitive cross attention, known as Slot Attention, with the slots as the\nquery. However, once initialized, these slots are reused naively, causing\nredundant slots to compete with informative ones for representing objects. This\noften results in objects being erroneously segmented into parts. Additionally,\nmainstream methods derive supervision signals solely from decoding slots into\nthe input's reconstruction, overlooking potential supervision based on internal\ninformation. To address these issues, we propose Slot Attention with\nre-Initialization and self-Distillation (DIAS): $\\emph{i)}$ We reduce\nredundancy in the aggregated slots and re-initialize extra aggregation to\nupdate the remaining slots; $\\emph{ii)}$ We drive the bad attention map at the\nfirst aggregation iteration to approximate the good at the last iteration to\nenable self-distillation. Experiments demonstrate that DIAS achieves\nstate-of-the-art on OCL tasks like object discovery and recognition, while also\nimproving advanced visual prediction and reasoning. Our code is available on\nhttps://github.com/Genera1Z/DIAS.\n", "link": "http://arxiv.org/abs/2507.23755v1", "date": "2025-07-31", "relevancy": 2.7526, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5692}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slot%20Attention%20with%20Re-Initialization%20and%20Self-Distillation&body=Title%3A%20Slot%20Attention%20with%20Re-Initialization%20and%20Self-Distillation%0AAuthor%3A%20Rongzhen%20Zhao%20and%20Yi%20Zhao%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Unlike%20popular%20solutions%20based%20on%20dense%20feature%20maps%2C%20Object-Centric%20Learning%0A%28OCL%29%20represents%20visual%20scenes%20as%20sub-symbolic%20object-level%20feature%20vectors%2C%0Atermed%20slots%2C%20which%20are%20highly%20versatile%20for%20tasks%20involving%20visual%20modalities.%0AOCL%20typically%20aggregates%20object%20superpixels%20into%20slots%20by%20iteratively%20applying%0Acompetitive%20cross%20attention%2C%20known%20as%20Slot%20Attention%2C%20with%20the%20slots%20as%20the%0Aquery.%20However%2C%20once%20initialized%2C%20these%20slots%20are%20reused%20naively%2C%20causing%0Aredundant%20slots%20to%20compete%20with%20informative%20ones%20for%20representing%20objects.%20This%0Aoften%20results%20in%20objects%20being%20erroneously%20segmented%20into%20parts.%20Additionally%2C%0Amainstream%20methods%20derive%20supervision%20signals%20solely%20from%20decoding%20slots%20into%0Athe%20input%27s%20reconstruction%2C%20overlooking%20potential%20supervision%20based%20on%20internal%0Ainformation.%20To%20address%20these%20issues%2C%20we%20propose%20Slot%20Attention%20with%0Are-Initialization%20and%20self-Distillation%20%28DIAS%29%3A%20%24%5Cemph%7Bi%29%7D%24%20We%20reduce%0Aredundancy%20in%20the%20aggregated%20slots%20and%20re-initialize%20extra%20aggregation%20to%0Aupdate%20the%20remaining%20slots%3B%20%24%5Cemph%7Bii%29%7D%24%20We%20drive%20the%20bad%20attention%20map%20at%20the%0Afirst%20aggregation%20iteration%20to%20approximate%20the%20good%20at%20the%20last%20iteration%20to%0Aenable%20self-distillation.%20Experiments%20demonstrate%20that%20DIAS%20achieves%0Astate-of-the-art%20on%20OCL%20tasks%20like%20object%20discovery%20and%20recognition%2C%20while%20also%0Aimproving%20advanced%20visual%20prediction%20and%20reasoning.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/Genera1Z/DIAS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlot%2520Attention%2520with%2520Re-Initialization%2520and%2520Self-Distillation%26entry.906535625%3DRongzhen%2520Zhao%2520and%2520Yi%2520Zhao%2520and%2520Juho%2520Kannala%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Unlike%2520popular%2520solutions%2520based%2520on%2520dense%2520feature%2520maps%252C%2520Object-Centric%2520Learning%250A%2528OCL%2529%2520represents%2520visual%2520scenes%2520as%2520sub-symbolic%2520object-level%2520feature%2520vectors%252C%250Atermed%2520slots%252C%2520which%2520are%2520highly%2520versatile%2520for%2520tasks%2520involving%2520visual%2520modalities.%250AOCL%2520typically%2520aggregates%2520object%2520superpixels%2520into%2520slots%2520by%2520iteratively%2520applying%250Acompetitive%2520cross%2520attention%252C%2520known%2520as%2520Slot%2520Attention%252C%2520with%2520the%2520slots%2520as%2520the%250Aquery.%2520However%252C%2520once%2520initialized%252C%2520these%2520slots%2520are%2520reused%2520naively%252C%2520causing%250Aredundant%2520slots%2520to%2520compete%2520with%2520informative%2520ones%2520for%2520representing%2520objects.%2520This%250Aoften%2520results%2520in%2520objects%2520being%2520erroneously%2520segmented%2520into%2520parts.%2520Additionally%252C%250Amainstream%2520methods%2520derive%2520supervision%2520signals%2520solely%2520from%2520decoding%2520slots%2520into%250Athe%2520input%2527s%2520reconstruction%252C%2520overlooking%2520potential%2520supervision%2520based%2520on%2520internal%250Ainformation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Slot%2520Attention%2520with%250Are-Initialization%2520and%2520self-Distillation%2520%2528DIAS%2529%253A%2520%2524%255Cemph%257Bi%2529%257D%2524%2520We%2520reduce%250Aredundancy%2520in%2520the%2520aggregated%2520slots%2520and%2520re-initialize%2520extra%2520aggregation%2520to%250Aupdate%2520the%2520remaining%2520slots%253B%2520%2524%255Cemph%257Bii%2529%257D%2524%2520We%2520drive%2520the%2520bad%2520attention%2520map%2520at%2520the%250Afirst%2520aggregation%2520iteration%2520to%2520approximate%2520the%2520good%2520at%2520the%2520last%2520iteration%2520to%250Aenable%2520self-distillation.%2520Experiments%2520demonstrate%2520that%2520DIAS%2520achieves%250Astate-of-the-art%2520on%2520OCL%2520tasks%2520like%2520object%2520discovery%2520and%2520recognition%252C%2520while%2520also%250Aimproving%2520advanced%2520visual%2520prediction%2520and%2520reasoning.%2520Our%2520code%2520is%2520available%2520on%250Ahttps%253A//github.com/Genera1Z/DIAS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slot%20Attention%20with%20Re-Initialization%20and%20Self-Distillation&entry.906535625=Rongzhen%20Zhao%20and%20Yi%20Zhao%20and%20Juho%20Kannala%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Unlike%20popular%20solutions%20based%20on%20dense%20feature%20maps%2C%20Object-Centric%20Learning%0A%28OCL%29%20represents%20visual%20scenes%20as%20sub-symbolic%20object-level%20feature%20vectors%2C%0Atermed%20slots%2C%20which%20are%20highly%20versatile%20for%20tasks%20involving%20visual%20modalities.%0AOCL%20typically%20aggregates%20object%20superpixels%20into%20slots%20by%20iteratively%20applying%0Acompetitive%20cross%20attention%2C%20known%20as%20Slot%20Attention%2C%20with%20the%20slots%20as%20the%0Aquery.%20However%2C%20once%20initialized%2C%20these%20slots%20are%20reused%20naively%2C%20causing%0Aredundant%20slots%20to%20compete%20with%20informative%20ones%20for%20representing%20objects.%20This%0Aoften%20results%20in%20objects%20being%20erroneously%20segmented%20into%20parts.%20Additionally%2C%0Amainstream%20methods%20derive%20supervision%20signals%20solely%20from%20decoding%20slots%20into%0Athe%20input%27s%20reconstruction%2C%20overlooking%20potential%20supervision%20based%20on%20internal%0Ainformation.%20To%20address%20these%20issues%2C%20we%20propose%20Slot%20Attention%20with%0Are-Initialization%20and%20self-Distillation%20%28DIAS%29%3A%20%24%5Cemph%7Bi%29%7D%24%20We%20reduce%0Aredundancy%20in%20the%20aggregated%20slots%20and%20re-initialize%20extra%20aggregation%20to%0Aupdate%20the%20remaining%20slots%3B%20%24%5Cemph%7Bii%29%7D%24%20We%20drive%20the%20bad%20attention%20map%20at%20the%0Afirst%20aggregation%20iteration%20to%20approximate%20the%20good%20at%20the%20last%20iteration%20to%0Aenable%20self-distillation.%20Experiments%20demonstrate%20that%20DIAS%20achieves%0Astate-of-the-art%20on%20OCL%20tasks%20like%20object%20discovery%20and%20recognition%2C%20while%20also%0Aimproving%20advanced%20visual%20prediction%20and%20reasoning.%20Our%20code%20is%20available%20on%0Ahttps%3A//github.com/Genera1Z/DIAS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23755v1&entry.124074799=Read"},
{"title": "AGA: An adaptive group alignment framework for structured medical\n  cross-modal representation learning", "author": "Wei Li and Xun Gong and Jiao Li and Xiaobin Sun", "abstract": "  Learning medical visual representations from paired images and reports is a\npromising direction in representation learning. However, current\nvision-language pretraining methods in the medical domain often simplify\nclinical reports into single entities or fragmented tokens, ignoring their\ninherent structure. In addition, contrastive learning frameworks typically\ndepend on large quantities of hard negative samples, which is impractical for\nsmall-scale medical datasets. To tackle these challenges, we propose Adaptive\nGrouped Alignment (AGA), a new framework that captures structured semantics\nfrom paired medical images and reports. AGA introduces a bidirectional grouping\nmechanism based on a sparse similarity matrix. For each image-report pair, we\ncompute fine-grained similarities between text tokens and image patches. Each\ntoken selects its top-matching patches to form a visual group, and each patch\nselects its most related tokens to form a language group. To enable adaptive\ngrouping, we design two threshold gating modules, called Language Grouped\nThreshold Gate and Vision Grouped Threshold Gate, which learn grouping\nthresholds dynamically. Group representations are computed as weighted averages\nbased on similarity scores. To align each token with its group representation,\nwe introduce an Instance Aware Group Alignment loss that operates within each\nimage-text pair, removing the need for external negatives. Finally, a\nBidirectional Cross-modal Grouped Alignment module is applied to enhance\nfine-grained alignment between visual and linguistic group representations.\nExtensive experiments on public and private datasets show that our method\nachieves strong performance on image-text retrieval and classification tasks\nunder both fine-tuning and zero-shot settings.\n", "link": "http://arxiv.org/abs/2507.23402v1", "date": "2025-07-31", "relevancy": 2.7476, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6016}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5291}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5178}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGA%3A%20An%20adaptive%20group%20alignment%20framework%20for%20structured%20medical%0A%20%20cross-modal%20representation%20learning&body=Title%3A%20AGA%3A%20An%20adaptive%20group%20alignment%20framework%20for%20structured%20medical%0A%20%20cross-modal%20representation%20learning%0AAuthor%3A%20Wei%20Li%20and%20Xun%20Gong%20and%20Jiao%20Li%20and%20Xiaobin%20Sun%0AAbstract%3A%20%20%20Learning%20medical%20visual%20representations%20from%20paired%20images%20and%20reports%20is%20a%0Apromising%20direction%20in%20representation%20learning.%20However%2C%20current%0Avision-language%20pretraining%20methods%20in%20the%20medical%20domain%20often%20simplify%0Aclinical%20reports%20into%20single%20entities%20or%20fragmented%20tokens%2C%20ignoring%20their%0Ainherent%20structure.%20In%20addition%2C%20contrastive%20learning%20frameworks%20typically%0Adepend%20on%20large%20quantities%20of%20hard%20negative%20samples%2C%20which%20is%20impractical%20for%0Asmall-scale%20medical%20datasets.%20To%20tackle%20these%20challenges%2C%20we%20propose%20Adaptive%0AGrouped%20Alignment%20%28AGA%29%2C%20a%20new%20framework%20that%20captures%20structured%20semantics%0Afrom%20paired%20medical%20images%20and%20reports.%20AGA%20introduces%20a%20bidirectional%20grouping%0Amechanism%20based%20on%20a%20sparse%20similarity%20matrix.%20For%20each%20image-report%20pair%2C%20we%0Acompute%20fine-grained%20similarities%20between%20text%20tokens%20and%20image%20patches.%20Each%0Atoken%20selects%20its%20top-matching%20patches%20to%20form%20a%20visual%20group%2C%20and%20each%20patch%0Aselects%20its%20most%20related%20tokens%20to%20form%20a%20language%20group.%20To%20enable%20adaptive%0Agrouping%2C%20we%20design%20two%20threshold%20gating%20modules%2C%20called%20Language%20Grouped%0AThreshold%20Gate%20and%20Vision%20Grouped%20Threshold%20Gate%2C%20which%20learn%20grouping%0Athresholds%20dynamically.%20Group%20representations%20are%20computed%20as%20weighted%20averages%0Abased%20on%20similarity%20scores.%20To%20align%20each%20token%20with%20its%20group%20representation%2C%0Awe%20introduce%20an%20Instance%20Aware%20Group%20Alignment%20loss%20that%20operates%20within%20each%0Aimage-text%20pair%2C%20removing%20the%20need%20for%20external%20negatives.%20Finally%2C%20a%0ABidirectional%20Cross-modal%20Grouped%20Alignment%20module%20is%20applied%20to%20enhance%0Afine-grained%20alignment%20between%20visual%20and%20linguistic%20group%20representations.%0AExtensive%20experiments%20on%20public%20and%20private%20datasets%20show%20that%20our%20method%0Aachieves%20strong%20performance%20on%20image-text%20retrieval%20and%20classification%20tasks%0Aunder%20both%20fine-tuning%20and%20zero-shot%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGA%253A%2520An%2520adaptive%2520group%2520alignment%2520framework%2520for%2520structured%2520medical%250A%2520%2520cross-modal%2520representation%2520learning%26entry.906535625%3DWei%2520Li%2520and%2520Xun%2520Gong%2520and%2520Jiao%2520Li%2520and%2520Xiaobin%2520Sun%26entry.1292438233%3D%2520%2520Learning%2520medical%2520visual%2520representations%2520from%2520paired%2520images%2520and%2520reports%2520is%2520a%250Apromising%2520direction%2520in%2520representation%2520learning.%2520However%252C%2520current%250Avision-language%2520pretraining%2520methods%2520in%2520the%2520medical%2520domain%2520often%2520simplify%250Aclinical%2520reports%2520into%2520single%2520entities%2520or%2520fragmented%2520tokens%252C%2520ignoring%2520their%250Ainherent%2520structure.%2520In%2520addition%252C%2520contrastive%2520learning%2520frameworks%2520typically%250Adepend%2520on%2520large%2520quantities%2520of%2520hard%2520negative%2520samples%252C%2520which%2520is%2520impractical%2520for%250Asmall-scale%2520medical%2520datasets.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520Adaptive%250AGrouped%2520Alignment%2520%2528AGA%2529%252C%2520a%2520new%2520framework%2520that%2520captures%2520structured%2520semantics%250Afrom%2520paired%2520medical%2520images%2520and%2520reports.%2520AGA%2520introduces%2520a%2520bidirectional%2520grouping%250Amechanism%2520based%2520on%2520a%2520sparse%2520similarity%2520matrix.%2520For%2520each%2520image-report%2520pair%252C%2520we%250Acompute%2520fine-grained%2520similarities%2520between%2520text%2520tokens%2520and%2520image%2520patches.%2520Each%250Atoken%2520selects%2520its%2520top-matching%2520patches%2520to%2520form%2520a%2520visual%2520group%252C%2520and%2520each%2520patch%250Aselects%2520its%2520most%2520related%2520tokens%2520to%2520form%2520a%2520language%2520group.%2520To%2520enable%2520adaptive%250Agrouping%252C%2520we%2520design%2520two%2520threshold%2520gating%2520modules%252C%2520called%2520Language%2520Grouped%250AThreshold%2520Gate%2520and%2520Vision%2520Grouped%2520Threshold%2520Gate%252C%2520which%2520learn%2520grouping%250Athresholds%2520dynamically.%2520Group%2520representations%2520are%2520computed%2520as%2520weighted%2520averages%250Abased%2520on%2520similarity%2520scores.%2520To%2520align%2520each%2520token%2520with%2520its%2520group%2520representation%252C%250Awe%2520introduce%2520an%2520Instance%2520Aware%2520Group%2520Alignment%2520loss%2520that%2520operates%2520within%2520each%250Aimage-text%2520pair%252C%2520removing%2520the%2520need%2520for%2520external%2520negatives.%2520Finally%252C%2520a%250ABidirectional%2520Cross-modal%2520Grouped%2520Alignment%2520module%2520is%2520applied%2520to%2520enhance%250Afine-grained%2520alignment%2520between%2520visual%2520and%2520linguistic%2520group%2520representations.%250AExtensive%2520experiments%2520on%2520public%2520and%2520private%2520datasets%2520show%2520that%2520our%2520method%250Aachieves%2520strong%2520performance%2520on%2520image-text%2520retrieval%2520and%2520classification%2520tasks%250Aunder%2520both%2520fine-tuning%2520and%2520zero-shot%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGA%3A%20An%20adaptive%20group%20alignment%20framework%20for%20structured%20medical%0A%20%20cross-modal%20representation%20learning&entry.906535625=Wei%20Li%20and%20Xun%20Gong%20and%20Jiao%20Li%20and%20Xiaobin%20Sun&entry.1292438233=%20%20Learning%20medical%20visual%20representations%20from%20paired%20images%20and%20reports%20is%20a%0Apromising%20direction%20in%20representation%20learning.%20However%2C%20current%0Avision-language%20pretraining%20methods%20in%20the%20medical%20domain%20often%20simplify%0Aclinical%20reports%20into%20single%20entities%20or%20fragmented%20tokens%2C%20ignoring%20their%0Ainherent%20structure.%20In%20addition%2C%20contrastive%20learning%20frameworks%20typically%0Adepend%20on%20large%20quantities%20of%20hard%20negative%20samples%2C%20which%20is%20impractical%20for%0Asmall-scale%20medical%20datasets.%20To%20tackle%20these%20challenges%2C%20we%20propose%20Adaptive%0AGrouped%20Alignment%20%28AGA%29%2C%20a%20new%20framework%20that%20captures%20structured%20semantics%0Afrom%20paired%20medical%20images%20and%20reports.%20AGA%20introduces%20a%20bidirectional%20grouping%0Amechanism%20based%20on%20a%20sparse%20similarity%20matrix.%20For%20each%20image-report%20pair%2C%20we%0Acompute%20fine-grained%20similarities%20between%20text%20tokens%20and%20image%20patches.%20Each%0Atoken%20selects%20its%20top-matching%20patches%20to%20form%20a%20visual%20group%2C%20and%20each%20patch%0Aselects%20its%20most%20related%20tokens%20to%20form%20a%20language%20group.%20To%20enable%20adaptive%0Agrouping%2C%20we%20design%20two%20threshold%20gating%20modules%2C%20called%20Language%20Grouped%0AThreshold%20Gate%20and%20Vision%20Grouped%20Threshold%20Gate%2C%20which%20learn%20grouping%0Athresholds%20dynamically.%20Group%20representations%20are%20computed%20as%20weighted%20averages%0Abased%20on%20similarity%20scores.%20To%20align%20each%20token%20with%20its%20group%20representation%2C%0Awe%20introduce%20an%20Instance%20Aware%20Group%20Alignment%20loss%20that%20operates%20within%20each%0Aimage-text%20pair%2C%20removing%20the%20need%20for%20external%20negatives.%20Finally%2C%20a%0ABidirectional%20Cross-modal%20Grouped%20Alignment%20module%20is%20applied%20to%20enhance%0Afine-grained%20alignment%20between%20visual%20and%20linguistic%20group%20representations.%0AExtensive%20experiments%20on%20public%20and%20private%20datasets%20show%20that%20our%20method%0Aachieves%20strong%20performance%20on%20image-text%20retrieval%20and%20classification%20tasks%0Aunder%20both%20fine-tuning%20and%20zero-shot%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23402v1&entry.124074799=Read"},
{"title": "Mantis Shrimp: Exploring Photometric Band Utilization in Computer Vision\n  Networks for Photometric Redshift Estimation", "author": "Andrew Engel and Nell Byler and Adam Tsou and Gautham Narayan and Emmanuel Bonilla and Ian Smith", "abstract": "  We present Mantis Shrimp, a multi-survey deep learning model for photometric\nredshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and\ninfrared (UnWISE) imagery. Machine learning is now an established approach for\nphotometric redshift estimation, with generally acknowledged higher performance\nin areas with a high density of spectroscopically identified galaxies over\ntemplate-based methods. Multiple works have shown that image-based\nconvolutional neural networks can outperform tabular-based color/magnitude\nmodels. In comparison to tabular models, image models have additional design\ncomplexities: it is largely unknown how to fuse inputs from different\ninstruments which have different resolutions or noise properties. The Mantis\nShrimp model estimates the conditional density estimate of redshift using\ncutout images. The density estimates are well calibrated and the point\nestimates perform well in the distribution of available spectroscopically\nconfirmed galaxies with (bias = 1e-2), scatter (NMAD = 2.44e-2) and\ncatastrophic outlier rate ($\\eta$=17.53$\\%$). We find that early fusion\napproaches (e.g., resampling and stacking images from different instruments)\nmatch the performance of late fusion approaches (e.g., concatenating latent\nspace representations), so that the design choice ultimately is left to the\nuser. Finally, we study how the models learn to use information across bands,\nfinding evidence that our models successfully incorporates information from all\nsurveys. The applicability of our model to the analysis of large populations of\ngalaxies is limited by the speed of downloading cutouts from external servers;\nhowever, our model could be useful in smaller studies such as generating priors\nover redshift for stellar population synthesis.\n", "link": "http://arxiv.org/abs/2501.09112v2", "date": "2025-07-31", "relevancy": 2.6973, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5722}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5273}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mantis%20Shrimp%3A%20Exploring%20Photometric%20Band%20Utilization%20in%20Computer%20Vision%0A%20%20Networks%20for%20Photometric%20Redshift%20Estimation&body=Title%3A%20Mantis%20Shrimp%3A%20Exploring%20Photometric%20Band%20Utilization%20in%20Computer%20Vision%0A%20%20Networks%20for%20Photometric%20Redshift%20Estimation%0AAuthor%3A%20Andrew%20Engel%20and%20Nell%20Byler%20and%20Adam%20Tsou%20and%20Gautham%20Narayan%20and%20Emmanuel%20Bonilla%20and%20Ian%20Smith%0AAbstract%3A%20%20%20We%20present%20Mantis%20Shrimp%2C%20a%20multi-survey%20deep%20learning%20model%20for%20photometric%0Aredshift%20estimation%20that%20fuses%20ultra-violet%20%28GALEX%29%2C%20optical%20%28PanSTARRS%29%2C%20and%0Ainfrared%20%28UnWISE%29%20imagery.%20Machine%20learning%20is%20now%20an%20established%20approach%20for%0Aphotometric%20redshift%20estimation%2C%20with%20generally%20acknowledged%20higher%20performance%0Ain%20areas%20with%20a%20high%20density%20of%20spectroscopically%20identified%20galaxies%20over%0Atemplate-based%20methods.%20Multiple%20works%20have%20shown%20that%20image-based%0Aconvolutional%20neural%20networks%20can%20outperform%20tabular-based%20color/magnitude%0Amodels.%20In%20comparison%20to%20tabular%20models%2C%20image%20models%20have%20additional%20design%0Acomplexities%3A%20it%20is%20largely%20unknown%20how%20to%20fuse%20inputs%20from%20different%0Ainstruments%20which%20have%20different%20resolutions%20or%20noise%20properties.%20The%20Mantis%0AShrimp%20model%20estimates%20the%20conditional%20density%20estimate%20of%20redshift%20using%0Acutout%20images.%20The%20density%20estimates%20are%20well%20calibrated%20and%20the%20point%0Aestimates%20perform%20well%20in%20the%20distribution%20of%20available%20spectroscopically%0Aconfirmed%20galaxies%20with%20%28bias%20%3D%201e-2%29%2C%20scatter%20%28NMAD%20%3D%202.44e-2%29%20and%0Acatastrophic%20outlier%20rate%20%28%24%5Ceta%24%3D17.53%24%5C%25%24%29.%20We%20find%20that%20early%20fusion%0Aapproaches%20%28e.g.%2C%20resampling%20and%20stacking%20images%20from%20different%20instruments%29%0Amatch%20the%20performance%20of%20late%20fusion%20approaches%20%28e.g.%2C%20concatenating%20latent%0Aspace%20representations%29%2C%20so%20that%20the%20design%20choice%20ultimately%20is%20left%20to%20the%0Auser.%20Finally%2C%20we%20study%20how%20the%20models%20learn%20to%20use%20information%20across%20bands%2C%0Afinding%20evidence%20that%20our%20models%20successfully%20incorporates%20information%20from%20all%0Asurveys.%20The%20applicability%20of%20our%20model%20to%20the%20analysis%20of%20large%20populations%20of%0Agalaxies%20is%20limited%20by%20the%20speed%20of%20downloading%20cutouts%20from%20external%20servers%3B%0Ahowever%2C%20our%20model%20could%20be%20useful%20in%20smaller%20studies%20such%20as%20generating%20priors%0Aover%20redshift%20for%20stellar%20population%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09112v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMantis%2520Shrimp%253A%2520Exploring%2520Photometric%2520Band%2520Utilization%2520in%2520Computer%2520Vision%250A%2520%2520Networks%2520for%2520Photometric%2520Redshift%2520Estimation%26entry.906535625%3DAndrew%2520Engel%2520and%2520Nell%2520Byler%2520and%2520Adam%2520Tsou%2520and%2520Gautham%2520Narayan%2520and%2520Emmanuel%2520Bonilla%2520and%2520Ian%2520Smith%26entry.1292438233%3D%2520%2520We%2520present%2520Mantis%2520Shrimp%252C%2520a%2520multi-survey%2520deep%2520learning%2520model%2520for%2520photometric%250Aredshift%2520estimation%2520that%2520fuses%2520ultra-violet%2520%2528GALEX%2529%252C%2520optical%2520%2528PanSTARRS%2529%252C%2520and%250Ainfrared%2520%2528UnWISE%2529%2520imagery.%2520Machine%2520learning%2520is%2520now%2520an%2520established%2520approach%2520for%250Aphotometric%2520redshift%2520estimation%252C%2520with%2520generally%2520acknowledged%2520higher%2520performance%250Ain%2520areas%2520with%2520a%2520high%2520density%2520of%2520spectroscopically%2520identified%2520galaxies%2520over%250Atemplate-based%2520methods.%2520Multiple%2520works%2520have%2520shown%2520that%2520image-based%250Aconvolutional%2520neural%2520networks%2520can%2520outperform%2520tabular-based%2520color/magnitude%250Amodels.%2520In%2520comparison%2520to%2520tabular%2520models%252C%2520image%2520models%2520have%2520additional%2520design%250Acomplexities%253A%2520it%2520is%2520largely%2520unknown%2520how%2520to%2520fuse%2520inputs%2520from%2520different%250Ainstruments%2520which%2520have%2520different%2520resolutions%2520or%2520noise%2520properties.%2520The%2520Mantis%250AShrimp%2520model%2520estimates%2520the%2520conditional%2520density%2520estimate%2520of%2520redshift%2520using%250Acutout%2520images.%2520The%2520density%2520estimates%2520are%2520well%2520calibrated%2520and%2520the%2520point%250Aestimates%2520perform%2520well%2520in%2520the%2520distribution%2520of%2520available%2520spectroscopically%250Aconfirmed%2520galaxies%2520with%2520%2528bias%2520%253D%25201e-2%2529%252C%2520scatter%2520%2528NMAD%2520%253D%25202.44e-2%2529%2520and%250Acatastrophic%2520outlier%2520rate%2520%2528%2524%255Ceta%2524%253D17.53%2524%255C%2525%2524%2529.%2520We%2520find%2520that%2520early%2520fusion%250Aapproaches%2520%2528e.g.%252C%2520resampling%2520and%2520stacking%2520images%2520from%2520different%2520instruments%2529%250Amatch%2520the%2520performance%2520of%2520late%2520fusion%2520approaches%2520%2528e.g.%252C%2520concatenating%2520latent%250Aspace%2520representations%2529%252C%2520so%2520that%2520the%2520design%2520choice%2520ultimately%2520is%2520left%2520to%2520the%250Auser.%2520Finally%252C%2520we%2520study%2520how%2520the%2520models%2520learn%2520to%2520use%2520information%2520across%2520bands%252C%250Afinding%2520evidence%2520that%2520our%2520models%2520successfully%2520incorporates%2520information%2520from%2520all%250Asurveys.%2520The%2520applicability%2520of%2520our%2520model%2520to%2520the%2520analysis%2520of%2520large%2520populations%2520of%250Agalaxies%2520is%2520limited%2520by%2520the%2520speed%2520of%2520downloading%2520cutouts%2520from%2520external%2520servers%253B%250Ahowever%252C%2520our%2520model%2520could%2520be%2520useful%2520in%2520smaller%2520studies%2520such%2520as%2520generating%2520priors%250Aover%2520redshift%2520for%2520stellar%2520population%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09112v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mantis%20Shrimp%3A%20Exploring%20Photometric%20Band%20Utilization%20in%20Computer%20Vision%0A%20%20Networks%20for%20Photometric%20Redshift%20Estimation&entry.906535625=Andrew%20Engel%20and%20Nell%20Byler%20and%20Adam%20Tsou%20and%20Gautham%20Narayan%20and%20Emmanuel%20Bonilla%20and%20Ian%20Smith&entry.1292438233=%20%20We%20present%20Mantis%20Shrimp%2C%20a%20multi-survey%20deep%20learning%20model%20for%20photometric%0Aredshift%20estimation%20that%20fuses%20ultra-violet%20%28GALEX%29%2C%20optical%20%28PanSTARRS%29%2C%20and%0Ainfrared%20%28UnWISE%29%20imagery.%20Machine%20learning%20is%20now%20an%20established%20approach%20for%0Aphotometric%20redshift%20estimation%2C%20with%20generally%20acknowledged%20higher%20performance%0Ain%20areas%20with%20a%20high%20density%20of%20spectroscopically%20identified%20galaxies%20over%0Atemplate-based%20methods.%20Multiple%20works%20have%20shown%20that%20image-based%0Aconvolutional%20neural%20networks%20can%20outperform%20tabular-based%20color/magnitude%0Amodels.%20In%20comparison%20to%20tabular%20models%2C%20image%20models%20have%20additional%20design%0Acomplexities%3A%20it%20is%20largely%20unknown%20how%20to%20fuse%20inputs%20from%20different%0Ainstruments%20which%20have%20different%20resolutions%20or%20noise%20properties.%20The%20Mantis%0AShrimp%20model%20estimates%20the%20conditional%20density%20estimate%20of%20redshift%20using%0Acutout%20images.%20The%20density%20estimates%20are%20well%20calibrated%20and%20the%20point%0Aestimates%20perform%20well%20in%20the%20distribution%20of%20available%20spectroscopically%0Aconfirmed%20galaxies%20with%20%28bias%20%3D%201e-2%29%2C%20scatter%20%28NMAD%20%3D%202.44e-2%29%20and%0Acatastrophic%20outlier%20rate%20%28%24%5Ceta%24%3D17.53%24%5C%25%24%29.%20We%20find%20that%20early%20fusion%0Aapproaches%20%28e.g.%2C%20resampling%20and%20stacking%20images%20from%20different%20instruments%29%0Amatch%20the%20performance%20of%20late%20fusion%20approaches%20%28e.g.%2C%20concatenating%20latent%0Aspace%20representations%29%2C%20so%20that%20the%20design%20choice%20ultimately%20is%20left%20to%20the%0Auser.%20Finally%2C%20we%20study%20how%20the%20models%20learn%20to%20use%20information%20across%20bands%2C%0Afinding%20evidence%20that%20our%20models%20successfully%20incorporates%20information%20from%20all%0Asurveys.%20The%20applicability%20of%20our%20model%20to%20the%20analysis%20of%20large%20populations%20of%0Agalaxies%20is%20limited%20by%20the%20speed%20of%20downloading%20cutouts%20from%20external%20servers%3B%0Ahowever%2C%20our%20model%20could%20be%20useful%20in%20smaller%20studies%20such%20as%20generating%20priors%0Aover%20redshift%20for%20stellar%20population%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09112v2&entry.124074799=Read"},
{"title": "KAN or MLP? Point Cloud Shows the Way Forward", "author": "Yan Shi and Qingdong He and Yijun Liu and Xiaoyu Liu and Jingyong Su", "abstract": "  Multi-Layer Perceptrons (MLPs) have become one of the fundamental\narchitectural component in point cloud analysis due to its effective feature\nlearning mechanism. However, when processing complex geometric structures in\npoint clouds, MLPs' fixed activation functions struggle to efficiently capture\nlocal geometric features, while suffering from poor parameter efficiency and\nhigh model redundancy. In this paper, we propose PointKAN, which applies\nKolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate\ntheir efficacy in hierarchical feature representation. First, we introduce a\nGeometric Affine Module (GAM) to transform local features, improving the\nmodel's robustness to geometric variations. Next, in the Local Feature\nProcessing (LFP), a parallel structure extracts both group-level features and\nglobal context, providing a rich representation of both fine details and\noverall structure. Finally, these features are combined and processed in the\nGlobal Feature Processing (GFP). By repeating these operations, the receptive\nfield gradually expands, enabling the model to capture complete geometric\ninformation of the point cloud. To overcome the high parameter counts and\ncomputational inefficiency of standard KANs, we develop Efficient-KANs in the\nPointKAN-elite variant, which significantly reduces parameters while\nmaintaining accuracy. Experimental results demonstrate that PointKAN\noutperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN,\nand ShapeNetPart, with particularly strong performance in Few-shot Learning\ntask. Additionally, PointKAN achieves substantial reductions in parameter\ncounts and computational complexity (FLOPs). This work highlights the potential\nof KANs-based architectures in 3D vision and opens new avenues for research in\npoint cloud understanding.\n", "link": "http://arxiv.org/abs/2504.13593v2", "date": "2025-07-31", "relevancy": 2.6721, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KAN%20or%20MLP%3F%20Point%20Cloud%20Shows%20the%20Way%20Forward&body=Title%3A%20KAN%20or%20MLP%3F%20Point%20Cloud%20Shows%20the%20Way%20Forward%0AAuthor%3A%20Yan%20Shi%20and%20Qingdong%20He%20and%20Yijun%20Liu%20and%20Xiaoyu%20Liu%20and%20Jingyong%20Su%0AAbstract%3A%20%20%20Multi-Layer%20Perceptrons%20%28MLPs%29%20have%20become%20one%20of%20the%20fundamental%0Aarchitectural%20component%20in%20point%20cloud%20analysis%20due%20to%20its%20effective%20feature%0Alearning%20mechanism.%20However%2C%20when%20processing%20complex%20geometric%20structures%20in%0Apoint%20clouds%2C%20MLPs%27%20fixed%20activation%20functions%20struggle%20to%20efficiently%20capture%0Alocal%20geometric%20features%2C%20while%20suffering%20from%20poor%20parameter%20efficiency%20and%0Ahigh%20model%20redundancy.%20In%20this%20paper%2C%20we%20propose%20PointKAN%2C%20which%20applies%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20to%20point%20cloud%20analysis%20tasks%20to%20investigate%0Atheir%20efficacy%20in%20hierarchical%20feature%20representation.%20First%2C%20we%20introduce%20a%0AGeometric%20Affine%20Module%20%28GAM%29%20to%20transform%20local%20features%2C%20improving%20the%0Amodel%27s%20robustness%20to%20geometric%20variations.%20Next%2C%20in%20the%20Local%20Feature%0AProcessing%20%28LFP%29%2C%20a%20parallel%20structure%20extracts%20both%20group-level%20features%20and%0Aglobal%20context%2C%20providing%20a%20rich%20representation%20of%20both%20fine%20details%20and%0Aoverall%20structure.%20Finally%2C%20these%20features%20are%20combined%20and%20processed%20in%20the%0AGlobal%20Feature%20Processing%20%28GFP%29.%20By%20repeating%20these%20operations%2C%20the%20receptive%0Afield%20gradually%20expands%2C%20enabling%20the%20model%20to%20capture%20complete%20geometric%0Ainformation%20of%20the%20point%20cloud.%20To%20overcome%20the%20high%20parameter%20counts%20and%0Acomputational%20inefficiency%20of%20standard%20KANs%2C%20we%20develop%20Efficient-KANs%20in%20the%0APointKAN-elite%20variant%2C%20which%20significantly%20reduces%20parameters%20while%0Amaintaining%20accuracy.%20Experimental%20results%20demonstrate%20that%20PointKAN%0Aoutperforms%20PointMLP%20on%20benchmark%20datasets%20such%20as%20ModelNet40%2C%20ScanObjectNN%2C%0Aand%20ShapeNetPart%2C%20with%20particularly%20strong%20performance%20in%20Few-shot%20Learning%0Atask.%20Additionally%2C%20PointKAN%20achieves%20substantial%20reductions%20in%20parameter%0Acounts%20and%20computational%20complexity%20%28FLOPs%29.%20This%20work%20highlights%20the%20potential%0Aof%20KANs-based%20architectures%20in%203D%20vision%20and%20opens%20new%20avenues%20for%20research%20in%0Apoint%20cloud%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKAN%2520or%2520MLP%253F%2520Point%2520Cloud%2520Shows%2520the%2520Way%2520Forward%26entry.906535625%3DYan%2520Shi%2520and%2520Qingdong%2520He%2520and%2520Yijun%2520Liu%2520and%2520Xiaoyu%2520Liu%2520and%2520Jingyong%2520Su%26entry.1292438233%3D%2520%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%2520have%2520become%2520one%2520of%2520the%2520fundamental%250Aarchitectural%2520component%2520in%2520point%2520cloud%2520analysis%2520due%2520to%2520its%2520effective%2520feature%250Alearning%2520mechanism.%2520However%252C%2520when%2520processing%2520complex%2520geometric%2520structures%2520in%250Apoint%2520clouds%252C%2520MLPs%2527%2520fixed%2520activation%2520functions%2520struggle%2520to%2520efficiently%2520capture%250Alocal%2520geometric%2520features%252C%2520while%2520suffering%2520from%2520poor%2520parameter%2520efficiency%2520and%250Ahigh%2520model%2520redundancy.%2520In%2520this%2520paper%252C%2520we%2520propose%2520PointKAN%252C%2520which%2520applies%250AKolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520to%2520point%2520cloud%2520analysis%2520tasks%2520to%2520investigate%250Atheir%2520efficacy%2520in%2520hierarchical%2520feature%2520representation.%2520First%252C%2520we%2520introduce%2520a%250AGeometric%2520Affine%2520Module%2520%2528GAM%2529%2520to%2520transform%2520local%2520features%252C%2520improving%2520the%250Amodel%2527s%2520robustness%2520to%2520geometric%2520variations.%2520Next%252C%2520in%2520the%2520Local%2520Feature%250AProcessing%2520%2528LFP%2529%252C%2520a%2520parallel%2520structure%2520extracts%2520both%2520group-level%2520features%2520and%250Aglobal%2520context%252C%2520providing%2520a%2520rich%2520representation%2520of%2520both%2520fine%2520details%2520and%250Aoverall%2520structure.%2520Finally%252C%2520these%2520features%2520are%2520combined%2520and%2520processed%2520in%2520the%250AGlobal%2520Feature%2520Processing%2520%2528GFP%2529.%2520By%2520repeating%2520these%2520operations%252C%2520the%2520receptive%250Afield%2520gradually%2520expands%252C%2520enabling%2520the%2520model%2520to%2520capture%2520complete%2520geometric%250Ainformation%2520of%2520the%2520point%2520cloud.%2520To%2520overcome%2520the%2520high%2520parameter%2520counts%2520and%250Acomputational%2520inefficiency%2520of%2520standard%2520KANs%252C%2520we%2520develop%2520Efficient-KANs%2520in%2520the%250APointKAN-elite%2520variant%252C%2520which%2520significantly%2520reduces%2520parameters%2520while%250Amaintaining%2520accuracy.%2520Experimental%2520results%2520demonstrate%2520that%2520PointKAN%250Aoutperforms%2520PointMLP%2520on%2520benchmark%2520datasets%2520such%2520as%2520ModelNet40%252C%2520ScanObjectNN%252C%250Aand%2520ShapeNetPart%252C%2520with%2520particularly%2520strong%2520performance%2520in%2520Few-shot%2520Learning%250Atask.%2520Additionally%252C%2520PointKAN%2520achieves%2520substantial%2520reductions%2520in%2520parameter%250Acounts%2520and%2520computational%2520complexity%2520%2528FLOPs%2529.%2520This%2520work%2520highlights%2520the%2520potential%250Aof%2520KANs-based%2520architectures%2520in%25203D%2520vision%2520and%2520opens%2520new%2520avenues%2520for%2520research%2520in%250Apoint%2520cloud%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAN%20or%20MLP%3F%20Point%20Cloud%20Shows%20the%20Way%20Forward&entry.906535625=Yan%20Shi%20and%20Qingdong%20He%20and%20Yijun%20Liu%20and%20Xiaoyu%20Liu%20and%20Jingyong%20Su&entry.1292438233=%20%20Multi-Layer%20Perceptrons%20%28MLPs%29%20have%20become%20one%20of%20the%20fundamental%0Aarchitectural%20component%20in%20point%20cloud%20analysis%20due%20to%20its%20effective%20feature%0Alearning%20mechanism.%20However%2C%20when%20processing%20complex%20geometric%20structures%20in%0Apoint%20clouds%2C%20MLPs%27%20fixed%20activation%20functions%20struggle%20to%20efficiently%20capture%0Alocal%20geometric%20features%2C%20while%20suffering%20from%20poor%20parameter%20efficiency%20and%0Ahigh%20model%20redundancy.%20In%20this%20paper%2C%20we%20propose%20PointKAN%2C%20which%20applies%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20to%20point%20cloud%20analysis%20tasks%20to%20investigate%0Atheir%20efficacy%20in%20hierarchical%20feature%20representation.%20First%2C%20we%20introduce%20a%0AGeometric%20Affine%20Module%20%28GAM%29%20to%20transform%20local%20features%2C%20improving%20the%0Amodel%27s%20robustness%20to%20geometric%20variations.%20Next%2C%20in%20the%20Local%20Feature%0AProcessing%20%28LFP%29%2C%20a%20parallel%20structure%20extracts%20both%20group-level%20features%20and%0Aglobal%20context%2C%20providing%20a%20rich%20representation%20of%20both%20fine%20details%20and%0Aoverall%20structure.%20Finally%2C%20these%20features%20are%20combined%20and%20processed%20in%20the%0AGlobal%20Feature%20Processing%20%28GFP%29.%20By%20repeating%20these%20operations%2C%20the%20receptive%0Afield%20gradually%20expands%2C%20enabling%20the%20model%20to%20capture%20complete%20geometric%0Ainformation%20of%20the%20point%20cloud.%20To%20overcome%20the%20high%20parameter%20counts%20and%0Acomputational%20inefficiency%20of%20standard%20KANs%2C%20we%20develop%20Efficient-KANs%20in%20the%0APointKAN-elite%20variant%2C%20which%20significantly%20reduces%20parameters%20while%0Amaintaining%20accuracy.%20Experimental%20results%20demonstrate%20that%20PointKAN%0Aoutperforms%20PointMLP%20on%20benchmark%20datasets%20such%20as%20ModelNet40%2C%20ScanObjectNN%2C%0Aand%20ShapeNetPart%2C%20with%20particularly%20strong%20performance%20in%20Few-shot%20Learning%0Atask.%20Additionally%2C%20PointKAN%20achieves%20substantial%20reductions%20in%20parameter%0Acounts%20and%20computational%20complexity%20%28FLOPs%29.%20This%20work%20highlights%20the%20potential%0Aof%20KANs-based%20architectures%20in%203D%20vision%20and%20opens%20new%20avenues%20for%20research%20in%0Apoint%20cloud%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13593v2&entry.124074799=Read"},
{"title": "Beyond the Encoder: Joint Encoder-Decoder Contrastive Pre-Training\n  Improves Dense Prediction", "author": "S\u00e9bastien Quetin and Tapotosh Ghosh and Farhad Maleki", "abstract": "  Contrastive learning methods in self-supervised settings have primarily\nfocused on pre-training encoders, while decoders are typically introduced and\ntrained separately for downstream dense prediction tasks. However, this\nconventional approach overlooks the potential benefits of jointly pre-training\nboth encoder and decoder. In this paper, we propose DeCon, an efficient\nencoder-decoder self-supervised learning (SSL) framework that supports joint\ncontrastive pre-training. We first extend existing SSL architectures to\naccommodate diverse decoders and their corresponding contrastive losses. Then,\nwe introduce a weighted encoder-decoder contrastive loss with non-competing\nobjectives to enable the joint pre-training of encoder-decoder architectures.\nBy adapting an established contrastive SSL framework for dense prediction\ntasks, DeCon achieves new state-of-the-art results: on COCO object detection\nand instance segmentation when pre-trained on COCO dataset; across almost all\ndense downstream benchmark tasks when pre-trained on COCO+ and ImageNet-1K. Our\nresults demonstrate that joint pre-training enhances the representation power\nof the encoder and improves performance in dense prediction tasks. This gain\npersists across heterogeneous decoder architectures, various encoder\narchitectures, and in out-of-domain limited-data scenarios.\n", "link": "http://arxiv.org/abs/2503.17526v2", "date": "2025-07-31", "relevancy": 2.6377, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5336}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20the%20Encoder%3A%20Joint%20Encoder-Decoder%20Contrastive%20Pre-Training%0A%20%20Improves%20Dense%20Prediction&body=Title%3A%20Beyond%20the%20Encoder%3A%20Joint%20Encoder-Decoder%20Contrastive%20Pre-Training%0A%20%20Improves%20Dense%20Prediction%0AAuthor%3A%20S%C3%A9bastien%20Quetin%20and%20Tapotosh%20Ghosh%20and%20Farhad%20Maleki%0AAbstract%3A%20%20%20Contrastive%20learning%20methods%20in%20self-supervised%20settings%20have%20primarily%0Afocused%20on%20pre-training%20encoders%2C%20while%20decoders%20are%20typically%20introduced%20and%0Atrained%20separately%20for%20downstream%20dense%20prediction%20tasks.%20However%2C%20this%0Aconventional%20approach%20overlooks%20the%20potential%20benefits%20of%20jointly%20pre-training%0Aboth%20encoder%20and%20decoder.%20In%20this%20paper%2C%20we%20propose%20DeCon%2C%20an%20efficient%0Aencoder-decoder%20self-supervised%20learning%20%28SSL%29%20framework%20that%20supports%20joint%0Acontrastive%20pre-training.%20We%20first%20extend%20existing%20SSL%20architectures%20to%0Aaccommodate%20diverse%20decoders%20and%20their%20corresponding%20contrastive%20losses.%20Then%2C%0Awe%20introduce%20a%20weighted%20encoder-decoder%20contrastive%20loss%20with%20non-competing%0Aobjectives%20to%20enable%20the%20joint%20pre-training%20of%20encoder-decoder%20architectures.%0ABy%20adapting%20an%20established%20contrastive%20SSL%20framework%20for%20dense%20prediction%0Atasks%2C%20DeCon%20achieves%20new%20state-of-the-art%20results%3A%20on%20COCO%20object%20detection%0Aand%20instance%20segmentation%20when%20pre-trained%20on%20COCO%20dataset%3B%20across%20almost%20all%0Adense%20downstream%20benchmark%20tasks%20when%20pre-trained%20on%20COCO%2B%20and%20ImageNet-1K.%20Our%0Aresults%20demonstrate%20that%20joint%20pre-training%20enhances%20the%20representation%20power%0Aof%20the%20encoder%20and%20improves%20performance%20in%20dense%20prediction%20tasks.%20This%20gain%0Apersists%20across%20heterogeneous%20decoder%20architectures%2C%20various%20encoder%0Aarchitectures%2C%20and%20in%20out-of-domain%20limited-data%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17526v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520the%2520Encoder%253A%2520Joint%2520Encoder-Decoder%2520Contrastive%2520Pre-Training%250A%2520%2520Improves%2520Dense%2520Prediction%26entry.906535625%3DS%25C3%25A9bastien%2520Quetin%2520and%2520Tapotosh%2520Ghosh%2520and%2520Farhad%2520Maleki%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520methods%2520in%2520self-supervised%2520settings%2520have%2520primarily%250Afocused%2520on%2520pre-training%2520encoders%252C%2520while%2520decoders%2520are%2520typically%2520introduced%2520and%250Atrained%2520separately%2520for%2520downstream%2520dense%2520prediction%2520tasks.%2520However%252C%2520this%250Aconventional%2520approach%2520overlooks%2520the%2520potential%2520benefits%2520of%2520jointly%2520pre-training%250Aboth%2520encoder%2520and%2520decoder.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DeCon%252C%2520an%2520efficient%250Aencoder-decoder%2520self-supervised%2520learning%2520%2528SSL%2529%2520framework%2520that%2520supports%2520joint%250Acontrastive%2520pre-training.%2520We%2520first%2520extend%2520existing%2520SSL%2520architectures%2520to%250Aaccommodate%2520diverse%2520decoders%2520and%2520their%2520corresponding%2520contrastive%2520losses.%2520Then%252C%250Awe%2520introduce%2520a%2520weighted%2520encoder-decoder%2520contrastive%2520loss%2520with%2520non-competing%250Aobjectives%2520to%2520enable%2520the%2520joint%2520pre-training%2520of%2520encoder-decoder%2520architectures.%250ABy%2520adapting%2520an%2520established%2520contrastive%2520SSL%2520framework%2520for%2520dense%2520prediction%250Atasks%252C%2520DeCon%2520achieves%2520new%2520state-of-the-art%2520results%253A%2520on%2520COCO%2520object%2520detection%250Aand%2520instance%2520segmentation%2520when%2520pre-trained%2520on%2520COCO%2520dataset%253B%2520across%2520almost%2520all%250Adense%2520downstream%2520benchmark%2520tasks%2520when%2520pre-trained%2520on%2520COCO%252B%2520and%2520ImageNet-1K.%2520Our%250Aresults%2520demonstrate%2520that%2520joint%2520pre-training%2520enhances%2520the%2520representation%2520power%250Aof%2520the%2520encoder%2520and%2520improves%2520performance%2520in%2520dense%2520prediction%2520tasks.%2520This%2520gain%250Apersists%2520across%2520heterogeneous%2520decoder%2520architectures%252C%2520various%2520encoder%250Aarchitectures%252C%2520and%2520in%2520out-of-domain%2520limited-data%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17526v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20the%20Encoder%3A%20Joint%20Encoder-Decoder%20Contrastive%20Pre-Training%0A%20%20Improves%20Dense%20Prediction&entry.906535625=S%C3%A9bastien%20Quetin%20and%20Tapotosh%20Ghosh%20and%20Farhad%20Maleki&entry.1292438233=%20%20Contrastive%20learning%20methods%20in%20self-supervised%20settings%20have%20primarily%0Afocused%20on%20pre-training%20encoders%2C%20while%20decoders%20are%20typically%20introduced%20and%0Atrained%20separately%20for%20downstream%20dense%20prediction%20tasks.%20However%2C%20this%0Aconventional%20approach%20overlooks%20the%20potential%20benefits%20of%20jointly%20pre-training%0Aboth%20encoder%20and%20decoder.%20In%20this%20paper%2C%20we%20propose%20DeCon%2C%20an%20efficient%0Aencoder-decoder%20self-supervised%20learning%20%28SSL%29%20framework%20that%20supports%20joint%0Acontrastive%20pre-training.%20We%20first%20extend%20existing%20SSL%20architectures%20to%0Aaccommodate%20diverse%20decoders%20and%20their%20corresponding%20contrastive%20losses.%20Then%2C%0Awe%20introduce%20a%20weighted%20encoder-decoder%20contrastive%20loss%20with%20non-competing%0Aobjectives%20to%20enable%20the%20joint%20pre-training%20of%20encoder-decoder%20architectures.%0ABy%20adapting%20an%20established%20contrastive%20SSL%20framework%20for%20dense%20prediction%0Atasks%2C%20DeCon%20achieves%20new%20state-of-the-art%20results%3A%20on%20COCO%20object%20detection%0Aand%20instance%20segmentation%20when%20pre-trained%20on%20COCO%20dataset%3B%20across%20almost%20all%0Adense%20downstream%20benchmark%20tasks%20when%20pre-trained%20on%20COCO%2B%20and%20ImageNet-1K.%20Our%0Aresults%20demonstrate%20that%20joint%20pre-training%20enhances%20the%20representation%20power%0Aof%20the%20encoder%20and%20improves%20performance%20in%20dense%20prediction%20tasks.%20This%20gain%0Apersists%20across%20heterogeneous%20decoder%20architectures%2C%20various%20encoder%0Aarchitectures%2C%20and%20in%20out-of-domain%20limited-data%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17526v2&entry.124074799=Read"},
{"title": "I Am Big, You Are Little; I Am Right, You Are Wrong", "author": "David A. Kelly and Akchunya Chanchal and Nathan Blake", "abstract": "  Machine learning for image classification is an active and rapidly developing\nfield. With the proliferation of classifiers of different sizes and different\narchitectures, the problem of choosing the right model becomes more and more\nimportant.\n  While we can assess a model's classification accuracy statistically, our\nunderstanding of the way these models work is unfortunately limited. In order\nto gain insight into the decision-making process of different vision models, we\npropose using minimal sufficient pixels sets to gauge a model's\n`concentration': the pixels that capture the essence of an image through the\nlens of the model. By comparing position, overlap, and size of sets of pixels,\nwe identify that different architectures have statistically different\nconcentration, in both size and position. In particular, ConvNext and EVA\nmodels differ markedly from the others. We also identify that images which are\nmisclassified are associated with larger pixels sets than correct\nclassifications.\n", "link": "http://arxiv.org/abs/2507.23509v1", "date": "2025-07-31", "relevancy": 2.6372, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I%20Am%20Big%2C%20You%20Are%20Little%3B%20I%20Am%20Right%2C%20You%20Are%20Wrong&body=Title%3A%20I%20Am%20Big%2C%20You%20Are%20Little%3B%20I%20Am%20Right%2C%20You%20Are%20Wrong%0AAuthor%3A%20David%20A.%20Kelly%20and%20Akchunya%20Chanchal%20and%20Nathan%20Blake%0AAbstract%3A%20%20%20Machine%20learning%20for%20image%20classification%20is%20an%20active%20and%20rapidly%20developing%0Afield.%20With%20the%20proliferation%20of%20classifiers%20of%20different%20sizes%20and%20different%0Aarchitectures%2C%20the%20problem%20of%20choosing%20the%20right%20model%20becomes%20more%20and%20more%0Aimportant.%0A%20%20While%20we%20can%20assess%20a%20model%27s%20classification%20accuracy%20statistically%2C%20our%0Aunderstanding%20of%20the%20way%20these%20models%20work%20is%20unfortunately%20limited.%20In%20order%0Ato%20gain%20insight%20into%20the%20decision-making%20process%20of%20different%20vision%20models%2C%20we%0Apropose%20using%20minimal%20sufficient%20pixels%20sets%20to%20gauge%20a%20model%27s%0A%60concentration%27%3A%20the%20pixels%20that%20capture%20the%20essence%20of%20an%20image%20through%20the%0Alens%20of%20the%20model.%20By%20comparing%20position%2C%20overlap%2C%20and%20size%20of%20sets%20of%20pixels%2C%0Awe%20identify%20that%20different%20architectures%20have%20statistically%20different%0Aconcentration%2C%20in%20both%20size%20and%20position.%20In%20particular%2C%20ConvNext%20and%20EVA%0Amodels%20differ%20markedly%20from%20the%20others.%20We%20also%20identify%20that%20images%20which%20are%0Amisclassified%20are%20associated%20with%20larger%20pixels%20sets%20than%20correct%0Aclassifications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23509v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI%2520Am%2520Big%252C%2520You%2520Are%2520Little%253B%2520I%2520Am%2520Right%252C%2520You%2520Are%2520Wrong%26entry.906535625%3DDavid%2520A.%2520Kelly%2520and%2520Akchunya%2520Chanchal%2520and%2520Nathan%2520Blake%26entry.1292438233%3D%2520%2520Machine%2520learning%2520for%2520image%2520classification%2520is%2520an%2520active%2520and%2520rapidly%2520developing%250Afield.%2520With%2520the%2520proliferation%2520of%2520classifiers%2520of%2520different%2520sizes%2520and%2520different%250Aarchitectures%252C%2520the%2520problem%2520of%2520choosing%2520the%2520right%2520model%2520becomes%2520more%2520and%2520more%250Aimportant.%250A%2520%2520While%2520we%2520can%2520assess%2520a%2520model%2527s%2520classification%2520accuracy%2520statistically%252C%2520our%250Aunderstanding%2520of%2520the%2520way%2520these%2520models%2520work%2520is%2520unfortunately%2520limited.%2520In%2520order%250Ato%2520gain%2520insight%2520into%2520the%2520decision-making%2520process%2520of%2520different%2520vision%2520models%252C%2520we%250Apropose%2520using%2520minimal%2520sufficient%2520pixels%2520sets%2520to%2520gauge%2520a%2520model%2527s%250A%2560concentration%2527%253A%2520the%2520pixels%2520that%2520capture%2520the%2520essence%2520of%2520an%2520image%2520through%2520the%250Alens%2520of%2520the%2520model.%2520By%2520comparing%2520position%252C%2520overlap%252C%2520and%2520size%2520of%2520sets%2520of%2520pixels%252C%250Awe%2520identify%2520that%2520different%2520architectures%2520have%2520statistically%2520different%250Aconcentration%252C%2520in%2520both%2520size%2520and%2520position.%2520In%2520particular%252C%2520ConvNext%2520and%2520EVA%250Amodels%2520differ%2520markedly%2520from%2520the%2520others.%2520We%2520also%2520identify%2520that%2520images%2520which%2520are%250Amisclassified%2520are%2520associated%2520with%2520larger%2520pixels%2520sets%2520than%2520correct%250Aclassifications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23509v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I%20Am%20Big%2C%20You%20Are%20Little%3B%20I%20Am%20Right%2C%20You%20Are%20Wrong&entry.906535625=David%20A.%20Kelly%20and%20Akchunya%20Chanchal%20and%20Nathan%20Blake&entry.1292438233=%20%20Machine%20learning%20for%20image%20classification%20is%20an%20active%20and%20rapidly%20developing%0Afield.%20With%20the%20proliferation%20of%20classifiers%20of%20different%20sizes%20and%20different%0Aarchitectures%2C%20the%20problem%20of%20choosing%20the%20right%20model%20becomes%20more%20and%20more%0Aimportant.%0A%20%20While%20we%20can%20assess%20a%20model%27s%20classification%20accuracy%20statistically%2C%20our%0Aunderstanding%20of%20the%20way%20these%20models%20work%20is%20unfortunately%20limited.%20In%20order%0Ato%20gain%20insight%20into%20the%20decision-making%20process%20of%20different%20vision%20models%2C%20we%0Apropose%20using%20minimal%20sufficient%20pixels%20sets%20to%20gauge%20a%20model%27s%0A%60concentration%27%3A%20the%20pixels%20that%20capture%20the%20essence%20of%20an%20image%20through%20the%0Alens%20of%20the%20model.%20By%20comparing%20position%2C%20overlap%2C%20and%20size%20of%20sets%20of%20pixels%2C%0Awe%20identify%20that%20different%20architectures%20have%20statistically%20different%0Aconcentration%2C%20in%20both%20size%20and%20position.%20In%20particular%2C%20ConvNext%20and%20EVA%0Amodels%20differ%20markedly%20from%20the%20others.%20We%20also%20identify%20that%20images%20which%20are%0Amisclassified%20are%20associated%20with%20larger%20pixels%20sets%20than%20correct%0Aclassifications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23509v1&entry.124074799=Read"},
{"title": "Color as the Impetus: Transforming Few-Shot Learner", "author": "Chaofei Qi and Zhitai Liu and Jianbin Qiu", "abstract": "  Humans possess innate meta-learning capabilities, partly attributable to\ntheir exceptional color perception. In this paper, we pioneer an innovative\nviewpoint on few-shot learning by simulating human color perception mechanisms.\nWe propose the ColorSense Learner, a bio-inspired meta-learning framework that\ncapitalizes on inter-channel feature extraction and interactive learning. By\nstrategically emphasizing distinct color information across different channels,\nour approach effectively filters irrelevant features while capturing\ndiscriminative characteristics. Color information represents the most intuitive\nvisual feature, yet conventional meta-learning methods have predominantly\nneglected this aspect, focusing instead on abstract feature differentiation\nacross categories. Our framework bridges the gap via synergistic color-channel\ninteractions, enabling better intra-class commonality extraction and larger\ninter-class differences. Furthermore, we introduce a meta-distiller based on\nknowledge distillation, ColorSense Distiller, which incorporates prior teacher\nknowledge to augment the student network's meta-learning capacity. We've\nconducted comprehensive coarse/fine-grained and cross-domain experiments on\neleven few-shot benchmarks for validation. Numerous experiments reveal that our\nmethods have extremely strong generalization ability, robustness, and\ntransferability, and effortless handle few-shot classification from the\nperspective of color perception.\n", "link": "http://arxiv.org/abs/2507.22136v2", "date": "2025-07-31", "relevancy": 2.632, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5346}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Color%20as%20the%20Impetus%3A%20Transforming%20Few-Shot%20Learner&body=Title%3A%20Color%20as%20the%20Impetus%3A%20Transforming%20Few-Shot%20Learner%0AAuthor%3A%20Chaofei%20Qi%20and%20Zhitai%20Liu%20and%20Jianbin%20Qiu%0AAbstract%3A%20%20%20Humans%20possess%20innate%20meta-learning%20capabilities%2C%20partly%20attributable%20to%0Atheir%20exceptional%20color%20perception.%20In%20this%20paper%2C%20we%20pioneer%20an%20innovative%0Aviewpoint%20on%20few-shot%20learning%20by%20simulating%20human%20color%20perception%20mechanisms.%0AWe%20propose%20the%20ColorSense%20Learner%2C%20a%20bio-inspired%20meta-learning%20framework%20that%0Acapitalizes%20on%20inter-channel%20feature%20extraction%20and%20interactive%20learning.%20By%0Astrategically%20emphasizing%20distinct%20color%20information%20across%20different%20channels%2C%0Aour%20approach%20effectively%20filters%20irrelevant%20features%20while%20capturing%0Adiscriminative%20characteristics.%20Color%20information%20represents%20the%20most%20intuitive%0Avisual%20feature%2C%20yet%20conventional%20meta-learning%20methods%20have%20predominantly%0Aneglected%20this%20aspect%2C%20focusing%20instead%20on%20abstract%20feature%20differentiation%0Aacross%20categories.%20Our%20framework%20bridges%20the%20gap%20via%20synergistic%20color-channel%0Ainteractions%2C%20enabling%20better%20intra-class%20commonality%20extraction%20and%20larger%0Ainter-class%20differences.%20Furthermore%2C%20we%20introduce%20a%20meta-distiller%20based%20on%0Aknowledge%20distillation%2C%20ColorSense%20Distiller%2C%20which%20incorporates%20prior%20teacher%0Aknowledge%20to%20augment%20the%20student%20network%27s%20meta-learning%20capacity.%20We%27ve%0Aconducted%20comprehensive%20coarse/fine-grained%20and%20cross-domain%20experiments%20on%0Aeleven%20few-shot%20benchmarks%20for%20validation.%20Numerous%20experiments%20reveal%20that%20our%0Amethods%20have%20extremely%20strong%20generalization%20ability%2C%20robustness%2C%20and%0Atransferability%2C%20and%20effortless%20handle%20few-shot%20classification%20from%20the%0Aperspective%20of%20color%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.22136v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColor%2520as%2520the%2520Impetus%253A%2520Transforming%2520Few-Shot%2520Learner%26entry.906535625%3DChaofei%2520Qi%2520and%2520Zhitai%2520Liu%2520and%2520Jianbin%2520Qiu%26entry.1292438233%3D%2520%2520Humans%2520possess%2520innate%2520meta-learning%2520capabilities%252C%2520partly%2520attributable%2520to%250Atheir%2520exceptional%2520color%2520perception.%2520In%2520this%2520paper%252C%2520we%2520pioneer%2520an%2520innovative%250Aviewpoint%2520on%2520few-shot%2520learning%2520by%2520simulating%2520human%2520color%2520perception%2520mechanisms.%250AWe%2520propose%2520the%2520ColorSense%2520Learner%252C%2520a%2520bio-inspired%2520meta-learning%2520framework%2520that%250Acapitalizes%2520on%2520inter-channel%2520feature%2520extraction%2520and%2520interactive%2520learning.%2520By%250Astrategically%2520emphasizing%2520distinct%2520color%2520information%2520across%2520different%2520channels%252C%250Aour%2520approach%2520effectively%2520filters%2520irrelevant%2520features%2520while%2520capturing%250Adiscriminative%2520characteristics.%2520Color%2520information%2520represents%2520the%2520most%2520intuitive%250Avisual%2520feature%252C%2520yet%2520conventional%2520meta-learning%2520methods%2520have%2520predominantly%250Aneglected%2520this%2520aspect%252C%2520focusing%2520instead%2520on%2520abstract%2520feature%2520differentiation%250Aacross%2520categories.%2520Our%2520framework%2520bridges%2520the%2520gap%2520via%2520synergistic%2520color-channel%250Ainteractions%252C%2520enabling%2520better%2520intra-class%2520commonality%2520extraction%2520and%2520larger%250Ainter-class%2520differences.%2520Furthermore%252C%2520we%2520introduce%2520a%2520meta-distiller%2520based%2520on%250Aknowledge%2520distillation%252C%2520ColorSense%2520Distiller%252C%2520which%2520incorporates%2520prior%2520teacher%250Aknowledge%2520to%2520augment%2520the%2520student%2520network%2527s%2520meta-learning%2520capacity.%2520We%2527ve%250Aconducted%2520comprehensive%2520coarse/fine-grained%2520and%2520cross-domain%2520experiments%2520on%250Aeleven%2520few-shot%2520benchmarks%2520for%2520validation.%2520Numerous%2520experiments%2520reveal%2520that%2520our%250Amethods%2520have%2520extremely%2520strong%2520generalization%2520ability%252C%2520robustness%252C%2520and%250Atransferability%252C%2520and%2520effortless%2520handle%2520few-shot%2520classification%2520from%2520the%250Aperspective%2520of%2520color%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22136v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Color%20as%20the%20Impetus%3A%20Transforming%20Few-Shot%20Learner&entry.906535625=Chaofei%20Qi%20and%20Zhitai%20Liu%20and%20Jianbin%20Qiu&entry.1292438233=%20%20Humans%20possess%20innate%20meta-learning%20capabilities%2C%20partly%20attributable%20to%0Atheir%20exceptional%20color%20perception.%20In%20this%20paper%2C%20we%20pioneer%20an%20innovative%0Aviewpoint%20on%20few-shot%20learning%20by%20simulating%20human%20color%20perception%20mechanisms.%0AWe%20propose%20the%20ColorSense%20Learner%2C%20a%20bio-inspired%20meta-learning%20framework%20that%0Acapitalizes%20on%20inter-channel%20feature%20extraction%20and%20interactive%20learning.%20By%0Astrategically%20emphasizing%20distinct%20color%20information%20across%20different%20channels%2C%0Aour%20approach%20effectively%20filters%20irrelevant%20features%20while%20capturing%0Adiscriminative%20characteristics.%20Color%20information%20represents%20the%20most%20intuitive%0Avisual%20feature%2C%20yet%20conventional%20meta-learning%20methods%20have%20predominantly%0Aneglected%20this%20aspect%2C%20focusing%20instead%20on%20abstract%20feature%20differentiation%0Aacross%20categories.%20Our%20framework%20bridges%20the%20gap%20via%20synergistic%20color-channel%0Ainteractions%2C%20enabling%20better%20intra-class%20commonality%20extraction%20and%20larger%0Ainter-class%20differences.%20Furthermore%2C%20we%20introduce%20a%20meta-distiller%20based%20on%0Aknowledge%20distillation%2C%20ColorSense%20Distiller%2C%20which%20incorporates%20prior%20teacher%0Aknowledge%20to%20augment%20the%20student%20network%27s%20meta-learning%20capacity.%20We%27ve%0Aconducted%20comprehensive%20coarse/fine-grained%20and%20cross-domain%20experiments%20on%0Aeleven%20few-shot%20benchmarks%20for%20validation.%20Numerous%20experiments%20reveal%20that%20our%0Amethods%20have%20extremely%20strong%20generalization%20ability%2C%20robustness%2C%20and%0Atransferability%2C%20and%20effortless%20handle%20few-shot%20classification%20from%20the%0Aperspective%20of%20color%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.22136v2&entry.124074799=Read"},
{"title": "Neutral Residues: Revisiting Adapters for Model Extension", "author": "Franck Signe Talla and Edouard Grave and Herv\u00e9 J\u00e9gou", "abstract": "  We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.\n", "link": "http://arxiv.org/abs/2410.02744v3", "date": "2025-07-31", "relevancy": 2.6155, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neutral%20Residues%3A%20Revisiting%20Adapters%20for%20Model%20Extension&body=Title%3A%20Neutral%20Residues%3A%20Revisiting%20Adapters%20for%20Model%20Extension%0AAuthor%3A%20Franck%20Signe%20Talla%20and%20Edouard%20Grave%20and%20Herv%C3%A9%20J%C3%A9gou%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20extending%20a%20pretrained%20large%20language%20model%20to%20a%0Anew%20domain%20that%20was%20not%20seen%20during%20training.%20Standard%20techniques%2C%20such%20as%0Afinetuning%20or%20low-rank%20adaptation%20%28LoRA%29%20are%20successful%20at%20domain%20adaptation%2C%0Abut%20do%20not%20formally%20add%20capacity%20to%20the%20model.%20This%20often%20leads%20to%20a%20trade-off%2C%0Abetween%20performing%20well%20on%20the%20new%20domain%20vs.%20degrading%20performance%20on%20the%0Aoriginal%20domain.%20Here%2C%20we%20revisit%20and%20improve%20adapters%20to%20extend%20LLMs%20from%0Athree%20angles%3A%20data%2C%20architecture%20and%20training%20procedure%2C%20which%20are%0Aadvantageously%20considered%20jointly.%20The%20resulting%20method%2C%20called%20neutral%0Aresidues%2C%20modifies%20adapters%20in%20a%20way%20that%20leads%20each%20new%20residual%20block%20to%0Aoutput%20near-zeros%20on%20the%20original%20domain.%20This%20solution%20leads%20to%20strong%20results%0Awhen%20adapting%20a%20state-of-the-art%20model%20originally%20trained%20on%20English%20to%20a%20new%0Alanguage.%20Neutral%20residues%20significantly%20outperform%20competing%20approaches%20such%0Aas%20finetuning%2C%20LoRA%20or%20vanilla%20adapters%20in%20terms%20of%20the%20trade-off%20between%0Alearning%20the%20new%20language%20and%20not%20forgetting%20English.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02744v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeutral%2520Residues%253A%2520Revisiting%2520Adapters%2520for%2520Model%2520Extension%26entry.906535625%3DFranck%2520Signe%2520Talla%2520and%2520Edouard%2520Grave%2520and%2520Herv%25C3%25A9%2520J%25C3%25A9gou%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520extending%2520a%2520pretrained%2520large%2520language%2520model%2520to%2520a%250Anew%2520domain%2520that%2520was%2520not%2520seen%2520during%2520training.%2520Standard%2520techniques%252C%2520such%2520as%250Afinetuning%2520or%2520low-rank%2520adaptation%2520%2528LoRA%2529%2520are%2520successful%2520at%2520domain%2520adaptation%252C%250Abut%2520do%2520not%2520formally%2520add%2520capacity%2520to%2520the%2520model.%2520This%2520often%2520leads%2520to%2520a%2520trade-off%252C%250Abetween%2520performing%2520well%2520on%2520the%2520new%2520domain%2520vs.%2520degrading%2520performance%2520on%2520the%250Aoriginal%2520domain.%2520Here%252C%2520we%2520revisit%2520and%2520improve%2520adapters%2520to%2520extend%2520LLMs%2520from%250Athree%2520angles%253A%2520data%252C%2520architecture%2520and%2520training%2520procedure%252C%2520which%2520are%250Aadvantageously%2520considered%2520jointly.%2520The%2520resulting%2520method%252C%2520called%2520neutral%250Aresidues%252C%2520modifies%2520adapters%2520in%2520a%2520way%2520that%2520leads%2520each%2520new%2520residual%2520block%2520to%250Aoutput%2520near-zeros%2520on%2520the%2520original%2520domain.%2520This%2520solution%2520leads%2520to%2520strong%2520results%250Awhen%2520adapting%2520a%2520state-of-the-art%2520model%2520originally%2520trained%2520on%2520English%2520to%2520a%2520new%250Alanguage.%2520Neutral%2520residues%2520significantly%2520outperform%2520competing%2520approaches%2520such%250Aas%2520finetuning%252C%2520LoRA%2520or%2520vanilla%2520adapters%2520in%2520terms%2520of%2520the%2520trade-off%2520between%250Alearning%2520the%2520new%2520language%2520and%2520not%2520forgetting%2520English.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02744v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neutral%20Residues%3A%20Revisiting%20Adapters%20for%20Model%20Extension&entry.906535625=Franck%20Signe%20Talla%20and%20Edouard%20Grave%20and%20Herv%C3%A9%20J%C3%A9gou&entry.1292438233=%20%20We%20address%20the%20problem%20of%20extending%20a%20pretrained%20large%20language%20model%20to%20a%0Anew%20domain%20that%20was%20not%20seen%20during%20training.%20Standard%20techniques%2C%20such%20as%0Afinetuning%20or%20low-rank%20adaptation%20%28LoRA%29%20are%20successful%20at%20domain%20adaptation%2C%0Abut%20do%20not%20formally%20add%20capacity%20to%20the%20model.%20This%20often%20leads%20to%20a%20trade-off%2C%0Abetween%20performing%20well%20on%20the%20new%20domain%20vs.%20degrading%20performance%20on%20the%0Aoriginal%20domain.%20Here%2C%20we%20revisit%20and%20improve%20adapters%20to%20extend%20LLMs%20from%0Athree%20angles%3A%20data%2C%20architecture%20and%20training%20procedure%2C%20which%20are%0Aadvantageously%20considered%20jointly.%20The%20resulting%20method%2C%20called%20neutral%0Aresidues%2C%20modifies%20adapters%20in%20a%20way%20that%20leads%20each%20new%20residual%20block%20to%0Aoutput%20near-zeros%20on%20the%20original%20domain.%20This%20solution%20leads%20to%20strong%20results%0Awhen%20adapting%20a%20state-of-the-art%20model%20originally%20trained%20on%20English%20to%20a%20new%0Alanguage.%20Neutral%20residues%20significantly%20outperform%20competing%20approaches%20such%0Aas%20finetuning%2C%20LoRA%20or%20vanilla%20adapters%20in%20terms%20of%20the%20trade-off%20between%0Alearning%20the%20new%20language%20and%20not%20forgetting%20English.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02744v3&entry.124074799=Read"},
{"title": "Consistent Point Matching", "author": "Halid Ziya Yerebakan and Gerardo Hermosillo Valadez", "abstract": "  This study demonstrates that incorporating a consistency heuristic into the\npoint-matching algorithm \\cite{yerebakan2023hierarchical} improves robustness\nin matching anatomical locations across pairs of medical images. We validated\nour approach on diverse longitudinal internal and public datasets spanning CT\nand MRI modalities. Notably, it surpasses state-of-the-art results on the Deep\nLesion Tracking dataset. Additionally, we show that the method effectively\naddresses landmark localization. The algorithm operates efficiently on standard\nCPU hardware and allows configurable trade-offs between speed and robustness.\nThe method enables high-precision navigation between medical images without\nrequiring a machine learning model or training data.\n", "link": "http://arxiv.org/abs/2507.23609v1", "date": "2025-07-31", "relevancy": 2.609, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5868}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4966}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Point%20Matching&body=Title%3A%20Consistent%20Point%20Matching%0AAuthor%3A%20Halid%20Ziya%20Yerebakan%20and%20Gerardo%20Hermosillo%20Valadez%0AAbstract%3A%20%20%20This%20study%20demonstrates%20that%20incorporating%20a%20consistency%20heuristic%20into%20the%0Apoint-matching%20algorithm%20%5Ccite%7Byerebakan2023hierarchical%7D%20improves%20robustness%0Ain%20matching%20anatomical%20locations%20across%20pairs%20of%20medical%20images.%20We%20validated%0Aour%20approach%20on%20diverse%20longitudinal%20internal%20and%20public%20datasets%20spanning%20CT%0Aand%20MRI%20modalities.%20Notably%2C%20it%20surpasses%20state-of-the-art%20results%20on%20the%20Deep%0ALesion%20Tracking%20dataset.%20Additionally%2C%20we%20show%20that%20the%20method%20effectively%0Aaddresses%20landmark%20localization.%20The%20algorithm%20operates%20efficiently%20on%20standard%0ACPU%20hardware%20and%20allows%20configurable%20trade-offs%20between%20speed%20and%20robustness.%0AThe%20method%20enables%20high-precision%20navigation%20between%20medical%20images%20without%0Arequiring%20a%20machine%20learning%20model%20or%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Point%2520Matching%26entry.906535625%3DHalid%2520Ziya%2520Yerebakan%2520and%2520Gerardo%2520Hermosillo%2520Valadez%26entry.1292438233%3D%2520%2520This%2520study%2520demonstrates%2520that%2520incorporating%2520a%2520consistency%2520heuristic%2520into%2520the%250Apoint-matching%2520algorithm%2520%255Ccite%257Byerebakan2023hierarchical%257D%2520improves%2520robustness%250Ain%2520matching%2520anatomical%2520locations%2520across%2520pairs%2520of%2520medical%2520images.%2520We%2520validated%250Aour%2520approach%2520on%2520diverse%2520longitudinal%2520internal%2520and%2520public%2520datasets%2520spanning%2520CT%250Aand%2520MRI%2520modalities.%2520Notably%252C%2520it%2520surpasses%2520state-of-the-art%2520results%2520on%2520the%2520Deep%250ALesion%2520Tracking%2520dataset.%2520Additionally%252C%2520we%2520show%2520that%2520the%2520method%2520effectively%250Aaddresses%2520landmark%2520localization.%2520The%2520algorithm%2520operates%2520efficiently%2520on%2520standard%250ACPU%2520hardware%2520and%2520allows%2520configurable%2520trade-offs%2520between%2520speed%2520and%2520robustness.%250AThe%2520method%2520enables%2520high-precision%2520navigation%2520between%2520medical%2520images%2520without%250Arequiring%2520a%2520machine%2520learning%2520model%2520or%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Point%20Matching&entry.906535625=Halid%20Ziya%20Yerebakan%20and%20Gerardo%20Hermosillo%20Valadez&entry.1292438233=%20%20This%20study%20demonstrates%20that%20incorporating%20a%20consistency%20heuristic%20into%20the%0Apoint-matching%20algorithm%20%5Ccite%7Byerebakan2023hierarchical%7D%20improves%20robustness%0Ain%20matching%20anatomical%20locations%20across%20pairs%20of%20medical%20images.%20We%20validated%0Aour%20approach%20on%20diverse%20longitudinal%20internal%20and%20public%20datasets%20spanning%20CT%0Aand%20MRI%20modalities.%20Notably%2C%20it%20surpasses%20state-of-the-art%20results%20on%20the%20Deep%0ALesion%20Tracking%20dataset.%20Additionally%2C%20we%20show%20that%20the%20method%20effectively%0Aaddresses%20landmark%20localization.%20The%20algorithm%20operates%20efficiently%20on%20standard%0ACPU%20hardware%20and%20allows%20configurable%20trade-offs%20between%20speed%20and%20robustness.%0AThe%20method%20enables%20high-precision%20navigation%20between%20medical%20images%20without%0Arequiring%20a%20machine%20learning%20model%20or%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23609v1&entry.124074799=Read"},
{"title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models", "author": "Ailiang Lin and Zhuoyun Li and Kotaro Funakoshi", "abstract": "  Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.\n", "link": "http://arxiv.org/abs/2507.23386v1", "date": "2025-07-31", "relevancy": 2.5912, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5314}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal2Vec%3A%20Improving%20Decoder-only%20LLMs%20as%20Versatile%20Embedding%20Models&body=Title%3A%20Causal2Vec%3A%20Improving%20Decoder-only%20LLMs%20as%20Versatile%20Embedding%20Models%0AAuthor%3A%20Ailiang%20Lin%20and%20Zhuoyun%20Li%20and%20Kotaro%20Funakoshi%0AAbstract%3A%20%20%20Decoder-only%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20to%20build%0Aembedding%20models%20that%20effectively%20encode%20the%20semantic%20information%20of%20natural%0Alanguage%20texts%20into%20dense%20vector%20representations%20for%20various%20embedding%20tasks.%0AHowever%2C%20many%20existing%20methods%20primarily%20focus%20on%20removing%20the%20causal%20attention%0Amask%20in%20LLMs%20to%20enable%20bidirectional%20attention%2C%20potentially%20undermining%20the%0Amodel%27s%20ability%20to%20extract%20semantic%20information%20acquired%20during%20pretraining.%0AAdditionally%2C%20leading%20unidirectional%20approaches%20often%20rely%20on%20extra%20input%20text%0Ato%20overcome%20the%20inherent%20limitations%20of%20causal%20attention%2C%20inevitably%20increasing%0Acomputational%20costs.%20In%20this%20work%2C%20we%20propose%20Causal2Vec%2C%20a%20general-purpose%0Aembedding%20model%20tailored%20to%20enhance%20the%20performance%20of%20decoder-only%20LLMs%0Awithout%20altering%20their%20original%20architectures%20or%20introducing%20significant%0Acomputational%20overhead.%20Specifically%2C%20we%20first%20employ%20a%20lightweight%20BERT-style%0Amodel%20to%20pre-encode%20the%20input%20text%20into%20a%20single%20Contextual%20token%2C%20which%20is%0Athen%20prepended%20to%20the%20LLM%27s%20input%20sequence%2C%20allowing%20each%20token%20to%20capture%0Acontextualized%20information%20even%20without%20attending%20to%20future%20tokens.%0AFurthermore%2C%20to%20mitigate%20the%20recency%20bias%20introduced%20by%20last-token%20pooling%20and%0Ahelp%20LLMs%20better%20leverage%20the%20semantic%20information%20encoded%20in%20the%20Contextual%0Atoken%2C%20we%20concatenate%20the%20last%20hidden%20states%20of%20Contextual%20and%20EOS%20tokens%20as%0Athe%20final%20text%20embedding.%20In%20practice%2C%20Causal2Vec%20achieves%20state-of-the-art%0Aperformance%20on%20the%20Massive%20Text%20Embeddings%20Benchmark%20%28MTEB%29%20among%20models%0Atrained%20solely%20on%20publicly%20available%20retrieval%20datasets%2C%20while%20reducing%20the%0Arequired%20sequence%20length%20by%20up%20to%2085%25%20and%20inference%20time%20by%20up%20to%2082%25%20compared%0Ato%20best-performing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal2Vec%253A%2520Improving%2520Decoder-only%2520LLMs%2520as%2520Versatile%2520Embedding%2520Models%26entry.906535625%3DAiliang%2520Lin%2520and%2520Zhuoyun%2520Li%2520and%2520Kotaro%2520Funakoshi%26entry.1292438233%3D%2520%2520Decoder-only%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520to%2520build%250Aembedding%2520models%2520that%2520effectively%2520encode%2520the%2520semantic%2520information%2520of%2520natural%250Alanguage%2520texts%2520into%2520dense%2520vector%2520representations%2520for%2520various%2520embedding%2520tasks.%250AHowever%252C%2520many%2520existing%2520methods%2520primarily%2520focus%2520on%2520removing%2520the%2520causal%2520attention%250Amask%2520in%2520LLMs%2520to%2520enable%2520bidirectional%2520attention%252C%2520potentially%2520undermining%2520the%250Amodel%2527s%2520ability%2520to%2520extract%2520semantic%2520information%2520acquired%2520during%2520pretraining.%250AAdditionally%252C%2520leading%2520unidirectional%2520approaches%2520often%2520rely%2520on%2520extra%2520input%2520text%250Ato%2520overcome%2520the%2520inherent%2520limitations%2520of%2520causal%2520attention%252C%2520inevitably%2520increasing%250Acomputational%2520costs.%2520In%2520this%2520work%252C%2520we%2520propose%2520Causal2Vec%252C%2520a%2520general-purpose%250Aembedding%2520model%2520tailored%2520to%2520enhance%2520the%2520performance%2520of%2520decoder-only%2520LLMs%250Awithout%2520altering%2520their%2520original%2520architectures%2520or%2520introducing%2520significant%250Acomputational%2520overhead.%2520Specifically%252C%2520we%2520first%2520employ%2520a%2520lightweight%2520BERT-style%250Amodel%2520to%2520pre-encode%2520the%2520input%2520text%2520into%2520a%2520single%2520Contextual%2520token%252C%2520which%2520is%250Athen%2520prepended%2520to%2520the%2520LLM%2527s%2520input%2520sequence%252C%2520allowing%2520each%2520token%2520to%2520capture%250Acontextualized%2520information%2520even%2520without%2520attending%2520to%2520future%2520tokens.%250AFurthermore%252C%2520to%2520mitigate%2520the%2520recency%2520bias%2520introduced%2520by%2520last-token%2520pooling%2520and%250Ahelp%2520LLMs%2520better%2520leverage%2520the%2520semantic%2520information%2520encoded%2520in%2520the%2520Contextual%250Atoken%252C%2520we%2520concatenate%2520the%2520last%2520hidden%2520states%2520of%2520Contextual%2520and%2520EOS%2520tokens%2520as%250Athe%2520final%2520text%2520embedding.%2520In%2520practice%252C%2520Causal2Vec%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520the%2520Massive%2520Text%2520Embeddings%2520Benchmark%2520%2528MTEB%2529%2520among%2520models%250Atrained%2520solely%2520on%2520publicly%2520available%2520retrieval%2520datasets%252C%2520while%2520reducing%2520the%250Arequired%2520sequence%2520length%2520by%2520up%2520to%252085%2525%2520and%2520inference%2520time%2520by%2520up%2520to%252082%2525%2520compared%250Ato%2520best-performing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal2Vec%3A%20Improving%20Decoder-only%20LLMs%20as%20Versatile%20Embedding%20Models&entry.906535625=Ailiang%20Lin%20and%20Zhuoyun%20Li%20and%20Kotaro%20Funakoshi&entry.1292438233=%20%20Decoder-only%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20to%20build%0Aembedding%20models%20that%20effectively%20encode%20the%20semantic%20information%20of%20natural%0Alanguage%20texts%20into%20dense%20vector%20representations%20for%20various%20embedding%20tasks.%0AHowever%2C%20many%20existing%20methods%20primarily%20focus%20on%20removing%20the%20causal%20attention%0Amask%20in%20LLMs%20to%20enable%20bidirectional%20attention%2C%20potentially%20undermining%20the%0Amodel%27s%20ability%20to%20extract%20semantic%20information%20acquired%20during%20pretraining.%0AAdditionally%2C%20leading%20unidirectional%20approaches%20often%20rely%20on%20extra%20input%20text%0Ato%20overcome%20the%20inherent%20limitations%20of%20causal%20attention%2C%20inevitably%20increasing%0Acomputational%20costs.%20In%20this%20work%2C%20we%20propose%20Causal2Vec%2C%20a%20general-purpose%0Aembedding%20model%20tailored%20to%20enhance%20the%20performance%20of%20decoder-only%20LLMs%0Awithout%20altering%20their%20original%20architectures%20or%20introducing%20significant%0Acomputational%20overhead.%20Specifically%2C%20we%20first%20employ%20a%20lightweight%20BERT-style%0Amodel%20to%20pre-encode%20the%20input%20text%20into%20a%20single%20Contextual%20token%2C%20which%20is%0Athen%20prepended%20to%20the%20LLM%27s%20input%20sequence%2C%20allowing%20each%20token%20to%20capture%0Acontextualized%20information%20even%20without%20attending%20to%20future%20tokens.%0AFurthermore%2C%20to%20mitigate%20the%20recency%20bias%20introduced%20by%20last-token%20pooling%20and%0Ahelp%20LLMs%20better%20leverage%20the%20semantic%20information%20encoded%20in%20the%20Contextual%0Atoken%2C%20we%20concatenate%20the%20last%20hidden%20states%20of%20Contextual%20and%20EOS%20tokens%20as%0Athe%20final%20text%20embedding.%20In%20practice%2C%20Causal2Vec%20achieves%20state-of-the-art%0Aperformance%20on%20the%20Massive%20Text%20Embeddings%20Benchmark%20%28MTEB%29%20among%20models%0Atrained%20solely%20on%20publicly%20available%20retrieval%20datasets%2C%20while%20reducing%20the%0Arequired%20sequence%20length%20by%20up%20to%2085%25%20and%20inference%20time%20by%20up%20to%2082%25%20compared%0Ato%20best-performing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23386v1&entry.124074799=Read"},
{"title": "Half-Physics: Enabling Kinematic 3D Human Model with Physical\n  Interactions", "author": "Li Siyao and Yao Feng and Omid Tehari and Chen Change Loy and Michael J. Black", "abstract": "  While current general-purpose 3D human models (e.g., SMPL-X) efficiently\nrepresent accurate human shape and pose, they lacks the ability to physically\ninteract with the environment due to the kinematic nature. As a result,\nkinematic-based interaction models often suffer from issues such as\ninterpenetration and unrealistic object dynamics. To address this limitation,\nwe introduce a novel approach that embeds SMPL-X into a tangible entity capable\nof dynamic physical interactions with its surroundings. Specifically, we\npropose a \"half-physics\" mechanism that transforms 3D kinematic motion into a\nphysics simulation. Our approach maintains kinematic control over inherent\nSMPL-X poses while ensuring physically plausible interactions with scenes and\nobjects, effectively eliminating penetration and unrealistic object dynamics.\nUnlike reinforcement learning-based methods, which demand extensive and complex\ntraining, our half-physics method is learning-free and generalizes to any body\nshape and motion; meanwhile, it operates in real time. Moreover, it preserves\nthe fidelity of the original kinematic motion while seamlessly integrating\nphysical interactions\n", "link": "http://arxiv.org/abs/2507.23778v1", "date": "2025-07-31", "relevancy": 2.5851, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6807}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Half-Physics%3A%20Enabling%20Kinematic%203D%20Human%20Model%20with%20Physical%0A%20%20Interactions&body=Title%3A%20Half-Physics%3A%20Enabling%20Kinematic%203D%20Human%20Model%20with%20Physical%0A%20%20Interactions%0AAuthor%3A%20Li%20Siyao%20and%20Yao%20Feng%20and%20Omid%20Tehari%20and%20Chen%20Change%20Loy%20and%20Michael%20J.%20Black%0AAbstract%3A%20%20%20While%20current%20general-purpose%203D%20human%20models%20%28e.g.%2C%20SMPL-X%29%20efficiently%0Arepresent%20accurate%20human%20shape%20and%20pose%2C%20they%20lacks%20the%20ability%20to%20physically%0Ainteract%20with%20the%20environment%20due%20to%20the%20kinematic%20nature.%20As%20a%20result%2C%0Akinematic-based%20interaction%20models%20often%20suffer%20from%20issues%20such%20as%0Ainterpenetration%20and%20unrealistic%20object%20dynamics.%20To%20address%20this%20limitation%2C%0Awe%20introduce%20a%20novel%20approach%20that%20embeds%20SMPL-X%20into%20a%20tangible%20entity%20capable%0Aof%20dynamic%20physical%20interactions%20with%20its%20surroundings.%20Specifically%2C%20we%0Apropose%20a%20%22half-physics%22%20mechanism%20that%20transforms%203D%20kinematic%20motion%20into%20a%0Aphysics%20simulation.%20Our%20approach%20maintains%20kinematic%20control%20over%20inherent%0ASMPL-X%20poses%20while%20ensuring%20physically%20plausible%20interactions%20with%20scenes%20and%0Aobjects%2C%20effectively%20eliminating%20penetration%20and%20unrealistic%20object%20dynamics.%0AUnlike%20reinforcement%20learning-based%20methods%2C%20which%20demand%20extensive%20and%20complex%0Atraining%2C%20our%20half-physics%20method%20is%20learning-free%20and%20generalizes%20to%20any%20body%0Ashape%20and%20motion%3B%20meanwhile%2C%20it%20operates%20in%20real%20time.%20Moreover%2C%20it%20preserves%0Athe%20fidelity%20of%20the%20original%20kinematic%20motion%20while%20seamlessly%20integrating%0Aphysical%20interactions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHalf-Physics%253A%2520Enabling%2520Kinematic%25203D%2520Human%2520Model%2520with%2520Physical%250A%2520%2520Interactions%26entry.906535625%3DLi%2520Siyao%2520and%2520Yao%2520Feng%2520and%2520Omid%2520Tehari%2520and%2520Chen%2520Change%2520Loy%2520and%2520Michael%2520J.%2520Black%26entry.1292438233%3D%2520%2520While%2520current%2520general-purpose%25203D%2520human%2520models%2520%2528e.g.%252C%2520SMPL-X%2529%2520efficiently%250Arepresent%2520accurate%2520human%2520shape%2520and%2520pose%252C%2520they%2520lacks%2520the%2520ability%2520to%2520physically%250Ainteract%2520with%2520the%2520environment%2520due%2520to%2520the%2520kinematic%2520nature.%2520As%2520a%2520result%252C%250Akinematic-based%2520interaction%2520models%2520often%2520suffer%2520from%2520issues%2520such%2520as%250Ainterpenetration%2520and%2520unrealistic%2520object%2520dynamics.%2520To%2520address%2520this%2520limitation%252C%250Awe%2520introduce%2520a%2520novel%2520approach%2520that%2520embeds%2520SMPL-X%2520into%2520a%2520tangible%2520entity%2520capable%250Aof%2520dynamic%2520physical%2520interactions%2520with%2520its%2520surroundings.%2520Specifically%252C%2520we%250Apropose%2520a%2520%2522half-physics%2522%2520mechanism%2520that%2520transforms%25203D%2520kinematic%2520motion%2520into%2520a%250Aphysics%2520simulation.%2520Our%2520approach%2520maintains%2520kinematic%2520control%2520over%2520inherent%250ASMPL-X%2520poses%2520while%2520ensuring%2520physically%2520plausible%2520interactions%2520with%2520scenes%2520and%250Aobjects%252C%2520effectively%2520eliminating%2520penetration%2520and%2520unrealistic%2520object%2520dynamics.%250AUnlike%2520reinforcement%2520learning-based%2520methods%252C%2520which%2520demand%2520extensive%2520and%2520complex%250Atraining%252C%2520our%2520half-physics%2520method%2520is%2520learning-free%2520and%2520generalizes%2520to%2520any%2520body%250Ashape%2520and%2520motion%253B%2520meanwhile%252C%2520it%2520operates%2520in%2520real%2520time.%2520Moreover%252C%2520it%2520preserves%250Athe%2520fidelity%2520of%2520the%2520original%2520kinematic%2520motion%2520while%2520seamlessly%2520integrating%250Aphysical%2520interactions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Half-Physics%3A%20Enabling%20Kinematic%203D%20Human%20Model%20with%20Physical%0A%20%20Interactions&entry.906535625=Li%20Siyao%20and%20Yao%20Feng%20and%20Omid%20Tehari%20and%20Chen%20Change%20Loy%20and%20Michael%20J.%20Black&entry.1292438233=%20%20While%20current%20general-purpose%203D%20human%20models%20%28e.g.%2C%20SMPL-X%29%20efficiently%0Arepresent%20accurate%20human%20shape%20and%20pose%2C%20they%20lacks%20the%20ability%20to%20physically%0Ainteract%20with%20the%20environment%20due%20to%20the%20kinematic%20nature.%20As%20a%20result%2C%0Akinematic-based%20interaction%20models%20often%20suffer%20from%20issues%20such%20as%0Ainterpenetration%20and%20unrealistic%20object%20dynamics.%20To%20address%20this%20limitation%2C%0Awe%20introduce%20a%20novel%20approach%20that%20embeds%20SMPL-X%20into%20a%20tangible%20entity%20capable%0Aof%20dynamic%20physical%20interactions%20with%20its%20surroundings.%20Specifically%2C%20we%0Apropose%20a%20%22half-physics%22%20mechanism%20that%20transforms%203D%20kinematic%20motion%20into%20a%0Aphysics%20simulation.%20Our%20approach%20maintains%20kinematic%20control%20over%20inherent%0ASMPL-X%20poses%20while%20ensuring%20physically%20plausible%20interactions%20with%20scenes%20and%0Aobjects%2C%20effectively%20eliminating%20penetration%20and%20unrealistic%20object%20dynamics.%0AUnlike%20reinforcement%20learning-based%20methods%2C%20which%20demand%20extensive%20and%20complex%0Atraining%2C%20our%20half-physics%20method%20is%20learning-free%20and%20generalizes%20to%20any%20body%0Ashape%20and%20motion%3B%20meanwhile%2C%20it%20operates%20in%20real%20time.%20Moreover%2C%20it%20preserves%0Athe%20fidelity%20of%20the%20original%20kinematic%20motion%20while%20seamlessly%20integrating%0Aphysical%20interactions%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23778v1&entry.124074799=Read"},
{"title": "ART: Adaptive Relation Tuning for Generalized Relation Prediction", "author": "Gopika Sudhakaran and Hikaru Shindo and Patrick Schramowski and Simone Schaub-Meyer and Kristian Kersting and Stefan Roth", "abstract": "  Visual relation detection (VRD) is the task of identifying the relationships\nbetween objects in a scene. VRD models trained solely on relation detection\ndata struggle to generalize beyond the relations on which they are trained.\nWhile prompt tuning has been used to adapt vision-language models (VLMs) for\nVRD, it uses handcrafted prompts and struggles with novel or complex relations.\nWe argue that instruction tuning offers a more effective solution by\nfine-tuning VLMs on diverse instructional data. We thus introduce ART, an\nAdaptive Relation Tuning framework that adapts VLMs for VRD through instruction\ntuning and strategic instance selection. By converting VRD datasets into an\ninstruction tuning format and employing an adaptive sampling algorithm, ART\ndirects the VLM to focus on informative relations while maintaining\ngeneralizability. Specifically, we focus on the relation classification, where\nsubject-object boxes are given and the model predicts the predicate between\nthem. We tune on a held-in set and evaluate across multiple held-out datasets\nof varying complexity. Our approach strongly improves over its baselines and\ncan infer unseen relation concepts, a capability absent in mainstream VRD\nmethods. We demonstrate ART's practical value by using the predicted relations\nfor segmenting complex scenes.\n", "link": "http://arxiv.org/abs/2507.23543v1", "date": "2025-07-31", "relevancy": 2.5644, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ART%3A%20Adaptive%20Relation%20Tuning%20for%20Generalized%20Relation%20Prediction&body=Title%3A%20ART%3A%20Adaptive%20Relation%20Tuning%20for%20Generalized%20Relation%20Prediction%0AAuthor%3A%20Gopika%20Sudhakaran%20and%20Hikaru%20Shindo%20and%20Patrick%20Schramowski%20and%20Simone%20Schaub-Meyer%20and%20Kristian%20Kersting%20and%20Stefan%20Roth%0AAbstract%3A%20%20%20Visual%20relation%20detection%20%28VRD%29%20is%20the%20task%20of%20identifying%20the%20relationships%0Abetween%20objects%20in%20a%20scene.%20VRD%20models%20trained%20solely%20on%20relation%20detection%0Adata%20struggle%20to%20generalize%20beyond%20the%20relations%20on%20which%20they%20are%20trained.%0AWhile%20prompt%20tuning%20has%20been%20used%20to%20adapt%20vision-language%20models%20%28VLMs%29%20for%0AVRD%2C%20it%20uses%20handcrafted%20prompts%20and%20struggles%20with%20novel%20or%20complex%20relations.%0AWe%20argue%20that%20instruction%20tuning%20offers%20a%20more%20effective%20solution%20by%0Afine-tuning%20VLMs%20on%20diverse%20instructional%20data.%20We%20thus%20introduce%20ART%2C%20an%0AAdaptive%20Relation%20Tuning%20framework%20that%20adapts%20VLMs%20for%20VRD%20through%20instruction%0Atuning%20and%20strategic%20instance%20selection.%20By%20converting%20VRD%20datasets%20into%20an%0Ainstruction%20tuning%20format%20and%20employing%20an%20adaptive%20sampling%20algorithm%2C%20ART%0Adirects%20the%20VLM%20to%20focus%20on%20informative%20relations%20while%20maintaining%0Ageneralizability.%20Specifically%2C%20we%20focus%20on%20the%20relation%20classification%2C%20where%0Asubject-object%20boxes%20are%20given%20and%20the%20model%20predicts%20the%20predicate%20between%0Athem.%20We%20tune%20on%20a%20held-in%20set%20and%20evaluate%20across%20multiple%20held-out%20datasets%0Aof%20varying%20complexity.%20Our%20approach%20strongly%20improves%20over%20its%20baselines%20and%0Acan%20infer%20unseen%20relation%20concepts%2C%20a%20capability%20absent%20in%20mainstream%20VRD%0Amethods.%20We%20demonstrate%20ART%27s%20practical%20value%20by%20using%20the%20predicted%20relations%0Afor%20segmenting%20complex%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DART%253A%2520Adaptive%2520Relation%2520Tuning%2520for%2520Generalized%2520Relation%2520Prediction%26entry.906535625%3DGopika%2520Sudhakaran%2520and%2520Hikaru%2520Shindo%2520and%2520Patrick%2520Schramowski%2520and%2520Simone%2520Schaub-Meyer%2520and%2520Kristian%2520Kersting%2520and%2520Stefan%2520Roth%26entry.1292438233%3D%2520%2520Visual%2520relation%2520detection%2520%2528VRD%2529%2520is%2520the%2520task%2520of%2520identifying%2520the%2520relationships%250Abetween%2520objects%2520in%2520a%2520scene.%2520VRD%2520models%2520trained%2520solely%2520on%2520relation%2520detection%250Adata%2520struggle%2520to%2520generalize%2520beyond%2520the%2520relations%2520on%2520which%2520they%2520are%2520trained.%250AWhile%2520prompt%2520tuning%2520has%2520been%2520used%2520to%2520adapt%2520vision-language%2520models%2520%2528VLMs%2529%2520for%250AVRD%252C%2520it%2520uses%2520handcrafted%2520prompts%2520and%2520struggles%2520with%2520novel%2520or%2520complex%2520relations.%250AWe%2520argue%2520that%2520instruction%2520tuning%2520offers%2520a%2520more%2520effective%2520solution%2520by%250Afine-tuning%2520VLMs%2520on%2520diverse%2520instructional%2520data.%2520We%2520thus%2520introduce%2520ART%252C%2520an%250AAdaptive%2520Relation%2520Tuning%2520framework%2520that%2520adapts%2520VLMs%2520for%2520VRD%2520through%2520instruction%250Atuning%2520and%2520strategic%2520instance%2520selection.%2520By%2520converting%2520VRD%2520datasets%2520into%2520an%250Ainstruction%2520tuning%2520format%2520and%2520employing%2520an%2520adaptive%2520sampling%2520algorithm%252C%2520ART%250Adirects%2520the%2520VLM%2520to%2520focus%2520on%2520informative%2520relations%2520while%2520maintaining%250Ageneralizability.%2520Specifically%252C%2520we%2520focus%2520on%2520the%2520relation%2520classification%252C%2520where%250Asubject-object%2520boxes%2520are%2520given%2520and%2520the%2520model%2520predicts%2520the%2520predicate%2520between%250Athem.%2520We%2520tune%2520on%2520a%2520held-in%2520set%2520and%2520evaluate%2520across%2520multiple%2520held-out%2520datasets%250Aof%2520varying%2520complexity.%2520Our%2520approach%2520strongly%2520improves%2520over%2520its%2520baselines%2520and%250Acan%2520infer%2520unseen%2520relation%2520concepts%252C%2520a%2520capability%2520absent%2520in%2520mainstream%2520VRD%250Amethods.%2520We%2520demonstrate%2520ART%2527s%2520practical%2520value%2520by%2520using%2520the%2520predicted%2520relations%250Afor%2520segmenting%2520complex%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ART%3A%20Adaptive%20Relation%20Tuning%20for%20Generalized%20Relation%20Prediction&entry.906535625=Gopika%20Sudhakaran%20and%20Hikaru%20Shindo%20and%20Patrick%20Schramowski%20and%20Simone%20Schaub-Meyer%20and%20Kristian%20Kersting%20and%20Stefan%20Roth&entry.1292438233=%20%20Visual%20relation%20detection%20%28VRD%29%20is%20the%20task%20of%20identifying%20the%20relationships%0Abetween%20objects%20in%20a%20scene.%20VRD%20models%20trained%20solely%20on%20relation%20detection%0Adata%20struggle%20to%20generalize%20beyond%20the%20relations%20on%20which%20they%20are%20trained.%0AWhile%20prompt%20tuning%20has%20been%20used%20to%20adapt%20vision-language%20models%20%28VLMs%29%20for%0AVRD%2C%20it%20uses%20handcrafted%20prompts%20and%20struggles%20with%20novel%20or%20complex%20relations.%0AWe%20argue%20that%20instruction%20tuning%20offers%20a%20more%20effective%20solution%20by%0Afine-tuning%20VLMs%20on%20diverse%20instructional%20data.%20We%20thus%20introduce%20ART%2C%20an%0AAdaptive%20Relation%20Tuning%20framework%20that%20adapts%20VLMs%20for%20VRD%20through%20instruction%0Atuning%20and%20strategic%20instance%20selection.%20By%20converting%20VRD%20datasets%20into%20an%0Ainstruction%20tuning%20format%20and%20employing%20an%20adaptive%20sampling%20algorithm%2C%20ART%0Adirects%20the%20VLM%20to%20focus%20on%20informative%20relations%20while%20maintaining%0Ageneralizability.%20Specifically%2C%20we%20focus%20on%20the%20relation%20classification%2C%20where%0Asubject-object%20boxes%20are%20given%20and%20the%20model%20predicts%20the%20predicate%20between%0Athem.%20We%20tune%20on%20a%20held-in%20set%20and%20evaluate%20across%20multiple%20held-out%20datasets%0Aof%20varying%20complexity.%20Our%20approach%20strongly%20improves%20over%20its%20baselines%20and%0Acan%20infer%20unseen%20relation%20concepts%2C%20a%20capability%20absent%20in%20mainstream%20VRD%0Amethods.%20We%20demonstrate%20ART%27s%20practical%20value%20by%20using%20the%20predicted%20relations%0Afor%20segmenting%20complex%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23543v1&entry.124074799=Read"},
{"title": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D\n  Correspondence Learning", "author": "Bingran Chen and Baorun Li and Jian Yang and Yong Liu and Guangyao Zhai", "abstract": "  High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.\n", "link": "http://arxiv.org/abs/2507.14820v2", "date": "2025-07-31", "relevancy": 2.5236, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6487}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6182}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KGN-Pro%3A%20Keypoint-Based%20Grasp%20Prediction%20through%20Probabilistic%202D-3D%0A%20%20Correspondence%20Learning&body=Title%3A%20KGN-Pro%3A%20Keypoint-Based%20Grasp%20Prediction%20through%20Probabilistic%202D-3D%0A%20%20Correspondence%20Learning%0AAuthor%3A%20Bingran%20Chen%20and%20Baorun%20Li%20and%20Jian%20Yang%20and%20Yong%20Liu%20and%20Guangyao%20Zhai%0AAbstract%3A%20%20%20High-level%20robotic%20manipulation%20tasks%20demand%20flexible%206-DoF%20grasp%20estimation%0Ato%20serve%20as%20a%20basic%20function.%20Previous%20approaches%20either%20directly%20generate%0Agrasps%20from%20point-cloud%20data%2C%20suffering%20from%20challenges%20with%20small%20objects%20and%0Asensor%20noise%2C%20or%20infer%203D%20information%20from%20RGB%20images%2C%20which%20introduces%0Aexpensive%20annotation%20requirements%20and%20discretization%20issues.%20Recent%20methods%0Amitigate%20some%20challenges%20by%20retaining%20a%202D%20representation%20to%20estimate%20grasp%0Akeypoints%20and%20applying%20Perspective-n-Point%20%28PnP%29%20algorithms%20to%20compute%206-DoF%0Aposes.%20However%2C%20these%20methods%20are%20limited%20by%20their%20non-differentiable%20nature%0Aand%20reliance%20solely%20on%202D%20supervision%2C%20which%20hinders%20the%20full%20exploitation%20of%0Arich%203D%20information.%20In%20this%20work%2C%20we%20present%20KGN-Pro%2C%20a%20novel%20grasping%20network%0Athat%20preserves%20the%20efficiency%20and%20fine-grained%20object%20grasping%20of%20previous%20KGNs%0Awhile%20integrating%20direct%203D%20optimization%20through%20probabilistic%20PnP%20layers.%0AKGN-Pro%20encodes%20paired%20RGB-D%20images%20to%20generate%20Keypoint%20Map%2C%20and%20further%0Aoutputs%20a%202D%20confidence%20map%20to%20weight%20keypoint%20contributions%20during%0Are-projection%20error%20minimization.%20By%20modeling%20the%20weighted%20sum%20of%20squared%0Are-projection%20errors%20probabilistically%2C%20the%20network%20effectively%20transmits%203D%0Asupervision%20to%20its%202D%20keypoint%20predictions%2C%20enabling%20end-to-end%20learning.%0AExperiments%20on%20both%20simulated%20and%20real-world%20platforms%20demonstrate%20that%20KGN-Pro%0Aoutperforms%20existing%20methods%20in%20terms%20of%20grasp%20cover%20rate%20and%20success%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14820v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKGN-Pro%253A%2520Keypoint-Based%2520Grasp%2520Prediction%2520through%2520Probabilistic%25202D-3D%250A%2520%2520Correspondence%2520Learning%26entry.906535625%3DBingran%2520Chen%2520and%2520Baorun%2520Li%2520and%2520Jian%2520Yang%2520and%2520Yong%2520Liu%2520and%2520Guangyao%2520Zhai%26entry.1292438233%3D%2520%2520High-level%2520robotic%2520manipulation%2520tasks%2520demand%2520flexible%25206-DoF%2520grasp%2520estimation%250Ato%2520serve%2520as%2520a%2520basic%2520function.%2520Previous%2520approaches%2520either%2520directly%2520generate%250Agrasps%2520from%2520point-cloud%2520data%252C%2520suffering%2520from%2520challenges%2520with%2520small%2520objects%2520and%250Asensor%2520noise%252C%2520or%2520infer%25203D%2520information%2520from%2520RGB%2520images%252C%2520which%2520introduces%250Aexpensive%2520annotation%2520requirements%2520and%2520discretization%2520issues.%2520Recent%2520methods%250Amitigate%2520some%2520challenges%2520by%2520retaining%2520a%25202D%2520representation%2520to%2520estimate%2520grasp%250Akeypoints%2520and%2520applying%2520Perspective-n-Point%2520%2528PnP%2529%2520algorithms%2520to%2520compute%25206-DoF%250Aposes.%2520However%252C%2520these%2520methods%2520are%2520limited%2520by%2520their%2520non-differentiable%2520nature%250Aand%2520reliance%2520solely%2520on%25202D%2520supervision%252C%2520which%2520hinders%2520the%2520full%2520exploitation%2520of%250Arich%25203D%2520information.%2520In%2520this%2520work%252C%2520we%2520present%2520KGN-Pro%252C%2520a%2520novel%2520grasping%2520network%250Athat%2520preserves%2520the%2520efficiency%2520and%2520fine-grained%2520object%2520grasping%2520of%2520previous%2520KGNs%250Awhile%2520integrating%2520direct%25203D%2520optimization%2520through%2520probabilistic%2520PnP%2520layers.%250AKGN-Pro%2520encodes%2520paired%2520RGB-D%2520images%2520to%2520generate%2520Keypoint%2520Map%252C%2520and%2520further%250Aoutputs%2520a%25202D%2520confidence%2520map%2520to%2520weight%2520keypoint%2520contributions%2520during%250Are-projection%2520error%2520minimization.%2520By%2520modeling%2520the%2520weighted%2520sum%2520of%2520squared%250Are-projection%2520errors%2520probabilistically%252C%2520the%2520network%2520effectively%2520transmits%25203D%250Asupervision%2520to%2520its%25202D%2520keypoint%2520predictions%252C%2520enabling%2520end-to-end%2520learning.%250AExperiments%2520on%2520both%2520simulated%2520and%2520real-world%2520platforms%2520demonstrate%2520that%2520KGN-Pro%250Aoutperforms%2520existing%2520methods%2520in%2520terms%2520of%2520grasp%2520cover%2520rate%2520and%2520success%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14820v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KGN-Pro%3A%20Keypoint-Based%20Grasp%20Prediction%20through%20Probabilistic%202D-3D%0A%20%20Correspondence%20Learning&entry.906535625=Bingran%20Chen%20and%20Baorun%20Li%20and%20Jian%20Yang%20and%20Yong%20Liu%20and%20Guangyao%20Zhai&entry.1292438233=%20%20High-level%20robotic%20manipulation%20tasks%20demand%20flexible%206-DoF%20grasp%20estimation%0Ato%20serve%20as%20a%20basic%20function.%20Previous%20approaches%20either%20directly%20generate%0Agrasps%20from%20point-cloud%20data%2C%20suffering%20from%20challenges%20with%20small%20objects%20and%0Asensor%20noise%2C%20or%20infer%203D%20information%20from%20RGB%20images%2C%20which%20introduces%0Aexpensive%20annotation%20requirements%20and%20discretization%20issues.%20Recent%20methods%0Amitigate%20some%20challenges%20by%20retaining%20a%202D%20representation%20to%20estimate%20grasp%0Akeypoints%20and%20applying%20Perspective-n-Point%20%28PnP%29%20algorithms%20to%20compute%206-DoF%0Aposes.%20However%2C%20these%20methods%20are%20limited%20by%20their%20non-differentiable%20nature%0Aand%20reliance%20solely%20on%202D%20supervision%2C%20which%20hinders%20the%20full%20exploitation%20of%0Arich%203D%20information.%20In%20this%20work%2C%20we%20present%20KGN-Pro%2C%20a%20novel%20grasping%20network%0Athat%20preserves%20the%20efficiency%20and%20fine-grained%20object%20grasping%20of%20previous%20KGNs%0Awhile%20integrating%20direct%203D%20optimization%20through%20probabilistic%20PnP%20layers.%0AKGN-Pro%20encodes%20paired%20RGB-D%20images%20to%20generate%20Keypoint%20Map%2C%20and%20further%0Aoutputs%20a%202D%20confidence%20map%20to%20weight%20keypoint%20contributions%20during%0Are-projection%20error%20minimization.%20By%20modeling%20the%20weighted%20sum%20of%20squared%0Are-projection%20errors%20probabilistically%2C%20the%20network%20effectively%20transmits%203D%0Asupervision%20to%20its%202D%20keypoint%20predictions%2C%20enabling%20end-to-end%20learning.%0AExperiments%20on%20both%20simulated%20and%20real-world%20platforms%20demonstrate%20that%20KGN-Pro%0Aoutperforms%20existing%20methods%20in%20terms%20of%20grasp%20cover%20rate%20and%20success%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14820v2&entry.124074799=Read"},
{"title": "Mocap-2-to-3: Multi-view Lifting for Monocular Motion Recovery with 2D\n  Pretraining", "author": "Zhumei Wang and Zechen Hu and Ruoxi Guo and Huaijin Pi and Ziyong Feng and Sida Peng and Xiaowei Zhou and Mingtao Pei and Siyuan Huang", "abstract": "  Recovering absolute human motion from monocular inputs is challenging due to\ntwo main issues. First, existing methods depend on 3D training data collected\nfrom limited environments, constraining out-of-distribution generalization. The\nsecond issue is the difficulty of estimating metric-scale poses from monocular\ninput. To address these challenges, we introduce Mocap-2-to-3, a novel\nframework that performs multi-view lifting from monocular input by leveraging\n2D data pre-training, enabling the reconstruction of metrically accurate 3D\nmotions with absolute positions. To leverage abundant 2D data, we decompose\ncomplex 3D motion into multi-view syntheses. We first pretrain a single-view\ndiffusion model on extensive 2D datasets, then fine-tune a multi-view model\nusing public 3D data to enable view-consistent motion generation from monocular\ninput, allowing the model to acquire action priors and diversity through 2D\ndata. Furthermore, to recover absolute poses, we propose a novel human motion\nrepresentation that decouples the learning of local pose and global movements,\nwhile encoding geometric priors of the ground to accelerate convergence. This\nenables progressive recovery of motion in absolute space during inference.\nExperimental results on in-the-wild benchmarks demonstrate that our method\nsurpasses state-of-the-art approaches in both camera-space motion realism and\nworld-grounded human positioning, while exhibiting superior generalization\ncapability. Our code will be made publicly available.\n", "link": "http://arxiv.org/abs/2503.03222v5", "date": "2025-07-31", "relevancy": 2.52, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6558}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6238}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mocap-2-to-3%3A%20Multi-view%20Lifting%20for%20Monocular%20Motion%20Recovery%20with%202D%0A%20%20Pretraining&body=Title%3A%20Mocap-2-to-3%3A%20Multi-view%20Lifting%20for%20Monocular%20Motion%20Recovery%20with%202D%0A%20%20Pretraining%0AAuthor%3A%20Zhumei%20Wang%20and%20Zechen%20Hu%20and%20Ruoxi%20Guo%20and%20Huaijin%20Pi%20and%20Ziyong%20Feng%20and%20Sida%20Peng%20and%20Xiaowei%20Zhou%20and%20Mingtao%20Pei%20and%20Siyuan%20Huang%0AAbstract%3A%20%20%20Recovering%20absolute%20human%20motion%20from%20monocular%20inputs%20is%20challenging%20due%20to%0Atwo%20main%20issues.%20First%2C%20existing%20methods%20depend%20on%203D%20training%20data%20collected%0Afrom%20limited%20environments%2C%20constraining%20out-of-distribution%20generalization.%20The%0Asecond%20issue%20is%20the%20difficulty%20of%20estimating%20metric-scale%20poses%20from%20monocular%0Ainput.%20To%20address%20these%20challenges%2C%20we%20introduce%20Mocap-2-to-3%2C%20a%20novel%0Aframework%20that%20performs%20multi-view%20lifting%20from%20monocular%20input%20by%20leveraging%0A2D%20data%20pre-training%2C%20enabling%20the%20reconstruction%20of%20metrically%20accurate%203D%0Amotions%20with%20absolute%20positions.%20To%20leverage%20abundant%202D%20data%2C%20we%20decompose%0Acomplex%203D%20motion%20into%20multi-view%20syntheses.%20We%20first%20pretrain%20a%20single-view%0Adiffusion%20model%20on%20extensive%202D%20datasets%2C%20then%20fine-tune%20a%20multi-view%20model%0Ausing%20public%203D%20data%20to%20enable%20view-consistent%20motion%20generation%20from%20monocular%0Ainput%2C%20allowing%20the%20model%20to%20acquire%20action%20priors%20and%20diversity%20through%202D%0Adata.%20Furthermore%2C%20to%20recover%20absolute%20poses%2C%20we%20propose%20a%20novel%20human%20motion%0Arepresentation%20that%20decouples%20the%20learning%20of%20local%20pose%20and%20global%20movements%2C%0Awhile%20encoding%20geometric%20priors%20of%20the%20ground%20to%20accelerate%20convergence.%20This%0Aenables%20progressive%20recovery%20of%20motion%20in%20absolute%20space%20during%20inference.%0AExperimental%20results%20on%20in-the-wild%20benchmarks%20demonstrate%20that%20our%20method%0Asurpasses%20state-of-the-art%20approaches%20in%20both%20camera-space%20motion%20realism%20and%0Aworld-grounded%20human%20positioning%2C%20while%20exhibiting%20superior%20generalization%0Acapability.%20Our%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.03222v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMocap-2-to-3%253A%2520Multi-view%2520Lifting%2520for%2520Monocular%2520Motion%2520Recovery%2520with%25202D%250A%2520%2520Pretraining%26entry.906535625%3DZhumei%2520Wang%2520and%2520Zechen%2520Hu%2520and%2520Ruoxi%2520Guo%2520and%2520Huaijin%2520Pi%2520and%2520Ziyong%2520Feng%2520and%2520Sida%2520Peng%2520and%2520Xiaowei%2520Zhou%2520and%2520Mingtao%2520Pei%2520and%2520Siyuan%2520Huang%26entry.1292438233%3D%2520%2520Recovering%2520absolute%2520human%2520motion%2520from%2520monocular%2520inputs%2520is%2520challenging%2520due%2520to%250Atwo%2520main%2520issues.%2520First%252C%2520existing%2520methods%2520depend%2520on%25203D%2520training%2520data%2520collected%250Afrom%2520limited%2520environments%252C%2520constraining%2520out-of-distribution%2520generalization.%2520The%250Asecond%2520issue%2520is%2520the%2520difficulty%2520of%2520estimating%2520metric-scale%2520poses%2520from%2520monocular%250Ainput.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Mocap-2-to-3%252C%2520a%2520novel%250Aframework%2520that%2520performs%2520multi-view%2520lifting%2520from%2520monocular%2520input%2520by%2520leveraging%250A2D%2520data%2520pre-training%252C%2520enabling%2520the%2520reconstruction%2520of%2520metrically%2520accurate%25203D%250Amotions%2520with%2520absolute%2520positions.%2520To%2520leverage%2520abundant%25202D%2520data%252C%2520we%2520decompose%250Acomplex%25203D%2520motion%2520into%2520multi-view%2520syntheses.%2520We%2520first%2520pretrain%2520a%2520single-view%250Adiffusion%2520model%2520on%2520extensive%25202D%2520datasets%252C%2520then%2520fine-tune%2520a%2520multi-view%2520model%250Ausing%2520public%25203D%2520data%2520to%2520enable%2520view-consistent%2520motion%2520generation%2520from%2520monocular%250Ainput%252C%2520allowing%2520the%2520model%2520to%2520acquire%2520action%2520priors%2520and%2520diversity%2520through%25202D%250Adata.%2520Furthermore%252C%2520to%2520recover%2520absolute%2520poses%252C%2520we%2520propose%2520a%2520novel%2520human%2520motion%250Arepresentation%2520that%2520decouples%2520the%2520learning%2520of%2520local%2520pose%2520and%2520global%2520movements%252C%250Awhile%2520encoding%2520geometric%2520priors%2520of%2520the%2520ground%2520to%2520accelerate%2520convergence.%2520This%250Aenables%2520progressive%2520recovery%2520of%2520motion%2520in%2520absolute%2520space%2520during%2520inference.%250AExperimental%2520results%2520on%2520in-the-wild%2520benchmarks%2520demonstrate%2520that%2520our%2520method%250Asurpasses%2520state-of-the-art%2520approaches%2520in%2520both%2520camera-space%2520motion%2520realism%2520and%250Aworld-grounded%2520human%2520positioning%252C%2520while%2520exhibiting%2520superior%2520generalization%250Acapability.%2520Our%2520code%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.03222v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mocap-2-to-3%3A%20Multi-view%20Lifting%20for%20Monocular%20Motion%20Recovery%20with%202D%0A%20%20Pretraining&entry.906535625=Zhumei%20Wang%20and%20Zechen%20Hu%20and%20Ruoxi%20Guo%20and%20Huaijin%20Pi%20and%20Ziyong%20Feng%20and%20Sida%20Peng%20and%20Xiaowei%20Zhou%20and%20Mingtao%20Pei%20and%20Siyuan%20Huang&entry.1292438233=%20%20Recovering%20absolute%20human%20motion%20from%20monocular%20inputs%20is%20challenging%20due%20to%0Atwo%20main%20issues.%20First%2C%20existing%20methods%20depend%20on%203D%20training%20data%20collected%0Afrom%20limited%20environments%2C%20constraining%20out-of-distribution%20generalization.%20The%0Asecond%20issue%20is%20the%20difficulty%20of%20estimating%20metric-scale%20poses%20from%20monocular%0Ainput.%20To%20address%20these%20challenges%2C%20we%20introduce%20Mocap-2-to-3%2C%20a%20novel%0Aframework%20that%20performs%20multi-view%20lifting%20from%20monocular%20input%20by%20leveraging%0A2D%20data%20pre-training%2C%20enabling%20the%20reconstruction%20of%20metrically%20accurate%203D%0Amotions%20with%20absolute%20positions.%20To%20leverage%20abundant%202D%20data%2C%20we%20decompose%0Acomplex%203D%20motion%20into%20multi-view%20syntheses.%20We%20first%20pretrain%20a%20single-view%0Adiffusion%20model%20on%20extensive%202D%20datasets%2C%20then%20fine-tune%20a%20multi-view%20model%0Ausing%20public%203D%20data%20to%20enable%20view-consistent%20motion%20generation%20from%20monocular%0Ainput%2C%20allowing%20the%20model%20to%20acquire%20action%20priors%20and%20diversity%20through%202D%0Adata.%20Furthermore%2C%20to%20recover%20absolute%20poses%2C%20we%20propose%20a%20novel%20human%20motion%0Arepresentation%20that%20decouples%20the%20learning%20of%20local%20pose%20and%20global%20movements%2C%0Awhile%20encoding%20geometric%20priors%20of%20the%20ground%20to%20accelerate%20convergence.%20This%0Aenables%20progressive%20recovery%20of%20motion%20in%20absolute%20space%20during%20inference.%0AExperimental%20results%20on%20in-the-wild%20benchmarks%20demonstrate%20that%20our%20method%0Asurpasses%20state-of-the-art%20approaches%20in%20both%20camera-space%20motion%20realism%20and%0Aworld-grounded%20human%20positioning%2C%20while%20exhibiting%20superior%20generalization%0Acapability.%20Our%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.03222v5&entry.124074799=Read"},
{"title": "MolParser: End-to-end Visual Recognition of Molecule Structures in the\n  Wild", "author": "Xi Fang and Jiankun Wang and Xiaochen Cai and Shangqian Chen and Shuwen Yang and Haoyi Tao and Nan Wang and Lin Yao and Linfeng Zhang and Guolin Ke", "abstract": "  In recent decades, chemistry publications and patents have increased rapidly.\nA significant portion of key information is embedded in molecular structure\nfigures, complicating large-scale literature searches and limiting the\napplication of large language models in fields such as biology, chemistry, and\npharmaceuticals. The automatic extraction of precise chemical structures is of\ncritical importance. However, the presence of numerous Markush structures in\nreal-world documents, along with variations in molecular image quality, drawing\nstyles, and noise, significantly limits the performance of existing optical\nchemical structure recognition (OCSR) methods. We present MolParser, a novel\nend-to-end OCSR method that efficiently and accurately recognizes chemical\nstructures from real-world documents, including difficult Markush structure. We\nuse a extended SMILES encoding rule to annotate our training dataset. Under\nthis rule, we build MolParser-7M, the largest annotated molecular image dataset\nto our knowledge. While utilizing a large amount of synthetic data, we employed\nactive learning methods to incorporate substantial in-the-wild data,\nspecifically samples cropped from real patents and scientific literature, into\nthe training process. We trained an end-to-end molecular image captioning\nmodel, MolParser, using a curriculum learning approach. MolParser significantly\noutperforms classical and learning-based methods across most scenarios, with\npotential for broader downstream applications. The dataset is publicly\navailable in huggingface.\n", "link": "http://arxiv.org/abs/2411.11098v3", "date": "2025-07-31", "relevancy": 2.517, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MolParser%3A%20End-to-end%20Visual%20Recognition%20of%20Molecule%20Structures%20in%20the%0A%20%20Wild&body=Title%3A%20MolParser%3A%20End-to-end%20Visual%20Recognition%20of%20Molecule%20Structures%20in%20the%0A%20%20Wild%0AAuthor%3A%20Xi%20Fang%20and%20Jiankun%20Wang%20and%20Xiaochen%20Cai%20and%20Shangqian%20Chen%20and%20Shuwen%20Yang%20and%20Haoyi%20Tao%20and%20Nan%20Wang%20and%20Lin%20Yao%20and%20Linfeng%20Zhang%20and%20Guolin%20Ke%0AAbstract%3A%20%20%20In%20recent%20decades%2C%20chemistry%20publications%20and%20patents%20have%20increased%20rapidly.%0AA%20significant%20portion%20of%20key%20information%20is%20embedded%20in%20molecular%20structure%0Afigures%2C%20complicating%20large-scale%20literature%20searches%20and%20limiting%20the%0Aapplication%20of%20large%20language%20models%20in%20fields%20such%20as%20biology%2C%20chemistry%2C%20and%0Apharmaceuticals.%20The%20automatic%20extraction%20of%20precise%20chemical%20structures%20is%20of%0Acritical%20importance.%20However%2C%20the%20presence%20of%20numerous%20Markush%20structures%20in%0Areal-world%20documents%2C%20along%20with%20variations%20in%20molecular%20image%20quality%2C%20drawing%0Astyles%2C%20and%20noise%2C%20significantly%20limits%20the%20performance%20of%20existing%20optical%0Achemical%20structure%20recognition%20%28OCSR%29%20methods.%20We%20present%20MolParser%2C%20a%20novel%0Aend-to-end%20OCSR%20method%20that%20efficiently%20and%20accurately%20recognizes%20chemical%0Astructures%20from%20real-world%20documents%2C%20including%20difficult%20Markush%20structure.%20We%0Ause%20a%20extended%20SMILES%20encoding%20rule%20to%20annotate%20our%20training%20dataset.%20Under%0Athis%20rule%2C%20we%20build%20MolParser-7M%2C%20the%20largest%20annotated%20molecular%20image%20dataset%0Ato%20our%20knowledge.%20While%20utilizing%20a%20large%20amount%20of%20synthetic%20data%2C%20we%20employed%0Aactive%20learning%20methods%20to%20incorporate%20substantial%20in-the-wild%20data%2C%0Aspecifically%20samples%20cropped%20from%20real%20patents%20and%20scientific%20literature%2C%20into%0Athe%20training%20process.%20We%20trained%20an%20end-to-end%20molecular%20image%20captioning%0Amodel%2C%20MolParser%2C%20using%20a%20curriculum%20learning%20approach.%20MolParser%20significantly%0Aoutperforms%20classical%20and%20learning-based%20methods%20across%20most%20scenarios%2C%20with%0Apotential%20for%20broader%20downstream%20applications.%20The%20dataset%20is%20publicly%0Aavailable%20in%20huggingface.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11098v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolParser%253A%2520End-to-end%2520Visual%2520Recognition%2520of%2520Molecule%2520Structures%2520in%2520the%250A%2520%2520Wild%26entry.906535625%3DXi%2520Fang%2520and%2520Jiankun%2520Wang%2520and%2520Xiaochen%2520Cai%2520and%2520Shangqian%2520Chen%2520and%2520Shuwen%2520Yang%2520and%2520Haoyi%2520Tao%2520and%2520Nan%2520Wang%2520and%2520Lin%2520Yao%2520and%2520Linfeng%2520Zhang%2520and%2520Guolin%2520Ke%26entry.1292438233%3D%2520%2520In%2520recent%2520decades%252C%2520chemistry%2520publications%2520and%2520patents%2520have%2520increased%2520rapidly.%250AA%2520significant%2520portion%2520of%2520key%2520information%2520is%2520embedded%2520in%2520molecular%2520structure%250Afigures%252C%2520complicating%2520large-scale%2520literature%2520searches%2520and%2520limiting%2520the%250Aapplication%2520of%2520large%2520language%2520models%2520in%2520fields%2520such%2520as%2520biology%252C%2520chemistry%252C%2520and%250Apharmaceuticals.%2520The%2520automatic%2520extraction%2520of%2520precise%2520chemical%2520structures%2520is%2520of%250Acritical%2520importance.%2520However%252C%2520the%2520presence%2520of%2520numerous%2520Markush%2520structures%2520in%250Areal-world%2520documents%252C%2520along%2520with%2520variations%2520in%2520molecular%2520image%2520quality%252C%2520drawing%250Astyles%252C%2520and%2520noise%252C%2520significantly%2520limits%2520the%2520performance%2520of%2520existing%2520optical%250Achemical%2520structure%2520recognition%2520%2528OCSR%2529%2520methods.%2520We%2520present%2520MolParser%252C%2520a%2520novel%250Aend-to-end%2520OCSR%2520method%2520that%2520efficiently%2520and%2520accurately%2520recognizes%2520chemical%250Astructures%2520from%2520real-world%2520documents%252C%2520including%2520difficult%2520Markush%2520structure.%2520We%250Ause%2520a%2520extended%2520SMILES%2520encoding%2520rule%2520to%2520annotate%2520our%2520training%2520dataset.%2520Under%250Athis%2520rule%252C%2520we%2520build%2520MolParser-7M%252C%2520the%2520largest%2520annotated%2520molecular%2520image%2520dataset%250Ato%2520our%2520knowledge.%2520While%2520utilizing%2520a%2520large%2520amount%2520of%2520synthetic%2520data%252C%2520we%2520employed%250Aactive%2520learning%2520methods%2520to%2520incorporate%2520substantial%2520in-the-wild%2520data%252C%250Aspecifically%2520samples%2520cropped%2520from%2520real%2520patents%2520and%2520scientific%2520literature%252C%2520into%250Athe%2520training%2520process.%2520We%2520trained%2520an%2520end-to-end%2520molecular%2520image%2520captioning%250Amodel%252C%2520MolParser%252C%2520using%2520a%2520curriculum%2520learning%2520approach.%2520MolParser%2520significantly%250Aoutperforms%2520classical%2520and%2520learning-based%2520methods%2520across%2520most%2520scenarios%252C%2520with%250Apotential%2520for%2520broader%2520downstream%2520applications.%2520The%2520dataset%2520is%2520publicly%250Aavailable%2520in%2520huggingface.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11098v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MolParser%3A%20End-to-end%20Visual%20Recognition%20of%20Molecule%20Structures%20in%20the%0A%20%20Wild&entry.906535625=Xi%20Fang%20and%20Jiankun%20Wang%20and%20Xiaochen%20Cai%20and%20Shangqian%20Chen%20and%20Shuwen%20Yang%20and%20Haoyi%20Tao%20and%20Nan%20Wang%20and%20Lin%20Yao%20and%20Linfeng%20Zhang%20and%20Guolin%20Ke&entry.1292438233=%20%20In%20recent%20decades%2C%20chemistry%20publications%20and%20patents%20have%20increased%20rapidly.%0AA%20significant%20portion%20of%20key%20information%20is%20embedded%20in%20molecular%20structure%0Afigures%2C%20complicating%20large-scale%20literature%20searches%20and%20limiting%20the%0Aapplication%20of%20large%20language%20models%20in%20fields%20such%20as%20biology%2C%20chemistry%2C%20and%0Apharmaceuticals.%20The%20automatic%20extraction%20of%20precise%20chemical%20structures%20is%20of%0Acritical%20importance.%20However%2C%20the%20presence%20of%20numerous%20Markush%20structures%20in%0Areal-world%20documents%2C%20along%20with%20variations%20in%20molecular%20image%20quality%2C%20drawing%0Astyles%2C%20and%20noise%2C%20significantly%20limits%20the%20performance%20of%20existing%20optical%0Achemical%20structure%20recognition%20%28OCSR%29%20methods.%20We%20present%20MolParser%2C%20a%20novel%0Aend-to-end%20OCSR%20method%20that%20efficiently%20and%20accurately%20recognizes%20chemical%0Astructures%20from%20real-world%20documents%2C%20including%20difficult%20Markush%20structure.%20We%0Ause%20a%20extended%20SMILES%20encoding%20rule%20to%20annotate%20our%20training%20dataset.%20Under%0Athis%20rule%2C%20we%20build%20MolParser-7M%2C%20the%20largest%20annotated%20molecular%20image%20dataset%0Ato%20our%20knowledge.%20While%20utilizing%20a%20large%20amount%20of%20synthetic%20data%2C%20we%20employed%0Aactive%20learning%20methods%20to%20incorporate%20substantial%20in-the-wild%20data%2C%0Aspecifically%20samples%20cropped%20from%20real%20patents%20and%20scientific%20literature%2C%20into%0Athe%20training%20process.%20We%20trained%20an%20end-to-end%20molecular%20image%20captioning%0Amodel%2C%20MolParser%2C%20using%20a%20curriculum%20learning%20approach.%20MolParser%20significantly%0Aoutperforms%20classical%20and%20learning-based%20methods%20across%20most%20scenarios%2C%20with%0Apotential%20for%20broader%20downstream%20applications.%20The%20dataset%20is%20publicly%0Aavailable%20in%20huggingface.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11098v3&entry.124074799=Read"},
{"title": "SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D\n  Gaussian Splatting", "author": "Di Li and Jie Feng and Jiahao Chen and Weisheng Dong and Guanbin Li and Yuhui Zheng and Mingtao Feng and Guangming Shi", "abstract": "  3D affordance reasoning, the task of associating human instructions with the\nfunctional regions of 3D objects, is a critical capability for embodied agents.\nCurrent methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited\nto single-object, single-step interactions, a paradigm that falls short of\naddressing the long-horizon, multi-object tasks required for complex real-world\napplications. To bridge this gap, we introduce the novel task of Sequential 3D\nGaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale\nbenchmark featuring 1800+ scenes to support research on long-horizon affordance\nunderstanding in complex 3DGS environments. We then propose SeqSplatNet, an\nend-to-end framework that directly maps an instruction to a sequence of 3D\naffordance masks. SeqSplatNet employs a large language model that\nautoregressively generates text interleaved with special segmentation tokens,\nguiding a conditional decoder to produce the corresponding 3D mask. To handle\ncomplex scene geometry, we introduce a pre-training strategy, Conditional\nGeometric Reconstruction, where the model learns to reconstruct complete\naffordance region masks from known geometric observations, thereby building a\nrobust geometric prior. Furthermore, to resolve semantic ambiguities, we design\na feature injection mechanism that lifts rich semantic features from 2D Vision\nFoundation Models (VFM) and fuses them into the 3D decoder at multiple scales.\nExtensive experiments demonstrate that our method sets a new state-of-the-art\non our challenging benchmark, effectively advancing affordance reasoning from\nsingle-step interactions to complex, sequential tasks at the scene level.\n", "link": "http://arxiv.org/abs/2507.23772v1", "date": "2025-07-31", "relevancy": 2.5115, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6403}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6313}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeqAffordSplat%3A%20Scene-level%20Sequential%20Affordance%20Reasoning%20on%203D%0A%20%20Gaussian%20Splatting&body=Title%3A%20SeqAffordSplat%3A%20Scene-level%20Sequential%20Affordance%20Reasoning%20on%203D%0A%20%20Gaussian%20Splatting%0AAuthor%3A%20Di%20Li%20and%20Jie%20Feng%20and%20Jiahao%20Chen%20and%20Weisheng%20Dong%20and%20Guanbin%20Li%20and%20Yuhui%20Zheng%20and%20Mingtao%20Feng%20and%20Guangming%20Shi%0AAbstract%3A%20%20%203D%20affordance%20reasoning%2C%20the%20task%20of%20associating%20human%20instructions%20with%20the%0Afunctional%20regions%20of%203D%20objects%2C%20is%20a%20critical%20capability%20for%20embodied%20agents.%0ACurrent%20methods%20based%20on%203D%20Gaussian%20Splatting%20%283DGS%29%20are%20fundamentally%20limited%0Ato%20single-object%2C%20single-step%20interactions%2C%20a%20paradigm%20that%20falls%20short%20of%0Aaddressing%20the%20long-horizon%2C%20multi-object%20tasks%20required%20for%20complex%20real-world%0Aapplications.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20novel%20task%20of%20Sequential%203D%0AGaussian%20Affordance%20Reasoning%20and%20establish%20SeqAffordSplat%2C%20a%20large-scale%0Abenchmark%20featuring%201800%2B%20scenes%20to%20support%20research%20on%20long-horizon%20affordance%0Aunderstanding%20in%20complex%203DGS%20environments.%20We%20then%20propose%20SeqSplatNet%2C%20an%0Aend-to-end%20framework%20that%20directly%20maps%20an%20instruction%20to%20a%20sequence%20of%203D%0Aaffordance%20masks.%20SeqSplatNet%20employs%20a%20large%20language%20model%20that%0Aautoregressively%20generates%20text%20interleaved%20with%20special%20segmentation%20tokens%2C%0Aguiding%20a%20conditional%20decoder%20to%20produce%20the%20corresponding%203D%20mask.%20To%20handle%0Acomplex%20scene%20geometry%2C%20we%20introduce%20a%20pre-training%20strategy%2C%20Conditional%0AGeometric%20Reconstruction%2C%20where%20the%20model%20learns%20to%20reconstruct%20complete%0Aaffordance%20region%20masks%20from%20known%20geometric%20observations%2C%20thereby%20building%20a%0Arobust%20geometric%20prior.%20Furthermore%2C%20to%20resolve%20semantic%20ambiguities%2C%20we%20design%0Aa%20feature%20injection%20mechanism%20that%20lifts%20rich%20semantic%20features%20from%202D%20Vision%0AFoundation%20Models%20%28VFM%29%20and%20fuses%20them%20into%20the%203D%20decoder%20at%20multiple%20scales.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20sets%20a%20new%20state-of-the-art%0Aon%20our%20challenging%20benchmark%2C%20effectively%20advancing%20affordance%20reasoning%20from%0Asingle-step%20interactions%20to%20complex%2C%20sequential%20tasks%20at%20the%20scene%20level.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeqAffordSplat%253A%2520Scene-level%2520Sequential%2520Affordance%2520Reasoning%2520on%25203D%250A%2520%2520Gaussian%2520Splatting%26entry.906535625%3DDi%2520Li%2520and%2520Jie%2520Feng%2520and%2520Jiahao%2520Chen%2520and%2520Weisheng%2520Dong%2520and%2520Guanbin%2520Li%2520and%2520Yuhui%2520Zheng%2520and%2520Mingtao%2520Feng%2520and%2520Guangming%2520Shi%26entry.1292438233%3D%2520%25203D%2520affordance%2520reasoning%252C%2520the%2520task%2520of%2520associating%2520human%2520instructions%2520with%2520the%250Afunctional%2520regions%2520of%25203D%2520objects%252C%2520is%2520a%2520critical%2520capability%2520for%2520embodied%2520agents.%250ACurrent%2520methods%2520based%2520on%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520are%2520fundamentally%2520limited%250Ato%2520single-object%252C%2520single-step%2520interactions%252C%2520a%2520paradigm%2520that%2520falls%2520short%2520of%250Aaddressing%2520the%2520long-horizon%252C%2520multi-object%2520tasks%2520required%2520for%2520complex%2520real-world%250Aapplications.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520the%2520novel%2520task%2520of%2520Sequential%25203D%250AGaussian%2520Affordance%2520Reasoning%2520and%2520establish%2520SeqAffordSplat%252C%2520a%2520large-scale%250Abenchmark%2520featuring%25201800%252B%2520scenes%2520to%2520support%2520research%2520on%2520long-horizon%2520affordance%250Aunderstanding%2520in%2520complex%25203DGS%2520environments.%2520We%2520then%2520propose%2520SeqSplatNet%252C%2520an%250Aend-to-end%2520framework%2520that%2520directly%2520maps%2520an%2520instruction%2520to%2520a%2520sequence%2520of%25203D%250Aaffordance%2520masks.%2520SeqSplatNet%2520employs%2520a%2520large%2520language%2520model%2520that%250Aautoregressively%2520generates%2520text%2520interleaved%2520with%2520special%2520segmentation%2520tokens%252C%250Aguiding%2520a%2520conditional%2520decoder%2520to%2520produce%2520the%2520corresponding%25203D%2520mask.%2520To%2520handle%250Acomplex%2520scene%2520geometry%252C%2520we%2520introduce%2520a%2520pre-training%2520strategy%252C%2520Conditional%250AGeometric%2520Reconstruction%252C%2520where%2520the%2520model%2520learns%2520to%2520reconstruct%2520complete%250Aaffordance%2520region%2520masks%2520from%2520known%2520geometric%2520observations%252C%2520thereby%2520building%2520a%250Arobust%2520geometric%2520prior.%2520Furthermore%252C%2520to%2520resolve%2520semantic%2520ambiguities%252C%2520we%2520design%250Aa%2520feature%2520injection%2520mechanism%2520that%2520lifts%2520rich%2520semantic%2520features%2520from%25202D%2520Vision%250AFoundation%2520Models%2520%2528VFM%2529%2520and%2520fuses%2520them%2520into%2520the%25203D%2520decoder%2520at%2520multiple%2520scales.%250AExtensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520sets%2520a%2520new%2520state-of-the-art%250Aon%2520our%2520challenging%2520benchmark%252C%2520effectively%2520advancing%2520affordance%2520reasoning%2520from%250Asingle-step%2520interactions%2520to%2520complex%252C%2520sequential%2520tasks%2520at%2520the%2520scene%2520level.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeqAffordSplat%3A%20Scene-level%20Sequential%20Affordance%20Reasoning%20on%203D%0A%20%20Gaussian%20Splatting&entry.906535625=Di%20Li%20and%20Jie%20Feng%20and%20Jiahao%20Chen%20and%20Weisheng%20Dong%20and%20Guanbin%20Li%20and%20Yuhui%20Zheng%20and%20Mingtao%20Feng%20and%20Guangming%20Shi&entry.1292438233=%20%203D%20affordance%20reasoning%2C%20the%20task%20of%20associating%20human%20instructions%20with%20the%0Afunctional%20regions%20of%203D%20objects%2C%20is%20a%20critical%20capability%20for%20embodied%20agents.%0ACurrent%20methods%20based%20on%203D%20Gaussian%20Splatting%20%283DGS%29%20are%20fundamentally%20limited%0Ato%20single-object%2C%20single-step%20interactions%2C%20a%20paradigm%20that%20falls%20short%20of%0Aaddressing%20the%20long-horizon%2C%20multi-object%20tasks%20required%20for%20complex%20real-world%0Aapplications.%20To%20bridge%20this%20gap%2C%20we%20introduce%20the%20novel%20task%20of%20Sequential%203D%0AGaussian%20Affordance%20Reasoning%20and%20establish%20SeqAffordSplat%2C%20a%20large-scale%0Abenchmark%20featuring%201800%2B%20scenes%20to%20support%20research%20on%20long-horizon%20affordance%0Aunderstanding%20in%20complex%203DGS%20environments.%20We%20then%20propose%20SeqSplatNet%2C%20an%0Aend-to-end%20framework%20that%20directly%20maps%20an%20instruction%20to%20a%20sequence%20of%203D%0Aaffordance%20masks.%20SeqSplatNet%20employs%20a%20large%20language%20model%20that%0Aautoregressively%20generates%20text%20interleaved%20with%20special%20segmentation%20tokens%2C%0Aguiding%20a%20conditional%20decoder%20to%20produce%20the%20corresponding%203D%20mask.%20To%20handle%0Acomplex%20scene%20geometry%2C%20we%20introduce%20a%20pre-training%20strategy%2C%20Conditional%0AGeometric%20Reconstruction%2C%20where%20the%20model%20learns%20to%20reconstruct%20complete%0Aaffordance%20region%20masks%20from%20known%20geometric%20observations%2C%20thereby%20building%20a%0Arobust%20geometric%20prior.%20Furthermore%2C%20to%20resolve%20semantic%20ambiguities%2C%20we%20design%0Aa%20feature%20injection%20mechanism%20that%20lifts%20rich%20semantic%20features%20from%202D%20Vision%0AFoundation%20Models%20%28VFM%29%20and%20fuses%20them%20into%20the%203D%20decoder%20at%20multiple%20scales.%0AExtensive%20experiments%20demonstrate%20that%20our%20method%20sets%20a%20new%20state-of-the-art%0Aon%20our%20challenging%20benchmark%2C%20effectively%20advancing%20affordance%20reasoning%20from%0Asingle-step%20interactions%20to%20complex%2C%20sequential%20tasks%20at%20the%20scene%20level.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23772v1&entry.124074799=Read"},
{"title": "Causal Identification of Sufficient, Contrastive and Complete Feature\n  Sets in Image Classification", "author": "David A Kelly and Hana Chockler", "abstract": "  Existing algorithms for explaining the outputs of image classifiers are based\non a variety of approaches and produce explanations that lack formal rigor. On\nthe other hand, logic-based explanations are formally and rigorously defined\nbut their computability relies on strict assumptions about the model that do\nnot hold on image classifiers.\n  In this paper, we show that causal explanations, in addition to being\nformally and rigorously defined, enjoy the same formal properties as\nlogic-based ones, while still lending themselves to black-box algorithms and\nbeing a natural fit for image classifiers. We prove formal properties of causal\nexplanations and introduce contrastive causal explanations for image\nclassifiers. Moreover, we augment the definition of explanation with confidence\nawareness and introduce complete causal explanations: explanations that are\nclassified with exactly the same confidence as the original image.\n  We implement our definitions, and our experimental results demonstrate that\ndifferent models have different patterns of sufficiency, contrastiveness, and\ncompleteness. Our algorithms are efficiently computable, taking on average 6s\nper image on a ResNet50 model to compute all types of explanations, and are\ntotally black-box, needing no knowledge of the model, no access to model\ninternals, no access to gradient, nor requiring any properties, such as\nmonotonicity, of the model.\n", "link": "http://arxiv.org/abs/2507.23497v1", "date": "2025-07-31", "relevancy": 2.4913, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5234}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Identification%20of%20Sufficient%2C%20Contrastive%20and%20Complete%20Feature%0A%20%20Sets%20in%20Image%20Classification&body=Title%3A%20Causal%20Identification%20of%20Sufficient%2C%20Contrastive%20and%20Complete%20Feature%0A%20%20Sets%20in%20Image%20Classification%0AAuthor%3A%20David%20A%20Kelly%20and%20Hana%20Chockler%0AAbstract%3A%20%20%20Existing%20algorithms%20for%20explaining%20the%20outputs%20of%20image%20classifiers%20are%20based%0Aon%20a%20variety%20of%20approaches%20and%20produce%20explanations%20that%20lack%20formal%20rigor.%20On%0Athe%20other%20hand%2C%20logic-based%20explanations%20are%20formally%20and%20rigorously%20defined%0Abut%20their%20computability%20relies%20on%20strict%20assumptions%20about%20the%20model%20that%20do%0Anot%20hold%20on%20image%20classifiers.%0A%20%20In%20this%20paper%2C%20we%20show%20that%20causal%20explanations%2C%20in%20addition%20to%20being%0Aformally%20and%20rigorously%20defined%2C%20enjoy%20the%20same%20formal%20properties%20as%0Alogic-based%20ones%2C%20while%20still%20lending%20themselves%20to%20black-box%20algorithms%20and%0Abeing%20a%20natural%20fit%20for%20image%20classifiers.%20We%20prove%20formal%20properties%20of%20causal%0Aexplanations%20and%20introduce%20contrastive%20causal%20explanations%20for%20image%0Aclassifiers.%20Moreover%2C%20we%20augment%20the%20definition%20of%20explanation%20with%20confidence%0Aawareness%20and%20introduce%20complete%20causal%20explanations%3A%20explanations%20that%20are%0Aclassified%20with%20exactly%20the%20same%20confidence%20as%20the%20original%20image.%0A%20%20We%20implement%20our%20definitions%2C%20and%20our%20experimental%20results%20demonstrate%20that%0Adifferent%20models%20have%20different%20patterns%20of%20sufficiency%2C%20contrastiveness%2C%20and%0Acompleteness.%20Our%20algorithms%20are%20efficiently%20computable%2C%20taking%20on%20average%206s%0Aper%20image%20on%20a%20ResNet50%20model%20to%20compute%20all%20types%20of%20explanations%2C%20and%20are%0Atotally%20black-box%2C%20needing%20no%20knowledge%20of%20the%20model%2C%20no%20access%20to%20model%0Ainternals%2C%20no%20access%20to%20gradient%2C%20nor%20requiring%20any%20properties%2C%20such%20as%0Amonotonicity%2C%20of%20the%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Identification%2520of%2520Sufficient%252C%2520Contrastive%2520and%2520Complete%2520Feature%250A%2520%2520Sets%2520in%2520Image%2520Classification%26entry.906535625%3DDavid%2520A%2520Kelly%2520and%2520Hana%2520Chockler%26entry.1292438233%3D%2520%2520Existing%2520algorithms%2520for%2520explaining%2520the%2520outputs%2520of%2520image%2520classifiers%2520are%2520based%250Aon%2520a%2520variety%2520of%2520approaches%2520and%2520produce%2520explanations%2520that%2520lack%2520formal%2520rigor.%2520On%250Athe%2520other%2520hand%252C%2520logic-based%2520explanations%2520are%2520formally%2520and%2520rigorously%2520defined%250Abut%2520their%2520computability%2520relies%2520on%2520strict%2520assumptions%2520about%2520the%2520model%2520that%2520do%250Anot%2520hold%2520on%2520image%2520classifiers.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520causal%2520explanations%252C%2520in%2520addition%2520to%2520being%250Aformally%2520and%2520rigorously%2520defined%252C%2520enjoy%2520the%2520same%2520formal%2520properties%2520as%250Alogic-based%2520ones%252C%2520while%2520still%2520lending%2520themselves%2520to%2520black-box%2520algorithms%2520and%250Abeing%2520a%2520natural%2520fit%2520for%2520image%2520classifiers.%2520We%2520prove%2520formal%2520properties%2520of%2520causal%250Aexplanations%2520and%2520introduce%2520contrastive%2520causal%2520explanations%2520for%2520image%250Aclassifiers.%2520Moreover%252C%2520we%2520augment%2520the%2520definition%2520of%2520explanation%2520with%2520confidence%250Aawareness%2520and%2520introduce%2520complete%2520causal%2520explanations%253A%2520explanations%2520that%2520are%250Aclassified%2520with%2520exactly%2520the%2520same%2520confidence%2520as%2520the%2520original%2520image.%250A%2520%2520We%2520implement%2520our%2520definitions%252C%2520and%2520our%2520experimental%2520results%2520demonstrate%2520that%250Adifferent%2520models%2520have%2520different%2520patterns%2520of%2520sufficiency%252C%2520contrastiveness%252C%2520and%250Acompleteness.%2520Our%2520algorithms%2520are%2520efficiently%2520computable%252C%2520taking%2520on%2520average%25206s%250Aper%2520image%2520on%2520a%2520ResNet50%2520model%2520to%2520compute%2520all%2520types%2520of%2520explanations%252C%2520and%2520are%250Atotally%2520black-box%252C%2520needing%2520no%2520knowledge%2520of%2520the%2520model%252C%2520no%2520access%2520to%2520model%250Ainternals%252C%2520no%2520access%2520to%2520gradient%252C%2520nor%2520requiring%2520any%2520properties%252C%2520such%2520as%250Amonotonicity%252C%2520of%2520the%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Identification%20of%20Sufficient%2C%20Contrastive%20and%20Complete%20Feature%0A%20%20Sets%20in%20Image%20Classification&entry.906535625=David%20A%20Kelly%20and%20Hana%20Chockler&entry.1292438233=%20%20Existing%20algorithms%20for%20explaining%20the%20outputs%20of%20image%20classifiers%20are%20based%0Aon%20a%20variety%20of%20approaches%20and%20produce%20explanations%20that%20lack%20formal%20rigor.%20On%0Athe%20other%20hand%2C%20logic-based%20explanations%20are%20formally%20and%20rigorously%20defined%0Abut%20their%20computability%20relies%20on%20strict%20assumptions%20about%20the%20model%20that%20do%0Anot%20hold%20on%20image%20classifiers.%0A%20%20In%20this%20paper%2C%20we%20show%20that%20causal%20explanations%2C%20in%20addition%20to%20being%0Aformally%20and%20rigorously%20defined%2C%20enjoy%20the%20same%20formal%20properties%20as%0Alogic-based%20ones%2C%20while%20still%20lending%20themselves%20to%20black-box%20algorithms%20and%0Abeing%20a%20natural%20fit%20for%20image%20classifiers.%20We%20prove%20formal%20properties%20of%20causal%0Aexplanations%20and%20introduce%20contrastive%20causal%20explanations%20for%20image%0Aclassifiers.%20Moreover%2C%20we%20augment%20the%20definition%20of%20explanation%20with%20confidence%0Aawareness%20and%20introduce%20complete%20causal%20explanations%3A%20explanations%20that%20are%0Aclassified%20with%20exactly%20the%20same%20confidence%20as%20the%20original%20image.%0A%20%20We%20implement%20our%20definitions%2C%20and%20our%20experimental%20results%20demonstrate%20that%0Adifferent%20models%20have%20different%20patterns%20of%20sufficiency%2C%20contrastiveness%2C%20and%0Acompleteness.%20Our%20algorithms%20are%20efficiently%20computable%2C%20taking%20on%20average%206s%0Aper%20image%20on%20a%20ResNet50%20model%20to%20compute%20all%20types%20of%20explanations%2C%20and%20are%0Atotally%20black-box%2C%20needing%20no%20knowledge%20of%20the%20model%2C%20no%20access%20to%20model%0Ainternals%2C%20no%20access%20to%20gradient%2C%20nor%20requiring%20any%20properties%2C%20such%20as%0Amonotonicity%2C%20of%20the%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23497v1&entry.124074799=Read"},
{"title": "Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion", "author": "Timing Li and Bing Cao and Jiahe Feng and Haifang Cao and Qinghau Hu and Pengfei Zhu", "abstract": "  Image fusion synthesizes complementary information from multiple sources,\nmitigating the inherent limitations of unimodal imaging systems. Accurate image\nregistration is essential for effective multi-source data fusion. However,\nexisting registration methods, often based on image translation in Euclidean\nspace, fail to handle cross-modal misalignment effectively, resulting in\nsuboptimal alignment and fusion quality. To overcome this limitation, we\nexplore image alignment in non-Euclidean space and propose a Hyperbolic Cycle\nAlignment Network (Hy-CycleAlign). To the best of our knowledge, Hy-CycleAlign\nis the first image registration method based on hyperbolic space. It introduces\na dual-path cross-modal cyclic registration framework, in which a forward\nregistration network aligns cross-modal inputs, while a backward registration\nnetwork reconstructs the original image, forming a closed-loop registration\nstructure with geometric consistency. Additionally, we design a Hyperbolic\nHierarchy Contrastive Alignment (H$^{2}$CA) module, which maps images into\nhyperbolic space and imposes registration constraints, effectively reducing\ninterference caused by modality discrepancies. We further analyze image\nregistration in both Euclidean and hyperbolic spaces, demonstrating that\nhyperbolic space enables more sensitive and effective multi-modal image\nregistration. Extensive experiments on misaligned multi-modal images\ndemonstrate that our method significantly outperforms existing approaches in\nboth image alignment and fusion. Our code will be publicly available.\n", "link": "http://arxiv.org/abs/2507.23508v1", "date": "2025-07-31", "relevancy": 2.4796, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5014}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4944}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperbolic%20Cycle%20Alignment%20for%20Infrared-Visible%20Image%20Fusion&body=Title%3A%20Hyperbolic%20Cycle%20Alignment%20for%20Infrared-Visible%20Image%20Fusion%0AAuthor%3A%20Timing%20Li%20and%20Bing%20Cao%20and%20Jiahe%20Feng%20and%20Haifang%20Cao%20and%20Qinghau%20Hu%20and%20Pengfei%20Zhu%0AAbstract%3A%20%20%20Image%20fusion%20synthesizes%20complementary%20information%20from%20multiple%20sources%2C%0Amitigating%20the%20inherent%20limitations%20of%20unimodal%20imaging%20systems.%20Accurate%20image%0Aregistration%20is%20essential%20for%20effective%20multi-source%20data%20fusion.%20However%2C%0Aexisting%20registration%20methods%2C%20often%20based%20on%20image%20translation%20in%20Euclidean%0Aspace%2C%20fail%20to%20handle%20cross-modal%20misalignment%20effectively%2C%20resulting%20in%0Asuboptimal%20alignment%20and%20fusion%20quality.%20To%20overcome%20this%20limitation%2C%20we%0Aexplore%20image%20alignment%20in%20non-Euclidean%20space%20and%20propose%20a%20Hyperbolic%20Cycle%0AAlignment%20Network%20%28Hy-CycleAlign%29.%20To%20the%20best%20of%20our%20knowledge%2C%20Hy-CycleAlign%0Ais%20the%20first%20image%20registration%20method%20based%20on%20hyperbolic%20space.%20It%20introduces%0Aa%20dual-path%20cross-modal%20cyclic%20registration%20framework%2C%20in%20which%20a%20forward%0Aregistration%20network%20aligns%20cross-modal%20inputs%2C%20while%20a%20backward%20registration%0Anetwork%20reconstructs%20the%20original%20image%2C%20forming%20a%20closed-loop%20registration%0Astructure%20with%20geometric%20consistency.%20Additionally%2C%20we%20design%20a%20Hyperbolic%0AHierarchy%20Contrastive%20Alignment%20%28H%24%5E%7B2%7D%24CA%29%20module%2C%20which%20maps%20images%20into%0Ahyperbolic%20space%20and%20imposes%20registration%20constraints%2C%20effectively%20reducing%0Ainterference%20caused%20by%20modality%20discrepancies.%20We%20further%20analyze%20image%0Aregistration%20in%20both%20Euclidean%20and%20hyperbolic%20spaces%2C%20demonstrating%20that%0Ahyperbolic%20space%20enables%20more%20sensitive%20and%20effective%20multi-modal%20image%0Aregistration.%20Extensive%20experiments%20on%20misaligned%20multi-modal%20images%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20approaches%20in%0Aboth%20image%20alignment%20and%20fusion.%20Our%20code%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperbolic%2520Cycle%2520Alignment%2520for%2520Infrared-Visible%2520Image%2520Fusion%26entry.906535625%3DTiming%2520Li%2520and%2520Bing%2520Cao%2520and%2520Jiahe%2520Feng%2520and%2520Haifang%2520Cao%2520and%2520Qinghau%2520Hu%2520and%2520Pengfei%2520Zhu%26entry.1292438233%3D%2520%2520Image%2520fusion%2520synthesizes%2520complementary%2520information%2520from%2520multiple%2520sources%252C%250Amitigating%2520the%2520inherent%2520limitations%2520of%2520unimodal%2520imaging%2520systems.%2520Accurate%2520image%250Aregistration%2520is%2520essential%2520for%2520effective%2520multi-source%2520data%2520fusion.%2520However%252C%250Aexisting%2520registration%2520methods%252C%2520often%2520based%2520on%2520image%2520translation%2520in%2520Euclidean%250Aspace%252C%2520fail%2520to%2520handle%2520cross-modal%2520misalignment%2520effectively%252C%2520resulting%2520in%250Asuboptimal%2520alignment%2520and%2520fusion%2520quality.%2520To%2520overcome%2520this%2520limitation%252C%2520we%250Aexplore%2520image%2520alignment%2520in%2520non-Euclidean%2520space%2520and%2520propose%2520a%2520Hyperbolic%2520Cycle%250AAlignment%2520Network%2520%2528Hy-CycleAlign%2529.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520Hy-CycleAlign%250Ais%2520the%2520first%2520image%2520registration%2520method%2520based%2520on%2520hyperbolic%2520space.%2520It%2520introduces%250Aa%2520dual-path%2520cross-modal%2520cyclic%2520registration%2520framework%252C%2520in%2520which%2520a%2520forward%250Aregistration%2520network%2520aligns%2520cross-modal%2520inputs%252C%2520while%2520a%2520backward%2520registration%250Anetwork%2520reconstructs%2520the%2520original%2520image%252C%2520forming%2520a%2520closed-loop%2520registration%250Astructure%2520with%2520geometric%2520consistency.%2520Additionally%252C%2520we%2520design%2520a%2520Hyperbolic%250AHierarchy%2520Contrastive%2520Alignment%2520%2528H%2524%255E%257B2%257D%2524CA%2529%2520module%252C%2520which%2520maps%2520images%2520into%250Ahyperbolic%2520space%2520and%2520imposes%2520registration%2520constraints%252C%2520effectively%2520reducing%250Ainterference%2520caused%2520by%2520modality%2520discrepancies.%2520We%2520further%2520analyze%2520image%250Aregistration%2520in%2520both%2520Euclidean%2520and%2520hyperbolic%2520spaces%252C%2520demonstrating%2520that%250Ahyperbolic%2520space%2520enables%2520more%2520sensitive%2520and%2520effective%2520multi-modal%2520image%250Aregistration.%2520Extensive%2520experiments%2520on%2520misaligned%2520multi-modal%2520images%250Ademonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520existing%2520approaches%2520in%250Aboth%2520image%2520alignment%2520and%2520fusion.%2520Our%2520code%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperbolic%20Cycle%20Alignment%20for%20Infrared-Visible%20Image%20Fusion&entry.906535625=Timing%20Li%20and%20Bing%20Cao%20and%20Jiahe%20Feng%20and%20Haifang%20Cao%20and%20Qinghau%20Hu%20and%20Pengfei%20Zhu&entry.1292438233=%20%20Image%20fusion%20synthesizes%20complementary%20information%20from%20multiple%20sources%2C%0Amitigating%20the%20inherent%20limitations%20of%20unimodal%20imaging%20systems.%20Accurate%20image%0Aregistration%20is%20essential%20for%20effective%20multi-source%20data%20fusion.%20However%2C%0Aexisting%20registration%20methods%2C%20often%20based%20on%20image%20translation%20in%20Euclidean%0Aspace%2C%20fail%20to%20handle%20cross-modal%20misalignment%20effectively%2C%20resulting%20in%0Asuboptimal%20alignment%20and%20fusion%20quality.%20To%20overcome%20this%20limitation%2C%20we%0Aexplore%20image%20alignment%20in%20non-Euclidean%20space%20and%20propose%20a%20Hyperbolic%20Cycle%0AAlignment%20Network%20%28Hy-CycleAlign%29.%20To%20the%20best%20of%20our%20knowledge%2C%20Hy-CycleAlign%0Ais%20the%20first%20image%20registration%20method%20based%20on%20hyperbolic%20space.%20It%20introduces%0Aa%20dual-path%20cross-modal%20cyclic%20registration%20framework%2C%20in%20which%20a%20forward%0Aregistration%20network%20aligns%20cross-modal%20inputs%2C%20while%20a%20backward%20registration%0Anetwork%20reconstructs%20the%20original%20image%2C%20forming%20a%20closed-loop%20registration%0Astructure%20with%20geometric%20consistency.%20Additionally%2C%20we%20design%20a%20Hyperbolic%0AHierarchy%20Contrastive%20Alignment%20%28H%24%5E%7B2%7D%24CA%29%20module%2C%20which%20maps%20images%20into%0Ahyperbolic%20space%20and%20imposes%20registration%20constraints%2C%20effectively%20reducing%0Ainterference%20caused%20by%20modality%20discrepancies.%20We%20further%20analyze%20image%0Aregistration%20in%20both%20Euclidean%20and%20hyperbolic%20spaces%2C%20demonstrating%20that%0Ahyperbolic%20space%20enables%20more%20sensitive%20and%20effective%20multi-modal%20image%0Aregistration.%20Extensive%20experiments%20on%20misaligned%20multi-modal%20images%0Ademonstrate%20that%20our%20method%20significantly%20outperforms%20existing%20approaches%20in%0Aboth%20image%20alignment%20and%20fusion.%20Our%20code%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23508v1&entry.124074799=Read"},
{"title": "Anomalous Samples for Few-Shot Anomaly Detection", "author": "Aymane Abdali and Bartosz Boguslawski and Lucas Drumetz and Vincent Gripon", "abstract": "  Several anomaly detection and classification methods rely on large amounts of\nnon-anomalous or \"normal\" samples under the assump- tion that anomalous data is\ntypically harder to acquire. This hypothesis becomes questionable in Few-Shot\nsettings, where as little as one anno- tated sample can make a significant\ndifference. In this paper, we tackle the question of utilizing anomalous\nsamples in training a model for bi- nary anomaly classification. We propose a\nmethodology that incorporates anomalous samples in a multi-score anomaly\ndetection score leveraging recent Zero-Shot and memory-based techniques. We\ncompare the utility of anomalous samples to that of regular samples and study\nthe benefits and limitations of each. In addition, we propose an\naugmentation-based validation technique to optimize the aggregation of the\ndifferent anomaly scores and demonstrate its effectiveness on popular\nindustrial anomaly detection datasets.\n", "link": "http://arxiv.org/abs/2507.23712v1", "date": "2025-07-31", "relevancy": 2.4649, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5619}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.474}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomalous%20Samples%20for%20Few-Shot%20Anomaly%20Detection&body=Title%3A%20Anomalous%20Samples%20for%20Few-Shot%20Anomaly%20Detection%0AAuthor%3A%20Aymane%20Abdali%20and%20Bartosz%20Boguslawski%20and%20Lucas%20Drumetz%20and%20Vincent%20Gripon%0AAbstract%3A%20%20%20Several%20anomaly%20detection%20and%20classification%20methods%20rely%20on%20large%20amounts%20of%0Anon-anomalous%20or%20%22normal%22%20samples%20under%20the%20assump-%20tion%20that%20anomalous%20data%20is%0Atypically%20harder%20to%20acquire.%20This%20hypothesis%20becomes%20questionable%20in%20Few-Shot%0Asettings%2C%20where%20as%20little%20as%20one%20anno-%20tated%20sample%20can%20make%20a%20significant%0Adifference.%20In%20this%20paper%2C%20we%20tackle%20the%20question%20of%20utilizing%20anomalous%0Asamples%20in%20training%20a%20model%20for%20bi-%20nary%20anomaly%20classification.%20We%20propose%20a%0Amethodology%20that%20incorporates%20anomalous%20samples%20in%20a%20multi-score%20anomaly%0Adetection%20score%20leveraging%20recent%20Zero-Shot%20and%20memory-based%20techniques.%20We%0Acompare%20the%20utility%20of%20anomalous%20samples%20to%20that%20of%20regular%20samples%20and%20study%0Athe%20benefits%20and%20limitations%20of%20each.%20In%20addition%2C%20we%20propose%20an%0Aaugmentation-based%20validation%20technique%20to%20optimize%20the%20aggregation%20of%20the%0Adifferent%20anomaly%20scores%20and%20demonstrate%20its%20effectiveness%20on%20popular%0Aindustrial%20anomaly%20detection%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalous%2520Samples%2520for%2520Few-Shot%2520Anomaly%2520Detection%26entry.906535625%3DAymane%2520Abdali%2520and%2520Bartosz%2520Boguslawski%2520and%2520Lucas%2520Drumetz%2520and%2520Vincent%2520Gripon%26entry.1292438233%3D%2520%2520Several%2520anomaly%2520detection%2520and%2520classification%2520methods%2520rely%2520on%2520large%2520amounts%2520of%250Anon-anomalous%2520or%2520%2522normal%2522%2520samples%2520under%2520the%2520assump-%2520tion%2520that%2520anomalous%2520data%2520is%250Atypically%2520harder%2520to%2520acquire.%2520This%2520hypothesis%2520becomes%2520questionable%2520in%2520Few-Shot%250Asettings%252C%2520where%2520as%2520little%2520as%2520one%2520anno-%2520tated%2520sample%2520can%2520make%2520a%2520significant%250Adifference.%2520In%2520this%2520paper%252C%2520we%2520tackle%2520the%2520question%2520of%2520utilizing%2520anomalous%250Asamples%2520in%2520training%2520a%2520model%2520for%2520bi-%2520nary%2520anomaly%2520classification.%2520We%2520propose%2520a%250Amethodology%2520that%2520incorporates%2520anomalous%2520samples%2520in%2520a%2520multi-score%2520anomaly%250Adetection%2520score%2520leveraging%2520recent%2520Zero-Shot%2520and%2520memory-based%2520techniques.%2520We%250Acompare%2520the%2520utility%2520of%2520anomalous%2520samples%2520to%2520that%2520of%2520regular%2520samples%2520and%2520study%250Athe%2520benefits%2520and%2520limitations%2520of%2520each.%2520In%2520addition%252C%2520we%2520propose%2520an%250Aaugmentation-based%2520validation%2520technique%2520to%2520optimize%2520the%2520aggregation%2520of%2520the%250Adifferent%2520anomaly%2520scores%2520and%2520demonstrate%2520its%2520effectiveness%2520on%2520popular%250Aindustrial%2520anomaly%2520detection%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomalous%20Samples%20for%20Few-Shot%20Anomaly%20Detection&entry.906535625=Aymane%20Abdali%20and%20Bartosz%20Boguslawski%20and%20Lucas%20Drumetz%20and%20Vincent%20Gripon&entry.1292438233=%20%20Several%20anomaly%20detection%20and%20classification%20methods%20rely%20on%20large%20amounts%20of%0Anon-anomalous%20or%20%22normal%22%20samples%20under%20the%20assump-%20tion%20that%20anomalous%20data%20is%0Atypically%20harder%20to%20acquire.%20This%20hypothesis%20becomes%20questionable%20in%20Few-Shot%0Asettings%2C%20where%20as%20little%20as%20one%20anno-%20tated%20sample%20can%20make%20a%20significant%0Adifference.%20In%20this%20paper%2C%20we%20tackle%20the%20question%20of%20utilizing%20anomalous%0Asamples%20in%20training%20a%20model%20for%20bi-%20nary%20anomaly%20classification.%20We%20propose%20a%0Amethodology%20that%20incorporates%20anomalous%20samples%20in%20a%20multi-score%20anomaly%0Adetection%20score%20leveraging%20recent%20Zero-Shot%20and%20memory-based%20techniques.%20We%0Acompare%20the%20utility%20of%20anomalous%20samples%20to%20that%20of%20regular%20samples%20and%20study%0Athe%20benefits%20and%20limitations%20of%20each.%20In%20addition%2C%20we%20propose%20an%0Aaugmentation-based%20validation%20technique%20to%20optimize%20the%20aggregation%20of%20the%0Adifferent%20anomaly%20scores%20and%20demonstrate%20its%20effectiveness%20on%20popular%0Aindustrial%20anomaly%20detection%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23712v1&entry.124074799=Read"},
{"title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic\n  Expansive Scenarios", "author": "Haoxuan Jiang and Peicong Qian and Yusen Xie and Xiaocong Li and Ming Liu and Jun Ma", "abstract": "  LiDAR-based localization serves as a critical component in autonomous\nsystems, yet existing approaches face persistent challenges in balancing\nrepeatability, accuracy, and environmental adaptability. Traditional point\ncloud registration methods relying solely on offline maps often exhibit limited\nrobustness against long-term environmental changes, leading to localization\ndrift and reliability degradation in dynamic real-world scenarios. To address\nthese challenges, this paper proposes DuLoc, a robust and accurate localization\nmethod that tightly couples LiDAR-inertial odometry with offline map-based\nlocalization, incorporating a constant-velocity motion model to mitigate\noutlier noise in real-world scenarios. Specifically, we develop a LiDAR-based\nlocalization framework that seamlessly integrates a prior global map with\ndynamic real-time local maps, enabling robust localization in unbounded and\nchanging environments. Extensive real-world experiments in ultra unbounded port\nthat involve 2,856 hours of operational data across 32 Intelligent Guided\nVehicles (IGVs) are conducted and reported in this study. The results attained\ndemonstrate that our system outperforms other state-of-the-art LiDAR\nlocalization systems in large-scale changing outdoor environments.\n", "link": "http://arxiv.org/abs/2507.23660v1", "date": "2025-07-31", "relevancy": 2.4579, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6504}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6079}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DuLoc%3A%20Life-Long%20Dual-Layer%20Localization%20in%20Changing%20and%20Dynamic%0A%20%20Expansive%20Scenarios&body=Title%3A%20DuLoc%3A%20Life-Long%20Dual-Layer%20Localization%20in%20Changing%20and%20Dynamic%0A%20%20Expansive%20Scenarios%0AAuthor%3A%20Haoxuan%20Jiang%20and%20Peicong%20Qian%20and%20Yusen%20Xie%20and%20Xiaocong%20Li%20and%20Ming%20Liu%20and%20Jun%20Ma%0AAbstract%3A%20%20%20LiDAR-based%20localization%20serves%20as%20a%20critical%20component%20in%20autonomous%0Asystems%2C%20yet%20existing%20approaches%20face%20persistent%20challenges%20in%20balancing%0Arepeatability%2C%20accuracy%2C%20and%20environmental%20adaptability.%20Traditional%20point%0Acloud%20registration%20methods%20relying%20solely%20on%20offline%20maps%20often%20exhibit%20limited%0Arobustness%20against%20long-term%20environmental%20changes%2C%20leading%20to%20localization%0Adrift%20and%20reliability%20degradation%20in%20dynamic%20real-world%20scenarios.%20To%20address%0Athese%20challenges%2C%20this%20paper%20proposes%20DuLoc%2C%20a%20robust%20and%20accurate%20localization%0Amethod%20that%20tightly%20couples%20LiDAR-inertial%20odometry%20with%20offline%20map-based%0Alocalization%2C%20incorporating%20a%20constant-velocity%20motion%20model%20to%20mitigate%0Aoutlier%20noise%20in%20real-world%20scenarios.%20Specifically%2C%20we%20develop%20a%20LiDAR-based%0Alocalization%20framework%20that%20seamlessly%20integrates%20a%20prior%20global%20map%20with%0Adynamic%20real-time%20local%20maps%2C%20enabling%20robust%20localization%20in%20unbounded%20and%0Achanging%20environments.%20Extensive%20real-world%20experiments%20in%20ultra%20unbounded%20port%0Athat%20involve%202%2C856%20hours%20of%20operational%20data%20across%2032%20Intelligent%20Guided%0AVehicles%20%28IGVs%29%20are%20conducted%20and%20reported%20in%20this%20study.%20The%20results%20attained%0Ademonstrate%20that%20our%20system%20outperforms%20other%20state-of-the-art%20LiDAR%0Alocalization%20systems%20in%20large-scale%20changing%20outdoor%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuLoc%253A%2520Life-Long%2520Dual-Layer%2520Localization%2520in%2520Changing%2520and%2520Dynamic%250A%2520%2520Expansive%2520Scenarios%26entry.906535625%3DHaoxuan%2520Jiang%2520and%2520Peicong%2520Qian%2520and%2520Yusen%2520Xie%2520and%2520Xiaocong%2520Li%2520and%2520Ming%2520Liu%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520LiDAR-based%2520localization%2520serves%2520as%2520a%2520critical%2520component%2520in%2520autonomous%250Asystems%252C%2520yet%2520existing%2520approaches%2520face%2520persistent%2520challenges%2520in%2520balancing%250Arepeatability%252C%2520accuracy%252C%2520and%2520environmental%2520adaptability.%2520Traditional%2520point%250Acloud%2520registration%2520methods%2520relying%2520solely%2520on%2520offline%2520maps%2520often%2520exhibit%2520limited%250Arobustness%2520against%2520long-term%2520environmental%2520changes%252C%2520leading%2520to%2520localization%250Adrift%2520and%2520reliability%2520degradation%2520in%2520dynamic%2520real-world%2520scenarios.%2520To%2520address%250Athese%2520challenges%252C%2520this%2520paper%2520proposes%2520DuLoc%252C%2520a%2520robust%2520and%2520accurate%2520localization%250Amethod%2520that%2520tightly%2520couples%2520LiDAR-inertial%2520odometry%2520with%2520offline%2520map-based%250Alocalization%252C%2520incorporating%2520a%2520constant-velocity%2520motion%2520model%2520to%2520mitigate%250Aoutlier%2520noise%2520in%2520real-world%2520scenarios.%2520Specifically%252C%2520we%2520develop%2520a%2520LiDAR-based%250Alocalization%2520framework%2520that%2520seamlessly%2520integrates%2520a%2520prior%2520global%2520map%2520with%250Adynamic%2520real-time%2520local%2520maps%252C%2520enabling%2520robust%2520localization%2520in%2520unbounded%2520and%250Achanging%2520environments.%2520Extensive%2520real-world%2520experiments%2520in%2520ultra%2520unbounded%2520port%250Athat%2520involve%25202%252C856%2520hours%2520of%2520operational%2520data%2520across%252032%2520Intelligent%2520Guided%250AVehicles%2520%2528IGVs%2529%2520are%2520conducted%2520and%2520reported%2520in%2520this%2520study.%2520The%2520results%2520attained%250Ademonstrate%2520that%2520our%2520system%2520outperforms%2520other%2520state-of-the-art%2520LiDAR%250Alocalization%2520systems%2520in%2520large-scale%2520changing%2520outdoor%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DuLoc%3A%20Life-Long%20Dual-Layer%20Localization%20in%20Changing%20and%20Dynamic%0A%20%20Expansive%20Scenarios&entry.906535625=Haoxuan%20Jiang%20and%20Peicong%20Qian%20and%20Yusen%20Xie%20and%20Xiaocong%20Li%20and%20Ming%20Liu%20and%20Jun%20Ma&entry.1292438233=%20%20LiDAR-based%20localization%20serves%20as%20a%20critical%20component%20in%20autonomous%0Asystems%2C%20yet%20existing%20approaches%20face%20persistent%20challenges%20in%20balancing%0Arepeatability%2C%20accuracy%2C%20and%20environmental%20adaptability.%20Traditional%20point%0Acloud%20registration%20methods%20relying%20solely%20on%20offline%20maps%20often%20exhibit%20limited%0Arobustness%20against%20long-term%20environmental%20changes%2C%20leading%20to%20localization%0Adrift%20and%20reliability%20degradation%20in%20dynamic%20real-world%20scenarios.%20To%20address%0Athese%20challenges%2C%20this%20paper%20proposes%20DuLoc%2C%20a%20robust%20and%20accurate%20localization%0Amethod%20that%20tightly%20couples%20LiDAR-inertial%20odometry%20with%20offline%20map-based%0Alocalization%2C%20incorporating%20a%20constant-velocity%20motion%20model%20to%20mitigate%0Aoutlier%20noise%20in%20real-world%20scenarios.%20Specifically%2C%20we%20develop%20a%20LiDAR-based%0Alocalization%20framework%20that%20seamlessly%20integrates%20a%20prior%20global%20map%20with%0Adynamic%20real-time%20local%20maps%2C%20enabling%20robust%20localization%20in%20unbounded%20and%0Achanging%20environments.%20Extensive%20real-world%20experiments%20in%20ultra%20unbounded%20port%0Athat%20involve%202%2C856%20hours%20of%20operational%20data%20across%2032%20Intelligent%20Guided%0AVehicles%20%28IGVs%29%20are%20conducted%20and%20reported%20in%20this%20study.%20The%20results%20attained%0Ademonstrate%20that%20our%20system%20outperforms%20other%20state-of-the-art%20LiDAR%0Alocalization%20systems%20in%20large-scale%20changing%20outdoor%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23660v1&entry.124074799=Read"},
{"title": "GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers", "author": "Shijie Ma and Yuying Ge and Teng Wang and Yuxin Guo and Yixiao Ge and Ying Shan", "abstract": "  The synergy between generative and discriminative models receives growing\nattention. While discriminative Contrastive Language-Image Pre-Training (CLIP)\nexcels in high-level semantics, it struggles with perceiving fine-grained\nvisual details. Generally, to enhance representations, generative models take\nCLIP's visual features as conditions for reconstruction. However, the\nunderlying principle remains underexplored. In this work, we empirically found\nthat visually perfect generations are not always optimal for representation\nenhancement. The essence lies in effectively extracting fine-grained knowledge\nfrom generative models while mitigating irrelevant information. To explore\ncritical factors, we delve into three aspects: (1) Conditioning mechanisms: We\nfound that even a small number of local tokens can drastically reduce the\ndifficulty of reconstruction, leading to collapsed training. We thus conclude\nthat utilizing only global visual tokens as conditions is the most effective\nstrategy. (2) Denoising configurations: We observed that end-to-end training\nintroduces extraneous information. To address this, we propose a two-stage\ntraining strategy to prioritize learning useful visual knowledge. Additionally,\nwe demonstrate that lightweight denoisers can yield remarkable improvements.\n(3) Generation paradigms: We explore both continuous and discrete denoisers\nwith desirable outcomes, validating the versatility of our method. Through our\nin-depth explorations, we have finally arrived at an effective method, namely\nGenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark,\ne.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into\nmultimodal large language models for better vision-centric performance. All the\nmodels and codes are made publicly available.\n", "link": "http://arxiv.org/abs/2503.19480v3", "date": "2025-07-31", "relevancy": 2.4573, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6211}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.612}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenHancer%3A%20Imperfect%20Generative%20Models%20are%20Secretly%20Strong%0A%20%20Vision-Centric%20Enhancers&body=Title%3A%20GenHancer%3A%20Imperfect%20Generative%20Models%20are%20Secretly%20Strong%0A%20%20Vision-Centric%20Enhancers%0AAuthor%3A%20Shijie%20Ma%20and%20Yuying%20Ge%20and%20Teng%20Wang%20and%20Yuxin%20Guo%20and%20Yixiao%20Ge%20and%20Ying%20Shan%0AAbstract%3A%20%20%20The%20synergy%20between%20generative%20and%20discriminative%20models%20receives%20growing%0Aattention.%20While%20discriminative%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%0Aexcels%20in%20high-level%20semantics%2C%20it%20struggles%20with%20perceiving%20fine-grained%0Avisual%20details.%20Generally%2C%20to%20enhance%20representations%2C%20generative%20models%20take%0ACLIP%27s%20visual%20features%20as%20conditions%20for%20reconstruction.%20However%2C%20the%0Aunderlying%20principle%20remains%20underexplored.%20In%20this%20work%2C%20we%20empirically%20found%0Athat%20visually%20perfect%20generations%20are%20not%20always%20optimal%20for%20representation%0Aenhancement.%20The%20essence%20lies%20in%20effectively%20extracting%20fine-grained%20knowledge%0Afrom%20generative%20models%20while%20mitigating%20irrelevant%20information.%20To%20explore%0Acritical%20factors%2C%20we%20delve%20into%20three%20aspects%3A%20%281%29%20Conditioning%20mechanisms%3A%20We%0Afound%20that%20even%20a%20small%20number%20of%20local%20tokens%20can%20drastically%20reduce%20the%0Adifficulty%20of%20reconstruction%2C%20leading%20to%20collapsed%20training.%20We%20thus%20conclude%0Athat%20utilizing%20only%20global%20visual%20tokens%20as%20conditions%20is%20the%20most%20effective%0Astrategy.%20%282%29%20Denoising%20configurations%3A%20We%20observed%20that%20end-to-end%20training%0Aintroduces%20extraneous%20information.%20To%20address%20this%2C%20we%20propose%20a%20two-stage%0Atraining%20strategy%20to%20prioritize%20learning%20useful%20visual%20knowledge.%20Additionally%2C%0Awe%20demonstrate%20that%20lightweight%20denoisers%20can%20yield%20remarkable%20improvements.%0A%283%29%20Generation%20paradigms%3A%20We%20explore%20both%20continuous%20and%20discrete%20denoisers%0Awith%20desirable%20outcomes%2C%20validating%20the%20versatility%20of%20our%20method.%20Through%20our%0Ain-depth%20explorations%2C%20we%20have%20finally%20arrived%20at%20an%20effective%20method%2C%20namely%0AGenHancer%2C%20which%20consistently%20outperforms%20prior%20arts%20on%20the%20MMVP-VLM%20benchmark%2C%0Ae.g.%2C%206.0%25%20on%20OpenAICLIP.%20The%20enhanced%20CLIP%20can%20be%20further%20plugged%20into%0Amultimodal%20large%20language%20models%20for%20better%20vision-centric%20performance.%20All%20the%0Amodels%20and%20codes%20are%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.19480v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenHancer%253A%2520Imperfect%2520Generative%2520Models%2520are%2520Secretly%2520Strong%250A%2520%2520Vision-Centric%2520Enhancers%26entry.906535625%3DShijie%2520Ma%2520and%2520Yuying%2520Ge%2520and%2520Teng%2520Wang%2520and%2520Yuxin%2520Guo%2520and%2520Yixiao%2520Ge%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520The%2520synergy%2520between%2520generative%2520and%2520discriminative%2520models%2520receives%2520growing%250Aattention.%2520While%2520discriminative%2520Contrastive%2520Language-Image%2520Pre-Training%2520%2528CLIP%2529%250Aexcels%2520in%2520high-level%2520semantics%252C%2520it%2520struggles%2520with%2520perceiving%2520fine-grained%250Avisual%2520details.%2520Generally%252C%2520to%2520enhance%2520representations%252C%2520generative%2520models%2520take%250ACLIP%2527s%2520visual%2520features%2520as%2520conditions%2520for%2520reconstruction.%2520However%252C%2520the%250Aunderlying%2520principle%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520empirically%2520found%250Athat%2520visually%2520perfect%2520generations%2520are%2520not%2520always%2520optimal%2520for%2520representation%250Aenhancement.%2520The%2520essence%2520lies%2520in%2520effectively%2520extracting%2520fine-grained%2520knowledge%250Afrom%2520generative%2520models%2520while%2520mitigating%2520irrelevant%2520information.%2520To%2520explore%250Acritical%2520factors%252C%2520we%2520delve%2520into%2520three%2520aspects%253A%2520%25281%2529%2520Conditioning%2520mechanisms%253A%2520We%250Afound%2520that%2520even%2520a%2520small%2520number%2520of%2520local%2520tokens%2520can%2520drastically%2520reduce%2520the%250Adifficulty%2520of%2520reconstruction%252C%2520leading%2520to%2520collapsed%2520training.%2520We%2520thus%2520conclude%250Athat%2520utilizing%2520only%2520global%2520visual%2520tokens%2520as%2520conditions%2520is%2520the%2520most%2520effective%250Astrategy.%2520%25282%2529%2520Denoising%2520configurations%253A%2520We%2520observed%2520that%2520end-to-end%2520training%250Aintroduces%2520extraneous%2520information.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520two-stage%250Atraining%2520strategy%2520to%2520prioritize%2520learning%2520useful%2520visual%2520knowledge.%2520Additionally%252C%250Awe%2520demonstrate%2520that%2520lightweight%2520denoisers%2520can%2520yield%2520remarkable%2520improvements.%250A%25283%2529%2520Generation%2520paradigms%253A%2520We%2520explore%2520both%2520continuous%2520and%2520discrete%2520denoisers%250Awith%2520desirable%2520outcomes%252C%2520validating%2520the%2520versatility%2520of%2520our%2520method.%2520Through%2520our%250Ain-depth%2520explorations%252C%2520we%2520have%2520finally%2520arrived%2520at%2520an%2520effective%2520method%252C%2520namely%250AGenHancer%252C%2520which%2520consistently%2520outperforms%2520prior%2520arts%2520on%2520the%2520MMVP-VLM%2520benchmark%252C%250Ae.g.%252C%25206.0%2525%2520on%2520OpenAICLIP.%2520The%2520enhanced%2520CLIP%2520can%2520be%2520further%2520plugged%2520into%250Amultimodal%2520large%2520language%2520models%2520for%2520better%2520vision-centric%2520performance.%2520All%2520the%250Amodels%2520and%2520codes%2520are%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19480v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenHancer%3A%20Imperfect%20Generative%20Models%20are%20Secretly%20Strong%0A%20%20Vision-Centric%20Enhancers&entry.906535625=Shijie%20Ma%20and%20Yuying%20Ge%20and%20Teng%20Wang%20and%20Yuxin%20Guo%20and%20Yixiao%20Ge%20and%20Ying%20Shan&entry.1292438233=%20%20The%20synergy%20between%20generative%20and%20discriminative%20models%20receives%20growing%0Aattention.%20While%20discriminative%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%0Aexcels%20in%20high-level%20semantics%2C%20it%20struggles%20with%20perceiving%20fine-grained%0Avisual%20details.%20Generally%2C%20to%20enhance%20representations%2C%20generative%20models%20take%0ACLIP%27s%20visual%20features%20as%20conditions%20for%20reconstruction.%20However%2C%20the%0Aunderlying%20principle%20remains%20underexplored.%20In%20this%20work%2C%20we%20empirically%20found%0Athat%20visually%20perfect%20generations%20are%20not%20always%20optimal%20for%20representation%0Aenhancement.%20The%20essence%20lies%20in%20effectively%20extracting%20fine-grained%20knowledge%0Afrom%20generative%20models%20while%20mitigating%20irrelevant%20information.%20To%20explore%0Acritical%20factors%2C%20we%20delve%20into%20three%20aspects%3A%20%281%29%20Conditioning%20mechanisms%3A%20We%0Afound%20that%20even%20a%20small%20number%20of%20local%20tokens%20can%20drastically%20reduce%20the%0Adifficulty%20of%20reconstruction%2C%20leading%20to%20collapsed%20training.%20We%20thus%20conclude%0Athat%20utilizing%20only%20global%20visual%20tokens%20as%20conditions%20is%20the%20most%20effective%0Astrategy.%20%282%29%20Denoising%20configurations%3A%20We%20observed%20that%20end-to-end%20training%0Aintroduces%20extraneous%20information.%20To%20address%20this%2C%20we%20propose%20a%20two-stage%0Atraining%20strategy%20to%20prioritize%20learning%20useful%20visual%20knowledge.%20Additionally%2C%0Awe%20demonstrate%20that%20lightweight%20denoisers%20can%20yield%20remarkable%20improvements.%0A%283%29%20Generation%20paradigms%3A%20We%20explore%20both%20continuous%20and%20discrete%20denoisers%0Awith%20desirable%20outcomes%2C%20validating%20the%20versatility%20of%20our%20method.%20Through%20our%0Ain-depth%20explorations%2C%20we%20have%20finally%20arrived%20at%20an%20effective%20method%2C%20namely%0AGenHancer%2C%20which%20consistently%20outperforms%20prior%20arts%20on%20the%20MMVP-VLM%20benchmark%2C%0Ae.g.%2C%206.0%25%20on%20OpenAICLIP.%20The%20enhanced%20CLIP%20can%20be%20further%20plugged%20into%0Amultimodal%20large%20language%20models%20for%20better%20vision-centric%20performance.%20All%20the%0Amodels%20and%20codes%20are%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.19480v3&entry.124074799=Read"},
{"title": "H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation", "author": "Hongzhe Bi and Lingxuan Wu and Tianwei Lin and Hengkai Tan and Zhizhong Su and Hang Su and Jun Zhu", "abstract": "  Imitation learning for robotic manipulation faces a fundamental challenge:\nthe scarcity of large-scale, high-quality robot demonstration data. Recent\nrobotic foundation models often pre-train on cross-embodiment robot datasets to\nincrease data scale, while they face significant limitations as the diverse\nmorphologies and action spaces across different robot embodiments make unified\ntraining challenging. In this paper, we present H-RDT (Human to Robotics\nDiffusion Transformer), a novel approach that leverages human manipulation data\nto enhance robot manipulation capabilities. Our key insight is that large-scale\negocentric human manipulation videos with paired 3D hand pose annotations\nprovide rich behavioral priors that capture natural manipulation strategies and\ncan benefit robotic policy learning. We introduce a two-stage training\nparadigm: (1) pre-training on large-scale egocentric human manipulation data,\nand (2) cross-embodiment fine-tuning on robot-specific data with modular action\nencoders and decoders. Built on a diffusion transformer architecture with 2B\nparameters, H-RDT uses flow matching to model complex action distributions.\nExtensive evaluations encompassing both simulation and real-world experiments,\nsingle-task and multitask scenarios, as well as few-shot learning and\nrobustness assessments, demonstrate that H-RDT outperforms training from\nscratch and existing state-of-the-art methods, including Pi0 and RDT, achieving\nsignificant improvements of 13.9% and 40.5% over training from scratch in\nsimulation and real-world experiments, respectively. The results validate our\ncore hypothesis that human manipulation data can serve as a powerful foundation\nfor learning bimanual robotic manipulation policies.\n", "link": "http://arxiv.org/abs/2507.23523v1", "date": "2025-07-31", "relevancy": 2.443, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6202}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6201}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20H-RDT%3A%20Human%20Manipulation%20Enhanced%20Bimanual%20Robotic%20Manipulation&body=Title%3A%20H-RDT%3A%20Human%20Manipulation%20Enhanced%20Bimanual%20Robotic%20Manipulation%0AAuthor%3A%20Hongzhe%20Bi%20and%20Lingxuan%20Wu%20and%20Tianwei%20Lin%20and%20Hengkai%20Tan%20and%20Zhizhong%20Su%20and%20Hang%20Su%20and%20Jun%20Zhu%0AAbstract%3A%20%20%20Imitation%20learning%20for%20robotic%20manipulation%20faces%20a%20fundamental%20challenge%3A%0Athe%20scarcity%20of%20large-scale%2C%20high-quality%20robot%20demonstration%20data.%20Recent%0Arobotic%20foundation%20models%20often%20pre-train%20on%20cross-embodiment%20robot%20datasets%20to%0Aincrease%20data%20scale%2C%20while%20they%20face%20significant%20limitations%20as%20the%20diverse%0Amorphologies%20and%20action%20spaces%20across%20different%20robot%20embodiments%20make%20unified%0Atraining%20challenging.%20In%20this%20paper%2C%20we%20present%20H-RDT%20%28Human%20to%20Robotics%0ADiffusion%20Transformer%29%2C%20a%20novel%20approach%20that%20leverages%20human%20manipulation%20data%0Ato%20enhance%20robot%20manipulation%20capabilities.%20Our%20key%20insight%20is%20that%20large-scale%0Aegocentric%20human%20manipulation%20videos%20with%20paired%203D%20hand%20pose%20annotations%0Aprovide%20rich%20behavioral%20priors%20that%20capture%20natural%20manipulation%20strategies%20and%0Acan%20benefit%20robotic%20policy%20learning.%20We%20introduce%20a%20two-stage%20training%0Aparadigm%3A%20%281%29%20pre-training%20on%20large-scale%20egocentric%20human%20manipulation%20data%2C%0Aand%20%282%29%20cross-embodiment%20fine-tuning%20on%20robot-specific%20data%20with%20modular%20action%0Aencoders%20and%20decoders.%20Built%20on%20a%20diffusion%20transformer%20architecture%20with%202B%0Aparameters%2C%20H-RDT%20uses%20flow%20matching%20to%20model%20complex%20action%20distributions.%0AExtensive%20evaluations%20encompassing%20both%20simulation%20and%20real-world%20experiments%2C%0Asingle-task%20and%20multitask%20scenarios%2C%20as%20well%20as%20few-shot%20learning%20and%0Arobustness%20assessments%2C%20demonstrate%20that%20H-RDT%20outperforms%20training%20from%0Ascratch%20and%20existing%20state-of-the-art%20methods%2C%20including%20Pi0%20and%20RDT%2C%20achieving%0Asignificant%20improvements%20of%2013.9%25%20and%2040.5%25%20over%20training%20from%20scratch%20in%0Asimulation%20and%20real-world%20experiments%2C%20respectively.%20The%20results%20validate%20our%0Acore%20hypothesis%20that%20human%20manipulation%20data%20can%20serve%20as%20a%20powerful%20foundation%0Afor%20learning%20bimanual%20robotic%20manipulation%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DH-RDT%253A%2520Human%2520Manipulation%2520Enhanced%2520Bimanual%2520Robotic%2520Manipulation%26entry.906535625%3DHongzhe%2520Bi%2520and%2520Lingxuan%2520Wu%2520and%2520Tianwei%2520Lin%2520and%2520Hengkai%2520Tan%2520and%2520Zhizhong%2520Su%2520and%2520Hang%2520Su%2520and%2520Jun%2520Zhu%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520for%2520robotic%2520manipulation%2520faces%2520a%2520fundamental%2520challenge%253A%250Athe%2520scarcity%2520of%2520large-scale%252C%2520high-quality%2520robot%2520demonstration%2520data.%2520Recent%250Arobotic%2520foundation%2520models%2520often%2520pre-train%2520on%2520cross-embodiment%2520robot%2520datasets%2520to%250Aincrease%2520data%2520scale%252C%2520while%2520they%2520face%2520significant%2520limitations%2520as%2520the%2520diverse%250Amorphologies%2520and%2520action%2520spaces%2520across%2520different%2520robot%2520embodiments%2520make%2520unified%250Atraining%2520challenging.%2520In%2520this%2520paper%252C%2520we%2520present%2520H-RDT%2520%2528Human%2520to%2520Robotics%250ADiffusion%2520Transformer%2529%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520human%2520manipulation%2520data%250Ato%2520enhance%2520robot%2520manipulation%2520capabilities.%2520Our%2520key%2520insight%2520is%2520that%2520large-scale%250Aegocentric%2520human%2520manipulation%2520videos%2520with%2520paired%25203D%2520hand%2520pose%2520annotations%250Aprovide%2520rich%2520behavioral%2520priors%2520that%2520capture%2520natural%2520manipulation%2520strategies%2520and%250Acan%2520benefit%2520robotic%2520policy%2520learning.%2520We%2520introduce%2520a%2520two-stage%2520training%250Aparadigm%253A%2520%25281%2529%2520pre-training%2520on%2520large-scale%2520egocentric%2520human%2520manipulation%2520data%252C%250Aand%2520%25282%2529%2520cross-embodiment%2520fine-tuning%2520on%2520robot-specific%2520data%2520with%2520modular%2520action%250Aencoders%2520and%2520decoders.%2520Built%2520on%2520a%2520diffusion%2520transformer%2520architecture%2520with%25202B%250Aparameters%252C%2520H-RDT%2520uses%2520flow%2520matching%2520to%2520model%2520complex%2520action%2520distributions.%250AExtensive%2520evaluations%2520encompassing%2520both%2520simulation%2520and%2520real-world%2520experiments%252C%250Asingle-task%2520and%2520multitask%2520scenarios%252C%2520as%2520well%2520as%2520few-shot%2520learning%2520and%250Arobustness%2520assessments%252C%2520demonstrate%2520that%2520H-RDT%2520outperforms%2520training%2520from%250Ascratch%2520and%2520existing%2520state-of-the-art%2520methods%252C%2520including%2520Pi0%2520and%2520RDT%252C%2520achieving%250Asignificant%2520improvements%2520of%252013.9%2525%2520and%252040.5%2525%2520over%2520training%2520from%2520scratch%2520in%250Asimulation%2520and%2520real-world%2520experiments%252C%2520respectively.%2520The%2520results%2520validate%2520our%250Acore%2520hypothesis%2520that%2520human%2520manipulation%2520data%2520can%2520serve%2520as%2520a%2520powerful%2520foundation%250Afor%2520learning%2520bimanual%2520robotic%2520manipulation%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=H-RDT%3A%20Human%20Manipulation%20Enhanced%20Bimanual%20Robotic%20Manipulation&entry.906535625=Hongzhe%20Bi%20and%20Lingxuan%20Wu%20and%20Tianwei%20Lin%20and%20Hengkai%20Tan%20and%20Zhizhong%20Su%20and%20Hang%20Su%20and%20Jun%20Zhu&entry.1292438233=%20%20Imitation%20learning%20for%20robotic%20manipulation%20faces%20a%20fundamental%20challenge%3A%0Athe%20scarcity%20of%20large-scale%2C%20high-quality%20robot%20demonstration%20data.%20Recent%0Arobotic%20foundation%20models%20often%20pre-train%20on%20cross-embodiment%20robot%20datasets%20to%0Aincrease%20data%20scale%2C%20while%20they%20face%20significant%20limitations%20as%20the%20diverse%0Amorphologies%20and%20action%20spaces%20across%20different%20robot%20embodiments%20make%20unified%0Atraining%20challenging.%20In%20this%20paper%2C%20we%20present%20H-RDT%20%28Human%20to%20Robotics%0ADiffusion%20Transformer%29%2C%20a%20novel%20approach%20that%20leverages%20human%20manipulation%20data%0Ato%20enhance%20robot%20manipulation%20capabilities.%20Our%20key%20insight%20is%20that%20large-scale%0Aegocentric%20human%20manipulation%20videos%20with%20paired%203D%20hand%20pose%20annotations%0Aprovide%20rich%20behavioral%20priors%20that%20capture%20natural%20manipulation%20strategies%20and%0Acan%20benefit%20robotic%20policy%20learning.%20We%20introduce%20a%20two-stage%20training%0Aparadigm%3A%20%281%29%20pre-training%20on%20large-scale%20egocentric%20human%20manipulation%20data%2C%0Aand%20%282%29%20cross-embodiment%20fine-tuning%20on%20robot-specific%20data%20with%20modular%20action%0Aencoders%20and%20decoders.%20Built%20on%20a%20diffusion%20transformer%20architecture%20with%202B%0Aparameters%2C%20H-RDT%20uses%20flow%20matching%20to%20model%20complex%20action%20distributions.%0AExtensive%20evaluations%20encompassing%20both%20simulation%20and%20real-world%20experiments%2C%0Asingle-task%20and%20multitask%20scenarios%2C%20as%20well%20as%20few-shot%20learning%20and%0Arobustness%20assessments%2C%20demonstrate%20that%20H-RDT%20outperforms%20training%20from%0Ascratch%20and%20existing%20state-of-the-art%20methods%2C%20including%20Pi0%20and%20RDT%2C%20achieving%0Asignificant%20improvements%20of%2013.9%25%20and%2040.5%25%20over%20training%20from%20scratch%20in%0Asimulation%20and%20real-world%20experiments%2C%20respectively.%20The%20results%20validate%20our%0Acore%20hypothesis%20that%20human%20manipulation%20data%20can%20serve%20as%20a%20powerful%20foundation%0Afor%20learning%20bimanual%20robotic%20manipulation%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23523v1&entry.124074799=Read"},
{"title": "ClaraVid: A Holistic Scene Reconstruction Benchmark From Aerial\n  Perspective With Delentropy-Based Complexity Profiling", "author": "Radu Beche and Sergiu Nedevschi", "abstract": "  The development of aerial holistic scene understanding algorithms is hindered\nby the scarcity of comprehensive datasets that enable both semantic and\ngeometric reconstruction. While synthetic datasets offer an alternative,\nexisting options exhibit task-specific limitations, unrealistic scene\ncompositions, and rendering artifacts that compromise real-world applicability.\nWe introduce ClaraVid, a synthetic aerial dataset specifically designed to\novercome these limitations. Comprising 16,917 high-resolution images captured\nat 4032x3024 from multiple viewpoints across diverse landscapes, ClaraVid\nprovides dense depth maps, panoptic segmentation, sparse point clouds, and\ndynamic object masks, while mitigating common rendering artifacts. To further\nadvance neural reconstruction, we introduce the Delentropic Scene Profile\n(DSP), a novel complexity metric derived from differential entropy analysis,\ndesigned to quantitatively assess scene difficulty and inform reconstruction\ntasks. Utilizing DSP, we systematically benchmark neural reconstruction\nmethods, uncovering a consistent, measurable correlation between scene\ncomplexity and reconstruction accuracy. Empirical results indicate that higher\ndelentropy strongly correlates with increased reconstruction errors, validating\nDSP as a reliable complexity prior. The data and code are available on the\nproject page at https://rdbch.github.io/claravid/\n", "link": "http://arxiv.org/abs/2503.17856v2", "date": "2025-07-31", "relevancy": 2.4177, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6061}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClaraVid%3A%20A%20Holistic%20Scene%20Reconstruction%20Benchmark%20From%20Aerial%0A%20%20Perspective%20With%20Delentropy-Based%20Complexity%20Profiling&body=Title%3A%20ClaraVid%3A%20A%20Holistic%20Scene%20Reconstruction%20Benchmark%20From%20Aerial%0A%20%20Perspective%20With%20Delentropy-Based%20Complexity%20Profiling%0AAuthor%3A%20Radu%20Beche%20and%20Sergiu%20Nedevschi%0AAbstract%3A%20%20%20The%20development%20of%20aerial%20holistic%20scene%20understanding%20algorithms%20is%20hindered%0Aby%20the%20scarcity%20of%20comprehensive%20datasets%20that%20enable%20both%20semantic%20and%0Ageometric%20reconstruction.%20While%20synthetic%20datasets%20offer%20an%20alternative%2C%0Aexisting%20options%20exhibit%20task-specific%20limitations%2C%20unrealistic%20scene%0Acompositions%2C%20and%20rendering%20artifacts%20that%20compromise%20real-world%20applicability.%0AWe%20introduce%20ClaraVid%2C%20a%20synthetic%20aerial%20dataset%20specifically%20designed%20to%0Aovercome%20these%20limitations.%20Comprising%2016%2C917%20high-resolution%20images%20captured%0Aat%204032x3024%20from%20multiple%20viewpoints%20across%20diverse%20landscapes%2C%20ClaraVid%0Aprovides%20dense%20depth%20maps%2C%20panoptic%20segmentation%2C%20sparse%20point%20clouds%2C%20and%0Adynamic%20object%20masks%2C%20while%20mitigating%20common%20rendering%20artifacts.%20To%20further%0Aadvance%20neural%20reconstruction%2C%20we%20introduce%20the%20Delentropic%20Scene%20Profile%0A%28DSP%29%2C%20a%20novel%20complexity%20metric%20derived%20from%20differential%20entropy%20analysis%2C%0Adesigned%20to%20quantitatively%20assess%20scene%20difficulty%20and%20inform%20reconstruction%0Atasks.%20Utilizing%20DSP%2C%20we%20systematically%20benchmark%20neural%20reconstruction%0Amethods%2C%20uncovering%20a%20consistent%2C%20measurable%20correlation%20between%20scene%0Acomplexity%20and%20reconstruction%20accuracy.%20Empirical%20results%20indicate%20that%20higher%0Adelentropy%20strongly%20correlates%20with%20increased%20reconstruction%20errors%2C%20validating%0ADSP%20as%20a%20reliable%20complexity%20prior.%20The%20data%20and%20code%20are%20available%20on%20the%0Aproject%20page%20at%20https%3A//rdbch.github.io/claravid/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClaraVid%253A%2520A%2520Holistic%2520Scene%2520Reconstruction%2520Benchmark%2520From%2520Aerial%250A%2520%2520Perspective%2520With%2520Delentropy-Based%2520Complexity%2520Profiling%26entry.906535625%3DRadu%2520Beche%2520and%2520Sergiu%2520Nedevschi%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520aerial%2520holistic%2520scene%2520understanding%2520algorithms%2520is%2520hindered%250Aby%2520the%2520scarcity%2520of%2520comprehensive%2520datasets%2520that%2520enable%2520both%2520semantic%2520and%250Ageometric%2520reconstruction.%2520While%2520synthetic%2520datasets%2520offer%2520an%2520alternative%252C%250Aexisting%2520options%2520exhibit%2520task-specific%2520limitations%252C%2520unrealistic%2520scene%250Acompositions%252C%2520and%2520rendering%2520artifacts%2520that%2520compromise%2520real-world%2520applicability.%250AWe%2520introduce%2520ClaraVid%252C%2520a%2520synthetic%2520aerial%2520dataset%2520specifically%2520designed%2520to%250Aovercome%2520these%2520limitations.%2520Comprising%252016%252C917%2520high-resolution%2520images%2520captured%250Aat%25204032x3024%2520from%2520multiple%2520viewpoints%2520across%2520diverse%2520landscapes%252C%2520ClaraVid%250Aprovides%2520dense%2520depth%2520maps%252C%2520panoptic%2520segmentation%252C%2520sparse%2520point%2520clouds%252C%2520and%250Adynamic%2520object%2520masks%252C%2520while%2520mitigating%2520common%2520rendering%2520artifacts.%2520To%2520further%250Aadvance%2520neural%2520reconstruction%252C%2520we%2520introduce%2520the%2520Delentropic%2520Scene%2520Profile%250A%2528DSP%2529%252C%2520a%2520novel%2520complexity%2520metric%2520derived%2520from%2520differential%2520entropy%2520analysis%252C%250Adesigned%2520to%2520quantitatively%2520assess%2520scene%2520difficulty%2520and%2520inform%2520reconstruction%250Atasks.%2520Utilizing%2520DSP%252C%2520we%2520systematically%2520benchmark%2520neural%2520reconstruction%250Amethods%252C%2520uncovering%2520a%2520consistent%252C%2520measurable%2520correlation%2520between%2520scene%250Acomplexity%2520and%2520reconstruction%2520accuracy.%2520Empirical%2520results%2520indicate%2520that%2520higher%250Adelentropy%2520strongly%2520correlates%2520with%2520increased%2520reconstruction%2520errors%252C%2520validating%250ADSP%2520as%2520a%2520reliable%2520complexity%2520prior.%2520The%2520data%2520and%2520code%2520are%2520available%2520on%2520the%250Aproject%2520page%2520at%2520https%253A//rdbch.github.io/claravid/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClaraVid%3A%20A%20Holistic%20Scene%20Reconstruction%20Benchmark%20From%20Aerial%0A%20%20Perspective%20With%20Delentropy-Based%20Complexity%20Profiling&entry.906535625=Radu%20Beche%20and%20Sergiu%20Nedevschi&entry.1292438233=%20%20The%20development%20of%20aerial%20holistic%20scene%20understanding%20algorithms%20is%20hindered%0Aby%20the%20scarcity%20of%20comprehensive%20datasets%20that%20enable%20both%20semantic%20and%0Ageometric%20reconstruction.%20While%20synthetic%20datasets%20offer%20an%20alternative%2C%0Aexisting%20options%20exhibit%20task-specific%20limitations%2C%20unrealistic%20scene%0Acompositions%2C%20and%20rendering%20artifacts%20that%20compromise%20real-world%20applicability.%0AWe%20introduce%20ClaraVid%2C%20a%20synthetic%20aerial%20dataset%20specifically%20designed%20to%0Aovercome%20these%20limitations.%20Comprising%2016%2C917%20high-resolution%20images%20captured%0Aat%204032x3024%20from%20multiple%20viewpoints%20across%20diverse%20landscapes%2C%20ClaraVid%0Aprovides%20dense%20depth%20maps%2C%20panoptic%20segmentation%2C%20sparse%20point%20clouds%2C%20and%0Adynamic%20object%20masks%2C%20while%20mitigating%20common%20rendering%20artifacts.%20To%20further%0Aadvance%20neural%20reconstruction%2C%20we%20introduce%20the%20Delentropic%20Scene%20Profile%0A%28DSP%29%2C%20a%20novel%20complexity%20metric%20derived%20from%20differential%20entropy%20analysis%2C%0Adesigned%20to%20quantitatively%20assess%20scene%20difficulty%20and%20inform%20reconstruction%0Atasks.%20Utilizing%20DSP%2C%20we%20systematically%20benchmark%20neural%20reconstruction%0Amethods%2C%20uncovering%20a%20consistent%2C%20measurable%20correlation%20between%20scene%0Acomplexity%20and%20reconstruction%20accuracy.%20Empirical%20results%20indicate%20that%20higher%0Adelentropy%20strongly%20correlates%20with%20increased%20reconstruction%20errors%2C%20validating%0ADSP%20as%20a%20reliable%20complexity%20prior.%20The%20data%20and%20code%20are%20available%20on%20the%0Aproject%20page%20at%20https%3A//rdbch.github.io/claravid/%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17856v2&entry.124074799=Read"},
{"title": "Unable to Forget: Proactive Interference Reveals Working Memory Limits\n  in LLMs Beyond Context Length", "author": "Chupei Wang and Jiaqiu Vince Sun", "abstract": "  Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.\n", "link": "http://arxiv.org/abs/2506.08184v3", "date": "2025-07-31", "relevancy": 2.3958, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unable%20to%20Forget%3A%20Proactive%20Interference%20Reveals%20Working%20Memory%20Limits%0A%20%20in%20LLMs%20Beyond%20Context%20Length&body=Title%3A%20Unable%20to%20Forget%3A%20Proactive%20Interference%20Reveals%20Working%20Memory%20Limits%0A%20%20in%20LLMs%20Beyond%20Context%20Length%0AAuthor%3A%20Chupei%20Wang%20and%20Jiaqiu%20Vince%20Sun%0AAbstract%3A%20%20%20Information%20retrieval%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20increasingly%0Arecognized%20as%20intertwined%20with%20generation%20capabilities%20rather%20than%20mere%20lookup.%0AWhile%20longer%20contexts%20are%20often%20assumed%20to%20improve%20retrieval%2C%20the%20effects%20of%0Aintra-context%20interference%20remain%20understudied.%20To%20address%20this%2C%20we%20adapt%20the%0Aproactive%20interference%20%28PI%29%20paradigm%20from%20cognitive%20science%2C%20where%20earlier%0Ainformation%20disrupts%20recall%20of%20newer%20updates.%20In%20humans%2C%20susceptibility%20to%20such%0Ainterference%20is%20inversely%20linked%20to%20working%20memory%20capacity.%20We%20introduce%0API-LLM%2C%20an%20evaluation%20that%20sequentially%20streams%20semantically%20related%20key-value%0Aupdates%20and%20queries%20only%20the%20final%20values.%20Although%20these%20final%20values%20are%0Aclearly%20positioned%20just%20before%20the%20query%2C%20LLM%20retrieval%20accuracy%20declines%0Alog-linearly%20toward%20zero%20as%20interference%20accumulates%3B%20errors%20arise%20from%0Aretrieving%20previously%20overwritten%20values.%20Attempts%20to%20mitigate%20interference%20via%0Aprompt%20engineering%20%28e.g.%2C%20instructing%20models%20to%20ignore%20earlier%20input%29%20yield%0Alimited%20success.%20These%20findings%20reveal%20a%20fundamental%20constraint%20on%20LLMs%27%0Aability%20to%20disentangle%20interference%20and%20flexibly%20manipulate%20information%2C%0Asuggesting%20a%20working%20memory%20bottleneck%20beyond%20mere%20context%20access.%20This%20calls%0Afor%20approaches%20that%20strengthen%20models%27%20ability%20to%20suppress%20irrelevant%20content%0Aduring%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.08184v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnable%2520to%2520Forget%253A%2520Proactive%2520Interference%2520Reveals%2520Working%2520Memory%2520Limits%250A%2520%2520in%2520LLMs%2520Beyond%2520Context%2520Length%26entry.906535625%3DChupei%2520Wang%2520and%2520Jiaqiu%2520Vince%2520Sun%26entry.1292438233%3D%2520%2520Information%2520retrieval%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520increasingly%250Arecognized%2520as%2520intertwined%2520with%2520generation%2520capabilities%2520rather%2520than%2520mere%2520lookup.%250AWhile%2520longer%2520contexts%2520are%2520often%2520assumed%2520to%2520improve%2520retrieval%252C%2520the%2520effects%2520of%250Aintra-context%2520interference%2520remain%2520understudied.%2520To%2520address%2520this%252C%2520we%2520adapt%2520the%250Aproactive%2520interference%2520%2528PI%2529%2520paradigm%2520from%2520cognitive%2520science%252C%2520where%2520earlier%250Ainformation%2520disrupts%2520recall%2520of%2520newer%2520updates.%2520In%2520humans%252C%2520susceptibility%2520to%2520such%250Ainterference%2520is%2520inversely%2520linked%2520to%2520working%2520memory%2520capacity.%2520We%2520introduce%250API-LLM%252C%2520an%2520evaluation%2520that%2520sequentially%2520streams%2520semantically%2520related%2520key-value%250Aupdates%2520and%2520queries%2520only%2520the%2520final%2520values.%2520Although%2520these%2520final%2520values%2520are%250Aclearly%2520positioned%2520just%2520before%2520the%2520query%252C%2520LLM%2520retrieval%2520accuracy%2520declines%250Alog-linearly%2520toward%2520zero%2520as%2520interference%2520accumulates%253B%2520errors%2520arise%2520from%250Aretrieving%2520previously%2520overwritten%2520values.%2520Attempts%2520to%2520mitigate%2520interference%2520via%250Aprompt%2520engineering%2520%2528e.g.%252C%2520instructing%2520models%2520to%2520ignore%2520earlier%2520input%2529%2520yield%250Alimited%2520success.%2520These%2520findings%2520reveal%2520a%2520fundamental%2520constraint%2520on%2520LLMs%2527%250Aability%2520to%2520disentangle%2520interference%2520and%2520flexibly%2520manipulate%2520information%252C%250Asuggesting%2520a%2520working%2520memory%2520bottleneck%2520beyond%2520mere%2520context%2520access.%2520This%2520calls%250Afor%2520approaches%2520that%2520strengthen%2520models%2527%2520ability%2520to%2520suppress%2520irrelevant%2520content%250Aduring%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.08184v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unable%20to%20Forget%3A%20Proactive%20Interference%20Reveals%20Working%20Memory%20Limits%0A%20%20in%20LLMs%20Beyond%20Context%20Length&entry.906535625=Chupei%20Wang%20and%20Jiaqiu%20Vince%20Sun&entry.1292438233=%20%20Information%20retrieval%20in%20Large%20Language%20Models%20%28LLMs%29%20is%20increasingly%0Arecognized%20as%20intertwined%20with%20generation%20capabilities%20rather%20than%20mere%20lookup.%0AWhile%20longer%20contexts%20are%20often%20assumed%20to%20improve%20retrieval%2C%20the%20effects%20of%0Aintra-context%20interference%20remain%20understudied.%20To%20address%20this%2C%20we%20adapt%20the%0Aproactive%20interference%20%28PI%29%20paradigm%20from%20cognitive%20science%2C%20where%20earlier%0Ainformation%20disrupts%20recall%20of%20newer%20updates.%20In%20humans%2C%20susceptibility%20to%20such%0Ainterference%20is%20inversely%20linked%20to%20working%20memory%20capacity.%20We%20introduce%0API-LLM%2C%20an%20evaluation%20that%20sequentially%20streams%20semantically%20related%20key-value%0Aupdates%20and%20queries%20only%20the%20final%20values.%20Although%20these%20final%20values%20are%0Aclearly%20positioned%20just%20before%20the%20query%2C%20LLM%20retrieval%20accuracy%20declines%0Alog-linearly%20toward%20zero%20as%20interference%20accumulates%3B%20errors%20arise%20from%0Aretrieving%20previously%20overwritten%20values.%20Attempts%20to%20mitigate%20interference%20via%0Aprompt%20engineering%20%28e.g.%2C%20instructing%20models%20to%20ignore%20earlier%20input%29%20yield%0Alimited%20success.%20These%20findings%20reveal%20a%20fundamental%20constraint%20on%20LLMs%27%0Aability%20to%20disentangle%20interference%20and%20flexibly%20manipulate%20information%2C%0Asuggesting%20a%20working%20memory%20bottleneck%20beyond%20mere%20context%20access.%20This%20calls%0Afor%20approaches%20that%20strengthen%20models%27%20ability%20to%20suppress%20irrelevant%20content%0Aduring%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.08184v3&entry.124074799=Read"},
{"title": "3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection", "author": "Yung-Hsu Yang and Luigi Piccinelli and Mattia Segu and Siyuan Li and Rui Huang and Yuqian Fu and Marc Pollefeys and Hermann Blum and Zuria Bauer", "abstract": "  Monocular 3D object detection is valuable for various applications such as\nrobotics and AR/VR. Existing methods are confined to closed-set settings, where\nthe training and testing sets consist of the same scenes and/or object\ncategories. However, real-world applications often introduce new environments\nand novel object categories, posing a challenge to these methods. In this\npaper, we address monocular 3D object detection in an open-set setting and\nintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).\nWe propose to lift the open-set 2D detection into 3D space through our designed\n3D bounding box head, enabling end-to-end joint training for both 2D and 3D\ntasks to yield better overall performance. We condition the object queries with\ngeometry prior and overcome the generalization for 3D estimation across diverse\nscenes. To further improve performance, we design the canonical image space for\nmore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-set\nsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), and\nachieve new state-of-the-art results. Code and models are available at\nroyyang0714.github.io/3D-MOOD.\n", "link": "http://arxiv.org/abs/2507.23567v1", "date": "2025-07-31", "relevancy": 2.3839, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6133}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5891}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-MOOD%3A%20Lifting%202D%20to%203D%20for%20Monocular%20Open-Set%20Object%20Detection&body=Title%3A%203D-MOOD%3A%20Lifting%202D%20to%203D%20for%20Monocular%20Open-Set%20Object%20Detection%0AAuthor%3A%20Yung-Hsu%20Yang%20and%20Luigi%20Piccinelli%20and%20Mattia%20Segu%20and%20Siyuan%20Li%20and%20Rui%20Huang%20and%20Yuqian%20Fu%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Zuria%20Bauer%0AAbstract%3A%20%20%20Monocular%203D%20object%20detection%20is%20valuable%20for%20various%20applications%20such%20as%0Arobotics%20and%20AR/VR.%20Existing%20methods%20are%20confined%20to%20closed-set%20settings%2C%20where%0Athe%20training%20and%20testing%20sets%20consist%20of%20the%20same%20scenes%20and/or%20object%0Acategories.%20However%2C%20real-world%20applications%20often%20introduce%20new%20environments%0Aand%20novel%20object%20categories%2C%20posing%20a%20challenge%20to%20these%20methods.%20In%20this%0Apaper%2C%20we%20address%20monocular%203D%20object%20detection%20in%20an%20open-set%20setting%20and%0Aintroduce%20the%20first%20end-to-end%203D%20Monocular%20Open-set%20Object%20Detector%20%283D-MOOD%29.%0AWe%20propose%20to%20lift%20the%20open-set%202D%20detection%20into%203D%20space%20through%20our%20designed%0A3D%20bounding%20box%20head%2C%20enabling%20end-to-end%20joint%20training%20for%20both%202D%20and%203D%0Atasks%20to%20yield%20better%20overall%20performance.%20We%20condition%20the%20object%20queries%20with%0Ageometry%20prior%20and%20overcome%20the%20generalization%20for%203D%20estimation%20across%20diverse%0Ascenes.%20To%20further%20improve%20performance%2C%20we%20design%20the%20canonical%20image%20space%20for%0Amore%20efficient%20cross-dataset%20training.%20We%20evaluate%203D-MOOD%20on%20both%20closed-set%0Asettings%20%28Omni3D%29%20and%20open-set%20settings%20%28Omni3D%20to%20Argoverse%202%2C%20ScanNet%29%2C%20and%0Aachieve%20new%20state-of-the-art%20results.%20Code%20and%20models%20are%20available%20at%0Aroyyang0714.github.io/3D-MOOD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-MOOD%253A%2520Lifting%25202D%2520to%25203D%2520for%2520Monocular%2520Open-Set%2520Object%2520Detection%26entry.906535625%3DYung-Hsu%2520Yang%2520and%2520Luigi%2520Piccinelli%2520and%2520Mattia%2520Segu%2520and%2520Siyuan%2520Li%2520and%2520Rui%2520Huang%2520and%2520Yuqian%2520Fu%2520and%2520Marc%2520Pollefeys%2520and%2520Hermann%2520Blum%2520and%2520Zuria%2520Bauer%26entry.1292438233%3D%2520%2520Monocular%25203D%2520object%2520detection%2520is%2520valuable%2520for%2520various%2520applications%2520such%2520as%250Arobotics%2520and%2520AR/VR.%2520Existing%2520methods%2520are%2520confined%2520to%2520closed-set%2520settings%252C%2520where%250Athe%2520training%2520and%2520testing%2520sets%2520consist%2520of%2520the%2520same%2520scenes%2520and/or%2520object%250Acategories.%2520However%252C%2520real-world%2520applications%2520often%2520introduce%2520new%2520environments%250Aand%2520novel%2520object%2520categories%252C%2520posing%2520a%2520challenge%2520to%2520these%2520methods.%2520In%2520this%250Apaper%252C%2520we%2520address%2520monocular%25203D%2520object%2520detection%2520in%2520an%2520open-set%2520setting%2520and%250Aintroduce%2520the%2520first%2520end-to-end%25203D%2520Monocular%2520Open-set%2520Object%2520Detector%2520%25283D-MOOD%2529.%250AWe%2520propose%2520to%2520lift%2520the%2520open-set%25202D%2520detection%2520into%25203D%2520space%2520through%2520our%2520designed%250A3D%2520bounding%2520box%2520head%252C%2520enabling%2520end-to-end%2520joint%2520training%2520for%2520both%25202D%2520and%25203D%250Atasks%2520to%2520yield%2520better%2520overall%2520performance.%2520We%2520condition%2520the%2520object%2520queries%2520with%250Ageometry%2520prior%2520and%2520overcome%2520the%2520generalization%2520for%25203D%2520estimation%2520across%2520diverse%250Ascenes.%2520To%2520further%2520improve%2520performance%252C%2520we%2520design%2520the%2520canonical%2520image%2520space%2520for%250Amore%2520efficient%2520cross-dataset%2520training.%2520We%2520evaluate%25203D-MOOD%2520on%2520both%2520closed-set%250Asettings%2520%2528Omni3D%2529%2520and%2520open-set%2520settings%2520%2528Omni3D%2520to%2520Argoverse%25202%252C%2520ScanNet%2529%252C%2520and%250Aachieve%2520new%2520state-of-the-art%2520results.%2520Code%2520and%2520models%2520are%2520available%2520at%250Aroyyang0714.github.io/3D-MOOD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-MOOD%3A%20Lifting%202D%20to%203D%20for%20Monocular%20Open-Set%20Object%20Detection&entry.906535625=Yung-Hsu%20Yang%20and%20Luigi%20Piccinelli%20and%20Mattia%20Segu%20and%20Siyuan%20Li%20and%20Rui%20Huang%20and%20Yuqian%20Fu%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%20and%20Zuria%20Bauer&entry.1292438233=%20%20Monocular%203D%20object%20detection%20is%20valuable%20for%20various%20applications%20such%20as%0Arobotics%20and%20AR/VR.%20Existing%20methods%20are%20confined%20to%20closed-set%20settings%2C%20where%0Athe%20training%20and%20testing%20sets%20consist%20of%20the%20same%20scenes%20and/or%20object%0Acategories.%20However%2C%20real-world%20applications%20often%20introduce%20new%20environments%0Aand%20novel%20object%20categories%2C%20posing%20a%20challenge%20to%20these%20methods.%20In%20this%0Apaper%2C%20we%20address%20monocular%203D%20object%20detection%20in%20an%20open-set%20setting%20and%0Aintroduce%20the%20first%20end-to-end%203D%20Monocular%20Open-set%20Object%20Detector%20%283D-MOOD%29.%0AWe%20propose%20to%20lift%20the%20open-set%202D%20detection%20into%203D%20space%20through%20our%20designed%0A3D%20bounding%20box%20head%2C%20enabling%20end-to-end%20joint%20training%20for%20both%202D%20and%203D%0Atasks%20to%20yield%20better%20overall%20performance.%20We%20condition%20the%20object%20queries%20with%0Ageometry%20prior%20and%20overcome%20the%20generalization%20for%203D%20estimation%20across%20diverse%0Ascenes.%20To%20further%20improve%20performance%2C%20we%20design%20the%20canonical%20image%20space%20for%0Amore%20efficient%20cross-dataset%20training.%20We%20evaluate%203D-MOOD%20on%20both%20closed-set%0Asettings%20%28Omni3D%29%20and%20open-set%20settings%20%28Omni3D%20to%20Argoverse%202%2C%20ScanNet%29%2C%20and%0Aachieve%20new%20state-of-the-art%20results.%20Code%20and%20models%20are%20available%20at%0Aroyyang0714.github.io/3D-MOOD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23567v1&entry.124074799=Read"},
{"title": "Weighted least-squares approximation with determinantal point processes\n  and generalized volume sampling", "author": "Anthony Nouy and Bertrand Michel", "abstract": "  We consider the problem of approximating a function from $L^2$ by an element\nof a given $m$-dimensional space $V_m$, associated with some feature map\n$\\boldsymbol{\\varphi}$, using evaluations of the function at random points\n$x_1, \\dots,x_n$. After recalling some results on optimal weighted\nleast-squares using independent and identically distributed points, we consider\nweighted least-squares using projection determinantal point processes (DPP) or\nvolume sampling. These distributions introduce dependence between the points\nthat promotes diversity in the selected features $\\boldsymbol{\\varphi}(x_i)$.\nWe first provide a generalized version of volume-rescaled sampling yielding\nquasi-optimality results in expectation with a number of samples $n =\nO(m\\log(m))$, that means that the expected $L^2$ error is bounded by a constant\ntimes the best approximation error in $L^2$. Also, further assuming that the\nfunction is in some normed vector space $H$ continuously embedded in $L^2$, we\nfurther prove that the approximation error in $L^2$ is almost surely bounded by\nthe best approximation error measured in the $H$-norm. This includes the cases\nof functions from $L^\\infty$ or reproducing kernel Hilbert spaces. Finally, we\npresent an alternative strategy consisting in using independent repetitions of\nprojection DPP (or volume sampling), yielding similar error bounds as with\ni.i.d. or volume sampling, but in practice with a much lower number of samples.\nNumerical experiments illustrate the performance of the different strategies.\n", "link": "http://arxiv.org/abs/2312.14057v4", "date": "2025-07-31", "relevancy": 2.3813, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4818}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4748}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weighted%20least-squares%20approximation%20with%20determinantal%20point%20processes%0A%20%20and%20generalized%20volume%20sampling&body=Title%3A%20Weighted%20least-squares%20approximation%20with%20determinantal%20point%20processes%0A%20%20and%20generalized%20volume%20sampling%0AAuthor%3A%20Anthony%20Nouy%20and%20Bertrand%20Michel%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20approximating%20a%20function%20from%20%24L%5E2%24%20by%20an%20element%0Aof%20a%20given%20%24m%24-dimensional%20space%20%24V_m%24%2C%20associated%20with%20some%20feature%20map%0A%24%5Cboldsymbol%7B%5Cvarphi%7D%24%2C%20using%20evaluations%20of%20the%20function%20at%20random%20points%0A%24x_1%2C%20%5Cdots%2Cx_n%24.%20After%20recalling%20some%20results%20on%20optimal%20weighted%0Aleast-squares%20using%20independent%20and%20identically%20distributed%20points%2C%20we%20consider%0Aweighted%20least-squares%20using%20projection%20determinantal%20point%20processes%20%28DPP%29%20or%0Avolume%20sampling.%20These%20distributions%20introduce%20dependence%20between%20the%20points%0Athat%20promotes%20diversity%20in%20the%20selected%20features%20%24%5Cboldsymbol%7B%5Cvarphi%7D%28x_i%29%24.%0AWe%20first%20provide%20a%20generalized%20version%20of%20volume-rescaled%20sampling%20yielding%0Aquasi-optimality%20results%20in%20expectation%20with%20a%20number%20of%20samples%20%24n%20%3D%0AO%28m%5Clog%28m%29%29%24%2C%20that%20means%20that%20the%20expected%20%24L%5E2%24%20error%20is%20bounded%20by%20a%20constant%0Atimes%20the%20best%20approximation%20error%20in%20%24L%5E2%24.%20Also%2C%20further%20assuming%20that%20the%0Afunction%20is%20in%20some%20normed%20vector%20space%20%24H%24%20continuously%20embedded%20in%20%24L%5E2%24%2C%20we%0Afurther%20prove%20that%20the%20approximation%20error%20in%20%24L%5E2%24%20is%20almost%20surely%20bounded%20by%0Athe%20best%20approximation%20error%20measured%20in%20the%20%24H%24-norm.%20This%20includes%20the%20cases%0Aof%20functions%20from%20%24L%5E%5Cinfty%24%20or%20reproducing%20kernel%20Hilbert%20spaces.%20Finally%2C%20we%0Apresent%20an%20alternative%20strategy%20consisting%20in%20using%20independent%20repetitions%20of%0Aprojection%20DPP%20%28or%20volume%20sampling%29%2C%20yielding%20similar%20error%20bounds%20as%20with%0Ai.i.d.%20or%20volume%20sampling%2C%20but%20in%20practice%20with%20a%20much%20lower%20number%20of%20samples.%0ANumerical%20experiments%20illustrate%20the%20performance%20of%20the%20different%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14057v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeighted%2520least-squares%2520approximation%2520with%2520determinantal%2520point%2520processes%250A%2520%2520and%2520generalized%2520volume%2520sampling%26entry.906535625%3DAnthony%2520Nouy%2520and%2520Bertrand%2520Michel%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520approximating%2520a%2520function%2520from%2520%2524L%255E2%2524%2520by%2520an%2520element%250Aof%2520a%2520given%2520%2524m%2524-dimensional%2520space%2520%2524V_m%2524%252C%2520associated%2520with%2520some%2520feature%2520map%250A%2524%255Cboldsymbol%257B%255Cvarphi%257D%2524%252C%2520using%2520evaluations%2520of%2520the%2520function%2520at%2520random%2520points%250A%2524x_1%252C%2520%255Cdots%252Cx_n%2524.%2520After%2520recalling%2520some%2520results%2520on%2520optimal%2520weighted%250Aleast-squares%2520using%2520independent%2520and%2520identically%2520distributed%2520points%252C%2520we%2520consider%250Aweighted%2520least-squares%2520using%2520projection%2520determinantal%2520point%2520processes%2520%2528DPP%2529%2520or%250Avolume%2520sampling.%2520These%2520distributions%2520introduce%2520dependence%2520between%2520the%2520points%250Athat%2520promotes%2520diversity%2520in%2520the%2520selected%2520features%2520%2524%255Cboldsymbol%257B%255Cvarphi%257D%2528x_i%2529%2524.%250AWe%2520first%2520provide%2520a%2520generalized%2520version%2520of%2520volume-rescaled%2520sampling%2520yielding%250Aquasi-optimality%2520results%2520in%2520expectation%2520with%2520a%2520number%2520of%2520samples%2520%2524n%2520%253D%250AO%2528m%255Clog%2528m%2529%2529%2524%252C%2520that%2520means%2520that%2520the%2520expected%2520%2524L%255E2%2524%2520error%2520is%2520bounded%2520by%2520a%2520constant%250Atimes%2520the%2520best%2520approximation%2520error%2520in%2520%2524L%255E2%2524.%2520Also%252C%2520further%2520assuming%2520that%2520the%250Afunction%2520is%2520in%2520some%2520normed%2520vector%2520space%2520%2524H%2524%2520continuously%2520embedded%2520in%2520%2524L%255E2%2524%252C%2520we%250Afurther%2520prove%2520that%2520the%2520approximation%2520error%2520in%2520%2524L%255E2%2524%2520is%2520almost%2520surely%2520bounded%2520by%250Athe%2520best%2520approximation%2520error%2520measured%2520in%2520the%2520%2524H%2524-norm.%2520This%2520includes%2520the%2520cases%250Aof%2520functions%2520from%2520%2524L%255E%255Cinfty%2524%2520or%2520reproducing%2520kernel%2520Hilbert%2520spaces.%2520Finally%252C%2520we%250Apresent%2520an%2520alternative%2520strategy%2520consisting%2520in%2520using%2520independent%2520repetitions%2520of%250Aprojection%2520DPP%2520%2528or%2520volume%2520sampling%2529%252C%2520yielding%2520similar%2520error%2520bounds%2520as%2520with%250Ai.i.d.%2520or%2520volume%2520sampling%252C%2520but%2520in%2520practice%2520with%2520a%2520much%2520lower%2520number%2520of%2520samples.%250ANumerical%2520experiments%2520illustrate%2520the%2520performance%2520of%2520the%2520different%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14057v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weighted%20least-squares%20approximation%20with%20determinantal%20point%20processes%0A%20%20and%20generalized%20volume%20sampling&entry.906535625=Anthony%20Nouy%20and%20Bertrand%20Michel&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20approximating%20a%20function%20from%20%24L%5E2%24%20by%20an%20element%0Aof%20a%20given%20%24m%24-dimensional%20space%20%24V_m%24%2C%20associated%20with%20some%20feature%20map%0A%24%5Cboldsymbol%7B%5Cvarphi%7D%24%2C%20using%20evaluations%20of%20the%20function%20at%20random%20points%0A%24x_1%2C%20%5Cdots%2Cx_n%24.%20After%20recalling%20some%20results%20on%20optimal%20weighted%0Aleast-squares%20using%20independent%20and%20identically%20distributed%20points%2C%20we%20consider%0Aweighted%20least-squares%20using%20projection%20determinantal%20point%20processes%20%28DPP%29%20or%0Avolume%20sampling.%20These%20distributions%20introduce%20dependence%20between%20the%20points%0Athat%20promotes%20diversity%20in%20the%20selected%20features%20%24%5Cboldsymbol%7B%5Cvarphi%7D%28x_i%29%24.%0AWe%20first%20provide%20a%20generalized%20version%20of%20volume-rescaled%20sampling%20yielding%0Aquasi-optimality%20results%20in%20expectation%20with%20a%20number%20of%20samples%20%24n%20%3D%0AO%28m%5Clog%28m%29%29%24%2C%20that%20means%20that%20the%20expected%20%24L%5E2%24%20error%20is%20bounded%20by%20a%20constant%0Atimes%20the%20best%20approximation%20error%20in%20%24L%5E2%24.%20Also%2C%20further%20assuming%20that%20the%0Afunction%20is%20in%20some%20normed%20vector%20space%20%24H%24%20continuously%20embedded%20in%20%24L%5E2%24%2C%20we%0Afurther%20prove%20that%20the%20approximation%20error%20in%20%24L%5E2%24%20is%20almost%20surely%20bounded%20by%0Athe%20best%20approximation%20error%20measured%20in%20the%20%24H%24-norm.%20This%20includes%20the%20cases%0Aof%20functions%20from%20%24L%5E%5Cinfty%24%20or%20reproducing%20kernel%20Hilbert%20spaces.%20Finally%2C%20we%0Apresent%20an%20alternative%20strategy%20consisting%20in%20using%20independent%20repetitions%20of%0Aprojection%20DPP%20%28or%20volume%20sampling%29%2C%20yielding%20similar%20error%20bounds%20as%20with%0Ai.i.d.%20or%20volume%20sampling%2C%20but%20in%20practice%20with%20a%20much%20lower%20number%20of%20samples.%0ANumerical%20experiments%20illustrate%20the%20performance%20of%20the%20different%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14057v4&entry.124074799=Read"},
{"title": "Divided Attention: Unsupervised Multi-Object Discovery with Contextually\n  Separated Slots", "author": "Dong Lao and Zhengyang Hu and Francesco Locatello and Yanchao Yang and Stefano Soatto", "abstract": "  We investigate the emergence of objects in visual perception in the absence\nof any semantic annotation. The resulting model has received no supervision,\ndoes not use any pre-trained features, and yet it can segment the domain of an\nimage into multiple independently moving regions. The resulting motion\nsegmentation method can handle an unknown and varying number of objects in\nreal-time. The core multi-modal conditional encoder-decoder architecture has\none modality (optical flow) feed the encoder to produce a collection of latent\ncodes (slots), and the other modality (color image) conditions the decoder to\ngenerate the first modality (flow) from the slots. The training criterion is\ndesigned to foster 'information separation' among the slots, while the\narchitecture explicitly allocates activations to individual slots, leading to a\nmethod we call Divided Attention (DivA). At test time, DivA handles a different\nnumber of objects and different image resolution than seen at training, and is\ninvariant to permutations of the slots. DivA achieves state-of-the-art\nperformance while tripling the runtime speed of comparable methods, up to 104\nFPS, and reduces the performance gap from supervised methods to 12% or less.\nObjects bootstrapped by DivA can then be used to prime static classifiers via\ncontrastive learning. On fewer than 5,000 video clips, training DINO on DivA's\nobject proposals narrows the performance gap to ImageNet-based training by up\nto 30.2% compared to training directly on the video frames.\n", "link": "http://arxiv.org/abs/2304.01430v3", "date": "2025-07-31", "relevancy": 2.3734, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Divided%20Attention%3A%20Unsupervised%20Multi-Object%20Discovery%20with%20Contextually%0A%20%20Separated%20Slots&body=Title%3A%20Divided%20Attention%3A%20Unsupervised%20Multi-Object%20Discovery%20with%20Contextually%0A%20%20Separated%20Slots%0AAuthor%3A%20Dong%20Lao%20and%20Zhengyang%20Hu%20and%20Francesco%20Locatello%20and%20Yanchao%20Yang%20and%20Stefano%20Soatto%0AAbstract%3A%20%20%20We%20investigate%20the%20emergence%20of%20objects%20in%20visual%20perception%20in%20the%20absence%0Aof%20any%20semantic%20annotation.%20The%20resulting%20model%20has%20received%20no%20supervision%2C%0Adoes%20not%20use%20any%20pre-trained%20features%2C%20and%20yet%20it%20can%20segment%20the%20domain%20of%20an%0Aimage%20into%20multiple%20independently%20moving%20regions.%20The%20resulting%20motion%0Asegmentation%20method%20can%20handle%20an%20unknown%20and%20varying%20number%20of%20objects%20in%0Areal-time.%20The%20core%20multi-modal%20conditional%20encoder-decoder%20architecture%20has%0Aone%20modality%20%28optical%20flow%29%20feed%20the%20encoder%20to%20produce%20a%20collection%20of%20latent%0Acodes%20%28slots%29%2C%20and%20the%20other%20modality%20%28color%20image%29%20conditions%20the%20decoder%20to%0Agenerate%20the%20first%20modality%20%28flow%29%20from%20the%20slots.%20The%20training%20criterion%20is%0Adesigned%20to%20foster%20%27information%20separation%27%20among%20the%20slots%2C%20while%20the%0Aarchitecture%20explicitly%20allocates%20activations%20to%20individual%20slots%2C%20leading%20to%20a%0Amethod%20we%20call%20Divided%20Attention%20%28DivA%29.%20At%20test%20time%2C%20DivA%20handles%20a%20different%0Anumber%20of%20objects%20and%20different%20image%20resolution%20than%20seen%20at%20training%2C%20and%20is%0Ainvariant%20to%20permutations%20of%20the%20slots.%20DivA%20achieves%20state-of-the-art%0Aperformance%20while%20tripling%20the%20runtime%20speed%20of%20comparable%20methods%2C%20up%20to%20104%0AFPS%2C%20and%20reduces%20the%20performance%20gap%20from%20supervised%20methods%20to%2012%25%20or%20less.%0AObjects%20bootstrapped%20by%20DivA%20can%20then%20be%20used%20to%20prime%20static%20classifiers%20via%0Acontrastive%20learning.%20On%20fewer%20than%205%2C000%20video%20clips%2C%20training%20DINO%20on%20DivA%27s%0Aobject%20proposals%20narrows%20the%20performance%20gap%20to%20ImageNet-based%20training%20by%20up%0Ato%2030.2%25%20compared%20to%20training%20directly%20on%20the%20video%20frames.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.01430v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDivided%2520Attention%253A%2520Unsupervised%2520Multi-Object%2520Discovery%2520with%2520Contextually%250A%2520%2520Separated%2520Slots%26entry.906535625%3DDong%2520Lao%2520and%2520Zhengyang%2520Hu%2520and%2520Francesco%2520Locatello%2520and%2520Yanchao%2520Yang%2520and%2520Stefano%2520Soatto%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520emergence%2520of%2520objects%2520in%2520visual%2520perception%2520in%2520the%2520absence%250Aof%2520any%2520semantic%2520annotation.%2520The%2520resulting%2520model%2520has%2520received%2520no%2520supervision%252C%250Adoes%2520not%2520use%2520any%2520pre-trained%2520features%252C%2520and%2520yet%2520it%2520can%2520segment%2520the%2520domain%2520of%2520an%250Aimage%2520into%2520multiple%2520independently%2520moving%2520regions.%2520The%2520resulting%2520motion%250Asegmentation%2520method%2520can%2520handle%2520an%2520unknown%2520and%2520varying%2520number%2520of%2520objects%2520in%250Areal-time.%2520The%2520core%2520multi-modal%2520conditional%2520encoder-decoder%2520architecture%2520has%250Aone%2520modality%2520%2528optical%2520flow%2529%2520feed%2520the%2520encoder%2520to%2520produce%2520a%2520collection%2520of%2520latent%250Acodes%2520%2528slots%2529%252C%2520and%2520the%2520other%2520modality%2520%2528color%2520image%2529%2520conditions%2520the%2520decoder%2520to%250Agenerate%2520the%2520first%2520modality%2520%2528flow%2529%2520from%2520the%2520slots.%2520The%2520training%2520criterion%2520is%250Adesigned%2520to%2520foster%2520%2527information%2520separation%2527%2520among%2520the%2520slots%252C%2520while%2520the%250Aarchitecture%2520explicitly%2520allocates%2520activations%2520to%2520individual%2520slots%252C%2520leading%2520to%2520a%250Amethod%2520we%2520call%2520Divided%2520Attention%2520%2528DivA%2529.%2520At%2520test%2520time%252C%2520DivA%2520handles%2520a%2520different%250Anumber%2520of%2520objects%2520and%2520different%2520image%2520resolution%2520than%2520seen%2520at%2520training%252C%2520and%2520is%250Ainvariant%2520to%2520permutations%2520of%2520the%2520slots.%2520DivA%2520achieves%2520state-of-the-art%250Aperformance%2520while%2520tripling%2520the%2520runtime%2520speed%2520of%2520comparable%2520methods%252C%2520up%2520to%2520104%250AFPS%252C%2520and%2520reduces%2520the%2520performance%2520gap%2520from%2520supervised%2520methods%2520to%252012%2525%2520or%2520less.%250AObjects%2520bootstrapped%2520by%2520DivA%2520can%2520then%2520be%2520used%2520to%2520prime%2520static%2520classifiers%2520via%250Acontrastive%2520learning.%2520On%2520fewer%2520than%25205%252C000%2520video%2520clips%252C%2520training%2520DINO%2520on%2520DivA%2527s%250Aobject%2520proposals%2520narrows%2520the%2520performance%2520gap%2520to%2520ImageNet-based%2520training%2520by%2520up%250Ato%252030.2%2525%2520compared%2520to%2520training%2520directly%2520on%2520the%2520video%2520frames.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.01430v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Divided%20Attention%3A%20Unsupervised%20Multi-Object%20Discovery%20with%20Contextually%0A%20%20Separated%20Slots&entry.906535625=Dong%20Lao%20and%20Zhengyang%20Hu%20and%20Francesco%20Locatello%20and%20Yanchao%20Yang%20and%20Stefano%20Soatto&entry.1292438233=%20%20We%20investigate%20the%20emergence%20of%20objects%20in%20visual%20perception%20in%20the%20absence%0Aof%20any%20semantic%20annotation.%20The%20resulting%20model%20has%20received%20no%20supervision%2C%0Adoes%20not%20use%20any%20pre-trained%20features%2C%20and%20yet%20it%20can%20segment%20the%20domain%20of%20an%0Aimage%20into%20multiple%20independently%20moving%20regions.%20The%20resulting%20motion%0Asegmentation%20method%20can%20handle%20an%20unknown%20and%20varying%20number%20of%20objects%20in%0Areal-time.%20The%20core%20multi-modal%20conditional%20encoder-decoder%20architecture%20has%0Aone%20modality%20%28optical%20flow%29%20feed%20the%20encoder%20to%20produce%20a%20collection%20of%20latent%0Acodes%20%28slots%29%2C%20and%20the%20other%20modality%20%28color%20image%29%20conditions%20the%20decoder%20to%0Agenerate%20the%20first%20modality%20%28flow%29%20from%20the%20slots.%20The%20training%20criterion%20is%0Adesigned%20to%20foster%20%27information%20separation%27%20among%20the%20slots%2C%20while%20the%0Aarchitecture%20explicitly%20allocates%20activations%20to%20individual%20slots%2C%20leading%20to%20a%0Amethod%20we%20call%20Divided%20Attention%20%28DivA%29.%20At%20test%20time%2C%20DivA%20handles%20a%20different%0Anumber%20of%20objects%20and%20different%20image%20resolution%20than%20seen%20at%20training%2C%20and%20is%0Ainvariant%20to%20permutations%20of%20the%20slots.%20DivA%20achieves%20state-of-the-art%0Aperformance%20while%20tripling%20the%20runtime%20speed%20of%20comparable%20methods%2C%20up%20to%20104%0AFPS%2C%20and%20reduces%20the%20performance%20gap%20from%20supervised%20methods%20to%2012%25%20or%20less.%0AObjects%20bootstrapped%20by%20DivA%20can%20then%20be%20used%20to%20prime%20static%20classifiers%20via%0Acontrastive%20learning.%20On%20fewer%20than%205%2C000%20video%20clips%2C%20training%20DINO%20on%20DivA%27s%0Aobject%20proposals%20narrows%20the%20performance%20gap%20to%20ImageNet-based%20training%20by%20up%0Ato%2030.2%25%20compared%20to%20training%20directly%20on%20the%20video%20frames.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.01430v3&entry.124074799=Read"},
{"title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous\n  Driving", "author": "Yi Zhang and Erik Leo Ha\u00df and Kuo-Yi Chao and Nenad Petrovic and Yinglei Song and Chengdong Wu and Alois Knoll", "abstract": "  Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems.\n", "link": "http://arxiv.org/abs/2507.23540v1", "date": "2025-07-31", "relevancy": 2.3688, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6177}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Perception-Language-Action%20Framework%20for%20Adaptive%20Autonomous%0A%20%20Driving&body=Title%3A%20A%20Unified%20Perception-Language-Action%20Framework%20for%20Adaptive%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Yi%20Zhang%20and%20Erik%20Leo%20Ha%C3%9F%20and%20Kuo-Yi%20Chao%20and%20Nenad%20Petrovic%20and%20Yinglei%20Song%20and%20Chengdong%20Wu%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20face%20significant%20challenges%20in%20achieving%0Ahuman-like%20adaptability%2C%20robustness%2C%20and%20interpretability%20in%20complex%2C%0Aopen-world%20environments.%20These%20challenges%20stem%20from%20fragmented%20architectures%2C%0Alimited%20generalization%20to%20novel%20scenarios%2C%20and%20insufficient%20semantic%20extraction%0Afrom%20perception.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20unified%0APerception-Language-Action%20%28PLA%29%20framework%20that%20integrates%20multi-sensor%20fusion%0A%28cameras%2C%20LiDAR%2C%20radar%29%20with%20a%20large%20language%20model%20%28LLM%29-augmented%0AVision-Language-Action%20%28VLA%29%20architecture%2C%20specifically%20a%20GPT-4.1-powered%0Areasoning%20core.%20This%20framework%20unifies%20low-level%20sensory%20processing%20with%0Ahigh-level%20contextual%20reasoning%2C%20tightly%20coupling%20perception%20with%20natural%0Alanguage-based%20semantic%20understanding%20and%20decision-making%20to%20enable%0Acontext-aware%2C%20explainable%2C%20and%20safety-bounded%20autonomous%20driving.%20Evaluations%0Aon%20an%20urban%20intersection%20scenario%20with%20a%20construction%20zone%20demonstrate%20superior%0Aperformance%20in%20trajectory%20tracking%2C%20speed%20prediction%2C%20and%20adaptive%20planning.%0AThe%20results%20highlight%20the%20potential%20of%20language-augmented%20cognitive%20frameworks%0Afor%20advancing%20the%20safety%2C%20interpretability%2C%20and%20scalability%20of%20autonomous%0Adriving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Perception-Language-Action%2520Framework%2520for%2520Adaptive%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DYi%2520Zhang%2520and%2520Erik%2520Leo%2520Ha%25C3%259F%2520and%2520Kuo-Yi%2520Chao%2520and%2520Nenad%2520Petrovic%2520and%2520Yinglei%2520Song%2520and%2520Chengdong%2520Wu%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520systems%2520face%2520significant%2520challenges%2520in%2520achieving%250Ahuman-like%2520adaptability%252C%2520robustness%252C%2520and%2520interpretability%2520in%2520complex%252C%250Aopen-world%2520environments.%2520These%2520challenges%2520stem%2520from%2520fragmented%2520architectures%252C%250Alimited%2520generalization%2520to%2520novel%2520scenarios%252C%2520and%2520insufficient%2520semantic%2520extraction%250Afrom%2520perception.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520unified%250APerception-Language-Action%2520%2528PLA%2529%2520framework%2520that%2520integrates%2520multi-sensor%2520fusion%250A%2528cameras%252C%2520LiDAR%252C%2520radar%2529%2520with%2520a%2520large%2520language%2520model%2520%2528LLM%2529-augmented%250AVision-Language-Action%2520%2528VLA%2529%2520architecture%252C%2520specifically%2520a%2520GPT-4.1-powered%250Areasoning%2520core.%2520This%2520framework%2520unifies%2520low-level%2520sensory%2520processing%2520with%250Ahigh-level%2520contextual%2520reasoning%252C%2520tightly%2520coupling%2520perception%2520with%2520natural%250Alanguage-based%2520semantic%2520understanding%2520and%2520decision-making%2520to%2520enable%250Acontext-aware%252C%2520explainable%252C%2520and%2520safety-bounded%2520autonomous%2520driving.%2520Evaluations%250Aon%2520an%2520urban%2520intersection%2520scenario%2520with%2520a%2520construction%2520zone%2520demonstrate%2520superior%250Aperformance%2520in%2520trajectory%2520tracking%252C%2520speed%2520prediction%252C%2520and%2520adaptive%2520planning.%250AThe%2520results%2520highlight%2520the%2520potential%2520of%2520language-augmented%2520cognitive%2520frameworks%250Afor%2520advancing%2520the%2520safety%252C%2520interpretability%252C%2520and%2520scalability%2520of%2520autonomous%250Adriving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Perception-Language-Action%20Framework%20for%20Adaptive%20Autonomous%0A%20%20Driving&entry.906535625=Yi%20Zhang%20and%20Erik%20Leo%20Ha%C3%9F%20and%20Kuo-Yi%20Chao%20and%20Nenad%20Petrovic%20and%20Yinglei%20Song%20and%20Chengdong%20Wu%20and%20Alois%20Knoll&entry.1292438233=%20%20Autonomous%20driving%20systems%20face%20significant%20challenges%20in%20achieving%0Ahuman-like%20adaptability%2C%20robustness%2C%20and%20interpretability%20in%20complex%2C%0Aopen-world%20environments.%20These%20challenges%20stem%20from%20fragmented%20architectures%2C%0Alimited%20generalization%20to%20novel%20scenarios%2C%20and%20insufficient%20semantic%20extraction%0Afrom%20perception.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20unified%0APerception-Language-Action%20%28PLA%29%20framework%20that%20integrates%20multi-sensor%20fusion%0A%28cameras%2C%20LiDAR%2C%20radar%29%20with%20a%20large%20language%20model%20%28LLM%29-augmented%0AVision-Language-Action%20%28VLA%29%20architecture%2C%20specifically%20a%20GPT-4.1-powered%0Areasoning%20core.%20This%20framework%20unifies%20low-level%20sensory%20processing%20with%0Ahigh-level%20contextual%20reasoning%2C%20tightly%20coupling%20perception%20with%20natural%0Alanguage-based%20semantic%20understanding%20and%20decision-making%20to%20enable%0Acontext-aware%2C%20explainable%2C%20and%20safety-bounded%20autonomous%20driving.%20Evaluations%0Aon%20an%20urban%20intersection%20scenario%20with%20a%20construction%20zone%20demonstrate%20superior%0Aperformance%20in%20trajectory%20tracking%2C%20speed%20prediction%2C%20and%20adaptive%20planning.%0AThe%20results%20highlight%20the%20potential%20of%20language-augmented%20cognitive%20frameworks%0Afor%20advancing%20the%20safety%2C%20interpretability%2C%20and%20scalability%20of%20autonomous%0Adriving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23540v1&entry.124074799=Read"},
{"title": "Decentralized Uncertainty-Aware Multi-Agent Collision Avoidance with\n  Model Predictive Path Integral", "author": "Stepan Dergachev and Konstantin Yakovlev", "abstract": "  Decentralized multi-agent navigation under uncertainty is a complex task that\narises in numerous robotic applications. It requires collision avoidance\nstrategies that account for both kinematic constraints, sensing and action\nexecution noise. In this paper, we propose a novel approach that integrates the\nModel Predictive Path Integral (MPPI) with a probabilistic adaptation of\nOptimal Reciprocal Collision Avoidance. Our method ensures safe and efficient\nmulti-agent navigation by incorporating probabilistic safety constraints\ndirectly into the MPPI sampling process via a Second-Order Cone Programming\nformulation. This approach enables agents to operate independently using local\nnoisy observations while maintaining safety guarantees. We validate our\nalgorithm through extensive simulations with differential-drive robots and\nbenchmark it against state-of-the-art methods, including ORCA-DD and B-UAVC.\nResults demonstrate that our approach outperforms them while achieving high\nsuccess rates, even in densely populated environments. Additionally, validation\nin the Gazebo simulator confirms its practical applicability to robotic\nplatforms. A source code is available at\nhttp://github.com/PathPlanning/MPPI-Collision-Avoidance.\n", "link": "http://arxiv.org/abs/2507.20293v2", "date": "2025-07-31", "relevancy": 2.3624, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6297}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Uncertainty-Aware%20Multi-Agent%20Collision%20Avoidance%20with%0A%20%20Model%20Predictive%20Path%20Integral&body=Title%3A%20Decentralized%20Uncertainty-Aware%20Multi-Agent%20Collision%20Avoidance%20with%0A%20%20Model%20Predictive%20Path%20Integral%0AAuthor%3A%20Stepan%20Dergachev%20and%20Konstantin%20Yakovlev%0AAbstract%3A%20%20%20Decentralized%20multi-agent%20navigation%20under%20uncertainty%20is%20a%20complex%20task%20that%0Aarises%20in%20numerous%20robotic%20applications.%20It%20requires%20collision%20avoidance%0Astrategies%20that%20account%20for%20both%20kinematic%20constraints%2C%20sensing%20and%20action%0Aexecution%20noise.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20that%20integrates%20the%0AModel%20Predictive%20Path%20Integral%20%28MPPI%29%20with%20a%20probabilistic%20adaptation%20of%0AOptimal%20Reciprocal%20Collision%20Avoidance.%20Our%20method%20ensures%20safe%20and%20efficient%0Amulti-agent%20navigation%20by%20incorporating%20probabilistic%20safety%20constraints%0Adirectly%20into%20the%20MPPI%20sampling%20process%20via%20a%20Second-Order%20Cone%20Programming%0Aformulation.%20This%20approach%20enables%20agents%20to%20operate%20independently%20using%20local%0Anoisy%20observations%20while%20maintaining%20safety%20guarantees.%20We%20validate%20our%0Aalgorithm%20through%20extensive%20simulations%20with%20differential-drive%20robots%20and%0Abenchmark%20it%20against%20state-of-the-art%20methods%2C%20including%20ORCA-DD%20and%20B-UAVC.%0AResults%20demonstrate%20that%20our%20approach%20outperforms%20them%20while%20achieving%20high%0Asuccess%20rates%2C%20even%20in%20densely%20populated%20environments.%20Additionally%2C%20validation%0Ain%20the%20Gazebo%20simulator%20confirms%20its%20practical%20applicability%20to%20robotic%0Aplatforms.%20A%20source%20code%20is%20available%20at%0Ahttp%3A//github.com/PathPlanning/MPPI-Collision-Avoidance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.20293v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Uncertainty-Aware%2520Multi-Agent%2520Collision%2520Avoidance%2520with%250A%2520%2520Model%2520Predictive%2520Path%2520Integral%26entry.906535625%3DStepan%2520Dergachev%2520and%2520Konstantin%2520Yakovlev%26entry.1292438233%3D%2520%2520Decentralized%2520multi-agent%2520navigation%2520under%2520uncertainty%2520is%2520a%2520complex%2520task%2520that%250Aarises%2520in%2520numerous%2520robotic%2520applications.%2520It%2520requires%2520collision%2520avoidance%250Astrategies%2520that%2520account%2520for%2520both%2520kinematic%2520constraints%252C%2520sensing%2520and%2520action%250Aexecution%2520noise.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520integrates%2520the%250AModel%2520Predictive%2520Path%2520Integral%2520%2528MPPI%2529%2520with%2520a%2520probabilistic%2520adaptation%2520of%250AOptimal%2520Reciprocal%2520Collision%2520Avoidance.%2520Our%2520method%2520ensures%2520safe%2520and%2520efficient%250Amulti-agent%2520navigation%2520by%2520incorporating%2520probabilistic%2520safety%2520constraints%250Adirectly%2520into%2520the%2520MPPI%2520sampling%2520process%2520via%2520a%2520Second-Order%2520Cone%2520Programming%250Aformulation.%2520This%2520approach%2520enables%2520agents%2520to%2520operate%2520independently%2520using%2520local%250Anoisy%2520observations%2520while%2520maintaining%2520safety%2520guarantees.%2520We%2520validate%2520our%250Aalgorithm%2520through%2520extensive%2520simulations%2520with%2520differential-drive%2520robots%2520and%250Abenchmark%2520it%2520against%2520state-of-the-art%2520methods%252C%2520including%2520ORCA-DD%2520and%2520B-UAVC.%250AResults%2520demonstrate%2520that%2520our%2520approach%2520outperforms%2520them%2520while%2520achieving%2520high%250Asuccess%2520rates%252C%2520even%2520in%2520densely%2520populated%2520environments.%2520Additionally%252C%2520validation%250Ain%2520the%2520Gazebo%2520simulator%2520confirms%2520its%2520practical%2520applicability%2520to%2520robotic%250Aplatforms.%2520A%2520source%2520code%2520is%2520available%2520at%250Ahttp%253A//github.com/PathPlanning/MPPI-Collision-Avoidance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.20293v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Uncertainty-Aware%20Multi-Agent%20Collision%20Avoidance%20with%0A%20%20Model%20Predictive%20Path%20Integral&entry.906535625=Stepan%20Dergachev%20and%20Konstantin%20Yakovlev&entry.1292438233=%20%20Decentralized%20multi-agent%20navigation%20under%20uncertainty%20is%20a%20complex%20task%20that%0Aarises%20in%20numerous%20robotic%20applications.%20It%20requires%20collision%20avoidance%0Astrategies%20that%20account%20for%20both%20kinematic%20constraints%2C%20sensing%20and%20action%0Aexecution%20noise.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20that%20integrates%20the%0AModel%20Predictive%20Path%20Integral%20%28MPPI%29%20with%20a%20probabilistic%20adaptation%20of%0AOptimal%20Reciprocal%20Collision%20Avoidance.%20Our%20method%20ensures%20safe%20and%20efficient%0Amulti-agent%20navigation%20by%20incorporating%20probabilistic%20safety%20constraints%0Adirectly%20into%20the%20MPPI%20sampling%20process%20via%20a%20Second-Order%20Cone%20Programming%0Aformulation.%20This%20approach%20enables%20agents%20to%20operate%20independently%20using%20local%0Anoisy%20observations%20while%20maintaining%20safety%20guarantees.%20We%20validate%20our%0Aalgorithm%20through%20extensive%20simulations%20with%20differential-drive%20robots%20and%0Abenchmark%20it%20against%20state-of-the-art%20methods%2C%20including%20ORCA-DD%20and%20B-UAVC.%0AResults%20demonstrate%20that%20our%20approach%20outperforms%20them%20while%20achieving%20high%0Asuccess%20rates%2C%20even%20in%20densely%20populated%20environments.%20Additionally%2C%20validation%0Ain%20the%20Gazebo%20simulator%20confirms%20its%20practical%20applicability%20to%20robotic%0Aplatforms.%20A%20source%20code%20is%20available%20at%0Ahttp%3A//github.com/PathPlanning/MPPI-Collision-Avoidance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.20293v2&entry.124074799=Read"},
{"title": "PLMP -- Point-Line Minimal Problems for Projective SfM", "author": "Kim Kiehn and Albin Ahlb\u00e4ck and Kathl\u00e9n Kohn", "abstract": "  We completely classify all minimal problems for Structure-from-Motion (SfM)\nwhere arrangements of points and lines are fully observed by multiple\nuncalibrated pinhole cameras. We find 291 minimal problems, 73 of which have\nunique solutions and can thus be solved linearly. Two of the linear problems\nallow an arbitrary number of views, while all other minimal problems have at\nmost 9 cameras. All minimal problems have at most 7 points and at most 12\nlines. We compute the number of solutions of each minimal problem, as this\ngives a measurement of the problem's intrinsic difficulty, and find that these\nnumber are relatively low (e.g., when comparing with minimal problems for\ncalibrated cameras). Finally, by exploring stabilizer subgroups of\nsubarrangements, we develop a geometric and systematic way to 1) factorize\nminimal problems into smaller problems, 2) identify minimal problems in\nunderconstrained problems, and 3) formally prove non-minimality.\n", "link": "http://arxiv.org/abs/2503.04351v2", "date": "2025-07-31", "relevancy": 2.3605, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5045}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4565}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLMP%20--%20Point-Line%20Minimal%20Problems%20for%20Projective%20SfM&body=Title%3A%20PLMP%20--%20Point-Line%20Minimal%20Problems%20for%20Projective%20SfM%0AAuthor%3A%20Kim%20Kiehn%20and%20Albin%20Ahlb%C3%A4ck%20and%20Kathl%C3%A9n%20Kohn%0AAbstract%3A%20%20%20We%20completely%20classify%20all%20minimal%20problems%20for%20Structure-from-Motion%20%28SfM%29%0Awhere%20arrangements%20of%20points%20and%20lines%20are%20fully%20observed%20by%20multiple%0Auncalibrated%20pinhole%20cameras.%20We%20find%20291%20minimal%20problems%2C%2073%20of%20which%20have%0Aunique%20solutions%20and%20can%20thus%20be%20solved%20linearly.%20Two%20of%20the%20linear%20problems%0Aallow%20an%20arbitrary%20number%20of%20views%2C%20while%20all%20other%20minimal%20problems%20have%20at%0Amost%209%20cameras.%20All%20minimal%20problems%20have%20at%20most%207%20points%20and%20at%20most%2012%0Alines.%20We%20compute%20the%20number%20of%20solutions%20of%20each%20minimal%20problem%2C%20as%20this%0Agives%20a%20measurement%20of%20the%20problem%27s%20intrinsic%20difficulty%2C%20and%20find%20that%20these%0Anumber%20are%20relatively%20low%20%28e.g.%2C%20when%20comparing%20with%20minimal%20problems%20for%0Acalibrated%20cameras%29.%20Finally%2C%20by%20exploring%20stabilizer%20subgroups%20of%0Asubarrangements%2C%20we%20develop%20a%20geometric%20and%20systematic%20way%20to%201%29%20factorize%0Aminimal%20problems%20into%20smaller%20problems%2C%202%29%20identify%20minimal%20problems%20in%0Aunderconstrained%20problems%2C%20and%203%29%20formally%20prove%20non-minimality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04351v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLMP%2520--%2520Point-Line%2520Minimal%2520Problems%2520for%2520Projective%2520SfM%26entry.906535625%3DKim%2520Kiehn%2520and%2520Albin%2520Ahlb%25C3%25A4ck%2520and%2520Kathl%25C3%25A9n%2520Kohn%26entry.1292438233%3D%2520%2520We%2520completely%2520classify%2520all%2520minimal%2520problems%2520for%2520Structure-from-Motion%2520%2528SfM%2529%250Awhere%2520arrangements%2520of%2520points%2520and%2520lines%2520are%2520fully%2520observed%2520by%2520multiple%250Auncalibrated%2520pinhole%2520cameras.%2520We%2520find%2520291%2520minimal%2520problems%252C%252073%2520of%2520which%2520have%250Aunique%2520solutions%2520and%2520can%2520thus%2520be%2520solved%2520linearly.%2520Two%2520of%2520the%2520linear%2520problems%250Aallow%2520an%2520arbitrary%2520number%2520of%2520views%252C%2520while%2520all%2520other%2520minimal%2520problems%2520have%2520at%250Amost%25209%2520cameras.%2520All%2520minimal%2520problems%2520have%2520at%2520most%25207%2520points%2520and%2520at%2520most%252012%250Alines.%2520We%2520compute%2520the%2520number%2520of%2520solutions%2520of%2520each%2520minimal%2520problem%252C%2520as%2520this%250Agives%2520a%2520measurement%2520of%2520the%2520problem%2527s%2520intrinsic%2520difficulty%252C%2520and%2520find%2520that%2520these%250Anumber%2520are%2520relatively%2520low%2520%2528e.g.%252C%2520when%2520comparing%2520with%2520minimal%2520problems%2520for%250Acalibrated%2520cameras%2529.%2520Finally%252C%2520by%2520exploring%2520stabilizer%2520subgroups%2520of%250Asubarrangements%252C%2520we%2520develop%2520a%2520geometric%2520and%2520systematic%2520way%2520to%25201%2529%2520factorize%250Aminimal%2520problems%2520into%2520smaller%2520problems%252C%25202%2529%2520identify%2520minimal%2520problems%2520in%250Aunderconstrained%2520problems%252C%2520and%25203%2529%2520formally%2520prove%2520non-minimality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04351v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLMP%20--%20Point-Line%20Minimal%20Problems%20for%20Projective%20SfM&entry.906535625=Kim%20Kiehn%20and%20Albin%20Ahlb%C3%A4ck%20and%20Kathl%C3%A9n%20Kohn&entry.1292438233=%20%20We%20completely%20classify%20all%20minimal%20problems%20for%20Structure-from-Motion%20%28SfM%29%0Awhere%20arrangements%20of%20points%20and%20lines%20are%20fully%20observed%20by%20multiple%0Auncalibrated%20pinhole%20cameras.%20We%20find%20291%20minimal%20problems%2C%2073%20of%20which%20have%0Aunique%20solutions%20and%20can%20thus%20be%20solved%20linearly.%20Two%20of%20the%20linear%20problems%0Aallow%20an%20arbitrary%20number%20of%20views%2C%20while%20all%20other%20minimal%20problems%20have%20at%0Amost%209%20cameras.%20All%20minimal%20problems%20have%20at%20most%207%20points%20and%20at%20most%2012%0Alines.%20We%20compute%20the%20number%20of%20solutions%20of%20each%20minimal%20problem%2C%20as%20this%0Agives%20a%20measurement%20of%20the%20problem%27s%20intrinsic%20difficulty%2C%20and%20find%20that%20these%0Anumber%20are%20relatively%20low%20%28e.g.%2C%20when%20comparing%20with%20minimal%20problems%20for%0Acalibrated%20cameras%29.%20Finally%2C%20by%20exploring%20stabilizer%20subgroups%20of%0Asubarrangements%2C%20we%20develop%20a%20geometric%20and%20systematic%20way%20to%201%29%20factorize%0Aminimal%20problems%20into%20smaller%20problems%2C%202%29%20identify%20minimal%20problems%20in%0Aunderconstrained%20problems%2C%20and%203%29%20formally%20prove%20non-minimality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04351v2&entry.124074799=Read"},
{"title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global\n  3D Earth Observation", "author": "Camille Billouard and Dawa Derksen and Alexandre Constantin and Bruno Vallet", "abstract": "  Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.\n", "link": "http://arxiv.org/abs/2507.01631v2", "date": "2025-07-31", "relevancy": 2.3482, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6366}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5655}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tile%20and%20Slide%20%3A%20A%20New%20Framework%20for%20Scaling%20NeRF%20from%20Local%20to%20Global%0A%20%203D%20Earth%20Observation&body=Title%3A%20Tile%20and%20Slide%20%3A%20A%20New%20Framework%20for%20Scaling%20NeRF%20from%20Local%20to%20Global%0A%20%203D%20Earth%20Observation%0AAuthor%3A%20Camille%20Billouard%20and%20Dawa%20Derksen%20and%20Alexandre%20Constantin%20and%20Bruno%20Vallet%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20recently%20emerged%20as%20a%20paradigm%20for%203D%0Areconstruction%20from%20multiview%20satellite%20imagery.%20However%2C%20state-of-the-art%20NeRF%0Amethods%20are%20typically%20constrained%20to%20small%20scenes%20due%20to%20the%20memory%20footprint%0Aduring%20training%2C%20which%20we%20study%20in%20this%20paper.%20Previous%20work%20on%20large-scale%0ANeRFs%20palliate%20this%20by%20dividing%20the%20scene%20into%20NeRFs.%20This%20paper%20introduces%0ASnake-NeRF%2C%20a%20framework%20that%20scales%20to%20large%20scenes.%20Our%20out-of-core%20method%0Aeliminates%20the%20need%20to%20load%20all%20images%20and%20networks%20simultaneously%2C%20and%0Aoperates%20on%20a%20single%20device.%20We%20achieve%20this%20by%20dividing%20the%20region%20of%20interest%0Ainto%20NeRFs%20that%203D%20tile%20without%20overlap.%20Importantly%2C%20we%20crop%20the%20images%20with%0Aoverlap%20to%20ensure%20each%20NeRFs%20is%20trained%20with%20all%20the%20necessary%20pixels.%20We%0Aintroduce%20a%20novel%20%242%5Ctimes%202%24%203D%20tile%20progression%20strategy%20and%20segmented%0Asampler%2C%20which%20together%20prevent%203D%20reconstruction%20errors%20along%20the%20tile%20edges.%0AOur%20experiments%20conclude%20that%20large%20satellite%20images%20can%20effectively%20be%0Aprocessed%20with%20linear%20time%20complexity%2C%20on%20a%20single%20GPU%2C%20and%20without%20compromise%0Ain%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.01631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTile%2520and%2520Slide%2520%253A%2520A%2520New%2520Framework%2520for%2520Scaling%2520NeRF%2520from%2520Local%2520to%2520Global%250A%2520%25203D%2520Earth%2520Observation%26entry.906535625%3DCamille%2520Billouard%2520and%2520Dawa%2520Derksen%2520and%2520Alexandre%2520Constantin%2520and%2520Bruno%2520Vallet%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520recently%2520emerged%2520as%2520a%2520paradigm%2520for%25203D%250Areconstruction%2520from%2520multiview%2520satellite%2520imagery.%2520However%252C%2520state-of-the-art%2520NeRF%250Amethods%2520are%2520typically%2520constrained%2520to%2520small%2520scenes%2520due%2520to%2520the%2520memory%2520footprint%250Aduring%2520training%252C%2520which%2520we%2520study%2520in%2520this%2520paper.%2520Previous%2520work%2520on%2520large-scale%250ANeRFs%2520palliate%2520this%2520by%2520dividing%2520the%2520scene%2520into%2520NeRFs.%2520This%2520paper%2520introduces%250ASnake-NeRF%252C%2520a%2520framework%2520that%2520scales%2520to%2520large%2520scenes.%2520Our%2520out-of-core%2520method%250Aeliminates%2520the%2520need%2520to%2520load%2520all%2520images%2520and%2520networks%2520simultaneously%252C%2520and%250Aoperates%2520on%2520a%2520single%2520device.%2520We%2520achieve%2520this%2520by%2520dividing%2520the%2520region%2520of%2520interest%250Ainto%2520NeRFs%2520that%25203D%2520tile%2520without%2520overlap.%2520Importantly%252C%2520we%2520crop%2520the%2520images%2520with%250Aoverlap%2520to%2520ensure%2520each%2520NeRFs%2520is%2520trained%2520with%2520all%2520the%2520necessary%2520pixels.%2520We%250Aintroduce%2520a%2520novel%2520%25242%255Ctimes%25202%2524%25203D%2520tile%2520progression%2520strategy%2520and%2520segmented%250Asampler%252C%2520which%2520together%2520prevent%25203D%2520reconstruction%2520errors%2520along%2520the%2520tile%2520edges.%250AOur%2520experiments%2520conclude%2520that%2520large%2520satellite%2520images%2520can%2520effectively%2520be%250Aprocessed%2520with%2520linear%2520time%2520complexity%252C%2520on%2520a%2520single%2520GPU%252C%2520and%2520without%2520compromise%250Ain%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tile%20and%20Slide%20%3A%20A%20New%20Framework%20for%20Scaling%20NeRF%20from%20Local%20to%20Global%0A%20%203D%20Earth%20Observation&entry.906535625=Camille%20Billouard%20and%20Dawa%20Derksen%20and%20Alexandre%20Constantin%20and%20Bruno%20Vallet&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20recently%20emerged%20as%20a%20paradigm%20for%203D%0Areconstruction%20from%20multiview%20satellite%20imagery.%20However%2C%20state-of-the-art%20NeRF%0Amethods%20are%20typically%20constrained%20to%20small%20scenes%20due%20to%20the%20memory%20footprint%0Aduring%20training%2C%20which%20we%20study%20in%20this%20paper.%20Previous%20work%20on%20large-scale%0ANeRFs%20palliate%20this%20by%20dividing%20the%20scene%20into%20NeRFs.%20This%20paper%20introduces%0ASnake-NeRF%2C%20a%20framework%20that%20scales%20to%20large%20scenes.%20Our%20out-of-core%20method%0Aeliminates%20the%20need%20to%20load%20all%20images%20and%20networks%20simultaneously%2C%20and%0Aoperates%20on%20a%20single%20device.%20We%20achieve%20this%20by%20dividing%20the%20region%20of%20interest%0Ainto%20NeRFs%20that%203D%20tile%20without%20overlap.%20Importantly%2C%20we%20crop%20the%20images%20with%0Aoverlap%20to%20ensure%20each%20NeRFs%20is%20trained%20with%20all%20the%20necessary%20pixels.%20We%0Aintroduce%20a%20novel%20%242%5Ctimes%202%24%203D%20tile%20progression%20strategy%20and%20segmented%0Asampler%2C%20which%20together%20prevent%203D%20reconstruction%20errors%20along%20the%20tile%20edges.%0AOur%20experiments%20conclude%20that%20large%20satellite%20images%20can%20effectively%20be%0Aprocessed%20with%20linear%20time%20complexity%2C%20on%20a%20single%20GPU%2C%20and%20without%20compromise%0Ain%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.01631v2&entry.124074799=Read"},
{"title": "Learning to Align and Refine: A Foundation-to-Diffusion Framework for\n  Occlusion-Robust Two-Hand Reconstruction", "author": "Gaoge Han and Yongkang Cheng and Zhe Chen and Shaoli Huang and Tongliang Liu", "abstract": "  Two-hand reconstruction from monocular images faces persistent challenges due\nto complex and dynamic hand postures and occlusions, causing significant\ndifficulty in achieving plausible interaction alignment. Existing approaches\nstruggle with such alignment issues, often resulting in misalignment and\npenetration artifacts. To tackle this, we propose a dual-stage\nFoundation-to-Diffusion framework that precisely align 2D prior guidance from\nvision foundation models and diffusion-based generative 3D interaction\nrefinement to achieve occlusion-robust two-hand reconstruction. First, we\nintroduce a lightweight fusion alignment encoder that aligns fused multimodal\n2D priors like key points, segmentation maps, and depth cues from vision\nfoundation models during training. This provides robust structured guidance,\nfurther enabling efficient inference without heavy foundation model encoders at\ntest time while maintaining high reconstruction accuracy. Second, we implement\na two-hand diffusion model explicitly trained to convert interpenetrated 3D\nposes into plausible, penetration-free counterparts. Through collision\ngradient-guided denoising, the model rectifies artifacts while preserving\nnatural spatial relationships between hands. Extensive evaluations demonstrate\nthat our method achieves state-of-the-art performance on InterHand2.6M, HIC,\nand FreiHAND datasets, significantly advancing occlusion handling and\ninteraction robustness. Our code will be publicly released.\n", "link": "http://arxiv.org/abs/2503.17788v2", "date": "2025-07-31", "relevancy": 2.3446, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5995}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.578}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Align%20and%20Refine%3A%20A%20Foundation-to-Diffusion%20Framework%20for%0A%20%20Occlusion-Robust%20Two-Hand%20Reconstruction&body=Title%3A%20Learning%20to%20Align%20and%20Refine%3A%20A%20Foundation-to-Diffusion%20Framework%20for%0A%20%20Occlusion-Robust%20Two-Hand%20Reconstruction%0AAuthor%3A%20Gaoge%20Han%20and%20Yongkang%20Cheng%20and%20Zhe%20Chen%20and%20Shaoli%20Huang%20and%20Tongliang%20Liu%0AAbstract%3A%20%20%20Two-hand%20reconstruction%20from%20monocular%20images%20faces%20persistent%20challenges%20due%0Ato%20complex%20and%20dynamic%20hand%20postures%20and%20occlusions%2C%20causing%20significant%0Adifficulty%20in%20achieving%20plausible%20interaction%20alignment.%20Existing%20approaches%0Astruggle%20with%20such%20alignment%20issues%2C%20often%20resulting%20in%20misalignment%20and%0Apenetration%20artifacts.%20To%20tackle%20this%2C%20we%20propose%20a%20dual-stage%0AFoundation-to-Diffusion%20framework%20that%20precisely%20align%202D%20prior%20guidance%20from%0Avision%20foundation%20models%20and%20diffusion-based%20generative%203D%20interaction%0Arefinement%20to%20achieve%20occlusion-robust%20two-hand%20reconstruction.%20First%2C%20we%0Aintroduce%20a%20lightweight%20fusion%20alignment%20encoder%20that%20aligns%20fused%20multimodal%0A2D%20priors%20like%20key%20points%2C%20segmentation%20maps%2C%20and%20depth%20cues%20from%20vision%0Afoundation%20models%20during%20training.%20This%20provides%20robust%20structured%20guidance%2C%0Afurther%20enabling%20efficient%20inference%20without%20heavy%20foundation%20model%20encoders%20at%0Atest%20time%20while%20maintaining%20high%20reconstruction%20accuracy.%20Second%2C%20we%20implement%0Aa%20two-hand%20diffusion%20model%20explicitly%20trained%20to%20convert%20interpenetrated%203D%0Aposes%20into%20plausible%2C%20penetration-free%20counterparts.%20Through%20collision%0Agradient-guided%20denoising%2C%20the%20model%20rectifies%20artifacts%20while%20preserving%0Anatural%20spatial%20relationships%20between%20hands.%20Extensive%20evaluations%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20performance%20on%20InterHand2.6M%2C%20HIC%2C%0Aand%20FreiHAND%20datasets%2C%20significantly%20advancing%20occlusion%20handling%20and%0Ainteraction%20robustness.%20Our%20code%20will%20be%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17788v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Align%2520and%2520Refine%253A%2520A%2520Foundation-to-Diffusion%2520Framework%2520for%250A%2520%2520Occlusion-Robust%2520Two-Hand%2520Reconstruction%26entry.906535625%3DGaoge%2520Han%2520and%2520Yongkang%2520Cheng%2520and%2520Zhe%2520Chen%2520and%2520Shaoli%2520Huang%2520and%2520Tongliang%2520Liu%26entry.1292438233%3D%2520%2520Two-hand%2520reconstruction%2520from%2520monocular%2520images%2520faces%2520persistent%2520challenges%2520due%250Ato%2520complex%2520and%2520dynamic%2520hand%2520postures%2520and%2520occlusions%252C%2520causing%2520significant%250Adifficulty%2520in%2520achieving%2520plausible%2520interaction%2520alignment.%2520Existing%2520approaches%250Astruggle%2520with%2520such%2520alignment%2520issues%252C%2520often%2520resulting%2520in%2520misalignment%2520and%250Apenetration%2520artifacts.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520a%2520dual-stage%250AFoundation-to-Diffusion%2520framework%2520that%2520precisely%2520align%25202D%2520prior%2520guidance%2520from%250Avision%2520foundation%2520models%2520and%2520diffusion-based%2520generative%25203D%2520interaction%250Arefinement%2520to%2520achieve%2520occlusion-robust%2520two-hand%2520reconstruction.%2520First%252C%2520we%250Aintroduce%2520a%2520lightweight%2520fusion%2520alignment%2520encoder%2520that%2520aligns%2520fused%2520multimodal%250A2D%2520priors%2520like%2520key%2520points%252C%2520segmentation%2520maps%252C%2520and%2520depth%2520cues%2520from%2520vision%250Afoundation%2520models%2520during%2520training.%2520This%2520provides%2520robust%2520structured%2520guidance%252C%250Afurther%2520enabling%2520efficient%2520inference%2520without%2520heavy%2520foundation%2520model%2520encoders%2520at%250Atest%2520time%2520while%2520maintaining%2520high%2520reconstruction%2520accuracy.%2520Second%252C%2520we%2520implement%250Aa%2520two-hand%2520diffusion%2520model%2520explicitly%2520trained%2520to%2520convert%2520interpenetrated%25203D%250Aposes%2520into%2520plausible%252C%2520penetration-free%2520counterparts.%2520Through%2520collision%250Agradient-guided%2520denoising%252C%2520the%2520model%2520rectifies%2520artifacts%2520while%2520preserving%250Anatural%2520spatial%2520relationships%2520between%2520hands.%2520Extensive%2520evaluations%2520demonstrate%250Athat%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520InterHand2.6M%252C%2520HIC%252C%250Aand%2520FreiHAND%2520datasets%252C%2520significantly%2520advancing%2520occlusion%2520handling%2520and%250Ainteraction%2520robustness.%2520Our%2520code%2520will%2520be%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17788v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Align%20and%20Refine%3A%20A%20Foundation-to-Diffusion%20Framework%20for%0A%20%20Occlusion-Robust%20Two-Hand%20Reconstruction&entry.906535625=Gaoge%20Han%20and%20Yongkang%20Cheng%20and%20Zhe%20Chen%20and%20Shaoli%20Huang%20and%20Tongliang%20Liu&entry.1292438233=%20%20Two-hand%20reconstruction%20from%20monocular%20images%20faces%20persistent%20challenges%20due%0Ato%20complex%20and%20dynamic%20hand%20postures%20and%20occlusions%2C%20causing%20significant%0Adifficulty%20in%20achieving%20plausible%20interaction%20alignment.%20Existing%20approaches%0Astruggle%20with%20such%20alignment%20issues%2C%20often%20resulting%20in%20misalignment%20and%0Apenetration%20artifacts.%20To%20tackle%20this%2C%20we%20propose%20a%20dual-stage%0AFoundation-to-Diffusion%20framework%20that%20precisely%20align%202D%20prior%20guidance%20from%0Avision%20foundation%20models%20and%20diffusion-based%20generative%203D%20interaction%0Arefinement%20to%20achieve%20occlusion-robust%20two-hand%20reconstruction.%20First%2C%20we%0Aintroduce%20a%20lightweight%20fusion%20alignment%20encoder%20that%20aligns%20fused%20multimodal%0A2D%20priors%20like%20key%20points%2C%20segmentation%20maps%2C%20and%20depth%20cues%20from%20vision%0Afoundation%20models%20during%20training.%20This%20provides%20robust%20structured%20guidance%2C%0Afurther%20enabling%20efficient%20inference%20without%20heavy%20foundation%20model%20encoders%20at%0Atest%20time%20while%20maintaining%20high%20reconstruction%20accuracy.%20Second%2C%20we%20implement%0Aa%20two-hand%20diffusion%20model%20explicitly%20trained%20to%20convert%20interpenetrated%203D%0Aposes%20into%20plausible%2C%20penetration-free%20counterparts.%20Through%20collision%0Agradient-guided%20denoising%2C%20the%20model%20rectifies%20artifacts%20while%20preserving%0Anatural%20spatial%20relationships%20between%20hands.%20Extensive%20evaluations%20demonstrate%0Athat%20our%20method%20achieves%20state-of-the-art%20performance%20on%20InterHand2.6M%2C%20HIC%2C%0Aand%20FreiHAND%20datasets%2C%20significantly%20advancing%20occlusion%20handling%20and%0Ainteraction%20robustness.%20Our%20code%20will%20be%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17788v2&entry.124074799=Read"},
{"title": "One Look is Enough: Seamless Patchwise Refinement for Zero-Shot\n  Monocular Depth Estimation on High-Resolution Images", "author": "Byeongjun Kwon and Munchurl Kim", "abstract": "  Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches, resulting in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluations on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrate that our PRO can be seamlessly integrated into existing depth\nestimation models.\n", "link": "http://arxiv.org/abs/2503.22351v3", "date": "2025-07-31", "relevancy": 2.3281, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5882}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Look%20is%20Enough%3A%20Seamless%20Patchwise%20Refinement%20for%20Zero-Shot%0A%20%20Monocular%20Depth%20Estimation%20on%20High-Resolution%20Images&body=Title%3A%20One%20Look%20is%20Enough%3A%20Seamless%20Patchwise%20Refinement%20for%20Zero-Shot%0A%20%20Monocular%20Depth%20Estimation%20on%20High-Resolution%20Images%0AAuthor%3A%20Byeongjun%20Kwon%20and%20Munchurl%20Kim%0AAbstract%3A%20%20%20Zero-shot%20depth%20estimation%20%28DE%29%20models%20exhibit%20strong%20generalization%0Aperformance%20as%20they%20are%20trained%20on%20large-scale%20datasets.%20However%2C%20existing%0Amodels%20struggle%20with%20high-resolution%20images%20due%20to%20the%20discrepancy%20in%20image%0Aresolutions%20of%20training%20%28with%20smaller%20resolutions%29%20and%20inference%20%28for%20high%0Aresolutions%29.%20Processing%20them%20at%20full%20resolution%20leads%20to%20decreased%20estimation%0Aaccuracy%20on%20depth%20with%20tremendous%20memory%20consumption%2C%20while%20downsampling%20to%20the%0Atraining%20resolution%20results%20in%20blurred%20edges%20in%20the%20estimated%20depth%20images.%0APrevailing%20high-resolution%20depth%20estimation%20methods%20adopt%20a%20patch-based%0Aapproach%2C%20which%20introduces%20depth%20discontinuity%20issues%20when%20reassembling%20the%0Aestimated%20depth%20patches%2C%20resulting%20in%20test-time%20inefficiency.%20Additionally%2C%20to%0Aobtain%20fine-grained%20depth%20details%2C%20these%20methods%20rely%20on%20synthetic%20datasets%20due%0Ato%20the%20real-world%20sparse%20ground%20truth%20depth%2C%20leading%20to%20poor%20generalizability.%0ATo%20tackle%20these%20limitations%2C%20we%20propose%20Patch%20Refine%20Once%20%28PRO%29%2C%20an%20efficient%0Aand%20generalizable%20tile-based%20framework.%20Our%20PRO%20consists%20of%20two%20key%20components%3A%0A%28i%29%20Grouped%20Patch%20Consistency%20Training%20that%20enhances%20test-time%20efficiency%20while%0Amitigating%20the%20depth%20discontinuity%20problem%20by%20jointly%20processing%20four%0Aoverlapping%20patches%20and%20enforcing%20a%20consistency%20loss%20on%20their%20overlapping%0Aregions%20within%20a%20single%20backpropagation%20step%2C%20and%20%28ii%29%20Bias%20Free%20Masking%20that%0Aprevents%20the%20DE%20models%20from%20overfitting%20to%20dataset-specific%20biases%2C%20enabling%0Abetter%20generalization%20to%20real-world%20datasets%20even%20after%20training%20on%20synthetic%0Adata.%20Zero-shot%20evaluations%20on%20Booster%2C%20ETH3D%2C%20Middlebury%202014%2C%20and%20NuScenes%0Ademonstrate%20that%20our%20PRO%20can%20be%20seamlessly%20integrated%20into%20existing%20depth%0Aestimation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.22351v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Look%2520is%2520Enough%253A%2520Seamless%2520Patchwise%2520Refinement%2520for%2520Zero-Shot%250A%2520%2520Monocular%2520Depth%2520Estimation%2520on%2520High-Resolution%2520Images%26entry.906535625%3DByeongjun%2520Kwon%2520and%2520Munchurl%2520Kim%26entry.1292438233%3D%2520%2520Zero-shot%2520depth%2520estimation%2520%2528DE%2529%2520models%2520exhibit%2520strong%2520generalization%250Aperformance%2520as%2520they%2520are%2520trained%2520on%2520large-scale%2520datasets.%2520However%252C%2520existing%250Amodels%2520struggle%2520with%2520high-resolution%2520images%2520due%2520to%2520the%2520discrepancy%2520in%2520image%250Aresolutions%2520of%2520training%2520%2528with%2520smaller%2520resolutions%2529%2520and%2520inference%2520%2528for%2520high%250Aresolutions%2529.%2520Processing%2520them%2520at%2520full%2520resolution%2520leads%2520to%2520decreased%2520estimation%250Aaccuracy%2520on%2520depth%2520with%2520tremendous%2520memory%2520consumption%252C%2520while%2520downsampling%2520to%2520the%250Atraining%2520resolution%2520results%2520in%2520blurred%2520edges%2520in%2520the%2520estimated%2520depth%2520images.%250APrevailing%2520high-resolution%2520depth%2520estimation%2520methods%2520adopt%2520a%2520patch-based%250Aapproach%252C%2520which%2520introduces%2520depth%2520discontinuity%2520issues%2520when%2520reassembling%2520the%250Aestimated%2520depth%2520patches%252C%2520resulting%2520in%2520test-time%2520inefficiency.%2520Additionally%252C%2520to%250Aobtain%2520fine-grained%2520depth%2520details%252C%2520these%2520methods%2520rely%2520on%2520synthetic%2520datasets%2520due%250Ato%2520the%2520real-world%2520sparse%2520ground%2520truth%2520depth%252C%2520leading%2520to%2520poor%2520generalizability.%250ATo%2520tackle%2520these%2520limitations%252C%2520we%2520propose%2520Patch%2520Refine%2520Once%2520%2528PRO%2529%252C%2520an%2520efficient%250Aand%2520generalizable%2520tile-based%2520framework.%2520Our%2520PRO%2520consists%2520of%2520two%2520key%2520components%253A%250A%2528i%2529%2520Grouped%2520Patch%2520Consistency%2520Training%2520that%2520enhances%2520test-time%2520efficiency%2520while%250Amitigating%2520the%2520depth%2520discontinuity%2520problem%2520by%2520jointly%2520processing%2520four%250Aoverlapping%2520patches%2520and%2520enforcing%2520a%2520consistency%2520loss%2520on%2520their%2520overlapping%250Aregions%2520within%2520a%2520single%2520backpropagation%2520step%252C%2520and%2520%2528ii%2529%2520Bias%2520Free%2520Masking%2520that%250Aprevents%2520the%2520DE%2520models%2520from%2520overfitting%2520to%2520dataset-specific%2520biases%252C%2520enabling%250Abetter%2520generalization%2520to%2520real-world%2520datasets%2520even%2520after%2520training%2520on%2520synthetic%250Adata.%2520Zero-shot%2520evaluations%2520on%2520Booster%252C%2520ETH3D%252C%2520Middlebury%25202014%252C%2520and%2520NuScenes%250Ademonstrate%2520that%2520our%2520PRO%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%2520depth%250Aestimation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.22351v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Look%20is%20Enough%3A%20Seamless%20Patchwise%20Refinement%20for%20Zero-Shot%0A%20%20Monocular%20Depth%20Estimation%20on%20High-Resolution%20Images&entry.906535625=Byeongjun%20Kwon%20and%20Munchurl%20Kim&entry.1292438233=%20%20Zero-shot%20depth%20estimation%20%28DE%29%20models%20exhibit%20strong%20generalization%0Aperformance%20as%20they%20are%20trained%20on%20large-scale%20datasets.%20However%2C%20existing%0Amodels%20struggle%20with%20high-resolution%20images%20due%20to%20the%20discrepancy%20in%20image%0Aresolutions%20of%20training%20%28with%20smaller%20resolutions%29%20and%20inference%20%28for%20high%0Aresolutions%29.%20Processing%20them%20at%20full%20resolution%20leads%20to%20decreased%20estimation%0Aaccuracy%20on%20depth%20with%20tremendous%20memory%20consumption%2C%20while%20downsampling%20to%20the%0Atraining%20resolution%20results%20in%20blurred%20edges%20in%20the%20estimated%20depth%20images.%0APrevailing%20high-resolution%20depth%20estimation%20methods%20adopt%20a%20patch-based%0Aapproach%2C%20which%20introduces%20depth%20discontinuity%20issues%20when%20reassembling%20the%0Aestimated%20depth%20patches%2C%20resulting%20in%20test-time%20inefficiency.%20Additionally%2C%20to%0Aobtain%20fine-grained%20depth%20details%2C%20these%20methods%20rely%20on%20synthetic%20datasets%20due%0Ato%20the%20real-world%20sparse%20ground%20truth%20depth%2C%20leading%20to%20poor%20generalizability.%0ATo%20tackle%20these%20limitations%2C%20we%20propose%20Patch%20Refine%20Once%20%28PRO%29%2C%20an%20efficient%0Aand%20generalizable%20tile-based%20framework.%20Our%20PRO%20consists%20of%20two%20key%20components%3A%0A%28i%29%20Grouped%20Patch%20Consistency%20Training%20that%20enhances%20test-time%20efficiency%20while%0Amitigating%20the%20depth%20discontinuity%20problem%20by%20jointly%20processing%20four%0Aoverlapping%20patches%20and%20enforcing%20a%20consistency%20loss%20on%20their%20overlapping%0Aregions%20within%20a%20single%20backpropagation%20step%2C%20and%20%28ii%29%20Bias%20Free%20Masking%20that%0Aprevents%20the%20DE%20models%20from%20overfitting%20to%20dataset-specific%20biases%2C%20enabling%0Abetter%20generalization%20to%20real-world%20datasets%20even%20after%20training%20on%20synthetic%0Adata.%20Zero-shot%20evaluations%20on%20Booster%2C%20ETH3D%2C%20Middlebury%202014%2C%20and%20NuScenes%0Ademonstrate%20that%20our%20PRO%20can%20be%20seamlessly%20integrated%20into%20existing%20depth%0Aestimation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.22351v3&entry.124074799=Read"},
{"title": "Generalizable Motion Policies through Keypoint Parameterization and\n  Transportation Maps", "author": "Giovanni Franzese and Ravi Prakash and Cosimo Della Santina and Jens Kober", "abstract": "  Learning from Interactive Demonstrations has revolutionized the way\nnon-expert humans teach robots. It is enough to kinesthetically move the robot\naround to teach pick-and-place, dressing, or cleaning policies. However, the\nmain challenge is correctly generalizing to novel situations, e.g., different\nsurfaces to clean or different arm postures to dress. This article proposes a\nnovel task parameterization and generalization to transport the original robot\npolicy, i.e., position, velocity, orientation, and stiffness. Unlike the state\nof the art, only a set of keypoints is tracked during the demonstration and the\nexecution, e.g., a point cloud of the surface to clean. We then propose to fit\na nonlinear transformation that would deform the space and then the original\npolicy using the paired source and target point sets. The use of function\napproximators like Gaussian Processes allows us to generalize, or transport,\nthe policy from every space location while estimating the uncertainty of the\nresulting policy due to the limited task keypoints and the reduced number of\ndemonstrations. We compare the algorithm's performance with state-of-the-art\ntask parameterization alternatives and analyze the effect of different function\napproximators. We also validated the algorithm on robot manipulation tasks,\ni.e., different posture arm dressing, different location product reshelving,\nand different shape surface cleaning.\n", "link": "http://arxiv.org/abs/2404.13458v2", "date": "2025-07-31", "relevancy": 2.3224, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6331}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5744}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Motion%20Policies%20through%20Keypoint%20Parameterization%20and%0A%20%20Transportation%20Maps&body=Title%3A%20Generalizable%20Motion%20Policies%20through%20Keypoint%20Parameterization%20and%0A%20%20Transportation%20Maps%0AAuthor%3A%20Giovanni%20Franzese%20and%20Ravi%20Prakash%20and%20Cosimo%20Della%20Santina%20and%20Jens%20Kober%0AAbstract%3A%20%20%20Learning%20from%20Interactive%20Demonstrations%20has%20revolutionized%20the%20way%0Anon-expert%20humans%20teach%20robots.%20It%20is%20enough%20to%20kinesthetically%20move%20the%20robot%0Aaround%20to%20teach%20pick-and-place%2C%20dressing%2C%20or%20cleaning%20policies.%20However%2C%20the%0Amain%20challenge%20is%20correctly%20generalizing%20to%20novel%20situations%2C%20e.g.%2C%20different%0Asurfaces%20to%20clean%20or%20different%20arm%20postures%20to%20dress.%20This%20article%20proposes%20a%0Anovel%20task%20parameterization%20and%20generalization%20to%20transport%20the%20original%20robot%0Apolicy%2C%20i.e.%2C%20position%2C%20velocity%2C%20orientation%2C%20and%20stiffness.%20Unlike%20the%20state%0Aof%20the%20art%2C%20only%20a%20set%20of%20keypoints%20is%20tracked%20during%20the%20demonstration%20and%20the%0Aexecution%2C%20e.g.%2C%20a%20point%20cloud%20of%20the%20surface%20to%20clean.%20We%20then%20propose%20to%20fit%0Aa%20nonlinear%20transformation%20that%20would%20deform%20the%20space%20and%20then%20the%20original%0Apolicy%20using%20the%20paired%20source%20and%20target%20point%20sets.%20The%20use%20of%20function%0Aapproximators%20like%20Gaussian%20Processes%20allows%20us%20to%20generalize%2C%20or%20transport%2C%0Athe%20policy%20from%20every%20space%20location%20while%20estimating%20the%20uncertainty%20of%20the%0Aresulting%20policy%20due%20to%20the%20limited%20task%20keypoints%20and%20the%20reduced%20number%20of%0Ademonstrations.%20We%20compare%20the%20algorithm%27s%20performance%20with%20state-of-the-art%0Atask%20parameterization%20alternatives%20and%20analyze%20the%20effect%20of%20different%20function%0Aapproximators.%20We%20also%20validated%20the%20algorithm%20on%20robot%20manipulation%20tasks%2C%0Ai.e.%2C%20different%20posture%20arm%20dressing%2C%20different%20location%20product%20reshelving%2C%0Aand%20different%20shape%20surface%20cleaning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13458v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Motion%2520Policies%2520through%2520Keypoint%2520Parameterization%2520and%250A%2520%2520Transportation%2520Maps%26entry.906535625%3DGiovanni%2520Franzese%2520and%2520Ravi%2520Prakash%2520and%2520Cosimo%2520Della%2520Santina%2520and%2520Jens%2520Kober%26entry.1292438233%3D%2520%2520Learning%2520from%2520Interactive%2520Demonstrations%2520has%2520revolutionized%2520the%2520way%250Anon-expert%2520humans%2520teach%2520robots.%2520It%2520is%2520enough%2520to%2520kinesthetically%2520move%2520the%2520robot%250Aaround%2520to%2520teach%2520pick-and-place%252C%2520dressing%252C%2520or%2520cleaning%2520policies.%2520However%252C%2520the%250Amain%2520challenge%2520is%2520correctly%2520generalizing%2520to%2520novel%2520situations%252C%2520e.g.%252C%2520different%250Asurfaces%2520to%2520clean%2520or%2520different%2520arm%2520postures%2520to%2520dress.%2520This%2520article%2520proposes%2520a%250Anovel%2520task%2520parameterization%2520and%2520generalization%2520to%2520transport%2520the%2520original%2520robot%250Apolicy%252C%2520i.e.%252C%2520position%252C%2520velocity%252C%2520orientation%252C%2520and%2520stiffness.%2520Unlike%2520the%2520state%250Aof%2520the%2520art%252C%2520only%2520a%2520set%2520of%2520keypoints%2520is%2520tracked%2520during%2520the%2520demonstration%2520and%2520the%250Aexecution%252C%2520e.g.%252C%2520a%2520point%2520cloud%2520of%2520the%2520surface%2520to%2520clean.%2520We%2520then%2520propose%2520to%2520fit%250Aa%2520nonlinear%2520transformation%2520that%2520would%2520deform%2520the%2520space%2520and%2520then%2520the%2520original%250Apolicy%2520using%2520the%2520paired%2520source%2520and%2520target%2520point%2520sets.%2520The%2520use%2520of%2520function%250Aapproximators%2520like%2520Gaussian%2520Processes%2520allows%2520us%2520to%2520generalize%252C%2520or%2520transport%252C%250Athe%2520policy%2520from%2520every%2520space%2520location%2520while%2520estimating%2520the%2520uncertainty%2520of%2520the%250Aresulting%2520policy%2520due%2520to%2520the%2520limited%2520task%2520keypoints%2520and%2520the%2520reduced%2520number%2520of%250Ademonstrations.%2520We%2520compare%2520the%2520algorithm%2527s%2520performance%2520with%2520state-of-the-art%250Atask%2520parameterization%2520alternatives%2520and%2520analyze%2520the%2520effect%2520of%2520different%2520function%250Aapproximators.%2520We%2520also%2520validated%2520the%2520algorithm%2520on%2520robot%2520manipulation%2520tasks%252C%250Ai.e.%252C%2520different%2520posture%2520arm%2520dressing%252C%2520different%2520location%2520product%2520reshelving%252C%250Aand%2520different%2520shape%2520surface%2520cleaning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13458v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Motion%20Policies%20through%20Keypoint%20Parameterization%20and%0A%20%20Transportation%20Maps&entry.906535625=Giovanni%20Franzese%20and%20Ravi%20Prakash%20and%20Cosimo%20Della%20Santina%20and%20Jens%20Kober&entry.1292438233=%20%20Learning%20from%20Interactive%20Demonstrations%20has%20revolutionized%20the%20way%0Anon-expert%20humans%20teach%20robots.%20It%20is%20enough%20to%20kinesthetically%20move%20the%20robot%0Aaround%20to%20teach%20pick-and-place%2C%20dressing%2C%20or%20cleaning%20policies.%20However%2C%20the%0Amain%20challenge%20is%20correctly%20generalizing%20to%20novel%20situations%2C%20e.g.%2C%20different%0Asurfaces%20to%20clean%20or%20different%20arm%20postures%20to%20dress.%20This%20article%20proposes%20a%0Anovel%20task%20parameterization%20and%20generalization%20to%20transport%20the%20original%20robot%0Apolicy%2C%20i.e.%2C%20position%2C%20velocity%2C%20orientation%2C%20and%20stiffness.%20Unlike%20the%20state%0Aof%20the%20art%2C%20only%20a%20set%20of%20keypoints%20is%20tracked%20during%20the%20demonstration%20and%20the%0Aexecution%2C%20e.g.%2C%20a%20point%20cloud%20of%20the%20surface%20to%20clean.%20We%20then%20propose%20to%20fit%0Aa%20nonlinear%20transformation%20that%20would%20deform%20the%20space%20and%20then%20the%20original%0Apolicy%20using%20the%20paired%20source%20and%20target%20point%20sets.%20The%20use%20of%20function%0Aapproximators%20like%20Gaussian%20Processes%20allows%20us%20to%20generalize%2C%20or%20transport%2C%0Athe%20policy%20from%20every%20space%20location%20while%20estimating%20the%20uncertainty%20of%20the%0Aresulting%20policy%20due%20to%20the%20limited%20task%20keypoints%20and%20the%20reduced%20number%20of%0Ademonstrations.%20We%20compare%20the%20algorithm%27s%20performance%20with%20state-of-the-art%0Atask%20parameterization%20alternatives%20and%20analyze%20the%20effect%20of%20different%20function%0Aapproximators.%20We%20also%20validated%20the%20algorithm%20on%20robot%20manipulation%20tasks%2C%0Ai.e.%2C%20different%20posture%20arm%20dressing%2C%20different%20location%20product%20reshelving%2C%0Aand%20different%20shape%20surface%20cleaning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13458v2&entry.124074799=Read"},
{"title": "A ZeNN architecture to avoid the Gaussian trap", "author": "Lu\u00eds Carvalho and Jo\u00e3o L. Costa and Jos\u00e9 Mour\u00e3o and Gon\u00e7alo Oliveira", "abstract": "  We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order\nto overcome several shortcomings of standard multi-layer perceptrons (MLPs).\nNamely, in the large width limit, MLPs are non-parametric, they do not have a\nwell-defined pointwise limit, they lose non-Gaussian attributes and become\nunable to perform feature learning; moreover, finite width MLPs perform poorly\nin learning high frequencies. The new ZeNN architecture is inspired by three\nsimple principles from harmonic analysis:\n  i) Enumerate the perceptons and introduce a non-learnable weight to enforce\nconvergence;\n  ii) Introduce a scaling (or frequency) factor;\n  iii) Choose activation functions that lead to near orthogonal systems.\n  We will show that these ideas allow us to fix the referred shortcomings of\nMLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they\nexhibit a rich asymptotic structure beyond Gaussianity, and perform feature\nlearning. Moreover, when appropriate activation functions are chosen, (finite\nwidth) ZeNNs excel at learning high-frequency features of functions with low\ndimensional domains.\n", "link": "http://arxiv.org/abs/2505.20553v2", "date": "2025-07-31", "relevancy": 2.3192, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4831}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4651}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20ZeNN%20architecture%20to%20avoid%20the%20Gaussian%20trap&body=Title%3A%20A%20ZeNN%20architecture%20to%20avoid%20the%20Gaussian%20trap%0AAuthor%3A%20Lu%C3%ADs%20Carvalho%20and%20Jo%C3%A3o%20L.%20Costa%20and%20Jos%C3%A9%20Mour%C3%A3o%20and%20Gon%C3%A7alo%20Oliveira%0AAbstract%3A%20%20%20We%20propose%20a%20new%20simple%20architecture%2C%20Zeta%20Neural%20Networks%20%28ZeNNs%29%2C%20in%20order%0Ato%20overcome%20several%20shortcomings%20of%20standard%20multi-layer%20perceptrons%20%28MLPs%29.%0ANamely%2C%20in%20the%20large%20width%20limit%2C%20MLPs%20are%20non-parametric%2C%20they%20do%20not%20have%20a%0Awell-defined%20pointwise%20limit%2C%20they%20lose%20non-Gaussian%20attributes%20and%20become%0Aunable%20to%20perform%20feature%20learning%3B%20moreover%2C%20finite%20width%20MLPs%20perform%20poorly%0Ain%20learning%20high%20frequencies.%20The%20new%20ZeNN%20architecture%20is%20inspired%20by%20three%0Asimple%20principles%20from%20harmonic%20analysis%3A%0A%20%20i%29%20Enumerate%20the%20perceptons%20and%20introduce%20a%20non-learnable%20weight%20to%20enforce%0Aconvergence%3B%0A%20%20ii%29%20Introduce%20a%20scaling%20%28or%20frequency%29%20factor%3B%0A%20%20iii%29%20Choose%20activation%20functions%20that%20lead%20to%20near%20orthogonal%20systems.%0A%20%20We%20will%20show%20that%20these%20ideas%20allow%20us%20to%20fix%20the%20referred%20shortcomings%20of%0AMLPs.%20In%20fact%2C%20in%20the%20infinite%20width%20limit%2C%20ZeNNs%20converge%20pointwise%2C%20they%0Aexhibit%20a%20rich%20asymptotic%20structure%20beyond%20Gaussianity%2C%20and%20perform%20feature%0Alearning.%20Moreover%2C%20when%20appropriate%20activation%20functions%20are%20chosen%2C%20%28finite%0Awidth%29%20ZeNNs%20excel%20at%20learning%20high-frequency%20features%20of%20functions%20with%20low%0Adimensional%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520ZeNN%2520architecture%2520to%2520avoid%2520the%2520Gaussian%2520trap%26entry.906535625%3DLu%25C3%25ADs%2520Carvalho%2520and%2520Jo%25C3%25A3o%2520L.%2520Costa%2520and%2520Jos%25C3%25A9%2520Mour%25C3%25A3o%2520and%2520Gon%25C3%25A7alo%2520Oliveira%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520simple%2520architecture%252C%2520Zeta%2520Neural%2520Networks%2520%2528ZeNNs%2529%252C%2520in%2520order%250Ato%2520overcome%2520several%2520shortcomings%2520of%2520standard%2520multi-layer%2520perceptrons%2520%2528MLPs%2529.%250ANamely%252C%2520in%2520the%2520large%2520width%2520limit%252C%2520MLPs%2520are%2520non-parametric%252C%2520they%2520do%2520not%2520have%2520a%250Awell-defined%2520pointwise%2520limit%252C%2520they%2520lose%2520non-Gaussian%2520attributes%2520and%2520become%250Aunable%2520to%2520perform%2520feature%2520learning%253B%2520moreover%252C%2520finite%2520width%2520MLPs%2520perform%2520poorly%250Ain%2520learning%2520high%2520frequencies.%2520The%2520new%2520ZeNN%2520architecture%2520is%2520inspired%2520by%2520three%250Asimple%2520principles%2520from%2520harmonic%2520analysis%253A%250A%2520%2520i%2529%2520Enumerate%2520the%2520perceptons%2520and%2520introduce%2520a%2520non-learnable%2520weight%2520to%2520enforce%250Aconvergence%253B%250A%2520%2520ii%2529%2520Introduce%2520a%2520scaling%2520%2528or%2520frequency%2529%2520factor%253B%250A%2520%2520iii%2529%2520Choose%2520activation%2520functions%2520that%2520lead%2520to%2520near%2520orthogonal%2520systems.%250A%2520%2520We%2520will%2520show%2520that%2520these%2520ideas%2520allow%2520us%2520to%2520fix%2520the%2520referred%2520shortcomings%2520of%250AMLPs.%2520In%2520fact%252C%2520in%2520the%2520infinite%2520width%2520limit%252C%2520ZeNNs%2520converge%2520pointwise%252C%2520they%250Aexhibit%2520a%2520rich%2520asymptotic%2520structure%2520beyond%2520Gaussianity%252C%2520and%2520perform%2520feature%250Alearning.%2520Moreover%252C%2520when%2520appropriate%2520activation%2520functions%2520are%2520chosen%252C%2520%2528finite%250Awidth%2529%2520ZeNNs%2520excel%2520at%2520learning%2520high-frequency%2520features%2520of%2520functions%2520with%2520low%250Adimensional%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20ZeNN%20architecture%20to%20avoid%20the%20Gaussian%20trap&entry.906535625=Lu%C3%ADs%20Carvalho%20and%20Jo%C3%A3o%20L.%20Costa%20and%20Jos%C3%A9%20Mour%C3%A3o%20and%20Gon%C3%A7alo%20Oliveira&entry.1292438233=%20%20We%20propose%20a%20new%20simple%20architecture%2C%20Zeta%20Neural%20Networks%20%28ZeNNs%29%2C%20in%20order%0Ato%20overcome%20several%20shortcomings%20of%20standard%20multi-layer%20perceptrons%20%28MLPs%29.%0ANamely%2C%20in%20the%20large%20width%20limit%2C%20MLPs%20are%20non-parametric%2C%20they%20do%20not%20have%20a%0Awell-defined%20pointwise%20limit%2C%20they%20lose%20non-Gaussian%20attributes%20and%20become%0Aunable%20to%20perform%20feature%20learning%3B%20moreover%2C%20finite%20width%20MLPs%20perform%20poorly%0Ain%20learning%20high%20frequencies.%20The%20new%20ZeNN%20architecture%20is%20inspired%20by%20three%0Asimple%20principles%20from%20harmonic%20analysis%3A%0A%20%20i%29%20Enumerate%20the%20perceptons%20and%20introduce%20a%20non-learnable%20weight%20to%20enforce%0Aconvergence%3B%0A%20%20ii%29%20Introduce%20a%20scaling%20%28or%20frequency%29%20factor%3B%0A%20%20iii%29%20Choose%20activation%20functions%20that%20lead%20to%20near%20orthogonal%20systems.%0A%20%20We%20will%20show%20that%20these%20ideas%20allow%20us%20to%20fix%20the%20referred%20shortcomings%20of%0AMLPs.%20In%20fact%2C%20in%20the%20infinite%20width%20limit%2C%20ZeNNs%20converge%20pointwise%2C%20they%0Aexhibit%20a%20rich%20asymptotic%20structure%20beyond%20Gaussianity%2C%20and%20perform%20feature%0Alearning.%20Moreover%2C%20when%20appropriate%20activation%20functions%20are%20chosen%2C%20%28finite%0Awidth%29%20ZeNNs%20excel%20at%20learning%20high-frequency%20features%20of%20functions%20with%20low%0Adimensional%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20553v2&entry.124074799=Read"},
{"title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM\n  for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "author": "Yewei Huang and John McConnell and Xi Lin and Brendan Englot", "abstract": "  We present DRACo-SLAM2, a distributed SLAM framework for underwater robot\nteams equipped with multibeam imaging sonar. This framework improves upon the\noriginal DRACo-SLAM by introducing a novel representation of sonar maps as\nobject graphs and utilizing object graph matching to achieve time-efficient\ninter-robot loop closure detection without relying on prior geometric\ninformation. To better-accommodate the needs and characteristics of underwater\nscan matching, we propose incremental Group-wise Consistent Measurement Set\nMaximization (GCM), a modification of Pairwise Consistent Measurement Set\nMaximization (PCM), which effectively handles scenarios where nearby\ninter-robot loop closures share similar registration errors. The proposed\napproach is validated through extensive comparative analyses on simulated and\nreal-world datasets.\n", "link": "http://arxiv.org/abs/2507.23629v1", "date": "2025-07-31", "relevancy": 2.3099, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.61}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5658}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRACo-SLAM2%3A%20Distributed%20Robust%20Acoustic%20Communication-efficient%20SLAM%0A%20%20for%20Imaging%20Sonar%20EquippedUnderwater%20Robot%20Teams%20with%20Object%20Graph%20Matching&body=Title%3A%20DRACo-SLAM2%3A%20Distributed%20Robust%20Acoustic%20Communication-efficient%20SLAM%0A%20%20for%20Imaging%20Sonar%20EquippedUnderwater%20Robot%20Teams%20with%20Object%20Graph%20Matching%0AAuthor%3A%20Yewei%20Huang%20and%20John%20McConnell%20and%20Xi%20Lin%20and%20Brendan%20Englot%0AAbstract%3A%20%20%20We%20present%20DRACo-SLAM2%2C%20a%20distributed%20SLAM%20framework%20for%20underwater%20robot%0Ateams%20equipped%20with%20multibeam%20imaging%20sonar.%20This%20framework%20improves%20upon%20the%0Aoriginal%20DRACo-SLAM%20by%20introducing%20a%20novel%20representation%20of%20sonar%20maps%20as%0Aobject%20graphs%20and%20utilizing%20object%20graph%20matching%20to%20achieve%20time-efficient%0Ainter-robot%20loop%20closure%20detection%20without%20relying%20on%20prior%20geometric%0Ainformation.%20To%20better-accommodate%20the%20needs%20and%20characteristics%20of%20underwater%0Ascan%20matching%2C%20we%20propose%20incremental%20Group-wise%20Consistent%20Measurement%20Set%0AMaximization%20%28GCM%29%2C%20a%20modification%20of%20Pairwise%20Consistent%20Measurement%20Set%0AMaximization%20%28PCM%29%2C%20which%20effectively%20handles%20scenarios%20where%20nearby%0Ainter-robot%20loop%20closures%20share%20similar%20registration%20errors.%20The%20proposed%0Aapproach%20is%20validated%20through%20extensive%20comparative%20analyses%20on%20simulated%20and%0Areal-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRACo-SLAM2%253A%2520Distributed%2520Robust%2520Acoustic%2520Communication-efficient%2520SLAM%250A%2520%2520for%2520Imaging%2520Sonar%2520EquippedUnderwater%2520Robot%2520Teams%2520with%2520Object%2520Graph%2520Matching%26entry.906535625%3DYewei%2520Huang%2520and%2520John%2520McConnell%2520and%2520Xi%2520Lin%2520and%2520Brendan%2520Englot%26entry.1292438233%3D%2520%2520We%2520present%2520DRACo-SLAM2%252C%2520a%2520distributed%2520SLAM%2520framework%2520for%2520underwater%2520robot%250Ateams%2520equipped%2520with%2520multibeam%2520imaging%2520sonar.%2520This%2520framework%2520improves%2520upon%2520the%250Aoriginal%2520DRACo-SLAM%2520by%2520introducing%2520a%2520novel%2520representation%2520of%2520sonar%2520maps%2520as%250Aobject%2520graphs%2520and%2520utilizing%2520object%2520graph%2520matching%2520to%2520achieve%2520time-efficient%250Ainter-robot%2520loop%2520closure%2520detection%2520without%2520relying%2520on%2520prior%2520geometric%250Ainformation.%2520To%2520better-accommodate%2520the%2520needs%2520and%2520characteristics%2520of%2520underwater%250Ascan%2520matching%252C%2520we%2520propose%2520incremental%2520Group-wise%2520Consistent%2520Measurement%2520Set%250AMaximization%2520%2528GCM%2529%252C%2520a%2520modification%2520of%2520Pairwise%2520Consistent%2520Measurement%2520Set%250AMaximization%2520%2528PCM%2529%252C%2520which%2520effectively%2520handles%2520scenarios%2520where%2520nearby%250Ainter-robot%2520loop%2520closures%2520share%2520similar%2520registration%2520errors.%2520The%2520proposed%250Aapproach%2520is%2520validated%2520through%2520extensive%2520comparative%2520analyses%2520on%2520simulated%2520and%250Areal-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRACo-SLAM2%3A%20Distributed%20Robust%20Acoustic%20Communication-efficient%20SLAM%0A%20%20for%20Imaging%20Sonar%20EquippedUnderwater%20Robot%20Teams%20with%20Object%20Graph%20Matching&entry.906535625=Yewei%20Huang%20and%20John%20McConnell%20and%20Xi%20Lin%20and%20Brendan%20Englot&entry.1292438233=%20%20We%20present%20DRACo-SLAM2%2C%20a%20distributed%20SLAM%20framework%20for%20underwater%20robot%0Ateams%20equipped%20with%20multibeam%20imaging%20sonar.%20This%20framework%20improves%20upon%20the%0Aoriginal%20DRACo-SLAM%20by%20introducing%20a%20novel%20representation%20of%20sonar%20maps%20as%0Aobject%20graphs%20and%20utilizing%20object%20graph%20matching%20to%20achieve%20time-efficient%0Ainter-robot%20loop%20closure%20detection%20without%20relying%20on%20prior%20geometric%0Ainformation.%20To%20better-accommodate%20the%20needs%20and%20characteristics%20of%20underwater%0Ascan%20matching%2C%20we%20propose%20incremental%20Group-wise%20Consistent%20Measurement%20Set%0AMaximization%20%28GCM%29%2C%20a%20modification%20of%20Pairwise%20Consistent%20Measurement%20Set%0AMaximization%20%28PCM%29%2C%20which%20effectively%20handles%20scenarios%20where%20nearby%0Ainter-robot%20loop%20closures%20share%20similar%20registration%20errors.%20The%20proposed%0Aapproach%20is%20validated%20through%20extensive%20comparative%20analyses%20on%20simulated%20and%0Areal-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23629v1&entry.124074799=Read"},
{"title": "OmniTraj: Pre-Training on Heterogeneous Data for Adaptive and Zero-Shot\n  Human Trajectory Prediction", "author": "Yang Gao and Po-Chien Luan and Kaouther Messaoud and Lan Feng and Alexandre Alahi", "abstract": "  While large-scale pre-training has advanced human trajectory prediction, a\ncritical challenge remains: zero-shot transfer to unseen dataset with varying\ntemporal dynamics. State-of-the-art pre-trained models often require\nfine-tuning to adapt to new datasets with different frame rates or observation\nhorizons, limiting their scalability and practical utility. In this work, we\nsystematically investigate this limitation and propose a robust solution. We\nfirst demonstrate that existing data-aware discrete models struggle when\ntransferred to new scenarios with shifted temporal setups. We then isolate the\ntemporal generalization from dataset shift, revealing that a simple, explicit\nconditioning mechanism for temporal metadata is a highly effective solution.\nBased on this insight, we present OmniTraj, a Transformer-based model\npre-trained on a large-scale, heterogeneous dataset. Our experiments show that\nexplicitly conditioning on the frame rate enables OmniTraj to achieve\nstate-of-the-art zero-shot transfer performance, reducing prediction error by\nover 70\\% in challenging cross-setup scenarios. After fine-tuning, OmniTraj\nachieves state-of-the-art results on four datasets, including NBA, JTA,\nWorldPose, and ETH-UCY. The code is publicly available:\nhttps://github.com/vita-epfl/omnitraj\n", "link": "http://arxiv.org/abs/2507.23657v1", "date": "2025-07-31", "relevancy": 2.2861, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5724}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5721}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniTraj%3A%20Pre-Training%20on%20Heterogeneous%20Data%20for%20Adaptive%20and%20Zero-Shot%0A%20%20Human%20Trajectory%20Prediction&body=Title%3A%20OmniTraj%3A%20Pre-Training%20on%20Heterogeneous%20Data%20for%20Adaptive%20and%20Zero-Shot%0A%20%20Human%20Trajectory%20Prediction%0AAuthor%3A%20Yang%20Gao%20and%20Po-Chien%20Luan%20and%20Kaouther%20Messaoud%20and%20Lan%20Feng%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20While%20large-scale%20pre-training%20has%20advanced%20human%20trajectory%20prediction%2C%20a%0Acritical%20challenge%20remains%3A%20zero-shot%20transfer%20to%20unseen%20dataset%20with%20varying%0Atemporal%20dynamics.%20State-of-the-art%20pre-trained%20models%20often%20require%0Afine-tuning%20to%20adapt%20to%20new%20datasets%20with%20different%20frame%20rates%20or%20observation%0Ahorizons%2C%20limiting%20their%20scalability%20and%20practical%20utility.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20this%20limitation%20and%20propose%20a%20robust%20solution.%20We%0Afirst%20demonstrate%20that%20existing%20data-aware%20discrete%20models%20struggle%20when%0Atransferred%20to%20new%20scenarios%20with%20shifted%20temporal%20setups.%20We%20then%20isolate%20the%0Atemporal%20generalization%20from%20dataset%20shift%2C%20revealing%20that%20a%20simple%2C%20explicit%0Aconditioning%20mechanism%20for%20temporal%20metadata%20is%20a%20highly%20effective%20solution.%0ABased%20on%20this%20insight%2C%20we%20present%20OmniTraj%2C%20a%20Transformer-based%20model%0Apre-trained%20on%20a%20large-scale%2C%20heterogeneous%20dataset.%20Our%20experiments%20show%20that%0Aexplicitly%20conditioning%20on%20the%20frame%20rate%20enables%20OmniTraj%20to%20achieve%0Astate-of-the-art%20zero-shot%20transfer%20performance%2C%20reducing%20prediction%20error%20by%0Aover%2070%5C%25%20in%20challenging%20cross-setup%20scenarios.%20After%20fine-tuning%2C%20OmniTraj%0Aachieves%20state-of-the-art%20results%20on%20four%20datasets%2C%20including%20NBA%2C%20JTA%2C%0AWorldPose%2C%20and%20ETH-UCY.%20The%20code%20is%20publicly%20available%3A%0Ahttps%3A//github.com/vita-epfl/omnitraj%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniTraj%253A%2520Pre-Training%2520on%2520Heterogeneous%2520Data%2520for%2520Adaptive%2520and%2520Zero-Shot%250A%2520%2520Human%2520Trajectory%2520Prediction%26entry.906535625%3DYang%2520Gao%2520and%2520Po-Chien%2520Luan%2520and%2520Kaouther%2520Messaoud%2520and%2520Lan%2520Feng%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3D%2520%2520While%2520large-scale%2520pre-training%2520has%2520advanced%2520human%2520trajectory%2520prediction%252C%2520a%250Acritical%2520challenge%2520remains%253A%2520zero-shot%2520transfer%2520to%2520unseen%2520dataset%2520with%2520varying%250Atemporal%2520dynamics.%2520State-of-the-art%2520pre-trained%2520models%2520often%2520require%250Afine-tuning%2520to%2520adapt%2520to%2520new%2520datasets%2520with%2520different%2520frame%2520rates%2520or%2520observation%250Ahorizons%252C%2520limiting%2520their%2520scalability%2520and%2520practical%2520utility.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520investigate%2520this%2520limitation%2520and%2520propose%2520a%2520robust%2520solution.%2520We%250Afirst%2520demonstrate%2520that%2520existing%2520data-aware%2520discrete%2520models%2520struggle%2520when%250Atransferred%2520to%2520new%2520scenarios%2520with%2520shifted%2520temporal%2520setups.%2520We%2520then%2520isolate%2520the%250Atemporal%2520generalization%2520from%2520dataset%2520shift%252C%2520revealing%2520that%2520a%2520simple%252C%2520explicit%250Aconditioning%2520mechanism%2520for%2520temporal%2520metadata%2520is%2520a%2520highly%2520effective%2520solution.%250ABased%2520on%2520this%2520insight%252C%2520we%2520present%2520OmniTraj%252C%2520a%2520Transformer-based%2520model%250Apre-trained%2520on%2520a%2520large-scale%252C%2520heterogeneous%2520dataset.%2520Our%2520experiments%2520show%2520that%250Aexplicitly%2520conditioning%2520on%2520the%2520frame%2520rate%2520enables%2520OmniTraj%2520to%2520achieve%250Astate-of-the-art%2520zero-shot%2520transfer%2520performance%252C%2520reducing%2520prediction%2520error%2520by%250Aover%252070%255C%2525%2520in%2520challenging%2520cross-setup%2520scenarios.%2520After%2520fine-tuning%252C%2520OmniTraj%250Aachieves%2520state-of-the-art%2520results%2520on%2520four%2520datasets%252C%2520including%2520NBA%252C%2520JTA%252C%250AWorldPose%252C%2520and%2520ETH-UCY.%2520The%2520code%2520is%2520publicly%2520available%253A%250Ahttps%253A//github.com/vita-epfl/omnitraj%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniTraj%3A%20Pre-Training%20on%20Heterogeneous%20Data%20for%20Adaptive%20and%20Zero-Shot%0A%20%20Human%20Trajectory%20Prediction&entry.906535625=Yang%20Gao%20and%20Po-Chien%20Luan%20and%20Kaouther%20Messaoud%20and%20Lan%20Feng%20and%20Alexandre%20Alahi&entry.1292438233=%20%20While%20large-scale%20pre-training%20has%20advanced%20human%20trajectory%20prediction%2C%20a%0Acritical%20challenge%20remains%3A%20zero-shot%20transfer%20to%20unseen%20dataset%20with%20varying%0Atemporal%20dynamics.%20State-of-the-art%20pre-trained%20models%20often%20require%0Afine-tuning%20to%20adapt%20to%20new%20datasets%20with%20different%20frame%20rates%20or%20observation%0Ahorizons%2C%20limiting%20their%20scalability%20and%20practical%20utility.%20In%20this%20work%2C%20we%0Asystematically%20investigate%20this%20limitation%20and%20propose%20a%20robust%20solution.%20We%0Afirst%20demonstrate%20that%20existing%20data-aware%20discrete%20models%20struggle%20when%0Atransferred%20to%20new%20scenarios%20with%20shifted%20temporal%20setups.%20We%20then%20isolate%20the%0Atemporal%20generalization%20from%20dataset%20shift%2C%20revealing%20that%20a%20simple%2C%20explicit%0Aconditioning%20mechanism%20for%20temporal%20metadata%20is%20a%20highly%20effective%20solution.%0ABased%20on%20this%20insight%2C%20we%20present%20OmniTraj%2C%20a%20Transformer-based%20model%0Apre-trained%20on%20a%20large-scale%2C%20heterogeneous%20dataset.%20Our%20experiments%20show%20that%0Aexplicitly%20conditioning%20on%20the%20frame%20rate%20enables%20OmniTraj%20to%20achieve%0Astate-of-the-art%20zero-shot%20transfer%20performance%2C%20reducing%20prediction%20error%20by%0Aover%2070%5C%25%20in%20challenging%20cross-setup%20scenarios.%20After%20fine-tuning%2C%20OmniTraj%0Aachieves%20state-of-the-art%20results%20on%20four%20datasets%2C%20including%20NBA%2C%20JTA%2C%0AWorldPose%2C%20and%20ETH-UCY.%20The%20code%20is%20publicly%20available%3A%0Ahttps%3A//github.com/vita-epfl/omnitraj%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23657v1&entry.124074799=Read"},
{"title": "DiffuMatch: Category-Agnostic Spectral Diffusion Priors for Robust\n  Non-rigid Shape Matching", "author": "Emery Pierson and Lei Li and Angela Dai and Maks Ovsjanikov", "abstract": "  Deep functional maps have recently emerged as a powerful tool for solving\nnon-rigid shape correspondence tasks. Methods that use this approach combine\nthe power and flexibility of the functional map framework, with data-driven\nlearning for improved accuracy and generality. However, most existing methods\nin this area restrict the learning aspect only to the feature functions and\nstill rely on axiomatic modeling for formulating the training loss or for\nfunctional map regularization inside the networks. This limits both the\naccuracy and the applicability of the resulting approaches only to scenarios\nwhere assumptions of the axiomatic models hold. In this work, we show, for the\nfirst time, that both in-network regularization and functional map training can\nbe replaced with data-driven methods. For this, we first train a generative\nmodel of functional maps in the spectral domain using score-based generative\nmodeling, built from a large collection of high-quality maps. We then exploit\nthe resulting model to promote the structural properties of ground truth\nfunctional maps on new shape collections. Remarkably, we demonstrate that the\nlearned models are category-agnostic, and can fully replace commonly used\nstrategies such as enforcing Laplacian commutativity or orthogonality of\nfunctional maps. Our key technical contribution is a novel distillation\nstrategy from diffusion models in the spectral domain. Experiments demonstrate\nthat our learned regularization leads to better results than axiomatic\napproaches for zero-shot non-rigid shape matching. Our code is available at:\nhttps://github.com/daidedou/diffumatch/\n", "link": "http://arxiv.org/abs/2507.23715v1", "date": "2025-07-31", "relevancy": 2.2711, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5727}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5651}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffuMatch%3A%20Category-Agnostic%20Spectral%20Diffusion%20Priors%20for%20Robust%0A%20%20Non-rigid%20Shape%20Matching&body=Title%3A%20DiffuMatch%3A%20Category-Agnostic%20Spectral%20Diffusion%20Priors%20for%20Robust%0A%20%20Non-rigid%20Shape%20Matching%0AAuthor%3A%20Emery%20Pierson%20and%20Lei%20Li%20and%20Angela%20Dai%20and%20Maks%20Ovsjanikov%0AAbstract%3A%20%20%20Deep%20functional%20maps%20have%20recently%20emerged%20as%20a%20powerful%20tool%20for%20solving%0Anon-rigid%20shape%20correspondence%20tasks.%20Methods%20that%20use%20this%20approach%20combine%0Athe%20power%20and%20flexibility%20of%20the%20functional%20map%20framework%2C%20with%20data-driven%0Alearning%20for%20improved%20accuracy%20and%20generality.%20However%2C%20most%20existing%20methods%0Ain%20this%20area%20restrict%20the%20learning%20aspect%20only%20to%20the%20feature%20functions%20and%0Astill%20rely%20on%20axiomatic%20modeling%20for%20formulating%20the%20training%20loss%20or%20for%0Afunctional%20map%20regularization%20inside%20the%20networks.%20This%20limits%20both%20the%0Aaccuracy%20and%20the%20applicability%20of%20the%20resulting%20approaches%20only%20to%20scenarios%0Awhere%20assumptions%20of%20the%20axiomatic%20models%20hold.%20In%20this%20work%2C%20we%20show%2C%20for%20the%0Afirst%20time%2C%20that%20both%20in-network%20regularization%20and%20functional%20map%20training%20can%0Abe%20replaced%20with%20data-driven%20methods.%20For%20this%2C%20we%20first%20train%20a%20generative%0Amodel%20of%20functional%20maps%20in%20the%20spectral%20domain%20using%20score-based%20generative%0Amodeling%2C%20built%20from%20a%20large%20collection%20of%20high-quality%20maps.%20We%20then%20exploit%0Athe%20resulting%20model%20to%20promote%20the%20structural%20properties%20of%20ground%20truth%0Afunctional%20maps%20on%20new%20shape%20collections.%20Remarkably%2C%20we%20demonstrate%20that%20the%0Alearned%20models%20are%20category-agnostic%2C%20and%20can%20fully%20replace%20commonly%20used%0Astrategies%20such%20as%20enforcing%20Laplacian%20commutativity%20or%20orthogonality%20of%0Afunctional%20maps.%20Our%20key%20technical%20contribution%20is%20a%20novel%20distillation%0Astrategy%20from%20diffusion%20models%20in%20the%20spectral%20domain.%20Experiments%20demonstrate%0Athat%20our%20learned%20regularization%20leads%20to%20better%20results%20than%20axiomatic%0Aapproaches%20for%20zero-shot%20non-rigid%20shape%20matching.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/daidedou/diffumatch/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffuMatch%253A%2520Category-Agnostic%2520Spectral%2520Diffusion%2520Priors%2520for%2520Robust%250A%2520%2520Non-rigid%2520Shape%2520Matching%26entry.906535625%3DEmery%2520Pierson%2520and%2520Lei%2520Li%2520and%2520Angela%2520Dai%2520and%2520Maks%2520Ovsjanikov%26entry.1292438233%3D%2520%2520Deep%2520functional%2520maps%2520have%2520recently%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520solving%250Anon-rigid%2520shape%2520correspondence%2520tasks.%2520Methods%2520that%2520use%2520this%2520approach%2520combine%250Athe%2520power%2520and%2520flexibility%2520of%2520the%2520functional%2520map%2520framework%252C%2520with%2520data-driven%250Alearning%2520for%2520improved%2520accuracy%2520and%2520generality.%2520However%252C%2520most%2520existing%2520methods%250Ain%2520this%2520area%2520restrict%2520the%2520learning%2520aspect%2520only%2520to%2520the%2520feature%2520functions%2520and%250Astill%2520rely%2520on%2520axiomatic%2520modeling%2520for%2520formulating%2520the%2520training%2520loss%2520or%2520for%250Afunctional%2520map%2520regularization%2520inside%2520the%2520networks.%2520This%2520limits%2520both%2520the%250Aaccuracy%2520and%2520the%2520applicability%2520of%2520the%2520resulting%2520approaches%2520only%2520to%2520scenarios%250Awhere%2520assumptions%2520of%2520the%2520axiomatic%2520models%2520hold.%2520In%2520this%2520work%252C%2520we%2520show%252C%2520for%2520the%250Afirst%2520time%252C%2520that%2520both%2520in-network%2520regularization%2520and%2520functional%2520map%2520training%2520can%250Abe%2520replaced%2520with%2520data-driven%2520methods.%2520For%2520this%252C%2520we%2520first%2520train%2520a%2520generative%250Amodel%2520of%2520functional%2520maps%2520in%2520the%2520spectral%2520domain%2520using%2520score-based%2520generative%250Amodeling%252C%2520built%2520from%2520a%2520large%2520collection%2520of%2520high-quality%2520maps.%2520We%2520then%2520exploit%250Athe%2520resulting%2520model%2520to%2520promote%2520the%2520structural%2520properties%2520of%2520ground%2520truth%250Afunctional%2520maps%2520on%2520new%2520shape%2520collections.%2520Remarkably%252C%2520we%2520demonstrate%2520that%2520the%250Alearned%2520models%2520are%2520category-agnostic%252C%2520and%2520can%2520fully%2520replace%2520commonly%2520used%250Astrategies%2520such%2520as%2520enforcing%2520Laplacian%2520commutativity%2520or%2520orthogonality%2520of%250Afunctional%2520maps.%2520Our%2520key%2520technical%2520contribution%2520is%2520a%2520novel%2520distillation%250Astrategy%2520from%2520diffusion%2520models%2520in%2520the%2520spectral%2520domain.%2520Experiments%2520demonstrate%250Athat%2520our%2520learned%2520regularization%2520leads%2520to%2520better%2520results%2520than%2520axiomatic%250Aapproaches%2520for%2520zero-shot%2520non-rigid%2520shape%2520matching.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/daidedou/diffumatch/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffuMatch%3A%20Category-Agnostic%20Spectral%20Diffusion%20Priors%20for%20Robust%0A%20%20Non-rigid%20Shape%20Matching&entry.906535625=Emery%20Pierson%20and%20Lei%20Li%20and%20Angela%20Dai%20and%20Maks%20Ovsjanikov&entry.1292438233=%20%20Deep%20functional%20maps%20have%20recently%20emerged%20as%20a%20powerful%20tool%20for%20solving%0Anon-rigid%20shape%20correspondence%20tasks.%20Methods%20that%20use%20this%20approach%20combine%0Athe%20power%20and%20flexibility%20of%20the%20functional%20map%20framework%2C%20with%20data-driven%0Alearning%20for%20improved%20accuracy%20and%20generality.%20However%2C%20most%20existing%20methods%0Ain%20this%20area%20restrict%20the%20learning%20aspect%20only%20to%20the%20feature%20functions%20and%0Astill%20rely%20on%20axiomatic%20modeling%20for%20formulating%20the%20training%20loss%20or%20for%0Afunctional%20map%20regularization%20inside%20the%20networks.%20This%20limits%20both%20the%0Aaccuracy%20and%20the%20applicability%20of%20the%20resulting%20approaches%20only%20to%20scenarios%0Awhere%20assumptions%20of%20the%20axiomatic%20models%20hold.%20In%20this%20work%2C%20we%20show%2C%20for%20the%0Afirst%20time%2C%20that%20both%20in-network%20regularization%20and%20functional%20map%20training%20can%0Abe%20replaced%20with%20data-driven%20methods.%20For%20this%2C%20we%20first%20train%20a%20generative%0Amodel%20of%20functional%20maps%20in%20the%20spectral%20domain%20using%20score-based%20generative%0Amodeling%2C%20built%20from%20a%20large%20collection%20of%20high-quality%20maps.%20We%20then%20exploit%0Athe%20resulting%20model%20to%20promote%20the%20structural%20properties%20of%20ground%20truth%0Afunctional%20maps%20on%20new%20shape%20collections.%20Remarkably%2C%20we%20demonstrate%20that%20the%0Alearned%20models%20are%20category-agnostic%2C%20and%20can%20fully%20replace%20commonly%20used%0Astrategies%20such%20as%20enforcing%20Laplacian%20commutativity%20or%20orthogonality%20of%0Afunctional%20maps.%20Our%20key%20technical%20contribution%20is%20a%20novel%20distillation%0Astrategy%20from%20diffusion%20models%20in%20the%20spectral%20domain.%20Experiments%20demonstrate%0Athat%20our%20learned%20regularization%20leads%20to%20better%20results%20than%20axiomatic%0Aapproaches%20for%20zero-shot%20non-rigid%20shape%20matching.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/daidedou/diffumatch/%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23715v1&entry.124074799=Read"},
{"title": "Splits! A Flexible Dataset and Evaluation Framework for Sociocultural\n  Linguistic Investigation", "author": "Eylon Caplan and Tania Chakraborty and Dan Goldwasser", "abstract": "  Variation in language use, shaped by speakers' sociocultural background and\nspecific context of use, offers a rich lens into cultural perspectives, values,\nand opinions. However, the computational study of these Sociocultural\nLinguistic Phenomena (SLP) has often been limited to bespoke analyses of\nspecific groups or topics, hindering the pace of scientific discovery. To\naddress this, we introduce Splits!, a 9.7 million-post dataset from Reddit\ndesigned for systematic and flexible research. The dataset contains posts from\nover 53,000 users across 6 demographic groups, organized into 89 discussion\ntopics to enable comparative analysis. We validate Splits! via\nself-identification and by successfully replicating several known SLPs from\nexisting literature. We complement this dataset with a framework that leverages\nefficient retrieval methods to rapidly validate potential SLPs (PSLPs) by\nautomatically evaluating whether a given hypothesis is supported by our data.\nCrucially, to distinguish between novel and obvious insights, the framework\nincorporates a human-validated measure of a hypothesis's ``unexpectedness.'' We\ndemonstrate that the two-stage process reduces the number of statistically\nsignificant findings requiring manual inspection by a factor of 1.5-1.8x,\nstreamlining the discovery of promising phenomena for further investigation.\n", "link": "http://arxiv.org/abs/2504.04640v2", "date": "2025-07-31", "relevancy": 2.2686, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.458}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.458}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splits%21%20A%20Flexible%20Dataset%20and%20Evaluation%20Framework%20for%20Sociocultural%0A%20%20Linguistic%20Investigation&body=Title%3A%20Splits%21%20A%20Flexible%20Dataset%20and%20Evaluation%20Framework%20for%20Sociocultural%0A%20%20Linguistic%20Investigation%0AAuthor%3A%20Eylon%20Caplan%20and%20Tania%20Chakraborty%20and%20Dan%20Goldwasser%0AAbstract%3A%20%20%20Variation%20in%20language%20use%2C%20shaped%20by%20speakers%27%20sociocultural%20background%20and%0Aspecific%20context%20of%20use%2C%20offers%20a%20rich%20lens%20into%20cultural%20perspectives%2C%20values%2C%0Aand%20opinions.%20However%2C%20the%20computational%20study%20of%20these%20Sociocultural%0ALinguistic%20Phenomena%20%28SLP%29%20has%20often%20been%20limited%20to%20bespoke%20analyses%20of%0Aspecific%20groups%20or%20topics%2C%20hindering%20the%20pace%20of%20scientific%20discovery.%20To%0Aaddress%20this%2C%20we%20introduce%20Splits%21%2C%20a%209.7%20million-post%20dataset%20from%20Reddit%0Adesigned%20for%20systematic%20and%20flexible%20research.%20The%20dataset%20contains%20posts%20from%0Aover%2053%2C000%20users%20across%206%20demographic%20groups%2C%20organized%20into%2089%20discussion%0Atopics%20to%20enable%20comparative%20analysis.%20We%20validate%20Splits%21%20via%0Aself-identification%20and%20by%20successfully%20replicating%20several%20known%20SLPs%20from%0Aexisting%20literature.%20We%20complement%20this%20dataset%20with%20a%20framework%20that%20leverages%0Aefficient%20retrieval%20methods%20to%20rapidly%20validate%20potential%20SLPs%20%28PSLPs%29%20by%0Aautomatically%20evaluating%20whether%20a%20given%20hypothesis%20is%20supported%20by%20our%20data.%0ACrucially%2C%20to%20distinguish%20between%20novel%20and%20obvious%20insights%2C%20the%20framework%0Aincorporates%20a%20human-validated%20measure%20of%20a%20hypothesis%27s%20%60%60unexpectedness.%27%27%20We%0Ademonstrate%20that%20the%20two-stage%20process%20reduces%20the%20number%20of%20statistically%0Asignificant%20findings%20requiring%20manual%20inspection%20by%20a%20factor%20of%201.5-1.8x%2C%0Astreamlining%20the%20discovery%20of%20promising%20phenomena%20for%20further%20investigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04640v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplits%2521%2520A%2520Flexible%2520Dataset%2520and%2520Evaluation%2520Framework%2520for%2520Sociocultural%250A%2520%2520Linguistic%2520Investigation%26entry.906535625%3DEylon%2520Caplan%2520and%2520Tania%2520Chakraborty%2520and%2520Dan%2520Goldwasser%26entry.1292438233%3D%2520%2520Variation%2520in%2520language%2520use%252C%2520shaped%2520by%2520speakers%2527%2520sociocultural%2520background%2520and%250Aspecific%2520context%2520of%2520use%252C%2520offers%2520a%2520rich%2520lens%2520into%2520cultural%2520perspectives%252C%2520values%252C%250Aand%2520opinions.%2520However%252C%2520the%2520computational%2520study%2520of%2520these%2520Sociocultural%250ALinguistic%2520Phenomena%2520%2528SLP%2529%2520has%2520often%2520been%2520limited%2520to%2520bespoke%2520analyses%2520of%250Aspecific%2520groups%2520or%2520topics%252C%2520hindering%2520the%2520pace%2520of%2520scientific%2520discovery.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520Splits%2521%252C%2520a%25209.7%2520million-post%2520dataset%2520from%2520Reddit%250Adesigned%2520for%2520systematic%2520and%2520flexible%2520research.%2520The%2520dataset%2520contains%2520posts%2520from%250Aover%252053%252C000%2520users%2520across%25206%2520demographic%2520groups%252C%2520organized%2520into%252089%2520discussion%250Atopics%2520to%2520enable%2520comparative%2520analysis.%2520We%2520validate%2520Splits%2521%2520via%250Aself-identification%2520and%2520by%2520successfully%2520replicating%2520several%2520known%2520SLPs%2520from%250Aexisting%2520literature.%2520We%2520complement%2520this%2520dataset%2520with%2520a%2520framework%2520that%2520leverages%250Aefficient%2520retrieval%2520methods%2520to%2520rapidly%2520validate%2520potential%2520SLPs%2520%2528PSLPs%2529%2520by%250Aautomatically%2520evaluating%2520whether%2520a%2520given%2520hypothesis%2520is%2520supported%2520by%2520our%2520data.%250ACrucially%252C%2520to%2520distinguish%2520between%2520novel%2520and%2520obvious%2520insights%252C%2520the%2520framework%250Aincorporates%2520a%2520human-validated%2520measure%2520of%2520a%2520hypothesis%2527s%2520%2560%2560unexpectedness.%2527%2527%2520We%250Ademonstrate%2520that%2520the%2520two-stage%2520process%2520reduces%2520the%2520number%2520of%2520statistically%250Asignificant%2520findings%2520requiring%2520manual%2520inspection%2520by%2520a%2520factor%2520of%25201.5-1.8x%252C%250Astreamlining%2520the%2520discovery%2520of%2520promising%2520phenomena%2520for%2520further%2520investigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04640v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splits%21%20A%20Flexible%20Dataset%20and%20Evaluation%20Framework%20for%20Sociocultural%0A%20%20Linguistic%20Investigation&entry.906535625=Eylon%20Caplan%20and%20Tania%20Chakraborty%20and%20Dan%20Goldwasser&entry.1292438233=%20%20Variation%20in%20language%20use%2C%20shaped%20by%20speakers%27%20sociocultural%20background%20and%0Aspecific%20context%20of%20use%2C%20offers%20a%20rich%20lens%20into%20cultural%20perspectives%2C%20values%2C%0Aand%20opinions.%20However%2C%20the%20computational%20study%20of%20these%20Sociocultural%0ALinguistic%20Phenomena%20%28SLP%29%20has%20often%20been%20limited%20to%20bespoke%20analyses%20of%0Aspecific%20groups%20or%20topics%2C%20hindering%20the%20pace%20of%20scientific%20discovery.%20To%0Aaddress%20this%2C%20we%20introduce%20Splits%21%2C%20a%209.7%20million-post%20dataset%20from%20Reddit%0Adesigned%20for%20systematic%20and%20flexible%20research.%20The%20dataset%20contains%20posts%20from%0Aover%2053%2C000%20users%20across%206%20demographic%20groups%2C%20organized%20into%2089%20discussion%0Atopics%20to%20enable%20comparative%20analysis.%20We%20validate%20Splits%21%20via%0Aself-identification%20and%20by%20successfully%20replicating%20several%20known%20SLPs%20from%0Aexisting%20literature.%20We%20complement%20this%20dataset%20with%20a%20framework%20that%20leverages%0Aefficient%20retrieval%20methods%20to%20rapidly%20validate%20potential%20SLPs%20%28PSLPs%29%20by%0Aautomatically%20evaluating%20whether%20a%20given%20hypothesis%20is%20supported%20by%20our%20data.%0ACrucially%2C%20to%20distinguish%20between%20novel%20and%20obvious%20insights%2C%20the%20framework%0Aincorporates%20a%20human-validated%20measure%20of%20a%20hypothesis%27s%20%60%60unexpectedness.%27%27%20We%0Ademonstrate%20that%20the%20two-stage%20process%20reduces%20the%20number%20of%20statistically%0Asignificant%20findings%20requiring%20manual%20inspection%20by%20a%20factor%20of%201.5-1.8x%2C%0Astreamlining%20the%20discovery%20of%20promising%20phenomena%20for%20further%20investigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04640v2&entry.124074799=Read"},
{"title": "DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for\n  Geometric Structure Preservation", "author": "Yuchen Zhou and Yan Luo and Xiangang Wang and Xingjian Gu and Mingzhou Lu", "abstract": "  Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring\nthe performance of autonomous driving (AD) systems. However, many current\nmethods focus on high accuracy at the expense of real-time processing needs. To\naddress this challenge of balancing accuracy and inference speed, we propose a\ndirectional pure 2D approach. Our method involves slicing 3D voxel features to\npreserve complete vertical geometric information. This strategy compensates for\nthe loss of height cues in Bird's-Eye View (BEV) representations, thereby\nmaintaining the integrity of the 3D geometric structure. By employing a\ndirectional attention mechanism, we efficiently extract geometric features from\ndifferent orientations, striking a balance between accuracy and computational\nefficiency. Experimental results highlight the significant advantages of our\napproach for autonomous driving. On the Occ3D-nuScenes, the proposed method\nachieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively\nbalancing accuracy and efficiency. In simulations on edge devices, the\ninference speed reaches 14.8 FPS, further demonstrating the method's\napplicability for real-time deployment in resource-constrained environments.\n", "link": "http://arxiv.org/abs/2507.23599v1", "date": "2025-07-31", "relevancy": 2.2533, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DA-Occ%3A%20Efficient%203D%20Voxel%20Occupancy%20Prediction%20via%20Directional%202D%20for%0A%20%20Geometric%20Structure%20Preservation&body=Title%3A%20DA-Occ%3A%20Efficient%203D%20Voxel%20Occupancy%20Prediction%20via%20Directional%202D%20for%0A%20%20Geometric%20Structure%20Preservation%0AAuthor%3A%20Yuchen%20Zhou%20and%20Yan%20Luo%20and%20Xiangang%20Wang%20and%20Xingjian%20Gu%20and%20Mingzhou%20Lu%0AAbstract%3A%20%20%20Efficient%20and%20high-accuracy%203D%20occupancy%20prediction%20is%20crucial%20for%20ensuring%0Athe%20performance%20of%20autonomous%20driving%20%28AD%29%20systems.%20However%2C%20many%20current%0Amethods%20focus%20on%20high%20accuracy%20at%20the%20expense%20of%20real-time%20processing%20needs.%20To%0Aaddress%20this%20challenge%20of%20balancing%20accuracy%20and%20inference%20speed%2C%20we%20propose%20a%0Adirectional%20pure%202D%20approach.%20Our%20method%20involves%20slicing%203D%20voxel%20features%20to%0Apreserve%20complete%20vertical%20geometric%20information.%20This%20strategy%20compensates%20for%0Athe%20loss%20of%20height%20cues%20in%20Bird%27s-Eye%20View%20%28BEV%29%20representations%2C%20thereby%0Amaintaining%20the%20integrity%20of%20the%203D%20geometric%20structure.%20By%20employing%20a%0Adirectional%20attention%20mechanism%2C%20we%20efficiently%20extract%20geometric%20features%20from%0Adifferent%20orientations%2C%20striking%20a%20balance%20between%20accuracy%20and%20computational%0Aefficiency.%20Experimental%20results%20highlight%20the%20significant%20advantages%20of%20our%0Aapproach%20for%20autonomous%20driving.%20On%20the%20Occ3D-nuScenes%2C%20the%20proposed%20method%0Aachieves%20an%20mIoU%20of%2039.3%25%20and%20an%20inference%20speed%20of%2027.7%20FPS%2C%20effectively%0Abalancing%20accuracy%20and%20efficiency.%20In%20simulations%20on%20edge%20devices%2C%20the%0Ainference%20speed%20reaches%2014.8%20FPS%2C%20further%20demonstrating%20the%20method%27s%0Aapplicability%20for%20real-time%20deployment%20in%20resource-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDA-Occ%253A%2520Efficient%25203D%2520Voxel%2520Occupancy%2520Prediction%2520via%2520Directional%25202D%2520for%250A%2520%2520Geometric%2520Structure%2520Preservation%26entry.906535625%3DYuchen%2520Zhou%2520and%2520Yan%2520Luo%2520and%2520Xiangang%2520Wang%2520and%2520Xingjian%2520Gu%2520and%2520Mingzhou%2520Lu%26entry.1292438233%3D%2520%2520Efficient%2520and%2520high-accuracy%25203D%2520occupancy%2520prediction%2520is%2520crucial%2520for%2520ensuring%250Athe%2520performance%2520of%2520autonomous%2520driving%2520%2528AD%2529%2520systems.%2520However%252C%2520many%2520current%250Amethods%2520focus%2520on%2520high%2520accuracy%2520at%2520the%2520expense%2520of%2520real-time%2520processing%2520needs.%2520To%250Aaddress%2520this%2520challenge%2520of%2520balancing%2520accuracy%2520and%2520inference%2520speed%252C%2520we%2520propose%2520a%250Adirectional%2520pure%25202D%2520approach.%2520Our%2520method%2520involves%2520slicing%25203D%2520voxel%2520features%2520to%250Apreserve%2520complete%2520vertical%2520geometric%2520information.%2520This%2520strategy%2520compensates%2520for%250Athe%2520loss%2520of%2520height%2520cues%2520in%2520Bird%2527s-Eye%2520View%2520%2528BEV%2529%2520representations%252C%2520thereby%250Amaintaining%2520the%2520integrity%2520of%2520the%25203D%2520geometric%2520structure.%2520By%2520employing%2520a%250Adirectional%2520attention%2520mechanism%252C%2520we%2520efficiently%2520extract%2520geometric%2520features%2520from%250Adifferent%2520orientations%252C%2520striking%2520a%2520balance%2520between%2520accuracy%2520and%2520computational%250Aefficiency.%2520Experimental%2520results%2520highlight%2520the%2520significant%2520advantages%2520of%2520our%250Aapproach%2520for%2520autonomous%2520driving.%2520On%2520the%2520Occ3D-nuScenes%252C%2520the%2520proposed%2520method%250Aachieves%2520an%2520mIoU%2520of%252039.3%2525%2520and%2520an%2520inference%2520speed%2520of%252027.7%2520FPS%252C%2520effectively%250Abalancing%2520accuracy%2520and%2520efficiency.%2520In%2520simulations%2520on%2520edge%2520devices%252C%2520the%250Ainference%2520speed%2520reaches%252014.8%2520FPS%252C%2520further%2520demonstrating%2520the%2520method%2527s%250Aapplicability%2520for%2520real-time%2520deployment%2520in%2520resource-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DA-Occ%3A%20Efficient%203D%20Voxel%20Occupancy%20Prediction%20via%20Directional%202D%20for%0A%20%20Geometric%20Structure%20Preservation&entry.906535625=Yuchen%20Zhou%20and%20Yan%20Luo%20and%20Xiangang%20Wang%20and%20Xingjian%20Gu%20and%20Mingzhou%20Lu&entry.1292438233=%20%20Efficient%20and%20high-accuracy%203D%20occupancy%20prediction%20is%20crucial%20for%20ensuring%0Athe%20performance%20of%20autonomous%20driving%20%28AD%29%20systems.%20However%2C%20many%20current%0Amethods%20focus%20on%20high%20accuracy%20at%20the%20expense%20of%20real-time%20processing%20needs.%20To%0Aaddress%20this%20challenge%20of%20balancing%20accuracy%20and%20inference%20speed%2C%20we%20propose%20a%0Adirectional%20pure%202D%20approach.%20Our%20method%20involves%20slicing%203D%20voxel%20features%20to%0Apreserve%20complete%20vertical%20geometric%20information.%20This%20strategy%20compensates%20for%0Athe%20loss%20of%20height%20cues%20in%20Bird%27s-Eye%20View%20%28BEV%29%20representations%2C%20thereby%0Amaintaining%20the%20integrity%20of%20the%203D%20geometric%20structure.%20By%20employing%20a%0Adirectional%20attention%20mechanism%2C%20we%20efficiently%20extract%20geometric%20features%20from%0Adifferent%20orientations%2C%20striking%20a%20balance%20between%20accuracy%20and%20computational%0Aefficiency.%20Experimental%20results%20highlight%20the%20significant%20advantages%20of%20our%0Aapproach%20for%20autonomous%20driving.%20On%20the%20Occ3D-nuScenes%2C%20the%20proposed%20method%0Aachieves%20an%20mIoU%20of%2039.3%25%20and%20an%20inference%20speed%20of%2027.7%20FPS%2C%20effectively%0Abalancing%20accuracy%20and%20efficiency.%20In%20simulations%20on%20edge%20devices%2C%20the%0Ainference%20speed%20reaches%2014.8%20FPS%2C%20further%20demonstrating%20the%20method%27s%0Aapplicability%20for%20real-time%20deployment%20in%20resource-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23599v1&entry.124074799=Read"},
{"title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models", "author": "Xiaoyu Chen and Hangxing Wei and Pushi Zhang and Chuheng Zhang and Kaixin Wang and Yanjiang Guo and Rushuai Yang and Yucen Wang and Xinquan Xiao and Li Zhao and Jianyu Chen and Jiang Bian", "abstract": "  Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.\n", "link": "http://arxiv.org/abs/2507.23682v1", "date": "2025-07-31", "relevancy": 2.2403, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5629}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20villa-X%3A%20Enhancing%20Latent%20Action%20Modeling%20in%20Vision-Language-Action%0A%20%20Models&body=Title%3A%20villa-X%3A%20Enhancing%20Latent%20Action%20Modeling%20in%20Vision-Language-Action%0A%20%20Models%0AAuthor%3A%20Xiaoyu%20Chen%20and%20Hangxing%20Wei%20and%20Pushi%20Zhang%20and%20Chuheng%20Zhang%20and%20Kaixin%20Wang%20and%20Yanjiang%20Guo%20and%20Rushuai%20Yang%20and%20Yucen%20Wang%20and%20Xinquan%20Xiao%20and%20Li%20Zhao%20and%20Jianyu%20Chen%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Visual-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20popular%20paradigm%20for%0Alearning%20robot%20manipulation%20policies%20that%20can%20follow%20language%20instructions%20and%0Ageneralize%20to%20novel%20scenarios.%20Recent%20work%20has%20begun%20to%20explore%20the%0Aincorporation%20of%20latent%20actions%2C%20an%20abstract%20representation%20of%20visual%20change%0Abetween%20two%20frames%2C%20into%20VLA%20pre-training.%20In%20this%20paper%2C%20we%20introduce%20villa-X%2C%0Aa%20novel%20Visual-Language-Latent-Action%20%28ViLLA%29%20framework%20that%20advances%20latent%0Aaction%20modeling%20for%20learning%20generalizable%20robot%20manipulation%20policies.%20Our%0Aapproach%20improves%20both%20how%20latent%20actions%20are%20learned%20and%20how%20they%20are%0Aincorporated%20into%20VLA%20pre-training.%20Together%2C%20these%20contributions%20enable%0Avilla-X%20to%20achieve%20superior%20performance%20across%20simulated%20environments%20including%0ASIMPLER%20and%20LIBERO%2C%20as%20well%20as%20on%20two%20real-world%20robot%20setups%20including%20gripper%0Aand%20dexterous%20hand%20manipulation.%20We%20believe%20the%20ViLLA%20paradigm%20holds%0Asignificant%20promise%2C%20and%20that%20our%20villa-X%20provides%20a%20strong%20foundation%20for%0Afuture%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dvilla-X%253A%2520Enhancing%2520Latent%2520Action%2520Modeling%2520in%2520Vision-Language-Action%250A%2520%2520Models%26entry.906535625%3DXiaoyu%2520Chen%2520and%2520Hangxing%2520Wei%2520and%2520Pushi%2520Zhang%2520and%2520Chuheng%2520Zhang%2520and%2520Kaixin%2520Wang%2520and%2520Yanjiang%2520Guo%2520and%2520Rushuai%2520Yang%2520and%2520Yucen%2520Wang%2520and%2520Xinquan%2520Xiao%2520and%2520Li%2520Zhao%2520and%2520Jianyu%2520Chen%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Visual-Language-Action%2520%2528VLA%2529%2520models%2520have%2520emerged%2520as%2520a%2520popular%2520paradigm%2520for%250Alearning%2520robot%2520manipulation%2520policies%2520that%2520can%2520follow%2520language%2520instructions%2520and%250Ageneralize%2520to%2520novel%2520scenarios.%2520Recent%2520work%2520has%2520begun%2520to%2520explore%2520the%250Aincorporation%2520of%2520latent%2520actions%252C%2520an%2520abstract%2520representation%2520of%2520visual%2520change%250Abetween%2520two%2520frames%252C%2520into%2520VLA%2520pre-training.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520villa-X%252C%250Aa%2520novel%2520Visual-Language-Latent-Action%2520%2528ViLLA%2529%2520framework%2520that%2520advances%2520latent%250Aaction%2520modeling%2520for%2520learning%2520generalizable%2520robot%2520manipulation%2520policies.%2520Our%250Aapproach%2520improves%2520both%2520how%2520latent%2520actions%2520are%2520learned%2520and%2520how%2520they%2520are%250Aincorporated%2520into%2520VLA%2520pre-training.%2520Together%252C%2520these%2520contributions%2520enable%250Avilla-X%2520to%2520achieve%2520superior%2520performance%2520across%2520simulated%2520environments%2520including%250ASIMPLER%2520and%2520LIBERO%252C%2520as%2520well%2520as%2520on%2520two%2520real-world%2520robot%2520setups%2520including%2520gripper%250Aand%2520dexterous%2520hand%2520manipulation.%2520We%2520believe%2520the%2520ViLLA%2520paradigm%2520holds%250Asignificant%2520promise%252C%2520and%2520that%2520our%2520villa-X%2520provides%2520a%2520strong%2520foundation%2520for%250Afuture%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=villa-X%3A%20Enhancing%20Latent%20Action%20Modeling%20in%20Vision-Language-Action%0A%20%20Models&entry.906535625=Xiaoyu%20Chen%20and%20Hangxing%20Wei%20and%20Pushi%20Zhang%20and%20Chuheng%20Zhang%20and%20Kaixin%20Wang%20and%20Yanjiang%20Guo%20and%20Rushuai%20Yang%20and%20Yucen%20Wang%20and%20Xinquan%20Xiao%20and%20Li%20Zhao%20and%20Jianyu%20Chen%20and%20Jiang%20Bian&entry.1292438233=%20%20Visual-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20a%20popular%20paradigm%20for%0Alearning%20robot%20manipulation%20policies%20that%20can%20follow%20language%20instructions%20and%0Ageneralize%20to%20novel%20scenarios.%20Recent%20work%20has%20begun%20to%20explore%20the%0Aincorporation%20of%20latent%20actions%2C%20an%20abstract%20representation%20of%20visual%20change%0Abetween%20two%20frames%2C%20into%20VLA%20pre-training.%20In%20this%20paper%2C%20we%20introduce%20villa-X%2C%0Aa%20novel%20Visual-Language-Latent-Action%20%28ViLLA%29%20framework%20that%20advances%20latent%0Aaction%20modeling%20for%20learning%20generalizable%20robot%20manipulation%20policies.%20Our%0Aapproach%20improves%20both%20how%20latent%20actions%20are%20learned%20and%20how%20they%20are%0Aincorporated%20into%20VLA%20pre-training.%20Together%2C%20these%20contributions%20enable%0Avilla-X%20to%20achieve%20superior%20performance%20across%20simulated%20environments%20including%0ASIMPLER%20and%20LIBERO%2C%20as%20well%20as%20on%20two%20real-world%20robot%20setups%20including%20gripper%0Aand%20dexterous%20hand%20manipulation.%20We%20believe%20the%20ViLLA%20paradigm%20holds%0Asignificant%20promise%2C%20and%20that%20our%20villa-X%20provides%20a%20strong%20foundation%20for%0Afuture%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23682v1&entry.124074799=Read"},
{"title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection\n  and Explanation with MLLM", "author": "Haiquan Wen and Tianxiao Li and Zhenglin Huang and Yiwei He and Guangliang Cheng", "abstract": "  Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.\n", "link": "http://arxiv.org/abs/2507.14632v2", "date": "2025-07-31", "relevancy": 2.2348, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5719}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5568}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BusterX%2B%2B%3A%20Towards%20Unified%20Cross-Modal%20AI-Generated%20Content%20Detection%0A%20%20and%20Explanation%20with%20MLLM&body=Title%3A%20BusterX%2B%2B%3A%20Towards%20Unified%20Cross-Modal%20AI-Generated%20Content%20Detection%0A%20%20and%20Explanation%20with%20MLLM%0AAuthor%3A%20Haiquan%20Wen%20and%20Tianxiao%20Li%20and%20Zhenglin%20Huang%20and%20Yiwei%20He%20and%20Guangliang%20Cheng%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20AI%20have%20dramatically%20improved%20image%20and%20video%0Asynthesis%20capabilities%2C%20significantly%20increasing%20the%20risk%20of%20misinformation%0Athrough%20sophisticated%20fake%20content.%20In%20response%2C%20detection%20methods%20have%20evolved%0Afrom%20traditional%20approaches%20to%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%0Aoffering%20enhanced%20transparency%20and%20interpretability%20in%20identifying%20synthetic%0Amedia.%20However%2C%20current%20detection%20systems%20remain%20fundamentally%20limited%20by%20their%0Asingle-modality%20design.%20These%20approaches%20analyze%20images%20or%20videos%20separately%2C%0Amaking%20them%20ineffective%20against%20synthetic%20content%20that%20combines%20multiple%20media%0Aformats.%20To%20address%20these%20challenges%2C%20we%20introduce%20%5Ctextbf%7BBusterX%2B%2B%7D%2C%20a%20novel%0Aframework%20designed%20specifically%20for%20cross-modal%20detection%20and%20explanation%20of%0Asynthetic%20media.%20Our%20approach%20incorporates%20an%20advanced%20reinforcement%20learning%0A%28RL%29%20post-training%20strategy%20that%20eliminates%20cold-start.%20Through%20Multi-stage%0ATraining%2C%20Thinking%20Reward%2C%20and%20Hybrid%20Reasoning%2C%20BusterX%2B%2B%20achieves%20stable%20and%0Asubstantial%20performance%20improvements.%20To%20enable%20comprehensive%20evaluation%2C%20we%0Aalso%20present%20%5Ctextbf%7BGenBuster%2B%2B%7D%2C%20a%20cross-modal%20benchmark%20leveraging%0Astate-of-the-art%20image%20and%20video%20generation%20techniques.%20This%20benchmark%0Acomprises%204%2C000%20images%20and%20video%20clips%2C%20meticulously%20curated%20by%20human%20experts%0Ausing%20a%20novel%20filtering%20methodology%20to%20ensure%20high%20quality%2C%20diversity%2C%20and%0Areal-world%20applicability.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aand%20generalizability%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.14632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBusterX%252B%252B%253A%2520Towards%2520Unified%2520Cross-Modal%2520AI-Generated%2520Content%2520Detection%250A%2520%2520and%2520Explanation%2520with%2520MLLM%26entry.906535625%3DHaiquan%2520Wen%2520and%2520Tianxiao%2520Li%2520and%2520Zhenglin%2520Huang%2520and%2520Yiwei%2520He%2520and%2520Guangliang%2520Cheng%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520AI%2520have%2520dramatically%2520improved%2520image%2520and%2520video%250Asynthesis%2520capabilities%252C%2520significantly%2520increasing%2520the%2520risk%2520of%2520misinformation%250Athrough%2520sophisticated%2520fake%2520content.%2520In%2520response%252C%2520detection%2520methods%2520have%2520evolved%250Afrom%2520traditional%2520approaches%2520to%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%250Aoffering%2520enhanced%2520transparency%2520and%2520interpretability%2520in%2520identifying%2520synthetic%250Amedia.%2520However%252C%2520current%2520detection%2520systems%2520remain%2520fundamentally%2520limited%2520by%2520their%250Asingle-modality%2520design.%2520These%2520approaches%2520analyze%2520images%2520or%2520videos%2520separately%252C%250Amaking%2520them%2520ineffective%2520against%2520synthetic%2520content%2520that%2520combines%2520multiple%2520media%250Aformats.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520%255Ctextbf%257BBusterX%252B%252B%257D%252C%2520a%2520novel%250Aframework%2520designed%2520specifically%2520for%2520cross-modal%2520detection%2520and%2520explanation%2520of%250Asynthetic%2520media.%2520Our%2520approach%2520incorporates%2520an%2520advanced%2520reinforcement%2520learning%250A%2528RL%2529%2520post-training%2520strategy%2520that%2520eliminates%2520cold-start.%2520Through%2520Multi-stage%250ATraining%252C%2520Thinking%2520Reward%252C%2520and%2520Hybrid%2520Reasoning%252C%2520BusterX%252B%252B%2520achieves%2520stable%2520and%250Asubstantial%2520performance%2520improvements.%2520To%2520enable%2520comprehensive%2520evaluation%252C%2520we%250Aalso%2520present%2520%255Ctextbf%257BGenBuster%252B%252B%257D%252C%2520a%2520cross-modal%2520benchmark%2520leveraging%250Astate-of-the-art%2520image%2520and%2520video%2520generation%2520techniques.%2520This%2520benchmark%250Acomprises%25204%252C000%2520images%2520and%2520video%2520clips%252C%2520meticulously%2520curated%2520by%2520human%2520experts%250Ausing%2520a%2520novel%2520filtering%2520methodology%2520to%2520ensure%2520high%2520quality%252C%2520diversity%252C%2520and%250Areal-world%2520applicability.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%250Aand%2520generalizability%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.14632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BusterX%2B%2B%3A%20Towards%20Unified%20Cross-Modal%20AI-Generated%20Content%20Detection%0A%20%20and%20Explanation%20with%20MLLM&entry.906535625=Haiquan%20Wen%20and%20Tianxiao%20Li%20and%20Zhenglin%20Huang%20and%20Yiwei%20He%20and%20Guangliang%20Cheng&entry.1292438233=%20%20Recent%20advances%20in%20generative%20AI%20have%20dramatically%20improved%20image%20and%20video%0Asynthesis%20capabilities%2C%20significantly%20increasing%20the%20risk%20of%20misinformation%0Athrough%20sophisticated%20fake%20content.%20In%20response%2C%20detection%20methods%20have%20evolved%0Afrom%20traditional%20approaches%20to%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%0Aoffering%20enhanced%20transparency%20and%20interpretability%20in%20identifying%20synthetic%0Amedia.%20However%2C%20current%20detection%20systems%20remain%20fundamentally%20limited%20by%20their%0Asingle-modality%20design.%20These%20approaches%20analyze%20images%20or%20videos%20separately%2C%0Amaking%20them%20ineffective%20against%20synthetic%20content%20that%20combines%20multiple%20media%0Aformats.%20To%20address%20these%20challenges%2C%20we%20introduce%20%5Ctextbf%7BBusterX%2B%2B%7D%2C%20a%20novel%0Aframework%20designed%20specifically%20for%20cross-modal%20detection%20and%20explanation%20of%0Asynthetic%20media.%20Our%20approach%20incorporates%20an%20advanced%20reinforcement%20learning%0A%28RL%29%20post-training%20strategy%20that%20eliminates%20cold-start.%20Through%20Multi-stage%0ATraining%2C%20Thinking%20Reward%2C%20and%20Hybrid%20Reasoning%2C%20BusterX%2B%2B%20achieves%20stable%20and%0Asubstantial%20performance%20improvements.%20To%20enable%20comprehensive%20evaluation%2C%20we%0Aalso%20present%20%5Ctextbf%7BGenBuster%2B%2B%7D%2C%20a%20cross-modal%20benchmark%20leveraging%0Astate-of-the-art%20image%20and%20video%20generation%20techniques.%20This%20benchmark%0Acomprises%204%2C000%20images%20and%20video%20clips%2C%20meticulously%20curated%20by%20human%20experts%0Ausing%20a%20novel%20filtering%20methodology%20to%20ensure%20high%20quality%2C%20diversity%2C%20and%0Areal-world%20applicability.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aand%20generalizability%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.14632v2&entry.124074799=Read"},
{"title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for\n  Dexterous Teleoperation", "author": "Haiyun Zhang and Stefano Dalla Gasperina and Saad N. Yousaf and Toshimitsu Tsuboi and Tetsuya Narita and Ashish D. Deshpande", "abstract": "  Hand exoskeletons are critical tools for dexterous teleoperation and\nimmersive manipulation interfaces, but achieving accurate hand tracking remains\na challenge due to user-specific anatomical variability and donning\ninconsistencies. These issues lead to kinematic misalignments that degrade\ntracking performance and limit applicability in precision tasks. We propose a\nsubject-specific calibration framework for exoskeleton-based hand tracking that\nuses redundant joint sensing and a residual-weighted optimization strategy to\nestimate virtual link parameters. Implemented on the Maestro exoskeleton, our\nmethod improves joint angle and fingertip position estimation across users with\nvarying hand geometries. We introduce a data-driven approach to empirically\ntune cost function weights using motion capture ground truth, enabling more\naccurate and consistent calibration across participants. Quantitative results\nfrom seven subjects show substantial reductions in joint and fingertip tracking\nerrors compared to uncalibrated and evenly weighted models. Qualitative\nvisualizations using a Unity-based virtual hand further confirm improvements in\nmotion fidelity. The proposed framework generalizes across exoskeleton designs\nwith closed-loop kinematics and minimal sensing, and lays the foundation for\nhigh-fidelity teleoperation and learning-from-demonstration applications.\n", "link": "http://arxiv.org/abs/2507.23592v1", "date": "2025-07-31", "relevancy": 2.2325, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5778}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5507}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Exoskeleton%20Kinematic%20Calibration%20to%20Improve%20Hand%20Tracking%20for%0A%20%20Dexterous%20Teleoperation&body=Title%3A%20Human-Exoskeleton%20Kinematic%20Calibration%20to%20Improve%20Hand%20Tracking%20for%0A%20%20Dexterous%20Teleoperation%0AAuthor%3A%20Haiyun%20Zhang%20and%20Stefano%20Dalla%20Gasperina%20and%20Saad%20N.%20Yousaf%20and%20Toshimitsu%20Tsuboi%20and%20Tetsuya%20Narita%20and%20Ashish%20D.%20Deshpande%0AAbstract%3A%20%20%20Hand%20exoskeletons%20are%20critical%20tools%20for%20dexterous%20teleoperation%20and%0Aimmersive%20manipulation%20interfaces%2C%20but%20achieving%20accurate%20hand%20tracking%20remains%0Aa%20challenge%20due%20to%20user-specific%20anatomical%20variability%20and%20donning%0Ainconsistencies.%20These%20issues%20lead%20to%20kinematic%20misalignments%20that%20degrade%0Atracking%20performance%20and%20limit%20applicability%20in%20precision%20tasks.%20We%20propose%20a%0Asubject-specific%20calibration%20framework%20for%20exoskeleton-based%20hand%20tracking%20that%0Auses%20redundant%20joint%20sensing%20and%20a%20residual-weighted%20optimization%20strategy%20to%0Aestimate%20virtual%20link%20parameters.%20Implemented%20on%20the%20Maestro%20exoskeleton%2C%20our%0Amethod%20improves%20joint%20angle%20and%20fingertip%20position%20estimation%20across%20users%20with%0Avarying%20hand%20geometries.%20We%20introduce%20a%20data-driven%20approach%20to%20empirically%0Atune%20cost%20function%20weights%20using%20motion%20capture%20ground%20truth%2C%20enabling%20more%0Aaccurate%20and%20consistent%20calibration%20across%20participants.%20Quantitative%20results%0Afrom%20seven%20subjects%20show%20substantial%20reductions%20in%20joint%20and%20fingertip%20tracking%0Aerrors%20compared%20to%20uncalibrated%20and%20evenly%20weighted%20models.%20Qualitative%0Avisualizations%20using%20a%20Unity-based%20virtual%20hand%20further%20confirm%20improvements%20in%0Amotion%20fidelity.%20The%20proposed%20framework%20generalizes%20across%20exoskeleton%20designs%0Awith%20closed-loop%20kinematics%20and%20minimal%20sensing%2C%20and%20lays%20the%20foundation%20for%0Ahigh-fidelity%20teleoperation%20and%20learning-from-demonstration%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Exoskeleton%2520Kinematic%2520Calibration%2520to%2520Improve%2520Hand%2520Tracking%2520for%250A%2520%2520Dexterous%2520Teleoperation%26entry.906535625%3DHaiyun%2520Zhang%2520and%2520Stefano%2520Dalla%2520Gasperina%2520and%2520Saad%2520N.%2520Yousaf%2520and%2520Toshimitsu%2520Tsuboi%2520and%2520Tetsuya%2520Narita%2520and%2520Ashish%2520D.%2520Deshpande%26entry.1292438233%3D%2520%2520Hand%2520exoskeletons%2520are%2520critical%2520tools%2520for%2520dexterous%2520teleoperation%2520and%250Aimmersive%2520manipulation%2520interfaces%252C%2520but%2520achieving%2520accurate%2520hand%2520tracking%2520remains%250Aa%2520challenge%2520due%2520to%2520user-specific%2520anatomical%2520variability%2520and%2520donning%250Ainconsistencies.%2520These%2520issues%2520lead%2520to%2520kinematic%2520misalignments%2520that%2520degrade%250Atracking%2520performance%2520and%2520limit%2520applicability%2520in%2520precision%2520tasks.%2520We%2520propose%2520a%250Asubject-specific%2520calibration%2520framework%2520for%2520exoskeleton-based%2520hand%2520tracking%2520that%250Auses%2520redundant%2520joint%2520sensing%2520and%2520a%2520residual-weighted%2520optimization%2520strategy%2520to%250Aestimate%2520virtual%2520link%2520parameters.%2520Implemented%2520on%2520the%2520Maestro%2520exoskeleton%252C%2520our%250Amethod%2520improves%2520joint%2520angle%2520and%2520fingertip%2520position%2520estimation%2520across%2520users%2520with%250Avarying%2520hand%2520geometries.%2520We%2520introduce%2520a%2520data-driven%2520approach%2520to%2520empirically%250Atune%2520cost%2520function%2520weights%2520using%2520motion%2520capture%2520ground%2520truth%252C%2520enabling%2520more%250Aaccurate%2520and%2520consistent%2520calibration%2520across%2520participants.%2520Quantitative%2520results%250Afrom%2520seven%2520subjects%2520show%2520substantial%2520reductions%2520in%2520joint%2520and%2520fingertip%2520tracking%250Aerrors%2520compared%2520to%2520uncalibrated%2520and%2520evenly%2520weighted%2520models.%2520Qualitative%250Avisualizations%2520using%2520a%2520Unity-based%2520virtual%2520hand%2520further%2520confirm%2520improvements%2520in%250Amotion%2520fidelity.%2520The%2520proposed%2520framework%2520generalizes%2520across%2520exoskeleton%2520designs%250Awith%2520closed-loop%2520kinematics%2520and%2520minimal%2520sensing%252C%2520and%2520lays%2520the%2520foundation%2520for%250Ahigh-fidelity%2520teleoperation%2520and%2520learning-from-demonstration%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Exoskeleton%20Kinematic%20Calibration%20to%20Improve%20Hand%20Tracking%20for%0A%20%20Dexterous%20Teleoperation&entry.906535625=Haiyun%20Zhang%20and%20Stefano%20Dalla%20Gasperina%20and%20Saad%20N.%20Yousaf%20and%20Toshimitsu%20Tsuboi%20and%20Tetsuya%20Narita%20and%20Ashish%20D.%20Deshpande&entry.1292438233=%20%20Hand%20exoskeletons%20are%20critical%20tools%20for%20dexterous%20teleoperation%20and%0Aimmersive%20manipulation%20interfaces%2C%20but%20achieving%20accurate%20hand%20tracking%20remains%0Aa%20challenge%20due%20to%20user-specific%20anatomical%20variability%20and%20donning%0Ainconsistencies.%20These%20issues%20lead%20to%20kinematic%20misalignments%20that%20degrade%0Atracking%20performance%20and%20limit%20applicability%20in%20precision%20tasks.%20We%20propose%20a%0Asubject-specific%20calibration%20framework%20for%20exoskeleton-based%20hand%20tracking%20that%0Auses%20redundant%20joint%20sensing%20and%20a%20residual-weighted%20optimization%20strategy%20to%0Aestimate%20virtual%20link%20parameters.%20Implemented%20on%20the%20Maestro%20exoskeleton%2C%20our%0Amethod%20improves%20joint%20angle%20and%20fingertip%20position%20estimation%20across%20users%20with%0Avarying%20hand%20geometries.%20We%20introduce%20a%20data-driven%20approach%20to%20empirically%0Atune%20cost%20function%20weights%20using%20motion%20capture%20ground%20truth%2C%20enabling%20more%0Aaccurate%20and%20consistent%20calibration%20across%20participants.%20Quantitative%20results%0Afrom%20seven%20subjects%20show%20substantial%20reductions%20in%20joint%20and%20fingertip%20tracking%0Aerrors%20compared%20to%20uncalibrated%20and%20evenly%20weighted%20models.%20Qualitative%0Avisualizations%20using%20a%20Unity-based%20virtual%20hand%20further%20confirm%20improvements%20in%0Amotion%20fidelity.%20The%20proposed%20framework%20generalizes%20across%20exoskeleton%20designs%0Awith%20closed-loop%20kinematics%20and%20minimal%20sensing%2C%20and%20lays%20the%20foundation%20for%0Ahigh-fidelity%20teleoperation%20and%20learning-from-demonstration%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23592v1&entry.124074799=Read"},
{"title": "Mitigating Resolution-Drift in Federated Learning: Case of Keypoint\n  Detection", "author": "Taeheon Lim and Joohyung Lee and Kyungjae Lee and Jungchan Cho", "abstract": "  The Federated Learning (FL) approach enables effective learning across\ndistributed systems, while preserving user data privacy. To date, research has\nprimarily focused on addressing statistical heterogeneity and communication\nefficiency, through which FL has achieved success in classification tasks.\nHowever, its application to non-classification tasks, such as human pose\nestimation, remains underexplored. This paper identifies and investigates a\ncritical issue termed ``resolution-drift,'' where performance degrades\nsignificantly due to resolution variability across clients. Unlike class-level\nheterogeneity, resolution drift highlights the importance of resolution as\nanother axis of not independent or identically distributed (non-IID) data. To\naddress this issue, we present resolution-adaptive federated learning (RAF), a\nmethod that leverages heatmap-based knowledge distillation. Through\nmulti-resolution knowledge distillation between higher-resolution outputs\n(teachers) and lower-resolution outputs (students), our approach enhances\nresolution robustness without overfitting. Extensive experiments and\ntheoretical analysis demonstrate that RAF not only effectively mitigates\nresolution drift and achieves significant performance improvements, but also\ncan be integrated seamlessly into existing FL frameworks. Furthermore, although\nthis paper focuses on human pose estimation, our t-SNE analysis reveals\ndistinct characteristics between classification and high-resolution\nrepresentation tasks, supporting the generalizability of RAF to other tasks\nthat rely on preserving spatial detail.\n", "link": "http://arxiv.org/abs/2507.23461v1", "date": "2025-07-31", "relevancy": 2.2129, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5715}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5447}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Resolution-Drift%20in%20Federated%20Learning%3A%20Case%20of%20Keypoint%0A%20%20Detection&body=Title%3A%20Mitigating%20Resolution-Drift%20in%20Federated%20Learning%3A%20Case%20of%20Keypoint%0A%20%20Detection%0AAuthor%3A%20Taeheon%20Lim%20and%20Joohyung%20Lee%20and%20Kyungjae%20Lee%20and%20Jungchan%20Cho%0AAbstract%3A%20%20%20The%20Federated%20Learning%20%28FL%29%20approach%20enables%20effective%20learning%20across%0Adistributed%20systems%2C%20while%20preserving%20user%20data%20privacy.%20To%20date%2C%20research%20has%0Aprimarily%20focused%20on%20addressing%20statistical%20heterogeneity%20and%20communication%0Aefficiency%2C%20through%20which%20FL%20has%20achieved%20success%20in%20classification%20tasks.%0AHowever%2C%20its%20application%20to%20non-classification%20tasks%2C%20such%20as%20human%20pose%0Aestimation%2C%20remains%20underexplored.%20This%20paper%20identifies%20and%20investigates%20a%0Acritical%20issue%20termed%20%60%60resolution-drift%2C%27%27%20where%20performance%20degrades%0Asignificantly%20due%20to%20resolution%20variability%20across%20clients.%20Unlike%20class-level%0Aheterogeneity%2C%20resolution%20drift%20highlights%20the%20importance%20of%20resolution%20as%0Aanother%20axis%20of%20not%20independent%20or%20identically%20distributed%20%28non-IID%29%20data.%20To%0Aaddress%20this%20issue%2C%20we%20present%20resolution-adaptive%20federated%20learning%20%28RAF%29%2C%20a%0Amethod%20that%20leverages%20heatmap-based%20knowledge%20distillation.%20Through%0Amulti-resolution%20knowledge%20distillation%20between%20higher-resolution%20outputs%0A%28teachers%29%20and%20lower-resolution%20outputs%20%28students%29%2C%20our%20approach%20enhances%0Aresolution%20robustness%20without%20overfitting.%20Extensive%20experiments%20and%0Atheoretical%20analysis%20demonstrate%20that%20RAF%20not%20only%20effectively%20mitigates%0Aresolution%20drift%20and%20achieves%20significant%20performance%20improvements%2C%20but%20also%0Acan%20be%20integrated%20seamlessly%20into%20existing%20FL%20frameworks.%20Furthermore%2C%20although%0Athis%20paper%20focuses%20on%20human%20pose%20estimation%2C%20our%20t-SNE%20analysis%20reveals%0Adistinct%20characteristics%20between%20classification%20and%20high-resolution%0Arepresentation%20tasks%2C%20supporting%20the%20generalizability%20of%20RAF%20to%20other%20tasks%0Athat%20rely%20on%20preserving%20spatial%20detail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Resolution-Drift%2520in%2520Federated%2520Learning%253A%2520Case%2520of%2520Keypoint%250A%2520%2520Detection%26entry.906535625%3DTaeheon%2520Lim%2520and%2520Joohyung%2520Lee%2520and%2520Kyungjae%2520Lee%2520and%2520Jungchan%2520Cho%26entry.1292438233%3D%2520%2520The%2520Federated%2520Learning%2520%2528FL%2529%2520approach%2520enables%2520effective%2520learning%2520across%250Adistributed%2520systems%252C%2520while%2520preserving%2520user%2520data%2520privacy.%2520To%2520date%252C%2520research%2520has%250Aprimarily%2520focused%2520on%2520addressing%2520statistical%2520heterogeneity%2520and%2520communication%250Aefficiency%252C%2520through%2520which%2520FL%2520has%2520achieved%2520success%2520in%2520classification%2520tasks.%250AHowever%252C%2520its%2520application%2520to%2520non-classification%2520tasks%252C%2520such%2520as%2520human%2520pose%250Aestimation%252C%2520remains%2520underexplored.%2520This%2520paper%2520identifies%2520and%2520investigates%2520a%250Acritical%2520issue%2520termed%2520%2560%2560resolution-drift%252C%2527%2527%2520where%2520performance%2520degrades%250Asignificantly%2520due%2520to%2520resolution%2520variability%2520across%2520clients.%2520Unlike%2520class-level%250Aheterogeneity%252C%2520resolution%2520drift%2520highlights%2520the%2520importance%2520of%2520resolution%2520as%250Aanother%2520axis%2520of%2520not%2520independent%2520or%2520identically%2520distributed%2520%2528non-IID%2529%2520data.%2520To%250Aaddress%2520this%2520issue%252C%2520we%2520present%2520resolution-adaptive%2520federated%2520learning%2520%2528RAF%2529%252C%2520a%250Amethod%2520that%2520leverages%2520heatmap-based%2520knowledge%2520distillation.%2520Through%250Amulti-resolution%2520knowledge%2520distillation%2520between%2520higher-resolution%2520outputs%250A%2528teachers%2529%2520and%2520lower-resolution%2520outputs%2520%2528students%2529%252C%2520our%2520approach%2520enhances%250Aresolution%2520robustness%2520without%2520overfitting.%2520Extensive%2520experiments%2520and%250Atheoretical%2520analysis%2520demonstrate%2520that%2520RAF%2520not%2520only%2520effectively%2520mitigates%250Aresolution%2520drift%2520and%2520achieves%2520significant%2520performance%2520improvements%252C%2520but%2520also%250Acan%2520be%2520integrated%2520seamlessly%2520into%2520existing%2520FL%2520frameworks.%2520Furthermore%252C%2520although%250Athis%2520paper%2520focuses%2520on%2520human%2520pose%2520estimation%252C%2520our%2520t-SNE%2520analysis%2520reveals%250Adistinct%2520characteristics%2520between%2520classification%2520and%2520high-resolution%250Arepresentation%2520tasks%252C%2520supporting%2520the%2520generalizability%2520of%2520RAF%2520to%2520other%2520tasks%250Athat%2520rely%2520on%2520preserving%2520spatial%2520detail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Resolution-Drift%20in%20Federated%20Learning%3A%20Case%20of%20Keypoint%0A%20%20Detection&entry.906535625=Taeheon%20Lim%20and%20Joohyung%20Lee%20and%20Kyungjae%20Lee%20and%20Jungchan%20Cho&entry.1292438233=%20%20The%20Federated%20Learning%20%28FL%29%20approach%20enables%20effective%20learning%20across%0Adistributed%20systems%2C%20while%20preserving%20user%20data%20privacy.%20To%20date%2C%20research%20has%0Aprimarily%20focused%20on%20addressing%20statistical%20heterogeneity%20and%20communication%0Aefficiency%2C%20through%20which%20FL%20has%20achieved%20success%20in%20classification%20tasks.%0AHowever%2C%20its%20application%20to%20non-classification%20tasks%2C%20such%20as%20human%20pose%0Aestimation%2C%20remains%20underexplored.%20This%20paper%20identifies%20and%20investigates%20a%0Acritical%20issue%20termed%20%60%60resolution-drift%2C%27%27%20where%20performance%20degrades%0Asignificantly%20due%20to%20resolution%20variability%20across%20clients.%20Unlike%20class-level%0Aheterogeneity%2C%20resolution%20drift%20highlights%20the%20importance%20of%20resolution%20as%0Aanother%20axis%20of%20not%20independent%20or%20identically%20distributed%20%28non-IID%29%20data.%20To%0Aaddress%20this%20issue%2C%20we%20present%20resolution-adaptive%20federated%20learning%20%28RAF%29%2C%20a%0Amethod%20that%20leverages%20heatmap-based%20knowledge%20distillation.%20Through%0Amulti-resolution%20knowledge%20distillation%20between%20higher-resolution%20outputs%0A%28teachers%29%20and%20lower-resolution%20outputs%20%28students%29%2C%20our%20approach%20enhances%0Aresolution%20robustness%20without%20overfitting.%20Extensive%20experiments%20and%0Atheoretical%20analysis%20demonstrate%20that%20RAF%20not%20only%20effectively%20mitigates%0Aresolution%20drift%20and%20achieves%20significant%20performance%20improvements%2C%20but%20also%0Acan%20be%20integrated%20seamlessly%20into%20existing%20FL%20frameworks.%20Furthermore%2C%20although%0Athis%20paper%20focuses%20on%20human%20pose%20estimation%2C%20our%20t-SNE%20analysis%20reveals%0Adistinct%20characteristics%20between%20classification%20and%20high-resolution%0Arepresentation%20tasks%2C%20supporting%20the%20generalizability%20of%20RAF%20to%20other%20tasks%0Athat%20rely%20on%20preserving%20spatial%20detail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23461v1&entry.124074799=Read"},
{"title": "Generalizable Image Repair for Robust Visual Control", "author": "Carson Sobolewski and Zhenjiang Mao and Kshitij Maruti Vejre and Ivan Ruchkin", "abstract": "  Vision-based control relies on accurate perception to achieve robustness.\nHowever, image distribution changes caused by sensor noise, adverse weather,\nand dynamic lighting can degrade perception, leading to suboptimal control\ndecisions. Existing approaches, including domain adaptation and adversarial\ntraining, improve robustness but struggle to generalize to unseen corruptions\nwhile introducing computational overhead. To address this challenge, we propose\na real-time image repair module that restores corrupted images before they are\nused by the controller. Our method leverages generative adversarial models,\nspecifically CycleGAN and pix2pix, for image repair. CycleGAN enables unpaired\nimage-to-image translation to adapt to novel corruptions, while pix2pix\nexploits paired image data when available to improve the quality. To ensure\nalignment with control performance, we introduce a control-focused loss\nfunction that prioritizes perceptual consistency in repaired images. We\nevaluated our method in a simulated autonomous racing environment with various\nvisual corruptions. The results show that our approach significantly improves\nperformance compared to baselines, mitigating distribution shift and enhancing\ncontroller reliability.\n", "link": "http://arxiv.org/abs/2503.05911v2", "date": "2025-07-31", "relevancy": 2.1947, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.561}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.543}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Image%20Repair%20for%20Robust%20Visual%20Control&body=Title%3A%20Generalizable%20Image%20Repair%20for%20Robust%20Visual%20Control%0AAuthor%3A%20Carson%20Sobolewski%20and%20Zhenjiang%20Mao%20and%20Kshitij%20Maruti%20Vejre%20and%20Ivan%20Ruchkin%0AAbstract%3A%20%20%20Vision-based%20control%20relies%20on%20accurate%20perception%20to%20achieve%20robustness.%0AHowever%2C%20image%20distribution%20changes%20caused%20by%20sensor%20noise%2C%20adverse%20weather%2C%0Aand%20dynamic%20lighting%20can%20degrade%20perception%2C%20leading%20to%20suboptimal%20control%0Adecisions.%20Existing%20approaches%2C%20including%20domain%20adaptation%20and%20adversarial%0Atraining%2C%20improve%20robustness%20but%20struggle%20to%20generalize%20to%20unseen%20corruptions%0Awhile%20introducing%20computational%20overhead.%20To%20address%20this%20challenge%2C%20we%20propose%0Aa%20real-time%20image%20repair%20module%20that%20restores%20corrupted%20images%20before%20they%20are%0Aused%20by%20the%20controller.%20Our%20method%20leverages%20generative%20adversarial%20models%2C%0Aspecifically%20CycleGAN%20and%20pix2pix%2C%20for%20image%20repair.%20CycleGAN%20enables%20unpaired%0Aimage-to-image%20translation%20to%20adapt%20to%20novel%20corruptions%2C%20while%20pix2pix%0Aexploits%20paired%20image%20data%20when%20available%20to%20improve%20the%20quality.%20To%20ensure%0Aalignment%20with%20control%20performance%2C%20we%20introduce%20a%20control-focused%20loss%0Afunction%20that%20prioritizes%20perceptual%20consistency%20in%20repaired%20images.%20We%0Aevaluated%20our%20method%20in%20a%20simulated%20autonomous%20racing%20environment%20with%20various%0Avisual%20corruptions.%20The%20results%20show%20that%20our%20approach%20significantly%20improves%0Aperformance%20compared%20to%20baselines%2C%20mitigating%20distribution%20shift%20and%20enhancing%0Acontroller%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.05911v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Image%2520Repair%2520for%2520Robust%2520Visual%2520Control%26entry.906535625%3DCarson%2520Sobolewski%2520and%2520Zhenjiang%2520Mao%2520and%2520Kshitij%2520Maruti%2520Vejre%2520and%2520Ivan%2520Ruchkin%26entry.1292438233%3D%2520%2520Vision-based%2520control%2520relies%2520on%2520accurate%2520perception%2520to%2520achieve%2520robustness.%250AHowever%252C%2520image%2520distribution%2520changes%2520caused%2520by%2520sensor%2520noise%252C%2520adverse%2520weather%252C%250Aand%2520dynamic%2520lighting%2520can%2520degrade%2520perception%252C%2520leading%2520to%2520suboptimal%2520control%250Adecisions.%2520Existing%2520approaches%252C%2520including%2520domain%2520adaptation%2520and%2520adversarial%250Atraining%252C%2520improve%2520robustness%2520but%2520struggle%2520to%2520generalize%2520to%2520unseen%2520corruptions%250Awhile%2520introducing%2520computational%2520overhead.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%250Aa%2520real-time%2520image%2520repair%2520module%2520that%2520restores%2520corrupted%2520images%2520before%2520they%2520are%250Aused%2520by%2520the%2520controller.%2520Our%2520method%2520leverages%2520generative%2520adversarial%2520models%252C%250Aspecifically%2520CycleGAN%2520and%2520pix2pix%252C%2520for%2520image%2520repair.%2520CycleGAN%2520enables%2520unpaired%250Aimage-to-image%2520translation%2520to%2520adapt%2520to%2520novel%2520corruptions%252C%2520while%2520pix2pix%250Aexploits%2520paired%2520image%2520data%2520when%2520available%2520to%2520improve%2520the%2520quality.%2520To%2520ensure%250Aalignment%2520with%2520control%2520performance%252C%2520we%2520introduce%2520a%2520control-focused%2520loss%250Afunction%2520that%2520prioritizes%2520perceptual%2520consistency%2520in%2520repaired%2520images.%2520We%250Aevaluated%2520our%2520method%2520in%2520a%2520simulated%2520autonomous%2520racing%2520environment%2520with%2520various%250Avisual%2520corruptions.%2520The%2520results%2520show%2520that%2520our%2520approach%2520significantly%2520improves%250Aperformance%2520compared%2520to%2520baselines%252C%2520mitigating%2520distribution%2520shift%2520and%2520enhancing%250Acontroller%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.05911v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Image%20Repair%20for%20Robust%20Visual%20Control&entry.906535625=Carson%20Sobolewski%20and%20Zhenjiang%20Mao%20and%20Kshitij%20Maruti%20Vejre%20and%20Ivan%20Ruchkin&entry.1292438233=%20%20Vision-based%20control%20relies%20on%20accurate%20perception%20to%20achieve%20robustness.%0AHowever%2C%20image%20distribution%20changes%20caused%20by%20sensor%20noise%2C%20adverse%20weather%2C%0Aand%20dynamic%20lighting%20can%20degrade%20perception%2C%20leading%20to%20suboptimal%20control%0Adecisions.%20Existing%20approaches%2C%20including%20domain%20adaptation%20and%20adversarial%0Atraining%2C%20improve%20robustness%20but%20struggle%20to%20generalize%20to%20unseen%20corruptions%0Awhile%20introducing%20computational%20overhead.%20To%20address%20this%20challenge%2C%20we%20propose%0Aa%20real-time%20image%20repair%20module%20that%20restores%20corrupted%20images%20before%20they%20are%0Aused%20by%20the%20controller.%20Our%20method%20leverages%20generative%20adversarial%20models%2C%0Aspecifically%20CycleGAN%20and%20pix2pix%2C%20for%20image%20repair.%20CycleGAN%20enables%20unpaired%0Aimage-to-image%20translation%20to%20adapt%20to%20novel%20corruptions%2C%20while%20pix2pix%0Aexploits%20paired%20image%20data%20when%20available%20to%20improve%20the%20quality.%20To%20ensure%0Aalignment%20with%20control%20performance%2C%20we%20introduce%20a%20control-focused%20loss%0Afunction%20that%20prioritizes%20perceptual%20consistency%20in%20repaired%20images.%20We%0Aevaluated%20our%20method%20in%20a%20simulated%20autonomous%20racing%20environment%20with%20various%0Avisual%20corruptions.%20The%20results%20show%20that%20our%20approach%20significantly%20improves%0Aperformance%20compared%20to%20baselines%2C%20mitigating%20distribution%20shift%20and%20enhancing%0Acontroller%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.05911v2&entry.124074799=Read"},
{"title": "DeepForest: Sensing Into Self-Occluding Volumes of Vegetation With\n  Aerial Imaging", "author": "Mohamed Youssef and Jian Peng and Oliver Bimber", "abstract": "  Access to below-canopy volumetric vegetation data is crucial for\nunderstanding ecosystem dynamics. We address the long-standing limitation of\nremote sensing to penetrate deep into dense canopy layers. LiDAR and radar are\ncurrently considered the primary options for measuring 3D vegetation\nstructures, while cameras can only extract the reflectance and depth of top\nlayers. Using conventional, high-resolution aerial images, our approach allows\nsensing deep into self-occluding vegetation volumes, such as forests. It is\nsimilar in spirit to the imaging process of wide-field microscopy, but can\nhandle much larger scales and strong occlusion. We scan focal stacks by\nsynthetic-aperture imaging with drones and reduce out-of-focus signal\ncontributions using pre-trained 3D convolutional neural networks with mean\nsquared error (MSE) as the loss function. The resulting volumetric reflectance\nstacks contain low-frequency representations of the vegetation volume.\nCombining multiple reflectance stacks from various spectral channels provides\ninsights into plant health, growth, and environmental conditions throughout the\nentire vegetation volume. Compared with simulated ground truth, our correction\nleads to ~x7 average improvements (min: ~x2, max: ~x12) for forest densities of\n220 trees/ha - 1680 trees/ha. In our field experiment, we achieved an MSE of\n0.05 when comparing with the top-vegetation layer that was measured with\nclassical multispectral aerial imaging.\n", "link": "http://arxiv.org/abs/2502.02171v3", "date": "2025-07-31", "relevancy": 2.1697, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5604}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5388}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepForest%3A%20Sensing%20Into%20Self-Occluding%20Volumes%20of%20Vegetation%20With%0A%20%20Aerial%20Imaging&body=Title%3A%20DeepForest%3A%20Sensing%20Into%20Self-Occluding%20Volumes%20of%20Vegetation%20With%0A%20%20Aerial%20Imaging%0AAuthor%3A%20Mohamed%20Youssef%20and%20Jian%20Peng%20and%20Oliver%20Bimber%0AAbstract%3A%20%20%20Access%20to%20below-canopy%20volumetric%20vegetation%20data%20is%20crucial%20for%0Aunderstanding%20ecosystem%20dynamics.%20We%20address%20the%20long-standing%20limitation%20of%0Aremote%20sensing%20to%20penetrate%20deep%20into%20dense%20canopy%20layers.%20LiDAR%20and%20radar%20are%0Acurrently%20considered%20the%20primary%20options%20for%20measuring%203D%20vegetation%0Astructures%2C%20while%20cameras%20can%20only%20extract%20the%20reflectance%20and%20depth%20of%20top%0Alayers.%20Using%20conventional%2C%20high-resolution%20aerial%20images%2C%20our%20approach%20allows%0Asensing%20deep%20into%20self-occluding%20vegetation%20volumes%2C%20such%20as%20forests.%20It%20is%0Asimilar%20in%20spirit%20to%20the%20imaging%20process%20of%20wide-field%20microscopy%2C%20but%20can%0Ahandle%20much%20larger%20scales%20and%20strong%20occlusion.%20We%20scan%20focal%20stacks%20by%0Asynthetic-aperture%20imaging%20with%20drones%20and%20reduce%20out-of-focus%20signal%0Acontributions%20using%20pre-trained%203D%20convolutional%20neural%20networks%20with%20mean%0Asquared%20error%20%28MSE%29%20as%20the%20loss%20function.%20The%20resulting%20volumetric%20reflectance%0Astacks%20contain%20low-frequency%20representations%20of%20the%20vegetation%20volume.%0ACombining%20multiple%20reflectance%20stacks%20from%20various%20spectral%20channels%20provides%0Ainsights%20into%20plant%20health%2C%20growth%2C%20and%20environmental%20conditions%20throughout%20the%0Aentire%20vegetation%20volume.%20Compared%20with%20simulated%20ground%20truth%2C%20our%20correction%0Aleads%20to%20~x7%20average%20improvements%20%28min%3A%20~x2%2C%20max%3A%20~x12%29%20for%20forest%20densities%20of%0A220%20trees/ha%20-%201680%20trees/ha.%20In%20our%20field%20experiment%2C%20we%20achieved%20an%20MSE%20of%0A0.05%20when%20comparing%20with%20the%20top-vegetation%20layer%20that%20was%20measured%20with%0Aclassical%20multispectral%20aerial%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02171v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepForest%253A%2520Sensing%2520Into%2520Self-Occluding%2520Volumes%2520of%2520Vegetation%2520With%250A%2520%2520Aerial%2520Imaging%26entry.906535625%3DMohamed%2520Youssef%2520and%2520Jian%2520Peng%2520and%2520Oliver%2520Bimber%26entry.1292438233%3D%2520%2520Access%2520to%2520below-canopy%2520volumetric%2520vegetation%2520data%2520is%2520crucial%2520for%250Aunderstanding%2520ecosystem%2520dynamics.%2520We%2520address%2520the%2520long-standing%2520limitation%2520of%250Aremote%2520sensing%2520to%2520penetrate%2520deep%2520into%2520dense%2520canopy%2520layers.%2520LiDAR%2520and%2520radar%2520are%250Acurrently%2520considered%2520the%2520primary%2520options%2520for%2520measuring%25203D%2520vegetation%250Astructures%252C%2520while%2520cameras%2520can%2520only%2520extract%2520the%2520reflectance%2520and%2520depth%2520of%2520top%250Alayers.%2520Using%2520conventional%252C%2520high-resolution%2520aerial%2520images%252C%2520our%2520approach%2520allows%250Asensing%2520deep%2520into%2520self-occluding%2520vegetation%2520volumes%252C%2520such%2520as%2520forests.%2520It%2520is%250Asimilar%2520in%2520spirit%2520to%2520the%2520imaging%2520process%2520of%2520wide-field%2520microscopy%252C%2520but%2520can%250Ahandle%2520much%2520larger%2520scales%2520and%2520strong%2520occlusion.%2520We%2520scan%2520focal%2520stacks%2520by%250Asynthetic-aperture%2520imaging%2520with%2520drones%2520and%2520reduce%2520out-of-focus%2520signal%250Acontributions%2520using%2520pre-trained%25203D%2520convolutional%2520neural%2520networks%2520with%2520mean%250Asquared%2520error%2520%2528MSE%2529%2520as%2520the%2520loss%2520function.%2520The%2520resulting%2520volumetric%2520reflectance%250Astacks%2520contain%2520low-frequency%2520representations%2520of%2520the%2520vegetation%2520volume.%250ACombining%2520multiple%2520reflectance%2520stacks%2520from%2520various%2520spectral%2520channels%2520provides%250Ainsights%2520into%2520plant%2520health%252C%2520growth%252C%2520and%2520environmental%2520conditions%2520throughout%2520the%250Aentire%2520vegetation%2520volume.%2520Compared%2520with%2520simulated%2520ground%2520truth%252C%2520our%2520correction%250Aleads%2520to%2520~x7%2520average%2520improvements%2520%2528min%253A%2520~x2%252C%2520max%253A%2520~x12%2529%2520for%2520forest%2520densities%2520of%250A220%2520trees/ha%2520-%25201680%2520trees/ha.%2520In%2520our%2520field%2520experiment%252C%2520we%2520achieved%2520an%2520MSE%2520of%250A0.05%2520when%2520comparing%2520with%2520the%2520top-vegetation%2520layer%2520that%2520was%2520measured%2520with%250Aclassical%2520multispectral%2520aerial%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02171v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepForest%3A%20Sensing%20Into%20Self-Occluding%20Volumes%20of%20Vegetation%20With%0A%20%20Aerial%20Imaging&entry.906535625=Mohamed%20Youssef%20and%20Jian%20Peng%20and%20Oliver%20Bimber&entry.1292438233=%20%20Access%20to%20below-canopy%20volumetric%20vegetation%20data%20is%20crucial%20for%0Aunderstanding%20ecosystem%20dynamics.%20We%20address%20the%20long-standing%20limitation%20of%0Aremote%20sensing%20to%20penetrate%20deep%20into%20dense%20canopy%20layers.%20LiDAR%20and%20radar%20are%0Acurrently%20considered%20the%20primary%20options%20for%20measuring%203D%20vegetation%0Astructures%2C%20while%20cameras%20can%20only%20extract%20the%20reflectance%20and%20depth%20of%20top%0Alayers.%20Using%20conventional%2C%20high-resolution%20aerial%20images%2C%20our%20approach%20allows%0Asensing%20deep%20into%20self-occluding%20vegetation%20volumes%2C%20such%20as%20forests.%20It%20is%0Asimilar%20in%20spirit%20to%20the%20imaging%20process%20of%20wide-field%20microscopy%2C%20but%20can%0Ahandle%20much%20larger%20scales%20and%20strong%20occlusion.%20We%20scan%20focal%20stacks%20by%0Asynthetic-aperture%20imaging%20with%20drones%20and%20reduce%20out-of-focus%20signal%0Acontributions%20using%20pre-trained%203D%20convolutional%20neural%20networks%20with%20mean%0Asquared%20error%20%28MSE%29%20as%20the%20loss%20function.%20The%20resulting%20volumetric%20reflectance%0Astacks%20contain%20low-frequency%20representations%20of%20the%20vegetation%20volume.%0ACombining%20multiple%20reflectance%20stacks%20from%20various%20spectral%20channels%20provides%0Ainsights%20into%20plant%20health%2C%20growth%2C%20and%20environmental%20conditions%20throughout%20the%0Aentire%20vegetation%20volume.%20Compared%20with%20simulated%20ground%20truth%2C%20our%20correction%0Aleads%20to%20~x7%20average%20improvements%20%28min%3A%20~x2%2C%20max%3A%20~x12%29%20for%20forest%20densities%20of%0A220%20trees/ha%20-%201680%20trees/ha.%20In%20our%20field%20experiment%2C%20we%20achieved%20an%20MSE%20of%0A0.05%20when%20comparing%20with%20the%20top-vegetation%20layer%20that%20was%20measured%20with%0Aclassical%20multispectral%20aerial%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02171v3&entry.124074799=Read"},
{"title": "XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation\n  Acceleration via Multi-Head Speculative Decoding", "author": "Dian Chen and Yansong Qu and Xinyang Li and Ming Li and Shengchuan Zhang", "abstract": "  Current auto-regressive models can generate high-quality, topologically\nprecise meshes; however, they necessitate thousands-or even tens of\nthousands-of next-token predictions during inference, resulting in substantial\nlatency. We introduce XSpecMesh, a quality-preserving acceleration method for\nauto-regressive mesh generation models. XSpecMesh employs a lightweight,\nmulti-head speculative decoding scheme to predict multiple tokens in parallel\nwithin a single forward pass, thereby accelerating inference. We further\npropose a verification and resampling strategy: the backbone model verifies\neach predicted token and resamples any tokens that do not meet the quality\ncriteria. In addition, we propose a distillation strategy that trains the\nlightweight decoding heads by distilling from the backbone model, encouraging\ntheir prediction distributions to align and improving the success rate of\nspeculative predictions. Extensive experiments demonstrate that our method\nachieves a 1.7x speedup without sacrificing generation quality. Our code will\nbe released.\n", "link": "http://arxiv.org/abs/2507.23777v1", "date": "2025-07-31", "relevancy": 2.1683, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5763}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5331}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XSpecMesh%3A%20Quality-Preserving%20Auto-Regressive%20Mesh%20Generation%0A%20%20Acceleration%20via%20Multi-Head%20Speculative%20Decoding&body=Title%3A%20XSpecMesh%3A%20Quality-Preserving%20Auto-Regressive%20Mesh%20Generation%0A%20%20Acceleration%20via%20Multi-Head%20Speculative%20Decoding%0AAuthor%3A%20Dian%20Chen%20and%20Yansong%20Qu%20and%20Xinyang%20Li%20and%20Ming%20Li%20and%20Shengchuan%20Zhang%0AAbstract%3A%20%20%20Current%20auto-regressive%20models%20can%20generate%20high-quality%2C%20topologically%0Aprecise%20meshes%3B%20however%2C%20they%20necessitate%20thousands-or%20even%20tens%20of%0Athousands-of%20next-token%20predictions%20during%20inference%2C%20resulting%20in%20substantial%0Alatency.%20We%20introduce%20XSpecMesh%2C%20a%20quality-preserving%20acceleration%20method%20for%0Aauto-regressive%20mesh%20generation%20models.%20XSpecMesh%20employs%20a%20lightweight%2C%0Amulti-head%20speculative%20decoding%20scheme%20to%20predict%20multiple%20tokens%20in%20parallel%0Awithin%20a%20single%20forward%20pass%2C%20thereby%20accelerating%20inference.%20We%20further%0Apropose%20a%20verification%20and%20resampling%20strategy%3A%20the%20backbone%20model%20verifies%0Aeach%20predicted%20token%20and%20resamples%20any%20tokens%20that%20do%20not%20meet%20the%20quality%0Acriteria.%20In%20addition%2C%20we%20propose%20a%20distillation%20strategy%20that%20trains%20the%0Alightweight%20decoding%20heads%20by%20distilling%20from%20the%20backbone%20model%2C%20encouraging%0Atheir%20prediction%20distributions%20to%20align%20and%20improving%20the%20success%20rate%20of%0Aspeculative%20predictions.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aachieves%20a%201.7x%20speedup%20without%20sacrificing%20generation%20quality.%20Our%20code%20will%0Abe%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXSpecMesh%253A%2520Quality-Preserving%2520Auto-Regressive%2520Mesh%2520Generation%250A%2520%2520Acceleration%2520via%2520Multi-Head%2520Speculative%2520Decoding%26entry.906535625%3DDian%2520Chen%2520and%2520Yansong%2520Qu%2520and%2520Xinyang%2520Li%2520and%2520Ming%2520Li%2520and%2520Shengchuan%2520Zhang%26entry.1292438233%3D%2520%2520Current%2520auto-regressive%2520models%2520can%2520generate%2520high-quality%252C%2520topologically%250Aprecise%2520meshes%253B%2520however%252C%2520they%2520necessitate%2520thousands-or%2520even%2520tens%2520of%250Athousands-of%2520next-token%2520predictions%2520during%2520inference%252C%2520resulting%2520in%2520substantial%250Alatency.%2520We%2520introduce%2520XSpecMesh%252C%2520a%2520quality-preserving%2520acceleration%2520method%2520for%250Aauto-regressive%2520mesh%2520generation%2520models.%2520XSpecMesh%2520employs%2520a%2520lightweight%252C%250Amulti-head%2520speculative%2520decoding%2520scheme%2520to%2520predict%2520multiple%2520tokens%2520in%2520parallel%250Awithin%2520a%2520single%2520forward%2520pass%252C%2520thereby%2520accelerating%2520inference.%2520We%2520further%250Apropose%2520a%2520verification%2520and%2520resampling%2520strategy%253A%2520the%2520backbone%2520model%2520verifies%250Aeach%2520predicted%2520token%2520and%2520resamples%2520any%2520tokens%2520that%2520do%2520not%2520meet%2520the%2520quality%250Acriteria.%2520In%2520addition%252C%2520we%2520propose%2520a%2520distillation%2520strategy%2520that%2520trains%2520the%250Alightweight%2520decoding%2520heads%2520by%2520distilling%2520from%2520the%2520backbone%2520model%252C%2520encouraging%250Atheir%2520prediction%2520distributions%2520to%2520align%2520and%2520improving%2520the%2520success%2520rate%2520of%250Aspeculative%2520predictions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aachieves%2520a%25201.7x%2520speedup%2520without%2520sacrificing%2520generation%2520quality.%2520Our%2520code%2520will%250Abe%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XSpecMesh%3A%20Quality-Preserving%20Auto-Regressive%20Mesh%20Generation%0A%20%20Acceleration%20via%20Multi-Head%20Speculative%20Decoding&entry.906535625=Dian%20Chen%20and%20Yansong%20Qu%20and%20Xinyang%20Li%20and%20Ming%20Li%20and%20Shengchuan%20Zhang&entry.1292438233=%20%20Current%20auto-regressive%20models%20can%20generate%20high-quality%2C%20topologically%0Aprecise%20meshes%3B%20however%2C%20they%20necessitate%20thousands-or%20even%20tens%20of%0Athousands-of%20next-token%20predictions%20during%20inference%2C%20resulting%20in%20substantial%0Alatency.%20We%20introduce%20XSpecMesh%2C%20a%20quality-preserving%20acceleration%20method%20for%0Aauto-regressive%20mesh%20generation%20models.%20XSpecMesh%20employs%20a%20lightweight%2C%0Amulti-head%20speculative%20decoding%20scheme%20to%20predict%20multiple%20tokens%20in%20parallel%0Awithin%20a%20single%20forward%20pass%2C%20thereby%20accelerating%20inference.%20We%20further%0Apropose%20a%20verification%20and%20resampling%20strategy%3A%20the%20backbone%20model%20verifies%0Aeach%20predicted%20token%20and%20resamples%20any%20tokens%20that%20do%20not%20meet%20the%20quality%0Acriteria.%20In%20addition%2C%20we%20propose%20a%20distillation%20strategy%20that%20trains%20the%0Alightweight%20decoding%20heads%20by%20distilling%20from%20the%20backbone%20model%2C%20encouraging%0Atheir%20prediction%20distributions%20to%20align%20and%20improving%20the%20success%20rate%20of%0Aspeculative%20predictions.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aachieves%20a%201.7x%20speedup%20without%20sacrificing%20generation%20quality.%20Our%20code%20will%0Abe%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23777v1&entry.124074799=Read"},
{"title": "MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration\n  with State Space Model", "author": "Yaoye Zhu and Zhe Wang and Yan Wang", "abstract": "  As cooperative systems that leverage roadside cameras to assist autonomous\nvehicle perception become increasingly widespread, large-scale precise\ncalibration of infrastructure cameras has become a critical issue. Traditional\nmanual calibration methods are often time-consuming, labor-intensive, and may\nrequire road closures. This paper proposes MamV2XCalib, the first V2X-based\ninfrastructure camera calibration method with the assistance of vehicle-side\nLiDAR. MamV2XCalib only requires autonomous vehicles equipped with LiDAR to\ndrive near the cameras to be calibrated in the infrastructure, without the need\nfor specific reference objects or manual intervention. We also introduce a new\ntargetless LiDAR-camera calibration method, which combines multi-scale features\nand a 4D correlation volume to estimate the correlation between vehicle-side\npoint clouds and roadside images. We model the temporal information and\nestimate the rotation angles with Mamba, effectively addressing calibration\nfailures in V2X scenarios caused by defects in the vehicle-side data (such as\nocclusions) and large differences in viewpoint. We evaluate MamV2XCalib on the\nV2X-Seq and TUMTraf-V2X real-world datasets, demonstrating the effectiveness\nand robustness of our V2X-based automatic calibration approach. Compared to\nprevious LiDAR-camera methods designed for calibration on one car, our approach\nachieves better and more stable calibration performance in V2X scenarios with\nfewer parameters. The code is available at\nhttps://github.com/zhuyaoye/MamV2XCalib.\n", "link": "http://arxiv.org/abs/2507.23595v1", "date": "2025-07-31", "relevancy": 2.1681, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5607}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5529}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MamV2XCalib%3A%20V2X-based%20Target-less%20Infrastructure%20Camera%20Calibration%0A%20%20with%20State%20Space%20Model&body=Title%3A%20MamV2XCalib%3A%20V2X-based%20Target-less%20Infrastructure%20Camera%20Calibration%0A%20%20with%20State%20Space%20Model%0AAuthor%3A%20Yaoye%20Zhu%20and%20Zhe%20Wang%20and%20Yan%20Wang%0AAbstract%3A%20%20%20As%20cooperative%20systems%20that%20leverage%20roadside%20cameras%20to%20assist%20autonomous%0Avehicle%20perception%20become%20increasingly%20widespread%2C%20large-scale%20precise%0Acalibration%20of%20infrastructure%20cameras%20has%20become%20a%20critical%20issue.%20Traditional%0Amanual%20calibration%20methods%20are%20often%20time-consuming%2C%20labor-intensive%2C%20and%20may%0Arequire%20road%20closures.%20This%20paper%20proposes%20MamV2XCalib%2C%20the%20first%20V2X-based%0Ainfrastructure%20camera%20calibration%20method%20with%20the%20assistance%20of%20vehicle-side%0ALiDAR.%20MamV2XCalib%20only%20requires%20autonomous%20vehicles%20equipped%20with%20LiDAR%20to%0Adrive%20near%20the%20cameras%20to%20be%20calibrated%20in%20the%20infrastructure%2C%20without%20the%20need%0Afor%20specific%20reference%20objects%20or%20manual%20intervention.%20We%20also%20introduce%20a%20new%0Atargetless%20LiDAR-camera%20calibration%20method%2C%20which%20combines%20multi-scale%20features%0Aand%20a%204D%20correlation%20volume%20to%20estimate%20the%20correlation%20between%20vehicle-side%0Apoint%20clouds%20and%20roadside%20images.%20We%20model%20the%20temporal%20information%20and%0Aestimate%20the%20rotation%20angles%20with%20Mamba%2C%20effectively%20addressing%20calibration%0Afailures%20in%20V2X%20scenarios%20caused%20by%20defects%20in%20the%20vehicle-side%20data%20%28such%20as%0Aocclusions%29%20and%20large%20differences%20in%20viewpoint.%20We%20evaluate%20MamV2XCalib%20on%20the%0AV2X-Seq%20and%20TUMTraf-V2X%20real-world%20datasets%2C%20demonstrating%20the%20effectiveness%0Aand%20robustness%20of%20our%20V2X-based%20automatic%20calibration%20approach.%20Compared%20to%0Aprevious%20LiDAR-camera%20methods%20designed%20for%20calibration%20on%20one%20car%2C%20our%20approach%0Aachieves%20better%20and%20more%20stable%20calibration%20performance%20in%20V2X%20scenarios%20with%0Afewer%20parameters.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zhuyaoye/MamV2XCalib.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamV2XCalib%253A%2520V2X-based%2520Target-less%2520Infrastructure%2520Camera%2520Calibration%250A%2520%2520with%2520State%2520Space%2520Model%26entry.906535625%3DYaoye%2520Zhu%2520and%2520Zhe%2520Wang%2520and%2520Yan%2520Wang%26entry.1292438233%3D%2520%2520As%2520cooperative%2520systems%2520that%2520leverage%2520roadside%2520cameras%2520to%2520assist%2520autonomous%250Avehicle%2520perception%2520become%2520increasingly%2520widespread%252C%2520large-scale%2520precise%250Acalibration%2520of%2520infrastructure%2520cameras%2520has%2520become%2520a%2520critical%2520issue.%2520Traditional%250Amanual%2520calibration%2520methods%2520are%2520often%2520time-consuming%252C%2520labor-intensive%252C%2520and%2520may%250Arequire%2520road%2520closures.%2520This%2520paper%2520proposes%2520MamV2XCalib%252C%2520the%2520first%2520V2X-based%250Ainfrastructure%2520camera%2520calibration%2520method%2520with%2520the%2520assistance%2520of%2520vehicle-side%250ALiDAR.%2520MamV2XCalib%2520only%2520requires%2520autonomous%2520vehicles%2520equipped%2520with%2520LiDAR%2520to%250Adrive%2520near%2520the%2520cameras%2520to%2520be%2520calibrated%2520in%2520the%2520infrastructure%252C%2520without%2520the%2520need%250Afor%2520specific%2520reference%2520objects%2520or%2520manual%2520intervention.%2520We%2520also%2520introduce%2520a%2520new%250Atargetless%2520LiDAR-camera%2520calibration%2520method%252C%2520which%2520combines%2520multi-scale%2520features%250Aand%2520a%25204D%2520correlation%2520volume%2520to%2520estimate%2520the%2520correlation%2520between%2520vehicle-side%250Apoint%2520clouds%2520and%2520roadside%2520images.%2520We%2520model%2520the%2520temporal%2520information%2520and%250Aestimate%2520the%2520rotation%2520angles%2520with%2520Mamba%252C%2520effectively%2520addressing%2520calibration%250Afailures%2520in%2520V2X%2520scenarios%2520caused%2520by%2520defects%2520in%2520the%2520vehicle-side%2520data%2520%2528such%2520as%250Aocclusions%2529%2520and%2520large%2520differences%2520in%2520viewpoint.%2520We%2520evaluate%2520MamV2XCalib%2520on%2520the%250AV2X-Seq%2520and%2520TUMTraf-V2X%2520real-world%2520datasets%252C%2520demonstrating%2520the%2520effectiveness%250Aand%2520robustness%2520of%2520our%2520V2X-based%2520automatic%2520calibration%2520approach.%2520Compared%2520to%250Aprevious%2520LiDAR-camera%2520methods%2520designed%2520for%2520calibration%2520on%2520one%2520car%252C%2520our%2520approach%250Aachieves%2520better%2520and%2520more%2520stable%2520calibration%2520performance%2520in%2520V2X%2520scenarios%2520with%250Afewer%2520parameters.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zhuyaoye/MamV2XCalib.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MamV2XCalib%3A%20V2X-based%20Target-less%20Infrastructure%20Camera%20Calibration%0A%20%20with%20State%20Space%20Model&entry.906535625=Yaoye%20Zhu%20and%20Zhe%20Wang%20and%20Yan%20Wang&entry.1292438233=%20%20As%20cooperative%20systems%20that%20leverage%20roadside%20cameras%20to%20assist%20autonomous%0Avehicle%20perception%20become%20increasingly%20widespread%2C%20large-scale%20precise%0Acalibration%20of%20infrastructure%20cameras%20has%20become%20a%20critical%20issue.%20Traditional%0Amanual%20calibration%20methods%20are%20often%20time-consuming%2C%20labor-intensive%2C%20and%20may%0Arequire%20road%20closures.%20This%20paper%20proposes%20MamV2XCalib%2C%20the%20first%20V2X-based%0Ainfrastructure%20camera%20calibration%20method%20with%20the%20assistance%20of%20vehicle-side%0ALiDAR.%20MamV2XCalib%20only%20requires%20autonomous%20vehicles%20equipped%20with%20LiDAR%20to%0Adrive%20near%20the%20cameras%20to%20be%20calibrated%20in%20the%20infrastructure%2C%20without%20the%20need%0Afor%20specific%20reference%20objects%20or%20manual%20intervention.%20We%20also%20introduce%20a%20new%0Atargetless%20LiDAR-camera%20calibration%20method%2C%20which%20combines%20multi-scale%20features%0Aand%20a%204D%20correlation%20volume%20to%20estimate%20the%20correlation%20between%20vehicle-side%0Apoint%20clouds%20and%20roadside%20images.%20We%20model%20the%20temporal%20information%20and%0Aestimate%20the%20rotation%20angles%20with%20Mamba%2C%20effectively%20addressing%20calibration%0Afailures%20in%20V2X%20scenarios%20caused%20by%20defects%20in%20the%20vehicle-side%20data%20%28such%20as%0Aocclusions%29%20and%20large%20differences%20in%20viewpoint.%20We%20evaluate%20MamV2XCalib%20on%20the%0AV2X-Seq%20and%20TUMTraf-V2X%20real-world%20datasets%2C%20demonstrating%20the%20effectiveness%0Aand%20robustness%20of%20our%20V2X-based%20automatic%20calibration%20approach.%20Compared%20to%0Aprevious%20LiDAR-camera%20methods%20designed%20for%20calibration%20on%20one%20car%2C%20our%20approach%0Aachieves%20better%20and%20more%20stable%20calibration%20performance%20in%20V2X%20scenarios%20with%0Afewer%20parameters.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/zhuyaoye/MamV2XCalib.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23595v1&entry.124074799=Read"},
{"title": "PatchTraj: Unified Time-Frequency Representation Learning via Dynamic\n  Patches for Trajectory Prediction", "author": "Yanghong Liu and Xingping Dong and Ming Li and Weixing Zhang and Yidong Lou", "abstract": "  Pedestrian trajectory prediction is crucial for autonomous driving and\nrobotics. While existing point-based and grid-based methods expose two main\nlimitations: insufficiently modeling human motion dynamics, as they fail to\nbalance local motion details with long-range spatiotemporal dependencies, and\nthe time representations lack interaction with their frequency components in\njointly modeling trajectory sequences. To address these challenges, we propose\nPatchTraj, a dynamic patch-based framework that integrates time-frequency joint\nmodeling for trajectory prediction. Specifically, we decompose the trajectory\ninto raw time sequences and frequency components, and employ dynamic patch\npartitioning to perform multi-scale segmentation, capturing hierarchical motion\npatterns. Each patch undergoes adaptive embedding with scale-aware feature\nextraction, followed by hierarchical feature aggregation to model both\nfine-grained and long-range dependencies. The outputs of the two branches are\nfurther enhanced via cross-modal attention, facilitating complementary fusion\nof temporal and spectral cues. The resulting enhanced embeddings exhibit strong\nexpressive power, enabling accurate predictions even when using a vanilla\nTransformer architecture. Extensive experiments on ETH-UCY, SDD, NBA, and JRDB\ndatasets demonstrate that our method achieves state-of-the-art performance.\nNotably, on the egocentric JRDB dataset, PatchTraj attains significant relative\nimprovements of 26.7% in ADE and 17.4% in FDE, underscoring its substantial\npotential in embodied intelligence.\n", "link": "http://arxiv.org/abs/2507.19119v3", "date": "2025-07-31", "relevancy": 2.1563, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5789}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5432}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PatchTraj%3A%20Unified%20Time-Frequency%20Representation%20Learning%20via%20Dynamic%0A%20%20Patches%20for%20Trajectory%20Prediction&body=Title%3A%20PatchTraj%3A%20Unified%20Time-Frequency%20Representation%20Learning%20via%20Dynamic%0A%20%20Patches%20for%20Trajectory%20Prediction%0AAuthor%3A%20Yanghong%20Liu%20and%20Xingping%20Dong%20and%20Ming%20Li%20and%20Weixing%20Zhang%20and%20Yidong%20Lou%0AAbstract%3A%20%20%20Pedestrian%20trajectory%20prediction%20is%20crucial%20for%20autonomous%20driving%20and%0Arobotics.%20While%20existing%20point-based%20and%20grid-based%20methods%20expose%20two%20main%0Alimitations%3A%20insufficiently%20modeling%20human%20motion%20dynamics%2C%20as%20they%20fail%20to%0Abalance%20local%20motion%20details%20with%20long-range%20spatiotemporal%20dependencies%2C%20and%0Athe%20time%20representations%20lack%20interaction%20with%20their%20frequency%20components%20in%0Ajointly%20modeling%20trajectory%20sequences.%20To%20address%20these%20challenges%2C%20we%20propose%0APatchTraj%2C%20a%20dynamic%20patch-based%20framework%20that%20integrates%20time-frequency%20joint%0Amodeling%20for%20trajectory%20prediction.%20Specifically%2C%20we%20decompose%20the%20trajectory%0Ainto%20raw%20time%20sequences%20and%20frequency%20components%2C%20and%20employ%20dynamic%20patch%0Apartitioning%20to%20perform%20multi-scale%20segmentation%2C%20capturing%20hierarchical%20motion%0Apatterns.%20Each%20patch%20undergoes%20adaptive%20embedding%20with%20scale-aware%20feature%0Aextraction%2C%20followed%20by%20hierarchical%20feature%20aggregation%20to%20model%20both%0Afine-grained%20and%20long-range%20dependencies.%20The%20outputs%20of%20the%20two%20branches%20are%0Afurther%20enhanced%20via%20cross-modal%20attention%2C%20facilitating%20complementary%20fusion%0Aof%20temporal%20and%20spectral%20cues.%20The%20resulting%20enhanced%20embeddings%20exhibit%20strong%0Aexpressive%20power%2C%20enabling%20accurate%20predictions%20even%20when%20using%20a%20vanilla%0ATransformer%20architecture.%20Extensive%20experiments%20on%20ETH-UCY%2C%20SDD%2C%20NBA%2C%20and%20JRDB%0Adatasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0ANotably%2C%20on%20the%20egocentric%20JRDB%20dataset%2C%20PatchTraj%20attains%20significant%20relative%0Aimprovements%20of%2026.7%25%20in%20ADE%20and%2017.4%25%20in%20FDE%2C%20underscoring%20its%20substantial%0Apotential%20in%20embodied%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.19119v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatchTraj%253A%2520Unified%2520Time-Frequency%2520Representation%2520Learning%2520via%2520Dynamic%250A%2520%2520Patches%2520for%2520Trajectory%2520Prediction%26entry.906535625%3DYanghong%2520Liu%2520and%2520Xingping%2520Dong%2520and%2520Ming%2520Li%2520and%2520Weixing%2520Zhang%2520and%2520Yidong%2520Lou%26entry.1292438233%3D%2520%2520Pedestrian%2520trajectory%2520prediction%2520is%2520crucial%2520for%2520autonomous%2520driving%2520and%250Arobotics.%2520While%2520existing%2520point-based%2520and%2520grid-based%2520methods%2520expose%2520two%2520main%250Alimitations%253A%2520insufficiently%2520modeling%2520human%2520motion%2520dynamics%252C%2520as%2520they%2520fail%2520to%250Abalance%2520local%2520motion%2520details%2520with%2520long-range%2520spatiotemporal%2520dependencies%252C%2520and%250Athe%2520time%2520representations%2520lack%2520interaction%2520with%2520their%2520frequency%2520components%2520in%250Ajointly%2520modeling%2520trajectory%2520sequences.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250APatchTraj%252C%2520a%2520dynamic%2520patch-based%2520framework%2520that%2520integrates%2520time-frequency%2520joint%250Amodeling%2520for%2520trajectory%2520prediction.%2520Specifically%252C%2520we%2520decompose%2520the%2520trajectory%250Ainto%2520raw%2520time%2520sequences%2520and%2520frequency%2520components%252C%2520and%2520employ%2520dynamic%2520patch%250Apartitioning%2520to%2520perform%2520multi-scale%2520segmentation%252C%2520capturing%2520hierarchical%2520motion%250Apatterns.%2520Each%2520patch%2520undergoes%2520adaptive%2520embedding%2520with%2520scale-aware%2520feature%250Aextraction%252C%2520followed%2520by%2520hierarchical%2520feature%2520aggregation%2520to%2520model%2520both%250Afine-grained%2520and%2520long-range%2520dependencies.%2520The%2520outputs%2520of%2520the%2520two%2520branches%2520are%250Afurther%2520enhanced%2520via%2520cross-modal%2520attention%252C%2520facilitating%2520complementary%2520fusion%250Aof%2520temporal%2520and%2520spectral%2520cues.%2520The%2520resulting%2520enhanced%2520embeddings%2520exhibit%2520strong%250Aexpressive%2520power%252C%2520enabling%2520accurate%2520predictions%2520even%2520when%2520using%2520a%2520vanilla%250ATransformer%2520architecture.%2520Extensive%2520experiments%2520on%2520ETH-UCY%252C%2520SDD%252C%2520NBA%252C%2520and%2520JRDB%250Adatasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance.%250ANotably%252C%2520on%2520the%2520egocentric%2520JRDB%2520dataset%252C%2520PatchTraj%2520attains%2520significant%2520relative%250Aimprovements%2520of%252026.7%2525%2520in%2520ADE%2520and%252017.4%2525%2520in%2520FDE%252C%2520underscoring%2520its%2520substantial%250Apotential%2520in%2520embodied%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19119v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PatchTraj%3A%20Unified%20Time-Frequency%20Representation%20Learning%20via%20Dynamic%0A%20%20Patches%20for%20Trajectory%20Prediction&entry.906535625=Yanghong%20Liu%20and%20Xingping%20Dong%20and%20Ming%20Li%20and%20Weixing%20Zhang%20and%20Yidong%20Lou&entry.1292438233=%20%20Pedestrian%20trajectory%20prediction%20is%20crucial%20for%20autonomous%20driving%20and%0Arobotics.%20While%20existing%20point-based%20and%20grid-based%20methods%20expose%20two%20main%0Alimitations%3A%20insufficiently%20modeling%20human%20motion%20dynamics%2C%20as%20they%20fail%20to%0Abalance%20local%20motion%20details%20with%20long-range%20spatiotemporal%20dependencies%2C%20and%0Athe%20time%20representations%20lack%20interaction%20with%20their%20frequency%20components%20in%0Ajointly%20modeling%20trajectory%20sequences.%20To%20address%20these%20challenges%2C%20we%20propose%0APatchTraj%2C%20a%20dynamic%20patch-based%20framework%20that%20integrates%20time-frequency%20joint%0Amodeling%20for%20trajectory%20prediction.%20Specifically%2C%20we%20decompose%20the%20trajectory%0Ainto%20raw%20time%20sequences%20and%20frequency%20components%2C%20and%20employ%20dynamic%20patch%0Apartitioning%20to%20perform%20multi-scale%20segmentation%2C%20capturing%20hierarchical%20motion%0Apatterns.%20Each%20patch%20undergoes%20adaptive%20embedding%20with%20scale-aware%20feature%0Aextraction%2C%20followed%20by%20hierarchical%20feature%20aggregation%20to%20model%20both%0Afine-grained%20and%20long-range%20dependencies.%20The%20outputs%20of%20the%20two%20branches%20are%0Afurther%20enhanced%20via%20cross-modal%20attention%2C%20facilitating%20complementary%20fusion%0Aof%20temporal%20and%20spectral%20cues.%20The%20resulting%20enhanced%20embeddings%20exhibit%20strong%0Aexpressive%20power%2C%20enabling%20accurate%20predictions%20even%20when%20using%20a%20vanilla%0ATransformer%20architecture.%20Extensive%20experiments%20on%20ETH-UCY%2C%20SDD%2C%20NBA%2C%20and%20JRDB%0Adatasets%20demonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance.%0ANotably%2C%20on%20the%20egocentric%20JRDB%20dataset%2C%20PatchTraj%20attains%20significant%20relative%0Aimprovements%20of%2026.7%25%20in%20ADE%20and%2017.4%25%20in%20FDE%2C%20underscoring%20its%20substantial%0Apotential%20in%20embodied%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.19119v3&entry.124074799=Read"},
{"title": "MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio\n  Understanding Tasks", "author": "Yadong Niu and Tianzi Wang and Heinrich Dinkel and Xingwei Sun and Jiahao Zhou and Gang Li and Jizhong Liu and Xunying Liu and Junbo Zhang and Jian Luan", "abstract": "  While large audio-language models have advanced open-ended audio\nunderstanding, they still fall short of nuanced human-level comprehension. This\ngap persists largely because current benchmarks, limited by data annotations\nand evaluation metrics, fail to reliably distinguish between generic and highly\ndetailed model outputs. To this end, this work introduces MECAT, a Multi-Expert\nConstructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via\na pipeline that integrates analysis from specialized expert models with\nChain-of-Thought large language model reasoning, MECAT provides\nmulti-perspective, fine-grained captions and open-set question-answering pairs.\nThe benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced\nAudio Text Evaluation). This metric penalizes generic terms and rewards\ndetailed descriptions by combining single-sample semantic similarity with\ncross-sample discriminability. A comprehensive evaluation of state-of-the-art\naudio models is also presented, providing new insights into their current\ncapabilities and limitations. The data and code are available at\nhttps://github.com/xiaomi-research/mecat\n", "link": "http://arxiv.org/abs/2507.23511v1", "date": "2025-07-31", "relevancy": 2.145, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.537}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5324}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MECAT%3A%20A%20Multi-Experts%20Constructed%20Benchmark%20for%20Fine-Grained%20Audio%0A%20%20Understanding%20Tasks&body=Title%3A%20MECAT%3A%20A%20Multi-Experts%20Constructed%20Benchmark%20for%20Fine-Grained%20Audio%0A%20%20Understanding%20Tasks%0AAuthor%3A%20Yadong%20Niu%20and%20Tianzi%20Wang%20and%20Heinrich%20Dinkel%20and%20Xingwei%20Sun%20and%20Jiahao%20Zhou%20and%20Gang%20Li%20and%20Jizhong%20Liu%20and%20Xunying%20Liu%20and%20Junbo%20Zhang%20and%20Jian%20Luan%0AAbstract%3A%20%20%20While%20large%20audio-language%20models%20have%20advanced%20open-ended%20audio%0Aunderstanding%2C%20they%20still%20fall%20short%20of%20nuanced%20human-level%20comprehension.%20This%0Agap%20persists%20largely%20because%20current%20benchmarks%2C%20limited%20by%20data%20annotations%0Aand%20evaluation%20metrics%2C%20fail%20to%20reliably%20distinguish%20between%20generic%20and%20highly%0Adetailed%20model%20outputs.%20To%20this%20end%2C%20this%20work%20introduces%20MECAT%2C%20a%20Multi-Expert%0AConstructed%20Benchmark%20for%20Fine-Grained%20Audio%20Understanding%20Tasks.%20Generated%20via%0Aa%20pipeline%20that%20integrates%20analysis%20from%20specialized%20expert%20models%20with%0AChain-of-Thought%20large%20language%20model%20reasoning%2C%20MECAT%20provides%0Amulti-perspective%2C%20fine-grained%20captions%20and%20open-set%20question-answering%20pairs.%0AThe%20benchmark%20is%20complemented%20by%20a%20novel%20metric%3A%20DATE%20%28Discriminative-Enhanced%0AAudio%20Text%20Evaluation%29.%20This%20metric%20penalizes%20generic%20terms%20and%20rewards%0Adetailed%20descriptions%20by%20combining%20single-sample%20semantic%20similarity%20with%0Across-sample%20discriminability.%20A%20comprehensive%20evaluation%20of%20state-of-the-art%0Aaudio%20models%20is%20also%20presented%2C%20providing%20new%20insights%20into%20their%20current%0Acapabilities%20and%20limitations.%20The%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/xiaomi-research/mecat%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMECAT%253A%2520A%2520Multi-Experts%2520Constructed%2520Benchmark%2520for%2520Fine-Grained%2520Audio%250A%2520%2520Understanding%2520Tasks%26entry.906535625%3DYadong%2520Niu%2520and%2520Tianzi%2520Wang%2520and%2520Heinrich%2520Dinkel%2520and%2520Xingwei%2520Sun%2520and%2520Jiahao%2520Zhou%2520and%2520Gang%2520Li%2520and%2520Jizhong%2520Liu%2520and%2520Xunying%2520Liu%2520and%2520Junbo%2520Zhang%2520and%2520Jian%2520Luan%26entry.1292438233%3D%2520%2520While%2520large%2520audio-language%2520models%2520have%2520advanced%2520open-ended%2520audio%250Aunderstanding%252C%2520they%2520still%2520fall%2520short%2520of%2520nuanced%2520human-level%2520comprehension.%2520This%250Agap%2520persists%2520largely%2520because%2520current%2520benchmarks%252C%2520limited%2520by%2520data%2520annotations%250Aand%2520evaluation%2520metrics%252C%2520fail%2520to%2520reliably%2520distinguish%2520between%2520generic%2520and%2520highly%250Adetailed%2520model%2520outputs.%2520To%2520this%2520end%252C%2520this%2520work%2520introduces%2520MECAT%252C%2520a%2520Multi-Expert%250AConstructed%2520Benchmark%2520for%2520Fine-Grained%2520Audio%2520Understanding%2520Tasks.%2520Generated%2520via%250Aa%2520pipeline%2520that%2520integrates%2520analysis%2520from%2520specialized%2520expert%2520models%2520with%250AChain-of-Thought%2520large%2520language%2520model%2520reasoning%252C%2520MECAT%2520provides%250Amulti-perspective%252C%2520fine-grained%2520captions%2520and%2520open-set%2520question-answering%2520pairs.%250AThe%2520benchmark%2520is%2520complemented%2520by%2520a%2520novel%2520metric%253A%2520DATE%2520%2528Discriminative-Enhanced%250AAudio%2520Text%2520Evaluation%2529.%2520This%2520metric%2520penalizes%2520generic%2520terms%2520and%2520rewards%250Adetailed%2520descriptions%2520by%2520combining%2520single-sample%2520semantic%2520similarity%2520with%250Across-sample%2520discriminability.%2520A%2520comprehensive%2520evaluation%2520of%2520state-of-the-art%250Aaudio%2520models%2520is%2520also%2520presented%252C%2520providing%2520new%2520insights%2520into%2520their%2520current%250Acapabilities%2520and%2520limitations.%2520The%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/xiaomi-research/mecat%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MECAT%3A%20A%20Multi-Experts%20Constructed%20Benchmark%20for%20Fine-Grained%20Audio%0A%20%20Understanding%20Tasks&entry.906535625=Yadong%20Niu%20and%20Tianzi%20Wang%20and%20Heinrich%20Dinkel%20and%20Xingwei%20Sun%20and%20Jiahao%20Zhou%20and%20Gang%20Li%20and%20Jizhong%20Liu%20and%20Xunying%20Liu%20and%20Junbo%20Zhang%20and%20Jian%20Luan&entry.1292438233=%20%20While%20large%20audio-language%20models%20have%20advanced%20open-ended%20audio%0Aunderstanding%2C%20they%20still%20fall%20short%20of%20nuanced%20human-level%20comprehension.%20This%0Agap%20persists%20largely%20because%20current%20benchmarks%2C%20limited%20by%20data%20annotations%0Aand%20evaluation%20metrics%2C%20fail%20to%20reliably%20distinguish%20between%20generic%20and%20highly%0Adetailed%20model%20outputs.%20To%20this%20end%2C%20this%20work%20introduces%20MECAT%2C%20a%20Multi-Expert%0AConstructed%20Benchmark%20for%20Fine-Grained%20Audio%20Understanding%20Tasks.%20Generated%20via%0Aa%20pipeline%20that%20integrates%20analysis%20from%20specialized%20expert%20models%20with%0AChain-of-Thought%20large%20language%20model%20reasoning%2C%20MECAT%20provides%0Amulti-perspective%2C%20fine-grained%20captions%20and%20open-set%20question-answering%20pairs.%0AThe%20benchmark%20is%20complemented%20by%20a%20novel%20metric%3A%20DATE%20%28Discriminative-Enhanced%0AAudio%20Text%20Evaluation%29.%20This%20metric%20penalizes%20generic%20terms%20and%20rewards%0Adetailed%20descriptions%20by%20combining%20single-sample%20semantic%20similarity%20with%0Across-sample%20discriminability.%20A%20comprehensive%20evaluation%20of%20state-of-the-art%0Aaudio%20models%20is%20also%20presented%2C%20providing%20new%20insights%20into%20their%20current%0Acapabilities%20and%20limitations.%20The%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/xiaomi-research/mecat%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23511v1&entry.124074799=Read"},
{"title": "L-GTA: Latent Generative Modeling for Time Series Augmentation", "author": "Luis Roque and Carlos Soares and Vitor Cerqueira and Luis Torgo", "abstract": "  Data augmentation is gaining importance across various aspects of time series\nanalysis, from forecasting to classification and anomaly detection tasks. We\nintroduce the Latent Generative Transformer Augmentation (L-GTA) model, a\ngenerative approach using a transformer-based variational recurrent\nautoencoder. This model uses controlled transformations within the latent space\nof the model to generate new time series that preserve the intrinsic properties\nof the original dataset. L-GTA enables the application of diverse\ntransformations, ranging from simple jittering to magnitude warping, and\ncombining these basic transformations to generate more complex synthetic time\nseries datasets. Our evaluation of several real-world datasets demonstrates the\nability of L-GTA to produce more reliable, consistent, and controllable\naugmented data. This translates into significant improvements in predictive\naccuracy and similarity measures compared to direct transformation methods.\n", "link": "http://arxiv.org/abs/2507.23615v1", "date": "2025-07-31", "relevancy": 2.1426, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5444}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5383}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L-GTA%3A%20Latent%20Generative%20Modeling%20for%20Time%20Series%20Augmentation&body=Title%3A%20L-GTA%3A%20Latent%20Generative%20Modeling%20for%20Time%20Series%20Augmentation%0AAuthor%3A%20Luis%20Roque%20and%20Carlos%20Soares%20and%20Vitor%20Cerqueira%20and%20Luis%20Torgo%0AAbstract%3A%20%20%20Data%20augmentation%20is%20gaining%20importance%20across%20various%20aspects%20of%20time%20series%0Aanalysis%2C%20from%20forecasting%20to%20classification%20and%20anomaly%20detection%20tasks.%20We%0Aintroduce%20the%20Latent%20Generative%20Transformer%20Augmentation%20%28L-GTA%29%20model%2C%20a%0Agenerative%20approach%20using%20a%20transformer-based%20variational%20recurrent%0Aautoencoder.%20This%20model%20uses%20controlled%20transformations%20within%20the%20latent%20space%0Aof%20the%20model%20to%20generate%20new%20time%20series%20that%20preserve%20the%20intrinsic%20properties%0Aof%20the%20original%20dataset.%20L-GTA%20enables%20the%20application%20of%20diverse%0Atransformations%2C%20ranging%20from%20simple%20jittering%20to%20magnitude%20warping%2C%20and%0Acombining%20these%20basic%20transformations%20to%20generate%20more%20complex%20synthetic%20time%0Aseries%20datasets.%20Our%20evaluation%20of%20several%20real-world%20datasets%20demonstrates%20the%0Aability%20of%20L-GTA%20to%20produce%20more%20reliable%2C%20consistent%2C%20and%20controllable%0Aaugmented%20data.%20This%20translates%20into%20significant%20improvements%20in%20predictive%0Aaccuracy%20and%20similarity%20measures%20compared%20to%20direct%20transformation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL-GTA%253A%2520Latent%2520Generative%2520Modeling%2520for%2520Time%2520Series%2520Augmentation%26entry.906535625%3DLuis%2520Roque%2520and%2520Carlos%2520Soares%2520and%2520Vitor%2520Cerqueira%2520and%2520Luis%2520Torgo%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520gaining%2520importance%2520across%2520various%2520aspects%2520of%2520time%2520series%250Aanalysis%252C%2520from%2520forecasting%2520to%2520classification%2520and%2520anomaly%2520detection%2520tasks.%2520We%250Aintroduce%2520the%2520Latent%2520Generative%2520Transformer%2520Augmentation%2520%2528L-GTA%2529%2520model%252C%2520a%250Agenerative%2520approach%2520using%2520a%2520transformer-based%2520variational%2520recurrent%250Aautoencoder.%2520This%2520model%2520uses%2520controlled%2520transformations%2520within%2520the%2520latent%2520space%250Aof%2520the%2520model%2520to%2520generate%2520new%2520time%2520series%2520that%2520preserve%2520the%2520intrinsic%2520properties%250Aof%2520the%2520original%2520dataset.%2520L-GTA%2520enables%2520the%2520application%2520of%2520diverse%250Atransformations%252C%2520ranging%2520from%2520simple%2520jittering%2520to%2520magnitude%2520warping%252C%2520and%250Acombining%2520these%2520basic%2520transformations%2520to%2520generate%2520more%2520complex%2520synthetic%2520time%250Aseries%2520datasets.%2520Our%2520evaluation%2520of%2520several%2520real-world%2520datasets%2520demonstrates%2520the%250Aability%2520of%2520L-GTA%2520to%2520produce%2520more%2520reliable%252C%2520consistent%252C%2520and%2520controllable%250Aaugmented%2520data.%2520This%2520translates%2520into%2520significant%2520improvements%2520in%2520predictive%250Aaccuracy%2520and%2520similarity%2520measures%2520compared%2520to%2520direct%2520transformation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L-GTA%3A%20Latent%20Generative%20Modeling%20for%20Time%20Series%20Augmentation&entry.906535625=Luis%20Roque%20and%20Carlos%20Soares%20and%20Vitor%20Cerqueira%20and%20Luis%20Torgo&entry.1292438233=%20%20Data%20augmentation%20is%20gaining%20importance%20across%20various%20aspects%20of%20time%20series%0Aanalysis%2C%20from%20forecasting%20to%20classification%20and%20anomaly%20detection%20tasks.%20We%0Aintroduce%20the%20Latent%20Generative%20Transformer%20Augmentation%20%28L-GTA%29%20model%2C%20a%0Agenerative%20approach%20using%20a%20transformer-based%20variational%20recurrent%0Aautoencoder.%20This%20model%20uses%20controlled%20transformations%20within%20the%20latent%20space%0Aof%20the%20model%20to%20generate%20new%20time%20series%20that%20preserve%20the%20intrinsic%20properties%0Aof%20the%20original%20dataset.%20L-GTA%20enables%20the%20application%20of%20diverse%0Atransformations%2C%20ranging%20from%20simple%20jittering%20to%20magnitude%20warping%2C%20and%0Acombining%20these%20basic%20transformations%20to%20generate%20more%20complex%20synthetic%20time%0Aseries%20datasets.%20Our%20evaluation%20of%20several%20real-world%20datasets%20demonstrates%20the%0Aability%20of%20L-GTA%20to%20produce%20more%20reliable%2C%20consistent%2C%20and%20controllable%0Aaugmented%20data.%20This%20translates%20into%20significant%20improvements%20in%20predictive%0Aaccuracy%20and%20similarity%20measures%20compared%20to%20direct%20transformation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23615v1&entry.124074799=Read"},
{"title": "Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study", "author": "Kai Goebel and Patrik Zips", "abstract": "  Recent advancements in Large Language Models have sparked interest in their\npotential for robotic task planning. While these models demonstrate strong\ngenerative capabilities, their effectiveness in producing structured and\nexecutable plans remains uncertain. This paper presents a systematic evaluation\nof a broad spectrum of current state of the art language models, each directly\nprompted using Planning Domain Definition Language domain and problem files,\nand compares their planning performance with the Fast Downward planner across a\nvariety of benchmarks. In addition to measuring success rates, we assess how\nfaithfully the generated plans translate into sequences of actions that can\nactually be executed, identifying both strengths and limitations of using these\nmodels in this setting. Our findings show that while the models perform well on\nsimpler planning tasks, they continue to struggle with more complex scenarios\nthat require precise resource management, consistent state tracking, and strict\nconstraint compliance. These results underscore fundamental challenges in\napplying language models to robotic planning in real world environments. By\noutlining the gaps that emerge during execution, we aim to guide future\nresearch toward combined approaches that integrate language models with\nclassical planners in order to enhance the reliability and scalability of\nplanning in autonomous robotics.\n", "link": "http://arxiv.org/abs/2507.23589v1", "date": "2025-07-31", "relevancy": 2.1414, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5398}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLM-Reasoning%20Models%20Replace%20Classical%20Planning%3F%20A%20Benchmark%20Study&body=Title%3A%20Can%20LLM-Reasoning%20Models%20Replace%20Classical%20Planning%3F%20A%20Benchmark%20Study%0AAuthor%3A%20Kai%20Goebel%20and%20Patrik%20Zips%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20have%20sparked%20interest%20in%20their%0Apotential%20for%20robotic%20task%20planning.%20While%20these%20models%20demonstrate%20strong%0Agenerative%20capabilities%2C%20their%20effectiveness%20in%20producing%20structured%20and%0Aexecutable%20plans%20remains%20uncertain.%20This%20paper%20presents%20a%20systematic%20evaluation%0Aof%20a%20broad%20spectrum%20of%20current%20state%20of%20the%20art%20language%20models%2C%20each%20directly%0Aprompted%20using%20Planning%20Domain%20Definition%20Language%20domain%20and%20problem%20files%2C%0Aand%20compares%20their%20planning%20performance%20with%20the%20Fast%20Downward%20planner%20across%20a%0Avariety%20of%20benchmarks.%20In%20addition%20to%20measuring%20success%20rates%2C%20we%20assess%20how%0Afaithfully%20the%20generated%20plans%20translate%20into%20sequences%20of%20actions%20that%20can%0Aactually%20be%20executed%2C%20identifying%20both%20strengths%20and%20limitations%20of%20using%20these%0Amodels%20in%20this%20setting.%20Our%20findings%20show%20that%20while%20the%20models%20perform%20well%20on%0Asimpler%20planning%20tasks%2C%20they%20continue%20to%20struggle%20with%20more%20complex%20scenarios%0Athat%20require%20precise%20resource%20management%2C%20consistent%20state%20tracking%2C%20and%20strict%0Aconstraint%20compliance.%20These%20results%20underscore%20fundamental%20challenges%20in%0Aapplying%20language%20models%20to%20robotic%20planning%20in%20real%20world%20environments.%20By%0Aoutlining%20the%20gaps%20that%20emerge%20during%20execution%2C%20we%20aim%20to%20guide%20future%0Aresearch%20toward%20combined%20approaches%20that%20integrate%20language%20models%20with%0Aclassical%20planners%20in%20order%20to%20enhance%20the%20reliability%20and%20scalability%20of%0Aplanning%20in%20autonomous%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLM-Reasoning%2520Models%2520Replace%2520Classical%2520Planning%253F%2520A%2520Benchmark%2520Study%26entry.906535625%3DKai%2520Goebel%2520and%2520Patrik%2520Zips%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520have%2520sparked%2520interest%2520in%2520their%250Apotential%2520for%2520robotic%2520task%2520planning.%2520While%2520these%2520models%2520demonstrate%2520strong%250Agenerative%2520capabilities%252C%2520their%2520effectiveness%2520in%2520producing%2520structured%2520and%250Aexecutable%2520plans%2520remains%2520uncertain.%2520This%2520paper%2520presents%2520a%2520systematic%2520evaluation%250Aof%2520a%2520broad%2520spectrum%2520of%2520current%2520state%2520of%2520the%2520art%2520language%2520models%252C%2520each%2520directly%250Aprompted%2520using%2520Planning%2520Domain%2520Definition%2520Language%2520domain%2520and%2520problem%2520files%252C%250Aand%2520compares%2520their%2520planning%2520performance%2520with%2520the%2520Fast%2520Downward%2520planner%2520across%2520a%250Avariety%2520of%2520benchmarks.%2520In%2520addition%2520to%2520measuring%2520success%2520rates%252C%2520we%2520assess%2520how%250Afaithfully%2520the%2520generated%2520plans%2520translate%2520into%2520sequences%2520of%2520actions%2520that%2520can%250Aactually%2520be%2520executed%252C%2520identifying%2520both%2520strengths%2520and%2520limitations%2520of%2520using%2520these%250Amodels%2520in%2520this%2520setting.%2520Our%2520findings%2520show%2520that%2520while%2520the%2520models%2520perform%2520well%2520on%250Asimpler%2520planning%2520tasks%252C%2520they%2520continue%2520to%2520struggle%2520with%2520more%2520complex%2520scenarios%250Athat%2520require%2520precise%2520resource%2520management%252C%2520consistent%2520state%2520tracking%252C%2520and%2520strict%250Aconstraint%2520compliance.%2520These%2520results%2520underscore%2520fundamental%2520challenges%2520in%250Aapplying%2520language%2520models%2520to%2520robotic%2520planning%2520in%2520real%2520world%2520environments.%2520By%250Aoutlining%2520the%2520gaps%2520that%2520emerge%2520during%2520execution%252C%2520we%2520aim%2520to%2520guide%2520future%250Aresearch%2520toward%2520combined%2520approaches%2520that%2520integrate%2520language%2520models%2520with%250Aclassical%2520planners%2520in%2520order%2520to%2520enhance%2520the%2520reliability%2520and%2520scalability%2520of%250Aplanning%2520in%2520autonomous%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLM-Reasoning%20Models%20Replace%20Classical%20Planning%3F%20A%20Benchmark%20Study&entry.906535625=Kai%20Goebel%20and%20Patrik%20Zips&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20have%20sparked%20interest%20in%20their%0Apotential%20for%20robotic%20task%20planning.%20While%20these%20models%20demonstrate%20strong%0Agenerative%20capabilities%2C%20their%20effectiveness%20in%20producing%20structured%20and%0Aexecutable%20plans%20remains%20uncertain.%20This%20paper%20presents%20a%20systematic%20evaluation%0Aof%20a%20broad%20spectrum%20of%20current%20state%20of%20the%20art%20language%20models%2C%20each%20directly%0Aprompted%20using%20Planning%20Domain%20Definition%20Language%20domain%20and%20problem%20files%2C%0Aand%20compares%20their%20planning%20performance%20with%20the%20Fast%20Downward%20planner%20across%20a%0Avariety%20of%20benchmarks.%20In%20addition%20to%20measuring%20success%20rates%2C%20we%20assess%20how%0Afaithfully%20the%20generated%20plans%20translate%20into%20sequences%20of%20actions%20that%20can%0Aactually%20be%20executed%2C%20identifying%20both%20strengths%20and%20limitations%20of%20using%20these%0Amodels%20in%20this%20setting.%20Our%20findings%20show%20that%20while%20the%20models%20perform%20well%20on%0Asimpler%20planning%20tasks%2C%20they%20continue%20to%20struggle%20with%20more%20complex%20scenarios%0Athat%20require%20precise%20resource%20management%2C%20consistent%20state%20tracking%2C%20and%20strict%0Aconstraint%20compliance.%20These%20results%20underscore%20fundamental%20challenges%20in%0Aapplying%20language%20models%20to%20robotic%20planning%20in%20real%20world%20environments.%20By%0Aoutlining%20the%20gaps%20that%20emerge%20during%20execution%2C%20we%20aim%20to%20guide%20future%0Aresearch%20toward%20combined%20approaches%20that%20integrate%20language%20models%20with%0Aclassical%20planners%20in%20order%20to%20enhance%20the%20reliability%20and%20scalability%20of%0Aplanning%20in%20autonomous%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23589v1&entry.124074799=Read"},
{"title": "Neural-ANOVA: Analytical Model Decomposition using Automatic Integration", "author": "Steffen Limmer and Steffen Udluft and Clemens Otte", "abstract": "  The analysis of variance (ANOVA) decomposition offers a systematic method to\nunderstand the interaction effects that contribute to a specific decision\noutput. In this paper we introduce Neural-ANOVA, an approach to decompose\nneural networks into the sum of lower-order models using the functional ANOVA\ndecomposition. Our approach formulates a learning problem, which enables fast\nanalytical evaluation of integrals over subspaces that appear in the\ncalculation of the ANOVA decomposition. Finally, we conduct numerical\nexperiments to provide insights into the approximation properties compared to\nother regression approaches from the literature.\n", "link": "http://arxiv.org/abs/2408.12319v2", "date": "2025-07-31", "relevancy": 2.1271, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural-ANOVA%3A%20Analytical%20Model%20Decomposition%20using%20Automatic%20Integration&body=Title%3A%20Neural-ANOVA%3A%20Analytical%20Model%20Decomposition%20using%20Automatic%20Integration%0AAuthor%3A%20Steffen%20Limmer%20and%20Steffen%20Udluft%20and%20Clemens%20Otte%0AAbstract%3A%20%20%20The%20analysis%20of%20variance%20%28ANOVA%29%20decomposition%20offers%20a%20systematic%20method%20to%0Aunderstand%20the%20interaction%20effects%20that%20contribute%20to%20a%20specific%20decision%0Aoutput.%20In%20this%20paper%20we%20introduce%20Neural-ANOVA%2C%20an%20approach%20to%20decompose%0Aneural%20networks%20into%20the%20sum%20of%20lower-order%20models%20using%20the%20functional%20ANOVA%0Adecomposition.%20Our%20approach%20formulates%20a%20learning%20problem%2C%20which%20enables%20fast%0Aanalytical%20evaluation%20of%20integrals%20over%20subspaces%20that%20appear%20in%20the%0Acalculation%20of%20the%20ANOVA%20decomposition.%20Finally%2C%20we%20conduct%20numerical%0Aexperiments%20to%20provide%20insights%20into%20the%20approximation%20properties%20compared%20to%0Aother%20regression%20approaches%20from%20the%20literature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural-ANOVA%253A%2520Analytical%2520Model%2520Decomposition%2520using%2520Automatic%2520Integration%26entry.906535625%3DSteffen%2520Limmer%2520and%2520Steffen%2520Udluft%2520and%2520Clemens%2520Otte%26entry.1292438233%3D%2520%2520The%2520analysis%2520of%2520variance%2520%2528ANOVA%2529%2520decomposition%2520offers%2520a%2520systematic%2520method%2520to%250Aunderstand%2520the%2520interaction%2520effects%2520that%2520contribute%2520to%2520a%2520specific%2520decision%250Aoutput.%2520In%2520this%2520paper%2520we%2520introduce%2520Neural-ANOVA%252C%2520an%2520approach%2520to%2520decompose%250Aneural%2520networks%2520into%2520the%2520sum%2520of%2520lower-order%2520models%2520using%2520the%2520functional%2520ANOVA%250Adecomposition.%2520Our%2520approach%2520formulates%2520a%2520learning%2520problem%252C%2520which%2520enables%2520fast%250Aanalytical%2520evaluation%2520of%2520integrals%2520over%2520subspaces%2520that%2520appear%2520in%2520the%250Acalculation%2520of%2520the%2520ANOVA%2520decomposition.%2520Finally%252C%2520we%2520conduct%2520numerical%250Aexperiments%2520to%2520provide%2520insights%2520into%2520the%2520approximation%2520properties%2520compared%2520to%250Aother%2520regression%2520approaches%2520from%2520the%2520literature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural-ANOVA%3A%20Analytical%20Model%20Decomposition%20using%20Automatic%20Integration&entry.906535625=Steffen%20Limmer%20and%20Steffen%20Udluft%20and%20Clemens%20Otte&entry.1292438233=%20%20The%20analysis%20of%20variance%20%28ANOVA%29%20decomposition%20offers%20a%20systematic%20method%20to%0Aunderstand%20the%20interaction%20effects%20that%20contribute%20to%20a%20specific%20decision%0Aoutput.%20In%20this%20paper%20we%20introduce%20Neural-ANOVA%2C%20an%20approach%20to%20decompose%0Aneural%20networks%20into%20the%20sum%20of%20lower-order%20models%20using%20the%20functional%20ANOVA%0Adecomposition.%20Our%20approach%20formulates%20a%20learning%20problem%2C%20which%20enables%20fast%0Aanalytical%20evaluation%20of%20integrals%20over%20subspaces%20that%20appear%20in%20the%0Acalculation%20of%20the%20ANOVA%20decomposition.%20Finally%2C%20we%20conduct%20numerical%0Aexperiments%20to%20provide%20insights%20into%20the%20approximation%20properties%20compared%20to%0Aother%20regression%20approaches%20from%20the%20literature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12319v2&entry.124074799=Read"},
{"title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "author": "Miaosen Zhang and Ziqiang Xu and Jialiang Zhu and Qi Dai and Kai Qiu and Yifan Yang and Chong Luo and Tianyi Chen and Justin Wagle and Tim Franklin and Baining Guo", "abstract": "  With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the \\textbf{Phi-Ground} model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder $10B$ parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on\nScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\n\\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}\n", "link": "http://arxiv.org/abs/2507.23779v1", "date": "2025-07-31", "relevancy": 2.1183, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5456}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Phi-Ground%20Tech%20Report%3A%20Advancing%20Perception%20in%20GUI%20Grounding&body=Title%3A%20Phi-Ground%20Tech%20Report%3A%20Advancing%20Perception%20in%20GUI%20Grounding%0AAuthor%3A%20Miaosen%20Zhang%20and%20Ziqiang%20Xu%20and%20Jialiang%20Zhu%20and%20Qi%20Dai%20and%20Kai%20Qiu%20and%20Yifan%20Yang%20and%20Chong%20Luo%20and%20Tianyi%20Chen%20and%20Justin%20Wagle%20and%20Tim%20Franklin%20and%20Baining%20Guo%0AAbstract%3A%20%20%20With%20the%20development%20of%20multimodal%20reasoning%20models%2C%20Computer%20Use%20Agents%0A%28CUAs%29%2C%20akin%20to%20Jarvis%20from%20%5Ctextit%7B%22Iron%20Man%22%7D%2C%20are%20becoming%20a%20reality.%20GUI%0Agrounding%20is%20a%20core%20component%20for%20CUAs%20to%20execute%20actual%20actions%2C%20similar%20to%0Amechanical%20control%20in%20robotics%2C%20and%20it%20directly%20leads%20to%20the%20success%20or%20failure%0Aof%20the%20system.%20It%20determines%20actions%20such%20as%20clicking%20and%20typing%2C%20as%20well%20as%0Arelated%20parameters%20like%20the%20coordinates%20for%20clicks.%20Current%20end-to-end%0Agrounding%20models%20still%20achieve%20less%20than%2065%5C%25%20accuracy%20on%20challenging%0Abenchmarks%20like%20ScreenSpot-pro%20and%20UI-Vision%2C%20indicating%20they%20are%20far%20from%0Abeing%20ready%20for%20deployment.%20%25%20%2C%20as%20a%20single%20misclick%20can%20result%20in%20unacceptable%0Aconsequences.%20In%20this%20work%2C%20we%20conduct%20an%20empirical%20study%20on%20the%20training%20of%0Agrounding%20models%2C%20examining%20details%20from%20data%20collection%20to%20model%20training.%0AUltimately%2C%20we%20developed%20the%20%5Ctextbf%7BPhi-Ground%7D%20model%20family%2C%20which%20achieves%0Astate-of-the-art%20performance%20across%20all%20five%20grounding%20benchmarks%20for%20models%0Aunder%20%2410B%24%20parameters%20in%20agent%20settings.%20In%20the%20end-to-end%20model%20setting%2C%20our%0Amodel%20still%20achieves%20SOTA%20results%20with%20scores%20of%20%5Ctextit%7B%5Ctextbf%7B43.2%7D%7D%20on%0AScreenSpot-pro%20and%20%5Ctextit%7B%5Ctextbf%7B27.2%7D%7D%20on%20UI-Vision.%20We%20believe%20that%20the%0Avarious%20details%20discussed%20in%20this%20paper%2C%20along%20with%20our%20successes%20and%20failures%2C%0Anot%20only%20clarify%20the%20construction%20of%20grounding%20models%20but%20also%20benefit%20other%0Aperception%20tasks.%20Project%20homepage%3A%0A%5Chref%7Bhttps%3A//zhangmiaosen2000.github.io/Phi-Ground/%7D%7Bhttps%3A//zhangmiaosen2000.github.io/Phi-Ground/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhi-Ground%2520Tech%2520Report%253A%2520Advancing%2520Perception%2520in%2520GUI%2520Grounding%26entry.906535625%3DMiaosen%2520Zhang%2520and%2520Ziqiang%2520Xu%2520and%2520Jialiang%2520Zhu%2520and%2520Qi%2520Dai%2520and%2520Kai%2520Qiu%2520and%2520Yifan%2520Yang%2520and%2520Chong%2520Luo%2520and%2520Tianyi%2520Chen%2520and%2520Justin%2520Wagle%2520and%2520Tim%2520Franklin%2520and%2520Baining%2520Guo%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520multimodal%2520reasoning%2520models%252C%2520Computer%2520Use%2520Agents%250A%2528CUAs%2529%252C%2520akin%2520to%2520Jarvis%2520from%2520%255Ctextit%257B%2522Iron%2520Man%2522%257D%252C%2520are%2520becoming%2520a%2520reality.%2520GUI%250Agrounding%2520is%2520a%2520core%2520component%2520for%2520CUAs%2520to%2520execute%2520actual%2520actions%252C%2520similar%2520to%250Amechanical%2520control%2520in%2520robotics%252C%2520and%2520it%2520directly%2520leads%2520to%2520the%2520success%2520or%2520failure%250Aof%2520the%2520system.%2520It%2520determines%2520actions%2520such%2520as%2520clicking%2520and%2520typing%252C%2520as%2520well%2520as%250Arelated%2520parameters%2520like%2520the%2520coordinates%2520for%2520clicks.%2520Current%2520end-to-end%250Agrounding%2520models%2520still%2520achieve%2520less%2520than%252065%255C%2525%2520accuracy%2520on%2520challenging%250Abenchmarks%2520like%2520ScreenSpot-pro%2520and%2520UI-Vision%252C%2520indicating%2520they%2520are%2520far%2520from%250Abeing%2520ready%2520for%2520deployment.%2520%2525%2520%252C%2520as%2520a%2520single%2520misclick%2520can%2520result%2520in%2520unacceptable%250Aconsequences.%2520In%2520this%2520work%252C%2520we%2520conduct%2520an%2520empirical%2520study%2520on%2520the%2520training%2520of%250Agrounding%2520models%252C%2520examining%2520details%2520from%2520data%2520collection%2520to%2520model%2520training.%250AUltimately%252C%2520we%2520developed%2520the%2520%255Ctextbf%257BPhi-Ground%257D%2520model%2520family%252C%2520which%2520achieves%250Astate-of-the-art%2520performance%2520across%2520all%2520five%2520grounding%2520benchmarks%2520for%2520models%250Aunder%2520%252410B%2524%2520parameters%2520in%2520agent%2520settings.%2520In%2520the%2520end-to-end%2520model%2520setting%252C%2520our%250Amodel%2520still%2520achieves%2520SOTA%2520results%2520with%2520scores%2520of%2520%255Ctextit%257B%255Ctextbf%257B43.2%257D%257D%2520on%250AScreenSpot-pro%2520and%2520%255Ctextit%257B%255Ctextbf%257B27.2%257D%257D%2520on%2520UI-Vision.%2520We%2520believe%2520that%2520the%250Avarious%2520details%2520discussed%2520in%2520this%2520paper%252C%2520along%2520with%2520our%2520successes%2520and%2520failures%252C%250Anot%2520only%2520clarify%2520the%2520construction%2520of%2520grounding%2520models%2520but%2520also%2520benefit%2520other%250Aperception%2520tasks.%2520Project%2520homepage%253A%250A%255Chref%257Bhttps%253A//zhangmiaosen2000.github.io/Phi-Ground/%257D%257Bhttps%253A//zhangmiaosen2000.github.io/Phi-Ground/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Phi-Ground%20Tech%20Report%3A%20Advancing%20Perception%20in%20GUI%20Grounding&entry.906535625=Miaosen%20Zhang%20and%20Ziqiang%20Xu%20and%20Jialiang%20Zhu%20and%20Qi%20Dai%20and%20Kai%20Qiu%20and%20Yifan%20Yang%20and%20Chong%20Luo%20and%20Tianyi%20Chen%20and%20Justin%20Wagle%20and%20Tim%20Franklin%20and%20Baining%20Guo&entry.1292438233=%20%20With%20the%20development%20of%20multimodal%20reasoning%20models%2C%20Computer%20Use%20Agents%0A%28CUAs%29%2C%20akin%20to%20Jarvis%20from%20%5Ctextit%7B%22Iron%20Man%22%7D%2C%20are%20becoming%20a%20reality.%20GUI%0Agrounding%20is%20a%20core%20component%20for%20CUAs%20to%20execute%20actual%20actions%2C%20similar%20to%0Amechanical%20control%20in%20robotics%2C%20and%20it%20directly%20leads%20to%20the%20success%20or%20failure%0Aof%20the%20system.%20It%20determines%20actions%20such%20as%20clicking%20and%20typing%2C%20as%20well%20as%0Arelated%20parameters%20like%20the%20coordinates%20for%20clicks.%20Current%20end-to-end%0Agrounding%20models%20still%20achieve%20less%20than%2065%5C%25%20accuracy%20on%20challenging%0Abenchmarks%20like%20ScreenSpot-pro%20and%20UI-Vision%2C%20indicating%20they%20are%20far%20from%0Abeing%20ready%20for%20deployment.%20%25%20%2C%20as%20a%20single%20misclick%20can%20result%20in%20unacceptable%0Aconsequences.%20In%20this%20work%2C%20we%20conduct%20an%20empirical%20study%20on%20the%20training%20of%0Agrounding%20models%2C%20examining%20details%20from%20data%20collection%20to%20model%20training.%0AUltimately%2C%20we%20developed%20the%20%5Ctextbf%7BPhi-Ground%7D%20model%20family%2C%20which%20achieves%0Astate-of-the-art%20performance%20across%20all%20five%20grounding%20benchmarks%20for%20models%0Aunder%20%2410B%24%20parameters%20in%20agent%20settings.%20In%20the%20end-to-end%20model%20setting%2C%20our%0Amodel%20still%20achieves%20SOTA%20results%20with%20scores%20of%20%5Ctextit%7B%5Ctextbf%7B43.2%7D%7D%20on%0AScreenSpot-pro%20and%20%5Ctextit%7B%5Ctextbf%7B27.2%7D%7D%20on%20UI-Vision.%20We%20believe%20that%20the%0Avarious%20details%20discussed%20in%20this%20paper%2C%20along%20with%20our%20successes%20and%20failures%2C%0Anot%20only%20clarify%20the%20construction%20of%20grounding%20models%20but%20also%20benefit%20other%0Aperception%20tasks.%20Project%20homepage%3A%0A%5Chref%7Bhttps%3A//zhangmiaosen2000.github.io/Phi-Ground/%7D%7Bhttps%3A//zhangmiaosen2000.github.io/Phi-Ground/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23779v1&entry.124074799=Read"},
{"title": "Transparent AI: The Case for Interpretability and Explainability", "author": "Dhanesh Ramachandram and Himanshu Joshi and Judy Zhu and Dhari Gandhi and Lucas Hartman and Ananya Raval", "abstract": "  As artificial intelligence systems increasingly inform high-stakes decisions\nacross sectors, transparency has become foundational to responsible and\ntrustworthy AI implementation. Leveraging our role as a leading institute in\nadvancing AI research and enabling industry adoption, we present key insights\nand lessons learned from practical interpretability applications across diverse\ndomains. This paper offers actionable strategies and implementation guidance\ntailored to organizations at varying stages of AI maturity, emphasizing the\nintegration of interpretability as a core design principle rather than a\nretrospective add-on.\n", "link": "http://arxiv.org/abs/2507.23535v1", "date": "2025-07-31", "relevancy": 2.1117, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transparent%20AI%3A%20The%20Case%20for%20Interpretability%20and%20Explainability&body=Title%3A%20Transparent%20AI%3A%20The%20Case%20for%20Interpretability%20and%20Explainability%0AAuthor%3A%20Dhanesh%20Ramachandram%20and%20Himanshu%20Joshi%20and%20Judy%20Zhu%20and%20Dhari%20Gandhi%20and%20Lucas%20Hartman%20and%20Ananya%20Raval%0AAbstract%3A%20%20%20As%20artificial%20intelligence%20systems%20increasingly%20inform%20high-stakes%20decisions%0Aacross%20sectors%2C%20transparency%20has%20become%20foundational%20to%20responsible%20and%0Atrustworthy%20AI%20implementation.%20Leveraging%20our%20role%20as%20a%20leading%20institute%20in%0Aadvancing%20AI%20research%20and%20enabling%20industry%20adoption%2C%20we%20present%20key%20insights%0Aand%20lessons%20learned%20from%20practical%20interpretability%20applications%20across%20diverse%0Adomains.%20This%20paper%20offers%20actionable%20strategies%20and%20implementation%20guidance%0Atailored%20to%20organizations%20at%20varying%20stages%20of%20AI%20maturity%2C%20emphasizing%20the%0Aintegration%20of%20interpretability%20as%20a%20core%20design%20principle%20rather%20than%20a%0Aretrospective%20add-on.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransparent%2520AI%253A%2520The%2520Case%2520for%2520Interpretability%2520and%2520Explainability%26entry.906535625%3DDhanesh%2520Ramachandram%2520and%2520Himanshu%2520Joshi%2520and%2520Judy%2520Zhu%2520and%2520Dhari%2520Gandhi%2520and%2520Lucas%2520Hartman%2520and%2520Ananya%2520Raval%26entry.1292438233%3D%2520%2520As%2520artificial%2520intelligence%2520systems%2520increasingly%2520inform%2520high-stakes%2520decisions%250Aacross%2520sectors%252C%2520transparency%2520has%2520become%2520foundational%2520to%2520responsible%2520and%250Atrustworthy%2520AI%2520implementation.%2520Leveraging%2520our%2520role%2520as%2520a%2520leading%2520institute%2520in%250Aadvancing%2520AI%2520research%2520and%2520enabling%2520industry%2520adoption%252C%2520we%2520present%2520key%2520insights%250Aand%2520lessons%2520learned%2520from%2520practical%2520interpretability%2520applications%2520across%2520diverse%250Adomains.%2520This%2520paper%2520offers%2520actionable%2520strategies%2520and%2520implementation%2520guidance%250Atailored%2520to%2520organizations%2520at%2520varying%2520stages%2520of%2520AI%2520maturity%252C%2520emphasizing%2520the%250Aintegration%2520of%2520interpretability%2520as%2520a%2520core%2520design%2520principle%2520rather%2520than%2520a%250Aretrospective%2520add-on.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transparent%20AI%3A%20The%20Case%20for%20Interpretability%20and%20Explainability&entry.906535625=Dhanesh%20Ramachandram%20and%20Himanshu%20Joshi%20and%20Judy%20Zhu%20and%20Dhari%20Gandhi%20and%20Lucas%20Hartman%20and%20Ananya%20Raval&entry.1292438233=%20%20As%20artificial%20intelligence%20systems%20increasingly%20inform%20high-stakes%20decisions%0Aacross%20sectors%2C%20transparency%20has%20become%20foundational%20to%20responsible%20and%0Atrustworthy%20AI%20implementation.%20Leveraging%20our%20role%20as%20a%20leading%20institute%20in%0Aadvancing%20AI%20research%20and%20enabling%20industry%20adoption%2C%20we%20present%20key%20insights%0Aand%20lessons%20learned%20from%20practical%20interpretability%20applications%20across%20diverse%0Adomains.%20This%20paper%20offers%20actionable%20strategies%20and%20implementation%20guidance%0Atailored%20to%20organizations%20at%20varying%20stages%20of%20AI%20maturity%2C%20emphasizing%20the%0Aintegration%20of%20interpretability%20as%20a%20core%20design%20principle%20rather%20than%20a%0Aretrospective%20add-on.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23535v1&entry.124074799=Read"},
{"title": "Causal Reasoning in Pieces: Modular In-Context Learning for Causal\n  Discovery", "author": "Kacper Kadziolka and Saber Salehkaleybar", "abstract": "  Causal inference remains a fundamental challenge for large language models.\nRecent advances in internal reasoning with large language models have sparked\ninterest in whether state-of-the-art reasoning models can robustly perform\ncausal discovery-a task where conventional models often suffer from severe\noverfitting and near-random performance under data perturbations. We study\ncausal discovery on the Corr2Cause benchmark using the emergent OpenAI's\no-series and DeepSeek-R model families and find that these reasoning-first\narchitectures achieve significantly greater native gains than prior approaches.\nTo capitalize on these strengths, we introduce a modular in-context pipeline\ninspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding\nnearly three-fold improvements over conventional baselines. We further probe\nthe pipeline's impact by analyzing reasoning chain length, complexity, and\nconducting qualitative and quantitative comparisons between conventional and\nreasoning models. Our findings suggest that while advanced reasoning models\nrepresent a substantial leap forward, carefully structured in-context\nframeworks are essential to maximize their capabilities and offer a\ngeneralizable blueprint for causal discovery across diverse domains.\n", "link": "http://arxiv.org/abs/2507.23488v1", "date": "2025-07-31", "relevancy": 2.0964, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Reasoning%20in%20Pieces%3A%20Modular%20In-Context%20Learning%20for%20Causal%0A%20%20Discovery&body=Title%3A%20Causal%20Reasoning%20in%20Pieces%3A%20Modular%20In-Context%20Learning%20for%20Causal%0A%20%20Discovery%0AAuthor%3A%20Kacper%20Kadziolka%20and%20Saber%20Salehkaleybar%0AAbstract%3A%20%20%20Causal%20inference%20remains%20a%20fundamental%20challenge%20for%20large%20language%20models.%0ARecent%20advances%20in%20internal%20reasoning%20with%20large%20language%20models%20have%20sparked%0Ainterest%20in%20whether%20state-of-the-art%20reasoning%20models%20can%20robustly%20perform%0Acausal%20discovery-a%20task%20where%20conventional%20models%20often%20suffer%20from%20severe%0Aoverfitting%20and%20near-random%20performance%20under%20data%20perturbations.%20We%20study%0Acausal%20discovery%20on%20the%20Corr2Cause%20benchmark%20using%20the%20emergent%20OpenAI%27s%0Ao-series%20and%20DeepSeek-R%20model%20families%20and%20find%20that%20these%20reasoning-first%0Aarchitectures%20achieve%20significantly%20greater%20native%20gains%20than%20prior%20approaches.%0ATo%20capitalize%20on%20these%20strengths%2C%20we%20introduce%20a%20modular%20in-context%20pipeline%0Ainspired%20by%20the%20Tree-of-Thoughts%20and%20Chain-of-Thoughts%20methodologies%2C%20yielding%0Anearly%20three-fold%20improvements%20over%20conventional%20baselines.%20We%20further%20probe%0Athe%20pipeline%27s%20impact%20by%20analyzing%20reasoning%20chain%20length%2C%20complexity%2C%20and%0Aconducting%20qualitative%20and%20quantitative%20comparisons%20between%20conventional%20and%0Areasoning%20models.%20Our%20findings%20suggest%20that%20while%20advanced%20reasoning%20models%0Arepresent%20a%20substantial%20leap%20forward%2C%20carefully%20structured%20in-context%0Aframeworks%20are%20essential%20to%20maximize%20their%20capabilities%20and%20offer%20a%0Ageneralizable%20blueprint%20for%20causal%20discovery%20across%20diverse%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Reasoning%2520in%2520Pieces%253A%2520Modular%2520In-Context%2520Learning%2520for%2520Causal%250A%2520%2520Discovery%26entry.906535625%3DKacper%2520Kadziolka%2520and%2520Saber%2520Salehkaleybar%26entry.1292438233%3D%2520%2520Causal%2520inference%2520remains%2520a%2520fundamental%2520challenge%2520for%2520large%2520language%2520models.%250ARecent%2520advances%2520in%2520internal%2520reasoning%2520with%2520large%2520language%2520models%2520have%2520sparked%250Ainterest%2520in%2520whether%2520state-of-the-art%2520reasoning%2520models%2520can%2520robustly%2520perform%250Acausal%2520discovery-a%2520task%2520where%2520conventional%2520models%2520often%2520suffer%2520from%2520severe%250Aoverfitting%2520and%2520near-random%2520performance%2520under%2520data%2520perturbations.%2520We%2520study%250Acausal%2520discovery%2520on%2520the%2520Corr2Cause%2520benchmark%2520using%2520the%2520emergent%2520OpenAI%2527s%250Ao-series%2520and%2520DeepSeek-R%2520model%2520families%2520and%2520find%2520that%2520these%2520reasoning-first%250Aarchitectures%2520achieve%2520significantly%2520greater%2520native%2520gains%2520than%2520prior%2520approaches.%250ATo%2520capitalize%2520on%2520these%2520strengths%252C%2520we%2520introduce%2520a%2520modular%2520in-context%2520pipeline%250Ainspired%2520by%2520the%2520Tree-of-Thoughts%2520and%2520Chain-of-Thoughts%2520methodologies%252C%2520yielding%250Anearly%2520three-fold%2520improvements%2520over%2520conventional%2520baselines.%2520We%2520further%2520probe%250Athe%2520pipeline%2527s%2520impact%2520by%2520analyzing%2520reasoning%2520chain%2520length%252C%2520complexity%252C%2520and%250Aconducting%2520qualitative%2520and%2520quantitative%2520comparisons%2520between%2520conventional%2520and%250Areasoning%2520models.%2520Our%2520findings%2520suggest%2520that%2520while%2520advanced%2520reasoning%2520models%250Arepresent%2520a%2520substantial%2520leap%2520forward%252C%2520carefully%2520structured%2520in-context%250Aframeworks%2520are%2520essential%2520to%2520maximize%2520their%2520capabilities%2520and%2520offer%2520a%250Ageneralizable%2520blueprint%2520for%2520causal%2520discovery%2520across%2520diverse%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Reasoning%20in%20Pieces%3A%20Modular%20In-Context%20Learning%20for%20Causal%0A%20%20Discovery&entry.906535625=Kacper%20Kadziolka%20and%20Saber%20Salehkaleybar&entry.1292438233=%20%20Causal%20inference%20remains%20a%20fundamental%20challenge%20for%20large%20language%20models.%0ARecent%20advances%20in%20internal%20reasoning%20with%20large%20language%20models%20have%20sparked%0Ainterest%20in%20whether%20state-of-the-art%20reasoning%20models%20can%20robustly%20perform%0Acausal%20discovery-a%20task%20where%20conventional%20models%20often%20suffer%20from%20severe%0Aoverfitting%20and%20near-random%20performance%20under%20data%20perturbations.%20We%20study%0Acausal%20discovery%20on%20the%20Corr2Cause%20benchmark%20using%20the%20emergent%20OpenAI%27s%0Ao-series%20and%20DeepSeek-R%20model%20families%20and%20find%20that%20these%20reasoning-first%0Aarchitectures%20achieve%20significantly%20greater%20native%20gains%20than%20prior%20approaches.%0ATo%20capitalize%20on%20these%20strengths%2C%20we%20introduce%20a%20modular%20in-context%20pipeline%0Ainspired%20by%20the%20Tree-of-Thoughts%20and%20Chain-of-Thoughts%20methodologies%2C%20yielding%0Anearly%20three-fold%20improvements%20over%20conventional%20baselines.%20We%20further%20probe%0Athe%20pipeline%27s%20impact%20by%20analyzing%20reasoning%20chain%20length%2C%20complexity%2C%20and%0Aconducting%20qualitative%20and%20quantitative%20comparisons%20between%20conventional%20and%0Areasoning%20models.%20Our%20findings%20suggest%20that%20while%20advanced%20reasoning%20models%0Arepresent%20a%20substantial%20leap%20forward%2C%20carefully%20structured%20in-context%0Aframeworks%20are%20essential%20to%20maximize%20their%20capabilities%20and%20offer%20a%0Ageneralizable%20blueprint%20for%20causal%20discovery%20across%20diverse%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23488v1&entry.124074799=Read"},
{"title": "FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free\n  Training Framework for Spiking Neural Networks", "author": "Changqing Xu and Ziqiang Yang and Yi Liu and Xinfang Liao and Guiqi Mo and Hao Zeng and Yintang Yang", "abstract": "  Spiking Neural Networks (SNNs) offer a biologically plausible framework for\nenergy-efficient neuromorphic computing. However, it is a challenge to train\nSNNs due to their non-differentiability, efficiently. Existing gradient\napproximation approaches frequently sacrifice accuracy and face deployment\nlimitations on edge devices due to the substantial computational requirements\nof backpropagation. To address these challenges, we propose a Forward-Forward\n(FF) based gradient approximation-free training framework for Spiking Neural\nNetworks, which treats spiking activations as black-box modules, thereby\neliminating the need for gradient approximation while significantly reducing\ncomputational complexity. Furthermore, we introduce a class-aware complexity\nadaptation mechanism that dynamically optimizes the loss function based on\ninter-class difficulty metrics, enabling efficient allocation of network\nresources across different categories. Experimental results demonstrate that\nour proposed training framework achieves test accuracies of 99.58%, 92.13%, and\n75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,\nsurpassing all existing FF-based SNN approaches. Additionally, our proposed\nmethod exhibits significant advantages in terms of memory access and\ncomputational power consumption.\n", "link": "http://arxiv.org/abs/2507.23643v1", "date": "2025-07-31", "relevancy": 2.092, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5604}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5014}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FFGAF-SNN%3A%20The%20Forward-Forward%20Based%20Gradient%20Approximation%20Free%0A%20%20Training%20Framework%20for%20Spiking%20Neural%20Networks&body=Title%3A%20FFGAF-SNN%3A%20The%20Forward-Forward%20Based%20Gradient%20Approximation%20Free%0A%20%20Training%20Framework%20for%20Spiking%20Neural%20Networks%0AAuthor%3A%20Changqing%20Xu%20and%20Ziqiang%20Yang%20and%20Yi%20Liu%20and%20Xinfang%20Liao%20and%20Guiqi%20Mo%20and%20Hao%20Zeng%20and%20Yintang%20Yang%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20offer%20a%20biologically%20plausible%20framework%20for%0Aenergy-efficient%20neuromorphic%20computing.%20However%2C%20it%20is%20a%20challenge%20to%20train%0ASNNs%20due%20to%20their%20non-differentiability%2C%20efficiently.%20Existing%20gradient%0Aapproximation%20approaches%20frequently%20sacrifice%20accuracy%20and%20face%20deployment%0Alimitations%20on%20edge%20devices%20due%20to%20the%20substantial%20computational%20requirements%0Aof%20backpropagation.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Forward-Forward%0A%28FF%29%20based%20gradient%20approximation-free%20training%20framework%20for%20Spiking%20Neural%0ANetworks%2C%20which%20treats%20spiking%20activations%20as%20black-box%20modules%2C%20thereby%0Aeliminating%20the%20need%20for%20gradient%20approximation%20while%20significantly%20reducing%0Acomputational%20complexity.%20Furthermore%2C%20we%20introduce%20a%20class-aware%20complexity%0Aadaptation%20mechanism%20that%20dynamically%20optimizes%20the%20loss%20function%20based%20on%0Ainter-class%20difficulty%20metrics%2C%20enabling%20efficient%20allocation%20of%20network%0Aresources%20across%20different%20categories.%20Experimental%20results%20demonstrate%20that%0Aour%20proposed%20training%20framework%20achieves%20test%20accuracies%20of%2099.58%25%2C%2092.13%25%2C%20and%0A75.64%25%20on%20the%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR-10%20datasets%2C%20respectively%2C%0Asurpassing%20all%20existing%20FF-based%20SNN%20approaches.%20Additionally%2C%20our%20proposed%0Amethod%20exhibits%20significant%20advantages%20in%20terms%20of%20memory%20access%20and%0Acomputational%20power%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFFGAF-SNN%253A%2520The%2520Forward-Forward%2520Based%2520Gradient%2520Approximation%2520Free%250A%2520%2520Training%2520Framework%2520for%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DChangqing%2520Xu%2520and%2520Ziqiang%2520Yang%2520and%2520Yi%2520Liu%2520and%2520Xinfang%2520Liao%2520and%2520Guiqi%2520Mo%2520and%2520Hao%2520Zeng%2520and%2520Yintang%2520Yang%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520offer%2520a%2520biologically%2520plausible%2520framework%2520for%250Aenergy-efficient%2520neuromorphic%2520computing.%2520However%252C%2520it%2520is%2520a%2520challenge%2520to%2520train%250ASNNs%2520due%2520to%2520their%2520non-differentiability%252C%2520efficiently.%2520Existing%2520gradient%250Aapproximation%2520approaches%2520frequently%2520sacrifice%2520accuracy%2520and%2520face%2520deployment%250Alimitations%2520on%2520edge%2520devices%2520due%2520to%2520the%2520substantial%2520computational%2520requirements%250Aof%2520backpropagation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520Forward-Forward%250A%2528FF%2529%2520based%2520gradient%2520approximation-free%2520training%2520framework%2520for%2520Spiking%2520Neural%250ANetworks%252C%2520which%2520treats%2520spiking%2520activations%2520as%2520black-box%2520modules%252C%2520thereby%250Aeliminating%2520the%2520need%2520for%2520gradient%2520approximation%2520while%2520significantly%2520reducing%250Acomputational%2520complexity.%2520Furthermore%252C%2520we%2520introduce%2520a%2520class-aware%2520complexity%250Aadaptation%2520mechanism%2520that%2520dynamically%2520optimizes%2520the%2520loss%2520function%2520based%2520on%250Ainter-class%2520difficulty%2520metrics%252C%2520enabling%2520efficient%2520allocation%2520of%2520network%250Aresources%2520across%2520different%2520categories.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520proposed%2520training%2520framework%2520achieves%2520test%2520accuracies%2520of%252099.58%2525%252C%252092.13%2525%252C%2520and%250A75.64%2525%2520on%2520the%2520MNIST%252C%2520Fashion-MNIST%252C%2520and%2520CIFAR-10%2520datasets%252C%2520respectively%252C%250Asurpassing%2520all%2520existing%2520FF-based%2520SNN%2520approaches.%2520Additionally%252C%2520our%2520proposed%250Amethod%2520exhibits%2520significant%2520advantages%2520in%2520terms%2520of%2520memory%2520access%2520and%250Acomputational%2520power%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FFGAF-SNN%3A%20The%20Forward-Forward%20Based%20Gradient%20Approximation%20Free%0A%20%20Training%20Framework%20for%20Spiking%20Neural%20Networks&entry.906535625=Changqing%20Xu%20and%20Ziqiang%20Yang%20and%20Yi%20Liu%20and%20Xinfang%20Liao%20and%20Guiqi%20Mo%20and%20Hao%20Zeng%20and%20Yintang%20Yang&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20offer%20a%20biologically%20plausible%20framework%20for%0Aenergy-efficient%20neuromorphic%20computing.%20However%2C%20it%20is%20a%20challenge%20to%20train%0ASNNs%20due%20to%20their%20non-differentiability%2C%20efficiently.%20Existing%20gradient%0Aapproximation%20approaches%20frequently%20sacrifice%20accuracy%20and%20face%20deployment%0Alimitations%20on%20edge%20devices%20due%20to%20the%20substantial%20computational%20requirements%0Aof%20backpropagation.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20Forward-Forward%0A%28FF%29%20based%20gradient%20approximation-free%20training%20framework%20for%20Spiking%20Neural%0ANetworks%2C%20which%20treats%20spiking%20activations%20as%20black-box%20modules%2C%20thereby%0Aeliminating%20the%20need%20for%20gradient%20approximation%20while%20significantly%20reducing%0Acomputational%20complexity.%20Furthermore%2C%20we%20introduce%20a%20class-aware%20complexity%0Aadaptation%20mechanism%20that%20dynamically%20optimizes%20the%20loss%20function%20based%20on%0Ainter-class%20difficulty%20metrics%2C%20enabling%20efficient%20allocation%20of%20network%0Aresources%20across%20different%20categories.%20Experimental%20results%20demonstrate%20that%0Aour%20proposed%20training%20framework%20achieves%20test%20accuracies%20of%2099.58%25%2C%2092.13%25%2C%20and%0A75.64%25%20on%20the%20MNIST%2C%20Fashion-MNIST%2C%20and%20CIFAR-10%20datasets%2C%20respectively%2C%0Asurpassing%20all%20existing%20FF-based%20SNN%20approaches.%20Additionally%2C%20our%20proposed%0Amethod%20exhibits%20significant%20advantages%20in%20terms%20of%20memory%20access%20and%0Acomputational%20power%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23643v1&entry.124074799=Read"},
{"title": "CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in\n  Complex Scenes", "author": "Bin Xie and Congxuan Zhang and Fagan Wang and Peng Liu and Feng Lu and Zhen Chen and Weiming Hu", "abstract": "  The widespread application of Unmanned Aerial Vehicles (UAVs) has raised\nserious public safety and privacy concerns, making UAV perception crucial for\nanti-UAV tasks. However, existing UAV tracking datasets predominantly feature\nconspicuous objects and lack diversity in scene complexity and attribute\nrepresentation, limiting their applicability to real-world scenarios. To\novercome these limitations, we present the CST Anti-UAV, a new thermal infrared\ndataset specifically designed for Single Object Tracking (SOT) in Complex\nScenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k\nhigh-quality bounding box annotations, highlighting two key properties: a\nsignificant number of tiny-sized UAV targets and the diverse and complex\nscenes. To the best of our knowledge, CST Anti-UAV is the first dataset to\nincorporate complete manual frame-level attribute annotations, enabling precise\nevaluations under varied challenges. To conduct an in-depth performance\nanalysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed\ndataset. Experimental results demonstrate that tracking tiny UAVs in complex\nenvironments remains a challenge, as the state-of-the-art method achieves only\n35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410\ndataset. These findings underscore the limitations of existing benchmarks and\nthe need for further advancements in UAV tracking research. The CST Anti-UAV\nbenchmark is about to be publicly released, which not only fosters the\ndevelopment of more robust SOT methods but also drives innovation in anti-UAV\nsystems.\n", "link": "http://arxiv.org/abs/2507.23473v1", "date": "2025-07-31", "relevancy": 2.0879, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5621}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5197}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CST%20Anti-UAV%3A%20A%20Thermal%20Infrared%20Benchmark%20for%20Tiny%20UAV%20Tracking%20in%0A%20%20Complex%20Scenes&body=Title%3A%20CST%20Anti-UAV%3A%20A%20Thermal%20Infrared%20Benchmark%20for%20Tiny%20UAV%20Tracking%20in%0A%20%20Complex%20Scenes%0AAuthor%3A%20Bin%20Xie%20and%20Congxuan%20Zhang%20and%20Fagan%20Wang%20and%20Peng%20Liu%20and%20Feng%20Lu%20and%20Zhen%20Chen%20and%20Weiming%20Hu%0AAbstract%3A%20%20%20The%20widespread%20application%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20has%20raised%0Aserious%20public%20safety%20and%20privacy%20concerns%2C%20making%20UAV%20perception%20crucial%20for%0Aanti-UAV%20tasks.%20However%2C%20existing%20UAV%20tracking%20datasets%20predominantly%20feature%0Aconspicuous%20objects%20and%20lack%20diversity%20in%20scene%20complexity%20and%20attribute%0Arepresentation%2C%20limiting%20their%20applicability%20to%20real-world%20scenarios.%20To%0Aovercome%20these%20limitations%2C%20we%20present%20the%20CST%20Anti-UAV%2C%20a%20new%20thermal%20infrared%0Adataset%20specifically%20designed%20for%20Single%20Object%20Tracking%20%28SOT%29%20in%20Complex%0AScenes%20with%20Tiny%20UAVs%20%28CST%29.%20It%20contains%20220%20video%20sequences%20with%20over%20240k%0Ahigh-quality%20bounding%20box%20annotations%2C%20highlighting%20two%20key%20properties%3A%20a%0Asignificant%20number%20of%20tiny-sized%20UAV%20targets%20and%20the%20diverse%20and%20complex%0Ascenes.%20To%20the%20best%20of%20our%20knowledge%2C%20CST%20Anti-UAV%20is%20the%20first%20dataset%20to%0Aincorporate%20complete%20manual%20frame-level%20attribute%20annotations%2C%20enabling%20precise%0Aevaluations%20under%20varied%20challenges.%20To%20conduct%20an%20in-depth%20performance%0Aanalysis%20for%20CST%20Anti-UAV%2C%20we%20evaluate%2020%20existing%20SOT%20methods%20on%20the%20proposed%0Adataset.%20Experimental%20results%20demonstrate%20that%20tracking%20tiny%20UAVs%20in%20complex%0Aenvironments%20remains%20a%20challenge%2C%20as%20the%20state-of-the-art%20method%20achieves%20only%0A35.92%25%20state%20accuracy%2C%20much%20lower%20than%20the%2067.69%25%20observed%20on%20the%20Anti-UAV410%0Adataset.%20These%20findings%20underscore%20the%20limitations%20of%20existing%20benchmarks%20and%0Athe%20need%20for%20further%20advancements%20in%20UAV%20tracking%20research.%20The%20CST%20Anti-UAV%0Abenchmark%20is%20about%20to%20be%20publicly%20released%2C%20which%20not%20only%20fosters%20the%0Adevelopment%20of%20more%20robust%20SOT%20methods%20but%20also%20drives%20innovation%20in%20anti-UAV%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCST%2520Anti-UAV%253A%2520A%2520Thermal%2520Infrared%2520Benchmark%2520for%2520Tiny%2520UAV%2520Tracking%2520in%250A%2520%2520Complex%2520Scenes%26entry.906535625%3DBin%2520Xie%2520and%2520Congxuan%2520Zhang%2520and%2520Fagan%2520Wang%2520and%2520Peng%2520Liu%2520and%2520Feng%2520Lu%2520and%2520Zhen%2520Chen%2520and%2520Weiming%2520Hu%26entry.1292438233%3D%2520%2520The%2520widespread%2520application%2520of%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520has%2520raised%250Aserious%2520public%2520safety%2520and%2520privacy%2520concerns%252C%2520making%2520UAV%2520perception%2520crucial%2520for%250Aanti-UAV%2520tasks.%2520However%252C%2520existing%2520UAV%2520tracking%2520datasets%2520predominantly%2520feature%250Aconspicuous%2520objects%2520and%2520lack%2520diversity%2520in%2520scene%2520complexity%2520and%2520attribute%250Arepresentation%252C%2520limiting%2520their%2520applicability%2520to%2520real-world%2520scenarios.%2520To%250Aovercome%2520these%2520limitations%252C%2520we%2520present%2520the%2520CST%2520Anti-UAV%252C%2520a%2520new%2520thermal%2520infrared%250Adataset%2520specifically%2520designed%2520for%2520Single%2520Object%2520Tracking%2520%2528SOT%2529%2520in%2520Complex%250AScenes%2520with%2520Tiny%2520UAVs%2520%2528CST%2529.%2520It%2520contains%2520220%2520video%2520sequences%2520with%2520over%2520240k%250Ahigh-quality%2520bounding%2520box%2520annotations%252C%2520highlighting%2520two%2520key%2520properties%253A%2520a%250Asignificant%2520number%2520of%2520tiny-sized%2520UAV%2520targets%2520and%2520the%2520diverse%2520and%2520complex%250Ascenes.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520CST%2520Anti-UAV%2520is%2520the%2520first%2520dataset%2520to%250Aincorporate%2520complete%2520manual%2520frame-level%2520attribute%2520annotations%252C%2520enabling%2520precise%250Aevaluations%2520under%2520varied%2520challenges.%2520To%2520conduct%2520an%2520in-depth%2520performance%250Aanalysis%2520for%2520CST%2520Anti-UAV%252C%2520we%2520evaluate%252020%2520existing%2520SOT%2520methods%2520on%2520the%2520proposed%250Adataset.%2520Experimental%2520results%2520demonstrate%2520that%2520tracking%2520tiny%2520UAVs%2520in%2520complex%250Aenvironments%2520remains%2520a%2520challenge%252C%2520as%2520the%2520state-of-the-art%2520method%2520achieves%2520only%250A35.92%2525%2520state%2520accuracy%252C%2520much%2520lower%2520than%2520the%252067.69%2525%2520observed%2520on%2520the%2520Anti-UAV410%250Adataset.%2520These%2520findings%2520underscore%2520the%2520limitations%2520of%2520existing%2520benchmarks%2520and%250Athe%2520need%2520for%2520further%2520advancements%2520in%2520UAV%2520tracking%2520research.%2520The%2520CST%2520Anti-UAV%250Abenchmark%2520is%2520about%2520to%2520be%2520publicly%2520released%252C%2520which%2520not%2520only%2520fosters%2520the%250Adevelopment%2520of%2520more%2520robust%2520SOT%2520methods%2520but%2520also%2520drives%2520innovation%2520in%2520anti-UAV%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CST%20Anti-UAV%3A%20A%20Thermal%20Infrared%20Benchmark%20for%20Tiny%20UAV%20Tracking%20in%0A%20%20Complex%20Scenes&entry.906535625=Bin%20Xie%20and%20Congxuan%20Zhang%20and%20Fagan%20Wang%20and%20Peng%20Liu%20and%20Feng%20Lu%20and%20Zhen%20Chen%20and%20Weiming%20Hu&entry.1292438233=%20%20The%20widespread%20application%20of%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20has%20raised%0Aserious%20public%20safety%20and%20privacy%20concerns%2C%20making%20UAV%20perception%20crucial%20for%0Aanti-UAV%20tasks.%20However%2C%20existing%20UAV%20tracking%20datasets%20predominantly%20feature%0Aconspicuous%20objects%20and%20lack%20diversity%20in%20scene%20complexity%20and%20attribute%0Arepresentation%2C%20limiting%20their%20applicability%20to%20real-world%20scenarios.%20To%0Aovercome%20these%20limitations%2C%20we%20present%20the%20CST%20Anti-UAV%2C%20a%20new%20thermal%20infrared%0Adataset%20specifically%20designed%20for%20Single%20Object%20Tracking%20%28SOT%29%20in%20Complex%0AScenes%20with%20Tiny%20UAVs%20%28CST%29.%20It%20contains%20220%20video%20sequences%20with%20over%20240k%0Ahigh-quality%20bounding%20box%20annotations%2C%20highlighting%20two%20key%20properties%3A%20a%0Asignificant%20number%20of%20tiny-sized%20UAV%20targets%20and%20the%20diverse%20and%20complex%0Ascenes.%20To%20the%20best%20of%20our%20knowledge%2C%20CST%20Anti-UAV%20is%20the%20first%20dataset%20to%0Aincorporate%20complete%20manual%20frame-level%20attribute%20annotations%2C%20enabling%20precise%0Aevaluations%20under%20varied%20challenges.%20To%20conduct%20an%20in-depth%20performance%0Aanalysis%20for%20CST%20Anti-UAV%2C%20we%20evaluate%2020%20existing%20SOT%20methods%20on%20the%20proposed%0Adataset.%20Experimental%20results%20demonstrate%20that%20tracking%20tiny%20UAVs%20in%20complex%0Aenvironments%20remains%20a%20challenge%2C%20as%20the%20state-of-the-art%20method%20achieves%20only%0A35.92%25%20state%20accuracy%2C%20much%20lower%20than%20the%2067.69%25%20observed%20on%20the%20Anti-UAV410%0Adataset.%20These%20findings%20underscore%20the%20limitations%20of%20existing%20benchmarks%20and%0Athe%20need%20for%20further%20advancements%20in%20UAV%20tracking%20research.%20The%20CST%20Anti-UAV%0Abenchmark%20is%20about%20to%20be%20publicly%20released%2C%20which%20not%20only%20fosters%20the%0Adevelopment%20of%20more%20robust%20SOT%20methods%20but%20also%20drives%20innovation%20in%20anti-UAV%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23473v1&entry.124074799=Read"},
{"title": "GraphRAG-R1: Graph Retrieval-Augmented Generation with\n  Process-Constrained Reinforcement Learning", "author": "Chuanyue Yu and Kuo Zhao and Yuhan Li and Heng Chang and Mingjian Feng and Xiangzhe Jiang and Yufei Sun and Jia Li and Yuzhi Zhang and Jianxin Li and Ziwei Zhang", "abstract": "  Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness\nin enhancing the reasoning abilities of LLMs by leveraging graph structures for\nknowledge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottlenecks when\nhandling complex problems that require multi-hop reasoning, as their query and\nretrieval phases are largely based on pre-defined heuristics and do not fully\nutilize the reasoning potentials of LLMs. To address this problem, we propose\nGraphRAG-R1, an adaptive GraphRAG framework by training LLMs with\nprocess-constrained outcome-based reinforcement learning (RL) to enhance the\nmulti-hop reasoning ability. Our method can decompose complex problems,\nautonomously invoke retrieval tools to acquire necessary information, and\nperform effective reasoning. Specifically, we utilize a modified version of\nGroup Relative Policy Optimization (GRPO) that supports rollout-with-thinking\ncapability. Next, we design two process-constrained reward functions. To handle\nthe shallow retrieval problem, we design a Progressive Retrieval Attenuation\n(PRA) reward to encourage essential retrievals. Then, to handle the\nover-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the\nmodel performance with computational costs. We further design a phase-dependent\ntraining strategy, containing three training stages corresponding to cold start\nand these two rewards. Lastly, our method adopts a hybrid graph-textual\nretrieval to improve the reasoning capacity. Extensive experimental results\ndemonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex\nreasoning problems compared to state-of-the-art GraphRAG methods on both\nin-domain and out-of-domain datasets. Furthermore, our framework can be\nflexibly integrated with various existing retrieval methods, consistently\ndelivering performance improvements.\n", "link": "http://arxiv.org/abs/2507.23581v1", "date": "2025-07-31", "relevancy": 2.0851, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5852}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5142}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphRAG-R1%3A%20Graph%20Retrieval-Augmented%20Generation%20with%0A%20%20Process-Constrained%20Reinforcement%20Learning&body=Title%3A%20GraphRAG-R1%3A%20Graph%20Retrieval-Augmented%20Generation%20with%0A%20%20Process-Constrained%20Reinforcement%20Learning%0AAuthor%3A%20Chuanyue%20Yu%20and%20Kuo%20Zhao%20and%20Yuhan%20Li%20and%20Heng%20Chang%20and%20Mingjian%20Feng%20and%20Xiangzhe%20Jiang%20and%20Yufei%20Sun%20and%20Jia%20Li%20and%20Yuzhi%20Zhang%20and%20Jianxin%20Li%20and%20Ziwei%20Zhang%0AAbstract%3A%20%20%20Graph%20Retrieval-Augmented%20Generation%20%28GraphRAG%29%20has%20shown%20great%20effectiveness%0Ain%20enhancing%20the%20reasoning%20abilities%20of%20LLMs%20by%20leveraging%20graph%20structures%20for%0Aknowledge%20representation%20and%20modeling%20complex%20real-world%20relationships.%0AHowever%2C%20existing%20GraphRAG%20methods%20still%20face%20significant%20bottlenecks%20when%0Ahandling%20complex%20problems%20that%20require%20multi-hop%20reasoning%2C%20as%20their%20query%20and%0Aretrieval%20phases%20are%20largely%20based%20on%20pre-defined%20heuristics%20and%20do%20not%20fully%0Autilize%20the%20reasoning%20potentials%20of%20LLMs.%20To%20address%20this%20problem%2C%20we%20propose%0AGraphRAG-R1%2C%20an%20adaptive%20GraphRAG%20framework%20by%20training%20LLMs%20with%0Aprocess-constrained%20outcome-based%20reinforcement%20learning%20%28RL%29%20to%20enhance%20the%0Amulti-hop%20reasoning%20ability.%20Our%20method%20can%20decompose%20complex%20problems%2C%0Aautonomously%20invoke%20retrieval%20tools%20to%20acquire%20necessary%20information%2C%20and%0Aperform%20effective%20reasoning.%20Specifically%2C%20we%20utilize%20a%20modified%20version%20of%0AGroup%20Relative%20Policy%20Optimization%20%28GRPO%29%20that%20supports%20rollout-with-thinking%0Acapability.%20Next%2C%20we%20design%20two%20process-constrained%20reward%20functions.%20To%20handle%0Athe%20shallow%20retrieval%20problem%2C%20we%20design%20a%20Progressive%20Retrieval%20Attenuation%0A%28PRA%29%20reward%20to%20encourage%20essential%20retrievals.%20Then%2C%20to%20handle%20the%0Aover-thinking%20problem%2C%20we%20design%20Cost-Aware%20F1%20%28CAF%29%20reward%20to%20balance%20the%0Amodel%20performance%20with%20computational%20costs.%20We%20further%20design%20a%20phase-dependent%0Atraining%20strategy%2C%20containing%20three%20training%20stages%20corresponding%20to%20cold%20start%0Aand%20these%20two%20rewards.%20Lastly%2C%20our%20method%20adopts%20a%20hybrid%20graph-textual%0Aretrieval%20to%20improve%20the%20reasoning%20capacity.%20Extensive%20experimental%20results%0Ademonstrate%20that%20GraphRAG-R1%20boosts%20LLM%20capabilities%20in%20solving%20complex%0Areasoning%20problems%20compared%20to%20state-of-the-art%20GraphRAG%20methods%20on%20both%0Ain-domain%20and%20out-of-domain%20datasets.%20Furthermore%2C%20our%20framework%20can%20be%0Aflexibly%20integrated%20with%20various%20existing%20retrieval%20methods%2C%20consistently%0Adelivering%20performance%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphRAG-R1%253A%2520Graph%2520Retrieval-Augmented%2520Generation%2520with%250A%2520%2520Process-Constrained%2520Reinforcement%2520Learning%26entry.906535625%3DChuanyue%2520Yu%2520and%2520Kuo%2520Zhao%2520and%2520Yuhan%2520Li%2520and%2520Heng%2520Chang%2520and%2520Mingjian%2520Feng%2520and%2520Xiangzhe%2520Jiang%2520and%2520Yufei%2520Sun%2520and%2520Jia%2520Li%2520and%2520Yuzhi%2520Zhang%2520and%2520Jianxin%2520Li%2520and%2520Ziwei%2520Zhang%26entry.1292438233%3D%2520%2520Graph%2520Retrieval-Augmented%2520Generation%2520%2528GraphRAG%2529%2520has%2520shown%2520great%2520effectiveness%250Ain%2520enhancing%2520the%2520reasoning%2520abilities%2520of%2520LLMs%2520by%2520leveraging%2520graph%2520structures%2520for%250Aknowledge%2520representation%2520and%2520modeling%2520complex%2520real-world%2520relationships.%250AHowever%252C%2520existing%2520GraphRAG%2520methods%2520still%2520face%2520significant%2520bottlenecks%2520when%250Ahandling%2520complex%2520problems%2520that%2520require%2520multi-hop%2520reasoning%252C%2520as%2520their%2520query%2520and%250Aretrieval%2520phases%2520are%2520largely%2520based%2520on%2520pre-defined%2520heuristics%2520and%2520do%2520not%2520fully%250Autilize%2520the%2520reasoning%2520potentials%2520of%2520LLMs.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%250AGraphRAG-R1%252C%2520an%2520adaptive%2520GraphRAG%2520framework%2520by%2520training%2520LLMs%2520with%250Aprocess-constrained%2520outcome-based%2520reinforcement%2520learning%2520%2528RL%2529%2520to%2520enhance%2520the%250Amulti-hop%2520reasoning%2520ability.%2520Our%2520method%2520can%2520decompose%2520complex%2520problems%252C%250Aautonomously%2520invoke%2520retrieval%2520tools%2520to%2520acquire%2520necessary%2520information%252C%2520and%250Aperform%2520effective%2520reasoning.%2520Specifically%252C%2520we%2520utilize%2520a%2520modified%2520version%2520of%250AGroup%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520that%2520supports%2520rollout-with-thinking%250Acapability.%2520Next%252C%2520we%2520design%2520two%2520process-constrained%2520reward%2520functions.%2520To%2520handle%250Athe%2520shallow%2520retrieval%2520problem%252C%2520we%2520design%2520a%2520Progressive%2520Retrieval%2520Attenuation%250A%2528PRA%2529%2520reward%2520to%2520encourage%2520essential%2520retrievals.%2520Then%252C%2520to%2520handle%2520the%250Aover-thinking%2520problem%252C%2520we%2520design%2520Cost-Aware%2520F1%2520%2528CAF%2529%2520reward%2520to%2520balance%2520the%250Amodel%2520performance%2520with%2520computational%2520costs.%2520We%2520further%2520design%2520a%2520phase-dependent%250Atraining%2520strategy%252C%2520containing%2520three%2520training%2520stages%2520corresponding%2520to%2520cold%2520start%250Aand%2520these%2520two%2520rewards.%2520Lastly%252C%2520our%2520method%2520adopts%2520a%2520hybrid%2520graph-textual%250Aretrieval%2520to%2520improve%2520the%2520reasoning%2520capacity.%2520Extensive%2520experimental%2520results%250Ademonstrate%2520that%2520GraphRAG-R1%2520boosts%2520LLM%2520capabilities%2520in%2520solving%2520complex%250Areasoning%2520problems%2520compared%2520to%2520state-of-the-art%2520GraphRAG%2520methods%2520on%2520both%250Ain-domain%2520and%2520out-of-domain%2520datasets.%2520Furthermore%252C%2520our%2520framework%2520can%2520be%250Aflexibly%2520integrated%2520with%2520various%2520existing%2520retrieval%2520methods%252C%2520consistently%250Adelivering%2520performance%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphRAG-R1%3A%20Graph%20Retrieval-Augmented%20Generation%20with%0A%20%20Process-Constrained%20Reinforcement%20Learning&entry.906535625=Chuanyue%20Yu%20and%20Kuo%20Zhao%20and%20Yuhan%20Li%20and%20Heng%20Chang%20and%20Mingjian%20Feng%20and%20Xiangzhe%20Jiang%20and%20Yufei%20Sun%20and%20Jia%20Li%20and%20Yuzhi%20Zhang%20and%20Jianxin%20Li%20and%20Ziwei%20Zhang&entry.1292438233=%20%20Graph%20Retrieval-Augmented%20Generation%20%28GraphRAG%29%20has%20shown%20great%20effectiveness%0Ain%20enhancing%20the%20reasoning%20abilities%20of%20LLMs%20by%20leveraging%20graph%20structures%20for%0Aknowledge%20representation%20and%20modeling%20complex%20real-world%20relationships.%0AHowever%2C%20existing%20GraphRAG%20methods%20still%20face%20significant%20bottlenecks%20when%0Ahandling%20complex%20problems%20that%20require%20multi-hop%20reasoning%2C%20as%20their%20query%20and%0Aretrieval%20phases%20are%20largely%20based%20on%20pre-defined%20heuristics%20and%20do%20not%20fully%0Autilize%20the%20reasoning%20potentials%20of%20LLMs.%20To%20address%20this%20problem%2C%20we%20propose%0AGraphRAG-R1%2C%20an%20adaptive%20GraphRAG%20framework%20by%20training%20LLMs%20with%0Aprocess-constrained%20outcome-based%20reinforcement%20learning%20%28RL%29%20to%20enhance%20the%0Amulti-hop%20reasoning%20ability.%20Our%20method%20can%20decompose%20complex%20problems%2C%0Aautonomously%20invoke%20retrieval%20tools%20to%20acquire%20necessary%20information%2C%20and%0Aperform%20effective%20reasoning.%20Specifically%2C%20we%20utilize%20a%20modified%20version%20of%0AGroup%20Relative%20Policy%20Optimization%20%28GRPO%29%20that%20supports%20rollout-with-thinking%0Acapability.%20Next%2C%20we%20design%20two%20process-constrained%20reward%20functions.%20To%20handle%0Athe%20shallow%20retrieval%20problem%2C%20we%20design%20a%20Progressive%20Retrieval%20Attenuation%0A%28PRA%29%20reward%20to%20encourage%20essential%20retrievals.%20Then%2C%20to%20handle%20the%0Aover-thinking%20problem%2C%20we%20design%20Cost-Aware%20F1%20%28CAF%29%20reward%20to%20balance%20the%0Amodel%20performance%20with%20computational%20costs.%20We%20further%20design%20a%20phase-dependent%0Atraining%20strategy%2C%20containing%20three%20training%20stages%20corresponding%20to%20cold%20start%0Aand%20these%20two%20rewards.%20Lastly%2C%20our%20method%20adopts%20a%20hybrid%20graph-textual%0Aretrieval%20to%20improve%20the%20reasoning%20capacity.%20Extensive%20experimental%20results%0Ademonstrate%20that%20GraphRAG-R1%20boosts%20LLM%20capabilities%20in%20solving%20complex%0Areasoning%20problems%20compared%20to%20state-of-the-art%20GraphRAG%20methods%20on%20both%0Ain-domain%20and%20out-of-domain%20datasets.%20Furthermore%2C%20our%20framework%20can%20be%0Aflexibly%20integrated%20with%20various%20existing%20retrieval%20methods%2C%20consistently%0Adelivering%20performance%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23581v1&entry.124074799=Read"},
{"title": "LLM-Based Identification of Infostealer Infection Vectors from\n  Screenshots: The Case of Aurora", "author": "Estelle Ruellan and Eric Clay and Nicholas Ascoli", "abstract": "  Infostealers exfiltrate credentials, session cookies, and sensitive data from\ninfected systems. With over 29 million stealer logs reported in 2024, manual\nanalysis and mitigation at scale are virtually unfeasible/unpractical. While\nmost research focuses on proactive malware detection, a significant gap remains\nin leveraging reactive analysis of stealer logs and their associated artifacts.\nSpecifically, infection artifacts such as screenshots, image captured at the\npoint of compromise, are largely overlooked by the current literature. This\npaper introduces a novel approach leveraging Large Language Models (LLMs), more\nspecifically gpt-4o-mini, to analyze infection screenshots to extract potential\nIndicators of Compromise (IoCs), map infection vectors, and track campaigns.\nFocusing on the Aurora infostealer, we demonstrate how LLMs can process\nscreenshots to identify infection vectors, such as malicious URLs, installer\nfiles, and exploited software themes. Our method extracted 337 actionable URLs\nand 246 relevant files from 1000 screenshots, revealing key malware\ndistribution methods and social engineering tactics. By correlating extracted\nfilenames, URLs, and infection themes, we identified three distinct malware\ncampaigns, demonstrating the potential of LLM-driven analysis for uncovering\ninfection workflows and enhancing threat intelligence. By shifting malware\nanalysis from traditional log-based detection methods to a reactive,\nartifact-driven approach that leverages infection screenshots, this research\npresents a scalable method for identifying infection vectors and enabling early\nintervention.\n", "link": "http://arxiv.org/abs/2507.23611v1", "date": "2025-07-31", "relevancy": 2.0837, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4055}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Based%20Identification%20of%20Infostealer%20Infection%20Vectors%20from%0A%20%20Screenshots%3A%20The%20Case%20of%20Aurora&body=Title%3A%20LLM-Based%20Identification%20of%20Infostealer%20Infection%20Vectors%20from%0A%20%20Screenshots%3A%20The%20Case%20of%20Aurora%0AAuthor%3A%20Estelle%20Ruellan%20and%20Eric%20Clay%20and%20Nicholas%20Ascoli%0AAbstract%3A%20%20%20Infostealers%20exfiltrate%20credentials%2C%20session%20cookies%2C%20and%20sensitive%20data%20from%0Ainfected%20systems.%20With%20over%2029%20million%20stealer%20logs%20reported%20in%202024%2C%20manual%0Aanalysis%20and%20mitigation%20at%20scale%20are%20virtually%20unfeasible/unpractical.%20While%0Amost%20research%20focuses%20on%20proactive%20malware%20detection%2C%20a%20significant%20gap%20remains%0Ain%20leveraging%20reactive%20analysis%20of%20stealer%20logs%20and%20their%20associated%20artifacts.%0ASpecifically%2C%20infection%20artifacts%20such%20as%20screenshots%2C%20image%20captured%20at%20the%0Apoint%20of%20compromise%2C%20are%20largely%20overlooked%20by%20the%20current%20literature.%20This%0Apaper%20introduces%20a%20novel%20approach%20leveraging%20Large%20Language%20Models%20%28LLMs%29%2C%20more%0Aspecifically%20gpt-4o-mini%2C%20to%20analyze%20infection%20screenshots%20to%20extract%20potential%0AIndicators%20of%20Compromise%20%28IoCs%29%2C%20map%20infection%20vectors%2C%20and%20track%20campaigns.%0AFocusing%20on%20the%20Aurora%20infostealer%2C%20we%20demonstrate%20how%20LLMs%20can%20process%0Ascreenshots%20to%20identify%20infection%20vectors%2C%20such%20as%20malicious%20URLs%2C%20installer%0Afiles%2C%20and%20exploited%20software%20themes.%20Our%20method%20extracted%20337%20actionable%20URLs%0Aand%20246%20relevant%20files%20from%201000%20screenshots%2C%20revealing%20key%20malware%0Adistribution%20methods%20and%20social%20engineering%20tactics.%20By%20correlating%20extracted%0Afilenames%2C%20URLs%2C%20and%20infection%20themes%2C%20we%20identified%20three%20distinct%20malware%0Acampaigns%2C%20demonstrating%20the%20potential%20of%20LLM-driven%20analysis%20for%20uncovering%0Ainfection%20workflows%20and%20enhancing%20threat%20intelligence.%20By%20shifting%20malware%0Aanalysis%20from%20traditional%20log-based%20detection%20methods%20to%20a%20reactive%2C%0Aartifact-driven%20approach%20that%20leverages%20infection%20screenshots%2C%20this%20research%0Apresents%20a%20scalable%20method%20for%20identifying%20infection%20vectors%20and%20enabling%20early%0Aintervention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Based%2520Identification%2520of%2520Infostealer%2520Infection%2520Vectors%2520from%250A%2520%2520Screenshots%253A%2520The%2520Case%2520of%2520Aurora%26entry.906535625%3DEstelle%2520Ruellan%2520and%2520Eric%2520Clay%2520and%2520Nicholas%2520Ascoli%26entry.1292438233%3D%2520%2520Infostealers%2520exfiltrate%2520credentials%252C%2520session%2520cookies%252C%2520and%2520sensitive%2520data%2520from%250Ainfected%2520systems.%2520With%2520over%252029%2520million%2520stealer%2520logs%2520reported%2520in%25202024%252C%2520manual%250Aanalysis%2520and%2520mitigation%2520at%2520scale%2520are%2520virtually%2520unfeasible/unpractical.%2520While%250Amost%2520research%2520focuses%2520on%2520proactive%2520malware%2520detection%252C%2520a%2520significant%2520gap%2520remains%250Ain%2520leveraging%2520reactive%2520analysis%2520of%2520stealer%2520logs%2520and%2520their%2520associated%2520artifacts.%250ASpecifically%252C%2520infection%2520artifacts%2520such%2520as%2520screenshots%252C%2520image%2520captured%2520at%2520the%250Apoint%2520of%2520compromise%252C%2520are%2520largely%2520overlooked%2520by%2520the%2520current%2520literature.%2520This%250Apaper%2520introduces%2520a%2520novel%2520approach%2520leveraging%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520more%250Aspecifically%2520gpt-4o-mini%252C%2520to%2520analyze%2520infection%2520screenshots%2520to%2520extract%2520potential%250AIndicators%2520of%2520Compromise%2520%2528IoCs%2529%252C%2520map%2520infection%2520vectors%252C%2520and%2520track%2520campaigns.%250AFocusing%2520on%2520the%2520Aurora%2520infostealer%252C%2520we%2520demonstrate%2520how%2520LLMs%2520can%2520process%250Ascreenshots%2520to%2520identify%2520infection%2520vectors%252C%2520such%2520as%2520malicious%2520URLs%252C%2520installer%250Afiles%252C%2520and%2520exploited%2520software%2520themes.%2520Our%2520method%2520extracted%2520337%2520actionable%2520URLs%250Aand%2520246%2520relevant%2520files%2520from%25201000%2520screenshots%252C%2520revealing%2520key%2520malware%250Adistribution%2520methods%2520and%2520social%2520engineering%2520tactics.%2520By%2520correlating%2520extracted%250Afilenames%252C%2520URLs%252C%2520and%2520infection%2520themes%252C%2520we%2520identified%2520three%2520distinct%2520malware%250Acampaigns%252C%2520demonstrating%2520the%2520potential%2520of%2520LLM-driven%2520analysis%2520for%2520uncovering%250Ainfection%2520workflows%2520and%2520enhancing%2520threat%2520intelligence.%2520By%2520shifting%2520malware%250Aanalysis%2520from%2520traditional%2520log-based%2520detection%2520methods%2520to%2520a%2520reactive%252C%250Aartifact-driven%2520approach%2520that%2520leverages%2520infection%2520screenshots%252C%2520this%2520research%250Apresents%2520a%2520scalable%2520method%2520for%2520identifying%2520infection%2520vectors%2520and%2520enabling%2520early%250Aintervention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Based%20Identification%20of%20Infostealer%20Infection%20Vectors%20from%0A%20%20Screenshots%3A%20The%20Case%20of%20Aurora&entry.906535625=Estelle%20Ruellan%20and%20Eric%20Clay%20and%20Nicholas%20Ascoli&entry.1292438233=%20%20Infostealers%20exfiltrate%20credentials%2C%20session%20cookies%2C%20and%20sensitive%20data%20from%0Ainfected%20systems.%20With%20over%2029%20million%20stealer%20logs%20reported%20in%202024%2C%20manual%0Aanalysis%20and%20mitigation%20at%20scale%20are%20virtually%20unfeasible/unpractical.%20While%0Amost%20research%20focuses%20on%20proactive%20malware%20detection%2C%20a%20significant%20gap%20remains%0Ain%20leveraging%20reactive%20analysis%20of%20stealer%20logs%20and%20their%20associated%20artifacts.%0ASpecifically%2C%20infection%20artifacts%20such%20as%20screenshots%2C%20image%20captured%20at%20the%0Apoint%20of%20compromise%2C%20are%20largely%20overlooked%20by%20the%20current%20literature.%20This%0Apaper%20introduces%20a%20novel%20approach%20leveraging%20Large%20Language%20Models%20%28LLMs%29%2C%20more%0Aspecifically%20gpt-4o-mini%2C%20to%20analyze%20infection%20screenshots%20to%20extract%20potential%0AIndicators%20of%20Compromise%20%28IoCs%29%2C%20map%20infection%20vectors%2C%20and%20track%20campaigns.%0AFocusing%20on%20the%20Aurora%20infostealer%2C%20we%20demonstrate%20how%20LLMs%20can%20process%0Ascreenshots%20to%20identify%20infection%20vectors%2C%20such%20as%20malicious%20URLs%2C%20installer%0Afiles%2C%20and%20exploited%20software%20themes.%20Our%20method%20extracted%20337%20actionable%20URLs%0Aand%20246%20relevant%20files%20from%201000%20screenshots%2C%20revealing%20key%20malware%0Adistribution%20methods%20and%20social%20engineering%20tactics.%20By%20correlating%20extracted%0Afilenames%2C%20URLs%2C%20and%20infection%20themes%2C%20we%20identified%20three%20distinct%20malware%0Acampaigns%2C%20demonstrating%20the%20potential%20of%20LLM-driven%20analysis%20for%20uncovering%0Ainfection%20workflows%20and%20enhancing%20threat%20intelligence.%20By%20shifting%20malware%0Aanalysis%20from%20traditional%20log-based%20detection%20methods%20to%20a%20reactive%2C%0Aartifact-driven%20approach%20that%20leverages%20infection%20screenshots%2C%20this%20research%0Apresents%20a%20scalable%20method%20for%20identifying%20infection%20vectors%20and%20enabling%20early%0Aintervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23611v1&entry.124074799=Read"},
{"title": "Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for\n  Culturally Diverse Art Style Classification", "author": "Abdellah Zakaria Sellam and Salah Eddine Bekhouche and Cosimo Distante and Abdelmalik Taleb-Ahmed", "abstract": "  Art style classification remains a formidable challenge in computational\naesthetics due to the scarcity of expertly labeled datasets and the intricate,\noften nonlinear interplay of stylistic elements. While recent dual-teacher\nself-supervised frameworks reduce reliance on labeled data, their linear\nprojection layers and localized focus struggle to model global compositional\ncontext and complex style-feature interactions. We enhance the dual-teacher\nknowledge distillation framework to address these limitations by replacing\nconventional MLP projection and prediction heads with Kolmogorov-Arnold\nNetworks (KANs). Our approach retains complementary guidance from two teacher\nnetworks, one emphasizing localized texture and brushstroke patterns, the other\ncapturing broader stylistic hierarchies while leveraging KANs' spline-based\nactivations to model nonlinear feature correlations with mathematical\nprecision. Experiments on WikiArt and Pandora18k demonstrate that our approach\noutperforms the base dual teacher architecture in Top-1 accuracy. Our findings\nhighlight the importance of KANs in disentangling complex style manifolds,\nleading to better linear probe accuracy than MLP projections.\n", "link": "http://arxiv.org/abs/2507.23436v1", "date": "2025-07-31", "relevancy": 2.0822, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5366}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5188}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Linear%20Bottlenecks%3A%20Spline-Based%20Knowledge%20Distillation%20for%0A%20%20Culturally%20Diverse%20Art%20Style%20Classification&body=Title%3A%20Beyond%20Linear%20Bottlenecks%3A%20Spline-Based%20Knowledge%20Distillation%20for%0A%20%20Culturally%20Diverse%20Art%20Style%20Classification%0AAuthor%3A%20Abdellah%20Zakaria%20Sellam%20and%20Salah%20Eddine%20Bekhouche%20and%20Cosimo%20Distante%20and%20Abdelmalik%20Taleb-Ahmed%0AAbstract%3A%20%20%20Art%20style%20classification%20remains%20a%20formidable%20challenge%20in%20computational%0Aaesthetics%20due%20to%20the%20scarcity%20of%20expertly%20labeled%20datasets%20and%20the%20intricate%2C%0Aoften%20nonlinear%20interplay%20of%20stylistic%20elements.%20While%20recent%20dual-teacher%0Aself-supervised%20frameworks%20reduce%20reliance%20on%20labeled%20data%2C%20their%20linear%0Aprojection%20layers%20and%20localized%20focus%20struggle%20to%20model%20global%20compositional%0Acontext%20and%20complex%20style-feature%20interactions.%20We%20enhance%20the%20dual-teacher%0Aknowledge%20distillation%20framework%20to%20address%20these%20limitations%20by%20replacing%0Aconventional%20MLP%20projection%20and%20prediction%20heads%20with%20Kolmogorov-Arnold%0ANetworks%20%28KANs%29.%20Our%20approach%20retains%20complementary%20guidance%20from%20two%20teacher%0Anetworks%2C%20one%20emphasizing%20localized%20texture%20and%20brushstroke%20patterns%2C%20the%20other%0Acapturing%20broader%20stylistic%20hierarchies%20while%20leveraging%20KANs%27%20spline-based%0Aactivations%20to%20model%20nonlinear%20feature%20correlations%20with%20mathematical%0Aprecision.%20Experiments%20on%20WikiArt%20and%20Pandora18k%20demonstrate%20that%20our%20approach%0Aoutperforms%20the%20base%20dual%20teacher%20architecture%20in%20Top-1%20accuracy.%20Our%20findings%0Ahighlight%20the%20importance%20of%20KANs%20in%20disentangling%20complex%20style%20manifolds%2C%0Aleading%20to%20better%20linear%20probe%20accuracy%20than%20MLP%20projections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23436v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Linear%2520Bottlenecks%253A%2520Spline-Based%2520Knowledge%2520Distillation%2520for%250A%2520%2520Culturally%2520Diverse%2520Art%2520Style%2520Classification%26entry.906535625%3DAbdellah%2520Zakaria%2520Sellam%2520and%2520Salah%2520Eddine%2520Bekhouche%2520and%2520Cosimo%2520Distante%2520and%2520Abdelmalik%2520Taleb-Ahmed%26entry.1292438233%3D%2520%2520Art%2520style%2520classification%2520remains%2520a%2520formidable%2520challenge%2520in%2520computational%250Aaesthetics%2520due%2520to%2520the%2520scarcity%2520of%2520expertly%2520labeled%2520datasets%2520and%2520the%2520intricate%252C%250Aoften%2520nonlinear%2520interplay%2520of%2520stylistic%2520elements.%2520While%2520recent%2520dual-teacher%250Aself-supervised%2520frameworks%2520reduce%2520reliance%2520on%2520labeled%2520data%252C%2520their%2520linear%250Aprojection%2520layers%2520and%2520localized%2520focus%2520struggle%2520to%2520model%2520global%2520compositional%250Acontext%2520and%2520complex%2520style-feature%2520interactions.%2520We%2520enhance%2520the%2520dual-teacher%250Aknowledge%2520distillation%2520framework%2520to%2520address%2520these%2520limitations%2520by%2520replacing%250Aconventional%2520MLP%2520projection%2520and%2520prediction%2520heads%2520with%2520Kolmogorov-Arnold%250ANetworks%2520%2528KANs%2529.%2520Our%2520approach%2520retains%2520complementary%2520guidance%2520from%2520two%2520teacher%250Anetworks%252C%2520one%2520emphasizing%2520localized%2520texture%2520and%2520brushstroke%2520patterns%252C%2520the%2520other%250Acapturing%2520broader%2520stylistic%2520hierarchies%2520while%2520leveraging%2520KANs%2527%2520spline-based%250Aactivations%2520to%2520model%2520nonlinear%2520feature%2520correlations%2520with%2520mathematical%250Aprecision.%2520Experiments%2520on%2520WikiArt%2520and%2520Pandora18k%2520demonstrate%2520that%2520our%2520approach%250Aoutperforms%2520the%2520base%2520dual%2520teacher%2520architecture%2520in%2520Top-1%2520accuracy.%2520Our%2520findings%250Ahighlight%2520the%2520importance%2520of%2520KANs%2520in%2520disentangling%2520complex%2520style%2520manifolds%252C%250Aleading%2520to%2520better%2520linear%2520probe%2520accuracy%2520than%2520MLP%2520projections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23436v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Linear%20Bottlenecks%3A%20Spline-Based%20Knowledge%20Distillation%20for%0A%20%20Culturally%20Diverse%20Art%20Style%20Classification&entry.906535625=Abdellah%20Zakaria%20Sellam%20and%20Salah%20Eddine%20Bekhouche%20and%20Cosimo%20Distante%20and%20Abdelmalik%20Taleb-Ahmed&entry.1292438233=%20%20Art%20style%20classification%20remains%20a%20formidable%20challenge%20in%20computational%0Aaesthetics%20due%20to%20the%20scarcity%20of%20expertly%20labeled%20datasets%20and%20the%20intricate%2C%0Aoften%20nonlinear%20interplay%20of%20stylistic%20elements.%20While%20recent%20dual-teacher%0Aself-supervised%20frameworks%20reduce%20reliance%20on%20labeled%20data%2C%20their%20linear%0Aprojection%20layers%20and%20localized%20focus%20struggle%20to%20model%20global%20compositional%0Acontext%20and%20complex%20style-feature%20interactions.%20We%20enhance%20the%20dual-teacher%0Aknowledge%20distillation%20framework%20to%20address%20these%20limitations%20by%20replacing%0Aconventional%20MLP%20projection%20and%20prediction%20heads%20with%20Kolmogorov-Arnold%0ANetworks%20%28KANs%29.%20Our%20approach%20retains%20complementary%20guidance%20from%20two%20teacher%0Anetworks%2C%20one%20emphasizing%20localized%20texture%20and%20brushstroke%20patterns%2C%20the%20other%0Acapturing%20broader%20stylistic%20hierarchies%20while%20leveraging%20KANs%27%20spline-based%0Aactivations%20to%20model%20nonlinear%20feature%20correlations%20with%20mathematical%0Aprecision.%20Experiments%20on%20WikiArt%20and%20Pandora18k%20demonstrate%20that%20our%20approach%0Aoutperforms%20the%20base%20dual%20teacher%20architecture%20in%20Top-1%20accuracy.%20Our%20findings%0Ahighlight%20the%20importance%20of%20KANs%20in%20disentangling%20complex%20style%20manifolds%2C%0Aleading%20to%20better%20linear%20probe%20accuracy%20than%20MLP%20projections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23436v1&entry.124074799=Read"},
{"title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework", "author": "Yao Shi and Rongkeng Liang and Yong Xu", "abstract": "  Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.\n", "link": "http://arxiv.org/abs/2504.14928v3", "date": "2025-07-31", "relevancy": 2.0821, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5229}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5229}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EducationQ%3A%20Evaluating%20LLMs%27%20Teaching%20Capabilities%20Through%20Multi-Agent%0A%20%20Dialogue%20Framework&body=Title%3A%20EducationQ%3A%20Evaluating%20LLMs%27%20Teaching%20Capabilities%20Through%20Multi-Agent%0A%20%20Dialogue%20Framework%0AAuthor%3A%20Yao%20Shi%20and%20Rongkeng%20Liang%20and%20Yong%20Xu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20increasingly%20serve%20as%20educational%20tools%2C%20yet%0Aevaluating%20their%20teaching%20capabilities%20remains%20challenging%20due%20to%20the%0Aresource-intensive%2C%20context-dependent%2C%20and%20methodologically%20complex%20nature%20of%0Ateacher-student%20interactions.%20We%20introduce%20EducationQ%2C%20a%20multi-agent%20dialogue%0Aframework%20that%20efficiently%20assesses%20teaching%20capabilities%20through%20simulated%0Adynamic%20educational%20scenarios%2C%20featuring%20specialized%20agents%20for%20teaching%2C%0Alearning%2C%20and%20evaluation.%20Testing%2014%20LLMs%20across%20major%20AI%20Organizations%0A%28OpenAI%2C%20Meta%2C%20Google%2C%20Anthropic%2C%20and%20others%29%20on%201%2C498%20questions%20spanning%2013%0Adisciplines%20and%2010%20difficulty%20levels%20reveals%20that%20teaching%20effectiveness%20does%0Anot%20correlate%20linearly%20with%20model%20scale%20or%20general%20reasoning%20capabilities%20-%0Awith%20some%20smaller%20open-source%20models%20outperforming%20larger%20commercial%0Acounterparts%20in%20teaching%20contexts.%20This%20finding%20highlights%20a%20critical%20gap%20in%0Acurrent%20evaluations%20that%20prioritize%20knowledge%20recall%20over%20interactive%20pedagogy.%0AOur%20mixed-methods%20evaluation%2C%20combining%20quantitative%20metrics%20with%20qualitative%0Aanalysis%20and%20expert%20case%20studies%2C%20identifies%20distinct%20pedagogical%20strengths%0Aemployed%20by%20top-performing%20models%20%28e.g.%2C%20sophisticated%20questioning%20strategies%2C%0Aadaptive%20feedback%20mechanisms%29.%20Human%20expert%20evaluations%20show%2078%25%20agreement%20with%0Aour%20automated%20qualitative%20analysis%20of%20effective%20teaching%20behaviors%2C%20validating%0Aour%20methodology.%20EducationQ%20demonstrates%20that%20LLMs-as-teachers%20require%0Aspecialized%20optimization%20beyond%20simple%20scaling%2C%20suggesting%20next-generation%0Aeducational%20AI%20prioritize%20targeted%20enhancement%20of%20specific%20pedagogical%0Aeffectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.14928v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEducationQ%253A%2520Evaluating%2520LLMs%2527%2520Teaching%2520Capabilities%2520Through%2520Multi-Agent%250A%2520%2520Dialogue%2520Framework%26entry.906535625%3DYao%2520Shi%2520and%2520Rongkeng%2520Liang%2520and%2520Yong%2520Xu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%2520serve%2520as%2520educational%2520tools%252C%2520yet%250Aevaluating%2520their%2520teaching%2520capabilities%2520remains%2520challenging%2520due%2520to%2520the%250Aresource-intensive%252C%2520context-dependent%252C%2520and%2520methodologically%2520complex%2520nature%2520of%250Ateacher-student%2520interactions.%2520We%2520introduce%2520EducationQ%252C%2520a%2520multi-agent%2520dialogue%250Aframework%2520that%2520efficiently%2520assesses%2520teaching%2520capabilities%2520through%2520simulated%250Adynamic%2520educational%2520scenarios%252C%2520featuring%2520specialized%2520agents%2520for%2520teaching%252C%250Alearning%252C%2520and%2520evaluation.%2520Testing%252014%2520LLMs%2520across%2520major%2520AI%2520Organizations%250A%2528OpenAI%252C%2520Meta%252C%2520Google%252C%2520Anthropic%252C%2520and%2520others%2529%2520on%25201%252C498%2520questions%2520spanning%252013%250Adisciplines%2520and%252010%2520difficulty%2520levels%2520reveals%2520that%2520teaching%2520effectiveness%2520does%250Anot%2520correlate%2520linearly%2520with%2520model%2520scale%2520or%2520general%2520reasoning%2520capabilities%2520-%250Awith%2520some%2520smaller%2520open-source%2520models%2520outperforming%2520larger%2520commercial%250Acounterparts%2520in%2520teaching%2520contexts.%2520This%2520finding%2520highlights%2520a%2520critical%2520gap%2520in%250Acurrent%2520evaluations%2520that%2520prioritize%2520knowledge%2520recall%2520over%2520interactive%2520pedagogy.%250AOur%2520mixed-methods%2520evaluation%252C%2520combining%2520quantitative%2520metrics%2520with%2520qualitative%250Aanalysis%2520and%2520expert%2520case%2520studies%252C%2520identifies%2520distinct%2520pedagogical%2520strengths%250Aemployed%2520by%2520top-performing%2520models%2520%2528e.g.%252C%2520sophisticated%2520questioning%2520strategies%252C%250Aadaptive%2520feedback%2520mechanisms%2529.%2520Human%2520expert%2520evaluations%2520show%252078%2525%2520agreement%2520with%250Aour%2520automated%2520qualitative%2520analysis%2520of%2520effective%2520teaching%2520behaviors%252C%2520validating%250Aour%2520methodology.%2520EducationQ%2520demonstrates%2520that%2520LLMs-as-teachers%2520require%250Aspecialized%2520optimization%2520beyond%2520simple%2520scaling%252C%2520suggesting%2520next-generation%250Aeducational%2520AI%2520prioritize%2520targeted%2520enhancement%2520of%2520specific%2520pedagogical%250Aeffectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.14928v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EducationQ%3A%20Evaluating%20LLMs%27%20Teaching%20Capabilities%20Through%20Multi-Agent%0A%20%20Dialogue%20Framework&entry.906535625=Yao%20Shi%20and%20Rongkeng%20Liang%20and%20Yong%20Xu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20increasingly%20serve%20as%20educational%20tools%2C%20yet%0Aevaluating%20their%20teaching%20capabilities%20remains%20challenging%20due%20to%20the%0Aresource-intensive%2C%20context-dependent%2C%20and%20methodologically%20complex%20nature%20of%0Ateacher-student%20interactions.%20We%20introduce%20EducationQ%2C%20a%20multi-agent%20dialogue%0Aframework%20that%20efficiently%20assesses%20teaching%20capabilities%20through%20simulated%0Adynamic%20educational%20scenarios%2C%20featuring%20specialized%20agents%20for%20teaching%2C%0Alearning%2C%20and%20evaluation.%20Testing%2014%20LLMs%20across%20major%20AI%20Organizations%0A%28OpenAI%2C%20Meta%2C%20Google%2C%20Anthropic%2C%20and%20others%29%20on%201%2C498%20questions%20spanning%2013%0Adisciplines%20and%2010%20difficulty%20levels%20reveals%20that%20teaching%20effectiveness%20does%0Anot%20correlate%20linearly%20with%20model%20scale%20or%20general%20reasoning%20capabilities%20-%0Awith%20some%20smaller%20open-source%20models%20outperforming%20larger%20commercial%0Acounterparts%20in%20teaching%20contexts.%20This%20finding%20highlights%20a%20critical%20gap%20in%0Acurrent%20evaluations%20that%20prioritize%20knowledge%20recall%20over%20interactive%20pedagogy.%0AOur%20mixed-methods%20evaluation%2C%20combining%20quantitative%20metrics%20with%20qualitative%0Aanalysis%20and%20expert%20case%20studies%2C%20identifies%20distinct%20pedagogical%20strengths%0Aemployed%20by%20top-performing%20models%20%28e.g.%2C%20sophisticated%20questioning%20strategies%2C%0Aadaptive%20feedback%20mechanisms%29.%20Human%20expert%20evaluations%20show%2078%25%20agreement%20with%0Aour%20automated%20qualitative%20analysis%20of%20effective%20teaching%20behaviors%2C%20validating%0Aour%20methodology.%20EducationQ%20demonstrates%20that%20LLMs-as-teachers%20require%0Aspecialized%20optimization%20beyond%20simple%20scaling%2C%20suggesting%20next-generation%0Aeducational%20AI%20prioritize%20targeted%20enhancement%20of%20specific%20pedagogical%0Aeffectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.14928v3&entry.124074799=Read"},
{"title": "SUB: Benchmarking CBM Generalization via Synthetic Attribute\n  Substitutions", "author": "Jessica Bader and Leander Girrbach and Stephan Alaniz and Zeynep Akata", "abstract": "  Concept Bottleneck Models (CBMs) and other concept-based interpretable models\nshow great promise for making AI applications more transparent, which is\nessential in fields like medicine. Despite their success, we demonstrate that\nCBMs struggle to reliably identify the correct concepts under distribution\nshifts. To assess the robustness of CBMs to concept variations, we introduce\nSUB: a fine-grained image and concept benchmark containing 38,400 synthetic\nimages based on the CUB dataset. To create SUB, we select a CUB subset of 33\nbird classes and 45 concepts to generate images which substitute a specific\nconcept, such as wing color or belly pattern. We introduce a novel Tied\nDiffusion Guidance (TDG) method to precisely control generated images, where\nnoise sharing for two parallel denoising processes ensures that both the\ncorrect bird class and the correct attribute are generated. This novel\nbenchmark enables rigorous evaluation of CBMs and similar interpretable models,\ncontributing to the development of more robust methods. Our code is available\nat https://github.com/ExplainableML/sub and the dataset at\nhttp://huggingface.co/datasets/Jessica-bader/SUB.\n", "link": "http://arxiv.org/abs/2507.23784v1", "date": "2025-07-31", "relevancy": 2.0784, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5617}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SUB%3A%20Benchmarking%20CBM%20Generalization%20via%20Synthetic%20Attribute%0A%20%20Substitutions&body=Title%3A%20SUB%3A%20Benchmarking%20CBM%20Generalization%20via%20Synthetic%20Attribute%0A%20%20Substitutions%0AAuthor%3A%20Jessica%20Bader%20and%20Leander%20Girrbach%20and%20Stephan%20Alaniz%20and%20Zeynep%20Akata%0AAbstract%3A%20%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20and%20other%20concept-based%20interpretable%20models%0Ashow%20great%20promise%20for%20making%20AI%20applications%20more%20transparent%2C%20which%20is%0Aessential%20in%20fields%20like%20medicine.%20Despite%20their%20success%2C%20we%20demonstrate%20that%0ACBMs%20struggle%20to%20reliably%20identify%20the%20correct%20concepts%20under%20distribution%0Ashifts.%20To%20assess%20the%20robustness%20of%20CBMs%20to%20concept%20variations%2C%20we%20introduce%0ASUB%3A%20a%20fine-grained%20image%20and%20concept%20benchmark%20containing%2038%2C400%20synthetic%0Aimages%20based%20on%20the%20CUB%20dataset.%20To%20create%20SUB%2C%20we%20select%20a%20CUB%20subset%20of%2033%0Abird%20classes%20and%2045%20concepts%20to%20generate%20images%20which%20substitute%20a%20specific%0Aconcept%2C%20such%20as%20wing%20color%20or%20belly%20pattern.%20We%20introduce%20a%20novel%20Tied%0ADiffusion%20Guidance%20%28TDG%29%20method%20to%20precisely%20control%20generated%20images%2C%20where%0Anoise%20sharing%20for%20two%20parallel%20denoising%20processes%20ensures%20that%20both%20the%0Acorrect%20bird%20class%20and%20the%20correct%20attribute%20are%20generated.%20This%20novel%0Abenchmark%20enables%20rigorous%20evaluation%20of%20CBMs%20and%20similar%20interpretable%20models%2C%0Acontributing%20to%20the%20development%20of%20more%20robust%20methods.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/ExplainableML/sub%20and%20the%20dataset%20at%0Ahttp%3A//huggingface.co/datasets/Jessica-bader/SUB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSUB%253A%2520Benchmarking%2520CBM%2520Generalization%2520via%2520Synthetic%2520Attribute%250A%2520%2520Substitutions%26entry.906535625%3DJessica%2520Bader%2520and%2520Leander%2520Girrbach%2520and%2520Stephan%2520Alaniz%2520and%2520Zeynep%2520Akata%26entry.1292438233%3D%2520%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520and%2520other%2520concept-based%2520interpretable%2520models%250Ashow%2520great%2520promise%2520for%2520making%2520AI%2520applications%2520more%2520transparent%252C%2520which%2520is%250Aessential%2520in%2520fields%2520like%2520medicine.%2520Despite%2520their%2520success%252C%2520we%2520demonstrate%2520that%250ACBMs%2520struggle%2520to%2520reliably%2520identify%2520the%2520correct%2520concepts%2520under%2520distribution%250Ashifts.%2520To%2520assess%2520the%2520robustness%2520of%2520CBMs%2520to%2520concept%2520variations%252C%2520we%2520introduce%250ASUB%253A%2520a%2520fine-grained%2520image%2520and%2520concept%2520benchmark%2520containing%252038%252C400%2520synthetic%250Aimages%2520based%2520on%2520the%2520CUB%2520dataset.%2520To%2520create%2520SUB%252C%2520we%2520select%2520a%2520CUB%2520subset%2520of%252033%250Abird%2520classes%2520and%252045%2520concepts%2520to%2520generate%2520images%2520which%2520substitute%2520a%2520specific%250Aconcept%252C%2520such%2520as%2520wing%2520color%2520or%2520belly%2520pattern.%2520We%2520introduce%2520a%2520novel%2520Tied%250ADiffusion%2520Guidance%2520%2528TDG%2529%2520method%2520to%2520precisely%2520control%2520generated%2520images%252C%2520where%250Anoise%2520sharing%2520for%2520two%2520parallel%2520denoising%2520processes%2520ensures%2520that%2520both%2520the%250Acorrect%2520bird%2520class%2520and%2520the%2520correct%2520attribute%2520are%2520generated.%2520This%2520novel%250Abenchmark%2520enables%2520rigorous%2520evaluation%2520of%2520CBMs%2520and%2520similar%2520interpretable%2520models%252C%250Acontributing%2520to%2520the%2520development%2520of%2520more%2520robust%2520methods.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/ExplainableML/sub%2520and%2520the%2520dataset%2520at%250Ahttp%253A//huggingface.co/datasets/Jessica-bader/SUB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SUB%3A%20Benchmarking%20CBM%20Generalization%20via%20Synthetic%20Attribute%0A%20%20Substitutions&entry.906535625=Jessica%20Bader%20and%20Leander%20Girrbach%20and%20Stephan%20Alaniz%20and%20Zeynep%20Akata&entry.1292438233=%20%20Concept%20Bottleneck%20Models%20%28CBMs%29%20and%20other%20concept-based%20interpretable%20models%0Ashow%20great%20promise%20for%20making%20AI%20applications%20more%20transparent%2C%20which%20is%0Aessential%20in%20fields%20like%20medicine.%20Despite%20their%20success%2C%20we%20demonstrate%20that%0ACBMs%20struggle%20to%20reliably%20identify%20the%20correct%20concepts%20under%20distribution%0Ashifts.%20To%20assess%20the%20robustness%20of%20CBMs%20to%20concept%20variations%2C%20we%20introduce%0ASUB%3A%20a%20fine-grained%20image%20and%20concept%20benchmark%20containing%2038%2C400%20synthetic%0Aimages%20based%20on%20the%20CUB%20dataset.%20To%20create%20SUB%2C%20we%20select%20a%20CUB%20subset%20of%2033%0Abird%20classes%20and%2045%20concepts%20to%20generate%20images%20which%20substitute%20a%20specific%0Aconcept%2C%20such%20as%20wing%20color%20or%20belly%20pattern.%20We%20introduce%20a%20novel%20Tied%0ADiffusion%20Guidance%20%28TDG%29%20method%20to%20precisely%20control%20generated%20images%2C%20where%0Anoise%20sharing%20for%20two%20parallel%20denoising%20processes%20ensures%20that%20both%20the%0Acorrect%20bird%20class%20and%20the%20correct%20attribute%20are%20generated.%20This%20novel%0Abenchmark%20enables%20rigorous%20evaluation%20of%20CBMs%20and%20similar%20interpretable%20models%2C%0Acontributing%20to%20the%20development%20of%20more%20robust%20methods.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/ExplainableML/sub%20and%20the%20dataset%20at%0Ahttp%3A//huggingface.co/datasets/Jessica-bader/SUB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23784v1&entry.124074799=Read"},
{"title": "MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints\n  in Multimodal Large Language Models", "author": "Yiyan Ji and Haoran Chen and Qiguang Chen and Chengyue Wu and Libo Qin and Wanxiang Che", "abstract": "  Multimodal planning capabilities refer to the ability to predict, reason, and\ndesign steps for task execution with multimodal context, which is essential for\ncomplex reasoning and decision-making across multiple steps. However, current\nbenchmarks face two key challenges: (1) they cannot directly assess multimodal\nreal-world planning capabilities, and (2) they lack constraints or implicit\nconstraints across modalities. To address these issues, we introduce Multimodal\nPlanning with Complex Constraints (MPCC), the first benchmark to systematically\nevaluate MLLMs' ability to handle multimodal constraints in planning. To\naddress the first challenge, MPCC focuses on three real-world tasks: Flight\nPlanning, Calendar Planning, and Meeting Planning. To solve the second\nchallenge, we introduce complex constraints (e.g. budget, temporal, and\nspatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to\nseparate constraint complexity from search space expansion. Experiments on 13\nadvanced MLLMs reveal significant challenges: closed-source models achieve only\n21.3% feasible plans, while open-source models average below 11%. Additionally,\nwe observe that MLLMs are highly sensitive to constraint complexity and that\ntraditional multimodal prompting strategies fail in multi-constraint scenarios.\nOur work formalizes multimodal constraints in planning, provides a rigorous\nevaluation framework, and highlights the need for advancements in\nconstraint-aware reasoning for real-world MLLM applications.\n", "link": "http://arxiv.org/abs/2507.23382v1", "date": "2025-07-31", "relevancy": 2.0778, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MPCC%3A%20A%20Novel%20Benchmark%20for%20Multimodal%20Planning%20with%20Complex%20Constraints%0A%20%20in%20Multimodal%20Large%20Language%20Models&body=Title%3A%20MPCC%3A%20A%20Novel%20Benchmark%20for%20Multimodal%20Planning%20with%20Complex%20Constraints%0A%20%20in%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Yiyan%20Ji%20and%20Haoran%20Chen%20and%20Qiguang%20Chen%20and%20Chengyue%20Wu%20and%20Libo%20Qin%20and%20Wanxiang%20Che%0AAbstract%3A%20%20%20Multimodal%20planning%20capabilities%20refer%20to%20the%20ability%20to%20predict%2C%20reason%2C%20and%0Adesign%20steps%20for%20task%20execution%20with%20multimodal%20context%2C%20which%20is%20essential%20for%0Acomplex%20reasoning%20and%20decision-making%20across%20multiple%20steps.%20However%2C%20current%0Abenchmarks%20face%20two%20key%20challenges%3A%20%281%29%20they%20cannot%20directly%20assess%20multimodal%0Areal-world%20planning%20capabilities%2C%20and%20%282%29%20they%20lack%20constraints%20or%20implicit%0Aconstraints%20across%20modalities.%20To%20address%20these%20issues%2C%20we%20introduce%20Multimodal%0APlanning%20with%20Complex%20Constraints%20%28MPCC%29%2C%20the%20first%20benchmark%20to%20systematically%0Aevaluate%20MLLMs%27%20ability%20to%20handle%20multimodal%20constraints%20in%20planning.%20To%0Aaddress%20the%20first%20challenge%2C%20MPCC%20focuses%20on%20three%20real-world%20tasks%3A%20Flight%0APlanning%2C%20Calendar%20Planning%2C%20and%20Meeting%20Planning.%20To%20solve%20the%20second%0Achallenge%2C%20we%20introduce%20complex%20constraints%20%28e.g.%20budget%2C%20temporal%2C%20and%0Aspatial%29%20in%20these%20tasks%2C%20with%20graded%20difficulty%20levels%20%28EASY%2C%20MEDIUM%2C%20HARD%29%20to%0Aseparate%20constraint%20complexity%20from%20search%20space%20expansion.%20Experiments%20on%2013%0Aadvanced%20MLLMs%20reveal%20significant%20challenges%3A%20closed-source%20models%20achieve%20only%0A21.3%25%20feasible%20plans%2C%20while%20open-source%20models%20average%20below%2011%25.%20Additionally%2C%0Awe%20observe%20that%20MLLMs%20are%20highly%20sensitive%20to%20constraint%20complexity%20and%20that%0Atraditional%20multimodal%20prompting%20strategies%20fail%20in%20multi-constraint%20scenarios.%0AOur%20work%20formalizes%20multimodal%20constraints%20in%20planning%2C%20provides%20a%20rigorous%0Aevaluation%20framework%2C%20and%20highlights%20the%20need%20for%20advancements%20in%0Aconstraint-aware%20reasoning%20for%20real-world%20MLLM%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMPCC%253A%2520A%2520Novel%2520Benchmark%2520for%2520Multimodal%2520Planning%2520with%2520Complex%2520Constraints%250A%2520%2520in%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DYiyan%2520Ji%2520and%2520Haoran%2520Chen%2520and%2520Qiguang%2520Chen%2520and%2520Chengyue%2520Wu%2520and%2520Libo%2520Qin%2520and%2520Wanxiang%2520Che%26entry.1292438233%3D%2520%2520Multimodal%2520planning%2520capabilities%2520refer%2520to%2520the%2520ability%2520to%2520predict%252C%2520reason%252C%2520and%250Adesign%2520steps%2520for%2520task%2520execution%2520with%2520multimodal%2520context%252C%2520which%2520is%2520essential%2520for%250Acomplex%2520reasoning%2520and%2520decision-making%2520across%2520multiple%2520steps.%2520However%252C%2520current%250Abenchmarks%2520face%2520two%2520key%2520challenges%253A%2520%25281%2529%2520they%2520cannot%2520directly%2520assess%2520multimodal%250Areal-world%2520planning%2520capabilities%252C%2520and%2520%25282%2529%2520they%2520lack%2520constraints%2520or%2520implicit%250Aconstraints%2520across%2520modalities.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520Multimodal%250APlanning%2520with%2520Complex%2520Constraints%2520%2528MPCC%2529%252C%2520the%2520first%2520benchmark%2520to%2520systematically%250Aevaluate%2520MLLMs%2527%2520ability%2520to%2520handle%2520multimodal%2520constraints%2520in%2520planning.%2520To%250Aaddress%2520the%2520first%2520challenge%252C%2520MPCC%2520focuses%2520on%2520three%2520real-world%2520tasks%253A%2520Flight%250APlanning%252C%2520Calendar%2520Planning%252C%2520and%2520Meeting%2520Planning.%2520To%2520solve%2520the%2520second%250Achallenge%252C%2520we%2520introduce%2520complex%2520constraints%2520%2528e.g.%2520budget%252C%2520temporal%252C%2520and%250Aspatial%2529%2520in%2520these%2520tasks%252C%2520with%2520graded%2520difficulty%2520levels%2520%2528EASY%252C%2520MEDIUM%252C%2520HARD%2529%2520to%250Aseparate%2520constraint%2520complexity%2520from%2520search%2520space%2520expansion.%2520Experiments%2520on%252013%250Aadvanced%2520MLLMs%2520reveal%2520significant%2520challenges%253A%2520closed-source%2520models%2520achieve%2520only%250A21.3%2525%2520feasible%2520plans%252C%2520while%2520open-source%2520models%2520average%2520below%252011%2525.%2520Additionally%252C%250Awe%2520observe%2520that%2520MLLMs%2520are%2520highly%2520sensitive%2520to%2520constraint%2520complexity%2520and%2520that%250Atraditional%2520multimodal%2520prompting%2520strategies%2520fail%2520in%2520multi-constraint%2520scenarios.%250AOur%2520work%2520formalizes%2520multimodal%2520constraints%2520in%2520planning%252C%2520provides%2520a%2520rigorous%250Aevaluation%2520framework%252C%2520and%2520highlights%2520the%2520need%2520for%2520advancements%2520in%250Aconstraint-aware%2520reasoning%2520for%2520real-world%2520MLLM%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MPCC%3A%20A%20Novel%20Benchmark%20for%20Multimodal%20Planning%20with%20Complex%20Constraints%0A%20%20in%20Multimodal%20Large%20Language%20Models&entry.906535625=Yiyan%20Ji%20and%20Haoran%20Chen%20and%20Qiguang%20Chen%20and%20Chengyue%20Wu%20and%20Libo%20Qin%20and%20Wanxiang%20Che&entry.1292438233=%20%20Multimodal%20planning%20capabilities%20refer%20to%20the%20ability%20to%20predict%2C%20reason%2C%20and%0Adesign%20steps%20for%20task%20execution%20with%20multimodal%20context%2C%20which%20is%20essential%20for%0Acomplex%20reasoning%20and%20decision-making%20across%20multiple%20steps.%20However%2C%20current%0Abenchmarks%20face%20two%20key%20challenges%3A%20%281%29%20they%20cannot%20directly%20assess%20multimodal%0Areal-world%20planning%20capabilities%2C%20and%20%282%29%20they%20lack%20constraints%20or%20implicit%0Aconstraints%20across%20modalities.%20To%20address%20these%20issues%2C%20we%20introduce%20Multimodal%0APlanning%20with%20Complex%20Constraints%20%28MPCC%29%2C%20the%20first%20benchmark%20to%20systematically%0Aevaluate%20MLLMs%27%20ability%20to%20handle%20multimodal%20constraints%20in%20planning.%20To%0Aaddress%20the%20first%20challenge%2C%20MPCC%20focuses%20on%20three%20real-world%20tasks%3A%20Flight%0APlanning%2C%20Calendar%20Planning%2C%20and%20Meeting%20Planning.%20To%20solve%20the%20second%0Achallenge%2C%20we%20introduce%20complex%20constraints%20%28e.g.%20budget%2C%20temporal%2C%20and%0Aspatial%29%20in%20these%20tasks%2C%20with%20graded%20difficulty%20levels%20%28EASY%2C%20MEDIUM%2C%20HARD%29%20to%0Aseparate%20constraint%20complexity%20from%20search%20space%20expansion.%20Experiments%20on%2013%0Aadvanced%20MLLMs%20reveal%20significant%20challenges%3A%20closed-source%20models%20achieve%20only%0A21.3%25%20feasible%20plans%2C%20while%20open-source%20models%20average%20below%2011%25.%20Additionally%2C%0Awe%20observe%20that%20MLLMs%20are%20highly%20sensitive%20to%20constraint%20complexity%20and%20that%0Atraditional%20multimodal%20prompting%20strategies%20fail%20in%20multi-constraint%20scenarios.%0AOur%20work%20formalizes%20multimodal%20constraints%20in%20planning%2C%20provides%20a%20rigorous%0Aevaluation%20framework%2C%20and%20highlights%20the%20need%20for%20advancements%20in%0Aconstraint-aware%20reasoning%20for%20real-world%20MLLM%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23382v1&entry.124074799=Read"},
{"title": "Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language\n  Translation", "author": "Sobhan Asasi and Mohamed Ilyas Lakhal and Ozge Mercanoglu Sincan and Richard Bowden", "abstract": "  Sign Language Translation (SLT) is a challenging task that requires bridging\nthe modality gap between visual and linguistic information while capturing\nsubtle variations in hand shapes and movements. To address these challenges, we\nintroduce \\textbf{BeyondGloss}, a novel gloss-free SLT framework that leverages\nthe spatio-temporal reasoning capabilities of Video Large Language Models\n(VideoLLMs). Since existing VideoLLMs struggle to model long videos in detail,\nwe propose a novel approach to generate fine-grained, temporally-aware textual\ndescriptions of hand motion. A contrastive alignment module aligns these\ndescriptions with video features during pre-training, encouraging the model to\nfocus on hand-centric temporal dynamics and distinguish signs more effectively.\nTo further enrich hand-specific representations, we distill fine-grained\nfeatures from HaMeR. Additionally, we apply a contrastive loss between sign\nvideo representations and target language embeddings to reduce the modality gap\nin pre-training. \\textbf{BeyondGloss} achieves state-of-the-art performance on\nthe Phoenix14T and CSL-Daily benchmarks, demonstrating the effectiveness of the\nproposed framework. We will release the code upon acceptance of the paper.\n", "link": "http://arxiv.org/abs/2507.23575v1", "date": "2025-07-31", "relevancy": 2.077, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5273}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5138}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Gloss%3A%20A%20Hand-Centric%20Framework%20for%20Gloss-Free%20Sign%20Language%0A%20%20Translation&body=Title%3A%20Beyond%20Gloss%3A%20A%20Hand-Centric%20Framework%20for%20Gloss-Free%20Sign%20Language%0A%20%20Translation%0AAuthor%3A%20Sobhan%20Asasi%20and%20Mohamed%20Ilyas%20Lakhal%20and%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Sign%20Language%20Translation%20%28SLT%29%20is%20a%20challenging%20task%20that%20requires%20bridging%0Athe%20modality%20gap%20between%20visual%20and%20linguistic%20information%20while%20capturing%0Asubtle%20variations%20in%20hand%20shapes%20and%20movements.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20%5Ctextbf%7BBeyondGloss%7D%2C%20a%20novel%20gloss-free%20SLT%20framework%20that%20leverages%0Athe%20spatio-temporal%20reasoning%20capabilities%20of%20Video%20Large%20Language%20Models%0A%28VideoLLMs%29.%20Since%20existing%20VideoLLMs%20struggle%20to%20model%20long%20videos%20in%20detail%2C%0Awe%20propose%20a%20novel%20approach%20to%20generate%20fine-grained%2C%20temporally-aware%20textual%0Adescriptions%20of%20hand%20motion.%20A%20contrastive%20alignment%20module%20aligns%20these%0Adescriptions%20with%20video%20features%20during%20pre-training%2C%20encouraging%20the%20model%20to%0Afocus%20on%20hand-centric%20temporal%20dynamics%20and%20distinguish%20signs%20more%20effectively.%0ATo%20further%20enrich%20hand-specific%20representations%2C%20we%20distill%20fine-grained%0Afeatures%20from%20HaMeR.%20Additionally%2C%20we%20apply%20a%20contrastive%20loss%20between%20sign%0Avideo%20representations%20and%20target%20language%20embeddings%20to%20reduce%20the%20modality%20gap%0Ain%20pre-training.%20%5Ctextbf%7BBeyondGloss%7D%20achieves%20state-of-the-art%20performance%20on%0Athe%20Phoenix14T%20and%20CSL-Daily%20benchmarks%2C%20demonstrating%20the%20effectiveness%20of%20the%0Aproposed%20framework.%20We%20will%20release%20the%20code%20upon%20acceptance%20of%20the%20paper.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Gloss%253A%2520A%2520Hand-Centric%2520Framework%2520for%2520Gloss-Free%2520Sign%2520Language%250A%2520%2520Translation%26entry.906535625%3DSobhan%2520Asasi%2520and%2520Mohamed%2520Ilyas%2520Lakhal%2520and%2520Ozge%2520Mercanoglu%2520Sincan%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Translation%2520%2528SLT%2529%2520is%2520a%2520challenging%2520task%2520that%2520requires%2520bridging%250Athe%2520modality%2520gap%2520between%2520visual%2520and%2520linguistic%2520information%2520while%2520capturing%250Asubtle%2520variations%2520in%2520hand%2520shapes%2520and%2520movements.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520%255Ctextbf%257BBeyondGloss%257D%252C%2520a%2520novel%2520gloss-free%2520SLT%2520framework%2520that%2520leverages%250Athe%2520spatio-temporal%2520reasoning%2520capabilities%2520of%2520Video%2520Large%2520Language%2520Models%250A%2528VideoLLMs%2529.%2520Since%2520existing%2520VideoLLMs%2520struggle%2520to%2520model%2520long%2520videos%2520in%2520detail%252C%250Awe%2520propose%2520a%2520novel%2520approach%2520to%2520generate%2520fine-grained%252C%2520temporally-aware%2520textual%250Adescriptions%2520of%2520hand%2520motion.%2520A%2520contrastive%2520alignment%2520module%2520aligns%2520these%250Adescriptions%2520with%2520video%2520features%2520during%2520pre-training%252C%2520encouraging%2520the%2520model%2520to%250Afocus%2520on%2520hand-centric%2520temporal%2520dynamics%2520and%2520distinguish%2520signs%2520more%2520effectively.%250ATo%2520further%2520enrich%2520hand-specific%2520representations%252C%2520we%2520distill%2520fine-grained%250Afeatures%2520from%2520HaMeR.%2520Additionally%252C%2520we%2520apply%2520a%2520contrastive%2520loss%2520between%2520sign%250Avideo%2520representations%2520and%2520target%2520language%2520embeddings%2520to%2520reduce%2520the%2520modality%2520gap%250Ain%2520pre-training.%2520%255Ctextbf%257BBeyondGloss%257D%2520achieves%2520state-of-the-art%2520performance%2520on%250Athe%2520Phoenix14T%2520and%2520CSL-Daily%2520benchmarks%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520framework.%2520We%2520will%2520release%2520the%2520code%2520upon%2520acceptance%2520of%2520the%2520paper.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Gloss%3A%20A%20Hand-Centric%20Framework%20for%20Gloss-Free%20Sign%20Language%0A%20%20Translation&entry.906535625=Sobhan%20Asasi%20and%20Mohamed%20Ilyas%20Lakhal%20and%20Ozge%20Mercanoglu%20Sincan%20and%20Richard%20Bowden&entry.1292438233=%20%20Sign%20Language%20Translation%20%28SLT%29%20is%20a%20challenging%20task%20that%20requires%20bridging%0Athe%20modality%20gap%20between%20visual%20and%20linguistic%20information%20while%20capturing%0Asubtle%20variations%20in%20hand%20shapes%20and%20movements.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20%5Ctextbf%7BBeyondGloss%7D%2C%20a%20novel%20gloss-free%20SLT%20framework%20that%20leverages%0Athe%20spatio-temporal%20reasoning%20capabilities%20of%20Video%20Large%20Language%20Models%0A%28VideoLLMs%29.%20Since%20existing%20VideoLLMs%20struggle%20to%20model%20long%20videos%20in%20detail%2C%0Awe%20propose%20a%20novel%20approach%20to%20generate%20fine-grained%2C%20temporally-aware%20textual%0Adescriptions%20of%20hand%20motion.%20A%20contrastive%20alignment%20module%20aligns%20these%0Adescriptions%20with%20video%20features%20during%20pre-training%2C%20encouraging%20the%20model%20to%0Afocus%20on%20hand-centric%20temporal%20dynamics%20and%20distinguish%20signs%20more%20effectively.%0ATo%20further%20enrich%20hand-specific%20representations%2C%20we%20distill%20fine-grained%0Afeatures%20from%20HaMeR.%20Additionally%2C%20we%20apply%20a%20contrastive%20loss%20between%20sign%0Avideo%20representations%20and%20target%20language%20embeddings%20to%20reduce%20the%20modality%20gap%0Ain%20pre-training.%20%5Ctextbf%7BBeyondGloss%7D%20achieves%20state-of-the-art%20performance%20on%0Athe%20Phoenix14T%20and%20CSL-Daily%20benchmarks%2C%20demonstrating%20the%20effectiveness%20of%20the%0Aproposed%20framework.%20We%20will%20release%20the%20code%20upon%20acceptance%20of%20the%20paper.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23575v1&entry.124074799=Read"},
{"title": "Explainable Image Classification with Reduced Overconfidence for Tissue\n  Characterisation", "author": "Alfie Roddan and Chi Xu and Serine Ajlouni and Irini Kakaletri and Patra Charalampaki and Stamatia Giannarou", "abstract": "  The deployment of Machine Learning models intraoperatively for tissue\ncharacterisation can assist decision making and guide safe tumour resections.\nFor image classification models, pixel attribution methods are popular to infer\nexplainability. However, overconfidence in deep learning model's predictions\ntranslates to overconfidence in pixel attribution. In this paper, we propose\nthe first approach which incorporates risk estimation into a pixel attribution\nmethod for improved image classification explainability. The proposed method\niteratively applies a classification model with a pixel attribution method to\ncreate a volume of PA maps. This volume is used for the first time, to generate\na pixel-wise distribution of PA values. We introduce a method to generate an\nenhanced PA map by estimating the expectation values of the pixel-wise\ndistributions. In addition, the coefficient of variation (CV) is used to\nestimate pixel-wise risk of this enhanced PA map. Hence, the proposed method\nnot only provides an improved PA map but also produces an estimation of risk on\nthe output PA values. Performance evaluation on probe-based Confocal Laser\nEndomicroscopy (pCLE) data and ImageNet verifies that our improved\nexplainability method outperforms the state-of-the-art.\n", "link": "http://arxiv.org/abs/2507.23709v1", "date": "2025-07-31", "relevancy": 2.0725, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5323}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5307}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20Image%20Classification%20with%20Reduced%20Overconfidence%20for%20Tissue%0A%20%20Characterisation&body=Title%3A%20Explainable%20Image%20Classification%20with%20Reduced%20Overconfidence%20for%20Tissue%0A%20%20Characterisation%0AAuthor%3A%20Alfie%20Roddan%20and%20Chi%20Xu%20and%20Serine%20Ajlouni%20and%20Irini%20Kakaletri%20and%20Patra%20Charalampaki%20and%20Stamatia%20Giannarou%0AAbstract%3A%20%20%20The%20deployment%20of%20Machine%20Learning%20models%20intraoperatively%20for%20tissue%0Acharacterisation%20can%20assist%20decision%20making%20and%20guide%20safe%20tumour%20resections.%0AFor%20image%20classification%20models%2C%20pixel%20attribution%20methods%20are%20popular%20to%20infer%0Aexplainability.%20However%2C%20overconfidence%20in%20deep%20learning%20model%27s%20predictions%0Atranslates%20to%20overconfidence%20in%20pixel%20attribution.%20In%20this%20paper%2C%20we%20propose%0Athe%20first%20approach%20which%20incorporates%20risk%20estimation%20into%20a%20pixel%20attribution%0Amethod%20for%20improved%20image%20classification%20explainability.%20The%20proposed%20method%0Aiteratively%20applies%20a%20classification%20model%20with%20a%20pixel%20attribution%20method%20to%0Acreate%20a%20volume%20of%20PA%20maps.%20This%20volume%20is%20used%20for%20the%20first%20time%2C%20to%20generate%0Aa%20pixel-wise%20distribution%20of%20PA%20values.%20We%20introduce%20a%20method%20to%20generate%20an%0Aenhanced%20PA%20map%20by%20estimating%20the%20expectation%20values%20of%20the%20pixel-wise%0Adistributions.%20In%20addition%2C%20the%20coefficient%20of%20variation%20%28CV%29%20is%20used%20to%0Aestimate%20pixel-wise%20risk%20of%20this%20enhanced%20PA%20map.%20Hence%2C%20the%20proposed%20method%0Anot%20only%20provides%20an%20improved%20PA%20map%20but%20also%20produces%20an%20estimation%20of%20risk%20on%0Athe%20output%20PA%20values.%20Performance%20evaluation%20on%20probe-based%20Confocal%20Laser%0AEndomicroscopy%20%28pCLE%29%20data%20and%20ImageNet%20verifies%20that%20our%20improved%0Aexplainability%20method%20outperforms%20the%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23709v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520Image%2520Classification%2520with%2520Reduced%2520Overconfidence%2520for%2520Tissue%250A%2520%2520Characterisation%26entry.906535625%3DAlfie%2520Roddan%2520and%2520Chi%2520Xu%2520and%2520Serine%2520Ajlouni%2520and%2520Irini%2520Kakaletri%2520and%2520Patra%2520Charalampaki%2520and%2520Stamatia%2520Giannarou%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520Machine%2520Learning%2520models%2520intraoperatively%2520for%2520tissue%250Acharacterisation%2520can%2520assist%2520decision%2520making%2520and%2520guide%2520safe%2520tumour%2520resections.%250AFor%2520image%2520classification%2520models%252C%2520pixel%2520attribution%2520methods%2520are%2520popular%2520to%2520infer%250Aexplainability.%2520However%252C%2520overconfidence%2520in%2520deep%2520learning%2520model%2527s%2520predictions%250Atranslates%2520to%2520overconfidence%2520in%2520pixel%2520attribution.%2520In%2520this%2520paper%252C%2520we%2520propose%250Athe%2520first%2520approach%2520which%2520incorporates%2520risk%2520estimation%2520into%2520a%2520pixel%2520attribution%250Amethod%2520for%2520improved%2520image%2520classification%2520explainability.%2520The%2520proposed%2520method%250Aiteratively%2520applies%2520a%2520classification%2520model%2520with%2520a%2520pixel%2520attribution%2520method%2520to%250Acreate%2520a%2520volume%2520of%2520PA%2520maps.%2520This%2520volume%2520is%2520used%2520for%2520the%2520first%2520time%252C%2520to%2520generate%250Aa%2520pixel-wise%2520distribution%2520of%2520PA%2520values.%2520We%2520introduce%2520a%2520method%2520to%2520generate%2520an%250Aenhanced%2520PA%2520map%2520by%2520estimating%2520the%2520expectation%2520values%2520of%2520the%2520pixel-wise%250Adistributions.%2520In%2520addition%252C%2520the%2520coefficient%2520of%2520variation%2520%2528CV%2529%2520is%2520used%2520to%250Aestimate%2520pixel-wise%2520risk%2520of%2520this%2520enhanced%2520PA%2520map.%2520Hence%252C%2520the%2520proposed%2520method%250Anot%2520only%2520provides%2520an%2520improved%2520PA%2520map%2520but%2520also%2520produces%2520an%2520estimation%2520of%2520risk%2520on%250Athe%2520output%2520PA%2520values.%2520Performance%2520evaluation%2520on%2520probe-based%2520Confocal%2520Laser%250AEndomicroscopy%2520%2528pCLE%2529%2520data%2520and%2520ImageNet%2520verifies%2520that%2520our%2520improved%250Aexplainability%2520method%2520outperforms%2520the%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23709v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20Image%20Classification%20with%20Reduced%20Overconfidence%20for%20Tissue%0A%20%20Characterisation&entry.906535625=Alfie%20Roddan%20and%20Chi%20Xu%20and%20Serine%20Ajlouni%20and%20Irini%20Kakaletri%20and%20Patra%20Charalampaki%20and%20Stamatia%20Giannarou&entry.1292438233=%20%20The%20deployment%20of%20Machine%20Learning%20models%20intraoperatively%20for%20tissue%0Acharacterisation%20can%20assist%20decision%20making%20and%20guide%20safe%20tumour%20resections.%0AFor%20image%20classification%20models%2C%20pixel%20attribution%20methods%20are%20popular%20to%20infer%0Aexplainability.%20However%2C%20overconfidence%20in%20deep%20learning%20model%27s%20predictions%0Atranslates%20to%20overconfidence%20in%20pixel%20attribution.%20In%20this%20paper%2C%20we%20propose%0Athe%20first%20approach%20which%20incorporates%20risk%20estimation%20into%20a%20pixel%20attribution%0Amethod%20for%20improved%20image%20classification%20explainability.%20The%20proposed%20method%0Aiteratively%20applies%20a%20classification%20model%20with%20a%20pixel%20attribution%20method%20to%0Acreate%20a%20volume%20of%20PA%20maps.%20This%20volume%20is%20used%20for%20the%20first%20time%2C%20to%20generate%0Aa%20pixel-wise%20distribution%20of%20PA%20values.%20We%20introduce%20a%20method%20to%20generate%20an%0Aenhanced%20PA%20map%20by%20estimating%20the%20expectation%20values%20of%20the%20pixel-wise%0Adistributions.%20In%20addition%2C%20the%20coefficient%20of%20variation%20%28CV%29%20is%20used%20to%0Aestimate%20pixel-wise%20risk%20of%20this%20enhanced%20PA%20map.%20Hence%2C%20the%20proposed%20method%0Anot%20only%20provides%20an%20improved%20PA%20map%20but%20also%20produces%20an%20estimation%20of%20risk%20on%0Athe%20output%20PA%20values.%20Performance%20evaluation%20on%20probe-based%20Confocal%20Laser%0AEndomicroscopy%20%28pCLE%29%20data%20and%20ImageNet%20verifies%20that%20our%20improved%0Aexplainability%20method%20outperforms%20the%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23709v1&entry.124074799=Read"},
{"title": "Optimizing against Infeasible Inclusions from Data for Semantic\n  Segmentation through Morphology", "author": "Shamik Basu and Luc Van Gool and Christos Sakaridis", "abstract": "  State-of-the-art semantic segmentation models are typically optimized in a\ndata-driven fashion, minimizing solely per-pixel or per-segment classification\nobjectives on their training data. This purely data-driven paradigm often leads\nto absurd segmentations, especially when the domain of input images is shifted\nfrom the one encountered during training. For instance, state-of-the-art models\nmay assign the label \"road\" to a segment that is included by another segment\nthat is respectively labeled as \"sky\". However, the ground truth of the\nexisting dataset at hand dictates that such inclusion is not feasible. Our\nmethod, Infeasible Semantic Inclusions (InSeIn), first extracts explicit\ninclusion constraints that govern spatial class relations from the semantic\nsegmentation training set at hand in an offline, data-driven fashion, and then\nenforces a morphological yet differentiable loss that penalizes violations of\nthese constraints during training to promote prediction feasibility. InSeIn is\na light-weight plug-and-play method, constitutes a novel step towards\nminimizing infeasible semantic inclusions in the predictions of learned\nsegmentation models, and yields consistent and significant performance\nimprovements over diverse state-of-the-art networks across the ADE20K,\nCityscapes, and ACDC datasets. https://github.com/SHAMIK-97/InSeIn\n", "link": "http://arxiv.org/abs/2408.14672v6", "date": "2025-07-31", "relevancy": 2.0655, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20against%20Infeasible%20Inclusions%20from%20Data%20for%20Semantic%0A%20%20Segmentation%20through%20Morphology&body=Title%3A%20Optimizing%20against%20Infeasible%20Inclusions%20from%20Data%20for%20Semantic%0A%20%20Segmentation%20through%20Morphology%0AAuthor%3A%20Shamik%20Basu%20and%20Luc%20Van%20Gool%20and%20Christos%20Sakaridis%0AAbstract%3A%20%20%20State-of-the-art%20semantic%20segmentation%20models%20are%20typically%20optimized%20in%20a%0Adata-driven%20fashion%2C%20minimizing%20solely%20per-pixel%20or%20per-segment%20classification%0Aobjectives%20on%20their%20training%20data.%20This%20purely%20data-driven%20paradigm%20often%20leads%0Ato%20absurd%20segmentations%2C%20especially%20when%20the%20domain%20of%20input%20images%20is%20shifted%0Afrom%20the%20one%20encountered%20during%20training.%20For%20instance%2C%20state-of-the-art%20models%0Amay%20assign%20the%20label%20%22road%22%20to%20a%20segment%20that%20is%20included%20by%20another%20segment%0Athat%20is%20respectively%20labeled%20as%20%22sky%22.%20However%2C%20the%20ground%20truth%20of%20the%0Aexisting%20dataset%20at%20hand%20dictates%20that%20such%20inclusion%20is%20not%20feasible.%20Our%0Amethod%2C%20Infeasible%20Semantic%20Inclusions%20%28InSeIn%29%2C%20first%20extracts%20explicit%0Ainclusion%20constraints%20that%20govern%20spatial%20class%20relations%20from%20the%20semantic%0Asegmentation%20training%20set%20at%20hand%20in%20an%20offline%2C%20data-driven%20fashion%2C%20and%20then%0Aenforces%20a%20morphological%20yet%20differentiable%20loss%20that%20penalizes%20violations%20of%0Athese%20constraints%20during%20training%20to%20promote%20prediction%20feasibility.%20InSeIn%20is%0Aa%20light-weight%20plug-and-play%20method%2C%20constitutes%20a%20novel%20step%20towards%0Aminimizing%20infeasible%20semantic%20inclusions%20in%20the%20predictions%20of%20learned%0Asegmentation%20models%2C%20and%20yields%20consistent%20and%20significant%20performance%0Aimprovements%20over%20diverse%20state-of-the-art%20networks%20across%20the%20ADE20K%2C%0ACityscapes%2C%20and%20ACDC%20datasets.%20https%3A//github.com/SHAMIK-97/InSeIn%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14672v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520against%2520Infeasible%2520Inclusions%2520from%2520Data%2520for%2520Semantic%250A%2520%2520Segmentation%2520through%2520Morphology%26entry.906535625%3DShamik%2520Basu%2520and%2520Luc%2520Van%2520Gool%2520and%2520Christos%2520Sakaridis%26entry.1292438233%3D%2520%2520State-of-the-art%2520semantic%2520segmentation%2520models%2520are%2520typically%2520optimized%2520in%2520a%250Adata-driven%2520fashion%252C%2520minimizing%2520solely%2520per-pixel%2520or%2520per-segment%2520classification%250Aobjectives%2520on%2520their%2520training%2520data.%2520This%2520purely%2520data-driven%2520paradigm%2520often%2520leads%250Ato%2520absurd%2520segmentations%252C%2520especially%2520when%2520the%2520domain%2520of%2520input%2520images%2520is%2520shifted%250Afrom%2520the%2520one%2520encountered%2520during%2520training.%2520For%2520instance%252C%2520state-of-the-art%2520models%250Amay%2520assign%2520the%2520label%2520%2522road%2522%2520to%2520a%2520segment%2520that%2520is%2520included%2520by%2520another%2520segment%250Athat%2520is%2520respectively%2520labeled%2520as%2520%2522sky%2522.%2520However%252C%2520the%2520ground%2520truth%2520of%2520the%250Aexisting%2520dataset%2520at%2520hand%2520dictates%2520that%2520such%2520inclusion%2520is%2520not%2520feasible.%2520Our%250Amethod%252C%2520Infeasible%2520Semantic%2520Inclusions%2520%2528InSeIn%2529%252C%2520first%2520extracts%2520explicit%250Ainclusion%2520constraints%2520that%2520govern%2520spatial%2520class%2520relations%2520from%2520the%2520semantic%250Asegmentation%2520training%2520set%2520at%2520hand%2520in%2520an%2520offline%252C%2520data-driven%2520fashion%252C%2520and%2520then%250Aenforces%2520a%2520morphological%2520yet%2520differentiable%2520loss%2520that%2520penalizes%2520violations%2520of%250Athese%2520constraints%2520during%2520training%2520to%2520promote%2520prediction%2520feasibility.%2520InSeIn%2520is%250Aa%2520light-weight%2520plug-and-play%2520method%252C%2520constitutes%2520a%2520novel%2520step%2520towards%250Aminimizing%2520infeasible%2520semantic%2520inclusions%2520in%2520the%2520predictions%2520of%2520learned%250Asegmentation%2520models%252C%2520and%2520yields%2520consistent%2520and%2520significant%2520performance%250Aimprovements%2520over%2520diverse%2520state-of-the-art%2520networks%2520across%2520the%2520ADE20K%252C%250ACityscapes%252C%2520and%2520ACDC%2520datasets.%2520https%253A//github.com/SHAMIK-97/InSeIn%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14672v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20against%20Infeasible%20Inclusions%20from%20Data%20for%20Semantic%0A%20%20Segmentation%20through%20Morphology&entry.906535625=Shamik%20Basu%20and%20Luc%20Van%20Gool%20and%20Christos%20Sakaridis&entry.1292438233=%20%20State-of-the-art%20semantic%20segmentation%20models%20are%20typically%20optimized%20in%20a%0Adata-driven%20fashion%2C%20minimizing%20solely%20per-pixel%20or%20per-segment%20classification%0Aobjectives%20on%20their%20training%20data.%20This%20purely%20data-driven%20paradigm%20often%20leads%0Ato%20absurd%20segmentations%2C%20especially%20when%20the%20domain%20of%20input%20images%20is%20shifted%0Afrom%20the%20one%20encountered%20during%20training.%20For%20instance%2C%20state-of-the-art%20models%0Amay%20assign%20the%20label%20%22road%22%20to%20a%20segment%20that%20is%20included%20by%20another%20segment%0Athat%20is%20respectively%20labeled%20as%20%22sky%22.%20However%2C%20the%20ground%20truth%20of%20the%0Aexisting%20dataset%20at%20hand%20dictates%20that%20such%20inclusion%20is%20not%20feasible.%20Our%0Amethod%2C%20Infeasible%20Semantic%20Inclusions%20%28InSeIn%29%2C%20first%20extracts%20explicit%0Ainclusion%20constraints%20that%20govern%20spatial%20class%20relations%20from%20the%20semantic%0Asegmentation%20training%20set%20at%20hand%20in%20an%20offline%2C%20data-driven%20fashion%2C%20and%20then%0Aenforces%20a%20morphological%20yet%20differentiable%20loss%20that%20penalizes%20violations%20of%0Athese%20constraints%20during%20training%20to%20promote%20prediction%20feasibility.%20InSeIn%20is%0Aa%20light-weight%20plug-and-play%20method%2C%20constitutes%20a%20novel%20step%20towards%0Aminimizing%20infeasible%20semantic%20inclusions%20in%20the%20predictions%20of%20learned%0Asegmentation%20models%2C%20and%20yields%20consistent%20and%20significant%20performance%0Aimprovements%20over%20diverse%20state-of-the-art%20networks%20across%20the%20ADE20K%2C%0ACityscapes%2C%20and%20ACDC%20datasets.%20https%3A//github.com/SHAMIK-97/InSeIn%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14672v6&entry.124074799=Read"},
{"title": "DICOM De-Identification via Hybrid AI and Rule-Based Framework for\n  Scalable, Uncertainty-Aware Redaction", "author": "Kyle Naddeo and Nikolas Koutsoubis and Rahul Krish and Ghulam Rasool and Nidhal Bouaynaya and Tony OSullivan and Raj Krish", "abstract": "  Access to medical imaging and associated text data has the potential to drive\nmajor advances in healthcare research and patient outcomes. However, the\npresence of Protected Health Information (PHI) and Personally Identifiable\nInformation (PII) in Digital Imaging and Communications in Medicine (DICOM)\nfiles presents a significant barrier to the ethical and secure sharing of\nimaging datasets. This paper presents a hybrid de-identification framework\ndeveloped by Impact Business Information Solutions (IBIS) that combines\nrule-based and AI-driven techniques, and rigorous uncertainty quantification\nfor comprehensive PHI/PII removal from both metadata and pixel data.\n  Our approach begins with a two-tiered rule-based system targeting explicit\nand inferred metadata elements, further augmented by a large language model\n(LLM) fine-tuned for Named Entity Recognition (NER), and trained on a suite of\nsynthetic datasets simulating realistic clinical PHI/PII. For pixel data, we\nemploy an uncertainty-aware Faster R-CNN model to localize embedded text,\nextract candidate PHI via Optical Character Recognition (OCR), and apply the\nNER pipeline for final redaction. Crucially, uncertainty quantification\nprovides confidence measures for AI-based detections to enhance automation\nreliability and enable informed human-in-the-loop verification to manage\nresidual risks.\n  This uncertainty-aware deidentification framework achieves robust performance\nacross benchmark datasets and regulatory standards, including DICOM, HIPAA, and\nTCIA compliance metrics. By combining scalable automation, uncertainty\nquantification, and rigorous quality assurance, our solution addresses critical\nchallenges in medical data de-identification and supports the secure, ethical,\nand trustworthy release of imaging data for research.\n", "link": "http://arxiv.org/abs/2507.23736v1", "date": "2025-07-31", "relevancy": 2.0637, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5894}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5022}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DICOM%20De-Identification%20via%20Hybrid%20AI%20and%20Rule-Based%20Framework%20for%0A%20%20Scalable%2C%20Uncertainty-Aware%20Redaction&body=Title%3A%20DICOM%20De-Identification%20via%20Hybrid%20AI%20and%20Rule-Based%20Framework%20for%0A%20%20Scalable%2C%20Uncertainty-Aware%20Redaction%0AAuthor%3A%20Kyle%20Naddeo%20and%20Nikolas%20Koutsoubis%20and%20Rahul%20Krish%20and%20Ghulam%20Rasool%20and%20Nidhal%20Bouaynaya%20and%20Tony%20OSullivan%20and%20Raj%20Krish%0AAbstract%3A%20%20%20Access%20to%20medical%20imaging%20and%20associated%20text%20data%20has%20the%20potential%20to%20drive%0Amajor%20advances%20in%20healthcare%20research%20and%20patient%20outcomes.%20However%2C%20the%0Apresence%20of%20Protected%20Health%20Information%20%28PHI%29%20and%20Personally%20Identifiable%0AInformation%20%28PII%29%20in%20Digital%20Imaging%20and%20Communications%20in%20Medicine%20%28DICOM%29%0Afiles%20presents%20a%20significant%20barrier%20to%20the%20ethical%20and%20secure%20sharing%20of%0Aimaging%20datasets.%20This%20paper%20presents%20a%20hybrid%20de-identification%20framework%0Adeveloped%20by%20Impact%20Business%20Information%20Solutions%20%28IBIS%29%20that%20combines%0Arule-based%20and%20AI-driven%20techniques%2C%20and%20rigorous%20uncertainty%20quantification%0Afor%20comprehensive%20PHI/PII%20removal%20from%20both%20metadata%20and%20pixel%20data.%0A%20%20Our%20approach%20begins%20with%20a%20two-tiered%20rule-based%20system%20targeting%20explicit%0Aand%20inferred%20metadata%20elements%2C%20further%20augmented%20by%20a%20large%20language%20model%0A%28LLM%29%20fine-tuned%20for%20Named%20Entity%20Recognition%20%28NER%29%2C%20and%20trained%20on%20a%20suite%20of%0Asynthetic%20datasets%20simulating%20realistic%20clinical%20PHI/PII.%20For%20pixel%20data%2C%20we%0Aemploy%20an%20uncertainty-aware%20Faster%20R-CNN%20model%20to%20localize%20embedded%20text%2C%0Aextract%20candidate%20PHI%20via%20Optical%20Character%20Recognition%20%28OCR%29%2C%20and%20apply%20the%0ANER%20pipeline%20for%20final%20redaction.%20Crucially%2C%20uncertainty%20quantification%0Aprovides%20confidence%20measures%20for%20AI-based%20detections%20to%20enhance%20automation%0Areliability%20and%20enable%20informed%20human-in-the-loop%20verification%20to%20manage%0Aresidual%20risks.%0A%20%20This%20uncertainty-aware%20deidentification%20framework%20achieves%20robust%20performance%0Aacross%20benchmark%20datasets%20and%20regulatory%20standards%2C%20including%20DICOM%2C%20HIPAA%2C%20and%0ATCIA%20compliance%20metrics.%20By%20combining%20scalable%20automation%2C%20uncertainty%0Aquantification%2C%20and%20rigorous%20quality%20assurance%2C%20our%20solution%20addresses%20critical%0Achallenges%20in%20medical%20data%20de-identification%20and%20supports%20the%20secure%2C%20ethical%2C%0Aand%20trustworthy%20release%20of%20imaging%20data%20for%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDICOM%2520De-Identification%2520via%2520Hybrid%2520AI%2520and%2520Rule-Based%2520Framework%2520for%250A%2520%2520Scalable%252C%2520Uncertainty-Aware%2520Redaction%26entry.906535625%3DKyle%2520Naddeo%2520and%2520Nikolas%2520Koutsoubis%2520and%2520Rahul%2520Krish%2520and%2520Ghulam%2520Rasool%2520and%2520Nidhal%2520Bouaynaya%2520and%2520Tony%2520OSullivan%2520and%2520Raj%2520Krish%26entry.1292438233%3D%2520%2520Access%2520to%2520medical%2520imaging%2520and%2520associated%2520text%2520data%2520has%2520the%2520potential%2520to%2520drive%250Amajor%2520advances%2520in%2520healthcare%2520research%2520and%2520patient%2520outcomes.%2520However%252C%2520the%250Apresence%2520of%2520Protected%2520Health%2520Information%2520%2528PHI%2529%2520and%2520Personally%2520Identifiable%250AInformation%2520%2528PII%2529%2520in%2520Digital%2520Imaging%2520and%2520Communications%2520in%2520Medicine%2520%2528DICOM%2529%250Afiles%2520presents%2520a%2520significant%2520barrier%2520to%2520the%2520ethical%2520and%2520secure%2520sharing%2520of%250Aimaging%2520datasets.%2520This%2520paper%2520presents%2520a%2520hybrid%2520de-identification%2520framework%250Adeveloped%2520by%2520Impact%2520Business%2520Information%2520Solutions%2520%2528IBIS%2529%2520that%2520combines%250Arule-based%2520and%2520AI-driven%2520techniques%252C%2520and%2520rigorous%2520uncertainty%2520quantification%250Afor%2520comprehensive%2520PHI/PII%2520removal%2520from%2520both%2520metadata%2520and%2520pixel%2520data.%250A%2520%2520Our%2520approach%2520begins%2520with%2520a%2520two-tiered%2520rule-based%2520system%2520targeting%2520explicit%250Aand%2520inferred%2520metadata%2520elements%252C%2520further%2520augmented%2520by%2520a%2520large%2520language%2520model%250A%2528LLM%2529%2520fine-tuned%2520for%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%252C%2520and%2520trained%2520on%2520a%2520suite%2520of%250Asynthetic%2520datasets%2520simulating%2520realistic%2520clinical%2520PHI/PII.%2520For%2520pixel%2520data%252C%2520we%250Aemploy%2520an%2520uncertainty-aware%2520Faster%2520R-CNN%2520model%2520to%2520localize%2520embedded%2520text%252C%250Aextract%2520candidate%2520PHI%2520via%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529%252C%2520and%2520apply%2520the%250ANER%2520pipeline%2520for%2520final%2520redaction.%2520Crucially%252C%2520uncertainty%2520quantification%250Aprovides%2520confidence%2520measures%2520for%2520AI-based%2520detections%2520to%2520enhance%2520automation%250Areliability%2520and%2520enable%2520informed%2520human-in-the-loop%2520verification%2520to%2520manage%250Aresidual%2520risks.%250A%2520%2520This%2520uncertainty-aware%2520deidentification%2520framework%2520achieves%2520robust%2520performance%250Aacross%2520benchmark%2520datasets%2520and%2520regulatory%2520standards%252C%2520including%2520DICOM%252C%2520HIPAA%252C%2520and%250ATCIA%2520compliance%2520metrics.%2520By%2520combining%2520scalable%2520automation%252C%2520uncertainty%250Aquantification%252C%2520and%2520rigorous%2520quality%2520assurance%252C%2520our%2520solution%2520addresses%2520critical%250Achallenges%2520in%2520medical%2520data%2520de-identification%2520and%2520supports%2520the%2520secure%252C%2520ethical%252C%250Aand%2520trustworthy%2520release%2520of%2520imaging%2520data%2520for%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DICOM%20De-Identification%20via%20Hybrid%20AI%20and%20Rule-Based%20Framework%20for%0A%20%20Scalable%2C%20Uncertainty-Aware%20Redaction&entry.906535625=Kyle%20Naddeo%20and%20Nikolas%20Koutsoubis%20and%20Rahul%20Krish%20and%20Ghulam%20Rasool%20and%20Nidhal%20Bouaynaya%20and%20Tony%20OSullivan%20and%20Raj%20Krish&entry.1292438233=%20%20Access%20to%20medical%20imaging%20and%20associated%20text%20data%20has%20the%20potential%20to%20drive%0Amajor%20advances%20in%20healthcare%20research%20and%20patient%20outcomes.%20However%2C%20the%0Apresence%20of%20Protected%20Health%20Information%20%28PHI%29%20and%20Personally%20Identifiable%0AInformation%20%28PII%29%20in%20Digital%20Imaging%20and%20Communications%20in%20Medicine%20%28DICOM%29%0Afiles%20presents%20a%20significant%20barrier%20to%20the%20ethical%20and%20secure%20sharing%20of%0Aimaging%20datasets.%20This%20paper%20presents%20a%20hybrid%20de-identification%20framework%0Adeveloped%20by%20Impact%20Business%20Information%20Solutions%20%28IBIS%29%20that%20combines%0Arule-based%20and%20AI-driven%20techniques%2C%20and%20rigorous%20uncertainty%20quantification%0Afor%20comprehensive%20PHI/PII%20removal%20from%20both%20metadata%20and%20pixel%20data.%0A%20%20Our%20approach%20begins%20with%20a%20two-tiered%20rule-based%20system%20targeting%20explicit%0Aand%20inferred%20metadata%20elements%2C%20further%20augmented%20by%20a%20large%20language%20model%0A%28LLM%29%20fine-tuned%20for%20Named%20Entity%20Recognition%20%28NER%29%2C%20and%20trained%20on%20a%20suite%20of%0Asynthetic%20datasets%20simulating%20realistic%20clinical%20PHI/PII.%20For%20pixel%20data%2C%20we%0Aemploy%20an%20uncertainty-aware%20Faster%20R-CNN%20model%20to%20localize%20embedded%20text%2C%0Aextract%20candidate%20PHI%20via%20Optical%20Character%20Recognition%20%28OCR%29%2C%20and%20apply%20the%0ANER%20pipeline%20for%20final%20redaction.%20Crucially%2C%20uncertainty%20quantification%0Aprovides%20confidence%20measures%20for%20AI-based%20detections%20to%20enhance%20automation%0Areliability%20and%20enable%20informed%20human-in-the-loop%20verification%20to%20manage%0Aresidual%20risks.%0A%20%20This%20uncertainty-aware%20deidentification%20framework%20achieves%20robust%20performance%0Aacross%20benchmark%20datasets%20and%20regulatory%20standards%2C%20including%20DICOM%2C%20HIPAA%2C%20and%0ATCIA%20compliance%20metrics.%20By%20combining%20scalable%20automation%2C%20uncertainty%0Aquantification%2C%20and%20rigorous%20quality%20assurance%2C%20our%20solution%20addresses%20critical%0Achallenges%20in%20medical%20data%20de-identification%20and%20supports%20the%20secure%2C%20ethical%2C%0Aand%20trustworthy%20release%20of%20imaging%20data%20for%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23736v1&entry.124074799=Read"},
{"title": "Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient\n  and Scalable DNN Accelerator Design", "author": "Yinhui Ma and Tomomasa Yamasaki and Zhehui Wang and Tao Luo and Bo Wang", "abstract": "  Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach\nto automatically co-optimizing neural network performance and hardware energy\nefficiency, making it particularly useful for the development of Deep Neural\nNetwork accelerators on the edge. However, the extensive search space and high\ncomputational cost pose significant challenges to its practical adoption. To\naddress these limitations, we propose Coflex, a novel HW-NAS framework that\nintegrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian\noptimization. By leveraging sparse inducing points, Coflex reduces the GP\nkernel complexity from cubic to near-linear with respect to the number of\ntraining samples, without compromising optimization performance. This enables\nscalable approximation of large-scale search space, substantially decreasing\ncomputational overhead while preserving high predictive accuracy. We evaluate\nthe efficacy of Coflex across various benchmarks, focusing on\naccelerator-specific architecture. Our experi- mental results show that Coflex\noutperforms state-of-the-art methods in terms of network accuracy and\nEnergy-Delay-Product, while achieving a computational speed-up ranging from\n1.9x to 9.5x.\n", "link": "http://arxiv.org/abs/2507.23437v1", "date": "2025-07-31", "relevancy": 2.0634, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5227}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5208}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coflex%3A%20Enhancing%20HW-NAS%20with%20Sparse%20Gaussian%20Processes%20for%20Efficient%0A%20%20and%20Scalable%20DNN%20Accelerator%20Design&body=Title%3A%20Coflex%3A%20Enhancing%20HW-NAS%20with%20Sparse%20Gaussian%20Processes%20for%20Efficient%0A%20%20and%20Scalable%20DNN%20Accelerator%20Design%0AAuthor%3A%20Yinhui%20Ma%20and%20Tomomasa%20Yamasaki%20and%20Zhehui%20Wang%20and%20Tao%20Luo%20and%20Bo%20Wang%0AAbstract%3A%20%20%20Hardware-Aware%20Neural%20Architecture%20Search%20%28HW-NAS%29%20is%20an%20efficient%20approach%0Ato%20automatically%20co-optimizing%20neural%20network%20performance%20and%20hardware%20energy%0Aefficiency%2C%20making%20it%20particularly%20useful%20for%20the%20development%20of%20Deep%20Neural%0ANetwork%20accelerators%20on%20the%20edge.%20However%2C%20the%20extensive%20search%20space%20and%20high%0Acomputational%20cost%20pose%20significant%20challenges%20to%20its%20practical%20adoption.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20Coflex%2C%20a%20novel%20HW-NAS%20framework%20that%0Aintegrates%20the%20Sparse%20Gaussian%20Process%20%28SGP%29%20with%20multi-objective%20Bayesian%0Aoptimization.%20By%20leveraging%20sparse%20inducing%20points%2C%20Coflex%20reduces%20the%20GP%0Akernel%20complexity%20from%20cubic%20to%20near-linear%20with%20respect%20to%20the%20number%20of%0Atraining%20samples%2C%20without%20compromising%20optimization%20performance.%20This%20enables%0Ascalable%20approximation%20of%20large-scale%20search%20space%2C%20substantially%20decreasing%0Acomputational%20overhead%20while%20preserving%20high%20predictive%20accuracy.%20We%20evaluate%0Athe%20efficacy%20of%20Coflex%20across%20various%20benchmarks%2C%20focusing%20on%0Aaccelerator-specific%20architecture.%20Our%20experi-%20mental%20results%20show%20that%20Coflex%0Aoutperforms%20state-of-the-art%20methods%20in%20terms%20of%20network%20accuracy%20and%0AEnergy-Delay-Product%2C%20while%20achieving%20a%20computational%20speed-up%20ranging%20from%0A1.9x%20to%209.5x.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoflex%253A%2520Enhancing%2520HW-NAS%2520with%2520Sparse%2520Gaussian%2520Processes%2520for%2520Efficient%250A%2520%2520and%2520Scalable%2520DNN%2520Accelerator%2520Design%26entry.906535625%3DYinhui%2520Ma%2520and%2520Tomomasa%2520Yamasaki%2520and%2520Zhehui%2520Wang%2520and%2520Tao%2520Luo%2520and%2520Bo%2520Wang%26entry.1292438233%3D%2520%2520Hardware-Aware%2520Neural%2520Architecture%2520Search%2520%2528HW-NAS%2529%2520is%2520an%2520efficient%2520approach%250Ato%2520automatically%2520co-optimizing%2520neural%2520network%2520performance%2520and%2520hardware%2520energy%250Aefficiency%252C%2520making%2520it%2520particularly%2520useful%2520for%2520the%2520development%2520of%2520Deep%2520Neural%250ANetwork%2520accelerators%2520on%2520the%2520edge.%2520However%252C%2520the%2520extensive%2520search%2520space%2520and%2520high%250Acomputational%2520cost%2520pose%2520significant%2520challenges%2520to%2520its%2520practical%2520adoption.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520Coflex%252C%2520a%2520novel%2520HW-NAS%2520framework%2520that%250Aintegrates%2520the%2520Sparse%2520Gaussian%2520Process%2520%2528SGP%2529%2520with%2520multi-objective%2520Bayesian%250Aoptimization.%2520By%2520leveraging%2520sparse%2520inducing%2520points%252C%2520Coflex%2520reduces%2520the%2520GP%250Akernel%2520complexity%2520from%2520cubic%2520to%2520near-linear%2520with%2520respect%2520to%2520the%2520number%2520of%250Atraining%2520samples%252C%2520without%2520compromising%2520optimization%2520performance.%2520This%2520enables%250Ascalable%2520approximation%2520of%2520large-scale%2520search%2520space%252C%2520substantially%2520decreasing%250Acomputational%2520overhead%2520while%2520preserving%2520high%2520predictive%2520accuracy.%2520We%2520evaluate%250Athe%2520efficacy%2520of%2520Coflex%2520across%2520various%2520benchmarks%252C%2520focusing%2520on%250Aaccelerator-specific%2520architecture.%2520Our%2520experi-%2520mental%2520results%2520show%2520that%2520Coflex%250Aoutperforms%2520state-of-the-art%2520methods%2520in%2520terms%2520of%2520network%2520accuracy%2520and%250AEnergy-Delay-Product%252C%2520while%2520achieving%2520a%2520computational%2520speed-up%2520ranging%2520from%250A1.9x%2520to%25209.5x.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coflex%3A%20Enhancing%20HW-NAS%20with%20Sparse%20Gaussian%20Processes%20for%20Efficient%0A%20%20and%20Scalable%20DNN%20Accelerator%20Design&entry.906535625=Yinhui%20Ma%20and%20Tomomasa%20Yamasaki%20and%20Zhehui%20Wang%20and%20Tao%20Luo%20and%20Bo%20Wang&entry.1292438233=%20%20Hardware-Aware%20Neural%20Architecture%20Search%20%28HW-NAS%29%20is%20an%20efficient%20approach%0Ato%20automatically%20co-optimizing%20neural%20network%20performance%20and%20hardware%20energy%0Aefficiency%2C%20making%20it%20particularly%20useful%20for%20the%20development%20of%20Deep%20Neural%0ANetwork%20accelerators%20on%20the%20edge.%20However%2C%20the%20extensive%20search%20space%20and%20high%0Acomputational%20cost%20pose%20significant%20challenges%20to%20its%20practical%20adoption.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20Coflex%2C%20a%20novel%20HW-NAS%20framework%20that%0Aintegrates%20the%20Sparse%20Gaussian%20Process%20%28SGP%29%20with%20multi-objective%20Bayesian%0Aoptimization.%20By%20leveraging%20sparse%20inducing%20points%2C%20Coflex%20reduces%20the%20GP%0Akernel%20complexity%20from%20cubic%20to%20near-linear%20with%20respect%20to%20the%20number%20of%0Atraining%20samples%2C%20without%20compromising%20optimization%20performance.%20This%20enables%0Ascalable%20approximation%20of%20large-scale%20search%20space%2C%20substantially%20decreasing%0Acomputational%20overhead%20while%20preserving%20high%20predictive%20accuracy.%20We%20evaluate%0Athe%20efficacy%20of%20Coflex%20across%20various%20benchmarks%2C%20focusing%20on%0Aaccelerator-specific%20architecture.%20Our%20experi-%20mental%20results%20show%20that%20Coflex%0Aoutperforms%20state-of-the-art%20methods%20in%20terms%20of%20network%20accuracy%20and%0AEnergy-Delay-Product%2C%20while%20achieving%20a%20computational%20speed-up%20ranging%20from%0A1.9x%20to%209.5x.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23437v1&entry.124074799=Read"},
{"title": "A Lightweight Optimization Framework for Estimating 3D Brain Tumor\n  Infiltration", "author": "Jonas Weidner and Michal Balcerak and Ivan Ezhov and Andr\u00e9 Datchev and Laurin Lux and Lucas Zimmer and Daniel Rueckert and Bj\u00f6rn Menze and Benedikt Wiestler", "abstract": "  Glioblastoma, the most aggressive primary brain tumor, poses a severe\nclinical challenge due to its diffuse microscopic infiltration, which remains\nlargely undetected on standard MRI. As a result, current radiotherapy planning\nemploys a uniform 15 mm margin around the resection cavity, failing to capture\npatient-specific tumor spread. Tumor growth modeling offers a promising\napproach to reveal this hidden infiltration. However, methods based on partial\ndifferential equations or physics-informed neural networks tend to be\ncomputationally intensive or overly constrained, limiting their clinical\nadaptability to individual patients. In this work, we propose a lightweight,\nrapid, and robust optimization framework that estimates the 3D tumor\nconcentration by fitting it to MRI tumor segmentations while enforcing a smooth\nconcentration landscape. This approach achieves superior tumor recurrence\nprediction on 192 brain tumor patients across two public datasets,\noutperforming state-of-the-art baselines while reducing runtime from 30 minutes\nto less than one minute. Furthermore, we demonstrate the framework's\nversatility and adaptability by showing its ability to seamlessly integrate\nadditional imaging modalities or physical constraints.\n", "link": "http://arxiv.org/abs/2412.13811v2", "date": "2025-07-31", "relevancy": 2.0602, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5212}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Optimization%20Framework%20for%20Estimating%203D%20Brain%20Tumor%0A%20%20Infiltration&body=Title%3A%20A%20Lightweight%20Optimization%20Framework%20for%20Estimating%203D%20Brain%20Tumor%0A%20%20Infiltration%0AAuthor%3A%20Jonas%20Weidner%20and%20Michal%20Balcerak%20and%20Ivan%20Ezhov%20and%20Andr%C3%A9%20Datchev%20and%20Laurin%20Lux%20and%20Lucas%20Zimmer%20and%20Daniel%20Rueckert%20and%20Bj%C3%B6rn%20Menze%20and%20Benedikt%20Wiestler%0AAbstract%3A%20%20%20Glioblastoma%2C%20the%20most%20aggressive%20primary%20brain%20tumor%2C%20poses%20a%20severe%0Aclinical%20challenge%20due%20to%20its%20diffuse%20microscopic%20infiltration%2C%20which%20remains%0Alargely%20undetected%20on%20standard%20MRI.%20As%20a%20result%2C%20current%20radiotherapy%20planning%0Aemploys%20a%20uniform%2015%20mm%20margin%20around%20the%20resection%20cavity%2C%20failing%20to%20capture%0Apatient-specific%20tumor%20spread.%20Tumor%20growth%20modeling%20offers%20a%20promising%0Aapproach%20to%20reveal%20this%20hidden%20infiltration.%20However%2C%20methods%20based%20on%20partial%0Adifferential%20equations%20or%20physics-informed%20neural%20networks%20tend%20to%20be%0Acomputationally%20intensive%20or%20overly%20constrained%2C%20limiting%20their%20clinical%0Aadaptability%20to%20individual%20patients.%20In%20this%20work%2C%20we%20propose%20a%20lightweight%2C%0Arapid%2C%20and%20robust%20optimization%20framework%20that%20estimates%20the%203D%20tumor%0Aconcentration%20by%20fitting%20it%20to%20MRI%20tumor%20segmentations%20while%20enforcing%20a%20smooth%0Aconcentration%20landscape.%20This%20approach%20achieves%20superior%20tumor%20recurrence%0Aprediction%20on%20192%20brain%20tumor%20patients%20across%20two%20public%20datasets%2C%0Aoutperforming%20state-of-the-art%20baselines%20while%20reducing%20runtime%20from%2030%20minutes%0Ato%20less%20than%20one%20minute.%20Furthermore%2C%20we%20demonstrate%20the%20framework%27s%0Aversatility%20and%20adaptability%20by%20showing%20its%20ability%20to%20seamlessly%20integrate%0Aadditional%20imaging%20modalities%20or%20physical%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.13811v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Optimization%2520Framework%2520for%2520Estimating%25203D%2520Brain%2520Tumor%250A%2520%2520Infiltration%26entry.906535625%3DJonas%2520Weidner%2520and%2520Michal%2520Balcerak%2520and%2520Ivan%2520Ezhov%2520and%2520Andr%25C3%25A9%2520Datchev%2520and%2520Laurin%2520Lux%2520and%2520Lucas%2520Zimmer%2520and%2520Daniel%2520Rueckert%2520and%2520Bj%25C3%25B6rn%2520Menze%2520and%2520Benedikt%2520Wiestler%26entry.1292438233%3D%2520%2520Glioblastoma%252C%2520the%2520most%2520aggressive%2520primary%2520brain%2520tumor%252C%2520poses%2520a%2520severe%250Aclinical%2520challenge%2520due%2520to%2520its%2520diffuse%2520microscopic%2520infiltration%252C%2520which%2520remains%250Alargely%2520undetected%2520on%2520standard%2520MRI.%2520As%2520a%2520result%252C%2520current%2520radiotherapy%2520planning%250Aemploys%2520a%2520uniform%252015%2520mm%2520margin%2520around%2520the%2520resection%2520cavity%252C%2520failing%2520to%2520capture%250Apatient-specific%2520tumor%2520spread.%2520Tumor%2520growth%2520modeling%2520offers%2520a%2520promising%250Aapproach%2520to%2520reveal%2520this%2520hidden%2520infiltration.%2520However%252C%2520methods%2520based%2520on%2520partial%250Adifferential%2520equations%2520or%2520physics-informed%2520neural%2520networks%2520tend%2520to%2520be%250Acomputationally%2520intensive%2520or%2520overly%2520constrained%252C%2520limiting%2520their%2520clinical%250Aadaptability%2520to%2520individual%2520patients.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520lightweight%252C%250Arapid%252C%2520and%2520robust%2520optimization%2520framework%2520that%2520estimates%2520the%25203D%2520tumor%250Aconcentration%2520by%2520fitting%2520it%2520to%2520MRI%2520tumor%2520segmentations%2520while%2520enforcing%2520a%2520smooth%250Aconcentration%2520landscape.%2520This%2520approach%2520achieves%2520superior%2520tumor%2520recurrence%250Aprediction%2520on%2520192%2520brain%2520tumor%2520patients%2520across%2520two%2520public%2520datasets%252C%250Aoutperforming%2520state-of-the-art%2520baselines%2520while%2520reducing%2520runtime%2520from%252030%2520minutes%250Ato%2520less%2520than%2520one%2520minute.%2520Furthermore%252C%2520we%2520demonstrate%2520the%2520framework%2527s%250Aversatility%2520and%2520adaptability%2520by%2520showing%2520its%2520ability%2520to%2520seamlessly%2520integrate%250Aadditional%2520imaging%2520modalities%2520or%2520physical%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.13811v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Optimization%20Framework%20for%20Estimating%203D%20Brain%20Tumor%0A%20%20Infiltration&entry.906535625=Jonas%20Weidner%20and%20Michal%20Balcerak%20and%20Ivan%20Ezhov%20and%20Andr%C3%A9%20Datchev%20and%20Laurin%20Lux%20and%20Lucas%20Zimmer%20and%20Daniel%20Rueckert%20and%20Bj%C3%B6rn%20Menze%20and%20Benedikt%20Wiestler&entry.1292438233=%20%20Glioblastoma%2C%20the%20most%20aggressive%20primary%20brain%20tumor%2C%20poses%20a%20severe%0Aclinical%20challenge%20due%20to%20its%20diffuse%20microscopic%20infiltration%2C%20which%20remains%0Alargely%20undetected%20on%20standard%20MRI.%20As%20a%20result%2C%20current%20radiotherapy%20planning%0Aemploys%20a%20uniform%2015%20mm%20margin%20around%20the%20resection%20cavity%2C%20failing%20to%20capture%0Apatient-specific%20tumor%20spread.%20Tumor%20growth%20modeling%20offers%20a%20promising%0Aapproach%20to%20reveal%20this%20hidden%20infiltration.%20However%2C%20methods%20based%20on%20partial%0Adifferential%20equations%20or%20physics-informed%20neural%20networks%20tend%20to%20be%0Acomputationally%20intensive%20or%20overly%20constrained%2C%20limiting%20their%20clinical%0Aadaptability%20to%20individual%20patients.%20In%20this%20work%2C%20we%20propose%20a%20lightweight%2C%0Arapid%2C%20and%20robust%20optimization%20framework%20that%20estimates%20the%203D%20tumor%0Aconcentration%20by%20fitting%20it%20to%20MRI%20tumor%20segmentations%20while%20enforcing%20a%20smooth%0Aconcentration%20landscape.%20This%20approach%20achieves%20superior%20tumor%20recurrence%0Aprediction%20on%20192%20brain%20tumor%20patients%20across%20two%20public%20datasets%2C%0Aoutperforming%20state-of-the-art%20baselines%20while%20reducing%20runtime%20from%2030%20minutes%0Ato%20less%20than%20one%20minute.%20Furthermore%2C%20we%20demonstrate%20the%20framework%27s%0Aversatility%20and%20adaptability%20by%20showing%20its%20ability%20to%20seamlessly%20integrate%0Aadditional%20imaging%20modalities%20or%20physical%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.13811v2&entry.124074799=Read"},
{"title": "Adjustable Spatio-Spectral Hyperspectral Image Compression Network", "author": "Martin Hermann Paul Fuchs and Behnood Rasti and Beg\u00fcm Demir", "abstract": "  With the rapid growth of hyperspectral data archives in remote sensing (RS),\nthe need for efficient storage has become essential, driving significant\nattention toward learning-based hyperspectral image (HSI) compression. However,\na comprehensive investigation of the individual and joint effects of spectral\nand spatial compression on learning-based HSI compression has not been\nthoroughly examined yet. Conducting such an analysis is crucial for\nunderstanding how the exploitation of spectral, spatial, and joint\nspatio-spectral redundancies affects HSI compression. To address this issue, we\npropose Adjustable Spatio-Spectral Hyperspectral Image Compression Network\n(HyCASS), a learning-based model designed for adjustable HSI compression in\nboth spectral and spatial dimensions. HyCASS consists of six main modules: 1)\nspectral encoder; 2) spatial encoder; 3) compression ratio (CR) adapter\nencoder; 4) CR adapter decoder; 5) spatial decoder; and 6) spectral decoder\nmodule. The modules employ convolutional layers and transformer blocks to\ncapture both short-range and long-range redundancies. Experimental results on\ntwo HSI benchmark datasets demonstrate the effectiveness of our proposed\nadjustable model compared to existing learning-based compression models. Based\non our results, we establish a guideline for effectively balancing spectral and\nspatial compression across different CRs, taking into account the spatial\nresolution of the HSIs. Our code and pre-trained model weights are publicly\navailable at https://git.tu-berlin.de/rsim/hycass .\n", "link": "http://arxiv.org/abs/2507.23447v1", "date": "2025-07-31", "relevancy": 2.0599, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5398}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5277}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adjustable%20Spatio-Spectral%20Hyperspectral%20Image%20Compression%20Network&body=Title%3A%20Adjustable%20Spatio-Spectral%20Hyperspectral%20Image%20Compression%20Network%0AAuthor%3A%20Martin%20Hermann%20Paul%20Fuchs%20and%20Behnood%20Rasti%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20With%20the%20rapid%20growth%20of%20hyperspectral%20data%20archives%20in%20remote%20sensing%20%28RS%29%2C%0Athe%20need%20for%20efficient%20storage%20has%20become%20essential%2C%20driving%20significant%0Aattention%20toward%20learning-based%20hyperspectral%20image%20%28HSI%29%20compression.%20However%2C%0Aa%20comprehensive%20investigation%20of%20the%20individual%20and%20joint%20effects%20of%20spectral%0Aand%20spatial%20compression%20on%20learning-based%20HSI%20compression%20has%20not%20been%0Athoroughly%20examined%20yet.%20Conducting%20such%20an%20analysis%20is%20crucial%20for%0Aunderstanding%20how%20the%20exploitation%20of%20spectral%2C%20spatial%2C%20and%20joint%0Aspatio-spectral%20redundancies%20affects%20HSI%20compression.%20To%20address%20this%20issue%2C%20we%0Apropose%20Adjustable%20Spatio-Spectral%20Hyperspectral%20Image%20Compression%20Network%0A%28HyCASS%29%2C%20a%20learning-based%20model%20designed%20for%20adjustable%20HSI%20compression%20in%0Aboth%20spectral%20and%20spatial%20dimensions.%20HyCASS%20consists%20of%20six%20main%20modules%3A%201%29%0Aspectral%20encoder%3B%202%29%20spatial%20encoder%3B%203%29%20compression%20ratio%20%28CR%29%20adapter%0Aencoder%3B%204%29%20CR%20adapter%20decoder%3B%205%29%20spatial%20decoder%3B%20and%206%29%20spectral%20decoder%0Amodule.%20The%20modules%20employ%20convolutional%20layers%20and%20transformer%20blocks%20to%0Acapture%20both%20short-range%20and%20long-range%20redundancies.%20Experimental%20results%20on%0Atwo%20HSI%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Aadjustable%20model%20compared%20to%20existing%20learning-based%20compression%20models.%20Based%0Aon%20our%20results%2C%20we%20establish%20a%20guideline%20for%20effectively%20balancing%20spectral%20and%0Aspatial%20compression%20across%20different%20CRs%2C%20taking%20into%20account%20the%20spatial%0Aresolution%20of%20the%20HSIs.%20Our%20code%20and%20pre-trained%20model%20weights%20are%20publicly%0Aavailable%20at%20https%3A//git.tu-berlin.de/rsim/hycass%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdjustable%2520Spatio-Spectral%2520Hyperspectral%2520Image%2520Compression%2520Network%26entry.906535625%3DMartin%2520Hermann%2520Paul%2520Fuchs%2520and%2520Behnood%2520Rasti%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520growth%2520of%2520hyperspectral%2520data%2520archives%2520in%2520remote%2520sensing%2520%2528RS%2529%252C%250Athe%2520need%2520for%2520efficient%2520storage%2520has%2520become%2520essential%252C%2520driving%2520significant%250Aattention%2520toward%2520learning-based%2520hyperspectral%2520image%2520%2528HSI%2529%2520compression.%2520However%252C%250Aa%2520comprehensive%2520investigation%2520of%2520the%2520individual%2520and%2520joint%2520effects%2520of%2520spectral%250Aand%2520spatial%2520compression%2520on%2520learning-based%2520HSI%2520compression%2520has%2520not%2520been%250Athoroughly%2520examined%2520yet.%2520Conducting%2520such%2520an%2520analysis%2520is%2520crucial%2520for%250Aunderstanding%2520how%2520the%2520exploitation%2520of%2520spectral%252C%2520spatial%252C%2520and%2520joint%250Aspatio-spectral%2520redundancies%2520affects%2520HSI%2520compression.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520Adjustable%2520Spatio-Spectral%2520Hyperspectral%2520Image%2520Compression%2520Network%250A%2528HyCASS%2529%252C%2520a%2520learning-based%2520model%2520designed%2520for%2520adjustable%2520HSI%2520compression%2520in%250Aboth%2520spectral%2520and%2520spatial%2520dimensions.%2520HyCASS%2520consists%2520of%2520six%2520main%2520modules%253A%25201%2529%250Aspectral%2520encoder%253B%25202%2529%2520spatial%2520encoder%253B%25203%2529%2520compression%2520ratio%2520%2528CR%2529%2520adapter%250Aencoder%253B%25204%2529%2520CR%2520adapter%2520decoder%253B%25205%2529%2520spatial%2520decoder%253B%2520and%25206%2529%2520spectral%2520decoder%250Amodule.%2520The%2520modules%2520employ%2520convolutional%2520layers%2520and%2520transformer%2520blocks%2520to%250Acapture%2520both%2520short-range%2520and%2520long-range%2520redundancies.%2520Experimental%2520results%2520on%250Atwo%2520HSI%2520benchmark%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%250Aadjustable%2520model%2520compared%2520to%2520existing%2520learning-based%2520compression%2520models.%2520Based%250Aon%2520our%2520results%252C%2520we%2520establish%2520a%2520guideline%2520for%2520effectively%2520balancing%2520spectral%2520and%250Aspatial%2520compression%2520across%2520different%2520CRs%252C%2520taking%2520into%2520account%2520the%2520spatial%250Aresolution%2520of%2520the%2520HSIs.%2520Our%2520code%2520and%2520pre-trained%2520model%2520weights%2520are%2520publicly%250Aavailable%2520at%2520https%253A//git.tu-berlin.de/rsim/hycass%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adjustable%20Spatio-Spectral%20Hyperspectral%20Image%20Compression%20Network&entry.906535625=Martin%20Hermann%20Paul%20Fuchs%20and%20Behnood%20Rasti%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20With%20the%20rapid%20growth%20of%20hyperspectral%20data%20archives%20in%20remote%20sensing%20%28RS%29%2C%0Athe%20need%20for%20efficient%20storage%20has%20become%20essential%2C%20driving%20significant%0Aattention%20toward%20learning-based%20hyperspectral%20image%20%28HSI%29%20compression.%20However%2C%0Aa%20comprehensive%20investigation%20of%20the%20individual%20and%20joint%20effects%20of%20spectral%0Aand%20spatial%20compression%20on%20learning-based%20HSI%20compression%20has%20not%20been%0Athoroughly%20examined%20yet.%20Conducting%20such%20an%20analysis%20is%20crucial%20for%0Aunderstanding%20how%20the%20exploitation%20of%20spectral%2C%20spatial%2C%20and%20joint%0Aspatio-spectral%20redundancies%20affects%20HSI%20compression.%20To%20address%20this%20issue%2C%20we%0Apropose%20Adjustable%20Spatio-Spectral%20Hyperspectral%20Image%20Compression%20Network%0A%28HyCASS%29%2C%20a%20learning-based%20model%20designed%20for%20adjustable%20HSI%20compression%20in%0Aboth%20spectral%20and%20spatial%20dimensions.%20HyCASS%20consists%20of%20six%20main%20modules%3A%201%29%0Aspectral%20encoder%3B%202%29%20spatial%20encoder%3B%203%29%20compression%20ratio%20%28CR%29%20adapter%0Aencoder%3B%204%29%20CR%20adapter%20decoder%3B%205%29%20spatial%20decoder%3B%20and%206%29%20spectral%20decoder%0Amodule.%20The%20modules%20employ%20convolutional%20layers%20and%20transformer%20blocks%20to%0Acapture%20both%20short-range%20and%20long-range%20redundancies.%20Experimental%20results%20on%0Atwo%20HSI%20benchmark%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20proposed%0Aadjustable%20model%20compared%20to%20existing%20learning-based%20compression%20models.%20Based%0Aon%20our%20results%2C%20we%20establish%20a%20guideline%20for%20effectively%20balancing%20spectral%20and%0Aspatial%20compression%20across%20different%20CRs%2C%20taking%20into%20account%20the%20spatial%0Aresolution%20of%20the%20HSIs.%20Our%20code%20and%20pre-trained%20model%20weights%20are%20publicly%0Aavailable%20at%20https%3A//git.tu-berlin.de/rsim/hycass%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23447v1&entry.124074799=Read"},
{"title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous\n  Driving Object Detection", "author": "Xiaojian Lin and Wenxin Zhang and Yuchu Jiang and Wangyu Wu and Yiran Guo and Kangxu Wang and Zongzheng Zhang and Guijin Wang and Lei Jin and Hao Zhao", "abstract": "  Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity.\n", "link": "http://arxiv.org/abs/2507.13373v2", "date": "2025-07-31", "relevancy": 2.0519, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5236}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5064}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Butter%3A%20Frequency%20Consistency%20and%20Hierarchical%20Fusion%20for%20Autonomous%0A%20%20Driving%20Object%20Detection&body=Title%3A%20Butter%3A%20Frequency%20Consistency%20and%20Hierarchical%20Fusion%20for%20Autonomous%0A%20%20Driving%20Object%20Detection%0AAuthor%3A%20Xiaojian%20Lin%20and%20Wenxin%20Zhang%20and%20Yuchu%20Jiang%20and%20Wangyu%20Wu%20and%20Yiran%20Guo%20and%20Kangxu%20Wang%20and%20Zongzheng%20Zhang%20and%20Guijin%20Wang%20and%20Lei%20Jin%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Hierarchical%20feature%20representations%20play%20a%20pivotal%20role%20in%20computer%20vision%2C%0Aparticularly%20in%20object%20detection%20for%20autonomous%20driving.%20Multi-level%20semantic%0Aunderstanding%20is%20crucial%20for%20accurately%20identifying%20pedestrians%2C%20vehicles%2C%20and%0Atraffic%20signs%20in%20dynamic%20environments.%20However%2C%20existing%20architectures%2C%20such%20as%0AYOLO%20and%20DETR%2C%20struggle%20to%20maintain%20feature%20consistency%20across%20different%20scales%0Awhile%20balancing%20detection%20precision%20and%20computational%20efficiency.%20To%20address%0Athese%20challenges%2C%20we%20propose%20Butter%2C%20a%20novel%20object%20detection%20framework%0Adesigned%20to%20enhance%20hierarchical%20feature%20representations%20for%20improving%0Adetection%20robustness.%20Specifically%2C%20Butter%20introduces%20two%20key%20innovations%3A%0AFrequency-Adaptive%20Feature%20Consistency%20Enhancement%20%28FAFCE%29%20Component%2C%20which%0Arefines%20multi-scale%20feature%20consistency%20by%20leveraging%20adaptive%20frequency%0Afiltering%20to%20enhance%20structural%20and%20boundary%20precision%2C%20and%20Progressive%0AHierarchical%20Feature%20Fusion%20Network%20%28PHFFNet%29%20Module%2C%20which%20progressively%0Aintegrates%20multi-level%20features%20to%20mitigate%20semantic%20gaps%20and%20strengthen%0Ahierarchical%20feature%20learning.%20Through%20extensive%20experiments%20on%20BDD100K%2C%20KITTI%2C%0Aand%20Cityscapes%2C%20Butter%20demonstrates%20superior%20feature%20representation%0Acapabilities%2C%20leading%20to%20notable%20improvements%20in%20detection%20accuracy%20while%0Areducing%20model%20complexity.%20By%20focusing%20on%20hierarchical%20feature%20refinement%20and%0Aintegration%2C%20Butter%20provides%20an%20advanced%20approach%20to%20object%20detection%20that%0Aachieves%20a%20balance%20between%20accuracy%2C%20deployability%2C%20and%20computational%0Aefficiency%20in%20real-time%20autonomous%20driving%20scenarios.%20Our%20model%20and%0Aimplementation%20are%20publicly%20available%20at%20https%3A//github.com/Aveiro-Lin/Butter%2C%0Afacilitating%20further%20research%20and%20validation%20within%20the%20autonomous%20driving%0Acommunity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.13373v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DButter%253A%2520Frequency%2520Consistency%2520and%2520Hierarchical%2520Fusion%2520for%2520Autonomous%250A%2520%2520Driving%2520Object%2520Detection%26entry.906535625%3DXiaojian%2520Lin%2520and%2520Wenxin%2520Zhang%2520and%2520Yuchu%2520Jiang%2520and%2520Wangyu%2520Wu%2520and%2520Yiran%2520Guo%2520and%2520Kangxu%2520Wang%2520and%2520Zongzheng%2520Zhang%2520and%2520Guijin%2520Wang%2520and%2520Lei%2520Jin%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Hierarchical%2520feature%2520representations%2520play%2520a%2520pivotal%2520role%2520in%2520computer%2520vision%252C%250Aparticularly%2520in%2520object%2520detection%2520for%2520autonomous%2520driving.%2520Multi-level%2520semantic%250Aunderstanding%2520is%2520crucial%2520for%2520accurately%2520identifying%2520pedestrians%252C%2520vehicles%252C%2520and%250Atraffic%2520signs%2520in%2520dynamic%2520environments.%2520However%252C%2520existing%2520architectures%252C%2520such%2520as%250AYOLO%2520and%2520DETR%252C%2520struggle%2520to%2520maintain%2520feature%2520consistency%2520across%2520different%2520scales%250Awhile%2520balancing%2520detection%2520precision%2520and%2520computational%2520efficiency.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520Butter%252C%2520a%2520novel%2520object%2520detection%2520framework%250Adesigned%2520to%2520enhance%2520hierarchical%2520feature%2520representations%2520for%2520improving%250Adetection%2520robustness.%2520Specifically%252C%2520Butter%2520introduces%2520two%2520key%2520innovations%253A%250AFrequency-Adaptive%2520Feature%2520Consistency%2520Enhancement%2520%2528FAFCE%2529%2520Component%252C%2520which%250Arefines%2520multi-scale%2520feature%2520consistency%2520by%2520leveraging%2520adaptive%2520frequency%250Afiltering%2520to%2520enhance%2520structural%2520and%2520boundary%2520precision%252C%2520and%2520Progressive%250AHierarchical%2520Feature%2520Fusion%2520Network%2520%2528PHFFNet%2529%2520Module%252C%2520which%2520progressively%250Aintegrates%2520multi-level%2520features%2520to%2520mitigate%2520semantic%2520gaps%2520and%2520strengthen%250Ahierarchical%2520feature%2520learning.%2520Through%2520extensive%2520experiments%2520on%2520BDD100K%252C%2520KITTI%252C%250Aand%2520Cityscapes%252C%2520Butter%2520demonstrates%2520superior%2520feature%2520representation%250Acapabilities%252C%2520leading%2520to%2520notable%2520improvements%2520in%2520detection%2520accuracy%2520while%250Areducing%2520model%2520complexity.%2520By%2520focusing%2520on%2520hierarchical%2520feature%2520refinement%2520and%250Aintegration%252C%2520Butter%2520provides%2520an%2520advanced%2520approach%2520to%2520object%2520detection%2520that%250Aachieves%2520a%2520balance%2520between%2520accuracy%252C%2520deployability%252C%2520and%2520computational%250Aefficiency%2520in%2520real-time%2520autonomous%2520driving%2520scenarios.%2520Our%2520model%2520and%250Aimplementation%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/Aveiro-Lin/Butter%252C%250Afacilitating%2520further%2520research%2520and%2520validation%2520within%2520the%2520autonomous%2520driving%250Acommunity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.13373v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Butter%3A%20Frequency%20Consistency%20and%20Hierarchical%20Fusion%20for%20Autonomous%0A%20%20Driving%20Object%20Detection&entry.906535625=Xiaojian%20Lin%20and%20Wenxin%20Zhang%20and%20Yuchu%20Jiang%20and%20Wangyu%20Wu%20and%20Yiran%20Guo%20and%20Kangxu%20Wang%20and%20Zongzheng%20Zhang%20and%20Guijin%20Wang%20and%20Lei%20Jin%20and%20Hao%20Zhao&entry.1292438233=%20%20Hierarchical%20feature%20representations%20play%20a%20pivotal%20role%20in%20computer%20vision%2C%0Aparticularly%20in%20object%20detection%20for%20autonomous%20driving.%20Multi-level%20semantic%0Aunderstanding%20is%20crucial%20for%20accurately%20identifying%20pedestrians%2C%20vehicles%2C%20and%0Atraffic%20signs%20in%20dynamic%20environments.%20However%2C%20existing%20architectures%2C%20such%20as%0AYOLO%20and%20DETR%2C%20struggle%20to%20maintain%20feature%20consistency%20across%20different%20scales%0Awhile%20balancing%20detection%20precision%20and%20computational%20efficiency.%20To%20address%0Athese%20challenges%2C%20we%20propose%20Butter%2C%20a%20novel%20object%20detection%20framework%0Adesigned%20to%20enhance%20hierarchical%20feature%20representations%20for%20improving%0Adetection%20robustness.%20Specifically%2C%20Butter%20introduces%20two%20key%20innovations%3A%0AFrequency-Adaptive%20Feature%20Consistency%20Enhancement%20%28FAFCE%29%20Component%2C%20which%0Arefines%20multi-scale%20feature%20consistency%20by%20leveraging%20adaptive%20frequency%0Afiltering%20to%20enhance%20structural%20and%20boundary%20precision%2C%20and%20Progressive%0AHierarchical%20Feature%20Fusion%20Network%20%28PHFFNet%29%20Module%2C%20which%20progressively%0Aintegrates%20multi-level%20features%20to%20mitigate%20semantic%20gaps%20and%20strengthen%0Ahierarchical%20feature%20learning.%20Through%20extensive%20experiments%20on%20BDD100K%2C%20KITTI%2C%0Aand%20Cityscapes%2C%20Butter%20demonstrates%20superior%20feature%20representation%0Acapabilities%2C%20leading%20to%20notable%20improvements%20in%20detection%20accuracy%20while%0Areducing%20model%20complexity.%20By%20focusing%20on%20hierarchical%20feature%20refinement%20and%0Aintegration%2C%20Butter%20provides%20an%20advanced%20approach%20to%20object%20detection%20that%0Aachieves%20a%20balance%20between%20accuracy%2C%20deployability%2C%20and%20computational%0Aefficiency%20in%20real-time%20autonomous%20driving%20scenarios.%20Our%20model%20and%0Aimplementation%20are%20publicly%20available%20at%20https%3A//github.com/Aveiro-Lin/Butter%2C%0Afacilitating%20further%20research%20and%20validation%20within%20the%20autonomous%20driving%0Acommunity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.13373v2&entry.124074799=Read"},
{"title": "Impact of a Lower Limb Exosuit Anchor Points on Energetics and\n  Biomechanics", "author": "Chiara Lambranzi and Giulia Oberti and Christian Di Natali and Darwin G. Caldwell and Manuela Galli and Elena De Momi and Jes\u00f9s Ortiz", "abstract": "  Anchor point placement is a crucial yet often overlooked aspect of exosuit\ndesign since it determines how forces interact with the human body. This work\nanalyzes the impact of different anchor point positions on gait kinematics,\nmuscular activation and energetic consumption. A total of six experiments were\nconducted with 11 subjects wearing the XoSoft exosuit, which assists hip\nflexion in five configurations. Subjects were instrumented with an IMU-based\nmotion tracking system, EMG sensors, and a mask to measure metabolic\nconsumption. The results show that positioning the knee anchor point on the\nposterior side while keeping the hip anchor on the anterior part can reduce\nmuscle activation in the hip flexors by up to 10.21\\% and metabolic expenditure\nby up to 18.45\\%. Even if the only assisted joint was the hip, all the\nconfigurations introduced changes also in the knee and ankle kinematics.\nOverall, no single configuration was optimal across all subjects, suggesting\nthat a personalized approach is necessary to transmit the assistance forces\noptimally. These findings emphasize that anchor point position does indeed have\na significant impact on exoskeleton effectiveness and efficiency. However,\nthese optimal positions are subject-specific to the exosuit design, and there\nis a strong need for future work to tailor musculoskeletal models to individual\ncharacteristics and validate these results in clinical populations.\n", "link": "http://arxiv.org/abs/2507.23579v1", "date": "2025-07-31", "relevancy": 1.6779, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4358}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4168}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20a%20Lower%20Limb%20Exosuit%20Anchor%20Points%20on%20Energetics%20and%0A%20%20Biomechanics&body=Title%3A%20Impact%20of%20a%20Lower%20Limb%20Exosuit%20Anchor%20Points%20on%20Energetics%20and%0A%20%20Biomechanics%0AAuthor%3A%20Chiara%20Lambranzi%20and%20Giulia%20Oberti%20and%20Christian%20Di%20Natali%20and%20Darwin%20G.%20Caldwell%20and%20Manuela%20Galli%20and%20Elena%20De%20Momi%20and%20Jes%C3%B9s%20Ortiz%0AAbstract%3A%20%20%20Anchor%20point%20placement%20is%20a%20crucial%20yet%20often%20overlooked%20aspect%20of%20exosuit%0Adesign%20since%20it%20determines%20how%20forces%20interact%20with%20the%20human%20body.%20This%20work%0Aanalyzes%20the%20impact%20of%20different%20anchor%20point%20positions%20on%20gait%20kinematics%2C%0Amuscular%20activation%20and%20energetic%20consumption.%20A%20total%20of%20six%20experiments%20were%0Aconducted%20with%2011%20subjects%20wearing%20the%20XoSoft%20exosuit%2C%20which%20assists%20hip%0Aflexion%20in%20five%20configurations.%20Subjects%20were%20instrumented%20with%20an%20IMU-based%0Amotion%20tracking%20system%2C%20EMG%20sensors%2C%20and%20a%20mask%20to%20measure%20metabolic%0Aconsumption.%20The%20results%20show%20that%20positioning%20the%20knee%20anchor%20point%20on%20the%0Aposterior%20side%20while%20keeping%20the%20hip%20anchor%20on%20the%20anterior%20part%20can%20reduce%0Amuscle%20activation%20in%20the%20hip%20flexors%20by%20up%20to%2010.21%5C%25%20and%20metabolic%20expenditure%0Aby%20up%20to%2018.45%5C%25.%20Even%20if%20the%20only%20assisted%20joint%20was%20the%20hip%2C%20all%20the%0Aconfigurations%20introduced%20changes%20also%20in%20the%20knee%20and%20ankle%20kinematics.%0AOverall%2C%20no%20single%20configuration%20was%20optimal%20across%20all%20subjects%2C%20suggesting%0Athat%20a%20personalized%20approach%20is%20necessary%20to%20transmit%20the%20assistance%20forces%0Aoptimally.%20These%20findings%20emphasize%20that%20anchor%20point%20position%20does%20indeed%20have%0Aa%20significant%20impact%20on%20exoskeleton%20effectiveness%20and%20efficiency.%20However%2C%0Athese%20optimal%20positions%20are%20subject-specific%20to%20the%20exosuit%20design%2C%20and%20there%0Ais%20a%20strong%20need%20for%20future%20work%20to%20tailor%20musculoskeletal%20models%20to%20individual%0Acharacteristics%20and%20validate%20these%20results%20in%20clinical%20populations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%2520a%2520Lower%2520Limb%2520Exosuit%2520Anchor%2520Points%2520on%2520Energetics%2520and%250A%2520%2520Biomechanics%26entry.906535625%3DChiara%2520Lambranzi%2520and%2520Giulia%2520Oberti%2520and%2520Christian%2520Di%2520Natali%2520and%2520Darwin%2520G.%2520Caldwell%2520and%2520Manuela%2520Galli%2520and%2520Elena%2520De%2520Momi%2520and%2520Jes%25C3%25B9s%2520Ortiz%26entry.1292438233%3D%2520%2520Anchor%2520point%2520placement%2520is%2520a%2520crucial%2520yet%2520often%2520overlooked%2520aspect%2520of%2520exosuit%250Adesign%2520since%2520it%2520determines%2520how%2520forces%2520interact%2520with%2520the%2520human%2520body.%2520This%2520work%250Aanalyzes%2520the%2520impact%2520of%2520different%2520anchor%2520point%2520positions%2520on%2520gait%2520kinematics%252C%250Amuscular%2520activation%2520and%2520energetic%2520consumption.%2520A%2520total%2520of%2520six%2520experiments%2520were%250Aconducted%2520with%252011%2520subjects%2520wearing%2520the%2520XoSoft%2520exosuit%252C%2520which%2520assists%2520hip%250Aflexion%2520in%2520five%2520configurations.%2520Subjects%2520were%2520instrumented%2520with%2520an%2520IMU-based%250Amotion%2520tracking%2520system%252C%2520EMG%2520sensors%252C%2520and%2520a%2520mask%2520to%2520measure%2520metabolic%250Aconsumption.%2520The%2520results%2520show%2520that%2520positioning%2520the%2520knee%2520anchor%2520point%2520on%2520the%250Aposterior%2520side%2520while%2520keeping%2520the%2520hip%2520anchor%2520on%2520the%2520anterior%2520part%2520can%2520reduce%250Amuscle%2520activation%2520in%2520the%2520hip%2520flexors%2520by%2520up%2520to%252010.21%255C%2525%2520and%2520metabolic%2520expenditure%250Aby%2520up%2520to%252018.45%255C%2525.%2520Even%2520if%2520the%2520only%2520assisted%2520joint%2520was%2520the%2520hip%252C%2520all%2520the%250Aconfigurations%2520introduced%2520changes%2520also%2520in%2520the%2520knee%2520and%2520ankle%2520kinematics.%250AOverall%252C%2520no%2520single%2520configuration%2520was%2520optimal%2520across%2520all%2520subjects%252C%2520suggesting%250Athat%2520a%2520personalized%2520approach%2520is%2520necessary%2520to%2520transmit%2520the%2520assistance%2520forces%250Aoptimally.%2520These%2520findings%2520emphasize%2520that%2520anchor%2520point%2520position%2520does%2520indeed%2520have%250Aa%2520significant%2520impact%2520on%2520exoskeleton%2520effectiveness%2520and%2520efficiency.%2520However%252C%250Athese%2520optimal%2520positions%2520are%2520subject-specific%2520to%2520the%2520exosuit%2520design%252C%2520and%2520there%250Ais%2520a%2520strong%2520need%2520for%2520future%2520work%2520to%2520tailor%2520musculoskeletal%2520models%2520to%2520individual%250Acharacteristics%2520and%2520validate%2520these%2520results%2520in%2520clinical%2520populations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20a%20Lower%20Limb%20Exosuit%20Anchor%20Points%20on%20Energetics%20and%0A%20%20Biomechanics&entry.906535625=Chiara%20Lambranzi%20and%20Giulia%20Oberti%20and%20Christian%20Di%20Natali%20and%20Darwin%20G.%20Caldwell%20and%20Manuela%20Galli%20and%20Elena%20De%20Momi%20and%20Jes%C3%B9s%20Ortiz&entry.1292438233=%20%20Anchor%20point%20placement%20is%20a%20crucial%20yet%20often%20overlooked%20aspect%20of%20exosuit%0Adesign%20since%20it%20determines%20how%20forces%20interact%20with%20the%20human%20body.%20This%20work%0Aanalyzes%20the%20impact%20of%20different%20anchor%20point%20positions%20on%20gait%20kinematics%2C%0Amuscular%20activation%20and%20energetic%20consumption.%20A%20total%20of%20six%20experiments%20were%0Aconducted%20with%2011%20subjects%20wearing%20the%20XoSoft%20exosuit%2C%20which%20assists%20hip%0Aflexion%20in%20five%20configurations.%20Subjects%20were%20instrumented%20with%20an%20IMU-based%0Amotion%20tracking%20system%2C%20EMG%20sensors%2C%20and%20a%20mask%20to%20measure%20metabolic%0Aconsumption.%20The%20results%20show%20that%20positioning%20the%20knee%20anchor%20point%20on%20the%0Aposterior%20side%20while%20keeping%20the%20hip%20anchor%20on%20the%20anterior%20part%20can%20reduce%0Amuscle%20activation%20in%20the%20hip%20flexors%20by%20up%20to%2010.21%5C%25%20and%20metabolic%20expenditure%0Aby%20up%20to%2018.45%5C%25.%20Even%20if%20the%20only%20assisted%20joint%20was%20the%20hip%2C%20all%20the%0Aconfigurations%20introduced%20changes%20also%20in%20the%20knee%20and%20ankle%20kinematics.%0AOverall%2C%20no%20single%20configuration%20was%20optimal%20across%20all%20subjects%2C%20suggesting%0Athat%20a%20personalized%20approach%20is%20necessary%20to%20transmit%20the%20assistance%20forces%0Aoptimally.%20These%20findings%20emphasize%20that%20anchor%20point%20position%20does%20indeed%20have%0Aa%20significant%20impact%20on%20exoskeleton%20effectiveness%20and%20efficiency.%20However%2C%0Athese%20optimal%20positions%20are%20subject-specific%20to%20the%20exosuit%20design%2C%20and%20there%0Ais%20a%20strong%20need%20for%20future%20work%20to%20tailor%20musculoskeletal%20models%20to%20individual%0Acharacteristics%20and%20validate%20these%20results%20in%20clinical%20populations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23579v1&entry.124074799=Read"},
{"title": "Molecule Graph Networks with Many-body Equivariant Interactions", "author": "Zetian Mao and Chuan-Shen Hu and Jiawen Li and Chen Liang and Diptesh Das and Masato Sumita and Kelin Xia and Koji Tsuda", "abstract": "  Message passing neural networks have demonstrated significant efficacy in\npredicting molecular interactions. Introducing equivariant vectorial\nrepresentations augments expressivity by capturing geometric data symmetries,\nthereby improving model accuracy. However, two-body bond vectors in opposition\nmay cancel each other out during message passing, leading to the loss of\ndirectional information on their shared node. In this study, we develop\nEquivariant N-body Interaction Networks (ENINet) that explicitly integrates l =\n1 equivariant many-body interactions to enhance directional symmetric\ninformation in the message passing scheme. We provided a mathematical analysis\ndemonstrating the necessity of incorporating many-body equivariant interactions\nand generalized the formulation to $N$-body interactions. Experiments indicate\nthat integrating many-body equivariant representations enhances prediction\naccuracy across diverse scalar and tensorial quantum chemical properties.\n", "link": "http://arxiv.org/abs/2406.13265v3", "date": "2025-07-31", "relevancy": 1.8487, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4748}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4691}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Molecule%20Graph%20Networks%20with%20Many-body%20Equivariant%20Interactions&body=Title%3A%20Molecule%20Graph%20Networks%20with%20Many-body%20Equivariant%20Interactions%0AAuthor%3A%20Zetian%20Mao%20and%20Chuan-Shen%20Hu%20and%20Jiawen%20Li%20and%20Chen%20Liang%20and%20Diptesh%20Das%20and%20Masato%20Sumita%20and%20Kelin%20Xia%20and%20Koji%20Tsuda%0AAbstract%3A%20%20%20Message%20passing%20neural%20networks%20have%20demonstrated%20significant%20efficacy%20in%0Apredicting%20molecular%20interactions.%20Introducing%20equivariant%20vectorial%0Arepresentations%20augments%20expressivity%20by%20capturing%20geometric%20data%20symmetries%2C%0Athereby%20improving%20model%20accuracy.%20However%2C%20two-body%20bond%20vectors%20in%20opposition%0Amay%20cancel%20each%20other%20out%20during%20message%20passing%2C%20leading%20to%20the%20loss%20of%0Adirectional%20information%20on%20their%20shared%20node.%20In%20this%20study%2C%20we%20develop%0AEquivariant%20N-body%20Interaction%20Networks%20%28ENINet%29%20that%20explicitly%20integrates%20l%20%3D%0A1%20equivariant%20many-body%20interactions%20to%20enhance%20directional%20symmetric%0Ainformation%20in%20the%20message%20passing%20scheme.%20We%20provided%20a%20mathematical%20analysis%0Ademonstrating%20the%20necessity%20of%20incorporating%20many-body%20equivariant%20interactions%0Aand%20generalized%20the%20formulation%20to%20%24N%24-body%20interactions.%20Experiments%20indicate%0Athat%20integrating%20many-body%20equivariant%20representations%20enhances%20prediction%0Aaccuracy%20across%20diverse%20scalar%20and%20tensorial%20quantum%20chemical%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13265v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolecule%2520Graph%2520Networks%2520with%2520Many-body%2520Equivariant%2520Interactions%26entry.906535625%3DZetian%2520Mao%2520and%2520Chuan-Shen%2520Hu%2520and%2520Jiawen%2520Li%2520and%2520Chen%2520Liang%2520and%2520Diptesh%2520Das%2520and%2520Masato%2520Sumita%2520and%2520Kelin%2520Xia%2520and%2520Koji%2520Tsuda%26entry.1292438233%3D%2520%2520Message%2520passing%2520neural%2520networks%2520have%2520demonstrated%2520significant%2520efficacy%2520in%250Apredicting%2520molecular%2520interactions.%2520Introducing%2520equivariant%2520vectorial%250Arepresentations%2520augments%2520expressivity%2520by%2520capturing%2520geometric%2520data%2520symmetries%252C%250Athereby%2520improving%2520model%2520accuracy.%2520However%252C%2520two-body%2520bond%2520vectors%2520in%2520opposition%250Amay%2520cancel%2520each%2520other%2520out%2520during%2520message%2520passing%252C%2520leading%2520to%2520the%2520loss%2520of%250Adirectional%2520information%2520on%2520their%2520shared%2520node.%2520In%2520this%2520study%252C%2520we%2520develop%250AEquivariant%2520N-body%2520Interaction%2520Networks%2520%2528ENINet%2529%2520that%2520explicitly%2520integrates%2520l%2520%253D%250A1%2520equivariant%2520many-body%2520interactions%2520to%2520enhance%2520directional%2520symmetric%250Ainformation%2520in%2520the%2520message%2520passing%2520scheme.%2520We%2520provided%2520a%2520mathematical%2520analysis%250Ademonstrating%2520the%2520necessity%2520of%2520incorporating%2520many-body%2520equivariant%2520interactions%250Aand%2520generalized%2520the%2520formulation%2520to%2520%2524N%2524-body%2520interactions.%2520Experiments%2520indicate%250Athat%2520integrating%2520many-body%2520equivariant%2520representations%2520enhances%2520prediction%250Aaccuracy%2520across%2520diverse%2520scalar%2520and%2520tensorial%2520quantum%2520chemical%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13265v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Molecule%20Graph%20Networks%20with%20Many-body%20Equivariant%20Interactions&entry.906535625=Zetian%20Mao%20and%20Chuan-Shen%20Hu%20and%20Jiawen%20Li%20and%20Chen%20Liang%20and%20Diptesh%20Das%20and%20Masato%20Sumita%20and%20Kelin%20Xia%20and%20Koji%20Tsuda&entry.1292438233=%20%20Message%20passing%20neural%20networks%20have%20demonstrated%20significant%20efficacy%20in%0Apredicting%20molecular%20interactions.%20Introducing%20equivariant%20vectorial%0Arepresentations%20augments%20expressivity%20by%20capturing%20geometric%20data%20symmetries%2C%0Athereby%20improving%20model%20accuracy.%20However%2C%20two-body%20bond%20vectors%20in%20opposition%0Amay%20cancel%20each%20other%20out%20during%20message%20passing%2C%20leading%20to%20the%20loss%20of%0Adirectional%20information%20on%20their%20shared%20node.%20In%20this%20study%2C%20we%20develop%0AEquivariant%20N-body%20Interaction%20Networks%20%28ENINet%29%20that%20explicitly%20integrates%20l%20%3D%0A1%20equivariant%20many-body%20interactions%20to%20enhance%20directional%20symmetric%0Ainformation%20in%20the%20message%20passing%20scheme.%20We%20provided%20a%20mathematical%20analysis%0Ademonstrating%20the%20necessity%20of%20incorporating%20many-body%20equivariant%20interactions%0Aand%20generalized%20the%20formulation%20to%20%24N%24-body%20interactions.%20Experiments%20indicate%0Athat%20integrating%20many-body%20equivariant%20representations%20enhances%20prediction%0Aaccuracy%20across%20diverse%20scalar%20and%20tensorial%20quantum%20chemical%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13265v3&entry.124074799=Read"},
{"title": "RAVine: Reality-Aligned Evaluation for Agentic Search", "author": "Yilong Xu and Xiang Long and Zhi Zheng and Jinhua Gao", "abstract": "  Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.\n", "link": "http://arxiv.org/abs/2507.16725v2", "date": "2025-07-31", "relevancy": 2.0162, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.507}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4893}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAVine%3A%20Reality-Aligned%20Evaluation%20for%20Agentic%20Search&body=Title%3A%20RAVine%3A%20Reality-Aligned%20Evaluation%20for%20Agentic%20Search%0AAuthor%3A%20Yilong%20Xu%20and%20Xiang%20Long%20and%20Zhi%20Zheng%20and%20Jinhua%20Gao%0AAbstract%3A%20%20%20Agentic%20search%2C%20as%20a%20more%20autonomous%20and%20adaptive%20paradigm%20of%20retrieval%0Aaugmentation%2C%20is%20driving%20the%20evolution%20of%20intelligent%20search%20systems.%20However%2C%0Aexisting%20evaluation%20frameworks%20fail%20to%20align%20well%20with%20the%20goals%20of%20agentic%0Asearch.%20First%2C%20the%20complex%20queries%20commonly%20used%20in%20current%20benchmarks%20often%0Adeviate%20from%20realistic%20user%20search%20scenarios.%20Second%2C%20prior%20approaches%20tend%20to%0Aintroduce%20noise%20when%20extracting%20ground%20truth%20for%20end-to-end%20evaluations%2C%0Aleading%20to%20distorted%20assessments%20at%20a%20fine-grained%20level.%20Third%2C%20most%20current%0Aframeworks%20focus%20solely%20on%20the%20quality%20of%20final%20answers%2C%20neglecting%20the%0Aevaluation%20of%20the%20iterative%20process%20inherent%20to%20agentic%20search.%20To%20address%0Athese%20limitations%2C%20we%20propose%20RAVine%20--%20a%20Reality-Aligned%20eValuation%20framework%0Afor%20agentic%20LLMs%20with%20search.%20RAVine%20targets%20multi-point%20queries%20and%20long-form%0Aanswers%20that%20better%20reflect%20user%20intents%2C%20and%20introduces%20an%20attributable%20ground%0Atruth%20construction%20strategy%20to%20enhance%20the%20accuracy%20of%20fine-grained%20evaluation.%0AMoreover%2C%20RAVine%20examines%20model%27s%20interaction%20with%20search%20tools%20throughout%20the%0Aiterative%20process%2C%20and%20accounts%20for%20factors%20of%20efficiency.%20We%20benchmark%20a%0Aseries%20of%20models%20using%20RAVine%20and%20derive%20several%20insights%2C%20which%20we%20hope%20will%0Acontribute%20to%20advancing%20the%20development%20of%20agentic%20search%20systems.%20The%20code%20and%0Adatasets%20are%20available%20at%20https%3A//github.com/SwordFaith/RAVine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.16725v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAVine%253A%2520Reality-Aligned%2520Evaluation%2520for%2520Agentic%2520Search%26entry.906535625%3DYilong%2520Xu%2520and%2520Xiang%2520Long%2520and%2520Zhi%2520Zheng%2520and%2520Jinhua%2520Gao%26entry.1292438233%3D%2520%2520Agentic%2520search%252C%2520as%2520a%2520more%2520autonomous%2520and%2520adaptive%2520paradigm%2520of%2520retrieval%250Aaugmentation%252C%2520is%2520driving%2520the%2520evolution%2520of%2520intelligent%2520search%2520systems.%2520However%252C%250Aexisting%2520evaluation%2520frameworks%2520fail%2520to%2520align%2520well%2520with%2520the%2520goals%2520of%2520agentic%250Asearch.%2520First%252C%2520the%2520complex%2520queries%2520commonly%2520used%2520in%2520current%2520benchmarks%2520often%250Adeviate%2520from%2520realistic%2520user%2520search%2520scenarios.%2520Second%252C%2520prior%2520approaches%2520tend%2520to%250Aintroduce%2520noise%2520when%2520extracting%2520ground%2520truth%2520for%2520end-to-end%2520evaluations%252C%250Aleading%2520to%2520distorted%2520assessments%2520at%2520a%2520fine-grained%2520level.%2520Third%252C%2520most%2520current%250Aframeworks%2520focus%2520solely%2520on%2520the%2520quality%2520of%2520final%2520answers%252C%2520neglecting%2520the%250Aevaluation%2520of%2520the%2520iterative%2520process%2520inherent%2520to%2520agentic%2520search.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520propose%2520RAVine%2520--%2520a%2520Reality-Aligned%2520eValuation%2520framework%250Afor%2520agentic%2520LLMs%2520with%2520search.%2520RAVine%2520targets%2520multi-point%2520queries%2520and%2520long-form%250Aanswers%2520that%2520better%2520reflect%2520user%2520intents%252C%2520and%2520introduces%2520an%2520attributable%2520ground%250Atruth%2520construction%2520strategy%2520to%2520enhance%2520the%2520accuracy%2520of%2520fine-grained%2520evaluation.%250AMoreover%252C%2520RAVine%2520examines%2520model%2527s%2520interaction%2520with%2520search%2520tools%2520throughout%2520the%250Aiterative%2520process%252C%2520and%2520accounts%2520for%2520factors%2520of%2520efficiency.%2520We%2520benchmark%2520a%250Aseries%2520of%2520models%2520using%2520RAVine%2520and%2520derive%2520several%2520insights%252C%2520which%2520we%2520hope%2520will%250Acontribute%2520to%2520advancing%2520the%2520development%2520of%2520agentic%2520search%2520systems.%2520The%2520code%2520and%250Adatasets%2520are%2520available%2520at%2520https%253A//github.com/SwordFaith/RAVine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.16725v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAVine%3A%20Reality-Aligned%20Evaluation%20for%20Agentic%20Search&entry.906535625=Yilong%20Xu%20and%20Xiang%20Long%20and%20Zhi%20Zheng%20and%20Jinhua%20Gao&entry.1292438233=%20%20Agentic%20search%2C%20as%20a%20more%20autonomous%20and%20adaptive%20paradigm%20of%20retrieval%0Aaugmentation%2C%20is%20driving%20the%20evolution%20of%20intelligent%20search%20systems.%20However%2C%0Aexisting%20evaluation%20frameworks%20fail%20to%20align%20well%20with%20the%20goals%20of%20agentic%0Asearch.%20First%2C%20the%20complex%20queries%20commonly%20used%20in%20current%20benchmarks%20often%0Adeviate%20from%20realistic%20user%20search%20scenarios.%20Second%2C%20prior%20approaches%20tend%20to%0Aintroduce%20noise%20when%20extracting%20ground%20truth%20for%20end-to-end%20evaluations%2C%0Aleading%20to%20distorted%20assessments%20at%20a%20fine-grained%20level.%20Third%2C%20most%20current%0Aframeworks%20focus%20solely%20on%20the%20quality%20of%20final%20answers%2C%20neglecting%20the%0Aevaluation%20of%20the%20iterative%20process%20inherent%20to%20agentic%20search.%20To%20address%0Athese%20limitations%2C%20we%20propose%20RAVine%20--%20a%20Reality-Aligned%20eValuation%20framework%0Afor%20agentic%20LLMs%20with%20search.%20RAVine%20targets%20multi-point%20queries%20and%20long-form%0Aanswers%20that%20better%20reflect%20user%20intents%2C%20and%20introduces%20an%20attributable%20ground%0Atruth%20construction%20strategy%20to%20enhance%20the%20accuracy%20of%20fine-grained%20evaluation.%0AMoreover%2C%20RAVine%20examines%20model%27s%20interaction%20with%20search%20tools%20throughout%20the%0Aiterative%20process%2C%20and%20accounts%20for%20factors%20of%20efficiency.%20We%20benchmark%20a%0Aseries%20of%20models%20using%20RAVine%20and%20derive%20several%20insights%2C%20which%20we%20hope%20will%0Acontribute%20to%20advancing%20the%20development%20of%20agentic%20search%20systems.%20The%20code%20and%0Adatasets%20are%20available%20at%20https%3A//github.com/SwordFaith/RAVine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.16725v2&entry.124074799=Read"},
{"title": "Manifold-regularised Signature Kernel Large-Margin $\\ell_p$-SVDD for\n  Multidimensional Time Series Anomaly Detection", "author": "Shervin Rahimzadeh Arashloo", "abstract": "  We generalise the recently introduced large-margin $\\ell_p$-SVDD approach to\nexploit the geometry of data distribution via manifold regularising and a\nsignature kernel representation for time series anomaly detection.\nSpecifically, we formulate a manifold-regularised variant of the $\\ell_p$-SVDD\nmethod to encourage label smoothness on the underlying manifold to capture\nstructural information for improved detection performance. Drawing on an\nexisting Representer theorem, we then provide an effective optimisation\ntechnique for the proposed method and show that it can benefit from the\nsignature kernel to capture time series complexities for anomaly detection.\n  We theoretically study the proposed approach using Rademacher complexities to\nanalyse its generalisation performance and also provide an experimental\nassessment of the proposed method across various data sets to compare its\nperformance against other methods.\n", "link": "http://arxiv.org/abs/2507.23449v1", "date": "2025-07-31", "relevancy": 1.4304, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4965}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4705}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manifold-regularised%20Signature%20Kernel%20Large-Margin%20%24%5Cell_p%24-SVDD%20for%0A%20%20Multidimensional%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20Manifold-regularised%20Signature%20Kernel%20Large-Margin%20%24%5Cell_p%24-SVDD%20for%0A%20%20Multidimensional%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Shervin%20Rahimzadeh%20Arashloo%0AAbstract%3A%20%20%20We%20generalise%20the%20recently%20introduced%20large-margin%20%24%5Cell_p%24-SVDD%20approach%20to%0Aexploit%20the%20geometry%20of%20data%20distribution%20via%20manifold%20regularising%20and%20a%0Asignature%20kernel%20representation%20for%20time%20series%20anomaly%20detection.%0ASpecifically%2C%20we%20formulate%20a%20manifold-regularised%20variant%20of%20the%20%24%5Cell_p%24-SVDD%0Amethod%20to%20encourage%20label%20smoothness%20on%20the%20underlying%20manifold%20to%20capture%0Astructural%20information%20for%20improved%20detection%20performance.%20Drawing%20on%20an%0Aexisting%20Representer%20theorem%2C%20we%20then%20provide%20an%20effective%20optimisation%0Atechnique%20for%20the%20proposed%20method%20and%20show%20that%20it%20can%20benefit%20from%20the%0Asignature%20kernel%20to%20capture%20time%20series%20complexities%20for%20anomaly%20detection.%0A%20%20We%20theoretically%20study%20the%20proposed%20approach%20using%20Rademacher%20complexities%20to%0Aanalyse%20its%20generalisation%20performance%20and%20also%20provide%20an%20experimental%0Aassessment%20of%20the%20proposed%20method%20across%20various%20data%20sets%20to%20compare%20its%0Aperformance%20against%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManifold-regularised%2520Signature%2520Kernel%2520Large-Margin%2520%2524%255Cell_p%2524-SVDD%2520for%250A%2520%2520Multidimensional%2520Time%2520Series%2520Anomaly%2520Detection%26entry.906535625%3DShervin%2520Rahimzadeh%2520Arashloo%26entry.1292438233%3D%2520%2520We%2520generalise%2520the%2520recently%2520introduced%2520large-margin%2520%2524%255Cell_p%2524-SVDD%2520approach%2520to%250Aexploit%2520the%2520geometry%2520of%2520data%2520distribution%2520via%2520manifold%2520regularising%2520and%2520a%250Asignature%2520kernel%2520representation%2520for%2520time%2520series%2520anomaly%2520detection.%250ASpecifically%252C%2520we%2520formulate%2520a%2520manifold-regularised%2520variant%2520of%2520the%2520%2524%255Cell_p%2524-SVDD%250Amethod%2520to%2520encourage%2520label%2520smoothness%2520on%2520the%2520underlying%2520manifold%2520to%2520capture%250Astructural%2520information%2520for%2520improved%2520detection%2520performance.%2520Drawing%2520on%2520an%250Aexisting%2520Representer%2520theorem%252C%2520we%2520then%2520provide%2520an%2520effective%2520optimisation%250Atechnique%2520for%2520the%2520proposed%2520method%2520and%2520show%2520that%2520it%2520can%2520benefit%2520from%2520the%250Asignature%2520kernel%2520to%2520capture%2520time%2520series%2520complexities%2520for%2520anomaly%2520detection.%250A%2520%2520We%2520theoretically%2520study%2520the%2520proposed%2520approach%2520using%2520Rademacher%2520complexities%2520to%250Aanalyse%2520its%2520generalisation%2520performance%2520and%2520also%2520provide%2520an%2520experimental%250Aassessment%2520of%2520the%2520proposed%2520method%2520across%2520various%2520data%2520sets%2520to%2520compare%2520its%250Aperformance%2520against%2520other%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manifold-regularised%20Signature%20Kernel%20Large-Margin%20%24%5Cell_p%24-SVDD%20for%0A%20%20Multidimensional%20Time%20Series%20Anomaly%20Detection&entry.906535625=Shervin%20Rahimzadeh%20Arashloo&entry.1292438233=%20%20We%20generalise%20the%20recently%20introduced%20large-margin%20%24%5Cell_p%24-SVDD%20approach%20to%0Aexploit%20the%20geometry%20of%20data%20distribution%20via%20manifold%20regularising%20and%20a%0Asignature%20kernel%20representation%20for%20time%20series%20anomaly%20detection.%0ASpecifically%2C%20we%20formulate%20a%20manifold-regularised%20variant%20of%20the%20%24%5Cell_p%24-SVDD%0Amethod%20to%20encourage%20label%20smoothness%20on%20the%20underlying%20manifold%20to%20capture%0Astructural%20information%20for%20improved%20detection%20performance.%20Drawing%20on%20an%0Aexisting%20Representer%20theorem%2C%20we%20then%20provide%20an%20effective%20optimisation%0Atechnique%20for%20the%20proposed%20method%20and%20show%20that%20it%20can%20benefit%20from%20the%0Asignature%20kernel%20to%20capture%20time%20series%20complexities%20for%20anomaly%20detection.%0A%20%20We%20theoretically%20study%20the%20proposed%20approach%20using%20Rademacher%20complexities%20to%0Aanalyse%20its%20generalisation%20performance%20and%20also%20provide%20an%20experimental%0Aassessment%20of%20the%20proposed%20method%20across%20various%20data%20sets%20to%20compare%20its%0Aperformance%20against%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23449v1&entry.124074799=Read"},
{"title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human\n  Ideas: Evidence From a Large, Dynamic Experiment", "author": "Joshua Ashkinaze and Julia Mendelsohn and Li Qiwei and Ceren Budak and Eric Gilbert", "abstract": "  Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity.\n", "link": "http://arxiv.org/abs/2401.13481v3", "date": "2025-07-31", "relevancy": 1.8727, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4715}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4712}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20AI%20Ideas%20Affect%20the%20Creativity%2C%20Diversity%2C%20and%20Evolution%20of%20Human%0A%20%20Ideas%3A%20Evidence%20From%20a%20Large%2C%20Dynamic%20Experiment&body=Title%3A%20How%20AI%20Ideas%20Affect%20the%20Creativity%2C%20Diversity%2C%20and%20Evolution%20of%20Human%0A%20%20Ideas%3A%20Evidence%20From%20a%20Large%2C%20Dynamic%20Experiment%0AAuthor%3A%20Joshua%20Ashkinaze%20and%20Julia%20Mendelsohn%20and%20Li%20Qiwei%20and%20Ceren%20Budak%20and%20Eric%20Gilbert%0AAbstract%3A%20%20%20Exposure%20to%20large%20language%20model%20output%20is%20rapidly%20increasing.%20How%20will%0Aseeing%20AI-generated%20ideas%20affect%20human%20ideas%3F%20We%20conducted%20an%20experiment%20%28800%2B%0Aparticipants%2C%2040%2B%20countries%29%20where%20participants%20viewed%20creative%20ideas%20that%20were%0Afrom%20ChatGPT%20or%20prior%20experimental%20participants%20and%20then%20brainstormed%20their%20own%0Aidea.%20We%20varied%20the%20number%20of%20AI-generated%20examples%20%28none%2C%20low%2C%20or%20high%0Aexposure%29%20and%20if%20the%20examples%20were%20labeled%20as%20%27AI%27%20%28disclosure%29.%20Our%20dynamic%0Aexperiment%20design%20--%20ideas%20from%20prior%20participants%20in%20an%20experimental%20condition%0Aare%20used%20as%20stimuli%20for%20future%20participants%20in%20the%20same%20experimental%20condition%0A--%20speaks%20to%20the%20interdependent%20process%20of%20cultural%20creation%3A%20creative%20ideas%0Aare%20built%20upon%20prior%20ideas.%20Hence%2C%20we%20capture%20the%20compounding%20effects%20of%20having%0ALLMs%20%27in%20the%20culture%20loop%27.%20We%20find%20that%20high%20AI%20exposure%20%28but%20not%20low%20AI%0Aexposure%29%20did%20not%20affect%20the%20creativity%20of%20individual%20ideas%20but%20did%20increase%0Athe%20average%20amount%20and%20rate%20of%20change%20of%20collective%20idea%20diversity.%20AI%20made%0Aideas%20different%2C%20not%20better.%20There%20were%20no%20main%20effects%20of%20disclosure.%20We%20also%0Afound%20that%20self-reported%20creative%20people%20were%20less%20influenced%20by%20knowing%20an%0Aidea%20was%20from%20AI%20and%20that%20participants%20may%20knowingly%20adopt%20AI%20ideas%20when%20the%0Atask%20is%20difficult.%20Our%20findings%20suggest%20that%20introducing%20AI%20ideas%20may%20increase%0Acollective%20diversity%20but%20not%20individual%20creativity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.13481v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520AI%2520Ideas%2520Affect%2520the%2520Creativity%252C%2520Diversity%252C%2520and%2520Evolution%2520of%2520Human%250A%2520%2520Ideas%253A%2520Evidence%2520From%2520a%2520Large%252C%2520Dynamic%2520Experiment%26entry.906535625%3DJoshua%2520Ashkinaze%2520and%2520Julia%2520Mendelsohn%2520and%2520Li%2520Qiwei%2520and%2520Ceren%2520Budak%2520and%2520Eric%2520Gilbert%26entry.1292438233%3D%2520%2520Exposure%2520to%2520large%2520language%2520model%2520output%2520is%2520rapidly%2520increasing.%2520How%2520will%250Aseeing%2520AI-generated%2520ideas%2520affect%2520human%2520ideas%253F%2520We%2520conducted%2520an%2520experiment%2520%2528800%252B%250Aparticipants%252C%252040%252B%2520countries%2529%2520where%2520participants%2520viewed%2520creative%2520ideas%2520that%2520were%250Afrom%2520ChatGPT%2520or%2520prior%2520experimental%2520participants%2520and%2520then%2520brainstormed%2520their%2520own%250Aidea.%2520We%2520varied%2520the%2520number%2520of%2520AI-generated%2520examples%2520%2528none%252C%2520low%252C%2520or%2520high%250Aexposure%2529%2520and%2520if%2520the%2520examples%2520were%2520labeled%2520as%2520%2527AI%2527%2520%2528disclosure%2529.%2520Our%2520dynamic%250Aexperiment%2520design%2520--%2520ideas%2520from%2520prior%2520participants%2520in%2520an%2520experimental%2520condition%250Aare%2520used%2520as%2520stimuli%2520for%2520future%2520participants%2520in%2520the%2520same%2520experimental%2520condition%250A--%2520speaks%2520to%2520the%2520interdependent%2520process%2520of%2520cultural%2520creation%253A%2520creative%2520ideas%250Aare%2520built%2520upon%2520prior%2520ideas.%2520Hence%252C%2520we%2520capture%2520the%2520compounding%2520effects%2520of%2520having%250ALLMs%2520%2527in%2520the%2520culture%2520loop%2527.%2520We%2520find%2520that%2520high%2520AI%2520exposure%2520%2528but%2520not%2520low%2520AI%250Aexposure%2529%2520did%2520not%2520affect%2520the%2520creativity%2520of%2520individual%2520ideas%2520but%2520did%2520increase%250Athe%2520average%2520amount%2520and%2520rate%2520of%2520change%2520of%2520collective%2520idea%2520diversity.%2520AI%2520made%250Aideas%2520different%252C%2520not%2520better.%2520There%2520were%2520no%2520main%2520effects%2520of%2520disclosure.%2520We%2520also%250Afound%2520that%2520self-reported%2520creative%2520people%2520were%2520less%2520influenced%2520by%2520knowing%2520an%250Aidea%2520was%2520from%2520AI%2520and%2520that%2520participants%2520may%2520knowingly%2520adopt%2520AI%2520ideas%2520when%2520the%250Atask%2520is%2520difficult.%2520Our%2520findings%2520suggest%2520that%2520introducing%2520AI%2520ideas%2520may%2520increase%250Acollective%2520diversity%2520but%2520not%2520individual%2520creativity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.13481v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20AI%20Ideas%20Affect%20the%20Creativity%2C%20Diversity%2C%20and%20Evolution%20of%20Human%0A%20%20Ideas%3A%20Evidence%20From%20a%20Large%2C%20Dynamic%20Experiment&entry.906535625=Joshua%20Ashkinaze%20and%20Julia%20Mendelsohn%20and%20Li%20Qiwei%20and%20Ceren%20Budak%20and%20Eric%20Gilbert&entry.1292438233=%20%20Exposure%20to%20large%20language%20model%20output%20is%20rapidly%20increasing.%20How%20will%0Aseeing%20AI-generated%20ideas%20affect%20human%20ideas%3F%20We%20conducted%20an%20experiment%20%28800%2B%0Aparticipants%2C%2040%2B%20countries%29%20where%20participants%20viewed%20creative%20ideas%20that%20were%0Afrom%20ChatGPT%20or%20prior%20experimental%20participants%20and%20then%20brainstormed%20their%20own%0Aidea.%20We%20varied%20the%20number%20of%20AI-generated%20examples%20%28none%2C%20low%2C%20or%20high%0Aexposure%29%20and%20if%20the%20examples%20were%20labeled%20as%20%27AI%27%20%28disclosure%29.%20Our%20dynamic%0Aexperiment%20design%20--%20ideas%20from%20prior%20participants%20in%20an%20experimental%20condition%0Aare%20used%20as%20stimuli%20for%20future%20participants%20in%20the%20same%20experimental%20condition%0A--%20speaks%20to%20the%20interdependent%20process%20of%20cultural%20creation%3A%20creative%20ideas%0Aare%20built%20upon%20prior%20ideas.%20Hence%2C%20we%20capture%20the%20compounding%20effects%20of%20having%0ALLMs%20%27in%20the%20culture%20loop%27.%20We%20find%20that%20high%20AI%20exposure%20%28but%20not%20low%20AI%0Aexposure%29%20did%20not%20affect%20the%20creativity%20of%20individual%20ideas%20but%20did%20increase%0Athe%20average%20amount%20and%20rate%20of%20change%20of%20collective%20idea%20diversity.%20AI%20made%0Aideas%20different%2C%20not%20better.%20There%20were%20no%20main%20effects%20of%20disclosure.%20We%20also%0Afound%20that%20self-reported%20creative%20people%20were%20less%20influenced%20by%20knowing%20an%0Aidea%20was%20from%20AI%20and%20that%20participants%20may%20knowingly%20adopt%20AI%20ideas%20when%20the%0Atask%20is%20difficult.%20Our%20findings%20suggest%20that%20introducing%20AI%20ideas%20may%20increase%0Acollective%20diversity%20but%20not%20individual%20creativity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.13481v3&entry.124074799=Read"},
{"title": "Identifying Super Spreaders in Multilayer Networks", "author": "Micha\u0142 Czuba and Mateusz Stolarski and Adam Pir\u00f3g and Piotr Bielak and Piotr Br\u00f3dka", "abstract": "  Identifying super-spreaders can be framed as a subtask of the influence\nmaximisation problem. It seeks to pinpoint agents within a network that, if\nselected as single diffusion seeds, disseminate information most effectively.\nMultilayer networks, a specific class of heterogeneous graphs, can capture\ndiverse types of interactions (e.g., physical-virtual or professional-social),\nand thus offer a more accurate representation of complex relational structures.\nIn this work, we introduce a novel approach to identifying super-spreaders in\nsuch networks by leveraging graph neural networks. To this end, we construct a\ndataset by simulating information diffusion across hundreds of networks - to\nthe best of our knowledge, the first of its kind tailored specifically to\nmultilayer networks. We further formulate the task as a variation of the\nranking prediction problem based on a four-dimensional vector that quantifies\neach agent's spreading potential: (i) the number of activations; (ii) the\nduration of the diffusion process; (iii) the peak number of activations; and\n(iv) the simulation step at which this peak occurs. Our model,\nTopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom\naggregation layer. This design enables generalisation to previously unseen data\nand adapts to varying graph sizes. In an extensive evaluation, we compare our\nmodel against classic centrality-based heuristics and competitive deep learning\nmethods. The results, obtained across a broad spectrum of real-world and\nsynthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves\nsuperior performance in identifying high-impact nodes, while also offering\nimproved interpretability through its structured output.\n", "link": "http://arxiv.org/abs/2505.20980v2", "date": "2025-07-31", "relevancy": 1.4316, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4982}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4542}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identifying%20Super%20Spreaders%20in%20Multilayer%20Networks&body=Title%3A%20Identifying%20Super%20Spreaders%20in%20Multilayer%20Networks%0AAuthor%3A%20Micha%C5%82%20Czuba%20and%20Mateusz%20Stolarski%20and%20Adam%20Pir%C3%B3g%20and%20Piotr%20Bielak%20and%20Piotr%20Br%C3%B3dka%0AAbstract%3A%20%20%20Identifying%20super-spreaders%20can%20be%20framed%20as%20a%20subtask%20of%20the%20influence%0Amaximisation%20problem.%20It%20seeks%20to%20pinpoint%20agents%20within%20a%20network%20that%2C%20if%0Aselected%20as%20single%20diffusion%20seeds%2C%20disseminate%20information%20most%20effectively.%0AMultilayer%20networks%2C%20a%20specific%20class%20of%20heterogeneous%20graphs%2C%20can%20capture%0Adiverse%20types%20of%20interactions%20%28e.g.%2C%20physical-virtual%20or%20professional-social%29%2C%0Aand%20thus%20offer%20a%20more%20accurate%20representation%20of%20complex%20relational%20structures.%0AIn%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20to%20identifying%20super-spreaders%20in%0Asuch%20networks%20by%20leveraging%20graph%20neural%20networks.%20To%20this%20end%2C%20we%20construct%20a%0Adataset%20by%20simulating%20information%20diffusion%20across%20hundreds%20of%20networks%20-%20to%0Athe%20best%20of%20our%20knowledge%2C%20the%20first%20of%20its%20kind%20tailored%20specifically%20to%0Amultilayer%20networks.%20We%20further%20formulate%20the%20task%20as%20a%20variation%20of%20the%0Aranking%20prediction%20problem%20based%20on%20a%20four-dimensional%20vector%20that%20quantifies%0Aeach%20agent%27s%20spreading%20potential%3A%20%28i%29%20the%20number%20of%20activations%3B%20%28ii%29%20the%0Aduration%20of%20the%20diffusion%20process%3B%20%28iii%29%20the%20peak%20number%20of%20activations%3B%20and%0A%28iv%29%20the%20simulation%20step%20at%20which%20this%20peak%20occurs.%20Our%20model%2C%0ATopSpreadersNetwork%2C%20comprises%20a%20relationship-agnostic%20encoder%20and%20a%20custom%0Aaggregation%20layer.%20This%20design%20enables%20generalisation%20to%20previously%20unseen%20data%0Aand%20adapts%20to%20varying%20graph%20sizes.%20In%20an%20extensive%20evaluation%2C%20we%20compare%20our%0Amodel%20against%20classic%20centrality-based%20heuristics%20and%20competitive%20deep%20learning%0Amethods.%20The%20results%2C%20obtained%20across%20a%20broad%20spectrum%20of%20real-world%20and%0Asynthetic%20multilayer%20networks%2C%20demonstrate%20that%20TopSpreadersNetwork%20achieves%0Asuperior%20performance%20in%20identifying%20high-impact%20nodes%2C%20while%20also%20offering%0Aimproved%20interpretability%20through%20its%20structured%20output.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentifying%2520Super%2520Spreaders%2520in%2520Multilayer%2520Networks%26entry.906535625%3DMicha%25C5%2582%2520Czuba%2520and%2520Mateusz%2520Stolarski%2520and%2520Adam%2520Pir%25C3%25B3g%2520and%2520Piotr%2520Bielak%2520and%2520Piotr%2520Br%25C3%25B3dka%26entry.1292438233%3D%2520%2520Identifying%2520super-spreaders%2520can%2520be%2520framed%2520as%2520a%2520subtask%2520of%2520the%2520influence%250Amaximisation%2520problem.%2520It%2520seeks%2520to%2520pinpoint%2520agents%2520within%2520a%2520network%2520that%252C%2520if%250Aselected%2520as%2520single%2520diffusion%2520seeds%252C%2520disseminate%2520information%2520most%2520effectively.%250AMultilayer%2520networks%252C%2520a%2520specific%2520class%2520of%2520heterogeneous%2520graphs%252C%2520can%2520capture%250Adiverse%2520types%2520of%2520interactions%2520%2528e.g.%252C%2520physical-virtual%2520or%2520professional-social%2529%252C%250Aand%2520thus%2520offer%2520a%2520more%2520accurate%2520representation%2520of%2520complex%2520relational%2520structures.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520to%2520identifying%2520super-spreaders%2520in%250Asuch%2520networks%2520by%2520leveraging%2520graph%2520neural%2520networks.%2520To%2520this%2520end%252C%2520we%2520construct%2520a%250Adataset%2520by%2520simulating%2520information%2520diffusion%2520across%2520hundreds%2520of%2520networks%2520-%2520to%250Athe%2520best%2520of%2520our%2520knowledge%252C%2520the%2520first%2520of%2520its%2520kind%2520tailored%2520specifically%2520to%250Amultilayer%2520networks.%2520We%2520further%2520formulate%2520the%2520task%2520as%2520a%2520variation%2520of%2520the%250Aranking%2520prediction%2520problem%2520based%2520on%2520a%2520four-dimensional%2520vector%2520that%2520quantifies%250Aeach%2520agent%2527s%2520spreading%2520potential%253A%2520%2528i%2529%2520the%2520number%2520of%2520activations%253B%2520%2528ii%2529%2520the%250Aduration%2520of%2520the%2520diffusion%2520process%253B%2520%2528iii%2529%2520the%2520peak%2520number%2520of%2520activations%253B%2520and%250A%2528iv%2529%2520the%2520simulation%2520step%2520at%2520which%2520this%2520peak%2520occurs.%2520Our%2520model%252C%250ATopSpreadersNetwork%252C%2520comprises%2520a%2520relationship-agnostic%2520encoder%2520and%2520a%2520custom%250Aaggregation%2520layer.%2520This%2520design%2520enables%2520generalisation%2520to%2520previously%2520unseen%2520data%250Aand%2520adapts%2520to%2520varying%2520graph%2520sizes.%2520In%2520an%2520extensive%2520evaluation%252C%2520we%2520compare%2520our%250Amodel%2520against%2520classic%2520centrality-based%2520heuristics%2520and%2520competitive%2520deep%2520learning%250Amethods.%2520The%2520results%252C%2520obtained%2520across%2520a%2520broad%2520spectrum%2520of%2520real-world%2520and%250Asynthetic%2520multilayer%2520networks%252C%2520demonstrate%2520that%2520TopSpreadersNetwork%2520achieves%250Asuperior%2520performance%2520in%2520identifying%2520high-impact%2520nodes%252C%2520while%2520also%2520offering%250Aimproved%2520interpretability%2520through%2520its%2520structured%2520output.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Super%20Spreaders%20in%20Multilayer%20Networks&entry.906535625=Micha%C5%82%20Czuba%20and%20Mateusz%20Stolarski%20and%20Adam%20Pir%C3%B3g%20and%20Piotr%20Bielak%20and%20Piotr%20Br%C3%B3dka&entry.1292438233=%20%20Identifying%20super-spreaders%20can%20be%20framed%20as%20a%20subtask%20of%20the%20influence%0Amaximisation%20problem.%20It%20seeks%20to%20pinpoint%20agents%20within%20a%20network%20that%2C%20if%0Aselected%20as%20single%20diffusion%20seeds%2C%20disseminate%20information%20most%20effectively.%0AMultilayer%20networks%2C%20a%20specific%20class%20of%20heterogeneous%20graphs%2C%20can%20capture%0Adiverse%20types%20of%20interactions%20%28e.g.%2C%20physical-virtual%20or%20professional-social%29%2C%0Aand%20thus%20offer%20a%20more%20accurate%20representation%20of%20complex%20relational%20structures.%0AIn%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20to%20identifying%20super-spreaders%20in%0Asuch%20networks%20by%20leveraging%20graph%20neural%20networks.%20To%20this%20end%2C%20we%20construct%20a%0Adataset%20by%20simulating%20information%20diffusion%20across%20hundreds%20of%20networks%20-%20to%0Athe%20best%20of%20our%20knowledge%2C%20the%20first%20of%20its%20kind%20tailored%20specifically%20to%0Amultilayer%20networks.%20We%20further%20formulate%20the%20task%20as%20a%20variation%20of%20the%0Aranking%20prediction%20problem%20based%20on%20a%20four-dimensional%20vector%20that%20quantifies%0Aeach%20agent%27s%20spreading%20potential%3A%20%28i%29%20the%20number%20of%20activations%3B%20%28ii%29%20the%0Aduration%20of%20the%20diffusion%20process%3B%20%28iii%29%20the%20peak%20number%20of%20activations%3B%20and%0A%28iv%29%20the%20simulation%20step%20at%20which%20this%20peak%20occurs.%20Our%20model%2C%0ATopSpreadersNetwork%2C%20comprises%20a%20relationship-agnostic%20encoder%20and%20a%20custom%0Aaggregation%20layer.%20This%20design%20enables%20generalisation%20to%20previously%20unseen%20data%0Aand%20adapts%20to%20varying%20graph%20sizes.%20In%20an%20extensive%20evaluation%2C%20we%20compare%20our%0Amodel%20against%20classic%20centrality-based%20heuristics%20and%20competitive%20deep%20learning%0Amethods.%20The%20results%2C%20obtained%20across%20a%20broad%20spectrum%20of%20real-world%20and%0Asynthetic%20multilayer%20networks%2C%20demonstrate%20that%20TopSpreadersNetwork%20achieves%0Asuperior%20performance%20in%20identifying%20high-impact%20nodes%2C%20while%20also%20offering%0Aimproved%20interpretability%20through%20its%20structured%20output.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20980v2&entry.124074799=Read"},
{"title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and\n  Monte Carlo Tree Search", "author": "Zerui Yang and Yuwei Wan and Siyu Yan and Yudai Matsuda and Tong Xie and Bram Hoex and Linqi Song", "abstract": "  Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug repositioning. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repositioning. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Extensive experiments on the\nDrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially\nhigher recall and robustness compared to both general-purpose LLMs and deep\nlearning baselines. Our results highlight the importance of structured\nreasoning, agent-based collaboration, and feedback-driven search mechanisms in\nadvancing LLM applications for drug repositioning.\n", "link": "http://arxiv.org/abs/2507.07426v3", "date": "2025-07-31", "relevancy": 1.4753, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5142}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4985}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4801}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DrugMCTS%3A%20a%20drug%20repurposing%20framework%20combining%20multi-agent%2C%20RAG%20and%0A%20%20Monte%20Carlo%20Tree%20Search&body=Title%3A%20DrugMCTS%3A%20a%20drug%20repurposing%20framework%20combining%20multi-agent%2C%20RAG%20and%0A%20%20Monte%20Carlo%20Tree%20Search%0AAuthor%3A%20Zerui%20Yang%20and%20Yuwei%20Wan%20and%20Siyu%20Yan%20and%20Yudai%20Matsuda%20and%20Tong%20Xie%20and%20Bram%20Hoex%20and%20Linqi%20Song%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20have%20demonstrated%20considerable%0Apotential%20in%20scientific%20domains%20such%20as%20drug%20repositioning.%20However%2C%20their%0Aeffectiveness%20remains%20constrained%20when%20reasoning%20extends%20beyond%20the%20knowledge%0Aacquired%20during%20pretraining.%20Conventional%20approaches%2C%20such%20as%20fine-tuning%20or%0Aretrieval-augmented%20generation%2C%20face%20limitations%20in%20either%20imposing%20high%0Acomputational%20overhead%20or%20failing%20to%20fully%20exploit%20structured%20scientific%20data.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20DrugMCTS%2C%20a%20novel%20framework%20that%0Asynergistically%20integrates%20RAG%2C%20multi-agent%20collaboration%2C%20and%20Monte%20Carlo%20Tree%0ASearch%20for%20drug%20repositioning.%20The%20framework%20employs%20five%20specialized%20agents%0Atasked%20with%20retrieving%20and%20analyzing%20molecular%20and%20protein%20information%2C%20thereby%0Aenabling%20structured%20and%20iterative%20reasoning.%20Extensive%20experiments%20on%20the%0ADrugBank%20and%20KIBA%20datasets%20demonstrate%20that%20DrugMCTS%20achieves%20substantially%0Ahigher%20recall%20and%20robustness%20compared%20to%20both%20general-purpose%20LLMs%20and%20deep%0Alearning%20baselines.%20Our%20results%20highlight%20the%20importance%20of%20structured%0Areasoning%2C%20agent-based%20collaboration%2C%20and%20feedback-driven%20search%20mechanisms%20in%0Aadvancing%20LLM%20applications%20for%20drug%20repositioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07426v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrugMCTS%253A%2520a%2520drug%2520repurposing%2520framework%2520combining%2520multi-agent%252C%2520RAG%2520and%250A%2520%2520Monte%2520Carlo%2520Tree%2520Search%26entry.906535625%3DZerui%2520Yang%2520and%2520Yuwei%2520Wan%2520and%2520Siyu%2520Yan%2520and%2520Yudai%2520Matsuda%2520and%2520Tong%2520Xie%2520and%2520Bram%2520Hoex%2520and%2520Linqi%2520Song%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520have%2520demonstrated%2520considerable%250Apotential%2520in%2520scientific%2520domains%2520such%2520as%2520drug%2520repositioning.%2520However%252C%2520their%250Aeffectiveness%2520remains%2520constrained%2520when%2520reasoning%2520extends%2520beyond%2520the%2520knowledge%250Aacquired%2520during%2520pretraining.%2520Conventional%2520approaches%252C%2520such%2520as%2520fine-tuning%2520or%250Aretrieval-augmented%2520generation%252C%2520face%2520limitations%2520in%2520either%2520imposing%2520high%250Acomputational%2520overhead%2520or%2520failing%2520to%2520fully%2520exploit%2520structured%2520scientific%2520data.%250ATo%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520DrugMCTS%252C%2520a%2520novel%2520framework%2520that%250Asynergistically%2520integrates%2520RAG%252C%2520multi-agent%2520collaboration%252C%2520and%2520Monte%2520Carlo%2520Tree%250ASearch%2520for%2520drug%2520repositioning.%2520The%2520framework%2520employs%2520five%2520specialized%2520agents%250Atasked%2520with%2520retrieving%2520and%2520analyzing%2520molecular%2520and%2520protein%2520information%252C%2520thereby%250Aenabling%2520structured%2520and%2520iterative%2520reasoning.%2520Extensive%2520experiments%2520on%2520the%250ADrugBank%2520and%2520KIBA%2520datasets%2520demonstrate%2520that%2520DrugMCTS%2520achieves%2520substantially%250Ahigher%2520recall%2520and%2520robustness%2520compared%2520to%2520both%2520general-purpose%2520LLMs%2520and%2520deep%250Alearning%2520baselines.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520structured%250Areasoning%252C%2520agent-based%2520collaboration%252C%2520and%2520feedback-driven%2520search%2520mechanisms%2520in%250Aadvancing%2520LLM%2520applications%2520for%2520drug%2520repositioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07426v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DrugMCTS%3A%20a%20drug%20repurposing%20framework%20combining%20multi-agent%2C%20RAG%20and%0A%20%20Monte%20Carlo%20Tree%20Search&entry.906535625=Zerui%20Yang%20and%20Yuwei%20Wan%20and%20Siyu%20Yan%20and%20Yudai%20Matsuda%20and%20Tong%20Xie%20and%20Bram%20Hoex%20and%20Linqi%20Song&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20have%20demonstrated%20considerable%0Apotential%20in%20scientific%20domains%20such%20as%20drug%20repositioning.%20However%2C%20their%0Aeffectiveness%20remains%20constrained%20when%20reasoning%20extends%20beyond%20the%20knowledge%0Aacquired%20during%20pretraining.%20Conventional%20approaches%2C%20such%20as%20fine-tuning%20or%0Aretrieval-augmented%20generation%2C%20face%20limitations%20in%20either%20imposing%20high%0Acomputational%20overhead%20or%20failing%20to%20fully%20exploit%20structured%20scientific%20data.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20DrugMCTS%2C%20a%20novel%20framework%20that%0Asynergistically%20integrates%20RAG%2C%20multi-agent%20collaboration%2C%20and%20Monte%20Carlo%20Tree%0ASearch%20for%20drug%20repositioning.%20The%20framework%20employs%20five%20specialized%20agents%0Atasked%20with%20retrieving%20and%20analyzing%20molecular%20and%20protein%20information%2C%20thereby%0Aenabling%20structured%20and%20iterative%20reasoning.%20Extensive%20experiments%20on%20the%0ADrugBank%20and%20KIBA%20datasets%20demonstrate%20that%20DrugMCTS%20achieves%20substantially%0Ahigher%20recall%20and%20robustness%20compared%20to%20both%20general-purpose%20LLMs%20and%20deep%0Alearning%20baselines.%20Our%20results%20highlight%20the%20importance%20of%20structured%0Areasoning%2C%20agent-based%20collaboration%2C%20and%20feedback-driven%20search%20mechanisms%20in%0Aadvancing%20LLM%20applications%20for%20drug%20repositioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07426v3&entry.124074799=Read"},
{"title": "Mamba-based Efficient Spatio-Frequency Motion Perception for Video\n  Camouflaged Object Detection", "author": "Xin Li and Keren Fu and Qijun Zhao", "abstract": "  Existing video camouflaged object detection (VCOD) methods primarily rely on\nspatial appearance features to perceive motion cues for breaking camouflage.\nHowever, the high similarity between foreground and background in VCOD results\nin limited discriminability of spatial appearance features (e.g., color and\ntexture), restricting detection accuracy and completeness. Recent studies\ndemonstrate that frequency features can not only enhance feature representation\nto compensate for appearance limitations but also perceive motion through\ndynamic variations in frequency energy. Furthermore, the emerging state space\nmodel called Mamba, enables efficient perception of motion cues in frame\nsequences due to its linear-time long-sequence modeling capability. Motivated\nby this, we propose a novel visual camouflage Mamba (Vcamba) based on\nspatio-frequency motion perception that integrates frequency and spatial\nfeatures for efficient and accurate VCOD. Specifically, we propose a receptive\nfield visual state space (RFVSS) module to extract multi-scale spatial features\nafter sequence modeling. For frequency learning, we introduce an adaptive\nfrequency component enhancement (AFE) module with a novel frequency-domain\nsequential scanning strategy to maintain semantic consistency. Then we propose\na space-based long-range motion perception (SLMP) module and a frequency-based\nlong-range motion perception (FLMP) module to model spatio-temporal and\nfrequency-temporal sequences in spatial and frequency phase domains. Finally,\nthe space and frequency motion fusion module (SFMF) integrates dual-domain\nfeatures for unified motion representation. Experimental results show that our\nVcamba outperforms state-of-the-art methods across 6 evaluation metrics on 2\ndatasets with lower computation cost, confirming the superiority of Vcamba. Our\ncode is available at: https://github.com/BoydeLi/Vcamba.\n", "link": "http://arxiv.org/abs/2507.23601v1", "date": "2025-07-31", "relevancy": 1.7044, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5708}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5691}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba-based%20Efficient%20Spatio-Frequency%20Motion%20Perception%20for%20Video%0A%20%20Camouflaged%20Object%20Detection&body=Title%3A%20Mamba-based%20Efficient%20Spatio-Frequency%20Motion%20Perception%20for%20Video%0A%20%20Camouflaged%20Object%20Detection%0AAuthor%3A%20Xin%20Li%20and%20Keren%20Fu%20and%20Qijun%20Zhao%0AAbstract%3A%20%20%20Existing%20video%20camouflaged%20object%20detection%20%28VCOD%29%20methods%20primarily%20rely%20on%0Aspatial%20appearance%20features%20to%20perceive%20motion%20cues%20for%20breaking%20camouflage.%0AHowever%2C%20the%20high%20similarity%20between%20foreground%20and%20background%20in%20VCOD%20results%0Ain%20limited%20discriminability%20of%20spatial%20appearance%20features%20%28e.g.%2C%20color%20and%0Atexture%29%2C%20restricting%20detection%20accuracy%20and%20completeness.%20Recent%20studies%0Ademonstrate%20that%20frequency%20features%20can%20not%20only%20enhance%20feature%20representation%0Ato%20compensate%20for%20appearance%20limitations%20but%20also%20perceive%20motion%20through%0Adynamic%20variations%20in%20frequency%20energy.%20Furthermore%2C%20the%20emerging%20state%20space%0Amodel%20called%20Mamba%2C%20enables%20efficient%20perception%20of%20motion%20cues%20in%20frame%0Asequences%20due%20to%20its%20linear-time%20long-sequence%20modeling%20capability.%20Motivated%0Aby%20this%2C%20we%20propose%20a%20novel%20visual%20camouflage%20Mamba%20%28Vcamba%29%20based%20on%0Aspatio-frequency%20motion%20perception%20that%20integrates%20frequency%20and%20spatial%0Afeatures%20for%20efficient%20and%20accurate%20VCOD.%20Specifically%2C%20we%20propose%20a%20receptive%0Afield%20visual%20state%20space%20%28RFVSS%29%20module%20to%20extract%20multi-scale%20spatial%20features%0Aafter%20sequence%20modeling.%20For%20frequency%20learning%2C%20we%20introduce%20an%20adaptive%0Afrequency%20component%20enhancement%20%28AFE%29%20module%20with%20a%20novel%20frequency-domain%0Asequential%20scanning%20strategy%20to%20maintain%20semantic%20consistency.%20Then%20we%20propose%0Aa%20space-based%20long-range%20motion%20perception%20%28SLMP%29%20module%20and%20a%20frequency-based%0Along-range%20motion%20perception%20%28FLMP%29%20module%20to%20model%20spatio-temporal%20and%0Afrequency-temporal%20sequences%20in%20spatial%20and%20frequency%20phase%20domains.%20Finally%2C%0Athe%20space%20and%20frequency%20motion%20fusion%20module%20%28SFMF%29%20integrates%20dual-domain%0Afeatures%20for%20unified%20motion%20representation.%20Experimental%20results%20show%20that%20our%0AVcamba%20outperforms%20state-of-the-art%20methods%20across%206%20evaluation%20metrics%20on%202%0Adatasets%20with%20lower%20computation%20cost%2C%20confirming%20the%20superiority%20of%20Vcamba.%20Our%0Acode%20is%20available%20at%3A%20https%3A//github.com/BoydeLi/Vcamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba-based%2520Efficient%2520Spatio-Frequency%2520Motion%2520Perception%2520for%2520Video%250A%2520%2520Camouflaged%2520Object%2520Detection%26entry.906535625%3DXin%2520Li%2520and%2520Keren%2520Fu%2520and%2520Qijun%2520Zhao%26entry.1292438233%3D%2520%2520Existing%2520video%2520camouflaged%2520object%2520detection%2520%2528VCOD%2529%2520methods%2520primarily%2520rely%2520on%250Aspatial%2520appearance%2520features%2520to%2520perceive%2520motion%2520cues%2520for%2520breaking%2520camouflage.%250AHowever%252C%2520the%2520high%2520similarity%2520between%2520foreground%2520and%2520background%2520in%2520VCOD%2520results%250Ain%2520limited%2520discriminability%2520of%2520spatial%2520appearance%2520features%2520%2528e.g.%252C%2520color%2520and%250Atexture%2529%252C%2520restricting%2520detection%2520accuracy%2520and%2520completeness.%2520Recent%2520studies%250Ademonstrate%2520that%2520frequency%2520features%2520can%2520not%2520only%2520enhance%2520feature%2520representation%250Ato%2520compensate%2520for%2520appearance%2520limitations%2520but%2520also%2520perceive%2520motion%2520through%250Adynamic%2520variations%2520in%2520frequency%2520energy.%2520Furthermore%252C%2520the%2520emerging%2520state%2520space%250Amodel%2520called%2520Mamba%252C%2520enables%2520efficient%2520perception%2520of%2520motion%2520cues%2520in%2520frame%250Asequences%2520due%2520to%2520its%2520linear-time%2520long-sequence%2520modeling%2520capability.%2520Motivated%250Aby%2520this%252C%2520we%2520propose%2520a%2520novel%2520visual%2520camouflage%2520Mamba%2520%2528Vcamba%2529%2520based%2520on%250Aspatio-frequency%2520motion%2520perception%2520that%2520integrates%2520frequency%2520and%2520spatial%250Afeatures%2520for%2520efficient%2520and%2520accurate%2520VCOD.%2520Specifically%252C%2520we%2520propose%2520a%2520receptive%250Afield%2520visual%2520state%2520space%2520%2528RFVSS%2529%2520module%2520to%2520extract%2520multi-scale%2520spatial%2520features%250Aafter%2520sequence%2520modeling.%2520For%2520frequency%2520learning%252C%2520we%2520introduce%2520an%2520adaptive%250Afrequency%2520component%2520enhancement%2520%2528AFE%2529%2520module%2520with%2520a%2520novel%2520frequency-domain%250Asequential%2520scanning%2520strategy%2520to%2520maintain%2520semantic%2520consistency.%2520Then%2520we%2520propose%250Aa%2520space-based%2520long-range%2520motion%2520perception%2520%2528SLMP%2529%2520module%2520and%2520a%2520frequency-based%250Along-range%2520motion%2520perception%2520%2528FLMP%2529%2520module%2520to%2520model%2520spatio-temporal%2520and%250Afrequency-temporal%2520sequences%2520in%2520spatial%2520and%2520frequency%2520phase%2520domains.%2520Finally%252C%250Athe%2520space%2520and%2520frequency%2520motion%2520fusion%2520module%2520%2528SFMF%2529%2520integrates%2520dual-domain%250Afeatures%2520for%2520unified%2520motion%2520representation.%2520Experimental%2520results%2520show%2520that%2520our%250AVcamba%2520outperforms%2520state-of-the-art%2520methods%2520across%25206%2520evaluation%2520metrics%2520on%25202%250Adatasets%2520with%2520lower%2520computation%2520cost%252C%2520confirming%2520the%2520superiority%2520of%2520Vcamba.%2520Our%250Acode%2520is%2520available%2520at%253A%2520https%253A//github.com/BoydeLi/Vcamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba-based%20Efficient%20Spatio-Frequency%20Motion%20Perception%20for%20Video%0A%20%20Camouflaged%20Object%20Detection&entry.906535625=Xin%20Li%20and%20Keren%20Fu%20and%20Qijun%20Zhao&entry.1292438233=%20%20Existing%20video%20camouflaged%20object%20detection%20%28VCOD%29%20methods%20primarily%20rely%20on%0Aspatial%20appearance%20features%20to%20perceive%20motion%20cues%20for%20breaking%20camouflage.%0AHowever%2C%20the%20high%20similarity%20between%20foreground%20and%20background%20in%20VCOD%20results%0Ain%20limited%20discriminability%20of%20spatial%20appearance%20features%20%28e.g.%2C%20color%20and%0Atexture%29%2C%20restricting%20detection%20accuracy%20and%20completeness.%20Recent%20studies%0Ademonstrate%20that%20frequency%20features%20can%20not%20only%20enhance%20feature%20representation%0Ato%20compensate%20for%20appearance%20limitations%20but%20also%20perceive%20motion%20through%0Adynamic%20variations%20in%20frequency%20energy.%20Furthermore%2C%20the%20emerging%20state%20space%0Amodel%20called%20Mamba%2C%20enables%20efficient%20perception%20of%20motion%20cues%20in%20frame%0Asequences%20due%20to%20its%20linear-time%20long-sequence%20modeling%20capability.%20Motivated%0Aby%20this%2C%20we%20propose%20a%20novel%20visual%20camouflage%20Mamba%20%28Vcamba%29%20based%20on%0Aspatio-frequency%20motion%20perception%20that%20integrates%20frequency%20and%20spatial%0Afeatures%20for%20efficient%20and%20accurate%20VCOD.%20Specifically%2C%20we%20propose%20a%20receptive%0Afield%20visual%20state%20space%20%28RFVSS%29%20module%20to%20extract%20multi-scale%20spatial%20features%0Aafter%20sequence%20modeling.%20For%20frequency%20learning%2C%20we%20introduce%20an%20adaptive%0Afrequency%20component%20enhancement%20%28AFE%29%20module%20with%20a%20novel%20frequency-domain%0Asequential%20scanning%20strategy%20to%20maintain%20semantic%20consistency.%20Then%20we%20propose%0Aa%20space-based%20long-range%20motion%20perception%20%28SLMP%29%20module%20and%20a%20frequency-based%0Along-range%20motion%20perception%20%28FLMP%29%20module%20to%20model%20spatio-temporal%20and%0Afrequency-temporal%20sequences%20in%20spatial%20and%20frequency%20phase%20domains.%20Finally%2C%0Athe%20space%20and%20frequency%20motion%20fusion%20module%20%28SFMF%29%20integrates%20dual-domain%0Afeatures%20for%20unified%20motion%20representation.%20Experimental%20results%20show%20that%20our%0AVcamba%20outperforms%20state-of-the-art%20methods%20across%206%20evaluation%20metrics%20on%202%0Adatasets%20with%20lower%20computation%20cost%2C%20confirming%20the%20superiority%20of%20Vcamba.%20Our%0Acode%20is%20available%20at%3A%20https%3A//github.com/BoydeLi/Vcamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23601v1&entry.124074799=Read"},
{"title": "Differentially Private Clipped-SGD: High-Probability Convergence with\n  Arbitrary Clipping Level", "author": "Saleh Vatan Khah and Savelii Chezhegov and Shahrokh Farahmand and Samuel Horv\u00e1th and Eduard Gorbunov", "abstract": "  Gradient clipping is a fundamental tool in Deep Learning, improving the\nhigh-probability convergence of stochastic first-order methods like SGD,\nAdaGrad, and Adam under heavy-tailed noise, which is common in training large\nlanguage models. It is also a crucial component of Differential Privacy (DP)\nmechanisms. However, existing high-probability convergence analyses typically\nrequire the clipping threshold to increase with the number of optimization\nsteps, which is incompatible with standard DP mechanisms like the Gaussian\nmechanism. In this work, we close this gap by providing the first\nhigh-probability convergence analysis for DP-Clipped-SGD with a fixed clipping\nlevel, applicable to both convex and non-convex smooth optimization under\nheavy-tailed noise, characterized by a bounded central $\\alpha$-th moment\nassumption, $\\alpha \\in (1,2]$. Our results show that, with a fixed clipping\nlevel, the method converges to a neighborhood of the optimal solution with a\nfaster rate than the existing ones. The neighborhood can be balanced against\nthe noise introduced by DP, providing a refined trade-off between convergence\nspeed and privacy guarantees.\n", "link": "http://arxiv.org/abs/2507.23512v1", "date": "2025-07-31", "relevancy": 1.8775, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4831}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4648}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentially%20Private%20Clipped-SGD%3A%20High-Probability%20Convergence%20with%0A%20%20Arbitrary%20Clipping%20Level&body=Title%3A%20Differentially%20Private%20Clipped-SGD%3A%20High-Probability%20Convergence%20with%0A%20%20Arbitrary%20Clipping%20Level%0AAuthor%3A%20Saleh%20Vatan%20Khah%20and%20Savelii%20Chezhegov%20and%20Shahrokh%20Farahmand%20and%20Samuel%20Horv%C3%A1th%20and%20Eduard%20Gorbunov%0AAbstract%3A%20%20%20Gradient%20clipping%20is%20a%20fundamental%20tool%20in%20Deep%20Learning%2C%20improving%20the%0Ahigh-probability%20convergence%20of%20stochastic%20first-order%20methods%20like%20SGD%2C%0AAdaGrad%2C%20and%20Adam%20under%20heavy-tailed%20noise%2C%20which%20is%20common%20in%20training%20large%0Alanguage%20models.%20It%20is%20also%20a%20crucial%20component%20of%20Differential%20Privacy%20%28DP%29%0Amechanisms.%20However%2C%20existing%20high-probability%20convergence%20analyses%20typically%0Arequire%20the%20clipping%20threshold%20to%20increase%20with%20the%20number%20of%20optimization%0Asteps%2C%20which%20is%20incompatible%20with%20standard%20DP%20mechanisms%20like%20the%20Gaussian%0Amechanism.%20In%20this%20work%2C%20we%20close%20this%20gap%20by%20providing%20the%20first%0Ahigh-probability%20convergence%20analysis%20for%20DP-Clipped-SGD%20with%20a%20fixed%20clipping%0Alevel%2C%20applicable%20to%20both%20convex%20and%20non-convex%20smooth%20optimization%20under%0Aheavy-tailed%20noise%2C%20characterized%20by%20a%20bounded%20central%20%24%5Calpha%24-th%20moment%0Aassumption%2C%20%24%5Calpha%20%5Cin%20%281%2C2%5D%24.%20Our%20results%20show%20that%2C%20with%20a%20fixed%20clipping%0Alevel%2C%20the%20method%20converges%20to%20a%20neighborhood%20of%20the%20optimal%20solution%20with%20a%0Afaster%20rate%20than%20the%20existing%20ones.%20The%20neighborhood%20can%20be%20balanced%20against%0Athe%20noise%20introduced%20by%20DP%2C%20providing%20a%20refined%20trade-off%20between%20convergence%0Aspeed%20and%20privacy%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentially%2520Private%2520Clipped-SGD%253A%2520High-Probability%2520Convergence%2520with%250A%2520%2520Arbitrary%2520Clipping%2520Level%26entry.906535625%3DSaleh%2520Vatan%2520Khah%2520and%2520Savelii%2520Chezhegov%2520and%2520Shahrokh%2520Farahmand%2520and%2520Samuel%2520Horv%25C3%25A1th%2520and%2520Eduard%2520Gorbunov%26entry.1292438233%3D%2520%2520Gradient%2520clipping%2520is%2520a%2520fundamental%2520tool%2520in%2520Deep%2520Learning%252C%2520improving%2520the%250Ahigh-probability%2520convergence%2520of%2520stochastic%2520first-order%2520methods%2520like%2520SGD%252C%250AAdaGrad%252C%2520and%2520Adam%2520under%2520heavy-tailed%2520noise%252C%2520which%2520is%2520common%2520in%2520training%2520large%250Alanguage%2520models.%2520It%2520is%2520also%2520a%2520crucial%2520component%2520of%2520Differential%2520Privacy%2520%2528DP%2529%250Amechanisms.%2520However%252C%2520existing%2520high-probability%2520convergence%2520analyses%2520typically%250Arequire%2520the%2520clipping%2520threshold%2520to%2520increase%2520with%2520the%2520number%2520of%2520optimization%250Asteps%252C%2520which%2520is%2520incompatible%2520with%2520standard%2520DP%2520mechanisms%2520like%2520the%2520Gaussian%250Amechanism.%2520In%2520this%2520work%252C%2520we%2520close%2520this%2520gap%2520by%2520providing%2520the%2520first%250Ahigh-probability%2520convergence%2520analysis%2520for%2520DP-Clipped-SGD%2520with%2520a%2520fixed%2520clipping%250Alevel%252C%2520applicable%2520to%2520both%2520convex%2520and%2520non-convex%2520smooth%2520optimization%2520under%250Aheavy-tailed%2520noise%252C%2520characterized%2520by%2520a%2520bounded%2520central%2520%2524%255Calpha%2524-th%2520moment%250Aassumption%252C%2520%2524%255Calpha%2520%255Cin%2520%25281%252C2%255D%2524.%2520Our%2520results%2520show%2520that%252C%2520with%2520a%2520fixed%2520clipping%250Alevel%252C%2520the%2520method%2520converges%2520to%2520a%2520neighborhood%2520of%2520the%2520optimal%2520solution%2520with%2520a%250Afaster%2520rate%2520than%2520the%2520existing%2520ones.%2520The%2520neighborhood%2520can%2520be%2520balanced%2520against%250Athe%2520noise%2520introduced%2520by%2520DP%252C%2520providing%2520a%2520refined%2520trade-off%2520between%2520convergence%250Aspeed%2520and%2520privacy%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentially%20Private%20Clipped-SGD%3A%20High-Probability%20Convergence%20with%0A%20%20Arbitrary%20Clipping%20Level&entry.906535625=Saleh%20Vatan%20Khah%20and%20Savelii%20Chezhegov%20and%20Shahrokh%20Farahmand%20and%20Samuel%20Horv%C3%A1th%20and%20Eduard%20Gorbunov&entry.1292438233=%20%20Gradient%20clipping%20is%20a%20fundamental%20tool%20in%20Deep%20Learning%2C%20improving%20the%0Ahigh-probability%20convergence%20of%20stochastic%20first-order%20methods%20like%20SGD%2C%0AAdaGrad%2C%20and%20Adam%20under%20heavy-tailed%20noise%2C%20which%20is%20common%20in%20training%20large%0Alanguage%20models.%20It%20is%20also%20a%20crucial%20component%20of%20Differential%20Privacy%20%28DP%29%0Amechanisms.%20However%2C%20existing%20high-probability%20convergence%20analyses%20typically%0Arequire%20the%20clipping%20threshold%20to%20increase%20with%20the%20number%20of%20optimization%0Asteps%2C%20which%20is%20incompatible%20with%20standard%20DP%20mechanisms%20like%20the%20Gaussian%0Amechanism.%20In%20this%20work%2C%20we%20close%20this%20gap%20by%20providing%20the%20first%0Ahigh-probability%20convergence%20analysis%20for%20DP-Clipped-SGD%20with%20a%20fixed%20clipping%0Alevel%2C%20applicable%20to%20both%20convex%20and%20non-convex%20smooth%20optimization%20under%0Aheavy-tailed%20noise%2C%20characterized%20by%20a%20bounded%20central%20%24%5Calpha%24-th%20moment%0Aassumption%2C%20%24%5Calpha%20%5Cin%20%281%2C2%5D%24.%20Our%20results%20show%20that%2C%20with%20a%20fixed%20clipping%0Alevel%2C%20the%20method%20converges%20to%20a%20neighborhood%20of%20the%20optimal%20solution%20with%20a%0Afaster%20rate%20than%20the%20existing%20ones.%20The%20neighborhood%20can%20be%20balanced%20against%0Athe%20noise%20introduced%20by%20DP%2C%20providing%20a%20refined%20trade-off%20between%20convergence%0Aspeed%20and%20privacy%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23512v1&entry.124074799=Read"},
{"title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "author": "Long Phan and Mantas Mazeika and Andy Zou and Dan Hendrycks", "abstract": "  Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.\n", "link": "http://arxiv.org/abs/2507.23701v1", "date": "2025-07-31", "relevancy": 2.0478, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5204}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TextQuests%3A%20How%20Good%20are%20LLMs%20at%20Text-Based%20Video%20Games%3F&body=Title%3A%20TextQuests%3A%20How%20Good%20are%20LLMs%20at%20Text-Based%20Video%20Games%3F%0AAuthor%3A%20Long%20Phan%20and%20Mantas%20Mazeika%20and%20Andy%20Zou%20and%20Dan%20Hendrycks%0AAbstract%3A%20%20%20Evaluating%20AI%20agents%20within%20complex%2C%20interactive%20environments%20that%20mirror%0Areal-world%20challenges%20is%20critical%20for%20understanding%20their%20practical%0Acapabilities.%20While%20existing%20agent%20benchmarks%20effectively%20assess%20skills%20like%0Atool%20use%20or%20performance%20on%20structured%20tasks%2C%20they%20often%20do%20not%20fully%20capture%20an%0Aagent%27s%20ability%20to%20operate%20autonomously%20in%20exploratory%20environments%20that%20demand%0Asustained%2C%20self-directed%20reasoning%20over%20a%20long%20and%20growing%20context.%20To%20spur%20the%0Adevelopment%20of%20agents%20capable%20of%20more%20robust%20intrinsic%20reasoning%20over%20long%0Ahorizons%2C%20we%20introduce%20TextQuests%2C%20a%20benchmark%20based%20on%20the%20Infocom%20suite%20of%0Ainteractive%20fiction%20games.%20These%20text-based%20adventures%2C%20which%20can%20take%20human%0Aplayers%20over%2030%20hours%20and%20require%20hundreds%20of%20precise%20actions%20to%20solve%2C%20serve%0Aas%20an%20effective%20proxy%20for%20evaluating%20AI%20agents%20on%20focused%2C%20stateful%20tasks.%20The%0Abenchmark%20is%20specifically%20designed%20to%20assess%20an%20LLM%20agent%27s%20capacity%20for%0Aself-contained%20problem-solving%20by%20precluding%20the%20use%20of%20external%20tools%2C%20thereby%0Afocusing%20on%20intrinsic%20long-context%20reasoning%20capabilities%20in%20an%20exploratory%0Aenvironment%20characterized%20by%20the%20need%20for%20trial-and-error%20learning%20and%0Asustained%20problem-solving%20within%20a%20single%20interactive%20session.%20We%20release%0ATextQuests%20at%20https%3A//textquests.ai.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextQuests%253A%2520How%2520Good%2520are%2520LLMs%2520at%2520Text-Based%2520Video%2520Games%253F%26entry.906535625%3DLong%2520Phan%2520and%2520Mantas%2520Mazeika%2520and%2520Andy%2520Zou%2520and%2520Dan%2520Hendrycks%26entry.1292438233%3D%2520%2520Evaluating%2520AI%2520agents%2520within%2520complex%252C%2520interactive%2520environments%2520that%2520mirror%250Areal-world%2520challenges%2520is%2520critical%2520for%2520understanding%2520their%2520practical%250Acapabilities.%2520While%2520existing%2520agent%2520benchmarks%2520effectively%2520assess%2520skills%2520like%250Atool%2520use%2520or%2520performance%2520on%2520structured%2520tasks%252C%2520they%2520often%2520do%2520not%2520fully%2520capture%2520an%250Aagent%2527s%2520ability%2520to%2520operate%2520autonomously%2520in%2520exploratory%2520environments%2520that%2520demand%250Asustained%252C%2520self-directed%2520reasoning%2520over%2520a%2520long%2520and%2520growing%2520context.%2520To%2520spur%2520the%250Adevelopment%2520of%2520agents%2520capable%2520of%2520more%2520robust%2520intrinsic%2520reasoning%2520over%2520long%250Ahorizons%252C%2520we%2520introduce%2520TextQuests%252C%2520a%2520benchmark%2520based%2520on%2520the%2520Infocom%2520suite%2520of%250Ainteractive%2520fiction%2520games.%2520These%2520text-based%2520adventures%252C%2520which%2520can%2520take%2520human%250Aplayers%2520over%252030%2520hours%2520and%2520require%2520hundreds%2520of%2520precise%2520actions%2520to%2520solve%252C%2520serve%250Aas%2520an%2520effective%2520proxy%2520for%2520evaluating%2520AI%2520agents%2520on%2520focused%252C%2520stateful%2520tasks.%2520The%250Abenchmark%2520is%2520specifically%2520designed%2520to%2520assess%2520an%2520LLM%2520agent%2527s%2520capacity%2520for%250Aself-contained%2520problem-solving%2520by%2520precluding%2520the%2520use%2520of%2520external%2520tools%252C%2520thereby%250Afocusing%2520on%2520intrinsic%2520long-context%2520reasoning%2520capabilities%2520in%2520an%2520exploratory%250Aenvironment%2520characterized%2520by%2520the%2520need%2520for%2520trial-and-error%2520learning%2520and%250Asustained%2520problem-solving%2520within%2520a%2520single%2520interactive%2520session.%2520We%2520release%250ATextQuests%2520at%2520https%253A//textquests.ai.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TextQuests%3A%20How%20Good%20are%20LLMs%20at%20Text-Based%20Video%20Games%3F&entry.906535625=Long%20Phan%20and%20Mantas%20Mazeika%20and%20Andy%20Zou%20and%20Dan%20Hendrycks&entry.1292438233=%20%20Evaluating%20AI%20agents%20within%20complex%2C%20interactive%20environments%20that%20mirror%0Areal-world%20challenges%20is%20critical%20for%20understanding%20their%20practical%0Acapabilities.%20While%20existing%20agent%20benchmarks%20effectively%20assess%20skills%20like%0Atool%20use%20or%20performance%20on%20structured%20tasks%2C%20they%20often%20do%20not%20fully%20capture%20an%0Aagent%27s%20ability%20to%20operate%20autonomously%20in%20exploratory%20environments%20that%20demand%0Asustained%2C%20self-directed%20reasoning%20over%20a%20long%20and%20growing%20context.%20To%20spur%20the%0Adevelopment%20of%20agents%20capable%20of%20more%20robust%20intrinsic%20reasoning%20over%20long%0Ahorizons%2C%20we%20introduce%20TextQuests%2C%20a%20benchmark%20based%20on%20the%20Infocom%20suite%20of%0Ainteractive%20fiction%20games.%20These%20text-based%20adventures%2C%20which%20can%20take%20human%0Aplayers%20over%2030%20hours%20and%20require%20hundreds%20of%20precise%20actions%20to%20solve%2C%20serve%0Aas%20an%20effective%20proxy%20for%20evaluating%20AI%20agents%20on%20focused%2C%20stateful%20tasks.%20The%0Abenchmark%20is%20specifically%20designed%20to%20assess%20an%20LLM%20agent%27s%20capacity%20for%0Aself-contained%20problem-solving%20by%20precluding%20the%20use%20of%20external%20tools%2C%20thereby%0Afocusing%20on%20intrinsic%20long-context%20reasoning%20capabilities%20in%20an%20exploratory%0Aenvironment%20characterized%20by%20the%20need%20for%20trial-and-error%20learning%20and%0Asustained%20problem-solving%20within%20a%20single%20interactive%20session.%20We%20release%0ATextQuests%20at%20https%3A//textquests.ai.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23701v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


