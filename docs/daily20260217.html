<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260216.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning", "author": "Guanting Ye and Qiyan Zhao and Wenhao Yu and Xiaofeng Zhang and Jianmin Ji and Yanyong Zhang and Ka-Veng Yuen", "abstract": "Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing. Specifically, applying 1D temporal positional indices disrupts the continuity of visual features along the column dimension, resulting in spatial locality loss. Moreover, RoPE follows the prior that temporally closer image tokens are more causally related, leading to long-term decay in attention allocation and causing the model to progressively neglect earlier visual tokens as the sequence length increases. To address these issues, we propose C^2RoPE, an improved RoPE that explicitly models local spatial Continuity and spatial Causal relationships for visual processing. C^2RoPE introduces a spatio-temporal continuous positional embedding mechanism for visual tokens. It first integrates 1D temporal positions with Cartesian-based spatial coordinates to construct a triplet hybrid positional index, and then employs a frequency allocation strategy to encode spatio-temporal positional information across the three index components. Additionally, we introduce Chebyshev Causal Masking, which determines causal dependencies by computing the Chebyshev distance of image tokens in 2D space. Evaluation results across various benchmarks, including 3D scene reasoning and 3D visual question answering, demonstrate C^2RoPE's effectiveness. The code is be available at https://github.com/ErikZ719/C2RoPE.", "link": "http://arxiv.org/abs/2602.10551v2", "date": "2026-02-16", "relevancy": 3.0122, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C%5E2ROPE%3A%20Causal%20Continuous%20Rotary%20Positional%20Encoding%20for%203D%20Large%20Multimodal-Models%20Reasoning&body=Title%3A%20C%5E2ROPE%3A%20Causal%20Continuous%20Rotary%20Positional%20Encoding%20for%203D%20Large%20Multimodal-Models%20Reasoning%0AAuthor%3A%20Guanting%20Ye%20and%20Qiyan%20Zhao%20and%20Wenhao%20Yu%20and%20Xiaofeng%20Zhang%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang%20and%20Ka-Veng%20Yuen%0AAbstract%3A%20Recent%20advances%20in%203D%20Large%20Multimodal%20Models%20%28LMMs%29%20built%20on%20Large%20Language%20Models%20%28LLMs%29%20have%20established%20the%20alignment%20of%203D%20visual%20features%20with%20LLM%20representations%20as%20the%20dominant%20paradigm.%20However%2C%20the%20inherited%20Rotary%20Position%20Embedding%20%28RoPE%29%20introduces%20limitations%20for%20multimodal%20processing.%20Specifically%2C%20applying%201D%20temporal%20positional%20indices%20disrupts%20the%20continuity%20of%20visual%20features%20along%20the%20column%20dimension%2C%20resulting%20in%20spatial%20locality%20loss.%20Moreover%2C%20RoPE%20follows%20the%20prior%20that%20temporally%20closer%20image%20tokens%20are%20more%20causally%20related%2C%20leading%20to%20long-term%20decay%20in%20attention%20allocation%20and%20causing%20the%20model%20to%20progressively%20neglect%20earlier%20visual%20tokens%20as%20the%20sequence%20length%20increases.%20To%20address%20these%20issues%2C%20we%20propose%20C%5E2RoPE%2C%20an%20improved%20RoPE%20that%20explicitly%20models%20local%20spatial%20Continuity%20and%20spatial%20Causal%20relationships%20for%20visual%20processing.%20C%5E2RoPE%20introduces%20a%20spatio-temporal%20continuous%20positional%20embedding%20mechanism%20for%20visual%20tokens.%20It%20first%20integrates%201D%20temporal%20positions%20with%20Cartesian-based%20spatial%20coordinates%20to%20construct%20a%20triplet%20hybrid%20positional%20index%2C%20and%20then%20employs%20a%20frequency%20allocation%20strategy%20to%20encode%20spatio-temporal%20positional%20information%20across%20the%20three%20index%20components.%20Additionally%2C%20we%20introduce%20Chebyshev%20Causal%20Masking%2C%20which%20determines%20causal%20dependencies%20by%20computing%20the%20Chebyshev%20distance%20of%20image%20tokens%20in%202D%20space.%20Evaluation%20results%20across%20various%20benchmarks%2C%20including%203D%20scene%20reasoning%20and%203D%20visual%20question%20answering%2C%20demonstrate%20C%5E2RoPE%27s%20effectiveness.%20The%20code%20is%20be%20available%20at%20https%3A//github.com/ErikZ719/C2RoPE.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC%255E2ROPE%253A%2520Causal%2520Continuous%2520Rotary%2520Positional%2520Encoding%2520for%25203D%2520Large%2520Multimodal-Models%2520Reasoning%26entry.906535625%3DGuanting%2520Ye%2520and%2520Qiyan%2520Zhao%2520and%2520Wenhao%2520Yu%2520and%2520Xiaofeng%2520Zhang%2520and%2520Jianmin%2520Ji%2520and%2520Yanyong%2520Zhang%2520and%2520Ka-Veng%2520Yuen%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520Large%2520Multimodal%2520Models%2520%2528LMMs%2529%2520built%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520established%2520the%2520alignment%2520of%25203D%2520visual%2520features%2520with%2520LLM%2520representations%2520as%2520the%2520dominant%2520paradigm.%2520However%252C%2520the%2520inherited%2520Rotary%2520Position%2520Embedding%2520%2528RoPE%2529%2520introduces%2520limitations%2520for%2520multimodal%2520processing.%2520Specifically%252C%2520applying%25201D%2520temporal%2520positional%2520indices%2520disrupts%2520the%2520continuity%2520of%2520visual%2520features%2520along%2520the%2520column%2520dimension%252C%2520resulting%2520in%2520spatial%2520locality%2520loss.%2520Moreover%252C%2520RoPE%2520follows%2520the%2520prior%2520that%2520temporally%2520closer%2520image%2520tokens%2520are%2520more%2520causally%2520related%252C%2520leading%2520to%2520long-term%2520decay%2520in%2520attention%2520allocation%2520and%2520causing%2520the%2520model%2520to%2520progressively%2520neglect%2520earlier%2520visual%2520tokens%2520as%2520the%2520sequence%2520length%2520increases.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520C%255E2RoPE%252C%2520an%2520improved%2520RoPE%2520that%2520explicitly%2520models%2520local%2520spatial%2520Continuity%2520and%2520spatial%2520Causal%2520relationships%2520for%2520visual%2520processing.%2520C%255E2RoPE%2520introduces%2520a%2520spatio-temporal%2520continuous%2520positional%2520embedding%2520mechanism%2520for%2520visual%2520tokens.%2520It%2520first%2520integrates%25201D%2520temporal%2520positions%2520with%2520Cartesian-based%2520spatial%2520coordinates%2520to%2520construct%2520a%2520triplet%2520hybrid%2520positional%2520index%252C%2520and%2520then%2520employs%2520a%2520frequency%2520allocation%2520strategy%2520to%2520encode%2520spatio-temporal%2520positional%2520information%2520across%2520the%2520three%2520index%2520components.%2520Additionally%252C%2520we%2520introduce%2520Chebyshev%2520Causal%2520Masking%252C%2520which%2520determines%2520causal%2520dependencies%2520by%2520computing%2520the%2520Chebyshev%2520distance%2520of%2520image%2520tokens%2520in%25202D%2520space.%2520Evaluation%2520results%2520across%2520various%2520benchmarks%252C%2520including%25203D%2520scene%2520reasoning%2520and%25203D%2520visual%2520question%2520answering%252C%2520demonstrate%2520C%255E2RoPE%2527s%2520effectiveness.%2520The%2520code%2520is%2520be%2520available%2520at%2520https%253A//github.com/ErikZ719/C2RoPE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C%5E2ROPE%3A%20Causal%20Continuous%20Rotary%20Positional%20Encoding%20for%203D%20Large%20Multimodal-Models%20Reasoning&entry.906535625=Guanting%20Ye%20and%20Qiyan%20Zhao%20and%20Wenhao%20Yu%20and%20Xiaofeng%20Zhang%20and%20Jianmin%20Ji%20and%20Yanyong%20Zhang%20and%20Ka-Veng%20Yuen&entry.1292438233=Recent%20advances%20in%203D%20Large%20Multimodal%20Models%20%28LMMs%29%20built%20on%20Large%20Language%20Models%20%28LLMs%29%20have%20established%20the%20alignment%20of%203D%20visual%20features%20with%20LLM%20representations%20as%20the%20dominant%20paradigm.%20However%2C%20the%20inherited%20Rotary%20Position%20Embedding%20%28RoPE%29%20introduces%20limitations%20for%20multimodal%20processing.%20Specifically%2C%20applying%201D%20temporal%20positional%20indices%20disrupts%20the%20continuity%20of%20visual%20features%20along%20the%20column%20dimension%2C%20resulting%20in%20spatial%20locality%20loss.%20Moreover%2C%20RoPE%20follows%20the%20prior%20that%20temporally%20closer%20image%20tokens%20are%20more%20causally%20related%2C%20leading%20to%20long-term%20decay%20in%20attention%20allocation%20and%20causing%20the%20model%20to%20progressively%20neglect%20earlier%20visual%20tokens%20as%20the%20sequence%20length%20increases.%20To%20address%20these%20issues%2C%20we%20propose%20C%5E2RoPE%2C%20an%20improved%20RoPE%20that%20explicitly%20models%20local%20spatial%20Continuity%20and%20spatial%20Causal%20relationships%20for%20visual%20processing.%20C%5E2RoPE%20introduces%20a%20spatio-temporal%20continuous%20positional%20embedding%20mechanism%20for%20visual%20tokens.%20It%20first%20integrates%201D%20temporal%20positions%20with%20Cartesian-based%20spatial%20coordinates%20to%20construct%20a%20triplet%20hybrid%20positional%20index%2C%20and%20then%20employs%20a%20frequency%20allocation%20strategy%20to%20encode%20spatio-temporal%20positional%20information%20across%20the%20three%20index%20components.%20Additionally%2C%20we%20introduce%20Chebyshev%20Causal%20Masking%2C%20which%20determines%20causal%20dependencies%20by%20computing%20the%20Chebyshev%20distance%20of%20image%20tokens%20in%202D%20space.%20Evaluation%20results%20across%20various%20benchmarks%2C%20including%203D%20scene%20reasoning%20and%203D%20visual%20question%20answering%2C%20demonstrate%20C%5E2RoPE%27s%20effectiveness.%20The%20code%20is%20be%20available%20at%20https%3A//github.com/ErikZ719/C2RoPE.&entry.1838667208=http%3A//arxiv.org/abs/2602.10551v2&entry.124074799=Read"},
{"title": "Are foundation models for computer vision good conformal predictors?", "author": "Leo Fillioux and Julio Silva-Rodr\u00edguez and Ismail Ben Ayed and Paul-Henry Courn\u00e8de and Maria Vakalopoulou and Stergios Christodoulidis and Jose Dolz", "abstract": "Recent advances in self-supervision and contrastive learning have brought the performance of foundation models to unprecedented levels in a variety of tasks. Fueled by this progress, these models are becoming the prevailing approach for a wide array of real-world vision problems, including risk-sensitive and high-stakes applications. However, ensuring safe deployment in these scenarios requires a more comprehensive understanding of their uncertainty modeling capabilities, which has received little attention. In this work, we delve into the behaviour of vision and vision-language foundation models under Conformal Prediction (CP), a statistical framework that provides theoretical guarantees of marginal coverage of the true class. Across extensive experiments including popular vision classification benchmarks, well-known foundation vision models, and three CP methods, our findings reveal that foundation models are well-suited for conformalization procedures, particularly those integrating Vision Transformers. We also show that calibrating the confidence predictions of these models, a popular strategy to improve their uncertainty quantification, actually leads to efficiency degradation of the conformal set on adaptive CP methods. Furthermore, few-shot adaptation of Vision-Language Models (VLMs) to downstream tasks, whose popularity is surging, enhances conformal scores compared to zero-shot predictions. Last, our empirical study exposes APS as particularly promising in the context of vision foundation models, as it does not violate the marginal coverage guarantees across multiple challenging, yet realistic scenarios.", "link": "http://arxiv.org/abs/2412.06082v3", "date": "2026-02-16", "relevancy": 2.8592, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5844}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20foundation%20models%20for%20computer%20vision%20good%20conformal%20predictors%3F&body=Title%3A%20Are%20foundation%20models%20for%20computer%20vision%20good%20conformal%20predictors%3F%0AAuthor%3A%20Leo%20Fillioux%20and%20Julio%20Silva-Rodr%C3%ADguez%20and%20Ismail%20Ben%20Ayed%20and%20Paul-Henry%20Courn%C3%A8de%20and%20Maria%20Vakalopoulou%20and%20Stergios%20Christodoulidis%20and%20Jose%20Dolz%0AAbstract%3A%20Recent%20advances%20in%20self-supervision%20and%20contrastive%20learning%20have%20brought%20the%20performance%20of%20foundation%20models%20to%20unprecedented%20levels%20in%20a%20variety%20of%20tasks.%20Fueled%20by%20this%20progress%2C%20these%20models%20are%20becoming%20the%20prevailing%20approach%20for%20a%20wide%20array%20of%20real-world%20vision%20problems%2C%20including%20risk-sensitive%20and%20high-stakes%20applications.%20However%2C%20ensuring%20safe%20deployment%20in%20these%20scenarios%20requires%20a%20more%20comprehensive%20understanding%20of%20their%20uncertainty%20modeling%20capabilities%2C%20which%20has%20received%20little%20attention.%20In%20this%20work%2C%20we%20delve%20into%20the%20behaviour%20of%20vision%20and%20vision-language%20foundation%20models%20under%20Conformal%20Prediction%20%28CP%29%2C%20a%20statistical%20framework%20that%20provides%20theoretical%20guarantees%20of%20marginal%20coverage%20of%20the%20true%20class.%20Across%20extensive%20experiments%20including%20popular%20vision%20classification%20benchmarks%2C%20well-known%20foundation%20vision%20models%2C%20and%20three%20CP%20methods%2C%20our%20findings%20reveal%20that%20foundation%20models%20are%20well-suited%20for%20conformalization%20procedures%2C%20particularly%20those%20integrating%20Vision%20Transformers.%20We%20also%20show%20that%20calibrating%20the%20confidence%20predictions%20of%20these%20models%2C%20a%20popular%20strategy%20to%20improve%20their%20uncertainty%20quantification%2C%20actually%20leads%20to%20efficiency%20degradation%20of%20the%20conformal%20set%20on%20adaptive%20CP%20methods.%20Furthermore%2C%20few-shot%20adaptation%20of%20Vision-Language%20Models%20%28VLMs%29%20to%20downstream%20tasks%2C%20whose%20popularity%20is%20surging%2C%20enhances%20conformal%20scores%20compared%20to%20zero-shot%20predictions.%20Last%2C%20our%20empirical%20study%20exposes%20APS%20as%20particularly%20promising%20in%20the%20context%20of%20vision%20foundation%20models%2C%20as%20it%20does%20not%20violate%20the%20marginal%20coverage%20guarantees%20across%20multiple%20challenging%2C%20yet%20realistic%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2412.06082v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520foundation%2520models%2520for%2520computer%2520vision%2520good%2520conformal%2520predictors%253F%26entry.906535625%3DLeo%2520Fillioux%2520and%2520Julio%2520Silva-Rodr%25C3%25ADguez%2520and%2520Ismail%2520Ben%2520Ayed%2520and%2520Paul-Henry%2520Courn%25C3%25A8de%2520and%2520Maria%2520Vakalopoulou%2520and%2520Stergios%2520Christodoulidis%2520and%2520Jose%2520Dolz%26entry.1292438233%3DRecent%2520advances%2520in%2520self-supervision%2520and%2520contrastive%2520learning%2520have%2520brought%2520the%2520performance%2520of%2520foundation%2520models%2520to%2520unprecedented%2520levels%2520in%2520a%2520variety%2520of%2520tasks.%2520Fueled%2520by%2520this%2520progress%252C%2520these%2520models%2520are%2520becoming%2520the%2520prevailing%2520approach%2520for%2520a%2520wide%2520array%2520of%2520real-world%2520vision%2520problems%252C%2520including%2520risk-sensitive%2520and%2520high-stakes%2520applications.%2520However%252C%2520ensuring%2520safe%2520deployment%2520in%2520these%2520scenarios%2520requires%2520a%2520more%2520comprehensive%2520understanding%2520of%2520their%2520uncertainty%2520modeling%2520capabilities%252C%2520which%2520has%2520received%2520little%2520attention.%2520In%2520this%2520work%252C%2520we%2520delve%2520into%2520the%2520behaviour%2520of%2520vision%2520and%2520vision-language%2520foundation%2520models%2520under%2520Conformal%2520Prediction%2520%2528CP%2529%252C%2520a%2520statistical%2520framework%2520that%2520provides%2520theoretical%2520guarantees%2520of%2520marginal%2520coverage%2520of%2520the%2520true%2520class.%2520Across%2520extensive%2520experiments%2520including%2520popular%2520vision%2520classification%2520benchmarks%252C%2520well-known%2520foundation%2520vision%2520models%252C%2520and%2520three%2520CP%2520methods%252C%2520our%2520findings%2520reveal%2520that%2520foundation%2520models%2520are%2520well-suited%2520for%2520conformalization%2520procedures%252C%2520particularly%2520those%2520integrating%2520Vision%2520Transformers.%2520We%2520also%2520show%2520that%2520calibrating%2520the%2520confidence%2520predictions%2520of%2520these%2520models%252C%2520a%2520popular%2520strategy%2520to%2520improve%2520their%2520uncertainty%2520quantification%252C%2520actually%2520leads%2520to%2520efficiency%2520degradation%2520of%2520the%2520conformal%2520set%2520on%2520adaptive%2520CP%2520methods.%2520Furthermore%252C%2520few-shot%2520adaptation%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%2520downstream%2520tasks%252C%2520whose%2520popularity%2520is%2520surging%252C%2520enhances%2520conformal%2520scores%2520compared%2520to%2520zero-shot%2520predictions.%2520Last%252C%2520our%2520empirical%2520study%2520exposes%2520APS%2520as%2520particularly%2520promising%2520in%2520the%2520context%2520of%2520vision%2520foundation%2520models%252C%2520as%2520it%2520does%2520not%2520violate%2520the%2520marginal%2520coverage%2520guarantees%2520across%2520multiple%2520challenging%252C%2520yet%2520realistic%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.06082v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20foundation%20models%20for%20computer%20vision%20good%20conformal%20predictors%3F&entry.906535625=Leo%20Fillioux%20and%20Julio%20Silva-Rodr%C3%ADguez%20and%20Ismail%20Ben%20Ayed%20and%20Paul-Henry%20Courn%C3%A8de%20and%20Maria%20Vakalopoulou%20and%20Stergios%20Christodoulidis%20and%20Jose%20Dolz&entry.1292438233=Recent%20advances%20in%20self-supervision%20and%20contrastive%20learning%20have%20brought%20the%20performance%20of%20foundation%20models%20to%20unprecedented%20levels%20in%20a%20variety%20of%20tasks.%20Fueled%20by%20this%20progress%2C%20these%20models%20are%20becoming%20the%20prevailing%20approach%20for%20a%20wide%20array%20of%20real-world%20vision%20problems%2C%20including%20risk-sensitive%20and%20high-stakes%20applications.%20However%2C%20ensuring%20safe%20deployment%20in%20these%20scenarios%20requires%20a%20more%20comprehensive%20understanding%20of%20their%20uncertainty%20modeling%20capabilities%2C%20which%20has%20received%20little%20attention.%20In%20this%20work%2C%20we%20delve%20into%20the%20behaviour%20of%20vision%20and%20vision-language%20foundation%20models%20under%20Conformal%20Prediction%20%28CP%29%2C%20a%20statistical%20framework%20that%20provides%20theoretical%20guarantees%20of%20marginal%20coverage%20of%20the%20true%20class.%20Across%20extensive%20experiments%20including%20popular%20vision%20classification%20benchmarks%2C%20well-known%20foundation%20vision%20models%2C%20and%20three%20CP%20methods%2C%20our%20findings%20reveal%20that%20foundation%20models%20are%20well-suited%20for%20conformalization%20procedures%2C%20particularly%20those%20integrating%20Vision%20Transformers.%20We%20also%20show%20that%20calibrating%20the%20confidence%20predictions%20of%20these%20models%2C%20a%20popular%20strategy%20to%20improve%20their%20uncertainty%20quantification%2C%20actually%20leads%20to%20efficiency%20degradation%20of%20the%20conformal%20set%20on%20adaptive%20CP%20methods.%20Furthermore%2C%20few-shot%20adaptation%20of%20Vision-Language%20Models%20%28VLMs%29%20to%20downstream%20tasks%2C%20whose%20popularity%20is%20surging%2C%20enhances%20conformal%20scores%20compared%20to%20zero-shot%20predictions.%20Last%2C%20our%20empirical%20study%20exposes%20APS%20as%20particularly%20promising%20in%20the%20context%20of%20vision%20foundation%20models%2C%20as%20it%20does%20not%20violate%20the%20marginal%20coverage%20guarantees%20across%20multiple%20challenging%2C%20yet%20realistic%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2412.06082v3&entry.124074799=Read"},
{"title": "ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery", "author": "Ayush Shrivastava and Kirtan Gangani and Laksh Jain and Mayank Goel and Nipun Batra", "abstract": "Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.", "link": "http://arxiv.org/abs/2602.14989v1", "date": "2026-02-16", "relevancy": 2.8442, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5891}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ThermEval%3A%20A%20Structured%20Benchmark%20for%20Evaluation%20of%20Vision-Language%20Models%20on%20Thermal%20Imagery&body=Title%3A%20ThermEval%3A%20A%20Structured%20Benchmark%20for%20Evaluation%20of%20Vision-Language%20Models%20on%20Thermal%20Imagery%0AAuthor%3A%20Ayush%20Shrivastava%20and%20Kirtan%20Gangani%20and%20Laksh%20Jain%20and%20Mayank%20Goel%20and%20Nipun%20Batra%0AAbstract%3A%20Vision%20language%20models%20%28VLMs%29%20achieve%20strong%20performance%20on%20RGB%20imagery%2C%20but%20they%20do%20not%20generalize%20to%20thermal%20images.%20Thermal%20sensing%20plays%20a%20critical%20role%20in%20settings%20where%20visible%20light%20fails%2C%20including%20nighttime%20surveillance%2C%20search%20and%20rescue%2C%20autonomous%20driving%2C%20and%20medical%20screening.%20Unlike%20RGB%20imagery%2C%20thermal%20images%20encode%20physical%20temperature%20rather%20than%20color%20or%20texture%2C%20requiring%20perceptual%20and%20reasoning%20capabilities%20that%20existing%20RGB-centric%20benchmarks%20do%20not%20evaluate.%20We%20introduce%20ThermEval-B%2C%20a%20structured%20benchmark%20of%20approximately%2055%2C000%20thermal%20visual%20question%20answering%20pairs%20designed%20to%20assess%20the%20foundational%20primitives%20required%20for%20thermal%20vision%20language%20understanding.%20ThermEval-B%20integrates%20public%20datasets%20with%20our%20newly%20collected%20ThermEval-D%2C%20the%20first%20dataset%20to%20provide%20dense%20per-pixel%20temperature%20maps%20with%20semantic%20body-part%20annotations%20across%20diverse%20indoor%20and%20outdoor%20environments.%20Evaluating%2025%20open-source%20and%20closed-source%20VLMs%2C%20we%20find%20that%20models%20consistently%20fail%20at%20temperature-grounded%20reasoning%2C%20degrade%20under%20colormap%20transformations%2C%20and%20default%20to%20language%20priors%20or%20fixed%20responses%2C%20with%20only%20marginal%20gains%20from%20prompting%20or%20supervised%20fine-tuning.%20These%20results%20demonstrate%20that%20thermal%20understanding%20requires%20dedicated%20evaluation%20beyond%20RGB-centric%20assumptions%2C%20positioning%20ThermEval%20as%20a%20benchmark%20to%20drive%20progress%20in%20thermal%20vision%20language%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThermEval%253A%2520A%2520Structured%2520Benchmark%2520for%2520Evaluation%2520of%2520Vision-Language%2520Models%2520on%2520Thermal%2520Imagery%26entry.906535625%3DAyush%2520Shrivastava%2520and%2520Kirtan%2520Gangani%2520and%2520Laksh%2520Jain%2520and%2520Mayank%2520Goel%2520and%2520Nipun%2520Batra%26entry.1292438233%3DVision%2520language%2520models%2520%2528VLMs%2529%2520achieve%2520strong%2520performance%2520on%2520RGB%2520imagery%252C%2520but%2520they%2520do%2520not%2520generalize%2520to%2520thermal%2520images.%2520Thermal%2520sensing%2520plays%2520a%2520critical%2520role%2520in%2520settings%2520where%2520visible%2520light%2520fails%252C%2520including%2520nighttime%2520surveillance%252C%2520search%2520and%2520rescue%252C%2520autonomous%2520driving%252C%2520and%2520medical%2520screening.%2520Unlike%2520RGB%2520imagery%252C%2520thermal%2520images%2520encode%2520physical%2520temperature%2520rather%2520than%2520color%2520or%2520texture%252C%2520requiring%2520perceptual%2520and%2520reasoning%2520capabilities%2520that%2520existing%2520RGB-centric%2520benchmarks%2520do%2520not%2520evaluate.%2520We%2520introduce%2520ThermEval-B%252C%2520a%2520structured%2520benchmark%2520of%2520approximately%252055%252C000%2520thermal%2520visual%2520question%2520answering%2520pairs%2520designed%2520to%2520assess%2520the%2520foundational%2520primitives%2520required%2520for%2520thermal%2520vision%2520language%2520understanding.%2520ThermEval-B%2520integrates%2520public%2520datasets%2520with%2520our%2520newly%2520collected%2520ThermEval-D%252C%2520the%2520first%2520dataset%2520to%2520provide%2520dense%2520per-pixel%2520temperature%2520maps%2520with%2520semantic%2520body-part%2520annotations%2520across%2520diverse%2520indoor%2520and%2520outdoor%2520environments.%2520Evaluating%252025%2520open-source%2520and%2520closed-source%2520VLMs%252C%2520we%2520find%2520that%2520models%2520consistently%2520fail%2520at%2520temperature-grounded%2520reasoning%252C%2520degrade%2520under%2520colormap%2520transformations%252C%2520and%2520default%2520to%2520language%2520priors%2520or%2520fixed%2520responses%252C%2520with%2520only%2520marginal%2520gains%2520from%2520prompting%2520or%2520supervised%2520fine-tuning.%2520These%2520results%2520demonstrate%2520that%2520thermal%2520understanding%2520requires%2520dedicated%2520evaluation%2520beyond%2520RGB-centric%2520assumptions%252C%2520positioning%2520ThermEval%2520as%2520a%2520benchmark%2520to%2520drive%2520progress%2520in%2520thermal%2520vision%2520language%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ThermEval%3A%20A%20Structured%20Benchmark%20for%20Evaluation%20of%20Vision-Language%20Models%20on%20Thermal%20Imagery&entry.906535625=Ayush%20Shrivastava%20and%20Kirtan%20Gangani%20and%20Laksh%20Jain%20and%20Mayank%20Goel%20and%20Nipun%20Batra&entry.1292438233=Vision%20language%20models%20%28VLMs%29%20achieve%20strong%20performance%20on%20RGB%20imagery%2C%20but%20they%20do%20not%20generalize%20to%20thermal%20images.%20Thermal%20sensing%20plays%20a%20critical%20role%20in%20settings%20where%20visible%20light%20fails%2C%20including%20nighttime%20surveillance%2C%20search%20and%20rescue%2C%20autonomous%20driving%2C%20and%20medical%20screening.%20Unlike%20RGB%20imagery%2C%20thermal%20images%20encode%20physical%20temperature%20rather%20than%20color%20or%20texture%2C%20requiring%20perceptual%20and%20reasoning%20capabilities%20that%20existing%20RGB-centric%20benchmarks%20do%20not%20evaluate.%20We%20introduce%20ThermEval-B%2C%20a%20structured%20benchmark%20of%20approximately%2055%2C000%20thermal%20visual%20question%20answering%20pairs%20designed%20to%20assess%20the%20foundational%20primitives%20required%20for%20thermal%20vision%20language%20understanding.%20ThermEval-B%20integrates%20public%20datasets%20with%20our%20newly%20collected%20ThermEval-D%2C%20the%20first%20dataset%20to%20provide%20dense%20per-pixel%20temperature%20maps%20with%20semantic%20body-part%20annotations%20across%20diverse%20indoor%20and%20outdoor%20environments.%20Evaluating%2025%20open-source%20and%20closed-source%20VLMs%2C%20we%20find%20that%20models%20consistently%20fail%20at%20temperature-grounded%20reasoning%2C%20degrade%20under%20colormap%20transformations%2C%20and%20default%20to%20language%20priors%20or%20fixed%20responses%2C%20with%20only%20marginal%20gains%20from%20prompting%20or%20supervised%20fine-tuning.%20These%20results%20demonstrate%20that%20thermal%20understanding%20requires%20dedicated%20evaluation%20beyond%20RGB-centric%20assumptions%2C%20positioning%20ThermEval%20as%20a%20benchmark%20to%20drive%20progress%20in%20thermal%20vision%20language%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2602.14989v1&entry.124074799=Read"},
{"title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception", "author": "Lai Wei and Liangbo He and Jun Lan and Lingzhong Dong and Yutong Cai and Siyuan Li and Huijia Zhu and Weiqiang Wang and Linghe Kong and Yue Wang and Zhuosheng Zhang and Weiran Huang", "abstract": "Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming.", "link": "http://arxiv.org/abs/2602.11858v2", "date": "2026-02-16", "relevancy": 2.8171, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5796}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zooming%20without%20Zooming%3A%20Region-to-Image%20Distillation%20for%20Fine-Grained%20Multimodal%20Perception&body=Title%3A%20Zooming%20without%20Zooming%3A%20Region-to-Image%20Distillation%20for%20Fine-Grained%20Multimodal%20Perception%0AAuthor%3A%20Lai%20Wei%20and%20Liangbo%20He%20and%20Jun%20Lan%20and%20Lingzhong%20Dong%20and%20Yutong%20Cai%20and%20Siyuan%20Li%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Linghe%20Kong%20and%20Yue%20Wang%20and%20Zhuosheng%20Zhang%20and%20Weiran%20Huang%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20broad%20visual%20understanding%20but%20still%20struggle%20with%20fine-grained%20perception%2C%20where%20decisive%20evidence%20is%20small%20and%20easily%20overwhelmed%20by%20global%20context.%20Recent%20%22Thinking-with-Images%22%20methods%20alleviate%20this%20by%20iteratively%20zooming%20in%20and%20out%20regions%20of%20interest%20during%20inference%2C%20but%20incur%20high%20latency%20due%20to%20repeated%20tool%20calls%20and%20visual%20re-encoding.%20To%20address%20this%2C%20we%20propose%20Region-to-Image%20Distillation%2C%20which%20transforms%20zooming%20from%20an%20inference-time%20tool%20into%20a%20training-time%20primitive%2C%20thereby%20internalizing%20the%20benefits%20of%20agentic%20zooming%20into%20a%20single%20forward%20pass%20of%20an%20MLLM.%20In%20particular%2C%20we%20first%20zoom%20in%20to%20micro-cropped%20regions%20to%20let%20strong%20teacher%20models%20generate%20high-quality%20VQA%20data%2C%20and%20then%20distill%20this%20region-grounded%20supervision%20back%20to%20the%20full%20image.%20After%20training%20on%20such%20data%2C%20the%20smaller%20student%20model%20improves%20%22single-glance%22%20fine-grained%20perception%20without%20tool%20use.%20To%20rigorously%20evaluate%20this%20capability%2C%20we%20further%20present%20ZoomBench%2C%20a%20hybrid-annotated%20benchmark%20of%20845%20VQA%20data%20spanning%20six%20fine-grained%20perceptual%20dimensions%2C%20together%20with%20a%20dual-view%20protocol%20that%20quantifies%20the%20global--regional%20%22zooming%20gap%22.%20Experiments%20show%20that%20our%20models%20achieve%20leading%20performance%20across%20multiple%20fine-grained%20perception%20benchmarks%2C%20and%20also%20improve%20general%20multimodal%20cognition%20on%20benchmarks%20such%20as%20visual%20reasoning%20and%20GUI%20agents.%20We%20further%20discuss%20when%20%22Thinking-with-Images%22%20is%20necessary%20versus%20when%20its%20gains%20can%20be%20distilled%20into%20a%20single%20forward%20pass.%20Our%20code%20is%20available%20at%20https%3A//github.com/inclusionAI/Zooming-without-Zooming.%0ALink%3A%20http%3A//arxiv.org/abs/2602.11858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZooming%2520without%2520Zooming%253A%2520Region-to-Image%2520Distillation%2520for%2520Fine-Grained%2520Multimodal%2520Perception%26entry.906535625%3DLai%2520Wei%2520and%2520Liangbo%2520He%2520and%2520Jun%2520Lan%2520and%2520Lingzhong%2520Dong%2520and%2520Yutong%2520Cai%2520and%2520Siyuan%2520Li%2520and%2520Huijia%2520Zhu%2520and%2520Weiqiang%2520Wang%2520and%2520Linghe%2520Kong%2520and%2520Yue%2520Wang%2520and%2520Zhuosheng%2520Zhang%2520and%2520Weiran%2520Huang%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520excel%2520at%2520broad%2520visual%2520understanding%2520but%2520still%2520struggle%2520with%2520fine-grained%2520perception%252C%2520where%2520decisive%2520evidence%2520is%2520small%2520and%2520easily%2520overwhelmed%2520by%2520global%2520context.%2520Recent%2520%2522Thinking-with-Images%2522%2520methods%2520alleviate%2520this%2520by%2520iteratively%2520zooming%2520in%2520and%2520out%2520regions%2520of%2520interest%2520during%2520inference%252C%2520but%2520incur%2520high%2520latency%2520due%2520to%2520repeated%2520tool%2520calls%2520and%2520visual%2520re-encoding.%2520To%2520address%2520this%252C%2520we%2520propose%2520Region-to-Image%2520Distillation%252C%2520which%2520transforms%2520zooming%2520from%2520an%2520inference-time%2520tool%2520into%2520a%2520training-time%2520primitive%252C%2520thereby%2520internalizing%2520the%2520benefits%2520of%2520agentic%2520zooming%2520into%2520a%2520single%2520forward%2520pass%2520of%2520an%2520MLLM.%2520In%2520particular%252C%2520we%2520first%2520zoom%2520in%2520to%2520micro-cropped%2520regions%2520to%2520let%2520strong%2520teacher%2520models%2520generate%2520high-quality%2520VQA%2520data%252C%2520and%2520then%2520distill%2520this%2520region-grounded%2520supervision%2520back%2520to%2520the%2520full%2520image.%2520After%2520training%2520on%2520such%2520data%252C%2520the%2520smaller%2520student%2520model%2520improves%2520%2522single-glance%2522%2520fine-grained%2520perception%2520without%2520tool%2520use.%2520To%2520rigorously%2520evaluate%2520this%2520capability%252C%2520we%2520further%2520present%2520ZoomBench%252C%2520a%2520hybrid-annotated%2520benchmark%2520of%2520845%2520VQA%2520data%2520spanning%2520six%2520fine-grained%2520perceptual%2520dimensions%252C%2520together%2520with%2520a%2520dual-view%2520protocol%2520that%2520quantifies%2520the%2520global--regional%2520%2522zooming%2520gap%2522.%2520Experiments%2520show%2520that%2520our%2520models%2520achieve%2520leading%2520performance%2520across%2520multiple%2520fine-grained%2520perception%2520benchmarks%252C%2520and%2520also%2520improve%2520general%2520multimodal%2520cognition%2520on%2520benchmarks%2520such%2520as%2520visual%2520reasoning%2520and%2520GUI%2520agents.%2520We%2520further%2520discuss%2520when%2520%2522Thinking-with-Images%2522%2520is%2520necessary%2520versus%2520when%2520its%2520gains%2520can%2520be%2520distilled%2520into%2520a%2520single%2520forward%2520pass.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/inclusionAI/Zooming-without-Zooming.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.11858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zooming%20without%20Zooming%3A%20Region-to-Image%20Distillation%20for%20Fine-Grained%20Multimodal%20Perception&entry.906535625=Lai%20Wei%20and%20Liangbo%20He%20and%20Jun%20Lan%20and%20Lingzhong%20Dong%20and%20Yutong%20Cai%20and%20Siyuan%20Li%20and%20Huijia%20Zhu%20and%20Weiqiang%20Wang%20and%20Linghe%20Kong%20and%20Yue%20Wang%20and%20Zhuosheng%20Zhang%20and%20Weiran%20Huang&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20excel%20at%20broad%20visual%20understanding%20but%20still%20struggle%20with%20fine-grained%20perception%2C%20where%20decisive%20evidence%20is%20small%20and%20easily%20overwhelmed%20by%20global%20context.%20Recent%20%22Thinking-with-Images%22%20methods%20alleviate%20this%20by%20iteratively%20zooming%20in%20and%20out%20regions%20of%20interest%20during%20inference%2C%20but%20incur%20high%20latency%20due%20to%20repeated%20tool%20calls%20and%20visual%20re-encoding.%20To%20address%20this%2C%20we%20propose%20Region-to-Image%20Distillation%2C%20which%20transforms%20zooming%20from%20an%20inference-time%20tool%20into%20a%20training-time%20primitive%2C%20thereby%20internalizing%20the%20benefits%20of%20agentic%20zooming%20into%20a%20single%20forward%20pass%20of%20an%20MLLM.%20In%20particular%2C%20we%20first%20zoom%20in%20to%20micro-cropped%20regions%20to%20let%20strong%20teacher%20models%20generate%20high-quality%20VQA%20data%2C%20and%20then%20distill%20this%20region-grounded%20supervision%20back%20to%20the%20full%20image.%20After%20training%20on%20such%20data%2C%20the%20smaller%20student%20model%20improves%20%22single-glance%22%20fine-grained%20perception%20without%20tool%20use.%20To%20rigorously%20evaluate%20this%20capability%2C%20we%20further%20present%20ZoomBench%2C%20a%20hybrid-annotated%20benchmark%20of%20845%20VQA%20data%20spanning%20six%20fine-grained%20perceptual%20dimensions%2C%20together%20with%20a%20dual-view%20protocol%20that%20quantifies%20the%20global--regional%20%22zooming%20gap%22.%20Experiments%20show%20that%20our%20models%20achieve%20leading%20performance%20across%20multiple%20fine-grained%20perception%20benchmarks%2C%20and%20also%20improve%20general%20multimodal%20cognition%20on%20benchmarks%20such%20as%20visual%20reasoning%20and%20GUI%20agents.%20We%20further%20discuss%20when%20%22Thinking-with-Images%22%20is%20necessary%20versus%20when%20its%20gains%20can%20be%20distilled%20into%20a%20single%20forward%20pass.%20Our%20code%20is%20available%20at%20https%3A//github.com/inclusionAI/Zooming-without-Zooming.&entry.1838667208=http%3A//arxiv.org/abs/2602.11858v2&entry.124074799=Read"},
{"title": "Measure Twice, Cut Once: A Semantic-Oriented Approach to Video Temporal Localization with Video LLMs", "author": "Zongshang Pang and Mayu Otani and Yuta Nakashima", "abstract": "Temporally localizing user-queried events through natural language is a crucial capability for video models. Recent methods predominantly adapt video LLMs to generate event boundary timestamps for temporal localization tasks, which struggle to leverage LLMs' pre-trained semantic understanding capabilities due to the uninformative nature of timestamp outputs. In this work, we explore a timestamp-free, semantic-oriented framework that fine-tunes video LLMs using two generative learning tasks and one discriminative learning task. We first introduce a structural token generation task that enables the video LLM to recognize the temporal structure of input videos based on the input query. Through this task, the video LLM generates a sequence of special tokens, called structural tokens, which partition the video into consecutive segments and categorize them as either target events or background transitions. To enhance precise recognition of event segments, we further propose a query-focused captioning task that enables the video LLM to extract fine-grained event semantics that can be effectively utilized by the structural tokens. Finally, we introduce a structural token grounding module driven by contrastive learning to associate each structural token with its corresponding video segment, achieving holistic temporal segmentation of the input video and readily yielding the target event segments for localization. Extensive experiments across diverse temporal localization tasks demonstrate that our proposed framework, MeCo, consistently outperforms methods relying on boundary timestamp generation, highlighting the potential of a semantic-driven approach for temporal localization with video LLMs \\footnote{Code available at https://github.com/pangzss/MeCo.", "link": "http://arxiv.org/abs/2503.09027v2", "date": "2026-02-16", "relevancy": 2.7297, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5574}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5553}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measure%20Twice%2C%20Cut%20Once%3A%20A%20Semantic-Oriented%20Approach%20to%20Video%20Temporal%20Localization%20with%20Video%20LLMs&body=Title%3A%20Measure%20Twice%2C%20Cut%20Once%3A%20A%20Semantic-Oriented%20Approach%20to%20Video%20Temporal%20Localization%20with%20Video%20LLMs%0AAuthor%3A%20Zongshang%20Pang%20and%20Mayu%20Otani%20and%20Yuta%20Nakashima%0AAbstract%3A%20Temporally%20localizing%20user-queried%20events%20through%20natural%20language%20is%20a%20crucial%20capability%20for%20video%20models.%20Recent%20methods%20predominantly%20adapt%20video%20LLMs%20to%20generate%20event%20boundary%20timestamps%20for%20temporal%20localization%20tasks%2C%20which%20struggle%20to%20leverage%20LLMs%27%20pre-trained%20semantic%20understanding%20capabilities%20due%20to%20the%20uninformative%20nature%20of%20timestamp%20outputs.%20In%20this%20work%2C%20we%20explore%20a%20timestamp-free%2C%20semantic-oriented%20framework%20that%20fine-tunes%20video%20LLMs%20using%20two%20generative%20learning%20tasks%20and%20one%20discriminative%20learning%20task.%20We%20first%20introduce%20a%20structural%20token%20generation%20task%20that%20enables%20the%20video%20LLM%20to%20recognize%20the%20temporal%20structure%20of%20input%20videos%20based%20on%20the%20input%20query.%20Through%20this%20task%2C%20the%20video%20LLM%20generates%20a%20sequence%20of%20special%20tokens%2C%20called%20structural%20tokens%2C%20which%20partition%20the%20video%20into%20consecutive%20segments%20and%20categorize%20them%20as%20either%20target%20events%20or%20background%20transitions.%20To%20enhance%20precise%20recognition%20of%20event%20segments%2C%20we%20further%20propose%20a%20query-focused%20captioning%20task%20that%20enables%20the%20video%20LLM%20to%20extract%20fine-grained%20event%20semantics%20that%20can%20be%20effectively%20utilized%20by%20the%20structural%20tokens.%20Finally%2C%20we%20introduce%20a%20structural%20token%20grounding%20module%20driven%20by%20contrastive%20learning%20to%20associate%20each%20structural%20token%20with%20its%20corresponding%20video%20segment%2C%20achieving%20holistic%20temporal%20segmentation%20of%20the%20input%20video%20and%20readily%20yielding%20the%20target%20event%20segments%20for%20localization.%20Extensive%20experiments%20across%20diverse%20temporal%20localization%20tasks%20demonstrate%20that%20our%20proposed%20framework%2C%20MeCo%2C%20consistently%20outperforms%20methods%20relying%20on%20boundary%20timestamp%20generation%2C%20highlighting%20the%20potential%20of%20a%20semantic-driven%20approach%20for%20temporal%20localization%20with%20video%20LLMs%20%5Cfootnote%7BCode%20available%20at%20https%3A//github.com/pangzss/MeCo.%0ALink%3A%20http%3A//arxiv.org/abs/2503.09027v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasure%2520Twice%252C%2520Cut%2520Once%253A%2520A%2520Semantic-Oriented%2520Approach%2520to%2520Video%2520Temporal%2520Localization%2520with%2520Video%2520LLMs%26entry.906535625%3DZongshang%2520Pang%2520and%2520Mayu%2520Otani%2520and%2520Yuta%2520Nakashima%26entry.1292438233%3DTemporally%2520localizing%2520user-queried%2520events%2520through%2520natural%2520language%2520is%2520a%2520crucial%2520capability%2520for%2520video%2520models.%2520Recent%2520methods%2520predominantly%2520adapt%2520video%2520LLMs%2520to%2520generate%2520event%2520boundary%2520timestamps%2520for%2520temporal%2520localization%2520tasks%252C%2520which%2520struggle%2520to%2520leverage%2520LLMs%2527%2520pre-trained%2520semantic%2520understanding%2520capabilities%2520due%2520to%2520the%2520uninformative%2520nature%2520of%2520timestamp%2520outputs.%2520In%2520this%2520work%252C%2520we%2520explore%2520a%2520timestamp-free%252C%2520semantic-oriented%2520framework%2520that%2520fine-tunes%2520video%2520LLMs%2520using%2520two%2520generative%2520learning%2520tasks%2520and%2520one%2520discriminative%2520learning%2520task.%2520We%2520first%2520introduce%2520a%2520structural%2520token%2520generation%2520task%2520that%2520enables%2520the%2520video%2520LLM%2520to%2520recognize%2520the%2520temporal%2520structure%2520of%2520input%2520videos%2520based%2520on%2520the%2520input%2520query.%2520Through%2520this%2520task%252C%2520the%2520video%2520LLM%2520generates%2520a%2520sequence%2520of%2520special%2520tokens%252C%2520called%2520structural%2520tokens%252C%2520which%2520partition%2520the%2520video%2520into%2520consecutive%2520segments%2520and%2520categorize%2520them%2520as%2520either%2520target%2520events%2520or%2520background%2520transitions.%2520To%2520enhance%2520precise%2520recognition%2520of%2520event%2520segments%252C%2520we%2520further%2520propose%2520a%2520query-focused%2520captioning%2520task%2520that%2520enables%2520the%2520video%2520LLM%2520to%2520extract%2520fine-grained%2520event%2520semantics%2520that%2520can%2520be%2520effectively%2520utilized%2520by%2520the%2520structural%2520tokens.%2520Finally%252C%2520we%2520introduce%2520a%2520structural%2520token%2520grounding%2520module%2520driven%2520by%2520contrastive%2520learning%2520to%2520associate%2520each%2520structural%2520token%2520with%2520its%2520corresponding%2520video%2520segment%252C%2520achieving%2520holistic%2520temporal%2520segmentation%2520of%2520the%2520input%2520video%2520and%2520readily%2520yielding%2520the%2520target%2520event%2520segments%2520for%2520localization.%2520Extensive%2520experiments%2520across%2520diverse%2520temporal%2520localization%2520tasks%2520demonstrate%2520that%2520our%2520proposed%2520framework%252C%2520MeCo%252C%2520consistently%2520outperforms%2520methods%2520relying%2520on%2520boundary%2520timestamp%2520generation%252C%2520highlighting%2520the%2520potential%2520of%2520a%2520semantic-driven%2520approach%2520for%2520temporal%2520localization%2520with%2520video%2520LLMs%2520%255Cfootnote%257BCode%2520available%2520at%2520https%253A//github.com/pangzss/MeCo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09027v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measure%20Twice%2C%20Cut%20Once%3A%20A%20Semantic-Oriented%20Approach%20to%20Video%20Temporal%20Localization%20with%20Video%20LLMs&entry.906535625=Zongshang%20Pang%20and%20Mayu%20Otani%20and%20Yuta%20Nakashima&entry.1292438233=Temporally%20localizing%20user-queried%20events%20through%20natural%20language%20is%20a%20crucial%20capability%20for%20video%20models.%20Recent%20methods%20predominantly%20adapt%20video%20LLMs%20to%20generate%20event%20boundary%20timestamps%20for%20temporal%20localization%20tasks%2C%20which%20struggle%20to%20leverage%20LLMs%27%20pre-trained%20semantic%20understanding%20capabilities%20due%20to%20the%20uninformative%20nature%20of%20timestamp%20outputs.%20In%20this%20work%2C%20we%20explore%20a%20timestamp-free%2C%20semantic-oriented%20framework%20that%20fine-tunes%20video%20LLMs%20using%20two%20generative%20learning%20tasks%20and%20one%20discriminative%20learning%20task.%20We%20first%20introduce%20a%20structural%20token%20generation%20task%20that%20enables%20the%20video%20LLM%20to%20recognize%20the%20temporal%20structure%20of%20input%20videos%20based%20on%20the%20input%20query.%20Through%20this%20task%2C%20the%20video%20LLM%20generates%20a%20sequence%20of%20special%20tokens%2C%20called%20structural%20tokens%2C%20which%20partition%20the%20video%20into%20consecutive%20segments%20and%20categorize%20them%20as%20either%20target%20events%20or%20background%20transitions.%20To%20enhance%20precise%20recognition%20of%20event%20segments%2C%20we%20further%20propose%20a%20query-focused%20captioning%20task%20that%20enables%20the%20video%20LLM%20to%20extract%20fine-grained%20event%20semantics%20that%20can%20be%20effectively%20utilized%20by%20the%20structural%20tokens.%20Finally%2C%20we%20introduce%20a%20structural%20token%20grounding%20module%20driven%20by%20contrastive%20learning%20to%20associate%20each%20structural%20token%20with%20its%20corresponding%20video%20segment%2C%20achieving%20holistic%20temporal%20segmentation%20of%20the%20input%20video%20and%20readily%20yielding%20the%20target%20event%20segments%20for%20localization.%20Extensive%20experiments%20across%20diverse%20temporal%20localization%20tasks%20demonstrate%20that%20our%20proposed%20framework%2C%20MeCo%2C%20consistently%20outperforms%20methods%20relying%20on%20boundary%20timestamp%20generation%2C%20highlighting%20the%20potential%20of%20a%20semantic-driven%20approach%20for%20temporal%20localization%20with%20video%20LLMs%20%5Cfootnote%7BCode%20available%20at%20https%3A//github.com/pangzss/MeCo.&entry.1838667208=http%3A//arxiv.org/abs/2503.09027v2&entry.124074799=Read"},
{"title": "BHyGNN+: Unsupervised Representation Learning for Heterophilic Hypergraphs", "author": "Tianyi Ma and Yiyue Qian and Zehong Wang and Zheyuan Zhang and Chuxu Zhang and Yanfang Ye", "abstract": "Hypergraph Neural Networks (HyGNNs) have demonstrated remarkable success in modeling higher-order relationships among entities. However, their performance often degrades on heterophilic hypergraphs, where nodes connected by the same hyperedge tend to have dissimilar semantic representations or belong to different classes. While several HyGNNs, including our prior work BHyGNN, have been proposed to address heterophily, their reliance on labeled data significantly limits their applicability in real-world scenarios where annotations are scarce or costly. To overcome this limitation, we introduce BHyGNN+, a self-supervised learning framework that extends BHyGNN for representation learning on heterophilic hypergraphs without requiring ground-truth labels. The core idea of BHyGNN+ is hypergraph duality, a structural transformation where the roles of nodes and hyperedges are interchanged. By contrasting augmented views of a hypergraph against its dual using cosine similarity, our framework captures essential structural patterns in a fully unsupervised manner. Notably, this duality-based formulation eliminates the need for negative samples, a common requirement in existing hypergraph contrastive learning methods that is often difficult to satisfy in practice. Extensive experiments on eleven benchmark datasets demonstrate that BHyGNN+ consistently outperforms state-of-the-art supervised and self-supervised baselines on both heterophilic and homophilic hypergraphs. Our results validate the effectiveness of leveraging hypergraph duality for self-supervised learning and establish a new paradigm for representation learning on challenging, unlabeled hypergraphs.", "link": "http://arxiv.org/abs/2602.14919v1", "date": "2026-02-16", "relevancy": 2.6688, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.563}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5213}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BHyGNN%2B%3A%20Unsupervised%20Representation%20Learning%20for%20Heterophilic%20Hypergraphs&body=Title%3A%20BHyGNN%2B%3A%20Unsupervised%20Representation%20Learning%20for%20Heterophilic%20Hypergraphs%0AAuthor%3A%20Tianyi%20Ma%20and%20Yiyue%20Qian%20and%20Zehong%20Wang%20and%20Zheyuan%20Zhang%20and%20Chuxu%20Zhang%20and%20Yanfang%20Ye%0AAbstract%3A%20Hypergraph%20Neural%20Networks%20%28HyGNNs%29%20have%20demonstrated%20remarkable%20success%20in%20modeling%20higher-order%20relationships%20among%20entities.%20However%2C%20their%20performance%20often%20degrades%20on%20heterophilic%20hypergraphs%2C%20where%20nodes%20connected%20by%20the%20same%20hyperedge%20tend%20to%20have%20dissimilar%20semantic%20representations%20or%20belong%20to%20different%20classes.%20While%20several%20HyGNNs%2C%20including%20our%20prior%20work%20BHyGNN%2C%20have%20been%20proposed%20to%20address%20heterophily%2C%20their%20reliance%20on%20labeled%20data%20significantly%20limits%20their%20applicability%20in%20real-world%20scenarios%20where%20annotations%20are%20scarce%20or%20costly.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20BHyGNN%2B%2C%20a%20self-supervised%20learning%20framework%20that%20extends%20BHyGNN%20for%20representation%20learning%20on%20heterophilic%20hypergraphs%20without%20requiring%20ground-truth%20labels.%20The%20core%20idea%20of%20BHyGNN%2B%20is%20hypergraph%20duality%2C%20a%20structural%20transformation%20where%20the%20roles%20of%20nodes%20and%20hyperedges%20are%20interchanged.%20By%20contrasting%20augmented%20views%20of%20a%20hypergraph%20against%20its%20dual%20using%20cosine%20similarity%2C%20our%20framework%20captures%20essential%20structural%20patterns%20in%20a%20fully%20unsupervised%20manner.%20Notably%2C%20this%20duality-based%20formulation%20eliminates%20the%20need%20for%20negative%20samples%2C%20a%20common%20requirement%20in%20existing%20hypergraph%20contrastive%20learning%20methods%20that%20is%20often%20difficult%20to%20satisfy%20in%20practice.%20Extensive%20experiments%20on%20eleven%20benchmark%20datasets%20demonstrate%20that%20BHyGNN%2B%20consistently%20outperforms%20state-of-the-art%20supervised%20and%20self-supervised%20baselines%20on%20both%20heterophilic%20and%20homophilic%20hypergraphs.%20Our%20results%20validate%20the%20effectiveness%20of%20leveraging%20hypergraph%20duality%20for%20self-supervised%20learning%20and%20establish%20a%20new%20paradigm%20for%20representation%20learning%20on%20challenging%2C%20unlabeled%20hypergraphs.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBHyGNN%252B%253A%2520Unsupervised%2520Representation%2520Learning%2520for%2520Heterophilic%2520Hypergraphs%26entry.906535625%3DTianyi%2520Ma%2520and%2520Yiyue%2520Qian%2520and%2520Zehong%2520Wang%2520and%2520Zheyuan%2520Zhang%2520and%2520Chuxu%2520Zhang%2520and%2520Yanfang%2520Ye%26entry.1292438233%3DHypergraph%2520Neural%2520Networks%2520%2528HyGNNs%2529%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520modeling%2520higher-order%2520relationships%2520among%2520entities.%2520However%252C%2520their%2520performance%2520often%2520degrades%2520on%2520heterophilic%2520hypergraphs%252C%2520where%2520nodes%2520connected%2520by%2520the%2520same%2520hyperedge%2520tend%2520to%2520have%2520dissimilar%2520semantic%2520representations%2520or%2520belong%2520to%2520different%2520classes.%2520While%2520several%2520HyGNNs%252C%2520including%2520our%2520prior%2520work%2520BHyGNN%252C%2520have%2520been%2520proposed%2520to%2520address%2520heterophily%252C%2520their%2520reliance%2520on%2520labeled%2520data%2520significantly%2520limits%2520their%2520applicability%2520in%2520real-world%2520scenarios%2520where%2520annotations%2520are%2520scarce%2520or%2520costly.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520BHyGNN%252B%252C%2520a%2520self-supervised%2520learning%2520framework%2520that%2520extends%2520BHyGNN%2520for%2520representation%2520learning%2520on%2520heterophilic%2520hypergraphs%2520without%2520requiring%2520ground-truth%2520labels.%2520The%2520core%2520idea%2520of%2520BHyGNN%252B%2520is%2520hypergraph%2520duality%252C%2520a%2520structural%2520transformation%2520where%2520the%2520roles%2520of%2520nodes%2520and%2520hyperedges%2520are%2520interchanged.%2520By%2520contrasting%2520augmented%2520views%2520of%2520a%2520hypergraph%2520against%2520its%2520dual%2520using%2520cosine%2520similarity%252C%2520our%2520framework%2520captures%2520essential%2520structural%2520patterns%2520in%2520a%2520fully%2520unsupervised%2520manner.%2520Notably%252C%2520this%2520duality-based%2520formulation%2520eliminates%2520the%2520need%2520for%2520negative%2520samples%252C%2520a%2520common%2520requirement%2520in%2520existing%2520hypergraph%2520contrastive%2520learning%2520methods%2520that%2520is%2520often%2520difficult%2520to%2520satisfy%2520in%2520practice.%2520Extensive%2520experiments%2520on%2520eleven%2520benchmark%2520datasets%2520demonstrate%2520that%2520BHyGNN%252B%2520consistently%2520outperforms%2520state-of-the-art%2520supervised%2520and%2520self-supervised%2520baselines%2520on%2520both%2520heterophilic%2520and%2520homophilic%2520hypergraphs.%2520Our%2520results%2520validate%2520the%2520effectiveness%2520of%2520leveraging%2520hypergraph%2520duality%2520for%2520self-supervised%2520learning%2520and%2520establish%2520a%2520new%2520paradigm%2520for%2520representation%2520learning%2520on%2520challenging%252C%2520unlabeled%2520hypergraphs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BHyGNN%2B%3A%20Unsupervised%20Representation%20Learning%20for%20Heterophilic%20Hypergraphs&entry.906535625=Tianyi%20Ma%20and%20Yiyue%20Qian%20and%20Zehong%20Wang%20and%20Zheyuan%20Zhang%20and%20Chuxu%20Zhang%20and%20Yanfang%20Ye&entry.1292438233=Hypergraph%20Neural%20Networks%20%28HyGNNs%29%20have%20demonstrated%20remarkable%20success%20in%20modeling%20higher-order%20relationships%20among%20entities.%20However%2C%20their%20performance%20often%20degrades%20on%20heterophilic%20hypergraphs%2C%20where%20nodes%20connected%20by%20the%20same%20hyperedge%20tend%20to%20have%20dissimilar%20semantic%20representations%20or%20belong%20to%20different%20classes.%20While%20several%20HyGNNs%2C%20including%20our%20prior%20work%20BHyGNN%2C%20have%20been%20proposed%20to%20address%20heterophily%2C%20their%20reliance%20on%20labeled%20data%20significantly%20limits%20their%20applicability%20in%20real-world%20scenarios%20where%20annotations%20are%20scarce%20or%20costly.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20BHyGNN%2B%2C%20a%20self-supervised%20learning%20framework%20that%20extends%20BHyGNN%20for%20representation%20learning%20on%20heterophilic%20hypergraphs%20without%20requiring%20ground-truth%20labels.%20The%20core%20idea%20of%20BHyGNN%2B%20is%20hypergraph%20duality%2C%20a%20structural%20transformation%20where%20the%20roles%20of%20nodes%20and%20hyperedges%20are%20interchanged.%20By%20contrasting%20augmented%20views%20of%20a%20hypergraph%20against%20its%20dual%20using%20cosine%20similarity%2C%20our%20framework%20captures%20essential%20structural%20patterns%20in%20a%20fully%20unsupervised%20manner.%20Notably%2C%20this%20duality-based%20formulation%20eliminates%20the%20need%20for%20negative%20samples%2C%20a%20common%20requirement%20in%20existing%20hypergraph%20contrastive%20learning%20methods%20that%20is%20often%20difficult%20to%20satisfy%20in%20practice.%20Extensive%20experiments%20on%20eleven%20benchmark%20datasets%20demonstrate%20that%20BHyGNN%2B%20consistently%20outperforms%20state-of-the-art%20supervised%20and%20self-supervised%20baselines%20on%20both%20heterophilic%20and%20homophilic%20hypergraphs.%20Our%20results%20validate%20the%20effectiveness%20of%20leveraging%20hypergraph%20duality%20for%20self-supervised%20learning%20and%20establish%20a%20new%20paradigm%20for%20representation%20learning%20on%20challenging%2C%20unlabeled%20hypergraphs.&entry.1838667208=http%3A//arxiv.org/abs/2602.14919v1&entry.124074799=Read"},
{"title": "Mitigating Pretraining-Induced Attention Asymmetry in 2D+ Electron Microscopy Image Segmentation", "author": "Zs\u00f3fia Moln\u00e1r and Gergely Szab\u00f3 and Andr\u00e1s Horv\u00e1th", "abstract": "Vision models pretrained on large-scale RGB natural image datasets are widely reused for electron microscopy image segmentation. In electron microscopy, volumetric data are acquired as serial sections and processed as stacks of adjacent grayscale slices, where neighboring slices provide symmetric contextual information for identifying features on the central slice. The common strategy maps such stacks to pseudo-RGB inputs to enable transfer learning from pretrained models. However, this mapping imposes channel-specific semantics inherited from natural images, even though electron microscopy slices are homogeneous in the modality and symmetric in their predictive roles. As a result, pretrained models may encode inductive biases that are misaligned with the inherent symmetry of volumetric electron microscopy data. In this work, it is demonstrated that RGB-pretrained models systematically assign unequal importance to individual input slices when applied to stacked electron microscopy data, despite the absence of any intrinsic channel ordering. Using saliency-based attribution analysis across multiple architectures, a consistent channel-level asymmetry was observed that persists after fine-tuning and affects model interpretability, even when segmentation performance is unchanged. To address this issue, a targeted modification of pretraining weights based on uniform channel initialization was proposed, which restores symmetric feature attribution while preserving the benefits of pretraining. Experiments on the SNEMI, Lucchi and GF-PA66 datasets confirm a substantial reduction in attribution bias without compromising or even improving segmentation accuracy.", "link": "http://arxiv.org/abs/2505.14105v3", "date": "2026-02-16", "relevancy": 2.6347, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.548}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Pretraining-Induced%20Attention%20Asymmetry%20in%202D%2B%20Electron%20Microscopy%20Image%20Segmentation&body=Title%3A%20Mitigating%20Pretraining-Induced%20Attention%20Asymmetry%20in%202D%2B%20Electron%20Microscopy%20Image%20Segmentation%0AAuthor%3A%20Zs%C3%B3fia%20Moln%C3%A1r%20and%20Gergely%20Szab%C3%B3%20and%20Andr%C3%A1s%20Horv%C3%A1th%0AAbstract%3A%20Vision%20models%20pretrained%20on%20large-scale%20RGB%20natural%20image%20datasets%20are%20widely%20reused%20for%20electron%20microscopy%20image%20segmentation.%20In%20electron%20microscopy%2C%20volumetric%20data%20are%20acquired%20as%20serial%20sections%20and%20processed%20as%20stacks%20of%20adjacent%20grayscale%20slices%2C%20where%20neighboring%20slices%20provide%20symmetric%20contextual%20information%20for%20identifying%20features%20on%20the%20central%20slice.%20The%20common%20strategy%20maps%20such%20stacks%20to%20pseudo-RGB%20inputs%20to%20enable%20transfer%20learning%20from%20pretrained%20models.%20However%2C%20this%20mapping%20imposes%20channel-specific%20semantics%20inherited%20from%20natural%20images%2C%20even%20though%20electron%20microscopy%20slices%20are%20homogeneous%20in%20the%20modality%20and%20symmetric%20in%20their%20predictive%20roles.%20As%20a%20result%2C%20pretrained%20models%20may%20encode%20inductive%20biases%20that%20are%20misaligned%20with%20the%20inherent%20symmetry%20of%20volumetric%20electron%20microscopy%20data.%20In%20this%20work%2C%20it%20is%20demonstrated%20that%20RGB-pretrained%20models%20systematically%20assign%20unequal%20importance%20to%20individual%20input%20slices%20when%20applied%20to%20stacked%20electron%20microscopy%20data%2C%20despite%20the%20absence%20of%20any%20intrinsic%20channel%20ordering.%20Using%20saliency-based%20attribution%20analysis%20across%20multiple%20architectures%2C%20a%20consistent%20channel-level%20asymmetry%20was%20observed%20that%20persists%20after%20fine-tuning%20and%20affects%20model%20interpretability%2C%20even%20when%20segmentation%20performance%20is%20unchanged.%20To%20address%20this%20issue%2C%20a%20targeted%20modification%20of%20pretraining%20weights%20based%20on%20uniform%20channel%20initialization%20was%20proposed%2C%20which%20restores%20symmetric%20feature%20attribution%20while%20preserving%20the%20benefits%20of%20pretraining.%20Experiments%20on%20the%20SNEMI%2C%20Lucchi%20and%20GF-PA66%20datasets%20confirm%20a%20substantial%20reduction%20in%20attribution%20bias%20without%20compromising%20or%20even%20improving%20segmentation%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2505.14105v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Pretraining-Induced%2520Attention%2520Asymmetry%2520in%25202D%252B%2520Electron%2520Microscopy%2520Image%2520Segmentation%26entry.906535625%3DZs%25C3%25B3fia%2520Moln%25C3%25A1r%2520and%2520Gergely%2520Szab%25C3%25B3%2520and%2520Andr%25C3%25A1s%2520Horv%25C3%25A1th%26entry.1292438233%3DVision%2520models%2520pretrained%2520on%2520large-scale%2520RGB%2520natural%2520image%2520datasets%2520are%2520widely%2520reused%2520for%2520electron%2520microscopy%2520image%2520segmentation.%2520In%2520electron%2520microscopy%252C%2520volumetric%2520data%2520are%2520acquired%2520as%2520serial%2520sections%2520and%2520processed%2520as%2520stacks%2520of%2520adjacent%2520grayscale%2520slices%252C%2520where%2520neighboring%2520slices%2520provide%2520symmetric%2520contextual%2520information%2520for%2520identifying%2520features%2520on%2520the%2520central%2520slice.%2520The%2520common%2520strategy%2520maps%2520such%2520stacks%2520to%2520pseudo-RGB%2520inputs%2520to%2520enable%2520transfer%2520learning%2520from%2520pretrained%2520models.%2520However%252C%2520this%2520mapping%2520imposes%2520channel-specific%2520semantics%2520inherited%2520from%2520natural%2520images%252C%2520even%2520though%2520electron%2520microscopy%2520slices%2520are%2520homogeneous%2520in%2520the%2520modality%2520and%2520symmetric%2520in%2520their%2520predictive%2520roles.%2520As%2520a%2520result%252C%2520pretrained%2520models%2520may%2520encode%2520inductive%2520biases%2520that%2520are%2520misaligned%2520with%2520the%2520inherent%2520symmetry%2520of%2520volumetric%2520electron%2520microscopy%2520data.%2520In%2520this%2520work%252C%2520it%2520is%2520demonstrated%2520that%2520RGB-pretrained%2520models%2520systematically%2520assign%2520unequal%2520importance%2520to%2520individual%2520input%2520slices%2520when%2520applied%2520to%2520stacked%2520electron%2520microscopy%2520data%252C%2520despite%2520the%2520absence%2520of%2520any%2520intrinsic%2520channel%2520ordering.%2520Using%2520saliency-based%2520attribution%2520analysis%2520across%2520multiple%2520architectures%252C%2520a%2520consistent%2520channel-level%2520asymmetry%2520was%2520observed%2520that%2520persists%2520after%2520fine-tuning%2520and%2520affects%2520model%2520interpretability%252C%2520even%2520when%2520segmentation%2520performance%2520is%2520unchanged.%2520To%2520address%2520this%2520issue%252C%2520a%2520targeted%2520modification%2520of%2520pretraining%2520weights%2520based%2520on%2520uniform%2520channel%2520initialization%2520was%2520proposed%252C%2520which%2520restores%2520symmetric%2520feature%2520attribution%2520while%2520preserving%2520the%2520benefits%2520of%2520pretraining.%2520Experiments%2520on%2520the%2520SNEMI%252C%2520Lucchi%2520and%2520GF-PA66%2520datasets%2520confirm%2520a%2520substantial%2520reduction%2520in%2520attribution%2520bias%2520without%2520compromising%2520or%2520even%2520improving%2520segmentation%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14105v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Pretraining-Induced%20Attention%20Asymmetry%20in%202D%2B%20Electron%20Microscopy%20Image%20Segmentation&entry.906535625=Zs%C3%B3fia%20Moln%C3%A1r%20and%20Gergely%20Szab%C3%B3%20and%20Andr%C3%A1s%20Horv%C3%A1th&entry.1292438233=Vision%20models%20pretrained%20on%20large-scale%20RGB%20natural%20image%20datasets%20are%20widely%20reused%20for%20electron%20microscopy%20image%20segmentation.%20In%20electron%20microscopy%2C%20volumetric%20data%20are%20acquired%20as%20serial%20sections%20and%20processed%20as%20stacks%20of%20adjacent%20grayscale%20slices%2C%20where%20neighboring%20slices%20provide%20symmetric%20contextual%20information%20for%20identifying%20features%20on%20the%20central%20slice.%20The%20common%20strategy%20maps%20such%20stacks%20to%20pseudo-RGB%20inputs%20to%20enable%20transfer%20learning%20from%20pretrained%20models.%20However%2C%20this%20mapping%20imposes%20channel-specific%20semantics%20inherited%20from%20natural%20images%2C%20even%20though%20electron%20microscopy%20slices%20are%20homogeneous%20in%20the%20modality%20and%20symmetric%20in%20their%20predictive%20roles.%20As%20a%20result%2C%20pretrained%20models%20may%20encode%20inductive%20biases%20that%20are%20misaligned%20with%20the%20inherent%20symmetry%20of%20volumetric%20electron%20microscopy%20data.%20In%20this%20work%2C%20it%20is%20demonstrated%20that%20RGB-pretrained%20models%20systematically%20assign%20unequal%20importance%20to%20individual%20input%20slices%20when%20applied%20to%20stacked%20electron%20microscopy%20data%2C%20despite%20the%20absence%20of%20any%20intrinsic%20channel%20ordering.%20Using%20saliency-based%20attribution%20analysis%20across%20multiple%20architectures%2C%20a%20consistent%20channel-level%20asymmetry%20was%20observed%20that%20persists%20after%20fine-tuning%20and%20affects%20model%20interpretability%2C%20even%20when%20segmentation%20performance%20is%20unchanged.%20To%20address%20this%20issue%2C%20a%20targeted%20modification%20of%20pretraining%20weights%20based%20on%20uniform%20channel%20initialization%20was%20proposed%2C%20which%20restores%20symmetric%20feature%20attribution%20while%20preserving%20the%20benefits%20of%20pretraining.%20Experiments%20on%20the%20SNEMI%2C%20Lucchi%20and%20GF-PA66%20datasets%20confirm%20a%20substantial%20reduction%20in%20attribution%20bias%20without%20compromising%20or%20even%20improving%20segmentation%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2505.14105v3&entry.124074799=Read"},
{"title": "Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution", "author": "Matthew Kowal and Goncalo Paulo and Louis Jaburi and Tom Tseng and Lev E McKinney and Stefan Heimersheim and Aaron David Tucker and Adam Gleave and Kellin Pelrine", "abstract": "As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.", "link": "http://arxiv.org/abs/2602.14869v1", "date": "2026-02-16", "relevancy": 2.589, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5196}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept%20Influence%3A%20Leveraging%20Interpretability%20to%20Improve%20Performance%20and%20Efficiency%20in%20Training%20Data%20Attribution&body=Title%3A%20Concept%20Influence%3A%20Leveraging%20Interpretability%20to%20Improve%20Performance%20and%20Efficiency%20in%20Training%20Data%20Attribution%0AAuthor%3A%20Matthew%20Kowal%20and%20Goncalo%20Paulo%20and%20Louis%20Jaburi%20and%20Tom%20Tseng%20and%20Lev%20E%20McKinney%20and%20Stefan%20Heimersheim%20and%20Aaron%20David%20Tucker%20and%20Adam%20Gleave%20and%20Kellin%20Pelrine%0AAbstract%3A%20As%20large%20language%20models%20are%20increasingly%20trained%20and%20fine-tuned%2C%20practitioners%20need%20methods%20to%20identify%20which%20training%20data%20drive%20specific%20behaviors%2C%20particularly%20unintended%20ones.%20Training%20Data%20Attribution%20%28TDA%29%20methods%20address%20this%20by%20estimating%20datapoint%20influence.%20Existing%20approaches%20like%20influence%20functions%20are%20both%20computationally%20expensive%20and%20attribute%20based%20on%20single%20test%20examples%2C%20which%20can%20bias%20results%20toward%20syntactic%20rather%20than%20semantic%20similarity.%20To%20address%20these%20issues%20of%20scalability%20and%20influence%20to%20abstract%20behavior%2C%20we%20leverage%20interpretable%20structures%20within%20the%20model%20during%20the%20attribution.%20First%2C%20we%20introduce%20Concept%20Influence%20which%20attribute%20model%20behavior%20to%20semantic%20directions%20%28such%20as%20linear%20probes%20or%20sparse%20autoencoder%20features%29%20rather%20than%20individual%20test%20examples.%20Second%2C%20we%20show%20that%20simple%20probe-based%20attribution%20methods%20are%20first-order%20approximations%20of%20Concept%20Influence%20that%20achieve%20comparable%20performance%20while%20being%20over%20an%20order-of-magnitude%20faster.%20We%20empirically%20validate%20Concept%20Influence%20and%20approximations%20across%20emergent%20misalignment%20benchmarks%20and%20real%20post-training%20datasets%2C%20and%20demonstrate%20they%20achieve%20comparable%20performance%20to%20classical%20influence%20functions%20while%20being%20substantially%20more%20scalable.%20More%20broadly%2C%20we%20show%20that%20incorporating%20interpretable%20structure%20within%20traditional%20TDA%20pipelines%20can%20enable%20more%20scalable%2C%20explainable%2C%20and%20better%20control%20of%20model%20behavior%20through%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept%2520Influence%253A%2520Leveraging%2520Interpretability%2520to%2520Improve%2520Performance%2520and%2520Efficiency%2520in%2520Training%2520Data%2520Attribution%26entry.906535625%3DMatthew%2520Kowal%2520and%2520Goncalo%2520Paulo%2520and%2520Louis%2520Jaburi%2520and%2520Tom%2520Tseng%2520and%2520Lev%2520E%2520McKinney%2520and%2520Stefan%2520Heimersheim%2520and%2520Aaron%2520David%2520Tucker%2520and%2520Adam%2520Gleave%2520and%2520Kellin%2520Pelrine%26entry.1292438233%3DAs%2520large%2520language%2520models%2520are%2520increasingly%2520trained%2520and%2520fine-tuned%252C%2520practitioners%2520need%2520methods%2520to%2520identify%2520which%2520training%2520data%2520drive%2520specific%2520behaviors%252C%2520particularly%2520unintended%2520ones.%2520Training%2520Data%2520Attribution%2520%2528TDA%2529%2520methods%2520address%2520this%2520by%2520estimating%2520datapoint%2520influence.%2520Existing%2520approaches%2520like%2520influence%2520functions%2520are%2520both%2520computationally%2520expensive%2520and%2520attribute%2520based%2520on%2520single%2520test%2520examples%252C%2520which%2520can%2520bias%2520results%2520toward%2520syntactic%2520rather%2520than%2520semantic%2520similarity.%2520To%2520address%2520these%2520issues%2520of%2520scalability%2520and%2520influence%2520to%2520abstract%2520behavior%252C%2520we%2520leverage%2520interpretable%2520structures%2520within%2520the%2520model%2520during%2520the%2520attribution.%2520First%252C%2520we%2520introduce%2520Concept%2520Influence%2520which%2520attribute%2520model%2520behavior%2520to%2520semantic%2520directions%2520%2528such%2520as%2520linear%2520probes%2520or%2520sparse%2520autoencoder%2520features%2529%2520rather%2520than%2520individual%2520test%2520examples.%2520Second%252C%2520we%2520show%2520that%2520simple%2520probe-based%2520attribution%2520methods%2520are%2520first-order%2520approximations%2520of%2520Concept%2520Influence%2520that%2520achieve%2520comparable%2520performance%2520while%2520being%2520over%2520an%2520order-of-magnitude%2520faster.%2520We%2520empirically%2520validate%2520Concept%2520Influence%2520and%2520approximations%2520across%2520emergent%2520misalignment%2520benchmarks%2520and%2520real%2520post-training%2520datasets%252C%2520and%2520demonstrate%2520they%2520achieve%2520comparable%2520performance%2520to%2520classical%2520influence%2520functions%2520while%2520being%2520substantially%2520more%2520scalable.%2520More%2520broadly%252C%2520we%2520show%2520that%2520incorporating%2520interpretable%2520structure%2520within%2520traditional%2520TDA%2520pipelines%2520can%2520enable%2520more%2520scalable%252C%2520explainable%252C%2520and%2520better%2520control%2520of%2520model%2520behavior%2520through%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20Influence%3A%20Leveraging%20Interpretability%20to%20Improve%20Performance%20and%20Efficiency%20in%20Training%20Data%20Attribution&entry.906535625=Matthew%20Kowal%20and%20Goncalo%20Paulo%20and%20Louis%20Jaburi%20and%20Tom%20Tseng%20and%20Lev%20E%20McKinney%20and%20Stefan%20Heimersheim%20and%20Aaron%20David%20Tucker%20and%20Adam%20Gleave%20and%20Kellin%20Pelrine&entry.1292438233=As%20large%20language%20models%20are%20increasingly%20trained%20and%20fine-tuned%2C%20practitioners%20need%20methods%20to%20identify%20which%20training%20data%20drive%20specific%20behaviors%2C%20particularly%20unintended%20ones.%20Training%20Data%20Attribution%20%28TDA%29%20methods%20address%20this%20by%20estimating%20datapoint%20influence.%20Existing%20approaches%20like%20influence%20functions%20are%20both%20computationally%20expensive%20and%20attribute%20based%20on%20single%20test%20examples%2C%20which%20can%20bias%20results%20toward%20syntactic%20rather%20than%20semantic%20similarity.%20To%20address%20these%20issues%20of%20scalability%20and%20influence%20to%20abstract%20behavior%2C%20we%20leverage%20interpretable%20structures%20within%20the%20model%20during%20the%20attribution.%20First%2C%20we%20introduce%20Concept%20Influence%20which%20attribute%20model%20behavior%20to%20semantic%20directions%20%28such%20as%20linear%20probes%20or%20sparse%20autoencoder%20features%29%20rather%20than%20individual%20test%20examples.%20Second%2C%20we%20show%20that%20simple%20probe-based%20attribution%20methods%20are%20first-order%20approximations%20of%20Concept%20Influence%20that%20achieve%20comparable%20performance%20while%20being%20over%20an%20order-of-magnitude%20faster.%20We%20empirically%20validate%20Concept%20Influence%20and%20approximations%20across%20emergent%20misalignment%20benchmarks%20and%20real%20post-training%20datasets%2C%20and%20demonstrate%20they%20achieve%20comparable%20performance%20to%20classical%20influence%20functions%20while%20being%20substantially%20more%20scalable.%20More%20broadly%2C%20we%20show%20that%20incorporating%20interpretable%20structure%20within%20traditional%20TDA%20pipelines%20can%20enable%20more%20scalable%2C%20explainable%2C%20and%20better%20control%20of%20model%20behavior%20through%20data.&entry.1838667208=http%3A//arxiv.org/abs/2602.14869v1&entry.124074799=Read"},
{"title": "Activation-Space Uncertainty Quantification for Pretrained Networks", "author": "Richard Bergna and Stefan Depeweg and Sergio Calvo-Ordo\u00f1ez and Jonathan Plenk and Alvaro Cartea and Jose Miguel Hern\u00e1ndez-Lobato", "abstract": "Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.", "link": "http://arxiv.org/abs/2602.14934v1", "date": "2026-02-16", "relevancy": 2.58, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5568}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5058}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activation-Space%20Uncertainty%20Quantification%20for%20Pretrained%20Networks&body=Title%3A%20Activation-Space%20Uncertainty%20Quantification%20for%20Pretrained%20Networks%0AAuthor%3A%20Richard%20Bergna%20and%20Stefan%20Depeweg%20and%20Sergio%20Calvo-Ordo%C3%B1ez%20and%20Jonathan%20Plenk%20and%20Alvaro%20Cartea%20and%20Jose%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20Reliable%20uncertainty%20estimates%20are%20crucial%20for%20deploying%20pretrained%20models%3B%20yet%2C%20many%20strong%20methods%20for%20quantifying%20uncertainty%20require%20retraining%2C%20Monte%20Carlo%20sampling%2C%20or%20expensive%20second-order%20computations%20and%20may%20alter%20a%20frozen%20backbone%27s%20predictions.%20To%20address%20this%2C%20we%20introduce%20Gaussian%20Process%20Activations%20%28GAPA%29%2C%20a%20post-hoc%20method%20that%20shifts%20Bayesian%20modeling%20from%20weights%20to%20activations.%20GAPA%20replaces%20standard%20nonlinearities%20with%20Gaussian-process%20activations%20whose%20posterior%20mean%20exactly%20matches%20the%20original%20activation%2C%20preserving%20the%20backbone%27s%20point%20predictions%20by%20construction%20while%20providing%20closed-form%20epistemic%20variances%20in%20activation%20space.%20To%20scale%20to%20modern%20architectures%2C%20we%20use%20a%20sparse%20variational%20inducing-point%20approximation%20over%20cached%20training%20activations%2C%20combined%20with%20local%20k-nearest-neighbor%20subset%20conditioning%2C%20enabling%20deterministic%20single-pass%20uncertainty%20propagation%20without%20sampling%2C%20backpropagation%2C%20or%20second-order%20information.%20Across%20regression%2C%20classification%2C%20image%20segmentation%2C%20and%20language%20modeling%2C%20GAPA%20matches%20or%20outperforms%20strong%20post-hoc%20baselines%20in%20calibration%20and%20out-of-distribution%20detection%20while%20remaining%20efficient%20at%20test%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivation-Space%2520Uncertainty%2520Quantification%2520for%2520Pretrained%2520Networks%26entry.906535625%3DRichard%2520Bergna%2520and%2520Stefan%2520Depeweg%2520and%2520Sergio%2520Calvo-Ordo%25C3%25B1ez%2520and%2520Jonathan%2520Plenk%2520and%2520Alvaro%2520Cartea%2520and%2520Jose%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3DReliable%2520uncertainty%2520estimates%2520are%2520crucial%2520for%2520deploying%2520pretrained%2520models%253B%2520yet%252C%2520many%2520strong%2520methods%2520for%2520quantifying%2520uncertainty%2520require%2520retraining%252C%2520Monte%2520Carlo%2520sampling%252C%2520or%2520expensive%2520second-order%2520computations%2520and%2520may%2520alter%2520a%2520frozen%2520backbone%2527s%2520predictions.%2520To%2520address%2520this%252C%2520we%2520introduce%2520Gaussian%2520Process%2520Activations%2520%2528GAPA%2529%252C%2520a%2520post-hoc%2520method%2520that%2520shifts%2520Bayesian%2520modeling%2520from%2520weights%2520to%2520activations.%2520GAPA%2520replaces%2520standard%2520nonlinearities%2520with%2520Gaussian-process%2520activations%2520whose%2520posterior%2520mean%2520exactly%2520matches%2520the%2520original%2520activation%252C%2520preserving%2520the%2520backbone%2527s%2520point%2520predictions%2520by%2520construction%2520while%2520providing%2520closed-form%2520epistemic%2520variances%2520in%2520activation%2520space.%2520To%2520scale%2520to%2520modern%2520architectures%252C%2520we%2520use%2520a%2520sparse%2520variational%2520inducing-point%2520approximation%2520over%2520cached%2520training%2520activations%252C%2520combined%2520with%2520local%2520k-nearest-neighbor%2520subset%2520conditioning%252C%2520enabling%2520deterministic%2520single-pass%2520uncertainty%2520propagation%2520without%2520sampling%252C%2520backpropagation%252C%2520or%2520second-order%2520information.%2520Across%2520regression%252C%2520classification%252C%2520image%2520segmentation%252C%2520and%2520language%2520modeling%252C%2520GAPA%2520matches%2520or%2520outperforms%2520strong%2520post-hoc%2520baselines%2520in%2520calibration%2520and%2520out-of-distribution%2520detection%2520while%2520remaining%2520efficient%2520at%2520test%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activation-Space%20Uncertainty%20Quantification%20for%20Pretrained%20Networks&entry.906535625=Richard%20Bergna%20and%20Stefan%20Depeweg%20and%20Sergio%20Calvo-Ordo%C3%B1ez%20and%20Jonathan%20Plenk%20and%20Alvaro%20Cartea%20and%20Jose%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=Reliable%20uncertainty%20estimates%20are%20crucial%20for%20deploying%20pretrained%20models%3B%20yet%2C%20many%20strong%20methods%20for%20quantifying%20uncertainty%20require%20retraining%2C%20Monte%20Carlo%20sampling%2C%20or%20expensive%20second-order%20computations%20and%20may%20alter%20a%20frozen%20backbone%27s%20predictions.%20To%20address%20this%2C%20we%20introduce%20Gaussian%20Process%20Activations%20%28GAPA%29%2C%20a%20post-hoc%20method%20that%20shifts%20Bayesian%20modeling%20from%20weights%20to%20activations.%20GAPA%20replaces%20standard%20nonlinearities%20with%20Gaussian-process%20activations%20whose%20posterior%20mean%20exactly%20matches%20the%20original%20activation%2C%20preserving%20the%20backbone%27s%20point%20predictions%20by%20construction%20while%20providing%20closed-form%20epistemic%20variances%20in%20activation%20space.%20To%20scale%20to%20modern%20architectures%2C%20we%20use%20a%20sparse%20variational%20inducing-point%20approximation%20over%20cached%20training%20activations%2C%20combined%20with%20local%20k-nearest-neighbor%20subset%20conditioning%2C%20enabling%20deterministic%20single-pass%20uncertainty%20propagation%20without%20sampling%2C%20backpropagation%2C%20or%20second-order%20information.%20Across%20regression%2C%20classification%2C%20image%20segmentation%2C%20and%20language%20modeling%2C%20GAPA%20matches%20or%20outperforms%20strong%20post-hoc%20baselines%20in%20calibration%20and%20out-of-distribution%20detection%20while%20remaining%20efficient%20at%20test%20time.&entry.1838667208=http%3A//arxiv.org/abs/2602.14934v1&entry.124074799=Read"},
{"title": "VIPA: Visual Informative Part Attention for Referring Image Segmentation", "author": "Yubin Cho and Hyunwoo Yu and Kyeongbo Kong and Kyomin Sohn and Bongjoon Hyun and Suk-Ju Kang", "abstract": "Referring Image Segmentation (RIS) aims to segment a target object described by a natural language expression. Existing methods have evolved by leveraging the vision information into the language tokens. To more effectively exploit visual contexts for fine-grained segmentation, we propose a novel Visual Informative Part Attention (VIPA) framework for referring image segmentation. VIPA leverages the informative parts of visual contexts, called a visual expression, which can effectively provide the structural and semantic visual target information to the network. This design reduces high-variance cross-modal projection and enhances semantic consistency in an attention mechanism of the referring image segmentation. We also design a visual expression generator (VEG) module, which retrieves informative visual tokens via local-global linguistic context cues and refines the retrieved tokens for reducing noise information and sharing informative visual attributes. This module allows the visual expression to consider comprehensive contexts and capture semantic visual contexts of informative regions. In this way, our framework enables the network's attention to robustly align with the fine-grained regions of interest. Extensive experiments and visual analysis demonstrate the effectiveness of our approach. Our VIPA outperforms the existing state-of-the-art methods on four public RIS benchmarks.", "link": "http://arxiv.org/abs/2602.14788v1", "date": "2026-02-16", "relevancy": 2.5522, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5079}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIPA%3A%20Visual%20Informative%20Part%20Attention%20for%20Referring%20Image%20Segmentation&body=Title%3A%20VIPA%3A%20Visual%20Informative%20Part%20Attention%20for%20Referring%20Image%20Segmentation%0AAuthor%3A%20Yubin%20Cho%20and%20Hyunwoo%20Yu%20and%20Kyeongbo%20Kong%20and%20Kyomin%20Sohn%20and%20Bongjoon%20Hyun%20and%20Suk-Ju%20Kang%0AAbstract%3A%20Referring%20Image%20Segmentation%20%28RIS%29%20aims%20to%20segment%20a%20target%20object%20described%20by%20a%20natural%20language%20expression.%20Existing%20methods%20have%20evolved%20by%20leveraging%20the%20vision%20information%20into%20the%20language%20tokens.%20To%20more%20effectively%20exploit%20visual%20contexts%20for%20fine-grained%20segmentation%2C%20we%20propose%20a%20novel%20Visual%20Informative%20Part%20Attention%20%28VIPA%29%20framework%20for%20referring%20image%20segmentation.%20VIPA%20leverages%20the%20informative%20parts%20of%20visual%20contexts%2C%20called%20a%20visual%20expression%2C%20which%20can%20effectively%20provide%20the%20structural%20and%20semantic%20visual%20target%20information%20to%20the%20network.%20This%20design%20reduces%20high-variance%20cross-modal%20projection%20and%20enhances%20semantic%20consistency%20in%20an%20attention%20mechanism%20of%20the%20referring%20image%20segmentation.%20We%20also%20design%20a%20visual%20expression%20generator%20%28VEG%29%20module%2C%20which%20retrieves%20informative%20visual%20tokens%20via%20local-global%20linguistic%20context%20cues%20and%20refines%20the%20retrieved%20tokens%20for%20reducing%20noise%20information%20and%20sharing%20informative%20visual%20attributes.%20This%20module%20allows%20the%20visual%20expression%20to%20consider%20comprehensive%20contexts%20and%20capture%20semantic%20visual%20contexts%20of%20informative%20regions.%20In%20this%20way%2C%20our%20framework%20enables%20the%20network%27s%20attention%20to%20robustly%20align%20with%20the%20fine-grained%20regions%20of%20interest.%20Extensive%20experiments%20and%20visual%20analysis%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20Our%20VIPA%20outperforms%20the%20existing%20state-of-the-art%20methods%20on%20four%20public%20RIS%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIPA%253A%2520Visual%2520Informative%2520Part%2520Attention%2520for%2520Referring%2520Image%2520Segmentation%26entry.906535625%3DYubin%2520Cho%2520and%2520Hyunwoo%2520Yu%2520and%2520Kyeongbo%2520Kong%2520and%2520Kyomin%2520Sohn%2520and%2520Bongjoon%2520Hyun%2520and%2520Suk-Ju%2520Kang%26entry.1292438233%3DReferring%2520Image%2520Segmentation%2520%2528RIS%2529%2520aims%2520to%2520segment%2520a%2520target%2520object%2520described%2520by%2520a%2520natural%2520language%2520expression.%2520Existing%2520methods%2520have%2520evolved%2520by%2520leveraging%2520the%2520vision%2520information%2520into%2520the%2520language%2520tokens.%2520To%2520more%2520effectively%2520exploit%2520visual%2520contexts%2520for%2520fine-grained%2520segmentation%252C%2520we%2520propose%2520a%2520novel%2520Visual%2520Informative%2520Part%2520Attention%2520%2528VIPA%2529%2520framework%2520for%2520referring%2520image%2520segmentation.%2520VIPA%2520leverages%2520the%2520informative%2520parts%2520of%2520visual%2520contexts%252C%2520called%2520a%2520visual%2520expression%252C%2520which%2520can%2520effectively%2520provide%2520the%2520structural%2520and%2520semantic%2520visual%2520target%2520information%2520to%2520the%2520network.%2520This%2520design%2520reduces%2520high-variance%2520cross-modal%2520projection%2520and%2520enhances%2520semantic%2520consistency%2520in%2520an%2520attention%2520mechanism%2520of%2520the%2520referring%2520image%2520segmentation.%2520We%2520also%2520design%2520a%2520visual%2520expression%2520generator%2520%2528VEG%2529%2520module%252C%2520which%2520retrieves%2520informative%2520visual%2520tokens%2520via%2520local-global%2520linguistic%2520context%2520cues%2520and%2520refines%2520the%2520retrieved%2520tokens%2520for%2520reducing%2520noise%2520information%2520and%2520sharing%2520informative%2520visual%2520attributes.%2520This%2520module%2520allows%2520the%2520visual%2520expression%2520to%2520consider%2520comprehensive%2520contexts%2520and%2520capture%2520semantic%2520visual%2520contexts%2520of%2520informative%2520regions.%2520In%2520this%2520way%252C%2520our%2520framework%2520enables%2520the%2520network%2527s%2520attention%2520to%2520robustly%2520align%2520with%2520the%2520fine-grained%2520regions%2520of%2520interest.%2520Extensive%2520experiments%2520and%2520visual%2520analysis%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Our%2520VIPA%2520outperforms%2520the%2520existing%2520state-of-the-art%2520methods%2520on%2520four%2520public%2520RIS%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIPA%3A%20Visual%20Informative%20Part%20Attention%20for%20Referring%20Image%20Segmentation&entry.906535625=Yubin%20Cho%20and%20Hyunwoo%20Yu%20and%20Kyeongbo%20Kong%20and%20Kyomin%20Sohn%20and%20Bongjoon%20Hyun%20and%20Suk-Ju%20Kang&entry.1292438233=Referring%20Image%20Segmentation%20%28RIS%29%20aims%20to%20segment%20a%20target%20object%20described%20by%20a%20natural%20language%20expression.%20Existing%20methods%20have%20evolved%20by%20leveraging%20the%20vision%20information%20into%20the%20language%20tokens.%20To%20more%20effectively%20exploit%20visual%20contexts%20for%20fine-grained%20segmentation%2C%20we%20propose%20a%20novel%20Visual%20Informative%20Part%20Attention%20%28VIPA%29%20framework%20for%20referring%20image%20segmentation.%20VIPA%20leverages%20the%20informative%20parts%20of%20visual%20contexts%2C%20called%20a%20visual%20expression%2C%20which%20can%20effectively%20provide%20the%20structural%20and%20semantic%20visual%20target%20information%20to%20the%20network.%20This%20design%20reduces%20high-variance%20cross-modal%20projection%20and%20enhances%20semantic%20consistency%20in%20an%20attention%20mechanism%20of%20the%20referring%20image%20segmentation.%20We%20also%20design%20a%20visual%20expression%20generator%20%28VEG%29%20module%2C%20which%20retrieves%20informative%20visual%20tokens%20via%20local-global%20linguistic%20context%20cues%20and%20refines%20the%20retrieved%20tokens%20for%20reducing%20noise%20information%20and%20sharing%20informative%20visual%20attributes.%20This%20module%20allows%20the%20visual%20expression%20to%20consider%20comprehensive%20contexts%20and%20capture%20semantic%20visual%20contexts%20of%20informative%20regions.%20In%20this%20way%2C%20our%20framework%20enables%20the%20network%27s%20attention%20to%20robustly%20align%20with%20the%20fine-grained%20regions%20of%20interest.%20Extensive%20experiments%20and%20visual%20analysis%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20Our%20VIPA%20outperforms%20the%20existing%20state-of-the-art%20methods%20on%20four%20public%20RIS%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2602.14788v1&entry.124074799=Read"},
{"title": "Robust Generalization with Adaptive Optimal Transport Priors for Decision-Focused Learning", "author": "Haixiang Sun and Andrew L. Liu", "abstract": "Few-shot learning requires models to generalize under limited supervision while remaining robust to distribution shifts. Existing Sinkhorn Distributionally Robust Optimization (DRO) methods provide theoretical guarantees but rely on a fixed reference distribution, which limits their adaptability. We propose a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework that learns class-adaptive priors from abundant base data via hierarchical optimal transport and embeds them into the Sinkhorn DRO formulation. This design enables few-shot information to be organically integrated into producing class-specific robust decisions that are both theoretically grounded and efficient, and further aligns the uncertainty set with transferable structural knowledge. Experiments show that PG-DRO achieves stronger robust generalization in few-shot scenarios, outperforming both standard learners and DRO baselines.", "link": "http://arxiv.org/abs/2602.01427v2", "date": "2026-02-16", "relevancy": 2.5197, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5143}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Generalization%20with%20Adaptive%20Optimal%20Transport%20Priors%20for%20Decision-Focused%20Learning&body=Title%3A%20Robust%20Generalization%20with%20Adaptive%20Optimal%20Transport%20Priors%20for%20Decision-Focused%20Learning%0AAuthor%3A%20Haixiang%20Sun%20and%20Andrew%20L.%20Liu%0AAbstract%3A%20Few-shot%20learning%20requires%20models%20to%20generalize%20under%20limited%20supervision%20while%20remaining%20robust%20to%20distribution%20shifts.%20Existing%20Sinkhorn%20Distributionally%20Robust%20Optimization%20%28DRO%29%20methods%20provide%20theoretical%20guarantees%20but%20rely%20on%20a%20fixed%20reference%20distribution%2C%20which%20limits%20their%20adaptability.%20We%20propose%20a%20Prototype-Guided%20Distributionally%20Robust%20Optimization%20%28PG-DRO%29%20framework%20that%20learns%20class-adaptive%20priors%20from%20abundant%20base%20data%20via%20hierarchical%20optimal%20transport%20and%20embeds%20them%20into%20the%20Sinkhorn%20DRO%20formulation.%20This%20design%20enables%20few-shot%20information%20to%20be%20organically%20integrated%20into%20producing%20class-specific%20robust%20decisions%20that%20are%20both%20theoretically%20grounded%20and%20efficient%2C%20and%20further%20aligns%20the%20uncertainty%20set%20with%20transferable%20structural%20knowledge.%20Experiments%20show%20that%20PG-DRO%20achieves%20stronger%20robust%20generalization%20in%20few-shot%20scenarios%2C%20outperforming%20both%20standard%20learners%20and%20DRO%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Generalization%2520with%2520Adaptive%2520Optimal%2520Transport%2520Priors%2520for%2520Decision-Focused%2520Learning%26entry.906535625%3DHaixiang%2520Sun%2520and%2520Andrew%2520L.%2520Liu%26entry.1292438233%3DFew-shot%2520learning%2520requires%2520models%2520to%2520generalize%2520under%2520limited%2520supervision%2520while%2520remaining%2520robust%2520to%2520distribution%2520shifts.%2520Existing%2520Sinkhorn%2520Distributionally%2520Robust%2520Optimization%2520%2528DRO%2529%2520methods%2520provide%2520theoretical%2520guarantees%2520but%2520rely%2520on%2520a%2520fixed%2520reference%2520distribution%252C%2520which%2520limits%2520their%2520adaptability.%2520We%2520propose%2520a%2520Prototype-Guided%2520Distributionally%2520Robust%2520Optimization%2520%2528PG-DRO%2529%2520framework%2520that%2520learns%2520class-adaptive%2520priors%2520from%2520abundant%2520base%2520data%2520via%2520hierarchical%2520optimal%2520transport%2520and%2520embeds%2520them%2520into%2520the%2520Sinkhorn%2520DRO%2520formulation.%2520This%2520design%2520enables%2520few-shot%2520information%2520to%2520be%2520organically%2520integrated%2520into%2520producing%2520class-specific%2520robust%2520decisions%2520that%2520are%2520both%2520theoretically%2520grounded%2520and%2520efficient%252C%2520and%2520further%2520aligns%2520the%2520uncertainty%2520set%2520with%2520transferable%2520structural%2520knowledge.%2520Experiments%2520show%2520that%2520PG-DRO%2520achieves%2520stronger%2520robust%2520generalization%2520in%2520few-shot%2520scenarios%252C%2520outperforming%2520both%2520standard%2520learners%2520and%2520DRO%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Generalization%20with%20Adaptive%20Optimal%20Transport%20Priors%20for%20Decision-Focused%20Learning&entry.906535625=Haixiang%20Sun%20and%20Andrew%20L.%20Liu&entry.1292438233=Few-shot%20learning%20requires%20models%20to%20generalize%20under%20limited%20supervision%20while%20remaining%20robust%20to%20distribution%20shifts.%20Existing%20Sinkhorn%20Distributionally%20Robust%20Optimization%20%28DRO%29%20methods%20provide%20theoretical%20guarantees%20but%20rely%20on%20a%20fixed%20reference%20distribution%2C%20which%20limits%20their%20adaptability.%20We%20propose%20a%20Prototype-Guided%20Distributionally%20Robust%20Optimization%20%28PG-DRO%29%20framework%20that%20learns%20class-adaptive%20priors%20from%20abundant%20base%20data%20via%20hierarchical%20optimal%20transport%20and%20embeds%20them%20into%20the%20Sinkhorn%20DRO%20formulation.%20This%20design%20enables%20few-shot%20information%20to%20be%20organically%20integrated%20into%20producing%20class-specific%20robust%20decisions%20that%20are%20both%20theoretically%20grounded%20and%20efficient%2C%20and%20further%20aligns%20the%20uncertainty%20set%20with%20transferable%20structural%20knowledge.%20Experiments%20show%20that%20PG-DRO%20achieves%20stronger%20robust%20generalization%20in%20few-shot%20scenarios%2C%20outperforming%20both%20standard%20learners%20and%20DRO%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2602.01427v2&entry.124074799=Read"},
{"title": "Pseudo-differential-enhanced physics-informed neural networks", "author": "Andrew Gracyk", "abstract": "We present pseudo-differential enhanced physics-informed neural networks (PINNs), an extension of gradient enhancement but in Fourier space. Gradient enhancement of PINNs dictates that the PDE residual is taken to a higher differential order than prescribed by the PDE, added to the objective as an augmented term in order to improve training and overall learning fidelity. We propose the same procedure after application via Fourier transforms, since differentiating in Fourier space is multiplication with the Fourier wavenumber under suitable decay. Our methods are fast and efficient. Our methods oftentimes achieve superior PINN versus numerical error in fewer training iterations, potentially pair well with few samples in collocation, and can on occasion break plateaus in low collocation settings. Moreover, our methods are suitable for fractional derivatives. We establish that our methods improve spectral eigenvalue decay of the neural tangent kernel (NTK), and so our methods contribute towards the learning of high frequencies in early training, mitigating the effects of frequency bias up to the polynomial order and possibly greater with smooth activations. Our methods accommodate advanced techniques in PINNs, such as Fourier feature embeddings. A pitfall of discrete Fourier transforms via the Fast Fourier Transform (FFT) is mesh subjugation, and so we demonstrate compatibility of our methods for greater mesh flexibility and invariance on alternative Euclidean and non-Euclidean domains via Monte Carlo methods and otherwise.", "link": "http://arxiv.org/abs/2602.14663v1", "date": "2026-02-16", "relevancy": 2.5183, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.547}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4848}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-differential-enhanced%20physics-informed%20neural%20networks&body=Title%3A%20Pseudo-differential-enhanced%20physics-informed%20neural%20networks%0AAuthor%3A%20Andrew%20Gracyk%0AAbstract%3A%20We%20present%20pseudo-differential%20enhanced%20physics-informed%20neural%20networks%20%28PINNs%29%2C%20an%20extension%20of%20gradient%20enhancement%20but%20in%20Fourier%20space.%20Gradient%20enhancement%20of%20PINNs%20dictates%20that%20the%20PDE%20residual%20is%20taken%20to%20a%20higher%20differential%20order%20than%20prescribed%20by%20the%20PDE%2C%20added%20to%20the%20objective%20as%20an%20augmented%20term%20in%20order%20to%20improve%20training%20and%20overall%20learning%20fidelity.%20We%20propose%20the%20same%20procedure%20after%20application%20via%20Fourier%20transforms%2C%20since%20differentiating%20in%20Fourier%20space%20is%20multiplication%20with%20the%20Fourier%20wavenumber%20under%20suitable%20decay.%20Our%20methods%20are%20fast%20and%20efficient.%20Our%20methods%20oftentimes%20achieve%20superior%20PINN%20versus%20numerical%20error%20in%20fewer%20training%20iterations%2C%20potentially%20pair%20well%20with%20few%20samples%20in%20collocation%2C%20and%20can%20on%20occasion%20break%20plateaus%20in%20low%20collocation%20settings.%20Moreover%2C%20our%20methods%20are%20suitable%20for%20fractional%20derivatives.%20We%20establish%20that%20our%20methods%20improve%20spectral%20eigenvalue%20decay%20of%20the%20neural%20tangent%20kernel%20%28NTK%29%2C%20and%20so%20our%20methods%20contribute%20towards%20the%20learning%20of%20high%20frequencies%20in%20early%20training%2C%20mitigating%20the%20effects%20of%20frequency%20bias%20up%20to%20the%20polynomial%20order%20and%20possibly%20greater%20with%20smooth%20activations.%20Our%20methods%20accommodate%20advanced%20techniques%20in%20PINNs%2C%20such%20as%20Fourier%20feature%20embeddings.%20A%20pitfall%20of%20discrete%20Fourier%20transforms%20via%20the%20Fast%20Fourier%20Transform%20%28FFT%29%20is%20mesh%20subjugation%2C%20and%20so%20we%20demonstrate%20compatibility%20of%20our%20methods%20for%20greater%20mesh%20flexibility%20and%20invariance%20on%20alternative%20Euclidean%20and%20non-Euclidean%20domains%20via%20Monte%20Carlo%20methods%20and%20otherwise.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-differential-enhanced%2520physics-informed%2520neural%2520networks%26entry.906535625%3DAndrew%2520Gracyk%26entry.1292438233%3DWe%2520present%2520pseudo-differential%2520enhanced%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529%252C%2520an%2520extension%2520of%2520gradient%2520enhancement%2520but%2520in%2520Fourier%2520space.%2520Gradient%2520enhancement%2520of%2520PINNs%2520dictates%2520that%2520the%2520PDE%2520residual%2520is%2520taken%2520to%2520a%2520higher%2520differential%2520order%2520than%2520prescribed%2520by%2520the%2520PDE%252C%2520added%2520to%2520the%2520objective%2520as%2520an%2520augmented%2520term%2520in%2520order%2520to%2520improve%2520training%2520and%2520overall%2520learning%2520fidelity.%2520We%2520propose%2520the%2520same%2520procedure%2520after%2520application%2520via%2520Fourier%2520transforms%252C%2520since%2520differentiating%2520in%2520Fourier%2520space%2520is%2520multiplication%2520with%2520the%2520Fourier%2520wavenumber%2520under%2520suitable%2520decay.%2520Our%2520methods%2520are%2520fast%2520and%2520efficient.%2520Our%2520methods%2520oftentimes%2520achieve%2520superior%2520PINN%2520versus%2520numerical%2520error%2520in%2520fewer%2520training%2520iterations%252C%2520potentially%2520pair%2520well%2520with%2520few%2520samples%2520in%2520collocation%252C%2520and%2520can%2520on%2520occasion%2520break%2520plateaus%2520in%2520low%2520collocation%2520settings.%2520Moreover%252C%2520our%2520methods%2520are%2520suitable%2520for%2520fractional%2520derivatives.%2520We%2520establish%2520that%2520our%2520methods%2520improve%2520spectral%2520eigenvalue%2520decay%2520of%2520the%2520neural%2520tangent%2520kernel%2520%2528NTK%2529%252C%2520and%2520so%2520our%2520methods%2520contribute%2520towards%2520the%2520learning%2520of%2520high%2520frequencies%2520in%2520early%2520training%252C%2520mitigating%2520the%2520effects%2520of%2520frequency%2520bias%2520up%2520to%2520the%2520polynomial%2520order%2520and%2520possibly%2520greater%2520with%2520smooth%2520activations.%2520Our%2520methods%2520accommodate%2520advanced%2520techniques%2520in%2520PINNs%252C%2520such%2520as%2520Fourier%2520feature%2520embeddings.%2520A%2520pitfall%2520of%2520discrete%2520Fourier%2520transforms%2520via%2520the%2520Fast%2520Fourier%2520Transform%2520%2528FFT%2529%2520is%2520mesh%2520subjugation%252C%2520and%2520so%2520we%2520demonstrate%2520compatibility%2520of%2520our%2520methods%2520for%2520greater%2520mesh%2520flexibility%2520and%2520invariance%2520on%2520alternative%2520Euclidean%2520and%2520non-Euclidean%2520domains%2520via%2520Monte%2520Carlo%2520methods%2520and%2520otherwise.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-differential-enhanced%20physics-informed%20neural%20networks&entry.906535625=Andrew%20Gracyk&entry.1292438233=We%20present%20pseudo-differential%20enhanced%20physics-informed%20neural%20networks%20%28PINNs%29%2C%20an%20extension%20of%20gradient%20enhancement%20but%20in%20Fourier%20space.%20Gradient%20enhancement%20of%20PINNs%20dictates%20that%20the%20PDE%20residual%20is%20taken%20to%20a%20higher%20differential%20order%20than%20prescribed%20by%20the%20PDE%2C%20added%20to%20the%20objective%20as%20an%20augmented%20term%20in%20order%20to%20improve%20training%20and%20overall%20learning%20fidelity.%20We%20propose%20the%20same%20procedure%20after%20application%20via%20Fourier%20transforms%2C%20since%20differentiating%20in%20Fourier%20space%20is%20multiplication%20with%20the%20Fourier%20wavenumber%20under%20suitable%20decay.%20Our%20methods%20are%20fast%20and%20efficient.%20Our%20methods%20oftentimes%20achieve%20superior%20PINN%20versus%20numerical%20error%20in%20fewer%20training%20iterations%2C%20potentially%20pair%20well%20with%20few%20samples%20in%20collocation%2C%20and%20can%20on%20occasion%20break%20plateaus%20in%20low%20collocation%20settings.%20Moreover%2C%20our%20methods%20are%20suitable%20for%20fractional%20derivatives.%20We%20establish%20that%20our%20methods%20improve%20spectral%20eigenvalue%20decay%20of%20the%20neural%20tangent%20kernel%20%28NTK%29%2C%20and%20so%20our%20methods%20contribute%20towards%20the%20learning%20of%20high%20frequencies%20in%20early%20training%2C%20mitigating%20the%20effects%20of%20frequency%20bias%20up%20to%20the%20polynomial%20order%20and%20possibly%20greater%20with%20smooth%20activations.%20Our%20methods%20accommodate%20advanced%20techniques%20in%20PINNs%2C%20such%20as%20Fourier%20feature%20embeddings.%20A%20pitfall%20of%20discrete%20Fourier%20transforms%20via%20the%20Fast%20Fourier%20Transform%20%28FFT%29%20is%20mesh%20subjugation%2C%20and%20so%20we%20demonstrate%20compatibility%20of%20our%20methods%20for%20greater%20mesh%20flexibility%20and%20invariance%20on%20alternative%20Euclidean%20and%20non-Euclidean%20domains%20via%20Monte%20Carlo%20methods%20and%20otherwise.&entry.1838667208=http%3A//arxiv.org/abs/2602.14663v1&entry.124074799=Read"},
{"title": "SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning", "author": "Shishir Muralidhara and Didier Stricker and Ren\u00e9 Schuster", "abstract": "Continual learning remains constrained by the need for repeated retraining, high computational costs, and the persistent challenge of forgetting. These factors significantly limit the applicability of continuous learning in real-world settings, as iterative model updates require significant computational resources and inherently exacerbate forgetting. We present SAILS -- Segment Anything with Incrementally Learned Semantics, a training-free framework for Class-Incremental Semantic Segmentation (CISS) that sidesteps these challenges entirely. SAILS leverages foundational models to decouple CISS into two stages: Zero-shot region extraction using Segment Anything Model (SAM), followed by semantic association through prototypes in a fixed feature space. SAILS incorporates selective intra-class clustering, resulting in multiple prototypes per class to better model intra-class variability. Our results demonstrate that, despite requiring no incremental training, SAILS typically surpasses the performance of existing training-based approaches on standard CISS datasets, particularly in long and challenging task sequences where forgetting tends to be most severe. By avoiding parameter updates, SAILS completely eliminates forgetting and maintains consistent, task-invariant performance. Furthermore, SAILS exhibits positive backward transfer, where the introduction of new classes can enhance performance on previous classes.", "link": "http://arxiv.org/abs/2602.14767v1", "date": "2026-02-16", "relevancy": 2.5134, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5242}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4937}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAILS%3A%20Segment%20Anything%20with%20Incrementally%20Learned%20Semantics%20for%20Task-Invariant%20and%20Training-Free%20Continual%20Learning&body=Title%3A%20SAILS%3A%20Segment%20Anything%20with%20Incrementally%20Learned%20Semantics%20for%20Task-Invariant%20and%20Training-Free%20Continual%20Learning%0AAuthor%3A%20Shishir%20Muralidhara%20and%20Didier%20Stricker%20and%20Ren%C3%A9%20Schuster%0AAbstract%3A%20Continual%20learning%20remains%20constrained%20by%20the%20need%20for%20repeated%20retraining%2C%20high%20computational%20costs%2C%20and%20the%20persistent%20challenge%20of%20forgetting.%20These%20factors%20significantly%20limit%20the%20applicability%20of%20continuous%20learning%20in%20real-world%20settings%2C%20as%20iterative%20model%20updates%20require%20significant%20computational%20resources%20and%20inherently%20exacerbate%20forgetting.%20We%20present%20SAILS%20--%20Segment%20Anything%20with%20Incrementally%20Learned%20Semantics%2C%20a%20training-free%20framework%20for%20Class-Incremental%20Semantic%20Segmentation%20%28CISS%29%20that%20sidesteps%20these%20challenges%20entirely.%20SAILS%20leverages%20foundational%20models%20to%20decouple%20CISS%20into%20two%20stages%3A%20Zero-shot%20region%20extraction%20using%20Segment%20Anything%20Model%20%28SAM%29%2C%20followed%20by%20semantic%20association%20through%20prototypes%20in%20a%20fixed%20feature%20space.%20SAILS%20incorporates%20selective%20intra-class%20clustering%2C%20resulting%20in%20multiple%20prototypes%20per%20class%20to%20better%20model%20intra-class%20variability.%20Our%20results%20demonstrate%20that%2C%20despite%20requiring%20no%20incremental%20training%2C%20SAILS%20typically%20surpasses%20the%20performance%20of%20existing%20training-based%20approaches%20on%20standard%20CISS%20datasets%2C%20particularly%20in%20long%20and%20challenging%20task%20sequences%20where%20forgetting%20tends%20to%20be%20most%20severe.%20By%20avoiding%20parameter%20updates%2C%20SAILS%20completely%20eliminates%20forgetting%20and%20maintains%20consistent%2C%20task-invariant%20performance.%20Furthermore%2C%20SAILS%20exhibits%20positive%20backward%20transfer%2C%20where%20the%20introduction%20of%20new%20classes%20can%20enhance%20performance%20on%20previous%20classes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAILS%253A%2520Segment%2520Anything%2520with%2520Incrementally%2520Learned%2520Semantics%2520for%2520Task-Invariant%2520and%2520Training-Free%2520Continual%2520Learning%26entry.906535625%3DShishir%2520Muralidhara%2520and%2520Didier%2520Stricker%2520and%2520Ren%25C3%25A9%2520Schuster%26entry.1292438233%3DContinual%2520learning%2520remains%2520constrained%2520by%2520the%2520need%2520for%2520repeated%2520retraining%252C%2520high%2520computational%2520costs%252C%2520and%2520the%2520persistent%2520challenge%2520of%2520forgetting.%2520These%2520factors%2520significantly%2520limit%2520the%2520applicability%2520of%2520continuous%2520learning%2520in%2520real-world%2520settings%252C%2520as%2520iterative%2520model%2520updates%2520require%2520significant%2520computational%2520resources%2520and%2520inherently%2520exacerbate%2520forgetting.%2520We%2520present%2520SAILS%2520--%2520Segment%2520Anything%2520with%2520Incrementally%2520Learned%2520Semantics%252C%2520a%2520training-free%2520framework%2520for%2520Class-Incremental%2520Semantic%2520Segmentation%2520%2528CISS%2529%2520that%2520sidesteps%2520these%2520challenges%2520entirely.%2520SAILS%2520leverages%2520foundational%2520models%2520to%2520decouple%2520CISS%2520into%2520two%2520stages%253A%2520Zero-shot%2520region%2520extraction%2520using%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520followed%2520by%2520semantic%2520association%2520through%2520prototypes%2520in%2520a%2520fixed%2520feature%2520space.%2520SAILS%2520incorporates%2520selective%2520intra-class%2520clustering%252C%2520resulting%2520in%2520multiple%2520prototypes%2520per%2520class%2520to%2520better%2520model%2520intra-class%2520variability.%2520Our%2520results%2520demonstrate%2520that%252C%2520despite%2520requiring%2520no%2520incremental%2520training%252C%2520SAILS%2520typically%2520surpasses%2520the%2520performance%2520of%2520existing%2520training-based%2520approaches%2520on%2520standard%2520CISS%2520datasets%252C%2520particularly%2520in%2520long%2520and%2520challenging%2520task%2520sequences%2520where%2520forgetting%2520tends%2520to%2520be%2520most%2520severe.%2520By%2520avoiding%2520parameter%2520updates%252C%2520SAILS%2520completely%2520eliminates%2520forgetting%2520and%2520maintains%2520consistent%252C%2520task-invariant%2520performance.%2520Furthermore%252C%2520SAILS%2520exhibits%2520positive%2520backward%2520transfer%252C%2520where%2520the%2520introduction%2520of%2520new%2520classes%2520can%2520enhance%2520performance%2520on%2520previous%2520classes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAILS%3A%20Segment%20Anything%20with%20Incrementally%20Learned%20Semantics%20for%20Task-Invariant%20and%20Training-Free%20Continual%20Learning&entry.906535625=Shishir%20Muralidhara%20and%20Didier%20Stricker%20and%20Ren%C3%A9%20Schuster&entry.1292438233=Continual%20learning%20remains%20constrained%20by%20the%20need%20for%20repeated%20retraining%2C%20high%20computational%20costs%2C%20and%20the%20persistent%20challenge%20of%20forgetting.%20These%20factors%20significantly%20limit%20the%20applicability%20of%20continuous%20learning%20in%20real-world%20settings%2C%20as%20iterative%20model%20updates%20require%20significant%20computational%20resources%20and%20inherently%20exacerbate%20forgetting.%20We%20present%20SAILS%20--%20Segment%20Anything%20with%20Incrementally%20Learned%20Semantics%2C%20a%20training-free%20framework%20for%20Class-Incremental%20Semantic%20Segmentation%20%28CISS%29%20that%20sidesteps%20these%20challenges%20entirely.%20SAILS%20leverages%20foundational%20models%20to%20decouple%20CISS%20into%20two%20stages%3A%20Zero-shot%20region%20extraction%20using%20Segment%20Anything%20Model%20%28SAM%29%2C%20followed%20by%20semantic%20association%20through%20prototypes%20in%20a%20fixed%20feature%20space.%20SAILS%20incorporates%20selective%20intra-class%20clustering%2C%20resulting%20in%20multiple%20prototypes%20per%20class%20to%20better%20model%20intra-class%20variability.%20Our%20results%20demonstrate%20that%2C%20despite%20requiring%20no%20incremental%20training%2C%20SAILS%20typically%20surpasses%20the%20performance%20of%20existing%20training-based%20approaches%20on%20standard%20CISS%20datasets%2C%20particularly%20in%20long%20and%20challenging%20task%20sequences%20where%20forgetting%20tends%20to%20be%20most%20severe.%20By%20avoiding%20parameter%20updates%2C%20SAILS%20completely%20eliminates%20forgetting%20and%20maintains%20consistent%2C%20task-invariant%20performance.%20Furthermore%2C%20SAILS%20exhibits%20positive%20backward%20transfer%2C%20where%20the%20introduction%20of%20new%20classes%20can%20enhance%20performance%20on%20previous%20classes.&entry.1838667208=http%3A//arxiv.org/abs/2602.14767v1&entry.124074799=Read"},
{"title": "Multi-dimensional Persistent Sheaf Laplacians for Image Analysis", "author": "Xiang Xiang Wang and Guo-Wei Wei", "abstract": "We propose a multi-dimensional persistent sheaf Laplacian (MPSL) framework on simplicial complexes for image analysis. The proposed method is motivated by the strong sensitivity of commonly used dimensionality reduction techniques, such as principal component analysis (PCA), to the choice of reduced dimension. Rather than selecting a single reduced dimension or averaging results across dimensions, we exploit complementary advantages of multiple reduced dimensions. At a given dimension, image samples are regarded as simplicial complexes, and persistent sheaf Laplacians are utilized to extract a multiscale localized topological spectral representation for individual image samples. Statistical summaries of the resulting spectra are then aggregated across scales and dimensions to form multiscale multi-dimensional image representations. We evaluate the proposed framework on the COIL20 and ETH80 image datasets using standard classification protocols. Experimental results show that the proposed method provides more stable performance across a wide range of reduced dimensions and achieves consistent improvements to PCA-based baselines in moderate dimensional regimes.", "link": "http://arxiv.org/abs/2602.14846v1", "date": "2026-02-16", "relevancy": 2.5109, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5094}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5017}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-dimensional%20Persistent%20Sheaf%20Laplacians%20for%20Image%20Analysis&body=Title%3A%20Multi-dimensional%20Persistent%20Sheaf%20Laplacians%20for%20Image%20Analysis%0AAuthor%3A%20Xiang%20Xiang%20Wang%20and%20Guo-Wei%20Wei%0AAbstract%3A%20We%20propose%20a%20multi-dimensional%20persistent%20sheaf%20Laplacian%20%28MPSL%29%20framework%20on%20simplicial%20complexes%20for%20image%20analysis.%20The%20proposed%20method%20is%20motivated%20by%20the%20strong%20sensitivity%20of%20commonly%20used%20dimensionality%20reduction%20techniques%2C%20such%20as%20principal%20component%20analysis%20%28PCA%29%2C%20to%20the%20choice%20of%20reduced%20dimension.%20Rather%20than%20selecting%20a%20single%20reduced%20dimension%20or%20averaging%20results%20across%20dimensions%2C%20we%20exploit%20complementary%20advantages%20of%20multiple%20reduced%20dimensions.%20At%20a%20given%20dimension%2C%20image%20samples%20are%20regarded%20as%20simplicial%20complexes%2C%20and%20persistent%20sheaf%20Laplacians%20are%20utilized%20to%20extract%20a%20multiscale%20localized%20topological%20spectral%20representation%20for%20individual%20image%20samples.%20Statistical%20summaries%20of%20the%20resulting%20spectra%20are%20then%20aggregated%20across%20scales%20and%20dimensions%20to%20form%20multiscale%20multi-dimensional%20image%20representations.%20We%20evaluate%20the%20proposed%20framework%20on%20the%20COIL20%20and%20ETH80%20image%20datasets%20using%20standard%20classification%20protocols.%20Experimental%20results%20show%20that%20the%20proposed%20method%20provides%20more%20stable%20performance%20across%20a%20wide%20range%20of%20reduced%20dimensions%20and%20achieves%20consistent%20improvements%20to%20PCA-based%20baselines%20in%20moderate%20dimensional%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-dimensional%2520Persistent%2520Sheaf%2520Laplacians%2520for%2520Image%2520Analysis%26entry.906535625%3DXiang%2520Xiang%2520Wang%2520and%2520Guo-Wei%2520Wei%26entry.1292438233%3DWe%2520propose%2520a%2520multi-dimensional%2520persistent%2520sheaf%2520Laplacian%2520%2528MPSL%2529%2520framework%2520on%2520simplicial%2520complexes%2520for%2520image%2520analysis.%2520The%2520proposed%2520method%2520is%2520motivated%2520by%2520the%2520strong%2520sensitivity%2520of%2520commonly%2520used%2520dimensionality%2520reduction%2520techniques%252C%2520such%2520as%2520principal%2520component%2520analysis%2520%2528PCA%2529%252C%2520to%2520the%2520choice%2520of%2520reduced%2520dimension.%2520Rather%2520than%2520selecting%2520a%2520single%2520reduced%2520dimension%2520or%2520averaging%2520results%2520across%2520dimensions%252C%2520we%2520exploit%2520complementary%2520advantages%2520of%2520multiple%2520reduced%2520dimensions.%2520At%2520a%2520given%2520dimension%252C%2520image%2520samples%2520are%2520regarded%2520as%2520simplicial%2520complexes%252C%2520and%2520persistent%2520sheaf%2520Laplacians%2520are%2520utilized%2520to%2520extract%2520a%2520multiscale%2520localized%2520topological%2520spectral%2520representation%2520for%2520individual%2520image%2520samples.%2520Statistical%2520summaries%2520of%2520the%2520resulting%2520spectra%2520are%2520then%2520aggregated%2520across%2520scales%2520and%2520dimensions%2520to%2520form%2520multiscale%2520multi-dimensional%2520image%2520representations.%2520We%2520evaluate%2520the%2520proposed%2520framework%2520on%2520the%2520COIL20%2520and%2520ETH80%2520image%2520datasets%2520using%2520standard%2520classification%2520protocols.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520method%2520provides%2520more%2520stable%2520performance%2520across%2520a%2520wide%2520range%2520of%2520reduced%2520dimensions%2520and%2520achieves%2520consistent%2520improvements%2520to%2520PCA-based%2520baselines%2520in%2520moderate%2520dimensional%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-dimensional%20Persistent%20Sheaf%20Laplacians%20for%20Image%20Analysis&entry.906535625=Xiang%20Xiang%20Wang%20and%20Guo-Wei%20Wei&entry.1292438233=We%20propose%20a%20multi-dimensional%20persistent%20sheaf%20Laplacian%20%28MPSL%29%20framework%20on%20simplicial%20complexes%20for%20image%20analysis.%20The%20proposed%20method%20is%20motivated%20by%20the%20strong%20sensitivity%20of%20commonly%20used%20dimensionality%20reduction%20techniques%2C%20such%20as%20principal%20component%20analysis%20%28PCA%29%2C%20to%20the%20choice%20of%20reduced%20dimension.%20Rather%20than%20selecting%20a%20single%20reduced%20dimension%20or%20averaging%20results%20across%20dimensions%2C%20we%20exploit%20complementary%20advantages%20of%20multiple%20reduced%20dimensions.%20At%20a%20given%20dimension%2C%20image%20samples%20are%20regarded%20as%20simplicial%20complexes%2C%20and%20persistent%20sheaf%20Laplacians%20are%20utilized%20to%20extract%20a%20multiscale%20localized%20topological%20spectral%20representation%20for%20individual%20image%20samples.%20Statistical%20summaries%20of%20the%20resulting%20spectra%20are%20then%20aggregated%20across%20scales%20and%20dimensions%20to%20form%20multiscale%20multi-dimensional%20image%20representations.%20We%20evaluate%20the%20proposed%20framework%20on%20the%20COIL20%20and%20ETH80%20image%20datasets%20using%20standard%20classification%20protocols.%20Experimental%20results%20show%20that%20the%20proposed%20method%20provides%20more%20stable%20performance%20across%20a%20wide%20range%20of%20reduced%20dimensions%20and%20achieves%20consistent%20improvements%20to%20PCA-based%20baselines%20in%20moderate%20dimensional%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2602.14846v1&entry.124074799=Read"},
{"title": "Synergizing Foundation Models and Federated Learning: A Survey", "author": "Shenghui Li and Fanghua Ye and Meng Fang and Jiaxu Zhao and Yun-Hin Chan and Edith C. H. Ngai and Thiemo Voigt", "abstract": "Over the past few years, the landscape of Artificial Intelligence (AI) has been reshaped by the emergence of Foundation Models (FMs). Pre-trained on massive datasets, these models exhibit exceptional performance across diverse downstream tasks through adaptation techniques like fine-tuning and prompt learning. More recently, the synergy of FMs and Federated Learning (FL) has emerged as a promising paradigm, often termed Federated Foundation Models (FedFM), allowing for collaborative model adaptation while preserving data privacy. This survey paper provides a systematic review of the current state of the art in FedFM, offering insights and guidance into the evolving landscape. Specifically, we present a comprehensive multi-tiered taxonomy based on three major dimensions, namely efficiency, adaptability, and trustworthiness. To facilitate practical implementation and experimental research, we undertake a thorough review of existing libraries and benchmarks. Furthermore, we discuss the diverse real-world applications of this paradigm across multiple domains. Finally, we outline promising research directions to foster future advancements in FedFM. Overall, this survey serves as a resource for researchers and practitioners, offering a thorough understanding of FedFM's role in revolutionizing privacy-preserving AI and pointing toward future innovations in this promising area. A periodically updated paper collection on FM-FL is available at https://github.com/lishenghui/awesome-fm-fl.", "link": "http://arxiv.org/abs/2406.12844v2", "date": "2026-02-16", "relevancy": 2.4817, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synergizing%20Foundation%20Models%20and%20Federated%20Learning%3A%20A%20Survey&body=Title%3A%20Synergizing%20Foundation%20Models%20and%20Federated%20Learning%3A%20A%20Survey%0AAuthor%3A%20Shenghui%20Li%20and%20Fanghua%20Ye%20and%20Meng%20Fang%20and%20Jiaxu%20Zhao%20and%20Yun-Hin%20Chan%20and%20Edith%20C.%20H.%20Ngai%20and%20Thiemo%20Voigt%0AAbstract%3A%20Over%20the%20past%20few%20years%2C%20the%20landscape%20of%20Artificial%20Intelligence%20%28AI%29%20has%20been%20reshaped%20by%20the%20emergence%20of%20Foundation%20Models%20%28FMs%29.%20Pre-trained%20on%20massive%20datasets%2C%20these%20models%20exhibit%20exceptional%20performance%20across%20diverse%20downstream%20tasks%20through%20adaptation%20techniques%20like%20fine-tuning%20and%20prompt%20learning.%20More%20recently%2C%20the%20synergy%20of%20FMs%20and%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20paradigm%2C%20often%20termed%20Federated%20Foundation%20Models%20%28FedFM%29%2C%20allowing%20for%20collaborative%20model%20adaptation%20while%20preserving%20data%20privacy.%20This%20survey%20paper%20provides%20a%20systematic%20review%20of%20the%20current%20state%20of%20the%20art%20in%20FedFM%2C%20offering%20insights%20and%20guidance%20into%20the%20evolving%20landscape.%20Specifically%2C%20we%20present%20a%20comprehensive%20multi-tiered%20taxonomy%20based%20on%20three%20major%20dimensions%2C%20namely%20efficiency%2C%20adaptability%2C%20and%20trustworthiness.%20To%20facilitate%20practical%20implementation%20and%20experimental%20research%2C%20we%20undertake%20a%20thorough%20review%20of%20existing%20libraries%20and%20benchmarks.%20Furthermore%2C%20we%20discuss%20the%20diverse%20real-world%20applications%20of%20this%20paradigm%20across%20multiple%20domains.%20Finally%2C%20we%20outline%20promising%20research%20directions%20to%20foster%20future%20advancements%20in%20FedFM.%20Overall%2C%20this%20survey%20serves%20as%20a%20resource%20for%20researchers%20and%20practitioners%2C%20offering%20a%20thorough%20understanding%20of%20FedFM%27s%20role%20in%20revolutionizing%20privacy-preserving%20AI%20and%20pointing%20toward%20future%20innovations%20in%20this%20promising%20area.%20A%20periodically%20updated%20paper%20collection%20on%20FM-FL%20is%20available%20at%20https%3A//github.com/lishenghui/awesome-fm-fl.%0ALink%3A%20http%3A//arxiv.org/abs/2406.12844v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynergizing%2520Foundation%2520Models%2520and%2520Federated%2520Learning%253A%2520A%2520Survey%26entry.906535625%3DShenghui%2520Li%2520and%2520Fanghua%2520Ye%2520and%2520Meng%2520Fang%2520and%2520Jiaxu%2520Zhao%2520and%2520Yun-Hin%2520Chan%2520and%2520Edith%2520C.%2520H.%2520Ngai%2520and%2520Thiemo%2520Voigt%26entry.1292438233%3DOver%2520the%2520past%2520few%2520years%252C%2520the%2520landscape%2520of%2520Artificial%2520Intelligence%2520%2528AI%2529%2520has%2520been%2520reshaped%2520by%2520the%2520emergence%2520of%2520Foundation%2520Models%2520%2528FMs%2529.%2520Pre-trained%2520on%2520massive%2520datasets%252C%2520these%2520models%2520exhibit%2520exceptional%2520performance%2520across%2520diverse%2520downstream%2520tasks%2520through%2520adaptation%2520techniques%2520like%2520fine-tuning%2520and%2520prompt%2520learning.%2520More%2520recently%252C%2520the%2520synergy%2520of%2520FMs%2520and%2520Federated%2520Learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%252C%2520often%2520termed%2520Federated%2520Foundation%2520Models%2520%2528FedFM%2529%252C%2520allowing%2520for%2520collaborative%2520model%2520adaptation%2520while%2520preserving%2520data%2520privacy.%2520This%2520survey%2520paper%2520provides%2520a%2520systematic%2520review%2520of%2520the%2520current%2520state%2520of%2520the%2520art%2520in%2520FedFM%252C%2520offering%2520insights%2520and%2520guidance%2520into%2520the%2520evolving%2520landscape.%2520Specifically%252C%2520we%2520present%2520a%2520comprehensive%2520multi-tiered%2520taxonomy%2520based%2520on%2520three%2520major%2520dimensions%252C%2520namely%2520efficiency%252C%2520adaptability%252C%2520and%2520trustworthiness.%2520To%2520facilitate%2520practical%2520implementation%2520and%2520experimental%2520research%252C%2520we%2520undertake%2520a%2520thorough%2520review%2520of%2520existing%2520libraries%2520and%2520benchmarks.%2520Furthermore%252C%2520we%2520discuss%2520the%2520diverse%2520real-world%2520applications%2520of%2520this%2520paradigm%2520across%2520multiple%2520domains.%2520Finally%252C%2520we%2520outline%2520promising%2520research%2520directions%2520to%2520foster%2520future%2520advancements%2520in%2520FedFM.%2520Overall%252C%2520this%2520survey%2520serves%2520as%2520a%2520resource%2520for%2520researchers%2520and%2520practitioners%252C%2520offering%2520a%2520thorough%2520understanding%2520of%2520FedFM%2527s%2520role%2520in%2520revolutionizing%2520privacy-preserving%2520AI%2520and%2520pointing%2520toward%2520future%2520innovations%2520in%2520this%2520promising%2520area.%2520A%2520periodically%2520updated%2520paper%2520collection%2520on%2520FM-FL%2520is%2520available%2520at%2520https%253A//github.com/lishenghui/awesome-fm-fl.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12844v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synergizing%20Foundation%20Models%20and%20Federated%20Learning%3A%20A%20Survey&entry.906535625=Shenghui%20Li%20and%20Fanghua%20Ye%20and%20Meng%20Fang%20and%20Jiaxu%20Zhao%20and%20Yun-Hin%20Chan%20and%20Edith%20C.%20H.%20Ngai%20and%20Thiemo%20Voigt&entry.1292438233=Over%20the%20past%20few%20years%2C%20the%20landscape%20of%20Artificial%20Intelligence%20%28AI%29%20has%20been%20reshaped%20by%20the%20emergence%20of%20Foundation%20Models%20%28FMs%29.%20Pre-trained%20on%20massive%20datasets%2C%20these%20models%20exhibit%20exceptional%20performance%20across%20diverse%20downstream%20tasks%20through%20adaptation%20techniques%20like%20fine-tuning%20and%20prompt%20learning.%20More%20recently%2C%20the%20synergy%20of%20FMs%20and%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20paradigm%2C%20often%20termed%20Federated%20Foundation%20Models%20%28FedFM%29%2C%20allowing%20for%20collaborative%20model%20adaptation%20while%20preserving%20data%20privacy.%20This%20survey%20paper%20provides%20a%20systematic%20review%20of%20the%20current%20state%20of%20the%20art%20in%20FedFM%2C%20offering%20insights%20and%20guidance%20into%20the%20evolving%20landscape.%20Specifically%2C%20we%20present%20a%20comprehensive%20multi-tiered%20taxonomy%20based%20on%20three%20major%20dimensions%2C%20namely%20efficiency%2C%20adaptability%2C%20and%20trustworthiness.%20To%20facilitate%20practical%20implementation%20and%20experimental%20research%2C%20we%20undertake%20a%20thorough%20review%20of%20existing%20libraries%20and%20benchmarks.%20Furthermore%2C%20we%20discuss%20the%20diverse%20real-world%20applications%20of%20this%20paradigm%20across%20multiple%20domains.%20Finally%2C%20we%20outline%20promising%20research%20directions%20to%20foster%20future%20advancements%20in%20FedFM.%20Overall%2C%20this%20survey%20serves%20as%20a%20resource%20for%20researchers%20and%20practitioners%2C%20offering%20a%20thorough%20understanding%20of%20FedFM%27s%20role%20in%20revolutionizing%20privacy-preserving%20AI%20and%20pointing%20toward%20future%20innovations%20in%20this%20promising%20area.%20A%20periodically%20updated%20paper%20collection%20on%20FM-FL%20is%20available%20at%20https%3A//github.com/lishenghui/awesome-fm-fl.&entry.1838667208=http%3A//arxiv.org/abs/2406.12844v2&entry.124074799=Read"},
{"title": "Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center Dialogue Generation", "author": "Rishikesh Devanathan and Varun Nathan and Ayush Kumar", "abstract": "Synthetic data is increasingly critical for contact centers, where privacy constraints and data scarcity limit the availability of real conversations. However, generating synthetic dialogues that are realistic and useful for downstream applications remains challenging. In this work, we benchmark multiple generation strategies guided by structured supervision on call attributes (Intent Summaries, Topic Flows, and Quality Assurance (QA) Forms) across multiple languages. To test downstream utility, we evaluate synthetic transcripts on an automated quality assurance (AutoQA) task, finding that prompts optimized on real transcripts consistently outperform those optimized on synthetic transcripts. These results suggest that current synthetic transcripts fall short in capturing the full realism of real agent-customer interactions. To highlight these downstream gaps, we introduce a diagnostic evaluation framework comprising 17 metrics across four dimensions: (1) Emotional and Sentiment Arcs, (2) Linguistic Complexity, (3) Interaction Style, and (4) Conversational Properties. Our analysis shows that even with structured supervision, current generation strategies exhibit measurable deficiencies in sentiment fidelity, disfluency modeling, behavioral variation, and conversational realism. Together, these results highlight the importance of diagnostic, metric-driven evaluation for synthetic conversation generation intended for downstream applications.", "link": "http://arxiv.org/abs/2508.18210v2", "date": "2026-02-16", "relevancy": 2.4516, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4988}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4894}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Synthetic%20Isn%27t%20Real%20Yet%3A%20A%20Diagnostic%20Framework%20for%20Contact%20Center%20Dialogue%20Generation&body=Title%3A%20Why%20Synthetic%20Isn%27t%20Real%20Yet%3A%20A%20Diagnostic%20Framework%20for%20Contact%20Center%20Dialogue%20Generation%0AAuthor%3A%20Rishikesh%20Devanathan%20and%20Varun%20Nathan%20and%20Ayush%20Kumar%0AAbstract%3A%20Synthetic%20data%20is%20increasingly%20critical%20for%20contact%20centers%2C%20where%20privacy%20constraints%20and%20data%20scarcity%20limit%20the%20availability%20of%20real%20conversations.%20However%2C%20generating%20synthetic%20dialogues%20that%20are%20realistic%20and%20useful%20for%20downstream%20applications%20remains%20challenging.%20In%20this%20work%2C%20we%20benchmark%20multiple%20generation%20strategies%20guided%20by%20structured%20supervision%20on%20call%20attributes%20%28Intent%20Summaries%2C%20Topic%20Flows%2C%20and%20Quality%20Assurance%20%28QA%29%20Forms%29%20across%20multiple%20languages.%20To%20test%20downstream%20utility%2C%20we%20evaluate%20synthetic%20transcripts%20on%20an%20automated%20quality%20assurance%20%28AutoQA%29%20task%2C%20finding%20that%20prompts%20optimized%20on%20real%20transcripts%20consistently%20outperform%20those%20optimized%20on%20synthetic%20transcripts.%20These%20results%20suggest%20that%20current%20synthetic%20transcripts%20fall%20short%20in%20capturing%20the%20full%20realism%20of%20real%20agent-customer%20interactions.%20To%20highlight%20these%20downstream%20gaps%2C%20we%20introduce%20a%20diagnostic%20evaluation%20framework%20comprising%2017%20metrics%20across%20four%20dimensions%3A%20%281%29%20Emotional%20and%20Sentiment%20Arcs%2C%20%282%29%20Linguistic%20Complexity%2C%20%283%29%20Interaction%20Style%2C%20and%20%284%29%20Conversational%20Properties.%20Our%20analysis%20shows%20that%20even%20with%20structured%20supervision%2C%20current%20generation%20strategies%20exhibit%20measurable%20deficiencies%20in%20sentiment%20fidelity%2C%20disfluency%20modeling%2C%20behavioral%20variation%2C%20and%20conversational%20realism.%20Together%2C%20these%20results%20highlight%20the%20importance%20of%20diagnostic%2C%20metric-driven%20evaluation%20for%20synthetic%20conversation%20generation%20intended%20for%20downstream%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18210v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Synthetic%2520Isn%2527t%2520Real%2520Yet%253A%2520A%2520Diagnostic%2520Framework%2520for%2520Contact%2520Center%2520Dialogue%2520Generation%26entry.906535625%3DRishikesh%2520Devanathan%2520and%2520Varun%2520Nathan%2520and%2520Ayush%2520Kumar%26entry.1292438233%3DSynthetic%2520data%2520is%2520increasingly%2520critical%2520for%2520contact%2520centers%252C%2520where%2520privacy%2520constraints%2520and%2520data%2520scarcity%2520limit%2520the%2520availability%2520of%2520real%2520conversations.%2520However%252C%2520generating%2520synthetic%2520dialogues%2520that%2520are%2520realistic%2520and%2520useful%2520for%2520downstream%2520applications%2520remains%2520challenging.%2520In%2520this%2520work%252C%2520we%2520benchmark%2520multiple%2520generation%2520strategies%2520guided%2520by%2520structured%2520supervision%2520on%2520call%2520attributes%2520%2528Intent%2520Summaries%252C%2520Topic%2520Flows%252C%2520and%2520Quality%2520Assurance%2520%2528QA%2529%2520Forms%2529%2520across%2520multiple%2520languages.%2520To%2520test%2520downstream%2520utility%252C%2520we%2520evaluate%2520synthetic%2520transcripts%2520on%2520an%2520automated%2520quality%2520assurance%2520%2528AutoQA%2529%2520task%252C%2520finding%2520that%2520prompts%2520optimized%2520on%2520real%2520transcripts%2520consistently%2520outperform%2520those%2520optimized%2520on%2520synthetic%2520transcripts.%2520These%2520results%2520suggest%2520that%2520current%2520synthetic%2520transcripts%2520fall%2520short%2520in%2520capturing%2520the%2520full%2520realism%2520of%2520real%2520agent-customer%2520interactions.%2520To%2520highlight%2520these%2520downstream%2520gaps%252C%2520we%2520introduce%2520a%2520diagnostic%2520evaluation%2520framework%2520comprising%252017%2520metrics%2520across%2520four%2520dimensions%253A%2520%25281%2529%2520Emotional%2520and%2520Sentiment%2520Arcs%252C%2520%25282%2529%2520Linguistic%2520Complexity%252C%2520%25283%2529%2520Interaction%2520Style%252C%2520and%2520%25284%2529%2520Conversational%2520Properties.%2520Our%2520analysis%2520shows%2520that%2520even%2520with%2520structured%2520supervision%252C%2520current%2520generation%2520strategies%2520exhibit%2520measurable%2520deficiencies%2520in%2520sentiment%2520fidelity%252C%2520disfluency%2520modeling%252C%2520behavioral%2520variation%252C%2520and%2520conversational%2520realism.%2520Together%252C%2520these%2520results%2520highlight%2520the%2520importance%2520of%2520diagnostic%252C%2520metric-driven%2520evaluation%2520for%2520synthetic%2520conversation%2520generation%2520intended%2520for%2520downstream%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18210v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Synthetic%20Isn%27t%20Real%20Yet%3A%20A%20Diagnostic%20Framework%20for%20Contact%20Center%20Dialogue%20Generation&entry.906535625=Rishikesh%20Devanathan%20and%20Varun%20Nathan%20and%20Ayush%20Kumar&entry.1292438233=Synthetic%20data%20is%20increasingly%20critical%20for%20contact%20centers%2C%20where%20privacy%20constraints%20and%20data%20scarcity%20limit%20the%20availability%20of%20real%20conversations.%20However%2C%20generating%20synthetic%20dialogues%20that%20are%20realistic%20and%20useful%20for%20downstream%20applications%20remains%20challenging.%20In%20this%20work%2C%20we%20benchmark%20multiple%20generation%20strategies%20guided%20by%20structured%20supervision%20on%20call%20attributes%20%28Intent%20Summaries%2C%20Topic%20Flows%2C%20and%20Quality%20Assurance%20%28QA%29%20Forms%29%20across%20multiple%20languages.%20To%20test%20downstream%20utility%2C%20we%20evaluate%20synthetic%20transcripts%20on%20an%20automated%20quality%20assurance%20%28AutoQA%29%20task%2C%20finding%20that%20prompts%20optimized%20on%20real%20transcripts%20consistently%20outperform%20those%20optimized%20on%20synthetic%20transcripts.%20These%20results%20suggest%20that%20current%20synthetic%20transcripts%20fall%20short%20in%20capturing%20the%20full%20realism%20of%20real%20agent-customer%20interactions.%20To%20highlight%20these%20downstream%20gaps%2C%20we%20introduce%20a%20diagnostic%20evaluation%20framework%20comprising%2017%20metrics%20across%20four%20dimensions%3A%20%281%29%20Emotional%20and%20Sentiment%20Arcs%2C%20%282%29%20Linguistic%20Complexity%2C%20%283%29%20Interaction%20Style%2C%20and%20%284%29%20Conversational%20Properties.%20Our%20analysis%20shows%20that%20even%20with%20structured%20supervision%2C%20current%20generation%20strategies%20exhibit%20measurable%20deficiencies%20in%20sentiment%20fidelity%2C%20disfluency%20modeling%2C%20behavioral%20variation%2C%20and%20conversational%20realism.%20Together%2C%20these%20results%20highlight%20the%20importance%20of%20diagnostic%2C%20metric-driven%20evaluation%20for%20synthetic%20conversation%20generation%20intended%20for%20downstream%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2508.18210v2&entry.124074799=Read"},
{"title": "SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data", "author": "David Chanin and Adri\u00e0 Garriga-Alonso", "abstract": "Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, and a standardized benchmark model, SynthSAEBench-16k, enabling direct comparison of SAE architectures. Our benchmark reproduces several previously observed LLM SAE phenomena, including the disconnect between reconstruction and latent quality metrics, poor SAE probing results, and a precision-recall trade-off mediated by L0. We further use our benchmark to identify a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting that more expressive encoders can easily overfit. SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.", "link": "http://arxiv.org/abs/2602.14687v1", "date": "2026-02-16", "relevancy": 2.4511, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthSAEBench%3A%20Evaluating%20Sparse%20Autoencoders%20on%20Scalable%20Realistic%20Synthetic%20Data&body=Title%3A%20SynthSAEBench%3A%20Evaluating%20Sparse%20Autoencoders%20on%20Scalable%20Realistic%20Synthetic%20Data%0AAuthor%3A%20David%20Chanin%20and%20Adri%C3%A0%20Garriga-Alonso%0AAbstract%3A%20Improving%20Sparse%20Autoencoders%20%28SAEs%29%20requires%20benchmarks%20that%20can%20precisely%20validate%20architectural%20innovations.%20However%2C%20current%20SAE%20benchmarks%20on%20LLMs%20are%20often%20too%20noisy%20to%20differentiate%20architectural%20improvements%2C%20and%20current%20synthetic%20data%20experiments%20are%20too%20small-scale%20and%20unrealistic%20to%20provide%20meaningful%20comparisons.%20We%20introduce%20SynthSAEBench%2C%20a%20toolkit%20for%20generating%20large-scale%20synthetic%20data%20with%20realistic%20feature%20characteristics%20including%20correlation%2C%20hierarchy%2C%20and%20superposition%2C%20and%20a%20standardized%20benchmark%20model%2C%20SynthSAEBench-16k%2C%20enabling%20direct%20comparison%20of%20SAE%20architectures.%20Our%20benchmark%20reproduces%20several%20previously%20observed%20LLM%20SAE%20phenomena%2C%20including%20the%20disconnect%20between%20reconstruction%20and%20latent%20quality%20metrics%2C%20poor%20SAE%20probing%20results%2C%20and%20a%20precision-recall%20trade-off%20mediated%20by%20L0.%20We%20further%20use%20our%20benchmark%20to%20identify%20a%20new%20failure%20mode%3A%20Matching%20Pursuit%20SAEs%20exploit%20superposition%20noise%20to%20improve%20reconstruction%20without%20learning%20ground-truth%20features%2C%20suggesting%20that%20more%20expressive%20encoders%20can%20easily%20overfit.%20SynthSAEBench%20complements%20LLM%20benchmarks%20by%20providing%20ground-truth%20features%20and%20controlled%20ablations%2C%20enabling%20researchers%20to%20precisely%20diagnose%20SAE%20failure%20modes%20and%20validate%20architectural%20improvements%20before%20scaling%20to%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthSAEBench%253A%2520Evaluating%2520Sparse%2520Autoencoders%2520on%2520Scalable%2520Realistic%2520Synthetic%2520Data%26entry.906535625%3DDavid%2520Chanin%2520and%2520Adri%25C3%25A0%2520Garriga-Alonso%26entry.1292438233%3DImproving%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520requires%2520benchmarks%2520that%2520can%2520precisely%2520validate%2520architectural%2520innovations.%2520However%252C%2520current%2520SAE%2520benchmarks%2520on%2520LLMs%2520are%2520often%2520too%2520noisy%2520to%2520differentiate%2520architectural%2520improvements%252C%2520and%2520current%2520synthetic%2520data%2520experiments%2520are%2520too%2520small-scale%2520and%2520unrealistic%2520to%2520provide%2520meaningful%2520comparisons.%2520We%2520introduce%2520SynthSAEBench%252C%2520a%2520toolkit%2520for%2520generating%2520large-scale%2520synthetic%2520data%2520with%2520realistic%2520feature%2520characteristics%2520including%2520correlation%252C%2520hierarchy%252C%2520and%2520superposition%252C%2520and%2520a%2520standardized%2520benchmark%2520model%252C%2520SynthSAEBench-16k%252C%2520enabling%2520direct%2520comparison%2520of%2520SAE%2520architectures.%2520Our%2520benchmark%2520reproduces%2520several%2520previously%2520observed%2520LLM%2520SAE%2520phenomena%252C%2520including%2520the%2520disconnect%2520between%2520reconstruction%2520and%2520latent%2520quality%2520metrics%252C%2520poor%2520SAE%2520probing%2520results%252C%2520and%2520a%2520precision-recall%2520trade-off%2520mediated%2520by%2520L0.%2520We%2520further%2520use%2520our%2520benchmark%2520to%2520identify%2520a%2520new%2520failure%2520mode%253A%2520Matching%2520Pursuit%2520SAEs%2520exploit%2520superposition%2520noise%2520to%2520improve%2520reconstruction%2520without%2520learning%2520ground-truth%2520features%252C%2520suggesting%2520that%2520more%2520expressive%2520encoders%2520can%2520easily%2520overfit.%2520SynthSAEBench%2520complements%2520LLM%2520benchmarks%2520by%2520providing%2520ground-truth%2520features%2520and%2520controlled%2520ablations%252C%2520enabling%2520researchers%2520to%2520precisely%2520diagnose%2520SAE%2520failure%2520modes%2520and%2520validate%2520architectural%2520improvements%2520before%2520scaling%2520to%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthSAEBench%3A%20Evaluating%20Sparse%20Autoencoders%20on%20Scalable%20Realistic%20Synthetic%20Data&entry.906535625=David%20Chanin%20and%20Adri%C3%A0%20Garriga-Alonso&entry.1292438233=Improving%20Sparse%20Autoencoders%20%28SAEs%29%20requires%20benchmarks%20that%20can%20precisely%20validate%20architectural%20innovations.%20However%2C%20current%20SAE%20benchmarks%20on%20LLMs%20are%20often%20too%20noisy%20to%20differentiate%20architectural%20improvements%2C%20and%20current%20synthetic%20data%20experiments%20are%20too%20small-scale%20and%20unrealistic%20to%20provide%20meaningful%20comparisons.%20We%20introduce%20SynthSAEBench%2C%20a%20toolkit%20for%20generating%20large-scale%20synthetic%20data%20with%20realistic%20feature%20characteristics%20including%20correlation%2C%20hierarchy%2C%20and%20superposition%2C%20and%20a%20standardized%20benchmark%20model%2C%20SynthSAEBench-16k%2C%20enabling%20direct%20comparison%20of%20SAE%20architectures.%20Our%20benchmark%20reproduces%20several%20previously%20observed%20LLM%20SAE%20phenomena%2C%20including%20the%20disconnect%20between%20reconstruction%20and%20latent%20quality%20metrics%2C%20poor%20SAE%20probing%20results%2C%20and%20a%20precision-recall%20trade-off%20mediated%20by%20L0.%20We%20further%20use%20our%20benchmark%20to%20identify%20a%20new%20failure%20mode%3A%20Matching%20Pursuit%20SAEs%20exploit%20superposition%20noise%20to%20improve%20reconstruction%20without%20learning%20ground-truth%20features%2C%20suggesting%20that%20more%20expressive%20encoders%20can%20easily%20overfit.%20SynthSAEBench%20complements%20LLM%20benchmarks%20by%20providing%20ground-truth%20features%20and%20controlled%20ablations%2C%20enabling%20researchers%20to%20precisely%20diagnose%20SAE%20failure%20modes%20and%20validate%20architectural%20improvements%20before%20scaling%20to%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2602.14687v1&entry.124074799=Read"},
{"title": "AMAP-APP: Efficient Segmentation and Morphometry Quantification of Fluorescent Microscopy Images of Podocytes", "author": "Arash Fatehi and David Unnersj\u00f6-Jess and Linus Butt and No\u00e9mie Moreau and Thomas Benzing and Katarzyna Bozek", "abstract": "Background: Automated podocyte foot process quantification is vital for kidney research, but the established \"Automatic Morphological Analysis of Podocytes\" (AMAP) method is hindered by high computational demands, a lack of a user interface, and Linux dependency. We developed AMAP-APP, a cross-platform desktop application designed to overcome these barriers.\n  Methods: AMAP-APP optimizes efficiency by replacing intensive instance segmentation with classic image processing while retaining the original semantic segmentation model. It introduces a refined Region of Interest (ROI) algorithm to improve precision. Validation involved 365 mouse and human images (STED and confocal), benchmarking performance against the original AMAP via Pearson correlation and Two One-Sided T-tests (TOST).\n  Results: AMAP-APP achieved a 147-fold increase in processing speed on consumer hardware. Morphometric outputs (area, perimeter, circularity, and slit diaphragm density) showed high correlation (r>0.90) and statistical equivalence (TOST P<0.05) to the original method. Additionally, the new ROI algorithm demonstrated superior accuracy compared to the original, showing reduced deviation from manual delineations.\n  Conclusion: AMAP-APP democratizes deep learning-based podocyte morphometry. By eliminating the need for high-performance computing clusters and providing a user-friendly interface for Windows, macOS, and Linux, it enables widespread adoption in nephrology research and potential clinical diagnostics.", "link": "http://arxiv.org/abs/2602.10663v2", "date": "2026-02-16", "relevancy": 2.4469, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.493}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4876}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AMAP-APP%3A%20Efficient%20Segmentation%20and%20Morphometry%20Quantification%20of%20Fluorescent%20Microscopy%20Images%20of%20Podocytes&body=Title%3A%20AMAP-APP%3A%20Efficient%20Segmentation%20and%20Morphometry%20Quantification%20of%20Fluorescent%20Microscopy%20Images%20of%20Podocytes%0AAuthor%3A%20Arash%20Fatehi%20and%20David%20Unnersj%C3%B6-Jess%20and%20Linus%20Butt%20and%20No%C3%A9mie%20Moreau%20and%20Thomas%20Benzing%20and%20Katarzyna%20Bozek%0AAbstract%3A%20Background%3A%20Automated%20podocyte%20foot%20process%20quantification%20is%20vital%20for%20kidney%20research%2C%20but%20the%20established%20%22Automatic%20Morphological%20Analysis%20of%20Podocytes%22%20%28AMAP%29%20method%20is%20hindered%20by%20high%20computational%20demands%2C%20a%20lack%20of%20a%20user%20interface%2C%20and%20Linux%20dependency.%20We%20developed%20AMAP-APP%2C%20a%20cross-platform%20desktop%20application%20designed%20to%20overcome%20these%20barriers.%0A%20%20Methods%3A%20AMAP-APP%20optimizes%20efficiency%20by%20replacing%20intensive%20instance%20segmentation%20with%20classic%20image%20processing%20while%20retaining%20the%20original%20semantic%20segmentation%20model.%20It%20introduces%20a%20refined%20Region%20of%20Interest%20%28ROI%29%20algorithm%20to%20improve%20precision.%20Validation%20involved%20365%20mouse%20and%20human%20images%20%28STED%20and%20confocal%29%2C%20benchmarking%20performance%20against%20the%20original%20AMAP%20via%20Pearson%20correlation%20and%20Two%20One-Sided%20T-tests%20%28TOST%29.%0A%20%20Results%3A%20AMAP-APP%20achieved%20a%20147-fold%20increase%20in%20processing%20speed%20on%20consumer%20hardware.%20Morphometric%20outputs%20%28area%2C%20perimeter%2C%20circularity%2C%20and%20slit%20diaphragm%20density%29%20showed%20high%20correlation%20%28r%3E0.90%29%20and%20statistical%20equivalence%20%28TOST%20P%3C0.05%29%20to%20the%20original%20method.%20Additionally%2C%20the%20new%20ROI%20algorithm%20demonstrated%20superior%20accuracy%20compared%20to%20the%20original%2C%20showing%20reduced%20deviation%20from%20manual%20delineations.%0A%20%20Conclusion%3A%20AMAP-APP%20democratizes%20deep%20learning-based%20podocyte%20morphometry.%20By%20eliminating%20the%20need%20for%20high-performance%20computing%20clusters%20and%20providing%20a%20user-friendly%20interface%20for%20Windows%2C%20macOS%2C%20and%20Linux%2C%20it%20enables%20widespread%20adoption%20in%20nephrology%20research%20and%20potential%20clinical%20diagnostics.%0ALink%3A%20http%3A//arxiv.org/abs/2602.10663v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAMAP-APP%253A%2520Efficient%2520Segmentation%2520and%2520Morphometry%2520Quantification%2520of%2520Fluorescent%2520Microscopy%2520Images%2520of%2520Podocytes%26entry.906535625%3DArash%2520Fatehi%2520and%2520David%2520Unnersj%25C3%25B6-Jess%2520and%2520Linus%2520Butt%2520and%2520No%25C3%25A9mie%2520Moreau%2520and%2520Thomas%2520Benzing%2520and%2520Katarzyna%2520Bozek%26entry.1292438233%3DBackground%253A%2520Automated%2520podocyte%2520foot%2520process%2520quantification%2520is%2520vital%2520for%2520kidney%2520research%252C%2520but%2520the%2520established%2520%2522Automatic%2520Morphological%2520Analysis%2520of%2520Podocytes%2522%2520%2528AMAP%2529%2520method%2520is%2520hindered%2520by%2520high%2520computational%2520demands%252C%2520a%2520lack%2520of%2520a%2520user%2520interface%252C%2520and%2520Linux%2520dependency.%2520We%2520developed%2520AMAP-APP%252C%2520a%2520cross-platform%2520desktop%2520application%2520designed%2520to%2520overcome%2520these%2520barriers.%250A%2520%2520Methods%253A%2520AMAP-APP%2520optimizes%2520efficiency%2520by%2520replacing%2520intensive%2520instance%2520segmentation%2520with%2520classic%2520image%2520processing%2520while%2520retaining%2520the%2520original%2520semantic%2520segmentation%2520model.%2520It%2520introduces%2520a%2520refined%2520Region%2520of%2520Interest%2520%2528ROI%2529%2520algorithm%2520to%2520improve%2520precision.%2520Validation%2520involved%2520365%2520mouse%2520and%2520human%2520images%2520%2528STED%2520and%2520confocal%2529%252C%2520benchmarking%2520performance%2520against%2520the%2520original%2520AMAP%2520via%2520Pearson%2520correlation%2520and%2520Two%2520One-Sided%2520T-tests%2520%2528TOST%2529.%250A%2520%2520Results%253A%2520AMAP-APP%2520achieved%2520a%2520147-fold%2520increase%2520in%2520processing%2520speed%2520on%2520consumer%2520hardware.%2520Morphometric%2520outputs%2520%2528area%252C%2520perimeter%252C%2520circularity%252C%2520and%2520slit%2520diaphragm%2520density%2529%2520showed%2520high%2520correlation%2520%2528r%253E0.90%2529%2520and%2520statistical%2520equivalence%2520%2528TOST%2520P%253C0.05%2529%2520to%2520the%2520original%2520method.%2520Additionally%252C%2520the%2520new%2520ROI%2520algorithm%2520demonstrated%2520superior%2520accuracy%2520compared%2520to%2520the%2520original%252C%2520showing%2520reduced%2520deviation%2520from%2520manual%2520delineations.%250A%2520%2520Conclusion%253A%2520AMAP-APP%2520democratizes%2520deep%2520learning-based%2520podocyte%2520morphometry.%2520By%2520eliminating%2520the%2520need%2520for%2520high-performance%2520computing%2520clusters%2520and%2520providing%2520a%2520user-friendly%2520interface%2520for%2520Windows%252C%2520macOS%252C%2520and%2520Linux%252C%2520it%2520enables%2520widespread%2520adoption%2520in%2520nephrology%2520research%2520and%2520potential%2520clinical%2520diagnostics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.10663v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AMAP-APP%3A%20Efficient%20Segmentation%20and%20Morphometry%20Quantification%20of%20Fluorescent%20Microscopy%20Images%20of%20Podocytes&entry.906535625=Arash%20Fatehi%20and%20David%20Unnersj%C3%B6-Jess%20and%20Linus%20Butt%20and%20No%C3%A9mie%20Moreau%20and%20Thomas%20Benzing%20and%20Katarzyna%20Bozek&entry.1292438233=Background%3A%20Automated%20podocyte%20foot%20process%20quantification%20is%20vital%20for%20kidney%20research%2C%20but%20the%20established%20%22Automatic%20Morphological%20Analysis%20of%20Podocytes%22%20%28AMAP%29%20method%20is%20hindered%20by%20high%20computational%20demands%2C%20a%20lack%20of%20a%20user%20interface%2C%20and%20Linux%20dependency.%20We%20developed%20AMAP-APP%2C%20a%20cross-platform%20desktop%20application%20designed%20to%20overcome%20these%20barriers.%0A%20%20Methods%3A%20AMAP-APP%20optimizes%20efficiency%20by%20replacing%20intensive%20instance%20segmentation%20with%20classic%20image%20processing%20while%20retaining%20the%20original%20semantic%20segmentation%20model.%20It%20introduces%20a%20refined%20Region%20of%20Interest%20%28ROI%29%20algorithm%20to%20improve%20precision.%20Validation%20involved%20365%20mouse%20and%20human%20images%20%28STED%20and%20confocal%29%2C%20benchmarking%20performance%20against%20the%20original%20AMAP%20via%20Pearson%20correlation%20and%20Two%20One-Sided%20T-tests%20%28TOST%29.%0A%20%20Results%3A%20AMAP-APP%20achieved%20a%20147-fold%20increase%20in%20processing%20speed%20on%20consumer%20hardware.%20Morphometric%20outputs%20%28area%2C%20perimeter%2C%20circularity%2C%20and%20slit%20diaphragm%20density%29%20showed%20high%20correlation%20%28r%3E0.90%29%20and%20statistical%20equivalence%20%28TOST%20P%3C0.05%29%20to%20the%20original%20method.%20Additionally%2C%20the%20new%20ROI%20algorithm%20demonstrated%20superior%20accuracy%20compared%20to%20the%20original%2C%20showing%20reduced%20deviation%20from%20manual%20delineations.%0A%20%20Conclusion%3A%20AMAP-APP%20democratizes%20deep%20learning-based%20podocyte%20morphometry.%20By%20eliminating%20the%20need%20for%20high-performance%20computing%20clusters%20and%20providing%20a%20user-friendly%20interface%20for%20Windows%2C%20macOS%2C%20and%20Linux%2C%20it%20enables%20widespread%20adoption%20in%20nephrology%20research%20and%20potential%20clinical%20diagnostics.&entry.1838667208=http%3A//arxiv.org/abs/2602.10663v2&entry.124074799=Read"},
{"title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)", "author": "Nihal V. Nayak and Paula Rodriguez-Diaz and Neha Hulkund and Sara Beery and David Alvarez-Melis", "abstract": "Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.", "link": "http://arxiv.org/abs/2602.14696v1", "date": "2026-02-16", "relevancy": 2.442, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Critical%20Look%20at%20Targeted%20Instruction%20Selection%3A%20Disentangling%20What%20Matters%20%28and%20What%20Doesn%27t%29&body=Title%3A%20A%20Critical%20Look%20at%20Targeted%20Instruction%20Selection%3A%20Disentangling%20What%20Matters%20%28and%20What%20Doesn%27t%29%0AAuthor%3A%20Nihal%20V.%20Nayak%20and%20Paula%20Rodriguez-Diaz%20and%20Neha%20Hulkund%20and%20Sara%20Beery%20and%20David%20Alvarez-Melis%0AAbstract%3A%20Instruction%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29%20often%20involves%20selecting%20a%20subset%20of%20instruction%20training%20data%20from%20a%20large%20candidate%20pool%2C%20using%20a%20small%20query%20set%20from%20the%20target%20task.%20Despite%20growing%20interest%2C%20the%20literature%20on%20targeted%20instruction%20selection%20remains%20fragmented%20and%20opaque%3A%20methods%20vary%20widely%20in%20selection%20budgets%2C%20often%20omit%20zero-shot%20baselines%2C%20and%20frequently%20entangle%20the%20contributions%20of%20key%20components.%20As%20a%20result%2C%20practitioners%20lack%20actionable%20guidance%20on%20selecting%20instructions%20for%20their%20target%20tasks.%20In%20this%20work%2C%20we%20aim%20to%20bring%20clarity%20to%20this%20landscape%20by%20disentangling%20and%20systematically%20analyzing%20the%20two%20core%20ingredients%3A%20data%20representation%20and%20selection%20algorithms.%20Our%20framework%20enables%20controlled%20comparisons%20across%20models%2C%20tasks%2C%20and%20budgets.%20We%20find%20that%20only%20gradient-based%20data%20representations%20choose%20subsets%20whose%20similarity%20to%20the%20query%20consistently%20predicts%20performance%20across%20datasets%20and%20models.%20While%20no%20single%20method%20dominates%2C%20gradient-based%20representations%20paired%20with%20a%20greedy%20round-robin%20selection%20algorithm%20tend%20to%20perform%20best%20on%20average%20at%20low%20budgets%2C%20but%20these%20benefits%20diminish%20at%20larger%20budgets.%20Finally%2C%20we%20unify%20several%20existing%20selection%20algorithms%20as%20forms%20of%20approximate%20distance%20minimization%20between%20the%20selected%20subset%20and%20the%20query%20set%2C%20and%20support%20this%20view%20with%20new%20generalization%20bounds.%20More%20broadly%2C%20our%20findings%20provide%20critical%20insights%20and%20a%20foundation%20for%20more%20principled%20data%20selection%20in%20LLM%20fine-tuning.%20The%20code%20is%20available%20at%20https%3A//github.com/dcml-lab/targeted-instruction-selection.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Critical%2520Look%2520at%2520Targeted%2520Instruction%2520Selection%253A%2520Disentangling%2520What%2520Matters%2520%2528and%2520What%2520Doesn%2527t%2529%26entry.906535625%3DNihal%2520V.%2520Nayak%2520and%2520Paula%2520Rodriguez-Diaz%2520and%2520Neha%2520Hulkund%2520and%2520Sara%2520Beery%2520and%2520David%2520Alvarez-Melis%26entry.1292438233%3DInstruction%2520fine-tuning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520often%2520involves%2520selecting%2520a%2520subset%2520of%2520instruction%2520training%2520data%2520from%2520a%2520large%2520candidate%2520pool%252C%2520using%2520a%2520small%2520query%2520set%2520from%2520the%2520target%2520task.%2520Despite%2520growing%2520interest%252C%2520the%2520literature%2520on%2520targeted%2520instruction%2520selection%2520remains%2520fragmented%2520and%2520opaque%253A%2520methods%2520vary%2520widely%2520in%2520selection%2520budgets%252C%2520often%2520omit%2520zero-shot%2520baselines%252C%2520and%2520frequently%2520entangle%2520the%2520contributions%2520of%2520key%2520components.%2520As%2520a%2520result%252C%2520practitioners%2520lack%2520actionable%2520guidance%2520on%2520selecting%2520instructions%2520for%2520their%2520target%2520tasks.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520bring%2520clarity%2520to%2520this%2520landscape%2520by%2520disentangling%2520and%2520systematically%2520analyzing%2520the%2520two%2520core%2520ingredients%253A%2520data%2520representation%2520and%2520selection%2520algorithms.%2520Our%2520framework%2520enables%2520controlled%2520comparisons%2520across%2520models%252C%2520tasks%252C%2520and%2520budgets.%2520We%2520find%2520that%2520only%2520gradient-based%2520data%2520representations%2520choose%2520subsets%2520whose%2520similarity%2520to%2520the%2520query%2520consistently%2520predicts%2520performance%2520across%2520datasets%2520and%2520models.%2520While%2520no%2520single%2520method%2520dominates%252C%2520gradient-based%2520representations%2520paired%2520with%2520a%2520greedy%2520round-robin%2520selection%2520algorithm%2520tend%2520to%2520perform%2520best%2520on%2520average%2520at%2520low%2520budgets%252C%2520but%2520these%2520benefits%2520diminish%2520at%2520larger%2520budgets.%2520Finally%252C%2520we%2520unify%2520several%2520existing%2520selection%2520algorithms%2520as%2520forms%2520of%2520approximate%2520distance%2520minimization%2520between%2520the%2520selected%2520subset%2520and%2520the%2520query%2520set%252C%2520and%2520support%2520this%2520view%2520with%2520new%2520generalization%2520bounds.%2520More%2520broadly%252C%2520our%2520findings%2520provide%2520critical%2520insights%2520and%2520a%2520foundation%2520for%2520more%2520principled%2520data%2520selection%2520in%2520LLM%2520fine-tuning.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/dcml-lab/targeted-instruction-selection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Critical%20Look%20at%20Targeted%20Instruction%20Selection%3A%20Disentangling%20What%20Matters%20%28and%20What%20Doesn%27t%29&entry.906535625=Nihal%20V.%20Nayak%20and%20Paula%20Rodriguez-Diaz%20and%20Neha%20Hulkund%20and%20Sara%20Beery%20and%20David%20Alvarez-Melis&entry.1292438233=Instruction%20fine-tuning%20of%20large%20language%20models%20%28LLMs%29%20often%20involves%20selecting%20a%20subset%20of%20instruction%20training%20data%20from%20a%20large%20candidate%20pool%2C%20using%20a%20small%20query%20set%20from%20the%20target%20task.%20Despite%20growing%20interest%2C%20the%20literature%20on%20targeted%20instruction%20selection%20remains%20fragmented%20and%20opaque%3A%20methods%20vary%20widely%20in%20selection%20budgets%2C%20often%20omit%20zero-shot%20baselines%2C%20and%20frequently%20entangle%20the%20contributions%20of%20key%20components.%20As%20a%20result%2C%20practitioners%20lack%20actionable%20guidance%20on%20selecting%20instructions%20for%20their%20target%20tasks.%20In%20this%20work%2C%20we%20aim%20to%20bring%20clarity%20to%20this%20landscape%20by%20disentangling%20and%20systematically%20analyzing%20the%20two%20core%20ingredients%3A%20data%20representation%20and%20selection%20algorithms.%20Our%20framework%20enables%20controlled%20comparisons%20across%20models%2C%20tasks%2C%20and%20budgets.%20We%20find%20that%20only%20gradient-based%20data%20representations%20choose%20subsets%20whose%20similarity%20to%20the%20query%20consistently%20predicts%20performance%20across%20datasets%20and%20models.%20While%20no%20single%20method%20dominates%2C%20gradient-based%20representations%20paired%20with%20a%20greedy%20round-robin%20selection%20algorithm%20tend%20to%20perform%20best%20on%20average%20at%20low%20budgets%2C%20but%20these%20benefits%20diminish%20at%20larger%20budgets.%20Finally%2C%20we%20unify%20several%20existing%20selection%20algorithms%20as%20forms%20of%20approximate%20distance%20minimization%20between%20the%20selected%20subset%20and%20the%20query%20set%2C%20and%20support%20this%20view%20with%20new%20generalization%20bounds.%20More%20broadly%2C%20our%20findings%20provide%20critical%20insights%20and%20a%20foundation%20for%20more%20principled%20data%20selection%20in%20LLM%20fine-tuning.%20The%20code%20is%20available%20at%20https%3A//github.com/dcml-lab/targeted-instruction-selection.&entry.1838667208=http%3A//arxiv.org/abs/2602.14696v1&entry.124074799=Read"},
{"title": "LLMStructBench: Benchmarking Large Language Model Structured Data Extraction", "author": "S\u00f6nke Tenckhoff and Mario Koddenbrock and Erik Rodner", "abstract": "We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.\n  In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.", "link": "http://arxiv.org/abs/2602.14743v1", "date": "2026-02-16", "relevancy": 2.4116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5061}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5061}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4347}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMStructBench%3A%20Benchmarking%20Large%20Language%20Model%20Structured%20Data%20Extraction&body=Title%3A%20LLMStructBench%3A%20Benchmarking%20Large%20Language%20Model%20Structured%20Data%20Extraction%0AAuthor%3A%20S%C3%B6nke%20Tenckhoff%20and%20Mario%20Koddenbrock%20and%20Erik%20Rodner%0AAbstract%3A%20We%20present%20LLMStructBench%2C%20a%20novel%20benchmark%20for%20evaluating%20Large%20Language%20Models%20%28LLMs%29%20on%20extracting%20structured%20data%20and%20generating%20valid%20JavaScript%20Object%20Notation%20%28JSON%29%20outputs%20from%20natural-language%20text.%20Our%20open%20dataset%20comprises%20diverse%2C%20manually%20verified%20parsing%20scenarios%20of%20varying%20complexity%20and%20enables%20systematic%20testing%20across%2022%20models%20and%20five%20prompting%20strategies.%20We%20further%20introduce%20complementary%20performance%20metrics%20that%20capture%20both%20token-level%20accuracy%20and%20document-level%20validity%2C%20facilitating%20rigorous%20comparison%20of%20model%2C%20size%2C%20and%20prompting%20effects%20on%20parsing%20reliability.%0A%20%20In%20particular%2C%20we%20show%20that%20choosing%20the%20right%20prompting%20strategy%20is%20more%20important%20than%20standard%20attributes%20such%20as%20model%20size.%20This%20especially%20ensures%20structural%20validity%20for%20smaller%20or%20less%20reliable%20models%20but%20increase%20the%20number%20of%20semantic%20errors.%20Our%20benchmark%20suite%20is%20an%20step%20towards%20future%20research%20in%20the%20area%20of%20LLM%20applied%20to%20parsing%20or%20Extract%2C%20Transform%20and%20Load%20%28ETL%29%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMStructBench%253A%2520Benchmarking%2520Large%2520Language%2520Model%2520Structured%2520Data%2520Extraction%26entry.906535625%3DS%25C3%25B6nke%2520Tenckhoff%2520and%2520Mario%2520Koddenbrock%2520and%2520Erik%2520Rodner%26entry.1292438233%3DWe%2520present%2520LLMStructBench%252C%2520a%2520novel%2520benchmark%2520for%2520evaluating%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520extracting%2520structured%2520data%2520and%2520generating%2520valid%2520JavaScript%2520Object%2520Notation%2520%2528JSON%2529%2520outputs%2520from%2520natural-language%2520text.%2520Our%2520open%2520dataset%2520comprises%2520diverse%252C%2520manually%2520verified%2520parsing%2520scenarios%2520of%2520varying%2520complexity%2520and%2520enables%2520systematic%2520testing%2520across%252022%2520models%2520and%2520five%2520prompting%2520strategies.%2520We%2520further%2520introduce%2520complementary%2520performance%2520metrics%2520that%2520capture%2520both%2520token-level%2520accuracy%2520and%2520document-level%2520validity%252C%2520facilitating%2520rigorous%2520comparison%2520of%2520model%252C%2520size%252C%2520and%2520prompting%2520effects%2520on%2520parsing%2520reliability.%250A%2520%2520In%2520particular%252C%2520we%2520show%2520that%2520choosing%2520the%2520right%2520prompting%2520strategy%2520is%2520more%2520important%2520than%2520standard%2520attributes%2520such%2520as%2520model%2520size.%2520This%2520especially%2520ensures%2520structural%2520validity%2520for%2520smaller%2520or%2520less%2520reliable%2520models%2520but%2520increase%2520the%2520number%2520of%2520semantic%2520errors.%2520Our%2520benchmark%2520suite%2520is%2520an%2520step%2520towards%2520future%2520research%2520in%2520the%2520area%2520of%2520LLM%2520applied%2520to%2520parsing%2520or%2520Extract%252C%2520Transform%2520and%2520Load%2520%2528ETL%2529%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMStructBench%3A%20Benchmarking%20Large%20Language%20Model%20Structured%20Data%20Extraction&entry.906535625=S%C3%B6nke%20Tenckhoff%20and%20Mario%20Koddenbrock%20and%20Erik%20Rodner&entry.1292438233=We%20present%20LLMStructBench%2C%20a%20novel%20benchmark%20for%20evaluating%20Large%20Language%20Models%20%28LLMs%29%20on%20extracting%20structured%20data%20and%20generating%20valid%20JavaScript%20Object%20Notation%20%28JSON%29%20outputs%20from%20natural-language%20text.%20Our%20open%20dataset%20comprises%20diverse%2C%20manually%20verified%20parsing%20scenarios%20of%20varying%20complexity%20and%20enables%20systematic%20testing%20across%2022%20models%20and%20five%20prompting%20strategies.%20We%20further%20introduce%20complementary%20performance%20metrics%20that%20capture%20both%20token-level%20accuracy%20and%20document-level%20validity%2C%20facilitating%20rigorous%20comparison%20of%20model%2C%20size%2C%20and%20prompting%20effects%20on%20parsing%20reliability.%0A%20%20In%20particular%2C%20we%20show%20that%20choosing%20the%20right%20prompting%20strategy%20is%20more%20important%20than%20standard%20attributes%20such%20as%20model%20size.%20This%20especially%20ensures%20structural%20validity%20for%20smaller%20or%20less%20reliable%20models%20but%20increase%20the%20number%20of%20semantic%20errors.%20Our%20benchmark%20suite%20is%20an%20step%20towards%20future%20research%20in%20the%20area%20of%20LLM%20applied%20to%20parsing%20or%20Extract%2C%20Transform%20and%20Load%20%28ETL%29%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2602.14743v1&entry.124074799=Read"},
{"title": "Fault Detection in Electrical Distribution System using Autoencoders", "author": "Sidharthenee Nayak and Victor Sam Moses Babu and Chandrashekhar Narayan Bhende and Pratyush Chakraborty and Mayukha Pal", "abstract": "In recent times, there has been considerable interest in fault detection within electrical power systems, garnering attention from both academic researchers and industry professionals. Despite the development of numerous fault detection methods and their adaptations over the past decade, their practical application remains highly challenging. Given the probabilistic nature of fault occurrences and parameters, certain decision-making tasks could be approached from a probabilistic standpoint. Protective systems are tasked with the detection, classification, and localization of faulty voltage and current line magnitudes, culminating in the activation of circuit breakers to isolate the faulty line. An essential aspect of designing effective fault detection systems lies in obtaining reliable data for training and testing, which is often scarce. Leveraging deep learning techniques, particularly the powerful capabilities of pattern classifiers in learning, generalizing, and parallel processing, offers promising avenues for intelligent fault detection. To address this, our paper proposes an anomaly-based approach for fault detection in electrical power systems, employing deep autoencoders. Additionally, we utilize Convolutional Autoencoders (CAE) for dimensionality reduction, which, due to its fewer parameters, requires less training time compared to conventional autoencoders. The proposed method demonstrates superior performance and accuracy compared to alternative detection approaches by achieving an accuracy of 97.62% and 99.92% on simulated and publicly available datasets.", "link": "http://arxiv.org/abs/2602.14939v1", "date": "2026-02-16", "relevancy": 2.4112, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4908}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4824}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fault%20Detection%20in%20Electrical%20Distribution%20System%20using%20Autoencoders&body=Title%3A%20Fault%20Detection%20in%20Electrical%20Distribution%20System%20using%20Autoencoders%0AAuthor%3A%20Sidharthenee%20Nayak%20and%20Victor%20Sam%20Moses%20Babu%20and%20Chandrashekhar%20Narayan%20Bhende%20and%20Pratyush%20Chakraborty%20and%20Mayukha%20Pal%0AAbstract%3A%20In%20recent%20times%2C%20there%20has%20been%20considerable%20interest%20in%20fault%20detection%20within%20electrical%20power%20systems%2C%20garnering%20attention%20from%20both%20academic%20researchers%20and%20industry%20professionals.%20Despite%20the%20development%20of%20numerous%20fault%20detection%20methods%20and%20their%20adaptations%20over%20the%20past%20decade%2C%20their%20practical%20application%20remains%20highly%20challenging.%20Given%20the%20probabilistic%20nature%20of%20fault%20occurrences%20and%20parameters%2C%20certain%20decision-making%20tasks%20could%20be%20approached%20from%20a%20probabilistic%20standpoint.%20Protective%20systems%20are%20tasked%20with%20the%20detection%2C%20classification%2C%20and%20localization%20of%20faulty%20voltage%20and%20current%20line%20magnitudes%2C%20culminating%20in%20the%20activation%20of%20circuit%20breakers%20to%20isolate%20the%20faulty%20line.%20An%20essential%20aspect%20of%20designing%20effective%20fault%20detection%20systems%20lies%20in%20obtaining%20reliable%20data%20for%20training%20and%20testing%2C%20which%20is%20often%20scarce.%20Leveraging%20deep%20learning%20techniques%2C%20particularly%20the%20powerful%20capabilities%20of%20pattern%20classifiers%20in%20learning%2C%20generalizing%2C%20and%20parallel%20processing%2C%20offers%20promising%20avenues%20for%20intelligent%20fault%20detection.%20To%20address%20this%2C%20our%20paper%20proposes%20an%20anomaly-based%20approach%20for%20fault%20detection%20in%20electrical%20power%20systems%2C%20employing%20deep%20autoencoders.%20Additionally%2C%20we%20utilize%20Convolutional%20Autoencoders%20%28CAE%29%20for%20dimensionality%20reduction%2C%20which%2C%20due%20to%20its%20fewer%20parameters%2C%20requires%20less%20training%20time%20compared%20to%20conventional%20autoencoders.%20The%20proposed%20method%20demonstrates%20superior%20performance%20and%20accuracy%20compared%20to%20alternative%20detection%20approaches%20by%20achieving%20an%20accuracy%20of%2097.62%25%20and%2099.92%25%20on%20simulated%20and%20publicly%20available%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14939v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFault%2520Detection%2520in%2520Electrical%2520Distribution%2520System%2520using%2520Autoencoders%26entry.906535625%3DSidharthenee%2520Nayak%2520and%2520Victor%2520Sam%2520Moses%2520Babu%2520and%2520Chandrashekhar%2520Narayan%2520Bhende%2520and%2520Pratyush%2520Chakraborty%2520and%2520Mayukha%2520Pal%26entry.1292438233%3DIn%2520recent%2520times%252C%2520there%2520has%2520been%2520considerable%2520interest%2520in%2520fault%2520detection%2520within%2520electrical%2520power%2520systems%252C%2520garnering%2520attention%2520from%2520both%2520academic%2520researchers%2520and%2520industry%2520professionals.%2520Despite%2520the%2520development%2520of%2520numerous%2520fault%2520detection%2520methods%2520and%2520their%2520adaptations%2520over%2520the%2520past%2520decade%252C%2520their%2520practical%2520application%2520remains%2520highly%2520challenging.%2520Given%2520the%2520probabilistic%2520nature%2520of%2520fault%2520occurrences%2520and%2520parameters%252C%2520certain%2520decision-making%2520tasks%2520could%2520be%2520approached%2520from%2520a%2520probabilistic%2520standpoint.%2520Protective%2520systems%2520are%2520tasked%2520with%2520the%2520detection%252C%2520classification%252C%2520and%2520localization%2520of%2520faulty%2520voltage%2520and%2520current%2520line%2520magnitudes%252C%2520culminating%2520in%2520the%2520activation%2520of%2520circuit%2520breakers%2520to%2520isolate%2520the%2520faulty%2520line.%2520An%2520essential%2520aspect%2520of%2520designing%2520effective%2520fault%2520detection%2520systems%2520lies%2520in%2520obtaining%2520reliable%2520data%2520for%2520training%2520and%2520testing%252C%2520which%2520is%2520often%2520scarce.%2520Leveraging%2520deep%2520learning%2520techniques%252C%2520particularly%2520the%2520powerful%2520capabilities%2520of%2520pattern%2520classifiers%2520in%2520learning%252C%2520generalizing%252C%2520and%2520parallel%2520processing%252C%2520offers%2520promising%2520avenues%2520for%2520intelligent%2520fault%2520detection.%2520To%2520address%2520this%252C%2520our%2520paper%2520proposes%2520an%2520anomaly-based%2520approach%2520for%2520fault%2520detection%2520in%2520electrical%2520power%2520systems%252C%2520employing%2520deep%2520autoencoders.%2520Additionally%252C%2520we%2520utilize%2520Convolutional%2520Autoencoders%2520%2528CAE%2529%2520for%2520dimensionality%2520reduction%252C%2520which%252C%2520due%2520to%2520its%2520fewer%2520parameters%252C%2520requires%2520less%2520training%2520time%2520compared%2520to%2520conventional%2520autoencoders.%2520The%2520proposed%2520method%2520demonstrates%2520superior%2520performance%2520and%2520accuracy%2520compared%2520to%2520alternative%2520detection%2520approaches%2520by%2520achieving%2520an%2520accuracy%2520of%252097.62%2525%2520and%252099.92%2525%2520on%2520simulated%2520and%2520publicly%2520available%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14939v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fault%20Detection%20in%20Electrical%20Distribution%20System%20using%20Autoencoders&entry.906535625=Sidharthenee%20Nayak%20and%20Victor%20Sam%20Moses%20Babu%20and%20Chandrashekhar%20Narayan%20Bhende%20and%20Pratyush%20Chakraborty%20and%20Mayukha%20Pal&entry.1292438233=In%20recent%20times%2C%20there%20has%20been%20considerable%20interest%20in%20fault%20detection%20within%20electrical%20power%20systems%2C%20garnering%20attention%20from%20both%20academic%20researchers%20and%20industry%20professionals.%20Despite%20the%20development%20of%20numerous%20fault%20detection%20methods%20and%20their%20adaptations%20over%20the%20past%20decade%2C%20their%20practical%20application%20remains%20highly%20challenging.%20Given%20the%20probabilistic%20nature%20of%20fault%20occurrences%20and%20parameters%2C%20certain%20decision-making%20tasks%20could%20be%20approached%20from%20a%20probabilistic%20standpoint.%20Protective%20systems%20are%20tasked%20with%20the%20detection%2C%20classification%2C%20and%20localization%20of%20faulty%20voltage%20and%20current%20line%20magnitudes%2C%20culminating%20in%20the%20activation%20of%20circuit%20breakers%20to%20isolate%20the%20faulty%20line.%20An%20essential%20aspect%20of%20designing%20effective%20fault%20detection%20systems%20lies%20in%20obtaining%20reliable%20data%20for%20training%20and%20testing%2C%20which%20is%20often%20scarce.%20Leveraging%20deep%20learning%20techniques%2C%20particularly%20the%20powerful%20capabilities%20of%20pattern%20classifiers%20in%20learning%2C%20generalizing%2C%20and%20parallel%20processing%2C%20offers%20promising%20avenues%20for%20intelligent%20fault%20detection.%20To%20address%20this%2C%20our%20paper%20proposes%20an%20anomaly-based%20approach%20for%20fault%20detection%20in%20electrical%20power%20systems%2C%20employing%20deep%20autoencoders.%20Additionally%2C%20we%20utilize%20Convolutional%20Autoencoders%20%28CAE%29%20for%20dimensionality%20reduction%2C%20which%2C%20due%20to%20its%20fewer%20parameters%2C%20requires%20less%20training%20time%20compared%20to%20conventional%20autoencoders.%20The%20proposed%20method%20demonstrates%20superior%20performance%20and%20accuracy%20compared%20to%20alternative%20detection%20approaches%20by%20achieving%20an%20accuracy%20of%2097.62%25%20and%2099.92%25%20on%20simulated%20and%20publicly%20available%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2602.14939v1&entry.124074799=Read"},
{"title": "Exploring the limits of pre-trained embeddings in machine-guided protein design: a case study on predicting AAV vector viability", "author": "Ana F. Rodrigues and Lucas Ferraz and Laura Balbi and Pedro Giesteira Cotovio and Catia Pesquita", "abstract": "Effective representations of protein sequences are widely recognized as a cornerstone of machine learning-based protein design. Yet, protein bioengineering poses unique challenges for sequence representation, as experimental datasets typically feature few mutations, which are either sparsely distributed across the entire sequence or densely concentrated within localized regions. This limits the ability of sequence-level representations to extract functionally meaningful signals. In addition, comprehensive comparative studies remain scarce, despite their crucial role in clarifying which representations best encode relevant information and ultimately support superior predictive performance. In this study, we systematically evaluate multiple ProtBERT and ESM2 embedding variants as sequence representations, using the adeno-associated virus capsid as a case study and prototypical example of bioengineering, where functional optimization is targeted through highly localized sequence variation within an otherwise large protein. Our results reveal that, prior to fine-tuning, amino acid-level embeddings outperform sequence-level representations in supervised predictive tasks, whereas the latter tend to be more effective in unsupervised settings. However, optimal performance is only achieved when embeddings are fine-tuned with task-specific labels, with sequence-level representations providing the best performance. Moreover, our findings indicate that the extent of sequence variation required to produce notable shifts in sequence representations exceeds what is typically explored in bioengineering studies, showing the need for fine-tuning in datasets characterized by sparse or highly localized mutations.", "link": "http://arxiv.org/abs/2602.14828v1", "date": "2026-02-16", "relevancy": 2.4029, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4886}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20limits%20of%20pre-trained%20embeddings%20in%20machine-guided%20protein%20design%3A%20a%20case%20study%20on%20predicting%20AAV%20vector%20viability&body=Title%3A%20Exploring%20the%20limits%20of%20pre-trained%20embeddings%20in%20machine-guided%20protein%20design%3A%20a%20case%20study%20on%20predicting%20AAV%20vector%20viability%0AAuthor%3A%20Ana%20F.%20Rodrigues%20and%20Lucas%20Ferraz%20and%20Laura%20Balbi%20and%20Pedro%20Giesteira%20Cotovio%20and%20Catia%20Pesquita%0AAbstract%3A%20Effective%20representations%20of%20protein%20sequences%20are%20widely%20recognized%20as%20a%20cornerstone%20of%20machine%20learning-based%20protein%20design.%20Yet%2C%20protein%20bioengineering%20poses%20unique%20challenges%20for%20sequence%20representation%2C%20as%20experimental%20datasets%20typically%20feature%20few%20mutations%2C%20which%20are%20either%20sparsely%20distributed%20across%20the%20entire%20sequence%20or%20densely%20concentrated%20within%20localized%20regions.%20This%20limits%20the%20ability%20of%20sequence-level%20representations%20to%20extract%20functionally%20meaningful%20signals.%20In%20addition%2C%20comprehensive%20comparative%20studies%20remain%20scarce%2C%20despite%20their%20crucial%20role%20in%20clarifying%20which%20representations%20best%20encode%20relevant%20information%20and%20ultimately%20support%20superior%20predictive%20performance.%20In%20this%20study%2C%20we%20systematically%20evaluate%20multiple%20ProtBERT%20and%20ESM2%20embedding%20variants%20as%20sequence%20representations%2C%20using%20the%20adeno-associated%20virus%20capsid%20as%20a%20case%20study%20and%20prototypical%20example%20of%20bioengineering%2C%20where%20functional%20optimization%20is%20targeted%20through%20highly%20localized%20sequence%20variation%20within%20an%20otherwise%20large%20protein.%20Our%20results%20reveal%20that%2C%20prior%20to%20fine-tuning%2C%20amino%20acid-level%20embeddings%20outperform%20sequence-level%20representations%20in%20supervised%20predictive%20tasks%2C%20whereas%20the%20latter%20tend%20to%20be%20more%20effective%20in%20unsupervised%20settings.%20However%2C%20optimal%20performance%20is%20only%20achieved%20when%20embeddings%20are%20fine-tuned%20with%20task-specific%20labels%2C%20with%20sequence-level%20representations%20providing%20the%20best%20performance.%20Moreover%2C%20our%20findings%20indicate%20that%20the%20extent%20of%20sequence%20variation%20required%20to%20produce%20notable%20shifts%20in%20sequence%20representations%20exceeds%20what%20is%20typically%20explored%20in%20bioengineering%20studies%2C%20showing%20the%20need%20for%20fine-tuning%20in%20datasets%20characterized%20by%20sparse%20or%20highly%20localized%20mutations.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520limits%2520of%2520pre-trained%2520embeddings%2520in%2520machine-guided%2520protein%2520design%253A%2520a%2520case%2520study%2520on%2520predicting%2520AAV%2520vector%2520viability%26entry.906535625%3DAna%2520F.%2520Rodrigues%2520and%2520Lucas%2520Ferraz%2520and%2520Laura%2520Balbi%2520and%2520Pedro%2520Giesteira%2520Cotovio%2520and%2520Catia%2520Pesquita%26entry.1292438233%3DEffective%2520representations%2520of%2520protein%2520sequences%2520are%2520widely%2520recognized%2520as%2520a%2520cornerstone%2520of%2520machine%2520learning-based%2520protein%2520design.%2520Yet%252C%2520protein%2520bioengineering%2520poses%2520unique%2520challenges%2520for%2520sequence%2520representation%252C%2520as%2520experimental%2520datasets%2520typically%2520feature%2520few%2520mutations%252C%2520which%2520are%2520either%2520sparsely%2520distributed%2520across%2520the%2520entire%2520sequence%2520or%2520densely%2520concentrated%2520within%2520localized%2520regions.%2520This%2520limits%2520the%2520ability%2520of%2520sequence-level%2520representations%2520to%2520extract%2520functionally%2520meaningful%2520signals.%2520In%2520addition%252C%2520comprehensive%2520comparative%2520studies%2520remain%2520scarce%252C%2520despite%2520their%2520crucial%2520role%2520in%2520clarifying%2520which%2520representations%2520best%2520encode%2520relevant%2520information%2520and%2520ultimately%2520support%2520superior%2520predictive%2520performance.%2520In%2520this%2520study%252C%2520we%2520systematically%2520evaluate%2520multiple%2520ProtBERT%2520and%2520ESM2%2520embedding%2520variants%2520as%2520sequence%2520representations%252C%2520using%2520the%2520adeno-associated%2520virus%2520capsid%2520as%2520a%2520case%2520study%2520and%2520prototypical%2520example%2520of%2520bioengineering%252C%2520where%2520functional%2520optimization%2520is%2520targeted%2520through%2520highly%2520localized%2520sequence%2520variation%2520within%2520an%2520otherwise%2520large%2520protein.%2520Our%2520results%2520reveal%2520that%252C%2520prior%2520to%2520fine-tuning%252C%2520amino%2520acid-level%2520embeddings%2520outperform%2520sequence-level%2520representations%2520in%2520supervised%2520predictive%2520tasks%252C%2520whereas%2520the%2520latter%2520tend%2520to%2520be%2520more%2520effective%2520in%2520unsupervised%2520settings.%2520However%252C%2520optimal%2520performance%2520is%2520only%2520achieved%2520when%2520embeddings%2520are%2520fine-tuned%2520with%2520task-specific%2520labels%252C%2520with%2520sequence-level%2520representations%2520providing%2520the%2520best%2520performance.%2520Moreover%252C%2520our%2520findings%2520indicate%2520that%2520the%2520extent%2520of%2520sequence%2520variation%2520required%2520to%2520produce%2520notable%2520shifts%2520in%2520sequence%2520representations%2520exceeds%2520what%2520is%2520typically%2520explored%2520in%2520bioengineering%2520studies%252C%2520showing%2520the%2520need%2520for%2520fine-tuning%2520in%2520datasets%2520characterized%2520by%2520sparse%2520or%2520highly%2520localized%2520mutations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20limits%20of%20pre-trained%20embeddings%20in%20machine-guided%20protein%20design%3A%20a%20case%20study%20on%20predicting%20AAV%20vector%20viability&entry.906535625=Ana%20F.%20Rodrigues%20and%20Lucas%20Ferraz%20and%20Laura%20Balbi%20and%20Pedro%20Giesteira%20Cotovio%20and%20Catia%20Pesquita&entry.1292438233=Effective%20representations%20of%20protein%20sequences%20are%20widely%20recognized%20as%20a%20cornerstone%20of%20machine%20learning-based%20protein%20design.%20Yet%2C%20protein%20bioengineering%20poses%20unique%20challenges%20for%20sequence%20representation%2C%20as%20experimental%20datasets%20typically%20feature%20few%20mutations%2C%20which%20are%20either%20sparsely%20distributed%20across%20the%20entire%20sequence%20or%20densely%20concentrated%20within%20localized%20regions.%20This%20limits%20the%20ability%20of%20sequence-level%20representations%20to%20extract%20functionally%20meaningful%20signals.%20In%20addition%2C%20comprehensive%20comparative%20studies%20remain%20scarce%2C%20despite%20their%20crucial%20role%20in%20clarifying%20which%20representations%20best%20encode%20relevant%20information%20and%20ultimately%20support%20superior%20predictive%20performance.%20In%20this%20study%2C%20we%20systematically%20evaluate%20multiple%20ProtBERT%20and%20ESM2%20embedding%20variants%20as%20sequence%20representations%2C%20using%20the%20adeno-associated%20virus%20capsid%20as%20a%20case%20study%20and%20prototypical%20example%20of%20bioengineering%2C%20where%20functional%20optimization%20is%20targeted%20through%20highly%20localized%20sequence%20variation%20within%20an%20otherwise%20large%20protein.%20Our%20results%20reveal%20that%2C%20prior%20to%20fine-tuning%2C%20amino%20acid-level%20embeddings%20outperform%20sequence-level%20representations%20in%20supervised%20predictive%20tasks%2C%20whereas%20the%20latter%20tend%20to%20be%20more%20effective%20in%20unsupervised%20settings.%20However%2C%20optimal%20performance%20is%20only%20achieved%20when%20embeddings%20are%20fine-tuned%20with%20task-specific%20labels%2C%20with%20sequence-level%20representations%20providing%20the%20best%20performance.%20Moreover%2C%20our%20findings%20indicate%20that%20the%20extent%20of%20sequence%20variation%20required%20to%20produce%20notable%20shifts%20in%20sequence%20representations%20exceeds%20what%20is%20typically%20explored%20in%20bioengineering%20studies%2C%20showing%20the%20need%20for%20fine-tuning%20in%20datasets%20characterized%20by%20sparse%20or%20highly%20localized%20mutations.&entry.1838667208=http%3A//arxiv.org/abs/2602.14828v1&entry.124074799=Read"},
{"title": "PAct: Part-Decomposed Single-View Articulated Object Generation", "author": "Qingming Liu and Xinyue Yao and Shuyuan Zhang and Yueci Deng and Guiliang Liu and Zhen Liu and Kui Jia", "abstract": "Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.", "link": "http://arxiv.org/abs/2602.14965v1", "date": "2026-02-16", "relevancy": 2.3681, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5885}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAct%3A%20Part-Decomposed%20Single-View%20Articulated%20Object%20Generation&body=Title%3A%20PAct%3A%20Part-Decomposed%20Single-View%20Articulated%20Object%20Generation%0AAuthor%3A%20Qingming%20Liu%20and%20Xinyue%20Yao%20and%20Shuyuan%20Zhang%20and%20Yueci%20Deng%20and%20Guiliang%20Liu%20and%20Zhen%20Liu%20and%20Kui%20Jia%0AAbstract%3A%20Articulated%20objects%20are%20central%20to%20interactive%203D%20applications%2C%20including%20embodied%20AI%2C%20robotics%2C%20and%20VR/AR%2C%20where%20functional%20part%20decomposition%20and%20kinematic%20motion%20are%20essential.%20Yet%20producing%20high-fidelity%20articulated%20assets%20remains%20difficult%20to%20scale%20because%20it%20requires%20reliable%20part%20decomposition%20and%20kinematic%20rigging.%20Existing%20approaches%20largely%20fall%20into%20two%20paradigms%3A%20optimization-based%20reconstruction%20or%20distillation%2C%20which%20can%20be%20accurate%20but%20often%20takes%20tens%20of%20minutes%20to%20hours%20per%20instance%2C%20and%20inference-time%20methods%20that%20rely%20on%20template%20or%20part%20retrieval%2C%20producing%20plausible%20results%20that%20may%20not%20match%20the%20specific%20structure%20and%20appearance%20in%20the%20input%20observation.%20We%20introduce%20a%20part-centric%20generative%20framework%20for%20articulated%20object%20creation%20that%20synthesizes%20part%20geometry%2C%20composition%2C%20and%20articulation%20under%20explicit%20part-aware%20conditioning.%20Our%20representation%20models%20an%20object%20as%20a%20set%20of%20movable%20parts%2C%20each%20encoded%20by%20latent%20tokens%20augmented%20with%20part%20identity%20and%20articulation%20cues.%20Conditioned%20on%20a%20single%20image%2C%20the%20model%20generates%20articulated%203D%20assets%20that%20preserve%20instance-level%20correspondence%20while%20maintaining%20valid%20part%20structure%20and%20motion.%20The%20resulting%20approach%20avoids%20per-instance%20optimization%2C%20enables%20fast%20feed-forward%20inference%2C%20and%20supports%20controllable%20assembly%20and%20articulation%2C%20which%20are%20important%20for%20embodied%20interaction.%20Experiments%20on%20common%20articulated%20categories%20%28e.g.%2C%20drawers%20and%20doors%29%20show%20improved%20input%20consistency%2C%20part%20accuracy%2C%20and%20articulation%20plausibility%20over%20optimization-based%20and%20retrieval-driven%20baselines%2C%20while%20substantially%20reducing%20inference%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAct%253A%2520Part-Decomposed%2520Single-View%2520Articulated%2520Object%2520Generation%26entry.906535625%3DQingming%2520Liu%2520and%2520Xinyue%2520Yao%2520and%2520Shuyuan%2520Zhang%2520and%2520Yueci%2520Deng%2520and%2520Guiliang%2520Liu%2520and%2520Zhen%2520Liu%2520and%2520Kui%2520Jia%26entry.1292438233%3DArticulated%2520objects%2520are%2520central%2520to%2520interactive%25203D%2520applications%252C%2520including%2520embodied%2520AI%252C%2520robotics%252C%2520and%2520VR/AR%252C%2520where%2520functional%2520part%2520decomposition%2520and%2520kinematic%2520motion%2520are%2520essential.%2520Yet%2520producing%2520high-fidelity%2520articulated%2520assets%2520remains%2520difficult%2520to%2520scale%2520because%2520it%2520requires%2520reliable%2520part%2520decomposition%2520and%2520kinematic%2520rigging.%2520Existing%2520approaches%2520largely%2520fall%2520into%2520two%2520paradigms%253A%2520optimization-based%2520reconstruction%2520or%2520distillation%252C%2520which%2520can%2520be%2520accurate%2520but%2520often%2520takes%2520tens%2520of%2520minutes%2520to%2520hours%2520per%2520instance%252C%2520and%2520inference-time%2520methods%2520that%2520rely%2520on%2520template%2520or%2520part%2520retrieval%252C%2520producing%2520plausible%2520results%2520that%2520may%2520not%2520match%2520the%2520specific%2520structure%2520and%2520appearance%2520in%2520the%2520input%2520observation.%2520We%2520introduce%2520a%2520part-centric%2520generative%2520framework%2520for%2520articulated%2520object%2520creation%2520that%2520synthesizes%2520part%2520geometry%252C%2520composition%252C%2520and%2520articulation%2520under%2520explicit%2520part-aware%2520conditioning.%2520Our%2520representation%2520models%2520an%2520object%2520as%2520a%2520set%2520of%2520movable%2520parts%252C%2520each%2520encoded%2520by%2520latent%2520tokens%2520augmented%2520with%2520part%2520identity%2520and%2520articulation%2520cues.%2520Conditioned%2520on%2520a%2520single%2520image%252C%2520the%2520model%2520generates%2520articulated%25203D%2520assets%2520that%2520preserve%2520instance-level%2520correspondence%2520while%2520maintaining%2520valid%2520part%2520structure%2520and%2520motion.%2520The%2520resulting%2520approach%2520avoids%2520per-instance%2520optimization%252C%2520enables%2520fast%2520feed-forward%2520inference%252C%2520and%2520supports%2520controllable%2520assembly%2520and%2520articulation%252C%2520which%2520are%2520important%2520for%2520embodied%2520interaction.%2520Experiments%2520on%2520common%2520articulated%2520categories%2520%2528e.g.%252C%2520drawers%2520and%2520doors%2529%2520show%2520improved%2520input%2520consistency%252C%2520part%2520accuracy%252C%2520and%2520articulation%2520plausibility%2520over%2520optimization-based%2520and%2520retrieval-driven%2520baselines%252C%2520while%2520substantially%2520reducing%2520inference%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAct%3A%20Part-Decomposed%20Single-View%20Articulated%20Object%20Generation&entry.906535625=Qingming%20Liu%20and%20Xinyue%20Yao%20and%20Shuyuan%20Zhang%20and%20Yueci%20Deng%20and%20Guiliang%20Liu%20and%20Zhen%20Liu%20and%20Kui%20Jia&entry.1292438233=Articulated%20objects%20are%20central%20to%20interactive%203D%20applications%2C%20including%20embodied%20AI%2C%20robotics%2C%20and%20VR/AR%2C%20where%20functional%20part%20decomposition%20and%20kinematic%20motion%20are%20essential.%20Yet%20producing%20high-fidelity%20articulated%20assets%20remains%20difficult%20to%20scale%20because%20it%20requires%20reliable%20part%20decomposition%20and%20kinematic%20rigging.%20Existing%20approaches%20largely%20fall%20into%20two%20paradigms%3A%20optimization-based%20reconstruction%20or%20distillation%2C%20which%20can%20be%20accurate%20but%20often%20takes%20tens%20of%20minutes%20to%20hours%20per%20instance%2C%20and%20inference-time%20methods%20that%20rely%20on%20template%20or%20part%20retrieval%2C%20producing%20plausible%20results%20that%20may%20not%20match%20the%20specific%20structure%20and%20appearance%20in%20the%20input%20observation.%20We%20introduce%20a%20part-centric%20generative%20framework%20for%20articulated%20object%20creation%20that%20synthesizes%20part%20geometry%2C%20composition%2C%20and%20articulation%20under%20explicit%20part-aware%20conditioning.%20Our%20representation%20models%20an%20object%20as%20a%20set%20of%20movable%20parts%2C%20each%20encoded%20by%20latent%20tokens%20augmented%20with%20part%20identity%20and%20articulation%20cues.%20Conditioned%20on%20a%20single%20image%2C%20the%20model%20generates%20articulated%203D%20assets%20that%20preserve%20instance-level%20correspondence%20while%20maintaining%20valid%20part%20structure%20and%20motion.%20The%20resulting%20approach%20avoids%20per-instance%20optimization%2C%20enables%20fast%20feed-forward%20inference%2C%20and%20supports%20controllable%20assembly%20and%20articulation%2C%20which%20are%20important%20for%20embodied%20interaction.%20Experiments%20on%20common%20articulated%20categories%20%28e.g.%2C%20drawers%20and%20doors%29%20show%20improved%20input%20consistency%2C%20part%20accuracy%2C%20and%20articulation%20plausibility%20over%20optimization-based%20and%20retrieval-driven%20baselines%2C%20while%20substantially%20reducing%20inference%20time.&entry.1838667208=http%3A//arxiv.org/abs/2602.14965v1&entry.124074799=Read"},
{"title": "The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach", "author": "Nizar El Ghazal and Antoine Caubri\u00e8re and Valentin Vielzeuf", "abstract": "This paper presents a comparative study of context management strategies for end-to-end Spoken Dialog State Tracking using Speech-LLMs. We systematically evaluate traditional multimodal context (combining text history and spoken current turn), full spoken history, and compressed spoken history approaches. Our experiments on the SpokenWOZ corpus demonstrate that providing the full spoken conversation as input yields the highest performance among models of similar size, significantly surpassing prior methods. Furthermore, we show that attention-pooling-based compression of the spoken history offers a strong trade-off, maintaining competitive accuracy with reduced context size. Detailed analysis confirms that improvements stem from more effective context utilization.", "link": "http://arxiv.org/abs/2510.09424v2", "date": "2026-02-16", "relevancy": 2.3671, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Speech-LLM%20Takes%20It%20All%3A%20A%20Truly%20Fully%20End-to-End%20Spoken%20Dialogue%20State%20Tracking%20Approach&body=Title%3A%20The%20Speech-LLM%20Takes%20It%20All%3A%20A%20Truly%20Fully%20End-to-End%20Spoken%20Dialogue%20State%20Tracking%20Approach%0AAuthor%3A%20Nizar%20El%20Ghazal%20and%20Antoine%20Caubri%C3%A8re%20and%20Valentin%20Vielzeuf%0AAbstract%3A%20This%20paper%20presents%20a%20comparative%20study%20of%20context%20management%20strategies%20for%20end-to-end%20Spoken%20Dialog%20State%20Tracking%20using%20Speech-LLMs.%20We%20systematically%20evaluate%20traditional%20multimodal%20context%20%28combining%20text%20history%20and%20spoken%20current%20turn%29%2C%20full%20spoken%20history%2C%20and%20compressed%20spoken%20history%20approaches.%20Our%20experiments%20on%20the%20SpokenWOZ%20corpus%20demonstrate%20that%20providing%20the%20full%20spoken%20conversation%20as%20input%20yields%20the%20highest%20performance%20among%20models%20of%20similar%20size%2C%20significantly%20surpassing%20prior%20methods.%20Furthermore%2C%20we%20show%20that%20attention-pooling-based%20compression%20of%20the%20spoken%20history%20offers%20a%20strong%20trade-off%2C%20maintaining%20competitive%20accuracy%20with%20reduced%20context%20size.%20Detailed%20analysis%20confirms%20that%20improvements%20stem%20from%20more%20effective%20context%20utilization.%0ALink%3A%20http%3A//arxiv.org/abs/2510.09424v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Speech-LLM%2520Takes%2520It%2520All%253A%2520A%2520Truly%2520Fully%2520End-to-End%2520Spoken%2520Dialogue%2520State%2520Tracking%2520Approach%26entry.906535625%3DNizar%2520El%2520Ghazal%2520and%2520Antoine%2520Caubri%25C3%25A8re%2520and%2520Valentin%2520Vielzeuf%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520comparative%2520study%2520of%2520context%2520management%2520strategies%2520for%2520end-to-end%2520Spoken%2520Dialog%2520State%2520Tracking%2520using%2520Speech-LLMs.%2520We%2520systematically%2520evaluate%2520traditional%2520multimodal%2520context%2520%2528combining%2520text%2520history%2520and%2520spoken%2520current%2520turn%2529%252C%2520full%2520spoken%2520history%252C%2520and%2520compressed%2520spoken%2520history%2520approaches.%2520Our%2520experiments%2520on%2520the%2520SpokenWOZ%2520corpus%2520demonstrate%2520that%2520providing%2520the%2520full%2520spoken%2520conversation%2520as%2520input%2520yields%2520the%2520highest%2520performance%2520among%2520models%2520of%2520similar%2520size%252C%2520significantly%2520surpassing%2520prior%2520methods.%2520Furthermore%252C%2520we%2520show%2520that%2520attention-pooling-based%2520compression%2520of%2520the%2520spoken%2520history%2520offers%2520a%2520strong%2520trade-off%252C%2520maintaining%2520competitive%2520accuracy%2520with%2520reduced%2520context%2520size.%2520Detailed%2520analysis%2520confirms%2520that%2520improvements%2520stem%2520from%2520more%2520effective%2520context%2520utilization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09424v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Speech-LLM%20Takes%20It%20All%3A%20A%20Truly%20Fully%20End-to-End%20Spoken%20Dialogue%20State%20Tracking%20Approach&entry.906535625=Nizar%20El%20Ghazal%20and%20Antoine%20Caubri%C3%A8re%20and%20Valentin%20Vielzeuf&entry.1292438233=This%20paper%20presents%20a%20comparative%20study%20of%20context%20management%20strategies%20for%20end-to-end%20Spoken%20Dialog%20State%20Tracking%20using%20Speech-LLMs.%20We%20systematically%20evaluate%20traditional%20multimodal%20context%20%28combining%20text%20history%20and%20spoken%20current%20turn%29%2C%20full%20spoken%20history%2C%20and%20compressed%20spoken%20history%20approaches.%20Our%20experiments%20on%20the%20SpokenWOZ%20corpus%20demonstrate%20that%20providing%20the%20full%20spoken%20conversation%20as%20input%20yields%20the%20highest%20performance%20among%20models%20of%20similar%20size%2C%20significantly%20surpassing%20prior%20methods.%20Furthermore%2C%20we%20show%20that%20attention-pooling-based%20compression%20of%20the%20spoken%20history%20offers%20a%20strong%20trade-off%2C%20maintaining%20competitive%20accuracy%20with%20reduced%20context%20size.%20Detailed%20analysis%20confirms%20that%20improvements%20stem%20from%20more%20effective%20context%20utilization.&entry.1838667208=http%3A//arxiv.org/abs/2510.09424v2&entry.124074799=Read"},
{"title": "3DRot: Rediscovering the Missing Primitive for RGB-Based 3D Augmentation", "author": "Shitian Yang and Deyu Li and Xiaoke Jiang and Lei Zhang", "abstract": "RGB-based 3D tasks, e.g., 3D detection, depth estimation, 3D keypoint estimation, still suffer from scarce, expensive annotations and a thin augmentation toolbox, since many image transforms, including rotations and warps, disrupt geometric consistency. While horizontal flipping and color jitter are standard, rigorous 3D rotation augmentation has surprisingly remained absent from RGB-based pipelines, largely due to the misconception that it requires scene depth or scene reconstruction. In this paper, we introduce 3DRot, a plug-and-play augmentation that rotates and mirrors images about the camera's optical center while synchronously updating RGB images, camera intrinsics, object poses, and 3D annotations to preserve projective geometry, achieving geometry-consistent rotations and reflections without relying on any scene depth. We first validate 3DRot on a classical RGB-based 3D task, monocular 3D detection. On SUN RGB-D, inserting 3DRot into a frozen DINO-X + Cube R-CNN pipeline raises $IoU_{3D}$ from 43.21 to 44.51, cuts rotation error (ROT) from 22.91$^\\circ$ to 20.93$^\\circ$, and boosts $mAP_{0.5}$ from 35.70 to 38.11; smaller but consistent gains appear on a cross-domain IN10 split. Beyond monocular detection, adding 3DRot on top of the standard BTS augmentation schedule further improves NYU Depth v2 from 0.1783 to 0.1685 in abs-rel (and 0.7472 to 0.7548 in $\u03b4<1.25$), and reduces cross-dataset error on SUN RGB-D. On KITTI, applying the same camera-centric rotations in MVX-Net (LiDAR+RGB) raises moderate 3D AP from about 63.85 to 65.16 while remaining compatible with standard 3D augmentations.", "link": "http://arxiv.org/abs/2508.01423v3", "date": "2026-02-16", "relevancy": 2.3599, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6281}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5703}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DRot%3A%20Rediscovering%20the%20Missing%20Primitive%20for%20RGB-Based%203D%20Augmentation&body=Title%3A%203DRot%3A%20Rediscovering%20the%20Missing%20Primitive%20for%20RGB-Based%203D%20Augmentation%0AAuthor%3A%20Shitian%20Yang%20and%20Deyu%20Li%20and%20Xiaoke%20Jiang%20and%20Lei%20Zhang%0AAbstract%3A%20RGB-based%203D%20tasks%2C%20e.g.%2C%203D%20detection%2C%20depth%20estimation%2C%203D%20keypoint%20estimation%2C%20still%20suffer%20from%20scarce%2C%20expensive%20annotations%20and%20a%20thin%20augmentation%20toolbox%2C%20since%20many%20image%20transforms%2C%20including%20rotations%20and%20warps%2C%20disrupt%20geometric%20consistency.%20While%20horizontal%20flipping%20and%20color%20jitter%20are%20standard%2C%20rigorous%203D%20rotation%20augmentation%20has%20surprisingly%20remained%20absent%20from%20RGB-based%20pipelines%2C%20largely%20due%20to%20the%20misconception%20that%20it%20requires%20scene%20depth%20or%20scene%20reconstruction.%20In%20this%20paper%2C%20we%20introduce%203DRot%2C%20a%20plug-and-play%20augmentation%20that%20rotates%20and%20mirrors%20images%20about%20the%20camera%27s%20optical%20center%20while%20synchronously%20updating%20RGB%20images%2C%20camera%20intrinsics%2C%20object%20poses%2C%20and%203D%20annotations%20to%20preserve%20projective%20geometry%2C%20achieving%20geometry-consistent%20rotations%20and%20reflections%20without%20relying%20on%20any%20scene%20depth.%20We%20first%20validate%203DRot%20on%20a%20classical%20RGB-based%203D%20task%2C%20monocular%203D%20detection.%20On%20SUN%20RGB-D%2C%20inserting%203DRot%20into%20a%20frozen%20DINO-X%20%2B%20Cube%20R-CNN%20pipeline%20raises%20%24IoU_%7B3D%7D%24%20from%2043.21%20to%2044.51%2C%20cuts%20rotation%20error%20%28ROT%29%20from%2022.91%24%5E%5Ccirc%24%20to%2020.93%24%5E%5Ccirc%24%2C%20and%20boosts%20%24mAP_%7B0.5%7D%24%20from%2035.70%20to%2038.11%3B%20smaller%20but%20consistent%20gains%20appear%20on%20a%20cross-domain%20IN10%20split.%20Beyond%20monocular%20detection%2C%20adding%203DRot%20on%20top%20of%20the%20standard%20BTS%20augmentation%20schedule%20further%20improves%20NYU%20Depth%20v2%20from%200.1783%20to%200.1685%20in%20abs-rel%20%28and%200.7472%20to%200.7548%20in%20%24%CE%B4%3C1.25%24%29%2C%20and%20reduces%20cross-dataset%20error%20on%20SUN%20RGB-D.%20On%20KITTI%2C%20applying%20the%20same%20camera-centric%20rotations%20in%20MVX-Net%20%28LiDAR%2BRGB%29%20raises%20moderate%203D%20AP%20from%20about%2063.85%20to%2065.16%20while%20remaining%20compatible%20with%20standard%203D%20augmentations.%0ALink%3A%20http%3A//arxiv.org/abs/2508.01423v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DRot%253A%2520Rediscovering%2520the%2520Missing%2520Primitive%2520for%2520RGB-Based%25203D%2520Augmentation%26entry.906535625%3DShitian%2520Yang%2520and%2520Deyu%2520Li%2520and%2520Xiaoke%2520Jiang%2520and%2520Lei%2520Zhang%26entry.1292438233%3DRGB-based%25203D%2520tasks%252C%2520e.g.%252C%25203D%2520detection%252C%2520depth%2520estimation%252C%25203D%2520keypoint%2520estimation%252C%2520still%2520suffer%2520from%2520scarce%252C%2520expensive%2520annotations%2520and%2520a%2520thin%2520augmentation%2520toolbox%252C%2520since%2520many%2520image%2520transforms%252C%2520including%2520rotations%2520and%2520warps%252C%2520disrupt%2520geometric%2520consistency.%2520While%2520horizontal%2520flipping%2520and%2520color%2520jitter%2520are%2520standard%252C%2520rigorous%25203D%2520rotation%2520augmentation%2520has%2520surprisingly%2520remained%2520absent%2520from%2520RGB-based%2520pipelines%252C%2520largely%2520due%2520to%2520the%2520misconception%2520that%2520it%2520requires%2520scene%2520depth%2520or%2520scene%2520reconstruction.%2520In%2520this%2520paper%252C%2520we%2520introduce%25203DRot%252C%2520a%2520plug-and-play%2520augmentation%2520that%2520rotates%2520and%2520mirrors%2520images%2520about%2520the%2520camera%2527s%2520optical%2520center%2520while%2520synchronously%2520updating%2520RGB%2520images%252C%2520camera%2520intrinsics%252C%2520object%2520poses%252C%2520and%25203D%2520annotations%2520to%2520preserve%2520projective%2520geometry%252C%2520achieving%2520geometry-consistent%2520rotations%2520and%2520reflections%2520without%2520relying%2520on%2520any%2520scene%2520depth.%2520We%2520first%2520validate%25203DRot%2520on%2520a%2520classical%2520RGB-based%25203D%2520task%252C%2520monocular%25203D%2520detection.%2520On%2520SUN%2520RGB-D%252C%2520inserting%25203DRot%2520into%2520a%2520frozen%2520DINO-X%2520%252B%2520Cube%2520R-CNN%2520pipeline%2520raises%2520%2524IoU_%257B3D%257D%2524%2520from%252043.21%2520to%252044.51%252C%2520cuts%2520rotation%2520error%2520%2528ROT%2529%2520from%252022.91%2524%255E%255Ccirc%2524%2520to%252020.93%2524%255E%255Ccirc%2524%252C%2520and%2520boosts%2520%2524mAP_%257B0.5%257D%2524%2520from%252035.70%2520to%252038.11%253B%2520smaller%2520but%2520consistent%2520gains%2520appear%2520on%2520a%2520cross-domain%2520IN10%2520split.%2520Beyond%2520monocular%2520detection%252C%2520adding%25203DRot%2520on%2520top%2520of%2520the%2520standard%2520BTS%2520augmentation%2520schedule%2520further%2520improves%2520NYU%2520Depth%2520v2%2520from%25200.1783%2520to%25200.1685%2520in%2520abs-rel%2520%2528and%25200.7472%2520to%25200.7548%2520in%2520%2524%25CE%25B4%253C1.25%2524%2529%252C%2520and%2520reduces%2520cross-dataset%2520error%2520on%2520SUN%2520RGB-D.%2520On%2520KITTI%252C%2520applying%2520the%2520same%2520camera-centric%2520rotations%2520in%2520MVX-Net%2520%2528LiDAR%252BRGB%2529%2520raises%2520moderate%25203D%2520AP%2520from%2520about%252063.85%2520to%252065.16%2520while%2520remaining%2520compatible%2520with%2520standard%25203D%2520augmentations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.01423v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DRot%3A%20Rediscovering%20the%20Missing%20Primitive%20for%20RGB-Based%203D%20Augmentation&entry.906535625=Shitian%20Yang%20and%20Deyu%20Li%20and%20Xiaoke%20Jiang%20and%20Lei%20Zhang&entry.1292438233=RGB-based%203D%20tasks%2C%20e.g.%2C%203D%20detection%2C%20depth%20estimation%2C%203D%20keypoint%20estimation%2C%20still%20suffer%20from%20scarce%2C%20expensive%20annotations%20and%20a%20thin%20augmentation%20toolbox%2C%20since%20many%20image%20transforms%2C%20including%20rotations%20and%20warps%2C%20disrupt%20geometric%20consistency.%20While%20horizontal%20flipping%20and%20color%20jitter%20are%20standard%2C%20rigorous%203D%20rotation%20augmentation%20has%20surprisingly%20remained%20absent%20from%20RGB-based%20pipelines%2C%20largely%20due%20to%20the%20misconception%20that%20it%20requires%20scene%20depth%20or%20scene%20reconstruction.%20In%20this%20paper%2C%20we%20introduce%203DRot%2C%20a%20plug-and-play%20augmentation%20that%20rotates%20and%20mirrors%20images%20about%20the%20camera%27s%20optical%20center%20while%20synchronously%20updating%20RGB%20images%2C%20camera%20intrinsics%2C%20object%20poses%2C%20and%203D%20annotations%20to%20preserve%20projective%20geometry%2C%20achieving%20geometry-consistent%20rotations%20and%20reflections%20without%20relying%20on%20any%20scene%20depth.%20We%20first%20validate%203DRot%20on%20a%20classical%20RGB-based%203D%20task%2C%20monocular%203D%20detection.%20On%20SUN%20RGB-D%2C%20inserting%203DRot%20into%20a%20frozen%20DINO-X%20%2B%20Cube%20R-CNN%20pipeline%20raises%20%24IoU_%7B3D%7D%24%20from%2043.21%20to%2044.51%2C%20cuts%20rotation%20error%20%28ROT%29%20from%2022.91%24%5E%5Ccirc%24%20to%2020.93%24%5E%5Ccirc%24%2C%20and%20boosts%20%24mAP_%7B0.5%7D%24%20from%2035.70%20to%2038.11%3B%20smaller%20but%20consistent%20gains%20appear%20on%20a%20cross-domain%20IN10%20split.%20Beyond%20monocular%20detection%2C%20adding%203DRot%20on%20top%20of%20the%20standard%20BTS%20augmentation%20schedule%20further%20improves%20NYU%20Depth%20v2%20from%200.1783%20to%200.1685%20in%20abs-rel%20%28and%200.7472%20to%200.7548%20in%20%24%CE%B4%3C1.25%24%29%2C%20and%20reduces%20cross-dataset%20error%20on%20SUN%20RGB-D.%20On%20KITTI%2C%20applying%20the%20same%20camera-centric%20rotations%20in%20MVX-Net%20%28LiDAR%2BRGB%29%20raises%20moderate%203D%20AP%20from%20about%2063.85%20to%2065.16%20while%20remaining%20compatible%20with%20standard%203D%20augmentations.&entry.1838667208=http%3A//arxiv.org/abs/2508.01423v3&entry.124074799=Read"},
{"title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques", "author": "David P. Woodruff and Vincent Cohen-Addad and Lalit Jain and Jieming Mao and Song Zuo and MohammadHossein Bateni and Simina Branzei and Michael P. Brenner and Lin Chen and Ying Feng and Lance Fortnow and Gang Fu and Ziyi Guan and Zahra Hadizadeh and Mohammad T. Hajiaghayi and Mahdi JafariRaviz and Adel Javanmard and Karthik C. S. and Ken-ichi Kawarabayashi and Ravi Kumar and Silvio Lattanzi and Euiwoong Lee and Yi Li and Ioannis Panageas and Dimitris Paparas and Benjamin Przybocki and Bernardo Subercaseaux and Ola Svensson and Shayan Taherijam and Xuan Wu and Eylon Yogev and Morteza Zadimoghaddam and Samson Zhou and Yossi Matias and James Manyika and Vahab Mirrokni", "abstract": "Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.", "link": "http://arxiv.org/abs/2602.03837v2", "date": "2026-02-16", "relevancy": 2.3429, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4827}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4624}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerating%20Scientific%20Research%20with%20Gemini%3A%20Case%20Studies%20and%20Common%20Techniques&body=Title%3A%20Accelerating%20Scientific%20Research%20with%20Gemini%3A%20Case%20Studies%20and%20Common%20Techniques%0AAuthor%3A%20David%20P.%20Woodruff%20and%20Vincent%20Cohen-Addad%20and%20Lalit%20Jain%20and%20Jieming%20Mao%20and%20Song%20Zuo%20and%20MohammadHossein%20Bateni%20and%20Simina%20Branzei%20and%20Michael%20P.%20Brenner%20and%20Lin%20Chen%20and%20Ying%20Feng%20and%20Lance%20Fortnow%20and%20Gang%20Fu%20and%20Ziyi%20Guan%20and%20Zahra%20Hadizadeh%20and%20Mohammad%20T.%20Hajiaghayi%20and%20Mahdi%20JafariRaviz%20and%20Adel%20Javanmard%20and%20Karthik%20C.%20S.%20and%20Ken-ichi%20Kawarabayashi%20and%20Ravi%20Kumar%20and%20Silvio%20Lattanzi%20and%20Euiwoong%20Lee%20and%20Yi%20Li%20and%20Ioannis%20Panageas%20and%20Dimitris%20Paparas%20and%20Benjamin%20Przybocki%20and%20Bernardo%20Subercaseaux%20and%20Ola%20Svensson%20and%20Shayan%20Taherijam%20and%20Xuan%20Wu%20and%20Eylon%20Yogev%20and%20Morteza%20Zadimoghaddam%20and%20Samson%20Zhou%20and%20Yossi%20Matias%20and%20James%20Manyika%20and%20Vahab%20Mirrokni%0AAbstract%3A%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20opened%20new%20avenues%20for%20accelerating%20scientific%20research.%20While%20models%20are%20increasingly%20capable%20of%20assisting%20with%20routine%20tasks%2C%20their%20ability%20to%20contribute%20to%20novel%2C%20expert-level%20mathematical%20discovery%20is%20less%20understood.%20We%20present%20a%20collection%20of%20case%20studies%20demonstrating%20how%20researchers%20have%20successfully%20collaborated%20with%20advanced%20AI%20models%2C%20specifically%20Google%27s%20Gemini-based%20models%20%28in%20particular%20Gemini%20Deep%20Think%20and%20its%20advanced%20variants%29%2C%20to%20solve%20open%20problems%2C%20refute%20conjectures%2C%20and%20generate%20new%20proofs%20across%20diverse%20areas%20in%20theoretical%20computer%20science%2C%20as%20well%20as%20other%20areas%20such%20as%20economics%2C%20optimization%2C%20and%20physics.%20Based%20on%20these%20experiences%2C%20we%20extract%20common%20techniques%20for%20effective%20human-AI%20collaboration%20in%20theoretical%20research%2C%20such%20as%20iterative%20refinement%2C%20problem%20decomposition%2C%20and%20cross-disciplinary%20knowledge%20transfer.%20While%20the%20majority%20of%20our%20results%20stem%20from%20this%20interactive%2C%20conversational%20methodology%2C%20we%20also%20highlight%20specific%20instances%20that%20push%20beyond%20standard%20chat%20interfaces.%20These%20include%20deploying%20the%20model%20as%20a%20rigorous%20adversarial%20reviewer%20to%20detect%20subtle%20flaws%20in%20existing%20proofs%2C%20and%20embedding%20it%20within%20a%20%22neuro-symbolic%22%20loop%20that%20autonomously%20writes%20and%20executes%20code%20to%20verify%20complex%20derivations.%20Together%2C%20these%20examples%20highlight%20the%20potential%20of%20AI%20not%20just%20as%20a%20tool%20for%20automation%2C%20but%20as%20a%20versatile%2C%20genuine%20partner%20in%20the%20creative%20process%20of%20scientific%20discovery.%0ALink%3A%20http%3A//arxiv.org/abs/2602.03837v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerating%2520Scientific%2520Research%2520with%2520Gemini%253A%2520Case%2520Studies%2520and%2520Common%2520Techniques%26entry.906535625%3DDavid%2520P.%2520Woodruff%2520and%2520Vincent%2520Cohen-Addad%2520and%2520Lalit%2520Jain%2520and%2520Jieming%2520Mao%2520and%2520Song%2520Zuo%2520and%2520MohammadHossein%2520Bateni%2520and%2520Simina%2520Branzei%2520and%2520Michael%2520P.%2520Brenner%2520and%2520Lin%2520Chen%2520and%2520Ying%2520Feng%2520and%2520Lance%2520Fortnow%2520and%2520Gang%2520Fu%2520and%2520Ziyi%2520Guan%2520and%2520Zahra%2520Hadizadeh%2520and%2520Mohammad%2520T.%2520Hajiaghayi%2520and%2520Mahdi%2520JafariRaviz%2520and%2520Adel%2520Javanmard%2520and%2520Karthik%2520C.%2520S.%2520and%2520Ken-ichi%2520Kawarabayashi%2520and%2520Ravi%2520Kumar%2520and%2520Silvio%2520Lattanzi%2520and%2520Euiwoong%2520Lee%2520and%2520Yi%2520Li%2520and%2520Ioannis%2520Panageas%2520and%2520Dimitris%2520Paparas%2520and%2520Benjamin%2520Przybocki%2520and%2520Bernardo%2520Subercaseaux%2520and%2520Ola%2520Svensson%2520and%2520Shayan%2520Taherijam%2520and%2520Xuan%2520Wu%2520and%2520Eylon%2520Yogev%2520and%2520Morteza%2520Zadimoghaddam%2520and%2520Samson%2520Zhou%2520and%2520Yossi%2520Matias%2520and%2520James%2520Manyika%2520and%2520Vahab%2520Mirrokni%26entry.1292438233%3DRecent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520opened%2520new%2520avenues%2520for%2520accelerating%2520scientific%2520research.%2520While%2520models%2520are%2520increasingly%2520capable%2520of%2520assisting%2520with%2520routine%2520tasks%252C%2520their%2520ability%2520to%2520contribute%2520to%2520novel%252C%2520expert-level%2520mathematical%2520discovery%2520is%2520less%2520understood.%2520We%2520present%2520a%2520collection%2520of%2520case%2520studies%2520demonstrating%2520how%2520researchers%2520have%2520successfully%2520collaborated%2520with%2520advanced%2520AI%2520models%252C%2520specifically%2520Google%2527s%2520Gemini-based%2520models%2520%2528in%2520particular%2520Gemini%2520Deep%2520Think%2520and%2520its%2520advanced%2520variants%2529%252C%2520to%2520solve%2520open%2520problems%252C%2520refute%2520conjectures%252C%2520and%2520generate%2520new%2520proofs%2520across%2520diverse%2520areas%2520in%2520theoretical%2520computer%2520science%252C%2520as%2520well%2520as%2520other%2520areas%2520such%2520as%2520economics%252C%2520optimization%252C%2520and%2520physics.%2520Based%2520on%2520these%2520experiences%252C%2520we%2520extract%2520common%2520techniques%2520for%2520effective%2520human-AI%2520collaboration%2520in%2520theoretical%2520research%252C%2520such%2520as%2520iterative%2520refinement%252C%2520problem%2520decomposition%252C%2520and%2520cross-disciplinary%2520knowledge%2520transfer.%2520While%2520the%2520majority%2520of%2520our%2520results%2520stem%2520from%2520this%2520interactive%252C%2520conversational%2520methodology%252C%2520we%2520also%2520highlight%2520specific%2520instances%2520that%2520push%2520beyond%2520standard%2520chat%2520interfaces.%2520These%2520include%2520deploying%2520the%2520model%2520as%2520a%2520rigorous%2520adversarial%2520reviewer%2520to%2520detect%2520subtle%2520flaws%2520in%2520existing%2520proofs%252C%2520and%2520embedding%2520it%2520within%2520a%2520%2522neuro-symbolic%2522%2520loop%2520that%2520autonomously%2520writes%2520and%2520executes%2520code%2520to%2520verify%2520complex%2520derivations.%2520Together%252C%2520these%2520examples%2520highlight%2520the%2520potential%2520of%2520AI%2520not%2520just%2520as%2520a%2520tool%2520for%2520automation%252C%2520but%2520as%2520a%2520versatile%252C%2520genuine%2520partner%2520in%2520the%2520creative%2520process%2520of%2520scientific%2520discovery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.03837v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerating%20Scientific%20Research%20with%20Gemini%3A%20Case%20Studies%20and%20Common%20Techniques&entry.906535625=David%20P.%20Woodruff%20and%20Vincent%20Cohen-Addad%20and%20Lalit%20Jain%20and%20Jieming%20Mao%20and%20Song%20Zuo%20and%20MohammadHossein%20Bateni%20and%20Simina%20Branzei%20and%20Michael%20P.%20Brenner%20and%20Lin%20Chen%20and%20Ying%20Feng%20and%20Lance%20Fortnow%20and%20Gang%20Fu%20and%20Ziyi%20Guan%20and%20Zahra%20Hadizadeh%20and%20Mohammad%20T.%20Hajiaghayi%20and%20Mahdi%20JafariRaviz%20and%20Adel%20Javanmard%20and%20Karthik%20C.%20S.%20and%20Ken-ichi%20Kawarabayashi%20and%20Ravi%20Kumar%20and%20Silvio%20Lattanzi%20and%20Euiwoong%20Lee%20and%20Yi%20Li%20and%20Ioannis%20Panageas%20and%20Dimitris%20Paparas%20and%20Benjamin%20Przybocki%20and%20Bernardo%20Subercaseaux%20and%20Ola%20Svensson%20and%20Shayan%20Taherijam%20and%20Xuan%20Wu%20and%20Eylon%20Yogev%20and%20Morteza%20Zadimoghaddam%20and%20Samson%20Zhou%20and%20Yossi%20Matias%20and%20James%20Manyika%20and%20Vahab%20Mirrokni&entry.1292438233=Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20opened%20new%20avenues%20for%20accelerating%20scientific%20research.%20While%20models%20are%20increasingly%20capable%20of%20assisting%20with%20routine%20tasks%2C%20their%20ability%20to%20contribute%20to%20novel%2C%20expert-level%20mathematical%20discovery%20is%20less%20understood.%20We%20present%20a%20collection%20of%20case%20studies%20demonstrating%20how%20researchers%20have%20successfully%20collaborated%20with%20advanced%20AI%20models%2C%20specifically%20Google%27s%20Gemini-based%20models%20%28in%20particular%20Gemini%20Deep%20Think%20and%20its%20advanced%20variants%29%2C%20to%20solve%20open%20problems%2C%20refute%20conjectures%2C%20and%20generate%20new%20proofs%20across%20diverse%20areas%20in%20theoretical%20computer%20science%2C%20as%20well%20as%20other%20areas%20such%20as%20economics%2C%20optimization%2C%20and%20physics.%20Based%20on%20these%20experiences%2C%20we%20extract%20common%20techniques%20for%20effective%20human-AI%20collaboration%20in%20theoretical%20research%2C%20such%20as%20iterative%20refinement%2C%20problem%20decomposition%2C%20and%20cross-disciplinary%20knowledge%20transfer.%20While%20the%20majority%20of%20our%20results%20stem%20from%20this%20interactive%2C%20conversational%20methodology%2C%20we%20also%20highlight%20specific%20instances%20that%20push%20beyond%20standard%20chat%20interfaces.%20These%20include%20deploying%20the%20model%20as%20a%20rigorous%20adversarial%20reviewer%20to%20detect%20subtle%20flaws%20in%20existing%20proofs%2C%20and%20embedding%20it%20within%20a%20%22neuro-symbolic%22%20loop%20that%20autonomously%20writes%20and%20executes%20code%20to%20verify%20complex%20derivations.%20Together%2C%20these%20examples%20highlight%20the%20potential%20of%20AI%20not%20just%20as%20a%20tool%20for%20automation%2C%20but%20as%20a%20versatile%2C%20genuine%20partner%20in%20the%20creative%20process%20of%20scientific%20discovery.&entry.1838667208=http%3A//arxiv.org/abs/2602.03837v2&entry.124074799=Read"},
{"title": "Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis", "author": "Yuan Gao and Mattia Piccinini and Yuchen Zhang and Dingrui Wang and Korbinian Moller and Roberto Brusnicki and Baha Zarrouki and Alessio Gambi and Jan Frederik Totz and Kai Storms and Steven Peters and Andrea Stocco and Bassam Alrifaee and Marco Pavone and Johannes Betz", "abstract": "For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key approaches to development and validation of autonomous driving systems. Traditional scenario generation relies on rule-based systems, knowledge-driven models, and data-driven synthesis, often producing limited diversity and unrealistic safety-critical cases. With the emergence of foundation models, which represent a new generation of pre-trained, general-purpose AI models, developers can process heterogeneous inputs (e.g., natural language, sensor data, HD maps, and control actions), enabling the synthesis and interpretation of complex driving scenarios. In this paper, we conduct a survey about the application of foundation models for scenario generation and scenario analysis in autonomous driving (as of May 2025). Our survey presents a unified taxonomy that includes large language models, vision-language models, multimodal large language models, diffusion models, and world models for the generation and analysis of autonomous driving scenarios. In addition, we review the methodologies, open-source datasets, simulation platforms, and benchmark challenges, and we examine the evaluation metrics tailored explicitly to scenario generation and analysis. Finally, the survey concludes by highlighting the open challenges and research questions, and outlining promising future research directions. All reviewed papers are listed in a continuously maintained repository, which contains supplementary materials and is available at https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.", "link": "http://arxiv.org/abs/2506.11526v4", "date": "2026-02-16", "relevancy": 2.3395, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5932}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5932}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20in%20Autonomous%20Driving%3A%20A%20Survey%20on%20Scenario%20Generation%20and%20Scenario%20Analysis&body=Title%3A%20Foundation%20Models%20in%20Autonomous%20Driving%3A%20A%20Survey%20on%20Scenario%20Generation%20and%20Scenario%20Analysis%0AAuthor%3A%20Yuan%20Gao%20and%20Mattia%20Piccinini%20and%20Yuchen%20Zhang%20and%20Dingrui%20Wang%20and%20Korbinian%20Moller%20and%20Roberto%20Brusnicki%20and%20Baha%20Zarrouki%20and%20Alessio%20Gambi%20and%20Jan%20Frederik%20Totz%20and%20Kai%20Storms%20and%20Steven%20Peters%20and%20Andrea%20Stocco%20and%20Bassam%20Alrifaee%20and%20Marco%20Pavone%20and%20Johannes%20Betz%0AAbstract%3A%20For%20autonomous%20vehicles%2C%20safe%20navigation%20in%20complex%20environments%20depends%20on%20handling%20a%20broad%20range%20of%20diverse%20and%20rare%20driving%20scenarios.%20Simulation-%20and%20scenario-based%20testing%20have%20emerged%20as%20key%20approaches%20to%20development%20and%20validation%20of%20autonomous%20driving%20systems.%20Traditional%20scenario%20generation%20relies%20on%20rule-based%20systems%2C%20knowledge-driven%20models%2C%20and%20data-driven%20synthesis%2C%20often%20producing%20limited%20diversity%20and%20unrealistic%20safety-critical%20cases.%20With%20the%20emergence%20of%20foundation%20models%2C%20which%20represent%20a%20new%20generation%20of%20pre-trained%2C%20general-purpose%20AI%20models%2C%20developers%20can%20process%20heterogeneous%20inputs%20%28e.g.%2C%20natural%20language%2C%20sensor%20data%2C%20HD%20maps%2C%20and%20control%20actions%29%2C%20enabling%20the%20synthesis%20and%20interpretation%20of%20complex%20driving%20scenarios.%20In%20this%20paper%2C%20we%20conduct%20a%20survey%20about%20the%20application%20of%20foundation%20models%20for%20scenario%20generation%20and%20scenario%20analysis%20in%20autonomous%20driving%20%28as%20of%20May%202025%29.%20Our%20survey%20presents%20a%20unified%20taxonomy%20that%20includes%20large%20language%20models%2C%20vision-language%20models%2C%20multimodal%20large%20language%20models%2C%20diffusion%20models%2C%20and%20world%20models%20for%20the%20generation%20and%20analysis%20of%20autonomous%20driving%20scenarios.%20In%20addition%2C%20we%20review%20the%20methodologies%2C%20open-source%20datasets%2C%20simulation%20platforms%2C%20and%20benchmark%20challenges%2C%20and%20we%20examine%20the%20evaluation%20metrics%20tailored%20explicitly%20to%20scenario%20generation%20and%20analysis.%20Finally%2C%20the%20survey%20concludes%20by%20highlighting%20the%20open%20challenges%20and%20research%20questions%2C%20and%20outlining%20promising%20future%20research%20directions.%20All%20reviewed%20papers%20are%20listed%20in%20a%20continuously%20maintained%20repository%2C%20which%20contains%20supplementary%20materials%20and%20is%20available%20at%20https%3A//github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11526v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520in%2520Autonomous%2520Driving%253A%2520A%2520Survey%2520on%2520Scenario%2520Generation%2520and%2520Scenario%2520Analysis%26entry.906535625%3DYuan%2520Gao%2520and%2520Mattia%2520Piccinini%2520and%2520Yuchen%2520Zhang%2520and%2520Dingrui%2520Wang%2520and%2520Korbinian%2520Moller%2520and%2520Roberto%2520Brusnicki%2520and%2520Baha%2520Zarrouki%2520and%2520Alessio%2520Gambi%2520and%2520Jan%2520Frederik%2520Totz%2520and%2520Kai%2520Storms%2520and%2520Steven%2520Peters%2520and%2520Andrea%2520Stocco%2520and%2520Bassam%2520Alrifaee%2520and%2520Marco%2520Pavone%2520and%2520Johannes%2520Betz%26entry.1292438233%3DFor%2520autonomous%2520vehicles%252C%2520safe%2520navigation%2520in%2520complex%2520environments%2520depends%2520on%2520handling%2520a%2520broad%2520range%2520of%2520diverse%2520and%2520rare%2520driving%2520scenarios.%2520Simulation-%2520and%2520scenario-based%2520testing%2520have%2520emerged%2520as%2520key%2520approaches%2520to%2520development%2520and%2520validation%2520of%2520autonomous%2520driving%2520systems.%2520Traditional%2520scenario%2520generation%2520relies%2520on%2520rule-based%2520systems%252C%2520knowledge-driven%2520models%252C%2520and%2520data-driven%2520synthesis%252C%2520often%2520producing%2520limited%2520diversity%2520and%2520unrealistic%2520safety-critical%2520cases.%2520With%2520the%2520emergence%2520of%2520foundation%2520models%252C%2520which%2520represent%2520a%2520new%2520generation%2520of%2520pre-trained%252C%2520general-purpose%2520AI%2520models%252C%2520developers%2520can%2520process%2520heterogeneous%2520inputs%2520%2528e.g.%252C%2520natural%2520language%252C%2520sensor%2520data%252C%2520HD%2520maps%252C%2520and%2520control%2520actions%2529%252C%2520enabling%2520the%2520synthesis%2520and%2520interpretation%2520of%2520complex%2520driving%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520survey%2520about%2520the%2520application%2520of%2520foundation%2520models%2520for%2520scenario%2520generation%2520and%2520scenario%2520analysis%2520in%2520autonomous%2520driving%2520%2528as%2520of%2520May%25202025%2529.%2520Our%2520survey%2520presents%2520a%2520unified%2520taxonomy%2520that%2520includes%2520large%2520language%2520models%252C%2520vision-language%2520models%252C%2520multimodal%2520large%2520language%2520models%252C%2520diffusion%2520models%252C%2520and%2520world%2520models%2520for%2520the%2520generation%2520and%2520analysis%2520of%2520autonomous%2520driving%2520scenarios.%2520In%2520addition%252C%2520we%2520review%2520the%2520methodologies%252C%2520open-source%2520datasets%252C%2520simulation%2520platforms%252C%2520and%2520benchmark%2520challenges%252C%2520and%2520we%2520examine%2520the%2520evaluation%2520metrics%2520tailored%2520explicitly%2520to%2520scenario%2520generation%2520and%2520analysis.%2520Finally%252C%2520the%2520survey%2520concludes%2520by%2520highlighting%2520the%2520open%2520challenges%2520and%2520research%2520questions%252C%2520and%2520outlining%2520promising%2520future%2520research%2520directions.%2520All%2520reviewed%2520papers%2520are%2520listed%2520in%2520a%2520continuously%2520maintained%2520repository%252C%2520which%2520contains%2520supplementary%2520materials%2520and%2520is%2520available%2520at%2520https%253A//github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11526v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20in%20Autonomous%20Driving%3A%20A%20Survey%20on%20Scenario%20Generation%20and%20Scenario%20Analysis&entry.906535625=Yuan%20Gao%20and%20Mattia%20Piccinini%20and%20Yuchen%20Zhang%20and%20Dingrui%20Wang%20and%20Korbinian%20Moller%20and%20Roberto%20Brusnicki%20and%20Baha%20Zarrouki%20and%20Alessio%20Gambi%20and%20Jan%20Frederik%20Totz%20and%20Kai%20Storms%20and%20Steven%20Peters%20and%20Andrea%20Stocco%20and%20Bassam%20Alrifaee%20and%20Marco%20Pavone%20and%20Johannes%20Betz&entry.1292438233=For%20autonomous%20vehicles%2C%20safe%20navigation%20in%20complex%20environments%20depends%20on%20handling%20a%20broad%20range%20of%20diverse%20and%20rare%20driving%20scenarios.%20Simulation-%20and%20scenario-based%20testing%20have%20emerged%20as%20key%20approaches%20to%20development%20and%20validation%20of%20autonomous%20driving%20systems.%20Traditional%20scenario%20generation%20relies%20on%20rule-based%20systems%2C%20knowledge-driven%20models%2C%20and%20data-driven%20synthesis%2C%20often%20producing%20limited%20diversity%20and%20unrealistic%20safety-critical%20cases.%20With%20the%20emergence%20of%20foundation%20models%2C%20which%20represent%20a%20new%20generation%20of%20pre-trained%2C%20general-purpose%20AI%20models%2C%20developers%20can%20process%20heterogeneous%20inputs%20%28e.g.%2C%20natural%20language%2C%20sensor%20data%2C%20HD%20maps%2C%20and%20control%20actions%29%2C%20enabling%20the%20synthesis%20and%20interpretation%20of%20complex%20driving%20scenarios.%20In%20this%20paper%2C%20we%20conduct%20a%20survey%20about%20the%20application%20of%20foundation%20models%20for%20scenario%20generation%20and%20scenario%20analysis%20in%20autonomous%20driving%20%28as%20of%20May%202025%29.%20Our%20survey%20presents%20a%20unified%20taxonomy%20that%20includes%20large%20language%20models%2C%20vision-language%20models%2C%20multimodal%20large%20language%20models%2C%20diffusion%20models%2C%20and%20world%20models%20for%20the%20generation%20and%20analysis%20of%20autonomous%20driving%20scenarios.%20In%20addition%2C%20we%20review%20the%20methodologies%2C%20open-source%20datasets%2C%20simulation%20platforms%2C%20and%20benchmark%20challenges%2C%20and%20we%20examine%20the%20evaluation%20metrics%20tailored%20explicitly%20to%20scenario%20generation%20and%20analysis.%20Finally%2C%20the%20survey%20concludes%20by%20highlighting%20the%20open%20challenges%20and%20research%20questions%2C%20and%20outlining%20promising%20future%20research%20directions.%20All%20reviewed%20papers%20are%20listed%20in%20a%20continuously%20maintained%20repository%2C%20which%20contains%20supplementary%20materials%20and%20is%20available%20at%20https%3A//github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.&entry.1838667208=http%3A//arxiv.org/abs/2506.11526v4&entry.124074799=Read"},
{"title": "Advances in Global Solvers for 3D Vision", "author": "Zhenjun Zhao and Heng Yang and Bangyan Liao and Yingping Zeng and Shaocheng Yan and Yingdong Gu and Peidong Liu and Yi Zhou and Haoang Li and Javier Civera", "abstract": "Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.", "link": "http://arxiv.org/abs/2602.14662v1", "date": "2026-02-16", "relevancy": 2.3357, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advances%20in%20Global%20Solvers%20for%203D%20Vision&body=Title%3A%20Advances%20in%20Global%20Solvers%20for%203D%20Vision%0AAuthor%3A%20Zhenjun%20Zhao%20and%20Heng%20Yang%20and%20Bangyan%20Liao%20and%20Yingping%20Zeng%20and%20Shaocheng%20Yan%20and%20Yingdong%20Gu%20and%20Peidong%20Liu%20and%20Yi%20Zhou%20and%20Haoang%20Li%20and%20Javier%20Civera%0AAbstract%3A%20Global%20solvers%20have%20emerged%20as%20a%20powerful%20paradigm%20for%203D%20vision%2C%20offering%20certifiable%20solutions%20to%20nonconvex%20geometric%20optimization%20problems%20traditionally%20addressed%20by%20local%20or%20heuristic%20methods.%20This%20survey%20presents%20the%20first%20systematic%20review%20of%20global%20solvers%20in%20geometric%20vision%2C%20unifying%20the%20field%20through%20a%20comprehensive%20taxonomy%20of%20three%20core%20paradigms%3A%20Branch-and-Bound%20%28BnB%29%2C%20Convex%20Relaxation%20%28CR%29%2C%20and%20Graduated%20Non-Convexity%20%28GNC%29.%20We%20present%20their%20theoretical%20foundations%2C%20algorithmic%20designs%2C%20and%20practical%20enhancements%20for%20robustness%20and%20scalability%2C%20examining%20how%20each%20addresses%20the%20fundamental%20nonconvexity%20of%20geometric%20estimation%20problems.%20Our%20analysis%20spans%20ten%20core%20vision%20tasks%2C%20from%20Wahba%20problem%20to%20bundle%20adjustment%2C%20revealing%20the%20optimality-robustness-scalability%20trade-offs%20that%20govern%20solver%20selection.%20We%20identify%20critical%20future%20directions%3A%20scaling%20algorithms%20while%20maintaining%20guarantees%2C%20integrating%20data-driven%20priors%20with%20certifiable%20optimization%2C%20establishing%20standardized%20benchmarks%2C%20and%20addressing%20societal%20implications%20for%20safety-critical%20deployment.%20By%20consolidating%20theoretical%20foundations%2C%20practical%20advances%2C%20and%20broader%20impacts%2C%20this%20survey%20provides%20a%20unified%20perspective%20and%20roadmap%20toward%20certifiable%2C%20trustworthy%20perception%20for%20real-world%20applications.%20A%20continuously-updated%20literature%20summary%20and%20companion%20code%20tutorials%20are%20available%20at%20https%3A//github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvances%2520in%2520Global%2520Solvers%2520for%25203D%2520Vision%26entry.906535625%3DZhenjun%2520Zhao%2520and%2520Heng%2520Yang%2520and%2520Bangyan%2520Liao%2520and%2520Yingping%2520Zeng%2520and%2520Shaocheng%2520Yan%2520and%2520Yingdong%2520Gu%2520and%2520Peidong%2520Liu%2520and%2520Yi%2520Zhou%2520and%2520Haoang%2520Li%2520and%2520Javier%2520Civera%26entry.1292438233%3DGlobal%2520solvers%2520have%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%25203D%2520vision%252C%2520offering%2520certifiable%2520solutions%2520to%2520nonconvex%2520geometric%2520optimization%2520problems%2520traditionally%2520addressed%2520by%2520local%2520or%2520heuristic%2520methods.%2520This%2520survey%2520presents%2520the%2520first%2520systematic%2520review%2520of%2520global%2520solvers%2520in%2520geometric%2520vision%252C%2520unifying%2520the%2520field%2520through%2520a%2520comprehensive%2520taxonomy%2520of%2520three%2520core%2520paradigms%253A%2520Branch-and-Bound%2520%2528BnB%2529%252C%2520Convex%2520Relaxation%2520%2528CR%2529%252C%2520and%2520Graduated%2520Non-Convexity%2520%2528GNC%2529.%2520We%2520present%2520their%2520theoretical%2520foundations%252C%2520algorithmic%2520designs%252C%2520and%2520practical%2520enhancements%2520for%2520robustness%2520and%2520scalability%252C%2520examining%2520how%2520each%2520addresses%2520the%2520fundamental%2520nonconvexity%2520of%2520geometric%2520estimation%2520problems.%2520Our%2520analysis%2520spans%2520ten%2520core%2520vision%2520tasks%252C%2520from%2520Wahba%2520problem%2520to%2520bundle%2520adjustment%252C%2520revealing%2520the%2520optimality-robustness-scalability%2520trade-offs%2520that%2520govern%2520solver%2520selection.%2520We%2520identify%2520critical%2520future%2520directions%253A%2520scaling%2520algorithms%2520while%2520maintaining%2520guarantees%252C%2520integrating%2520data-driven%2520priors%2520with%2520certifiable%2520optimization%252C%2520establishing%2520standardized%2520benchmarks%252C%2520and%2520addressing%2520societal%2520implications%2520for%2520safety-critical%2520deployment.%2520By%2520consolidating%2520theoretical%2520foundations%252C%2520practical%2520advances%252C%2520and%2520broader%2520impacts%252C%2520this%2520survey%2520provides%2520a%2520unified%2520perspective%2520and%2520roadmap%2520toward%2520certifiable%252C%2520trustworthy%2520perception%2520for%2520real-world%2520applications.%2520A%2520continuously-updated%2520literature%2520summary%2520and%2520companion%2520code%2520tutorials%2520are%2520available%2520at%2520https%253A//github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20in%20Global%20Solvers%20for%203D%20Vision&entry.906535625=Zhenjun%20Zhao%20and%20Heng%20Yang%20and%20Bangyan%20Liao%20and%20Yingping%20Zeng%20and%20Shaocheng%20Yan%20and%20Yingdong%20Gu%20and%20Peidong%20Liu%20and%20Yi%20Zhou%20and%20Haoang%20Li%20and%20Javier%20Civera&entry.1292438233=Global%20solvers%20have%20emerged%20as%20a%20powerful%20paradigm%20for%203D%20vision%2C%20offering%20certifiable%20solutions%20to%20nonconvex%20geometric%20optimization%20problems%20traditionally%20addressed%20by%20local%20or%20heuristic%20methods.%20This%20survey%20presents%20the%20first%20systematic%20review%20of%20global%20solvers%20in%20geometric%20vision%2C%20unifying%20the%20field%20through%20a%20comprehensive%20taxonomy%20of%20three%20core%20paradigms%3A%20Branch-and-Bound%20%28BnB%29%2C%20Convex%20Relaxation%20%28CR%29%2C%20and%20Graduated%20Non-Convexity%20%28GNC%29.%20We%20present%20their%20theoretical%20foundations%2C%20algorithmic%20designs%2C%20and%20practical%20enhancements%20for%20robustness%20and%20scalability%2C%20examining%20how%20each%20addresses%20the%20fundamental%20nonconvexity%20of%20geometric%20estimation%20problems.%20Our%20analysis%20spans%20ten%20core%20vision%20tasks%2C%20from%20Wahba%20problem%20to%20bundle%20adjustment%2C%20revealing%20the%20optimality-robustness-scalability%20trade-offs%20that%20govern%20solver%20selection.%20We%20identify%20critical%20future%20directions%3A%20scaling%20algorithms%20while%20maintaining%20guarantees%2C%20integrating%20data-driven%20priors%20with%20certifiable%20optimization%2C%20establishing%20standardized%20benchmarks%2C%20and%20addressing%20societal%20implications%20for%20safety-critical%20deployment.%20By%20consolidating%20theoretical%20foundations%2C%20practical%20advances%2C%20and%20broader%20impacts%2C%20this%20survey%20provides%20a%20unified%20perspective%20and%20roadmap%20toward%20certifiable%2C%20trustworthy%20perception%20for%20real-world%20applications.%20A%20continuously-updated%20literature%20summary%20and%20companion%20code%20tutorials%20are%20available%20at%20https%3A//github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.&entry.1838667208=http%3A//arxiv.org/abs/2602.14662v1&entry.124074799=Read"},
{"title": "Learning Structural Hardness for Combinatorial Auctions: Instance-Dependent Algorithm Selection via Graph Neural Networks", "author": "Sungwoo Kang", "abstract": "The Winner Determination Problem (WDP) in combinatorial auctions is NP-hard, and no existing method reliably predicts which instances will defeat fast greedy heuristics. The ML-for-combinatorial-optimization community has focused on learning to \\emph{replace} solvers, yet recent evidence shows that graph neural networks (GNNs) rarely outperform well-tuned classical methods on standard benchmarks. We pursue a different objective: learning to predict \\emph{when} a given instance is hard for greedy allocation, enabling instance-dependent algorithm selection. We design a 20-dimensional structural feature vector and train a lightweight MLP hardness classifier that predicts the greedy optimality gap with mean absolute error 0.033, Pearson correlation 0.937, and binary classification accuracy 94.7\\% across three random seeds. For instances identified as hard -- those exhibiting ``whale-fish'' trap structure where greedy provably fails -- we deploy a heterogeneous GNN specialist that achieves ${\\approx}0\\%$ optimality gap on all six adversarial configurations tested (vs.\\ 3.75--59.24\\% for greedy). A hybrid allocator combining the hardness classifier with GNN and greedy solvers achieves 0.51\\% overall gap on mixed distributions. Our honest evaluation on CATS benchmarks confirms that GNNs do not outperform Gurobi (0.45--0.71 vs.\\ 0.20 gap), motivating the algorithm selection framing. Learning \\emph{when} to deploy expensive solvers is more tractable than learning to replace them.", "link": "http://arxiv.org/abs/2602.14772v1", "date": "2026-02-16", "relevancy": 2.3306, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5031}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4505}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Structural%20Hardness%20for%20Combinatorial%20Auctions%3A%20Instance-Dependent%20Algorithm%20Selection%20via%20Graph%20Neural%20Networks&body=Title%3A%20Learning%20Structural%20Hardness%20for%20Combinatorial%20Auctions%3A%20Instance-Dependent%20Algorithm%20Selection%20via%20Graph%20Neural%20Networks%0AAuthor%3A%20Sungwoo%20Kang%0AAbstract%3A%20The%20Winner%20Determination%20Problem%20%28WDP%29%20in%20combinatorial%20auctions%20is%20NP-hard%2C%20and%20no%20existing%20method%20reliably%20predicts%20which%20instances%20will%20defeat%20fast%20greedy%20heuristics.%20The%20ML-for-combinatorial-optimization%20community%20has%20focused%20on%20learning%20to%20%5Cemph%7Breplace%7D%20solvers%2C%20yet%20recent%20evidence%20shows%20that%20graph%20neural%20networks%20%28GNNs%29%20rarely%20outperform%20well-tuned%20classical%20methods%20on%20standard%20benchmarks.%20We%20pursue%20a%20different%20objective%3A%20learning%20to%20predict%20%5Cemph%7Bwhen%7D%20a%20given%20instance%20is%20hard%20for%20greedy%20allocation%2C%20enabling%20instance-dependent%20algorithm%20selection.%20We%20design%20a%2020-dimensional%20structural%20feature%20vector%20and%20train%20a%20lightweight%20MLP%20hardness%20classifier%20that%20predicts%20the%20greedy%20optimality%20gap%20with%20mean%20absolute%20error%200.033%2C%20Pearson%20correlation%200.937%2C%20and%20binary%20classification%20accuracy%2094.7%5C%25%20across%20three%20random%20seeds.%20For%20instances%20identified%20as%20hard%20--%20those%20exhibiting%20%60%60whale-fish%27%27%20trap%20structure%20where%20greedy%20provably%20fails%20--%20we%20deploy%20a%20heterogeneous%20GNN%20specialist%20that%20achieves%20%24%7B%5Capprox%7D0%5C%25%24%20optimality%20gap%20on%20all%20six%20adversarial%20configurations%20tested%20%28vs.%5C%203.75--59.24%5C%25%20for%20greedy%29.%20A%20hybrid%20allocator%20combining%20the%20hardness%20classifier%20with%20GNN%20and%20greedy%20solvers%20achieves%200.51%5C%25%20overall%20gap%20on%20mixed%20distributions.%20Our%20honest%20evaluation%20on%20CATS%20benchmarks%20confirms%20that%20GNNs%20do%20not%20outperform%20Gurobi%20%280.45--0.71%20vs.%5C%200.20%20gap%29%2C%20motivating%20the%20algorithm%20selection%20framing.%20Learning%20%5Cemph%7Bwhen%7D%20to%20deploy%20expensive%20solvers%20is%20more%20tractable%20than%20learning%20to%20replace%20them.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14772v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Structural%2520Hardness%2520for%2520Combinatorial%2520Auctions%253A%2520Instance-Dependent%2520Algorithm%2520Selection%2520via%2520Graph%2520Neural%2520Networks%26entry.906535625%3DSungwoo%2520Kang%26entry.1292438233%3DThe%2520Winner%2520Determination%2520Problem%2520%2528WDP%2529%2520in%2520combinatorial%2520auctions%2520is%2520NP-hard%252C%2520and%2520no%2520existing%2520method%2520reliably%2520predicts%2520which%2520instances%2520will%2520defeat%2520fast%2520greedy%2520heuristics.%2520The%2520ML-for-combinatorial-optimization%2520community%2520has%2520focused%2520on%2520learning%2520to%2520%255Cemph%257Breplace%257D%2520solvers%252C%2520yet%2520recent%2520evidence%2520shows%2520that%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520rarely%2520outperform%2520well-tuned%2520classical%2520methods%2520on%2520standard%2520benchmarks.%2520We%2520pursue%2520a%2520different%2520objective%253A%2520learning%2520to%2520predict%2520%255Cemph%257Bwhen%257D%2520a%2520given%2520instance%2520is%2520hard%2520for%2520greedy%2520allocation%252C%2520enabling%2520instance-dependent%2520algorithm%2520selection.%2520We%2520design%2520a%252020-dimensional%2520structural%2520feature%2520vector%2520and%2520train%2520a%2520lightweight%2520MLP%2520hardness%2520classifier%2520that%2520predicts%2520the%2520greedy%2520optimality%2520gap%2520with%2520mean%2520absolute%2520error%25200.033%252C%2520Pearson%2520correlation%25200.937%252C%2520and%2520binary%2520classification%2520accuracy%252094.7%255C%2525%2520across%2520three%2520random%2520seeds.%2520For%2520instances%2520identified%2520as%2520hard%2520--%2520those%2520exhibiting%2520%2560%2560whale-fish%2527%2527%2520trap%2520structure%2520where%2520greedy%2520provably%2520fails%2520--%2520we%2520deploy%2520a%2520heterogeneous%2520GNN%2520specialist%2520that%2520achieves%2520%2524%257B%255Capprox%257D0%255C%2525%2524%2520optimality%2520gap%2520on%2520all%2520six%2520adversarial%2520configurations%2520tested%2520%2528vs.%255C%25203.75--59.24%255C%2525%2520for%2520greedy%2529.%2520A%2520hybrid%2520allocator%2520combining%2520the%2520hardness%2520classifier%2520with%2520GNN%2520and%2520greedy%2520solvers%2520achieves%25200.51%255C%2525%2520overall%2520gap%2520on%2520mixed%2520distributions.%2520Our%2520honest%2520evaluation%2520on%2520CATS%2520benchmarks%2520confirms%2520that%2520GNNs%2520do%2520not%2520outperform%2520Gurobi%2520%25280.45--0.71%2520vs.%255C%25200.20%2520gap%2529%252C%2520motivating%2520the%2520algorithm%2520selection%2520framing.%2520Learning%2520%255Cemph%257Bwhen%257D%2520to%2520deploy%2520expensive%2520solvers%2520is%2520more%2520tractable%2520than%2520learning%2520to%2520replace%2520them.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14772v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Structural%20Hardness%20for%20Combinatorial%20Auctions%3A%20Instance-Dependent%20Algorithm%20Selection%20via%20Graph%20Neural%20Networks&entry.906535625=Sungwoo%20Kang&entry.1292438233=The%20Winner%20Determination%20Problem%20%28WDP%29%20in%20combinatorial%20auctions%20is%20NP-hard%2C%20and%20no%20existing%20method%20reliably%20predicts%20which%20instances%20will%20defeat%20fast%20greedy%20heuristics.%20The%20ML-for-combinatorial-optimization%20community%20has%20focused%20on%20learning%20to%20%5Cemph%7Breplace%7D%20solvers%2C%20yet%20recent%20evidence%20shows%20that%20graph%20neural%20networks%20%28GNNs%29%20rarely%20outperform%20well-tuned%20classical%20methods%20on%20standard%20benchmarks.%20We%20pursue%20a%20different%20objective%3A%20learning%20to%20predict%20%5Cemph%7Bwhen%7D%20a%20given%20instance%20is%20hard%20for%20greedy%20allocation%2C%20enabling%20instance-dependent%20algorithm%20selection.%20We%20design%20a%2020-dimensional%20structural%20feature%20vector%20and%20train%20a%20lightweight%20MLP%20hardness%20classifier%20that%20predicts%20the%20greedy%20optimality%20gap%20with%20mean%20absolute%20error%200.033%2C%20Pearson%20correlation%200.937%2C%20and%20binary%20classification%20accuracy%2094.7%5C%25%20across%20three%20random%20seeds.%20For%20instances%20identified%20as%20hard%20--%20those%20exhibiting%20%60%60whale-fish%27%27%20trap%20structure%20where%20greedy%20provably%20fails%20--%20we%20deploy%20a%20heterogeneous%20GNN%20specialist%20that%20achieves%20%24%7B%5Capprox%7D0%5C%25%24%20optimality%20gap%20on%20all%20six%20adversarial%20configurations%20tested%20%28vs.%5C%203.75--59.24%5C%25%20for%20greedy%29.%20A%20hybrid%20allocator%20combining%20the%20hardness%20classifier%20with%20GNN%20and%20greedy%20solvers%20achieves%200.51%5C%25%20overall%20gap%20on%20mixed%20distributions.%20Our%20honest%20evaluation%20on%20CATS%20benchmarks%20confirms%20that%20GNNs%20do%20not%20outperform%20Gurobi%20%280.45--0.71%20vs.%5C%200.20%20gap%29%2C%20motivating%20the%20algorithm%20selection%20framing.%20Learning%20%5Cemph%7Bwhen%7D%20to%20deploy%20expensive%20solvers%20is%20more%20tractable%20than%20learning%20to%20replace%20them.&entry.1838667208=http%3A//arxiv.org/abs/2602.14772v1&entry.124074799=Read"},
{"title": "Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI", "author": "Xiaosheng Zhao and Yuan-Sen Ting and Rosemary F. G. Wyse and Alexander S. Szalay and Yang Huang and L\u00e1szl\u00f3 Dobos and Tam\u00e1s Budav\u00e1ri and Viska Wei", "abstract": "Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem using pre-trained models, focusing on simple neural networks such as multilayer perceptrons (MLPs), with a case study transferring from LAMOST low-resolution spectra (LRS) to DESI medium-resolution spectra (MRS). Specifically, we pre-train MLPs on either LRS or their embeddings and fine-tune them for application to DESI stellar spectra. We compare MLPs trained directly on spectra with those trained on embeddings derived from transformer-based models (self-supervised foundation models pre-trained for multiple downstream tasks). We also evaluate different fine-tuning strategies, including residual-head adapters, LoRA, and full fine-tuning. We find that MLPs pre-trained on LAMOST LRS achieve strong performance, even without fine-tuning, and that modest fine-tuning with DESI spectra further improves the results. For iron abundance, embeddings from a transformer-based model yield advantages in the metal-rich ([Fe/H] > -1.0) regime, but underperform in the metal-poor regime compared to MLPs trained directly on LRS. We also show that the optimal fine-tuning strategy depends on the specific stellar parameter under consideration. These results highlight that simple pre-trained MLPs can provide competitive cross-survey generalization, while the role of spectral foundation models for cross-survey stellar parameter estimation requires further exploration.", "link": "http://arxiv.org/abs/2602.15021v1", "date": "2026-02-16", "relevancy": 2.3292, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4695}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4695}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20from%20Low-%20to%20Moderate-Resolution%20Spectra%20with%20Neural%20Networks%20for%20Stellar%20Parameter%20Estimation%3A%20A%20Case%20Study%20with%20DESI&body=Title%3A%20Generalization%20from%20Low-%20to%20Moderate-Resolution%20Spectra%20with%20Neural%20Networks%20for%20Stellar%20Parameter%20Estimation%3A%20A%20Case%20Study%20with%20DESI%0AAuthor%3A%20Xiaosheng%20Zhao%20and%20Yuan-Sen%20Ting%20and%20Rosemary%20F.%20G.%20Wyse%20and%20Alexander%20S.%20Szalay%20and%20Yang%20Huang%20and%20L%C3%A1szl%C3%B3%20Dobos%20and%20Tam%C3%A1s%20Budav%C3%A1ri%20and%20Viska%20Wei%0AAbstract%3A%20Cross-survey%20generalization%20is%20a%20critical%20challenge%20in%20stellar%20spectral%20analysis%2C%20particularly%20in%20cases%20such%20as%20transferring%20from%20low-%20to%20moderate-resolution%20surveys.%20We%20investigate%20this%20problem%20using%20pre-trained%20models%2C%20focusing%20on%20simple%20neural%20networks%20such%20as%20multilayer%20perceptrons%20%28MLPs%29%2C%20with%20a%20case%20study%20transferring%20from%20LAMOST%20low-resolution%20spectra%20%28LRS%29%20to%20DESI%20medium-resolution%20spectra%20%28MRS%29.%20Specifically%2C%20we%20pre-train%20MLPs%20on%20either%20LRS%20or%20their%20embeddings%20and%20fine-tune%20them%20for%20application%20to%20DESI%20stellar%20spectra.%20We%20compare%20MLPs%20trained%20directly%20on%20spectra%20with%20those%20trained%20on%20embeddings%20derived%20from%20transformer-based%20models%20%28self-supervised%20foundation%20models%20pre-trained%20for%20multiple%20downstream%20tasks%29.%20We%20also%20evaluate%20different%20fine-tuning%20strategies%2C%20including%20residual-head%20adapters%2C%20LoRA%2C%20and%20full%20fine-tuning.%20We%20find%20that%20MLPs%20pre-trained%20on%20LAMOST%20LRS%20achieve%20strong%20performance%2C%20even%20without%20fine-tuning%2C%20and%20that%20modest%20fine-tuning%20with%20DESI%20spectra%20further%20improves%20the%20results.%20For%20iron%20abundance%2C%20embeddings%20from%20a%20transformer-based%20model%20yield%20advantages%20in%20the%20metal-rich%20%28%5BFe/H%5D%20%3E%20-1.0%29%20regime%2C%20but%20underperform%20in%20the%20metal-poor%20regime%20compared%20to%20MLPs%20trained%20directly%20on%20LRS.%20We%20also%20show%20that%20the%20optimal%20fine-tuning%20strategy%20depends%20on%20the%20specific%20stellar%20parameter%20under%20consideration.%20These%20results%20highlight%20that%20simple%20pre-trained%20MLPs%20can%20provide%20competitive%20cross-survey%20generalization%2C%20while%20the%20role%20of%20spectral%20foundation%20models%20for%20cross-survey%20stellar%20parameter%20estimation%20requires%20further%20exploration.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520from%2520Low-%2520to%2520Moderate-Resolution%2520Spectra%2520with%2520Neural%2520Networks%2520for%2520Stellar%2520Parameter%2520Estimation%253A%2520A%2520Case%2520Study%2520with%2520DESI%26entry.906535625%3DXiaosheng%2520Zhao%2520and%2520Yuan-Sen%2520Ting%2520and%2520Rosemary%2520F.%2520G.%2520Wyse%2520and%2520Alexander%2520S.%2520Szalay%2520and%2520Yang%2520Huang%2520and%2520L%25C3%25A1szl%25C3%25B3%2520Dobos%2520and%2520Tam%25C3%25A1s%2520Budav%25C3%25A1ri%2520and%2520Viska%2520Wei%26entry.1292438233%3DCross-survey%2520generalization%2520is%2520a%2520critical%2520challenge%2520in%2520stellar%2520spectral%2520analysis%252C%2520particularly%2520in%2520cases%2520such%2520as%2520transferring%2520from%2520low-%2520to%2520moderate-resolution%2520surveys.%2520We%2520investigate%2520this%2520problem%2520using%2520pre-trained%2520models%252C%2520focusing%2520on%2520simple%2520neural%2520networks%2520such%2520as%2520multilayer%2520perceptrons%2520%2528MLPs%2529%252C%2520with%2520a%2520case%2520study%2520transferring%2520from%2520LAMOST%2520low-resolution%2520spectra%2520%2528LRS%2529%2520to%2520DESI%2520medium-resolution%2520spectra%2520%2528MRS%2529.%2520Specifically%252C%2520we%2520pre-train%2520MLPs%2520on%2520either%2520LRS%2520or%2520their%2520embeddings%2520and%2520fine-tune%2520them%2520for%2520application%2520to%2520DESI%2520stellar%2520spectra.%2520We%2520compare%2520MLPs%2520trained%2520directly%2520on%2520spectra%2520with%2520those%2520trained%2520on%2520embeddings%2520derived%2520from%2520transformer-based%2520models%2520%2528self-supervised%2520foundation%2520models%2520pre-trained%2520for%2520multiple%2520downstream%2520tasks%2529.%2520We%2520also%2520evaluate%2520different%2520fine-tuning%2520strategies%252C%2520including%2520residual-head%2520adapters%252C%2520LoRA%252C%2520and%2520full%2520fine-tuning.%2520We%2520find%2520that%2520MLPs%2520pre-trained%2520on%2520LAMOST%2520LRS%2520achieve%2520strong%2520performance%252C%2520even%2520without%2520fine-tuning%252C%2520and%2520that%2520modest%2520fine-tuning%2520with%2520DESI%2520spectra%2520further%2520improves%2520the%2520results.%2520For%2520iron%2520abundance%252C%2520embeddings%2520from%2520a%2520transformer-based%2520model%2520yield%2520advantages%2520in%2520the%2520metal-rich%2520%2528%255BFe/H%255D%2520%253E%2520-1.0%2529%2520regime%252C%2520but%2520underperform%2520in%2520the%2520metal-poor%2520regime%2520compared%2520to%2520MLPs%2520trained%2520directly%2520on%2520LRS.%2520We%2520also%2520show%2520that%2520the%2520optimal%2520fine-tuning%2520strategy%2520depends%2520on%2520the%2520specific%2520stellar%2520parameter%2520under%2520consideration.%2520These%2520results%2520highlight%2520that%2520simple%2520pre-trained%2520MLPs%2520can%2520provide%2520competitive%2520cross-survey%2520generalization%252C%2520while%2520the%2520role%2520of%2520spectral%2520foundation%2520models%2520for%2520cross-survey%2520stellar%2520parameter%2520estimation%2520requires%2520further%2520exploration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20from%20Low-%20to%20Moderate-Resolution%20Spectra%20with%20Neural%20Networks%20for%20Stellar%20Parameter%20Estimation%3A%20A%20Case%20Study%20with%20DESI&entry.906535625=Xiaosheng%20Zhao%20and%20Yuan-Sen%20Ting%20and%20Rosemary%20F.%20G.%20Wyse%20and%20Alexander%20S.%20Szalay%20and%20Yang%20Huang%20and%20L%C3%A1szl%C3%B3%20Dobos%20and%20Tam%C3%A1s%20Budav%C3%A1ri%20and%20Viska%20Wei&entry.1292438233=Cross-survey%20generalization%20is%20a%20critical%20challenge%20in%20stellar%20spectral%20analysis%2C%20particularly%20in%20cases%20such%20as%20transferring%20from%20low-%20to%20moderate-resolution%20surveys.%20We%20investigate%20this%20problem%20using%20pre-trained%20models%2C%20focusing%20on%20simple%20neural%20networks%20such%20as%20multilayer%20perceptrons%20%28MLPs%29%2C%20with%20a%20case%20study%20transferring%20from%20LAMOST%20low-resolution%20spectra%20%28LRS%29%20to%20DESI%20medium-resolution%20spectra%20%28MRS%29.%20Specifically%2C%20we%20pre-train%20MLPs%20on%20either%20LRS%20or%20their%20embeddings%20and%20fine-tune%20them%20for%20application%20to%20DESI%20stellar%20spectra.%20We%20compare%20MLPs%20trained%20directly%20on%20spectra%20with%20those%20trained%20on%20embeddings%20derived%20from%20transformer-based%20models%20%28self-supervised%20foundation%20models%20pre-trained%20for%20multiple%20downstream%20tasks%29.%20We%20also%20evaluate%20different%20fine-tuning%20strategies%2C%20including%20residual-head%20adapters%2C%20LoRA%2C%20and%20full%20fine-tuning.%20We%20find%20that%20MLPs%20pre-trained%20on%20LAMOST%20LRS%20achieve%20strong%20performance%2C%20even%20without%20fine-tuning%2C%20and%20that%20modest%20fine-tuning%20with%20DESI%20spectra%20further%20improves%20the%20results.%20For%20iron%20abundance%2C%20embeddings%20from%20a%20transformer-based%20model%20yield%20advantages%20in%20the%20metal-rich%20%28%5BFe/H%5D%20%3E%20-1.0%29%20regime%2C%20but%20underperform%20in%20the%20metal-poor%20regime%20compared%20to%20MLPs%20trained%20directly%20on%20LRS.%20We%20also%20show%20that%20the%20optimal%20fine-tuning%20strategy%20depends%20on%20the%20specific%20stellar%20parameter%20under%20consideration.%20These%20results%20highlight%20that%20simple%20pre-trained%20MLPs%20can%20provide%20competitive%20cross-survey%20generalization%2C%20while%20the%20role%20of%20spectral%20foundation%20models%20for%20cross-survey%20stellar%20parameter%20estimation%20requires%20further%20exploration.&entry.1838667208=http%3A//arxiv.org/abs/2602.15021v1&entry.124074799=Read"},
{"title": "Symmetry in language statistics shapes the geometry of model representations", "author": "Dhruva Karkada and Daniel J. Korchinski and Andres Nava and Matthieu Wyart and Yasaman Bahri", "abstract": "Although learned representations underlie neural networks' success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.", "link": "http://arxiv.org/abs/2602.15029v1", "date": "2026-02-16", "relevancy": 2.292, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symmetry%20in%20language%20statistics%20shapes%20the%20geometry%20of%20model%20representations&body=Title%3A%20Symmetry%20in%20language%20statistics%20shapes%20the%20geometry%20of%20model%20representations%0AAuthor%3A%20Dhruva%20Karkada%20and%20Daniel%20J.%20Korchinski%20and%20Andres%20Nava%20and%20Matthieu%20Wyart%20and%20Yasaman%20Bahri%0AAbstract%3A%20Although%20learned%20representations%20underlie%20neural%20networks%27%20success%2C%20their%20fundamental%20properties%20remain%20poorly%20understood.%20A%20striking%20example%20is%20the%20emergence%20of%20simple%20geometric%20structures%20in%20LLM%20representations%3A%20for%20example%2C%20calendar%20months%20organize%20into%20a%20circle%2C%20years%20form%20a%20smooth%20one-dimensional%20manifold%2C%20and%20cities%27%20latitudes%20and%20longitudes%20can%20be%20decoded%20by%20a%20linear%20probe.%20We%20show%20that%20the%20statistics%20of%20language%20exhibit%20a%20translation%20symmetry%20--%20e.g.%2C%20the%20co-occurrence%20probability%20of%20two%20months%20depends%20only%20on%20the%20time%20interval%20between%20them%20--%20and%20we%20prove%20that%20the%20latter%20governs%20the%20aforementioned%20geometric%20structures%20in%20high-dimensional%20word%20embedding%20models.%20Moreover%2C%20we%20find%20that%20these%20structures%20persist%20even%20when%20the%20co-occurrence%20statistics%20are%20strongly%20perturbed%20%28for%20example%2C%20by%20removing%20all%20sentences%20in%20which%20two%20months%20appear%20together%29%20and%20at%20moderate%20embedding%20dimension.%20We%20show%20that%20this%20robustness%20naturally%20emerges%20if%20the%20co-occurrence%20statistics%20are%20collectively%20controlled%20by%20an%20underlying%20continuous%20latent%20variable.%20We%20empirically%20validate%20this%20theoretical%20framework%20in%20word%20embedding%20models%2C%20text%20embedding%20models%2C%20and%20large%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymmetry%2520in%2520language%2520statistics%2520shapes%2520the%2520geometry%2520of%2520model%2520representations%26entry.906535625%3DDhruva%2520Karkada%2520and%2520Daniel%2520J.%2520Korchinski%2520and%2520Andres%2520Nava%2520and%2520Matthieu%2520Wyart%2520and%2520Yasaman%2520Bahri%26entry.1292438233%3DAlthough%2520learned%2520representations%2520underlie%2520neural%2520networks%2527%2520success%252C%2520their%2520fundamental%2520properties%2520remain%2520poorly%2520understood.%2520A%2520striking%2520example%2520is%2520the%2520emergence%2520of%2520simple%2520geometric%2520structures%2520in%2520LLM%2520representations%253A%2520for%2520example%252C%2520calendar%2520months%2520organize%2520into%2520a%2520circle%252C%2520years%2520form%2520a%2520smooth%2520one-dimensional%2520manifold%252C%2520and%2520cities%2527%2520latitudes%2520and%2520longitudes%2520can%2520be%2520decoded%2520by%2520a%2520linear%2520probe.%2520We%2520show%2520that%2520the%2520statistics%2520of%2520language%2520exhibit%2520a%2520translation%2520symmetry%2520--%2520e.g.%252C%2520the%2520co-occurrence%2520probability%2520of%2520two%2520months%2520depends%2520only%2520on%2520the%2520time%2520interval%2520between%2520them%2520--%2520and%2520we%2520prove%2520that%2520the%2520latter%2520governs%2520the%2520aforementioned%2520geometric%2520structures%2520in%2520high-dimensional%2520word%2520embedding%2520models.%2520Moreover%252C%2520we%2520find%2520that%2520these%2520structures%2520persist%2520even%2520when%2520the%2520co-occurrence%2520statistics%2520are%2520strongly%2520perturbed%2520%2528for%2520example%252C%2520by%2520removing%2520all%2520sentences%2520in%2520which%2520two%2520months%2520appear%2520together%2529%2520and%2520at%2520moderate%2520embedding%2520dimension.%2520We%2520show%2520that%2520this%2520robustness%2520naturally%2520emerges%2520if%2520the%2520co-occurrence%2520statistics%2520are%2520collectively%2520controlled%2520by%2520an%2520underlying%2520continuous%2520latent%2520variable.%2520We%2520empirically%2520validate%2520this%2520theoretical%2520framework%2520in%2520word%2520embedding%2520models%252C%2520text%2520embedding%2520models%252C%2520and%2520large%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symmetry%20in%20language%20statistics%20shapes%20the%20geometry%20of%20model%20representations&entry.906535625=Dhruva%20Karkada%20and%20Daniel%20J.%20Korchinski%20and%20Andres%20Nava%20and%20Matthieu%20Wyart%20and%20Yasaman%20Bahri&entry.1292438233=Although%20learned%20representations%20underlie%20neural%20networks%27%20success%2C%20their%20fundamental%20properties%20remain%20poorly%20understood.%20A%20striking%20example%20is%20the%20emergence%20of%20simple%20geometric%20structures%20in%20LLM%20representations%3A%20for%20example%2C%20calendar%20months%20organize%20into%20a%20circle%2C%20years%20form%20a%20smooth%20one-dimensional%20manifold%2C%20and%20cities%27%20latitudes%20and%20longitudes%20can%20be%20decoded%20by%20a%20linear%20probe.%20We%20show%20that%20the%20statistics%20of%20language%20exhibit%20a%20translation%20symmetry%20--%20e.g.%2C%20the%20co-occurrence%20probability%20of%20two%20months%20depends%20only%20on%20the%20time%20interval%20between%20them%20--%20and%20we%20prove%20that%20the%20latter%20governs%20the%20aforementioned%20geometric%20structures%20in%20high-dimensional%20word%20embedding%20models.%20Moreover%2C%20we%20find%20that%20these%20structures%20persist%20even%20when%20the%20co-occurrence%20statistics%20are%20strongly%20perturbed%20%28for%20example%2C%20by%20removing%20all%20sentences%20in%20which%20two%20months%20appear%20together%29%20and%20at%20moderate%20embedding%20dimension.%20We%20show%20that%20this%20robustness%20naturally%20emerges%20if%20the%20co-occurrence%20statistics%20are%20collectively%20controlled%20by%20an%20underlying%20continuous%20latent%20variable.%20We%20empirically%20validate%20this%20theoretical%20framework%20in%20word%20embedding%20models%2C%20text%20embedding%20models%2C%20and%20large%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.15029v1&entry.124074799=Read"},
{"title": "Depth Completion as Parameter-Efficient Test-Time Adaptation", "author": "Bingxin Ke and Qunjie Zhou and Jiahui Huang and Xuanchi Ren and Tianchang Shen and Konrad Schindler and Laura Leal-Taix\u00e9 and Shengyu Huang", "abstract": "We introduce CAPA, a parameter-efficient test-time optimization framework that adapts pre-trained 3D foundation models (FMs) for depth completion, using sparse geometric cues. Unlike prior methods that train task-specific encoders for auxiliary inputs, which often overfit and generalize poorly, CAPA freezes the FM backbone. Instead, it updates only a minimal set of parameters using Parameter-Efficient Fine-Tuning (e.g. LoRA or VPT), guided by gradients calculated directly from the sparse observations available at inference time. This approach effectively grounds the foundation model's geometric prior in the scene-specific measurements, correcting distortions and misplaced structures. For videos, CAPA introduces sequence-level parameter sharing, jointly adapting all frames to exploit temporal correlations, improve robustness, and enforce multi-frame consistency. CAPA is model-agnostic, compatible with any ViT-based FM, and achieves state-of-the-art results across diverse condition patterns on both indoor and outdoor datasets. Project page: research.nvidia.com/labs/dvl/projects/capa.", "link": "http://arxiv.org/abs/2602.14751v1", "date": "2026-02-16", "relevancy": 2.2892, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Completion%20as%20Parameter-Efficient%20Test-Time%20Adaptation&body=Title%3A%20Depth%20Completion%20as%20Parameter-Efficient%20Test-Time%20Adaptation%0AAuthor%3A%20Bingxin%20Ke%20and%20Qunjie%20Zhou%20and%20Jiahui%20Huang%20and%20Xuanchi%20Ren%20and%20Tianchang%20Shen%20and%20Konrad%20Schindler%20and%20Laura%20Leal-Taix%C3%A9%20and%20Shengyu%20Huang%0AAbstract%3A%20We%20introduce%20CAPA%2C%20a%20parameter-efficient%20test-time%20optimization%20framework%20that%20adapts%20pre-trained%203D%20foundation%20models%20%28FMs%29%20for%20depth%20completion%2C%20using%20sparse%20geometric%20cues.%20Unlike%20prior%20methods%20that%20train%20task-specific%20encoders%20for%20auxiliary%20inputs%2C%20which%20often%20overfit%20and%20generalize%20poorly%2C%20CAPA%20freezes%20the%20FM%20backbone.%20Instead%2C%20it%20updates%20only%20a%20minimal%20set%20of%20parameters%20using%20Parameter-Efficient%20Fine-Tuning%20%28e.g.%20LoRA%20or%20VPT%29%2C%20guided%20by%20gradients%20calculated%20directly%20from%20the%20sparse%20observations%20available%20at%20inference%20time.%20This%20approach%20effectively%20grounds%20the%20foundation%20model%27s%20geometric%20prior%20in%20the%20scene-specific%20measurements%2C%20correcting%20distortions%20and%20misplaced%20structures.%20For%20videos%2C%20CAPA%20introduces%20sequence-level%20parameter%20sharing%2C%20jointly%20adapting%20all%20frames%20to%20exploit%20temporal%20correlations%2C%20improve%20robustness%2C%20and%20enforce%20multi-frame%20consistency.%20CAPA%20is%20model-agnostic%2C%20compatible%20with%20any%20ViT-based%20FM%2C%20and%20achieves%20state-of-the-art%20results%20across%20diverse%20condition%20patterns%20on%20both%20indoor%20and%20outdoor%20datasets.%20Project%20page%3A%20research.nvidia.com/labs/dvl/projects/capa.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Completion%2520as%2520Parameter-Efficient%2520Test-Time%2520Adaptation%26entry.906535625%3DBingxin%2520Ke%2520and%2520Qunjie%2520Zhou%2520and%2520Jiahui%2520Huang%2520and%2520Xuanchi%2520Ren%2520and%2520Tianchang%2520Shen%2520and%2520Konrad%2520Schindler%2520and%2520Laura%2520Leal-Taix%25C3%25A9%2520and%2520Shengyu%2520Huang%26entry.1292438233%3DWe%2520introduce%2520CAPA%252C%2520a%2520parameter-efficient%2520test-time%2520optimization%2520framework%2520that%2520adapts%2520pre-trained%25203D%2520foundation%2520models%2520%2528FMs%2529%2520for%2520depth%2520completion%252C%2520using%2520sparse%2520geometric%2520cues.%2520Unlike%2520prior%2520methods%2520that%2520train%2520task-specific%2520encoders%2520for%2520auxiliary%2520inputs%252C%2520which%2520often%2520overfit%2520and%2520generalize%2520poorly%252C%2520CAPA%2520freezes%2520the%2520FM%2520backbone.%2520Instead%252C%2520it%2520updates%2520only%2520a%2520minimal%2520set%2520of%2520parameters%2520using%2520Parameter-Efficient%2520Fine-Tuning%2520%2528e.g.%2520LoRA%2520or%2520VPT%2529%252C%2520guided%2520by%2520gradients%2520calculated%2520directly%2520from%2520the%2520sparse%2520observations%2520available%2520at%2520inference%2520time.%2520This%2520approach%2520effectively%2520grounds%2520the%2520foundation%2520model%2527s%2520geometric%2520prior%2520in%2520the%2520scene-specific%2520measurements%252C%2520correcting%2520distortions%2520and%2520misplaced%2520structures.%2520For%2520videos%252C%2520CAPA%2520introduces%2520sequence-level%2520parameter%2520sharing%252C%2520jointly%2520adapting%2520all%2520frames%2520to%2520exploit%2520temporal%2520correlations%252C%2520improve%2520robustness%252C%2520and%2520enforce%2520multi-frame%2520consistency.%2520CAPA%2520is%2520model-agnostic%252C%2520compatible%2520with%2520any%2520ViT-based%2520FM%252C%2520and%2520achieves%2520state-of-the-art%2520results%2520across%2520diverse%2520condition%2520patterns%2520on%2520both%2520indoor%2520and%2520outdoor%2520datasets.%2520Project%2520page%253A%2520research.nvidia.com/labs/dvl/projects/capa.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Completion%20as%20Parameter-Efficient%20Test-Time%20Adaptation&entry.906535625=Bingxin%20Ke%20and%20Qunjie%20Zhou%20and%20Jiahui%20Huang%20and%20Xuanchi%20Ren%20and%20Tianchang%20Shen%20and%20Konrad%20Schindler%20and%20Laura%20Leal-Taix%C3%A9%20and%20Shengyu%20Huang&entry.1292438233=We%20introduce%20CAPA%2C%20a%20parameter-efficient%20test-time%20optimization%20framework%20that%20adapts%20pre-trained%203D%20foundation%20models%20%28FMs%29%20for%20depth%20completion%2C%20using%20sparse%20geometric%20cues.%20Unlike%20prior%20methods%20that%20train%20task-specific%20encoders%20for%20auxiliary%20inputs%2C%20which%20often%20overfit%20and%20generalize%20poorly%2C%20CAPA%20freezes%20the%20FM%20backbone.%20Instead%2C%20it%20updates%20only%20a%20minimal%20set%20of%20parameters%20using%20Parameter-Efficient%20Fine-Tuning%20%28e.g.%20LoRA%20or%20VPT%29%2C%20guided%20by%20gradients%20calculated%20directly%20from%20the%20sparse%20observations%20available%20at%20inference%20time.%20This%20approach%20effectively%20grounds%20the%20foundation%20model%27s%20geometric%20prior%20in%20the%20scene-specific%20measurements%2C%20correcting%20distortions%20and%20misplaced%20structures.%20For%20videos%2C%20CAPA%20introduces%20sequence-level%20parameter%20sharing%2C%20jointly%20adapting%20all%20frames%20to%20exploit%20temporal%20correlations%2C%20improve%20robustness%2C%20and%20enforce%20multi-frame%20consistency.%20CAPA%20is%20model-agnostic%2C%20compatible%20with%20any%20ViT-based%20FM%2C%20and%20achieves%20state-of-the-art%20results%20across%20diverse%20condition%20patterns%20on%20both%20indoor%20and%20outdoor%20datasets.%20Project%20page%3A%20research.nvidia.com/labs/dvl/projects/capa.&entry.1838667208=http%3A//arxiv.org/abs/2602.14751v1&entry.124074799=Read"},
{"title": "DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI", "author": "En Yu and Haoran Lv and Jianjian Sun and Kangheng Lin and Ruitao Zhang and Yukang Shi and Yuyang Chen and Ze Chen and Ziheng Zhang and Fan Jia and Kaixin Liu and Meng Zhang and Ruitao Hao and Saike Huang and Songhan Xie and Yu Liu and Zhao Wu and Bin Xie and Pengwei Zhang and Qi Yang and Xianchi Deng and Yunfei Wei and Enwen Zhang and Hongyang Peng and Jie Zhao and Kai Liu and Wei Sun and Yajun Wei and Yi Yang and Yunqiao Zhang and Ziwei Yan and Haitao Yang and Hao Liu and Haoqiang Fan and Haowei Zhang and Junwen Huang and Yang Chen and Yunchao Ma and Yunhuan Yang and Zhengyuan Du and Ziming Liu and Jiahui Niu and Yucheng Zhao and Daxin Jiang and Wenbin Tang and Xiangyu Zhang and Zheng Ge and Erjin Zhou and Tiancai Wang", "abstract": "Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.", "link": "http://arxiv.org/abs/2602.14974v1", "date": "2026-02-16", "relevancy": 2.2868, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DM0%3A%20An%20Embodied-Native%20Vision-Language-Action%20Model%20towards%20Physical%20AI&body=Title%3A%20DM0%3A%20An%20Embodied-Native%20Vision-Language-Action%20Model%20towards%20Physical%20AI%0AAuthor%3A%20En%20Yu%20and%20Haoran%20Lv%20and%20Jianjian%20Sun%20and%20Kangheng%20Lin%20and%20Ruitao%20Zhang%20and%20Yukang%20Shi%20and%20Yuyang%20Chen%20and%20Ze%20Chen%20and%20Ziheng%20Zhang%20and%20Fan%20Jia%20and%20Kaixin%20Liu%20and%20Meng%20Zhang%20and%20Ruitao%20Hao%20and%20Saike%20Huang%20and%20Songhan%20Xie%20and%20Yu%20Liu%20and%20Zhao%20Wu%20and%20Bin%20Xie%20and%20Pengwei%20Zhang%20and%20Qi%20Yang%20and%20Xianchi%20Deng%20and%20Yunfei%20Wei%20and%20Enwen%20Zhang%20and%20Hongyang%20Peng%20and%20Jie%20Zhao%20and%20Kai%20Liu%20and%20Wei%20Sun%20and%20Yajun%20Wei%20and%20Yi%20Yang%20and%20Yunqiao%20Zhang%20and%20Ziwei%20Yan%20and%20Haitao%20Yang%20and%20Hao%20Liu%20and%20Haoqiang%20Fan%20and%20Haowei%20Zhang%20and%20Junwen%20Huang%20and%20Yang%20Chen%20and%20Yunchao%20Ma%20and%20Yunhuan%20Yang%20and%20Zhengyuan%20Du%20and%20Ziming%20Liu%20and%20Jiahui%20Niu%20and%20Yucheng%20Zhao%20and%20Daxin%20Jiang%20and%20Wenbin%20Tang%20and%20Xiangyu%20Zhang%20and%20Zheng%20Ge%20and%20Erjin%20Zhou%20and%20Tiancai%20Wang%0AAbstract%3A%20Moving%20beyond%20the%20traditional%20paradigm%20of%20adapting%20internet-pretrained%20models%20to%20physical%20tasks%2C%20we%20present%20DM0%2C%20an%20Embodied-Native%20Vision-Language-Action%20%28VLA%29%20framework%20designed%20for%20Physical%20AI.%20Unlike%20approaches%20that%20treat%20physical%20grounding%20as%20a%20fine-tuning%20afterthought%2C%20DM0%20unifies%20embodied%20manipulation%20and%20navigation%20by%20learning%20from%20heterogeneous%20data%20sources%20from%20the%20onset.%20Our%20methodology%20follows%20a%20comprehensive%20three-stage%20pipeline%3A%20Pretraining%2C%20Mid-Training%2C%20and%20Post-Training.%20First%2C%20we%20conduct%20large-scale%20unified%20pretraining%20on%20the%20Vision-Language%20Model%20%28VLM%29%20using%20diverse%20corpora--seamlessly%20integrating%20web%20text%2C%20autonomous%20driving%20scenarios%2C%20and%20embodied%20interaction%20logs-to%20jointly%20acquire%20semantic%20knowledge%20and%20physical%20priors.%20Subsequently%2C%20we%20build%20a%20flow-matching%20action%20expert%20atop%20the%20VLM.%20To%20reconcile%20high-level%20reasoning%20with%20low-level%20control%2C%20DM0%20employs%20a%20hybrid%20training%20strategy%3A%20for%20embodied%20data%2C%20gradients%20from%20the%20action%20expert%20are%20not%20backpropagated%20to%20the%20VLM%20to%20preserve%20generalized%20representations%2C%20while%20the%20VLM%20remains%20trainable%20on%20non-embodied%20data.%20Furthermore%2C%20we%20introduce%20an%20Embodied%20Spatial%20Scaffolding%20strategy%20to%20construct%20spatial%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20effectively%20constraining%20the%20action%20solution%20space.%20Experiments%20on%20the%20RoboChallenge%20benchmark%20demonstrate%20that%20DM0%20achieves%20state-of-the-art%20performance%20in%20both%20Specialist%20and%20Generalist%20settings%20on%20Table30.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDM0%253A%2520An%2520Embodied-Native%2520Vision-Language-Action%2520Model%2520towards%2520Physical%2520AI%26entry.906535625%3DEn%2520Yu%2520and%2520Haoran%2520Lv%2520and%2520Jianjian%2520Sun%2520and%2520Kangheng%2520Lin%2520and%2520Ruitao%2520Zhang%2520and%2520Yukang%2520Shi%2520and%2520Yuyang%2520Chen%2520and%2520Ze%2520Chen%2520and%2520Ziheng%2520Zhang%2520and%2520Fan%2520Jia%2520and%2520Kaixin%2520Liu%2520and%2520Meng%2520Zhang%2520and%2520Ruitao%2520Hao%2520and%2520Saike%2520Huang%2520and%2520Songhan%2520Xie%2520and%2520Yu%2520Liu%2520and%2520Zhao%2520Wu%2520and%2520Bin%2520Xie%2520and%2520Pengwei%2520Zhang%2520and%2520Qi%2520Yang%2520and%2520Xianchi%2520Deng%2520and%2520Yunfei%2520Wei%2520and%2520Enwen%2520Zhang%2520and%2520Hongyang%2520Peng%2520and%2520Jie%2520Zhao%2520and%2520Kai%2520Liu%2520and%2520Wei%2520Sun%2520and%2520Yajun%2520Wei%2520and%2520Yi%2520Yang%2520and%2520Yunqiao%2520Zhang%2520and%2520Ziwei%2520Yan%2520and%2520Haitao%2520Yang%2520and%2520Hao%2520Liu%2520and%2520Haoqiang%2520Fan%2520and%2520Haowei%2520Zhang%2520and%2520Junwen%2520Huang%2520and%2520Yang%2520Chen%2520and%2520Yunchao%2520Ma%2520and%2520Yunhuan%2520Yang%2520and%2520Zhengyuan%2520Du%2520and%2520Ziming%2520Liu%2520and%2520Jiahui%2520Niu%2520and%2520Yucheng%2520Zhao%2520and%2520Daxin%2520Jiang%2520and%2520Wenbin%2520Tang%2520and%2520Xiangyu%2520Zhang%2520and%2520Zheng%2520Ge%2520and%2520Erjin%2520Zhou%2520and%2520Tiancai%2520Wang%26entry.1292438233%3DMoving%2520beyond%2520the%2520traditional%2520paradigm%2520of%2520adapting%2520internet-pretrained%2520models%2520to%2520physical%2520tasks%252C%2520we%2520present%2520DM0%252C%2520an%2520Embodied-Native%2520Vision-Language-Action%2520%2528VLA%2529%2520framework%2520designed%2520for%2520Physical%2520AI.%2520Unlike%2520approaches%2520that%2520treat%2520physical%2520grounding%2520as%2520a%2520fine-tuning%2520afterthought%252C%2520DM0%2520unifies%2520embodied%2520manipulation%2520and%2520navigation%2520by%2520learning%2520from%2520heterogeneous%2520data%2520sources%2520from%2520the%2520onset.%2520Our%2520methodology%2520follows%2520a%2520comprehensive%2520three-stage%2520pipeline%253A%2520Pretraining%252C%2520Mid-Training%252C%2520and%2520Post-Training.%2520First%252C%2520we%2520conduct%2520large-scale%2520unified%2520pretraining%2520on%2520the%2520Vision-Language%2520Model%2520%2528VLM%2529%2520using%2520diverse%2520corpora--seamlessly%2520integrating%2520web%2520text%252C%2520autonomous%2520driving%2520scenarios%252C%2520and%2520embodied%2520interaction%2520logs-to%2520jointly%2520acquire%2520semantic%2520knowledge%2520and%2520physical%2520priors.%2520Subsequently%252C%2520we%2520build%2520a%2520flow-matching%2520action%2520expert%2520atop%2520the%2520VLM.%2520To%2520reconcile%2520high-level%2520reasoning%2520with%2520low-level%2520control%252C%2520DM0%2520employs%2520a%2520hybrid%2520training%2520strategy%253A%2520for%2520embodied%2520data%252C%2520gradients%2520from%2520the%2520action%2520expert%2520are%2520not%2520backpropagated%2520to%2520the%2520VLM%2520to%2520preserve%2520generalized%2520representations%252C%2520while%2520the%2520VLM%2520remains%2520trainable%2520on%2520non-embodied%2520data.%2520Furthermore%252C%2520we%2520introduce%2520an%2520Embodied%2520Spatial%2520Scaffolding%2520strategy%2520to%2520construct%2520spatial%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%252C%2520effectively%2520constraining%2520the%2520action%2520solution%2520space.%2520Experiments%2520on%2520the%2520RoboChallenge%2520benchmark%2520demonstrate%2520that%2520DM0%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520Specialist%2520and%2520Generalist%2520settings%2520on%2520Table30.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DM0%3A%20An%20Embodied-Native%20Vision-Language-Action%20Model%20towards%20Physical%20AI&entry.906535625=En%20Yu%20and%20Haoran%20Lv%20and%20Jianjian%20Sun%20and%20Kangheng%20Lin%20and%20Ruitao%20Zhang%20and%20Yukang%20Shi%20and%20Yuyang%20Chen%20and%20Ze%20Chen%20and%20Ziheng%20Zhang%20and%20Fan%20Jia%20and%20Kaixin%20Liu%20and%20Meng%20Zhang%20and%20Ruitao%20Hao%20and%20Saike%20Huang%20and%20Songhan%20Xie%20and%20Yu%20Liu%20and%20Zhao%20Wu%20and%20Bin%20Xie%20and%20Pengwei%20Zhang%20and%20Qi%20Yang%20and%20Xianchi%20Deng%20and%20Yunfei%20Wei%20and%20Enwen%20Zhang%20and%20Hongyang%20Peng%20and%20Jie%20Zhao%20and%20Kai%20Liu%20and%20Wei%20Sun%20and%20Yajun%20Wei%20and%20Yi%20Yang%20and%20Yunqiao%20Zhang%20and%20Ziwei%20Yan%20and%20Haitao%20Yang%20and%20Hao%20Liu%20and%20Haoqiang%20Fan%20and%20Haowei%20Zhang%20and%20Junwen%20Huang%20and%20Yang%20Chen%20and%20Yunchao%20Ma%20and%20Yunhuan%20Yang%20and%20Zhengyuan%20Du%20and%20Ziming%20Liu%20and%20Jiahui%20Niu%20and%20Yucheng%20Zhao%20and%20Daxin%20Jiang%20and%20Wenbin%20Tang%20and%20Xiangyu%20Zhang%20and%20Zheng%20Ge%20and%20Erjin%20Zhou%20and%20Tiancai%20Wang&entry.1292438233=Moving%20beyond%20the%20traditional%20paradigm%20of%20adapting%20internet-pretrained%20models%20to%20physical%20tasks%2C%20we%20present%20DM0%2C%20an%20Embodied-Native%20Vision-Language-Action%20%28VLA%29%20framework%20designed%20for%20Physical%20AI.%20Unlike%20approaches%20that%20treat%20physical%20grounding%20as%20a%20fine-tuning%20afterthought%2C%20DM0%20unifies%20embodied%20manipulation%20and%20navigation%20by%20learning%20from%20heterogeneous%20data%20sources%20from%20the%20onset.%20Our%20methodology%20follows%20a%20comprehensive%20three-stage%20pipeline%3A%20Pretraining%2C%20Mid-Training%2C%20and%20Post-Training.%20First%2C%20we%20conduct%20large-scale%20unified%20pretraining%20on%20the%20Vision-Language%20Model%20%28VLM%29%20using%20diverse%20corpora--seamlessly%20integrating%20web%20text%2C%20autonomous%20driving%20scenarios%2C%20and%20embodied%20interaction%20logs-to%20jointly%20acquire%20semantic%20knowledge%20and%20physical%20priors.%20Subsequently%2C%20we%20build%20a%20flow-matching%20action%20expert%20atop%20the%20VLM.%20To%20reconcile%20high-level%20reasoning%20with%20low-level%20control%2C%20DM0%20employs%20a%20hybrid%20training%20strategy%3A%20for%20embodied%20data%2C%20gradients%20from%20the%20action%20expert%20are%20not%20backpropagated%20to%20the%20VLM%20to%20preserve%20generalized%20representations%2C%20while%20the%20VLM%20remains%20trainable%20on%20non-embodied%20data.%20Furthermore%2C%20we%20introduce%20an%20Embodied%20Spatial%20Scaffolding%20strategy%20to%20construct%20spatial%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20effectively%20constraining%20the%20action%20solution%20space.%20Experiments%20on%20the%20RoboChallenge%20benchmark%20demonstrate%20that%20DM0%20achieves%20state-of-the-art%20performance%20in%20both%20Specialist%20and%20Generalist%20settings%20on%20Table30.&entry.1838667208=http%3A//arxiv.org/abs/2602.14974v1&entry.124074799=Read"},
{"title": "LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations", "author": "Vittoria De Pellegrini and Tariq Alkhalifah", "abstract": "Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and to quantify uncertainty. To tackle this challenge, we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running two orders of magnitude faster than traditional numerical solvers.", "link": "http://arxiv.org/abs/2601.13190v2", "date": "2026-02-16", "relevancy": 2.2847, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6147}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5675}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LAViG-FLOW%3A%20Latent%20Autoregressive%20Video%20Generation%20for%20Fluid%20Flow%20Simulations&body=Title%3A%20LAViG-FLOW%3A%20Latent%20Autoregressive%20Video%20Generation%20for%20Fluid%20Flow%20Simulations%0AAuthor%3A%20Vittoria%20De%20Pellegrini%20and%20Tariq%20Alkhalifah%0AAbstract%3A%20Modeling%20and%20forecasting%20subsurface%20multiphase%20fluid%20flow%20fields%20underpin%20applications%20ranging%20from%20geological%20CO2%20sequestration%20%28GCS%29%20operations%20to%20geothermal%20production.%20This%20is%20essential%20for%20ensuring%20both%20operational%20performance%20and%20long-term%20safety.%20While%20high%20fidelity%20multiphase%20simulators%20are%20widely%20used%20for%20this%20purpose%2C%20they%20become%20prohibitively%20expensive%20once%20many%20forward%20runs%20are%20required%20for%20inversion%20purposes%20and%20to%20quantify%20uncertainty.%20To%20tackle%20this%20challenge%2C%20we%20propose%20LAViG-FLOW%2C%20a%20latent%20autoregressive%20video%20generation%20diffusion%20framework%20that%20explicitly%20learns%20the%20coupled%20evolution%20of%20saturation%20and%20pressure%20fields.%20Each%20state%20variable%20is%20compressed%20by%20a%20dedicated%202D%20autoencoder%2C%20and%20a%20Video%20Diffusion%20Transformer%20%28VDiT%29%20models%20their%20coupled%20distribution%20across%20time.%20We%20first%20train%20the%20model%20on%20a%20given%20time%20horizon%20to%20learn%20their%20coupled%20relationship%20and%20then%20fine-tune%20it%20autoregressively%20so%20it%20can%20extrapolate%20beyond%20the%20observed%20time%20window.%20Evaluated%20on%20an%20open-source%20CO2%20sequestration%20dataset%2C%20LAViG-FLOW%20generates%20saturation%20and%20pressure%20fields%20that%20stay%20consistent%20across%20time%20while%20running%20two%20orders%20of%20magnitude%20faster%20than%20traditional%20numerical%20solvers.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13190v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLAViG-FLOW%253A%2520Latent%2520Autoregressive%2520Video%2520Generation%2520for%2520Fluid%2520Flow%2520Simulations%26entry.906535625%3DVittoria%2520De%2520Pellegrini%2520and%2520Tariq%2520Alkhalifah%26entry.1292438233%3DModeling%2520and%2520forecasting%2520subsurface%2520multiphase%2520fluid%2520flow%2520fields%2520underpin%2520applications%2520ranging%2520from%2520geological%2520CO2%2520sequestration%2520%2528GCS%2529%2520operations%2520to%2520geothermal%2520production.%2520This%2520is%2520essential%2520for%2520ensuring%2520both%2520operational%2520performance%2520and%2520long-term%2520safety.%2520While%2520high%2520fidelity%2520multiphase%2520simulators%2520are%2520widely%2520used%2520for%2520this%2520purpose%252C%2520they%2520become%2520prohibitively%2520expensive%2520once%2520many%2520forward%2520runs%2520are%2520required%2520for%2520inversion%2520purposes%2520and%2520to%2520quantify%2520uncertainty.%2520To%2520tackle%2520this%2520challenge%252C%2520we%2520propose%2520LAViG-FLOW%252C%2520a%2520latent%2520autoregressive%2520video%2520generation%2520diffusion%2520framework%2520that%2520explicitly%2520learns%2520the%2520coupled%2520evolution%2520of%2520saturation%2520and%2520pressure%2520fields.%2520Each%2520state%2520variable%2520is%2520compressed%2520by%2520a%2520dedicated%25202D%2520autoencoder%252C%2520and%2520a%2520Video%2520Diffusion%2520Transformer%2520%2528VDiT%2529%2520models%2520their%2520coupled%2520distribution%2520across%2520time.%2520We%2520first%2520train%2520the%2520model%2520on%2520a%2520given%2520time%2520horizon%2520to%2520learn%2520their%2520coupled%2520relationship%2520and%2520then%2520fine-tune%2520it%2520autoregressively%2520so%2520it%2520can%2520extrapolate%2520beyond%2520the%2520observed%2520time%2520window.%2520Evaluated%2520on%2520an%2520open-source%2520CO2%2520sequestration%2520dataset%252C%2520LAViG-FLOW%2520generates%2520saturation%2520and%2520pressure%2520fields%2520that%2520stay%2520consistent%2520across%2520time%2520while%2520running%2520two%2520orders%2520of%2520magnitude%2520faster%2520than%2520traditional%2520numerical%2520solvers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13190v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LAViG-FLOW%3A%20Latent%20Autoregressive%20Video%20Generation%20for%20Fluid%20Flow%20Simulations&entry.906535625=Vittoria%20De%20Pellegrini%20and%20Tariq%20Alkhalifah&entry.1292438233=Modeling%20and%20forecasting%20subsurface%20multiphase%20fluid%20flow%20fields%20underpin%20applications%20ranging%20from%20geological%20CO2%20sequestration%20%28GCS%29%20operations%20to%20geothermal%20production.%20This%20is%20essential%20for%20ensuring%20both%20operational%20performance%20and%20long-term%20safety.%20While%20high%20fidelity%20multiphase%20simulators%20are%20widely%20used%20for%20this%20purpose%2C%20they%20become%20prohibitively%20expensive%20once%20many%20forward%20runs%20are%20required%20for%20inversion%20purposes%20and%20to%20quantify%20uncertainty.%20To%20tackle%20this%20challenge%2C%20we%20propose%20LAViG-FLOW%2C%20a%20latent%20autoregressive%20video%20generation%20diffusion%20framework%20that%20explicitly%20learns%20the%20coupled%20evolution%20of%20saturation%20and%20pressure%20fields.%20Each%20state%20variable%20is%20compressed%20by%20a%20dedicated%202D%20autoencoder%2C%20and%20a%20Video%20Diffusion%20Transformer%20%28VDiT%29%20models%20their%20coupled%20distribution%20across%20time.%20We%20first%20train%20the%20model%20on%20a%20given%20time%20horizon%20to%20learn%20their%20coupled%20relationship%20and%20then%20fine-tune%20it%20autoregressively%20so%20it%20can%20extrapolate%20beyond%20the%20observed%20time%20window.%20Evaluated%20on%20an%20open-source%20CO2%20sequestration%20dataset%2C%20LAViG-FLOW%20generates%20saturation%20and%20pressure%20fields%20that%20stay%20consistent%20across%20time%20while%20running%20two%20orders%20of%20magnitude%20faster%20than%20traditional%20numerical%20solvers.&entry.1838667208=http%3A//arxiv.org/abs/2601.13190v2&entry.124074799=Read"},
{"title": "Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation", "author": "Lorenzo Mur Labadia and Ruben Martinez-Cantin and Jose J. Guerrero and Giovanni M. Farinella and Antonino Furnari", "abstract": "Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.", "link": "http://arxiv.org/abs/2602.14837v1", "date": "2026-02-16", "relevancy": 2.2684, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5695}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5681}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Affordances%20and%20Attention%20models%20for%20Short-Term%20Object%20Interaction%20Anticipation&body=Title%3A%20Integrating%20Affordances%20and%20Attention%20models%20for%20Short-Term%20Object%20Interaction%20Anticipation%0AAuthor%3A%20Lorenzo%20Mur%20Labadia%20and%20Ruben%20Martinez-Cantin%20and%20Jose%20J.%20Guerrero%20and%20Giovanni%20M.%20Farinella%20and%20Antonino%20Furnari%0AAbstract%3A%20Short%20Term%20object-interaction%20Anticipation%20consists%20in%20detecting%20the%20location%20of%20the%20next%20active%20objects%2C%20the%20noun%20and%20verb%20categories%20of%20the%20interaction%2C%20as%20well%20as%20the%20time%20to%20contact%20from%20the%20observation%20of%20egocentric%20video.%20This%20ability%20is%20fundamental%20for%20wearable%20assistants%20to%20understand%20user%20goals%20and%20provide%20timely%20assistance%2C%20or%20to%20enable%20human-robot%20interaction.%20In%20this%20work%2C%20we%20present%20a%20method%20to%20improve%20the%20performance%20of%20STA%20predictions.%20Our%20contributions%20are%20two-fold%3A%201%20We%20propose%20STAformer%20and%20STAformer%20plus%20plus%2C%20two%20novel%20attention-based%20architectures%20integrating%20frame-guided%20temporal%20pooling%2C%20dual%20image-video%20attention%2C%20and%20multiscale%20feature%20fusion%20to%20support%20STA%20predictions%20from%20an%20image-input%20video%20pair%3B%202%20We%20introduce%20two%20novel%20modules%20to%20ground%20STA%20predictions%20on%20human%20behavior%20by%20modeling%20affordances.%20First%2C%20we%20integrate%20an%20environment%20affordance%20model%20which%20acts%20as%20a%20persistent%20memory%20of%20interactions%20that%20can%20take%20place%20in%20a%20given%20physical%20scene.%20We%20explore%20how%20to%20integrate%20environment%20affordances%20via%20simple%20late%20fusion%20and%20with%20an%20approach%20which%20adaptively%20learns%20how%20to%20best%20fuse%20affordances%20with%20end-to-end%20predictions.%20Second%2C%20we%20predict%20interaction%20hotspots%20from%20the%20observation%20of%20hands%20and%20object%20trajectories%2C%20increasing%20confidence%20in%20STA%20predictions%20localized%20around%20the%20hotspot.%20Our%20results%20show%20significant%20improvements%20on%20Overall%20Top-5%20mAP%2C%20with%20gain%20up%20to%20%2B23p.p%20on%20Ego4D%20and%20%2B31p.p%20on%20a%20novel%20set%20of%20curated%20EPIC-Kitchens%20STA%20labels.%20We%20released%20the%20code%2C%20annotations%2C%20and%20pre-extracted%20affordances%20on%20Ego4D%20and%20EPIC-Kitchens%20to%20encourage%20future%20research%20in%20this%20area.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Affordances%2520and%2520Attention%2520models%2520for%2520Short-Term%2520Object%2520Interaction%2520Anticipation%26entry.906535625%3DLorenzo%2520Mur%2520Labadia%2520and%2520Ruben%2520Martinez-Cantin%2520and%2520Jose%2520J.%2520Guerrero%2520and%2520Giovanni%2520M.%2520Farinella%2520and%2520Antonino%2520Furnari%26entry.1292438233%3DShort%2520Term%2520object-interaction%2520Anticipation%2520consists%2520in%2520detecting%2520the%2520location%2520of%2520the%2520next%2520active%2520objects%252C%2520the%2520noun%2520and%2520verb%2520categories%2520of%2520the%2520interaction%252C%2520as%2520well%2520as%2520the%2520time%2520to%2520contact%2520from%2520the%2520observation%2520of%2520egocentric%2520video.%2520This%2520ability%2520is%2520fundamental%2520for%2520wearable%2520assistants%2520to%2520understand%2520user%2520goals%2520and%2520provide%2520timely%2520assistance%252C%2520or%2520to%2520enable%2520human-robot%2520interaction.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520method%2520to%2520improve%2520the%2520performance%2520of%2520STA%2520predictions.%2520Our%2520contributions%2520are%2520two-fold%253A%25201%2520We%2520propose%2520STAformer%2520and%2520STAformer%2520plus%2520plus%252C%2520two%2520novel%2520attention-based%2520architectures%2520integrating%2520frame-guided%2520temporal%2520pooling%252C%2520dual%2520image-video%2520attention%252C%2520and%2520multiscale%2520feature%2520fusion%2520to%2520support%2520STA%2520predictions%2520from%2520an%2520image-input%2520video%2520pair%253B%25202%2520We%2520introduce%2520two%2520novel%2520modules%2520to%2520ground%2520STA%2520predictions%2520on%2520human%2520behavior%2520by%2520modeling%2520affordances.%2520First%252C%2520we%2520integrate%2520an%2520environment%2520affordance%2520model%2520which%2520acts%2520as%2520a%2520persistent%2520memory%2520of%2520interactions%2520that%2520can%2520take%2520place%2520in%2520a%2520given%2520physical%2520scene.%2520We%2520explore%2520how%2520to%2520integrate%2520environment%2520affordances%2520via%2520simple%2520late%2520fusion%2520and%2520with%2520an%2520approach%2520which%2520adaptively%2520learns%2520how%2520to%2520best%2520fuse%2520affordances%2520with%2520end-to-end%2520predictions.%2520Second%252C%2520we%2520predict%2520interaction%2520hotspots%2520from%2520the%2520observation%2520of%2520hands%2520and%2520object%2520trajectories%252C%2520increasing%2520confidence%2520in%2520STA%2520predictions%2520localized%2520around%2520the%2520hotspot.%2520Our%2520results%2520show%2520significant%2520improvements%2520on%2520Overall%2520Top-5%2520mAP%252C%2520with%2520gain%2520up%2520to%2520%252B23p.p%2520on%2520Ego4D%2520and%2520%252B31p.p%2520on%2520a%2520novel%2520set%2520of%2520curated%2520EPIC-Kitchens%2520STA%2520labels.%2520We%2520released%2520the%2520code%252C%2520annotations%252C%2520and%2520pre-extracted%2520affordances%2520on%2520Ego4D%2520and%2520EPIC-Kitchens%2520to%2520encourage%2520future%2520research%2520in%2520this%2520area.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Affordances%20and%20Attention%20models%20for%20Short-Term%20Object%20Interaction%20Anticipation&entry.906535625=Lorenzo%20Mur%20Labadia%20and%20Ruben%20Martinez-Cantin%20and%20Jose%20J.%20Guerrero%20and%20Giovanni%20M.%20Farinella%20and%20Antonino%20Furnari&entry.1292438233=Short%20Term%20object-interaction%20Anticipation%20consists%20in%20detecting%20the%20location%20of%20the%20next%20active%20objects%2C%20the%20noun%20and%20verb%20categories%20of%20the%20interaction%2C%20as%20well%20as%20the%20time%20to%20contact%20from%20the%20observation%20of%20egocentric%20video.%20This%20ability%20is%20fundamental%20for%20wearable%20assistants%20to%20understand%20user%20goals%20and%20provide%20timely%20assistance%2C%20or%20to%20enable%20human-robot%20interaction.%20In%20this%20work%2C%20we%20present%20a%20method%20to%20improve%20the%20performance%20of%20STA%20predictions.%20Our%20contributions%20are%20two-fold%3A%201%20We%20propose%20STAformer%20and%20STAformer%20plus%20plus%2C%20two%20novel%20attention-based%20architectures%20integrating%20frame-guided%20temporal%20pooling%2C%20dual%20image-video%20attention%2C%20and%20multiscale%20feature%20fusion%20to%20support%20STA%20predictions%20from%20an%20image-input%20video%20pair%3B%202%20We%20introduce%20two%20novel%20modules%20to%20ground%20STA%20predictions%20on%20human%20behavior%20by%20modeling%20affordances.%20First%2C%20we%20integrate%20an%20environment%20affordance%20model%20which%20acts%20as%20a%20persistent%20memory%20of%20interactions%20that%20can%20take%20place%20in%20a%20given%20physical%20scene.%20We%20explore%20how%20to%20integrate%20environment%20affordances%20via%20simple%20late%20fusion%20and%20with%20an%20approach%20which%20adaptively%20learns%20how%20to%20best%20fuse%20affordances%20with%20end-to-end%20predictions.%20Second%2C%20we%20predict%20interaction%20hotspots%20from%20the%20observation%20of%20hands%20and%20object%20trajectories%2C%20increasing%20confidence%20in%20STA%20predictions%20localized%20around%20the%20hotspot.%20Our%20results%20show%20significant%20improvements%20on%20Overall%20Top-5%20mAP%2C%20with%20gain%20up%20to%20%2B23p.p%20on%20Ego4D%20and%20%2B31p.p%20on%20a%20novel%20set%20of%20curated%20EPIC-Kitchens%20STA%20labels.%20We%20released%20the%20code%2C%20annotations%2C%20and%20pre-extracted%20affordances%20on%20Ego4D%20and%20EPIC-Kitchens%20to%20encourage%20future%20research%20in%20this%20area.&entry.1838667208=http%3A//arxiv.org/abs/2602.14837v1&entry.124074799=Read"},
{"title": "Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery", "author": "Chandrakanth Gudavalli and Tajuddin Manhar Mohammed and Abhay Yadav and Ananth Vishnu Bhaskar and Hardik Prajapati and Cheng Peng and Rama Chellappa and Shivkumar Chandrasekaran and B. S. Manjunath", "abstract": "Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.", "link": "http://arxiv.org/abs/2602.14929v1", "date": "2026-02-16", "relevancy": 2.2427, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5658}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.564}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wrivinder%3A%20Towards%20Spatial%20Intelligence%20for%20Geo-locating%20Ground%20Images%20onto%20Satellite%20Imagery&body=Title%3A%20Wrivinder%3A%20Towards%20Spatial%20Intelligence%20for%20Geo-locating%20Ground%20Images%20onto%20Satellite%20Imagery%0AAuthor%3A%20Chandrakanth%20Gudavalli%20and%20Tajuddin%20Manhar%20Mohammed%20and%20Abhay%20Yadav%20and%20Ananth%20Vishnu%20Bhaskar%20and%20Hardik%20Prajapati%20and%20Cheng%20Peng%20and%20Rama%20Chellappa%20and%20Shivkumar%20Chandrasekaran%20and%20B.%20S.%20Manjunath%0AAbstract%3A%20Aligning%20ground-level%20imagery%20with%20geo-registered%20satellite%20maps%20is%20crucial%20for%20mapping%2C%20navigation%2C%20and%20situational%20awareness%2C%20yet%20remains%20challenging%20under%20large%20viewpoint%20gaps%20or%20when%20GPS%20is%20unreliable.%20We%20introduce%20Wrivinder%2C%20a%20zero-shot%2C%20geometry-driven%20framework%20that%20aggregates%20multiple%20ground%20photographs%20to%20reconstruct%20a%20consistent%203D%20scene%20and%20align%20it%20with%20overhead%20satellite%20imagery.%20Wrivinder%20combines%20SfM%20reconstruction%2C%203D%20Gaussian%20Splatting%2C%20semantic%20grounding%2C%20and%20monocular%20depth--based%20metric%20cues%20to%20produce%20a%20stable%20zenith-view%20rendering%20that%20can%20be%20directly%20matched%20to%20satellite%20context%20for%20metrically%20accurate%20camera%20geo-localization.%20To%20support%20systematic%20evaluation%20of%20this%20task%2C%20which%20lacks%20suitable%20benchmarks%2C%20we%20also%20release%20MC-Sat%2C%20a%20curated%20dataset%20linking%20multi-view%20ground%20imagery%20with%20geo-registered%20satellite%20tiles%20across%20diverse%20outdoor%20environments.%20Together%2C%20Wrivinder%20and%20MC-Sat%20provide%20a%20first%20comprehensive%20baseline%20and%20testbed%20for%20studying%20geometry-centered%20cross-view%20alignment%20without%20paired%20supervision.%20In%20zero-shot%20experiments%2C%20Wrivinder%20achieves%20sub-30%5C%2Cm%20geolocation%20accuracy%20across%20both%20dense%20and%20large-area%20scenes%2C%20highlighting%20the%20promise%20of%20geometry-based%20aggregation%20for%20robust%20ground-to-satellite%20localization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWrivinder%253A%2520Towards%2520Spatial%2520Intelligence%2520for%2520Geo-locating%2520Ground%2520Images%2520onto%2520Satellite%2520Imagery%26entry.906535625%3DChandrakanth%2520Gudavalli%2520and%2520Tajuddin%2520Manhar%2520Mohammed%2520and%2520Abhay%2520Yadav%2520and%2520Ananth%2520Vishnu%2520Bhaskar%2520and%2520Hardik%2520Prajapati%2520and%2520Cheng%2520Peng%2520and%2520Rama%2520Chellappa%2520and%2520Shivkumar%2520Chandrasekaran%2520and%2520B.%2520S.%2520Manjunath%26entry.1292438233%3DAligning%2520ground-level%2520imagery%2520with%2520geo-registered%2520satellite%2520maps%2520is%2520crucial%2520for%2520mapping%252C%2520navigation%252C%2520and%2520situational%2520awareness%252C%2520yet%2520remains%2520challenging%2520under%2520large%2520viewpoint%2520gaps%2520or%2520when%2520GPS%2520is%2520unreliable.%2520We%2520introduce%2520Wrivinder%252C%2520a%2520zero-shot%252C%2520geometry-driven%2520framework%2520that%2520aggregates%2520multiple%2520ground%2520photographs%2520to%2520reconstruct%2520a%2520consistent%25203D%2520scene%2520and%2520align%2520it%2520with%2520overhead%2520satellite%2520imagery.%2520Wrivinder%2520combines%2520SfM%2520reconstruction%252C%25203D%2520Gaussian%2520Splatting%252C%2520semantic%2520grounding%252C%2520and%2520monocular%2520depth--based%2520metric%2520cues%2520to%2520produce%2520a%2520stable%2520zenith-view%2520rendering%2520that%2520can%2520be%2520directly%2520matched%2520to%2520satellite%2520context%2520for%2520metrically%2520accurate%2520camera%2520geo-localization.%2520To%2520support%2520systematic%2520evaluation%2520of%2520this%2520task%252C%2520which%2520lacks%2520suitable%2520benchmarks%252C%2520we%2520also%2520release%2520MC-Sat%252C%2520a%2520curated%2520dataset%2520linking%2520multi-view%2520ground%2520imagery%2520with%2520geo-registered%2520satellite%2520tiles%2520across%2520diverse%2520outdoor%2520environments.%2520Together%252C%2520Wrivinder%2520and%2520MC-Sat%2520provide%2520a%2520first%2520comprehensive%2520baseline%2520and%2520testbed%2520for%2520studying%2520geometry-centered%2520cross-view%2520alignment%2520without%2520paired%2520supervision.%2520In%2520zero-shot%2520experiments%252C%2520Wrivinder%2520achieves%2520sub-30%255C%252Cm%2520geolocation%2520accuracy%2520across%2520both%2520dense%2520and%2520large-area%2520scenes%252C%2520highlighting%2520the%2520promise%2520of%2520geometry-based%2520aggregation%2520for%2520robust%2520ground-to-satellite%2520localization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wrivinder%3A%20Towards%20Spatial%20Intelligence%20for%20Geo-locating%20Ground%20Images%20onto%20Satellite%20Imagery&entry.906535625=Chandrakanth%20Gudavalli%20and%20Tajuddin%20Manhar%20Mohammed%20and%20Abhay%20Yadav%20and%20Ananth%20Vishnu%20Bhaskar%20and%20Hardik%20Prajapati%20and%20Cheng%20Peng%20and%20Rama%20Chellappa%20and%20Shivkumar%20Chandrasekaran%20and%20B.%20S.%20Manjunath&entry.1292438233=Aligning%20ground-level%20imagery%20with%20geo-registered%20satellite%20maps%20is%20crucial%20for%20mapping%2C%20navigation%2C%20and%20situational%20awareness%2C%20yet%20remains%20challenging%20under%20large%20viewpoint%20gaps%20or%20when%20GPS%20is%20unreliable.%20We%20introduce%20Wrivinder%2C%20a%20zero-shot%2C%20geometry-driven%20framework%20that%20aggregates%20multiple%20ground%20photographs%20to%20reconstruct%20a%20consistent%203D%20scene%20and%20align%20it%20with%20overhead%20satellite%20imagery.%20Wrivinder%20combines%20SfM%20reconstruction%2C%203D%20Gaussian%20Splatting%2C%20semantic%20grounding%2C%20and%20monocular%20depth--based%20metric%20cues%20to%20produce%20a%20stable%20zenith-view%20rendering%20that%20can%20be%20directly%20matched%20to%20satellite%20context%20for%20metrically%20accurate%20camera%20geo-localization.%20To%20support%20systematic%20evaluation%20of%20this%20task%2C%20which%20lacks%20suitable%20benchmarks%2C%20we%20also%20release%20MC-Sat%2C%20a%20curated%20dataset%20linking%20multi-view%20ground%20imagery%20with%20geo-registered%20satellite%20tiles%20across%20diverse%20outdoor%20environments.%20Together%2C%20Wrivinder%20and%20MC-Sat%20provide%20a%20first%20comprehensive%20baseline%20and%20testbed%20for%20studying%20geometry-centered%20cross-view%20alignment%20without%20paired%20supervision.%20In%20zero-shot%20experiments%2C%20Wrivinder%20achieves%20sub-30%5C%2Cm%20geolocation%20accuracy%20across%20both%20dense%20and%20large-area%20scenes%2C%20highlighting%20the%20promise%20of%20geometry-based%20aggregation%20for%20robust%20ground-to-satellite%20localization.&entry.1838667208=http%3A//arxiv.org/abs/2602.14929v1&entry.124074799=Read"},
{"title": "Challenges and Requirements for Benchmarking Time Series Foundation Models", "author": "Marcel Meyer and Sascha Kaltenpoth and Kevin Zalipski and Oliver M\u00fcller", "abstract": "Time Series Foundation Models (TSFMs) represent a new paradigm for time-series forecasting, promising zero-shot predictions without the need for task-specific training or fine-tuning. However, similar to Large Language Models (LLMs), the evaluation of TSFMs is challenging: as training corpora grow increasingly large, it becomes difficult to ensure the integrity of the test sets used for benchmarking. Our investigation of existing TSFM evaluation studies identifies two kinds of information leakage: (1) train-test sample overlaps arising from the multi-purpose reuse of datasets and (2) temporal overlap of correlated train and test series. Ignoring these forms of information leakage when benchmarking TSFMs risks producing overly optimistic performance estimates that fail to generalize to real-world settings. We therefore argue for the development of novel evaluation methodologies that avoid pitfalls already observed in both LLM and classical time-series benchmarking, and we call on the research community to adopt principled approaches to safeguard the integrity of TSFM evaluation.", "link": "http://arxiv.org/abs/2510.13654v2", "date": "2026-02-16", "relevancy": 2.2316, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.451}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.451}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenges%20and%20Requirements%20for%20Benchmarking%20Time%20Series%20Foundation%20Models&body=Title%3A%20Challenges%20and%20Requirements%20for%20Benchmarking%20Time%20Series%20Foundation%20Models%0AAuthor%3A%20Marcel%20Meyer%20and%20Sascha%20Kaltenpoth%20and%20Kevin%20Zalipski%20and%20Oliver%20M%C3%BCller%0AAbstract%3A%20Time%20Series%20Foundation%20Models%20%28TSFMs%29%20represent%20a%20new%20paradigm%20for%20time-series%20forecasting%2C%20promising%20zero-shot%20predictions%20without%20the%20need%20for%20task-specific%20training%20or%20fine-tuning.%20However%2C%20similar%20to%20Large%20Language%20Models%20%28LLMs%29%2C%20the%20evaluation%20of%20TSFMs%20is%20challenging%3A%20as%20training%20corpora%20grow%20increasingly%20large%2C%20it%20becomes%20difficult%20to%20ensure%20the%20integrity%20of%20the%20test%20sets%20used%20for%20benchmarking.%20Our%20investigation%20of%20existing%20TSFM%20evaluation%20studies%20identifies%20two%20kinds%20of%20information%20leakage%3A%20%281%29%20train-test%20sample%20overlaps%20arising%20from%20the%20multi-purpose%20reuse%20of%20datasets%20and%20%282%29%20temporal%20overlap%20of%20correlated%20train%20and%20test%20series.%20Ignoring%20these%20forms%20of%20information%20leakage%20when%20benchmarking%20TSFMs%20risks%20producing%20overly%20optimistic%20performance%20estimates%20that%20fail%20to%20generalize%20to%20real-world%20settings.%20We%20therefore%20argue%20for%20the%20development%20of%20novel%20evaluation%20methodologies%20that%20avoid%20pitfalls%20already%20observed%20in%20both%20LLM%20and%20classical%20time-series%20benchmarking%2C%20and%20we%20call%20on%20the%20research%20community%20to%20adopt%20principled%20approaches%20to%20safeguard%20the%20integrity%20of%20TSFM%20evaluation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenges%2520and%2520Requirements%2520for%2520Benchmarking%2520Time%2520Series%2520Foundation%2520Models%26entry.906535625%3DMarcel%2520Meyer%2520and%2520Sascha%2520Kaltenpoth%2520and%2520Kevin%2520Zalipski%2520and%2520Oliver%2520M%25C3%25BCller%26entry.1292438233%3DTime%2520Series%2520Foundation%2520Models%2520%2528TSFMs%2529%2520represent%2520a%2520new%2520paradigm%2520for%2520time-series%2520forecasting%252C%2520promising%2520zero-shot%2520predictions%2520without%2520the%2520need%2520for%2520task-specific%2520training%2520or%2520fine-tuning.%2520However%252C%2520similar%2520to%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520the%2520evaluation%2520of%2520TSFMs%2520is%2520challenging%253A%2520as%2520training%2520corpora%2520grow%2520increasingly%2520large%252C%2520it%2520becomes%2520difficult%2520to%2520ensure%2520the%2520integrity%2520of%2520the%2520test%2520sets%2520used%2520for%2520benchmarking.%2520Our%2520investigation%2520of%2520existing%2520TSFM%2520evaluation%2520studies%2520identifies%2520two%2520kinds%2520of%2520information%2520leakage%253A%2520%25281%2529%2520train-test%2520sample%2520overlaps%2520arising%2520from%2520the%2520multi-purpose%2520reuse%2520of%2520datasets%2520and%2520%25282%2529%2520temporal%2520overlap%2520of%2520correlated%2520train%2520and%2520test%2520series.%2520Ignoring%2520these%2520forms%2520of%2520information%2520leakage%2520when%2520benchmarking%2520TSFMs%2520risks%2520producing%2520overly%2520optimistic%2520performance%2520estimates%2520that%2520fail%2520to%2520generalize%2520to%2520real-world%2520settings.%2520We%2520therefore%2520argue%2520for%2520the%2520development%2520of%2520novel%2520evaluation%2520methodologies%2520that%2520avoid%2520pitfalls%2520already%2520observed%2520in%2520both%2520LLM%2520and%2520classical%2520time-series%2520benchmarking%252C%2520and%2520we%2520call%2520on%2520the%2520research%2520community%2520to%2520adopt%2520principled%2520approaches%2520to%2520safeguard%2520the%2520integrity%2520of%2520TSFM%2520evaluation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenges%20and%20Requirements%20for%20Benchmarking%20Time%20Series%20Foundation%20Models&entry.906535625=Marcel%20Meyer%20and%20Sascha%20Kaltenpoth%20and%20Kevin%20Zalipski%20and%20Oliver%20M%C3%BCller&entry.1292438233=Time%20Series%20Foundation%20Models%20%28TSFMs%29%20represent%20a%20new%20paradigm%20for%20time-series%20forecasting%2C%20promising%20zero-shot%20predictions%20without%20the%20need%20for%20task-specific%20training%20or%20fine-tuning.%20However%2C%20similar%20to%20Large%20Language%20Models%20%28LLMs%29%2C%20the%20evaluation%20of%20TSFMs%20is%20challenging%3A%20as%20training%20corpora%20grow%20increasingly%20large%2C%20it%20becomes%20difficult%20to%20ensure%20the%20integrity%20of%20the%20test%20sets%20used%20for%20benchmarking.%20Our%20investigation%20of%20existing%20TSFM%20evaluation%20studies%20identifies%20two%20kinds%20of%20information%20leakage%3A%20%281%29%20train-test%20sample%20overlaps%20arising%20from%20the%20multi-purpose%20reuse%20of%20datasets%20and%20%282%29%20temporal%20overlap%20of%20correlated%20train%20and%20test%20series.%20Ignoring%20these%20forms%20of%20information%20leakage%20when%20benchmarking%20TSFMs%20risks%20producing%20overly%20optimistic%20performance%20estimates%20that%20fail%20to%20generalize%20to%20real-world%20settings.%20We%20therefore%20argue%20for%20the%20development%20of%20novel%20evaluation%20methodologies%20that%20avoid%20pitfalls%20already%20observed%20in%20both%20LLM%20and%20classical%20time-series%20benchmarking%2C%20and%20we%20call%20on%20the%20research%20community%20to%20adopt%20principled%20approaches%20to%20safeguard%20the%20integrity%20of%20TSFM%20evaluation.&entry.1838667208=http%3A//arxiv.org/abs/2510.13654v2&entry.124074799=Read"},
{"title": "Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks", "author": "Lukas Struppek and Adam Gleave and Kellin Pelrine", "abstract": "As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses, open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that prefill attacks are consistently effective against all major contemporary open-weight models, revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling, they remain vulnerable to tailored, model-specific strategies. Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.", "link": "http://arxiv.org/abs/2602.14689v1", "date": "2026-02-16", "relevancy": 2.2314, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4551}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exposing%20the%20Systematic%20Vulnerability%20of%20Open-Weight%20Models%20to%20Prefill%20Attacks&body=Title%3A%20Exposing%20the%20Systematic%20Vulnerability%20of%20Open-Weight%20Models%20to%20Prefill%20Attacks%0AAuthor%3A%20Lukas%20Struppek%20and%20Adam%20Gleave%20and%20Kellin%20Pelrine%0AAbstract%3A%20As%20the%20capabilities%20of%20large%20language%20models%20continue%20to%20advance%2C%20so%20does%20their%20potential%20for%20misuse.%20While%20closed-source%20models%20typically%20rely%20on%20external%20defenses%2C%20open-weight%20models%20must%20primarily%20depend%20on%20internal%20safeguards%20to%20mitigate%20harmful%20behavior.%20Prior%20red-teaming%20research%20has%20largely%20focused%20on%20input-based%20jailbreaking%20and%20parameter-level%20manipulations.%20However%2C%20open-weight%20models%20also%20natively%20support%20prefilling%2C%20which%20allows%20an%20attacker%20to%20predefine%20initial%20response%20tokens%20before%20generation%20begins.%20Despite%20its%20potential%2C%20this%20attack%20vector%20has%20received%20little%20systematic%20attention.%20We%20present%20the%20largest%20empirical%20study%20to%20date%20of%20prefill%20attacks%2C%20evaluating%20over%2020%20existing%20and%20novel%20strategies%20across%20multiple%20model%20families%20and%20state-of-the-art%20open-weight%20models.%20Our%20results%20show%20that%20prefill%20attacks%20are%20consistently%20effective%20against%20all%20major%20contemporary%20open-weight%20models%2C%20revealing%20a%20critical%20and%20previously%20underexplored%20vulnerability%20with%20significant%20implications%20for%20deployment.%20While%20certain%20large%20reasoning%20models%20exhibit%20some%20robustness%20against%20generic%20prefilling%2C%20they%20remain%20vulnerable%20to%20tailored%2C%20model-specific%20strategies.%20Our%20findings%20underscore%20the%20urgent%20need%20for%20model%20developers%20to%20prioritize%20defenses%20against%20prefill%20attacks%20in%20open-weight%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExposing%2520the%2520Systematic%2520Vulnerability%2520of%2520Open-Weight%2520Models%2520to%2520Prefill%2520Attacks%26entry.906535625%3DLukas%2520Struppek%2520and%2520Adam%2520Gleave%2520and%2520Kellin%2520Pelrine%26entry.1292438233%3DAs%2520the%2520capabilities%2520of%2520large%2520language%2520models%2520continue%2520to%2520advance%252C%2520so%2520does%2520their%2520potential%2520for%2520misuse.%2520While%2520closed-source%2520models%2520typically%2520rely%2520on%2520external%2520defenses%252C%2520open-weight%2520models%2520must%2520primarily%2520depend%2520on%2520internal%2520safeguards%2520to%2520mitigate%2520harmful%2520behavior.%2520Prior%2520red-teaming%2520research%2520has%2520largely%2520focused%2520on%2520input-based%2520jailbreaking%2520and%2520parameter-level%2520manipulations.%2520However%252C%2520open-weight%2520models%2520also%2520natively%2520support%2520prefilling%252C%2520which%2520allows%2520an%2520attacker%2520to%2520predefine%2520initial%2520response%2520tokens%2520before%2520generation%2520begins.%2520Despite%2520its%2520potential%252C%2520this%2520attack%2520vector%2520has%2520received%2520little%2520systematic%2520attention.%2520We%2520present%2520the%2520largest%2520empirical%2520study%2520to%2520date%2520of%2520prefill%2520attacks%252C%2520evaluating%2520over%252020%2520existing%2520and%2520novel%2520strategies%2520across%2520multiple%2520model%2520families%2520and%2520state-of-the-art%2520open-weight%2520models.%2520Our%2520results%2520show%2520that%2520prefill%2520attacks%2520are%2520consistently%2520effective%2520against%2520all%2520major%2520contemporary%2520open-weight%2520models%252C%2520revealing%2520a%2520critical%2520and%2520previously%2520underexplored%2520vulnerability%2520with%2520significant%2520implications%2520for%2520deployment.%2520While%2520certain%2520large%2520reasoning%2520models%2520exhibit%2520some%2520robustness%2520against%2520generic%2520prefilling%252C%2520they%2520remain%2520vulnerable%2520to%2520tailored%252C%2520model-specific%2520strategies.%2520Our%2520findings%2520underscore%2520the%2520urgent%2520need%2520for%2520model%2520developers%2520to%2520prioritize%2520defenses%2520against%2520prefill%2520attacks%2520in%2520open-weight%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exposing%20the%20Systematic%20Vulnerability%20of%20Open-Weight%20Models%20to%20Prefill%20Attacks&entry.906535625=Lukas%20Struppek%20and%20Adam%20Gleave%20and%20Kellin%20Pelrine&entry.1292438233=As%20the%20capabilities%20of%20large%20language%20models%20continue%20to%20advance%2C%20so%20does%20their%20potential%20for%20misuse.%20While%20closed-source%20models%20typically%20rely%20on%20external%20defenses%2C%20open-weight%20models%20must%20primarily%20depend%20on%20internal%20safeguards%20to%20mitigate%20harmful%20behavior.%20Prior%20red-teaming%20research%20has%20largely%20focused%20on%20input-based%20jailbreaking%20and%20parameter-level%20manipulations.%20However%2C%20open-weight%20models%20also%20natively%20support%20prefilling%2C%20which%20allows%20an%20attacker%20to%20predefine%20initial%20response%20tokens%20before%20generation%20begins.%20Despite%20its%20potential%2C%20this%20attack%20vector%20has%20received%20little%20systematic%20attention.%20We%20present%20the%20largest%20empirical%20study%20to%20date%20of%20prefill%20attacks%2C%20evaluating%20over%2020%20existing%20and%20novel%20strategies%20across%20multiple%20model%20families%20and%20state-of-the-art%20open-weight%20models.%20Our%20results%20show%20that%20prefill%20attacks%20are%20consistently%20effective%20against%20all%20major%20contemporary%20open-weight%20models%2C%20revealing%20a%20critical%20and%20previously%20underexplored%20vulnerability%20with%20significant%20implications%20for%20deployment.%20While%20certain%20large%20reasoning%20models%20exhibit%20some%20robustness%20against%20generic%20prefilling%2C%20they%20remain%20vulnerable%20to%20tailored%2C%20model-specific%20strategies.%20Our%20findings%20underscore%20the%20urgent%20need%20for%20model%20developers%20to%20prioritize%20defenses%20against%20prefill%20attacks%20in%20open-weight%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2602.14689v1&entry.124074799=Read"},
{"title": "Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems", "author": "Pramit Saha and Joshua Strong and Mohammad Alsharid and Divyanshu Mishra and J. Alison Noble", "abstract": "Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single \"best\" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.", "link": "http://arxiv.org/abs/2602.14901v1", "date": "2026-02-16", "relevancy": 2.2295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4467}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4467}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Picking%20the%20Right%20Specialist%3A%20Attentive%20Neural%20Process-based%20Selection%20of%20Task-Specialized%20Models%20as%20Tools%20for%20Agentic%20Healthcare%20Systems&body=Title%3A%20Picking%20the%20Right%20Specialist%3A%20Attentive%20Neural%20Process-based%20Selection%20of%20Task-Specialized%20Models%20as%20Tools%20for%20Agentic%20Healthcare%20Systems%0AAuthor%3A%20Pramit%20Saha%20and%20Joshua%20Strong%20and%20Mohammad%20Alsharid%20and%20Divyanshu%20Mishra%20and%20J.%20Alison%20Noble%0AAbstract%3A%20Task-specialized%20models%20form%20the%20backbone%20of%20agentic%20healthcare%20systems%2C%20enabling%20the%20agents%20to%20answer%20clinical%20queries%20across%20tasks%20such%20as%20disease%20diagnosis%2C%20localization%2C%20and%20report%20generation.%20Yet%2C%20for%20a%20given%20task%2C%20a%20single%20%22best%22%20model%20rarely%20exists.%20In%20practice%2C%20each%20task%20is%20better%20served%20by%20multiple%20competing%20specialist%20models%20where%20different%20models%20excel%20on%20different%20data%20samples.%20As%20a%20result%2C%20for%20any%20given%20query%2C%20agents%20must%20reliably%20select%20the%20right%20specialist%20model%20from%20a%20heterogeneous%20pool%20of%20tool%20candidates.%20To%20this%20end%2C%20we%20introduce%20ToolSelect%2C%20which%20adaptively%20learns%20model%20selection%20for%20tools%20by%20minimizing%20a%20population%20risk%20over%20sampled%20specialist%20tool%20candidates%20using%20a%20consistent%20surrogate%20of%20the%20task-conditional%20selection%20loss.%20Concretely%2C%20we%20propose%20an%20Attentive%20Neural%20Process-based%20selector%20conditioned%20on%20the%20query%20and%20per-model%20behavioral%20summaries%20to%20choose%20among%20the%20specialist%20models.%20Motivated%20by%20the%20absence%20of%20any%20established%20testbed%2C%20we%2C%20for%20the%20first%20time%2C%20introduce%20an%20agentic%20Chest%20X-ray%20environment%20equipped%20with%20a%20diverse%20suite%20of%20task-specialized%20models%20%2817%20disease%20detection%2C%2019%20report%20generation%2C%206%20visual%20grounding%2C%20and%2013%20VQA%29%20and%20develop%20ToolSelectBench%2C%20a%20benchmark%20of%201448%20queries.%20Our%20results%20demonstrate%20that%20ToolSelect%20consistently%20outperforms%2010%20SOTA%20methods%20across%20four%20different%20task%20families.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPicking%2520the%2520Right%2520Specialist%253A%2520Attentive%2520Neural%2520Process-based%2520Selection%2520of%2520Task-Specialized%2520Models%2520as%2520Tools%2520for%2520Agentic%2520Healthcare%2520Systems%26entry.906535625%3DPramit%2520Saha%2520and%2520Joshua%2520Strong%2520and%2520Mohammad%2520Alsharid%2520and%2520Divyanshu%2520Mishra%2520and%2520J.%2520Alison%2520Noble%26entry.1292438233%3DTask-specialized%2520models%2520form%2520the%2520backbone%2520of%2520agentic%2520healthcare%2520systems%252C%2520enabling%2520the%2520agents%2520to%2520answer%2520clinical%2520queries%2520across%2520tasks%2520such%2520as%2520disease%2520diagnosis%252C%2520localization%252C%2520and%2520report%2520generation.%2520Yet%252C%2520for%2520a%2520given%2520task%252C%2520a%2520single%2520%2522best%2522%2520model%2520rarely%2520exists.%2520In%2520practice%252C%2520each%2520task%2520is%2520better%2520served%2520by%2520multiple%2520competing%2520specialist%2520models%2520where%2520different%2520models%2520excel%2520on%2520different%2520data%2520samples.%2520As%2520a%2520result%252C%2520for%2520any%2520given%2520query%252C%2520agents%2520must%2520reliably%2520select%2520the%2520right%2520specialist%2520model%2520from%2520a%2520heterogeneous%2520pool%2520of%2520tool%2520candidates.%2520To%2520this%2520end%252C%2520we%2520introduce%2520ToolSelect%252C%2520which%2520adaptively%2520learns%2520model%2520selection%2520for%2520tools%2520by%2520minimizing%2520a%2520population%2520risk%2520over%2520sampled%2520specialist%2520tool%2520candidates%2520using%2520a%2520consistent%2520surrogate%2520of%2520the%2520task-conditional%2520selection%2520loss.%2520Concretely%252C%2520we%2520propose%2520an%2520Attentive%2520Neural%2520Process-based%2520selector%2520conditioned%2520on%2520the%2520query%2520and%2520per-model%2520behavioral%2520summaries%2520to%2520choose%2520among%2520the%2520specialist%2520models.%2520Motivated%2520by%2520the%2520absence%2520of%2520any%2520established%2520testbed%252C%2520we%252C%2520for%2520the%2520first%2520time%252C%2520introduce%2520an%2520agentic%2520Chest%2520X-ray%2520environment%2520equipped%2520with%2520a%2520diverse%2520suite%2520of%2520task-specialized%2520models%2520%252817%2520disease%2520detection%252C%252019%2520report%2520generation%252C%25206%2520visual%2520grounding%252C%2520and%252013%2520VQA%2529%2520and%2520develop%2520ToolSelectBench%252C%2520a%2520benchmark%2520of%25201448%2520queries.%2520Our%2520results%2520demonstrate%2520that%2520ToolSelect%2520consistently%2520outperforms%252010%2520SOTA%2520methods%2520across%2520four%2520different%2520task%2520families.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Picking%20the%20Right%20Specialist%3A%20Attentive%20Neural%20Process-based%20Selection%20of%20Task-Specialized%20Models%20as%20Tools%20for%20Agentic%20Healthcare%20Systems&entry.906535625=Pramit%20Saha%20and%20Joshua%20Strong%20and%20Mohammad%20Alsharid%20and%20Divyanshu%20Mishra%20and%20J.%20Alison%20Noble&entry.1292438233=Task-specialized%20models%20form%20the%20backbone%20of%20agentic%20healthcare%20systems%2C%20enabling%20the%20agents%20to%20answer%20clinical%20queries%20across%20tasks%20such%20as%20disease%20diagnosis%2C%20localization%2C%20and%20report%20generation.%20Yet%2C%20for%20a%20given%20task%2C%20a%20single%20%22best%22%20model%20rarely%20exists.%20In%20practice%2C%20each%20task%20is%20better%20served%20by%20multiple%20competing%20specialist%20models%20where%20different%20models%20excel%20on%20different%20data%20samples.%20As%20a%20result%2C%20for%20any%20given%20query%2C%20agents%20must%20reliably%20select%20the%20right%20specialist%20model%20from%20a%20heterogeneous%20pool%20of%20tool%20candidates.%20To%20this%20end%2C%20we%20introduce%20ToolSelect%2C%20which%20adaptively%20learns%20model%20selection%20for%20tools%20by%20minimizing%20a%20population%20risk%20over%20sampled%20specialist%20tool%20candidates%20using%20a%20consistent%20surrogate%20of%20the%20task-conditional%20selection%20loss.%20Concretely%2C%20we%20propose%20an%20Attentive%20Neural%20Process-based%20selector%20conditioned%20on%20the%20query%20and%20per-model%20behavioral%20summaries%20to%20choose%20among%20the%20specialist%20models.%20Motivated%20by%20the%20absence%20of%20any%20established%20testbed%2C%20we%2C%20for%20the%20first%20time%2C%20introduce%20an%20agentic%20Chest%20X-ray%20environment%20equipped%20with%20a%20diverse%20suite%20of%20task-specialized%20models%20%2817%20disease%20detection%2C%2019%20report%20generation%2C%206%20visual%20grounding%2C%20and%2013%20VQA%29%20and%20develop%20ToolSelectBench%2C%20a%20benchmark%20of%201448%20queries.%20Our%20results%20demonstrate%20that%20ToolSelect%20consistently%20outperforms%2010%20SOTA%20methods%20across%20four%20different%20task%20families.&entry.1838667208=http%3A//arxiv.org/abs/2602.14901v1&entry.124074799=Read"},
{"title": "Universal Algorithm-Implicit Learning", "author": "Stefano Woerner and Seong Joon Oh and Christian F. Baumgartner", "abstract": "Current meta-learning methods are constrained to narrow task distributions with fixed feature and label spaces, limiting applicability. Moreover, the current meta-learning literature uses key terms like \"universal\" and \"general-purpose\" inconsistently and lacks precise definitions, hindering comparability. We introduce a theoretical framework for meta-learning which formally defines practical universality and introduces a distinction between algorithm-explicit and algorithm-implicit learning, providing a principled vocabulary for reasoning about universal meta-learning methods. Guided by this framework, we present TAIL, a transformer-based algorithm-implicit meta-learner that functions across tasks with varying domains, modalities, and label configurations. TAIL features three innovations over prior transformer-based meta-learners: random projections for cross-modal feature encoding, random injection label embeddings that extrapolate to larger label spaces, and efficient inline query processing. TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. Unlike other meta-learning methods, it also generalizes to unseen modalities, solving text classification tasks despite training exclusively on images, handles tasks with up to 20$\\times$ more classes than seen during training, and provides orders-of-magnitude computational savings over prior transformer-based approaches.", "link": "http://arxiv.org/abs/2602.14761v1", "date": "2026-02-16", "relevancy": 2.229, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5748}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5452}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Algorithm-Implicit%20Learning&body=Title%3A%20Universal%20Algorithm-Implicit%20Learning%0AAuthor%3A%20Stefano%20Woerner%20and%20Seong%20Joon%20Oh%20and%20Christian%20F.%20Baumgartner%0AAbstract%3A%20Current%20meta-learning%20methods%20are%20constrained%20to%20narrow%20task%20distributions%20with%20fixed%20feature%20and%20label%20spaces%2C%20limiting%20applicability.%20Moreover%2C%20the%20current%20meta-learning%20literature%20uses%20key%20terms%20like%20%22universal%22%20and%20%22general-purpose%22%20inconsistently%20and%20lacks%20precise%20definitions%2C%20hindering%20comparability.%20We%20introduce%20a%20theoretical%20framework%20for%20meta-learning%20which%20formally%20defines%20practical%20universality%20and%20introduces%20a%20distinction%20between%20algorithm-explicit%20and%20algorithm-implicit%20learning%2C%20providing%20a%20principled%20vocabulary%20for%20reasoning%20about%20universal%20meta-learning%20methods.%20Guided%20by%20this%20framework%2C%20we%20present%20TAIL%2C%20a%20transformer-based%20algorithm-implicit%20meta-learner%20that%20functions%20across%20tasks%20with%20varying%20domains%2C%20modalities%2C%20and%20label%20configurations.%20TAIL%20features%20three%20innovations%20over%20prior%20transformer-based%20meta-learners%3A%20random%20projections%20for%20cross-modal%20feature%20encoding%2C%20random%20injection%20label%20embeddings%20that%20extrapolate%20to%20larger%20label%20spaces%2C%20and%20efficient%20inline%20query%20processing.%20TAIL%20achieves%20state-of-the-art%20performance%20on%20standard%20few-shot%20benchmarks%20while%20generalizing%20to%20unseen%20domains.%20Unlike%20other%20meta-learning%20methods%2C%20it%20also%20generalizes%20to%20unseen%20modalities%2C%20solving%20text%20classification%20tasks%20despite%20training%20exclusively%20on%20images%2C%20handles%20tasks%20with%20up%20to%2020%24%5Ctimes%24%20more%20classes%20than%20seen%20during%20training%2C%20and%20provides%20orders-of-magnitude%20computational%20savings%20over%20prior%20transformer-based%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Algorithm-Implicit%2520Learning%26entry.906535625%3DStefano%2520Woerner%2520and%2520Seong%2520Joon%2520Oh%2520and%2520Christian%2520F.%2520Baumgartner%26entry.1292438233%3DCurrent%2520meta-learning%2520methods%2520are%2520constrained%2520to%2520narrow%2520task%2520distributions%2520with%2520fixed%2520feature%2520and%2520label%2520spaces%252C%2520limiting%2520applicability.%2520Moreover%252C%2520the%2520current%2520meta-learning%2520literature%2520uses%2520key%2520terms%2520like%2520%2522universal%2522%2520and%2520%2522general-purpose%2522%2520inconsistently%2520and%2520lacks%2520precise%2520definitions%252C%2520hindering%2520comparability.%2520We%2520introduce%2520a%2520theoretical%2520framework%2520for%2520meta-learning%2520which%2520formally%2520defines%2520practical%2520universality%2520and%2520introduces%2520a%2520distinction%2520between%2520algorithm-explicit%2520and%2520algorithm-implicit%2520learning%252C%2520providing%2520a%2520principled%2520vocabulary%2520for%2520reasoning%2520about%2520universal%2520meta-learning%2520methods.%2520Guided%2520by%2520this%2520framework%252C%2520we%2520present%2520TAIL%252C%2520a%2520transformer-based%2520algorithm-implicit%2520meta-learner%2520that%2520functions%2520across%2520tasks%2520with%2520varying%2520domains%252C%2520modalities%252C%2520and%2520label%2520configurations.%2520TAIL%2520features%2520three%2520innovations%2520over%2520prior%2520transformer-based%2520meta-learners%253A%2520random%2520projections%2520for%2520cross-modal%2520feature%2520encoding%252C%2520random%2520injection%2520label%2520embeddings%2520that%2520extrapolate%2520to%2520larger%2520label%2520spaces%252C%2520and%2520efficient%2520inline%2520query%2520processing.%2520TAIL%2520achieves%2520state-of-the-art%2520performance%2520on%2520standard%2520few-shot%2520benchmarks%2520while%2520generalizing%2520to%2520unseen%2520domains.%2520Unlike%2520other%2520meta-learning%2520methods%252C%2520it%2520also%2520generalizes%2520to%2520unseen%2520modalities%252C%2520solving%2520text%2520classification%2520tasks%2520despite%2520training%2520exclusively%2520on%2520images%252C%2520handles%2520tasks%2520with%2520up%2520to%252020%2524%255Ctimes%2524%2520more%2520classes%2520than%2520seen%2520during%2520training%252C%2520and%2520provides%2520orders-of-magnitude%2520computational%2520savings%2520over%2520prior%2520transformer-based%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Algorithm-Implicit%20Learning&entry.906535625=Stefano%20Woerner%20and%20Seong%20Joon%20Oh%20and%20Christian%20F.%20Baumgartner&entry.1292438233=Current%20meta-learning%20methods%20are%20constrained%20to%20narrow%20task%20distributions%20with%20fixed%20feature%20and%20label%20spaces%2C%20limiting%20applicability.%20Moreover%2C%20the%20current%20meta-learning%20literature%20uses%20key%20terms%20like%20%22universal%22%20and%20%22general-purpose%22%20inconsistently%20and%20lacks%20precise%20definitions%2C%20hindering%20comparability.%20We%20introduce%20a%20theoretical%20framework%20for%20meta-learning%20which%20formally%20defines%20practical%20universality%20and%20introduces%20a%20distinction%20between%20algorithm-explicit%20and%20algorithm-implicit%20learning%2C%20providing%20a%20principled%20vocabulary%20for%20reasoning%20about%20universal%20meta-learning%20methods.%20Guided%20by%20this%20framework%2C%20we%20present%20TAIL%2C%20a%20transformer-based%20algorithm-implicit%20meta-learner%20that%20functions%20across%20tasks%20with%20varying%20domains%2C%20modalities%2C%20and%20label%20configurations.%20TAIL%20features%20three%20innovations%20over%20prior%20transformer-based%20meta-learners%3A%20random%20projections%20for%20cross-modal%20feature%20encoding%2C%20random%20injection%20label%20embeddings%20that%20extrapolate%20to%20larger%20label%20spaces%2C%20and%20efficient%20inline%20query%20processing.%20TAIL%20achieves%20state-of-the-art%20performance%20on%20standard%20few-shot%20benchmarks%20while%20generalizing%20to%20unseen%20domains.%20Unlike%20other%20meta-learning%20methods%2C%20it%20also%20generalizes%20to%20unseen%20modalities%2C%20solving%20text%20classification%20tasks%20despite%20training%20exclusively%20on%20images%2C%20handles%20tasks%20with%20up%20to%2020%24%5Ctimes%24%20more%20classes%20than%20seen%20during%20training%2C%20and%20provides%20orders-of-magnitude%20computational%20savings%20over%20prior%20transformer-based%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2602.14761v1&entry.124074799=Read"},
{"title": "A representational framework for learning and encoding structurally enriched trajectories in complex agent environments", "author": "Corina Catarau-Cotutiu and Esther Mondragon and Eduardo Alonso", "abstract": "The ability of artificial intelligence agents to make optimal decisions and generalise them to different domains and tasks is compromised in complex scenarios. One way to address this issue has focused on learning efficient representations of the world and on how the actions of agents affect them in state-action transitions. Whereas such representations are procedurally efficient, they lack structural richness. To address this problem, we propose to enhance the agent's ontology and extend the traditional conceptualisation of trajectories to provide a more nuanced view of task execution. Structurally Enriched Trajectories (SETs) extend the encoding of sequences of states and their transitions by incorporating hierarchical relations between objects, interactions, and affordances. SETs are built as multi-level graphs, providing a detailed representation of the agent dynamics and a transferable functional abstraction of the task. SETs are integrated into an architecture, Structurally Enriched Trajectory Learning and Encoding (SETLE), that employs a heterogeneous graph-based memory structure of multi-level relational dependencies essential for generalisation. We demonstrate that SETLE can support downstream tasks, enabling agents to recognise task relevant structural patterns across CREATE and MiniGrid environments. Finally, we integrate SETLE with reinforcement learning and show measurable improvements in downstream performance, including breakthrough success rates in complex, sparse-reward tasks.", "link": "http://arxiv.org/abs/2503.13194v3", "date": "2026-02-16", "relevancy": 2.2137, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5838}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20representational%20framework%20for%20learning%20and%20encoding%20structurally%20enriched%20trajectories%20in%20complex%20agent%20environments&body=Title%3A%20A%20representational%20framework%20for%20learning%20and%20encoding%20structurally%20enriched%20trajectories%20in%20complex%20agent%20environments%0AAuthor%3A%20Corina%20Catarau-Cotutiu%20and%20Esther%20Mondragon%20and%20Eduardo%20Alonso%0AAbstract%3A%20The%20ability%20of%20artificial%20intelligence%20agents%20to%20make%20optimal%20decisions%20and%20generalise%20them%20to%20different%20domains%20and%20tasks%20is%20compromised%20in%20complex%20scenarios.%20One%20way%20to%20address%20this%20issue%20has%20focused%20on%20learning%20efficient%20representations%20of%20the%20world%20and%20on%20how%20the%20actions%20of%20agents%20affect%20them%20in%20state-action%20transitions.%20Whereas%20such%20representations%20are%20procedurally%20efficient%2C%20they%20lack%20structural%20richness.%20To%20address%20this%20problem%2C%20we%20propose%20to%20enhance%20the%20agent%27s%20ontology%20and%20extend%20the%20traditional%20conceptualisation%20of%20trajectories%20to%20provide%20a%20more%20nuanced%20view%20of%20task%20execution.%20Structurally%20Enriched%20Trajectories%20%28SETs%29%20extend%20the%20encoding%20of%20sequences%20of%20states%20and%20their%20transitions%20by%20incorporating%20hierarchical%20relations%20between%20objects%2C%20interactions%2C%20and%20affordances.%20SETs%20are%20built%20as%20multi-level%20graphs%2C%20providing%20a%20detailed%20representation%20of%20the%20agent%20dynamics%20and%20a%20transferable%20functional%20abstraction%20of%20the%20task.%20SETs%20are%20integrated%20into%20an%20architecture%2C%20Structurally%20Enriched%20Trajectory%20Learning%20and%20Encoding%20%28SETLE%29%2C%20that%20employs%20a%20heterogeneous%20graph-based%20memory%20structure%20of%20multi-level%20relational%20dependencies%20essential%20for%20generalisation.%20We%20demonstrate%20that%20SETLE%20can%20support%20downstream%20tasks%2C%20enabling%20agents%20to%20recognise%20task%20relevant%20structural%20patterns%20across%20CREATE%20and%20MiniGrid%20environments.%20Finally%2C%20we%20integrate%20SETLE%20with%20reinforcement%20learning%20and%20show%20measurable%20improvements%20in%20downstream%20performance%2C%20including%20breakthrough%20success%20rates%20in%20complex%2C%20sparse-reward%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2503.13194v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520representational%2520framework%2520for%2520learning%2520and%2520encoding%2520structurally%2520enriched%2520trajectories%2520in%2520complex%2520agent%2520environments%26entry.906535625%3DCorina%2520Catarau-Cotutiu%2520and%2520Esther%2520Mondragon%2520and%2520Eduardo%2520Alonso%26entry.1292438233%3DThe%2520ability%2520of%2520artificial%2520intelligence%2520agents%2520to%2520make%2520optimal%2520decisions%2520and%2520generalise%2520them%2520to%2520different%2520domains%2520and%2520tasks%2520is%2520compromised%2520in%2520complex%2520scenarios.%2520One%2520way%2520to%2520address%2520this%2520issue%2520has%2520focused%2520on%2520learning%2520efficient%2520representations%2520of%2520the%2520world%2520and%2520on%2520how%2520the%2520actions%2520of%2520agents%2520affect%2520them%2520in%2520state-action%2520transitions.%2520Whereas%2520such%2520representations%2520are%2520procedurally%2520efficient%252C%2520they%2520lack%2520structural%2520richness.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520to%2520enhance%2520the%2520agent%2527s%2520ontology%2520and%2520extend%2520the%2520traditional%2520conceptualisation%2520of%2520trajectories%2520to%2520provide%2520a%2520more%2520nuanced%2520view%2520of%2520task%2520execution.%2520Structurally%2520Enriched%2520Trajectories%2520%2528SETs%2529%2520extend%2520the%2520encoding%2520of%2520sequences%2520of%2520states%2520and%2520their%2520transitions%2520by%2520incorporating%2520hierarchical%2520relations%2520between%2520objects%252C%2520interactions%252C%2520and%2520affordances.%2520SETs%2520are%2520built%2520as%2520multi-level%2520graphs%252C%2520providing%2520a%2520detailed%2520representation%2520of%2520the%2520agent%2520dynamics%2520and%2520a%2520transferable%2520functional%2520abstraction%2520of%2520the%2520task.%2520SETs%2520are%2520integrated%2520into%2520an%2520architecture%252C%2520Structurally%2520Enriched%2520Trajectory%2520Learning%2520and%2520Encoding%2520%2528SETLE%2529%252C%2520that%2520employs%2520a%2520heterogeneous%2520graph-based%2520memory%2520structure%2520of%2520multi-level%2520relational%2520dependencies%2520essential%2520for%2520generalisation.%2520We%2520demonstrate%2520that%2520SETLE%2520can%2520support%2520downstream%2520tasks%252C%2520enabling%2520agents%2520to%2520recognise%2520task%2520relevant%2520structural%2520patterns%2520across%2520CREATE%2520and%2520MiniGrid%2520environments.%2520Finally%252C%2520we%2520integrate%2520SETLE%2520with%2520reinforcement%2520learning%2520and%2520show%2520measurable%2520improvements%2520in%2520downstream%2520performance%252C%2520including%2520breakthrough%2520success%2520rates%2520in%2520complex%252C%2520sparse-reward%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.13194v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20representational%20framework%20for%20learning%20and%20encoding%20structurally%20enriched%20trajectories%20in%20complex%20agent%20environments&entry.906535625=Corina%20Catarau-Cotutiu%20and%20Esther%20Mondragon%20and%20Eduardo%20Alonso&entry.1292438233=The%20ability%20of%20artificial%20intelligence%20agents%20to%20make%20optimal%20decisions%20and%20generalise%20them%20to%20different%20domains%20and%20tasks%20is%20compromised%20in%20complex%20scenarios.%20One%20way%20to%20address%20this%20issue%20has%20focused%20on%20learning%20efficient%20representations%20of%20the%20world%20and%20on%20how%20the%20actions%20of%20agents%20affect%20them%20in%20state-action%20transitions.%20Whereas%20such%20representations%20are%20procedurally%20efficient%2C%20they%20lack%20structural%20richness.%20To%20address%20this%20problem%2C%20we%20propose%20to%20enhance%20the%20agent%27s%20ontology%20and%20extend%20the%20traditional%20conceptualisation%20of%20trajectories%20to%20provide%20a%20more%20nuanced%20view%20of%20task%20execution.%20Structurally%20Enriched%20Trajectories%20%28SETs%29%20extend%20the%20encoding%20of%20sequences%20of%20states%20and%20their%20transitions%20by%20incorporating%20hierarchical%20relations%20between%20objects%2C%20interactions%2C%20and%20affordances.%20SETs%20are%20built%20as%20multi-level%20graphs%2C%20providing%20a%20detailed%20representation%20of%20the%20agent%20dynamics%20and%20a%20transferable%20functional%20abstraction%20of%20the%20task.%20SETs%20are%20integrated%20into%20an%20architecture%2C%20Structurally%20Enriched%20Trajectory%20Learning%20and%20Encoding%20%28SETLE%29%2C%20that%20employs%20a%20heterogeneous%20graph-based%20memory%20structure%20of%20multi-level%20relational%20dependencies%20essential%20for%20generalisation.%20We%20demonstrate%20that%20SETLE%20can%20support%20downstream%20tasks%2C%20enabling%20agents%20to%20recognise%20task%20relevant%20structural%20patterns%20across%20CREATE%20and%20MiniGrid%20environments.%20Finally%2C%20we%20integrate%20SETLE%20with%20reinforcement%20learning%20and%20show%20measurable%20improvements%20in%20downstream%20performance%2C%20including%20breakthrough%20success%20rates%20in%20complex%2C%20sparse-reward%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2503.13194v3&entry.124074799=Read"},
{"title": "Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation", "author": "Hugo Carlesso and Josiane Mothe and Radu Tudor Ionescu", "abstract": "Hyperspectral imaging (HSI) captures detailed spectral signatures across hundreds of contiguous bands per pixel, being indispensable for remote sensing applications such as land-cover classification, change detection, and environmental monitoring. Due to the high dimensionality of HSI data and the slow rate of data transfer in satellite-based systems, compact and efficient models are required to support onboard processing and minimize the transmission of redundant or low-value data. To this end, we introduce a novel curriculum multi-task self-supervised learning (CMTSSL) framework designed for lightweight architectures for HSI analysis. CMTSSL integrates masked image modeling with decoupled spatial and spectral jigsaw puzzle solving, guided by a curriculum learning strategy that progressively increases data difficulty during self-supervision. This enables the encoder to jointly capture fine-grained spectral continuity, spatial structure, and global semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously addresses spatial and spectral reasoning within a unified and computationally efficient design, being particularly suitable for training lightweight models for onboard satellite deployment. We validate our approach on four public benchmark datasets, demonstrating consistent gains in downstream segmentation tasks, using architectures that are over 16,000x lighter than some state-of-the-art models. These results highlight the potential of CMTSSL in generalizable representation learning with lightweight architectures for real-world HSI applications. Our code is publicly available at https://github.com/hugocarlesso/CMTSSL.", "link": "http://arxiv.org/abs/2509.13229v2", "date": "2026-02-16", "relevancy": 2.2131, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5642}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curriculum%20Multi-Task%20Self-Supervision%20Improves%20Lightweight%20Architectures%20for%20Onboard%20Satellite%20Hyperspectral%20Image%20Segmentation&body=Title%3A%20Curriculum%20Multi-Task%20Self-Supervision%20Improves%20Lightweight%20Architectures%20for%20Onboard%20Satellite%20Hyperspectral%20Image%20Segmentation%0AAuthor%3A%20Hugo%20Carlesso%20and%20Josiane%20Mothe%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20Hyperspectral%20imaging%20%28HSI%29%20captures%20detailed%20spectral%20signatures%20across%20hundreds%20of%20contiguous%20bands%20per%20pixel%2C%20being%20indispensable%20for%20remote%20sensing%20applications%20such%20as%20land-cover%20classification%2C%20change%20detection%2C%20and%20environmental%20monitoring.%20Due%20to%20the%20high%20dimensionality%20of%20HSI%20data%20and%20the%20slow%20rate%20of%20data%20transfer%20in%20satellite-based%20systems%2C%20compact%20and%20efficient%20models%20are%20required%20to%20support%20onboard%20processing%20and%20minimize%20the%20transmission%20of%20redundant%20or%20low-value%20data.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20curriculum%20multi-task%20self-supervised%20learning%20%28CMTSSL%29%20framework%20designed%20for%20lightweight%20architectures%20for%20HSI%20analysis.%20CMTSSL%20integrates%20masked%20image%20modeling%20with%20decoupled%20spatial%20and%20spectral%20jigsaw%20puzzle%20solving%2C%20guided%20by%20a%20curriculum%20learning%20strategy%20that%20progressively%20increases%20data%20difficulty%20during%20self-supervision.%20This%20enables%20the%20encoder%20to%20jointly%20capture%20fine-grained%20spectral%20continuity%2C%20spatial%20structure%2C%20and%20global%20semantic%20features.%20Unlike%20prior%20dual-task%20SSL%20methods%2C%20CMTSSL%20simultaneously%20addresses%20spatial%20and%20spectral%20reasoning%20within%20a%20unified%20and%20computationally%20efficient%20design%2C%20being%20particularly%20suitable%20for%20training%20lightweight%20models%20for%20onboard%20satellite%20deployment.%20We%20validate%20our%20approach%20on%20four%20public%20benchmark%20datasets%2C%20demonstrating%20consistent%20gains%20in%20downstream%20segmentation%20tasks%2C%20using%20architectures%20that%20are%20over%2016%2C000x%20lighter%20than%20some%20state-of-the-art%20models.%20These%20results%20highlight%20the%20potential%20of%20CMTSSL%20in%20generalizable%20representation%20learning%20with%20lightweight%20architectures%20for%20real-world%20HSI%20applications.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/hugocarlesso/CMTSSL.%0ALink%3A%20http%3A//arxiv.org/abs/2509.13229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurriculum%2520Multi-Task%2520Self-Supervision%2520Improves%2520Lightweight%2520Architectures%2520for%2520Onboard%2520Satellite%2520Hyperspectral%2520Image%2520Segmentation%26entry.906535625%3DHugo%2520Carlesso%2520and%2520Josiane%2520Mothe%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3DHyperspectral%2520imaging%2520%2528HSI%2529%2520captures%2520detailed%2520spectral%2520signatures%2520across%2520hundreds%2520of%2520contiguous%2520bands%2520per%2520pixel%252C%2520being%2520indispensable%2520for%2520remote%2520sensing%2520applications%2520such%2520as%2520land-cover%2520classification%252C%2520change%2520detection%252C%2520and%2520environmental%2520monitoring.%2520Due%2520to%2520the%2520high%2520dimensionality%2520of%2520HSI%2520data%2520and%2520the%2520slow%2520rate%2520of%2520data%2520transfer%2520in%2520satellite-based%2520systems%252C%2520compact%2520and%2520efficient%2520models%2520are%2520required%2520to%2520support%2520onboard%2520processing%2520and%2520minimize%2520the%2520transmission%2520of%2520redundant%2520or%2520low-value%2520data.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520novel%2520curriculum%2520multi-task%2520self-supervised%2520learning%2520%2528CMTSSL%2529%2520framework%2520designed%2520for%2520lightweight%2520architectures%2520for%2520HSI%2520analysis.%2520CMTSSL%2520integrates%2520masked%2520image%2520modeling%2520with%2520decoupled%2520spatial%2520and%2520spectral%2520jigsaw%2520puzzle%2520solving%252C%2520guided%2520by%2520a%2520curriculum%2520learning%2520strategy%2520that%2520progressively%2520increases%2520data%2520difficulty%2520during%2520self-supervision.%2520This%2520enables%2520the%2520encoder%2520to%2520jointly%2520capture%2520fine-grained%2520spectral%2520continuity%252C%2520spatial%2520structure%252C%2520and%2520global%2520semantic%2520features.%2520Unlike%2520prior%2520dual-task%2520SSL%2520methods%252C%2520CMTSSL%2520simultaneously%2520addresses%2520spatial%2520and%2520spectral%2520reasoning%2520within%2520a%2520unified%2520and%2520computationally%2520efficient%2520design%252C%2520being%2520particularly%2520suitable%2520for%2520training%2520lightweight%2520models%2520for%2520onboard%2520satellite%2520deployment.%2520We%2520validate%2520our%2520approach%2520on%2520four%2520public%2520benchmark%2520datasets%252C%2520demonstrating%2520consistent%2520gains%2520in%2520downstream%2520segmentation%2520tasks%252C%2520using%2520architectures%2520that%2520are%2520over%252016%252C000x%2520lighter%2520than%2520some%2520state-of-the-art%2520models.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520CMTSSL%2520in%2520generalizable%2520representation%2520learning%2520with%2520lightweight%2520architectures%2520for%2520real-world%2520HSI%2520applications.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/hugocarlesso/CMTSSL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curriculum%20Multi-Task%20Self-Supervision%20Improves%20Lightweight%20Architectures%20for%20Onboard%20Satellite%20Hyperspectral%20Image%20Segmentation&entry.906535625=Hugo%20Carlesso%20and%20Josiane%20Mothe%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=Hyperspectral%20imaging%20%28HSI%29%20captures%20detailed%20spectral%20signatures%20across%20hundreds%20of%20contiguous%20bands%20per%20pixel%2C%20being%20indispensable%20for%20remote%20sensing%20applications%20such%20as%20land-cover%20classification%2C%20change%20detection%2C%20and%20environmental%20monitoring.%20Due%20to%20the%20high%20dimensionality%20of%20HSI%20data%20and%20the%20slow%20rate%20of%20data%20transfer%20in%20satellite-based%20systems%2C%20compact%20and%20efficient%20models%20are%20required%20to%20support%20onboard%20processing%20and%20minimize%20the%20transmission%20of%20redundant%20or%20low-value%20data.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20curriculum%20multi-task%20self-supervised%20learning%20%28CMTSSL%29%20framework%20designed%20for%20lightweight%20architectures%20for%20HSI%20analysis.%20CMTSSL%20integrates%20masked%20image%20modeling%20with%20decoupled%20spatial%20and%20spectral%20jigsaw%20puzzle%20solving%2C%20guided%20by%20a%20curriculum%20learning%20strategy%20that%20progressively%20increases%20data%20difficulty%20during%20self-supervision.%20This%20enables%20the%20encoder%20to%20jointly%20capture%20fine-grained%20spectral%20continuity%2C%20spatial%20structure%2C%20and%20global%20semantic%20features.%20Unlike%20prior%20dual-task%20SSL%20methods%2C%20CMTSSL%20simultaneously%20addresses%20spatial%20and%20spectral%20reasoning%20within%20a%20unified%20and%20computationally%20efficient%20design%2C%20being%20particularly%20suitable%20for%20training%20lightweight%20models%20for%20onboard%20satellite%20deployment.%20We%20validate%20our%20approach%20on%20four%20public%20benchmark%20datasets%2C%20demonstrating%20consistent%20gains%20in%20downstream%20segmentation%20tasks%2C%20using%20architectures%20that%20are%20over%2016%2C000x%20lighter%20than%20some%20state-of-the-art%20models.%20These%20results%20highlight%20the%20potential%20of%20CMTSSL%20in%20generalizable%20representation%20learning%20with%20lightweight%20architectures%20for%20real-world%20HSI%20applications.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/hugocarlesso/CMTSSL.&entry.1838667208=http%3A//arxiv.org/abs/2509.13229v2&entry.124074799=Read"},
{"title": "Fast and accurate quasi-atom method for simultaneous atomistic and continuum simulation of solids", "author": "Artem Chuprov and Egor E. Nuzhin and Alexey A. Tsukanov and Nikolay V. Brilliantov", "abstract": "We report a novel hybrid method of simultaneous atomistic simulation of solids in critical regions (contacts surfaces, cracks areas, etc.), along with continuum modeling of other parts. The continuum is treated in terms of quasi-atoms of different size, comprising composite medium. The parameters of interaction potential between the quasi-atoms are optimized to match elastic properties of the composite medium to those of the atomic one. The optimization method coincides conceptually with the online Machine Learning (ML) methods, making it computationally very efficient. Such an approach allows a straightforward application of standard software packages for molecular dynamics (MD), supplemented by the ML-based optimizer. The new method is applied to model systems with a simple, pairwise Lennard-Jones potential, as well with multi-body Tersoff potential, describing covalent bonds. Using LAMMPS software we simulate collision of particles of different size. Comparing simulation results, obtained by the novel method, with full-atomic simulations, we demonstrate its accuracy, validity and overwhelming superiority in computational speed. Furthermore, we compare our method with other hybrid methods, specifically, with the closest one -- AtC (Atomic to Continuum) method. We demonstrate a significant superiority of our approach in computational speed and implementation convenience. Finally, we discuss a possible extension of the method for modeling other phenomena.", "link": "http://arxiv.org/abs/2602.14867v1", "date": "2026-02-16", "relevancy": 2.2013, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4445}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4445}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20and%20accurate%20quasi-atom%20method%20for%20simultaneous%20atomistic%20and%20continuum%20simulation%20of%20solids&body=Title%3A%20Fast%20and%20accurate%20quasi-atom%20method%20for%20simultaneous%20atomistic%20and%20continuum%20simulation%20of%20solids%0AAuthor%3A%20Artem%20Chuprov%20and%20Egor%20E.%20Nuzhin%20and%20Alexey%20A.%20Tsukanov%20and%20Nikolay%20V.%20Brilliantov%0AAbstract%3A%20We%20report%20a%20novel%20hybrid%20method%20of%20simultaneous%20atomistic%20simulation%20of%20solids%20in%20critical%20regions%20%28contacts%20surfaces%2C%20cracks%20areas%2C%20etc.%29%2C%20along%20with%20continuum%20modeling%20of%20other%20parts.%20The%20continuum%20is%20treated%20in%20terms%20of%20quasi-atoms%20of%20different%20size%2C%20comprising%20composite%20medium.%20The%20parameters%20of%20interaction%20potential%20between%20the%20quasi-atoms%20are%20optimized%20to%20match%20elastic%20properties%20of%20the%20composite%20medium%20to%20those%20of%20the%20atomic%20one.%20The%20optimization%20method%20coincides%20conceptually%20with%20the%20online%20Machine%20Learning%20%28ML%29%20methods%2C%20making%20it%20computationally%20very%20efficient.%20Such%20an%20approach%20allows%20a%20straightforward%20application%20of%20standard%20software%20packages%20for%20molecular%20dynamics%20%28MD%29%2C%20supplemented%20by%20the%20ML-based%20optimizer.%20The%20new%20method%20is%20applied%20to%20model%20systems%20with%20a%20simple%2C%20pairwise%20Lennard-Jones%20potential%2C%20as%20well%20with%20multi-body%20Tersoff%20potential%2C%20describing%20covalent%20bonds.%20Using%20LAMMPS%20software%20we%20simulate%20collision%20of%20particles%20of%20different%20size.%20Comparing%20simulation%20results%2C%20obtained%20by%20the%20novel%20method%2C%20with%20full-atomic%20simulations%2C%20we%20demonstrate%20its%20accuracy%2C%20validity%20and%20overwhelming%20superiority%20in%20computational%20speed.%20Furthermore%2C%20we%20compare%20our%20method%20with%20other%20hybrid%20methods%2C%20specifically%2C%20with%20the%20closest%20one%20--%20AtC%20%28Atomic%20to%20Continuum%29%20method.%20We%20demonstrate%20a%20significant%20superiority%20of%20our%20approach%20in%20computational%20speed%20and%20implementation%20convenience.%20Finally%2C%20we%20discuss%20a%20possible%20extension%20of%20the%20method%20for%20modeling%20other%20phenomena.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520and%2520accurate%2520quasi-atom%2520method%2520for%2520simultaneous%2520atomistic%2520and%2520continuum%2520simulation%2520of%2520solids%26entry.906535625%3DArtem%2520Chuprov%2520and%2520Egor%2520E.%2520Nuzhin%2520and%2520Alexey%2520A.%2520Tsukanov%2520and%2520Nikolay%2520V.%2520Brilliantov%26entry.1292438233%3DWe%2520report%2520a%2520novel%2520hybrid%2520method%2520of%2520simultaneous%2520atomistic%2520simulation%2520of%2520solids%2520in%2520critical%2520regions%2520%2528contacts%2520surfaces%252C%2520cracks%2520areas%252C%2520etc.%2529%252C%2520along%2520with%2520continuum%2520modeling%2520of%2520other%2520parts.%2520The%2520continuum%2520is%2520treated%2520in%2520terms%2520of%2520quasi-atoms%2520of%2520different%2520size%252C%2520comprising%2520composite%2520medium.%2520The%2520parameters%2520of%2520interaction%2520potential%2520between%2520the%2520quasi-atoms%2520are%2520optimized%2520to%2520match%2520elastic%2520properties%2520of%2520the%2520composite%2520medium%2520to%2520those%2520of%2520the%2520atomic%2520one.%2520The%2520optimization%2520method%2520coincides%2520conceptually%2520with%2520the%2520online%2520Machine%2520Learning%2520%2528ML%2529%2520methods%252C%2520making%2520it%2520computationally%2520very%2520efficient.%2520Such%2520an%2520approach%2520allows%2520a%2520straightforward%2520application%2520of%2520standard%2520software%2520packages%2520for%2520molecular%2520dynamics%2520%2528MD%2529%252C%2520supplemented%2520by%2520the%2520ML-based%2520optimizer.%2520The%2520new%2520method%2520is%2520applied%2520to%2520model%2520systems%2520with%2520a%2520simple%252C%2520pairwise%2520Lennard-Jones%2520potential%252C%2520as%2520well%2520with%2520multi-body%2520Tersoff%2520potential%252C%2520describing%2520covalent%2520bonds.%2520Using%2520LAMMPS%2520software%2520we%2520simulate%2520collision%2520of%2520particles%2520of%2520different%2520size.%2520Comparing%2520simulation%2520results%252C%2520obtained%2520by%2520the%2520novel%2520method%252C%2520with%2520full-atomic%2520simulations%252C%2520we%2520demonstrate%2520its%2520accuracy%252C%2520validity%2520and%2520overwhelming%2520superiority%2520in%2520computational%2520speed.%2520Furthermore%252C%2520we%2520compare%2520our%2520method%2520with%2520other%2520hybrid%2520methods%252C%2520specifically%252C%2520with%2520the%2520closest%2520one%2520--%2520AtC%2520%2528Atomic%2520to%2520Continuum%2529%2520method.%2520We%2520demonstrate%2520a%2520significant%2520superiority%2520of%2520our%2520approach%2520in%2520computational%2520speed%2520and%2520implementation%2520convenience.%2520Finally%252C%2520we%2520discuss%2520a%2520possible%2520extension%2520of%2520the%2520method%2520for%2520modeling%2520other%2520phenomena.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20and%20accurate%20quasi-atom%20method%20for%20simultaneous%20atomistic%20and%20continuum%20simulation%20of%20solids&entry.906535625=Artem%20Chuprov%20and%20Egor%20E.%20Nuzhin%20and%20Alexey%20A.%20Tsukanov%20and%20Nikolay%20V.%20Brilliantov&entry.1292438233=We%20report%20a%20novel%20hybrid%20method%20of%20simultaneous%20atomistic%20simulation%20of%20solids%20in%20critical%20regions%20%28contacts%20surfaces%2C%20cracks%20areas%2C%20etc.%29%2C%20along%20with%20continuum%20modeling%20of%20other%20parts.%20The%20continuum%20is%20treated%20in%20terms%20of%20quasi-atoms%20of%20different%20size%2C%20comprising%20composite%20medium.%20The%20parameters%20of%20interaction%20potential%20between%20the%20quasi-atoms%20are%20optimized%20to%20match%20elastic%20properties%20of%20the%20composite%20medium%20to%20those%20of%20the%20atomic%20one.%20The%20optimization%20method%20coincides%20conceptually%20with%20the%20online%20Machine%20Learning%20%28ML%29%20methods%2C%20making%20it%20computationally%20very%20efficient.%20Such%20an%20approach%20allows%20a%20straightforward%20application%20of%20standard%20software%20packages%20for%20molecular%20dynamics%20%28MD%29%2C%20supplemented%20by%20the%20ML-based%20optimizer.%20The%20new%20method%20is%20applied%20to%20model%20systems%20with%20a%20simple%2C%20pairwise%20Lennard-Jones%20potential%2C%20as%20well%20with%20multi-body%20Tersoff%20potential%2C%20describing%20covalent%20bonds.%20Using%20LAMMPS%20software%20we%20simulate%20collision%20of%20particles%20of%20different%20size.%20Comparing%20simulation%20results%2C%20obtained%20by%20the%20novel%20method%2C%20with%20full-atomic%20simulations%2C%20we%20demonstrate%20its%20accuracy%2C%20validity%20and%20overwhelming%20superiority%20in%20computational%20speed.%20Furthermore%2C%20we%20compare%20our%20method%20with%20other%20hybrid%20methods%2C%20specifically%2C%20with%20the%20closest%20one%20--%20AtC%20%28Atomic%20to%20Continuum%29%20method.%20We%20demonstrate%20a%20significant%20superiority%20of%20our%20approach%20in%20computational%20speed%20and%20implementation%20convenience.%20Finally%2C%20we%20discuss%20a%20possible%20extension%20of%20the%20method%20for%20modeling%20other%20phenomena.&entry.1838667208=http%3A//arxiv.org/abs/2602.14867v1&entry.124074799=Read"},
{"title": "On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials", "author": "Rotem Mulayoff and Sebastian U. Stich", "abstract": "The dynamical stability of the iterates during training plays a key role in determining the minima obtained by optimization algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, which have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. Recent work has shown that GD may stably oscillate near a linearly unstable minimum and still converge once the step size decays, indicating that linear analysis can be misleading. In this work, we explicitly study the effect of nonlinear terms. Specifically, we derive an exact criterion for stable oscillations of GD near minima in the multivariate setting. Our condition depends on high-order derivatives, generalizing existing results. Extending the analysis to stochastic gradient descent (SGD), we show that nonlinear dynamics can diverge in expectation even if a single batch is unstable. This implies that stability can be dictated by a single batch that oscillates unstably, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, the nonlinear dynamics of SGD are stable in expectation.", "link": "http://arxiv.org/abs/2602.14789v1", "date": "2026-02-16", "relevancy": 2.1938, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.45}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4454}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Stability%20of%20Nonlinear%20Dynamics%20in%20GD%20and%20SGD%3A%20Beyond%20Quadratic%20Potentials&body=Title%3A%20On%20the%20Stability%20of%20Nonlinear%20Dynamics%20in%20GD%20and%20SGD%3A%20Beyond%20Quadratic%20Potentials%0AAuthor%3A%20Rotem%20Mulayoff%20and%20Sebastian%20U.%20Stich%0AAbstract%3A%20The%20dynamical%20stability%20of%20the%20iterates%20during%20training%20plays%20a%20key%20role%20in%20determining%20the%20minima%20obtained%20by%20optimization%20algorithms.%20For%20example%2C%20stable%20solutions%20of%20gradient%20descent%20%28GD%29%20correspond%20to%20flat%20minima%2C%20which%20have%20been%20associated%20with%20favorable%20features.%20While%20prior%20work%20often%20relies%20on%20linearization%20to%20determine%20stability%2C%20it%20remains%20unclear%20whether%20linearized%20dynamics%20faithfully%20capture%20the%20full%20nonlinear%20behavior.%20Recent%20work%20has%20shown%20that%20GD%20may%20stably%20oscillate%20near%20a%20linearly%20unstable%20minimum%20and%20still%20converge%20once%20the%20step%20size%20decays%2C%20indicating%20that%20linear%20analysis%20can%20be%20misleading.%20In%20this%20work%2C%20we%20explicitly%20study%20the%20effect%20of%20nonlinear%20terms.%20Specifically%2C%20we%20derive%20an%20exact%20criterion%20for%20stable%20oscillations%20of%20GD%20near%20minima%20in%20the%20multivariate%20setting.%20Our%20condition%20depends%20on%20high-order%20derivatives%2C%20generalizing%20existing%20results.%20Extending%20the%20analysis%20to%20stochastic%20gradient%20descent%20%28SGD%29%2C%20we%20show%20that%20nonlinear%20dynamics%20can%20diverge%20in%20expectation%20even%20if%20a%20single%20batch%20is%20unstable.%20This%20implies%20that%20stability%20can%20be%20dictated%20by%20a%20single%20batch%20that%20oscillates%20unstably%2C%20rather%20than%20an%20average%20effect%2C%20as%20linear%20analysis%20suggests.%20Finally%2C%20we%20prove%20that%20if%20all%20batches%20are%20linearly%20stable%2C%20the%20nonlinear%20dynamics%20of%20SGD%20are%20stable%20in%20expectation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Stability%2520of%2520Nonlinear%2520Dynamics%2520in%2520GD%2520and%2520SGD%253A%2520Beyond%2520Quadratic%2520Potentials%26entry.906535625%3DRotem%2520Mulayoff%2520and%2520Sebastian%2520U.%2520Stich%26entry.1292438233%3DThe%2520dynamical%2520stability%2520of%2520the%2520iterates%2520during%2520training%2520plays%2520a%2520key%2520role%2520in%2520determining%2520the%2520minima%2520obtained%2520by%2520optimization%2520algorithms.%2520For%2520example%252C%2520stable%2520solutions%2520of%2520gradient%2520descent%2520%2528GD%2529%2520correspond%2520to%2520flat%2520minima%252C%2520which%2520have%2520been%2520associated%2520with%2520favorable%2520features.%2520While%2520prior%2520work%2520often%2520relies%2520on%2520linearization%2520to%2520determine%2520stability%252C%2520it%2520remains%2520unclear%2520whether%2520linearized%2520dynamics%2520faithfully%2520capture%2520the%2520full%2520nonlinear%2520behavior.%2520Recent%2520work%2520has%2520shown%2520that%2520GD%2520may%2520stably%2520oscillate%2520near%2520a%2520linearly%2520unstable%2520minimum%2520and%2520still%2520converge%2520once%2520the%2520step%2520size%2520decays%252C%2520indicating%2520that%2520linear%2520analysis%2520can%2520be%2520misleading.%2520In%2520this%2520work%252C%2520we%2520explicitly%2520study%2520the%2520effect%2520of%2520nonlinear%2520terms.%2520Specifically%252C%2520we%2520derive%2520an%2520exact%2520criterion%2520for%2520stable%2520oscillations%2520of%2520GD%2520near%2520minima%2520in%2520the%2520multivariate%2520setting.%2520Our%2520condition%2520depends%2520on%2520high-order%2520derivatives%252C%2520generalizing%2520existing%2520results.%2520Extending%2520the%2520analysis%2520to%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%252C%2520we%2520show%2520that%2520nonlinear%2520dynamics%2520can%2520diverge%2520in%2520expectation%2520even%2520if%2520a%2520single%2520batch%2520is%2520unstable.%2520This%2520implies%2520that%2520stability%2520can%2520be%2520dictated%2520by%2520a%2520single%2520batch%2520that%2520oscillates%2520unstably%252C%2520rather%2520than%2520an%2520average%2520effect%252C%2520as%2520linear%2520analysis%2520suggests.%2520Finally%252C%2520we%2520prove%2520that%2520if%2520all%2520batches%2520are%2520linearly%2520stable%252C%2520the%2520nonlinear%2520dynamics%2520of%2520SGD%2520are%2520stable%2520in%2520expectation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Stability%20of%20Nonlinear%20Dynamics%20in%20GD%20and%20SGD%3A%20Beyond%20Quadratic%20Potentials&entry.906535625=Rotem%20Mulayoff%20and%20Sebastian%20U.%20Stich&entry.1292438233=The%20dynamical%20stability%20of%20the%20iterates%20during%20training%20plays%20a%20key%20role%20in%20determining%20the%20minima%20obtained%20by%20optimization%20algorithms.%20For%20example%2C%20stable%20solutions%20of%20gradient%20descent%20%28GD%29%20correspond%20to%20flat%20minima%2C%20which%20have%20been%20associated%20with%20favorable%20features.%20While%20prior%20work%20often%20relies%20on%20linearization%20to%20determine%20stability%2C%20it%20remains%20unclear%20whether%20linearized%20dynamics%20faithfully%20capture%20the%20full%20nonlinear%20behavior.%20Recent%20work%20has%20shown%20that%20GD%20may%20stably%20oscillate%20near%20a%20linearly%20unstable%20minimum%20and%20still%20converge%20once%20the%20step%20size%20decays%2C%20indicating%20that%20linear%20analysis%20can%20be%20misleading.%20In%20this%20work%2C%20we%20explicitly%20study%20the%20effect%20of%20nonlinear%20terms.%20Specifically%2C%20we%20derive%20an%20exact%20criterion%20for%20stable%20oscillations%20of%20GD%20near%20minima%20in%20the%20multivariate%20setting.%20Our%20condition%20depends%20on%20high-order%20derivatives%2C%20generalizing%20existing%20results.%20Extending%20the%20analysis%20to%20stochastic%20gradient%20descent%20%28SGD%29%2C%20we%20show%20that%20nonlinear%20dynamics%20can%20diverge%20in%20expectation%20even%20if%20a%20single%20batch%20is%20unstable.%20This%20implies%20that%20stability%20can%20be%20dictated%20by%20a%20single%20batch%20that%20oscillates%20unstably%2C%20rather%20than%20an%20average%20effect%2C%20as%20linear%20analysis%20suggests.%20Finally%2C%20we%20prove%20that%20if%20all%20batches%20are%20linearly%20stable%2C%20the%20nonlinear%20dynamics%20of%20SGD%20are%20stable%20in%20expectation.&entry.1838667208=http%3A//arxiv.org/abs/2602.14789v1&entry.124074799=Read"},
{"title": "GenPANIS: A Latent-Variable Generative Framework for Forward and Inverse PDE Problems in Multiphase Media", "author": "Matthaios Chatzopoulos and Phaedon-Stelios Koutsourelakis", "abstract": "Inverse problems and inverse design in multiphase media, i.e., recovering or engineering microstructures to achieve target macroscopic responses, require operating on discrete-valued material fields, rendering the problem non-differentiable and incompatible with gradient-based methods. Existing approaches either relax to continuous approximations, compromising physical fidelity, or employ separate heavyweight models for forward and inverse tasks. We propose GenPANIS, a unified generative framework that preserves exact discrete microstructures while enabling gradient-based inference through continuous latent embeddings. The model learns a joint distribution over microstructures and PDE solutions, supporting bidirectional inference (forward prediction and inverse recovery) within a single architecture. The generative formulation enables training with unlabeled data, physics residuals, and minimal labeled pairs. A physics-aware decoder incorporating a differentiable coarse-grained PDE solver preserves governing equation structure, enabling extrapolation to varying boundary conditions and microstructural statistics. A learnable normalizing flow prior captures complex posterior structure for inverse problems. Demonstrated on Darcy flow and Helmholtz equations, GenPANIS maintains accuracy on challenging extrapolative scenarios - including unseen boundary conditions, volume fractions, and microstructural morphologies, with sparse, noisy observations. It outperforms state-of-the-art methods while using 10 - 100 times fewer parameters and providing principled uncertainty quantification.", "link": "http://arxiv.org/abs/2602.14642v1", "date": "2026-02-16", "relevancy": 2.1876, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5577}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.545}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenPANIS%3A%20A%20Latent-Variable%20Generative%20Framework%20for%20Forward%20and%20Inverse%20PDE%20Problems%20in%20Multiphase%20Media&body=Title%3A%20GenPANIS%3A%20A%20Latent-Variable%20Generative%20Framework%20for%20Forward%20and%20Inverse%20PDE%20Problems%20in%20Multiphase%20Media%0AAuthor%3A%20Matthaios%20Chatzopoulos%20and%20Phaedon-Stelios%20Koutsourelakis%0AAbstract%3A%20Inverse%20problems%20and%20inverse%20design%20in%20multiphase%20media%2C%20i.e.%2C%20recovering%20or%20engineering%20microstructures%20to%20achieve%20target%20macroscopic%20responses%2C%20require%20operating%20on%20discrete-valued%20material%20fields%2C%20rendering%20the%20problem%20non-differentiable%20and%20incompatible%20with%20gradient-based%20methods.%20Existing%20approaches%20either%20relax%20to%20continuous%20approximations%2C%20compromising%20physical%20fidelity%2C%20or%20employ%20separate%20heavyweight%20models%20for%20forward%20and%20inverse%20tasks.%20We%20propose%20GenPANIS%2C%20a%20unified%20generative%20framework%20that%20preserves%20exact%20discrete%20microstructures%20while%20enabling%20gradient-based%20inference%20through%20continuous%20latent%20embeddings.%20The%20model%20learns%20a%20joint%20distribution%20over%20microstructures%20and%20PDE%20solutions%2C%20supporting%20bidirectional%20inference%20%28forward%20prediction%20and%20inverse%20recovery%29%20within%20a%20single%20architecture.%20The%20generative%20formulation%20enables%20training%20with%20unlabeled%20data%2C%20physics%20residuals%2C%20and%20minimal%20labeled%20pairs.%20A%20physics-aware%20decoder%20incorporating%20a%20differentiable%20coarse-grained%20PDE%20solver%20preserves%20governing%20equation%20structure%2C%20enabling%20extrapolation%20to%20varying%20boundary%20conditions%20and%20microstructural%20statistics.%20A%20learnable%20normalizing%20flow%20prior%20captures%20complex%20posterior%20structure%20for%20inverse%20problems.%20Demonstrated%20on%20Darcy%20flow%20and%20Helmholtz%20equations%2C%20GenPANIS%20maintains%20accuracy%20on%20challenging%20extrapolative%20scenarios%20-%20including%20unseen%20boundary%20conditions%2C%20volume%20fractions%2C%20and%20microstructural%20morphologies%2C%20with%20sparse%2C%20noisy%20observations.%20It%20outperforms%20state-of-the-art%20methods%20while%20using%2010%20-%20100%20times%20fewer%20parameters%20and%20providing%20principled%20uncertainty%20quantification.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenPANIS%253A%2520A%2520Latent-Variable%2520Generative%2520Framework%2520for%2520Forward%2520and%2520Inverse%2520PDE%2520Problems%2520in%2520Multiphase%2520Media%26entry.906535625%3DMatthaios%2520Chatzopoulos%2520and%2520Phaedon-Stelios%2520Koutsourelakis%26entry.1292438233%3DInverse%2520problems%2520and%2520inverse%2520design%2520in%2520multiphase%2520media%252C%2520i.e.%252C%2520recovering%2520or%2520engineering%2520microstructures%2520to%2520achieve%2520target%2520macroscopic%2520responses%252C%2520require%2520operating%2520on%2520discrete-valued%2520material%2520fields%252C%2520rendering%2520the%2520problem%2520non-differentiable%2520and%2520incompatible%2520with%2520gradient-based%2520methods.%2520Existing%2520approaches%2520either%2520relax%2520to%2520continuous%2520approximations%252C%2520compromising%2520physical%2520fidelity%252C%2520or%2520employ%2520separate%2520heavyweight%2520models%2520for%2520forward%2520and%2520inverse%2520tasks.%2520We%2520propose%2520GenPANIS%252C%2520a%2520unified%2520generative%2520framework%2520that%2520preserves%2520exact%2520discrete%2520microstructures%2520while%2520enabling%2520gradient-based%2520inference%2520through%2520continuous%2520latent%2520embeddings.%2520The%2520model%2520learns%2520a%2520joint%2520distribution%2520over%2520microstructures%2520and%2520PDE%2520solutions%252C%2520supporting%2520bidirectional%2520inference%2520%2528forward%2520prediction%2520and%2520inverse%2520recovery%2529%2520within%2520a%2520single%2520architecture.%2520The%2520generative%2520formulation%2520enables%2520training%2520with%2520unlabeled%2520data%252C%2520physics%2520residuals%252C%2520and%2520minimal%2520labeled%2520pairs.%2520A%2520physics-aware%2520decoder%2520incorporating%2520a%2520differentiable%2520coarse-grained%2520PDE%2520solver%2520preserves%2520governing%2520equation%2520structure%252C%2520enabling%2520extrapolation%2520to%2520varying%2520boundary%2520conditions%2520and%2520microstructural%2520statistics.%2520A%2520learnable%2520normalizing%2520flow%2520prior%2520captures%2520complex%2520posterior%2520structure%2520for%2520inverse%2520problems.%2520Demonstrated%2520on%2520Darcy%2520flow%2520and%2520Helmholtz%2520equations%252C%2520GenPANIS%2520maintains%2520accuracy%2520on%2520challenging%2520extrapolative%2520scenarios%2520-%2520including%2520unseen%2520boundary%2520conditions%252C%2520volume%2520fractions%252C%2520and%2520microstructural%2520morphologies%252C%2520with%2520sparse%252C%2520noisy%2520observations.%2520It%2520outperforms%2520state-of-the-art%2520methods%2520while%2520using%252010%2520-%2520100%2520times%2520fewer%2520parameters%2520and%2520providing%2520principled%2520uncertainty%2520quantification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenPANIS%3A%20A%20Latent-Variable%20Generative%20Framework%20for%20Forward%20and%20Inverse%20PDE%20Problems%20in%20Multiphase%20Media&entry.906535625=Matthaios%20Chatzopoulos%20and%20Phaedon-Stelios%20Koutsourelakis&entry.1292438233=Inverse%20problems%20and%20inverse%20design%20in%20multiphase%20media%2C%20i.e.%2C%20recovering%20or%20engineering%20microstructures%20to%20achieve%20target%20macroscopic%20responses%2C%20require%20operating%20on%20discrete-valued%20material%20fields%2C%20rendering%20the%20problem%20non-differentiable%20and%20incompatible%20with%20gradient-based%20methods.%20Existing%20approaches%20either%20relax%20to%20continuous%20approximations%2C%20compromising%20physical%20fidelity%2C%20or%20employ%20separate%20heavyweight%20models%20for%20forward%20and%20inverse%20tasks.%20We%20propose%20GenPANIS%2C%20a%20unified%20generative%20framework%20that%20preserves%20exact%20discrete%20microstructures%20while%20enabling%20gradient-based%20inference%20through%20continuous%20latent%20embeddings.%20The%20model%20learns%20a%20joint%20distribution%20over%20microstructures%20and%20PDE%20solutions%2C%20supporting%20bidirectional%20inference%20%28forward%20prediction%20and%20inverse%20recovery%29%20within%20a%20single%20architecture.%20The%20generative%20formulation%20enables%20training%20with%20unlabeled%20data%2C%20physics%20residuals%2C%20and%20minimal%20labeled%20pairs.%20A%20physics-aware%20decoder%20incorporating%20a%20differentiable%20coarse-grained%20PDE%20solver%20preserves%20governing%20equation%20structure%2C%20enabling%20extrapolation%20to%20varying%20boundary%20conditions%20and%20microstructural%20statistics.%20A%20learnable%20normalizing%20flow%20prior%20captures%20complex%20posterior%20structure%20for%20inverse%20problems.%20Demonstrated%20on%20Darcy%20flow%20and%20Helmholtz%20equations%2C%20GenPANIS%20maintains%20accuracy%20on%20challenging%20extrapolative%20scenarios%20-%20including%20unseen%20boundary%20conditions%2C%20volume%20fractions%2C%20and%20microstructural%20morphologies%2C%20with%20sparse%2C%20noisy%20observations.%20It%20outperforms%20state-of-the-art%20methods%20while%20using%2010%20-%20100%20times%20fewer%20parameters%20and%20providing%20principled%20uncertainty%20quantification.&entry.1838667208=http%3A//arxiv.org/abs/2602.14642v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application", "author": "Thomas Hickling and Maxwell Hogan and Abdulla Tammam and Nabil Aouf", "abstract": "This paper presents the first end-to-end framework that combines guidance, navigation, and centralised task allocation for multiple UAVs performing autonomous search-and-rescue (SAR) in GNSS-denied indoor environments. A Twin Delayed Deep Deterministic Policy Gradient controller is trained with an Artificial Potential Field (APF) reward that blends attractive and repulsive potentials with continuous control, accelerating convergence and yielding smoother, safer trajectories than distance-only baselines. Collaborative mission assignment is solved by a deep Graph Attention Network that, at each decision step, reasons over the drone-task graph to produce near-optimal allocations with negligible on-board compute. To arrest the notorious Z-drift of indoor LiDAR-SLAM, we fuse depth-camera altimetry with IMU vertical velocity in a lightweight complementary filter, giving centimetre-level altitude stability without external beacons. The resulting system was deployed on two 1m-class quad-rotors and flight-tested in a cluttered, multi-level disaster mock-up designed for the NATO-Sapience Autonomous Cooperative Drone Competition. Compared with prior DRL guidance that remains largely in simulation, our framework demonstrates an ability to navigate complex indoor environments, securing first place in the 2024 event. These results demonstrate that APF-shaped DRL and GAT-driven cooperation can translate to reliable real-world SAR operations.", "link": "http://arxiv.org/abs/2502.20326v2", "date": "2026-02-16", "relevancy": 2.1829, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5486}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%20based%20Autonomous%20Decision-Making%20for%20Cooperative%20UAVs%3A%20A%20Search%20and%20Rescue%20Real%20World%20Application&body=Title%3A%20Deep%20Reinforcement%20Learning%20based%20Autonomous%20Decision-Making%20for%20Cooperative%20UAVs%3A%20A%20Search%20and%20Rescue%20Real%20World%20Application%0AAuthor%3A%20Thomas%20Hickling%20and%20Maxwell%20Hogan%20and%20Abdulla%20Tammam%20and%20Nabil%20Aouf%0AAbstract%3A%20This%20paper%20presents%20the%20first%20end-to-end%20framework%20that%20combines%20guidance%2C%20navigation%2C%20and%20centralised%20task%20allocation%20for%20multiple%20UAVs%20performing%20autonomous%20search-and-rescue%20%28SAR%29%20in%20GNSS-denied%20indoor%20environments.%20A%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%20controller%20is%20trained%20with%20an%20Artificial%20Potential%20Field%20%28APF%29%20reward%20that%20blends%20attractive%20and%20repulsive%20potentials%20with%20continuous%20control%2C%20accelerating%20convergence%20and%20yielding%20smoother%2C%20safer%20trajectories%20than%20distance-only%20baselines.%20Collaborative%20mission%20assignment%20is%20solved%20by%20a%20deep%20Graph%20Attention%20Network%20that%2C%20at%20each%20decision%20step%2C%20reasons%20over%20the%20drone-task%20graph%20to%20produce%20near-optimal%20allocations%20with%20negligible%20on-board%20compute.%20To%20arrest%20the%20notorious%20Z-drift%20of%20indoor%20LiDAR-SLAM%2C%20we%20fuse%20depth-camera%20altimetry%20with%20IMU%20vertical%20velocity%20in%20a%20lightweight%20complementary%20filter%2C%20giving%20centimetre-level%20altitude%20stability%20without%20external%20beacons.%20The%20resulting%20system%20was%20deployed%20on%20two%201m-class%20quad-rotors%20and%20flight-tested%20in%20a%20cluttered%2C%20multi-level%20disaster%20mock-up%20designed%20for%20the%20NATO-Sapience%20Autonomous%20Cooperative%20Drone%20Competition.%20Compared%20with%20prior%20DRL%20guidance%20that%20remains%20largely%20in%20simulation%2C%20our%20framework%20demonstrates%20an%20ability%20to%20navigate%20complex%20indoor%20environments%2C%20securing%20first%20place%20in%20the%202024%20event.%20These%20results%20demonstrate%20that%20APF-shaped%20DRL%20and%20GAT-driven%20cooperation%20can%20translate%20to%20reliable%20real-world%20SAR%20operations.%0ALink%3A%20http%3A//arxiv.org/abs/2502.20326v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning%2520based%2520Autonomous%2520Decision-Making%2520for%2520Cooperative%2520UAVs%253A%2520A%2520Search%2520and%2520Rescue%2520Real%2520World%2520Application%26entry.906535625%3DThomas%2520Hickling%2520and%2520Maxwell%2520Hogan%2520and%2520Abdulla%2520Tammam%2520and%2520Nabil%2520Aouf%26entry.1292438233%3DThis%2520paper%2520presents%2520the%2520first%2520end-to-end%2520framework%2520that%2520combines%2520guidance%252C%2520navigation%252C%2520and%2520centralised%2520task%2520allocation%2520for%2520multiple%2520UAVs%2520performing%2520autonomous%2520search-and-rescue%2520%2528SAR%2529%2520in%2520GNSS-denied%2520indoor%2520environments.%2520A%2520Twin%2520Delayed%2520Deep%2520Deterministic%2520Policy%2520Gradient%2520controller%2520is%2520trained%2520with%2520an%2520Artificial%2520Potential%2520Field%2520%2528APF%2529%2520reward%2520that%2520blends%2520attractive%2520and%2520repulsive%2520potentials%2520with%2520continuous%2520control%252C%2520accelerating%2520convergence%2520and%2520yielding%2520smoother%252C%2520safer%2520trajectories%2520than%2520distance-only%2520baselines.%2520Collaborative%2520mission%2520assignment%2520is%2520solved%2520by%2520a%2520deep%2520Graph%2520Attention%2520Network%2520that%252C%2520at%2520each%2520decision%2520step%252C%2520reasons%2520over%2520the%2520drone-task%2520graph%2520to%2520produce%2520near-optimal%2520allocations%2520with%2520negligible%2520on-board%2520compute.%2520To%2520arrest%2520the%2520notorious%2520Z-drift%2520of%2520indoor%2520LiDAR-SLAM%252C%2520we%2520fuse%2520depth-camera%2520altimetry%2520with%2520IMU%2520vertical%2520velocity%2520in%2520a%2520lightweight%2520complementary%2520filter%252C%2520giving%2520centimetre-level%2520altitude%2520stability%2520without%2520external%2520beacons.%2520The%2520resulting%2520system%2520was%2520deployed%2520on%2520two%25201m-class%2520quad-rotors%2520and%2520flight-tested%2520in%2520a%2520cluttered%252C%2520multi-level%2520disaster%2520mock-up%2520designed%2520for%2520the%2520NATO-Sapience%2520Autonomous%2520Cooperative%2520Drone%2520Competition.%2520Compared%2520with%2520prior%2520DRL%2520guidance%2520that%2520remains%2520largely%2520in%2520simulation%252C%2520our%2520framework%2520demonstrates%2520an%2520ability%2520to%2520navigate%2520complex%2520indoor%2520environments%252C%2520securing%2520first%2520place%2520in%2520the%25202024%2520event.%2520These%2520results%2520demonstrate%2520that%2520APF-shaped%2520DRL%2520and%2520GAT-driven%2520cooperation%2520can%2520translate%2520to%2520reliable%2520real-world%2520SAR%2520operations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.20326v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20based%20Autonomous%20Decision-Making%20for%20Cooperative%20UAVs%3A%20A%20Search%20and%20Rescue%20Real%20World%20Application&entry.906535625=Thomas%20Hickling%20and%20Maxwell%20Hogan%20and%20Abdulla%20Tammam%20and%20Nabil%20Aouf&entry.1292438233=This%20paper%20presents%20the%20first%20end-to-end%20framework%20that%20combines%20guidance%2C%20navigation%2C%20and%20centralised%20task%20allocation%20for%20multiple%20UAVs%20performing%20autonomous%20search-and-rescue%20%28SAR%29%20in%20GNSS-denied%20indoor%20environments.%20A%20Twin%20Delayed%20Deep%20Deterministic%20Policy%20Gradient%20controller%20is%20trained%20with%20an%20Artificial%20Potential%20Field%20%28APF%29%20reward%20that%20blends%20attractive%20and%20repulsive%20potentials%20with%20continuous%20control%2C%20accelerating%20convergence%20and%20yielding%20smoother%2C%20safer%20trajectories%20than%20distance-only%20baselines.%20Collaborative%20mission%20assignment%20is%20solved%20by%20a%20deep%20Graph%20Attention%20Network%20that%2C%20at%20each%20decision%20step%2C%20reasons%20over%20the%20drone-task%20graph%20to%20produce%20near-optimal%20allocations%20with%20negligible%20on-board%20compute.%20To%20arrest%20the%20notorious%20Z-drift%20of%20indoor%20LiDAR-SLAM%2C%20we%20fuse%20depth-camera%20altimetry%20with%20IMU%20vertical%20velocity%20in%20a%20lightweight%20complementary%20filter%2C%20giving%20centimetre-level%20altitude%20stability%20without%20external%20beacons.%20The%20resulting%20system%20was%20deployed%20on%20two%201m-class%20quad-rotors%20and%20flight-tested%20in%20a%20cluttered%2C%20multi-level%20disaster%20mock-up%20designed%20for%20the%20NATO-Sapience%20Autonomous%20Cooperative%20Drone%20Competition.%20Compared%20with%20prior%20DRL%20guidance%20that%20remains%20largely%20in%20simulation%2C%20our%20framework%20demonstrates%20an%20ability%20to%20navigate%20complex%20indoor%20environments%2C%20securing%20first%20place%20in%20the%202024%20event.%20These%20results%20demonstrate%20that%20APF-shaped%20DRL%20and%20GAT-driven%20cooperation%20can%20translate%20to%20reliable%20real-world%20SAR%20operations.&entry.1838667208=http%3A//arxiv.org/abs/2502.20326v2&entry.124074799=Read"},
{"title": "Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models", "author": "Samuel Lippl and Thomas McGee and Kimberly Lopez and Ziwen Pan and Pierce Zhang and Salma Ziadi and Oliver Eberle and Ida Momennejad", "abstract": "How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activations and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering activations and annotating their matched reasoning traces using an automated LLM pipeline. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4 induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.", "link": "http://arxiv.org/abs/2510.15987v2", "date": "2026-02-16", "relevancy": 2.1755, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.555}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.555}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Algorithmic%20Primitives%20and%20Compositional%20Geometry%20of%20Reasoning%20in%20Language%20Models&body=Title%3A%20Algorithmic%20Primitives%20and%20Compositional%20Geometry%20of%20Reasoning%20in%20Language%20Models%0AAuthor%3A%20Samuel%20Lippl%20and%20Thomas%20McGee%20and%20Kimberly%20Lopez%20and%20Ziwen%20Pan%20and%20Pierce%20Zhang%20and%20Salma%20Ziadi%20and%20Oliver%20Eberle%20and%20Ida%20Momennejad%0AAbstract%3A%20How%20do%20latent%20and%20inference%20time%20computations%20enable%20large%20language%20models%20%28LLMs%29%20to%20solve%20multi-step%20reasoning%3F%20We%20introduce%20a%20framework%20for%20tracing%20and%20steering%20algorithmic%20primitives%20that%20underlie%20model%20reasoning.%20Our%20approach%20links%20reasoning%20traces%20to%20internal%20activations%20and%20evaluates%20algorithmic%20primitives%20by%20injecting%20them%20into%20residual%20streams%20and%20measuring%20their%20effect%20on%20reasoning%20steps%20and%20task%20performance.%20We%20consider%20four%20benchmarks%3A%20Traveling%20Salesperson%20Problem%20%28TSP%29%2C%203SAT%2C%20AIME%2C%20and%20graph%20navigation.%20We%20operationalize%20primitives%20by%20clustering%20activations%20and%20annotating%20their%20matched%20reasoning%20traces%20using%20an%20automated%20LLM%20pipeline.%20We%20then%20apply%20function%20vector%20methods%20to%20derive%20primitive%20vectors%20as%20reusable%20compositional%20building%20blocks%20of%20reasoning.%20Primitive%20vectors%20can%20be%20combined%20through%20addition%2C%20subtraction%2C%20and%20scalar%20operations%2C%20revealing%20a%20geometric%20logic%20in%20activation%20space.%20Cross-task%20and%20cross-model%20evaluations%20%28Phi-4%2C%20Phi-4-Reasoning%2C%20Llama-3-8B%29%20show%20both%20shared%20and%20task-specific%20primitives.%20Notably%2C%20comparing%20Phi-4%20with%20its%20reasoning-finetuned%20variant%20highlights%20compositional%20generalization%20after%20finetuning%3A%20Phi-4-Reasoning%20exhibits%20more%20systematic%20use%20of%20verification%20and%20path-generation%20primitives.%20Injecting%20the%20associated%20primitive%20vectors%20in%20Phi-4%20induces%20behavioral%20hallmarks%20associated%20with%20Phi-4-Reasoning.%20Together%2C%20these%20findings%20demonstrate%20that%20reasoning%20in%20LLMs%20may%20be%20supported%20by%20a%20compositional%20geometry%20of%20algorithmic%20primitives%2C%20that%20primitives%20transfer%20cross-task%20and%20cross-model%2C%20and%20that%20reasoning%20finetuning%20strengthens%20algorithmic%20generalization%20across%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2510.15987v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgorithmic%2520Primitives%2520and%2520Compositional%2520Geometry%2520of%2520Reasoning%2520in%2520Language%2520Models%26entry.906535625%3DSamuel%2520Lippl%2520and%2520Thomas%2520McGee%2520and%2520Kimberly%2520Lopez%2520and%2520Ziwen%2520Pan%2520and%2520Pierce%2520Zhang%2520and%2520Salma%2520Ziadi%2520and%2520Oliver%2520Eberle%2520and%2520Ida%2520Momennejad%26entry.1292438233%3DHow%2520do%2520latent%2520and%2520inference%2520time%2520computations%2520enable%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520solve%2520multi-step%2520reasoning%253F%2520We%2520introduce%2520a%2520framework%2520for%2520tracing%2520and%2520steering%2520algorithmic%2520primitives%2520that%2520underlie%2520model%2520reasoning.%2520Our%2520approach%2520links%2520reasoning%2520traces%2520to%2520internal%2520activations%2520and%2520evaluates%2520algorithmic%2520primitives%2520by%2520injecting%2520them%2520into%2520residual%2520streams%2520and%2520measuring%2520their%2520effect%2520on%2520reasoning%2520steps%2520and%2520task%2520performance.%2520We%2520consider%2520four%2520benchmarks%253A%2520Traveling%2520Salesperson%2520Problem%2520%2528TSP%2529%252C%25203SAT%252C%2520AIME%252C%2520and%2520graph%2520navigation.%2520We%2520operationalize%2520primitives%2520by%2520clustering%2520activations%2520and%2520annotating%2520their%2520matched%2520reasoning%2520traces%2520using%2520an%2520automated%2520LLM%2520pipeline.%2520We%2520then%2520apply%2520function%2520vector%2520methods%2520to%2520derive%2520primitive%2520vectors%2520as%2520reusable%2520compositional%2520building%2520blocks%2520of%2520reasoning.%2520Primitive%2520vectors%2520can%2520be%2520combined%2520through%2520addition%252C%2520subtraction%252C%2520and%2520scalar%2520operations%252C%2520revealing%2520a%2520geometric%2520logic%2520in%2520activation%2520space.%2520Cross-task%2520and%2520cross-model%2520evaluations%2520%2528Phi-4%252C%2520Phi-4-Reasoning%252C%2520Llama-3-8B%2529%2520show%2520both%2520shared%2520and%2520task-specific%2520primitives.%2520Notably%252C%2520comparing%2520Phi-4%2520with%2520its%2520reasoning-finetuned%2520variant%2520highlights%2520compositional%2520generalization%2520after%2520finetuning%253A%2520Phi-4-Reasoning%2520exhibits%2520more%2520systematic%2520use%2520of%2520verification%2520and%2520path-generation%2520primitives.%2520Injecting%2520the%2520associated%2520primitive%2520vectors%2520in%2520Phi-4%2520induces%2520behavioral%2520hallmarks%2520associated%2520with%2520Phi-4-Reasoning.%2520Together%252C%2520these%2520findings%2520demonstrate%2520that%2520reasoning%2520in%2520LLMs%2520may%2520be%2520supported%2520by%2520a%2520compositional%2520geometry%2520of%2520algorithmic%2520primitives%252C%2520that%2520primitives%2520transfer%2520cross-task%2520and%2520cross-model%252C%2520and%2520that%2520reasoning%2520finetuning%2520strengthens%2520algorithmic%2520generalization%2520across%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.15987v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithmic%20Primitives%20and%20Compositional%20Geometry%20of%20Reasoning%20in%20Language%20Models&entry.906535625=Samuel%20Lippl%20and%20Thomas%20McGee%20and%20Kimberly%20Lopez%20and%20Ziwen%20Pan%20and%20Pierce%20Zhang%20and%20Salma%20Ziadi%20and%20Oliver%20Eberle%20and%20Ida%20Momennejad&entry.1292438233=How%20do%20latent%20and%20inference%20time%20computations%20enable%20large%20language%20models%20%28LLMs%29%20to%20solve%20multi-step%20reasoning%3F%20We%20introduce%20a%20framework%20for%20tracing%20and%20steering%20algorithmic%20primitives%20that%20underlie%20model%20reasoning.%20Our%20approach%20links%20reasoning%20traces%20to%20internal%20activations%20and%20evaluates%20algorithmic%20primitives%20by%20injecting%20them%20into%20residual%20streams%20and%20measuring%20their%20effect%20on%20reasoning%20steps%20and%20task%20performance.%20We%20consider%20four%20benchmarks%3A%20Traveling%20Salesperson%20Problem%20%28TSP%29%2C%203SAT%2C%20AIME%2C%20and%20graph%20navigation.%20We%20operationalize%20primitives%20by%20clustering%20activations%20and%20annotating%20their%20matched%20reasoning%20traces%20using%20an%20automated%20LLM%20pipeline.%20We%20then%20apply%20function%20vector%20methods%20to%20derive%20primitive%20vectors%20as%20reusable%20compositional%20building%20blocks%20of%20reasoning.%20Primitive%20vectors%20can%20be%20combined%20through%20addition%2C%20subtraction%2C%20and%20scalar%20operations%2C%20revealing%20a%20geometric%20logic%20in%20activation%20space.%20Cross-task%20and%20cross-model%20evaluations%20%28Phi-4%2C%20Phi-4-Reasoning%2C%20Llama-3-8B%29%20show%20both%20shared%20and%20task-specific%20primitives.%20Notably%2C%20comparing%20Phi-4%20with%20its%20reasoning-finetuned%20variant%20highlights%20compositional%20generalization%20after%20finetuning%3A%20Phi-4-Reasoning%20exhibits%20more%20systematic%20use%20of%20verification%20and%20path-generation%20primitives.%20Injecting%20the%20associated%20primitive%20vectors%20in%20Phi-4%20induces%20behavioral%20hallmarks%20associated%20with%20Phi-4-Reasoning.%20Together%2C%20these%20findings%20demonstrate%20that%20reasoning%20in%20LLMs%20may%20be%20supported%20by%20a%20compositional%20geometry%20of%20algorithmic%20primitives%2C%20that%20primitives%20transfer%20cross-task%20and%20cross-model%2C%20and%20that%20reasoning%20finetuning%20strengthens%20algorithmic%20generalization%20across%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2510.15987v2&entry.124074799=Read"},
{"title": "VariViT: A Vision Transformer for Variable Image Sizes", "author": "Aswathi Varma and Suprosanna Shit and Chinmay Prabhakar and Daniel Scholz and Hongwei Bran Li and Bjoern Menze and Daniel Rueckert and Benedikt Wiestler", "abstract": "Vision Transformers (ViTs) have emerged as the state-of-the-art architecture in representation learning, leveraging self-attention mechanisms to excel in various tasks. ViTs split images into fixed-size patches, constraining them to a predefined size and necessitating pre-processing steps like resizing, padding, or cropping. This poses challenges in medical imaging, particularly with irregularly shaped structures like tumors. A fixed bounding box crop size produces input images with highly variable foreground-to-background ratios. Resizing medical images can degrade information and introduce artefacts, impacting diagnosis. Hence, tailoring variable-sized crops to regions of interest can enhance feature representation capabilities. Moreover, large images are computationally expensive, and smaller sizes risk information loss, presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT model crafted to handle variable image sizes while maintaining a consistent patch size. VariViT employs a novel positional embedding resizing scheme for a variable number of patches. We also implement a new batching strategy within VariViT to reduce computational complexity, resulting in faster training and inference times. In our evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification. It achieves F1-scores of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed batching strategy reduces computation time by up to 30% compared to conventional architectures. These findings underscore the efficacy of VariViT in image representation learning. Our code can be found here: https://github.com/Aswathi-Varma/varivit", "link": "http://arxiv.org/abs/2602.14615v1", "date": "2026-02-16", "relevancy": 2.1676, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5496}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5452}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VariViT%3A%20A%20Vision%20Transformer%20for%20Variable%20Image%20Sizes&body=Title%3A%20VariViT%3A%20A%20Vision%20Transformer%20for%20Variable%20Image%20Sizes%0AAuthor%3A%20Aswathi%20Varma%20and%20Suprosanna%20Shit%20and%20Chinmay%20Prabhakar%20and%20Daniel%20Scholz%20and%20Hongwei%20Bran%20Li%20and%20Bjoern%20Menze%20and%20Daniel%20Rueckert%20and%20Benedikt%20Wiestler%0AAbstract%3A%20Vision%20Transformers%20%28ViTs%29%20have%20emerged%20as%20the%20state-of-the-art%20architecture%20in%20representation%20learning%2C%20leveraging%20self-attention%20mechanisms%20to%20excel%20in%20various%20tasks.%20ViTs%20split%20images%20into%20fixed-size%20patches%2C%20constraining%20them%20to%20a%20predefined%20size%20and%20necessitating%20pre-processing%20steps%20like%20resizing%2C%20padding%2C%20or%20cropping.%20This%20poses%20challenges%20in%20medical%20imaging%2C%20particularly%20with%20irregularly%20shaped%20structures%20like%20tumors.%20A%20fixed%20bounding%20box%20crop%20size%20produces%20input%20images%20with%20highly%20variable%20foreground-to-background%20ratios.%20Resizing%20medical%20images%20can%20degrade%20information%20and%20introduce%20artefacts%2C%20impacting%20diagnosis.%20Hence%2C%20tailoring%20variable-sized%20crops%20to%20regions%20of%20interest%20can%20enhance%20feature%20representation%20capabilities.%20Moreover%2C%20large%20images%20are%20computationally%20expensive%2C%20and%20smaller%20sizes%20risk%20information%20loss%2C%20presenting%20a%20computation-accuracy%20tradeoff.%20We%20propose%20VariViT%2C%20an%20improved%20ViT%20model%20crafted%20to%20handle%20variable%20image%20sizes%20while%20maintaining%20a%20consistent%20patch%20size.%20VariViT%20employs%20a%20novel%20positional%20embedding%20resizing%20scheme%20for%20a%20variable%20number%20of%20patches.%20We%20also%20implement%20a%20new%20batching%20strategy%20within%20VariViT%20to%20reduce%20computational%20complexity%2C%20resulting%20in%20faster%20training%20and%20inference%20times.%20In%20our%20evaluations%20on%20two%203D%20brain%20MRI%20datasets%2C%20VariViT%20surpasses%20vanilla%20ViTs%20and%20ResNet%20in%20glioma%20genotype%20prediction%20and%20brain%20tumor%20classification.%20It%20achieves%20F1-scores%20of%2075.5%25%20and%2076.3%25%2C%20respectively%2C%20learning%20more%20discriminative%20features.%20Our%20proposed%20batching%20strategy%20reduces%20computation%20time%20by%20up%20to%2030%25%20compared%20to%20conventional%20architectures.%20These%20findings%20underscore%20the%20efficacy%20of%20VariViT%20in%20image%20representation%20learning.%20Our%20code%20can%20be%20found%20here%3A%20https%3A//github.com/Aswathi-Varma/varivit%0ALink%3A%20http%3A//arxiv.org/abs/2602.14615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVariViT%253A%2520A%2520Vision%2520Transformer%2520for%2520Variable%2520Image%2520Sizes%26entry.906535625%3DAswathi%2520Varma%2520and%2520Suprosanna%2520Shit%2520and%2520Chinmay%2520Prabhakar%2520and%2520Daniel%2520Scholz%2520and%2520Hongwei%2520Bran%2520Li%2520and%2520Bjoern%2520Menze%2520and%2520Daniel%2520Rueckert%2520and%2520Benedikt%2520Wiestler%26entry.1292438233%3DVision%2520Transformers%2520%2528ViTs%2529%2520have%2520emerged%2520as%2520the%2520state-of-the-art%2520architecture%2520in%2520representation%2520learning%252C%2520leveraging%2520self-attention%2520mechanisms%2520to%2520excel%2520in%2520various%2520tasks.%2520ViTs%2520split%2520images%2520into%2520fixed-size%2520patches%252C%2520constraining%2520them%2520to%2520a%2520predefined%2520size%2520and%2520necessitating%2520pre-processing%2520steps%2520like%2520resizing%252C%2520padding%252C%2520or%2520cropping.%2520This%2520poses%2520challenges%2520in%2520medical%2520imaging%252C%2520particularly%2520with%2520irregularly%2520shaped%2520structures%2520like%2520tumors.%2520A%2520fixed%2520bounding%2520box%2520crop%2520size%2520produces%2520input%2520images%2520with%2520highly%2520variable%2520foreground-to-background%2520ratios.%2520Resizing%2520medical%2520images%2520can%2520degrade%2520information%2520and%2520introduce%2520artefacts%252C%2520impacting%2520diagnosis.%2520Hence%252C%2520tailoring%2520variable-sized%2520crops%2520to%2520regions%2520of%2520interest%2520can%2520enhance%2520feature%2520representation%2520capabilities.%2520Moreover%252C%2520large%2520images%2520are%2520computationally%2520expensive%252C%2520and%2520smaller%2520sizes%2520risk%2520information%2520loss%252C%2520presenting%2520a%2520computation-accuracy%2520tradeoff.%2520We%2520propose%2520VariViT%252C%2520an%2520improved%2520ViT%2520model%2520crafted%2520to%2520handle%2520variable%2520image%2520sizes%2520while%2520maintaining%2520a%2520consistent%2520patch%2520size.%2520VariViT%2520employs%2520a%2520novel%2520positional%2520embedding%2520resizing%2520scheme%2520for%2520a%2520variable%2520number%2520of%2520patches.%2520We%2520also%2520implement%2520a%2520new%2520batching%2520strategy%2520within%2520VariViT%2520to%2520reduce%2520computational%2520complexity%252C%2520resulting%2520in%2520faster%2520training%2520and%2520inference%2520times.%2520In%2520our%2520evaluations%2520on%2520two%25203D%2520brain%2520MRI%2520datasets%252C%2520VariViT%2520surpasses%2520vanilla%2520ViTs%2520and%2520ResNet%2520in%2520glioma%2520genotype%2520prediction%2520and%2520brain%2520tumor%2520classification.%2520It%2520achieves%2520F1-scores%2520of%252075.5%2525%2520and%252076.3%2525%252C%2520respectively%252C%2520learning%2520more%2520discriminative%2520features.%2520Our%2520proposed%2520batching%2520strategy%2520reduces%2520computation%2520time%2520by%2520up%2520to%252030%2525%2520compared%2520to%2520conventional%2520architectures.%2520These%2520findings%2520underscore%2520the%2520efficacy%2520of%2520VariViT%2520in%2520image%2520representation%2520learning.%2520Our%2520code%2520can%2520be%2520found%2520here%253A%2520https%253A//github.com/Aswathi-Varma/varivit%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VariViT%3A%20A%20Vision%20Transformer%20for%20Variable%20Image%20Sizes&entry.906535625=Aswathi%20Varma%20and%20Suprosanna%20Shit%20and%20Chinmay%20Prabhakar%20and%20Daniel%20Scholz%20and%20Hongwei%20Bran%20Li%20and%20Bjoern%20Menze%20and%20Daniel%20Rueckert%20and%20Benedikt%20Wiestler&entry.1292438233=Vision%20Transformers%20%28ViTs%29%20have%20emerged%20as%20the%20state-of-the-art%20architecture%20in%20representation%20learning%2C%20leveraging%20self-attention%20mechanisms%20to%20excel%20in%20various%20tasks.%20ViTs%20split%20images%20into%20fixed-size%20patches%2C%20constraining%20them%20to%20a%20predefined%20size%20and%20necessitating%20pre-processing%20steps%20like%20resizing%2C%20padding%2C%20or%20cropping.%20This%20poses%20challenges%20in%20medical%20imaging%2C%20particularly%20with%20irregularly%20shaped%20structures%20like%20tumors.%20A%20fixed%20bounding%20box%20crop%20size%20produces%20input%20images%20with%20highly%20variable%20foreground-to-background%20ratios.%20Resizing%20medical%20images%20can%20degrade%20information%20and%20introduce%20artefacts%2C%20impacting%20diagnosis.%20Hence%2C%20tailoring%20variable-sized%20crops%20to%20regions%20of%20interest%20can%20enhance%20feature%20representation%20capabilities.%20Moreover%2C%20large%20images%20are%20computationally%20expensive%2C%20and%20smaller%20sizes%20risk%20information%20loss%2C%20presenting%20a%20computation-accuracy%20tradeoff.%20We%20propose%20VariViT%2C%20an%20improved%20ViT%20model%20crafted%20to%20handle%20variable%20image%20sizes%20while%20maintaining%20a%20consistent%20patch%20size.%20VariViT%20employs%20a%20novel%20positional%20embedding%20resizing%20scheme%20for%20a%20variable%20number%20of%20patches.%20We%20also%20implement%20a%20new%20batching%20strategy%20within%20VariViT%20to%20reduce%20computational%20complexity%2C%20resulting%20in%20faster%20training%20and%20inference%20times.%20In%20our%20evaluations%20on%20two%203D%20brain%20MRI%20datasets%2C%20VariViT%20surpasses%20vanilla%20ViTs%20and%20ResNet%20in%20glioma%20genotype%20prediction%20and%20brain%20tumor%20classification.%20It%20achieves%20F1-scores%20of%2075.5%25%20and%2076.3%25%2C%20respectively%2C%20learning%20more%20discriminative%20features.%20Our%20proposed%20batching%20strategy%20reduces%20computation%20time%20by%20up%20to%2030%25%20compared%20to%20conventional%20architectures.%20These%20findings%20underscore%20the%20efficacy%20of%20VariViT%20in%20image%20representation%20learning.%20Our%20code%20can%20be%20found%20here%3A%20https%3A//github.com/Aswathi-Varma/varivit&entry.1838667208=http%3A//arxiv.org/abs/2602.14615v1&entry.124074799=Read"},
{"title": "Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment", "author": "Mounvik K and N Harshit", "abstract": "We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.", "link": "http://arxiv.org/abs/2602.14889v1", "date": "2026-02-16", "relevancy": 2.1621, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5968}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Web-Scale%20Multimodal%20Summarization%20using%20CLIP-Based%20Semantic%20Alignment&body=Title%3A%20Web-Scale%20Multimodal%20Summarization%20using%20CLIP-Based%20Semantic%20Alignment%0AAuthor%3A%20Mounvik%20K%20and%20N%20Harshit%0AAbstract%3A%20We%20introduce%20Web-Scale%20Multimodal%20Summarization%2C%20a%20lightweight%20framework%20for%20generating%20summaries%20by%20combining%20retrieved%20text%20and%20image%20data%20from%20web%20sources.%20Given%20a%20user-defined%20topic%2C%20the%20system%20performs%20parallel%20web%2C%20news%2C%20and%20image%20searches.%20Retrieved%20images%20are%20ranked%20using%20a%20fine-tuned%20CLIP%20model%20to%20measure%20semantic%20alignment%20with%20topic%20and%20text.%20Optional%20BLIP%20captioning%20enables%20image-only%20summaries%20for%20stronger%20multimodal%20coherence.The%20pipeline%20supports%20features%20such%20as%20adjustable%20fetch%20limits%2C%20semantic%20filtering%2C%20summary%20styling%2C%20and%20downloading%20structured%20outputs.%20We%20expose%20the%20system%20via%20a%20Gradio-based%20API%20with%20controllable%20parameters%20and%20preconfigured%20presets.Evaluation%20on%20500%20image-caption%20pairs%20with%2020%3A1%20contrastive%20negatives%20yields%20a%20ROC-AUC%20of%200.9270%2C%20an%20F1-score%20of%200.6504%2C%20and%20an%20accuracy%20of%2096.99%25%2C%20demonstrating%20strong%20multimodal%20alignment.%20This%20work%20provides%20a%20configurable%2C%20deployable%20tool%20for%20web-scale%20summarization%20that%20integrates%20language%2C%20retrieval%2C%20and%20vision%20models%20in%20a%20user-extensible%20pipeline.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeb-Scale%2520Multimodal%2520Summarization%2520using%2520CLIP-Based%2520Semantic%2520Alignment%26entry.906535625%3DMounvik%2520K%2520and%2520N%2520Harshit%26entry.1292438233%3DWe%2520introduce%2520Web-Scale%2520Multimodal%2520Summarization%252C%2520a%2520lightweight%2520framework%2520for%2520generating%2520summaries%2520by%2520combining%2520retrieved%2520text%2520and%2520image%2520data%2520from%2520web%2520sources.%2520Given%2520a%2520user-defined%2520topic%252C%2520the%2520system%2520performs%2520parallel%2520web%252C%2520news%252C%2520and%2520image%2520searches.%2520Retrieved%2520images%2520are%2520ranked%2520using%2520a%2520fine-tuned%2520CLIP%2520model%2520to%2520measure%2520semantic%2520alignment%2520with%2520topic%2520and%2520text.%2520Optional%2520BLIP%2520captioning%2520enables%2520image-only%2520summaries%2520for%2520stronger%2520multimodal%2520coherence.The%2520pipeline%2520supports%2520features%2520such%2520as%2520adjustable%2520fetch%2520limits%252C%2520semantic%2520filtering%252C%2520summary%2520styling%252C%2520and%2520downloading%2520structured%2520outputs.%2520We%2520expose%2520the%2520system%2520via%2520a%2520Gradio-based%2520API%2520with%2520controllable%2520parameters%2520and%2520preconfigured%2520presets.Evaluation%2520on%2520500%2520image-caption%2520pairs%2520with%252020%253A1%2520contrastive%2520negatives%2520yields%2520a%2520ROC-AUC%2520of%25200.9270%252C%2520an%2520F1-score%2520of%25200.6504%252C%2520and%2520an%2520accuracy%2520of%252096.99%2525%252C%2520demonstrating%2520strong%2520multimodal%2520alignment.%2520This%2520work%2520provides%2520a%2520configurable%252C%2520deployable%2520tool%2520for%2520web-scale%2520summarization%2520that%2520integrates%2520language%252C%2520retrieval%252C%2520and%2520vision%2520models%2520in%2520a%2520user-extensible%2520pipeline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Web-Scale%20Multimodal%20Summarization%20using%20CLIP-Based%20Semantic%20Alignment&entry.906535625=Mounvik%20K%20and%20N%20Harshit&entry.1292438233=We%20introduce%20Web-Scale%20Multimodal%20Summarization%2C%20a%20lightweight%20framework%20for%20generating%20summaries%20by%20combining%20retrieved%20text%20and%20image%20data%20from%20web%20sources.%20Given%20a%20user-defined%20topic%2C%20the%20system%20performs%20parallel%20web%2C%20news%2C%20and%20image%20searches.%20Retrieved%20images%20are%20ranked%20using%20a%20fine-tuned%20CLIP%20model%20to%20measure%20semantic%20alignment%20with%20topic%20and%20text.%20Optional%20BLIP%20captioning%20enables%20image-only%20summaries%20for%20stronger%20multimodal%20coherence.The%20pipeline%20supports%20features%20such%20as%20adjustable%20fetch%20limits%2C%20semantic%20filtering%2C%20summary%20styling%2C%20and%20downloading%20structured%20outputs.%20We%20expose%20the%20system%20via%20a%20Gradio-based%20API%20with%20controllable%20parameters%20and%20preconfigured%20presets.Evaluation%20on%20500%20image-caption%20pairs%20with%2020%3A1%20contrastive%20negatives%20yields%20a%20ROC-AUC%20of%200.9270%2C%20an%20F1-score%20of%200.6504%2C%20and%20an%20accuracy%20of%2096.99%25%2C%20demonstrating%20strong%20multimodal%20alignment.%20This%20work%20provides%20a%20configurable%2C%20deployable%20tool%20for%20web-scale%20summarization%20that%20integrates%20language%2C%20retrieval%2C%20and%20vision%20models%20in%20a%20user-extensible%20pipeline.&entry.1838667208=http%3A//arxiv.org/abs/2602.14889v1&entry.124074799=Read"},
{"title": "Robust MultiSpecies Agricultural Segmentation Across Devices, Seasons, and Sensors Using Hierarchical DINOv2 Models", "author": "Artzai Picon and Itziar Eguskiza and Daniel Mugica and Javier Romero and Carlos Javier Jimenez and Eric White and Gabriel Do-Lago-Junqueira and Christian Klukas and Ramon Navarra-Mestre", "abstract": "Reliable plant species and damage segmentation for herbicide field research trials requires models that can withstand substantial real-world variation across seasons, geographies, devices, and sensing modalities. Most deep learning approaches trained on controlled datasets fail to generalize under these domain shifts, limiting their suitability for operational phenotyping pipelines. This study evaluates a segmentation framework that integrates vision foundation models (DINOv2) with hierarchical taxonomic inference to improve robustness across heterogeneous agricultural conditions. We train on a large, multi-year dataset collected in Germany and Spain (2018-2020), comprising 14 plant species and 4 herbicide damage classes, and assess generalization under increasingly challenging shifts: temporal and device changes (2023), geographic transfer to the United States, and extreme sensor shift to drone imagery (2024). Results show that the foundation-model backbone consistently outperforms prior baselines, improving species-level F1 from 0.52 to 0.87 on in-distribution data and maintaining significant advantages under moderate (0.77 vs. 0.24) and extreme (0.44 vs. 0.14) shift conditions. Hierarchical inference provides an additional layer of robustness, enabling meaningful predictions even when fine-grained species classification degrades (family F1: 0.68, class F1: 0.88 on aerial imagery). Error analysis reveals that failures under severe shift stem primarily from vegetation-soil confusion, suggesting that taxonomic distinctions remain preserved despite background and viewpoint variability. The system is now deployed within BASF's phenotyping workflow for herbicide research trials across multiple regions, illustrating the practical viability of combining foundation models with structured biological hierarchies for scalable, shift-resilient agricultural monitoring.", "link": "http://arxiv.org/abs/2508.07514v2", "date": "2026-02-16", "relevancy": 2.1615, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20MultiSpecies%20Agricultural%20Segmentation%20Across%20Devices%2C%20Seasons%2C%20and%20Sensors%20Using%20Hierarchical%20DINOv2%20Models&body=Title%3A%20Robust%20MultiSpecies%20Agricultural%20Segmentation%20Across%20Devices%2C%20Seasons%2C%20and%20Sensors%20Using%20Hierarchical%20DINOv2%20Models%0AAuthor%3A%20Artzai%20Picon%20and%20Itziar%20Eguskiza%20and%20Daniel%20Mugica%20and%20Javier%20Romero%20and%20Carlos%20Javier%20Jimenez%20and%20Eric%20White%20and%20Gabriel%20Do-Lago-Junqueira%20and%20Christian%20Klukas%20and%20Ramon%20Navarra-Mestre%0AAbstract%3A%20Reliable%20plant%20species%20and%20damage%20segmentation%20for%20herbicide%20field%20research%20trials%20requires%20models%20that%20can%20withstand%20substantial%20real-world%20variation%20across%20seasons%2C%20geographies%2C%20devices%2C%20and%20sensing%20modalities.%20Most%20deep%20learning%20approaches%20trained%20on%20controlled%20datasets%20fail%20to%20generalize%20under%20these%20domain%20shifts%2C%20limiting%20their%20suitability%20for%20operational%20phenotyping%20pipelines.%20This%20study%20evaluates%20a%20segmentation%20framework%20that%20integrates%20vision%20foundation%20models%20%28DINOv2%29%20with%20hierarchical%20taxonomic%20inference%20to%20improve%20robustness%20across%20heterogeneous%20agricultural%20conditions.%20We%20train%20on%20a%20large%2C%20multi-year%20dataset%20collected%20in%20Germany%20and%20Spain%20%282018-2020%29%2C%20comprising%2014%20plant%20species%20and%204%20herbicide%20damage%20classes%2C%20and%20assess%20generalization%20under%20increasingly%20challenging%20shifts%3A%20temporal%20and%20device%20changes%20%282023%29%2C%20geographic%20transfer%20to%20the%20United%20States%2C%20and%20extreme%20sensor%20shift%20to%20drone%20imagery%20%282024%29.%20Results%20show%20that%20the%20foundation-model%20backbone%20consistently%20outperforms%20prior%20baselines%2C%20improving%20species-level%20F1%20from%200.52%20to%200.87%20on%20in-distribution%20data%20and%20maintaining%20significant%20advantages%20under%20moderate%20%280.77%20vs.%200.24%29%20and%20extreme%20%280.44%20vs.%200.14%29%20shift%20conditions.%20Hierarchical%20inference%20provides%20an%20additional%20layer%20of%20robustness%2C%20enabling%20meaningful%20predictions%20even%20when%20fine-grained%20species%20classification%20degrades%20%28family%20F1%3A%200.68%2C%20class%20F1%3A%200.88%20on%20aerial%20imagery%29.%20Error%20analysis%20reveals%20that%20failures%20under%20severe%20shift%20stem%20primarily%20from%20vegetation-soil%20confusion%2C%20suggesting%20that%20taxonomic%20distinctions%20remain%20preserved%20despite%20background%20and%20viewpoint%20variability.%20The%20system%20is%20now%20deployed%20within%20BASF%27s%20phenotyping%20workflow%20for%20herbicide%20research%20trials%20across%20multiple%20regions%2C%20illustrating%20the%20practical%20viability%20of%20combining%20foundation%20models%20with%20structured%20biological%20hierarchies%20for%20scalable%2C%20shift-resilient%20agricultural%20monitoring.%0ALink%3A%20http%3A//arxiv.org/abs/2508.07514v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520MultiSpecies%2520Agricultural%2520Segmentation%2520Across%2520Devices%252C%2520Seasons%252C%2520and%2520Sensors%2520Using%2520Hierarchical%2520DINOv2%2520Models%26entry.906535625%3DArtzai%2520Picon%2520and%2520Itziar%2520Eguskiza%2520and%2520Daniel%2520Mugica%2520and%2520Javier%2520Romero%2520and%2520Carlos%2520Javier%2520Jimenez%2520and%2520Eric%2520White%2520and%2520Gabriel%2520Do-Lago-Junqueira%2520and%2520Christian%2520Klukas%2520and%2520Ramon%2520Navarra-Mestre%26entry.1292438233%3DReliable%2520plant%2520species%2520and%2520damage%2520segmentation%2520for%2520herbicide%2520field%2520research%2520trials%2520requires%2520models%2520that%2520can%2520withstand%2520substantial%2520real-world%2520variation%2520across%2520seasons%252C%2520geographies%252C%2520devices%252C%2520and%2520sensing%2520modalities.%2520Most%2520deep%2520learning%2520approaches%2520trained%2520on%2520controlled%2520datasets%2520fail%2520to%2520generalize%2520under%2520these%2520domain%2520shifts%252C%2520limiting%2520their%2520suitability%2520for%2520operational%2520phenotyping%2520pipelines.%2520This%2520study%2520evaluates%2520a%2520segmentation%2520framework%2520that%2520integrates%2520vision%2520foundation%2520models%2520%2528DINOv2%2529%2520with%2520hierarchical%2520taxonomic%2520inference%2520to%2520improve%2520robustness%2520across%2520heterogeneous%2520agricultural%2520conditions.%2520We%2520train%2520on%2520a%2520large%252C%2520multi-year%2520dataset%2520collected%2520in%2520Germany%2520and%2520Spain%2520%25282018-2020%2529%252C%2520comprising%252014%2520plant%2520species%2520and%25204%2520herbicide%2520damage%2520classes%252C%2520and%2520assess%2520generalization%2520under%2520increasingly%2520challenging%2520shifts%253A%2520temporal%2520and%2520device%2520changes%2520%25282023%2529%252C%2520geographic%2520transfer%2520to%2520the%2520United%2520States%252C%2520and%2520extreme%2520sensor%2520shift%2520to%2520drone%2520imagery%2520%25282024%2529.%2520Results%2520show%2520that%2520the%2520foundation-model%2520backbone%2520consistently%2520outperforms%2520prior%2520baselines%252C%2520improving%2520species-level%2520F1%2520from%25200.52%2520to%25200.87%2520on%2520in-distribution%2520data%2520and%2520maintaining%2520significant%2520advantages%2520under%2520moderate%2520%25280.77%2520vs.%25200.24%2529%2520and%2520extreme%2520%25280.44%2520vs.%25200.14%2529%2520shift%2520conditions.%2520Hierarchical%2520inference%2520provides%2520an%2520additional%2520layer%2520of%2520robustness%252C%2520enabling%2520meaningful%2520predictions%2520even%2520when%2520fine-grained%2520species%2520classification%2520degrades%2520%2528family%2520F1%253A%25200.68%252C%2520class%2520F1%253A%25200.88%2520on%2520aerial%2520imagery%2529.%2520Error%2520analysis%2520reveals%2520that%2520failures%2520under%2520severe%2520shift%2520stem%2520primarily%2520from%2520vegetation-soil%2520confusion%252C%2520suggesting%2520that%2520taxonomic%2520distinctions%2520remain%2520preserved%2520despite%2520background%2520and%2520viewpoint%2520variability.%2520The%2520system%2520is%2520now%2520deployed%2520within%2520BASF%2527s%2520phenotyping%2520workflow%2520for%2520herbicide%2520research%2520trials%2520across%2520multiple%2520regions%252C%2520illustrating%2520the%2520practical%2520viability%2520of%2520combining%2520foundation%2520models%2520with%2520structured%2520biological%2520hierarchies%2520for%2520scalable%252C%2520shift-resilient%2520agricultural%2520monitoring.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07514v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20MultiSpecies%20Agricultural%20Segmentation%20Across%20Devices%2C%20Seasons%2C%20and%20Sensors%20Using%20Hierarchical%20DINOv2%20Models&entry.906535625=Artzai%20Picon%20and%20Itziar%20Eguskiza%20and%20Daniel%20Mugica%20and%20Javier%20Romero%20and%20Carlos%20Javier%20Jimenez%20and%20Eric%20White%20and%20Gabriel%20Do-Lago-Junqueira%20and%20Christian%20Klukas%20and%20Ramon%20Navarra-Mestre&entry.1292438233=Reliable%20plant%20species%20and%20damage%20segmentation%20for%20herbicide%20field%20research%20trials%20requires%20models%20that%20can%20withstand%20substantial%20real-world%20variation%20across%20seasons%2C%20geographies%2C%20devices%2C%20and%20sensing%20modalities.%20Most%20deep%20learning%20approaches%20trained%20on%20controlled%20datasets%20fail%20to%20generalize%20under%20these%20domain%20shifts%2C%20limiting%20their%20suitability%20for%20operational%20phenotyping%20pipelines.%20This%20study%20evaluates%20a%20segmentation%20framework%20that%20integrates%20vision%20foundation%20models%20%28DINOv2%29%20with%20hierarchical%20taxonomic%20inference%20to%20improve%20robustness%20across%20heterogeneous%20agricultural%20conditions.%20We%20train%20on%20a%20large%2C%20multi-year%20dataset%20collected%20in%20Germany%20and%20Spain%20%282018-2020%29%2C%20comprising%2014%20plant%20species%20and%204%20herbicide%20damage%20classes%2C%20and%20assess%20generalization%20under%20increasingly%20challenging%20shifts%3A%20temporal%20and%20device%20changes%20%282023%29%2C%20geographic%20transfer%20to%20the%20United%20States%2C%20and%20extreme%20sensor%20shift%20to%20drone%20imagery%20%282024%29.%20Results%20show%20that%20the%20foundation-model%20backbone%20consistently%20outperforms%20prior%20baselines%2C%20improving%20species-level%20F1%20from%200.52%20to%200.87%20on%20in-distribution%20data%20and%20maintaining%20significant%20advantages%20under%20moderate%20%280.77%20vs.%200.24%29%20and%20extreme%20%280.44%20vs.%200.14%29%20shift%20conditions.%20Hierarchical%20inference%20provides%20an%20additional%20layer%20of%20robustness%2C%20enabling%20meaningful%20predictions%20even%20when%20fine-grained%20species%20classification%20degrades%20%28family%20F1%3A%200.68%2C%20class%20F1%3A%200.88%20on%20aerial%20imagery%29.%20Error%20analysis%20reveals%20that%20failures%20under%20severe%20shift%20stem%20primarily%20from%20vegetation-soil%20confusion%2C%20suggesting%20that%20taxonomic%20distinctions%20remain%20preserved%20despite%20background%20and%20viewpoint%20variability.%20The%20system%20is%20now%20deployed%20within%20BASF%27s%20phenotyping%20workflow%20for%20herbicide%20research%20trials%20across%20multiple%20regions%2C%20illustrating%20the%20practical%20viability%20of%20combining%20foundation%20models%20with%20structured%20biological%20hierarchies%20for%20scalable%2C%20shift-resilient%20agricultural%20monitoring.&entry.1838667208=http%3A//arxiv.org/abs/2508.07514v2&entry.124074799=Read"},
{"title": "VIGIL: Tackling Hallucination Detection in Image Recontextualization", "author": "Joanna Wojciechowicz and Maria \u0141ubniewska and Jakub Antczak and Justyna Baczy\u0144ska and Wojciech Gromski and Wojciech Koz\u0142owski and Maciej Zi\u0119ba", "abstract": "We introduce VIGIL (Visual Inconsistency & Generative In-context Lucidity), the first benchmark dataset and framework providing a fine-grained categorization of hallucinations in the multimodal image recontextualization task for large multimodal models (LMMs). While existing research often treats hallucinations as a uniform issue, our work addresses a significant gap in multimodal evaluation by decomposing these errors into five categories: pasted object hallucinations, background hallucinations, object omission, positional & logical inconsistencies, and physical law violations. To address these complexities, we propose a multi-stage detection pipeline. Our architecture processes recontextualized images through a series of specialized steps targeting object-level fidelity, background consistency, and omission detection, leveraging a coordinated ensemble of open-source models, whose effectiveness is demonstrated through extensive experimental evaluations. Our approach enables a deeper understanding of where the models fail with an explanation; thus, we fill a gap in the field, as no prior methods offer such categorization and decomposition for this task. To promote transparency and further exploration, we openly release VIGIL, along with the detection pipeline and benchmark code, through our GitHub repository: https://github.com/mlubneuskaya/vigil and Data repository: https://huggingface.co/datasets/joannaww/VIGIL.", "link": "http://arxiv.org/abs/2602.14633v1", "date": "2026-02-16", "relevancy": 2.1459, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5503}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIGIL%3A%20Tackling%20Hallucination%20Detection%20in%20Image%20Recontextualization&body=Title%3A%20VIGIL%3A%20Tackling%20Hallucination%20Detection%20in%20Image%20Recontextualization%0AAuthor%3A%20Joanna%20Wojciechowicz%20and%20Maria%20%C5%81ubniewska%20and%20Jakub%20Antczak%20and%20Justyna%20Baczy%C5%84ska%20and%20Wojciech%20Gromski%20and%20Wojciech%20Koz%C5%82owski%20and%20Maciej%20Zi%C4%99ba%0AAbstract%3A%20We%20introduce%20VIGIL%20%28Visual%20Inconsistency%20%26%20Generative%20In-context%20Lucidity%29%2C%20the%20first%20benchmark%20dataset%20and%20framework%20providing%20a%20fine-grained%20categorization%20of%20hallucinations%20in%20the%20multimodal%20image%20recontextualization%20task%20for%20large%20multimodal%20models%20%28LMMs%29.%20While%20existing%20research%20often%20treats%20hallucinations%20as%20a%20uniform%20issue%2C%20our%20work%20addresses%20a%20significant%20gap%20in%20multimodal%20evaluation%20by%20decomposing%20these%20errors%20into%20five%20categories%3A%20pasted%20object%20hallucinations%2C%20background%20hallucinations%2C%20object%20omission%2C%20positional%20%26%20logical%20inconsistencies%2C%20and%20physical%20law%20violations.%20To%20address%20these%20complexities%2C%20we%20propose%20a%20multi-stage%20detection%20pipeline.%20Our%20architecture%20processes%20recontextualized%20images%20through%20a%20series%20of%20specialized%20steps%20targeting%20object-level%20fidelity%2C%20background%20consistency%2C%20and%20omission%20detection%2C%20leveraging%20a%20coordinated%20ensemble%20of%20open-source%20models%2C%20whose%20effectiveness%20is%20demonstrated%20through%20extensive%20experimental%20evaluations.%20Our%20approach%20enables%20a%20deeper%20understanding%20of%20where%20the%20models%20fail%20with%20an%20explanation%3B%20thus%2C%20we%20fill%20a%20gap%20in%20the%20field%2C%20as%20no%20prior%20methods%20offer%20such%20categorization%20and%20decomposition%20for%20this%20task.%20To%20promote%20transparency%20and%20further%20exploration%2C%20we%20openly%20release%20VIGIL%2C%20along%20with%20the%20detection%20pipeline%20and%20benchmark%20code%2C%20through%20our%20GitHub%20repository%3A%20https%3A//github.com/mlubneuskaya/vigil%20and%20Data%20repository%3A%20https%3A//huggingface.co/datasets/joannaww/VIGIL.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIGIL%253A%2520Tackling%2520Hallucination%2520Detection%2520in%2520Image%2520Recontextualization%26entry.906535625%3DJoanna%2520Wojciechowicz%2520and%2520Maria%2520%25C5%2581ubniewska%2520and%2520Jakub%2520Antczak%2520and%2520Justyna%2520Baczy%25C5%2584ska%2520and%2520Wojciech%2520Gromski%2520and%2520Wojciech%2520Koz%25C5%2582owski%2520and%2520Maciej%2520Zi%25C4%2599ba%26entry.1292438233%3DWe%2520introduce%2520VIGIL%2520%2528Visual%2520Inconsistency%2520%2526%2520Generative%2520In-context%2520Lucidity%2529%252C%2520the%2520first%2520benchmark%2520dataset%2520and%2520framework%2520providing%2520a%2520fine-grained%2520categorization%2520of%2520hallucinations%2520in%2520the%2520multimodal%2520image%2520recontextualization%2520task%2520for%2520large%2520multimodal%2520models%2520%2528LMMs%2529.%2520While%2520existing%2520research%2520often%2520treats%2520hallucinations%2520as%2520a%2520uniform%2520issue%252C%2520our%2520work%2520addresses%2520a%2520significant%2520gap%2520in%2520multimodal%2520evaluation%2520by%2520decomposing%2520these%2520errors%2520into%2520five%2520categories%253A%2520pasted%2520object%2520hallucinations%252C%2520background%2520hallucinations%252C%2520object%2520omission%252C%2520positional%2520%2526%2520logical%2520inconsistencies%252C%2520and%2520physical%2520law%2520violations.%2520To%2520address%2520these%2520complexities%252C%2520we%2520propose%2520a%2520multi-stage%2520detection%2520pipeline.%2520Our%2520architecture%2520processes%2520recontextualized%2520images%2520through%2520a%2520series%2520of%2520specialized%2520steps%2520targeting%2520object-level%2520fidelity%252C%2520background%2520consistency%252C%2520and%2520omission%2520detection%252C%2520leveraging%2520a%2520coordinated%2520ensemble%2520of%2520open-source%2520models%252C%2520whose%2520effectiveness%2520is%2520demonstrated%2520through%2520extensive%2520experimental%2520evaluations.%2520Our%2520approach%2520enables%2520a%2520deeper%2520understanding%2520of%2520where%2520the%2520models%2520fail%2520with%2520an%2520explanation%253B%2520thus%252C%2520we%2520fill%2520a%2520gap%2520in%2520the%2520field%252C%2520as%2520no%2520prior%2520methods%2520offer%2520such%2520categorization%2520and%2520decomposition%2520for%2520this%2520task.%2520To%2520promote%2520transparency%2520and%2520further%2520exploration%252C%2520we%2520openly%2520release%2520VIGIL%252C%2520along%2520with%2520the%2520detection%2520pipeline%2520and%2520benchmark%2520code%252C%2520through%2520our%2520GitHub%2520repository%253A%2520https%253A//github.com/mlubneuskaya/vigil%2520and%2520Data%2520repository%253A%2520https%253A//huggingface.co/datasets/joannaww/VIGIL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIGIL%3A%20Tackling%20Hallucination%20Detection%20in%20Image%20Recontextualization&entry.906535625=Joanna%20Wojciechowicz%20and%20Maria%20%C5%81ubniewska%20and%20Jakub%20Antczak%20and%20Justyna%20Baczy%C5%84ska%20and%20Wojciech%20Gromski%20and%20Wojciech%20Koz%C5%82owski%20and%20Maciej%20Zi%C4%99ba&entry.1292438233=We%20introduce%20VIGIL%20%28Visual%20Inconsistency%20%26%20Generative%20In-context%20Lucidity%29%2C%20the%20first%20benchmark%20dataset%20and%20framework%20providing%20a%20fine-grained%20categorization%20of%20hallucinations%20in%20the%20multimodal%20image%20recontextualization%20task%20for%20large%20multimodal%20models%20%28LMMs%29.%20While%20existing%20research%20often%20treats%20hallucinations%20as%20a%20uniform%20issue%2C%20our%20work%20addresses%20a%20significant%20gap%20in%20multimodal%20evaluation%20by%20decomposing%20these%20errors%20into%20five%20categories%3A%20pasted%20object%20hallucinations%2C%20background%20hallucinations%2C%20object%20omission%2C%20positional%20%26%20logical%20inconsistencies%2C%20and%20physical%20law%20violations.%20To%20address%20these%20complexities%2C%20we%20propose%20a%20multi-stage%20detection%20pipeline.%20Our%20architecture%20processes%20recontextualized%20images%20through%20a%20series%20of%20specialized%20steps%20targeting%20object-level%20fidelity%2C%20background%20consistency%2C%20and%20omission%20detection%2C%20leveraging%20a%20coordinated%20ensemble%20of%20open-source%20models%2C%20whose%20effectiveness%20is%20demonstrated%20through%20extensive%20experimental%20evaluations.%20Our%20approach%20enables%20a%20deeper%20understanding%20of%20where%20the%20models%20fail%20with%20an%20explanation%3B%20thus%2C%20we%20fill%20a%20gap%20in%20the%20field%2C%20as%20no%20prior%20methods%20offer%20such%20categorization%20and%20decomposition%20for%20this%20task.%20To%20promote%20transparency%20and%20further%20exploration%2C%20we%20openly%20release%20VIGIL%2C%20along%20with%20the%20detection%20pipeline%20and%20benchmark%20code%2C%20through%20our%20GitHub%20repository%3A%20https%3A//github.com/mlubneuskaya/vigil%20and%20Data%20repository%3A%20https%3A//huggingface.co/datasets/joannaww/VIGIL.&entry.1838667208=http%3A//arxiv.org/abs/2602.14633v1&entry.124074799=Read"},
{"title": "MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation", "author": "Penghui Niu and Jiashuai She and Taotao Cai and Yajuan Zhang and Ping Zhang and Junhua Gu and Jianxin Li", "abstract": "Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.", "link": "http://arxiv.org/abs/2511.11681v2", "date": "2026-02-16", "relevancy": 2.1246, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5571}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5242}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MPCM-Net%3A%20Multi-scale%20network%20integrates%20partial%20attention%20convolution%20with%20Mamba%20for%20ground-based%20cloud%20image%20segmentation&body=Title%3A%20MPCM-Net%3A%20Multi-scale%20network%20integrates%20partial%20attention%20convolution%20with%20Mamba%20for%20ground-based%20cloud%20image%20segmentation%0AAuthor%3A%20Penghui%20Niu%20and%20Jiashuai%20She%20and%20Taotao%20Cai%20and%20Yajuan%20Zhang%20and%20Ping%20Zhang%20and%20Junhua%20Gu%20and%20Jianxin%20Li%0AAbstract%3A%20Ground-based%20cloud%20image%20segmentation%20is%20a%20critical%20research%20domain%20for%20photovoltaic%20power%20forecasting.%20Current%20deep%20learning%20approaches%20primarily%20focus%20on%20encoder-decoder%20architectural%20refinements.%20However%2C%20existing%20methodologies%20exhibit%20several%20limitations%3A%281%29they%20rely%20on%20dilated%20convolutions%20for%20multi-scale%20context%20extraction%2C%20lacking%20the%20partial%20feature%20effectiveness%20and%20interoperability%20of%20inter-channel%3B%282%29attention-based%20feature%20enhancement%20implementations%20neglect%20accuracy-throughput%20balance%3B%20and%20%283%29the%20decoder%20modifications%20fail%20to%20establish%20global%20interdependencies%20among%20hierarchical%20local%20features%2C%20limiting%20inference%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20MPCM-Net%2C%20a%20Multi-scale%20network%20that%20integrates%20Partial%20attention%20Convolutions%20with%20Mamba%20architectures%20to%20enhance%20segmentation%20accuracy%20and%20computational%20efficiency.%20Specifically%2C%20the%20encoder%20incorporates%20MPAC%2C%20which%20comprises%3A%281%29a%20MPC%20block%20with%20ParCM%20and%20ParSM%20that%20enables%20global%20spatial%20interaction%20across%20multi-scale%20cloud%20formations%2C%20and%20%282%29a%20MPA%20block%20combining%20ParAM%20and%20ParSM%20to%20extract%20discriminative%20features%20with%20reduced%20computational%20complexity.%20On%20the%20decoder%20side%2C%20a%20M2B%20is%20employed%20to%20mitigate%20contextual%20loss%20through%20a%20SSHD%20that%20maintains%20linear%20complexity%20while%20enabling%20deep%20feature%20aggregation%20across%20spatial%20and%20scale%20dimensions.%20As%20a%20key%20contribution%20to%20the%20community%2C%20we%20also%20introduce%20and%20release%20a%20dataset%20CSRC%2C%20which%20is%20a%20clear-label%2C%20fine-grained%20segmentation%20benchmark%20designed%20to%20overcome%20the%20critical%20limitations%20of%20existing%20public%20datasets.%20Extensive%20experiments%20on%20CSRC%20demonstrate%20the%20superior%20performance%20of%20MPCM-Net%20over%20state-of-the-art%20methods%2C%20achieving%20an%20optimal%20balance%20between%20segmentation%20accuracy%20and%20inference%20speed.%20The%20dataset%20and%20source%20code%20will%20be%20available%20at%20https%3A//github.com/she1110/CSRC.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMPCM-Net%253A%2520Multi-scale%2520network%2520integrates%2520partial%2520attention%2520convolution%2520with%2520Mamba%2520for%2520ground-based%2520cloud%2520image%2520segmentation%26entry.906535625%3DPenghui%2520Niu%2520and%2520Jiashuai%2520She%2520and%2520Taotao%2520Cai%2520and%2520Yajuan%2520Zhang%2520and%2520Ping%2520Zhang%2520and%2520Junhua%2520Gu%2520and%2520Jianxin%2520Li%26entry.1292438233%3DGround-based%2520cloud%2520image%2520segmentation%2520is%2520a%2520critical%2520research%2520domain%2520for%2520photovoltaic%2520power%2520forecasting.%2520Current%2520deep%2520learning%2520approaches%2520primarily%2520focus%2520on%2520encoder-decoder%2520architectural%2520refinements.%2520However%252C%2520existing%2520methodologies%2520exhibit%2520several%2520limitations%253A%25281%2529they%2520rely%2520on%2520dilated%2520convolutions%2520for%2520multi-scale%2520context%2520extraction%252C%2520lacking%2520the%2520partial%2520feature%2520effectiveness%2520and%2520interoperability%2520of%2520inter-channel%253B%25282%2529attention-based%2520feature%2520enhancement%2520implementations%2520neglect%2520accuracy-throughput%2520balance%253B%2520and%2520%25283%2529the%2520decoder%2520modifications%2520fail%2520to%2520establish%2520global%2520interdependencies%2520among%2520hierarchical%2520local%2520features%252C%2520limiting%2520inference%2520efficiency.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MPCM-Net%252C%2520a%2520Multi-scale%2520network%2520that%2520integrates%2520Partial%2520attention%2520Convolutions%2520with%2520Mamba%2520architectures%2520to%2520enhance%2520segmentation%2520accuracy%2520and%2520computational%2520efficiency.%2520Specifically%252C%2520the%2520encoder%2520incorporates%2520MPAC%252C%2520which%2520comprises%253A%25281%2529a%2520MPC%2520block%2520with%2520ParCM%2520and%2520ParSM%2520that%2520enables%2520global%2520spatial%2520interaction%2520across%2520multi-scale%2520cloud%2520formations%252C%2520and%2520%25282%2529a%2520MPA%2520block%2520combining%2520ParAM%2520and%2520ParSM%2520to%2520extract%2520discriminative%2520features%2520with%2520reduced%2520computational%2520complexity.%2520On%2520the%2520decoder%2520side%252C%2520a%2520M2B%2520is%2520employed%2520to%2520mitigate%2520contextual%2520loss%2520through%2520a%2520SSHD%2520that%2520maintains%2520linear%2520complexity%2520while%2520enabling%2520deep%2520feature%2520aggregation%2520across%2520spatial%2520and%2520scale%2520dimensions.%2520As%2520a%2520key%2520contribution%2520to%2520the%2520community%252C%2520we%2520also%2520introduce%2520and%2520release%2520a%2520dataset%2520CSRC%252C%2520which%2520is%2520a%2520clear-label%252C%2520fine-grained%2520segmentation%2520benchmark%2520designed%2520to%2520overcome%2520the%2520critical%2520limitations%2520of%2520existing%2520public%2520datasets.%2520Extensive%2520experiments%2520on%2520CSRC%2520demonstrate%2520the%2520superior%2520performance%2520of%2520MPCM-Net%2520over%2520state-of-the-art%2520methods%252C%2520achieving%2520an%2520optimal%2520balance%2520between%2520segmentation%2520accuracy%2520and%2520inference%2520speed.%2520The%2520dataset%2520and%2520source%2520code%2520will%2520be%2520available%2520at%2520https%253A//github.com/she1110/CSRC.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MPCM-Net%3A%20Multi-scale%20network%20integrates%20partial%20attention%20convolution%20with%20Mamba%20for%20ground-based%20cloud%20image%20segmentation&entry.906535625=Penghui%20Niu%20and%20Jiashuai%20She%20and%20Taotao%20Cai%20and%20Yajuan%20Zhang%20and%20Ping%20Zhang%20and%20Junhua%20Gu%20and%20Jianxin%20Li&entry.1292438233=Ground-based%20cloud%20image%20segmentation%20is%20a%20critical%20research%20domain%20for%20photovoltaic%20power%20forecasting.%20Current%20deep%20learning%20approaches%20primarily%20focus%20on%20encoder-decoder%20architectural%20refinements.%20However%2C%20existing%20methodologies%20exhibit%20several%20limitations%3A%281%29they%20rely%20on%20dilated%20convolutions%20for%20multi-scale%20context%20extraction%2C%20lacking%20the%20partial%20feature%20effectiveness%20and%20interoperability%20of%20inter-channel%3B%282%29attention-based%20feature%20enhancement%20implementations%20neglect%20accuracy-throughput%20balance%3B%20and%20%283%29the%20decoder%20modifications%20fail%20to%20establish%20global%20interdependencies%20among%20hierarchical%20local%20features%2C%20limiting%20inference%20efficiency.%20To%20address%20these%20challenges%2C%20we%20propose%20MPCM-Net%2C%20a%20Multi-scale%20network%20that%20integrates%20Partial%20attention%20Convolutions%20with%20Mamba%20architectures%20to%20enhance%20segmentation%20accuracy%20and%20computational%20efficiency.%20Specifically%2C%20the%20encoder%20incorporates%20MPAC%2C%20which%20comprises%3A%281%29a%20MPC%20block%20with%20ParCM%20and%20ParSM%20that%20enables%20global%20spatial%20interaction%20across%20multi-scale%20cloud%20formations%2C%20and%20%282%29a%20MPA%20block%20combining%20ParAM%20and%20ParSM%20to%20extract%20discriminative%20features%20with%20reduced%20computational%20complexity.%20On%20the%20decoder%20side%2C%20a%20M2B%20is%20employed%20to%20mitigate%20contextual%20loss%20through%20a%20SSHD%20that%20maintains%20linear%20complexity%20while%20enabling%20deep%20feature%20aggregation%20across%20spatial%20and%20scale%20dimensions.%20As%20a%20key%20contribution%20to%20the%20community%2C%20we%20also%20introduce%20and%20release%20a%20dataset%20CSRC%2C%20which%20is%20a%20clear-label%2C%20fine-grained%20segmentation%20benchmark%20designed%20to%20overcome%20the%20critical%20limitations%20of%20existing%20public%20datasets.%20Extensive%20experiments%20on%20CSRC%20demonstrate%20the%20superior%20performance%20of%20MPCM-Net%20over%20state-of-the-art%20methods%2C%20achieving%20an%20optimal%20balance%20between%20segmentation%20accuracy%20and%20inference%20speed.%20The%20dataset%20and%20source%20code%20will%20be%20available%20at%20https%3A//github.com/she1110/CSRC.&entry.1838667208=http%3A//arxiv.org/abs/2511.11681v2&entry.124074799=Read"},
{"title": "Distributed Quantum Gaussian Processes for Multi-Agent Systems", "author": "Meet Gandhi and George P. Kontoudis", "abstract": "Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA's Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.", "link": "http://arxiv.org/abs/2602.15006v1", "date": "2026-02-16", "relevancy": 2.1222, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5458}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5221}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20Quantum%20Gaussian%20Processes%20for%20Multi-Agent%20Systems&body=Title%3A%20Distributed%20Quantum%20Gaussian%20Processes%20for%20Multi-Agent%20Systems%0AAuthor%3A%20Meet%20Gandhi%20and%20George%20P.%20Kontoudis%0AAbstract%3A%20Gaussian%20Processes%20%28GPs%29%20are%20a%20powerful%20tool%20for%20probabilistic%20modeling%2C%20but%20their%20performance%20is%20often%20constrained%20in%20complex%2C%20largescale%20real-world%20domains%20due%20to%20the%20limited%20expressivity%20of%20classical%20kernels.%20Quantum%20computing%20offers%20the%20potential%20to%20overcome%20this%20limitation%20by%20embedding%20data%20into%20exponentially%20large%20Hilbert%20spaces%2C%20capturing%20complex%20correlations%20that%20remain%20inaccessible%20to%20classical%20computing%20approaches.%20In%20this%20paper%2C%20we%20propose%20a%20Distributed%20Quantum%20Gaussian%20Process%20%28DQGP%29%20method%20in%20a%20multiagent%20setting%20to%20enhance%20modeling%20capabilities%20and%20scalability.%20To%20address%20the%20challenging%20non-Euclidean%20optimization%20problem%2C%20we%20develop%20a%20Distributed%20consensus%20Riemannian%20Alternating%20Direction%20Method%20of%20Multipliers%20%28DR-ADMM%29%20algorithm%20that%20aggregates%20local%20agent%20models%20into%20a%20global%20model.%20We%20evaluate%20the%20efficacy%20of%20our%20method%20through%20numerical%20experiments%20conducted%20on%20a%20quantum%20simulator%20in%20classical%20hardware.%20We%20use%20real-world%2C%20non-stationary%20elevation%20datasets%20of%20NASA%27s%20Shuttle%20Radar%20Topography%20Mission%20and%20synthetic%20datasets%20generated%20by%20Quantum%20Gaussian%20Processes.%20Beyond%20modeling%20advantages%2C%20our%20framework%20highlights%20potential%20computational%20speedups%20that%20quantum%20hardware%20may%20provide%2C%20particularly%20in%20Gaussian%20processes%20and%20distributed%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520Quantum%2520Gaussian%2520Processes%2520for%2520Multi-Agent%2520Systems%26entry.906535625%3DMeet%2520Gandhi%2520and%2520George%2520P.%2520Kontoudis%26entry.1292438233%3DGaussian%2520Processes%2520%2528GPs%2529%2520are%2520a%2520powerful%2520tool%2520for%2520probabilistic%2520modeling%252C%2520but%2520their%2520performance%2520is%2520often%2520constrained%2520in%2520complex%252C%2520largescale%2520real-world%2520domains%2520due%2520to%2520the%2520limited%2520expressivity%2520of%2520classical%2520kernels.%2520Quantum%2520computing%2520offers%2520the%2520potential%2520to%2520overcome%2520this%2520limitation%2520by%2520embedding%2520data%2520into%2520exponentially%2520large%2520Hilbert%2520spaces%252C%2520capturing%2520complex%2520correlations%2520that%2520remain%2520inaccessible%2520to%2520classical%2520computing%2520approaches.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Distributed%2520Quantum%2520Gaussian%2520Process%2520%2528DQGP%2529%2520method%2520in%2520a%2520multiagent%2520setting%2520to%2520enhance%2520modeling%2520capabilities%2520and%2520scalability.%2520To%2520address%2520the%2520challenging%2520non-Euclidean%2520optimization%2520problem%252C%2520we%2520develop%2520a%2520Distributed%2520consensus%2520Riemannian%2520Alternating%2520Direction%2520Method%2520of%2520Multipliers%2520%2528DR-ADMM%2529%2520algorithm%2520that%2520aggregates%2520local%2520agent%2520models%2520into%2520a%2520global%2520model.%2520We%2520evaluate%2520the%2520efficacy%2520of%2520our%2520method%2520through%2520numerical%2520experiments%2520conducted%2520on%2520a%2520quantum%2520simulator%2520in%2520classical%2520hardware.%2520We%2520use%2520real-world%252C%2520non-stationary%2520elevation%2520datasets%2520of%2520NASA%2527s%2520Shuttle%2520Radar%2520Topography%2520Mission%2520and%2520synthetic%2520datasets%2520generated%2520by%2520Quantum%2520Gaussian%2520Processes.%2520Beyond%2520modeling%2520advantages%252C%2520our%2520framework%2520highlights%2520potential%2520computational%2520speedups%2520that%2520quantum%2520hardware%2520may%2520provide%252C%2520particularly%2520in%2520Gaussian%2520processes%2520and%2520distributed%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20Quantum%20Gaussian%20Processes%20for%20Multi-Agent%20Systems&entry.906535625=Meet%20Gandhi%20and%20George%20P.%20Kontoudis&entry.1292438233=Gaussian%20Processes%20%28GPs%29%20are%20a%20powerful%20tool%20for%20probabilistic%20modeling%2C%20but%20their%20performance%20is%20often%20constrained%20in%20complex%2C%20largescale%20real-world%20domains%20due%20to%20the%20limited%20expressivity%20of%20classical%20kernels.%20Quantum%20computing%20offers%20the%20potential%20to%20overcome%20this%20limitation%20by%20embedding%20data%20into%20exponentially%20large%20Hilbert%20spaces%2C%20capturing%20complex%20correlations%20that%20remain%20inaccessible%20to%20classical%20computing%20approaches.%20In%20this%20paper%2C%20we%20propose%20a%20Distributed%20Quantum%20Gaussian%20Process%20%28DQGP%29%20method%20in%20a%20multiagent%20setting%20to%20enhance%20modeling%20capabilities%20and%20scalability.%20To%20address%20the%20challenging%20non-Euclidean%20optimization%20problem%2C%20we%20develop%20a%20Distributed%20consensus%20Riemannian%20Alternating%20Direction%20Method%20of%20Multipliers%20%28DR-ADMM%29%20algorithm%20that%20aggregates%20local%20agent%20models%20into%20a%20global%20model.%20We%20evaluate%20the%20efficacy%20of%20our%20method%20through%20numerical%20experiments%20conducted%20on%20a%20quantum%20simulator%20in%20classical%20hardware.%20We%20use%20real-world%2C%20non-stationary%20elevation%20datasets%20of%20NASA%27s%20Shuttle%20Radar%20Topography%20Mission%20and%20synthetic%20datasets%20generated%20by%20Quantum%20Gaussian%20Processes.%20Beyond%20modeling%20advantages%2C%20our%20framework%20highlights%20potential%20computational%20speedups%20that%20quantum%20hardware%20may%20provide%2C%20particularly%20in%20Gaussian%20processes%20and%20distributed%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2602.15006v1&entry.124074799=Read"},
{"title": "VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery", "author": "Gene Yu and Ce Guo and Wayne Luk", "abstract": "Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by evaluating the stability of causal relations across blocked temporal subsets. VCDF requires no modification to base algorithms and can be applied to methods such as VAR-LiNGAM and PCMCI. Experiments on synthetic datasets show that VCDF improves VAR-LiNGAM by approximately 0.08-0.12 in both window and summary F1 scores across diverse data characteristics, with gains most pronounced for moderate-to-long sequences. The framework also benefits from longer sequences, yielding up to 0.18 absolute improvement on time series of length 1000 and above. Evaluations on simulated fMRI data and IT-monitoring scenarios further demonstrate enhanced stability and structural accuracy under realistic noise conditions. VCDF provides an effective reliability layer for time series causal discovery without altering underlying modeling assumptions.", "link": "http://arxiv.org/abs/2410.19412v2", "date": "2026-02-16", "relevancy": 2.1217, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4348}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4191}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCDF%3A%20A%20Validated%20Consensus-Driven%20Framework%20for%20Time%20Series%20Causal%20Discovery&body=Title%3A%20VCDF%3A%20A%20Validated%20Consensus-Driven%20Framework%20for%20Time%20Series%20Causal%20Discovery%0AAuthor%3A%20Gene%20Yu%20and%20Ce%20Guo%20and%20Wayne%20Luk%0AAbstract%3A%20Time%20series%20causal%20discovery%20is%20essential%20for%20understanding%20dynamic%20systems%2C%20yet%20many%20existing%20methods%20remain%20sensitive%20to%20noise%2C%20non-stationarity%2C%20and%20sampling%20variability.%20We%20propose%20the%20Validated%20Consensus-Driven%20Framework%20%28VCDF%29%2C%20a%20simple%20and%20method-agnostic%20layer%20that%20improves%20robustness%20by%20evaluating%20the%20stability%20of%20causal%20relations%20across%20blocked%20temporal%20subsets.%20VCDF%20requires%20no%20modification%20to%20base%20algorithms%20and%20can%20be%20applied%20to%20methods%20such%20as%20VAR-LiNGAM%20and%20PCMCI.%20Experiments%20on%20synthetic%20datasets%20show%20that%20VCDF%20improves%20VAR-LiNGAM%20by%20approximately%200.08-0.12%20in%20both%20window%20and%20summary%20F1%20scores%20across%20diverse%20data%20characteristics%2C%20with%20gains%20most%20pronounced%20for%20moderate-to-long%20sequences.%20The%20framework%20also%20benefits%20from%20longer%20sequences%2C%20yielding%20up%20to%200.18%20absolute%20improvement%20on%20time%20series%20of%20length%201000%20and%20above.%20Evaluations%20on%20simulated%20fMRI%20data%20and%20IT-monitoring%20scenarios%20further%20demonstrate%20enhanced%20stability%20and%20structural%20accuracy%20under%20realistic%20noise%20conditions.%20VCDF%20provides%20an%20effective%20reliability%20layer%20for%20time%20series%20causal%20discovery%20without%20altering%20underlying%20modeling%20assumptions.%0ALink%3A%20http%3A//arxiv.org/abs/2410.19412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCDF%253A%2520A%2520Validated%2520Consensus-Driven%2520Framework%2520for%2520Time%2520Series%2520Causal%2520Discovery%26entry.906535625%3DGene%2520Yu%2520and%2520Ce%2520Guo%2520and%2520Wayne%2520Luk%26entry.1292438233%3DTime%2520series%2520causal%2520discovery%2520is%2520essential%2520for%2520understanding%2520dynamic%2520systems%252C%2520yet%2520many%2520existing%2520methods%2520remain%2520sensitive%2520to%2520noise%252C%2520non-stationarity%252C%2520and%2520sampling%2520variability.%2520We%2520propose%2520the%2520Validated%2520Consensus-Driven%2520Framework%2520%2528VCDF%2529%252C%2520a%2520simple%2520and%2520method-agnostic%2520layer%2520that%2520improves%2520robustness%2520by%2520evaluating%2520the%2520stability%2520of%2520causal%2520relations%2520across%2520blocked%2520temporal%2520subsets.%2520VCDF%2520requires%2520no%2520modification%2520to%2520base%2520algorithms%2520and%2520can%2520be%2520applied%2520to%2520methods%2520such%2520as%2520VAR-LiNGAM%2520and%2520PCMCI.%2520Experiments%2520on%2520synthetic%2520datasets%2520show%2520that%2520VCDF%2520improves%2520VAR-LiNGAM%2520by%2520approximately%25200.08-0.12%2520in%2520both%2520window%2520and%2520summary%2520F1%2520scores%2520across%2520diverse%2520data%2520characteristics%252C%2520with%2520gains%2520most%2520pronounced%2520for%2520moderate-to-long%2520sequences.%2520The%2520framework%2520also%2520benefits%2520from%2520longer%2520sequences%252C%2520yielding%2520up%2520to%25200.18%2520absolute%2520improvement%2520on%2520time%2520series%2520of%2520length%25201000%2520and%2520above.%2520Evaluations%2520on%2520simulated%2520fMRI%2520data%2520and%2520IT-monitoring%2520scenarios%2520further%2520demonstrate%2520enhanced%2520stability%2520and%2520structural%2520accuracy%2520under%2520realistic%2520noise%2520conditions.%2520VCDF%2520provides%2520an%2520effective%2520reliability%2520layer%2520for%2520time%2520series%2520causal%2520discovery%2520without%2520altering%2520underlying%2520modeling%2520assumptions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.19412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCDF%3A%20A%20Validated%20Consensus-Driven%20Framework%20for%20Time%20Series%20Causal%20Discovery&entry.906535625=Gene%20Yu%20and%20Ce%20Guo%20and%20Wayne%20Luk&entry.1292438233=Time%20series%20causal%20discovery%20is%20essential%20for%20understanding%20dynamic%20systems%2C%20yet%20many%20existing%20methods%20remain%20sensitive%20to%20noise%2C%20non-stationarity%2C%20and%20sampling%20variability.%20We%20propose%20the%20Validated%20Consensus-Driven%20Framework%20%28VCDF%29%2C%20a%20simple%20and%20method-agnostic%20layer%20that%20improves%20robustness%20by%20evaluating%20the%20stability%20of%20causal%20relations%20across%20blocked%20temporal%20subsets.%20VCDF%20requires%20no%20modification%20to%20base%20algorithms%20and%20can%20be%20applied%20to%20methods%20such%20as%20VAR-LiNGAM%20and%20PCMCI.%20Experiments%20on%20synthetic%20datasets%20show%20that%20VCDF%20improves%20VAR-LiNGAM%20by%20approximately%200.08-0.12%20in%20both%20window%20and%20summary%20F1%20scores%20across%20diverse%20data%20characteristics%2C%20with%20gains%20most%20pronounced%20for%20moderate-to-long%20sequences.%20The%20framework%20also%20benefits%20from%20longer%20sequences%2C%20yielding%20up%20to%200.18%20absolute%20improvement%20on%20time%20series%20of%20length%201000%20and%20above.%20Evaluations%20on%20simulated%20fMRI%20data%20and%20IT-monitoring%20scenarios%20further%20demonstrate%20enhanced%20stability%20and%20structural%20accuracy%20under%20realistic%20noise%20conditions.%20VCDF%20provides%20an%20effective%20reliability%20layer%20for%20time%20series%20causal%20discovery%20without%20altering%20underlying%20modeling%20assumptions.&entry.1838667208=http%3A//arxiv.org/abs/2410.19412v2&entry.124074799=Read"},
{"title": "CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography", "author": "Qingqing Zhu and Qiao Jin and Tejas S. Mathai and Yin Fang and Zhizheng Wang and Yifan Yang and Maame Sarfo-Gyamfi and Benjamin Hou and Ran Gu and Praveen T. S. Balamuralikrishna and Kenneth C. Wang and Ronald M. Summers and Zhiyong Lu", "abstract": "Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.", "link": "http://arxiv.org/abs/2602.14879v1", "date": "2026-02-16", "relevancy": 2.1162, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5349}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5285}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5272}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT-Bench%3A%20A%20Benchmark%20for%20Multimodal%20Lesion%20Understanding%20in%20Computed%20Tomography&body=Title%3A%20CT-Bench%3A%20A%20Benchmark%20for%20Multimodal%20Lesion%20Understanding%20in%20Computed%20Tomography%0AAuthor%3A%20Qingqing%20Zhu%20and%20Qiao%20Jin%20and%20Tejas%20S.%20Mathai%20and%20Yin%20Fang%20and%20Zhizheng%20Wang%20and%20Yifan%20Yang%20and%20Maame%20Sarfo-Gyamfi%20and%20Benjamin%20Hou%20and%20Ran%20Gu%20and%20Praveen%20T.%20S.%20Balamuralikrishna%20and%20Kenneth%20C.%20Wang%20and%20Ronald%20M.%20Summers%20and%20Zhiyong%20Lu%0AAbstract%3A%20Artificial%20intelligence%20%28AI%29%20can%20automatically%20delineate%20lesions%20on%20computed%20tomography%20%28CT%29%20and%20generate%20radiology%20report%20content%2C%20yet%20progress%20is%20limited%20by%20the%20scarcity%20of%20publicly%20available%20CT%20datasets%20with%20lesion-level%20annotations.%20To%20bridge%20this%20gap%2C%20we%20introduce%20CT-Bench%2C%20a%20first-of-its-kind%20benchmark%20dataset%20comprising%20two%20components%3A%20a%20Lesion%20Image%20and%20Metadata%20Set%20containing%2020%2C335%20lesions%20from%207%2C795%20CT%20studies%20with%20bounding%20boxes%2C%20descriptions%2C%20and%20size%20information%2C%20and%20a%20multitask%20visual%20question%20answering%20benchmark%20with%202%2C850%20QA%20pairs%20covering%20lesion%20localization%2C%20description%2C%20size%20estimation%2C%20and%20attribute%20categorization.%20Hard%20negative%20examples%20are%20included%20to%20reflect%20real-world%20diagnostic%20challenges.%20We%20evaluate%20multiple%20state-of-the-art%20multimodal%20models%2C%20including%20vision-language%20and%20medical%20CLIP%20variants%2C%20by%20comparing%20their%20performance%20to%20radiologist%20assessments%2C%20demonstrating%20the%20value%20of%20CT-Bench%20as%20a%20comprehensive%20benchmark%20for%20lesion%20analysis.%20Moreover%2C%20fine-tuning%20models%20on%20the%20Lesion%20Image%20and%20Metadata%20Set%20yields%20significant%20performance%20gains%20across%20both%20components%2C%20underscoring%20the%20clinical%20utility%20of%20CT-Bench.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT-Bench%253A%2520A%2520Benchmark%2520for%2520Multimodal%2520Lesion%2520Understanding%2520in%2520Computed%2520Tomography%26entry.906535625%3DQingqing%2520Zhu%2520and%2520Qiao%2520Jin%2520and%2520Tejas%2520S.%2520Mathai%2520and%2520Yin%2520Fang%2520and%2520Zhizheng%2520Wang%2520and%2520Yifan%2520Yang%2520and%2520Maame%2520Sarfo-Gyamfi%2520and%2520Benjamin%2520Hou%2520and%2520Ran%2520Gu%2520and%2520Praveen%2520T.%2520S.%2520Balamuralikrishna%2520and%2520Kenneth%2520C.%2520Wang%2520and%2520Ronald%2520M.%2520Summers%2520and%2520Zhiyong%2520Lu%26entry.1292438233%3DArtificial%2520intelligence%2520%2528AI%2529%2520can%2520automatically%2520delineate%2520lesions%2520on%2520computed%2520tomography%2520%2528CT%2529%2520and%2520generate%2520radiology%2520report%2520content%252C%2520yet%2520progress%2520is%2520limited%2520by%2520the%2520scarcity%2520of%2520publicly%2520available%2520CT%2520datasets%2520with%2520lesion-level%2520annotations.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520CT-Bench%252C%2520a%2520first-of-its-kind%2520benchmark%2520dataset%2520comprising%2520two%2520components%253A%2520a%2520Lesion%2520Image%2520and%2520Metadata%2520Set%2520containing%252020%252C335%2520lesions%2520from%25207%252C795%2520CT%2520studies%2520with%2520bounding%2520boxes%252C%2520descriptions%252C%2520and%2520size%2520information%252C%2520and%2520a%2520multitask%2520visual%2520question%2520answering%2520benchmark%2520with%25202%252C850%2520QA%2520pairs%2520covering%2520lesion%2520localization%252C%2520description%252C%2520size%2520estimation%252C%2520and%2520attribute%2520categorization.%2520Hard%2520negative%2520examples%2520are%2520included%2520to%2520reflect%2520real-world%2520diagnostic%2520challenges.%2520We%2520evaluate%2520multiple%2520state-of-the-art%2520multimodal%2520models%252C%2520including%2520vision-language%2520and%2520medical%2520CLIP%2520variants%252C%2520by%2520comparing%2520their%2520performance%2520to%2520radiologist%2520assessments%252C%2520demonstrating%2520the%2520value%2520of%2520CT-Bench%2520as%2520a%2520comprehensive%2520benchmark%2520for%2520lesion%2520analysis.%2520Moreover%252C%2520fine-tuning%2520models%2520on%2520the%2520Lesion%2520Image%2520and%2520Metadata%2520Set%2520yields%2520significant%2520performance%2520gains%2520across%2520both%2520components%252C%2520underscoring%2520the%2520clinical%2520utility%2520of%2520CT-Bench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT-Bench%3A%20A%20Benchmark%20for%20Multimodal%20Lesion%20Understanding%20in%20Computed%20Tomography&entry.906535625=Qingqing%20Zhu%20and%20Qiao%20Jin%20and%20Tejas%20S.%20Mathai%20and%20Yin%20Fang%20and%20Zhizheng%20Wang%20and%20Yifan%20Yang%20and%20Maame%20Sarfo-Gyamfi%20and%20Benjamin%20Hou%20and%20Ran%20Gu%20and%20Praveen%20T.%20S.%20Balamuralikrishna%20and%20Kenneth%20C.%20Wang%20and%20Ronald%20M.%20Summers%20and%20Zhiyong%20Lu&entry.1292438233=Artificial%20intelligence%20%28AI%29%20can%20automatically%20delineate%20lesions%20on%20computed%20tomography%20%28CT%29%20and%20generate%20radiology%20report%20content%2C%20yet%20progress%20is%20limited%20by%20the%20scarcity%20of%20publicly%20available%20CT%20datasets%20with%20lesion-level%20annotations.%20To%20bridge%20this%20gap%2C%20we%20introduce%20CT-Bench%2C%20a%20first-of-its-kind%20benchmark%20dataset%20comprising%20two%20components%3A%20a%20Lesion%20Image%20and%20Metadata%20Set%20containing%2020%2C335%20lesions%20from%207%2C795%20CT%20studies%20with%20bounding%20boxes%2C%20descriptions%2C%20and%20size%20information%2C%20and%20a%20multitask%20visual%20question%20answering%20benchmark%20with%202%2C850%20QA%20pairs%20covering%20lesion%20localization%2C%20description%2C%20size%20estimation%2C%20and%20attribute%20categorization.%20Hard%20negative%20examples%20are%20included%20to%20reflect%20real-world%20diagnostic%20challenges.%20We%20evaluate%20multiple%20state-of-the-art%20multimodal%20models%2C%20including%20vision-language%20and%20medical%20CLIP%20variants%2C%20by%20comparing%20their%20performance%20to%20radiologist%20assessments%2C%20demonstrating%20the%20value%20of%20CT-Bench%20as%20a%20comprehensive%20benchmark%20for%20lesion%20analysis.%20Moreover%2C%20fine-tuning%20models%20on%20the%20Lesion%20Image%20and%20Metadata%20Set%20yields%20significant%20performance%20gains%20across%20both%20components%2C%20underscoring%20the%20clinical%20utility%20of%20CT-Bench.&entry.1838667208=http%3A//arxiv.org/abs/2602.14879v1&entry.124074799=Read"},
{"title": "RF-GPT: Teaching AI to See the Wireless World", "author": "Hang Zou and Yu Tian and Bohao Wang and Lina Bariah and Samson Lasaulce and Chongwen Huang and M\u00e9rouane Debbah", "abstract": "Large language models (LLMs) and multimodal models have become powerful general-purpose reasoning systems. However, radio-frequency (RF) signals, which underpin wireless systems, are still not natively supported by these models. Existing LLM-based approaches for telecom focus mainly on text and structured data, while conventional RF deep-learning models are built separately for specific signal-processing tasks, highlighting a clear gap between RF perception and high-level reasoning. To bridge this gap, we introduce RF-GPT, a radio-frequency language model (RFLM) that utilizes the visual encoders of multimodal LLMs to process and understand RF spectrograms. In this framework, complex in-phase/quadrature (IQ) waveforms are mapped to time-frequency spectrograms and then passed to pretrained visual encoders. The resulting representations are injected as RF tokens into a decoder-only LLM, which generates RF-grounded answers, explanations, and structured outputs. To train RF-GPT, we perform supervised instruction fine-tuning of a pretrained multimodal LLM using a fully synthetic RF corpus. Standards-compliant waveform generators produce wideband scenes for six wireless technologies, from which we derive time-frequency spectrograms, exact configuration metadata, and dense captions. A text-only LLM then converts these captions into RF-grounded instruction-answer pairs, yielding roughly 12,000 RF scenes and 0.625 million instruction examples without any manual labeling. Across benchmarks for wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction, RF-GPT achieves strong multi-task performance, whereas general-purpose VLMs with no RF grounding largely fail.", "link": "http://arxiv.org/abs/2602.14833v1", "date": "2026-02-16", "relevancy": 2.0934, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RF-GPT%3A%20Teaching%20AI%20to%20See%20the%20Wireless%20World&body=Title%3A%20RF-GPT%3A%20Teaching%20AI%20to%20See%20the%20Wireless%20World%0AAuthor%3A%20Hang%20Zou%20and%20Yu%20Tian%20and%20Bohao%20Wang%20and%20Lina%20Bariah%20and%20Samson%20Lasaulce%20and%20Chongwen%20Huang%20and%20M%C3%A9rouane%20Debbah%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20and%20multimodal%20models%20have%20become%20powerful%20general-purpose%20reasoning%20systems.%20However%2C%20radio-frequency%20%28RF%29%20signals%2C%20which%20underpin%20wireless%20systems%2C%20are%20still%20not%20natively%20supported%20by%20these%20models.%20Existing%20LLM-based%20approaches%20for%20telecom%20focus%20mainly%20on%20text%20and%20structured%20data%2C%20while%20conventional%20RF%20deep-learning%20models%20are%20built%20separately%20for%20specific%20signal-processing%20tasks%2C%20highlighting%20a%20clear%20gap%20between%20RF%20perception%20and%20high-level%20reasoning.%20To%20bridge%20this%20gap%2C%20we%20introduce%20RF-GPT%2C%20a%20radio-frequency%20language%20model%20%28RFLM%29%20that%20utilizes%20the%20visual%20encoders%20of%20multimodal%20LLMs%20to%20process%20and%20understand%20RF%20spectrograms.%20In%20this%20framework%2C%20complex%20in-phase/quadrature%20%28IQ%29%20waveforms%20are%20mapped%20to%20time-frequency%20spectrograms%20and%20then%20passed%20to%20pretrained%20visual%20encoders.%20The%20resulting%20representations%20are%20injected%20as%20RF%20tokens%20into%20a%20decoder-only%20LLM%2C%20which%20generates%20RF-grounded%20answers%2C%20explanations%2C%20and%20structured%20outputs.%20To%20train%20RF-GPT%2C%20we%20perform%20supervised%20instruction%20fine-tuning%20of%20a%20pretrained%20multimodal%20LLM%20using%20a%20fully%20synthetic%20RF%20corpus.%20Standards-compliant%20waveform%20generators%20produce%20wideband%20scenes%20for%20six%20wireless%20technologies%2C%20from%20which%20we%20derive%20time-frequency%20spectrograms%2C%20exact%20configuration%20metadata%2C%20and%20dense%20captions.%20A%20text-only%20LLM%20then%20converts%20these%20captions%20into%20RF-grounded%20instruction-answer%20pairs%2C%20yielding%20roughly%2012%2C000%20RF%20scenes%20and%200.625%20million%20instruction%20examples%20without%20any%20manual%20labeling.%20Across%20benchmarks%20for%20wideband%20modulation%20classification%2C%20overlap%20analysis%2C%20wireless-technology%20recognition%2C%20WLAN%20user%20counting%2C%20and%205G%20NR%20information%20extraction%2C%20RF-GPT%20achieves%20strong%20multi-task%20performance%2C%20whereas%20general-purpose%20VLMs%20with%20no%20RF%20grounding%20largely%20fail.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRF-GPT%253A%2520Teaching%2520AI%2520to%2520See%2520the%2520Wireless%2520World%26entry.906535625%3DHang%2520Zou%2520and%2520Yu%2520Tian%2520and%2520Bohao%2520Wang%2520and%2520Lina%2520Bariah%2520and%2520Samson%2520Lasaulce%2520and%2520Chongwen%2520Huang%2520and%2520M%25C3%25A9rouane%2520Debbah%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520and%2520multimodal%2520models%2520have%2520become%2520powerful%2520general-purpose%2520reasoning%2520systems.%2520However%252C%2520radio-frequency%2520%2528RF%2529%2520signals%252C%2520which%2520underpin%2520wireless%2520systems%252C%2520are%2520still%2520not%2520natively%2520supported%2520by%2520these%2520models.%2520Existing%2520LLM-based%2520approaches%2520for%2520telecom%2520focus%2520mainly%2520on%2520text%2520and%2520structured%2520data%252C%2520while%2520conventional%2520RF%2520deep-learning%2520models%2520are%2520built%2520separately%2520for%2520specific%2520signal-processing%2520tasks%252C%2520highlighting%2520a%2520clear%2520gap%2520between%2520RF%2520perception%2520and%2520high-level%2520reasoning.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520RF-GPT%252C%2520a%2520radio-frequency%2520language%2520model%2520%2528RFLM%2529%2520that%2520utilizes%2520the%2520visual%2520encoders%2520of%2520multimodal%2520LLMs%2520to%2520process%2520and%2520understand%2520RF%2520spectrograms.%2520In%2520this%2520framework%252C%2520complex%2520in-phase/quadrature%2520%2528IQ%2529%2520waveforms%2520are%2520mapped%2520to%2520time-frequency%2520spectrograms%2520and%2520then%2520passed%2520to%2520pretrained%2520visual%2520encoders.%2520The%2520resulting%2520representations%2520are%2520injected%2520as%2520RF%2520tokens%2520into%2520a%2520decoder-only%2520LLM%252C%2520which%2520generates%2520RF-grounded%2520answers%252C%2520explanations%252C%2520and%2520structured%2520outputs.%2520To%2520train%2520RF-GPT%252C%2520we%2520perform%2520supervised%2520instruction%2520fine-tuning%2520of%2520a%2520pretrained%2520multimodal%2520LLM%2520using%2520a%2520fully%2520synthetic%2520RF%2520corpus.%2520Standards-compliant%2520waveform%2520generators%2520produce%2520wideband%2520scenes%2520for%2520six%2520wireless%2520technologies%252C%2520from%2520which%2520we%2520derive%2520time-frequency%2520spectrograms%252C%2520exact%2520configuration%2520metadata%252C%2520and%2520dense%2520captions.%2520A%2520text-only%2520LLM%2520then%2520converts%2520these%2520captions%2520into%2520RF-grounded%2520instruction-answer%2520pairs%252C%2520yielding%2520roughly%252012%252C000%2520RF%2520scenes%2520and%25200.625%2520million%2520instruction%2520examples%2520without%2520any%2520manual%2520labeling.%2520Across%2520benchmarks%2520for%2520wideband%2520modulation%2520classification%252C%2520overlap%2520analysis%252C%2520wireless-technology%2520recognition%252C%2520WLAN%2520user%2520counting%252C%2520and%25205G%2520NR%2520information%2520extraction%252C%2520RF-GPT%2520achieves%2520strong%2520multi-task%2520performance%252C%2520whereas%2520general-purpose%2520VLMs%2520with%2520no%2520RF%2520grounding%2520largely%2520fail.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RF-GPT%3A%20Teaching%20AI%20to%20See%20the%20Wireless%20World&entry.906535625=Hang%20Zou%20and%20Yu%20Tian%20and%20Bohao%20Wang%20and%20Lina%20Bariah%20and%20Samson%20Lasaulce%20and%20Chongwen%20Huang%20and%20M%C3%A9rouane%20Debbah&entry.1292438233=Large%20language%20models%20%28LLMs%29%20and%20multimodal%20models%20have%20become%20powerful%20general-purpose%20reasoning%20systems.%20However%2C%20radio-frequency%20%28RF%29%20signals%2C%20which%20underpin%20wireless%20systems%2C%20are%20still%20not%20natively%20supported%20by%20these%20models.%20Existing%20LLM-based%20approaches%20for%20telecom%20focus%20mainly%20on%20text%20and%20structured%20data%2C%20while%20conventional%20RF%20deep-learning%20models%20are%20built%20separately%20for%20specific%20signal-processing%20tasks%2C%20highlighting%20a%20clear%20gap%20between%20RF%20perception%20and%20high-level%20reasoning.%20To%20bridge%20this%20gap%2C%20we%20introduce%20RF-GPT%2C%20a%20radio-frequency%20language%20model%20%28RFLM%29%20that%20utilizes%20the%20visual%20encoders%20of%20multimodal%20LLMs%20to%20process%20and%20understand%20RF%20spectrograms.%20In%20this%20framework%2C%20complex%20in-phase/quadrature%20%28IQ%29%20waveforms%20are%20mapped%20to%20time-frequency%20spectrograms%20and%20then%20passed%20to%20pretrained%20visual%20encoders.%20The%20resulting%20representations%20are%20injected%20as%20RF%20tokens%20into%20a%20decoder-only%20LLM%2C%20which%20generates%20RF-grounded%20answers%2C%20explanations%2C%20and%20structured%20outputs.%20To%20train%20RF-GPT%2C%20we%20perform%20supervised%20instruction%20fine-tuning%20of%20a%20pretrained%20multimodal%20LLM%20using%20a%20fully%20synthetic%20RF%20corpus.%20Standards-compliant%20waveform%20generators%20produce%20wideband%20scenes%20for%20six%20wireless%20technologies%2C%20from%20which%20we%20derive%20time-frequency%20spectrograms%2C%20exact%20configuration%20metadata%2C%20and%20dense%20captions.%20A%20text-only%20LLM%20then%20converts%20these%20captions%20into%20RF-grounded%20instruction-answer%20pairs%2C%20yielding%20roughly%2012%2C000%20RF%20scenes%20and%200.625%20million%20instruction%20examples%20without%20any%20manual%20labeling.%20Across%20benchmarks%20for%20wideband%20modulation%20classification%2C%20overlap%20analysis%2C%20wireless-technology%20recognition%2C%20WLAN%20user%20counting%2C%20and%205G%20NR%20information%20extraction%2C%20RF-GPT%20achieves%20strong%20multi-task%20performance%2C%20whereas%20general-purpose%20VLMs%20with%20no%20RF%20grounding%20largely%20fail.&entry.1838667208=http%3A//arxiv.org/abs/2602.14833v1&entry.124074799=Read"},
{"title": "Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error", "author": "Farzan Farnia and Mohammad Jalali and Azim Ospanov", "abstract": "Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.", "link": "http://arxiv.org/abs/2602.14682v1", "date": "2026-02-16", "relevancy": 2.0817, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5322}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5279}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exposing%20Diversity%20Bias%20in%20Deep%20Generative%20Models%3A%20Statistical%20Origins%20and%20Correction%20of%20Diversity%20Error&body=Title%3A%20Exposing%20Diversity%20Bias%20in%20Deep%20Generative%20Models%3A%20Statistical%20Origins%20and%20Correction%20of%20Diversity%20Error%0AAuthor%3A%20Farzan%20Farnia%20and%20Mohammad%20Jalali%20and%20Azim%20Ospanov%0AAbstract%3A%20Deep%20generative%20models%20have%20achieved%20great%20success%20in%20producing%20high-quality%20samples%2C%20making%20them%20a%20central%20tool%20across%20machine%20learning%20applications.%20Beyond%20sample%20quality%2C%20an%20important%20yet%20less%20systematically%20studied%20question%20is%20whether%20trained%20generative%20models%20faithfully%20capture%20the%20diversity%20of%20the%20underlying%20data%20distribution.%20In%20this%20work%2C%20we%20address%20this%20question%20by%20directly%20comparing%20the%20diversity%20of%20samples%20generated%20by%20state-of-the-art%20models%20with%20that%20of%20test%20samples%20drawn%20from%20the%20target%20data%20distribution%2C%20using%20recently%20proposed%20reference-free%20entropy-based%20diversity%20scores%2C%20Vendi%20and%20RKE.%20Across%20multiple%20benchmark%20datasets%2C%20we%20find%20that%20test%20data%20consistently%20attains%20substantially%20higher%20Vendi%20and%20RKE%20diversity%20scores%20than%20the%20generated%20samples%2C%20suggesting%20a%20systematic%20downward%20diversity%20bias%20in%20modern%20generative%20models.%20To%20understand%20the%20origin%20of%20this%20bias%2C%20we%20analyze%20the%20finite-sample%20behavior%20of%20entropy-based%20diversity%20scores%20and%20show%20that%20their%20expected%20values%20increase%20with%20sample%20size%2C%20implying%20that%20diversity%20estimated%20from%20finite%20training%20sets%20could%20inherently%20underestimate%20the%20diversity%20of%20the%20true%20distribution.%20As%20a%20result%2C%20optimizing%20the%20generators%20to%20minimize%20divergence%20to%20empirical%20data%20distributions%20would%20induce%20a%20loss%20of%20diversity.%20Finally%2C%20we%20discuss%20potential%20diversity-aware%20regularization%20and%20guidance%20strategies%20based%20on%20Vendi%20and%20RKE%20as%20principled%20directions%20for%20mitigating%20this%20bias%2C%20and%20provide%20empirical%20evidence%20suggesting%20their%20potential%20to%20improve%20the%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExposing%2520Diversity%2520Bias%2520in%2520Deep%2520Generative%2520Models%253A%2520Statistical%2520Origins%2520and%2520Correction%2520of%2520Diversity%2520Error%26entry.906535625%3DFarzan%2520Farnia%2520and%2520Mohammad%2520Jalali%2520and%2520Azim%2520Ospanov%26entry.1292438233%3DDeep%2520generative%2520models%2520have%2520achieved%2520great%2520success%2520in%2520producing%2520high-quality%2520samples%252C%2520making%2520them%2520a%2520central%2520tool%2520across%2520machine%2520learning%2520applications.%2520Beyond%2520sample%2520quality%252C%2520an%2520important%2520yet%2520less%2520systematically%2520studied%2520question%2520is%2520whether%2520trained%2520generative%2520models%2520faithfully%2520capture%2520the%2520diversity%2520of%2520the%2520underlying%2520data%2520distribution.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520question%2520by%2520directly%2520comparing%2520the%2520diversity%2520of%2520samples%2520generated%2520by%2520state-of-the-art%2520models%2520with%2520that%2520of%2520test%2520samples%2520drawn%2520from%2520the%2520target%2520data%2520distribution%252C%2520using%2520recently%2520proposed%2520reference-free%2520entropy-based%2520diversity%2520scores%252C%2520Vendi%2520and%2520RKE.%2520Across%2520multiple%2520benchmark%2520datasets%252C%2520we%2520find%2520that%2520test%2520data%2520consistently%2520attains%2520substantially%2520higher%2520Vendi%2520and%2520RKE%2520diversity%2520scores%2520than%2520the%2520generated%2520samples%252C%2520suggesting%2520a%2520systematic%2520downward%2520diversity%2520bias%2520in%2520modern%2520generative%2520models.%2520To%2520understand%2520the%2520origin%2520of%2520this%2520bias%252C%2520we%2520analyze%2520the%2520finite-sample%2520behavior%2520of%2520entropy-based%2520diversity%2520scores%2520and%2520show%2520that%2520their%2520expected%2520values%2520increase%2520with%2520sample%2520size%252C%2520implying%2520that%2520diversity%2520estimated%2520from%2520finite%2520training%2520sets%2520could%2520inherently%2520underestimate%2520the%2520diversity%2520of%2520the%2520true%2520distribution.%2520As%2520a%2520result%252C%2520optimizing%2520the%2520generators%2520to%2520minimize%2520divergence%2520to%2520empirical%2520data%2520distributions%2520would%2520induce%2520a%2520loss%2520of%2520diversity.%2520Finally%252C%2520we%2520discuss%2520potential%2520diversity-aware%2520regularization%2520and%2520guidance%2520strategies%2520based%2520on%2520Vendi%2520and%2520RKE%2520as%2520principled%2520directions%2520for%2520mitigating%2520this%2520bias%252C%2520and%2520provide%2520empirical%2520evidence%2520suggesting%2520their%2520potential%2520to%2520improve%2520the%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exposing%20Diversity%20Bias%20in%20Deep%20Generative%20Models%3A%20Statistical%20Origins%20and%20Correction%20of%20Diversity%20Error&entry.906535625=Farzan%20Farnia%20and%20Mohammad%20Jalali%20and%20Azim%20Ospanov&entry.1292438233=Deep%20generative%20models%20have%20achieved%20great%20success%20in%20producing%20high-quality%20samples%2C%20making%20them%20a%20central%20tool%20across%20machine%20learning%20applications.%20Beyond%20sample%20quality%2C%20an%20important%20yet%20less%20systematically%20studied%20question%20is%20whether%20trained%20generative%20models%20faithfully%20capture%20the%20diversity%20of%20the%20underlying%20data%20distribution.%20In%20this%20work%2C%20we%20address%20this%20question%20by%20directly%20comparing%20the%20diversity%20of%20samples%20generated%20by%20state-of-the-art%20models%20with%20that%20of%20test%20samples%20drawn%20from%20the%20target%20data%20distribution%2C%20using%20recently%20proposed%20reference-free%20entropy-based%20diversity%20scores%2C%20Vendi%20and%20RKE.%20Across%20multiple%20benchmark%20datasets%2C%20we%20find%20that%20test%20data%20consistently%20attains%20substantially%20higher%20Vendi%20and%20RKE%20diversity%20scores%20than%20the%20generated%20samples%2C%20suggesting%20a%20systematic%20downward%20diversity%20bias%20in%20modern%20generative%20models.%20To%20understand%20the%20origin%20of%20this%20bias%2C%20we%20analyze%20the%20finite-sample%20behavior%20of%20entropy-based%20diversity%20scores%20and%20show%20that%20their%20expected%20values%20increase%20with%20sample%20size%2C%20implying%20that%20diversity%20estimated%20from%20finite%20training%20sets%20could%20inherently%20underestimate%20the%20diversity%20of%20the%20true%20distribution.%20As%20a%20result%2C%20optimizing%20the%20generators%20to%20minimize%20divergence%20to%20empirical%20data%20distributions%20would%20induce%20a%20loss%20of%20diversity.%20Finally%2C%20we%20discuss%20potential%20diversity-aware%20regularization%20and%20guidance%20strategies%20based%20on%20Vendi%20and%20RKE%20as%20principled%20directions%20for%20mitigating%20this%20bias%2C%20and%20provide%20empirical%20evidence%20suggesting%20their%20potential%20to%20improve%20the%20results.&entry.1838667208=http%3A//arxiv.org/abs/2602.14682v1&entry.124074799=Read"},
{"title": "Neurosim: A Fast Simulator for Neuromorphic Robot Perception", "author": "Richeek Das and Pratik Chaudhari", "abstract": "Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .", "link": "http://arxiv.org/abs/2602.15018v1", "date": "2026-02-16", "relevancy": 2.0565, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5529}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5075}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neurosim%3A%20A%20Fast%20Simulator%20for%20Neuromorphic%20Robot%20Perception&body=Title%3A%20Neurosim%3A%20A%20Fast%20Simulator%20for%20Neuromorphic%20Robot%20Perception%0AAuthor%3A%20Richeek%20Das%20and%20Pratik%20Chaudhari%0AAbstract%3A%20Neurosim%20is%20a%20fast%2C%20real-time%2C%20high-performance%20library%20for%20simulating%20sensors%20such%20as%20dynamic%20vision%20sensors%2C%20RGB%20cameras%2C%20depth%20sensors%2C%20and%20inertial%20sensors.%20It%20can%20also%20simulate%20agile%20dynamics%20of%20multi-rotor%20vehicles%20in%20complex%20and%20dynamic%20environments.%20Neurosim%20can%20achieve%20frame%20rates%20as%20high%20as%20~2700%20FPS%20on%20a%20desktop%20GPU.%20Neurosim%20integrates%20with%20a%20ZeroMQ-based%20communication%20library%20called%20Cortex%20to%20facilitate%20seamless%20integration%20with%20machine%20learning%20and%20robotics%20workflows.%20Cortex%20provides%20a%20high-throughput%2C%20low-latency%20message-passing%20system%20for%20Python%20and%20C%2B%2B%20applications%2C%20with%20native%20support%20for%20NumPy%20arrays%20and%20PyTorch%20tensors.%20This%20paper%20discusses%20the%20design%20philosophy%20behind%20Neurosim%20and%20Cortex.%20It%20demonstrates%20how%20they%20can%20be%20used%20to%20%28i%29%20train%20neuromorphic%20perception%20and%20control%20algorithms%2C%20e.g.%2C%20using%20self-supervised%20learning%20on%20time-synchronized%20multi-modal%20data%2C%20and%20%28ii%29%20test%20real-time%20implementations%20of%20these%20algorithms%20in%20closed-loop.%20Neurosim%20and%20Cortex%20are%20available%20at%20https%3A//github.com/grasp-lyrl/neurosim%20.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeurosim%253A%2520A%2520Fast%2520Simulator%2520for%2520Neuromorphic%2520Robot%2520Perception%26entry.906535625%3DRicheek%2520Das%2520and%2520Pratik%2520Chaudhari%26entry.1292438233%3DNeurosim%2520is%2520a%2520fast%252C%2520real-time%252C%2520high-performance%2520library%2520for%2520simulating%2520sensors%2520such%2520as%2520dynamic%2520vision%2520sensors%252C%2520RGB%2520cameras%252C%2520depth%2520sensors%252C%2520and%2520inertial%2520sensors.%2520It%2520can%2520also%2520simulate%2520agile%2520dynamics%2520of%2520multi-rotor%2520vehicles%2520in%2520complex%2520and%2520dynamic%2520environments.%2520Neurosim%2520can%2520achieve%2520frame%2520rates%2520as%2520high%2520as%2520~2700%2520FPS%2520on%2520a%2520desktop%2520GPU.%2520Neurosim%2520integrates%2520with%2520a%2520ZeroMQ-based%2520communication%2520library%2520called%2520Cortex%2520to%2520facilitate%2520seamless%2520integration%2520with%2520machine%2520learning%2520and%2520robotics%2520workflows.%2520Cortex%2520provides%2520a%2520high-throughput%252C%2520low-latency%2520message-passing%2520system%2520for%2520Python%2520and%2520C%252B%252B%2520applications%252C%2520with%2520native%2520support%2520for%2520NumPy%2520arrays%2520and%2520PyTorch%2520tensors.%2520This%2520paper%2520discusses%2520the%2520design%2520philosophy%2520behind%2520Neurosim%2520and%2520Cortex.%2520It%2520demonstrates%2520how%2520they%2520can%2520be%2520used%2520to%2520%2528i%2529%2520train%2520neuromorphic%2520perception%2520and%2520control%2520algorithms%252C%2520e.g.%252C%2520using%2520self-supervised%2520learning%2520on%2520time-synchronized%2520multi-modal%2520data%252C%2520and%2520%2528ii%2529%2520test%2520real-time%2520implementations%2520of%2520these%2520algorithms%2520in%2520closed-loop.%2520Neurosim%2520and%2520Cortex%2520are%2520available%2520at%2520https%253A//github.com/grasp-lyrl/neurosim%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neurosim%3A%20A%20Fast%20Simulator%20for%20Neuromorphic%20Robot%20Perception&entry.906535625=Richeek%20Das%20and%20Pratik%20Chaudhari&entry.1292438233=Neurosim%20is%20a%20fast%2C%20real-time%2C%20high-performance%20library%20for%20simulating%20sensors%20such%20as%20dynamic%20vision%20sensors%2C%20RGB%20cameras%2C%20depth%20sensors%2C%20and%20inertial%20sensors.%20It%20can%20also%20simulate%20agile%20dynamics%20of%20multi-rotor%20vehicles%20in%20complex%20and%20dynamic%20environments.%20Neurosim%20can%20achieve%20frame%20rates%20as%20high%20as%20~2700%20FPS%20on%20a%20desktop%20GPU.%20Neurosim%20integrates%20with%20a%20ZeroMQ-based%20communication%20library%20called%20Cortex%20to%20facilitate%20seamless%20integration%20with%20machine%20learning%20and%20robotics%20workflows.%20Cortex%20provides%20a%20high-throughput%2C%20low-latency%20message-passing%20system%20for%20Python%20and%20C%2B%2B%20applications%2C%20with%20native%20support%20for%20NumPy%20arrays%20and%20PyTorch%20tensors.%20This%20paper%20discusses%20the%20design%20philosophy%20behind%20Neurosim%20and%20Cortex.%20It%20demonstrates%20how%20they%20can%20be%20used%20to%20%28i%29%20train%20neuromorphic%20perception%20and%20control%20algorithms%2C%20e.g.%2C%20using%20self-supervised%20learning%20on%20time-synchronized%20multi-modal%20data%2C%20and%20%28ii%29%20test%20real-time%20implementations%20of%20these%20algorithms%20in%20closed-loop.%20Neurosim%20and%20Cortex%20are%20available%20at%20https%3A//github.com/grasp-lyrl/neurosim%20.&entry.1838667208=http%3A//arxiv.org/abs/2602.15018v1&entry.124074799=Read"},
{"title": "Debiasing Central Fixation Confounds Reveals a Peripheral \"Sweet Spot\" for Human-like Scanpaths in Hard-Attention Vision", "author": "Pengcheng Pan and Yonekura Shogo and Yasuo Kuniyosh", "abstract": "Human eye movements in visual recognition reflect a balance between foveal sampling and peripheral context. Task-driven hard-attention models for vision are often evaluated by how well their scanpaths match human gaze. However, common scanpath metrics can be strongly confounded by dataset-specific center bias, especially on object-centric datasets. Using Gaze-CIFAR-10, we show that a trivial center-fixation baseline achieves surprisingly strong scanpath scores, approaching many learned policies. This makes standard metrics optimistic and blurs the distinction between genuine behavioral alignment and mere central tendency. We then analyze a hard-attention classifier under constrained vision by sweeping foveal patch size and peripheral context, revealing a peripheral sweet spot: only a narrow range of sensory constraints yields scanpaths that are simultaneously (i) above the center baseline after debiasing and (ii) temporally human-like in movement statistics. To address center bias, we propose GCS (Gaze Consistency Score), a center-debiased composite metric augmented with movement similarity. GCS uncovers a robust sweet spot at medium patch size with both foveal and peripheral vision, that is not obvious from raw scanpath metrics or accuracy alone, and also highlights a \"shortcut regime\" when the field-of-view becomes too large. We discuss implications for evaluating active perception on object-centric datasets and for designing gaze benchmarks that better separate behavioral alignment from center bias.", "link": "http://arxiv.org/abs/2602.14834v1", "date": "2026-02-16", "relevancy": 2.0439, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Debiasing%20Central%20Fixation%20Confounds%20Reveals%20a%20Peripheral%20%22Sweet%20Spot%22%20for%20Human-like%20Scanpaths%20in%20Hard-Attention%20Vision&body=Title%3A%20Debiasing%20Central%20Fixation%20Confounds%20Reveals%20a%20Peripheral%20%22Sweet%20Spot%22%20for%20Human-like%20Scanpaths%20in%20Hard-Attention%20Vision%0AAuthor%3A%20Pengcheng%20Pan%20and%20Yonekura%20Shogo%20and%20Yasuo%20Kuniyosh%0AAbstract%3A%20Human%20eye%20movements%20in%20visual%20recognition%20reflect%20a%20balance%20between%20foveal%20sampling%20and%20peripheral%20context.%20Task-driven%20hard-attention%20models%20for%20vision%20are%20often%20evaluated%20by%20how%20well%20their%20scanpaths%20match%20human%20gaze.%20However%2C%20common%20scanpath%20metrics%20can%20be%20strongly%20confounded%20by%20dataset-specific%20center%20bias%2C%20especially%20on%20object-centric%20datasets.%20Using%20Gaze-CIFAR-10%2C%20we%20show%20that%20a%20trivial%20center-fixation%20baseline%20achieves%20surprisingly%20strong%20scanpath%20scores%2C%20approaching%20many%20learned%20policies.%20This%20makes%20standard%20metrics%20optimistic%20and%20blurs%20the%20distinction%20between%20genuine%20behavioral%20alignment%20and%20mere%20central%20tendency.%20We%20then%20analyze%20a%20hard-attention%20classifier%20under%20constrained%20vision%20by%20sweeping%20foveal%20patch%20size%20and%20peripheral%20context%2C%20revealing%20a%20peripheral%20sweet%20spot%3A%20only%20a%20narrow%20range%20of%20sensory%20constraints%20yields%20scanpaths%20that%20are%20simultaneously%20%28i%29%20above%20the%20center%20baseline%20after%20debiasing%20and%20%28ii%29%20temporally%20human-like%20in%20movement%20statistics.%20To%20address%20center%20bias%2C%20we%20propose%20GCS%20%28Gaze%20Consistency%20Score%29%2C%20a%20center-debiased%20composite%20metric%20augmented%20with%20movement%20similarity.%20GCS%20uncovers%20a%20robust%20sweet%20spot%20at%20medium%20patch%20size%20with%20both%20foveal%20and%20peripheral%20vision%2C%20that%20is%20not%20obvious%20from%20raw%20scanpath%20metrics%20or%20accuracy%20alone%2C%20and%20also%20highlights%20a%20%22shortcut%20regime%22%20when%20the%20field-of-view%20becomes%20too%20large.%20We%20discuss%20implications%20for%20evaluating%20active%20perception%20on%20object-centric%20datasets%20and%20for%20designing%20gaze%20benchmarks%20that%20better%20separate%20behavioral%20alignment%20from%20center%20bias.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDebiasing%2520Central%2520Fixation%2520Confounds%2520Reveals%2520a%2520Peripheral%2520%2522Sweet%2520Spot%2522%2520for%2520Human-like%2520Scanpaths%2520in%2520Hard-Attention%2520Vision%26entry.906535625%3DPengcheng%2520Pan%2520and%2520Yonekura%2520Shogo%2520and%2520Yasuo%2520Kuniyosh%26entry.1292438233%3DHuman%2520eye%2520movements%2520in%2520visual%2520recognition%2520reflect%2520a%2520balance%2520between%2520foveal%2520sampling%2520and%2520peripheral%2520context.%2520Task-driven%2520hard-attention%2520models%2520for%2520vision%2520are%2520often%2520evaluated%2520by%2520how%2520well%2520their%2520scanpaths%2520match%2520human%2520gaze.%2520However%252C%2520common%2520scanpath%2520metrics%2520can%2520be%2520strongly%2520confounded%2520by%2520dataset-specific%2520center%2520bias%252C%2520especially%2520on%2520object-centric%2520datasets.%2520Using%2520Gaze-CIFAR-10%252C%2520we%2520show%2520that%2520a%2520trivial%2520center-fixation%2520baseline%2520achieves%2520surprisingly%2520strong%2520scanpath%2520scores%252C%2520approaching%2520many%2520learned%2520policies.%2520This%2520makes%2520standard%2520metrics%2520optimistic%2520and%2520blurs%2520the%2520distinction%2520between%2520genuine%2520behavioral%2520alignment%2520and%2520mere%2520central%2520tendency.%2520We%2520then%2520analyze%2520a%2520hard-attention%2520classifier%2520under%2520constrained%2520vision%2520by%2520sweeping%2520foveal%2520patch%2520size%2520and%2520peripheral%2520context%252C%2520revealing%2520a%2520peripheral%2520sweet%2520spot%253A%2520only%2520a%2520narrow%2520range%2520of%2520sensory%2520constraints%2520yields%2520scanpaths%2520that%2520are%2520simultaneously%2520%2528i%2529%2520above%2520the%2520center%2520baseline%2520after%2520debiasing%2520and%2520%2528ii%2529%2520temporally%2520human-like%2520in%2520movement%2520statistics.%2520To%2520address%2520center%2520bias%252C%2520we%2520propose%2520GCS%2520%2528Gaze%2520Consistency%2520Score%2529%252C%2520a%2520center-debiased%2520composite%2520metric%2520augmented%2520with%2520movement%2520similarity.%2520GCS%2520uncovers%2520a%2520robust%2520sweet%2520spot%2520at%2520medium%2520patch%2520size%2520with%2520both%2520foveal%2520and%2520peripheral%2520vision%252C%2520that%2520is%2520not%2520obvious%2520from%2520raw%2520scanpath%2520metrics%2520or%2520accuracy%2520alone%252C%2520and%2520also%2520highlights%2520a%2520%2522shortcut%2520regime%2522%2520when%2520the%2520field-of-view%2520becomes%2520too%2520large.%2520We%2520discuss%2520implications%2520for%2520evaluating%2520active%2520perception%2520on%2520object-centric%2520datasets%2520and%2520for%2520designing%2520gaze%2520benchmarks%2520that%2520better%2520separate%2520behavioral%2520alignment%2520from%2520center%2520bias.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debiasing%20Central%20Fixation%20Confounds%20Reveals%20a%20Peripheral%20%22Sweet%20Spot%22%20for%20Human-like%20Scanpaths%20in%20Hard-Attention%20Vision&entry.906535625=Pengcheng%20Pan%20and%20Yonekura%20Shogo%20and%20Yasuo%20Kuniyosh&entry.1292438233=Human%20eye%20movements%20in%20visual%20recognition%20reflect%20a%20balance%20between%20foveal%20sampling%20and%20peripheral%20context.%20Task-driven%20hard-attention%20models%20for%20vision%20are%20often%20evaluated%20by%20how%20well%20their%20scanpaths%20match%20human%20gaze.%20However%2C%20common%20scanpath%20metrics%20can%20be%20strongly%20confounded%20by%20dataset-specific%20center%20bias%2C%20especially%20on%20object-centric%20datasets.%20Using%20Gaze-CIFAR-10%2C%20we%20show%20that%20a%20trivial%20center-fixation%20baseline%20achieves%20surprisingly%20strong%20scanpath%20scores%2C%20approaching%20many%20learned%20policies.%20This%20makes%20standard%20metrics%20optimistic%20and%20blurs%20the%20distinction%20between%20genuine%20behavioral%20alignment%20and%20mere%20central%20tendency.%20We%20then%20analyze%20a%20hard-attention%20classifier%20under%20constrained%20vision%20by%20sweeping%20foveal%20patch%20size%20and%20peripheral%20context%2C%20revealing%20a%20peripheral%20sweet%20spot%3A%20only%20a%20narrow%20range%20of%20sensory%20constraints%20yields%20scanpaths%20that%20are%20simultaneously%20%28i%29%20above%20the%20center%20baseline%20after%20debiasing%20and%20%28ii%29%20temporally%20human-like%20in%20movement%20statistics.%20To%20address%20center%20bias%2C%20we%20propose%20GCS%20%28Gaze%20Consistency%20Score%29%2C%20a%20center-debiased%20composite%20metric%20augmented%20with%20movement%20similarity.%20GCS%20uncovers%20a%20robust%20sweet%20spot%20at%20medium%20patch%20size%20with%20both%20foveal%20and%20peripheral%20vision%2C%20that%20is%20not%20obvious%20from%20raw%20scanpath%20metrics%20or%20accuracy%20alone%2C%20and%20also%20highlights%20a%20%22shortcut%20regime%22%20when%20the%20field-of-view%20becomes%20too%20large.%20We%20discuss%20implications%20for%20evaluating%20active%20perception%20on%20object-centric%20datasets%20and%20for%20designing%20gaze%20benchmarks%20that%20better%20separate%20behavioral%20alignment%20from%20center%20bias.&entry.1838667208=http%3A//arxiv.org/abs/2602.14834v1&entry.124074799=Read"},
{"title": "Evolution Strategies at the Hyperscale", "author": "Bidipta Sarkar and Mattie Fellows and Juan Agustin Duque and Alistair Letcher and Antonio Le\u00f3n Villares and Anya Sims and Clarisse Wibault and Dmitry Samsonov and Dylan Cope and Jarek Liesen and Kang Li and Lukas Seier and Theo Wolf and Uljad Berdica and Valentin Mohl and Alexander David Goldie and Aaron Courville and Karin Sevegnani and Shimon Whiteson and Jakob Nicolaus Foerster", "abstract": "Evolution Strategies (ES) is a class of powerful black-box optimisation methods that are highly parallelisable and can handle non-differentiable and noisy objectives. However, na\u00efve ES becomes prohibitively expensive at scale on GPUs due to the low arithmetic intensity of batched matrix multiplications with unstructured random perturbations. We introduce Evolution Guided GeneRal Optimisation via Low-rank Learning (EGGROLL), which improves arithmetic intensity by structuring individual perturbations as rank-$r$ matrices, resulting in a hundredfold increase in training speed for billion-parameter models at large population sizes, achieving up to 91% of the throughput of pure batch inference. We provide a rigorous theoretical analysis of Gaussian ES for high-dimensional parameter objectives, investigating conditions needed for ES updates to converge in high dimensions. Our results reveal a linearising effect, and proving consistency between EGGROLL and ES as parameter dimension increases. Our experiments show that EGGROLL: (1) enables the stable pretraining of nonlinear recurrent language models that operate purely in integer datatypes, (2) is competitive with GRPO for post-training LLMs on reasoning tasks, and (3) does not compromise performance compared to ES in tabula rasa RL settings, despite being faster.", "link": "http://arxiv.org/abs/2511.16652v2", "date": "2026-02-16", "relevancy": 2.0434, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5147}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5134}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolution%20Strategies%20at%20the%20Hyperscale&body=Title%3A%20Evolution%20Strategies%20at%20the%20Hyperscale%0AAuthor%3A%20Bidipta%20Sarkar%20and%20Mattie%20Fellows%20and%20Juan%20Agustin%20Duque%20and%20Alistair%20Letcher%20and%20Antonio%20Le%C3%B3n%20Villares%20and%20Anya%20Sims%20and%20Clarisse%20Wibault%20and%20Dmitry%20Samsonov%20and%20Dylan%20Cope%20and%20Jarek%20Liesen%20and%20Kang%20Li%20and%20Lukas%20Seier%20and%20Theo%20Wolf%20and%20Uljad%20Berdica%20and%20Valentin%20Mohl%20and%20Alexander%20David%20Goldie%20and%20Aaron%20Courville%20and%20Karin%20Sevegnani%20and%20Shimon%20Whiteson%20and%20Jakob%20Nicolaus%20Foerster%0AAbstract%3A%20Evolution%20Strategies%20%28ES%29%20is%20a%20class%20of%20powerful%20black-box%20optimisation%20methods%20that%20are%20highly%20parallelisable%20and%20can%20handle%20non-differentiable%20and%20noisy%20objectives.%20However%2C%20na%C3%AFve%20ES%20becomes%20prohibitively%20expensive%20at%20scale%20on%20GPUs%20due%20to%20the%20low%20arithmetic%20intensity%20of%20batched%20matrix%20multiplications%20with%20unstructured%20random%20perturbations.%20We%20introduce%20Evolution%20Guided%20GeneRal%20Optimisation%20via%20Low-rank%20Learning%20%28EGGROLL%29%2C%20which%20improves%20arithmetic%20intensity%20by%20structuring%20individual%20perturbations%20as%20rank-%24r%24%20matrices%2C%20resulting%20in%20a%20hundredfold%20increase%20in%20training%20speed%20for%20billion-parameter%20models%20at%20large%20population%20sizes%2C%20achieving%20up%20to%2091%25%20of%20the%20throughput%20of%20pure%20batch%20inference.%20We%20provide%20a%20rigorous%20theoretical%20analysis%20of%20Gaussian%20ES%20for%20high-dimensional%20parameter%20objectives%2C%20investigating%20conditions%20needed%20for%20ES%20updates%20to%20converge%20in%20high%20dimensions.%20Our%20results%20reveal%20a%20linearising%20effect%2C%20and%20proving%20consistency%20between%20EGGROLL%20and%20ES%20as%20parameter%20dimension%20increases.%20Our%20experiments%20show%20that%20EGGROLL%3A%20%281%29%20enables%20the%20stable%20pretraining%20of%20nonlinear%20recurrent%20language%20models%20that%20operate%20purely%20in%20integer%20datatypes%2C%20%282%29%20is%20competitive%20with%20GRPO%20for%20post-training%20LLMs%20on%20reasoning%20tasks%2C%20and%20%283%29%20does%20not%20compromise%20performance%20compared%20to%20ES%20in%20tabula%20rasa%20RL%20settings%2C%20despite%20being%20faster.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16652v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolution%2520Strategies%2520at%2520the%2520Hyperscale%26entry.906535625%3DBidipta%2520Sarkar%2520and%2520Mattie%2520Fellows%2520and%2520Juan%2520Agustin%2520Duque%2520and%2520Alistair%2520Letcher%2520and%2520Antonio%2520Le%25C3%25B3n%2520Villares%2520and%2520Anya%2520Sims%2520and%2520Clarisse%2520Wibault%2520and%2520Dmitry%2520Samsonov%2520and%2520Dylan%2520Cope%2520and%2520Jarek%2520Liesen%2520and%2520Kang%2520Li%2520and%2520Lukas%2520Seier%2520and%2520Theo%2520Wolf%2520and%2520Uljad%2520Berdica%2520and%2520Valentin%2520Mohl%2520and%2520Alexander%2520David%2520Goldie%2520and%2520Aaron%2520Courville%2520and%2520Karin%2520Sevegnani%2520and%2520Shimon%2520Whiteson%2520and%2520Jakob%2520Nicolaus%2520Foerster%26entry.1292438233%3DEvolution%2520Strategies%2520%2528ES%2529%2520is%2520a%2520class%2520of%2520powerful%2520black-box%2520optimisation%2520methods%2520that%2520are%2520highly%2520parallelisable%2520and%2520can%2520handle%2520non-differentiable%2520and%2520noisy%2520objectives.%2520However%252C%2520na%25C3%25AFve%2520ES%2520becomes%2520prohibitively%2520expensive%2520at%2520scale%2520on%2520GPUs%2520due%2520to%2520the%2520low%2520arithmetic%2520intensity%2520of%2520batched%2520matrix%2520multiplications%2520with%2520unstructured%2520random%2520perturbations.%2520We%2520introduce%2520Evolution%2520Guided%2520GeneRal%2520Optimisation%2520via%2520Low-rank%2520Learning%2520%2528EGGROLL%2529%252C%2520which%2520improves%2520arithmetic%2520intensity%2520by%2520structuring%2520individual%2520perturbations%2520as%2520rank-%2524r%2524%2520matrices%252C%2520resulting%2520in%2520a%2520hundredfold%2520increase%2520in%2520training%2520speed%2520for%2520billion-parameter%2520models%2520at%2520large%2520population%2520sizes%252C%2520achieving%2520up%2520to%252091%2525%2520of%2520the%2520throughput%2520of%2520pure%2520batch%2520inference.%2520We%2520provide%2520a%2520rigorous%2520theoretical%2520analysis%2520of%2520Gaussian%2520ES%2520for%2520high-dimensional%2520parameter%2520objectives%252C%2520investigating%2520conditions%2520needed%2520for%2520ES%2520updates%2520to%2520converge%2520in%2520high%2520dimensions.%2520Our%2520results%2520reveal%2520a%2520linearising%2520effect%252C%2520and%2520proving%2520consistency%2520between%2520EGGROLL%2520and%2520ES%2520as%2520parameter%2520dimension%2520increases.%2520Our%2520experiments%2520show%2520that%2520EGGROLL%253A%2520%25281%2529%2520enables%2520the%2520stable%2520pretraining%2520of%2520nonlinear%2520recurrent%2520language%2520models%2520that%2520operate%2520purely%2520in%2520integer%2520datatypes%252C%2520%25282%2529%2520is%2520competitive%2520with%2520GRPO%2520for%2520post-training%2520LLMs%2520on%2520reasoning%2520tasks%252C%2520and%2520%25283%2529%2520does%2520not%2520compromise%2520performance%2520compared%2520to%2520ES%2520in%2520tabula%2520rasa%2520RL%2520settings%252C%2520despite%2520being%2520faster.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16652v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolution%20Strategies%20at%20the%20Hyperscale&entry.906535625=Bidipta%20Sarkar%20and%20Mattie%20Fellows%20and%20Juan%20Agustin%20Duque%20and%20Alistair%20Letcher%20and%20Antonio%20Le%C3%B3n%20Villares%20and%20Anya%20Sims%20and%20Clarisse%20Wibault%20and%20Dmitry%20Samsonov%20and%20Dylan%20Cope%20and%20Jarek%20Liesen%20and%20Kang%20Li%20and%20Lukas%20Seier%20and%20Theo%20Wolf%20and%20Uljad%20Berdica%20and%20Valentin%20Mohl%20and%20Alexander%20David%20Goldie%20and%20Aaron%20Courville%20and%20Karin%20Sevegnani%20and%20Shimon%20Whiteson%20and%20Jakob%20Nicolaus%20Foerster&entry.1292438233=Evolution%20Strategies%20%28ES%29%20is%20a%20class%20of%20powerful%20black-box%20optimisation%20methods%20that%20are%20highly%20parallelisable%20and%20can%20handle%20non-differentiable%20and%20noisy%20objectives.%20However%2C%20na%C3%AFve%20ES%20becomes%20prohibitively%20expensive%20at%20scale%20on%20GPUs%20due%20to%20the%20low%20arithmetic%20intensity%20of%20batched%20matrix%20multiplications%20with%20unstructured%20random%20perturbations.%20We%20introduce%20Evolution%20Guided%20GeneRal%20Optimisation%20via%20Low-rank%20Learning%20%28EGGROLL%29%2C%20which%20improves%20arithmetic%20intensity%20by%20structuring%20individual%20perturbations%20as%20rank-%24r%24%20matrices%2C%20resulting%20in%20a%20hundredfold%20increase%20in%20training%20speed%20for%20billion-parameter%20models%20at%20large%20population%20sizes%2C%20achieving%20up%20to%2091%25%20of%20the%20throughput%20of%20pure%20batch%20inference.%20We%20provide%20a%20rigorous%20theoretical%20analysis%20of%20Gaussian%20ES%20for%20high-dimensional%20parameter%20objectives%2C%20investigating%20conditions%20needed%20for%20ES%20updates%20to%20converge%20in%20high%20dimensions.%20Our%20results%20reveal%20a%20linearising%20effect%2C%20and%20proving%20consistency%20between%20EGGROLL%20and%20ES%20as%20parameter%20dimension%20increases.%20Our%20experiments%20show%20that%20EGGROLL%3A%20%281%29%20enables%20the%20stable%20pretraining%20of%20nonlinear%20recurrent%20language%20models%20that%20operate%20purely%20in%20integer%20datatypes%2C%20%282%29%20is%20competitive%20with%20GRPO%20for%20post-training%20LLMs%20on%20reasoning%20tasks%2C%20and%20%283%29%20does%20not%20compromise%20performance%20compared%20to%20ES%20in%20tabula%20rasa%20RL%20settings%2C%20despite%20being%20faster.&entry.1838667208=http%3A//arxiv.org/abs/2511.16652v2&entry.124074799=Read"},
{"title": "Drift-Diffusion Matching: Embedding dynamics in latent manifolds of asymmetric neural networks", "author": "Ram\u00f3n Nartallo-Kaluarachchi and Renaud Lambiotte and Alain Goriely", "abstract": "Recurrent neural networks (RNNs) provide a theoretical framework for understanding computation in biological neural circuits, yet classical results, such as Hopfield's model of associative memory, rely on symmetric connectivity that restricts network dynamics to gradient-like flows. In contrast, biological networks support rich time-dependent behaviour facilitated by their asymmetry. Here we introduce a general framework, which we term drift-diffusion matching, for training continuous-time RNNs to represent arbitrary stochastic dynamical systems within a low-dimensional latent subspace. Allowing asymmetric connectivity, we show that RNNs can faithfully embed the drift and diffusion of a given stochastic differential equation, including nonlinear and nonequilibrium dynamics such as chaotic attractors. As an application, we construct RNN realisations of stochastic systems that transiently explore various attractors through both input-driven switching and autonomous transitions driven by nonequilibrium currents, which we interpret as models of associative and sequential (episodic) memory. To elucidate how these dynamics are encoded in the network, we introduce decompositions of the RNN based on its asymmetric connectivity and its time-irreversibility. Our results extend attractor neural network theory beyond equilibrium, showing that asymmetric neural populations can implement a broad class of dynamical computations within low-dimensional manifolds, unifying ideas from associative memory, nonequilibrium statistical mechanics, and neural computation.", "link": "http://arxiv.org/abs/2602.14885v1", "date": "2026-02-16", "relevancy": 2.0336, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5372}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5066}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Drift-Diffusion%20Matching%3A%20Embedding%20dynamics%20in%20latent%20manifolds%20of%20asymmetric%20neural%20networks&body=Title%3A%20Drift-Diffusion%20Matching%3A%20Embedding%20dynamics%20in%20latent%20manifolds%20of%20asymmetric%20neural%20networks%0AAuthor%3A%20Ram%C3%B3n%20Nartallo-Kaluarachchi%20and%20Renaud%20Lambiotte%20and%20Alain%20Goriely%0AAbstract%3A%20Recurrent%20neural%20networks%20%28RNNs%29%20provide%20a%20theoretical%20framework%20for%20understanding%20computation%20in%20biological%20neural%20circuits%2C%20yet%20classical%20results%2C%20such%20as%20Hopfield%27s%20model%20of%20associative%20memory%2C%20rely%20on%20symmetric%20connectivity%20that%20restricts%20network%20dynamics%20to%20gradient-like%20flows.%20In%20contrast%2C%20biological%20networks%20support%20rich%20time-dependent%20behaviour%20facilitated%20by%20their%20asymmetry.%20Here%20we%20introduce%20a%20general%20framework%2C%20which%20we%20term%20drift-diffusion%20matching%2C%20for%20training%20continuous-time%20RNNs%20to%20represent%20arbitrary%20stochastic%20dynamical%20systems%20within%20a%20low-dimensional%20latent%20subspace.%20Allowing%20asymmetric%20connectivity%2C%20we%20show%20that%20RNNs%20can%20faithfully%20embed%20the%20drift%20and%20diffusion%20of%20a%20given%20stochastic%20differential%20equation%2C%20including%20nonlinear%20and%20nonequilibrium%20dynamics%20such%20as%20chaotic%20attractors.%20As%20an%20application%2C%20we%20construct%20RNN%20realisations%20of%20stochastic%20systems%20that%20transiently%20explore%20various%20attractors%20through%20both%20input-driven%20switching%20and%20autonomous%20transitions%20driven%20by%20nonequilibrium%20currents%2C%20which%20we%20interpret%20as%20models%20of%20associative%20and%20sequential%20%28episodic%29%20memory.%20To%20elucidate%20how%20these%20dynamics%20are%20encoded%20in%20the%20network%2C%20we%20introduce%20decompositions%20of%20the%20RNN%20based%20on%20its%20asymmetric%20connectivity%20and%20its%20time-irreversibility.%20Our%20results%20extend%20attractor%20neural%20network%20theory%20beyond%20equilibrium%2C%20showing%20that%20asymmetric%20neural%20populations%20can%20implement%20a%20broad%20class%20of%20dynamical%20computations%20within%20low-dimensional%20manifolds%2C%20unifying%20ideas%20from%20associative%20memory%2C%20nonequilibrium%20statistical%20mechanics%2C%20and%20neural%20computation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrift-Diffusion%2520Matching%253A%2520Embedding%2520dynamics%2520in%2520latent%2520manifolds%2520of%2520asymmetric%2520neural%2520networks%26entry.906535625%3DRam%25C3%25B3n%2520Nartallo-Kaluarachchi%2520and%2520Renaud%2520Lambiotte%2520and%2520Alain%2520Goriely%26entry.1292438233%3DRecurrent%2520neural%2520networks%2520%2528RNNs%2529%2520provide%2520a%2520theoretical%2520framework%2520for%2520understanding%2520computation%2520in%2520biological%2520neural%2520circuits%252C%2520yet%2520classical%2520results%252C%2520such%2520as%2520Hopfield%2527s%2520model%2520of%2520associative%2520memory%252C%2520rely%2520on%2520symmetric%2520connectivity%2520that%2520restricts%2520network%2520dynamics%2520to%2520gradient-like%2520flows.%2520In%2520contrast%252C%2520biological%2520networks%2520support%2520rich%2520time-dependent%2520behaviour%2520facilitated%2520by%2520their%2520asymmetry.%2520Here%2520we%2520introduce%2520a%2520general%2520framework%252C%2520which%2520we%2520term%2520drift-diffusion%2520matching%252C%2520for%2520training%2520continuous-time%2520RNNs%2520to%2520represent%2520arbitrary%2520stochastic%2520dynamical%2520systems%2520within%2520a%2520low-dimensional%2520latent%2520subspace.%2520Allowing%2520asymmetric%2520connectivity%252C%2520we%2520show%2520that%2520RNNs%2520can%2520faithfully%2520embed%2520the%2520drift%2520and%2520diffusion%2520of%2520a%2520given%2520stochastic%2520differential%2520equation%252C%2520including%2520nonlinear%2520and%2520nonequilibrium%2520dynamics%2520such%2520as%2520chaotic%2520attractors.%2520As%2520an%2520application%252C%2520we%2520construct%2520RNN%2520realisations%2520of%2520stochastic%2520systems%2520that%2520transiently%2520explore%2520various%2520attractors%2520through%2520both%2520input-driven%2520switching%2520and%2520autonomous%2520transitions%2520driven%2520by%2520nonequilibrium%2520currents%252C%2520which%2520we%2520interpret%2520as%2520models%2520of%2520associative%2520and%2520sequential%2520%2528episodic%2529%2520memory.%2520To%2520elucidate%2520how%2520these%2520dynamics%2520are%2520encoded%2520in%2520the%2520network%252C%2520we%2520introduce%2520decompositions%2520of%2520the%2520RNN%2520based%2520on%2520its%2520asymmetric%2520connectivity%2520and%2520its%2520time-irreversibility.%2520Our%2520results%2520extend%2520attractor%2520neural%2520network%2520theory%2520beyond%2520equilibrium%252C%2520showing%2520that%2520asymmetric%2520neural%2520populations%2520can%2520implement%2520a%2520broad%2520class%2520of%2520dynamical%2520computations%2520within%2520low-dimensional%2520manifolds%252C%2520unifying%2520ideas%2520from%2520associative%2520memory%252C%2520nonequilibrium%2520statistical%2520mechanics%252C%2520and%2520neural%2520computation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Drift-Diffusion%20Matching%3A%20Embedding%20dynamics%20in%20latent%20manifolds%20of%20asymmetric%20neural%20networks&entry.906535625=Ram%C3%B3n%20Nartallo-Kaluarachchi%20and%20Renaud%20Lambiotte%20and%20Alain%20Goriely&entry.1292438233=Recurrent%20neural%20networks%20%28RNNs%29%20provide%20a%20theoretical%20framework%20for%20understanding%20computation%20in%20biological%20neural%20circuits%2C%20yet%20classical%20results%2C%20such%20as%20Hopfield%27s%20model%20of%20associative%20memory%2C%20rely%20on%20symmetric%20connectivity%20that%20restricts%20network%20dynamics%20to%20gradient-like%20flows.%20In%20contrast%2C%20biological%20networks%20support%20rich%20time-dependent%20behaviour%20facilitated%20by%20their%20asymmetry.%20Here%20we%20introduce%20a%20general%20framework%2C%20which%20we%20term%20drift-diffusion%20matching%2C%20for%20training%20continuous-time%20RNNs%20to%20represent%20arbitrary%20stochastic%20dynamical%20systems%20within%20a%20low-dimensional%20latent%20subspace.%20Allowing%20asymmetric%20connectivity%2C%20we%20show%20that%20RNNs%20can%20faithfully%20embed%20the%20drift%20and%20diffusion%20of%20a%20given%20stochastic%20differential%20equation%2C%20including%20nonlinear%20and%20nonequilibrium%20dynamics%20such%20as%20chaotic%20attractors.%20As%20an%20application%2C%20we%20construct%20RNN%20realisations%20of%20stochastic%20systems%20that%20transiently%20explore%20various%20attractors%20through%20both%20input-driven%20switching%20and%20autonomous%20transitions%20driven%20by%20nonequilibrium%20currents%2C%20which%20we%20interpret%20as%20models%20of%20associative%20and%20sequential%20%28episodic%29%20memory.%20To%20elucidate%20how%20these%20dynamics%20are%20encoded%20in%20the%20network%2C%20we%20introduce%20decompositions%20of%20the%20RNN%20based%20on%20its%20asymmetric%20connectivity%20and%20its%20time-irreversibility.%20Our%20results%20extend%20attractor%20neural%20network%20theory%20beyond%20equilibrium%2C%20showing%20that%20asymmetric%20neural%20populations%20can%20implement%20a%20broad%20class%20of%20dynamical%20computations%20within%20low-dimensional%20manifolds%2C%20unifying%20ideas%20from%20associative%20memory%2C%20nonequilibrium%20statistical%20mechanics%2C%20and%20neural%20computation.&entry.1838667208=http%3A//arxiv.org/abs/2602.14885v1&entry.124074799=Read"},
{"title": "Robust Multi-Objective Controlled Decoding of Large Language Models", "author": "Seongho Son and William Bankes and Sangwoong Yoon and Shyam Sundhar Ramesh and Xiaohang Tang and Ilija Bogunovic", "abstract": "We introduce Robust Multi-Objective Decoding (RMOD), a novel inference-time algorithm that robustly aligns Large Language Models (LLMs) to multiple human objectives (e.g., instruction-following, helpfulness, safety) by maximizing the worst-case rewards. RMOD formulates the robust decoding problem as a maximin two-player game between adversarially computed reward weights and the sampling policy, solvable through a Nash equilibrium. We demonstrate that this game reduces to a convex optimization problem to identify the worst-case reward weights, with the optimal sampling policy analytically derived. For practical applications, we propose an efficient algorithm of RMOD tailored for contemporary LLMs, introducing minimal computational overhead compared to standard non-robust Controlled Decoding methods. Experimental results across a range of popular alignment datasets with up to 10 objectives show the effectiveness of RMOD and its distilled version, consistently outperforming baselines in worst-case rewards and win rates.", "link": "http://arxiv.org/abs/2503.08796v2", "date": "2026-02-16", "relevancy": 2.0305, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Multi-Objective%20Controlled%20Decoding%20of%20Large%20Language%20Models&body=Title%3A%20Robust%20Multi-Objective%20Controlled%20Decoding%20of%20Large%20Language%20Models%0AAuthor%3A%20Seongho%20Son%20and%20William%20Bankes%20and%20Sangwoong%20Yoon%20and%20Shyam%20Sundhar%20Ramesh%20and%20Xiaohang%20Tang%20and%20Ilija%20Bogunovic%0AAbstract%3A%20We%20introduce%20Robust%20Multi-Objective%20Decoding%20%28RMOD%29%2C%20a%20novel%20inference-time%20algorithm%20that%20robustly%20aligns%20Large%20Language%20Models%20%28LLMs%29%20to%20multiple%20human%20objectives%20%28e.g.%2C%20instruction-following%2C%20helpfulness%2C%20safety%29%20by%20maximizing%20the%20worst-case%20rewards.%20RMOD%20formulates%20the%20robust%20decoding%20problem%20as%20a%20maximin%20two-player%20game%20between%20adversarially%20computed%20reward%20weights%20and%20the%20sampling%20policy%2C%20solvable%20through%20a%20Nash%20equilibrium.%20We%20demonstrate%20that%20this%20game%20reduces%20to%20a%20convex%20optimization%20problem%20to%20identify%20the%20worst-case%20reward%20weights%2C%20with%20the%20optimal%20sampling%20policy%20analytically%20derived.%20For%20practical%20applications%2C%20we%20propose%20an%20efficient%20algorithm%20of%20RMOD%20tailored%20for%20contemporary%20LLMs%2C%20introducing%20minimal%20computational%20overhead%20compared%20to%20standard%20non-robust%20Controlled%20Decoding%20methods.%20Experimental%20results%20across%20a%20range%20of%20popular%20alignment%20datasets%20with%20up%20to%2010%20objectives%20show%20the%20effectiveness%20of%20RMOD%20and%20its%20distilled%20version%2C%20consistently%20outperforming%20baselines%20in%20worst-case%20rewards%20and%20win%20rates.%0ALink%3A%20http%3A//arxiv.org/abs/2503.08796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Multi-Objective%2520Controlled%2520Decoding%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DSeongho%2520Son%2520and%2520William%2520Bankes%2520and%2520Sangwoong%2520Yoon%2520and%2520Shyam%2520Sundhar%2520Ramesh%2520and%2520Xiaohang%2520Tang%2520and%2520Ilija%2520Bogunovic%26entry.1292438233%3DWe%2520introduce%2520Robust%2520Multi-Objective%2520Decoding%2520%2528RMOD%2529%252C%2520a%2520novel%2520inference-time%2520algorithm%2520that%2520robustly%2520aligns%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520multiple%2520human%2520objectives%2520%2528e.g.%252C%2520instruction-following%252C%2520helpfulness%252C%2520safety%2529%2520by%2520maximizing%2520the%2520worst-case%2520rewards.%2520RMOD%2520formulates%2520the%2520robust%2520decoding%2520problem%2520as%2520a%2520maximin%2520two-player%2520game%2520between%2520adversarially%2520computed%2520reward%2520weights%2520and%2520the%2520sampling%2520policy%252C%2520solvable%2520through%2520a%2520Nash%2520equilibrium.%2520We%2520demonstrate%2520that%2520this%2520game%2520reduces%2520to%2520a%2520convex%2520optimization%2520problem%2520to%2520identify%2520the%2520worst-case%2520reward%2520weights%252C%2520with%2520the%2520optimal%2520sampling%2520policy%2520analytically%2520derived.%2520For%2520practical%2520applications%252C%2520we%2520propose%2520an%2520efficient%2520algorithm%2520of%2520RMOD%2520tailored%2520for%2520contemporary%2520LLMs%252C%2520introducing%2520minimal%2520computational%2520overhead%2520compared%2520to%2520standard%2520non-robust%2520Controlled%2520Decoding%2520methods.%2520Experimental%2520results%2520across%2520a%2520range%2520of%2520popular%2520alignment%2520datasets%2520with%2520up%2520to%252010%2520objectives%2520show%2520the%2520effectiveness%2520of%2520RMOD%2520and%2520its%2520distilled%2520version%252C%2520consistently%2520outperforming%2520baselines%2520in%2520worst-case%2520rewards%2520and%2520win%2520rates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Multi-Objective%20Controlled%20Decoding%20of%20Large%20Language%20Models&entry.906535625=Seongho%20Son%20and%20William%20Bankes%20and%20Sangwoong%20Yoon%20and%20Shyam%20Sundhar%20Ramesh%20and%20Xiaohang%20Tang%20and%20Ilija%20Bogunovic&entry.1292438233=We%20introduce%20Robust%20Multi-Objective%20Decoding%20%28RMOD%29%2C%20a%20novel%20inference-time%20algorithm%20that%20robustly%20aligns%20Large%20Language%20Models%20%28LLMs%29%20to%20multiple%20human%20objectives%20%28e.g.%2C%20instruction-following%2C%20helpfulness%2C%20safety%29%20by%20maximizing%20the%20worst-case%20rewards.%20RMOD%20formulates%20the%20robust%20decoding%20problem%20as%20a%20maximin%20two-player%20game%20between%20adversarially%20computed%20reward%20weights%20and%20the%20sampling%20policy%2C%20solvable%20through%20a%20Nash%20equilibrium.%20We%20demonstrate%20that%20this%20game%20reduces%20to%20a%20convex%20optimization%20problem%20to%20identify%20the%20worst-case%20reward%20weights%2C%20with%20the%20optimal%20sampling%20policy%20analytically%20derived.%20For%20practical%20applications%2C%20we%20propose%20an%20efficient%20algorithm%20of%20RMOD%20tailored%20for%20contemporary%20LLMs%2C%20introducing%20minimal%20computational%20overhead%20compared%20to%20standard%20non-robust%20Controlled%20Decoding%20methods.%20Experimental%20results%20across%20a%20range%20of%20popular%20alignment%20datasets%20with%20up%20to%2010%20objectives%20show%20the%20effectiveness%20of%20RMOD%20and%20its%20distilled%20version%2C%20consistently%20outperforming%20baselines%20in%20worst-case%20rewards%20and%20win%20rates.&entry.1838667208=http%3A//arxiv.org/abs/2503.08796v2&entry.124074799=Read"},
{"title": "iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering", "author": "Shuai Wang and Yinan Yu", "abstract": "Large Language Models (LLMs) excel in many natural language processing tasks but often exhibit factual inconsistencies in knowledge-intensive settings. Integrating external knowledge resources, particularly knowledge graphs (KGs), provides a transparent and updatable foundation for more reliable reasoning. Knowledge Base Question Answering (KBQA), which queries and reasons over KGs, is central to this effort, especially for complex, multi-hop queries. However, multi-hop reasoning poses two key challenges: (1)~maintaining coherent reasoning paths, and (2)~avoiding prematurely discarding critical multi-hop connections. To tackle these challenges, we introduce iQUEST, a question-guided KBQA framework that iteratively decomposes complex queries into simpler sub-questions, ensuring a structured and focused reasoning trajectory. Additionally, we integrate a Graph Neural Network (GNN) to look ahead and incorporate 2-hop neighbor information at each reasoning step. This dual approach strengthens the reasoning process, enabling the model to explore viable paths more effectively. Detailed experiments demonstrate the consistent improvement delivered by iQUEST across four benchmark datasets and four LLMs. The code is publicly available at: https://github.com/Wangshuaiia/iQUEST.", "link": "http://arxiv.org/abs/2506.01784v6", "date": "2026-02-16", "relevancy": 2.0279, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5129}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4776}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iQUEST%3A%20An%20Iterative%20Question-Guided%20Framework%20for%20Knowledge%20Base%20Question%20Answering&body=Title%3A%20iQUEST%3A%20An%20Iterative%20Question-Guided%20Framework%20for%20Knowledge%20Base%20Question%20Answering%0AAuthor%3A%20Shuai%20Wang%20and%20Yinan%20Yu%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20many%20natural%20language%20processing%20tasks%20but%20often%20exhibit%20factual%20inconsistencies%20in%20knowledge-intensive%20settings.%20Integrating%20external%20knowledge%20resources%2C%20particularly%20knowledge%20graphs%20%28KGs%29%2C%20provides%20a%20transparent%20and%20updatable%20foundation%20for%20more%20reliable%20reasoning.%20Knowledge%20Base%20Question%20Answering%20%28KBQA%29%2C%20which%20queries%20and%20reasons%20over%20KGs%2C%20is%20central%20to%20this%20effort%2C%20especially%20for%20complex%2C%20multi-hop%20queries.%20However%2C%20multi-hop%20reasoning%20poses%20two%20key%20challenges%3A%20%281%29~maintaining%20coherent%20reasoning%20paths%2C%20and%20%282%29~avoiding%20prematurely%20discarding%20critical%20multi-hop%20connections.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20iQUEST%2C%20a%20question-guided%20KBQA%20framework%20that%20iteratively%20decomposes%20complex%20queries%20into%20simpler%20sub-questions%2C%20ensuring%20a%20structured%20and%20focused%20reasoning%20trajectory.%20Additionally%2C%20we%20integrate%20a%20Graph%20Neural%20Network%20%28GNN%29%20to%20look%20ahead%20and%20incorporate%202-hop%20neighbor%20information%20at%20each%20reasoning%20step.%20This%20dual%20approach%20strengthens%20the%20reasoning%20process%2C%20enabling%20the%20model%20to%20explore%20viable%20paths%20more%20effectively.%20Detailed%20experiments%20demonstrate%20the%20consistent%20improvement%20delivered%20by%20iQUEST%20across%20four%20benchmark%20datasets%20and%20four%20LLMs.%20The%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/Wangshuaiia/iQUEST.%0ALink%3A%20http%3A//arxiv.org/abs/2506.01784v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiQUEST%253A%2520An%2520Iterative%2520Question-Guided%2520Framework%2520for%2520Knowledge%2520Base%2520Question%2520Answering%26entry.906535625%3DShuai%2520Wang%2520and%2520Yinan%2520Yu%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520many%2520natural%2520language%2520processing%2520tasks%2520but%2520often%2520exhibit%2520factual%2520inconsistencies%2520in%2520knowledge-intensive%2520settings.%2520Integrating%2520external%2520knowledge%2520resources%252C%2520particularly%2520knowledge%2520graphs%2520%2528KGs%2529%252C%2520provides%2520a%2520transparent%2520and%2520updatable%2520foundation%2520for%2520more%2520reliable%2520reasoning.%2520Knowledge%2520Base%2520Question%2520Answering%2520%2528KBQA%2529%252C%2520which%2520queries%2520and%2520reasons%2520over%2520KGs%252C%2520is%2520central%2520to%2520this%2520effort%252C%2520especially%2520for%2520complex%252C%2520multi-hop%2520queries.%2520However%252C%2520multi-hop%2520reasoning%2520poses%2520two%2520key%2520challenges%253A%2520%25281%2529~maintaining%2520coherent%2520reasoning%2520paths%252C%2520and%2520%25282%2529~avoiding%2520prematurely%2520discarding%2520critical%2520multi-hop%2520connections.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520introduce%2520iQUEST%252C%2520a%2520question-guided%2520KBQA%2520framework%2520that%2520iteratively%2520decomposes%2520complex%2520queries%2520into%2520simpler%2520sub-questions%252C%2520ensuring%2520a%2520structured%2520and%2520focused%2520reasoning%2520trajectory.%2520Additionally%252C%2520we%2520integrate%2520a%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520to%2520look%2520ahead%2520and%2520incorporate%25202-hop%2520neighbor%2520information%2520at%2520each%2520reasoning%2520step.%2520This%2520dual%2520approach%2520strengthens%2520the%2520reasoning%2520process%252C%2520enabling%2520the%2520model%2520to%2520explore%2520viable%2520paths%2520more%2520effectively.%2520Detailed%2520experiments%2520demonstrate%2520the%2520consistent%2520improvement%2520delivered%2520by%2520iQUEST%2520across%2520four%2520benchmark%2520datasets%2520and%2520four%2520LLMs.%2520The%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/Wangshuaiia/iQUEST.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01784v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iQUEST%3A%20An%20Iterative%20Question-Guided%20Framework%20for%20Knowledge%20Base%20Question%20Answering&entry.906535625=Shuai%20Wang%20and%20Yinan%20Yu&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20excel%20in%20many%20natural%20language%20processing%20tasks%20but%20often%20exhibit%20factual%20inconsistencies%20in%20knowledge-intensive%20settings.%20Integrating%20external%20knowledge%20resources%2C%20particularly%20knowledge%20graphs%20%28KGs%29%2C%20provides%20a%20transparent%20and%20updatable%20foundation%20for%20more%20reliable%20reasoning.%20Knowledge%20Base%20Question%20Answering%20%28KBQA%29%2C%20which%20queries%20and%20reasons%20over%20KGs%2C%20is%20central%20to%20this%20effort%2C%20especially%20for%20complex%2C%20multi-hop%20queries.%20However%2C%20multi-hop%20reasoning%20poses%20two%20key%20challenges%3A%20%281%29~maintaining%20coherent%20reasoning%20paths%2C%20and%20%282%29~avoiding%20prematurely%20discarding%20critical%20multi-hop%20connections.%20To%20tackle%20these%20challenges%2C%20we%20introduce%20iQUEST%2C%20a%20question-guided%20KBQA%20framework%20that%20iteratively%20decomposes%20complex%20queries%20into%20simpler%20sub-questions%2C%20ensuring%20a%20structured%20and%20focused%20reasoning%20trajectory.%20Additionally%2C%20we%20integrate%20a%20Graph%20Neural%20Network%20%28GNN%29%20to%20look%20ahead%20and%20incorporate%202-hop%20neighbor%20information%20at%20each%20reasoning%20step.%20This%20dual%20approach%20strengthens%20the%20reasoning%20process%2C%20enabling%20the%20model%20to%20explore%20viable%20paths%20more%20effectively.%20Detailed%20experiments%20demonstrate%20the%20consistent%20improvement%20delivered%20by%20iQUEST%20across%20four%20benchmark%20datasets%20and%20four%20LLMs.%20The%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/Wangshuaiia/iQUEST.&entry.1838667208=http%3A//arxiv.org/abs/2506.01784v6&entry.124074799=Read"},
{"title": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation", "author": "Alisa Vinogradova and Vlad Vinogradov and Luba Greenwood and Ilya Yasny and Dmitry Kobyzev and Shoman Kasbekar and Kong Nguyen and Dmitrii Radkevich and Roman Doronin and Andrey Doronichev", "abstract": "Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface \"under-the-radar\" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.\n  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.", "link": "http://arxiv.org/abs/2602.15019v1", "date": "2026-02-16", "relevancy": 2.0214, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hunt%20Globally%3A%20Deep%20Research%20AI%20Agents%20for%20Drug%20Asset%20Scouting%20in%20Investing%2C%20Business%20Development%2C%20and%20Search%20%26%20Evaluation&body=Title%3A%20Hunt%20Globally%3A%20Deep%20Research%20AI%20Agents%20for%20Drug%20Asset%20Scouting%20in%20Investing%2C%20Business%20Development%2C%20and%20Search%20%26%20Evaluation%0AAuthor%3A%20Alisa%20Vinogradova%20and%20Vlad%20Vinogradov%20and%20Luba%20Greenwood%20and%20Ilya%20Yasny%20and%20Dmitry%20Kobyzev%20and%20Shoman%20Kasbekar%20and%20Kong%20Nguyen%20and%20Dmitrii%20Radkevich%20and%20Roman%20Doronin%20and%20Andrey%20Doronichev%0AAbstract%3A%20Bio-pharmaceutical%20innovation%20has%20shifted%3A%20many%20new%20drug%20assets%20now%20originate%20outside%20the%20United%20States%20and%20are%20disclosed%20primarily%20via%20regional%2C%20non-English%20channels.%20Recent%20data%20suggests%20%3E85%25%20of%20patent%20filings%20originate%20outside%20the%20U.S.%2C%20with%20China%20accounting%20for%20nearly%20half%20of%20the%20global%20total%3B%20a%20growing%20share%20of%20scholarly%20output%20is%20also%20non-U.S.%20Industry%20estimates%20put%20China%20at%20~30%25%20of%20global%20drug%20development%2C%20spanning%201%2C200%2B%20novel%20candidates.%20In%20this%20high-stakes%20environment%2C%20failing%20to%20surface%20%22under-the-radar%22%20assets%20creates%20multi-billion-dollar%20risk%20for%20investors%20and%20business%20development%20teams%2C%20making%20asset%20scouting%20a%20coverage-critical%20competition%20where%20speed%20and%20completeness%20drive%20value.%20Yet%20today%27s%20Deep%20Research%20AI%20agents%20still%20lag%20human%20experts%20in%20achieving%20high-recall%20discovery%20across%20heterogeneous%2C%20multilingual%20sources%20without%20hallucinations.%0A%20%20We%20propose%20a%20benchmarking%20methodology%20for%20drug%20asset%20scouting%20and%20a%20tuned%2C%20tree-based%20self-learning%20Bioptic%20Agent%20aimed%20at%20complete%2C%20non-hallucinated%20scouting.%20We%20construct%20a%20challenging%20completeness%20benchmark%20using%20a%20multilingual%20multi-agent%20pipeline%3A%20complex%20user%20queries%20paired%20with%20ground-truth%20assets%20that%20are%20largely%20outside%20U.S.-centric%20radar.%20To%20reflect%20real%20deal%20complexity%2C%20we%20collected%20screening%20queries%20from%20expert%20investors%2C%20BD%2C%20and%20VC%20professionals%20and%20used%20them%20as%20priors%20to%20conditionally%20generate%20benchmark%20queries.%20For%20grading%2C%20we%20use%20LLM-as-judge%20evaluation%20calibrated%20to%20expert%20opinions.%20We%20compare%20Bioptic%20Agent%20against%20Claude%20Opus%204.6%2C%20OpenAI%20GPT-5.2%20Pro%2C%20Perplexity%20Deep%20Research%2C%20Gemini%203%20Pro%20%2B%20Deep%20Research%2C%20and%20Exa%20Websets.%20Bioptic%20Agent%20achieves%2079.7%25%20F1%20versus%2056.2%25%20%28Claude%20Opus%204.6%29%2C%2050.6%25%20%28Gemini%203%20Pro%20%2B%20Deep%20Research%29%2C%2046.6%25%20%28GPT-5.2%20Pro%29%2C%2044.2%25%20%28Perplexity%20Deep%20Research%29%2C%20and%2026.9%25%20%28Exa%20Websets%29.%20Performance%20improves%20steeply%20with%20additional%20compute%2C%20supporting%20the%20view%20that%20more%20compute%20yields%20better%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15019v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunt%2520Globally%253A%2520Deep%2520Research%2520AI%2520Agents%2520for%2520Drug%2520Asset%2520Scouting%2520in%2520Investing%252C%2520Business%2520Development%252C%2520and%2520Search%2520%2526%2520Evaluation%26entry.906535625%3DAlisa%2520Vinogradova%2520and%2520Vlad%2520Vinogradov%2520and%2520Luba%2520Greenwood%2520and%2520Ilya%2520Yasny%2520and%2520Dmitry%2520Kobyzev%2520and%2520Shoman%2520Kasbekar%2520and%2520Kong%2520Nguyen%2520and%2520Dmitrii%2520Radkevich%2520and%2520Roman%2520Doronin%2520and%2520Andrey%2520Doronichev%26entry.1292438233%3DBio-pharmaceutical%2520innovation%2520has%2520shifted%253A%2520many%2520new%2520drug%2520assets%2520now%2520originate%2520outside%2520the%2520United%2520States%2520and%2520are%2520disclosed%2520primarily%2520via%2520regional%252C%2520non-English%2520channels.%2520Recent%2520data%2520suggests%2520%253E85%2525%2520of%2520patent%2520filings%2520originate%2520outside%2520the%2520U.S.%252C%2520with%2520China%2520accounting%2520for%2520nearly%2520half%2520of%2520the%2520global%2520total%253B%2520a%2520growing%2520share%2520of%2520scholarly%2520output%2520is%2520also%2520non-U.S.%2520Industry%2520estimates%2520put%2520China%2520at%2520~30%2525%2520of%2520global%2520drug%2520development%252C%2520spanning%25201%252C200%252B%2520novel%2520candidates.%2520In%2520this%2520high-stakes%2520environment%252C%2520failing%2520to%2520surface%2520%2522under-the-radar%2522%2520assets%2520creates%2520multi-billion-dollar%2520risk%2520for%2520investors%2520and%2520business%2520development%2520teams%252C%2520making%2520asset%2520scouting%2520a%2520coverage-critical%2520competition%2520where%2520speed%2520and%2520completeness%2520drive%2520value.%2520Yet%2520today%2527s%2520Deep%2520Research%2520AI%2520agents%2520still%2520lag%2520human%2520experts%2520in%2520achieving%2520high-recall%2520discovery%2520across%2520heterogeneous%252C%2520multilingual%2520sources%2520without%2520hallucinations.%250A%2520%2520We%2520propose%2520a%2520benchmarking%2520methodology%2520for%2520drug%2520asset%2520scouting%2520and%2520a%2520tuned%252C%2520tree-based%2520self-learning%2520Bioptic%2520Agent%2520aimed%2520at%2520complete%252C%2520non-hallucinated%2520scouting.%2520We%2520construct%2520a%2520challenging%2520completeness%2520benchmark%2520using%2520a%2520multilingual%2520multi-agent%2520pipeline%253A%2520complex%2520user%2520queries%2520paired%2520with%2520ground-truth%2520assets%2520that%2520are%2520largely%2520outside%2520U.S.-centric%2520radar.%2520To%2520reflect%2520real%2520deal%2520complexity%252C%2520we%2520collected%2520screening%2520queries%2520from%2520expert%2520investors%252C%2520BD%252C%2520and%2520VC%2520professionals%2520and%2520used%2520them%2520as%2520priors%2520to%2520conditionally%2520generate%2520benchmark%2520queries.%2520For%2520grading%252C%2520we%2520use%2520LLM-as-judge%2520evaluation%2520calibrated%2520to%2520expert%2520opinions.%2520We%2520compare%2520Bioptic%2520Agent%2520against%2520Claude%2520Opus%25204.6%252C%2520OpenAI%2520GPT-5.2%2520Pro%252C%2520Perplexity%2520Deep%2520Research%252C%2520Gemini%25203%2520Pro%2520%252B%2520Deep%2520Research%252C%2520and%2520Exa%2520Websets.%2520Bioptic%2520Agent%2520achieves%252079.7%2525%2520F1%2520versus%252056.2%2525%2520%2528Claude%2520Opus%25204.6%2529%252C%252050.6%2525%2520%2528Gemini%25203%2520Pro%2520%252B%2520Deep%2520Research%2529%252C%252046.6%2525%2520%2528GPT-5.2%2520Pro%2529%252C%252044.2%2525%2520%2528Perplexity%2520Deep%2520Research%2529%252C%2520and%252026.9%2525%2520%2528Exa%2520Websets%2529.%2520Performance%2520improves%2520steeply%2520with%2520additional%2520compute%252C%2520supporting%2520the%2520view%2520that%2520more%2520compute%2520yields%2520better%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15019v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hunt%20Globally%3A%20Deep%20Research%20AI%20Agents%20for%20Drug%20Asset%20Scouting%20in%20Investing%2C%20Business%20Development%2C%20and%20Search%20%26%20Evaluation&entry.906535625=Alisa%20Vinogradova%20and%20Vlad%20Vinogradov%20and%20Luba%20Greenwood%20and%20Ilya%20Yasny%20and%20Dmitry%20Kobyzev%20and%20Shoman%20Kasbekar%20and%20Kong%20Nguyen%20and%20Dmitrii%20Radkevich%20and%20Roman%20Doronin%20and%20Andrey%20Doronichev&entry.1292438233=Bio-pharmaceutical%20innovation%20has%20shifted%3A%20many%20new%20drug%20assets%20now%20originate%20outside%20the%20United%20States%20and%20are%20disclosed%20primarily%20via%20regional%2C%20non-English%20channels.%20Recent%20data%20suggests%20%3E85%25%20of%20patent%20filings%20originate%20outside%20the%20U.S.%2C%20with%20China%20accounting%20for%20nearly%20half%20of%20the%20global%20total%3B%20a%20growing%20share%20of%20scholarly%20output%20is%20also%20non-U.S.%20Industry%20estimates%20put%20China%20at%20~30%25%20of%20global%20drug%20development%2C%20spanning%201%2C200%2B%20novel%20candidates.%20In%20this%20high-stakes%20environment%2C%20failing%20to%20surface%20%22under-the-radar%22%20assets%20creates%20multi-billion-dollar%20risk%20for%20investors%20and%20business%20development%20teams%2C%20making%20asset%20scouting%20a%20coverage-critical%20competition%20where%20speed%20and%20completeness%20drive%20value.%20Yet%20today%27s%20Deep%20Research%20AI%20agents%20still%20lag%20human%20experts%20in%20achieving%20high-recall%20discovery%20across%20heterogeneous%2C%20multilingual%20sources%20without%20hallucinations.%0A%20%20We%20propose%20a%20benchmarking%20methodology%20for%20drug%20asset%20scouting%20and%20a%20tuned%2C%20tree-based%20self-learning%20Bioptic%20Agent%20aimed%20at%20complete%2C%20non-hallucinated%20scouting.%20We%20construct%20a%20challenging%20completeness%20benchmark%20using%20a%20multilingual%20multi-agent%20pipeline%3A%20complex%20user%20queries%20paired%20with%20ground-truth%20assets%20that%20are%20largely%20outside%20U.S.-centric%20radar.%20To%20reflect%20real%20deal%20complexity%2C%20we%20collected%20screening%20queries%20from%20expert%20investors%2C%20BD%2C%20and%20VC%20professionals%20and%20used%20them%20as%20priors%20to%20conditionally%20generate%20benchmark%20queries.%20For%20grading%2C%20we%20use%20LLM-as-judge%20evaluation%20calibrated%20to%20expert%20opinions.%20We%20compare%20Bioptic%20Agent%20against%20Claude%20Opus%204.6%2C%20OpenAI%20GPT-5.2%20Pro%2C%20Perplexity%20Deep%20Research%2C%20Gemini%203%20Pro%20%2B%20Deep%20Research%2C%20and%20Exa%20Websets.%20Bioptic%20Agent%20achieves%2079.7%25%20F1%20versus%2056.2%25%20%28Claude%20Opus%204.6%29%2C%2050.6%25%20%28Gemini%203%20Pro%20%2B%20Deep%20Research%29%2C%2046.6%25%20%28GPT-5.2%20Pro%29%2C%2044.2%25%20%28Perplexity%20Deep%20Research%29%2C%20and%2026.9%25%20%28Exa%20Websets%29.%20Performance%20improves%20steeply%20with%20additional%20compute%2C%20supporting%20the%20view%20that%20more%20compute%20yields%20better%20results.&entry.1838667208=http%3A//arxiv.org/abs/2602.15019v1&entry.124074799=Read"},
{"title": "Locally Adaptive Multi-Objective Learning", "author": "Jivat Neet Kaur and Isaac Gibbs and Michael I. Jordan", "abstract": "We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.", "link": "http://arxiv.org/abs/2602.14952v1", "date": "2026-02-16", "relevancy": 2.0176, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5162}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5022}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locally%20Adaptive%20Multi-Objective%20Learning&body=Title%3A%20Locally%20Adaptive%20Multi-Objective%20Learning%0AAuthor%3A%20Jivat%20Neet%20Kaur%20and%20Isaac%20Gibbs%20and%20Michael%20I.%20Jordan%0AAbstract%3A%20We%20consider%20the%20general%20problem%20of%20learning%20a%20predictor%20that%20satisfies%20multiple%20objectives%20of%20interest%20simultaneously%2C%20a%20broad%20framework%20that%20captures%20a%20range%20of%20specific%20learning%20goals%20including%20calibration%2C%20regret%2C%20and%20multiaccuracy.%20We%20work%20in%20an%20online%20setting%20where%20the%20data%20distribution%20can%20change%20arbitrarily%20over%20time.%20Existing%20approaches%20to%20this%20problem%20aim%20to%20minimize%20the%20set%20of%20objectives%20over%20the%20entire%20time%20horizon%20in%20a%20worst-case%20sense%2C%20and%20in%20practice%20they%20do%20not%20necessarily%20adapt%20to%20distribution%20shifts.%20Earlier%20work%20has%20aimed%20to%20alleviate%20this%20problem%20by%20incorporating%20additional%20objectives%20that%20target%20local%20guarantees%20over%20contiguous%20subintervals.%20Empirical%20evaluation%20of%20these%20proposals%20is%2C%20however%2C%20scarce.%20In%20this%20article%2C%20we%20consider%20an%20alternative%20procedure%20that%20achieves%20local%20adaptivity%20by%20replacing%20one%20part%20of%20the%20multi-objective%20learning%20method%20with%20an%20adaptive%20online%20algorithm.%20Empirical%20evaluations%20on%20datasets%20from%20energy%20forecasting%20and%20algorithmic%20fairness%20show%20that%20our%20proposed%20method%20improves%20upon%20existing%20approaches%20and%20achieves%20unbiased%20predictions%20over%20subgroups%2C%20while%20remaining%20robust%20under%20distribution%20shift.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocally%2520Adaptive%2520Multi-Objective%2520Learning%26entry.906535625%3DJivat%2520Neet%2520Kaur%2520and%2520Isaac%2520Gibbs%2520and%2520Michael%2520I.%2520Jordan%26entry.1292438233%3DWe%2520consider%2520the%2520general%2520problem%2520of%2520learning%2520a%2520predictor%2520that%2520satisfies%2520multiple%2520objectives%2520of%2520interest%2520simultaneously%252C%2520a%2520broad%2520framework%2520that%2520captures%2520a%2520range%2520of%2520specific%2520learning%2520goals%2520including%2520calibration%252C%2520regret%252C%2520and%2520multiaccuracy.%2520We%2520work%2520in%2520an%2520online%2520setting%2520where%2520the%2520data%2520distribution%2520can%2520change%2520arbitrarily%2520over%2520time.%2520Existing%2520approaches%2520to%2520this%2520problem%2520aim%2520to%2520minimize%2520the%2520set%2520of%2520objectives%2520over%2520the%2520entire%2520time%2520horizon%2520in%2520a%2520worst-case%2520sense%252C%2520and%2520in%2520practice%2520they%2520do%2520not%2520necessarily%2520adapt%2520to%2520distribution%2520shifts.%2520Earlier%2520work%2520has%2520aimed%2520to%2520alleviate%2520this%2520problem%2520by%2520incorporating%2520additional%2520objectives%2520that%2520target%2520local%2520guarantees%2520over%2520contiguous%2520subintervals.%2520Empirical%2520evaluation%2520of%2520these%2520proposals%2520is%252C%2520however%252C%2520scarce.%2520In%2520this%2520article%252C%2520we%2520consider%2520an%2520alternative%2520procedure%2520that%2520achieves%2520local%2520adaptivity%2520by%2520replacing%2520one%2520part%2520of%2520the%2520multi-objective%2520learning%2520method%2520with%2520an%2520adaptive%2520online%2520algorithm.%2520Empirical%2520evaluations%2520on%2520datasets%2520from%2520energy%2520forecasting%2520and%2520algorithmic%2520fairness%2520show%2520that%2520our%2520proposed%2520method%2520improves%2520upon%2520existing%2520approaches%2520and%2520achieves%2520unbiased%2520predictions%2520over%2520subgroups%252C%2520while%2520remaining%2520robust%2520under%2520distribution%2520shift.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locally%20Adaptive%20Multi-Objective%20Learning&entry.906535625=Jivat%20Neet%20Kaur%20and%20Isaac%20Gibbs%20and%20Michael%20I.%20Jordan&entry.1292438233=We%20consider%20the%20general%20problem%20of%20learning%20a%20predictor%20that%20satisfies%20multiple%20objectives%20of%20interest%20simultaneously%2C%20a%20broad%20framework%20that%20captures%20a%20range%20of%20specific%20learning%20goals%20including%20calibration%2C%20regret%2C%20and%20multiaccuracy.%20We%20work%20in%20an%20online%20setting%20where%20the%20data%20distribution%20can%20change%20arbitrarily%20over%20time.%20Existing%20approaches%20to%20this%20problem%20aim%20to%20minimize%20the%20set%20of%20objectives%20over%20the%20entire%20time%20horizon%20in%20a%20worst-case%20sense%2C%20and%20in%20practice%20they%20do%20not%20necessarily%20adapt%20to%20distribution%20shifts.%20Earlier%20work%20has%20aimed%20to%20alleviate%20this%20problem%20by%20incorporating%20additional%20objectives%20that%20target%20local%20guarantees%20over%20contiguous%20subintervals.%20Empirical%20evaluation%20of%20these%20proposals%20is%2C%20however%2C%20scarce.%20In%20this%20article%2C%20we%20consider%20an%20alternative%20procedure%20that%20achieves%20local%20adaptivity%20by%20replacing%20one%20part%20of%20the%20multi-objective%20learning%20method%20with%20an%20adaptive%20online%20algorithm.%20Empirical%20evaluations%20on%20datasets%20from%20energy%20forecasting%20and%20algorithmic%20fairness%20show%20that%20our%20proposed%20method%20improves%20upon%20existing%20approaches%20and%20achieves%20unbiased%20predictions%20over%20subgroups%2C%20while%20remaining%20robust%20under%20distribution%20shift.&entry.1838667208=http%3A//arxiv.org/abs/2602.14952v1&entry.124074799=Read"},
{"title": "Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer's Disease Detection via Speech", "author": "Xiao Wei and Bin Wen and Yuqin Lin and Kai Li and Mingyang gu and Xiaobao Wang and Longbiao Wang and Jianwu Dang", "abstract": "Early diagnosis of Alzheimer's Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers. Therefore, we propose FAL-AD, a novel framework that synergistically integrates federated learning with data augmentation to systematically optimize data efficiency. Our approach delivers three key breakthroughs: First, absolute efficiency improvement through voice conversion-based augmentation, which generates diverse pathological speech samples via cross-category voice-content recombination. Second, collaborative efficiency breakthrough via an adaptive federated learning paradigm, maximizing cross-institutional benefits under privacy constraints. Finally, representational efficiency optimization by an attentive cross-modal fusion model, which achieves fine-grained word-level alignment and acoustic-textual interaction. Evaluated on ADReSSo, FAL-AD achieves a state-of-the-art multi-modal accuracy of 91.52%, outperforming all centralized baselines and demonstrating a practical solution to the data efficiency dilemma. Our source code is publicly available at https://github.com/smileix/fal-ad.", "link": "http://arxiv.org/abs/2602.14655v1", "date": "2026-02-16", "relevancy": 2.0175, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5002}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Data%20Efficiency%20Dilemma%3A%20A%20Federated%20and%20Augmented%20Learning%20Framework%20For%20Alzheimer%27s%20Disease%20Detection%20via%20Speech&body=Title%3A%20Breaking%20Data%20Efficiency%20Dilemma%3A%20A%20Federated%20and%20Augmented%20Learning%20Framework%20For%20Alzheimer%27s%20Disease%20Detection%20via%20Speech%0AAuthor%3A%20Xiao%20Wei%20and%20Bin%20Wen%20and%20Yuqin%20Lin%20and%20Kai%20Li%20and%20Mingyang%20gu%20and%20Xiaobao%20Wang%20and%20Longbiao%20Wang%20and%20Jianwu%20Dang%0AAbstract%3A%20Early%20diagnosis%20of%20Alzheimer%27s%20Disease%20%28AD%29%20is%20crucial%20for%20delaying%20its%20progression.%20While%20AI-based%20speech%20detection%20is%20non-invasive%20and%20cost-effective%2C%20it%20faces%20a%20critical%20data%20efficiency%20dilemma%20due%20to%20medical%20data%20scarcity%20and%20privacy%20barriers.%20Therefore%2C%20we%20propose%20FAL-AD%2C%20a%20novel%20framework%20that%20synergistically%20integrates%20federated%20learning%20with%20data%20augmentation%20to%20systematically%20optimize%20data%20efficiency.%20Our%20approach%20delivers%20three%20key%20breakthroughs%3A%20First%2C%20absolute%20efficiency%20improvement%20through%20voice%20conversion-based%20augmentation%2C%20which%20generates%20diverse%20pathological%20speech%20samples%20via%20cross-category%20voice-content%20recombination.%20Second%2C%20collaborative%20efficiency%20breakthrough%20via%20an%20adaptive%20federated%20learning%20paradigm%2C%20maximizing%20cross-institutional%20benefits%20under%20privacy%20constraints.%20Finally%2C%20representational%20efficiency%20optimization%20by%20an%20attentive%20cross-modal%20fusion%20model%2C%20which%20achieves%20fine-grained%20word-level%20alignment%20and%20acoustic-textual%20interaction.%20Evaluated%20on%20ADReSSo%2C%20FAL-AD%20achieves%20a%20state-of-the-art%20multi-modal%20accuracy%20of%2091.52%25%2C%20outperforming%20all%20centralized%20baselines%20and%20demonstrating%20a%20practical%20solution%20to%20the%20data%20efficiency%20dilemma.%20Our%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/smileix/fal-ad.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Data%2520Efficiency%2520Dilemma%253A%2520A%2520Federated%2520and%2520Augmented%2520Learning%2520Framework%2520For%2520Alzheimer%2527s%2520Disease%2520Detection%2520via%2520Speech%26entry.906535625%3DXiao%2520Wei%2520and%2520Bin%2520Wen%2520and%2520Yuqin%2520Lin%2520and%2520Kai%2520Li%2520and%2520Mingyang%2520gu%2520and%2520Xiaobao%2520Wang%2520and%2520Longbiao%2520Wang%2520and%2520Jianwu%2520Dang%26entry.1292438233%3DEarly%2520diagnosis%2520of%2520Alzheimer%2527s%2520Disease%2520%2528AD%2529%2520is%2520crucial%2520for%2520delaying%2520its%2520progression.%2520While%2520AI-based%2520speech%2520detection%2520is%2520non-invasive%2520and%2520cost-effective%252C%2520it%2520faces%2520a%2520critical%2520data%2520efficiency%2520dilemma%2520due%2520to%2520medical%2520data%2520scarcity%2520and%2520privacy%2520barriers.%2520Therefore%252C%2520we%2520propose%2520FAL-AD%252C%2520a%2520novel%2520framework%2520that%2520synergistically%2520integrates%2520federated%2520learning%2520with%2520data%2520augmentation%2520to%2520systematically%2520optimize%2520data%2520efficiency.%2520Our%2520approach%2520delivers%2520three%2520key%2520breakthroughs%253A%2520First%252C%2520absolute%2520efficiency%2520improvement%2520through%2520voice%2520conversion-based%2520augmentation%252C%2520which%2520generates%2520diverse%2520pathological%2520speech%2520samples%2520via%2520cross-category%2520voice-content%2520recombination.%2520Second%252C%2520collaborative%2520efficiency%2520breakthrough%2520via%2520an%2520adaptive%2520federated%2520learning%2520paradigm%252C%2520maximizing%2520cross-institutional%2520benefits%2520under%2520privacy%2520constraints.%2520Finally%252C%2520representational%2520efficiency%2520optimization%2520by%2520an%2520attentive%2520cross-modal%2520fusion%2520model%252C%2520which%2520achieves%2520fine-grained%2520word-level%2520alignment%2520and%2520acoustic-textual%2520interaction.%2520Evaluated%2520on%2520ADReSSo%252C%2520FAL-AD%2520achieves%2520a%2520state-of-the-art%2520multi-modal%2520accuracy%2520of%252091.52%2525%252C%2520outperforming%2520all%2520centralized%2520baselines%2520and%2520demonstrating%2520a%2520practical%2520solution%2520to%2520the%2520data%2520efficiency%2520dilemma.%2520Our%2520source%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/smileix/fal-ad.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Data%20Efficiency%20Dilemma%3A%20A%20Federated%20and%20Augmented%20Learning%20Framework%20For%20Alzheimer%27s%20Disease%20Detection%20via%20Speech&entry.906535625=Xiao%20Wei%20and%20Bin%20Wen%20and%20Yuqin%20Lin%20and%20Kai%20Li%20and%20Mingyang%20gu%20and%20Xiaobao%20Wang%20and%20Longbiao%20Wang%20and%20Jianwu%20Dang&entry.1292438233=Early%20diagnosis%20of%20Alzheimer%27s%20Disease%20%28AD%29%20is%20crucial%20for%20delaying%20its%20progression.%20While%20AI-based%20speech%20detection%20is%20non-invasive%20and%20cost-effective%2C%20it%20faces%20a%20critical%20data%20efficiency%20dilemma%20due%20to%20medical%20data%20scarcity%20and%20privacy%20barriers.%20Therefore%2C%20we%20propose%20FAL-AD%2C%20a%20novel%20framework%20that%20synergistically%20integrates%20federated%20learning%20with%20data%20augmentation%20to%20systematically%20optimize%20data%20efficiency.%20Our%20approach%20delivers%20three%20key%20breakthroughs%3A%20First%2C%20absolute%20efficiency%20improvement%20through%20voice%20conversion-based%20augmentation%2C%20which%20generates%20diverse%20pathological%20speech%20samples%20via%20cross-category%20voice-content%20recombination.%20Second%2C%20collaborative%20efficiency%20breakthrough%20via%20an%20adaptive%20federated%20learning%20paradigm%2C%20maximizing%20cross-institutional%20benefits%20under%20privacy%20constraints.%20Finally%2C%20representational%20efficiency%20optimization%20by%20an%20attentive%20cross-modal%20fusion%20model%2C%20which%20achieves%20fine-grained%20word-level%20alignment%20and%20acoustic-textual%20interaction.%20Evaluated%20on%20ADReSSo%2C%20FAL-AD%20achieves%20a%20state-of-the-art%20multi-modal%20accuracy%20of%2091.52%25%2C%20outperforming%20all%20centralized%20baselines%20and%20demonstrating%20a%20practical%20solution%20to%20the%20data%20efficiency%20dilemma.%20Our%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/smileix/fal-ad.&entry.1838667208=http%3A//arxiv.org/abs/2602.14655v1&entry.124074799=Read"},
{"title": "Alignment Adapter to Improve the Performance of Compressed Deep Learning Models", "author": "Rohit Raj Rai and Abhishek Dhaka and Amit Awekar", "abstract": "Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.", "link": "http://arxiv.org/abs/2602.14635v1", "date": "2026-02-16", "relevancy": 2.0175, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5467}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4744}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment%20Adapter%20to%20Improve%20the%20Performance%20of%20Compressed%20Deep%20Learning%20Models&body=Title%3A%20Alignment%20Adapter%20to%20Improve%20the%20Performance%20of%20Compressed%20Deep%20Learning%20Models%0AAuthor%3A%20Rohit%20Raj%20Rai%20and%20Abhishek%20Dhaka%20and%20Amit%20Awekar%0AAbstract%3A%20Compressed%20Deep%20Learning%20%28DL%29%20models%20are%20essential%20for%20deployment%20in%20resource-constrained%20environments.%20But%20their%20performance%20often%20lags%20behind%20their%20large-scale%20counterparts.%20To%20bridge%20this%20gap%2C%20we%20propose%20Alignment%20Adapter%20%28AlAd%29%3A%20a%20lightweight%2C%20sliding-window-based%20adapter.%20It%20aligns%20the%20token-level%20embeddings%20of%20a%20compressed%20model%20with%20those%20of%20the%20original%20large%20model.%20AlAd%20preserves%20local%20contextual%20semantics%2C%20enables%20flexible%20alignment%20across%20differing%20dimensionalities%20or%20architectures%2C%20and%20is%20entirely%20agnostic%20to%20the%20underlying%20compression%20method.%20AlAd%20can%20be%20deployed%20in%20two%20ways%3A%20as%20a%20plug-and-play%20module%20over%20a%20frozen%20compressed%20model%2C%20or%20by%20jointly%20fine-tuning%20AlAd%20with%20the%20compressed%20model%20for%20further%20performance%20gains.%20Through%20experiments%20on%20BERT-family%20models%20across%20three%20token-level%20NLP%20tasks%2C%20we%20demonstrate%20that%20AlAd%20significantly%20boosts%20the%20performance%20of%20compressed%20models%20with%20only%20marginal%20overhead%20in%20size%20and%20latency.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment%2520Adapter%2520to%2520Improve%2520the%2520Performance%2520of%2520Compressed%2520Deep%2520Learning%2520Models%26entry.906535625%3DRohit%2520Raj%2520Rai%2520and%2520Abhishek%2520Dhaka%2520and%2520Amit%2520Awekar%26entry.1292438233%3DCompressed%2520Deep%2520Learning%2520%2528DL%2529%2520models%2520are%2520essential%2520for%2520deployment%2520in%2520resource-constrained%2520environments.%2520But%2520their%2520performance%2520often%2520lags%2520behind%2520their%2520large-scale%2520counterparts.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520Alignment%2520Adapter%2520%2528AlAd%2529%253A%2520a%2520lightweight%252C%2520sliding-window-based%2520adapter.%2520It%2520aligns%2520the%2520token-level%2520embeddings%2520of%2520a%2520compressed%2520model%2520with%2520those%2520of%2520the%2520original%2520large%2520model.%2520AlAd%2520preserves%2520local%2520contextual%2520semantics%252C%2520enables%2520flexible%2520alignment%2520across%2520differing%2520dimensionalities%2520or%2520architectures%252C%2520and%2520is%2520entirely%2520agnostic%2520to%2520the%2520underlying%2520compression%2520method.%2520AlAd%2520can%2520be%2520deployed%2520in%2520two%2520ways%253A%2520as%2520a%2520plug-and-play%2520module%2520over%2520a%2520frozen%2520compressed%2520model%252C%2520or%2520by%2520jointly%2520fine-tuning%2520AlAd%2520with%2520the%2520compressed%2520model%2520for%2520further%2520performance%2520gains.%2520Through%2520experiments%2520on%2520BERT-family%2520models%2520across%2520three%2520token-level%2520NLP%2520tasks%252C%2520we%2520demonstrate%2520that%2520AlAd%2520significantly%2520boosts%2520the%2520performance%2520of%2520compressed%2520models%2520with%2520only%2520marginal%2520overhead%2520in%2520size%2520and%2520latency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment%20Adapter%20to%20Improve%20the%20Performance%20of%20Compressed%20Deep%20Learning%20Models&entry.906535625=Rohit%20Raj%20Rai%20and%20Abhishek%20Dhaka%20and%20Amit%20Awekar&entry.1292438233=Compressed%20Deep%20Learning%20%28DL%29%20models%20are%20essential%20for%20deployment%20in%20resource-constrained%20environments.%20But%20their%20performance%20often%20lags%20behind%20their%20large-scale%20counterparts.%20To%20bridge%20this%20gap%2C%20we%20propose%20Alignment%20Adapter%20%28AlAd%29%3A%20a%20lightweight%2C%20sliding-window-based%20adapter.%20It%20aligns%20the%20token-level%20embeddings%20of%20a%20compressed%20model%20with%20those%20of%20the%20original%20large%20model.%20AlAd%20preserves%20local%20contextual%20semantics%2C%20enables%20flexible%20alignment%20across%20differing%20dimensionalities%20or%20architectures%2C%20and%20is%20entirely%20agnostic%20to%20the%20underlying%20compression%20method.%20AlAd%20can%20be%20deployed%20in%20two%20ways%3A%20as%20a%20plug-and-play%20module%20over%20a%20frozen%20compressed%20model%2C%20or%20by%20jointly%20fine-tuning%20AlAd%20with%20the%20compressed%20model%20for%20further%20performance%20gains.%20Through%20experiments%20on%20BERT-family%20models%20across%20three%20token-level%20NLP%20tasks%2C%20we%20demonstrate%20that%20AlAd%20significantly%20boosts%20the%20performance%20of%20compressed%20models%20with%20only%20marginal%20overhead%20in%20size%20and%20latency.&entry.1838667208=http%3A//arxiv.org/abs/2602.14635v1&entry.124074799=Read"},
{"title": "BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations", "author": "Jonathan Gorard and Ammar Hakim and James Juno", "abstract": "The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.", "link": "http://arxiv.org/abs/2602.14853v1", "date": "2026-02-16", "relevancy": 2.0076, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5151}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4941}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEACONS%3A%20Bounded-Error%2C%20Algebraically-Composable%20Neural%20Solvers%20for%20Partial%20Differential%20Equations&body=Title%3A%20BEACONS%3A%20Bounded-Error%2C%20Algebraically-Composable%20Neural%20Solvers%20for%20Partial%20Differential%20Equations%0AAuthor%3A%20Jonathan%20Gorard%20and%20Ammar%20Hakim%20and%20James%20Juno%0AAbstract%3A%20The%20traditional%20limitations%20of%20neural%20networks%20in%20reliably%20generalizing%20beyond%20the%20convex%20hulls%20of%20their%20training%20data%20present%20a%20significant%20problem%20for%20computational%20physics%2C%20in%20which%20one%20often%20wishes%20to%20solve%20PDEs%20in%20regimes%20far%20beyond%20anything%20which%20can%20be%20experimentally%20or%20analytically%20validated.%20In%20this%20paper%2C%20we%20show%20how%20it%20is%20possible%20to%20circumvent%20these%20limitations%20by%20constructing%20formally-verified%20neural%20network%20solvers%20for%20PDEs%2C%20with%20rigorous%20convergence%2C%20stability%2C%20and%20conservation%20properties%2C%20whose%20correctness%20can%20therefore%20be%20guaranteed%20even%20in%20extrapolatory%20regimes.%20By%20using%20the%20method%20of%20characteristics%20to%20predict%20the%20analytical%20properties%20of%20PDE%20solutions%20a%20priori%20%28even%20in%20regions%20arbitrarily%20far%20from%20the%20training%20domain%29%2C%20we%20show%20how%20it%20is%20possible%20to%20construct%20rigorous%20extrapolatory%20bounds%20on%20the%20worst-case%20L%5Einf%20errors%20of%20shallow%20neural%20network%20approximations.%20Then%2C%20by%20decomposing%20PDE%20solutions%20into%20compositions%20of%20simpler%20functions%2C%20we%20show%20how%20it%20is%20possible%20to%20compose%20these%20shallow%20neural%20networks%20together%20to%20form%20deep%20architectures%2C%20based%20on%20ideas%20from%20compositional%20deep%20learning%2C%20in%20which%20the%20large%20L%5Einf%20errors%20in%20the%20approximations%20have%20been%20suppressed.%20The%20resulting%20framework%2C%20called%20BEACONS%20%28Bounded-Error%2C%20Algebraically-COmposable%20Neural%20Solvers%29%2C%20comprises%20both%20an%20automatic%20code-generator%20for%20the%20neural%20solvers%20themselves%2C%20as%20well%20as%20a%20bespoke%20automated%20theorem-proving%20system%20for%20producing%20machine-checkable%20certificates%20of%20correctness.%20We%20apply%20the%20framework%20to%20a%20variety%20of%20linear%20and%20non-linear%20PDEs%2C%20including%20the%20linear%20advection%20and%20inviscid%20Burgers%27%20equations%2C%20as%20well%20as%20the%20full%20compressible%20Euler%20equations%2C%20in%20both%201D%20and%202D%2C%20and%20illustrate%20how%20BEACONS%20architectures%20are%20able%20to%20extrapolate%20solutions%20far%20beyond%20the%20training%20data%20in%20a%20reliable%20and%20bounded%20way.%20Various%20advantages%20of%20the%20approach%20over%20the%20classical%20PINN%20approach%20are%20discussed.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEACONS%253A%2520Bounded-Error%252C%2520Algebraically-Composable%2520Neural%2520Solvers%2520for%2520Partial%2520Differential%2520Equations%26entry.906535625%3DJonathan%2520Gorard%2520and%2520Ammar%2520Hakim%2520and%2520James%2520Juno%26entry.1292438233%3DThe%2520traditional%2520limitations%2520of%2520neural%2520networks%2520in%2520reliably%2520generalizing%2520beyond%2520the%2520convex%2520hulls%2520of%2520their%2520training%2520data%2520present%2520a%2520significant%2520problem%2520for%2520computational%2520physics%252C%2520in%2520which%2520one%2520often%2520wishes%2520to%2520solve%2520PDEs%2520in%2520regimes%2520far%2520beyond%2520anything%2520which%2520can%2520be%2520experimentally%2520or%2520analytically%2520validated.%2520In%2520this%2520paper%252C%2520we%2520show%2520how%2520it%2520is%2520possible%2520to%2520circumvent%2520these%2520limitations%2520by%2520constructing%2520formally-verified%2520neural%2520network%2520solvers%2520for%2520PDEs%252C%2520with%2520rigorous%2520convergence%252C%2520stability%252C%2520and%2520conservation%2520properties%252C%2520whose%2520correctness%2520can%2520therefore%2520be%2520guaranteed%2520even%2520in%2520extrapolatory%2520regimes.%2520By%2520using%2520the%2520method%2520of%2520characteristics%2520to%2520predict%2520the%2520analytical%2520properties%2520of%2520PDE%2520solutions%2520a%2520priori%2520%2528even%2520in%2520regions%2520arbitrarily%2520far%2520from%2520the%2520training%2520domain%2529%252C%2520we%2520show%2520how%2520it%2520is%2520possible%2520to%2520construct%2520rigorous%2520extrapolatory%2520bounds%2520on%2520the%2520worst-case%2520L%255Einf%2520errors%2520of%2520shallow%2520neural%2520network%2520approximations.%2520Then%252C%2520by%2520decomposing%2520PDE%2520solutions%2520into%2520compositions%2520of%2520simpler%2520functions%252C%2520we%2520show%2520how%2520it%2520is%2520possible%2520to%2520compose%2520these%2520shallow%2520neural%2520networks%2520together%2520to%2520form%2520deep%2520architectures%252C%2520based%2520on%2520ideas%2520from%2520compositional%2520deep%2520learning%252C%2520in%2520which%2520the%2520large%2520L%255Einf%2520errors%2520in%2520the%2520approximations%2520have%2520been%2520suppressed.%2520The%2520resulting%2520framework%252C%2520called%2520BEACONS%2520%2528Bounded-Error%252C%2520Algebraically-COmposable%2520Neural%2520Solvers%2529%252C%2520comprises%2520both%2520an%2520automatic%2520code-generator%2520for%2520the%2520neural%2520solvers%2520themselves%252C%2520as%2520well%2520as%2520a%2520bespoke%2520automated%2520theorem-proving%2520system%2520for%2520producing%2520machine-checkable%2520certificates%2520of%2520correctness.%2520We%2520apply%2520the%2520framework%2520to%2520a%2520variety%2520of%2520linear%2520and%2520non-linear%2520PDEs%252C%2520including%2520the%2520linear%2520advection%2520and%2520inviscid%2520Burgers%2527%2520equations%252C%2520as%2520well%2520as%2520the%2520full%2520compressible%2520Euler%2520equations%252C%2520in%2520both%25201D%2520and%25202D%252C%2520and%2520illustrate%2520how%2520BEACONS%2520architectures%2520are%2520able%2520to%2520extrapolate%2520solutions%2520far%2520beyond%2520the%2520training%2520data%2520in%2520a%2520reliable%2520and%2520bounded%2520way.%2520Various%2520advantages%2520of%2520the%2520approach%2520over%2520the%2520classical%2520PINN%2520approach%2520are%2520discussed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEACONS%3A%20Bounded-Error%2C%20Algebraically-Composable%20Neural%20Solvers%20for%20Partial%20Differential%20Equations&entry.906535625=Jonathan%20Gorard%20and%20Ammar%20Hakim%20and%20James%20Juno&entry.1292438233=The%20traditional%20limitations%20of%20neural%20networks%20in%20reliably%20generalizing%20beyond%20the%20convex%20hulls%20of%20their%20training%20data%20present%20a%20significant%20problem%20for%20computational%20physics%2C%20in%20which%20one%20often%20wishes%20to%20solve%20PDEs%20in%20regimes%20far%20beyond%20anything%20which%20can%20be%20experimentally%20or%20analytically%20validated.%20In%20this%20paper%2C%20we%20show%20how%20it%20is%20possible%20to%20circumvent%20these%20limitations%20by%20constructing%20formally-verified%20neural%20network%20solvers%20for%20PDEs%2C%20with%20rigorous%20convergence%2C%20stability%2C%20and%20conservation%20properties%2C%20whose%20correctness%20can%20therefore%20be%20guaranteed%20even%20in%20extrapolatory%20regimes.%20By%20using%20the%20method%20of%20characteristics%20to%20predict%20the%20analytical%20properties%20of%20PDE%20solutions%20a%20priori%20%28even%20in%20regions%20arbitrarily%20far%20from%20the%20training%20domain%29%2C%20we%20show%20how%20it%20is%20possible%20to%20construct%20rigorous%20extrapolatory%20bounds%20on%20the%20worst-case%20L%5Einf%20errors%20of%20shallow%20neural%20network%20approximations.%20Then%2C%20by%20decomposing%20PDE%20solutions%20into%20compositions%20of%20simpler%20functions%2C%20we%20show%20how%20it%20is%20possible%20to%20compose%20these%20shallow%20neural%20networks%20together%20to%20form%20deep%20architectures%2C%20based%20on%20ideas%20from%20compositional%20deep%20learning%2C%20in%20which%20the%20large%20L%5Einf%20errors%20in%20the%20approximations%20have%20been%20suppressed.%20The%20resulting%20framework%2C%20called%20BEACONS%20%28Bounded-Error%2C%20Algebraically-COmposable%20Neural%20Solvers%29%2C%20comprises%20both%20an%20automatic%20code-generator%20for%20the%20neural%20solvers%20themselves%2C%20as%20well%20as%20a%20bespoke%20automated%20theorem-proving%20system%20for%20producing%20machine-checkable%20certificates%20of%20correctness.%20We%20apply%20the%20framework%20to%20a%20variety%20of%20linear%20and%20non-linear%20PDEs%2C%20including%20the%20linear%20advection%20and%20inviscid%20Burgers%27%20equations%2C%20as%20well%20as%20the%20full%20compressible%20Euler%20equations%2C%20in%20both%201D%20and%202D%2C%20and%20illustrate%20how%20BEACONS%20architectures%20are%20able%20to%20extrapolate%20solutions%20far%20beyond%20the%20training%20data%20in%20a%20reliable%20and%20bounded%20way.%20Various%20advantages%20of%20the%20approach%20over%20the%20classical%20PINN%20approach%20are%20discussed.&entry.1838667208=http%3A//arxiv.org/abs/2602.14853v1&entry.124074799=Read"},
{"title": "Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training", "author": "Jonathan Lys and Vincent Gripon and Bastien Pasdeloup and Lukas Mauch and Fabien Cardinaux and Ghouthi Boukli Hacene", "abstract": "Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.", "link": "http://arxiv.org/abs/2602.14759v1", "date": "2026-02-16", "relevancy": 2.0026, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5512}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4921}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inner%20Loop%20Inference%20for%20Pretrained%20Transformers%3A%20Unlocking%20Latent%20Capabilities%20Without%20Training&body=Title%3A%20Inner%20Loop%20Inference%20for%20Pretrained%20Transformers%3A%20Unlocking%20Latent%20Capabilities%20Without%20Training%0AAuthor%3A%20Jonathan%20Lys%20and%20Vincent%20Gripon%20and%20Bastien%20Pasdeloup%20and%20Lukas%20Mauch%20and%20Fabien%20Cardinaux%20and%20Ghouthi%20Boukli%20Hacene%0AAbstract%3A%20Deep%20Learning%20architectures%2C%20and%20in%20particular%20Transformers%2C%20are%20conventionally%20viewed%20as%20a%20composition%20of%20layers.%20These%20layers%20are%20actually%20often%20obtained%20as%20the%20sum%20of%20two%20contributions%3A%20a%20residual%20path%20that%20copies%20the%20input%20and%20the%20output%20of%20a%20Transformer%20block.%20As%20a%20consequence%2C%20the%20inner%20representations%20%28i.e.%20the%20input%20of%20these%20blocks%29%20can%20be%20interpreted%20as%20iterative%20refinement%20of%20a%20propagated%20latent%20representation.%20Under%20this%20lens%2C%20many%20works%20suggest%20that%20the%20inner%20space%20is%20shared%20across%20layers%2C%20meaning%20that%20tokens%20can%20be%20decoded%20at%20early%20stages.%20Mechanistic%20interpretability%20even%20goes%20further%20by%20conjecturing%20that%20some%20layers%20act%20as%20refinement%20layers.%20Following%20this%20path%2C%20we%20propose%20inference-time%20inner%20looping%2C%20which%20prolongs%20refinement%20in%20pretrained%20off-the-shelf%20language%20models%20by%20repeatedly%20re-applying%20a%20selected%20block%20range.%20Across%20multiple%20benchmarks%2C%20inner%20looping%20yields%20modest%20but%20consistent%20accuracy%20improvements.%20Analyses%20of%20the%20resulting%20latent%20trajectories%20suggest%20more%20stable%20state%20evolution%20and%20continued%20semantic%20refinement.%20Overall%2C%20our%20results%20suggest%20that%20additional%20refinement%20can%20be%20obtained%20through%20simple%20test-time%20looping%2C%20extending%20computation%20in%20frozen%20pretrained%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14759v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInner%2520Loop%2520Inference%2520for%2520Pretrained%2520Transformers%253A%2520Unlocking%2520Latent%2520Capabilities%2520Without%2520Training%26entry.906535625%3DJonathan%2520Lys%2520and%2520Vincent%2520Gripon%2520and%2520Bastien%2520Pasdeloup%2520and%2520Lukas%2520Mauch%2520and%2520Fabien%2520Cardinaux%2520and%2520Ghouthi%2520Boukli%2520Hacene%26entry.1292438233%3DDeep%2520Learning%2520architectures%252C%2520and%2520in%2520particular%2520Transformers%252C%2520are%2520conventionally%2520viewed%2520as%2520a%2520composition%2520of%2520layers.%2520These%2520layers%2520are%2520actually%2520often%2520obtained%2520as%2520the%2520sum%2520of%2520two%2520contributions%253A%2520a%2520residual%2520path%2520that%2520copies%2520the%2520input%2520and%2520the%2520output%2520of%2520a%2520Transformer%2520block.%2520As%2520a%2520consequence%252C%2520the%2520inner%2520representations%2520%2528i.e.%2520the%2520input%2520of%2520these%2520blocks%2529%2520can%2520be%2520interpreted%2520as%2520iterative%2520refinement%2520of%2520a%2520propagated%2520latent%2520representation.%2520Under%2520this%2520lens%252C%2520many%2520works%2520suggest%2520that%2520the%2520inner%2520space%2520is%2520shared%2520across%2520layers%252C%2520meaning%2520that%2520tokens%2520can%2520be%2520decoded%2520at%2520early%2520stages.%2520Mechanistic%2520interpretability%2520even%2520goes%2520further%2520by%2520conjecturing%2520that%2520some%2520layers%2520act%2520as%2520refinement%2520layers.%2520Following%2520this%2520path%252C%2520we%2520propose%2520inference-time%2520inner%2520looping%252C%2520which%2520prolongs%2520refinement%2520in%2520pretrained%2520off-the-shelf%2520language%2520models%2520by%2520repeatedly%2520re-applying%2520a%2520selected%2520block%2520range.%2520Across%2520multiple%2520benchmarks%252C%2520inner%2520looping%2520yields%2520modest%2520but%2520consistent%2520accuracy%2520improvements.%2520Analyses%2520of%2520the%2520resulting%2520latent%2520trajectories%2520suggest%2520more%2520stable%2520state%2520evolution%2520and%2520continued%2520semantic%2520refinement.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520additional%2520refinement%2520can%2520be%2520obtained%2520through%2520simple%2520test-time%2520looping%252C%2520extending%2520computation%2520in%2520frozen%2520pretrained%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14759v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inner%20Loop%20Inference%20for%20Pretrained%20Transformers%3A%20Unlocking%20Latent%20Capabilities%20Without%20Training&entry.906535625=Jonathan%20Lys%20and%20Vincent%20Gripon%20and%20Bastien%20Pasdeloup%20and%20Lukas%20Mauch%20and%20Fabien%20Cardinaux%20and%20Ghouthi%20Boukli%20Hacene&entry.1292438233=Deep%20Learning%20architectures%2C%20and%20in%20particular%20Transformers%2C%20are%20conventionally%20viewed%20as%20a%20composition%20of%20layers.%20These%20layers%20are%20actually%20often%20obtained%20as%20the%20sum%20of%20two%20contributions%3A%20a%20residual%20path%20that%20copies%20the%20input%20and%20the%20output%20of%20a%20Transformer%20block.%20As%20a%20consequence%2C%20the%20inner%20representations%20%28i.e.%20the%20input%20of%20these%20blocks%29%20can%20be%20interpreted%20as%20iterative%20refinement%20of%20a%20propagated%20latent%20representation.%20Under%20this%20lens%2C%20many%20works%20suggest%20that%20the%20inner%20space%20is%20shared%20across%20layers%2C%20meaning%20that%20tokens%20can%20be%20decoded%20at%20early%20stages.%20Mechanistic%20interpretability%20even%20goes%20further%20by%20conjecturing%20that%20some%20layers%20act%20as%20refinement%20layers.%20Following%20this%20path%2C%20we%20propose%20inference-time%20inner%20looping%2C%20which%20prolongs%20refinement%20in%20pretrained%20off-the-shelf%20language%20models%20by%20repeatedly%20re-applying%20a%20selected%20block%20range.%20Across%20multiple%20benchmarks%2C%20inner%20looping%20yields%20modest%20but%20consistent%20accuracy%20improvements.%20Analyses%20of%20the%20resulting%20latent%20trajectories%20suggest%20more%20stable%20state%20evolution%20and%20continued%20semantic%20refinement.%20Overall%2C%20our%20results%20suggest%20that%20additional%20refinement%20can%20be%20obtained%20through%20simple%20test-time%20looping%2C%20extending%20computation%20in%20frozen%20pretrained%20models.&entry.1838667208=http%3A//arxiv.org/abs/2602.14759v1&entry.124074799=Read"},
{"title": "Gradient Networks for Universal Magnetic Modeling of Synchronous Machines", "author": "Junyi Li and Tim Foissner and Floran Martin and Antti Piippo and Marko Hinkkanen", "abstract": "This paper presents a physics-informed neural network approach for dynamic modeling of saturable synchronous machines, including cases with spatial harmonics. We introduce an architecture that incorporates gradient networks directly into the fundamental machine equations, enabling accurate modeling of the nonlinear and coupled electromagnetic constitutive relationship. By learning the gradient of the magnetic field energy, the model inherently satisfies energy balance (reciprocity conditions). The proposed architecture can universally approximate any physically feasible magnetic behavior and offers several advantages over lookup tables and standard machine learning models: it requires less training data, ensures monotonicity and reliable extrapolation, and produces smooth outputs. These properties further enable robust model inversion and optimal trajectory generation, often needed in control applications. We validate the proposed approach using measured and finite-element method (FEM) datasets from a 5.6-kW permanent-magnet (PM) synchronous reluctance machine. Results demonstrate accurate and physically consistent models, even with limited training data.", "link": "http://arxiv.org/abs/2602.14947v1", "date": "2026-02-16", "relevancy": 2.0021, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5135}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4955}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Networks%20for%20Universal%20Magnetic%20Modeling%20of%20Synchronous%20Machines&body=Title%3A%20Gradient%20Networks%20for%20Universal%20Magnetic%20Modeling%20of%20Synchronous%20Machines%0AAuthor%3A%20Junyi%20Li%20and%20Tim%20Foissner%20and%20Floran%20Martin%20and%20Antti%20Piippo%20and%20Marko%20Hinkkanen%0AAbstract%3A%20This%20paper%20presents%20a%20physics-informed%20neural%20network%20approach%20for%20dynamic%20modeling%20of%20saturable%20synchronous%20machines%2C%20including%20cases%20with%20spatial%20harmonics.%20We%20introduce%20an%20architecture%20that%20incorporates%20gradient%20networks%20directly%20into%20the%20fundamental%20machine%20equations%2C%20enabling%20accurate%20modeling%20of%20the%20nonlinear%20and%20coupled%20electromagnetic%20constitutive%20relationship.%20By%20learning%20the%20gradient%20of%20the%20magnetic%20field%20energy%2C%20the%20model%20inherently%20satisfies%20energy%20balance%20%28reciprocity%20conditions%29.%20The%20proposed%20architecture%20can%20universally%20approximate%20any%20physically%20feasible%20magnetic%20behavior%20and%20offers%20several%20advantages%20over%20lookup%20tables%20and%20standard%20machine%20learning%20models%3A%20it%20requires%20less%20training%20data%2C%20ensures%20monotonicity%20and%20reliable%20extrapolation%2C%20and%20produces%20smooth%20outputs.%20These%20properties%20further%20enable%20robust%20model%20inversion%20and%20optimal%20trajectory%20generation%2C%20often%20needed%20in%20control%20applications.%20We%20validate%20the%20proposed%20approach%20using%20measured%20and%20finite-element%20method%20%28FEM%29%20datasets%20from%20a%205.6-kW%20permanent-magnet%20%28PM%29%20synchronous%20reluctance%20machine.%20Results%20demonstrate%20accurate%20and%20physically%20consistent%20models%2C%20even%20with%20limited%20training%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Networks%2520for%2520Universal%2520Magnetic%2520Modeling%2520of%2520Synchronous%2520Machines%26entry.906535625%3DJunyi%2520Li%2520and%2520Tim%2520Foissner%2520and%2520Floran%2520Martin%2520and%2520Antti%2520Piippo%2520and%2520Marko%2520Hinkkanen%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520physics-informed%2520neural%2520network%2520approach%2520for%2520dynamic%2520modeling%2520of%2520saturable%2520synchronous%2520machines%252C%2520including%2520cases%2520with%2520spatial%2520harmonics.%2520We%2520introduce%2520an%2520architecture%2520that%2520incorporates%2520gradient%2520networks%2520directly%2520into%2520the%2520fundamental%2520machine%2520equations%252C%2520enabling%2520accurate%2520modeling%2520of%2520the%2520nonlinear%2520and%2520coupled%2520electromagnetic%2520constitutive%2520relationship.%2520By%2520learning%2520the%2520gradient%2520of%2520the%2520magnetic%2520field%2520energy%252C%2520the%2520model%2520inherently%2520satisfies%2520energy%2520balance%2520%2528reciprocity%2520conditions%2529.%2520The%2520proposed%2520architecture%2520can%2520universally%2520approximate%2520any%2520physically%2520feasible%2520magnetic%2520behavior%2520and%2520offers%2520several%2520advantages%2520over%2520lookup%2520tables%2520and%2520standard%2520machine%2520learning%2520models%253A%2520it%2520requires%2520less%2520training%2520data%252C%2520ensures%2520monotonicity%2520and%2520reliable%2520extrapolation%252C%2520and%2520produces%2520smooth%2520outputs.%2520These%2520properties%2520further%2520enable%2520robust%2520model%2520inversion%2520and%2520optimal%2520trajectory%2520generation%252C%2520often%2520needed%2520in%2520control%2520applications.%2520We%2520validate%2520the%2520proposed%2520approach%2520using%2520measured%2520and%2520finite-element%2520method%2520%2528FEM%2529%2520datasets%2520from%2520a%25205.6-kW%2520permanent-magnet%2520%2528PM%2529%2520synchronous%2520reluctance%2520machine.%2520Results%2520demonstrate%2520accurate%2520and%2520physically%2520consistent%2520models%252C%2520even%2520with%2520limited%2520training%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Networks%20for%20Universal%20Magnetic%20Modeling%20of%20Synchronous%20Machines&entry.906535625=Junyi%20Li%20and%20Tim%20Foissner%20and%20Floran%20Martin%20and%20Antti%20Piippo%20and%20Marko%20Hinkkanen&entry.1292438233=This%20paper%20presents%20a%20physics-informed%20neural%20network%20approach%20for%20dynamic%20modeling%20of%20saturable%20synchronous%20machines%2C%20including%20cases%20with%20spatial%20harmonics.%20We%20introduce%20an%20architecture%20that%20incorporates%20gradient%20networks%20directly%20into%20the%20fundamental%20machine%20equations%2C%20enabling%20accurate%20modeling%20of%20the%20nonlinear%20and%20coupled%20electromagnetic%20constitutive%20relationship.%20By%20learning%20the%20gradient%20of%20the%20magnetic%20field%20energy%2C%20the%20model%20inherently%20satisfies%20energy%20balance%20%28reciprocity%20conditions%29.%20The%20proposed%20architecture%20can%20universally%20approximate%20any%20physically%20feasible%20magnetic%20behavior%20and%20offers%20several%20advantages%20over%20lookup%20tables%20and%20standard%20machine%20learning%20models%3A%20it%20requires%20less%20training%20data%2C%20ensures%20monotonicity%20and%20reliable%20extrapolation%2C%20and%20produces%20smooth%20outputs.%20These%20properties%20further%20enable%20robust%20model%20inversion%20and%20optimal%20trajectory%20generation%2C%20often%20needed%20in%20control%20applications.%20We%20validate%20the%20proposed%20approach%20using%20measured%20and%20finite-element%20method%20%28FEM%29%20datasets%20from%20a%205.6-kW%20permanent-magnet%20%28PM%29%20synchronous%20reluctance%20machine.%20Results%20demonstrate%20accurate%20and%20physically%20consistent%20models%2C%20even%20with%20limited%20training%20data.&entry.1838667208=http%3A//arxiv.org/abs/2602.14947v1&entry.124074799=Read"},
{"title": "MeFEm: Medical Face Embedding model", "author": "Yury Borets and Stepan Botman", "abstract": "We present MeFEm, a vision model based on a modified Joint Embedding Predictive Architecture (JEPA) for biometric and medical analysis from facial images. Key modifications include an axial stripe masking strategy to focus learning on semantically relevant regions, a circular loss weighting scheme, and the probabilistic reassignment of the CLS token for high quality linear probing. Trained on a consolidated dataset of curated images, MeFEm outperforms strong baselines like FaRL and Franca on core anthropometric tasks despite using significantly less data. It also shows promising results on Body Mass Index (BMI) estimation, evaluated on a novel, consolidated closed-source dataset that addresses the domain bias prevalent in existing data. Model weights are available at https://huggingface.co/boretsyury/MeFEm , offering a strong baseline for future work in this domain.", "link": "http://arxiv.org/abs/2602.14672v1", "date": "2026-02-16", "relevancy": 1.9973, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5324}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeFEm%3A%20Medical%20Face%20Embedding%20model&body=Title%3A%20MeFEm%3A%20Medical%20Face%20Embedding%20model%0AAuthor%3A%20Yury%20Borets%20and%20Stepan%20Botman%0AAbstract%3A%20We%20present%20MeFEm%2C%20a%20vision%20model%20based%20on%20a%20modified%20Joint%20Embedding%20Predictive%20Architecture%20%28JEPA%29%20for%20biometric%20and%20medical%20analysis%20from%20facial%20images.%20Key%20modifications%20include%20an%20axial%20stripe%20masking%20strategy%20to%20focus%20learning%20on%20semantically%20relevant%20regions%2C%20a%20circular%20loss%20weighting%20scheme%2C%20and%20the%20probabilistic%20reassignment%20of%20the%20CLS%20token%20for%20high%20quality%20linear%20probing.%20Trained%20on%20a%20consolidated%20dataset%20of%20curated%20images%2C%20MeFEm%20outperforms%20strong%20baselines%20like%20FaRL%20and%20Franca%20on%20core%20anthropometric%20tasks%20despite%20using%20significantly%20less%20data.%20It%20also%20shows%20promising%20results%20on%20Body%20Mass%20Index%20%28BMI%29%20estimation%2C%20evaluated%20on%20a%20novel%2C%20consolidated%20closed-source%20dataset%20that%20addresses%20the%20domain%20bias%20prevalent%20in%20existing%20data.%20Model%20weights%20are%20available%20at%20https%3A//huggingface.co/boretsyury/MeFEm%20%2C%20offering%20a%20strong%20baseline%20for%20future%20work%20in%20this%20domain.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeFEm%253A%2520Medical%2520Face%2520Embedding%2520model%26entry.906535625%3DYury%2520Borets%2520and%2520Stepan%2520Botman%26entry.1292438233%3DWe%2520present%2520MeFEm%252C%2520a%2520vision%2520model%2520based%2520on%2520a%2520modified%2520Joint%2520Embedding%2520Predictive%2520Architecture%2520%2528JEPA%2529%2520for%2520biometric%2520and%2520medical%2520analysis%2520from%2520facial%2520images.%2520Key%2520modifications%2520include%2520an%2520axial%2520stripe%2520masking%2520strategy%2520to%2520focus%2520learning%2520on%2520semantically%2520relevant%2520regions%252C%2520a%2520circular%2520loss%2520weighting%2520scheme%252C%2520and%2520the%2520probabilistic%2520reassignment%2520of%2520the%2520CLS%2520token%2520for%2520high%2520quality%2520linear%2520probing.%2520Trained%2520on%2520a%2520consolidated%2520dataset%2520of%2520curated%2520images%252C%2520MeFEm%2520outperforms%2520strong%2520baselines%2520like%2520FaRL%2520and%2520Franca%2520on%2520core%2520anthropometric%2520tasks%2520despite%2520using%2520significantly%2520less%2520data.%2520It%2520also%2520shows%2520promising%2520results%2520on%2520Body%2520Mass%2520Index%2520%2528BMI%2529%2520estimation%252C%2520evaluated%2520on%2520a%2520novel%252C%2520consolidated%2520closed-source%2520dataset%2520that%2520addresses%2520the%2520domain%2520bias%2520prevalent%2520in%2520existing%2520data.%2520Model%2520weights%2520are%2520available%2520at%2520https%253A//huggingface.co/boretsyury/MeFEm%2520%252C%2520offering%2520a%2520strong%2520baseline%2520for%2520future%2520work%2520in%2520this%2520domain.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeFEm%3A%20Medical%20Face%20Embedding%20model&entry.906535625=Yury%20Borets%20and%20Stepan%20Botman&entry.1292438233=We%20present%20MeFEm%2C%20a%20vision%20model%20based%20on%20a%20modified%20Joint%20Embedding%20Predictive%20Architecture%20%28JEPA%29%20for%20biometric%20and%20medical%20analysis%20from%20facial%20images.%20Key%20modifications%20include%20an%20axial%20stripe%20masking%20strategy%20to%20focus%20learning%20on%20semantically%20relevant%20regions%2C%20a%20circular%20loss%20weighting%20scheme%2C%20and%20the%20probabilistic%20reassignment%20of%20the%20CLS%20token%20for%20high%20quality%20linear%20probing.%20Trained%20on%20a%20consolidated%20dataset%20of%20curated%20images%2C%20MeFEm%20outperforms%20strong%20baselines%20like%20FaRL%20and%20Franca%20on%20core%20anthropometric%20tasks%20despite%20using%20significantly%20less%20data.%20It%20also%20shows%20promising%20results%20on%20Body%20Mass%20Index%20%28BMI%29%20estimation%2C%20evaluated%20on%20a%20novel%2C%20consolidated%20closed-source%20dataset%20that%20addresses%20the%20domain%20bias%20prevalent%20in%20existing%20data.%20Model%20weights%20are%20available%20at%20https%3A//huggingface.co/boretsyury/MeFEm%20%2C%20offering%20a%20strong%20baseline%20for%20future%20work%20in%20this%20domain.&entry.1838667208=http%3A//arxiv.org/abs/2602.14672v1&entry.124074799=Read"},
{"title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving", "author": "Rui Li and Zhaoning Zhang and Libo Zhang and Huaimin Wang and Xiang Fu and Zhiquan Lai", "abstract": "Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.", "link": "http://arxiv.org/abs/2512.22420v2", "date": "2026-02-16", "relevancy": 1.9952, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5505}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nightjar%3A%20Dynamic%20Adaptive%20Speculative%20Decoding%20for%20Large%20Language%20Models%20Serving&body=Title%3A%20Nightjar%3A%20Dynamic%20Adaptive%20Speculative%20Decoding%20for%20Large%20Language%20Models%20Serving%0AAuthor%3A%20Rui%20Li%20and%20Zhaoning%20Zhang%20and%20Libo%20Zhang%20and%20Huaimin%20Wang%20and%20Xiang%20Fu%20and%20Zhiquan%20Lai%0AAbstract%3A%20Speculative%20decoding%20%28SD%29%20accelerates%20LLM%20inference%20by%20verifying%20draft%20tokens%20in%20parallel.%20However%2C%20this%20method%20presents%20a%20critical%20trade-off%3A%20it%20improves%20throughput%20in%20low-load%2C%20memory-bound%20systems%20but%20degrades%20performance%20in%20high-load%2C%20compute-bound%20environments%20due%20to%20verification%20overhead.%20Current%20SD%20implementations%20use%20a%20fixed%20speculative%20length%2C%20failing%20to%20adapt%20to%20dynamic%20request%20rates%20and%20creating%20a%20significant%20performance%20bottleneck%20in%20real-world%20serving%20scenarios.%20To%20overcome%20this%2C%20we%20propose%20Nightjar%2C%20a%20novel%20learning-based%20algorithm%20for%20adaptive%20speculative%20inference%20that%20adjusts%20to%20request%20load%20by%20dynamically%20selecting%20the%20optimal%20speculative%20length%20for%20different%20batch%20sizes%20and%20even%20disabling%20speculative%20decoding%20when%20it%20provides%20no%20benefit.%20Experiments%20show%20that%20Nightjar%20achieves%20up%20to%2014.8%25%20higher%20throughput%20and%2020.2%25%20lower%20latency%20compared%20to%20standard%20speculative%20decoding%2C%20demonstrating%20robust%20efficiency%20for%20real-time%20serving.%0ALink%3A%20http%3A//arxiv.org/abs/2512.22420v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNightjar%253A%2520Dynamic%2520Adaptive%2520Speculative%2520Decoding%2520for%2520Large%2520Language%2520Models%2520Serving%26entry.906535625%3DRui%2520Li%2520and%2520Zhaoning%2520Zhang%2520and%2520Libo%2520Zhang%2520and%2520Huaimin%2520Wang%2520and%2520Xiang%2520Fu%2520and%2520Zhiquan%2520Lai%26entry.1292438233%3DSpeculative%2520decoding%2520%2528SD%2529%2520accelerates%2520LLM%2520inference%2520by%2520verifying%2520draft%2520tokens%2520in%2520parallel.%2520However%252C%2520this%2520method%2520presents%2520a%2520critical%2520trade-off%253A%2520it%2520improves%2520throughput%2520in%2520low-load%252C%2520memory-bound%2520systems%2520but%2520degrades%2520performance%2520in%2520high-load%252C%2520compute-bound%2520environments%2520due%2520to%2520verification%2520overhead.%2520Current%2520SD%2520implementations%2520use%2520a%2520fixed%2520speculative%2520length%252C%2520failing%2520to%2520adapt%2520to%2520dynamic%2520request%2520rates%2520and%2520creating%2520a%2520significant%2520performance%2520bottleneck%2520in%2520real-world%2520serving%2520scenarios.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520Nightjar%252C%2520a%2520novel%2520learning-based%2520algorithm%2520for%2520adaptive%2520speculative%2520inference%2520that%2520adjusts%2520to%2520request%2520load%2520by%2520dynamically%2520selecting%2520the%2520optimal%2520speculative%2520length%2520for%2520different%2520batch%2520sizes%2520and%2520even%2520disabling%2520speculative%2520decoding%2520when%2520it%2520provides%2520no%2520benefit.%2520Experiments%2520show%2520that%2520Nightjar%2520achieves%2520up%2520to%252014.8%2525%2520higher%2520throughput%2520and%252020.2%2525%2520lower%2520latency%2520compared%2520to%2520standard%2520speculative%2520decoding%252C%2520demonstrating%2520robust%2520efficiency%2520for%2520real-time%2520serving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.22420v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nightjar%3A%20Dynamic%20Adaptive%20Speculative%20Decoding%20for%20Large%20Language%20Models%20Serving&entry.906535625=Rui%20Li%20and%20Zhaoning%20Zhang%20and%20Libo%20Zhang%20and%20Huaimin%20Wang%20and%20Xiang%20Fu%20and%20Zhiquan%20Lai&entry.1292438233=Speculative%20decoding%20%28SD%29%20accelerates%20LLM%20inference%20by%20verifying%20draft%20tokens%20in%20parallel.%20However%2C%20this%20method%20presents%20a%20critical%20trade-off%3A%20it%20improves%20throughput%20in%20low-load%2C%20memory-bound%20systems%20but%20degrades%20performance%20in%20high-load%2C%20compute-bound%20environments%20due%20to%20verification%20overhead.%20Current%20SD%20implementations%20use%20a%20fixed%20speculative%20length%2C%20failing%20to%20adapt%20to%20dynamic%20request%20rates%20and%20creating%20a%20significant%20performance%20bottleneck%20in%20real-world%20serving%20scenarios.%20To%20overcome%20this%2C%20we%20propose%20Nightjar%2C%20a%20novel%20learning-based%20algorithm%20for%20adaptive%20speculative%20inference%20that%20adjusts%20to%20request%20load%20by%20dynamically%20selecting%20the%20optimal%20speculative%20length%20for%20different%20batch%20sizes%20and%20even%20disabling%20speculative%20decoding%20when%20it%20provides%20no%20benefit.%20Experiments%20show%20that%20Nightjar%20achieves%20up%20to%2014.8%25%20higher%20throughput%20and%2020.2%25%20lower%20latency%20compared%20to%20standard%20speculative%20decoding%2C%20demonstrating%20robust%20efficiency%20for%20real-time%20serving.&entry.1838667208=http%3A//arxiv.org/abs/2512.22420v2&entry.124074799=Read"},
{"title": "What Can Be Recovered Under Sparse Adversarial Corruption? Assumption-Free Theory for Linear Measurements", "author": "Vishal Halder and Alexandre Reiffers-Masson and Abdeldjalil A\u00efssa-El-Bey and Gugan Thoppe", "abstract": "Let $A \\in \\mathbb{R}^{m \\times n}$ be an arbitrary, known matrix and $e$ a $q$-sparse adversarial vector. Given $y = A x^\\star + e$ and $q$, we seek the smallest robust solution set containing $x^\\star$ that is uniformly recoverable from $y$ without knowing $e$. While exact recovery of $x^\\star$ via strong (and often impractical) structural assumptions on $A$ or $x^\\star$ (e.g., restricted isometry, sparsity) is well studied, recoverability for arbitrary $A$ and $x^\\star$ remains open. Our main result shows that the smallest robust solution set is $x^\\star + \\ker(U)$, where $U$ is the unique projection matrix onto the intersection of rowspaces of all possible submatrices of $A$ obtained by deleting $2q$ rows. Moreover, we prove that every $x$ that minimizes the $\\ell_0$-norm of $y - A x$ lies in $x^\\star + \\ker(U)$, which then gives a constructive approach to recover this set.", "link": "http://arxiv.org/abs/2510.24215v3", "date": "2026-02-16", "relevancy": 1.9908, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.417}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3962}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Can%20Be%20Recovered%20Under%20Sparse%20Adversarial%20Corruption%3F%20Assumption-Free%20Theory%20for%20Linear%20Measurements&body=Title%3A%20What%20Can%20Be%20Recovered%20Under%20Sparse%20Adversarial%20Corruption%3F%20Assumption-Free%20Theory%20for%20Linear%20Measurements%0AAuthor%3A%20Vishal%20Halder%20and%20Alexandre%20Reiffers-Masson%20and%20Abdeldjalil%20A%C3%AFssa-El-Bey%20and%20Gugan%20Thoppe%0AAbstract%3A%20Let%20%24A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D%24%20be%20an%20arbitrary%2C%20known%20matrix%20and%20%24e%24%20a%20%24q%24-sparse%20adversarial%20vector.%20Given%20%24y%20%3D%20A%20x%5E%5Cstar%20%2B%20e%24%20and%20%24q%24%2C%20we%20seek%20the%20smallest%20robust%20solution%20set%20containing%20%24x%5E%5Cstar%24%20that%20is%20uniformly%20recoverable%20from%20%24y%24%20without%20knowing%20%24e%24.%20While%20exact%20recovery%20of%20%24x%5E%5Cstar%24%20via%20strong%20%28and%20often%20impractical%29%20structural%20assumptions%20on%20%24A%24%20or%20%24x%5E%5Cstar%24%20%28e.g.%2C%20restricted%20isometry%2C%20sparsity%29%20is%20well%20studied%2C%20recoverability%20for%20arbitrary%20%24A%24%20and%20%24x%5E%5Cstar%24%20remains%20open.%20Our%20main%20result%20shows%20that%20the%20smallest%20robust%20solution%20set%20is%20%24x%5E%5Cstar%20%2B%20%5Cker%28U%29%24%2C%20where%20%24U%24%20is%20the%20unique%20projection%20matrix%20onto%20the%20intersection%20of%20rowspaces%20of%20all%20possible%20submatrices%20of%20%24A%24%20obtained%20by%20deleting%20%242q%24%20rows.%20Moreover%2C%20we%20prove%20that%20every%20%24x%24%20that%20minimizes%20the%20%24%5Cell_0%24-norm%20of%20%24y%20-%20A%20x%24%20lies%20in%20%24x%5E%5Cstar%20%2B%20%5Cker%28U%29%24%2C%20which%20then%20gives%20a%20constructive%20approach%20to%20recover%20this%20set.%0ALink%3A%20http%3A//arxiv.org/abs/2510.24215v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Can%2520Be%2520Recovered%2520Under%2520Sparse%2520Adversarial%2520Corruption%253F%2520Assumption-Free%2520Theory%2520for%2520Linear%2520Measurements%26entry.906535625%3DVishal%2520Halder%2520and%2520Alexandre%2520Reiffers-Masson%2520and%2520Abdeldjalil%2520A%25C3%25AFssa-El-Bey%2520and%2520Gugan%2520Thoppe%26entry.1292438233%3DLet%2520%2524A%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bm%2520%255Ctimes%2520n%257D%2524%2520be%2520an%2520arbitrary%252C%2520known%2520matrix%2520and%2520%2524e%2524%2520a%2520%2524q%2524-sparse%2520adversarial%2520vector.%2520Given%2520%2524y%2520%253D%2520A%2520x%255E%255Cstar%2520%252B%2520e%2524%2520and%2520%2524q%2524%252C%2520we%2520seek%2520the%2520smallest%2520robust%2520solution%2520set%2520containing%2520%2524x%255E%255Cstar%2524%2520that%2520is%2520uniformly%2520recoverable%2520from%2520%2524y%2524%2520without%2520knowing%2520%2524e%2524.%2520While%2520exact%2520recovery%2520of%2520%2524x%255E%255Cstar%2524%2520via%2520strong%2520%2528and%2520often%2520impractical%2529%2520structural%2520assumptions%2520on%2520%2524A%2524%2520or%2520%2524x%255E%255Cstar%2524%2520%2528e.g.%252C%2520restricted%2520isometry%252C%2520sparsity%2529%2520is%2520well%2520studied%252C%2520recoverability%2520for%2520arbitrary%2520%2524A%2524%2520and%2520%2524x%255E%255Cstar%2524%2520remains%2520open.%2520Our%2520main%2520result%2520shows%2520that%2520the%2520smallest%2520robust%2520solution%2520set%2520is%2520%2524x%255E%255Cstar%2520%252B%2520%255Cker%2528U%2529%2524%252C%2520where%2520%2524U%2524%2520is%2520the%2520unique%2520projection%2520matrix%2520onto%2520the%2520intersection%2520of%2520rowspaces%2520of%2520all%2520possible%2520submatrices%2520of%2520%2524A%2524%2520obtained%2520by%2520deleting%2520%25242q%2524%2520rows.%2520Moreover%252C%2520we%2520prove%2520that%2520every%2520%2524x%2524%2520that%2520minimizes%2520the%2520%2524%255Cell_0%2524-norm%2520of%2520%2524y%2520-%2520A%2520x%2524%2520lies%2520in%2520%2524x%255E%255Cstar%2520%252B%2520%255Cker%2528U%2529%2524%252C%2520which%2520then%2520gives%2520a%2520constructive%2520approach%2520to%2520recover%2520this%2520set.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.24215v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Can%20Be%20Recovered%20Under%20Sparse%20Adversarial%20Corruption%3F%20Assumption-Free%20Theory%20for%20Linear%20Measurements&entry.906535625=Vishal%20Halder%20and%20Alexandre%20Reiffers-Masson%20and%20Abdeldjalil%20A%C3%AFssa-El-Bey%20and%20Gugan%20Thoppe&entry.1292438233=Let%20%24A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D%24%20be%20an%20arbitrary%2C%20known%20matrix%20and%20%24e%24%20a%20%24q%24-sparse%20adversarial%20vector.%20Given%20%24y%20%3D%20A%20x%5E%5Cstar%20%2B%20e%24%20and%20%24q%24%2C%20we%20seek%20the%20smallest%20robust%20solution%20set%20containing%20%24x%5E%5Cstar%24%20that%20is%20uniformly%20recoverable%20from%20%24y%24%20without%20knowing%20%24e%24.%20While%20exact%20recovery%20of%20%24x%5E%5Cstar%24%20via%20strong%20%28and%20often%20impractical%29%20structural%20assumptions%20on%20%24A%24%20or%20%24x%5E%5Cstar%24%20%28e.g.%2C%20restricted%20isometry%2C%20sparsity%29%20is%20well%20studied%2C%20recoverability%20for%20arbitrary%20%24A%24%20and%20%24x%5E%5Cstar%24%20remains%20open.%20Our%20main%20result%20shows%20that%20the%20smallest%20robust%20solution%20set%20is%20%24x%5E%5Cstar%20%2B%20%5Cker%28U%29%24%2C%20where%20%24U%24%20is%20the%20unique%20projection%20matrix%20onto%20the%20intersection%20of%20rowspaces%20of%20all%20possible%20submatrices%20of%20%24A%24%20obtained%20by%20deleting%20%242q%24%20rows.%20Moreover%2C%20we%20prove%20that%20every%20%24x%24%20that%20minimizes%20the%20%24%5Cell_0%24-norm%20of%20%24y%20-%20A%20x%24%20lies%20in%20%24x%5E%5Cstar%20%2B%20%5Cker%28U%29%24%2C%20which%20then%20gives%20a%20constructive%20approach%20to%20recover%20this%20set.&entry.1838667208=http%3A//arxiv.org/abs/2510.24215v3&entry.124074799=Read"},
{"title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum", "author": "V\u00edctor Yeste and Paolo Rosso", "abstract": "We study sentence-level detection of the 19 human values in the refined Schwartz continuum in about 74k English sentences from news and political manifestos (ValueEval'24 corpus). Each sentence is annotated with value presence, yielding a binary moral-presence label and a 19-way multi-label task under severe class imbalance. First, we show that moral presence is learnable from single sentences: a DeBERTa-base classifier attains positive-class F1 = 0.74 with calibrated thresholds. Second, we compare direct multi-label value detectors with presence-gated hierarchies in a setting where only a single consumer-grade GPU with 8 GB of VRAM is available, and we explicitly choose all training and inference configurations to fit within this budget. Presence gating does not improve over direct prediction, indicating that gate recall becomes a bottleneck. Third, we investigate lightweight auxiliary signals - short-range context, LIWC-22, and moral lexica - and small ensembles. Our best supervised configuration, a soft-voting ensemble of DeBERTa-based models enriched with such signals, reaches macro-F1 = 0.332 on the 19 values, improving over the best previous English-only baseline on this corpus, namely the best official ValueEval'24 English run (macro-F1 = 0.28 on the same 19-value test set). Methodologically, our study provides, to our knowledge, the first systematic comparison of direct versus presence-gated architectures, lightweight feature-augmented encoders, and medium-sized instruction-tuned Large Language Models (LLMs) for refined Schwartz values at sentence level. We additionally benchmark 7-9B instruction-tuned LLMs (Gemma 2 9B, Llama 3.1 8B, Mistral 8B, Qwen 2.5 7B) in zero-/few-shot and QLoRA setups, and find that they lag behind the supervised ensemble under the same compute budget. Overall, our results provide empirical guidance for building compute-efficient, value-aware NLP models.", "link": "http://arxiv.org/abs/2601.14172v3", "date": "2026-02-16", "relevancy": 1.9878, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5024}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Values%20in%20a%20Single%20Sentence%3A%20Moral%20Presence%2C%20Hierarchies%2C%20and%20Transformer%20Ensembles%20on%20the%20Schwartz%20Continuum&body=Title%3A%20Human%20Values%20in%20a%20Single%20Sentence%3A%20Moral%20Presence%2C%20Hierarchies%2C%20and%20Transformer%20Ensembles%20on%20the%20Schwartz%20Continuum%0AAuthor%3A%20V%C3%ADctor%20Yeste%20and%20Paolo%20Rosso%0AAbstract%3A%20We%20study%20sentence-level%20detection%20of%20the%2019%20human%20values%20in%20the%20refined%20Schwartz%20continuum%20in%20about%2074k%20English%20sentences%20from%20news%20and%20political%20manifestos%20%28ValueEval%2724%20corpus%29.%20Each%20sentence%20is%20annotated%20with%20value%20presence%2C%20yielding%20a%20binary%20moral-presence%20label%20and%20a%2019-way%20multi-label%20task%20under%20severe%20class%20imbalance.%20First%2C%20we%20show%20that%20moral%20presence%20is%20learnable%20from%20single%20sentences%3A%20a%20DeBERTa-base%20classifier%20attains%20positive-class%20F1%20%3D%200.74%20with%20calibrated%20thresholds.%20Second%2C%20we%20compare%20direct%20multi-label%20value%20detectors%20with%20presence-gated%20hierarchies%20in%20a%20setting%20where%20only%20a%20single%20consumer-grade%20GPU%20with%208%20GB%20of%20VRAM%20is%20available%2C%20and%20we%20explicitly%20choose%20all%20training%20and%20inference%20configurations%20to%20fit%20within%20this%20budget.%20Presence%20gating%20does%20not%20improve%20over%20direct%20prediction%2C%20indicating%20that%20gate%20recall%20becomes%20a%20bottleneck.%20Third%2C%20we%20investigate%20lightweight%20auxiliary%20signals%20-%20short-range%20context%2C%20LIWC-22%2C%20and%20moral%20lexica%20-%20and%20small%20ensembles.%20Our%20best%20supervised%20configuration%2C%20a%20soft-voting%20ensemble%20of%20DeBERTa-based%20models%20enriched%20with%20such%20signals%2C%20reaches%20macro-F1%20%3D%200.332%20on%20the%2019%20values%2C%20improving%20over%20the%20best%20previous%20English-only%20baseline%20on%20this%20corpus%2C%20namely%20the%20best%20official%20ValueEval%2724%20English%20run%20%28macro-F1%20%3D%200.28%20on%20the%20same%2019-value%20test%20set%29.%20Methodologically%2C%20our%20study%20provides%2C%20to%20our%20knowledge%2C%20the%20first%20systematic%20comparison%20of%20direct%20versus%20presence-gated%20architectures%2C%20lightweight%20feature-augmented%20encoders%2C%20and%20medium-sized%20instruction-tuned%20Large%20Language%20Models%20%28LLMs%29%20for%20refined%20Schwartz%20values%20at%20sentence%20level.%20We%20additionally%20benchmark%207-9B%20instruction-tuned%20LLMs%20%28Gemma%202%209B%2C%20Llama%203.1%208B%2C%20Mistral%208B%2C%20Qwen%202.5%207B%29%20in%20zero-/few-shot%20and%20QLoRA%20setups%2C%20and%20find%20that%20they%20lag%20behind%20the%20supervised%20ensemble%20under%20the%20same%20compute%20budget.%20Overall%2C%20our%20results%20provide%20empirical%20guidance%20for%20building%20compute-efficient%2C%20value-aware%20NLP%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14172v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Values%2520in%2520a%2520Single%2520Sentence%253A%2520Moral%2520Presence%252C%2520Hierarchies%252C%2520and%2520Transformer%2520Ensembles%2520on%2520the%2520Schwartz%2520Continuum%26entry.906535625%3DV%25C3%25ADctor%2520Yeste%2520and%2520Paolo%2520Rosso%26entry.1292438233%3DWe%2520study%2520sentence-level%2520detection%2520of%2520the%252019%2520human%2520values%2520in%2520the%2520refined%2520Schwartz%2520continuum%2520in%2520about%252074k%2520English%2520sentences%2520from%2520news%2520and%2520political%2520manifestos%2520%2528ValueEval%252724%2520corpus%2529.%2520Each%2520sentence%2520is%2520annotated%2520with%2520value%2520presence%252C%2520yielding%2520a%2520binary%2520moral-presence%2520label%2520and%2520a%252019-way%2520multi-label%2520task%2520under%2520severe%2520class%2520imbalance.%2520First%252C%2520we%2520show%2520that%2520moral%2520presence%2520is%2520learnable%2520from%2520single%2520sentences%253A%2520a%2520DeBERTa-base%2520classifier%2520attains%2520positive-class%2520F1%2520%253D%25200.74%2520with%2520calibrated%2520thresholds.%2520Second%252C%2520we%2520compare%2520direct%2520multi-label%2520value%2520detectors%2520with%2520presence-gated%2520hierarchies%2520in%2520a%2520setting%2520where%2520only%2520a%2520single%2520consumer-grade%2520GPU%2520with%25208%2520GB%2520of%2520VRAM%2520is%2520available%252C%2520and%2520we%2520explicitly%2520choose%2520all%2520training%2520and%2520inference%2520configurations%2520to%2520fit%2520within%2520this%2520budget.%2520Presence%2520gating%2520does%2520not%2520improve%2520over%2520direct%2520prediction%252C%2520indicating%2520that%2520gate%2520recall%2520becomes%2520a%2520bottleneck.%2520Third%252C%2520we%2520investigate%2520lightweight%2520auxiliary%2520signals%2520-%2520short-range%2520context%252C%2520LIWC-22%252C%2520and%2520moral%2520lexica%2520-%2520and%2520small%2520ensembles.%2520Our%2520best%2520supervised%2520configuration%252C%2520a%2520soft-voting%2520ensemble%2520of%2520DeBERTa-based%2520models%2520enriched%2520with%2520such%2520signals%252C%2520reaches%2520macro-F1%2520%253D%25200.332%2520on%2520the%252019%2520values%252C%2520improving%2520over%2520the%2520best%2520previous%2520English-only%2520baseline%2520on%2520this%2520corpus%252C%2520namely%2520the%2520best%2520official%2520ValueEval%252724%2520English%2520run%2520%2528macro-F1%2520%253D%25200.28%2520on%2520the%2520same%252019-value%2520test%2520set%2529.%2520Methodologically%252C%2520our%2520study%2520provides%252C%2520to%2520our%2520knowledge%252C%2520the%2520first%2520systematic%2520comparison%2520of%2520direct%2520versus%2520presence-gated%2520architectures%252C%2520lightweight%2520feature-augmented%2520encoders%252C%2520and%2520medium-sized%2520instruction-tuned%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%2520refined%2520Schwartz%2520values%2520at%2520sentence%2520level.%2520We%2520additionally%2520benchmark%25207-9B%2520instruction-tuned%2520LLMs%2520%2528Gemma%25202%25209B%252C%2520Llama%25203.1%25208B%252C%2520Mistral%25208B%252C%2520Qwen%25202.5%25207B%2529%2520in%2520zero-/few-shot%2520and%2520QLoRA%2520setups%252C%2520and%2520find%2520that%2520they%2520lag%2520behind%2520the%2520supervised%2520ensemble%2520under%2520the%2520same%2520compute%2520budget.%2520Overall%252C%2520our%2520results%2520provide%2520empirical%2520guidance%2520for%2520building%2520compute-efficient%252C%2520value-aware%2520NLP%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14172v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Values%20in%20a%20Single%20Sentence%3A%20Moral%20Presence%2C%20Hierarchies%2C%20and%20Transformer%20Ensembles%20on%20the%20Schwartz%20Continuum&entry.906535625=V%C3%ADctor%20Yeste%20and%20Paolo%20Rosso&entry.1292438233=We%20study%20sentence-level%20detection%20of%20the%2019%20human%20values%20in%20the%20refined%20Schwartz%20continuum%20in%20about%2074k%20English%20sentences%20from%20news%20and%20political%20manifestos%20%28ValueEval%2724%20corpus%29.%20Each%20sentence%20is%20annotated%20with%20value%20presence%2C%20yielding%20a%20binary%20moral-presence%20label%20and%20a%2019-way%20multi-label%20task%20under%20severe%20class%20imbalance.%20First%2C%20we%20show%20that%20moral%20presence%20is%20learnable%20from%20single%20sentences%3A%20a%20DeBERTa-base%20classifier%20attains%20positive-class%20F1%20%3D%200.74%20with%20calibrated%20thresholds.%20Second%2C%20we%20compare%20direct%20multi-label%20value%20detectors%20with%20presence-gated%20hierarchies%20in%20a%20setting%20where%20only%20a%20single%20consumer-grade%20GPU%20with%208%20GB%20of%20VRAM%20is%20available%2C%20and%20we%20explicitly%20choose%20all%20training%20and%20inference%20configurations%20to%20fit%20within%20this%20budget.%20Presence%20gating%20does%20not%20improve%20over%20direct%20prediction%2C%20indicating%20that%20gate%20recall%20becomes%20a%20bottleneck.%20Third%2C%20we%20investigate%20lightweight%20auxiliary%20signals%20-%20short-range%20context%2C%20LIWC-22%2C%20and%20moral%20lexica%20-%20and%20small%20ensembles.%20Our%20best%20supervised%20configuration%2C%20a%20soft-voting%20ensemble%20of%20DeBERTa-based%20models%20enriched%20with%20such%20signals%2C%20reaches%20macro-F1%20%3D%200.332%20on%20the%2019%20values%2C%20improving%20over%20the%20best%20previous%20English-only%20baseline%20on%20this%20corpus%2C%20namely%20the%20best%20official%20ValueEval%2724%20English%20run%20%28macro-F1%20%3D%200.28%20on%20the%20same%2019-value%20test%20set%29.%20Methodologically%2C%20our%20study%20provides%2C%20to%20our%20knowledge%2C%20the%20first%20systematic%20comparison%20of%20direct%20versus%20presence-gated%20architectures%2C%20lightweight%20feature-augmented%20encoders%2C%20and%20medium-sized%20instruction-tuned%20Large%20Language%20Models%20%28LLMs%29%20for%20refined%20Schwartz%20values%20at%20sentence%20level.%20We%20additionally%20benchmark%207-9B%20instruction-tuned%20LLMs%20%28Gemma%202%209B%2C%20Llama%203.1%208B%2C%20Mistral%208B%2C%20Qwen%202.5%207B%29%20in%20zero-/few-shot%20and%20QLoRA%20setups%2C%20and%20find%20that%20they%20lag%20behind%20the%20supervised%20ensemble%20under%20the%20same%20compute%20budget.%20Overall%2C%20our%20results%20provide%20empirical%20guidance%20for%20building%20compute-efficient%2C%20value-aware%20NLP%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.14172v3&entry.124074799=Read"},
{"title": "ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees", "author": "Muhammad Rashid and Elvio G. Amparore and Enrico Ferrari and Damiano Verda", "abstract": "Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.", "link": "http://arxiv.org/abs/2602.07047v2", "date": "2026-02-16", "relevancy": 1.9861, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4964}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapBPT%3A%20Image%20Feature%20Attributions%20Using%20Data-Aware%20Binary%20Partition%20Trees&body=Title%3A%20ShapBPT%3A%20Image%20Feature%20Attributions%20Using%20Data-Aware%20Binary%20Partition%20Trees%0AAuthor%3A%20Muhammad%20Rashid%20and%20Elvio%20G.%20Amparore%20and%20Enrico%20Ferrari%20and%20Damiano%20Verda%0AAbstract%3A%20Pixel-level%20feature%20attributions%20are%20an%20important%20tool%20in%20eXplainable%20AI%20for%20Computer%20Vision%20%28XCV%29%2C%20providing%20visual%20insights%20into%20how%20image%20features%20influence%20model%20predictions.%20The%20Owen%20formula%20for%20hierarchical%20Shapley%20values%20has%20been%20widely%20used%20to%20interpret%20machine%20learning%20%28ML%29%20models%20and%20their%20learned%20representations.%20However%2C%20existing%20hierarchical%20Shapley%20approaches%20do%20not%20exploit%20the%20multiscale%20structure%20of%20image%20data%2C%20leading%20to%20slow%20convergence%20and%20weak%20alignment%20with%20the%20actual%20morphological%20features.%20Moreover%2C%20no%20prior%20Shapley%20method%20has%20leveraged%20data-aware%20hierarchies%20for%20Computer%20Vision%20tasks%2C%20leaving%20a%20gap%20in%20model%20interpretability%20of%20structured%20visual%20data.%20To%20address%20this%2C%20this%20paper%20introduces%20ShapBPT%2C%20a%20novel%20data-aware%20XCV%20method%20based%20on%20the%20hierarchical%20Shapley%20formula.%20ShapBPT%20assigns%20Shapley%20coefficients%20to%20a%20multiscale%20hierarchical%20structure%20tailored%20for%20images%2C%20the%20Binary%20Partition%20Tree%20%28BPT%29.%20By%20using%20this%20data-aware%20hierarchical%20partitioning%2C%20ShapBPT%20ensures%20that%20feature%20attributions%20align%20with%20intrinsic%20image%20morphology%2C%20effectively%20prioritizing%20relevant%20regions%20while%20reducing%20computational%20overhead.%20This%20advancement%20connects%20hierarchical%20Shapley%20methods%20with%20image%20data%2C%20providing%20a%20more%20efficient%20and%20semantically%20meaningful%20approach%20to%20visual%20interpretability.%20Experimental%20results%20confirm%20ShapBPT%27s%20effectiveness%2C%20demonstrating%20superior%20alignment%20with%20image%20structures%20and%20improved%20efficiency%20over%20existing%20XCV%20methods%2C%20and%20a%2020-subject%20user%20study%20confirming%20that%20ShapBPT%20explanations%20are%20preferred%20by%20humans.%0ALink%3A%20http%3A//arxiv.org/abs/2602.07047v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapBPT%253A%2520Image%2520Feature%2520Attributions%2520Using%2520Data-Aware%2520Binary%2520Partition%2520Trees%26entry.906535625%3DMuhammad%2520Rashid%2520and%2520Elvio%2520G.%2520Amparore%2520and%2520Enrico%2520Ferrari%2520and%2520Damiano%2520Verda%26entry.1292438233%3DPixel-level%2520feature%2520attributions%2520are%2520an%2520important%2520tool%2520in%2520eXplainable%2520AI%2520for%2520Computer%2520Vision%2520%2528XCV%2529%252C%2520providing%2520visual%2520insights%2520into%2520how%2520image%2520features%2520influence%2520model%2520predictions.%2520The%2520Owen%2520formula%2520for%2520hierarchical%2520Shapley%2520values%2520has%2520been%2520widely%2520used%2520to%2520interpret%2520machine%2520learning%2520%2528ML%2529%2520models%2520and%2520their%2520learned%2520representations.%2520However%252C%2520existing%2520hierarchical%2520Shapley%2520approaches%2520do%2520not%2520exploit%2520the%2520multiscale%2520structure%2520of%2520image%2520data%252C%2520leading%2520to%2520slow%2520convergence%2520and%2520weak%2520alignment%2520with%2520the%2520actual%2520morphological%2520features.%2520Moreover%252C%2520no%2520prior%2520Shapley%2520method%2520has%2520leveraged%2520data-aware%2520hierarchies%2520for%2520Computer%2520Vision%2520tasks%252C%2520leaving%2520a%2520gap%2520in%2520model%2520interpretability%2520of%2520structured%2520visual%2520data.%2520To%2520address%2520this%252C%2520this%2520paper%2520introduces%2520ShapBPT%252C%2520a%2520novel%2520data-aware%2520XCV%2520method%2520based%2520on%2520the%2520hierarchical%2520Shapley%2520formula.%2520ShapBPT%2520assigns%2520Shapley%2520coefficients%2520to%2520a%2520multiscale%2520hierarchical%2520structure%2520tailored%2520for%2520images%252C%2520the%2520Binary%2520Partition%2520Tree%2520%2528BPT%2529.%2520By%2520using%2520this%2520data-aware%2520hierarchical%2520partitioning%252C%2520ShapBPT%2520ensures%2520that%2520feature%2520attributions%2520align%2520with%2520intrinsic%2520image%2520morphology%252C%2520effectively%2520prioritizing%2520relevant%2520regions%2520while%2520reducing%2520computational%2520overhead.%2520This%2520advancement%2520connects%2520hierarchical%2520Shapley%2520methods%2520with%2520image%2520data%252C%2520providing%2520a%2520more%2520efficient%2520and%2520semantically%2520meaningful%2520approach%2520to%2520visual%2520interpretability.%2520Experimental%2520results%2520confirm%2520ShapBPT%2527s%2520effectiveness%252C%2520demonstrating%2520superior%2520alignment%2520with%2520image%2520structures%2520and%2520improved%2520efficiency%2520over%2520existing%2520XCV%2520methods%252C%2520and%2520a%252020-subject%2520user%2520study%2520confirming%2520that%2520ShapBPT%2520explanations%2520are%2520preferred%2520by%2520humans.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.07047v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapBPT%3A%20Image%20Feature%20Attributions%20Using%20Data-Aware%20Binary%20Partition%20Trees&entry.906535625=Muhammad%20Rashid%20and%20Elvio%20G.%20Amparore%20and%20Enrico%20Ferrari%20and%20Damiano%20Verda&entry.1292438233=Pixel-level%20feature%20attributions%20are%20an%20important%20tool%20in%20eXplainable%20AI%20for%20Computer%20Vision%20%28XCV%29%2C%20providing%20visual%20insights%20into%20how%20image%20features%20influence%20model%20predictions.%20The%20Owen%20formula%20for%20hierarchical%20Shapley%20values%20has%20been%20widely%20used%20to%20interpret%20machine%20learning%20%28ML%29%20models%20and%20their%20learned%20representations.%20However%2C%20existing%20hierarchical%20Shapley%20approaches%20do%20not%20exploit%20the%20multiscale%20structure%20of%20image%20data%2C%20leading%20to%20slow%20convergence%20and%20weak%20alignment%20with%20the%20actual%20morphological%20features.%20Moreover%2C%20no%20prior%20Shapley%20method%20has%20leveraged%20data-aware%20hierarchies%20for%20Computer%20Vision%20tasks%2C%20leaving%20a%20gap%20in%20model%20interpretability%20of%20structured%20visual%20data.%20To%20address%20this%2C%20this%20paper%20introduces%20ShapBPT%2C%20a%20novel%20data-aware%20XCV%20method%20based%20on%20the%20hierarchical%20Shapley%20formula.%20ShapBPT%20assigns%20Shapley%20coefficients%20to%20a%20multiscale%20hierarchical%20structure%20tailored%20for%20images%2C%20the%20Binary%20Partition%20Tree%20%28BPT%29.%20By%20using%20this%20data-aware%20hierarchical%20partitioning%2C%20ShapBPT%20ensures%20that%20feature%20attributions%20align%20with%20intrinsic%20image%20morphology%2C%20effectively%20prioritizing%20relevant%20regions%20while%20reducing%20computational%20overhead.%20This%20advancement%20connects%20hierarchical%20Shapley%20methods%20with%20image%20data%2C%20providing%20a%20more%20efficient%20and%20semantically%20meaningful%20approach%20to%20visual%20interpretability.%20Experimental%20results%20confirm%20ShapBPT%27s%20effectiveness%2C%20demonstrating%20superior%20alignment%20with%20image%20structures%20and%20improved%20efficiency%20over%20existing%20XCV%20methods%2C%20and%20a%2020-subject%20user%20study%20confirming%20that%20ShapBPT%20explanations%20are%20preferred%20by%20humans.&entry.1838667208=http%3A//arxiv.org/abs/2602.07047v2&entry.124074799=Read"},
{"title": "SAFER: Risk-Constrained Sample-then-Filter in Large Language Models", "author": "Qingni Wang and Yue Fan and Xin Eric Wang", "abstract": "As large language models (LLMs) are increasingly deployed in risk-sensitive applications such as real-world open-ended question answering (QA), ensuring the trustworthiness of their outputs has become critical. Existing selective conformal prediction (SCP) methods provide statistical guarantees by constructing prediction sets with a constrained miscoverage rate for correct answers. However, prior works unrealistically assume that admissible answers for all instances can be obtained via finite sampling, even for open-ended QA scenarios that lack a fixed and finite solution space. To address this, we introduce a two-stage risk control framework comprising abstention-aware sampling and conformalized filtering (SAFER). Firstly, on a held-out calibration set, SAFER calibrates a sampling budget within the maximum sampling cap, using the Clopper-Pearson exact method at a user-desired risk level (i.e., the maximum allowable miscoverage rate of the sampling sets). If the risk level cannot be satisfied within the cap, we abstain; otherwise, the calibrated sampling budget becomes the minimum requirements at test time. Then, we employ calibration instances where correct answers are attainable under the calibrated budget and apply the conformal risk control method to determine a statistically valid uncertainty threshold, which filters unreliable distractors from the candidate set for each test data point. In this stage, SAFER introduces an additional risk level to guide the calculation of the threshold, thereby controlling the risk of correct answers being excluded. Furthermore, we show that SAFER is compatible with various task-specific admission criteria and calibration-test split ratios, highlighting its robustness and high data efficiency.", "link": "http://arxiv.org/abs/2510.10193v3", "date": "2026-02-16", "relevancy": 1.9814, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4969}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4947}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAFER%3A%20Risk-Constrained%20Sample-then-Filter%20in%20Large%20Language%20Models&body=Title%3A%20SAFER%3A%20Risk-Constrained%20Sample-then-Filter%20in%20Large%20Language%20Models%0AAuthor%3A%20Qingni%20Wang%20and%20Yue%20Fan%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20risk-sensitive%20applications%20such%20as%20real-world%20open-ended%20question%20answering%20%28QA%29%2C%20ensuring%20the%20trustworthiness%20of%20their%20outputs%20has%20become%20critical.%20Existing%20selective%20conformal%20prediction%20%28SCP%29%20methods%20provide%20statistical%20guarantees%20by%20constructing%20prediction%20sets%20with%20a%20constrained%20miscoverage%20rate%20for%20correct%20answers.%20However%2C%20prior%20works%20unrealistically%20assume%20that%20admissible%20answers%20for%20all%20instances%20can%20be%20obtained%20via%20finite%20sampling%2C%20even%20for%20open-ended%20QA%20scenarios%20that%20lack%20a%20fixed%20and%20finite%20solution%20space.%20To%20address%20this%2C%20we%20introduce%20a%20two-stage%20risk%20control%20framework%20comprising%20abstention-aware%20sampling%20and%20conformalized%20filtering%20%28SAFER%29.%20Firstly%2C%20on%20a%20held-out%20calibration%20set%2C%20SAFER%20calibrates%20a%20sampling%20budget%20within%20the%20maximum%20sampling%20cap%2C%20using%20the%20Clopper-Pearson%20exact%20method%20at%20a%20user-desired%20risk%20level%20%28i.e.%2C%20the%20maximum%20allowable%20miscoverage%20rate%20of%20the%20sampling%20sets%29.%20If%20the%20risk%20level%20cannot%20be%20satisfied%20within%20the%20cap%2C%20we%20abstain%3B%20otherwise%2C%20the%20calibrated%20sampling%20budget%20becomes%20the%20minimum%20requirements%20at%20test%20time.%20Then%2C%20we%20employ%20calibration%20instances%20where%20correct%20answers%20are%20attainable%20under%20the%20calibrated%20budget%20and%20apply%20the%20conformal%20risk%20control%20method%20to%20determine%20a%20statistically%20valid%20uncertainty%20threshold%2C%20which%20filters%20unreliable%20distractors%20from%20the%20candidate%20set%20for%20each%20test%20data%20point.%20In%20this%20stage%2C%20SAFER%20introduces%20an%20additional%20risk%20level%20to%20guide%20the%20calculation%20of%20the%20threshold%2C%20thereby%20controlling%20the%20risk%20of%20correct%20answers%20being%20excluded.%20Furthermore%2C%20we%20show%20that%20SAFER%20is%20compatible%20with%20various%20task-specific%20admission%20criteria%20and%20calibration-test%20split%20ratios%2C%20highlighting%20its%20robustness%20and%20high%20data%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10193v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAFER%253A%2520Risk-Constrained%2520Sample-then-Filter%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DQingni%2520Wang%2520and%2520Yue%2520Fan%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3DAs%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520risk-sensitive%2520applications%2520such%2520as%2520real-world%2520open-ended%2520question%2520answering%2520%2528QA%2529%252C%2520ensuring%2520the%2520trustworthiness%2520of%2520their%2520outputs%2520has%2520become%2520critical.%2520Existing%2520selective%2520conformal%2520prediction%2520%2528SCP%2529%2520methods%2520provide%2520statistical%2520guarantees%2520by%2520constructing%2520prediction%2520sets%2520with%2520a%2520constrained%2520miscoverage%2520rate%2520for%2520correct%2520answers.%2520However%252C%2520prior%2520works%2520unrealistically%2520assume%2520that%2520admissible%2520answers%2520for%2520all%2520instances%2520can%2520be%2520obtained%2520via%2520finite%2520sampling%252C%2520even%2520for%2520open-ended%2520QA%2520scenarios%2520that%2520lack%2520a%2520fixed%2520and%2520finite%2520solution%2520space.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520two-stage%2520risk%2520control%2520framework%2520comprising%2520abstention-aware%2520sampling%2520and%2520conformalized%2520filtering%2520%2528SAFER%2529.%2520Firstly%252C%2520on%2520a%2520held-out%2520calibration%2520set%252C%2520SAFER%2520calibrates%2520a%2520sampling%2520budget%2520within%2520the%2520maximum%2520sampling%2520cap%252C%2520using%2520the%2520Clopper-Pearson%2520exact%2520method%2520at%2520a%2520user-desired%2520risk%2520level%2520%2528i.e.%252C%2520the%2520maximum%2520allowable%2520miscoverage%2520rate%2520of%2520the%2520sampling%2520sets%2529.%2520If%2520the%2520risk%2520level%2520cannot%2520be%2520satisfied%2520within%2520the%2520cap%252C%2520we%2520abstain%253B%2520otherwise%252C%2520the%2520calibrated%2520sampling%2520budget%2520becomes%2520the%2520minimum%2520requirements%2520at%2520test%2520time.%2520Then%252C%2520we%2520employ%2520calibration%2520instances%2520where%2520correct%2520answers%2520are%2520attainable%2520under%2520the%2520calibrated%2520budget%2520and%2520apply%2520the%2520conformal%2520risk%2520control%2520method%2520to%2520determine%2520a%2520statistically%2520valid%2520uncertainty%2520threshold%252C%2520which%2520filters%2520unreliable%2520distractors%2520from%2520the%2520candidate%2520set%2520for%2520each%2520test%2520data%2520point.%2520In%2520this%2520stage%252C%2520SAFER%2520introduces%2520an%2520additional%2520risk%2520level%2520to%2520guide%2520the%2520calculation%2520of%2520the%2520threshold%252C%2520thereby%2520controlling%2520the%2520risk%2520of%2520correct%2520answers%2520being%2520excluded.%2520Furthermore%252C%2520we%2520show%2520that%2520SAFER%2520is%2520compatible%2520with%2520various%2520task-specific%2520admission%2520criteria%2520and%2520calibration-test%2520split%2520ratios%252C%2520highlighting%2520its%2520robustness%2520and%2520high%2520data%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10193v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAFER%3A%20Risk-Constrained%20Sample-then-Filter%20in%20Large%20Language%20Models&entry.906535625=Qingni%20Wang%20and%20Yue%20Fan%20and%20Xin%20Eric%20Wang&entry.1292438233=As%20large%20language%20models%20%28LLMs%29%20are%20increasingly%20deployed%20in%20risk-sensitive%20applications%20such%20as%20real-world%20open-ended%20question%20answering%20%28QA%29%2C%20ensuring%20the%20trustworthiness%20of%20their%20outputs%20has%20become%20critical.%20Existing%20selective%20conformal%20prediction%20%28SCP%29%20methods%20provide%20statistical%20guarantees%20by%20constructing%20prediction%20sets%20with%20a%20constrained%20miscoverage%20rate%20for%20correct%20answers.%20However%2C%20prior%20works%20unrealistically%20assume%20that%20admissible%20answers%20for%20all%20instances%20can%20be%20obtained%20via%20finite%20sampling%2C%20even%20for%20open-ended%20QA%20scenarios%20that%20lack%20a%20fixed%20and%20finite%20solution%20space.%20To%20address%20this%2C%20we%20introduce%20a%20two-stage%20risk%20control%20framework%20comprising%20abstention-aware%20sampling%20and%20conformalized%20filtering%20%28SAFER%29.%20Firstly%2C%20on%20a%20held-out%20calibration%20set%2C%20SAFER%20calibrates%20a%20sampling%20budget%20within%20the%20maximum%20sampling%20cap%2C%20using%20the%20Clopper-Pearson%20exact%20method%20at%20a%20user-desired%20risk%20level%20%28i.e.%2C%20the%20maximum%20allowable%20miscoverage%20rate%20of%20the%20sampling%20sets%29.%20If%20the%20risk%20level%20cannot%20be%20satisfied%20within%20the%20cap%2C%20we%20abstain%3B%20otherwise%2C%20the%20calibrated%20sampling%20budget%20becomes%20the%20minimum%20requirements%20at%20test%20time.%20Then%2C%20we%20employ%20calibration%20instances%20where%20correct%20answers%20are%20attainable%20under%20the%20calibrated%20budget%20and%20apply%20the%20conformal%20risk%20control%20method%20to%20determine%20a%20statistically%20valid%20uncertainty%20threshold%2C%20which%20filters%20unreliable%20distractors%20from%20the%20candidate%20set%20for%20each%20test%20data%20point.%20In%20this%20stage%2C%20SAFER%20introduces%20an%20additional%20risk%20level%20to%20guide%20the%20calculation%20of%20the%20threshold%2C%20thereby%20controlling%20the%20risk%20of%20correct%20answers%20being%20excluded.%20Furthermore%2C%20we%20show%20that%20SAFER%20is%20compatible%20with%20various%20task-specific%20admission%20criteria%20and%20calibration-test%20split%20ratios%2C%20highlighting%20its%20robustness%20and%20high%20data%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2510.10193v3&entry.124074799=Read"},
{"title": "From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs", "author": "Louis Schiekiera and Max Zimmer and Christophe Roux and Sebastian Pokutta and Fritz G\u00fcnther", "abstract": "We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.", "link": "http://arxiv.org/abs/2602.00628v2", "date": "2026-02-16", "relevancy": 1.9808, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4952}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4952}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Associations%20to%20Activations%3A%20Comparing%20Behavioral%20and%20Hidden-State%20Semantic%20Geometry%20in%20LLMs&body=Title%3A%20From%20Associations%20to%20Activations%3A%20Comparing%20Behavioral%20and%20Hidden-State%20Semantic%20Geometry%20in%20LLMs%0AAuthor%3A%20Louis%20Schiekiera%20and%20Max%20Zimmer%20and%20Christophe%20Roux%20and%20Sebastian%20Pokutta%20and%20Fritz%20G%C3%BCnther%0AAbstract%3A%20We%20investigate%20the%20extent%20to%20which%20an%20LLM%27s%20hidden-state%20geometry%20can%20be%20recovered%20from%20its%20behavior%20in%20psycholinguistic%20experiments.%20Across%20eight%20instruction-tuned%20transformer%20models%2C%20we%20run%20two%20experimental%20paradigms%20--%20similarity-based%20forced%20choice%20and%20free%20association%20--%20over%20a%20shared%205%2C000-word%20vocabulary%2C%20collecting%2017.5M%2B%20trials%20to%20build%20behavior-based%20similarity%20matrices.%20Using%20representational%20similarity%20analysis%2C%20we%20compare%20behavioral%20geometries%20to%20layerwise%20hidden-state%20similarity%20and%20benchmark%20against%20FastText%2C%20BERT%2C%20and%20cross-model%20consensus.%20We%20find%20that%20forced-choice%20behavior%20aligns%20substantially%20more%20with%20hidden-state%20geometry%20than%20free%20association.%20In%20a%20held-out-words%20regression%2C%20behavioral%20similarity%20%28especially%20forced%20choice%29%20predicts%20unseen%20hidden-state%20similarities%20beyond%20lexical%20baselines%20and%20cross-model%20consensus%2C%20indicating%20that%20behavior-only%20measurements%20retain%20recoverable%20information%20about%20internal%20semantic%20geometry.%20Finally%2C%20we%20discuss%20implications%20for%20the%20ability%20of%20behavioral%20tasks%20to%20uncover%20hidden%20cognitive%20states.%0ALink%3A%20http%3A//arxiv.org/abs/2602.00628v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Associations%2520to%2520Activations%253A%2520Comparing%2520Behavioral%2520and%2520Hidden-State%2520Semantic%2520Geometry%2520in%2520LLMs%26entry.906535625%3DLouis%2520Schiekiera%2520and%2520Max%2520Zimmer%2520and%2520Christophe%2520Roux%2520and%2520Sebastian%2520Pokutta%2520and%2520Fritz%2520G%25C3%25BCnther%26entry.1292438233%3DWe%2520investigate%2520the%2520extent%2520to%2520which%2520an%2520LLM%2527s%2520hidden-state%2520geometry%2520can%2520be%2520recovered%2520from%2520its%2520behavior%2520in%2520psycholinguistic%2520experiments.%2520Across%2520eight%2520instruction-tuned%2520transformer%2520models%252C%2520we%2520run%2520two%2520experimental%2520paradigms%2520--%2520similarity-based%2520forced%2520choice%2520and%2520free%2520association%2520--%2520over%2520a%2520shared%25205%252C000-word%2520vocabulary%252C%2520collecting%252017.5M%252B%2520trials%2520to%2520build%2520behavior-based%2520similarity%2520matrices.%2520Using%2520representational%2520similarity%2520analysis%252C%2520we%2520compare%2520behavioral%2520geometries%2520to%2520layerwise%2520hidden-state%2520similarity%2520and%2520benchmark%2520against%2520FastText%252C%2520BERT%252C%2520and%2520cross-model%2520consensus.%2520We%2520find%2520that%2520forced-choice%2520behavior%2520aligns%2520substantially%2520more%2520with%2520hidden-state%2520geometry%2520than%2520free%2520association.%2520In%2520a%2520held-out-words%2520regression%252C%2520behavioral%2520similarity%2520%2528especially%2520forced%2520choice%2529%2520predicts%2520unseen%2520hidden-state%2520similarities%2520beyond%2520lexical%2520baselines%2520and%2520cross-model%2520consensus%252C%2520indicating%2520that%2520behavior-only%2520measurements%2520retain%2520recoverable%2520information%2520about%2520internal%2520semantic%2520geometry.%2520Finally%252C%2520we%2520discuss%2520implications%2520for%2520the%2520ability%2520of%2520behavioral%2520tasks%2520to%2520uncover%2520hidden%2520cognitive%2520states.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.00628v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Associations%20to%20Activations%3A%20Comparing%20Behavioral%20and%20Hidden-State%20Semantic%20Geometry%20in%20LLMs&entry.906535625=Louis%20Schiekiera%20and%20Max%20Zimmer%20and%20Christophe%20Roux%20and%20Sebastian%20Pokutta%20and%20Fritz%20G%C3%BCnther&entry.1292438233=We%20investigate%20the%20extent%20to%20which%20an%20LLM%27s%20hidden-state%20geometry%20can%20be%20recovered%20from%20its%20behavior%20in%20psycholinguistic%20experiments.%20Across%20eight%20instruction-tuned%20transformer%20models%2C%20we%20run%20two%20experimental%20paradigms%20--%20similarity-based%20forced%20choice%20and%20free%20association%20--%20over%20a%20shared%205%2C000-word%20vocabulary%2C%20collecting%2017.5M%2B%20trials%20to%20build%20behavior-based%20similarity%20matrices.%20Using%20representational%20similarity%20analysis%2C%20we%20compare%20behavioral%20geometries%20to%20layerwise%20hidden-state%20similarity%20and%20benchmark%20against%20FastText%2C%20BERT%2C%20and%20cross-model%20consensus.%20We%20find%20that%20forced-choice%20behavior%20aligns%20substantially%20more%20with%20hidden-state%20geometry%20than%20free%20association.%20In%20a%20held-out-words%20regression%2C%20behavioral%20similarity%20%28especially%20forced%20choice%29%20predicts%20unseen%20hidden-state%20similarities%20beyond%20lexical%20baselines%20and%20cross-model%20consensus%2C%20indicating%20that%20behavior-only%20measurements%20retain%20recoverable%20information%20about%20internal%20semantic%20geometry.%20Finally%2C%20we%20discuss%20implications%20for%20the%20ability%20of%20behavioral%20tasks%20to%20uncover%20hidden%20cognitive%20states.&entry.1838667208=http%3A//arxiv.org/abs/2602.00628v2&entry.124074799=Read"},
{"title": "Algorithmic Simplification of Neural Networks with Mosaic-of-Motifs", "author": "Pedram Bakhtiarifard and Tong Chen and Jonathan Wensh\u00f8j and Erik B Dam and Raghavendra Selvan", "abstract": "Large-scale deep learning models are well-suited for compression. Methods like pruning, quantization, and knowledge distillation have been used to achieve massive reductions in the number of model parameters, with marginal performance drops across a variety of architectures and tasks. This raises the central question: \\emph{Why are deep neural networks suited for compression?} In this work, we take up the perspective of algorithmic complexity to explain this behavior. We hypothesize that the parameters of trained models have more structure and, hence, exhibit lower algorithmic complexity compared to the weights at (random) initialization. Furthermore, that model compression methods harness this reduced algorithmic complexity to compress models. Although an unconstrained parameterization of model weights, $\\mathbf{w} \\in \\mathbb{R}^n$, can represent arbitrary weight assignments, the solutions found during training exhibit repeatability and structure, making them algorithmically simpler than a generic program. To this end, we formalize the Kolmogorov complexity of $\\mathbf{w}$ by $\\mathcal{K}(\\mathbf{w})$. We introduce a constrained parameterization $\\widehat{\\mathbf{w}}$, that partitions parameters into blocks of size $s$, and restricts each block to be selected from a set of $k$ reusable motifs, specified by a reuse pattern (or mosaic). The resulting method, $\\textit{Mosaic-of-Motifs}$ (MoMos), yields algorithmically simpler model parameterization compared to unconstrained models. Empirical evidence from multiple experiments shows that the algorithmic complexity of neural networks, measured using approximations to Kolmogorov complexity, can be reduced during training. This results in models that perform comparably with unconstrained models while being algorithmically simpler.", "link": "http://arxiv.org/abs/2602.14896v1", "date": "2026-02-16", "relevancy": 1.9704, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5039}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5025}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Algorithmic%20Simplification%20of%20Neural%20Networks%20with%20Mosaic-of-Motifs&body=Title%3A%20Algorithmic%20Simplification%20of%20Neural%20Networks%20with%20Mosaic-of-Motifs%0AAuthor%3A%20Pedram%20Bakhtiarifard%20and%20Tong%20Chen%20and%20Jonathan%20Wensh%C3%B8j%20and%20Erik%20B%20Dam%20and%20Raghavendra%20Selvan%0AAbstract%3A%20Large-scale%20deep%20learning%20models%20are%20well-suited%20for%20compression.%20Methods%20like%20pruning%2C%20quantization%2C%20and%20knowledge%20distillation%20have%20been%20used%20to%20achieve%20massive%20reductions%20in%20the%20number%20of%20model%20parameters%2C%20with%20marginal%20performance%20drops%20across%20a%20variety%20of%20architectures%20and%20tasks.%20This%20raises%20the%20central%20question%3A%20%5Cemph%7BWhy%20are%20deep%20neural%20networks%20suited%20for%20compression%3F%7D%20In%20this%20work%2C%20we%20take%20up%20the%20perspective%20of%20algorithmic%20complexity%20to%20explain%20this%20behavior.%20We%20hypothesize%20that%20the%20parameters%20of%20trained%20models%20have%20more%20structure%20and%2C%20hence%2C%20exhibit%20lower%20algorithmic%20complexity%20compared%20to%20the%20weights%20at%20%28random%29%20initialization.%20Furthermore%2C%20that%20model%20compression%20methods%20harness%20this%20reduced%20algorithmic%20complexity%20to%20compress%20models.%20Although%20an%20unconstrained%20parameterization%20of%20model%20weights%2C%20%24%5Cmathbf%7Bw%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En%24%2C%20can%20represent%20arbitrary%20weight%20assignments%2C%20the%20solutions%20found%20during%20training%20exhibit%20repeatability%20and%20structure%2C%20making%20them%20algorithmically%20simpler%20than%20a%20generic%20program.%20To%20this%20end%2C%20we%20formalize%20the%20Kolmogorov%20complexity%20of%20%24%5Cmathbf%7Bw%7D%24%20by%20%24%5Cmathcal%7BK%7D%28%5Cmathbf%7Bw%7D%29%24.%20We%20introduce%20a%20constrained%20parameterization%20%24%5Cwidehat%7B%5Cmathbf%7Bw%7D%7D%24%2C%20that%20partitions%20parameters%20into%20blocks%20of%20size%20%24s%24%2C%20and%20restricts%20each%20block%20to%20be%20selected%20from%20a%20set%20of%20%24k%24%20reusable%20motifs%2C%20specified%20by%20a%20reuse%20pattern%20%28or%20mosaic%29.%20The%20resulting%20method%2C%20%24%5Ctextit%7BMosaic-of-Motifs%7D%24%20%28MoMos%29%2C%20yields%20algorithmically%20simpler%20model%20parameterization%20compared%20to%20unconstrained%20models.%20Empirical%20evidence%20from%20multiple%20experiments%20shows%20that%20the%20algorithmic%20complexity%20of%20neural%20networks%2C%20measured%20using%20approximations%20to%20Kolmogorov%20complexity%2C%20can%20be%20reduced%20during%20training.%20This%20results%20in%20models%20that%20perform%20comparably%20with%20unconstrained%20models%20while%20being%20algorithmically%20simpler.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlgorithmic%2520Simplification%2520of%2520Neural%2520Networks%2520with%2520Mosaic-of-Motifs%26entry.906535625%3DPedram%2520Bakhtiarifard%2520and%2520Tong%2520Chen%2520and%2520Jonathan%2520Wensh%25C3%25B8j%2520and%2520Erik%2520B%2520Dam%2520and%2520Raghavendra%2520Selvan%26entry.1292438233%3DLarge-scale%2520deep%2520learning%2520models%2520are%2520well-suited%2520for%2520compression.%2520Methods%2520like%2520pruning%252C%2520quantization%252C%2520and%2520knowledge%2520distillation%2520have%2520been%2520used%2520to%2520achieve%2520massive%2520reductions%2520in%2520the%2520number%2520of%2520model%2520parameters%252C%2520with%2520marginal%2520performance%2520drops%2520across%2520a%2520variety%2520of%2520architectures%2520and%2520tasks.%2520This%2520raises%2520the%2520central%2520question%253A%2520%255Cemph%257BWhy%2520are%2520deep%2520neural%2520networks%2520suited%2520for%2520compression%253F%257D%2520In%2520this%2520work%252C%2520we%2520take%2520up%2520the%2520perspective%2520of%2520algorithmic%2520complexity%2520to%2520explain%2520this%2520behavior.%2520We%2520hypothesize%2520that%2520the%2520parameters%2520of%2520trained%2520models%2520have%2520more%2520structure%2520and%252C%2520hence%252C%2520exhibit%2520lower%2520algorithmic%2520complexity%2520compared%2520to%2520the%2520weights%2520at%2520%2528random%2529%2520initialization.%2520Furthermore%252C%2520that%2520model%2520compression%2520methods%2520harness%2520this%2520reduced%2520algorithmic%2520complexity%2520to%2520compress%2520models.%2520Although%2520an%2520unconstrained%2520parameterization%2520of%2520model%2520weights%252C%2520%2524%255Cmathbf%257Bw%257D%2520%255Cin%2520%255Cmathbb%257BR%257D%255En%2524%252C%2520can%2520represent%2520arbitrary%2520weight%2520assignments%252C%2520the%2520solutions%2520found%2520during%2520training%2520exhibit%2520repeatability%2520and%2520structure%252C%2520making%2520them%2520algorithmically%2520simpler%2520than%2520a%2520generic%2520program.%2520To%2520this%2520end%252C%2520we%2520formalize%2520the%2520Kolmogorov%2520complexity%2520of%2520%2524%255Cmathbf%257Bw%257D%2524%2520by%2520%2524%255Cmathcal%257BK%257D%2528%255Cmathbf%257Bw%257D%2529%2524.%2520We%2520introduce%2520a%2520constrained%2520parameterization%2520%2524%255Cwidehat%257B%255Cmathbf%257Bw%257D%257D%2524%252C%2520that%2520partitions%2520parameters%2520into%2520blocks%2520of%2520size%2520%2524s%2524%252C%2520and%2520restricts%2520each%2520block%2520to%2520be%2520selected%2520from%2520a%2520set%2520of%2520%2524k%2524%2520reusable%2520motifs%252C%2520specified%2520by%2520a%2520reuse%2520pattern%2520%2528or%2520mosaic%2529.%2520The%2520resulting%2520method%252C%2520%2524%255Ctextit%257BMosaic-of-Motifs%257D%2524%2520%2528MoMos%2529%252C%2520yields%2520algorithmically%2520simpler%2520model%2520parameterization%2520compared%2520to%2520unconstrained%2520models.%2520Empirical%2520evidence%2520from%2520multiple%2520experiments%2520shows%2520that%2520the%2520algorithmic%2520complexity%2520of%2520neural%2520networks%252C%2520measured%2520using%2520approximations%2520to%2520Kolmogorov%2520complexity%252C%2520can%2520be%2520reduced%2520during%2520training.%2520This%2520results%2520in%2520models%2520that%2520perform%2520comparably%2520with%2520unconstrained%2520models%2520while%2520being%2520algorithmically%2520simpler.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Algorithmic%20Simplification%20of%20Neural%20Networks%20with%20Mosaic-of-Motifs&entry.906535625=Pedram%20Bakhtiarifard%20and%20Tong%20Chen%20and%20Jonathan%20Wensh%C3%B8j%20and%20Erik%20B%20Dam%20and%20Raghavendra%20Selvan&entry.1292438233=Large-scale%20deep%20learning%20models%20are%20well-suited%20for%20compression.%20Methods%20like%20pruning%2C%20quantization%2C%20and%20knowledge%20distillation%20have%20been%20used%20to%20achieve%20massive%20reductions%20in%20the%20number%20of%20model%20parameters%2C%20with%20marginal%20performance%20drops%20across%20a%20variety%20of%20architectures%20and%20tasks.%20This%20raises%20the%20central%20question%3A%20%5Cemph%7BWhy%20are%20deep%20neural%20networks%20suited%20for%20compression%3F%7D%20In%20this%20work%2C%20we%20take%20up%20the%20perspective%20of%20algorithmic%20complexity%20to%20explain%20this%20behavior.%20We%20hypothesize%20that%20the%20parameters%20of%20trained%20models%20have%20more%20structure%20and%2C%20hence%2C%20exhibit%20lower%20algorithmic%20complexity%20compared%20to%20the%20weights%20at%20%28random%29%20initialization.%20Furthermore%2C%20that%20model%20compression%20methods%20harness%20this%20reduced%20algorithmic%20complexity%20to%20compress%20models.%20Although%20an%20unconstrained%20parameterization%20of%20model%20weights%2C%20%24%5Cmathbf%7Bw%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En%24%2C%20can%20represent%20arbitrary%20weight%20assignments%2C%20the%20solutions%20found%20during%20training%20exhibit%20repeatability%20and%20structure%2C%20making%20them%20algorithmically%20simpler%20than%20a%20generic%20program.%20To%20this%20end%2C%20we%20formalize%20the%20Kolmogorov%20complexity%20of%20%24%5Cmathbf%7Bw%7D%24%20by%20%24%5Cmathcal%7BK%7D%28%5Cmathbf%7Bw%7D%29%24.%20We%20introduce%20a%20constrained%20parameterization%20%24%5Cwidehat%7B%5Cmathbf%7Bw%7D%7D%24%2C%20that%20partitions%20parameters%20into%20blocks%20of%20size%20%24s%24%2C%20and%20restricts%20each%20block%20to%20be%20selected%20from%20a%20set%20of%20%24k%24%20reusable%20motifs%2C%20specified%20by%20a%20reuse%20pattern%20%28or%20mosaic%29.%20The%20resulting%20method%2C%20%24%5Ctextit%7BMosaic-of-Motifs%7D%24%20%28MoMos%29%2C%20yields%20algorithmically%20simpler%20model%20parameterization%20compared%20to%20unconstrained%20models.%20Empirical%20evidence%20from%20multiple%20experiments%20shows%20that%20the%20algorithmic%20complexity%20of%20neural%20networks%2C%20measured%20using%20approximations%20to%20Kolmogorov%20complexity%2C%20can%20be%20reduced%20during%20training.%20This%20results%20in%20models%20that%20perform%20comparably%20with%20unconstrained%20models%20while%20being%20algorithmically%20simpler.&entry.1838667208=http%3A//arxiv.org/abs/2602.14896v1&entry.124074799=Read"},
{"title": "Virne: A Comprehensive Benchmark for RL-based Network Resource Allocation in NFV", "author": "Tianfu Wang and Liwei Deng and Xi Chen and Junyang Wang and Huiguo He and Zhengyu Hu and Wei Wu and Leilei Ding and Qilin Fan and Hui Xiong", "abstract": "Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available at https://github.com/GeminiLight/virne.", "link": "http://arxiv.org/abs/2507.19234v2", "date": "2026-02-16", "relevancy": 1.9694, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.3966}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3931}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virne%3A%20A%20Comprehensive%20Benchmark%20for%20RL-based%20Network%20Resource%20Allocation%20in%20NFV&body=Title%3A%20Virne%3A%20A%20Comprehensive%20Benchmark%20for%20RL-based%20Network%20Resource%20Allocation%20in%20NFV%0AAuthor%3A%20Tianfu%20Wang%20and%20Liwei%20Deng%20and%20Xi%20Chen%20and%20Junyang%20Wang%20and%20Huiguo%20He%20and%20Zhengyu%20Hu%20and%20Wei%20Wu%20and%20Leilei%20Ding%20and%20Qilin%20Fan%20and%20Hui%20Xiong%0AAbstract%3A%20Resource%20allocation%20%28RA%29%20is%20critical%20to%20efficient%20service%20deployment%20in%20Network%20Function%20Virtualization%20%28NFV%29%2C%20a%20transformative%20networking%20paradigm.%20Recently%2C%20deep%20Reinforcement%20Learning%20%28RL%29-based%20methods%20have%20been%20showing%20promising%20potential%20to%20address%20this%20complexity.%20However%2C%20the%20lack%20of%20a%20systematic%20benchmarking%20framework%20and%20thorough%20analysis%20hinders%20the%20exploration%20of%20emerging%20networks%20and%20the%20development%20of%20more%20robust%20algorithms%20while%20causing%20inconsistent%20evaluation.%20In%20this%20paper%2C%20we%20introduce%20Virne%2C%20a%20comprehensive%20benchmarking%20framework%20for%20the%20NFV-RA%20problem%2C%20with%20a%20focus%20on%20supporting%20deep%20RL-based%20methods.%20Virne%20provides%20customizable%20simulations%20for%20diverse%20network%20scenarios%2C%20including%20cloud%2C%20edge%2C%20and%205G%20environments.%20It%20also%20features%20a%20modular%20and%20extensible%20implementation%20pipeline%20that%20supports%20over%2030%20methods%20of%20various%20types%2C%20and%20includes%20practical%20evaluation%20perspectives%20beyond%20effectiveness%2C%20such%20as%20scalability%2C%20generalization%2C%20and%20scalability.%20Furthermore%2C%20we%20conduct%20in-depth%20analysis%20through%20extensive%20experiments%20to%20provide%20valuable%20insights%20into%20performance%20trade-offs%20for%20efficient%20implementation%20and%20offer%20actionable%20guidance%20for%20future%20research%20directions.%20Overall%2C%20with%20its%20diverse%20simulations%2C%20rich%20implementations%2C%20and%20extensive%20evaluation%20capabilities%2C%20Virne%20could%20serve%20as%20a%20comprehensive%20benchmark%20for%20advancing%20NFV-RA%20methods%20and%20deep%20RL%20applications.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/GeminiLight/virne.%0ALink%3A%20http%3A//arxiv.org/abs/2507.19234v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirne%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520RL-based%2520Network%2520Resource%2520Allocation%2520in%2520NFV%26entry.906535625%3DTianfu%2520Wang%2520and%2520Liwei%2520Deng%2520and%2520Xi%2520Chen%2520and%2520Junyang%2520Wang%2520and%2520Huiguo%2520He%2520and%2520Zhengyu%2520Hu%2520and%2520Wei%2520Wu%2520and%2520Leilei%2520Ding%2520and%2520Qilin%2520Fan%2520and%2520Hui%2520Xiong%26entry.1292438233%3DResource%2520allocation%2520%2528RA%2529%2520is%2520critical%2520to%2520efficient%2520service%2520deployment%2520in%2520Network%2520Function%2520Virtualization%2520%2528NFV%2529%252C%2520a%2520transformative%2520networking%2520paradigm.%2520Recently%252C%2520deep%2520Reinforcement%2520Learning%2520%2528RL%2529-based%2520methods%2520have%2520been%2520showing%2520promising%2520potential%2520to%2520address%2520this%2520complexity.%2520However%252C%2520the%2520lack%2520of%2520a%2520systematic%2520benchmarking%2520framework%2520and%2520thorough%2520analysis%2520hinders%2520the%2520exploration%2520of%2520emerging%2520networks%2520and%2520the%2520development%2520of%2520more%2520robust%2520algorithms%2520while%2520causing%2520inconsistent%2520evaluation.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Virne%252C%2520a%2520comprehensive%2520benchmarking%2520framework%2520for%2520the%2520NFV-RA%2520problem%252C%2520with%2520a%2520focus%2520on%2520supporting%2520deep%2520RL-based%2520methods.%2520Virne%2520provides%2520customizable%2520simulations%2520for%2520diverse%2520network%2520scenarios%252C%2520including%2520cloud%252C%2520edge%252C%2520and%25205G%2520environments.%2520It%2520also%2520features%2520a%2520modular%2520and%2520extensible%2520implementation%2520pipeline%2520that%2520supports%2520over%252030%2520methods%2520of%2520various%2520types%252C%2520and%2520includes%2520practical%2520evaluation%2520perspectives%2520beyond%2520effectiveness%252C%2520such%2520as%2520scalability%252C%2520generalization%252C%2520and%2520scalability.%2520Furthermore%252C%2520we%2520conduct%2520in-depth%2520analysis%2520through%2520extensive%2520experiments%2520to%2520provide%2520valuable%2520insights%2520into%2520performance%2520trade-offs%2520for%2520efficient%2520implementation%2520and%2520offer%2520actionable%2520guidance%2520for%2520future%2520research%2520directions.%2520Overall%252C%2520with%2520its%2520diverse%2520simulations%252C%2520rich%2520implementations%252C%2520and%2520extensive%2520evaluation%2520capabilities%252C%2520Virne%2520could%2520serve%2520as%2520a%2520comprehensive%2520benchmark%2520for%2520advancing%2520NFV-RA%2520methods%2520and%2520deep%2520RL%2520applications.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/GeminiLight/virne.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.19234v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virne%3A%20A%20Comprehensive%20Benchmark%20for%20RL-based%20Network%20Resource%20Allocation%20in%20NFV&entry.906535625=Tianfu%20Wang%20and%20Liwei%20Deng%20and%20Xi%20Chen%20and%20Junyang%20Wang%20and%20Huiguo%20He%20and%20Zhengyu%20Hu%20and%20Wei%20Wu%20and%20Leilei%20Ding%20and%20Qilin%20Fan%20and%20Hui%20Xiong&entry.1292438233=Resource%20allocation%20%28RA%29%20is%20critical%20to%20efficient%20service%20deployment%20in%20Network%20Function%20Virtualization%20%28NFV%29%2C%20a%20transformative%20networking%20paradigm.%20Recently%2C%20deep%20Reinforcement%20Learning%20%28RL%29-based%20methods%20have%20been%20showing%20promising%20potential%20to%20address%20this%20complexity.%20However%2C%20the%20lack%20of%20a%20systematic%20benchmarking%20framework%20and%20thorough%20analysis%20hinders%20the%20exploration%20of%20emerging%20networks%20and%20the%20development%20of%20more%20robust%20algorithms%20while%20causing%20inconsistent%20evaluation.%20In%20this%20paper%2C%20we%20introduce%20Virne%2C%20a%20comprehensive%20benchmarking%20framework%20for%20the%20NFV-RA%20problem%2C%20with%20a%20focus%20on%20supporting%20deep%20RL-based%20methods.%20Virne%20provides%20customizable%20simulations%20for%20diverse%20network%20scenarios%2C%20including%20cloud%2C%20edge%2C%20and%205G%20environments.%20It%20also%20features%20a%20modular%20and%20extensible%20implementation%20pipeline%20that%20supports%20over%2030%20methods%20of%20various%20types%2C%20and%20includes%20practical%20evaluation%20perspectives%20beyond%20effectiveness%2C%20such%20as%20scalability%2C%20generalization%2C%20and%20scalability.%20Furthermore%2C%20we%20conduct%20in-depth%20analysis%20through%20extensive%20experiments%20to%20provide%20valuable%20insights%20into%20performance%20trade-offs%20for%20efficient%20implementation%20and%20offer%20actionable%20guidance%20for%20future%20research%20directions.%20Overall%2C%20with%20its%20diverse%20simulations%2C%20rich%20implementations%2C%20and%20extensive%20evaluation%20capabilities%2C%20Virne%20could%20serve%20as%20a%20comprehensive%20benchmark%20for%20advancing%20NFV-RA%20methods%20and%20deep%20RL%20applications.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/GeminiLight/virne.&entry.1838667208=http%3A//arxiv.org/abs/2507.19234v2&entry.124074799=Read"},
{"title": "Residual Connections and the Causal Shift: Uncovering a Structural Misalignment in Transformers", "author": "Jonathan Lys and Vincent Gripon and Bastien Pasdeloup and Lukas Mauch and Fabien Cardinaux and Ghouthi Boukli Hacene", "abstract": "Large Language Models (LLMs) are trained with next-token prediction, implemented in autoregressive Transformers via causal masking for parallelism. This creates a subtle misalignment: residual connections tie activations to the current token, while supervision targets the next token, potentially propagating mismatched information if the current token is not the most informative for prediction. In this work, we empirically localize this input-output alignment shift in pretrained LLMs, using decoding trajectories over tied embedding spaces and similarity-based metrics. Our experiments reveal that the hidden token representations switch from input alignment to output alignment deep within the network. Motivated by this observation, we propose a lightweight residual-path mitigation based on residual attenuation, implemented either as a fixed-layer intervention or as a learnable gating mechanism. Experiments on multiple benchmarks show that these strategies alleviate the representation misalignment and yield improvements, providing an efficient and general architectural enhancement for autoregressive Transformers.", "link": "http://arxiv.org/abs/2602.14760v1", "date": "2026-02-16", "relevancy": 1.9662, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5422}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4992}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Residual%20Connections%20and%20the%20Causal%20Shift%3A%20Uncovering%20a%20Structural%20Misalignment%20in%20Transformers&body=Title%3A%20Residual%20Connections%20and%20the%20Causal%20Shift%3A%20Uncovering%20a%20Structural%20Misalignment%20in%20Transformers%0AAuthor%3A%20Jonathan%20Lys%20and%20Vincent%20Gripon%20and%20Bastien%20Pasdeloup%20and%20Lukas%20Mauch%20and%20Fabien%20Cardinaux%20and%20Ghouthi%20Boukli%20Hacene%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20trained%20with%20next-token%20prediction%2C%20implemented%20in%20autoregressive%20Transformers%20via%20causal%20masking%20for%20parallelism.%20This%20creates%20a%20subtle%20misalignment%3A%20residual%20connections%20tie%20activations%20to%20the%20current%20token%2C%20while%20supervision%20targets%20the%20next%20token%2C%20potentially%20propagating%20mismatched%20information%20if%20the%20current%20token%20is%20not%20the%20most%20informative%20for%20prediction.%20In%20this%20work%2C%20we%20empirically%20localize%20this%20input-output%20alignment%20shift%20in%20pretrained%20LLMs%2C%20using%20decoding%20trajectories%20over%20tied%20embedding%20spaces%20and%20similarity-based%20metrics.%20Our%20experiments%20reveal%20that%20the%20hidden%20token%20representations%20switch%20from%20input%20alignment%20to%20output%20alignment%20deep%20within%20the%20network.%20Motivated%20by%20this%20observation%2C%20we%20propose%20a%20lightweight%20residual-path%20mitigation%20based%20on%20residual%20attenuation%2C%20implemented%20either%20as%20a%20fixed-layer%20intervention%20or%20as%20a%20learnable%20gating%20mechanism.%20Experiments%20on%20multiple%20benchmarks%20show%20that%20these%20strategies%20alleviate%20the%20representation%20misalignment%20and%20yield%20improvements%2C%20providing%20an%20efficient%20and%20general%20architectural%20enhancement%20for%20autoregressive%20Transformers.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidual%2520Connections%2520and%2520the%2520Causal%2520Shift%253A%2520Uncovering%2520a%2520Structural%2520Misalignment%2520in%2520Transformers%26entry.906535625%3DJonathan%2520Lys%2520and%2520Vincent%2520Gripon%2520and%2520Bastien%2520Pasdeloup%2520and%2520Lukas%2520Mauch%2520and%2520Fabien%2520Cardinaux%2520and%2520Ghouthi%2520Boukli%2520Hacene%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520trained%2520with%2520next-token%2520prediction%252C%2520implemented%2520in%2520autoregressive%2520Transformers%2520via%2520causal%2520masking%2520for%2520parallelism.%2520This%2520creates%2520a%2520subtle%2520misalignment%253A%2520residual%2520connections%2520tie%2520activations%2520to%2520the%2520current%2520token%252C%2520while%2520supervision%2520targets%2520the%2520next%2520token%252C%2520potentially%2520propagating%2520mismatched%2520information%2520if%2520the%2520current%2520token%2520is%2520not%2520the%2520most%2520informative%2520for%2520prediction.%2520In%2520this%2520work%252C%2520we%2520empirically%2520localize%2520this%2520input-output%2520alignment%2520shift%2520in%2520pretrained%2520LLMs%252C%2520using%2520decoding%2520trajectories%2520over%2520tied%2520embedding%2520spaces%2520and%2520similarity-based%2520metrics.%2520Our%2520experiments%2520reveal%2520that%2520the%2520hidden%2520token%2520representations%2520switch%2520from%2520input%2520alignment%2520to%2520output%2520alignment%2520deep%2520within%2520the%2520network.%2520Motivated%2520by%2520this%2520observation%252C%2520we%2520propose%2520a%2520lightweight%2520residual-path%2520mitigation%2520based%2520on%2520residual%2520attenuation%252C%2520implemented%2520either%2520as%2520a%2520fixed-layer%2520intervention%2520or%2520as%2520a%2520learnable%2520gating%2520mechanism.%2520Experiments%2520on%2520multiple%2520benchmarks%2520show%2520that%2520these%2520strategies%2520alleviate%2520the%2520representation%2520misalignment%2520and%2520yield%2520improvements%252C%2520providing%2520an%2520efficient%2520and%2520general%2520architectural%2520enhancement%2520for%2520autoregressive%2520Transformers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Residual%20Connections%20and%20the%20Causal%20Shift%3A%20Uncovering%20a%20Structural%20Misalignment%20in%20Transformers&entry.906535625=Jonathan%20Lys%20and%20Vincent%20Gripon%20and%20Bastien%20Pasdeloup%20and%20Lukas%20Mauch%20and%20Fabien%20Cardinaux%20and%20Ghouthi%20Boukli%20Hacene&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20trained%20with%20next-token%20prediction%2C%20implemented%20in%20autoregressive%20Transformers%20via%20causal%20masking%20for%20parallelism.%20This%20creates%20a%20subtle%20misalignment%3A%20residual%20connections%20tie%20activations%20to%20the%20current%20token%2C%20while%20supervision%20targets%20the%20next%20token%2C%20potentially%20propagating%20mismatched%20information%20if%20the%20current%20token%20is%20not%20the%20most%20informative%20for%20prediction.%20In%20this%20work%2C%20we%20empirically%20localize%20this%20input-output%20alignment%20shift%20in%20pretrained%20LLMs%2C%20using%20decoding%20trajectories%20over%20tied%20embedding%20spaces%20and%20similarity-based%20metrics.%20Our%20experiments%20reveal%20that%20the%20hidden%20token%20representations%20switch%20from%20input%20alignment%20to%20output%20alignment%20deep%20within%20the%20network.%20Motivated%20by%20this%20observation%2C%20we%20propose%20a%20lightweight%20residual-path%20mitigation%20based%20on%20residual%20attenuation%2C%20implemented%20either%20as%20a%20fixed-layer%20intervention%20or%20as%20a%20learnable%20gating%20mechanism.%20Experiments%20on%20multiple%20benchmarks%20show%20that%20these%20strategies%20alleviate%20the%20representation%20misalignment%20and%20yield%20improvements%2C%20providing%20an%20efficient%20and%20general%20architectural%20enhancement%20for%20autoregressive%20Transformers.&entry.1838667208=http%3A//arxiv.org/abs/2602.14760v1&entry.124074799=Read"},
{"title": "Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment", "author": "Elias Malomgr\u00e9 and Pieter Simoens", "abstract": "AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.", "link": "http://arxiv.org/abs/2602.14844v1", "date": "2026-02-16", "relevancy": 1.9652, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5101}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4853}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactionless%20Inverse%20Reinforcement%20Learning%3A%20A%20Data-Centric%20Framework%20for%20Durable%20Alignment&body=Title%3A%20Interactionless%20Inverse%20Reinforcement%20Learning%3A%20A%20Data-Centric%20Framework%20for%20Durable%20Alignment%0AAuthor%3A%20Elias%20Malomgr%C3%A9%20and%20Pieter%20Simoens%0AAbstract%3A%20AI%20alignment%20is%20growing%20in%20importance%2C%20yet%20current%20approaches%20suffer%20from%20a%20critical%20structural%20flaw%20that%20entangles%20the%20safety%20objectives%20with%20the%20agent%27s%20policy.%20Methods%20such%20as%20Reinforcement%20Learning%20from%20Human%20Feedback%20and%20Direct%20Preference%20Optimization%20create%20opaque%2C%20single-use%20alignment%20artifacts%2C%20which%20we%20term%20Alignment%20Waste.%20We%20propose%20Interactionless%20Inverse%20Reinforcement%20Learning%20to%20decouple%20alignment%20artifact%20learning%20from%20policy%20optimization%2C%20producing%20an%20inspectable%2C%20editable%2C%20and%20model-agnostic%20reward%20model.%20Additionally%2C%20we%20introduce%20the%20Alignment%20Flywheel%2C%20a%20human-in-the-loop%20lifecycle%20that%20iteratively%20hardens%20the%20reward%20model%20through%20automated%20audits%20and%20refinement.%20This%20architecture%20transforms%20safety%20from%20a%20disposable%20expense%20into%20a%20durable%2C%20verifiable%20engineering%20asset.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractionless%2520Inverse%2520Reinforcement%2520Learning%253A%2520A%2520Data-Centric%2520Framework%2520for%2520Durable%2520Alignment%26entry.906535625%3DElias%2520Malomgr%25C3%25A9%2520and%2520Pieter%2520Simoens%26entry.1292438233%3DAI%2520alignment%2520is%2520growing%2520in%2520importance%252C%2520yet%2520current%2520approaches%2520suffer%2520from%2520a%2520critical%2520structural%2520flaw%2520that%2520entangles%2520the%2520safety%2520objectives%2520with%2520the%2520agent%2527s%2520policy.%2520Methods%2520such%2520as%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520and%2520Direct%2520Preference%2520Optimization%2520create%2520opaque%252C%2520single-use%2520alignment%2520artifacts%252C%2520which%2520we%2520term%2520Alignment%2520Waste.%2520We%2520propose%2520Interactionless%2520Inverse%2520Reinforcement%2520Learning%2520to%2520decouple%2520alignment%2520artifact%2520learning%2520from%2520policy%2520optimization%252C%2520producing%2520an%2520inspectable%252C%2520editable%252C%2520and%2520model-agnostic%2520reward%2520model.%2520Additionally%252C%2520we%2520introduce%2520the%2520Alignment%2520Flywheel%252C%2520a%2520human-in-the-loop%2520lifecycle%2520that%2520iteratively%2520hardens%2520the%2520reward%2520model%2520through%2520automated%2520audits%2520and%2520refinement.%2520This%2520architecture%2520transforms%2520safety%2520from%2520a%2520disposable%2520expense%2520into%2520a%2520durable%252C%2520verifiable%2520engineering%2520asset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactionless%20Inverse%20Reinforcement%20Learning%3A%20A%20Data-Centric%20Framework%20for%20Durable%20Alignment&entry.906535625=Elias%20Malomgr%C3%A9%20and%20Pieter%20Simoens&entry.1292438233=AI%20alignment%20is%20growing%20in%20importance%2C%20yet%20current%20approaches%20suffer%20from%20a%20critical%20structural%20flaw%20that%20entangles%20the%20safety%20objectives%20with%20the%20agent%27s%20policy.%20Methods%20such%20as%20Reinforcement%20Learning%20from%20Human%20Feedback%20and%20Direct%20Preference%20Optimization%20create%20opaque%2C%20single-use%20alignment%20artifacts%2C%20which%20we%20term%20Alignment%20Waste.%20We%20propose%20Interactionless%20Inverse%20Reinforcement%20Learning%20to%20decouple%20alignment%20artifact%20learning%20from%20policy%20optimization%2C%20producing%20an%20inspectable%2C%20editable%2C%20and%20model-agnostic%20reward%20model.%20Additionally%2C%20we%20introduce%20the%20Alignment%20Flywheel%2C%20a%20human-in-the-loop%20lifecycle%20that%20iteratively%20hardens%20the%20reward%20model%20through%20automated%20audits%20and%20refinement.%20This%20architecture%20transforms%20safety%20from%20a%20disposable%20expense%20into%20a%20durable%2C%20verifiable%20engineering%20asset.&entry.1838667208=http%3A//arxiv.org/abs/2602.14844v1&entry.124074799=Read"},
{"title": "A Geometric Analysis of Small-sized Language Model Hallucinations", "author": "Emanuele Ricco and Elia Onofri and Lorenzo Cima and Stefano Cresci and Roberto Di Pietro", "abstract": "Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings.\n  This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%.\n  Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.", "link": "http://arxiv.org/abs/2602.14778v1", "date": "2026-02-16", "relevancy": 1.9619, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Geometric%20Analysis%20of%20Small-sized%20Language%20Model%20Hallucinations&body=Title%3A%20A%20Geometric%20Analysis%20of%20Small-sized%20Language%20Model%20Hallucinations%0AAuthor%3A%20Emanuele%20Ricco%20and%20Elia%20Onofri%20and%20Lorenzo%20Cima%20and%20Stefano%20Cresci%20and%20Roberto%20Di%20Pietro%0AAbstract%3A%20Hallucinations%20--%20fluent%20but%20factually%20incorrect%20responses%20--%20pose%20a%20major%20challenge%20to%20the%20reliability%20of%20language%20models%2C%20especially%20in%20multi-step%20or%20agentic%20settings.%0A%20%20This%20work%20investigates%20hallucinations%20in%20small-sized%20LLMs%20through%20a%20geometric%20perspective%2C%20starting%20from%20the%20hypothesis%20that%20when%20models%20generate%20multiple%20responses%20to%20the%20same%20prompt%2C%20genuine%20ones%20exhibit%20tighter%20clustering%20in%20the%20embedding%20space%2C%20we%20prove%20this%20hypothesis%20and%2C%20leveraging%20this%20geometrical%20insight%2C%20we%20also%20show%20that%20it%20is%20possible%20to%20achieve%20a%20consistent%20level%20of%20separability.%20This%20latter%20result%20is%20used%20to%20introduce%20a%20label-efficient%20propagation%20method%20that%20classifies%20large%20collections%20of%20responses%20from%20just%2030-50%20annotations%2C%20achieving%20F1%20scores%20above%2090%25.%0A%20%20Our%20findings%2C%20framing%20hallucinations%20from%20a%20geometric%20perspective%20in%20the%20embedding%20space%2C%20complement%20traditional%20knowledge-centric%20and%20single-response%20evaluation%20paradigms%2C%20paving%20the%20way%20for%20further%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Geometric%2520Analysis%2520of%2520Small-sized%2520Language%2520Model%2520Hallucinations%26entry.906535625%3DEmanuele%2520Ricco%2520and%2520Elia%2520Onofri%2520and%2520Lorenzo%2520Cima%2520and%2520Stefano%2520Cresci%2520and%2520Roberto%2520Di%2520Pietro%26entry.1292438233%3DHallucinations%2520--%2520fluent%2520but%2520factually%2520incorrect%2520responses%2520--%2520pose%2520a%2520major%2520challenge%2520to%2520the%2520reliability%2520of%2520language%2520models%252C%2520especially%2520in%2520multi-step%2520or%2520agentic%2520settings.%250A%2520%2520This%2520work%2520investigates%2520hallucinations%2520in%2520small-sized%2520LLMs%2520through%2520a%2520geometric%2520perspective%252C%2520starting%2520from%2520the%2520hypothesis%2520that%2520when%2520models%2520generate%2520multiple%2520responses%2520to%2520the%2520same%2520prompt%252C%2520genuine%2520ones%2520exhibit%2520tighter%2520clustering%2520in%2520the%2520embedding%2520space%252C%2520we%2520prove%2520this%2520hypothesis%2520and%252C%2520leveraging%2520this%2520geometrical%2520insight%252C%2520we%2520also%2520show%2520that%2520it%2520is%2520possible%2520to%2520achieve%2520a%2520consistent%2520level%2520of%2520separability.%2520This%2520latter%2520result%2520is%2520used%2520to%2520introduce%2520a%2520label-efficient%2520propagation%2520method%2520that%2520classifies%2520large%2520collections%2520of%2520responses%2520from%2520just%252030-50%2520annotations%252C%2520achieving%2520F1%2520scores%2520above%252090%2525.%250A%2520%2520Our%2520findings%252C%2520framing%2520hallucinations%2520from%2520a%2520geometric%2520perspective%2520in%2520the%2520embedding%2520space%252C%2520complement%2520traditional%2520knowledge-centric%2520and%2520single-response%2520evaluation%2520paradigms%252C%2520paving%2520the%2520way%2520for%2520further%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Geometric%20Analysis%20of%20Small-sized%20Language%20Model%20Hallucinations&entry.906535625=Emanuele%20Ricco%20and%20Elia%20Onofri%20and%20Lorenzo%20Cima%20and%20Stefano%20Cresci%20and%20Roberto%20Di%20Pietro&entry.1292438233=Hallucinations%20--%20fluent%20but%20factually%20incorrect%20responses%20--%20pose%20a%20major%20challenge%20to%20the%20reliability%20of%20language%20models%2C%20especially%20in%20multi-step%20or%20agentic%20settings.%0A%20%20This%20work%20investigates%20hallucinations%20in%20small-sized%20LLMs%20through%20a%20geometric%20perspective%2C%20starting%20from%20the%20hypothesis%20that%20when%20models%20generate%20multiple%20responses%20to%20the%20same%20prompt%2C%20genuine%20ones%20exhibit%20tighter%20clustering%20in%20the%20embedding%20space%2C%20we%20prove%20this%20hypothesis%20and%2C%20leveraging%20this%20geometrical%20insight%2C%20we%20also%20show%20that%20it%20is%20possible%20to%20achieve%20a%20consistent%20level%20of%20separability.%20This%20latter%20result%20is%20used%20to%20introduce%20a%20label-efficient%20propagation%20method%20that%20classifies%20large%20collections%20of%20responses%20from%20just%2030-50%20annotations%2C%20achieving%20F1%20scores%20above%2090%25.%0A%20%20Our%20findings%2C%20framing%20hallucinations%20from%20a%20geometric%20perspective%20in%20the%20embedding%20space%2C%20complement%20traditional%20knowledge-centric%20and%20single-response%20evaluation%20paradigms%2C%20paving%20the%20way%20for%20further%20research.&entry.1838667208=http%3A//arxiv.org/abs/2602.14778v1&entry.124074799=Read"},
{"title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR", "author": "Jiaming Li and Longze Chen and Ze Gong and Yukun Chen and Lu Wang and Wanwei He and Run Luo and Min Yang", "abstract": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, inherent to RL-based approaches. To address the challenges, we propose $\\textbf{PACS}$, a novel RLVR framework that achieves im$\\textbf{P}$licit $\\textbf{A}$ctor $\\textbf{C}$ritic coupling via a $\\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while providing more stable and efficient training. Extensive experiments demonstrate that PACS significantly outperforms strong open-source models and RLVR baselines, yielding substantial average gains of $\\textbf{+8.26\\%}$ (4B) and $\\textbf{+9.57\\%}$ (8B) over base models offering a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.", "link": "http://arxiv.org/abs/2509.02522v2", "date": "2026-02-16", "relevancy": 1.9586, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4918}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4903}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Actor%20Critic%20Coupling%20via%20a%20Supervised%20Learning%20Framework%20for%20RLVR&body=Title%3A%20Implicit%20Actor%20Critic%20Coupling%20via%20a%20Supervised%20Learning%20Framework%20for%20RLVR%0AAuthor%3A%20Jiaming%20Li%20and%20Longze%20Chen%20and%20Ze%20Gong%20and%20Yukun%20Chen%20and%20Lu%20Wang%20and%20Wanwei%20He%20and%20Run%20Luo%20and%20Min%20Yang%0AAbstract%3A%20Recent%20advances%20in%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20have%20empowered%20large%20language%20models%20%28LLMs%29%20to%20tackle%20challenging%20reasoning%20tasks%20such%20as%20mathematics%20and%20programming.%20Despite%20its%20promise%2C%20the%20RLVR%20paradigm%20poses%20significant%20challenges%2C%20as%20existing%20methods%20often%20suffer%20from%20sparse%20reward%20signals%20and%20unstable%20policy%20gradient%20updates%2C%20inherent%20to%20RL-based%20approaches.%20To%20address%20the%20challenges%2C%20we%20propose%20%24%5Ctextbf%7BPACS%7D%24%2C%20a%20novel%20RLVR%20framework%20that%20achieves%20im%24%5Ctextbf%7BP%7D%24licit%20%24%5Ctextbf%7BA%7D%24ctor%20%24%5Ctextbf%7BC%7D%24ritic%20coupling%20via%20a%20%24%5Ctextbf%7BS%7D%24upervised%20learning%20framework.%20By%20treating%20the%20outcome%20reward%20as%20a%20predictable%20label%2C%20we%20reformulate%20the%20RLVR%20problem%20into%20a%20supervised%20learning%20task%20over%20a%20score%20function%20parameterized%20by%20the%20policy%20model%20and%20optimized%20using%20cross-entropy%20loss.%20A%20detailed%20gradient%20analysis%20shows%20that%20this%20supervised%20formulation%20inherently%20recovers%20the%20classical%20policy%20gradient%20update%20while%20providing%20more%20stable%20and%20efficient%20training.%20Extensive%20experiments%20demonstrate%20that%20PACS%20significantly%20outperforms%20strong%20open-source%20models%20and%20RLVR%20baselines%2C%20yielding%20substantial%20average%20gains%20of%20%24%5Ctextbf%7B%2B8.26%5C%25%7D%24%20%284B%29%20and%20%24%5Ctextbf%7B%2B9.57%5C%25%7D%24%20%288B%29%20over%20base%20models%20offering%20a%20promising%20avenue%20for%20LLMs%20post-training%20with%20verifiable%20rewards.%20Our%20code%20and%20data%20are%20available%20as%20open%20source%20at%20https%3A//github.com/ritzz-ai/PACS.%0ALink%3A%20http%3A//arxiv.org/abs/2509.02522v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Actor%2520Critic%2520Coupling%2520via%2520a%2520Supervised%2520Learning%2520Framework%2520for%2520RLVR%26entry.906535625%3DJiaming%2520Li%2520and%2520Longze%2520Chen%2520and%2520Ze%2520Gong%2520and%2520Yukun%2520Chen%2520and%2520Lu%2520Wang%2520and%2520Wanwei%2520He%2520and%2520Run%2520Luo%2520and%2520Min%2520Yang%26entry.1292438233%3DRecent%2520advances%2520in%2520Reinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520have%2520empowered%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520tackle%2520challenging%2520reasoning%2520tasks%2520such%2520as%2520mathematics%2520and%2520programming.%2520Despite%2520its%2520promise%252C%2520the%2520RLVR%2520paradigm%2520poses%2520significant%2520challenges%252C%2520as%2520existing%2520methods%2520often%2520suffer%2520from%2520sparse%2520reward%2520signals%2520and%2520unstable%2520policy%2520gradient%2520updates%252C%2520inherent%2520to%2520RL-based%2520approaches.%2520To%2520address%2520the%2520challenges%252C%2520we%2520propose%2520%2524%255Ctextbf%257BPACS%257D%2524%252C%2520a%2520novel%2520RLVR%2520framework%2520that%2520achieves%2520im%2524%255Ctextbf%257BP%257D%2524licit%2520%2524%255Ctextbf%257BA%257D%2524ctor%2520%2524%255Ctextbf%257BC%257D%2524ritic%2520coupling%2520via%2520a%2520%2524%255Ctextbf%257BS%257D%2524upervised%2520learning%2520framework.%2520By%2520treating%2520the%2520outcome%2520reward%2520as%2520a%2520predictable%2520label%252C%2520we%2520reformulate%2520the%2520RLVR%2520problem%2520into%2520a%2520supervised%2520learning%2520task%2520over%2520a%2520score%2520function%2520parameterized%2520by%2520the%2520policy%2520model%2520and%2520optimized%2520using%2520cross-entropy%2520loss.%2520A%2520detailed%2520gradient%2520analysis%2520shows%2520that%2520this%2520supervised%2520formulation%2520inherently%2520recovers%2520the%2520classical%2520policy%2520gradient%2520update%2520while%2520providing%2520more%2520stable%2520and%2520efficient%2520training.%2520Extensive%2520experiments%2520demonstrate%2520that%2520PACS%2520significantly%2520outperforms%2520strong%2520open-source%2520models%2520and%2520RLVR%2520baselines%252C%2520yielding%2520substantial%2520average%2520gains%2520of%2520%2524%255Ctextbf%257B%252B8.26%255C%2525%257D%2524%2520%25284B%2529%2520and%2520%2524%255Ctextbf%257B%252B9.57%255C%2525%257D%2524%2520%25288B%2529%2520over%2520base%2520models%2520offering%2520a%2520promising%2520avenue%2520for%2520LLMs%2520post-training%2520with%2520verifiable%2520rewards.%2520Our%2520code%2520and%2520data%2520are%2520available%2520as%2520open%2520source%2520at%2520https%253A//github.com/ritzz-ai/PACS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.02522v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Actor%20Critic%20Coupling%20via%20a%20Supervised%20Learning%20Framework%20for%20RLVR&entry.906535625=Jiaming%20Li%20and%20Longze%20Chen%20and%20Ze%20Gong%20and%20Yukun%20Chen%20and%20Lu%20Wang%20and%20Wanwei%20He%20and%20Run%20Luo%20and%20Min%20Yang&entry.1292438233=Recent%20advances%20in%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20have%20empowered%20large%20language%20models%20%28LLMs%29%20to%20tackle%20challenging%20reasoning%20tasks%20such%20as%20mathematics%20and%20programming.%20Despite%20its%20promise%2C%20the%20RLVR%20paradigm%20poses%20significant%20challenges%2C%20as%20existing%20methods%20often%20suffer%20from%20sparse%20reward%20signals%20and%20unstable%20policy%20gradient%20updates%2C%20inherent%20to%20RL-based%20approaches.%20To%20address%20the%20challenges%2C%20we%20propose%20%24%5Ctextbf%7BPACS%7D%24%2C%20a%20novel%20RLVR%20framework%20that%20achieves%20im%24%5Ctextbf%7BP%7D%24licit%20%24%5Ctextbf%7BA%7D%24ctor%20%24%5Ctextbf%7BC%7D%24ritic%20coupling%20via%20a%20%24%5Ctextbf%7BS%7D%24upervised%20learning%20framework.%20By%20treating%20the%20outcome%20reward%20as%20a%20predictable%20label%2C%20we%20reformulate%20the%20RLVR%20problem%20into%20a%20supervised%20learning%20task%20over%20a%20score%20function%20parameterized%20by%20the%20policy%20model%20and%20optimized%20using%20cross-entropy%20loss.%20A%20detailed%20gradient%20analysis%20shows%20that%20this%20supervised%20formulation%20inherently%20recovers%20the%20classical%20policy%20gradient%20update%20while%20providing%20more%20stable%20and%20efficient%20training.%20Extensive%20experiments%20demonstrate%20that%20PACS%20significantly%20outperforms%20strong%20open-source%20models%20and%20RLVR%20baselines%2C%20yielding%20substantial%20average%20gains%20of%20%24%5Ctextbf%7B%2B8.26%5C%25%7D%24%20%284B%29%20and%20%24%5Ctextbf%7B%2B9.57%5C%25%7D%24%20%288B%29%20over%20base%20models%20offering%20a%20promising%20avenue%20for%20LLMs%20post-training%20with%20verifiable%20rewards.%20Our%20code%20and%20data%20are%20available%20as%20open%20source%20at%20https%3A//github.com/ritzz-ai/PACS.&entry.1838667208=http%3A//arxiv.org/abs/2509.02522v2&entry.124074799=Read"},
{"title": "Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks", "author": "Laura L\u00fctzow and Michael Eichelbeck and Mykel J. Kochenderfer and Matthias Althoff", "abstract": "Conformal prediction is a popular uncertainty quantification method that augments a base predictor to return sets of predictions with statistically valid coverage guarantees. However, current methods are often computationally expensive and data-intensive, as they require constructing an uncertainty model before calibration. Moreover, existing approaches typically represent the prediction sets with intervals, which limits their ability to capture dependencies in multi-dimensional outputs. We address these limitations by introducing zono-conformal prediction, a novel approach inspired by interval predictor models and reachset-conformant identification that constructs prediction zonotopes with assured coverage. By placing zonotopic uncertainty sets directly into the model of the base predictor, zono-conformal predictors can be identified via a single, data-efficient linear program. While we can apply zono-conformal prediction to arbitrary nonlinear base predictors, we focus on feed-forward neural networks in this work. Aside from regression tasks, we also construct optimal zono-conformal predictors in classification settings where the output of an uncertain predictor is a set of possible classes. We provide probabilistic coverage guarantees and present methods for detecting outliers in the identification data. In extensive numerical experiments, we show that zono-conformal predictors are less conservative than interval predictor models and standard conformal prediction methods, while achieving a similar coverage over the test data.", "link": "http://arxiv.org/abs/2508.11025v2", "date": "2026-02-16", "relevancy": 1.9534, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5023}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4969}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zono-Conformal%20Prediction%3A%20Zonotope-Based%20Uncertainty%20Quantification%20for%20Regression%20and%20Classification%20Tasks&body=Title%3A%20Zono-Conformal%20Prediction%3A%20Zonotope-Based%20Uncertainty%20Quantification%20for%20Regression%20and%20Classification%20Tasks%0AAuthor%3A%20Laura%20L%C3%BCtzow%20and%20Michael%20Eichelbeck%20and%20Mykel%20J.%20Kochenderfer%20and%20Matthias%20Althoff%0AAbstract%3A%20Conformal%20prediction%20is%20a%20popular%20uncertainty%20quantification%20method%20that%20augments%20a%20base%20predictor%20to%20return%20sets%20of%20predictions%20with%20statistically%20valid%20coverage%20guarantees.%20However%2C%20current%20methods%20are%20often%20computationally%20expensive%20and%20data-intensive%2C%20as%20they%20require%20constructing%20an%20uncertainty%20model%20before%20calibration.%20Moreover%2C%20existing%20approaches%20typically%20represent%20the%20prediction%20sets%20with%20intervals%2C%20which%20limits%20their%20ability%20to%20capture%20dependencies%20in%20multi-dimensional%20outputs.%20We%20address%20these%20limitations%20by%20introducing%20zono-conformal%20prediction%2C%20a%20novel%20approach%20inspired%20by%20interval%20predictor%20models%20and%20reachset-conformant%20identification%20that%20constructs%20prediction%20zonotopes%20with%20assured%20coverage.%20By%20placing%20zonotopic%20uncertainty%20sets%20directly%20into%20the%20model%20of%20the%20base%20predictor%2C%20zono-conformal%20predictors%20can%20be%20identified%20via%20a%20single%2C%20data-efficient%20linear%20program.%20While%20we%20can%20apply%20zono-conformal%20prediction%20to%20arbitrary%20nonlinear%20base%20predictors%2C%20we%20focus%20on%20feed-forward%20neural%20networks%20in%20this%20work.%20Aside%20from%20regression%20tasks%2C%20we%20also%20construct%20optimal%20zono-conformal%20predictors%20in%20classification%20settings%20where%20the%20output%20of%20an%20uncertain%20predictor%20is%20a%20set%20of%20possible%20classes.%20We%20provide%20probabilistic%20coverage%20guarantees%20and%20present%20methods%20for%20detecting%20outliers%20in%20the%20identification%20data.%20In%20extensive%20numerical%20experiments%2C%20we%20show%20that%20zono-conformal%20predictors%20are%20less%20conservative%20than%20interval%20predictor%20models%20and%20standard%20conformal%20prediction%20methods%2C%20while%20achieving%20a%20similar%20coverage%20over%20the%20test%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2508.11025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZono-Conformal%2520Prediction%253A%2520Zonotope-Based%2520Uncertainty%2520Quantification%2520for%2520Regression%2520and%2520Classification%2520Tasks%26entry.906535625%3DLaura%2520L%25C3%25BCtzow%2520and%2520Michael%2520Eichelbeck%2520and%2520Mykel%2520J.%2520Kochenderfer%2520and%2520Matthias%2520Althoff%26entry.1292438233%3DConformal%2520prediction%2520is%2520a%2520popular%2520uncertainty%2520quantification%2520method%2520that%2520augments%2520a%2520base%2520predictor%2520to%2520return%2520sets%2520of%2520predictions%2520with%2520statistically%2520valid%2520coverage%2520guarantees.%2520However%252C%2520current%2520methods%2520are%2520often%2520computationally%2520expensive%2520and%2520data-intensive%252C%2520as%2520they%2520require%2520constructing%2520an%2520uncertainty%2520model%2520before%2520calibration.%2520Moreover%252C%2520existing%2520approaches%2520typically%2520represent%2520the%2520prediction%2520sets%2520with%2520intervals%252C%2520which%2520limits%2520their%2520ability%2520to%2520capture%2520dependencies%2520in%2520multi-dimensional%2520outputs.%2520We%2520address%2520these%2520limitations%2520by%2520introducing%2520zono-conformal%2520prediction%252C%2520a%2520novel%2520approach%2520inspired%2520by%2520interval%2520predictor%2520models%2520and%2520reachset-conformant%2520identification%2520that%2520constructs%2520prediction%2520zonotopes%2520with%2520assured%2520coverage.%2520By%2520placing%2520zonotopic%2520uncertainty%2520sets%2520directly%2520into%2520the%2520model%2520of%2520the%2520base%2520predictor%252C%2520zono-conformal%2520predictors%2520can%2520be%2520identified%2520via%2520a%2520single%252C%2520data-efficient%2520linear%2520program.%2520While%2520we%2520can%2520apply%2520zono-conformal%2520prediction%2520to%2520arbitrary%2520nonlinear%2520base%2520predictors%252C%2520we%2520focus%2520on%2520feed-forward%2520neural%2520networks%2520in%2520this%2520work.%2520Aside%2520from%2520regression%2520tasks%252C%2520we%2520also%2520construct%2520optimal%2520zono-conformal%2520predictors%2520in%2520classification%2520settings%2520where%2520the%2520output%2520of%2520an%2520uncertain%2520predictor%2520is%2520a%2520set%2520of%2520possible%2520classes.%2520We%2520provide%2520probabilistic%2520coverage%2520guarantees%2520and%2520present%2520methods%2520for%2520detecting%2520outliers%2520in%2520the%2520identification%2520data.%2520In%2520extensive%2520numerical%2520experiments%252C%2520we%2520show%2520that%2520zono-conformal%2520predictors%2520are%2520less%2520conservative%2520than%2520interval%2520predictor%2520models%2520and%2520standard%2520conformal%2520prediction%2520methods%252C%2520while%2520achieving%2520a%2520similar%2520coverage%2520over%2520the%2520test%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.11025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zono-Conformal%20Prediction%3A%20Zonotope-Based%20Uncertainty%20Quantification%20for%20Regression%20and%20Classification%20Tasks&entry.906535625=Laura%20L%C3%BCtzow%20and%20Michael%20Eichelbeck%20and%20Mykel%20J.%20Kochenderfer%20and%20Matthias%20Althoff&entry.1292438233=Conformal%20prediction%20is%20a%20popular%20uncertainty%20quantification%20method%20that%20augments%20a%20base%20predictor%20to%20return%20sets%20of%20predictions%20with%20statistically%20valid%20coverage%20guarantees.%20However%2C%20current%20methods%20are%20often%20computationally%20expensive%20and%20data-intensive%2C%20as%20they%20require%20constructing%20an%20uncertainty%20model%20before%20calibration.%20Moreover%2C%20existing%20approaches%20typically%20represent%20the%20prediction%20sets%20with%20intervals%2C%20which%20limits%20their%20ability%20to%20capture%20dependencies%20in%20multi-dimensional%20outputs.%20We%20address%20these%20limitations%20by%20introducing%20zono-conformal%20prediction%2C%20a%20novel%20approach%20inspired%20by%20interval%20predictor%20models%20and%20reachset-conformant%20identification%20that%20constructs%20prediction%20zonotopes%20with%20assured%20coverage.%20By%20placing%20zonotopic%20uncertainty%20sets%20directly%20into%20the%20model%20of%20the%20base%20predictor%2C%20zono-conformal%20predictors%20can%20be%20identified%20via%20a%20single%2C%20data-efficient%20linear%20program.%20While%20we%20can%20apply%20zono-conformal%20prediction%20to%20arbitrary%20nonlinear%20base%20predictors%2C%20we%20focus%20on%20feed-forward%20neural%20networks%20in%20this%20work.%20Aside%20from%20regression%20tasks%2C%20we%20also%20construct%20optimal%20zono-conformal%20predictors%20in%20classification%20settings%20where%20the%20output%20of%20an%20uncertain%20predictor%20is%20a%20set%20of%20possible%20classes.%20We%20provide%20probabilistic%20coverage%20guarantees%20and%20present%20methods%20for%20detecting%20outliers%20in%20the%20identification%20data.%20In%20extensive%20numerical%20experiments%2C%20we%20show%20that%20zono-conformal%20predictors%20are%20less%20conservative%20than%20interval%20predictor%20models%20and%20standard%20conformal%20prediction%20methods%2C%20while%20achieving%20a%20similar%20coverage%20over%20the%20test%20data.&entry.1838667208=http%3A//arxiv.org/abs/2508.11025v2&entry.124074799=Read"},
{"title": "GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses", "author": "Attila Lischka and Bal\u00e1zs Kulcs\u00e1r", "abstract": "Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of great importance. In this work, we identify and propose the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization problem with the goal of evacuating as many people from an affected area by bus in a short, predefined amount of time. The purpose of bus-based evacuation is to reduce congestion and disorder that arises in purely car-focused evacuation scenarios. To solve the BEOP, we propose a deep reinforcement learning-based method utilizing graph learning, which, once trained, achieves fast inference speed and is able to create evacuation routes in fractions of seconds. We can bound the gap of our evacuation plans using an MILP formulation. To validate our method, we create evacuation scenarios for San Francisco using real-world road networks and travel times. We show that we achieve near-optimal solution quality and are further able to investigate how many evacuation vehicles are necessary to achieve certain bus-based evacuation quotas given a predefined evacuation time while keeping run time adequate.", "link": "http://arxiv.org/abs/2602.14676v1", "date": "2026-02-16", "relevancy": 1.9522, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5062}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4896}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GREAT-EER%3A%20Graph%20Edge%20Attention%20Network%20for%20Emergency%20Evacuation%20Responses&body=Title%3A%20GREAT-EER%3A%20Graph%20Edge%20Attention%20Network%20for%20Emergency%20Evacuation%20Responses%0AAuthor%3A%20Attila%20Lischka%20and%20Bal%C3%A1zs%20Kulcs%C3%A1r%0AAbstract%3A%20Emergency%20situations%20that%20require%20the%20evacuation%20of%20urban%20areas%20can%20arise%20from%20man-made%20causes%20%28e.g.%2C%20terrorist%20attacks%20or%20industrial%20accidents%29%20or%20natural%20disasters%2C%20the%20latter%20becoming%20more%20frequent%20due%20to%20climate%20change.%20As%20a%20result%2C%20effective%20and%20fast%20methods%20to%20develop%20evacuation%20plans%20are%20of%20great%20importance.%20In%20this%20work%2C%20we%20identify%20and%20propose%20the%20Bus%20Evacuation%20Orienteering%20Problem%20%28BEOP%29%2C%20an%20NP-hard%20combinatorial%20optimization%20problem%20with%20the%20goal%20of%20evacuating%20as%20many%20people%20from%20an%20affected%20area%20by%20bus%20in%20a%20short%2C%20predefined%20amount%20of%20time.%20The%20purpose%20of%20bus-based%20evacuation%20is%20to%20reduce%20congestion%20and%20disorder%20that%20arises%20in%20purely%20car-focused%20evacuation%20scenarios.%20To%20solve%20the%20BEOP%2C%20we%20propose%20a%20deep%20reinforcement%20learning-based%20method%20utilizing%20graph%20learning%2C%20which%2C%20once%20trained%2C%20achieves%20fast%20inference%20speed%20and%20is%20able%20to%20create%20evacuation%20routes%20in%20fractions%20of%20seconds.%20We%20can%20bound%20the%20gap%20of%20our%20evacuation%20plans%20using%20an%20MILP%20formulation.%20To%20validate%20our%20method%2C%20we%20create%20evacuation%20scenarios%20for%20San%20Francisco%20using%20real-world%20road%20networks%20and%20travel%20times.%20We%20show%20that%20we%20achieve%20near-optimal%20solution%20quality%20and%20are%20further%20able%20to%20investigate%20how%20many%20evacuation%20vehicles%20are%20necessary%20to%20achieve%20certain%20bus-based%20evacuation%20quotas%20given%20a%20predefined%20evacuation%20time%20while%20keeping%20run%20time%20adequate.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGREAT-EER%253A%2520Graph%2520Edge%2520Attention%2520Network%2520for%2520Emergency%2520Evacuation%2520Responses%26entry.906535625%3DAttila%2520Lischka%2520and%2520Bal%25C3%25A1zs%2520Kulcs%25C3%25A1r%26entry.1292438233%3DEmergency%2520situations%2520that%2520require%2520the%2520evacuation%2520of%2520urban%2520areas%2520can%2520arise%2520from%2520man-made%2520causes%2520%2528e.g.%252C%2520terrorist%2520attacks%2520or%2520industrial%2520accidents%2529%2520or%2520natural%2520disasters%252C%2520the%2520latter%2520becoming%2520more%2520frequent%2520due%2520to%2520climate%2520change.%2520As%2520a%2520result%252C%2520effective%2520and%2520fast%2520methods%2520to%2520develop%2520evacuation%2520plans%2520are%2520of%2520great%2520importance.%2520In%2520this%2520work%252C%2520we%2520identify%2520and%2520propose%2520the%2520Bus%2520Evacuation%2520Orienteering%2520Problem%2520%2528BEOP%2529%252C%2520an%2520NP-hard%2520combinatorial%2520optimization%2520problem%2520with%2520the%2520goal%2520of%2520evacuating%2520as%2520many%2520people%2520from%2520an%2520affected%2520area%2520by%2520bus%2520in%2520a%2520short%252C%2520predefined%2520amount%2520of%2520time.%2520The%2520purpose%2520of%2520bus-based%2520evacuation%2520is%2520to%2520reduce%2520congestion%2520and%2520disorder%2520that%2520arises%2520in%2520purely%2520car-focused%2520evacuation%2520scenarios.%2520To%2520solve%2520the%2520BEOP%252C%2520we%2520propose%2520a%2520deep%2520reinforcement%2520learning-based%2520method%2520utilizing%2520graph%2520learning%252C%2520which%252C%2520once%2520trained%252C%2520achieves%2520fast%2520inference%2520speed%2520and%2520is%2520able%2520to%2520create%2520evacuation%2520routes%2520in%2520fractions%2520of%2520seconds.%2520We%2520can%2520bound%2520the%2520gap%2520of%2520our%2520evacuation%2520plans%2520using%2520an%2520MILP%2520formulation.%2520To%2520validate%2520our%2520method%252C%2520we%2520create%2520evacuation%2520scenarios%2520for%2520San%2520Francisco%2520using%2520real-world%2520road%2520networks%2520and%2520travel%2520times.%2520We%2520show%2520that%2520we%2520achieve%2520near-optimal%2520solution%2520quality%2520and%2520are%2520further%2520able%2520to%2520investigate%2520how%2520many%2520evacuation%2520vehicles%2520are%2520necessary%2520to%2520achieve%2520certain%2520bus-based%2520evacuation%2520quotas%2520given%2520a%2520predefined%2520evacuation%2520time%2520while%2520keeping%2520run%2520time%2520adequate.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GREAT-EER%3A%20Graph%20Edge%20Attention%20Network%20for%20Emergency%20Evacuation%20Responses&entry.906535625=Attila%20Lischka%20and%20Bal%C3%A1zs%20Kulcs%C3%A1r&entry.1292438233=Emergency%20situations%20that%20require%20the%20evacuation%20of%20urban%20areas%20can%20arise%20from%20man-made%20causes%20%28e.g.%2C%20terrorist%20attacks%20or%20industrial%20accidents%29%20or%20natural%20disasters%2C%20the%20latter%20becoming%20more%20frequent%20due%20to%20climate%20change.%20As%20a%20result%2C%20effective%20and%20fast%20methods%20to%20develop%20evacuation%20plans%20are%20of%20great%20importance.%20In%20this%20work%2C%20we%20identify%20and%20propose%20the%20Bus%20Evacuation%20Orienteering%20Problem%20%28BEOP%29%2C%20an%20NP-hard%20combinatorial%20optimization%20problem%20with%20the%20goal%20of%20evacuating%20as%20many%20people%20from%20an%20affected%20area%20by%20bus%20in%20a%20short%2C%20predefined%20amount%20of%20time.%20The%20purpose%20of%20bus-based%20evacuation%20is%20to%20reduce%20congestion%20and%20disorder%20that%20arises%20in%20purely%20car-focused%20evacuation%20scenarios.%20To%20solve%20the%20BEOP%2C%20we%20propose%20a%20deep%20reinforcement%20learning-based%20method%20utilizing%20graph%20learning%2C%20which%2C%20once%20trained%2C%20achieves%20fast%20inference%20speed%20and%20is%20able%20to%20create%20evacuation%20routes%20in%20fractions%20of%20seconds.%20We%20can%20bound%20the%20gap%20of%20our%20evacuation%20plans%20using%20an%20MILP%20formulation.%20To%20validate%20our%20method%2C%20we%20create%20evacuation%20scenarios%20for%20San%20Francisco%20using%20real-world%20road%20networks%20and%20travel%20times.%20We%20show%20that%20we%20achieve%20near-optimal%20solution%20quality%20and%20are%20further%20able%20to%20investigate%20how%20many%20evacuation%20vehicles%20are%20necessary%20to%20achieve%20certain%20bus-based%20evacuation%20quotas%20given%20a%20predefined%20evacuation%20time%20while%20keeping%20run%20time%20adequate.&entry.1838667208=http%3A//arxiv.org/abs/2602.14676v1&entry.124074799=Read"},
{"title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay", "author": "Yifan Sun and Jingyan Shen and Yibin Wang and Tianyu Chen and Zhendong Wang and Mingyuan Zhou and Huan Zhang", "abstract": "Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism inspired by experience replay in traditional RL. This technique reuses recent rollouts, lowering per-step computation while maintaining stable updates. Experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 23% to 62% while reaching the same level of performance as the original GRPO algorithm. Our code is available at https://github.com/ASTRAL-Group/data-efficient-llm-rl.", "link": "http://arxiv.org/abs/2506.05316v4", "date": "2026-02-16", "relevancy": 1.9484, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5029}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4967}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Data%20Efficiency%20for%20LLM%20Reinforcement%20Fine-tuning%20Through%20Difficulty-targeted%20Online%20Data%20Selection%20and%20Rollout%20Replay&body=Title%3A%20Improving%20Data%20Efficiency%20for%20LLM%20Reinforcement%20Fine-tuning%20Through%20Difficulty-targeted%20Online%20Data%20Selection%20and%20Rollout%20Replay%0AAuthor%3A%20Yifan%20Sun%20and%20Jingyan%20Shen%20and%20Yibin%20Wang%20and%20Tianyu%20Chen%20and%20Zhendong%20Wang%20and%20Mingyuan%20Zhou%20and%20Huan%20Zhang%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20has%20become%20an%20effective%20approach%20for%20fine-tuning%20large%20language%20models%20%28LLMs%29%2C%20particularly%20to%20enhance%20their%20reasoning%20capabilities.%20However%2C%20RL%20fine-tuning%20remains%20highly%20resource-intensive%2C%20and%20existing%20work%20has%20largely%20overlooked%20the%20problem%20of%20data%20efficiency.%20In%20this%20paper%2C%20we%20propose%20two%20techniques%20to%20improve%20data%20efficiency%20in%20LLM%20RL%20fine-tuning%3A%20difficulty-targeted%20online%20data%20selection%20and%20rollout%20replay.%20We%20introduce%20the%20notion%20of%20adaptive%20difficulty%20to%20guide%20online%20data%20selection%2C%20prioritizing%20questions%20of%20moderate%20difficulty%20that%20are%20more%20likely%20to%20yield%20informative%20learning%20signals.%20To%20estimate%20adaptive%20difficulty%20efficiently%2C%20we%20develop%20an%20attention-based%20framework%20that%20requires%20rollouts%20for%20only%20a%20small%20reference%20set%20of%20questions.%20The%20adaptive%20difficulty%20of%20the%20remaining%20questions%20is%20then%20estimated%20based%20on%20their%20similarity%20to%20this%20set.%20To%20further%20reduce%20rollout%20cost%2C%20we%20introduce%20a%20rollout%20replay%20mechanism%20inspired%20by%20experience%20replay%20in%20traditional%20RL.%20This%20technique%20reuses%20recent%20rollouts%2C%20lowering%20per-step%20computation%20while%20maintaining%20stable%20updates.%20Experiments%20across%206%20LLM-dataset%20combinations%20show%20that%20our%20method%20reduces%20RL%20fine-tuning%20time%20by%2023%25%20to%2062%25%20while%20reaching%20the%20same%20level%20of%20performance%20as%20the%20original%20GRPO%20algorithm.%20Our%20code%20is%20available%20at%20https%3A//github.com/ASTRAL-Group/data-efficient-llm-rl.%0ALink%3A%20http%3A//arxiv.org/abs/2506.05316v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Data%2520Efficiency%2520for%2520LLM%2520Reinforcement%2520Fine-tuning%2520Through%2520Difficulty-targeted%2520Online%2520Data%2520Selection%2520and%2520Rollout%2520Replay%26entry.906535625%3DYifan%2520Sun%2520and%2520Jingyan%2520Shen%2520and%2520Yibin%2520Wang%2520and%2520Tianyu%2520Chen%2520and%2520Zhendong%2520Wang%2520and%2520Mingyuan%2520Zhou%2520and%2520Huan%2520Zhang%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520has%2520become%2520an%2520effective%2520approach%2520for%2520fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520particularly%2520to%2520enhance%2520their%2520reasoning%2520capabilities.%2520However%252C%2520RL%2520fine-tuning%2520remains%2520highly%2520resource-intensive%252C%2520and%2520existing%2520work%2520has%2520largely%2520overlooked%2520the%2520problem%2520of%2520data%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520two%2520techniques%2520to%2520improve%2520data%2520efficiency%2520in%2520LLM%2520RL%2520fine-tuning%253A%2520difficulty-targeted%2520online%2520data%2520selection%2520and%2520rollout%2520replay.%2520We%2520introduce%2520the%2520notion%2520of%2520adaptive%2520difficulty%2520to%2520guide%2520online%2520data%2520selection%252C%2520prioritizing%2520questions%2520of%2520moderate%2520difficulty%2520that%2520are%2520more%2520likely%2520to%2520yield%2520informative%2520learning%2520signals.%2520To%2520estimate%2520adaptive%2520difficulty%2520efficiently%252C%2520we%2520develop%2520an%2520attention-based%2520framework%2520that%2520requires%2520rollouts%2520for%2520only%2520a%2520small%2520reference%2520set%2520of%2520questions.%2520The%2520adaptive%2520difficulty%2520of%2520the%2520remaining%2520questions%2520is%2520then%2520estimated%2520based%2520on%2520their%2520similarity%2520to%2520this%2520set.%2520To%2520further%2520reduce%2520rollout%2520cost%252C%2520we%2520introduce%2520a%2520rollout%2520replay%2520mechanism%2520inspired%2520by%2520experience%2520replay%2520in%2520traditional%2520RL.%2520This%2520technique%2520reuses%2520recent%2520rollouts%252C%2520lowering%2520per-step%2520computation%2520while%2520maintaining%2520stable%2520updates.%2520Experiments%2520across%25206%2520LLM-dataset%2520combinations%2520show%2520that%2520our%2520method%2520reduces%2520RL%2520fine-tuning%2520time%2520by%252023%2525%2520to%252062%2525%2520while%2520reaching%2520the%2520same%2520level%2520of%2520performance%2520as%2520the%2520original%2520GRPO%2520algorithm.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ASTRAL-Group/data-efficient-llm-rl.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.05316v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Data%20Efficiency%20for%20LLM%20Reinforcement%20Fine-tuning%20Through%20Difficulty-targeted%20Online%20Data%20Selection%20and%20Rollout%20Replay&entry.906535625=Yifan%20Sun%20and%20Jingyan%20Shen%20and%20Yibin%20Wang%20and%20Tianyu%20Chen%20and%20Zhendong%20Wang%20and%20Mingyuan%20Zhou%20and%20Huan%20Zhang&entry.1292438233=Reinforcement%20learning%20%28RL%29%20has%20become%20an%20effective%20approach%20for%20fine-tuning%20large%20language%20models%20%28LLMs%29%2C%20particularly%20to%20enhance%20their%20reasoning%20capabilities.%20However%2C%20RL%20fine-tuning%20remains%20highly%20resource-intensive%2C%20and%20existing%20work%20has%20largely%20overlooked%20the%20problem%20of%20data%20efficiency.%20In%20this%20paper%2C%20we%20propose%20two%20techniques%20to%20improve%20data%20efficiency%20in%20LLM%20RL%20fine-tuning%3A%20difficulty-targeted%20online%20data%20selection%20and%20rollout%20replay.%20We%20introduce%20the%20notion%20of%20adaptive%20difficulty%20to%20guide%20online%20data%20selection%2C%20prioritizing%20questions%20of%20moderate%20difficulty%20that%20are%20more%20likely%20to%20yield%20informative%20learning%20signals.%20To%20estimate%20adaptive%20difficulty%20efficiently%2C%20we%20develop%20an%20attention-based%20framework%20that%20requires%20rollouts%20for%20only%20a%20small%20reference%20set%20of%20questions.%20The%20adaptive%20difficulty%20of%20the%20remaining%20questions%20is%20then%20estimated%20based%20on%20their%20similarity%20to%20this%20set.%20To%20further%20reduce%20rollout%20cost%2C%20we%20introduce%20a%20rollout%20replay%20mechanism%20inspired%20by%20experience%20replay%20in%20traditional%20RL.%20This%20technique%20reuses%20recent%20rollouts%2C%20lowering%20per-step%20computation%20while%20maintaining%20stable%20updates.%20Experiments%20across%206%20LLM-dataset%20combinations%20show%20that%20our%20method%20reduces%20RL%20fine-tuning%20time%20by%2023%25%20to%2062%25%20while%20reaching%20the%20same%20level%20of%20performance%20as%20the%20original%20GRPO%20algorithm.%20Our%20code%20is%20available%20at%20https%3A//github.com/ASTRAL-Group/data-efficient-llm-rl.&entry.1838667208=http%3A//arxiv.org/abs/2506.05316v4&entry.124074799=Read"},
{"title": "AnyUp: Universal Feature Upsampling", "author": "Thomas Wimmer and Prune Truong and Marie-Julie Rakotosaona and Michael Oechsle and Federico Tombari and Bernt Schiele and Jan Eric Lenssen", "abstract": "We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an inference-time feature-agnostic upsampling architecture to alleviate this limitation and improve upsampling quality. In our experiments, AnyUp sets a new state of the art for upsampled features, generalizes to different feature types, and preserves feature semantics while being efficient and easy to apply to a wide range of downstream tasks.", "link": "http://arxiv.org/abs/2510.12764v2", "date": "2026-02-16", "relevancy": 1.9476, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.517}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4667}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyUp%3A%20Universal%20Feature%20Upsampling&body=Title%3A%20AnyUp%3A%20Universal%20Feature%20Upsampling%0AAuthor%3A%20Thomas%20Wimmer%20and%20Prune%20Truong%20and%20Marie-Julie%20Rakotosaona%20and%20Michael%20Oechsle%20and%20Federico%20Tombari%20and%20Bernt%20Schiele%20and%20Jan%20Eric%20Lenssen%0AAbstract%3A%20We%20introduce%20AnyUp%2C%20a%20method%20for%20feature%20upsampling%20that%20can%20be%20applied%20to%20any%20vision%20feature%20at%20any%20resolution%2C%20without%20encoder-specific%20training.%20Existing%20learning-based%20upsamplers%20for%20features%20like%20DINO%20or%20CLIP%20need%20to%20be%20re-trained%20for%20every%20feature%20extractor%20and%20thus%20do%20not%20generalize%20to%20different%20feature%20types%20at%20inference%20time.%20In%20this%20work%2C%20we%20propose%20an%20inference-time%20feature-agnostic%20upsampling%20architecture%20to%20alleviate%20this%20limitation%20and%20improve%20upsampling%20quality.%20In%20our%20experiments%2C%20AnyUp%20sets%20a%20new%20state%20of%20the%20art%20for%20upsampled%20features%2C%20generalizes%20to%20different%20feature%20types%2C%20and%20preserves%20feature%20semantics%20while%20being%20efficient%20and%20easy%20to%20apply%20to%20a%20wide%20range%20of%20downstream%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2510.12764v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyUp%253A%2520Universal%2520Feature%2520Upsampling%26entry.906535625%3DThomas%2520Wimmer%2520and%2520Prune%2520Truong%2520and%2520Marie-Julie%2520Rakotosaona%2520and%2520Michael%2520Oechsle%2520and%2520Federico%2520Tombari%2520and%2520Bernt%2520Schiele%2520and%2520Jan%2520Eric%2520Lenssen%26entry.1292438233%3DWe%2520introduce%2520AnyUp%252C%2520a%2520method%2520for%2520feature%2520upsampling%2520that%2520can%2520be%2520applied%2520to%2520any%2520vision%2520feature%2520at%2520any%2520resolution%252C%2520without%2520encoder-specific%2520training.%2520Existing%2520learning-based%2520upsamplers%2520for%2520features%2520like%2520DINO%2520or%2520CLIP%2520need%2520to%2520be%2520re-trained%2520for%2520every%2520feature%2520extractor%2520and%2520thus%2520do%2520not%2520generalize%2520to%2520different%2520feature%2520types%2520at%2520inference%2520time.%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520inference-time%2520feature-agnostic%2520upsampling%2520architecture%2520to%2520alleviate%2520this%2520limitation%2520and%2520improve%2520upsampling%2520quality.%2520In%2520our%2520experiments%252C%2520AnyUp%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%2520for%2520upsampled%2520features%252C%2520generalizes%2520to%2520different%2520feature%2520types%252C%2520and%2520preserves%2520feature%2520semantics%2520while%2520being%2520efficient%2520and%2520easy%2520to%2520apply%2520to%2520a%2520wide%2520range%2520of%2520downstream%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12764v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyUp%3A%20Universal%20Feature%20Upsampling&entry.906535625=Thomas%20Wimmer%20and%20Prune%20Truong%20and%20Marie-Julie%20Rakotosaona%20and%20Michael%20Oechsle%20and%20Federico%20Tombari%20and%20Bernt%20Schiele%20and%20Jan%20Eric%20Lenssen&entry.1292438233=We%20introduce%20AnyUp%2C%20a%20method%20for%20feature%20upsampling%20that%20can%20be%20applied%20to%20any%20vision%20feature%20at%20any%20resolution%2C%20without%20encoder-specific%20training.%20Existing%20learning-based%20upsamplers%20for%20features%20like%20DINO%20or%20CLIP%20need%20to%20be%20re-trained%20for%20every%20feature%20extractor%20and%20thus%20do%20not%20generalize%20to%20different%20feature%20types%20at%20inference%20time.%20In%20this%20work%2C%20we%20propose%20an%20inference-time%20feature-agnostic%20upsampling%20architecture%20to%20alleviate%20this%20limitation%20and%20improve%20upsampling%20quality.%20In%20our%20experiments%2C%20AnyUp%20sets%20a%20new%20state%20of%20the%20art%20for%20upsampled%20features%2C%20generalizes%20to%20different%20feature%20types%2C%20and%20preserves%20feature%20semantics%20while%20being%20efficient%20and%20easy%20to%20apply%20to%20a%20wide%20range%20of%20downstream%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2510.12764v2&entry.124074799=Read"},
{"title": "MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design", "author": "Gen Zhou and Sugitha Janarthanan and Lianghong Chen and Pingzhao Hu", "abstract": "To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.", "link": "http://arxiv.org/abs/2602.14926v1", "date": "2026-02-16", "relevancy": 1.9444, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4795}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAC-AMP%3A%20A%20Closed-Loop%20Multi-Agent%20Collaboration%20System%20for%20Multi-Objective%20Antimicrobial%20Peptide%20Design&body=Title%3A%20MAC-AMP%3A%20A%20Closed-Loop%20Multi-Agent%20Collaboration%20System%20for%20Multi-Objective%20Antimicrobial%20Peptide%20Design%0AAuthor%3A%20Gen%20Zhou%20and%20Sugitha%20Janarthanan%20and%20Lianghong%20Chen%20and%20Pingzhao%20Hu%0AAbstract%3A%20To%20address%20the%20global%20health%20threat%20of%20antimicrobial%20resistance%2C%20antimicrobial%20peptides%20%28AMP%29%20are%20being%20explored%20for%20their%20potent%20and%20promising%20ability%20to%20fight%20resistant%20pathogens.%20While%20artificial%20intelligence%20%28AI%29%20is%20being%20employed%20to%20advance%20AMP%20discovery%20and%20design%2C%20most%20AMP%20design%20models%20struggle%20to%20balance%20key%20goals%20like%20activity%2C%20toxicity%2C%20and%20novelty%2C%20using%20rigid%20or%20unclear%20scoring%20methods%20that%20make%20results%20hard%20to%20interpret%20and%20optimize.%20As%20the%20capabilities%20of%20Large%20Language%20Models%20%28LLM%29%20advance%20and%20evolve%20swiftly%2C%20we%20turn%20to%20AI%20multi-agent%20collaboration%20based%20on%20such%20models%20%28multi-agent%20LLMs%29%2C%20which%20show%20rapidly%20rising%20potential%20in%20complex%20scientific%20design%20scenarios.%20Based%20on%20this%2C%20we%20introduce%20MAC-AMP%2C%20a%20closed-loop%20multi-agent%20collaboration%20%28MAC%29%20system%20for%20multi-objective%20AMP%20design.%20The%20system%20implements%20a%20fully%20autonomous%20simulated%20peer%20review-adaptive%20reinforcement%20learning%20framework%20that%20requires%20only%20a%20task%20description%20and%20example%20dataset%20to%20design%20novel%20AMPs.%20The%20novelty%20of%20our%20work%20lies%20in%20introducing%20a%20closed-loop%20multi-agent%20system%20for%20AMP%20design%2C%20with%20cross-domain%20transferability%2C%20that%20supports%20multi-objective%20optimization%20while%20remaining%20explainable%20rather%20than%20a%20%27black%20box%27.%20Experiments%20show%20that%20MAC-AMP%20outperforms%20other%20AMP%20generative%20models%20by%20effectively%20optimizing%20AMP%20generation%20for%20multiple%20key%20molecular%20properties%2C%20demonstrating%20exceptional%20results%20in%20antibacterial%20activity%2C%20AMP%20likeliness%2C%20toxicity%20compliance%2C%20and%20structural%20reliability.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAC-AMP%253A%2520A%2520Closed-Loop%2520Multi-Agent%2520Collaboration%2520System%2520for%2520Multi-Objective%2520Antimicrobial%2520Peptide%2520Design%26entry.906535625%3DGen%2520Zhou%2520and%2520Sugitha%2520Janarthanan%2520and%2520Lianghong%2520Chen%2520and%2520Pingzhao%2520Hu%26entry.1292438233%3DTo%2520address%2520the%2520global%2520health%2520threat%2520of%2520antimicrobial%2520resistance%252C%2520antimicrobial%2520peptides%2520%2528AMP%2529%2520are%2520being%2520explored%2520for%2520their%2520potent%2520and%2520promising%2520ability%2520to%2520fight%2520resistant%2520pathogens.%2520While%2520artificial%2520intelligence%2520%2528AI%2529%2520is%2520being%2520employed%2520to%2520advance%2520AMP%2520discovery%2520and%2520design%252C%2520most%2520AMP%2520design%2520models%2520struggle%2520to%2520balance%2520key%2520goals%2520like%2520activity%252C%2520toxicity%252C%2520and%2520novelty%252C%2520using%2520rigid%2520or%2520unclear%2520scoring%2520methods%2520that%2520make%2520results%2520hard%2520to%2520interpret%2520and%2520optimize.%2520As%2520the%2520capabilities%2520of%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520advance%2520and%2520evolve%2520swiftly%252C%2520we%2520turn%2520to%2520AI%2520multi-agent%2520collaboration%2520based%2520on%2520such%2520models%2520%2528multi-agent%2520LLMs%2529%252C%2520which%2520show%2520rapidly%2520rising%2520potential%2520in%2520complex%2520scientific%2520design%2520scenarios.%2520Based%2520on%2520this%252C%2520we%2520introduce%2520MAC-AMP%252C%2520a%2520closed-loop%2520multi-agent%2520collaboration%2520%2528MAC%2529%2520system%2520for%2520multi-objective%2520AMP%2520design.%2520The%2520system%2520implements%2520a%2520fully%2520autonomous%2520simulated%2520peer%2520review-adaptive%2520reinforcement%2520learning%2520framework%2520that%2520requires%2520only%2520a%2520task%2520description%2520and%2520example%2520dataset%2520to%2520design%2520novel%2520AMPs.%2520The%2520novelty%2520of%2520our%2520work%2520lies%2520in%2520introducing%2520a%2520closed-loop%2520multi-agent%2520system%2520for%2520AMP%2520design%252C%2520with%2520cross-domain%2520transferability%252C%2520that%2520supports%2520multi-objective%2520optimization%2520while%2520remaining%2520explainable%2520rather%2520than%2520a%2520%2527black%2520box%2527.%2520Experiments%2520show%2520that%2520MAC-AMP%2520outperforms%2520other%2520AMP%2520generative%2520models%2520by%2520effectively%2520optimizing%2520AMP%2520generation%2520for%2520multiple%2520key%2520molecular%2520properties%252C%2520demonstrating%2520exceptional%2520results%2520in%2520antibacterial%2520activity%252C%2520AMP%2520likeliness%252C%2520toxicity%2520compliance%252C%2520and%2520structural%2520reliability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAC-AMP%3A%20A%20Closed-Loop%20Multi-Agent%20Collaboration%20System%20for%20Multi-Objective%20Antimicrobial%20Peptide%20Design&entry.906535625=Gen%20Zhou%20and%20Sugitha%20Janarthanan%20and%20Lianghong%20Chen%20and%20Pingzhao%20Hu&entry.1292438233=To%20address%20the%20global%20health%20threat%20of%20antimicrobial%20resistance%2C%20antimicrobial%20peptides%20%28AMP%29%20are%20being%20explored%20for%20their%20potent%20and%20promising%20ability%20to%20fight%20resistant%20pathogens.%20While%20artificial%20intelligence%20%28AI%29%20is%20being%20employed%20to%20advance%20AMP%20discovery%20and%20design%2C%20most%20AMP%20design%20models%20struggle%20to%20balance%20key%20goals%20like%20activity%2C%20toxicity%2C%20and%20novelty%2C%20using%20rigid%20or%20unclear%20scoring%20methods%20that%20make%20results%20hard%20to%20interpret%20and%20optimize.%20As%20the%20capabilities%20of%20Large%20Language%20Models%20%28LLM%29%20advance%20and%20evolve%20swiftly%2C%20we%20turn%20to%20AI%20multi-agent%20collaboration%20based%20on%20such%20models%20%28multi-agent%20LLMs%29%2C%20which%20show%20rapidly%20rising%20potential%20in%20complex%20scientific%20design%20scenarios.%20Based%20on%20this%2C%20we%20introduce%20MAC-AMP%2C%20a%20closed-loop%20multi-agent%20collaboration%20%28MAC%29%20system%20for%20multi-objective%20AMP%20design.%20The%20system%20implements%20a%20fully%20autonomous%20simulated%20peer%20review-adaptive%20reinforcement%20learning%20framework%20that%20requires%20only%20a%20task%20description%20and%20example%20dataset%20to%20design%20novel%20AMPs.%20The%20novelty%20of%20our%20work%20lies%20in%20introducing%20a%20closed-loop%20multi-agent%20system%20for%20AMP%20design%2C%20with%20cross-domain%20transferability%2C%20that%20supports%20multi-objective%20optimization%20while%20remaining%20explainable%20rather%20than%20a%20%27black%20box%27.%20Experiments%20show%20that%20MAC-AMP%20outperforms%20other%20AMP%20generative%20models%20by%20effectively%20optimizing%20AMP%20generation%20for%20multiple%20key%20molecular%20properties%2C%20demonstrating%20exceptional%20results%20in%20antibacterial%20activity%2C%20AMP%20likeliness%2C%20toxicity%20compliance%2C%20and%20structural%20reliability.&entry.1838667208=http%3A//arxiv.org/abs/2602.14926v1&entry.124074799=Read"},
{"title": "Unbiased Approximate Vector-Jacobian Products for Efficient Backpropagation", "author": "Killian Bakong and Laurent Massouli\u00e9 and Edouard Oyallon and Kevin Scaman", "abstract": "In this work we introduce methods to reduce the computational and memory costs of training deep neural networks. Our approach consists in replacing exact vector-jacobian products by randomized, unbiased approximations thereof during backpropagation. We provide a theoretical analysis of the trade-off between the number of epochs needed to achieve a target precision and the cost reduction for each epoch. We then identify specific unbiased estimates of vector-jacobian products for which we establish desirable optimality properties of minimal variance under sparsity constraints. Finally we provide in-depth experiments on multi-layer perceptrons, BagNets and Visual Transfomers architectures. These validate our theoretical results, and confirm the potential of our proposed unbiased randomized backpropagation approach for reducing the cost of deep learning.", "link": "http://arxiv.org/abs/2602.14701v1", "date": "2026-02-16", "relevancy": 1.9323, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4913}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4903}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unbiased%20Approximate%20Vector-Jacobian%20Products%20for%20Efficient%20Backpropagation&body=Title%3A%20Unbiased%20Approximate%20Vector-Jacobian%20Products%20for%20Efficient%20Backpropagation%0AAuthor%3A%20Killian%20Bakong%20and%20Laurent%20Massouli%C3%A9%20and%20Edouard%20Oyallon%20and%20Kevin%20Scaman%0AAbstract%3A%20In%20this%20work%20we%20introduce%20methods%20to%20reduce%20the%20computational%20and%20memory%20costs%20of%20training%20deep%20neural%20networks.%20Our%20approach%20consists%20in%20replacing%20exact%20vector-jacobian%20products%20by%20randomized%2C%20unbiased%20approximations%20thereof%20during%20backpropagation.%20We%20provide%20a%20theoretical%20analysis%20of%20the%20trade-off%20between%20the%20number%20of%20epochs%20needed%20to%20achieve%20a%20target%20precision%20and%20the%20cost%20reduction%20for%20each%20epoch.%20We%20then%20identify%20specific%20unbiased%20estimates%20of%20vector-jacobian%20products%20for%20which%20we%20establish%20desirable%20optimality%20properties%20of%20minimal%20variance%20under%20sparsity%20constraints.%20Finally%20we%20provide%20in-depth%20experiments%20on%20multi-layer%20perceptrons%2C%20BagNets%20and%20Visual%20Transfomers%20architectures.%20These%20validate%20our%20theoretical%20results%2C%20and%20confirm%20the%20potential%20of%20our%20proposed%20unbiased%20randomized%20backpropagation%20approach%20for%20reducing%20the%20cost%20of%20deep%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnbiased%2520Approximate%2520Vector-Jacobian%2520Products%2520for%2520Efficient%2520Backpropagation%26entry.906535625%3DKillian%2520Bakong%2520and%2520Laurent%2520Massouli%25C3%25A9%2520and%2520Edouard%2520Oyallon%2520and%2520Kevin%2520Scaman%26entry.1292438233%3DIn%2520this%2520work%2520we%2520introduce%2520methods%2520to%2520reduce%2520the%2520computational%2520and%2520memory%2520costs%2520of%2520training%2520deep%2520neural%2520networks.%2520Our%2520approach%2520consists%2520in%2520replacing%2520exact%2520vector-jacobian%2520products%2520by%2520randomized%252C%2520unbiased%2520approximations%2520thereof%2520during%2520backpropagation.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520of%2520the%2520trade-off%2520between%2520the%2520number%2520of%2520epochs%2520needed%2520to%2520achieve%2520a%2520target%2520precision%2520and%2520the%2520cost%2520reduction%2520for%2520each%2520epoch.%2520We%2520then%2520identify%2520specific%2520unbiased%2520estimates%2520of%2520vector-jacobian%2520products%2520for%2520which%2520we%2520establish%2520desirable%2520optimality%2520properties%2520of%2520minimal%2520variance%2520under%2520sparsity%2520constraints.%2520Finally%2520we%2520provide%2520in-depth%2520experiments%2520on%2520multi-layer%2520perceptrons%252C%2520BagNets%2520and%2520Visual%2520Transfomers%2520architectures.%2520These%2520validate%2520our%2520theoretical%2520results%252C%2520and%2520confirm%2520the%2520potential%2520of%2520our%2520proposed%2520unbiased%2520randomized%2520backpropagation%2520approach%2520for%2520reducing%2520the%2520cost%2520of%2520deep%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unbiased%20Approximate%20Vector-Jacobian%20Products%20for%20Efficient%20Backpropagation&entry.906535625=Killian%20Bakong%20and%20Laurent%20Massouli%C3%A9%20and%20Edouard%20Oyallon%20and%20Kevin%20Scaman&entry.1292438233=In%20this%20work%20we%20introduce%20methods%20to%20reduce%20the%20computational%20and%20memory%20costs%20of%20training%20deep%20neural%20networks.%20Our%20approach%20consists%20in%20replacing%20exact%20vector-jacobian%20products%20by%20randomized%2C%20unbiased%20approximations%20thereof%20during%20backpropagation.%20We%20provide%20a%20theoretical%20analysis%20of%20the%20trade-off%20between%20the%20number%20of%20epochs%20needed%20to%20achieve%20a%20target%20precision%20and%20the%20cost%20reduction%20for%20each%20epoch.%20We%20then%20identify%20specific%20unbiased%20estimates%20of%20vector-jacobian%20products%20for%20which%20we%20establish%20desirable%20optimality%20properties%20of%20minimal%20variance%20under%20sparsity%20constraints.%20Finally%20we%20provide%20in-depth%20experiments%20on%20multi-layer%20perceptrons%2C%20BagNets%20and%20Visual%20Transfomers%20architectures.%20These%20validate%20our%20theoretical%20results%2C%20and%20confirm%20the%20potential%20of%20our%20proposed%20unbiased%20randomized%20backpropagation%20approach%20for%20reducing%20the%20cost%20of%20deep%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2602.14701v1&entry.124074799=Read"},
{"title": "BFS-PO: Best-First Search for Large Reasoning Models", "author": "Fiorenzo Parascandolo and Wenhui Tan and Enver Sangineto and Ruihua Song and Rita Cucchiara", "abstract": "Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.", "link": "http://arxiv.org/abs/2602.14917v1", "date": "2026-02-16", "relevancy": 1.9269, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BFS-PO%3A%20Best-First%20Search%20for%20Large%20Reasoning%20Models&body=Title%3A%20BFS-PO%3A%20Best-First%20Search%20for%20Large%20Reasoning%20Models%0AAuthor%3A%20Fiorenzo%20Parascandolo%20and%20Wenhui%20Tan%20and%20Enver%20Sangineto%20and%20Ruihua%20Song%20and%20Rita%20Cucchiara%0AAbstract%3A%20Large%20Reasoning%20Models%20%28LRMs%29%20such%20as%20OpenAI%20o1%20and%20DeepSeek-R1%20have%20shown%20excellent%20performance%20in%20reasoning%20tasks%20using%20long%20reasoning%20chains.%20However%2C%20this%20has%20also%20led%20to%20a%20significant%20increase%20of%20computational%20costs%20and%20the%20generation%20of%20verbose%20output%2C%20a%20phenomenon%20known%20as%20overthinking.%20The%20tendency%20to%20overthinking%20is%20often%20exacerbated%20by%20Reinforcement%20Learning%20%28RL%29%20algorithms%20such%20as%20GRPO/DAPO.%20In%20this%20paper%2C%20we%20propose%20BFS-PO%2C%20an%20RL%20algorithm%20which%20alleviates%20this%20problem%20using%20a%20Best-First%20Search%20exploration%20strategy.%20Specifically%2C%20BFS-PO%20looks%20for%20the%20shortest%20correct%20answer%20using%20a%20backtracking%20mechanism%20based%20on%20maximum%20entropy%20nodes.%20By%20generating%20progressively%20shorter%20responses%20during%20training%2C%20BFS-PO%20learns%20to%20produce%20concise%20reasoning%20chains.%20Using%20different%20benchmarks%20and%20base%20LRMs%2C%20we%20show%20that%20BFS-PO%20can%20simultaneously%20increase%20the%20LRM%20accuracy%20and%20shorten%20its%20answers.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBFS-PO%253A%2520Best-First%2520Search%2520for%2520Large%2520Reasoning%2520Models%26entry.906535625%3DFiorenzo%2520Parascandolo%2520and%2520Wenhui%2520Tan%2520and%2520Enver%2520Sangineto%2520and%2520Ruihua%2520Song%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3DLarge%2520Reasoning%2520Models%2520%2528LRMs%2529%2520such%2520as%2520OpenAI%2520o1%2520and%2520DeepSeek-R1%2520have%2520shown%2520excellent%2520performance%2520in%2520reasoning%2520tasks%2520using%2520long%2520reasoning%2520chains.%2520However%252C%2520this%2520has%2520also%2520led%2520to%2520a%2520significant%2520increase%2520of%2520computational%2520costs%2520and%2520the%2520generation%2520of%2520verbose%2520output%252C%2520a%2520phenomenon%2520known%2520as%2520overthinking.%2520The%2520tendency%2520to%2520overthinking%2520is%2520often%2520exacerbated%2520by%2520Reinforcement%2520Learning%2520%2528RL%2529%2520algorithms%2520such%2520as%2520GRPO/DAPO.%2520In%2520this%2520paper%252C%2520we%2520propose%2520BFS-PO%252C%2520an%2520RL%2520algorithm%2520which%2520alleviates%2520this%2520problem%2520using%2520a%2520Best-First%2520Search%2520exploration%2520strategy.%2520Specifically%252C%2520BFS-PO%2520looks%2520for%2520the%2520shortest%2520correct%2520answer%2520using%2520a%2520backtracking%2520mechanism%2520based%2520on%2520maximum%2520entropy%2520nodes.%2520By%2520generating%2520progressively%2520shorter%2520responses%2520during%2520training%252C%2520BFS-PO%2520learns%2520to%2520produce%2520concise%2520reasoning%2520chains.%2520Using%2520different%2520benchmarks%2520and%2520base%2520LRMs%252C%2520we%2520show%2520that%2520BFS-PO%2520can%2520simultaneously%2520increase%2520the%2520LRM%2520accuracy%2520and%2520shorten%2520its%2520answers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BFS-PO%3A%20Best-First%20Search%20for%20Large%20Reasoning%20Models&entry.906535625=Fiorenzo%20Parascandolo%20and%20Wenhui%20Tan%20and%20Enver%20Sangineto%20and%20Ruihua%20Song%20and%20Rita%20Cucchiara&entry.1292438233=Large%20Reasoning%20Models%20%28LRMs%29%20such%20as%20OpenAI%20o1%20and%20DeepSeek-R1%20have%20shown%20excellent%20performance%20in%20reasoning%20tasks%20using%20long%20reasoning%20chains.%20However%2C%20this%20has%20also%20led%20to%20a%20significant%20increase%20of%20computational%20costs%20and%20the%20generation%20of%20verbose%20output%2C%20a%20phenomenon%20known%20as%20overthinking.%20The%20tendency%20to%20overthinking%20is%20often%20exacerbated%20by%20Reinforcement%20Learning%20%28RL%29%20algorithms%20such%20as%20GRPO/DAPO.%20In%20this%20paper%2C%20we%20propose%20BFS-PO%2C%20an%20RL%20algorithm%20which%20alleviates%20this%20problem%20using%20a%20Best-First%20Search%20exploration%20strategy.%20Specifically%2C%20BFS-PO%20looks%20for%20the%20shortest%20correct%20answer%20using%20a%20backtracking%20mechanism%20based%20on%20maximum%20entropy%20nodes.%20By%20generating%20progressively%20shorter%20responses%20during%20training%2C%20BFS-PO%20learns%20to%20produce%20concise%20reasoning%20chains.%20Using%20different%20benchmarks%20and%20base%20LRMs%2C%20we%20show%20that%20BFS-PO%20can%20simultaneously%20increase%20the%20LRM%20accuracy%20and%20shorten%20its%20answers.&entry.1838667208=http%3A//arxiv.org/abs/2602.14917v1&entry.124074799=Read"},
{"title": "Real-time Monocular 2D and 3D Perception of Endoluminal Scenes for Controlling Flexible Robotic Endoscopic Instruments", "author": "Ruofeng Wei and Kai Chen and Yui Lun Ng and Yiyao Ma and Justin Di-Lang Ho and Hon Sing Tong and Xiaomei Wang and Jing Dai and Ka-Wai Kwok and Qi Dou", "abstract": "Endoluminal surgery offers a minimally invasive option for early-stage gastrointestinal and urinary tract cancers but is limited by surgical tools and a steep learning curve. Robotic systems, particularly continuum robots, provide flexible instruments that enable precise tissue resection, potentially improving outcomes. This paper presents a visual perception platform for a continuum robotic system in endoluminal surgery. Our goal is to utilize monocular endoscopic image-based perception algorithms to identify position and orientation of flexible instruments and measure their distances from tissues. We introduce 2D and 3D learning-based perception algorithms and develop a physically-realistic simulator that models flexible instruments dynamics. This simulator generates realistic endoluminal scenes, enabling control of flexible robots and substantial data collection. Using a continuum robot prototype, we conducted module and system-level evaluations. Results show that our algorithms improve control of flexible instruments, reducing manipulation time by over 70% for trajectory-following tasks and enhancing understanding of surgical scenarios, leading to robust endoluminal surgeries.", "link": "http://arxiv.org/abs/2602.14666v1", "date": "2026-02-16", "relevancy": 1.7934, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6225}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5805}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-time%20Monocular%202D%20and%203D%20Perception%20of%20Endoluminal%20Scenes%20for%20Controlling%20Flexible%20Robotic%20Endoscopic%20Instruments&body=Title%3A%20Real-time%20Monocular%202D%20and%203D%20Perception%20of%20Endoluminal%20Scenes%20for%20Controlling%20Flexible%20Robotic%20Endoscopic%20Instruments%0AAuthor%3A%20Ruofeng%20Wei%20and%20Kai%20Chen%20and%20Yui%20Lun%20Ng%20and%20Yiyao%20Ma%20and%20Justin%20Di-Lang%20Ho%20and%20Hon%20Sing%20Tong%20and%20Xiaomei%20Wang%20and%20Jing%20Dai%20and%20Ka-Wai%20Kwok%20and%20Qi%20Dou%0AAbstract%3A%20Endoluminal%20surgery%20offers%20a%20minimally%20invasive%20option%20for%20early-stage%20gastrointestinal%20and%20urinary%20tract%20cancers%20but%20is%20limited%20by%20surgical%20tools%20and%20a%20steep%20learning%20curve.%20Robotic%20systems%2C%20particularly%20continuum%20robots%2C%20provide%20flexible%20instruments%20that%20enable%20precise%20tissue%20resection%2C%20potentially%20improving%20outcomes.%20This%20paper%20presents%20a%20visual%20perception%20platform%20for%20a%20continuum%20robotic%20system%20in%20endoluminal%20surgery.%20Our%20goal%20is%20to%20utilize%20monocular%20endoscopic%20image-based%20perception%20algorithms%20to%20identify%20position%20and%20orientation%20of%20flexible%20instruments%20and%20measure%20their%20distances%20from%20tissues.%20We%20introduce%202D%20and%203D%20learning-based%20perception%20algorithms%20and%20develop%20a%20physically-realistic%20simulator%20that%20models%20flexible%20instruments%20dynamics.%20This%20simulator%20generates%20realistic%20endoluminal%20scenes%2C%20enabling%20control%20of%20flexible%20robots%20and%20substantial%20data%20collection.%20Using%20a%20continuum%20robot%20prototype%2C%20we%20conducted%20module%20and%20system-level%20evaluations.%20Results%20show%20that%20our%20algorithms%20improve%20control%20of%20flexible%20instruments%2C%20reducing%20manipulation%20time%20by%20over%2070%25%20for%20trajectory-following%20tasks%20and%20enhancing%20understanding%20of%20surgical%20scenarios%2C%20leading%20to%20robust%20endoluminal%20surgeries.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-time%2520Monocular%25202D%2520and%25203D%2520Perception%2520of%2520Endoluminal%2520Scenes%2520for%2520Controlling%2520Flexible%2520Robotic%2520Endoscopic%2520Instruments%26entry.906535625%3DRuofeng%2520Wei%2520and%2520Kai%2520Chen%2520and%2520Yui%2520Lun%2520Ng%2520and%2520Yiyao%2520Ma%2520and%2520Justin%2520Di-Lang%2520Ho%2520and%2520Hon%2520Sing%2520Tong%2520and%2520Xiaomei%2520Wang%2520and%2520Jing%2520Dai%2520and%2520Ka-Wai%2520Kwok%2520and%2520Qi%2520Dou%26entry.1292438233%3DEndoluminal%2520surgery%2520offers%2520a%2520minimally%2520invasive%2520option%2520for%2520early-stage%2520gastrointestinal%2520and%2520urinary%2520tract%2520cancers%2520but%2520is%2520limited%2520by%2520surgical%2520tools%2520and%2520a%2520steep%2520learning%2520curve.%2520Robotic%2520systems%252C%2520particularly%2520continuum%2520robots%252C%2520provide%2520flexible%2520instruments%2520that%2520enable%2520precise%2520tissue%2520resection%252C%2520potentially%2520improving%2520outcomes.%2520This%2520paper%2520presents%2520a%2520visual%2520perception%2520platform%2520for%2520a%2520continuum%2520robotic%2520system%2520in%2520endoluminal%2520surgery.%2520Our%2520goal%2520is%2520to%2520utilize%2520monocular%2520endoscopic%2520image-based%2520perception%2520algorithms%2520to%2520identify%2520position%2520and%2520orientation%2520of%2520flexible%2520instruments%2520and%2520measure%2520their%2520distances%2520from%2520tissues.%2520We%2520introduce%25202D%2520and%25203D%2520learning-based%2520perception%2520algorithms%2520and%2520develop%2520a%2520physically-realistic%2520simulator%2520that%2520models%2520flexible%2520instruments%2520dynamics.%2520This%2520simulator%2520generates%2520realistic%2520endoluminal%2520scenes%252C%2520enabling%2520control%2520of%2520flexible%2520robots%2520and%2520substantial%2520data%2520collection.%2520Using%2520a%2520continuum%2520robot%2520prototype%252C%2520we%2520conducted%2520module%2520and%2520system-level%2520evaluations.%2520Results%2520show%2520that%2520our%2520algorithms%2520improve%2520control%2520of%2520flexible%2520instruments%252C%2520reducing%2520manipulation%2520time%2520by%2520over%252070%2525%2520for%2520trajectory-following%2520tasks%2520and%2520enhancing%2520understanding%2520of%2520surgical%2520scenarios%252C%2520leading%2520to%2520robust%2520endoluminal%2520surgeries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%20Monocular%202D%20and%203D%20Perception%20of%20Endoluminal%20Scenes%20for%20Controlling%20Flexible%20Robotic%20Endoscopic%20Instruments&entry.906535625=Ruofeng%20Wei%20and%20Kai%20Chen%20and%20Yui%20Lun%20Ng%20and%20Yiyao%20Ma%20and%20Justin%20Di-Lang%20Ho%20and%20Hon%20Sing%20Tong%20and%20Xiaomei%20Wang%20and%20Jing%20Dai%20and%20Ka-Wai%20Kwok%20and%20Qi%20Dou&entry.1292438233=Endoluminal%20surgery%20offers%20a%20minimally%20invasive%20option%20for%20early-stage%20gastrointestinal%20and%20urinary%20tract%20cancers%20but%20is%20limited%20by%20surgical%20tools%20and%20a%20steep%20learning%20curve.%20Robotic%20systems%2C%20particularly%20continuum%20robots%2C%20provide%20flexible%20instruments%20that%20enable%20precise%20tissue%20resection%2C%20potentially%20improving%20outcomes.%20This%20paper%20presents%20a%20visual%20perception%20platform%20for%20a%20continuum%20robotic%20system%20in%20endoluminal%20surgery.%20Our%20goal%20is%20to%20utilize%20monocular%20endoscopic%20image-based%20perception%20algorithms%20to%20identify%20position%20and%20orientation%20of%20flexible%20instruments%20and%20measure%20their%20distances%20from%20tissues.%20We%20introduce%202D%20and%203D%20learning-based%20perception%20algorithms%20and%20develop%20a%20physically-realistic%20simulator%20that%20models%20flexible%20instruments%20dynamics.%20This%20simulator%20generates%20realistic%20endoluminal%20scenes%2C%20enabling%20control%20of%20flexible%20robots%20and%20substantial%20data%20collection.%20Using%20a%20continuum%20robot%20prototype%2C%20we%20conducted%20module%20and%20system-level%20evaluations.%20Results%20show%20that%20our%20algorithms%20improve%20control%20of%20flexible%20instruments%2C%20reducing%20manipulation%20time%20by%20over%2070%25%20for%20trajectory-following%20tasks%20and%20enhancing%20understanding%20of%20surgical%20scenarios%2C%20leading%20to%20robust%20endoluminal%20surgeries.&entry.1838667208=http%3A//arxiv.org/abs/2602.14666v1&entry.124074799=Read"},
{"title": "A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting", "author": "Ons Saadallah and M\u00e1ty\u00e1s and\u00f3 and Tam\u00e1s G\u00e1bor Orosz", "abstract": "Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.", "link": "http://arxiv.org/abs/2602.01445v2", "date": "2026-02-16", "relevancy": 1.4007, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.477}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4672}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Meta-Knowledge-Augmented%20LLM%20Framework%20for%20Hyperparameter%20Optimization%20in%20Time-Series%20Forecasting&body=Title%3A%20A%20Meta-Knowledge-Augmented%20LLM%20Framework%20for%20Hyperparameter%20Optimization%20in%20Time-Series%20Forecasting%0AAuthor%3A%20Ons%20Saadallah%20and%20M%C3%A1ty%C3%A1s%20and%C3%B3%20and%20Tam%C3%A1s%20G%C3%A1bor%20Orosz%0AAbstract%3A%20Hyperparameter%20optimization%20%28HPO%29%20plays%20a%20central%20role%20in%20the%20performance%20of%20deep%20learning%20models%2C%20yet%20remains%20computationally%20expensive%20and%20difficult%20to%20interpret%2C%20particularly%20for%20time-series%20forecasting.%20While%20Bayesian%20Optimization%20%28BO%29%20is%20a%20standard%20approach%2C%20it%20typically%20treats%20tuning%20tasks%20independently%20and%20provides%20limited%20insight%20into%20its%20decisions.%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20offer%20new%20opportunities%20to%20incorporate%20structured%20prior%20knowledge%20and%20reasoning%20into%20optimization%20pipelines.%20We%20introduce%20LLM-AutoOpt%2C%20a%20hybrid%20HPO%20framework%20that%20combines%20BO%20with%20LLM-based%20contextual%20reasoning.%20The%20framework%20encodes%20dataset%20meta-features%2C%20model%20descriptions%2C%20historical%20optimization%20outcomes%2C%20and%20target%20objectives%20as%20structured%20meta-knowledge%20within%20LLM%20prompts%2C%20using%20BO%20to%20initialize%20the%20search%20and%20mitigate%20cold-start%20effects.%20This%20design%20enables%20context-aware%20and%20stable%20hyperparameter%20refinement%20while%20exposing%20the%20reasoning%20behind%20optimization%20decisions.%20Experiments%20on%20a%20multivariate%20time%20series%20forecasting%20benchmark%20demonstrate%20that%20LLM-AutoOpt%20achieves%20improved%20predictive%20performance%20and%20more%20interpretable%20optimization%20behavior%20compared%20to%20BO%20and%20LLM%20baselines%20without%20meta-knowledge.%0ALink%3A%20http%3A//arxiv.org/abs/2602.01445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Meta-Knowledge-Augmented%2520LLM%2520Framework%2520for%2520Hyperparameter%2520Optimization%2520in%2520Time-Series%2520Forecasting%26entry.906535625%3DOns%2520Saadallah%2520and%2520M%25C3%25A1ty%25C3%25A1s%2520and%25C3%25B3%2520and%2520Tam%25C3%25A1s%2520G%25C3%25A1bor%2520Orosz%26entry.1292438233%3DHyperparameter%2520optimization%2520%2528HPO%2529%2520plays%2520a%2520central%2520role%2520in%2520the%2520performance%2520of%2520deep%2520learning%2520models%252C%2520yet%2520remains%2520computationally%2520expensive%2520and%2520difficult%2520to%2520interpret%252C%2520particularly%2520for%2520time-series%2520forecasting.%2520While%2520Bayesian%2520Optimization%2520%2528BO%2529%2520is%2520a%2520standard%2520approach%252C%2520it%2520typically%2520treats%2520tuning%2520tasks%2520independently%2520and%2520provides%2520limited%2520insight%2520into%2520its%2520decisions.%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520new%2520opportunities%2520to%2520incorporate%2520structured%2520prior%2520knowledge%2520and%2520reasoning%2520into%2520optimization%2520pipelines.%2520We%2520introduce%2520LLM-AutoOpt%252C%2520a%2520hybrid%2520HPO%2520framework%2520that%2520combines%2520BO%2520with%2520LLM-based%2520contextual%2520reasoning.%2520The%2520framework%2520encodes%2520dataset%2520meta-features%252C%2520model%2520descriptions%252C%2520historical%2520optimization%2520outcomes%252C%2520and%2520target%2520objectives%2520as%2520structured%2520meta-knowledge%2520within%2520LLM%2520prompts%252C%2520using%2520BO%2520to%2520initialize%2520the%2520search%2520and%2520mitigate%2520cold-start%2520effects.%2520This%2520design%2520enables%2520context-aware%2520and%2520stable%2520hyperparameter%2520refinement%2520while%2520exposing%2520the%2520reasoning%2520behind%2520optimization%2520decisions.%2520Experiments%2520on%2520a%2520multivariate%2520time%2520series%2520forecasting%2520benchmark%2520demonstrate%2520that%2520LLM-AutoOpt%2520achieves%2520improved%2520predictive%2520performance%2520and%2520more%2520interpretable%2520optimization%2520behavior%2520compared%2520to%2520BO%2520and%2520LLM%2520baselines%2520without%2520meta-knowledge.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.01445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Meta-Knowledge-Augmented%20LLM%20Framework%20for%20Hyperparameter%20Optimization%20in%20Time-Series%20Forecasting&entry.906535625=Ons%20Saadallah%20and%20M%C3%A1ty%C3%A1s%20and%C3%B3%20and%20Tam%C3%A1s%20G%C3%A1bor%20Orosz&entry.1292438233=Hyperparameter%20optimization%20%28HPO%29%20plays%20a%20central%20role%20in%20the%20performance%20of%20deep%20learning%20models%2C%20yet%20remains%20computationally%20expensive%20and%20difficult%20to%20interpret%2C%20particularly%20for%20time-series%20forecasting.%20While%20Bayesian%20Optimization%20%28BO%29%20is%20a%20standard%20approach%2C%20it%20typically%20treats%20tuning%20tasks%20independently%20and%20provides%20limited%20insight%20into%20its%20decisions.%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20offer%20new%20opportunities%20to%20incorporate%20structured%20prior%20knowledge%20and%20reasoning%20into%20optimization%20pipelines.%20We%20introduce%20LLM-AutoOpt%2C%20a%20hybrid%20HPO%20framework%20that%20combines%20BO%20with%20LLM-based%20contextual%20reasoning.%20The%20framework%20encodes%20dataset%20meta-features%2C%20model%20descriptions%2C%20historical%20optimization%20outcomes%2C%20and%20target%20objectives%20as%20structured%20meta-knowledge%20within%20LLM%20prompts%2C%20using%20BO%20to%20initialize%20the%20search%20and%20mitigate%20cold-start%20effects.%20This%20design%20enables%20context-aware%20and%20stable%20hyperparameter%20refinement%20while%20exposing%20the%20reasoning%20behind%20optimization%20decisions.%20Experiments%20on%20a%20multivariate%20time%20series%20forecasting%20benchmark%20demonstrate%20that%20LLM-AutoOpt%20achieves%20improved%20predictive%20performance%20and%20more%20interpretable%20optimization%20behavior%20compared%20to%20BO%20and%20LLM%20baselines%20without%20meta-knowledge.&entry.1838667208=http%3A//arxiv.org/abs/2602.01445v2&entry.124074799=Read"},
{"title": "From User Preferences to Base Score Extraction Functions in Gradual Argumentation", "author": "Aniol Civit and Antonio Rago and Antonio Andriella and Guillem Aleny\u00e0 and Francesca Toni", "abstract": "Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \\emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \\emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \\emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.", "link": "http://arxiv.org/abs/2602.14674v1", "date": "2026-02-16", "relevancy": 1.3304, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4836}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4337}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20User%20Preferences%20to%20Base%20Score%20Extraction%20Functions%20in%20Gradual%20Argumentation&body=Title%3A%20From%20User%20Preferences%20to%20Base%20Score%20Extraction%20Functions%20in%20Gradual%20Argumentation%0AAuthor%3A%20Aniol%20Civit%20and%20Antonio%20Rago%20and%20Antonio%20Andriella%20and%20Guillem%20Aleny%C3%A0%20and%20Francesca%20Toni%0AAbstract%3A%20Gradual%20argumentation%20is%20a%20field%20of%20symbolic%20AI%20which%20is%20attracting%20attention%20for%20its%20ability%20to%20support%20transparent%20and%20contestable%20AI%20systems.%20It%20is%20considered%20a%20useful%20tool%20in%20domains%20such%20as%20decision-making%2C%20recommendation%2C%20debate%20analysis%2C%20and%20others.%20The%20outcomes%20in%20such%20domains%20are%20usually%20dependent%20on%20the%20arguments%27%20base%20scores%2C%20which%20must%20be%20selected%20carefully.%20Often%2C%20this%20selection%20process%20requires%20user%20expertise%20and%20may%20not%20always%20be%20straightforward.%20On%20the%20other%20hand%2C%20organising%20the%20arguments%20by%20preference%20could%20simplify%20the%20task.%20In%20this%20work%2C%20we%20introduce%20%5Cemph%7BBase%20Score%20Extraction%20Functions%7D%2C%20which%20provide%20a%20mapping%20from%20users%27%20preferences%20over%20arguments%20to%20base%20scores.%20These%20functions%20can%20be%20applied%20to%20the%20arguments%20of%20a%20%5Cemph%7BBipolar%20Argumentation%20Framework%7D%20%28BAF%29%2C%20supplemented%20with%20preferences%2C%20to%20obtain%20a%20%5Cemph%7BQuantitative%20Bipolar%20Argumentation%20Framework%7D%20%28QBAF%29%2C%20allowing%20the%20use%20of%20well-established%20computational%20tools%20in%20gradual%20argumentation.%20We%20outline%20the%20desirable%20properties%20of%20base%20score%20extraction%20functions%2C%20discuss%20some%20design%20choices%2C%20and%20provide%20an%20algorithm%20for%20base%20score%20extraction.%20Our%20method%20incorporates%20an%20approximation%20of%20non-linearities%20in%20human%20preferences%20to%20allow%20for%20better%20approximation%20of%20the%20real%20ones.%20Finally%2C%20we%20evaluate%20our%20approach%20both%20theoretically%20and%20experimentally%20in%20a%20robotics%20setting%2C%20and%20offer%20recommendations%20for%20selecting%20appropriate%20gradual%20semantics%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14674v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520User%2520Preferences%2520to%2520Base%2520Score%2520Extraction%2520Functions%2520in%2520Gradual%2520Argumentation%26entry.906535625%3DAniol%2520Civit%2520and%2520Antonio%2520Rago%2520and%2520Antonio%2520Andriella%2520and%2520Guillem%2520Aleny%25C3%25A0%2520and%2520Francesca%2520Toni%26entry.1292438233%3DGradual%2520argumentation%2520is%2520a%2520field%2520of%2520symbolic%2520AI%2520which%2520is%2520attracting%2520attention%2520for%2520its%2520ability%2520to%2520support%2520transparent%2520and%2520contestable%2520AI%2520systems.%2520It%2520is%2520considered%2520a%2520useful%2520tool%2520in%2520domains%2520such%2520as%2520decision-making%252C%2520recommendation%252C%2520debate%2520analysis%252C%2520and%2520others.%2520The%2520outcomes%2520in%2520such%2520domains%2520are%2520usually%2520dependent%2520on%2520the%2520arguments%2527%2520base%2520scores%252C%2520which%2520must%2520be%2520selected%2520carefully.%2520Often%252C%2520this%2520selection%2520process%2520requires%2520user%2520expertise%2520and%2520may%2520not%2520always%2520be%2520straightforward.%2520On%2520the%2520other%2520hand%252C%2520organising%2520the%2520arguments%2520by%2520preference%2520could%2520simplify%2520the%2520task.%2520In%2520this%2520work%252C%2520we%2520introduce%2520%255Cemph%257BBase%2520Score%2520Extraction%2520Functions%257D%252C%2520which%2520provide%2520a%2520mapping%2520from%2520users%2527%2520preferences%2520over%2520arguments%2520to%2520base%2520scores.%2520These%2520functions%2520can%2520be%2520applied%2520to%2520the%2520arguments%2520of%2520a%2520%255Cemph%257BBipolar%2520Argumentation%2520Framework%257D%2520%2528BAF%2529%252C%2520supplemented%2520with%2520preferences%252C%2520to%2520obtain%2520a%2520%255Cemph%257BQuantitative%2520Bipolar%2520Argumentation%2520Framework%257D%2520%2528QBAF%2529%252C%2520allowing%2520the%2520use%2520of%2520well-established%2520computational%2520tools%2520in%2520gradual%2520argumentation.%2520We%2520outline%2520the%2520desirable%2520properties%2520of%2520base%2520score%2520extraction%2520functions%252C%2520discuss%2520some%2520design%2520choices%252C%2520and%2520provide%2520an%2520algorithm%2520for%2520base%2520score%2520extraction.%2520Our%2520method%2520incorporates%2520an%2520approximation%2520of%2520non-linearities%2520in%2520human%2520preferences%2520to%2520allow%2520for%2520better%2520approximation%2520of%2520the%2520real%2520ones.%2520Finally%252C%2520we%2520evaluate%2520our%2520approach%2520both%2520theoretically%2520and%2520experimentally%2520in%2520a%2520robotics%2520setting%252C%2520and%2520offer%2520recommendations%2520for%2520selecting%2520appropriate%2520gradual%2520semantics%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14674v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20User%20Preferences%20to%20Base%20Score%20Extraction%20Functions%20in%20Gradual%20Argumentation&entry.906535625=Aniol%20Civit%20and%20Antonio%20Rago%20and%20Antonio%20Andriella%20and%20Guillem%20Aleny%C3%A0%20and%20Francesca%20Toni&entry.1292438233=Gradual%20argumentation%20is%20a%20field%20of%20symbolic%20AI%20which%20is%20attracting%20attention%20for%20its%20ability%20to%20support%20transparent%20and%20contestable%20AI%20systems.%20It%20is%20considered%20a%20useful%20tool%20in%20domains%20such%20as%20decision-making%2C%20recommendation%2C%20debate%20analysis%2C%20and%20others.%20The%20outcomes%20in%20such%20domains%20are%20usually%20dependent%20on%20the%20arguments%27%20base%20scores%2C%20which%20must%20be%20selected%20carefully.%20Often%2C%20this%20selection%20process%20requires%20user%20expertise%20and%20may%20not%20always%20be%20straightforward.%20On%20the%20other%20hand%2C%20organising%20the%20arguments%20by%20preference%20could%20simplify%20the%20task.%20In%20this%20work%2C%20we%20introduce%20%5Cemph%7BBase%20Score%20Extraction%20Functions%7D%2C%20which%20provide%20a%20mapping%20from%20users%27%20preferences%20over%20arguments%20to%20base%20scores.%20These%20functions%20can%20be%20applied%20to%20the%20arguments%20of%20a%20%5Cemph%7BBipolar%20Argumentation%20Framework%7D%20%28BAF%29%2C%20supplemented%20with%20preferences%2C%20to%20obtain%20a%20%5Cemph%7BQuantitative%20Bipolar%20Argumentation%20Framework%7D%20%28QBAF%29%2C%20allowing%20the%20use%20of%20well-established%20computational%20tools%20in%20gradual%20argumentation.%20We%20outline%20the%20desirable%20properties%20of%20base%20score%20extraction%20functions%2C%20discuss%20some%20design%20choices%2C%20and%20provide%20an%20algorithm%20for%20base%20score%20extraction.%20Our%20method%20incorporates%20an%20approximation%20of%20non-linearities%20in%20human%20preferences%20to%20allow%20for%20better%20approximation%20of%20the%20real%20ones.%20Finally%2C%20we%20evaluate%20our%20approach%20both%20theoretically%20and%20experimentally%20in%20a%20robotics%20setting%2C%20and%20offer%20recommendations%20for%20selecting%20appropriate%20gradual%20semantics%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2602.14674v1&entry.124074799=Read"},
{"title": "Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning", "author": "Yong Liu and Zirui Zhu and Chaoyu Gong and Minhao Cheng and Cho-Jui Hsieh and Yang You", "abstract": "While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, compared with exact gradients, ZO-based gradients usually exhibit an estimation error, which can significantly hurt the optimization process, leading to slower convergence and suboptimal solutions. In addition, we find that the estimation error will hurt more when adding to large weights instead of small weights. Based on this observation, this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Sparse-MeZO. Additionally, we develop a memory-optimized implementation for sparse masking, ensuring the algorithm requires only inference-level memory consumption, allowing Sparse-MeZO to fine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that Sparse-MeZO consistently improves both performance and convergence speed over MeZO without any overhead. For example, it achieves a 9\\% absolute accuracy improvement and 3.5x speedup over MeZO on the RTE task. Code is available at https://github.com/NUS-HPC-AI-Lab/SparseMeZO.", "link": "http://arxiv.org/abs/2402.15751v2", "date": "2026-02-16", "relevancy": 1.4732, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5007}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4951}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20MeZO%3A%20Less%20Parameters%20for%20Better%20Performance%20in%20Zeroth-Order%20LLM%20Fine-Tuning&body=Title%3A%20Sparse%20MeZO%3A%20Less%20Parameters%20for%20Better%20Performance%20in%20Zeroth-Order%20LLM%20Fine-Tuning%0AAuthor%3A%20Yong%20Liu%20and%20Zirui%20Zhu%20and%20Chaoyu%20Gong%20and%20Minhao%20Cheng%20and%20Cho-Jui%20Hsieh%20and%20Yang%20You%0AAbstract%3A%20While%20fine-tuning%20large%20language%20models%20%28LLMs%29%20for%20specific%20tasks%20often%20yields%20impressive%20results%2C%20it%20comes%20at%20the%20cost%20of%20memory%20inefficiency%20due%20to%20back-propagation%20in%20gradient-based%20training.%20Memory-efficient%20Zeroth-order%20%28MeZO%29%20optimizers%2C%20recently%20proposed%20to%20address%20this%20issue%2C%20only%20require%20forward%20passes%20during%20training%2C%20making%20them%20more%20memory-friendly.%20However%2C%20compared%20with%20exact%20gradients%2C%20ZO-based%20gradients%20usually%20exhibit%20an%20estimation%20error%2C%20which%20can%20significantly%20hurt%20the%20optimization%20process%2C%20leading%20to%20slower%20convergence%20and%20suboptimal%20solutions.%20In%20addition%2C%20we%20find%20that%20the%20estimation%20error%20will%20hurt%20more%20when%20adding%20to%20large%20weights%20instead%20of%20small%20weights.%20Based%20on%20this%20observation%2C%20this%20paper%20introduces%20Sparse%20MeZO%2C%20a%20novel%20memory-efficient%20zeroth-order%20optimization%20approach%20that%20applies%20ZO%20only%20to%20a%20carefully%20chosen%20subset%20of%20parameters.%20We%20propose%20a%20simple%20yet%20effective%20parameter%20selection%20scheme%20that%20yields%20significant%20performance%20gains%20with%20Sparse-MeZO.%20Additionally%2C%20we%20develop%20a%20memory-optimized%20implementation%20for%20sparse%20masking%2C%20ensuring%20the%20algorithm%20requires%20only%20inference-level%20memory%20consumption%2C%20allowing%20Sparse-MeZO%20to%20fine-tune%20LLaMA-30b%20on%20a%20single%20A100%20GPU.%20Experimental%20results%20illustrate%20that%20Sparse-MeZO%20consistently%20improves%20both%20performance%20and%20convergence%20speed%20over%20MeZO%20without%20any%20overhead.%20For%20example%2C%20it%20achieves%20a%209%5C%25%20absolute%20accuracy%20improvement%20and%203.5x%20speedup%20over%20MeZO%20on%20the%20RTE%20task.%20Code%20is%20available%20at%20https%3A//github.com/NUS-HPC-AI-Lab/SparseMeZO.%0ALink%3A%20http%3A//arxiv.org/abs/2402.15751v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520MeZO%253A%2520Less%2520Parameters%2520for%2520Better%2520Performance%2520in%2520Zeroth-Order%2520LLM%2520Fine-Tuning%26entry.906535625%3DYong%2520Liu%2520and%2520Zirui%2520Zhu%2520and%2520Chaoyu%2520Gong%2520and%2520Minhao%2520Cheng%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Yang%2520You%26entry.1292438233%3DWhile%2520fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%2520specific%2520tasks%2520often%2520yields%2520impressive%2520results%252C%2520it%2520comes%2520at%2520the%2520cost%2520of%2520memory%2520inefficiency%2520due%2520to%2520back-propagation%2520in%2520gradient-based%2520training.%2520Memory-efficient%2520Zeroth-order%2520%2528MeZO%2529%2520optimizers%252C%2520recently%2520proposed%2520to%2520address%2520this%2520issue%252C%2520only%2520require%2520forward%2520passes%2520during%2520training%252C%2520making%2520them%2520more%2520memory-friendly.%2520However%252C%2520compared%2520with%2520exact%2520gradients%252C%2520ZO-based%2520gradients%2520usually%2520exhibit%2520an%2520estimation%2520error%252C%2520which%2520can%2520significantly%2520hurt%2520the%2520optimization%2520process%252C%2520leading%2520to%2520slower%2520convergence%2520and%2520suboptimal%2520solutions.%2520In%2520addition%252C%2520we%2520find%2520that%2520the%2520estimation%2520error%2520will%2520hurt%2520more%2520when%2520adding%2520to%2520large%2520weights%2520instead%2520of%2520small%2520weights.%2520Based%2520on%2520this%2520observation%252C%2520this%2520paper%2520introduces%2520Sparse%2520MeZO%252C%2520a%2520novel%2520memory-efficient%2520zeroth-order%2520optimization%2520approach%2520that%2520applies%2520ZO%2520only%2520to%2520a%2520carefully%2520chosen%2520subset%2520of%2520parameters.%2520We%2520propose%2520a%2520simple%2520yet%2520effective%2520parameter%2520selection%2520scheme%2520that%2520yields%2520significant%2520performance%2520gains%2520with%2520Sparse-MeZO.%2520Additionally%252C%2520we%2520develop%2520a%2520memory-optimized%2520implementation%2520for%2520sparse%2520masking%252C%2520ensuring%2520the%2520algorithm%2520requires%2520only%2520inference-level%2520memory%2520consumption%252C%2520allowing%2520Sparse-MeZO%2520to%2520fine-tune%2520LLaMA-30b%2520on%2520a%2520single%2520A100%2520GPU.%2520Experimental%2520results%2520illustrate%2520that%2520Sparse-MeZO%2520consistently%2520improves%2520both%2520performance%2520and%2520convergence%2520speed%2520over%2520MeZO%2520without%2520any%2520overhead.%2520For%2520example%252C%2520it%2520achieves%2520a%25209%255C%2525%2520absolute%2520accuracy%2520improvement%2520and%25203.5x%2520speedup%2520over%2520MeZO%2520on%2520the%2520RTE%2520task.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/NUS-HPC-AI-Lab/SparseMeZO.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15751v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20MeZO%3A%20Less%20Parameters%20for%20Better%20Performance%20in%20Zeroth-Order%20LLM%20Fine-Tuning&entry.906535625=Yong%20Liu%20and%20Zirui%20Zhu%20and%20Chaoyu%20Gong%20and%20Minhao%20Cheng%20and%20Cho-Jui%20Hsieh%20and%20Yang%20You&entry.1292438233=While%20fine-tuning%20large%20language%20models%20%28LLMs%29%20for%20specific%20tasks%20often%20yields%20impressive%20results%2C%20it%20comes%20at%20the%20cost%20of%20memory%20inefficiency%20due%20to%20back-propagation%20in%20gradient-based%20training.%20Memory-efficient%20Zeroth-order%20%28MeZO%29%20optimizers%2C%20recently%20proposed%20to%20address%20this%20issue%2C%20only%20require%20forward%20passes%20during%20training%2C%20making%20them%20more%20memory-friendly.%20However%2C%20compared%20with%20exact%20gradients%2C%20ZO-based%20gradients%20usually%20exhibit%20an%20estimation%20error%2C%20which%20can%20significantly%20hurt%20the%20optimization%20process%2C%20leading%20to%20slower%20convergence%20and%20suboptimal%20solutions.%20In%20addition%2C%20we%20find%20that%20the%20estimation%20error%20will%20hurt%20more%20when%20adding%20to%20large%20weights%20instead%20of%20small%20weights.%20Based%20on%20this%20observation%2C%20this%20paper%20introduces%20Sparse%20MeZO%2C%20a%20novel%20memory-efficient%20zeroth-order%20optimization%20approach%20that%20applies%20ZO%20only%20to%20a%20carefully%20chosen%20subset%20of%20parameters.%20We%20propose%20a%20simple%20yet%20effective%20parameter%20selection%20scheme%20that%20yields%20significant%20performance%20gains%20with%20Sparse-MeZO.%20Additionally%2C%20we%20develop%20a%20memory-optimized%20implementation%20for%20sparse%20masking%2C%20ensuring%20the%20algorithm%20requires%20only%20inference-level%20memory%20consumption%2C%20allowing%20Sparse-MeZO%20to%20fine-tune%20LLaMA-30b%20on%20a%20single%20A100%20GPU.%20Experimental%20results%20illustrate%20that%20Sparse-MeZO%20consistently%20improves%20both%20performance%20and%20convergence%20speed%20over%20MeZO%20without%20any%20overhead.%20For%20example%2C%20it%20achieves%20a%209%5C%25%20absolute%20accuracy%20improvement%20and%203.5x%20speedup%20over%20MeZO%20on%20the%20RTE%20task.%20Code%20is%20available%20at%20https%3A//github.com/NUS-HPC-AI-Lab/SparseMeZO.&entry.1838667208=http%3A//arxiv.org/abs/2402.15751v2&entry.124074799=Read"},
{"title": "OPBench: A Graph Benchmark to Combat the Opioid Crisis", "author": "Tianyi Ma and Yiyang Li and Yiyue Qian and Zheyuan Zhang and Zehong Wang and Chuxu Zhang and Yanfang Ye", "abstract": "The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.", "link": "http://arxiv.org/abs/2602.14602v1", "date": "2026-02-16", "relevancy": 1.2701, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.433}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4169}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OPBench%3A%20A%20Graph%20Benchmark%20to%20Combat%20the%20Opioid%20Crisis&body=Title%3A%20OPBench%3A%20A%20Graph%20Benchmark%20to%20Combat%20the%20Opioid%20Crisis%0AAuthor%3A%20Tianyi%20Ma%20and%20Yiyang%20Li%20and%20Yiyue%20Qian%20and%20Zheyuan%20Zhang%20and%20Zehong%20Wang%20and%20Chuxu%20Zhang%20and%20Yanfang%20Ye%0AAbstract%3A%20The%20opioid%20epidemic%20continues%20to%20ravage%20communities%20worldwide%2C%20straining%20healthcare%20systems%2C%20disrupting%20families%2C%20and%20demanding%20urgent%20computational%20solutions.%20To%20combat%20this%20lethal%20opioid%20crisis%2C%20graph%20learning%20methods%20have%20emerged%20as%20a%20promising%20paradigm%20for%20modeling%20complex%20drug-related%20phenomena.%20However%2C%20a%20significant%20gap%20remains%3A%20there%20is%20no%20comprehensive%20benchmark%20for%20systematically%20evaluating%20these%20methods%20across%20real-world%20opioid%20crisis%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20OPBench%2C%20the%20first%20comprehensive%20opioid%20benchmark%20comprising%20five%20datasets%20across%20three%20critical%20application%20domains%3A%20opioid%20overdose%20detection%20from%20healthcare%20claims%2C%20illicit%20drug%20trafficking%20detection%20from%20digital%20platforms%2C%20and%20drug%20misuse%20prediction%20from%20dietary%20patterns.%20Specifically%2C%20OPBench%20incorporates%20diverse%20graph%20structures%2C%20including%20heterogeneous%20graphs%20and%20hypergraphs%2C%20to%20preserve%20the%20rich%20and%20complex%20relational%20information%20among%20drug-related%20data.%20To%20address%20data%20scarcity%2C%20we%20collaborate%20with%20domain%20experts%20and%20authoritative%20institutions%20to%20curate%20and%20annotate%20datasets%20while%20adhering%20to%20privacy%20and%20ethical%20guidelines.%20Furthermore%2C%20we%20establish%20a%20unified%20evaluation%20framework%20with%20standardized%20protocols%2C%20predefined%20data%20splits%2C%20and%20reproducible%20baselines%20to%20facilitate%20fair%20and%20systematic%20comparison%20among%20graph%20learning%20methods.%20Through%20extensive%20experiments%2C%20we%20analyze%20the%20strengths%20and%20limitations%20of%20existing%20graph%20learning%20methods%2C%20thereby%20providing%20actionable%20insights%20for%20future%20research%20in%20combating%20the%20opioid%20crisis.%20Our%20source%20code%20and%20datasets%20are%20available%20at%20https%3A//github.com/Tianyi-Billy-Ma/OPBench.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14602v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOPBench%253A%2520A%2520Graph%2520Benchmark%2520to%2520Combat%2520the%2520Opioid%2520Crisis%26entry.906535625%3DTianyi%2520Ma%2520and%2520Yiyang%2520Li%2520and%2520Yiyue%2520Qian%2520and%2520Zheyuan%2520Zhang%2520and%2520Zehong%2520Wang%2520and%2520Chuxu%2520Zhang%2520and%2520Yanfang%2520Ye%26entry.1292438233%3DThe%2520opioid%2520epidemic%2520continues%2520to%2520ravage%2520communities%2520worldwide%252C%2520straining%2520healthcare%2520systems%252C%2520disrupting%2520families%252C%2520and%2520demanding%2520urgent%2520computational%2520solutions.%2520To%2520combat%2520this%2520lethal%2520opioid%2520crisis%252C%2520graph%2520learning%2520methods%2520have%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%2520modeling%2520complex%2520drug-related%2520phenomena.%2520However%252C%2520a%2520significant%2520gap%2520remains%253A%2520there%2520is%2520no%2520comprehensive%2520benchmark%2520for%2520systematically%2520evaluating%2520these%2520methods%2520across%2520real-world%2520opioid%2520crisis%2520scenarios.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520OPBench%252C%2520the%2520first%2520comprehensive%2520opioid%2520benchmark%2520comprising%2520five%2520datasets%2520across%2520three%2520critical%2520application%2520domains%253A%2520opioid%2520overdose%2520detection%2520from%2520healthcare%2520claims%252C%2520illicit%2520drug%2520trafficking%2520detection%2520from%2520digital%2520platforms%252C%2520and%2520drug%2520misuse%2520prediction%2520from%2520dietary%2520patterns.%2520Specifically%252C%2520OPBench%2520incorporates%2520diverse%2520graph%2520structures%252C%2520including%2520heterogeneous%2520graphs%2520and%2520hypergraphs%252C%2520to%2520preserve%2520the%2520rich%2520and%2520complex%2520relational%2520information%2520among%2520drug-related%2520data.%2520To%2520address%2520data%2520scarcity%252C%2520we%2520collaborate%2520with%2520domain%2520experts%2520and%2520authoritative%2520institutions%2520to%2520curate%2520and%2520annotate%2520datasets%2520while%2520adhering%2520to%2520privacy%2520and%2520ethical%2520guidelines.%2520Furthermore%252C%2520we%2520establish%2520a%2520unified%2520evaluation%2520framework%2520with%2520standardized%2520protocols%252C%2520predefined%2520data%2520splits%252C%2520and%2520reproducible%2520baselines%2520to%2520facilitate%2520fair%2520and%2520systematic%2520comparison%2520among%2520graph%2520learning%2520methods.%2520Through%2520extensive%2520experiments%252C%2520we%2520analyze%2520the%2520strengths%2520and%2520limitations%2520of%2520existing%2520graph%2520learning%2520methods%252C%2520thereby%2520providing%2520actionable%2520insights%2520for%2520future%2520research%2520in%2520combating%2520the%2520opioid%2520crisis.%2520Our%2520source%2520code%2520and%2520datasets%2520are%2520available%2520at%2520https%253A//github.com/Tianyi-Billy-Ma/OPBench.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14602v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OPBench%3A%20A%20Graph%20Benchmark%20to%20Combat%20the%20Opioid%20Crisis&entry.906535625=Tianyi%20Ma%20and%20Yiyang%20Li%20and%20Yiyue%20Qian%20and%20Zheyuan%20Zhang%20and%20Zehong%20Wang%20and%20Chuxu%20Zhang%20and%20Yanfang%20Ye&entry.1292438233=The%20opioid%20epidemic%20continues%20to%20ravage%20communities%20worldwide%2C%20straining%20healthcare%20systems%2C%20disrupting%20families%2C%20and%20demanding%20urgent%20computational%20solutions.%20To%20combat%20this%20lethal%20opioid%20crisis%2C%20graph%20learning%20methods%20have%20emerged%20as%20a%20promising%20paradigm%20for%20modeling%20complex%20drug-related%20phenomena.%20However%2C%20a%20significant%20gap%20remains%3A%20there%20is%20no%20comprehensive%20benchmark%20for%20systematically%20evaluating%20these%20methods%20across%20real-world%20opioid%20crisis%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20introduce%20OPBench%2C%20the%20first%20comprehensive%20opioid%20benchmark%20comprising%20five%20datasets%20across%20three%20critical%20application%20domains%3A%20opioid%20overdose%20detection%20from%20healthcare%20claims%2C%20illicit%20drug%20trafficking%20detection%20from%20digital%20platforms%2C%20and%20drug%20misuse%20prediction%20from%20dietary%20patterns.%20Specifically%2C%20OPBench%20incorporates%20diverse%20graph%20structures%2C%20including%20heterogeneous%20graphs%20and%20hypergraphs%2C%20to%20preserve%20the%20rich%20and%20complex%20relational%20information%20among%20drug-related%20data.%20To%20address%20data%20scarcity%2C%20we%20collaborate%20with%20domain%20experts%20and%20authoritative%20institutions%20to%20curate%20and%20annotate%20datasets%20while%20adhering%20to%20privacy%20and%20ethical%20guidelines.%20Furthermore%2C%20we%20establish%20a%20unified%20evaluation%20framework%20with%20standardized%20protocols%2C%20predefined%20data%20splits%2C%20and%20reproducible%20baselines%20to%20facilitate%20fair%20and%20systematic%20comparison%20among%20graph%20learning%20methods.%20Through%20extensive%20experiments%2C%20we%20analyze%20the%20strengths%20and%20limitations%20of%20existing%20graph%20learning%20methods%2C%20thereby%20providing%20actionable%20insights%20for%20future%20research%20in%20combating%20the%20opioid%20crisis.%20Our%20source%20code%20and%20datasets%20are%20available%20at%20https%3A//github.com/Tianyi-Billy-Ma/OPBench.&entry.1838667208=http%3A//arxiv.org/abs/2602.14602v1&entry.124074799=Read"},
{"title": "The Well-Tempered Classifier: Some Elementary Properties of Temperature Scaling", "author": "Pierre-Alexandre Mattei and Bruno Loureiro", "abstract": "Temperature scaling is a simple method that allows to control the uncertainty of probabilistic models. It is mostly used in two contexts: improving the calibration of classifiers and tuning the stochasticity of large language models (LLMs). In both cases, temperature scaling is the most popular method for the job. Despite its popularity, a rigorous theoretical analysis of the properties of temperature scaling has remained elusive. We investigate here some of these properties. For classification, we show that increasing the temperature increases the uncertainty in the model in a very general sense (and in particular increases its entropy). However, for LLMs, we challenge the common claim that increasing temperature increases diversity. Furthermore, we introduce two new characterisations of temperature scaling. The first one is geometric: the tempered model is shown to be the information projection of the original model onto the set of models with a given entropy. The second characterisation clarifies the role of temperature scaling as a submodel of more general linear scalers such as matrix scaling and Dirichlet calibration: we show that temperature scaling is the only linear scaler that does not change the hard predictions of the model.", "link": "http://arxiv.org/abs/2602.14862v1", "date": "2026-02-16", "relevancy": 1.3363, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4757}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.452}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Well-Tempered%20Classifier%3A%20Some%20Elementary%20Properties%20of%20Temperature%20Scaling&body=Title%3A%20The%20Well-Tempered%20Classifier%3A%20Some%20Elementary%20Properties%20of%20Temperature%20Scaling%0AAuthor%3A%20Pierre-Alexandre%20Mattei%20and%20Bruno%20Loureiro%0AAbstract%3A%20Temperature%20scaling%20is%20a%20simple%20method%20that%20allows%20to%20control%20the%20uncertainty%20of%20probabilistic%20models.%20It%20is%20mostly%20used%20in%20two%20contexts%3A%20improving%20the%20calibration%20of%20classifiers%20and%20tuning%20the%20stochasticity%20of%20large%20language%20models%20%28LLMs%29.%20In%20both%20cases%2C%20temperature%20scaling%20is%20the%20most%20popular%20method%20for%20the%20job.%20Despite%20its%20popularity%2C%20a%20rigorous%20theoretical%20analysis%20of%20the%20properties%20of%20temperature%20scaling%20has%20remained%20elusive.%20We%20investigate%20here%20some%20of%20these%20properties.%20For%20classification%2C%20we%20show%20that%20increasing%20the%20temperature%20increases%20the%20uncertainty%20in%20the%20model%20in%20a%20very%20general%20sense%20%28and%20in%20particular%20increases%20its%20entropy%29.%20However%2C%20for%20LLMs%2C%20we%20challenge%20the%20common%20claim%20that%20increasing%20temperature%20increases%20diversity.%20Furthermore%2C%20we%20introduce%20two%20new%20characterisations%20of%20temperature%20scaling.%20The%20first%20one%20is%20geometric%3A%20the%20tempered%20model%20is%20shown%20to%20be%20the%20information%20projection%20of%20the%20original%20model%20onto%20the%20set%20of%20models%20with%20a%20given%20entropy.%20The%20second%20characterisation%20clarifies%20the%20role%20of%20temperature%20scaling%20as%20a%20submodel%20of%20more%20general%20linear%20scalers%20such%20as%20matrix%20scaling%20and%20Dirichlet%20calibration%3A%20we%20show%20that%20temperature%20scaling%20is%20the%20only%20linear%20scaler%20that%20does%20not%20change%20the%20hard%20predictions%20of%20the%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Well-Tempered%2520Classifier%253A%2520Some%2520Elementary%2520Properties%2520of%2520Temperature%2520Scaling%26entry.906535625%3DPierre-Alexandre%2520Mattei%2520and%2520Bruno%2520Loureiro%26entry.1292438233%3DTemperature%2520scaling%2520is%2520a%2520simple%2520method%2520that%2520allows%2520to%2520control%2520the%2520uncertainty%2520of%2520probabilistic%2520models.%2520It%2520is%2520mostly%2520used%2520in%2520two%2520contexts%253A%2520improving%2520the%2520calibration%2520of%2520classifiers%2520and%2520tuning%2520the%2520stochasticity%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520In%2520both%2520cases%252C%2520temperature%2520scaling%2520is%2520the%2520most%2520popular%2520method%2520for%2520the%2520job.%2520Despite%2520its%2520popularity%252C%2520a%2520rigorous%2520theoretical%2520analysis%2520of%2520the%2520properties%2520of%2520temperature%2520scaling%2520has%2520remained%2520elusive.%2520We%2520investigate%2520here%2520some%2520of%2520these%2520properties.%2520For%2520classification%252C%2520we%2520show%2520that%2520increasing%2520the%2520temperature%2520increases%2520the%2520uncertainty%2520in%2520the%2520model%2520in%2520a%2520very%2520general%2520sense%2520%2528and%2520in%2520particular%2520increases%2520its%2520entropy%2529.%2520However%252C%2520for%2520LLMs%252C%2520we%2520challenge%2520the%2520common%2520claim%2520that%2520increasing%2520temperature%2520increases%2520diversity.%2520Furthermore%252C%2520we%2520introduce%2520two%2520new%2520characterisations%2520of%2520temperature%2520scaling.%2520The%2520first%2520one%2520is%2520geometric%253A%2520the%2520tempered%2520model%2520is%2520shown%2520to%2520be%2520the%2520information%2520projection%2520of%2520the%2520original%2520model%2520onto%2520the%2520set%2520of%2520models%2520with%2520a%2520given%2520entropy.%2520The%2520second%2520characterisation%2520clarifies%2520the%2520role%2520of%2520temperature%2520scaling%2520as%2520a%2520submodel%2520of%2520more%2520general%2520linear%2520scalers%2520such%2520as%2520matrix%2520scaling%2520and%2520Dirichlet%2520calibration%253A%2520we%2520show%2520that%2520temperature%2520scaling%2520is%2520the%2520only%2520linear%2520scaler%2520that%2520does%2520not%2520change%2520the%2520hard%2520predictions%2520of%2520the%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Well-Tempered%20Classifier%3A%20Some%20Elementary%20Properties%20of%20Temperature%20Scaling&entry.906535625=Pierre-Alexandre%20Mattei%20and%20Bruno%20Loureiro&entry.1292438233=Temperature%20scaling%20is%20a%20simple%20method%20that%20allows%20to%20control%20the%20uncertainty%20of%20probabilistic%20models.%20It%20is%20mostly%20used%20in%20two%20contexts%3A%20improving%20the%20calibration%20of%20classifiers%20and%20tuning%20the%20stochasticity%20of%20large%20language%20models%20%28LLMs%29.%20In%20both%20cases%2C%20temperature%20scaling%20is%20the%20most%20popular%20method%20for%20the%20job.%20Despite%20its%20popularity%2C%20a%20rigorous%20theoretical%20analysis%20of%20the%20properties%20of%20temperature%20scaling%20has%20remained%20elusive.%20We%20investigate%20here%20some%20of%20these%20properties.%20For%20classification%2C%20we%20show%20that%20increasing%20the%20temperature%20increases%20the%20uncertainty%20in%20the%20model%20in%20a%20very%20general%20sense%20%28and%20in%20particular%20increases%20its%20entropy%29.%20However%2C%20for%20LLMs%2C%20we%20challenge%20the%20common%20claim%20that%20increasing%20temperature%20increases%20diversity.%20Furthermore%2C%20we%20introduce%20two%20new%20characterisations%20of%20temperature%20scaling.%20The%20first%20one%20is%20geometric%3A%20the%20tempered%20model%20is%20shown%20to%20be%20the%20information%20projection%20of%20the%20original%20model%20onto%20the%20set%20of%20models%20with%20a%20given%20entropy.%20The%20second%20characterisation%20clarifies%20the%20role%20of%20temperature%20scaling%20as%20a%20submodel%20of%20more%20general%20linear%20scalers%20such%20as%20matrix%20scaling%20and%20Dirichlet%20calibration%3A%20we%20show%20that%20temperature%20scaling%20is%20the%20only%20linear%20scaler%20that%20does%20not%20change%20the%20hard%20predictions%20of%20the%20model.&entry.1838667208=http%3A//arxiv.org/abs/2602.14862v1&entry.124074799=Read"},
{"title": "Orcheo: A Modular Full-Stack Platform for Conversational Search", "author": "Shaojie Jiang and Svitlana Vakulenko and Maarten de Rijke", "abstract": "Conversational search (CS) requires a complex software engineering pipeline that integrates query reformulation, ranking, and response generation. CS researchers currently face two barriers: the lack of a unified framework for efficiently sharing contributions with the community, and the difficulty of deploying end-to-end prototypes needed for user evaluation. We introduce Orcheo, an open-source platform designed to bridge this gap. Orcheo offers three key advantages: (i) A modular architecture promotes component reuse through single-file node modules, facilitating sharing and reproducibility in CS research; (ii) Production-ready infrastructure bridges the prototype-to-system gap via dual execution modes, secure credential management, and execution telemetry, with built-in AI coding support that lowers the learning curve; (iii) Starter-kit assets include 50+ off-the-shelf components for query understanding, ranking, and response generation, enabling the rapid bootstrapping of complete CS pipelines. We describe the framework architecture and validate Orcheo's utility through case studies that highlight modularity and ease of use. Orcheo is released as open source under the MIT License at https://github.com/ShaojieJiang/orcheo.", "link": "http://arxiv.org/abs/2602.14710v1", "date": "2026-02-16", "relevancy": 1.7781, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4414}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orcheo%3A%20A%20Modular%20Full-Stack%20Platform%20for%20Conversational%20Search&body=Title%3A%20Orcheo%3A%20A%20Modular%20Full-Stack%20Platform%20for%20Conversational%20Search%0AAuthor%3A%20Shaojie%20Jiang%20and%20Svitlana%20Vakulenko%20and%20Maarten%20de%20Rijke%0AAbstract%3A%20Conversational%20search%20%28CS%29%20requires%20a%20complex%20software%20engineering%20pipeline%20that%20integrates%20query%20reformulation%2C%20ranking%2C%20and%20response%20generation.%20CS%20researchers%20currently%20face%20two%20barriers%3A%20the%20lack%20of%20a%20unified%20framework%20for%20efficiently%20sharing%20contributions%20with%20the%20community%2C%20and%20the%20difficulty%20of%20deploying%20end-to-end%20prototypes%20needed%20for%20user%20evaluation.%20We%20introduce%20Orcheo%2C%20an%20open-source%20platform%20designed%20to%20bridge%20this%20gap.%20Orcheo%20offers%20three%20key%20advantages%3A%20%28i%29%20A%20modular%20architecture%20promotes%20component%20reuse%20through%20single-file%20node%20modules%2C%20facilitating%20sharing%20and%20reproducibility%20in%20CS%20research%3B%20%28ii%29%20Production-ready%20infrastructure%20bridges%20the%20prototype-to-system%20gap%20via%20dual%20execution%20modes%2C%20secure%20credential%20management%2C%20and%20execution%20telemetry%2C%20with%20built-in%20AI%20coding%20support%20that%20lowers%20the%20learning%20curve%3B%20%28iii%29%20Starter-kit%20assets%20include%2050%2B%20off-the-shelf%20components%20for%20query%20understanding%2C%20ranking%2C%20and%20response%20generation%2C%20enabling%20the%20rapid%20bootstrapping%20of%20complete%20CS%20pipelines.%20We%20describe%20the%20framework%20architecture%20and%20validate%20Orcheo%27s%20utility%20through%20case%20studies%20that%20highlight%20modularity%20and%20ease%20of%20use.%20Orcheo%20is%20released%20as%20open%20source%20under%20the%20MIT%20License%20at%20https%3A//github.com/ShaojieJiang/orcheo.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrcheo%253A%2520A%2520Modular%2520Full-Stack%2520Platform%2520for%2520Conversational%2520Search%26entry.906535625%3DShaojie%2520Jiang%2520and%2520Svitlana%2520Vakulenko%2520and%2520Maarten%2520de%2520Rijke%26entry.1292438233%3DConversational%2520search%2520%2528CS%2529%2520requires%2520a%2520complex%2520software%2520engineering%2520pipeline%2520that%2520integrates%2520query%2520reformulation%252C%2520ranking%252C%2520and%2520response%2520generation.%2520CS%2520researchers%2520currently%2520face%2520two%2520barriers%253A%2520the%2520lack%2520of%2520a%2520unified%2520framework%2520for%2520efficiently%2520sharing%2520contributions%2520with%2520the%2520community%252C%2520and%2520the%2520difficulty%2520of%2520deploying%2520end-to-end%2520prototypes%2520needed%2520for%2520user%2520evaluation.%2520We%2520introduce%2520Orcheo%252C%2520an%2520open-source%2520platform%2520designed%2520to%2520bridge%2520this%2520gap.%2520Orcheo%2520offers%2520three%2520key%2520advantages%253A%2520%2528i%2529%2520A%2520modular%2520architecture%2520promotes%2520component%2520reuse%2520through%2520single-file%2520node%2520modules%252C%2520facilitating%2520sharing%2520and%2520reproducibility%2520in%2520CS%2520research%253B%2520%2528ii%2529%2520Production-ready%2520infrastructure%2520bridges%2520the%2520prototype-to-system%2520gap%2520via%2520dual%2520execution%2520modes%252C%2520secure%2520credential%2520management%252C%2520and%2520execution%2520telemetry%252C%2520with%2520built-in%2520AI%2520coding%2520support%2520that%2520lowers%2520the%2520learning%2520curve%253B%2520%2528iii%2529%2520Starter-kit%2520assets%2520include%252050%252B%2520off-the-shelf%2520components%2520for%2520query%2520understanding%252C%2520ranking%252C%2520and%2520response%2520generation%252C%2520enabling%2520the%2520rapid%2520bootstrapping%2520of%2520complete%2520CS%2520pipelines.%2520We%2520describe%2520the%2520framework%2520architecture%2520and%2520validate%2520Orcheo%2527s%2520utility%2520through%2520case%2520studies%2520that%2520highlight%2520modularity%2520and%2520ease%2520of%2520use.%2520Orcheo%2520is%2520released%2520as%2520open%2520source%2520under%2520the%2520MIT%2520License%2520at%2520https%253A//github.com/ShaojieJiang/orcheo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orcheo%3A%20A%20Modular%20Full-Stack%20Platform%20for%20Conversational%20Search&entry.906535625=Shaojie%20Jiang%20and%20Svitlana%20Vakulenko%20and%20Maarten%20de%20Rijke&entry.1292438233=Conversational%20search%20%28CS%29%20requires%20a%20complex%20software%20engineering%20pipeline%20that%20integrates%20query%20reformulation%2C%20ranking%2C%20and%20response%20generation.%20CS%20researchers%20currently%20face%20two%20barriers%3A%20the%20lack%20of%20a%20unified%20framework%20for%20efficiently%20sharing%20contributions%20with%20the%20community%2C%20and%20the%20difficulty%20of%20deploying%20end-to-end%20prototypes%20needed%20for%20user%20evaluation.%20We%20introduce%20Orcheo%2C%20an%20open-source%20platform%20designed%20to%20bridge%20this%20gap.%20Orcheo%20offers%20three%20key%20advantages%3A%20%28i%29%20A%20modular%20architecture%20promotes%20component%20reuse%20through%20single-file%20node%20modules%2C%20facilitating%20sharing%20and%20reproducibility%20in%20CS%20research%3B%20%28ii%29%20Production-ready%20infrastructure%20bridges%20the%20prototype-to-system%20gap%20via%20dual%20execution%20modes%2C%20secure%20credential%20management%2C%20and%20execution%20telemetry%2C%20with%20built-in%20AI%20coding%20support%20that%20lowers%20the%20learning%20curve%3B%20%28iii%29%20Starter-kit%20assets%20include%2050%2B%20off-the-shelf%20components%20for%20query%20understanding%2C%20ranking%2C%20and%20response%20generation%2C%20enabling%20the%20rapid%20bootstrapping%20of%20complete%20CS%20pipelines.%20We%20describe%20the%20framework%20architecture%20and%20validate%20Orcheo%27s%20utility%20through%20case%20studies%20that%20highlight%20modularity%20and%20ease%20of%20use.%20Orcheo%20is%20released%20as%20open%20source%20under%20the%20MIT%20License%20at%20https%3A//github.com/ShaojieJiang/orcheo.&entry.1838667208=http%3A//arxiv.org/abs/2602.14710v1&entry.124074799=Read"},
{"title": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs", "author": "Hen Davidov and Shai Feldman and Gilad Freidkin and Yaniv Romano", "abstract": "We introduce time-to-unsafe-sampling, a novel safety measure for generative models, defined as the number of generations required by a large language model (LLM) to trigger an unsafe (e.g., toxic) response. While providing a new dimension for prompt-adaptive safety evaluation, quantifying time-to-unsafe-sampling is challenging: unsafe outputs are often rare in well-aligned models and thus may not be observed under any feasible sampling budget. To address this challenge, we frame this estimation problem as one of survival analysis. We build on recent developments in conformal prediction and propose a novel calibration technique to construct a lower predictive bound (LPB) on the time-to-unsafe-sampling of a given prompt with rigorous coverage guarantees. Our key technical innovation is an optimized sampling-budget allocation scheme that improves sample efficiency while maintaining distribution-free guarantees. Experiments on both synthetic and real data support our theoretical results and demonstrate the practical utility of our method for safety risk assessment in generative AI models.", "link": "http://arxiv.org/abs/2506.13593v5", "date": "2026-02-16", "relevancy": 1.4117, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4964}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4637}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrated%20Predictive%20Lower%20Bounds%20on%20Time-to-Unsafe-Sampling%20in%20LLMs&body=Title%3A%20Calibrated%20Predictive%20Lower%20Bounds%20on%20Time-to-Unsafe-Sampling%20in%20LLMs%0AAuthor%3A%20Hen%20Davidov%20and%20Shai%20Feldman%20and%20Gilad%20Freidkin%20and%20Yaniv%20Romano%0AAbstract%3A%20We%20introduce%20time-to-unsafe-sampling%2C%20a%20novel%20safety%20measure%20for%20generative%20models%2C%20defined%20as%20the%20number%20of%20generations%20required%20by%20a%20large%20language%20model%20%28LLM%29%20to%20trigger%20an%20unsafe%20%28e.g.%2C%20toxic%29%20response.%20While%20providing%20a%20new%20dimension%20for%20prompt-adaptive%20safety%20evaluation%2C%20quantifying%20time-to-unsafe-sampling%20is%20challenging%3A%20unsafe%20outputs%20are%20often%20rare%20in%20well-aligned%20models%20and%20thus%20may%20not%20be%20observed%20under%20any%20feasible%20sampling%20budget.%20To%20address%20this%20challenge%2C%20we%20frame%20this%20estimation%20problem%20as%20one%20of%20survival%20analysis.%20We%20build%20on%20recent%20developments%20in%20conformal%20prediction%20and%20propose%20a%20novel%20calibration%20technique%20to%20construct%20a%20lower%20predictive%20bound%20%28LPB%29%20on%20the%20time-to-unsafe-sampling%20of%20a%20given%20prompt%20with%20rigorous%20coverage%20guarantees.%20Our%20key%20technical%20innovation%20is%20an%20optimized%20sampling-budget%20allocation%20scheme%20that%20improves%20sample%20efficiency%20while%20maintaining%20distribution-free%20guarantees.%20Experiments%20on%20both%20synthetic%20and%20real%20data%20support%20our%20theoretical%20results%20and%20demonstrate%20the%20practical%20utility%20of%20our%20method%20for%20safety%20risk%20assessment%20in%20generative%20AI%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2506.13593v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrated%2520Predictive%2520Lower%2520Bounds%2520on%2520Time-to-Unsafe-Sampling%2520in%2520LLMs%26entry.906535625%3DHen%2520Davidov%2520and%2520Shai%2520Feldman%2520and%2520Gilad%2520Freidkin%2520and%2520Yaniv%2520Romano%26entry.1292438233%3DWe%2520introduce%2520time-to-unsafe-sampling%252C%2520a%2520novel%2520safety%2520measure%2520for%2520generative%2520models%252C%2520defined%2520as%2520the%2520number%2520of%2520generations%2520required%2520by%2520a%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520trigger%2520an%2520unsafe%2520%2528e.g.%252C%2520toxic%2529%2520response.%2520While%2520providing%2520a%2520new%2520dimension%2520for%2520prompt-adaptive%2520safety%2520evaluation%252C%2520quantifying%2520time-to-unsafe-sampling%2520is%2520challenging%253A%2520unsafe%2520outputs%2520are%2520often%2520rare%2520in%2520well-aligned%2520models%2520and%2520thus%2520may%2520not%2520be%2520observed%2520under%2520any%2520feasible%2520sampling%2520budget.%2520To%2520address%2520this%2520challenge%252C%2520we%2520frame%2520this%2520estimation%2520problem%2520as%2520one%2520of%2520survival%2520analysis.%2520We%2520build%2520on%2520recent%2520developments%2520in%2520conformal%2520prediction%2520and%2520propose%2520a%2520novel%2520calibration%2520technique%2520to%2520construct%2520a%2520lower%2520predictive%2520bound%2520%2528LPB%2529%2520on%2520the%2520time-to-unsafe-sampling%2520of%2520a%2520given%2520prompt%2520with%2520rigorous%2520coverage%2520guarantees.%2520Our%2520key%2520technical%2520innovation%2520is%2520an%2520optimized%2520sampling-budget%2520allocation%2520scheme%2520that%2520improves%2520sample%2520efficiency%2520while%2520maintaining%2520distribution-free%2520guarantees.%2520Experiments%2520on%2520both%2520synthetic%2520and%2520real%2520data%2520support%2520our%2520theoretical%2520results%2520and%2520demonstrate%2520the%2520practical%2520utility%2520of%2520our%2520method%2520for%2520safety%2520risk%2520assessment%2520in%2520generative%2520AI%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.13593v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrated%20Predictive%20Lower%20Bounds%20on%20Time-to-Unsafe-Sampling%20in%20LLMs&entry.906535625=Hen%20Davidov%20and%20Shai%20Feldman%20and%20Gilad%20Freidkin%20and%20Yaniv%20Romano&entry.1292438233=We%20introduce%20time-to-unsafe-sampling%2C%20a%20novel%20safety%20measure%20for%20generative%20models%2C%20defined%20as%20the%20number%20of%20generations%20required%20by%20a%20large%20language%20model%20%28LLM%29%20to%20trigger%20an%20unsafe%20%28e.g.%2C%20toxic%29%20response.%20While%20providing%20a%20new%20dimension%20for%20prompt-adaptive%20safety%20evaluation%2C%20quantifying%20time-to-unsafe-sampling%20is%20challenging%3A%20unsafe%20outputs%20are%20often%20rare%20in%20well-aligned%20models%20and%20thus%20may%20not%20be%20observed%20under%20any%20feasible%20sampling%20budget.%20To%20address%20this%20challenge%2C%20we%20frame%20this%20estimation%20problem%20as%20one%20of%20survival%20analysis.%20We%20build%20on%20recent%20developments%20in%20conformal%20prediction%20and%20propose%20a%20novel%20calibration%20technique%20to%20construct%20a%20lower%20predictive%20bound%20%28LPB%29%20on%20the%20time-to-unsafe-sampling%20of%20a%20given%20prompt%20with%20rigorous%20coverage%20guarantees.%20Our%20key%20technical%20innovation%20is%20an%20optimized%20sampling-budget%20allocation%20scheme%20that%20improves%20sample%20efficiency%20while%20maintaining%20distribution-free%20guarantees.%20Experiments%20on%20both%20synthetic%20and%20real%20data%20support%20our%20theoretical%20results%20and%20demonstrate%20the%20practical%20utility%20of%20our%20method%20for%20safety%20risk%20assessment%20in%20generative%20AI%20models.&entry.1838667208=http%3A//arxiv.org/abs/2506.13593v5&entry.124074799=Read"},
{"title": "EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing", "author": "Yehonathan Litman and Shikun Liu and Dario Seyb and Nicholas Milef and Yang Zhou and Carl Marshall and Shubham Tulsiani and Caleb Leak", "abstract": "High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.", "link": "http://arxiv.org/abs/2602.15031v1", "date": "2026-02-16", "relevancy": 1.1582, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5904}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5764}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5706}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EditCtrl%3A%20Disentangled%20Local%20and%20Global%20Control%20for%20Real-Time%20Generative%20Video%20Editing&body=Title%3A%20EditCtrl%3A%20Disentangled%20Local%20and%20Global%20Control%20for%20Real-Time%20Generative%20Video%20Editing%0AAuthor%3A%20Yehonathan%20Litman%20and%20Shikun%20Liu%20and%20Dario%20Seyb%20and%20Nicholas%20Milef%20and%20Yang%20Zhou%20and%20Carl%20Marshall%20and%20Shubham%20Tulsiani%20and%20Caleb%20Leak%0AAbstract%3A%20High-fidelity%20generative%20video%20editing%20has%20seen%20significant%20quality%20improvements%20by%20leveraging%20pre-trained%20video%20foundation%20models.%20However%2C%20their%20computational%20cost%20is%20a%20major%20bottleneck%2C%20as%20they%20are%20often%20designed%20to%20inefficiently%20process%20the%20full%20video%20context%20regardless%20of%20the%20inpainting%20mask%27s%20size%2C%20even%20for%20sparse%2C%20localized%20edits.%20In%20this%20paper%2C%20we%20introduce%20EditCtrl%2C%20an%20efficient%20video%20inpainting%20control%20framework%20that%20focuses%20computation%20only%20where%20it%20is%20needed.%20Our%20approach%20features%20a%20novel%20local%20video%20context%20module%20that%20operates%20solely%20on%20masked%20tokens%2C%20yielding%20a%20computational%20cost%20proportional%20to%20the%20edit%20size.%20This%20local-first%20generation%20is%20then%20guided%20by%20a%20lightweight%20temporal%20global%20context%20embedder%20that%20ensures%20video-wide%20context%20consistency%20with%20minimal%20overhead.%20Not%20only%20is%20EditCtrl%2010%20times%20more%20compute%20efficient%20than%20state-of-the-art%20generative%20editing%20methods%2C%20it%20even%20improves%20editing%20quality%20compared%20to%20methods%20designed%20with%20full-attention.%20Finally%2C%20we%20showcase%20how%20EditCtrl%20unlocks%20new%20capabilities%2C%20including%20multi-region%20editing%20with%20text%20prompts%20and%20autoregressive%20content%20propagation.%0ALink%3A%20http%3A//arxiv.org/abs/2602.15031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEditCtrl%253A%2520Disentangled%2520Local%2520and%2520Global%2520Control%2520for%2520Real-Time%2520Generative%2520Video%2520Editing%26entry.906535625%3DYehonathan%2520Litman%2520and%2520Shikun%2520Liu%2520and%2520Dario%2520Seyb%2520and%2520Nicholas%2520Milef%2520and%2520Yang%2520Zhou%2520and%2520Carl%2520Marshall%2520and%2520Shubham%2520Tulsiani%2520and%2520Caleb%2520Leak%26entry.1292438233%3DHigh-fidelity%2520generative%2520video%2520editing%2520has%2520seen%2520significant%2520quality%2520improvements%2520by%2520leveraging%2520pre-trained%2520video%2520foundation%2520models.%2520However%252C%2520their%2520computational%2520cost%2520is%2520a%2520major%2520bottleneck%252C%2520as%2520they%2520are%2520often%2520designed%2520to%2520inefficiently%2520process%2520the%2520full%2520video%2520context%2520regardless%2520of%2520the%2520inpainting%2520mask%2527s%2520size%252C%2520even%2520for%2520sparse%252C%2520localized%2520edits.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520EditCtrl%252C%2520an%2520efficient%2520video%2520inpainting%2520control%2520framework%2520that%2520focuses%2520computation%2520only%2520where%2520it%2520is%2520needed.%2520Our%2520approach%2520features%2520a%2520novel%2520local%2520video%2520context%2520module%2520that%2520operates%2520solely%2520on%2520masked%2520tokens%252C%2520yielding%2520a%2520computational%2520cost%2520proportional%2520to%2520the%2520edit%2520size.%2520This%2520local-first%2520generation%2520is%2520then%2520guided%2520by%2520a%2520lightweight%2520temporal%2520global%2520context%2520embedder%2520that%2520ensures%2520video-wide%2520context%2520consistency%2520with%2520minimal%2520overhead.%2520Not%2520only%2520is%2520EditCtrl%252010%2520times%2520more%2520compute%2520efficient%2520than%2520state-of-the-art%2520generative%2520editing%2520methods%252C%2520it%2520even%2520improves%2520editing%2520quality%2520compared%2520to%2520methods%2520designed%2520with%2520full-attention.%2520Finally%252C%2520we%2520showcase%2520how%2520EditCtrl%2520unlocks%2520new%2520capabilities%252C%2520including%2520multi-region%2520editing%2520with%2520text%2520prompts%2520and%2520autoregressive%2520content%2520propagation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.15031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EditCtrl%3A%20Disentangled%20Local%20and%20Global%20Control%20for%20Real-Time%20Generative%20Video%20Editing&entry.906535625=Yehonathan%20Litman%20and%20Shikun%20Liu%20and%20Dario%20Seyb%20and%20Nicholas%20Milef%20and%20Yang%20Zhou%20and%20Carl%20Marshall%20and%20Shubham%20Tulsiani%20and%20Caleb%20Leak&entry.1292438233=High-fidelity%20generative%20video%20editing%20has%20seen%20significant%20quality%20improvements%20by%20leveraging%20pre-trained%20video%20foundation%20models.%20However%2C%20their%20computational%20cost%20is%20a%20major%20bottleneck%2C%20as%20they%20are%20often%20designed%20to%20inefficiently%20process%20the%20full%20video%20context%20regardless%20of%20the%20inpainting%20mask%27s%20size%2C%20even%20for%20sparse%2C%20localized%20edits.%20In%20this%20paper%2C%20we%20introduce%20EditCtrl%2C%20an%20efficient%20video%20inpainting%20control%20framework%20that%20focuses%20computation%20only%20where%20it%20is%20needed.%20Our%20approach%20features%20a%20novel%20local%20video%20context%20module%20that%20operates%20solely%20on%20masked%20tokens%2C%20yielding%20a%20computational%20cost%20proportional%20to%20the%20edit%20size.%20This%20local-first%20generation%20is%20then%20guided%20by%20a%20lightweight%20temporal%20global%20context%20embedder%20that%20ensures%20video-wide%20context%20consistency%20with%20minimal%20overhead.%20Not%20only%20is%20EditCtrl%2010%20times%20more%20compute%20efficient%20than%20state-of-the-art%20generative%20editing%20methods%2C%20it%20even%20improves%20editing%20quality%20compared%20to%20methods%20designed%20with%20full-attention.%20Finally%2C%20we%20showcase%20how%20EditCtrl%20unlocks%20new%20capabilities%2C%20including%20multi-region%20editing%20with%20text%20prompts%20and%20autoregressive%20content%20propagation.&entry.1838667208=http%3A//arxiv.org/abs/2602.15031v1&entry.124074799=Read"},
{"title": "Concepts' Information Bottleneck Models", "author": "Karim Galliamov and Syed M Ahsan Kazmi and Adil Khan and Ad\u00edn Ram\u00edrez Rivera", "abstract": "Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.", "link": "http://arxiv.org/abs/2602.14626v1", "date": "2026-02-16", "relevancy": 1.908, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.478}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concepts%27%20Information%20Bottleneck%20Models&body=Title%3A%20Concepts%27%20Information%20Bottleneck%20Models%0AAuthor%3A%20Karim%20Galliamov%20and%20Syed%20M%20Ahsan%20Kazmi%20and%20Adil%20Khan%20and%20Ad%C3%ADn%20Ram%C3%ADrez%20Rivera%0AAbstract%3A%20Concept%20Bottleneck%20Models%20%28CBMs%29%20aim%20to%20deliver%20interpretable%20predictions%20by%20routing%20decisions%20through%20a%20human-understandable%20concept%20layer%2C%20yet%20they%20often%20suffer%20reduced%20accuracy%20and%20concept%20leakage%20that%20undermines%20faithfulness.%20We%20introduce%20an%20explicit%20Information%20Bottleneck%20regularizer%20on%20the%20concept%20layer%20that%20penalizes%20%24I%28X%3BC%29%24%20while%20preserving%20task-relevant%20information%20in%20%24I%28C%3BY%29%24%2C%20encouraging%20minimal-sufficient%20concept%20representations.%20We%20derive%20two%20practical%20variants%20%28a%20variational%20objective%20and%20an%20entropy-based%20surrogate%29%20and%20integrate%20them%20into%20standard%20CBM%20training%20without%20architectural%20changes%20or%20additional%20supervision.%20Evaluated%20across%20six%20CBM%20families%20and%20three%20benchmarks%2C%20the%20IB-regularized%20models%20consistently%20outperform%20their%20vanilla%20counterparts.%20Information-plane%20analyses%20further%20corroborate%20the%20intended%20behavior.%20These%20results%20indicate%20that%20enforcing%20a%20minimal-sufficient%20concept%20bottleneck%20improves%20both%20predictive%20performance%20and%20the%20reliability%20of%20concept-level%20interventions.%20The%20proposed%20regularizer%20offers%20a%20theoretic-grounded%2C%20architecture-agnostic%20path%20to%20more%20faithful%20and%20intervenable%20CBMs%2C%20resolving%20prior%20evaluation%20inconsistencies%20by%20aligning%20training%20protocols%20and%20demonstrating%20robust%20gains%20across%20model%20families%20and%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2602.14626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcepts%2527%2520Information%2520Bottleneck%2520Models%26entry.906535625%3DKarim%2520Galliamov%2520and%2520Syed%2520M%2520Ahsan%2520Kazmi%2520and%2520Adil%2520Khan%2520and%2520Ad%25C3%25ADn%2520Ram%25C3%25ADrez%2520Rivera%26entry.1292438233%3DConcept%2520Bottleneck%2520Models%2520%2528CBMs%2529%2520aim%2520to%2520deliver%2520interpretable%2520predictions%2520by%2520routing%2520decisions%2520through%2520a%2520human-understandable%2520concept%2520layer%252C%2520yet%2520they%2520often%2520suffer%2520reduced%2520accuracy%2520and%2520concept%2520leakage%2520that%2520undermines%2520faithfulness.%2520We%2520introduce%2520an%2520explicit%2520Information%2520Bottleneck%2520regularizer%2520on%2520the%2520concept%2520layer%2520that%2520penalizes%2520%2524I%2528X%253BC%2529%2524%2520while%2520preserving%2520task-relevant%2520information%2520in%2520%2524I%2528C%253BY%2529%2524%252C%2520encouraging%2520minimal-sufficient%2520concept%2520representations.%2520We%2520derive%2520two%2520practical%2520variants%2520%2528a%2520variational%2520objective%2520and%2520an%2520entropy-based%2520surrogate%2529%2520and%2520integrate%2520them%2520into%2520standard%2520CBM%2520training%2520without%2520architectural%2520changes%2520or%2520additional%2520supervision.%2520Evaluated%2520across%2520six%2520CBM%2520families%2520and%2520three%2520benchmarks%252C%2520the%2520IB-regularized%2520models%2520consistently%2520outperform%2520their%2520vanilla%2520counterparts.%2520Information-plane%2520analyses%2520further%2520corroborate%2520the%2520intended%2520behavior.%2520These%2520results%2520indicate%2520that%2520enforcing%2520a%2520minimal-sufficient%2520concept%2520bottleneck%2520improves%2520both%2520predictive%2520performance%2520and%2520the%2520reliability%2520of%2520concept-level%2520interventions.%2520The%2520proposed%2520regularizer%2520offers%2520a%2520theoretic-grounded%252C%2520architecture-agnostic%2520path%2520to%2520more%2520faithful%2520and%2520intervenable%2520CBMs%252C%2520resolving%2520prior%2520evaluation%2520inconsistencies%2520by%2520aligning%2520training%2520protocols%2520and%2520demonstrating%2520robust%2520gains%2520across%2520model%2520families%2520and%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2602.14626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concepts%27%20Information%20Bottleneck%20Models&entry.906535625=Karim%20Galliamov%20and%20Syed%20M%20Ahsan%20Kazmi%20and%20Adil%20Khan%20and%20Ad%C3%ADn%20Ram%C3%ADrez%20Rivera&entry.1292438233=Concept%20Bottleneck%20Models%20%28CBMs%29%20aim%20to%20deliver%20interpretable%20predictions%20by%20routing%20decisions%20through%20a%20human-understandable%20concept%20layer%2C%20yet%20they%20often%20suffer%20reduced%20accuracy%20and%20concept%20leakage%20that%20undermines%20faithfulness.%20We%20introduce%20an%20explicit%20Information%20Bottleneck%20regularizer%20on%20the%20concept%20layer%20that%20penalizes%20%24I%28X%3BC%29%24%20while%20preserving%20task-relevant%20information%20in%20%24I%28C%3BY%29%24%2C%20encouraging%20minimal-sufficient%20concept%20representations.%20We%20derive%20two%20practical%20variants%20%28a%20variational%20objective%20and%20an%20entropy-based%20surrogate%29%20and%20integrate%20them%20into%20standard%20CBM%20training%20without%20architectural%20changes%20or%20additional%20supervision.%20Evaluated%20across%20six%20CBM%20families%20and%20three%20benchmarks%2C%20the%20IB-regularized%20models%20consistently%20outperform%20their%20vanilla%20counterparts.%20Information-plane%20analyses%20further%20corroborate%20the%20intended%20behavior.%20These%20results%20indicate%20that%20enforcing%20a%20minimal-sufficient%20concept%20bottleneck%20improves%20both%20predictive%20performance%20and%20the%20reliability%20of%20concept-level%20interventions.%20The%20proposed%20regularizer%20offers%20a%20theoretic-grounded%2C%20architecture-agnostic%20path%20to%20more%20faithful%20and%20intervenable%20CBMs%2C%20resolving%20prior%20evaluation%20inconsistencies%20by%20aligning%20training%20protocols%20and%20demonstrating%20robust%20gains%20across%20model%20families%20and%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2602.14626v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


