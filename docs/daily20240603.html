<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240602.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "A Pixel Is Worth More Than One 3D Gaussians in Single-View 3D\n  Reconstruction", "author": "Jianghao Shen and Xue Nan and Tianfu Wu", "abstract": "  Learning 3D scene representation from a single-view image is a long-standing\nfundamental problem in computer vision, with the inherent ambiguity in\npredicting contents unseen from the input view. Built on the recently proposed\n3D Gaussian Splatting (3DGS), the Splatter Image method has made promising\nprogress on fast single-image novel view synthesis via learning a single 3D\nGaussian for each pixel based on the U-Net feature map of an input image.\nHowever, it has limited expressive power to represent occluded components that\nare not observable in the input view. To address this problem, this paper\npresents a Hierarchical Splatter Image method in which a pixel is worth more\nthan one 3D Gaussians. Specifically, each pixel is represented by a parent 3D\nGaussian and a small number of child 3D Gaussians. Parent 3D Gaussians are\nlearned as done in the vanilla Splatter Image. Child 3D Gaussians are learned\nvia a lightweight Multi-Layer Perceptron (MLP) which takes as input the\nprojected image features of a parent 3D Gaussian and the embedding of a target\ncamera view. Both parent and child 3D Gaussians are learned end-to-end in a\nstage-wise way. The joint condition of input image features from eyes of the\nparent Gaussians and the target camera position facilitates learning to\nallocate child Gaussians to ``see the unseen'', recovering the occluded details\nthat are often missed by parent Gaussians.\n  In experiments, the proposed method is tested on the ShapeNet-SRN and CO3D\ndatasets with state-of-the-art performance obtained, especially showing\npromising capabilities of reconstructing occluded contents in the input view.\n", "link": "http://arxiv.org/abs/2405.20310v2", "date": "2024-05-31", "relevancy": 3.1943, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.696}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6483}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Pixel%20Is%20Worth%20More%20Than%20One%203D%20Gaussians%20in%20Single-View%203D%0A%20%20Reconstruction&body=Title%3A%20A%20Pixel%20Is%20Worth%20More%20Than%20One%203D%20Gaussians%20in%20Single-View%203D%0A%20%20Reconstruction%0AAuthor%3A%20Jianghao%20Shen%20and%20Xue%20Nan%20and%20Tianfu%20Wu%0AAbstract%3A%20%20%20Learning%203D%20scene%20representation%20from%20a%20single-view%20image%20is%20a%20long-standing%0Afundamental%20problem%20in%20computer%20vision%2C%20with%20the%20inherent%20ambiguity%20in%0Apredicting%20contents%20unseen%20from%20the%20input%20view.%20Built%20on%20the%20recently%20proposed%0A3D%20Gaussian%20Splatting%20%283DGS%29%2C%20the%20Splatter%20Image%20method%20has%20made%20promising%0Aprogress%20on%20fast%20single-image%20novel%20view%20synthesis%20via%20learning%20a%20single%203D%0AGaussian%20for%20each%20pixel%20based%20on%20the%20U-Net%20feature%20map%20of%20an%20input%20image.%0AHowever%2C%20it%20has%20limited%20expressive%20power%20to%20represent%20occluded%20components%20that%0Aare%20not%20observable%20in%20the%20input%20view.%20To%20address%20this%20problem%2C%20this%20paper%0Apresents%20a%20Hierarchical%20Splatter%20Image%20method%20in%20which%20a%20pixel%20is%20worth%20more%0Athan%20one%203D%20Gaussians.%20Specifically%2C%20each%20pixel%20is%20represented%20by%20a%20parent%203D%0AGaussian%20and%20a%20small%20number%20of%20child%203D%20Gaussians.%20Parent%203D%20Gaussians%20are%0Alearned%20as%20done%20in%20the%20vanilla%20Splatter%20Image.%20Child%203D%20Gaussians%20are%20learned%0Avia%20a%20lightweight%20Multi-Layer%20Perceptron%20%28MLP%29%20which%20takes%20as%20input%20the%0Aprojected%20image%20features%20of%20a%20parent%203D%20Gaussian%20and%20the%20embedding%20of%20a%20target%0Acamera%20view.%20Both%20parent%20and%20child%203D%20Gaussians%20are%20learned%20end-to-end%20in%20a%0Astage-wise%20way.%20The%20joint%20condition%20of%20input%20image%20features%20from%20eyes%20of%20the%0Aparent%20Gaussians%20and%20the%20target%20camera%20position%20facilitates%20learning%20to%0Aallocate%20child%20Gaussians%20to%20%60%60see%20the%20unseen%27%27%2C%20recovering%20the%20occluded%20details%0Athat%20are%20often%20missed%20by%20parent%20Gaussians.%0A%20%20In%20experiments%2C%20the%20proposed%20method%20is%20tested%20on%20the%20ShapeNet-SRN%20and%20CO3D%0Adatasets%20with%20state-of-the-art%20performance%20obtained%2C%20especially%20showing%0Apromising%20capabilities%20of%20reconstructing%20occluded%20contents%20in%20the%20input%20view.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20310v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Pixel%2520Is%2520Worth%2520More%2520Than%2520One%25203D%2520Gaussians%2520in%2520Single-View%25203D%250A%2520%2520Reconstruction%26entry.906535625%3DJianghao%2520Shen%2520and%2520Xue%2520Nan%2520and%2520Tianfu%2520Wu%26entry.1292438233%3D%2520%2520Learning%25203D%2520scene%2520representation%2520from%2520a%2520single-view%2520image%2520is%2520a%2520long-standing%250Afundamental%2520problem%2520in%2520computer%2520vision%252C%2520with%2520the%2520inherent%2520ambiguity%2520in%250Apredicting%2520contents%2520unseen%2520from%2520the%2520input%2520view.%2520Built%2520on%2520the%2520recently%2520proposed%250A3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520the%2520Splatter%2520Image%2520method%2520has%2520made%2520promising%250Aprogress%2520on%2520fast%2520single-image%2520novel%2520view%2520synthesis%2520via%2520learning%2520a%2520single%25203D%250AGaussian%2520for%2520each%2520pixel%2520based%2520on%2520the%2520U-Net%2520feature%2520map%2520of%2520an%2520input%2520image.%250AHowever%252C%2520it%2520has%2520limited%2520expressive%2520power%2520to%2520represent%2520occluded%2520components%2520that%250Aare%2520not%2520observable%2520in%2520the%2520input%2520view.%2520To%2520address%2520this%2520problem%252C%2520this%2520paper%250Apresents%2520a%2520Hierarchical%2520Splatter%2520Image%2520method%2520in%2520which%2520a%2520pixel%2520is%2520worth%2520more%250Athan%2520one%25203D%2520Gaussians.%2520Specifically%252C%2520each%2520pixel%2520is%2520represented%2520by%2520a%2520parent%25203D%250AGaussian%2520and%2520a%2520small%2520number%2520of%2520child%25203D%2520Gaussians.%2520Parent%25203D%2520Gaussians%2520are%250Alearned%2520as%2520done%2520in%2520the%2520vanilla%2520Splatter%2520Image.%2520Child%25203D%2520Gaussians%2520are%2520learned%250Avia%2520a%2520lightweight%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%2520which%2520takes%2520as%2520input%2520the%250Aprojected%2520image%2520features%2520of%2520a%2520parent%25203D%2520Gaussian%2520and%2520the%2520embedding%2520of%2520a%2520target%250Acamera%2520view.%2520Both%2520parent%2520and%2520child%25203D%2520Gaussians%2520are%2520learned%2520end-to-end%2520in%2520a%250Astage-wise%2520way.%2520The%2520joint%2520condition%2520of%2520input%2520image%2520features%2520from%2520eyes%2520of%2520the%250Aparent%2520Gaussians%2520and%2520the%2520target%2520camera%2520position%2520facilitates%2520learning%2520to%250Aallocate%2520child%2520Gaussians%2520to%2520%2560%2560see%2520the%2520unseen%2527%2527%252C%2520recovering%2520the%2520occluded%2520details%250Athat%2520are%2520often%2520missed%2520by%2520parent%2520Gaussians.%250A%2520%2520In%2520experiments%252C%2520the%2520proposed%2520method%2520is%2520tested%2520on%2520the%2520ShapeNet-SRN%2520and%2520CO3D%250Adatasets%2520with%2520state-of-the-art%2520performance%2520obtained%252C%2520especially%2520showing%250Apromising%2520capabilities%2520of%2520reconstructing%2520occluded%2520contents%2520in%2520the%2520input%2520view.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20310v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Pixel%20Is%20Worth%20More%20Than%20One%203D%20Gaussians%20in%20Single-View%203D%0A%20%20Reconstruction&entry.906535625=Jianghao%20Shen%20and%20Xue%20Nan%20and%20Tianfu%20Wu&entry.1292438233=%20%20Learning%203D%20scene%20representation%20from%20a%20single-view%20image%20is%20a%20long-standing%0Afundamental%20problem%20in%20computer%20vision%2C%20with%20the%20inherent%20ambiguity%20in%0Apredicting%20contents%20unseen%20from%20the%20input%20view.%20Built%20on%20the%20recently%20proposed%0A3D%20Gaussian%20Splatting%20%283DGS%29%2C%20the%20Splatter%20Image%20method%20has%20made%20promising%0Aprogress%20on%20fast%20single-image%20novel%20view%20synthesis%20via%20learning%20a%20single%203D%0AGaussian%20for%20each%20pixel%20based%20on%20the%20U-Net%20feature%20map%20of%20an%20input%20image.%0AHowever%2C%20it%20has%20limited%20expressive%20power%20to%20represent%20occluded%20components%20that%0Aare%20not%20observable%20in%20the%20input%20view.%20To%20address%20this%20problem%2C%20this%20paper%0Apresents%20a%20Hierarchical%20Splatter%20Image%20method%20in%20which%20a%20pixel%20is%20worth%20more%0Athan%20one%203D%20Gaussians.%20Specifically%2C%20each%20pixel%20is%20represented%20by%20a%20parent%203D%0AGaussian%20and%20a%20small%20number%20of%20child%203D%20Gaussians.%20Parent%203D%20Gaussians%20are%0Alearned%20as%20done%20in%20the%20vanilla%20Splatter%20Image.%20Child%203D%20Gaussians%20are%20learned%0Avia%20a%20lightweight%20Multi-Layer%20Perceptron%20%28MLP%29%20which%20takes%20as%20input%20the%0Aprojected%20image%20features%20of%20a%20parent%203D%20Gaussian%20and%20the%20embedding%20of%20a%20target%0Acamera%20view.%20Both%20parent%20and%20child%203D%20Gaussians%20are%20learned%20end-to-end%20in%20a%0Astage-wise%20way.%20The%20joint%20condition%20of%20input%20image%20features%20from%20eyes%20of%20the%0Aparent%20Gaussians%20and%20the%20target%20camera%20position%20facilitates%20learning%20to%0Aallocate%20child%20Gaussians%20to%20%60%60see%20the%20unseen%27%27%2C%20recovering%20the%20occluded%20details%0Athat%20are%20often%20missed%20by%20parent%20Gaussians.%0A%20%20In%20experiments%2C%20the%20proposed%20method%20is%20tested%20on%20the%20ShapeNet-SRN%20and%20CO3D%0Adatasets%20with%20state-of-the-art%20performance%20obtained%2C%20especially%20showing%0Apromising%20capabilities%20of%20reconstructing%20occluded%20contents%20in%20the%20input%20view.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20310v2&entry.124074799=Read"},
{"title": "360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization\n  with Cross-device Queries", "author": "Huajian Huang and Changkun Liu and Yipeng Zhu and Hui Cheng and Tristan Braud and Sai-Kit Yeung", "abstract": "  Portable 360$^\\circ$ cameras are becoming a cheap and efficient tool to\nestablish large visual databases. By capturing omnidirectional views of a\nscene, these cameras could expedite building environment models that are\nessential for visual localization. However, such an advantage is often\noverlooked due to the lack of valuable datasets. This paper introduces a new\nbenchmark dataset, 360Loc, composed of 360$^\\circ$ images with ground truth\nposes for visual localization. We present a practical implementation of\n360$^\\circ$ mapping combining 360$^\\circ$ images with lidar data to generate\nthe ground truth 6DoF poses. 360Loc is the first dataset and benchmark that\nexplores the challenge of cross-device visual positioning, involving\n360$^\\circ$ reference frames, and query frames from pinhole, ultra-wide FoV\nfisheye, and 360$^\\circ$ cameras. We propose a virtual camera approach to\ngenerate lower-FoV query frames from 360$^\\circ$ images, which ensures a fair\ncomparison of performance among different query types in visual localization\ntasks. We also extend this virtual camera approach to feature matching-based\nand pose regression-based methods to alleviate the performance loss caused by\nthe cross-device domain gap, and evaluate its effectiveness against\nstate-of-the-art baselines. We demonstrate that omnidirectional visual\nlocalization is more robust in challenging large-scale scenes with symmetries\nand repetitive structures. These results provide new insights into 360-camera\nmapping and omnidirectional visual localization with cross-device queries.\n", "link": "http://arxiv.org/abs/2311.17389v3", "date": "2024-05-31", "relevancy": 3.0101, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6131}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6124}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5805}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20360Loc%3A%20A%20Dataset%20and%20Benchmark%20for%20Omnidirectional%20Visual%20Localization%0A%20%20with%20Cross-device%20Queries&body=Title%3A%20360Loc%3A%20A%20Dataset%20and%20Benchmark%20for%20Omnidirectional%20Visual%20Localization%0A%20%20with%20Cross-device%20Queries%0AAuthor%3A%20Huajian%20Huang%20and%20Changkun%20Liu%20and%20Yipeng%20Zhu%20and%20Hui%20Cheng%20and%20Tristan%20Braud%20and%20Sai-Kit%20Yeung%0AAbstract%3A%20%20%20Portable%20360%24%5E%5Ccirc%24%20cameras%20are%20becoming%20a%20cheap%20and%20efficient%20tool%20to%0Aestablish%20large%20visual%20databases.%20By%20capturing%20omnidirectional%20views%20of%20a%0Ascene%2C%20these%20cameras%20could%20expedite%20building%20environment%20models%20that%20are%0Aessential%20for%20visual%20localization.%20However%2C%20such%20an%20advantage%20is%20often%0Aoverlooked%20due%20to%20the%20lack%20of%20valuable%20datasets.%20This%20paper%20introduces%20a%20new%0Abenchmark%20dataset%2C%20360Loc%2C%20composed%20of%20360%24%5E%5Ccirc%24%20images%20with%20ground%20truth%0Aposes%20for%20visual%20localization.%20We%20present%20a%20practical%20implementation%20of%0A360%24%5E%5Ccirc%24%20mapping%20combining%20360%24%5E%5Ccirc%24%20images%20with%20lidar%20data%20to%20generate%0Athe%20ground%20truth%206DoF%20poses.%20360Loc%20is%20the%20first%20dataset%20and%20benchmark%20that%0Aexplores%20the%20challenge%20of%20cross-device%20visual%20positioning%2C%20involving%0A360%24%5E%5Ccirc%24%20reference%20frames%2C%20and%20query%20frames%20from%20pinhole%2C%20ultra-wide%20FoV%0Afisheye%2C%20and%20360%24%5E%5Ccirc%24%20cameras.%20We%20propose%20a%20virtual%20camera%20approach%20to%0Agenerate%20lower-FoV%20query%20frames%20from%20360%24%5E%5Ccirc%24%20images%2C%20which%20ensures%20a%20fair%0Acomparison%20of%20performance%20among%20different%20query%20types%20in%20visual%20localization%0Atasks.%20We%20also%20extend%20this%20virtual%20camera%20approach%20to%20feature%20matching-based%0Aand%20pose%20regression-based%20methods%20to%20alleviate%20the%20performance%20loss%20caused%20by%0Athe%20cross-device%20domain%20gap%2C%20and%20evaluate%20its%20effectiveness%20against%0Astate-of-the-art%20baselines.%20We%20demonstrate%20that%20omnidirectional%20visual%0Alocalization%20is%20more%20robust%20in%20challenging%20large-scale%20scenes%20with%20symmetries%0Aand%20repetitive%20structures.%20These%20results%20provide%20new%20insights%20into%20360-camera%0Amapping%20and%20omnidirectional%20visual%20localization%20with%20cross-device%20queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17389v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D360Loc%253A%2520A%2520Dataset%2520and%2520Benchmark%2520for%2520Omnidirectional%2520Visual%2520Localization%250A%2520%2520with%2520Cross-device%2520Queries%26entry.906535625%3DHuajian%2520Huang%2520and%2520Changkun%2520Liu%2520and%2520Yipeng%2520Zhu%2520and%2520Hui%2520Cheng%2520and%2520Tristan%2520Braud%2520and%2520Sai-Kit%2520Yeung%26entry.1292438233%3D%2520%2520Portable%2520360%2524%255E%255Ccirc%2524%2520cameras%2520are%2520becoming%2520a%2520cheap%2520and%2520efficient%2520tool%2520to%250Aestablish%2520large%2520visual%2520databases.%2520By%2520capturing%2520omnidirectional%2520views%2520of%2520a%250Ascene%252C%2520these%2520cameras%2520could%2520expedite%2520building%2520environment%2520models%2520that%2520are%250Aessential%2520for%2520visual%2520localization.%2520However%252C%2520such%2520an%2520advantage%2520is%2520often%250Aoverlooked%2520due%2520to%2520the%2520lack%2520of%2520valuable%2520datasets.%2520This%2520paper%2520introduces%2520a%2520new%250Abenchmark%2520dataset%252C%2520360Loc%252C%2520composed%2520of%2520360%2524%255E%255Ccirc%2524%2520images%2520with%2520ground%2520truth%250Aposes%2520for%2520visual%2520localization.%2520We%2520present%2520a%2520practical%2520implementation%2520of%250A360%2524%255E%255Ccirc%2524%2520mapping%2520combining%2520360%2524%255E%255Ccirc%2524%2520images%2520with%2520lidar%2520data%2520to%2520generate%250Athe%2520ground%2520truth%25206DoF%2520poses.%2520360Loc%2520is%2520the%2520first%2520dataset%2520and%2520benchmark%2520that%250Aexplores%2520the%2520challenge%2520of%2520cross-device%2520visual%2520positioning%252C%2520involving%250A360%2524%255E%255Ccirc%2524%2520reference%2520frames%252C%2520and%2520query%2520frames%2520from%2520pinhole%252C%2520ultra-wide%2520FoV%250Afisheye%252C%2520and%2520360%2524%255E%255Ccirc%2524%2520cameras.%2520We%2520propose%2520a%2520virtual%2520camera%2520approach%2520to%250Agenerate%2520lower-FoV%2520query%2520frames%2520from%2520360%2524%255E%255Ccirc%2524%2520images%252C%2520which%2520ensures%2520a%2520fair%250Acomparison%2520of%2520performance%2520among%2520different%2520query%2520types%2520in%2520visual%2520localization%250Atasks.%2520We%2520also%2520extend%2520this%2520virtual%2520camera%2520approach%2520to%2520feature%2520matching-based%250Aand%2520pose%2520regression-based%2520methods%2520to%2520alleviate%2520the%2520performance%2520loss%2520caused%2520by%250Athe%2520cross-device%2520domain%2520gap%252C%2520and%2520evaluate%2520its%2520effectiveness%2520against%250Astate-of-the-art%2520baselines.%2520We%2520demonstrate%2520that%2520omnidirectional%2520visual%250Alocalization%2520is%2520more%2520robust%2520in%2520challenging%2520large-scale%2520scenes%2520with%2520symmetries%250Aand%2520repetitive%2520structures.%2520These%2520results%2520provide%2520new%2520insights%2520into%2520360-camera%250Amapping%2520and%2520omnidirectional%2520visual%2520localization%2520with%2520cross-device%2520queries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17389v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=360Loc%3A%20A%20Dataset%20and%20Benchmark%20for%20Omnidirectional%20Visual%20Localization%0A%20%20with%20Cross-device%20Queries&entry.906535625=Huajian%20Huang%20and%20Changkun%20Liu%20and%20Yipeng%20Zhu%20and%20Hui%20Cheng%20and%20Tristan%20Braud%20and%20Sai-Kit%20Yeung&entry.1292438233=%20%20Portable%20360%24%5E%5Ccirc%24%20cameras%20are%20becoming%20a%20cheap%20and%20efficient%20tool%20to%0Aestablish%20large%20visual%20databases.%20By%20capturing%20omnidirectional%20views%20of%20a%0Ascene%2C%20these%20cameras%20could%20expedite%20building%20environment%20models%20that%20are%0Aessential%20for%20visual%20localization.%20However%2C%20such%20an%20advantage%20is%20often%0Aoverlooked%20due%20to%20the%20lack%20of%20valuable%20datasets.%20This%20paper%20introduces%20a%20new%0Abenchmark%20dataset%2C%20360Loc%2C%20composed%20of%20360%24%5E%5Ccirc%24%20images%20with%20ground%20truth%0Aposes%20for%20visual%20localization.%20We%20present%20a%20practical%20implementation%20of%0A360%24%5E%5Ccirc%24%20mapping%20combining%20360%24%5E%5Ccirc%24%20images%20with%20lidar%20data%20to%20generate%0Athe%20ground%20truth%206DoF%20poses.%20360Loc%20is%20the%20first%20dataset%20and%20benchmark%20that%0Aexplores%20the%20challenge%20of%20cross-device%20visual%20positioning%2C%20involving%0A360%24%5E%5Ccirc%24%20reference%20frames%2C%20and%20query%20frames%20from%20pinhole%2C%20ultra-wide%20FoV%0Afisheye%2C%20and%20360%24%5E%5Ccirc%24%20cameras.%20We%20propose%20a%20virtual%20camera%20approach%20to%0Agenerate%20lower-FoV%20query%20frames%20from%20360%24%5E%5Ccirc%24%20images%2C%20which%20ensures%20a%20fair%0Acomparison%20of%20performance%20among%20different%20query%20types%20in%20visual%20localization%0Atasks.%20We%20also%20extend%20this%20virtual%20camera%20approach%20to%20feature%20matching-based%0Aand%20pose%20regression-based%20methods%20to%20alleviate%20the%20performance%20loss%20caused%20by%0Athe%20cross-device%20domain%20gap%2C%20and%20evaluate%20its%20effectiveness%20against%0Astate-of-the-art%20baselines.%20We%20demonstrate%20that%20omnidirectional%20visual%0Alocalization%20is%20more%20robust%20in%20challenging%20large-scale%20scenes%20with%20symmetries%0Aand%20repetitive%20structures.%20These%20results%20provide%20new%20insights%20into%20360-camera%0Amapping%20and%20omnidirectional%20visual%20localization%20with%20cross-device%20queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17389v3&entry.124074799=Read"},
{"title": "GS-Phong: Meta-Learned 3D Gaussians for Relightable Novel View Synthesis", "author": "Yumeng He and Yunbo Wang and Xiaokang Yang", "abstract": "  Decoupling the illumination in 3D scenes is crucial for novel view synthesis\nand relighting. In this paper, we propose a novel method for representing a\nscene illuminated by a point light using a set of relightable 3D Gaussian\npoints. Inspired by the Blinn-Phong model, our approach decomposes the scene\ninto ambient, diffuse, and specular components, enabling the synthesis of\nrealistic lighting effects. To facilitate the decomposition of geometric\ninformation independent of lighting conditions, we introduce a novel bilevel\noptimization-based meta-learning framework. The fundamental idea is to view the\nrendering tasks under various lighting positions as a multi-task learning\nproblem, which our meta-learning approach effectively addresses by generalizing\nthe learned Gaussian geometries not only across different viewpoints but also\nacross diverse light positions. Experimental results demonstrate the\neffectiveness of our approach in terms of training efficiency and rendering\nquality compared to existing methods for free-viewpoint relighting.\n", "link": "http://arxiv.org/abs/2405.20791v1", "date": "2024-05-31", "relevancy": 2.9622, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6048}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5987}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-Phong%3A%20Meta-Learned%203D%20Gaussians%20for%20Relightable%20Novel%20View%20Synthesis&body=Title%3A%20GS-Phong%3A%20Meta-Learned%203D%20Gaussians%20for%20Relightable%20Novel%20View%20Synthesis%0AAuthor%3A%20Yumeng%20He%20and%20Yunbo%20Wang%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20Decoupling%20the%20illumination%20in%203D%20scenes%20is%20crucial%20for%20novel%20view%20synthesis%0Aand%20relighting.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20representing%20a%0Ascene%20illuminated%20by%20a%20point%20light%20using%20a%20set%20of%20relightable%203D%20Gaussian%0Apoints.%20Inspired%20by%20the%20Blinn-Phong%20model%2C%20our%20approach%20decomposes%20the%20scene%0Ainto%20ambient%2C%20diffuse%2C%20and%20specular%20components%2C%20enabling%20the%20synthesis%20of%0Arealistic%20lighting%20effects.%20To%20facilitate%20the%20decomposition%20of%20geometric%0Ainformation%20independent%20of%20lighting%20conditions%2C%20we%20introduce%20a%20novel%20bilevel%0Aoptimization-based%20meta-learning%20framework.%20The%20fundamental%20idea%20is%20to%20view%20the%0Arendering%20tasks%20under%20various%20lighting%20positions%20as%20a%20multi-task%20learning%0Aproblem%2C%20which%20our%20meta-learning%20approach%20effectively%20addresses%20by%20generalizing%0Athe%20learned%20Gaussian%20geometries%20not%20only%20across%20different%20viewpoints%20but%20also%0Aacross%20diverse%20light%20positions.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20terms%20of%20training%20efficiency%20and%20rendering%0Aquality%20compared%20to%20existing%20methods%20for%20free-viewpoint%20relighting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-Phong%253A%2520Meta-Learned%25203D%2520Gaussians%2520for%2520Relightable%2520Novel%2520View%2520Synthesis%26entry.906535625%3DYumeng%2520He%2520and%2520Yunbo%2520Wang%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520Decoupling%2520the%2520illumination%2520in%25203D%2520scenes%2520is%2520crucial%2520for%2520novel%2520view%2520synthesis%250Aand%2520relighting.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520representing%2520a%250Ascene%2520illuminated%2520by%2520a%2520point%2520light%2520using%2520a%2520set%2520of%2520relightable%25203D%2520Gaussian%250Apoints.%2520Inspired%2520by%2520the%2520Blinn-Phong%2520model%252C%2520our%2520approach%2520decomposes%2520the%2520scene%250Ainto%2520ambient%252C%2520diffuse%252C%2520and%2520specular%2520components%252C%2520enabling%2520the%2520synthesis%2520of%250Arealistic%2520lighting%2520effects.%2520To%2520facilitate%2520the%2520decomposition%2520of%2520geometric%250Ainformation%2520independent%2520of%2520lighting%2520conditions%252C%2520we%2520introduce%2520a%2520novel%2520bilevel%250Aoptimization-based%2520meta-learning%2520framework.%2520The%2520fundamental%2520idea%2520is%2520to%2520view%2520the%250Arendering%2520tasks%2520under%2520various%2520lighting%2520positions%2520as%2520a%2520multi-task%2520learning%250Aproblem%252C%2520which%2520our%2520meta-learning%2520approach%2520effectively%2520addresses%2520by%2520generalizing%250Athe%2520learned%2520Gaussian%2520geometries%2520not%2520only%2520across%2520different%2520viewpoints%2520but%2520also%250Aacross%2520diverse%2520light%2520positions.%2520Experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520terms%2520of%2520training%2520efficiency%2520and%2520rendering%250Aquality%2520compared%2520to%2520existing%2520methods%2520for%2520free-viewpoint%2520relighting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-Phong%3A%20Meta-Learned%203D%20Gaussians%20for%20Relightable%20Novel%20View%20Synthesis&entry.906535625=Yumeng%20He%20and%20Yunbo%20Wang%20and%20Xiaokang%20Yang&entry.1292438233=%20%20Decoupling%20the%20illumination%20in%203D%20scenes%20is%20crucial%20for%20novel%20view%20synthesis%0Aand%20relighting.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%20for%20representing%20a%0Ascene%20illuminated%20by%20a%20point%20light%20using%20a%20set%20of%20relightable%203D%20Gaussian%0Apoints.%20Inspired%20by%20the%20Blinn-Phong%20model%2C%20our%20approach%20decomposes%20the%20scene%0Ainto%20ambient%2C%20diffuse%2C%20and%20specular%20components%2C%20enabling%20the%20synthesis%20of%0Arealistic%20lighting%20effects.%20To%20facilitate%20the%20decomposition%20of%20geometric%0Ainformation%20independent%20of%20lighting%20conditions%2C%20we%20introduce%20a%20novel%20bilevel%0Aoptimization-based%20meta-learning%20framework.%20The%20fundamental%20idea%20is%20to%20view%20the%0Arendering%20tasks%20under%20various%20lighting%20positions%20as%20a%20multi-task%20learning%0Aproblem%2C%20which%20our%20meta-learning%20approach%20effectively%20addresses%20by%20generalizing%0Athe%20learned%20Gaussian%20geometries%20not%20only%20across%20different%20viewpoints%20but%20also%0Aacross%20diverse%20light%20positions.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20terms%20of%20training%20efficiency%20and%20rendering%0Aquality%20compared%20to%20existing%20methods%20for%20free-viewpoint%20relighting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20791v1&entry.124074799=Read"},
{"title": "MeshXL: Neural Coordinate Field for Generative 3D Foundation Models", "author": "Sijin Chen and Xin Chen and Anqi Pang and Xianfang Zeng and Wei Cheng and Yijun Fu and Fukun Yin and Yanru Wang and Zhibin Wang and Chi Zhang and Jingyi Yu and Gang Yu and Bin Fu and Tao Chen", "abstract": "  The polygon mesh representation of 3D data exhibits great flexibility, fast\nrendering speed, and storage efficiency, which is widely preferred in various\napplications. However, given its unstructured graph representation, the direct\ngeneration of high-fidelity 3D meshes is challenging. Fortunately, with a\npre-defined ordering strategy, 3D meshes can be represented as sequences, and\nthe generation process can be seamlessly treated as an auto-regressive problem.\nIn this paper, we validate the Neural Coordinate Field (NeurCF), an explicit\ncoordinate representation with implicit neural embeddings, is a\nsimple-yet-effective representation for large-scale sequential mesh modeling.\nAfter that, we present MeshXL, a family of generative pre-trained\nauto-regressive models, which addresses the process of 3D mesh generation with\nmodern large language model approaches. Extensive experiments show that MeshXL\nis able to generate high-quality 3D meshes, and can also serve as foundation\nmodels for various down-stream applications.\n", "link": "http://arxiv.org/abs/2405.20853v1", "date": "2024-05-31", "relevancy": 2.8412, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6519}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5312}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshXL%3A%20Neural%20Coordinate%20Field%20for%20Generative%203D%20Foundation%20Models&body=Title%3A%20MeshXL%3A%20Neural%20Coordinate%20Field%20for%20Generative%203D%20Foundation%20Models%0AAuthor%3A%20Sijin%20Chen%20and%20Xin%20Chen%20and%20Anqi%20Pang%20and%20Xianfang%20Zeng%20and%20Wei%20Cheng%20and%20Yijun%20Fu%20and%20Fukun%20Yin%20and%20Yanru%20Wang%20and%20Zhibin%20Wang%20and%20Chi%20Zhang%20and%20Jingyi%20Yu%20and%20Gang%20Yu%20and%20Bin%20Fu%20and%20Tao%20Chen%0AAbstract%3A%20%20%20The%20polygon%20mesh%20representation%20of%203D%20data%20exhibits%20great%20flexibility%2C%20fast%0Arendering%20speed%2C%20and%20storage%20efficiency%2C%20which%20is%20widely%20preferred%20in%20various%0Aapplications.%20However%2C%20given%20its%20unstructured%20graph%20representation%2C%20the%20direct%0Ageneration%20of%20high-fidelity%203D%20meshes%20is%20challenging.%20Fortunately%2C%20with%20a%0Apre-defined%20ordering%20strategy%2C%203D%20meshes%20can%20be%20represented%20as%20sequences%2C%20and%0Athe%20generation%20process%20can%20be%20seamlessly%20treated%20as%20an%20auto-regressive%20problem.%0AIn%20this%20paper%2C%20we%20validate%20the%20Neural%20Coordinate%20Field%20%28NeurCF%29%2C%20an%20explicit%0Acoordinate%20representation%20with%20implicit%20neural%20embeddings%2C%20is%20a%0Asimple-yet-effective%20representation%20for%20large-scale%20sequential%20mesh%20modeling.%0AAfter%20that%2C%20we%20present%20MeshXL%2C%20a%20family%20of%20generative%20pre-trained%0Aauto-regressive%20models%2C%20which%20addresses%20the%20process%20of%203D%20mesh%20generation%20with%0Amodern%20large%20language%20model%20approaches.%20Extensive%20experiments%20show%20that%20MeshXL%0Ais%20able%20to%20generate%20high-quality%203D%20meshes%2C%20and%20can%20also%20serve%20as%20foundation%0Amodels%20for%20various%20down-stream%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshXL%253A%2520Neural%2520Coordinate%2520Field%2520for%2520Generative%25203D%2520Foundation%2520Models%26entry.906535625%3DSijin%2520Chen%2520and%2520Xin%2520Chen%2520and%2520Anqi%2520Pang%2520and%2520Xianfang%2520Zeng%2520and%2520Wei%2520Cheng%2520and%2520Yijun%2520Fu%2520and%2520Fukun%2520Yin%2520and%2520Yanru%2520Wang%2520and%2520Zhibin%2520Wang%2520and%2520Chi%2520Zhang%2520and%2520Jingyi%2520Yu%2520and%2520Gang%2520Yu%2520and%2520Bin%2520Fu%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520The%2520polygon%2520mesh%2520representation%2520of%25203D%2520data%2520exhibits%2520great%2520flexibility%252C%2520fast%250Arendering%2520speed%252C%2520and%2520storage%2520efficiency%252C%2520which%2520is%2520widely%2520preferred%2520in%2520various%250Aapplications.%2520However%252C%2520given%2520its%2520unstructured%2520graph%2520representation%252C%2520the%2520direct%250Ageneration%2520of%2520high-fidelity%25203D%2520meshes%2520is%2520challenging.%2520Fortunately%252C%2520with%2520a%250Apre-defined%2520ordering%2520strategy%252C%25203D%2520meshes%2520can%2520be%2520represented%2520as%2520sequences%252C%2520and%250Athe%2520generation%2520process%2520can%2520be%2520seamlessly%2520treated%2520as%2520an%2520auto-regressive%2520problem.%250AIn%2520this%2520paper%252C%2520we%2520validate%2520the%2520Neural%2520Coordinate%2520Field%2520%2528NeurCF%2529%252C%2520an%2520explicit%250Acoordinate%2520representation%2520with%2520implicit%2520neural%2520embeddings%252C%2520is%2520a%250Asimple-yet-effective%2520representation%2520for%2520large-scale%2520sequential%2520mesh%2520modeling.%250AAfter%2520that%252C%2520we%2520present%2520MeshXL%252C%2520a%2520family%2520of%2520generative%2520pre-trained%250Aauto-regressive%2520models%252C%2520which%2520addresses%2520the%2520process%2520of%25203D%2520mesh%2520generation%2520with%250Amodern%2520large%2520language%2520model%2520approaches.%2520Extensive%2520experiments%2520show%2520that%2520MeshXL%250Ais%2520able%2520to%2520generate%2520high-quality%25203D%2520meshes%252C%2520and%2520can%2520also%2520serve%2520as%2520foundation%250Amodels%2520for%2520various%2520down-stream%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshXL%3A%20Neural%20Coordinate%20Field%20for%20Generative%203D%20Foundation%20Models&entry.906535625=Sijin%20Chen%20and%20Xin%20Chen%20and%20Anqi%20Pang%20and%20Xianfang%20Zeng%20and%20Wei%20Cheng%20and%20Yijun%20Fu%20and%20Fukun%20Yin%20and%20Yanru%20Wang%20and%20Zhibin%20Wang%20and%20Chi%20Zhang%20and%20Jingyi%20Yu%20and%20Gang%20Yu%20and%20Bin%20Fu%20and%20Tao%20Chen&entry.1292438233=%20%20The%20polygon%20mesh%20representation%20of%203D%20data%20exhibits%20great%20flexibility%2C%20fast%0Arendering%20speed%2C%20and%20storage%20efficiency%2C%20which%20is%20widely%20preferred%20in%20various%0Aapplications.%20However%2C%20given%20its%20unstructured%20graph%20representation%2C%20the%20direct%0Ageneration%20of%20high-fidelity%203D%20meshes%20is%20challenging.%20Fortunately%2C%20with%20a%0Apre-defined%20ordering%20strategy%2C%203D%20meshes%20can%20be%20represented%20as%20sequences%2C%20and%0Athe%20generation%20process%20can%20be%20seamlessly%20treated%20as%20an%20auto-regressive%20problem.%0AIn%20this%20paper%2C%20we%20validate%20the%20Neural%20Coordinate%20Field%20%28NeurCF%29%2C%20an%20explicit%0Acoordinate%20representation%20with%20implicit%20neural%20embeddings%2C%20is%20a%0Asimple-yet-effective%20representation%20for%20large-scale%20sequential%20mesh%20modeling.%0AAfter%20that%2C%20we%20present%20MeshXL%2C%20a%20family%20of%20generative%20pre-trained%0Aauto-regressive%20models%2C%20which%20addresses%20the%20process%20of%203D%20mesh%20generation%20with%0Amodern%20large%20language%20model%20approaches.%20Extensive%20experiments%20show%20that%20MeshXL%0Ais%20able%20to%20generate%20high-quality%203D%20meshes%2C%20and%20can%20also%20serve%20as%20foundation%0Amodels%20for%20various%20down-stream%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20853v1&entry.124074799=Read"},
{"title": "4DHands: Reconstructing Interactive Hands in 4D with Transformers", "author": "Dixuan Lin and Yuxiang Zhang and Mengcheng Li and Yebin Liu and Wei Jing and Qi Yan and Qianying Wang and Hongwen Zhang", "abstract": "  In this paper, we introduce 4DHands, a robust approach to recovering\ninteractive hand meshes and their relative movement from monocular inputs. Our\napproach addresses two major limitations of previous methods: lacking a unified\nsolution for handling various hand image inputs and neglecting the positional\nrelationship of two hands within images. To overcome these challenges, we\ndevelop a transformer-based architecture with novel tokenization and feature\nfusion strategies. Specifically, we propose a Relation-aware Two-Hand\nTokenization (RAT) method to embed positional relation information into the\nhand tokens. In this way, our network can handle both single-hand and two-hand\ninputs and explicitly leverage relative hand positions, facilitating the\nreconstruction of intricate hand interactions in real-world scenarios. As such\ntokenization indicates the relative relationship of two hands, it also supports\nmore effective feature fusion. To this end, we further develop a\nSpatio-temporal Interaction Reasoning (SIR) module to fuse hand tokens in 4D\nwith attention and decode them into 3D hand meshes and relative temporal\nmovements. The efficacy of our approach is validated on several benchmark\ndatasets. The results on in-the-wild videos and real-world scenarios\ndemonstrate the superior performances of our approach for interactive hand\nreconstruction. More video results can be found on the project page:\nhttps://4dhands.github.io.\n", "link": "http://arxiv.org/abs/2405.20330v2", "date": "2024-05-31", "relevancy": 2.7993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5667}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5588}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DHands%3A%20Reconstructing%20Interactive%20Hands%20in%204D%20with%20Transformers&body=Title%3A%204DHands%3A%20Reconstructing%20Interactive%20Hands%20in%204D%20with%20Transformers%0AAuthor%3A%20Dixuan%20Lin%20and%20Yuxiang%20Zhang%20and%20Mengcheng%20Li%20and%20Yebin%20Liu%20and%20Wei%20Jing%20and%20Qi%20Yan%20and%20Qianying%20Wang%20and%20Hongwen%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%204DHands%2C%20a%20robust%20approach%20to%20recovering%0Ainteractive%20hand%20meshes%20and%20their%20relative%20movement%20from%20monocular%20inputs.%20Our%0Aapproach%20addresses%20two%20major%20limitations%20of%20previous%20methods%3A%20lacking%20a%20unified%0Asolution%20for%20handling%20various%20hand%20image%20inputs%20and%20neglecting%20the%20positional%0Arelationship%20of%20two%20hands%20within%20images.%20To%20overcome%20these%20challenges%2C%20we%0Adevelop%20a%20transformer-based%20architecture%20with%20novel%20tokenization%20and%20feature%0Afusion%20strategies.%20Specifically%2C%20we%20propose%20a%20Relation-aware%20Two-Hand%0ATokenization%20%28RAT%29%20method%20to%20embed%20positional%20relation%20information%20into%20the%0Ahand%20tokens.%20In%20this%20way%2C%20our%20network%20can%20handle%20both%20single-hand%20and%20two-hand%0Ainputs%20and%20explicitly%20leverage%20relative%20hand%20positions%2C%20facilitating%20the%0Areconstruction%20of%20intricate%20hand%20interactions%20in%20real-world%20scenarios.%20As%20such%0Atokenization%20indicates%20the%20relative%20relationship%20of%20two%20hands%2C%20it%20also%20supports%0Amore%20effective%20feature%20fusion.%20To%20this%20end%2C%20we%20further%20develop%20a%0ASpatio-temporal%20Interaction%20Reasoning%20%28SIR%29%20module%20to%20fuse%20hand%20tokens%20in%204D%0Awith%20attention%20and%20decode%20them%20into%203D%20hand%20meshes%20and%20relative%20temporal%0Amovements.%20The%20efficacy%20of%20our%20approach%20is%20validated%20on%20several%20benchmark%0Adatasets.%20The%20results%20on%20in-the-wild%20videos%20and%20real-world%20scenarios%0Ademonstrate%20the%20superior%20performances%20of%20our%20approach%20for%20interactive%20hand%0Areconstruction.%20More%20video%20results%20can%20be%20found%20on%20the%20project%20page%3A%0Ahttps%3A//4dhands.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20330v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DHands%253A%2520Reconstructing%2520Interactive%2520Hands%2520in%25204D%2520with%2520Transformers%26entry.906535625%3DDixuan%2520Lin%2520and%2520Yuxiang%2520Zhang%2520and%2520Mengcheng%2520Li%2520and%2520Yebin%2520Liu%2520and%2520Wei%2520Jing%2520and%2520Qi%2520Yan%2520and%2520Qianying%2520Wang%2520and%2520Hongwen%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%25204DHands%252C%2520a%2520robust%2520approach%2520to%2520recovering%250Ainteractive%2520hand%2520meshes%2520and%2520their%2520relative%2520movement%2520from%2520monocular%2520inputs.%2520Our%250Aapproach%2520addresses%2520two%2520major%2520limitations%2520of%2520previous%2520methods%253A%2520lacking%2520a%2520unified%250Asolution%2520for%2520handling%2520various%2520hand%2520image%2520inputs%2520and%2520neglecting%2520the%2520positional%250Arelationship%2520of%2520two%2520hands%2520within%2520images.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Adevelop%2520a%2520transformer-based%2520architecture%2520with%2520novel%2520tokenization%2520and%2520feature%250Afusion%2520strategies.%2520Specifically%252C%2520we%2520propose%2520a%2520Relation-aware%2520Two-Hand%250ATokenization%2520%2528RAT%2529%2520method%2520to%2520embed%2520positional%2520relation%2520information%2520into%2520the%250Ahand%2520tokens.%2520In%2520this%2520way%252C%2520our%2520network%2520can%2520handle%2520both%2520single-hand%2520and%2520two-hand%250Ainputs%2520and%2520explicitly%2520leverage%2520relative%2520hand%2520positions%252C%2520facilitating%2520the%250Areconstruction%2520of%2520intricate%2520hand%2520interactions%2520in%2520real-world%2520scenarios.%2520As%2520such%250Atokenization%2520indicates%2520the%2520relative%2520relationship%2520of%2520two%2520hands%252C%2520it%2520also%2520supports%250Amore%2520effective%2520feature%2520fusion.%2520To%2520this%2520end%252C%2520we%2520further%2520develop%2520a%250ASpatio-temporal%2520Interaction%2520Reasoning%2520%2528SIR%2529%2520module%2520to%2520fuse%2520hand%2520tokens%2520in%25204D%250Awith%2520attention%2520and%2520decode%2520them%2520into%25203D%2520hand%2520meshes%2520and%2520relative%2520temporal%250Amovements.%2520The%2520efficacy%2520of%2520our%2520approach%2520is%2520validated%2520on%2520several%2520benchmark%250Adatasets.%2520The%2520results%2520on%2520in-the-wild%2520videos%2520and%2520real-world%2520scenarios%250Ademonstrate%2520the%2520superior%2520performances%2520of%2520our%2520approach%2520for%2520interactive%2520hand%250Areconstruction.%2520More%2520video%2520results%2520can%2520be%2520found%2520on%2520the%2520project%2520page%253A%250Ahttps%253A//4dhands.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20330v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DHands%3A%20Reconstructing%20Interactive%20Hands%20in%204D%20with%20Transformers&entry.906535625=Dixuan%20Lin%20and%20Yuxiang%20Zhang%20and%20Mengcheng%20Li%20and%20Yebin%20Liu%20and%20Wei%20Jing%20and%20Qi%20Yan%20and%20Qianying%20Wang%20and%20Hongwen%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%204DHands%2C%20a%20robust%20approach%20to%20recovering%0Ainteractive%20hand%20meshes%20and%20their%20relative%20movement%20from%20monocular%20inputs.%20Our%0Aapproach%20addresses%20two%20major%20limitations%20of%20previous%20methods%3A%20lacking%20a%20unified%0Asolution%20for%20handling%20various%20hand%20image%20inputs%20and%20neglecting%20the%20positional%0Arelationship%20of%20two%20hands%20within%20images.%20To%20overcome%20these%20challenges%2C%20we%0Adevelop%20a%20transformer-based%20architecture%20with%20novel%20tokenization%20and%20feature%0Afusion%20strategies.%20Specifically%2C%20we%20propose%20a%20Relation-aware%20Two-Hand%0ATokenization%20%28RAT%29%20method%20to%20embed%20positional%20relation%20information%20into%20the%0Ahand%20tokens.%20In%20this%20way%2C%20our%20network%20can%20handle%20both%20single-hand%20and%20two-hand%0Ainputs%20and%20explicitly%20leverage%20relative%20hand%20positions%2C%20facilitating%20the%0Areconstruction%20of%20intricate%20hand%20interactions%20in%20real-world%20scenarios.%20As%20such%0Atokenization%20indicates%20the%20relative%20relationship%20of%20two%20hands%2C%20it%20also%20supports%0Amore%20effective%20feature%20fusion.%20To%20this%20end%2C%20we%20further%20develop%20a%0ASpatio-temporal%20Interaction%20Reasoning%20%28SIR%29%20module%20to%20fuse%20hand%20tokens%20in%204D%0Awith%20attention%20and%20decode%20them%20into%203D%20hand%20meshes%20and%20relative%20temporal%0Amovements.%20The%20efficacy%20of%20our%20approach%20is%20validated%20on%20several%20benchmark%0Adatasets.%20The%20results%20on%20in-the-wild%20videos%20and%20real-world%20scenarios%0Ademonstrate%20the%20superior%20performances%20of%20our%20approach%20for%20interactive%20hand%0Areconstruction.%20More%20video%20results%20can%20be%20found%20on%20the%20project%20page%3A%0Ahttps%3A//4dhands.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20330v2&entry.124074799=Read"},
{"title": "Generalization Beyond Data Imbalance: A Controlled Study on CLIP for\n  Transferable Insights", "author": "Xin Wen and Bingchen Zhao and Yilun Chen and Jiangmiao Pang and Xiaojuan Qi", "abstract": "  Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode will be available at: https://github.com/CVMI-Lab/clip-beyond-tail.\n", "link": "http://arxiv.org/abs/2405.21070v1", "date": "2024-05-31", "relevancy": 2.7785, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6064}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5322}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalization%20Beyond%20Data%20Imbalance%3A%20A%20Controlled%20Study%20on%20CLIP%20for%0A%20%20Transferable%20Insights&body=Title%3A%20Generalization%20Beyond%20Data%20Imbalance%3A%20A%20Controlled%20Study%20on%20CLIP%20for%0A%20%20Transferable%20Insights%0AAuthor%3A%20Xin%20Wen%20and%20Bingchen%20Zhao%20and%20Yilun%20Chen%20and%20Jiangmiao%20Pang%20and%20Xiaojuan%20Qi%0AAbstract%3A%20%20%20Severe%20data%20imbalance%20naturally%20exists%20among%20web-scale%20vision-language%0Adatasets.%20Despite%20this%2C%20we%20find%20CLIP%20pre-trained%20thereupon%20exhibits%20notable%0Arobustness%20to%20the%20data%20imbalance%20compared%20to%20supervised%20learning%2C%20and%0Ademonstrates%20significant%20effectiveness%20in%20learning%20generalizable%0Arepresentations.%20With%20an%20aim%20to%20investigate%20the%20reasons%20behind%20this%20finding%2C%20we%0Aconduct%20controlled%20experiments%20to%20study%20various%20underlying%20factors%2C%20and%20reveal%0Athat%20CLIP%27s%20pretext%20task%20forms%20a%20dynamic%20classification%20problem%20wherein%20only%20a%0Asubset%20of%20classes%20is%20present%20in%20training.%20This%20isolates%20the%20bias%20from%20dominant%0Aclasses%20and%20implicitly%20balances%20the%20learning%20signal.%20Furthermore%2C%20the%0Arobustness%20and%20discriminability%20of%20CLIP%20improve%20with%20more%20descriptive%20language%0Asupervision%2C%20larger%20data%20scale%2C%20and%20broader%20open-world%20concepts%2C%20which%20are%0Ainaccessible%20to%20supervised%20learning.%20Our%20study%20not%20only%20uncovers%20the%20mechanisms%0Abehind%20CLIP%27s%20generalizability%20beyond%20data%20imbalance%20but%20also%20provides%0Atransferable%20insights%20for%20the%20research%20community.%20The%20findings%20are%20validated%20in%0Aboth%20supervised%20and%20self-supervised%20learning%2C%20enabling%20models%20trained%20on%0Aimbalanced%20data%20to%20achieve%20CLIP-level%20performance%20on%20diverse%20recognition%20tasks.%0ACode%20will%20be%20available%20at%3A%20https%3A//github.com/CVMI-Lab/clip-beyond-tail.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralization%2520Beyond%2520Data%2520Imbalance%253A%2520A%2520Controlled%2520Study%2520on%2520CLIP%2520for%250A%2520%2520Transferable%2520Insights%26entry.906535625%3DXin%2520Wen%2520and%2520Bingchen%2520Zhao%2520and%2520Yilun%2520Chen%2520and%2520Jiangmiao%2520Pang%2520and%2520Xiaojuan%2520Qi%26entry.1292438233%3D%2520%2520Severe%2520data%2520imbalance%2520naturally%2520exists%2520among%2520web-scale%2520vision-language%250Adatasets.%2520Despite%2520this%252C%2520we%2520find%2520CLIP%2520pre-trained%2520thereupon%2520exhibits%2520notable%250Arobustness%2520to%2520the%2520data%2520imbalance%2520compared%2520to%2520supervised%2520learning%252C%2520and%250Ademonstrates%2520significant%2520effectiveness%2520in%2520learning%2520generalizable%250Arepresentations.%2520With%2520an%2520aim%2520to%2520investigate%2520the%2520reasons%2520behind%2520this%2520finding%252C%2520we%250Aconduct%2520controlled%2520experiments%2520to%2520study%2520various%2520underlying%2520factors%252C%2520and%2520reveal%250Athat%2520CLIP%2527s%2520pretext%2520task%2520forms%2520a%2520dynamic%2520classification%2520problem%2520wherein%2520only%2520a%250Asubset%2520of%2520classes%2520is%2520present%2520in%2520training.%2520This%2520isolates%2520the%2520bias%2520from%2520dominant%250Aclasses%2520and%2520implicitly%2520balances%2520the%2520learning%2520signal.%2520Furthermore%252C%2520the%250Arobustness%2520and%2520discriminability%2520of%2520CLIP%2520improve%2520with%2520more%2520descriptive%2520language%250Asupervision%252C%2520larger%2520data%2520scale%252C%2520and%2520broader%2520open-world%2520concepts%252C%2520which%2520are%250Ainaccessible%2520to%2520supervised%2520learning.%2520Our%2520study%2520not%2520only%2520uncovers%2520the%2520mechanisms%250Abehind%2520CLIP%2527s%2520generalizability%2520beyond%2520data%2520imbalance%2520but%2520also%2520provides%250Atransferable%2520insights%2520for%2520the%2520research%2520community.%2520The%2520findings%2520are%2520validated%2520in%250Aboth%2520supervised%2520and%2520self-supervised%2520learning%252C%2520enabling%2520models%2520trained%2520on%250Aimbalanced%2520data%2520to%2520achieve%2520CLIP-level%2520performance%2520on%2520diverse%2520recognition%2520tasks.%250ACode%2520will%2520be%2520available%2520at%253A%2520https%253A//github.com/CVMI-Lab/clip-beyond-tail.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20Beyond%20Data%20Imbalance%3A%20A%20Controlled%20Study%20on%20CLIP%20for%0A%20%20Transferable%20Insights&entry.906535625=Xin%20Wen%20and%20Bingchen%20Zhao%20and%20Yilun%20Chen%20and%20Jiangmiao%20Pang%20and%20Xiaojuan%20Qi&entry.1292438233=%20%20Severe%20data%20imbalance%20naturally%20exists%20among%20web-scale%20vision-language%0Adatasets.%20Despite%20this%2C%20we%20find%20CLIP%20pre-trained%20thereupon%20exhibits%20notable%0Arobustness%20to%20the%20data%20imbalance%20compared%20to%20supervised%20learning%2C%20and%0Ademonstrates%20significant%20effectiveness%20in%20learning%20generalizable%0Arepresentations.%20With%20an%20aim%20to%20investigate%20the%20reasons%20behind%20this%20finding%2C%20we%0Aconduct%20controlled%20experiments%20to%20study%20various%20underlying%20factors%2C%20and%20reveal%0Athat%20CLIP%27s%20pretext%20task%20forms%20a%20dynamic%20classification%20problem%20wherein%20only%20a%0Asubset%20of%20classes%20is%20present%20in%20training.%20This%20isolates%20the%20bias%20from%20dominant%0Aclasses%20and%20implicitly%20balances%20the%20learning%20signal.%20Furthermore%2C%20the%0Arobustness%20and%20discriminability%20of%20CLIP%20improve%20with%20more%20descriptive%20language%0Asupervision%2C%20larger%20data%20scale%2C%20and%20broader%20open-world%20concepts%2C%20which%20are%0Ainaccessible%20to%20supervised%20learning.%20Our%20study%20not%20only%20uncovers%20the%20mechanisms%0Abehind%20CLIP%27s%20generalizability%20beyond%20data%20imbalance%20but%20also%20provides%0Atransferable%20insights%20for%20the%20research%20community.%20The%20findings%20are%20validated%20in%0Aboth%20supervised%20and%20self-supervised%20learning%2C%20enabling%20models%20trained%20on%0Aimbalanced%20data%20to%20achieve%20CLIP-level%20performance%20on%20diverse%20recognition%20tasks.%0ACode%20will%20be%20available%20at%3A%20https%3A//github.com/CVMI-Lab/clip-beyond-tail.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21070v1&entry.124074799=Read"},
{"title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey", "author": "Penghao Zhao and Hailin Zhang and Qinhan Yu and Zhengren Wang and Yunteng Geng and Fangcheng Fu and Ling Yang and Wentao Zhang and Jie Jiang and Bin Cui", "abstract": "  Advancements in model algorithms, the growth of foundational models, and\naccess to high-quality datasets have propelled the evolution of Artificial\nIntelligence Generated Content (AIGC). Despite its notable successes, AIGC\nstill faces hurdles such as updating knowledge, handling long-tail data,\nmitigating data leakage, and managing high training and inference costs.\nRetrieval-Augmented Generation (RAG) has recently emerged as a paradigm to\naddress such challenges. In particular, RAG introduces the information\nretrieval process, which enhances the generation process by retrieving relevant\nobjects from available data stores, leading to higher accuracy and better\nrobustness. In this paper, we comprehensively review existing efforts that\nintegrate RAG technique into AIGC scenarios. We first classify RAG foundations\naccording to how the retriever augments the generator, distilling the\nfundamental abstractions of the augmentation methodologies for various\nretrievers and generators. This unified perspective encompasses all RAG\nscenarios, illuminating advancements and pivotal technologies that help with\npotential future progress. We also summarize additional enhancements methods\nfor RAG, facilitating effective engineering and implementation of RAG systems.\nThen from another view, we survey on practical applications of RAG across\ndifferent modalities and tasks, offering valuable references for researchers\nand practitioners. Furthermore, we introduce the benchmarks for RAG, discuss\nthe limitations of current RAG systems, and suggest potential directions for\nfuture research. Github: https://github.com/PKU-DAIR/RAG-Survey.\n", "link": "http://arxiv.org/abs/2402.19473v5", "date": "2024-05-31", "relevancy": 2.7395, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.573}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5517}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval-Augmented%20Generation%20for%20AI-Generated%20Content%3A%20A%20Survey&body=Title%3A%20Retrieval-Augmented%20Generation%20for%20AI-Generated%20Content%3A%20A%20Survey%0AAuthor%3A%20Penghao%20Zhao%20and%20Hailin%20Zhang%20and%20Qinhan%20Yu%20and%20Zhengren%20Wang%20and%20Yunteng%20Geng%20and%20Fangcheng%20Fu%20and%20Ling%20Yang%20and%20Wentao%20Zhang%20and%20Jie%20Jiang%20and%20Bin%20Cui%0AAbstract%3A%20%20%20Advancements%20in%20model%20algorithms%2C%20the%20growth%20of%20foundational%20models%2C%20and%0Aaccess%20to%20high-quality%20datasets%20have%20propelled%20the%20evolution%20of%20Artificial%0AIntelligence%20Generated%20Content%20%28AIGC%29.%20Despite%20its%20notable%20successes%2C%20AIGC%0Astill%20faces%20hurdles%20such%20as%20updating%20knowledge%2C%20handling%20long-tail%20data%2C%0Amitigating%20data%20leakage%2C%20and%20managing%20high%20training%20and%20inference%20costs.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20has%20recently%20emerged%20as%20a%20paradigm%20to%0Aaddress%20such%20challenges.%20In%20particular%2C%20RAG%20introduces%20the%20information%0Aretrieval%20process%2C%20which%20enhances%20the%20generation%20process%20by%20retrieving%20relevant%0Aobjects%20from%20available%20data%20stores%2C%20leading%20to%20higher%20accuracy%20and%20better%0Arobustness.%20In%20this%20paper%2C%20we%20comprehensively%20review%20existing%20efforts%20that%0Aintegrate%20RAG%20technique%20into%20AIGC%20scenarios.%20We%20first%20classify%20RAG%20foundations%0Aaccording%20to%20how%20the%20retriever%20augments%20the%20generator%2C%20distilling%20the%0Afundamental%20abstractions%20of%20the%20augmentation%20methodologies%20for%20various%0Aretrievers%20and%20generators.%20This%20unified%20perspective%20encompasses%20all%20RAG%0Ascenarios%2C%20illuminating%20advancements%20and%20pivotal%20technologies%20that%20help%20with%0Apotential%20future%20progress.%20We%20also%20summarize%20additional%20enhancements%20methods%0Afor%20RAG%2C%20facilitating%20effective%20engineering%20and%20implementation%20of%20RAG%20systems.%0AThen%20from%20another%20view%2C%20we%20survey%20on%20practical%20applications%20of%20RAG%20across%0Adifferent%20modalities%20and%20tasks%2C%20offering%20valuable%20references%20for%20researchers%0Aand%20practitioners.%20Furthermore%2C%20we%20introduce%20the%20benchmarks%20for%20RAG%2C%20discuss%0Athe%20limitations%20of%20current%20RAG%20systems%2C%20and%20suggest%20potential%20directions%20for%0Afuture%20research.%20Github%3A%20https%3A//github.com/PKU-DAIR/RAG-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19473v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval-Augmented%2520Generation%2520for%2520AI-Generated%2520Content%253A%2520A%2520Survey%26entry.906535625%3DPenghao%2520Zhao%2520and%2520Hailin%2520Zhang%2520and%2520Qinhan%2520Yu%2520and%2520Zhengren%2520Wang%2520and%2520Yunteng%2520Geng%2520and%2520Fangcheng%2520Fu%2520and%2520Ling%2520Yang%2520and%2520Wentao%2520Zhang%2520and%2520Jie%2520Jiang%2520and%2520Bin%2520Cui%26entry.1292438233%3D%2520%2520Advancements%2520in%2520model%2520algorithms%252C%2520the%2520growth%2520of%2520foundational%2520models%252C%2520and%250Aaccess%2520to%2520high-quality%2520datasets%2520have%2520propelled%2520the%2520evolution%2520of%2520Artificial%250AIntelligence%2520Generated%2520Content%2520%2528AIGC%2529.%2520Despite%2520its%2520notable%2520successes%252C%2520AIGC%250Astill%2520faces%2520hurdles%2520such%2520as%2520updating%2520knowledge%252C%2520handling%2520long-tail%2520data%252C%250Amitigating%2520data%2520leakage%252C%2520and%2520managing%2520high%2520training%2520and%2520inference%2520costs.%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520has%2520recently%2520emerged%2520as%2520a%2520paradigm%2520to%250Aaddress%2520such%2520challenges.%2520In%2520particular%252C%2520RAG%2520introduces%2520the%2520information%250Aretrieval%2520process%252C%2520which%2520enhances%2520the%2520generation%2520process%2520by%2520retrieving%2520relevant%250Aobjects%2520from%2520available%2520data%2520stores%252C%2520leading%2520to%2520higher%2520accuracy%2520and%2520better%250Arobustness.%2520In%2520this%2520paper%252C%2520we%2520comprehensively%2520review%2520existing%2520efforts%2520that%250Aintegrate%2520RAG%2520technique%2520into%2520AIGC%2520scenarios.%2520We%2520first%2520classify%2520RAG%2520foundations%250Aaccording%2520to%2520how%2520the%2520retriever%2520augments%2520the%2520generator%252C%2520distilling%2520the%250Afundamental%2520abstractions%2520of%2520the%2520augmentation%2520methodologies%2520for%2520various%250Aretrievers%2520and%2520generators.%2520This%2520unified%2520perspective%2520encompasses%2520all%2520RAG%250Ascenarios%252C%2520illuminating%2520advancements%2520and%2520pivotal%2520technologies%2520that%2520help%2520with%250Apotential%2520future%2520progress.%2520We%2520also%2520summarize%2520additional%2520enhancements%2520methods%250Afor%2520RAG%252C%2520facilitating%2520effective%2520engineering%2520and%2520implementation%2520of%2520RAG%2520systems.%250AThen%2520from%2520another%2520view%252C%2520we%2520survey%2520on%2520practical%2520applications%2520of%2520RAG%2520across%250Adifferent%2520modalities%2520and%2520tasks%252C%2520offering%2520valuable%2520references%2520for%2520researchers%250Aand%2520practitioners.%2520Furthermore%252C%2520we%2520introduce%2520the%2520benchmarks%2520for%2520RAG%252C%2520discuss%250Athe%2520limitations%2520of%2520current%2520RAG%2520systems%252C%2520and%2520suggest%2520potential%2520directions%2520for%250Afuture%2520research.%2520Github%253A%2520https%253A//github.com/PKU-DAIR/RAG-Survey.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19473v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-Augmented%20Generation%20for%20AI-Generated%20Content%3A%20A%20Survey&entry.906535625=Penghao%20Zhao%20and%20Hailin%20Zhang%20and%20Qinhan%20Yu%20and%20Zhengren%20Wang%20and%20Yunteng%20Geng%20and%20Fangcheng%20Fu%20and%20Ling%20Yang%20and%20Wentao%20Zhang%20and%20Jie%20Jiang%20and%20Bin%20Cui&entry.1292438233=%20%20Advancements%20in%20model%20algorithms%2C%20the%20growth%20of%20foundational%20models%2C%20and%0Aaccess%20to%20high-quality%20datasets%20have%20propelled%20the%20evolution%20of%20Artificial%0AIntelligence%20Generated%20Content%20%28AIGC%29.%20Despite%20its%20notable%20successes%2C%20AIGC%0Astill%20faces%20hurdles%20such%20as%20updating%20knowledge%2C%20handling%20long-tail%20data%2C%0Amitigating%20data%20leakage%2C%20and%20managing%20high%20training%20and%20inference%20costs.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20has%20recently%20emerged%20as%20a%20paradigm%20to%0Aaddress%20such%20challenges.%20In%20particular%2C%20RAG%20introduces%20the%20information%0Aretrieval%20process%2C%20which%20enhances%20the%20generation%20process%20by%20retrieving%20relevant%0Aobjects%20from%20available%20data%20stores%2C%20leading%20to%20higher%20accuracy%20and%20better%0Arobustness.%20In%20this%20paper%2C%20we%20comprehensively%20review%20existing%20efforts%20that%0Aintegrate%20RAG%20technique%20into%20AIGC%20scenarios.%20We%20first%20classify%20RAG%20foundations%0Aaccording%20to%20how%20the%20retriever%20augments%20the%20generator%2C%20distilling%20the%0Afundamental%20abstractions%20of%20the%20augmentation%20methodologies%20for%20various%0Aretrievers%20and%20generators.%20This%20unified%20perspective%20encompasses%20all%20RAG%0Ascenarios%2C%20illuminating%20advancements%20and%20pivotal%20technologies%20that%20help%20with%0Apotential%20future%20progress.%20We%20also%20summarize%20additional%20enhancements%20methods%0Afor%20RAG%2C%20facilitating%20effective%20engineering%20and%20implementation%20of%20RAG%20systems.%0AThen%20from%20another%20view%2C%20we%20survey%20on%20practical%20applications%20of%20RAG%20across%0Adifferent%20modalities%20and%20tasks%2C%20offering%20valuable%20references%20for%20researchers%0Aand%20practitioners.%20Furthermore%2C%20we%20introduce%20the%20benchmarks%20for%20RAG%2C%20discuss%0Athe%20limitations%20of%20current%20RAG%20systems%2C%20and%20suggest%20potential%20directions%20for%0Afuture%20research.%20Github%3A%20https%3A//github.com/PKU-DAIR/RAG-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19473v5&entry.124074799=Read"},
{"title": "Mastering Long-Tail Complexity on Graphs: Characterization, Learning,\n  and Generalization", "author": "Haohui Wang and Baoyu Jing and Kaize Ding and Yada Zhu and Wei Cheng and Si Zhang and Yonghui Fan and Liqing Zhang and Dawei Zhou", "abstract": "  In the context of long-tail classification on graphs, the vast majority of\nexisting work primarily revolves around the development of model debiasing\nstrategies, intending to mitigate class imbalances and enhance the overall\nperformance. Despite the notable success, there is very limited literature that\nprovides a theoretical tool for characterizing the behaviors of long-tail\nclasses in graphs and gaining insight into generalization performance in\nreal-world scenarios. To bridge this gap, we propose a generalization bound for\nlong-tail classification on graphs by formulating the problem in the fashion of\nmulti-task learning, i.e., each task corresponds to the prediction of one\nparticular class. Our theoretical results show that the generalization\nperformance of long-tail classification is dominated by the overall loss range\nand the task complexity. Building upon the theoretical findings, we propose a\nnovel generic framework HierTail for long-tail classification on graphs. In\nparticular, we start with a hierarchical task grouping module that allows us to\nassign related tasks into hypertasks and thus control the complexity of the\ntask space; then, we further design a balanced contrastive learning module to\nadaptively balance the gradients of both head and tail classes to control the\nloss range across all tasks in a unified fashion. Extensive experiments\ndemonstrate the effectiveness of HierTail in characterizing long-tail classes\non real graphs, which achieves up to 12.9% improvement over the leading\nbaseline method in accuracy.\n", "link": "http://arxiv.org/abs/2305.09938v4", "date": "2024-05-31", "relevancy": 2.6502, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5729}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mastering%20Long-Tail%20Complexity%20on%20Graphs%3A%20Characterization%2C%20Learning%2C%0A%20%20and%20Generalization&body=Title%3A%20Mastering%20Long-Tail%20Complexity%20on%20Graphs%3A%20Characterization%2C%20Learning%2C%0A%20%20and%20Generalization%0AAuthor%3A%20Haohui%20Wang%20and%20Baoyu%20Jing%20and%20Kaize%20Ding%20and%20Yada%20Zhu%20and%20Wei%20Cheng%20and%20Si%20Zhang%20and%20Yonghui%20Fan%20and%20Liqing%20Zhang%20and%20Dawei%20Zhou%0AAbstract%3A%20%20%20In%20the%20context%20of%20long-tail%20classification%20on%20graphs%2C%20the%20vast%20majority%20of%0Aexisting%20work%20primarily%20revolves%20around%20the%20development%20of%20model%20debiasing%0Astrategies%2C%20intending%20to%20mitigate%20class%20imbalances%20and%20enhance%20the%20overall%0Aperformance.%20Despite%20the%20notable%20success%2C%20there%20is%20very%20limited%20literature%20that%0Aprovides%20a%20theoretical%20tool%20for%20characterizing%20the%20behaviors%20of%20long-tail%0Aclasses%20in%20graphs%20and%20gaining%20insight%20into%20generalization%20performance%20in%0Areal-world%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20generalization%20bound%20for%0Along-tail%20classification%20on%20graphs%20by%20formulating%20the%20problem%20in%20the%20fashion%20of%0Amulti-task%20learning%2C%20i.e.%2C%20each%20task%20corresponds%20to%20the%20prediction%20of%20one%0Aparticular%20class.%20Our%20theoretical%20results%20show%20that%20the%20generalization%0Aperformance%20of%20long-tail%20classification%20is%20dominated%20by%20the%20overall%20loss%20range%0Aand%20the%20task%20complexity.%20Building%20upon%20the%20theoretical%20findings%2C%20we%20propose%20a%0Anovel%20generic%20framework%20HierTail%20for%20long-tail%20classification%20on%20graphs.%20In%0Aparticular%2C%20we%20start%20with%20a%20hierarchical%20task%20grouping%20module%20that%20allows%20us%20to%0Aassign%20related%20tasks%20into%20hypertasks%20and%20thus%20control%20the%20complexity%20of%20the%0Atask%20space%3B%20then%2C%20we%20further%20design%20a%20balanced%20contrastive%20learning%20module%20to%0Aadaptively%20balance%20the%20gradients%20of%20both%20head%20and%20tail%20classes%20to%20control%20the%0Aloss%20range%20across%20all%20tasks%20in%20a%20unified%20fashion.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20HierTail%20in%20characterizing%20long-tail%20classes%0Aon%20real%20graphs%2C%20which%20achieves%20up%20to%2012.9%25%20improvement%20over%20the%20leading%0Abaseline%20method%20in%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.09938v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMastering%2520Long-Tail%2520Complexity%2520on%2520Graphs%253A%2520Characterization%252C%2520Learning%252C%250A%2520%2520and%2520Generalization%26entry.906535625%3DHaohui%2520Wang%2520and%2520Baoyu%2520Jing%2520and%2520Kaize%2520Ding%2520and%2520Yada%2520Zhu%2520and%2520Wei%2520Cheng%2520and%2520Si%2520Zhang%2520and%2520Yonghui%2520Fan%2520and%2520Liqing%2520Zhang%2520and%2520Dawei%2520Zhou%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520long-tail%2520classification%2520on%2520graphs%252C%2520the%2520vast%2520majority%2520of%250Aexisting%2520work%2520primarily%2520revolves%2520around%2520the%2520development%2520of%2520model%2520debiasing%250Astrategies%252C%2520intending%2520to%2520mitigate%2520class%2520imbalances%2520and%2520enhance%2520the%2520overall%250Aperformance.%2520Despite%2520the%2520notable%2520success%252C%2520there%2520is%2520very%2520limited%2520literature%2520that%250Aprovides%2520a%2520theoretical%2520tool%2520for%2520characterizing%2520the%2520behaviors%2520of%2520long-tail%250Aclasses%2520in%2520graphs%2520and%2520gaining%2520insight%2520into%2520generalization%2520performance%2520in%250Areal-world%2520scenarios.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520a%2520generalization%2520bound%2520for%250Along-tail%2520classification%2520on%2520graphs%2520by%2520formulating%2520the%2520problem%2520in%2520the%2520fashion%2520of%250Amulti-task%2520learning%252C%2520i.e.%252C%2520each%2520task%2520corresponds%2520to%2520the%2520prediction%2520of%2520one%250Aparticular%2520class.%2520Our%2520theoretical%2520results%2520show%2520that%2520the%2520generalization%250Aperformance%2520of%2520long-tail%2520classification%2520is%2520dominated%2520by%2520the%2520overall%2520loss%2520range%250Aand%2520the%2520task%2520complexity.%2520Building%2520upon%2520the%2520theoretical%2520findings%252C%2520we%2520propose%2520a%250Anovel%2520generic%2520framework%2520HierTail%2520for%2520long-tail%2520classification%2520on%2520graphs.%2520In%250Aparticular%252C%2520we%2520start%2520with%2520a%2520hierarchical%2520task%2520grouping%2520module%2520that%2520allows%2520us%2520to%250Aassign%2520related%2520tasks%2520into%2520hypertasks%2520and%2520thus%2520control%2520the%2520complexity%2520of%2520the%250Atask%2520space%253B%2520then%252C%2520we%2520further%2520design%2520a%2520balanced%2520contrastive%2520learning%2520module%2520to%250Aadaptively%2520balance%2520the%2520gradients%2520of%2520both%2520head%2520and%2520tail%2520classes%2520to%2520control%2520the%250Aloss%2520range%2520across%2520all%2520tasks%2520in%2520a%2520unified%2520fashion.%2520Extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520of%2520HierTail%2520in%2520characterizing%2520long-tail%2520classes%250Aon%2520real%2520graphs%252C%2520which%2520achieves%2520up%2520to%252012.9%2525%2520improvement%2520over%2520the%2520leading%250Abaseline%2520method%2520in%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.09938v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mastering%20Long-Tail%20Complexity%20on%20Graphs%3A%20Characterization%2C%20Learning%2C%0A%20%20and%20Generalization&entry.906535625=Haohui%20Wang%20and%20Baoyu%20Jing%20and%20Kaize%20Ding%20and%20Yada%20Zhu%20and%20Wei%20Cheng%20and%20Si%20Zhang%20and%20Yonghui%20Fan%20and%20Liqing%20Zhang%20and%20Dawei%20Zhou&entry.1292438233=%20%20In%20the%20context%20of%20long-tail%20classification%20on%20graphs%2C%20the%20vast%20majority%20of%0Aexisting%20work%20primarily%20revolves%20around%20the%20development%20of%20model%20debiasing%0Astrategies%2C%20intending%20to%20mitigate%20class%20imbalances%20and%20enhance%20the%20overall%0Aperformance.%20Despite%20the%20notable%20success%2C%20there%20is%20very%20limited%20literature%20that%0Aprovides%20a%20theoretical%20tool%20for%20characterizing%20the%20behaviors%20of%20long-tail%0Aclasses%20in%20graphs%20and%20gaining%20insight%20into%20generalization%20performance%20in%0Areal-world%20scenarios.%20To%20bridge%20this%20gap%2C%20we%20propose%20a%20generalization%20bound%20for%0Along-tail%20classification%20on%20graphs%20by%20formulating%20the%20problem%20in%20the%20fashion%20of%0Amulti-task%20learning%2C%20i.e.%2C%20each%20task%20corresponds%20to%20the%20prediction%20of%20one%0Aparticular%20class.%20Our%20theoretical%20results%20show%20that%20the%20generalization%0Aperformance%20of%20long-tail%20classification%20is%20dominated%20by%20the%20overall%20loss%20range%0Aand%20the%20task%20complexity.%20Building%20upon%20the%20theoretical%20findings%2C%20we%20propose%20a%0Anovel%20generic%20framework%20HierTail%20for%20long-tail%20classification%20on%20graphs.%20In%0Aparticular%2C%20we%20start%20with%20a%20hierarchical%20task%20grouping%20module%20that%20allows%20us%20to%0Aassign%20related%20tasks%20into%20hypertasks%20and%20thus%20control%20the%20complexity%20of%20the%0Atask%20space%3B%20then%2C%20we%20further%20design%20a%20balanced%20contrastive%20learning%20module%20to%0Aadaptively%20balance%20the%20gradients%20of%20both%20head%20and%20tail%20classes%20to%20control%20the%0Aloss%20range%20across%20all%20tasks%20in%20a%20unified%20fashion.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20HierTail%20in%20characterizing%20long-tail%20classes%0Aon%20real%20graphs%2C%20which%20achieves%20up%20to%2012.9%25%20improvement%20over%20the%20leading%0Abaseline%20method%20in%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.09938v4&entry.124074799=Read"},
{"title": "Generative Adversarial Networks in Ultrasound Imaging: Extending Field\n  of View Beyond Conventional Limits", "author": "Matej Gazda and Samuel Kadoury and Jakub Gazda and Peter Drotar", "abstract": "  Transthoracic Echocardiography (TTE) is a fundamental, non-invasive\ndiagnostic tool in cardiovascular medicine, enabling detailed visualization of\ncardiac structures crucial for diagnosing various heart conditions. Despite its\nwidespread use, TTE ultrasound imaging faces inherent limitations, notably the\ntrade-off between field of view (FoV) and resolution. This paper introduces a\nnovel application of conditional Generative Adversarial Networks (cGANs),\nspecifically designed to extend the FoV in TTE ultrasound imaging while\nmaintaining high resolution. Our proposed cGAN architecture, termed echoGAN,\ndemonstrates the capability to generate realistic anatomical structures through\noutpainting, effectively broadening the viewable area in medical imaging. This\nadvancement has the potential to enhance both automatic and manual ultrasound\nnavigation, offering a more comprehensive view that could significantly reduce\nthe learning curve associated with ultrasound imaging and aid in more accurate\ndiagnoses. The results confirm that echoGAN reliably reproduce detailed cardiac\nfeatures, thereby promising a significant step forward in the field of\nnon-invasive cardiac naviagation and diagnostics.\n", "link": "http://arxiv.org/abs/2405.20981v1", "date": "2024-05-31", "relevancy": 2.6468, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5333}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5277}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Adversarial%20Networks%20in%20Ultrasound%20Imaging%3A%20Extending%20Field%0A%20%20of%20View%20Beyond%20Conventional%20Limits&body=Title%3A%20Generative%20Adversarial%20Networks%20in%20Ultrasound%20Imaging%3A%20Extending%20Field%0A%20%20of%20View%20Beyond%20Conventional%20Limits%0AAuthor%3A%20Matej%20Gazda%20and%20Samuel%20Kadoury%20and%20Jakub%20Gazda%20and%20Peter%20Drotar%0AAbstract%3A%20%20%20Transthoracic%20Echocardiography%20%28TTE%29%20is%20a%20fundamental%2C%20non-invasive%0Adiagnostic%20tool%20in%20cardiovascular%20medicine%2C%20enabling%20detailed%20visualization%20of%0Acardiac%20structures%20crucial%20for%20diagnosing%20various%20heart%20conditions.%20Despite%20its%0Awidespread%20use%2C%20TTE%20ultrasound%20imaging%20faces%20inherent%20limitations%2C%20notably%20the%0Atrade-off%20between%20field%20of%20view%20%28FoV%29%20and%20resolution.%20This%20paper%20introduces%20a%0Anovel%20application%20of%20conditional%20Generative%20Adversarial%20Networks%20%28cGANs%29%2C%0Aspecifically%20designed%20to%20extend%20the%20FoV%20in%20TTE%20ultrasound%20imaging%20while%0Amaintaining%20high%20resolution.%20Our%20proposed%20cGAN%20architecture%2C%20termed%20echoGAN%2C%0Ademonstrates%20the%20capability%20to%20generate%20realistic%20anatomical%20structures%20through%0Aoutpainting%2C%20effectively%20broadening%20the%20viewable%20area%20in%20medical%20imaging.%20This%0Aadvancement%20has%20the%20potential%20to%20enhance%20both%20automatic%20and%20manual%20ultrasound%0Anavigation%2C%20offering%20a%20more%20comprehensive%20view%20that%20could%20significantly%20reduce%0Athe%20learning%20curve%20associated%20with%20ultrasound%20imaging%20and%20aid%20in%20more%20accurate%0Adiagnoses.%20The%20results%20confirm%20that%20echoGAN%20reliably%20reproduce%20detailed%20cardiac%0Afeatures%2C%20thereby%20promising%20a%20significant%20step%20forward%20in%20the%20field%20of%0Anon-invasive%20cardiac%20naviagation%20and%20diagnostics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Adversarial%2520Networks%2520in%2520Ultrasound%2520Imaging%253A%2520Extending%2520Field%250A%2520%2520of%2520View%2520Beyond%2520Conventional%2520Limits%26entry.906535625%3DMatej%2520Gazda%2520and%2520Samuel%2520Kadoury%2520and%2520Jakub%2520Gazda%2520and%2520Peter%2520Drotar%26entry.1292438233%3D%2520%2520Transthoracic%2520Echocardiography%2520%2528TTE%2529%2520is%2520a%2520fundamental%252C%2520non-invasive%250Adiagnostic%2520tool%2520in%2520cardiovascular%2520medicine%252C%2520enabling%2520detailed%2520visualization%2520of%250Acardiac%2520structures%2520crucial%2520for%2520diagnosing%2520various%2520heart%2520conditions.%2520Despite%2520its%250Awidespread%2520use%252C%2520TTE%2520ultrasound%2520imaging%2520faces%2520inherent%2520limitations%252C%2520notably%2520the%250Atrade-off%2520between%2520field%2520of%2520view%2520%2528FoV%2529%2520and%2520resolution.%2520This%2520paper%2520introduces%2520a%250Anovel%2520application%2520of%2520conditional%2520Generative%2520Adversarial%2520Networks%2520%2528cGANs%2529%252C%250Aspecifically%2520designed%2520to%2520extend%2520the%2520FoV%2520in%2520TTE%2520ultrasound%2520imaging%2520while%250Amaintaining%2520high%2520resolution.%2520Our%2520proposed%2520cGAN%2520architecture%252C%2520termed%2520echoGAN%252C%250Ademonstrates%2520the%2520capability%2520to%2520generate%2520realistic%2520anatomical%2520structures%2520through%250Aoutpainting%252C%2520effectively%2520broadening%2520the%2520viewable%2520area%2520in%2520medical%2520imaging.%2520This%250Aadvancement%2520has%2520the%2520potential%2520to%2520enhance%2520both%2520automatic%2520and%2520manual%2520ultrasound%250Anavigation%252C%2520offering%2520a%2520more%2520comprehensive%2520view%2520that%2520could%2520significantly%2520reduce%250Athe%2520learning%2520curve%2520associated%2520with%2520ultrasound%2520imaging%2520and%2520aid%2520in%2520more%2520accurate%250Adiagnoses.%2520The%2520results%2520confirm%2520that%2520echoGAN%2520reliably%2520reproduce%2520detailed%2520cardiac%250Afeatures%252C%2520thereby%2520promising%2520a%2520significant%2520step%2520forward%2520in%2520the%2520field%2520of%250Anon-invasive%2520cardiac%2520naviagation%2520and%2520diagnostics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Adversarial%20Networks%20in%20Ultrasound%20Imaging%3A%20Extending%20Field%0A%20%20of%20View%20Beyond%20Conventional%20Limits&entry.906535625=Matej%20Gazda%20and%20Samuel%20Kadoury%20and%20Jakub%20Gazda%20and%20Peter%20Drotar&entry.1292438233=%20%20Transthoracic%20Echocardiography%20%28TTE%29%20is%20a%20fundamental%2C%20non-invasive%0Adiagnostic%20tool%20in%20cardiovascular%20medicine%2C%20enabling%20detailed%20visualization%20of%0Acardiac%20structures%20crucial%20for%20diagnosing%20various%20heart%20conditions.%20Despite%20its%0Awidespread%20use%2C%20TTE%20ultrasound%20imaging%20faces%20inherent%20limitations%2C%20notably%20the%0Atrade-off%20between%20field%20of%20view%20%28FoV%29%20and%20resolution.%20This%20paper%20introduces%20a%0Anovel%20application%20of%20conditional%20Generative%20Adversarial%20Networks%20%28cGANs%29%2C%0Aspecifically%20designed%20to%20extend%20the%20FoV%20in%20TTE%20ultrasound%20imaging%20while%0Amaintaining%20high%20resolution.%20Our%20proposed%20cGAN%20architecture%2C%20termed%20echoGAN%2C%0Ademonstrates%20the%20capability%20to%20generate%20realistic%20anatomical%20structures%20through%0Aoutpainting%2C%20effectively%20broadening%20the%20viewable%20area%20in%20medical%20imaging.%20This%0Aadvancement%20has%20the%20potential%20to%20enhance%20both%20automatic%20and%20manual%20ultrasound%0Anavigation%2C%20offering%20a%20more%20comprehensive%20view%20that%20could%20significantly%20reduce%0Athe%20learning%20curve%20associated%20with%20ultrasound%20imaging%20and%20aid%20in%20more%20accurate%0Adiagnoses.%20The%20results%20confirm%20that%20echoGAN%20reliably%20reproduce%20detailed%20cardiac%0Afeatures%2C%20thereby%20promising%20a%20significant%20step%20forward%20in%20the%20field%20of%0Anon-invasive%20cardiac%20naviagation%20and%20diagnostics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20981v1&entry.124074799=Read"},
{"title": "Mixed Diffusion for 3D Indoor Scene Synthesis", "author": "Siyi Hu and Diego Martin Arroyo and Stephanie Debats and Fabian Manhardt and Luca Carlone and Federico Tombari", "abstract": "  Realistic conditional 3D scene synthesis significantly enhances and\naccelerates the creation of virtual environments, which can also provide\nextensive training data for computer vision and robotics research among other\napplications. Diffusion models have shown great performance in related\napplications, e.g., making precise arrangements of unordered sets. However,\nthese models have not been fully explored in floor-conditioned scene synthesis\nproblems. We present MiDiffusion, a novel mixed discrete-continuous diffusion\nmodel architecture, designed to synthesize plausible 3D indoor scenes from\ngiven room types, floor plans, and potentially pre-existing objects. We\nrepresent a scene layout by a 2D floor plan and a set of objects, each defined\nby its category, location, size, and orientation. Our approach uniquely\nimplements structured corruption across the mixed discrete semantic and\ncontinuous geometric domains, resulting in a better conditioned problem for the\nreverse denoising step. We evaluate our approach on the 3D-FRONT dataset. Our\nexperimental results demonstrate that MiDiffusion substantially outperforms\nstate-of-the-art autoregressive and diffusion models in floor-conditioned 3D\nscene synthesis. In addition, our models can handle partial object constraints\nvia a corruption-and-masking strategy without task specific training. We show\nMiDiffusion maintains clear advantages over existing approaches in scene\ncompletion and furniture arrangement experiments.\n", "link": "http://arxiv.org/abs/2405.21066v1", "date": "2024-05-31", "relevancy": 2.6099, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6558}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6558}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixed%20Diffusion%20for%203D%20Indoor%20Scene%20Synthesis&body=Title%3A%20Mixed%20Diffusion%20for%203D%20Indoor%20Scene%20Synthesis%0AAuthor%3A%20Siyi%20Hu%20and%20Diego%20Martin%20Arroyo%20and%20Stephanie%20Debats%20and%20Fabian%20Manhardt%20and%20Luca%20Carlone%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Realistic%20conditional%203D%20scene%20synthesis%20significantly%20enhances%20and%0Aaccelerates%20the%20creation%20of%20virtual%20environments%2C%20which%20can%20also%20provide%0Aextensive%20training%20data%20for%20computer%20vision%20and%20robotics%20research%20among%20other%0Aapplications.%20Diffusion%20models%20have%20shown%20great%20performance%20in%20related%0Aapplications%2C%20e.g.%2C%20making%20precise%20arrangements%20of%20unordered%20sets.%20However%2C%0Athese%20models%20have%20not%20been%20fully%20explored%20in%20floor-conditioned%20scene%20synthesis%0Aproblems.%20We%20present%20MiDiffusion%2C%20a%20novel%20mixed%20discrete-continuous%20diffusion%0Amodel%20architecture%2C%20designed%20to%20synthesize%20plausible%203D%20indoor%20scenes%20from%0Agiven%20room%20types%2C%20floor%20plans%2C%20and%20potentially%20pre-existing%20objects.%20We%0Arepresent%20a%20scene%20layout%20by%20a%202D%20floor%20plan%20and%20a%20set%20of%20objects%2C%20each%20defined%0Aby%20its%20category%2C%20location%2C%20size%2C%20and%20orientation.%20Our%20approach%20uniquely%0Aimplements%20structured%20corruption%20across%20the%20mixed%20discrete%20semantic%20and%0Acontinuous%20geometric%20domains%2C%20resulting%20in%20a%20better%20conditioned%20problem%20for%20the%0Areverse%20denoising%20step.%20We%20evaluate%20our%20approach%20on%20the%203D-FRONT%20dataset.%20Our%0Aexperimental%20results%20demonstrate%20that%20MiDiffusion%20substantially%20outperforms%0Astate-of-the-art%20autoregressive%20and%20diffusion%20models%20in%20floor-conditioned%203D%0Ascene%20synthesis.%20In%20addition%2C%20our%20models%20can%20handle%20partial%20object%20constraints%0Avia%20a%20corruption-and-masking%20strategy%20without%20task%20specific%20training.%20We%20show%0AMiDiffusion%20maintains%20clear%20advantages%20over%20existing%20approaches%20in%20scene%0Acompletion%20and%20furniture%20arrangement%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixed%2520Diffusion%2520for%25203D%2520Indoor%2520Scene%2520Synthesis%26entry.906535625%3DSiyi%2520Hu%2520and%2520Diego%2520Martin%2520Arroyo%2520and%2520Stephanie%2520Debats%2520and%2520Fabian%2520Manhardt%2520and%2520Luca%2520Carlone%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520Realistic%2520conditional%25203D%2520scene%2520synthesis%2520significantly%2520enhances%2520and%250Aaccelerates%2520the%2520creation%2520of%2520virtual%2520environments%252C%2520which%2520can%2520also%2520provide%250Aextensive%2520training%2520data%2520for%2520computer%2520vision%2520and%2520robotics%2520research%2520among%2520other%250Aapplications.%2520Diffusion%2520models%2520have%2520shown%2520great%2520performance%2520in%2520related%250Aapplications%252C%2520e.g.%252C%2520making%2520precise%2520arrangements%2520of%2520unordered%2520sets.%2520However%252C%250Athese%2520models%2520have%2520not%2520been%2520fully%2520explored%2520in%2520floor-conditioned%2520scene%2520synthesis%250Aproblems.%2520We%2520present%2520MiDiffusion%252C%2520a%2520novel%2520mixed%2520discrete-continuous%2520diffusion%250Amodel%2520architecture%252C%2520designed%2520to%2520synthesize%2520plausible%25203D%2520indoor%2520scenes%2520from%250Agiven%2520room%2520types%252C%2520floor%2520plans%252C%2520and%2520potentially%2520pre-existing%2520objects.%2520We%250Arepresent%2520a%2520scene%2520layout%2520by%2520a%25202D%2520floor%2520plan%2520and%2520a%2520set%2520of%2520objects%252C%2520each%2520defined%250Aby%2520its%2520category%252C%2520location%252C%2520size%252C%2520and%2520orientation.%2520Our%2520approach%2520uniquely%250Aimplements%2520structured%2520corruption%2520across%2520the%2520mixed%2520discrete%2520semantic%2520and%250Acontinuous%2520geometric%2520domains%252C%2520resulting%2520in%2520a%2520better%2520conditioned%2520problem%2520for%2520the%250Areverse%2520denoising%2520step.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%25203D-FRONT%2520dataset.%2520Our%250Aexperimental%2520results%2520demonstrate%2520that%2520MiDiffusion%2520substantially%2520outperforms%250Astate-of-the-art%2520autoregressive%2520and%2520diffusion%2520models%2520in%2520floor-conditioned%25203D%250Ascene%2520synthesis.%2520In%2520addition%252C%2520our%2520models%2520can%2520handle%2520partial%2520object%2520constraints%250Avia%2520a%2520corruption-and-masking%2520strategy%2520without%2520task%2520specific%2520training.%2520We%2520show%250AMiDiffusion%2520maintains%2520clear%2520advantages%2520over%2520existing%2520approaches%2520in%2520scene%250Acompletion%2520and%2520furniture%2520arrangement%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixed%20Diffusion%20for%203D%20Indoor%20Scene%20Synthesis&entry.906535625=Siyi%20Hu%20and%20Diego%20Martin%20Arroyo%20and%20Stephanie%20Debats%20and%20Fabian%20Manhardt%20and%20Luca%20Carlone%20and%20Federico%20Tombari&entry.1292438233=%20%20Realistic%20conditional%203D%20scene%20synthesis%20significantly%20enhances%20and%0Aaccelerates%20the%20creation%20of%20virtual%20environments%2C%20which%20can%20also%20provide%0Aextensive%20training%20data%20for%20computer%20vision%20and%20robotics%20research%20among%20other%0Aapplications.%20Diffusion%20models%20have%20shown%20great%20performance%20in%20related%0Aapplications%2C%20e.g.%2C%20making%20precise%20arrangements%20of%20unordered%20sets.%20However%2C%0Athese%20models%20have%20not%20been%20fully%20explored%20in%20floor-conditioned%20scene%20synthesis%0Aproblems.%20We%20present%20MiDiffusion%2C%20a%20novel%20mixed%20discrete-continuous%20diffusion%0Amodel%20architecture%2C%20designed%20to%20synthesize%20plausible%203D%20indoor%20scenes%20from%0Agiven%20room%20types%2C%20floor%20plans%2C%20and%20potentially%20pre-existing%20objects.%20We%0Arepresent%20a%20scene%20layout%20by%20a%202D%20floor%20plan%20and%20a%20set%20of%20objects%2C%20each%20defined%0Aby%20its%20category%2C%20location%2C%20size%2C%20and%20orientation.%20Our%20approach%20uniquely%0Aimplements%20structured%20corruption%20across%20the%20mixed%20discrete%20semantic%20and%0Acontinuous%20geometric%20domains%2C%20resulting%20in%20a%20better%20conditioned%20problem%20for%20the%0Areverse%20denoising%20step.%20We%20evaluate%20our%20approach%20on%20the%203D-FRONT%20dataset.%20Our%0Aexperimental%20results%20demonstrate%20that%20MiDiffusion%20substantially%20outperforms%0Astate-of-the-art%20autoregressive%20and%20diffusion%20models%20in%20floor-conditioned%203D%0Ascene%20synthesis.%20In%20addition%2C%20our%20models%20can%20handle%20partial%20object%20constraints%0Avia%20a%20corruption-and-masking%20strategy%20without%20task%20specific%20training.%20We%20show%0AMiDiffusion%20maintains%20clear%20advantages%20over%20existing%20approaches%20in%20scene%0Acompletion%20and%20furniture%20arrangement%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21066v1&entry.124074799=Read"},
{"title": "Distributed agency in second language learning and teaching through\n  generative AI", "author": "Robert Godwin-Jones", "abstract": "  Generative AI offers significant opportunities for language learning. Tools\nlike ChatGPT can provide informal second language practice through chats in\nwritten or voice forms, with the learner specifying through prompts\nconversational parameters such as proficiency level, language register, and\ndiscussion topics. AI can be instructed to give corrective feedback, create\npractice exercises, or develop an extended study plan. Instructors can use AI\nto build learning and assessment materials in a variety of media. AI is likely\nto make immersive technologies more powerful and versatile, moving away from\nscripted interactions. For both learners and teachers, it is important to\nunderstand the limitations of AI systems that arise from their purely\nstatistical model of human language, which limits their ability to deal with\nnuanced social and cultural aspects of language use. Additionally, there are\nethical concerns over how AI systems are created as well as practical\nconstraints in their use, especially for less privileged populations. The power\nand versatility of AI tools are likely to turn them into valuable and constant\ncompanions in many peoples lives (akin to smartphones), creating a close\nconnection that goes beyond simple tool use. Ecological theories such as\nsociomaterialism are helpful in examining the shared agency that develops\nthrough close user-AI interactions, as are the perspectives on human-object\nrelations from Indigenous cultures.\n", "link": "http://arxiv.org/abs/2403.20216v4", "date": "2024-05-31", "relevancy": 2.539, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5571}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4979}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distributed%20agency%20in%20second%20language%20learning%20and%20teaching%20through%0A%20%20generative%20AI&body=Title%3A%20Distributed%20agency%20in%20second%20language%20learning%20and%20teaching%20through%0A%20%20generative%20AI%0AAuthor%3A%20Robert%20Godwin-Jones%0AAbstract%3A%20%20%20Generative%20AI%20offers%20significant%20opportunities%20for%20language%20learning.%20Tools%0Alike%20ChatGPT%20can%20provide%20informal%20second%20language%20practice%20through%20chats%20in%0Awritten%20or%20voice%20forms%2C%20with%20the%20learner%20specifying%20through%20prompts%0Aconversational%20parameters%20such%20as%20proficiency%20level%2C%20language%20register%2C%20and%0Adiscussion%20topics.%20AI%20can%20be%20instructed%20to%20give%20corrective%20feedback%2C%20create%0Apractice%20exercises%2C%20or%20develop%20an%20extended%20study%20plan.%20Instructors%20can%20use%20AI%0Ato%20build%20learning%20and%20assessment%20materials%20in%20a%20variety%20of%20media.%20AI%20is%20likely%0Ato%20make%20immersive%20technologies%20more%20powerful%20and%20versatile%2C%20moving%20away%20from%0Ascripted%20interactions.%20For%20both%20learners%20and%20teachers%2C%20it%20is%20important%20to%0Aunderstand%20the%20limitations%20of%20AI%20systems%20that%20arise%20from%20their%20purely%0Astatistical%20model%20of%20human%20language%2C%20which%20limits%20their%20ability%20to%20deal%20with%0Anuanced%20social%20and%20cultural%20aspects%20of%20language%20use.%20Additionally%2C%20there%20are%0Aethical%20concerns%20over%20how%20AI%20systems%20are%20created%20as%20well%20as%20practical%0Aconstraints%20in%20their%20use%2C%20especially%20for%20less%20privileged%20populations.%20The%20power%0Aand%20versatility%20of%20AI%20tools%20are%20likely%20to%20turn%20them%20into%20valuable%20and%20constant%0Acompanions%20in%20many%20peoples%20lives%20%28akin%20to%20smartphones%29%2C%20creating%20a%20close%0Aconnection%20that%20goes%20beyond%20simple%20tool%20use.%20Ecological%20theories%20such%20as%0Asociomaterialism%20are%20helpful%20in%20examining%20the%20shared%20agency%20that%20develops%0Athrough%20close%20user-AI%20interactions%2C%20as%20are%20the%20perspectives%20on%20human-object%0Arelations%20from%20Indigenous%20cultures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.20216v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistributed%2520agency%2520in%2520second%2520language%2520learning%2520and%2520teaching%2520through%250A%2520%2520generative%2520AI%26entry.906535625%3DRobert%2520Godwin-Jones%26entry.1292438233%3D%2520%2520Generative%2520AI%2520offers%2520significant%2520opportunities%2520for%2520language%2520learning.%2520Tools%250Alike%2520ChatGPT%2520can%2520provide%2520informal%2520second%2520language%2520practice%2520through%2520chats%2520in%250Awritten%2520or%2520voice%2520forms%252C%2520with%2520the%2520learner%2520specifying%2520through%2520prompts%250Aconversational%2520parameters%2520such%2520as%2520proficiency%2520level%252C%2520language%2520register%252C%2520and%250Adiscussion%2520topics.%2520AI%2520can%2520be%2520instructed%2520to%2520give%2520corrective%2520feedback%252C%2520create%250Apractice%2520exercises%252C%2520or%2520develop%2520an%2520extended%2520study%2520plan.%2520Instructors%2520can%2520use%2520AI%250Ato%2520build%2520learning%2520and%2520assessment%2520materials%2520in%2520a%2520variety%2520of%2520media.%2520AI%2520is%2520likely%250Ato%2520make%2520immersive%2520technologies%2520more%2520powerful%2520and%2520versatile%252C%2520moving%2520away%2520from%250Ascripted%2520interactions.%2520For%2520both%2520learners%2520and%2520teachers%252C%2520it%2520is%2520important%2520to%250Aunderstand%2520the%2520limitations%2520of%2520AI%2520systems%2520that%2520arise%2520from%2520their%2520purely%250Astatistical%2520model%2520of%2520human%2520language%252C%2520which%2520limits%2520their%2520ability%2520to%2520deal%2520with%250Anuanced%2520social%2520and%2520cultural%2520aspects%2520of%2520language%2520use.%2520Additionally%252C%2520there%2520are%250Aethical%2520concerns%2520over%2520how%2520AI%2520systems%2520are%2520created%2520as%2520well%2520as%2520practical%250Aconstraints%2520in%2520their%2520use%252C%2520especially%2520for%2520less%2520privileged%2520populations.%2520The%2520power%250Aand%2520versatility%2520of%2520AI%2520tools%2520are%2520likely%2520to%2520turn%2520them%2520into%2520valuable%2520and%2520constant%250Acompanions%2520in%2520many%2520peoples%2520lives%2520%2528akin%2520to%2520smartphones%2529%252C%2520creating%2520a%2520close%250Aconnection%2520that%2520goes%2520beyond%2520simple%2520tool%2520use.%2520Ecological%2520theories%2520such%2520as%250Asociomaterialism%2520are%2520helpful%2520in%2520examining%2520the%2520shared%2520agency%2520that%2520develops%250Athrough%2520close%2520user-AI%2520interactions%252C%2520as%2520are%2520the%2520perspectives%2520on%2520human-object%250Arelations%2520from%2520Indigenous%2520cultures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.20216v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distributed%20agency%20in%20second%20language%20learning%20and%20teaching%20through%0A%20%20generative%20AI&entry.906535625=Robert%20Godwin-Jones&entry.1292438233=%20%20Generative%20AI%20offers%20significant%20opportunities%20for%20language%20learning.%20Tools%0Alike%20ChatGPT%20can%20provide%20informal%20second%20language%20practice%20through%20chats%20in%0Awritten%20or%20voice%20forms%2C%20with%20the%20learner%20specifying%20through%20prompts%0Aconversational%20parameters%20such%20as%20proficiency%20level%2C%20language%20register%2C%20and%0Adiscussion%20topics.%20AI%20can%20be%20instructed%20to%20give%20corrective%20feedback%2C%20create%0Apractice%20exercises%2C%20or%20develop%20an%20extended%20study%20plan.%20Instructors%20can%20use%20AI%0Ato%20build%20learning%20and%20assessment%20materials%20in%20a%20variety%20of%20media.%20AI%20is%20likely%0Ato%20make%20immersive%20technologies%20more%20powerful%20and%20versatile%2C%20moving%20away%20from%0Ascripted%20interactions.%20For%20both%20learners%20and%20teachers%2C%20it%20is%20important%20to%0Aunderstand%20the%20limitations%20of%20AI%20systems%20that%20arise%20from%20their%20purely%0Astatistical%20model%20of%20human%20language%2C%20which%20limits%20their%20ability%20to%20deal%20with%0Anuanced%20social%20and%20cultural%20aspects%20of%20language%20use.%20Additionally%2C%20there%20are%0Aethical%20concerns%20over%20how%20AI%20systems%20are%20created%20as%20well%20as%20practical%0Aconstraints%20in%20their%20use%2C%20especially%20for%20less%20privileged%20populations.%20The%20power%0Aand%20versatility%20of%20AI%20tools%20are%20likely%20to%20turn%20them%20into%20valuable%20and%20constant%0Acompanions%20in%20many%20peoples%20lives%20%28akin%20to%20smartphones%29%2C%20creating%20a%20close%0Aconnection%20that%20goes%20beyond%20simple%20tool%20use.%20Ecological%20theories%20such%20as%0Asociomaterialism%20are%20helpful%20in%20examining%20the%20shared%20agency%20that%20develops%0Athrough%20close%20user-AI%20interactions%2C%20as%20are%20the%20perspectives%20on%20human-object%0Arelations%20from%20Indigenous%20cultures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.20216v4&entry.124074799=Read"},
{"title": "The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a\n  Clean Model on Poisoned Data", "author": "Zixuan Zhu and Rui Wang and Cong Zou and Lihua Jing", "abstract": "  Recently, backdoor attacks have posed a serious security threat to the\ntraining process of deep neural networks (DNNs). The attacked model behaves\nnormally on benign samples but outputs a specific result when the trigger is\npresent. However, compared with the rocketing progress of backdoor attacks,\nexisting defenses are difficult to deal with these threats effectively or\nrequire benign samples to work, which may be unavailable in real scenarios. In\nthis paper, we find that the poisoned samples and benign samples can be\ndistinguished with prediction entropy. This inspires us to propose a novel\ndual-network training framework: The Victim and The Beneficiary (V&B), which\nexploits a poisoned model to train a clean model without extra benign samples.\nFirstly, we sacrifice the Victim network to be a powerful poisoned sample\ndetector by training on suspicious samples. Secondly, we train the Beneficiary\nnetwork on the credible samples selected by the Victim to inhibit backdoor\ninjection. Thirdly, a semi-supervised suppression strategy is adopted for\nerasing potential backdoors and improving model performance. Furthermore, to\nbetter inhibit missed poisoned samples, we propose a strong data augmentation\nmethod, AttentionMix, which works well with our proposed V&B framework.\nExtensive experiments on two widely used datasets against 6 state-of-the-art\nattacks demonstrate that our framework is effective in preventing backdoor\ninjection and robust to various attacks while maintaining the performance on\nbenign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB.\n", "link": "http://arxiv.org/abs/2404.11265v2", "date": "2024-05-31", "relevancy": 2.5249, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5342}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5132}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Victim%20and%20The%20Beneficiary%3A%20Exploiting%20a%20Poisoned%20Model%20to%20Train%20a%0A%20%20Clean%20Model%20on%20Poisoned%20Data&body=Title%3A%20The%20Victim%20and%20The%20Beneficiary%3A%20Exploiting%20a%20Poisoned%20Model%20to%20Train%20a%0A%20%20Clean%20Model%20on%20Poisoned%20Data%0AAuthor%3A%20Zixuan%20Zhu%20and%20Rui%20Wang%20and%20Cong%20Zou%20and%20Lihua%20Jing%0AAbstract%3A%20%20%20Recently%2C%20backdoor%20attacks%20have%20posed%20a%20serious%20security%20threat%20to%20the%0Atraining%20process%20of%20deep%20neural%20networks%20%28DNNs%29.%20The%20attacked%20model%20behaves%0Anormally%20on%20benign%20samples%20but%20outputs%20a%20specific%20result%20when%20the%20trigger%20is%0Apresent.%20However%2C%20compared%20with%20the%20rocketing%20progress%20of%20backdoor%20attacks%2C%0Aexisting%20defenses%20are%20difficult%20to%20deal%20with%20these%20threats%20effectively%20or%0Arequire%20benign%20samples%20to%20work%2C%20which%20may%20be%20unavailable%20in%20real%20scenarios.%20In%0Athis%20paper%2C%20we%20find%20that%20the%20poisoned%20samples%20and%20benign%20samples%20can%20be%0Adistinguished%20with%20prediction%20entropy.%20This%20inspires%20us%20to%20propose%20a%20novel%0Adual-network%20training%20framework%3A%20The%20Victim%20and%20The%20Beneficiary%20%28V%26B%29%2C%20which%0Aexploits%20a%20poisoned%20model%20to%20train%20a%20clean%20model%20without%20extra%20benign%20samples.%0AFirstly%2C%20we%20sacrifice%20the%20Victim%20network%20to%20be%20a%20powerful%20poisoned%20sample%0Adetector%20by%20training%20on%20suspicious%20samples.%20Secondly%2C%20we%20train%20the%20Beneficiary%0Anetwork%20on%20the%20credible%20samples%20selected%20by%20the%20Victim%20to%20inhibit%20backdoor%0Ainjection.%20Thirdly%2C%20a%20semi-supervised%20suppression%20strategy%20is%20adopted%20for%0Aerasing%20potential%20backdoors%20and%20improving%20model%20performance.%20Furthermore%2C%20to%0Abetter%20inhibit%20missed%20poisoned%20samples%2C%20we%20propose%20a%20strong%20data%20augmentation%0Amethod%2C%20AttentionMix%2C%20which%20works%20well%20with%20our%20proposed%20V%26B%20framework.%0AExtensive%20experiments%20on%20two%20widely%20used%20datasets%20against%206%20state-of-the-art%0Aattacks%20demonstrate%20that%20our%20framework%20is%20effective%20in%20preventing%20backdoor%0Ainjection%20and%20robust%20to%20various%20attacks%20while%20maintaining%20the%20performance%20on%0Abenign%20samples.%20Our%20code%20is%20available%20at%20https%3A//github.com/Zixuan-Zhu/VaB.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Victim%2520and%2520The%2520Beneficiary%253A%2520Exploiting%2520a%2520Poisoned%2520Model%2520to%2520Train%2520a%250A%2520%2520Clean%2520Model%2520on%2520Poisoned%2520Data%26entry.906535625%3DZixuan%2520Zhu%2520and%2520Rui%2520Wang%2520and%2520Cong%2520Zou%2520and%2520Lihua%2520Jing%26entry.1292438233%3D%2520%2520Recently%252C%2520backdoor%2520attacks%2520have%2520posed%2520a%2520serious%2520security%2520threat%2520to%2520the%250Atraining%2520process%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520The%2520attacked%2520model%2520behaves%250Anormally%2520on%2520benign%2520samples%2520but%2520outputs%2520a%2520specific%2520result%2520when%2520the%2520trigger%2520is%250Apresent.%2520However%252C%2520compared%2520with%2520the%2520rocketing%2520progress%2520of%2520backdoor%2520attacks%252C%250Aexisting%2520defenses%2520are%2520difficult%2520to%2520deal%2520with%2520these%2520threats%2520effectively%2520or%250Arequire%2520benign%2520samples%2520to%2520work%252C%2520which%2520may%2520be%2520unavailable%2520in%2520real%2520scenarios.%2520In%250Athis%2520paper%252C%2520we%2520find%2520that%2520the%2520poisoned%2520samples%2520and%2520benign%2520samples%2520can%2520be%250Adistinguished%2520with%2520prediction%2520entropy.%2520This%2520inspires%2520us%2520to%2520propose%2520a%2520novel%250Adual-network%2520training%2520framework%253A%2520The%2520Victim%2520and%2520The%2520Beneficiary%2520%2528V%2526B%2529%252C%2520which%250Aexploits%2520a%2520poisoned%2520model%2520to%2520train%2520a%2520clean%2520model%2520without%2520extra%2520benign%2520samples.%250AFirstly%252C%2520we%2520sacrifice%2520the%2520Victim%2520network%2520to%2520be%2520a%2520powerful%2520poisoned%2520sample%250Adetector%2520by%2520training%2520on%2520suspicious%2520samples.%2520Secondly%252C%2520we%2520train%2520the%2520Beneficiary%250Anetwork%2520on%2520the%2520credible%2520samples%2520selected%2520by%2520the%2520Victim%2520to%2520inhibit%2520backdoor%250Ainjection.%2520Thirdly%252C%2520a%2520semi-supervised%2520suppression%2520strategy%2520is%2520adopted%2520for%250Aerasing%2520potential%2520backdoors%2520and%2520improving%2520model%2520performance.%2520Furthermore%252C%2520to%250Abetter%2520inhibit%2520missed%2520poisoned%2520samples%252C%2520we%2520propose%2520a%2520strong%2520data%2520augmentation%250Amethod%252C%2520AttentionMix%252C%2520which%2520works%2520well%2520with%2520our%2520proposed%2520V%2526B%2520framework.%250AExtensive%2520experiments%2520on%2520two%2520widely%2520used%2520datasets%2520against%25206%2520state-of-the-art%250Aattacks%2520demonstrate%2520that%2520our%2520framework%2520is%2520effective%2520in%2520preventing%2520backdoor%250Ainjection%2520and%2520robust%2520to%2520various%2520attacks%2520while%2520maintaining%2520the%2520performance%2520on%250Abenign%2520samples.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Zixuan-Zhu/VaB.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Victim%20and%20The%20Beneficiary%3A%20Exploiting%20a%20Poisoned%20Model%20to%20Train%20a%0A%20%20Clean%20Model%20on%20Poisoned%20Data&entry.906535625=Zixuan%20Zhu%20and%20Rui%20Wang%20and%20Cong%20Zou%20and%20Lihua%20Jing&entry.1292438233=%20%20Recently%2C%20backdoor%20attacks%20have%20posed%20a%20serious%20security%20threat%20to%20the%0Atraining%20process%20of%20deep%20neural%20networks%20%28DNNs%29.%20The%20attacked%20model%20behaves%0Anormally%20on%20benign%20samples%20but%20outputs%20a%20specific%20result%20when%20the%20trigger%20is%0Apresent.%20However%2C%20compared%20with%20the%20rocketing%20progress%20of%20backdoor%20attacks%2C%0Aexisting%20defenses%20are%20difficult%20to%20deal%20with%20these%20threats%20effectively%20or%0Arequire%20benign%20samples%20to%20work%2C%20which%20may%20be%20unavailable%20in%20real%20scenarios.%20In%0Athis%20paper%2C%20we%20find%20that%20the%20poisoned%20samples%20and%20benign%20samples%20can%20be%0Adistinguished%20with%20prediction%20entropy.%20This%20inspires%20us%20to%20propose%20a%20novel%0Adual-network%20training%20framework%3A%20The%20Victim%20and%20The%20Beneficiary%20%28V%26B%29%2C%20which%0Aexploits%20a%20poisoned%20model%20to%20train%20a%20clean%20model%20without%20extra%20benign%20samples.%0AFirstly%2C%20we%20sacrifice%20the%20Victim%20network%20to%20be%20a%20powerful%20poisoned%20sample%0Adetector%20by%20training%20on%20suspicious%20samples.%20Secondly%2C%20we%20train%20the%20Beneficiary%0Anetwork%20on%20the%20credible%20samples%20selected%20by%20the%20Victim%20to%20inhibit%20backdoor%0Ainjection.%20Thirdly%2C%20a%20semi-supervised%20suppression%20strategy%20is%20adopted%20for%0Aerasing%20potential%20backdoors%20and%20improving%20model%20performance.%20Furthermore%2C%20to%0Abetter%20inhibit%20missed%20poisoned%20samples%2C%20we%20propose%20a%20strong%20data%20augmentation%0Amethod%2C%20AttentionMix%2C%20which%20works%20well%20with%20our%20proposed%20V%26B%20framework.%0AExtensive%20experiments%20on%20two%20widely%20used%20datasets%20against%206%20state-of-the-art%0Aattacks%20demonstrate%20that%20our%20framework%20is%20effective%20in%20preventing%20backdoor%0Ainjection%20and%20robust%20to%20various%20attacks%20while%20maintaining%20the%20performance%20on%0Abenign%20samples.%20Our%20code%20is%20available%20at%20https%3A//github.com/Zixuan-Zhu/VaB.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11265v2&entry.124074799=Read"},
{"title": "URDFormer: A Pipeline for Constructing Articulated Simulation\n  Environments from Real-World Images", "author": "Zoey Chen and Aaron Walsman and Marius Memmel and Kaichun Mo and Alex Fang and Karthikeya Vemuri and Alan Wu and Dieter Fox and Abhishek Gupta", "abstract": "  Constructing simulation scenes that are both visually and physically\nrealistic is a problem of practical interest in domains ranging from robotics\nto computer vision. This problem has become even more relevant as researchers\nwielding large data-hungry learning methods seek new sources of training data\nfor physical decision-making systems. However, building simulation models is\noften still done by hand. A graphic designer and a simulation engineer work\nwith predefined assets to construct rich scenes with realistic dynamic and\nkinematic properties. While this may scale to small numbers of scenes, to\nachieve the generalization properties that are required for data-driven robotic\ncontrol, we require a pipeline that is able to synthesize large numbers of\nrealistic scenes, complete with 'natural' kinematic and dynamic structures. To\nattack this problem, we develop models for inferring structure and generating\nsimulation scenes from natural images, allowing for scalable scene generation\nfrom web-scale datasets. To train these image-to-simulation models, we show how\ncontrollable text-to-image generative models can be used in generating paired\ntraining data that allows for modeling of the inverse problem, mapping from\nrealistic images back to complete scene models. We show how this paradigm\nallows us to build large datasets of scenes in simulation with semantic and\nphysical realism. We present an integrated end-to-end pipeline that generates\nsimulation scenes complete with articulated kinematic and dynamic structures\nfrom real-world images and use these for training robotic control policies. We\nthen robustly deploy in the real world for tasks like articulated object\nmanipulation. In doing so, our work provides both a pipeline for large-scale\ngeneration of simulation environments and an integrated system for training\nrobust robotic control policies in the resulting environments.\n", "link": "http://arxiv.org/abs/2405.11656v3", "date": "2024-05-31", "relevancy": 2.5118, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6776}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.618}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20URDFormer%3A%20A%20Pipeline%20for%20Constructing%20Articulated%20Simulation%0A%20%20Environments%20from%20Real-World%20Images&body=Title%3A%20URDFormer%3A%20A%20Pipeline%20for%20Constructing%20Articulated%20Simulation%0A%20%20Environments%20from%20Real-World%20Images%0AAuthor%3A%20Zoey%20Chen%20and%20Aaron%20Walsman%20and%20Marius%20Memmel%20and%20Kaichun%20Mo%20and%20Alex%20Fang%20and%20Karthikeya%20Vemuri%20and%20Alan%20Wu%20and%20Dieter%20Fox%20and%20Abhishek%20Gupta%0AAbstract%3A%20%20%20Constructing%20simulation%20scenes%20that%20are%20both%20visually%20and%20physically%0Arealistic%20is%20a%20problem%20of%20practical%20interest%20in%20domains%20ranging%20from%20robotics%0Ato%20computer%20vision.%20This%20problem%20has%20become%20even%20more%20relevant%20as%20researchers%0Awielding%20large%20data-hungry%20learning%20methods%20seek%20new%20sources%20of%20training%20data%0Afor%20physical%20decision-making%20systems.%20However%2C%20building%20simulation%20models%20is%0Aoften%20still%20done%20by%20hand.%20A%20graphic%20designer%20and%20a%20simulation%20engineer%20work%0Awith%20predefined%20assets%20to%20construct%20rich%20scenes%20with%20realistic%20dynamic%20and%0Akinematic%20properties.%20While%20this%20may%20scale%20to%20small%20numbers%20of%20scenes%2C%20to%0Aachieve%20the%20generalization%20properties%20that%20are%20required%20for%20data-driven%20robotic%0Acontrol%2C%20we%20require%20a%20pipeline%20that%20is%20able%20to%20synthesize%20large%20numbers%20of%0Arealistic%20scenes%2C%20complete%20with%20%27natural%27%20kinematic%20and%20dynamic%20structures.%20To%0Aattack%20this%20problem%2C%20we%20develop%20models%20for%20inferring%20structure%20and%20generating%0Asimulation%20scenes%20from%20natural%20images%2C%20allowing%20for%20scalable%20scene%20generation%0Afrom%20web-scale%20datasets.%20To%20train%20these%20image-to-simulation%20models%2C%20we%20show%20how%0Acontrollable%20text-to-image%20generative%20models%20can%20be%20used%20in%20generating%20paired%0Atraining%20data%20that%20allows%20for%20modeling%20of%20the%20inverse%20problem%2C%20mapping%20from%0Arealistic%20images%20back%20to%20complete%20scene%20models.%20We%20show%20how%20this%20paradigm%0Aallows%20us%20to%20build%20large%20datasets%20of%20scenes%20in%20simulation%20with%20semantic%20and%0Aphysical%20realism.%20We%20present%20an%20integrated%20end-to-end%20pipeline%20that%20generates%0Asimulation%20scenes%20complete%20with%20articulated%20kinematic%20and%20dynamic%20structures%0Afrom%20real-world%20images%20and%20use%20these%20for%20training%20robotic%20control%20policies.%20We%0Athen%20robustly%20deploy%20in%20the%20real%20world%20for%20tasks%20like%20articulated%20object%0Amanipulation.%20In%20doing%20so%2C%20our%20work%20provides%20both%20a%20pipeline%20for%20large-scale%0Ageneration%20of%20simulation%20environments%20and%20an%20integrated%20system%20for%20training%0Arobust%20robotic%20control%20policies%20in%20the%20resulting%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11656v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DURDFormer%253A%2520A%2520Pipeline%2520for%2520Constructing%2520Articulated%2520Simulation%250A%2520%2520Environments%2520from%2520Real-World%2520Images%26entry.906535625%3DZoey%2520Chen%2520and%2520Aaron%2520Walsman%2520and%2520Marius%2520Memmel%2520and%2520Kaichun%2520Mo%2520and%2520Alex%2520Fang%2520and%2520Karthikeya%2520Vemuri%2520and%2520Alan%2520Wu%2520and%2520Dieter%2520Fox%2520and%2520Abhishek%2520Gupta%26entry.1292438233%3D%2520%2520Constructing%2520simulation%2520scenes%2520that%2520are%2520both%2520visually%2520and%2520physically%250Arealistic%2520is%2520a%2520problem%2520of%2520practical%2520interest%2520in%2520domains%2520ranging%2520from%2520robotics%250Ato%2520computer%2520vision.%2520This%2520problem%2520has%2520become%2520even%2520more%2520relevant%2520as%2520researchers%250Awielding%2520large%2520data-hungry%2520learning%2520methods%2520seek%2520new%2520sources%2520of%2520training%2520data%250Afor%2520physical%2520decision-making%2520systems.%2520However%252C%2520building%2520simulation%2520models%2520is%250Aoften%2520still%2520done%2520by%2520hand.%2520A%2520graphic%2520designer%2520and%2520a%2520simulation%2520engineer%2520work%250Awith%2520predefined%2520assets%2520to%2520construct%2520rich%2520scenes%2520with%2520realistic%2520dynamic%2520and%250Akinematic%2520properties.%2520While%2520this%2520may%2520scale%2520to%2520small%2520numbers%2520of%2520scenes%252C%2520to%250Aachieve%2520the%2520generalization%2520properties%2520that%2520are%2520required%2520for%2520data-driven%2520robotic%250Acontrol%252C%2520we%2520require%2520a%2520pipeline%2520that%2520is%2520able%2520to%2520synthesize%2520large%2520numbers%2520of%250Arealistic%2520scenes%252C%2520complete%2520with%2520%2527natural%2527%2520kinematic%2520and%2520dynamic%2520structures.%2520To%250Aattack%2520this%2520problem%252C%2520we%2520develop%2520models%2520for%2520inferring%2520structure%2520and%2520generating%250Asimulation%2520scenes%2520from%2520natural%2520images%252C%2520allowing%2520for%2520scalable%2520scene%2520generation%250Afrom%2520web-scale%2520datasets.%2520To%2520train%2520these%2520image-to-simulation%2520models%252C%2520we%2520show%2520how%250Acontrollable%2520text-to-image%2520generative%2520models%2520can%2520be%2520used%2520in%2520generating%2520paired%250Atraining%2520data%2520that%2520allows%2520for%2520modeling%2520of%2520the%2520inverse%2520problem%252C%2520mapping%2520from%250Arealistic%2520images%2520back%2520to%2520complete%2520scene%2520models.%2520We%2520show%2520how%2520this%2520paradigm%250Aallows%2520us%2520to%2520build%2520large%2520datasets%2520of%2520scenes%2520in%2520simulation%2520with%2520semantic%2520and%250Aphysical%2520realism.%2520We%2520present%2520an%2520integrated%2520end-to-end%2520pipeline%2520that%2520generates%250Asimulation%2520scenes%2520complete%2520with%2520articulated%2520kinematic%2520and%2520dynamic%2520structures%250Afrom%2520real-world%2520images%2520and%2520use%2520these%2520for%2520training%2520robotic%2520control%2520policies.%2520We%250Athen%2520robustly%2520deploy%2520in%2520the%2520real%2520world%2520for%2520tasks%2520like%2520articulated%2520object%250Amanipulation.%2520In%2520doing%2520so%252C%2520our%2520work%2520provides%2520both%2520a%2520pipeline%2520for%2520large-scale%250Ageneration%2520of%2520simulation%2520environments%2520and%2520an%2520integrated%2520system%2520for%2520training%250Arobust%2520robotic%2520control%2520policies%2520in%2520the%2520resulting%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11656v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=URDFormer%3A%20A%20Pipeline%20for%20Constructing%20Articulated%20Simulation%0A%20%20Environments%20from%20Real-World%20Images&entry.906535625=Zoey%20Chen%20and%20Aaron%20Walsman%20and%20Marius%20Memmel%20and%20Kaichun%20Mo%20and%20Alex%20Fang%20and%20Karthikeya%20Vemuri%20and%20Alan%20Wu%20and%20Dieter%20Fox%20and%20Abhishek%20Gupta&entry.1292438233=%20%20Constructing%20simulation%20scenes%20that%20are%20both%20visually%20and%20physically%0Arealistic%20is%20a%20problem%20of%20practical%20interest%20in%20domains%20ranging%20from%20robotics%0Ato%20computer%20vision.%20This%20problem%20has%20become%20even%20more%20relevant%20as%20researchers%0Awielding%20large%20data-hungry%20learning%20methods%20seek%20new%20sources%20of%20training%20data%0Afor%20physical%20decision-making%20systems.%20However%2C%20building%20simulation%20models%20is%0Aoften%20still%20done%20by%20hand.%20A%20graphic%20designer%20and%20a%20simulation%20engineer%20work%0Awith%20predefined%20assets%20to%20construct%20rich%20scenes%20with%20realistic%20dynamic%20and%0Akinematic%20properties.%20While%20this%20may%20scale%20to%20small%20numbers%20of%20scenes%2C%20to%0Aachieve%20the%20generalization%20properties%20that%20are%20required%20for%20data-driven%20robotic%0Acontrol%2C%20we%20require%20a%20pipeline%20that%20is%20able%20to%20synthesize%20large%20numbers%20of%0Arealistic%20scenes%2C%20complete%20with%20%27natural%27%20kinematic%20and%20dynamic%20structures.%20To%0Aattack%20this%20problem%2C%20we%20develop%20models%20for%20inferring%20structure%20and%20generating%0Asimulation%20scenes%20from%20natural%20images%2C%20allowing%20for%20scalable%20scene%20generation%0Afrom%20web-scale%20datasets.%20To%20train%20these%20image-to-simulation%20models%2C%20we%20show%20how%0Acontrollable%20text-to-image%20generative%20models%20can%20be%20used%20in%20generating%20paired%0Atraining%20data%20that%20allows%20for%20modeling%20of%20the%20inverse%20problem%2C%20mapping%20from%0Arealistic%20images%20back%20to%20complete%20scene%20models.%20We%20show%20how%20this%20paradigm%0Aallows%20us%20to%20build%20large%20datasets%20of%20scenes%20in%20simulation%20with%20semantic%20and%0Aphysical%20realism.%20We%20present%20an%20integrated%20end-to-end%20pipeline%20that%20generates%0Asimulation%20scenes%20complete%20with%20articulated%20kinematic%20and%20dynamic%20structures%0Afrom%20real-world%20images%20and%20use%20these%20for%20training%20robotic%20control%20policies.%20We%0Athen%20robustly%20deploy%20in%20the%20real%20world%20for%20tasks%20like%20articulated%20object%0Amanipulation.%20In%20doing%20so%2C%20our%20work%20provides%20both%20a%20pipeline%20for%20large-scale%0Ageneration%20of%20simulation%20environments%20and%20an%20integrated%20system%20for%20training%0Arobust%20robotic%20control%20policies%20in%20the%20resulting%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11656v3&entry.124074799=Read"},
{"title": "MegActor: Harness the Power of Raw Video for Vivid Portrait Animation", "author": "Shurong Yang and Huadong Li and Juhao Wu and Minhao Jing and Linze Li and Renhe Ji and Jiajun Liang and Haoqiang Fan", "abstract": "  Despite raw driving videos contain richer information on facial expressions\nthan intermediate representations such as landmarks in the field of portrait\nanimation, they are seldom the subject of research. This is due to two\nchallenges inherent in portrait animation driven with raw videos: 1)\nsignificant identity leakage; 2) Irrelevant background and facial details such\nas wrinkles degrade performance. To harnesses the power of the raw videos for\nvivid portrait animation, we proposed a pioneering conditional diffusion model\nnamed as MegActor. First, we introduced a synthetic data generation framework\nfor creating videos with consistent motion and expressions but inconsistent IDs\nto mitigate the issue of ID leakage. Second, we segmented the foreground and\nbackground of the reference image and employed CLIP to encode the background\ndetails. This encoded information is then integrated into the network via a\ntext embedding module, thereby ensuring the stability of the background.\nFinally, we further style transfer the appearance of the reference image to the\ndriving video to eliminate the influence of facial details in the driving\nvideos. Our final model was trained solely on public datasets, achieving\nresults comparable to commercial models. We hope this will help the open-source\ncommunity.The code is available at\nhttps://github.com/megvii-research/MegFaceAnimate.\n", "link": "http://arxiv.org/abs/2405.20851v1", "date": "2024-05-31", "relevancy": 2.5071, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6461}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6148}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegActor%3A%20Harness%20the%20Power%20of%20Raw%20Video%20for%20Vivid%20Portrait%20Animation&body=Title%3A%20MegActor%3A%20Harness%20the%20Power%20of%20Raw%20Video%20for%20Vivid%20Portrait%20Animation%0AAuthor%3A%20Shurong%20Yang%20and%20Huadong%20Li%20and%20Juhao%20Wu%20and%20Minhao%20Jing%20and%20Linze%20Li%20and%20Renhe%20Ji%20and%20Jiajun%20Liang%20and%20Haoqiang%20Fan%0AAbstract%3A%20%20%20Despite%20raw%20driving%20videos%20contain%20richer%20information%20on%20facial%20expressions%0Athan%20intermediate%20representations%20such%20as%20landmarks%20in%20the%20field%20of%20portrait%0Aanimation%2C%20they%20are%20seldom%20the%20subject%20of%20research.%20This%20is%20due%20to%20two%0Achallenges%20inherent%20in%20portrait%20animation%20driven%20with%20raw%20videos%3A%201%29%0Asignificant%20identity%20leakage%3B%202%29%20Irrelevant%20background%20and%20facial%20details%20such%0Aas%20wrinkles%20degrade%20performance.%20To%20harnesses%20the%20power%20of%20the%20raw%20videos%20for%0Avivid%20portrait%20animation%2C%20we%20proposed%20a%20pioneering%20conditional%20diffusion%20model%0Anamed%20as%20MegActor.%20First%2C%20we%20introduced%20a%20synthetic%20data%20generation%20framework%0Afor%20creating%20videos%20with%20consistent%20motion%20and%20expressions%20but%20inconsistent%20IDs%0Ato%20mitigate%20the%20issue%20of%20ID%20leakage.%20Second%2C%20we%20segmented%20the%20foreground%20and%0Abackground%20of%20the%20reference%20image%20and%20employed%20CLIP%20to%20encode%20the%20background%0Adetails.%20This%20encoded%20information%20is%20then%20integrated%20into%20the%20network%20via%20a%0Atext%20embedding%20module%2C%20thereby%20ensuring%20the%20stability%20of%20the%20background.%0AFinally%2C%20we%20further%20style%20transfer%20the%20appearance%20of%20the%20reference%20image%20to%20the%0Adriving%20video%20to%20eliminate%20the%20influence%20of%20facial%20details%20in%20the%20driving%0Avideos.%20Our%20final%20model%20was%20trained%20solely%20on%20public%20datasets%2C%20achieving%0Aresults%20comparable%20to%20commercial%20models.%20We%20hope%20this%20will%20help%20the%20open-source%0Acommunity.The%20code%20is%20available%20at%0Ahttps%3A//github.com/megvii-research/MegFaceAnimate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegActor%253A%2520Harness%2520the%2520Power%2520of%2520Raw%2520Video%2520for%2520Vivid%2520Portrait%2520Animation%26entry.906535625%3DShurong%2520Yang%2520and%2520Huadong%2520Li%2520and%2520Juhao%2520Wu%2520and%2520Minhao%2520Jing%2520and%2520Linze%2520Li%2520and%2520Renhe%2520Ji%2520and%2520Jiajun%2520Liang%2520and%2520Haoqiang%2520Fan%26entry.1292438233%3D%2520%2520Despite%2520raw%2520driving%2520videos%2520contain%2520richer%2520information%2520on%2520facial%2520expressions%250Athan%2520intermediate%2520representations%2520such%2520as%2520landmarks%2520in%2520the%2520field%2520of%2520portrait%250Aanimation%252C%2520they%2520are%2520seldom%2520the%2520subject%2520of%2520research.%2520This%2520is%2520due%2520to%2520two%250Achallenges%2520inherent%2520in%2520portrait%2520animation%2520driven%2520with%2520raw%2520videos%253A%25201%2529%250Asignificant%2520identity%2520leakage%253B%25202%2529%2520Irrelevant%2520background%2520and%2520facial%2520details%2520such%250Aas%2520wrinkles%2520degrade%2520performance.%2520To%2520harnesses%2520the%2520power%2520of%2520the%2520raw%2520videos%2520for%250Avivid%2520portrait%2520animation%252C%2520we%2520proposed%2520a%2520pioneering%2520conditional%2520diffusion%2520model%250Anamed%2520as%2520MegActor.%2520First%252C%2520we%2520introduced%2520a%2520synthetic%2520data%2520generation%2520framework%250Afor%2520creating%2520videos%2520with%2520consistent%2520motion%2520and%2520expressions%2520but%2520inconsistent%2520IDs%250Ato%2520mitigate%2520the%2520issue%2520of%2520ID%2520leakage.%2520Second%252C%2520we%2520segmented%2520the%2520foreground%2520and%250Abackground%2520of%2520the%2520reference%2520image%2520and%2520employed%2520CLIP%2520to%2520encode%2520the%2520background%250Adetails.%2520This%2520encoded%2520information%2520is%2520then%2520integrated%2520into%2520the%2520network%2520via%2520a%250Atext%2520embedding%2520module%252C%2520thereby%2520ensuring%2520the%2520stability%2520of%2520the%2520background.%250AFinally%252C%2520we%2520further%2520style%2520transfer%2520the%2520appearance%2520of%2520the%2520reference%2520image%2520to%2520the%250Adriving%2520video%2520to%2520eliminate%2520the%2520influence%2520of%2520facial%2520details%2520in%2520the%2520driving%250Avideos.%2520Our%2520final%2520model%2520was%2520trained%2520solely%2520on%2520public%2520datasets%252C%2520achieving%250Aresults%2520comparable%2520to%2520commercial%2520models.%2520We%2520hope%2520this%2520will%2520help%2520the%2520open-source%250Acommunity.The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/megvii-research/MegFaceAnimate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegActor%3A%20Harness%20the%20Power%20of%20Raw%20Video%20for%20Vivid%20Portrait%20Animation&entry.906535625=Shurong%20Yang%20and%20Huadong%20Li%20and%20Juhao%20Wu%20and%20Minhao%20Jing%20and%20Linze%20Li%20and%20Renhe%20Ji%20and%20Jiajun%20Liang%20and%20Haoqiang%20Fan&entry.1292438233=%20%20Despite%20raw%20driving%20videos%20contain%20richer%20information%20on%20facial%20expressions%0Athan%20intermediate%20representations%20such%20as%20landmarks%20in%20the%20field%20of%20portrait%0Aanimation%2C%20they%20are%20seldom%20the%20subject%20of%20research.%20This%20is%20due%20to%20two%0Achallenges%20inherent%20in%20portrait%20animation%20driven%20with%20raw%20videos%3A%201%29%0Asignificant%20identity%20leakage%3B%202%29%20Irrelevant%20background%20and%20facial%20details%20such%0Aas%20wrinkles%20degrade%20performance.%20To%20harnesses%20the%20power%20of%20the%20raw%20videos%20for%0Avivid%20portrait%20animation%2C%20we%20proposed%20a%20pioneering%20conditional%20diffusion%20model%0Anamed%20as%20MegActor.%20First%2C%20we%20introduced%20a%20synthetic%20data%20generation%20framework%0Afor%20creating%20videos%20with%20consistent%20motion%20and%20expressions%20but%20inconsistent%20IDs%0Ato%20mitigate%20the%20issue%20of%20ID%20leakage.%20Second%2C%20we%20segmented%20the%20foreground%20and%0Abackground%20of%20the%20reference%20image%20and%20employed%20CLIP%20to%20encode%20the%20background%0Adetails.%20This%20encoded%20information%20is%20then%20integrated%20into%20the%20network%20via%20a%0Atext%20embedding%20module%2C%20thereby%20ensuring%20the%20stability%20of%20the%20background.%0AFinally%2C%20we%20further%20style%20transfer%20the%20appearance%20of%20the%20reference%20image%20to%20the%0Adriving%20video%20to%20eliminate%20the%20influence%20of%20facial%20details%20in%20the%20driving%0Avideos.%20Our%20final%20model%20was%20trained%20solely%20on%20public%20datasets%2C%20achieving%0Aresults%20comparable%20to%20commercial%20models.%20We%20hope%20this%20will%20help%20the%20open-source%0Acommunity.The%20code%20is%20available%20at%0Ahttps%3A//github.com/megvii-research/MegFaceAnimate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20851v1&entry.124074799=Read"},
{"title": "SelfGNN: Self-Supervised Graph Neural Networks for Sequential\n  Recommendation", "author": "Yuxi Liu and Lianghao Xia and Chao Huang", "abstract": "  Sequential recommendation effectively addresses information overload by\nmodeling users' temporal and sequential interaction patterns. To overcome the\nlimitations of supervision signals, recent approaches have adopted\nself-supervised learning techniques in recommender systems. However, there are\nstill two critical challenges that remain unsolved. Firstly, existing\nsequential models primarily focus on long-term modeling of individual\ninteraction sequences, overlooking the valuable short-term collaborative\nrelationships among the behaviors of different users. Secondly, real-world data\noften contain noise, particularly in users' short-term behaviors, which can\narise from temporary intents or misclicks. Such noise negatively impacts the\naccuracy of both graph and sequence models, further complicating the modeling\nprocess. To address these challenges, we propose a novel framework called\nSelf-Supervised Graph Neural Network (SelfGNN) for sequential recommendation.\nThe SelfGNN framework encodes short-term graphs based on time intervals and\nutilizes Graph Neural Networks (GNNs) to learn short-term collaborative\nrelationships. It captures long-term user and item representations at multiple\ngranularity levels through interval fusion and dynamic behavior modeling.\nImportantly, our personalized self-augmented learning structure enhances model\nrobustness by mitigating noise in short-term graphs based on long-term user\ninterests and personal stability. Extensive experiments conducted on four\nreal-world datasets demonstrate that SelfGNN outperforms various\nstate-of-the-art baselines. Our model implementation codes are available at\nhttps://github.com/HKUDS/SelfGNN.\n", "link": "http://arxiv.org/abs/2405.20878v1", "date": "2024-05-31", "relevancy": 2.5003, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5131}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4975}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfGNN%3A%20Self-Supervised%20Graph%20Neural%20Networks%20for%20Sequential%0A%20%20Recommendation&body=Title%3A%20SelfGNN%3A%20Self-Supervised%20Graph%20Neural%20Networks%20for%20Sequential%0A%20%20Recommendation%0AAuthor%3A%20Yuxi%20Liu%20and%20Lianghao%20Xia%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Sequential%20recommendation%20effectively%20addresses%20information%20overload%20by%0Amodeling%20users%27%20temporal%20and%20sequential%20interaction%20patterns.%20To%20overcome%20the%0Alimitations%20of%20supervision%20signals%2C%20recent%20approaches%20have%20adopted%0Aself-supervised%20learning%20techniques%20in%20recommender%20systems.%20However%2C%20there%20are%0Astill%20two%20critical%20challenges%20that%20remain%20unsolved.%20Firstly%2C%20existing%0Asequential%20models%20primarily%20focus%20on%20long-term%20modeling%20of%20individual%0Ainteraction%20sequences%2C%20overlooking%20the%20valuable%20short-term%20collaborative%0Arelationships%20among%20the%20behaviors%20of%20different%20users.%20Secondly%2C%20real-world%20data%0Aoften%20contain%20noise%2C%20particularly%20in%20users%27%20short-term%20behaviors%2C%20which%20can%0Aarise%20from%20temporary%20intents%20or%20misclicks.%20Such%20noise%20negatively%20impacts%20the%0Aaccuracy%20of%20both%20graph%20and%20sequence%20models%2C%20further%20complicating%20the%20modeling%0Aprocess.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20framework%20called%0ASelf-Supervised%20Graph%20Neural%20Network%20%28SelfGNN%29%20for%20sequential%20recommendation.%0AThe%20SelfGNN%20framework%20encodes%20short-term%20graphs%20based%20on%20time%20intervals%20and%0Autilizes%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20learn%20short-term%20collaborative%0Arelationships.%20It%20captures%20long-term%20user%20and%20item%20representations%20at%20multiple%0Agranularity%20levels%20through%20interval%20fusion%20and%20dynamic%20behavior%20modeling.%0AImportantly%2C%20our%20personalized%20self-augmented%20learning%20structure%20enhances%20model%0Arobustness%20by%20mitigating%20noise%20in%20short-term%20graphs%20based%20on%20long-term%20user%0Ainterests%20and%20personal%20stability.%20Extensive%20experiments%20conducted%20on%20four%0Areal-world%20datasets%20demonstrate%20that%20SelfGNN%20outperforms%20various%0Astate-of-the-art%20baselines.%20Our%20model%20implementation%20codes%20are%20available%20at%0Ahttps%3A//github.com/HKUDS/SelfGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfGNN%253A%2520Self-Supervised%2520Graph%2520Neural%2520Networks%2520for%2520Sequential%250A%2520%2520Recommendation%26entry.906535625%3DYuxi%2520Liu%2520and%2520Lianghao%2520Xia%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Sequential%2520recommendation%2520effectively%2520addresses%2520information%2520overload%2520by%250Amodeling%2520users%2527%2520temporal%2520and%2520sequential%2520interaction%2520patterns.%2520To%2520overcome%2520the%250Alimitations%2520of%2520supervision%2520signals%252C%2520recent%2520approaches%2520have%2520adopted%250Aself-supervised%2520learning%2520techniques%2520in%2520recommender%2520systems.%2520However%252C%2520there%2520are%250Astill%2520two%2520critical%2520challenges%2520that%2520remain%2520unsolved.%2520Firstly%252C%2520existing%250Asequential%2520models%2520primarily%2520focus%2520on%2520long-term%2520modeling%2520of%2520individual%250Ainteraction%2520sequences%252C%2520overlooking%2520the%2520valuable%2520short-term%2520collaborative%250Arelationships%2520among%2520the%2520behaviors%2520of%2520different%2520users.%2520Secondly%252C%2520real-world%2520data%250Aoften%2520contain%2520noise%252C%2520particularly%2520in%2520users%2527%2520short-term%2520behaviors%252C%2520which%2520can%250Aarise%2520from%2520temporary%2520intents%2520or%2520misclicks.%2520Such%2520noise%2520negatively%2520impacts%2520the%250Aaccuracy%2520of%2520both%2520graph%2520and%2520sequence%2520models%252C%2520further%2520complicating%2520the%2520modeling%250Aprocess.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%250ASelf-Supervised%2520Graph%2520Neural%2520Network%2520%2528SelfGNN%2529%2520for%2520sequential%2520recommendation.%250AThe%2520SelfGNN%2520framework%2520encodes%2520short-term%2520graphs%2520based%2520on%2520time%2520intervals%2520and%250Autilizes%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520to%2520learn%2520short-term%2520collaborative%250Arelationships.%2520It%2520captures%2520long-term%2520user%2520and%2520item%2520representations%2520at%2520multiple%250Agranularity%2520levels%2520through%2520interval%2520fusion%2520and%2520dynamic%2520behavior%2520modeling.%250AImportantly%252C%2520our%2520personalized%2520self-augmented%2520learning%2520structure%2520enhances%2520model%250Arobustness%2520by%2520mitigating%2520noise%2520in%2520short-term%2520graphs%2520based%2520on%2520long-term%2520user%250Ainterests%2520and%2520personal%2520stability.%2520Extensive%2520experiments%2520conducted%2520on%2520four%250Areal-world%2520datasets%2520demonstrate%2520that%2520SelfGNN%2520outperforms%2520various%250Astate-of-the-art%2520baselines.%2520Our%2520model%2520implementation%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/HKUDS/SelfGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfGNN%3A%20Self-Supervised%20Graph%20Neural%20Networks%20for%20Sequential%0A%20%20Recommendation&entry.906535625=Yuxi%20Liu%20and%20Lianghao%20Xia%20and%20Chao%20Huang&entry.1292438233=%20%20Sequential%20recommendation%20effectively%20addresses%20information%20overload%20by%0Amodeling%20users%27%20temporal%20and%20sequential%20interaction%20patterns.%20To%20overcome%20the%0Alimitations%20of%20supervision%20signals%2C%20recent%20approaches%20have%20adopted%0Aself-supervised%20learning%20techniques%20in%20recommender%20systems.%20However%2C%20there%20are%0Astill%20two%20critical%20challenges%20that%20remain%20unsolved.%20Firstly%2C%20existing%0Asequential%20models%20primarily%20focus%20on%20long-term%20modeling%20of%20individual%0Ainteraction%20sequences%2C%20overlooking%20the%20valuable%20short-term%20collaborative%0Arelationships%20among%20the%20behaviors%20of%20different%20users.%20Secondly%2C%20real-world%20data%0Aoften%20contain%20noise%2C%20particularly%20in%20users%27%20short-term%20behaviors%2C%20which%20can%0Aarise%20from%20temporary%20intents%20or%20misclicks.%20Such%20noise%20negatively%20impacts%20the%0Aaccuracy%20of%20both%20graph%20and%20sequence%20models%2C%20further%20complicating%20the%20modeling%0Aprocess.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20framework%20called%0ASelf-Supervised%20Graph%20Neural%20Network%20%28SelfGNN%29%20for%20sequential%20recommendation.%0AThe%20SelfGNN%20framework%20encodes%20short-term%20graphs%20based%20on%20time%20intervals%20and%0Autilizes%20Graph%20Neural%20Networks%20%28GNNs%29%20to%20learn%20short-term%20collaborative%0Arelationships.%20It%20captures%20long-term%20user%20and%20item%20representations%20at%20multiple%0Agranularity%20levels%20through%20interval%20fusion%20and%20dynamic%20behavior%20modeling.%0AImportantly%2C%20our%20personalized%20self-augmented%20learning%20structure%20enhances%20model%0Arobustness%20by%20mitigating%20noise%20in%20short-term%20graphs%20based%20on%20long-term%20user%0Ainterests%20and%20personal%20stability.%20Extensive%20experiments%20conducted%20on%20four%0Areal-world%20datasets%20demonstrate%20that%20SelfGNN%20outperforms%20various%0Astate-of-the-art%20baselines.%20Our%20model%20implementation%20codes%20are%20available%20at%0Ahttps%3A//github.com/HKUDS/SelfGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20878v1&entry.124074799=Read"},
{"title": "Intersectional Unfairness Discovery", "author": "Gezheng Xu and Qi Chen and Charles Ling and Boyu Wang and Changjian Shui", "abstract": "  AI systems have been shown to produce unfair results for certain subgroups of\npopulation, highlighting the need to understand bias on certain sensitive\nattributes. Current research often falls short, primarily focusing on the\nsubgroups characterized by a single sensitive attribute, while neglecting the\nnature of intersectional fairness of multiple sensitive attributes. This paper\nfocuses on its one fundamental aspect by discovering diverse high-bias\nsubgroups under intersectional sensitive attributes. Specifically, we propose a\nBias-Guided Generative Network (BGGN). By treating each bias value as a reward,\nBGGN efficiently generates high-bias intersectional sensitive attributes.\nExperiments on real-world text and image datasets demonstrate a diverse and\nefficient discovery of BGGN. To further evaluate the generated unseen but\npossible unfair intersectional sensitive attributes, we formulate them as\nprompts and use modern generative AI to produce new texts and images. The\nresults of frequently generating biased data provides new insights of\ndiscovering potential unfairness in popular modern generative AI systems.\nWarning: This paper contains generative examples that are offensive in nature.\n", "link": "http://arxiv.org/abs/2405.20790v1", "date": "2024-05-31", "relevancy": 2.4899, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5143}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5049}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intersectional%20Unfairness%20Discovery&body=Title%3A%20Intersectional%20Unfairness%20Discovery%0AAuthor%3A%20Gezheng%20Xu%20and%20Qi%20Chen%20and%20Charles%20Ling%20and%20Boyu%20Wang%20and%20Changjian%20Shui%0AAbstract%3A%20%20%20AI%20systems%20have%20been%20shown%20to%20produce%20unfair%20results%20for%20certain%20subgroups%20of%0Apopulation%2C%20highlighting%20the%20need%20to%20understand%20bias%20on%20certain%20sensitive%0Aattributes.%20Current%20research%20often%20falls%20short%2C%20primarily%20focusing%20on%20the%0Asubgroups%20characterized%20by%20a%20single%20sensitive%20attribute%2C%20while%20neglecting%20the%0Anature%20of%20intersectional%20fairness%20of%20multiple%20sensitive%20attributes.%20This%20paper%0Afocuses%20on%20its%20one%20fundamental%20aspect%20by%20discovering%20diverse%20high-bias%0Asubgroups%20under%20intersectional%20sensitive%20attributes.%20Specifically%2C%20we%20propose%20a%0ABias-Guided%20Generative%20Network%20%28BGGN%29.%20By%20treating%20each%20bias%20value%20as%20a%20reward%2C%0ABGGN%20efficiently%20generates%20high-bias%20intersectional%20sensitive%20attributes.%0AExperiments%20on%20real-world%20text%20and%20image%20datasets%20demonstrate%20a%20diverse%20and%0Aefficient%20discovery%20of%20BGGN.%20To%20further%20evaluate%20the%20generated%20unseen%20but%0Apossible%20unfair%20intersectional%20sensitive%20attributes%2C%20we%20formulate%20them%20as%0Aprompts%20and%20use%20modern%20generative%20AI%20to%20produce%20new%20texts%20and%20images.%20The%0Aresults%20of%20frequently%20generating%20biased%20data%20provides%20new%20insights%20of%0Adiscovering%20potential%20unfairness%20in%20popular%20modern%20generative%20AI%20systems.%0AWarning%3A%20This%20paper%20contains%20generative%20examples%20that%20are%20offensive%20in%20nature.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntersectional%2520Unfairness%2520Discovery%26entry.906535625%3DGezheng%2520Xu%2520and%2520Qi%2520Chen%2520and%2520Charles%2520Ling%2520and%2520Boyu%2520Wang%2520and%2520Changjian%2520Shui%26entry.1292438233%3D%2520%2520AI%2520systems%2520have%2520been%2520shown%2520to%2520produce%2520unfair%2520results%2520for%2520certain%2520subgroups%2520of%250Apopulation%252C%2520highlighting%2520the%2520need%2520to%2520understand%2520bias%2520on%2520certain%2520sensitive%250Aattributes.%2520Current%2520research%2520often%2520falls%2520short%252C%2520primarily%2520focusing%2520on%2520the%250Asubgroups%2520characterized%2520by%2520a%2520single%2520sensitive%2520attribute%252C%2520while%2520neglecting%2520the%250Anature%2520of%2520intersectional%2520fairness%2520of%2520multiple%2520sensitive%2520attributes.%2520This%2520paper%250Afocuses%2520on%2520its%2520one%2520fundamental%2520aspect%2520by%2520discovering%2520diverse%2520high-bias%250Asubgroups%2520under%2520intersectional%2520sensitive%2520attributes.%2520Specifically%252C%2520we%2520propose%2520a%250ABias-Guided%2520Generative%2520Network%2520%2528BGGN%2529.%2520By%2520treating%2520each%2520bias%2520value%2520as%2520a%2520reward%252C%250ABGGN%2520efficiently%2520generates%2520high-bias%2520intersectional%2520sensitive%2520attributes.%250AExperiments%2520on%2520real-world%2520text%2520and%2520image%2520datasets%2520demonstrate%2520a%2520diverse%2520and%250Aefficient%2520discovery%2520of%2520BGGN.%2520To%2520further%2520evaluate%2520the%2520generated%2520unseen%2520but%250Apossible%2520unfair%2520intersectional%2520sensitive%2520attributes%252C%2520we%2520formulate%2520them%2520as%250Aprompts%2520and%2520use%2520modern%2520generative%2520AI%2520to%2520produce%2520new%2520texts%2520and%2520images.%2520The%250Aresults%2520of%2520frequently%2520generating%2520biased%2520data%2520provides%2520new%2520insights%2520of%250Adiscovering%2520potential%2520unfairness%2520in%2520popular%2520modern%2520generative%2520AI%2520systems.%250AWarning%253A%2520This%2520paper%2520contains%2520generative%2520examples%2520that%2520are%2520offensive%2520in%2520nature.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intersectional%20Unfairness%20Discovery&entry.906535625=Gezheng%20Xu%20and%20Qi%20Chen%20and%20Charles%20Ling%20and%20Boyu%20Wang%20and%20Changjian%20Shui&entry.1292438233=%20%20AI%20systems%20have%20been%20shown%20to%20produce%20unfair%20results%20for%20certain%20subgroups%20of%0Apopulation%2C%20highlighting%20the%20need%20to%20understand%20bias%20on%20certain%20sensitive%0Aattributes.%20Current%20research%20often%20falls%20short%2C%20primarily%20focusing%20on%20the%0Asubgroups%20characterized%20by%20a%20single%20sensitive%20attribute%2C%20while%20neglecting%20the%0Anature%20of%20intersectional%20fairness%20of%20multiple%20sensitive%20attributes.%20This%20paper%0Afocuses%20on%20its%20one%20fundamental%20aspect%20by%20discovering%20diverse%20high-bias%0Asubgroups%20under%20intersectional%20sensitive%20attributes.%20Specifically%2C%20we%20propose%20a%0ABias-Guided%20Generative%20Network%20%28BGGN%29.%20By%20treating%20each%20bias%20value%20as%20a%20reward%2C%0ABGGN%20efficiently%20generates%20high-bias%20intersectional%20sensitive%20attributes.%0AExperiments%20on%20real-world%20text%20and%20image%20datasets%20demonstrate%20a%20diverse%20and%0Aefficient%20discovery%20of%20BGGN.%20To%20further%20evaluate%20the%20generated%20unseen%20but%0Apossible%20unfair%20intersectional%20sensitive%20attributes%2C%20we%20formulate%20them%20as%0Aprompts%20and%20use%20modern%20generative%20AI%20to%20produce%20new%20texts%20and%20images.%20The%0Aresults%20of%20frequently%20generating%20biased%20data%20provides%20new%20insights%20of%0Adiscovering%20potential%20unfairness%20in%20popular%20modern%20generative%20AI%20systems.%0AWarning%3A%20This%20paper%20contains%20generative%20examples%20that%20are%20offensive%20in%20nature.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20790v1&entry.124074799=Read"},
{"title": "Equivariant Deep Weight Space Alignment", "author": "Aviv Navon and Aviv Shamsian and Ethan Fetaya and Gal Chechik and Nadav Dym and Haggai Maron", "abstract": "  Permutation symmetries of deep networks make basic operations like model\nmerging and similarity estimation challenging. In many cases, aligning the\nweights of the networks, i.e., finding optimal permutations between their\nweights, is necessary. Unfortunately, weight alignment is an NP-hard problem.\nPrior research has mainly focused on solving relaxed versions of the alignment\nproblem, leading to either time-consuming methods or sub-optimal solutions. To\naccelerate the alignment process and improve its quality, we propose a novel\nframework aimed at learning to solve the weight alignment problem, which we\nname Deep-Align. To that end, we first prove that weight alignment adheres to\ntwo fundamental symmetries and then, propose a deep architecture that respects\nthese symmetries. Notably, our framework does not require any labeled data. We\nprovide a theoretical analysis of our approach and evaluate Deep-Align on\nseveral types of network architectures and learning setups. Our experimental\nresults indicate that a feed-forward pass with Deep-Align produces better or\nequivalent alignments compared to those produced by current optimization\nalgorithms. Additionally, our alignments can be used as an effective\ninitialization for other methods, leading to improved solutions with a\nsignificant speedup in convergence.\n", "link": "http://arxiv.org/abs/2310.13397v3", "date": "2024-05-31", "relevancy": 2.4813, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5287}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4829}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Deep%20Weight%20Space%20Alignment&body=Title%3A%20Equivariant%20Deep%20Weight%20Space%20Alignment%0AAuthor%3A%20Aviv%20Navon%20and%20Aviv%20Shamsian%20and%20Ethan%20Fetaya%20and%20Gal%20Chechik%20and%20Nadav%20Dym%20and%20Haggai%20Maron%0AAbstract%3A%20%20%20Permutation%20symmetries%20of%20deep%20networks%20make%20basic%20operations%20like%20model%0Amerging%20and%20similarity%20estimation%20challenging.%20In%20many%20cases%2C%20aligning%20the%0Aweights%20of%20the%20networks%2C%20i.e.%2C%20finding%20optimal%20permutations%20between%20their%0Aweights%2C%20is%20necessary.%20Unfortunately%2C%20weight%20alignment%20is%20an%20NP-hard%20problem.%0APrior%20research%20has%20mainly%20focused%20on%20solving%20relaxed%20versions%20of%20the%20alignment%0Aproblem%2C%20leading%20to%20either%20time-consuming%20methods%20or%20sub-optimal%20solutions.%20To%0Aaccelerate%20the%20alignment%20process%20and%20improve%20its%20quality%2C%20we%20propose%20a%20novel%0Aframework%20aimed%20at%20learning%20to%20solve%20the%20weight%20alignment%20problem%2C%20which%20we%0Aname%20Deep-Align.%20To%20that%20end%2C%20we%20first%20prove%20that%20weight%20alignment%20adheres%20to%0Atwo%20fundamental%20symmetries%20and%20then%2C%20propose%20a%20deep%20architecture%20that%20respects%0Athese%20symmetries.%20Notably%2C%20our%20framework%20does%20not%20require%20any%20labeled%20data.%20We%0Aprovide%20a%20theoretical%20analysis%20of%20our%20approach%20and%20evaluate%20Deep-Align%20on%0Aseveral%20types%20of%20network%20architectures%20and%20learning%20setups.%20Our%20experimental%0Aresults%20indicate%20that%20a%20feed-forward%20pass%20with%20Deep-Align%20produces%20better%20or%0Aequivalent%20alignments%20compared%20to%20those%20produced%20by%20current%20optimization%0Aalgorithms.%20Additionally%2C%20our%20alignments%20can%20be%20used%20as%20an%20effective%0Ainitialization%20for%20other%20methods%2C%20leading%20to%20improved%20solutions%20with%20a%0Asignificant%20speedup%20in%20convergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13397v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Deep%2520Weight%2520Space%2520Alignment%26entry.906535625%3DAviv%2520Navon%2520and%2520Aviv%2520Shamsian%2520and%2520Ethan%2520Fetaya%2520and%2520Gal%2520Chechik%2520and%2520Nadav%2520Dym%2520and%2520Haggai%2520Maron%26entry.1292438233%3D%2520%2520Permutation%2520symmetries%2520of%2520deep%2520networks%2520make%2520basic%2520operations%2520like%2520model%250Amerging%2520and%2520similarity%2520estimation%2520challenging.%2520In%2520many%2520cases%252C%2520aligning%2520the%250Aweights%2520of%2520the%2520networks%252C%2520i.e.%252C%2520finding%2520optimal%2520permutations%2520between%2520their%250Aweights%252C%2520is%2520necessary.%2520Unfortunately%252C%2520weight%2520alignment%2520is%2520an%2520NP-hard%2520problem.%250APrior%2520research%2520has%2520mainly%2520focused%2520on%2520solving%2520relaxed%2520versions%2520of%2520the%2520alignment%250Aproblem%252C%2520leading%2520to%2520either%2520time-consuming%2520methods%2520or%2520sub-optimal%2520solutions.%2520To%250Aaccelerate%2520the%2520alignment%2520process%2520and%2520improve%2520its%2520quality%252C%2520we%2520propose%2520a%2520novel%250Aframework%2520aimed%2520at%2520learning%2520to%2520solve%2520the%2520weight%2520alignment%2520problem%252C%2520which%2520we%250Aname%2520Deep-Align.%2520To%2520that%2520end%252C%2520we%2520first%2520prove%2520that%2520weight%2520alignment%2520adheres%2520to%250Atwo%2520fundamental%2520symmetries%2520and%2520then%252C%2520propose%2520a%2520deep%2520architecture%2520that%2520respects%250Athese%2520symmetries.%2520Notably%252C%2520our%2520framework%2520does%2520not%2520require%2520any%2520labeled%2520data.%2520We%250Aprovide%2520a%2520theoretical%2520analysis%2520of%2520our%2520approach%2520and%2520evaluate%2520Deep-Align%2520on%250Aseveral%2520types%2520of%2520network%2520architectures%2520and%2520learning%2520setups.%2520Our%2520experimental%250Aresults%2520indicate%2520that%2520a%2520feed-forward%2520pass%2520with%2520Deep-Align%2520produces%2520better%2520or%250Aequivalent%2520alignments%2520compared%2520to%2520those%2520produced%2520by%2520current%2520optimization%250Aalgorithms.%2520Additionally%252C%2520our%2520alignments%2520can%2520be%2520used%2520as%2520an%2520effective%250Ainitialization%2520for%2520other%2520methods%252C%2520leading%2520to%2520improved%2520solutions%2520with%2520a%250Asignificant%2520speedup%2520in%2520convergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.13397v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Deep%20Weight%20Space%20Alignment&entry.906535625=Aviv%20Navon%20and%20Aviv%20Shamsian%20and%20Ethan%20Fetaya%20and%20Gal%20Chechik%20and%20Nadav%20Dym%20and%20Haggai%20Maron&entry.1292438233=%20%20Permutation%20symmetries%20of%20deep%20networks%20make%20basic%20operations%20like%20model%0Amerging%20and%20similarity%20estimation%20challenging.%20In%20many%20cases%2C%20aligning%20the%0Aweights%20of%20the%20networks%2C%20i.e.%2C%20finding%20optimal%20permutations%20between%20their%0Aweights%2C%20is%20necessary.%20Unfortunately%2C%20weight%20alignment%20is%20an%20NP-hard%20problem.%0APrior%20research%20has%20mainly%20focused%20on%20solving%20relaxed%20versions%20of%20the%20alignment%0Aproblem%2C%20leading%20to%20either%20time-consuming%20methods%20or%20sub-optimal%20solutions.%20To%0Aaccelerate%20the%20alignment%20process%20and%20improve%20its%20quality%2C%20we%20propose%20a%20novel%0Aframework%20aimed%20at%20learning%20to%20solve%20the%20weight%20alignment%20problem%2C%20which%20we%0Aname%20Deep-Align.%20To%20that%20end%2C%20we%20first%20prove%20that%20weight%20alignment%20adheres%20to%0Atwo%20fundamental%20symmetries%20and%20then%2C%20propose%20a%20deep%20architecture%20that%20respects%0Athese%20symmetries.%20Notably%2C%20our%20framework%20does%20not%20require%20any%20labeled%20data.%20We%0Aprovide%20a%20theoretical%20analysis%20of%20our%20approach%20and%20evaluate%20Deep-Align%20on%0Aseveral%20types%20of%20network%20architectures%20and%20learning%20setups.%20Our%20experimental%0Aresults%20indicate%20that%20a%20feed-forward%20pass%20with%20Deep-Align%20produces%20better%20or%0Aequivalent%20alignments%20compared%20to%20those%20produced%20by%20current%20optimization%0Aalgorithms.%20Additionally%2C%20our%20alignments%20can%20be%20used%20as%20an%20effective%0Ainitialization%20for%20other%20methods%2C%20leading%20to%20improved%20solutions%20with%20a%0Asignificant%20speedup%20in%20convergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13397v3&entry.124074799=Read"},
{"title": "Optimally Improving Cooperative Learning in a Social Setting", "author": "Shahrzad Haddadan and Cheng Xin and Jie Gao", "abstract": "  We consider a cooperative learning scenario where a collection of networked\nagents with individually owned classifiers dynamically update their\npredictions, for the same classification task, through communication or\nobservations of each other's predictions. Clearly if highly influential\nvertices use erroneous classifiers, there will be a negative effect on the\naccuracy of all the agents in the network. We ask the following question: how\ncan we optimally fix the prediction of a few classifiers so as maximize the\noverall accuracy in the entire network. To this end we consider an aggregate\nand an egalitarian objective function. We show a polynomial time algorithm for\noptimizing the aggregate objective function, and show that optimizing the\negalitarian objective function is NP-hard. Furthermore, we develop\napproximation algorithms for the egalitarian improvement. The performance of\nall of our algorithms are guaranteed by mathematical analysis and backed by\nexperiments on synthetic and real data.\n", "link": "http://arxiv.org/abs/2405.20808v1", "date": "2024-05-31", "relevancy": 2.4301, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4976}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4827}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimally%20Improving%20Cooperative%20Learning%20in%20a%20Social%20Setting&body=Title%3A%20Optimally%20Improving%20Cooperative%20Learning%20in%20a%20Social%20Setting%0AAuthor%3A%20Shahrzad%20Haddadan%20and%20Cheng%20Xin%20and%20Jie%20Gao%0AAbstract%3A%20%20%20We%20consider%20a%20cooperative%20learning%20scenario%20where%20a%20collection%20of%20networked%0Aagents%20with%20individually%20owned%20classifiers%20dynamically%20update%20their%0Apredictions%2C%20for%20the%20same%20classification%20task%2C%20through%20communication%20or%0Aobservations%20of%20each%20other%27s%20predictions.%20Clearly%20if%20highly%20influential%0Avertices%20use%20erroneous%20classifiers%2C%20there%20will%20be%20a%20negative%20effect%20on%20the%0Aaccuracy%20of%20all%20the%20agents%20in%20the%20network.%20We%20ask%20the%20following%20question%3A%20how%0Acan%20we%20optimally%20fix%20the%20prediction%20of%20a%20few%20classifiers%20so%20as%20maximize%20the%0Aoverall%20accuracy%20in%20the%20entire%20network.%20To%20this%20end%20we%20consider%20an%20aggregate%0Aand%20an%20egalitarian%20objective%20function.%20We%20show%20a%20polynomial%20time%20algorithm%20for%0Aoptimizing%20the%20aggregate%20objective%20function%2C%20and%20show%20that%20optimizing%20the%0Aegalitarian%20objective%20function%20is%20NP-hard.%20Furthermore%2C%20we%20develop%0Aapproximation%20algorithms%20for%20the%20egalitarian%20improvement.%20The%20performance%20of%0Aall%20of%20our%20algorithms%20are%20guaranteed%20by%20mathematical%20analysis%20and%20backed%20by%0Aexperiments%20on%20synthetic%20and%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimally%2520Improving%2520Cooperative%2520Learning%2520in%2520a%2520Social%2520Setting%26entry.906535625%3DShahrzad%2520Haddadan%2520and%2520Cheng%2520Xin%2520and%2520Jie%2520Gao%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520cooperative%2520learning%2520scenario%2520where%2520a%2520collection%2520of%2520networked%250Aagents%2520with%2520individually%2520owned%2520classifiers%2520dynamically%2520update%2520their%250Apredictions%252C%2520for%2520the%2520same%2520classification%2520task%252C%2520through%2520communication%2520or%250Aobservations%2520of%2520each%2520other%2527s%2520predictions.%2520Clearly%2520if%2520highly%2520influential%250Avertices%2520use%2520erroneous%2520classifiers%252C%2520there%2520will%2520be%2520a%2520negative%2520effect%2520on%2520the%250Aaccuracy%2520of%2520all%2520the%2520agents%2520in%2520the%2520network.%2520We%2520ask%2520the%2520following%2520question%253A%2520how%250Acan%2520we%2520optimally%2520fix%2520the%2520prediction%2520of%2520a%2520few%2520classifiers%2520so%2520as%2520maximize%2520the%250Aoverall%2520accuracy%2520in%2520the%2520entire%2520network.%2520To%2520this%2520end%2520we%2520consider%2520an%2520aggregate%250Aand%2520an%2520egalitarian%2520objective%2520function.%2520We%2520show%2520a%2520polynomial%2520time%2520algorithm%2520for%250Aoptimizing%2520the%2520aggregate%2520objective%2520function%252C%2520and%2520show%2520that%2520optimizing%2520the%250Aegalitarian%2520objective%2520function%2520is%2520NP-hard.%2520Furthermore%252C%2520we%2520develop%250Aapproximation%2520algorithms%2520for%2520the%2520egalitarian%2520improvement.%2520The%2520performance%2520of%250Aall%2520of%2520our%2520algorithms%2520are%2520guaranteed%2520by%2520mathematical%2520analysis%2520and%2520backed%2520by%250Aexperiments%2520on%2520synthetic%2520and%2520real%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimally%20Improving%20Cooperative%20Learning%20in%20a%20Social%20Setting&entry.906535625=Shahrzad%20Haddadan%20and%20Cheng%20Xin%20and%20Jie%20Gao&entry.1292438233=%20%20We%20consider%20a%20cooperative%20learning%20scenario%20where%20a%20collection%20of%20networked%0Aagents%20with%20individually%20owned%20classifiers%20dynamically%20update%20their%0Apredictions%2C%20for%20the%20same%20classification%20task%2C%20through%20communication%20or%0Aobservations%20of%20each%20other%27s%20predictions.%20Clearly%20if%20highly%20influential%0Avertices%20use%20erroneous%20classifiers%2C%20there%20will%20be%20a%20negative%20effect%20on%20the%0Aaccuracy%20of%20all%20the%20agents%20in%20the%20network.%20We%20ask%20the%20following%20question%3A%20how%0Acan%20we%20optimally%20fix%20the%20prediction%20of%20a%20few%20classifiers%20so%20as%20maximize%20the%0Aoverall%20accuracy%20in%20the%20entire%20network.%20To%20this%20end%20we%20consider%20an%20aggregate%0Aand%20an%20egalitarian%20objective%20function.%20We%20show%20a%20polynomial%20time%20algorithm%20for%0Aoptimizing%20the%20aggregate%20objective%20function%2C%20and%20show%20that%20optimizing%20the%0Aegalitarian%20objective%20function%20is%20NP-hard.%20Furthermore%2C%20we%20develop%0Aapproximation%20algorithms%20for%20the%20egalitarian%20improvement.%20The%20performance%20of%0Aall%20of%20our%20algorithms%20are%20guaranteed%20by%20mathematical%20analysis%20and%20backed%20by%0Aexperiments%20on%20synthetic%20and%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20808v1&entry.124074799=Read"},
{"title": "Sheaf HyperNetworks for Personalized Federated Learning", "author": "Bao Nguyen and Lorenzo Sani and Xinchi Qiu and Pietro Li\u00f2 and Nicholas D. Lane", "abstract": "  Graph hypernetworks (GHNs), constructed by combining graph neural networks\n(GNNs) with hypernetworks (HNs), leverage relational data across various\ndomains such as neural architecture search, molecular property prediction and\nfederated learning. Despite GNNs and HNs being individually successful, we show\nthat GHNs present problems compromising their performance, such as\nover-smoothing and heterophily. Moreover, we cannot apply GHNs directly to\npersonalized federated learning (PFL) scenarios, where a priori client relation\ngraph may be absent, private, or inaccessible. To mitigate these limitations in\nthe context of PFL, we propose a novel class of HNs, sheaf hypernetworks\n(SHNs), which combine cellular sheaf theory with HNs to improve parameter\nsharing for PFL. We thoroughly evaluate SHNs across diverse PFL tasks,\nincluding multi-class classification, traffic and weather forecasting.\nAdditionally, we provide a methodology for constructing client relation graphs\nin scenarios where such graphs are unavailable. We show that SHNs consistently\noutperform existing PFL solutions in complex non-IID scenarios. While the\nbaselines' performance fluctuates depending on the task, SHNs show improvements\nof up to 2.7% in accuracy and 5.3% in lower mean squared error over the\nbest-performing baseline.\n", "link": "http://arxiv.org/abs/2405.20882v1", "date": "2024-05-31", "relevancy": 2.3527, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5017}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4646}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sheaf%20HyperNetworks%20for%20Personalized%20Federated%20Learning&body=Title%3A%20Sheaf%20HyperNetworks%20for%20Personalized%20Federated%20Learning%0AAuthor%3A%20Bao%20Nguyen%20and%20Lorenzo%20Sani%20and%20Xinchi%20Qiu%20and%20Pietro%20Li%C3%B2%20and%20Nicholas%20D.%20Lane%0AAbstract%3A%20%20%20Graph%20hypernetworks%20%28GHNs%29%2C%20constructed%20by%20combining%20graph%20neural%20networks%0A%28GNNs%29%20with%20hypernetworks%20%28HNs%29%2C%20leverage%20relational%20data%20across%20various%0Adomains%20such%20as%20neural%20architecture%20search%2C%20molecular%20property%20prediction%20and%0Afederated%20learning.%20Despite%20GNNs%20and%20HNs%20being%20individually%20successful%2C%20we%20show%0Athat%20GHNs%20present%20problems%20compromising%20their%20performance%2C%20such%20as%0Aover-smoothing%20and%20heterophily.%20Moreover%2C%20we%20cannot%20apply%20GHNs%20directly%20to%0Apersonalized%20federated%20learning%20%28PFL%29%20scenarios%2C%20where%20a%20priori%20client%20relation%0Agraph%20may%20be%20absent%2C%20private%2C%20or%20inaccessible.%20To%20mitigate%20these%20limitations%20in%0Athe%20context%20of%20PFL%2C%20we%20propose%20a%20novel%20class%20of%20HNs%2C%20sheaf%20hypernetworks%0A%28SHNs%29%2C%20which%20combine%20cellular%20sheaf%20theory%20with%20HNs%20to%20improve%20parameter%0Asharing%20for%20PFL.%20We%20thoroughly%20evaluate%20SHNs%20across%20diverse%20PFL%20tasks%2C%0Aincluding%20multi-class%20classification%2C%20traffic%20and%20weather%20forecasting.%0AAdditionally%2C%20we%20provide%20a%20methodology%20for%20constructing%20client%20relation%20graphs%0Ain%20scenarios%20where%20such%20graphs%20are%20unavailable.%20We%20show%20that%20SHNs%20consistently%0Aoutperform%20existing%20PFL%20solutions%20in%20complex%20non-IID%20scenarios.%20While%20the%0Abaselines%27%20performance%20fluctuates%20depending%20on%20the%20task%2C%20SHNs%20show%20improvements%0Aof%20up%20to%202.7%25%20in%20accuracy%20and%205.3%25%20in%20lower%20mean%20squared%20error%20over%20the%0Abest-performing%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSheaf%2520HyperNetworks%2520for%2520Personalized%2520Federated%2520Learning%26entry.906535625%3DBao%2520Nguyen%2520and%2520Lorenzo%2520Sani%2520and%2520Xinchi%2520Qiu%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Nicholas%2520D.%2520Lane%26entry.1292438233%3D%2520%2520Graph%2520hypernetworks%2520%2528GHNs%2529%252C%2520constructed%2520by%2520combining%2520graph%2520neural%2520networks%250A%2528GNNs%2529%2520with%2520hypernetworks%2520%2528HNs%2529%252C%2520leverage%2520relational%2520data%2520across%2520various%250Adomains%2520such%2520as%2520neural%2520architecture%2520search%252C%2520molecular%2520property%2520prediction%2520and%250Afederated%2520learning.%2520Despite%2520GNNs%2520and%2520HNs%2520being%2520individually%2520successful%252C%2520we%2520show%250Athat%2520GHNs%2520present%2520problems%2520compromising%2520their%2520performance%252C%2520such%2520as%250Aover-smoothing%2520and%2520heterophily.%2520Moreover%252C%2520we%2520cannot%2520apply%2520GHNs%2520directly%2520to%250Apersonalized%2520federated%2520learning%2520%2528PFL%2529%2520scenarios%252C%2520where%2520a%2520priori%2520client%2520relation%250Agraph%2520may%2520be%2520absent%252C%2520private%252C%2520or%2520inaccessible.%2520To%2520mitigate%2520these%2520limitations%2520in%250Athe%2520context%2520of%2520PFL%252C%2520we%2520propose%2520a%2520novel%2520class%2520of%2520HNs%252C%2520sheaf%2520hypernetworks%250A%2528SHNs%2529%252C%2520which%2520combine%2520cellular%2520sheaf%2520theory%2520with%2520HNs%2520to%2520improve%2520parameter%250Asharing%2520for%2520PFL.%2520We%2520thoroughly%2520evaluate%2520SHNs%2520across%2520diverse%2520PFL%2520tasks%252C%250Aincluding%2520multi-class%2520classification%252C%2520traffic%2520and%2520weather%2520forecasting.%250AAdditionally%252C%2520we%2520provide%2520a%2520methodology%2520for%2520constructing%2520client%2520relation%2520graphs%250Ain%2520scenarios%2520where%2520such%2520graphs%2520are%2520unavailable.%2520We%2520show%2520that%2520SHNs%2520consistently%250Aoutperform%2520existing%2520PFL%2520solutions%2520in%2520complex%2520non-IID%2520scenarios.%2520While%2520the%250Abaselines%2527%2520performance%2520fluctuates%2520depending%2520on%2520the%2520task%252C%2520SHNs%2520show%2520improvements%250Aof%2520up%2520to%25202.7%2525%2520in%2520accuracy%2520and%25205.3%2525%2520in%2520lower%2520mean%2520squared%2520error%2520over%2520the%250Abest-performing%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sheaf%20HyperNetworks%20for%20Personalized%20Federated%20Learning&entry.906535625=Bao%20Nguyen%20and%20Lorenzo%20Sani%20and%20Xinchi%20Qiu%20and%20Pietro%20Li%C3%B2%20and%20Nicholas%20D.%20Lane&entry.1292438233=%20%20Graph%20hypernetworks%20%28GHNs%29%2C%20constructed%20by%20combining%20graph%20neural%20networks%0A%28GNNs%29%20with%20hypernetworks%20%28HNs%29%2C%20leverage%20relational%20data%20across%20various%0Adomains%20such%20as%20neural%20architecture%20search%2C%20molecular%20property%20prediction%20and%0Afederated%20learning.%20Despite%20GNNs%20and%20HNs%20being%20individually%20successful%2C%20we%20show%0Athat%20GHNs%20present%20problems%20compromising%20their%20performance%2C%20such%20as%0Aover-smoothing%20and%20heterophily.%20Moreover%2C%20we%20cannot%20apply%20GHNs%20directly%20to%0Apersonalized%20federated%20learning%20%28PFL%29%20scenarios%2C%20where%20a%20priori%20client%20relation%0Agraph%20may%20be%20absent%2C%20private%2C%20or%20inaccessible.%20To%20mitigate%20these%20limitations%20in%0Athe%20context%20of%20PFL%2C%20we%20propose%20a%20novel%20class%20of%20HNs%2C%20sheaf%20hypernetworks%0A%28SHNs%29%2C%20which%20combine%20cellular%20sheaf%20theory%20with%20HNs%20to%20improve%20parameter%0Asharing%20for%20PFL.%20We%20thoroughly%20evaluate%20SHNs%20across%20diverse%20PFL%20tasks%2C%0Aincluding%20multi-class%20classification%2C%20traffic%20and%20weather%20forecasting.%0AAdditionally%2C%20we%20provide%20a%20methodology%20for%20constructing%20client%20relation%20graphs%0Ain%20scenarios%20where%20such%20graphs%20are%20unavailable.%20We%20show%20that%20SHNs%20consistently%0Aoutperform%20existing%20PFL%20solutions%20in%20complex%20non-IID%20scenarios.%20While%20the%0Abaselines%27%20performance%20fluctuates%20depending%20on%20the%20task%2C%20SHNs%20show%20improvements%0Aof%20up%20to%202.7%25%20in%20accuracy%20and%205.3%25%20in%20lower%20mean%20squared%20error%20over%20the%0Abest-performing%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20882v1&entry.124074799=Read"},
{"title": "Enhancing Vision Models for Text-Heavy Content Understanding and\n  Interaction", "author": "Adithya TG and Adithya SK and Abhinav R Bharadwaj and Abhiram HA and Dr. Surabhi Narayan", "abstract": "  Interacting and understanding with text heavy visual content with multiple\nimages is a major challenge for traditional vision models. This paper is on\nenhancing vision models' capability to comprehend or understand and learn from\nimages containing a huge amount of textual information from the likes of\ntextbooks and research papers which contain multiple images like graphs, etc\nand tables in them with different types of axes and scales. The approach\ninvolves dataset preprocessing, fine tuning which is by using instructional\noriented data and evaluation. We also built a visual chat application\nintegrating CLIP for image encoding and a model from the Massive Text Embedding\nBenchmark which is developed to consider both textual and visual inputs. An\naccuracy of 96.71% was obtained. The aim of the project is to increase and also\nenhance the advance vision models' capabilities in understanding complex visual\ntextual data interconnected data, contributing to multimodal AI.\n", "link": "http://arxiv.org/abs/2405.20906v1", "date": "2024-05-31", "relevancy": 2.3424, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6147}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5848}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vision%20Models%20for%20Text-Heavy%20Content%20Understanding%20and%0A%20%20Interaction&body=Title%3A%20Enhancing%20Vision%20Models%20for%20Text-Heavy%20Content%20Understanding%20and%0A%20%20Interaction%0AAuthor%3A%20Adithya%20TG%20and%20Adithya%20SK%20and%20Abhinav%20R%20Bharadwaj%20and%20Abhiram%20HA%20and%20Dr.%20Surabhi%20Narayan%0AAbstract%3A%20%20%20Interacting%20and%20understanding%20with%20text%20heavy%20visual%20content%20with%20multiple%0Aimages%20is%20a%20major%20challenge%20for%20traditional%20vision%20models.%20This%20paper%20is%20on%0Aenhancing%20vision%20models%27%20capability%20to%20comprehend%20or%20understand%20and%20learn%20from%0Aimages%20containing%20a%20huge%20amount%20of%20textual%20information%20from%20the%20likes%20of%0Atextbooks%20and%20research%20papers%20which%20contain%20multiple%20images%20like%20graphs%2C%20etc%0Aand%20tables%20in%20them%20with%20different%20types%20of%20axes%20and%20scales.%20The%20approach%0Ainvolves%20dataset%20preprocessing%2C%20fine%20tuning%20which%20is%20by%20using%20instructional%0Aoriented%20data%20and%20evaluation.%20We%20also%20built%20a%20visual%20chat%20application%0Aintegrating%20CLIP%20for%20image%20encoding%20and%20a%20model%20from%20the%20Massive%20Text%20Embedding%0ABenchmark%20which%20is%20developed%20to%20consider%20both%20textual%20and%20visual%20inputs.%20An%0Aaccuracy%20of%2096.71%25%20was%20obtained.%20The%20aim%20of%20the%20project%20is%20to%20increase%20and%20also%0Aenhance%20the%20advance%20vision%20models%27%20capabilities%20in%20understanding%20complex%20visual%0Atextual%20data%20interconnected%20data%2C%20contributing%20to%20multimodal%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vision%2520Models%2520for%2520Text-Heavy%2520Content%2520Understanding%2520and%250A%2520%2520Interaction%26entry.906535625%3DAdithya%2520TG%2520and%2520Adithya%2520SK%2520and%2520Abhinav%2520R%2520Bharadwaj%2520and%2520Abhiram%2520HA%2520and%2520Dr.%2520Surabhi%2520Narayan%26entry.1292438233%3D%2520%2520Interacting%2520and%2520understanding%2520with%2520text%2520heavy%2520visual%2520content%2520with%2520multiple%250Aimages%2520is%2520a%2520major%2520challenge%2520for%2520traditional%2520vision%2520models.%2520This%2520paper%2520is%2520on%250Aenhancing%2520vision%2520models%2527%2520capability%2520to%2520comprehend%2520or%2520understand%2520and%2520learn%2520from%250Aimages%2520containing%2520a%2520huge%2520amount%2520of%2520textual%2520information%2520from%2520the%2520likes%2520of%250Atextbooks%2520and%2520research%2520papers%2520which%2520contain%2520multiple%2520images%2520like%2520graphs%252C%2520etc%250Aand%2520tables%2520in%2520them%2520with%2520different%2520types%2520of%2520axes%2520and%2520scales.%2520The%2520approach%250Ainvolves%2520dataset%2520preprocessing%252C%2520fine%2520tuning%2520which%2520is%2520by%2520using%2520instructional%250Aoriented%2520data%2520and%2520evaluation.%2520We%2520also%2520built%2520a%2520visual%2520chat%2520application%250Aintegrating%2520CLIP%2520for%2520image%2520encoding%2520and%2520a%2520model%2520from%2520the%2520Massive%2520Text%2520Embedding%250ABenchmark%2520which%2520is%2520developed%2520to%2520consider%2520both%2520textual%2520and%2520visual%2520inputs.%2520An%250Aaccuracy%2520of%252096.71%2525%2520was%2520obtained.%2520The%2520aim%2520of%2520the%2520project%2520is%2520to%2520increase%2520and%2520also%250Aenhance%2520the%2520advance%2520vision%2520models%2527%2520capabilities%2520in%2520understanding%2520complex%2520visual%250Atextual%2520data%2520interconnected%2520data%252C%2520contributing%2520to%2520multimodal%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vision%20Models%20for%20Text-Heavy%20Content%20Understanding%20and%0A%20%20Interaction&entry.906535625=Adithya%20TG%20and%20Adithya%20SK%20and%20Abhinav%20R%20Bharadwaj%20and%20Abhiram%20HA%20and%20Dr.%20Surabhi%20Narayan&entry.1292438233=%20%20Interacting%20and%20understanding%20with%20text%20heavy%20visual%20content%20with%20multiple%0Aimages%20is%20a%20major%20challenge%20for%20traditional%20vision%20models.%20This%20paper%20is%20on%0Aenhancing%20vision%20models%27%20capability%20to%20comprehend%20or%20understand%20and%20learn%20from%0Aimages%20containing%20a%20huge%20amount%20of%20textual%20information%20from%20the%20likes%20of%0Atextbooks%20and%20research%20papers%20which%20contain%20multiple%20images%20like%20graphs%2C%20etc%0Aand%20tables%20in%20them%20with%20different%20types%20of%20axes%20and%20scales.%20The%20approach%0Ainvolves%20dataset%20preprocessing%2C%20fine%20tuning%20which%20is%20by%20using%20instructional%0Aoriented%20data%20and%20evaluation.%20We%20also%20built%20a%20visual%20chat%20application%0Aintegrating%20CLIP%20for%20image%20encoding%20and%20a%20model%20from%20the%20Massive%20Text%20Embedding%0ABenchmark%20which%20is%20developed%20to%20consider%20both%20textual%20and%20visual%20inputs.%20An%0Aaccuracy%20of%2096.71%25%20was%20obtained.%20The%20aim%20of%20the%20project%20is%20to%20increase%20and%20also%0Aenhance%20the%20advance%20vision%20models%27%20capabilities%20in%20understanding%20complex%20visual%0Atextual%20data%20interconnected%20data%2C%20contributing%20to%20multimodal%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20906v1&entry.124074799=Read"},
{"title": "P4: Towards private, personalized, and Peer-to-Peer learning", "author": "Mohammad Mahdi Maheri and Sandra Siby and Sina Abdollahi and Anastasia Borovykh and Hamed Haddadi", "abstract": "  Personalized learning is a proposed approach to address the problem of data\nheterogeneity in collaborative machine learning. In a decentralized setting,\nthe two main challenges of personalization are client clustering and data\nprivacy. In this paper, we address these challenges by developing P4\n(Personalized Private Peer-to-Peer) a method that ensures that each client\nreceives a personalized model while maintaining differential privacy guarantee\nof each client's local dataset during and after the training. Our approach\nincludes the design of a lightweight algorithm to identify similar clients and\ngroup them in a private, peer-to-peer (P2P) manner. Once grouped, we develop\ndifferentially-private knowledge distillation for clients to co-train with\nminimal impact on accuracy. We evaluate our proposed method on three benchmark\ndatasets (FEMNIST or Federated EMNIST, CIFAR-10 and CIFAR-100) and two\ndifferent neural network architectures (Linear and CNN-based networks) across a\nrange of privacy parameters. The results demonstrate the potential of P4, as it\noutperforms the state-of-the-art of differential private P2P by up to 40\npercent in terms of accuracy. We also show the practicality of P4 by\nimplementing it on resource constrained devices, and validating that it has\nminimal overhead, e.g., about 7 seconds to run collaborative training between\ntwo clients.\n", "link": "http://arxiv.org/abs/2405.17697v2", "date": "2024-05-31", "relevancy": 2.3374, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4744}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.471}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P4%3A%20Towards%20private%2C%20personalized%2C%20and%20Peer-to-Peer%20learning&body=Title%3A%20P4%3A%20Towards%20private%2C%20personalized%2C%20and%20Peer-to-Peer%20learning%0AAuthor%3A%20Mohammad%20Mahdi%20Maheri%20and%20Sandra%20Siby%20and%20Sina%20Abdollahi%20and%20Anastasia%20Borovykh%20and%20Hamed%20Haddadi%0AAbstract%3A%20%20%20Personalized%20learning%20is%20a%20proposed%20approach%20to%20address%20the%20problem%20of%20data%0Aheterogeneity%20in%20collaborative%20machine%20learning.%20In%20a%20decentralized%20setting%2C%0Athe%20two%20main%20challenges%20of%20personalization%20are%20client%20clustering%20and%20data%0Aprivacy.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%20developing%20P4%0A%28Personalized%20Private%20Peer-to-Peer%29%20a%20method%20that%20ensures%20that%20each%20client%0Areceives%20a%20personalized%20model%20while%20maintaining%20differential%20privacy%20guarantee%0Aof%20each%20client%27s%20local%20dataset%20during%20and%20after%20the%20training.%20Our%20approach%0Aincludes%20the%20design%20of%20a%20lightweight%20algorithm%20to%20identify%20similar%20clients%20and%0Agroup%20them%20in%20a%20private%2C%20peer-to-peer%20%28P2P%29%20manner.%20Once%20grouped%2C%20we%20develop%0Adifferentially-private%20knowledge%20distillation%20for%20clients%20to%20co-train%20with%0Aminimal%20impact%20on%20accuracy.%20We%20evaluate%20our%20proposed%20method%20on%20three%20benchmark%0Adatasets%20%28FEMNIST%20or%20Federated%20EMNIST%2C%20CIFAR-10%20and%20CIFAR-100%29%20and%20two%0Adifferent%20neural%20network%20architectures%20%28Linear%20and%20CNN-based%20networks%29%20across%20a%0Arange%20of%20privacy%20parameters.%20The%20results%20demonstrate%20the%20potential%20of%20P4%2C%20as%20it%0Aoutperforms%20the%20state-of-the-art%20of%20differential%20private%20P2P%20by%20up%20to%2040%0Apercent%20in%20terms%20of%20accuracy.%20We%20also%20show%20the%20practicality%20of%20P4%20by%0Aimplementing%20it%20on%20resource%20constrained%20devices%2C%20and%20validating%20that%20it%20has%0Aminimal%20overhead%2C%20e.g.%2C%20about%207%20seconds%20to%20run%20collaborative%20training%20between%0Atwo%20clients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17697v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP4%253A%2520Towards%2520private%252C%2520personalized%252C%2520and%2520Peer-to-Peer%2520learning%26entry.906535625%3DMohammad%2520Mahdi%2520Maheri%2520and%2520Sandra%2520Siby%2520and%2520Sina%2520Abdollahi%2520and%2520Anastasia%2520Borovykh%2520and%2520Hamed%2520Haddadi%26entry.1292438233%3D%2520%2520Personalized%2520learning%2520is%2520a%2520proposed%2520approach%2520to%2520address%2520the%2520problem%2520of%2520data%250Aheterogeneity%2520in%2520collaborative%2520machine%2520learning.%2520In%2520a%2520decentralized%2520setting%252C%250Athe%2520two%2520main%2520challenges%2520of%2520personalization%2520are%2520client%2520clustering%2520and%2520data%250Aprivacy.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520challenges%2520by%2520developing%2520P4%250A%2528Personalized%2520Private%2520Peer-to-Peer%2529%2520a%2520method%2520that%2520ensures%2520that%2520each%2520client%250Areceives%2520a%2520personalized%2520model%2520while%2520maintaining%2520differential%2520privacy%2520guarantee%250Aof%2520each%2520client%2527s%2520local%2520dataset%2520during%2520and%2520after%2520the%2520training.%2520Our%2520approach%250Aincludes%2520the%2520design%2520of%2520a%2520lightweight%2520algorithm%2520to%2520identify%2520similar%2520clients%2520and%250Agroup%2520them%2520in%2520a%2520private%252C%2520peer-to-peer%2520%2528P2P%2529%2520manner.%2520Once%2520grouped%252C%2520we%2520develop%250Adifferentially-private%2520knowledge%2520distillation%2520for%2520clients%2520to%2520co-train%2520with%250Aminimal%2520impact%2520on%2520accuracy.%2520We%2520evaluate%2520our%2520proposed%2520method%2520on%2520three%2520benchmark%250Adatasets%2520%2528FEMNIST%2520or%2520Federated%2520EMNIST%252C%2520CIFAR-10%2520and%2520CIFAR-100%2529%2520and%2520two%250Adifferent%2520neural%2520network%2520architectures%2520%2528Linear%2520and%2520CNN-based%2520networks%2529%2520across%2520a%250Arange%2520of%2520privacy%2520parameters.%2520The%2520results%2520demonstrate%2520the%2520potential%2520of%2520P4%252C%2520as%2520it%250Aoutperforms%2520the%2520state-of-the-art%2520of%2520differential%2520private%2520P2P%2520by%2520up%2520to%252040%250Apercent%2520in%2520terms%2520of%2520accuracy.%2520We%2520also%2520show%2520the%2520practicality%2520of%2520P4%2520by%250Aimplementing%2520it%2520on%2520resource%2520constrained%2520devices%252C%2520and%2520validating%2520that%2520it%2520has%250Aminimal%2520overhead%252C%2520e.g.%252C%2520about%25207%2520seconds%2520to%2520run%2520collaborative%2520training%2520between%250Atwo%2520clients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17697v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P4%3A%20Towards%20private%2C%20personalized%2C%20and%20Peer-to-Peer%20learning&entry.906535625=Mohammad%20Mahdi%20Maheri%20and%20Sandra%20Siby%20and%20Sina%20Abdollahi%20and%20Anastasia%20Borovykh%20and%20Hamed%20Haddadi&entry.1292438233=%20%20Personalized%20learning%20is%20a%20proposed%20approach%20to%20address%20the%20problem%20of%20data%0Aheterogeneity%20in%20collaborative%20machine%20learning.%20In%20a%20decentralized%20setting%2C%0Athe%20two%20main%20challenges%20of%20personalization%20are%20client%20clustering%20and%20data%0Aprivacy.%20In%20this%20paper%2C%20we%20address%20these%20challenges%20by%20developing%20P4%0A%28Personalized%20Private%20Peer-to-Peer%29%20a%20method%20that%20ensures%20that%20each%20client%0Areceives%20a%20personalized%20model%20while%20maintaining%20differential%20privacy%20guarantee%0Aof%20each%20client%27s%20local%20dataset%20during%20and%20after%20the%20training.%20Our%20approach%0Aincludes%20the%20design%20of%20a%20lightweight%20algorithm%20to%20identify%20similar%20clients%20and%0Agroup%20them%20in%20a%20private%2C%20peer-to-peer%20%28P2P%29%20manner.%20Once%20grouped%2C%20we%20develop%0Adifferentially-private%20knowledge%20distillation%20for%20clients%20to%20co-train%20with%0Aminimal%20impact%20on%20accuracy.%20We%20evaluate%20our%20proposed%20method%20on%20three%20benchmark%0Adatasets%20%28FEMNIST%20or%20Federated%20EMNIST%2C%20CIFAR-10%20and%20CIFAR-100%29%20and%20two%0Adifferent%20neural%20network%20architectures%20%28Linear%20and%20CNN-based%20networks%29%20across%20a%0Arange%20of%20privacy%20parameters.%20The%20results%20demonstrate%20the%20potential%20of%20P4%2C%20as%20it%0Aoutperforms%20the%20state-of-the-art%20of%20differential%20private%20P2P%20by%20up%20to%2040%0Apercent%20in%20terms%20of%20accuracy.%20We%20also%20show%20the%20practicality%20of%20P4%20by%0Aimplementing%20it%20on%20resource%20constrained%20devices%2C%20and%20validating%20that%20it%20has%0Aminimal%20overhead%2C%20e.g.%2C%20about%207%20seconds%20to%20run%20collaborative%20training%20between%0Atwo%20clients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17697v2&entry.124074799=Read"},
{"title": "Behind Every Domain There is a Shift: Adapting Distortion-aware Vision\n  Transformers for Panoramic Semantic Segmentation", "author": "Jiaming Zhang and Kailun Yang and Hao Shi and Simon Rei\u00df and Kunyu Peng and Chaoxiang Ma and Haodong Fu and Philip H. S. Torr and Kaiwei Wang and Rainer Stiefelhagen", "abstract": "  In this paper, we address panoramic semantic segmentation which is\nunder-explored due to two critical challenges: (1) image distortions and object\ndeformations on panoramas; (2) lack of semantic annotations in the 360{\\deg}\nimagery. To tackle these problems, first, we propose the upgraded Transformer\nfor Panoramic Semantic Segmentation, i.e., Trans4PASS+, equipped with\nDeformable Patch Embedding (DPE) and Deformable MLP (DMLPv2) modules for\nhandling object deformations and image distortions whenever (before or after\nadaptation) and wherever (shallow or deep levels). Second, we enhance the\nMutual Prototypical Adaptation (MPA) strategy via pseudo-label rectification\nfor unsupervised domain adaptive panoramic segmentation. Third, aside from\nPinhole-to-Panoramic (Pin2Pan) adaptation, we create a new dataset (SynPASS)\nwith 9,080 panoramic images, facilitating Synthetic-to-Real (Syn2Real)\nadaptation scheme in 360{\\deg} imagery. Extensive experiments are conducted,\nwhich cover indoor and outdoor scenarios, and each of them is investigated with\nPin2Pan and Syn2Real regimens. Trans4PASS+ achieves state-of-the-art\nperformances on four domain adaptive panoramic semantic segmentation\nbenchmarks. Code is available at https://github.com/jamycheung/Trans4PASS.\n", "link": "http://arxiv.org/abs/2207.11860v5", "date": "2024-05-31", "relevancy": 2.3207, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6028}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5694}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Behind%20Every%20Domain%20There%20is%20a%20Shift%3A%20Adapting%20Distortion-aware%20Vision%0A%20%20Transformers%20for%20Panoramic%20Semantic%20Segmentation&body=Title%3A%20Behind%20Every%20Domain%20There%20is%20a%20Shift%3A%20Adapting%20Distortion-aware%20Vision%0A%20%20Transformers%20for%20Panoramic%20Semantic%20Segmentation%0AAuthor%3A%20Jiaming%20Zhang%20and%20Kailun%20Yang%20and%20Hao%20Shi%20and%20Simon%20Rei%C3%9F%20and%20Kunyu%20Peng%20and%20Chaoxiang%20Ma%20and%20Haodong%20Fu%20and%20Philip%20H.%20S.%20Torr%20and%20Kaiwei%20Wang%20and%20Rainer%20Stiefelhagen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20panoramic%20semantic%20segmentation%20which%20is%0Aunder-explored%20due%20to%20two%20critical%20challenges%3A%20%281%29%20image%20distortions%20and%20object%0Adeformations%20on%20panoramas%3B%20%282%29%20lack%20of%20semantic%20annotations%20in%20the%20360%7B%5Cdeg%7D%0Aimagery.%20To%20tackle%20these%20problems%2C%20first%2C%20we%20propose%20the%20upgraded%20Transformer%0Afor%20Panoramic%20Semantic%20Segmentation%2C%20i.e.%2C%20Trans4PASS%2B%2C%20equipped%20with%0ADeformable%20Patch%20Embedding%20%28DPE%29%20and%20Deformable%20MLP%20%28DMLPv2%29%20modules%20for%0Ahandling%20object%20deformations%20and%20image%20distortions%20whenever%20%28before%20or%20after%0Aadaptation%29%20and%20wherever%20%28shallow%20or%20deep%20levels%29.%20Second%2C%20we%20enhance%20the%0AMutual%20Prototypical%20Adaptation%20%28MPA%29%20strategy%20via%20pseudo-label%20rectification%0Afor%20unsupervised%20domain%20adaptive%20panoramic%20segmentation.%20Third%2C%20aside%20from%0APinhole-to-Panoramic%20%28Pin2Pan%29%20adaptation%2C%20we%20create%20a%20new%20dataset%20%28SynPASS%29%0Awith%209%2C080%20panoramic%20images%2C%20facilitating%20Synthetic-to-Real%20%28Syn2Real%29%0Aadaptation%20scheme%20in%20360%7B%5Cdeg%7D%20imagery.%20Extensive%20experiments%20are%20conducted%2C%0Awhich%20cover%20indoor%20and%20outdoor%20scenarios%2C%20and%20each%20of%20them%20is%20investigated%20with%0APin2Pan%20and%20Syn2Real%20regimens.%20Trans4PASS%2B%20achieves%20state-of-the-art%0Aperformances%20on%20four%20domain%20adaptive%20panoramic%20semantic%20segmentation%0Abenchmarks.%20Code%20is%20available%20at%20https%3A//github.com/jamycheung/Trans4PASS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2207.11860v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBehind%2520Every%2520Domain%2520There%2520is%2520a%2520Shift%253A%2520Adapting%2520Distortion-aware%2520Vision%250A%2520%2520Transformers%2520for%2520Panoramic%2520Semantic%2520Segmentation%26entry.906535625%3DJiaming%2520Zhang%2520and%2520Kailun%2520Yang%2520and%2520Hao%2520Shi%2520and%2520Simon%2520Rei%25C3%259F%2520and%2520Kunyu%2520Peng%2520and%2520Chaoxiang%2520Ma%2520and%2520Haodong%2520Fu%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Kaiwei%2520Wang%2520and%2520Rainer%2520Stiefelhagen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520panoramic%2520semantic%2520segmentation%2520which%2520is%250Aunder-explored%2520due%2520to%2520two%2520critical%2520challenges%253A%2520%25281%2529%2520image%2520distortions%2520and%2520object%250Adeformations%2520on%2520panoramas%253B%2520%25282%2529%2520lack%2520of%2520semantic%2520annotations%2520in%2520the%2520360%257B%255Cdeg%257D%250Aimagery.%2520To%2520tackle%2520these%2520problems%252C%2520first%252C%2520we%2520propose%2520the%2520upgraded%2520Transformer%250Afor%2520Panoramic%2520Semantic%2520Segmentation%252C%2520i.e.%252C%2520Trans4PASS%252B%252C%2520equipped%2520with%250ADeformable%2520Patch%2520Embedding%2520%2528DPE%2529%2520and%2520Deformable%2520MLP%2520%2528DMLPv2%2529%2520modules%2520for%250Ahandling%2520object%2520deformations%2520and%2520image%2520distortions%2520whenever%2520%2528before%2520or%2520after%250Aadaptation%2529%2520and%2520wherever%2520%2528shallow%2520or%2520deep%2520levels%2529.%2520Second%252C%2520we%2520enhance%2520the%250AMutual%2520Prototypical%2520Adaptation%2520%2528MPA%2529%2520strategy%2520via%2520pseudo-label%2520rectification%250Afor%2520unsupervised%2520domain%2520adaptive%2520panoramic%2520segmentation.%2520Third%252C%2520aside%2520from%250APinhole-to-Panoramic%2520%2528Pin2Pan%2529%2520adaptation%252C%2520we%2520create%2520a%2520new%2520dataset%2520%2528SynPASS%2529%250Awith%25209%252C080%2520panoramic%2520images%252C%2520facilitating%2520Synthetic-to-Real%2520%2528Syn2Real%2529%250Aadaptation%2520scheme%2520in%2520360%257B%255Cdeg%257D%2520imagery.%2520Extensive%2520experiments%2520are%2520conducted%252C%250Awhich%2520cover%2520indoor%2520and%2520outdoor%2520scenarios%252C%2520and%2520each%2520of%2520them%2520is%2520investigated%2520with%250APin2Pan%2520and%2520Syn2Real%2520regimens.%2520Trans4PASS%252B%2520achieves%2520state-of-the-art%250Aperformances%2520on%2520four%2520domain%2520adaptive%2520panoramic%2520semantic%2520segmentation%250Abenchmarks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/jamycheung/Trans4PASS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2207.11860v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Behind%20Every%20Domain%20There%20is%20a%20Shift%3A%20Adapting%20Distortion-aware%20Vision%0A%20%20Transformers%20for%20Panoramic%20Semantic%20Segmentation&entry.906535625=Jiaming%20Zhang%20and%20Kailun%20Yang%20and%20Hao%20Shi%20and%20Simon%20Rei%C3%9F%20and%20Kunyu%20Peng%20and%20Chaoxiang%20Ma%20and%20Haodong%20Fu%20and%20Philip%20H.%20S.%20Torr%20and%20Kaiwei%20Wang%20and%20Rainer%20Stiefelhagen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20panoramic%20semantic%20segmentation%20which%20is%0Aunder-explored%20due%20to%20two%20critical%20challenges%3A%20%281%29%20image%20distortions%20and%20object%0Adeformations%20on%20panoramas%3B%20%282%29%20lack%20of%20semantic%20annotations%20in%20the%20360%7B%5Cdeg%7D%0Aimagery.%20To%20tackle%20these%20problems%2C%20first%2C%20we%20propose%20the%20upgraded%20Transformer%0Afor%20Panoramic%20Semantic%20Segmentation%2C%20i.e.%2C%20Trans4PASS%2B%2C%20equipped%20with%0ADeformable%20Patch%20Embedding%20%28DPE%29%20and%20Deformable%20MLP%20%28DMLPv2%29%20modules%20for%0Ahandling%20object%20deformations%20and%20image%20distortions%20whenever%20%28before%20or%20after%0Aadaptation%29%20and%20wherever%20%28shallow%20or%20deep%20levels%29.%20Second%2C%20we%20enhance%20the%0AMutual%20Prototypical%20Adaptation%20%28MPA%29%20strategy%20via%20pseudo-label%20rectification%0Afor%20unsupervised%20domain%20adaptive%20panoramic%20segmentation.%20Third%2C%20aside%20from%0APinhole-to-Panoramic%20%28Pin2Pan%29%20adaptation%2C%20we%20create%20a%20new%20dataset%20%28SynPASS%29%0Awith%209%2C080%20panoramic%20images%2C%20facilitating%20Synthetic-to-Real%20%28Syn2Real%29%0Aadaptation%20scheme%20in%20360%7B%5Cdeg%7D%20imagery.%20Extensive%20experiments%20are%20conducted%2C%0Awhich%20cover%20indoor%20and%20outdoor%20scenarios%2C%20and%20each%20of%20them%20is%20investigated%20with%0APin2Pan%20and%20Syn2Real%20regimens.%20Trans4PASS%2B%20achieves%20state-of-the-art%0Aperformances%20on%20four%20domain%20adaptive%20panoramic%20semantic%20segmentation%0Abenchmarks.%20Code%20is%20available%20at%20https%3A//github.com/jamycheung/Trans4PASS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2207.11860v5&entry.124074799=Read"},
{"title": "Robust Collaborative Perception without External Localization and Clock\n  Devices", "author": "Zixing Lei and Zhenyang Ni and Ruize Han and Shuo Tang and Dingju Wang and Chen Feng and Siheng Chen and Yanfeng Wang", "abstract": "  A consistent spatial-temporal coordination across multiple agents is\nfundamental for collaborative perception, which seeks to improve perception\nabilities through information exchange among agents. To achieve this\nspatial-temporal alignment, traditional methods depend on external devices to\nprovide localization and clock signals. However, hardware-generated signals\ncould be vulnerable to noise and potentially malicious attack, jeopardizing the\nprecision of spatial-temporal alignment. Rather than relying on external\nhardwares, this work proposes a novel approach: aligning by recognizing the\ninherent geometric patterns within the perceptual data of various agents.\nFollowing this spirit, we propose a robust collaborative perception system that\noperates independently of external localization and clock devices. The key\nmodule of our system,~\\emph{FreeAlign}, constructs a salient object graph for\neach agent based on its detected boxes and uses a graph neural network to\nidentify common subgraphs between agents, leading to accurate relative pose and\ntime. We validate \\emph{FreeAlign} on both real-world and simulated datasets.\nThe results show that, the ~\\emph{FreeAlign} empowered robust collaborative\nperception system perform comparably to systems relying on precise localization\nand clock devices.\n", "link": "http://arxiv.org/abs/2405.02965v2", "date": "2024-05-31", "relevancy": 2.295, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6145}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5514}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Collaborative%20Perception%20without%20External%20Localization%20and%20Clock%0A%20%20Devices&body=Title%3A%20Robust%20Collaborative%20Perception%20without%20External%20Localization%20and%20Clock%0A%20%20Devices%0AAuthor%3A%20Zixing%20Lei%20and%20Zhenyang%20Ni%20and%20Ruize%20Han%20and%20Shuo%20Tang%20and%20Dingju%20Wang%20and%20Chen%20Feng%20and%20Siheng%20Chen%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20A%20consistent%20spatial-temporal%20coordination%20across%20multiple%20agents%20is%0Afundamental%20for%20collaborative%20perception%2C%20which%20seeks%20to%20improve%20perception%0Aabilities%20through%20information%20exchange%20among%20agents.%20To%20achieve%20this%0Aspatial-temporal%20alignment%2C%20traditional%20methods%20depend%20on%20external%20devices%20to%0Aprovide%20localization%20and%20clock%20signals.%20However%2C%20hardware-generated%20signals%0Acould%20be%20vulnerable%20to%20noise%20and%20potentially%20malicious%20attack%2C%20jeopardizing%20the%0Aprecision%20of%20spatial-temporal%20alignment.%20Rather%20than%20relying%20on%20external%0Ahardwares%2C%20this%20work%20proposes%20a%20novel%20approach%3A%20aligning%20by%20recognizing%20the%0Ainherent%20geometric%20patterns%20within%20the%20perceptual%20data%20of%20various%20agents.%0AFollowing%20this%20spirit%2C%20we%20propose%20a%20robust%20collaborative%20perception%20system%20that%0Aoperates%20independently%20of%20external%20localization%20and%20clock%20devices.%20The%20key%0Amodule%20of%20our%20system%2C~%5Cemph%7BFreeAlign%7D%2C%20constructs%20a%20salient%20object%20graph%20for%0Aeach%20agent%20based%20on%20its%20detected%20boxes%20and%20uses%20a%20graph%20neural%20network%20to%0Aidentify%20common%20subgraphs%20between%20agents%2C%20leading%20to%20accurate%20relative%20pose%20and%0Atime.%20We%20validate%20%5Cemph%7BFreeAlign%7D%20on%20both%20real-world%20and%20simulated%20datasets.%0AThe%20results%20show%20that%2C%20the%20~%5Cemph%7BFreeAlign%7D%20empowered%20robust%20collaborative%0Aperception%20system%20perform%20comparably%20to%20systems%20relying%20on%20precise%20localization%0Aand%20clock%20devices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02965v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Collaborative%2520Perception%2520without%2520External%2520Localization%2520and%2520Clock%250A%2520%2520Devices%26entry.906535625%3DZixing%2520Lei%2520and%2520Zhenyang%2520Ni%2520and%2520Ruize%2520Han%2520and%2520Shuo%2520Tang%2520and%2520Dingju%2520Wang%2520and%2520Chen%2520Feng%2520and%2520Siheng%2520Chen%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520A%2520consistent%2520spatial-temporal%2520coordination%2520across%2520multiple%2520agents%2520is%250Afundamental%2520for%2520collaborative%2520perception%252C%2520which%2520seeks%2520to%2520improve%2520perception%250Aabilities%2520through%2520information%2520exchange%2520among%2520agents.%2520To%2520achieve%2520this%250Aspatial-temporal%2520alignment%252C%2520traditional%2520methods%2520depend%2520on%2520external%2520devices%2520to%250Aprovide%2520localization%2520and%2520clock%2520signals.%2520However%252C%2520hardware-generated%2520signals%250Acould%2520be%2520vulnerable%2520to%2520noise%2520and%2520potentially%2520malicious%2520attack%252C%2520jeopardizing%2520the%250Aprecision%2520of%2520spatial-temporal%2520alignment.%2520Rather%2520than%2520relying%2520on%2520external%250Ahardwares%252C%2520this%2520work%2520proposes%2520a%2520novel%2520approach%253A%2520aligning%2520by%2520recognizing%2520the%250Ainherent%2520geometric%2520patterns%2520within%2520the%2520perceptual%2520data%2520of%2520various%2520agents.%250AFollowing%2520this%2520spirit%252C%2520we%2520propose%2520a%2520robust%2520collaborative%2520perception%2520system%2520that%250Aoperates%2520independently%2520of%2520external%2520localization%2520and%2520clock%2520devices.%2520The%2520key%250Amodule%2520of%2520our%2520system%252C~%255Cemph%257BFreeAlign%257D%252C%2520constructs%2520a%2520salient%2520object%2520graph%2520for%250Aeach%2520agent%2520based%2520on%2520its%2520detected%2520boxes%2520and%2520uses%2520a%2520graph%2520neural%2520network%2520to%250Aidentify%2520common%2520subgraphs%2520between%2520agents%252C%2520leading%2520to%2520accurate%2520relative%2520pose%2520and%250Atime.%2520We%2520validate%2520%255Cemph%257BFreeAlign%257D%2520on%2520both%2520real-world%2520and%2520simulated%2520datasets.%250AThe%2520results%2520show%2520that%252C%2520the%2520~%255Cemph%257BFreeAlign%257D%2520empowered%2520robust%2520collaborative%250Aperception%2520system%2520perform%2520comparably%2520to%2520systems%2520relying%2520on%2520precise%2520localization%250Aand%2520clock%2520devices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02965v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Collaborative%20Perception%20without%20External%20Localization%20and%20Clock%0A%20%20Devices&entry.906535625=Zixing%20Lei%20and%20Zhenyang%20Ni%20and%20Ruize%20Han%20and%20Shuo%20Tang%20and%20Dingju%20Wang%20and%20Chen%20Feng%20and%20Siheng%20Chen%20and%20Yanfeng%20Wang&entry.1292438233=%20%20A%20consistent%20spatial-temporal%20coordination%20across%20multiple%20agents%20is%0Afundamental%20for%20collaborative%20perception%2C%20which%20seeks%20to%20improve%20perception%0Aabilities%20through%20information%20exchange%20among%20agents.%20To%20achieve%20this%0Aspatial-temporal%20alignment%2C%20traditional%20methods%20depend%20on%20external%20devices%20to%0Aprovide%20localization%20and%20clock%20signals.%20However%2C%20hardware-generated%20signals%0Acould%20be%20vulnerable%20to%20noise%20and%20potentially%20malicious%20attack%2C%20jeopardizing%20the%0Aprecision%20of%20spatial-temporal%20alignment.%20Rather%20than%20relying%20on%20external%0Ahardwares%2C%20this%20work%20proposes%20a%20novel%20approach%3A%20aligning%20by%20recognizing%20the%0Ainherent%20geometric%20patterns%20within%20the%20perceptual%20data%20of%20various%20agents.%0AFollowing%20this%20spirit%2C%20we%20propose%20a%20robust%20collaborative%20perception%20system%20that%0Aoperates%20independently%20of%20external%20localization%20and%20clock%20devices.%20The%20key%0Amodule%20of%20our%20system%2C~%5Cemph%7BFreeAlign%7D%2C%20constructs%20a%20salient%20object%20graph%20for%0Aeach%20agent%20based%20on%20its%20detected%20boxes%20and%20uses%20a%20graph%20neural%20network%20to%0Aidentify%20common%20subgraphs%20between%20agents%2C%20leading%20to%20accurate%20relative%20pose%20and%0Atime.%20We%20validate%20%5Cemph%7BFreeAlign%7D%20on%20both%20real-world%20and%20simulated%20datasets.%0AThe%20results%20show%20that%2C%20the%20~%5Cemph%7BFreeAlign%7D%20empowered%20robust%20collaborative%0Aperception%20system%20perform%20comparably%20to%20systems%20relying%20on%20precise%20localization%0Aand%20clock%20devices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02965v2&entry.124074799=Read"},
{"title": "DP-IQA: Utilizing Diffusion Prior for Blind Image Quality Assessment in\n  the Wild", "author": "Honghao Fu and Yufei Wang and Wenhan Yang and Bihan Wen", "abstract": "  Image quality assessment (IQA) plays a critical role in selecting\nhigh-quality images and guiding compression and enhancement methods in a series\nof applications. The blind IQA, which assesses the quality of in-the-wild\nimages containing complex authentic distortions without reference images, poses\ngreater challenges. Existing methods are limited to modeling a uniform\ndistribution with local patches and are bothered by the gap between low and\nhigh-level visions (caused by widely adopted pre-trained classification\nnetworks). In this paper, we propose a novel IQA method called diffusion\npriors-based IQA (DP-IQA), which leverages the prior knowledge from the\npre-trained diffusion model with its excellent powers to bridge semantic gaps\nin the perception of the visual quality of images. Specifically, we use\npre-trained stable diffusion as the backbone, extract multi-level features from\nthe denoising U-Net during the upsampling process at a specified timestep, and\ndecode them to estimate the image quality score. The text and image adapters\nare adopted to mitigate the domain gap for downstream tasks and correct the\ninformation loss caused by the variational autoencoder bottleneck. Finally, we\ndistill the knowledge in the above model into a CNN-based student model,\nsignificantly reducing the parameter to enhance applicability, with the student\nmodel performing similarly or even better than the teacher model surprisingly.\nExperimental results demonstrate that our DP-IQA achieves state-of-the-art\nresults on various in-the-wild datasets with better generalization capability,\nwhich shows the superiority of our method in global modeling and utilizing the\nhierarchical feature clues of diffusion for evaluating image quality.\n", "link": "http://arxiv.org/abs/2405.19996v2", "date": "2024-05-31", "relevancy": 2.2934, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6329}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5632}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-IQA%3A%20Utilizing%20Diffusion%20Prior%20for%20Blind%20Image%20Quality%20Assessment%20in%0A%20%20the%20Wild&body=Title%3A%20DP-IQA%3A%20Utilizing%20Diffusion%20Prior%20for%20Blind%20Image%20Quality%20Assessment%20in%0A%20%20the%20Wild%0AAuthor%3A%20Honghao%20Fu%20and%20Yufei%20Wang%20and%20Wenhan%20Yang%20and%20Bihan%20Wen%0AAbstract%3A%20%20%20Image%20quality%20assessment%20%28IQA%29%20plays%20a%20critical%20role%20in%20selecting%0Ahigh-quality%20images%20and%20guiding%20compression%20and%20enhancement%20methods%20in%20a%20series%0Aof%20applications.%20The%20blind%20IQA%2C%20which%20assesses%20the%20quality%20of%20in-the-wild%0Aimages%20containing%20complex%20authentic%20distortions%20without%20reference%20images%2C%20poses%0Agreater%20challenges.%20Existing%20methods%20are%20limited%20to%20modeling%20a%20uniform%0Adistribution%20with%20local%20patches%20and%20are%20bothered%20by%20the%20gap%20between%20low%20and%0Ahigh-level%20visions%20%28caused%20by%20widely%20adopted%20pre-trained%20classification%0Anetworks%29.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20IQA%20method%20called%20diffusion%0Apriors-based%20IQA%20%28DP-IQA%29%2C%20which%20leverages%20the%20prior%20knowledge%20from%20the%0Apre-trained%20diffusion%20model%20with%20its%20excellent%20powers%20to%20bridge%20semantic%20gaps%0Ain%20the%20perception%20of%20the%20visual%20quality%20of%20images.%20Specifically%2C%20we%20use%0Apre-trained%20stable%20diffusion%20as%20the%20backbone%2C%20extract%20multi-level%20features%20from%0Athe%20denoising%20U-Net%20during%20the%20upsampling%20process%20at%20a%20specified%20timestep%2C%20and%0Adecode%20them%20to%20estimate%20the%20image%20quality%20score.%20The%20text%20and%20image%20adapters%0Aare%20adopted%20to%20mitigate%20the%20domain%20gap%20for%20downstream%20tasks%20and%20correct%20the%0Ainformation%20loss%20caused%20by%20the%20variational%20autoencoder%20bottleneck.%20Finally%2C%20we%0Adistill%20the%20knowledge%20in%20the%20above%20model%20into%20a%20CNN-based%20student%20model%2C%0Asignificantly%20reducing%20the%20parameter%20to%20enhance%20applicability%2C%20with%20the%20student%0Amodel%20performing%20similarly%20or%20even%20better%20than%20the%20teacher%20model%20surprisingly.%0AExperimental%20results%20demonstrate%20that%20our%20DP-IQA%20achieves%20state-of-the-art%0Aresults%20on%20various%20in-the-wild%20datasets%20with%20better%20generalization%20capability%2C%0Awhich%20shows%20the%20superiority%20of%20our%20method%20in%20global%20modeling%20and%20utilizing%20the%0Ahierarchical%20feature%20clues%20of%20diffusion%20for%20evaluating%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19996v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-IQA%253A%2520Utilizing%2520Diffusion%2520Prior%2520for%2520Blind%2520Image%2520Quality%2520Assessment%2520in%250A%2520%2520the%2520Wild%26entry.906535625%3DHonghao%2520Fu%2520and%2520Yufei%2520Wang%2520and%2520Wenhan%2520Yang%2520and%2520Bihan%2520Wen%26entry.1292438233%3D%2520%2520Image%2520quality%2520assessment%2520%2528IQA%2529%2520plays%2520a%2520critical%2520role%2520in%2520selecting%250Ahigh-quality%2520images%2520and%2520guiding%2520compression%2520and%2520enhancement%2520methods%2520in%2520a%2520series%250Aof%2520applications.%2520The%2520blind%2520IQA%252C%2520which%2520assesses%2520the%2520quality%2520of%2520in-the-wild%250Aimages%2520containing%2520complex%2520authentic%2520distortions%2520without%2520reference%2520images%252C%2520poses%250Agreater%2520challenges.%2520Existing%2520methods%2520are%2520limited%2520to%2520modeling%2520a%2520uniform%250Adistribution%2520with%2520local%2520patches%2520and%2520are%2520bothered%2520by%2520the%2520gap%2520between%2520low%2520and%250Ahigh-level%2520visions%2520%2528caused%2520by%2520widely%2520adopted%2520pre-trained%2520classification%250Anetworks%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520IQA%2520method%2520called%2520diffusion%250Apriors-based%2520IQA%2520%2528DP-IQA%2529%252C%2520which%2520leverages%2520the%2520prior%2520knowledge%2520from%2520the%250Apre-trained%2520diffusion%2520model%2520with%2520its%2520excellent%2520powers%2520to%2520bridge%2520semantic%2520gaps%250Ain%2520the%2520perception%2520of%2520the%2520visual%2520quality%2520of%2520images.%2520Specifically%252C%2520we%2520use%250Apre-trained%2520stable%2520diffusion%2520as%2520the%2520backbone%252C%2520extract%2520multi-level%2520features%2520from%250Athe%2520denoising%2520U-Net%2520during%2520the%2520upsampling%2520process%2520at%2520a%2520specified%2520timestep%252C%2520and%250Adecode%2520them%2520to%2520estimate%2520the%2520image%2520quality%2520score.%2520The%2520text%2520and%2520image%2520adapters%250Aare%2520adopted%2520to%2520mitigate%2520the%2520domain%2520gap%2520for%2520downstream%2520tasks%2520and%2520correct%2520the%250Ainformation%2520loss%2520caused%2520by%2520the%2520variational%2520autoencoder%2520bottleneck.%2520Finally%252C%2520we%250Adistill%2520the%2520knowledge%2520in%2520the%2520above%2520model%2520into%2520a%2520CNN-based%2520student%2520model%252C%250Asignificantly%2520reducing%2520the%2520parameter%2520to%2520enhance%2520applicability%252C%2520with%2520the%2520student%250Amodel%2520performing%2520similarly%2520or%2520even%2520better%2520than%2520the%2520teacher%2520model%2520surprisingly.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520DP-IQA%2520achieves%2520state-of-the-art%250Aresults%2520on%2520various%2520in-the-wild%2520datasets%2520with%2520better%2520generalization%2520capability%252C%250Awhich%2520shows%2520the%2520superiority%2520of%2520our%2520method%2520in%2520global%2520modeling%2520and%2520utilizing%2520the%250Ahierarchical%2520feature%2520clues%2520of%2520diffusion%2520for%2520evaluating%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19996v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-IQA%3A%20Utilizing%20Diffusion%20Prior%20for%20Blind%20Image%20Quality%20Assessment%20in%0A%20%20the%20Wild&entry.906535625=Honghao%20Fu%20and%20Yufei%20Wang%20and%20Wenhan%20Yang%20and%20Bihan%20Wen&entry.1292438233=%20%20Image%20quality%20assessment%20%28IQA%29%20plays%20a%20critical%20role%20in%20selecting%0Ahigh-quality%20images%20and%20guiding%20compression%20and%20enhancement%20methods%20in%20a%20series%0Aof%20applications.%20The%20blind%20IQA%2C%20which%20assesses%20the%20quality%20of%20in-the-wild%0Aimages%20containing%20complex%20authentic%20distortions%20without%20reference%20images%2C%20poses%0Agreater%20challenges.%20Existing%20methods%20are%20limited%20to%20modeling%20a%20uniform%0Adistribution%20with%20local%20patches%20and%20are%20bothered%20by%20the%20gap%20between%20low%20and%0Ahigh-level%20visions%20%28caused%20by%20widely%20adopted%20pre-trained%20classification%0Anetworks%29.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20IQA%20method%20called%20diffusion%0Apriors-based%20IQA%20%28DP-IQA%29%2C%20which%20leverages%20the%20prior%20knowledge%20from%20the%0Apre-trained%20diffusion%20model%20with%20its%20excellent%20powers%20to%20bridge%20semantic%20gaps%0Ain%20the%20perception%20of%20the%20visual%20quality%20of%20images.%20Specifically%2C%20we%20use%0Apre-trained%20stable%20diffusion%20as%20the%20backbone%2C%20extract%20multi-level%20features%20from%0Athe%20denoising%20U-Net%20during%20the%20upsampling%20process%20at%20a%20specified%20timestep%2C%20and%0Adecode%20them%20to%20estimate%20the%20image%20quality%20score.%20The%20text%20and%20image%20adapters%0Aare%20adopted%20to%20mitigate%20the%20domain%20gap%20for%20downstream%20tasks%20and%20correct%20the%0Ainformation%20loss%20caused%20by%20the%20variational%20autoencoder%20bottleneck.%20Finally%2C%20we%0Adistill%20the%20knowledge%20in%20the%20above%20model%20into%20a%20CNN-based%20student%20model%2C%0Asignificantly%20reducing%20the%20parameter%20to%20enhance%20applicability%2C%20with%20the%20student%0Amodel%20performing%20similarly%20or%20even%20better%20than%20the%20teacher%20model%20surprisingly.%0AExperimental%20results%20demonstrate%20that%20our%20DP-IQA%20achieves%20state-of-the-art%0Aresults%20on%20various%20in-the-wild%20datasets%20with%20better%20generalization%20capability%2C%0Awhich%20shows%20the%20superiority%20of%20our%20method%20in%20global%20modeling%20and%20utilizing%20the%0Ahierarchical%20feature%20clues%20of%20diffusion%20for%20evaluating%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19996v2&entry.124074799=Read"},
{"title": "MALT: Multi-scale Action Learning Transformer for Online Action\n  Detection", "author": "Zhipeng Yang and Ruoyu Wang and Yang Tan and Liping Xie", "abstract": "  Online action detection (OAD) aims to identify ongoing actions from streaming\nvideo in real-time, without access to future frames. Since these actions\nmanifest at varying scales of granularity, ranging from coarse to fine,\nprojecting an entire set of action frames to a single latent encoding may\nresult in a lack of local information, necessitating the acquisition of action\nfeatures across multiple scales. In this paper, we propose a multi-scale action\nlearning transformer (MALT), which includes a novel recurrent decoder (used for\nfeature fusion) that includes fewer parameters and can be trained more\nefficiently. A hierarchical encoder with multiple encoding branches is further\nproposed to capture multi-scale action features. The output from the preceding\nbranch is then incrementally input to the subsequent branch as part of a\ncross-attention calculation. In this way, output features transition from\ncoarse to fine as the branches deepen. We also introduce an explicit frame\nscoring mechanism employing sparse attention, which filters irrelevant frames\nmore efficiently, without requiring an additional network. The proposed method\nachieved state-of-the-art performance on two benchmark datasets (THUMOS'14 and\nTVSeries), outperforming all existing models used for comparison, with an mAP\nof 0.2% for THUMOS'14 and an mcAP of 0.1% for TVseries.\n", "link": "http://arxiv.org/abs/2405.20892v1", "date": "2024-05-31", "relevancy": 2.2846, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5892}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5708}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MALT%3A%20Multi-scale%20Action%20Learning%20Transformer%20for%20Online%20Action%0A%20%20Detection&body=Title%3A%20MALT%3A%20Multi-scale%20Action%20Learning%20Transformer%20for%20Online%20Action%0A%20%20Detection%0AAuthor%3A%20Zhipeng%20Yang%20and%20Ruoyu%20Wang%20and%20Yang%20Tan%20and%20Liping%20Xie%0AAbstract%3A%20%20%20Online%20action%20detection%20%28OAD%29%20aims%20to%20identify%20ongoing%20actions%20from%20streaming%0Avideo%20in%20real-time%2C%20without%20access%20to%20future%20frames.%20Since%20these%20actions%0Amanifest%20at%20varying%20scales%20of%20granularity%2C%20ranging%20from%20coarse%20to%20fine%2C%0Aprojecting%20an%20entire%20set%20of%20action%20frames%20to%20a%20single%20latent%20encoding%20may%0Aresult%20in%20a%20lack%20of%20local%20information%2C%20necessitating%20the%20acquisition%20of%20action%0Afeatures%20across%20multiple%20scales.%20In%20this%20paper%2C%20we%20propose%20a%20multi-scale%20action%0Alearning%20transformer%20%28MALT%29%2C%20which%20includes%20a%20novel%20recurrent%20decoder%20%28used%20for%0Afeature%20fusion%29%20that%20includes%20fewer%20parameters%20and%20can%20be%20trained%20more%0Aefficiently.%20A%20hierarchical%20encoder%20with%20multiple%20encoding%20branches%20is%20further%0Aproposed%20to%20capture%20multi-scale%20action%20features.%20The%20output%20from%20the%20preceding%0Abranch%20is%20then%20incrementally%20input%20to%20the%20subsequent%20branch%20as%20part%20of%20a%0Across-attention%20calculation.%20In%20this%20way%2C%20output%20features%20transition%20from%0Acoarse%20to%20fine%20as%20the%20branches%20deepen.%20We%20also%20introduce%20an%20explicit%20frame%0Ascoring%20mechanism%20employing%20sparse%20attention%2C%20which%20filters%20irrelevant%20frames%0Amore%20efficiently%2C%20without%20requiring%20an%20additional%20network.%20The%20proposed%20method%0Aachieved%20state-of-the-art%20performance%20on%20two%20benchmark%20datasets%20%28THUMOS%2714%20and%0ATVSeries%29%2C%20outperforming%20all%20existing%20models%20used%20for%20comparison%2C%20with%20an%20mAP%0Aof%200.2%25%20for%20THUMOS%2714%20and%20an%20mcAP%20of%200.1%25%20for%20TVseries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMALT%253A%2520Multi-scale%2520Action%2520Learning%2520Transformer%2520for%2520Online%2520Action%250A%2520%2520Detection%26entry.906535625%3DZhipeng%2520Yang%2520and%2520Ruoyu%2520Wang%2520and%2520Yang%2520Tan%2520and%2520Liping%2520Xie%26entry.1292438233%3D%2520%2520Online%2520action%2520detection%2520%2528OAD%2529%2520aims%2520to%2520identify%2520ongoing%2520actions%2520from%2520streaming%250Avideo%2520in%2520real-time%252C%2520without%2520access%2520to%2520future%2520frames.%2520Since%2520these%2520actions%250Amanifest%2520at%2520varying%2520scales%2520of%2520granularity%252C%2520ranging%2520from%2520coarse%2520to%2520fine%252C%250Aprojecting%2520an%2520entire%2520set%2520of%2520action%2520frames%2520to%2520a%2520single%2520latent%2520encoding%2520may%250Aresult%2520in%2520a%2520lack%2520of%2520local%2520information%252C%2520necessitating%2520the%2520acquisition%2520of%2520action%250Afeatures%2520across%2520multiple%2520scales.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520multi-scale%2520action%250Alearning%2520transformer%2520%2528MALT%2529%252C%2520which%2520includes%2520a%2520novel%2520recurrent%2520decoder%2520%2528used%2520for%250Afeature%2520fusion%2529%2520that%2520includes%2520fewer%2520parameters%2520and%2520can%2520be%2520trained%2520more%250Aefficiently.%2520A%2520hierarchical%2520encoder%2520with%2520multiple%2520encoding%2520branches%2520is%2520further%250Aproposed%2520to%2520capture%2520multi-scale%2520action%2520features.%2520The%2520output%2520from%2520the%2520preceding%250Abranch%2520is%2520then%2520incrementally%2520input%2520to%2520the%2520subsequent%2520branch%2520as%2520part%2520of%2520a%250Across-attention%2520calculation.%2520In%2520this%2520way%252C%2520output%2520features%2520transition%2520from%250Acoarse%2520to%2520fine%2520as%2520the%2520branches%2520deepen.%2520We%2520also%2520introduce%2520an%2520explicit%2520frame%250Ascoring%2520mechanism%2520employing%2520sparse%2520attention%252C%2520which%2520filters%2520irrelevant%2520frames%250Amore%2520efficiently%252C%2520without%2520requiring%2520an%2520additional%2520network.%2520The%2520proposed%2520method%250Aachieved%2520state-of-the-art%2520performance%2520on%2520two%2520benchmark%2520datasets%2520%2528THUMOS%252714%2520and%250ATVSeries%2529%252C%2520outperforming%2520all%2520existing%2520models%2520used%2520for%2520comparison%252C%2520with%2520an%2520mAP%250Aof%25200.2%2525%2520for%2520THUMOS%252714%2520and%2520an%2520mcAP%2520of%25200.1%2525%2520for%2520TVseries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MALT%3A%20Multi-scale%20Action%20Learning%20Transformer%20for%20Online%20Action%0A%20%20Detection&entry.906535625=Zhipeng%20Yang%20and%20Ruoyu%20Wang%20and%20Yang%20Tan%20and%20Liping%20Xie&entry.1292438233=%20%20Online%20action%20detection%20%28OAD%29%20aims%20to%20identify%20ongoing%20actions%20from%20streaming%0Avideo%20in%20real-time%2C%20without%20access%20to%20future%20frames.%20Since%20these%20actions%0Amanifest%20at%20varying%20scales%20of%20granularity%2C%20ranging%20from%20coarse%20to%20fine%2C%0Aprojecting%20an%20entire%20set%20of%20action%20frames%20to%20a%20single%20latent%20encoding%20may%0Aresult%20in%20a%20lack%20of%20local%20information%2C%20necessitating%20the%20acquisition%20of%20action%0Afeatures%20across%20multiple%20scales.%20In%20this%20paper%2C%20we%20propose%20a%20multi-scale%20action%0Alearning%20transformer%20%28MALT%29%2C%20which%20includes%20a%20novel%20recurrent%20decoder%20%28used%20for%0Afeature%20fusion%29%20that%20includes%20fewer%20parameters%20and%20can%20be%20trained%20more%0Aefficiently.%20A%20hierarchical%20encoder%20with%20multiple%20encoding%20branches%20is%20further%0Aproposed%20to%20capture%20multi-scale%20action%20features.%20The%20output%20from%20the%20preceding%0Abranch%20is%20then%20incrementally%20input%20to%20the%20subsequent%20branch%20as%20part%20of%20a%0Across-attention%20calculation.%20In%20this%20way%2C%20output%20features%20transition%20from%0Acoarse%20to%20fine%20as%20the%20branches%20deepen.%20We%20also%20introduce%20an%20explicit%20frame%0Ascoring%20mechanism%20employing%20sparse%20attention%2C%20which%20filters%20irrelevant%20frames%0Amore%20efficiently%2C%20without%20requiring%20an%20additional%20network.%20The%20proposed%20method%0Aachieved%20state-of-the-art%20performance%20on%20two%20benchmark%20datasets%20%28THUMOS%2714%20and%0ATVSeries%29%2C%20outperforming%20all%20existing%20models%20used%20for%20comparison%2C%20with%20an%20mAP%0Aof%200.2%25%20for%20THUMOS%2714%20and%20an%20mcAP%20of%200.1%25%20for%20TVseries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20892v1&entry.124074799=Read"},
{"title": "MEGA: Masked Generative Autoencoder for Human Mesh Recovery", "author": "Gu\u00e9nol\u00e9 Fiche and Simon Leglaive and Xavier Alameda-Pineda and Francesc Moreno-Noguer", "abstract": "  Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous\nproblem, as similar 2D projections can correspond to multiple 3D\ninterpretations. Nevertheless, most HMR methods overlook this ambiguity and\nmake a single prediction without accounting for the associated uncertainty. A\nfew approaches generate a distribution of human meshes, enabling the sampling\nof multiple predictions; however, none of them is competitive with the latest\nsingle-output model when making a single prediction. This work proposes a new\napproach based on masked generative modeling. By tokenizing the human pose and\nshape, we formulate the HMR task as generating a sequence of discrete tokens\nconditioned on an input image. We introduce MEGA, a MaskEd Generative\nAutoencoder trained to recover human meshes from images and partial human mesh\ntoken sequences. Given an image, our flexible generation scheme allows us to\npredict a single human mesh in deterministic mode or to generate multiple human\nmeshes in stochastic mode. MEGA enables us to propose multiple outputs and to\nevaluate the uncertainty of the predictions. Experiments on in-the-wild\nbenchmarks show that MEGA achieves state-of-the-art performance in\ndeterministic and stochastic modes, outperforming single-output and\nmulti-output approaches.\n", "link": "http://arxiv.org/abs/2405.18839v2", "date": "2024-05-31", "relevancy": 2.2789, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.59}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5581}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEGA%3A%20Masked%20Generative%20Autoencoder%20for%20Human%20Mesh%20Recovery&body=Title%3A%20MEGA%3A%20Masked%20Generative%20Autoencoder%20for%20Human%20Mesh%20Recovery%0AAuthor%3A%20Gu%C3%A9nol%C3%A9%20Fiche%20and%20Simon%20Leglaive%20and%20Xavier%20Alameda-Pineda%20and%20Francesc%20Moreno-Noguer%0AAbstract%3A%20%20%20Human%20Mesh%20Recovery%20%28HMR%29%20from%20a%20single%20RGB%20image%20is%20a%20highly%20ambiguous%0Aproblem%2C%20as%20similar%202D%20projections%20can%20correspond%20to%20multiple%203D%0Ainterpretations.%20Nevertheless%2C%20most%20HMR%20methods%20overlook%20this%20ambiguity%20and%0Amake%20a%20single%20prediction%20without%20accounting%20for%20the%20associated%20uncertainty.%20A%0Afew%20approaches%20generate%20a%20distribution%20of%20human%20meshes%2C%20enabling%20the%20sampling%0Aof%20multiple%20predictions%3B%20however%2C%20none%20of%20them%20is%20competitive%20with%20the%20latest%0Asingle-output%20model%20when%20making%20a%20single%20prediction.%20This%20work%20proposes%20a%20new%0Aapproach%20based%20on%20masked%20generative%20modeling.%20By%20tokenizing%20the%20human%20pose%20and%0Ashape%2C%20we%20formulate%20the%20HMR%20task%20as%20generating%20a%20sequence%20of%20discrete%20tokens%0Aconditioned%20on%20an%20input%20image.%20We%20introduce%20MEGA%2C%20a%20MaskEd%20Generative%0AAutoencoder%20trained%20to%20recover%20human%20meshes%20from%20images%20and%20partial%20human%20mesh%0Atoken%20sequences.%20Given%20an%20image%2C%20our%20flexible%20generation%20scheme%20allows%20us%20to%0Apredict%20a%20single%20human%20mesh%20in%20deterministic%20mode%20or%20to%20generate%20multiple%20human%0Ameshes%20in%20stochastic%20mode.%20MEGA%20enables%20us%20to%20propose%20multiple%20outputs%20and%20to%0Aevaluate%20the%20uncertainty%20of%20the%20predictions.%20Experiments%20on%20in-the-wild%0Abenchmarks%20show%20that%20MEGA%20achieves%20state-of-the-art%20performance%20in%0Adeterministic%20and%20stochastic%20modes%2C%20outperforming%20single-output%20and%0Amulti-output%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEGA%253A%2520Masked%2520Generative%2520Autoencoder%2520for%2520Human%2520Mesh%2520Recovery%26entry.906535625%3DGu%25C3%25A9nol%25C3%25A9%2520Fiche%2520and%2520Simon%2520Leglaive%2520and%2520Xavier%2520Alameda-Pineda%2520and%2520Francesc%2520Moreno-Noguer%26entry.1292438233%3D%2520%2520Human%2520Mesh%2520Recovery%2520%2528HMR%2529%2520from%2520a%2520single%2520RGB%2520image%2520is%2520a%2520highly%2520ambiguous%250Aproblem%252C%2520as%2520similar%25202D%2520projections%2520can%2520correspond%2520to%2520multiple%25203D%250Ainterpretations.%2520Nevertheless%252C%2520most%2520HMR%2520methods%2520overlook%2520this%2520ambiguity%2520and%250Amake%2520a%2520single%2520prediction%2520without%2520accounting%2520for%2520the%2520associated%2520uncertainty.%2520A%250Afew%2520approaches%2520generate%2520a%2520distribution%2520of%2520human%2520meshes%252C%2520enabling%2520the%2520sampling%250Aof%2520multiple%2520predictions%253B%2520however%252C%2520none%2520of%2520them%2520is%2520competitive%2520with%2520the%2520latest%250Asingle-output%2520model%2520when%2520making%2520a%2520single%2520prediction.%2520This%2520work%2520proposes%2520a%2520new%250Aapproach%2520based%2520on%2520masked%2520generative%2520modeling.%2520By%2520tokenizing%2520the%2520human%2520pose%2520and%250Ashape%252C%2520we%2520formulate%2520the%2520HMR%2520task%2520as%2520generating%2520a%2520sequence%2520of%2520discrete%2520tokens%250Aconditioned%2520on%2520an%2520input%2520image.%2520We%2520introduce%2520MEGA%252C%2520a%2520MaskEd%2520Generative%250AAutoencoder%2520trained%2520to%2520recover%2520human%2520meshes%2520from%2520images%2520and%2520partial%2520human%2520mesh%250Atoken%2520sequences.%2520Given%2520an%2520image%252C%2520our%2520flexible%2520generation%2520scheme%2520allows%2520us%2520to%250Apredict%2520a%2520single%2520human%2520mesh%2520in%2520deterministic%2520mode%2520or%2520to%2520generate%2520multiple%2520human%250Ameshes%2520in%2520stochastic%2520mode.%2520MEGA%2520enables%2520us%2520to%2520propose%2520multiple%2520outputs%2520and%2520to%250Aevaluate%2520the%2520uncertainty%2520of%2520the%2520predictions.%2520Experiments%2520on%2520in-the-wild%250Abenchmarks%2520show%2520that%2520MEGA%2520achieves%2520state-of-the-art%2520performance%2520in%250Adeterministic%2520and%2520stochastic%2520modes%252C%2520outperforming%2520single-output%2520and%250Amulti-output%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEGA%3A%20Masked%20Generative%20Autoencoder%20for%20Human%20Mesh%20Recovery&entry.906535625=Gu%C3%A9nol%C3%A9%20Fiche%20and%20Simon%20Leglaive%20and%20Xavier%20Alameda-Pineda%20and%20Francesc%20Moreno-Noguer&entry.1292438233=%20%20Human%20Mesh%20Recovery%20%28HMR%29%20from%20a%20single%20RGB%20image%20is%20a%20highly%20ambiguous%0Aproblem%2C%20as%20similar%202D%20projections%20can%20correspond%20to%20multiple%203D%0Ainterpretations.%20Nevertheless%2C%20most%20HMR%20methods%20overlook%20this%20ambiguity%20and%0Amake%20a%20single%20prediction%20without%20accounting%20for%20the%20associated%20uncertainty.%20A%0Afew%20approaches%20generate%20a%20distribution%20of%20human%20meshes%2C%20enabling%20the%20sampling%0Aof%20multiple%20predictions%3B%20however%2C%20none%20of%20them%20is%20competitive%20with%20the%20latest%0Asingle-output%20model%20when%20making%20a%20single%20prediction.%20This%20work%20proposes%20a%20new%0Aapproach%20based%20on%20masked%20generative%20modeling.%20By%20tokenizing%20the%20human%20pose%20and%0Ashape%2C%20we%20formulate%20the%20HMR%20task%20as%20generating%20a%20sequence%20of%20discrete%20tokens%0Aconditioned%20on%20an%20input%20image.%20We%20introduce%20MEGA%2C%20a%20MaskEd%20Generative%0AAutoencoder%20trained%20to%20recover%20human%20meshes%20from%20images%20and%20partial%20human%20mesh%0Atoken%20sequences.%20Given%20an%20image%2C%20our%20flexible%20generation%20scheme%20allows%20us%20to%0Apredict%20a%20single%20human%20mesh%20in%20deterministic%20mode%20or%20to%20generate%20multiple%20human%0Ameshes%20in%20stochastic%20mode.%20MEGA%20enables%20us%20to%20propose%20multiple%20outputs%20and%20to%0Aevaluate%20the%20uncertainty%20of%20the%20predictions.%20Experiments%20on%20in-the-wild%0Abenchmarks%20show%20that%20MEGA%20achieves%20state-of-the-art%20performance%20in%0Adeterministic%20and%20stochastic%20modes%2C%20outperforming%20single-output%20and%0Amulti-output%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18839v2&entry.124074799=Read"},
{"title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of\n  Multi-modal LLMs in Video Analysis", "author": "Chaoyou Fu and Yuhan Dai and Yondong Luo and Lei Li and Shuhuai Ren and Renrui Zhang and Zihan Wang and Chenyu Zhou and Yunhang Shen and Mengdan Zhang and Peixian Chen and Yanwei Li and Shaohui Lin and Sirui Zhao and Ke Li and Tong Xu and Xiawu Zheng and Enhong Chen and Rongrong Ji and Xing Sun", "abstract": "  In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 256 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io\n", "link": "http://arxiv.org/abs/2405.21075v1", "date": "2024-05-31", "relevancy": 2.2673, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5738}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5635}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-MME%3A%20The%20First-Ever%20Comprehensive%20Evaluation%20Benchmark%20of%0A%20%20Multi-modal%20LLMs%20in%20Video%20Analysis&body=Title%3A%20Video-MME%3A%20The%20First-Ever%20Comprehensive%20Evaluation%20Benchmark%20of%0A%20%20Multi-modal%20LLMs%20in%20Video%20Analysis%0AAuthor%3A%20Chaoyou%20Fu%20and%20Yuhan%20Dai%20and%20Yondong%20Luo%20and%20Lei%20Li%20and%20Shuhuai%20Ren%20and%20Renrui%20Zhang%20and%20Zihan%20Wang%20and%20Chenyu%20Zhou%20and%20Yunhang%20Shen%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Yanwei%20Li%20and%20Shaohui%20Lin%20and%20Sirui%20Zhao%20and%20Ke%20Li%20and%20Tong%20Xu%20and%20Xiawu%20Zheng%20and%20Enhong%20Chen%20and%20Rongrong%20Ji%20and%20Xing%20Sun%0AAbstract%3A%20%20%20In%20the%20quest%20for%20artificial%20general%20intelligence%2C%20Multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%20have%20emerged%20as%20a%20focal%20point%20in%20recent%20advancements.%20However%2C%0Athe%20predominant%20focus%20remains%20on%20developing%20their%20capabilities%20in%20static%20image%0Aunderstanding.%20The%20potential%20of%20MLLMs%20in%20processing%20sequential%20visual%20data%20is%0Astill%20insufficiently%20explored%2C%20highlighting%20the%20absence%20of%20a%20comprehensive%2C%0Ahigh-quality%20assessment%20of%20their%20performance.%20In%20this%20paper%2C%20we%20introduce%0AVideo-MME%2C%20the%20first-ever%20full-spectrum%2C%20Multi-Modal%20Evaluation%20benchmark%20of%0AMLLMs%20in%20Video%20analysis.%20Our%20work%20distinguishes%20from%20existing%20benchmarks%0Athrough%20four%20key%20features%3A%201%29%20Diversity%20in%20video%20types%2C%20spanning%206%20primary%0Avisual%20domains%20with%2030%20subfields%20to%20ensure%20broad%20scenario%20generalizability%3B%202%29%0ADuration%20in%20temporal%20dimension%2C%20encompassing%20both%20short-%2C%20medium-%2C%20and%0Along-term%20videos%2C%20ranging%20from%2011%20seconds%20to%201%20hour%2C%20for%20robust%20contextual%0Adynamics%3B%203%29%20Breadth%20in%20data%20modalities%2C%20integrating%20multi-modal%20inputs%20besides%0Avideo%20frames%2C%20including%20subtitles%20and%20audios%2C%20to%20unveil%20the%20all-round%0Acapabilities%20of%20MLLMs%3B%204%29%20Quality%20in%20annotations%2C%20utilizing%20rigorous%20manual%0Alabeling%20by%20expert%20annotators%20to%20facilitate%20precise%20and%20reliable%20model%0Aassessment.%20900%20videos%20with%20a%20total%20of%20256%20hours%20are%20manually%20selected%20and%0Aannotated%20by%20repeatedly%20viewing%20all%20the%20video%20content%2C%20resulting%20in%202%2C700%0Aquestion-answer%20pairs.%20With%20Video-MME%2C%20we%20extensively%20evaluate%20various%0Astate-of-the-art%20MLLMs%2C%20including%20GPT-4%20series%20and%20Gemini%201.5%20Pro%2C%20as%20well%20as%0Aopen-source%20image%20models%20like%20InternVL-Chat-V1.5%20and%20video%20models%20like%0ALLaVA-NeXT-Video.%20Our%20experiments%20reveal%20that%20Gemini%201.5%20Pro%20is%20the%0Abest-performing%20commercial%20model%2C%20significantly%20outperforming%20the%20open-source%0Amodels.%20Our%20dataset%20along%20with%20these%20findings%20underscores%20the%20need%20for%20further%0Aimprovements%20in%20handling%20longer%20sequences%20and%20multi-modal%20data.%20Project%20Page%3A%0Ahttps%3A//video-mme.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-MME%253A%2520The%2520First-Ever%2520Comprehensive%2520Evaluation%2520Benchmark%2520of%250A%2520%2520Multi-modal%2520LLMs%2520in%2520Video%2520Analysis%26entry.906535625%3DChaoyou%2520Fu%2520and%2520Yuhan%2520Dai%2520and%2520Yondong%2520Luo%2520and%2520Lei%2520Li%2520and%2520Shuhuai%2520Ren%2520and%2520Renrui%2520Zhang%2520and%2520Zihan%2520Wang%2520and%2520Chenyu%2520Zhou%2520and%2520Yunhang%2520Shen%2520and%2520Mengdan%2520Zhang%2520and%2520Peixian%2520Chen%2520and%2520Yanwei%2520Li%2520and%2520Shaohui%2520Lin%2520and%2520Sirui%2520Zhao%2520and%2520Ke%2520Li%2520and%2520Tong%2520Xu%2520and%2520Xiawu%2520Zheng%2520and%2520Enhong%2520Chen%2520and%2520Rongrong%2520Ji%2520and%2520Xing%2520Sun%26entry.1292438233%3D%2520%2520In%2520the%2520quest%2520for%2520artificial%2520general%2520intelligence%252C%2520Multi-modal%2520Large%2520Language%250AModels%2520%2528MLLMs%2529%2520have%2520emerged%2520as%2520a%2520focal%2520point%2520in%2520recent%2520advancements.%2520However%252C%250Athe%2520predominant%2520focus%2520remains%2520on%2520developing%2520their%2520capabilities%2520in%2520static%2520image%250Aunderstanding.%2520The%2520potential%2520of%2520MLLMs%2520in%2520processing%2520sequential%2520visual%2520data%2520is%250Astill%2520insufficiently%2520explored%252C%2520highlighting%2520the%2520absence%2520of%2520a%2520comprehensive%252C%250Ahigh-quality%2520assessment%2520of%2520their%2520performance.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AVideo-MME%252C%2520the%2520first-ever%2520full-spectrum%252C%2520Multi-Modal%2520Evaluation%2520benchmark%2520of%250AMLLMs%2520in%2520Video%2520analysis.%2520Our%2520work%2520distinguishes%2520from%2520existing%2520benchmarks%250Athrough%2520four%2520key%2520features%253A%25201%2529%2520Diversity%2520in%2520video%2520types%252C%2520spanning%25206%2520primary%250Avisual%2520domains%2520with%252030%2520subfields%2520to%2520ensure%2520broad%2520scenario%2520generalizability%253B%25202%2529%250ADuration%2520in%2520temporal%2520dimension%252C%2520encompassing%2520both%2520short-%252C%2520medium-%252C%2520and%250Along-term%2520videos%252C%2520ranging%2520from%252011%2520seconds%2520to%25201%2520hour%252C%2520for%2520robust%2520contextual%250Adynamics%253B%25203%2529%2520Breadth%2520in%2520data%2520modalities%252C%2520integrating%2520multi-modal%2520inputs%2520besides%250Avideo%2520frames%252C%2520including%2520subtitles%2520and%2520audios%252C%2520to%2520unveil%2520the%2520all-round%250Acapabilities%2520of%2520MLLMs%253B%25204%2529%2520Quality%2520in%2520annotations%252C%2520utilizing%2520rigorous%2520manual%250Alabeling%2520by%2520expert%2520annotators%2520to%2520facilitate%2520precise%2520and%2520reliable%2520model%250Aassessment.%2520900%2520videos%2520with%2520a%2520total%2520of%2520256%2520hours%2520are%2520manually%2520selected%2520and%250Aannotated%2520by%2520repeatedly%2520viewing%2520all%2520the%2520video%2520content%252C%2520resulting%2520in%25202%252C700%250Aquestion-answer%2520pairs.%2520With%2520Video-MME%252C%2520we%2520extensively%2520evaluate%2520various%250Astate-of-the-art%2520MLLMs%252C%2520including%2520GPT-4%2520series%2520and%2520Gemini%25201.5%2520Pro%252C%2520as%2520well%2520as%250Aopen-source%2520image%2520models%2520like%2520InternVL-Chat-V1.5%2520and%2520video%2520models%2520like%250ALLaVA-NeXT-Video.%2520Our%2520experiments%2520reveal%2520that%2520Gemini%25201.5%2520Pro%2520is%2520the%250Abest-performing%2520commercial%2520model%252C%2520significantly%2520outperforming%2520the%2520open-source%250Amodels.%2520Our%2520dataset%2520along%2520with%2520these%2520findings%2520underscores%2520the%2520need%2520for%2520further%250Aimprovements%2520in%2520handling%2520longer%2520sequences%2520and%2520multi-modal%2520data.%2520Project%2520Page%253A%250Ahttps%253A//video-mme.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-MME%3A%20The%20First-Ever%20Comprehensive%20Evaluation%20Benchmark%20of%0A%20%20Multi-modal%20LLMs%20in%20Video%20Analysis&entry.906535625=Chaoyou%20Fu%20and%20Yuhan%20Dai%20and%20Yondong%20Luo%20and%20Lei%20Li%20and%20Shuhuai%20Ren%20and%20Renrui%20Zhang%20and%20Zihan%20Wang%20and%20Chenyu%20Zhou%20and%20Yunhang%20Shen%20and%20Mengdan%20Zhang%20and%20Peixian%20Chen%20and%20Yanwei%20Li%20and%20Shaohui%20Lin%20and%20Sirui%20Zhao%20and%20Ke%20Li%20and%20Tong%20Xu%20and%20Xiawu%20Zheng%20and%20Enhong%20Chen%20and%20Rongrong%20Ji%20and%20Xing%20Sun&entry.1292438233=%20%20In%20the%20quest%20for%20artificial%20general%20intelligence%2C%20Multi-modal%20Large%20Language%0AModels%20%28MLLMs%29%20have%20emerged%20as%20a%20focal%20point%20in%20recent%20advancements.%20However%2C%0Athe%20predominant%20focus%20remains%20on%20developing%20their%20capabilities%20in%20static%20image%0Aunderstanding.%20The%20potential%20of%20MLLMs%20in%20processing%20sequential%20visual%20data%20is%0Astill%20insufficiently%20explored%2C%20highlighting%20the%20absence%20of%20a%20comprehensive%2C%0Ahigh-quality%20assessment%20of%20their%20performance.%20In%20this%20paper%2C%20we%20introduce%0AVideo-MME%2C%20the%20first-ever%20full-spectrum%2C%20Multi-Modal%20Evaluation%20benchmark%20of%0AMLLMs%20in%20Video%20analysis.%20Our%20work%20distinguishes%20from%20existing%20benchmarks%0Athrough%20four%20key%20features%3A%201%29%20Diversity%20in%20video%20types%2C%20spanning%206%20primary%0Avisual%20domains%20with%2030%20subfields%20to%20ensure%20broad%20scenario%20generalizability%3B%202%29%0ADuration%20in%20temporal%20dimension%2C%20encompassing%20both%20short-%2C%20medium-%2C%20and%0Along-term%20videos%2C%20ranging%20from%2011%20seconds%20to%201%20hour%2C%20for%20robust%20contextual%0Adynamics%3B%203%29%20Breadth%20in%20data%20modalities%2C%20integrating%20multi-modal%20inputs%20besides%0Avideo%20frames%2C%20including%20subtitles%20and%20audios%2C%20to%20unveil%20the%20all-round%0Acapabilities%20of%20MLLMs%3B%204%29%20Quality%20in%20annotations%2C%20utilizing%20rigorous%20manual%0Alabeling%20by%20expert%20annotators%20to%20facilitate%20precise%20and%20reliable%20model%0Aassessment.%20900%20videos%20with%20a%20total%20of%20256%20hours%20are%20manually%20selected%20and%0Aannotated%20by%20repeatedly%20viewing%20all%20the%20video%20content%2C%20resulting%20in%202%2C700%0Aquestion-answer%20pairs.%20With%20Video-MME%2C%20we%20extensively%20evaluate%20various%0Astate-of-the-art%20MLLMs%2C%20including%20GPT-4%20series%20and%20Gemini%201.5%20Pro%2C%20as%20well%20as%0Aopen-source%20image%20models%20like%20InternVL-Chat-V1.5%20and%20video%20models%20like%0ALLaVA-NeXT-Video.%20Our%20experiments%20reveal%20that%20Gemini%201.5%20Pro%20is%20the%0Abest-performing%20commercial%20model%2C%20significantly%20outperforming%20the%20open-source%0Amodels.%20Our%20dataset%20along%20with%20these%20findings%20underscores%20the%20need%20for%20further%0Aimprovements%20in%20handling%20longer%20sequences%20and%20multi-modal%20data.%20Project%20Page%3A%0Ahttps%3A//video-mme.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21075v1&entry.124074799=Read"},
{"title": "EvoluNet: Advancing Dynamic Non-IID Transfer Learning on Graphs", "author": "Haohui Wang and Yuzhen Mao and Yujun Yan and Yaoqing Yang and Jianhui Sun and Kevin Choi and Balaji Veeramani and Alison Hu and Edward Bowen and Tyler Cody and Dawei Zhou", "abstract": "  Non-IID transfer learning on graphs is crucial in many high-stakes domains.\nThe majority of existing works assume stationary distribution for both source\nand target domains. However, real-world graphs are intrinsically dynamic,\npresenting challenges in terms of domain evolution and dynamic discrepancy\nbetween source and target domains. To bridge the gap, we shift the problem to\nthe dynamic setting and pose the question: given the label-rich source graphs\nand the label-scarce target graphs both observed in previous T timestamps, how\ncan we effectively characterize the evolving domain discrepancy and optimize\nthe generalization performance of the target domain at the incoming T+1\ntimestamp? To answer it, we propose a generalization bound for dynamic non-IID\ntransfer learning on graphs, which implies the generalization performance is\ndominated by domain evolution and domain discrepancy between source and target\ngraphs. Inspired by the theoretical results, we introduce a novel generic\nframework named EvoluNet. It leverages a transformer-based temporal encoding\nmodule to model temporal information of the evolving domains and then uses a\ndynamic domain unification module to efficiently learn domain-invariant\nrepresentations across the source and target domains. Finally, EvoluNet\noutperforms the state-of-the-art models by up to 12.1%, demonstrating its\neffectiveness in transferring knowledge from dynamic source graphs to dynamic\ntarget graphs.\n", "link": "http://arxiv.org/abs/2305.00664v5", "date": "2024-05-31", "relevancy": 2.2638, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6334}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5421}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EvoluNet%3A%20Advancing%20Dynamic%20Non-IID%20Transfer%20Learning%20on%20Graphs&body=Title%3A%20EvoluNet%3A%20Advancing%20Dynamic%20Non-IID%20Transfer%20Learning%20on%20Graphs%0AAuthor%3A%20Haohui%20Wang%20and%20Yuzhen%20Mao%20and%20Yujun%20Yan%20and%20Yaoqing%20Yang%20and%20Jianhui%20Sun%20and%20Kevin%20Choi%20and%20Balaji%20Veeramani%20and%20Alison%20Hu%20and%20Edward%20Bowen%20and%20Tyler%20Cody%20and%20Dawei%20Zhou%0AAbstract%3A%20%20%20Non-IID%20transfer%20learning%20on%20graphs%20is%20crucial%20in%20many%20high-stakes%20domains.%0AThe%20majority%20of%20existing%20works%20assume%20stationary%20distribution%20for%20both%20source%0Aand%20target%20domains.%20However%2C%20real-world%20graphs%20are%20intrinsically%20dynamic%2C%0Apresenting%20challenges%20in%20terms%20of%20domain%20evolution%20and%20dynamic%20discrepancy%0Abetween%20source%20and%20target%20domains.%20To%20bridge%20the%20gap%2C%20we%20shift%20the%20problem%20to%0Athe%20dynamic%20setting%20and%20pose%20the%20question%3A%20given%20the%20label-rich%20source%20graphs%0Aand%20the%20label-scarce%20target%20graphs%20both%20observed%20in%20previous%20T%20timestamps%2C%20how%0Acan%20we%20effectively%20characterize%20the%20evolving%20domain%20discrepancy%20and%20optimize%0Athe%20generalization%20performance%20of%20the%20target%20domain%20at%20the%20incoming%20T%2B1%0Atimestamp%3F%20To%20answer%20it%2C%20we%20propose%20a%20generalization%20bound%20for%20dynamic%20non-IID%0Atransfer%20learning%20on%20graphs%2C%20which%20implies%20the%20generalization%20performance%20is%0Adominated%20by%20domain%20evolution%20and%20domain%20discrepancy%20between%20source%20and%20target%0Agraphs.%20Inspired%20by%20the%20theoretical%20results%2C%20we%20introduce%20a%20novel%20generic%0Aframework%20named%20EvoluNet.%20It%20leverages%20a%20transformer-based%20temporal%20encoding%0Amodule%20to%20model%20temporal%20information%20of%20the%20evolving%20domains%20and%20then%20uses%20a%0Adynamic%20domain%20unification%20module%20to%20efficiently%20learn%20domain-invariant%0Arepresentations%20across%20the%20source%20and%20target%20domains.%20Finally%2C%20EvoluNet%0Aoutperforms%20the%20state-of-the-art%20models%20by%20up%20to%2012.1%25%2C%20demonstrating%20its%0Aeffectiveness%20in%20transferring%20knowledge%20from%20dynamic%20source%20graphs%20to%20dynamic%0Atarget%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.00664v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvoluNet%253A%2520Advancing%2520Dynamic%2520Non-IID%2520Transfer%2520Learning%2520on%2520Graphs%26entry.906535625%3DHaohui%2520Wang%2520and%2520Yuzhen%2520Mao%2520and%2520Yujun%2520Yan%2520and%2520Yaoqing%2520Yang%2520and%2520Jianhui%2520Sun%2520and%2520Kevin%2520Choi%2520and%2520Balaji%2520Veeramani%2520and%2520Alison%2520Hu%2520and%2520Edward%2520Bowen%2520and%2520Tyler%2520Cody%2520and%2520Dawei%2520Zhou%26entry.1292438233%3D%2520%2520Non-IID%2520transfer%2520learning%2520on%2520graphs%2520is%2520crucial%2520in%2520many%2520high-stakes%2520domains.%250AThe%2520majority%2520of%2520existing%2520works%2520assume%2520stationary%2520distribution%2520for%2520both%2520source%250Aand%2520target%2520domains.%2520However%252C%2520real-world%2520graphs%2520are%2520intrinsically%2520dynamic%252C%250Apresenting%2520challenges%2520in%2520terms%2520of%2520domain%2520evolution%2520and%2520dynamic%2520discrepancy%250Abetween%2520source%2520and%2520target%2520domains.%2520To%2520bridge%2520the%2520gap%252C%2520we%2520shift%2520the%2520problem%2520to%250Athe%2520dynamic%2520setting%2520and%2520pose%2520the%2520question%253A%2520given%2520the%2520label-rich%2520source%2520graphs%250Aand%2520the%2520label-scarce%2520target%2520graphs%2520both%2520observed%2520in%2520previous%2520T%2520timestamps%252C%2520how%250Acan%2520we%2520effectively%2520characterize%2520the%2520evolving%2520domain%2520discrepancy%2520and%2520optimize%250Athe%2520generalization%2520performance%2520of%2520the%2520target%2520domain%2520at%2520the%2520incoming%2520T%252B1%250Atimestamp%253F%2520To%2520answer%2520it%252C%2520we%2520propose%2520a%2520generalization%2520bound%2520for%2520dynamic%2520non-IID%250Atransfer%2520learning%2520on%2520graphs%252C%2520which%2520implies%2520the%2520generalization%2520performance%2520is%250Adominated%2520by%2520domain%2520evolution%2520and%2520domain%2520discrepancy%2520between%2520source%2520and%2520target%250Agraphs.%2520Inspired%2520by%2520the%2520theoretical%2520results%252C%2520we%2520introduce%2520a%2520novel%2520generic%250Aframework%2520named%2520EvoluNet.%2520It%2520leverages%2520a%2520transformer-based%2520temporal%2520encoding%250Amodule%2520to%2520model%2520temporal%2520information%2520of%2520the%2520evolving%2520domains%2520and%2520then%2520uses%2520a%250Adynamic%2520domain%2520unification%2520module%2520to%2520efficiently%2520learn%2520domain-invariant%250Arepresentations%2520across%2520the%2520source%2520and%2520target%2520domains.%2520Finally%252C%2520EvoluNet%250Aoutperforms%2520the%2520state-of-the-art%2520models%2520by%2520up%2520to%252012.1%2525%252C%2520demonstrating%2520its%250Aeffectiveness%2520in%2520transferring%2520knowledge%2520from%2520dynamic%2520source%2520graphs%2520to%2520dynamic%250Atarget%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.00664v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EvoluNet%3A%20Advancing%20Dynamic%20Non-IID%20Transfer%20Learning%20on%20Graphs&entry.906535625=Haohui%20Wang%20and%20Yuzhen%20Mao%20and%20Yujun%20Yan%20and%20Yaoqing%20Yang%20and%20Jianhui%20Sun%20and%20Kevin%20Choi%20and%20Balaji%20Veeramani%20and%20Alison%20Hu%20and%20Edward%20Bowen%20and%20Tyler%20Cody%20and%20Dawei%20Zhou&entry.1292438233=%20%20Non-IID%20transfer%20learning%20on%20graphs%20is%20crucial%20in%20many%20high-stakes%20domains.%0AThe%20majority%20of%20existing%20works%20assume%20stationary%20distribution%20for%20both%20source%0Aand%20target%20domains.%20However%2C%20real-world%20graphs%20are%20intrinsically%20dynamic%2C%0Apresenting%20challenges%20in%20terms%20of%20domain%20evolution%20and%20dynamic%20discrepancy%0Abetween%20source%20and%20target%20domains.%20To%20bridge%20the%20gap%2C%20we%20shift%20the%20problem%20to%0Athe%20dynamic%20setting%20and%20pose%20the%20question%3A%20given%20the%20label-rich%20source%20graphs%0Aand%20the%20label-scarce%20target%20graphs%20both%20observed%20in%20previous%20T%20timestamps%2C%20how%0Acan%20we%20effectively%20characterize%20the%20evolving%20domain%20discrepancy%20and%20optimize%0Athe%20generalization%20performance%20of%20the%20target%20domain%20at%20the%20incoming%20T%2B1%0Atimestamp%3F%20To%20answer%20it%2C%20we%20propose%20a%20generalization%20bound%20for%20dynamic%20non-IID%0Atransfer%20learning%20on%20graphs%2C%20which%20implies%20the%20generalization%20performance%20is%0Adominated%20by%20domain%20evolution%20and%20domain%20discrepancy%20between%20source%20and%20target%0Agraphs.%20Inspired%20by%20the%20theoretical%20results%2C%20we%20introduce%20a%20novel%20generic%0Aframework%20named%20EvoluNet.%20It%20leverages%20a%20transformer-based%20temporal%20encoding%0Amodule%20to%20model%20temporal%20information%20of%20the%20evolving%20domains%20and%20then%20uses%20a%0Adynamic%20domain%20unification%20module%20to%20efficiently%20learn%20domain-invariant%0Arepresentations%20across%20the%20source%20and%20target%20domains.%20Finally%2C%20EvoluNet%0Aoutperforms%20the%20state-of-the-art%20models%20by%20up%20to%2012.1%25%2C%20demonstrating%20its%0Aeffectiveness%20in%20transferring%20knowledge%20from%20dynamic%20source%20graphs%20to%20dynamic%0Atarget%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.00664v5&entry.124074799=Read"},
{"title": "Towards Imbalanced Motion: Part-Decoupling Network for Video Portrait\n  Segmentation", "author": "Tianshu Yu and Changqun Xia and Jia Li", "abstract": "  Video portrait segmentation (VPS), aiming at segmenting prominent foreground\nportraits from video frames, has received much attention in recent years.\nHowever, simplicity of existing VPS datasets leads to a limitation on extensive\nresearch of the task. In this work, we propose a new intricate large-scale\nMulti-scene Video Portrait Segmentation dataset MVPS consisting of 101 video\nclips in 7 scenario categories, in which 10,843 sampled frames are finely\nannotated at pixel level. The dataset has diverse scenes and complicated\nbackground environments, which is the most complex dataset in VPS to our best\nknowledge. Through the observation of a large number of videos with portraits\nduring dataset construction, we find that due to the joint structure of human\nbody, motion of portraits is part-associated, which leads that different parts\nare relatively independent in motion. That is, motion of different parts of the\nportraits is imbalanced. Towards this imbalance, an intuitive and reasonable\nidea is that different motion states in portraits can be better exploited by\ndecoupling the portraits into parts. To achieve this, we propose a\nPart-Decoupling Network (PDNet) for video portrait segmentation. Specifically,\nan Inter-frame Part-Discriminated Attention (IPDA) module is proposed which\nunsupervisedly segments portrait into parts and utilizes different\nattentiveness on discriminative features specified to each different part. In\nthis way, appropriate attention can be imposed to portrait parts with\nimbalanced motion to extract part-discriminated correlations, so that the\nportraits can be segmented more accurately. Experimental results demonstrate\nthat our method achieves leading performance with the comparison to\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2307.16565v2", "date": "2024-05-31", "relevancy": 2.2607, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5724}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5696}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Imbalanced%20Motion%3A%20Part-Decoupling%20Network%20for%20Video%20Portrait%0A%20%20Segmentation&body=Title%3A%20Towards%20Imbalanced%20Motion%3A%20Part-Decoupling%20Network%20for%20Video%20Portrait%0A%20%20Segmentation%0AAuthor%3A%20Tianshu%20Yu%20and%20Changqun%20Xia%20and%20Jia%20Li%0AAbstract%3A%20%20%20Video%20portrait%20segmentation%20%28VPS%29%2C%20aiming%20at%20segmenting%20prominent%20foreground%0Aportraits%20from%20video%20frames%2C%20has%20received%20much%20attention%20in%20recent%20years.%0AHowever%2C%20simplicity%20of%20existing%20VPS%20datasets%20leads%20to%20a%20limitation%20on%20extensive%0Aresearch%20of%20the%20task.%20In%20this%20work%2C%20we%20propose%20a%20new%20intricate%20large-scale%0AMulti-scene%20Video%20Portrait%20Segmentation%20dataset%20MVPS%20consisting%20of%20101%20video%0Aclips%20in%207%20scenario%20categories%2C%20in%20which%2010%2C843%20sampled%20frames%20are%20finely%0Aannotated%20at%20pixel%20level.%20The%20dataset%20has%20diverse%20scenes%20and%20complicated%0Abackground%20environments%2C%20which%20is%20the%20most%20complex%20dataset%20in%20VPS%20to%20our%20best%0Aknowledge.%20Through%20the%20observation%20of%20a%20large%20number%20of%20videos%20with%20portraits%0Aduring%20dataset%20construction%2C%20we%20find%20that%20due%20to%20the%20joint%20structure%20of%20human%0Abody%2C%20motion%20of%20portraits%20is%20part-associated%2C%20which%20leads%20that%20different%20parts%0Aare%20relatively%20independent%20in%20motion.%20That%20is%2C%20motion%20of%20different%20parts%20of%20the%0Aportraits%20is%20imbalanced.%20Towards%20this%20imbalance%2C%20an%20intuitive%20and%20reasonable%0Aidea%20is%20that%20different%20motion%20states%20in%20portraits%20can%20be%20better%20exploited%20by%0Adecoupling%20the%20portraits%20into%20parts.%20To%20achieve%20this%2C%20we%20propose%20a%0APart-Decoupling%20Network%20%28PDNet%29%20for%20video%20portrait%20segmentation.%20Specifically%2C%0Aan%20Inter-frame%20Part-Discriminated%20Attention%20%28IPDA%29%20module%20is%20proposed%20which%0Aunsupervisedly%20segments%20portrait%20into%20parts%20and%20utilizes%20different%0Aattentiveness%20on%20discriminative%20features%20specified%20to%20each%20different%20part.%20In%0Athis%20way%2C%20appropriate%20attention%20can%20be%20imposed%20to%20portrait%20parts%20with%0Aimbalanced%20motion%20to%20extract%20part-discriminated%20correlations%2C%20so%20that%20the%0Aportraits%20can%20be%20segmented%20more%20accurately.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20achieves%20leading%20performance%20with%20the%20comparison%20to%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Imbalanced%2520Motion%253A%2520Part-Decoupling%2520Network%2520for%2520Video%2520Portrait%250A%2520%2520Segmentation%26entry.906535625%3DTianshu%2520Yu%2520and%2520Changqun%2520Xia%2520and%2520Jia%2520Li%26entry.1292438233%3D%2520%2520Video%2520portrait%2520segmentation%2520%2528VPS%2529%252C%2520aiming%2520at%2520segmenting%2520prominent%2520foreground%250Aportraits%2520from%2520video%2520frames%252C%2520has%2520received%2520much%2520attention%2520in%2520recent%2520years.%250AHowever%252C%2520simplicity%2520of%2520existing%2520VPS%2520datasets%2520leads%2520to%2520a%2520limitation%2520on%2520extensive%250Aresearch%2520of%2520the%2520task.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520intricate%2520large-scale%250AMulti-scene%2520Video%2520Portrait%2520Segmentation%2520dataset%2520MVPS%2520consisting%2520of%2520101%2520video%250Aclips%2520in%25207%2520scenario%2520categories%252C%2520in%2520which%252010%252C843%2520sampled%2520frames%2520are%2520finely%250Aannotated%2520at%2520pixel%2520level.%2520The%2520dataset%2520has%2520diverse%2520scenes%2520and%2520complicated%250Abackground%2520environments%252C%2520which%2520is%2520the%2520most%2520complex%2520dataset%2520in%2520VPS%2520to%2520our%2520best%250Aknowledge.%2520Through%2520the%2520observation%2520of%2520a%2520large%2520number%2520of%2520videos%2520with%2520portraits%250Aduring%2520dataset%2520construction%252C%2520we%2520find%2520that%2520due%2520to%2520the%2520joint%2520structure%2520of%2520human%250Abody%252C%2520motion%2520of%2520portraits%2520is%2520part-associated%252C%2520which%2520leads%2520that%2520different%2520parts%250Aare%2520relatively%2520independent%2520in%2520motion.%2520That%2520is%252C%2520motion%2520of%2520different%2520parts%2520of%2520the%250Aportraits%2520is%2520imbalanced.%2520Towards%2520this%2520imbalance%252C%2520an%2520intuitive%2520and%2520reasonable%250Aidea%2520is%2520that%2520different%2520motion%2520states%2520in%2520portraits%2520can%2520be%2520better%2520exploited%2520by%250Adecoupling%2520the%2520portraits%2520into%2520parts.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520a%250APart-Decoupling%2520Network%2520%2528PDNet%2529%2520for%2520video%2520portrait%2520segmentation.%2520Specifically%252C%250Aan%2520Inter-frame%2520Part-Discriminated%2520Attention%2520%2528IPDA%2529%2520module%2520is%2520proposed%2520which%250Aunsupervisedly%2520segments%2520portrait%2520into%2520parts%2520and%2520utilizes%2520different%250Aattentiveness%2520on%2520discriminative%2520features%2520specified%2520to%2520each%2520different%2520part.%2520In%250Athis%2520way%252C%2520appropriate%2520attention%2520can%2520be%2520imposed%2520to%2520portrait%2520parts%2520with%250Aimbalanced%2520motion%2520to%2520extract%2520part-discriminated%2520correlations%252C%2520so%2520that%2520the%250Aportraits%2520can%2520be%2520segmented%2520more%2520accurately.%2520Experimental%2520results%2520demonstrate%250Athat%2520our%2520method%2520achieves%2520leading%2520performance%2520with%2520the%2520comparison%2520to%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.16565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Imbalanced%20Motion%3A%20Part-Decoupling%20Network%20for%20Video%20Portrait%0A%20%20Segmentation&entry.906535625=Tianshu%20Yu%20and%20Changqun%20Xia%20and%20Jia%20Li&entry.1292438233=%20%20Video%20portrait%20segmentation%20%28VPS%29%2C%20aiming%20at%20segmenting%20prominent%20foreground%0Aportraits%20from%20video%20frames%2C%20has%20received%20much%20attention%20in%20recent%20years.%0AHowever%2C%20simplicity%20of%20existing%20VPS%20datasets%20leads%20to%20a%20limitation%20on%20extensive%0Aresearch%20of%20the%20task.%20In%20this%20work%2C%20we%20propose%20a%20new%20intricate%20large-scale%0AMulti-scene%20Video%20Portrait%20Segmentation%20dataset%20MVPS%20consisting%20of%20101%20video%0Aclips%20in%207%20scenario%20categories%2C%20in%20which%2010%2C843%20sampled%20frames%20are%20finely%0Aannotated%20at%20pixel%20level.%20The%20dataset%20has%20diverse%20scenes%20and%20complicated%0Abackground%20environments%2C%20which%20is%20the%20most%20complex%20dataset%20in%20VPS%20to%20our%20best%0Aknowledge.%20Through%20the%20observation%20of%20a%20large%20number%20of%20videos%20with%20portraits%0Aduring%20dataset%20construction%2C%20we%20find%20that%20due%20to%20the%20joint%20structure%20of%20human%0Abody%2C%20motion%20of%20portraits%20is%20part-associated%2C%20which%20leads%20that%20different%20parts%0Aare%20relatively%20independent%20in%20motion.%20That%20is%2C%20motion%20of%20different%20parts%20of%20the%0Aportraits%20is%20imbalanced.%20Towards%20this%20imbalance%2C%20an%20intuitive%20and%20reasonable%0Aidea%20is%20that%20different%20motion%20states%20in%20portraits%20can%20be%20better%20exploited%20by%0Adecoupling%20the%20portraits%20into%20parts.%20To%20achieve%20this%2C%20we%20propose%20a%0APart-Decoupling%20Network%20%28PDNet%29%20for%20video%20portrait%20segmentation.%20Specifically%2C%0Aan%20Inter-frame%20Part-Discriminated%20Attention%20%28IPDA%29%20module%20is%20proposed%20which%0Aunsupervisedly%20segments%20portrait%20into%20parts%20and%20utilizes%20different%0Aattentiveness%20on%20discriminative%20features%20specified%20to%20each%20different%20part.%20In%0Athis%20way%2C%20appropriate%20attention%20can%20be%20imposed%20to%20portrait%20parts%20with%0Aimbalanced%20motion%20to%20extract%20part-discriminated%20correlations%2C%20so%20that%20the%0Aportraits%20can%20be%20segmented%20more%20accurately.%20Experimental%20results%20demonstrate%0Athat%20our%20method%20achieves%20leading%20performance%20with%20the%20comparison%20to%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16565v2&entry.124074799=Read"},
{"title": "Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive\n  Multimodal Models", "author": "A. Bavaresco and A. Testoni and R. Fern\u00e1ndez", "abstract": "  Image-based advertisements are complex multimodal stimuli that often contain\nunusual visual elements and figurative language. Previous research on automatic\nad understanding has reported impressive zero-shot accuracy of contrastive\nvision-and-language models (VLMs) on an ad-explanation retrieval task. Here, we\nexamine the original task setup and show that contrastive VLMs can solve it by\nexploiting grounding heuristics. To control for this confound, we introduce\nTRADE, a new evaluation test set with adversarial grounded explanations. While\nthese explanations look implausible to humans, we show that they \"fool\" four\ndifferent contrastive VLMs. Our findings highlight the need for an improved\noperationalisation of automatic ad understanding that truly evaluates VLMs'\nmultimodal reasoning abilities. We make our code and TRADE available at\nhttps://github.com/dmg-illc/trade .\n", "link": "http://arxiv.org/abs/2405.20846v1", "date": "2024-05-31", "relevancy": 2.2557, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5841}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5502}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Don%27t%20Buy%20it%21%20Reassessing%20the%20Ad%20Understanding%20Abilities%20of%20Contrastive%0A%20%20Multimodal%20Models&body=Title%3A%20Don%27t%20Buy%20it%21%20Reassessing%20the%20Ad%20Understanding%20Abilities%20of%20Contrastive%0A%20%20Multimodal%20Models%0AAuthor%3A%20A.%20Bavaresco%20and%20A.%20Testoni%20and%20R.%20Fern%C3%A1ndez%0AAbstract%3A%20%20%20Image-based%20advertisements%20are%20complex%20multimodal%20stimuli%20that%20often%20contain%0Aunusual%20visual%20elements%20and%20figurative%20language.%20Previous%20research%20on%20automatic%0Aad%20understanding%20has%20reported%20impressive%20zero-shot%20accuracy%20of%20contrastive%0Avision-and-language%20models%20%28VLMs%29%20on%20an%20ad-explanation%20retrieval%20task.%20Here%2C%20we%0Aexamine%20the%20original%20task%20setup%20and%20show%20that%20contrastive%20VLMs%20can%20solve%20it%20by%0Aexploiting%20grounding%20heuristics.%20To%20control%20for%20this%20confound%2C%20we%20introduce%0ATRADE%2C%20a%20new%20evaluation%20test%20set%20with%20adversarial%20grounded%20explanations.%20While%0Athese%20explanations%20look%20implausible%20to%20humans%2C%20we%20show%20that%20they%20%22fool%22%20four%0Adifferent%20contrastive%20VLMs.%20Our%20findings%20highlight%20the%20need%20for%20an%20improved%0Aoperationalisation%20of%20automatic%20ad%20understanding%20that%20truly%20evaluates%20VLMs%27%0Amultimodal%20reasoning%20abilities.%20We%20make%20our%20code%20and%20TRADE%20available%20at%0Ahttps%3A//github.com/dmg-illc/trade%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDon%2527t%2520Buy%2520it%2521%2520Reassessing%2520the%2520Ad%2520Understanding%2520Abilities%2520of%2520Contrastive%250A%2520%2520Multimodal%2520Models%26entry.906535625%3DA.%2520Bavaresco%2520and%2520A.%2520Testoni%2520and%2520R.%2520Fern%25C3%25A1ndez%26entry.1292438233%3D%2520%2520Image-based%2520advertisements%2520are%2520complex%2520multimodal%2520stimuli%2520that%2520often%2520contain%250Aunusual%2520visual%2520elements%2520and%2520figurative%2520language.%2520Previous%2520research%2520on%2520automatic%250Aad%2520understanding%2520has%2520reported%2520impressive%2520zero-shot%2520accuracy%2520of%2520contrastive%250Avision-and-language%2520models%2520%2528VLMs%2529%2520on%2520an%2520ad-explanation%2520retrieval%2520task.%2520Here%252C%2520we%250Aexamine%2520the%2520original%2520task%2520setup%2520and%2520show%2520that%2520contrastive%2520VLMs%2520can%2520solve%2520it%2520by%250Aexploiting%2520grounding%2520heuristics.%2520To%2520control%2520for%2520this%2520confound%252C%2520we%2520introduce%250ATRADE%252C%2520a%2520new%2520evaluation%2520test%2520set%2520with%2520adversarial%2520grounded%2520explanations.%2520While%250Athese%2520explanations%2520look%2520implausible%2520to%2520humans%252C%2520we%2520show%2520that%2520they%2520%2522fool%2522%2520four%250Adifferent%2520contrastive%2520VLMs.%2520Our%2520findings%2520highlight%2520the%2520need%2520for%2520an%2520improved%250Aoperationalisation%2520of%2520automatic%2520ad%2520understanding%2520that%2520truly%2520evaluates%2520VLMs%2527%250Amultimodal%2520reasoning%2520abilities.%2520We%2520make%2520our%2520code%2520and%2520TRADE%2520available%2520at%250Ahttps%253A//github.com/dmg-illc/trade%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Don%27t%20Buy%20it%21%20Reassessing%20the%20Ad%20Understanding%20Abilities%20of%20Contrastive%0A%20%20Multimodal%20Models&entry.906535625=A.%20Bavaresco%20and%20A.%20Testoni%20and%20R.%20Fern%C3%A1ndez&entry.1292438233=%20%20Image-based%20advertisements%20are%20complex%20multimodal%20stimuli%20that%20often%20contain%0Aunusual%20visual%20elements%20and%20figurative%20language.%20Previous%20research%20on%20automatic%0Aad%20understanding%20has%20reported%20impressive%20zero-shot%20accuracy%20of%20contrastive%0Avision-and-language%20models%20%28VLMs%29%20on%20an%20ad-explanation%20retrieval%20task.%20Here%2C%20we%0Aexamine%20the%20original%20task%20setup%20and%20show%20that%20contrastive%20VLMs%20can%20solve%20it%20by%0Aexploiting%20grounding%20heuristics.%20To%20control%20for%20this%20confound%2C%20we%20introduce%0ATRADE%2C%20a%20new%20evaluation%20test%20set%20with%20adversarial%20grounded%20explanations.%20While%0Athese%20explanations%20look%20implausible%20to%20humans%2C%20we%20show%20that%20they%20%22fool%22%20four%0Adifferent%20contrastive%20VLMs.%20Our%20findings%20highlight%20the%20need%20for%20an%20improved%0Aoperationalisation%20of%20automatic%20ad%20understanding%20that%20truly%20evaluates%20VLMs%27%0Amultimodal%20reasoning%20abilities.%20We%20make%20our%20code%20and%20TRADE%20available%20at%0Ahttps%3A//github.com/dmg-illc/trade%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20846v1&entry.124074799=Read"},
{"title": "Memory Consolidation Enables Long-Context Video Understanding", "author": "Ivana Bala\u017eevi\u0107 and Yuge Shi and Pinelopi Papalampidi and Rahma Chaabouni and Skanda Koppula and Olivier J. H\u00e9naff", "abstract": "  Most transformer-based video encoders are limited to short temporal contexts\ndue to their quadratic complexity. While various attempts have been made to\nextend this context, this has often come at the cost of both conceptual and\ncomputational complexity. We propose to instead re-purpose existing pre-trained\nvideo transformers by simply fine-tuning them to attend to memories derived\nnon-parametrically from past activations. By leveraging redundancy reduction,\nour memory-consolidated vision transformer (MC-ViT) effortlessly extends its\ncontext far into the past and exhibits excellent scaling behavior when learning\nfrom longer videos. In doing so, MC-ViT sets a new state-of-the-art in\nlong-context video understanding on EgoSchema, Perception Test, and Diving48,\noutperforming methods that benefit from orders of magnitude more parameters.\n", "link": "http://arxiv.org/abs/2402.05861v2", "date": "2024-05-31", "relevancy": 2.2508, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.575}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5562}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Memory%20Consolidation%20Enables%20Long-Context%20Video%20Understanding&body=Title%3A%20Memory%20Consolidation%20Enables%20Long-Context%20Video%20Understanding%0AAuthor%3A%20Ivana%20Bala%C5%BEevi%C4%87%20and%20Yuge%20Shi%20and%20Pinelopi%20Papalampidi%20and%20Rahma%20Chaabouni%20and%20Skanda%20Koppula%20and%20Olivier%20J.%20H%C3%A9naff%0AAbstract%3A%20%20%20Most%20transformer-based%20video%20encoders%20are%20limited%20to%20short%20temporal%20contexts%0Adue%20to%20their%20quadratic%20complexity.%20While%20various%20attempts%20have%20been%20made%20to%0Aextend%20this%20context%2C%20this%20has%20often%20come%20at%20the%20cost%20of%20both%20conceptual%20and%0Acomputational%20complexity.%20We%20propose%20to%20instead%20re-purpose%20existing%20pre-trained%0Avideo%20transformers%20by%20simply%20fine-tuning%20them%20to%20attend%20to%20memories%20derived%0Anon-parametrically%20from%20past%20activations.%20By%20leveraging%20redundancy%20reduction%2C%0Aour%20memory-consolidated%20vision%20transformer%20%28MC-ViT%29%20effortlessly%20extends%20its%0Acontext%20far%20into%20the%20past%20and%20exhibits%20excellent%20scaling%20behavior%20when%20learning%0Afrom%20longer%20videos.%20In%20doing%20so%2C%20MC-ViT%20sets%20a%20new%20state-of-the-art%20in%0Along-context%20video%20understanding%20on%20EgoSchema%2C%20Perception%20Test%2C%20and%20Diving48%2C%0Aoutperforming%20methods%20that%20benefit%20from%20orders%20of%20magnitude%20more%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05861v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMemory%2520Consolidation%2520Enables%2520Long-Context%2520Video%2520Understanding%26entry.906535625%3DIvana%2520Bala%25C5%25BEevi%25C4%2587%2520and%2520Yuge%2520Shi%2520and%2520Pinelopi%2520Papalampidi%2520and%2520Rahma%2520Chaabouni%2520and%2520Skanda%2520Koppula%2520and%2520Olivier%2520J.%2520H%25C3%25A9naff%26entry.1292438233%3D%2520%2520Most%2520transformer-based%2520video%2520encoders%2520are%2520limited%2520to%2520short%2520temporal%2520contexts%250Adue%2520to%2520their%2520quadratic%2520complexity.%2520While%2520various%2520attempts%2520have%2520been%2520made%2520to%250Aextend%2520this%2520context%252C%2520this%2520has%2520often%2520come%2520at%2520the%2520cost%2520of%2520both%2520conceptual%2520and%250Acomputational%2520complexity.%2520We%2520propose%2520to%2520instead%2520re-purpose%2520existing%2520pre-trained%250Avideo%2520transformers%2520by%2520simply%2520fine-tuning%2520them%2520to%2520attend%2520to%2520memories%2520derived%250Anon-parametrically%2520from%2520past%2520activations.%2520By%2520leveraging%2520redundancy%2520reduction%252C%250Aour%2520memory-consolidated%2520vision%2520transformer%2520%2528MC-ViT%2529%2520effortlessly%2520extends%2520its%250Acontext%2520far%2520into%2520the%2520past%2520and%2520exhibits%2520excellent%2520scaling%2520behavior%2520when%2520learning%250Afrom%2520longer%2520videos.%2520In%2520doing%2520so%252C%2520MC-ViT%2520sets%2520a%2520new%2520state-of-the-art%2520in%250Along-context%2520video%2520understanding%2520on%2520EgoSchema%252C%2520Perception%2520Test%252C%2520and%2520Diving48%252C%250Aoutperforming%2520methods%2520that%2520benefit%2520from%2520orders%2520of%2520magnitude%2520more%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05861v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Memory%20Consolidation%20Enables%20Long-Context%20Video%20Understanding&entry.906535625=Ivana%20Bala%C5%BEevi%C4%87%20and%20Yuge%20Shi%20and%20Pinelopi%20Papalampidi%20and%20Rahma%20Chaabouni%20and%20Skanda%20Koppula%20and%20Olivier%20J.%20H%C3%A9naff&entry.1292438233=%20%20Most%20transformer-based%20video%20encoders%20are%20limited%20to%20short%20temporal%20contexts%0Adue%20to%20their%20quadratic%20complexity.%20While%20various%20attempts%20have%20been%20made%20to%0Aextend%20this%20context%2C%20this%20has%20often%20come%20at%20the%20cost%20of%20both%20conceptual%20and%0Acomputational%20complexity.%20We%20propose%20to%20instead%20re-purpose%20existing%20pre-trained%0Avideo%20transformers%20by%20simply%20fine-tuning%20them%20to%20attend%20to%20memories%20derived%0Anon-parametrically%20from%20past%20activations.%20By%20leveraging%20redundancy%20reduction%2C%0Aour%20memory-consolidated%20vision%20transformer%20%28MC-ViT%29%20effortlessly%20extends%20its%0Acontext%20far%20into%20the%20past%20and%20exhibits%20excellent%20scaling%20behavior%20when%20learning%0Afrom%20longer%20videos.%20In%20doing%20so%2C%20MC-ViT%20sets%20a%20new%20state-of-the-art%20in%0Along-context%20video%20understanding%20on%20EgoSchema%2C%20Perception%20Test%2C%20and%20Diving48%2C%0Aoutperforming%20methods%20that%20benefit%20from%20orders%20of%20magnitude%20more%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05861v2&entry.124074799=Read"},
{"title": "Hard Cases Detection in Motion Prediction by Vision-Language Foundation\n  Models", "author": "Yi Yang and Qingwen Zhang and Kei Ikemura and Nazre Batool and John Folkesson", "abstract": "  Addressing hard cases in autonomous driving, such as anomalous road users,\nextreme weather conditions, and complex traffic interactions, presents\nsignificant challenges. To ensure safety, it is crucial to detect and manage\nthese scenarios effectively for autonomous driving systems. However, the rarity\nand high-risk nature of these cases demand extensive, diverse datasets for\ntraining robust models. Vision-Language Foundation Models (VLMs) have shown\nremarkable zero-shot capabilities as being trained on extensive datasets. This\nwork explores the potential of VLMs in detecting hard cases in autonomous\ndriving. We demonstrate the capability of VLMs such as GPT-4v in detecting hard\ncases in traffic participant motion prediction on both agent and scenario\nlevels. We introduce a feasible pipeline where VLMs, fed with sequential image\nframes with designed prompts, effectively identify challenging agents or\nscenarios, which are verified by existing prediction models. Moreover, by\ntaking advantage of this detection of hard cases by VLMs, we further improve\nthe training efficiency of the existing motion prediction pipeline by\nperforming data selection for the training samples suggested by GPT. We show\nthe effectiveness and feasibility of our pipeline incorporating VLMs with\nstate-of-the-art methods on NuScenes datasets. The code is accessible at\nhttps://github.com/KTH-RPL/Detect_VLM.\n", "link": "http://arxiv.org/abs/2405.20991v1", "date": "2024-05-31", "relevancy": 2.2478, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5915}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5688}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hard%20Cases%20Detection%20in%20Motion%20Prediction%20by%20Vision-Language%20Foundation%0A%20%20Models&body=Title%3A%20Hard%20Cases%20Detection%20in%20Motion%20Prediction%20by%20Vision-Language%20Foundation%0A%20%20Models%0AAuthor%3A%20Yi%20Yang%20and%20Qingwen%20Zhang%20and%20Kei%20Ikemura%20and%20Nazre%20Batool%20and%20John%20Folkesson%0AAbstract%3A%20%20%20Addressing%20hard%20cases%20in%20autonomous%20driving%2C%20such%20as%20anomalous%20road%20users%2C%0Aextreme%20weather%20conditions%2C%20and%20complex%20traffic%20interactions%2C%20presents%0Asignificant%20challenges.%20To%20ensure%20safety%2C%20it%20is%20crucial%20to%20detect%20and%20manage%0Athese%20scenarios%20effectively%20for%20autonomous%20driving%20systems.%20However%2C%20the%20rarity%0Aand%20high-risk%20nature%20of%20these%20cases%20demand%20extensive%2C%20diverse%20datasets%20for%0Atraining%20robust%20models.%20Vision-Language%20Foundation%20Models%20%28VLMs%29%20have%20shown%0Aremarkable%20zero-shot%20capabilities%20as%20being%20trained%20on%20extensive%20datasets.%20This%0Awork%20explores%20the%20potential%20of%20VLMs%20in%20detecting%20hard%20cases%20in%20autonomous%0Adriving.%20We%20demonstrate%20the%20capability%20of%20VLMs%20such%20as%20GPT-4v%20in%20detecting%20hard%0Acases%20in%20traffic%20participant%20motion%20prediction%20on%20both%20agent%20and%20scenario%0Alevels.%20We%20introduce%20a%20feasible%20pipeline%20where%20VLMs%2C%20fed%20with%20sequential%20image%0Aframes%20with%20designed%20prompts%2C%20effectively%20identify%20challenging%20agents%20or%0Ascenarios%2C%20which%20are%20verified%20by%20existing%20prediction%20models.%20Moreover%2C%20by%0Ataking%20advantage%20of%20this%20detection%20of%20hard%20cases%20by%20VLMs%2C%20we%20further%20improve%0Athe%20training%20efficiency%20of%20the%20existing%20motion%20prediction%20pipeline%20by%0Aperforming%20data%20selection%20for%20the%20training%20samples%20suggested%20by%20GPT.%20We%20show%0Athe%20effectiveness%20and%20feasibility%20of%20our%20pipeline%20incorporating%20VLMs%20with%0Astate-of-the-art%20methods%20on%20NuScenes%20datasets.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/KTH-RPL/Detect_VLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHard%2520Cases%2520Detection%2520in%2520Motion%2520Prediction%2520by%2520Vision-Language%2520Foundation%250A%2520%2520Models%26entry.906535625%3DYi%2520Yang%2520and%2520Qingwen%2520Zhang%2520and%2520Kei%2520Ikemura%2520and%2520Nazre%2520Batool%2520and%2520John%2520Folkesson%26entry.1292438233%3D%2520%2520Addressing%2520hard%2520cases%2520in%2520autonomous%2520driving%252C%2520such%2520as%2520anomalous%2520road%2520users%252C%250Aextreme%2520weather%2520conditions%252C%2520and%2520complex%2520traffic%2520interactions%252C%2520presents%250Asignificant%2520challenges.%2520To%2520ensure%2520safety%252C%2520it%2520is%2520crucial%2520to%2520detect%2520and%2520manage%250Athese%2520scenarios%2520effectively%2520for%2520autonomous%2520driving%2520systems.%2520However%252C%2520the%2520rarity%250Aand%2520high-risk%2520nature%2520of%2520these%2520cases%2520demand%2520extensive%252C%2520diverse%2520datasets%2520for%250Atraining%2520robust%2520models.%2520Vision-Language%2520Foundation%2520Models%2520%2528VLMs%2529%2520have%2520shown%250Aremarkable%2520zero-shot%2520capabilities%2520as%2520being%2520trained%2520on%2520extensive%2520datasets.%2520This%250Awork%2520explores%2520the%2520potential%2520of%2520VLMs%2520in%2520detecting%2520hard%2520cases%2520in%2520autonomous%250Adriving.%2520We%2520demonstrate%2520the%2520capability%2520of%2520VLMs%2520such%2520as%2520GPT-4v%2520in%2520detecting%2520hard%250Acases%2520in%2520traffic%2520participant%2520motion%2520prediction%2520on%2520both%2520agent%2520and%2520scenario%250Alevels.%2520We%2520introduce%2520a%2520feasible%2520pipeline%2520where%2520VLMs%252C%2520fed%2520with%2520sequential%2520image%250Aframes%2520with%2520designed%2520prompts%252C%2520effectively%2520identify%2520challenging%2520agents%2520or%250Ascenarios%252C%2520which%2520are%2520verified%2520by%2520existing%2520prediction%2520models.%2520Moreover%252C%2520by%250Ataking%2520advantage%2520of%2520this%2520detection%2520of%2520hard%2520cases%2520by%2520VLMs%252C%2520we%2520further%2520improve%250Athe%2520training%2520efficiency%2520of%2520the%2520existing%2520motion%2520prediction%2520pipeline%2520by%250Aperforming%2520data%2520selection%2520for%2520the%2520training%2520samples%2520suggested%2520by%2520GPT.%2520We%2520show%250Athe%2520effectiveness%2520and%2520feasibility%2520of%2520our%2520pipeline%2520incorporating%2520VLMs%2520with%250Astate-of-the-art%2520methods%2520on%2520NuScenes%2520datasets.%2520The%2520code%2520is%2520accessible%2520at%250Ahttps%253A//github.com/KTH-RPL/Detect_VLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hard%20Cases%20Detection%20in%20Motion%20Prediction%20by%20Vision-Language%20Foundation%0A%20%20Models&entry.906535625=Yi%20Yang%20and%20Qingwen%20Zhang%20and%20Kei%20Ikemura%20and%20Nazre%20Batool%20and%20John%20Folkesson&entry.1292438233=%20%20Addressing%20hard%20cases%20in%20autonomous%20driving%2C%20such%20as%20anomalous%20road%20users%2C%0Aextreme%20weather%20conditions%2C%20and%20complex%20traffic%20interactions%2C%20presents%0Asignificant%20challenges.%20To%20ensure%20safety%2C%20it%20is%20crucial%20to%20detect%20and%20manage%0Athese%20scenarios%20effectively%20for%20autonomous%20driving%20systems.%20However%2C%20the%20rarity%0Aand%20high-risk%20nature%20of%20these%20cases%20demand%20extensive%2C%20diverse%20datasets%20for%0Atraining%20robust%20models.%20Vision-Language%20Foundation%20Models%20%28VLMs%29%20have%20shown%0Aremarkable%20zero-shot%20capabilities%20as%20being%20trained%20on%20extensive%20datasets.%20This%0Awork%20explores%20the%20potential%20of%20VLMs%20in%20detecting%20hard%20cases%20in%20autonomous%0Adriving.%20We%20demonstrate%20the%20capability%20of%20VLMs%20such%20as%20GPT-4v%20in%20detecting%20hard%0Acases%20in%20traffic%20participant%20motion%20prediction%20on%20both%20agent%20and%20scenario%0Alevels.%20We%20introduce%20a%20feasible%20pipeline%20where%20VLMs%2C%20fed%20with%20sequential%20image%0Aframes%20with%20designed%20prompts%2C%20effectively%20identify%20challenging%20agents%20or%0Ascenarios%2C%20which%20are%20verified%20by%20existing%20prediction%20models.%20Moreover%2C%20by%0Ataking%20advantage%20of%20this%20detection%20of%20hard%20cases%20by%20VLMs%2C%20we%20further%20improve%0Athe%20training%20efficiency%20of%20the%20existing%20motion%20prediction%20pipeline%20by%0Aperforming%20data%20selection%20for%20the%20training%20samples%20suggested%20by%20GPT.%20We%20show%0Athe%20effectiveness%20and%20feasibility%20of%20our%20pipeline%20incorporating%20VLMs%20with%0Astate-of-the-art%20methods%20on%20NuScenes%20datasets.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/KTH-RPL/Detect_VLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20991v1&entry.124074799=Read"},
{"title": "Awesome Multi-modal Object Tracking", "author": "Chunhui Zhang and Li Liu and Hao Wen and Xi Zhou and Yanfeng Wang", "abstract": "  Multi-modal object tracking (MMOT) is an emerging field that combines data\nfrom various modalities, \\eg vision (RGB), depth, thermal infrared, event,\nlanguage and audio, to estimate the state of an arbitrary object in a video\nsequence. It is of great significance for many applications such as autonomous\ndriving and intelligent surveillance. In recent years, MMOT has received more\nand more attention. However, existing MMOT algorithms mainly focus on two\nmodalities (\\eg RGB+depth, RGB+thermal infrared, and RGB+language). To leverage\nmore modalities, some recent efforts have been made to learn a unified visual\nobject tracking model for any modality. Additionally, some large-scale\nmulti-modal tracking benchmarks have been established by simultaneously\nproviding more than two modalities, such as vision-language-audio (\\eg\nWebUAV-3M) and vision-depth-language (\\eg UniMod1K). To track the latest\nprogress in MMOT, we conduct a comprehensive investigation in this report.\nSpecifically, we first divide existing MMOT tasks into five main categories,\n\\ie RGBL tracking, RGBE tracking, RGBD tracking, RGBT tracking, and\nmiscellaneous (RGB+X), where X can be any modality, such as language, depth,\nand event. Then, we analyze and summarize each MMOT task, focusing on widely\nused datasets and mainstream tracking algorithms based on their technical\nparadigms (\\eg self-supervised learning, prompt learning, knowledge\ndistillation, generative models, and state space models). Finally, we maintain\na continuously updated paper list for MMOT at\nhttps://github.com/983632847/Awesome-Multimodal-Object-Tracking.\n", "link": "http://arxiv.org/abs/2405.14200v2", "date": "2024-05-31", "relevancy": 2.2435, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5727}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Awesome%20Multi-modal%20Object%20Tracking&body=Title%3A%20Awesome%20Multi-modal%20Object%20Tracking%0AAuthor%3A%20Chunhui%20Zhang%20and%20Li%20Liu%20and%20Hao%20Wen%20and%20Xi%20Zhou%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Multi-modal%20object%20tracking%20%28MMOT%29%20is%20an%20emerging%20field%20that%20combines%20data%0Afrom%20various%20modalities%2C%20%5Ceg%20vision%20%28RGB%29%2C%20depth%2C%20thermal%20infrared%2C%20event%2C%0Alanguage%20and%20audio%2C%20to%20estimate%20the%20state%20of%20an%20arbitrary%20object%20in%20a%20video%0Asequence.%20It%20is%20of%20great%20significance%20for%20many%20applications%20such%20as%20autonomous%0Adriving%20and%20intelligent%20surveillance.%20In%20recent%20years%2C%20MMOT%20has%20received%20more%0Aand%20more%20attention.%20However%2C%20existing%20MMOT%20algorithms%20mainly%20focus%20on%20two%0Amodalities%20%28%5Ceg%20RGB%2Bdepth%2C%20RGB%2Bthermal%20infrared%2C%20and%20RGB%2Blanguage%29.%20To%20leverage%0Amore%20modalities%2C%20some%20recent%20efforts%20have%20been%20made%20to%20learn%20a%20unified%20visual%0Aobject%20tracking%20model%20for%20any%20modality.%20Additionally%2C%20some%20large-scale%0Amulti-modal%20tracking%20benchmarks%20have%20been%20established%20by%20simultaneously%0Aproviding%20more%20than%20two%20modalities%2C%20such%20as%20vision-language-audio%20%28%5Ceg%0AWebUAV-3M%29%20and%20vision-depth-language%20%28%5Ceg%20UniMod1K%29.%20To%20track%20the%20latest%0Aprogress%20in%20MMOT%2C%20we%20conduct%20a%20comprehensive%20investigation%20in%20this%20report.%0ASpecifically%2C%20we%20first%20divide%20existing%20MMOT%20tasks%20into%20five%20main%20categories%2C%0A%5Cie%20RGBL%20tracking%2C%20RGBE%20tracking%2C%20RGBD%20tracking%2C%20RGBT%20tracking%2C%20and%0Amiscellaneous%20%28RGB%2BX%29%2C%20where%20X%20can%20be%20any%20modality%2C%20such%20as%20language%2C%20depth%2C%0Aand%20event.%20Then%2C%20we%20analyze%20and%20summarize%20each%20MMOT%20task%2C%20focusing%20on%20widely%0Aused%20datasets%20and%20mainstream%20tracking%20algorithms%20based%20on%20their%20technical%0Aparadigms%20%28%5Ceg%20self-supervised%20learning%2C%20prompt%20learning%2C%20knowledge%0Adistillation%2C%20generative%20models%2C%20and%20state%20space%20models%29.%20Finally%2C%20we%20maintain%0Aa%20continuously%20updated%20paper%20list%20for%20MMOT%20at%0Ahttps%3A//github.com/983632847/Awesome-Multimodal-Object-Tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14200v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAwesome%2520Multi-modal%2520Object%2520Tracking%26entry.906535625%3DChunhui%2520Zhang%2520and%2520Li%2520Liu%2520and%2520Hao%2520Wen%2520and%2520Xi%2520Zhou%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Multi-modal%2520object%2520tracking%2520%2528MMOT%2529%2520is%2520an%2520emerging%2520field%2520that%2520combines%2520data%250Afrom%2520various%2520modalities%252C%2520%255Ceg%2520vision%2520%2528RGB%2529%252C%2520depth%252C%2520thermal%2520infrared%252C%2520event%252C%250Alanguage%2520and%2520audio%252C%2520to%2520estimate%2520the%2520state%2520of%2520an%2520arbitrary%2520object%2520in%2520a%2520video%250Asequence.%2520It%2520is%2520of%2520great%2520significance%2520for%2520many%2520applications%2520such%2520as%2520autonomous%250Adriving%2520and%2520intelligent%2520surveillance.%2520In%2520recent%2520years%252C%2520MMOT%2520has%2520received%2520more%250Aand%2520more%2520attention.%2520However%252C%2520existing%2520MMOT%2520algorithms%2520mainly%2520focus%2520on%2520two%250Amodalities%2520%2528%255Ceg%2520RGB%252Bdepth%252C%2520RGB%252Bthermal%2520infrared%252C%2520and%2520RGB%252Blanguage%2529.%2520To%2520leverage%250Amore%2520modalities%252C%2520some%2520recent%2520efforts%2520have%2520been%2520made%2520to%2520learn%2520a%2520unified%2520visual%250Aobject%2520tracking%2520model%2520for%2520any%2520modality.%2520Additionally%252C%2520some%2520large-scale%250Amulti-modal%2520tracking%2520benchmarks%2520have%2520been%2520established%2520by%2520simultaneously%250Aproviding%2520more%2520than%2520two%2520modalities%252C%2520such%2520as%2520vision-language-audio%2520%2528%255Ceg%250AWebUAV-3M%2529%2520and%2520vision-depth-language%2520%2528%255Ceg%2520UniMod1K%2529.%2520To%2520track%2520the%2520latest%250Aprogress%2520in%2520MMOT%252C%2520we%2520conduct%2520a%2520comprehensive%2520investigation%2520in%2520this%2520report.%250ASpecifically%252C%2520we%2520first%2520divide%2520existing%2520MMOT%2520tasks%2520into%2520five%2520main%2520categories%252C%250A%255Cie%2520RGBL%2520tracking%252C%2520RGBE%2520tracking%252C%2520RGBD%2520tracking%252C%2520RGBT%2520tracking%252C%2520and%250Amiscellaneous%2520%2528RGB%252BX%2529%252C%2520where%2520X%2520can%2520be%2520any%2520modality%252C%2520such%2520as%2520language%252C%2520depth%252C%250Aand%2520event.%2520Then%252C%2520we%2520analyze%2520and%2520summarize%2520each%2520MMOT%2520task%252C%2520focusing%2520on%2520widely%250Aused%2520datasets%2520and%2520mainstream%2520tracking%2520algorithms%2520based%2520on%2520their%2520technical%250Aparadigms%2520%2528%255Ceg%2520self-supervised%2520learning%252C%2520prompt%2520learning%252C%2520knowledge%250Adistillation%252C%2520generative%2520models%252C%2520and%2520state%2520space%2520models%2529.%2520Finally%252C%2520we%2520maintain%250Aa%2520continuously%2520updated%2520paper%2520list%2520for%2520MMOT%2520at%250Ahttps%253A//github.com/983632847/Awesome-Multimodal-Object-Tracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14200v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Awesome%20Multi-modal%20Object%20Tracking&entry.906535625=Chunhui%20Zhang%20and%20Li%20Liu%20and%20Hao%20Wen%20and%20Xi%20Zhou%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Multi-modal%20object%20tracking%20%28MMOT%29%20is%20an%20emerging%20field%20that%20combines%20data%0Afrom%20various%20modalities%2C%20%5Ceg%20vision%20%28RGB%29%2C%20depth%2C%20thermal%20infrared%2C%20event%2C%0Alanguage%20and%20audio%2C%20to%20estimate%20the%20state%20of%20an%20arbitrary%20object%20in%20a%20video%0Asequence.%20It%20is%20of%20great%20significance%20for%20many%20applications%20such%20as%20autonomous%0Adriving%20and%20intelligent%20surveillance.%20In%20recent%20years%2C%20MMOT%20has%20received%20more%0Aand%20more%20attention.%20However%2C%20existing%20MMOT%20algorithms%20mainly%20focus%20on%20two%0Amodalities%20%28%5Ceg%20RGB%2Bdepth%2C%20RGB%2Bthermal%20infrared%2C%20and%20RGB%2Blanguage%29.%20To%20leverage%0Amore%20modalities%2C%20some%20recent%20efforts%20have%20been%20made%20to%20learn%20a%20unified%20visual%0Aobject%20tracking%20model%20for%20any%20modality.%20Additionally%2C%20some%20large-scale%0Amulti-modal%20tracking%20benchmarks%20have%20been%20established%20by%20simultaneously%0Aproviding%20more%20than%20two%20modalities%2C%20such%20as%20vision-language-audio%20%28%5Ceg%0AWebUAV-3M%29%20and%20vision-depth-language%20%28%5Ceg%20UniMod1K%29.%20To%20track%20the%20latest%0Aprogress%20in%20MMOT%2C%20we%20conduct%20a%20comprehensive%20investigation%20in%20this%20report.%0ASpecifically%2C%20we%20first%20divide%20existing%20MMOT%20tasks%20into%20five%20main%20categories%2C%0A%5Cie%20RGBL%20tracking%2C%20RGBE%20tracking%2C%20RGBD%20tracking%2C%20RGBT%20tracking%2C%20and%0Amiscellaneous%20%28RGB%2BX%29%2C%20where%20X%20can%20be%20any%20modality%2C%20such%20as%20language%2C%20depth%2C%0Aand%20event.%20Then%2C%20we%20analyze%20and%20summarize%20each%20MMOT%20task%2C%20focusing%20on%20widely%0Aused%20datasets%20and%20mainstream%20tracking%20algorithms%20based%20on%20their%20technical%0Aparadigms%20%28%5Ceg%20self-supervised%20learning%2C%20prompt%20learning%2C%20knowledge%0Adistillation%2C%20generative%20models%2C%20and%20state%20space%20models%29.%20Finally%2C%20we%20maintain%0Aa%20continuously%20updated%20paper%20list%20for%20MMOT%20at%0Ahttps%3A//github.com/983632847/Awesome-Multimodal-Object-Tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14200v2&entry.124074799=Read"},
{"title": "Neural Gaussian Scale-Space Fields", "author": "Felix Mujkanovic and Ntumba Elie Nsampi and Christian Theobalt and Hans-Peter Seidel and Thomas Leimk\u00fchler", "abstract": "  Gaussian scale spaces are a cornerstone of signal representation and\nprocessing, with applications in filtering, multiscale analysis, anti-aliasing,\nand many more. However, obtaining such a scale space is costly and cumbersome,\nin particular for continuous representations such as neural fields. We present\nan efficient and lightweight method to learn the fully continuous, anisotropic\nGaussian scale space of an arbitrary signal. Based on Fourier feature\nmodulation and Lipschitz bounding, our approach is trained self-supervised,\ni.e., training does not require any manual filtering. Our neural Gaussian\nscale-space fields faithfully capture multiscale representations across a broad\nrange of modalities, and support a diverse set of applications. These include\nimages, geometry, light-stage data, texture anti-aliasing, and multiscale\noptimization.\n", "link": "http://arxiv.org/abs/2405.20980v1", "date": "2024-05-31", "relevancy": 2.228, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5666}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5587}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5288}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Gaussian%20Scale-Space%20Fields&body=Title%3A%20Neural%20Gaussian%20Scale-Space%20Fields%0AAuthor%3A%20Felix%20Mujkanovic%20and%20Ntumba%20Elie%20Nsampi%20and%20Christian%20Theobalt%20and%20Hans-Peter%20Seidel%20and%20Thomas%20Leimk%C3%BChler%0AAbstract%3A%20%20%20Gaussian%20scale%20spaces%20are%20a%20cornerstone%20of%20signal%20representation%20and%0Aprocessing%2C%20with%20applications%20in%20filtering%2C%20multiscale%20analysis%2C%20anti-aliasing%2C%0Aand%20many%20more.%20However%2C%20obtaining%20such%20a%20scale%20space%20is%20costly%20and%20cumbersome%2C%0Ain%20particular%20for%20continuous%20representations%20such%20as%20neural%20fields.%20We%20present%0Aan%20efficient%20and%20lightweight%20method%20to%20learn%20the%20fully%20continuous%2C%20anisotropic%0AGaussian%20scale%20space%20of%20an%20arbitrary%20signal.%20Based%20on%20Fourier%20feature%0Amodulation%20and%20Lipschitz%20bounding%2C%20our%20approach%20is%20trained%20self-supervised%2C%0Ai.e.%2C%20training%20does%20not%20require%20any%20manual%20filtering.%20Our%20neural%20Gaussian%0Ascale-space%20fields%20faithfully%20capture%20multiscale%20representations%20across%20a%20broad%0Arange%20of%20modalities%2C%20and%20support%20a%20diverse%20set%20of%20applications.%20These%20include%0Aimages%2C%20geometry%2C%20light-stage%20data%2C%20texture%20anti-aliasing%2C%20and%20multiscale%0Aoptimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Gaussian%2520Scale-Space%2520Fields%26entry.906535625%3DFelix%2520Mujkanovic%2520and%2520Ntumba%2520Elie%2520Nsampi%2520and%2520Christian%2520Theobalt%2520and%2520Hans-Peter%2520Seidel%2520and%2520Thomas%2520Leimk%25C3%25BChler%26entry.1292438233%3D%2520%2520Gaussian%2520scale%2520spaces%2520are%2520a%2520cornerstone%2520of%2520signal%2520representation%2520and%250Aprocessing%252C%2520with%2520applications%2520in%2520filtering%252C%2520multiscale%2520analysis%252C%2520anti-aliasing%252C%250Aand%2520many%2520more.%2520However%252C%2520obtaining%2520such%2520a%2520scale%2520space%2520is%2520costly%2520and%2520cumbersome%252C%250Ain%2520particular%2520for%2520continuous%2520representations%2520such%2520as%2520neural%2520fields.%2520We%2520present%250Aan%2520efficient%2520and%2520lightweight%2520method%2520to%2520learn%2520the%2520fully%2520continuous%252C%2520anisotropic%250AGaussian%2520scale%2520space%2520of%2520an%2520arbitrary%2520signal.%2520Based%2520on%2520Fourier%2520feature%250Amodulation%2520and%2520Lipschitz%2520bounding%252C%2520our%2520approach%2520is%2520trained%2520self-supervised%252C%250Ai.e.%252C%2520training%2520does%2520not%2520require%2520any%2520manual%2520filtering.%2520Our%2520neural%2520Gaussian%250Ascale-space%2520fields%2520faithfully%2520capture%2520multiscale%2520representations%2520across%2520a%2520broad%250Arange%2520of%2520modalities%252C%2520and%2520support%2520a%2520diverse%2520set%2520of%2520applications.%2520These%2520include%250Aimages%252C%2520geometry%252C%2520light-stage%2520data%252C%2520texture%2520anti-aliasing%252C%2520and%2520multiscale%250Aoptimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Gaussian%20Scale-Space%20Fields&entry.906535625=Felix%20Mujkanovic%20and%20Ntumba%20Elie%20Nsampi%20and%20Christian%20Theobalt%20and%20Hans-Peter%20Seidel%20and%20Thomas%20Leimk%C3%BChler&entry.1292438233=%20%20Gaussian%20scale%20spaces%20are%20a%20cornerstone%20of%20signal%20representation%20and%0Aprocessing%2C%20with%20applications%20in%20filtering%2C%20multiscale%20analysis%2C%20anti-aliasing%2C%0Aand%20many%20more.%20However%2C%20obtaining%20such%20a%20scale%20space%20is%20costly%20and%20cumbersome%2C%0Ain%20particular%20for%20continuous%20representations%20such%20as%20neural%20fields.%20We%20present%0Aan%20efficient%20and%20lightweight%20method%20to%20learn%20the%20fully%20continuous%2C%20anisotropic%0AGaussian%20scale%20space%20of%20an%20arbitrary%20signal.%20Based%20on%20Fourier%20feature%0Amodulation%20and%20Lipschitz%20bounding%2C%20our%20approach%20is%20trained%20self-supervised%2C%0Ai.e.%2C%20training%20does%20not%20require%20any%20manual%20filtering.%20Our%20neural%20Gaussian%0Ascale-space%20fields%20faithfully%20capture%20multiscale%20representations%20across%20a%20broad%0Arange%20of%20modalities%2C%20and%20support%20a%20diverse%20set%20of%20applications.%20These%20include%0Aimages%2C%20geometry%2C%20light-stage%20data%2C%20texture%20anti-aliasing%2C%20and%20multiscale%0Aoptimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20980v1&entry.124074799=Read"},
{"title": "Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities", "author": "Vicky Zayats and Peter Chen and Melissa Ferrari and Dirk Padfield", "abstract": "  Integrating multiple generative foundation models, especially those trained\non different modalities, into something greater than the sum of its parts poses\nsignificant challenges. Two key hurdles are the availability of aligned data\n(concepts that contain similar meaning but is expressed differently in\ndifferent modalities), and effectively leveraging unimodal representations in\ncross-domain generative tasks, without compromising their original unimodal\ncapabilities.\n  We propose Zipper, a multi-tower decoder architecture that addresses these\nconcerns by using cross-attention to flexibly compose multimodal generative\nmodels from independently pre-trained unimodal decoders. In our experiments\nfusing speech and text modalities, we show the proposed architecture performs\nvery competitively in scenarios with limited aligned text-speech data. We also\nshowcase the flexibility of our model to selectively maintain unimodal (e.g.,\ntext-to-text generation) generation performance by freezing the corresponding\nmodal tower (e.g. text). In cross-modal tasks such as automatic speech\nrecognition (ASR) where the output modality is text, we show that freezing the\ntext backbone results in negligible performance degradation. In cross-modal\ntasks such as text-to-speech generation (TTS) where the output modality is\nspeech, we show that using a pre-trained speech backbone results in superior\nperformance to the baseline.\n", "link": "http://arxiv.org/abs/2405.18669v2", "date": "2024-05-31", "relevancy": 2.1987, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5956}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5242}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zipper%3A%20A%20Multi-Tower%20Decoder%20Architecture%20for%20Fusing%20Modalities&body=Title%3A%20Zipper%3A%20A%20Multi-Tower%20Decoder%20Architecture%20for%20Fusing%20Modalities%0AAuthor%3A%20Vicky%20Zayats%20and%20Peter%20Chen%20and%20Melissa%20Ferrari%20and%20Dirk%20Padfield%0AAbstract%3A%20%20%20Integrating%20multiple%20generative%20foundation%20models%2C%20especially%20those%20trained%0Aon%20different%20modalities%2C%20into%20something%20greater%20than%20the%20sum%20of%20its%20parts%20poses%0Asignificant%20challenges.%20Two%20key%20hurdles%20are%20the%20availability%20of%20aligned%20data%0A%28concepts%20that%20contain%20similar%20meaning%20but%20is%20expressed%20differently%20in%0Adifferent%20modalities%29%2C%20and%20effectively%20leveraging%20unimodal%20representations%20in%0Across-domain%20generative%20tasks%2C%20without%20compromising%20their%20original%20unimodal%0Acapabilities.%0A%20%20We%20propose%20Zipper%2C%20a%20multi-tower%20decoder%20architecture%20that%20addresses%20these%0Aconcerns%20by%20using%20cross-attention%20to%20flexibly%20compose%20multimodal%20generative%0Amodels%20from%20independently%20pre-trained%20unimodal%20decoders.%20In%20our%20experiments%0Afusing%20speech%20and%20text%20modalities%2C%20we%20show%20the%20proposed%20architecture%20performs%0Avery%20competitively%20in%20scenarios%20with%20limited%20aligned%20text-speech%20data.%20We%20also%0Ashowcase%20the%20flexibility%20of%20our%20model%20to%20selectively%20maintain%20unimodal%20%28e.g.%2C%0Atext-to-text%20generation%29%20generation%20performance%20by%20freezing%20the%20corresponding%0Amodal%20tower%20%28e.g.%20text%29.%20In%20cross-modal%20tasks%20such%20as%20automatic%20speech%0Arecognition%20%28ASR%29%20where%20the%20output%20modality%20is%20text%2C%20we%20show%20that%20freezing%20the%0Atext%20backbone%20results%20in%20negligible%20performance%20degradation.%20In%20cross-modal%0Atasks%20such%20as%20text-to-speech%20generation%20%28TTS%29%20where%20the%20output%20modality%20is%0Aspeech%2C%20we%20show%20that%20using%20a%20pre-trained%20speech%20backbone%20results%20in%20superior%0Aperformance%20to%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18669v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZipper%253A%2520A%2520Multi-Tower%2520Decoder%2520Architecture%2520for%2520Fusing%2520Modalities%26entry.906535625%3DVicky%2520Zayats%2520and%2520Peter%2520Chen%2520and%2520Melissa%2520Ferrari%2520and%2520Dirk%2520Padfield%26entry.1292438233%3D%2520%2520Integrating%2520multiple%2520generative%2520foundation%2520models%252C%2520especially%2520those%2520trained%250Aon%2520different%2520modalities%252C%2520into%2520something%2520greater%2520than%2520the%2520sum%2520of%2520its%2520parts%2520poses%250Asignificant%2520challenges.%2520Two%2520key%2520hurdles%2520are%2520the%2520availability%2520of%2520aligned%2520data%250A%2528concepts%2520that%2520contain%2520similar%2520meaning%2520but%2520is%2520expressed%2520differently%2520in%250Adifferent%2520modalities%2529%252C%2520and%2520effectively%2520leveraging%2520unimodal%2520representations%2520in%250Across-domain%2520generative%2520tasks%252C%2520without%2520compromising%2520their%2520original%2520unimodal%250Acapabilities.%250A%2520%2520We%2520propose%2520Zipper%252C%2520a%2520multi-tower%2520decoder%2520architecture%2520that%2520addresses%2520these%250Aconcerns%2520by%2520using%2520cross-attention%2520to%2520flexibly%2520compose%2520multimodal%2520generative%250Amodels%2520from%2520independently%2520pre-trained%2520unimodal%2520decoders.%2520In%2520our%2520experiments%250Afusing%2520speech%2520and%2520text%2520modalities%252C%2520we%2520show%2520the%2520proposed%2520architecture%2520performs%250Avery%2520competitively%2520in%2520scenarios%2520with%2520limited%2520aligned%2520text-speech%2520data.%2520We%2520also%250Ashowcase%2520the%2520flexibility%2520of%2520our%2520model%2520to%2520selectively%2520maintain%2520unimodal%2520%2528e.g.%252C%250Atext-to-text%2520generation%2529%2520generation%2520performance%2520by%2520freezing%2520the%2520corresponding%250Amodal%2520tower%2520%2528e.g.%2520text%2529.%2520In%2520cross-modal%2520tasks%2520such%2520as%2520automatic%2520speech%250Arecognition%2520%2528ASR%2529%2520where%2520the%2520output%2520modality%2520is%2520text%252C%2520we%2520show%2520that%2520freezing%2520the%250Atext%2520backbone%2520results%2520in%2520negligible%2520performance%2520degradation.%2520In%2520cross-modal%250Atasks%2520such%2520as%2520text-to-speech%2520generation%2520%2528TTS%2529%2520where%2520the%2520output%2520modality%2520is%250Aspeech%252C%2520we%2520show%2520that%2520using%2520a%2520pre-trained%2520speech%2520backbone%2520results%2520in%2520superior%250Aperformance%2520to%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18669v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zipper%3A%20A%20Multi-Tower%20Decoder%20Architecture%20for%20Fusing%20Modalities&entry.906535625=Vicky%20Zayats%20and%20Peter%20Chen%20and%20Melissa%20Ferrari%20and%20Dirk%20Padfield&entry.1292438233=%20%20Integrating%20multiple%20generative%20foundation%20models%2C%20especially%20those%20trained%0Aon%20different%20modalities%2C%20into%20something%20greater%20than%20the%20sum%20of%20its%20parts%20poses%0Asignificant%20challenges.%20Two%20key%20hurdles%20are%20the%20availability%20of%20aligned%20data%0A%28concepts%20that%20contain%20similar%20meaning%20but%20is%20expressed%20differently%20in%0Adifferent%20modalities%29%2C%20and%20effectively%20leveraging%20unimodal%20representations%20in%0Across-domain%20generative%20tasks%2C%20without%20compromising%20their%20original%20unimodal%0Acapabilities.%0A%20%20We%20propose%20Zipper%2C%20a%20multi-tower%20decoder%20architecture%20that%20addresses%20these%0Aconcerns%20by%20using%20cross-attention%20to%20flexibly%20compose%20multimodal%20generative%0Amodels%20from%20independently%20pre-trained%20unimodal%20decoders.%20In%20our%20experiments%0Afusing%20speech%20and%20text%20modalities%2C%20we%20show%20the%20proposed%20architecture%20performs%0Avery%20competitively%20in%20scenarios%20with%20limited%20aligned%20text-speech%20data.%20We%20also%0Ashowcase%20the%20flexibility%20of%20our%20model%20to%20selectively%20maintain%20unimodal%20%28e.g.%2C%0Atext-to-text%20generation%29%20generation%20performance%20by%20freezing%20the%20corresponding%0Amodal%20tower%20%28e.g.%20text%29.%20In%20cross-modal%20tasks%20such%20as%20automatic%20speech%0Arecognition%20%28ASR%29%20where%20the%20output%20modality%20is%20text%2C%20we%20show%20that%20freezing%20the%0Atext%20backbone%20results%20in%20negligible%20performance%20degradation.%20In%20cross-modal%0Atasks%20such%20as%20text-to-speech%20generation%20%28TTS%29%20where%20the%20output%20modality%20is%0Aspeech%2C%20we%20show%20that%20using%20a%20pre-trained%20speech%20backbone%20results%20in%20superior%0Aperformance%20to%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18669v2&entry.124074799=Read"},
{"title": "Fast yet Safe: Early-Exiting with Risk Control", "author": "Metod Jazbec and Alexander Timans and Tin Had\u017ei Veljkovi\u0107 and Kaspar Sakmann and Dan Zhang and Christian A. Naesseth and Eric Nalisnick", "abstract": "  Scaling machine learning models significantly improves their performance.\nHowever, such gains come at the cost of inference being slow and\nresource-intensive. Early-exit neural networks (EENNs) offer a promising\nsolution: they accelerate inference by allowing intermediate layers to exit and\nproduce a prediction early. Yet a fundamental issue with EENNs is how to\ndetermine when to exit without severely degrading performance. In other words,\nwhen is it 'safe' for an EENN to go 'fast'? To address this issue, we\ninvestigate how to adapt frameworks of risk control to EENNs. Risk control\noffers a distribution-free, post-hoc solution that tunes the EENN's exiting\nmechanism so that exits only occur when the output is of sufficient quality. We\nempirically validate our insights on a range of vision and language tasks,\ndemonstrating that risk control can produce substantial computational savings,\nall the while preserving user-specified performance goals.\n", "link": "http://arxiv.org/abs/2405.20915v1", "date": "2024-05-31", "relevancy": 2.1875, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.6089}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5073}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20yet%20Safe%3A%20Early-Exiting%20with%20Risk%20Control&body=Title%3A%20Fast%20yet%20Safe%3A%20Early-Exiting%20with%20Risk%20Control%0AAuthor%3A%20Metod%20Jazbec%20and%20Alexander%20Timans%20and%20Tin%20Had%C5%BEi%20Veljkovi%C4%87%20and%20Kaspar%20Sakmann%20and%20Dan%20Zhang%20and%20Christian%20A.%20Naesseth%20and%20Eric%20Nalisnick%0AAbstract%3A%20%20%20Scaling%20machine%20learning%20models%20significantly%20improves%20their%20performance.%0AHowever%2C%20such%20gains%20come%20at%20the%20cost%20of%20inference%20being%20slow%20and%0Aresource-intensive.%20Early-exit%20neural%20networks%20%28EENNs%29%20offer%20a%20promising%0Asolution%3A%20they%20accelerate%20inference%20by%20allowing%20intermediate%20layers%20to%20exit%20and%0Aproduce%20a%20prediction%20early.%20Yet%20a%20fundamental%20issue%20with%20EENNs%20is%20how%20to%0Adetermine%20when%20to%20exit%20without%20severely%20degrading%20performance.%20In%20other%20words%2C%0Awhen%20is%20it%20%27safe%27%20for%20an%20EENN%20to%20go%20%27fast%27%3F%20To%20address%20this%20issue%2C%20we%0Ainvestigate%20how%20to%20adapt%20frameworks%20of%20risk%20control%20to%20EENNs.%20Risk%20control%0Aoffers%20a%20distribution-free%2C%20post-hoc%20solution%20that%20tunes%20the%20EENN%27s%20exiting%0Amechanism%20so%20that%20exits%20only%20occur%20when%20the%20output%20is%20of%20sufficient%20quality.%20We%0Aempirically%20validate%20our%20insights%20on%20a%20range%20of%20vision%20and%20language%20tasks%2C%0Ademonstrating%20that%20risk%20control%20can%20produce%20substantial%20computational%20savings%2C%0Aall%20the%20while%20preserving%20user-specified%20performance%20goals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520yet%2520Safe%253A%2520Early-Exiting%2520with%2520Risk%2520Control%26entry.906535625%3DMetod%2520Jazbec%2520and%2520Alexander%2520Timans%2520and%2520Tin%2520Had%25C5%25BEi%2520Veljkovi%25C4%2587%2520and%2520Kaspar%2520Sakmann%2520and%2520Dan%2520Zhang%2520and%2520Christian%2520A.%2520Naesseth%2520and%2520Eric%2520Nalisnick%26entry.1292438233%3D%2520%2520Scaling%2520machine%2520learning%2520models%2520significantly%2520improves%2520their%2520performance.%250AHowever%252C%2520such%2520gains%2520come%2520at%2520the%2520cost%2520of%2520inference%2520being%2520slow%2520and%250Aresource-intensive.%2520Early-exit%2520neural%2520networks%2520%2528EENNs%2529%2520offer%2520a%2520promising%250Asolution%253A%2520they%2520accelerate%2520inference%2520by%2520allowing%2520intermediate%2520layers%2520to%2520exit%2520and%250Aproduce%2520a%2520prediction%2520early.%2520Yet%2520a%2520fundamental%2520issue%2520with%2520EENNs%2520is%2520how%2520to%250Adetermine%2520when%2520to%2520exit%2520without%2520severely%2520degrading%2520performance.%2520In%2520other%2520words%252C%250Awhen%2520is%2520it%2520%2527safe%2527%2520for%2520an%2520EENN%2520to%2520go%2520%2527fast%2527%253F%2520To%2520address%2520this%2520issue%252C%2520we%250Ainvestigate%2520how%2520to%2520adapt%2520frameworks%2520of%2520risk%2520control%2520to%2520EENNs.%2520Risk%2520control%250Aoffers%2520a%2520distribution-free%252C%2520post-hoc%2520solution%2520that%2520tunes%2520the%2520EENN%2527s%2520exiting%250Amechanism%2520so%2520that%2520exits%2520only%2520occur%2520when%2520the%2520output%2520is%2520of%2520sufficient%2520quality.%2520We%250Aempirically%2520validate%2520our%2520insights%2520on%2520a%2520range%2520of%2520vision%2520and%2520language%2520tasks%252C%250Ademonstrating%2520that%2520risk%2520control%2520can%2520produce%2520substantial%2520computational%2520savings%252C%250Aall%2520the%2520while%2520preserving%2520user-specified%2520performance%2520goals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20yet%20Safe%3A%20Early-Exiting%20with%20Risk%20Control&entry.906535625=Metod%20Jazbec%20and%20Alexander%20Timans%20and%20Tin%20Had%C5%BEi%20Veljkovi%C4%87%20and%20Kaspar%20Sakmann%20and%20Dan%20Zhang%20and%20Christian%20A.%20Naesseth%20and%20Eric%20Nalisnick&entry.1292438233=%20%20Scaling%20machine%20learning%20models%20significantly%20improves%20their%20performance.%0AHowever%2C%20such%20gains%20come%20at%20the%20cost%20of%20inference%20being%20slow%20and%0Aresource-intensive.%20Early-exit%20neural%20networks%20%28EENNs%29%20offer%20a%20promising%0Asolution%3A%20they%20accelerate%20inference%20by%20allowing%20intermediate%20layers%20to%20exit%20and%0Aproduce%20a%20prediction%20early.%20Yet%20a%20fundamental%20issue%20with%20EENNs%20is%20how%20to%0Adetermine%20when%20to%20exit%20without%20severely%20degrading%20performance.%20In%20other%20words%2C%0Awhen%20is%20it%20%27safe%27%20for%20an%20EENN%20to%20go%20%27fast%27%3F%20To%20address%20this%20issue%2C%20we%0Ainvestigate%20how%20to%20adapt%20frameworks%20of%20risk%20control%20to%20EENNs.%20Risk%20control%0Aoffers%20a%20distribution-free%2C%20post-hoc%20solution%20that%20tunes%20the%20EENN%27s%20exiting%0Amechanism%20so%20that%20exits%20only%20occur%20when%20the%20output%20is%20of%20sufficient%20quality.%20We%0Aempirically%20validate%20our%20insights%20on%20a%20range%20of%20vision%20and%20language%20tasks%2C%0Ademonstrating%20that%20risk%20control%20can%20produce%20substantial%20computational%20savings%2C%0Aall%20the%20while%20preserving%20user-specified%20performance%20goals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20915v1&entry.124074799=Read"},
{"title": "Improving Generalization and Convergence by Enhancing Implicit\n  Regularization", "author": "Mingze Wang and Haotian He and Jinbo Wang and Zilin Wang and Guanhua Huang and Feiyu Xiong and Zhiyu Li and Weinan E and Lei Wu", "abstract": "  In this work, we propose an Implicit Regularization Enhancement (IRE)\nframework to accelerate the discovery of flat solutions in deep learning,\nthereby improving generalization and convergence. Specifically, IRE decouples\nthe dynamics of flat and sharp directions, which boosts the sharpness reduction\nalong flat directions while maintaining the training stability in sharp\ndirections. We show that IRE can be practically incorporated with {\\em generic\nbase optimizers} without introducing significant computational overload.\nExperiments show that IRE consistently improves the generalization performance\nfor image classification tasks across a variety of benchmark datasets\n(CIFAR-10/100, ImageNet) and models (ResNets and ViTs). Surprisingly, IRE also\nachieves a $2\\times$ {\\em speed-up} compared to AdamW in the pre-training of\nLlama models (of sizes ranging from 60M to 229M) on datasets including\nWikitext-103, Minipile, and Openwebtext. Moreover, we provide theoretical\nguarantees, showing that IRE can substantially accelerate the convergence\ntowards flat minima in Sharpness-aware Minimization (SAM).\n", "link": "http://arxiv.org/abs/2405.20763v1", "date": "2024-05-31", "relevancy": 2.1818, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5489}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5486}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Generalization%20and%20Convergence%20by%20Enhancing%20Implicit%0A%20%20Regularization&body=Title%3A%20Improving%20Generalization%20and%20Convergence%20by%20Enhancing%20Implicit%0A%20%20Regularization%0AAuthor%3A%20Mingze%20Wang%20and%20Haotian%20He%20and%20Jinbo%20Wang%20and%20Zilin%20Wang%20and%20Guanhua%20Huang%20and%20Feiyu%20Xiong%20and%20Zhiyu%20Li%20and%20Weinan%20E%20and%20Lei%20Wu%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20an%20Implicit%20Regularization%20Enhancement%20%28IRE%29%0Aframework%20to%20accelerate%20the%20discovery%20of%20flat%20solutions%20in%20deep%20learning%2C%0Athereby%20improving%20generalization%20and%20convergence.%20Specifically%2C%20IRE%20decouples%0Athe%20dynamics%20of%20flat%20and%20sharp%20directions%2C%20which%20boosts%20the%20sharpness%20reduction%0Aalong%20flat%20directions%20while%20maintaining%20the%20training%20stability%20in%20sharp%0Adirections.%20We%20show%20that%20IRE%20can%20be%20practically%20incorporated%20with%20%7B%5Cem%20generic%0Abase%20optimizers%7D%20without%20introducing%20significant%20computational%20overload.%0AExperiments%20show%20that%20IRE%20consistently%20improves%20the%20generalization%20performance%0Afor%20image%20classification%20tasks%20across%20a%20variety%20of%20benchmark%20datasets%0A%28CIFAR-10/100%2C%20ImageNet%29%20and%20models%20%28ResNets%20and%20ViTs%29.%20Surprisingly%2C%20IRE%20also%0Aachieves%20a%20%242%5Ctimes%24%20%7B%5Cem%20speed-up%7D%20compared%20to%20AdamW%20in%20the%20pre-training%20of%0ALlama%20models%20%28of%20sizes%20ranging%20from%2060M%20to%20229M%29%20on%20datasets%20including%0AWikitext-103%2C%20Minipile%2C%20and%20Openwebtext.%20Moreover%2C%20we%20provide%20theoretical%0Aguarantees%2C%20showing%20that%20IRE%20can%20substantially%20accelerate%20the%20convergence%0Atowards%20flat%20minima%20in%20Sharpness-aware%20Minimization%20%28SAM%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Generalization%2520and%2520Convergence%2520by%2520Enhancing%2520Implicit%250A%2520%2520Regularization%26entry.906535625%3DMingze%2520Wang%2520and%2520Haotian%2520He%2520and%2520Jinbo%2520Wang%2520and%2520Zilin%2520Wang%2520and%2520Guanhua%2520Huang%2520and%2520Feiyu%2520Xiong%2520and%2520Zhiyu%2520Li%2520and%2520Weinan%2520E%2520and%2520Lei%2520Wu%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520an%2520Implicit%2520Regularization%2520Enhancement%2520%2528IRE%2529%250Aframework%2520to%2520accelerate%2520the%2520discovery%2520of%2520flat%2520solutions%2520in%2520deep%2520learning%252C%250Athereby%2520improving%2520generalization%2520and%2520convergence.%2520Specifically%252C%2520IRE%2520decouples%250Athe%2520dynamics%2520of%2520flat%2520and%2520sharp%2520directions%252C%2520which%2520boosts%2520the%2520sharpness%2520reduction%250Aalong%2520flat%2520directions%2520while%2520maintaining%2520the%2520training%2520stability%2520in%2520sharp%250Adirections.%2520We%2520show%2520that%2520IRE%2520can%2520be%2520practically%2520incorporated%2520with%2520%257B%255Cem%2520generic%250Abase%2520optimizers%257D%2520without%2520introducing%2520significant%2520computational%2520overload.%250AExperiments%2520show%2520that%2520IRE%2520consistently%2520improves%2520the%2520generalization%2520performance%250Afor%2520image%2520classification%2520tasks%2520across%2520a%2520variety%2520of%2520benchmark%2520datasets%250A%2528CIFAR-10/100%252C%2520ImageNet%2529%2520and%2520models%2520%2528ResNets%2520and%2520ViTs%2529.%2520Surprisingly%252C%2520IRE%2520also%250Aachieves%2520a%2520%25242%255Ctimes%2524%2520%257B%255Cem%2520speed-up%257D%2520compared%2520to%2520AdamW%2520in%2520the%2520pre-training%2520of%250ALlama%2520models%2520%2528of%2520sizes%2520ranging%2520from%252060M%2520to%2520229M%2529%2520on%2520datasets%2520including%250AWikitext-103%252C%2520Minipile%252C%2520and%2520Openwebtext.%2520Moreover%252C%2520we%2520provide%2520theoretical%250Aguarantees%252C%2520showing%2520that%2520IRE%2520can%2520substantially%2520accelerate%2520the%2520convergence%250Atowards%2520flat%2520minima%2520in%2520Sharpness-aware%2520Minimization%2520%2528SAM%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Generalization%20and%20Convergence%20by%20Enhancing%20Implicit%0A%20%20Regularization&entry.906535625=Mingze%20Wang%20and%20Haotian%20He%20and%20Jinbo%20Wang%20and%20Zilin%20Wang%20and%20Guanhua%20Huang%20and%20Feiyu%20Xiong%20and%20Zhiyu%20Li%20and%20Weinan%20E%20and%20Lei%20Wu&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20an%20Implicit%20Regularization%20Enhancement%20%28IRE%29%0Aframework%20to%20accelerate%20the%20discovery%20of%20flat%20solutions%20in%20deep%20learning%2C%0Athereby%20improving%20generalization%20and%20convergence.%20Specifically%2C%20IRE%20decouples%0Athe%20dynamics%20of%20flat%20and%20sharp%20directions%2C%20which%20boosts%20the%20sharpness%20reduction%0Aalong%20flat%20directions%20while%20maintaining%20the%20training%20stability%20in%20sharp%0Adirections.%20We%20show%20that%20IRE%20can%20be%20practically%20incorporated%20with%20%7B%5Cem%20generic%0Abase%20optimizers%7D%20without%20introducing%20significant%20computational%20overload.%0AExperiments%20show%20that%20IRE%20consistently%20improves%20the%20generalization%20performance%0Afor%20image%20classification%20tasks%20across%20a%20variety%20of%20benchmark%20datasets%0A%28CIFAR-10/100%2C%20ImageNet%29%20and%20models%20%28ResNets%20and%20ViTs%29.%20Surprisingly%2C%20IRE%20also%0Aachieves%20a%20%242%5Ctimes%24%20%7B%5Cem%20speed-up%7D%20compared%20to%20AdamW%20in%20the%20pre-training%20of%0ALlama%20models%20%28of%20sizes%20ranging%20from%2060M%20to%20229M%29%20on%20datasets%20including%0AWikitext-103%2C%20Minipile%2C%20and%20Openwebtext.%20Moreover%2C%20we%20provide%20theoretical%0Aguarantees%2C%20showing%20that%20IRE%20can%20substantially%20accelerate%20the%20convergence%0Atowards%20flat%20minima%20in%20Sharpness-aware%20Minimization%20%28SAM%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20763v1&entry.124074799=Read"},
{"title": "Early Stopping Criteria for Training Generative Adversarial Networks in\n  Biomedical Imaging", "author": "Muhammad Muneeb Saad and Mubashir Husain Rehmani and Ruairi O'Reilly", "abstract": "  Generative Adversarial Networks (GANs) have high computational costs to train\ntheir complex architectures. Throughout the training process, GANs' output is\nanalyzed qualitatively based on the loss and synthetic images' diversity and\nquality. Based on this qualitative analysis, training is manually halted once\nthe desired synthetic images are generated. By utilizing an early stopping\ncriterion, the computational cost and dependence on manual oversight can be\nreduced yet impacted by training problems such as mode collapse,\nnon-convergence, and instability. This is particularly prevalent in biomedical\nimagery, where training problems degrade the diversity and quality of synthetic\nimages, and the high computational cost associated with training makes complex\narchitectures increasingly inaccessible. This work proposes a novel early\nstopping criteria to quantitatively detect training problems, halt training,\nand reduce the computational costs associated with synthesizing biomedical\nimages. Firstly, the range of generator and discriminator loss values is\ninvestigated to assess whether mode collapse, non-convergence, and instability\noccur sequentially, concurrently, or interchangeably throughout the training of\nGANs. Secondly, utilizing these occurrences in conjunction with the Mean\nStructural Similarity Index (MS-SSIM) and Fr\\'echet Inception Distance (FID)\nscores of synthetic images forms the basis of the proposed early stopping\ncriteria. This work helps identify the occurrence of training problems in GANs\nusing low-resource computational cost and reduces training time to generate\ndiversified and high-quality synthetic images.\n", "link": "http://arxiv.org/abs/2405.20987v1", "date": "2024-05-31", "relevancy": 2.1793, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5794}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5215}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Stopping%20Criteria%20for%20Training%20Generative%20Adversarial%20Networks%20in%0A%20%20Biomedical%20Imaging&body=Title%3A%20Early%20Stopping%20Criteria%20for%20Training%20Generative%20Adversarial%20Networks%20in%0A%20%20Biomedical%20Imaging%0AAuthor%3A%20Muhammad%20Muneeb%20Saad%20and%20Mubashir%20Husain%20Rehmani%20and%20Ruairi%20O%27Reilly%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20high%20computational%20costs%20to%20train%0Atheir%20complex%20architectures.%20Throughout%20the%20training%20process%2C%20GANs%27%20output%20is%0Aanalyzed%20qualitatively%20based%20on%20the%20loss%20and%20synthetic%20images%27%20diversity%20and%0Aquality.%20Based%20on%20this%20qualitative%20analysis%2C%20training%20is%20manually%20halted%20once%0Athe%20desired%20synthetic%20images%20are%20generated.%20By%20utilizing%20an%20early%20stopping%0Acriterion%2C%20the%20computational%20cost%20and%20dependence%20on%20manual%20oversight%20can%20be%0Areduced%20yet%20impacted%20by%20training%20problems%20such%20as%20mode%20collapse%2C%0Anon-convergence%2C%20and%20instability.%20This%20is%20particularly%20prevalent%20in%20biomedical%0Aimagery%2C%20where%20training%20problems%20degrade%20the%20diversity%20and%20quality%20of%20synthetic%0Aimages%2C%20and%20the%20high%20computational%20cost%20associated%20with%20training%20makes%20complex%0Aarchitectures%20increasingly%20inaccessible.%20This%20work%20proposes%20a%20novel%20early%0Astopping%20criteria%20to%20quantitatively%20detect%20training%20problems%2C%20halt%20training%2C%0Aand%20reduce%20the%20computational%20costs%20associated%20with%20synthesizing%20biomedical%0Aimages.%20Firstly%2C%20the%20range%20of%20generator%20and%20discriminator%20loss%20values%20is%0Ainvestigated%20to%20assess%20whether%20mode%20collapse%2C%20non-convergence%2C%20and%20instability%0Aoccur%20sequentially%2C%20concurrently%2C%20or%20interchangeably%20throughout%20the%20training%20of%0AGANs.%20Secondly%2C%20utilizing%20these%20occurrences%20in%20conjunction%20with%20the%20Mean%0AStructural%20Similarity%20Index%20%28MS-SSIM%29%20and%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%0Ascores%20of%20synthetic%20images%20forms%20the%20basis%20of%20the%20proposed%20early%20stopping%0Acriteria.%20This%20work%20helps%20identify%20the%20occurrence%20of%20training%20problems%20in%20GANs%0Ausing%20low-resource%20computational%20cost%20and%20reduces%20training%20time%20to%20generate%0Adiversified%20and%20high-quality%20synthetic%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Stopping%2520Criteria%2520for%2520Training%2520Generative%2520Adversarial%2520Networks%2520in%250A%2520%2520Biomedical%2520Imaging%26entry.906535625%3DMuhammad%2520Muneeb%2520Saad%2520and%2520Mubashir%2520Husain%2520Rehmani%2520and%2520Ruairi%2520O%2527Reilly%26entry.1292438233%3D%2520%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520have%2520high%2520computational%2520costs%2520to%2520train%250Atheir%2520complex%2520architectures.%2520Throughout%2520the%2520training%2520process%252C%2520GANs%2527%2520output%2520is%250Aanalyzed%2520qualitatively%2520based%2520on%2520the%2520loss%2520and%2520synthetic%2520images%2527%2520diversity%2520and%250Aquality.%2520Based%2520on%2520this%2520qualitative%2520analysis%252C%2520training%2520is%2520manually%2520halted%2520once%250Athe%2520desired%2520synthetic%2520images%2520are%2520generated.%2520By%2520utilizing%2520an%2520early%2520stopping%250Acriterion%252C%2520the%2520computational%2520cost%2520and%2520dependence%2520on%2520manual%2520oversight%2520can%2520be%250Areduced%2520yet%2520impacted%2520by%2520training%2520problems%2520such%2520as%2520mode%2520collapse%252C%250Anon-convergence%252C%2520and%2520instability.%2520This%2520is%2520particularly%2520prevalent%2520in%2520biomedical%250Aimagery%252C%2520where%2520training%2520problems%2520degrade%2520the%2520diversity%2520and%2520quality%2520of%2520synthetic%250Aimages%252C%2520and%2520the%2520high%2520computational%2520cost%2520associated%2520with%2520training%2520makes%2520complex%250Aarchitectures%2520increasingly%2520inaccessible.%2520This%2520work%2520proposes%2520a%2520novel%2520early%250Astopping%2520criteria%2520to%2520quantitatively%2520detect%2520training%2520problems%252C%2520halt%2520training%252C%250Aand%2520reduce%2520the%2520computational%2520costs%2520associated%2520with%2520synthesizing%2520biomedical%250Aimages.%2520Firstly%252C%2520the%2520range%2520of%2520generator%2520and%2520discriminator%2520loss%2520values%2520is%250Ainvestigated%2520to%2520assess%2520whether%2520mode%2520collapse%252C%2520non-convergence%252C%2520and%2520instability%250Aoccur%2520sequentially%252C%2520concurrently%252C%2520or%2520interchangeably%2520throughout%2520the%2520training%2520of%250AGANs.%2520Secondly%252C%2520utilizing%2520these%2520occurrences%2520in%2520conjunction%2520with%2520the%2520Mean%250AStructural%2520Similarity%2520Index%2520%2528MS-SSIM%2529%2520and%2520Fr%255C%2527echet%2520Inception%2520Distance%2520%2528FID%2529%250Ascores%2520of%2520synthetic%2520images%2520forms%2520the%2520basis%2520of%2520the%2520proposed%2520early%2520stopping%250Acriteria.%2520This%2520work%2520helps%2520identify%2520the%2520occurrence%2520of%2520training%2520problems%2520in%2520GANs%250Ausing%2520low-resource%2520computational%2520cost%2520and%2520reduces%2520training%2520time%2520to%2520generate%250Adiversified%2520and%2520high-quality%2520synthetic%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Stopping%20Criteria%20for%20Training%20Generative%20Adversarial%20Networks%20in%0A%20%20Biomedical%20Imaging&entry.906535625=Muhammad%20Muneeb%20Saad%20and%20Mubashir%20Husain%20Rehmani%20and%20Ruairi%20O%27Reilly&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20high%20computational%20costs%20to%20train%0Atheir%20complex%20architectures.%20Throughout%20the%20training%20process%2C%20GANs%27%20output%20is%0Aanalyzed%20qualitatively%20based%20on%20the%20loss%20and%20synthetic%20images%27%20diversity%20and%0Aquality.%20Based%20on%20this%20qualitative%20analysis%2C%20training%20is%20manually%20halted%20once%0Athe%20desired%20synthetic%20images%20are%20generated.%20By%20utilizing%20an%20early%20stopping%0Acriterion%2C%20the%20computational%20cost%20and%20dependence%20on%20manual%20oversight%20can%20be%0Areduced%20yet%20impacted%20by%20training%20problems%20such%20as%20mode%20collapse%2C%0Anon-convergence%2C%20and%20instability.%20This%20is%20particularly%20prevalent%20in%20biomedical%0Aimagery%2C%20where%20training%20problems%20degrade%20the%20diversity%20and%20quality%20of%20synthetic%0Aimages%2C%20and%20the%20high%20computational%20cost%20associated%20with%20training%20makes%20complex%0Aarchitectures%20increasingly%20inaccessible.%20This%20work%20proposes%20a%20novel%20early%0Astopping%20criteria%20to%20quantitatively%20detect%20training%20problems%2C%20halt%20training%2C%0Aand%20reduce%20the%20computational%20costs%20associated%20with%20synthesizing%20biomedical%0Aimages.%20Firstly%2C%20the%20range%20of%20generator%20and%20discriminator%20loss%20values%20is%0Ainvestigated%20to%20assess%20whether%20mode%20collapse%2C%20non-convergence%2C%20and%20instability%0Aoccur%20sequentially%2C%20concurrently%2C%20or%20interchangeably%20throughout%20the%20training%20of%0AGANs.%20Secondly%2C%20utilizing%20these%20occurrences%20in%20conjunction%20with%20the%20Mean%0AStructural%20Similarity%20Index%20%28MS-SSIM%29%20and%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%0Ascores%20of%20synthetic%20images%20forms%20the%20basis%20of%20the%20proposed%20early%20stopping%0Acriteria.%20This%20work%20helps%20identify%20the%20occurrence%20of%20training%20problems%20in%20GANs%0Ausing%20low-resource%20computational%20cost%20and%20reduces%20training%20time%20to%20generate%0Adiversified%20and%20high-quality%20synthetic%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20987v1&entry.124074799=Read"},
{"title": "Context-aware Difference Distilling for Multi-change Captioning", "author": "Yunbin Tu and Liang Li and Li Su and Zheng-Jun Zha and Chenggang Yan and Qingming Huang", "abstract": "  Multi-change captioning aims to describe complex and coupled changes within\nan image pair in natural language. Compared with single-change captioning, this\ntask requires the model to have higher-level cognition ability to reason an\narbitrary number of changes. In this paper, we propose a novel context-aware\ndifference distilling (CARD) network to capture all genuine changes for\nyielding sentences. Given an image pair, CARD first decouples context features\nthat aggregate all similar/dissimilar semantics, termed common/difference\ncontext features. Then, the consistency and independence constraints are\ndesigned to guarantee the alignment/discrepancy of common/difference context\nfeatures. Further, the common context features guide the model to mine locally\nunchanged features, which are subtracted from the pair to distill locally\ndifference features. Next, the difference context features augment the locally\ndifference features to ensure that all changes are distilled. In this way, we\nobtain an omni-representation of all changes, which is translated into\nlinguistic sentences by a transformer decoder. Extensive experiments on three\npublic datasets show CARD performs favourably against state-of-the-art\nmethods.The code is available at https://github.com/tuyunbin/CARD.\n", "link": "http://arxiv.org/abs/2405.20810v1", "date": "2024-05-31", "relevancy": 2.1773, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5461}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5447}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-aware%20Difference%20Distilling%20for%20Multi-change%20Captioning&body=Title%3A%20Context-aware%20Difference%20Distilling%20for%20Multi-change%20Captioning%0AAuthor%3A%20Yunbin%20Tu%20and%20Liang%20Li%20and%20Li%20Su%20and%20Zheng-Jun%20Zha%20and%20Chenggang%20Yan%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20Multi-change%20captioning%20aims%20to%20describe%20complex%20and%20coupled%20changes%20within%0Aan%20image%20pair%20in%20natural%20language.%20Compared%20with%20single-change%20captioning%2C%20this%0Atask%20requires%20the%20model%20to%20have%20higher-level%20cognition%20ability%20to%20reason%20an%0Aarbitrary%20number%20of%20changes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20context-aware%0Adifference%20distilling%20%28CARD%29%20network%20to%20capture%20all%20genuine%20changes%20for%0Ayielding%20sentences.%20Given%20an%20image%20pair%2C%20CARD%20first%20decouples%20context%20features%0Athat%20aggregate%20all%20similar/dissimilar%20semantics%2C%20termed%20common/difference%0Acontext%20features.%20Then%2C%20the%20consistency%20and%20independence%20constraints%20are%0Adesigned%20to%20guarantee%20the%20alignment/discrepancy%20of%20common/difference%20context%0Afeatures.%20Further%2C%20the%20common%20context%20features%20guide%20the%20model%20to%20mine%20locally%0Aunchanged%20features%2C%20which%20are%20subtracted%20from%20the%20pair%20to%20distill%20locally%0Adifference%20features.%20Next%2C%20the%20difference%20context%20features%20augment%20the%20locally%0Adifference%20features%20to%20ensure%20that%20all%20changes%20are%20distilled.%20In%20this%20way%2C%20we%0Aobtain%20an%20omni-representation%20of%20all%20changes%2C%20which%20is%20translated%20into%0Alinguistic%20sentences%20by%20a%20transformer%20decoder.%20Extensive%20experiments%20on%20three%0Apublic%20datasets%20show%20CARD%20performs%20favourably%20against%20state-of-the-art%0Amethods.The%20code%20is%20available%20at%20https%3A//github.com/tuyunbin/CARD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-aware%2520Difference%2520Distilling%2520for%2520Multi-change%2520Captioning%26entry.906535625%3DYunbin%2520Tu%2520and%2520Liang%2520Li%2520and%2520Li%2520Su%2520and%2520Zheng-Jun%2520Zha%2520and%2520Chenggang%2520Yan%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520Multi-change%2520captioning%2520aims%2520to%2520describe%2520complex%2520and%2520coupled%2520changes%2520within%250Aan%2520image%2520pair%2520in%2520natural%2520language.%2520Compared%2520with%2520single-change%2520captioning%252C%2520this%250Atask%2520requires%2520the%2520model%2520to%2520have%2520higher-level%2520cognition%2520ability%2520to%2520reason%2520an%250Aarbitrary%2520number%2520of%2520changes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520context-aware%250Adifference%2520distilling%2520%2528CARD%2529%2520network%2520to%2520capture%2520all%2520genuine%2520changes%2520for%250Ayielding%2520sentences.%2520Given%2520an%2520image%2520pair%252C%2520CARD%2520first%2520decouples%2520context%2520features%250Athat%2520aggregate%2520all%2520similar/dissimilar%2520semantics%252C%2520termed%2520common/difference%250Acontext%2520features.%2520Then%252C%2520the%2520consistency%2520and%2520independence%2520constraints%2520are%250Adesigned%2520to%2520guarantee%2520the%2520alignment/discrepancy%2520of%2520common/difference%2520context%250Afeatures.%2520Further%252C%2520the%2520common%2520context%2520features%2520guide%2520the%2520model%2520to%2520mine%2520locally%250Aunchanged%2520features%252C%2520which%2520are%2520subtracted%2520from%2520the%2520pair%2520to%2520distill%2520locally%250Adifference%2520features.%2520Next%252C%2520the%2520difference%2520context%2520features%2520augment%2520the%2520locally%250Adifference%2520features%2520to%2520ensure%2520that%2520all%2520changes%2520are%2520distilled.%2520In%2520this%2520way%252C%2520we%250Aobtain%2520an%2520omni-representation%2520of%2520all%2520changes%252C%2520which%2520is%2520translated%2520into%250Alinguistic%2520sentences%2520by%2520a%2520transformer%2520decoder.%2520Extensive%2520experiments%2520on%2520three%250Apublic%2520datasets%2520show%2520CARD%2520performs%2520favourably%2520against%2520state-of-the-art%250Amethods.The%2520code%2520is%2520available%2520at%2520https%253A//github.com/tuyunbin/CARD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-aware%20Difference%20Distilling%20for%20Multi-change%20Captioning&entry.906535625=Yunbin%20Tu%20and%20Liang%20Li%20and%20Li%20Su%20and%20Zheng-Jun%20Zha%20and%20Chenggang%20Yan%20and%20Qingming%20Huang&entry.1292438233=%20%20Multi-change%20captioning%20aims%20to%20describe%20complex%20and%20coupled%20changes%20within%0Aan%20image%20pair%20in%20natural%20language.%20Compared%20with%20single-change%20captioning%2C%20this%0Atask%20requires%20the%20model%20to%20have%20higher-level%20cognition%20ability%20to%20reason%20an%0Aarbitrary%20number%20of%20changes.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20context-aware%0Adifference%20distilling%20%28CARD%29%20network%20to%20capture%20all%20genuine%20changes%20for%0Ayielding%20sentences.%20Given%20an%20image%20pair%2C%20CARD%20first%20decouples%20context%20features%0Athat%20aggregate%20all%20similar/dissimilar%20semantics%2C%20termed%20common/difference%0Acontext%20features.%20Then%2C%20the%20consistency%20and%20independence%20constraints%20are%0Adesigned%20to%20guarantee%20the%20alignment/discrepancy%20of%20common/difference%20context%0Afeatures.%20Further%2C%20the%20common%20context%20features%20guide%20the%20model%20to%20mine%20locally%0Aunchanged%20features%2C%20which%20are%20subtracted%20from%20the%20pair%20to%20distill%20locally%0Adifference%20features.%20Next%2C%20the%20difference%20context%20features%20augment%20the%20locally%0Adifference%20features%20to%20ensure%20that%20all%20changes%20are%20distilled.%20In%20this%20way%2C%20we%0Aobtain%20an%20omni-representation%20of%20all%20changes%2C%20which%20is%20translated%20into%0Alinguistic%20sentences%20by%20a%20transformer%20decoder.%20Extensive%20experiments%20on%20three%0Apublic%20datasets%20show%20CARD%20performs%20favourably%20against%20state-of-the-art%0Amethods.The%20code%20is%20available%20at%20https%3A//github.com/tuyunbin/CARD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20810v1&entry.124074799=Read"},
{"title": "GAD-Generative Learning for HD Map-Free Autonomous Driving", "author": "Weijian Sun and Yanbo Jia and Qi Zeng and Zihao Liu and Jiang Liao and Yue Li and Xianfeng Li", "abstract": "  Deep-learning-based techniques have been widely adopted for autonomous\ndriving software stacks for mass production in recent years, focusing primarily\non perception modules, with some work extending this method to prediction\nmodules. However, the downstream planning and control modules are still\ndesigned with hefty handcrafted rules, dominated by optimization-based methods\nsuch as quadratic programming or model predictive control. This results in a\nperformance bottleneck for autonomous driving systems in that corner cases\nsimply cannot be solved by enumerating hand-crafted rules. We present a\ndeep-learning-based approach that brings prediction, decision, and planning\nmodules together with the attempt to overcome the rule-based methods'\ndeficiency in real-world applications of autonomous driving, especially for\nurban scenes. The DNN model we proposed is solely trained with 10 hours of\nhuman driver data, and it supports all mass-production ADAS features available\non the market to date. This method is deployed onto a Jiyue test car with no\nmodification to its factory-ready sensor set and compute platform. the\nfeasibility, usability, and commercial potential are demonstrated in this\narticle.\n", "link": "http://arxiv.org/abs/2405.00515v3", "date": "2024-05-31", "relevancy": 2.1694, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5446}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5444}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GAD-Generative%20Learning%20for%20HD%20Map-Free%20Autonomous%20Driving&body=Title%3A%20GAD-Generative%20Learning%20for%20HD%20Map-Free%20Autonomous%20Driving%0AAuthor%3A%20Weijian%20Sun%20and%20Yanbo%20Jia%20and%20Qi%20Zeng%20and%20Zihao%20Liu%20and%20Jiang%20Liao%20and%20Yue%20Li%20and%20Xianfeng%20Li%0AAbstract%3A%20%20%20Deep-learning-based%20techniques%20have%20been%20widely%20adopted%20for%20autonomous%0Adriving%20software%20stacks%20for%20mass%20production%20in%20recent%20years%2C%20focusing%20primarily%0Aon%20perception%20modules%2C%20with%20some%20work%20extending%20this%20method%20to%20prediction%0Amodules.%20However%2C%20the%20downstream%20planning%20and%20control%20modules%20are%20still%0Adesigned%20with%20hefty%20handcrafted%20rules%2C%20dominated%20by%20optimization-based%20methods%0Asuch%20as%20quadratic%20programming%20or%20model%20predictive%20control.%20This%20results%20in%20a%0Aperformance%20bottleneck%20for%20autonomous%20driving%20systems%20in%20that%20corner%20cases%0Asimply%20cannot%20be%20solved%20by%20enumerating%20hand-crafted%20rules.%20We%20present%20a%0Adeep-learning-based%20approach%20that%20brings%20prediction%2C%20decision%2C%20and%20planning%0Amodules%20together%20with%20the%20attempt%20to%20overcome%20the%20rule-based%20methods%27%0Adeficiency%20in%20real-world%20applications%20of%20autonomous%20driving%2C%20especially%20for%0Aurban%20scenes.%20The%20DNN%20model%20we%20proposed%20is%20solely%20trained%20with%2010%20hours%20of%0Ahuman%20driver%20data%2C%20and%20it%20supports%20all%20mass-production%20ADAS%20features%20available%0Aon%20the%20market%20to%20date.%20This%20method%20is%20deployed%20onto%20a%20Jiyue%20test%20car%20with%20no%0Amodification%20to%20its%20factory-ready%20sensor%20set%20and%20compute%20platform.%20the%0Afeasibility%2C%20usability%2C%20and%20commercial%20potential%20are%20demonstrated%20in%20this%0Aarticle.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00515v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGAD-Generative%2520Learning%2520for%2520HD%2520Map-Free%2520Autonomous%2520Driving%26entry.906535625%3DWeijian%2520Sun%2520and%2520Yanbo%2520Jia%2520and%2520Qi%2520Zeng%2520and%2520Zihao%2520Liu%2520and%2520Jiang%2520Liao%2520and%2520Yue%2520Li%2520and%2520Xianfeng%2520Li%26entry.1292438233%3D%2520%2520Deep-learning-based%2520techniques%2520have%2520been%2520widely%2520adopted%2520for%2520autonomous%250Adriving%2520software%2520stacks%2520for%2520mass%2520production%2520in%2520recent%2520years%252C%2520focusing%2520primarily%250Aon%2520perception%2520modules%252C%2520with%2520some%2520work%2520extending%2520this%2520method%2520to%2520prediction%250Amodules.%2520However%252C%2520the%2520downstream%2520planning%2520and%2520control%2520modules%2520are%2520still%250Adesigned%2520with%2520hefty%2520handcrafted%2520rules%252C%2520dominated%2520by%2520optimization-based%2520methods%250Asuch%2520as%2520quadratic%2520programming%2520or%2520model%2520predictive%2520control.%2520This%2520results%2520in%2520a%250Aperformance%2520bottleneck%2520for%2520autonomous%2520driving%2520systems%2520in%2520that%2520corner%2520cases%250Asimply%2520cannot%2520be%2520solved%2520by%2520enumerating%2520hand-crafted%2520rules.%2520We%2520present%2520a%250Adeep-learning-based%2520approach%2520that%2520brings%2520prediction%252C%2520decision%252C%2520and%2520planning%250Amodules%2520together%2520with%2520the%2520attempt%2520to%2520overcome%2520the%2520rule-based%2520methods%2527%250Adeficiency%2520in%2520real-world%2520applications%2520of%2520autonomous%2520driving%252C%2520especially%2520for%250Aurban%2520scenes.%2520The%2520DNN%2520model%2520we%2520proposed%2520is%2520solely%2520trained%2520with%252010%2520hours%2520of%250Ahuman%2520driver%2520data%252C%2520and%2520it%2520supports%2520all%2520mass-production%2520ADAS%2520features%2520available%250Aon%2520the%2520market%2520to%2520date.%2520This%2520method%2520is%2520deployed%2520onto%2520a%2520Jiyue%2520test%2520car%2520with%2520no%250Amodification%2520to%2520its%2520factory-ready%2520sensor%2520set%2520and%2520compute%2520platform.%2520the%250Afeasibility%252C%2520usability%252C%2520and%2520commercial%2520potential%2520are%2520demonstrated%2520in%2520this%250Aarticle.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00515v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GAD-Generative%20Learning%20for%20HD%20Map-Free%20Autonomous%20Driving&entry.906535625=Weijian%20Sun%20and%20Yanbo%20Jia%20and%20Qi%20Zeng%20and%20Zihao%20Liu%20and%20Jiang%20Liao%20and%20Yue%20Li%20and%20Xianfeng%20Li&entry.1292438233=%20%20Deep-learning-based%20techniques%20have%20been%20widely%20adopted%20for%20autonomous%0Adriving%20software%20stacks%20for%20mass%20production%20in%20recent%20years%2C%20focusing%20primarily%0Aon%20perception%20modules%2C%20with%20some%20work%20extending%20this%20method%20to%20prediction%0Amodules.%20However%2C%20the%20downstream%20planning%20and%20control%20modules%20are%20still%0Adesigned%20with%20hefty%20handcrafted%20rules%2C%20dominated%20by%20optimization-based%20methods%0Asuch%20as%20quadratic%20programming%20or%20model%20predictive%20control.%20This%20results%20in%20a%0Aperformance%20bottleneck%20for%20autonomous%20driving%20systems%20in%20that%20corner%20cases%0Asimply%20cannot%20be%20solved%20by%20enumerating%20hand-crafted%20rules.%20We%20present%20a%0Adeep-learning-based%20approach%20that%20brings%20prediction%2C%20decision%2C%20and%20planning%0Amodules%20together%20with%20the%20attempt%20to%20overcome%20the%20rule-based%20methods%27%0Adeficiency%20in%20real-world%20applications%20of%20autonomous%20driving%2C%20especially%20for%0Aurban%20scenes.%20The%20DNN%20model%20we%20proposed%20is%20solely%20trained%20with%2010%20hours%20of%0Ahuman%20driver%20data%2C%20and%20it%20supports%20all%20mass-production%20ADAS%20features%20available%0Aon%20the%20market%20to%20date.%20This%20method%20is%20deployed%20onto%20a%20Jiyue%20test%20car%20with%20no%0Amodification%20to%20its%20factory-ready%20sensor%20set%20and%20compute%20platform.%20the%0Afeasibility%2C%20usability%2C%20and%20commercial%20potential%20are%20demonstrated%20in%20this%0Aarticle.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00515v3&entry.124074799=Read"},
{"title": "DeCo: Decoupling Token Compression from Semantic Abstraction in\n  Multimodal Large Language Models", "author": "Linli Yao and Lei Li and Shuhuai Ren and Lean Wang and Yuanxin Liu and Xu Sun and Lu Hou", "abstract": "  The visual projector, which bridges the vision and language modalities and\nfacilitates cross-modal alignment, serves as a crucial component in MLLMs.\nHowever, measuring the effectiveness of projectors in vision-language alignment\nremains under-explored, which currently can only be inferred from the\nperformance of MLLMs on downstream tasks. Motivated by the problem, this study\nexamines the projector module by interpreting the vision-language semantic flow\nwithin MLLMs. Specifically, we trace back the semantic relevance flow from\ngenerated language tokens to raw visual encoder patches and the intermediate\noutputs produced by projectors. Our findings reveal that compressive projectors\n(e.g., QFormer), abstract visual patches into a limited set of semantic\nconcepts, such as objects or attributes, resulting in a 'double abstraction'\nphenomenon. This involves a first visual semantic abstraction by the projector\nreferring to pre-defined query tokens, and a second extraction by the LLM based\non text instructions. The double abstraction is inefficient in training and\nwill result in cumulative vision semantics deficiency. To mitigate this issue,\nwe propose the key insight of 'Decouple Compression from Abstraction (DeCo),\nthat is compressing the visual token number at the patch level by projectors\nand allowing the LLM to handle visual semantic abstraction entirely.\nConsequently, we adopt a simple compressor, i.e., 2D Adaptive Pooling, to\ndownsample visual patches in a parameter-free manner. Empirical evaluation\ndemonstrates that DeCo surpasses traditional compressive projectors regarding\nboth performance and efficiency. It achieves performance gains of 0.9%, 7.1%,\nand 2.9% across the MLLM Benchmarks, Visual Localization, and Open-ended VQA\ntasks with fewer trainable parameters and faster convergence speed.\n", "link": "http://arxiv.org/abs/2405.20985v1", "date": "2024-05-31", "relevancy": 2.1564, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5645}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5309}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCo%3A%20Decoupling%20Token%20Compression%20from%20Semantic%20Abstraction%20in%0A%20%20Multimodal%20Large%20Language%20Models&body=Title%3A%20DeCo%3A%20Decoupling%20Token%20Compression%20from%20Semantic%20Abstraction%20in%0A%20%20Multimodal%20Large%20Language%20Models%0AAuthor%3A%20Linli%20Yao%20and%20Lei%20Li%20and%20Shuhuai%20Ren%20and%20Lean%20Wang%20and%20Yuanxin%20Liu%20and%20Xu%20Sun%20and%20Lu%20Hou%0AAbstract%3A%20%20%20The%20visual%20projector%2C%20which%20bridges%20the%20vision%20and%20language%20modalities%20and%0Afacilitates%20cross-modal%20alignment%2C%20serves%20as%20a%20crucial%20component%20in%20MLLMs.%0AHowever%2C%20measuring%20the%20effectiveness%20of%20projectors%20in%20vision-language%20alignment%0Aremains%20under-explored%2C%20which%20currently%20can%20only%20be%20inferred%20from%20the%0Aperformance%20of%20MLLMs%20on%20downstream%20tasks.%20Motivated%20by%20the%20problem%2C%20this%20study%0Aexamines%20the%20projector%20module%20by%20interpreting%20the%20vision-language%20semantic%20flow%0Awithin%20MLLMs.%20Specifically%2C%20we%20trace%20back%20the%20semantic%20relevance%20flow%20from%0Agenerated%20language%20tokens%20to%20raw%20visual%20encoder%20patches%20and%20the%20intermediate%0Aoutputs%20produced%20by%20projectors.%20Our%20findings%20reveal%20that%20compressive%20projectors%0A%28e.g.%2C%20QFormer%29%2C%20abstract%20visual%20patches%20into%20a%20limited%20set%20of%20semantic%0Aconcepts%2C%20such%20as%20objects%20or%20attributes%2C%20resulting%20in%20a%20%27double%20abstraction%27%0Aphenomenon.%20This%20involves%20a%20first%20visual%20semantic%20abstraction%20by%20the%20projector%0Areferring%20to%20pre-defined%20query%20tokens%2C%20and%20a%20second%20extraction%20by%20the%20LLM%20based%0Aon%20text%20instructions.%20The%20double%20abstraction%20is%20inefficient%20in%20training%20and%0Awill%20result%20in%20cumulative%20vision%20semantics%20deficiency.%20To%20mitigate%20this%20issue%2C%0Awe%20propose%20the%20key%20insight%20of%20%27Decouple%20Compression%20from%20Abstraction%20%28DeCo%29%2C%0Athat%20is%20compressing%20the%20visual%20token%20number%20at%20the%20patch%20level%20by%20projectors%0Aand%20allowing%20the%20LLM%20to%20handle%20visual%20semantic%20abstraction%20entirely.%0AConsequently%2C%20we%20adopt%20a%20simple%20compressor%2C%20i.e.%2C%202D%20Adaptive%20Pooling%2C%20to%0Adownsample%20visual%20patches%20in%20a%20parameter-free%20manner.%20Empirical%20evaluation%0Ademonstrates%20that%20DeCo%20surpasses%20traditional%20compressive%20projectors%20regarding%0Aboth%20performance%20and%20efficiency.%20It%20achieves%20performance%20gains%20of%200.9%25%2C%207.1%25%2C%0Aand%202.9%25%20across%20the%20MLLM%20Benchmarks%2C%20Visual%20Localization%2C%20and%20Open-ended%20VQA%0Atasks%20with%20fewer%20trainable%20parameters%20and%20faster%20convergence%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20985v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCo%253A%2520Decoupling%2520Token%2520Compression%2520from%2520Semantic%2520Abstraction%2520in%250A%2520%2520Multimodal%2520Large%2520Language%2520Models%26entry.906535625%3DLinli%2520Yao%2520and%2520Lei%2520Li%2520and%2520Shuhuai%2520Ren%2520and%2520Lean%2520Wang%2520and%2520Yuanxin%2520Liu%2520and%2520Xu%2520Sun%2520and%2520Lu%2520Hou%26entry.1292438233%3D%2520%2520The%2520visual%2520projector%252C%2520which%2520bridges%2520the%2520vision%2520and%2520language%2520modalities%2520and%250Afacilitates%2520cross-modal%2520alignment%252C%2520serves%2520as%2520a%2520crucial%2520component%2520in%2520MLLMs.%250AHowever%252C%2520measuring%2520the%2520effectiveness%2520of%2520projectors%2520in%2520vision-language%2520alignment%250Aremains%2520under-explored%252C%2520which%2520currently%2520can%2520only%2520be%2520inferred%2520from%2520the%250Aperformance%2520of%2520MLLMs%2520on%2520downstream%2520tasks.%2520Motivated%2520by%2520the%2520problem%252C%2520this%2520study%250Aexamines%2520the%2520projector%2520module%2520by%2520interpreting%2520the%2520vision-language%2520semantic%2520flow%250Awithin%2520MLLMs.%2520Specifically%252C%2520we%2520trace%2520back%2520the%2520semantic%2520relevance%2520flow%2520from%250Agenerated%2520language%2520tokens%2520to%2520raw%2520visual%2520encoder%2520patches%2520and%2520the%2520intermediate%250Aoutputs%2520produced%2520by%2520projectors.%2520Our%2520findings%2520reveal%2520that%2520compressive%2520projectors%250A%2528e.g.%252C%2520QFormer%2529%252C%2520abstract%2520visual%2520patches%2520into%2520a%2520limited%2520set%2520of%2520semantic%250Aconcepts%252C%2520such%2520as%2520objects%2520or%2520attributes%252C%2520resulting%2520in%2520a%2520%2527double%2520abstraction%2527%250Aphenomenon.%2520This%2520involves%2520a%2520first%2520visual%2520semantic%2520abstraction%2520by%2520the%2520projector%250Areferring%2520to%2520pre-defined%2520query%2520tokens%252C%2520and%2520a%2520second%2520extraction%2520by%2520the%2520LLM%2520based%250Aon%2520text%2520instructions.%2520The%2520double%2520abstraction%2520is%2520inefficient%2520in%2520training%2520and%250Awill%2520result%2520in%2520cumulative%2520vision%2520semantics%2520deficiency.%2520To%2520mitigate%2520this%2520issue%252C%250Awe%2520propose%2520the%2520key%2520insight%2520of%2520%2527Decouple%2520Compression%2520from%2520Abstraction%2520%2528DeCo%2529%252C%250Athat%2520is%2520compressing%2520the%2520visual%2520token%2520number%2520at%2520the%2520patch%2520level%2520by%2520projectors%250Aand%2520allowing%2520the%2520LLM%2520to%2520handle%2520visual%2520semantic%2520abstraction%2520entirely.%250AConsequently%252C%2520we%2520adopt%2520a%2520simple%2520compressor%252C%2520i.e.%252C%25202D%2520Adaptive%2520Pooling%252C%2520to%250Adownsample%2520visual%2520patches%2520in%2520a%2520parameter-free%2520manner.%2520Empirical%2520evaluation%250Ademonstrates%2520that%2520DeCo%2520surpasses%2520traditional%2520compressive%2520projectors%2520regarding%250Aboth%2520performance%2520and%2520efficiency.%2520It%2520achieves%2520performance%2520gains%2520of%25200.9%2525%252C%25207.1%2525%252C%250Aand%25202.9%2525%2520across%2520the%2520MLLM%2520Benchmarks%252C%2520Visual%2520Localization%252C%2520and%2520Open-ended%2520VQA%250Atasks%2520with%2520fewer%2520trainable%2520parameters%2520and%2520faster%2520convergence%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20985v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCo%3A%20Decoupling%20Token%20Compression%20from%20Semantic%20Abstraction%20in%0A%20%20Multimodal%20Large%20Language%20Models&entry.906535625=Linli%20Yao%20and%20Lei%20Li%20and%20Shuhuai%20Ren%20and%20Lean%20Wang%20and%20Yuanxin%20Liu%20and%20Xu%20Sun%20and%20Lu%20Hou&entry.1292438233=%20%20The%20visual%20projector%2C%20which%20bridges%20the%20vision%20and%20language%20modalities%20and%0Afacilitates%20cross-modal%20alignment%2C%20serves%20as%20a%20crucial%20component%20in%20MLLMs.%0AHowever%2C%20measuring%20the%20effectiveness%20of%20projectors%20in%20vision-language%20alignment%0Aremains%20under-explored%2C%20which%20currently%20can%20only%20be%20inferred%20from%20the%0Aperformance%20of%20MLLMs%20on%20downstream%20tasks.%20Motivated%20by%20the%20problem%2C%20this%20study%0Aexamines%20the%20projector%20module%20by%20interpreting%20the%20vision-language%20semantic%20flow%0Awithin%20MLLMs.%20Specifically%2C%20we%20trace%20back%20the%20semantic%20relevance%20flow%20from%0Agenerated%20language%20tokens%20to%20raw%20visual%20encoder%20patches%20and%20the%20intermediate%0Aoutputs%20produced%20by%20projectors.%20Our%20findings%20reveal%20that%20compressive%20projectors%0A%28e.g.%2C%20QFormer%29%2C%20abstract%20visual%20patches%20into%20a%20limited%20set%20of%20semantic%0Aconcepts%2C%20such%20as%20objects%20or%20attributes%2C%20resulting%20in%20a%20%27double%20abstraction%27%0Aphenomenon.%20This%20involves%20a%20first%20visual%20semantic%20abstraction%20by%20the%20projector%0Areferring%20to%20pre-defined%20query%20tokens%2C%20and%20a%20second%20extraction%20by%20the%20LLM%20based%0Aon%20text%20instructions.%20The%20double%20abstraction%20is%20inefficient%20in%20training%20and%0Awill%20result%20in%20cumulative%20vision%20semantics%20deficiency.%20To%20mitigate%20this%20issue%2C%0Awe%20propose%20the%20key%20insight%20of%20%27Decouple%20Compression%20from%20Abstraction%20%28DeCo%29%2C%0Athat%20is%20compressing%20the%20visual%20token%20number%20at%20the%20patch%20level%20by%20projectors%0Aand%20allowing%20the%20LLM%20to%20handle%20visual%20semantic%20abstraction%20entirely.%0AConsequently%2C%20we%20adopt%20a%20simple%20compressor%2C%20i.e.%2C%202D%20Adaptive%20Pooling%2C%20to%0Adownsample%20visual%20patches%20in%20a%20parameter-free%20manner.%20Empirical%20evaluation%0Ademonstrates%20that%20DeCo%20surpasses%20traditional%20compressive%20projectors%20regarding%0Aboth%20performance%20and%20efficiency.%20It%20achieves%20performance%20gains%20of%200.9%25%2C%207.1%25%2C%0Aand%202.9%25%20across%20the%20MLLM%20Benchmarks%2C%20Visual%20Localization%2C%20and%20Open-ended%20VQA%0Atasks%20with%20fewer%20trainable%20parameters%20and%20faster%20convergence%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20985v1&entry.124074799=Read"},
{"title": "CARTE: Pretraining and Transfer for Tabular Learning", "author": "Myung Jun Kim and L\u00e9o Grinsztajn and Ga\u00ebl Varoquaux", "abstract": "  Pretrained deep-learning models are the go-to solution for images or text.\nHowever, for tabular data the standard is still to train tree-based models.\nIndeed, transfer learning on tables hits the challenge of data integration:\nfinding correspondences, correspondences in the entries (entity matching) where\ndifferent words may denote the same entity, correspondences across columns\n(schema matching), which may come in different orders, names... We propose a\nneural architecture that does not need such correspondences. As a result, we\ncan pretrain it on background data that has not been matched. The architecture\n-- CARTE for Context Aware Representation of Table Entries -- uses a graph\nrepresentation of tabular (or relational) data to process tables with different\ncolumns, string embedding of entries and columns names to model an open\nvocabulary, and a graph-attentional network to contextualize entries with\ncolumn names and neighboring entries. An extensive benchmark shows that CARTE\nfacilitates learning, outperforming a solid set of baselines including the best\ntree-based models. CARTE also enables joint learning across tables with\nunmatched columns, enhancing a small table with bigger ones. CARTE opens the\ndoor to large pretrained models for tabular data.\n", "link": "http://arxiv.org/abs/2402.16785v2", "date": "2024-05-31", "relevancy": 2.1458, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5551}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5385}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARTE%3A%20Pretraining%20and%20Transfer%20for%20Tabular%20Learning&body=Title%3A%20CARTE%3A%20Pretraining%20and%20Transfer%20for%20Tabular%20Learning%0AAuthor%3A%20Myung%20Jun%20Kim%20and%20L%C3%A9o%20Grinsztajn%20and%20Ga%C3%ABl%20Varoquaux%0AAbstract%3A%20%20%20Pretrained%20deep-learning%20models%20are%20the%20go-to%20solution%20for%20images%20or%20text.%0AHowever%2C%20for%20tabular%20data%20the%20standard%20is%20still%20to%20train%20tree-based%20models.%0AIndeed%2C%20transfer%20learning%20on%20tables%20hits%20the%20challenge%20of%20data%20integration%3A%0Afinding%20correspondences%2C%20correspondences%20in%20the%20entries%20%28entity%20matching%29%20where%0Adifferent%20words%20may%20denote%20the%20same%20entity%2C%20correspondences%20across%20columns%0A%28schema%20matching%29%2C%20which%20may%20come%20in%20different%20orders%2C%20names...%20We%20propose%20a%0Aneural%20architecture%20that%20does%20not%20need%20such%20correspondences.%20As%20a%20result%2C%20we%0Acan%20pretrain%20it%20on%20background%20data%20that%20has%20not%20been%20matched.%20The%20architecture%0A--%20CARTE%20for%20Context%20Aware%20Representation%20of%20Table%20Entries%20--%20uses%20a%20graph%0Arepresentation%20of%20tabular%20%28or%20relational%29%20data%20to%20process%20tables%20with%20different%0Acolumns%2C%20string%20embedding%20of%20entries%20and%20columns%20names%20to%20model%20an%20open%0Avocabulary%2C%20and%20a%20graph-attentional%20network%20to%20contextualize%20entries%20with%0Acolumn%20names%20and%20neighboring%20entries.%20An%20extensive%20benchmark%20shows%20that%20CARTE%0Afacilitates%20learning%2C%20outperforming%20a%20solid%20set%20of%20baselines%20including%20the%20best%0Atree-based%20models.%20CARTE%20also%20enables%20joint%20learning%20across%20tables%20with%0Aunmatched%20columns%2C%20enhancing%20a%20small%20table%20with%20bigger%20ones.%20CARTE%20opens%20the%0Adoor%20to%20large%20pretrained%20models%20for%20tabular%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARTE%253A%2520Pretraining%2520and%2520Transfer%2520for%2520Tabular%2520Learning%26entry.906535625%3DMyung%2520Jun%2520Kim%2520and%2520L%25C3%25A9o%2520Grinsztajn%2520and%2520Ga%25C3%25ABl%2520Varoquaux%26entry.1292438233%3D%2520%2520Pretrained%2520deep-learning%2520models%2520are%2520the%2520go-to%2520solution%2520for%2520images%2520or%2520text.%250AHowever%252C%2520for%2520tabular%2520data%2520the%2520standard%2520is%2520still%2520to%2520train%2520tree-based%2520models.%250AIndeed%252C%2520transfer%2520learning%2520on%2520tables%2520hits%2520the%2520challenge%2520of%2520data%2520integration%253A%250Afinding%2520correspondences%252C%2520correspondences%2520in%2520the%2520entries%2520%2528entity%2520matching%2529%2520where%250Adifferent%2520words%2520may%2520denote%2520the%2520same%2520entity%252C%2520correspondences%2520across%2520columns%250A%2528schema%2520matching%2529%252C%2520which%2520may%2520come%2520in%2520different%2520orders%252C%2520names...%2520We%2520propose%2520a%250Aneural%2520architecture%2520that%2520does%2520not%2520need%2520such%2520correspondences.%2520As%2520a%2520result%252C%2520we%250Acan%2520pretrain%2520it%2520on%2520background%2520data%2520that%2520has%2520not%2520been%2520matched.%2520The%2520architecture%250A--%2520CARTE%2520for%2520Context%2520Aware%2520Representation%2520of%2520Table%2520Entries%2520--%2520uses%2520a%2520graph%250Arepresentation%2520of%2520tabular%2520%2528or%2520relational%2529%2520data%2520to%2520process%2520tables%2520with%2520different%250Acolumns%252C%2520string%2520embedding%2520of%2520entries%2520and%2520columns%2520names%2520to%2520model%2520an%2520open%250Avocabulary%252C%2520and%2520a%2520graph-attentional%2520network%2520to%2520contextualize%2520entries%2520with%250Acolumn%2520names%2520and%2520neighboring%2520entries.%2520An%2520extensive%2520benchmark%2520shows%2520that%2520CARTE%250Afacilitates%2520learning%252C%2520outperforming%2520a%2520solid%2520set%2520of%2520baselines%2520including%2520the%2520best%250Atree-based%2520models.%2520CARTE%2520also%2520enables%2520joint%2520learning%2520across%2520tables%2520with%250Aunmatched%2520columns%252C%2520enhancing%2520a%2520small%2520table%2520with%2520bigger%2520ones.%2520CARTE%2520opens%2520the%250Adoor%2520to%2520large%2520pretrained%2520models%2520for%2520tabular%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARTE%3A%20Pretraining%20and%20Transfer%20for%20Tabular%20Learning&entry.906535625=Myung%20Jun%20Kim%20and%20L%C3%A9o%20Grinsztajn%20and%20Ga%C3%ABl%20Varoquaux&entry.1292438233=%20%20Pretrained%20deep-learning%20models%20are%20the%20go-to%20solution%20for%20images%20or%20text.%0AHowever%2C%20for%20tabular%20data%20the%20standard%20is%20still%20to%20train%20tree-based%20models.%0AIndeed%2C%20transfer%20learning%20on%20tables%20hits%20the%20challenge%20of%20data%20integration%3A%0Afinding%20correspondences%2C%20correspondences%20in%20the%20entries%20%28entity%20matching%29%20where%0Adifferent%20words%20may%20denote%20the%20same%20entity%2C%20correspondences%20across%20columns%0A%28schema%20matching%29%2C%20which%20may%20come%20in%20different%20orders%2C%20names...%20We%20propose%20a%0Aneural%20architecture%20that%20does%20not%20need%20such%20correspondences.%20As%20a%20result%2C%20we%0Acan%20pretrain%20it%20on%20background%20data%20that%20has%20not%20been%20matched.%20The%20architecture%0A--%20CARTE%20for%20Context%20Aware%20Representation%20of%20Table%20Entries%20--%20uses%20a%20graph%0Arepresentation%20of%20tabular%20%28or%20relational%29%20data%20to%20process%20tables%20with%20different%0Acolumns%2C%20string%20embedding%20of%20entries%20and%20columns%20names%20to%20model%20an%20open%0Avocabulary%2C%20and%20a%20graph-attentional%20network%20to%20contextualize%20entries%20with%0Acolumn%20names%20and%20neighboring%20entries.%20An%20extensive%20benchmark%20shows%20that%20CARTE%0Afacilitates%20learning%2C%20outperforming%20a%20solid%20set%20of%20baselines%20including%20the%20best%0Atree-based%20models.%20CARTE%20also%20enables%20joint%20learning%20across%20tables%20with%0Aunmatched%20columns%2C%20enhancing%20a%20small%20table%20with%20bigger%20ones.%20CARTE%20opens%20the%0Adoor%20to%20large%20pretrained%20models%20for%20tabular%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16785v2&entry.124074799=Read"},
{"title": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "author": "Shiyin Lu and Yang Li and Qing-Guo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang and Han-Jia Ye", "abstract": "  Current Multimodal Large Language Models (MLLMs) typically integrate a\npre-trained LLM with another pre-trained vision transformer through a\nconnector, such as an MLP, endowing the LLM with visual capabilities. However,\nthe misalignment between two embedding strategies in MLLMs -- the structural\ntextual embeddings based on an embedding look-up table and the continuous\nembeddings generated directly by the vision encoder -- makes challenges for a\nmore seamless fusion of visual and textual information. We propose Ovis, a\nnovel MLLM architecture designed to structurally align visual and textual\nembeddings. Ovis integrates an additional learnable visual embedding table into\nthe visual encoder's process. To capture rich visual semantics, each image\npatch indexes the visual embedding table multiple times, resulting in a final\nvisual embedding that is a probabilistic combination of the indexed embeddings.\nThis structural approach mirrors the method used for generating textual\nembeddings. Empirical evaluations on various multimodal benchmarks demonstrate\nthat Ovis outperforms open-source MLLMs of similar parameter scales and even\nsurpasses the proprietary model Qwen-VL-Plus overall. These results highlight\nthe potential of Ovis' structured visual representation for advancing MLLM\narchitectural design and promoting more effective multimodal learning. Both the\nsource code and the training dataset of Ovis will be made publicly available.\n", "link": "http://arxiv.org/abs/2405.20797v1", "date": "2024-05-31", "relevancy": 2.1351, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5795}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5036}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ovis%3A%20Structural%20Embedding%20Alignment%20for%20Multimodal%20Large%20Language%20Model&body=Title%3A%20Ovis%3A%20Structural%20Embedding%20Alignment%20for%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Shiyin%20Lu%20and%20Yang%20Li%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Han-Jia%20Ye%0AAbstract%3A%20%20%20Current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20typically%20integrate%20a%0Apre-trained%20LLM%20with%20another%20pre-trained%20vision%20transformer%20through%20a%0Aconnector%2C%20such%20as%20an%20MLP%2C%20endowing%20the%20LLM%20with%20visual%20capabilities.%20However%2C%0Athe%20misalignment%20between%20two%20embedding%20strategies%20in%20MLLMs%20--%20the%20structural%0Atextual%20embeddings%20based%20on%20an%20embedding%20look-up%20table%20and%20the%20continuous%0Aembeddings%20generated%20directly%20by%20the%20vision%20encoder%20--%20makes%20challenges%20for%20a%0Amore%20seamless%20fusion%20of%20visual%20and%20textual%20information.%20We%20propose%20Ovis%2C%20a%0Anovel%20MLLM%20architecture%20designed%20to%20structurally%20align%20visual%20and%20textual%0Aembeddings.%20Ovis%20integrates%20an%20additional%20learnable%20visual%20embedding%20table%20into%0Athe%20visual%20encoder%27s%20process.%20To%20capture%20rich%20visual%20semantics%2C%20each%20image%0Apatch%20indexes%20the%20visual%20embedding%20table%20multiple%20times%2C%20resulting%20in%20a%20final%0Avisual%20embedding%20that%20is%20a%20probabilistic%20combination%20of%20the%20indexed%20embeddings.%0AThis%20structural%20approach%20mirrors%20the%20method%20used%20for%20generating%20textual%0Aembeddings.%20Empirical%20evaluations%20on%20various%20multimodal%20benchmarks%20demonstrate%0Athat%20Ovis%20outperforms%20open-source%20MLLMs%20of%20similar%20parameter%20scales%20and%20even%0Asurpasses%20the%20proprietary%20model%20Qwen-VL-Plus%20overall.%20These%20results%20highlight%0Athe%20potential%20of%20Ovis%27%20structured%20visual%20representation%20for%20advancing%20MLLM%0Aarchitectural%20design%20and%20promoting%20more%20effective%20multimodal%20learning.%20Both%20the%0Asource%20code%20and%20the%20training%20dataset%20of%20Ovis%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvis%253A%2520Structural%2520Embedding%2520Alignment%2520for%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DShiyin%2520Lu%2520and%2520Yang%2520Li%2520and%2520Qing-Guo%2520Chen%2520and%2520Zhao%2520Xu%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%2520and%2520Han-Jia%2520Ye%26entry.1292438233%3D%2520%2520Current%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520typically%2520integrate%2520a%250Apre-trained%2520LLM%2520with%2520another%2520pre-trained%2520vision%2520transformer%2520through%2520a%250Aconnector%252C%2520such%2520as%2520an%2520MLP%252C%2520endowing%2520the%2520LLM%2520with%2520visual%2520capabilities.%2520However%252C%250Athe%2520misalignment%2520between%2520two%2520embedding%2520strategies%2520in%2520MLLMs%2520--%2520the%2520structural%250Atextual%2520embeddings%2520based%2520on%2520an%2520embedding%2520look-up%2520table%2520and%2520the%2520continuous%250Aembeddings%2520generated%2520directly%2520by%2520the%2520vision%2520encoder%2520--%2520makes%2520challenges%2520for%2520a%250Amore%2520seamless%2520fusion%2520of%2520visual%2520and%2520textual%2520information.%2520We%2520propose%2520Ovis%252C%2520a%250Anovel%2520MLLM%2520architecture%2520designed%2520to%2520structurally%2520align%2520visual%2520and%2520textual%250Aembeddings.%2520Ovis%2520integrates%2520an%2520additional%2520learnable%2520visual%2520embedding%2520table%2520into%250Athe%2520visual%2520encoder%2527s%2520process.%2520To%2520capture%2520rich%2520visual%2520semantics%252C%2520each%2520image%250Apatch%2520indexes%2520the%2520visual%2520embedding%2520table%2520multiple%2520times%252C%2520resulting%2520in%2520a%2520final%250Avisual%2520embedding%2520that%2520is%2520a%2520probabilistic%2520combination%2520of%2520the%2520indexed%2520embeddings.%250AThis%2520structural%2520approach%2520mirrors%2520the%2520method%2520used%2520for%2520generating%2520textual%250Aembeddings.%2520Empirical%2520evaluations%2520on%2520various%2520multimodal%2520benchmarks%2520demonstrate%250Athat%2520Ovis%2520outperforms%2520open-source%2520MLLMs%2520of%2520similar%2520parameter%2520scales%2520and%2520even%250Asurpasses%2520the%2520proprietary%2520model%2520Qwen-VL-Plus%2520overall.%2520These%2520results%2520highlight%250Athe%2520potential%2520of%2520Ovis%2527%2520structured%2520visual%2520representation%2520for%2520advancing%2520MLLM%250Aarchitectural%2520design%2520and%2520promoting%2520more%2520effective%2520multimodal%2520learning.%2520Both%2520the%250Asource%2520code%2520and%2520the%2520training%2520dataset%2520of%2520Ovis%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ovis%3A%20Structural%20Embedding%20Alignment%20for%20Multimodal%20Large%20Language%20Model&entry.906535625=Shiyin%20Lu%20and%20Yang%20Li%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Han-Jia%20Ye&entry.1292438233=%20%20Current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20typically%20integrate%20a%0Apre-trained%20LLM%20with%20another%20pre-trained%20vision%20transformer%20through%20a%0Aconnector%2C%20such%20as%20an%20MLP%2C%20endowing%20the%20LLM%20with%20visual%20capabilities.%20However%2C%0Athe%20misalignment%20between%20two%20embedding%20strategies%20in%20MLLMs%20--%20the%20structural%0Atextual%20embeddings%20based%20on%20an%20embedding%20look-up%20table%20and%20the%20continuous%0Aembeddings%20generated%20directly%20by%20the%20vision%20encoder%20--%20makes%20challenges%20for%20a%0Amore%20seamless%20fusion%20of%20visual%20and%20textual%20information.%20We%20propose%20Ovis%2C%20a%0Anovel%20MLLM%20architecture%20designed%20to%20structurally%20align%20visual%20and%20textual%0Aembeddings.%20Ovis%20integrates%20an%20additional%20learnable%20visual%20embedding%20table%20into%0Athe%20visual%20encoder%27s%20process.%20To%20capture%20rich%20visual%20semantics%2C%20each%20image%0Apatch%20indexes%20the%20visual%20embedding%20table%20multiple%20times%2C%20resulting%20in%20a%20final%0Avisual%20embedding%20that%20is%20a%20probabilistic%20combination%20of%20the%20indexed%20embeddings.%0AThis%20structural%20approach%20mirrors%20the%20method%20used%20for%20generating%20textual%0Aembeddings.%20Empirical%20evaluations%20on%20various%20multimodal%20benchmarks%20demonstrate%0Athat%20Ovis%20outperforms%20open-source%20MLLMs%20of%20similar%20parameter%20scales%20and%20even%0Asurpasses%20the%20proprietary%20model%20Qwen-VL-Plus%20overall.%20These%20results%20highlight%0Athe%20potential%20of%20Ovis%27%20structured%20visual%20representation%20for%20advancing%20MLLM%0Aarchitectural%20design%20and%20promoting%20more%20effective%20multimodal%20learning.%20Both%20the%0Asource%20code%20and%20the%20training%20dataset%20of%20Ovis%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20797v1&entry.124074799=Read"},
{"title": "Beyond Conventional Parametric Modeling: Data-Driven Framework for\n  Estimation and Prediction of Time Activity Curves in Dynamic PET Imaging", "author": "Niloufar Zakariaei and Arman Rahmim and Eldad Haber", "abstract": "  Dynamic Positron Emission Tomography (dPET) imaging and Time-Activity Curve\n(TAC) analyses are essential for understanding and quantifying the\nbiodistribution of radiopharmaceuticals over time and space. Traditional\ncompartmental modeling, while foundational, commonly struggles to fully capture\nthe complexities of biological systems, including non-linear dynamics and\nvariability. This study introduces an innovative data-driven neural\nnetwork-based framework, inspired by Reaction Diffusion systems, designed to\naddress these limitations. Our approach, which adaptively fits TACs from dPET,\nenables the direct calibration of diffusion coefficients and reaction terms\nfrom observed data, offering significant improvements in predictive accuracy\nand robustness over traditional methods, especially in complex biological\nscenarios. By more accurately modeling the spatio-temporal dynamics of\nradiopharmaceuticals, our method advances modeling of pharmacokinetic and\npharmacodynamic processes, enabling new possibilities in quantitative nuclear\nmedicine.\n", "link": "http://arxiv.org/abs/2405.21021v1", "date": "2024-05-31", "relevancy": 2.1303, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5338}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5338}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Conventional%20Parametric%20Modeling%3A%20Data-Driven%20Framework%20for%0A%20%20Estimation%20and%20Prediction%20of%20Time%20Activity%20Curves%20in%20Dynamic%20PET%20Imaging&body=Title%3A%20Beyond%20Conventional%20Parametric%20Modeling%3A%20Data-Driven%20Framework%20for%0A%20%20Estimation%20and%20Prediction%20of%20Time%20Activity%20Curves%20in%20Dynamic%20PET%20Imaging%0AAuthor%3A%20Niloufar%20Zakariaei%20and%20Arman%20Rahmim%20and%20Eldad%20Haber%0AAbstract%3A%20%20%20Dynamic%20Positron%20Emission%20Tomography%20%28dPET%29%20imaging%20and%20Time-Activity%20Curve%0A%28TAC%29%20analyses%20are%20essential%20for%20understanding%20and%20quantifying%20the%0Abiodistribution%20of%20radiopharmaceuticals%20over%20time%20and%20space.%20Traditional%0Acompartmental%20modeling%2C%20while%20foundational%2C%20commonly%20struggles%20to%20fully%20capture%0Athe%20complexities%20of%20biological%20systems%2C%20including%20non-linear%20dynamics%20and%0Avariability.%20This%20study%20introduces%20an%20innovative%20data-driven%20neural%0Anetwork-based%20framework%2C%20inspired%20by%20Reaction%20Diffusion%20systems%2C%20designed%20to%0Aaddress%20these%20limitations.%20Our%20approach%2C%20which%20adaptively%20fits%20TACs%20from%20dPET%2C%0Aenables%20the%20direct%20calibration%20of%20diffusion%20coefficients%20and%20reaction%20terms%0Afrom%20observed%20data%2C%20offering%20significant%20improvements%20in%20predictive%20accuracy%0Aand%20robustness%20over%20traditional%20methods%2C%20especially%20in%20complex%20biological%0Ascenarios.%20By%20more%20accurately%20modeling%20the%20spatio-temporal%20dynamics%20of%0Aradiopharmaceuticals%2C%20our%20method%20advances%20modeling%20of%20pharmacokinetic%20and%0Apharmacodynamic%20processes%2C%20enabling%20new%20possibilities%20in%20quantitative%20nuclear%0Amedicine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Conventional%2520Parametric%2520Modeling%253A%2520Data-Driven%2520Framework%2520for%250A%2520%2520Estimation%2520and%2520Prediction%2520of%2520Time%2520Activity%2520Curves%2520in%2520Dynamic%2520PET%2520Imaging%26entry.906535625%3DNiloufar%2520Zakariaei%2520and%2520Arman%2520Rahmim%2520and%2520Eldad%2520Haber%26entry.1292438233%3D%2520%2520Dynamic%2520Positron%2520Emission%2520Tomography%2520%2528dPET%2529%2520imaging%2520and%2520Time-Activity%2520Curve%250A%2528TAC%2529%2520analyses%2520are%2520essential%2520for%2520understanding%2520and%2520quantifying%2520the%250Abiodistribution%2520of%2520radiopharmaceuticals%2520over%2520time%2520and%2520space.%2520Traditional%250Acompartmental%2520modeling%252C%2520while%2520foundational%252C%2520commonly%2520struggles%2520to%2520fully%2520capture%250Athe%2520complexities%2520of%2520biological%2520systems%252C%2520including%2520non-linear%2520dynamics%2520and%250Avariability.%2520This%2520study%2520introduces%2520an%2520innovative%2520data-driven%2520neural%250Anetwork-based%2520framework%252C%2520inspired%2520by%2520Reaction%2520Diffusion%2520systems%252C%2520designed%2520to%250Aaddress%2520these%2520limitations.%2520Our%2520approach%252C%2520which%2520adaptively%2520fits%2520TACs%2520from%2520dPET%252C%250Aenables%2520the%2520direct%2520calibration%2520of%2520diffusion%2520coefficients%2520and%2520reaction%2520terms%250Afrom%2520observed%2520data%252C%2520offering%2520significant%2520improvements%2520in%2520predictive%2520accuracy%250Aand%2520robustness%2520over%2520traditional%2520methods%252C%2520especially%2520in%2520complex%2520biological%250Ascenarios.%2520By%2520more%2520accurately%2520modeling%2520the%2520spatio-temporal%2520dynamics%2520of%250Aradiopharmaceuticals%252C%2520our%2520method%2520advances%2520modeling%2520of%2520pharmacokinetic%2520and%250Apharmacodynamic%2520processes%252C%2520enabling%2520new%2520possibilities%2520in%2520quantitative%2520nuclear%250Amedicine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Conventional%20Parametric%20Modeling%3A%20Data-Driven%20Framework%20for%0A%20%20Estimation%20and%20Prediction%20of%20Time%20Activity%20Curves%20in%20Dynamic%20PET%20Imaging&entry.906535625=Niloufar%20Zakariaei%20and%20Arman%20Rahmim%20and%20Eldad%20Haber&entry.1292438233=%20%20Dynamic%20Positron%20Emission%20Tomography%20%28dPET%29%20imaging%20and%20Time-Activity%20Curve%0A%28TAC%29%20analyses%20are%20essential%20for%20understanding%20and%20quantifying%20the%0Abiodistribution%20of%20radiopharmaceuticals%20over%20time%20and%20space.%20Traditional%0Acompartmental%20modeling%2C%20while%20foundational%2C%20commonly%20struggles%20to%20fully%20capture%0Athe%20complexities%20of%20biological%20systems%2C%20including%20non-linear%20dynamics%20and%0Avariability.%20This%20study%20introduces%20an%20innovative%20data-driven%20neural%0Anetwork-based%20framework%2C%20inspired%20by%20Reaction%20Diffusion%20systems%2C%20designed%20to%0Aaddress%20these%20limitations.%20Our%20approach%2C%20which%20adaptively%20fits%20TACs%20from%20dPET%2C%0Aenables%20the%20direct%20calibration%20of%20diffusion%20coefficients%20and%20reaction%20terms%0Afrom%20observed%20data%2C%20offering%20significant%20improvements%20in%20predictive%20accuracy%0Aand%20robustness%20over%20traditional%20methods%2C%20especially%20in%20complex%20biological%0Ascenarios.%20By%20more%20accurately%20modeling%20the%20spatio-temporal%20dynamics%20of%0Aradiopharmaceuticals%2C%20our%20method%20advances%20modeling%20of%20pharmacokinetic%20and%0Apharmacodynamic%20processes%2C%20enabling%20new%20possibilities%20in%20quantitative%20nuclear%0Amedicine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21021v1&entry.124074799=Read"},
{"title": "Permutation Decision Trees", "author": "Harikrishnan N B and Arham Jain and Nithin Nagaraj", "abstract": "  Decision Tree is a well understood Machine Learning model that is based on\nminimizing impurities in the internal nodes. The most common impurity measures\nare Shannon entropy and Gini impurity. These impurity measures are insensitive\nto the order of training data and hence the final tree obtained is invariant to\nany permutation of the data. This is a limitation in terms of modeling when\nthere are temporal order dependencies between data instances. In this research,\nwe propose the adoption of Effort-To-Compress (ETC) - a complexity measure, for\nthe first time, as an alternative impurity measure. Unlike Shannon entropy and\nGini impurity, structural impurity based on ETC is able to capture order\ndependencies in the data, thus obtaining potentially different decision trees\nfor different permutations of the same data instances, a concept we term as\nPermutation Decision Trees (PDT). We then introduce the notion of Permutation\nBagging achieved using permutation decision trees without the need for random\nfeature selection and sub-sampling. We conduct a performance comparison between\nPermutation Decision Trees and classical decision trees across various\nreal-world datasets, including Appendicitis, Breast Cancer Wisconsin, Diabetes\nPima Indian, Ionosphere, Iris, Sonar, and Wine. Our findings reveal that PDT\ndemonstrates comparable performance to classical decision trees across most\ndatasets. Remarkably, in certain instances, PDT even slightly surpasses the\nperformance of classical decision trees. In comparing Permutation Bagging with\nRandom Forest, we attain comparable performance to Random Forest models\nconsisting of 50 to 1000 trees, using merely 21 trees. This highlights the\nefficiency and effectiveness of Permutation Bagging in achieving comparable\nperformance outcomes with significantly fewer trees.\n", "link": "http://arxiv.org/abs/2306.02617v3", "date": "2024-05-31", "relevancy": 2.1263, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4302}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4242}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Permutation%20Decision%20Trees&body=Title%3A%20Permutation%20Decision%20Trees%0AAuthor%3A%20Harikrishnan%20N%20B%20and%20Arham%20Jain%20and%20Nithin%20Nagaraj%0AAbstract%3A%20%20%20Decision%20Tree%20is%20a%20well%20understood%20Machine%20Learning%20model%20that%20is%20based%20on%0Aminimizing%20impurities%20in%20the%20internal%20nodes.%20The%20most%20common%20impurity%20measures%0Aare%20Shannon%20entropy%20and%20Gini%20impurity.%20These%20impurity%20measures%20are%20insensitive%0Ato%20the%20order%20of%20training%20data%20and%20hence%20the%20final%20tree%20obtained%20is%20invariant%20to%0Aany%20permutation%20of%20the%20data.%20This%20is%20a%20limitation%20in%20terms%20of%20modeling%20when%0Athere%20are%20temporal%20order%20dependencies%20between%20data%20instances.%20In%20this%20research%2C%0Awe%20propose%20the%20adoption%20of%20Effort-To-Compress%20%28ETC%29%20-%20a%20complexity%20measure%2C%20for%0Athe%20first%20time%2C%20as%20an%20alternative%20impurity%20measure.%20Unlike%20Shannon%20entropy%20and%0AGini%20impurity%2C%20structural%20impurity%20based%20on%20ETC%20is%20able%20to%20capture%20order%0Adependencies%20in%20the%20data%2C%20thus%20obtaining%20potentially%20different%20decision%20trees%0Afor%20different%20permutations%20of%20the%20same%20data%20instances%2C%20a%20concept%20we%20term%20as%0APermutation%20Decision%20Trees%20%28PDT%29.%20We%20then%20introduce%20the%20notion%20of%20Permutation%0ABagging%20achieved%20using%20permutation%20decision%20trees%20without%20the%20need%20for%20random%0Afeature%20selection%20and%20sub-sampling.%20We%20conduct%20a%20performance%20comparison%20between%0APermutation%20Decision%20Trees%20and%20classical%20decision%20trees%20across%20various%0Areal-world%20datasets%2C%20including%20Appendicitis%2C%20Breast%20Cancer%20Wisconsin%2C%20Diabetes%0APima%20Indian%2C%20Ionosphere%2C%20Iris%2C%20Sonar%2C%20and%20Wine.%20Our%20findings%20reveal%20that%20PDT%0Ademonstrates%20comparable%20performance%20to%20classical%20decision%20trees%20across%20most%0Adatasets.%20Remarkably%2C%20in%20certain%20instances%2C%20PDT%20even%20slightly%20surpasses%20the%0Aperformance%20of%20classical%20decision%20trees.%20In%20comparing%20Permutation%20Bagging%20with%0ARandom%20Forest%2C%20we%20attain%20comparable%20performance%20to%20Random%20Forest%20models%0Aconsisting%20of%2050%20to%201000%20trees%2C%20using%20merely%2021%20trees.%20This%20highlights%20the%0Aefficiency%20and%20effectiveness%20of%20Permutation%20Bagging%20in%20achieving%20comparable%0Aperformance%20outcomes%20with%20significantly%20fewer%20trees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.02617v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPermutation%2520Decision%2520Trees%26entry.906535625%3DHarikrishnan%2520N%2520B%2520and%2520Arham%2520Jain%2520and%2520Nithin%2520Nagaraj%26entry.1292438233%3D%2520%2520Decision%2520Tree%2520is%2520a%2520well%2520understood%2520Machine%2520Learning%2520model%2520that%2520is%2520based%2520on%250Aminimizing%2520impurities%2520in%2520the%2520internal%2520nodes.%2520The%2520most%2520common%2520impurity%2520measures%250Aare%2520Shannon%2520entropy%2520and%2520Gini%2520impurity.%2520These%2520impurity%2520measures%2520are%2520insensitive%250Ato%2520the%2520order%2520of%2520training%2520data%2520and%2520hence%2520the%2520final%2520tree%2520obtained%2520is%2520invariant%2520to%250Aany%2520permutation%2520of%2520the%2520data.%2520This%2520is%2520a%2520limitation%2520in%2520terms%2520of%2520modeling%2520when%250Athere%2520are%2520temporal%2520order%2520dependencies%2520between%2520data%2520instances.%2520In%2520this%2520research%252C%250Awe%2520propose%2520the%2520adoption%2520of%2520Effort-To-Compress%2520%2528ETC%2529%2520-%2520a%2520complexity%2520measure%252C%2520for%250Athe%2520first%2520time%252C%2520as%2520an%2520alternative%2520impurity%2520measure.%2520Unlike%2520Shannon%2520entropy%2520and%250AGini%2520impurity%252C%2520structural%2520impurity%2520based%2520on%2520ETC%2520is%2520able%2520to%2520capture%2520order%250Adependencies%2520in%2520the%2520data%252C%2520thus%2520obtaining%2520potentially%2520different%2520decision%2520trees%250Afor%2520different%2520permutations%2520of%2520the%2520same%2520data%2520instances%252C%2520a%2520concept%2520we%2520term%2520as%250APermutation%2520Decision%2520Trees%2520%2528PDT%2529.%2520We%2520then%2520introduce%2520the%2520notion%2520of%2520Permutation%250ABagging%2520achieved%2520using%2520permutation%2520decision%2520trees%2520without%2520the%2520need%2520for%2520random%250Afeature%2520selection%2520and%2520sub-sampling.%2520We%2520conduct%2520a%2520performance%2520comparison%2520between%250APermutation%2520Decision%2520Trees%2520and%2520classical%2520decision%2520trees%2520across%2520various%250Areal-world%2520datasets%252C%2520including%2520Appendicitis%252C%2520Breast%2520Cancer%2520Wisconsin%252C%2520Diabetes%250APima%2520Indian%252C%2520Ionosphere%252C%2520Iris%252C%2520Sonar%252C%2520and%2520Wine.%2520Our%2520findings%2520reveal%2520that%2520PDT%250Ademonstrates%2520comparable%2520performance%2520to%2520classical%2520decision%2520trees%2520across%2520most%250Adatasets.%2520Remarkably%252C%2520in%2520certain%2520instances%252C%2520PDT%2520even%2520slightly%2520surpasses%2520the%250Aperformance%2520of%2520classical%2520decision%2520trees.%2520In%2520comparing%2520Permutation%2520Bagging%2520with%250ARandom%2520Forest%252C%2520we%2520attain%2520comparable%2520performance%2520to%2520Random%2520Forest%2520models%250Aconsisting%2520of%252050%2520to%25201000%2520trees%252C%2520using%2520merely%252021%2520trees.%2520This%2520highlights%2520the%250Aefficiency%2520and%2520effectiveness%2520of%2520Permutation%2520Bagging%2520in%2520achieving%2520comparable%250Aperformance%2520outcomes%2520with%2520significantly%2520fewer%2520trees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.02617v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Permutation%20Decision%20Trees&entry.906535625=Harikrishnan%20N%20B%20and%20Arham%20Jain%20and%20Nithin%20Nagaraj&entry.1292438233=%20%20Decision%20Tree%20is%20a%20well%20understood%20Machine%20Learning%20model%20that%20is%20based%20on%0Aminimizing%20impurities%20in%20the%20internal%20nodes.%20The%20most%20common%20impurity%20measures%0Aare%20Shannon%20entropy%20and%20Gini%20impurity.%20These%20impurity%20measures%20are%20insensitive%0Ato%20the%20order%20of%20training%20data%20and%20hence%20the%20final%20tree%20obtained%20is%20invariant%20to%0Aany%20permutation%20of%20the%20data.%20This%20is%20a%20limitation%20in%20terms%20of%20modeling%20when%0Athere%20are%20temporal%20order%20dependencies%20between%20data%20instances.%20In%20this%20research%2C%0Awe%20propose%20the%20adoption%20of%20Effort-To-Compress%20%28ETC%29%20-%20a%20complexity%20measure%2C%20for%0Athe%20first%20time%2C%20as%20an%20alternative%20impurity%20measure.%20Unlike%20Shannon%20entropy%20and%0AGini%20impurity%2C%20structural%20impurity%20based%20on%20ETC%20is%20able%20to%20capture%20order%0Adependencies%20in%20the%20data%2C%20thus%20obtaining%20potentially%20different%20decision%20trees%0Afor%20different%20permutations%20of%20the%20same%20data%20instances%2C%20a%20concept%20we%20term%20as%0APermutation%20Decision%20Trees%20%28PDT%29.%20We%20then%20introduce%20the%20notion%20of%20Permutation%0ABagging%20achieved%20using%20permutation%20decision%20trees%20without%20the%20need%20for%20random%0Afeature%20selection%20and%20sub-sampling.%20We%20conduct%20a%20performance%20comparison%20between%0APermutation%20Decision%20Trees%20and%20classical%20decision%20trees%20across%20various%0Areal-world%20datasets%2C%20including%20Appendicitis%2C%20Breast%20Cancer%20Wisconsin%2C%20Diabetes%0APima%20Indian%2C%20Ionosphere%2C%20Iris%2C%20Sonar%2C%20and%20Wine.%20Our%20findings%20reveal%20that%20PDT%0Ademonstrates%20comparable%20performance%20to%20classical%20decision%20trees%20across%20most%0Adatasets.%20Remarkably%2C%20in%20certain%20instances%2C%20PDT%20even%20slightly%20surpasses%20the%0Aperformance%20of%20classical%20decision%20trees.%20In%20comparing%20Permutation%20Bagging%20with%0ARandom%20Forest%2C%20we%20attain%20comparable%20performance%20to%20Random%20Forest%20models%0Aconsisting%20of%2050%20to%201000%20trees%2C%20using%20merely%2021%20trees.%20This%20highlights%20the%0Aefficiency%20and%20effectiveness%20of%20Permutation%20Bagging%20in%20achieving%20comparable%0Aperformance%20outcomes%20with%20significantly%20fewer%20trees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.02617v3&entry.124074799=Read"},
{"title": "Calibrated Self-Rewarding Vision Language Models", "author": "Yiyang Zhou and Zhiyuan Fan and Dongjie Cheng and Sihan Yang and Zhaorun Chen and Chenhang Cui and Xiyao Wang and Yun Li and Linjun Zhang and Huaxiu Yao", "abstract": "  Large Vision-Language Models (LVLMs) have made substantial progress by\nintegrating pre-trained large language models (LLMs) and vision models through\ninstruction tuning. Despite these advancements, LVLMs often exhibit the\nhallucination phenomenon, where generated text responses appear linguistically\nplausible but contradict the input image, indicating a misalignment between\nimage and text pairs. This misalignment arises because the model tends to\nprioritize textual information over visual input, even when both the language\nmodel and visual representations are of high quality. Existing methods leverage\nadditional models or human annotations to curate preference data and enhance\nmodality alignment through preference optimization. These approaches may not\neffectively reflect the target LVLM's preferences, making the curated\npreferences easily distinguishable. Our work addresses these challenges by\nproposing the Calibrated Self-Rewarding (CSR) approach, which enables the model\nto self-improve by iteratively generating candidate responses, evaluating the\nreward for each response, and curating preference data for fine-tuning. In the\nreward modeling, we employ a step-wise strategy and incorporate visual\nconstraints into the self-rewarding process to place greater emphasis on visual\ninput. Empirical results demonstrate that CSR enhances performance and reduces\nhallucinations across ten benchmarks and tasks, achieving substantial\nimprovements over existing methods by 7.62%. Our empirical results are further\nsupported by rigorous theoretical analysis, under mild assumptions, verifying\nthe effectiveness of introducing visual constraints into the self-rewarding\nparadigm. Additionally, CSR shows compatibility with different vision-language\nmodels and the ability to incrementally improve performance through iterative\nfine-tuning. Our data and code are available at\nhttps://github.com/YiyangZhou/CSR.\n", "link": "http://arxiv.org/abs/2405.14622v3", "date": "2024-05-31", "relevancy": 2.1102, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5302}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5257}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Calibrated%20Self-Rewarding%20Vision%20Language%20Models&body=Title%3A%20Calibrated%20Self-Rewarding%20Vision%20Language%20Models%0AAuthor%3A%20Yiyang%20Zhou%20and%20Zhiyuan%20Fan%20and%20Dongjie%20Cheng%20and%20Sihan%20Yang%20and%20Zhaorun%20Chen%20and%20Chenhang%20Cui%20and%20Xiyao%20Wang%20and%20Yun%20Li%20and%20Linjun%20Zhang%20and%20Huaxiu%20Yao%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20substantial%20progress%20by%0Aintegrating%20pre-trained%20large%20language%20models%20%28LLMs%29%20and%20vision%20models%20through%0Ainstruction%20tuning.%20Despite%20these%20advancements%2C%20LVLMs%20often%20exhibit%20the%0Ahallucination%20phenomenon%2C%20where%20generated%20text%20responses%20appear%20linguistically%0Aplausible%20but%20contradict%20the%20input%20image%2C%20indicating%20a%20misalignment%20between%0Aimage%20and%20text%20pairs.%20This%20misalignment%20arises%20because%20the%20model%20tends%20to%0Aprioritize%20textual%20information%20over%20visual%20input%2C%20even%20when%20both%20the%20language%0Amodel%20and%20visual%20representations%20are%20of%20high%20quality.%20Existing%20methods%20leverage%0Aadditional%20models%20or%20human%20annotations%20to%20curate%20preference%20data%20and%20enhance%0Amodality%20alignment%20through%20preference%20optimization.%20These%20approaches%20may%20not%0Aeffectively%20reflect%20the%20target%20LVLM%27s%20preferences%2C%20making%20the%20curated%0Apreferences%20easily%20distinguishable.%20Our%20work%20addresses%20these%20challenges%20by%0Aproposing%20the%20Calibrated%20Self-Rewarding%20%28CSR%29%20approach%2C%20which%20enables%20the%20model%0Ato%20self-improve%20by%20iteratively%20generating%20candidate%20responses%2C%20evaluating%20the%0Areward%20for%20each%20response%2C%20and%20curating%20preference%20data%20for%20fine-tuning.%20In%20the%0Areward%20modeling%2C%20we%20employ%20a%20step-wise%20strategy%20and%20incorporate%20visual%0Aconstraints%20into%20the%20self-rewarding%20process%20to%20place%20greater%20emphasis%20on%20visual%0Ainput.%20Empirical%20results%20demonstrate%20that%20CSR%20enhances%20performance%20and%20reduces%0Ahallucinations%20across%20ten%20benchmarks%20and%20tasks%2C%20achieving%20substantial%0Aimprovements%20over%20existing%20methods%20by%207.62%25.%20Our%20empirical%20results%20are%20further%0Asupported%20by%20rigorous%20theoretical%20analysis%2C%20under%20mild%20assumptions%2C%20verifying%0Athe%20effectiveness%20of%20introducing%20visual%20constraints%20into%20the%20self-rewarding%0Aparadigm.%20Additionally%2C%20CSR%20shows%20compatibility%20with%20different%20vision-language%0Amodels%20and%20the%20ability%20to%20incrementally%20improve%20performance%20through%20iterative%0Afine-tuning.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/YiyangZhou/CSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14622v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCalibrated%2520Self-Rewarding%2520Vision%2520Language%2520Models%26entry.906535625%3DYiyang%2520Zhou%2520and%2520Zhiyuan%2520Fan%2520and%2520Dongjie%2520Cheng%2520and%2520Sihan%2520Yang%2520and%2520Zhaorun%2520Chen%2520and%2520Chenhang%2520Cui%2520and%2520Xiyao%2520Wang%2520and%2520Yun%2520Li%2520and%2520Linjun%2520Zhang%2520and%2520Huaxiu%2520Yao%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520made%2520substantial%2520progress%2520by%250Aintegrating%2520pre-trained%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520vision%2520models%2520through%250Ainstruction%2520tuning.%2520Despite%2520these%2520advancements%252C%2520LVLMs%2520often%2520exhibit%2520the%250Ahallucination%2520phenomenon%252C%2520where%2520generated%2520text%2520responses%2520appear%2520linguistically%250Aplausible%2520but%2520contradict%2520the%2520input%2520image%252C%2520indicating%2520a%2520misalignment%2520between%250Aimage%2520and%2520text%2520pairs.%2520This%2520misalignment%2520arises%2520because%2520the%2520model%2520tends%2520to%250Aprioritize%2520textual%2520information%2520over%2520visual%2520input%252C%2520even%2520when%2520both%2520the%2520language%250Amodel%2520and%2520visual%2520representations%2520are%2520of%2520high%2520quality.%2520Existing%2520methods%2520leverage%250Aadditional%2520models%2520or%2520human%2520annotations%2520to%2520curate%2520preference%2520data%2520and%2520enhance%250Amodality%2520alignment%2520through%2520preference%2520optimization.%2520These%2520approaches%2520may%2520not%250Aeffectively%2520reflect%2520the%2520target%2520LVLM%2527s%2520preferences%252C%2520making%2520the%2520curated%250Apreferences%2520easily%2520distinguishable.%2520Our%2520work%2520addresses%2520these%2520challenges%2520by%250Aproposing%2520the%2520Calibrated%2520Self-Rewarding%2520%2528CSR%2529%2520approach%252C%2520which%2520enables%2520the%2520model%250Ato%2520self-improve%2520by%2520iteratively%2520generating%2520candidate%2520responses%252C%2520evaluating%2520the%250Areward%2520for%2520each%2520response%252C%2520and%2520curating%2520preference%2520data%2520for%2520fine-tuning.%2520In%2520the%250Areward%2520modeling%252C%2520we%2520employ%2520a%2520step-wise%2520strategy%2520and%2520incorporate%2520visual%250Aconstraints%2520into%2520the%2520self-rewarding%2520process%2520to%2520place%2520greater%2520emphasis%2520on%2520visual%250Ainput.%2520Empirical%2520results%2520demonstrate%2520that%2520CSR%2520enhances%2520performance%2520and%2520reduces%250Ahallucinations%2520across%2520ten%2520benchmarks%2520and%2520tasks%252C%2520achieving%2520substantial%250Aimprovements%2520over%2520existing%2520methods%2520by%25207.62%2525.%2520Our%2520empirical%2520results%2520are%2520further%250Asupported%2520by%2520rigorous%2520theoretical%2520analysis%252C%2520under%2520mild%2520assumptions%252C%2520verifying%250Athe%2520effectiveness%2520of%2520introducing%2520visual%2520constraints%2520into%2520the%2520self-rewarding%250Aparadigm.%2520Additionally%252C%2520CSR%2520shows%2520compatibility%2520with%2520different%2520vision-language%250Amodels%2520and%2520the%2520ability%2520to%2520incrementally%2520improve%2520performance%2520through%2520iterative%250Afine-tuning.%2520Our%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/YiyangZhou/CSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14622v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Calibrated%20Self-Rewarding%20Vision%20Language%20Models&entry.906535625=Yiyang%20Zhou%20and%20Zhiyuan%20Fan%20and%20Dongjie%20Cheng%20and%20Sihan%20Yang%20and%20Zhaorun%20Chen%20and%20Chenhang%20Cui%20and%20Xiyao%20Wang%20and%20Yun%20Li%20and%20Linjun%20Zhang%20and%20Huaxiu%20Yao&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20substantial%20progress%20by%0Aintegrating%20pre-trained%20large%20language%20models%20%28LLMs%29%20and%20vision%20models%20through%0Ainstruction%20tuning.%20Despite%20these%20advancements%2C%20LVLMs%20often%20exhibit%20the%0Ahallucination%20phenomenon%2C%20where%20generated%20text%20responses%20appear%20linguistically%0Aplausible%20but%20contradict%20the%20input%20image%2C%20indicating%20a%20misalignment%20between%0Aimage%20and%20text%20pairs.%20This%20misalignment%20arises%20because%20the%20model%20tends%20to%0Aprioritize%20textual%20information%20over%20visual%20input%2C%20even%20when%20both%20the%20language%0Amodel%20and%20visual%20representations%20are%20of%20high%20quality.%20Existing%20methods%20leverage%0Aadditional%20models%20or%20human%20annotations%20to%20curate%20preference%20data%20and%20enhance%0Amodality%20alignment%20through%20preference%20optimization.%20These%20approaches%20may%20not%0Aeffectively%20reflect%20the%20target%20LVLM%27s%20preferences%2C%20making%20the%20curated%0Apreferences%20easily%20distinguishable.%20Our%20work%20addresses%20these%20challenges%20by%0Aproposing%20the%20Calibrated%20Self-Rewarding%20%28CSR%29%20approach%2C%20which%20enables%20the%20model%0Ato%20self-improve%20by%20iteratively%20generating%20candidate%20responses%2C%20evaluating%20the%0Areward%20for%20each%20response%2C%20and%20curating%20preference%20data%20for%20fine-tuning.%20In%20the%0Areward%20modeling%2C%20we%20employ%20a%20step-wise%20strategy%20and%20incorporate%20visual%0Aconstraints%20into%20the%20self-rewarding%20process%20to%20place%20greater%20emphasis%20on%20visual%0Ainput.%20Empirical%20results%20demonstrate%20that%20CSR%20enhances%20performance%20and%20reduces%0Ahallucinations%20across%20ten%20benchmarks%20and%20tasks%2C%20achieving%20substantial%0Aimprovements%20over%20existing%20methods%20by%207.62%25.%20Our%20empirical%20results%20are%20further%0Asupported%20by%20rigorous%20theoretical%20analysis%2C%20under%20mild%20assumptions%2C%20verifying%0Athe%20effectiveness%20of%20introducing%20visual%20constraints%20into%20the%20self-rewarding%0Aparadigm.%20Additionally%2C%20CSR%20shows%20compatibility%20with%20different%20vision-language%0Amodels%20and%20the%20ability%20to%20incrementally%20improve%20performance%20through%20iterative%0Afine-tuning.%20Our%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/YiyangZhou/CSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14622v3&entry.124074799=Read"},
{"title": "Communication-Efficient Distributed Deep Learning via Federated Dynamic\n  Averaging", "author": "Michail Theologitis and Georgios Frangias and Georgios Anestis and Vasilis Samoladas and Antonios Deligiannakis", "abstract": "  Driven by the ever-growing volume and decentralized nature of data, coupled\nwith the escalating size of modern models, distributed deep learning (DDL) has\nbeen entrenched as the preferred paradigm for training. However, frequent\nsynchronization of DL models, encompassing millions to many billions of\nparameters, creates a communication bottleneck, severely hindering scalability.\nWorse yet, DDL algorithms typically waste valuable bandwidth, and make\nthemselves less practical in bandwidth-constrained federated settings, by\nrelying on overly simplistic, periodic, and rigid synchronization schedules. To\naddress these shortcomings, we propose Federated Dynamic Averaging (FDA), a\ncommunication-efficient DDL strategy that dynamically triggers synchronization\nbased on the value of the model variance. Through extensive experiments across\na wide range of learning tasks we demonstrate that FDA reduces communication\ncost by orders of magnitude, compared to both traditional and cutting-edge\ncommunication-efficient algorithms. Remarkably, FDA achieves this without\nsacrificing convergence speed - in stark contrast to the trade-offs encountered\nin the field. Additionally, we show that FDA maintains robust performance\nacross diverse data heterogeneity settings.\n", "link": "http://arxiv.org/abs/2405.20988v1", "date": "2024-05-31", "relevancy": 2.0994, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5384}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5364}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Distributed%20Deep%20Learning%20via%20Federated%20Dynamic%0A%20%20Averaging&body=Title%3A%20Communication-Efficient%20Distributed%20Deep%20Learning%20via%20Federated%20Dynamic%0A%20%20Averaging%0AAuthor%3A%20Michail%20Theologitis%20and%20Georgios%20Frangias%20and%20Georgios%20Anestis%20and%20Vasilis%20Samoladas%20and%20Antonios%20Deligiannakis%0AAbstract%3A%20%20%20Driven%20by%20the%20ever-growing%20volume%20and%20decentralized%20nature%20of%20data%2C%20coupled%0Awith%20the%20escalating%20size%20of%20modern%20models%2C%20distributed%20deep%20learning%20%28DDL%29%20has%0Abeen%20entrenched%20as%20the%20preferred%20paradigm%20for%20training.%20However%2C%20frequent%0Asynchronization%20of%20DL%20models%2C%20encompassing%20millions%20to%20many%20billions%20of%0Aparameters%2C%20creates%20a%20communication%20bottleneck%2C%20severely%20hindering%20scalability.%0AWorse%20yet%2C%20DDL%20algorithms%20typically%20waste%20valuable%20bandwidth%2C%20and%20make%0Athemselves%20less%20practical%20in%20bandwidth-constrained%20federated%20settings%2C%20by%0Arelying%20on%20overly%20simplistic%2C%20periodic%2C%20and%20rigid%20synchronization%20schedules.%20To%0Aaddress%20these%20shortcomings%2C%20we%20propose%20Federated%20Dynamic%20Averaging%20%28FDA%29%2C%20a%0Acommunication-efficient%20DDL%20strategy%20that%20dynamically%20triggers%20synchronization%0Abased%20on%20the%20value%20of%20the%20model%20variance.%20Through%20extensive%20experiments%20across%0Aa%20wide%20range%20of%20learning%20tasks%20we%20demonstrate%20that%20FDA%20reduces%20communication%0Acost%20by%20orders%20of%20magnitude%2C%20compared%20to%20both%20traditional%20and%20cutting-edge%0Acommunication-efficient%20algorithms.%20Remarkably%2C%20FDA%20achieves%20this%20without%0Asacrificing%20convergence%20speed%20-%20in%20stark%20contrast%20to%20the%20trade-offs%20encountered%0Ain%20the%20field.%20Additionally%2C%20we%20show%20that%20FDA%20maintains%20robust%20performance%0Aacross%20diverse%20data%20heterogeneity%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Distributed%2520Deep%2520Learning%2520via%2520Federated%2520Dynamic%250A%2520%2520Averaging%26entry.906535625%3DMichail%2520Theologitis%2520and%2520Georgios%2520Frangias%2520and%2520Georgios%2520Anestis%2520and%2520Vasilis%2520Samoladas%2520and%2520Antonios%2520Deligiannakis%26entry.1292438233%3D%2520%2520Driven%2520by%2520the%2520ever-growing%2520volume%2520and%2520decentralized%2520nature%2520of%2520data%252C%2520coupled%250Awith%2520the%2520escalating%2520size%2520of%2520modern%2520models%252C%2520distributed%2520deep%2520learning%2520%2528DDL%2529%2520has%250Abeen%2520entrenched%2520as%2520the%2520preferred%2520paradigm%2520for%2520training.%2520However%252C%2520frequent%250Asynchronization%2520of%2520DL%2520models%252C%2520encompassing%2520millions%2520to%2520many%2520billions%2520of%250Aparameters%252C%2520creates%2520a%2520communication%2520bottleneck%252C%2520severely%2520hindering%2520scalability.%250AWorse%2520yet%252C%2520DDL%2520algorithms%2520typically%2520waste%2520valuable%2520bandwidth%252C%2520and%2520make%250Athemselves%2520less%2520practical%2520in%2520bandwidth-constrained%2520federated%2520settings%252C%2520by%250Arelying%2520on%2520overly%2520simplistic%252C%2520periodic%252C%2520and%2520rigid%2520synchronization%2520schedules.%2520To%250Aaddress%2520these%2520shortcomings%252C%2520we%2520propose%2520Federated%2520Dynamic%2520Averaging%2520%2528FDA%2529%252C%2520a%250Acommunication-efficient%2520DDL%2520strategy%2520that%2520dynamically%2520triggers%2520synchronization%250Abased%2520on%2520the%2520value%2520of%2520the%2520model%2520variance.%2520Through%2520extensive%2520experiments%2520across%250Aa%2520wide%2520range%2520of%2520learning%2520tasks%2520we%2520demonstrate%2520that%2520FDA%2520reduces%2520communication%250Acost%2520by%2520orders%2520of%2520magnitude%252C%2520compared%2520to%2520both%2520traditional%2520and%2520cutting-edge%250Acommunication-efficient%2520algorithms.%2520Remarkably%252C%2520FDA%2520achieves%2520this%2520without%250Asacrificing%2520convergence%2520speed%2520-%2520in%2520stark%2520contrast%2520to%2520the%2520trade-offs%2520encountered%250Ain%2520the%2520field.%2520Additionally%252C%2520we%2520show%2520that%2520FDA%2520maintains%2520robust%2520performance%250Aacross%2520diverse%2520data%2520heterogeneity%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Distributed%20Deep%20Learning%20via%20Federated%20Dynamic%0A%20%20Averaging&entry.906535625=Michail%20Theologitis%20and%20Georgios%20Frangias%20and%20Georgios%20Anestis%20and%20Vasilis%20Samoladas%20and%20Antonios%20Deligiannakis&entry.1292438233=%20%20Driven%20by%20the%20ever-growing%20volume%20and%20decentralized%20nature%20of%20data%2C%20coupled%0Awith%20the%20escalating%20size%20of%20modern%20models%2C%20distributed%20deep%20learning%20%28DDL%29%20has%0Abeen%20entrenched%20as%20the%20preferred%20paradigm%20for%20training.%20However%2C%20frequent%0Asynchronization%20of%20DL%20models%2C%20encompassing%20millions%20to%20many%20billions%20of%0Aparameters%2C%20creates%20a%20communication%20bottleneck%2C%20severely%20hindering%20scalability.%0AWorse%20yet%2C%20DDL%20algorithms%20typically%20waste%20valuable%20bandwidth%2C%20and%20make%0Athemselves%20less%20practical%20in%20bandwidth-constrained%20federated%20settings%2C%20by%0Arelying%20on%20overly%20simplistic%2C%20periodic%2C%20and%20rigid%20synchronization%20schedules.%20To%0Aaddress%20these%20shortcomings%2C%20we%20propose%20Federated%20Dynamic%20Averaging%20%28FDA%29%2C%20a%0Acommunication-efficient%20DDL%20strategy%20that%20dynamically%20triggers%20synchronization%0Abased%20on%20the%20value%20of%20the%20model%20variance.%20Through%20extensive%20experiments%20across%0Aa%20wide%20range%20of%20learning%20tasks%20we%20demonstrate%20that%20FDA%20reduces%20communication%0Acost%20by%20orders%20of%20magnitude%2C%20compared%20to%20both%20traditional%20and%20cutting-edge%0Acommunication-efficient%20algorithms.%20Remarkably%2C%20FDA%20achieves%20this%20without%0Asacrificing%20convergence%20speed%20-%20in%20stark%20contrast%20to%20the%20trade-offs%20encountered%0Ain%20the%20field.%20Additionally%2C%20we%20show%20that%20FDA%20maintains%20robust%20performance%0Aacross%20diverse%20data%20heterogeneity%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20988v1&entry.124074799=Read"},
{"title": "Automatic Channel Pruning for Multi-Head Attention", "author": "Eunho Lee and Youngbae Hwang", "abstract": "  Despite the strong performance of Transformers, their quadratic computation\ncomplexity presents challenges in applying them to vision tasks. Automatic\npruning is one of effective methods for reducing computation complexity without\nheuristic approaches. However, directly applying it to multi-head attention is\nnot straightforward due to channel misalignment. In this paper, we propose an\nautomatic channel pruning method to take into account the multi-head attention\nmechanism. First, we incorporate channel similarity-based weights into the\npruning indicator to preserve more informative channels in each head. Then, we\nadjust pruning indicator to enforce removal of channels in equal proportions\nacross all heads, preventing the channel misalignment. We also add a reweight\nmodule to compensate for information loss resulting from channel removal, and\nan effective initialization step for pruning indicator based on difference of\nattention between original structure and each channel. Our proposed method can\nbe used to not only original attention, but also linear attention, which is\nmore efficient as linear complexity with respect to the number of tokens. On\nImageNet-1K, applying our pruning method to the FLattenTransformer, which\nincludes both attention mechanisms, shows outperformed accuracy for several\nMACs compared with previous state-of-the-art efficient models and pruned\nmethods. Code will be available soon.\n", "link": "http://arxiv.org/abs/2405.20867v1", "date": "2024-05-31", "relevancy": 2.0904, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5374}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5212}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Channel%20Pruning%20for%20Multi-Head%20Attention&body=Title%3A%20Automatic%20Channel%20Pruning%20for%20Multi-Head%20Attention%0AAuthor%3A%20Eunho%20Lee%20and%20Youngbae%20Hwang%0AAbstract%3A%20%20%20Despite%20the%20strong%20performance%20of%20Transformers%2C%20their%20quadratic%20computation%0Acomplexity%20presents%20challenges%20in%20applying%20them%20to%20vision%20tasks.%20Automatic%0Apruning%20is%20one%20of%20effective%20methods%20for%20reducing%20computation%20complexity%20without%0Aheuristic%20approaches.%20However%2C%20directly%20applying%20it%20to%20multi-head%20attention%20is%0Anot%20straightforward%20due%20to%20channel%20misalignment.%20In%20this%20paper%2C%20we%20propose%20an%0Aautomatic%20channel%20pruning%20method%20to%20take%20into%20account%20the%20multi-head%20attention%0Amechanism.%20First%2C%20we%20incorporate%20channel%20similarity-based%20weights%20into%20the%0Apruning%20indicator%20to%20preserve%20more%20informative%20channels%20in%20each%20head.%20Then%2C%20we%0Aadjust%20pruning%20indicator%20to%20enforce%20removal%20of%20channels%20in%20equal%20proportions%0Aacross%20all%20heads%2C%20preventing%20the%20channel%20misalignment.%20We%20also%20add%20a%20reweight%0Amodule%20to%20compensate%20for%20information%20loss%20resulting%20from%20channel%20removal%2C%20and%0Aan%20effective%20initialization%20step%20for%20pruning%20indicator%20based%20on%20difference%20of%0Aattention%20between%20original%20structure%20and%20each%20channel.%20Our%20proposed%20method%20can%0Abe%20used%20to%20not%20only%20original%20attention%2C%20but%20also%20linear%20attention%2C%20which%20is%0Amore%20efficient%20as%20linear%20complexity%20with%20respect%20to%20the%20number%20of%20tokens.%20On%0AImageNet-1K%2C%20applying%20our%20pruning%20method%20to%20the%20FLattenTransformer%2C%20which%0Aincludes%20both%20attention%20mechanisms%2C%20shows%20outperformed%20accuracy%20for%20several%0AMACs%20compared%20with%20previous%20state-of-the-art%20efficient%20models%20and%20pruned%0Amethods.%20Code%20will%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Channel%2520Pruning%2520for%2520Multi-Head%2520Attention%26entry.906535625%3DEunho%2520Lee%2520and%2520Youngbae%2520Hwang%26entry.1292438233%3D%2520%2520Despite%2520the%2520strong%2520performance%2520of%2520Transformers%252C%2520their%2520quadratic%2520computation%250Acomplexity%2520presents%2520challenges%2520in%2520applying%2520them%2520to%2520vision%2520tasks.%2520Automatic%250Apruning%2520is%2520one%2520of%2520effective%2520methods%2520for%2520reducing%2520computation%2520complexity%2520without%250Aheuristic%2520approaches.%2520However%252C%2520directly%2520applying%2520it%2520to%2520multi-head%2520attention%2520is%250Anot%2520straightforward%2520due%2520to%2520channel%2520misalignment.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%250Aautomatic%2520channel%2520pruning%2520method%2520to%2520take%2520into%2520account%2520the%2520multi-head%2520attention%250Amechanism.%2520First%252C%2520we%2520incorporate%2520channel%2520similarity-based%2520weights%2520into%2520the%250Apruning%2520indicator%2520to%2520preserve%2520more%2520informative%2520channels%2520in%2520each%2520head.%2520Then%252C%2520we%250Aadjust%2520pruning%2520indicator%2520to%2520enforce%2520removal%2520of%2520channels%2520in%2520equal%2520proportions%250Aacross%2520all%2520heads%252C%2520preventing%2520the%2520channel%2520misalignment.%2520We%2520also%2520add%2520a%2520reweight%250Amodule%2520to%2520compensate%2520for%2520information%2520loss%2520resulting%2520from%2520channel%2520removal%252C%2520and%250Aan%2520effective%2520initialization%2520step%2520for%2520pruning%2520indicator%2520based%2520on%2520difference%2520of%250Aattention%2520between%2520original%2520structure%2520and%2520each%2520channel.%2520Our%2520proposed%2520method%2520can%250Abe%2520used%2520to%2520not%2520only%2520original%2520attention%252C%2520but%2520also%2520linear%2520attention%252C%2520which%2520is%250Amore%2520efficient%2520as%2520linear%2520complexity%2520with%2520respect%2520to%2520the%2520number%2520of%2520tokens.%2520On%250AImageNet-1K%252C%2520applying%2520our%2520pruning%2520method%2520to%2520the%2520FLattenTransformer%252C%2520which%250Aincludes%2520both%2520attention%2520mechanisms%252C%2520shows%2520outperformed%2520accuracy%2520for%2520several%250AMACs%2520compared%2520with%2520previous%2520state-of-the-art%2520efficient%2520models%2520and%2520pruned%250Amethods.%2520Code%2520will%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Channel%20Pruning%20for%20Multi-Head%20Attention&entry.906535625=Eunho%20Lee%20and%20Youngbae%20Hwang&entry.1292438233=%20%20Despite%20the%20strong%20performance%20of%20Transformers%2C%20their%20quadratic%20computation%0Acomplexity%20presents%20challenges%20in%20applying%20them%20to%20vision%20tasks.%20Automatic%0Apruning%20is%20one%20of%20effective%20methods%20for%20reducing%20computation%20complexity%20without%0Aheuristic%20approaches.%20However%2C%20directly%20applying%20it%20to%20multi-head%20attention%20is%0Anot%20straightforward%20due%20to%20channel%20misalignment.%20In%20this%20paper%2C%20we%20propose%20an%0Aautomatic%20channel%20pruning%20method%20to%20take%20into%20account%20the%20multi-head%20attention%0Amechanism.%20First%2C%20we%20incorporate%20channel%20similarity-based%20weights%20into%20the%0Apruning%20indicator%20to%20preserve%20more%20informative%20channels%20in%20each%20head.%20Then%2C%20we%0Aadjust%20pruning%20indicator%20to%20enforce%20removal%20of%20channels%20in%20equal%20proportions%0Aacross%20all%20heads%2C%20preventing%20the%20channel%20misalignment.%20We%20also%20add%20a%20reweight%0Amodule%20to%20compensate%20for%20information%20loss%20resulting%20from%20channel%20removal%2C%20and%0Aan%20effective%20initialization%20step%20for%20pruning%20indicator%20based%20on%20difference%20of%0Aattention%20between%20original%20structure%20and%20each%20channel.%20Our%20proposed%20method%20can%0Abe%20used%20to%20not%20only%20original%20attention%2C%20but%20also%20linear%20attention%2C%20which%20is%0Amore%20efficient%20as%20linear%20complexity%20with%20respect%20to%20the%20number%20of%20tokens.%20On%0AImageNet-1K%2C%20applying%20our%20pruning%20method%20to%20the%20FLattenTransformer%2C%20which%0Aincludes%20both%20attention%20mechanisms%2C%20shows%20outperformed%20accuracy%20for%20several%0AMACs%20compared%20with%20previous%20state-of-the-art%20efficient%20models%20and%20pruned%0Amethods.%20Code%20will%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20867v1&entry.124074799=Read"},
{"title": "II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in\n  Visual Question Answering", "author": "Jihyung Kil and Farideh Tavazoee and Dongyeop Kang and Joo-Kyung Kim", "abstract": "  Visual Question Answering (VQA) often involves diverse reasoning scenarios\nacross Vision and Language (V&L). Most prior VQA studies, however, have merely\nfocused on assessing the model's overall accuracy without evaluating it on\ndifferent reasoning cases. Furthermore, some recent works observe that\nconventional Chain-of-Thought (CoT) prompting fails to generate effective\nreasoning for VQA, especially for complex scenarios requiring multi-hop\nreasoning. In this paper, we propose II-MMR, a novel idea to identify and\nimprove multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA\nquestion with an image and finds a reasoning path to reach its answer using two\nnovel language promptings: (i) answer prediction-guided CoT prompt, or (ii)\nknowledge triplet-guided prompt. II-MMR then analyzes this path to identify\ndifferent reasoning cases in current VQA benchmarks by estimating how many hops\nand what types (i.e., visual or beyond-visual) of reasoning are required to\nanswer the question. On popular benchmarks including GQA and A-OKVQA, II-MMR\nobserves that most of their VQA questions are easy to answer, simply demanding\n\"single-hop\" reasoning, whereas only a few questions require \"multi-hop\"\nreasoning. Moreover, while the recent V&L model struggles with such complex\nmulti-hop reasoning questions even using the traditional CoT method, II-MMR\nshows its effectiveness across all reasoning cases in both zero-shot and\nfine-tuning settings.\n", "link": "http://arxiv.org/abs/2402.11058v2", "date": "2024-05-31", "relevancy": 2.082, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5502}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20II-MMR%3A%20Identifying%20and%20Improving%20Multi-modal%20Multi-hop%20Reasoning%20in%0A%20%20Visual%20Question%20Answering&body=Title%3A%20II-MMR%3A%20Identifying%20and%20Improving%20Multi-modal%20Multi-hop%20Reasoning%20in%0A%20%20Visual%20Question%20Answering%0AAuthor%3A%20Jihyung%20Kil%20and%20Farideh%20Tavazoee%20and%20Dongyeop%20Kang%20and%20Joo-Kyung%20Kim%0AAbstract%3A%20%20%20Visual%20Question%20Answering%20%28VQA%29%20often%20involves%20diverse%20reasoning%20scenarios%0Aacross%20Vision%20and%20Language%20%28V%26L%29.%20Most%20prior%20VQA%20studies%2C%20however%2C%20have%20merely%0Afocused%20on%20assessing%20the%20model%27s%20overall%20accuracy%20without%20evaluating%20it%20on%0Adifferent%20reasoning%20cases.%20Furthermore%2C%20some%20recent%20works%20observe%20that%0Aconventional%20Chain-of-Thought%20%28CoT%29%20prompting%20fails%20to%20generate%20effective%0Areasoning%20for%20VQA%2C%20especially%20for%20complex%20scenarios%20requiring%20multi-hop%0Areasoning.%20In%20this%20paper%2C%20we%20propose%20II-MMR%2C%20a%20novel%20idea%20to%20identify%20and%0Aimprove%20multi-modal%20multi-hop%20reasoning%20in%20VQA.%20In%20specific%2C%20II-MMR%20takes%20a%20VQA%0Aquestion%20with%20an%20image%20and%20finds%20a%20reasoning%20path%20to%20reach%20its%20answer%20using%20two%0Anovel%20language%20promptings%3A%20%28i%29%20answer%20prediction-guided%20CoT%20prompt%2C%20or%20%28ii%29%0Aknowledge%20triplet-guided%20prompt.%20II-MMR%20then%20analyzes%20this%20path%20to%20identify%0Adifferent%20reasoning%20cases%20in%20current%20VQA%20benchmarks%20by%20estimating%20how%20many%20hops%0Aand%20what%20types%20%28i.e.%2C%20visual%20or%20beyond-visual%29%20of%20reasoning%20are%20required%20to%0Aanswer%20the%20question.%20On%20popular%20benchmarks%20including%20GQA%20and%20A-OKVQA%2C%20II-MMR%0Aobserves%20that%20most%20of%20their%20VQA%20questions%20are%20easy%20to%20answer%2C%20simply%20demanding%0A%22single-hop%22%20reasoning%2C%20whereas%20only%20a%20few%20questions%20require%20%22multi-hop%22%0Areasoning.%20Moreover%2C%20while%20the%20recent%20V%26L%20model%20struggles%20with%20such%20complex%0Amulti-hop%20reasoning%20questions%20even%20using%20the%20traditional%20CoT%20method%2C%20II-MMR%0Ashows%20its%20effectiveness%20across%20all%20reasoning%20cases%20in%20both%20zero-shot%20and%0Afine-tuning%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11058v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DII-MMR%253A%2520Identifying%2520and%2520Improving%2520Multi-modal%2520Multi-hop%2520Reasoning%2520in%250A%2520%2520Visual%2520Question%2520Answering%26entry.906535625%3DJihyung%2520Kil%2520and%2520Farideh%2520Tavazoee%2520and%2520Dongyeop%2520Kang%2520and%2520Joo-Kyung%2520Kim%26entry.1292438233%3D%2520%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520often%2520involves%2520diverse%2520reasoning%2520scenarios%250Aacross%2520Vision%2520and%2520Language%2520%2528V%2526L%2529.%2520Most%2520prior%2520VQA%2520studies%252C%2520however%252C%2520have%2520merely%250Afocused%2520on%2520assessing%2520the%2520model%2527s%2520overall%2520accuracy%2520without%2520evaluating%2520it%2520on%250Adifferent%2520reasoning%2520cases.%2520Furthermore%252C%2520some%2520recent%2520works%2520observe%2520that%250Aconventional%2520Chain-of-Thought%2520%2528CoT%2529%2520prompting%2520fails%2520to%2520generate%2520effective%250Areasoning%2520for%2520VQA%252C%2520especially%2520for%2520complex%2520scenarios%2520requiring%2520multi-hop%250Areasoning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520II-MMR%252C%2520a%2520novel%2520idea%2520to%2520identify%2520and%250Aimprove%2520multi-modal%2520multi-hop%2520reasoning%2520in%2520VQA.%2520In%2520specific%252C%2520II-MMR%2520takes%2520a%2520VQA%250Aquestion%2520with%2520an%2520image%2520and%2520finds%2520a%2520reasoning%2520path%2520to%2520reach%2520its%2520answer%2520using%2520two%250Anovel%2520language%2520promptings%253A%2520%2528i%2529%2520answer%2520prediction-guided%2520CoT%2520prompt%252C%2520or%2520%2528ii%2529%250Aknowledge%2520triplet-guided%2520prompt.%2520II-MMR%2520then%2520analyzes%2520this%2520path%2520to%2520identify%250Adifferent%2520reasoning%2520cases%2520in%2520current%2520VQA%2520benchmarks%2520by%2520estimating%2520how%2520many%2520hops%250Aand%2520what%2520types%2520%2528i.e.%252C%2520visual%2520or%2520beyond-visual%2529%2520of%2520reasoning%2520are%2520required%2520to%250Aanswer%2520the%2520question.%2520On%2520popular%2520benchmarks%2520including%2520GQA%2520and%2520A-OKVQA%252C%2520II-MMR%250Aobserves%2520that%2520most%2520of%2520their%2520VQA%2520questions%2520are%2520easy%2520to%2520answer%252C%2520simply%2520demanding%250A%2522single-hop%2522%2520reasoning%252C%2520whereas%2520only%2520a%2520few%2520questions%2520require%2520%2522multi-hop%2522%250Areasoning.%2520Moreover%252C%2520while%2520the%2520recent%2520V%2526L%2520model%2520struggles%2520with%2520such%2520complex%250Amulti-hop%2520reasoning%2520questions%2520even%2520using%2520the%2520traditional%2520CoT%2520method%252C%2520II-MMR%250Ashows%2520its%2520effectiveness%2520across%2520all%2520reasoning%2520cases%2520in%2520both%2520zero-shot%2520and%250Afine-tuning%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11058v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=II-MMR%3A%20Identifying%20and%20Improving%20Multi-modal%20Multi-hop%20Reasoning%20in%0A%20%20Visual%20Question%20Answering&entry.906535625=Jihyung%20Kil%20and%20Farideh%20Tavazoee%20and%20Dongyeop%20Kang%20and%20Joo-Kyung%20Kim&entry.1292438233=%20%20Visual%20Question%20Answering%20%28VQA%29%20often%20involves%20diverse%20reasoning%20scenarios%0Aacross%20Vision%20and%20Language%20%28V%26L%29.%20Most%20prior%20VQA%20studies%2C%20however%2C%20have%20merely%0Afocused%20on%20assessing%20the%20model%27s%20overall%20accuracy%20without%20evaluating%20it%20on%0Adifferent%20reasoning%20cases.%20Furthermore%2C%20some%20recent%20works%20observe%20that%0Aconventional%20Chain-of-Thought%20%28CoT%29%20prompting%20fails%20to%20generate%20effective%0Areasoning%20for%20VQA%2C%20especially%20for%20complex%20scenarios%20requiring%20multi-hop%0Areasoning.%20In%20this%20paper%2C%20we%20propose%20II-MMR%2C%20a%20novel%20idea%20to%20identify%20and%0Aimprove%20multi-modal%20multi-hop%20reasoning%20in%20VQA.%20In%20specific%2C%20II-MMR%20takes%20a%20VQA%0Aquestion%20with%20an%20image%20and%20finds%20a%20reasoning%20path%20to%20reach%20its%20answer%20using%20two%0Anovel%20language%20promptings%3A%20%28i%29%20answer%20prediction-guided%20CoT%20prompt%2C%20or%20%28ii%29%0Aknowledge%20triplet-guided%20prompt.%20II-MMR%20then%20analyzes%20this%20path%20to%20identify%0Adifferent%20reasoning%20cases%20in%20current%20VQA%20benchmarks%20by%20estimating%20how%20many%20hops%0Aand%20what%20types%20%28i.e.%2C%20visual%20or%20beyond-visual%29%20of%20reasoning%20are%20required%20to%0Aanswer%20the%20question.%20On%20popular%20benchmarks%20including%20GQA%20and%20A-OKVQA%2C%20II-MMR%0Aobserves%20that%20most%20of%20their%20VQA%20questions%20are%20easy%20to%20answer%2C%20simply%20demanding%0A%22single-hop%22%20reasoning%2C%20whereas%20only%20a%20few%20questions%20require%20%22multi-hop%22%0Areasoning.%20Moreover%2C%20while%20the%20recent%20V%26L%20model%20struggles%20with%20such%20complex%0Amulti-hop%20reasoning%20questions%20even%20using%20the%20traditional%20CoT%20method%2C%20II-MMR%0Ashows%20its%20effectiveness%20across%20all%20reasoning%20cases%20in%20both%20zero-shot%20and%0Afine-tuning%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11058v2&entry.124074799=Read"},
{"title": "Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour\n  Segmentation", "author": "Richard Osuala and Smriti Joshi and Apostolia Tsirikoglou and Lidia Garrucho and Walter H. L. Pinaya and Oliver Diaz and Karim Lekadir", "abstract": "  Despite its benefits for tumour detection and treatment, the administration\nof contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated\nwith a range of issues, including their invasiveness, bioaccumulation, and a\nrisk of nephrogenic systemic fibrosis. This study explores the feasibility of\nproducing synthetic contrast enhancements by translating pre-contrast\nT1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI\nsequence leveraging the capabilities of a generative adversarial network (GAN).\nAdditionally, we introduce a Scaled Aggregate Measure (SAMe) designed for\nquantitatively evaluating the quality of synthetic data in a principled manner\nand serving as a basis for selecting the optimal generative model. We assess\nthe generated DCE-MRI data using quantitative image quality metrics and apply\nthem to the downstream task of 3D breast tumour segmentation. Our results\nhighlight the potential of post-contrast DCE-MRI synthesis in enhancing the\nrobustness of breast tumour segmentation models via data augmentation. Our code\nis available at https://github.com/RichardObi/pre_post_synthesis.\n", "link": "http://arxiv.org/abs/2311.10879v3", "date": "2024-05-31", "relevancy": 2.0818, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5244}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5244}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-%20to%20Post-Contrast%20Breast%20MRI%20Synthesis%20for%20Enhanced%20Tumour%0A%20%20Segmentation&body=Title%3A%20Pre-%20to%20Post-Contrast%20Breast%20MRI%20Synthesis%20for%20Enhanced%20Tumour%0A%20%20Segmentation%0AAuthor%3A%20Richard%20Osuala%20and%20Smriti%20Joshi%20and%20Apostolia%20Tsirikoglou%20and%20Lidia%20Garrucho%20and%20Walter%20H.%20L.%20Pinaya%20and%20Oliver%20Diaz%20and%20Karim%20Lekadir%0AAbstract%3A%20%20%20Despite%20its%20benefits%20for%20tumour%20detection%20and%20treatment%2C%20the%20administration%0Aof%20contrast%20agents%20in%20dynamic%20contrast-enhanced%20MRI%20%28DCE-MRI%29%20is%20associated%0Awith%20a%20range%20of%20issues%2C%20including%20their%20invasiveness%2C%20bioaccumulation%2C%20and%20a%0Arisk%20of%20nephrogenic%20systemic%20fibrosis.%20This%20study%20explores%20the%20feasibility%20of%0Aproducing%20synthetic%20contrast%20enhancements%20by%20translating%20pre-contrast%0AT1-weighted%20fat-saturated%20breast%20MRI%20to%20their%20corresponding%20first%20DCE-MRI%0Asequence%20leveraging%20the%20capabilities%20of%20a%20generative%20adversarial%20network%20%28GAN%29.%0AAdditionally%2C%20we%20introduce%20a%20Scaled%20Aggregate%20Measure%20%28SAMe%29%20designed%20for%0Aquantitatively%20evaluating%20the%20quality%20of%20synthetic%20data%20in%20a%20principled%20manner%0Aand%20serving%20as%20a%20basis%20for%20selecting%20the%20optimal%20generative%20model.%20We%20assess%0Athe%20generated%20DCE-MRI%20data%20using%20quantitative%20image%20quality%20metrics%20and%20apply%0Athem%20to%20the%20downstream%20task%20of%203D%20breast%20tumour%20segmentation.%20Our%20results%0Ahighlight%20the%20potential%20of%20post-contrast%20DCE-MRI%20synthesis%20in%20enhancing%20the%0Arobustness%20of%20breast%20tumour%20segmentation%20models%20via%20data%20augmentation.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/RichardObi/pre_post_synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10879v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-%2520to%2520Post-Contrast%2520Breast%2520MRI%2520Synthesis%2520for%2520Enhanced%2520Tumour%250A%2520%2520Segmentation%26entry.906535625%3DRichard%2520Osuala%2520and%2520Smriti%2520Joshi%2520and%2520Apostolia%2520Tsirikoglou%2520and%2520Lidia%2520Garrucho%2520and%2520Walter%2520H.%2520L.%2520Pinaya%2520and%2520Oliver%2520Diaz%2520and%2520Karim%2520Lekadir%26entry.1292438233%3D%2520%2520Despite%2520its%2520benefits%2520for%2520tumour%2520detection%2520and%2520treatment%252C%2520the%2520administration%250Aof%2520contrast%2520agents%2520in%2520dynamic%2520contrast-enhanced%2520MRI%2520%2528DCE-MRI%2529%2520is%2520associated%250Awith%2520a%2520range%2520of%2520issues%252C%2520including%2520their%2520invasiveness%252C%2520bioaccumulation%252C%2520and%2520a%250Arisk%2520of%2520nephrogenic%2520systemic%2520fibrosis.%2520This%2520study%2520explores%2520the%2520feasibility%2520of%250Aproducing%2520synthetic%2520contrast%2520enhancements%2520by%2520translating%2520pre-contrast%250AT1-weighted%2520fat-saturated%2520breast%2520MRI%2520to%2520their%2520corresponding%2520first%2520DCE-MRI%250Asequence%2520leveraging%2520the%2520capabilities%2520of%2520a%2520generative%2520adversarial%2520network%2520%2528GAN%2529.%250AAdditionally%252C%2520we%2520introduce%2520a%2520Scaled%2520Aggregate%2520Measure%2520%2528SAMe%2529%2520designed%2520for%250Aquantitatively%2520evaluating%2520the%2520quality%2520of%2520synthetic%2520data%2520in%2520a%2520principled%2520manner%250Aand%2520serving%2520as%2520a%2520basis%2520for%2520selecting%2520the%2520optimal%2520generative%2520model.%2520We%2520assess%250Athe%2520generated%2520DCE-MRI%2520data%2520using%2520quantitative%2520image%2520quality%2520metrics%2520and%2520apply%250Athem%2520to%2520the%2520downstream%2520task%2520of%25203D%2520breast%2520tumour%2520segmentation.%2520Our%2520results%250Ahighlight%2520the%2520potential%2520of%2520post-contrast%2520DCE-MRI%2520synthesis%2520in%2520enhancing%2520the%250Arobustness%2520of%2520breast%2520tumour%2520segmentation%2520models%2520via%2520data%2520augmentation.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/RichardObi/pre_post_synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10879v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-%20to%20Post-Contrast%20Breast%20MRI%20Synthesis%20for%20Enhanced%20Tumour%0A%20%20Segmentation&entry.906535625=Richard%20Osuala%20and%20Smriti%20Joshi%20and%20Apostolia%20Tsirikoglou%20and%20Lidia%20Garrucho%20and%20Walter%20H.%20L.%20Pinaya%20and%20Oliver%20Diaz%20and%20Karim%20Lekadir&entry.1292438233=%20%20Despite%20its%20benefits%20for%20tumour%20detection%20and%20treatment%2C%20the%20administration%0Aof%20contrast%20agents%20in%20dynamic%20contrast-enhanced%20MRI%20%28DCE-MRI%29%20is%20associated%0Awith%20a%20range%20of%20issues%2C%20including%20their%20invasiveness%2C%20bioaccumulation%2C%20and%20a%0Arisk%20of%20nephrogenic%20systemic%20fibrosis.%20This%20study%20explores%20the%20feasibility%20of%0Aproducing%20synthetic%20contrast%20enhancements%20by%20translating%20pre-contrast%0AT1-weighted%20fat-saturated%20breast%20MRI%20to%20their%20corresponding%20first%20DCE-MRI%0Asequence%20leveraging%20the%20capabilities%20of%20a%20generative%20adversarial%20network%20%28GAN%29.%0AAdditionally%2C%20we%20introduce%20a%20Scaled%20Aggregate%20Measure%20%28SAMe%29%20designed%20for%0Aquantitatively%20evaluating%20the%20quality%20of%20synthetic%20data%20in%20a%20principled%20manner%0Aand%20serving%20as%20a%20basis%20for%20selecting%20the%20optimal%20generative%20model.%20We%20assess%0Athe%20generated%20DCE-MRI%20data%20using%20quantitative%20image%20quality%20metrics%20and%20apply%0Athem%20to%20the%20downstream%20task%20of%203D%20breast%20tumour%20segmentation.%20Our%20results%0Ahighlight%20the%20potential%20of%20post-contrast%20DCE-MRI%20synthesis%20in%20enhancing%20the%0Arobustness%20of%20breast%20tumour%20segmentation%20models%20via%20data%20augmentation.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/RichardObi/pre_post_synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10879v3&entry.124074799=Read"},
{"title": "Scalable Distance-based Multi-Agent Relative State Estimation via Block\n  Multiconvex Optimization", "author": "Tianyue Wu and Gongye Zaitian and Qianhao Wang and Fei Gao", "abstract": "  This paper explores the distance-based relative state estimation problem in\nlarge-scale systems, which is hard to solve effectively due to its\nhigh-dimensionality and non-convexity. In this paper, we alleviate this\ninherent hardness to simultaneously achieve scalability and robustness of\ninference on this problem. Our idea is launched from a universal geometric\nformulation, called \\emph{generalized graph realization}, for the\ndistance-based relative state estimation problem. Based on this formulation, we\nintroduce two collaborative optimization models, one of which is convex and\nthus globally solvable, and the other enables fast searching on non-convex\nlandscapes to refine the solution offered by the convex one. Importantly, both\nmodels enjoy \\emph{multiconvex} and \\emph{decomposable} structures, allowing\nefficient and safe solutions using \\emph{block coordinate descent} that enjoys\nscalability and a distributed nature. The proposed algorithms collaborate to\ndemonstrate superior or comparable solution precision to the current\ncentralized convex relaxation-based methods, which are known for their high\noptimality. Distinctly, the proposed methods demonstrate scalability beyond the\nreach of previous convex relaxation-based methods. We also demonstrate that the\ncombination of the two proposed algorithms achieves a more robust pipeline than\ndeploying the local search method alone in a continuous-time scenario.\n", "link": "http://arxiv.org/abs/2405.20883v1", "date": "2024-05-31", "relevancy": 2.0794, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5738}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5125}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Distance-based%20Multi-Agent%20Relative%20State%20Estimation%20via%20Block%0A%20%20Multiconvex%20Optimization&body=Title%3A%20Scalable%20Distance-based%20Multi-Agent%20Relative%20State%20Estimation%20via%20Block%0A%20%20Multiconvex%20Optimization%0AAuthor%3A%20Tianyue%20Wu%20and%20Gongye%20Zaitian%20and%20Qianhao%20Wang%20and%20Fei%20Gao%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20distance-based%20relative%20state%20estimation%20problem%20in%0Alarge-scale%20systems%2C%20which%20is%20hard%20to%20solve%20effectively%20due%20to%20its%0Ahigh-dimensionality%20and%20non-convexity.%20In%20this%20paper%2C%20we%20alleviate%20this%0Ainherent%20hardness%20to%20simultaneously%20achieve%20scalability%20and%20robustness%20of%0Ainference%20on%20this%20problem.%20Our%20idea%20is%20launched%20from%20a%20universal%20geometric%0Aformulation%2C%20called%20%5Cemph%7Bgeneralized%20graph%20realization%7D%2C%20for%20the%0Adistance-based%20relative%20state%20estimation%20problem.%20Based%20on%20this%20formulation%2C%20we%0Aintroduce%20two%20collaborative%20optimization%20models%2C%20one%20of%20which%20is%20convex%20and%0Athus%20globally%20solvable%2C%20and%20the%20other%20enables%20fast%20searching%20on%20non-convex%0Alandscapes%20to%20refine%20the%20solution%20offered%20by%20the%20convex%20one.%20Importantly%2C%20both%0Amodels%20enjoy%20%5Cemph%7Bmulticonvex%7D%20and%20%5Cemph%7Bdecomposable%7D%20structures%2C%20allowing%0Aefficient%20and%20safe%20solutions%20using%20%5Cemph%7Bblock%20coordinate%20descent%7D%20that%20enjoys%0Ascalability%20and%20a%20distributed%20nature.%20The%20proposed%20algorithms%20collaborate%20to%0Ademonstrate%20superior%20or%20comparable%20solution%20precision%20to%20the%20current%0Acentralized%20convex%20relaxation-based%20methods%2C%20which%20are%20known%20for%20their%20high%0Aoptimality.%20Distinctly%2C%20the%20proposed%20methods%20demonstrate%20scalability%20beyond%20the%0Areach%20of%20previous%20convex%20relaxation-based%20methods.%20We%20also%20demonstrate%20that%20the%0Acombination%20of%20the%20two%20proposed%20algorithms%20achieves%20a%20more%20robust%20pipeline%20than%0Adeploying%20the%20local%20search%20method%20alone%20in%20a%20continuous-time%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Distance-based%2520Multi-Agent%2520Relative%2520State%2520Estimation%2520via%2520Block%250A%2520%2520Multiconvex%2520Optimization%26entry.906535625%3DTianyue%2520Wu%2520and%2520Gongye%2520Zaitian%2520and%2520Qianhao%2520Wang%2520and%2520Fei%2520Gao%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520distance-based%2520relative%2520state%2520estimation%2520problem%2520in%250Alarge-scale%2520systems%252C%2520which%2520is%2520hard%2520to%2520solve%2520effectively%2520due%2520to%2520its%250Ahigh-dimensionality%2520and%2520non-convexity.%2520In%2520this%2520paper%252C%2520we%2520alleviate%2520this%250Ainherent%2520hardness%2520to%2520simultaneously%2520achieve%2520scalability%2520and%2520robustness%2520of%250Ainference%2520on%2520this%2520problem.%2520Our%2520idea%2520is%2520launched%2520from%2520a%2520universal%2520geometric%250Aformulation%252C%2520called%2520%255Cemph%257Bgeneralized%2520graph%2520realization%257D%252C%2520for%2520the%250Adistance-based%2520relative%2520state%2520estimation%2520problem.%2520Based%2520on%2520this%2520formulation%252C%2520we%250Aintroduce%2520two%2520collaborative%2520optimization%2520models%252C%2520one%2520of%2520which%2520is%2520convex%2520and%250Athus%2520globally%2520solvable%252C%2520and%2520the%2520other%2520enables%2520fast%2520searching%2520on%2520non-convex%250Alandscapes%2520to%2520refine%2520the%2520solution%2520offered%2520by%2520the%2520convex%2520one.%2520Importantly%252C%2520both%250Amodels%2520enjoy%2520%255Cemph%257Bmulticonvex%257D%2520and%2520%255Cemph%257Bdecomposable%257D%2520structures%252C%2520allowing%250Aefficient%2520and%2520safe%2520solutions%2520using%2520%255Cemph%257Bblock%2520coordinate%2520descent%257D%2520that%2520enjoys%250Ascalability%2520and%2520a%2520distributed%2520nature.%2520The%2520proposed%2520algorithms%2520collaborate%2520to%250Ademonstrate%2520superior%2520or%2520comparable%2520solution%2520precision%2520to%2520the%2520current%250Acentralized%2520convex%2520relaxation-based%2520methods%252C%2520which%2520are%2520known%2520for%2520their%2520high%250Aoptimality.%2520Distinctly%252C%2520the%2520proposed%2520methods%2520demonstrate%2520scalability%2520beyond%2520the%250Areach%2520of%2520previous%2520convex%2520relaxation-based%2520methods.%2520We%2520also%2520demonstrate%2520that%2520the%250Acombination%2520of%2520the%2520two%2520proposed%2520algorithms%2520achieves%2520a%2520more%2520robust%2520pipeline%2520than%250Adeploying%2520the%2520local%2520search%2520method%2520alone%2520in%2520a%2520continuous-time%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Distance-based%20Multi-Agent%20Relative%20State%20Estimation%20via%20Block%0A%20%20Multiconvex%20Optimization&entry.906535625=Tianyue%20Wu%20and%20Gongye%20Zaitian%20and%20Qianhao%20Wang%20and%20Fei%20Gao&entry.1292438233=%20%20This%20paper%20explores%20the%20distance-based%20relative%20state%20estimation%20problem%20in%0Alarge-scale%20systems%2C%20which%20is%20hard%20to%20solve%20effectively%20due%20to%20its%0Ahigh-dimensionality%20and%20non-convexity.%20In%20this%20paper%2C%20we%20alleviate%20this%0Ainherent%20hardness%20to%20simultaneously%20achieve%20scalability%20and%20robustness%20of%0Ainference%20on%20this%20problem.%20Our%20idea%20is%20launched%20from%20a%20universal%20geometric%0Aformulation%2C%20called%20%5Cemph%7Bgeneralized%20graph%20realization%7D%2C%20for%20the%0Adistance-based%20relative%20state%20estimation%20problem.%20Based%20on%20this%20formulation%2C%20we%0Aintroduce%20two%20collaborative%20optimization%20models%2C%20one%20of%20which%20is%20convex%20and%0Athus%20globally%20solvable%2C%20and%20the%20other%20enables%20fast%20searching%20on%20non-convex%0Alandscapes%20to%20refine%20the%20solution%20offered%20by%20the%20convex%20one.%20Importantly%2C%20both%0Amodels%20enjoy%20%5Cemph%7Bmulticonvex%7D%20and%20%5Cemph%7Bdecomposable%7D%20structures%2C%20allowing%0Aefficient%20and%20safe%20solutions%20using%20%5Cemph%7Bblock%20coordinate%20descent%7D%20that%20enjoys%0Ascalability%20and%20a%20distributed%20nature.%20The%20proposed%20algorithms%20collaborate%20to%0Ademonstrate%20superior%20or%20comparable%20solution%20precision%20to%20the%20current%0Acentralized%20convex%20relaxation-based%20methods%2C%20which%20are%20known%20for%20their%20high%0Aoptimality.%20Distinctly%2C%20the%20proposed%20methods%20demonstrate%20scalability%20beyond%20the%0Areach%20of%20previous%20convex%20relaxation-based%20methods.%20We%20also%20demonstrate%20that%20the%0Acombination%20of%20the%20two%20proposed%20algorithms%20achieves%20a%20more%20robust%20pipeline%20than%0Adeploying%20the%20local%20search%20method%20alone%20in%20a%20continuous-time%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20883v1&entry.124074799=Read"},
{"title": "VENI, VINDy, VICI: a variational reduced-order modeling framework with\n  uncertainty quantification", "author": "Paolo Conti and Jonas Kneifl and Andrea Manzoni and Attilio Frangi and J\u00f6rg Fehr and Steven L. Brunton and J. Nathan Kutz", "abstract": "  The simulation of many complex phenomena in engineering and science requires\nsolving expensive, high-dimensional systems of partial differential equations\n(PDEs). To circumvent this, reduced-order models (ROMs) have been developed to\nspeed up computations. However, when governing equations are unknown or\npartially known, typically ROMs lack interpretability and reliability of the\npredicted solutions.\n  In this work we present a data-driven, non-intrusive framework for building\nROMs where the latent variables and dynamics are identified in an interpretable\nmanner and uncertainty is quantified. Starting from a limited amount of\nhigh-dimensional, noisy data the proposed framework constructs an efficient ROM\nby leveraging variational autoencoders for dimensionality reduction along with\na newly introduced, variational version of sparse identification of nonlinear\ndynamics (SINDy), which we refer to as Variational Identification of Nonlinear\nDynamics (VINDy).\n  In detail, the method consists of Variational Encoding of Noisy Inputs (VENI)\nto identify the distribution of reduced coordinates. Simultaneously, we learn\nthe distribution of the coefficients of a pre-determined set of candidate\nfunctions by VINDy. Once trained offline, the identified model can be queried\nfor new parameter instances and new initial conditions to compute the\ncorresponding full-time solutions. The probabilistic setup enables uncertainty\nquantification as the online testing consists of Variational Inference\nnaturally providing Certainty Intervals (VICI). In this work we showcase the\neffectiveness of the newly proposed VINDy method in identifying interpretable\nand accurate dynamical system for the R\\\"ossler system with different noise\nintensities and sources. Then the performance of the overall method - named\nVENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics\nand fluid dynamics.\n", "link": "http://arxiv.org/abs/2405.20905v1", "date": "2024-05-31", "relevancy": 2.0732, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5418}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5055}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VENI%2C%20VINDy%2C%20VICI%3A%20a%20variational%20reduced-order%20modeling%20framework%20with%0A%20%20uncertainty%20quantification&body=Title%3A%20VENI%2C%20VINDy%2C%20VICI%3A%20a%20variational%20reduced-order%20modeling%20framework%20with%0A%20%20uncertainty%20quantification%0AAuthor%3A%20Paolo%20Conti%20and%20Jonas%20Kneifl%20and%20Andrea%20Manzoni%20and%20Attilio%20Frangi%20and%20J%C3%B6rg%20Fehr%20and%20Steven%20L.%20Brunton%20and%20J.%20Nathan%20Kutz%0AAbstract%3A%20%20%20The%20simulation%20of%20many%20complex%20phenomena%20in%20engineering%20and%20science%20requires%0Asolving%20expensive%2C%20high-dimensional%20systems%20of%20partial%20differential%20equations%0A%28PDEs%29.%20To%20circumvent%20this%2C%20reduced-order%20models%20%28ROMs%29%20have%20been%20developed%20to%0Aspeed%20up%20computations.%20However%2C%20when%20governing%20equations%20are%20unknown%20or%0Apartially%20known%2C%20typically%20ROMs%20lack%20interpretability%20and%20reliability%20of%20the%0Apredicted%20solutions.%0A%20%20In%20this%20work%20we%20present%20a%20data-driven%2C%20non-intrusive%20framework%20for%20building%0AROMs%20where%20the%20latent%20variables%20and%20dynamics%20are%20identified%20in%20an%20interpretable%0Amanner%20and%20uncertainty%20is%20quantified.%20Starting%20from%20a%20limited%20amount%20of%0Ahigh-dimensional%2C%20noisy%20data%20the%20proposed%20framework%20constructs%20an%20efficient%20ROM%0Aby%20leveraging%20variational%20autoencoders%20for%20dimensionality%20reduction%20along%20with%0Aa%20newly%20introduced%2C%20variational%20version%20of%20sparse%20identification%20of%20nonlinear%0Adynamics%20%28SINDy%29%2C%20which%20we%20refer%20to%20as%20Variational%20Identification%20of%20Nonlinear%0ADynamics%20%28VINDy%29.%0A%20%20In%20detail%2C%20the%20method%20consists%20of%20Variational%20Encoding%20of%20Noisy%20Inputs%20%28VENI%29%0Ato%20identify%20the%20distribution%20of%20reduced%20coordinates.%20Simultaneously%2C%20we%20learn%0Athe%20distribution%20of%20the%20coefficients%20of%20a%20pre-determined%20set%20of%20candidate%0Afunctions%20by%20VINDy.%20Once%20trained%20offline%2C%20the%20identified%20model%20can%20be%20queried%0Afor%20new%20parameter%20instances%20and%20new%20initial%20conditions%20to%20compute%20the%0Acorresponding%20full-time%20solutions.%20The%20probabilistic%20setup%20enables%20uncertainty%0Aquantification%20as%20the%20online%20testing%20consists%20of%20Variational%20Inference%0Anaturally%20providing%20Certainty%20Intervals%20%28VICI%29.%20In%20this%20work%20we%20showcase%20the%0Aeffectiveness%20of%20the%20newly%20proposed%20VINDy%20method%20in%20identifying%20interpretable%0Aand%20accurate%20dynamical%20system%20for%20the%20R%5C%22ossler%20system%20with%20different%20noise%0Aintensities%20and%20sources.%20Then%20the%20performance%20of%20the%20overall%20method%20-%20named%0AVENI%2C%20VINDy%2C%20VICI%20-%20is%20tested%20on%20PDE%20benchmarks%20including%20structural%20mechanics%0Aand%20fluid%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVENI%252C%2520VINDy%252C%2520VICI%253A%2520a%2520variational%2520reduced-order%2520modeling%2520framework%2520with%250A%2520%2520uncertainty%2520quantification%26entry.906535625%3DPaolo%2520Conti%2520and%2520Jonas%2520Kneifl%2520and%2520Andrea%2520Manzoni%2520and%2520Attilio%2520Frangi%2520and%2520J%25C3%25B6rg%2520Fehr%2520and%2520Steven%2520L.%2520Brunton%2520and%2520J.%2520Nathan%2520Kutz%26entry.1292438233%3D%2520%2520The%2520simulation%2520of%2520many%2520complex%2520phenomena%2520in%2520engineering%2520and%2520science%2520requires%250Asolving%2520expensive%252C%2520high-dimensional%2520systems%2520of%2520partial%2520differential%2520equations%250A%2528PDEs%2529.%2520To%2520circumvent%2520this%252C%2520reduced-order%2520models%2520%2528ROMs%2529%2520have%2520been%2520developed%2520to%250Aspeed%2520up%2520computations.%2520However%252C%2520when%2520governing%2520equations%2520are%2520unknown%2520or%250Apartially%2520known%252C%2520typically%2520ROMs%2520lack%2520interpretability%2520and%2520reliability%2520of%2520the%250Apredicted%2520solutions.%250A%2520%2520In%2520this%2520work%2520we%2520present%2520a%2520data-driven%252C%2520non-intrusive%2520framework%2520for%2520building%250AROMs%2520where%2520the%2520latent%2520variables%2520and%2520dynamics%2520are%2520identified%2520in%2520an%2520interpretable%250Amanner%2520and%2520uncertainty%2520is%2520quantified.%2520Starting%2520from%2520a%2520limited%2520amount%2520of%250Ahigh-dimensional%252C%2520noisy%2520data%2520the%2520proposed%2520framework%2520constructs%2520an%2520efficient%2520ROM%250Aby%2520leveraging%2520variational%2520autoencoders%2520for%2520dimensionality%2520reduction%2520along%2520with%250Aa%2520newly%2520introduced%252C%2520variational%2520version%2520of%2520sparse%2520identification%2520of%2520nonlinear%250Adynamics%2520%2528SINDy%2529%252C%2520which%2520we%2520refer%2520to%2520as%2520Variational%2520Identification%2520of%2520Nonlinear%250ADynamics%2520%2528VINDy%2529.%250A%2520%2520In%2520detail%252C%2520the%2520method%2520consists%2520of%2520Variational%2520Encoding%2520of%2520Noisy%2520Inputs%2520%2528VENI%2529%250Ato%2520identify%2520the%2520distribution%2520of%2520reduced%2520coordinates.%2520Simultaneously%252C%2520we%2520learn%250Athe%2520distribution%2520of%2520the%2520coefficients%2520of%2520a%2520pre-determined%2520set%2520of%2520candidate%250Afunctions%2520by%2520VINDy.%2520Once%2520trained%2520offline%252C%2520the%2520identified%2520model%2520can%2520be%2520queried%250Afor%2520new%2520parameter%2520instances%2520and%2520new%2520initial%2520conditions%2520to%2520compute%2520the%250Acorresponding%2520full-time%2520solutions.%2520The%2520probabilistic%2520setup%2520enables%2520uncertainty%250Aquantification%2520as%2520the%2520online%2520testing%2520consists%2520of%2520Variational%2520Inference%250Anaturally%2520providing%2520Certainty%2520Intervals%2520%2528VICI%2529.%2520In%2520this%2520work%2520we%2520showcase%2520the%250Aeffectiveness%2520of%2520the%2520newly%2520proposed%2520VINDy%2520method%2520in%2520identifying%2520interpretable%250Aand%2520accurate%2520dynamical%2520system%2520for%2520the%2520R%255C%2522ossler%2520system%2520with%2520different%2520noise%250Aintensities%2520and%2520sources.%2520Then%2520the%2520performance%2520of%2520the%2520overall%2520method%2520-%2520named%250AVENI%252C%2520VINDy%252C%2520VICI%2520-%2520is%2520tested%2520on%2520PDE%2520benchmarks%2520including%2520structural%2520mechanics%250Aand%2520fluid%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VENI%2C%20VINDy%2C%20VICI%3A%20a%20variational%20reduced-order%20modeling%20framework%20with%0A%20%20uncertainty%20quantification&entry.906535625=Paolo%20Conti%20and%20Jonas%20Kneifl%20and%20Andrea%20Manzoni%20and%20Attilio%20Frangi%20and%20J%C3%B6rg%20Fehr%20and%20Steven%20L.%20Brunton%20and%20J.%20Nathan%20Kutz&entry.1292438233=%20%20The%20simulation%20of%20many%20complex%20phenomena%20in%20engineering%20and%20science%20requires%0Asolving%20expensive%2C%20high-dimensional%20systems%20of%20partial%20differential%20equations%0A%28PDEs%29.%20To%20circumvent%20this%2C%20reduced-order%20models%20%28ROMs%29%20have%20been%20developed%20to%0Aspeed%20up%20computations.%20However%2C%20when%20governing%20equations%20are%20unknown%20or%0Apartially%20known%2C%20typically%20ROMs%20lack%20interpretability%20and%20reliability%20of%20the%0Apredicted%20solutions.%0A%20%20In%20this%20work%20we%20present%20a%20data-driven%2C%20non-intrusive%20framework%20for%20building%0AROMs%20where%20the%20latent%20variables%20and%20dynamics%20are%20identified%20in%20an%20interpretable%0Amanner%20and%20uncertainty%20is%20quantified.%20Starting%20from%20a%20limited%20amount%20of%0Ahigh-dimensional%2C%20noisy%20data%20the%20proposed%20framework%20constructs%20an%20efficient%20ROM%0Aby%20leveraging%20variational%20autoencoders%20for%20dimensionality%20reduction%20along%20with%0Aa%20newly%20introduced%2C%20variational%20version%20of%20sparse%20identification%20of%20nonlinear%0Adynamics%20%28SINDy%29%2C%20which%20we%20refer%20to%20as%20Variational%20Identification%20of%20Nonlinear%0ADynamics%20%28VINDy%29.%0A%20%20In%20detail%2C%20the%20method%20consists%20of%20Variational%20Encoding%20of%20Noisy%20Inputs%20%28VENI%29%0Ato%20identify%20the%20distribution%20of%20reduced%20coordinates.%20Simultaneously%2C%20we%20learn%0Athe%20distribution%20of%20the%20coefficients%20of%20a%20pre-determined%20set%20of%20candidate%0Afunctions%20by%20VINDy.%20Once%20trained%20offline%2C%20the%20identified%20model%20can%20be%20queried%0Afor%20new%20parameter%20instances%20and%20new%20initial%20conditions%20to%20compute%20the%0Acorresponding%20full-time%20solutions.%20The%20probabilistic%20setup%20enables%20uncertainty%0Aquantification%20as%20the%20online%20testing%20consists%20of%20Variational%20Inference%0Anaturally%20providing%20Certainty%20Intervals%20%28VICI%29.%20In%20this%20work%20we%20showcase%20the%0Aeffectiveness%20of%20the%20newly%20proposed%20VINDy%20method%20in%20identifying%20interpretable%0Aand%20accurate%20dynamical%20system%20for%20the%20R%5C%22ossler%20system%20with%20different%20noise%0Aintensities%20and%20sources.%20Then%20the%20performance%20of%20the%20overall%20method%20-%20named%0AVENI%2C%20VINDy%2C%20VICI%20-%20is%20tested%20on%20PDE%20benchmarks%20including%20structural%20mechanics%0Aand%20fluid%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20905v1&entry.124074799=Read"},
{"title": "GUIDE: Guidance-based Incremental Learning with Diffusion Models", "author": "Bartosz Cywi\u0144ski and Kamil Deja and Tomasz Trzci\u0144ski and Bart\u0142omiej Twardowski and \u0141ukasz Kuci\u0144ski", "abstract": "  We introduce GUIDE, a novel continual learning approach that directs\ndiffusion models to rehearse samples at risk of being forgotten. Existing\ngenerative strategies combat catastrophic forgetting by randomly sampling\nrehearsal examples from a generative model. Such an approach contradicts\nbuffer-based approaches where sampling strategy plays an important role. We\npropose to bridge this gap by incorporating classifier guidance into the\ndiffusion process to produce rehearsal examples specifically targeting\ninformation forgotten by a continuously trained model. This approach enables\nthe generation of samples from preceding task distributions, which are more\nlikely to be misclassified in the context of recently encountered classes. Our\nexperimental results show that GUIDE significantly reduces catastrophic\nforgetting, outperforming conventional random sampling approaches and\nsurpassing recent state-of-the-art methods in continual learning with\ngenerative replay.\n", "link": "http://arxiv.org/abs/2403.03938v2", "date": "2024-05-31", "relevancy": 2.0593, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.517}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5157}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUIDE%3A%20Guidance-based%20Incremental%20Learning%20with%20Diffusion%20Models&body=Title%3A%20GUIDE%3A%20Guidance-based%20Incremental%20Learning%20with%20Diffusion%20Models%0AAuthor%3A%20Bartosz%20Cywi%C5%84ski%20and%20Kamil%20Deja%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski%20and%20%C5%81ukasz%20Kuci%C5%84ski%0AAbstract%3A%20%20%20We%20introduce%20GUIDE%2C%20a%20novel%20continual%20learning%20approach%20that%20directs%0Adiffusion%20models%20to%20rehearse%20samples%20at%20risk%20of%20being%20forgotten.%20Existing%0Agenerative%20strategies%20combat%20catastrophic%20forgetting%20by%20randomly%20sampling%0Arehearsal%20examples%20from%20a%20generative%20model.%20Such%20an%20approach%20contradicts%0Abuffer-based%20approaches%20where%20sampling%20strategy%20plays%20an%20important%20role.%20We%0Apropose%20to%20bridge%20this%20gap%20by%20incorporating%20classifier%20guidance%20into%20the%0Adiffusion%20process%20to%20produce%20rehearsal%20examples%20specifically%20targeting%0Ainformation%20forgotten%20by%20a%20continuously%20trained%20model.%20This%20approach%20enables%0Athe%20generation%20of%20samples%20from%20preceding%20task%20distributions%2C%20which%20are%20more%0Alikely%20to%20be%20misclassified%20in%20the%20context%20of%20recently%20encountered%20classes.%20Our%0Aexperimental%20results%20show%20that%20GUIDE%20significantly%20reduces%20catastrophic%0Aforgetting%2C%20outperforming%20conventional%20random%20sampling%20approaches%20and%0Asurpassing%20recent%20state-of-the-art%20methods%20in%20continual%20learning%20with%0Agenerative%20replay.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03938v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUIDE%253A%2520Guidance-based%2520Incremental%2520Learning%2520with%2520Diffusion%2520Models%26entry.906535625%3DBartosz%2520Cywi%25C5%2584ski%2520and%2520Kamil%2520Deja%2520and%2520Tomasz%2520Trzci%25C5%2584ski%2520and%2520Bart%25C5%2582omiej%2520Twardowski%2520and%2520%25C5%2581ukasz%2520Kuci%25C5%2584ski%26entry.1292438233%3D%2520%2520We%2520introduce%2520GUIDE%252C%2520a%2520novel%2520continual%2520learning%2520approach%2520that%2520directs%250Adiffusion%2520models%2520to%2520rehearse%2520samples%2520at%2520risk%2520of%2520being%2520forgotten.%2520Existing%250Agenerative%2520strategies%2520combat%2520catastrophic%2520forgetting%2520by%2520randomly%2520sampling%250Arehearsal%2520examples%2520from%2520a%2520generative%2520model.%2520Such%2520an%2520approach%2520contradicts%250Abuffer-based%2520approaches%2520where%2520sampling%2520strategy%2520plays%2520an%2520important%2520role.%2520We%250Apropose%2520to%2520bridge%2520this%2520gap%2520by%2520incorporating%2520classifier%2520guidance%2520into%2520the%250Adiffusion%2520process%2520to%2520produce%2520rehearsal%2520examples%2520specifically%2520targeting%250Ainformation%2520forgotten%2520by%2520a%2520continuously%2520trained%2520model.%2520This%2520approach%2520enables%250Athe%2520generation%2520of%2520samples%2520from%2520preceding%2520task%2520distributions%252C%2520which%2520are%2520more%250Alikely%2520to%2520be%2520misclassified%2520in%2520the%2520context%2520of%2520recently%2520encountered%2520classes.%2520Our%250Aexperimental%2520results%2520show%2520that%2520GUIDE%2520significantly%2520reduces%2520catastrophic%250Aforgetting%252C%2520outperforming%2520conventional%2520random%2520sampling%2520approaches%2520and%250Asurpassing%2520recent%2520state-of-the-art%2520methods%2520in%2520continual%2520learning%2520with%250Agenerative%2520replay.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03938v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUIDE%3A%20Guidance-based%20Incremental%20Learning%20with%20Diffusion%20Models&entry.906535625=Bartosz%20Cywi%C5%84ski%20and%20Kamil%20Deja%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski%20and%20%C5%81ukasz%20Kuci%C5%84ski&entry.1292438233=%20%20We%20introduce%20GUIDE%2C%20a%20novel%20continual%20learning%20approach%20that%20directs%0Adiffusion%20models%20to%20rehearse%20samples%20at%20risk%20of%20being%20forgotten.%20Existing%0Agenerative%20strategies%20combat%20catastrophic%20forgetting%20by%20randomly%20sampling%0Arehearsal%20examples%20from%20a%20generative%20model.%20Such%20an%20approach%20contradicts%0Abuffer-based%20approaches%20where%20sampling%20strategy%20plays%20an%20important%20role.%20We%0Apropose%20to%20bridge%20this%20gap%20by%20incorporating%20classifier%20guidance%20into%20the%0Adiffusion%20process%20to%20produce%20rehearsal%20examples%20specifically%20targeting%0Ainformation%20forgotten%20by%20a%20continuously%20trained%20model.%20This%20approach%20enables%0Athe%20generation%20of%20samples%20from%20preceding%20task%20distributions%2C%20which%20are%20more%0Alikely%20to%20be%20misclassified%20in%20the%20context%20of%20recently%20encountered%20classes.%20Our%0Aexperimental%20results%20show%20that%20GUIDE%20significantly%20reduces%20catastrophic%0Aforgetting%2C%20outperforming%20conventional%20random%20sampling%20approaches%20and%0Asurpassing%20recent%20state-of-the-art%20methods%20in%20continual%20learning%20with%0Agenerative%20replay.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03938v2&entry.124074799=Read"},
{"title": "Federated Random Forest for Partially Overlapping Clinical Data", "author": "Youngjun Park and Cord Eric Schmidt and Benedikt Marcel Batton and Anne-Christin Hauschild", "abstract": "  In the healthcare sector, a consciousness surrounding data privacy and\ncorresponding data protection regulations, as well as heterogeneous and\nnon-harmonized data, pose huge challenges to large-scale data analysis.\nMoreover, clinical data often involves partially overlapping features, as some\nobservations may be missing due to various reasons, such as differences in\nprocedures, diagnostic tests, or other recorded patient history information\nacross hospitals or institutes. To address the challenges posed by partially\noverlapping features and incomplete data in clinical datasets, a comprehensive\napproach is required. Particularly in the domain of medical data, promising\noutcomes are achieved by federated random forests whenever features align.\nHowever, for most standard algorithms, like random forest, it is essential that\nall data sets have identical parameters. Therefore, in this work the concept of\nfederated random forest is adapted to a setting with partially overlapping\nfeatures. Moreover, our research assesses the effectiveness of the newly\ndeveloped federated random forest models for partially overlapping clinical\ndata. For aggregating the federated, globally optimized model, only features\navailable locally at each site can be used. We tackled two issues in\nfederation: (i) the quantity of involved parties, (ii) the varying overlap of\nfeatures. This evaluation was conducted across three clinical datasets. The\nfederated random forest model even in cases where only a subset of features\noverlaps consistently demonstrates superior performance compared to its local\ncounterpart. This holds true across various scenarios, including datasets with\nimbalanced classes. Consequently, federated random forests for partially\noverlapped data offer a promising solution to transcend barriers in\ncollaborative research and corporate cooperation.\n", "link": "http://arxiv.org/abs/2405.20738v1", "date": "2024-05-31", "relevancy": 2.0548, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4207}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.412}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Random%20Forest%20for%20Partially%20Overlapping%20Clinical%20Data&body=Title%3A%20Federated%20Random%20Forest%20for%20Partially%20Overlapping%20Clinical%20Data%0AAuthor%3A%20Youngjun%20Park%20and%20Cord%20Eric%20Schmidt%20and%20Benedikt%20Marcel%20Batton%20and%20Anne-Christin%20Hauschild%0AAbstract%3A%20%20%20In%20the%20healthcare%20sector%2C%20a%20consciousness%20surrounding%20data%20privacy%20and%0Acorresponding%20data%20protection%20regulations%2C%20as%20well%20as%20heterogeneous%20and%0Anon-harmonized%20data%2C%20pose%20huge%20challenges%20to%20large-scale%20data%20analysis.%0AMoreover%2C%20clinical%20data%20often%20involves%20partially%20overlapping%20features%2C%20as%20some%0Aobservations%20may%20be%20missing%20due%20to%20various%20reasons%2C%20such%20as%20differences%20in%0Aprocedures%2C%20diagnostic%20tests%2C%20or%20other%20recorded%20patient%20history%20information%0Aacross%20hospitals%20or%20institutes.%20To%20address%20the%20challenges%20posed%20by%20partially%0Aoverlapping%20features%20and%20incomplete%20data%20in%20clinical%20datasets%2C%20a%20comprehensive%0Aapproach%20is%20required.%20Particularly%20in%20the%20domain%20of%20medical%20data%2C%20promising%0Aoutcomes%20are%20achieved%20by%20federated%20random%20forests%20whenever%20features%20align.%0AHowever%2C%20for%20most%20standard%20algorithms%2C%20like%20random%20forest%2C%20it%20is%20essential%20that%0Aall%20data%20sets%20have%20identical%20parameters.%20Therefore%2C%20in%20this%20work%20the%20concept%20of%0Afederated%20random%20forest%20is%20adapted%20to%20a%20setting%20with%20partially%20overlapping%0Afeatures.%20Moreover%2C%20our%20research%20assesses%20the%20effectiveness%20of%20the%20newly%0Adeveloped%20federated%20random%20forest%20models%20for%20partially%20overlapping%20clinical%0Adata.%20For%20aggregating%20the%20federated%2C%20globally%20optimized%20model%2C%20only%20features%0Aavailable%20locally%20at%20each%20site%20can%20be%20used.%20We%20tackled%20two%20issues%20in%0Afederation%3A%20%28i%29%20the%20quantity%20of%20involved%20parties%2C%20%28ii%29%20the%20varying%20overlap%20of%0Afeatures.%20This%20evaluation%20was%20conducted%20across%20three%20clinical%20datasets.%20The%0Afederated%20random%20forest%20model%20even%20in%20cases%20where%20only%20a%20subset%20of%20features%0Aoverlaps%20consistently%20demonstrates%20superior%20performance%20compared%20to%20its%20local%0Acounterpart.%20This%20holds%20true%20across%20various%20scenarios%2C%20including%20datasets%20with%0Aimbalanced%20classes.%20Consequently%2C%20federated%20random%20forests%20for%20partially%0Aoverlapped%20data%20offer%20a%20promising%20solution%20to%20transcend%20barriers%20in%0Acollaborative%20research%20and%20corporate%20cooperation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Random%2520Forest%2520for%2520Partially%2520Overlapping%2520Clinical%2520Data%26entry.906535625%3DYoungjun%2520Park%2520and%2520Cord%2520Eric%2520Schmidt%2520and%2520Benedikt%2520Marcel%2520Batton%2520and%2520Anne-Christin%2520Hauschild%26entry.1292438233%3D%2520%2520In%2520the%2520healthcare%2520sector%252C%2520a%2520consciousness%2520surrounding%2520data%2520privacy%2520and%250Acorresponding%2520data%2520protection%2520regulations%252C%2520as%2520well%2520as%2520heterogeneous%2520and%250Anon-harmonized%2520data%252C%2520pose%2520huge%2520challenges%2520to%2520large-scale%2520data%2520analysis.%250AMoreover%252C%2520clinical%2520data%2520often%2520involves%2520partially%2520overlapping%2520features%252C%2520as%2520some%250Aobservations%2520may%2520be%2520missing%2520due%2520to%2520various%2520reasons%252C%2520such%2520as%2520differences%2520in%250Aprocedures%252C%2520diagnostic%2520tests%252C%2520or%2520other%2520recorded%2520patient%2520history%2520information%250Aacross%2520hospitals%2520or%2520institutes.%2520To%2520address%2520the%2520challenges%2520posed%2520by%2520partially%250Aoverlapping%2520features%2520and%2520incomplete%2520data%2520in%2520clinical%2520datasets%252C%2520a%2520comprehensive%250Aapproach%2520is%2520required.%2520Particularly%2520in%2520the%2520domain%2520of%2520medical%2520data%252C%2520promising%250Aoutcomes%2520are%2520achieved%2520by%2520federated%2520random%2520forests%2520whenever%2520features%2520align.%250AHowever%252C%2520for%2520most%2520standard%2520algorithms%252C%2520like%2520random%2520forest%252C%2520it%2520is%2520essential%2520that%250Aall%2520data%2520sets%2520have%2520identical%2520parameters.%2520Therefore%252C%2520in%2520this%2520work%2520the%2520concept%2520of%250Afederated%2520random%2520forest%2520is%2520adapted%2520to%2520a%2520setting%2520with%2520partially%2520overlapping%250Afeatures.%2520Moreover%252C%2520our%2520research%2520assesses%2520the%2520effectiveness%2520of%2520the%2520newly%250Adeveloped%2520federated%2520random%2520forest%2520models%2520for%2520partially%2520overlapping%2520clinical%250Adata.%2520For%2520aggregating%2520the%2520federated%252C%2520globally%2520optimized%2520model%252C%2520only%2520features%250Aavailable%2520locally%2520at%2520each%2520site%2520can%2520be%2520used.%2520We%2520tackled%2520two%2520issues%2520in%250Afederation%253A%2520%2528i%2529%2520the%2520quantity%2520of%2520involved%2520parties%252C%2520%2528ii%2529%2520the%2520varying%2520overlap%2520of%250Afeatures.%2520This%2520evaluation%2520was%2520conducted%2520across%2520three%2520clinical%2520datasets.%2520The%250Afederated%2520random%2520forest%2520model%2520even%2520in%2520cases%2520where%2520only%2520a%2520subset%2520of%2520features%250Aoverlaps%2520consistently%2520demonstrates%2520superior%2520performance%2520compared%2520to%2520its%2520local%250Acounterpart.%2520This%2520holds%2520true%2520across%2520various%2520scenarios%252C%2520including%2520datasets%2520with%250Aimbalanced%2520classes.%2520Consequently%252C%2520federated%2520random%2520forests%2520for%2520partially%250Aoverlapped%2520data%2520offer%2520a%2520promising%2520solution%2520to%2520transcend%2520barriers%2520in%250Acollaborative%2520research%2520and%2520corporate%2520cooperation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Random%20Forest%20for%20Partially%20Overlapping%20Clinical%20Data&entry.906535625=Youngjun%20Park%20and%20Cord%20Eric%20Schmidt%20and%20Benedikt%20Marcel%20Batton%20and%20Anne-Christin%20Hauschild&entry.1292438233=%20%20In%20the%20healthcare%20sector%2C%20a%20consciousness%20surrounding%20data%20privacy%20and%0Acorresponding%20data%20protection%20regulations%2C%20as%20well%20as%20heterogeneous%20and%0Anon-harmonized%20data%2C%20pose%20huge%20challenges%20to%20large-scale%20data%20analysis.%0AMoreover%2C%20clinical%20data%20often%20involves%20partially%20overlapping%20features%2C%20as%20some%0Aobservations%20may%20be%20missing%20due%20to%20various%20reasons%2C%20such%20as%20differences%20in%0Aprocedures%2C%20diagnostic%20tests%2C%20or%20other%20recorded%20patient%20history%20information%0Aacross%20hospitals%20or%20institutes.%20To%20address%20the%20challenges%20posed%20by%20partially%0Aoverlapping%20features%20and%20incomplete%20data%20in%20clinical%20datasets%2C%20a%20comprehensive%0Aapproach%20is%20required.%20Particularly%20in%20the%20domain%20of%20medical%20data%2C%20promising%0Aoutcomes%20are%20achieved%20by%20federated%20random%20forests%20whenever%20features%20align.%0AHowever%2C%20for%20most%20standard%20algorithms%2C%20like%20random%20forest%2C%20it%20is%20essential%20that%0Aall%20data%20sets%20have%20identical%20parameters.%20Therefore%2C%20in%20this%20work%20the%20concept%20of%0Afederated%20random%20forest%20is%20adapted%20to%20a%20setting%20with%20partially%20overlapping%0Afeatures.%20Moreover%2C%20our%20research%20assesses%20the%20effectiveness%20of%20the%20newly%0Adeveloped%20federated%20random%20forest%20models%20for%20partially%20overlapping%20clinical%0Adata.%20For%20aggregating%20the%20federated%2C%20globally%20optimized%20model%2C%20only%20features%0Aavailable%20locally%20at%20each%20site%20can%20be%20used.%20We%20tackled%20two%20issues%20in%0Afederation%3A%20%28i%29%20the%20quantity%20of%20involved%20parties%2C%20%28ii%29%20the%20varying%20overlap%20of%0Afeatures.%20This%20evaluation%20was%20conducted%20across%20three%20clinical%20datasets.%20The%0Afederated%20random%20forest%20model%20even%20in%20cases%20where%20only%20a%20subset%20of%20features%0Aoverlaps%20consistently%20demonstrates%20superior%20performance%20compared%20to%20its%20local%0Acounterpart.%20This%20holds%20true%20across%20various%20scenarios%2C%20including%20datasets%20with%0Aimbalanced%20classes.%20Consequently%2C%20federated%20random%20forests%20for%20partially%0Aoverlapped%20data%20offer%20a%20promising%20solution%20to%20transcend%20barriers%20in%0Acollaborative%20research%20and%20corporate%20cooperation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20738v1&entry.124074799=Read"},
{"title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive\n  Transformers", "author": "Sotiris Anagnostidis and Dario Pavllo and Luca Biggio and Lorenzo Noci and Aurelien Lucchi and Thomas Hofmann", "abstract": "  Autoregressive Transformers adopted in Large Language Models (LLMs) are hard\nto scale to long sequences. Despite several works trying to reduce their\ncomputational cost, most of LLMs still adopt attention layers between all pairs\nof tokens in the sequence, thus incurring a quadratic cost. In this study, we\npresent a novel approach that dynamically prunes contextual information while\npreserving the model's expressiveness, resulting in reduced memory and\ncomputational requirements during inference. Our method employs a learnable\nmechanism that determines which uninformative tokens can be dropped from the\ncontext at any point across the generation process. By doing so, our approach\nnot only addresses performance concerns but also enhances interpretability,\nproviding valuable insight into the model's decision-making process. Our\ntechnique can be applied to existing pre-trained models through a\nstraightforward fine-tuning process, and the pruning strength can be specified\nby a sparsity parameter. Notably, our empirical findings demonstrate that we\ncan effectively prune up to 80\\% of the context without significant performance\ndegradation on downstream tasks, offering a valuable tool for mitigating\ninference costs. Our reference implementation achieves up to $2\\times$ increase\nin inference throughput and even greater memory savings.\n", "link": "http://arxiv.org/abs/2305.15805v3", "date": "2024-05-31", "relevancy": 2.053, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.555}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5201}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Context%20Pruning%20for%20Efficient%20and%20Interpretable%20Autoregressive%0A%20%20Transformers&body=Title%3A%20Dynamic%20Context%20Pruning%20for%20Efficient%20and%20Interpretable%20Autoregressive%0A%20%20Transformers%0AAuthor%3A%20Sotiris%20Anagnostidis%20and%20Dario%20Pavllo%20and%20Luca%20Biggio%20and%20Lorenzo%20Noci%20and%20Aurelien%20Lucchi%20and%20Thomas%20Hofmann%0AAbstract%3A%20%20%20Autoregressive%20Transformers%20adopted%20in%20Large%20Language%20Models%20%28LLMs%29%20are%20hard%0Ato%20scale%20to%20long%20sequences.%20Despite%20several%20works%20trying%20to%20reduce%20their%0Acomputational%20cost%2C%20most%20of%20LLMs%20still%20adopt%20attention%20layers%20between%20all%20pairs%0Aof%20tokens%20in%20the%20sequence%2C%20thus%20incurring%20a%20quadratic%20cost.%20In%20this%20study%2C%20we%0Apresent%20a%20novel%20approach%20that%20dynamically%20prunes%20contextual%20information%20while%0Apreserving%20the%20model%27s%20expressiveness%2C%20resulting%20in%20reduced%20memory%20and%0Acomputational%20requirements%20during%20inference.%20Our%20method%20employs%20a%20learnable%0Amechanism%20that%20determines%20which%20uninformative%20tokens%20can%20be%20dropped%20from%20the%0Acontext%20at%20any%20point%20across%20the%20generation%20process.%20By%20doing%20so%2C%20our%20approach%0Anot%20only%20addresses%20performance%20concerns%20but%20also%20enhances%20interpretability%2C%0Aproviding%20valuable%20insight%20into%20the%20model%27s%20decision-making%20process.%20Our%0Atechnique%20can%20be%20applied%20to%20existing%20pre-trained%20models%20through%20a%0Astraightforward%20fine-tuning%20process%2C%20and%20the%20pruning%20strength%20can%20be%20specified%0Aby%20a%20sparsity%20parameter.%20Notably%2C%20our%20empirical%20findings%20demonstrate%20that%20we%0Acan%20effectively%20prune%20up%20to%2080%5C%25%20of%20the%20context%20without%20significant%20performance%0Adegradation%20on%20downstream%20tasks%2C%20offering%20a%20valuable%20tool%20for%20mitigating%0Ainference%20costs.%20Our%20reference%20implementation%20achieves%20up%20to%20%242%5Ctimes%24%20increase%0Ain%20inference%20throughput%20and%20even%20greater%20memory%20savings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15805v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Context%2520Pruning%2520for%2520Efficient%2520and%2520Interpretable%2520Autoregressive%250A%2520%2520Transformers%26entry.906535625%3DSotiris%2520Anagnostidis%2520and%2520Dario%2520Pavllo%2520and%2520Luca%2520Biggio%2520and%2520Lorenzo%2520Noci%2520and%2520Aurelien%2520Lucchi%2520and%2520Thomas%2520Hofmann%26entry.1292438233%3D%2520%2520Autoregressive%2520Transformers%2520adopted%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520hard%250Ato%2520scale%2520to%2520long%2520sequences.%2520Despite%2520several%2520works%2520trying%2520to%2520reduce%2520their%250Acomputational%2520cost%252C%2520most%2520of%2520LLMs%2520still%2520adopt%2520attention%2520layers%2520between%2520all%2520pairs%250Aof%2520tokens%2520in%2520the%2520sequence%252C%2520thus%2520incurring%2520a%2520quadratic%2520cost.%2520In%2520this%2520study%252C%2520we%250Apresent%2520a%2520novel%2520approach%2520that%2520dynamically%2520prunes%2520contextual%2520information%2520while%250Apreserving%2520the%2520model%2527s%2520expressiveness%252C%2520resulting%2520in%2520reduced%2520memory%2520and%250Acomputational%2520requirements%2520during%2520inference.%2520Our%2520method%2520employs%2520a%2520learnable%250Amechanism%2520that%2520determines%2520which%2520uninformative%2520tokens%2520can%2520be%2520dropped%2520from%2520the%250Acontext%2520at%2520any%2520point%2520across%2520the%2520generation%2520process.%2520By%2520doing%2520so%252C%2520our%2520approach%250Anot%2520only%2520addresses%2520performance%2520concerns%2520but%2520also%2520enhances%2520interpretability%252C%250Aproviding%2520valuable%2520insight%2520into%2520the%2520model%2527s%2520decision-making%2520process.%2520Our%250Atechnique%2520can%2520be%2520applied%2520to%2520existing%2520pre-trained%2520models%2520through%2520a%250Astraightforward%2520fine-tuning%2520process%252C%2520and%2520the%2520pruning%2520strength%2520can%2520be%2520specified%250Aby%2520a%2520sparsity%2520parameter.%2520Notably%252C%2520our%2520empirical%2520findings%2520demonstrate%2520that%2520we%250Acan%2520effectively%2520prune%2520up%2520to%252080%255C%2525%2520of%2520the%2520context%2520without%2520significant%2520performance%250Adegradation%2520on%2520downstream%2520tasks%252C%2520offering%2520a%2520valuable%2520tool%2520for%2520mitigating%250Ainference%2520costs.%2520Our%2520reference%2520implementation%2520achieves%2520up%2520to%2520%25242%255Ctimes%2524%2520increase%250Ain%2520inference%2520throughput%2520and%2520even%2520greater%2520memory%2520savings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15805v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Context%20Pruning%20for%20Efficient%20and%20Interpretable%20Autoregressive%0A%20%20Transformers&entry.906535625=Sotiris%20Anagnostidis%20and%20Dario%20Pavllo%20and%20Luca%20Biggio%20and%20Lorenzo%20Noci%20and%20Aurelien%20Lucchi%20and%20Thomas%20Hofmann&entry.1292438233=%20%20Autoregressive%20Transformers%20adopted%20in%20Large%20Language%20Models%20%28LLMs%29%20are%20hard%0Ato%20scale%20to%20long%20sequences.%20Despite%20several%20works%20trying%20to%20reduce%20their%0Acomputational%20cost%2C%20most%20of%20LLMs%20still%20adopt%20attention%20layers%20between%20all%20pairs%0Aof%20tokens%20in%20the%20sequence%2C%20thus%20incurring%20a%20quadratic%20cost.%20In%20this%20study%2C%20we%0Apresent%20a%20novel%20approach%20that%20dynamically%20prunes%20contextual%20information%20while%0Apreserving%20the%20model%27s%20expressiveness%2C%20resulting%20in%20reduced%20memory%20and%0Acomputational%20requirements%20during%20inference.%20Our%20method%20employs%20a%20learnable%0Amechanism%20that%20determines%20which%20uninformative%20tokens%20can%20be%20dropped%20from%20the%0Acontext%20at%20any%20point%20across%20the%20generation%20process.%20By%20doing%20so%2C%20our%20approach%0Anot%20only%20addresses%20performance%20concerns%20but%20also%20enhances%20interpretability%2C%0Aproviding%20valuable%20insight%20into%20the%20model%27s%20decision-making%20process.%20Our%0Atechnique%20can%20be%20applied%20to%20existing%20pre-trained%20models%20through%20a%0Astraightforward%20fine-tuning%20process%2C%20and%20the%20pruning%20strength%20can%20be%20specified%0Aby%20a%20sparsity%20parameter.%20Notably%2C%20our%20empirical%20findings%20demonstrate%20that%20we%0Acan%20effectively%20prune%20up%20to%2080%5C%25%20of%20the%20context%20without%20significant%20performance%0Adegradation%20on%20downstream%20tasks%2C%20offering%20a%20valuable%20tool%20for%20mitigating%0Ainference%20costs.%20Our%20reference%20implementation%20achieves%20up%20to%20%242%5Ctimes%24%20increase%0Ain%20inference%20throughput%20and%20even%20greater%20memory%20savings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15805v3&entry.124074799=Read"},
{"title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with\n  Adaptive Adversarial Training", "author": "Feiteng Fang and Yuelin Bai and Shiwen Ni and Min Yang and Xiaojun Chen and Ruifeng Xu", "abstract": "  Large Language Models (LLMs) exhibit substantial capabilities yet encounter\nchallenges, including hallucination, outdated knowledge, and untraceable\nreasoning processes. Retrieval-augmented generation (RAG) has emerged as a\npromising solution, integrating knowledge from external databases to mitigate\nthese challenges. However, inappropriate retrieved passages can potentially\nhinder the LLMs' capacity to generate comprehensive and high-quality responses.\nPrior RAG studies on the robustness of retrieval noises often confine\nthemselves to a limited set of noise types, deviating from real-world retrieval\nenvironments and limiting practical applicability. In this study, we initially\ninvestigate retrieval noises and categorize them into three distinct types,\nreflecting real-world environments. We analyze the impact of these various\nretrieval noises on the robustness of LLMs. Subsequently, we propose a novel\nRAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT).\nRAAT leverages adaptive adversarial training to dynamically adjust the model's\ntraining process in response to retrieval noises. Concurrently, it employs\nmulti-task learning to ensure the model's capacity to internally recognize\nnoisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model\ntrained using RAAT exhibits significant improvements in F1 and EM scores under\ndiverse noise conditions. For reproducibility, we release our code and data at:\nhttps://github.com/calubkk/RAAT.\n", "link": "http://arxiv.org/abs/2405.20978v1", "date": "2024-05-31", "relevancy": 2.0433, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5351}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5148}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Noise%20Robustness%20of%20Retrieval-Augmented%20Language%20Models%20with%0A%20%20Adaptive%20Adversarial%20Training&body=Title%3A%20Enhancing%20Noise%20Robustness%20of%20Retrieval-Augmented%20Language%20Models%20with%0A%20%20Adaptive%20Adversarial%20Training%0AAuthor%3A%20Feiteng%20Fang%20and%20Yuelin%20Bai%20and%20Shiwen%20Ni%20and%20Min%20Yang%20and%20Xiaojun%20Chen%20and%20Ruifeng%20Xu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20substantial%20capabilities%20yet%20encounter%0Achallenges%2C%20including%20hallucination%2C%20outdated%20knowledge%2C%20and%20untraceable%0Areasoning%20processes.%20Retrieval-augmented%20generation%20%28RAG%29%20has%20emerged%20as%20a%0Apromising%20solution%2C%20integrating%20knowledge%20from%20external%20databases%20to%20mitigate%0Athese%20challenges.%20However%2C%20inappropriate%20retrieved%20passages%20can%20potentially%0Ahinder%20the%20LLMs%27%20capacity%20to%20generate%20comprehensive%20and%20high-quality%20responses.%0APrior%20RAG%20studies%20on%20the%20robustness%20of%20retrieval%20noises%20often%20confine%0Athemselves%20to%20a%20limited%20set%20of%20noise%20types%2C%20deviating%20from%20real-world%20retrieval%0Aenvironments%20and%20limiting%20practical%20applicability.%20In%20this%20study%2C%20we%20initially%0Ainvestigate%20retrieval%20noises%20and%20categorize%20them%20into%20three%20distinct%20types%2C%0Areflecting%20real-world%20environments.%20We%20analyze%20the%20impact%20of%20these%20various%0Aretrieval%20noises%20on%20the%20robustness%20of%20LLMs.%20Subsequently%2C%20we%20propose%20a%20novel%0ARAG%20approach%20known%20as%20Retrieval-augmented%20Adaptive%20Adversarial%20Training%20%28RAAT%29.%0ARAAT%20leverages%20adaptive%20adversarial%20training%20to%20dynamically%20adjust%20the%20model%27s%0Atraining%20process%20in%20response%20to%20retrieval%20noises.%20Concurrently%2C%20it%20employs%0Amulti-task%20learning%20to%20ensure%20the%20model%27s%20capacity%20to%20internally%20recognize%0Anoisy%20contexts.%20Extensive%20experiments%20demonstrate%20that%20the%20LLaMA-2%207B%20model%0Atrained%20using%20RAAT%20exhibits%20significant%20improvements%20in%20F1%20and%20EM%20scores%20under%0Adiverse%20noise%20conditions.%20For%20reproducibility%2C%20we%20release%20our%20code%20and%20data%20at%3A%0Ahttps%3A//github.com/calubkk/RAAT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Noise%2520Robustness%2520of%2520Retrieval-Augmented%2520Language%2520Models%2520with%250A%2520%2520Adaptive%2520Adversarial%2520Training%26entry.906535625%3DFeiteng%2520Fang%2520and%2520Yuelin%2520Bai%2520and%2520Shiwen%2520Ni%2520and%2520Min%2520Yang%2520and%2520Xiaojun%2520Chen%2520and%2520Ruifeng%2520Xu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520exhibit%2520substantial%2520capabilities%2520yet%2520encounter%250Achallenges%252C%2520including%2520hallucination%252C%2520outdated%2520knowledge%252C%2520and%2520untraceable%250Areasoning%2520processes.%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520has%2520emerged%2520as%2520a%250Apromising%2520solution%252C%2520integrating%2520knowledge%2520from%2520external%2520databases%2520to%2520mitigate%250Athese%2520challenges.%2520However%252C%2520inappropriate%2520retrieved%2520passages%2520can%2520potentially%250Ahinder%2520the%2520LLMs%2527%2520capacity%2520to%2520generate%2520comprehensive%2520and%2520high-quality%2520responses.%250APrior%2520RAG%2520studies%2520on%2520the%2520robustness%2520of%2520retrieval%2520noises%2520often%2520confine%250Athemselves%2520to%2520a%2520limited%2520set%2520of%2520noise%2520types%252C%2520deviating%2520from%2520real-world%2520retrieval%250Aenvironments%2520and%2520limiting%2520practical%2520applicability.%2520In%2520this%2520study%252C%2520we%2520initially%250Ainvestigate%2520retrieval%2520noises%2520and%2520categorize%2520them%2520into%2520three%2520distinct%2520types%252C%250Areflecting%2520real-world%2520environments.%2520We%2520analyze%2520the%2520impact%2520of%2520these%2520various%250Aretrieval%2520noises%2520on%2520the%2520robustness%2520of%2520LLMs.%2520Subsequently%252C%2520we%2520propose%2520a%2520novel%250ARAG%2520approach%2520known%2520as%2520Retrieval-augmented%2520Adaptive%2520Adversarial%2520Training%2520%2528RAAT%2529.%250ARAAT%2520leverages%2520adaptive%2520adversarial%2520training%2520to%2520dynamically%2520adjust%2520the%2520model%2527s%250Atraining%2520process%2520in%2520response%2520to%2520retrieval%2520noises.%2520Concurrently%252C%2520it%2520employs%250Amulti-task%2520learning%2520to%2520ensure%2520the%2520model%2527s%2520capacity%2520to%2520internally%2520recognize%250Anoisy%2520contexts.%2520Extensive%2520experiments%2520demonstrate%2520that%2520the%2520LLaMA-2%25207B%2520model%250Atrained%2520using%2520RAAT%2520exhibits%2520significant%2520improvements%2520in%2520F1%2520and%2520EM%2520scores%2520under%250Adiverse%2520noise%2520conditions.%2520For%2520reproducibility%252C%2520we%2520release%2520our%2520code%2520and%2520data%2520at%253A%250Ahttps%253A//github.com/calubkk/RAAT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Noise%20Robustness%20of%20Retrieval-Augmented%20Language%20Models%20with%0A%20%20Adaptive%20Adversarial%20Training&entry.906535625=Feiteng%20Fang%20and%20Yuelin%20Bai%20and%20Shiwen%20Ni%20and%20Min%20Yang%20and%20Xiaojun%20Chen%20and%20Ruifeng%20Xu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20exhibit%20substantial%20capabilities%20yet%20encounter%0Achallenges%2C%20including%20hallucination%2C%20outdated%20knowledge%2C%20and%20untraceable%0Areasoning%20processes.%20Retrieval-augmented%20generation%20%28RAG%29%20has%20emerged%20as%20a%0Apromising%20solution%2C%20integrating%20knowledge%20from%20external%20databases%20to%20mitigate%0Athese%20challenges.%20However%2C%20inappropriate%20retrieved%20passages%20can%20potentially%0Ahinder%20the%20LLMs%27%20capacity%20to%20generate%20comprehensive%20and%20high-quality%20responses.%0APrior%20RAG%20studies%20on%20the%20robustness%20of%20retrieval%20noises%20often%20confine%0Athemselves%20to%20a%20limited%20set%20of%20noise%20types%2C%20deviating%20from%20real-world%20retrieval%0Aenvironments%20and%20limiting%20practical%20applicability.%20In%20this%20study%2C%20we%20initially%0Ainvestigate%20retrieval%20noises%20and%20categorize%20them%20into%20three%20distinct%20types%2C%0Areflecting%20real-world%20environments.%20We%20analyze%20the%20impact%20of%20these%20various%0Aretrieval%20noises%20on%20the%20robustness%20of%20LLMs.%20Subsequently%2C%20we%20propose%20a%20novel%0ARAG%20approach%20known%20as%20Retrieval-augmented%20Adaptive%20Adversarial%20Training%20%28RAAT%29.%0ARAAT%20leverages%20adaptive%20adversarial%20training%20to%20dynamically%20adjust%20the%20model%27s%0Atraining%20process%20in%20response%20to%20retrieval%20noises.%20Concurrently%2C%20it%20employs%0Amulti-task%20learning%20to%20ensure%20the%20model%27s%20capacity%20to%20internally%20recognize%0Anoisy%20contexts.%20Extensive%20experiments%20demonstrate%20that%20the%20LLaMA-2%207B%20model%0Atrained%20using%20RAAT%20exhibits%20significant%20improvements%20in%20F1%20and%20EM%20scores%20under%0Adiverse%20noise%20conditions.%20For%20reproducibility%2C%20we%20release%20our%20code%20and%20data%20at%3A%0Ahttps%3A//github.com/calubkk/RAAT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20978v1&entry.124074799=Read"},
{"title": "Self-Augmented Preference Optimization: Off-Policy Paradigms for\n  Language Model Alignment", "author": "Yueqin Yin and Zhendong Wang and Yujia Xie and Weizhu Chen and Mingyuan Zhou", "abstract": "  Traditional language model alignment methods, such as Direct Preference\nOptimization (DPO), are limited by their dependence on static, pre-collected\npaired preference data, which hampers their adaptability and practical\napplicability. To overcome this limitation, we introduce Self-Augmented\nPreference Optimization (SAPO), an effective and scalable training paradigm\nthat does not require existing paired data. Building on the self-play concept,\nwhich autonomously generates negative responses, we further incorporate an\noff-policy learning pipeline to enhance data exploration and exploitation.\nSpecifically, we employ an Exponential Moving Average (EMA) model in\nconjunction with a replay buffer to enable dynamic updates of response\nsegments, effectively integrating real-time feedback with insights from\nhistorical data. Our comprehensive evaluations of the LLaMA3-8B and Mistral-7B\nmodels across benchmarks, including the Open LLM Leaderboard, IFEval,\nAlpacaEval 2.0, and MT-Bench, demonstrate that SAPO matches or surpasses\nestablished offline contrastive baselines, such as DPO and Odds Ratio\nPreference Optimization, and outperforms offline self-play methods like SPIN.\nOur code is available at https://github.com/yinyueqin/SAPO\n", "link": "http://arxiv.org/abs/2405.20830v1", "date": "2024-05-31", "relevancy": 2.0417, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5229}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5049}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Augmented%20Preference%20Optimization%3A%20Off-Policy%20Paradigms%20for%0A%20%20Language%20Model%20Alignment&body=Title%3A%20Self-Augmented%20Preference%20Optimization%3A%20Off-Policy%20Paradigms%20for%0A%20%20Language%20Model%20Alignment%0AAuthor%3A%20Yueqin%20Yin%20and%20Zhendong%20Wang%20and%20Yujia%20Xie%20and%20Weizhu%20Chen%20and%20Mingyuan%20Zhou%0AAbstract%3A%20%20%20Traditional%20language%20model%20alignment%20methods%2C%20such%20as%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20are%20limited%20by%20their%20dependence%20on%20static%2C%20pre-collected%0Apaired%20preference%20data%2C%20which%20hampers%20their%20adaptability%20and%20practical%0Aapplicability.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20Self-Augmented%0APreference%20Optimization%20%28SAPO%29%2C%20an%20effective%20and%20scalable%20training%20paradigm%0Athat%20does%20not%20require%20existing%20paired%20data.%20Building%20on%20the%20self-play%20concept%2C%0Awhich%20autonomously%20generates%20negative%20responses%2C%20we%20further%20incorporate%20an%0Aoff-policy%20learning%20pipeline%20to%20enhance%20data%20exploration%20and%20exploitation.%0ASpecifically%2C%20we%20employ%20an%20Exponential%20Moving%20Average%20%28EMA%29%20model%20in%0Aconjunction%20with%20a%20replay%20buffer%20to%20enable%20dynamic%20updates%20of%20response%0Asegments%2C%20effectively%20integrating%20real-time%20feedback%20with%20insights%20from%0Ahistorical%20data.%20Our%20comprehensive%20evaluations%20of%20the%20LLaMA3-8B%20and%20Mistral-7B%0Amodels%20across%20benchmarks%2C%20including%20the%20Open%20LLM%20Leaderboard%2C%20IFEval%2C%0AAlpacaEval%202.0%2C%20and%20MT-Bench%2C%20demonstrate%20that%20SAPO%20matches%20or%20surpasses%0Aestablished%20offline%20contrastive%20baselines%2C%20such%20as%20DPO%20and%20Odds%20Ratio%0APreference%20Optimization%2C%20and%20outperforms%20offline%20self-play%20methods%20like%20SPIN.%0AOur%20code%20is%20available%20at%20https%3A//github.com/yinyueqin/SAPO%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Augmented%2520Preference%2520Optimization%253A%2520Off-Policy%2520Paradigms%2520for%250A%2520%2520Language%2520Model%2520Alignment%26entry.906535625%3DYueqin%2520Yin%2520and%2520Zhendong%2520Wang%2520and%2520Yujia%2520Xie%2520and%2520Weizhu%2520Chen%2520and%2520Mingyuan%2520Zhou%26entry.1292438233%3D%2520%2520Traditional%2520language%2520model%2520alignment%2520methods%252C%2520such%2520as%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%252C%2520are%2520limited%2520by%2520their%2520dependence%2520on%2520static%252C%2520pre-collected%250Apaired%2520preference%2520data%252C%2520which%2520hampers%2520their%2520adaptability%2520and%2520practical%250Aapplicability.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520Self-Augmented%250APreference%2520Optimization%2520%2528SAPO%2529%252C%2520an%2520effective%2520and%2520scalable%2520training%2520paradigm%250Athat%2520does%2520not%2520require%2520existing%2520paired%2520data.%2520Building%2520on%2520the%2520self-play%2520concept%252C%250Awhich%2520autonomously%2520generates%2520negative%2520responses%252C%2520we%2520further%2520incorporate%2520an%250Aoff-policy%2520learning%2520pipeline%2520to%2520enhance%2520data%2520exploration%2520and%2520exploitation.%250ASpecifically%252C%2520we%2520employ%2520an%2520Exponential%2520Moving%2520Average%2520%2528EMA%2529%2520model%2520in%250Aconjunction%2520with%2520a%2520replay%2520buffer%2520to%2520enable%2520dynamic%2520updates%2520of%2520response%250Asegments%252C%2520effectively%2520integrating%2520real-time%2520feedback%2520with%2520insights%2520from%250Ahistorical%2520data.%2520Our%2520comprehensive%2520evaluations%2520of%2520the%2520LLaMA3-8B%2520and%2520Mistral-7B%250Amodels%2520across%2520benchmarks%252C%2520including%2520the%2520Open%2520LLM%2520Leaderboard%252C%2520IFEval%252C%250AAlpacaEval%25202.0%252C%2520and%2520MT-Bench%252C%2520demonstrate%2520that%2520SAPO%2520matches%2520or%2520surpasses%250Aestablished%2520offline%2520contrastive%2520baselines%252C%2520such%2520as%2520DPO%2520and%2520Odds%2520Ratio%250APreference%2520Optimization%252C%2520and%2520outperforms%2520offline%2520self-play%2520methods%2520like%2520SPIN.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/yinyueqin/SAPO%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Augmented%20Preference%20Optimization%3A%20Off-Policy%20Paradigms%20for%0A%20%20Language%20Model%20Alignment&entry.906535625=Yueqin%20Yin%20and%20Zhendong%20Wang%20and%20Yujia%20Xie%20and%20Weizhu%20Chen%20and%20Mingyuan%20Zhou&entry.1292438233=%20%20Traditional%20language%20model%20alignment%20methods%2C%20such%20as%20Direct%20Preference%0AOptimization%20%28DPO%29%2C%20are%20limited%20by%20their%20dependence%20on%20static%2C%20pre-collected%0Apaired%20preference%20data%2C%20which%20hampers%20their%20adaptability%20and%20practical%0Aapplicability.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20Self-Augmented%0APreference%20Optimization%20%28SAPO%29%2C%20an%20effective%20and%20scalable%20training%20paradigm%0Athat%20does%20not%20require%20existing%20paired%20data.%20Building%20on%20the%20self-play%20concept%2C%0Awhich%20autonomously%20generates%20negative%20responses%2C%20we%20further%20incorporate%20an%0Aoff-policy%20learning%20pipeline%20to%20enhance%20data%20exploration%20and%20exploitation.%0ASpecifically%2C%20we%20employ%20an%20Exponential%20Moving%20Average%20%28EMA%29%20model%20in%0Aconjunction%20with%20a%20replay%20buffer%20to%20enable%20dynamic%20updates%20of%20response%0Asegments%2C%20effectively%20integrating%20real-time%20feedback%20with%20insights%20from%0Ahistorical%20data.%20Our%20comprehensive%20evaluations%20of%20the%20LLaMA3-8B%20and%20Mistral-7B%0Amodels%20across%20benchmarks%2C%20including%20the%20Open%20LLM%20Leaderboard%2C%20IFEval%2C%0AAlpacaEval%202.0%2C%20and%20MT-Bench%2C%20demonstrate%20that%20SAPO%20matches%20or%20surpasses%0Aestablished%20offline%20contrastive%20baselines%2C%20such%20as%20DPO%20and%20Odds%20Ratio%0APreference%20Optimization%2C%20and%20outperforms%20offline%20self-play%20methods%20like%20SPIN.%0AOur%20code%20is%20available%20at%20https%3A//github.com/yinyueqin/SAPO%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20830v1&entry.124074799=Read"},
{"title": "Accuracy Booster: Enabling 4-bit Fixed-point Arithmetic for DNN Training", "author": "Simla Burcu Harma and Ayan Chakraborty and Nicholas Sperry and Babak Falsafi and Martin Jaggi and Yunho Oh", "abstract": "  The unprecedented demand for computing resources to train DNN models has led\nto a search for minimal numerical encoding. Recent state-of-the-art (SOTA)\nproposals advocate for multi-level scaled narrow bitwidth numerical formats. In\nthis paper, we show that single-level scaling is sufficient to maintain\ntraining accuracy while maximizing arithmetic density. We identify a previously\nproposed single-level scaled format for 8-bit training, Hybrid Block Floating\nPoint (HBFP), as the optimal candidate to minimize. We perform a full-scale\nexploration of the HBFP design space using mathematical tools to study the\ninterplay among various parameters and identify opportunities for even smaller\nencodings across layers and epochs. Based on our findings, we propose Accuracy\nBooster, a mixed-mantissa HBFP technique that uses 4-bit mantissas for over 99%\nof all arithmetic operations in training and 6-bit mantissas only in the last\nepoch and first/last layers. We show Accuracy Booster enables increasing\narithmetic density over all other SOTA formats by at least 2.3x while achieving\nstate-of-the-art accuracies in 4-bit training.\n", "link": "http://arxiv.org/abs/2211.10737v4", "date": "2024-05-31", "relevancy": 2.0403, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.524}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5033}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accuracy%20Booster%3A%20Enabling%204-bit%20Fixed-point%20Arithmetic%20for%20DNN%20Training&body=Title%3A%20Accuracy%20Booster%3A%20Enabling%204-bit%20Fixed-point%20Arithmetic%20for%20DNN%20Training%0AAuthor%3A%20Simla%20Burcu%20Harma%20and%20Ayan%20Chakraborty%20and%20Nicholas%20Sperry%20and%20Babak%20Falsafi%20and%20Martin%20Jaggi%20and%20Yunho%20Oh%0AAbstract%3A%20%20%20The%20unprecedented%20demand%20for%20computing%20resources%20to%20train%20DNN%20models%20has%20led%0Ato%20a%20search%20for%20minimal%20numerical%20encoding.%20Recent%20state-of-the-art%20%28SOTA%29%0Aproposals%20advocate%20for%20multi-level%20scaled%20narrow%20bitwidth%20numerical%20formats.%20In%0Athis%20paper%2C%20we%20show%20that%20single-level%20scaling%20is%20sufficient%20to%20maintain%0Atraining%20accuracy%20while%20maximizing%20arithmetic%20density.%20We%20identify%20a%20previously%0Aproposed%20single-level%20scaled%20format%20for%208-bit%20training%2C%20Hybrid%20Block%20Floating%0APoint%20%28HBFP%29%2C%20as%20the%20optimal%20candidate%20to%20minimize.%20We%20perform%20a%20full-scale%0Aexploration%20of%20the%20HBFP%20design%20space%20using%20mathematical%20tools%20to%20study%20the%0Ainterplay%20among%20various%20parameters%20and%20identify%20opportunities%20for%20even%20smaller%0Aencodings%20across%20layers%20and%20epochs.%20Based%20on%20our%20findings%2C%20we%20propose%20Accuracy%0ABooster%2C%20a%20mixed-mantissa%20HBFP%20technique%20that%20uses%204-bit%20mantissas%20for%20over%2099%25%0Aof%20all%20arithmetic%20operations%20in%20training%20and%206-bit%20mantissas%20only%20in%20the%20last%0Aepoch%20and%20first/last%20layers.%20We%20show%20Accuracy%20Booster%20enables%20increasing%0Aarithmetic%20density%20over%20all%20other%20SOTA%20formats%20by%20at%20least%202.3x%20while%20achieving%0Astate-of-the-art%20accuracies%20in%204-bit%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.10737v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccuracy%2520Booster%253A%2520Enabling%25204-bit%2520Fixed-point%2520Arithmetic%2520for%2520DNN%2520Training%26entry.906535625%3DSimla%2520Burcu%2520Harma%2520and%2520Ayan%2520Chakraborty%2520and%2520Nicholas%2520Sperry%2520and%2520Babak%2520Falsafi%2520and%2520Martin%2520Jaggi%2520and%2520Yunho%2520Oh%26entry.1292438233%3D%2520%2520The%2520unprecedented%2520demand%2520for%2520computing%2520resources%2520to%2520train%2520DNN%2520models%2520has%2520led%250Ato%2520a%2520search%2520for%2520minimal%2520numerical%2520encoding.%2520Recent%2520state-of-the-art%2520%2528SOTA%2529%250Aproposals%2520advocate%2520for%2520multi-level%2520scaled%2520narrow%2520bitwidth%2520numerical%2520formats.%2520In%250Athis%2520paper%252C%2520we%2520show%2520that%2520single-level%2520scaling%2520is%2520sufficient%2520to%2520maintain%250Atraining%2520accuracy%2520while%2520maximizing%2520arithmetic%2520density.%2520We%2520identify%2520a%2520previously%250Aproposed%2520single-level%2520scaled%2520format%2520for%25208-bit%2520training%252C%2520Hybrid%2520Block%2520Floating%250APoint%2520%2528HBFP%2529%252C%2520as%2520the%2520optimal%2520candidate%2520to%2520minimize.%2520We%2520perform%2520a%2520full-scale%250Aexploration%2520of%2520the%2520HBFP%2520design%2520space%2520using%2520mathematical%2520tools%2520to%2520study%2520the%250Ainterplay%2520among%2520various%2520parameters%2520and%2520identify%2520opportunities%2520for%2520even%2520smaller%250Aencodings%2520across%2520layers%2520and%2520epochs.%2520Based%2520on%2520our%2520findings%252C%2520we%2520propose%2520Accuracy%250ABooster%252C%2520a%2520mixed-mantissa%2520HBFP%2520technique%2520that%2520uses%25204-bit%2520mantissas%2520for%2520over%252099%2525%250Aof%2520all%2520arithmetic%2520operations%2520in%2520training%2520and%25206-bit%2520mantissas%2520only%2520in%2520the%2520last%250Aepoch%2520and%2520first/last%2520layers.%2520We%2520show%2520Accuracy%2520Booster%2520enables%2520increasing%250Aarithmetic%2520density%2520over%2520all%2520other%2520SOTA%2520formats%2520by%2520at%2520least%25202.3x%2520while%2520achieving%250Astate-of-the-art%2520accuracies%2520in%25204-bit%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.10737v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accuracy%20Booster%3A%20Enabling%204-bit%20Fixed-point%20Arithmetic%20for%20DNN%20Training&entry.906535625=Simla%20Burcu%20Harma%20and%20Ayan%20Chakraborty%20and%20Nicholas%20Sperry%20and%20Babak%20Falsafi%20and%20Martin%20Jaggi%20and%20Yunho%20Oh&entry.1292438233=%20%20The%20unprecedented%20demand%20for%20computing%20resources%20to%20train%20DNN%20models%20has%20led%0Ato%20a%20search%20for%20minimal%20numerical%20encoding.%20Recent%20state-of-the-art%20%28SOTA%29%0Aproposals%20advocate%20for%20multi-level%20scaled%20narrow%20bitwidth%20numerical%20formats.%20In%0Athis%20paper%2C%20we%20show%20that%20single-level%20scaling%20is%20sufficient%20to%20maintain%0Atraining%20accuracy%20while%20maximizing%20arithmetic%20density.%20We%20identify%20a%20previously%0Aproposed%20single-level%20scaled%20format%20for%208-bit%20training%2C%20Hybrid%20Block%20Floating%0APoint%20%28HBFP%29%2C%20as%20the%20optimal%20candidate%20to%20minimize.%20We%20perform%20a%20full-scale%0Aexploration%20of%20the%20HBFP%20design%20space%20using%20mathematical%20tools%20to%20study%20the%0Ainterplay%20among%20various%20parameters%20and%20identify%20opportunities%20for%20even%20smaller%0Aencodings%20across%20layers%20and%20epochs.%20Based%20on%20our%20findings%2C%20we%20propose%20Accuracy%0ABooster%2C%20a%20mixed-mantissa%20HBFP%20technique%20that%20uses%204-bit%20mantissas%20for%20over%2099%25%0Aof%20all%20arithmetic%20operations%20in%20training%20and%206-bit%20mantissas%20only%20in%20the%20last%0Aepoch%20and%20first/last%20layers.%20We%20show%20Accuracy%20Booster%20enables%20increasing%0Aarithmetic%20density%20over%20all%20other%20SOTA%20formats%20by%20at%20least%202.3x%20while%20achieving%0Astate-of-the-art%20accuracies%20in%204-bit%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.10737v4&entry.124074799=Read"},
{"title": "Investigating Calibration and Corruption Robustness of Post-hoc Pruned\n  Perception CNNs: An Image Classification Benchmark Study", "author": "Pallavi Mitra and Gesina Schwalbe and Nadja Klein", "abstract": "  Convolutional Neural Networks (CNNs) have achieved state-of-the-art\nperformance in many computer vision tasks. However, high computational and\nstorage demands hinder their deployment into resource-constrained environments,\nsuch as embedded devices. Model pruning helps to meet these restrictions by\nreducing the model size, while maintaining superior performance. Meanwhile,\nsafety-critical applications pose more than just resource and performance\nconstraints. In particular, predictions must not be overly confident, i.e.,\nprovide properly calibrated uncertainty estimations (proper uncertainty\ncalibration), and CNNs must be robust against corruptions like naturally\noccurring input perturbations (natural corruption robustness). This work\ninvestigates the important trade-off between uncertainty calibration, natural\ncorruption robustness, and performance for current state-of-research post-hoc\nCNN pruning techniques in the context of image classification tasks. Our study\nreveals that post-hoc pruning substantially improves the model's uncertainty\ncalibration, performance, and natural corruption robustness, sparking hope for\nsafe and robust embedded CNNs.Furthermore, uncertainty calibration and natural\ncorruption robustness are not mutually exclusive targets under pruning, as\nevidenced by the improved safety aspects obtained by post-hoc unstructured\npruning with increasing compression.\n", "link": "http://arxiv.org/abs/2405.20876v1", "date": "2024-05-31", "relevancy": 2.0386, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5595}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4749}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20Calibration%20and%20Corruption%20Robustness%20of%20Post-hoc%20Pruned%0A%20%20Perception%20CNNs%3A%20An%20Image%20Classification%20Benchmark%20Study&body=Title%3A%20Investigating%20Calibration%20and%20Corruption%20Robustness%20of%20Post-hoc%20Pruned%0A%20%20Perception%20CNNs%3A%20An%20Image%20Classification%20Benchmark%20Study%0AAuthor%3A%20Pallavi%20Mitra%20and%20Gesina%20Schwalbe%20and%20Nadja%20Klein%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20achieved%20state-of-the-art%0Aperformance%20in%20many%20computer%20vision%20tasks.%20However%2C%20high%20computational%20and%0Astorage%20demands%20hinder%20their%20deployment%20into%20resource-constrained%20environments%2C%0Asuch%20as%20embedded%20devices.%20Model%20pruning%20helps%20to%20meet%20these%20restrictions%20by%0Areducing%20the%20model%20size%2C%20while%20maintaining%20superior%20performance.%20Meanwhile%2C%0Asafety-critical%20applications%20pose%20more%20than%20just%20resource%20and%20performance%0Aconstraints.%20In%20particular%2C%20predictions%20must%20not%20be%20overly%20confident%2C%20i.e.%2C%0Aprovide%20properly%20calibrated%20uncertainty%20estimations%20%28proper%20uncertainty%0Acalibration%29%2C%20and%20CNNs%20must%20be%20robust%20against%20corruptions%20like%20naturally%0Aoccurring%20input%20perturbations%20%28natural%20corruption%20robustness%29.%20This%20work%0Ainvestigates%20the%20important%20trade-off%20between%20uncertainty%20calibration%2C%20natural%0Acorruption%20robustness%2C%20and%20performance%20for%20current%20state-of-research%20post-hoc%0ACNN%20pruning%20techniques%20in%20the%20context%20of%20image%20classification%20tasks.%20Our%20study%0Areveals%20that%20post-hoc%20pruning%20substantially%20improves%20the%20model%27s%20uncertainty%0Acalibration%2C%20performance%2C%20and%20natural%20corruption%20robustness%2C%20sparking%20hope%20for%0Asafe%20and%20robust%20embedded%20CNNs.Furthermore%2C%20uncertainty%20calibration%20and%20natural%0Acorruption%20robustness%20are%20not%20mutually%20exclusive%20targets%20under%20pruning%2C%20as%0Aevidenced%20by%20the%20improved%20safety%20aspects%20obtained%20by%20post-hoc%20unstructured%0Apruning%20with%20increasing%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520Calibration%2520and%2520Corruption%2520Robustness%2520of%2520Post-hoc%2520Pruned%250A%2520%2520Perception%2520CNNs%253A%2520An%2520Image%2520Classification%2520Benchmark%2520Study%26entry.906535625%3DPallavi%2520Mitra%2520and%2520Gesina%2520Schwalbe%2520and%2520Nadja%2520Klein%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520achieved%2520state-of-the-art%250Aperformance%2520in%2520many%2520computer%2520vision%2520tasks.%2520However%252C%2520high%2520computational%2520and%250Astorage%2520demands%2520hinder%2520their%2520deployment%2520into%2520resource-constrained%2520environments%252C%250Asuch%2520as%2520embedded%2520devices.%2520Model%2520pruning%2520helps%2520to%2520meet%2520these%2520restrictions%2520by%250Areducing%2520the%2520model%2520size%252C%2520while%2520maintaining%2520superior%2520performance.%2520Meanwhile%252C%250Asafety-critical%2520applications%2520pose%2520more%2520than%2520just%2520resource%2520and%2520performance%250Aconstraints.%2520In%2520particular%252C%2520predictions%2520must%2520not%2520be%2520overly%2520confident%252C%2520i.e.%252C%250Aprovide%2520properly%2520calibrated%2520uncertainty%2520estimations%2520%2528proper%2520uncertainty%250Acalibration%2529%252C%2520and%2520CNNs%2520must%2520be%2520robust%2520against%2520corruptions%2520like%2520naturally%250Aoccurring%2520input%2520perturbations%2520%2528natural%2520corruption%2520robustness%2529.%2520This%2520work%250Ainvestigates%2520the%2520important%2520trade-off%2520between%2520uncertainty%2520calibration%252C%2520natural%250Acorruption%2520robustness%252C%2520and%2520performance%2520for%2520current%2520state-of-research%2520post-hoc%250ACNN%2520pruning%2520techniques%2520in%2520the%2520context%2520of%2520image%2520classification%2520tasks.%2520Our%2520study%250Areveals%2520that%2520post-hoc%2520pruning%2520substantially%2520improves%2520the%2520model%2527s%2520uncertainty%250Acalibration%252C%2520performance%252C%2520and%2520natural%2520corruption%2520robustness%252C%2520sparking%2520hope%2520for%250Asafe%2520and%2520robust%2520embedded%2520CNNs.Furthermore%252C%2520uncertainty%2520calibration%2520and%2520natural%250Acorruption%2520robustness%2520are%2520not%2520mutually%2520exclusive%2520targets%2520under%2520pruning%252C%2520as%250Aevidenced%2520by%2520the%2520improved%2520safety%2520aspects%2520obtained%2520by%2520post-hoc%2520unstructured%250Apruning%2520with%2520increasing%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Calibration%20and%20Corruption%20Robustness%20of%20Post-hoc%20Pruned%0A%20%20Perception%20CNNs%3A%20An%20Image%20Classification%20Benchmark%20Study&entry.906535625=Pallavi%20Mitra%20and%20Gesina%20Schwalbe%20and%20Nadja%20Klein&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20achieved%20state-of-the-art%0Aperformance%20in%20many%20computer%20vision%20tasks.%20However%2C%20high%20computational%20and%0Astorage%20demands%20hinder%20their%20deployment%20into%20resource-constrained%20environments%2C%0Asuch%20as%20embedded%20devices.%20Model%20pruning%20helps%20to%20meet%20these%20restrictions%20by%0Areducing%20the%20model%20size%2C%20while%20maintaining%20superior%20performance.%20Meanwhile%2C%0Asafety-critical%20applications%20pose%20more%20than%20just%20resource%20and%20performance%0Aconstraints.%20In%20particular%2C%20predictions%20must%20not%20be%20overly%20confident%2C%20i.e.%2C%0Aprovide%20properly%20calibrated%20uncertainty%20estimations%20%28proper%20uncertainty%0Acalibration%29%2C%20and%20CNNs%20must%20be%20robust%20against%20corruptions%20like%20naturally%0Aoccurring%20input%20perturbations%20%28natural%20corruption%20robustness%29.%20This%20work%0Ainvestigates%20the%20important%20trade-off%20between%20uncertainty%20calibration%2C%20natural%0Acorruption%20robustness%2C%20and%20performance%20for%20current%20state-of-research%20post-hoc%0ACNN%20pruning%20techniques%20in%20the%20context%20of%20image%20classification%20tasks.%20Our%20study%0Areveals%20that%20post-hoc%20pruning%20substantially%20improves%20the%20model%27s%20uncertainty%0Acalibration%2C%20performance%2C%20and%20natural%20corruption%20robustness%2C%20sparking%20hope%20for%0Asafe%20and%20robust%20embedded%20CNNs.Furthermore%2C%20uncertainty%20calibration%20and%20natural%0Acorruption%20robustness%20are%20not%20mutually%20exclusive%20targets%20under%20pruning%2C%20as%0Aevidenced%20by%20the%20improved%20safety%20aspects%20obtained%20by%20post-hoc%20unstructured%0Apruning%20with%20increasing%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20876v1&entry.124074799=Read"},
{"title": "Design, Calibration, and Control of Compliant Force-sensing Gripping\n  Pads for Humanoid Robots", "author": "Yuanfeng Han and Boren Jiang and Gregory S. Chirikjian", "abstract": "  This paper introduces a pair of low-cost, light-weight and compliant\nforce-sensing gripping pads used for manipulating box-like objects with\nsmaller-sized humanoid robots. These pads measure normal gripping forces and\ncenter of pressure (CoP). A calibration method is developed to improve the CoP\nmeasurement accuracy. A hybrid force-alignment-position control framework is\nproposed to regulate the gripping forces and to ensure the surface alignment\nbetween the grippers and the object. Limit surface theory is incorporated as a\ncontact friction modeling approach to determine the magnitude of gripping\nforces for slippage avoidance. The integrated hardware and software system is\ndemonstrated with a NAO humanoid robot. Experiments show the effectiveness of\nthe overall approach.\n", "link": "http://arxiv.org/abs/2405.20969v1", "date": "2024-05-31", "relevancy": 2.0353, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5362}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5061}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%2C%20Calibration%2C%20and%20Control%20of%20Compliant%20Force-sensing%20Gripping%0A%20%20Pads%20for%20Humanoid%20Robots&body=Title%3A%20Design%2C%20Calibration%2C%20and%20Control%20of%20Compliant%20Force-sensing%20Gripping%0A%20%20Pads%20for%20Humanoid%20Robots%0AAuthor%3A%20Yuanfeng%20Han%20and%20Boren%20Jiang%20and%20Gregory%20S.%20Chirikjian%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20pair%20of%20low-cost%2C%20light-weight%20and%20compliant%0Aforce-sensing%20gripping%20pads%20used%20for%20manipulating%20box-like%20objects%20with%0Asmaller-sized%20humanoid%20robots.%20These%20pads%20measure%20normal%20gripping%20forces%20and%0Acenter%20of%20pressure%20%28CoP%29.%20A%20calibration%20method%20is%20developed%20to%20improve%20the%20CoP%0Ameasurement%20accuracy.%20A%20hybrid%20force-alignment-position%20control%20framework%20is%0Aproposed%20to%20regulate%20the%20gripping%20forces%20and%20to%20ensure%20the%20surface%20alignment%0Abetween%20the%20grippers%20and%20the%20object.%20Limit%20surface%20theory%20is%20incorporated%20as%20a%0Acontact%20friction%20modeling%20approach%20to%20determine%20the%20magnitude%20of%20gripping%0Aforces%20for%20slippage%20avoidance.%20The%20integrated%20hardware%20and%20software%20system%20is%0Ademonstrated%20with%20a%20NAO%20humanoid%20robot.%20Experiments%20show%20the%20effectiveness%20of%0Athe%20overall%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%252C%2520Calibration%252C%2520and%2520Control%2520of%2520Compliant%2520Force-sensing%2520Gripping%250A%2520%2520Pads%2520for%2520Humanoid%2520Robots%26entry.906535625%3DYuanfeng%2520Han%2520and%2520Boren%2520Jiang%2520and%2520Gregory%2520S.%2520Chirikjian%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520pair%2520of%2520low-cost%252C%2520light-weight%2520and%2520compliant%250Aforce-sensing%2520gripping%2520pads%2520used%2520for%2520manipulating%2520box-like%2520objects%2520with%250Asmaller-sized%2520humanoid%2520robots.%2520These%2520pads%2520measure%2520normal%2520gripping%2520forces%2520and%250Acenter%2520of%2520pressure%2520%2528CoP%2529.%2520A%2520calibration%2520method%2520is%2520developed%2520to%2520improve%2520the%2520CoP%250Ameasurement%2520accuracy.%2520A%2520hybrid%2520force-alignment-position%2520control%2520framework%2520is%250Aproposed%2520to%2520regulate%2520the%2520gripping%2520forces%2520and%2520to%2520ensure%2520the%2520surface%2520alignment%250Abetween%2520the%2520grippers%2520and%2520the%2520object.%2520Limit%2520surface%2520theory%2520is%2520incorporated%2520as%2520a%250Acontact%2520friction%2520modeling%2520approach%2520to%2520determine%2520the%2520magnitude%2520of%2520gripping%250Aforces%2520for%2520slippage%2520avoidance.%2520The%2520integrated%2520hardware%2520and%2520software%2520system%2520is%250Ademonstrated%2520with%2520a%2520NAO%2520humanoid%2520robot.%2520Experiments%2520show%2520the%2520effectiveness%2520of%250Athe%2520overall%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%2C%20Calibration%2C%20and%20Control%20of%20Compliant%20Force-sensing%20Gripping%0A%20%20Pads%20for%20Humanoid%20Robots&entry.906535625=Yuanfeng%20Han%20and%20Boren%20Jiang%20and%20Gregory%20S.%20Chirikjian&entry.1292438233=%20%20This%20paper%20introduces%20a%20pair%20of%20low-cost%2C%20light-weight%20and%20compliant%0Aforce-sensing%20gripping%20pads%20used%20for%20manipulating%20box-like%20objects%20with%0Asmaller-sized%20humanoid%20robots.%20These%20pads%20measure%20normal%20gripping%20forces%20and%0Acenter%20of%20pressure%20%28CoP%29.%20A%20calibration%20method%20is%20developed%20to%20improve%20the%20CoP%0Ameasurement%20accuracy.%20A%20hybrid%20force-alignment-position%20control%20framework%20is%0Aproposed%20to%20regulate%20the%20gripping%20forces%20and%20to%20ensure%20the%20surface%20alignment%0Abetween%20the%20grippers%20and%20the%20object.%20Limit%20surface%20theory%20is%20incorporated%20as%20a%0Acontact%20friction%20modeling%20approach%20to%20determine%20the%20magnitude%20of%20gripping%0Aforces%20for%20slippage%20avoidance.%20The%20integrated%20hardware%20and%20software%20system%20is%0Ademonstrated%20with%20a%20NAO%20humanoid%20robot.%20Experiments%20show%20the%20effectiveness%20of%0Athe%20overall%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20969v1&entry.124074799=Read"},
{"title": "CLIP-QDA: An Explainable Concept Bottleneck Model", "author": "R\u00e9mi Kazmierczak and Elo\u00efse Berthier and Goran Frehse and Gianni Franchi", "abstract": "  In this paper, we introduce an explainable algorithm designed from a\nmulti-modal foundation model, that performs fast and explainable image\nclassification. Drawing inspiration from CLIP-based Concept Bottleneck Models\n(CBMs), our method creates a latent space where each neuron is linked to a\nspecific word. Observing that this latent space can be modeled with simple\ndistributions, we use a Mixture of Gaussians (MoG) formalism to enhance the\ninterpretability of this latent space. Then, we introduce CLIP-QDA, a\nclassifier that only uses statistical values to infer labels from the concepts.\nIn addition, this formalism allows for both local and global explanations.\nThese explanations come from the inner design of our architecture, our work is\npart of a new family of greybox models, combining performances of opaque\nfoundation models and the interpretability of transparent models. Our empirical\nfindings show that in instances where the MoG assumption holds, CLIP-QDA\nachieves similar accuracy with state-of-the-art methods CBMs. Our explanations\ncompete with existing XAI methods while being faster to compute.\n", "link": "http://arxiv.org/abs/2312.00110v3", "date": "2024-05-31", "relevancy": 2.0345, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5186}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5114}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP-QDA%3A%20An%20Explainable%20Concept%20Bottleneck%20Model&body=Title%3A%20CLIP-QDA%3A%20An%20Explainable%20Concept%20Bottleneck%20Model%0AAuthor%3A%20R%C3%A9mi%20Kazmierczak%20and%20Elo%C3%AFse%20Berthier%20and%20Goran%20Frehse%20and%20Gianni%20Franchi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20an%20explainable%20algorithm%20designed%20from%20a%0Amulti-modal%20foundation%20model%2C%20that%20performs%20fast%20and%20explainable%20image%0Aclassification.%20Drawing%20inspiration%20from%20CLIP-based%20Concept%20Bottleneck%20Models%0A%28CBMs%29%2C%20our%20method%20creates%20a%20latent%20space%20where%20each%20neuron%20is%20linked%20to%20a%0Aspecific%20word.%20Observing%20that%20this%20latent%20space%20can%20be%20modeled%20with%20simple%0Adistributions%2C%20we%20use%20a%20Mixture%20of%20Gaussians%20%28MoG%29%20formalism%20to%20enhance%20the%0Ainterpretability%20of%20this%20latent%20space.%20Then%2C%20we%20introduce%20CLIP-QDA%2C%20a%0Aclassifier%20that%20only%20uses%20statistical%20values%20to%20infer%20labels%20from%20the%20concepts.%0AIn%20addition%2C%20this%20formalism%20allows%20for%20both%20local%20and%20global%20explanations.%0AThese%20explanations%20come%20from%20the%20inner%20design%20of%20our%20architecture%2C%20our%20work%20is%0Apart%20of%20a%20new%20family%20of%20greybox%20models%2C%20combining%20performances%20of%20opaque%0Afoundation%20models%20and%20the%20interpretability%20of%20transparent%20models.%20Our%20empirical%0Afindings%20show%20that%20in%20instances%20where%20the%20MoG%20assumption%20holds%2C%20CLIP-QDA%0Aachieves%20similar%20accuracy%20with%20state-of-the-art%20methods%20CBMs.%20Our%20explanations%0Acompete%20with%20existing%20XAI%20methods%20while%20being%20faster%20to%20compute.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00110v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP-QDA%253A%2520An%2520Explainable%2520Concept%2520Bottleneck%2520Model%26entry.906535625%3DR%25C3%25A9mi%2520Kazmierczak%2520and%2520Elo%25C3%25AFse%2520Berthier%2520and%2520Goran%2520Frehse%2520and%2520Gianni%2520Franchi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%2520explainable%2520algorithm%2520designed%2520from%2520a%250Amulti-modal%2520foundation%2520model%252C%2520that%2520performs%2520fast%2520and%2520explainable%2520image%250Aclassification.%2520Drawing%2520inspiration%2520from%2520CLIP-based%2520Concept%2520Bottleneck%2520Models%250A%2528CBMs%2529%252C%2520our%2520method%2520creates%2520a%2520latent%2520space%2520where%2520each%2520neuron%2520is%2520linked%2520to%2520a%250Aspecific%2520word.%2520Observing%2520that%2520this%2520latent%2520space%2520can%2520be%2520modeled%2520with%2520simple%250Adistributions%252C%2520we%2520use%2520a%2520Mixture%2520of%2520Gaussians%2520%2528MoG%2529%2520formalism%2520to%2520enhance%2520the%250Ainterpretability%2520of%2520this%2520latent%2520space.%2520Then%252C%2520we%2520introduce%2520CLIP-QDA%252C%2520a%250Aclassifier%2520that%2520only%2520uses%2520statistical%2520values%2520to%2520infer%2520labels%2520from%2520the%2520concepts.%250AIn%2520addition%252C%2520this%2520formalism%2520allows%2520for%2520both%2520local%2520and%2520global%2520explanations.%250AThese%2520explanations%2520come%2520from%2520the%2520inner%2520design%2520of%2520our%2520architecture%252C%2520our%2520work%2520is%250Apart%2520of%2520a%2520new%2520family%2520of%2520greybox%2520models%252C%2520combining%2520performances%2520of%2520opaque%250Afoundation%2520models%2520and%2520the%2520interpretability%2520of%2520transparent%2520models.%2520Our%2520empirical%250Afindings%2520show%2520that%2520in%2520instances%2520where%2520the%2520MoG%2520assumption%2520holds%252C%2520CLIP-QDA%250Aachieves%2520similar%2520accuracy%2520with%2520state-of-the-art%2520methods%2520CBMs.%2520Our%2520explanations%250Acompete%2520with%2520existing%2520XAI%2520methods%2520while%2520being%2520faster%2520to%2520compute.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00110v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP-QDA%3A%20An%20Explainable%20Concept%20Bottleneck%20Model&entry.906535625=R%C3%A9mi%20Kazmierczak%20and%20Elo%C3%AFse%20Berthier%20and%20Goran%20Frehse%20and%20Gianni%20Franchi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20an%20explainable%20algorithm%20designed%20from%20a%0Amulti-modal%20foundation%20model%2C%20that%20performs%20fast%20and%20explainable%20image%0Aclassification.%20Drawing%20inspiration%20from%20CLIP-based%20Concept%20Bottleneck%20Models%0A%28CBMs%29%2C%20our%20method%20creates%20a%20latent%20space%20where%20each%20neuron%20is%20linked%20to%20a%0Aspecific%20word.%20Observing%20that%20this%20latent%20space%20can%20be%20modeled%20with%20simple%0Adistributions%2C%20we%20use%20a%20Mixture%20of%20Gaussians%20%28MoG%29%20formalism%20to%20enhance%20the%0Ainterpretability%20of%20this%20latent%20space.%20Then%2C%20we%20introduce%20CLIP-QDA%2C%20a%0Aclassifier%20that%20only%20uses%20statistical%20values%20to%20infer%20labels%20from%20the%20concepts.%0AIn%20addition%2C%20this%20formalism%20allows%20for%20both%20local%20and%20global%20explanations.%0AThese%20explanations%20come%20from%20the%20inner%20design%20of%20our%20architecture%2C%20our%20work%20is%0Apart%20of%20a%20new%20family%20of%20greybox%20models%2C%20combining%20performances%20of%20opaque%0Afoundation%20models%20and%20the%20interpretability%20of%20transparent%20models.%20Our%20empirical%0Afindings%20show%20that%20in%20instances%20where%20the%20MoG%20assumption%20holds%2C%20CLIP-QDA%0Aachieves%20similar%20accuracy%20with%20state-of-the-art%20methods%20CBMs.%20Our%20explanations%0Acompete%20with%20existing%20XAI%20methods%20while%20being%20faster%20to%20compute.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00110v3&entry.124074799=Read"},
{"title": "Diffusion Models Are Innate One-Step Generators", "author": "Bowen Zheng and Tianming Yang", "abstract": "  Diffusion Models (DMs) have achieved great success in image generation and\nother fields. By fine sampling through the trajectory defined by the SDE/ODE\nsolver based on a well-trained score model, DMs can generate remarkable\nhigh-quality results. However, this precise sampling often requires multiple\nsteps and is computationally demanding. To address this problem, instance-based\ndistillation methods have been proposed to distill a one-step generator from a\nDM by having a simpler student model mimic a more complex teacher model. Yet,\nour research reveals an inherent limitations in these methods: the teacher\nmodel, with more steps and more parameters, occupies different local minima\ncompared to the student model, leading to suboptimal performance when the\nstudent model attempts to replicate the teacher. To avoid this problem, we\nintroduce a novel distributional distillation method, which uses an exclusive\ndistributional loss. This method exceeds state-of-the-art (SOTA) results while\nrequiring significantly fewer training images. Additionally, we show that DMs'\nlayers are activated differently at different time steps, leading to an\ninherent capability to generate images in a single step. Freezing most of the\nconvolutional layers in a DM during distributional distillation leads to\nfurther performance improvements. Our method achieves the SOTA results on\nCIFAR-10 (FID 1.54), AFHQv2 64x64 (FID 1.23), FFHQ 64x64 (FID 0.85) and\nImageNet 64x64 (FID 1.16) with great efficiency. Most of those results are\nobtained with only 5 million training images within 6 hours on 8 A100 GPUs.\nThis breakthrough not only enhances the understanding of efficient image\ngeneration models but also offers a scalable framework for advancing the state\nof the art in various applications.\n", "link": "http://arxiv.org/abs/2405.20750v1", "date": "2024-05-31", "relevancy": 2.0201, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7492}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.656}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.65}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Models%20Are%20Innate%20One-Step%20Generators&body=Title%3A%20Diffusion%20Models%20Are%20Innate%20One-Step%20Generators%0AAuthor%3A%20Bowen%20Zheng%20and%20Tianming%20Yang%0AAbstract%3A%20%20%20Diffusion%20Models%20%28DMs%29%20have%20achieved%20great%20success%20in%20image%20generation%20and%0Aother%20fields.%20By%20fine%20sampling%20through%20the%20trajectory%20defined%20by%20the%20SDE/ODE%0Asolver%20based%20on%20a%20well-trained%20score%20model%2C%20DMs%20can%20generate%20remarkable%0Ahigh-quality%20results.%20However%2C%20this%20precise%20sampling%20often%20requires%20multiple%0Asteps%20and%20is%20computationally%20demanding.%20To%20address%20this%20problem%2C%20instance-based%0Adistillation%20methods%20have%20been%20proposed%20to%20distill%20a%20one-step%20generator%20from%20a%0ADM%20by%20having%20a%20simpler%20student%20model%20mimic%20a%20more%20complex%20teacher%20model.%20Yet%2C%0Aour%20research%20reveals%20an%20inherent%20limitations%20in%20these%20methods%3A%20the%20teacher%0Amodel%2C%20with%20more%20steps%20and%20more%20parameters%2C%20occupies%20different%20local%20minima%0Acompared%20to%20the%20student%20model%2C%20leading%20to%20suboptimal%20performance%20when%20the%0Astudent%20model%20attempts%20to%20replicate%20the%20teacher.%20To%20avoid%20this%20problem%2C%20we%0Aintroduce%20a%20novel%20distributional%20distillation%20method%2C%20which%20uses%20an%20exclusive%0Adistributional%20loss.%20This%20method%20exceeds%20state-of-the-art%20%28SOTA%29%20results%20while%0Arequiring%20significantly%20fewer%20training%20images.%20Additionally%2C%20we%20show%20that%20DMs%27%0Alayers%20are%20activated%20differently%20at%20different%20time%20steps%2C%20leading%20to%20an%0Ainherent%20capability%20to%20generate%20images%20in%20a%20single%20step.%20Freezing%20most%20of%20the%0Aconvolutional%20layers%20in%20a%20DM%20during%20distributional%20distillation%20leads%20to%0Afurther%20performance%20improvements.%20Our%20method%20achieves%20the%20SOTA%20results%20on%0ACIFAR-10%20%28FID%201.54%29%2C%20AFHQv2%2064x64%20%28FID%201.23%29%2C%20FFHQ%2064x64%20%28FID%200.85%29%20and%0AImageNet%2064x64%20%28FID%201.16%29%20with%20great%20efficiency.%20Most%20of%20those%20results%20are%0Aobtained%20with%20only%205%20million%20training%20images%20within%206%20hours%20on%208%20A100%20GPUs.%0AThis%20breakthrough%20not%20only%20enhances%20the%20understanding%20of%20efficient%20image%0Ageneration%20models%20but%20also%20offers%20a%20scalable%20framework%20for%20advancing%20the%20state%0Aof%20the%20art%20in%20various%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Models%2520Are%2520Innate%2520One-Step%2520Generators%26entry.906535625%3DBowen%2520Zheng%2520and%2520Tianming%2520Yang%26entry.1292438233%3D%2520%2520Diffusion%2520Models%2520%2528DMs%2529%2520have%2520achieved%2520great%2520success%2520in%2520image%2520generation%2520and%250Aother%2520fields.%2520By%2520fine%2520sampling%2520through%2520the%2520trajectory%2520defined%2520by%2520the%2520SDE/ODE%250Asolver%2520based%2520on%2520a%2520well-trained%2520score%2520model%252C%2520DMs%2520can%2520generate%2520remarkable%250Ahigh-quality%2520results.%2520However%252C%2520this%2520precise%2520sampling%2520often%2520requires%2520multiple%250Asteps%2520and%2520is%2520computationally%2520demanding.%2520To%2520address%2520this%2520problem%252C%2520instance-based%250Adistillation%2520methods%2520have%2520been%2520proposed%2520to%2520distill%2520a%2520one-step%2520generator%2520from%2520a%250ADM%2520by%2520having%2520a%2520simpler%2520student%2520model%2520mimic%2520a%2520more%2520complex%2520teacher%2520model.%2520Yet%252C%250Aour%2520research%2520reveals%2520an%2520inherent%2520limitations%2520in%2520these%2520methods%253A%2520the%2520teacher%250Amodel%252C%2520with%2520more%2520steps%2520and%2520more%2520parameters%252C%2520occupies%2520different%2520local%2520minima%250Acompared%2520to%2520the%2520student%2520model%252C%2520leading%2520to%2520suboptimal%2520performance%2520when%2520the%250Astudent%2520model%2520attempts%2520to%2520replicate%2520the%2520teacher.%2520To%2520avoid%2520this%2520problem%252C%2520we%250Aintroduce%2520a%2520novel%2520distributional%2520distillation%2520method%252C%2520which%2520uses%2520an%2520exclusive%250Adistributional%2520loss.%2520This%2520method%2520exceeds%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520while%250Arequiring%2520significantly%2520fewer%2520training%2520images.%2520Additionally%252C%2520we%2520show%2520that%2520DMs%2527%250Alayers%2520are%2520activated%2520differently%2520at%2520different%2520time%2520steps%252C%2520leading%2520to%2520an%250Ainherent%2520capability%2520to%2520generate%2520images%2520in%2520a%2520single%2520step.%2520Freezing%2520most%2520of%2520the%250Aconvolutional%2520layers%2520in%2520a%2520DM%2520during%2520distributional%2520distillation%2520leads%2520to%250Afurther%2520performance%2520improvements.%2520Our%2520method%2520achieves%2520the%2520SOTA%2520results%2520on%250ACIFAR-10%2520%2528FID%25201.54%2529%252C%2520AFHQv2%252064x64%2520%2528FID%25201.23%2529%252C%2520FFHQ%252064x64%2520%2528FID%25200.85%2529%2520and%250AImageNet%252064x64%2520%2528FID%25201.16%2529%2520with%2520great%2520efficiency.%2520Most%2520of%2520those%2520results%2520are%250Aobtained%2520with%2520only%25205%2520million%2520training%2520images%2520within%25206%2520hours%2520on%25208%2520A100%2520GPUs.%250AThis%2520breakthrough%2520not%2520only%2520enhances%2520the%2520understanding%2520of%2520efficient%2520image%250Ageneration%2520models%2520but%2520also%2520offers%2520a%2520scalable%2520framework%2520for%2520advancing%2520the%2520state%250Aof%2520the%2520art%2520in%2520various%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Models%20Are%20Innate%20One-Step%20Generators&entry.906535625=Bowen%20Zheng%20and%20Tianming%20Yang&entry.1292438233=%20%20Diffusion%20Models%20%28DMs%29%20have%20achieved%20great%20success%20in%20image%20generation%20and%0Aother%20fields.%20By%20fine%20sampling%20through%20the%20trajectory%20defined%20by%20the%20SDE/ODE%0Asolver%20based%20on%20a%20well-trained%20score%20model%2C%20DMs%20can%20generate%20remarkable%0Ahigh-quality%20results.%20However%2C%20this%20precise%20sampling%20often%20requires%20multiple%0Asteps%20and%20is%20computationally%20demanding.%20To%20address%20this%20problem%2C%20instance-based%0Adistillation%20methods%20have%20been%20proposed%20to%20distill%20a%20one-step%20generator%20from%20a%0ADM%20by%20having%20a%20simpler%20student%20model%20mimic%20a%20more%20complex%20teacher%20model.%20Yet%2C%0Aour%20research%20reveals%20an%20inherent%20limitations%20in%20these%20methods%3A%20the%20teacher%0Amodel%2C%20with%20more%20steps%20and%20more%20parameters%2C%20occupies%20different%20local%20minima%0Acompared%20to%20the%20student%20model%2C%20leading%20to%20suboptimal%20performance%20when%20the%0Astudent%20model%20attempts%20to%20replicate%20the%20teacher.%20To%20avoid%20this%20problem%2C%20we%0Aintroduce%20a%20novel%20distributional%20distillation%20method%2C%20which%20uses%20an%20exclusive%0Adistributional%20loss.%20This%20method%20exceeds%20state-of-the-art%20%28SOTA%29%20results%20while%0Arequiring%20significantly%20fewer%20training%20images.%20Additionally%2C%20we%20show%20that%20DMs%27%0Alayers%20are%20activated%20differently%20at%20different%20time%20steps%2C%20leading%20to%20an%0Ainherent%20capability%20to%20generate%20images%20in%20a%20single%20step.%20Freezing%20most%20of%20the%0Aconvolutional%20layers%20in%20a%20DM%20during%20distributional%20distillation%20leads%20to%0Afurther%20performance%20improvements.%20Our%20method%20achieves%20the%20SOTA%20results%20on%0ACIFAR-10%20%28FID%201.54%29%2C%20AFHQv2%2064x64%20%28FID%201.23%29%2C%20FFHQ%2064x64%20%28FID%200.85%29%20and%0AImageNet%2064x64%20%28FID%201.16%29%20with%20great%20efficiency.%20Most%20of%20those%20results%20are%0Aobtained%20with%20only%205%20million%20training%20images%20within%206%20hours%20on%208%20A100%20GPUs.%0AThis%20breakthrough%20not%20only%20enhances%20the%20understanding%20of%20efficient%20image%0Ageneration%20models%20but%20also%20offers%20a%20scalable%20framework%20for%20advancing%20the%20state%0Aof%20the%20art%20in%20various%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20750v1&entry.124074799=Read"},
{"title": "Predicting ptychography probe positions using single-shot phase\n  retrieval neural network", "author": "Ming Du and Tao Zhou and Junjing Deng and Daniel J. Ching and Steven Henke and Mathew J. Cherukara", "abstract": "  Ptychography is a powerful imaging technique that is used in a variety of\nfields, including materials science, biology, and nanotechnology. However, the\naccuracy of the reconstructed ptychography image is highly dependent on the\naccuracy of the recorded probe positions which often contain errors. These\nerrors are typically corrected jointly with phase retrieval through numerical\noptimization approaches. When the error accumulates along the scan path or when\nthe error magnitude is large, these approaches may not converge with\nsatisfactory result. We propose a fundamentally new approach for ptychography\nprobe position prediction for data with large position errors, where a neural\nnetwork is used to make single-shot phase retrieval on individual diffraction\npatterns, yielding the object image at each scan point. The pairwise offsets\namong these images are then found using a robust image registration method, and\nthe results are combined to yield the complete scan path by constructing and\nsolving a linear equation. We show that our method can achieve good position\nprediction accuracy for data with large and accumulating errors on the order of\n$10^2$ pixels, a magnitude that often makes optimization-based algorithms fail\nto converge. For ptychography instruments without sophisticated position\ncontrol equipment such as interferometers, our method is of significant\npractical potential.\n", "link": "http://arxiv.org/abs/2405.20910v1", "date": "2024-05-31", "relevancy": 2.0158, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5064}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5029}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20ptychography%20probe%20positions%20using%20single-shot%20phase%0A%20%20retrieval%20neural%20network&body=Title%3A%20Predicting%20ptychography%20probe%20positions%20using%20single-shot%20phase%0A%20%20retrieval%20neural%20network%0AAuthor%3A%20Ming%20Du%20and%20Tao%20Zhou%20and%20Junjing%20Deng%20and%20Daniel%20J.%20Ching%20and%20Steven%20Henke%20and%20Mathew%20J.%20Cherukara%0AAbstract%3A%20%20%20Ptychography%20is%20a%20powerful%20imaging%20technique%20that%20is%20used%20in%20a%20variety%20of%0Afields%2C%20including%20materials%20science%2C%20biology%2C%20and%20nanotechnology.%20However%2C%20the%0Aaccuracy%20of%20the%20reconstructed%20ptychography%20image%20is%20highly%20dependent%20on%20the%0Aaccuracy%20of%20the%20recorded%20probe%20positions%20which%20often%20contain%20errors.%20These%0Aerrors%20are%20typically%20corrected%20jointly%20with%20phase%20retrieval%20through%20numerical%0Aoptimization%20approaches.%20When%20the%20error%20accumulates%20along%20the%20scan%20path%20or%20when%0Athe%20error%20magnitude%20is%20large%2C%20these%20approaches%20may%20not%20converge%20with%0Asatisfactory%20result.%20We%20propose%20a%20fundamentally%20new%20approach%20for%20ptychography%0Aprobe%20position%20prediction%20for%20data%20with%20large%20position%20errors%2C%20where%20a%20neural%0Anetwork%20is%20used%20to%20make%20single-shot%20phase%20retrieval%20on%20individual%20diffraction%0Apatterns%2C%20yielding%20the%20object%20image%20at%20each%20scan%20point.%20The%20pairwise%20offsets%0Aamong%20these%20images%20are%20then%20found%20using%20a%20robust%20image%20registration%20method%2C%20and%0Athe%20results%20are%20combined%20to%20yield%20the%20complete%20scan%20path%20by%20constructing%20and%0Asolving%20a%20linear%20equation.%20We%20show%20that%20our%20method%20can%20achieve%20good%20position%0Aprediction%20accuracy%20for%20data%20with%20large%20and%20accumulating%20errors%20on%20the%20order%20of%0A%2410%5E2%24%20pixels%2C%20a%20magnitude%20that%20often%20makes%20optimization-based%20algorithms%20fail%0Ato%20converge.%20For%20ptychography%20instruments%20without%20sophisticated%20position%0Acontrol%20equipment%20such%20as%20interferometers%2C%20our%20method%20is%20of%20significant%0Apractical%20potential.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520ptychography%2520probe%2520positions%2520using%2520single-shot%2520phase%250A%2520%2520retrieval%2520neural%2520network%26entry.906535625%3DMing%2520Du%2520and%2520Tao%2520Zhou%2520and%2520Junjing%2520Deng%2520and%2520Daniel%2520J.%2520Ching%2520and%2520Steven%2520Henke%2520and%2520Mathew%2520J.%2520Cherukara%26entry.1292438233%3D%2520%2520Ptychography%2520is%2520a%2520powerful%2520imaging%2520technique%2520that%2520is%2520used%2520in%2520a%2520variety%2520of%250Afields%252C%2520including%2520materials%2520science%252C%2520biology%252C%2520and%2520nanotechnology.%2520However%252C%2520the%250Aaccuracy%2520of%2520the%2520reconstructed%2520ptychography%2520image%2520is%2520highly%2520dependent%2520on%2520the%250Aaccuracy%2520of%2520the%2520recorded%2520probe%2520positions%2520which%2520often%2520contain%2520errors.%2520These%250Aerrors%2520are%2520typically%2520corrected%2520jointly%2520with%2520phase%2520retrieval%2520through%2520numerical%250Aoptimization%2520approaches.%2520When%2520the%2520error%2520accumulates%2520along%2520the%2520scan%2520path%2520or%2520when%250Athe%2520error%2520magnitude%2520is%2520large%252C%2520these%2520approaches%2520may%2520not%2520converge%2520with%250Asatisfactory%2520result.%2520We%2520propose%2520a%2520fundamentally%2520new%2520approach%2520for%2520ptychography%250Aprobe%2520position%2520prediction%2520for%2520data%2520with%2520large%2520position%2520errors%252C%2520where%2520a%2520neural%250Anetwork%2520is%2520used%2520to%2520make%2520single-shot%2520phase%2520retrieval%2520on%2520individual%2520diffraction%250Apatterns%252C%2520yielding%2520the%2520object%2520image%2520at%2520each%2520scan%2520point.%2520The%2520pairwise%2520offsets%250Aamong%2520these%2520images%2520are%2520then%2520found%2520using%2520a%2520robust%2520image%2520registration%2520method%252C%2520and%250Athe%2520results%2520are%2520combined%2520to%2520yield%2520the%2520complete%2520scan%2520path%2520by%2520constructing%2520and%250Asolving%2520a%2520linear%2520equation.%2520We%2520show%2520that%2520our%2520method%2520can%2520achieve%2520good%2520position%250Aprediction%2520accuracy%2520for%2520data%2520with%2520large%2520and%2520accumulating%2520errors%2520on%2520the%2520order%2520of%250A%252410%255E2%2524%2520pixels%252C%2520a%2520magnitude%2520that%2520often%2520makes%2520optimization-based%2520algorithms%2520fail%250Ato%2520converge.%2520For%2520ptychography%2520instruments%2520without%2520sophisticated%2520position%250Acontrol%2520equipment%2520such%2520as%2520interferometers%252C%2520our%2520method%2520is%2520of%2520significant%250Apractical%2520potential.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20ptychography%20probe%20positions%20using%20single-shot%20phase%0A%20%20retrieval%20neural%20network&entry.906535625=Ming%20Du%20and%20Tao%20Zhou%20and%20Junjing%20Deng%20and%20Daniel%20J.%20Ching%20and%20Steven%20Henke%20and%20Mathew%20J.%20Cherukara&entry.1292438233=%20%20Ptychography%20is%20a%20powerful%20imaging%20technique%20that%20is%20used%20in%20a%20variety%20of%0Afields%2C%20including%20materials%20science%2C%20biology%2C%20and%20nanotechnology.%20However%2C%20the%0Aaccuracy%20of%20the%20reconstructed%20ptychography%20image%20is%20highly%20dependent%20on%20the%0Aaccuracy%20of%20the%20recorded%20probe%20positions%20which%20often%20contain%20errors.%20These%0Aerrors%20are%20typically%20corrected%20jointly%20with%20phase%20retrieval%20through%20numerical%0Aoptimization%20approaches.%20When%20the%20error%20accumulates%20along%20the%20scan%20path%20or%20when%0Athe%20error%20magnitude%20is%20large%2C%20these%20approaches%20may%20not%20converge%20with%0Asatisfactory%20result.%20We%20propose%20a%20fundamentally%20new%20approach%20for%20ptychography%0Aprobe%20position%20prediction%20for%20data%20with%20large%20position%20errors%2C%20where%20a%20neural%0Anetwork%20is%20used%20to%20make%20single-shot%20phase%20retrieval%20on%20individual%20diffraction%0Apatterns%2C%20yielding%20the%20object%20image%20at%20each%20scan%20point.%20The%20pairwise%20offsets%0Aamong%20these%20images%20are%20then%20found%20using%20a%20robust%20image%20registration%20method%2C%20and%0Athe%20results%20are%20combined%20to%20yield%20the%20complete%20scan%20path%20by%20constructing%20and%0Asolving%20a%20linear%20equation.%20We%20show%20that%20our%20method%20can%20achieve%20good%20position%0Aprediction%20accuracy%20for%20data%20with%20large%20and%20accumulating%20errors%20on%20the%20order%20of%0A%2410%5E2%24%20pixels%2C%20a%20magnitude%20that%20often%20makes%20optimization-based%20algorithms%20fail%0Ato%20converge.%20For%20ptychography%20instruments%20without%20sophisticated%20position%0Acontrol%20equipment%20such%20as%20interferometers%2C%20our%20method%20is%20of%20significant%0Apractical%20potential.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20910v1&entry.124074799=Read"},
{"title": "Safe Aerial Manipulator Maneuvering and Force Exertion via Control\n  Barrier Functions", "author": "Dimitris Chaikalis and Vinicius Goncalves and Nikolaos Evangeliou and Anthony Tzes and Farshad Khorrami", "abstract": "  This article introduces a safe control strategy for application of forces to\nan external object using a dexterous robotic arm mounted on an unmanned Aerial\nVehicle (UAV). A hybrid force-motion controller has been developed for this\npurpose. This controller employs a Control Barrier Function (CBF) constraint\nwithin an optimization framework based on Quadratic Programming (QP). The\nobjective is to enforce a predefined relationship between the end-effector's\napproach motion and its alignment with the surface, thereby ensuring safe\noperational dynamics. No compliance model for the environment is necessary to\nimplement the controller, provided end-effector force feedback exists.\nFurthermore, the paper provides formal results, like guarantees of feasibility\nfor the optimization problem, continuity of the controller input as a function\nof the configuration, and Lyapunov stability. In addition, it presents\nexperimental results in various situations to demonstrate its practical\napplicability on an aerial manipulator platform.\n", "link": "http://arxiv.org/abs/2309.07709v3", "date": "2024-05-31", "relevancy": 2.0135, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5111}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4997}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Aerial%20Manipulator%20Maneuvering%20and%20Force%20Exertion%20via%20Control%0A%20%20Barrier%20Functions&body=Title%3A%20Safe%20Aerial%20Manipulator%20Maneuvering%20and%20Force%20Exertion%20via%20Control%0A%20%20Barrier%20Functions%0AAuthor%3A%20Dimitris%20Chaikalis%20and%20Vinicius%20Goncalves%20and%20Nikolaos%20Evangeliou%20and%20Anthony%20Tzes%20and%20Farshad%20Khorrami%0AAbstract%3A%20%20%20This%20article%20introduces%20a%20safe%20control%20strategy%20for%20application%20of%20forces%20to%0Aan%20external%20object%20using%20a%20dexterous%20robotic%20arm%20mounted%20on%20an%20unmanned%20Aerial%0AVehicle%20%28UAV%29.%20A%20hybrid%20force-motion%20controller%20has%20been%20developed%20for%20this%0Apurpose.%20This%20controller%20employs%20a%20Control%20Barrier%20Function%20%28CBF%29%20constraint%0Awithin%20an%20optimization%20framework%20based%20on%20Quadratic%20Programming%20%28QP%29.%20The%0Aobjective%20is%20to%20enforce%20a%20predefined%20relationship%20between%20the%20end-effector%27s%0Aapproach%20motion%20and%20its%20alignment%20with%20the%20surface%2C%20thereby%20ensuring%20safe%0Aoperational%20dynamics.%20No%20compliance%20model%20for%20the%20environment%20is%20necessary%20to%0Aimplement%20the%20controller%2C%20provided%20end-effector%20force%20feedback%20exists.%0AFurthermore%2C%20the%20paper%20provides%20formal%20results%2C%20like%20guarantees%20of%20feasibility%0Afor%20the%20optimization%20problem%2C%20continuity%20of%20the%20controller%20input%20as%20a%20function%0Aof%20the%20configuration%2C%20and%20Lyapunov%20stability.%20In%20addition%2C%20it%20presents%0Aexperimental%20results%20in%20various%20situations%20to%20demonstrate%20its%20practical%0Aapplicability%20on%20an%20aerial%20manipulator%20platform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.07709v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Aerial%2520Manipulator%2520Maneuvering%2520and%2520Force%2520Exertion%2520via%2520Control%250A%2520%2520Barrier%2520Functions%26entry.906535625%3DDimitris%2520Chaikalis%2520and%2520Vinicius%2520Goncalves%2520and%2520Nikolaos%2520Evangeliou%2520and%2520Anthony%2520Tzes%2520and%2520Farshad%2520Khorrami%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520a%2520safe%2520control%2520strategy%2520for%2520application%2520of%2520forces%2520to%250Aan%2520external%2520object%2520using%2520a%2520dexterous%2520robotic%2520arm%2520mounted%2520on%2520an%2520unmanned%2520Aerial%250AVehicle%2520%2528UAV%2529.%2520A%2520hybrid%2520force-motion%2520controller%2520has%2520been%2520developed%2520for%2520this%250Apurpose.%2520This%2520controller%2520employs%2520a%2520Control%2520Barrier%2520Function%2520%2528CBF%2529%2520constraint%250Awithin%2520an%2520optimization%2520framework%2520based%2520on%2520Quadratic%2520Programming%2520%2528QP%2529.%2520The%250Aobjective%2520is%2520to%2520enforce%2520a%2520predefined%2520relationship%2520between%2520the%2520end-effector%2527s%250Aapproach%2520motion%2520and%2520its%2520alignment%2520with%2520the%2520surface%252C%2520thereby%2520ensuring%2520safe%250Aoperational%2520dynamics.%2520No%2520compliance%2520model%2520for%2520the%2520environment%2520is%2520necessary%2520to%250Aimplement%2520the%2520controller%252C%2520provided%2520end-effector%2520force%2520feedback%2520exists.%250AFurthermore%252C%2520the%2520paper%2520provides%2520formal%2520results%252C%2520like%2520guarantees%2520of%2520feasibility%250Afor%2520the%2520optimization%2520problem%252C%2520continuity%2520of%2520the%2520controller%2520input%2520as%2520a%2520function%250Aof%2520the%2520configuration%252C%2520and%2520Lyapunov%2520stability.%2520In%2520addition%252C%2520it%2520presents%250Aexperimental%2520results%2520in%2520various%2520situations%2520to%2520demonstrate%2520its%2520practical%250Aapplicability%2520on%2520an%2520aerial%2520manipulator%2520platform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.07709v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Aerial%20Manipulator%20Maneuvering%20and%20Force%20Exertion%20via%20Control%0A%20%20Barrier%20Functions&entry.906535625=Dimitris%20Chaikalis%20and%20Vinicius%20Goncalves%20and%20Nikolaos%20Evangeliou%20and%20Anthony%20Tzes%20and%20Farshad%20Khorrami&entry.1292438233=%20%20This%20article%20introduces%20a%20safe%20control%20strategy%20for%20application%20of%20forces%20to%0Aan%20external%20object%20using%20a%20dexterous%20robotic%20arm%20mounted%20on%20an%20unmanned%20Aerial%0AVehicle%20%28UAV%29.%20A%20hybrid%20force-motion%20controller%20has%20been%20developed%20for%20this%0Apurpose.%20This%20controller%20employs%20a%20Control%20Barrier%20Function%20%28CBF%29%20constraint%0Awithin%20an%20optimization%20framework%20based%20on%20Quadratic%20Programming%20%28QP%29.%20The%0Aobjective%20is%20to%20enforce%20a%20predefined%20relationship%20between%20the%20end-effector%27s%0Aapproach%20motion%20and%20its%20alignment%20with%20the%20surface%2C%20thereby%20ensuring%20safe%0Aoperational%20dynamics.%20No%20compliance%20model%20for%20the%20environment%20is%20necessary%20to%0Aimplement%20the%20controller%2C%20provided%20end-effector%20force%20feedback%20exists.%0AFurthermore%2C%20the%20paper%20provides%20formal%20results%2C%20like%20guarantees%20of%20feasibility%0Afor%20the%20optimization%20problem%2C%20continuity%20of%20the%20controller%20input%20as%20a%20function%0Aof%20the%20configuration%2C%20and%20Lyapunov%20stability.%20In%20addition%2C%20it%20presents%0Aexperimental%20results%20in%20various%20situations%20to%20demonstrate%20its%20practical%0Aapplicability%20on%20an%20aerial%20manipulator%20platform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.07709v3&entry.124074799=Read"},
{"title": "Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural\n  bandits Coupled with Transformers", "author": "Xiaoqiang Lin and Zhaoxuan Wu and Zhongxiang Dai and Wenyang Hu and Yao Shu and See-Kiong Ng and Patrick Jaillet and Bryan Kian Hsiang Low", "abstract": "  Large language models (LLMs) have shown remarkable instruction-following\ncapabilities and achieved impressive performances in various applications.\nHowever, the performances of LLMs depend heavily on the instructions given to\nthem, which are typically manually tuned with substantial human efforts. Recent\nwork has used the query-efficient Bayesian optimization (BO) algorithm to\nautomatically optimize the instructions given to black-box LLMs. However, BO\nusually falls short when optimizing highly sophisticated (e.g.,\nhigh-dimensional) objective functions, such as the functions mapping an\ninstruction to the performance of an LLM. This is mainly due to the limited\nexpressive power of the Gaussian process (GP) which is used by BO as a\nsurrogate to model the objective function. Meanwhile, it has been repeatedly\nshown that neural networks (NNs), especially pre-trained transformers, possess\nstrong expressive power and can model highly complex functions. So, we adopt a\nneural bandit algorithm which replaces the GP in BO by an NN surrogate to\noptimize instructions for black-box LLMs. More importantly, the neural bandit\nalgorithm allows us to naturally couple the NN surrogate with the hidden\nrepresentation learned by a pre-trained transformer (i.e., an open-source LLM),\nwhich significantly boosts its performance. These motivate us to propose our\nINSTruction optimization usIng Neural bandits Coupled with Transformers\n(INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use\nextensive experiments to show that INSTINCT consistently outperforms baselines\nin different tasks, e.g., various instruction induction tasks and the task of\nimproving zero-shot chain-of-thought instructions. Our code is available at\nhttps://github.com/xqlin98/INSTINCT.\n", "link": "http://arxiv.org/abs/2310.02905v2", "date": "2024-05-31", "relevancy": 2.0019, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.503}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5009}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Use%20Your%20INSTINCT%3A%20INSTruction%20optimization%20for%20LLMs%20usIng%20Neural%0A%20%20bandits%20Coupled%20with%20Transformers&body=Title%3A%20Use%20Your%20INSTINCT%3A%20INSTruction%20optimization%20for%20LLMs%20usIng%20Neural%0A%20%20bandits%20Coupled%20with%20Transformers%0AAuthor%3A%20Xiaoqiang%20Lin%20and%20Zhaoxuan%20Wu%20and%20Zhongxiang%20Dai%20and%20Wenyang%20Hu%20and%20Yao%20Shu%20and%20See-Kiong%20Ng%20and%20Patrick%20Jaillet%20and%20Bryan%20Kian%20Hsiang%20Low%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20instruction-following%0Acapabilities%20and%20achieved%20impressive%20performances%20in%20various%20applications.%0AHowever%2C%20the%20performances%20of%20LLMs%20depend%20heavily%20on%20the%20instructions%20given%20to%0Athem%2C%20which%20are%20typically%20manually%20tuned%20with%20substantial%20human%20efforts.%20Recent%0Awork%20has%20used%20the%20query-efficient%20Bayesian%20optimization%20%28BO%29%20algorithm%20to%0Aautomatically%20optimize%20the%20instructions%20given%20to%20black-box%20LLMs.%20However%2C%20BO%0Ausually%20falls%20short%20when%20optimizing%20highly%20sophisticated%20%28e.g.%2C%0Ahigh-dimensional%29%20objective%20functions%2C%20such%20as%20the%20functions%20mapping%20an%0Ainstruction%20to%20the%20performance%20of%20an%20LLM.%20This%20is%20mainly%20due%20to%20the%20limited%0Aexpressive%20power%20of%20the%20Gaussian%20process%20%28GP%29%20which%20is%20used%20by%20BO%20as%20a%0Asurrogate%20to%20model%20the%20objective%20function.%20Meanwhile%2C%20it%20has%20been%20repeatedly%0Ashown%20that%20neural%20networks%20%28NNs%29%2C%20especially%20pre-trained%20transformers%2C%20possess%0Astrong%20expressive%20power%20and%20can%20model%20highly%20complex%20functions.%20So%2C%20we%20adopt%20a%0Aneural%20bandit%20algorithm%20which%20replaces%20the%20GP%20in%20BO%20by%20an%20NN%20surrogate%20to%0Aoptimize%20instructions%20for%20black-box%20LLMs.%20More%20importantly%2C%20the%20neural%20bandit%0Aalgorithm%20allows%20us%20to%20naturally%20couple%20the%20NN%20surrogate%20with%20the%20hidden%0Arepresentation%20learned%20by%20a%20pre-trained%20transformer%20%28i.e.%2C%20an%20open-source%20LLM%29%2C%0Awhich%20significantly%20boosts%20its%20performance.%20These%20motivate%20us%20to%20propose%20our%0AINSTruction%20optimization%20usIng%20Neural%20bandits%20Coupled%20with%20Transformers%0A%28INSTINCT%29%20algorithm.%20We%20perform%20instruction%20optimization%20for%20ChatGPT%20and%20use%0Aextensive%20experiments%20to%20show%20that%20INSTINCT%20consistently%20outperforms%20baselines%0Ain%20different%20tasks%2C%20e.g.%2C%20various%20instruction%20induction%20tasks%20and%20the%20task%20of%0Aimproving%20zero-shot%20chain-of-thought%20instructions.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/xqlin98/INSTINCT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02905v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUse%2520Your%2520INSTINCT%253A%2520INSTruction%2520optimization%2520for%2520LLMs%2520usIng%2520Neural%250A%2520%2520bandits%2520Coupled%2520with%2520Transformers%26entry.906535625%3DXiaoqiang%2520Lin%2520and%2520Zhaoxuan%2520Wu%2520and%2520Zhongxiang%2520Dai%2520and%2520Wenyang%2520Hu%2520and%2520Yao%2520Shu%2520and%2520See-Kiong%2520Ng%2520and%2520Patrick%2520Jaillet%2520and%2520Bryan%2520Kian%2520Hsiang%2520Low%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520instruction-following%250Acapabilities%2520and%2520achieved%2520impressive%2520performances%2520in%2520various%2520applications.%250AHowever%252C%2520the%2520performances%2520of%2520LLMs%2520depend%2520heavily%2520on%2520the%2520instructions%2520given%2520to%250Athem%252C%2520which%2520are%2520typically%2520manually%2520tuned%2520with%2520substantial%2520human%2520efforts.%2520Recent%250Awork%2520has%2520used%2520the%2520query-efficient%2520Bayesian%2520optimization%2520%2528BO%2529%2520algorithm%2520to%250Aautomatically%2520optimize%2520the%2520instructions%2520given%2520to%2520black-box%2520LLMs.%2520However%252C%2520BO%250Ausually%2520falls%2520short%2520when%2520optimizing%2520highly%2520sophisticated%2520%2528e.g.%252C%250Ahigh-dimensional%2529%2520objective%2520functions%252C%2520such%2520as%2520the%2520functions%2520mapping%2520an%250Ainstruction%2520to%2520the%2520performance%2520of%2520an%2520LLM.%2520This%2520is%2520mainly%2520due%2520to%2520the%2520limited%250Aexpressive%2520power%2520of%2520the%2520Gaussian%2520process%2520%2528GP%2529%2520which%2520is%2520used%2520by%2520BO%2520as%2520a%250Asurrogate%2520to%2520model%2520the%2520objective%2520function.%2520Meanwhile%252C%2520it%2520has%2520been%2520repeatedly%250Ashown%2520that%2520neural%2520networks%2520%2528NNs%2529%252C%2520especially%2520pre-trained%2520transformers%252C%2520possess%250Astrong%2520expressive%2520power%2520and%2520can%2520model%2520highly%2520complex%2520functions.%2520So%252C%2520we%2520adopt%2520a%250Aneural%2520bandit%2520algorithm%2520which%2520replaces%2520the%2520GP%2520in%2520BO%2520by%2520an%2520NN%2520surrogate%2520to%250Aoptimize%2520instructions%2520for%2520black-box%2520LLMs.%2520More%2520importantly%252C%2520the%2520neural%2520bandit%250Aalgorithm%2520allows%2520us%2520to%2520naturally%2520couple%2520the%2520NN%2520surrogate%2520with%2520the%2520hidden%250Arepresentation%2520learned%2520by%2520a%2520pre-trained%2520transformer%2520%2528i.e.%252C%2520an%2520open-source%2520LLM%2529%252C%250Awhich%2520significantly%2520boosts%2520its%2520performance.%2520These%2520motivate%2520us%2520to%2520propose%2520our%250AINSTruction%2520optimization%2520usIng%2520Neural%2520bandits%2520Coupled%2520with%2520Transformers%250A%2528INSTINCT%2529%2520algorithm.%2520We%2520perform%2520instruction%2520optimization%2520for%2520ChatGPT%2520and%2520use%250Aextensive%2520experiments%2520to%2520show%2520that%2520INSTINCT%2520consistently%2520outperforms%2520baselines%250Ain%2520different%2520tasks%252C%2520e.g.%252C%2520various%2520instruction%2520induction%2520tasks%2520and%2520the%2520task%2520of%250Aimproving%2520zero-shot%2520chain-of-thought%2520instructions.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/xqlin98/INSTINCT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02905v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Use%20Your%20INSTINCT%3A%20INSTruction%20optimization%20for%20LLMs%20usIng%20Neural%0A%20%20bandits%20Coupled%20with%20Transformers&entry.906535625=Xiaoqiang%20Lin%20and%20Zhaoxuan%20Wu%20and%20Zhongxiang%20Dai%20and%20Wenyang%20Hu%20and%20Yao%20Shu%20and%20See-Kiong%20Ng%20and%20Patrick%20Jaillet%20and%20Bryan%20Kian%20Hsiang%20Low&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20instruction-following%0Acapabilities%20and%20achieved%20impressive%20performances%20in%20various%20applications.%0AHowever%2C%20the%20performances%20of%20LLMs%20depend%20heavily%20on%20the%20instructions%20given%20to%0Athem%2C%20which%20are%20typically%20manually%20tuned%20with%20substantial%20human%20efforts.%20Recent%0Awork%20has%20used%20the%20query-efficient%20Bayesian%20optimization%20%28BO%29%20algorithm%20to%0Aautomatically%20optimize%20the%20instructions%20given%20to%20black-box%20LLMs.%20However%2C%20BO%0Ausually%20falls%20short%20when%20optimizing%20highly%20sophisticated%20%28e.g.%2C%0Ahigh-dimensional%29%20objective%20functions%2C%20such%20as%20the%20functions%20mapping%20an%0Ainstruction%20to%20the%20performance%20of%20an%20LLM.%20This%20is%20mainly%20due%20to%20the%20limited%0Aexpressive%20power%20of%20the%20Gaussian%20process%20%28GP%29%20which%20is%20used%20by%20BO%20as%20a%0Asurrogate%20to%20model%20the%20objective%20function.%20Meanwhile%2C%20it%20has%20been%20repeatedly%0Ashown%20that%20neural%20networks%20%28NNs%29%2C%20especially%20pre-trained%20transformers%2C%20possess%0Astrong%20expressive%20power%20and%20can%20model%20highly%20complex%20functions.%20So%2C%20we%20adopt%20a%0Aneural%20bandit%20algorithm%20which%20replaces%20the%20GP%20in%20BO%20by%20an%20NN%20surrogate%20to%0Aoptimize%20instructions%20for%20black-box%20LLMs.%20More%20importantly%2C%20the%20neural%20bandit%0Aalgorithm%20allows%20us%20to%20naturally%20couple%20the%20NN%20surrogate%20with%20the%20hidden%0Arepresentation%20learned%20by%20a%20pre-trained%20transformer%20%28i.e.%2C%20an%20open-source%20LLM%29%2C%0Awhich%20significantly%20boosts%20its%20performance.%20These%20motivate%20us%20to%20propose%20our%0AINSTruction%20optimization%20usIng%20Neural%20bandits%20Coupled%20with%20Transformers%0A%28INSTINCT%29%20algorithm.%20We%20perform%20instruction%20optimization%20for%20ChatGPT%20and%20use%0Aextensive%20experiments%20to%20show%20that%20INSTINCT%20consistently%20outperforms%20baselines%0Ain%20different%20tasks%2C%20e.g.%2C%20various%20instruction%20induction%20tasks%20and%20the%20task%20of%0Aimproving%20zero-shot%20chain-of-thought%20instructions.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/xqlin98/INSTINCT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02905v2&entry.124074799=Read"},
{"title": "einspace: Searching for Neural Architectures from Fundamental Operations", "author": "Linus Ericsson and Miguel Espinosa and Chenhongyi Yang and Antreas Antoniou and Amos Storkey and Shay B. Cohen and Steven McDonagh and Elliot J. Crowley", "abstract": "  Neural architecture search (NAS) finds high performing networks for a given\ntask. Yet the results of NAS are fairly prosaic; they did not e.g. create a\nshift from convolutional structures to transformers. This is not least because\nthe search spaces in NAS often aren't diverse enough to include such\ntransformations a priori. Instead, for NAS to provide greater potential for\nfundamental design shifts, we need a novel expressive search space design which\nis built from more fundamental operations. To this end, we introduce einspace,\na search space based on a parameterised probabilistic context-free grammar. Our\nspace is versatile, supporting architectures of various sizes and complexities,\nwhile also containing diverse network operations which allow it to model\nconvolutions, attention components and more. It contains many existing\ncompetitive architectures, and provides flexibility for discovering new ones.\nUsing this search space, we perform experiments to find novel architectures as\nwell as improvements on existing ones on the diverse Unseen NAS datasets. We\nshow that competitive architectures can be obtained by searching from scratch,\nand we consistently find large improvements when initialising the search with\nstrong baselines. We believe that this work is an important advancement towards\na transformative NAS paradigm where search space expressivity and strategic\nsearch initialisation play key roles.\n", "link": "http://arxiv.org/abs/2405.20838v1", "date": "2024-05-31", "relevancy": 1.9943, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.53}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5082}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4764}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20einspace%3A%20Searching%20for%20Neural%20Architectures%20from%20Fundamental%20Operations&body=Title%3A%20einspace%3A%20Searching%20for%20Neural%20Architectures%20from%20Fundamental%20Operations%0AAuthor%3A%20Linus%20Ericsson%20and%20Miguel%20Espinosa%20and%20Chenhongyi%20Yang%20and%20Antreas%20Antoniou%20and%20Amos%20Storkey%20and%20Shay%20B.%20Cohen%20and%20Steven%20McDonagh%20and%20Elliot%20J.%20Crowley%0AAbstract%3A%20%20%20Neural%20architecture%20search%20%28NAS%29%20finds%20high%20performing%20networks%20for%20a%20given%0Atask.%20Yet%20the%20results%20of%20NAS%20are%20fairly%20prosaic%3B%20they%20did%20not%20e.g.%20create%20a%0Ashift%20from%20convolutional%20structures%20to%20transformers.%20This%20is%20not%20least%20because%0Athe%20search%20spaces%20in%20NAS%20often%20aren%27t%20diverse%20enough%20to%20include%20such%0Atransformations%20a%20priori.%20Instead%2C%20for%20NAS%20to%20provide%20greater%20potential%20for%0Afundamental%20design%20shifts%2C%20we%20need%20a%20novel%20expressive%20search%20space%20design%20which%0Ais%20built%20from%20more%20fundamental%20operations.%20To%20this%20end%2C%20we%20introduce%20einspace%2C%0Aa%20search%20space%20based%20on%20a%20parameterised%20probabilistic%20context-free%20grammar.%20Our%0Aspace%20is%20versatile%2C%20supporting%20architectures%20of%20various%20sizes%20and%20complexities%2C%0Awhile%20also%20containing%20diverse%20network%20operations%20which%20allow%20it%20to%20model%0Aconvolutions%2C%20attention%20components%20and%20more.%20It%20contains%20many%20existing%0Acompetitive%20architectures%2C%20and%20provides%20flexibility%20for%20discovering%20new%20ones.%0AUsing%20this%20search%20space%2C%20we%20perform%20experiments%20to%20find%20novel%20architectures%20as%0Awell%20as%20improvements%20on%20existing%20ones%20on%20the%20diverse%20Unseen%20NAS%20datasets.%20We%0Ashow%20that%20competitive%20architectures%20can%20be%20obtained%20by%20searching%20from%20scratch%2C%0Aand%20we%20consistently%20find%20large%20improvements%20when%20initialising%20the%20search%20with%0Astrong%20baselines.%20We%20believe%20that%20this%20work%20is%20an%20important%20advancement%20towards%0Aa%20transformative%20NAS%20paradigm%20where%20search%20space%20expressivity%20and%20strategic%0Asearch%20initialisation%20play%20key%20roles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Deinspace%253A%2520Searching%2520for%2520Neural%2520Architectures%2520from%2520Fundamental%2520Operations%26entry.906535625%3DLinus%2520Ericsson%2520and%2520Miguel%2520Espinosa%2520and%2520Chenhongyi%2520Yang%2520and%2520Antreas%2520Antoniou%2520and%2520Amos%2520Storkey%2520and%2520Shay%2520B.%2520Cohen%2520and%2520Steven%2520McDonagh%2520and%2520Elliot%2520J.%2520Crowley%26entry.1292438233%3D%2520%2520Neural%2520architecture%2520search%2520%2528NAS%2529%2520finds%2520high%2520performing%2520networks%2520for%2520a%2520given%250Atask.%2520Yet%2520the%2520results%2520of%2520NAS%2520are%2520fairly%2520prosaic%253B%2520they%2520did%2520not%2520e.g.%2520create%2520a%250Ashift%2520from%2520convolutional%2520structures%2520to%2520transformers.%2520This%2520is%2520not%2520least%2520because%250Athe%2520search%2520spaces%2520in%2520NAS%2520often%2520aren%2527t%2520diverse%2520enough%2520to%2520include%2520such%250Atransformations%2520a%2520priori.%2520Instead%252C%2520for%2520NAS%2520to%2520provide%2520greater%2520potential%2520for%250Afundamental%2520design%2520shifts%252C%2520we%2520need%2520a%2520novel%2520expressive%2520search%2520space%2520design%2520which%250Ais%2520built%2520from%2520more%2520fundamental%2520operations.%2520To%2520this%2520end%252C%2520we%2520introduce%2520einspace%252C%250Aa%2520search%2520space%2520based%2520on%2520a%2520parameterised%2520probabilistic%2520context-free%2520grammar.%2520Our%250Aspace%2520is%2520versatile%252C%2520supporting%2520architectures%2520of%2520various%2520sizes%2520and%2520complexities%252C%250Awhile%2520also%2520containing%2520diverse%2520network%2520operations%2520which%2520allow%2520it%2520to%2520model%250Aconvolutions%252C%2520attention%2520components%2520and%2520more.%2520It%2520contains%2520many%2520existing%250Acompetitive%2520architectures%252C%2520and%2520provides%2520flexibility%2520for%2520discovering%2520new%2520ones.%250AUsing%2520this%2520search%2520space%252C%2520we%2520perform%2520experiments%2520to%2520find%2520novel%2520architectures%2520as%250Awell%2520as%2520improvements%2520on%2520existing%2520ones%2520on%2520the%2520diverse%2520Unseen%2520NAS%2520datasets.%2520We%250Ashow%2520that%2520competitive%2520architectures%2520can%2520be%2520obtained%2520by%2520searching%2520from%2520scratch%252C%250Aand%2520we%2520consistently%2520find%2520large%2520improvements%2520when%2520initialising%2520the%2520search%2520with%250Astrong%2520baselines.%2520We%2520believe%2520that%2520this%2520work%2520is%2520an%2520important%2520advancement%2520towards%250Aa%2520transformative%2520NAS%2520paradigm%2520where%2520search%2520space%2520expressivity%2520and%2520strategic%250Asearch%2520initialisation%2520play%2520key%2520roles.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=einspace%3A%20Searching%20for%20Neural%20Architectures%20from%20Fundamental%20Operations&entry.906535625=Linus%20Ericsson%20and%20Miguel%20Espinosa%20and%20Chenhongyi%20Yang%20and%20Antreas%20Antoniou%20and%20Amos%20Storkey%20and%20Shay%20B.%20Cohen%20and%20Steven%20McDonagh%20and%20Elliot%20J.%20Crowley&entry.1292438233=%20%20Neural%20architecture%20search%20%28NAS%29%20finds%20high%20performing%20networks%20for%20a%20given%0Atask.%20Yet%20the%20results%20of%20NAS%20are%20fairly%20prosaic%3B%20they%20did%20not%20e.g.%20create%20a%0Ashift%20from%20convolutional%20structures%20to%20transformers.%20This%20is%20not%20least%20because%0Athe%20search%20spaces%20in%20NAS%20often%20aren%27t%20diverse%20enough%20to%20include%20such%0Atransformations%20a%20priori.%20Instead%2C%20for%20NAS%20to%20provide%20greater%20potential%20for%0Afundamental%20design%20shifts%2C%20we%20need%20a%20novel%20expressive%20search%20space%20design%20which%0Ais%20built%20from%20more%20fundamental%20operations.%20To%20this%20end%2C%20we%20introduce%20einspace%2C%0Aa%20search%20space%20based%20on%20a%20parameterised%20probabilistic%20context-free%20grammar.%20Our%0Aspace%20is%20versatile%2C%20supporting%20architectures%20of%20various%20sizes%20and%20complexities%2C%0Awhile%20also%20containing%20diverse%20network%20operations%20which%20allow%20it%20to%20model%0Aconvolutions%2C%20attention%20components%20and%20more.%20It%20contains%20many%20existing%0Acompetitive%20architectures%2C%20and%20provides%20flexibility%20for%20discovering%20new%20ones.%0AUsing%20this%20search%20space%2C%20we%20perform%20experiments%20to%20find%20novel%20architectures%20as%0Awell%20as%20improvements%20on%20existing%20ones%20on%20the%20diverse%20Unseen%20NAS%20datasets.%20We%0Ashow%20that%20competitive%20architectures%20can%20be%20obtained%20by%20searching%20from%20scratch%2C%0Aand%20we%20consistently%20find%20large%20improvements%20when%20initialising%20the%20search%20with%0Astrong%20baselines.%20We%20believe%20that%20this%20work%20is%20an%20important%20advancement%20towards%0Aa%20transformative%20NAS%20paradigm%20where%20search%20space%20expressivity%20and%20strategic%0Asearch%20initialisation%20play%20key%20roles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20838v1&entry.124074799=Read"},
{"title": "Collective Variable Free Transition Path Sampling with Generative Flow\n  Network", "author": "Kiyoung Seong and Seonghyun Park and Seonghwan Kim and Woo Youn Kim and Sungsoo Ahn", "abstract": "  Understanding transition paths between meta-stable states in molecular\nsystems is fundamental for material design and drug discovery. However,\nsampling these paths via molecular dynamics simulations is computationally\nprohibitive due to the high-energy barriers between the meta-stable states.\nRecent machine learning approaches are often restricted to simple systems or\nrely on collective variables (CVs) extracted from expensive domain knowledge.\nIn this work, we propose to leverage generative flow networks (GFlowNets) to\nsample transition paths without relying on CVs. We reformulate the problem as\namortized energy-based sampling over molecular trajectories and train a bias\npotential by minimizing the squared log-ratio between the target distribution\nand the generator, derived from the flow matching objective of GFlowNets. Our\nevaluation on three proteins (Alanine Dipeptide, Polyproline, and Chignolin)\ndemonstrates that our approach, called TPS-GFN, generates more realistic and\ndiverse transition paths than the previous CV-free machine learning approach.\n", "link": "http://arxiv.org/abs/2405.19961v2", "date": "2024-05-31", "relevancy": 1.9902, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5515}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.493}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4806}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collective%20Variable%20Free%20Transition%20Path%20Sampling%20with%20Generative%20Flow%0A%20%20Network&body=Title%3A%20Collective%20Variable%20Free%20Transition%20Path%20Sampling%20with%20Generative%20Flow%0A%20%20Network%0AAuthor%3A%20Kiyoung%20Seong%20and%20Seonghyun%20Park%20and%20Seonghwan%20Kim%20and%20Woo%20Youn%20Kim%20and%20Sungsoo%20Ahn%0AAbstract%3A%20%20%20Understanding%20transition%20paths%20between%20meta-stable%20states%20in%20molecular%0Asystems%20is%20fundamental%20for%20material%20design%20and%20drug%20discovery.%20However%2C%0Asampling%20these%20paths%20via%20molecular%20dynamics%20simulations%20is%20computationally%0Aprohibitive%20due%20to%20the%20high-energy%20barriers%20between%20the%20meta-stable%20states.%0ARecent%20machine%20learning%20approaches%20are%20often%20restricted%20to%20simple%20systems%20or%0Arely%20on%20collective%20variables%20%28CVs%29%20extracted%20from%20expensive%20domain%20knowledge.%0AIn%20this%20work%2C%20we%20propose%20to%20leverage%20generative%20flow%20networks%20%28GFlowNets%29%20to%0Asample%20transition%20paths%20without%20relying%20on%20CVs.%20We%20reformulate%20the%20problem%20as%0Aamortized%20energy-based%20sampling%20over%20molecular%20trajectories%20and%20train%20a%20bias%0Apotential%20by%20minimizing%20the%20squared%20log-ratio%20between%20the%20target%20distribution%0Aand%20the%20generator%2C%20derived%20from%20the%20flow%20matching%20objective%20of%20GFlowNets.%20Our%0Aevaluation%20on%20three%20proteins%20%28Alanine%20Dipeptide%2C%20Polyproline%2C%20and%20Chignolin%29%0Ademonstrates%20that%20our%20approach%2C%20called%20TPS-GFN%2C%20generates%20more%20realistic%20and%0Adiverse%20transition%20paths%20than%20the%20previous%20CV-free%20machine%20learning%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19961v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollective%2520Variable%2520Free%2520Transition%2520Path%2520Sampling%2520with%2520Generative%2520Flow%250A%2520%2520Network%26entry.906535625%3DKiyoung%2520Seong%2520and%2520Seonghyun%2520Park%2520and%2520Seonghwan%2520Kim%2520and%2520Woo%2520Youn%2520Kim%2520and%2520Sungsoo%2520Ahn%26entry.1292438233%3D%2520%2520Understanding%2520transition%2520paths%2520between%2520meta-stable%2520states%2520in%2520molecular%250Asystems%2520is%2520fundamental%2520for%2520material%2520design%2520and%2520drug%2520discovery.%2520However%252C%250Asampling%2520these%2520paths%2520via%2520molecular%2520dynamics%2520simulations%2520is%2520computationally%250Aprohibitive%2520due%2520to%2520the%2520high-energy%2520barriers%2520between%2520the%2520meta-stable%2520states.%250ARecent%2520machine%2520learning%2520approaches%2520are%2520often%2520restricted%2520to%2520simple%2520systems%2520or%250Arely%2520on%2520collective%2520variables%2520%2528CVs%2529%2520extracted%2520from%2520expensive%2520domain%2520knowledge.%250AIn%2520this%2520work%252C%2520we%2520propose%2520to%2520leverage%2520generative%2520flow%2520networks%2520%2528GFlowNets%2529%2520to%250Asample%2520transition%2520paths%2520without%2520relying%2520on%2520CVs.%2520We%2520reformulate%2520the%2520problem%2520as%250Aamortized%2520energy-based%2520sampling%2520over%2520molecular%2520trajectories%2520and%2520train%2520a%2520bias%250Apotential%2520by%2520minimizing%2520the%2520squared%2520log-ratio%2520between%2520the%2520target%2520distribution%250Aand%2520the%2520generator%252C%2520derived%2520from%2520the%2520flow%2520matching%2520objective%2520of%2520GFlowNets.%2520Our%250Aevaluation%2520on%2520three%2520proteins%2520%2528Alanine%2520Dipeptide%252C%2520Polyproline%252C%2520and%2520Chignolin%2529%250Ademonstrates%2520that%2520our%2520approach%252C%2520called%2520TPS-GFN%252C%2520generates%2520more%2520realistic%2520and%250Adiverse%2520transition%2520paths%2520than%2520the%2520previous%2520CV-free%2520machine%2520learning%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19961v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collective%20Variable%20Free%20Transition%20Path%20Sampling%20with%20Generative%20Flow%0A%20%20Network&entry.906535625=Kiyoung%20Seong%20and%20Seonghyun%20Park%20and%20Seonghwan%20Kim%20and%20Woo%20Youn%20Kim%20and%20Sungsoo%20Ahn&entry.1292438233=%20%20Understanding%20transition%20paths%20between%20meta-stable%20states%20in%20molecular%0Asystems%20is%20fundamental%20for%20material%20design%20and%20drug%20discovery.%20However%2C%0Asampling%20these%20paths%20via%20molecular%20dynamics%20simulations%20is%20computationally%0Aprohibitive%20due%20to%20the%20high-energy%20barriers%20between%20the%20meta-stable%20states.%0ARecent%20machine%20learning%20approaches%20are%20often%20restricted%20to%20simple%20systems%20or%0Arely%20on%20collective%20variables%20%28CVs%29%20extracted%20from%20expensive%20domain%20knowledge.%0AIn%20this%20work%2C%20we%20propose%20to%20leverage%20generative%20flow%20networks%20%28GFlowNets%29%20to%0Asample%20transition%20paths%20without%20relying%20on%20CVs.%20We%20reformulate%20the%20problem%20as%0Aamortized%20energy-based%20sampling%20over%20molecular%20trajectories%20and%20train%20a%20bias%0Apotential%20by%20minimizing%20the%20squared%20log-ratio%20between%20the%20target%20distribution%0Aand%20the%20generator%2C%20derived%20from%20the%20flow%20matching%20objective%20of%20GFlowNets.%20Our%0Aevaluation%20on%20three%20proteins%20%28Alanine%20Dipeptide%2C%20Polyproline%2C%20and%20Chignolin%29%0Ademonstrates%20that%20our%20approach%2C%20called%20TPS-GFN%2C%20generates%20more%20realistic%20and%0Adiverse%20transition%20paths%20than%20the%20previous%20CV-free%20machine%20learning%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19961v2&entry.124074799=Read"},
{"title": "Target Networks and Over-parameterization Stabilize Off-policy\n  Bootstrapping with Function Approximation", "author": "Fengdi Che and Chenjun Xiao and Jincheng Mei and Bo Dai and Ramki Gummadi and Oscar A Ramirez and Christopher K Harris and A. Rupam Mahmood and Dale Schuurmans", "abstract": "  We prove that the combination of a target network and over-parameterized\nlinear function approximation establishes a weaker convergence condition for\nbootstrapped value estimation in certain cases, even with off-policy data. Our\ncondition is naturally satisfied for expected updates over the entire\nstate-action space or learning with a batch of complete trajectories from\nepisodic Markov decision processes. Notably, using only a target network or an\nover-parameterized model does not provide such a convergence guarantee.\nAdditionally, we extend our results to learning with truncated trajectories,\nshowing that convergence is achievable for all tasks with minor modifications,\nakin to value truncation for the final states in trajectories. Our primary\nresult focuses on temporal difference estimation for prediction, providing\nhigh-probability value estimation error bounds and empirical analysis on\nBaird's counterexample and a Four-room task. Furthermore, we explore the\ncontrol setting, demonstrating that similar convergence conditions apply to\nQ-learning.\n", "link": "http://arxiv.org/abs/2405.21043v1", "date": "2024-05-31", "relevancy": 1.9828, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5247}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4924}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Target%20Networks%20and%20Over-parameterization%20Stabilize%20Off-policy%0A%20%20Bootstrapping%20with%20Function%20Approximation&body=Title%3A%20Target%20Networks%20and%20Over-parameterization%20Stabilize%20Off-policy%0A%20%20Bootstrapping%20with%20Function%20Approximation%0AAuthor%3A%20Fengdi%20Che%20and%20Chenjun%20Xiao%20and%20Jincheng%20Mei%20and%20Bo%20Dai%20and%20Ramki%20Gummadi%20and%20Oscar%20A%20Ramirez%20and%20Christopher%20K%20Harris%20and%20A.%20Rupam%20Mahmood%20and%20Dale%20Schuurmans%0AAbstract%3A%20%20%20We%20prove%20that%20the%20combination%20of%20a%20target%20network%20and%20over-parameterized%0Alinear%20function%20approximation%20establishes%20a%20weaker%20convergence%20condition%20for%0Abootstrapped%20value%20estimation%20in%20certain%20cases%2C%20even%20with%20off-policy%20data.%20Our%0Acondition%20is%20naturally%20satisfied%20for%20expected%20updates%20over%20the%20entire%0Astate-action%20space%20or%20learning%20with%20a%20batch%20of%20complete%20trajectories%20from%0Aepisodic%20Markov%20decision%20processes.%20Notably%2C%20using%20only%20a%20target%20network%20or%20an%0Aover-parameterized%20model%20does%20not%20provide%20such%20a%20convergence%20guarantee.%0AAdditionally%2C%20we%20extend%20our%20results%20to%20learning%20with%20truncated%20trajectories%2C%0Ashowing%20that%20convergence%20is%20achievable%20for%20all%20tasks%20with%20minor%20modifications%2C%0Aakin%20to%20value%20truncation%20for%20the%20final%20states%20in%20trajectories.%20Our%20primary%0Aresult%20focuses%20on%20temporal%20difference%20estimation%20for%20prediction%2C%20providing%0Ahigh-probability%20value%20estimation%20error%20bounds%20and%20empirical%20analysis%20on%0ABaird%27s%20counterexample%20and%20a%20Four-room%20task.%20Furthermore%2C%20we%20explore%20the%0Acontrol%20setting%2C%20demonstrating%20that%20similar%20convergence%20conditions%20apply%20to%0AQ-learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTarget%2520Networks%2520and%2520Over-parameterization%2520Stabilize%2520Off-policy%250A%2520%2520Bootstrapping%2520with%2520Function%2520Approximation%26entry.906535625%3DFengdi%2520Che%2520and%2520Chenjun%2520Xiao%2520and%2520Jincheng%2520Mei%2520and%2520Bo%2520Dai%2520and%2520Ramki%2520Gummadi%2520and%2520Oscar%2520A%2520Ramirez%2520and%2520Christopher%2520K%2520Harris%2520and%2520A.%2520Rupam%2520Mahmood%2520and%2520Dale%2520Schuurmans%26entry.1292438233%3D%2520%2520We%2520prove%2520that%2520the%2520combination%2520of%2520a%2520target%2520network%2520and%2520over-parameterized%250Alinear%2520function%2520approximation%2520establishes%2520a%2520weaker%2520convergence%2520condition%2520for%250Abootstrapped%2520value%2520estimation%2520in%2520certain%2520cases%252C%2520even%2520with%2520off-policy%2520data.%2520Our%250Acondition%2520is%2520naturally%2520satisfied%2520for%2520expected%2520updates%2520over%2520the%2520entire%250Astate-action%2520space%2520or%2520learning%2520with%2520a%2520batch%2520of%2520complete%2520trajectories%2520from%250Aepisodic%2520Markov%2520decision%2520processes.%2520Notably%252C%2520using%2520only%2520a%2520target%2520network%2520or%2520an%250Aover-parameterized%2520model%2520does%2520not%2520provide%2520such%2520a%2520convergence%2520guarantee.%250AAdditionally%252C%2520we%2520extend%2520our%2520results%2520to%2520learning%2520with%2520truncated%2520trajectories%252C%250Ashowing%2520that%2520convergence%2520is%2520achievable%2520for%2520all%2520tasks%2520with%2520minor%2520modifications%252C%250Aakin%2520to%2520value%2520truncation%2520for%2520the%2520final%2520states%2520in%2520trajectories.%2520Our%2520primary%250Aresult%2520focuses%2520on%2520temporal%2520difference%2520estimation%2520for%2520prediction%252C%2520providing%250Ahigh-probability%2520value%2520estimation%2520error%2520bounds%2520and%2520empirical%2520analysis%2520on%250ABaird%2527s%2520counterexample%2520and%2520a%2520Four-room%2520task.%2520Furthermore%252C%2520we%2520explore%2520the%250Acontrol%2520setting%252C%2520demonstrating%2520that%2520similar%2520convergence%2520conditions%2520apply%2520to%250AQ-learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Target%20Networks%20and%20Over-parameterization%20Stabilize%20Off-policy%0A%20%20Bootstrapping%20with%20Function%20Approximation&entry.906535625=Fengdi%20Che%20and%20Chenjun%20Xiao%20and%20Jincheng%20Mei%20and%20Bo%20Dai%20and%20Ramki%20Gummadi%20and%20Oscar%20A%20Ramirez%20and%20Christopher%20K%20Harris%20and%20A.%20Rupam%20Mahmood%20and%20Dale%20Schuurmans&entry.1292438233=%20%20We%20prove%20that%20the%20combination%20of%20a%20target%20network%20and%20over-parameterized%0Alinear%20function%20approximation%20establishes%20a%20weaker%20convergence%20condition%20for%0Abootstrapped%20value%20estimation%20in%20certain%20cases%2C%20even%20with%20off-policy%20data.%20Our%0Acondition%20is%20naturally%20satisfied%20for%20expected%20updates%20over%20the%20entire%0Astate-action%20space%20or%20learning%20with%20a%20batch%20of%20complete%20trajectories%20from%0Aepisodic%20Markov%20decision%20processes.%20Notably%2C%20using%20only%20a%20target%20network%20or%20an%0Aover-parameterized%20model%20does%20not%20provide%20such%20a%20convergence%20guarantee.%0AAdditionally%2C%20we%20extend%20our%20results%20to%20learning%20with%20truncated%20trajectories%2C%0Ashowing%20that%20convergence%20is%20achievable%20for%20all%20tasks%20with%20minor%20modifications%2C%0Aakin%20to%20value%20truncation%20for%20the%20final%20states%20in%20trajectories.%20Our%20primary%0Aresult%20focuses%20on%20temporal%20difference%20estimation%20for%20prediction%2C%20providing%0Ahigh-probability%20value%20estimation%20error%20bounds%20and%20empirical%20analysis%20on%0ABaird%27s%20counterexample%20and%20a%20Four-room%20task.%20Furthermore%2C%20we%20explore%20the%0Acontrol%20setting%2C%20demonstrating%20that%20similar%20convergence%20conditions%20apply%20to%0AQ-learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21043v1&entry.124074799=Read"},
{"title": "LACIE: Listener-Aware Finetuning for Confidence Calibration in Large\n  Language Models", "author": "Elias Stengel-Eskin and Peter Hase and Mohit Bansal", "abstract": "  When answering questions, LLMs can convey not only an answer, but a level of\nconfidence about the answer being correct. This includes explicit confidence\nmarkers (e.g. giving a numeric score) as well as implicit markers, like an\nauthoritative tone or elaborating with additional knowledge. For LLMs to be\ntrustworthy knowledge sources, the confidence they convey should match their\nactual expertise; however, most current models tend towards overconfidence. To\ncalibrate both implicit and explicit confidence markers, we introduce a\npragmatic, listener-aware finetuning method (LACIE) that models the listener,\nconsidering not only whether an answer is right, but whether it will be\naccepted by a listener. We cast calibration as preference optimization,\ncreating data via a two-agent game, where a speaker model's outputs are judged\nby a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B,\nLlama3-70B) with LACIE, and show that the resulting models are better\ncalibrated w.r.t. a simulated listener. Crucially, these trends transfer to\nhuman listeners, helping them correctly predict model correctness: we conduct a\nhuman evaluation where annotators accept or reject an LLM's answers, finding\nthat training with LACIE results in 47% fewer incorrect answers being accepted\nwhile maintaining the same level of acceptance for correct answers.\nFurthermore, LACIE generalizes to another dataset, resulting in a large\nincrease in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis\nindicates that LACIE leads to a better confidence separation between correct\nand incorrect examples. Qualitatively, we find that a LACIE-trained model\nhedges more and implicitly signals certainty when it is correct by using an\nauthoritative tone or including details. Finally, LACIE finetuning leads to an\nemergent increase in model abstention (e.g. saying \"I don't know\") for answers\nthat are likely wrong.\n", "link": "http://arxiv.org/abs/2405.21028v1", "date": "2024-05-31", "relevancy": 1.98, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.546}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LACIE%3A%20Listener-Aware%20Finetuning%20for%20Confidence%20Calibration%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20LACIE%3A%20Listener-Aware%20Finetuning%20for%20Confidence%20Calibration%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Elias%20Stengel-Eskin%20and%20Peter%20Hase%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20When%20answering%20questions%2C%20LLMs%20can%20convey%20not%20only%20an%20answer%2C%20but%20a%20level%20of%0Aconfidence%20about%20the%20answer%20being%20correct.%20This%20includes%20explicit%20confidence%0Amarkers%20%28e.g.%20giving%20a%20numeric%20score%29%20as%20well%20as%20implicit%20markers%2C%20like%20an%0Aauthoritative%20tone%20or%20elaborating%20with%20additional%20knowledge.%20For%20LLMs%20to%20be%0Atrustworthy%20knowledge%20sources%2C%20the%20confidence%20they%20convey%20should%20match%20their%0Aactual%20expertise%3B%20however%2C%20most%20current%20models%20tend%20towards%20overconfidence.%20To%0Acalibrate%20both%20implicit%20and%20explicit%20confidence%20markers%2C%20we%20introduce%20a%0Apragmatic%2C%20listener-aware%20finetuning%20method%20%28LACIE%29%20that%20models%20the%20listener%2C%0Aconsidering%20not%20only%20whether%20an%20answer%20is%20right%2C%20but%20whether%20it%20will%20be%0Aaccepted%20by%20a%20listener.%20We%20cast%20calibration%20as%20preference%20optimization%2C%0Acreating%20data%20via%20a%20two-agent%20game%2C%20where%20a%20speaker%20model%27s%20outputs%20are%20judged%0Aby%20a%20simulated%20listener.%20We%20then%20finetune%20three%20LLMs%20%28Mistral-7B%2C%20Llama3-8B%2C%0ALlama3-70B%29%20with%20LACIE%2C%20and%20show%20that%20the%20resulting%20models%20are%20better%0Acalibrated%20w.r.t.%20a%20simulated%20listener.%20Crucially%2C%20these%20trends%20transfer%20to%0Ahuman%20listeners%2C%20helping%20them%20correctly%20predict%20model%20correctness%3A%20we%20conduct%20a%0Ahuman%20evaluation%20where%20annotators%20accept%20or%20reject%20an%20LLM%27s%20answers%2C%20finding%0Athat%20training%20with%20LACIE%20results%20in%2047%25%20fewer%20incorrect%20answers%20being%20accepted%0Awhile%20maintaining%20the%20same%20level%20of%20acceptance%20for%20correct%20answers.%0AFurthermore%2C%20LACIE%20generalizes%20to%20another%20dataset%2C%20resulting%20in%20a%20large%0Aincrease%20in%20truthfulness%20on%20TruthfulQA%20when%20trained%20on%20TriviaQA.%20Our%20analysis%0Aindicates%20that%20LACIE%20leads%20to%20a%20better%20confidence%20separation%20between%20correct%0Aand%20incorrect%20examples.%20Qualitatively%2C%20we%20find%20that%20a%20LACIE-trained%20model%0Ahedges%20more%20and%20implicitly%20signals%20certainty%20when%20it%20is%20correct%20by%20using%20an%0Aauthoritative%20tone%20or%20including%20details.%20Finally%2C%20LACIE%20finetuning%20leads%20to%20an%0Aemergent%20increase%20in%20model%20abstention%20%28e.g.%20saying%20%22I%20don%27t%20know%22%29%20for%20answers%0Athat%20are%20likely%20wrong.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLACIE%253A%2520Listener-Aware%2520Finetuning%2520for%2520Confidence%2520Calibration%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DElias%2520Stengel-Eskin%2520and%2520Peter%2520Hase%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520When%2520answering%2520questions%252C%2520LLMs%2520can%2520convey%2520not%2520only%2520an%2520answer%252C%2520but%2520a%2520level%2520of%250Aconfidence%2520about%2520the%2520answer%2520being%2520correct.%2520This%2520includes%2520explicit%2520confidence%250Amarkers%2520%2528e.g.%2520giving%2520a%2520numeric%2520score%2529%2520as%2520well%2520as%2520implicit%2520markers%252C%2520like%2520an%250Aauthoritative%2520tone%2520or%2520elaborating%2520with%2520additional%2520knowledge.%2520For%2520LLMs%2520to%2520be%250Atrustworthy%2520knowledge%2520sources%252C%2520the%2520confidence%2520they%2520convey%2520should%2520match%2520their%250Aactual%2520expertise%253B%2520however%252C%2520most%2520current%2520models%2520tend%2520towards%2520overconfidence.%2520To%250Acalibrate%2520both%2520implicit%2520and%2520explicit%2520confidence%2520markers%252C%2520we%2520introduce%2520a%250Apragmatic%252C%2520listener-aware%2520finetuning%2520method%2520%2528LACIE%2529%2520that%2520models%2520the%2520listener%252C%250Aconsidering%2520not%2520only%2520whether%2520an%2520answer%2520is%2520right%252C%2520but%2520whether%2520it%2520will%2520be%250Aaccepted%2520by%2520a%2520listener.%2520We%2520cast%2520calibration%2520as%2520preference%2520optimization%252C%250Acreating%2520data%2520via%2520a%2520two-agent%2520game%252C%2520where%2520a%2520speaker%2520model%2527s%2520outputs%2520are%2520judged%250Aby%2520a%2520simulated%2520listener.%2520We%2520then%2520finetune%2520three%2520LLMs%2520%2528Mistral-7B%252C%2520Llama3-8B%252C%250ALlama3-70B%2529%2520with%2520LACIE%252C%2520and%2520show%2520that%2520the%2520resulting%2520models%2520are%2520better%250Acalibrated%2520w.r.t.%2520a%2520simulated%2520listener.%2520Crucially%252C%2520these%2520trends%2520transfer%2520to%250Ahuman%2520listeners%252C%2520helping%2520them%2520correctly%2520predict%2520model%2520correctness%253A%2520we%2520conduct%2520a%250Ahuman%2520evaluation%2520where%2520annotators%2520accept%2520or%2520reject%2520an%2520LLM%2527s%2520answers%252C%2520finding%250Athat%2520training%2520with%2520LACIE%2520results%2520in%252047%2525%2520fewer%2520incorrect%2520answers%2520being%2520accepted%250Awhile%2520maintaining%2520the%2520same%2520level%2520of%2520acceptance%2520for%2520correct%2520answers.%250AFurthermore%252C%2520LACIE%2520generalizes%2520to%2520another%2520dataset%252C%2520resulting%2520in%2520a%2520large%250Aincrease%2520in%2520truthfulness%2520on%2520TruthfulQA%2520when%2520trained%2520on%2520TriviaQA.%2520Our%2520analysis%250Aindicates%2520that%2520LACIE%2520leads%2520to%2520a%2520better%2520confidence%2520separation%2520between%2520correct%250Aand%2520incorrect%2520examples.%2520Qualitatively%252C%2520we%2520find%2520that%2520a%2520LACIE-trained%2520model%250Ahedges%2520more%2520and%2520implicitly%2520signals%2520certainty%2520when%2520it%2520is%2520correct%2520by%2520using%2520an%250Aauthoritative%2520tone%2520or%2520including%2520details.%2520Finally%252C%2520LACIE%2520finetuning%2520leads%2520to%2520an%250Aemergent%2520increase%2520in%2520model%2520abstention%2520%2528e.g.%2520saying%2520%2522I%2520don%2527t%2520know%2522%2529%2520for%2520answers%250Athat%2520are%2520likely%2520wrong.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LACIE%3A%20Listener-Aware%20Finetuning%20for%20Confidence%20Calibration%20in%20Large%0A%20%20Language%20Models&entry.906535625=Elias%20Stengel-Eskin%20and%20Peter%20Hase%20and%20Mohit%20Bansal&entry.1292438233=%20%20When%20answering%20questions%2C%20LLMs%20can%20convey%20not%20only%20an%20answer%2C%20but%20a%20level%20of%0Aconfidence%20about%20the%20answer%20being%20correct.%20This%20includes%20explicit%20confidence%0Amarkers%20%28e.g.%20giving%20a%20numeric%20score%29%20as%20well%20as%20implicit%20markers%2C%20like%20an%0Aauthoritative%20tone%20or%20elaborating%20with%20additional%20knowledge.%20For%20LLMs%20to%20be%0Atrustworthy%20knowledge%20sources%2C%20the%20confidence%20they%20convey%20should%20match%20their%0Aactual%20expertise%3B%20however%2C%20most%20current%20models%20tend%20towards%20overconfidence.%20To%0Acalibrate%20both%20implicit%20and%20explicit%20confidence%20markers%2C%20we%20introduce%20a%0Apragmatic%2C%20listener-aware%20finetuning%20method%20%28LACIE%29%20that%20models%20the%20listener%2C%0Aconsidering%20not%20only%20whether%20an%20answer%20is%20right%2C%20but%20whether%20it%20will%20be%0Aaccepted%20by%20a%20listener.%20We%20cast%20calibration%20as%20preference%20optimization%2C%0Acreating%20data%20via%20a%20two-agent%20game%2C%20where%20a%20speaker%20model%27s%20outputs%20are%20judged%0Aby%20a%20simulated%20listener.%20We%20then%20finetune%20three%20LLMs%20%28Mistral-7B%2C%20Llama3-8B%2C%0ALlama3-70B%29%20with%20LACIE%2C%20and%20show%20that%20the%20resulting%20models%20are%20better%0Acalibrated%20w.r.t.%20a%20simulated%20listener.%20Crucially%2C%20these%20trends%20transfer%20to%0Ahuman%20listeners%2C%20helping%20them%20correctly%20predict%20model%20correctness%3A%20we%20conduct%20a%0Ahuman%20evaluation%20where%20annotators%20accept%20or%20reject%20an%20LLM%27s%20answers%2C%20finding%0Athat%20training%20with%20LACIE%20results%20in%2047%25%20fewer%20incorrect%20answers%20being%20accepted%0Awhile%20maintaining%20the%20same%20level%20of%20acceptance%20for%20correct%20answers.%0AFurthermore%2C%20LACIE%20generalizes%20to%20another%20dataset%2C%20resulting%20in%20a%20large%0Aincrease%20in%20truthfulness%20on%20TruthfulQA%20when%20trained%20on%20TriviaQA.%20Our%20analysis%0Aindicates%20that%20LACIE%20leads%20to%20a%20better%20confidence%20separation%20between%20correct%0Aand%20incorrect%20examples.%20Qualitatively%2C%20we%20find%20that%20a%20LACIE-trained%20model%0Ahedges%20more%20and%20implicitly%20signals%20certainty%20when%20it%20is%20correct%20by%20using%20an%0Aauthoritative%20tone%20or%20including%20details.%20Finally%2C%20LACIE%20finetuning%20leads%20to%20an%0Aemergent%20increase%20in%20model%20abstention%20%28e.g.%20saying%20%22I%20don%27t%20know%22%29%20for%20answers%0Athat%20are%20likely%20wrong.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21028v1&entry.124074799=Read"},
{"title": "BioT5+: Towards Generalized Biological Understanding with IUPAC\n  Integration and Multi-task Tuning", "author": "Qizhi Pei and Lijun Wu and Kaiyuan Gao and Xiaozhuan Liang and Yin Fang and Jinhua Zhu and Shufang Xie and Tao Qin and Rui Yan", "abstract": "  Recent research trends in computational biology have increasingly focused on\nintegrating text and bio-entity modeling, especially in the context of\nmolecules and proteins. However, previous efforts like BioT5 faced challenges\nin generalizing across diverse tasks and lacked a nuanced understanding of\nmolecular structures, particularly in their textual representations (e.g.,\nIUPAC). This paper introduces BioT5+, an extension of the BioT5 framework,\ntailored to enhance biological research and drug discovery. BioT5+ incorporates\nseveral novel features: integration of IUPAC names for molecular understanding,\ninclusion of extensive bio-text and molecule data from sources like bioRxiv and\nPubChem, the multi-task instruction tuning for generality across tasks, and a\nnumerical tokenization technique for improved processing of numerical data.\nThese enhancements allow BioT5+ to bridge the gap between molecular\nrepresentations and their textual descriptions, providing a more holistic\nunderstanding of biological entities, and largely improving the grounded\nreasoning of bio-text and bio-sequences. The model is pre-trained and\nfine-tuned with a large number of experiments, including \\emph{3 types of\nproblems (classification, regression, generation), 15 kinds of tasks, and 21\ntotal benchmark datasets}, demonstrating the remarkable performance and\nstate-of-the-art results in most cases. BioT5+ stands out for its ability to\ncapture intricate relationships in biological data, thereby contributing\nsignificantly to bioinformatics and computational biology. Our code is\navailable at \\url{https://github.com/QizhiPei/BioT5}.\n", "link": "http://arxiv.org/abs/2402.17810v2", "date": "2024-05-31", "relevancy": 1.9788, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5213}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4774}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BioT5%2B%3A%20Towards%20Generalized%20Biological%20Understanding%20with%20IUPAC%0A%20%20Integration%20and%20Multi-task%20Tuning&body=Title%3A%20BioT5%2B%3A%20Towards%20Generalized%20Biological%20Understanding%20with%20IUPAC%0A%20%20Integration%20and%20Multi-task%20Tuning%0AAuthor%3A%20Qizhi%20Pei%20and%20Lijun%20Wu%20and%20Kaiyuan%20Gao%20and%20Xiaozhuan%20Liang%20and%20Yin%20Fang%20and%20Jinhua%20Zhu%20and%20Shufang%20Xie%20and%20Tao%20Qin%20and%20Rui%20Yan%0AAbstract%3A%20%20%20Recent%20research%20trends%20in%20computational%20biology%20have%20increasingly%20focused%20on%0Aintegrating%20text%20and%20bio-entity%20modeling%2C%20especially%20in%20the%20context%20of%0Amolecules%20and%20proteins.%20However%2C%20previous%20efforts%20like%20BioT5%20faced%20challenges%0Ain%20generalizing%20across%20diverse%20tasks%20and%20lacked%20a%20nuanced%20understanding%20of%0Amolecular%20structures%2C%20particularly%20in%20their%20textual%20representations%20%28e.g.%2C%0AIUPAC%29.%20This%20paper%20introduces%20BioT5%2B%2C%20an%20extension%20of%20the%20BioT5%20framework%2C%0Atailored%20to%20enhance%20biological%20research%20and%20drug%20discovery.%20BioT5%2B%20incorporates%0Aseveral%20novel%20features%3A%20integration%20of%20IUPAC%20names%20for%20molecular%20understanding%2C%0Ainclusion%20of%20extensive%20bio-text%20and%20molecule%20data%20from%20sources%20like%20bioRxiv%20and%0APubChem%2C%20the%20multi-task%20instruction%20tuning%20for%20generality%20across%20tasks%2C%20and%20a%0Anumerical%20tokenization%20technique%20for%20improved%20processing%20of%20numerical%20data.%0AThese%20enhancements%20allow%20BioT5%2B%20to%20bridge%20the%20gap%20between%20molecular%0Arepresentations%20and%20their%20textual%20descriptions%2C%20providing%20a%20more%20holistic%0Aunderstanding%20of%20biological%20entities%2C%20and%20largely%20improving%20the%20grounded%0Areasoning%20of%20bio-text%20and%20bio-sequences.%20The%20model%20is%20pre-trained%20and%0Afine-tuned%20with%20a%20large%20number%20of%20experiments%2C%20including%20%5Cemph%7B3%20types%20of%0Aproblems%20%28classification%2C%20regression%2C%20generation%29%2C%2015%20kinds%20of%20tasks%2C%20and%2021%0Atotal%20benchmark%20datasets%7D%2C%20demonstrating%20the%20remarkable%20performance%20and%0Astate-of-the-art%20results%20in%20most%20cases.%20BioT5%2B%20stands%20out%20for%20its%20ability%20to%0Acapture%20intricate%20relationships%20in%20biological%20data%2C%20thereby%20contributing%0Asignificantly%20to%20bioinformatics%20and%20computational%20biology.%20Our%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/QizhiPei/BioT5%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17810v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBioT5%252B%253A%2520Towards%2520Generalized%2520Biological%2520Understanding%2520with%2520IUPAC%250A%2520%2520Integration%2520and%2520Multi-task%2520Tuning%26entry.906535625%3DQizhi%2520Pei%2520and%2520Lijun%2520Wu%2520and%2520Kaiyuan%2520Gao%2520and%2520Xiaozhuan%2520Liang%2520and%2520Yin%2520Fang%2520and%2520Jinhua%2520Zhu%2520and%2520Shufang%2520Xie%2520and%2520Tao%2520Qin%2520and%2520Rui%2520Yan%26entry.1292438233%3D%2520%2520Recent%2520research%2520trends%2520in%2520computational%2520biology%2520have%2520increasingly%2520focused%2520on%250Aintegrating%2520text%2520and%2520bio-entity%2520modeling%252C%2520especially%2520in%2520the%2520context%2520of%250Amolecules%2520and%2520proteins.%2520However%252C%2520previous%2520efforts%2520like%2520BioT5%2520faced%2520challenges%250Ain%2520generalizing%2520across%2520diverse%2520tasks%2520and%2520lacked%2520a%2520nuanced%2520understanding%2520of%250Amolecular%2520structures%252C%2520particularly%2520in%2520their%2520textual%2520representations%2520%2528e.g.%252C%250AIUPAC%2529.%2520This%2520paper%2520introduces%2520BioT5%252B%252C%2520an%2520extension%2520of%2520the%2520BioT5%2520framework%252C%250Atailored%2520to%2520enhance%2520biological%2520research%2520and%2520drug%2520discovery.%2520BioT5%252B%2520incorporates%250Aseveral%2520novel%2520features%253A%2520integration%2520of%2520IUPAC%2520names%2520for%2520molecular%2520understanding%252C%250Ainclusion%2520of%2520extensive%2520bio-text%2520and%2520molecule%2520data%2520from%2520sources%2520like%2520bioRxiv%2520and%250APubChem%252C%2520the%2520multi-task%2520instruction%2520tuning%2520for%2520generality%2520across%2520tasks%252C%2520and%2520a%250Anumerical%2520tokenization%2520technique%2520for%2520improved%2520processing%2520of%2520numerical%2520data.%250AThese%2520enhancements%2520allow%2520BioT5%252B%2520to%2520bridge%2520the%2520gap%2520between%2520molecular%250Arepresentations%2520and%2520their%2520textual%2520descriptions%252C%2520providing%2520a%2520more%2520holistic%250Aunderstanding%2520of%2520biological%2520entities%252C%2520and%2520largely%2520improving%2520the%2520grounded%250Areasoning%2520of%2520bio-text%2520and%2520bio-sequences.%2520The%2520model%2520is%2520pre-trained%2520and%250Afine-tuned%2520with%2520a%2520large%2520number%2520of%2520experiments%252C%2520including%2520%255Cemph%257B3%2520types%2520of%250Aproblems%2520%2528classification%252C%2520regression%252C%2520generation%2529%252C%252015%2520kinds%2520of%2520tasks%252C%2520and%252021%250Atotal%2520benchmark%2520datasets%257D%252C%2520demonstrating%2520the%2520remarkable%2520performance%2520and%250Astate-of-the-art%2520results%2520in%2520most%2520cases.%2520BioT5%252B%2520stands%2520out%2520for%2520its%2520ability%2520to%250Acapture%2520intricate%2520relationships%2520in%2520biological%2520data%252C%2520thereby%2520contributing%250Asignificantly%2520to%2520bioinformatics%2520and%2520computational%2520biology.%2520Our%2520code%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/QizhiPei/BioT5%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17810v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BioT5%2B%3A%20Towards%20Generalized%20Biological%20Understanding%20with%20IUPAC%0A%20%20Integration%20and%20Multi-task%20Tuning&entry.906535625=Qizhi%20Pei%20and%20Lijun%20Wu%20and%20Kaiyuan%20Gao%20and%20Xiaozhuan%20Liang%20and%20Yin%20Fang%20and%20Jinhua%20Zhu%20and%20Shufang%20Xie%20and%20Tao%20Qin%20and%20Rui%20Yan&entry.1292438233=%20%20Recent%20research%20trends%20in%20computational%20biology%20have%20increasingly%20focused%20on%0Aintegrating%20text%20and%20bio-entity%20modeling%2C%20especially%20in%20the%20context%20of%0Amolecules%20and%20proteins.%20However%2C%20previous%20efforts%20like%20BioT5%20faced%20challenges%0Ain%20generalizing%20across%20diverse%20tasks%20and%20lacked%20a%20nuanced%20understanding%20of%0Amolecular%20structures%2C%20particularly%20in%20their%20textual%20representations%20%28e.g.%2C%0AIUPAC%29.%20This%20paper%20introduces%20BioT5%2B%2C%20an%20extension%20of%20the%20BioT5%20framework%2C%0Atailored%20to%20enhance%20biological%20research%20and%20drug%20discovery.%20BioT5%2B%20incorporates%0Aseveral%20novel%20features%3A%20integration%20of%20IUPAC%20names%20for%20molecular%20understanding%2C%0Ainclusion%20of%20extensive%20bio-text%20and%20molecule%20data%20from%20sources%20like%20bioRxiv%20and%0APubChem%2C%20the%20multi-task%20instruction%20tuning%20for%20generality%20across%20tasks%2C%20and%20a%0Anumerical%20tokenization%20technique%20for%20improved%20processing%20of%20numerical%20data.%0AThese%20enhancements%20allow%20BioT5%2B%20to%20bridge%20the%20gap%20between%20molecular%0Arepresentations%20and%20their%20textual%20descriptions%2C%20providing%20a%20more%20holistic%0Aunderstanding%20of%20biological%20entities%2C%20and%20largely%20improving%20the%20grounded%0Areasoning%20of%20bio-text%20and%20bio-sequences.%20The%20model%20is%20pre-trained%20and%0Afine-tuned%20with%20a%20large%20number%20of%20experiments%2C%20including%20%5Cemph%7B3%20types%20of%0Aproblems%20%28classification%2C%20regression%2C%20generation%29%2C%2015%20kinds%20of%20tasks%2C%20and%2021%0Atotal%20benchmark%20datasets%7D%2C%20demonstrating%20the%20remarkable%20performance%20and%0Astate-of-the-art%20results%20in%20most%20cases.%20BioT5%2B%20stands%20out%20for%20its%20ability%20to%0Acapture%20intricate%20relationships%20in%20biological%20data%2C%20thereby%20contributing%0Asignificantly%20to%20bioinformatics%20and%20computational%20biology.%20Our%20code%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/QizhiPei/BioT5%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17810v2&entry.124074799=Read"},
{"title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws", "author": "Elvis Dohmatob and Yunzhen Feng and Pu Yang and Francois Charton and Julia Kempe", "abstract": "  As AI model size grows, neural scaling laws have become a crucial tool to\npredict the improvements of large models when increasing capacity and the size\nof original (human or natural) training data. Yet, the widespread use of\npopular models means that the ecosystem of online data and text will co-evolve\nto progressively contain increased amounts of synthesized data. In this paper\nwe ask: How will the scaling laws change in the inevitable regime where\nsynthetic data makes its way into the training corpus? Will future models,\nstill improve, or be doomed to degenerate up to total (model) collapse? We\ndevelop a theoretical framework of model collapse through the lens of scaling\nlaws. We discover a wide range of decay phenomena, analyzing loss of scaling,\nshifted scaling with number of generations, the ''un-learning\" of skills, and\ngrokking when mixing human and synthesized data. Our theory is validated by\nlarge-scale experiments with a transformer on an arithmetic task and text\ngeneration using the large language model Llama2.\n", "link": "http://arxiv.org/abs/2402.07043v2", "date": "2024-05-31", "relevancy": 1.9772, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5949}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.483}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4653}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Tale%20of%20Tails%3A%20Model%20Collapse%20as%20a%20Change%20of%20Scaling%20Laws&body=Title%3A%20A%20Tale%20of%20Tails%3A%20Model%20Collapse%20as%20a%20Change%20of%20Scaling%20Laws%0AAuthor%3A%20Elvis%20Dohmatob%20and%20Yunzhen%20Feng%20and%20Pu%20Yang%20and%20Francois%20Charton%20and%20Julia%20Kempe%0AAbstract%3A%20%20%20As%20AI%20model%20size%20grows%2C%20neural%20scaling%20laws%20have%20become%20a%20crucial%20tool%20to%0Apredict%20the%20improvements%20of%20large%20models%20when%20increasing%20capacity%20and%20the%20size%0Aof%20original%20%28human%20or%20natural%29%20training%20data.%20Yet%2C%20the%20widespread%20use%20of%0Apopular%20models%20means%20that%20the%20ecosystem%20of%20online%20data%20and%20text%20will%20co-evolve%0Ato%20progressively%20contain%20increased%20amounts%20of%20synthesized%20data.%20In%20this%20paper%0Awe%20ask%3A%20How%20will%20the%20scaling%20laws%20change%20in%20the%20inevitable%20regime%20where%0Asynthetic%20data%20makes%20its%20way%20into%20the%20training%20corpus%3F%20Will%20future%20models%2C%0Astill%20improve%2C%20or%20be%20doomed%20to%20degenerate%20up%20to%20total%20%28model%29%20collapse%3F%20We%0Adevelop%20a%20theoretical%20framework%20of%20model%20collapse%20through%20the%20lens%20of%20scaling%0Alaws.%20We%20discover%20a%20wide%20range%20of%20decay%20phenomena%2C%20analyzing%20loss%20of%20scaling%2C%0Ashifted%20scaling%20with%20number%20of%20generations%2C%20the%20%27%27un-learning%22%20of%20skills%2C%20and%0Agrokking%20when%20mixing%20human%20and%20synthesized%20data.%20Our%20theory%20is%20validated%20by%0Alarge-scale%20experiments%20with%20a%20transformer%20on%20an%20arithmetic%20task%20and%20text%0Ageneration%20using%20the%20large%20language%20model%20Llama2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Tale%2520of%2520Tails%253A%2520Model%2520Collapse%2520as%2520a%2520Change%2520of%2520Scaling%2520Laws%26entry.906535625%3DElvis%2520Dohmatob%2520and%2520Yunzhen%2520Feng%2520and%2520Pu%2520Yang%2520and%2520Francois%2520Charton%2520and%2520Julia%2520Kempe%26entry.1292438233%3D%2520%2520As%2520AI%2520model%2520size%2520grows%252C%2520neural%2520scaling%2520laws%2520have%2520become%2520a%2520crucial%2520tool%2520to%250Apredict%2520the%2520improvements%2520of%2520large%2520models%2520when%2520increasing%2520capacity%2520and%2520the%2520size%250Aof%2520original%2520%2528human%2520or%2520natural%2529%2520training%2520data.%2520Yet%252C%2520the%2520widespread%2520use%2520of%250Apopular%2520models%2520means%2520that%2520the%2520ecosystem%2520of%2520online%2520data%2520and%2520text%2520will%2520co-evolve%250Ato%2520progressively%2520contain%2520increased%2520amounts%2520of%2520synthesized%2520data.%2520In%2520this%2520paper%250Awe%2520ask%253A%2520How%2520will%2520the%2520scaling%2520laws%2520change%2520in%2520the%2520inevitable%2520regime%2520where%250Asynthetic%2520data%2520makes%2520its%2520way%2520into%2520the%2520training%2520corpus%253F%2520Will%2520future%2520models%252C%250Astill%2520improve%252C%2520or%2520be%2520doomed%2520to%2520degenerate%2520up%2520to%2520total%2520%2528model%2529%2520collapse%253F%2520We%250Adevelop%2520a%2520theoretical%2520framework%2520of%2520model%2520collapse%2520through%2520the%2520lens%2520of%2520scaling%250Alaws.%2520We%2520discover%2520a%2520wide%2520range%2520of%2520decay%2520phenomena%252C%2520analyzing%2520loss%2520of%2520scaling%252C%250Ashifted%2520scaling%2520with%2520number%2520of%2520generations%252C%2520the%2520%2527%2527un-learning%2522%2520of%2520skills%252C%2520and%250Agrokking%2520when%2520mixing%2520human%2520and%2520synthesized%2520data.%2520Our%2520theory%2520is%2520validated%2520by%250Alarge-scale%2520experiments%2520with%2520a%2520transformer%2520on%2520an%2520arithmetic%2520task%2520and%2520text%250Ageneration%2520using%2520the%2520large%2520language%2520model%2520Llama2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Tale%20of%20Tails%3A%20Model%20Collapse%20as%20a%20Change%20of%20Scaling%20Laws&entry.906535625=Elvis%20Dohmatob%20and%20Yunzhen%20Feng%20and%20Pu%20Yang%20and%20Francois%20Charton%20and%20Julia%20Kempe&entry.1292438233=%20%20As%20AI%20model%20size%20grows%2C%20neural%20scaling%20laws%20have%20become%20a%20crucial%20tool%20to%0Apredict%20the%20improvements%20of%20large%20models%20when%20increasing%20capacity%20and%20the%20size%0Aof%20original%20%28human%20or%20natural%29%20training%20data.%20Yet%2C%20the%20widespread%20use%20of%0Apopular%20models%20means%20that%20the%20ecosystem%20of%20online%20data%20and%20text%20will%20co-evolve%0Ato%20progressively%20contain%20increased%20amounts%20of%20synthesized%20data.%20In%20this%20paper%0Awe%20ask%3A%20How%20will%20the%20scaling%20laws%20change%20in%20the%20inevitable%20regime%20where%0Asynthetic%20data%20makes%20its%20way%20into%20the%20training%20corpus%3F%20Will%20future%20models%2C%0Astill%20improve%2C%20or%20be%20doomed%20to%20degenerate%20up%20to%20total%20%28model%29%20collapse%3F%20We%0Adevelop%20a%20theoretical%20framework%20of%20model%20collapse%20through%20the%20lens%20of%20scaling%0Alaws.%20We%20discover%20a%20wide%20range%20of%20decay%20phenomena%2C%20analyzing%20loss%20of%20scaling%2C%0Ashifted%20scaling%20with%20number%20of%20generations%2C%20the%20%27%27un-learning%22%20of%20skills%2C%20and%0Agrokking%20when%20mixing%20human%20and%20synthesized%20data.%20Our%20theory%20is%20validated%20by%0Alarge-scale%20experiments%20with%20a%20transformer%20on%20an%20arithmetic%20task%20and%20text%0Ageneration%20using%20the%20large%20language%20model%20Llama2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07043v2&entry.124074799=Read"},
{"title": "Primal Dual Continual Learning: Balancing Stability and Plasticity\n  through Adaptive Memory Allocation", "author": "Juan Elenter and Navid NaderiAlizadeh and Tara Javidi and Alejandro Ribeiro", "abstract": "  Continual learning is inherently a constrained learning problem. The goal is\nto learn a predictor under a no-forgetting requirement. Although several prior\nstudies formulate it as such, they do not solve the constrained problem\nexplicitly. In this work, we show that it is both possible and beneficial to\nundertake the constrained optimization problem directly. To do this, we\nleverage recent results in constrained learning through Lagrangian duality. We\nfocus on memory-based methods, where a small subset of samples from previous\ntasks can be stored in a replay buffer. In this setting, we analyze two\nversions of the continual learning problem: a coarse approach with constraints\nat the task level and a fine approach with constraints at the sample level. We\nshow that dual variables indicate the sensitivity of the optimal value of the\ncontinual learning problem with respect to constraint perturbations. We then\nleverage this result to partition the buffer in the coarse approach, allocating\nmore resources to harder tasks, and to populate the buffer in the fine\napproach, including only impactful samples. We derive a deviation bound on dual\nvariables as sensitivity indicators, and empirically corroborate this result in\ndiverse continual learning benchmarks. We also discuss the limitations of these\nmethods with respect to the amount of memory available and the expressiveness\nof the parametrization.\n", "link": "http://arxiv.org/abs/2310.00154v2", "date": "2024-05-31", "relevancy": 1.9748, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5096}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4942}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Primal%20Dual%20Continual%20Learning%3A%20Balancing%20Stability%20and%20Plasticity%0A%20%20through%20Adaptive%20Memory%20Allocation&body=Title%3A%20Primal%20Dual%20Continual%20Learning%3A%20Balancing%20Stability%20and%20Plasticity%0A%20%20through%20Adaptive%20Memory%20Allocation%0AAuthor%3A%20Juan%20Elenter%20and%20Navid%20NaderiAlizadeh%20and%20Tara%20Javidi%20and%20Alejandro%20Ribeiro%0AAbstract%3A%20%20%20Continual%20learning%20is%20inherently%20a%20constrained%20learning%20problem.%20The%20goal%20is%0Ato%20learn%20a%20predictor%20under%20a%20no-forgetting%20requirement.%20Although%20several%20prior%0Astudies%20formulate%20it%20as%20such%2C%20they%20do%20not%20solve%20the%20constrained%20problem%0Aexplicitly.%20In%20this%20work%2C%20we%20show%20that%20it%20is%20both%20possible%20and%20beneficial%20to%0Aundertake%20the%20constrained%20optimization%20problem%20directly.%20To%20do%20this%2C%20we%0Aleverage%20recent%20results%20in%20constrained%20learning%20through%20Lagrangian%20duality.%20We%0Afocus%20on%20memory-based%20methods%2C%20where%20a%20small%20subset%20of%20samples%20from%20previous%0Atasks%20can%20be%20stored%20in%20a%20replay%20buffer.%20In%20this%20setting%2C%20we%20analyze%20two%0Aversions%20of%20the%20continual%20learning%20problem%3A%20a%20coarse%20approach%20with%20constraints%0Aat%20the%20task%20level%20and%20a%20fine%20approach%20with%20constraints%20at%20the%20sample%20level.%20We%0Ashow%20that%20dual%20variables%20indicate%20the%20sensitivity%20of%20the%20optimal%20value%20of%20the%0Acontinual%20learning%20problem%20with%20respect%20to%20constraint%20perturbations.%20We%20then%0Aleverage%20this%20result%20to%20partition%20the%20buffer%20in%20the%20coarse%20approach%2C%20allocating%0Amore%20resources%20to%20harder%20tasks%2C%20and%20to%20populate%20the%20buffer%20in%20the%20fine%0Aapproach%2C%20including%20only%20impactful%20samples.%20We%20derive%20a%20deviation%20bound%20on%20dual%0Avariables%20as%20sensitivity%20indicators%2C%20and%20empirically%20corroborate%20this%20result%20in%0Adiverse%20continual%20learning%20benchmarks.%20We%20also%20discuss%20the%20limitations%20of%20these%0Amethods%20with%20respect%20to%20the%20amount%20of%20memory%20available%20and%20the%20expressiveness%0Aof%20the%20parametrization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.00154v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrimal%2520Dual%2520Continual%2520Learning%253A%2520Balancing%2520Stability%2520and%2520Plasticity%250A%2520%2520through%2520Adaptive%2520Memory%2520Allocation%26entry.906535625%3DJuan%2520Elenter%2520and%2520Navid%2520NaderiAlizadeh%2520and%2520Tara%2520Javidi%2520and%2520Alejandro%2520Ribeiro%26entry.1292438233%3D%2520%2520Continual%2520learning%2520is%2520inherently%2520a%2520constrained%2520learning%2520problem.%2520The%2520goal%2520is%250Ato%2520learn%2520a%2520predictor%2520under%2520a%2520no-forgetting%2520requirement.%2520Although%2520several%2520prior%250Astudies%2520formulate%2520it%2520as%2520such%252C%2520they%2520do%2520not%2520solve%2520the%2520constrained%2520problem%250Aexplicitly.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520it%2520is%2520both%2520possible%2520and%2520beneficial%2520to%250Aundertake%2520the%2520constrained%2520optimization%2520problem%2520directly.%2520To%2520do%2520this%252C%2520we%250Aleverage%2520recent%2520results%2520in%2520constrained%2520learning%2520through%2520Lagrangian%2520duality.%2520We%250Afocus%2520on%2520memory-based%2520methods%252C%2520where%2520a%2520small%2520subset%2520of%2520samples%2520from%2520previous%250Atasks%2520can%2520be%2520stored%2520in%2520a%2520replay%2520buffer.%2520In%2520this%2520setting%252C%2520we%2520analyze%2520two%250Aversions%2520of%2520the%2520continual%2520learning%2520problem%253A%2520a%2520coarse%2520approach%2520with%2520constraints%250Aat%2520the%2520task%2520level%2520and%2520a%2520fine%2520approach%2520with%2520constraints%2520at%2520the%2520sample%2520level.%2520We%250Ashow%2520that%2520dual%2520variables%2520indicate%2520the%2520sensitivity%2520of%2520the%2520optimal%2520value%2520of%2520the%250Acontinual%2520learning%2520problem%2520with%2520respect%2520to%2520constraint%2520perturbations.%2520We%2520then%250Aleverage%2520this%2520result%2520to%2520partition%2520the%2520buffer%2520in%2520the%2520coarse%2520approach%252C%2520allocating%250Amore%2520resources%2520to%2520harder%2520tasks%252C%2520and%2520to%2520populate%2520the%2520buffer%2520in%2520the%2520fine%250Aapproach%252C%2520including%2520only%2520impactful%2520samples.%2520We%2520derive%2520a%2520deviation%2520bound%2520on%2520dual%250Avariables%2520as%2520sensitivity%2520indicators%252C%2520and%2520empirically%2520corroborate%2520this%2520result%2520in%250Adiverse%2520continual%2520learning%2520benchmarks.%2520We%2520also%2520discuss%2520the%2520limitations%2520of%2520these%250Amethods%2520with%2520respect%2520to%2520the%2520amount%2520of%2520memory%2520available%2520and%2520the%2520expressiveness%250Aof%2520the%2520parametrization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00154v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Primal%20Dual%20Continual%20Learning%3A%20Balancing%20Stability%20and%20Plasticity%0A%20%20through%20Adaptive%20Memory%20Allocation&entry.906535625=Juan%20Elenter%20and%20Navid%20NaderiAlizadeh%20and%20Tara%20Javidi%20and%20Alejandro%20Ribeiro&entry.1292438233=%20%20Continual%20learning%20is%20inherently%20a%20constrained%20learning%20problem.%20The%20goal%20is%0Ato%20learn%20a%20predictor%20under%20a%20no-forgetting%20requirement.%20Although%20several%20prior%0Astudies%20formulate%20it%20as%20such%2C%20they%20do%20not%20solve%20the%20constrained%20problem%0Aexplicitly.%20In%20this%20work%2C%20we%20show%20that%20it%20is%20both%20possible%20and%20beneficial%20to%0Aundertake%20the%20constrained%20optimization%20problem%20directly.%20To%20do%20this%2C%20we%0Aleverage%20recent%20results%20in%20constrained%20learning%20through%20Lagrangian%20duality.%20We%0Afocus%20on%20memory-based%20methods%2C%20where%20a%20small%20subset%20of%20samples%20from%20previous%0Atasks%20can%20be%20stored%20in%20a%20replay%20buffer.%20In%20this%20setting%2C%20we%20analyze%20two%0Aversions%20of%20the%20continual%20learning%20problem%3A%20a%20coarse%20approach%20with%20constraints%0Aat%20the%20task%20level%20and%20a%20fine%20approach%20with%20constraints%20at%20the%20sample%20level.%20We%0Ashow%20that%20dual%20variables%20indicate%20the%20sensitivity%20of%20the%20optimal%20value%20of%20the%0Acontinual%20learning%20problem%20with%20respect%20to%20constraint%20perturbations.%20We%20then%0Aleverage%20this%20result%20to%20partition%20the%20buffer%20in%20the%20coarse%20approach%2C%20allocating%0Amore%20resources%20to%20harder%20tasks%2C%20and%20to%20populate%20the%20buffer%20in%20the%20fine%0Aapproach%2C%20including%20only%20impactful%20samples.%20We%20derive%20a%20deviation%20bound%20on%20dual%0Avariables%20as%20sensitivity%20indicators%2C%20and%20empirically%20corroborate%20this%20result%20in%0Adiverse%20continual%20learning%20benchmarks.%20We%20also%20discuss%20the%20limitations%20of%20these%0Amethods%20with%20respect%20to%20the%20amount%20of%20memory%20available%20and%20the%20expressiveness%0Aof%20the%20parametrization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.00154v2&entry.124074799=Read"},
{"title": "PUAL: A Classifier on Trifurcate Positive-Unlabeled Data", "author": "Xiaoke Wang and Xiaochen Yang and Rui Zhu and Jing-Hao Xue", "abstract": "  Positive-unlabeled (PU) learning aims to train a classifier using the data\ncontaining only labeled-positive instances and unlabeled instances. However,\nexisting PU learning methods are generally hard to achieve satisfactory\nperformance on trifurcate data, where the positive instances distribute on both\nsides of the negative instances. To address this issue, firstly we propose a PU\nclassifier with asymmetric loss (PUAL), by introducing a structure of\nasymmetric loss on positive instances into the objective function of the global\nand local learning classifier. Then we develop a kernel-based algorithm to\nenable PUAL to obtain non-linear decision boundary. We show that, through\nexperiments on both simulated and real-world datasets, PUAL can achieve\nsatisfactory classification on trifurcate data.\n", "link": "http://arxiv.org/abs/2405.20970v1", "date": "2024-05-31", "relevancy": 1.9747, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5255}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4903}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PUAL%3A%20A%20Classifier%20on%20Trifurcate%20Positive-Unlabeled%20Data&body=Title%3A%20PUAL%3A%20A%20Classifier%20on%20Trifurcate%20Positive-Unlabeled%20Data%0AAuthor%3A%20Xiaoke%20Wang%20and%20Xiaochen%20Yang%20and%20Rui%20Zhu%20and%20Jing-Hao%20Xue%0AAbstract%3A%20%20%20Positive-unlabeled%20%28PU%29%20learning%20aims%20to%20train%20a%20classifier%20using%20the%20data%0Acontaining%20only%20labeled-positive%20instances%20and%20unlabeled%20instances.%20However%2C%0Aexisting%20PU%20learning%20methods%20are%20generally%20hard%20to%20achieve%20satisfactory%0Aperformance%20on%20trifurcate%20data%2C%20where%20the%20positive%20instances%20distribute%20on%20both%0Asides%20of%20the%20negative%20instances.%20To%20address%20this%20issue%2C%20firstly%20we%20propose%20a%20PU%0Aclassifier%20with%20asymmetric%20loss%20%28PUAL%29%2C%20by%20introducing%20a%20structure%20of%0Aasymmetric%20loss%20on%20positive%20instances%20into%20the%20objective%20function%20of%20the%20global%0Aand%20local%20learning%20classifier.%20Then%20we%20develop%20a%20kernel-based%20algorithm%20to%0Aenable%20PUAL%20to%20obtain%20non-linear%20decision%20boundary.%20We%20show%20that%2C%20through%0Aexperiments%20on%20both%20simulated%20and%20real-world%20datasets%2C%20PUAL%20can%20achieve%0Asatisfactory%20classification%20on%20trifurcate%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPUAL%253A%2520A%2520Classifier%2520on%2520Trifurcate%2520Positive-Unlabeled%2520Data%26entry.906535625%3DXiaoke%2520Wang%2520and%2520Xiaochen%2520Yang%2520and%2520Rui%2520Zhu%2520and%2520Jing-Hao%2520Xue%26entry.1292438233%3D%2520%2520Positive-unlabeled%2520%2528PU%2529%2520learning%2520aims%2520to%2520train%2520a%2520classifier%2520using%2520the%2520data%250Acontaining%2520only%2520labeled-positive%2520instances%2520and%2520unlabeled%2520instances.%2520However%252C%250Aexisting%2520PU%2520learning%2520methods%2520are%2520generally%2520hard%2520to%2520achieve%2520satisfactory%250Aperformance%2520on%2520trifurcate%2520data%252C%2520where%2520the%2520positive%2520instances%2520distribute%2520on%2520both%250Asides%2520of%2520the%2520negative%2520instances.%2520To%2520address%2520this%2520issue%252C%2520firstly%2520we%2520propose%2520a%2520PU%250Aclassifier%2520with%2520asymmetric%2520loss%2520%2528PUAL%2529%252C%2520by%2520introducing%2520a%2520structure%2520of%250Aasymmetric%2520loss%2520on%2520positive%2520instances%2520into%2520the%2520objective%2520function%2520of%2520the%2520global%250Aand%2520local%2520learning%2520classifier.%2520Then%2520we%2520develop%2520a%2520kernel-based%2520algorithm%2520to%250Aenable%2520PUAL%2520to%2520obtain%2520non-linear%2520decision%2520boundary.%2520We%2520show%2520that%252C%2520through%250Aexperiments%2520on%2520both%2520simulated%2520and%2520real-world%2520datasets%252C%2520PUAL%2520can%2520achieve%250Asatisfactory%2520classification%2520on%2520trifurcate%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PUAL%3A%20A%20Classifier%20on%20Trifurcate%20Positive-Unlabeled%20Data&entry.906535625=Xiaoke%20Wang%20and%20Xiaochen%20Yang%20and%20Rui%20Zhu%20and%20Jing-Hao%20Xue&entry.1292438233=%20%20Positive-unlabeled%20%28PU%29%20learning%20aims%20to%20train%20a%20classifier%20using%20the%20data%0Acontaining%20only%20labeled-positive%20instances%20and%20unlabeled%20instances.%20However%2C%0Aexisting%20PU%20learning%20methods%20are%20generally%20hard%20to%20achieve%20satisfactory%0Aperformance%20on%20trifurcate%20data%2C%20where%20the%20positive%20instances%20distribute%20on%20both%0Asides%20of%20the%20negative%20instances.%20To%20address%20this%20issue%2C%20firstly%20we%20propose%20a%20PU%0Aclassifier%20with%20asymmetric%20loss%20%28PUAL%29%2C%20by%20introducing%20a%20structure%20of%0Aasymmetric%20loss%20on%20positive%20instances%20into%20the%20objective%20function%20of%20the%20global%0Aand%20local%20learning%20classifier.%20Then%20we%20develop%20a%20kernel-based%20algorithm%20to%0Aenable%20PUAL%20to%20obtain%20non-linear%20decision%20boundary.%20We%20show%20that%2C%20through%0Aexperiments%20on%20both%20simulated%20and%20real-world%20datasets%2C%20PUAL%20can%20achieve%0Asatisfactory%20classification%20on%20trifurcate%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20970v1&entry.124074799=Read"},
{"title": "Not Just Novelty: A Longitudinal Study on Utility and Customization of\n  an AI Workflow", "author": "Tao Long and Katy Ilonka Gero and Lydia B. Chilton", "abstract": "  Generative AI brings novel and impressive abilities to help people in\neveryday tasks. There are many AI workflows that solve real and complex\nproblems by chaining AI outputs together with human interaction. Although there\nis an undeniable lure of AI, it is uncertain how useful generative AI workflows\nare after the novelty wears off. Additionally, workflows built with generative\nAI have the potential to be easily customized to fit users' individual needs,\nbut do users take advantage of this? We conducted a three-week longitudinal\nstudy with 12 users to understand the familiarization and customization of\ngenerative AI tools for science communication. Our study revealed that there\nexists a familiarization phase, during which users were exploring the novel\ncapabilities of the workflow and discovering which aspects they found useful.\nAfter this phase, users understood the workflow and were able to anticipate the\noutputs. Surprisingly, after familiarization the perceived utility of the\nsystem was rated higher than before, indicating that the perceived utility of\nAI is not just a novelty effect. The increase in benefits mainly comes from\nend-users' ability to customize prompts, and thus potentially appropriate the\nsystem to their own needs. This points to a future where generative AI systems\ncan allow us to design for appropriation.\n", "link": "http://arxiv.org/abs/2402.09894v2", "date": "2024-05-31", "relevancy": 1.9708, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4989}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.495}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Not%20Just%20Novelty%3A%20A%20Longitudinal%20Study%20on%20Utility%20and%20Customization%20of%0A%20%20an%20AI%20Workflow&body=Title%3A%20Not%20Just%20Novelty%3A%20A%20Longitudinal%20Study%20on%20Utility%20and%20Customization%20of%0A%20%20an%20AI%20Workflow%0AAuthor%3A%20Tao%20Long%20and%20Katy%20Ilonka%20Gero%20and%20Lydia%20B.%20Chilton%0AAbstract%3A%20%20%20Generative%20AI%20brings%20novel%20and%20impressive%20abilities%20to%20help%20people%20in%0Aeveryday%20tasks.%20There%20are%20many%20AI%20workflows%20that%20solve%20real%20and%20complex%0Aproblems%20by%20chaining%20AI%20outputs%20together%20with%20human%20interaction.%20Although%20there%0Ais%20an%20undeniable%20lure%20of%20AI%2C%20it%20is%20uncertain%20how%20useful%20generative%20AI%20workflows%0Aare%20after%20the%20novelty%20wears%20off.%20Additionally%2C%20workflows%20built%20with%20generative%0AAI%20have%20the%20potential%20to%20be%20easily%20customized%20to%20fit%20users%27%20individual%20needs%2C%0Abut%20do%20users%20take%20advantage%20of%20this%3F%20We%20conducted%20a%20three-week%20longitudinal%0Astudy%20with%2012%20users%20to%20understand%20the%20familiarization%20and%20customization%20of%0Agenerative%20AI%20tools%20for%20science%20communication.%20Our%20study%20revealed%20that%20there%0Aexists%20a%20familiarization%20phase%2C%20during%20which%20users%20were%20exploring%20the%20novel%0Acapabilities%20of%20the%20workflow%20and%20discovering%20which%20aspects%20they%20found%20useful.%0AAfter%20this%20phase%2C%20users%20understood%20the%20workflow%20and%20were%20able%20to%20anticipate%20the%0Aoutputs.%20Surprisingly%2C%20after%20familiarization%20the%20perceived%20utility%20of%20the%0Asystem%20was%20rated%20higher%20than%20before%2C%20indicating%20that%20the%20perceived%20utility%20of%0AAI%20is%20not%20just%20a%20novelty%20effect.%20The%20increase%20in%20benefits%20mainly%20comes%20from%0Aend-users%27%20ability%20to%20customize%20prompts%2C%20and%20thus%20potentially%20appropriate%20the%0Asystem%20to%20their%20own%20needs.%20This%20points%20to%20a%20future%20where%20generative%20AI%20systems%0Acan%20allow%20us%20to%20design%20for%20appropriation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09894v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNot%2520Just%2520Novelty%253A%2520A%2520Longitudinal%2520Study%2520on%2520Utility%2520and%2520Customization%2520of%250A%2520%2520an%2520AI%2520Workflow%26entry.906535625%3DTao%2520Long%2520and%2520Katy%2520Ilonka%2520Gero%2520and%2520Lydia%2520B.%2520Chilton%26entry.1292438233%3D%2520%2520Generative%2520AI%2520brings%2520novel%2520and%2520impressive%2520abilities%2520to%2520help%2520people%2520in%250Aeveryday%2520tasks.%2520There%2520are%2520many%2520AI%2520workflows%2520that%2520solve%2520real%2520and%2520complex%250Aproblems%2520by%2520chaining%2520AI%2520outputs%2520together%2520with%2520human%2520interaction.%2520Although%2520there%250Ais%2520an%2520undeniable%2520lure%2520of%2520AI%252C%2520it%2520is%2520uncertain%2520how%2520useful%2520generative%2520AI%2520workflows%250Aare%2520after%2520the%2520novelty%2520wears%2520off.%2520Additionally%252C%2520workflows%2520built%2520with%2520generative%250AAI%2520have%2520the%2520potential%2520to%2520be%2520easily%2520customized%2520to%2520fit%2520users%2527%2520individual%2520needs%252C%250Abut%2520do%2520users%2520take%2520advantage%2520of%2520this%253F%2520We%2520conducted%2520a%2520three-week%2520longitudinal%250Astudy%2520with%252012%2520users%2520to%2520understand%2520the%2520familiarization%2520and%2520customization%2520of%250Agenerative%2520AI%2520tools%2520for%2520science%2520communication.%2520Our%2520study%2520revealed%2520that%2520there%250Aexists%2520a%2520familiarization%2520phase%252C%2520during%2520which%2520users%2520were%2520exploring%2520the%2520novel%250Acapabilities%2520of%2520the%2520workflow%2520and%2520discovering%2520which%2520aspects%2520they%2520found%2520useful.%250AAfter%2520this%2520phase%252C%2520users%2520understood%2520the%2520workflow%2520and%2520were%2520able%2520to%2520anticipate%2520the%250Aoutputs.%2520Surprisingly%252C%2520after%2520familiarization%2520the%2520perceived%2520utility%2520of%2520the%250Asystem%2520was%2520rated%2520higher%2520than%2520before%252C%2520indicating%2520that%2520the%2520perceived%2520utility%2520of%250AAI%2520is%2520not%2520just%2520a%2520novelty%2520effect.%2520The%2520increase%2520in%2520benefits%2520mainly%2520comes%2520from%250Aend-users%2527%2520ability%2520to%2520customize%2520prompts%252C%2520and%2520thus%2520potentially%2520appropriate%2520the%250Asystem%2520to%2520their%2520own%2520needs.%2520This%2520points%2520to%2520a%2520future%2520where%2520generative%2520AI%2520systems%250Acan%2520allow%2520us%2520to%2520design%2520for%2520appropriation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09894v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Not%20Just%20Novelty%3A%20A%20Longitudinal%20Study%20on%20Utility%20and%20Customization%20of%0A%20%20an%20AI%20Workflow&entry.906535625=Tao%20Long%20and%20Katy%20Ilonka%20Gero%20and%20Lydia%20B.%20Chilton&entry.1292438233=%20%20Generative%20AI%20brings%20novel%20and%20impressive%20abilities%20to%20help%20people%20in%0Aeveryday%20tasks.%20There%20are%20many%20AI%20workflows%20that%20solve%20real%20and%20complex%0Aproblems%20by%20chaining%20AI%20outputs%20together%20with%20human%20interaction.%20Although%20there%0Ais%20an%20undeniable%20lure%20of%20AI%2C%20it%20is%20uncertain%20how%20useful%20generative%20AI%20workflows%0Aare%20after%20the%20novelty%20wears%20off.%20Additionally%2C%20workflows%20built%20with%20generative%0AAI%20have%20the%20potential%20to%20be%20easily%20customized%20to%20fit%20users%27%20individual%20needs%2C%0Abut%20do%20users%20take%20advantage%20of%20this%3F%20We%20conducted%20a%20three-week%20longitudinal%0Astudy%20with%2012%20users%20to%20understand%20the%20familiarization%20and%20customization%20of%0Agenerative%20AI%20tools%20for%20science%20communication.%20Our%20study%20revealed%20that%20there%0Aexists%20a%20familiarization%20phase%2C%20during%20which%20users%20were%20exploring%20the%20novel%0Acapabilities%20of%20the%20workflow%20and%20discovering%20which%20aspects%20they%20found%20useful.%0AAfter%20this%20phase%2C%20users%20understood%20the%20workflow%20and%20were%20able%20to%20anticipate%20the%0Aoutputs.%20Surprisingly%2C%20after%20familiarization%20the%20perceived%20utility%20of%20the%0Asystem%20was%20rated%20higher%20than%20before%2C%20indicating%20that%20the%20perceived%20utility%20of%0AAI%20is%20not%20just%20a%20novelty%20effect.%20The%20increase%20in%20benefits%20mainly%20comes%20from%0Aend-users%27%20ability%20to%20customize%20prompts%2C%20and%20thus%20potentially%20appropriate%20the%0Asystem%20to%20their%20own%20needs.%20This%20points%20to%20a%20future%20where%20generative%20AI%20systems%0Acan%20allow%20us%20to%20design%20for%20appropriation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09894v2&entry.124074799=Read"},
{"title": "The Common Intuition to Transfer Learning Can Win or Lose: Case Studies\n  for Linear Regression", "author": "Yehuda Dar and Daniel LeJeune and Richard G. Baraniuk", "abstract": "  We study a fundamental transfer learning process from source to target linear\nregression tasks, including overparameterized settings where there are more\nlearned parameters than data samples. The target task learning is addressed by\nusing its training data together with the parameters previously computed for\nthe source task. We define a transfer learning approach to the target task as a\nlinear regression optimization with a regularization on the distance between\nthe to-be-learned target parameters and the already-learned source parameters.\nWe analytically characterize the generalization performance of our transfer\nlearning approach and demonstrate its ability to resolve the peak in\ngeneralization errors in double descent phenomena of the minimum L2-norm\nsolution to linear regression. Moreover, we show that for sufficiently related\ntasks, the optimally tuned transfer learning approach can outperform the\noptimally tuned ridge regression method, even when the true parameter vector\nconforms to an isotropic Gaussian prior distribution. Namely, we demonstrate\nthat transfer learning can beat the minimum mean square error (MMSE) solution\nof the independent target task. Our results emphasize the ability of transfer\nlearning to extend the solution space to the target task and, by that, to have\nan improved MMSE solution. We formulate the linear MMSE solution to our\ntransfer learning setting and point out its key differences from the common\ndesign philosophy to transfer learning.\n", "link": "http://arxiv.org/abs/2103.05621v4", "date": "2024-05-31", "relevancy": 1.9517, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5117}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4838}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Common%20Intuition%20to%20Transfer%20Learning%20Can%20Win%20or%20Lose%3A%20Case%20Studies%0A%20%20for%20Linear%20Regression&body=Title%3A%20The%20Common%20Intuition%20to%20Transfer%20Learning%20Can%20Win%20or%20Lose%3A%20Case%20Studies%0A%20%20for%20Linear%20Regression%0AAuthor%3A%20Yehuda%20Dar%20and%20Daniel%20LeJeune%20and%20Richard%20G.%20Baraniuk%0AAbstract%3A%20%20%20We%20study%20a%20fundamental%20transfer%20learning%20process%20from%20source%20to%20target%20linear%0Aregression%20tasks%2C%20including%20overparameterized%20settings%20where%20there%20are%20more%0Alearned%20parameters%20than%20data%20samples.%20The%20target%20task%20learning%20is%20addressed%20by%0Ausing%20its%20training%20data%20together%20with%20the%20parameters%20previously%20computed%20for%0Athe%20source%20task.%20We%20define%20a%20transfer%20learning%20approach%20to%20the%20target%20task%20as%20a%0Alinear%20regression%20optimization%20with%20a%20regularization%20on%20the%20distance%20between%0Athe%20to-be-learned%20target%20parameters%20and%20the%20already-learned%20source%20parameters.%0AWe%20analytically%20characterize%20the%20generalization%20performance%20of%20our%20transfer%0Alearning%20approach%20and%20demonstrate%20its%20ability%20to%20resolve%20the%20peak%20in%0Ageneralization%20errors%20in%20double%20descent%20phenomena%20of%20the%20minimum%20L2-norm%0Asolution%20to%20linear%20regression.%20Moreover%2C%20we%20show%20that%20for%20sufficiently%20related%0Atasks%2C%20the%20optimally%20tuned%20transfer%20learning%20approach%20can%20outperform%20the%0Aoptimally%20tuned%20ridge%20regression%20method%2C%20even%20when%20the%20true%20parameter%20vector%0Aconforms%20to%20an%20isotropic%20Gaussian%20prior%20distribution.%20Namely%2C%20we%20demonstrate%0Athat%20transfer%20learning%20can%20beat%20the%20minimum%20mean%20square%20error%20%28MMSE%29%20solution%0Aof%20the%20independent%20target%20task.%20Our%20results%20emphasize%20the%20ability%20of%20transfer%0Alearning%20to%20extend%20the%20solution%20space%20to%20the%20target%20task%20and%2C%20by%20that%2C%20to%20have%0Aan%20improved%20MMSE%20solution.%20We%20formulate%20the%20linear%20MMSE%20solution%20to%20our%0Atransfer%20learning%20setting%20and%20point%20out%20its%20key%20differences%20from%20the%20common%0Adesign%20philosophy%20to%20transfer%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2103.05621v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Common%2520Intuition%2520to%2520Transfer%2520Learning%2520Can%2520Win%2520or%2520Lose%253A%2520Case%2520Studies%250A%2520%2520for%2520Linear%2520Regression%26entry.906535625%3DYehuda%2520Dar%2520and%2520Daniel%2520LeJeune%2520and%2520Richard%2520G.%2520Baraniuk%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520fundamental%2520transfer%2520learning%2520process%2520from%2520source%2520to%2520target%2520linear%250Aregression%2520tasks%252C%2520including%2520overparameterized%2520settings%2520where%2520there%2520are%2520more%250Alearned%2520parameters%2520than%2520data%2520samples.%2520The%2520target%2520task%2520learning%2520is%2520addressed%2520by%250Ausing%2520its%2520training%2520data%2520together%2520with%2520the%2520parameters%2520previously%2520computed%2520for%250Athe%2520source%2520task.%2520We%2520define%2520a%2520transfer%2520learning%2520approach%2520to%2520the%2520target%2520task%2520as%2520a%250Alinear%2520regression%2520optimization%2520with%2520a%2520regularization%2520on%2520the%2520distance%2520between%250Athe%2520to-be-learned%2520target%2520parameters%2520and%2520the%2520already-learned%2520source%2520parameters.%250AWe%2520analytically%2520characterize%2520the%2520generalization%2520performance%2520of%2520our%2520transfer%250Alearning%2520approach%2520and%2520demonstrate%2520its%2520ability%2520to%2520resolve%2520the%2520peak%2520in%250Ageneralization%2520errors%2520in%2520double%2520descent%2520phenomena%2520of%2520the%2520minimum%2520L2-norm%250Asolution%2520to%2520linear%2520regression.%2520Moreover%252C%2520we%2520show%2520that%2520for%2520sufficiently%2520related%250Atasks%252C%2520the%2520optimally%2520tuned%2520transfer%2520learning%2520approach%2520can%2520outperform%2520the%250Aoptimally%2520tuned%2520ridge%2520regression%2520method%252C%2520even%2520when%2520the%2520true%2520parameter%2520vector%250Aconforms%2520to%2520an%2520isotropic%2520Gaussian%2520prior%2520distribution.%2520Namely%252C%2520we%2520demonstrate%250Athat%2520transfer%2520learning%2520can%2520beat%2520the%2520minimum%2520mean%2520square%2520error%2520%2528MMSE%2529%2520solution%250Aof%2520the%2520independent%2520target%2520task.%2520Our%2520results%2520emphasize%2520the%2520ability%2520of%2520transfer%250Alearning%2520to%2520extend%2520the%2520solution%2520space%2520to%2520the%2520target%2520task%2520and%252C%2520by%2520that%252C%2520to%2520have%250Aan%2520improved%2520MMSE%2520solution.%2520We%2520formulate%2520the%2520linear%2520MMSE%2520solution%2520to%2520our%250Atransfer%2520learning%2520setting%2520and%2520point%2520out%2520its%2520key%2520differences%2520from%2520the%2520common%250Adesign%2520philosophy%2520to%2520transfer%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2103.05621v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Common%20Intuition%20to%20Transfer%20Learning%20Can%20Win%20or%20Lose%3A%20Case%20Studies%0A%20%20for%20Linear%20Regression&entry.906535625=Yehuda%20Dar%20and%20Daniel%20LeJeune%20and%20Richard%20G.%20Baraniuk&entry.1292438233=%20%20We%20study%20a%20fundamental%20transfer%20learning%20process%20from%20source%20to%20target%20linear%0Aregression%20tasks%2C%20including%20overparameterized%20settings%20where%20there%20are%20more%0Alearned%20parameters%20than%20data%20samples.%20The%20target%20task%20learning%20is%20addressed%20by%0Ausing%20its%20training%20data%20together%20with%20the%20parameters%20previously%20computed%20for%0Athe%20source%20task.%20We%20define%20a%20transfer%20learning%20approach%20to%20the%20target%20task%20as%20a%0Alinear%20regression%20optimization%20with%20a%20regularization%20on%20the%20distance%20between%0Athe%20to-be-learned%20target%20parameters%20and%20the%20already-learned%20source%20parameters.%0AWe%20analytically%20characterize%20the%20generalization%20performance%20of%20our%20transfer%0Alearning%20approach%20and%20demonstrate%20its%20ability%20to%20resolve%20the%20peak%20in%0Ageneralization%20errors%20in%20double%20descent%20phenomena%20of%20the%20minimum%20L2-norm%0Asolution%20to%20linear%20regression.%20Moreover%2C%20we%20show%20that%20for%20sufficiently%20related%0Atasks%2C%20the%20optimally%20tuned%20transfer%20learning%20approach%20can%20outperform%20the%0Aoptimally%20tuned%20ridge%20regression%20method%2C%20even%20when%20the%20true%20parameter%20vector%0Aconforms%20to%20an%20isotropic%20Gaussian%20prior%20distribution.%20Namely%2C%20we%20demonstrate%0Athat%20transfer%20learning%20can%20beat%20the%20minimum%20mean%20square%20error%20%28MMSE%29%20solution%0Aof%20the%20independent%20target%20task.%20Our%20results%20emphasize%20the%20ability%20of%20transfer%0Alearning%20to%20extend%20the%20solution%20space%20to%20the%20target%20task%20and%2C%20by%20that%2C%20to%20have%0Aan%20improved%20MMSE%20solution.%20We%20formulate%20the%20linear%20MMSE%20solution%20to%20our%0Atransfer%20learning%20setting%20and%20point%20out%20its%20key%20differences%20from%20the%20common%0Adesign%20philosophy%20to%20transfer%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2103.05621v4&entry.124074799=Read"},
{"title": "Uncertainty Quantification for Bird's Eye View Semantic Segmentation:\n  Methods and Benchmarks", "author": "Linlin Yu and Bowen Yang and Tianhao Wang and Kangshuo Li and Feng Chen", "abstract": "  The fusion of raw features from multiple sensors on an autonomous vehicle to\ncreate a Bird's Eye View (BEV) representation is crucial for planning and\ncontrol systems. There is growing interest in using deep learning models for\nBEV semantic segmentation. Anticipating segmentation errors and improving the\nexplainability of DNNs is essential for autonomous driving, yet it is\nunder-studied. This paper introduces a benchmark for predictive uncertainty\nquantification in BEV segmentation. The benchmark assesses various approaches\nacross three popular datasets using two representative backbones and focuses on\nthe effectiveness of predicted uncertainty in identifying misclassified and\nout-of-distribution (OOD) pixels, as well as calibration. Empirical findings\nhighlight the challenges in uncertainty quantification. Our results find that\nevidential deep learning based approaches show the most promise by efficiently\nquantifying aleatoric and epistemic uncertainty. We propose the\nUncertainty-Focal-Cross-Entropy (UFCE) loss, designed for highly imbalanced\ndata, which consistently improves the segmentation quality and calibration.\nAdditionally, we introduce a vacuity-scaled regularization term that enhances\nthe model's focus on high uncertainty pixels, improving epistemic uncertainty\nquantification.\n", "link": "http://arxiv.org/abs/2405.20986v1", "date": "2024-05-31", "relevancy": 1.9399, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.671}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6378}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Quantification%20for%20Bird%27s%20Eye%20View%20Semantic%20Segmentation%3A%0A%20%20Methods%20and%20Benchmarks&body=Title%3A%20Uncertainty%20Quantification%20for%20Bird%27s%20Eye%20View%20Semantic%20Segmentation%3A%0A%20%20Methods%20and%20Benchmarks%0AAuthor%3A%20Linlin%20Yu%20and%20Bowen%20Yang%20and%20Tianhao%20Wang%20and%20Kangshuo%20Li%20and%20Feng%20Chen%0AAbstract%3A%20%20%20The%20fusion%20of%20raw%20features%20from%20multiple%20sensors%20on%20an%20autonomous%20vehicle%20to%0Acreate%20a%20Bird%27s%20Eye%20View%20%28BEV%29%20representation%20is%20crucial%20for%20planning%20and%0Acontrol%20systems.%20There%20is%20growing%20interest%20in%20using%20deep%20learning%20models%20for%0ABEV%20semantic%20segmentation.%20Anticipating%20segmentation%20errors%20and%20improving%20the%0Aexplainability%20of%20DNNs%20is%20essential%20for%20autonomous%20driving%2C%20yet%20it%20is%0Aunder-studied.%20This%20paper%20introduces%20a%20benchmark%20for%20predictive%20uncertainty%0Aquantification%20in%20BEV%20segmentation.%20The%20benchmark%20assesses%20various%20approaches%0Aacross%20three%20popular%20datasets%20using%20two%20representative%20backbones%20and%20focuses%20on%0Athe%20effectiveness%20of%20predicted%20uncertainty%20in%20identifying%20misclassified%20and%0Aout-of-distribution%20%28OOD%29%20pixels%2C%20as%20well%20as%20calibration.%20Empirical%20findings%0Ahighlight%20the%20challenges%20in%20uncertainty%20quantification.%20Our%20results%20find%20that%0Aevidential%20deep%20learning%20based%20approaches%20show%20the%20most%20promise%20by%20efficiently%0Aquantifying%20aleatoric%20and%20epistemic%20uncertainty.%20We%20propose%20the%0AUncertainty-Focal-Cross-Entropy%20%28UFCE%29%20loss%2C%20designed%20for%20highly%20imbalanced%0Adata%2C%20which%20consistently%20improves%20the%20segmentation%20quality%20and%20calibration.%0AAdditionally%2C%20we%20introduce%20a%20vacuity-scaled%20regularization%20term%20that%20enhances%0Athe%20model%27s%20focus%20on%20high%20uncertainty%20pixels%2C%20improving%20epistemic%20uncertainty%0Aquantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Quantification%2520for%2520Bird%2527s%2520Eye%2520View%2520Semantic%2520Segmentation%253A%250A%2520%2520Methods%2520and%2520Benchmarks%26entry.906535625%3DLinlin%2520Yu%2520and%2520Bowen%2520Yang%2520and%2520Tianhao%2520Wang%2520and%2520Kangshuo%2520Li%2520and%2520Feng%2520Chen%26entry.1292438233%3D%2520%2520The%2520fusion%2520of%2520raw%2520features%2520from%2520multiple%2520sensors%2520on%2520an%2520autonomous%2520vehicle%2520to%250Acreate%2520a%2520Bird%2527s%2520Eye%2520View%2520%2528BEV%2529%2520representation%2520is%2520crucial%2520for%2520planning%2520and%250Acontrol%2520systems.%2520There%2520is%2520growing%2520interest%2520in%2520using%2520deep%2520learning%2520models%2520for%250ABEV%2520semantic%2520segmentation.%2520Anticipating%2520segmentation%2520errors%2520and%2520improving%2520the%250Aexplainability%2520of%2520DNNs%2520is%2520essential%2520for%2520autonomous%2520driving%252C%2520yet%2520it%2520is%250Aunder-studied.%2520This%2520paper%2520introduces%2520a%2520benchmark%2520for%2520predictive%2520uncertainty%250Aquantification%2520in%2520BEV%2520segmentation.%2520The%2520benchmark%2520assesses%2520various%2520approaches%250Aacross%2520three%2520popular%2520datasets%2520using%2520two%2520representative%2520backbones%2520and%2520focuses%2520on%250Athe%2520effectiveness%2520of%2520predicted%2520uncertainty%2520in%2520identifying%2520misclassified%2520and%250Aout-of-distribution%2520%2528OOD%2529%2520pixels%252C%2520as%2520well%2520as%2520calibration.%2520Empirical%2520findings%250Ahighlight%2520the%2520challenges%2520in%2520uncertainty%2520quantification.%2520Our%2520results%2520find%2520that%250Aevidential%2520deep%2520learning%2520based%2520approaches%2520show%2520the%2520most%2520promise%2520by%2520efficiently%250Aquantifying%2520aleatoric%2520and%2520epistemic%2520uncertainty.%2520We%2520propose%2520the%250AUncertainty-Focal-Cross-Entropy%2520%2528UFCE%2529%2520loss%252C%2520designed%2520for%2520highly%2520imbalanced%250Adata%252C%2520which%2520consistently%2520improves%2520the%2520segmentation%2520quality%2520and%2520calibration.%250AAdditionally%252C%2520we%2520introduce%2520a%2520vacuity-scaled%2520regularization%2520term%2520that%2520enhances%250Athe%2520model%2527s%2520focus%2520on%2520high%2520uncertainty%2520pixels%252C%2520improving%2520epistemic%2520uncertainty%250Aquantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Quantification%20for%20Bird%27s%20Eye%20View%20Semantic%20Segmentation%3A%0A%20%20Methods%20and%20Benchmarks&entry.906535625=Linlin%20Yu%20and%20Bowen%20Yang%20and%20Tianhao%20Wang%20and%20Kangshuo%20Li%20and%20Feng%20Chen&entry.1292438233=%20%20The%20fusion%20of%20raw%20features%20from%20multiple%20sensors%20on%20an%20autonomous%20vehicle%20to%0Acreate%20a%20Bird%27s%20Eye%20View%20%28BEV%29%20representation%20is%20crucial%20for%20planning%20and%0Acontrol%20systems.%20There%20is%20growing%20interest%20in%20using%20deep%20learning%20models%20for%0ABEV%20semantic%20segmentation.%20Anticipating%20segmentation%20errors%20and%20improving%20the%0Aexplainability%20of%20DNNs%20is%20essential%20for%20autonomous%20driving%2C%20yet%20it%20is%0Aunder-studied.%20This%20paper%20introduces%20a%20benchmark%20for%20predictive%20uncertainty%0Aquantification%20in%20BEV%20segmentation.%20The%20benchmark%20assesses%20various%20approaches%0Aacross%20three%20popular%20datasets%20using%20two%20representative%20backbones%20and%20focuses%20on%0Athe%20effectiveness%20of%20predicted%20uncertainty%20in%20identifying%20misclassified%20and%0Aout-of-distribution%20%28OOD%29%20pixels%2C%20as%20well%20as%20calibration.%20Empirical%20findings%0Ahighlight%20the%20challenges%20in%20uncertainty%20quantification.%20Our%20results%20find%20that%0Aevidential%20deep%20learning%20based%20approaches%20show%20the%20most%20promise%20by%20efficiently%0Aquantifying%20aleatoric%20and%20epistemic%20uncertainty.%20We%20propose%20the%0AUncertainty-Focal-Cross-Entropy%20%28UFCE%29%20loss%2C%20designed%20for%20highly%20imbalanced%0Adata%2C%20which%20consistently%20improves%20the%20segmentation%20quality%20and%20calibration.%0AAdditionally%2C%20we%20introduce%20a%20vacuity-scaled%20regularization%20term%20that%20enhances%0Athe%20model%27s%20focus%20on%20high%20uncertainty%20pixels%2C%20improving%20epistemic%20uncertainty%0Aquantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20986v1&entry.124074799=Read"},
{"title": "Perimeter Control with Heterogeneous Metering Rates for Cordon Signals:\n  A Physics-Regularized Multi-Agent Reinforcement Learning Approach", "author": "Jiajie Yu and Pierre-Antoine Laharotte and Yu Han and Wei Ma and Ludovic Leclercq", "abstract": "  Perimeter Control (PC) strategies have been proposed to address urban road\nnetwork control in oversaturated situations by regulating the transfer flow of\nthe Protected Network (PN) based on the Macroscopic Fundamental Diagram (MFD).\nThe uniform metering rate for cordon signals in most existing studies overlooks\nthe variance of local traffic states at the intersection level, which may cause\nsevere local traffic congestion and degradation of the network stability. PC\nstrategies with heterogeneous metering rates for cordon signals allow precise\ncontrol for the perimeter but the complexity of the problem increases\nexponentially with the scale of the PN. This paper leverages a Multi-Agent\nReinforcement Learning (MARL)-based traffic signal control framework to\ndecompose this PC problem, which considers heterogeneous metering rates for\ncordon signals, into multi-agent cooperation tasks. Each agent controls an\nindividual signal located in the cordon, decreasing the dimension of action\nspace for the controller compared to centralized methods. A physics\nregularization approach for the MARL framework is proposed to ensure the\ndistributed cordon signal controllers are aware of the global network state by\nencoding MFD-based knowledge into the action-value functions of the local\nagents. The proposed PC strategy is operated as a two-stage system, with a\nfeedback PC strategy detecting the overall traffic state within the PN and then\ndistributing local instructions to cordon signals controllers in the MARL\nframework via the physics regularization. Through numerical tests with\ndifferent demand patterns in a microscopic traffic environment, the proposed PC\nstrategy shows promising robustness and transferability. It outperforms\nstate-of-the-art feedback PC strategies in increasing network throughput,\ndecreasing distributed delay for gate links, and reducing carbon emissions.\n", "link": "http://arxiv.org/abs/2308.12985v2", "date": "2024-05-31", "relevancy": 1.9367, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5056}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4896}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perimeter%20Control%20with%20Heterogeneous%20Metering%20Rates%20for%20Cordon%20Signals%3A%0A%20%20A%20Physics-Regularized%20Multi-Agent%20Reinforcement%20Learning%20Approach&body=Title%3A%20Perimeter%20Control%20with%20Heterogeneous%20Metering%20Rates%20for%20Cordon%20Signals%3A%0A%20%20A%20Physics-Regularized%20Multi-Agent%20Reinforcement%20Learning%20Approach%0AAuthor%3A%20Jiajie%20Yu%20and%20Pierre-Antoine%20Laharotte%20and%20Yu%20Han%20and%20Wei%20Ma%20and%20Ludovic%20Leclercq%0AAbstract%3A%20%20%20Perimeter%20Control%20%28PC%29%20strategies%20have%20been%20proposed%20to%20address%20urban%20road%0Anetwork%20control%20in%20oversaturated%20situations%20by%20regulating%20the%20transfer%20flow%20of%0Athe%20Protected%20Network%20%28PN%29%20based%20on%20the%20Macroscopic%20Fundamental%20Diagram%20%28MFD%29.%0AThe%20uniform%20metering%20rate%20for%20cordon%20signals%20in%20most%20existing%20studies%20overlooks%0Athe%20variance%20of%20local%20traffic%20states%20at%20the%20intersection%20level%2C%20which%20may%20cause%0Asevere%20local%20traffic%20congestion%20and%20degradation%20of%20the%20network%20stability.%20PC%0Astrategies%20with%20heterogeneous%20metering%20rates%20for%20cordon%20signals%20allow%20precise%0Acontrol%20for%20the%20perimeter%20but%20the%20complexity%20of%20the%20problem%20increases%0Aexponentially%20with%20the%20scale%20of%20the%20PN.%20This%20paper%20leverages%20a%20Multi-Agent%0AReinforcement%20Learning%20%28MARL%29-based%20traffic%20signal%20control%20framework%20to%0Adecompose%20this%20PC%20problem%2C%20which%20considers%20heterogeneous%20metering%20rates%20for%0Acordon%20signals%2C%20into%20multi-agent%20cooperation%20tasks.%20Each%20agent%20controls%20an%0Aindividual%20signal%20located%20in%20the%20cordon%2C%20decreasing%20the%20dimension%20of%20action%0Aspace%20for%20the%20controller%20compared%20to%20centralized%20methods.%20A%20physics%0Aregularization%20approach%20for%20the%20MARL%20framework%20is%20proposed%20to%20ensure%20the%0Adistributed%20cordon%20signal%20controllers%20are%20aware%20of%20the%20global%20network%20state%20by%0Aencoding%20MFD-based%20knowledge%20into%20the%20action-value%20functions%20of%20the%20local%0Aagents.%20The%20proposed%20PC%20strategy%20is%20operated%20as%20a%20two-stage%20system%2C%20with%20a%0Afeedback%20PC%20strategy%20detecting%20the%20overall%20traffic%20state%20within%20the%20PN%20and%20then%0Adistributing%20local%20instructions%20to%20cordon%20signals%20controllers%20in%20the%20MARL%0Aframework%20via%20the%20physics%20regularization.%20Through%20numerical%20tests%20with%0Adifferent%20demand%20patterns%20in%20a%20microscopic%20traffic%20environment%2C%20the%20proposed%20PC%0Astrategy%20shows%20promising%20robustness%20and%20transferability.%20It%20outperforms%0Astate-of-the-art%20feedback%20PC%20strategies%20in%20increasing%20network%20throughput%2C%0Adecreasing%20distributed%20delay%20for%20gate%20links%2C%20and%20reducing%20carbon%20emissions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12985v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerimeter%2520Control%2520with%2520Heterogeneous%2520Metering%2520Rates%2520for%2520Cordon%2520Signals%253A%250A%2520%2520A%2520Physics-Regularized%2520Multi-Agent%2520Reinforcement%2520Learning%2520Approach%26entry.906535625%3DJiajie%2520Yu%2520and%2520Pierre-Antoine%2520Laharotte%2520and%2520Yu%2520Han%2520and%2520Wei%2520Ma%2520and%2520Ludovic%2520Leclercq%26entry.1292438233%3D%2520%2520Perimeter%2520Control%2520%2528PC%2529%2520strategies%2520have%2520been%2520proposed%2520to%2520address%2520urban%2520road%250Anetwork%2520control%2520in%2520oversaturated%2520situations%2520by%2520regulating%2520the%2520transfer%2520flow%2520of%250Athe%2520Protected%2520Network%2520%2528PN%2529%2520based%2520on%2520the%2520Macroscopic%2520Fundamental%2520Diagram%2520%2528MFD%2529.%250AThe%2520uniform%2520metering%2520rate%2520for%2520cordon%2520signals%2520in%2520most%2520existing%2520studies%2520overlooks%250Athe%2520variance%2520of%2520local%2520traffic%2520states%2520at%2520the%2520intersection%2520level%252C%2520which%2520may%2520cause%250Asevere%2520local%2520traffic%2520congestion%2520and%2520degradation%2520of%2520the%2520network%2520stability.%2520PC%250Astrategies%2520with%2520heterogeneous%2520metering%2520rates%2520for%2520cordon%2520signals%2520allow%2520precise%250Acontrol%2520for%2520the%2520perimeter%2520but%2520the%2520complexity%2520of%2520the%2520problem%2520increases%250Aexponentially%2520with%2520the%2520scale%2520of%2520the%2520PN.%2520This%2520paper%2520leverages%2520a%2520Multi-Agent%250AReinforcement%2520Learning%2520%2528MARL%2529-based%2520traffic%2520signal%2520control%2520framework%2520to%250Adecompose%2520this%2520PC%2520problem%252C%2520which%2520considers%2520heterogeneous%2520metering%2520rates%2520for%250Acordon%2520signals%252C%2520into%2520multi-agent%2520cooperation%2520tasks.%2520Each%2520agent%2520controls%2520an%250Aindividual%2520signal%2520located%2520in%2520the%2520cordon%252C%2520decreasing%2520the%2520dimension%2520of%2520action%250Aspace%2520for%2520the%2520controller%2520compared%2520to%2520centralized%2520methods.%2520A%2520physics%250Aregularization%2520approach%2520for%2520the%2520MARL%2520framework%2520is%2520proposed%2520to%2520ensure%2520the%250Adistributed%2520cordon%2520signal%2520controllers%2520are%2520aware%2520of%2520the%2520global%2520network%2520state%2520by%250Aencoding%2520MFD-based%2520knowledge%2520into%2520the%2520action-value%2520functions%2520of%2520the%2520local%250Aagents.%2520The%2520proposed%2520PC%2520strategy%2520is%2520operated%2520as%2520a%2520two-stage%2520system%252C%2520with%2520a%250Afeedback%2520PC%2520strategy%2520detecting%2520the%2520overall%2520traffic%2520state%2520within%2520the%2520PN%2520and%2520then%250Adistributing%2520local%2520instructions%2520to%2520cordon%2520signals%2520controllers%2520in%2520the%2520MARL%250Aframework%2520via%2520the%2520physics%2520regularization.%2520Through%2520numerical%2520tests%2520with%250Adifferent%2520demand%2520patterns%2520in%2520a%2520microscopic%2520traffic%2520environment%252C%2520the%2520proposed%2520PC%250Astrategy%2520shows%2520promising%2520robustness%2520and%2520transferability.%2520It%2520outperforms%250Astate-of-the-art%2520feedback%2520PC%2520strategies%2520in%2520increasing%2520network%2520throughput%252C%250Adecreasing%2520distributed%2520delay%2520for%2520gate%2520links%252C%2520and%2520reducing%2520carbon%2520emissions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.12985v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perimeter%20Control%20with%20Heterogeneous%20Metering%20Rates%20for%20Cordon%20Signals%3A%0A%20%20A%20Physics-Regularized%20Multi-Agent%20Reinforcement%20Learning%20Approach&entry.906535625=Jiajie%20Yu%20and%20Pierre-Antoine%20Laharotte%20and%20Yu%20Han%20and%20Wei%20Ma%20and%20Ludovic%20Leclercq&entry.1292438233=%20%20Perimeter%20Control%20%28PC%29%20strategies%20have%20been%20proposed%20to%20address%20urban%20road%0Anetwork%20control%20in%20oversaturated%20situations%20by%20regulating%20the%20transfer%20flow%20of%0Athe%20Protected%20Network%20%28PN%29%20based%20on%20the%20Macroscopic%20Fundamental%20Diagram%20%28MFD%29.%0AThe%20uniform%20metering%20rate%20for%20cordon%20signals%20in%20most%20existing%20studies%20overlooks%0Athe%20variance%20of%20local%20traffic%20states%20at%20the%20intersection%20level%2C%20which%20may%20cause%0Asevere%20local%20traffic%20congestion%20and%20degradation%20of%20the%20network%20stability.%20PC%0Astrategies%20with%20heterogeneous%20metering%20rates%20for%20cordon%20signals%20allow%20precise%0Acontrol%20for%20the%20perimeter%20but%20the%20complexity%20of%20the%20problem%20increases%0Aexponentially%20with%20the%20scale%20of%20the%20PN.%20This%20paper%20leverages%20a%20Multi-Agent%0AReinforcement%20Learning%20%28MARL%29-based%20traffic%20signal%20control%20framework%20to%0Adecompose%20this%20PC%20problem%2C%20which%20considers%20heterogeneous%20metering%20rates%20for%0Acordon%20signals%2C%20into%20multi-agent%20cooperation%20tasks.%20Each%20agent%20controls%20an%0Aindividual%20signal%20located%20in%20the%20cordon%2C%20decreasing%20the%20dimension%20of%20action%0Aspace%20for%20the%20controller%20compared%20to%20centralized%20methods.%20A%20physics%0Aregularization%20approach%20for%20the%20MARL%20framework%20is%20proposed%20to%20ensure%20the%0Adistributed%20cordon%20signal%20controllers%20are%20aware%20of%20the%20global%20network%20state%20by%0Aencoding%20MFD-based%20knowledge%20into%20the%20action-value%20functions%20of%20the%20local%0Aagents.%20The%20proposed%20PC%20strategy%20is%20operated%20as%20a%20two-stage%20system%2C%20with%20a%0Afeedback%20PC%20strategy%20detecting%20the%20overall%20traffic%20state%20within%20the%20PN%20and%20then%0Adistributing%20local%20instructions%20to%20cordon%20signals%20controllers%20in%20the%20MARL%0Aframework%20via%20the%20physics%20regularization.%20Through%20numerical%20tests%20with%0Adifferent%20demand%20patterns%20in%20a%20microscopic%20traffic%20environment%2C%20the%20proposed%20PC%0Astrategy%20shows%20promising%20robustness%20and%20transferability.%20It%20outperforms%0Astate-of-the-art%20feedback%20PC%20strategies%20in%20increasing%20network%20throughput%2C%0Adecreasing%20distributed%20delay%20for%20gate%20links%2C%20and%20reducing%20carbon%20emissions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12985v2&entry.124074799=Read"},
{"title": "FedSheafHN: Personalized Federated Learning on Graph-structured Data", "author": "Wenfei Liang and Yanan Zhao and Rui She and Yiming Li and Wee Peng Tay", "abstract": "  Personalized subgraph Federated Learning (FL) is a task that customizes Graph\nNeural Networks (GNNs) to individual client needs, accommodating diverse data\ndistributions. However, applying hypernetworks in FL, while aiming to\nfacilitate model personalization, often encounters challenges due to inadequate\nrepresentation of client-specific characteristics. To overcome these\nlimitations, we propose a model called FedSheafHN, using enhanced collaboration\ngraph embedding and efficient personalized model parameter generation.\nSpecifically, our model embeds each client's local subgraph into a\nserver-constructed collaboration graph. We utilize sheaf diffusion in the\ncollaboration graph to learn client representations. Our model improves the\nintegration and interpretation of complex client characteristics. Furthermore,\nour model ensures the generation of personalized models through advanced\nhypernetworks optimized for parallel operations across clients. Empirical\nevaluations demonstrate that FedSheafHN outperforms existing methods in most\nscenarios, in terms of client model performance on various graph-structured\ndatasets. It also has fast model convergence and effective new clients\ngeneralization.\n", "link": "http://arxiv.org/abs/2405.16056v3", "date": "2024-05-31", "relevancy": 1.9314, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4948}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4847}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedSheafHN%3A%20Personalized%20Federated%20Learning%20on%20Graph-structured%20Data&body=Title%3A%20FedSheafHN%3A%20Personalized%20Federated%20Learning%20on%20Graph-structured%20Data%0AAuthor%3A%20Wenfei%20Liang%20and%20Yanan%20Zhao%20and%20Rui%20She%20and%20Yiming%20Li%20and%20Wee%20Peng%20Tay%0AAbstract%3A%20%20%20Personalized%20subgraph%20Federated%20Learning%20%28FL%29%20is%20a%20task%20that%20customizes%20Graph%0ANeural%20Networks%20%28GNNs%29%20to%20individual%20client%20needs%2C%20accommodating%20diverse%20data%0Adistributions.%20However%2C%20applying%20hypernetworks%20in%20FL%2C%20while%20aiming%20to%0Afacilitate%20model%20personalization%2C%20often%20encounters%20challenges%20due%20to%20inadequate%0Arepresentation%20of%20client-specific%20characteristics.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20model%20called%20FedSheafHN%2C%20using%20enhanced%20collaboration%0Agraph%20embedding%20and%20efficient%20personalized%20model%20parameter%20generation.%0ASpecifically%2C%20our%20model%20embeds%20each%20client%27s%20local%20subgraph%20into%20a%0Aserver-constructed%20collaboration%20graph.%20We%20utilize%20sheaf%20diffusion%20in%20the%0Acollaboration%20graph%20to%20learn%20client%20representations.%20Our%20model%20improves%20the%0Aintegration%20and%20interpretation%20of%20complex%20client%20characteristics.%20Furthermore%2C%0Aour%20model%20ensures%20the%20generation%20of%20personalized%20models%20through%20advanced%0Ahypernetworks%20optimized%20for%20parallel%20operations%20across%20clients.%20Empirical%0Aevaluations%20demonstrate%20that%20FedSheafHN%20outperforms%20existing%20methods%20in%20most%0Ascenarios%2C%20in%20terms%20of%20client%20model%20performance%20on%20various%20graph-structured%0Adatasets.%20It%20also%20has%20fast%20model%20convergence%20and%20effective%20new%20clients%0Ageneralization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16056v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedSheafHN%253A%2520Personalized%2520Federated%2520Learning%2520on%2520Graph-structured%2520Data%26entry.906535625%3DWenfei%2520Liang%2520and%2520Yanan%2520Zhao%2520and%2520Rui%2520She%2520and%2520Yiming%2520Li%2520and%2520Wee%2520Peng%2520Tay%26entry.1292438233%3D%2520%2520Personalized%2520subgraph%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520task%2520that%2520customizes%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%2520to%2520individual%2520client%2520needs%252C%2520accommodating%2520diverse%2520data%250Adistributions.%2520However%252C%2520applying%2520hypernetworks%2520in%2520FL%252C%2520while%2520aiming%2520to%250Afacilitate%2520model%2520personalization%252C%2520often%2520encounters%2520challenges%2520due%2520to%2520inadequate%250Arepresentation%2520of%2520client-specific%2520characteristics.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520propose%2520a%2520model%2520called%2520FedSheafHN%252C%2520using%2520enhanced%2520collaboration%250Agraph%2520embedding%2520and%2520efficient%2520personalized%2520model%2520parameter%2520generation.%250ASpecifically%252C%2520our%2520model%2520embeds%2520each%2520client%2527s%2520local%2520subgraph%2520into%2520a%250Aserver-constructed%2520collaboration%2520graph.%2520We%2520utilize%2520sheaf%2520diffusion%2520in%2520the%250Acollaboration%2520graph%2520to%2520learn%2520client%2520representations.%2520Our%2520model%2520improves%2520the%250Aintegration%2520and%2520interpretation%2520of%2520complex%2520client%2520characteristics.%2520Furthermore%252C%250Aour%2520model%2520ensures%2520the%2520generation%2520of%2520personalized%2520models%2520through%2520advanced%250Ahypernetworks%2520optimized%2520for%2520parallel%2520operations%2520across%2520clients.%2520Empirical%250Aevaluations%2520demonstrate%2520that%2520FedSheafHN%2520outperforms%2520existing%2520methods%2520in%2520most%250Ascenarios%252C%2520in%2520terms%2520of%2520client%2520model%2520performance%2520on%2520various%2520graph-structured%250Adatasets.%2520It%2520also%2520has%2520fast%2520model%2520convergence%2520and%2520effective%2520new%2520clients%250Ageneralization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16056v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedSheafHN%3A%20Personalized%20Federated%20Learning%20on%20Graph-structured%20Data&entry.906535625=Wenfei%20Liang%20and%20Yanan%20Zhao%20and%20Rui%20She%20and%20Yiming%20Li%20and%20Wee%20Peng%20Tay&entry.1292438233=%20%20Personalized%20subgraph%20Federated%20Learning%20%28FL%29%20is%20a%20task%20that%20customizes%20Graph%0ANeural%20Networks%20%28GNNs%29%20to%20individual%20client%20needs%2C%20accommodating%20diverse%20data%0Adistributions.%20However%2C%20applying%20hypernetworks%20in%20FL%2C%20while%20aiming%20to%0Afacilitate%20model%20personalization%2C%20often%20encounters%20challenges%20due%20to%20inadequate%0Arepresentation%20of%20client-specific%20characteristics.%20To%20overcome%20these%0Alimitations%2C%20we%20propose%20a%20model%20called%20FedSheafHN%2C%20using%20enhanced%20collaboration%0Agraph%20embedding%20and%20efficient%20personalized%20model%20parameter%20generation.%0ASpecifically%2C%20our%20model%20embeds%20each%20client%27s%20local%20subgraph%20into%20a%0Aserver-constructed%20collaboration%20graph.%20We%20utilize%20sheaf%20diffusion%20in%20the%0Acollaboration%20graph%20to%20learn%20client%20representations.%20Our%20model%20improves%20the%0Aintegration%20and%20interpretation%20of%20complex%20client%20characteristics.%20Furthermore%2C%0Aour%20model%20ensures%20the%20generation%20of%20personalized%20models%20through%20advanced%0Ahypernetworks%20optimized%20for%20parallel%20operations%20across%20clients.%20Empirical%0Aevaluations%20demonstrate%20that%20FedSheafHN%20outperforms%20existing%20methods%20in%20most%0Ascenarios%2C%20in%20terms%20of%20client%20model%20performance%20on%20various%20graph-structured%0Adatasets.%20It%20also%20has%20fast%20model%20convergence%20and%20effective%20new%20clients%0Ageneralization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16056v3&entry.124074799=Read"},
{"title": "TIC-TAC: A Framework for Improved Covariance Estimation in Deep\n  Heteroscedastic Regression", "author": "Megh Shukla and Mathieu Salzmann and Alexandre Alahi", "abstract": "  Deep heteroscedastic regression involves jointly optimizing the mean and\ncovariance of the predicted distribution using the negative log-likelihood.\nHowever, recent works show that this may result in sub-optimal convergence due\nto the challenges associated with covariance estimation. While the literature\naddresses this by proposing alternate formulations to mitigate the impact of\nthe predicted covariance, we focus on improving the predicted covariance\nitself. We study two questions: (1) Does the predicted covariance truly capture\nthe randomness of the predicted mean? (2) In the absence of supervision, how\ncan we quantify the accuracy of covariance estimation? We address (1) with a\nTaylor Induced Covariance (TIC), which captures the randomness of the predicted\nmean by incorporating its gradient and curvature through the second order\nTaylor polynomial. Furthermore, we tackle (2) by introducing a Task Agnostic\nCorrelations (TAC) metric, which combines the notion of correlations and\nabsolute error to evaluate the covariance. We evaluate TIC-TAC across multiple\nexperiments spanning synthetic and real-world datasets. Our results show that\nnot only does TIC accurately learn the covariance, it additionally facilitates\nan improved convergence of the negative log-likelihood. Our code is available\nat https://github.com/vita-epfl/TIC-TAC\n", "link": "http://arxiv.org/abs/2310.18953v2", "date": "2024-05-31", "relevancy": 1.928, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4971}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4905}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TIC-TAC%3A%20A%20Framework%20for%20Improved%20Covariance%20Estimation%20in%20Deep%0A%20%20Heteroscedastic%20Regression&body=Title%3A%20TIC-TAC%3A%20A%20Framework%20for%20Improved%20Covariance%20Estimation%20in%20Deep%0A%20%20Heteroscedastic%20Regression%0AAuthor%3A%20Megh%20Shukla%20and%20Mathieu%20Salzmann%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Deep%20heteroscedastic%20regression%20involves%20jointly%20optimizing%20the%20mean%20and%0Acovariance%20of%20the%20predicted%20distribution%20using%20the%20negative%20log-likelihood.%0AHowever%2C%20recent%20works%20show%20that%20this%20may%20result%20in%20sub-optimal%20convergence%20due%0Ato%20the%20challenges%20associated%20with%20covariance%20estimation.%20While%20the%20literature%0Aaddresses%20this%20by%20proposing%20alternate%20formulations%20to%20mitigate%20the%20impact%20of%0Athe%20predicted%20covariance%2C%20we%20focus%20on%20improving%20the%20predicted%20covariance%0Aitself.%20We%20study%20two%20questions%3A%20%281%29%20Does%20the%20predicted%20covariance%20truly%20capture%0Athe%20randomness%20of%20the%20predicted%20mean%3F%20%282%29%20In%20the%20absence%20of%20supervision%2C%20how%0Acan%20we%20quantify%20the%20accuracy%20of%20covariance%20estimation%3F%20We%20address%20%281%29%20with%20a%0ATaylor%20Induced%20Covariance%20%28TIC%29%2C%20which%20captures%20the%20randomness%20of%20the%20predicted%0Amean%20by%20incorporating%20its%20gradient%20and%20curvature%20through%20the%20second%20order%0ATaylor%20polynomial.%20Furthermore%2C%20we%20tackle%20%282%29%20by%20introducing%20a%20Task%20Agnostic%0ACorrelations%20%28TAC%29%20metric%2C%20which%20combines%20the%20notion%20of%20correlations%20and%0Aabsolute%20error%20to%20evaluate%20the%20covariance.%20We%20evaluate%20TIC-TAC%20across%20multiple%0Aexperiments%20spanning%20synthetic%20and%20real-world%20datasets.%20Our%20results%20show%20that%0Anot%20only%20does%20TIC%20accurately%20learn%20the%20covariance%2C%20it%20additionally%20facilitates%0Aan%20improved%20convergence%20of%20the%20negative%20log-likelihood.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/vita-epfl/TIC-TAC%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18953v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTIC-TAC%253A%2520A%2520Framework%2520for%2520Improved%2520Covariance%2520Estimation%2520in%2520Deep%250A%2520%2520Heteroscedastic%2520Regression%26entry.906535625%3DMegh%2520Shukla%2520and%2520Mathieu%2520Salzmann%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3D%2520%2520Deep%2520heteroscedastic%2520regression%2520involves%2520jointly%2520optimizing%2520the%2520mean%2520and%250Acovariance%2520of%2520the%2520predicted%2520distribution%2520using%2520the%2520negative%2520log-likelihood.%250AHowever%252C%2520recent%2520works%2520show%2520that%2520this%2520may%2520result%2520in%2520sub-optimal%2520convergence%2520due%250Ato%2520the%2520challenges%2520associated%2520with%2520covariance%2520estimation.%2520While%2520the%2520literature%250Aaddresses%2520this%2520by%2520proposing%2520alternate%2520formulations%2520to%2520mitigate%2520the%2520impact%2520of%250Athe%2520predicted%2520covariance%252C%2520we%2520focus%2520on%2520improving%2520the%2520predicted%2520covariance%250Aitself.%2520We%2520study%2520two%2520questions%253A%2520%25281%2529%2520Does%2520the%2520predicted%2520covariance%2520truly%2520capture%250Athe%2520randomness%2520of%2520the%2520predicted%2520mean%253F%2520%25282%2529%2520In%2520the%2520absence%2520of%2520supervision%252C%2520how%250Acan%2520we%2520quantify%2520the%2520accuracy%2520of%2520covariance%2520estimation%253F%2520We%2520address%2520%25281%2529%2520with%2520a%250ATaylor%2520Induced%2520Covariance%2520%2528TIC%2529%252C%2520which%2520captures%2520the%2520randomness%2520of%2520the%2520predicted%250Amean%2520by%2520incorporating%2520its%2520gradient%2520and%2520curvature%2520through%2520the%2520second%2520order%250ATaylor%2520polynomial.%2520Furthermore%252C%2520we%2520tackle%2520%25282%2529%2520by%2520introducing%2520a%2520Task%2520Agnostic%250ACorrelations%2520%2528TAC%2529%2520metric%252C%2520which%2520combines%2520the%2520notion%2520of%2520correlations%2520and%250Aabsolute%2520error%2520to%2520evaluate%2520the%2520covariance.%2520We%2520evaluate%2520TIC-TAC%2520across%2520multiple%250Aexperiments%2520spanning%2520synthetic%2520and%2520real-world%2520datasets.%2520Our%2520results%2520show%2520that%250Anot%2520only%2520does%2520TIC%2520accurately%2520learn%2520the%2520covariance%252C%2520it%2520additionally%2520facilitates%250Aan%2520improved%2520convergence%2520of%2520the%2520negative%2520log-likelihood.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/vita-epfl/TIC-TAC%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18953v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TIC-TAC%3A%20A%20Framework%20for%20Improved%20Covariance%20Estimation%20in%20Deep%0A%20%20Heteroscedastic%20Regression&entry.906535625=Megh%20Shukla%20and%20Mathieu%20Salzmann%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Deep%20heteroscedastic%20regression%20involves%20jointly%20optimizing%20the%20mean%20and%0Acovariance%20of%20the%20predicted%20distribution%20using%20the%20negative%20log-likelihood.%0AHowever%2C%20recent%20works%20show%20that%20this%20may%20result%20in%20sub-optimal%20convergence%20due%0Ato%20the%20challenges%20associated%20with%20covariance%20estimation.%20While%20the%20literature%0Aaddresses%20this%20by%20proposing%20alternate%20formulations%20to%20mitigate%20the%20impact%20of%0Athe%20predicted%20covariance%2C%20we%20focus%20on%20improving%20the%20predicted%20covariance%0Aitself.%20We%20study%20two%20questions%3A%20%281%29%20Does%20the%20predicted%20covariance%20truly%20capture%0Athe%20randomness%20of%20the%20predicted%20mean%3F%20%282%29%20In%20the%20absence%20of%20supervision%2C%20how%0Acan%20we%20quantify%20the%20accuracy%20of%20covariance%20estimation%3F%20We%20address%20%281%29%20with%20a%0ATaylor%20Induced%20Covariance%20%28TIC%29%2C%20which%20captures%20the%20randomness%20of%20the%20predicted%0Amean%20by%20incorporating%20its%20gradient%20and%20curvature%20through%20the%20second%20order%0ATaylor%20polynomial.%20Furthermore%2C%20we%20tackle%20%282%29%20by%20introducing%20a%20Task%20Agnostic%0ACorrelations%20%28TAC%29%20metric%2C%20which%20combines%20the%20notion%20of%20correlations%20and%0Aabsolute%20error%20to%20evaluate%20the%20covariance.%20We%20evaluate%20TIC-TAC%20across%20multiple%0Aexperiments%20spanning%20synthetic%20and%20real-world%20datasets.%20Our%20results%20show%20that%0Anot%20only%20does%20TIC%20accurately%20learn%20the%20covariance%2C%20it%20additionally%20facilitates%0Aan%20improved%20convergence%20of%20the%20negative%20log-likelihood.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/vita-epfl/TIC-TAC%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18953v2&entry.124074799=Read"},
{"title": "Bayesian Design Principles for Offline-to-Online Reinforcement Learning", "author": "Hao Hu and Yiqin Yang and Jianing Ye and Chengjie Wu and Ziqing Mai and Yujing Hu and Tangjie Lv and Changjie Fan and Qianchuan Zhao and Chongjie Zhang", "abstract": "  Offline reinforcement learning (RL) is crucial for real-world applications\nwhere exploration can be costly or unsafe. However, offline learned policies\nare often suboptimal, and further online fine-tuning is required. In this\npaper, we tackle the fundamental dilemma of offline-to-online fine-tuning: if\nthe agent remains pessimistic, it may fail to learn a better policy, while if\nit becomes optimistic directly, performance may suffer from a sudden drop. We\nshow that Bayesian design principles are crucial in solving such a dilemma.\nInstead of adopting optimistic or pessimistic policies, the agent should act in\na way that matches its belief in optimal policies.\n  Such a probability-matching agent can avoid a sudden performance drop while\nstill being guaranteed to find the optimal policy. Based on our theoretical\nfindings, we introduce a novel algorithm that outperforms existing methods on\nvarious benchmarks, demonstrating the efficacy of our approach. Overall, the\nproposed approach provides a new perspective on offline-to-online RL that has\nthe potential to enable more effective learning from offline data.\n", "link": "http://arxiv.org/abs/2405.20984v1", "date": "2024-05-31", "relevancy": 1.9139, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5375}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4903}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Design%20Principles%20for%20Offline-to-Online%20Reinforcement%20Learning&body=Title%3A%20Bayesian%20Design%20Principles%20for%20Offline-to-Online%20Reinforcement%20Learning%0AAuthor%3A%20Hao%20Hu%20and%20Yiqin%20Yang%20and%20Jianing%20Ye%20and%20Chengjie%20Wu%20and%20Ziqing%20Mai%20and%20Yujing%20Hu%20and%20Tangjie%20Lv%20and%20Changjie%20Fan%20and%20Qianchuan%20Zhao%20and%20Chongjie%20Zhang%0AAbstract%3A%20%20%20Offline%20reinforcement%20learning%20%28RL%29%20is%20crucial%20for%20real-world%20applications%0Awhere%20exploration%20can%20be%20costly%20or%20unsafe.%20However%2C%20offline%20learned%20policies%0Aare%20often%20suboptimal%2C%20and%20further%20online%20fine-tuning%20is%20required.%20In%20this%0Apaper%2C%20we%20tackle%20the%20fundamental%20dilemma%20of%20offline-to-online%20fine-tuning%3A%20if%0Athe%20agent%20remains%20pessimistic%2C%20it%20may%20fail%20to%20learn%20a%20better%20policy%2C%20while%20if%0Ait%20becomes%20optimistic%20directly%2C%20performance%20may%20suffer%20from%20a%20sudden%20drop.%20We%0Ashow%20that%20Bayesian%20design%20principles%20are%20crucial%20in%20solving%20such%20a%20dilemma.%0AInstead%20of%20adopting%20optimistic%20or%20pessimistic%20policies%2C%20the%20agent%20should%20act%20in%0Aa%20way%20that%20matches%20its%20belief%20in%20optimal%20policies.%0A%20%20Such%20a%20probability-matching%20agent%20can%20avoid%20a%20sudden%20performance%20drop%20while%0Astill%20being%20guaranteed%20to%20find%20the%20optimal%20policy.%20Based%20on%20our%20theoretical%0Afindings%2C%20we%20introduce%20a%20novel%20algorithm%20that%20outperforms%20existing%20methods%20on%0Avarious%20benchmarks%2C%20demonstrating%20the%20efficacy%20of%20our%20approach.%20Overall%2C%20the%0Aproposed%20approach%20provides%20a%20new%20perspective%20on%20offline-to-online%20RL%20that%20has%0Athe%20potential%20to%20enable%20more%20effective%20learning%20from%20offline%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Design%2520Principles%2520for%2520Offline-to-Online%2520Reinforcement%2520Learning%26entry.906535625%3DHao%2520Hu%2520and%2520Yiqin%2520Yang%2520and%2520Jianing%2520Ye%2520and%2520Chengjie%2520Wu%2520and%2520Ziqing%2520Mai%2520and%2520Yujing%2520Hu%2520and%2520Tangjie%2520Lv%2520and%2520Changjie%2520Fan%2520and%2520Qianchuan%2520Zhao%2520and%2520Chongjie%2520Zhang%26entry.1292438233%3D%2520%2520Offline%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520crucial%2520for%2520real-world%2520applications%250Awhere%2520exploration%2520can%2520be%2520costly%2520or%2520unsafe.%2520However%252C%2520offline%2520learned%2520policies%250Aare%2520often%2520suboptimal%252C%2520and%2520further%2520online%2520fine-tuning%2520is%2520required.%2520In%2520this%250Apaper%252C%2520we%2520tackle%2520the%2520fundamental%2520dilemma%2520of%2520offline-to-online%2520fine-tuning%253A%2520if%250Athe%2520agent%2520remains%2520pessimistic%252C%2520it%2520may%2520fail%2520to%2520learn%2520a%2520better%2520policy%252C%2520while%2520if%250Ait%2520becomes%2520optimistic%2520directly%252C%2520performance%2520may%2520suffer%2520from%2520a%2520sudden%2520drop.%2520We%250Ashow%2520that%2520Bayesian%2520design%2520principles%2520are%2520crucial%2520in%2520solving%2520such%2520a%2520dilemma.%250AInstead%2520of%2520adopting%2520optimistic%2520or%2520pessimistic%2520policies%252C%2520the%2520agent%2520should%2520act%2520in%250Aa%2520way%2520that%2520matches%2520its%2520belief%2520in%2520optimal%2520policies.%250A%2520%2520Such%2520a%2520probability-matching%2520agent%2520can%2520avoid%2520a%2520sudden%2520performance%2520drop%2520while%250Astill%2520being%2520guaranteed%2520to%2520find%2520the%2520optimal%2520policy.%2520Based%2520on%2520our%2520theoretical%250Afindings%252C%2520we%2520introduce%2520a%2520novel%2520algorithm%2520that%2520outperforms%2520existing%2520methods%2520on%250Avarious%2520benchmarks%252C%2520demonstrating%2520the%2520efficacy%2520of%2520our%2520approach.%2520Overall%252C%2520the%250Aproposed%2520approach%2520provides%2520a%2520new%2520perspective%2520on%2520offline-to-online%2520RL%2520that%2520has%250Athe%2520potential%2520to%2520enable%2520more%2520effective%2520learning%2520from%2520offline%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Design%20Principles%20for%20Offline-to-Online%20Reinforcement%20Learning&entry.906535625=Hao%20Hu%20and%20Yiqin%20Yang%20and%20Jianing%20Ye%20and%20Chengjie%20Wu%20and%20Ziqing%20Mai%20and%20Yujing%20Hu%20and%20Tangjie%20Lv%20and%20Changjie%20Fan%20and%20Qianchuan%20Zhao%20and%20Chongjie%20Zhang&entry.1292438233=%20%20Offline%20reinforcement%20learning%20%28RL%29%20is%20crucial%20for%20real-world%20applications%0Awhere%20exploration%20can%20be%20costly%20or%20unsafe.%20However%2C%20offline%20learned%20policies%0Aare%20often%20suboptimal%2C%20and%20further%20online%20fine-tuning%20is%20required.%20In%20this%0Apaper%2C%20we%20tackle%20the%20fundamental%20dilemma%20of%20offline-to-online%20fine-tuning%3A%20if%0Athe%20agent%20remains%20pessimistic%2C%20it%20may%20fail%20to%20learn%20a%20better%20policy%2C%20while%20if%0Ait%20becomes%20optimistic%20directly%2C%20performance%20may%20suffer%20from%20a%20sudden%20drop.%20We%0Ashow%20that%20Bayesian%20design%20principles%20are%20crucial%20in%20solving%20such%20a%20dilemma.%0AInstead%20of%20adopting%20optimistic%20or%20pessimistic%20policies%2C%20the%20agent%20should%20act%20in%0Aa%20way%20that%20matches%20its%20belief%20in%20optimal%20policies.%0A%20%20Such%20a%20probability-matching%20agent%20can%20avoid%20a%20sudden%20performance%20drop%20while%0Astill%20being%20guaranteed%20to%20find%20the%20optimal%20policy.%20Based%20on%20our%20theoretical%0Afindings%2C%20we%20introduce%20a%20novel%20algorithm%20that%20outperforms%20existing%20methods%20on%0Avarious%20benchmarks%2C%20demonstrating%20the%20efficacy%20of%20our%20approach.%20Overall%2C%20the%0Aproposed%20approach%20provides%20a%20new%20perspective%20on%20offline-to-online%20RL%20that%20has%0Athe%20potential%20to%20enable%20more%20effective%20learning%20from%20offline%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20984v1&entry.124074799=Read"},
{"title": "Graph External Attention Enhanced Transformer", "author": "Jianqing Liang and Min Chen and Jiye Liang", "abstract": "  The Transformer architecture has recently gained considerable attention in\nthe field of graph representation learning, as it naturally overcomes several\nlimitations of Graph Neural Networks (GNNs) with customized attention\nmechanisms or positional and structural encodings. Despite making some\nprogress, existing works tend to overlook external information of graphs,\nspecifically the correlation between graphs. Intuitively, graphs with similar\nstructures should have similar representations. Therefore, we propose Graph\nExternal Attention (GEA) -- a novel attention mechanism that leverages multiple\nexternal node/edge key-value units to capture inter-graph correlations\nimplicitly. On this basis, we design an effective architecture called Graph\nExternal Attention Enhanced Transformer (GEAET), which integrates local\nstructure and global interaction information for more comprehensive graph\nrepresentations. Extensive experiments on benchmark datasets demonstrate that\nGEAET achieves state-of-the-art empirical performance. The source code is\navailable for reproducibility at: https://github.com/icm1018/GEAET.\n", "link": "http://arxiv.org/abs/2405.21061v1", "date": "2024-05-31", "relevancy": 1.9129, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5126}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4717}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20External%20Attention%20Enhanced%20Transformer&body=Title%3A%20Graph%20External%20Attention%20Enhanced%20Transformer%0AAuthor%3A%20Jianqing%20Liang%20and%20Min%20Chen%20and%20Jiye%20Liang%0AAbstract%3A%20%20%20The%20Transformer%20architecture%20has%20recently%20gained%20considerable%20attention%20in%0Athe%20field%20of%20graph%20representation%20learning%2C%20as%20it%20naturally%20overcomes%20several%0Alimitations%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20with%20customized%20attention%0Amechanisms%20or%20positional%20and%20structural%20encodings.%20Despite%20making%20some%0Aprogress%2C%20existing%20works%20tend%20to%20overlook%20external%20information%20of%20graphs%2C%0Aspecifically%20the%20correlation%20between%20graphs.%20Intuitively%2C%20graphs%20with%20similar%0Astructures%20should%20have%20similar%20representations.%20Therefore%2C%20we%20propose%20Graph%0AExternal%20Attention%20%28GEA%29%20--%20a%20novel%20attention%20mechanism%20that%20leverages%20multiple%0Aexternal%20node/edge%20key-value%20units%20to%20capture%20inter-graph%20correlations%0Aimplicitly.%20On%20this%20basis%2C%20we%20design%20an%20effective%20architecture%20called%20Graph%0AExternal%20Attention%20Enhanced%20Transformer%20%28GEAET%29%2C%20which%20integrates%20local%0Astructure%20and%20global%20interaction%20information%20for%20more%20comprehensive%20graph%0Arepresentations.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%0AGEAET%20achieves%20state-of-the-art%20empirical%20performance.%20The%20source%20code%20is%0Aavailable%20for%20reproducibility%20at%3A%20https%3A//github.com/icm1018/GEAET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520External%2520Attention%2520Enhanced%2520Transformer%26entry.906535625%3DJianqing%2520Liang%2520and%2520Min%2520Chen%2520and%2520Jiye%2520Liang%26entry.1292438233%3D%2520%2520The%2520Transformer%2520architecture%2520has%2520recently%2520gained%2520considerable%2520attention%2520in%250Athe%2520field%2520of%2520graph%2520representation%2520learning%252C%2520as%2520it%2520naturally%2520overcomes%2520several%250Alimitations%2520of%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520with%2520customized%2520attention%250Amechanisms%2520or%2520positional%2520and%2520structural%2520encodings.%2520Despite%2520making%2520some%250Aprogress%252C%2520existing%2520works%2520tend%2520to%2520overlook%2520external%2520information%2520of%2520graphs%252C%250Aspecifically%2520the%2520correlation%2520between%2520graphs.%2520Intuitively%252C%2520graphs%2520with%2520similar%250Astructures%2520should%2520have%2520similar%2520representations.%2520Therefore%252C%2520we%2520propose%2520Graph%250AExternal%2520Attention%2520%2528GEA%2529%2520--%2520a%2520novel%2520attention%2520mechanism%2520that%2520leverages%2520multiple%250Aexternal%2520node/edge%2520key-value%2520units%2520to%2520capture%2520inter-graph%2520correlations%250Aimplicitly.%2520On%2520this%2520basis%252C%2520we%2520design%2520an%2520effective%2520architecture%2520called%2520Graph%250AExternal%2520Attention%2520Enhanced%2520Transformer%2520%2528GEAET%2529%252C%2520which%2520integrates%2520local%250Astructure%2520and%2520global%2520interaction%2520information%2520for%2520more%2520comprehensive%2520graph%250Arepresentations.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%250AGEAET%2520achieves%2520state-of-the-art%2520empirical%2520performance.%2520The%2520source%2520code%2520is%250Aavailable%2520for%2520reproducibility%2520at%253A%2520https%253A//github.com/icm1018/GEAET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20External%20Attention%20Enhanced%20Transformer&entry.906535625=Jianqing%20Liang%20and%20Min%20Chen%20and%20Jiye%20Liang&entry.1292438233=%20%20The%20Transformer%20architecture%20has%20recently%20gained%20considerable%20attention%20in%0Athe%20field%20of%20graph%20representation%20learning%2C%20as%20it%20naturally%20overcomes%20several%0Alimitations%20of%20Graph%20Neural%20Networks%20%28GNNs%29%20with%20customized%20attention%0Amechanisms%20or%20positional%20and%20structural%20encodings.%20Despite%20making%20some%0Aprogress%2C%20existing%20works%20tend%20to%20overlook%20external%20information%20of%20graphs%2C%0Aspecifically%20the%20correlation%20between%20graphs.%20Intuitively%2C%20graphs%20with%20similar%0Astructures%20should%20have%20similar%20representations.%20Therefore%2C%20we%20propose%20Graph%0AExternal%20Attention%20%28GEA%29%20--%20a%20novel%20attention%20mechanism%20that%20leverages%20multiple%0Aexternal%20node/edge%20key-value%20units%20to%20capture%20inter-graph%20correlations%0Aimplicitly.%20On%20this%20basis%2C%20we%20design%20an%20effective%20architecture%20called%20Graph%0AExternal%20Attention%20Enhanced%20Transformer%20%28GEAET%29%2C%20which%20integrates%20local%0Astructure%20and%20global%20interaction%20information%20for%20more%20comprehensive%20graph%0Arepresentations.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%0AGEAET%20achieves%20state-of-the-art%20empirical%20performance.%20The%20source%20code%20is%0Aavailable%20for%20reproducibility%20at%3A%20https%3A//github.com/icm1018/GEAET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21061v1&entry.124074799=Read"},
{"title": "Unified Directly Denoising for Both Variance Preserving and Variance\n  Exploding Diffusion Models", "author": "Jingjing Wang and Dan Zhang and Feng Luo", "abstract": "  Previous work has demonstrated that, in the Variance Preserving (VP)\nscenario, the nascent Directly Denoising Diffusion Models (DDDM) can generate\nhigh-quality images in one step while achieving even better performance in\nmultistep sampling. However, the Pseudo-LPIPS loss used in DDDM leads to\nconcerns about the bias in assessment. Here, we propose a unified DDDM (uDDDM)\nframework that generates images in one-step/multiple steps for both Variance\nPreserving (VP) and Variance Exploding (VE) cases. We provide theoretical\nproofs of the existence and uniqueness of the model's solution paths, as well\nas the non-intersecting property of the sampling paths. Additionally, we\npropose an adaptive Pseudo-Huber loss function to balance the convergence to\nthe true solution and the stability of convergence process.Through a\ncomprehensive evaluation, we demonstrate that uDDDMs achieve FID scores\ncomparable to the best-performing methods available for CIFAR-10 in both VP and\nVE. Specifically, uDDDM achieves one-step generation on CIFAR10 with FID of\n2.63 and 2.53 for VE and VP respectively. By extending the sampling to 1000\nsteps, we further reduce FID score to 1.71 and 1.65 for VE and VP respectively,\nsetting state-of-the-art performance in both cases.\n", "link": "http://arxiv.org/abs/2405.21059v1", "date": "2024-05-31", "relevancy": 1.9063, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6964}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6231}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Directly%20Denoising%20for%20Both%20Variance%20Preserving%20and%20Variance%0A%20%20Exploding%20Diffusion%20Models&body=Title%3A%20Unified%20Directly%20Denoising%20for%20Both%20Variance%20Preserving%20and%20Variance%0A%20%20Exploding%20Diffusion%20Models%0AAuthor%3A%20Jingjing%20Wang%20and%20Dan%20Zhang%20and%20Feng%20Luo%0AAbstract%3A%20%20%20Previous%20work%20has%20demonstrated%20that%2C%20in%20the%20Variance%20Preserving%20%28VP%29%0Ascenario%2C%20the%20nascent%20Directly%20Denoising%20Diffusion%20Models%20%28DDDM%29%20can%20generate%0Ahigh-quality%20images%20in%20one%20step%20while%20achieving%20even%20better%20performance%20in%0Amultistep%20sampling.%20However%2C%20the%20Pseudo-LPIPS%20loss%20used%20in%20DDDM%20leads%20to%0Aconcerns%20about%20the%20bias%20in%20assessment.%20Here%2C%20we%20propose%20a%20unified%20DDDM%20%28uDDDM%29%0Aframework%20that%20generates%20images%20in%20one-step/multiple%20steps%20for%20both%20Variance%0APreserving%20%28VP%29%20and%20Variance%20Exploding%20%28VE%29%20cases.%20We%20provide%20theoretical%0Aproofs%20of%20the%20existence%20and%20uniqueness%20of%20the%20model%27s%20solution%20paths%2C%20as%20well%0Aas%20the%20non-intersecting%20property%20of%20the%20sampling%20paths.%20Additionally%2C%20we%0Apropose%20an%20adaptive%20Pseudo-Huber%20loss%20function%20to%20balance%20the%20convergence%20to%0Athe%20true%20solution%20and%20the%20stability%20of%20convergence%20process.Through%20a%0Acomprehensive%20evaluation%2C%20we%20demonstrate%20that%20uDDDMs%20achieve%20FID%20scores%0Acomparable%20to%20the%20best-performing%20methods%20available%20for%20CIFAR-10%20in%20both%20VP%20and%0AVE.%20Specifically%2C%20uDDDM%20achieves%20one-step%20generation%20on%20CIFAR10%20with%20FID%20of%0A2.63%20and%202.53%20for%20VE%20and%20VP%20respectively.%20By%20extending%20the%20sampling%20to%201000%0Asteps%2C%20we%20further%20reduce%20FID%20score%20to%201.71%20and%201.65%20for%20VE%20and%20VP%20respectively%2C%0Asetting%20state-of-the-art%20performance%20in%20both%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Directly%2520Denoising%2520for%2520Both%2520Variance%2520Preserving%2520and%2520Variance%250A%2520%2520Exploding%2520Diffusion%2520Models%26entry.906535625%3DJingjing%2520Wang%2520and%2520Dan%2520Zhang%2520and%2520Feng%2520Luo%26entry.1292438233%3D%2520%2520Previous%2520work%2520has%2520demonstrated%2520that%252C%2520in%2520the%2520Variance%2520Preserving%2520%2528VP%2529%250Ascenario%252C%2520the%2520nascent%2520Directly%2520Denoising%2520Diffusion%2520Models%2520%2528DDDM%2529%2520can%2520generate%250Ahigh-quality%2520images%2520in%2520one%2520step%2520while%2520achieving%2520even%2520better%2520performance%2520in%250Amultistep%2520sampling.%2520However%252C%2520the%2520Pseudo-LPIPS%2520loss%2520used%2520in%2520DDDM%2520leads%2520to%250Aconcerns%2520about%2520the%2520bias%2520in%2520assessment.%2520Here%252C%2520we%2520propose%2520a%2520unified%2520DDDM%2520%2528uDDDM%2529%250Aframework%2520that%2520generates%2520images%2520in%2520one-step/multiple%2520steps%2520for%2520both%2520Variance%250APreserving%2520%2528VP%2529%2520and%2520Variance%2520Exploding%2520%2528VE%2529%2520cases.%2520We%2520provide%2520theoretical%250Aproofs%2520of%2520the%2520existence%2520and%2520uniqueness%2520of%2520the%2520model%2527s%2520solution%2520paths%252C%2520as%2520well%250Aas%2520the%2520non-intersecting%2520property%2520of%2520the%2520sampling%2520paths.%2520Additionally%252C%2520we%250Apropose%2520an%2520adaptive%2520Pseudo-Huber%2520loss%2520function%2520to%2520balance%2520the%2520convergence%2520to%250Athe%2520true%2520solution%2520and%2520the%2520stability%2520of%2520convergence%2520process.Through%2520a%250Acomprehensive%2520evaluation%252C%2520we%2520demonstrate%2520that%2520uDDDMs%2520achieve%2520FID%2520scores%250Acomparable%2520to%2520the%2520best-performing%2520methods%2520available%2520for%2520CIFAR-10%2520in%2520both%2520VP%2520and%250AVE.%2520Specifically%252C%2520uDDDM%2520achieves%2520one-step%2520generation%2520on%2520CIFAR10%2520with%2520FID%2520of%250A2.63%2520and%25202.53%2520for%2520VE%2520and%2520VP%2520respectively.%2520By%2520extending%2520the%2520sampling%2520to%25201000%250Asteps%252C%2520we%2520further%2520reduce%2520FID%2520score%2520to%25201.71%2520and%25201.65%2520for%2520VE%2520and%2520VP%2520respectively%252C%250Asetting%2520state-of-the-art%2520performance%2520in%2520both%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Directly%20Denoising%20for%20Both%20Variance%20Preserving%20and%20Variance%0A%20%20Exploding%20Diffusion%20Models&entry.906535625=Jingjing%20Wang%20and%20Dan%20Zhang%20and%20Feng%20Luo&entry.1292438233=%20%20Previous%20work%20has%20demonstrated%20that%2C%20in%20the%20Variance%20Preserving%20%28VP%29%0Ascenario%2C%20the%20nascent%20Directly%20Denoising%20Diffusion%20Models%20%28DDDM%29%20can%20generate%0Ahigh-quality%20images%20in%20one%20step%20while%20achieving%20even%20better%20performance%20in%0Amultistep%20sampling.%20However%2C%20the%20Pseudo-LPIPS%20loss%20used%20in%20DDDM%20leads%20to%0Aconcerns%20about%20the%20bias%20in%20assessment.%20Here%2C%20we%20propose%20a%20unified%20DDDM%20%28uDDDM%29%0Aframework%20that%20generates%20images%20in%20one-step/multiple%20steps%20for%20both%20Variance%0APreserving%20%28VP%29%20and%20Variance%20Exploding%20%28VE%29%20cases.%20We%20provide%20theoretical%0Aproofs%20of%20the%20existence%20and%20uniqueness%20of%20the%20model%27s%20solution%20paths%2C%20as%20well%0Aas%20the%20non-intersecting%20property%20of%20the%20sampling%20paths.%20Additionally%2C%20we%0Apropose%20an%20adaptive%20Pseudo-Huber%20loss%20function%20to%20balance%20the%20convergence%20to%0Athe%20true%20solution%20and%20the%20stability%20of%20convergence%20process.Through%20a%0Acomprehensive%20evaluation%2C%20we%20demonstrate%20that%20uDDDMs%20achieve%20FID%20scores%0Acomparable%20to%20the%20best-performing%20methods%20available%20for%20CIFAR-10%20in%20both%20VP%20and%0AVE.%20Specifically%2C%20uDDDM%20achieves%20one-step%20generation%20on%20CIFAR10%20with%20FID%20of%0A2.63%20and%202.53%20for%20VE%20and%20VP%20respectively.%20By%20extending%20the%20sampling%20to%201000%0Asteps%2C%20we%20further%20reduce%20FID%20score%20to%201.71%20and%201.65%20for%20VE%20and%20VP%20respectively%2C%0Asetting%20state-of-the-art%20performance%20in%20both%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21059v1&entry.124074799=Read"},
{"title": "Grammar-Aligned Decoding", "author": "Kanghee Park and Jiayu Wang and Taylor Berg-Kirkpatrick and Nadia Polikarpova and Loris D'Antoni", "abstract": "  Large Language Models (LLMs) struggle with reliably generating highly\nstructured outputs, such as program code, mathematical formulas, or well-formed\nmarkup. Constrained decoding approaches mitigate this problem by greedily\nrestricting what tokens an LLM can output at each step to guarantee that the\noutput matches a given constraint. Specifically, in grammar-constrained\ndecoding (GCD), the LLM's output must follow a given grammar. In this paper we\ndemonstrate that GCD techniques (and in general constrained decoding\ntechniques) can distort the LLM's distribution, leading to outputs that are\ngrammatical but appear with likelihoods that are not proportional to the ones\ngiven by the LLM, and so ultimately are low-quality. We call the problem of\naligning sampling with a grammar constraint, grammar-aligned decoding (GAD),\nand propose adaptive sampling with approximate expected futures (ASAp), a\ndecoding algorithm that guarantees the output to be grammatical while provably\nproducing outputs that match the conditional probability of the LLM's\ndistribution conditioned on the given grammar constraint. Our algorithm uses\nprior sample outputs to soundly overapproximate the future grammaticality of\ndifferent output prefixes. Our evaluation on code generation and structured NLP\ntasks shows how ASAp often produces outputs with higher likelihood (according\nto the LLM's distribution) than existing GCD techniques, while still enforcing\nthe desired grammatical constraints.\n", "link": "http://arxiv.org/abs/2405.21047v1", "date": "2024-05-31", "relevancy": 1.9056, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.481}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4806}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grammar-Aligned%20Decoding&body=Title%3A%20Grammar-Aligned%20Decoding%0AAuthor%3A%20Kanghee%20Park%20and%20Jiayu%20Wang%20and%20Taylor%20Berg-Kirkpatrick%20and%20Nadia%20Polikarpova%20and%20Loris%20D%27Antoni%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20struggle%20with%20reliably%20generating%20highly%0Astructured%20outputs%2C%20such%20as%20program%20code%2C%20mathematical%20formulas%2C%20or%20well-formed%0Amarkup.%20Constrained%20decoding%20approaches%20mitigate%20this%20problem%20by%20greedily%0Arestricting%20what%20tokens%20an%20LLM%20can%20output%20at%20each%20step%20to%20guarantee%20that%20the%0Aoutput%20matches%20a%20given%20constraint.%20Specifically%2C%20in%20grammar-constrained%0Adecoding%20%28GCD%29%2C%20the%20LLM%27s%20output%20must%20follow%20a%20given%20grammar.%20In%20this%20paper%20we%0Ademonstrate%20that%20GCD%20techniques%20%28and%20in%20general%20constrained%20decoding%0Atechniques%29%20can%20distort%20the%20LLM%27s%20distribution%2C%20leading%20to%20outputs%20that%20are%0Agrammatical%20but%20appear%20with%20likelihoods%20that%20are%20not%20proportional%20to%20the%20ones%0Agiven%20by%20the%20LLM%2C%20and%20so%20ultimately%20are%20low-quality.%20We%20call%20the%20problem%20of%0Aaligning%20sampling%20with%20a%20grammar%20constraint%2C%20grammar-aligned%20decoding%20%28GAD%29%2C%0Aand%20propose%20adaptive%20sampling%20with%20approximate%20expected%20futures%20%28ASAp%29%2C%20a%0Adecoding%20algorithm%20that%20guarantees%20the%20output%20to%20be%20grammatical%20while%20provably%0Aproducing%20outputs%20that%20match%20the%20conditional%20probability%20of%20the%20LLM%27s%0Adistribution%20conditioned%20on%20the%20given%20grammar%20constraint.%20Our%20algorithm%20uses%0Aprior%20sample%20outputs%20to%20soundly%20overapproximate%20the%20future%20grammaticality%20of%0Adifferent%20output%20prefixes.%20Our%20evaluation%20on%20code%20generation%20and%20structured%20NLP%0Atasks%20shows%20how%20ASAp%20often%20produces%20outputs%20with%20higher%20likelihood%20%28according%0Ato%20the%20LLM%27s%20distribution%29%20than%20existing%20GCD%20techniques%2C%20while%20still%20enforcing%0Athe%20desired%20grammatical%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrammar-Aligned%2520Decoding%26entry.906535625%3DKanghee%2520Park%2520and%2520Jiayu%2520Wang%2520and%2520Taylor%2520Berg-Kirkpatrick%2520and%2520Nadia%2520Polikarpova%2520and%2520Loris%2520D%2527Antoni%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520struggle%2520with%2520reliably%2520generating%2520highly%250Astructured%2520outputs%252C%2520such%2520as%2520program%2520code%252C%2520mathematical%2520formulas%252C%2520or%2520well-formed%250Amarkup.%2520Constrained%2520decoding%2520approaches%2520mitigate%2520this%2520problem%2520by%2520greedily%250Arestricting%2520what%2520tokens%2520an%2520LLM%2520can%2520output%2520at%2520each%2520step%2520to%2520guarantee%2520that%2520the%250Aoutput%2520matches%2520a%2520given%2520constraint.%2520Specifically%252C%2520in%2520grammar-constrained%250Adecoding%2520%2528GCD%2529%252C%2520the%2520LLM%2527s%2520output%2520must%2520follow%2520a%2520given%2520grammar.%2520In%2520this%2520paper%2520we%250Ademonstrate%2520that%2520GCD%2520techniques%2520%2528and%2520in%2520general%2520constrained%2520decoding%250Atechniques%2529%2520can%2520distort%2520the%2520LLM%2527s%2520distribution%252C%2520leading%2520to%2520outputs%2520that%2520are%250Agrammatical%2520but%2520appear%2520with%2520likelihoods%2520that%2520are%2520not%2520proportional%2520to%2520the%2520ones%250Agiven%2520by%2520the%2520LLM%252C%2520and%2520so%2520ultimately%2520are%2520low-quality.%2520We%2520call%2520the%2520problem%2520of%250Aaligning%2520sampling%2520with%2520a%2520grammar%2520constraint%252C%2520grammar-aligned%2520decoding%2520%2528GAD%2529%252C%250Aand%2520propose%2520adaptive%2520sampling%2520with%2520approximate%2520expected%2520futures%2520%2528ASAp%2529%252C%2520a%250Adecoding%2520algorithm%2520that%2520guarantees%2520the%2520output%2520to%2520be%2520grammatical%2520while%2520provably%250Aproducing%2520outputs%2520that%2520match%2520the%2520conditional%2520probability%2520of%2520the%2520LLM%2527s%250Adistribution%2520conditioned%2520on%2520the%2520given%2520grammar%2520constraint.%2520Our%2520algorithm%2520uses%250Aprior%2520sample%2520outputs%2520to%2520soundly%2520overapproximate%2520the%2520future%2520grammaticality%2520of%250Adifferent%2520output%2520prefixes.%2520Our%2520evaluation%2520on%2520code%2520generation%2520and%2520structured%2520NLP%250Atasks%2520shows%2520how%2520ASAp%2520often%2520produces%2520outputs%2520with%2520higher%2520likelihood%2520%2528according%250Ato%2520the%2520LLM%2527s%2520distribution%2529%2520than%2520existing%2520GCD%2520techniques%252C%2520while%2520still%2520enforcing%250Athe%2520desired%2520grammatical%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grammar-Aligned%20Decoding&entry.906535625=Kanghee%20Park%20and%20Jiayu%20Wang%20and%20Taylor%20Berg-Kirkpatrick%20and%20Nadia%20Polikarpova%20and%20Loris%20D%27Antoni&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20struggle%20with%20reliably%20generating%20highly%0Astructured%20outputs%2C%20such%20as%20program%20code%2C%20mathematical%20formulas%2C%20or%20well-formed%0Amarkup.%20Constrained%20decoding%20approaches%20mitigate%20this%20problem%20by%20greedily%0Arestricting%20what%20tokens%20an%20LLM%20can%20output%20at%20each%20step%20to%20guarantee%20that%20the%0Aoutput%20matches%20a%20given%20constraint.%20Specifically%2C%20in%20grammar-constrained%0Adecoding%20%28GCD%29%2C%20the%20LLM%27s%20output%20must%20follow%20a%20given%20grammar.%20In%20this%20paper%20we%0Ademonstrate%20that%20GCD%20techniques%20%28and%20in%20general%20constrained%20decoding%0Atechniques%29%20can%20distort%20the%20LLM%27s%20distribution%2C%20leading%20to%20outputs%20that%20are%0Agrammatical%20but%20appear%20with%20likelihoods%20that%20are%20not%20proportional%20to%20the%20ones%0Agiven%20by%20the%20LLM%2C%20and%20so%20ultimately%20are%20low-quality.%20We%20call%20the%20problem%20of%0Aaligning%20sampling%20with%20a%20grammar%20constraint%2C%20grammar-aligned%20decoding%20%28GAD%29%2C%0Aand%20propose%20adaptive%20sampling%20with%20approximate%20expected%20futures%20%28ASAp%29%2C%20a%0Adecoding%20algorithm%20that%20guarantees%20the%20output%20to%20be%20grammatical%20while%20provably%0Aproducing%20outputs%20that%20match%20the%20conditional%20probability%20of%20the%20LLM%27s%0Adistribution%20conditioned%20on%20the%20given%20grammar%20constraint.%20Our%20algorithm%20uses%0Aprior%20sample%20outputs%20to%20soundly%20overapproximate%20the%20future%20grammaticality%20of%0Adifferent%20output%20prefixes.%20Our%20evaluation%20on%20code%20generation%20and%20structured%20NLP%0Atasks%20shows%20how%20ASAp%20often%20produces%20outputs%20with%20higher%20likelihood%20%28according%0Ato%20the%20LLM%27s%20distribution%29%20than%20existing%20GCD%20techniques%2C%20while%20still%20enforcing%0Athe%20desired%20grammatical%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21047v1&entry.124074799=Read"},
{"title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM\n  Unlearning", "author": "Jinghan Jia and Yihua Zhang and Yimeng Zhang and Jiancheng Liu and Bharat Runwal and James Diffenderfer and Bhavya Kailkhura and Sijia Liu", "abstract": "  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility out of the scope of unlearning. While\ninterest in studying LLM unlearning is growing,the impact of the optimizer\nchoice for LLM unlearning remains under-explored. In this work, we shed light\non the significance of optimizer selection in LLM unlearning for the first\ntime, establishing a clear connection between {second-order optimization} and\ninfluence unlearning (a classical approach using influence functions to update\nthe model for data influence removal). This insight propels us to develop a\nsecond-order unlearning framework, termed SOUL, built upon the second-order\nclipped stochastic optimization (Sophia)-based LLM training method. SOUL\nextends the static, one-shot model update using influence unlearning to a\ndynamic, iterative unlearning process. Our extensive experiments show that SOUL\nconsistently outperforms conventional first-order methods across various\nunlearning tasks, models, and metrics, suggesting the promise of second-order\noptimization in providing a scalable and easily implementable solution for LLM\nunlearning.\n", "link": "http://arxiv.org/abs/2404.18239v2", "date": "2024-05-31", "relevancy": 1.8977, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5028}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4742}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SOUL%3A%20Unlocking%20the%20Power%20of%20Second-Order%20Optimization%20for%20LLM%0A%20%20Unlearning&body=Title%3A%20SOUL%3A%20Unlocking%20the%20Power%20of%20Second-Order%20Optimization%20for%20LLM%0A%20%20Unlearning%0AAuthor%3A%20Jinghan%20Jia%20and%20Yihua%20Zhang%20and%20Yimeng%20Zhang%20and%20Jiancheng%20Liu%20and%20Bharat%20Runwal%20and%20James%20Diffenderfer%20and%20Bhavya%20Kailkhura%20and%20Sijia%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20highlighted%20the%20necessity%20of%20effective%0Aunlearning%20mechanisms%20to%20comply%20with%20data%20regulations%20and%20ethical%20AI%20practices.%0ALLM%20unlearning%20aims%20at%20removing%20undesired%20data%20influences%20and%20associated%20model%0Acapabilities%20without%20compromising%20utility%20out%20of%20the%20scope%20of%20unlearning.%20While%0Ainterest%20in%20studying%20LLM%20unlearning%20is%20growing%2Cthe%20impact%20of%20the%20optimizer%0Achoice%20for%20LLM%20unlearning%20remains%20under-explored.%20In%20this%20work%2C%20we%20shed%20light%0Aon%20the%20significance%20of%20optimizer%20selection%20in%20LLM%20unlearning%20for%20the%20first%0Atime%2C%20establishing%20a%20clear%20connection%20between%20%7Bsecond-order%20optimization%7D%20and%0Ainfluence%20unlearning%20%28a%20classical%20approach%20using%20influence%20functions%20to%20update%0Athe%20model%20for%20data%20influence%20removal%29.%20This%20insight%20propels%20us%20to%20develop%20a%0Asecond-order%20unlearning%20framework%2C%20termed%20SOUL%2C%20built%20upon%20the%20second-order%0Aclipped%20stochastic%20optimization%20%28Sophia%29-based%20LLM%20training%20method.%20SOUL%0Aextends%20the%20static%2C%20one-shot%20model%20update%20using%20influence%20unlearning%20to%20a%0Adynamic%2C%20iterative%20unlearning%20process.%20Our%20extensive%20experiments%20show%20that%20SOUL%0Aconsistently%20outperforms%20conventional%20first-order%20methods%20across%20various%0Aunlearning%20tasks%2C%20models%2C%20and%20metrics%2C%20suggesting%20the%20promise%20of%20second-order%0Aoptimization%20in%20providing%20a%20scalable%20and%20easily%20implementable%20solution%20for%20LLM%0Aunlearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSOUL%253A%2520Unlocking%2520the%2520Power%2520of%2520Second-Order%2520Optimization%2520for%2520LLM%250A%2520%2520Unlearning%26entry.906535625%3DJinghan%2520Jia%2520and%2520Yihua%2520Zhang%2520and%2520Yimeng%2520Zhang%2520and%2520Jiancheng%2520Liu%2520and%2520Bharat%2520Runwal%2520and%2520James%2520Diffenderfer%2520and%2520Bhavya%2520Kailkhura%2520and%2520Sijia%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520highlighted%2520the%2520necessity%2520of%2520effective%250Aunlearning%2520mechanisms%2520to%2520comply%2520with%2520data%2520regulations%2520and%2520ethical%2520AI%2520practices.%250ALLM%2520unlearning%2520aims%2520at%2520removing%2520undesired%2520data%2520influences%2520and%2520associated%2520model%250Acapabilities%2520without%2520compromising%2520utility%2520out%2520of%2520the%2520scope%2520of%2520unlearning.%2520While%250Ainterest%2520in%2520studying%2520LLM%2520unlearning%2520is%2520growing%252Cthe%2520impact%2520of%2520the%2520optimizer%250Achoice%2520for%2520LLM%2520unlearning%2520remains%2520under-explored.%2520In%2520this%2520work%252C%2520we%2520shed%2520light%250Aon%2520the%2520significance%2520of%2520optimizer%2520selection%2520in%2520LLM%2520unlearning%2520for%2520the%2520first%250Atime%252C%2520establishing%2520a%2520clear%2520connection%2520between%2520%257Bsecond-order%2520optimization%257D%2520and%250Ainfluence%2520unlearning%2520%2528a%2520classical%2520approach%2520using%2520influence%2520functions%2520to%2520update%250Athe%2520model%2520for%2520data%2520influence%2520removal%2529.%2520This%2520insight%2520propels%2520us%2520to%2520develop%2520a%250Asecond-order%2520unlearning%2520framework%252C%2520termed%2520SOUL%252C%2520built%2520upon%2520the%2520second-order%250Aclipped%2520stochastic%2520optimization%2520%2528Sophia%2529-based%2520LLM%2520training%2520method.%2520SOUL%250Aextends%2520the%2520static%252C%2520one-shot%2520model%2520update%2520using%2520influence%2520unlearning%2520to%2520a%250Adynamic%252C%2520iterative%2520unlearning%2520process.%2520Our%2520extensive%2520experiments%2520show%2520that%2520SOUL%250Aconsistently%2520outperforms%2520conventional%2520first-order%2520methods%2520across%2520various%250Aunlearning%2520tasks%252C%2520models%252C%2520and%2520metrics%252C%2520suggesting%2520the%2520promise%2520of%2520second-order%250Aoptimization%2520in%2520providing%2520a%2520scalable%2520and%2520easily%2520implementable%2520solution%2520for%2520LLM%250Aunlearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOUL%3A%20Unlocking%20the%20Power%20of%20Second-Order%20Optimization%20for%20LLM%0A%20%20Unlearning&entry.906535625=Jinghan%20Jia%20and%20Yihua%20Zhang%20and%20Yimeng%20Zhang%20and%20Jiancheng%20Liu%20and%20Bharat%20Runwal%20and%20James%20Diffenderfer%20and%20Bhavya%20Kailkhura%20and%20Sijia%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20highlighted%20the%20necessity%20of%20effective%0Aunlearning%20mechanisms%20to%20comply%20with%20data%20regulations%20and%20ethical%20AI%20practices.%0ALLM%20unlearning%20aims%20at%20removing%20undesired%20data%20influences%20and%20associated%20model%0Acapabilities%20without%20compromising%20utility%20out%20of%20the%20scope%20of%20unlearning.%20While%0Ainterest%20in%20studying%20LLM%20unlearning%20is%20growing%2Cthe%20impact%20of%20the%20optimizer%0Achoice%20for%20LLM%20unlearning%20remains%20under-explored.%20In%20this%20work%2C%20we%20shed%20light%0Aon%20the%20significance%20of%20optimizer%20selection%20in%20LLM%20unlearning%20for%20the%20first%0Atime%2C%20establishing%20a%20clear%20connection%20between%20%7Bsecond-order%20optimization%7D%20and%0Ainfluence%20unlearning%20%28a%20classical%20approach%20using%20influence%20functions%20to%20update%0Athe%20model%20for%20data%20influence%20removal%29.%20This%20insight%20propels%20us%20to%20develop%20a%0Asecond-order%20unlearning%20framework%2C%20termed%20SOUL%2C%20built%20upon%20the%20second-order%0Aclipped%20stochastic%20optimization%20%28Sophia%29-based%20LLM%20training%20method.%20SOUL%0Aextends%20the%20static%2C%20one-shot%20model%20update%20using%20influence%20unlearning%20to%20a%0Adynamic%2C%20iterative%20unlearning%20process.%20Our%20extensive%20experiments%20show%20that%20SOUL%0Aconsistently%20outperforms%20conventional%20first-order%20methods%20across%20various%0Aunlearning%20tasks%2C%20models%2C%20and%20metrics%2C%20suggesting%20the%20promise%20of%20second-order%0Aoptimization%20in%20providing%20a%20scalable%20and%20easily%20implementable%20solution%20for%20LLM%0Aunlearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18239v2&entry.124074799=Read"},
{"title": "Code Pretraining Improves Entity Tracking Abilities of Language Models", "author": "Najoung Kim and Sebastian Schuster and Shubham Toshniwal", "abstract": "  Recent work has provided indirect evidence that pretraining language models\non code improves the ability of models to track state changes of discourse\nentities expressed in natural language. In this work, we systematically test\nthis claim by comparing pairs of language models on their entity tracking\nperformance. Critically, the pairs consist of base models and models trained on\ntop of these base models with additional code data. We extend this analysis to\nadditionally examine the effect of math training, another highly structured\ndata type, and alignment tuning, an important step for enhancing the usability\nof models. We find clear evidence that models additionally trained on large\namounts of code outperform the base models. On the other hand, we find no\nconsistent benefit of additional math training or alignment tuning across\nvarious model families.\n", "link": "http://arxiv.org/abs/2405.21068v1", "date": "2024-05-31", "relevancy": 1.8763, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4775}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4678}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Code%20Pretraining%20Improves%20Entity%20Tracking%20Abilities%20of%20Language%20Models&body=Title%3A%20Code%20Pretraining%20Improves%20Entity%20Tracking%20Abilities%20of%20Language%20Models%0AAuthor%3A%20Najoung%20Kim%20and%20Sebastian%20Schuster%20and%20Shubham%20Toshniwal%0AAbstract%3A%20%20%20Recent%20work%20has%20provided%20indirect%20evidence%20that%20pretraining%20language%20models%0Aon%20code%20improves%20the%20ability%20of%20models%20to%20track%20state%20changes%20of%20discourse%0Aentities%20expressed%20in%20natural%20language.%20In%20this%20work%2C%20we%20systematically%20test%0Athis%20claim%20by%20comparing%20pairs%20of%20language%20models%20on%20their%20entity%20tracking%0Aperformance.%20Critically%2C%20the%20pairs%20consist%20of%20base%20models%20and%20models%20trained%20on%0Atop%20of%20these%20base%20models%20with%20additional%20code%20data.%20We%20extend%20this%20analysis%20to%0Aadditionally%20examine%20the%20effect%20of%20math%20training%2C%20another%20highly%20structured%0Adata%20type%2C%20and%20alignment%20tuning%2C%20an%20important%20step%20for%20enhancing%20the%20usability%0Aof%20models.%20We%20find%20clear%20evidence%20that%20models%20additionally%20trained%20on%20large%0Aamounts%20of%20code%20outperform%20the%20base%20models.%20On%20the%20other%20hand%2C%20we%20find%20no%0Aconsistent%20benefit%20of%20additional%20math%20training%20or%20alignment%20tuning%20across%0Avarious%20model%20families.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21068v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCode%2520Pretraining%2520Improves%2520Entity%2520Tracking%2520Abilities%2520of%2520Language%2520Models%26entry.906535625%3DNajoung%2520Kim%2520and%2520Sebastian%2520Schuster%2520and%2520Shubham%2520Toshniwal%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520provided%2520indirect%2520evidence%2520that%2520pretraining%2520language%2520models%250Aon%2520code%2520improves%2520the%2520ability%2520of%2520models%2520to%2520track%2520state%2520changes%2520of%2520discourse%250Aentities%2520expressed%2520in%2520natural%2520language.%2520In%2520this%2520work%252C%2520we%2520systematically%2520test%250Athis%2520claim%2520by%2520comparing%2520pairs%2520of%2520language%2520models%2520on%2520their%2520entity%2520tracking%250Aperformance.%2520Critically%252C%2520the%2520pairs%2520consist%2520of%2520base%2520models%2520and%2520models%2520trained%2520on%250Atop%2520of%2520these%2520base%2520models%2520with%2520additional%2520code%2520data.%2520We%2520extend%2520this%2520analysis%2520to%250Aadditionally%2520examine%2520the%2520effect%2520of%2520math%2520training%252C%2520another%2520highly%2520structured%250Adata%2520type%252C%2520and%2520alignment%2520tuning%252C%2520an%2520important%2520step%2520for%2520enhancing%2520the%2520usability%250Aof%2520models.%2520We%2520find%2520clear%2520evidence%2520that%2520models%2520additionally%2520trained%2520on%2520large%250Aamounts%2520of%2520code%2520outperform%2520the%2520base%2520models.%2520On%2520the%2520other%2520hand%252C%2520we%2520find%2520no%250Aconsistent%2520benefit%2520of%2520additional%2520math%2520training%2520or%2520alignment%2520tuning%2520across%250Avarious%2520model%2520families.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21068v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Code%20Pretraining%20Improves%20Entity%20Tracking%20Abilities%20of%20Language%20Models&entry.906535625=Najoung%20Kim%20and%20Sebastian%20Schuster%20and%20Shubham%20Toshniwal&entry.1292438233=%20%20Recent%20work%20has%20provided%20indirect%20evidence%20that%20pretraining%20language%20models%0Aon%20code%20improves%20the%20ability%20of%20models%20to%20track%20state%20changes%20of%20discourse%0Aentities%20expressed%20in%20natural%20language.%20In%20this%20work%2C%20we%20systematically%20test%0Athis%20claim%20by%20comparing%20pairs%20of%20language%20models%20on%20their%20entity%20tracking%0Aperformance.%20Critically%2C%20the%20pairs%20consist%20of%20base%20models%20and%20models%20trained%20on%0Atop%20of%20these%20base%20models%20with%20additional%20code%20data.%20We%20extend%20this%20analysis%20to%0Aadditionally%20examine%20the%20effect%20of%20math%20training%2C%20another%20highly%20structured%0Adata%20type%2C%20and%20alignment%20tuning%2C%20an%20important%20step%20for%20enhancing%20the%20usability%0Aof%20models.%20We%20find%20clear%20evidence%20that%20models%20additionally%20trained%20on%20large%0Aamounts%20of%20code%20outperform%20the%20base%20models.%20On%20the%20other%20hand%2C%20we%20find%20no%0Aconsistent%20benefit%20of%20additional%20math%20training%20or%20alignment%20tuning%20across%0Avarious%20model%20families.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21068v1&entry.124074799=Read"},
{"title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement", "author": "Runsheng Yu and Yong Wang and Xiaoqi Jiao and Youzhi Zhang and James T. Kwok", "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has been commonly used to\nalign the behaviors of Large Language Models (LLMs) with human preferences.\nRecently, a popular alternative is Direct Policy Optimization (DPO), which\nreplaces an LLM-based reward model with the policy itself, thus obviating the\nneed for extra memory and training time to learn the reward model. However, DPO\ndoes not consider the relative qualities of the positive and negative\nresponses, and can lead to sub-optimal training outcomes. To alleviate this\nproblem, we investigate the use of intrinsic knowledge within the on-the-fly\nfine-tuning LLM to obtain relative qualities and help to refine the loss\nfunction. Specifically, we leverage the knowledge of the LLM to design a\nrefinement function to estimate the quality of both the positive and negative\nresponses. We show that the constructed refinement function can help\nself-refine the loss function under mild assumptions. The refinement function\nis integrated into DPO and its variant Identity Policy Optimization (IPO).\nExperiments across various evaluators indicate that they can improve the\nperformance of the fine-tuned models over DPO and IPO.\n", "link": "http://arxiv.org/abs/2405.21040v1", "date": "2024-05-31", "relevancy": 1.864, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4786}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Direct%20Alignment%20of%20Language%20Models%20via%20Quality-Aware%20Self-Refinement&body=Title%3A%20Direct%20Alignment%20of%20Language%20Models%20via%20Quality-Aware%20Self-Refinement%0AAuthor%3A%20Runsheng%20Yu%20and%20Yong%20Wang%20and%20Xiaoqi%20Jiao%20and%20Youzhi%20Zhang%20and%20James%20T.%20Kwok%0AAbstract%3A%20%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20been%20commonly%20used%20to%0Aalign%20the%20behaviors%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20preferences.%0ARecently%2C%20a%20popular%20alternative%20is%20Direct%20Policy%20Optimization%20%28DPO%29%2C%20which%0Areplaces%20an%20LLM-based%20reward%20model%20with%20the%20policy%20itself%2C%20thus%20obviating%20the%0Aneed%20for%20extra%20memory%20and%20training%20time%20to%20learn%20the%20reward%20model.%20However%2C%20DPO%0Adoes%20not%20consider%20the%20relative%20qualities%20of%20the%20positive%20and%20negative%0Aresponses%2C%20and%20can%20lead%20to%20sub-optimal%20training%20outcomes.%20To%20alleviate%20this%0Aproblem%2C%20we%20investigate%20the%20use%20of%20intrinsic%20knowledge%20within%20the%20on-the-fly%0Afine-tuning%20LLM%20to%20obtain%20relative%20qualities%20and%20help%20to%20refine%20the%20loss%0Afunction.%20Specifically%2C%20we%20leverage%20the%20knowledge%20of%20the%20LLM%20to%20design%20a%0Arefinement%20function%20to%20estimate%20the%20quality%20of%20both%20the%20positive%20and%20negative%0Aresponses.%20We%20show%20that%20the%20constructed%20refinement%20function%20can%20help%0Aself-refine%20the%20loss%20function%20under%20mild%20assumptions.%20The%20refinement%20function%0Ais%20integrated%20into%20DPO%20and%20its%20variant%20Identity%20Policy%20Optimization%20%28IPO%29.%0AExperiments%20across%20various%20evaluators%20indicate%20that%20they%20can%20improve%20the%0Aperformance%20of%20the%20fine-tuned%20models%20over%20DPO%20and%20IPO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDirect%2520Alignment%2520of%2520Language%2520Models%2520via%2520Quality-Aware%2520Self-Refinement%26entry.906535625%3DRunsheng%2520Yu%2520and%2520Yong%2520Wang%2520and%2520Xiaoqi%2520Jiao%2520and%2520Youzhi%2520Zhang%2520and%2520James%2520T.%2520Kwok%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520has%2520been%2520commonly%2520used%2520to%250Aalign%2520the%2520behaviors%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520human%2520preferences.%250ARecently%252C%2520a%2520popular%2520alternative%2520is%2520Direct%2520Policy%2520Optimization%2520%2528DPO%2529%252C%2520which%250Areplaces%2520an%2520LLM-based%2520reward%2520model%2520with%2520the%2520policy%2520itself%252C%2520thus%2520obviating%2520the%250Aneed%2520for%2520extra%2520memory%2520and%2520training%2520time%2520to%2520learn%2520the%2520reward%2520model.%2520However%252C%2520DPO%250Adoes%2520not%2520consider%2520the%2520relative%2520qualities%2520of%2520the%2520positive%2520and%2520negative%250Aresponses%252C%2520and%2520can%2520lead%2520to%2520sub-optimal%2520training%2520outcomes.%2520To%2520alleviate%2520this%250Aproblem%252C%2520we%2520investigate%2520the%2520use%2520of%2520intrinsic%2520knowledge%2520within%2520the%2520on-the-fly%250Afine-tuning%2520LLM%2520to%2520obtain%2520relative%2520qualities%2520and%2520help%2520to%2520refine%2520the%2520loss%250Afunction.%2520Specifically%252C%2520we%2520leverage%2520the%2520knowledge%2520of%2520the%2520LLM%2520to%2520design%2520a%250Arefinement%2520function%2520to%2520estimate%2520the%2520quality%2520of%2520both%2520the%2520positive%2520and%2520negative%250Aresponses.%2520We%2520show%2520that%2520the%2520constructed%2520refinement%2520function%2520can%2520help%250Aself-refine%2520the%2520loss%2520function%2520under%2520mild%2520assumptions.%2520The%2520refinement%2520function%250Ais%2520integrated%2520into%2520DPO%2520and%2520its%2520variant%2520Identity%2520Policy%2520Optimization%2520%2528IPO%2529.%250AExperiments%2520across%2520various%2520evaluators%2520indicate%2520that%2520they%2520can%2520improve%2520the%250Aperformance%2520of%2520the%2520fine-tuned%2520models%2520over%2520DPO%2520and%2520IPO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Direct%20Alignment%20of%20Language%20Models%20via%20Quality-Aware%20Self-Refinement&entry.906535625=Runsheng%20Yu%20and%20Yong%20Wang%20and%20Xiaoqi%20Jiao%20and%20Youzhi%20Zhang%20and%20James%20T.%20Kwok&entry.1292438233=%20%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20has%20been%20commonly%20used%20to%0Aalign%20the%20behaviors%20of%20Large%20Language%20Models%20%28LLMs%29%20with%20human%20preferences.%0ARecently%2C%20a%20popular%20alternative%20is%20Direct%20Policy%20Optimization%20%28DPO%29%2C%20which%0Areplaces%20an%20LLM-based%20reward%20model%20with%20the%20policy%20itself%2C%20thus%20obviating%20the%0Aneed%20for%20extra%20memory%20and%20training%20time%20to%20learn%20the%20reward%20model.%20However%2C%20DPO%0Adoes%20not%20consider%20the%20relative%20qualities%20of%20the%20positive%20and%20negative%0Aresponses%2C%20and%20can%20lead%20to%20sub-optimal%20training%20outcomes.%20To%20alleviate%20this%0Aproblem%2C%20we%20investigate%20the%20use%20of%20intrinsic%20knowledge%20within%20the%20on-the-fly%0Afine-tuning%20LLM%20to%20obtain%20relative%20qualities%20and%20help%20to%20refine%20the%20loss%0Afunction.%20Specifically%2C%20we%20leverage%20the%20knowledge%20of%20the%20LLM%20to%20design%20a%0Arefinement%20function%20to%20estimate%20the%20quality%20of%20both%20the%20positive%20and%20negative%0Aresponses.%20We%20show%20that%20the%20constructed%20refinement%20function%20can%20help%0Aself-refine%20the%20loss%20function%20under%20mild%20assumptions.%20The%20refinement%20function%0Ais%20integrated%20into%20DPO%20and%20its%20variant%20Identity%20Policy%20Optimization%20%28IPO%29.%0AExperiments%20across%20various%20evaluators%20indicate%20that%20they%20can%20improve%20the%0Aperformance%20of%20the%20fine-tuned%20models%20over%20DPO%20and%20IPO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21040v1&entry.124074799=Read"},
{"title": "MpoxSLDNet: A Novel CNN Model for Detecting Monkeypox Lesions and\n  Performance Comparison with Pre-trained Models", "author": "Fatema Jannat Dihan and Saydul Akbar Murad and Abu Jafar Md Muzahid and K. M. Aslam Uddin and Mohammed J. F. Alenazi and Anupam Kumar Bairagi and Sujit Biswas", "abstract": "  Monkeypox virus (MPXV) is a zoonotic virus that poses a significant threat to\npublic health, particularly in remote parts of Central and West Africa. Early\ndetection of monkeypox lesions is crucial for effective treatment. However, due\nto its similarity with other skin diseases, monkeypox lesion detection is a\nchallenging task. To detect monkeypox, many researchers used various\ndeep-learning models such as MobileNetv2, VGG16, ResNet50, InceptionV3,\nDenseNet121, EfficientNetB3, MobileNetV2, and Xception. However, these models\noften require high storage space due to their large size. This study aims to\nimprove the existing challenges by introducing a CNN model named MpoxSLDNet\n(Monkeypox Skin Lesion Detector Network) to facilitate early detection and\ncategorization of Monkeypox lesions and Non-Monkeypox lesions in digital\nimages. Our model represents a significant advancement in the field of\nmonkeypox lesion detection by offering superior performance metrics, including\nprecision, recall, F1-score, accuracy, and AUC, compared to traditional\npre-trained models such as VGG16, ResNet50, and DenseNet121. The key novelty of\nour approach lies in MpoxSLDNet's ability to achieve high detection accuracy\nwhile requiring significantly less storage space than existing models. By\naddressing the challenge of high storage requirements, MpoxSLDNet presents a\npractical solution for early detection and categorization of monkeypox lesions\nin resource-constrained healthcare settings. In this study, we have used\n\"Monkeypox Skin Lesion Dataset\" comprising 1428 skin images of monkeypox\nlesions and 1764 skin images of Non-Monkeypox lesions. Dataset's limitations\ncould potentially impact the model's ability to generalize to unseen cases.\nHowever, the MpoxSLDNet model achieved a validation accuracy of 94.56%,\ncompared to 86.25%, 84.38%, and 67.19% for VGG16, DenseNet121, and ResNet50,\nrespectively.\n", "link": "http://arxiv.org/abs/2405.21016v1", "date": "2024-05-31", "relevancy": 1.8542, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5148}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4535}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MpoxSLDNet%3A%20A%20Novel%20CNN%20Model%20for%20Detecting%20Monkeypox%20Lesions%20and%0A%20%20Performance%20Comparison%20with%20Pre-trained%20Models&body=Title%3A%20MpoxSLDNet%3A%20A%20Novel%20CNN%20Model%20for%20Detecting%20Monkeypox%20Lesions%20and%0A%20%20Performance%20Comparison%20with%20Pre-trained%20Models%0AAuthor%3A%20Fatema%20Jannat%20Dihan%20and%20Saydul%20Akbar%20Murad%20and%20Abu%20Jafar%20Md%20Muzahid%20and%20K.%20M.%20Aslam%20Uddin%20and%20Mohammed%20J.%20F.%20Alenazi%20and%20Anupam%20Kumar%20Bairagi%20and%20Sujit%20Biswas%0AAbstract%3A%20%20%20Monkeypox%20virus%20%28MPXV%29%20is%20a%20zoonotic%20virus%20that%20poses%20a%20significant%20threat%20to%0Apublic%20health%2C%20particularly%20in%20remote%20parts%20of%20Central%20and%20West%20Africa.%20Early%0Adetection%20of%20monkeypox%20lesions%20is%20crucial%20for%20effective%20treatment.%20However%2C%20due%0Ato%20its%20similarity%20with%20other%20skin%20diseases%2C%20monkeypox%20lesion%20detection%20is%20a%0Achallenging%20task.%20To%20detect%20monkeypox%2C%20many%20researchers%20used%20various%0Adeep-learning%20models%20such%20as%20MobileNetv2%2C%20VGG16%2C%20ResNet50%2C%20InceptionV3%2C%0ADenseNet121%2C%20EfficientNetB3%2C%20MobileNetV2%2C%20and%20Xception.%20However%2C%20these%20models%0Aoften%20require%20high%20storage%20space%20due%20to%20their%20large%20size.%20This%20study%20aims%20to%0Aimprove%20the%20existing%20challenges%20by%20introducing%20a%20CNN%20model%20named%20MpoxSLDNet%0A%28Monkeypox%20Skin%20Lesion%20Detector%20Network%29%20to%20facilitate%20early%20detection%20and%0Acategorization%20of%20Monkeypox%20lesions%20and%20Non-Monkeypox%20lesions%20in%20digital%0Aimages.%20Our%20model%20represents%20a%20significant%20advancement%20in%20the%20field%20of%0Amonkeypox%20lesion%20detection%20by%20offering%20superior%20performance%20metrics%2C%20including%0Aprecision%2C%20recall%2C%20F1-score%2C%20accuracy%2C%20and%20AUC%2C%20compared%20to%20traditional%0Apre-trained%20models%20such%20as%20VGG16%2C%20ResNet50%2C%20and%20DenseNet121.%20The%20key%20novelty%20of%0Aour%20approach%20lies%20in%20MpoxSLDNet%27s%20ability%20to%20achieve%20high%20detection%20accuracy%0Awhile%20requiring%20significantly%20less%20storage%20space%20than%20existing%20models.%20By%0Aaddressing%20the%20challenge%20of%20high%20storage%20requirements%2C%20MpoxSLDNet%20presents%20a%0Apractical%20solution%20for%20early%20detection%20and%20categorization%20of%20monkeypox%20lesions%0Ain%20resource-constrained%20healthcare%20settings.%20In%20this%20study%2C%20we%20have%20used%0A%22Monkeypox%20Skin%20Lesion%20Dataset%22%20comprising%201428%20skin%20images%20of%20monkeypox%0Alesions%20and%201764%20skin%20images%20of%20Non-Monkeypox%20lesions.%20Dataset%27s%20limitations%0Acould%20potentially%20impact%20the%20model%27s%20ability%20to%20generalize%20to%20unseen%20cases.%0AHowever%2C%20the%20MpoxSLDNet%20model%20achieved%20a%20validation%20accuracy%20of%2094.56%25%2C%0Acompared%20to%2086.25%25%2C%2084.38%25%2C%20and%2067.19%25%20for%20VGG16%2C%20DenseNet121%2C%20and%20ResNet50%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMpoxSLDNet%253A%2520A%2520Novel%2520CNN%2520Model%2520for%2520Detecting%2520Monkeypox%2520Lesions%2520and%250A%2520%2520Performance%2520Comparison%2520with%2520Pre-trained%2520Models%26entry.906535625%3DFatema%2520Jannat%2520Dihan%2520and%2520Saydul%2520Akbar%2520Murad%2520and%2520Abu%2520Jafar%2520Md%2520Muzahid%2520and%2520K.%2520M.%2520Aslam%2520Uddin%2520and%2520Mohammed%2520J.%2520F.%2520Alenazi%2520and%2520Anupam%2520Kumar%2520Bairagi%2520and%2520Sujit%2520Biswas%26entry.1292438233%3D%2520%2520Monkeypox%2520virus%2520%2528MPXV%2529%2520is%2520a%2520zoonotic%2520virus%2520that%2520poses%2520a%2520significant%2520threat%2520to%250Apublic%2520health%252C%2520particularly%2520in%2520remote%2520parts%2520of%2520Central%2520and%2520West%2520Africa.%2520Early%250Adetection%2520of%2520monkeypox%2520lesions%2520is%2520crucial%2520for%2520effective%2520treatment.%2520However%252C%2520due%250Ato%2520its%2520similarity%2520with%2520other%2520skin%2520diseases%252C%2520monkeypox%2520lesion%2520detection%2520is%2520a%250Achallenging%2520task.%2520To%2520detect%2520monkeypox%252C%2520many%2520researchers%2520used%2520various%250Adeep-learning%2520models%2520such%2520as%2520MobileNetv2%252C%2520VGG16%252C%2520ResNet50%252C%2520InceptionV3%252C%250ADenseNet121%252C%2520EfficientNetB3%252C%2520MobileNetV2%252C%2520and%2520Xception.%2520However%252C%2520these%2520models%250Aoften%2520require%2520high%2520storage%2520space%2520due%2520to%2520their%2520large%2520size.%2520This%2520study%2520aims%2520to%250Aimprove%2520the%2520existing%2520challenges%2520by%2520introducing%2520a%2520CNN%2520model%2520named%2520MpoxSLDNet%250A%2528Monkeypox%2520Skin%2520Lesion%2520Detector%2520Network%2529%2520to%2520facilitate%2520early%2520detection%2520and%250Acategorization%2520of%2520Monkeypox%2520lesions%2520and%2520Non-Monkeypox%2520lesions%2520in%2520digital%250Aimages.%2520Our%2520model%2520represents%2520a%2520significant%2520advancement%2520in%2520the%2520field%2520of%250Amonkeypox%2520lesion%2520detection%2520by%2520offering%2520superior%2520performance%2520metrics%252C%2520including%250Aprecision%252C%2520recall%252C%2520F1-score%252C%2520accuracy%252C%2520and%2520AUC%252C%2520compared%2520to%2520traditional%250Apre-trained%2520models%2520such%2520as%2520VGG16%252C%2520ResNet50%252C%2520and%2520DenseNet121.%2520The%2520key%2520novelty%2520of%250Aour%2520approach%2520lies%2520in%2520MpoxSLDNet%2527s%2520ability%2520to%2520achieve%2520high%2520detection%2520accuracy%250Awhile%2520requiring%2520significantly%2520less%2520storage%2520space%2520than%2520existing%2520models.%2520By%250Aaddressing%2520the%2520challenge%2520of%2520high%2520storage%2520requirements%252C%2520MpoxSLDNet%2520presents%2520a%250Apractical%2520solution%2520for%2520early%2520detection%2520and%2520categorization%2520of%2520monkeypox%2520lesions%250Ain%2520resource-constrained%2520healthcare%2520settings.%2520In%2520this%2520study%252C%2520we%2520have%2520used%250A%2522Monkeypox%2520Skin%2520Lesion%2520Dataset%2522%2520comprising%25201428%2520skin%2520images%2520of%2520monkeypox%250Alesions%2520and%25201764%2520skin%2520images%2520of%2520Non-Monkeypox%2520lesions.%2520Dataset%2527s%2520limitations%250Acould%2520potentially%2520impact%2520the%2520model%2527s%2520ability%2520to%2520generalize%2520to%2520unseen%2520cases.%250AHowever%252C%2520the%2520MpoxSLDNet%2520model%2520achieved%2520a%2520validation%2520accuracy%2520of%252094.56%2525%252C%250Acompared%2520to%252086.25%2525%252C%252084.38%2525%252C%2520and%252067.19%2525%2520for%2520VGG16%252C%2520DenseNet121%252C%2520and%2520ResNet50%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MpoxSLDNet%3A%20A%20Novel%20CNN%20Model%20for%20Detecting%20Monkeypox%20Lesions%20and%0A%20%20Performance%20Comparison%20with%20Pre-trained%20Models&entry.906535625=Fatema%20Jannat%20Dihan%20and%20Saydul%20Akbar%20Murad%20and%20Abu%20Jafar%20Md%20Muzahid%20and%20K.%20M.%20Aslam%20Uddin%20and%20Mohammed%20J.%20F.%20Alenazi%20and%20Anupam%20Kumar%20Bairagi%20and%20Sujit%20Biswas&entry.1292438233=%20%20Monkeypox%20virus%20%28MPXV%29%20is%20a%20zoonotic%20virus%20that%20poses%20a%20significant%20threat%20to%0Apublic%20health%2C%20particularly%20in%20remote%20parts%20of%20Central%20and%20West%20Africa.%20Early%0Adetection%20of%20monkeypox%20lesions%20is%20crucial%20for%20effective%20treatment.%20However%2C%20due%0Ato%20its%20similarity%20with%20other%20skin%20diseases%2C%20monkeypox%20lesion%20detection%20is%20a%0Achallenging%20task.%20To%20detect%20monkeypox%2C%20many%20researchers%20used%20various%0Adeep-learning%20models%20such%20as%20MobileNetv2%2C%20VGG16%2C%20ResNet50%2C%20InceptionV3%2C%0ADenseNet121%2C%20EfficientNetB3%2C%20MobileNetV2%2C%20and%20Xception.%20However%2C%20these%20models%0Aoften%20require%20high%20storage%20space%20due%20to%20their%20large%20size.%20This%20study%20aims%20to%0Aimprove%20the%20existing%20challenges%20by%20introducing%20a%20CNN%20model%20named%20MpoxSLDNet%0A%28Monkeypox%20Skin%20Lesion%20Detector%20Network%29%20to%20facilitate%20early%20detection%20and%0Acategorization%20of%20Monkeypox%20lesions%20and%20Non-Monkeypox%20lesions%20in%20digital%0Aimages.%20Our%20model%20represents%20a%20significant%20advancement%20in%20the%20field%20of%0Amonkeypox%20lesion%20detection%20by%20offering%20superior%20performance%20metrics%2C%20including%0Aprecision%2C%20recall%2C%20F1-score%2C%20accuracy%2C%20and%20AUC%2C%20compared%20to%20traditional%0Apre-trained%20models%20such%20as%20VGG16%2C%20ResNet50%2C%20and%20DenseNet121.%20The%20key%20novelty%20of%0Aour%20approach%20lies%20in%20MpoxSLDNet%27s%20ability%20to%20achieve%20high%20detection%20accuracy%0Awhile%20requiring%20significantly%20less%20storage%20space%20than%20existing%20models.%20By%0Aaddressing%20the%20challenge%20of%20high%20storage%20requirements%2C%20MpoxSLDNet%20presents%20a%0Apractical%20solution%20for%20early%20detection%20and%20categorization%20of%20monkeypox%20lesions%0Ain%20resource-constrained%20healthcare%20settings.%20In%20this%20study%2C%20we%20have%20used%0A%22Monkeypox%20Skin%20Lesion%20Dataset%22%20comprising%201428%20skin%20images%20of%20monkeypox%0Alesions%20and%201764%20skin%20images%20of%20Non-Monkeypox%20lesions.%20Dataset%27s%20limitations%0Acould%20potentially%20impact%20the%20model%27s%20ability%20to%20generalize%20to%20unseen%20cases.%0AHowever%2C%20the%20MpoxSLDNet%20model%20achieved%20a%20validation%20accuracy%20of%2094.56%25%2C%0Acompared%20to%2086.25%25%2C%2084.38%25%2C%20and%2067.19%25%20for%20VGG16%2C%20DenseNet121%2C%20and%20ResNet50%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21016v1&entry.124074799=Read"},
{"title": "Improved Techniques for Optimization-Based Jailbreaking on Large\n  Language Models", "author": "Xiaojun Jia and Tianyu Pang and Chao Du and Yihao Huang and Jindong Gu and Yang Liu and Xiaochun Cao and Min Lin", "abstract": "  Large language models (LLMs) are being rapidly developed, and a key component\nof their widespread deployment is their safety-related alignment. Many\nred-teaming efforts aim to jailbreak LLMs, where among these efforts, the\nGreedy Coordinate Gradient (GCG) attack's success has led to a growing interest\nin the study of optimization-based jailbreaking techniques. Although GCG is a\nsignificant milestone, its attacking efficiency remains unsatisfactory. In this\npaper, we present several improved (empirical) techniques for\noptimization-based jailbreaks like GCG. We first observe that the single target\ntemplate of \"Sure\" largely limits the attacking performance of GCG; given this,\nwe propose to apply diverse target templates containing harmful self-suggestion\nand/or guidance to mislead LLMs. Besides, from the optimization aspects, we\npropose an automatic multi-coordinate updating strategy in GCG (i.e.,\nadaptively deciding how many tokens to replace in each step) to accelerate\nconvergence, as well as tricks like easy-to-hard initialisation. Then, we\ncombine these improved technologies to develop an efficient jailbreak method,\ndubbed $\\mathcal{I}$-GCG. In our experiments, we evaluate on a series of\nbenchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate\nthat our improved techniques can help GCG outperform state-of-the-art\njailbreaking attacks and achieve nearly 100% attack success rate. The code is\nreleased at https://github.com/jiaxiaojunQAQ/I-GCG.\n", "link": "http://arxiv.org/abs/2405.21018v1", "date": "2024-05-31", "relevancy": 1.8491, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4835}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4507}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Techniques%20for%20Optimization-Based%20Jailbreaking%20on%20Large%0A%20%20Language%20Models&body=Title%3A%20Improved%20Techniques%20for%20Optimization-Based%20Jailbreaking%20on%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Xiaojun%20Jia%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Yihao%20Huang%20and%20Jindong%20Gu%20and%20Yang%20Liu%20and%20Xiaochun%20Cao%20and%20Min%20Lin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20being%20rapidly%20developed%2C%20and%20a%20key%20component%0Aof%20their%20widespread%20deployment%20is%20their%20safety-related%20alignment.%20Many%0Ared-teaming%20efforts%20aim%20to%20jailbreak%20LLMs%2C%20where%20among%20these%20efforts%2C%20the%0AGreedy%20Coordinate%20Gradient%20%28GCG%29%20attack%27s%20success%20has%20led%20to%20a%20growing%20interest%0Ain%20the%20study%20of%20optimization-based%20jailbreaking%20techniques.%20Although%20GCG%20is%20a%0Asignificant%20milestone%2C%20its%20attacking%20efficiency%20remains%20unsatisfactory.%20In%20this%0Apaper%2C%20we%20present%20several%20improved%20%28empirical%29%20techniques%20for%0Aoptimization-based%20jailbreaks%20like%20GCG.%20We%20first%20observe%20that%20the%20single%20target%0Atemplate%20of%20%22Sure%22%20largely%20limits%20the%20attacking%20performance%20of%20GCG%3B%20given%20this%2C%0Awe%20propose%20to%20apply%20diverse%20target%20templates%20containing%20harmful%20self-suggestion%0Aand/or%20guidance%20to%20mislead%20LLMs.%20Besides%2C%20from%20the%20optimization%20aspects%2C%20we%0Apropose%20an%20automatic%20multi-coordinate%20updating%20strategy%20in%20GCG%20%28i.e.%2C%0Aadaptively%20deciding%20how%20many%20tokens%20to%20replace%20in%20each%20step%29%20to%20accelerate%0Aconvergence%2C%20as%20well%20as%20tricks%20like%20easy-to-hard%20initialisation.%20Then%2C%20we%0Acombine%20these%20improved%20technologies%20to%20develop%20an%20efficient%20jailbreak%20method%2C%0Adubbed%20%24%5Cmathcal%7BI%7D%24-GCG.%20In%20our%20experiments%2C%20we%20evaluate%20on%20a%20series%20of%0Abenchmarks%20%28such%20as%20NeurIPS%202023%20Red%20Teaming%20Track%29.%20The%20results%20demonstrate%0Athat%20our%20improved%20techniques%20can%20help%20GCG%20outperform%20state-of-the-art%0Ajailbreaking%20attacks%20and%20achieve%20nearly%20100%25%20attack%20success%20rate.%20The%20code%20is%0Areleased%20at%20https%3A//github.com/jiaxiaojunQAQ/I-GCG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Techniques%2520for%2520Optimization-Based%2520Jailbreaking%2520on%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DXiaojun%2520Jia%2520and%2520Tianyu%2520Pang%2520and%2520Chao%2520Du%2520and%2520Yihao%2520Huang%2520and%2520Jindong%2520Gu%2520and%2520Yang%2520Liu%2520and%2520Xiaochun%2520Cao%2520and%2520Min%2520Lin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520being%2520rapidly%2520developed%252C%2520and%2520a%2520key%2520component%250Aof%2520their%2520widespread%2520deployment%2520is%2520their%2520safety-related%2520alignment.%2520Many%250Ared-teaming%2520efforts%2520aim%2520to%2520jailbreak%2520LLMs%252C%2520where%2520among%2520these%2520efforts%252C%2520the%250AGreedy%2520Coordinate%2520Gradient%2520%2528GCG%2529%2520attack%2527s%2520success%2520has%2520led%2520to%2520a%2520growing%2520interest%250Ain%2520the%2520study%2520of%2520optimization-based%2520jailbreaking%2520techniques.%2520Although%2520GCG%2520is%2520a%250Asignificant%2520milestone%252C%2520its%2520attacking%2520efficiency%2520remains%2520unsatisfactory.%2520In%2520this%250Apaper%252C%2520we%2520present%2520several%2520improved%2520%2528empirical%2529%2520techniques%2520for%250Aoptimization-based%2520jailbreaks%2520like%2520GCG.%2520We%2520first%2520observe%2520that%2520the%2520single%2520target%250Atemplate%2520of%2520%2522Sure%2522%2520largely%2520limits%2520the%2520attacking%2520performance%2520of%2520GCG%253B%2520given%2520this%252C%250Awe%2520propose%2520to%2520apply%2520diverse%2520target%2520templates%2520containing%2520harmful%2520self-suggestion%250Aand/or%2520guidance%2520to%2520mislead%2520LLMs.%2520Besides%252C%2520from%2520the%2520optimization%2520aspects%252C%2520we%250Apropose%2520an%2520automatic%2520multi-coordinate%2520updating%2520strategy%2520in%2520GCG%2520%2528i.e.%252C%250Aadaptively%2520deciding%2520how%2520many%2520tokens%2520to%2520replace%2520in%2520each%2520step%2529%2520to%2520accelerate%250Aconvergence%252C%2520as%2520well%2520as%2520tricks%2520like%2520easy-to-hard%2520initialisation.%2520Then%252C%2520we%250Acombine%2520these%2520improved%2520technologies%2520to%2520develop%2520an%2520efficient%2520jailbreak%2520method%252C%250Adubbed%2520%2524%255Cmathcal%257BI%257D%2524-GCG.%2520In%2520our%2520experiments%252C%2520we%2520evaluate%2520on%2520a%2520series%2520of%250Abenchmarks%2520%2528such%2520as%2520NeurIPS%25202023%2520Red%2520Teaming%2520Track%2529.%2520The%2520results%2520demonstrate%250Athat%2520our%2520improved%2520techniques%2520can%2520help%2520GCG%2520outperform%2520state-of-the-art%250Ajailbreaking%2520attacks%2520and%2520achieve%2520nearly%2520100%2525%2520attack%2520success%2520rate.%2520The%2520code%2520is%250Areleased%2520at%2520https%253A//github.com/jiaxiaojunQAQ/I-GCG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Techniques%20for%20Optimization-Based%20Jailbreaking%20on%20Large%0A%20%20Language%20Models&entry.906535625=Xiaojun%20Jia%20and%20Tianyu%20Pang%20and%20Chao%20Du%20and%20Yihao%20Huang%20and%20Jindong%20Gu%20and%20Yang%20Liu%20and%20Xiaochun%20Cao%20and%20Min%20Lin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20being%20rapidly%20developed%2C%20and%20a%20key%20component%0Aof%20their%20widespread%20deployment%20is%20their%20safety-related%20alignment.%20Many%0Ared-teaming%20efforts%20aim%20to%20jailbreak%20LLMs%2C%20where%20among%20these%20efforts%2C%20the%0AGreedy%20Coordinate%20Gradient%20%28GCG%29%20attack%27s%20success%20has%20led%20to%20a%20growing%20interest%0Ain%20the%20study%20of%20optimization-based%20jailbreaking%20techniques.%20Although%20GCG%20is%20a%0Asignificant%20milestone%2C%20its%20attacking%20efficiency%20remains%20unsatisfactory.%20In%20this%0Apaper%2C%20we%20present%20several%20improved%20%28empirical%29%20techniques%20for%0Aoptimization-based%20jailbreaks%20like%20GCG.%20We%20first%20observe%20that%20the%20single%20target%0Atemplate%20of%20%22Sure%22%20largely%20limits%20the%20attacking%20performance%20of%20GCG%3B%20given%20this%2C%0Awe%20propose%20to%20apply%20diverse%20target%20templates%20containing%20harmful%20self-suggestion%0Aand/or%20guidance%20to%20mislead%20LLMs.%20Besides%2C%20from%20the%20optimization%20aspects%2C%20we%0Apropose%20an%20automatic%20multi-coordinate%20updating%20strategy%20in%20GCG%20%28i.e.%2C%0Aadaptively%20deciding%20how%20many%20tokens%20to%20replace%20in%20each%20step%29%20to%20accelerate%0Aconvergence%2C%20as%20well%20as%20tricks%20like%20easy-to-hard%20initialisation.%20Then%2C%20we%0Acombine%20these%20improved%20technologies%20to%20develop%20an%20efficient%20jailbreak%20method%2C%0Adubbed%20%24%5Cmathcal%7BI%7D%24-GCG.%20In%20our%20experiments%2C%20we%20evaluate%20on%20a%20series%20of%0Abenchmarks%20%28such%20as%20NeurIPS%202023%20Red%20Teaming%20Track%29.%20The%20results%20demonstrate%0Athat%20our%20improved%20techniques%20can%20help%20GCG%20outperform%20state-of-the-art%0Ajailbreaking%20attacks%20and%20achieve%20nearly%20100%25%20attack%20success%20rate.%20The%20code%20is%0Areleased%20at%20https%3A//github.com/jiaxiaojunQAQ/I-GCG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21018v1&entry.124074799=Read"},
{"title": "An Accelerated Gradient Method for Convex Smooth Simple Bilevel\n  Optimization", "author": "Jincheng Cao and Ruichen Jiang and Erfan Yazdandoost Hamedani and Aryan Mokhtari", "abstract": "  In this paper, we focus on simple bilevel optimization problems, where we\nminimize a convex smooth objective function over the optimal solution set of\nanother convex smooth constrained optimization problem. We present a novel\nbilevel optimization method that locally approximates the solution set of the\nlower-level problem using a cutting plane approach and employs an accelerated\ngradient-based update to reduce the upper-level objective function over the\napproximated solution set. We measure the performance of our method in terms of\nsuboptimality and infeasibility errors and provide non-asymptotic convergence\nguarantees for both error criteria. Specifically, when the feasible set is\ncompact, we show that our method requires at most\n$\\mathcal{O}(\\max\\{1/\\sqrt{\\epsilon_{f}}, 1/\\epsilon_g\\})$ iterations to find a\nsolution that is $\\epsilon_f$-suboptimal and $\\epsilon_g$-infeasible. Moreover,\nunder the additional assumption that the lower-level objective satisfies the\n$r$-th H\\\"olderian error bound, we show that our method achieves an iteration\ncomplexity of\n$\\mathcal{O}(\\max\\{\\epsilon_{f}^{-\\frac{2r-1}{2r}},\\epsilon_{g}^{-\\frac{2r-1}{2r}}\\})$,\nwhich matches the optimal complexity of single-level convex constrained\noptimization when $r=1$.\n", "link": "http://arxiv.org/abs/2402.08097v2", "date": "2024-05-31", "relevancy": 1.8466, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3732}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.3704}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Accelerated%20Gradient%20Method%20for%20Convex%20Smooth%20Simple%20Bilevel%0A%20%20Optimization&body=Title%3A%20An%20Accelerated%20Gradient%20Method%20for%20Convex%20Smooth%20Simple%20Bilevel%0A%20%20Optimization%0AAuthor%3A%20Jincheng%20Cao%20and%20Ruichen%20Jiang%20and%20Erfan%20Yazdandoost%20Hamedani%20and%20Aryan%20Mokhtari%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20focus%20on%20simple%20bilevel%20optimization%20problems%2C%20where%20we%0Aminimize%20a%20convex%20smooth%20objective%20function%20over%20the%20optimal%20solution%20set%20of%0Aanother%20convex%20smooth%20constrained%20optimization%20problem.%20We%20present%20a%20novel%0Abilevel%20optimization%20method%20that%20locally%20approximates%20the%20solution%20set%20of%20the%0Alower-level%20problem%20using%20a%20cutting%20plane%20approach%20and%20employs%20an%20accelerated%0Agradient-based%20update%20to%20reduce%20the%20upper-level%20objective%20function%20over%20the%0Aapproximated%20solution%20set.%20We%20measure%20the%20performance%20of%20our%20method%20in%20terms%20of%0Asuboptimality%20and%20infeasibility%20errors%20and%20provide%20non-asymptotic%20convergence%0Aguarantees%20for%20both%20error%20criteria.%20Specifically%2C%20when%20the%20feasible%20set%20is%0Acompact%2C%20we%20show%20that%20our%20method%20requires%20at%20most%0A%24%5Cmathcal%7BO%7D%28%5Cmax%5C%7B1/%5Csqrt%7B%5Cepsilon_%7Bf%7D%7D%2C%201/%5Cepsilon_g%5C%7D%29%24%20iterations%20to%20find%20a%0Asolution%20that%20is%20%24%5Cepsilon_f%24-suboptimal%20and%20%24%5Cepsilon_g%24-infeasible.%20Moreover%2C%0Aunder%20the%20additional%20assumption%20that%20the%20lower-level%20objective%20satisfies%20the%0A%24r%24-th%20H%5C%22olderian%20error%20bound%2C%20we%20show%20that%20our%20method%20achieves%20an%20iteration%0Acomplexity%20of%0A%24%5Cmathcal%7BO%7D%28%5Cmax%5C%7B%5Cepsilon_%7Bf%7D%5E%7B-%5Cfrac%7B2r-1%7D%7B2r%7D%7D%2C%5Cepsilon_%7Bg%7D%5E%7B-%5Cfrac%7B2r-1%7D%7B2r%7D%7D%5C%7D%29%24%2C%0Awhich%20matches%20the%20optimal%20complexity%20of%20single-level%20convex%20constrained%0Aoptimization%20when%20%24r%3D1%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08097v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Accelerated%2520Gradient%2520Method%2520for%2520Convex%2520Smooth%2520Simple%2520Bilevel%250A%2520%2520Optimization%26entry.906535625%3DJincheng%2520Cao%2520and%2520Ruichen%2520Jiang%2520and%2520Erfan%2520Yazdandoost%2520Hamedani%2520and%2520Aryan%2520Mokhtari%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520simple%2520bilevel%2520optimization%2520problems%252C%2520where%2520we%250Aminimize%2520a%2520convex%2520smooth%2520objective%2520function%2520over%2520the%2520optimal%2520solution%2520set%2520of%250Aanother%2520convex%2520smooth%2520constrained%2520optimization%2520problem.%2520We%2520present%2520a%2520novel%250Abilevel%2520optimization%2520method%2520that%2520locally%2520approximates%2520the%2520solution%2520set%2520of%2520the%250Alower-level%2520problem%2520using%2520a%2520cutting%2520plane%2520approach%2520and%2520employs%2520an%2520accelerated%250Agradient-based%2520update%2520to%2520reduce%2520the%2520upper-level%2520objective%2520function%2520over%2520the%250Aapproximated%2520solution%2520set.%2520We%2520measure%2520the%2520performance%2520of%2520our%2520method%2520in%2520terms%2520of%250Asuboptimality%2520and%2520infeasibility%2520errors%2520and%2520provide%2520non-asymptotic%2520convergence%250Aguarantees%2520for%2520both%2520error%2520criteria.%2520Specifically%252C%2520when%2520the%2520feasible%2520set%2520is%250Acompact%252C%2520we%2520show%2520that%2520our%2520method%2520requires%2520at%2520most%250A%2524%255Cmathcal%257BO%257D%2528%255Cmax%255C%257B1/%255Csqrt%257B%255Cepsilon_%257Bf%257D%257D%252C%25201/%255Cepsilon_g%255C%257D%2529%2524%2520iterations%2520to%2520find%2520a%250Asolution%2520that%2520is%2520%2524%255Cepsilon_f%2524-suboptimal%2520and%2520%2524%255Cepsilon_g%2524-infeasible.%2520Moreover%252C%250Aunder%2520the%2520additional%2520assumption%2520that%2520the%2520lower-level%2520objective%2520satisfies%2520the%250A%2524r%2524-th%2520H%255C%2522olderian%2520error%2520bound%252C%2520we%2520show%2520that%2520our%2520method%2520achieves%2520an%2520iteration%250Acomplexity%2520of%250A%2524%255Cmathcal%257BO%257D%2528%255Cmax%255C%257B%255Cepsilon_%257Bf%257D%255E%257B-%255Cfrac%257B2r-1%257D%257B2r%257D%257D%252C%255Cepsilon_%257Bg%257D%255E%257B-%255Cfrac%257B2r-1%257D%257B2r%257D%257D%255C%257D%2529%2524%252C%250Awhich%2520matches%2520the%2520optimal%2520complexity%2520of%2520single-level%2520convex%2520constrained%250Aoptimization%2520when%2520%2524r%253D1%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08097v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Accelerated%20Gradient%20Method%20for%20Convex%20Smooth%20Simple%20Bilevel%0A%20%20Optimization&entry.906535625=Jincheng%20Cao%20and%20Ruichen%20Jiang%20and%20Erfan%20Yazdandoost%20Hamedani%20and%20Aryan%20Mokhtari&entry.1292438233=%20%20In%20this%20paper%2C%20we%20focus%20on%20simple%20bilevel%20optimization%20problems%2C%20where%20we%0Aminimize%20a%20convex%20smooth%20objective%20function%20over%20the%20optimal%20solution%20set%20of%0Aanother%20convex%20smooth%20constrained%20optimization%20problem.%20We%20present%20a%20novel%0Abilevel%20optimization%20method%20that%20locally%20approximates%20the%20solution%20set%20of%20the%0Alower-level%20problem%20using%20a%20cutting%20plane%20approach%20and%20employs%20an%20accelerated%0Agradient-based%20update%20to%20reduce%20the%20upper-level%20objective%20function%20over%20the%0Aapproximated%20solution%20set.%20We%20measure%20the%20performance%20of%20our%20method%20in%20terms%20of%0Asuboptimality%20and%20infeasibility%20errors%20and%20provide%20non-asymptotic%20convergence%0Aguarantees%20for%20both%20error%20criteria.%20Specifically%2C%20when%20the%20feasible%20set%20is%0Acompact%2C%20we%20show%20that%20our%20method%20requires%20at%20most%0A%24%5Cmathcal%7BO%7D%28%5Cmax%5C%7B1/%5Csqrt%7B%5Cepsilon_%7Bf%7D%7D%2C%201/%5Cepsilon_g%5C%7D%29%24%20iterations%20to%20find%20a%0Asolution%20that%20is%20%24%5Cepsilon_f%24-suboptimal%20and%20%24%5Cepsilon_g%24-infeasible.%20Moreover%2C%0Aunder%20the%20additional%20assumption%20that%20the%20lower-level%20objective%20satisfies%20the%0A%24r%24-th%20H%5C%22olderian%20error%20bound%2C%20we%20show%20that%20our%20method%20achieves%20an%20iteration%0Acomplexity%20of%0A%24%5Cmathcal%7BO%7D%28%5Cmax%5C%7B%5Cepsilon_%7Bf%7D%5E%7B-%5Cfrac%7B2r-1%7D%7B2r%7D%7D%2C%5Cepsilon_%7Bg%7D%5E%7B-%5Cfrac%7B2r-1%7D%7B2r%7D%7D%5C%7D%29%24%2C%0Awhich%20matches%20the%20optimal%20complexity%20of%20single-level%20convex%20constrained%0Aoptimization%20when%20%24r%3D1%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08097v2&entry.124074799=Read"},
{"title": "Neural Network Verification with Branch-and-Bound for General\n  Nonlinearities", "author": "Zhouxing Shi and Qirui Jin and Zico Kolter and Suman Jana and Cho-Jui Hsieh and Huan Zhang", "abstract": "  Branch-and-bound (BaB) is among the most effective methods for neural network\n(NN) verification. However, existing works on BaB have mostly focused on NNs\nwith piecewise linear activations, especially ReLU networks. In this paper, we\ndevelop a general framework, named GenBaB, to conduct BaB for general\nnonlinearities in general computational graphs based on linear bound\npropagation. To decide which neuron to branch, we design a new branching\nheuristic which leverages linear bounds as shortcuts to efficiently estimate\nthe potential improvement after branching. To decide nontrivial branching\npoints for general nonlinear functions, we propose to optimize branching points\noffline, which can be efficiently leveraged during verification with a lookup\ntable. We demonstrate the effectiveness of our GenBaB on verifying a wide range\nof NNs, including networks with activation functions such as Sigmoid, Tanh,\nSine and GeLU, as well as networks involving multi-dimensional nonlinear\noperations such as multiplications in LSTMs and Vision Transformers. Our\nframework also allows the verification of general nonlinear computation graphs\nand enables verification applications beyond simple neural networks,\nparticularly for AC Optimal Power Flow (ACOPF). GenBaB is part of the latest\n$\\alpha,\\!\\beta$-CROWN, the winner of the 4th International Verification of\nNeural Networks Competition (VNN-COMP 2023).\n", "link": "http://arxiv.org/abs/2405.21063v1", "date": "2024-05-31", "relevancy": 1.8447, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4899}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Network%20Verification%20with%20Branch-and-Bound%20for%20General%0A%20%20Nonlinearities&body=Title%3A%20Neural%20Network%20Verification%20with%20Branch-and-Bound%20for%20General%0A%20%20Nonlinearities%0AAuthor%3A%20Zhouxing%20Shi%20and%20Qirui%20Jin%20and%20Zico%20Kolter%20and%20Suman%20Jana%20and%20Cho-Jui%20Hsieh%20and%20Huan%20Zhang%0AAbstract%3A%20%20%20Branch-and-bound%20%28BaB%29%20is%20among%20the%20most%20effective%20methods%20for%20neural%20network%0A%28NN%29%20verification.%20However%2C%20existing%20works%20on%20BaB%20have%20mostly%20focused%20on%20NNs%0Awith%20piecewise%20linear%20activations%2C%20especially%20ReLU%20networks.%20In%20this%20paper%2C%20we%0Adevelop%20a%20general%20framework%2C%20named%20GenBaB%2C%20to%20conduct%20BaB%20for%20general%0Anonlinearities%20in%20general%20computational%20graphs%20based%20on%20linear%20bound%0Apropagation.%20To%20decide%20which%20neuron%20to%20branch%2C%20we%20design%20a%20new%20branching%0Aheuristic%20which%20leverages%20linear%20bounds%20as%20shortcuts%20to%20efficiently%20estimate%0Athe%20potential%20improvement%20after%20branching.%20To%20decide%20nontrivial%20branching%0Apoints%20for%20general%20nonlinear%20functions%2C%20we%20propose%20to%20optimize%20branching%20points%0Aoffline%2C%20which%20can%20be%20efficiently%20leveraged%20during%20verification%20with%20a%20lookup%0Atable.%20We%20demonstrate%20the%20effectiveness%20of%20our%20GenBaB%20on%20verifying%20a%20wide%20range%0Aof%20NNs%2C%20including%20networks%20with%20activation%20functions%20such%20as%20Sigmoid%2C%20Tanh%2C%0ASine%20and%20GeLU%2C%20as%20well%20as%20networks%20involving%20multi-dimensional%20nonlinear%0Aoperations%20such%20as%20multiplications%20in%20LSTMs%20and%20Vision%20Transformers.%20Our%0Aframework%20also%20allows%20the%20verification%20of%20general%20nonlinear%20computation%20graphs%0Aand%20enables%20verification%20applications%20beyond%20simple%20neural%20networks%2C%0Aparticularly%20for%20AC%20Optimal%20Power%20Flow%20%28ACOPF%29.%20GenBaB%20is%20part%20of%20the%20latest%0A%24%5Calpha%2C%5C%21%5Cbeta%24-CROWN%2C%20the%20winner%20of%20the%204th%20International%20Verification%20of%0ANeural%20Networks%20Competition%20%28VNN-COMP%202023%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21063v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Network%2520Verification%2520with%2520Branch-and-Bound%2520for%2520General%250A%2520%2520Nonlinearities%26entry.906535625%3DZhouxing%2520Shi%2520and%2520Qirui%2520Jin%2520and%2520Zico%2520Kolter%2520and%2520Suman%2520Jana%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Huan%2520Zhang%26entry.1292438233%3D%2520%2520Branch-and-bound%2520%2528BaB%2529%2520is%2520among%2520the%2520most%2520effective%2520methods%2520for%2520neural%2520network%250A%2528NN%2529%2520verification.%2520However%252C%2520existing%2520works%2520on%2520BaB%2520have%2520mostly%2520focused%2520on%2520NNs%250Awith%2520piecewise%2520linear%2520activations%252C%2520especially%2520ReLU%2520networks.%2520In%2520this%2520paper%252C%2520we%250Adevelop%2520a%2520general%2520framework%252C%2520named%2520GenBaB%252C%2520to%2520conduct%2520BaB%2520for%2520general%250Anonlinearities%2520in%2520general%2520computational%2520graphs%2520based%2520on%2520linear%2520bound%250Apropagation.%2520To%2520decide%2520which%2520neuron%2520to%2520branch%252C%2520we%2520design%2520a%2520new%2520branching%250Aheuristic%2520which%2520leverages%2520linear%2520bounds%2520as%2520shortcuts%2520to%2520efficiently%2520estimate%250Athe%2520potential%2520improvement%2520after%2520branching.%2520To%2520decide%2520nontrivial%2520branching%250Apoints%2520for%2520general%2520nonlinear%2520functions%252C%2520we%2520propose%2520to%2520optimize%2520branching%2520points%250Aoffline%252C%2520which%2520can%2520be%2520efficiently%2520leveraged%2520during%2520verification%2520with%2520a%2520lookup%250Atable.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520GenBaB%2520on%2520verifying%2520a%2520wide%2520range%250Aof%2520NNs%252C%2520including%2520networks%2520with%2520activation%2520functions%2520such%2520as%2520Sigmoid%252C%2520Tanh%252C%250ASine%2520and%2520GeLU%252C%2520as%2520well%2520as%2520networks%2520involving%2520multi-dimensional%2520nonlinear%250Aoperations%2520such%2520as%2520multiplications%2520in%2520LSTMs%2520and%2520Vision%2520Transformers.%2520Our%250Aframework%2520also%2520allows%2520the%2520verification%2520of%2520general%2520nonlinear%2520computation%2520graphs%250Aand%2520enables%2520verification%2520applications%2520beyond%2520simple%2520neural%2520networks%252C%250Aparticularly%2520for%2520AC%2520Optimal%2520Power%2520Flow%2520%2528ACOPF%2529.%2520GenBaB%2520is%2520part%2520of%2520the%2520latest%250A%2524%255Calpha%252C%255C%2521%255Cbeta%2524-CROWN%252C%2520the%2520winner%2520of%2520the%25204th%2520International%2520Verification%2520of%250ANeural%2520Networks%2520Competition%2520%2528VNN-COMP%25202023%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21063v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network%20Verification%20with%20Branch-and-Bound%20for%20General%0A%20%20Nonlinearities&entry.906535625=Zhouxing%20Shi%20and%20Qirui%20Jin%20and%20Zico%20Kolter%20and%20Suman%20Jana%20and%20Cho-Jui%20Hsieh%20and%20Huan%20Zhang&entry.1292438233=%20%20Branch-and-bound%20%28BaB%29%20is%20among%20the%20most%20effective%20methods%20for%20neural%20network%0A%28NN%29%20verification.%20However%2C%20existing%20works%20on%20BaB%20have%20mostly%20focused%20on%20NNs%0Awith%20piecewise%20linear%20activations%2C%20especially%20ReLU%20networks.%20In%20this%20paper%2C%20we%0Adevelop%20a%20general%20framework%2C%20named%20GenBaB%2C%20to%20conduct%20BaB%20for%20general%0Anonlinearities%20in%20general%20computational%20graphs%20based%20on%20linear%20bound%0Apropagation.%20To%20decide%20which%20neuron%20to%20branch%2C%20we%20design%20a%20new%20branching%0Aheuristic%20which%20leverages%20linear%20bounds%20as%20shortcuts%20to%20efficiently%20estimate%0Athe%20potential%20improvement%20after%20branching.%20To%20decide%20nontrivial%20branching%0Apoints%20for%20general%20nonlinear%20functions%2C%20we%20propose%20to%20optimize%20branching%20points%0Aoffline%2C%20which%20can%20be%20efficiently%20leveraged%20during%20verification%20with%20a%20lookup%0Atable.%20We%20demonstrate%20the%20effectiveness%20of%20our%20GenBaB%20on%20verifying%20a%20wide%20range%0Aof%20NNs%2C%20including%20networks%20with%20activation%20functions%20such%20as%20Sigmoid%2C%20Tanh%2C%0ASine%20and%20GeLU%2C%20as%20well%20as%20networks%20involving%20multi-dimensional%20nonlinear%0Aoperations%20such%20as%20multiplications%20in%20LSTMs%20and%20Vision%20Transformers.%20Our%0Aframework%20also%20allows%20the%20verification%20of%20general%20nonlinear%20computation%20graphs%0Aand%20enables%20verification%20applications%20beyond%20simple%20neural%20networks%2C%0Aparticularly%20for%20AC%20Optimal%20Power%20Flow%20%28ACOPF%29.%20GenBaB%20is%20part%20of%20the%20latest%0A%24%5Calpha%2C%5C%21%5Cbeta%24-CROWN%2C%20the%20winner%20of%20the%204th%20International%20Verification%20of%0ANeural%20Networks%20Competition%20%28VNN-COMP%202023%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21063v1&entry.124074799=Read"},
{"title": "High-dimensional robust regression under heavy-tailed data: Asymptotics\n  and Universality", "author": "Urte Adomaityte and Leonardo Defilippis and Bruno Loureiro and Gabriele Sicuro", "abstract": "  We investigate the high-dimensional properties of robust regression\nestimators in the presence of heavy-tailed contamination of both the covariates\nand response functions. In particular, we provide a sharp asymptotic\ncharacterisation of M-estimators trained on a family of elliptical covariate\nand noise data distributions including cases where second and higher moments do\nnot exist. We show that, despite being consistent, the Huber loss with\noptimally tuned location parameter $\\delta$ is suboptimal in the\nhigh-dimensional regime in the presence of heavy-tailed noise, highlighting the\nnecessity of further regularisation to achieve optimal performance. This result\nalso uncovers the existence of a transition in $\\delta$ as a function of the\nsample complexity and contamination. Moreover, we derive the decay rates for\nthe excess risk of ridge regression. We show that, while it is both optimal and\nuniversal for covariate distributions with finite second moment, its decay rate\ncan be considerably faster when the covariates' second moment does not exist.\nFinally, we show that our formulas readily generalise to a richer family of\nmodels and data distributions, such as generalised linear estimation with\narbitrary convex regularisation trained on mixture models.\n", "link": "http://arxiv.org/abs/2309.16476v2", "date": "2024-05-31", "relevancy": 1.3074, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4403}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4391}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-dimensional%20robust%20regression%20under%20heavy-tailed%20data%3A%20Asymptotics%0A%20%20and%20Universality&body=Title%3A%20High-dimensional%20robust%20regression%20under%20heavy-tailed%20data%3A%20Asymptotics%0A%20%20and%20Universality%0AAuthor%3A%20Urte%20Adomaityte%20and%20Leonardo%20Defilippis%20and%20Bruno%20Loureiro%20and%20Gabriele%20Sicuro%0AAbstract%3A%20%20%20We%20investigate%20the%20high-dimensional%20properties%20of%20robust%20regression%0Aestimators%20in%20the%20presence%20of%20heavy-tailed%20contamination%20of%20both%20the%20covariates%0Aand%20response%20functions.%20In%20particular%2C%20we%20provide%20a%20sharp%20asymptotic%0Acharacterisation%20of%20M-estimators%20trained%20on%20a%20family%20of%20elliptical%20covariate%0Aand%20noise%20data%20distributions%20including%20cases%20where%20second%20and%20higher%20moments%20do%0Anot%20exist.%20We%20show%20that%2C%20despite%20being%20consistent%2C%20the%20Huber%20loss%20with%0Aoptimally%20tuned%20location%20parameter%20%24%5Cdelta%24%20is%20suboptimal%20in%20the%0Ahigh-dimensional%20regime%20in%20the%20presence%20of%20heavy-tailed%20noise%2C%20highlighting%20the%0Anecessity%20of%20further%20regularisation%20to%20achieve%20optimal%20performance.%20This%20result%0Aalso%20uncovers%20the%20existence%20of%20a%20transition%20in%20%24%5Cdelta%24%20as%20a%20function%20of%20the%0Asample%20complexity%20and%20contamination.%20Moreover%2C%20we%20derive%20the%20decay%20rates%20for%0Athe%20excess%20risk%20of%20ridge%20regression.%20We%20show%20that%2C%20while%20it%20is%20both%20optimal%20and%0Auniversal%20for%20covariate%20distributions%20with%20finite%20second%20moment%2C%20its%20decay%20rate%0Acan%20be%20considerably%20faster%20when%20the%20covariates%27%20second%20moment%20does%20not%20exist.%0AFinally%2C%20we%20show%20that%20our%20formulas%20readily%20generalise%20to%20a%20richer%20family%20of%0Amodels%20and%20data%20distributions%2C%20such%20as%20generalised%20linear%20estimation%20with%0Aarbitrary%20convex%20regularisation%20trained%20on%20mixture%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-dimensional%2520robust%2520regression%2520under%2520heavy-tailed%2520data%253A%2520Asymptotics%250A%2520%2520and%2520Universality%26entry.906535625%3DUrte%2520Adomaityte%2520and%2520Leonardo%2520Defilippis%2520and%2520Bruno%2520Loureiro%2520and%2520Gabriele%2520Sicuro%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520high-dimensional%2520properties%2520of%2520robust%2520regression%250Aestimators%2520in%2520the%2520presence%2520of%2520heavy-tailed%2520contamination%2520of%2520both%2520the%2520covariates%250Aand%2520response%2520functions.%2520In%2520particular%252C%2520we%2520provide%2520a%2520sharp%2520asymptotic%250Acharacterisation%2520of%2520M-estimators%2520trained%2520on%2520a%2520family%2520of%2520elliptical%2520covariate%250Aand%2520noise%2520data%2520distributions%2520including%2520cases%2520where%2520second%2520and%2520higher%2520moments%2520do%250Anot%2520exist.%2520We%2520show%2520that%252C%2520despite%2520being%2520consistent%252C%2520the%2520Huber%2520loss%2520with%250Aoptimally%2520tuned%2520location%2520parameter%2520%2524%255Cdelta%2524%2520is%2520suboptimal%2520in%2520the%250Ahigh-dimensional%2520regime%2520in%2520the%2520presence%2520of%2520heavy-tailed%2520noise%252C%2520highlighting%2520the%250Anecessity%2520of%2520further%2520regularisation%2520to%2520achieve%2520optimal%2520performance.%2520This%2520result%250Aalso%2520uncovers%2520the%2520existence%2520of%2520a%2520transition%2520in%2520%2524%255Cdelta%2524%2520as%2520a%2520function%2520of%2520the%250Asample%2520complexity%2520and%2520contamination.%2520Moreover%252C%2520we%2520derive%2520the%2520decay%2520rates%2520for%250Athe%2520excess%2520risk%2520of%2520ridge%2520regression.%2520We%2520show%2520that%252C%2520while%2520it%2520is%2520both%2520optimal%2520and%250Auniversal%2520for%2520covariate%2520distributions%2520with%2520finite%2520second%2520moment%252C%2520its%2520decay%2520rate%250Acan%2520be%2520considerably%2520faster%2520when%2520the%2520covariates%2527%2520second%2520moment%2520does%2520not%2520exist.%250AFinally%252C%2520we%2520show%2520that%2520our%2520formulas%2520readily%2520generalise%2520to%2520a%2520richer%2520family%2520of%250Amodels%2520and%2520data%2520distributions%252C%2520such%2520as%2520generalised%2520linear%2520estimation%2520with%250Aarbitrary%2520convex%2520regularisation%2520trained%2520on%2520mixture%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-dimensional%20robust%20regression%20under%20heavy-tailed%20data%3A%20Asymptotics%0A%20%20and%20Universality&entry.906535625=Urte%20Adomaityte%20and%20Leonardo%20Defilippis%20and%20Bruno%20Loureiro%20and%20Gabriele%20Sicuro&entry.1292438233=%20%20We%20investigate%20the%20high-dimensional%20properties%20of%20robust%20regression%0Aestimators%20in%20the%20presence%20of%20heavy-tailed%20contamination%20of%20both%20the%20covariates%0Aand%20response%20functions.%20In%20particular%2C%20we%20provide%20a%20sharp%20asymptotic%0Acharacterisation%20of%20M-estimators%20trained%20on%20a%20family%20of%20elliptical%20covariate%0Aand%20noise%20data%20distributions%20including%20cases%20where%20second%20and%20higher%20moments%20do%0Anot%20exist.%20We%20show%20that%2C%20despite%20being%20consistent%2C%20the%20Huber%20loss%20with%0Aoptimally%20tuned%20location%20parameter%20%24%5Cdelta%24%20is%20suboptimal%20in%20the%0Ahigh-dimensional%20regime%20in%20the%20presence%20of%20heavy-tailed%20noise%2C%20highlighting%20the%0Anecessity%20of%20further%20regularisation%20to%20achieve%20optimal%20performance.%20This%20result%0Aalso%20uncovers%20the%20existence%20of%20a%20transition%20in%20%24%5Cdelta%24%20as%20a%20function%20of%20the%0Asample%20complexity%20and%20contamination.%20Moreover%2C%20we%20derive%20the%20decay%20rates%20for%0Athe%20excess%20risk%20of%20ridge%20regression.%20We%20show%20that%2C%20while%20it%20is%20both%20optimal%20and%0Auniversal%20for%20covariate%20distributions%20with%20finite%20second%20moment%2C%20its%20decay%20rate%0Acan%20be%20considerably%20faster%20when%20the%20covariates%27%20second%20moment%20does%20not%20exist.%0AFinally%2C%20we%20show%20that%20our%20formulas%20readily%20generalise%20to%20a%20richer%20family%20of%0Amodels%20and%20data%20distributions%2C%20such%20as%20generalised%20linear%20estimation%20with%0Aarbitrary%20convex%20regularisation%20trained%20on%20mixture%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16476v2&entry.124074799=Read"},
{"title": "Transformers are SSMs: Generalized Models and Efficient Algorithms\n  Through Structured State Space Duality", "author": "Tri Dao and Albert Gu", "abstract": "  While Transformers have been the main architecture behind deep learning's\nsuccess in language modeling, state-space models (SSMs) such as Mamba have\nrecently been shown to match or outperform Transformers at small to medium\nscale. We show that these families of models are actually quite closely\nrelated, and develop a rich framework of theoretical connections between SSMs\nand variants of attention, connected through various decompositions of a\nwell-studied class of structured semiseparable matrices. Our state space\nduality (SSD) framework allows us to design a new architecture (Mamba-2) whose\ncore layer is an a refinement of Mamba's selective SSM that is 2-8X faster,\nwhile continuing to be competitive with Transformers on language modeling.\n", "link": "http://arxiv.org/abs/2405.21060v1", "date": "2024-05-31", "relevancy": 1.5895, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5667}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5238}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20are%20SSMs%3A%20Generalized%20Models%20and%20Efficient%20Algorithms%0A%20%20Through%20Structured%20State%20Space%20Duality&body=Title%3A%20Transformers%20are%20SSMs%3A%20Generalized%20Models%20and%20Efficient%20Algorithms%0A%20%20Through%20Structured%20State%20Space%20Duality%0AAuthor%3A%20Tri%20Dao%20and%20Albert%20Gu%0AAbstract%3A%20%20%20While%20Transformers%20have%20been%20the%20main%20architecture%20behind%20deep%20learning%27s%0Asuccess%20in%20language%20modeling%2C%20state-space%20models%20%28SSMs%29%20such%20as%20Mamba%20have%0Arecently%20been%20shown%20to%20match%20or%20outperform%20Transformers%20at%20small%20to%20medium%0Ascale.%20We%20show%20that%20these%20families%20of%20models%20are%20actually%20quite%20closely%0Arelated%2C%20and%20develop%20a%20rich%20framework%20of%20theoretical%20connections%20between%20SSMs%0Aand%20variants%20of%20attention%2C%20connected%20through%20various%20decompositions%20of%20a%0Awell-studied%20class%20of%20structured%20semiseparable%20matrices.%20Our%20state%20space%0Aduality%20%28SSD%29%20framework%20allows%20us%20to%20design%20a%20new%20architecture%20%28Mamba-2%29%20whose%0Acore%20layer%20is%20an%20a%20refinement%20of%20Mamba%27s%20selective%20SSM%20that%20is%202-8X%20faster%2C%0Awhile%20continuing%20to%20be%20competitive%20with%20Transformers%20on%20language%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520are%2520SSMs%253A%2520Generalized%2520Models%2520and%2520Efficient%2520Algorithms%250A%2520%2520Through%2520Structured%2520State%2520Space%2520Duality%26entry.906535625%3DTri%2520Dao%2520and%2520Albert%2520Gu%26entry.1292438233%3D%2520%2520While%2520Transformers%2520have%2520been%2520the%2520main%2520architecture%2520behind%2520deep%2520learning%2527s%250Asuccess%2520in%2520language%2520modeling%252C%2520state-space%2520models%2520%2528SSMs%2529%2520such%2520as%2520Mamba%2520have%250Arecently%2520been%2520shown%2520to%2520match%2520or%2520outperform%2520Transformers%2520at%2520small%2520to%2520medium%250Ascale.%2520We%2520show%2520that%2520these%2520families%2520of%2520models%2520are%2520actually%2520quite%2520closely%250Arelated%252C%2520and%2520develop%2520a%2520rich%2520framework%2520of%2520theoretical%2520connections%2520between%2520SSMs%250Aand%2520variants%2520of%2520attention%252C%2520connected%2520through%2520various%2520decompositions%2520of%2520a%250Awell-studied%2520class%2520of%2520structured%2520semiseparable%2520matrices.%2520Our%2520state%2520space%250Aduality%2520%2528SSD%2529%2520framework%2520allows%2520us%2520to%2520design%2520a%2520new%2520architecture%2520%2528Mamba-2%2529%2520whose%250Acore%2520layer%2520is%2520an%2520a%2520refinement%2520of%2520Mamba%2527s%2520selective%2520SSM%2520that%2520is%25202-8X%2520faster%252C%250Awhile%2520continuing%2520to%2520be%2520competitive%2520with%2520Transformers%2520on%2520language%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20are%20SSMs%3A%20Generalized%20Models%20and%20Efficient%20Algorithms%0A%20%20Through%20Structured%20State%20Space%20Duality&entry.906535625=Tri%20Dao%20and%20Albert%20Gu&entry.1292438233=%20%20While%20Transformers%20have%20been%20the%20main%20architecture%20behind%20deep%20learning%27s%0Asuccess%20in%20language%20modeling%2C%20state-space%20models%20%28SSMs%29%20such%20as%20Mamba%20have%0Arecently%20been%20shown%20to%20match%20or%20outperform%20Transformers%20at%20small%20to%20medium%0Ascale.%20We%20show%20that%20these%20families%20of%20models%20are%20actually%20quite%20closely%0Arelated%2C%20and%20develop%20a%20rich%20framework%20of%20theoretical%20connections%20between%20SSMs%0Aand%20variants%20of%20attention%2C%20connected%20through%20various%20decompositions%20of%20a%0Awell-studied%20class%20of%20structured%20semiseparable%20matrices.%20Our%20state%20space%0Aduality%20%28SSD%29%20framework%20allows%20us%20to%20design%20a%20new%20architecture%20%28Mamba-2%29%20whose%0Acore%20layer%20is%20an%20a%20refinement%20of%20Mamba%27s%20selective%20SSM%20that%20is%202-8X%20faster%2C%0Awhile%20continuing%20to%20be%20competitive%20with%20Transformers%20on%20language%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21060v1&entry.124074799=Read"},
{"title": "G-Transformer for Conditional Average Potential Outcome Estimation over\n  Time", "author": "Konstantin Hess and Dennis Frauen and Valentyn Melnychuk and Stefan Feuerriegel", "abstract": "  Estimating potential outcomes for treatments over time based on observational\ndata is important for personalized decision-making in medicine. Yet, existing\nneural methods for this task suffer from either (a) bias or (b) large variance.\nIn order to address both limitations, we introduce the G-transformer (GT). Our\nGT is a novel, neural end-to-end model designed for unbiased, low-variance\nestimation of conditional average potential outcomes (CAPOs) over time.\nSpecifically, our GT is the first neural model to perform regression-based\niterative G-computation for CAPOs in the time-varying setting. We evaluate the\neffectiveness of our GT across various experiments. In sum, this work\nrepresents a significant step towards personalized decision-making from\nelectronic health records.\n", "link": "http://arxiv.org/abs/2405.21012v1", "date": "2024-05-31", "relevancy": 1.3914, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4965}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4545}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-Transformer%20for%20Conditional%20Average%20Potential%20Outcome%20Estimation%20over%0A%20%20Time&body=Title%3A%20G-Transformer%20for%20Conditional%20Average%20Potential%20Outcome%20Estimation%20over%0A%20%20Time%0AAuthor%3A%20Konstantin%20Hess%20and%20Dennis%20Frauen%20and%20Valentyn%20Melnychuk%20and%20Stefan%20Feuerriegel%0AAbstract%3A%20%20%20Estimating%20potential%20outcomes%20for%20treatments%20over%20time%20based%20on%20observational%0Adata%20is%20important%20for%20personalized%20decision-making%20in%20medicine.%20Yet%2C%20existing%0Aneural%20methods%20for%20this%20task%20suffer%20from%20either%20%28a%29%20bias%20or%20%28b%29%20large%20variance.%0AIn%20order%20to%20address%20both%20limitations%2C%20we%20introduce%20the%20G-transformer%20%28GT%29.%20Our%0AGT%20is%20a%20novel%2C%20neural%20end-to-end%20model%20designed%20for%20unbiased%2C%20low-variance%0Aestimation%20of%20conditional%20average%20potential%20outcomes%20%28CAPOs%29%20over%20time.%0ASpecifically%2C%20our%20GT%20is%20the%20first%20neural%20model%20to%20perform%20regression-based%0Aiterative%20G-computation%20for%20CAPOs%20in%20the%20time-varying%20setting.%20We%20evaluate%20the%0Aeffectiveness%20of%20our%20GT%20across%20various%20experiments.%20In%20sum%2C%20this%20work%0Arepresents%20a%20significant%20step%20towards%20personalized%20decision-making%20from%0Aelectronic%20health%20records.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-Transformer%2520for%2520Conditional%2520Average%2520Potential%2520Outcome%2520Estimation%2520over%250A%2520%2520Time%26entry.906535625%3DKonstantin%2520Hess%2520and%2520Dennis%2520Frauen%2520and%2520Valentyn%2520Melnychuk%2520and%2520Stefan%2520Feuerriegel%26entry.1292438233%3D%2520%2520Estimating%2520potential%2520outcomes%2520for%2520treatments%2520over%2520time%2520based%2520on%2520observational%250Adata%2520is%2520important%2520for%2520personalized%2520decision-making%2520in%2520medicine.%2520Yet%252C%2520existing%250Aneural%2520methods%2520for%2520this%2520task%2520suffer%2520from%2520either%2520%2528a%2529%2520bias%2520or%2520%2528b%2529%2520large%2520variance.%250AIn%2520order%2520to%2520address%2520both%2520limitations%252C%2520we%2520introduce%2520the%2520G-transformer%2520%2528GT%2529.%2520Our%250AGT%2520is%2520a%2520novel%252C%2520neural%2520end-to-end%2520model%2520designed%2520for%2520unbiased%252C%2520low-variance%250Aestimation%2520of%2520conditional%2520average%2520potential%2520outcomes%2520%2528CAPOs%2529%2520over%2520time.%250ASpecifically%252C%2520our%2520GT%2520is%2520the%2520first%2520neural%2520model%2520to%2520perform%2520regression-based%250Aiterative%2520G-computation%2520for%2520CAPOs%2520in%2520the%2520time-varying%2520setting.%2520We%2520evaluate%2520the%250Aeffectiveness%2520of%2520our%2520GT%2520across%2520various%2520experiments.%2520In%2520sum%252C%2520this%2520work%250Arepresents%2520a%2520significant%2520step%2520towards%2520personalized%2520decision-making%2520from%250Aelectronic%2520health%2520records.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-Transformer%20for%20Conditional%20Average%20Potential%20Outcome%20Estimation%20over%0A%20%20Time&entry.906535625=Konstantin%20Hess%20and%20Dennis%20Frauen%20and%20Valentyn%20Melnychuk%20and%20Stefan%20Feuerriegel&entry.1292438233=%20%20Estimating%20potential%20outcomes%20for%20treatments%20over%20time%20based%20on%20observational%0Adata%20is%20important%20for%20personalized%20decision-making%20in%20medicine.%20Yet%2C%20existing%0Aneural%20methods%20for%20this%20task%20suffer%20from%20either%20%28a%29%20bias%20or%20%28b%29%20large%20variance.%0AIn%20order%20to%20address%20both%20limitations%2C%20we%20introduce%20the%20G-transformer%20%28GT%29.%20Our%0AGT%20is%20a%20novel%2C%20neural%20end-to-end%20model%20designed%20for%20unbiased%2C%20low-variance%0Aestimation%20of%20conditional%20average%20potential%20outcomes%20%28CAPOs%29%20over%20time.%0ASpecifically%2C%20our%20GT%20is%20the%20first%20neural%20model%20to%20perform%20regression-based%0Aiterative%20G-computation%20for%20CAPOs%20in%20the%20time-varying%20setting.%20We%20evaluate%20the%0Aeffectiveness%20of%20our%20GT%20across%20various%20experiments.%20In%20sum%2C%20this%20work%0Arepresents%20a%20significant%20step%20towards%20personalized%20decision-making%20from%0Aelectronic%20health%20records.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21012v1&entry.124074799=Read"},
{"title": "Constrained Dynamics Simulation: More With Less", "author": "Ajay Suresha Sathya", "abstract": "  Efficient robot dynamics simulation is a fundamental problem key for robot\ncontrol, identification, design and analysis. This research statement explores\nmy current progress in this field and future research directions.\n", "link": "http://arxiv.org/abs/2405.20820v1", "date": "2024-05-31", "relevancy": 1.378, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4931}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4784}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Dynamics%20Simulation%3A%20More%20With%20Less&body=Title%3A%20Constrained%20Dynamics%20Simulation%3A%20More%20With%20Less%0AAuthor%3A%20Ajay%20Suresha%20Sathya%0AAbstract%3A%20%20%20Efficient%20robot%20dynamics%20simulation%20is%20a%20fundamental%20problem%20key%20for%20robot%0Acontrol%2C%20identification%2C%20design%20and%20analysis.%20This%20research%20statement%20explores%0Amy%20current%20progress%20in%20this%20field%20and%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Dynamics%2520Simulation%253A%2520More%2520With%2520Less%26entry.906535625%3DAjay%2520Suresha%2520Sathya%26entry.1292438233%3D%2520%2520Efficient%2520robot%2520dynamics%2520simulation%2520is%2520a%2520fundamental%2520problem%2520key%2520for%2520robot%250Acontrol%252C%2520identification%252C%2520design%2520and%2520analysis.%2520This%2520research%2520statement%2520explores%250Amy%2520current%2520progress%2520in%2520this%2520field%2520and%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Dynamics%20Simulation%3A%20More%20With%20Less&entry.906535625=Ajay%20Suresha%20Sathya&entry.1292438233=%20%20Efficient%20robot%20dynamics%20simulation%20is%20a%20fundamental%20problem%20key%20for%20robot%0Acontrol%2C%20identification%2C%20design%20and%20analysis.%20This%20research%20statement%20explores%0Amy%20current%20progress%20in%20this%20field%20and%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20820v1&entry.124074799=Read"},
{"title": "Designing for Fairness in Human-Robot Interactions", "author": "Houston Claure", "abstract": "  The foundation of successful human collaboration is deeply rooted in the\nprinciples of fairness. As robots are increasingly prevalent in various parts\nof society where they are working alongside groups and teams of humans, their\nability to understand and act according to principles of fairness becomes\ncrucial for their effective integration. This is especially critical when\nrobots are part of multi-human teams, where they must make continuous decisions\nregarding the allocation of resources. These resources can be material, such as\ntools, or communicative, such as gaze direction, and must be distributed fairly\namong team members to ensure optimal team performance and healthy group\ndynamics. Therefore, our research focuses on understanding how robots can\neffectively participate within human groups by making fair decisions while\ncontributing positively to group dynamics and outcomes. In this paper, I\ndiscuss advances toward ensuring that robots are capable of considering human\nnotions of fairness in their decision-making.\n", "link": "http://arxiv.org/abs/2405.21044v1", "date": "2024-05-31", "relevancy": 1.4524, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5068}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4833}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Designing%20for%20Fairness%20in%20Human-Robot%20Interactions&body=Title%3A%20Designing%20for%20Fairness%20in%20Human-Robot%20Interactions%0AAuthor%3A%20Houston%20Claure%0AAbstract%3A%20%20%20The%20foundation%20of%20successful%20human%20collaboration%20is%20deeply%20rooted%20in%20the%0Aprinciples%20of%20fairness.%20As%20robots%20are%20increasingly%20prevalent%20in%20various%20parts%0Aof%20society%20where%20they%20are%20working%20alongside%20groups%20and%20teams%20of%20humans%2C%20their%0Aability%20to%20understand%20and%20act%20according%20to%20principles%20of%20fairness%20becomes%0Acrucial%20for%20their%20effective%20integration.%20This%20is%20especially%20critical%20when%0Arobots%20are%20part%20of%20multi-human%20teams%2C%20where%20they%20must%20make%20continuous%20decisions%0Aregarding%20the%20allocation%20of%20resources.%20These%20resources%20can%20be%20material%2C%20such%20as%0Atools%2C%20or%20communicative%2C%20such%20as%20gaze%20direction%2C%20and%20must%20be%20distributed%20fairly%0Aamong%20team%20members%20to%20ensure%20optimal%20team%20performance%20and%20healthy%20group%0Adynamics.%20Therefore%2C%20our%20research%20focuses%20on%20understanding%20how%20robots%20can%0Aeffectively%20participate%20within%20human%20groups%20by%20making%20fair%20decisions%20while%0Acontributing%20positively%20to%20group%20dynamics%20and%20outcomes.%20In%20this%20paper%2C%20I%0Adiscuss%20advances%20toward%20ensuring%20that%20robots%20are%20capable%20of%20considering%20human%0Anotions%20of%20fairness%20in%20their%20decision-making.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesigning%2520for%2520Fairness%2520in%2520Human-Robot%2520Interactions%26entry.906535625%3DHouston%2520Claure%26entry.1292438233%3D%2520%2520The%2520foundation%2520of%2520successful%2520human%2520collaboration%2520is%2520deeply%2520rooted%2520in%2520the%250Aprinciples%2520of%2520fairness.%2520As%2520robots%2520are%2520increasingly%2520prevalent%2520in%2520various%2520parts%250Aof%2520society%2520where%2520they%2520are%2520working%2520alongside%2520groups%2520and%2520teams%2520of%2520humans%252C%2520their%250Aability%2520to%2520understand%2520and%2520act%2520according%2520to%2520principles%2520of%2520fairness%2520becomes%250Acrucial%2520for%2520their%2520effective%2520integration.%2520This%2520is%2520especially%2520critical%2520when%250Arobots%2520are%2520part%2520of%2520multi-human%2520teams%252C%2520where%2520they%2520must%2520make%2520continuous%2520decisions%250Aregarding%2520the%2520allocation%2520of%2520resources.%2520These%2520resources%2520can%2520be%2520material%252C%2520such%2520as%250Atools%252C%2520or%2520communicative%252C%2520such%2520as%2520gaze%2520direction%252C%2520and%2520must%2520be%2520distributed%2520fairly%250Aamong%2520team%2520members%2520to%2520ensure%2520optimal%2520team%2520performance%2520and%2520healthy%2520group%250Adynamics.%2520Therefore%252C%2520our%2520research%2520focuses%2520on%2520understanding%2520how%2520robots%2520can%250Aeffectively%2520participate%2520within%2520human%2520groups%2520by%2520making%2520fair%2520decisions%2520while%250Acontributing%2520positively%2520to%2520group%2520dynamics%2520and%2520outcomes.%2520In%2520this%2520paper%252C%2520I%250Adiscuss%2520advances%2520toward%2520ensuring%2520that%2520robots%2520are%2520capable%2520of%2520considering%2520human%250Anotions%2520of%2520fairness%2520in%2520their%2520decision-making.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Designing%20for%20Fairness%20in%20Human-Robot%20Interactions&entry.906535625=Houston%20Claure&entry.1292438233=%20%20The%20foundation%20of%20successful%20human%20collaboration%20is%20deeply%20rooted%20in%20the%0Aprinciples%20of%20fairness.%20As%20robots%20are%20increasingly%20prevalent%20in%20various%20parts%0Aof%20society%20where%20they%20are%20working%20alongside%20groups%20and%20teams%20of%20humans%2C%20their%0Aability%20to%20understand%20and%20act%20according%20to%20principles%20of%20fairness%20becomes%0Acrucial%20for%20their%20effective%20integration.%20This%20is%20especially%20critical%20when%0Arobots%20are%20part%20of%20multi-human%20teams%2C%20where%20they%20must%20make%20continuous%20decisions%0Aregarding%20the%20allocation%20of%20resources.%20These%20resources%20can%20be%20material%2C%20such%20as%0Atools%2C%20or%20communicative%2C%20such%20as%20gaze%20direction%2C%20and%20must%20be%20distributed%20fairly%0Aamong%20team%20members%20to%20ensure%20optimal%20team%20performance%20and%20healthy%20group%0Adynamics.%20Therefore%2C%20our%20research%20focuses%20on%20understanding%20how%20robots%20can%0Aeffectively%20participate%20within%20human%20groups%20by%20making%20fair%20decisions%20while%0Acontributing%20positively%20to%20group%20dynamics%20and%20outcomes.%20In%20this%20paper%2C%20I%0Adiscuss%20advances%20toward%20ensuring%20that%20robots%20are%20capable%20of%20considering%20human%0Anotions%20of%20fairness%20in%20their%20decision-making.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21044v1&entry.124074799=Read"},
{"title": "LLMs achieve adult human performance on higher-order theory of mind\n  tasks", "author": "Winnie Street and John Oliver Siy and Geoff Keeling and Adrien Baranes and Benjamin Barnett and Michael McKibben and Tatenda Kanyere and Alison Lentz and Blaise Aguera y Arcas and Robin I. M. Dunbar", "abstract": "  This paper examines the extent to which large language models (LLMs) have\ndeveloped higher-order theory of mind (ToM); the human ability to reason about\nmultiple mental and emotional states in a recursive manner (e.g. I think that\nyou believe that she knows). This paper builds on prior work by introducing a\nhandwritten test suite -- Multi-Order Theory of Mind Q&A -- and using it to\ncompare the performance of five LLMs to a newly gathered adult human benchmark.\nWe find that GPT-4 and Flan-PaLM reach adult-level and near adult-level\nperformance on ToM tasks overall, and that GPT-4 exceeds adult performance on\n6th order inferences. Our results suggest that there is an interplay between\nmodel size and finetuning for the realisation of ToM abilities, and that the\nbest-performing LLMs have developed a generalised capacity for ToM. Given the\nrole that higher-order ToM plays in a wide range of cooperative and competitive\nhuman behaviours, these findings have significant implications for user-facing\nLLM applications.\n", "link": "http://arxiv.org/abs/2405.18870v2", "date": "2024-05-31", "relevancy": 1.4053, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4592}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20achieve%20adult%20human%20performance%20on%20higher-order%20theory%20of%20mind%0A%20%20tasks&body=Title%3A%20LLMs%20achieve%20adult%20human%20performance%20on%20higher-order%20theory%20of%20mind%0A%20%20tasks%0AAuthor%3A%20Winnie%20Street%20and%20John%20Oliver%20Siy%20and%20Geoff%20Keeling%20and%20Adrien%20Baranes%20and%20Benjamin%20Barnett%20and%20Michael%20McKibben%20and%20Tatenda%20Kanyere%20and%20Alison%20Lentz%20and%20Blaise%20Aguera%20y%20Arcas%20and%20Robin%20I.%20M.%20Dunbar%0AAbstract%3A%20%20%20This%20paper%20examines%20the%20extent%20to%20which%20large%20language%20models%20%28LLMs%29%20have%0Adeveloped%20higher-order%20theory%20of%20mind%20%28ToM%29%3B%20the%20human%20ability%20to%20reason%20about%0Amultiple%20mental%20and%20emotional%20states%20in%20a%20recursive%20manner%20%28e.g.%20I%20think%20that%0Ayou%20believe%20that%20she%20knows%29.%20This%20paper%20builds%20on%20prior%20work%20by%20introducing%20a%0Ahandwritten%20test%20suite%20--%20Multi-Order%20Theory%20of%20Mind%20Q%26A%20--%20and%20using%20it%20to%0Acompare%20the%20performance%20of%20five%20LLMs%20to%20a%20newly%20gathered%20adult%20human%20benchmark.%0AWe%20find%20that%20GPT-4%20and%20Flan-PaLM%20reach%20adult-level%20and%20near%20adult-level%0Aperformance%20on%20ToM%20tasks%20overall%2C%20and%20that%20GPT-4%20exceeds%20adult%20performance%20on%0A6th%20order%20inferences.%20Our%20results%20suggest%20that%20there%20is%20an%20interplay%20between%0Amodel%20size%20and%20finetuning%20for%20the%20realisation%20of%20ToM%20abilities%2C%20and%20that%20the%0Abest-performing%20LLMs%20have%20developed%20a%20generalised%20capacity%20for%20ToM.%20Given%20the%0Arole%20that%20higher-order%20ToM%20plays%20in%20a%20wide%20range%20of%20cooperative%20and%20competitive%0Ahuman%20behaviours%2C%20these%20findings%20have%20significant%20implications%20for%20user-facing%0ALLM%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520achieve%2520adult%2520human%2520performance%2520on%2520higher-order%2520theory%2520of%2520mind%250A%2520%2520tasks%26entry.906535625%3DWinnie%2520Street%2520and%2520John%2520Oliver%2520Siy%2520and%2520Geoff%2520Keeling%2520and%2520Adrien%2520Baranes%2520and%2520Benjamin%2520Barnett%2520and%2520Michael%2520McKibben%2520and%2520Tatenda%2520Kanyere%2520and%2520Alison%2520Lentz%2520and%2520Blaise%2520Aguera%2520y%2520Arcas%2520and%2520Robin%2520I.%2520M.%2520Dunbar%26entry.1292438233%3D%2520%2520This%2520paper%2520examines%2520the%2520extent%2520to%2520which%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Adeveloped%2520higher-order%2520theory%2520of%2520mind%2520%2528ToM%2529%253B%2520the%2520human%2520ability%2520to%2520reason%2520about%250Amultiple%2520mental%2520and%2520emotional%2520states%2520in%2520a%2520recursive%2520manner%2520%2528e.g.%2520I%2520think%2520that%250Ayou%2520believe%2520that%2520she%2520knows%2529.%2520This%2520paper%2520builds%2520on%2520prior%2520work%2520by%2520introducing%2520a%250Ahandwritten%2520test%2520suite%2520--%2520Multi-Order%2520Theory%2520of%2520Mind%2520Q%2526A%2520--%2520and%2520using%2520it%2520to%250Acompare%2520the%2520performance%2520of%2520five%2520LLMs%2520to%2520a%2520newly%2520gathered%2520adult%2520human%2520benchmark.%250AWe%2520find%2520that%2520GPT-4%2520and%2520Flan-PaLM%2520reach%2520adult-level%2520and%2520near%2520adult-level%250Aperformance%2520on%2520ToM%2520tasks%2520overall%252C%2520and%2520that%2520GPT-4%2520exceeds%2520adult%2520performance%2520on%250A6th%2520order%2520inferences.%2520Our%2520results%2520suggest%2520that%2520there%2520is%2520an%2520interplay%2520between%250Amodel%2520size%2520and%2520finetuning%2520for%2520the%2520realisation%2520of%2520ToM%2520abilities%252C%2520and%2520that%2520the%250Abest-performing%2520LLMs%2520have%2520developed%2520a%2520generalised%2520capacity%2520for%2520ToM.%2520Given%2520the%250Arole%2520that%2520higher-order%2520ToM%2520plays%2520in%2520a%2520wide%2520range%2520of%2520cooperative%2520and%2520competitive%250Ahuman%2520behaviours%252C%2520these%2520findings%2520have%2520significant%2520implications%2520for%2520user-facing%250ALLM%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20achieve%20adult%20human%20performance%20on%20higher-order%20theory%20of%20mind%0A%20%20tasks&entry.906535625=Winnie%20Street%20and%20John%20Oliver%20Siy%20and%20Geoff%20Keeling%20and%20Adrien%20Baranes%20and%20Benjamin%20Barnett%20and%20Michael%20McKibben%20and%20Tatenda%20Kanyere%20and%20Alison%20Lentz%20and%20Blaise%20Aguera%20y%20Arcas%20and%20Robin%20I.%20M.%20Dunbar&entry.1292438233=%20%20This%20paper%20examines%20the%20extent%20to%20which%20large%20language%20models%20%28LLMs%29%20have%0Adeveloped%20higher-order%20theory%20of%20mind%20%28ToM%29%3B%20the%20human%20ability%20to%20reason%20about%0Amultiple%20mental%20and%20emotional%20states%20in%20a%20recursive%20manner%20%28e.g.%20I%20think%20that%0Ayou%20believe%20that%20she%20knows%29.%20This%20paper%20builds%20on%20prior%20work%20by%20introducing%20a%0Ahandwritten%20test%20suite%20--%20Multi-Order%20Theory%20of%20Mind%20Q%26A%20--%20and%20using%20it%20to%0Acompare%20the%20performance%20of%20five%20LLMs%20to%20a%20newly%20gathered%20adult%20human%20benchmark.%0AWe%20find%20that%20GPT-4%20and%20Flan-PaLM%20reach%20adult-level%20and%20near%20adult-level%0Aperformance%20on%20ToM%20tasks%20overall%2C%20and%20that%20GPT-4%20exceeds%20adult%20performance%20on%0A6th%20order%20inferences.%20Our%20results%20suggest%20that%20there%20is%20an%20interplay%20between%0Amodel%20size%20and%20finetuning%20for%20the%20realisation%20of%20ToM%20abilities%2C%20and%20that%20the%0Abest-performing%20LLMs%20have%20developed%20a%20generalised%20capacity%20for%20ToM.%20Given%20the%0Arole%20that%20higher-order%20ToM%20plays%20in%20a%20wide%20range%20of%20cooperative%20and%20competitive%0Ahuman%20behaviours%2C%20these%20findings%20have%20significant%20implications%20for%20user-facing%0ALLM%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18870v2&entry.124074799=Read"},
{"title": "Concentration Bounds for Optimized Certainty Equivalent Risk Estimation", "author": "Ayon Ghosh and L. A. Prashanth and Krishna Jagannathan", "abstract": "  We consider the problem of estimating the Optimized Certainty Equivalent\n(OCE) risk from independent and identically distributed (i.i.d.) samples. For\nthe classic sample average approximation (SAA) of OCE, we derive mean-squared\nerror as well as concentration bounds (assuming sub-Gaussianity). Further, we\nanalyze an efficient stochastic approximation-based OCE estimator, and derive\nfinite sample bounds for the same. To show the applicability of our bounds, we\nconsider a risk-aware bandit problem, with OCE as the risk. For this problem,\nwe derive bound on the probability of mis-identification. Finally, we conduct\nnumerical experiments to validate the theoretical findings.\n", "link": "http://arxiv.org/abs/2405.20933v1", "date": "2024-05-31", "relevancy": 1.7135, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4599}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4132}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concentration%20Bounds%20for%20Optimized%20Certainty%20Equivalent%20Risk%20Estimation&body=Title%3A%20Concentration%20Bounds%20for%20Optimized%20Certainty%20Equivalent%20Risk%20Estimation%0AAuthor%3A%20Ayon%20Ghosh%20and%20L.%20A.%20Prashanth%20and%20Krishna%20Jagannathan%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20estimating%20the%20Optimized%20Certainty%20Equivalent%0A%28OCE%29%20risk%20from%20independent%20and%20identically%20distributed%20%28i.i.d.%29%20samples.%20For%0Athe%20classic%20sample%20average%20approximation%20%28SAA%29%20of%20OCE%2C%20we%20derive%20mean-squared%0Aerror%20as%20well%20as%20concentration%20bounds%20%28assuming%20sub-Gaussianity%29.%20Further%2C%20we%0Aanalyze%20an%20efficient%20stochastic%20approximation-based%20OCE%20estimator%2C%20and%20derive%0Afinite%20sample%20bounds%20for%20the%20same.%20To%20show%20the%20applicability%20of%20our%20bounds%2C%20we%0Aconsider%20a%20risk-aware%20bandit%20problem%2C%20with%20OCE%20as%20the%20risk.%20For%20this%20problem%2C%0Awe%20derive%20bound%20on%20the%20probability%20of%20mis-identification.%20Finally%2C%20we%20conduct%0Anumerical%20experiments%20to%20validate%20the%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcentration%2520Bounds%2520for%2520Optimized%2520Certainty%2520Equivalent%2520Risk%2520Estimation%26entry.906535625%3DAyon%2520Ghosh%2520and%2520L.%2520A.%2520Prashanth%2520and%2520Krishna%2520Jagannathan%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520estimating%2520the%2520Optimized%2520Certainty%2520Equivalent%250A%2528OCE%2529%2520risk%2520from%2520independent%2520and%2520identically%2520distributed%2520%2528i.i.d.%2529%2520samples.%2520For%250Athe%2520classic%2520sample%2520average%2520approximation%2520%2528SAA%2529%2520of%2520OCE%252C%2520we%2520derive%2520mean-squared%250Aerror%2520as%2520well%2520as%2520concentration%2520bounds%2520%2528assuming%2520sub-Gaussianity%2529.%2520Further%252C%2520we%250Aanalyze%2520an%2520efficient%2520stochastic%2520approximation-based%2520OCE%2520estimator%252C%2520and%2520derive%250Afinite%2520sample%2520bounds%2520for%2520the%2520same.%2520To%2520show%2520the%2520applicability%2520of%2520our%2520bounds%252C%2520we%250Aconsider%2520a%2520risk-aware%2520bandit%2520problem%252C%2520with%2520OCE%2520as%2520the%2520risk.%2520For%2520this%2520problem%252C%250Awe%2520derive%2520bound%2520on%2520the%2520probability%2520of%2520mis-identification.%2520Finally%252C%2520we%2520conduct%250Anumerical%2520experiments%2520to%2520validate%2520the%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concentration%20Bounds%20for%20Optimized%20Certainty%20Equivalent%20Risk%20Estimation&entry.906535625=Ayon%20Ghosh%20and%20L.%20A.%20Prashanth%20and%20Krishna%20Jagannathan&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20estimating%20the%20Optimized%20Certainty%20Equivalent%0A%28OCE%29%20risk%20from%20independent%20and%20identically%20distributed%20%28i.i.d.%29%20samples.%20For%0Athe%20classic%20sample%20average%20approximation%20%28SAA%29%20of%20OCE%2C%20we%20derive%20mean-squared%0Aerror%20as%20well%20as%20concentration%20bounds%20%28assuming%20sub-Gaussianity%29.%20Further%2C%20we%0Aanalyze%20an%20efficient%20stochastic%20approximation-based%20OCE%20estimator%2C%20and%20derive%0Afinite%20sample%20bounds%20for%20the%20same.%20To%20show%20the%20applicability%20of%20our%20bounds%2C%20we%0Aconsider%20a%20risk-aware%20bandit%20problem%2C%20with%20OCE%20as%20the%20risk.%20For%20this%20problem%2C%0Awe%20derive%20bound%20on%20the%20probability%20of%20mis-identification.%20Finally%2C%20we%20conduct%0Anumerical%20experiments%20to%20validate%20the%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20933v1&entry.124074799=Read"},
{"title": "Resampling methods for Private Statistical Inference", "author": "Karan Chadha and John Duchi and Rohith Kuditipudi", "abstract": "  We consider the task of constructing confidence intervals with differential\nprivacy. We propose two private variants of the non-parametric bootstrap, which\nprivately compute the median of the results of multiple \"little\" bootstraps run\non partitions of the data and give asymptotic bounds on the coverage error of\nthe resulting confidence intervals. For a fixed differential privacy parameter\n$\\epsilon$, our methods enjoy the same error rates as that of the non-private\nbootstrap to within logarithmic factors in the sample size $n$. We empirically\nvalidate the performance of our methods for mean estimation, median estimation,\nand logistic regression with both real and synthetic data. Our methods achieve\nsimilar coverage accuracy to existing methods (and non-private baselines) while\nproviding notably shorter ($\\gtrsim 10$ times) confidence intervals than\nprevious approaches.\n", "link": "http://arxiv.org/abs/2402.07131v2", "date": "2024-05-31", "relevancy": 1.5992, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4074}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3964}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resampling%20methods%20for%20Private%20Statistical%20Inference&body=Title%3A%20Resampling%20methods%20for%20Private%20Statistical%20Inference%0AAuthor%3A%20Karan%20Chadha%20and%20John%20Duchi%20and%20Rohith%20Kuditipudi%0AAbstract%3A%20%20%20We%20consider%20the%20task%20of%20constructing%20confidence%20intervals%20with%20differential%0Aprivacy.%20We%20propose%20two%20private%20variants%20of%20the%20non-parametric%20bootstrap%2C%20which%0Aprivately%20compute%20the%20median%20of%20the%20results%20of%20multiple%20%22little%22%20bootstraps%20run%0Aon%20partitions%20of%20the%20data%20and%20give%20asymptotic%20bounds%20on%20the%20coverage%20error%20of%0Athe%20resulting%20confidence%20intervals.%20For%20a%20fixed%20differential%20privacy%20parameter%0A%24%5Cepsilon%24%2C%20our%20methods%20enjoy%20the%20same%20error%20rates%20as%20that%20of%20the%20non-private%0Abootstrap%20to%20within%20logarithmic%20factors%20in%20the%20sample%20size%20%24n%24.%20We%20empirically%0Avalidate%20the%20performance%20of%20our%20methods%20for%20mean%20estimation%2C%20median%20estimation%2C%0Aand%20logistic%20regression%20with%20both%20real%20and%20synthetic%20data.%20Our%20methods%20achieve%0Asimilar%20coverage%20accuracy%20to%20existing%20methods%20%28and%20non-private%20baselines%29%20while%0Aproviding%20notably%20shorter%20%28%24%5Cgtrsim%2010%24%20times%29%20confidence%20intervals%20than%0Aprevious%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResampling%2520methods%2520for%2520Private%2520Statistical%2520Inference%26entry.906535625%3DKaran%2520Chadha%2520and%2520John%2520Duchi%2520and%2520Rohith%2520Kuditipudi%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520task%2520of%2520constructing%2520confidence%2520intervals%2520with%2520differential%250Aprivacy.%2520We%2520propose%2520two%2520private%2520variants%2520of%2520the%2520non-parametric%2520bootstrap%252C%2520which%250Aprivately%2520compute%2520the%2520median%2520of%2520the%2520results%2520of%2520multiple%2520%2522little%2522%2520bootstraps%2520run%250Aon%2520partitions%2520of%2520the%2520data%2520and%2520give%2520asymptotic%2520bounds%2520on%2520the%2520coverage%2520error%2520of%250Athe%2520resulting%2520confidence%2520intervals.%2520For%2520a%2520fixed%2520differential%2520privacy%2520parameter%250A%2524%255Cepsilon%2524%252C%2520our%2520methods%2520enjoy%2520the%2520same%2520error%2520rates%2520as%2520that%2520of%2520the%2520non-private%250Abootstrap%2520to%2520within%2520logarithmic%2520factors%2520in%2520the%2520sample%2520size%2520%2524n%2524.%2520We%2520empirically%250Avalidate%2520the%2520performance%2520of%2520our%2520methods%2520for%2520mean%2520estimation%252C%2520median%2520estimation%252C%250Aand%2520logistic%2520regression%2520with%2520both%2520real%2520and%2520synthetic%2520data.%2520Our%2520methods%2520achieve%250Asimilar%2520coverage%2520accuracy%2520to%2520existing%2520methods%2520%2528and%2520non-private%2520baselines%2529%2520while%250Aproviding%2520notably%2520shorter%2520%2528%2524%255Cgtrsim%252010%2524%2520times%2529%2520confidence%2520intervals%2520than%250Aprevious%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resampling%20methods%20for%20Private%20Statistical%20Inference&entry.906535625=Karan%20Chadha%20and%20John%20Duchi%20and%20Rohith%20Kuditipudi&entry.1292438233=%20%20We%20consider%20the%20task%20of%20constructing%20confidence%20intervals%20with%20differential%0Aprivacy.%20We%20propose%20two%20private%20variants%20of%20the%20non-parametric%20bootstrap%2C%20which%0Aprivately%20compute%20the%20median%20of%20the%20results%20of%20multiple%20%22little%22%20bootstraps%20run%0Aon%20partitions%20of%20the%20data%20and%20give%20asymptotic%20bounds%20on%20the%20coverage%20error%20of%0Athe%20resulting%20confidence%20intervals.%20For%20a%20fixed%20differential%20privacy%20parameter%0A%24%5Cepsilon%24%2C%20our%20methods%20enjoy%20the%20same%20error%20rates%20as%20that%20of%20the%20non-private%0Abootstrap%20to%20within%20logarithmic%20factors%20in%20the%20sample%20size%20%24n%24.%20We%20empirically%0Avalidate%20the%20performance%20of%20our%20methods%20for%20mean%20estimation%2C%20median%20estimation%2C%0Aand%20logistic%20regression%20with%20both%20real%20and%20synthetic%20data.%20Our%20methods%20achieve%0Asimilar%20coverage%20accuracy%20to%20existing%20methods%20%28and%20non-private%20baselines%29%20while%0Aproviding%20notably%20shorter%20%28%24%5Cgtrsim%2010%24%20times%29%20confidence%20intervals%20than%0Aprevious%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07131v2&entry.124074799=Read"},
{"title": "LCQ: Low-Rank Codebook based Quantization for Large Language Models", "author": "Wen-Pu Cai and Wu-Jun Li", "abstract": "  Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost.\n", "link": "http://arxiv.org/abs/2405.20973v1", "date": "2024-05-31", "relevancy": 1.7603, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4442}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4389}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LCQ%3A%20Low-Rank%20Codebook%20based%20Quantization%20for%20Large%20Language%20Models&body=Title%3A%20LCQ%3A%20Low-Rank%20Codebook%20based%20Quantization%20for%20Large%20Language%20Models%0AAuthor%3A%20Wen-Pu%20Cai%20and%20Wu-Jun%20Li%0AAbstract%3A%20%20%20Large%20language%20models~%28LLMs%29%20have%20recently%20demonstrated%20promising%20performance%0Ain%20many%20tasks.%20However%2C%20the%20high%20storage%20and%20computational%20cost%20of%20LLMs%20has%0Abecome%20a%20challenge%20for%20deploying%20LLMs.%20Weight%20quantization%20has%20been%20widely%20used%0Afor%20model%20compression%2C%20which%20can%20reduce%20both%20storage%20and%20computational%20cost.%0AMost%20existing%20weight%20quantization%20methods%20for%20LLMs%20use%20a%20rank-one%20codebook%20for%0Aquantization%2C%20which%20results%20in%20substantial%20accuracy%20loss%20when%20the%20compression%0Aratio%20is%20high.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20weight%20quantization%20method%2C%0Acalled%20low-rank%20codebook%20based%20quantization~%28LCQ%29%2C%20for%20LLMs.%20LCQ%20adopts%20a%0Alow-rank%20codebook%2C%20the%20rank%20of%20which%20can%20be%20larger%20than%20one%2C%20for%20quantization.%0AExperiments%20show%20that%20LCQ%20can%20achieve%20better%20accuracy%20than%20existing%20methods%0Awith%20a%20negligibly%20extra%20storage%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLCQ%253A%2520Low-Rank%2520Codebook%2520based%2520Quantization%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DWen-Pu%2520Cai%2520and%2520Wu-Jun%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models~%2528LLMs%2529%2520have%2520recently%2520demonstrated%2520promising%2520performance%250Ain%2520many%2520tasks.%2520However%252C%2520the%2520high%2520storage%2520and%2520computational%2520cost%2520of%2520LLMs%2520has%250Abecome%2520a%2520challenge%2520for%2520deploying%2520LLMs.%2520Weight%2520quantization%2520has%2520been%2520widely%2520used%250Afor%2520model%2520compression%252C%2520which%2520can%2520reduce%2520both%2520storage%2520and%2520computational%2520cost.%250AMost%2520existing%2520weight%2520quantization%2520methods%2520for%2520LLMs%2520use%2520a%2520rank-one%2520codebook%2520for%250Aquantization%252C%2520which%2520results%2520in%2520substantial%2520accuracy%2520loss%2520when%2520the%2520compression%250Aratio%2520is%2520high.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520weight%2520quantization%2520method%252C%250Acalled%2520low-rank%2520codebook%2520based%2520quantization~%2528LCQ%2529%252C%2520for%2520LLMs.%2520LCQ%2520adopts%2520a%250Alow-rank%2520codebook%252C%2520the%2520rank%2520of%2520which%2520can%2520be%2520larger%2520than%2520one%252C%2520for%2520quantization.%250AExperiments%2520show%2520that%2520LCQ%2520can%2520achieve%2520better%2520accuracy%2520than%2520existing%2520methods%250Awith%2520a%2520negligibly%2520extra%2520storage%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LCQ%3A%20Low-Rank%20Codebook%20based%20Quantization%20for%20Large%20Language%20Models&entry.906535625=Wen-Pu%20Cai%20and%20Wu-Jun%20Li&entry.1292438233=%20%20Large%20language%20models~%28LLMs%29%20have%20recently%20demonstrated%20promising%20performance%0Ain%20many%20tasks.%20However%2C%20the%20high%20storage%20and%20computational%20cost%20of%20LLMs%20has%0Abecome%20a%20challenge%20for%20deploying%20LLMs.%20Weight%20quantization%20has%20been%20widely%20used%0Afor%20model%20compression%2C%20which%20can%20reduce%20both%20storage%20and%20computational%20cost.%0AMost%20existing%20weight%20quantization%20methods%20for%20LLMs%20use%20a%20rank-one%20codebook%20for%0Aquantization%2C%20which%20results%20in%20substantial%20accuracy%20loss%20when%20the%20compression%0Aratio%20is%20high.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20weight%20quantization%20method%2C%0Acalled%20low-rank%20codebook%20based%20quantization~%28LCQ%29%2C%20for%20LLMs.%20LCQ%20adopts%20a%0Alow-rank%20codebook%2C%20the%20rank%20of%20which%20can%20be%20larger%20than%20one%2C%20for%20quantization.%0AExperiments%20show%20that%20LCQ%20can%20achieve%20better%20accuracy%20than%20existing%20methods%0Awith%20a%20negligibly%20extra%20storage%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20973v1&entry.124074799=Read"},
{"title": "Language Augmentation in CLIP for Improved Anatomy Detection on\n  Multi-modal Medical Images", "author": "Mansi Kakkar and Dattesh Shanbhag and Chandan Aladahalli and Gurunath Reddy M", "abstract": "  Vision-language models have emerged as a powerful tool for previously\nchallenging multi-modal classification problem in the medical domain. This\ndevelopment has led to the exploration of automated image description\ngeneration for multi-modal clinical scans, particularly for radiology report\ngeneration. Existing research has focused on clinical descriptions for specific\nmodalities or body regions, leaving a gap for a model providing entire-body\nmulti-modal descriptions. In this paper, we address this gap by automating the\ngeneration of standardized body station(s) and list of organ(s) across the\nwhole body in multi-modal MR and CT radiological images. Leveraging the\nversatility of the Contrastive Language-Image Pre-training (CLIP), we refine\nand augment the existing approach through multiple experiments, including\nbaseline model fine-tuning, adding station(s) as a superset for better\ncorrelation between organs, along with image and language augmentations. Our\nproposed approach demonstrates 47.6% performance improvement over baseline\nPubMedCLIP.\n", "link": "http://arxiv.org/abs/2405.20735v1", "date": "2024-05-31", "relevancy": 1.7261, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6256}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5198}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Augmentation%20in%20CLIP%20for%20Improved%20Anatomy%20Detection%20on%0A%20%20Multi-modal%20Medical%20Images&body=Title%3A%20Language%20Augmentation%20in%20CLIP%20for%20Improved%20Anatomy%20Detection%20on%0A%20%20Multi-modal%20Medical%20Images%0AAuthor%3A%20Mansi%20Kakkar%20and%20Dattesh%20Shanbhag%20and%20Chandan%20Aladahalli%20and%20Gurunath%20Reddy%20M%0AAbstract%3A%20%20%20Vision-language%20models%20have%20emerged%20as%20a%20powerful%20tool%20for%20previously%0Achallenging%20multi-modal%20classification%20problem%20in%20the%20medical%20domain.%20This%0Adevelopment%20has%20led%20to%20the%20exploration%20of%20automated%20image%20description%0Ageneration%20for%20multi-modal%20clinical%20scans%2C%20particularly%20for%20radiology%20report%0Ageneration.%20Existing%20research%20has%20focused%20on%20clinical%20descriptions%20for%20specific%0Amodalities%20or%20body%20regions%2C%20leaving%20a%20gap%20for%20a%20model%20providing%20entire-body%0Amulti-modal%20descriptions.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20automating%20the%0Ageneration%20of%20standardized%20body%20station%28s%29%20and%20list%20of%20organ%28s%29%20across%20the%0Awhole%20body%20in%20multi-modal%20MR%20and%20CT%20radiological%20images.%20Leveraging%20the%0Aversatility%20of%20the%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%2C%20we%20refine%0Aand%20augment%20the%20existing%20approach%20through%20multiple%20experiments%2C%20including%0Abaseline%20model%20fine-tuning%2C%20adding%20station%28s%29%20as%20a%20superset%20for%20better%0Acorrelation%20between%20organs%2C%20along%20with%20image%20and%20language%20augmentations.%20Our%0Aproposed%20approach%20demonstrates%2047.6%25%20performance%20improvement%20over%20baseline%0APubMedCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Augmentation%2520in%2520CLIP%2520for%2520Improved%2520Anatomy%2520Detection%2520on%250A%2520%2520Multi-modal%2520Medical%2520Images%26entry.906535625%3DMansi%2520Kakkar%2520and%2520Dattesh%2520Shanbhag%2520and%2520Chandan%2520Aladahalli%2520and%2520Gurunath%2520Reddy%2520M%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520previously%250Achallenging%2520multi-modal%2520classification%2520problem%2520in%2520the%2520medical%2520domain.%2520This%250Adevelopment%2520has%2520led%2520to%2520the%2520exploration%2520of%2520automated%2520image%2520description%250Ageneration%2520for%2520multi-modal%2520clinical%2520scans%252C%2520particularly%2520for%2520radiology%2520report%250Ageneration.%2520Existing%2520research%2520has%2520focused%2520on%2520clinical%2520descriptions%2520for%2520specific%250Amodalities%2520or%2520body%2520regions%252C%2520leaving%2520a%2520gap%2520for%2520a%2520model%2520providing%2520entire-body%250Amulti-modal%2520descriptions.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520automating%2520the%250Ageneration%2520of%2520standardized%2520body%2520station%2528s%2529%2520and%2520list%2520of%2520organ%2528s%2529%2520across%2520the%250Awhole%2520body%2520in%2520multi-modal%2520MR%2520and%2520CT%2520radiological%2520images.%2520Leveraging%2520the%250Aversatility%2520of%2520the%2520Contrastive%2520Language-Image%2520Pre-training%2520%2528CLIP%2529%252C%2520we%2520refine%250Aand%2520augment%2520the%2520existing%2520approach%2520through%2520multiple%2520experiments%252C%2520including%250Abaseline%2520model%2520fine-tuning%252C%2520adding%2520station%2528s%2529%2520as%2520a%2520superset%2520for%2520better%250Acorrelation%2520between%2520organs%252C%2520along%2520with%2520image%2520and%2520language%2520augmentations.%2520Our%250Aproposed%2520approach%2520demonstrates%252047.6%2525%2520performance%2520improvement%2520over%2520baseline%250APubMedCLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Augmentation%20in%20CLIP%20for%20Improved%20Anatomy%20Detection%20on%0A%20%20Multi-modal%20Medical%20Images&entry.906535625=Mansi%20Kakkar%20and%20Dattesh%20Shanbhag%20and%20Chandan%20Aladahalli%20and%20Gurunath%20Reddy%20M&entry.1292438233=%20%20Vision-language%20models%20have%20emerged%20as%20a%20powerful%20tool%20for%20previously%0Achallenging%20multi-modal%20classification%20problem%20in%20the%20medical%20domain.%20This%0Adevelopment%20has%20led%20to%20the%20exploration%20of%20automated%20image%20description%0Ageneration%20for%20multi-modal%20clinical%20scans%2C%20particularly%20for%20radiology%20report%0Ageneration.%20Existing%20research%20has%20focused%20on%20clinical%20descriptions%20for%20specific%0Amodalities%20or%20body%20regions%2C%20leaving%20a%20gap%20for%20a%20model%20providing%20entire-body%0Amulti-modal%20descriptions.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20automating%20the%0Ageneration%20of%20standardized%20body%20station%28s%29%20and%20list%20of%20organ%28s%29%20across%20the%0Awhole%20body%20in%20multi-modal%20MR%20and%20CT%20radiological%20images.%20Leveraging%20the%0Aversatility%20of%20the%20Contrastive%20Language-Image%20Pre-training%20%28CLIP%29%2C%20we%20refine%0Aand%20augment%20the%20existing%20approach%20through%20multiple%20experiments%2C%20including%0Abaseline%20model%20fine-tuning%2C%20adding%20station%28s%29%20as%20a%20superset%20for%20better%0Acorrelation%20between%20organs%2C%20along%20with%20image%20and%20language%20augmentations.%20Our%0Aproposed%20approach%20demonstrates%2047.6%25%20performance%20improvement%20over%20baseline%0APubMedCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20735v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


