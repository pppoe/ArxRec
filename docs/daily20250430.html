<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250429.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "TesserAct: Learning 4D Embodied World Models", "author": "Haoyu Zhen and Qiao Sun and Hongxin Zhang and Junyan Li and Siyuan Zhou and Yilun Du and Chuang Gan", "abstract": "  This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.\n", "link": "http://arxiv.org/abs/2504.20995v1", "date": "2025-04-29", "relevancy": 3.1804, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TesserAct%3A%20Learning%204D%20Embodied%20World%20Models&body=Title%3A%20TesserAct%3A%20Learning%204D%20Embodied%20World%20Models%0AAuthor%3A%20Haoyu%20Zhen%20and%20Qiao%20Sun%20and%20Hongxin%20Zhang%20and%20Junyan%20Li%20and%20Siyuan%20Zhou%20and%20Yilun%20Du%20and%20Chuang%20Gan%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20effective%20approach%20for%20learning%20novel%204D%20embodied%0Aworld%20models%2C%20which%20predict%20the%20dynamic%20evolution%20of%203D%20scenes%20over%20time%20in%0Aresponse%20to%20an%20embodied%20agent%27s%20actions%2C%20providing%20both%20spatial%20and%20temporal%0Aconsistency.%20We%20propose%20to%20learn%20a%204D%20world%20model%20by%20training%20on%20RGB-DN%20%28RGB%2C%0ADepth%2C%20and%20Normal%29%20videos.%20This%20not%20only%20surpasses%20traditional%202D%20models%20by%0Aincorporating%20detailed%20shape%2C%20configuration%2C%20and%20temporal%20changes%20into%20their%0Apredictions%2C%20but%20also%20allows%20us%20to%20effectively%20learn%20accurate%20inverse%20dynamic%0Amodels%20for%20an%20embodied%20agent.%20Specifically%2C%20we%20first%20extend%20existing%20robotic%0Amanipulation%20video%20datasets%20with%20depth%20and%20normal%20information%20leveraging%0Aoff-the-shelf%20models.%20Next%2C%20we%20fine-tune%20a%20video%20generation%20model%20on%20this%0Aannotated%20dataset%2C%20which%20jointly%20predicts%20RGB-DN%20%28RGB%2C%20Depth%2C%20and%20Normal%29%20for%0Aeach%20frame.%20We%20then%20present%20an%20algorithm%20to%20directly%20convert%20generated%20RGB%2C%0ADepth%2C%20and%20Normal%20videos%20into%20a%20high-quality%204D%20scene%20of%20the%20world.%20Our%20method%0Aensures%20temporal%20and%20spatial%20coherence%20in%204D%20scene%20predictions%20from%20embodied%0Ascenarios%2C%20enables%20novel%20view%20synthesis%20for%20embodied%20environments%2C%20and%0Afacilitates%20policy%20learning%20that%20significantly%20outperforms%20those%20derived%20from%0Aprior%20video-based%20world%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTesserAct%253A%2520Learning%25204D%2520Embodied%2520World%2520Models%26entry.906535625%3DHaoyu%2520Zhen%2520and%2520Qiao%2520Sun%2520and%2520Hongxin%2520Zhang%2520and%2520Junyan%2520Li%2520and%2520Siyuan%2520Zhou%2520and%2520Yilun%2520Du%2520and%2520Chuang%2520Gan%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520effective%2520approach%2520for%2520learning%2520novel%25204D%2520embodied%250Aworld%2520models%252C%2520which%2520predict%2520the%2520dynamic%2520evolution%2520of%25203D%2520scenes%2520over%2520time%2520in%250Aresponse%2520to%2520an%2520embodied%2520agent%2527s%2520actions%252C%2520providing%2520both%2520spatial%2520and%2520temporal%250Aconsistency.%2520We%2520propose%2520to%2520learn%2520a%25204D%2520world%2520model%2520by%2520training%2520on%2520RGB-DN%2520%2528RGB%252C%250ADepth%252C%2520and%2520Normal%2529%2520videos.%2520This%2520not%2520only%2520surpasses%2520traditional%25202D%2520models%2520by%250Aincorporating%2520detailed%2520shape%252C%2520configuration%252C%2520and%2520temporal%2520changes%2520into%2520their%250Apredictions%252C%2520but%2520also%2520allows%2520us%2520to%2520effectively%2520learn%2520accurate%2520inverse%2520dynamic%250Amodels%2520for%2520an%2520embodied%2520agent.%2520Specifically%252C%2520we%2520first%2520extend%2520existing%2520robotic%250Amanipulation%2520video%2520datasets%2520with%2520depth%2520and%2520normal%2520information%2520leveraging%250Aoff-the-shelf%2520models.%2520Next%252C%2520we%2520fine-tune%2520a%2520video%2520generation%2520model%2520on%2520this%250Aannotated%2520dataset%252C%2520which%2520jointly%2520predicts%2520RGB-DN%2520%2528RGB%252C%2520Depth%252C%2520and%2520Normal%2529%2520for%250Aeach%2520frame.%2520We%2520then%2520present%2520an%2520algorithm%2520to%2520directly%2520convert%2520generated%2520RGB%252C%250ADepth%252C%2520and%2520Normal%2520videos%2520into%2520a%2520high-quality%25204D%2520scene%2520of%2520the%2520world.%2520Our%2520method%250Aensures%2520temporal%2520and%2520spatial%2520coherence%2520in%25204D%2520scene%2520predictions%2520from%2520embodied%250Ascenarios%252C%2520enables%2520novel%2520view%2520synthesis%2520for%2520embodied%2520environments%252C%2520and%250Afacilitates%2520policy%2520learning%2520that%2520significantly%2520outperforms%2520those%2520derived%2520from%250Aprior%2520video-based%2520world%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TesserAct%3A%20Learning%204D%20Embodied%20World%20Models&entry.906535625=Haoyu%20Zhen%20and%20Qiao%20Sun%20and%20Hongxin%20Zhang%20and%20Junyan%20Li%20and%20Siyuan%20Zhou%20and%20Yilun%20Du%20and%20Chuang%20Gan&entry.1292438233=%20%20This%20paper%20presents%20an%20effective%20approach%20for%20learning%20novel%204D%20embodied%0Aworld%20models%2C%20which%20predict%20the%20dynamic%20evolution%20of%203D%20scenes%20over%20time%20in%0Aresponse%20to%20an%20embodied%20agent%27s%20actions%2C%20providing%20both%20spatial%20and%20temporal%0Aconsistency.%20We%20propose%20to%20learn%20a%204D%20world%20model%20by%20training%20on%20RGB-DN%20%28RGB%2C%0ADepth%2C%20and%20Normal%29%20videos.%20This%20not%20only%20surpasses%20traditional%202D%20models%20by%0Aincorporating%20detailed%20shape%2C%20configuration%2C%20and%20temporal%20changes%20into%20their%0Apredictions%2C%20but%20also%20allows%20us%20to%20effectively%20learn%20accurate%20inverse%20dynamic%0Amodels%20for%20an%20embodied%20agent.%20Specifically%2C%20we%20first%20extend%20existing%20robotic%0Amanipulation%20video%20datasets%20with%20depth%20and%20normal%20information%20leveraging%0Aoff-the-shelf%20models.%20Next%2C%20we%20fine-tune%20a%20video%20generation%20model%20on%20this%0Aannotated%20dataset%2C%20which%20jointly%20predicts%20RGB-DN%20%28RGB%2C%20Depth%2C%20and%20Normal%29%20for%0Aeach%20frame.%20We%20then%20present%20an%20algorithm%20to%20directly%20convert%20generated%20RGB%2C%0ADepth%2C%20and%20Normal%20videos%20into%20a%20high-quality%204D%20scene%20of%20the%20world.%20Our%20method%0Aensures%20temporal%20and%20spatial%20coherence%20in%204D%20scene%20predictions%20from%20embodied%0Ascenarios%2C%20enables%20novel%20view%20synthesis%20for%20embodied%20environments%2C%20and%0Afacilitates%20policy%20learning%20that%20significantly%20outperforms%20those%20derived%20from%0Aprior%20video-based%20world%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20995v1&entry.124074799=Read"},
{"title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "author": "Yongqian Peng and Yuxi Ma and Mengmeng Wang and Yuxuan Wang and Yizhou Wang and Chi Zhang and Yixin Zhu and Zilong Zheng", "abstract": "  The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs' outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.\n", "link": "http://arxiv.org/abs/2504.13120v2", "date": "2025-04-29", "relevancy": 3.0432, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6264}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probing%20and%20Inducing%20Combinational%20Creativity%20in%20Vision-Language%20Models&body=Title%3A%20Probing%20and%20Inducing%20Combinational%20Creativity%20in%20Vision-Language%20Models%0AAuthor%3A%20Yongqian%20Peng%20and%20Yuxi%20Ma%20and%20Mengmeng%20Wang%20and%20Yuxuan%20Wang%20and%20Yizhou%20Wang%20and%20Chi%20Zhang%20and%20Yixin%20Zhu%20and%20Zilong%20Zheng%0AAbstract%3A%20%20%20The%20ability%20to%20combine%20existing%20concepts%20into%20novel%20ideas%20stands%20as%20a%0Afundamental%20hallmark%20of%20human%20intelligence.%20Recent%20advances%20in%20Vision-Language%0AModels%20%28VLMs%29%20like%20GPT-4V%20and%20DALLE-3%20have%20sparked%20debate%20about%20whether%20their%0Aoutputs%20reflect%20combinational%20creativity--defined%20by%20M.%20A.%20Boden%20%281998%29%20as%0Asynthesizing%20novel%20ideas%20through%20combining%20existing%20concepts--or%20sophisticated%0Apattern%20matching%20of%20training%20data.%20Drawing%20inspiration%20from%20cognitive%20science%2C%0Awe%20investigate%20the%20combinational%20creativity%20of%20VLMs%20from%20the%20lens%20of%20concept%0Ablending.%20We%20propose%20the%20Identification-Explanation-Implication%20%28IEI%29%0Aframework%2C%20which%20decomposes%20creative%20processes%20into%20three%20levels%3A%20identifying%0Ainput%20spaces%2C%20extracting%20shared%20attributes%2C%20and%20deriving%20novel%20semantic%0Aimplications.%20To%20validate%20this%20framework%2C%20we%20curate%20CreativeMashup%2C%20a%0Ahigh-quality%20dataset%20of%20666%20artist-generated%20visual%20mashups%20annotated%20according%0Ato%20the%20IEI%20framework.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20in%0Acomprehension%20tasks%2C%20best%20VLMs%20have%20surpassed%20average%20human%20performance%20while%0Afalling%20short%20of%20expert-level%20understanding%3B%20in%20generation%20tasks%2C%20incorporating%0Aour%20IEI%20framework%20into%20the%20generation%20pipeline%20significantly%20enhances%20the%0Acreative%20quality%20of%20VLMs%27%20outputs.%20Our%20findings%20establish%20both%20a%20theoretical%0Afoundation%20for%20evaluating%20artificial%20creativity%20and%20practical%20guidelines%20for%0Aimproving%20creative%20generation%20in%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13120v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbing%2520and%2520Inducing%2520Combinational%2520Creativity%2520in%2520Vision-Language%2520Models%26entry.906535625%3DYongqian%2520Peng%2520and%2520Yuxi%2520Ma%2520and%2520Mengmeng%2520Wang%2520and%2520Yuxuan%2520Wang%2520and%2520Yizhou%2520Wang%2520and%2520Chi%2520Zhang%2520and%2520Yixin%2520Zhu%2520and%2520Zilong%2520Zheng%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520combine%2520existing%2520concepts%2520into%2520novel%2520ideas%2520stands%2520as%2520a%250Afundamental%2520hallmark%2520of%2520human%2520intelligence.%2520Recent%2520advances%2520in%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520like%2520GPT-4V%2520and%2520DALLE-3%2520have%2520sparked%2520debate%2520about%2520whether%2520their%250Aoutputs%2520reflect%2520combinational%2520creativity--defined%2520by%2520M.%2520A.%2520Boden%2520%25281998%2529%2520as%250Asynthesizing%2520novel%2520ideas%2520through%2520combining%2520existing%2520concepts--or%2520sophisticated%250Apattern%2520matching%2520of%2520training%2520data.%2520Drawing%2520inspiration%2520from%2520cognitive%2520science%252C%250Awe%2520investigate%2520the%2520combinational%2520creativity%2520of%2520VLMs%2520from%2520the%2520lens%2520of%2520concept%250Ablending.%2520We%2520propose%2520the%2520Identification-Explanation-Implication%2520%2528IEI%2529%250Aframework%252C%2520which%2520decomposes%2520creative%2520processes%2520into%2520three%2520levels%253A%2520identifying%250Ainput%2520spaces%252C%2520extracting%2520shared%2520attributes%252C%2520and%2520deriving%2520novel%2520semantic%250Aimplications.%2520To%2520validate%2520this%2520framework%252C%2520we%2520curate%2520CreativeMashup%252C%2520a%250Ahigh-quality%2520dataset%2520of%2520666%2520artist-generated%2520visual%2520mashups%2520annotated%2520according%250Ato%2520the%2520IEI%2520framework.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520in%250Acomprehension%2520tasks%252C%2520best%2520VLMs%2520have%2520surpassed%2520average%2520human%2520performance%2520while%250Afalling%2520short%2520of%2520expert-level%2520understanding%253B%2520in%2520generation%2520tasks%252C%2520incorporating%250Aour%2520IEI%2520framework%2520into%2520the%2520generation%2520pipeline%2520significantly%2520enhances%2520the%250Acreative%2520quality%2520of%2520VLMs%2527%2520outputs.%2520Our%2520findings%2520establish%2520both%2520a%2520theoretical%250Afoundation%2520for%2520evaluating%2520artificial%2520creativity%2520and%2520practical%2520guidelines%2520for%250Aimproving%2520creative%2520generation%2520in%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13120v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probing%20and%20Inducing%20Combinational%20Creativity%20in%20Vision-Language%20Models&entry.906535625=Yongqian%20Peng%20and%20Yuxi%20Ma%20and%20Mengmeng%20Wang%20and%20Yuxuan%20Wang%20and%20Yizhou%20Wang%20and%20Chi%20Zhang%20and%20Yixin%20Zhu%20and%20Zilong%20Zheng&entry.1292438233=%20%20The%20ability%20to%20combine%20existing%20concepts%20into%20novel%20ideas%20stands%20as%20a%0Afundamental%20hallmark%20of%20human%20intelligence.%20Recent%20advances%20in%20Vision-Language%0AModels%20%28VLMs%29%20like%20GPT-4V%20and%20DALLE-3%20have%20sparked%20debate%20about%20whether%20their%0Aoutputs%20reflect%20combinational%20creativity--defined%20by%20M.%20A.%20Boden%20%281998%29%20as%0Asynthesizing%20novel%20ideas%20through%20combining%20existing%20concepts--or%20sophisticated%0Apattern%20matching%20of%20training%20data.%20Drawing%20inspiration%20from%20cognitive%20science%2C%0Awe%20investigate%20the%20combinational%20creativity%20of%20VLMs%20from%20the%20lens%20of%20concept%0Ablending.%20We%20propose%20the%20Identification-Explanation-Implication%20%28IEI%29%0Aframework%2C%20which%20decomposes%20creative%20processes%20into%20three%20levels%3A%20identifying%0Ainput%20spaces%2C%20extracting%20shared%20attributes%2C%20and%20deriving%20novel%20semantic%0Aimplications.%20To%20validate%20this%20framework%2C%20we%20curate%20CreativeMashup%2C%20a%0Ahigh-quality%20dataset%20of%20666%20artist-generated%20visual%20mashups%20annotated%20according%0Ato%20the%20IEI%20framework.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20in%0Acomprehension%20tasks%2C%20best%20VLMs%20have%20surpassed%20average%20human%20performance%20while%0Afalling%20short%20of%20expert-level%20understanding%3B%20in%20generation%20tasks%2C%20incorporating%0Aour%20IEI%20framework%20into%20the%20generation%20pipeline%20significantly%20enhances%20the%0Acreative%20quality%20of%20VLMs%27%20outputs.%20Our%20findings%20establish%20both%20a%20theoretical%0Afoundation%20for%20evaluating%20artificial%20creativity%20and%20practical%20guidelines%20for%0Aimproving%20creative%20generation%20in%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13120v2&entry.124074799=Read"},
{"title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for\n  Targeted Scene Confusion", "author": "Jiaxin Hong and Sixu Chen and Shuoyang Sun and Hongyao Yu and Hao Fang and Yuqi Tan and Bin Chen and Shuhan Qi and Jiawei Li", "abstract": "  As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene\nrepresentation and novel view synthesis, its rapid adoption in safety-critical\ndomains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of\npotential security vulnerabilities. This paper presents the first systematic\nstudy of backdoor threats in 3DGS pipelines. We identify that adversaries may\nimplant backdoor views to induce malicious scene confusion during inference,\npotentially leading to environmental misperception in autonomous navigation or\nspatial distortion in immersive environments. To uncover this risk, we propose\nGuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap\ninjects malicious views at specific attack viewpoints while preserving\nhigh-quality rendering in non-target views, ensuring minimal detectability and\nmaximizing potential harm. Specifically, the proposed method consists of a\nthree-stage pipeline (attack, stabilization, and normal training) to implant\nstealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing\nattack efficacy and perceptual realism to expose security risks in 3D\nrendering. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that GuassTrap can effectively embed imperceptible yet harmful\nbackdoor views while maintaining high-quality rendering in normal views,\nvalidating its robustness, adaptability, and practical applicability.\n", "link": "http://arxiv.org/abs/2504.20829v1", "date": "2025-04-29", "relevancy": 3.0056, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.632}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5882}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussTrap%3A%20Stealthy%20Poisoning%20Attacks%20on%203D%20Gaussian%20Splatting%20for%0A%20%20Targeted%20Scene%20Confusion&body=Title%3A%20GaussTrap%3A%20Stealthy%20Poisoning%20Attacks%20on%203D%20Gaussian%20Splatting%20for%0A%20%20Targeted%20Scene%20Confusion%0AAuthor%3A%20Jiaxin%20Hong%20and%20Sixu%20Chen%20and%20Shuoyang%20Sun%20and%20Hongyao%20Yu%20and%20Hao%20Fang%20and%20Yuqi%20Tan%20and%20Bin%20Chen%20and%20Shuhan%20Qi%20and%20Jiawei%20Li%0AAbstract%3A%20%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20emerges%20as%20a%20breakthrough%20in%20scene%0Arepresentation%20and%20novel%20view%20synthesis%2C%20its%20rapid%20adoption%20in%20safety-critical%0Adomains%20%28e.g.%2C%20autonomous%20systems%2C%20AR/VR%29%20urgently%20demands%20scrutiny%20of%0Apotential%20security%20vulnerabilities.%20This%20paper%20presents%20the%20first%20systematic%0Astudy%20of%20backdoor%20threats%20in%203DGS%20pipelines.%20We%20identify%20that%20adversaries%20may%0Aimplant%20backdoor%20views%20to%20induce%20malicious%20scene%20confusion%20during%20inference%2C%0Apotentially%20leading%20to%20environmental%20misperception%20in%20autonomous%20navigation%20or%0Aspatial%20distortion%20in%20immersive%20environments.%20To%20uncover%20this%20risk%2C%20we%20propose%0AGuassTrap%2C%20a%20novel%20poisoning%20attack%20method%20targeting%203DGS%20models.%20GuassTrap%0Ainjects%20malicious%20views%20at%20specific%20attack%20viewpoints%20while%20preserving%0Ahigh-quality%20rendering%20in%20non-target%20views%2C%20ensuring%20minimal%20detectability%20and%0Amaximizing%20potential%20harm.%20Specifically%2C%20the%20proposed%20method%20consists%20of%20a%0Athree-stage%20pipeline%20%28attack%2C%20stabilization%2C%20and%20normal%20training%29%20to%20implant%0Astealthy%2C%20viewpoint-consistent%20poisoned%20renderings%20in%203DGS%2C%20jointly%20optimizing%0Aattack%20efficacy%20and%20perceptual%20realism%20to%20expose%20security%20risks%20in%203D%0Arendering.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20that%20GuassTrap%20can%20effectively%20embed%20imperceptible%20yet%20harmful%0Abackdoor%20views%20while%20maintaining%20high-quality%20rendering%20in%20normal%20views%2C%0Avalidating%20its%20robustness%2C%20adaptability%2C%20and%20practical%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussTrap%253A%2520Stealthy%2520Poisoning%2520Attacks%2520on%25203D%2520Gaussian%2520Splatting%2520for%250A%2520%2520Targeted%2520Scene%2520Confusion%26entry.906535625%3DJiaxin%2520Hong%2520and%2520Sixu%2520Chen%2520and%2520Shuoyang%2520Sun%2520and%2520Hongyao%2520Yu%2520and%2520Hao%2520Fang%2520and%2520Yuqi%2520Tan%2520and%2520Bin%2520Chen%2520and%2520Shuhan%2520Qi%2520and%2520Jiawei%2520Li%26entry.1292438233%3D%2520%2520As%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520emerges%2520as%2520a%2520breakthrough%2520in%2520scene%250Arepresentation%2520and%2520novel%2520view%2520synthesis%252C%2520its%2520rapid%2520adoption%2520in%2520safety-critical%250Adomains%2520%2528e.g.%252C%2520autonomous%2520systems%252C%2520AR/VR%2529%2520urgently%2520demands%2520scrutiny%2520of%250Apotential%2520security%2520vulnerabilities.%2520This%2520paper%2520presents%2520the%2520first%2520systematic%250Astudy%2520of%2520backdoor%2520threats%2520in%25203DGS%2520pipelines.%2520We%2520identify%2520that%2520adversaries%2520may%250Aimplant%2520backdoor%2520views%2520to%2520induce%2520malicious%2520scene%2520confusion%2520during%2520inference%252C%250Apotentially%2520leading%2520to%2520environmental%2520misperception%2520in%2520autonomous%2520navigation%2520or%250Aspatial%2520distortion%2520in%2520immersive%2520environments.%2520To%2520uncover%2520this%2520risk%252C%2520we%2520propose%250AGuassTrap%252C%2520a%2520novel%2520poisoning%2520attack%2520method%2520targeting%25203DGS%2520models.%2520GuassTrap%250Ainjects%2520malicious%2520views%2520at%2520specific%2520attack%2520viewpoints%2520while%2520preserving%250Ahigh-quality%2520rendering%2520in%2520non-target%2520views%252C%2520ensuring%2520minimal%2520detectability%2520and%250Amaximizing%2520potential%2520harm.%2520Specifically%252C%2520the%2520proposed%2520method%2520consists%2520of%2520a%250Athree-stage%2520pipeline%2520%2528attack%252C%2520stabilization%252C%2520and%2520normal%2520training%2529%2520to%2520implant%250Astealthy%252C%2520viewpoint-consistent%2520poisoned%2520renderings%2520in%25203DGS%252C%2520jointly%2520optimizing%250Aattack%2520efficacy%2520and%2520perceptual%2520realism%2520to%2520expose%2520security%2520risks%2520in%25203D%250Arendering.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%250Ademonstrate%2520that%2520GuassTrap%2520can%2520effectively%2520embed%2520imperceptible%2520yet%2520harmful%250Abackdoor%2520views%2520while%2520maintaining%2520high-quality%2520rendering%2520in%2520normal%2520views%252C%250Avalidating%2520its%2520robustness%252C%2520adaptability%252C%2520and%2520practical%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussTrap%3A%20Stealthy%20Poisoning%20Attacks%20on%203D%20Gaussian%20Splatting%20for%0A%20%20Targeted%20Scene%20Confusion&entry.906535625=Jiaxin%20Hong%20and%20Sixu%20Chen%20and%20Shuoyang%20Sun%20and%20Hongyao%20Yu%20and%20Hao%20Fang%20and%20Yuqi%20Tan%20and%20Bin%20Chen%20and%20Shuhan%20Qi%20and%20Jiawei%20Li&entry.1292438233=%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20emerges%20as%20a%20breakthrough%20in%20scene%0Arepresentation%20and%20novel%20view%20synthesis%2C%20its%20rapid%20adoption%20in%20safety-critical%0Adomains%20%28e.g.%2C%20autonomous%20systems%2C%20AR/VR%29%20urgently%20demands%20scrutiny%20of%0Apotential%20security%20vulnerabilities.%20This%20paper%20presents%20the%20first%20systematic%0Astudy%20of%20backdoor%20threats%20in%203DGS%20pipelines.%20We%20identify%20that%20adversaries%20may%0Aimplant%20backdoor%20views%20to%20induce%20malicious%20scene%20confusion%20during%20inference%2C%0Apotentially%20leading%20to%20environmental%20misperception%20in%20autonomous%20navigation%20or%0Aspatial%20distortion%20in%20immersive%20environments.%20To%20uncover%20this%20risk%2C%20we%20propose%0AGuassTrap%2C%20a%20novel%20poisoning%20attack%20method%20targeting%203DGS%20models.%20GuassTrap%0Ainjects%20malicious%20views%20at%20specific%20attack%20viewpoints%20while%20preserving%0Ahigh-quality%20rendering%20in%20non-target%20views%2C%20ensuring%20minimal%20detectability%20and%0Amaximizing%20potential%20harm.%20Specifically%2C%20the%20proposed%20method%20consists%20of%20a%0Athree-stage%20pipeline%20%28attack%2C%20stabilization%2C%20and%20normal%20training%29%20to%20implant%0Astealthy%2C%20viewpoint-consistent%20poisoned%20renderings%20in%203DGS%2C%20jointly%20optimizing%0Aattack%20efficacy%20and%20perceptual%20realism%20to%20expose%20security%20risks%20in%203D%0Arendering.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20that%20GuassTrap%20can%20effectively%20embed%20imperceptible%20yet%20harmful%0Abackdoor%20views%20while%20maintaining%20high-quality%20rendering%20in%20normal%20views%2C%0Avalidating%20its%20robustness%2C%20adaptability%2C%20and%20practical%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20829v1&entry.124074799=Read"},
{"title": "Omni-IML: Towards Unified Image Manipulation Localization", "author": "Chenfan Qu and Yiwu Zhong and Fengjun Guo and Lianwen Jin", "abstract": "  Existing Image Manipulation Localization (IML) methods mostly rely heavily on\ntask-specific designs, making them perform well only on the target IML task,\nwhile joint training on multiple IML tasks causes significant performance\ndegradation, hindering real applications.\n  To this end, we propose Omni-IML, the first generalist model designed to\nunify IML across diverse tasks.\n  Specifically, Omni-IML achieves generalization through three key components:\n(1) a Modal Gate Encoder, which adaptively selects the optimal encoding\nmodality per sample, (2) a Dynamic Weight Decoder, which dynamically adjusts\ndecoder filters to the task at hand, and (3) an Anomaly Enhancement module that\nleverages box supervision to highlight the tampered regions and facilitate the\nlearning of task-agnostic features.\n  Beyond localization, to support interpretation of the tampered images, we\nconstruct Omni-273k, a large high-quality dataset that includes natural\nlanguage descriptions of tampered artifact. It is annotated through our\nautomatic, chain-of-thoughts annotation technique.\n  We also design a simple-yet-effective interpretation module to better utilize\nthese descriptive annotations.\n  Our extensive experiments show that our single Omni-IML model achieves\nstate-of-the-art performance across all four major IML tasks, providing a\nvaluable solution for practical deployment and a promising direction of\ngeneralist models in image forensics. Our code and dataset will be publicly\navailable.\n", "link": "http://arxiv.org/abs/2411.14823v2", "date": "2025-04-29", "relevancy": 2.9049, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5878}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5797}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni-IML%3A%20Towards%20Unified%20Image%20Manipulation%20Localization&body=Title%3A%20Omni-IML%3A%20Towards%20Unified%20Image%20Manipulation%20Localization%0AAuthor%3A%20Chenfan%20Qu%20and%20Yiwu%20Zhong%20and%20Fengjun%20Guo%20and%20Lianwen%20Jin%0AAbstract%3A%20%20%20Existing%20Image%20Manipulation%20Localization%20%28IML%29%20methods%20mostly%20rely%20heavily%20on%0Atask-specific%20designs%2C%20making%20them%20perform%20well%20only%20on%20the%20target%20IML%20task%2C%0Awhile%20joint%20training%20on%20multiple%20IML%20tasks%20causes%20significant%20performance%0Adegradation%2C%20hindering%20real%20applications.%0A%20%20To%20this%20end%2C%20we%20propose%20Omni-IML%2C%20the%20first%20generalist%20model%20designed%20to%0Aunify%20IML%20across%20diverse%20tasks.%0A%20%20Specifically%2C%20Omni-IML%20achieves%20generalization%20through%20three%20key%20components%3A%0A%281%29%20a%20Modal%20Gate%20Encoder%2C%20which%20adaptively%20selects%20the%20optimal%20encoding%0Amodality%20per%20sample%2C%20%282%29%20a%20Dynamic%20Weight%20Decoder%2C%20which%20dynamically%20adjusts%0Adecoder%20filters%20to%20the%20task%20at%20hand%2C%20and%20%283%29%20an%20Anomaly%20Enhancement%20module%20that%0Aleverages%20box%20supervision%20to%20highlight%20the%20tampered%20regions%20and%20facilitate%20the%0Alearning%20of%20task-agnostic%20features.%0A%20%20Beyond%20localization%2C%20to%20support%20interpretation%20of%20the%20tampered%20images%2C%20we%0Aconstruct%20Omni-273k%2C%20a%20large%20high-quality%20dataset%20that%20includes%20natural%0Alanguage%20descriptions%20of%20tampered%20artifact.%20It%20is%20annotated%20through%20our%0Aautomatic%2C%20chain-of-thoughts%20annotation%20technique.%0A%20%20We%20also%20design%20a%20simple-yet-effective%20interpretation%20module%20to%20better%20utilize%0Athese%20descriptive%20annotations.%0A%20%20Our%20extensive%20experiments%20show%20that%20our%20single%20Omni-IML%20model%20achieves%0Astate-of-the-art%20performance%20across%20all%20four%20major%20IML%20tasks%2C%20providing%20a%0Avaluable%20solution%20for%20practical%20deployment%20and%20a%20promising%20direction%20of%0Ageneralist%20models%20in%20image%20forensics.%20Our%20code%20and%20dataset%20will%20be%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.14823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni-IML%253A%2520Towards%2520Unified%2520Image%2520Manipulation%2520Localization%26entry.906535625%3DChenfan%2520Qu%2520and%2520Yiwu%2520Zhong%2520and%2520Fengjun%2520Guo%2520and%2520Lianwen%2520Jin%26entry.1292438233%3D%2520%2520Existing%2520Image%2520Manipulation%2520Localization%2520%2528IML%2529%2520methods%2520mostly%2520rely%2520heavily%2520on%250Atask-specific%2520designs%252C%2520making%2520them%2520perform%2520well%2520only%2520on%2520the%2520target%2520IML%2520task%252C%250Awhile%2520joint%2520training%2520on%2520multiple%2520IML%2520tasks%2520causes%2520significant%2520performance%250Adegradation%252C%2520hindering%2520real%2520applications.%250A%2520%2520To%2520this%2520end%252C%2520we%2520propose%2520Omni-IML%252C%2520the%2520first%2520generalist%2520model%2520designed%2520to%250Aunify%2520IML%2520across%2520diverse%2520tasks.%250A%2520%2520Specifically%252C%2520Omni-IML%2520achieves%2520generalization%2520through%2520three%2520key%2520components%253A%250A%25281%2529%2520a%2520Modal%2520Gate%2520Encoder%252C%2520which%2520adaptively%2520selects%2520the%2520optimal%2520encoding%250Amodality%2520per%2520sample%252C%2520%25282%2529%2520a%2520Dynamic%2520Weight%2520Decoder%252C%2520which%2520dynamically%2520adjusts%250Adecoder%2520filters%2520to%2520the%2520task%2520at%2520hand%252C%2520and%2520%25283%2529%2520an%2520Anomaly%2520Enhancement%2520module%2520that%250Aleverages%2520box%2520supervision%2520to%2520highlight%2520the%2520tampered%2520regions%2520and%2520facilitate%2520the%250Alearning%2520of%2520task-agnostic%2520features.%250A%2520%2520Beyond%2520localization%252C%2520to%2520support%2520interpretation%2520of%2520the%2520tampered%2520images%252C%2520we%250Aconstruct%2520Omni-273k%252C%2520a%2520large%2520high-quality%2520dataset%2520that%2520includes%2520natural%250Alanguage%2520descriptions%2520of%2520tampered%2520artifact.%2520It%2520is%2520annotated%2520through%2520our%250Aautomatic%252C%2520chain-of-thoughts%2520annotation%2520technique.%250A%2520%2520We%2520also%2520design%2520a%2520simple-yet-effective%2520interpretation%2520module%2520to%2520better%2520utilize%250Athese%2520descriptive%2520annotations.%250A%2520%2520Our%2520extensive%2520experiments%2520show%2520that%2520our%2520single%2520Omni-IML%2520model%2520achieves%250Astate-of-the-art%2520performance%2520across%2520all%2520four%2520major%2520IML%2520tasks%252C%2520providing%2520a%250Avaluable%2520solution%2520for%2520practical%2520deployment%2520and%2520a%2520promising%2520direction%2520of%250Ageneralist%2520models%2520in%2520image%2520forensics.%2520Our%2520code%2520and%2520dataset%2520will%2520be%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.14823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni-IML%3A%20Towards%20Unified%20Image%20Manipulation%20Localization&entry.906535625=Chenfan%20Qu%20and%20Yiwu%20Zhong%20and%20Fengjun%20Guo%20and%20Lianwen%20Jin&entry.1292438233=%20%20Existing%20Image%20Manipulation%20Localization%20%28IML%29%20methods%20mostly%20rely%20heavily%20on%0Atask-specific%20designs%2C%20making%20them%20perform%20well%20only%20on%20the%20target%20IML%20task%2C%0Awhile%20joint%20training%20on%20multiple%20IML%20tasks%20causes%20significant%20performance%0Adegradation%2C%20hindering%20real%20applications.%0A%20%20To%20this%20end%2C%20we%20propose%20Omni-IML%2C%20the%20first%20generalist%20model%20designed%20to%0Aunify%20IML%20across%20diverse%20tasks.%0A%20%20Specifically%2C%20Omni-IML%20achieves%20generalization%20through%20three%20key%20components%3A%0A%281%29%20a%20Modal%20Gate%20Encoder%2C%20which%20adaptively%20selects%20the%20optimal%20encoding%0Amodality%20per%20sample%2C%20%282%29%20a%20Dynamic%20Weight%20Decoder%2C%20which%20dynamically%20adjusts%0Adecoder%20filters%20to%20the%20task%20at%20hand%2C%20and%20%283%29%20an%20Anomaly%20Enhancement%20module%20that%0Aleverages%20box%20supervision%20to%20highlight%20the%20tampered%20regions%20and%20facilitate%20the%0Alearning%20of%20task-agnostic%20features.%0A%20%20Beyond%20localization%2C%20to%20support%20interpretation%20of%20the%20tampered%20images%2C%20we%0Aconstruct%20Omni-273k%2C%20a%20large%20high-quality%20dataset%20that%20includes%20natural%0Alanguage%20descriptions%20of%20tampered%20artifact.%20It%20is%20annotated%20through%20our%0Aautomatic%2C%20chain-of-thoughts%20annotation%20technique.%0A%20%20We%20also%20design%20a%20simple-yet-effective%20interpretation%20module%20to%20better%20utilize%0Athese%20descriptive%20annotations.%0A%20%20Our%20extensive%20experiments%20show%20that%20our%20single%20Omni-IML%20model%20achieves%0Astate-of-the-art%20performance%20across%20all%20four%20major%20IML%20tasks%2C%20providing%20a%0Avaluable%20solution%20for%20practical%20deployment%20and%20a%20promising%20direction%20of%0Ageneralist%20models%20in%20image%20forensics.%20Our%20code%20and%20dataset%20will%20be%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.14823v2&entry.124074799=Read"},
{"title": "RadSAM: Segmenting 3D radiological images with a 2D promptable model", "author": "Julien Khlaut and Elodie Ferreres and Daniel Tordjman and H\u00e9l\u00e8ne Philippe and Tom Boeken and Pierre Manceron and Corentin Dancette", "abstract": "  Medical image segmentation is a crucial and time-consuming task in clinical\ncare, where mask precision is extremely important. The Segment Anything Model\n(SAM) offers a promising approach, as it provides an interactive interface\nbased on visual prompting and edition to refine an initial segmentation. This\nmodel has strong generalization capabilities, does not rely on predefined\nclasses, and adapts to diverse objects; however, it is pre-trained on natural\nimages and lacks the ability to process medical data effectively. In addition,\nthis model is built for 2D images, whereas a whole medical domain is based on\n3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging\nare based on 2D models, thus requiring one prompt per slice to segment 3D\nobjects, making the segmentation process tedious. They also lack important\nfeatures such as editing. To bridge this gap, we propose RadSAM, a novel method\nfor segmenting 3D objects with a 2D model from a single prompt. In practice, we\ntrain a 2D model using noisy masks as initial prompts, in addition to bounding\nboxes and points. We then use this novel prompt type with an iterative\ninference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a\nbenchmark to evaluate the model's ability to segment 3D objects in CT images\nfrom a single prompt and evaluate the models' out-of-domain transfer and\nedition capabilities. We demonstrate the effectiveness of our approach against\nstate-of-the-art models on this benchmark using the AMOS abdominal organ\nsegmentation dataset.\n", "link": "http://arxiv.org/abs/2504.20837v1", "date": "2025-04-29", "relevancy": 2.8862, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5774}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5774}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadSAM%3A%20Segmenting%203D%20radiological%20images%20with%20a%202D%20promptable%20model&body=Title%3A%20RadSAM%3A%20Segmenting%203D%20radiological%20images%20with%20a%202D%20promptable%20model%0AAuthor%3A%20Julien%20Khlaut%20and%20Elodie%20Ferreres%20and%20Daniel%20Tordjman%20and%20H%C3%A9l%C3%A8ne%20Philippe%20and%20Tom%20Boeken%20and%20Pierre%20Manceron%20and%20Corentin%20Dancette%0AAbstract%3A%20%20%20Medical%20image%20segmentation%20is%20a%20crucial%20and%20time-consuming%20task%20in%20clinical%0Acare%2C%20where%20mask%20precision%20is%20extremely%20important.%20The%20Segment%20Anything%20Model%0A%28SAM%29%20offers%20a%20promising%20approach%2C%20as%20it%20provides%20an%20interactive%20interface%0Abased%20on%20visual%20prompting%20and%20edition%20to%20refine%20an%20initial%20segmentation.%20This%0Amodel%20has%20strong%20generalization%20capabilities%2C%20does%20not%20rely%20on%20predefined%0Aclasses%2C%20and%20adapts%20to%20diverse%20objects%3B%20however%2C%20it%20is%20pre-trained%20on%20natural%0Aimages%20and%20lacks%20the%20ability%20to%20process%20medical%20data%20effectively.%20In%20addition%2C%0Athis%20model%20is%20built%20for%202D%20images%2C%20whereas%20a%20whole%20medical%20domain%20is%20based%20on%0A3D%20images%2C%20such%20as%20CT%20and%20MRI.%20Recent%20adaptations%20of%20SAM%20for%20medical%20imaging%0Aare%20based%20on%202D%20models%2C%20thus%20requiring%20one%20prompt%20per%20slice%20to%20segment%203D%0Aobjects%2C%20making%20the%20segmentation%20process%20tedious.%20They%20also%20lack%20important%0Afeatures%20such%20as%20editing.%20To%20bridge%20this%20gap%2C%20we%20propose%20RadSAM%2C%20a%20novel%20method%0Afor%20segmenting%203D%20objects%20with%20a%202D%20model%20from%20a%20single%20prompt.%20In%20practice%2C%20we%0Atrain%20a%202D%20model%20using%20noisy%20masks%20as%20initial%20prompts%2C%20in%20addition%20to%20bounding%0Aboxes%20and%20points.%20We%20then%20use%20this%20novel%20prompt%20type%20with%20an%20iterative%0Ainference%20pipeline%20to%20reconstruct%20the%203D%20mask%20slice-by-slice.%20We%20introduce%20a%0Abenchmark%20to%20evaluate%20the%20model%27s%20ability%20to%20segment%203D%20objects%20in%20CT%20images%0Afrom%20a%20single%20prompt%20and%20evaluate%20the%20models%27%20out-of-domain%20transfer%20and%0Aedition%20capabilities.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20against%0Astate-of-the-art%20models%20on%20this%20benchmark%20using%20the%20AMOS%20abdominal%20organ%0Asegmentation%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadSAM%253A%2520Segmenting%25203D%2520radiological%2520images%2520with%2520a%25202D%2520promptable%2520model%26entry.906535625%3DJulien%2520Khlaut%2520and%2520Elodie%2520Ferreres%2520and%2520Daniel%2520Tordjman%2520and%2520H%25C3%25A9l%25C3%25A8ne%2520Philippe%2520and%2520Tom%2520Boeken%2520and%2520Pierre%2520Manceron%2520and%2520Corentin%2520Dancette%26entry.1292438233%3D%2520%2520Medical%2520image%2520segmentation%2520is%2520a%2520crucial%2520and%2520time-consuming%2520task%2520in%2520clinical%250Acare%252C%2520where%2520mask%2520precision%2520is%2520extremely%2520important.%2520The%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%2520offers%2520a%2520promising%2520approach%252C%2520as%2520it%2520provides%2520an%2520interactive%2520interface%250Abased%2520on%2520visual%2520prompting%2520and%2520edition%2520to%2520refine%2520an%2520initial%2520segmentation.%2520This%250Amodel%2520has%2520strong%2520generalization%2520capabilities%252C%2520does%2520not%2520rely%2520on%2520predefined%250Aclasses%252C%2520and%2520adapts%2520to%2520diverse%2520objects%253B%2520however%252C%2520it%2520is%2520pre-trained%2520on%2520natural%250Aimages%2520and%2520lacks%2520the%2520ability%2520to%2520process%2520medical%2520data%2520effectively.%2520In%2520addition%252C%250Athis%2520model%2520is%2520built%2520for%25202D%2520images%252C%2520whereas%2520a%2520whole%2520medical%2520domain%2520is%2520based%2520on%250A3D%2520images%252C%2520such%2520as%2520CT%2520and%2520MRI.%2520Recent%2520adaptations%2520of%2520SAM%2520for%2520medical%2520imaging%250Aare%2520based%2520on%25202D%2520models%252C%2520thus%2520requiring%2520one%2520prompt%2520per%2520slice%2520to%2520segment%25203D%250Aobjects%252C%2520making%2520the%2520segmentation%2520process%2520tedious.%2520They%2520also%2520lack%2520important%250Afeatures%2520such%2520as%2520editing.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520RadSAM%252C%2520a%2520novel%2520method%250Afor%2520segmenting%25203D%2520objects%2520with%2520a%25202D%2520model%2520from%2520a%2520single%2520prompt.%2520In%2520practice%252C%2520we%250Atrain%2520a%25202D%2520model%2520using%2520noisy%2520masks%2520as%2520initial%2520prompts%252C%2520in%2520addition%2520to%2520bounding%250Aboxes%2520and%2520points.%2520We%2520then%2520use%2520this%2520novel%2520prompt%2520type%2520with%2520an%2520iterative%250Ainference%2520pipeline%2520to%2520reconstruct%2520the%25203D%2520mask%2520slice-by-slice.%2520We%2520introduce%2520a%250Abenchmark%2520to%2520evaluate%2520the%2520model%2527s%2520ability%2520to%2520segment%25203D%2520objects%2520in%2520CT%2520images%250Afrom%2520a%2520single%2520prompt%2520and%2520evaluate%2520the%2520models%2527%2520out-of-domain%2520transfer%2520and%250Aedition%2520capabilities.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520against%250Astate-of-the-art%2520models%2520on%2520this%2520benchmark%2520using%2520the%2520AMOS%2520abdominal%2520organ%250Asegmentation%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadSAM%3A%20Segmenting%203D%20radiological%20images%20with%20a%202D%20promptable%20model&entry.906535625=Julien%20Khlaut%20and%20Elodie%20Ferreres%20and%20Daniel%20Tordjman%20and%20H%C3%A9l%C3%A8ne%20Philippe%20and%20Tom%20Boeken%20and%20Pierre%20Manceron%20and%20Corentin%20Dancette&entry.1292438233=%20%20Medical%20image%20segmentation%20is%20a%20crucial%20and%20time-consuming%20task%20in%20clinical%0Acare%2C%20where%20mask%20precision%20is%20extremely%20important.%20The%20Segment%20Anything%20Model%0A%28SAM%29%20offers%20a%20promising%20approach%2C%20as%20it%20provides%20an%20interactive%20interface%0Abased%20on%20visual%20prompting%20and%20edition%20to%20refine%20an%20initial%20segmentation.%20This%0Amodel%20has%20strong%20generalization%20capabilities%2C%20does%20not%20rely%20on%20predefined%0Aclasses%2C%20and%20adapts%20to%20diverse%20objects%3B%20however%2C%20it%20is%20pre-trained%20on%20natural%0Aimages%20and%20lacks%20the%20ability%20to%20process%20medical%20data%20effectively.%20In%20addition%2C%0Athis%20model%20is%20built%20for%202D%20images%2C%20whereas%20a%20whole%20medical%20domain%20is%20based%20on%0A3D%20images%2C%20such%20as%20CT%20and%20MRI.%20Recent%20adaptations%20of%20SAM%20for%20medical%20imaging%0Aare%20based%20on%202D%20models%2C%20thus%20requiring%20one%20prompt%20per%20slice%20to%20segment%203D%0Aobjects%2C%20making%20the%20segmentation%20process%20tedious.%20They%20also%20lack%20important%0Afeatures%20such%20as%20editing.%20To%20bridge%20this%20gap%2C%20we%20propose%20RadSAM%2C%20a%20novel%20method%0Afor%20segmenting%203D%20objects%20with%20a%202D%20model%20from%20a%20single%20prompt.%20In%20practice%2C%20we%0Atrain%20a%202D%20model%20using%20noisy%20masks%20as%20initial%20prompts%2C%20in%20addition%20to%20bounding%0Aboxes%20and%20points.%20We%20then%20use%20this%20novel%20prompt%20type%20with%20an%20iterative%0Ainference%20pipeline%20to%20reconstruct%20the%203D%20mask%20slice-by-slice.%20We%20introduce%20a%0Abenchmark%20to%20evaluate%20the%20model%27s%20ability%20to%20segment%203D%20objects%20in%20CT%20images%0Afrom%20a%20single%20prompt%20and%20evaluate%20the%20models%27%20out-of-domain%20transfer%20and%0Aedition%20capabilities.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20against%0Astate-of-the-art%20models%20on%20this%20benchmark%20using%20the%20AMOS%20abdominal%20organ%0Asegmentation%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20837v1&entry.124074799=Read"},
{"title": "Video-Bench: Human-Aligned Video Generation Benchmark", "author": "Hui Han and Siyuan Li and Jiaqi Chen and Yiwen Yuan and Yuling Wu and Chak Tou Leong and Hanwen Du and Junchen Fu and Youhua Li and Jie Zhang and Chi Zhang and Li-jia Li and Yongxin Ni", "abstract": "  Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment.\n", "link": "http://arxiv.org/abs/2504.04907v2", "date": "2025-04-29", "relevancy": 2.8668, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5901}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5728}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-Bench%3A%20Human-Aligned%20Video%20Generation%20Benchmark&body=Title%3A%20Video-Bench%3A%20Human-Aligned%20Video%20Generation%20Benchmark%0AAuthor%3A%20Hui%20Han%20and%20Siyuan%20Li%20and%20Jiaqi%20Chen%20and%20Yiwen%20Yuan%20and%20Yuling%20Wu%20and%20Chak%20Tou%20Leong%20and%20Hanwen%20Du%20and%20Junchen%20Fu%20and%20Youhua%20Li%20and%20Jie%20Zhang%20and%20Chi%20Zhang%20and%20Li-jia%20Li%20and%20Yongxin%20Ni%0AAbstract%3A%20%20%20Video%20generation%20assessment%20is%20essential%20for%20ensuring%20that%20generative%20models%0Aproduce%20visually%20realistic%2C%20high-quality%20videos%20while%20aligning%20with%20human%0Aexpectations.%20Current%20video%20generation%20benchmarks%20fall%20into%20two%20main%0Acategories%3A%20traditional%20benchmarks%2C%20which%20use%20metrics%20and%20embeddings%20to%0Aevaluate%20generated%20video%20quality%20across%20multiple%20dimensions%20but%20often%20lack%0Aalignment%20with%20human%20judgments%3B%20and%20large%20language%20model%20%28LLM%29-based%0Abenchmarks%2C%20though%20capable%20of%20human-like%20reasoning%2C%20are%20constrained%20by%20a%0Alimited%20understanding%20of%20video%20quality%20metrics%20and%20cross-modal%20consistency.%20To%0Aaddress%20these%20challenges%20and%20establish%20a%20benchmark%20that%20better%20aligns%20with%0Ahuman%20preferences%2C%20this%20paper%20introduces%20Video-Bench%2C%20a%20comprehensive%20benchmark%0Afeaturing%20a%20rich%20prompt%20suite%20and%20extensive%20evaluation%20dimensions.%20This%0Abenchmark%20represents%20the%20first%20attempt%20to%20systematically%20leverage%20MLLMs%20across%0Aall%20dimensions%20relevant%20to%20video%20generation%20assessment%20in%20generative%20models.%20By%0Aincorporating%20few-shot%20scoring%20and%20chain-of-query%20techniques%2C%20Video-Bench%0Aprovides%20a%20structured%2C%20scalable%20approach%20to%20generated%20video%20evaluation.%0AExperiments%20on%20advanced%20models%20including%20Sora%20demonstrate%20that%20Video-Bench%0Aachieves%20superior%20alignment%20with%20human%20preferences%20across%20all%20dimensions.%0AMoreover%2C%20in%20instances%20where%20our%20framework%27s%20assessments%20diverge%20from%20human%0Aevaluations%2C%20it%20consistently%20offers%20more%20objective%20and%20accurate%20insights%2C%0Asuggesting%20an%20even%20greater%20potential%20advantage%20over%20traditional%20human%20judgment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04907v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-Bench%253A%2520Human-Aligned%2520Video%2520Generation%2520Benchmark%26entry.906535625%3DHui%2520Han%2520and%2520Siyuan%2520Li%2520and%2520Jiaqi%2520Chen%2520and%2520Yiwen%2520Yuan%2520and%2520Yuling%2520Wu%2520and%2520Chak%2520Tou%2520Leong%2520and%2520Hanwen%2520Du%2520and%2520Junchen%2520Fu%2520and%2520Youhua%2520Li%2520and%2520Jie%2520Zhang%2520and%2520Chi%2520Zhang%2520and%2520Li-jia%2520Li%2520and%2520Yongxin%2520Ni%26entry.1292438233%3D%2520%2520Video%2520generation%2520assessment%2520is%2520essential%2520for%2520ensuring%2520that%2520generative%2520models%250Aproduce%2520visually%2520realistic%252C%2520high-quality%2520videos%2520while%2520aligning%2520with%2520human%250Aexpectations.%2520Current%2520video%2520generation%2520benchmarks%2520fall%2520into%2520two%2520main%250Acategories%253A%2520traditional%2520benchmarks%252C%2520which%2520use%2520metrics%2520and%2520embeddings%2520to%250Aevaluate%2520generated%2520video%2520quality%2520across%2520multiple%2520dimensions%2520but%2520often%2520lack%250Aalignment%2520with%2520human%2520judgments%253B%2520and%2520large%2520language%2520model%2520%2528LLM%2529-based%250Abenchmarks%252C%2520though%2520capable%2520of%2520human-like%2520reasoning%252C%2520are%2520constrained%2520by%2520a%250Alimited%2520understanding%2520of%2520video%2520quality%2520metrics%2520and%2520cross-modal%2520consistency.%2520To%250Aaddress%2520these%2520challenges%2520and%2520establish%2520a%2520benchmark%2520that%2520better%2520aligns%2520with%250Ahuman%2520preferences%252C%2520this%2520paper%2520introduces%2520Video-Bench%252C%2520a%2520comprehensive%2520benchmark%250Afeaturing%2520a%2520rich%2520prompt%2520suite%2520and%2520extensive%2520evaluation%2520dimensions.%2520This%250Abenchmark%2520represents%2520the%2520first%2520attempt%2520to%2520systematically%2520leverage%2520MLLMs%2520across%250Aall%2520dimensions%2520relevant%2520to%2520video%2520generation%2520assessment%2520in%2520generative%2520models.%2520By%250Aincorporating%2520few-shot%2520scoring%2520and%2520chain-of-query%2520techniques%252C%2520Video-Bench%250Aprovides%2520a%2520structured%252C%2520scalable%2520approach%2520to%2520generated%2520video%2520evaluation.%250AExperiments%2520on%2520advanced%2520models%2520including%2520Sora%2520demonstrate%2520that%2520Video-Bench%250Aachieves%2520superior%2520alignment%2520with%2520human%2520preferences%2520across%2520all%2520dimensions.%250AMoreover%252C%2520in%2520instances%2520where%2520our%2520framework%2527s%2520assessments%2520diverge%2520from%2520human%250Aevaluations%252C%2520it%2520consistently%2520offers%2520more%2520objective%2520and%2520accurate%2520insights%252C%250Asuggesting%2520an%2520even%2520greater%2520potential%2520advantage%2520over%2520traditional%2520human%2520judgment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04907v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-Bench%3A%20Human-Aligned%20Video%20Generation%20Benchmark&entry.906535625=Hui%20Han%20and%20Siyuan%20Li%20and%20Jiaqi%20Chen%20and%20Yiwen%20Yuan%20and%20Yuling%20Wu%20and%20Chak%20Tou%20Leong%20and%20Hanwen%20Du%20and%20Junchen%20Fu%20and%20Youhua%20Li%20and%20Jie%20Zhang%20and%20Chi%20Zhang%20and%20Li-jia%20Li%20and%20Yongxin%20Ni&entry.1292438233=%20%20Video%20generation%20assessment%20is%20essential%20for%20ensuring%20that%20generative%20models%0Aproduce%20visually%20realistic%2C%20high-quality%20videos%20while%20aligning%20with%20human%0Aexpectations.%20Current%20video%20generation%20benchmarks%20fall%20into%20two%20main%0Acategories%3A%20traditional%20benchmarks%2C%20which%20use%20metrics%20and%20embeddings%20to%0Aevaluate%20generated%20video%20quality%20across%20multiple%20dimensions%20but%20often%20lack%0Aalignment%20with%20human%20judgments%3B%20and%20large%20language%20model%20%28LLM%29-based%0Abenchmarks%2C%20though%20capable%20of%20human-like%20reasoning%2C%20are%20constrained%20by%20a%0Alimited%20understanding%20of%20video%20quality%20metrics%20and%20cross-modal%20consistency.%20To%0Aaddress%20these%20challenges%20and%20establish%20a%20benchmark%20that%20better%20aligns%20with%0Ahuman%20preferences%2C%20this%20paper%20introduces%20Video-Bench%2C%20a%20comprehensive%20benchmark%0Afeaturing%20a%20rich%20prompt%20suite%20and%20extensive%20evaluation%20dimensions.%20This%0Abenchmark%20represents%20the%20first%20attempt%20to%20systematically%20leverage%20MLLMs%20across%0Aall%20dimensions%20relevant%20to%20video%20generation%20assessment%20in%20generative%20models.%20By%0Aincorporating%20few-shot%20scoring%20and%20chain-of-query%20techniques%2C%20Video-Bench%0Aprovides%20a%20structured%2C%20scalable%20approach%20to%20generated%20video%20evaluation.%0AExperiments%20on%20advanced%20models%20including%20Sora%20demonstrate%20that%20Video-Bench%0Aachieves%20superior%20alignment%20with%20human%20preferences%20across%20all%20dimensions.%0AMoreover%2C%20in%20instances%20where%20our%20framework%27s%20assessments%20diverge%20from%20human%0Aevaluations%2C%20it%20consistently%20offers%20more%20objective%20and%20accurate%20insights%2C%0Asuggesting%20an%20even%20greater%20potential%20advantage%20over%20traditional%20human%20judgment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04907v2&entry.124074799=Read"},
{"title": "X-Fusion: Introducing New Modality to Frozen Large Language Models", "author": "Sicheng Mo and Thao Nguyen and Xun Huang and Siddharth Srinivasan Iyer and Yijun Li and Yuchen Liu and Abhishek Tandon and Eli Shechtman and Krishna Kumar Singh and Yong Jae Lee and Bolei Zhou and Yuheng Li", "abstract": "  We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.\n", "link": "http://arxiv.org/abs/2504.20996v1", "date": "2025-04-29", "relevancy": 2.8625, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5793}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Fusion%3A%20Introducing%20New%20Modality%20to%20Frozen%20Large%20Language%20Models&body=Title%3A%20X-Fusion%3A%20Introducing%20New%20Modality%20to%20Frozen%20Large%20Language%20Models%0AAuthor%3A%20Sicheng%20Mo%20and%20Thao%20Nguyen%20and%20Xun%20Huang%20and%20Siddharth%20Srinivasan%20Iyer%20and%20Yijun%20Li%20and%20Yuchen%20Liu%20and%20Abhishek%20Tandon%20and%20Eli%20Shechtman%20and%20Krishna%20Kumar%20Singh%20and%20Yong%20Jae%20Lee%20and%20Bolei%20Zhou%20and%20Yuheng%20Li%0AAbstract%3A%20%20%20We%20propose%20X-Fusion%2C%20a%20framework%20that%20extends%20pretrained%20Large%20Language%0AModels%20%28LLMs%29%20for%20multimodal%20tasks%20while%20preserving%20their%20language%0Acapabilities.%20X-Fusion%20employs%20a%20dual-tower%20design%20with%20modality-specific%0Aweights%2C%20keeping%20the%20LLM%27s%20parameters%20frozen%20while%20integrating%20vision-specific%0Ainformation%20for%20both%20understanding%20and%20generation.%20Our%20experiments%20demonstrate%0Athat%20X-Fusion%20consistently%20outperforms%20alternative%20architectures%20on%20both%0Aimage-to-text%20and%20text-to-image%20tasks.%20We%20find%20that%20incorporating%0Aunderstanding-focused%20data%20improves%20generation%20quality%2C%20reducing%20image%20data%0Anoise%20enhances%20overall%20performance%2C%20and%20feature%20alignment%20accelerates%0Aconvergence%20for%20smaller%20models%20but%20has%20minimal%20impact%20on%20larger%20ones.%20Our%0Afindings%20provide%20valuable%20insights%20into%20building%20efficient%20unified%20multimodal%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Fusion%253A%2520Introducing%2520New%2520Modality%2520to%2520Frozen%2520Large%2520Language%2520Models%26entry.906535625%3DSicheng%2520Mo%2520and%2520Thao%2520Nguyen%2520and%2520Xun%2520Huang%2520and%2520Siddharth%2520Srinivasan%2520Iyer%2520and%2520Yijun%2520Li%2520and%2520Yuchen%2520Liu%2520and%2520Abhishek%2520Tandon%2520and%2520Eli%2520Shechtman%2520and%2520Krishna%2520Kumar%2520Singh%2520and%2520Yong%2520Jae%2520Lee%2520and%2520Bolei%2520Zhou%2520and%2520Yuheng%2520Li%26entry.1292438233%3D%2520%2520We%2520propose%2520X-Fusion%252C%2520a%2520framework%2520that%2520extends%2520pretrained%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520for%2520multimodal%2520tasks%2520while%2520preserving%2520their%2520language%250Acapabilities.%2520X-Fusion%2520employs%2520a%2520dual-tower%2520design%2520with%2520modality-specific%250Aweights%252C%2520keeping%2520the%2520LLM%2527s%2520parameters%2520frozen%2520while%2520integrating%2520vision-specific%250Ainformation%2520for%2520both%2520understanding%2520and%2520generation.%2520Our%2520experiments%2520demonstrate%250Athat%2520X-Fusion%2520consistently%2520outperforms%2520alternative%2520architectures%2520on%2520both%250Aimage-to-text%2520and%2520text-to-image%2520tasks.%2520We%2520find%2520that%2520incorporating%250Aunderstanding-focused%2520data%2520improves%2520generation%2520quality%252C%2520reducing%2520image%2520data%250Anoise%2520enhances%2520overall%2520performance%252C%2520and%2520feature%2520alignment%2520accelerates%250Aconvergence%2520for%2520smaller%2520models%2520but%2520has%2520minimal%2520impact%2520on%2520larger%2520ones.%2520Our%250Afindings%2520provide%2520valuable%2520insights%2520into%2520building%2520efficient%2520unified%2520multimodal%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Fusion%3A%20Introducing%20New%20Modality%20to%20Frozen%20Large%20Language%20Models&entry.906535625=Sicheng%20Mo%20and%20Thao%20Nguyen%20and%20Xun%20Huang%20and%20Siddharth%20Srinivasan%20Iyer%20and%20Yijun%20Li%20and%20Yuchen%20Liu%20and%20Abhishek%20Tandon%20and%20Eli%20Shechtman%20and%20Krishna%20Kumar%20Singh%20and%20Yong%20Jae%20Lee%20and%20Bolei%20Zhou%20and%20Yuheng%20Li&entry.1292438233=%20%20We%20propose%20X-Fusion%2C%20a%20framework%20that%20extends%20pretrained%20Large%20Language%0AModels%20%28LLMs%29%20for%20multimodal%20tasks%20while%20preserving%20their%20language%0Acapabilities.%20X-Fusion%20employs%20a%20dual-tower%20design%20with%20modality-specific%0Aweights%2C%20keeping%20the%20LLM%27s%20parameters%20frozen%20while%20integrating%20vision-specific%0Ainformation%20for%20both%20understanding%20and%20generation.%20Our%20experiments%20demonstrate%0Athat%20X-Fusion%20consistently%20outperforms%20alternative%20architectures%20on%20both%0Aimage-to-text%20and%20text-to-image%20tasks.%20We%20find%20that%20incorporating%0Aunderstanding-focused%20data%20improves%20generation%20quality%2C%20reducing%20image%20data%0Anoise%20enhances%20overall%20performance%2C%20and%20feature%20alignment%20accelerates%0Aconvergence%20for%20smaller%20models%20but%20has%20minimal%20impact%20on%20larger%20ones.%20Our%0Afindings%20provide%20valuable%20insights%20into%20building%20efficient%20unified%20multimodal%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20996v1&entry.124074799=Read"},
{"title": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models", "author": "Yunjia Qi and Hao Peng and Xiaozhi Wang and Bin Xu and Lei Hou and Juanzi Li", "abstract": "  Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.\n", "link": "http://arxiv.org/abs/2410.24175v2", "date": "2025-04-29", "relevancy": 2.5824, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constraint%20Back-translation%20Improves%20Complex%20Instruction%20Following%20of%0A%20%20Large%20Language%20Models&body=Title%3A%20Constraint%20Back-translation%20Improves%20Complex%20Instruction%20Following%20of%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yunjia%20Qi%20and%20Hao%20Peng%20and%20Xiaozhi%20Wang%20and%20Bin%20Xu%20and%20Lei%20Hou%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20struggle%20to%20follow%20instructions%20with%20complex%0Aconstraints%20in%20format%2C%20length%2C%20etc.%20Following%20the%20conventional%0Ainstruction-tuning%20practice%2C%20previous%20works%20conduct%20post-training%20on%20complex%0Ainstruction-response%20pairs%20generated%20by%20feeding%20complex%20instructions%20to%0Aadvanced%20LLMs.%20However%2C%20even%20advanced%20LLMs%20cannot%20follow%20complex%20instructions%0Awell%2C%20thus%20limiting%20the%20quality%20of%20generated%20data.%20In%20this%20work%2C%20we%20find%20that%0Aexisting%20datasets%20inherently%20contain%20implicit%20complex%20constraints%20and%20propose%20a%0Anovel%20data%20generation%20technique%2C%20constraint%20back-translation.%20Specifically%2C%20we%0Atake%20the%20high-quality%20instruction-response%20pairs%20in%20existing%20datasets%20and%20only%0Aadopt%20advanced%20LLMs%20to%20add%20complex%20constraints%20already%20met%20by%20the%20responses%20to%0Athe%20instructions%2C%20which%20naturally%20reduces%20costs%20and%20data%20noise.%20In%20the%0Aexperiments%2C%20we%20adopt%20Llama3-70B-Instruct%20to%20back-translate%20constraints%20and%0Acreate%20a%20high-quality%20complex%20instruction-response%20dataset%2C%20named%20CRAB.%20We%0Apresent%20that%20post-training%20on%20CRAB%20improves%20multiple%20backbone%20LLMs%27%20complex%0Ainstruction-following%20ability%2C%20evaluated%20on%20extensive%20instruction-following%0Abenchmarks.%20We%20further%20find%20that%20constraint%20back-translation%20also%20serves%20as%20a%0Auseful%20auxiliary%20training%20objective%20in%20post-training.%20Our%20code%2C%20data%2C%20and%0Amodels%20will%20be%20released%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24175v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstraint%2520Back-translation%2520Improves%2520Complex%2520Instruction%2520Following%2520of%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYunjia%2520Qi%2520and%2520Hao%2520Peng%2520and%2520Xiaozhi%2520Wang%2520and%2520Bin%2520Xu%2520and%2520Lei%2520Hou%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520struggle%2520to%2520follow%2520instructions%2520with%2520complex%250Aconstraints%2520in%2520format%252C%2520length%252C%2520etc.%2520Following%2520the%2520conventional%250Ainstruction-tuning%2520practice%252C%2520previous%2520works%2520conduct%2520post-training%2520on%2520complex%250Ainstruction-response%2520pairs%2520generated%2520by%2520feeding%2520complex%2520instructions%2520to%250Aadvanced%2520LLMs.%2520However%252C%2520even%2520advanced%2520LLMs%2520cannot%2520follow%2520complex%2520instructions%250Awell%252C%2520thus%2520limiting%2520the%2520quality%2520of%2520generated%2520data.%2520In%2520this%2520work%252C%2520we%2520find%2520that%250Aexisting%2520datasets%2520inherently%2520contain%2520implicit%2520complex%2520constraints%2520and%2520propose%2520a%250Anovel%2520data%2520generation%2520technique%252C%2520constraint%2520back-translation.%2520Specifically%252C%2520we%250Atake%2520the%2520high-quality%2520instruction-response%2520pairs%2520in%2520existing%2520datasets%2520and%2520only%250Aadopt%2520advanced%2520LLMs%2520to%2520add%2520complex%2520constraints%2520already%2520met%2520by%2520the%2520responses%2520to%250Athe%2520instructions%252C%2520which%2520naturally%2520reduces%2520costs%2520and%2520data%2520noise.%2520In%2520the%250Aexperiments%252C%2520we%2520adopt%2520Llama3-70B-Instruct%2520to%2520back-translate%2520constraints%2520and%250Acreate%2520a%2520high-quality%2520complex%2520instruction-response%2520dataset%252C%2520named%2520CRAB.%2520We%250Apresent%2520that%2520post-training%2520on%2520CRAB%2520improves%2520multiple%2520backbone%2520LLMs%2527%2520complex%250Ainstruction-following%2520ability%252C%2520evaluated%2520on%2520extensive%2520instruction-following%250Abenchmarks.%2520We%2520further%2520find%2520that%2520constraint%2520back-translation%2520also%2520serves%2520as%2520a%250Auseful%2520auxiliary%2520training%2520objective%2520in%2520post-training.%2520Our%2520code%252C%2520data%252C%2520and%250Amodels%2520will%2520be%2520released%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24175v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constraint%20Back-translation%20Improves%20Complex%20Instruction%20Following%20of%0A%20%20Large%20Language%20Models&entry.906535625=Yunjia%20Qi%20and%20Hao%20Peng%20and%20Xiaozhi%20Wang%20and%20Bin%20Xu%20and%20Lei%20Hou%20and%20Juanzi%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20struggle%20to%20follow%20instructions%20with%20complex%0Aconstraints%20in%20format%2C%20length%2C%20etc.%20Following%20the%20conventional%0Ainstruction-tuning%20practice%2C%20previous%20works%20conduct%20post-training%20on%20complex%0Ainstruction-response%20pairs%20generated%20by%20feeding%20complex%20instructions%20to%0Aadvanced%20LLMs.%20However%2C%20even%20advanced%20LLMs%20cannot%20follow%20complex%20instructions%0Awell%2C%20thus%20limiting%20the%20quality%20of%20generated%20data.%20In%20this%20work%2C%20we%20find%20that%0Aexisting%20datasets%20inherently%20contain%20implicit%20complex%20constraints%20and%20propose%20a%0Anovel%20data%20generation%20technique%2C%20constraint%20back-translation.%20Specifically%2C%20we%0Atake%20the%20high-quality%20instruction-response%20pairs%20in%20existing%20datasets%20and%20only%0Aadopt%20advanced%20LLMs%20to%20add%20complex%20constraints%20already%20met%20by%20the%20responses%20to%0Athe%20instructions%2C%20which%20naturally%20reduces%20costs%20and%20data%20noise.%20In%20the%0Aexperiments%2C%20we%20adopt%20Llama3-70B-Instruct%20to%20back-translate%20constraints%20and%0Acreate%20a%20high-quality%20complex%20instruction-response%20dataset%2C%20named%20CRAB.%20We%0Apresent%20that%20post-training%20on%20CRAB%20improves%20multiple%20backbone%20LLMs%27%20complex%0Ainstruction-following%20ability%2C%20evaluated%20on%20extensive%20instruction-following%0Abenchmarks.%20We%20further%20find%20that%20constraint%20back-translation%20also%20serves%20as%20a%0Auseful%20auxiliary%20training%20objective%20in%20post-training.%20Our%20code%2C%20data%2C%20and%0Amodels%20will%20be%20released%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24175v2&entry.124074799=Read"},
{"title": "Towards Understanding the Nature of Attention with Low-Rank Sparse\n  Decomposition", "author": "Zhengfu He and Junxuan Wang and Rui Lin and Xuyang Ge and Wentao Shu and Qiong Tang and Junping Zhang and Xipeng Qiu", "abstract": "  We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of\nTransformer attention layers to disentangle original Multi Head Self Attention\n(MHSA) into individually comprehensible components. Lorsa is designed to\naddress the challenge of attention superposition to understand\nattention-mediated interaction between features in different token positions.\nWe show that Lorsa heads find cleaner and finer-grained versions of previously\ndiscovered MHSA behaviors like induction heads, successor heads and attention\nsink behavior (i.e., heavily attending to the first token). Lorsa and Sparse\nAutoencoder (SAE) are both sparse dictionary learning methods applied to\ndifferent Transformer components, and lead to consistent findings in many ways.\nFor instance, we discover a comprehensive family of arithmetic-specific Lorsa\nheads, each corresponding to an atomic operation in Llama-3.1-8B. Automated\ninterpretability analysis indicates that Lorsa achieves parity with SAE in\ninterpretability while Lorsa exhibits superior circuit discovery properties,\nespecially for features computed collectively by multiple MHSA heads. We also\nconduct extensive experiments on architectural design ablation, Lorsa scaling\nlaw and error analysis.\n", "link": "http://arxiv.org/abs/2504.20938v1", "date": "2025-04-29", "relevancy": 2.5655, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5248}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20the%20Nature%20of%20Attention%20with%20Low-Rank%20Sparse%0A%20%20Decomposition&body=Title%3A%20Towards%20Understanding%20the%20Nature%20of%20Attention%20with%20Low-Rank%20Sparse%0A%20%20Decomposition%0AAuthor%3A%20Zhengfu%20He%20and%20Junxuan%20Wang%20and%20Rui%20Lin%20and%20Xuyang%20Ge%20and%20Wentao%20Shu%20and%20Qiong%20Tang%20and%20Junping%20Zhang%20and%20Xipeng%20Qiu%0AAbstract%3A%20%20%20We%20propose%20Low-Rank%20Sparse%20Attention%20%28Lorsa%29%2C%20a%20sparse%20replacement%20model%20of%0ATransformer%20attention%20layers%20to%20disentangle%20original%20Multi%20Head%20Self%20Attention%0A%28MHSA%29%20into%20individually%20comprehensible%20components.%20Lorsa%20is%20designed%20to%0Aaddress%20the%20challenge%20of%20attention%20superposition%20to%20understand%0Aattention-mediated%20interaction%20between%20features%20in%20different%20token%20positions.%0AWe%20show%20that%20Lorsa%20heads%20find%20cleaner%20and%20finer-grained%20versions%20of%20previously%0Adiscovered%20MHSA%20behaviors%20like%20induction%20heads%2C%20successor%20heads%20and%20attention%0Asink%20behavior%20%28i.e.%2C%20heavily%20attending%20to%20the%20first%20token%29.%20Lorsa%20and%20Sparse%0AAutoencoder%20%28SAE%29%20are%20both%20sparse%20dictionary%20learning%20methods%20applied%20to%0Adifferent%20Transformer%20components%2C%20and%20lead%20to%20consistent%20findings%20in%20many%20ways.%0AFor%20instance%2C%20we%20discover%20a%20comprehensive%20family%20of%20arithmetic-specific%20Lorsa%0Aheads%2C%20each%20corresponding%20to%20an%20atomic%20operation%20in%20Llama-3.1-8B.%20Automated%0Ainterpretability%20analysis%20indicates%20that%20Lorsa%20achieves%20parity%20with%20SAE%20in%0Ainterpretability%20while%20Lorsa%20exhibits%20superior%20circuit%20discovery%20properties%2C%0Aespecially%20for%20features%20computed%20collectively%20by%20multiple%20MHSA%20heads.%20We%20also%0Aconduct%20extensive%20experiments%20on%20architectural%20design%20ablation%2C%20Lorsa%20scaling%0Alaw%20and%20error%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520the%2520Nature%2520of%2520Attention%2520with%2520Low-Rank%2520Sparse%250A%2520%2520Decomposition%26entry.906535625%3DZhengfu%2520He%2520and%2520Junxuan%2520Wang%2520and%2520Rui%2520Lin%2520and%2520Xuyang%2520Ge%2520and%2520Wentao%2520Shu%2520and%2520Qiong%2520Tang%2520and%2520Junping%2520Zhang%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3D%2520%2520We%2520propose%2520Low-Rank%2520Sparse%2520Attention%2520%2528Lorsa%2529%252C%2520a%2520sparse%2520replacement%2520model%2520of%250ATransformer%2520attention%2520layers%2520to%2520disentangle%2520original%2520Multi%2520Head%2520Self%2520Attention%250A%2528MHSA%2529%2520into%2520individually%2520comprehensible%2520components.%2520Lorsa%2520is%2520designed%2520to%250Aaddress%2520the%2520challenge%2520of%2520attention%2520superposition%2520to%2520understand%250Aattention-mediated%2520interaction%2520between%2520features%2520in%2520different%2520token%2520positions.%250AWe%2520show%2520that%2520Lorsa%2520heads%2520find%2520cleaner%2520and%2520finer-grained%2520versions%2520of%2520previously%250Adiscovered%2520MHSA%2520behaviors%2520like%2520induction%2520heads%252C%2520successor%2520heads%2520and%2520attention%250Asink%2520behavior%2520%2528i.e.%252C%2520heavily%2520attending%2520to%2520the%2520first%2520token%2529.%2520Lorsa%2520and%2520Sparse%250AAutoencoder%2520%2528SAE%2529%2520are%2520both%2520sparse%2520dictionary%2520learning%2520methods%2520applied%2520to%250Adifferent%2520Transformer%2520components%252C%2520and%2520lead%2520to%2520consistent%2520findings%2520in%2520many%2520ways.%250AFor%2520instance%252C%2520we%2520discover%2520a%2520comprehensive%2520family%2520of%2520arithmetic-specific%2520Lorsa%250Aheads%252C%2520each%2520corresponding%2520to%2520an%2520atomic%2520operation%2520in%2520Llama-3.1-8B.%2520Automated%250Ainterpretability%2520analysis%2520indicates%2520that%2520Lorsa%2520achieves%2520parity%2520with%2520SAE%2520in%250Ainterpretability%2520while%2520Lorsa%2520exhibits%2520superior%2520circuit%2520discovery%2520properties%252C%250Aespecially%2520for%2520features%2520computed%2520collectively%2520by%2520multiple%2520MHSA%2520heads.%2520We%2520also%250Aconduct%2520extensive%2520experiments%2520on%2520architectural%2520design%2520ablation%252C%2520Lorsa%2520scaling%250Alaw%2520and%2520error%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20the%20Nature%20of%20Attention%20with%20Low-Rank%20Sparse%0A%20%20Decomposition&entry.906535625=Zhengfu%20He%20and%20Junxuan%20Wang%20and%20Rui%20Lin%20and%20Xuyang%20Ge%20and%20Wentao%20Shu%20and%20Qiong%20Tang%20and%20Junping%20Zhang%20and%20Xipeng%20Qiu&entry.1292438233=%20%20We%20propose%20Low-Rank%20Sparse%20Attention%20%28Lorsa%29%2C%20a%20sparse%20replacement%20model%20of%0ATransformer%20attention%20layers%20to%20disentangle%20original%20Multi%20Head%20Self%20Attention%0A%28MHSA%29%20into%20individually%20comprehensible%20components.%20Lorsa%20is%20designed%20to%0Aaddress%20the%20challenge%20of%20attention%20superposition%20to%20understand%0Aattention-mediated%20interaction%20between%20features%20in%20different%20token%20positions.%0AWe%20show%20that%20Lorsa%20heads%20find%20cleaner%20and%20finer-grained%20versions%20of%20previously%0Adiscovered%20MHSA%20behaviors%20like%20induction%20heads%2C%20successor%20heads%20and%20attention%0Asink%20behavior%20%28i.e.%2C%20heavily%20attending%20to%20the%20first%20token%29.%20Lorsa%20and%20Sparse%0AAutoencoder%20%28SAE%29%20are%20both%20sparse%20dictionary%20learning%20methods%20applied%20to%0Adifferent%20Transformer%20components%2C%20and%20lead%20to%20consistent%20findings%20in%20many%20ways.%0AFor%20instance%2C%20we%20discover%20a%20comprehensive%20family%20of%20arithmetic-specific%20Lorsa%0Aheads%2C%20each%20corresponding%20to%20an%20atomic%20operation%20in%20Llama-3.1-8B.%20Automated%0Ainterpretability%20analysis%20indicates%20that%20Lorsa%20achieves%20parity%20with%20SAE%20in%0Ainterpretability%20while%20Lorsa%20exhibits%20superior%20circuit%20discovery%20properties%2C%0Aespecially%20for%20features%20computed%20collectively%20by%20multiple%20MHSA%20heads.%20We%20also%0Aconduct%20extensive%20experiments%20on%20architectural%20design%20ablation%2C%20Lorsa%20scaling%0Alaw%20and%20error%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20938v1&entry.124074799=Read"},
{"title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation", "author": "Guy Hadad and Haggai Roitman and Yotam Eshel and Bracha Shapira and Lior Rokach", "abstract": "  As new products are emerging daily, recommendation systems are required to\nquickly adapt to possible new domains without needing extensive retraining.\nThis work presents ``X-Cross'' -- a novel cross-domain\nsequential-recommendation model that recommends products in new domains by\nintegrating several domain-specific language models; each model is fine-tuned\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\nby layer, X-Cross dynamically refines the representation of each source\nlanguage model by integrating knowledge from all other models. These refined\nrepresentations are propagated from one layer to the next, leveraging the\nactivations from each domain adapter to ensure domain-specific nuances are\npreserved while enabling adaptability across domains. Using Amazon datasets for\nsequential recommendation, X-Cross achieves performance comparable to a model\nthat is fine-tuned with LoRA, while using only 25% of the additional\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\nFurthermore, X-Cross achieves significant improvement in accuracy over\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\nadaptive cross-domain recommendations, reducing computational overhead and\nproviding an efficient solution for data-constrained environments.\n", "link": "http://arxiv.org/abs/2504.20859v1", "date": "2025-04-29", "relevancy": 2.5167, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Cross%3A%20Dynamic%20Integration%20of%20Language%20Models%20for%20Cross-Domain%0A%20%20Sequential%20Recommendation&body=Title%3A%20X-Cross%3A%20Dynamic%20Integration%20of%20Language%20Models%20for%20Cross-Domain%0A%20%20Sequential%20Recommendation%0AAuthor%3A%20Guy%20Hadad%20and%20Haggai%20Roitman%20and%20Yotam%20Eshel%20and%20Bracha%20Shapira%20and%20Lior%20Rokach%0AAbstract%3A%20%20%20As%20new%20products%20are%20emerging%20daily%2C%20recommendation%20systems%20are%20required%20to%0Aquickly%20adapt%20to%20possible%20new%20domains%20without%20needing%20extensive%20retraining.%0AThis%20work%20presents%20%60%60X-Cross%27%27%20--%20a%20novel%20cross-domain%0Asequential-recommendation%20model%20that%20recommends%20products%20in%20new%20domains%20by%0Aintegrating%20several%20domain-specific%20language%20models%3B%20each%20model%20is%20fine-tuned%0Awith%20low-rank%20adapters%20%28LoRA%29.%20Given%20a%20recommendation%20prompt%2C%20operating%20layer%0Aby%20layer%2C%20X-Cross%20dynamically%20refines%20the%20representation%20of%20each%20source%0Alanguage%20model%20by%20integrating%20knowledge%20from%20all%20other%20models.%20These%20refined%0Arepresentations%20are%20propagated%20from%20one%20layer%20to%20the%20next%2C%20leveraging%20the%0Aactivations%20from%20each%20domain%20adapter%20to%20ensure%20domain-specific%20nuances%20are%0Apreserved%20while%20enabling%20adaptability%20across%20domains.%20Using%20Amazon%20datasets%20for%0Asequential%20recommendation%2C%20X-Cross%20achieves%20performance%20comparable%20to%20a%20model%0Athat%20is%20fine-tuned%20with%20LoRA%2C%20while%20using%20only%2025%25%20of%20the%20additional%0Aparameters.%20In%20cross-domain%20tasks%2C%20such%20as%20adapting%20from%20Toys%20domain%20to%20Tools%2C%0AElectronics%20or%20Sports%2C%20X-Cross%20demonstrates%20robust%20performance%2C%20while%20requiring%0Aabout%2050%25-75%25%20less%20fine-tuning%20data%20than%20LoRA%20to%20make%20fine-tuning%20effective.%0AFurthermore%2C%20X-Cross%20achieves%20significant%20improvement%20in%20accuracy%20over%0Aalternative%20cross-domain%20baselines.%20Overall%2C%20X-Cross%20enables%20scalable%20and%0Aadaptive%20cross-domain%20recommendations%2C%20reducing%20computational%20overhead%20and%0Aproviding%20an%20efficient%20solution%20for%20data-constrained%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Cross%253A%2520Dynamic%2520Integration%2520of%2520Language%2520Models%2520for%2520Cross-Domain%250A%2520%2520Sequential%2520Recommendation%26entry.906535625%3DGuy%2520Hadad%2520and%2520Haggai%2520Roitman%2520and%2520Yotam%2520Eshel%2520and%2520Bracha%2520Shapira%2520and%2520Lior%2520Rokach%26entry.1292438233%3D%2520%2520As%2520new%2520products%2520are%2520emerging%2520daily%252C%2520recommendation%2520systems%2520are%2520required%2520to%250Aquickly%2520adapt%2520to%2520possible%2520new%2520domains%2520without%2520needing%2520extensive%2520retraining.%250AThis%2520work%2520presents%2520%2560%2560X-Cross%2527%2527%2520--%2520a%2520novel%2520cross-domain%250Asequential-recommendation%2520model%2520that%2520recommends%2520products%2520in%2520new%2520domains%2520by%250Aintegrating%2520several%2520domain-specific%2520language%2520models%253B%2520each%2520model%2520is%2520fine-tuned%250Awith%2520low-rank%2520adapters%2520%2528LoRA%2529.%2520Given%2520a%2520recommendation%2520prompt%252C%2520operating%2520layer%250Aby%2520layer%252C%2520X-Cross%2520dynamically%2520refines%2520the%2520representation%2520of%2520each%2520source%250Alanguage%2520model%2520by%2520integrating%2520knowledge%2520from%2520all%2520other%2520models.%2520These%2520refined%250Arepresentations%2520are%2520propagated%2520from%2520one%2520layer%2520to%2520the%2520next%252C%2520leveraging%2520the%250Aactivations%2520from%2520each%2520domain%2520adapter%2520to%2520ensure%2520domain-specific%2520nuances%2520are%250Apreserved%2520while%2520enabling%2520adaptability%2520across%2520domains.%2520Using%2520Amazon%2520datasets%2520for%250Asequential%2520recommendation%252C%2520X-Cross%2520achieves%2520performance%2520comparable%2520to%2520a%2520model%250Athat%2520is%2520fine-tuned%2520with%2520LoRA%252C%2520while%2520using%2520only%252025%2525%2520of%2520the%2520additional%250Aparameters.%2520In%2520cross-domain%2520tasks%252C%2520such%2520as%2520adapting%2520from%2520Toys%2520domain%2520to%2520Tools%252C%250AElectronics%2520or%2520Sports%252C%2520X-Cross%2520demonstrates%2520robust%2520performance%252C%2520while%2520requiring%250Aabout%252050%2525-75%2525%2520less%2520fine-tuning%2520data%2520than%2520LoRA%2520to%2520make%2520fine-tuning%2520effective.%250AFurthermore%252C%2520X-Cross%2520achieves%2520significant%2520improvement%2520in%2520accuracy%2520over%250Aalternative%2520cross-domain%2520baselines.%2520Overall%252C%2520X-Cross%2520enables%2520scalable%2520and%250Aadaptive%2520cross-domain%2520recommendations%252C%2520reducing%2520computational%2520overhead%2520and%250Aproviding%2520an%2520efficient%2520solution%2520for%2520data-constrained%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Cross%3A%20Dynamic%20Integration%20of%20Language%20Models%20for%20Cross-Domain%0A%20%20Sequential%20Recommendation&entry.906535625=Guy%20Hadad%20and%20Haggai%20Roitman%20and%20Yotam%20Eshel%20and%20Bracha%20Shapira%20and%20Lior%20Rokach&entry.1292438233=%20%20As%20new%20products%20are%20emerging%20daily%2C%20recommendation%20systems%20are%20required%20to%0Aquickly%20adapt%20to%20possible%20new%20domains%20without%20needing%20extensive%20retraining.%0AThis%20work%20presents%20%60%60X-Cross%27%27%20--%20a%20novel%20cross-domain%0Asequential-recommendation%20model%20that%20recommends%20products%20in%20new%20domains%20by%0Aintegrating%20several%20domain-specific%20language%20models%3B%20each%20model%20is%20fine-tuned%0Awith%20low-rank%20adapters%20%28LoRA%29.%20Given%20a%20recommendation%20prompt%2C%20operating%20layer%0Aby%20layer%2C%20X-Cross%20dynamically%20refines%20the%20representation%20of%20each%20source%0Alanguage%20model%20by%20integrating%20knowledge%20from%20all%20other%20models.%20These%20refined%0Arepresentations%20are%20propagated%20from%20one%20layer%20to%20the%20next%2C%20leveraging%20the%0Aactivations%20from%20each%20domain%20adapter%20to%20ensure%20domain-specific%20nuances%20are%0Apreserved%20while%20enabling%20adaptability%20across%20domains.%20Using%20Amazon%20datasets%20for%0Asequential%20recommendation%2C%20X-Cross%20achieves%20performance%20comparable%20to%20a%20model%0Athat%20is%20fine-tuned%20with%20LoRA%2C%20while%20using%20only%2025%25%20of%20the%20additional%0Aparameters.%20In%20cross-domain%20tasks%2C%20such%20as%20adapting%20from%20Toys%20domain%20to%20Tools%2C%0AElectronics%20or%20Sports%2C%20X-Cross%20demonstrates%20robust%20performance%2C%20while%20requiring%0Aabout%2050%25-75%25%20less%20fine-tuning%20data%20than%20LoRA%20to%20make%20fine-tuning%20effective.%0AFurthermore%2C%20X-Cross%20achieves%20significant%20improvement%20in%20accuracy%20over%0Aalternative%20cross-domain%20baselines.%20Overall%2C%20X-Cross%20enables%20scalable%20and%0Aadaptive%20cross-domain%20recommendations%2C%20reducing%20computational%20overhead%20and%0Aproviding%20an%20efficient%20solution%20for%20data-constrained%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20859v1&entry.124074799=Read"},
{"title": "Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block\n  for Voice Conversion", "author": "Na Li and Chuke Wang and Yu Gu and Zhifeng Li", "abstract": "  Voice conversion (VC) transforms source speech into a target voice by\npreserving the content. However, timbre information from the source speaker is\ninherently embedded in the content representations, causing significant timbre\nleakage and reducing similarity to the target speaker. To address this, we\nintroduce a residual block to a content extractor. The residual block consists\nof two weighted branches: 1) universal semantic dictionary based Content\nFeature Re-expression (CFR) module, supplying timbre-free content\nrepresentation. 2) skip connection to the original content layer, providing\ncomplementary fine-grained information. In the CFR module, each dictionary\nentry in the universal semantic dictionary represents a phoneme class, computed\nstatistically using speech from multiple speakers, creating a stable,\nspeaker-independent semantic set. We introduce a CFR method to obtain\ntimbre-free content representations by expressing each content frame as a\nweighted linear combination of dictionary entries using corresponding phoneme\nposteriors as weights. Extensive experiments across various VC frameworks\ndemonstrate that our approach effectively mitigates timbre leakage and\nsignificantly improves similarity to the target speaker.\n", "link": "http://arxiv.org/abs/2504.08524v2", "date": "2025-04-29", "relevancy": 2.4663, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4953}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20Timbre%20Leakage%20with%20Universal%20Semantic%20Mapping%20Residual%20Block%0A%20%20for%20Voice%20Conversion&body=Title%3A%20Mitigating%20Timbre%20Leakage%20with%20Universal%20Semantic%20Mapping%20Residual%20Block%0A%20%20for%20Voice%20Conversion%0AAuthor%3A%20Na%20Li%20and%20Chuke%20Wang%20and%20Yu%20Gu%20and%20Zhifeng%20Li%0AAbstract%3A%20%20%20Voice%20conversion%20%28VC%29%20transforms%20source%20speech%20into%20a%20target%20voice%20by%0Apreserving%20the%20content.%20However%2C%20timbre%20information%20from%20the%20source%20speaker%20is%0Ainherently%20embedded%20in%20the%20content%20representations%2C%20causing%20significant%20timbre%0Aleakage%20and%20reducing%20similarity%20to%20the%20target%20speaker.%20To%20address%20this%2C%20we%0Aintroduce%20a%20residual%20block%20to%20a%20content%20extractor.%20The%20residual%20block%20consists%0Aof%20two%20weighted%20branches%3A%201%29%20universal%20semantic%20dictionary%20based%20Content%0AFeature%20Re-expression%20%28CFR%29%20module%2C%20supplying%20timbre-free%20content%0Arepresentation.%202%29%20skip%20connection%20to%20the%20original%20content%20layer%2C%20providing%0Acomplementary%20fine-grained%20information.%20In%20the%20CFR%20module%2C%20each%20dictionary%0Aentry%20in%20the%20universal%20semantic%20dictionary%20represents%20a%20phoneme%20class%2C%20computed%0Astatistically%20using%20speech%20from%20multiple%20speakers%2C%20creating%20a%20stable%2C%0Aspeaker-independent%20semantic%20set.%20We%20introduce%20a%20CFR%20method%20to%20obtain%0Atimbre-free%20content%20representations%20by%20expressing%20each%20content%20frame%20as%20a%0Aweighted%20linear%20combination%20of%20dictionary%20entries%20using%20corresponding%20phoneme%0Aposteriors%20as%20weights.%20Extensive%20experiments%20across%20various%20VC%20frameworks%0Ademonstrate%20that%20our%20approach%20effectively%20mitigates%20timbre%20leakage%20and%0Asignificantly%20improves%20similarity%20to%20the%20target%20speaker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.08524v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520Timbre%2520Leakage%2520with%2520Universal%2520Semantic%2520Mapping%2520Residual%2520Block%250A%2520%2520for%2520Voice%2520Conversion%26entry.906535625%3DNa%2520Li%2520and%2520Chuke%2520Wang%2520and%2520Yu%2520Gu%2520and%2520Zhifeng%2520Li%26entry.1292438233%3D%2520%2520Voice%2520conversion%2520%2528VC%2529%2520transforms%2520source%2520speech%2520into%2520a%2520target%2520voice%2520by%250Apreserving%2520the%2520content.%2520However%252C%2520timbre%2520information%2520from%2520the%2520source%2520speaker%2520is%250Ainherently%2520embedded%2520in%2520the%2520content%2520representations%252C%2520causing%2520significant%2520timbre%250Aleakage%2520and%2520reducing%2520similarity%2520to%2520the%2520target%2520speaker.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520residual%2520block%2520to%2520a%2520content%2520extractor.%2520The%2520residual%2520block%2520consists%250Aof%2520two%2520weighted%2520branches%253A%25201%2529%2520universal%2520semantic%2520dictionary%2520based%2520Content%250AFeature%2520Re-expression%2520%2528CFR%2529%2520module%252C%2520supplying%2520timbre-free%2520content%250Arepresentation.%25202%2529%2520skip%2520connection%2520to%2520the%2520original%2520content%2520layer%252C%2520providing%250Acomplementary%2520fine-grained%2520information.%2520In%2520the%2520CFR%2520module%252C%2520each%2520dictionary%250Aentry%2520in%2520the%2520universal%2520semantic%2520dictionary%2520represents%2520a%2520phoneme%2520class%252C%2520computed%250Astatistically%2520using%2520speech%2520from%2520multiple%2520speakers%252C%2520creating%2520a%2520stable%252C%250Aspeaker-independent%2520semantic%2520set.%2520We%2520introduce%2520a%2520CFR%2520method%2520to%2520obtain%250Atimbre-free%2520content%2520representations%2520by%2520expressing%2520each%2520content%2520frame%2520as%2520a%250Aweighted%2520linear%2520combination%2520of%2520dictionary%2520entries%2520using%2520corresponding%2520phoneme%250Aposteriors%2520as%2520weights.%2520Extensive%2520experiments%2520across%2520various%2520VC%2520frameworks%250Ademonstrate%2520that%2520our%2520approach%2520effectively%2520mitigates%2520timbre%2520leakage%2520and%250Asignificantly%2520improves%2520similarity%2520to%2520the%2520target%2520speaker.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.08524v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20Timbre%20Leakage%20with%20Universal%20Semantic%20Mapping%20Residual%20Block%0A%20%20for%20Voice%20Conversion&entry.906535625=Na%20Li%20and%20Chuke%20Wang%20and%20Yu%20Gu%20and%20Zhifeng%20Li&entry.1292438233=%20%20Voice%20conversion%20%28VC%29%20transforms%20source%20speech%20into%20a%20target%20voice%20by%0Apreserving%20the%20content.%20However%2C%20timbre%20information%20from%20the%20source%20speaker%20is%0Ainherently%20embedded%20in%20the%20content%20representations%2C%20causing%20significant%20timbre%0Aleakage%20and%20reducing%20similarity%20to%20the%20target%20speaker.%20To%20address%20this%2C%20we%0Aintroduce%20a%20residual%20block%20to%20a%20content%20extractor.%20The%20residual%20block%20consists%0Aof%20two%20weighted%20branches%3A%201%29%20universal%20semantic%20dictionary%20based%20Content%0AFeature%20Re-expression%20%28CFR%29%20module%2C%20supplying%20timbre-free%20content%0Arepresentation.%202%29%20skip%20connection%20to%20the%20original%20content%20layer%2C%20providing%0Acomplementary%20fine-grained%20information.%20In%20the%20CFR%20module%2C%20each%20dictionary%0Aentry%20in%20the%20universal%20semantic%20dictionary%20represents%20a%20phoneme%20class%2C%20computed%0Astatistically%20using%20speech%20from%20multiple%20speakers%2C%20creating%20a%20stable%2C%0Aspeaker-independent%20semantic%20set.%20We%20introduce%20a%20CFR%20method%20to%20obtain%0Atimbre-free%20content%20representations%20by%20expressing%20each%20content%20frame%20as%20a%0Aweighted%20linear%20combination%20of%20dictionary%20entries%20using%20corresponding%20phoneme%0Aposteriors%20as%20weights.%20Extensive%20experiments%20across%20various%20VC%20frameworks%0Ademonstrate%20that%20our%20approach%20effectively%20mitigates%20timbre%20leakage%20and%0Asignificantly%20improves%20similarity%20to%20the%20target%20speaker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.08524v2&entry.124074799=Read"},
{"title": "Reinforcement Learning for LLM Reasoning Under Memory Constraints", "author": "Alan Lee and Harry Tong", "abstract": "  We explore reinforcement learning (RL) techniques to enhance reasoning within\ntargeted problem spaces in large language models (LLMs) under memory and\ncompute constraints. Our focus is on critic-free methods compatible with LoRA\nfine-tuning on a single 40GB GPU, a common limitation in academic settings. We\nintroduce S-GRPO, a memory-efficient variant of Group Relative Policy\nOptimization, and T-SPMO, a token-level prefix matching strategy for\nfine-grained credit assignment. Despite limited resources, when used to\nfine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark\naccuracy from 46% to above 70% using LoRA training. T-SPMO also excels in\nmulti-digit multiplication tasks, underscoring the potential of RL fine-tuning\nunder hardware constraints. Additionally, we find that our full-token GRPO\nbaseline under LoRA fine-tuning did not improve model performance (compared to\nbase model) on either task, suggesting that our memory-efficient methods may\nact as a form of regularization that stabilizes training when only a small\nsubset of parameters are updated.\n", "link": "http://arxiv.org/abs/2504.20834v1", "date": "2025-04-29", "relevancy": 2.451, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20for%20LLM%20Reasoning%20Under%20Memory%20Constraints&body=Title%3A%20Reinforcement%20Learning%20for%20LLM%20Reasoning%20Under%20Memory%20Constraints%0AAuthor%3A%20Alan%20Lee%20and%20Harry%20Tong%0AAbstract%3A%20%20%20We%20explore%20reinforcement%20learning%20%28RL%29%20techniques%20to%20enhance%20reasoning%20within%0Atargeted%20problem%20spaces%20in%20large%20language%20models%20%28LLMs%29%20under%20memory%20and%0Acompute%20constraints.%20Our%20focus%20is%20on%20critic-free%20methods%20compatible%20with%20LoRA%0Afine-tuning%20on%20a%20single%2040GB%20GPU%2C%20a%20common%20limitation%20in%20academic%20settings.%20We%0Aintroduce%20S-GRPO%2C%20a%20memory-efficient%20variant%20of%20Group%20Relative%20Policy%0AOptimization%2C%20and%20T-SPMO%2C%20a%20token-level%20prefix%20matching%20strategy%20for%0Afine-grained%20credit%20assignment.%20Despite%20limited%20resources%2C%20when%20used%20to%0Afine-tune%20Qwen2-1.5B%20both%20methods%20significantly%20improve%20SVAMP%20benchmark%0Aaccuracy%20from%2046%25%20to%20above%2070%25%20using%20LoRA%20training.%20T-SPMO%20also%20excels%20in%0Amulti-digit%20multiplication%20tasks%2C%20underscoring%20the%20potential%20of%20RL%20fine-tuning%0Aunder%20hardware%20constraints.%20Additionally%2C%20we%20find%20that%20our%20full-token%20GRPO%0Abaseline%20under%20LoRA%20fine-tuning%20did%20not%20improve%20model%20performance%20%28compared%20to%0Abase%20model%29%20on%20either%20task%2C%20suggesting%20that%20our%20memory-efficient%20methods%20may%0Aact%20as%20a%20form%20of%20regularization%20that%20stabilizes%20training%20when%20only%20a%20small%0Asubset%20of%20parameters%20are%20updated.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520for%2520LLM%2520Reasoning%2520Under%2520Memory%2520Constraints%26entry.906535625%3DAlan%2520Lee%2520and%2520Harry%2520Tong%26entry.1292438233%3D%2520%2520We%2520explore%2520reinforcement%2520learning%2520%2528RL%2529%2520techniques%2520to%2520enhance%2520reasoning%2520within%250Atargeted%2520problem%2520spaces%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520under%2520memory%2520and%250Acompute%2520constraints.%2520Our%2520focus%2520is%2520on%2520critic-free%2520methods%2520compatible%2520with%2520LoRA%250Afine-tuning%2520on%2520a%2520single%252040GB%2520GPU%252C%2520a%2520common%2520limitation%2520in%2520academic%2520settings.%2520We%250Aintroduce%2520S-GRPO%252C%2520a%2520memory-efficient%2520variant%2520of%2520Group%2520Relative%2520Policy%250AOptimization%252C%2520and%2520T-SPMO%252C%2520a%2520token-level%2520prefix%2520matching%2520strategy%2520for%250Afine-grained%2520credit%2520assignment.%2520Despite%2520limited%2520resources%252C%2520when%2520used%2520to%250Afine-tune%2520Qwen2-1.5B%2520both%2520methods%2520significantly%2520improve%2520SVAMP%2520benchmark%250Aaccuracy%2520from%252046%2525%2520to%2520above%252070%2525%2520using%2520LoRA%2520training.%2520T-SPMO%2520also%2520excels%2520in%250Amulti-digit%2520multiplication%2520tasks%252C%2520underscoring%2520the%2520potential%2520of%2520RL%2520fine-tuning%250Aunder%2520hardware%2520constraints.%2520Additionally%252C%2520we%2520find%2520that%2520our%2520full-token%2520GRPO%250Abaseline%2520under%2520LoRA%2520fine-tuning%2520did%2520not%2520improve%2520model%2520performance%2520%2528compared%2520to%250Abase%2520model%2529%2520on%2520either%2520task%252C%2520suggesting%2520that%2520our%2520memory-efficient%2520methods%2520may%250Aact%2520as%2520a%2520form%2520of%2520regularization%2520that%2520stabilizes%2520training%2520when%2520only%2520a%2520small%250Asubset%2520of%2520parameters%2520are%2520updated.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20for%20LLM%20Reasoning%20Under%20Memory%20Constraints&entry.906535625=Alan%20Lee%20and%20Harry%20Tong&entry.1292438233=%20%20We%20explore%20reinforcement%20learning%20%28RL%29%20techniques%20to%20enhance%20reasoning%20within%0Atargeted%20problem%20spaces%20in%20large%20language%20models%20%28LLMs%29%20under%20memory%20and%0Acompute%20constraints.%20Our%20focus%20is%20on%20critic-free%20methods%20compatible%20with%20LoRA%0Afine-tuning%20on%20a%20single%2040GB%20GPU%2C%20a%20common%20limitation%20in%20academic%20settings.%20We%0Aintroduce%20S-GRPO%2C%20a%20memory-efficient%20variant%20of%20Group%20Relative%20Policy%0AOptimization%2C%20and%20T-SPMO%2C%20a%20token-level%20prefix%20matching%20strategy%20for%0Afine-grained%20credit%20assignment.%20Despite%20limited%20resources%2C%20when%20used%20to%0Afine-tune%20Qwen2-1.5B%20both%20methods%20significantly%20improve%20SVAMP%20benchmark%0Aaccuracy%20from%2046%25%20to%20above%2070%25%20using%20LoRA%20training.%20T-SPMO%20also%20excels%20in%0Amulti-digit%20multiplication%20tasks%2C%20underscoring%20the%20potential%20of%20RL%20fine-tuning%0Aunder%20hardware%20constraints.%20Additionally%2C%20we%20find%20that%20our%20full-token%20GRPO%0Abaseline%20under%20LoRA%20fine-tuning%20did%20not%20improve%20model%20performance%20%28compared%20to%0Abase%20model%29%20on%20either%20task%2C%20suggesting%20that%20our%20memory-efficient%20methods%20may%0Aact%20as%20a%20form%20of%20regularization%20that%20stabilizes%20training%20when%20only%20a%20small%0Asubset%20of%20parameters%20are%20updated.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20834v1&entry.124074799=Read"},
{"title": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise\n  Learning", "author": "Weihang Chen and Cheng Yang and Jie Ren and Zhiqiang Li and Zheng Wang", "abstract": "  Real-life deployment of federated Learning (FL) often faces non-IID data,\nwhich leads to poor accuracy and slow convergence. Personalized FL (pFL)\ntackles these issues by tailoring local models to individual data sources and\nusing weighted aggregation methods for client-specific learning. However,\nexisting pFL methods often fail to provide each local model with global\nknowledge on demand while maintaining low computational overhead. Additionally,\nlocal models tend to over-personalize their data during the training process,\npotentially dropping previously acquired global information. We propose FLAYER,\na novel layer-wise learning method for pFL that optimizes local model\npersonalization performance. FLAYER considers the different roles and learning\nabilities of neural network layers of individual local models. It incorporates\nglobal information for each local model as needed to initialize the local model\ncost-effectively. It then dynamically adjusts learning rates for each layer\nduring local training, optimizing the personalized learning process for each\nlocal model while preserving global knowledge. Additionally, to enhance global\nrepresentation in pFL, FLAYER selectively uploads parameters for global\naggregation in a layer-wise manner. We evaluate FLAYER on four representative\ndatasets in computer vision and natural language processing domains. Compared\nto six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on\naverage, by 5.40\\% (up to 14.29\\%).\n", "link": "http://arxiv.org/abs/2412.07062v3", "date": "2025-04-29", "relevancy": 2.4504, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5072}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4835}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Personalized%20Federated%20Learning%20through%20Adaptive%20Layer-Wise%0A%20%20Learning&body=Title%3A%20Optimizing%20Personalized%20Federated%20Learning%20through%20Adaptive%20Layer-Wise%0A%20%20Learning%0AAuthor%3A%20Weihang%20Chen%20and%20Cheng%20Yang%20and%20Jie%20Ren%20and%20Zhiqiang%20Li%20and%20Zheng%20Wang%0AAbstract%3A%20%20%20Real-life%20deployment%20of%20federated%20Learning%20%28FL%29%20often%20faces%20non-IID%20data%2C%0Awhich%20leads%20to%20poor%20accuracy%20and%20slow%20convergence.%20Personalized%20FL%20%28pFL%29%0Atackles%20these%20issues%20by%20tailoring%20local%20models%20to%20individual%20data%20sources%20and%0Ausing%20weighted%20aggregation%20methods%20for%20client-specific%20learning.%20However%2C%0Aexisting%20pFL%20methods%20often%20fail%20to%20provide%20each%20local%20model%20with%20global%0Aknowledge%20on%20demand%20while%20maintaining%20low%20computational%20overhead.%20Additionally%2C%0Alocal%20models%20tend%20to%20over-personalize%20their%20data%20during%20the%20training%20process%2C%0Apotentially%20dropping%20previously%20acquired%20global%20information.%20We%20propose%20FLAYER%2C%0Aa%20novel%20layer-wise%20learning%20method%20for%20pFL%20that%20optimizes%20local%20model%0Apersonalization%20performance.%20FLAYER%20considers%20the%20different%20roles%20and%20learning%0Aabilities%20of%20neural%20network%20layers%20of%20individual%20local%20models.%20It%20incorporates%0Aglobal%20information%20for%20each%20local%20model%20as%20needed%20to%20initialize%20the%20local%20model%0Acost-effectively.%20It%20then%20dynamically%20adjusts%20learning%20rates%20for%20each%20layer%0Aduring%20local%20training%2C%20optimizing%20the%20personalized%20learning%20process%20for%20each%0Alocal%20model%20while%20preserving%20global%20knowledge.%20Additionally%2C%20to%20enhance%20global%0Arepresentation%20in%20pFL%2C%20FLAYER%20selectively%20uploads%20parameters%20for%20global%0Aaggregation%20in%20a%20layer-wise%20manner.%20We%20evaluate%20FLAYER%20on%20four%20representative%0Adatasets%20in%20computer%20vision%20and%20natural%20language%20processing%20domains.%20Compared%0Ato%20six%20state-of-the-art%20pFL%20methods%2C%20FLAYER%20improves%20the%20inference%20accuracy%2C%20on%0Aaverage%2C%20by%205.40%5C%25%20%28up%20to%2014.29%5C%25%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.07062v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Personalized%2520Federated%2520Learning%2520through%2520Adaptive%2520Layer-Wise%250A%2520%2520Learning%26entry.906535625%3DWeihang%2520Chen%2520and%2520Cheng%2520Yang%2520and%2520Jie%2520Ren%2520and%2520Zhiqiang%2520Li%2520and%2520Zheng%2520Wang%26entry.1292438233%3D%2520%2520Real-life%2520deployment%2520of%2520federated%2520Learning%2520%2528FL%2529%2520often%2520faces%2520non-IID%2520data%252C%250Awhich%2520leads%2520to%2520poor%2520accuracy%2520and%2520slow%2520convergence.%2520Personalized%2520FL%2520%2528pFL%2529%250Atackles%2520these%2520issues%2520by%2520tailoring%2520local%2520models%2520to%2520individual%2520data%2520sources%2520and%250Ausing%2520weighted%2520aggregation%2520methods%2520for%2520client-specific%2520learning.%2520However%252C%250Aexisting%2520pFL%2520methods%2520often%2520fail%2520to%2520provide%2520each%2520local%2520model%2520with%2520global%250Aknowledge%2520on%2520demand%2520while%2520maintaining%2520low%2520computational%2520overhead.%2520Additionally%252C%250Alocal%2520models%2520tend%2520to%2520over-personalize%2520their%2520data%2520during%2520the%2520training%2520process%252C%250Apotentially%2520dropping%2520previously%2520acquired%2520global%2520information.%2520We%2520propose%2520FLAYER%252C%250Aa%2520novel%2520layer-wise%2520learning%2520method%2520for%2520pFL%2520that%2520optimizes%2520local%2520model%250Apersonalization%2520performance.%2520FLAYER%2520considers%2520the%2520different%2520roles%2520and%2520learning%250Aabilities%2520of%2520neural%2520network%2520layers%2520of%2520individual%2520local%2520models.%2520It%2520incorporates%250Aglobal%2520information%2520for%2520each%2520local%2520model%2520as%2520needed%2520to%2520initialize%2520the%2520local%2520model%250Acost-effectively.%2520It%2520then%2520dynamically%2520adjusts%2520learning%2520rates%2520for%2520each%2520layer%250Aduring%2520local%2520training%252C%2520optimizing%2520the%2520personalized%2520learning%2520process%2520for%2520each%250Alocal%2520model%2520while%2520preserving%2520global%2520knowledge.%2520Additionally%252C%2520to%2520enhance%2520global%250Arepresentation%2520in%2520pFL%252C%2520FLAYER%2520selectively%2520uploads%2520parameters%2520for%2520global%250Aaggregation%2520in%2520a%2520layer-wise%2520manner.%2520We%2520evaluate%2520FLAYER%2520on%2520four%2520representative%250Adatasets%2520in%2520computer%2520vision%2520and%2520natural%2520language%2520processing%2520domains.%2520Compared%250Ato%2520six%2520state-of-the-art%2520pFL%2520methods%252C%2520FLAYER%2520improves%2520the%2520inference%2520accuracy%252C%2520on%250Aaverage%252C%2520by%25205.40%255C%2525%2520%2528up%2520to%252014.29%255C%2525%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.07062v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Personalized%20Federated%20Learning%20through%20Adaptive%20Layer-Wise%0A%20%20Learning&entry.906535625=Weihang%20Chen%20and%20Cheng%20Yang%20and%20Jie%20Ren%20and%20Zhiqiang%20Li%20and%20Zheng%20Wang&entry.1292438233=%20%20Real-life%20deployment%20of%20federated%20Learning%20%28FL%29%20often%20faces%20non-IID%20data%2C%0Awhich%20leads%20to%20poor%20accuracy%20and%20slow%20convergence.%20Personalized%20FL%20%28pFL%29%0Atackles%20these%20issues%20by%20tailoring%20local%20models%20to%20individual%20data%20sources%20and%0Ausing%20weighted%20aggregation%20methods%20for%20client-specific%20learning.%20However%2C%0Aexisting%20pFL%20methods%20often%20fail%20to%20provide%20each%20local%20model%20with%20global%0Aknowledge%20on%20demand%20while%20maintaining%20low%20computational%20overhead.%20Additionally%2C%0Alocal%20models%20tend%20to%20over-personalize%20their%20data%20during%20the%20training%20process%2C%0Apotentially%20dropping%20previously%20acquired%20global%20information.%20We%20propose%20FLAYER%2C%0Aa%20novel%20layer-wise%20learning%20method%20for%20pFL%20that%20optimizes%20local%20model%0Apersonalization%20performance.%20FLAYER%20considers%20the%20different%20roles%20and%20learning%0Aabilities%20of%20neural%20network%20layers%20of%20individual%20local%20models.%20It%20incorporates%0Aglobal%20information%20for%20each%20local%20model%20as%20needed%20to%20initialize%20the%20local%20model%0Acost-effectively.%20It%20then%20dynamically%20adjusts%20learning%20rates%20for%20each%20layer%0Aduring%20local%20training%2C%20optimizing%20the%20personalized%20learning%20process%20for%20each%0Alocal%20model%20while%20preserving%20global%20knowledge.%20Additionally%2C%20to%20enhance%20global%0Arepresentation%20in%20pFL%2C%20FLAYER%20selectively%20uploads%20parameters%20for%20global%0Aaggregation%20in%20a%20layer-wise%20manner.%20We%20evaluate%20FLAYER%20on%20four%20representative%0Adatasets%20in%20computer%20vision%20and%20natural%20language%20processing%20domains.%20Compared%0Ato%20six%20state-of-the-art%20pFL%20methods%2C%20FLAYER%20improves%20the%20inference%20accuracy%2C%20on%0Aaverage%2C%20by%205.40%5C%25%20%28up%20to%2014.29%5C%25%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.07062v3&entry.124074799=Read"},
{"title": "Mitigating the Structural Bias in Graph Adversarial Defenses", "author": "Junyuan Fang and Huimin Liu and Han Yang and Jiajing Wu and Zibin Zheng and Chi K. Tse", "abstract": "  In recent years, graph neural networks (GNNs) have shown great potential in\naddressing various graph structure-related downstream tasks. However, recent\nstudies have found that current GNNs are susceptible to malicious adversarial\nattacks. Given the inevitable presence of adversarial attacks in the real\nworld, a variety of defense methods have been proposed to counter these attacks\nand enhance the robustness of GNNs. Despite the commendable performance of\nthese defense methods, we have observed that they tend to exhibit a structural\nbias in terms of their defense capability on nodes with low degree (i.e., tail\nnodes), which is similar to the structural bias of traditional GNNs on nodes\nwith low degree in the clean graph. Therefore, in this work, we propose a\ndefense strategy by including hetero-homo augmented graph construction, $k$NN\naugmented graph construction, and multi-view node-wise attention modules to\nmitigate the structural bias of GNNs against adversarial attacks. Notably, the\nhetero-homo augmented graph consists of removing heterophilic links (i.e.,\nlinks connecting nodes with dissimilar features) globally and adding homophilic\nlinks (i.e., links connecting nodes with similar features) for nodes with low\ndegree. To further enhance the defense capability, an attention mechanism is\nadopted to adaptively combine the representations from the above two kinds of\ngraph views. We conduct extensive experiments to demonstrate the defense and\ndebiasing effect of the proposed strategy on benchmark datasets.\n", "link": "http://arxiv.org/abs/2504.20848v1", "date": "2025-04-29", "relevancy": 2.4402, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5109}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4805}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mitigating%20the%20Structural%20Bias%20in%20Graph%20Adversarial%20Defenses&body=Title%3A%20Mitigating%20the%20Structural%20Bias%20in%20Graph%20Adversarial%20Defenses%0AAuthor%3A%20Junyuan%20Fang%20and%20Huimin%20Liu%20and%20Han%20Yang%20and%20Jiajing%20Wu%20and%20Zibin%20Zheng%20and%20Chi%20K.%20Tse%0AAbstract%3A%20%20%20In%20recent%20years%2C%20graph%20neural%20networks%20%28GNNs%29%20have%20shown%20great%20potential%20in%0Aaddressing%20various%20graph%20structure-related%20downstream%20tasks.%20However%2C%20recent%0Astudies%20have%20found%20that%20current%20GNNs%20are%20susceptible%20to%20malicious%20adversarial%0Aattacks.%20Given%20the%20inevitable%20presence%20of%20adversarial%20attacks%20in%20the%20real%0Aworld%2C%20a%20variety%20of%20defense%20methods%20have%20been%20proposed%20to%20counter%20these%20attacks%0Aand%20enhance%20the%20robustness%20of%20GNNs.%20Despite%20the%20commendable%20performance%20of%0Athese%20defense%20methods%2C%20we%20have%20observed%20that%20they%20tend%20to%20exhibit%20a%20structural%0Abias%20in%20terms%20of%20their%20defense%20capability%20on%20nodes%20with%20low%20degree%20%28i.e.%2C%20tail%0Anodes%29%2C%20which%20is%20similar%20to%20the%20structural%20bias%20of%20traditional%20GNNs%20on%20nodes%0Awith%20low%20degree%20in%20the%20clean%20graph.%20Therefore%2C%20in%20this%20work%2C%20we%20propose%20a%0Adefense%20strategy%20by%20including%20hetero-homo%20augmented%20graph%20construction%2C%20%24k%24NN%0Aaugmented%20graph%20construction%2C%20and%20multi-view%20node-wise%20attention%20modules%20to%0Amitigate%20the%20structural%20bias%20of%20GNNs%20against%20adversarial%20attacks.%20Notably%2C%20the%0Ahetero-homo%20augmented%20graph%20consists%20of%20removing%20heterophilic%20links%20%28i.e.%2C%0Alinks%20connecting%20nodes%20with%20dissimilar%20features%29%20globally%20and%20adding%20homophilic%0Alinks%20%28i.e.%2C%20links%20connecting%20nodes%20with%20similar%20features%29%20for%20nodes%20with%20low%0Adegree.%20To%20further%20enhance%20the%20defense%20capability%2C%20an%20attention%20mechanism%20is%0Aadopted%20to%20adaptively%20combine%20the%20representations%20from%20the%20above%20two%20kinds%20of%0Agraph%20views.%20We%20conduct%20extensive%20experiments%20to%20demonstrate%20the%20defense%20and%0Adebiasing%20effect%20of%20the%20proposed%20strategy%20on%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMitigating%2520the%2520Structural%2520Bias%2520in%2520Graph%2520Adversarial%2520Defenses%26entry.906535625%3DJunyuan%2520Fang%2520and%2520Huimin%2520Liu%2520and%2520Han%2520Yang%2520and%2520Jiajing%2520Wu%2520and%2520Zibin%2520Zheng%2520and%2520Chi%2520K.%2520Tse%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520shown%2520great%2520potential%2520in%250Aaddressing%2520various%2520graph%2520structure-related%2520downstream%2520tasks.%2520However%252C%2520recent%250Astudies%2520have%2520found%2520that%2520current%2520GNNs%2520are%2520susceptible%2520to%2520malicious%2520adversarial%250Aattacks.%2520Given%2520the%2520inevitable%2520presence%2520of%2520adversarial%2520attacks%2520in%2520the%2520real%250Aworld%252C%2520a%2520variety%2520of%2520defense%2520methods%2520have%2520been%2520proposed%2520to%2520counter%2520these%2520attacks%250Aand%2520enhance%2520the%2520robustness%2520of%2520GNNs.%2520Despite%2520the%2520commendable%2520performance%2520of%250Athese%2520defense%2520methods%252C%2520we%2520have%2520observed%2520that%2520they%2520tend%2520to%2520exhibit%2520a%2520structural%250Abias%2520in%2520terms%2520of%2520their%2520defense%2520capability%2520on%2520nodes%2520with%2520low%2520degree%2520%2528i.e.%252C%2520tail%250Anodes%2529%252C%2520which%2520is%2520similar%2520to%2520the%2520structural%2520bias%2520of%2520traditional%2520GNNs%2520on%2520nodes%250Awith%2520low%2520degree%2520in%2520the%2520clean%2520graph.%2520Therefore%252C%2520in%2520this%2520work%252C%2520we%2520propose%2520a%250Adefense%2520strategy%2520by%2520including%2520hetero-homo%2520augmented%2520graph%2520construction%252C%2520%2524k%2524NN%250Aaugmented%2520graph%2520construction%252C%2520and%2520multi-view%2520node-wise%2520attention%2520modules%2520to%250Amitigate%2520the%2520structural%2520bias%2520of%2520GNNs%2520against%2520adversarial%2520attacks.%2520Notably%252C%2520the%250Ahetero-homo%2520augmented%2520graph%2520consists%2520of%2520removing%2520heterophilic%2520links%2520%2528i.e.%252C%250Alinks%2520connecting%2520nodes%2520with%2520dissimilar%2520features%2529%2520globally%2520and%2520adding%2520homophilic%250Alinks%2520%2528i.e.%252C%2520links%2520connecting%2520nodes%2520with%2520similar%2520features%2529%2520for%2520nodes%2520with%2520low%250Adegree.%2520To%2520further%2520enhance%2520the%2520defense%2520capability%252C%2520an%2520attention%2520mechanism%2520is%250Aadopted%2520to%2520adaptively%2520combine%2520the%2520representations%2520from%2520the%2520above%2520two%2520kinds%2520of%250Agraph%2520views.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520demonstrate%2520the%2520defense%2520and%250Adebiasing%2520effect%2520of%2520the%2520proposed%2520strategy%2520on%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mitigating%20the%20Structural%20Bias%20in%20Graph%20Adversarial%20Defenses&entry.906535625=Junyuan%20Fang%20and%20Huimin%20Liu%20and%20Han%20Yang%20and%20Jiajing%20Wu%20and%20Zibin%20Zheng%20and%20Chi%20K.%20Tse&entry.1292438233=%20%20In%20recent%20years%2C%20graph%20neural%20networks%20%28GNNs%29%20have%20shown%20great%20potential%20in%0Aaddressing%20various%20graph%20structure-related%20downstream%20tasks.%20However%2C%20recent%0Astudies%20have%20found%20that%20current%20GNNs%20are%20susceptible%20to%20malicious%20adversarial%0Aattacks.%20Given%20the%20inevitable%20presence%20of%20adversarial%20attacks%20in%20the%20real%0Aworld%2C%20a%20variety%20of%20defense%20methods%20have%20been%20proposed%20to%20counter%20these%20attacks%0Aand%20enhance%20the%20robustness%20of%20GNNs.%20Despite%20the%20commendable%20performance%20of%0Athese%20defense%20methods%2C%20we%20have%20observed%20that%20they%20tend%20to%20exhibit%20a%20structural%0Abias%20in%20terms%20of%20their%20defense%20capability%20on%20nodes%20with%20low%20degree%20%28i.e.%2C%20tail%0Anodes%29%2C%20which%20is%20similar%20to%20the%20structural%20bias%20of%20traditional%20GNNs%20on%20nodes%0Awith%20low%20degree%20in%20the%20clean%20graph.%20Therefore%2C%20in%20this%20work%2C%20we%20propose%20a%0Adefense%20strategy%20by%20including%20hetero-homo%20augmented%20graph%20construction%2C%20%24k%24NN%0Aaugmented%20graph%20construction%2C%20and%20multi-view%20node-wise%20attention%20modules%20to%0Amitigate%20the%20structural%20bias%20of%20GNNs%20against%20adversarial%20attacks.%20Notably%2C%20the%0Ahetero-homo%20augmented%20graph%20consists%20of%20removing%20heterophilic%20links%20%28i.e.%2C%0Alinks%20connecting%20nodes%20with%20dissimilar%20features%29%20globally%20and%20adding%20homophilic%0Alinks%20%28i.e.%2C%20links%20connecting%20nodes%20with%20similar%20features%29%20for%20nodes%20with%20low%0Adegree.%20To%20further%20enhance%20the%20defense%20capability%2C%20an%20attention%20mechanism%20is%0Aadopted%20to%20adaptively%20combine%20the%20representations%20from%20the%20above%20two%20kinds%20of%0Agraph%20views.%20We%20conduct%20extensive%20experiments%20to%20demonstrate%20the%20defense%20and%0Adebiasing%20effect%20of%20the%20proposed%20strategy%20on%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20848v1&entry.124074799=Read"},
{"title": "Fostering Self-Directed Growth with Generative AI: Toward a New Learning\n  Analytics Framework", "author": "Qianrun Mao", "abstract": "  In an era increasingly shaped by decentralized knowledge ecosystems and\npervasive AI technologies, fostering sustainable learner agency has become a\ncritical educational imperative. This study introduces a novel conceptual\nframework integrating Generative Artificial Intelligence and Learning Analytics\nto cultivate Self-Directed Growth, a dynamic competency that enables learners\nto iteratively drive their own developmental pathways across diverse\ncontexts.Building upon critical gaps in current research on Self Directed\nLearning and AI-mediated education, the proposed Aspire to Potentials for\nLearners (A2PL) model reconceptualizes the interplay of learner aspirations,\ncomplex thinking, and summative self-assessment within GAI supported\nenvironments.Methodological implications for future intervention design and\nlearning analytics applications are discussed, positioning Self-Directed Growth\nas a pivotal axis for developing equitable, adaptive, and sustainable learning\nsystems in the digital era.\n", "link": "http://arxiv.org/abs/2504.20851v1", "date": "2025-04-29", "relevancy": 2.3782, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5134}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4599}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fostering%20Self-Directed%20Growth%20with%20Generative%20AI%3A%20Toward%20a%20New%20Learning%0A%20%20Analytics%20Framework&body=Title%3A%20Fostering%20Self-Directed%20Growth%20with%20Generative%20AI%3A%20Toward%20a%20New%20Learning%0A%20%20Analytics%20Framework%0AAuthor%3A%20Qianrun%20Mao%0AAbstract%3A%20%20%20In%20an%20era%20increasingly%20shaped%20by%20decentralized%20knowledge%20ecosystems%20and%0Apervasive%20AI%20technologies%2C%20fostering%20sustainable%20learner%20agency%20has%20become%20a%0Acritical%20educational%20imperative.%20This%20study%20introduces%20a%20novel%20conceptual%0Aframework%20integrating%20Generative%20Artificial%20Intelligence%20and%20Learning%20Analytics%0Ato%20cultivate%20Self-Directed%20Growth%2C%20a%20dynamic%20competency%20that%20enables%20learners%0Ato%20iteratively%20drive%20their%20own%20developmental%20pathways%20across%20diverse%0Acontexts.Building%20upon%20critical%20gaps%20in%20current%20research%20on%20Self%20Directed%0ALearning%20and%20AI-mediated%20education%2C%20the%20proposed%20Aspire%20to%20Potentials%20for%0ALearners%20%28A2PL%29%20model%20reconceptualizes%20the%20interplay%20of%20learner%20aspirations%2C%0Acomplex%20thinking%2C%20and%20summative%20self-assessment%20within%20GAI%20supported%0Aenvironments.Methodological%20implications%20for%20future%20intervention%20design%20and%0Alearning%20analytics%20applications%20are%20discussed%2C%20positioning%20Self-Directed%20Growth%0Aas%20a%20pivotal%20axis%20for%20developing%20equitable%2C%20adaptive%2C%20and%20sustainable%20learning%0Asystems%20in%20the%20digital%20era.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFostering%2520Self-Directed%2520Growth%2520with%2520Generative%2520AI%253A%2520Toward%2520a%2520New%2520Learning%250A%2520%2520Analytics%2520Framework%26entry.906535625%3DQianrun%2520Mao%26entry.1292438233%3D%2520%2520In%2520an%2520era%2520increasingly%2520shaped%2520by%2520decentralized%2520knowledge%2520ecosystems%2520and%250Apervasive%2520AI%2520technologies%252C%2520fostering%2520sustainable%2520learner%2520agency%2520has%2520become%2520a%250Acritical%2520educational%2520imperative.%2520This%2520study%2520introduces%2520a%2520novel%2520conceptual%250Aframework%2520integrating%2520Generative%2520Artificial%2520Intelligence%2520and%2520Learning%2520Analytics%250Ato%2520cultivate%2520Self-Directed%2520Growth%252C%2520a%2520dynamic%2520competency%2520that%2520enables%2520learners%250Ato%2520iteratively%2520drive%2520their%2520own%2520developmental%2520pathways%2520across%2520diverse%250Acontexts.Building%2520upon%2520critical%2520gaps%2520in%2520current%2520research%2520on%2520Self%2520Directed%250ALearning%2520and%2520AI-mediated%2520education%252C%2520the%2520proposed%2520Aspire%2520to%2520Potentials%2520for%250ALearners%2520%2528A2PL%2529%2520model%2520reconceptualizes%2520the%2520interplay%2520of%2520learner%2520aspirations%252C%250Acomplex%2520thinking%252C%2520and%2520summative%2520self-assessment%2520within%2520GAI%2520supported%250Aenvironments.Methodological%2520implications%2520for%2520future%2520intervention%2520design%2520and%250Alearning%2520analytics%2520applications%2520are%2520discussed%252C%2520positioning%2520Self-Directed%2520Growth%250Aas%2520a%2520pivotal%2520axis%2520for%2520developing%2520equitable%252C%2520adaptive%252C%2520and%2520sustainable%2520learning%250Asystems%2520in%2520the%2520digital%2520era.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fostering%20Self-Directed%20Growth%20with%20Generative%20AI%3A%20Toward%20a%20New%20Learning%0A%20%20Analytics%20Framework&entry.906535625=Qianrun%20Mao&entry.1292438233=%20%20In%20an%20era%20increasingly%20shaped%20by%20decentralized%20knowledge%20ecosystems%20and%0Apervasive%20AI%20technologies%2C%20fostering%20sustainable%20learner%20agency%20has%20become%20a%0Acritical%20educational%20imperative.%20This%20study%20introduces%20a%20novel%20conceptual%0Aframework%20integrating%20Generative%20Artificial%20Intelligence%20and%20Learning%20Analytics%0Ato%20cultivate%20Self-Directed%20Growth%2C%20a%20dynamic%20competency%20that%20enables%20learners%0Ato%20iteratively%20drive%20their%20own%20developmental%20pathways%20across%20diverse%0Acontexts.Building%20upon%20critical%20gaps%20in%20current%20research%20on%20Self%20Directed%0ALearning%20and%20AI-mediated%20education%2C%20the%20proposed%20Aspire%20to%20Potentials%20for%0ALearners%20%28A2PL%29%20model%20reconceptualizes%20the%20interplay%20of%20learner%20aspirations%2C%0Acomplex%20thinking%2C%20and%20summative%20self-assessment%20within%20GAI%20supported%0Aenvironments.Methodological%20implications%20for%20future%20intervention%20design%20and%0Alearning%20analytics%20applications%20are%20discussed%2C%20positioning%20Self-Directed%20Growth%0Aas%20a%20pivotal%20axis%20for%20developing%20equitable%2C%20adaptive%2C%20and%20sustainable%20learning%0Asystems%20in%20the%20digital%20era.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20851v1&entry.124074799=Read"},
{"title": "YoChameleon: Personalized Vision and Language Generation", "author": "Thao Nguyen and Krishna Kumar Singh and Jing Shi and Trung Bui and Yong Jae Lee and Yuheng Li", "abstract": "  Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting.\n", "link": "http://arxiv.org/abs/2504.20998v1", "date": "2025-04-29", "relevancy": 2.3191, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5952}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5716}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YoChameleon%3A%20Personalized%20Vision%20and%20Language%20Generation&body=Title%3A%20YoChameleon%3A%20Personalized%20Vision%20and%20Language%20Generation%0AAuthor%3A%20Thao%20Nguyen%20and%20Krishna%20Kumar%20Singh%20and%20Jing%20Shi%20and%20Trung%20Bui%20and%20Yong%20Jae%20Lee%20and%20Yuheng%20Li%0AAbstract%3A%20%20%20Large%20Multimodal%20Models%20%28e.g.%2C%20GPT-4%2C%20Gemini%2C%20Chameleon%29%20have%20evolved%20into%0Apowerful%20tools%20with%20millions%20of%20users.%20However%2C%20they%20remain%20generic%20models%20and%0Alack%20personalized%20knowledge%20of%20specific%20user%20concepts.%20Previous%20work%20has%0Aexplored%20personalization%20for%20text%20generation%2C%20yet%20it%20remains%20unclear%20how%20these%0Amethods%20can%20be%20adapted%20to%20new%20modalities%2C%20such%20as%20image%20generation.%20In%20this%0Apaper%2C%20we%20introduce%20Yo%27Chameleon%2C%20the%20first%20attempt%20to%20study%20personalization%0Afor%20large%20multimodal%20models.%20Given%203-5%20images%20of%20a%20particular%20concept%2C%0AYo%27Chameleon%20leverages%20soft-prompt%20tuning%20to%20embed%20subject-specific%20information%0Ato%20%28i%29%20answer%20questions%20about%20the%20subject%20and%20%28ii%29%20recreate%20pixel-level%20details%0Ato%20produce%20images%20of%20the%20subject%20in%20new%20contexts.%20Yo%27Chameleon%20is%20trained%20with%0A%28i%29%20a%20self-prompting%20optimization%20mechanism%20to%20balance%20performance%20across%0Amultiple%20modalities%2C%20and%20%28ii%29%20a%20%60%60soft-positive%22%20image%20generation%20approach%20to%0Aenhance%20image%20quality%20in%20a%20few-shot%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYoChameleon%253A%2520Personalized%2520Vision%2520and%2520Language%2520Generation%26entry.906535625%3DThao%2520Nguyen%2520and%2520Krishna%2520Kumar%2520Singh%2520and%2520Jing%2520Shi%2520and%2520Trung%2520Bui%2520and%2520Yong%2520Jae%2520Lee%2520and%2520Yuheng%2520Li%26entry.1292438233%3D%2520%2520Large%2520Multimodal%2520Models%2520%2528e.g.%252C%2520GPT-4%252C%2520Gemini%252C%2520Chameleon%2529%2520have%2520evolved%2520into%250Apowerful%2520tools%2520with%2520millions%2520of%2520users.%2520However%252C%2520they%2520remain%2520generic%2520models%2520and%250Alack%2520personalized%2520knowledge%2520of%2520specific%2520user%2520concepts.%2520Previous%2520work%2520has%250Aexplored%2520personalization%2520for%2520text%2520generation%252C%2520yet%2520it%2520remains%2520unclear%2520how%2520these%250Amethods%2520can%2520be%2520adapted%2520to%2520new%2520modalities%252C%2520such%2520as%2520image%2520generation.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520Yo%2527Chameleon%252C%2520the%2520first%2520attempt%2520to%2520study%2520personalization%250Afor%2520large%2520multimodal%2520models.%2520Given%25203-5%2520images%2520of%2520a%2520particular%2520concept%252C%250AYo%2527Chameleon%2520leverages%2520soft-prompt%2520tuning%2520to%2520embed%2520subject-specific%2520information%250Ato%2520%2528i%2529%2520answer%2520questions%2520about%2520the%2520subject%2520and%2520%2528ii%2529%2520recreate%2520pixel-level%2520details%250Ato%2520produce%2520images%2520of%2520the%2520subject%2520in%2520new%2520contexts.%2520Yo%2527Chameleon%2520is%2520trained%2520with%250A%2528i%2529%2520a%2520self-prompting%2520optimization%2520mechanism%2520to%2520balance%2520performance%2520across%250Amultiple%2520modalities%252C%2520and%2520%2528ii%2529%2520a%2520%2560%2560soft-positive%2522%2520image%2520generation%2520approach%2520to%250Aenhance%2520image%2520quality%2520in%2520a%2520few-shot%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YoChameleon%3A%20Personalized%20Vision%20and%20Language%20Generation&entry.906535625=Thao%20Nguyen%20and%20Krishna%20Kumar%20Singh%20and%20Jing%20Shi%20and%20Trung%20Bui%20and%20Yong%20Jae%20Lee%20and%20Yuheng%20Li&entry.1292438233=%20%20Large%20Multimodal%20Models%20%28e.g.%2C%20GPT-4%2C%20Gemini%2C%20Chameleon%29%20have%20evolved%20into%0Apowerful%20tools%20with%20millions%20of%20users.%20However%2C%20they%20remain%20generic%20models%20and%0Alack%20personalized%20knowledge%20of%20specific%20user%20concepts.%20Previous%20work%20has%0Aexplored%20personalization%20for%20text%20generation%2C%20yet%20it%20remains%20unclear%20how%20these%0Amethods%20can%20be%20adapted%20to%20new%20modalities%2C%20such%20as%20image%20generation.%20In%20this%0Apaper%2C%20we%20introduce%20Yo%27Chameleon%2C%20the%20first%20attempt%20to%20study%20personalization%0Afor%20large%20multimodal%20models.%20Given%203-5%20images%20of%20a%20particular%20concept%2C%0AYo%27Chameleon%20leverages%20soft-prompt%20tuning%20to%20embed%20subject-specific%20information%0Ato%20%28i%29%20answer%20questions%20about%20the%20subject%20and%20%28ii%29%20recreate%20pixel-level%20details%0Ato%20produce%20images%20of%20the%20subject%20in%20new%20contexts.%20Yo%27Chameleon%20is%20trained%20with%0A%28i%29%20a%20self-prompting%20optimization%20mechanism%20to%20balance%20performance%20across%0Amultiple%20modalities%2C%20and%20%28ii%29%20a%20%60%60soft-positive%22%20image%20generation%20approach%20to%0Aenhance%20image%20quality%20in%20a%20few-shot%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20998v1&entry.124074799=Read"},
{"title": "EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds", "author": "Lu Chen and Yizhou Wang and Shixiang Tang and Qianhong Ma and Tong He and Wanli Ouyang and Xiaowei Zhou and Hujun Bao and Sida Peng", "abstract": "  This paper addresses the task of learning an agent model behaving like\nhumans, which can jointly perceive, predict, and act in egocentric worlds.\nPrevious methods usually train separate models for these three abilities, which\nprevents them from learning from each other. In this paper, we propose a joint\npredictive agent model, named EgoAgent, that simultaneously learns to represent\nthe world, predict future states, and take reasonable actions within a single\ntransformer. EgoAgent introduces two innovations to learn from the causal and\ntemporally intertwined nature of these abilities: (1) Interleaved sequential\nmodeling of states and actions with the causal attention mechanism, and (2) A\njoint embedding-action-prediction architecture featuring temporal asymmetric\npredictor-observer branches. Integrating these designs based on JEPA, EgoAgent\nunifies these capabilities in a cohesive learning framework. Comprehensive\nevaluations of EgoAgent on representative tasks such as image classification,\negocentric future state prediction, and 3D human motion prediction tasks\ndemonstrate the superiority of our method. The code and trained model will be\nreleased for reproducibility.\n", "link": "http://arxiv.org/abs/2502.05857v2", "date": "2025-04-29", "relevancy": 2.3052, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6071}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5933}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoAgent%3A%20A%20Joint%20Predictive%20Agent%20Model%20in%20Egocentric%20Worlds&body=Title%3A%20EgoAgent%3A%20A%20Joint%20Predictive%20Agent%20Model%20in%20Egocentric%20Worlds%0AAuthor%3A%20Lu%20Chen%20and%20Yizhou%20Wang%20and%20Shixiang%20Tang%20and%20Qianhong%20Ma%20and%20Tong%20He%20and%20Wanli%20Ouyang%20and%20Xiaowei%20Zhou%20and%20Hujun%20Bao%20and%20Sida%20Peng%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20task%20of%20learning%20an%20agent%20model%20behaving%20like%0Ahumans%2C%20which%20can%20jointly%20perceive%2C%20predict%2C%20and%20act%20in%20egocentric%20worlds.%0APrevious%20methods%20usually%20train%20separate%20models%20for%20these%20three%20abilities%2C%20which%0Aprevents%20them%20from%20learning%20from%20each%20other.%20In%20this%20paper%2C%20we%20propose%20a%20joint%0Apredictive%20agent%20model%2C%20named%20EgoAgent%2C%20that%20simultaneously%20learns%20to%20represent%0Athe%20world%2C%20predict%20future%20states%2C%20and%20take%20reasonable%20actions%20within%20a%20single%0Atransformer.%20EgoAgent%20introduces%20two%20innovations%20to%20learn%20from%20the%20causal%20and%0Atemporally%20intertwined%20nature%20of%20these%20abilities%3A%20%281%29%20Interleaved%20sequential%0Amodeling%20of%20states%20and%20actions%20with%20the%20causal%20attention%20mechanism%2C%20and%20%282%29%20A%0Ajoint%20embedding-action-prediction%20architecture%20featuring%20temporal%20asymmetric%0Apredictor-observer%20branches.%20Integrating%20these%20designs%20based%20on%20JEPA%2C%20EgoAgent%0Aunifies%20these%20capabilities%20in%20a%20cohesive%20learning%20framework.%20Comprehensive%0Aevaluations%20of%20EgoAgent%20on%20representative%20tasks%20such%20as%20image%20classification%2C%0Aegocentric%20future%20state%20prediction%2C%20and%203D%20human%20motion%20prediction%20tasks%0Ademonstrate%20the%20superiority%20of%20our%20method.%20The%20code%20and%20trained%20model%20will%20be%0Areleased%20for%20reproducibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.05857v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoAgent%253A%2520A%2520Joint%2520Predictive%2520Agent%2520Model%2520in%2520Egocentric%2520Worlds%26entry.906535625%3DLu%2520Chen%2520and%2520Yizhou%2520Wang%2520and%2520Shixiang%2520Tang%2520and%2520Qianhong%2520Ma%2520and%2520Tong%2520He%2520and%2520Wanli%2520Ouyang%2520and%2520Xiaowei%2520Zhou%2520and%2520Hujun%2520Bao%2520and%2520Sida%2520Peng%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520task%2520of%2520learning%2520an%2520agent%2520model%2520behaving%2520like%250Ahumans%252C%2520which%2520can%2520jointly%2520perceive%252C%2520predict%252C%2520and%2520act%2520in%2520egocentric%2520worlds.%250APrevious%2520methods%2520usually%2520train%2520separate%2520models%2520for%2520these%2520three%2520abilities%252C%2520which%250Aprevents%2520them%2520from%2520learning%2520from%2520each%2520other.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520joint%250Apredictive%2520agent%2520model%252C%2520named%2520EgoAgent%252C%2520that%2520simultaneously%2520learns%2520to%2520represent%250Athe%2520world%252C%2520predict%2520future%2520states%252C%2520and%2520take%2520reasonable%2520actions%2520within%2520a%2520single%250Atransformer.%2520EgoAgent%2520introduces%2520two%2520innovations%2520to%2520learn%2520from%2520the%2520causal%2520and%250Atemporally%2520intertwined%2520nature%2520of%2520these%2520abilities%253A%2520%25281%2529%2520Interleaved%2520sequential%250Amodeling%2520of%2520states%2520and%2520actions%2520with%2520the%2520causal%2520attention%2520mechanism%252C%2520and%2520%25282%2529%2520A%250Ajoint%2520embedding-action-prediction%2520architecture%2520featuring%2520temporal%2520asymmetric%250Apredictor-observer%2520branches.%2520Integrating%2520these%2520designs%2520based%2520on%2520JEPA%252C%2520EgoAgent%250Aunifies%2520these%2520capabilities%2520in%2520a%2520cohesive%2520learning%2520framework.%2520Comprehensive%250Aevaluations%2520of%2520EgoAgent%2520on%2520representative%2520tasks%2520such%2520as%2520image%2520classification%252C%250Aegocentric%2520future%2520state%2520prediction%252C%2520and%25203D%2520human%2520motion%2520prediction%2520tasks%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520method.%2520The%2520code%2520and%2520trained%2520model%2520will%2520be%250Areleased%2520for%2520reproducibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05857v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoAgent%3A%20A%20Joint%20Predictive%20Agent%20Model%20in%20Egocentric%20Worlds&entry.906535625=Lu%20Chen%20and%20Yizhou%20Wang%20and%20Shixiang%20Tang%20and%20Qianhong%20Ma%20and%20Tong%20He%20and%20Wanli%20Ouyang%20and%20Xiaowei%20Zhou%20and%20Hujun%20Bao%20and%20Sida%20Peng&entry.1292438233=%20%20This%20paper%20addresses%20the%20task%20of%20learning%20an%20agent%20model%20behaving%20like%0Ahumans%2C%20which%20can%20jointly%20perceive%2C%20predict%2C%20and%20act%20in%20egocentric%20worlds.%0APrevious%20methods%20usually%20train%20separate%20models%20for%20these%20three%20abilities%2C%20which%0Aprevents%20them%20from%20learning%20from%20each%20other.%20In%20this%20paper%2C%20we%20propose%20a%20joint%0Apredictive%20agent%20model%2C%20named%20EgoAgent%2C%20that%20simultaneously%20learns%20to%20represent%0Athe%20world%2C%20predict%20future%20states%2C%20and%20take%20reasonable%20actions%20within%20a%20single%0Atransformer.%20EgoAgent%20introduces%20two%20innovations%20to%20learn%20from%20the%20causal%20and%0Atemporally%20intertwined%20nature%20of%20these%20abilities%3A%20%281%29%20Interleaved%20sequential%0Amodeling%20of%20states%20and%20actions%20with%20the%20causal%20attention%20mechanism%2C%20and%20%282%29%20A%0Ajoint%20embedding-action-prediction%20architecture%20featuring%20temporal%20asymmetric%0Apredictor-observer%20branches.%20Integrating%20these%20designs%20based%20on%20JEPA%2C%20EgoAgent%0Aunifies%20these%20capabilities%20in%20a%20cohesive%20learning%20framework.%20Comprehensive%0Aevaluations%20of%20EgoAgent%20on%20representative%20tasks%20such%20as%20image%20classification%2C%0Aegocentric%20future%20state%20prediction%2C%20and%203D%20human%20motion%20prediction%20tasks%0Ademonstrate%20the%20superiority%20of%20our%20method.%20The%20code%20and%20trained%20model%20will%20be%0Areleased%20for%20reproducibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.05857v2&entry.124074799=Read"},
{"title": "Towards Easy and Realistic Network Infrastructure Testing for\n  Large-scale Machine Learning", "author": "Jinsun Yoo and ChonLam Lao and Lianjie Cao and Bob Lantz and Minlan Yu and Tushar Krishna and Puneet Sharma", "abstract": "  This paper lays the foundation for Genie, a testing framework that captures\nthe impact of real hardware network behavior on ML workload performance,\nwithout requiring expensive GPUs. Genie uses CPU-initiated traffic over a\nhardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim\nsimulator to model interaction between the network and the ML workload.\n", "link": "http://arxiv.org/abs/2504.20854v1", "date": "2025-04-29", "relevancy": 2.3029, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5022}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4447}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Easy%20and%20Realistic%20Network%20Infrastructure%20Testing%20for%0A%20%20Large-scale%20Machine%20Learning&body=Title%3A%20Towards%20Easy%20and%20Realistic%20Network%20Infrastructure%20Testing%20for%0A%20%20Large-scale%20Machine%20Learning%0AAuthor%3A%20Jinsun%20Yoo%20and%20ChonLam%20Lao%20and%20Lianjie%20Cao%20and%20Bob%20Lantz%20and%20Minlan%20Yu%20and%20Tushar%20Krishna%20and%20Puneet%20Sharma%0AAbstract%3A%20%20%20This%20paper%20lays%20the%20foundation%20for%20Genie%2C%20a%20testing%20framework%20that%20captures%0Athe%20impact%20of%20real%20hardware%20network%20behavior%20on%20ML%20workload%20performance%2C%0Awithout%20requiring%20expensive%20GPUs.%20Genie%20uses%20CPU-initiated%20traffic%20over%20a%0Ahardware%20testbed%20to%20emulate%20GPU%20to%20GPU%20communication%2C%20and%20adapts%20the%20ASTRA-sim%0Asimulator%20to%20model%20interaction%20between%20the%20network%20and%20the%20ML%20workload.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Easy%2520and%2520Realistic%2520Network%2520Infrastructure%2520Testing%2520for%250A%2520%2520Large-scale%2520Machine%2520Learning%26entry.906535625%3DJinsun%2520Yoo%2520and%2520ChonLam%2520Lao%2520and%2520Lianjie%2520Cao%2520and%2520Bob%2520Lantz%2520and%2520Minlan%2520Yu%2520and%2520Tushar%2520Krishna%2520and%2520Puneet%2520Sharma%26entry.1292438233%3D%2520%2520This%2520paper%2520lays%2520the%2520foundation%2520for%2520Genie%252C%2520a%2520testing%2520framework%2520that%2520captures%250Athe%2520impact%2520of%2520real%2520hardware%2520network%2520behavior%2520on%2520ML%2520workload%2520performance%252C%250Awithout%2520requiring%2520expensive%2520GPUs.%2520Genie%2520uses%2520CPU-initiated%2520traffic%2520over%2520a%250Ahardware%2520testbed%2520to%2520emulate%2520GPU%2520to%2520GPU%2520communication%252C%2520and%2520adapts%2520the%2520ASTRA-sim%250Asimulator%2520to%2520model%2520interaction%2520between%2520the%2520network%2520and%2520the%2520ML%2520workload.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Easy%20and%20Realistic%20Network%20Infrastructure%20Testing%20for%0A%20%20Large-scale%20Machine%20Learning&entry.906535625=Jinsun%20Yoo%20and%20ChonLam%20Lao%20and%20Lianjie%20Cao%20and%20Bob%20Lantz%20and%20Minlan%20Yu%20and%20Tushar%20Krishna%20and%20Puneet%20Sharma&entry.1292438233=%20%20This%20paper%20lays%20the%20foundation%20for%20Genie%2C%20a%20testing%20framework%20that%20captures%0Athe%20impact%20of%20real%20hardware%20network%20behavior%20on%20ML%20workload%20performance%2C%0Awithout%20requiring%20expensive%20GPUs.%20Genie%20uses%20CPU-initiated%20traffic%20over%20a%0Ahardware%20testbed%20to%20emulate%20GPU%20to%20GPU%20communication%2C%20and%20adapts%20the%20ASTRA-sim%0Asimulator%20to%20model%20interaction%20between%20the%20network%20and%20the%20ML%20workload.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20854v1&entry.124074799=Read"},
{"title": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark", "author": "Jasper G\u00f6tting and Pedro Medeiros and Jon G Sanders and Nathaniel Li and Long Phan and Karam Elabd and Lennart Justen and Dan Hendrycks and Seth Donoughe", "abstract": "  We present the Virology Capabilities Test (VCT), a large language model (LLM)\nbenchmark that measures the capability to troubleshoot complex virology\nlaboratory protocols. Constructed from the inputs of dozens of PhD-level expert\nvirologists, VCT consists of $322$ multimodal questions covering fundamental,\ntacit, and visual knowledge that is essential for practical work in virology\nlaboratories. VCT is difficult: expert virologists with access to the internet\nscore an average of $22.1\\%$ on questions specifically in their sub-areas of\nexpertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$\naccuracy, outperforming $94\\%$ of expert virologists even within their\nsub-areas of specialization. The ability to provide expert-level virology\ntroubleshooting is inherently dual-use: it is useful for beneficial research,\nbut it can also be misused. Therefore, the fact that publicly available models\noutperform virologists on VCT raises pressing governance considerations. We\npropose that the capability of LLMs to provide expert-level troubleshooting of\ndual-use virology work should be integrated into existing frameworks for\nhandling dual-use technologies in the life sciences.\n", "link": "http://arxiv.org/abs/2504.16137v2", "date": "2025-04-29", "relevancy": 2.2845, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4697}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virology%20Capabilities%20Test%20%28VCT%29%3A%20A%20Multimodal%20Virology%20Q%26A%20Benchmark&body=Title%3A%20Virology%20Capabilities%20Test%20%28VCT%29%3A%20A%20Multimodal%20Virology%20Q%26A%20Benchmark%0AAuthor%3A%20Jasper%20G%C3%B6tting%20and%20Pedro%20Medeiros%20and%20Jon%20G%20Sanders%20and%20Nathaniel%20Li%20and%20Long%20Phan%20and%20Karam%20Elabd%20and%20Lennart%20Justen%20and%20Dan%20Hendrycks%20and%20Seth%20Donoughe%0AAbstract%3A%20%20%20We%20present%20the%20Virology%20Capabilities%20Test%20%28VCT%29%2C%20a%20large%20language%20model%20%28LLM%29%0Abenchmark%20that%20measures%20the%20capability%20to%20troubleshoot%20complex%20virology%0Alaboratory%20protocols.%20Constructed%20from%20the%20inputs%20of%20dozens%20of%20PhD-level%20expert%0Avirologists%2C%20VCT%20consists%20of%20%24322%24%20multimodal%20questions%20covering%20fundamental%2C%0Atacit%2C%20and%20visual%20knowledge%20that%20is%20essential%20for%20practical%20work%20in%20virology%0Alaboratories.%20VCT%20is%20difficult%3A%20expert%20virologists%20with%20access%20to%20the%20internet%0Ascore%20an%20average%20of%20%2422.1%5C%25%24%20on%20questions%20specifically%20in%20their%20sub-areas%20of%0Aexpertise.%20However%2C%20the%20most%20performant%20LLM%2C%20OpenAI%27s%20o3%2C%20reaches%20%2443.8%5C%25%24%0Aaccuracy%2C%20outperforming%20%2494%5C%25%24%20of%20expert%20virologists%20even%20within%20their%0Asub-areas%20of%20specialization.%20The%20ability%20to%20provide%20expert-level%20virology%0Atroubleshooting%20is%20inherently%20dual-use%3A%20it%20is%20useful%20for%20beneficial%20research%2C%0Abut%20it%20can%20also%20be%20misused.%20Therefore%2C%20the%20fact%20that%20publicly%20available%20models%0Aoutperform%20virologists%20on%20VCT%20raises%20pressing%20governance%20considerations.%20We%0Apropose%20that%20the%20capability%20of%20LLMs%20to%20provide%20expert-level%20troubleshooting%20of%0Adual-use%20virology%20work%20should%20be%20integrated%20into%20existing%20frameworks%20for%0Ahandling%20dual-use%20technologies%20in%20the%20life%20sciences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16137v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirology%2520Capabilities%2520Test%2520%2528VCT%2529%253A%2520A%2520Multimodal%2520Virology%2520Q%2526A%2520Benchmark%26entry.906535625%3DJasper%2520G%25C3%25B6tting%2520and%2520Pedro%2520Medeiros%2520and%2520Jon%2520G%2520Sanders%2520and%2520Nathaniel%2520Li%2520and%2520Long%2520Phan%2520and%2520Karam%2520Elabd%2520and%2520Lennart%2520Justen%2520and%2520Dan%2520Hendrycks%2520and%2520Seth%2520Donoughe%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520Virology%2520Capabilities%2520Test%2520%2528VCT%2529%252C%2520a%2520large%2520language%2520model%2520%2528LLM%2529%250Abenchmark%2520that%2520measures%2520the%2520capability%2520to%2520troubleshoot%2520complex%2520virology%250Alaboratory%2520protocols.%2520Constructed%2520from%2520the%2520inputs%2520of%2520dozens%2520of%2520PhD-level%2520expert%250Avirologists%252C%2520VCT%2520consists%2520of%2520%2524322%2524%2520multimodal%2520questions%2520covering%2520fundamental%252C%250Atacit%252C%2520and%2520visual%2520knowledge%2520that%2520is%2520essential%2520for%2520practical%2520work%2520in%2520virology%250Alaboratories.%2520VCT%2520is%2520difficult%253A%2520expert%2520virologists%2520with%2520access%2520to%2520the%2520internet%250Ascore%2520an%2520average%2520of%2520%252422.1%255C%2525%2524%2520on%2520questions%2520specifically%2520in%2520their%2520sub-areas%2520of%250Aexpertise.%2520However%252C%2520the%2520most%2520performant%2520LLM%252C%2520OpenAI%2527s%2520o3%252C%2520reaches%2520%252443.8%255C%2525%2524%250Aaccuracy%252C%2520outperforming%2520%252494%255C%2525%2524%2520of%2520expert%2520virologists%2520even%2520within%2520their%250Asub-areas%2520of%2520specialization.%2520The%2520ability%2520to%2520provide%2520expert-level%2520virology%250Atroubleshooting%2520is%2520inherently%2520dual-use%253A%2520it%2520is%2520useful%2520for%2520beneficial%2520research%252C%250Abut%2520it%2520can%2520also%2520be%2520misused.%2520Therefore%252C%2520the%2520fact%2520that%2520publicly%2520available%2520models%250Aoutperform%2520virologists%2520on%2520VCT%2520raises%2520pressing%2520governance%2520considerations.%2520We%250Apropose%2520that%2520the%2520capability%2520of%2520LLMs%2520to%2520provide%2520expert-level%2520troubleshooting%2520of%250Adual-use%2520virology%2520work%2520should%2520be%2520integrated%2520into%2520existing%2520frameworks%2520for%250Ahandling%2520dual-use%2520technologies%2520in%2520the%2520life%2520sciences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16137v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virology%20Capabilities%20Test%20%28VCT%29%3A%20A%20Multimodal%20Virology%20Q%26A%20Benchmark&entry.906535625=Jasper%20G%C3%B6tting%20and%20Pedro%20Medeiros%20and%20Jon%20G%20Sanders%20and%20Nathaniel%20Li%20and%20Long%20Phan%20and%20Karam%20Elabd%20and%20Lennart%20Justen%20and%20Dan%20Hendrycks%20and%20Seth%20Donoughe&entry.1292438233=%20%20We%20present%20the%20Virology%20Capabilities%20Test%20%28VCT%29%2C%20a%20large%20language%20model%20%28LLM%29%0Abenchmark%20that%20measures%20the%20capability%20to%20troubleshoot%20complex%20virology%0Alaboratory%20protocols.%20Constructed%20from%20the%20inputs%20of%20dozens%20of%20PhD-level%20expert%0Avirologists%2C%20VCT%20consists%20of%20%24322%24%20multimodal%20questions%20covering%20fundamental%2C%0Atacit%2C%20and%20visual%20knowledge%20that%20is%20essential%20for%20practical%20work%20in%20virology%0Alaboratories.%20VCT%20is%20difficult%3A%20expert%20virologists%20with%20access%20to%20the%20internet%0Ascore%20an%20average%20of%20%2422.1%5C%25%24%20on%20questions%20specifically%20in%20their%20sub-areas%20of%0Aexpertise.%20However%2C%20the%20most%20performant%20LLM%2C%20OpenAI%27s%20o3%2C%20reaches%20%2443.8%5C%25%24%0Aaccuracy%2C%20outperforming%20%2494%5C%25%24%20of%20expert%20virologists%20even%20within%20their%0Asub-areas%20of%20specialization.%20The%20ability%20to%20provide%20expert-level%20virology%0Atroubleshooting%20is%20inherently%20dual-use%3A%20it%20is%20useful%20for%20beneficial%20research%2C%0Abut%20it%20can%20also%20be%20misused.%20Therefore%2C%20the%20fact%20that%20publicly%20available%20models%0Aoutperform%20virologists%20on%20VCT%20raises%20pressing%20governance%20considerations.%20We%0Apropose%20that%20the%20capability%20of%20LLMs%20to%20provide%20expert-level%20troubleshooting%20of%0Adual-use%20virology%20work%20should%20be%20integrated%20into%20existing%20frameworks%20for%0Ahandling%20dual-use%20technologies%20in%20the%20life%20sciences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16137v2&entry.124074799=Read"},
{"title": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from\n  Gameplay Recordings", "author": "Florian Vahl and J\u00f6rn Griepenburg and Jan Gutsche and Jasper G\u00fcldenstein and Jianwei Zhang", "abstract": "  This paper introduces SoccerDiffusion, a transformer-based diffusion model\ndesigned to learn end-to-end control policies for humanoid robot soccer\ndirectly from real-world gameplay recordings. Using data collected from RoboCup\ncompetitions, the model predicts joint command trajectories from multi-modal\nsensor inputs, including vision, proprioception, and game state. We employ a\ndistillation technique to enable real-time inference on embedded platforms that\nreduces the multi-step diffusion process to a single step. Our results\ndemonstrate the model's ability to replicate complex motion behaviors such as\nwalking, kicking, and fall recovery both in simulation and on physical robots.\nAlthough high-level tactical behavior remains limited, this work provides a\nrobust foundation for subsequent reinforcement learning or preference\noptimization methods. We release the dataset, pretrained models, and code\nunder: https://bit-bots.github.io/SoccerDiffusion\n", "link": "http://arxiv.org/abs/2504.20808v1", "date": "2025-04-29", "relevancy": 2.2804, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.591}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5826}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoccerDiffusion%3A%20Toward%20Learning%20End-to-End%20Humanoid%20Robot%20Soccer%20from%0A%20%20Gameplay%20Recordings&body=Title%3A%20SoccerDiffusion%3A%20Toward%20Learning%20End-to-End%20Humanoid%20Robot%20Soccer%20from%0A%20%20Gameplay%20Recordings%0AAuthor%3A%20Florian%20Vahl%20and%20J%C3%B6rn%20Griepenburg%20and%20Jan%20Gutsche%20and%20Jasper%20G%C3%BCldenstein%20and%20Jianwei%20Zhang%0AAbstract%3A%20%20%20This%20paper%20introduces%20SoccerDiffusion%2C%20a%20transformer-based%20diffusion%20model%0Adesigned%20to%20learn%20end-to-end%20control%20policies%20for%20humanoid%20robot%20soccer%0Adirectly%20from%20real-world%20gameplay%20recordings.%20Using%20data%20collected%20from%20RoboCup%0Acompetitions%2C%20the%20model%20predicts%20joint%20command%20trajectories%20from%20multi-modal%0Asensor%20inputs%2C%20including%20vision%2C%20proprioception%2C%20and%20game%20state.%20We%20employ%20a%0Adistillation%20technique%20to%20enable%20real-time%20inference%20on%20embedded%20platforms%20that%0Areduces%20the%20multi-step%20diffusion%20process%20to%20a%20single%20step.%20Our%20results%0Ademonstrate%20the%20model%27s%20ability%20to%20replicate%20complex%20motion%20behaviors%20such%20as%0Awalking%2C%20kicking%2C%20and%20fall%20recovery%20both%20in%20simulation%20and%20on%20physical%20robots.%0AAlthough%20high-level%20tactical%20behavior%20remains%20limited%2C%20this%20work%20provides%20a%0Arobust%20foundation%20for%20subsequent%20reinforcement%20learning%20or%20preference%0Aoptimization%20methods.%20We%20release%20the%20dataset%2C%20pretrained%20models%2C%20and%20code%0Aunder%3A%20https%3A//bit-bots.github.io/SoccerDiffusion%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoccerDiffusion%253A%2520Toward%2520Learning%2520End-to-End%2520Humanoid%2520Robot%2520Soccer%2520from%250A%2520%2520Gameplay%2520Recordings%26entry.906535625%3DFlorian%2520Vahl%2520and%2520J%25C3%25B6rn%2520Griepenburg%2520and%2520Jan%2520Gutsche%2520and%2520Jasper%2520G%25C3%25BCldenstein%2520and%2520Jianwei%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520SoccerDiffusion%252C%2520a%2520transformer-based%2520diffusion%2520model%250Adesigned%2520to%2520learn%2520end-to-end%2520control%2520policies%2520for%2520humanoid%2520robot%2520soccer%250Adirectly%2520from%2520real-world%2520gameplay%2520recordings.%2520Using%2520data%2520collected%2520from%2520RoboCup%250Acompetitions%252C%2520the%2520model%2520predicts%2520joint%2520command%2520trajectories%2520from%2520multi-modal%250Asensor%2520inputs%252C%2520including%2520vision%252C%2520proprioception%252C%2520and%2520game%2520state.%2520We%2520employ%2520a%250Adistillation%2520technique%2520to%2520enable%2520real-time%2520inference%2520on%2520embedded%2520platforms%2520that%250Areduces%2520the%2520multi-step%2520diffusion%2520process%2520to%2520a%2520single%2520step.%2520Our%2520results%250Ademonstrate%2520the%2520model%2527s%2520ability%2520to%2520replicate%2520complex%2520motion%2520behaviors%2520such%2520as%250Awalking%252C%2520kicking%252C%2520and%2520fall%2520recovery%2520both%2520in%2520simulation%2520and%2520on%2520physical%2520robots.%250AAlthough%2520high-level%2520tactical%2520behavior%2520remains%2520limited%252C%2520this%2520work%2520provides%2520a%250Arobust%2520foundation%2520for%2520subsequent%2520reinforcement%2520learning%2520or%2520preference%250Aoptimization%2520methods.%2520We%2520release%2520the%2520dataset%252C%2520pretrained%2520models%252C%2520and%2520code%250Aunder%253A%2520https%253A//bit-bots.github.io/SoccerDiffusion%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoccerDiffusion%3A%20Toward%20Learning%20End-to-End%20Humanoid%20Robot%20Soccer%20from%0A%20%20Gameplay%20Recordings&entry.906535625=Florian%20Vahl%20and%20J%C3%B6rn%20Griepenburg%20and%20Jan%20Gutsche%20and%20Jasper%20G%C3%BCldenstein%20and%20Jianwei%20Zhang&entry.1292438233=%20%20This%20paper%20introduces%20SoccerDiffusion%2C%20a%20transformer-based%20diffusion%20model%0Adesigned%20to%20learn%20end-to-end%20control%20policies%20for%20humanoid%20robot%20soccer%0Adirectly%20from%20real-world%20gameplay%20recordings.%20Using%20data%20collected%20from%20RoboCup%0Acompetitions%2C%20the%20model%20predicts%20joint%20command%20trajectories%20from%20multi-modal%0Asensor%20inputs%2C%20including%20vision%2C%20proprioception%2C%20and%20game%20state.%20We%20employ%20a%0Adistillation%20technique%20to%20enable%20real-time%20inference%20on%20embedded%20platforms%20that%0Areduces%20the%20multi-step%20diffusion%20process%20to%20a%20single%20step.%20Our%20results%0Ademonstrate%20the%20model%27s%20ability%20to%20replicate%20complex%20motion%20behaviors%20such%20as%0Awalking%2C%20kicking%2C%20and%20fall%20recovery%20both%20in%20simulation%20and%20on%20physical%20robots.%0AAlthough%20high-level%20tactical%20behavior%20remains%20limited%2C%20this%20work%20provides%20a%0Arobust%20foundation%20for%20subsequent%20reinforcement%20learning%20or%20preference%0Aoptimization%20methods.%20We%20release%20the%20dataset%2C%20pretrained%20models%2C%20and%20code%0Aunder%3A%20https%3A//bit-bots.github.io/SoccerDiffusion%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20808v1&entry.124074799=Read"},
{"title": "Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine\n  Learning", "author": "Atul Sharma and Kavindu Herath and Saurabh Bagchi and Chaoyue Liu and Somali Chaterji", "abstract": "  We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm\nfor collaborative machine learning that combines the strengths of Federated\nLearning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier\ncommunication structure that avoids the single point of failure inherent in FL\nand outperforms the state-of-the-art P2PL framework, Epidemic Learning Local\n(ELL). At equal communication budgets (total edges), HSL achieves higher\nperformance than ELL, while at significantly lower communication budgets, it\ncan match ELL's performance. For instance, with only 400 edges, HSL reaches the\nsame test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on\nCIFAR-10, demonstrating its suitability for resource-constrained systems. HSL\nalso achieves stronger consensus among nodes after mixing, resulting in\nimproved performance with fewer training rounds. We substantiate these claims\nthrough rigorous theoretical analyses and extensive experimental results,\nshowcasing HSL's practicality for large-scale collaborative learning.\n", "link": "http://arxiv.org/abs/2504.20988v1", "date": "2025-04-29", "relevancy": 2.2764, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4605}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4551}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hubs%20and%20Spokes%20Learning%3A%20Efficient%20and%20Scalable%20Collaborative%20Machine%0A%20%20Learning&body=Title%3A%20Hubs%20and%20Spokes%20Learning%3A%20Efficient%20and%20Scalable%20Collaborative%20Machine%0A%20%20Learning%0AAuthor%3A%20Atul%20Sharma%20and%20Kavindu%20Herath%20and%20Saurabh%20Bagchi%20and%20Chaoyue%20Liu%20and%20Somali%20Chaterji%0AAbstract%3A%20%20%20We%20introduce%20the%20Hubs%20and%20Spokes%20Learning%20%28HSL%29%20framework%2C%20a%20novel%20paradigm%0Afor%20collaborative%20machine%20learning%20that%20combines%20the%20strengths%20of%20Federated%0ALearning%20%28FL%29%20and%20Decentralized%20Learning%20%28P2PL%29.%20HSL%20employs%20a%20two-tier%0Acommunication%20structure%20that%20avoids%20the%20single%20point%20of%20failure%20inherent%20in%20FL%0Aand%20outperforms%20the%20state-of-the-art%20P2PL%20framework%2C%20Epidemic%20Learning%20Local%0A%28ELL%29.%20At%20equal%20communication%20budgets%20%28total%20edges%29%2C%20HSL%20achieves%20higher%0Aperformance%20than%20ELL%2C%20while%20at%20significantly%20lower%20communication%20budgets%2C%20it%0Acan%20match%20ELL%27s%20performance.%20For%20instance%2C%20with%20only%20400%20edges%2C%20HSL%20reaches%20the%0Asame%20test%20accuracy%20that%20ELL%20achieves%20with%201000%20edges%20for%20100%20peers%20%28spokes%29%20on%0ACIFAR-10%2C%20demonstrating%20its%20suitability%20for%20resource-constrained%20systems.%20HSL%0Aalso%20achieves%20stronger%20consensus%20among%20nodes%20after%20mixing%2C%20resulting%20in%0Aimproved%20performance%20with%20fewer%20training%20rounds.%20We%20substantiate%20these%20claims%0Athrough%20rigorous%20theoretical%20analyses%20and%20extensive%20experimental%20results%2C%0Ashowcasing%20HSL%27s%20practicality%20for%20large-scale%20collaborative%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHubs%2520and%2520Spokes%2520Learning%253A%2520Efficient%2520and%2520Scalable%2520Collaborative%2520Machine%250A%2520%2520Learning%26entry.906535625%3DAtul%2520Sharma%2520and%2520Kavindu%2520Herath%2520and%2520Saurabh%2520Bagchi%2520and%2520Chaoyue%2520Liu%2520and%2520Somali%2520Chaterji%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Hubs%2520and%2520Spokes%2520Learning%2520%2528HSL%2529%2520framework%252C%2520a%2520novel%2520paradigm%250Afor%2520collaborative%2520machine%2520learning%2520that%2520combines%2520the%2520strengths%2520of%2520Federated%250ALearning%2520%2528FL%2529%2520and%2520Decentralized%2520Learning%2520%2528P2PL%2529.%2520HSL%2520employs%2520a%2520two-tier%250Acommunication%2520structure%2520that%2520avoids%2520the%2520single%2520point%2520of%2520failure%2520inherent%2520in%2520FL%250Aand%2520outperforms%2520the%2520state-of-the-art%2520P2PL%2520framework%252C%2520Epidemic%2520Learning%2520Local%250A%2528ELL%2529.%2520At%2520equal%2520communication%2520budgets%2520%2528total%2520edges%2529%252C%2520HSL%2520achieves%2520higher%250Aperformance%2520than%2520ELL%252C%2520while%2520at%2520significantly%2520lower%2520communication%2520budgets%252C%2520it%250Acan%2520match%2520ELL%2527s%2520performance.%2520For%2520instance%252C%2520with%2520only%2520400%2520edges%252C%2520HSL%2520reaches%2520the%250Asame%2520test%2520accuracy%2520that%2520ELL%2520achieves%2520with%25201000%2520edges%2520for%2520100%2520peers%2520%2528spokes%2529%2520on%250ACIFAR-10%252C%2520demonstrating%2520its%2520suitability%2520for%2520resource-constrained%2520systems.%2520HSL%250Aalso%2520achieves%2520stronger%2520consensus%2520among%2520nodes%2520after%2520mixing%252C%2520resulting%2520in%250Aimproved%2520performance%2520with%2520fewer%2520training%2520rounds.%2520We%2520substantiate%2520these%2520claims%250Athrough%2520rigorous%2520theoretical%2520analyses%2520and%2520extensive%2520experimental%2520results%252C%250Ashowcasing%2520HSL%2527s%2520practicality%2520for%2520large-scale%2520collaborative%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hubs%20and%20Spokes%20Learning%3A%20Efficient%20and%20Scalable%20Collaborative%20Machine%0A%20%20Learning&entry.906535625=Atul%20Sharma%20and%20Kavindu%20Herath%20and%20Saurabh%20Bagchi%20and%20Chaoyue%20Liu%20and%20Somali%20Chaterji&entry.1292438233=%20%20We%20introduce%20the%20Hubs%20and%20Spokes%20Learning%20%28HSL%29%20framework%2C%20a%20novel%20paradigm%0Afor%20collaborative%20machine%20learning%20that%20combines%20the%20strengths%20of%20Federated%0ALearning%20%28FL%29%20and%20Decentralized%20Learning%20%28P2PL%29.%20HSL%20employs%20a%20two-tier%0Acommunication%20structure%20that%20avoids%20the%20single%20point%20of%20failure%20inherent%20in%20FL%0Aand%20outperforms%20the%20state-of-the-art%20P2PL%20framework%2C%20Epidemic%20Learning%20Local%0A%28ELL%29.%20At%20equal%20communication%20budgets%20%28total%20edges%29%2C%20HSL%20achieves%20higher%0Aperformance%20than%20ELL%2C%20while%20at%20significantly%20lower%20communication%20budgets%2C%20it%0Acan%20match%20ELL%27s%20performance.%20For%20instance%2C%20with%20only%20400%20edges%2C%20HSL%20reaches%20the%0Asame%20test%20accuracy%20that%20ELL%20achieves%20with%201000%20edges%20for%20100%20peers%20%28spokes%29%20on%0ACIFAR-10%2C%20demonstrating%20its%20suitability%20for%20resource-constrained%20systems.%20HSL%0Aalso%20achieves%20stronger%20consensus%20among%20nodes%20after%20mixing%2C%20resulting%20in%0Aimproved%20performance%20with%20fewer%20training%20rounds.%20We%20substantiate%20these%20claims%0Athrough%20rigorous%20theoretical%20analyses%20and%20extensive%20experimental%20results%2C%0Ashowcasing%20HSL%27s%20practicality%20for%20large-scale%20collaborative%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20988v1&entry.124074799=Read"},
{"title": "AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection", "author": "Lorenzo Pellegrini and Davide Cozzolino and Serafino Pandolfini and Davide Maltoni and Matteo Ferrara and Luisa Verdoliva and Marco Prati and Marco Ramilli", "abstract": "  The rapid advancement of generative AI has revolutionized image creation,\nenabling high-quality synthesis from text prompts while raising critical\nchallenges for media authenticity. We present Ai-GenBench, a novel benchmark\ndesigned to address the urgent need for robust detection of AI-generated images\nin real-world scenarios. Unlike existing solutions that evaluate models on\nstatic datasets, Ai-GenBench introduces a temporal evaluation framework where\ndetection methods are incrementally trained on synthetic images, historically\nordered by their generative models, to test their ability to generalize to new\ngenerative models, such as the transition from GANs to diffusion models. Our\nbenchmark focuses on high-quality, diverse visual content and overcomes key\nlimitations of current approaches, including arbitrary dataset splits, unfair\ncomparisons, and excessive computational demands. Ai-GenBench provides a\ncomprehensive dataset, a standardized evaluation protocol, and accessible tools\nfor both researchers and non-experts (e.g., journalists, fact-checkers),\nensuring reproducibility while maintaining practical training requirements. By\nestablishing clear evaluation rules and controlled augmentation strategies,\nAi-GenBench enables meaningful comparison of detection methods and scalable\nsolutions. Code and data are publicly available to ensure reproducibility and\nto support the development of robust forensic detectors to keep pace with the\nrise of new synthetic generators.\n", "link": "http://arxiv.org/abs/2504.20865v1", "date": "2025-04-29", "relevancy": 2.2713, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6018}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5688}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-GenBench%3A%20A%20New%20Ongoing%20Benchmark%20for%20AI-Generated%20Image%20Detection&body=Title%3A%20AI-GenBench%3A%20A%20New%20Ongoing%20Benchmark%20for%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Lorenzo%20Pellegrini%20and%20Davide%20Cozzolino%20and%20Serafino%20Pandolfini%20and%20Davide%20Maltoni%20and%20Matteo%20Ferrara%20and%20Luisa%20Verdoliva%20and%20Marco%20Prati%20and%20Marco%20Ramilli%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20generative%20AI%20has%20revolutionized%20image%20creation%2C%0Aenabling%20high-quality%20synthesis%20from%20text%20prompts%20while%20raising%20critical%0Achallenges%20for%20media%20authenticity.%20We%20present%20Ai-GenBench%2C%20a%20novel%20benchmark%0Adesigned%20to%20address%20the%20urgent%20need%20for%20robust%20detection%20of%20AI-generated%20images%0Ain%20real-world%20scenarios.%20Unlike%20existing%20solutions%20that%20evaluate%20models%20on%0Astatic%20datasets%2C%20Ai-GenBench%20introduces%20a%20temporal%20evaluation%20framework%20where%0Adetection%20methods%20are%20incrementally%20trained%20on%20synthetic%20images%2C%20historically%0Aordered%20by%20their%20generative%20models%2C%20to%20test%20their%20ability%20to%20generalize%20to%20new%0Agenerative%20models%2C%20such%20as%20the%20transition%20from%20GANs%20to%20diffusion%20models.%20Our%0Abenchmark%20focuses%20on%20high-quality%2C%20diverse%20visual%20content%20and%20overcomes%20key%0Alimitations%20of%20current%20approaches%2C%20including%20arbitrary%20dataset%20splits%2C%20unfair%0Acomparisons%2C%20and%20excessive%20computational%20demands.%20Ai-GenBench%20provides%20a%0Acomprehensive%20dataset%2C%20a%20standardized%20evaluation%20protocol%2C%20and%20accessible%20tools%0Afor%20both%20researchers%20and%20non-experts%20%28e.g.%2C%20journalists%2C%20fact-checkers%29%2C%0Aensuring%20reproducibility%20while%20maintaining%20practical%20training%20requirements.%20By%0Aestablishing%20clear%20evaluation%20rules%20and%20controlled%20augmentation%20strategies%2C%0AAi-GenBench%20enables%20meaningful%20comparison%20of%20detection%20methods%20and%20scalable%0Asolutions.%20Code%20and%20data%20are%20publicly%20available%20to%20ensure%20reproducibility%20and%0Ato%20support%20the%20development%20of%20robust%20forensic%20detectors%20to%20keep%20pace%20with%20the%0Arise%20of%20new%20synthetic%20generators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-GenBench%253A%2520A%2520New%2520Ongoing%2520Benchmark%2520for%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DLorenzo%2520Pellegrini%2520and%2520Davide%2520Cozzolino%2520and%2520Serafino%2520Pandolfini%2520and%2520Davide%2520Maltoni%2520and%2520Matteo%2520Ferrara%2520and%2520Luisa%2520Verdoliva%2520and%2520Marco%2520Prati%2520and%2520Marco%2520Ramilli%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520generative%2520AI%2520has%2520revolutionized%2520image%2520creation%252C%250Aenabling%2520high-quality%2520synthesis%2520from%2520text%2520prompts%2520while%2520raising%2520critical%250Achallenges%2520for%2520media%2520authenticity.%2520We%2520present%2520Ai-GenBench%252C%2520a%2520novel%2520benchmark%250Adesigned%2520to%2520address%2520the%2520urgent%2520need%2520for%2520robust%2520detection%2520of%2520AI-generated%2520images%250Ain%2520real-world%2520scenarios.%2520Unlike%2520existing%2520solutions%2520that%2520evaluate%2520models%2520on%250Astatic%2520datasets%252C%2520Ai-GenBench%2520introduces%2520a%2520temporal%2520evaluation%2520framework%2520where%250Adetection%2520methods%2520are%2520incrementally%2520trained%2520on%2520synthetic%2520images%252C%2520historically%250Aordered%2520by%2520their%2520generative%2520models%252C%2520to%2520test%2520their%2520ability%2520to%2520generalize%2520to%2520new%250Agenerative%2520models%252C%2520such%2520as%2520the%2520transition%2520from%2520GANs%2520to%2520diffusion%2520models.%2520Our%250Abenchmark%2520focuses%2520on%2520high-quality%252C%2520diverse%2520visual%2520content%2520and%2520overcomes%2520key%250Alimitations%2520of%2520current%2520approaches%252C%2520including%2520arbitrary%2520dataset%2520splits%252C%2520unfair%250Acomparisons%252C%2520and%2520excessive%2520computational%2520demands.%2520Ai-GenBench%2520provides%2520a%250Acomprehensive%2520dataset%252C%2520a%2520standardized%2520evaluation%2520protocol%252C%2520and%2520accessible%2520tools%250Afor%2520both%2520researchers%2520and%2520non-experts%2520%2528e.g.%252C%2520journalists%252C%2520fact-checkers%2529%252C%250Aensuring%2520reproducibility%2520while%2520maintaining%2520practical%2520training%2520requirements.%2520By%250Aestablishing%2520clear%2520evaluation%2520rules%2520and%2520controlled%2520augmentation%2520strategies%252C%250AAi-GenBench%2520enables%2520meaningful%2520comparison%2520of%2520detection%2520methods%2520and%2520scalable%250Asolutions.%2520Code%2520and%2520data%2520are%2520publicly%2520available%2520to%2520ensure%2520reproducibility%2520and%250Ato%2520support%2520the%2520development%2520of%2520robust%2520forensic%2520detectors%2520to%2520keep%2520pace%2520with%2520the%250Arise%2520of%2520new%2520synthetic%2520generators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-GenBench%3A%20A%20New%20Ongoing%20Benchmark%20for%20AI-Generated%20Image%20Detection&entry.906535625=Lorenzo%20Pellegrini%20and%20Davide%20Cozzolino%20and%20Serafino%20Pandolfini%20and%20Davide%20Maltoni%20and%20Matteo%20Ferrara%20and%20Luisa%20Verdoliva%20and%20Marco%20Prati%20and%20Marco%20Ramilli&entry.1292438233=%20%20The%20rapid%20advancement%20of%20generative%20AI%20has%20revolutionized%20image%20creation%2C%0Aenabling%20high-quality%20synthesis%20from%20text%20prompts%20while%20raising%20critical%0Achallenges%20for%20media%20authenticity.%20We%20present%20Ai-GenBench%2C%20a%20novel%20benchmark%0Adesigned%20to%20address%20the%20urgent%20need%20for%20robust%20detection%20of%20AI-generated%20images%0Ain%20real-world%20scenarios.%20Unlike%20existing%20solutions%20that%20evaluate%20models%20on%0Astatic%20datasets%2C%20Ai-GenBench%20introduces%20a%20temporal%20evaluation%20framework%20where%0Adetection%20methods%20are%20incrementally%20trained%20on%20synthetic%20images%2C%20historically%0Aordered%20by%20their%20generative%20models%2C%20to%20test%20their%20ability%20to%20generalize%20to%20new%0Agenerative%20models%2C%20such%20as%20the%20transition%20from%20GANs%20to%20diffusion%20models.%20Our%0Abenchmark%20focuses%20on%20high-quality%2C%20diverse%20visual%20content%20and%20overcomes%20key%0Alimitations%20of%20current%20approaches%2C%20including%20arbitrary%20dataset%20splits%2C%20unfair%0Acomparisons%2C%20and%20excessive%20computational%20demands.%20Ai-GenBench%20provides%20a%0Acomprehensive%20dataset%2C%20a%20standardized%20evaluation%20protocol%2C%20and%20accessible%20tools%0Afor%20both%20researchers%20and%20non-experts%20%28e.g.%2C%20journalists%2C%20fact-checkers%29%2C%0Aensuring%20reproducibility%20while%20maintaining%20practical%20training%20requirements.%20By%0Aestablishing%20clear%20evaluation%20rules%20and%20controlled%20augmentation%20strategies%2C%0AAi-GenBench%20enables%20meaningful%20comparison%20of%20detection%20methods%20and%20scalable%0Asolutions.%20Code%20and%20data%20are%20publicly%20available%20to%20ensure%20reproducibility%20and%0Ato%20support%20the%20development%20of%20robust%20forensic%20detectors%20to%20keep%20pace%20with%20the%0Arise%20of%20new%20synthetic%20generators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20865v1&entry.124074799=Read"},
{"title": "Radiance Surfaces: Optimizing Surface Representations with a 5D Radiance\n  Field Loss", "author": "Ziyi Zhang and Nicolas Roussel and Thomas M\u00fcller and Tizian Zeltner and Merlin Nimier-David and Fabrice Rousselle and Wenzel Jakob", "abstract": "  We present a fast and simple technique to convert images into a radiance\nsurface-based scene representation. Building on existing radiance volume\nreconstruction algorithms, we introduce a subtle yet impactful modification of\nthe loss function requiring changes to only a few lines of code: instead of\nintegrating the radiance field along rays and supervising the resulting images,\nwe project the training images into the scene to directly supervise the\nspatio-directional radiance field.\n  The primary outcome of this change is the complete removal of alpha blending\nand ray marching from the image formation model, instead moving these steps\ninto the loss computation. In addition to promoting convergence to surfaces,\nthis formulation assigns explicit semantic meaning to 2D subsets of the\nradiance field, turning them into well-defined radiance surfaces. We finally\nextract a level set from this representation, which results in a high-quality\nradiance surface model.\n  Our method retains much of the speed and quality of the baseline algorithm.\nFor instance, a suitably modified variant of Instant NGP maintains comparable\ncomputational efficiency, while achieving an average PSNR that is only 0.1 dB\nlower. Most importantly, our method generates explicit surfaces in place of an\nexponential volume, doing so with a level of simplicity not seen in prior work.\n", "link": "http://arxiv.org/abs/2501.18627v2", "date": "2025-04-29", "relevancy": 2.257, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5912}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5566}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radiance%20Surfaces%3A%20Optimizing%20Surface%20Representations%20with%20a%205D%20Radiance%0A%20%20Field%20Loss&body=Title%3A%20Radiance%20Surfaces%3A%20Optimizing%20Surface%20Representations%20with%20a%205D%20Radiance%0A%20%20Field%20Loss%0AAuthor%3A%20Ziyi%20Zhang%20and%20Nicolas%20Roussel%20and%20Thomas%20M%C3%BCller%20and%20Tizian%20Zeltner%20and%20Merlin%20Nimier-David%20and%20Fabrice%20Rousselle%20and%20Wenzel%20Jakob%0AAbstract%3A%20%20%20We%20present%20a%20fast%20and%20simple%20technique%20to%20convert%20images%20into%20a%20radiance%0Asurface-based%20scene%20representation.%20Building%20on%20existing%20radiance%20volume%0Areconstruction%20algorithms%2C%20we%20introduce%20a%20subtle%20yet%20impactful%20modification%20of%0Athe%20loss%20function%20requiring%20changes%20to%20only%20a%20few%20lines%20of%20code%3A%20instead%20of%0Aintegrating%20the%20radiance%20field%20along%20rays%20and%20supervising%20the%20resulting%20images%2C%0Awe%20project%20the%20training%20images%20into%20the%20scene%20to%20directly%20supervise%20the%0Aspatio-directional%20radiance%20field.%0A%20%20The%20primary%20outcome%20of%20this%20change%20is%20the%20complete%20removal%20of%20alpha%20blending%0Aand%20ray%20marching%20from%20the%20image%20formation%20model%2C%20instead%20moving%20these%20steps%0Ainto%20the%20loss%20computation.%20In%20addition%20to%20promoting%20convergence%20to%20surfaces%2C%0Athis%20formulation%20assigns%20explicit%20semantic%20meaning%20to%202D%20subsets%20of%20the%0Aradiance%20field%2C%20turning%20them%20into%20well-defined%20radiance%20surfaces.%20We%20finally%0Aextract%20a%20level%20set%20from%20this%20representation%2C%20which%20results%20in%20a%20high-quality%0Aradiance%20surface%20model.%0A%20%20Our%20method%20retains%20much%20of%20the%20speed%20and%20quality%20of%20the%20baseline%20algorithm.%0AFor%20instance%2C%20a%20suitably%20modified%20variant%20of%20Instant%20NGP%20maintains%20comparable%0Acomputational%20efficiency%2C%20while%20achieving%20an%20average%20PSNR%20that%20is%20only%200.1%20dB%0Alower.%20Most%20importantly%2C%20our%20method%20generates%20explicit%20surfaces%20in%20place%20of%20an%0Aexponential%20volume%2C%20doing%20so%20with%20a%20level%20of%20simplicity%20not%20seen%20in%20prior%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18627v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadiance%2520Surfaces%253A%2520Optimizing%2520Surface%2520Representations%2520with%2520a%25205D%2520Radiance%250A%2520%2520Field%2520Loss%26entry.906535625%3DZiyi%2520Zhang%2520and%2520Nicolas%2520Roussel%2520and%2520Thomas%2520M%25C3%25BCller%2520and%2520Tizian%2520Zeltner%2520and%2520Merlin%2520Nimier-David%2520and%2520Fabrice%2520Rousselle%2520and%2520Wenzel%2520Jakob%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520fast%2520and%2520simple%2520technique%2520to%2520convert%2520images%2520into%2520a%2520radiance%250Asurface-based%2520scene%2520representation.%2520Building%2520on%2520existing%2520radiance%2520volume%250Areconstruction%2520algorithms%252C%2520we%2520introduce%2520a%2520subtle%2520yet%2520impactful%2520modification%2520of%250Athe%2520loss%2520function%2520requiring%2520changes%2520to%2520only%2520a%2520few%2520lines%2520of%2520code%253A%2520instead%2520of%250Aintegrating%2520the%2520radiance%2520field%2520along%2520rays%2520and%2520supervising%2520the%2520resulting%2520images%252C%250Awe%2520project%2520the%2520training%2520images%2520into%2520the%2520scene%2520to%2520directly%2520supervise%2520the%250Aspatio-directional%2520radiance%2520field.%250A%2520%2520The%2520primary%2520outcome%2520of%2520this%2520change%2520is%2520the%2520complete%2520removal%2520of%2520alpha%2520blending%250Aand%2520ray%2520marching%2520from%2520the%2520image%2520formation%2520model%252C%2520instead%2520moving%2520these%2520steps%250Ainto%2520the%2520loss%2520computation.%2520In%2520addition%2520to%2520promoting%2520convergence%2520to%2520surfaces%252C%250Athis%2520formulation%2520assigns%2520explicit%2520semantic%2520meaning%2520to%25202D%2520subsets%2520of%2520the%250Aradiance%2520field%252C%2520turning%2520them%2520into%2520well-defined%2520radiance%2520surfaces.%2520We%2520finally%250Aextract%2520a%2520level%2520set%2520from%2520this%2520representation%252C%2520which%2520results%2520in%2520a%2520high-quality%250Aradiance%2520surface%2520model.%250A%2520%2520Our%2520method%2520retains%2520much%2520of%2520the%2520speed%2520and%2520quality%2520of%2520the%2520baseline%2520algorithm.%250AFor%2520instance%252C%2520a%2520suitably%2520modified%2520variant%2520of%2520Instant%2520NGP%2520maintains%2520comparable%250Acomputational%2520efficiency%252C%2520while%2520achieving%2520an%2520average%2520PSNR%2520that%2520is%2520only%25200.1%2520dB%250Alower.%2520Most%2520importantly%252C%2520our%2520method%2520generates%2520explicit%2520surfaces%2520in%2520place%2520of%2520an%250Aexponential%2520volume%252C%2520doing%2520so%2520with%2520a%2520level%2520of%2520simplicity%2520not%2520seen%2520in%2520prior%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18627v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radiance%20Surfaces%3A%20Optimizing%20Surface%20Representations%20with%20a%205D%20Radiance%0A%20%20Field%20Loss&entry.906535625=Ziyi%20Zhang%20and%20Nicolas%20Roussel%20and%20Thomas%20M%C3%BCller%20and%20Tizian%20Zeltner%20and%20Merlin%20Nimier-David%20and%20Fabrice%20Rousselle%20and%20Wenzel%20Jakob&entry.1292438233=%20%20We%20present%20a%20fast%20and%20simple%20technique%20to%20convert%20images%20into%20a%20radiance%0Asurface-based%20scene%20representation.%20Building%20on%20existing%20radiance%20volume%0Areconstruction%20algorithms%2C%20we%20introduce%20a%20subtle%20yet%20impactful%20modification%20of%0Athe%20loss%20function%20requiring%20changes%20to%20only%20a%20few%20lines%20of%20code%3A%20instead%20of%0Aintegrating%20the%20radiance%20field%20along%20rays%20and%20supervising%20the%20resulting%20images%2C%0Awe%20project%20the%20training%20images%20into%20the%20scene%20to%20directly%20supervise%20the%0Aspatio-directional%20radiance%20field.%0A%20%20The%20primary%20outcome%20of%20this%20change%20is%20the%20complete%20removal%20of%20alpha%20blending%0Aand%20ray%20marching%20from%20the%20image%20formation%20model%2C%20instead%20moving%20these%20steps%0Ainto%20the%20loss%20computation.%20In%20addition%20to%20promoting%20convergence%20to%20surfaces%2C%0Athis%20formulation%20assigns%20explicit%20semantic%20meaning%20to%202D%20subsets%20of%20the%0Aradiance%20field%2C%20turning%20them%20into%20well-defined%20radiance%20surfaces.%20We%20finally%0Aextract%20a%20level%20set%20from%20this%20representation%2C%20which%20results%20in%20a%20high-quality%0Aradiance%20surface%20model.%0A%20%20Our%20method%20retains%20much%20of%20the%20speed%20and%20quality%20of%20the%20baseline%20algorithm.%0AFor%20instance%2C%20a%20suitably%20modified%20variant%20of%20Instant%20NGP%20maintains%20comparable%0Acomputational%20efficiency%2C%20while%20achieving%20an%20average%20PSNR%20that%20is%20only%200.1%20dB%0Alower.%20Most%20importantly%2C%20our%20method%20generates%20explicit%20surfaces%20in%20place%20of%20an%0Aexponential%20volume%2C%20doing%20so%20with%20a%20level%20of%20simplicity%20not%20seen%20in%20prior%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18627v2&entry.124074799=Read"},
{"title": "CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional\n  CAD Generation", "author": "Jianyu Wu and Yizhou Wang and Xiangyu Yue and Xinzhu Ma and Jingyang Guo and Dongzhan Zhou and Wanli Ouyang and Shixiang Tang", "abstract": "  While accurate and user-friendly Computer-Aided Design (CAD) is crucial for\nindustrial design and manufacturing, existing methods still struggle to achieve\nthis due to their over-simplified representations or architectures incapable of\nsupporting multimodal design requirements. In this paper, we attempt to tackle\nthis problem from both methods and datasets aspects. First, we propose a\ncascade MAR with topology predictor (CMT), the first multimodal framework for\nCAD generation based on Boundary Representation (B-Rep). Specifically, the\ncascade MAR can effectively capture the ``edge-counters-surface'' priors that\nare essential in B-Reps, while the topology predictor directly estimates\ntopology in B-Reps from the compact tokens in MAR. Second, to facilitate\nlarge-scale training, we develop a large-scale multimodal CAD dataset, mmABC,\nwhich includes over 1.3 million B-Rep models with multimodal annotations,\nincluding point clouds, text descriptions, and multi-view images. Extensive\nexperiments show the superior of CMT in both conditional and unconditional CAD\ngeneration tasks. For example, we improve Coverage and Valid ratio by +10.68%\nand +10.3%, respectively, compared to state-of-the-art methods on ABC in\nunconditional generation. CMT also improves +4.01 Chamfer on image conditioned\nCAD generation on mmABC. The dataset, code and pretrained network shall be\nreleased.\n", "link": "http://arxiv.org/abs/2504.20830v1", "date": "2025-04-29", "relevancy": 2.2526, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5861}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5533}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CMT%3A%20A%20Cascade%20MAR%20with%20Topology%20Predictor%20for%20Multimodal%20Conditional%0A%20%20CAD%20Generation&body=Title%3A%20CMT%3A%20A%20Cascade%20MAR%20with%20Topology%20Predictor%20for%20Multimodal%20Conditional%0A%20%20CAD%20Generation%0AAuthor%3A%20Jianyu%20Wu%20and%20Yizhou%20Wang%20and%20Xiangyu%20Yue%20and%20Xinzhu%20Ma%20and%20Jingyang%20Guo%20and%20Dongzhan%20Zhou%20and%20Wanli%20Ouyang%20and%20Shixiang%20Tang%0AAbstract%3A%20%20%20While%20accurate%20and%20user-friendly%20Computer-Aided%20Design%20%28CAD%29%20is%20crucial%20for%0Aindustrial%20design%20and%20manufacturing%2C%20existing%20methods%20still%20struggle%20to%20achieve%0Athis%20due%20to%20their%20over-simplified%20representations%20or%20architectures%20incapable%20of%0Asupporting%20multimodal%20design%20requirements.%20In%20this%20paper%2C%20we%20attempt%20to%20tackle%0Athis%20problem%20from%20both%20methods%20and%20datasets%20aspects.%20First%2C%20we%20propose%20a%0Acascade%20MAR%20with%20topology%20predictor%20%28CMT%29%2C%20the%20first%20multimodal%20framework%20for%0ACAD%20generation%20based%20on%20Boundary%20Representation%20%28B-Rep%29.%20Specifically%2C%20the%0Acascade%20MAR%20can%20effectively%20capture%20the%20%60%60edge-counters-surface%27%27%20priors%20that%0Aare%20essential%20in%20B-Reps%2C%20while%20the%20topology%20predictor%20directly%20estimates%0Atopology%20in%20B-Reps%20from%20the%20compact%20tokens%20in%20MAR.%20Second%2C%20to%20facilitate%0Alarge-scale%20training%2C%20we%20develop%20a%20large-scale%20multimodal%20CAD%20dataset%2C%20mmABC%2C%0Awhich%20includes%20over%201.3%20million%20B-Rep%20models%20with%20multimodal%20annotations%2C%0Aincluding%20point%20clouds%2C%20text%20descriptions%2C%20and%20multi-view%20images.%20Extensive%0Aexperiments%20show%20the%20superior%20of%20CMT%20in%20both%20conditional%20and%20unconditional%20CAD%0Ageneration%20tasks.%20For%20example%2C%20we%20improve%20Coverage%20and%20Valid%20ratio%20by%20%2B10.68%25%0Aand%20%2B10.3%25%2C%20respectively%2C%20compared%20to%20state-of-the-art%20methods%20on%20ABC%20in%0Aunconditional%20generation.%20CMT%20also%20improves%20%2B4.01%20Chamfer%20on%20image%20conditioned%0ACAD%20generation%20on%20mmABC.%20The%20dataset%2C%20code%20and%20pretrained%20network%20shall%20be%0Areleased.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCMT%253A%2520A%2520Cascade%2520MAR%2520with%2520Topology%2520Predictor%2520for%2520Multimodal%2520Conditional%250A%2520%2520CAD%2520Generation%26entry.906535625%3DJianyu%2520Wu%2520and%2520Yizhou%2520Wang%2520and%2520Xiangyu%2520Yue%2520and%2520Xinzhu%2520Ma%2520and%2520Jingyang%2520Guo%2520and%2520Dongzhan%2520Zhou%2520and%2520Wanli%2520Ouyang%2520and%2520Shixiang%2520Tang%26entry.1292438233%3D%2520%2520While%2520accurate%2520and%2520user-friendly%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520is%2520crucial%2520for%250Aindustrial%2520design%2520and%2520manufacturing%252C%2520existing%2520methods%2520still%2520struggle%2520to%2520achieve%250Athis%2520due%2520to%2520their%2520over-simplified%2520representations%2520or%2520architectures%2520incapable%2520of%250Asupporting%2520multimodal%2520design%2520requirements.%2520In%2520this%2520paper%252C%2520we%2520attempt%2520to%2520tackle%250Athis%2520problem%2520from%2520both%2520methods%2520and%2520datasets%2520aspects.%2520First%252C%2520we%2520propose%2520a%250Acascade%2520MAR%2520with%2520topology%2520predictor%2520%2528CMT%2529%252C%2520the%2520first%2520multimodal%2520framework%2520for%250ACAD%2520generation%2520based%2520on%2520Boundary%2520Representation%2520%2528B-Rep%2529.%2520Specifically%252C%2520the%250Acascade%2520MAR%2520can%2520effectively%2520capture%2520the%2520%2560%2560edge-counters-surface%2527%2527%2520priors%2520that%250Aare%2520essential%2520in%2520B-Reps%252C%2520while%2520the%2520topology%2520predictor%2520directly%2520estimates%250Atopology%2520in%2520B-Reps%2520from%2520the%2520compact%2520tokens%2520in%2520MAR.%2520Second%252C%2520to%2520facilitate%250Alarge-scale%2520training%252C%2520we%2520develop%2520a%2520large-scale%2520multimodal%2520CAD%2520dataset%252C%2520mmABC%252C%250Awhich%2520includes%2520over%25201.3%2520million%2520B-Rep%2520models%2520with%2520multimodal%2520annotations%252C%250Aincluding%2520point%2520clouds%252C%2520text%2520descriptions%252C%2520and%2520multi-view%2520images.%2520Extensive%250Aexperiments%2520show%2520the%2520superior%2520of%2520CMT%2520in%2520both%2520conditional%2520and%2520unconditional%2520CAD%250Ageneration%2520tasks.%2520For%2520example%252C%2520we%2520improve%2520Coverage%2520and%2520Valid%2520ratio%2520by%2520%252B10.68%2525%250Aand%2520%252B10.3%2525%252C%2520respectively%252C%2520compared%2520to%2520state-of-the-art%2520methods%2520on%2520ABC%2520in%250Aunconditional%2520generation.%2520CMT%2520also%2520improves%2520%252B4.01%2520Chamfer%2520on%2520image%2520conditioned%250ACAD%2520generation%2520on%2520mmABC.%2520The%2520dataset%252C%2520code%2520and%2520pretrained%2520network%2520shall%2520be%250Areleased.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CMT%3A%20A%20Cascade%20MAR%20with%20Topology%20Predictor%20for%20Multimodal%20Conditional%0A%20%20CAD%20Generation&entry.906535625=Jianyu%20Wu%20and%20Yizhou%20Wang%20and%20Xiangyu%20Yue%20and%20Xinzhu%20Ma%20and%20Jingyang%20Guo%20and%20Dongzhan%20Zhou%20and%20Wanli%20Ouyang%20and%20Shixiang%20Tang&entry.1292438233=%20%20While%20accurate%20and%20user-friendly%20Computer-Aided%20Design%20%28CAD%29%20is%20crucial%20for%0Aindustrial%20design%20and%20manufacturing%2C%20existing%20methods%20still%20struggle%20to%20achieve%0Athis%20due%20to%20their%20over-simplified%20representations%20or%20architectures%20incapable%20of%0Asupporting%20multimodal%20design%20requirements.%20In%20this%20paper%2C%20we%20attempt%20to%20tackle%0Athis%20problem%20from%20both%20methods%20and%20datasets%20aspects.%20First%2C%20we%20propose%20a%0Acascade%20MAR%20with%20topology%20predictor%20%28CMT%29%2C%20the%20first%20multimodal%20framework%20for%0ACAD%20generation%20based%20on%20Boundary%20Representation%20%28B-Rep%29.%20Specifically%2C%20the%0Acascade%20MAR%20can%20effectively%20capture%20the%20%60%60edge-counters-surface%27%27%20priors%20that%0Aare%20essential%20in%20B-Reps%2C%20while%20the%20topology%20predictor%20directly%20estimates%0Atopology%20in%20B-Reps%20from%20the%20compact%20tokens%20in%20MAR.%20Second%2C%20to%20facilitate%0Alarge-scale%20training%2C%20we%20develop%20a%20large-scale%20multimodal%20CAD%20dataset%2C%20mmABC%2C%0Awhich%20includes%20over%201.3%20million%20B-Rep%20models%20with%20multimodal%20annotations%2C%0Aincluding%20point%20clouds%2C%20text%20descriptions%2C%20and%20multi-view%20images.%20Extensive%0Aexperiments%20show%20the%20superior%20of%20CMT%20in%20both%20conditional%20and%20unconditional%20CAD%0Ageneration%20tasks.%20For%20example%2C%20we%20improve%20Coverage%20and%20Valid%20ratio%20by%20%2B10.68%25%0Aand%20%2B10.3%25%2C%20respectively%2C%20compared%20to%20state-of-the-art%20methods%20on%20ABC%20in%0Aunconditional%20generation.%20CMT%20also%20improves%20%2B4.01%20Chamfer%20on%20image%20conditioned%0ACAD%20generation%20on%20mmABC.%20The%20dataset%2C%20code%20and%20pretrained%20network%20shall%20be%0Areleased.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20830v1&entry.124074799=Read"},
{"title": "MADGEN: Mass-Spec attends to De Novo Molecular generation", "author": "Yinkai Wang and Xiaohui Chen and Liping Liu and Soha Hassoun", "abstract": "  The annotation (assigning structural chemical identities) of MS/MS spectra\nremains a significant challenge due to the enormous molecular diversity in\nbiological samples and the limited scope of reference databases. Currently, the\nvast majority of spectral measurements remain in the \"dark chemical space\"\nwithout structural annotations. To improve annotation, we propose MADGEN\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\nfor de novo molecular structure generation guided by mass spectrometry data.\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\nmolecular generation starting with the scaffold. In the first stage, given an\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\nspectrum to guide an attention-based generative model to generate the final\nmolecule. Our approach constrains the molecular generation search space,\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\nperformance with a predictive scaffold retriever and with an oracle retriever.\nWe demonstrate the effectiveness of using attention to integrate spectral\ninformation throughout the generation process to achieve strong results with\nthe oracle retriever.\n", "link": "http://arxiv.org/abs/2501.01950v4", "date": "2025-04-29", "relevancy": 2.2524, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4674}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4473}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MADGEN%3A%20Mass-Spec%20attends%20to%20De%20Novo%20Molecular%20generation&body=Title%3A%20MADGEN%3A%20Mass-Spec%20attends%20to%20De%20Novo%20Molecular%20generation%0AAuthor%3A%20Yinkai%20Wang%20and%20Xiaohui%20Chen%20and%20Liping%20Liu%20and%20Soha%20Hassoun%0AAbstract%3A%20%20%20The%20annotation%20%28assigning%20structural%20chemical%20identities%29%20of%20MS/MS%20spectra%0Aremains%20a%20significant%20challenge%20due%20to%20the%20enormous%20molecular%20diversity%20in%0Abiological%20samples%20and%20the%20limited%20scope%20of%20reference%20databases.%20Currently%2C%20the%0Avast%20majority%20of%20spectral%20measurements%20remain%20in%20the%20%22dark%20chemical%20space%22%0Awithout%20structural%20annotations.%20To%20improve%20annotation%2C%20we%20propose%20MADGEN%0A%28Mass-spec%20Attends%20to%20De%20Novo%20Molecular%20GENeration%29%2C%20a%20scaffold-based%20method%0Afor%20de%20novo%20molecular%20structure%20generation%20guided%20by%20mass%20spectrometry%20data.%0AMADGEN%20operates%20in%20two%20stages%3A%20scaffold%20retrieval%20and%20spectra-conditioned%0Amolecular%20generation%20starting%20with%20the%20scaffold.%20In%20the%20first%20stage%2C%20given%20an%0AMS/MS%20spectrum%2C%20we%20formulate%20scaffold%20retrieval%20as%20a%20ranking%20problem%20and%20employ%0Acontrastive%20learning%20to%20align%20mass%20spectra%20with%20candidate%20molecular%20scaffolds.%0AIn%20the%20second%20stage%2C%20starting%20from%20the%20retrieved%20scaffold%2C%20we%20employ%20the%20MS/MS%0Aspectrum%20to%20guide%20an%20attention-based%20generative%20model%20to%20generate%20the%20final%0Amolecule.%20Our%20approach%20constrains%20the%20molecular%20generation%20search%20space%2C%0Areducing%20its%20complexity%20and%20improving%20generation%20accuracy.%20We%20evaluate%20MADGEN%0Aon%20three%20datasets%20%28NIST23%2C%20CANOPUS%2C%20and%20MassSpecGym%29%20and%20evaluate%20MADGEN%27s%0Aperformance%20with%20a%20predictive%20scaffold%20retriever%20and%20with%20an%20oracle%20retriever.%0AWe%20demonstrate%20the%20effectiveness%20of%20using%20attention%20to%20integrate%20spectral%0Ainformation%20throughout%20the%20generation%20process%20to%20achieve%20strong%20results%20with%0Athe%20oracle%20retriever.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.01950v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMADGEN%253A%2520Mass-Spec%2520attends%2520to%2520De%2520Novo%2520Molecular%2520generation%26entry.906535625%3DYinkai%2520Wang%2520and%2520Xiaohui%2520Chen%2520and%2520Liping%2520Liu%2520and%2520Soha%2520Hassoun%26entry.1292438233%3D%2520%2520The%2520annotation%2520%2528assigning%2520structural%2520chemical%2520identities%2529%2520of%2520MS/MS%2520spectra%250Aremains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520enormous%2520molecular%2520diversity%2520in%250Abiological%2520samples%2520and%2520the%2520limited%2520scope%2520of%2520reference%2520databases.%2520Currently%252C%2520the%250Avast%2520majority%2520of%2520spectral%2520measurements%2520remain%2520in%2520the%2520%2522dark%2520chemical%2520space%2522%250Awithout%2520structural%2520annotations.%2520To%2520improve%2520annotation%252C%2520we%2520propose%2520MADGEN%250A%2528Mass-spec%2520Attends%2520to%2520De%2520Novo%2520Molecular%2520GENeration%2529%252C%2520a%2520scaffold-based%2520method%250Afor%2520de%2520novo%2520molecular%2520structure%2520generation%2520guided%2520by%2520mass%2520spectrometry%2520data.%250AMADGEN%2520operates%2520in%2520two%2520stages%253A%2520scaffold%2520retrieval%2520and%2520spectra-conditioned%250Amolecular%2520generation%2520starting%2520with%2520the%2520scaffold.%2520In%2520the%2520first%2520stage%252C%2520given%2520an%250AMS/MS%2520spectrum%252C%2520we%2520formulate%2520scaffold%2520retrieval%2520as%2520a%2520ranking%2520problem%2520and%2520employ%250Acontrastive%2520learning%2520to%2520align%2520mass%2520spectra%2520with%2520candidate%2520molecular%2520scaffolds.%250AIn%2520the%2520second%2520stage%252C%2520starting%2520from%2520the%2520retrieved%2520scaffold%252C%2520we%2520employ%2520the%2520MS/MS%250Aspectrum%2520to%2520guide%2520an%2520attention-based%2520generative%2520model%2520to%2520generate%2520the%2520final%250Amolecule.%2520Our%2520approach%2520constrains%2520the%2520molecular%2520generation%2520search%2520space%252C%250Areducing%2520its%2520complexity%2520and%2520improving%2520generation%2520accuracy.%2520We%2520evaluate%2520MADGEN%250Aon%2520three%2520datasets%2520%2528NIST23%252C%2520CANOPUS%252C%2520and%2520MassSpecGym%2529%2520and%2520evaluate%2520MADGEN%2527s%250Aperformance%2520with%2520a%2520predictive%2520scaffold%2520retriever%2520and%2520with%2520an%2520oracle%2520retriever.%250AWe%2520demonstrate%2520the%2520effectiveness%2520of%2520using%2520attention%2520to%2520integrate%2520spectral%250Ainformation%2520throughout%2520the%2520generation%2520process%2520to%2520achieve%2520strong%2520results%2520with%250Athe%2520oracle%2520retriever.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.01950v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MADGEN%3A%20Mass-Spec%20attends%20to%20De%20Novo%20Molecular%20generation&entry.906535625=Yinkai%20Wang%20and%20Xiaohui%20Chen%20and%20Liping%20Liu%20and%20Soha%20Hassoun&entry.1292438233=%20%20The%20annotation%20%28assigning%20structural%20chemical%20identities%29%20of%20MS/MS%20spectra%0Aremains%20a%20significant%20challenge%20due%20to%20the%20enormous%20molecular%20diversity%20in%0Abiological%20samples%20and%20the%20limited%20scope%20of%20reference%20databases.%20Currently%2C%20the%0Avast%20majority%20of%20spectral%20measurements%20remain%20in%20the%20%22dark%20chemical%20space%22%0Awithout%20structural%20annotations.%20To%20improve%20annotation%2C%20we%20propose%20MADGEN%0A%28Mass-spec%20Attends%20to%20De%20Novo%20Molecular%20GENeration%29%2C%20a%20scaffold-based%20method%0Afor%20de%20novo%20molecular%20structure%20generation%20guided%20by%20mass%20spectrometry%20data.%0AMADGEN%20operates%20in%20two%20stages%3A%20scaffold%20retrieval%20and%20spectra-conditioned%0Amolecular%20generation%20starting%20with%20the%20scaffold.%20In%20the%20first%20stage%2C%20given%20an%0AMS/MS%20spectrum%2C%20we%20formulate%20scaffold%20retrieval%20as%20a%20ranking%20problem%20and%20employ%0Acontrastive%20learning%20to%20align%20mass%20spectra%20with%20candidate%20molecular%20scaffolds.%0AIn%20the%20second%20stage%2C%20starting%20from%20the%20retrieved%20scaffold%2C%20we%20employ%20the%20MS/MS%0Aspectrum%20to%20guide%20an%20attention-based%20generative%20model%20to%20generate%20the%20final%0Amolecule.%20Our%20approach%20constrains%20the%20molecular%20generation%20search%20space%2C%0Areducing%20its%20complexity%20and%20improving%20generation%20accuracy.%20We%20evaluate%20MADGEN%0Aon%20three%20datasets%20%28NIST23%2C%20CANOPUS%2C%20and%20MassSpecGym%29%20and%20evaluate%20MADGEN%27s%0Aperformance%20with%20a%20predictive%20scaffold%20retriever%20and%20with%20an%20oracle%20retriever.%0AWe%20demonstrate%20the%20effectiveness%20of%20using%20attention%20to%20integrate%20spectral%0Ainformation%20throughout%20the%20generation%20process%20to%20achieve%20strong%20results%20with%0Athe%20oracle%20retriever.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.01950v4&entry.124074799=Read"},
{"title": "Few-shot Sim2Real Based on High Fidelity Rendering with Force Feedback\n  Teleoperation", "author": "Yanwen Zou and Junda Huang and Boyuan Liang and Honghao Guo and Zhengyang Liu and Xin Ma and Jianshu Zhou and Masayoshi Tomizuka", "abstract": "  Teleoperation offers a promising approach to robotic data collection and\nhuman-robot interaction. However, existing teleoperation methods for data\ncollection are still limited by efficiency constraints in time and space, and\nthe pipeline for simulation-based data collection remains unclear. The problem\nis how to enhance task performance while minimizing reliance on real-world\ndata. To address this challenge, we propose a teleoperation pipeline for\ncollecting robotic manipulation data in simulation and training a few-shot\nsim-to-real visual-motor policy. Force feedback devices are integrated into the\nteleoperation system to provide precise end-effector gripping force feedback.\nExperiments across various manipulation tasks demonstrate that force feedback\nsignificantly improves both success rates and execution efficiency,\nparticularly in simulation. Furthermore, experiments with different levels of\nvisual rendering quality reveal that enhanced visual realism in simulation\nsubstantially boosts task performance while reducing the need for real-world\ndata.\n", "link": "http://arxiv.org/abs/2503.01301v2", "date": "2025-04-29", "relevancy": 2.2333, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5665}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5582}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20Sim2Real%20Based%20on%20High%20Fidelity%20Rendering%20with%20Force%20Feedback%0A%20%20Teleoperation&body=Title%3A%20Few-shot%20Sim2Real%20Based%20on%20High%20Fidelity%20Rendering%20with%20Force%20Feedback%0A%20%20Teleoperation%0AAuthor%3A%20Yanwen%20Zou%20and%20Junda%20Huang%20and%20Boyuan%20Liang%20and%20Honghao%20Guo%20and%20Zhengyang%20Liu%20and%20Xin%20Ma%20and%20Jianshu%20Zhou%20and%20Masayoshi%20Tomizuka%0AAbstract%3A%20%20%20Teleoperation%20offers%20a%20promising%20approach%20to%20robotic%20data%20collection%20and%0Ahuman-robot%20interaction.%20However%2C%20existing%20teleoperation%20methods%20for%20data%0Acollection%20are%20still%20limited%20by%20efficiency%20constraints%20in%20time%20and%20space%2C%20and%0Athe%20pipeline%20for%20simulation-based%20data%20collection%20remains%20unclear.%20The%20problem%0Ais%20how%20to%20enhance%20task%20performance%20while%20minimizing%20reliance%20on%20real-world%0Adata.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20teleoperation%20pipeline%20for%0Acollecting%20robotic%20manipulation%20data%20in%20simulation%20and%20training%20a%20few-shot%0Asim-to-real%20visual-motor%20policy.%20Force%20feedback%20devices%20are%20integrated%20into%20the%0Ateleoperation%20system%20to%20provide%20precise%20end-effector%20gripping%20force%20feedback.%0AExperiments%20across%20various%20manipulation%20tasks%20demonstrate%20that%20force%20feedback%0Asignificantly%20improves%20both%20success%20rates%20and%20execution%20efficiency%2C%0Aparticularly%20in%20simulation.%20Furthermore%2C%20experiments%20with%20different%20levels%20of%0Avisual%20rendering%20quality%20reveal%20that%20enhanced%20visual%20realism%20in%20simulation%0Asubstantially%20boosts%20task%20performance%20while%20reducing%20the%20need%20for%20real-world%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520Sim2Real%2520Based%2520on%2520High%2520Fidelity%2520Rendering%2520with%2520Force%2520Feedback%250A%2520%2520Teleoperation%26entry.906535625%3DYanwen%2520Zou%2520and%2520Junda%2520Huang%2520and%2520Boyuan%2520Liang%2520and%2520Honghao%2520Guo%2520and%2520Zhengyang%2520Liu%2520and%2520Xin%2520Ma%2520and%2520Jianshu%2520Zhou%2520and%2520Masayoshi%2520Tomizuka%26entry.1292438233%3D%2520%2520Teleoperation%2520offers%2520a%2520promising%2520approach%2520to%2520robotic%2520data%2520collection%2520and%250Ahuman-robot%2520interaction.%2520However%252C%2520existing%2520teleoperation%2520methods%2520for%2520data%250Acollection%2520are%2520still%2520limited%2520by%2520efficiency%2520constraints%2520in%2520time%2520and%2520space%252C%2520and%250Athe%2520pipeline%2520for%2520simulation-based%2520data%2520collection%2520remains%2520unclear.%2520The%2520problem%250Ais%2520how%2520to%2520enhance%2520task%2520performance%2520while%2520minimizing%2520reliance%2520on%2520real-world%250Adata.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520teleoperation%2520pipeline%2520for%250Acollecting%2520robotic%2520manipulation%2520data%2520in%2520simulation%2520and%2520training%2520a%2520few-shot%250Asim-to-real%2520visual-motor%2520policy.%2520Force%2520feedback%2520devices%2520are%2520integrated%2520into%2520the%250Ateleoperation%2520system%2520to%2520provide%2520precise%2520end-effector%2520gripping%2520force%2520feedback.%250AExperiments%2520across%2520various%2520manipulation%2520tasks%2520demonstrate%2520that%2520force%2520feedback%250Asignificantly%2520improves%2520both%2520success%2520rates%2520and%2520execution%2520efficiency%252C%250Aparticularly%2520in%2520simulation.%2520Furthermore%252C%2520experiments%2520with%2520different%2520levels%2520of%250Avisual%2520rendering%2520quality%2520reveal%2520that%2520enhanced%2520visual%2520realism%2520in%2520simulation%250Asubstantially%2520boosts%2520task%2520performance%2520while%2520reducing%2520the%2520need%2520for%2520real-world%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20Sim2Real%20Based%20on%20High%20Fidelity%20Rendering%20with%20Force%20Feedback%0A%20%20Teleoperation&entry.906535625=Yanwen%20Zou%20and%20Junda%20Huang%20and%20Boyuan%20Liang%20and%20Honghao%20Guo%20and%20Zhengyang%20Liu%20and%20Xin%20Ma%20and%20Jianshu%20Zhou%20and%20Masayoshi%20Tomizuka&entry.1292438233=%20%20Teleoperation%20offers%20a%20promising%20approach%20to%20robotic%20data%20collection%20and%0Ahuman-robot%20interaction.%20However%2C%20existing%20teleoperation%20methods%20for%20data%0Acollection%20are%20still%20limited%20by%20efficiency%20constraints%20in%20time%20and%20space%2C%20and%0Athe%20pipeline%20for%20simulation-based%20data%20collection%20remains%20unclear.%20The%20problem%0Ais%20how%20to%20enhance%20task%20performance%20while%20minimizing%20reliance%20on%20real-world%0Adata.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20teleoperation%20pipeline%20for%0Acollecting%20robotic%20manipulation%20data%20in%20simulation%20and%20training%20a%20few-shot%0Asim-to-real%20visual-motor%20policy.%20Force%20feedback%20devices%20are%20integrated%20into%20the%0Ateleoperation%20system%20to%20provide%20precise%20end-effector%20gripping%20force%20feedback.%0AExperiments%20across%20various%20manipulation%20tasks%20demonstrate%20that%20force%20feedback%0Asignificantly%20improves%20both%20success%20rates%20and%20execution%20efficiency%2C%0Aparticularly%20in%20simulation.%20Furthermore%2C%20experiments%20with%20different%20levels%20of%0Avisual%20rendering%20quality%20reveal%20that%20enhanced%20visual%20realism%20in%20simulation%0Asubstantially%20boosts%20task%20performance%20while%20reducing%20the%20need%20for%20real-world%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01301v2&entry.124074799=Read"},
{"title": "Scenario-based Compositional Verification of Autonomous Systems with\n  Neural Perception", "author": "Christopher Watson and Rajeev Alur and Divya Gopinath and Ravi Mangal and Corina S. Pasareanu", "abstract": "  Recent advances in deep learning have enabled the development of autonomous\nsystems that use deep neural networks for perception. Formal verification of\nthese systems is challenging due to the size and complexity of the perception\nDNNs as well as hard-to-quantify, changing environment conditions. To address\nthese challenges, we propose a probabilistic verification framework for\nautonomous systems based on the following key concepts: (1) Scenario-based\nModeling: We decompose the task (e.g., car navigation) into a composition of\nscenarios, each representing a different environment condition. (2)\nProbabilistic Abstractions: For each scenario, we build a compact abstraction\nof perception based on the DNN's performance on an offline dataset that\nrepresents the scenario's environment condition. (3) Symbolic Reasoning and\nAcceleration: The abstractions enable efficient compositional verification of\nthe autonomous system via symbolic reasoning and a novel acceleration proof\nrule that bounds the error probability of the system under arbitrary variations\nof environment conditions. We illustrate our approach on two case studies: an\nexperimental autonomous system that guides airplanes on taxiways using\nhigh-dimensional perception DNNs and a simulation model of an F1Tenth\nautonomous car using LiDAR observations.\n", "link": "http://arxiv.org/abs/2504.20942v1", "date": "2025-04-29", "relevancy": 2.2315, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5974}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scenario-based%20Compositional%20Verification%20of%20Autonomous%20Systems%20with%0A%20%20Neural%20Perception&body=Title%3A%20Scenario-based%20Compositional%20Verification%20of%20Autonomous%20Systems%20with%0A%20%20Neural%20Perception%0AAuthor%3A%20Christopher%20Watson%20and%20Rajeev%20Alur%20and%20Divya%20Gopinath%20and%20Ravi%20Mangal%20and%20Corina%20S.%20Pasareanu%0AAbstract%3A%20%20%20Recent%20advances%20in%20deep%20learning%20have%20enabled%20the%20development%20of%20autonomous%0Asystems%20that%20use%20deep%20neural%20networks%20for%20perception.%20Formal%20verification%20of%0Athese%20systems%20is%20challenging%20due%20to%20the%20size%20and%20complexity%20of%20the%20perception%0ADNNs%20as%20well%20as%20hard-to-quantify%2C%20changing%20environment%20conditions.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20probabilistic%20verification%20framework%20for%0Aautonomous%20systems%20based%20on%20the%20following%20key%20concepts%3A%20%281%29%20Scenario-based%0AModeling%3A%20We%20decompose%20the%20task%20%28e.g.%2C%20car%20navigation%29%20into%20a%20composition%20of%0Ascenarios%2C%20each%20representing%20a%20different%20environment%20condition.%20%282%29%0AProbabilistic%20Abstractions%3A%20For%20each%20scenario%2C%20we%20build%20a%20compact%20abstraction%0Aof%20perception%20based%20on%20the%20DNN%27s%20performance%20on%20an%20offline%20dataset%20that%0Arepresents%20the%20scenario%27s%20environment%20condition.%20%283%29%20Symbolic%20Reasoning%20and%0AAcceleration%3A%20The%20abstractions%20enable%20efficient%20compositional%20verification%20of%0Athe%20autonomous%20system%20via%20symbolic%20reasoning%20and%20a%20novel%20acceleration%20proof%0Arule%20that%20bounds%20the%20error%20probability%20of%20the%20system%20under%20arbitrary%20variations%0Aof%20environment%20conditions.%20We%20illustrate%20our%20approach%20on%20two%20case%20studies%3A%20an%0Aexperimental%20autonomous%20system%20that%20guides%20airplanes%20on%20taxiways%20using%0Ahigh-dimensional%20perception%20DNNs%20and%20a%20simulation%20model%20of%20an%20F1Tenth%0Aautonomous%20car%20using%20LiDAR%20observations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScenario-based%2520Compositional%2520Verification%2520of%2520Autonomous%2520Systems%2520with%250A%2520%2520Neural%2520Perception%26entry.906535625%3DChristopher%2520Watson%2520and%2520Rajeev%2520Alur%2520and%2520Divya%2520Gopinath%2520and%2520Ravi%2520Mangal%2520and%2520Corina%2520S.%2520Pasareanu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520deep%2520learning%2520have%2520enabled%2520the%2520development%2520of%2520autonomous%250Asystems%2520that%2520use%2520deep%2520neural%2520networks%2520for%2520perception.%2520Formal%2520verification%2520of%250Athese%2520systems%2520is%2520challenging%2520due%2520to%2520the%2520size%2520and%2520complexity%2520of%2520the%2520perception%250ADNNs%2520as%2520well%2520as%2520hard-to-quantify%252C%2520changing%2520environment%2520conditions.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520probabilistic%2520verification%2520framework%2520for%250Aautonomous%2520systems%2520based%2520on%2520the%2520following%2520key%2520concepts%253A%2520%25281%2529%2520Scenario-based%250AModeling%253A%2520We%2520decompose%2520the%2520task%2520%2528e.g.%252C%2520car%2520navigation%2529%2520into%2520a%2520composition%2520of%250Ascenarios%252C%2520each%2520representing%2520a%2520different%2520environment%2520condition.%2520%25282%2529%250AProbabilistic%2520Abstractions%253A%2520For%2520each%2520scenario%252C%2520we%2520build%2520a%2520compact%2520abstraction%250Aof%2520perception%2520based%2520on%2520the%2520DNN%2527s%2520performance%2520on%2520an%2520offline%2520dataset%2520that%250Arepresents%2520the%2520scenario%2527s%2520environment%2520condition.%2520%25283%2529%2520Symbolic%2520Reasoning%2520and%250AAcceleration%253A%2520The%2520abstractions%2520enable%2520efficient%2520compositional%2520verification%2520of%250Athe%2520autonomous%2520system%2520via%2520symbolic%2520reasoning%2520and%2520a%2520novel%2520acceleration%2520proof%250Arule%2520that%2520bounds%2520the%2520error%2520probability%2520of%2520the%2520system%2520under%2520arbitrary%2520variations%250Aof%2520environment%2520conditions.%2520We%2520illustrate%2520our%2520approach%2520on%2520two%2520case%2520studies%253A%2520an%250Aexperimental%2520autonomous%2520system%2520that%2520guides%2520airplanes%2520on%2520taxiways%2520using%250Ahigh-dimensional%2520perception%2520DNNs%2520and%2520a%2520simulation%2520model%2520of%2520an%2520F1Tenth%250Aautonomous%2520car%2520using%2520LiDAR%2520observations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scenario-based%20Compositional%20Verification%20of%20Autonomous%20Systems%20with%0A%20%20Neural%20Perception&entry.906535625=Christopher%20Watson%20and%20Rajeev%20Alur%20and%20Divya%20Gopinath%20and%20Ravi%20Mangal%20and%20Corina%20S.%20Pasareanu&entry.1292438233=%20%20Recent%20advances%20in%20deep%20learning%20have%20enabled%20the%20development%20of%20autonomous%0Asystems%20that%20use%20deep%20neural%20networks%20for%20perception.%20Formal%20verification%20of%0Athese%20systems%20is%20challenging%20due%20to%20the%20size%20and%20complexity%20of%20the%20perception%0ADNNs%20as%20well%20as%20hard-to-quantify%2C%20changing%20environment%20conditions.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20probabilistic%20verification%20framework%20for%0Aautonomous%20systems%20based%20on%20the%20following%20key%20concepts%3A%20%281%29%20Scenario-based%0AModeling%3A%20We%20decompose%20the%20task%20%28e.g.%2C%20car%20navigation%29%20into%20a%20composition%20of%0Ascenarios%2C%20each%20representing%20a%20different%20environment%20condition.%20%282%29%0AProbabilistic%20Abstractions%3A%20For%20each%20scenario%2C%20we%20build%20a%20compact%20abstraction%0Aof%20perception%20based%20on%20the%20DNN%27s%20performance%20on%20an%20offline%20dataset%20that%0Arepresents%20the%20scenario%27s%20environment%20condition.%20%283%29%20Symbolic%20Reasoning%20and%0AAcceleration%3A%20The%20abstractions%20enable%20efficient%20compositional%20verification%20of%0Athe%20autonomous%20system%20via%20symbolic%20reasoning%20and%20a%20novel%20acceleration%20proof%0Arule%20that%20bounds%20the%20error%20probability%20of%20the%20system%20under%20arbitrary%20variations%0Aof%20environment%20conditions.%20We%20illustrate%20our%20approach%20on%20two%20case%20studies%3A%20an%0Aexperimental%20autonomous%20system%20that%20guides%20airplanes%20on%20taxiways%20using%0Ahigh-dimensional%20perception%20DNNs%20and%20a%20simulation%20model%20of%20an%20F1Tenth%0Aautonomous%20car%20using%20LiDAR%20observations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20942v1&entry.124074799=Read"},
{"title": "FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language\n  Models", "author": "Mainak Singha and Subhankar Roy and Sarthak Mehrotra and Ankit Jha and Moloud Abdar and Biplab Banerjee and Elisa Ricci", "abstract": "  Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated\nlearning by tuning lightweight input tokens (or prompts) on local client data,\nwhile keeping network weights frozen. Post training, only the prompts are\nshared by the clients with the central server for aggregation. However, textual\nprompt tuning often struggles with overfitting to known concepts and may be\noverly reliant on memorized text features, limiting its adaptability to unseen\nconcepts. To address this limitation, we propose Federated Multimodal Visual\nPrompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual\ninformation -- image-conditioned features and textual attribute features of a\nclass -- that is multimodal in nature. At the core of FedMVP is a PromptFormer\nmodule that synergistically aligns textual and visual features through\ncross-attention, enabling richer contexual integration. The dynamically\ngenerated multimodal visual prompts are then input to the frozen vision encoder\nof CLIP, and trained with a combination of CLIP similarity loss and a\nconsistency loss. Extensive evaluation on 20 datasets spanning three\ngeneralization settings demonstrates that FedMVP not only preserves performance\non in-distribution classes and domains, but also displays higher\ngeneralizability to unseen classes and domains when compared to\nstate-of-the-art methods. Codes will be released upon acceptance.\n", "link": "http://arxiv.org/abs/2504.20860v1", "date": "2025-04-29", "relevancy": 2.2192, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5748}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedMVP%3A%20Federated%20Multi-modal%20Visual%20Prompt%20Tuning%20for%20Vision-Language%0A%20%20Models&body=Title%3A%20FedMVP%3A%20Federated%20Multi-modal%20Visual%20Prompt%20Tuning%20for%20Vision-Language%0A%20%20Models%0AAuthor%3A%20Mainak%20Singha%20and%20Subhankar%20Roy%20and%20Sarthak%20Mehrotra%20and%20Ankit%20Jha%20and%20Moloud%20Abdar%20and%20Biplab%20Banerjee%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20Textual%20prompt%20tuning%20adapts%20Vision-Language%20Models%20%28e.g.%2C%20CLIP%29%20in%20federated%0Alearning%20by%20tuning%20lightweight%20input%20tokens%20%28or%20prompts%29%20on%20local%20client%20data%2C%0Awhile%20keeping%20network%20weights%20frozen.%20Post%20training%2C%20only%20the%20prompts%20are%0Ashared%20by%20the%20clients%20with%20the%20central%20server%20for%20aggregation.%20However%2C%20textual%0Aprompt%20tuning%20often%20struggles%20with%20overfitting%20to%20known%20concepts%20and%20may%20be%0Aoverly%20reliant%20on%20memorized%20text%20features%2C%20limiting%20its%20adaptability%20to%20unseen%0Aconcepts.%20To%20address%20this%20limitation%2C%20we%20propose%20Federated%20Multimodal%20Visual%0APrompt%20Tuning%20%28FedMVP%29%20that%20conditions%20the%20prompts%20on%20comprehensive%20contextual%0Ainformation%20--%20image-conditioned%20features%20and%20textual%20attribute%20features%20of%20a%0Aclass%20--%20that%20is%20multimodal%20in%20nature.%20At%20the%20core%20of%20FedMVP%20is%20a%20PromptFormer%0Amodule%20that%20synergistically%20aligns%20textual%20and%20visual%20features%20through%0Across-attention%2C%20enabling%20richer%20contexual%20integration.%20The%20dynamically%0Agenerated%20multimodal%20visual%20prompts%20are%20then%20input%20to%20the%20frozen%20vision%20encoder%0Aof%20CLIP%2C%20and%20trained%20with%20a%20combination%20of%20CLIP%20similarity%20loss%20and%20a%0Aconsistency%20loss.%20Extensive%20evaluation%20on%2020%20datasets%20spanning%20three%0Ageneralization%20settings%20demonstrates%20that%20FedMVP%20not%20only%20preserves%20performance%0Aon%20in-distribution%20classes%20and%20domains%2C%20but%20also%20displays%20higher%0Ageneralizability%20to%20unseen%20classes%20and%20domains%20when%20compared%20to%0Astate-of-the-art%20methods.%20Codes%20will%20be%20released%20upon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedMVP%253A%2520Federated%2520Multi-modal%2520Visual%2520Prompt%2520Tuning%2520for%2520Vision-Language%250A%2520%2520Models%26entry.906535625%3DMainak%2520Singha%2520and%2520Subhankar%2520Roy%2520and%2520Sarthak%2520Mehrotra%2520and%2520Ankit%2520Jha%2520and%2520Moloud%2520Abdar%2520and%2520Biplab%2520Banerjee%2520and%2520Elisa%2520Ricci%26entry.1292438233%3D%2520%2520Textual%2520prompt%2520tuning%2520adapts%2520Vision-Language%2520Models%2520%2528e.g.%252C%2520CLIP%2529%2520in%2520federated%250Alearning%2520by%2520tuning%2520lightweight%2520input%2520tokens%2520%2528or%2520prompts%2529%2520on%2520local%2520client%2520data%252C%250Awhile%2520keeping%2520network%2520weights%2520frozen.%2520Post%2520training%252C%2520only%2520the%2520prompts%2520are%250Ashared%2520by%2520the%2520clients%2520with%2520the%2520central%2520server%2520for%2520aggregation.%2520However%252C%2520textual%250Aprompt%2520tuning%2520often%2520struggles%2520with%2520overfitting%2520to%2520known%2520concepts%2520and%2520may%2520be%250Aoverly%2520reliant%2520on%2520memorized%2520text%2520features%252C%2520limiting%2520its%2520adaptability%2520to%2520unseen%250Aconcepts.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Federated%2520Multimodal%2520Visual%250APrompt%2520Tuning%2520%2528FedMVP%2529%2520that%2520conditions%2520the%2520prompts%2520on%2520comprehensive%2520contextual%250Ainformation%2520--%2520image-conditioned%2520features%2520and%2520textual%2520attribute%2520features%2520of%2520a%250Aclass%2520--%2520that%2520is%2520multimodal%2520in%2520nature.%2520At%2520the%2520core%2520of%2520FedMVP%2520is%2520a%2520PromptFormer%250Amodule%2520that%2520synergistically%2520aligns%2520textual%2520and%2520visual%2520features%2520through%250Across-attention%252C%2520enabling%2520richer%2520contexual%2520integration.%2520The%2520dynamically%250Agenerated%2520multimodal%2520visual%2520prompts%2520are%2520then%2520input%2520to%2520the%2520frozen%2520vision%2520encoder%250Aof%2520CLIP%252C%2520and%2520trained%2520with%2520a%2520combination%2520of%2520CLIP%2520similarity%2520loss%2520and%2520a%250Aconsistency%2520loss.%2520Extensive%2520evaluation%2520on%252020%2520datasets%2520spanning%2520three%250Ageneralization%2520settings%2520demonstrates%2520that%2520FedMVP%2520not%2520only%2520preserves%2520performance%250Aon%2520in-distribution%2520classes%2520and%2520domains%252C%2520but%2520also%2520displays%2520higher%250Ageneralizability%2520to%2520unseen%2520classes%2520and%2520domains%2520when%2520compared%2520to%250Astate-of-the-art%2520methods.%2520Codes%2520will%2520be%2520released%2520upon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedMVP%3A%20Federated%20Multi-modal%20Visual%20Prompt%20Tuning%20for%20Vision-Language%0A%20%20Models&entry.906535625=Mainak%20Singha%20and%20Subhankar%20Roy%20and%20Sarthak%20Mehrotra%20and%20Ankit%20Jha%20and%20Moloud%20Abdar%20and%20Biplab%20Banerjee%20and%20Elisa%20Ricci&entry.1292438233=%20%20Textual%20prompt%20tuning%20adapts%20Vision-Language%20Models%20%28e.g.%2C%20CLIP%29%20in%20federated%0Alearning%20by%20tuning%20lightweight%20input%20tokens%20%28or%20prompts%29%20on%20local%20client%20data%2C%0Awhile%20keeping%20network%20weights%20frozen.%20Post%20training%2C%20only%20the%20prompts%20are%0Ashared%20by%20the%20clients%20with%20the%20central%20server%20for%20aggregation.%20However%2C%20textual%0Aprompt%20tuning%20often%20struggles%20with%20overfitting%20to%20known%20concepts%20and%20may%20be%0Aoverly%20reliant%20on%20memorized%20text%20features%2C%20limiting%20its%20adaptability%20to%20unseen%0Aconcepts.%20To%20address%20this%20limitation%2C%20we%20propose%20Federated%20Multimodal%20Visual%0APrompt%20Tuning%20%28FedMVP%29%20that%20conditions%20the%20prompts%20on%20comprehensive%20contextual%0Ainformation%20--%20image-conditioned%20features%20and%20textual%20attribute%20features%20of%20a%0Aclass%20--%20that%20is%20multimodal%20in%20nature.%20At%20the%20core%20of%20FedMVP%20is%20a%20PromptFormer%0Amodule%20that%20synergistically%20aligns%20textual%20and%20visual%20features%20through%0Across-attention%2C%20enabling%20richer%20contexual%20integration.%20The%20dynamically%0Agenerated%20multimodal%20visual%20prompts%20are%20then%20input%20to%20the%20frozen%20vision%20encoder%0Aof%20CLIP%2C%20and%20trained%20with%20a%20combination%20of%20CLIP%20similarity%20loss%20and%20a%0Aconsistency%20loss.%20Extensive%20evaluation%20on%2020%20datasets%20spanning%20three%0Ageneralization%20settings%20demonstrates%20that%20FedMVP%20not%20only%20preserves%20performance%0Aon%20in-distribution%20classes%20and%20domains%2C%20but%20also%20displays%20higher%0Ageneralizability%20to%20unseen%20classes%20and%20domains%20when%20compared%20to%0Astate-of-the-art%20methods.%20Codes%20will%20be%20released%20upon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20860v1&entry.124074799=Read"},
{"title": "Synthesising Activity Participations and Scheduling with Deep Generative\n  Machine Learning", "author": "Fred Shone and Tim Hillel", "abstract": "  Using a deep generative machine learning approach, we synthesise human\nactivity participations and scheduling (the choices of what activities to\nparticipate in and when). Activity schedules, which represent what people do\nand when, are a core component of many applied transport, energy, and\nepidemiology models. Our data-driven approach learns the distributions\nresulting from human preferences and scheduling logic without the need for\ncomplex interacting combinations of sub-models and custom rules, This makes our\napproach significantly faster and simpler to operate than existing approaches\nto synthesise or anonymise schedule data. We additionally contribute a novel\nschedule representation and a comprehensive evaluation framework. We evaluate a\nrange of schedule encoding and deep model architecture combinations. The\nevaluation shows our approach can rapidly generate large, diverse, novel, and\nrealistic synthetic samples of activity schedules.\n", "link": "http://arxiv.org/abs/2501.10221v2", "date": "2025-04-29", "relevancy": 2.1936, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5594}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5493}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthesising%20Activity%20Participations%20and%20Scheduling%20with%20Deep%20Generative%0A%20%20Machine%20Learning&body=Title%3A%20Synthesising%20Activity%20Participations%20and%20Scheduling%20with%20Deep%20Generative%0A%20%20Machine%20Learning%0AAuthor%3A%20Fred%20Shone%20and%20Tim%20Hillel%0AAbstract%3A%20%20%20Using%20a%20deep%20generative%20machine%20learning%20approach%2C%20we%20synthesise%20human%0Aactivity%20participations%20and%20scheduling%20%28the%20choices%20of%20what%20activities%20to%0Aparticipate%20in%20and%20when%29.%20Activity%20schedules%2C%20which%20represent%20what%20people%20do%0Aand%20when%2C%20are%20a%20core%20component%20of%20many%20applied%20transport%2C%20energy%2C%20and%0Aepidemiology%20models.%20Our%20data-driven%20approach%20learns%20the%20distributions%0Aresulting%20from%20human%20preferences%20and%20scheduling%20logic%20without%20the%20need%20for%0Acomplex%20interacting%20combinations%20of%20sub-models%20and%20custom%20rules%2C%20This%20makes%20our%0Aapproach%20significantly%20faster%20and%20simpler%20to%20operate%20than%20existing%20approaches%0Ato%20synthesise%20or%20anonymise%20schedule%20data.%20We%20additionally%20contribute%20a%20novel%0Aschedule%20representation%20and%20a%20comprehensive%20evaluation%20framework.%20We%20evaluate%20a%0Arange%20of%20schedule%20encoding%20and%20deep%20model%20architecture%20combinations.%20The%0Aevaluation%20shows%20our%20approach%20can%20rapidly%20generate%20large%2C%20diverse%2C%20novel%2C%20and%0Arealistic%20synthetic%20samples%20of%20activity%20schedules.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.10221v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesising%2520Activity%2520Participations%2520and%2520Scheduling%2520with%2520Deep%2520Generative%250A%2520%2520Machine%2520Learning%26entry.906535625%3DFred%2520Shone%2520and%2520Tim%2520Hillel%26entry.1292438233%3D%2520%2520Using%2520a%2520deep%2520generative%2520machine%2520learning%2520approach%252C%2520we%2520synthesise%2520human%250Aactivity%2520participations%2520and%2520scheduling%2520%2528the%2520choices%2520of%2520what%2520activities%2520to%250Aparticipate%2520in%2520and%2520when%2529.%2520Activity%2520schedules%252C%2520which%2520represent%2520what%2520people%2520do%250Aand%2520when%252C%2520are%2520a%2520core%2520component%2520of%2520many%2520applied%2520transport%252C%2520energy%252C%2520and%250Aepidemiology%2520models.%2520Our%2520data-driven%2520approach%2520learns%2520the%2520distributions%250Aresulting%2520from%2520human%2520preferences%2520and%2520scheduling%2520logic%2520without%2520the%2520need%2520for%250Acomplex%2520interacting%2520combinations%2520of%2520sub-models%2520and%2520custom%2520rules%252C%2520This%2520makes%2520our%250Aapproach%2520significantly%2520faster%2520and%2520simpler%2520to%2520operate%2520than%2520existing%2520approaches%250Ato%2520synthesise%2520or%2520anonymise%2520schedule%2520data.%2520We%2520additionally%2520contribute%2520a%2520novel%250Aschedule%2520representation%2520and%2520a%2520comprehensive%2520evaluation%2520framework.%2520We%2520evaluate%2520a%250Arange%2520of%2520schedule%2520encoding%2520and%2520deep%2520model%2520architecture%2520combinations.%2520The%250Aevaluation%2520shows%2520our%2520approach%2520can%2520rapidly%2520generate%2520large%252C%2520diverse%252C%2520novel%252C%2520and%250Arealistic%2520synthetic%2520samples%2520of%2520activity%2520schedules.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.10221v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesising%20Activity%20Participations%20and%20Scheduling%20with%20Deep%20Generative%0A%20%20Machine%20Learning&entry.906535625=Fred%20Shone%20and%20Tim%20Hillel&entry.1292438233=%20%20Using%20a%20deep%20generative%20machine%20learning%20approach%2C%20we%20synthesise%20human%0Aactivity%20participations%20and%20scheduling%20%28the%20choices%20of%20what%20activities%20to%0Aparticipate%20in%20and%20when%29.%20Activity%20schedules%2C%20which%20represent%20what%20people%20do%0Aand%20when%2C%20are%20a%20core%20component%20of%20many%20applied%20transport%2C%20energy%2C%20and%0Aepidemiology%20models.%20Our%20data-driven%20approach%20learns%20the%20distributions%0Aresulting%20from%20human%20preferences%20and%20scheduling%20logic%20without%20the%20need%20for%0Acomplex%20interacting%20combinations%20of%20sub-models%20and%20custom%20rules%2C%20This%20makes%20our%0Aapproach%20significantly%20faster%20and%20simpler%20to%20operate%20than%20existing%20approaches%0Ato%20synthesise%20or%20anonymise%20schedule%20data.%20We%20additionally%20contribute%20a%20novel%0Aschedule%20representation%20and%20a%20comprehensive%20evaluation%20framework.%20We%20evaluate%20a%0Arange%20of%20schedule%20encoding%20and%20deep%20model%20architecture%20combinations.%20The%0Aevaluation%20shows%20our%20approach%20can%20rapidly%20generate%20large%2C%20diverse%2C%20novel%2C%20and%0Arealistic%20synthetic%20samples%20of%20activity%20schedules.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.10221v2&entry.124074799=Read"},
{"title": "Geometry-Informed Neural Operator Transformer", "author": "Qibang Liu and Vincient Zhong and Hadi Meidani and Diab Abueidda and Seid Koric and Philippe Geubelle", "abstract": "  Machine-learning-based surrogate models offer significant computational\nefficiency and faster simulations compared to traditional numerical methods,\nespecially for problems requiring repeated evaluations of partial differential\nequations. This work introduces the Geometry-Informed Neural Operator\nTransformer (GINOT), which integrates the transformer architecture with the\nneural operator framework to enable forward predictions for arbitrary\ngeometries. GINOT encodes the surface points cloud of a geometry using a\nsampling and grouping mechanism combined with an attention mechanism, ensuring\ninvariance to point order and padding while maintaining robustness to\nvariations in point density. The geometry information is seamlessly integrated\nwith query points in the solution decoder through the attention mechanism. The\nperformance of GINOT is validated on multiple challenging datasets, showcasing\nits high accuracy and strong generalization capabilities for complex and\narbitrary 2D and 3D geometries.\n", "link": "http://arxiv.org/abs/2504.19452v2", "date": "2025-04-29", "relevancy": 2.1924, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5558}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5545}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Informed%20Neural%20Operator%20Transformer&body=Title%3A%20Geometry-Informed%20Neural%20Operator%20Transformer%0AAuthor%3A%20Qibang%20Liu%20and%20Vincient%20Zhong%20and%20Hadi%20Meidani%20and%20Diab%20Abueidda%20and%20Seid%20Koric%20and%20Philippe%20Geubelle%0AAbstract%3A%20%20%20Machine-learning-based%20surrogate%20models%20offer%20significant%20computational%0Aefficiency%20and%20faster%20simulations%20compared%20to%20traditional%20numerical%20methods%2C%0Aespecially%20for%20problems%20requiring%20repeated%20evaluations%20of%20partial%20differential%0Aequations.%20This%20work%20introduces%20the%20Geometry-Informed%20Neural%20Operator%0ATransformer%20%28GINOT%29%2C%20which%20integrates%20the%20transformer%20architecture%20with%20the%0Aneural%20operator%20framework%20to%20enable%20forward%20predictions%20for%20arbitrary%0Ageometries.%20GINOT%20encodes%20the%20surface%20points%20cloud%20of%20a%20geometry%20using%20a%0Asampling%20and%20grouping%20mechanism%20combined%20with%20an%20attention%20mechanism%2C%20ensuring%0Ainvariance%20to%20point%20order%20and%20padding%20while%20maintaining%20robustness%20to%0Avariations%20in%20point%20density.%20The%20geometry%20information%20is%20seamlessly%20integrated%0Awith%20query%20points%20in%20the%20solution%20decoder%20through%20the%20attention%20mechanism.%20The%0Aperformance%20of%20GINOT%20is%20validated%20on%20multiple%20challenging%20datasets%2C%20showcasing%0Aits%20high%20accuracy%20and%20strong%20generalization%20capabilities%20for%20complex%20and%0Aarbitrary%202D%20and%203D%20geometries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19452v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Informed%2520Neural%2520Operator%2520Transformer%26entry.906535625%3DQibang%2520Liu%2520and%2520Vincient%2520Zhong%2520and%2520Hadi%2520Meidani%2520and%2520Diab%2520Abueidda%2520and%2520Seid%2520Koric%2520and%2520Philippe%2520Geubelle%26entry.1292438233%3D%2520%2520Machine-learning-based%2520surrogate%2520models%2520offer%2520significant%2520computational%250Aefficiency%2520and%2520faster%2520simulations%2520compared%2520to%2520traditional%2520numerical%2520methods%252C%250Aespecially%2520for%2520problems%2520requiring%2520repeated%2520evaluations%2520of%2520partial%2520differential%250Aequations.%2520This%2520work%2520introduces%2520the%2520Geometry-Informed%2520Neural%2520Operator%250ATransformer%2520%2528GINOT%2529%252C%2520which%2520integrates%2520the%2520transformer%2520architecture%2520with%2520the%250Aneural%2520operator%2520framework%2520to%2520enable%2520forward%2520predictions%2520for%2520arbitrary%250Ageometries.%2520GINOT%2520encodes%2520the%2520surface%2520points%2520cloud%2520of%2520a%2520geometry%2520using%2520a%250Asampling%2520and%2520grouping%2520mechanism%2520combined%2520with%2520an%2520attention%2520mechanism%252C%2520ensuring%250Ainvariance%2520to%2520point%2520order%2520and%2520padding%2520while%2520maintaining%2520robustness%2520to%250Avariations%2520in%2520point%2520density.%2520The%2520geometry%2520information%2520is%2520seamlessly%2520integrated%250Awith%2520query%2520points%2520in%2520the%2520solution%2520decoder%2520through%2520the%2520attention%2520mechanism.%2520The%250Aperformance%2520of%2520GINOT%2520is%2520validated%2520on%2520multiple%2520challenging%2520datasets%252C%2520showcasing%250Aits%2520high%2520accuracy%2520and%2520strong%2520generalization%2520capabilities%2520for%2520complex%2520and%250Aarbitrary%25202D%2520and%25203D%2520geometries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19452v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Informed%20Neural%20Operator%20Transformer&entry.906535625=Qibang%20Liu%20and%20Vincient%20Zhong%20and%20Hadi%20Meidani%20and%20Diab%20Abueidda%20and%20Seid%20Koric%20and%20Philippe%20Geubelle&entry.1292438233=%20%20Machine-learning-based%20surrogate%20models%20offer%20significant%20computational%0Aefficiency%20and%20faster%20simulations%20compared%20to%20traditional%20numerical%20methods%2C%0Aespecially%20for%20problems%20requiring%20repeated%20evaluations%20of%20partial%20differential%0Aequations.%20This%20work%20introduces%20the%20Geometry-Informed%20Neural%20Operator%0ATransformer%20%28GINOT%29%2C%20which%20integrates%20the%20transformer%20architecture%20with%20the%0Aneural%20operator%20framework%20to%20enable%20forward%20predictions%20for%20arbitrary%0Ageometries.%20GINOT%20encodes%20the%20surface%20points%20cloud%20of%20a%20geometry%20using%20a%0Asampling%20and%20grouping%20mechanism%20combined%20with%20an%20attention%20mechanism%2C%20ensuring%0Ainvariance%20to%20point%20order%20and%20padding%20while%20maintaining%20robustness%20to%0Avariations%20in%20point%20density.%20The%20geometry%20information%20is%20seamlessly%20integrated%0Awith%20query%20points%20in%20the%20solution%20decoder%20through%20the%20attention%20mechanism.%20The%0Aperformance%20of%20GINOT%20is%20validated%20on%20multiple%20challenging%20datasets%2C%20showcasing%0Aits%20high%20accuracy%20and%20strong%20generalization%20capabilities%20for%20complex%20and%0Aarbitrary%202D%20and%203D%20geometries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19452v2&entry.124074799=Read"},
{"title": "Agentic AI: The Era of Semantic Decoding", "author": "Maxime Peyrard and Martin Josifoski and Robert West", "abstract": "  Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.\n", "link": "http://arxiv.org/abs/2403.14562v2", "date": "2025-04-29", "relevancy": 2.179, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20AI%3A%20The%20Era%20of%20Semantic%20Decoding&body=Title%3A%20Agentic%20AI%3A%20The%20Era%20of%20Semantic%20Decoding%0AAuthor%3A%20Maxime%20Peyrard%20and%20Martin%20Josifoski%20and%20Robert%20West%0AAbstract%3A%20%20%20Recent%20work%20demonstrated%20great%20promise%20in%20the%20idea%20of%20orchestrating%0Acollaborations%20between%20LLMs%2C%20human%20input%2C%20and%20various%20tools%20to%20address%20the%0Ainherent%20limitations%20of%20LLMs.%20We%20propose%20a%20novel%20perspective%20called%20semantic%0Adecoding%2C%20which%20frames%20these%20collaborative%20processes%20as%20optimization%20procedures%0Ain%20semantic%20space.%20Specifically%2C%20we%20conceptualize%20LLMs%20as%20semantic%20processors%0Athat%20manipulate%20meaningful%20pieces%20of%20information%20that%20we%20call%20semantic%20tokens%0A%28known%20thoughts%29.%20LLMs%20are%20among%20a%20large%20pool%20of%20other%20semantic%20processors%2C%0Aincluding%20humans%20and%20tools%2C%20such%20as%20search%20engines%20or%20code%20executors.%0ACollectively%2C%20semantic%20processors%20engage%20in%20dynamic%20exchanges%20of%20semantic%0Atokens%20to%20progressively%20construct%20high-utility%20outputs.%20We%20refer%20to%20these%0Aorchestrated%20interactions%20among%20semantic%20processors%2C%20optimizing%20and%20searching%0Ain%20semantic%20space%2C%20as%20semantic%20decoding%20algorithms.%20This%20concept%20draws%20a%20direct%0Aparallel%20to%20the%20well-studied%20problem%20of%20syntactic%20decoding%2C%20which%20involves%0Acrafting%20algorithms%20to%20best%20exploit%20auto-regressive%20language%20models%20for%0Aextracting%20high-utility%20sequences%20of%20syntactic%20tokens.%20By%20focusing%20on%20the%0Asemantic%20level%20and%20disregarding%20syntactic%20details%2C%20we%20gain%20a%20fresh%20perspective%0Aon%20the%20engineering%20of%20AI%20systems%2C%20enabling%20us%20to%20imagine%20systems%20with%20much%0Agreater%20complexity%20and%20capabilities.%20In%20this%20position%20paper%2C%20we%20formalize%20the%0Atransition%20from%20syntactic%20to%20semantic%20tokens%20as%20well%20as%20the%20analogy%20between%0Asyntactic%20and%20semantic%20decoding.%20Subsequently%2C%20we%20explore%20the%20possibilities%20of%0Aoptimizing%20within%20the%20space%20of%20semantic%20tokens%20via%20semantic%20decoding%0Aalgorithms.%20We%20conclude%20with%20a%20list%20of%20research%20opportunities%20and%20questions%0Aarising%20from%20this%20fresh%20perspective.%20The%20semantic%20decoding%20perspective%20offers%20a%0Apowerful%20abstraction%20for%20search%20and%20optimization%20directly%20in%20the%20space%20of%0Ameaningful%20concepts%2C%20with%20semantic%20tokens%20as%20the%20fundamental%20units%20of%20a%20new%0Atype%20of%20computation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14562v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520AI%253A%2520The%2520Era%2520of%2520Semantic%2520Decoding%26entry.906535625%3DMaxime%2520Peyrard%2520and%2520Martin%2520Josifoski%2520and%2520Robert%2520West%26entry.1292438233%3D%2520%2520Recent%2520work%2520demonstrated%2520great%2520promise%2520in%2520the%2520idea%2520of%2520orchestrating%250Acollaborations%2520between%2520LLMs%252C%2520human%2520input%252C%2520and%2520various%2520tools%2520to%2520address%2520the%250Ainherent%2520limitations%2520of%2520LLMs.%2520We%2520propose%2520a%2520novel%2520perspective%2520called%2520semantic%250Adecoding%252C%2520which%2520frames%2520these%2520collaborative%2520processes%2520as%2520optimization%2520procedures%250Ain%2520semantic%2520space.%2520Specifically%252C%2520we%2520conceptualize%2520LLMs%2520as%2520semantic%2520processors%250Athat%2520manipulate%2520meaningful%2520pieces%2520of%2520information%2520that%2520we%2520call%2520semantic%2520tokens%250A%2528known%2520thoughts%2529.%2520LLMs%2520are%2520among%2520a%2520large%2520pool%2520of%2520other%2520semantic%2520processors%252C%250Aincluding%2520humans%2520and%2520tools%252C%2520such%2520as%2520search%2520engines%2520or%2520code%2520executors.%250ACollectively%252C%2520semantic%2520processors%2520engage%2520in%2520dynamic%2520exchanges%2520of%2520semantic%250Atokens%2520to%2520progressively%2520construct%2520high-utility%2520outputs.%2520We%2520refer%2520to%2520these%250Aorchestrated%2520interactions%2520among%2520semantic%2520processors%252C%2520optimizing%2520and%2520searching%250Ain%2520semantic%2520space%252C%2520as%2520semantic%2520decoding%2520algorithms.%2520This%2520concept%2520draws%2520a%2520direct%250Aparallel%2520to%2520the%2520well-studied%2520problem%2520of%2520syntactic%2520decoding%252C%2520which%2520involves%250Acrafting%2520algorithms%2520to%2520best%2520exploit%2520auto-regressive%2520language%2520models%2520for%250Aextracting%2520high-utility%2520sequences%2520of%2520syntactic%2520tokens.%2520By%2520focusing%2520on%2520the%250Asemantic%2520level%2520and%2520disregarding%2520syntactic%2520details%252C%2520we%2520gain%2520a%2520fresh%2520perspective%250Aon%2520the%2520engineering%2520of%2520AI%2520systems%252C%2520enabling%2520us%2520to%2520imagine%2520systems%2520with%2520much%250Agreater%2520complexity%2520and%2520capabilities.%2520In%2520this%2520position%2520paper%252C%2520we%2520formalize%2520the%250Atransition%2520from%2520syntactic%2520to%2520semantic%2520tokens%2520as%2520well%2520as%2520the%2520analogy%2520between%250Asyntactic%2520and%2520semantic%2520decoding.%2520Subsequently%252C%2520we%2520explore%2520the%2520possibilities%2520of%250Aoptimizing%2520within%2520the%2520space%2520of%2520semantic%2520tokens%2520via%2520semantic%2520decoding%250Aalgorithms.%2520We%2520conclude%2520with%2520a%2520list%2520of%2520research%2520opportunities%2520and%2520questions%250Aarising%2520from%2520this%2520fresh%2520perspective.%2520The%2520semantic%2520decoding%2520perspective%2520offers%2520a%250Apowerful%2520abstraction%2520for%2520search%2520and%2520optimization%2520directly%2520in%2520the%2520space%2520of%250Ameaningful%2520concepts%252C%2520with%2520semantic%2520tokens%2520as%2520the%2520fundamental%2520units%2520of%2520a%2520new%250Atype%2520of%2520computation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14562v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20AI%3A%20The%20Era%20of%20Semantic%20Decoding&entry.906535625=Maxime%20Peyrard%20and%20Martin%20Josifoski%20and%20Robert%20West&entry.1292438233=%20%20Recent%20work%20demonstrated%20great%20promise%20in%20the%20idea%20of%20orchestrating%0Acollaborations%20between%20LLMs%2C%20human%20input%2C%20and%20various%20tools%20to%20address%20the%0Ainherent%20limitations%20of%20LLMs.%20We%20propose%20a%20novel%20perspective%20called%20semantic%0Adecoding%2C%20which%20frames%20these%20collaborative%20processes%20as%20optimization%20procedures%0Ain%20semantic%20space.%20Specifically%2C%20we%20conceptualize%20LLMs%20as%20semantic%20processors%0Athat%20manipulate%20meaningful%20pieces%20of%20information%20that%20we%20call%20semantic%20tokens%0A%28known%20thoughts%29.%20LLMs%20are%20among%20a%20large%20pool%20of%20other%20semantic%20processors%2C%0Aincluding%20humans%20and%20tools%2C%20such%20as%20search%20engines%20or%20code%20executors.%0ACollectively%2C%20semantic%20processors%20engage%20in%20dynamic%20exchanges%20of%20semantic%0Atokens%20to%20progressively%20construct%20high-utility%20outputs.%20We%20refer%20to%20these%0Aorchestrated%20interactions%20among%20semantic%20processors%2C%20optimizing%20and%20searching%0Ain%20semantic%20space%2C%20as%20semantic%20decoding%20algorithms.%20This%20concept%20draws%20a%20direct%0Aparallel%20to%20the%20well-studied%20problem%20of%20syntactic%20decoding%2C%20which%20involves%0Acrafting%20algorithms%20to%20best%20exploit%20auto-regressive%20language%20models%20for%0Aextracting%20high-utility%20sequences%20of%20syntactic%20tokens.%20By%20focusing%20on%20the%0Asemantic%20level%20and%20disregarding%20syntactic%20details%2C%20we%20gain%20a%20fresh%20perspective%0Aon%20the%20engineering%20of%20AI%20systems%2C%20enabling%20us%20to%20imagine%20systems%20with%20much%0Agreater%20complexity%20and%20capabilities.%20In%20this%20position%20paper%2C%20we%20formalize%20the%0Atransition%20from%20syntactic%20to%20semantic%20tokens%20as%20well%20as%20the%20analogy%20between%0Asyntactic%20and%20semantic%20decoding.%20Subsequently%2C%20we%20explore%20the%20possibilities%20of%0Aoptimizing%20within%20the%20space%20of%20semantic%20tokens%20via%20semantic%20decoding%0Aalgorithms.%20We%20conclude%20with%20a%20list%20of%20research%20opportunities%20and%20questions%0Aarising%20from%20this%20fresh%20perspective.%20The%20semantic%20decoding%20perspective%20offers%20a%0Apowerful%20abstraction%20for%20search%20and%20optimization%20directly%20in%20the%20space%20of%0Ameaningful%20concepts%2C%20with%20semantic%20tokens%20as%20the%20fundamental%20units%20of%20a%20new%0Atype%20of%20computation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14562v2&entry.124074799=Read"},
{"title": "One-Shot Clustering for Federated Learning", "author": "Maciej Krzysztof Zuziak and Roberto Pellungrini and Salvatore Rinzivillo", "abstract": "  Federated Learning (FL) is a widespread and well adopted paradigm of\ndecentralized learning that allows training one model from multiple sources\nwithout the need to directly transfer data between participating clients. Since\nits inception in 2015, it has been divided into numerous sub-fields that deal\nwith application-specific issues, be it data heterogeneity or resource\nallocation. One such sub-field, Clustered Federated Learning (CFL), is dealing\nwith the problem of clustering the population of clients into separate cohorts\nto deliver personalized models. Although few remarkable works have been\npublished in this domain, the problem is still largely unexplored, as its basic\nassumption and settings are slightly different from standard FL. In this work,\nwe present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic\nalgorithm that can automatically detect the earliest suitable moment for\nclustering. Our algorithm is based on the computation of cosine similarity\nbetween gradients of the clients and a temperature measure that detects when\nthe federated model starts to converge. We empirically evaluate our methodology\nby testing various one-shot clustering algorithms for over thirty different\ntasks on three benchmark datasets. Our experiments showcase the good\nperformance of our approach when used to perform CFL in an automated manner\nwithout the need to adjust hyperparameters.\n", "link": "http://arxiv.org/abs/2503.04231v2", "date": "2025-04-29", "relevancy": 2.1653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4453}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.428}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One-Shot%20Clustering%20for%20Federated%20Learning&body=Title%3A%20One-Shot%20Clustering%20for%20Federated%20Learning%0AAuthor%3A%20Maciej%20Krzysztof%20Zuziak%20and%20Roberto%20Pellungrini%20and%20Salvatore%20Rinzivillo%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20widespread%20and%20well%20adopted%20paradigm%20of%0Adecentralized%20learning%20that%20allows%20training%20one%20model%20from%20multiple%20sources%0Awithout%20the%20need%20to%20directly%20transfer%20data%20between%20participating%20clients.%20Since%0Aits%20inception%20in%202015%2C%20it%20has%20been%20divided%20into%20numerous%20sub-fields%20that%20deal%0Awith%20application-specific%20issues%2C%20be%20it%20data%20heterogeneity%20or%20resource%0Aallocation.%20One%20such%20sub-field%2C%20Clustered%20Federated%20Learning%20%28CFL%29%2C%20is%20dealing%0Awith%20the%20problem%20of%20clustering%20the%20population%20of%20clients%20into%20separate%20cohorts%0Ato%20deliver%20personalized%20models.%20Although%20few%20remarkable%20works%20have%20been%0Apublished%20in%20this%20domain%2C%20the%20problem%20is%20still%20largely%20unexplored%2C%20as%20its%20basic%0Aassumption%20and%20settings%20are%20slightly%20different%20from%20standard%20FL.%20In%20this%20work%2C%0Awe%20present%20One-Shot%20Clustered%20Federated%20Learning%20%28OCFL%29%2C%20a%20clustering-agnostic%0Aalgorithm%20that%20can%20automatically%20detect%20the%20earliest%20suitable%20moment%20for%0Aclustering.%20Our%20algorithm%20is%20based%20on%20the%20computation%20of%20cosine%20similarity%0Abetween%20gradients%20of%20the%20clients%20and%20a%20temperature%20measure%20that%20detects%20when%0Athe%20federated%20model%20starts%20to%20converge.%20We%20empirically%20evaluate%20our%20methodology%0Aby%20testing%20various%20one-shot%20clustering%20algorithms%20for%20over%20thirty%20different%0Atasks%20on%20three%20benchmark%20datasets.%20Our%20experiments%20showcase%20the%20good%0Aperformance%20of%20our%20approach%20when%20used%20to%20perform%20CFL%20in%20an%20automated%20manner%0Awithout%20the%20need%20to%20adjust%20hyperparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04231v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne-Shot%2520Clustering%2520for%2520Federated%2520Learning%26entry.906535625%3DMaciej%2520Krzysztof%2520Zuziak%2520and%2520Roberto%2520Pellungrini%2520and%2520Salvatore%2520Rinzivillo%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520widespread%2520and%2520well%2520adopted%2520paradigm%2520of%250Adecentralized%2520learning%2520that%2520allows%2520training%2520one%2520model%2520from%2520multiple%2520sources%250Awithout%2520the%2520need%2520to%2520directly%2520transfer%2520data%2520between%2520participating%2520clients.%2520Since%250Aits%2520inception%2520in%25202015%252C%2520it%2520has%2520been%2520divided%2520into%2520numerous%2520sub-fields%2520that%2520deal%250Awith%2520application-specific%2520issues%252C%2520be%2520it%2520data%2520heterogeneity%2520or%2520resource%250Aallocation.%2520One%2520such%2520sub-field%252C%2520Clustered%2520Federated%2520Learning%2520%2528CFL%2529%252C%2520is%2520dealing%250Awith%2520the%2520problem%2520of%2520clustering%2520the%2520population%2520of%2520clients%2520into%2520separate%2520cohorts%250Ato%2520deliver%2520personalized%2520models.%2520Although%2520few%2520remarkable%2520works%2520have%2520been%250Apublished%2520in%2520this%2520domain%252C%2520the%2520problem%2520is%2520still%2520largely%2520unexplored%252C%2520as%2520its%2520basic%250Aassumption%2520and%2520settings%2520are%2520slightly%2520different%2520from%2520standard%2520FL.%2520In%2520this%2520work%252C%250Awe%2520present%2520One-Shot%2520Clustered%2520Federated%2520Learning%2520%2528OCFL%2529%252C%2520a%2520clustering-agnostic%250Aalgorithm%2520that%2520can%2520automatically%2520detect%2520the%2520earliest%2520suitable%2520moment%2520for%250Aclustering.%2520Our%2520algorithm%2520is%2520based%2520on%2520the%2520computation%2520of%2520cosine%2520similarity%250Abetween%2520gradients%2520of%2520the%2520clients%2520and%2520a%2520temperature%2520measure%2520that%2520detects%2520when%250Athe%2520federated%2520model%2520starts%2520to%2520converge.%2520We%2520empirically%2520evaluate%2520our%2520methodology%250Aby%2520testing%2520various%2520one-shot%2520clustering%2520algorithms%2520for%2520over%2520thirty%2520different%250Atasks%2520on%2520three%2520benchmark%2520datasets.%2520Our%2520experiments%2520showcase%2520the%2520good%250Aperformance%2520of%2520our%2520approach%2520when%2520used%2520to%2520perform%2520CFL%2520in%2520an%2520automated%2520manner%250Awithout%2520the%2520need%2520to%2520adjust%2520hyperparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04231v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-Shot%20Clustering%20for%20Federated%20Learning&entry.906535625=Maciej%20Krzysztof%20Zuziak%20and%20Roberto%20Pellungrini%20and%20Salvatore%20Rinzivillo&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20widespread%20and%20well%20adopted%20paradigm%20of%0Adecentralized%20learning%20that%20allows%20training%20one%20model%20from%20multiple%20sources%0Awithout%20the%20need%20to%20directly%20transfer%20data%20between%20participating%20clients.%20Since%0Aits%20inception%20in%202015%2C%20it%20has%20been%20divided%20into%20numerous%20sub-fields%20that%20deal%0Awith%20application-specific%20issues%2C%20be%20it%20data%20heterogeneity%20or%20resource%0Aallocation.%20One%20such%20sub-field%2C%20Clustered%20Federated%20Learning%20%28CFL%29%2C%20is%20dealing%0Awith%20the%20problem%20of%20clustering%20the%20population%20of%20clients%20into%20separate%20cohorts%0Ato%20deliver%20personalized%20models.%20Although%20few%20remarkable%20works%20have%20been%0Apublished%20in%20this%20domain%2C%20the%20problem%20is%20still%20largely%20unexplored%2C%20as%20its%20basic%0Aassumption%20and%20settings%20are%20slightly%20different%20from%20standard%20FL.%20In%20this%20work%2C%0Awe%20present%20One-Shot%20Clustered%20Federated%20Learning%20%28OCFL%29%2C%20a%20clustering-agnostic%0Aalgorithm%20that%20can%20automatically%20detect%20the%20earliest%20suitable%20moment%20for%0Aclustering.%20Our%20algorithm%20is%20based%20on%20the%20computation%20of%20cosine%20similarity%0Abetween%20gradients%20of%20the%20clients%20and%20a%20temperature%20measure%20that%20detects%20when%0Athe%20federated%20model%20starts%20to%20converge.%20We%20empirically%20evaluate%20our%20methodology%0Aby%20testing%20various%20one-shot%20clustering%20algorithms%20for%20over%20thirty%20different%0Atasks%20on%20three%20benchmark%20datasets.%20Our%20experiments%20showcase%20the%20good%0Aperformance%20of%20our%20approach%20when%20used%20to%20perform%20CFL%20in%20an%20automated%20manner%0Awithout%20the%20need%20to%20adjust%20hyperparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04231v2&entry.124074799=Read"},
{"title": "Many-Worlds Inverse Rendering", "author": "Ziyi Zhang and Nicolas Roussel and Wenzel Jakob", "abstract": "  Discontinuous visibility changes remain a major bottleneck when optimizing\nsurfaces within a physically-based inverse renderer. Many previous works have\nproposed sophisticated algorithms and data structures to sample visibility\nsilhouettes more efficiently.\n  Our work presents another solution: instead of differentiating a tentative\nsurface locally, we differentiate a volumetric perturbation of a surface. We\nrefer this as a many-worlds representation because it models a non-interacting\nsuperposition of conflicting explanations (worlds) of the input dataset. Each\nworld is optically isolated from others, leading to a new transport law that\ndistinguishes our method from prior work based on exponential random media.\n  The resulting Monte Carlo algorithm is simpler and more efficient than prior\nmethods. We demonstrate that our method promotes rapid convergence, both in\nterms of the total iteration count and the cost per iteration.\n", "link": "http://arxiv.org/abs/2408.16005v4", "date": "2025-04-29", "relevancy": 2.1531, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5407}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5407}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Many-Worlds%20Inverse%20Rendering&body=Title%3A%20Many-Worlds%20Inverse%20Rendering%0AAuthor%3A%20Ziyi%20Zhang%20and%20Nicolas%20Roussel%20and%20Wenzel%20Jakob%0AAbstract%3A%20%20%20Discontinuous%20visibility%20changes%20remain%20a%20major%20bottleneck%20when%20optimizing%0Asurfaces%20within%20a%20physically-based%20inverse%20renderer.%20Many%20previous%20works%20have%0Aproposed%20sophisticated%20algorithms%20and%20data%20structures%20to%20sample%20visibility%0Asilhouettes%20more%20efficiently.%0A%20%20Our%20work%20presents%20another%20solution%3A%20instead%20of%20differentiating%20a%20tentative%0Asurface%20locally%2C%20we%20differentiate%20a%20volumetric%20perturbation%20of%20a%20surface.%20We%0Arefer%20this%20as%20a%20many-worlds%20representation%20because%20it%20models%20a%20non-interacting%0Asuperposition%20of%20conflicting%20explanations%20%28worlds%29%20of%20the%20input%20dataset.%20Each%0Aworld%20is%20optically%20isolated%20from%20others%2C%20leading%20to%20a%20new%20transport%20law%20that%0Adistinguishes%20our%20method%20from%20prior%20work%20based%20on%20exponential%20random%20media.%0A%20%20The%20resulting%20Monte%20Carlo%20algorithm%20is%20simpler%20and%20more%20efficient%20than%20prior%0Amethods.%20We%20demonstrate%20that%20our%20method%20promotes%20rapid%20convergence%2C%20both%20in%0Aterms%20of%20the%20total%20iteration%20count%20and%20the%20cost%20per%20iteration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.16005v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMany-Worlds%2520Inverse%2520Rendering%26entry.906535625%3DZiyi%2520Zhang%2520and%2520Nicolas%2520Roussel%2520and%2520Wenzel%2520Jakob%26entry.1292438233%3D%2520%2520Discontinuous%2520visibility%2520changes%2520remain%2520a%2520major%2520bottleneck%2520when%2520optimizing%250Asurfaces%2520within%2520a%2520physically-based%2520inverse%2520renderer.%2520Many%2520previous%2520works%2520have%250Aproposed%2520sophisticated%2520algorithms%2520and%2520data%2520structures%2520to%2520sample%2520visibility%250Asilhouettes%2520more%2520efficiently.%250A%2520%2520Our%2520work%2520presents%2520another%2520solution%253A%2520instead%2520of%2520differentiating%2520a%2520tentative%250Asurface%2520locally%252C%2520we%2520differentiate%2520a%2520volumetric%2520perturbation%2520of%2520a%2520surface.%2520We%250Arefer%2520this%2520as%2520a%2520many-worlds%2520representation%2520because%2520it%2520models%2520a%2520non-interacting%250Asuperposition%2520of%2520conflicting%2520explanations%2520%2528worlds%2529%2520of%2520the%2520input%2520dataset.%2520Each%250Aworld%2520is%2520optically%2520isolated%2520from%2520others%252C%2520leading%2520to%2520a%2520new%2520transport%2520law%2520that%250Adistinguishes%2520our%2520method%2520from%2520prior%2520work%2520based%2520on%2520exponential%2520random%2520media.%250A%2520%2520The%2520resulting%2520Monte%2520Carlo%2520algorithm%2520is%2520simpler%2520and%2520more%2520efficient%2520than%2520prior%250Amethods.%2520We%2520demonstrate%2520that%2520our%2520method%2520promotes%2520rapid%2520convergence%252C%2520both%2520in%250Aterms%2520of%2520the%2520total%2520iteration%2520count%2520and%2520the%2520cost%2520per%2520iteration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.16005v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Many-Worlds%20Inverse%20Rendering&entry.906535625=Ziyi%20Zhang%20and%20Nicolas%20Roussel%20and%20Wenzel%20Jakob&entry.1292438233=%20%20Discontinuous%20visibility%20changes%20remain%20a%20major%20bottleneck%20when%20optimizing%0Asurfaces%20within%20a%20physically-based%20inverse%20renderer.%20Many%20previous%20works%20have%0Aproposed%20sophisticated%20algorithms%20and%20data%20structures%20to%20sample%20visibility%0Asilhouettes%20more%20efficiently.%0A%20%20Our%20work%20presents%20another%20solution%3A%20instead%20of%20differentiating%20a%20tentative%0Asurface%20locally%2C%20we%20differentiate%20a%20volumetric%20perturbation%20of%20a%20surface.%20We%0Arefer%20this%20as%20a%20many-worlds%20representation%20because%20it%20models%20a%20non-interacting%0Asuperposition%20of%20conflicting%20explanations%20%28worlds%29%20of%20the%20input%20dataset.%20Each%0Aworld%20is%20optically%20isolated%20from%20others%2C%20leading%20to%20a%20new%20transport%20law%20that%0Adistinguishes%20our%20method%20from%20prior%20work%20based%20on%20exponential%20random%20media.%0A%20%20The%20resulting%20Monte%20Carlo%20algorithm%20is%20simpler%20and%20more%20efficient%20than%20prior%0Amethods.%20We%20demonstrate%20that%20our%20method%20promotes%20rapid%20convergence%2C%20both%20in%0Aterms%20of%20the%20total%20iteration%20count%20and%20the%20cost%20per%20iteration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.16005v4&entry.124074799=Read"},
{"title": "Imperative MPC: An End-to-End Self-Supervised Learning with\n  Differentiable MPC for UAV Attitude Control", "author": "Haonan He and Yuheng Qiu and Junyi Geng", "abstract": "  Modeling and control of nonlinear dynamics are critical in robotics,\nespecially in scenarios with unpredictable external influences and complex\ndynamics. Traditional cascaded modular control pipelines often yield suboptimal\nperformance due to conservative assumptions and tedious parameter tuning. Pure\ndata-driven approaches promise robust performance but suffer from low sample\nefficiency, sim-to-real gaps, and reliance on extensive datasets. Hybrid\nmethods combining learning-based and traditional model-based control in an\nend-to-end manner offer a promising alternative. This work presents a\nself-supervised learning framework combining learning-based inertial odometry\n(IO) module and differentiable model predictive control (d-MPC) for Unmanned\nAerial Vehicle (UAV) attitude control. The IO denoises raw IMU measurements and\npredicts UAV attitudes, which are then optimized by MPC for control actions in\na bi-level optimization (BLO) setup, where the inner MPC optimizes control\nactions and the upper level minimizes discrepancy between real-world and\npredicted performance. The framework is thus end-to-end and can be trained in a\nself-supervised manner. This approach combines the strength of learning-based\nperception with the interpretable model-based control. Results show the\neffectiveness even under strong wind. It can simultaneously enhance both the\nMPC parameter learning and IMU prediction performance.\n", "link": "http://arxiv.org/abs/2504.13088v2", "date": "2025-04-29", "relevancy": 2.1511, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5654}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5418}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Imperative%20MPC%3A%20An%20End-to-End%20Self-Supervised%20Learning%20with%0A%20%20Differentiable%20MPC%20for%20UAV%20Attitude%20Control&body=Title%3A%20Imperative%20MPC%3A%20An%20End-to-End%20Self-Supervised%20Learning%20with%0A%20%20Differentiable%20MPC%20for%20UAV%20Attitude%20Control%0AAuthor%3A%20Haonan%20He%20and%20Yuheng%20Qiu%20and%20Junyi%20Geng%0AAbstract%3A%20%20%20Modeling%20and%20control%20of%20nonlinear%20dynamics%20are%20critical%20in%20robotics%2C%0Aespecially%20in%20scenarios%20with%20unpredictable%20external%20influences%20and%20complex%0Adynamics.%20Traditional%20cascaded%20modular%20control%20pipelines%20often%20yield%20suboptimal%0Aperformance%20due%20to%20conservative%20assumptions%20and%20tedious%20parameter%20tuning.%20Pure%0Adata-driven%20approaches%20promise%20robust%20performance%20but%20suffer%20from%20low%20sample%0Aefficiency%2C%20sim-to-real%20gaps%2C%20and%20reliance%20on%20extensive%20datasets.%20Hybrid%0Amethods%20combining%20learning-based%20and%20traditional%20model-based%20control%20in%20an%0Aend-to-end%20manner%20offer%20a%20promising%20alternative.%20This%20work%20presents%20a%0Aself-supervised%20learning%20framework%20combining%20learning-based%20inertial%20odometry%0A%28IO%29%20module%20and%20differentiable%20model%20predictive%20control%20%28d-MPC%29%20for%20Unmanned%0AAerial%20Vehicle%20%28UAV%29%20attitude%20control.%20The%20IO%20denoises%20raw%20IMU%20measurements%20and%0Apredicts%20UAV%20attitudes%2C%20which%20are%20then%20optimized%20by%20MPC%20for%20control%20actions%20in%0Aa%20bi-level%20optimization%20%28BLO%29%20setup%2C%20where%20the%20inner%20MPC%20optimizes%20control%0Aactions%20and%20the%20upper%20level%20minimizes%20discrepancy%20between%20real-world%20and%0Apredicted%20performance.%20The%20framework%20is%20thus%20end-to-end%20and%20can%20be%20trained%20in%20a%0Aself-supervised%20manner.%20This%20approach%20combines%20the%20strength%20of%20learning-based%0Aperception%20with%20the%20interpretable%20model-based%20control.%20Results%20show%20the%0Aeffectiveness%20even%20under%20strong%20wind.%20It%20can%20simultaneously%20enhance%20both%20the%0AMPC%20parameter%20learning%20and%20IMU%20prediction%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13088v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImperative%2520MPC%253A%2520An%2520End-to-End%2520Self-Supervised%2520Learning%2520with%250A%2520%2520Differentiable%2520MPC%2520for%2520UAV%2520Attitude%2520Control%26entry.906535625%3DHaonan%2520He%2520and%2520Yuheng%2520Qiu%2520and%2520Junyi%2520Geng%26entry.1292438233%3D%2520%2520Modeling%2520and%2520control%2520of%2520nonlinear%2520dynamics%2520are%2520critical%2520in%2520robotics%252C%250Aespecially%2520in%2520scenarios%2520with%2520unpredictable%2520external%2520influences%2520and%2520complex%250Adynamics.%2520Traditional%2520cascaded%2520modular%2520control%2520pipelines%2520often%2520yield%2520suboptimal%250Aperformance%2520due%2520to%2520conservative%2520assumptions%2520and%2520tedious%2520parameter%2520tuning.%2520Pure%250Adata-driven%2520approaches%2520promise%2520robust%2520performance%2520but%2520suffer%2520from%2520low%2520sample%250Aefficiency%252C%2520sim-to-real%2520gaps%252C%2520and%2520reliance%2520on%2520extensive%2520datasets.%2520Hybrid%250Amethods%2520combining%2520learning-based%2520and%2520traditional%2520model-based%2520control%2520in%2520an%250Aend-to-end%2520manner%2520offer%2520a%2520promising%2520alternative.%2520This%2520work%2520presents%2520a%250Aself-supervised%2520learning%2520framework%2520combining%2520learning-based%2520inertial%2520odometry%250A%2528IO%2529%2520module%2520and%2520differentiable%2520model%2520predictive%2520control%2520%2528d-MPC%2529%2520for%2520Unmanned%250AAerial%2520Vehicle%2520%2528UAV%2529%2520attitude%2520control.%2520The%2520IO%2520denoises%2520raw%2520IMU%2520measurements%2520and%250Apredicts%2520UAV%2520attitudes%252C%2520which%2520are%2520then%2520optimized%2520by%2520MPC%2520for%2520control%2520actions%2520in%250Aa%2520bi-level%2520optimization%2520%2528BLO%2529%2520setup%252C%2520where%2520the%2520inner%2520MPC%2520optimizes%2520control%250Aactions%2520and%2520the%2520upper%2520level%2520minimizes%2520discrepancy%2520between%2520real-world%2520and%250Apredicted%2520performance.%2520The%2520framework%2520is%2520thus%2520end-to-end%2520and%2520can%2520be%2520trained%2520in%2520a%250Aself-supervised%2520manner.%2520This%2520approach%2520combines%2520the%2520strength%2520of%2520learning-based%250Aperception%2520with%2520the%2520interpretable%2520model-based%2520control.%2520Results%2520show%2520the%250Aeffectiveness%2520even%2520under%2520strong%2520wind.%2520It%2520can%2520simultaneously%2520enhance%2520both%2520the%250AMPC%2520parameter%2520learning%2520and%2520IMU%2520prediction%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13088v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Imperative%20MPC%3A%20An%20End-to-End%20Self-Supervised%20Learning%20with%0A%20%20Differentiable%20MPC%20for%20UAV%20Attitude%20Control&entry.906535625=Haonan%20He%20and%20Yuheng%20Qiu%20and%20Junyi%20Geng&entry.1292438233=%20%20Modeling%20and%20control%20of%20nonlinear%20dynamics%20are%20critical%20in%20robotics%2C%0Aespecially%20in%20scenarios%20with%20unpredictable%20external%20influences%20and%20complex%0Adynamics.%20Traditional%20cascaded%20modular%20control%20pipelines%20often%20yield%20suboptimal%0Aperformance%20due%20to%20conservative%20assumptions%20and%20tedious%20parameter%20tuning.%20Pure%0Adata-driven%20approaches%20promise%20robust%20performance%20but%20suffer%20from%20low%20sample%0Aefficiency%2C%20sim-to-real%20gaps%2C%20and%20reliance%20on%20extensive%20datasets.%20Hybrid%0Amethods%20combining%20learning-based%20and%20traditional%20model-based%20control%20in%20an%0Aend-to-end%20manner%20offer%20a%20promising%20alternative.%20This%20work%20presents%20a%0Aself-supervised%20learning%20framework%20combining%20learning-based%20inertial%20odometry%0A%28IO%29%20module%20and%20differentiable%20model%20predictive%20control%20%28d-MPC%29%20for%20Unmanned%0AAerial%20Vehicle%20%28UAV%29%20attitude%20control.%20The%20IO%20denoises%20raw%20IMU%20measurements%20and%0Apredicts%20UAV%20attitudes%2C%20which%20are%20then%20optimized%20by%20MPC%20for%20control%20actions%20in%0Aa%20bi-level%20optimization%20%28BLO%29%20setup%2C%20where%20the%20inner%20MPC%20optimizes%20control%0Aactions%20and%20the%20upper%20level%20minimizes%20discrepancy%20between%20real-world%20and%0Apredicted%20performance.%20The%20framework%20is%20thus%20end-to-end%20and%20can%20be%20trained%20in%20a%0Aself-supervised%20manner.%20This%20approach%20combines%20the%20strength%20of%20learning-based%0Aperception%20with%20the%20interpretable%20model-based%20control.%20Results%20show%20the%0Aeffectiveness%20even%20under%20strong%20wind.%20It%20can%20simultaneously%20enhance%20both%20the%0AMPC%20parameter%20learning%20and%20IMU%20prediction%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13088v2&entry.124074799=Read"},
{"title": "Toward Efficient Exploration by Large Language Model Agents", "author": "Dilip Arumugam and Thomas L. Griffiths", "abstract": "  A burgeoning area within reinforcement learning (RL) is the design of\nsequential decision-making agents centered around large language models (LLMs).\nWhile autonomous decision-making agents powered by modern LLMs could facilitate\nnumerous real-world applications, such successes demand agents that are capable\nof data-efficient RL. One key obstacle to achieving data efficiency in RL is\nexploration, a challenge that we demonstrate many recent proposals for LLM\nagent designs struggle to contend with. Meanwhile, classic algorithms from the\nRL literature known to gracefully address exploration require technical\nmachinery that can be challenging to operationalize in purely natural language\nsettings. In this work, rather than relying on finetuning or in-context\nlearning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\nhow LLMs can be used to explicitly implement an existing RL algorithm\n(Posterior Sampling for Reinforcement Learning) whose capacity for\nstatistically-efficient exploration is already well-studied. We offer empirical\nresults demonstrating how our LLM-based implementation of a known,\ndata-efficient RL algorithm can be considerably more effective in natural\nlanguage tasks that demand prudent exploration.\n", "link": "http://arxiv.org/abs/2504.20997v1", "date": "2025-04-29", "relevancy": 2.1132, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5816}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5243}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Efficient%20Exploration%20by%20Large%20Language%20Model%20Agents&body=Title%3A%20Toward%20Efficient%20Exploration%20by%20Large%20Language%20Model%20Agents%0AAuthor%3A%20Dilip%20Arumugam%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20A%20burgeoning%20area%20within%20reinforcement%20learning%20%28RL%29%20is%20the%20design%20of%0Asequential%20decision-making%20agents%20centered%20around%20large%20language%20models%20%28LLMs%29.%0AWhile%20autonomous%20decision-making%20agents%20powered%20by%20modern%20LLMs%20could%20facilitate%0Anumerous%20real-world%20applications%2C%20such%20successes%20demand%20agents%20that%20are%20capable%0Aof%20data-efficient%20RL.%20One%20key%20obstacle%20to%20achieving%20data%20efficiency%20in%20RL%20is%0Aexploration%2C%20a%20challenge%20that%20we%20demonstrate%20many%20recent%20proposals%20for%20LLM%0Aagent%20designs%20struggle%20to%20contend%20with.%20Meanwhile%2C%20classic%20algorithms%20from%20the%0ARL%20literature%20known%20to%20gracefully%20address%20exploration%20require%20technical%0Amachinery%20that%20can%20be%20challenging%20to%20operationalize%20in%20purely%20natural%20language%0Asettings.%20In%20this%20work%2C%20rather%20than%20relying%20on%20finetuning%20or%20in-context%0Alearning%20to%20coax%20LLMs%20into%20implicitly%20imitating%20a%20RL%20algorithm%2C%20we%20illustrate%0Ahow%20LLMs%20can%20be%20used%20to%20explicitly%20implement%20an%20existing%20RL%20algorithm%0A%28Posterior%20Sampling%20for%20Reinforcement%20Learning%29%20whose%20capacity%20for%0Astatistically-efficient%20exploration%20is%20already%20well-studied.%20We%20offer%20empirical%0Aresults%20demonstrating%20how%20our%20LLM-based%20implementation%20of%20a%20known%2C%0Adata-efficient%20RL%20algorithm%20can%20be%20considerably%20more%20effective%20in%20natural%0Alanguage%20tasks%20that%20demand%20prudent%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Efficient%2520Exploration%2520by%2520Large%2520Language%2520Model%2520Agents%26entry.906535625%3DDilip%2520Arumugam%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520A%2520burgeoning%2520area%2520within%2520reinforcement%2520learning%2520%2528RL%2529%2520is%2520the%2520design%2520of%250Asequential%2520decision-making%2520agents%2520centered%2520around%2520large%2520language%2520models%2520%2528LLMs%2529.%250AWhile%2520autonomous%2520decision-making%2520agents%2520powered%2520by%2520modern%2520LLMs%2520could%2520facilitate%250Anumerous%2520real-world%2520applications%252C%2520such%2520successes%2520demand%2520agents%2520that%2520are%2520capable%250Aof%2520data-efficient%2520RL.%2520One%2520key%2520obstacle%2520to%2520achieving%2520data%2520efficiency%2520in%2520RL%2520is%250Aexploration%252C%2520a%2520challenge%2520that%2520we%2520demonstrate%2520many%2520recent%2520proposals%2520for%2520LLM%250Aagent%2520designs%2520struggle%2520to%2520contend%2520with.%2520Meanwhile%252C%2520classic%2520algorithms%2520from%2520the%250ARL%2520literature%2520known%2520to%2520gracefully%2520address%2520exploration%2520require%2520technical%250Amachinery%2520that%2520can%2520be%2520challenging%2520to%2520operationalize%2520in%2520purely%2520natural%2520language%250Asettings.%2520In%2520this%2520work%252C%2520rather%2520than%2520relying%2520on%2520finetuning%2520or%2520in-context%250Alearning%2520to%2520coax%2520LLMs%2520into%2520implicitly%2520imitating%2520a%2520RL%2520algorithm%252C%2520we%2520illustrate%250Ahow%2520LLMs%2520can%2520be%2520used%2520to%2520explicitly%2520implement%2520an%2520existing%2520RL%2520algorithm%250A%2528Posterior%2520Sampling%2520for%2520Reinforcement%2520Learning%2529%2520whose%2520capacity%2520for%250Astatistically-efficient%2520exploration%2520is%2520already%2520well-studied.%2520We%2520offer%2520empirical%250Aresults%2520demonstrating%2520how%2520our%2520LLM-based%2520implementation%2520of%2520a%2520known%252C%250Adata-efficient%2520RL%2520algorithm%2520can%2520be%2520considerably%2520more%2520effective%2520in%2520natural%250Alanguage%2520tasks%2520that%2520demand%2520prudent%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Efficient%20Exploration%20by%20Large%20Language%20Model%20Agents&entry.906535625=Dilip%20Arumugam%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20A%20burgeoning%20area%20within%20reinforcement%20learning%20%28RL%29%20is%20the%20design%20of%0Asequential%20decision-making%20agents%20centered%20around%20large%20language%20models%20%28LLMs%29.%0AWhile%20autonomous%20decision-making%20agents%20powered%20by%20modern%20LLMs%20could%20facilitate%0Anumerous%20real-world%20applications%2C%20such%20successes%20demand%20agents%20that%20are%20capable%0Aof%20data-efficient%20RL.%20One%20key%20obstacle%20to%20achieving%20data%20efficiency%20in%20RL%20is%0Aexploration%2C%20a%20challenge%20that%20we%20demonstrate%20many%20recent%20proposals%20for%20LLM%0Aagent%20designs%20struggle%20to%20contend%20with.%20Meanwhile%2C%20classic%20algorithms%20from%20the%0ARL%20literature%20known%20to%20gracefully%20address%20exploration%20require%20technical%0Amachinery%20that%20can%20be%20challenging%20to%20operationalize%20in%20purely%20natural%20language%0Asettings.%20In%20this%20work%2C%20rather%20than%20relying%20on%20finetuning%20or%20in-context%0Alearning%20to%20coax%20LLMs%20into%20implicitly%20imitating%20a%20RL%20algorithm%2C%20we%20illustrate%0Ahow%20LLMs%20can%20be%20used%20to%20explicitly%20implement%20an%20existing%20RL%20algorithm%0A%28Posterior%20Sampling%20for%20Reinforcement%20Learning%29%20whose%20capacity%20for%0Astatistically-efficient%20exploration%20is%20already%20well-studied.%20We%20offer%20empirical%0Aresults%20demonstrating%20how%20our%20LLM-based%20implementation%20of%20a%20known%2C%0Adata-efficient%20RL%20algorithm%20can%20be%20considerably%20more%20effective%20in%20natural%0Alanguage%20tasks%20that%20demand%20prudent%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20997v1&entry.124074799=Read"},
{"title": "Differentiable GPU-Parallelized Task and Motion Planning", "author": "William Shen and Caelan Garrett and Nishanth Kumar and Ankit Goyal and Tucker Hermans and Leslie Pack Kaelbling and Tom\u00e1s Lozano-P\u00e9rez and Fabio Ramos", "abstract": "  Planning long-horizon robot manipulation requires making discrete decisions\nabout which objects to interact with and continuous decisions about how to\ninteract with them. A robot planner must select grasps, placements, and motions\nthat are feasible and safe. This class of problems falls under Task and Motion\nPlanning (TAMP) and poses significant computational challenges in terms of\nalgorithm runtime and solution quality, particularly when the solution space is\nhighly constrained. To address these challenges, we propose a new bilevel TAMP\nalgorithm that leverages GPU parallelism to efficiently explore thousands of\ncandidate continuous solutions simultaneously. Our approach uses GPU\nparallelism to sample an initial batch of solution seeds for a plan skeleton\nand to apply differentiable optimization on this batch to satisfy plan\nconstraints and minimize solution cost with respect to soft objectives. We\ndemonstrate that our algorithm can effectively solve highly constrained\nproblems with non-convex constraints in just seconds, substantially\noutperforming serial TAMP approaches, and validate our approach on multiple\nreal-world robots. Project website and code: https://cutamp.github.io\n", "link": "http://arxiv.org/abs/2411.11833v2", "date": "2025-04-29", "relevancy": 2.1044, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5321}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5261}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20GPU-Parallelized%20Task%20and%20Motion%20Planning&body=Title%3A%20Differentiable%20GPU-Parallelized%20Task%20and%20Motion%20Planning%0AAuthor%3A%20William%20Shen%20and%20Caelan%20Garrett%20and%20Nishanth%20Kumar%20and%20Ankit%20Goyal%20and%20Tucker%20Hermans%20and%20Leslie%20Pack%20Kaelbling%20and%20Tom%C3%A1s%20Lozano-P%C3%A9rez%20and%20Fabio%20Ramos%0AAbstract%3A%20%20%20Planning%20long-horizon%20robot%20manipulation%20requires%20making%20discrete%20decisions%0Aabout%20which%20objects%20to%20interact%20with%20and%20continuous%20decisions%20about%20how%20to%0Ainteract%20with%20them.%20A%20robot%20planner%20must%20select%20grasps%2C%20placements%2C%20and%20motions%0Athat%20are%20feasible%20and%20safe.%20This%20class%20of%20problems%20falls%20under%20Task%20and%20Motion%0APlanning%20%28TAMP%29%20and%20poses%20significant%20computational%20challenges%20in%20terms%20of%0Aalgorithm%20runtime%20and%20solution%20quality%2C%20particularly%20when%20the%20solution%20space%20is%0Ahighly%20constrained.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20new%20bilevel%20TAMP%0Aalgorithm%20that%20leverages%20GPU%20parallelism%20to%20efficiently%20explore%20thousands%20of%0Acandidate%20continuous%20solutions%20simultaneously.%20Our%20approach%20uses%20GPU%0Aparallelism%20to%20sample%20an%20initial%20batch%20of%20solution%20seeds%20for%20a%20plan%20skeleton%0Aand%20to%20apply%20differentiable%20optimization%20on%20this%20batch%20to%20satisfy%20plan%0Aconstraints%20and%20minimize%20solution%20cost%20with%20respect%20to%20soft%20objectives.%20We%0Ademonstrate%20that%20our%20algorithm%20can%20effectively%20solve%20highly%20constrained%0Aproblems%20with%20non-convex%20constraints%20in%20just%20seconds%2C%20substantially%0Aoutperforming%20serial%20TAMP%20approaches%2C%20and%20validate%20our%20approach%20on%20multiple%0Areal-world%20robots.%20Project%20website%20and%20code%3A%20https%3A//cutamp.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.11833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520GPU-Parallelized%2520Task%2520and%2520Motion%2520Planning%26entry.906535625%3DWilliam%2520Shen%2520and%2520Caelan%2520Garrett%2520and%2520Nishanth%2520Kumar%2520and%2520Ankit%2520Goyal%2520and%2520Tucker%2520Hermans%2520and%2520Leslie%2520Pack%2520Kaelbling%2520and%2520Tom%25C3%25A1s%2520Lozano-P%25C3%25A9rez%2520and%2520Fabio%2520Ramos%26entry.1292438233%3D%2520%2520Planning%2520long-horizon%2520robot%2520manipulation%2520requires%2520making%2520discrete%2520decisions%250Aabout%2520which%2520objects%2520to%2520interact%2520with%2520and%2520continuous%2520decisions%2520about%2520how%2520to%250Ainteract%2520with%2520them.%2520A%2520robot%2520planner%2520must%2520select%2520grasps%252C%2520placements%252C%2520and%2520motions%250Athat%2520are%2520feasible%2520and%2520safe.%2520This%2520class%2520of%2520problems%2520falls%2520under%2520Task%2520and%2520Motion%250APlanning%2520%2528TAMP%2529%2520and%2520poses%2520significant%2520computational%2520challenges%2520in%2520terms%2520of%250Aalgorithm%2520runtime%2520and%2520solution%2520quality%252C%2520particularly%2520when%2520the%2520solution%2520space%2520is%250Ahighly%2520constrained.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520new%2520bilevel%2520TAMP%250Aalgorithm%2520that%2520leverages%2520GPU%2520parallelism%2520to%2520efficiently%2520explore%2520thousands%2520of%250Acandidate%2520continuous%2520solutions%2520simultaneously.%2520Our%2520approach%2520uses%2520GPU%250Aparallelism%2520to%2520sample%2520an%2520initial%2520batch%2520of%2520solution%2520seeds%2520for%2520a%2520plan%2520skeleton%250Aand%2520to%2520apply%2520differentiable%2520optimization%2520on%2520this%2520batch%2520to%2520satisfy%2520plan%250Aconstraints%2520and%2520minimize%2520solution%2520cost%2520with%2520respect%2520to%2520soft%2520objectives.%2520We%250Ademonstrate%2520that%2520our%2520algorithm%2520can%2520effectively%2520solve%2520highly%2520constrained%250Aproblems%2520with%2520non-convex%2520constraints%2520in%2520just%2520seconds%252C%2520substantially%250Aoutperforming%2520serial%2520TAMP%2520approaches%252C%2520and%2520validate%2520our%2520approach%2520on%2520multiple%250Areal-world%2520robots.%2520Project%2520website%2520and%2520code%253A%2520https%253A//cutamp.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.11833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20GPU-Parallelized%20Task%20and%20Motion%20Planning&entry.906535625=William%20Shen%20and%20Caelan%20Garrett%20and%20Nishanth%20Kumar%20and%20Ankit%20Goyal%20and%20Tucker%20Hermans%20and%20Leslie%20Pack%20Kaelbling%20and%20Tom%C3%A1s%20Lozano-P%C3%A9rez%20and%20Fabio%20Ramos&entry.1292438233=%20%20Planning%20long-horizon%20robot%20manipulation%20requires%20making%20discrete%20decisions%0Aabout%20which%20objects%20to%20interact%20with%20and%20continuous%20decisions%20about%20how%20to%0Ainteract%20with%20them.%20A%20robot%20planner%20must%20select%20grasps%2C%20placements%2C%20and%20motions%0Athat%20are%20feasible%20and%20safe.%20This%20class%20of%20problems%20falls%20under%20Task%20and%20Motion%0APlanning%20%28TAMP%29%20and%20poses%20significant%20computational%20challenges%20in%20terms%20of%0Aalgorithm%20runtime%20and%20solution%20quality%2C%20particularly%20when%20the%20solution%20space%20is%0Ahighly%20constrained.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20new%20bilevel%20TAMP%0Aalgorithm%20that%20leverages%20GPU%20parallelism%20to%20efficiently%20explore%20thousands%20of%0Acandidate%20continuous%20solutions%20simultaneously.%20Our%20approach%20uses%20GPU%0Aparallelism%20to%20sample%20an%20initial%20batch%20of%20solution%20seeds%20for%20a%20plan%20skeleton%0Aand%20to%20apply%20differentiable%20optimization%20on%20this%20batch%20to%20satisfy%20plan%0Aconstraints%20and%20minimize%20solution%20cost%20with%20respect%20to%20soft%20objectives.%20We%0Ademonstrate%20that%20our%20algorithm%20can%20effectively%20solve%20highly%20constrained%0Aproblems%20with%20non-convex%20constraints%20in%20just%20seconds%2C%20substantially%0Aoutperforming%20serial%20TAMP%20approaches%2C%20and%20validate%20our%20approach%20on%20multiple%0Areal-world%20robots.%20Project%20website%20and%20code%3A%20https%3A//cutamp.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.11833v2&entry.124074799=Read"},
{"title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning\n  through Step-by-Step Verification", "author": "Ziqing Fan and Cheng Liang and Chaoyi Wu and Ya Zhang and Yanfeng Wang and Weidi Xie", "abstract": "  Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.\n", "link": "http://arxiv.org/abs/2504.20930v1", "date": "2025-04-29", "relevancy": 2.1025, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChestX-Reasoner%3A%20Advancing%20Radiology%20Foundation%20Models%20with%20Reasoning%0A%20%20through%20Step-by-Step%20Verification&body=Title%3A%20ChestX-Reasoner%3A%20Advancing%20Radiology%20Foundation%20Models%20with%20Reasoning%0A%20%20through%20Step-by-Step%20Verification%0AAuthor%3A%20Ziqing%20Fan%20and%20Cheng%20Liang%20and%20Chaoyi%20Wu%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie%0AAbstract%3A%20%20%20Recent%20advances%20in%20reasoning-enhanced%20large%20language%20models%20%28LLMs%29%20and%0Amultimodal%20LLMs%20%28MLLMs%29%20have%20significantly%20improved%20performance%20in%20complex%0Atasks%2C%20yet%20medical%20AI%20models%20often%20overlook%20the%20structured%20reasoning%20processes%0Ainherent%20in%20clinical%20practice.%20In%20this%20work%2C%20we%20present%20ChestX-Reasoner%2C%20a%0Aradiology%20diagnosis%20MLLM%20designed%20to%20leverage%20process%20supervision%20mined%0Adirectly%20from%20clinical%20reports%2C%20reflecting%20the%20step-by-step%20reasoning%20followed%0Aby%20radiologists.%20We%20construct%20a%20large%20dataset%20by%20extracting%20and%20refining%0Areasoning%20chains%20from%20routine%20radiology%20reports.%20Our%20two-stage%20training%0Aframework%20combines%20supervised%20fine-tuning%20and%20reinforcement%20learning%20guided%20by%0Aprocess%20rewards%20to%20better%20align%20model%20reasoning%20with%20clinical%20standards.%20We%0Aintroduce%20RadRBench-CXR%2C%20a%20comprehensive%20benchmark%20featuring%2059K%20visual%0Aquestion%20answering%20samples%20with%20301K%20clinically%20validated%20reasoning%20steps%2C%20and%0Apropose%20RadRScore%2C%20a%20metric%20evaluating%20reasoning%20factuality%2C%20completeness%2C%20and%0Aeffectiveness.%20ChestX-Reasoner%20outperforms%20existing%20medical%20and%20general-domain%0AMLLMs%20in%20both%20diagnostic%20accuracy%20and%20reasoning%20ability%2C%20achieving%2016%25%2C%205.9%25%2C%0Aand%2018%25%20improvements%20in%20reasoning%20ability%20compared%20to%20the%20best%20medical%20MLLM%2C%0Athe%20best%20general%20MLLM%2C%20and%20its%20base%20model%2C%20respectively%2C%20as%20well%20as%203.3%25%2C%2024%25%2C%0Aand%2027%25%20improvements%20in%20outcome%20accuracy.%20All%20resources%20are%20open-sourced%20to%0Afacilitate%20further%20research%20in%20medical%20reasoning%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChestX-Reasoner%253A%2520Advancing%2520Radiology%2520Foundation%2520Models%2520with%2520Reasoning%250A%2520%2520through%2520Step-by-Step%2520Verification%26entry.906535625%3DZiqing%2520Fan%2520and%2520Cheng%2520Liang%2520and%2520Chaoyi%2520Wu%2520and%2520Ya%2520Zhang%2520and%2520Yanfeng%2520Wang%2520and%2520Weidi%2520Xie%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reasoning-enhanced%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Amultimodal%2520LLMs%2520%2528MLLMs%2529%2520have%2520significantly%2520improved%2520performance%2520in%2520complex%250Atasks%252C%2520yet%2520medical%2520AI%2520models%2520often%2520overlook%2520the%2520structured%2520reasoning%2520processes%250Ainherent%2520in%2520clinical%2520practice.%2520In%2520this%2520work%252C%2520we%2520present%2520ChestX-Reasoner%252C%2520a%250Aradiology%2520diagnosis%2520MLLM%2520designed%2520to%2520leverage%2520process%2520supervision%2520mined%250Adirectly%2520from%2520clinical%2520reports%252C%2520reflecting%2520the%2520step-by-step%2520reasoning%2520followed%250Aby%2520radiologists.%2520We%2520construct%2520a%2520large%2520dataset%2520by%2520extracting%2520and%2520refining%250Areasoning%2520chains%2520from%2520routine%2520radiology%2520reports.%2520Our%2520two-stage%2520training%250Aframework%2520combines%2520supervised%2520fine-tuning%2520and%2520reinforcement%2520learning%2520guided%2520by%250Aprocess%2520rewards%2520to%2520better%2520align%2520model%2520reasoning%2520with%2520clinical%2520standards.%2520We%250Aintroduce%2520RadRBench-CXR%252C%2520a%2520comprehensive%2520benchmark%2520featuring%252059K%2520visual%250Aquestion%2520answering%2520samples%2520with%2520301K%2520clinically%2520validated%2520reasoning%2520steps%252C%2520and%250Apropose%2520RadRScore%252C%2520a%2520metric%2520evaluating%2520reasoning%2520factuality%252C%2520completeness%252C%2520and%250Aeffectiveness.%2520ChestX-Reasoner%2520outperforms%2520existing%2520medical%2520and%2520general-domain%250AMLLMs%2520in%2520both%2520diagnostic%2520accuracy%2520and%2520reasoning%2520ability%252C%2520achieving%252016%2525%252C%25205.9%2525%252C%250Aand%252018%2525%2520improvements%2520in%2520reasoning%2520ability%2520compared%2520to%2520the%2520best%2520medical%2520MLLM%252C%250Athe%2520best%2520general%2520MLLM%252C%2520and%2520its%2520base%2520model%252C%2520respectively%252C%2520as%2520well%2520as%25203.3%2525%252C%252024%2525%252C%250Aand%252027%2525%2520improvements%2520in%2520outcome%2520accuracy.%2520All%2520resources%2520are%2520open-sourced%2520to%250Afacilitate%2520further%2520research%2520in%2520medical%2520reasoning%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChestX-Reasoner%3A%20Advancing%20Radiology%20Foundation%20Models%20with%20Reasoning%0A%20%20through%20Step-by-Step%20Verification&entry.906535625=Ziqing%20Fan%20and%20Cheng%20Liang%20and%20Chaoyi%20Wu%20and%20Ya%20Zhang%20and%20Yanfeng%20Wang%20and%20Weidi%20Xie&entry.1292438233=%20%20Recent%20advances%20in%20reasoning-enhanced%20large%20language%20models%20%28LLMs%29%20and%0Amultimodal%20LLMs%20%28MLLMs%29%20have%20significantly%20improved%20performance%20in%20complex%0Atasks%2C%20yet%20medical%20AI%20models%20often%20overlook%20the%20structured%20reasoning%20processes%0Ainherent%20in%20clinical%20practice.%20In%20this%20work%2C%20we%20present%20ChestX-Reasoner%2C%20a%0Aradiology%20diagnosis%20MLLM%20designed%20to%20leverage%20process%20supervision%20mined%0Adirectly%20from%20clinical%20reports%2C%20reflecting%20the%20step-by-step%20reasoning%20followed%0Aby%20radiologists.%20We%20construct%20a%20large%20dataset%20by%20extracting%20and%20refining%0Areasoning%20chains%20from%20routine%20radiology%20reports.%20Our%20two-stage%20training%0Aframework%20combines%20supervised%20fine-tuning%20and%20reinforcement%20learning%20guided%20by%0Aprocess%20rewards%20to%20better%20align%20model%20reasoning%20with%20clinical%20standards.%20We%0Aintroduce%20RadRBench-CXR%2C%20a%20comprehensive%20benchmark%20featuring%2059K%20visual%0Aquestion%20answering%20samples%20with%20301K%20clinically%20validated%20reasoning%20steps%2C%20and%0Apropose%20RadRScore%2C%20a%20metric%20evaluating%20reasoning%20factuality%2C%20completeness%2C%20and%0Aeffectiveness.%20ChestX-Reasoner%20outperforms%20existing%20medical%20and%20general-domain%0AMLLMs%20in%20both%20diagnostic%20accuracy%20and%20reasoning%20ability%2C%20achieving%2016%25%2C%205.9%25%2C%0Aand%2018%25%20improvements%20in%20reasoning%20ability%20compared%20to%20the%20best%20medical%20MLLM%2C%0Athe%20best%20general%20MLLM%2C%20and%20its%20base%20model%2C%20respectively%2C%20as%20well%20as%203.3%25%2C%2024%25%2C%0Aand%2027%25%20improvements%20in%20outcome%20accuracy.%20All%20resources%20are%20open-sourced%20to%0Afacilitate%20further%20research%20in%20medical%20reasoning%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20930v1&entry.124074799=Read"},
{"title": "FLIM-based Salient Object Detection Networks with Adaptive Decoders", "author": "Gilson Junior Soares and Matheus Abrantes Cerqueira and Jancarlo F. Gomes and Laurent Najman and Silvio Jamil F. Guimar\u00e3es and Alexandre Xavier Falc\u00e3o", "abstract": "  Salient Object Detection (SOD) methods can locate objects that stand out in\nan image, assign higher values to their pixels in a saliency map, and binarize\nthe map outputting a predicted segmentation mask. A recent tendency is to\ninvestigate pre-trained lightweight models rather than deep neural networks in\nSOD tasks, coping with applications under limited computational resources. In\nthis context, we have investigated lightweight networks using a methodology\nnamed Feature Learning from Image Markers (FLIM), which assumes that the\nencoder's kernels can be estimated from marker pixels on discriminative regions\nof a few representative images. This work proposes flyweight networks, hundreds\nof times lighter than lightweight models, for SOD by combining a FLIM encoder\nwith an adaptive decoder, whose weights are estimated for each input image by a\ngiven heuristic function. Such FLIM networks are trained from three to four\nrepresentative images only and without backpropagation, making the models\nsuitable for applications under labeled data constraints as well. We study five\nadaptive decoders; two of them are introduced here. Differently from the\nprevious ones that rely on one neuron per pixel with shared weights, the\nheuristic functions of the new adaptive decoders estimate the weights of each\nneuron per pixel. We compare FLIM models with adaptive decoders for two\nchallenging SOD tasks with three lightweight networks from the\nstate-of-the-art, two FLIM networks with decoders trained by backpropagation,\nand one FLIM network whose labeled markers define the decoder's weights. The\nexperiments demonstrate the advantages of the proposed networks over the\nbaselines, revealing the importance of further investigating such methods in\nnew applications.\n", "link": "http://arxiv.org/abs/2504.20872v1", "date": "2025-04-29", "relevancy": 2.0753, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5196}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLIM-based%20Salient%20Object%20Detection%20Networks%20with%20Adaptive%20Decoders&body=Title%3A%20FLIM-based%20Salient%20Object%20Detection%20Networks%20with%20Adaptive%20Decoders%0AAuthor%3A%20Gilson%20Junior%20Soares%20and%20Matheus%20Abrantes%20Cerqueira%20and%20Jancarlo%20F.%20Gomes%20and%20Laurent%20Najman%20and%20Silvio%20Jamil%20F.%20Guimar%C3%A3es%20and%20Alexandre%20Xavier%20Falc%C3%A3o%0AAbstract%3A%20%20%20Salient%20Object%20Detection%20%28SOD%29%20methods%20can%20locate%20objects%20that%20stand%20out%20in%0Aan%20image%2C%20assign%20higher%20values%20to%20their%20pixels%20in%20a%20saliency%20map%2C%20and%20binarize%0Athe%20map%20outputting%20a%20predicted%20segmentation%20mask.%20A%20recent%20tendency%20is%20to%0Ainvestigate%20pre-trained%20lightweight%20models%20rather%20than%20deep%20neural%20networks%20in%0ASOD%20tasks%2C%20coping%20with%20applications%20under%20limited%20computational%20resources.%20In%0Athis%20context%2C%20we%20have%20investigated%20lightweight%20networks%20using%20a%20methodology%0Anamed%20Feature%20Learning%20from%20Image%20Markers%20%28FLIM%29%2C%20which%20assumes%20that%20the%0Aencoder%27s%20kernels%20can%20be%20estimated%20from%20marker%20pixels%20on%20discriminative%20regions%0Aof%20a%20few%20representative%20images.%20This%20work%20proposes%20flyweight%20networks%2C%20hundreds%0Aof%20times%20lighter%20than%20lightweight%20models%2C%20for%20SOD%20by%20combining%20a%20FLIM%20encoder%0Awith%20an%20adaptive%20decoder%2C%20whose%20weights%20are%20estimated%20for%20each%20input%20image%20by%20a%0Agiven%20heuristic%20function.%20Such%20FLIM%20networks%20are%20trained%20from%20three%20to%20four%0Arepresentative%20images%20only%20and%20without%20backpropagation%2C%20making%20the%20models%0Asuitable%20for%20applications%20under%20labeled%20data%20constraints%20as%20well.%20We%20study%20five%0Aadaptive%20decoders%3B%20two%20of%20them%20are%20introduced%20here.%20Differently%20from%20the%0Aprevious%20ones%20that%20rely%20on%20one%20neuron%20per%20pixel%20with%20shared%20weights%2C%20the%0Aheuristic%20functions%20of%20the%20new%20adaptive%20decoders%20estimate%20the%20weights%20of%20each%0Aneuron%20per%20pixel.%20We%20compare%20FLIM%20models%20with%20adaptive%20decoders%20for%20two%0Achallenging%20SOD%20tasks%20with%20three%20lightweight%20networks%20from%20the%0Astate-of-the-art%2C%20two%20FLIM%20networks%20with%20decoders%20trained%20by%20backpropagation%2C%0Aand%20one%20FLIM%20network%20whose%20labeled%20markers%20define%20the%20decoder%27s%20weights.%20The%0Aexperiments%20demonstrate%20the%20advantages%20of%20the%20proposed%20networks%20over%20the%0Abaselines%2C%20revealing%20the%20importance%20of%20further%20investigating%20such%20methods%20in%0Anew%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLIM-based%2520Salient%2520Object%2520Detection%2520Networks%2520with%2520Adaptive%2520Decoders%26entry.906535625%3DGilson%2520Junior%2520Soares%2520and%2520Matheus%2520Abrantes%2520Cerqueira%2520and%2520Jancarlo%2520F.%2520Gomes%2520and%2520Laurent%2520Najman%2520and%2520Silvio%2520Jamil%2520F.%2520Guimar%25C3%25A3es%2520and%2520Alexandre%2520Xavier%2520Falc%25C3%25A3o%26entry.1292438233%3D%2520%2520Salient%2520Object%2520Detection%2520%2528SOD%2529%2520methods%2520can%2520locate%2520objects%2520that%2520stand%2520out%2520in%250Aan%2520image%252C%2520assign%2520higher%2520values%2520to%2520their%2520pixels%2520in%2520a%2520saliency%2520map%252C%2520and%2520binarize%250Athe%2520map%2520outputting%2520a%2520predicted%2520segmentation%2520mask.%2520A%2520recent%2520tendency%2520is%2520to%250Ainvestigate%2520pre-trained%2520lightweight%2520models%2520rather%2520than%2520deep%2520neural%2520networks%2520in%250ASOD%2520tasks%252C%2520coping%2520with%2520applications%2520under%2520limited%2520computational%2520resources.%2520In%250Athis%2520context%252C%2520we%2520have%2520investigated%2520lightweight%2520networks%2520using%2520a%2520methodology%250Anamed%2520Feature%2520Learning%2520from%2520Image%2520Markers%2520%2528FLIM%2529%252C%2520which%2520assumes%2520that%2520the%250Aencoder%2527s%2520kernels%2520can%2520be%2520estimated%2520from%2520marker%2520pixels%2520on%2520discriminative%2520regions%250Aof%2520a%2520few%2520representative%2520images.%2520This%2520work%2520proposes%2520flyweight%2520networks%252C%2520hundreds%250Aof%2520times%2520lighter%2520than%2520lightweight%2520models%252C%2520for%2520SOD%2520by%2520combining%2520a%2520FLIM%2520encoder%250Awith%2520an%2520adaptive%2520decoder%252C%2520whose%2520weights%2520are%2520estimated%2520for%2520each%2520input%2520image%2520by%2520a%250Agiven%2520heuristic%2520function.%2520Such%2520FLIM%2520networks%2520are%2520trained%2520from%2520three%2520to%2520four%250Arepresentative%2520images%2520only%2520and%2520without%2520backpropagation%252C%2520making%2520the%2520models%250Asuitable%2520for%2520applications%2520under%2520labeled%2520data%2520constraints%2520as%2520well.%2520We%2520study%2520five%250Aadaptive%2520decoders%253B%2520two%2520of%2520them%2520are%2520introduced%2520here.%2520Differently%2520from%2520the%250Aprevious%2520ones%2520that%2520rely%2520on%2520one%2520neuron%2520per%2520pixel%2520with%2520shared%2520weights%252C%2520the%250Aheuristic%2520functions%2520of%2520the%2520new%2520adaptive%2520decoders%2520estimate%2520the%2520weights%2520of%2520each%250Aneuron%2520per%2520pixel.%2520We%2520compare%2520FLIM%2520models%2520with%2520adaptive%2520decoders%2520for%2520two%250Achallenging%2520SOD%2520tasks%2520with%2520three%2520lightweight%2520networks%2520from%2520the%250Astate-of-the-art%252C%2520two%2520FLIM%2520networks%2520with%2520decoders%2520trained%2520by%2520backpropagation%252C%250Aand%2520one%2520FLIM%2520network%2520whose%2520labeled%2520markers%2520define%2520the%2520decoder%2527s%2520weights.%2520The%250Aexperiments%2520demonstrate%2520the%2520advantages%2520of%2520the%2520proposed%2520networks%2520over%2520the%250Abaselines%252C%2520revealing%2520the%2520importance%2520of%2520further%2520investigating%2520such%2520methods%2520in%250Anew%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLIM-based%20Salient%20Object%20Detection%20Networks%20with%20Adaptive%20Decoders&entry.906535625=Gilson%20Junior%20Soares%20and%20Matheus%20Abrantes%20Cerqueira%20and%20Jancarlo%20F.%20Gomes%20and%20Laurent%20Najman%20and%20Silvio%20Jamil%20F.%20Guimar%C3%A3es%20and%20Alexandre%20Xavier%20Falc%C3%A3o&entry.1292438233=%20%20Salient%20Object%20Detection%20%28SOD%29%20methods%20can%20locate%20objects%20that%20stand%20out%20in%0Aan%20image%2C%20assign%20higher%20values%20to%20their%20pixels%20in%20a%20saliency%20map%2C%20and%20binarize%0Athe%20map%20outputting%20a%20predicted%20segmentation%20mask.%20A%20recent%20tendency%20is%20to%0Ainvestigate%20pre-trained%20lightweight%20models%20rather%20than%20deep%20neural%20networks%20in%0ASOD%20tasks%2C%20coping%20with%20applications%20under%20limited%20computational%20resources.%20In%0Athis%20context%2C%20we%20have%20investigated%20lightweight%20networks%20using%20a%20methodology%0Anamed%20Feature%20Learning%20from%20Image%20Markers%20%28FLIM%29%2C%20which%20assumes%20that%20the%0Aencoder%27s%20kernels%20can%20be%20estimated%20from%20marker%20pixels%20on%20discriminative%20regions%0Aof%20a%20few%20representative%20images.%20This%20work%20proposes%20flyweight%20networks%2C%20hundreds%0Aof%20times%20lighter%20than%20lightweight%20models%2C%20for%20SOD%20by%20combining%20a%20FLIM%20encoder%0Awith%20an%20adaptive%20decoder%2C%20whose%20weights%20are%20estimated%20for%20each%20input%20image%20by%20a%0Agiven%20heuristic%20function.%20Such%20FLIM%20networks%20are%20trained%20from%20three%20to%20four%0Arepresentative%20images%20only%20and%20without%20backpropagation%2C%20making%20the%20models%0Asuitable%20for%20applications%20under%20labeled%20data%20constraints%20as%20well.%20We%20study%20five%0Aadaptive%20decoders%3B%20two%20of%20them%20are%20introduced%20here.%20Differently%20from%20the%0Aprevious%20ones%20that%20rely%20on%20one%20neuron%20per%20pixel%20with%20shared%20weights%2C%20the%0Aheuristic%20functions%20of%20the%20new%20adaptive%20decoders%20estimate%20the%20weights%20of%20each%0Aneuron%20per%20pixel.%20We%20compare%20FLIM%20models%20with%20adaptive%20decoders%20for%20two%0Achallenging%20SOD%20tasks%20with%20three%20lightweight%20networks%20from%20the%0Astate-of-the-art%2C%20two%20FLIM%20networks%20with%20decoders%20trained%20by%20backpropagation%2C%0Aand%20one%20FLIM%20network%20whose%20labeled%20markers%20define%20the%20decoder%27s%20weights.%20The%0Aexperiments%20demonstrate%20the%20advantages%20of%20the%20proposed%20networks%20over%20the%0Abaselines%2C%20revealing%20the%20importance%20of%20further%20investigating%20such%20methods%20in%0Anew%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20872v1&entry.124074799=Read"},
{"title": "Learnable Residual-based Latent Denoising in Semantic Communication", "author": "Mingkai Xu and Yongpeng Wu and Yuxuan Shi and Xiang-Gen Xia and Wenjun Zhang and Ping Zhang", "abstract": "  A latent denoising semantic communication (SemCom) framework is proposed for\nrobust image transmission over noisy channels. By incorporating a learnable\nlatent denoiser into the receiver, the received signals are preprocessed to\neffectively remove the channel noise and recover the semantic information,\nthereby enhancing the quality of the decoded images. Specifically, a latent\ndenoising mapping is established by an iterative residual learning approach to\nimprove the denoising efficiency while ensuring stable performance. Moreover,\nchannel signal-to-noise ratio (SNR) is utilized to estimate and predict the\nlatent similarity score (SS) for conditional denoising, where the number of\ndenoising steps is adapted based on the predicted SS sequence, further reducing\nthe communication latency. Finally, simulations demonstrate that the proposed\nframework can effectively and efficiently remove the channel noise at various\nlevels and reconstruct visual-appealing images.\n", "link": "http://arxiv.org/abs/2502.07319v2", "date": "2025-04-29", "relevancy": 2.0665, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5403}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5172}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20Residual-based%20Latent%20Denoising%20in%20Semantic%20Communication&body=Title%3A%20Learnable%20Residual-based%20Latent%20Denoising%20in%20Semantic%20Communication%0AAuthor%3A%20Mingkai%20Xu%20and%20Yongpeng%20Wu%20and%20Yuxuan%20Shi%20and%20Xiang-Gen%20Xia%20and%20Wenjun%20Zhang%20and%20Ping%20Zhang%0AAbstract%3A%20%20%20A%20latent%20denoising%20semantic%20communication%20%28SemCom%29%20framework%20is%20proposed%20for%0Arobust%20image%20transmission%20over%20noisy%20channels.%20By%20incorporating%20a%20learnable%0Alatent%20denoiser%20into%20the%20receiver%2C%20the%20received%20signals%20are%20preprocessed%20to%0Aeffectively%20remove%20the%20channel%20noise%20and%20recover%20the%20semantic%20information%2C%0Athereby%20enhancing%20the%20quality%20of%20the%20decoded%20images.%20Specifically%2C%20a%20latent%0Adenoising%20mapping%20is%20established%20by%20an%20iterative%20residual%20learning%20approach%20to%0Aimprove%20the%20denoising%20efficiency%20while%20ensuring%20stable%20performance.%20Moreover%2C%0Achannel%20signal-to-noise%20ratio%20%28SNR%29%20is%20utilized%20to%20estimate%20and%20predict%20the%0Alatent%20similarity%20score%20%28SS%29%20for%20conditional%20denoising%2C%20where%20the%20number%20of%0Adenoising%20steps%20is%20adapted%20based%20on%20the%20predicted%20SS%20sequence%2C%20further%20reducing%0Athe%20communication%20latency.%20Finally%2C%20simulations%20demonstrate%20that%20the%20proposed%0Aframework%20can%20effectively%20and%20efficiently%20remove%20the%20channel%20noise%20at%20various%0Alevels%20and%20reconstruct%20visual-appealing%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520Residual-based%2520Latent%2520Denoising%2520in%2520Semantic%2520Communication%26entry.906535625%3DMingkai%2520Xu%2520and%2520Yongpeng%2520Wu%2520and%2520Yuxuan%2520Shi%2520and%2520Xiang-Gen%2520Xia%2520and%2520Wenjun%2520Zhang%2520and%2520Ping%2520Zhang%26entry.1292438233%3D%2520%2520A%2520latent%2520denoising%2520semantic%2520communication%2520%2528SemCom%2529%2520framework%2520is%2520proposed%2520for%250Arobust%2520image%2520transmission%2520over%2520noisy%2520channels.%2520By%2520incorporating%2520a%2520learnable%250Alatent%2520denoiser%2520into%2520the%2520receiver%252C%2520the%2520received%2520signals%2520are%2520preprocessed%2520to%250Aeffectively%2520remove%2520the%2520channel%2520noise%2520and%2520recover%2520the%2520semantic%2520information%252C%250Athereby%2520enhancing%2520the%2520quality%2520of%2520the%2520decoded%2520images.%2520Specifically%252C%2520a%2520latent%250Adenoising%2520mapping%2520is%2520established%2520by%2520an%2520iterative%2520residual%2520learning%2520approach%2520to%250Aimprove%2520the%2520denoising%2520efficiency%2520while%2520ensuring%2520stable%2520performance.%2520Moreover%252C%250Achannel%2520signal-to-noise%2520ratio%2520%2528SNR%2529%2520is%2520utilized%2520to%2520estimate%2520and%2520predict%2520the%250Alatent%2520similarity%2520score%2520%2528SS%2529%2520for%2520conditional%2520denoising%252C%2520where%2520the%2520number%2520of%250Adenoising%2520steps%2520is%2520adapted%2520based%2520on%2520the%2520predicted%2520SS%2520sequence%252C%2520further%2520reducing%250Athe%2520communication%2520latency.%2520Finally%252C%2520simulations%2520demonstrate%2520that%2520the%2520proposed%250Aframework%2520can%2520effectively%2520and%2520efficiently%2520remove%2520the%2520channel%2520noise%2520at%2520various%250Alevels%2520and%2520reconstruct%2520visual-appealing%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Residual-based%20Latent%20Denoising%20in%20Semantic%20Communication&entry.906535625=Mingkai%20Xu%20and%20Yongpeng%20Wu%20and%20Yuxuan%20Shi%20and%20Xiang-Gen%20Xia%20and%20Wenjun%20Zhang%20and%20Ping%20Zhang&entry.1292438233=%20%20A%20latent%20denoising%20semantic%20communication%20%28SemCom%29%20framework%20is%20proposed%20for%0Arobust%20image%20transmission%20over%20noisy%20channels.%20By%20incorporating%20a%20learnable%0Alatent%20denoiser%20into%20the%20receiver%2C%20the%20received%20signals%20are%20preprocessed%20to%0Aeffectively%20remove%20the%20channel%20noise%20and%20recover%20the%20semantic%20information%2C%0Athereby%20enhancing%20the%20quality%20of%20the%20decoded%20images.%20Specifically%2C%20a%20latent%0Adenoising%20mapping%20is%20established%20by%20an%20iterative%20residual%20learning%20approach%20to%0Aimprove%20the%20denoising%20efficiency%20while%20ensuring%20stable%20performance.%20Moreover%2C%0Achannel%20signal-to-noise%20ratio%20%28SNR%29%20is%20utilized%20to%20estimate%20and%20predict%20the%0Alatent%20similarity%20score%20%28SS%29%20for%20conditional%20denoising%2C%20where%20the%20number%20of%0Adenoising%20steps%20is%20adapted%20based%20on%20the%20predicted%20SS%20sequence%2C%20further%20reducing%0Athe%20communication%20latency.%20Finally%2C%20simulations%20demonstrate%20that%20the%20proposed%0Aframework%20can%20effectively%20and%20efficiently%20remove%20the%20channel%20noise%20at%20various%0Alevels%20and%20reconstruct%20visual-appealing%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07319v2&entry.124074799=Read"},
{"title": "Improvements of Dark Experience Replay and Reservoir Sampling towards\n  Better Balance between Consolidation and Plasticity", "author": "Taisuke Kobayashi", "abstract": "  Continual learning is the one of the most essential abilities for autonomous\nagents, which can incrementally learn daily-life skills. For this ultimate\ngoal, a simple but powerful method, dark experience replay (DER), has been\nproposed recently. DER mitigates catastrophic forgetting, in which the skills\nacquired in the past are unintentionally forgotten, by stochastically storing\nthe streaming data in a reservoir sampling (RS) buffer and by relearning them\nor retaining the past outputs for them. However, since DER considers multiple\nobjectives, it will not function properly without appropriate weighting of\nthem. In addition, the ability to retain past outputs inhibits learning if the\npast outputs are incorrect due to distribution shift or other effects. This is\ndue to a tradeoff between memory consolidation and plasticity. The tradeoff is\nhidden even in the RS buffer, which gradually stops storing new data for new\nskills in it as data is continuously passed to it. To alleviate the tradeoff\nand achieve better balance, this paper proposes improvement strategies to each\nof DER and RS. Specifically, DER is improved with automatic adaptation of\nweights, block of replaying erroneous data, and correction of past outputs. RS\nis also improved with generalization of acceptance probability, stratification\nof plural buffers, and intentional omission of unnecessary data. These\nimprovements are verified through multiple benchmarks including regression,\nclassification, and reinforcement learning problems. As a result, the proposed\nmethods achieve steady improvements in learning performance by balancing the\nmemory consolidation and plasticity.\n", "link": "http://arxiv.org/abs/2504.20932v1", "date": "2025-04-29", "relevancy": 2.0658, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5216}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5127}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improvements%20of%20Dark%20Experience%20Replay%20and%20Reservoir%20Sampling%20towards%0A%20%20Better%20Balance%20between%20Consolidation%20and%20Plasticity&body=Title%3A%20Improvements%20of%20Dark%20Experience%20Replay%20and%20Reservoir%20Sampling%20towards%0A%20%20Better%20Balance%20between%20Consolidation%20and%20Plasticity%0AAuthor%3A%20Taisuke%20Kobayashi%0AAbstract%3A%20%20%20Continual%20learning%20is%20the%20one%20of%20the%20most%20essential%20abilities%20for%20autonomous%0Aagents%2C%20which%20can%20incrementally%20learn%20daily-life%20skills.%20For%20this%20ultimate%0Agoal%2C%20a%20simple%20but%20powerful%20method%2C%20dark%20experience%20replay%20%28DER%29%2C%20has%20been%0Aproposed%20recently.%20DER%20mitigates%20catastrophic%20forgetting%2C%20in%20which%20the%20skills%0Aacquired%20in%20the%20past%20are%20unintentionally%20forgotten%2C%20by%20stochastically%20storing%0Athe%20streaming%20data%20in%20a%20reservoir%20sampling%20%28RS%29%20buffer%20and%20by%20relearning%20them%0Aor%20retaining%20the%20past%20outputs%20for%20them.%20However%2C%20since%20DER%20considers%20multiple%0Aobjectives%2C%20it%20will%20not%20function%20properly%20without%20appropriate%20weighting%20of%0Athem.%20In%20addition%2C%20the%20ability%20to%20retain%20past%20outputs%20inhibits%20learning%20if%20the%0Apast%20outputs%20are%20incorrect%20due%20to%20distribution%20shift%20or%20other%20effects.%20This%20is%0Adue%20to%20a%20tradeoff%20between%20memory%20consolidation%20and%20plasticity.%20The%20tradeoff%20is%0Ahidden%20even%20in%20the%20RS%20buffer%2C%20which%20gradually%20stops%20storing%20new%20data%20for%20new%0Askills%20in%20it%20as%20data%20is%20continuously%20passed%20to%20it.%20To%20alleviate%20the%20tradeoff%0Aand%20achieve%20better%20balance%2C%20this%20paper%20proposes%20improvement%20strategies%20to%20each%0Aof%20DER%20and%20RS.%20Specifically%2C%20DER%20is%20improved%20with%20automatic%20adaptation%20of%0Aweights%2C%20block%20of%20replaying%20erroneous%20data%2C%20and%20correction%20of%20past%20outputs.%20RS%0Ais%20also%20improved%20with%20generalization%20of%20acceptance%20probability%2C%20stratification%0Aof%20plural%20buffers%2C%20and%20intentional%20omission%20of%20unnecessary%20data.%20These%0Aimprovements%20are%20verified%20through%20multiple%20benchmarks%20including%20regression%2C%0Aclassification%2C%20and%20reinforcement%20learning%20problems.%20As%20a%20result%2C%20the%20proposed%0Amethods%20achieve%20steady%20improvements%20in%20learning%20performance%20by%20balancing%20the%0Amemory%20consolidation%20and%20plasticity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprovements%2520of%2520Dark%2520Experience%2520Replay%2520and%2520Reservoir%2520Sampling%2520towards%250A%2520%2520Better%2520Balance%2520between%2520Consolidation%2520and%2520Plasticity%26entry.906535625%3DTaisuke%2520Kobayashi%26entry.1292438233%3D%2520%2520Continual%2520learning%2520is%2520the%2520one%2520of%2520the%2520most%2520essential%2520abilities%2520for%2520autonomous%250Aagents%252C%2520which%2520can%2520incrementally%2520learn%2520daily-life%2520skills.%2520For%2520this%2520ultimate%250Agoal%252C%2520a%2520simple%2520but%2520powerful%2520method%252C%2520dark%2520experience%2520replay%2520%2528DER%2529%252C%2520has%2520been%250Aproposed%2520recently.%2520DER%2520mitigates%2520catastrophic%2520forgetting%252C%2520in%2520which%2520the%2520skills%250Aacquired%2520in%2520the%2520past%2520are%2520unintentionally%2520forgotten%252C%2520by%2520stochastically%2520storing%250Athe%2520streaming%2520data%2520in%2520a%2520reservoir%2520sampling%2520%2528RS%2529%2520buffer%2520and%2520by%2520relearning%2520them%250Aor%2520retaining%2520the%2520past%2520outputs%2520for%2520them.%2520However%252C%2520since%2520DER%2520considers%2520multiple%250Aobjectives%252C%2520it%2520will%2520not%2520function%2520properly%2520without%2520appropriate%2520weighting%2520of%250Athem.%2520In%2520addition%252C%2520the%2520ability%2520to%2520retain%2520past%2520outputs%2520inhibits%2520learning%2520if%2520the%250Apast%2520outputs%2520are%2520incorrect%2520due%2520to%2520distribution%2520shift%2520or%2520other%2520effects.%2520This%2520is%250Adue%2520to%2520a%2520tradeoff%2520between%2520memory%2520consolidation%2520and%2520plasticity.%2520The%2520tradeoff%2520is%250Ahidden%2520even%2520in%2520the%2520RS%2520buffer%252C%2520which%2520gradually%2520stops%2520storing%2520new%2520data%2520for%2520new%250Askills%2520in%2520it%2520as%2520data%2520is%2520continuously%2520passed%2520to%2520it.%2520To%2520alleviate%2520the%2520tradeoff%250Aand%2520achieve%2520better%2520balance%252C%2520this%2520paper%2520proposes%2520improvement%2520strategies%2520to%2520each%250Aof%2520DER%2520and%2520RS.%2520Specifically%252C%2520DER%2520is%2520improved%2520with%2520automatic%2520adaptation%2520of%250Aweights%252C%2520block%2520of%2520replaying%2520erroneous%2520data%252C%2520and%2520correction%2520of%2520past%2520outputs.%2520RS%250Ais%2520also%2520improved%2520with%2520generalization%2520of%2520acceptance%2520probability%252C%2520stratification%250Aof%2520plural%2520buffers%252C%2520and%2520intentional%2520omission%2520of%2520unnecessary%2520data.%2520These%250Aimprovements%2520are%2520verified%2520through%2520multiple%2520benchmarks%2520including%2520regression%252C%250Aclassification%252C%2520and%2520reinforcement%2520learning%2520problems.%2520As%2520a%2520result%252C%2520the%2520proposed%250Amethods%2520achieve%2520steady%2520improvements%2520in%2520learning%2520performance%2520by%2520balancing%2520the%250Amemory%2520consolidation%2520and%2520plasticity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improvements%20of%20Dark%20Experience%20Replay%20and%20Reservoir%20Sampling%20towards%0A%20%20Better%20Balance%20between%20Consolidation%20and%20Plasticity&entry.906535625=Taisuke%20Kobayashi&entry.1292438233=%20%20Continual%20learning%20is%20the%20one%20of%20the%20most%20essential%20abilities%20for%20autonomous%0Aagents%2C%20which%20can%20incrementally%20learn%20daily-life%20skills.%20For%20this%20ultimate%0Agoal%2C%20a%20simple%20but%20powerful%20method%2C%20dark%20experience%20replay%20%28DER%29%2C%20has%20been%0Aproposed%20recently.%20DER%20mitigates%20catastrophic%20forgetting%2C%20in%20which%20the%20skills%0Aacquired%20in%20the%20past%20are%20unintentionally%20forgotten%2C%20by%20stochastically%20storing%0Athe%20streaming%20data%20in%20a%20reservoir%20sampling%20%28RS%29%20buffer%20and%20by%20relearning%20them%0Aor%20retaining%20the%20past%20outputs%20for%20them.%20However%2C%20since%20DER%20considers%20multiple%0Aobjectives%2C%20it%20will%20not%20function%20properly%20without%20appropriate%20weighting%20of%0Athem.%20In%20addition%2C%20the%20ability%20to%20retain%20past%20outputs%20inhibits%20learning%20if%20the%0Apast%20outputs%20are%20incorrect%20due%20to%20distribution%20shift%20or%20other%20effects.%20This%20is%0Adue%20to%20a%20tradeoff%20between%20memory%20consolidation%20and%20plasticity.%20The%20tradeoff%20is%0Ahidden%20even%20in%20the%20RS%20buffer%2C%20which%20gradually%20stops%20storing%20new%20data%20for%20new%0Askills%20in%20it%20as%20data%20is%20continuously%20passed%20to%20it.%20To%20alleviate%20the%20tradeoff%0Aand%20achieve%20better%20balance%2C%20this%20paper%20proposes%20improvement%20strategies%20to%20each%0Aof%20DER%20and%20RS.%20Specifically%2C%20DER%20is%20improved%20with%20automatic%20adaptation%20of%0Aweights%2C%20block%20of%20replaying%20erroneous%20data%2C%20and%20correction%20of%20past%20outputs.%20RS%0Ais%20also%20improved%20with%20generalization%20of%20acceptance%20probability%2C%20stratification%0Aof%20plural%20buffers%2C%20and%20intentional%20omission%20of%20unnecessary%20data.%20These%0Aimprovements%20are%20verified%20through%20multiple%20benchmarks%20including%20regression%2C%0Aclassification%2C%20and%20reinforcement%20learning%20problems.%20As%20a%20result%2C%20the%20proposed%0Amethods%20achieve%20steady%20improvements%20in%20learning%20performance%20by%20balancing%20the%0Amemory%20consolidation%20and%20plasticity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20932v1&entry.124074799=Read"},
{"title": "A simulation-heuristics dual-process model for intuitive physics", "author": "Shiqian Li and Yuxi Ma and Jiajun Yan and Bo Dai and Yujia Peng and Chi Zhang and Yixin Zhu", "abstract": "  The role of mental simulation in human physical reasoning is widely\nacknowledged, but whether it is employed across scenarios with varying\nsimulation costs and where its boundary lies remains unclear. Using a\npouring-marble task, our human study revealed two distinct error patterns when\npredicting pouring angles, differentiated by simulation time. While mental\nsimulation accurately captured human judgments in simpler scenarios, a linear\nheuristic model better matched human predictions when simulation time exceeded\na certain boundary. Motivated by these observations, we propose a dual-process\nframework, Simulation-Heuristics Model (SHM), where intuitive physics employs\nsimulation for short-time simulation but switches to heuristics when simulation\nbecomes costly. By integrating computational methods previously viewed as\nseparate into a unified model, SHM quantitatively captures their switching\nmechanism. The SHM aligns more precisely with human behavior and demonstrates\nconsistent predictive performance across diverse scenarios, advancing our\nunderstanding of the adaptive nature of intuitive physical reasoning.\n", "link": "http://arxiv.org/abs/2504.09546v2", "date": "2025-04-29", "relevancy": 2.0599, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5794}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.511}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20simulation-heuristics%20dual-process%20model%20for%20intuitive%20physics&body=Title%3A%20A%20simulation-heuristics%20dual-process%20model%20for%20intuitive%20physics%0AAuthor%3A%20Shiqian%20Li%20and%20Yuxi%20Ma%20and%20Jiajun%20Yan%20and%20Bo%20Dai%20and%20Yujia%20Peng%20and%20Chi%20Zhang%20and%20Yixin%20Zhu%0AAbstract%3A%20%20%20The%20role%20of%20mental%20simulation%20in%20human%20physical%20reasoning%20is%20widely%0Aacknowledged%2C%20but%20whether%20it%20is%20employed%20across%20scenarios%20with%20varying%0Asimulation%20costs%20and%20where%20its%20boundary%20lies%20remains%20unclear.%20Using%20a%0Apouring-marble%20task%2C%20our%20human%20study%20revealed%20two%20distinct%20error%20patterns%20when%0Apredicting%20pouring%20angles%2C%20differentiated%20by%20simulation%20time.%20While%20mental%0Asimulation%20accurately%20captured%20human%20judgments%20in%20simpler%20scenarios%2C%20a%20linear%0Aheuristic%20model%20better%20matched%20human%20predictions%20when%20simulation%20time%20exceeded%0Aa%20certain%20boundary.%20Motivated%20by%20these%20observations%2C%20we%20propose%20a%20dual-process%0Aframework%2C%20Simulation-Heuristics%20Model%20%28SHM%29%2C%20where%20intuitive%20physics%20employs%0Asimulation%20for%20short-time%20simulation%20but%20switches%20to%20heuristics%20when%20simulation%0Abecomes%20costly.%20By%20integrating%20computational%20methods%20previously%20viewed%20as%0Aseparate%20into%20a%20unified%20model%2C%20SHM%20quantitatively%20captures%20their%20switching%0Amechanism.%20The%20SHM%20aligns%20more%20precisely%20with%20human%20behavior%20and%20demonstrates%0Aconsistent%20predictive%20performance%20across%20diverse%20scenarios%2C%20advancing%20our%0Aunderstanding%20of%20the%20adaptive%20nature%20of%20intuitive%20physical%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.09546v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520simulation-heuristics%2520dual-process%2520model%2520for%2520intuitive%2520physics%26entry.906535625%3DShiqian%2520Li%2520and%2520Yuxi%2520Ma%2520and%2520Jiajun%2520Yan%2520and%2520Bo%2520Dai%2520and%2520Yujia%2520Peng%2520and%2520Chi%2520Zhang%2520and%2520Yixin%2520Zhu%26entry.1292438233%3D%2520%2520The%2520role%2520of%2520mental%2520simulation%2520in%2520human%2520physical%2520reasoning%2520is%2520widely%250Aacknowledged%252C%2520but%2520whether%2520it%2520is%2520employed%2520across%2520scenarios%2520with%2520varying%250Asimulation%2520costs%2520and%2520where%2520its%2520boundary%2520lies%2520remains%2520unclear.%2520Using%2520a%250Apouring-marble%2520task%252C%2520our%2520human%2520study%2520revealed%2520two%2520distinct%2520error%2520patterns%2520when%250Apredicting%2520pouring%2520angles%252C%2520differentiated%2520by%2520simulation%2520time.%2520While%2520mental%250Asimulation%2520accurately%2520captured%2520human%2520judgments%2520in%2520simpler%2520scenarios%252C%2520a%2520linear%250Aheuristic%2520model%2520better%2520matched%2520human%2520predictions%2520when%2520simulation%2520time%2520exceeded%250Aa%2520certain%2520boundary.%2520Motivated%2520by%2520these%2520observations%252C%2520we%2520propose%2520a%2520dual-process%250Aframework%252C%2520Simulation-Heuristics%2520Model%2520%2528SHM%2529%252C%2520where%2520intuitive%2520physics%2520employs%250Asimulation%2520for%2520short-time%2520simulation%2520but%2520switches%2520to%2520heuristics%2520when%2520simulation%250Abecomes%2520costly.%2520By%2520integrating%2520computational%2520methods%2520previously%2520viewed%2520as%250Aseparate%2520into%2520a%2520unified%2520model%252C%2520SHM%2520quantitatively%2520captures%2520their%2520switching%250Amechanism.%2520The%2520SHM%2520aligns%2520more%2520precisely%2520with%2520human%2520behavior%2520and%2520demonstrates%250Aconsistent%2520predictive%2520performance%2520across%2520diverse%2520scenarios%252C%2520advancing%2520our%250Aunderstanding%2520of%2520the%2520adaptive%2520nature%2520of%2520intuitive%2520physical%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09546v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20simulation-heuristics%20dual-process%20model%20for%20intuitive%20physics&entry.906535625=Shiqian%20Li%20and%20Yuxi%20Ma%20and%20Jiajun%20Yan%20and%20Bo%20Dai%20and%20Yujia%20Peng%20and%20Chi%20Zhang%20and%20Yixin%20Zhu&entry.1292438233=%20%20The%20role%20of%20mental%20simulation%20in%20human%20physical%20reasoning%20is%20widely%0Aacknowledged%2C%20but%20whether%20it%20is%20employed%20across%20scenarios%20with%20varying%0Asimulation%20costs%20and%20where%20its%20boundary%20lies%20remains%20unclear.%20Using%20a%0Apouring-marble%20task%2C%20our%20human%20study%20revealed%20two%20distinct%20error%20patterns%20when%0Apredicting%20pouring%20angles%2C%20differentiated%20by%20simulation%20time.%20While%20mental%0Asimulation%20accurately%20captured%20human%20judgments%20in%20simpler%20scenarios%2C%20a%20linear%0Aheuristic%20model%20better%20matched%20human%20predictions%20when%20simulation%20time%20exceeded%0Aa%20certain%20boundary.%20Motivated%20by%20these%20observations%2C%20we%20propose%20a%20dual-process%0Aframework%2C%20Simulation-Heuristics%20Model%20%28SHM%29%2C%20where%20intuitive%20physics%20employs%0Asimulation%20for%20short-time%20simulation%20but%20switches%20to%20heuristics%20when%20simulation%0Abecomes%20costly.%20By%20integrating%20computational%20methods%20previously%20viewed%20as%0Aseparate%20into%20a%20unified%20model%2C%20SHM%20quantitatively%20captures%20their%20switching%0Amechanism.%20The%20SHM%20aligns%20more%20precisely%20with%20human%20behavior%20and%20demonstrates%0Aconsistent%20predictive%20performance%20across%20diverse%20scenarios%2C%20advancing%20our%0Aunderstanding%20of%20the%20adaptive%20nature%20of%20intuitive%20physical%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.09546v2&entry.124074799=Read"},
{"title": "Courteous MPC for Autonomous Driving with CBF-inspired Risk Assessment", "author": "Yanze Zhang and Yiwei Lyu and Sude E. Demir and Xingyu Zhou and Yupeng Yang and Junmin Wang and Wenhao Luo", "abstract": "  With more autonomous vehicles (AVs) sharing roadways with human-driven\nvehicles (HVs), ensuring safe and courteous maneuvers that respect HVs'\nbehavior becomes increasingly important. To promote both safety and courtesy in\nAV's behavior, an extension of Control Barrier Functions (CBFs)-inspired risk\nevaluation framework is proposed in this paper by considering both noisy\nobserved positions and velocities of surrounding vehicles. The perceived risk\nby the ego vehicle can be visualized as a risk map that reflects the\nunderstanding of the surrounding environment and thus shows the potential for\nfacilitating safe and courteous driving. By incorporating the risk evaluation\nframework into the Model Predictive Control (MPC) scheme, we propose a\nCourteous MPC for ego AV to generate courteous behaviors that 1) reduce the\noverall risk imposed on other vehicles and 2) respect the hard safety\nconstraints and the original objective for efficiency. We demonstrate the\nperformance of the proposed Courteous MPC via theoretical analysis and\nsimulation experiments.\n", "link": "http://arxiv.org/abs/2408.12822v2", "date": "2025-04-29", "relevancy": 2.0478, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5398}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5064}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Courteous%20MPC%20for%20Autonomous%20Driving%20with%20CBF-inspired%20Risk%20Assessment&body=Title%3A%20Courteous%20MPC%20for%20Autonomous%20Driving%20with%20CBF-inspired%20Risk%20Assessment%0AAuthor%3A%20Yanze%20Zhang%20and%20Yiwei%20Lyu%20and%20Sude%20E.%20Demir%20and%20Xingyu%20Zhou%20and%20Yupeng%20Yang%20and%20Junmin%20Wang%20and%20Wenhao%20Luo%0AAbstract%3A%20%20%20With%20more%20autonomous%20vehicles%20%28AVs%29%20sharing%20roadways%20with%20human-driven%0Avehicles%20%28HVs%29%2C%20ensuring%20safe%20and%20courteous%20maneuvers%20that%20respect%20HVs%27%0Abehavior%20becomes%20increasingly%20important.%20To%20promote%20both%20safety%20and%20courtesy%20in%0AAV%27s%20behavior%2C%20an%20extension%20of%20Control%20Barrier%20Functions%20%28CBFs%29-inspired%20risk%0Aevaluation%20framework%20is%20proposed%20in%20this%20paper%20by%20considering%20both%20noisy%0Aobserved%20positions%20and%20velocities%20of%20surrounding%20vehicles.%20The%20perceived%20risk%0Aby%20the%20ego%20vehicle%20can%20be%20visualized%20as%20a%20risk%20map%20that%20reflects%20the%0Aunderstanding%20of%20the%20surrounding%20environment%20and%20thus%20shows%20the%20potential%20for%0Afacilitating%20safe%20and%20courteous%20driving.%20By%20incorporating%20the%20risk%20evaluation%0Aframework%20into%20the%20Model%20Predictive%20Control%20%28MPC%29%20scheme%2C%20we%20propose%20a%0ACourteous%20MPC%20for%20ego%20AV%20to%20generate%20courteous%20behaviors%20that%201%29%20reduce%20the%0Aoverall%20risk%20imposed%20on%20other%20vehicles%20and%202%29%20respect%20the%20hard%20safety%0Aconstraints%20and%20the%20original%20objective%20for%20efficiency.%20We%20demonstrate%20the%0Aperformance%20of%20the%20proposed%20Courteous%20MPC%20via%20theoretical%20analysis%20and%0Asimulation%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12822v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCourteous%2520MPC%2520for%2520Autonomous%2520Driving%2520with%2520CBF-inspired%2520Risk%2520Assessment%26entry.906535625%3DYanze%2520Zhang%2520and%2520Yiwei%2520Lyu%2520and%2520Sude%2520E.%2520Demir%2520and%2520Xingyu%2520Zhou%2520and%2520Yupeng%2520Yang%2520and%2520Junmin%2520Wang%2520and%2520Wenhao%2520Luo%26entry.1292438233%3D%2520%2520With%2520more%2520autonomous%2520vehicles%2520%2528AVs%2529%2520sharing%2520roadways%2520with%2520human-driven%250Avehicles%2520%2528HVs%2529%252C%2520ensuring%2520safe%2520and%2520courteous%2520maneuvers%2520that%2520respect%2520HVs%2527%250Abehavior%2520becomes%2520increasingly%2520important.%2520To%2520promote%2520both%2520safety%2520and%2520courtesy%2520in%250AAV%2527s%2520behavior%252C%2520an%2520extension%2520of%2520Control%2520Barrier%2520Functions%2520%2528CBFs%2529-inspired%2520risk%250Aevaluation%2520framework%2520is%2520proposed%2520in%2520this%2520paper%2520by%2520considering%2520both%2520noisy%250Aobserved%2520positions%2520and%2520velocities%2520of%2520surrounding%2520vehicles.%2520The%2520perceived%2520risk%250Aby%2520the%2520ego%2520vehicle%2520can%2520be%2520visualized%2520as%2520a%2520risk%2520map%2520that%2520reflects%2520the%250Aunderstanding%2520of%2520the%2520surrounding%2520environment%2520and%2520thus%2520shows%2520the%2520potential%2520for%250Afacilitating%2520safe%2520and%2520courteous%2520driving.%2520By%2520incorporating%2520the%2520risk%2520evaluation%250Aframework%2520into%2520the%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520scheme%252C%2520we%2520propose%2520a%250ACourteous%2520MPC%2520for%2520ego%2520AV%2520to%2520generate%2520courteous%2520behaviors%2520that%25201%2529%2520reduce%2520the%250Aoverall%2520risk%2520imposed%2520on%2520other%2520vehicles%2520and%25202%2529%2520respect%2520the%2520hard%2520safety%250Aconstraints%2520and%2520the%2520original%2520objective%2520for%2520efficiency.%2520We%2520demonstrate%2520the%250Aperformance%2520of%2520the%2520proposed%2520Courteous%2520MPC%2520via%2520theoretical%2520analysis%2520and%250Asimulation%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12822v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Courteous%20MPC%20for%20Autonomous%20Driving%20with%20CBF-inspired%20Risk%20Assessment&entry.906535625=Yanze%20Zhang%20and%20Yiwei%20Lyu%20and%20Sude%20E.%20Demir%20and%20Xingyu%20Zhou%20and%20Yupeng%20Yang%20and%20Junmin%20Wang%20and%20Wenhao%20Luo&entry.1292438233=%20%20With%20more%20autonomous%20vehicles%20%28AVs%29%20sharing%20roadways%20with%20human-driven%0Avehicles%20%28HVs%29%2C%20ensuring%20safe%20and%20courteous%20maneuvers%20that%20respect%20HVs%27%0Abehavior%20becomes%20increasingly%20important.%20To%20promote%20both%20safety%20and%20courtesy%20in%0AAV%27s%20behavior%2C%20an%20extension%20of%20Control%20Barrier%20Functions%20%28CBFs%29-inspired%20risk%0Aevaluation%20framework%20is%20proposed%20in%20this%20paper%20by%20considering%20both%20noisy%0Aobserved%20positions%20and%20velocities%20of%20surrounding%20vehicles.%20The%20perceived%20risk%0Aby%20the%20ego%20vehicle%20can%20be%20visualized%20as%20a%20risk%20map%20that%20reflects%20the%0Aunderstanding%20of%20the%20surrounding%20environment%20and%20thus%20shows%20the%20potential%20for%0Afacilitating%20safe%20and%20courteous%20driving.%20By%20incorporating%20the%20risk%20evaluation%0Aframework%20into%20the%20Model%20Predictive%20Control%20%28MPC%29%20scheme%2C%20we%20propose%20a%0ACourteous%20MPC%20for%20ego%20AV%20to%20generate%20courteous%20behaviors%20that%201%29%20reduce%20the%0Aoverall%20risk%20imposed%20on%20other%20vehicles%20and%202%29%20respect%20the%20hard%20safety%0Aconstraints%20and%20the%20original%20objective%20for%20efficiency.%20We%20demonstrate%20the%0Aperformance%20of%20the%20proposed%20Courteous%20MPC%20via%20theoretical%20analysis%20and%0Asimulation%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12822v2&entry.124074799=Read"},
{"title": "NoPain: No-box Point Cloud Attack via Optimal Transport Singular\n  Boundary", "author": "Zezeng Li and Xiaoyu Du and Na Lei and Liming Chen and Weimin Wang", "abstract": "  Adversarial attacks exploit the vulnerability of deep models against\nadversarial samples. Existing point cloud attackers are tailored to specific\nmodels, iteratively optimizing perturbations based on gradients in either a\nwhite-box or black-box setting. Despite their promising attack performance,\nthey often struggle to produce transferable adversarial samples due to\noverfitting the specific parameters of surrogate models. To overcome this\nissue, we shift our focus to the data distribution itself and introduce a novel\napproach named NoPain, which employs optimal transport (OT) to identify the\ninherent singular boundaries of the data manifold for cross-network point cloud\nattacks. Specifically, we first calculate the OT mapping from noise to the\ntarget feature space, then identify singular boundaries by locating\nnon-differentiable positions. Finally, we sample along singular boundaries to\ngenerate adversarial point clouds. Once the singular boundaries are determined,\nNoPain can efficiently produce adversarial samples without the need of\niterative updates or guidance from the surrogate classifiers. Extensive\nexperiments demonstrate that the proposed end-to-end method outperforms\nbaseline approaches in terms of both transferability and efficiency, while also\nmaintaining notable advantages even against defense strategies. Code and model\nare available at https://github.com/cognaclee/nopain\n", "link": "http://arxiv.org/abs/2503.00063v4", "date": "2025-04-29", "relevancy": 2.0452, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5127}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5106}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoPain%3A%20No-box%20Point%20Cloud%20Attack%20via%20Optimal%20Transport%20Singular%0A%20%20Boundary&body=Title%3A%20NoPain%3A%20No-box%20Point%20Cloud%20Attack%20via%20Optimal%20Transport%20Singular%0A%20%20Boundary%0AAuthor%3A%20Zezeng%20Li%20and%20Xiaoyu%20Du%20and%20Na%20Lei%20and%20Liming%20Chen%20and%20Weimin%20Wang%0AAbstract%3A%20%20%20Adversarial%20attacks%20exploit%20the%20vulnerability%20of%20deep%20models%20against%0Aadversarial%20samples.%20Existing%20point%20cloud%20attackers%20are%20tailored%20to%20specific%0Amodels%2C%20iteratively%20optimizing%20perturbations%20based%20on%20gradients%20in%20either%20a%0Awhite-box%20or%20black-box%20setting.%20Despite%20their%20promising%20attack%20performance%2C%0Athey%20often%20struggle%20to%20produce%20transferable%20adversarial%20samples%20due%20to%0Aoverfitting%20the%20specific%20parameters%20of%20surrogate%20models.%20To%20overcome%20this%0Aissue%2C%20we%20shift%20our%20focus%20to%20the%20data%20distribution%20itself%20and%20introduce%20a%20novel%0Aapproach%20named%20NoPain%2C%20which%20employs%20optimal%20transport%20%28OT%29%20to%20identify%20the%0Ainherent%20singular%20boundaries%20of%20the%20data%20manifold%20for%20cross-network%20point%20cloud%0Aattacks.%20Specifically%2C%20we%20first%20calculate%20the%20OT%20mapping%20from%20noise%20to%20the%0Atarget%20feature%20space%2C%20then%20identify%20singular%20boundaries%20by%20locating%0Anon-differentiable%20positions.%20Finally%2C%20we%20sample%20along%20singular%20boundaries%20to%0Agenerate%20adversarial%20point%20clouds.%20Once%20the%20singular%20boundaries%20are%20determined%2C%0ANoPain%20can%20efficiently%20produce%20adversarial%20samples%20without%20the%20need%20of%0Aiterative%20updates%20or%20guidance%20from%20the%20surrogate%20classifiers.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20end-to-end%20method%20outperforms%0Abaseline%20approaches%20in%20terms%20of%20both%20transferability%20and%20efficiency%2C%20while%20also%0Amaintaining%20notable%20advantages%20even%20against%20defense%20strategies.%20Code%20and%20model%0Aare%20available%20at%20https%3A//github.com/cognaclee/nopain%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00063v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoPain%253A%2520No-box%2520Point%2520Cloud%2520Attack%2520via%2520Optimal%2520Transport%2520Singular%250A%2520%2520Boundary%26entry.906535625%3DZezeng%2520Li%2520and%2520Xiaoyu%2520Du%2520and%2520Na%2520Lei%2520and%2520Liming%2520Chen%2520and%2520Weimin%2520Wang%26entry.1292438233%3D%2520%2520Adversarial%2520attacks%2520exploit%2520the%2520vulnerability%2520of%2520deep%2520models%2520against%250Aadversarial%2520samples.%2520Existing%2520point%2520cloud%2520attackers%2520are%2520tailored%2520to%2520specific%250Amodels%252C%2520iteratively%2520optimizing%2520perturbations%2520based%2520on%2520gradients%2520in%2520either%2520a%250Awhite-box%2520or%2520black-box%2520setting.%2520Despite%2520their%2520promising%2520attack%2520performance%252C%250Athey%2520often%2520struggle%2520to%2520produce%2520transferable%2520adversarial%2520samples%2520due%2520to%250Aoverfitting%2520the%2520specific%2520parameters%2520of%2520surrogate%2520models.%2520To%2520overcome%2520this%250Aissue%252C%2520we%2520shift%2520our%2520focus%2520to%2520the%2520data%2520distribution%2520itself%2520and%2520introduce%2520a%2520novel%250Aapproach%2520named%2520NoPain%252C%2520which%2520employs%2520optimal%2520transport%2520%2528OT%2529%2520to%2520identify%2520the%250Ainherent%2520singular%2520boundaries%2520of%2520the%2520data%2520manifold%2520for%2520cross-network%2520point%2520cloud%250Aattacks.%2520Specifically%252C%2520we%2520first%2520calculate%2520the%2520OT%2520mapping%2520from%2520noise%2520to%2520the%250Atarget%2520feature%2520space%252C%2520then%2520identify%2520singular%2520boundaries%2520by%2520locating%250Anon-differentiable%2520positions.%2520Finally%252C%2520we%2520sample%2520along%2520singular%2520boundaries%2520to%250Agenerate%2520adversarial%2520point%2520clouds.%2520Once%2520the%2520singular%2520boundaries%2520are%2520determined%252C%250ANoPain%2520can%2520efficiently%2520produce%2520adversarial%2520samples%2520without%2520the%2520need%2520of%250Aiterative%2520updates%2520or%2520guidance%2520from%2520the%2520surrogate%2520classifiers.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520end-to-end%2520method%2520outperforms%250Abaseline%2520approaches%2520in%2520terms%2520of%2520both%2520transferability%2520and%2520efficiency%252C%2520while%2520also%250Amaintaining%2520notable%2520advantages%2520even%2520against%2520defense%2520strategies.%2520Code%2520and%2520model%250Aare%2520available%2520at%2520https%253A//github.com/cognaclee/nopain%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00063v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoPain%3A%20No-box%20Point%20Cloud%20Attack%20via%20Optimal%20Transport%20Singular%0A%20%20Boundary&entry.906535625=Zezeng%20Li%20and%20Xiaoyu%20Du%20and%20Na%20Lei%20and%20Liming%20Chen%20and%20Weimin%20Wang&entry.1292438233=%20%20Adversarial%20attacks%20exploit%20the%20vulnerability%20of%20deep%20models%20against%0Aadversarial%20samples.%20Existing%20point%20cloud%20attackers%20are%20tailored%20to%20specific%0Amodels%2C%20iteratively%20optimizing%20perturbations%20based%20on%20gradients%20in%20either%20a%0Awhite-box%20or%20black-box%20setting.%20Despite%20their%20promising%20attack%20performance%2C%0Athey%20often%20struggle%20to%20produce%20transferable%20adversarial%20samples%20due%20to%0Aoverfitting%20the%20specific%20parameters%20of%20surrogate%20models.%20To%20overcome%20this%0Aissue%2C%20we%20shift%20our%20focus%20to%20the%20data%20distribution%20itself%20and%20introduce%20a%20novel%0Aapproach%20named%20NoPain%2C%20which%20employs%20optimal%20transport%20%28OT%29%20to%20identify%20the%0Ainherent%20singular%20boundaries%20of%20the%20data%20manifold%20for%20cross-network%20point%20cloud%0Aattacks.%20Specifically%2C%20we%20first%20calculate%20the%20OT%20mapping%20from%20noise%20to%20the%0Atarget%20feature%20space%2C%20then%20identify%20singular%20boundaries%20by%20locating%0Anon-differentiable%20positions.%20Finally%2C%20we%20sample%20along%20singular%20boundaries%20to%0Agenerate%20adversarial%20point%20clouds.%20Once%20the%20singular%20boundaries%20are%20determined%2C%0ANoPain%20can%20efficiently%20produce%20adversarial%20samples%20without%20the%20need%20of%0Aiterative%20updates%20or%20guidance%20from%20the%20surrogate%20classifiers.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20end-to-end%20method%20outperforms%0Abaseline%20approaches%20in%20terms%20of%20both%20transferability%20and%20efficiency%2C%20while%20also%0Amaintaining%20notable%20advantages%20even%20against%20defense%20strategies.%20Code%20and%20model%0Aare%20available%20at%20https%3A//github.com/cognaclee/nopain%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00063v4&entry.124074799=Read"},
{"title": "Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private\n  Data", "author": "Dayananda Herurkar and J\u00f6rn Hees and Vesselin Tzvetkov and Andreas Dengel", "abstract": "  The remarkable success of Deep Learning approaches is often based and\ndemonstrated on large public datasets. However, when applying such approaches\nto internal, private datasets, one frequently faces challenges arising from\nstructural differences in the datasets, domain shift, and the lack of labels.\nIn this work, we introduce Tabular Data Adapters (TDA), a novel method for\ngenerating soft labels for unlabeled tabular data in outlier detection tasks.\nBy identifying statistically similar public datasets and transforming private\ndata (based on a shared autoencoder) into a format compatible with\nstate-of-the-art public models, our approach enables the generation of weak\nlabels. It thereby can help to mitigate the cold start problem of labeling by\nbasing on existing outlier detection models for public datasets. In experiments\non 50 tabular datasets across different domains, we demonstrate that our method\nis able to provide more accurate annotations than baseline approaches while\nreducing computational time. Our approach offers a scalable, efficient, and\ncost-effective solution, to bridge the gap between public research models and\nreal-world industrial applications.\n", "link": "http://arxiv.org/abs/2504.20862v1", "date": "2025-04-29", "relevancy": 2.0443, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5496}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5066}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tabular%20Data%20Adapters%3A%20Improving%20Outlier%20Detection%20for%20Unlabeled%20Private%0A%20%20Data&body=Title%3A%20Tabular%20Data%20Adapters%3A%20Improving%20Outlier%20Detection%20for%20Unlabeled%20Private%0A%20%20Data%0AAuthor%3A%20Dayananda%20Herurkar%20and%20J%C3%B6rn%20Hees%20and%20Vesselin%20Tzvetkov%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20The%20remarkable%20success%20of%20Deep%20Learning%20approaches%20is%20often%20based%20and%0Ademonstrated%20on%20large%20public%20datasets.%20However%2C%20when%20applying%20such%20approaches%0Ato%20internal%2C%20private%20datasets%2C%20one%20frequently%20faces%20challenges%20arising%20from%0Astructural%20differences%20in%20the%20datasets%2C%20domain%20shift%2C%20and%20the%20lack%20of%20labels.%0AIn%20this%20work%2C%20we%20introduce%20Tabular%20Data%20Adapters%20%28TDA%29%2C%20a%20novel%20method%20for%0Agenerating%20soft%20labels%20for%20unlabeled%20tabular%20data%20in%20outlier%20detection%20tasks.%0ABy%20identifying%20statistically%20similar%20public%20datasets%20and%20transforming%20private%0Adata%20%28based%20on%20a%20shared%20autoencoder%29%20into%20a%20format%20compatible%20with%0Astate-of-the-art%20public%20models%2C%20our%20approach%20enables%20the%20generation%20of%20weak%0Alabels.%20It%20thereby%20can%20help%20to%20mitigate%20the%20cold%20start%20problem%20of%20labeling%20by%0Abasing%20on%20existing%20outlier%20detection%20models%20for%20public%20datasets.%20In%20experiments%0Aon%2050%20tabular%20datasets%20across%20different%20domains%2C%20we%20demonstrate%20that%20our%20method%0Ais%20able%20to%20provide%20more%20accurate%20annotations%20than%20baseline%20approaches%20while%0Areducing%20computational%20time.%20Our%20approach%20offers%20a%20scalable%2C%20efficient%2C%20and%0Acost-effective%20solution%2C%20to%20bridge%20the%20gap%20between%20public%20research%20models%20and%0Areal-world%20industrial%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTabular%2520Data%2520Adapters%253A%2520Improving%2520Outlier%2520Detection%2520for%2520Unlabeled%2520Private%250A%2520%2520Data%26entry.906535625%3DDayananda%2520Herurkar%2520and%2520J%25C3%25B6rn%2520Hees%2520and%2520Vesselin%2520Tzvetkov%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520The%2520remarkable%2520success%2520of%2520Deep%2520Learning%2520approaches%2520is%2520often%2520based%2520and%250Ademonstrated%2520on%2520large%2520public%2520datasets.%2520However%252C%2520when%2520applying%2520such%2520approaches%250Ato%2520internal%252C%2520private%2520datasets%252C%2520one%2520frequently%2520faces%2520challenges%2520arising%2520from%250Astructural%2520differences%2520in%2520the%2520datasets%252C%2520domain%2520shift%252C%2520and%2520the%2520lack%2520of%2520labels.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520Tabular%2520Data%2520Adapters%2520%2528TDA%2529%252C%2520a%2520novel%2520method%2520for%250Agenerating%2520soft%2520labels%2520for%2520unlabeled%2520tabular%2520data%2520in%2520outlier%2520detection%2520tasks.%250ABy%2520identifying%2520statistically%2520similar%2520public%2520datasets%2520and%2520transforming%2520private%250Adata%2520%2528based%2520on%2520a%2520shared%2520autoencoder%2529%2520into%2520a%2520format%2520compatible%2520with%250Astate-of-the-art%2520public%2520models%252C%2520our%2520approach%2520enables%2520the%2520generation%2520of%2520weak%250Alabels.%2520It%2520thereby%2520can%2520help%2520to%2520mitigate%2520the%2520cold%2520start%2520problem%2520of%2520labeling%2520by%250Abasing%2520on%2520existing%2520outlier%2520detection%2520models%2520for%2520public%2520datasets.%2520In%2520experiments%250Aon%252050%2520tabular%2520datasets%2520across%2520different%2520domains%252C%2520we%2520demonstrate%2520that%2520our%2520method%250Ais%2520able%2520to%2520provide%2520more%2520accurate%2520annotations%2520than%2520baseline%2520approaches%2520while%250Areducing%2520computational%2520time.%2520Our%2520approach%2520offers%2520a%2520scalable%252C%2520efficient%252C%2520and%250Acost-effective%2520solution%252C%2520to%2520bridge%2520the%2520gap%2520between%2520public%2520research%2520models%2520and%250Areal-world%2520industrial%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tabular%20Data%20Adapters%3A%20Improving%20Outlier%20Detection%20for%20Unlabeled%20Private%0A%20%20Data&entry.906535625=Dayananda%20Herurkar%20and%20J%C3%B6rn%20Hees%20and%20Vesselin%20Tzvetkov%20and%20Andreas%20Dengel&entry.1292438233=%20%20The%20remarkable%20success%20of%20Deep%20Learning%20approaches%20is%20often%20based%20and%0Ademonstrated%20on%20large%20public%20datasets.%20However%2C%20when%20applying%20such%20approaches%0Ato%20internal%2C%20private%20datasets%2C%20one%20frequently%20faces%20challenges%20arising%20from%0Astructural%20differences%20in%20the%20datasets%2C%20domain%20shift%2C%20and%20the%20lack%20of%20labels.%0AIn%20this%20work%2C%20we%20introduce%20Tabular%20Data%20Adapters%20%28TDA%29%2C%20a%20novel%20method%20for%0Agenerating%20soft%20labels%20for%20unlabeled%20tabular%20data%20in%20outlier%20detection%20tasks.%0ABy%20identifying%20statistically%20similar%20public%20datasets%20and%20transforming%20private%0Adata%20%28based%20on%20a%20shared%20autoencoder%29%20into%20a%20format%20compatible%20with%0Astate-of-the-art%20public%20models%2C%20our%20approach%20enables%20the%20generation%20of%20weak%0Alabels.%20It%20thereby%20can%20help%20to%20mitigate%20the%20cold%20start%20problem%20of%20labeling%20by%0Abasing%20on%20existing%20outlier%20detection%20models%20for%20public%20datasets.%20In%20experiments%0Aon%2050%20tabular%20datasets%20across%20different%20domains%2C%20we%20demonstrate%20that%20our%20method%0Ais%20able%20to%20provide%20more%20accurate%20annotations%20than%20baseline%20approaches%20while%0Areducing%20computational%20time.%20Our%20approach%20offers%20a%20scalable%2C%20efficient%2C%20and%0Acost-effective%20solution%2C%20to%20bridge%20the%20gap%20between%20public%20research%20models%20and%0Areal-world%20industrial%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20862v1&entry.124074799=Read"},
{"title": "3D ReX: Causal Explanations in 3D Neuroimaging Classification", "author": "Melane Navaratnarajah and Sophie A. Martin and David A. Kelly and Nathan Blake and Hana Chockler", "abstract": "  Explainability remains a significant problem for AI models in medical\nimaging, making it challenging for clinicians to trust AI-driven predictions.\nWe introduce 3D ReX, the first causality-based post-hoc explainability tool for\n3D models. 3D ReX uses the theory of actual causality to generate\nresponsibility maps which highlight the regions most crucial to the model's\ndecision. We test 3D ReX on a stroke detection model, providing insight into\nthe spatial distribution of features relevant to stroke.\n", "link": "http://arxiv.org/abs/2502.12181v3", "date": "2025-04-29", "relevancy": 2.0295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5111}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20ReX%3A%20Causal%20Explanations%20in%203D%20Neuroimaging%20Classification&body=Title%3A%203D%20ReX%3A%20Causal%20Explanations%20in%203D%20Neuroimaging%20Classification%0AAuthor%3A%20Melane%20Navaratnarajah%20and%20Sophie%20A.%20Martin%20and%20David%20A.%20Kelly%20and%20Nathan%20Blake%20and%20Hana%20Chockler%0AAbstract%3A%20%20%20Explainability%20remains%20a%20significant%20problem%20for%20AI%20models%20in%20medical%0Aimaging%2C%20making%20it%20challenging%20for%20clinicians%20to%20trust%20AI-driven%20predictions.%0AWe%20introduce%203D%20ReX%2C%20the%20first%20causality-based%20post-hoc%20explainability%20tool%20for%0A3D%20models.%203D%20ReX%20uses%20the%20theory%20of%20actual%20causality%20to%20generate%0Aresponsibility%20maps%20which%20highlight%20the%20regions%20most%20crucial%20to%20the%20model%27s%0Adecision.%20We%20test%203D%20ReX%20on%20a%20stroke%20detection%20model%2C%20providing%20insight%20into%0Athe%20spatial%20distribution%20of%20features%20relevant%20to%20stroke.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12181v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520ReX%253A%2520Causal%2520Explanations%2520in%25203D%2520Neuroimaging%2520Classification%26entry.906535625%3DMelane%2520Navaratnarajah%2520and%2520Sophie%2520A.%2520Martin%2520and%2520David%2520A.%2520Kelly%2520and%2520Nathan%2520Blake%2520and%2520Hana%2520Chockler%26entry.1292438233%3D%2520%2520Explainability%2520remains%2520a%2520significant%2520problem%2520for%2520AI%2520models%2520in%2520medical%250Aimaging%252C%2520making%2520it%2520challenging%2520for%2520clinicians%2520to%2520trust%2520AI-driven%2520predictions.%250AWe%2520introduce%25203D%2520ReX%252C%2520the%2520first%2520causality-based%2520post-hoc%2520explainability%2520tool%2520for%250A3D%2520models.%25203D%2520ReX%2520uses%2520the%2520theory%2520of%2520actual%2520causality%2520to%2520generate%250Aresponsibility%2520maps%2520which%2520highlight%2520the%2520regions%2520most%2520crucial%2520to%2520the%2520model%2527s%250Adecision.%2520We%2520test%25203D%2520ReX%2520on%2520a%2520stroke%2520detection%2520model%252C%2520providing%2520insight%2520into%250Athe%2520spatial%2520distribution%2520of%2520features%2520relevant%2520to%2520stroke.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12181v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20ReX%3A%20Causal%20Explanations%20in%203D%20Neuroimaging%20Classification&entry.906535625=Melane%20Navaratnarajah%20and%20Sophie%20A.%20Martin%20and%20David%20A.%20Kelly%20and%20Nathan%20Blake%20and%20Hana%20Chockler&entry.1292438233=%20%20Explainability%20remains%20a%20significant%20problem%20for%20AI%20models%20in%20medical%0Aimaging%2C%20making%20it%20challenging%20for%20clinicians%20to%20trust%20AI-driven%20predictions.%0AWe%20introduce%203D%20ReX%2C%20the%20first%20causality-based%20post-hoc%20explainability%20tool%20for%0A3D%20models.%203D%20ReX%20uses%20the%20theory%20of%20actual%20causality%20to%20generate%0Aresponsibility%20maps%20which%20highlight%20the%20regions%20most%20crucial%20to%20the%20model%27s%0Adecision.%20We%20test%203D%20ReX%20on%20a%20stroke%20detection%20model%2C%20providing%20insight%20into%0Athe%20spatial%20distribution%20of%20features%20relevant%20to%20stroke.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12181v3&entry.124074799=Read"},
{"title": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning\n  Distillation From Large to Small Language Models", "author": "Tyler McDonald and Ali Emami", "abstract": "  As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications.\n", "link": "http://arxiv.org/abs/2504.20946v1", "date": "2025-04-29", "relevancy": 2.0194, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trace-of-Thought%3A%20Enhanced%20Arithmetic%20Problem%20Solving%20via%20Reasoning%0A%20%20Distillation%20From%20Large%20to%20Small%20Language%20Models&body=Title%3A%20Trace-of-Thought%3A%20Enhanced%20Arithmetic%20Problem%20Solving%20via%20Reasoning%0A%20%20Distillation%20From%20Large%20to%20Small%20Language%20Models%0AAuthor%3A%20Tyler%20McDonald%20and%20Ali%20Emami%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20be%20leveraged%20for%20daily%20tasks%2C%0Aprompt%20engineering%20remains%20an%20active%20field%20of%20contribution%20within%20computational%0Alinguistics%2C%20particularly%20in%20domains%20requiring%20specialized%20knowledge%20such%20as%0Aarithmetic%20reasoning.%20While%20these%20LLMs%20are%20optimized%20for%20a%20variety%20of%20tasks%2C%0Atheir%20exhaustive%20employment%20may%20become%20computationally%20or%20financially%0Acumbersome%20for%20small%20teams.%20Additionally%2C%20complete%20reliance%20on%20proprietary%2C%0Aclosed-source%20models%20often%20limits%20customization%20and%20adaptability%2C%20posing%0Asignificant%20challenges%20in%20research%20and%20application%20scalability.%20Instead%2C%20by%0Aleveraging%20open-source%20models%20at%20or%20below%207%20billion%20parameters%2C%20we%20can%20optimize%0Aour%20resource%20usage%20while%20still%20observing%20remarkable%20gains%20over%20standard%0Aprompting%20approaches.%20To%20cultivate%20this%20notion%2C%20we%20introduce%20Trace-of-Thought%0APrompting%2C%20a%20simple%2C%20zero-shot%20prompt%20engineering%20method%20that%20instructs%20LLMs%20to%0Acreate%20observable%20subproblems%20using%20critical%20problem-solving%2C%20specifically%0Adesigned%20to%20enhance%20arithmetic%20reasoning%20capabilities.%20When%20applied%20to%0Aopen-source%20models%20in%20tandem%20with%20GPT-4%2C%20we%20observe%20that%20Trace-of-Thought%20not%0Aonly%20allows%20novel%20insight%20into%20the%20problem-solving%20process%20but%20also%20introduces%0Aperformance%20gains%20as%20large%20as%20125%25%20on%20language%20models%20at%20or%20below%207%20billion%0Aparameters.%20This%20approach%20underscores%20the%20potential%20of%20open-source%20initiatives%0Ain%20democratizing%20AI%20research%20and%20improving%20the%20accessibility%20of%20high-quality%0Acomputational%20linguistics%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrace-of-Thought%253A%2520Enhanced%2520Arithmetic%2520Problem%2520Solving%2520via%2520Reasoning%250A%2520%2520Distillation%2520From%2520Large%2520to%2520Small%2520Language%2520Models%26entry.906535625%3DTyler%2520McDonald%2520and%2520Ali%2520Emami%26entry.1292438233%3D%2520%2520As%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520continue%2520to%2520be%2520leveraged%2520for%2520daily%2520tasks%252C%250Aprompt%2520engineering%2520remains%2520an%2520active%2520field%2520of%2520contribution%2520within%2520computational%250Alinguistics%252C%2520particularly%2520in%2520domains%2520requiring%2520specialized%2520knowledge%2520such%2520as%250Aarithmetic%2520reasoning.%2520While%2520these%2520LLMs%2520are%2520optimized%2520for%2520a%2520variety%2520of%2520tasks%252C%250Atheir%2520exhaustive%2520employment%2520may%2520become%2520computationally%2520or%2520financially%250Acumbersome%2520for%2520small%2520teams.%2520Additionally%252C%2520complete%2520reliance%2520on%2520proprietary%252C%250Aclosed-source%2520models%2520often%2520limits%2520customization%2520and%2520adaptability%252C%2520posing%250Asignificant%2520challenges%2520in%2520research%2520and%2520application%2520scalability.%2520Instead%252C%2520by%250Aleveraging%2520open-source%2520models%2520at%2520or%2520below%25207%2520billion%2520parameters%252C%2520we%2520can%2520optimize%250Aour%2520resource%2520usage%2520while%2520still%2520observing%2520remarkable%2520gains%2520over%2520standard%250Aprompting%2520approaches.%2520To%2520cultivate%2520this%2520notion%252C%2520we%2520introduce%2520Trace-of-Thought%250APrompting%252C%2520a%2520simple%252C%2520zero-shot%2520prompt%2520engineering%2520method%2520that%2520instructs%2520LLMs%2520to%250Acreate%2520observable%2520subproblems%2520using%2520critical%2520problem-solving%252C%2520specifically%250Adesigned%2520to%2520enhance%2520arithmetic%2520reasoning%2520capabilities.%2520When%2520applied%2520to%250Aopen-source%2520models%2520in%2520tandem%2520with%2520GPT-4%252C%2520we%2520observe%2520that%2520Trace-of-Thought%2520not%250Aonly%2520allows%2520novel%2520insight%2520into%2520the%2520problem-solving%2520process%2520but%2520also%2520introduces%250Aperformance%2520gains%2520as%2520large%2520as%2520125%2525%2520on%2520language%2520models%2520at%2520or%2520below%25207%2520billion%250Aparameters.%2520This%2520approach%2520underscores%2520the%2520potential%2520of%2520open-source%2520initiatives%250Ain%2520democratizing%2520AI%2520research%2520and%2520improving%2520the%2520accessibility%2520of%2520high-quality%250Acomputational%2520linguistics%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trace-of-Thought%3A%20Enhanced%20Arithmetic%20Problem%20Solving%20via%20Reasoning%0A%20%20Distillation%20From%20Large%20to%20Small%20Language%20Models&entry.906535625=Tyler%20McDonald%20and%20Ali%20Emami&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20continue%20to%20be%20leveraged%20for%20daily%20tasks%2C%0Aprompt%20engineering%20remains%20an%20active%20field%20of%20contribution%20within%20computational%0Alinguistics%2C%20particularly%20in%20domains%20requiring%20specialized%20knowledge%20such%20as%0Aarithmetic%20reasoning.%20While%20these%20LLMs%20are%20optimized%20for%20a%20variety%20of%20tasks%2C%0Atheir%20exhaustive%20employment%20may%20become%20computationally%20or%20financially%0Acumbersome%20for%20small%20teams.%20Additionally%2C%20complete%20reliance%20on%20proprietary%2C%0Aclosed-source%20models%20often%20limits%20customization%20and%20adaptability%2C%20posing%0Asignificant%20challenges%20in%20research%20and%20application%20scalability.%20Instead%2C%20by%0Aleveraging%20open-source%20models%20at%20or%20below%207%20billion%20parameters%2C%20we%20can%20optimize%0Aour%20resource%20usage%20while%20still%20observing%20remarkable%20gains%20over%20standard%0Aprompting%20approaches.%20To%20cultivate%20this%20notion%2C%20we%20introduce%20Trace-of-Thought%0APrompting%2C%20a%20simple%2C%20zero-shot%20prompt%20engineering%20method%20that%20instructs%20LLMs%20to%0Acreate%20observable%20subproblems%20using%20critical%20problem-solving%2C%20specifically%0Adesigned%20to%20enhance%20arithmetic%20reasoning%20capabilities.%20When%20applied%20to%0Aopen-source%20models%20in%20tandem%20with%20GPT-4%2C%20we%20observe%20that%20Trace-of-Thought%20not%0Aonly%20allows%20novel%20insight%20into%20the%20problem-solving%20process%20but%20also%20introduces%0Aperformance%20gains%20as%20large%20as%20125%25%20on%20language%20models%20at%20or%20below%207%20billion%0Aparameters.%20This%20approach%20underscores%20the%20potential%20of%20open-source%20initiatives%0Ain%20democratizing%20AI%20research%20and%20improving%20the%20accessibility%20of%20high-quality%0Acomputational%20linguistics%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20946v1&entry.124074799=Read"},
{"title": "LocAgent: Graph-Guided LLM Agents for Code Localization", "author": "Zhaoling Chen and Xiangru Tang and Gangda Deng and Fang Wu and Jialong Wu and Zhiwei Jiang and Viktor Prasanna and Arman Cohan and Xingyao Wang", "abstract": "  Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.\n", "link": "http://arxiv.org/abs/2503.09089v2", "date": "2025-04-29", "relevancy": 1.9992, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5219}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4864}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LocAgent%3A%20Graph-Guided%20LLM%20Agents%20for%20Code%20Localization&body=Title%3A%20LocAgent%3A%20Graph-Guided%20LLM%20Agents%20for%20Code%20Localization%0AAuthor%3A%20Zhaoling%20Chen%20and%20Xiangru%20Tang%20and%20Gangda%20Deng%20and%20Fang%20Wu%20and%20Jialong%20Wu%20and%20Zhiwei%20Jiang%20and%20Viktor%20Prasanna%20and%20Arman%20Cohan%20and%20Xingyao%20Wang%0AAbstract%3A%20%20%20Code%20localization--identifying%20precisely%20where%20in%20a%20codebase%20changes%20need%20to%0Abe%20made--is%20a%20fundamental%20yet%20challenging%20task%20in%20software%20maintenance.%0AExisting%20approaches%20struggle%20to%20efficiently%20navigate%20complex%20codebases%20when%0Aidentifying%20relevant%20code%20sections.%20The%20challenge%20lies%20in%20bridging%20natural%0Alanguage%20problem%20descriptions%20with%20the%20appropriate%20code%20elements%2C%20often%0Arequiring%20reasoning%20across%20hierarchical%20structures%20and%20multiple%20dependencies.%0AWe%20introduce%20LocAgent%2C%20a%20framework%20that%20addresses%20code%20localization%20through%0Agraph-based%20representation.%20By%20parsing%20codebases%20into%20directed%20heterogeneous%0Agraphs%2C%20LocAgent%20creates%20a%20lightweight%20representation%20that%20captures%20code%0Astructures%20%28files%2C%20classes%2C%20functions%29%20and%20their%20dependencies%20%28imports%2C%0Ainvocations%2C%20inheritance%29%2C%20enabling%20LLM%20agents%20to%20effectively%20search%20and%20locate%0Arelevant%20entities%20through%20powerful%20multi-hop%20reasoning.%20Experimental%20results%20on%0Areal-world%20benchmarks%20demonstrate%20that%20our%20approach%20significantly%20enhances%0Aaccuracy%20in%20code%20localization.%20Notably%2C%20our%20method%20with%20the%20fine-tuned%0AQwen-2.5-Coder-Instruct-32B%20model%20achieves%20comparable%20results%20to%20SOTA%0Aproprietary%20models%20at%20greatly%20reduced%20cost%20%28approximately%2086%25%20reduction%29%2C%0Areaching%20up%20to%2092.7%25%20accuracy%20on%20file-level%20localization%20while%20improving%0Adownstream%20GitHub%20issue%20resolution%20success%20rates%20by%2012%25%20for%20multiple%20attempts%0A%28Pass%4010%29.%20Our%20code%20is%20available%20at%20https%3A//github.com/gersteinlab/LocAgent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.09089v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocAgent%253A%2520Graph-Guided%2520LLM%2520Agents%2520for%2520Code%2520Localization%26entry.906535625%3DZhaoling%2520Chen%2520and%2520Xiangru%2520Tang%2520and%2520Gangda%2520Deng%2520and%2520Fang%2520Wu%2520and%2520Jialong%2520Wu%2520and%2520Zhiwei%2520Jiang%2520and%2520Viktor%2520Prasanna%2520and%2520Arman%2520Cohan%2520and%2520Xingyao%2520Wang%26entry.1292438233%3D%2520%2520Code%2520localization--identifying%2520precisely%2520where%2520in%2520a%2520codebase%2520changes%2520need%2520to%250Abe%2520made--is%2520a%2520fundamental%2520yet%2520challenging%2520task%2520in%2520software%2520maintenance.%250AExisting%2520approaches%2520struggle%2520to%2520efficiently%2520navigate%2520complex%2520codebases%2520when%250Aidentifying%2520relevant%2520code%2520sections.%2520The%2520challenge%2520lies%2520in%2520bridging%2520natural%250Alanguage%2520problem%2520descriptions%2520with%2520the%2520appropriate%2520code%2520elements%252C%2520often%250Arequiring%2520reasoning%2520across%2520hierarchical%2520structures%2520and%2520multiple%2520dependencies.%250AWe%2520introduce%2520LocAgent%252C%2520a%2520framework%2520that%2520addresses%2520code%2520localization%2520through%250Agraph-based%2520representation.%2520By%2520parsing%2520codebases%2520into%2520directed%2520heterogeneous%250Agraphs%252C%2520LocAgent%2520creates%2520a%2520lightweight%2520representation%2520that%2520captures%2520code%250Astructures%2520%2528files%252C%2520classes%252C%2520functions%2529%2520and%2520their%2520dependencies%2520%2528imports%252C%250Ainvocations%252C%2520inheritance%2529%252C%2520enabling%2520LLM%2520agents%2520to%2520effectively%2520search%2520and%2520locate%250Arelevant%2520entities%2520through%2520powerful%2520multi-hop%2520reasoning.%2520Experimental%2520results%2520on%250Areal-world%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%250Aaccuracy%2520in%2520code%2520localization.%2520Notably%252C%2520our%2520method%2520with%2520the%2520fine-tuned%250AQwen-2.5-Coder-Instruct-32B%2520model%2520achieves%2520comparable%2520results%2520to%2520SOTA%250Aproprietary%2520models%2520at%2520greatly%2520reduced%2520cost%2520%2528approximately%252086%2525%2520reduction%2529%252C%250Areaching%2520up%2520to%252092.7%2525%2520accuracy%2520on%2520file-level%2520localization%2520while%2520improving%250Adownstream%2520GitHub%2520issue%2520resolution%2520success%2520rates%2520by%252012%2525%2520for%2520multiple%2520attempts%250A%2528Pass%254010%2529.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/gersteinlab/LocAgent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.09089v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocAgent%3A%20Graph-Guided%20LLM%20Agents%20for%20Code%20Localization&entry.906535625=Zhaoling%20Chen%20and%20Xiangru%20Tang%20and%20Gangda%20Deng%20and%20Fang%20Wu%20and%20Jialong%20Wu%20and%20Zhiwei%20Jiang%20and%20Viktor%20Prasanna%20and%20Arman%20Cohan%20and%20Xingyao%20Wang&entry.1292438233=%20%20Code%20localization--identifying%20precisely%20where%20in%20a%20codebase%20changes%20need%20to%0Abe%20made--is%20a%20fundamental%20yet%20challenging%20task%20in%20software%20maintenance.%0AExisting%20approaches%20struggle%20to%20efficiently%20navigate%20complex%20codebases%20when%0Aidentifying%20relevant%20code%20sections.%20The%20challenge%20lies%20in%20bridging%20natural%0Alanguage%20problem%20descriptions%20with%20the%20appropriate%20code%20elements%2C%20often%0Arequiring%20reasoning%20across%20hierarchical%20structures%20and%20multiple%20dependencies.%0AWe%20introduce%20LocAgent%2C%20a%20framework%20that%20addresses%20code%20localization%20through%0Agraph-based%20representation.%20By%20parsing%20codebases%20into%20directed%20heterogeneous%0Agraphs%2C%20LocAgent%20creates%20a%20lightweight%20representation%20that%20captures%20code%0Astructures%20%28files%2C%20classes%2C%20functions%29%20and%20their%20dependencies%20%28imports%2C%0Ainvocations%2C%20inheritance%29%2C%20enabling%20LLM%20agents%20to%20effectively%20search%20and%20locate%0Arelevant%20entities%20through%20powerful%20multi-hop%20reasoning.%20Experimental%20results%20on%0Areal-world%20benchmarks%20demonstrate%20that%20our%20approach%20significantly%20enhances%0Aaccuracy%20in%20code%20localization.%20Notably%2C%20our%20method%20with%20the%20fine-tuned%0AQwen-2.5-Coder-Instruct-32B%20model%20achieves%20comparable%20results%20to%20SOTA%0Aproprietary%20models%20at%20greatly%20reduced%20cost%20%28approximately%2086%25%20reduction%29%2C%0Areaching%20up%20to%2092.7%25%20accuracy%20on%20file-level%20localization%20while%20improving%0Adownstream%20GitHub%20issue%20resolution%20success%20rates%20by%2012%25%20for%20multiple%20attempts%0A%28Pass%4010%29.%20Our%20code%20is%20available%20at%20https%3A//github.com/gersteinlab/LocAgent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.09089v2&entry.124074799=Read"},
{"title": "Advancing Physics Data Analysis through Machine Learning and\n  Physics-Informed Neural Networks", "author": "Vasileios Vatellis", "abstract": "  In an era increasingly focused on green computing and explainable AI,\nrevisiting traditional approaches in theoretical and phenomenological particle\nphysics is paramount. This project evaluates various machine learning (ML)\nalgorithms-including Nearest Neighbors, Decision Trees, Random Forest,\nAdaBoost, Naive Bayes, Quadratic Discriminant Analysis (QDA), and\nXGBoost-alongside standard neural networks and a novel Physics-Informed Neural\nNetwork (PINN) for physics data analysis. We apply these techniques to a binary\nclassification task that distinguishes the experimental viability of simulated\nscenarios based on Higgs observables and essential parameters. Through this\ncomprehensive analysis, we aim to showcase the capabilities and computational\nefficiency of each model in binary classification tasks, thereby contributing\nto the ongoing discourse on integrating ML and Deep Neural Networks (DNNs) into\nphysics research. In this study, XGBoost emerged as the preferred choice among\nthe evaluated machine learning algorithms for its speed and effectiveness,\nespecially in the initial stages of computation with limited datasets. However,\nwhile standard Neural Networks and Physics-Informed Neural Networks (PINNs)\ndemonstrated superior performance in terms of accuracy and adherence to\nphysical laws, they require more computational time. These findings underscore\nthe trade-offs between computational efficiency and model sophistication.\n", "link": "http://arxiv.org/abs/2410.14760v2", "date": "2025-04-29", "relevancy": 1.9878, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5368}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5024}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Physics%20Data%20Analysis%20through%20Machine%20Learning%20and%0A%20%20Physics-Informed%20Neural%20Networks&body=Title%3A%20Advancing%20Physics%20Data%20Analysis%20through%20Machine%20Learning%20and%0A%20%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Vasileios%20Vatellis%0AAbstract%3A%20%20%20In%20an%20era%20increasingly%20focused%20on%20green%20computing%20and%20explainable%20AI%2C%0Arevisiting%20traditional%20approaches%20in%20theoretical%20and%20phenomenological%20particle%0Aphysics%20is%20paramount.%20This%20project%20evaluates%20various%20machine%20learning%20%28ML%29%0Aalgorithms-including%20Nearest%20Neighbors%2C%20Decision%20Trees%2C%20Random%20Forest%2C%0AAdaBoost%2C%20Naive%20Bayes%2C%20Quadratic%20Discriminant%20Analysis%20%28QDA%29%2C%20and%0AXGBoost-alongside%20standard%20neural%20networks%20and%20a%20novel%20Physics-Informed%20Neural%0ANetwork%20%28PINN%29%20for%20physics%20data%20analysis.%20We%20apply%20these%20techniques%20to%20a%20binary%0Aclassification%20task%20that%20distinguishes%20the%20experimental%20viability%20of%20simulated%0Ascenarios%20based%20on%20Higgs%20observables%20and%20essential%20parameters.%20Through%20this%0Acomprehensive%20analysis%2C%20we%20aim%20to%20showcase%20the%20capabilities%20and%20computational%0Aefficiency%20of%20each%20model%20in%20binary%20classification%20tasks%2C%20thereby%20contributing%0Ato%20the%20ongoing%20discourse%20on%20integrating%20ML%20and%20Deep%20Neural%20Networks%20%28DNNs%29%20into%0Aphysics%20research.%20In%20this%20study%2C%20XGBoost%20emerged%20as%20the%20preferred%20choice%20among%0Athe%20evaluated%20machine%20learning%20algorithms%20for%20its%20speed%20and%20effectiveness%2C%0Aespecially%20in%20the%20initial%20stages%20of%20computation%20with%20limited%20datasets.%20However%2C%0Awhile%20standard%20Neural%20Networks%20and%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%0Ademonstrated%20superior%20performance%20in%20terms%20of%20accuracy%20and%20adherence%20to%0Aphysical%20laws%2C%20they%20require%20more%20computational%20time.%20These%20findings%20underscore%0Athe%20trade-offs%20between%20computational%20efficiency%20and%20model%20sophistication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14760v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Physics%2520Data%2520Analysis%2520through%2520Machine%2520Learning%2520and%250A%2520%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DVasileios%2520Vatellis%26entry.1292438233%3D%2520%2520In%2520an%2520era%2520increasingly%2520focused%2520on%2520green%2520computing%2520and%2520explainable%2520AI%252C%250Arevisiting%2520traditional%2520approaches%2520in%2520theoretical%2520and%2520phenomenological%2520particle%250Aphysics%2520is%2520paramount.%2520This%2520project%2520evaluates%2520various%2520machine%2520learning%2520%2528ML%2529%250Aalgorithms-including%2520Nearest%2520Neighbors%252C%2520Decision%2520Trees%252C%2520Random%2520Forest%252C%250AAdaBoost%252C%2520Naive%2520Bayes%252C%2520Quadratic%2520Discriminant%2520Analysis%2520%2528QDA%2529%252C%2520and%250AXGBoost-alongside%2520standard%2520neural%2520networks%2520and%2520a%2520novel%2520Physics-Informed%2520Neural%250ANetwork%2520%2528PINN%2529%2520for%2520physics%2520data%2520analysis.%2520We%2520apply%2520these%2520techniques%2520to%2520a%2520binary%250Aclassification%2520task%2520that%2520distinguishes%2520the%2520experimental%2520viability%2520of%2520simulated%250Ascenarios%2520based%2520on%2520Higgs%2520observables%2520and%2520essential%2520parameters.%2520Through%2520this%250Acomprehensive%2520analysis%252C%2520we%2520aim%2520to%2520showcase%2520the%2520capabilities%2520and%2520computational%250Aefficiency%2520of%2520each%2520model%2520in%2520binary%2520classification%2520tasks%252C%2520thereby%2520contributing%250Ato%2520the%2520ongoing%2520discourse%2520on%2520integrating%2520ML%2520and%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520into%250Aphysics%2520research.%2520In%2520this%2520study%252C%2520XGBoost%2520emerged%2520as%2520the%2520preferred%2520choice%2520among%250Athe%2520evaluated%2520machine%2520learning%2520algorithms%2520for%2520its%2520speed%2520and%2520effectiveness%252C%250Aespecially%2520in%2520the%2520initial%2520stages%2520of%2520computation%2520with%2520limited%2520datasets.%2520However%252C%250Awhile%2520standard%2520Neural%2520Networks%2520and%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529%250Ademonstrated%2520superior%2520performance%2520in%2520terms%2520of%2520accuracy%2520and%2520adherence%2520to%250Aphysical%2520laws%252C%2520they%2520require%2520more%2520computational%2520time.%2520These%2520findings%2520underscore%250Athe%2520trade-offs%2520between%2520computational%2520efficiency%2520and%2520model%2520sophistication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14760v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Physics%20Data%20Analysis%20through%20Machine%20Learning%20and%0A%20%20Physics-Informed%20Neural%20Networks&entry.906535625=Vasileios%20Vatellis&entry.1292438233=%20%20In%20an%20era%20increasingly%20focused%20on%20green%20computing%20and%20explainable%20AI%2C%0Arevisiting%20traditional%20approaches%20in%20theoretical%20and%20phenomenological%20particle%0Aphysics%20is%20paramount.%20This%20project%20evaluates%20various%20machine%20learning%20%28ML%29%0Aalgorithms-including%20Nearest%20Neighbors%2C%20Decision%20Trees%2C%20Random%20Forest%2C%0AAdaBoost%2C%20Naive%20Bayes%2C%20Quadratic%20Discriminant%20Analysis%20%28QDA%29%2C%20and%0AXGBoost-alongside%20standard%20neural%20networks%20and%20a%20novel%20Physics-Informed%20Neural%0ANetwork%20%28PINN%29%20for%20physics%20data%20analysis.%20We%20apply%20these%20techniques%20to%20a%20binary%0Aclassification%20task%20that%20distinguishes%20the%20experimental%20viability%20of%20simulated%0Ascenarios%20based%20on%20Higgs%20observables%20and%20essential%20parameters.%20Through%20this%0Acomprehensive%20analysis%2C%20we%20aim%20to%20showcase%20the%20capabilities%20and%20computational%0Aefficiency%20of%20each%20model%20in%20binary%20classification%20tasks%2C%20thereby%20contributing%0Ato%20the%20ongoing%20discourse%20on%20integrating%20ML%20and%20Deep%20Neural%20Networks%20%28DNNs%29%20into%0Aphysics%20research.%20In%20this%20study%2C%20XGBoost%20emerged%20as%20the%20preferred%20choice%20among%0Athe%20evaluated%20machine%20learning%20algorithms%20for%20its%20speed%20and%20effectiveness%2C%0Aespecially%20in%20the%20initial%20stages%20of%20computation%20with%20limited%20datasets.%20However%2C%0Awhile%20standard%20Neural%20Networks%20and%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%0Ademonstrated%20superior%20performance%20in%20terms%20of%20accuracy%20and%20adherence%20to%0Aphysical%20laws%2C%20they%20require%20more%20computational%20time.%20These%20findings%20underscore%0Athe%20trade-offs%20between%20computational%20efficiency%20and%20model%20sophistication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14760v2&entry.124074799=Read"},
{"title": "Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for\n  Visual Classifiers", "author": "Quentin Guimard and Moreno D'Inc\u00e0 and Massimiliano Mancini and Elisa Ricci", "abstract": "  A person downloading a pre-trained model from the web should be aware of its\nbiases. Existing approaches for bias identification rely on datasets containing\nlabels for the task of interest, something that a non-expert may not have\naccess to, or may not have the necessary resources to collect: this greatly\nlimits the number of tasks where model biases can be identified. In this work,\nwe present Classifier-to-Bias (C2B), the first bias discovery framework that\nworks without access to any labeled data: it only relies on a textual\ndescription of the classification task to identify biases in the target\nclassification model. This description is fed to a large language model to\ngenerate bias proposals and corresponding captions depicting biases together\nwith task-specific target labels. A retrieval model collects images for those\ncaptions, which are then used to assess the accuracy of the model w.r.t. the\ngiven biases. C2B is training-free, does not require any annotations, has no\nconstraints on the list of biases, and can be applied to any pre-trained model\non any classification task. Experiments on two publicly available datasets show\nthat C2B discovers biases beyond those of the original datasets and outperforms\na recent state-of-the-art bias detection baseline that relies on task-specific\nannotations, being a promising first step toward addressing task-agnostic\nunsupervised bias detection.\n", "link": "http://arxiv.org/abs/2504.20902v1", "date": "2025-04-29", "relevancy": 1.9803, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.503}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4959}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classifier-to-Bias%3A%20Toward%20Unsupervised%20Automatic%20Bias%20Detection%20for%0A%20%20Visual%20Classifiers&body=Title%3A%20Classifier-to-Bias%3A%20Toward%20Unsupervised%20Automatic%20Bias%20Detection%20for%0A%20%20Visual%20Classifiers%0AAuthor%3A%20Quentin%20Guimard%20and%20Moreno%20D%27Inc%C3%A0%20and%20Massimiliano%20Mancini%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20A%20person%20downloading%20a%20pre-trained%20model%20from%20the%20web%20should%20be%20aware%20of%20its%0Abiases.%20Existing%20approaches%20for%20bias%20identification%20rely%20on%20datasets%20containing%0Alabels%20for%20the%20task%20of%20interest%2C%20something%20that%20a%20non-expert%20may%20not%20have%0Aaccess%20to%2C%20or%20may%20not%20have%20the%20necessary%20resources%20to%20collect%3A%20this%20greatly%0Alimits%20the%20number%20of%20tasks%20where%20model%20biases%20can%20be%20identified.%20In%20this%20work%2C%0Awe%20present%20Classifier-to-Bias%20%28C2B%29%2C%20the%20first%20bias%20discovery%20framework%20that%0Aworks%20without%20access%20to%20any%20labeled%20data%3A%20it%20only%20relies%20on%20a%20textual%0Adescription%20of%20the%20classification%20task%20to%20identify%20biases%20in%20the%20target%0Aclassification%20model.%20This%20description%20is%20fed%20to%20a%20large%20language%20model%20to%0Agenerate%20bias%20proposals%20and%20corresponding%20captions%20depicting%20biases%20together%0Awith%20task-specific%20target%20labels.%20A%20retrieval%20model%20collects%20images%20for%20those%0Acaptions%2C%20which%20are%20then%20used%20to%20assess%20the%20accuracy%20of%20the%20model%20w.r.t.%20the%0Agiven%20biases.%20C2B%20is%20training-free%2C%20does%20not%20require%20any%20annotations%2C%20has%20no%0Aconstraints%20on%20the%20list%20of%20biases%2C%20and%20can%20be%20applied%20to%20any%20pre-trained%20model%0Aon%20any%20classification%20task.%20Experiments%20on%20two%20publicly%20available%20datasets%20show%0Athat%20C2B%20discovers%20biases%20beyond%20those%20of%20the%20original%20datasets%20and%20outperforms%0Aa%20recent%20state-of-the-art%20bias%20detection%20baseline%20that%20relies%20on%20task-specific%0Aannotations%2C%20being%20a%20promising%20first%20step%20toward%20addressing%20task-agnostic%0Aunsupervised%20bias%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassifier-to-Bias%253A%2520Toward%2520Unsupervised%2520Automatic%2520Bias%2520Detection%2520for%250A%2520%2520Visual%2520Classifiers%26entry.906535625%3DQuentin%2520Guimard%2520and%2520Moreno%2520D%2527Inc%25C3%25A0%2520and%2520Massimiliano%2520Mancini%2520and%2520Elisa%2520Ricci%26entry.1292438233%3D%2520%2520A%2520person%2520downloading%2520a%2520pre-trained%2520model%2520from%2520the%2520web%2520should%2520be%2520aware%2520of%2520its%250Abiases.%2520Existing%2520approaches%2520for%2520bias%2520identification%2520rely%2520on%2520datasets%2520containing%250Alabels%2520for%2520the%2520task%2520of%2520interest%252C%2520something%2520that%2520a%2520non-expert%2520may%2520not%2520have%250Aaccess%2520to%252C%2520or%2520may%2520not%2520have%2520the%2520necessary%2520resources%2520to%2520collect%253A%2520this%2520greatly%250Alimits%2520the%2520number%2520of%2520tasks%2520where%2520model%2520biases%2520can%2520be%2520identified.%2520In%2520this%2520work%252C%250Awe%2520present%2520Classifier-to-Bias%2520%2528C2B%2529%252C%2520the%2520first%2520bias%2520discovery%2520framework%2520that%250Aworks%2520without%2520access%2520to%2520any%2520labeled%2520data%253A%2520it%2520only%2520relies%2520on%2520a%2520textual%250Adescription%2520of%2520the%2520classification%2520task%2520to%2520identify%2520biases%2520in%2520the%2520target%250Aclassification%2520model.%2520This%2520description%2520is%2520fed%2520to%2520a%2520large%2520language%2520model%2520to%250Agenerate%2520bias%2520proposals%2520and%2520corresponding%2520captions%2520depicting%2520biases%2520together%250Awith%2520task-specific%2520target%2520labels.%2520A%2520retrieval%2520model%2520collects%2520images%2520for%2520those%250Acaptions%252C%2520which%2520are%2520then%2520used%2520to%2520assess%2520the%2520accuracy%2520of%2520the%2520model%2520w.r.t.%2520the%250Agiven%2520biases.%2520C2B%2520is%2520training-free%252C%2520does%2520not%2520require%2520any%2520annotations%252C%2520has%2520no%250Aconstraints%2520on%2520the%2520list%2520of%2520biases%252C%2520and%2520can%2520be%2520applied%2520to%2520any%2520pre-trained%2520model%250Aon%2520any%2520classification%2520task.%2520Experiments%2520on%2520two%2520publicly%2520available%2520datasets%2520show%250Athat%2520C2B%2520discovers%2520biases%2520beyond%2520those%2520of%2520the%2520original%2520datasets%2520and%2520outperforms%250Aa%2520recent%2520state-of-the-art%2520bias%2520detection%2520baseline%2520that%2520relies%2520on%2520task-specific%250Aannotations%252C%2520being%2520a%2520promising%2520first%2520step%2520toward%2520addressing%2520task-agnostic%250Aunsupervised%2520bias%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classifier-to-Bias%3A%20Toward%20Unsupervised%20Automatic%20Bias%20Detection%20for%0A%20%20Visual%20Classifiers&entry.906535625=Quentin%20Guimard%20and%20Moreno%20D%27Inc%C3%A0%20and%20Massimiliano%20Mancini%20and%20Elisa%20Ricci&entry.1292438233=%20%20A%20person%20downloading%20a%20pre-trained%20model%20from%20the%20web%20should%20be%20aware%20of%20its%0Abiases.%20Existing%20approaches%20for%20bias%20identification%20rely%20on%20datasets%20containing%0Alabels%20for%20the%20task%20of%20interest%2C%20something%20that%20a%20non-expert%20may%20not%20have%0Aaccess%20to%2C%20or%20may%20not%20have%20the%20necessary%20resources%20to%20collect%3A%20this%20greatly%0Alimits%20the%20number%20of%20tasks%20where%20model%20biases%20can%20be%20identified.%20In%20this%20work%2C%0Awe%20present%20Classifier-to-Bias%20%28C2B%29%2C%20the%20first%20bias%20discovery%20framework%20that%0Aworks%20without%20access%20to%20any%20labeled%20data%3A%20it%20only%20relies%20on%20a%20textual%0Adescription%20of%20the%20classification%20task%20to%20identify%20biases%20in%20the%20target%0Aclassification%20model.%20This%20description%20is%20fed%20to%20a%20large%20language%20model%20to%0Agenerate%20bias%20proposals%20and%20corresponding%20captions%20depicting%20biases%20together%0Awith%20task-specific%20target%20labels.%20A%20retrieval%20model%20collects%20images%20for%20those%0Acaptions%2C%20which%20are%20then%20used%20to%20assess%20the%20accuracy%20of%20the%20model%20w.r.t.%20the%0Agiven%20biases.%20C2B%20is%20training-free%2C%20does%20not%20require%20any%20annotations%2C%20has%20no%0Aconstraints%20on%20the%20list%20of%20biases%2C%20and%20can%20be%20applied%20to%20any%20pre-trained%20model%0Aon%20any%20classification%20task.%20Experiments%20on%20two%20publicly%20available%20datasets%20show%0Athat%20C2B%20discovers%20biases%20beyond%20those%20of%20the%20original%20datasets%20and%20outperforms%0Aa%20recent%20state-of-the-art%20bias%20detection%20baseline%20that%20relies%20on%20task-specific%0Aannotations%2C%20being%20a%20promising%20first%20step%20toward%20addressing%20task-agnostic%0Aunsupervised%20bias%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20902v1&entry.124074799=Read"},
{"title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation", "author": "Lucas Caccia and Alan Ansell and Edoardo Ponti and Ivan Vuli\u0107 and Alessandro Sordoni", "abstract": "  Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG.\n", "link": "http://arxiv.org/abs/2503.08727v2", "date": "2025-04-29", "relevancy": 1.9582, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5055}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4868}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Plug-n-Play%20Knowledge%20Modules%20with%20Deep%20Context%20Distillation&body=Title%3A%20Training%20Plug-n-Play%20Knowledge%20Modules%20with%20Deep%20Context%20Distillation%0AAuthor%3A%20Lucas%20Caccia%20and%20Alan%20Ansell%20and%20Edoardo%20Ponti%20and%20Ivan%20Vuli%C4%87%20and%20Alessandro%20Sordoni%0AAbstract%3A%20%20%20Dynamically%20integrating%20new%20or%20rapidly%20evolving%20information%20after%20%28Large%29%0ALanguage%20Model%20pre-training%20remains%20challenging%2C%20particularly%20in%20low-data%0Ascenarios%20or%20when%20dealing%20with%20private%20and%20specialized%20documents.%20In-context%0Alearning%20and%20retrieval-augmented%20generation%20%28RAG%29%20face%20limitations%2C%20including%0Atheir%20high%20inference%20costs%20and%20their%20inability%20to%20capture%20global%20document%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20way%20of%20modularizing%20knowledge%20by%0Atraining%20document-level%20Knowledge%20Modules%20%28KMs%29.%20KMs%20are%20lightweight%20components%0Aimplemented%20as%20parameter-efficient%20LoRA%20modules%2C%20which%20are%20trained%20to%20store%0Ainformation%20about%20new%20documents%20and%20can%20be%20easily%20plugged%20into%20models%20on%0Ademand.%20We%20show%20that%20next-token%20prediction%20performs%20poorly%20as%20the%20training%0Aobjective%20for%20KMs.%20We%20instead%20propose%20Deep%20Context%20Distillation%3A%20we%20learn%20KMs%0Aparameters%20such%20as%20to%20simulate%20hidden%20states%20and%20logits%20of%20a%20teacher%20that%20takes%0Athe%20document%20in%20context.%20Our%20method%20outperforms%20standard%20next-token%20prediction%0Aand%20pre-instruction%20training%20techniques%2C%20across%20two%20datasets.%20Finally%2C%20we%0Ahighlight%20synergies%20between%20KMs%20and%20RAG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.08727v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Plug-n-Play%2520Knowledge%2520Modules%2520with%2520Deep%2520Context%2520Distillation%26entry.906535625%3DLucas%2520Caccia%2520and%2520Alan%2520Ansell%2520and%2520Edoardo%2520Ponti%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Alessandro%2520Sordoni%26entry.1292438233%3D%2520%2520Dynamically%2520integrating%2520new%2520or%2520rapidly%2520evolving%2520information%2520after%2520%2528Large%2529%250ALanguage%2520Model%2520pre-training%2520remains%2520challenging%252C%2520particularly%2520in%2520low-data%250Ascenarios%2520or%2520when%2520dealing%2520with%2520private%2520and%2520specialized%2520documents.%2520In-context%250Alearning%2520and%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520face%2520limitations%252C%2520including%250Atheir%2520high%2520inference%2520costs%2520and%2520their%2520inability%2520to%2520capture%2520global%2520document%250Ainformation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520way%2520of%2520modularizing%2520knowledge%2520by%250Atraining%2520document-level%2520Knowledge%2520Modules%2520%2528KMs%2529.%2520KMs%2520are%2520lightweight%2520components%250Aimplemented%2520as%2520parameter-efficient%2520LoRA%2520modules%252C%2520which%2520are%2520trained%2520to%2520store%250Ainformation%2520about%2520new%2520documents%2520and%2520can%2520be%2520easily%2520plugged%2520into%2520models%2520on%250Ademand.%2520We%2520show%2520that%2520next-token%2520prediction%2520performs%2520poorly%2520as%2520the%2520training%250Aobjective%2520for%2520KMs.%2520We%2520instead%2520propose%2520Deep%2520Context%2520Distillation%253A%2520we%2520learn%2520KMs%250Aparameters%2520such%2520as%2520to%2520simulate%2520hidden%2520states%2520and%2520logits%2520of%2520a%2520teacher%2520that%2520takes%250Athe%2520document%2520in%2520context.%2520Our%2520method%2520outperforms%2520standard%2520next-token%2520prediction%250Aand%2520pre-instruction%2520training%2520techniques%252C%2520across%2520two%2520datasets.%2520Finally%252C%2520we%250Ahighlight%2520synergies%2520between%2520KMs%2520and%2520RAG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08727v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Plug-n-Play%20Knowledge%20Modules%20with%20Deep%20Context%20Distillation&entry.906535625=Lucas%20Caccia%20and%20Alan%20Ansell%20and%20Edoardo%20Ponti%20and%20Ivan%20Vuli%C4%87%20and%20Alessandro%20Sordoni&entry.1292438233=%20%20Dynamically%20integrating%20new%20or%20rapidly%20evolving%20information%20after%20%28Large%29%0ALanguage%20Model%20pre-training%20remains%20challenging%2C%20particularly%20in%20low-data%0Ascenarios%20or%20when%20dealing%20with%20private%20and%20specialized%20documents.%20In-context%0Alearning%20and%20retrieval-augmented%20generation%20%28RAG%29%20face%20limitations%2C%20including%0Atheir%20high%20inference%20costs%20and%20their%20inability%20to%20capture%20global%20document%0Ainformation.%20In%20this%20paper%2C%20we%20propose%20a%20way%20of%20modularizing%20knowledge%20by%0Atraining%20document-level%20Knowledge%20Modules%20%28KMs%29.%20KMs%20are%20lightweight%20components%0Aimplemented%20as%20parameter-efficient%20LoRA%20modules%2C%20which%20are%20trained%20to%20store%0Ainformation%20about%20new%20documents%20and%20can%20be%20easily%20plugged%20into%20models%20on%0Ademand.%20We%20show%20that%20next-token%20prediction%20performs%20poorly%20as%20the%20training%0Aobjective%20for%20KMs.%20We%20instead%20propose%20Deep%20Context%20Distillation%3A%20we%20learn%20KMs%0Aparameters%20such%20as%20to%20simulate%20hidden%20states%20and%20logits%20of%20a%20teacher%20that%20takes%0Athe%20document%20in%20context.%20Our%20method%20outperforms%20standard%20next-token%20prediction%0Aand%20pre-instruction%20training%20techniques%2C%20across%20two%20datasets.%20Finally%2C%20we%0Ahighlight%20synergies%20between%20KMs%20and%20RAG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.08727v2&entry.124074799=Read"},
{"title": "Wanda++: Pruning Large Language Models via Regional Gradients", "author": "Yifan Yang and Kai Zhen and Bhavana Ganesh and Aram Galstyan and Goeric Huybrechts and Markus M\u00fcller and Jonas M. K\u00fcbler and Rupak Vignesh Swaminathan and Athanasios Mouchtaris and Sravan Babu Bodapati and Nathan Susanj and Zheng Zhang and Jack FitzGerald and Abhishek Kumar", "abstract": "  Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal performance impact. However, existing methods\noften suffer from performance loss without full-model sparsity-aware\nfine-tuning. This paper presents Wanda++, a novel pruning framework that\noutperforms the state-of-the-art methods by utilizing decoder-block-level\n\\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score\nwith regional gradients for the first time and proposes an efficient regional\noptimization method to minimize pruning-induced output discrepancies between\nthe dense and sparse decoder output. Notably, Wanda++ improves perplexity by up\nto 32\\% over Wanda in the language modeling task and generalizes effectively to\ndownstream tasks. Further experiments indicate our proposed method is\northogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with\nLoRA fine-tuning to achieve a similar perplexity improvement as the Wanda\nmethod. The proposed method is lightweight, pruning a 7B LLaMA model in under\n10 minutes on a single NVIDIA H100 GPU.\n", "link": "http://arxiv.org/abs/2503.04992v2", "date": "2025-04-29", "relevancy": 1.9573, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wanda%2B%2B%3A%20Pruning%20Large%20Language%20Models%20via%20Regional%20Gradients&body=Title%3A%20Wanda%2B%2B%3A%20Pruning%20Large%20Language%20Models%20via%20Regional%20Gradients%0AAuthor%3A%20Yifan%20Yang%20and%20Kai%20Zhen%20and%20Bhavana%20Ganesh%20and%20Aram%20Galstyan%20and%20Goeric%20Huybrechts%20and%20Markus%20M%C3%BCller%20and%20Jonas%20M.%20K%C3%BCbler%20and%20Rupak%20Vignesh%20Swaminathan%20and%20Athanasios%20Mouchtaris%20and%20Sravan%20Babu%20Bodapati%20and%20Nathan%20Susanj%20and%20Zheng%20Zhang%20and%20Jack%20FitzGerald%20and%20Abhishek%20Kumar%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20pruning%20seeks%20to%20remove%20unimportant%20weights%20for%0Ainference%20speedup%20with%20minimal%20performance%20impact.%20However%2C%20existing%20methods%0Aoften%20suffer%20from%20performance%20loss%20without%20full-model%20sparsity-aware%0Afine-tuning.%20This%20paper%20presents%20Wanda%2B%2B%2C%20a%20novel%20pruning%20framework%20that%0Aoutperforms%20the%20state-of-the-art%20methods%20by%20utilizing%20decoder-block-level%0A%5Ctextbf%7Bregional%7D%20gradients.%20Specifically%2C%20Wanda%2B%2B%20improves%20the%20pruning%20score%0Awith%20regional%20gradients%20for%20the%20first%20time%20and%20proposes%20an%20efficient%20regional%0Aoptimization%20method%20to%20minimize%20pruning-induced%20output%20discrepancies%20between%0Athe%20dense%20and%20sparse%20decoder%20output.%20Notably%2C%20Wanda%2B%2B%20improves%20perplexity%20by%20up%0Ato%2032%5C%25%20over%20Wanda%20in%20the%20language%20modeling%20task%20and%20generalizes%20effectively%20to%0Adownstream%20tasks.%20Further%20experiments%20indicate%20our%20proposed%20method%20is%0Aorthogonal%20to%20sparsity-aware%20fine-tuning%2C%20where%20Wanda%2B%2B%20can%20be%20combined%20with%0ALoRA%20fine-tuning%20to%20achieve%20a%20similar%20perplexity%20improvement%20as%20the%20Wanda%0Amethod.%20The%20proposed%20method%20is%20lightweight%2C%20pruning%20a%207B%20LLaMA%20model%20in%20under%0A10%20minutes%20on%20a%20single%20NVIDIA%20H100%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.04992v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWanda%252B%252B%253A%2520Pruning%2520Large%2520Language%2520Models%2520via%2520Regional%2520Gradients%26entry.906535625%3DYifan%2520Yang%2520and%2520Kai%2520Zhen%2520and%2520Bhavana%2520Ganesh%2520and%2520Aram%2520Galstyan%2520and%2520Goeric%2520Huybrechts%2520and%2520Markus%2520M%25C3%25BCller%2520and%2520Jonas%2520M.%2520K%25C3%25BCbler%2520and%2520Rupak%2520Vignesh%2520Swaminathan%2520and%2520Athanasios%2520Mouchtaris%2520and%2520Sravan%2520Babu%2520Bodapati%2520and%2520Nathan%2520Susanj%2520and%2520Zheng%2520Zhang%2520and%2520Jack%2520FitzGerald%2520and%2520Abhishek%2520Kumar%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520pruning%2520seeks%2520to%2520remove%2520unimportant%2520weights%2520for%250Ainference%2520speedup%2520with%2520minimal%2520performance%2520impact.%2520However%252C%2520existing%2520methods%250Aoften%2520suffer%2520from%2520performance%2520loss%2520without%2520full-model%2520sparsity-aware%250Afine-tuning.%2520This%2520paper%2520presents%2520Wanda%252B%252B%252C%2520a%2520novel%2520pruning%2520framework%2520that%250Aoutperforms%2520the%2520state-of-the-art%2520methods%2520by%2520utilizing%2520decoder-block-level%250A%255Ctextbf%257Bregional%257D%2520gradients.%2520Specifically%252C%2520Wanda%252B%252B%2520improves%2520the%2520pruning%2520score%250Awith%2520regional%2520gradients%2520for%2520the%2520first%2520time%2520and%2520proposes%2520an%2520efficient%2520regional%250Aoptimization%2520method%2520to%2520minimize%2520pruning-induced%2520output%2520discrepancies%2520between%250Athe%2520dense%2520and%2520sparse%2520decoder%2520output.%2520Notably%252C%2520Wanda%252B%252B%2520improves%2520perplexity%2520by%2520up%250Ato%252032%255C%2525%2520over%2520Wanda%2520in%2520the%2520language%2520modeling%2520task%2520and%2520generalizes%2520effectively%2520to%250Adownstream%2520tasks.%2520Further%2520experiments%2520indicate%2520our%2520proposed%2520method%2520is%250Aorthogonal%2520to%2520sparsity-aware%2520fine-tuning%252C%2520where%2520Wanda%252B%252B%2520can%2520be%2520combined%2520with%250ALoRA%2520fine-tuning%2520to%2520achieve%2520a%2520similar%2520perplexity%2520improvement%2520as%2520the%2520Wanda%250Amethod.%2520The%2520proposed%2520method%2520is%2520lightweight%252C%2520pruning%2520a%25207B%2520LLaMA%2520model%2520in%2520under%250A10%2520minutes%2520on%2520a%2520single%2520NVIDIA%2520H100%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.04992v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wanda%2B%2B%3A%20Pruning%20Large%20Language%20Models%20via%20Regional%20Gradients&entry.906535625=Yifan%20Yang%20and%20Kai%20Zhen%20and%20Bhavana%20Ganesh%20and%20Aram%20Galstyan%20and%20Goeric%20Huybrechts%20and%20Markus%20M%C3%BCller%20and%20Jonas%20M.%20K%C3%BCbler%20and%20Rupak%20Vignesh%20Swaminathan%20and%20Athanasios%20Mouchtaris%20and%20Sravan%20Babu%20Bodapati%20and%20Nathan%20Susanj%20and%20Zheng%20Zhang%20and%20Jack%20FitzGerald%20and%20Abhishek%20Kumar&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20pruning%20seeks%20to%20remove%20unimportant%20weights%20for%0Ainference%20speedup%20with%20minimal%20performance%20impact.%20However%2C%20existing%20methods%0Aoften%20suffer%20from%20performance%20loss%20without%20full-model%20sparsity-aware%0Afine-tuning.%20This%20paper%20presents%20Wanda%2B%2B%2C%20a%20novel%20pruning%20framework%20that%0Aoutperforms%20the%20state-of-the-art%20methods%20by%20utilizing%20decoder-block-level%0A%5Ctextbf%7Bregional%7D%20gradients.%20Specifically%2C%20Wanda%2B%2B%20improves%20the%20pruning%20score%0Awith%20regional%20gradients%20for%20the%20first%20time%20and%20proposes%20an%20efficient%20regional%0Aoptimization%20method%20to%20minimize%20pruning-induced%20output%20discrepancies%20between%0Athe%20dense%20and%20sparse%20decoder%20output.%20Notably%2C%20Wanda%2B%2B%20improves%20perplexity%20by%20up%0Ato%2032%5C%25%20over%20Wanda%20in%20the%20language%20modeling%20task%20and%20generalizes%20effectively%20to%0Adownstream%20tasks.%20Further%20experiments%20indicate%20our%20proposed%20method%20is%0Aorthogonal%20to%20sparsity-aware%20fine-tuning%2C%20where%20Wanda%2B%2B%20can%20be%20combined%20with%0ALoRA%20fine-tuning%20to%20achieve%20a%20similar%20perplexity%20improvement%20as%20the%20Wanda%0Amethod.%20The%20proposed%20method%20is%20lightweight%2C%20pruning%20a%207B%20LLaMA%20model%20in%20under%0A10%20minutes%20on%20a%20single%20NVIDIA%20H100%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.04992v2&entry.124074799=Read"},
{"title": "Test-time regression: a unifying framework for designing sequence models\n  with associative memory", "author": "Ke Alexander Wang and Jiaxin Shi and Emily B. Fox", "abstract": "  Sequence models lie at the heart of modern deep learning. However, rapid\nadvancements have produced a diversity of seemingly unrelated architectures,\nsuch as Transformers and recurrent alternatives. In this paper, we introduce a\nunifying framework to understand and derive these sequence models, inspired by\nthe empirical importance of associative recall, the capability to retrieve\ncontextually relevant tokens. We formalize associative recall as a two-step\nprocess, memorization and retrieval, casting memorization as a regression\nproblem. Layers that combine these two steps perform associative recall via\n``test-time regression'' over its input tokens. Prominent layers, including\nlinear attention, state-space models, fast-weight programmers, online learners,\nand softmax attention, arise as special cases defined by three design choices:\nthe regression weights, the regressor function class, and the test-time\noptimization algorithm. Our approach clarifies how linear attention fails to\ncapture inter-token correlations and offers a mathematical justification for\nthe empirical effectiveness of query-key normalization in softmax attention.\nFurther, it illuminates unexplored regions within the design space, which we\nuse to derive novel higher-order generalizations of softmax attention. Beyond\nunification, our work bridges sequence modeling with classic regression\nmethods, a field with extensive literature, paving the way for developing more\npowerful and theoretically principled architectures.\n", "link": "http://arxiv.org/abs/2501.12352v2", "date": "2025-04-29", "relevancy": 1.9559, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-time%20regression%3A%20a%20unifying%20framework%20for%20designing%20sequence%20models%0A%20%20with%20associative%20memory&body=Title%3A%20Test-time%20regression%3A%20a%20unifying%20framework%20for%20designing%20sequence%20models%0A%20%20with%20associative%20memory%0AAuthor%3A%20Ke%20Alexander%20Wang%20and%20Jiaxin%20Shi%20and%20Emily%20B.%20Fox%0AAbstract%3A%20%20%20Sequence%20models%20lie%20at%20the%20heart%20of%20modern%20deep%20learning.%20However%2C%20rapid%0Aadvancements%20have%20produced%20a%20diversity%20of%20seemingly%20unrelated%20architectures%2C%0Asuch%20as%20Transformers%20and%20recurrent%20alternatives.%20In%20this%20paper%2C%20we%20introduce%20a%0Aunifying%20framework%20to%20understand%20and%20derive%20these%20sequence%20models%2C%20inspired%20by%0Athe%20empirical%20importance%20of%20associative%20recall%2C%20the%20capability%20to%20retrieve%0Acontextually%20relevant%20tokens.%20We%20formalize%20associative%20recall%20as%20a%20two-step%0Aprocess%2C%20memorization%20and%20retrieval%2C%20casting%20memorization%20as%20a%20regression%0Aproblem.%20Layers%20that%20combine%20these%20two%20steps%20perform%20associative%20recall%20via%0A%60%60test-time%20regression%27%27%20over%20its%20input%20tokens.%20Prominent%20layers%2C%20including%0Alinear%20attention%2C%20state-space%20models%2C%20fast-weight%20programmers%2C%20online%20learners%2C%0Aand%20softmax%20attention%2C%20arise%20as%20special%20cases%20defined%20by%20three%20design%20choices%3A%0Athe%20regression%20weights%2C%20the%20regressor%20function%20class%2C%20and%20the%20test-time%0Aoptimization%20algorithm.%20Our%20approach%20clarifies%20how%20linear%20attention%20fails%20to%0Acapture%20inter-token%20correlations%20and%20offers%20a%20mathematical%20justification%20for%0Athe%20empirical%20effectiveness%20of%20query-key%20normalization%20in%20softmax%20attention.%0AFurther%2C%20it%20illuminates%20unexplored%20regions%20within%20the%20design%20space%2C%20which%20we%0Ause%20to%20derive%20novel%20higher-order%20generalizations%20of%20softmax%20attention.%20Beyond%0Aunification%2C%20our%20work%20bridges%20sequence%20modeling%20with%20classic%20regression%0Amethods%2C%20a%20field%20with%20extensive%20literature%2C%20paving%20the%20way%20for%20developing%20more%0Apowerful%20and%20theoretically%20principled%20architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.12352v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-time%2520regression%253A%2520a%2520unifying%2520framework%2520for%2520designing%2520sequence%2520models%250A%2520%2520with%2520associative%2520memory%26entry.906535625%3DKe%2520Alexander%2520Wang%2520and%2520Jiaxin%2520Shi%2520and%2520Emily%2520B.%2520Fox%26entry.1292438233%3D%2520%2520Sequence%2520models%2520lie%2520at%2520the%2520heart%2520of%2520modern%2520deep%2520learning.%2520However%252C%2520rapid%250Aadvancements%2520have%2520produced%2520a%2520diversity%2520of%2520seemingly%2520unrelated%2520architectures%252C%250Asuch%2520as%2520Transformers%2520and%2520recurrent%2520alternatives.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%250Aunifying%2520framework%2520to%2520understand%2520and%2520derive%2520these%2520sequence%2520models%252C%2520inspired%2520by%250Athe%2520empirical%2520importance%2520of%2520associative%2520recall%252C%2520the%2520capability%2520to%2520retrieve%250Acontextually%2520relevant%2520tokens.%2520We%2520formalize%2520associative%2520recall%2520as%2520a%2520two-step%250Aprocess%252C%2520memorization%2520and%2520retrieval%252C%2520casting%2520memorization%2520as%2520a%2520regression%250Aproblem.%2520Layers%2520that%2520combine%2520these%2520two%2520steps%2520perform%2520associative%2520recall%2520via%250A%2560%2560test-time%2520regression%2527%2527%2520over%2520its%2520input%2520tokens.%2520Prominent%2520layers%252C%2520including%250Alinear%2520attention%252C%2520state-space%2520models%252C%2520fast-weight%2520programmers%252C%2520online%2520learners%252C%250Aand%2520softmax%2520attention%252C%2520arise%2520as%2520special%2520cases%2520defined%2520by%2520three%2520design%2520choices%253A%250Athe%2520regression%2520weights%252C%2520the%2520regressor%2520function%2520class%252C%2520and%2520the%2520test-time%250Aoptimization%2520algorithm.%2520Our%2520approach%2520clarifies%2520how%2520linear%2520attention%2520fails%2520to%250Acapture%2520inter-token%2520correlations%2520and%2520offers%2520a%2520mathematical%2520justification%2520for%250Athe%2520empirical%2520effectiveness%2520of%2520query-key%2520normalization%2520in%2520softmax%2520attention.%250AFurther%252C%2520it%2520illuminates%2520unexplored%2520regions%2520within%2520the%2520design%2520space%252C%2520which%2520we%250Ause%2520to%2520derive%2520novel%2520higher-order%2520generalizations%2520of%2520softmax%2520attention.%2520Beyond%250Aunification%252C%2520our%2520work%2520bridges%2520sequence%2520modeling%2520with%2520classic%2520regression%250Amethods%252C%2520a%2520field%2520with%2520extensive%2520literature%252C%2520paving%2520the%2520way%2520for%2520developing%2520more%250Apowerful%2520and%2520theoretically%2520principled%2520architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.12352v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-time%20regression%3A%20a%20unifying%20framework%20for%20designing%20sequence%20models%0A%20%20with%20associative%20memory&entry.906535625=Ke%20Alexander%20Wang%20and%20Jiaxin%20Shi%20and%20Emily%20B.%20Fox&entry.1292438233=%20%20Sequence%20models%20lie%20at%20the%20heart%20of%20modern%20deep%20learning.%20However%2C%20rapid%0Aadvancements%20have%20produced%20a%20diversity%20of%20seemingly%20unrelated%20architectures%2C%0Asuch%20as%20Transformers%20and%20recurrent%20alternatives.%20In%20this%20paper%2C%20we%20introduce%20a%0Aunifying%20framework%20to%20understand%20and%20derive%20these%20sequence%20models%2C%20inspired%20by%0Athe%20empirical%20importance%20of%20associative%20recall%2C%20the%20capability%20to%20retrieve%0Acontextually%20relevant%20tokens.%20We%20formalize%20associative%20recall%20as%20a%20two-step%0Aprocess%2C%20memorization%20and%20retrieval%2C%20casting%20memorization%20as%20a%20regression%0Aproblem.%20Layers%20that%20combine%20these%20two%20steps%20perform%20associative%20recall%20via%0A%60%60test-time%20regression%27%27%20over%20its%20input%20tokens.%20Prominent%20layers%2C%20including%0Alinear%20attention%2C%20state-space%20models%2C%20fast-weight%20programmers%2C%20online%20learners%2C%0Aand%20softmax%20attention%2C%20arise%20as%20special%20cases%20defined%20by%20three%20design%20choices%3A%0Athe%20regression%20weights%2C%20the%20regressor%20function%20class%2C%20and%20the%20test-time%0Aoptimization%20algorithm.%20Our%20approach%20clarifies%20how%20linear%20attention%20fails%20to%0Acapture%20inter-token%20correlations%20and%20offers%20a%20mathematical%20justification%20for%0Athe%20empirical%20effectiveness%20of%20query-key%20normalization%20in%20softmax%20attention.%0AFurther%2C%20it%20illuminates%20unexplored%20regions%20within%20the%20design%20space%2C%20which%20we%0Ause%20to%20derive%20novel%20higher-order%20generalizations%20of%20softmax%20attention.%20Beyond%0Aunification%2C%20our%20work%20bridges%20sequence%20modeling%20with%20classic%20regression%0Amethods%2C%20a%20field%20with%20extensive%20literature%2C%20paving%20the%20way%20for%20developing%20more%0Apowerful%20and%20theoretically%20principled%20architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.12352v2&entry.124074799=Read"},
{"title": "Preference-centric Bandits: Optimality of Mixtures and Regret-efficient\n  Algorithms", "author": "Meltem Tatl\u0131 and Arpan Mukherjee and Prashanth L. A. and Karthikeyan Shanmugam and Ali Tajer", "abstract": "  The objective of canonical multi-armed bandits is to identify and repeatedly\nselect an arm with the largest reward, often in the form of the expected value\nof the arm's probability distribution. Such a utilitarian perspective and focus\non the probability models' first moments, however, is agnostic to the\ndistributions' tail behavior and their implications for variability and risks\nin decision-making. This paper introduces a principled framework for shifting\nfrom expectation-based evaluation to an alternative reward formulation, termed\na preference metric (PM). The PMs can place the desired emphasis on different\nreward realization and can encode a richer modeling of preferences that\nincorporate risk aversion, robustness, or other desired attitudes toward\nuncertainty. A fundamentally distinct observation in such a PM-centric\nperspective is that designing bandit algorithms will have a significantly\ndifferent principle: as opposed to the reward-based models in which the optimal\nsampling policy converges to repeatedly sampling from the single best arm, in\nthe PM-centric framework the optimal policy converges to selecting a mix of\narms based on specific mixing weights. Designing such mixture policies departs\nfrom the principles for designing bandit algorithms in significant ways,\nprimarily because of uncountable mixture possibilities. The paper formalizes\nthe PM-centric framework and presents two algorithm classes (horizon-dependent\nand anytime) that learn and track mixtures in a regret-efficient fashion. These\nalgorithms have two distinctions from their canonical counterparts: (i) they\ninvolve an estimation routine to form reliable estimates of optimal mixtures,\nand (ii) they are equipped with tracking mechanisms to navigate arm selection\nfractions to track the optimal mixtures. These algorithms' regret guarantees\nare investigated under various algebraic forms of the PMs.\n", "link": "http://arxiv.org/abs/2504.20877v1", "date": "2025-04-29", "relevancy": 1.9422, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5299}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4915}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Preference-centric%20Bandits%3A%20Optimality%20of%20Mixtures%20and%20Regret-efficient%0A%20%20Algorithms&body=Title%3A%20Preference-centric%20Bandits%3A%20Optimality%20of%20Mixtures%20and%20Regret-efficient%0A%20%20Algorithms%0AAuthor%3A%20Meltem%20Tatl%C4%B1%20and%20Arpan%20Mukherjee%20and%20Prashanth%20L.%20A.%20and%20Karthikeyan%20Shanmugam%20and%20Ali%20Tajer%0AAbstract%3A%20%20%20The%20objective%20of%20canonical%20multi-armed%20bandits%20is%20to%20identify%20and%20repeatedly%0Aselect%20an%20arm%20with%20the%20largest%20reward%2C%20often%20in%20the%20form%20of%20the%20expected%20value%0Aof%20the%20arm%27s%20probability%20distribution.%20Such%20a%20utilitarian%20perspective%20and%20focus%0Aon%20the%20probability%20models%27%20first%20moments%2C%20however%2C%20is%20agnostic%20to%20the%0Adistributions%27%20tail%20behavior%20and%20their%20implications%20for%20variability%20and%20risks%0Ain%20decision-making.%20This%20paper%20introduces%20a%20principled%20framework%20for%20shifting%0Afrom%20expectation-based%20evaluation%20to%20an%20alternative%20reward%20formulation%2C%20termed%0Aa%20preference%20metric%20%28PM%29.%20The%20PMs%20can%20place%20the%20desired%20emphasis%20on%20different%0Areward%20realization%20and%20can%20encode%20a%20richer%20modeling%20of%20preferences%20that%0Aincorporate%20risk%20aversion%2C%20robustness%2C%20or%20other%20desired%20attitudes%20toward%0Auncertainty.%20A%20fundamentally%20distinct%20observation%20in%20such%20a%20PM-centric%0Aperspective%20is%20that%20designing%20bandit%20algorithms%20will%20have%20a%20significantly%0Adifferent%20principle%3A%20as%20opposed%20to%20the%20reward-based%20models%20in%20which%20the%20optimal%0Asampling%20policy%20converges%20to%20repeatedly%20sampling%20from%20the%20single%20best%20arm%2C%20in%0Athe%20PM-centric%20framework%20the%20optimal%20policy%20converges%20to%20selecting%20a%20mix%20of%0Aarms%20based%20on%20specific%20mixing%20weights.%20Designing%20such%20mixture%20policies%20departs%0Afrom%20the%20principles%20for%20designing%20bandit%20algorithms%20in%20significant%20ways%2C%0Aprimarily%20because%20of%20uncountable%20mixture%20possibilities.%20The%20paper%20formalizes%0Athe%20PM-centric%20framework%20and%20presents%20two%20algorithm%20classes%20%28horizon-dependent%0Aand%20anytime%29%20that%20learn%20and%20track%20mixtures%20in%20a%20regret-efficient%20fashion.%20These%0Aalgorithms%20have%20two%20distinctions%20from%20their%20canonical%20counterparts%3A%20%28i%29%20they%0Ainvolve%20an%20estimation%20routine%20to%20form%20reliable%20estimates%20of%20optimal%20mixtures%2C%0Aand%20%28ii%29%20they%20are%20equipped%20with%20tracking%20mechanisms%20to%20navigate%20arm%20selection%0Afractions%20to%20track%20the%20optimal%20mixtures.%20These%20algorithms%27%20regret%20guarantees%0Aare%20investigated%20under%20various%20algebraic%20forms%20of%20the%20PMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20877v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPreference-centric%2520Bandits%253A%2520Optimality%2520of%2520Mixtures%2520and%2520Regret-efficient%250A%2520%2520Algorithms%26entry.906535625%3DMeltem%2520Tatl%25C4%25B1%2520and%2520Arpan%2520Mukherjee%2520and%2520Prashanth%2520L.%2520A.%2520and%2520Karthikeyan%2520Shanmugam%2520and%2520Ali%2520Tajer%26entry.1292438233%3D%2520%2520The%2520objective%2520of%2520canonical%2520multi-armed%2520bandits%2520is%2520to%2520identify%2520and%2520repeatedly%250Aselect%2520an%2520arm%2520with%2520the%2520largest%2520reward%252C%2520often%2520in%2520the%2520form%2520of%2520the%2520expected%2520value%250Aof%2520the%2520arm%2527s%2520probability%2520distribution.%2520Such%2520a%2520utilitarian%2520perspective%2520and%2520focus%250Aon%2520the%2520probability%2520models%2527%2520first%2520moments%252C%2520however%252C%2520is%2520agnostic%2520to%2520the%250Adistributions%2527%2520tail%2520behavior%2520and%2520their%2520implications%2520for%2520variability%2520and%2520risks%250Ain%2520decision-making.%2520This%2520paper%2520introduces%2520a%2520principled%2520framework%2520for%2520shifting%250Afrom%2520expectation-based%2520evaluation%2520to%2520an%2520alternative%2520reward%2520formulation%252C%2520termed%250Aa%2520preference%2520metric%2520%2528PM%2529.%2520The%2520PMs%2520can%2520place%2520the%2520desired%2520emphasis%2520on%2520different%250Areward%2520realization%2520and%2520can%2520encode%2520a%2520richer%2520modeling%2520of%2520preferences%2520that%250Aincorporate%2520risk%2520aversion%252C%2520robustness%252C%2520or%2520other%2520desired%2520attitudes%2520toward%250Auncertainty.%2520A%2520fundamentally%2520distinct%2520observation%2520in%2520such%2520a%2520PM-centric%250Aperspective%2520is%2520that%2520designing%2520bandit%2520algorithms%2520will%2520have%2520a%2520significantly%250Adifferent%2520principle%253A%2520as%2520opposed%2520to%2520the%2520reward-based%2520models%2520in%2520which%2520the%2520optimal%250Asampling%2520policy%2520converges%2520to%2520repeatedly%2520sampling%2520from%2520the%2520single%2520best%2520arm%252C%2520in%250Athe%2520PM-centric%2520framework%2520the%2520optimal%2520policy%2520converges%2520to%2520selecting%2520a%2520mix%2520of%250Aarms%2520based%2520on%2520specific%2520mixing%2520weights.%2520Designing%2520such%2520mixture%2520policies%2520departs%250Afrom%2520the%2520principles%2520for%2520designing%2520bandit%2520algorithms%2520in%2520significant%2520ways%252C%250Aprimarily%2520because%2520of%2520uncountable%2520mixture%2520possibilities.%2520The%2520paper%2520formalizes%250Athe%2520PM-centric%2520framework%2520and%2520presents%2520two%2520algorithm%2520classes%2520%2528horizon-dependent%250Aand%2520anytime%2529%2520that%2520learn%2520and%2520track%2520mixtures%2520in%2520a%2520regret-efficient%2520fashion.%2520These%250Aalgorithms%2520have%2520two%2520distinctions%2520from%2520their%2520canonical%2520counterparts%253A%2520%2528i%2529%2520they%250Ainvolve%2520an%2520estimation%2520routine%2520to%2520form%2520reliable%2520estimates%2520of%2520optimal%2520mixtures%252C%250Aand%2520%2528ii%2529%2520they%2520are%2520equipped%2520with%2520tracking%2520mechanisms%2520to%2520navigate%2520arm%2520selection%250Afractions%2520to%2520track%2520the%2520optimal%2520mixtures.%2520These%2520algorithms%2527%2520regret%2520guarantees%250Aare%2520investigated%2520under%2520various%2520algebraic%2520forms%2520of%2520the%2520PMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20877v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Preference-centric%20Bandits%3A%20Optimality%20of%20Mixtures%20and%20Regret-efficient%0A%20%20Algorithms&entry.906535625=Meltem%20Tatl%C4%B1%20and%20Arpan%20Mukherjee%20and%20Prashanth%20L.%20A.%20and%20Karthikeyan%20Shanmugam%20and%20Ali%20Tajer&entry.1292438233=%20%20The%20objective%20of%20canonical%20multi-armed%20bandits%20is%20to%20identify%20and%20repeatedly%0Aselect%20an%20arm%20with%20the%20largest%20reward%2C%20often%20in%20the%20form%20of%20the%20expected%20value%0Aof%20the%20arm%27s%20probability%20distribution.%20Such%20a%20utilitarian%20perspective%20and%20focus%0Aon%20the%20probability%20models%27%20first%20moments%2C%20however%2C%20is%20agnostic%20to%20the%0Adistributions%27%20tail%20behavior%20and%20their%20implications%20for%20variability%20and%20risks%0Ain%20decision-making.%20This%20paper%20introduces%20a%20principled%20framework%20for%20shifting%0Afrom%20expectation-based%20evaluation%20to%20an%20alternative%20reward%20formulation%2C%20termed%0Aa%20preference%20metric%20%28PM%29.%20The%20PMs%20can%20place%20the%20desired%20emphasis%20on%20different%0Areward%20realization%20and%20can%20encode%20a%20richer%20modeling%20of%20preferences%20that%0Aincorporate%20risk%20aversion%2C%20robustness%2C%20or%20other%20desired%20attitudes%20toward%0Auncertainty.%20A%20fundamentally%20distinct%20observation%20in%20such%20a%20PM-centric%0Aperspective%20is%20that%20designing%20bandit%20algorithms%20will%20have%20a%20significantly%0Adifferent%20principle%3A%20as%20opposed%20to%20the%20reward-based%20models%20in%20which%20the%20optimal%0Asampling%20policy%20converges%20to%20repeatedly%20sampling%20from%20the%20single%20best%20arm%2C%20in%0Athe%20PM-centric%20framework%20the%20optimal%20policy%20converges%20to%20selecting%20a%20mix%20of%0Aarms%20based%20on%20specific%20mixing%20weights.%20Designing%20such%20mixture%20policies%20departs%0Afrom%20the%20principles%20for%20designing%20bandit%20algorithms%20in%20significant%20ways%2C%0Aprimarily%20because%20of%20uncountable%20mixture%20possibilities.%20The%20paper%20formalizes%0Athe%20PM-centric%20framework%20and%20presents%20two%20algorithm%20classes%20%28horizon-dependent%0Aand%20anytime%29%20that%20learn%20and%20track%20mixtures%20in%20a%20regret-efficient%20fashion.%20These%0Aalgorithms%20have%20two%20distinctions%20from%20their%20canonical%20counterparts%3A%20%28i%29%20they%0Ainvolve%20an%20estimation%20routine%20to%20form%20reliable%20estimates%20of%20optimal%20mixtures%2C%0Aand%20%28ii%29%20they%20are%20equipped%20with%20tracking%20mechanisms%20to%20navigate%20arm%20selection%0Afractions%20to%20track%20the%20optimal%20mixtures.%20These%20algorithms%27%20regret%20guarantees%0Aare%20investigated%20under%20various%20algebraic%20forms%20of%20the%20PMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20877v1&entry.124074799=Read"},
{"title": "Leveraging Generative AI Through Prompt Engineering and Rigorous\n  Validation to Create Comprehensive Synthetic Datasets for AI Training in\n  Healthcare", "author": "Polycarp Nalela", "abstract": "  Access to high-quality medical data is often restricted due to privacy\nconcerns, posing significant challenges for training artificial intelligence\n(AI) algorithms within Electronic Health Record (EHR) applications. In this\nstudy, prompt engineering with the GPT-4 API was employed to generate\nhigh-quality synthetic datasets aimed at overcoming this limitation. The\ngenerated data encompassed a comprehensive array of patient admission\ninformation, including healthcare provider details, hospital departments,\nwards, bed assignments, patient demographics, emergency contacts, vital signs,\nimmunizations, allergies, medical histories, appointments, hospital visits,\nlaboratory tests, diagnoses, treatment plans, medications, clinical notes,\nvisit logs, discharge summaries, and referrals. To ensure data quality and\nintegrity, advanced validation techniques were implemented utilizing models\nsuch as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for\noverall plausibility, RoBERTa for logical consistency, autoencoders for anomaly\ndetection, and conducted diversity analysis. Synthetic data that met all\nvalidation criteria were integrated into a comprehensive PostgreSQL database,\nserving as the data management system for the EHR application. This approach\ndemonstrates that leveraging generative AI models with rigorous validation can\neffectively produce high-quality synthetic medical data, facilitating the\ntraining of AI algorithms while addressing privacy concerns associated with\nreal patient data.\n", "link": "http://arxiv.org/abs/2504.20921v1", "date": "2025-04-29", "relevancy": 1.9415, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5008}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.478}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Generative%20AI%20Through%20Prompt%20Engineering%20and%20Rigorous%0A%20%20Validation%20to%20Create%20Comprehensive%20Synthetic%20Datasets%20for%20AI%20Training%20in%0A%20%20Healthcare&body=Title%3A%20Leveraging%20Generative%20AI%20Through%20Prompt%20Engineering%20and%20Rigorous%0A%20%20Validation%20to%20Create%20Comprehensive%20Synthetic%20Datasets%20for%20AI%20Training%20in%0A%20%20Healthcare%0AAuthor%3A%20Polycarp%20Nalela%0AAbstract%3A%20%20%20Access%20to%20high-quality%20medical%20data%20is%20often%20restricted%20due%20to%20privacy%0Aconcerns%2C%20posing%20significant%20challenges%20for%20training%20artificial%20intelligence%0A%28AI%29%20algorithms%20within%20Electronic%20Health%20Record%20%28EHR%29%20applications.%20In%20this%0Astudy%2C%20prompt%20engineering%20with%20the%20GPT-4%20API%20was%20employed%20to%20generate%0Ahigh-quality%20synthetic%20datasets%20aimed%20at%20overcoming%20this%20limitation.%20The%0Agenerated%20data%20encompassed%20a%20comprehensive%20array%20of%20patient%20admission%0Ainformation%2C%20including%20healthcare%20provider%20details%2C%20hospital%20departments%2C%0Awards%2C%20bed%20assignments%2C%20patient%20demographics%2C%20emergency%20contacts%2C%20vital%20signs%2C%0Aimmunizations%2C%20allergies%2C%20medical%20histories%2C%20appointments%2C%20hospital%20visits%2C%0Alaboratory%20tests%2C%20diagnoses%2C%20treatment%20plans%2C%20medications%2C%20clinical%20notes%2C%0Avisit%20logs%2C%20discharge%20summaries%2C%20and%20referrals.%20To%20ensure%20data%20quality%20and%0Aintegrity%2C%20advanced%20validation%20techniques%20were%20implemented%20utilizing%20models%0Asuch%20as%20BERT%27s%20Next%20Sentence%20Prediction%20for%20sentence%20coherence%2C%20GPT-2%20for%0Aoverall%20plausibility%2C%20RoBERTa%20for%20logical%20consistency%2C%20autoencoders%20for%20anomaly%0Adetection%2C%20and%20conducted%20diversity%20analysis.%20Synthetic%20data%20that%20met%20all%0Avalidation%20criteria%20were%20integrated%20into%20a%20comprehensive%20PostgreSQL%20database%2C%0Aserving%20as%20the%20data%20management%20system%20for%20the%20EHR%20application.%20This%20approach%0Ademonstrates%20that%20leveraging%20generative%20AI%20models%20with%20rigorous%20validation%20can%0Aeffectively%20produce%20high-quality%20synthetic%20medical%20data%2C%20facilitating%20the%0Atraining%20of%20AI%20algorithms%20while%20addressing%20privacy%20concerns%20associated%20with%0Areal%20patient%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Generative%2520AI%2520Through%2520Prompt%2520Engineering%2520and%2520Rigorous%250A%2520%2520Validation%2520to%2520Create%2520Comprehensive%2520Synthetic%2520Datasets%2520for%2520AI%2520Training%2520in%250A%2520%2520Healthcare%26entry.906535625%3DPolycarp%2520Nalela%26entry.1292438233%3D%2520%2520Access%2520to%2520high-quality%2520medical%2520data%2520is%2520often%2520restricted%2520due%2520to%2520privacy%250Aconcerns%252C%2520posing%2520significant%2520challenges%2520for%2520training%2520artificial%2520intelligence%250A%2528AI%2529%2520algorithms%2520within%2520Electronic%2520Health%2520Record%2520%2528EHR%2529%2520applications.%2520In%2520this%250Astudy%252C%2520prompt%2520engineering%2520with%2520the%2520GPT-4%2520API%2520was%2520employed%2520to%2520generate%250Ahigh-quality%2520synthetic%2520datasets%2520aimed%2520at%2520overcoming%2520this%2520limitation.%2520The%250Agenerated%2520data%2520encompassed%2520a%2520comprehensive%2520array%2520of%2520patient%2520admission%250Ainformation%252C%2520including%2520healthcare%2520provider%2520details%252C%2520hospital%2520departments%252C%250Awards%252C%2520bed%2520assignments%252C%2520patient%2520demographics%252C%2520emergency%2520contacts%252C%2520vital%2520signs%252C%250Aimmunizations%252C%2520allergies%252C%2520medical%2520histories%252C%2520appointments%252C%2520hospital%2520visits%252C%250Alaboratory%2520tests%252C%2520diagnoses%252C%2520treatment%2520plans%252C%2520medications%252C%2520clinical%2520notes%252C%250Avisit%2520logs%252C%2520discharge%2520summaries%252C%2520and%2520referrals.%2520To%2520ensure%2520data%2520quality%2520and%250Aintegrity%252C%2520advanced%2520validation%2520techniques%2520were%2520implemented%2520utilizing%2520models%250Asuch%2520as%2520BERT%2527s%2520Next%2520Sentence%2520Prediction%2520for%2520sentence%2520coherence%252C%2520GPT-2%2520for%250Aoverall%2520plausibility%252C%2520RoBERTa%2520for%2520logical%2520consistency%252C%2520autoencoders%2520for%2520anomaly%250Adetection%252C%2520and%2520conducted%2520diversity%2520analysis.%2520Synthetic%2520data%2520that%2520met%2520all%250Avalidation%2520criteria%2520were%2520integrated%2520into%2520a%2520comprehensive%2520PostgreSQL%2520database%252C%250Aserving%2520as%2520the%2520data%2520management%2520system%2520for%2520the%2520EHR%2520application.%2520This%2520approach%250Ademonstrates%2520that%2520leveraging%2520generative%2520AI%2520models%2520with%2520rigorous%2520validation%2520can%250Aeffectively%2520produce%2520high-quality%2520synthetic%2520medical%2520data%252C%2520facilitating%2520the%250Atraining%2520of%2520AI%2520algorithms%2520while%2520addressing%2520privacy%2520concerns%2520associated%2520with%250Areal%2520patient%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Generative%20AI%20Through%20Prompt%20Engineering%20and%20Rigorous%0A%20%20Validation%20to%20Create%20Comprehensive%20Synthetic%20Datasets%20for%20AI%20Training%20in%0A%20%20Healthcare&entry.906535625=Polycarp%20Nalela&entry.1292438233=%20%20Access%20to%20high-quality%20medical%20data%20is%20often%20restricted%20due%20to%20privacy%0Aconcerns%2C%20posing%20significant%20challenges%20for%20training%20artificial%20intelligence%0A%28AI%29%20algorithms%20within%20Electronic%20Health%20Record%20%28EHR%29%20applications.%20In%20this%0Astudy%2C%20prompt%20engineering%20with%20the%20GPT-4%20API%20was%20employed%20to%20generate%0Ahigh-quality%20synthetic%20datasets%20aimed%20at%20overcoming%20this%20limitation.%20The%0Agenerated%20data%20encompassed%20a%20comprehensive%20array%20of%20patient%20admission%0Ainformation%2C%20including%20healthcare%20provider%20details%2C%20hospital%20departments%2C%0Awards%2C%20bed%20assignments%2C%20patient%20demographics%2C%20emergency%20contacts%2C%20vital%20signs%2C%0Aimmunizations%2C%20allergies%2C%20medical%20histories%2C%20appointments%2C%20hospital%20visits%2C%0Alaboratory%20tests%2C%20diagnoses%2C%20treatment%20plans%2C%20medications%2C%20clinical%20notes%2C%0Avisit%20logs%2C%20discharge%20summaries%2C%20and%20referrals.%20To%20ensure%20data%20quality%20and%0Aintegrity%2C%20advanced%20validation%20techniques%20were%20implemented%20utilizing%20models%0Asuch%20as%20BERT%27s%20Next%20Sentence%20Prediction%20for%20sentence%20coherence%2C%20GPT-2%20for%0Aoverall%20plausibility%2C%20RoBERTa%20for%20logical%20consistency%2C%20autoencoders%20for%20anomaly%0Adetection%2C%20and%20conducted%20diversity%20analysis.%20Synthetic%20data%20that%20met%20all%0Avalidation%20criteria%20were%20integrated%20into%20a%20comprehensive%20PostgreSQL%20database%2C%0Aserving%20as%20the%20data%20management%20system%20for%20the%20EHR%20application.%20This%20approach%0Ademonstrates%20that%20leveraging%20generative%20AI%20models%20with%20rigorous%20validation%20can%0Aeffectively%20produce%20high-quality%20synthetic%20medical%20data%2C%20facilitating%20the%0Atraining%20of%20AI%20algorithms%20while%20addressing%20privacy%20concerns%20associated%20with%0Areal%20patient%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20921v1&entry.124074799=Read"},
{"title": "Equivariant non-linear maps for neural networks on homogeneous spaces", "author": "Elias Nyholm and Oscar Carlsson and Maurice Weiler and Daniel Persson", "abstract": "  This paper presents a novel framework for non-linear equivariant neural\nnetwork layers on homogeneous spaces. The seminal work of Cohen et al. on\nequivariant $G$-CNNs on homogeneous spaces characterized the representation\ntheory of such layers in the linear setting, finding that they are given by\nconvolutions with kernels satisfying so-called steerability constraints.\nMotivated by the empirical success of non-linear layers, such as self-attention\nor input dependent kernels, we set out to generalize these insights to the\nnon-linear setting. We derive generalized steerability constraints that any\nsuch layer needs to satisfy and prove the universality of our construction. The\ninsights gained into the symmetry-constrained functional dependence of\nequivariant operators on feature maps and group elements informs the design of\nfuture equivariant neural network layers. We demonstrate how several common\nequivariant network architectures - $G$-CNNs, implicit steerable kernel\nnetworks, conventional and relative position embedded attention based\ntransformers, and LieTransformers - may be derived from our framework.\n", "link": "http://arxiv.org/abs/2504.20974v1", "date": "2025-04-29", "relevancy": 1.8982, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4863}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4694}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20non-linear%20maps%20for%20neural%20networks%20on%20homogeneous%20spaces&body=Title%3A%20Equivariant%20non-linear%20maps%20for%20neural%20networks%20on%20homogeneous%20spaces%0AAuthor%3A%20Elias%20Nyholm%20and%20Oscar%20Carlsson%20and%20Maurice%20Weiler%20and%20Daniel%20Persson%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20framework%20for%20non-linear%20equivariant%20neural%0Anetwork%20layers%20on%20homogeneous%20spaces.%20The%20seminal%20work%20of%20Cohen%20et%20al.%20on%0Aequivariant%20%24G%24-CNNs%20on%20homogeneous%20spaces%20characterized%20the%20representation%0Atheory%20of%20such%20layers%20in%20the%20linear%20setting%2C%20finding%20that%20they%20are%20given%20by%0Aconvolutions%20with%20kernels%20satisfying%20so-called%20steerability%20constraints.%0AMotivated%20by%20the%20empirical%20success%20of%20non-linear%20layers%2C%20such%20as%20self-attention%0Aor%20input%20dependent%20kernels%2C%20we%20set%20out%20to%20generalize%20these%20insights%20to%20the%0Anon-linear%20setting.%20We%20derive%20generalized%20steerability%20constraints%20that%20any%0Asuch%20layer%20needs%20to%20satisfy%20and%20prove%20the%20universality%20of%20our%20construction.%20The%0Ainsights%20gained%20into%20the%20symmetry-constrained%20functional%20dependence%20of%0Aequivariant%20operators%20on%20feature%20maps%20and%20group%20elements%20informs%20the%20design%20of%0Afuture%20equivariant%20neural%20network%20layers.%20We%20demonstrate%20how%20several%20common%0Aequivariant%20network%20architectures%20-%20%24G%24-CNNs%2C%20implicit%20steerable%20kernel%0Anetworks%2C%20conventional%20and%20relative%20position%20embedded%20attention%20based%0Atransformers%2C%20and%20LieTransformers%20-%20may%20be%20derived%20from%20our%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520non-linear%2520maps%2520for%2520neural%2520networks%2520on%2520homogeneous%2520spaces%26entry.906535625%3DElias%2520Nyholm%2520and%2520Oscar%2520Carlsson%2520and%2520Maurice%2520Weiler%2520and%2520Daniel%2520Persson%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520for%2520non-linear%2520equivariant%2520neural%250Anetwork%2520layers%2520on%2520homogeneous%2520spaces.%2520The%2520seminal%2520work%2520of%2520Cohen%2520et%2520al.%2520on%250Aequivariant%2520%2524G%2524-CNNs%2520on%2520homogeneous%2520spaces%2520characterized%2520the%2520representation%250Atheory%2520of%2520such%2520layers%2520in%2520the%2520linear%2520setting%252C%2520finding%2520that%2520they%2520are%2520given%2520by%250Aconvolutions%2520with%2520kernels%2520satisfying%2520so-called%2520steerability%2520constraints.%250AMotivated%2520by%2520the%2520empirical%2520success%2520of%2520non-linear%2520layers%252C%2520such%2520as%2520self-attention%250Aor%2520input%2520dependent%2520kernels%252C%2520we%2520set%2520out%2520to%2520generalize%2520these%2520insights%2520to%2520the%250Anon-linear%2520setting.%2520We%2520derive%2520generalized%2520steerability%2520constraints%2520that%2520any%250Asuch%2520layer%2520needs%2520to%2520satisfy%2520and%2520prove%2520the%2520universality%2520of%2520our%2520construction.%2520The%250Ainsights%2520gained%2520into%2520the%2520symmetry-constrained%2520functional%2520dependence%2520of%250Aequivariant%2520operators%2520on%2520feature%2520maps%2520and%2520group%2520elements%2520informs%2520the%2520design%2520of%250Afuture%2520equivariant%2520neural%2520network%2520layers.%2520We%2520demonstrate%2520how%2520several%2520common%250Aequivariant%2520network%2520architectures%2520-%2520%2524G%2524-CNNs%252C%2520implicit%2520steerable%2520kernel%250Anetworks%252C%2520conventional%2520and%2520relative%2520position%2520embedded%2520attention%2520based%250Atransformers%252C%2520and%2520LieTransformers%2520-%2520may%2520be%2520derived%2520from%2520our%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20non-linear%20maps%20for%20neural%20networks%20on%20homogeneous%20spaces&entry.906535625=Elias%20Nyholm%20and%20Oscar%20Carlsson%20and%20Maurice%20Weiler%20and%20Daniel%20Persson&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20framework%20for%20non-linear%20equivariant%20neural%0Anetwork%20layers%20on%20homogeneous%20spaces.%20The%20seminal%20work%20of%20Cohen%20et%20al.%20on%0Aequivariant%20%24G%24-CNNs%20on%20homogeneous%20spaces%20characterized%20the%20representation%0Atheory%20of%20such%20layers%20in%20the%20linear%20setting%2C%20finding%20that%20they%20are%20given%20by%0Aconvolutions%20with%20kernels%20satisfying%20so-called%20steerability%20constraints.%0AMotivated%20by%20the%20empirical%20success%20of%20non-linear%20layers%2C%20such%20as%20self-attention%0Aor%20input%20dependent%20kernels%2C%20we%20set%20out%20to%20generalize%20these%20insights%20to%20the%0Anon-linear%20setting.%20We%20derive%20generalized%20steerability%20constraints%20that%20any%0Asuch%20layer%20needs%20to%20satisfy%20and%20prove%20the%20universality%20of%20our%20construction.%20The%0Ainsights%20gained%20into%20the%20symmetry-constrained%20functional%20dependence%20of%0Aequivariant%20operators%20on%20feature%20maps%20and%20group%20elements%20informs%20the%20design%20of%0Afuture%20equivariant%20neural%20network%20layers.%20We%20demonstrate%20how%20several%20common%0Aequivariant%20network%20architectures%20-%20%24G%24-CNNs%2C%20implicit%20steerable%20kernel%0Anetworks%2C%20conventional%20and%20relative%20position%20embedded%20attention%20based%0Atransformers%2C%20and%20LieTransformers%20-%20may%20be%20derived%20from%20our%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20974v1&entry.124074799=Read"},
{"title": "Foundations of Safe Online Reinforcement Learning in the Linear\n  Quadratic Regulator: Generalized Baselines", "author": "Benjamin Schiffer and Lucas Janson", "abstract": "  Many practical applications of online reinforcement learning require the\nsatisfaction of safety constraints while learning about the unknown\nenvironment. In this work, we establish theoretical foundations for\nreinforcement learning with safety constraints by studying the canonical\nproblem of Linear Quadratic Regulator learning with unknown dynamics, but with\nthe additional constraint that the position must stay within a safe region for\nthe entire trajectory with high probability. Our primary contribution is a\ngeneral framework for studying stronger baselines of nonlinear controllers that\nare better suited for constrained problems than linear controllers. Due to the\ndifficulty of analyzing non-linear controllers in a constrained problem, we\nfocus on 1-dimensional state- and action- spaces, however we also discuss how\nwe expect the high-level takeaways can generalize to higher dimensions. Using\nour framework, we show that for \\emph{any} non-linear baseline satisfying\nnatural assumptions, $\\tilde{O}_T(\\sqrt{T})$-regret is possible when the noise\ndistribution has sufficiently large support, and $\\tilde{O}_T(T^{2/3})$-regret\nis possible for \\emph{any} subgaussian noise distribution. In proving these\nresults, we introduce a new uncertainty estimation bound for nonlinear controls\nwhich shows that enforcing safety in the presence of sufficient noise can\nprovide ``free exploration'' that compensates for the added cost of uncertainty\nin safety-constrained control.\n", "link": "http://arxiv.org/abs/2410.21081v2", "date": "2025-04-29", "relevancy": 1.8949, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5126}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4764}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundations%20of%20Safe%20Online%20Reinforcement%20Learning%20in%20the%20Linear%0A%20%20Quadratic%20Regulator%3A%20Generalized%20Baselines&body=Title%3A%20Foundations%20of%20Safe%20Online%20Reinforcement%20Learning%20in%20the%20Linear%0A%20%20Quadratic%20Regulator%3A%20Generalized%20Baselines%0AAuthor%3A%20Benjamin%20Schiffer%20and%20Lucas%20Janson%0AAbstract%3A%20%20%20Many%20practical%20applications%20of%20online%20reinforcement%20learning%20require%20the%0Asatisfaction%20of%20safety%20constraints%20while%20learning%20about%20the%20unknown%0Aenvironment.%20In%20this%20work%2C%20we%20establish%20theoretical%20foundations%20for%0Areinforcement%20learning%20with%20safety%20constraints%20by%20studying%20the%20canonical%0Aproblem%20of%20Linear%20Quadratic%20Regulator%20learning%20with%20unknown%20dynamics%2C%20but%20with%0Athe%20additional%20constraint%20that%20the%20position%20must%20stay%20within%20a%20safe%20region%20for%0Athe%20entire%20trajectory%20with%20high%20probability.%20Our%20primary%20contribution%20is%20a%0Ageneral%20framework%20for%20studying%20stronger%20baselines%20of%20nonlinear%20controllers%20that%0Aare%20better%20suited%20for%20constrained%20problems%20than%20linear%20controllers.%20Due%20to%20the%0Adifficulty%20of%20analyzing%20non-linear%20controllers%20in%20a%20constrained%20problem%2C%20we%0Afocus%20on%201-dimensional%20state-%20and%20action-%20spaces%2C%20however%20we%20also%20discuss%20how%0Awe%20expect%20the%20high-level%20takeaways%20can%20generalize%20to%20higher%20dimensions.%20Using%0Aour%20framework%2C%20we%20show%20that%20for%20%5Cemph%7Bany%7D%20non-linear%20baseline%20satisfying%0Anatural%20assumptions%2C%20%24%5Ctilde%7BO%7D_T%28%5Csqrt%7BT%7D%29%24-regret%20is%20possible%20when%20the%20noise%0Adistribution%20has%20sufficiently%20large%20support%2C%20and%20%24%5Ctilde%7BO%7D_T%28T%5E%7B2/3%7D%29%24-regret%0Ais%20possible%20for%20%5Cemph%7Bany%7D%20subgaussian%20noise%20distribution.%20In%20proving%20these%0Aresults%2C%20we%20introduce%20a%20new%20uncertainty%20estimation%20bound%20for%20nonlinear%20controls%0Awhich%20shows%20that%20enforcing%20safety%20in%20the%20presence%20of%20sufficient%20noise%20can%0Aprovide%20%60%60free%20exploration%27%27%20that%20compensates%20for%20the%20added%20cost%20of%20uncertainty%0Ain%20safety-constrained%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundations%2520of%2520Safe%2520Online%2520Reinforcement%2520Learning%2520in%2520the%2520Linear%250A%2520%2520Quadratic%2520Regulator%253A%2520Generalized%2520Baselines%26entry.906535625%3DBenjamin%2520Schiffer%2520and%2520Lucas%2520Janson%26entry.1292438233%3D%2520%2520Many%2520practical%2520applications%2520of%2520online%2520reinforcement%2520learning%2520require%2520the%250Asatisfaction%2520of%2520safety%2520constraints%2520while%2520learning%2520about%2520the%2520unknown%250Aenvironment.%2520In%2520this%2520work%252C%2520we%2520establish%2520theoretical%2520foundations%2520for%250Areinforcement%2520learning%2520with%2520safety%2520constraints%2520by%2520studying%2520the%2520canonical%250Aproblem%2520of%2520Linear%2520Quadratic%2520Regulator%2520learning%2520with%2520unknown%2520dynamics%252C%2520but%2520with%250Athe%2520additional%2520constraint%2520that%2520the%2520position%2520must%2520stay%2520within%2520a%2520safe%2520region%2520for%250Athe%2520entire%2520trajectory%2520with%2520high%2520probability.%2520Our%2520primary%2520contribution%2520is%2520a%250Ageneral%2520framework%2520for%2520studying%2520stronger%2520baselines%2520of%2520nonlinear%2520controllers%2520that%250Aare%2520better%2520suited%2520for%2520constrained%2520problems%2520than%2520linear%2520controllers.%2520Due%2520to%2520the%250Adifficulty%2520of%2520analyzing%2520non-linear%2520controllers%2520in%2520a%2520constrained%2520problem%252C%2520we%250Afocus%2520on%25201-dimensional%2520state-%2520and%2520action-%2520spaces%252C%2520however%2520we%2520also%2520discuss%2520how%250Awe%2520expect%2520the%2520high-level%2520takeaways%2520can%2520generalize%2520to%2520higher%2520dimensions.%2520Using%250Aour%2520framework%252C%2520we%2520show%2520that%2520for%2520%255Cemph%257Bany%257D%2520non-linear%2520baseline%2520satisfying%250Anatural%2520assumptions%252C%2520%2524%255Ctilde%257BO%257D_T%2528%255Csqrt%257BT%257D%2529%2524-regret%2520is%2520possible%2520when%2520the%2520noise%250Adistribution%2520has%2520sufficiently%2520large%2520support%252C%2520and%2520%2524%255Ctilde%257BO%257D_T%2528T%255E%257B2/3%257D%2529%2524-regret%250Ais%2520possible%2520for%2520%255Cemph%257Bany%257D%2520subgaussian%2520noise%2520distribution.%2520In%2520proving%2520these%250Aresults%252C%2520we%2520introduce%2520a%2520new%2520uncertainty%2520estimation%2520bound%2520for%2520nonlinear%2520controls%250Awhich%2520shows%2520that%2520enforcing%2520safety%2520in%2520the%2520presence%2520of%2520sufficient%2520noise%2520can%250Aprovide%2520%2560%2560free%2520exploration%2527%2527%2520that%2520compensates%2520for%2520the%2520added%2520cost%2520of%2520uncertainty%250Ain%2520safety-constrained%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundations%20of%20Safe%20Online%20Reinforcement%20Learning%20in%20the%20Linear%0A%20%20Quadratic%20Regulator%3A%20Generalized%20Baselines&entry.906535625=Benjamin%20Schiffer%20and%20Lucas%20Janson&entry.1292438233=%20%20Many%20practical%20applications%20of%20online%20reinforcement%20learning%20require%20the%0Asatisfaction%20of%20safety%20constraints%20while%20learning%20about%20the%20unknown%0Aenvironment.%20In%20this%20work%2C%20we%20establish%20theoretical%20foundations%20for%0Areinforcement%20learning%20with%20safety%20constraints%20by%20studying%20the%20canonical%0Aproblem%20of%20Linear%20Quadratic%20Regulator%20learning%20with%20unknown%20dynamics%2C%20but%20with%0Athe%20additional%20constraint%20that%20the%20position%20must%20stay%20within%20a%20safe%20region%20for%0Athe%20entire%20trajectory%20with%20high%20probability.%20Our%20primary%20contribution%20is%20a%0Ageneral%20framework%20for%20studying%20stronger%20baselines%20of%20nonlinear%20controllers%20that%0Aare%20better%20suited%20for%20constrained%20problems%20than%20linear%20controllers.%20Due%20to%20the%0Adifficulty%20of%20analyzing%20non-linear%20controllers%20in%20a%20constrained%20problem%2C%20we%0Afocus%20on%201-dimensional%20state-%20and%20action-%20spaces%2C%20however%20we%20also%20discuss%20how%0Awe%20expect%20the%20high-level%20takeaways%20can%20generalize%20to%20higher%20dimensions.%20Using%0Aour%20framework%2C%20we%20show%20that%20for%20%5Cemph%7Bany%7D%20non-linear%20baseline%20satisfying%0Anatural%20assumptions%2C%20%24%5Ctilde%7BO%7D_T%28%5Csqrt%7BT%7D%29%24-regret%20is%20possible%20when%20the%20noise%0Adistribution%20has%20sufficiently%20large%20support%2C%20and%20%24%5Ctilde%7BO%7D_T%28T%5E%7B2/3%7D%29%24-regret%0Ais%20possible%20for%20%5Cemph%7Bany%7D%20subgaussian%20noise%20distribution.%20In%20proving%20these%0Aresults%2C%20we%20introduce%20a%20new%20uncertainty%20estimation%20bound%20for%20nonlinear%20controls%0Awhich%20shows%20that%20enforcing%20safety%20in%20the%20presence%20of%20sufficient%20noise%20can%0Aprovide%20%60%60free%20exploration%27%27%20that%20compensates%20for%20the%20added%20cost%20of%20uncertainty%0Ain%20safety-constrained%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21081v2&entry.124074799=Read"},
{"title": "Hybrid Quantum Recurrent Neural Network For Remaining Useful Life\n  Prediction", "author": "Olga Tsurkan and Aleksandra Konstantinova and Aleksandr Sedykh and Dmitrii Zhiganov and Arsenii Senokosov and Daniil Tarpanov and Matvei Anoshin and Leonid Fedichkin", "abstract": "  Predictive maintenance in aerospace heavily relies on accurate estimation of\nthe remaining useful life of jet engines. In this paper, we introduce a Hybrid\nQuantum Recurrent Neural Network framework, combining Quantum Long Short-Term\nMemory layers with classical dense layers for Remaining Useful Life forecasting\non NASA's Commercial Modular Aero-Propulsion System Simulation dataset. Each\nQuantum Long Short-Term Memory gate replaces conventional linear\ntransformations with Quantum Depth-Infused circuits, allowing the network to\nlearn high-frequency components more effectively. Experimental results\ndemonstrate that, despite having fewer trainable parameters, the Hybrid Quantum\nRecurrent Neural Network achieves up to a 5% improvement over a Recurrent\nNeural Network based on stacked Long Short-Term Memory layers in terms of mean\nroot mean squared error and mean absolute error. Moreover, a thorough\ncomparison of our method with established techniques, including Random Forest,\nConvolutional Neural Network, and Multilayer Perceptron, demonstrates that our\napproach, which achieves a Root Mean Squared Error of 15.46, surpasses these\nbaselines by approximately 13.68%, 16.21%, and 7.87%, respectively.\nNevertheless, it remains outperformed by certain advanced joint architectures.\nOur findings highlight the potential of hybrid quantum-classical approaches for\nrobust time-series forecasting under limited data conditions, offering new\navenues for enhancing reliability in predictive maintenance tasks.\n", "link": "http://arxiv.org/abs/2504.20823v1", "date": "2025-04-29", "relevancy": 1.8752, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.523}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4842}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Quantum%20Recurrent%20Neural%20Network%20For%20Remaining%20Useful%20Life%0A%20%20Prediction&body=Title%3A%20Hybrid%20Quantum%20Recurrent%20Neural%20Network%20For%20Remaining%20Useful%20Life%0A%20%20Prediction%0AAuthor%3A%20Olga%20Tsurkan%20and%20Aleksandra%20Konstantinova%20and%20Aleksandr%20Sedykh%20and%20Dmitrii%20Zhiganov%20and%20Arsenii%20Senokosov%20and%20Daniil%20Tarpanov%20and%20Matvei%20Anoshin%20and%20Leonid%20Fedichkin%0AAbstract%3A%20%20%20Predictive%20maintenance%20in%20aerospace%20heavily%20relies%20on%20accurate%20estimation%20of%0Athe%20remaining%20useful%20life%20of%20jet%20engines.%20In%20this%20paper%2C%20we%20introduce%20a%20Hybrid%0AQuantum%20Recurrent%20Neural%20Network%20framework%2C%20combining%20Quantum%20Long%20Short-Term%0AMemory%20layers%20with%20classical%20dense%20layers%20for%20Remaining%20Useful%20Life%20forecasting%0Aon%20NASA%27s%20Commercial%20Modular%20Aero-Propulsion%20System%20Simulation%20dataset.%20Each%0AQuantum%20Long%20Short-Term%20Memory%20gate%20replaces%20conventional%20linear%0Atransformations%20with%20Quantum%20Depth-Infused%20circuits%2C%20allowing%20the%20network%20to%0Alearn%20high-frequency%20components%20more%20effectively.%20Experimental%20results%0Ademonstrate%20that%2C%20despite%20having%20fewer%20trainable%20parameters%2C%20the%20Hybrid%20Quantum%0ARecurrent%20Neural%20Network%20achieves%20up%20to%20a%205%25%20improvement%20over%20a%20Recurrent%0ANeural%20Network%20based%20on%20stacked%20Long%20Short-Term%20Memory%20layers%20in%20terms%20of%20mean%0Aroot%20mean%20squared%20error%20and%20mean%20absolute%20error.%20Moreover%2C%20a%20thorough%0Acomparison%20of%20our%20method%20with%20established%20techniques%2C%20including%20Random%20Forest%2C%0AConvolutional%20Neural%20Network%2C%20and%20Multilayer%20Perceptron%2C%20demonstrates%20that%20our%0Aapproach%2C%20which%20achieves%20a%20Root%20Mean%20Squared%20Error%20of%2015.46%2C%20surpasses%20these%0Abaselines%20by%20approximately%2013.68%25%2C%2016.21%25%2C%20and%207.87%25%2C%20respectively.%0ANevertheless%2C%20it%20remains%20outperformed%20by%20certain%20advanced%20joint%20architectures.%0AOur%20findings%20highlight%20the%20potential%20of%20hybrid%20quantum-classical%20approaches%20for%0Arobust%20time-series%20forecasting%20under%20limited%20data%20conditions%2C%20offering%20new%0Aavenues%20for%20enhancing%20reliability%20in%20predictive%20maintenance%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Quantum%2520Recurrent%2520Neural%2520Network%2520For%2520Remaining%2520Useful%2520Life%250A%2520%2520Prediction%26entry.906535625%3DOlga%2520Tsurkan%2520and%2520Aleksandra%2520Konstantinova%2520and%2520Aleksandr%2520Sedykh%2520and%2520Dmitrii%2520Zhiganov%2520and%2520Arsenii%2520Senokosov%2520and%2520Daniil%2520Tarpanov%2520and%2520Matvei%2520Anoshin%2520and%2520Leonid%2520Fedichkin%26entry.1292438233%3D%2520%2520Predictive%2520maintenance%2520in%2520aerospace%2520heavily%2520relies%2520on%2520accurate%2520estimation%2520of%250Athe%2520remaining%2520useful%2520life%2520of%2520jet%2520engines.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520Hybrid%250AQuantum%2520Recurrent%2520Neural%2520Network%2520framework%252C%2520combining%2520Quantum%2520Long%2520Short-Term%250AMemory%2520layers%2520with%2520classical%2520dense%2520layers%2520for%2520Remaining%2520Useful%2520Life%2520forecasting%250Aon%2520NASA%2527s%2520Commercial%2520Modular%2520Aero-Propulsion%2520System%2520Simulation%2520dataset.%2520Each%250AQuantum%2520Long%2520Short-Term%2520Memory%2520gate%2520replaces%2520conventional%2520linear%250Atransformations%2520with%2520Quantum%2520Depth-Infused%2520circuits%252C%2520allowing%2520the%2520network%2520to%250Alearn%2520high-frequency%2520components%2520more%2520effectively.%2520Experimental%2520results%250Ademonstrate%2520that%252C%2520despite%2520having%2520fewer%2520trainable%2520parameters%252C%2520the%2520Hybrid%2520Quantum%250ARecurrent%2520Neural%2520Network%2520achieves%2520up%2520to%2520a%25205%2525%2520improvement%2520over%2520a%2520Recurrent%250ANeural%2520Network%2520based%2520on%2520stacked%2520Long%2520Short-Term%2520Memory%2520layers%2520in%2520terms%2520of%2520mean%250Aroot%2520mean%2520squared%2520error%2520and%2520mean%2520absolute%2520error.%2520Moreover%252C%2520a%2520thorough%250Acomparison%2520of%2520our%2520method%2520with%2520established%2520techniques%252C%2520including%2520Random%2520Forest%252C%250AConvolutional%2520Neural%2520Network%252C%2520and%2520Multilayer%2520Perceptron%252C%2520demonstrates%2520that%2520our%250Aapproach%252C%2520which%2520achieves%2520a%2520Root%2520Mean%2520Squared%2520Error%2520of%252015.46%252C%2520surpasses%2520these%250Abaselines%2520by%2520approximately%252013.68%2525%252C%252016.21%2525%252C%2520and%25207.87%2525%252C%2520respectively.%250ANevertheless%252C%2520it%2520remains%2520outperformed%2520by%2520certain%2520advanced%2520joint%2520architectures.%250AOur%2520findings%2520highlight%2520the%2520potential%2520of%2520hybrid%2520quantum-classical%2520approaches%2520for%250Arobust%2520time-series%2520forecasting%2520under%2520limited%2520data%2520conditions%252C%2520offering%2520new%250Aavenues%2520for%2520enhancing%2520reliability%2520in%2520predictive%2520maintenance%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Quantum%20Recurrent%20Neural%20Network%20For%20Remaining%20Useful%20Life%0A%20%20Prediction&entry.906535625=Olga%20Tsurkan%20and%20Aleksandra%20Konstantinova%20and%20Aleksandr%20Sedykh%20and%20Dmitrii%20Zhiganov%20and%20Arsenii%20Senokosov%20and%20Daniil%20Tarpanov%20and%20Matvei%20Anoshin%20and%20Leonid%20Fedichkin&entry.1292438233=%20%20Predictive%20maintenance%20in%20aerospace%20heavily%20relies%20on%20accurate%20estimation%20of%0Athe%20remaining%20useful%20life%20of%20jet%20engines.%20In%20this%20paper%2C%20we%20introduce%20a%20Hybrid%0AQuantum%20Recurrent%20Neural%20Network%20framework%2C%20combining%20Quantum%20Long%20Short-Term%0AMemory%20layers%20with%20classical%20dense%20layers%20for%20Remaining%20Useful%20Life%20forecasting%0Aon%20NASA%27s%20Commercial%20Modular%20Aero-Propulsion%20System%20Simulation%20dataset.%20Each%0AQuantum%20Long%20Short-Term%20Memory%20gate%20replaces%20conventional%20linear%0Atransformations%20with%20Quantum%20Depth-Infused%20circuits%2C%20allowing%20the%20network%20to%0Alearn%20high-frequency%20components%20more%20effectively.%20Experimental%20results%0Ademonstrate%20that%2C%20despite%20having%20fewer%20trainable%20parameters%2C%20the%20Hybrid%20Quantum%0ARecurrent%20Neural%20Network%20achieves%20up%20to%20a%205%25%20improvement%20over%20a%20Recurrent%0ANeural%20Network%20based%20on%20stacked%20Long%20Short-Term%20Memory%20layers%20in%20terms%20of%20mean%0Aroot%20mean%20squared%20error%20and%20mean%20absolute%20error.%20Moreover%2C%20a%20thorough%0Acomparison%20of%20our%20method%20with%20established%20techniques%2C%20including%20Random%20Forest%2C%0AConvolutional%20Neural%20Network%2C%20and%20Multilayer%20Perceptron%2C%20demonstrates%20that%20our%0Aapproach%2C%20which%20achieves%20a%20Root%20Mean%20Squared%20Error%20of%2015.46%2C%20surpasses%20these%0Abaselines%20by%20approximately%2013.68%25%2C%2016.21%25%2C%20and%207.87%25%2C%20respectively.%0ANevertheless%2C%20it%20remains%20outperformed%20by%20certain%20advanced%20joint%20architectures.%0AOur%20findings%20highlight%20the%20potential%20of%20hybrid%20quantum-classical%20approaches%20for%0Arobust%20time-series%20forecasting%20under%20limited%20data%20conditions%2C%20offering%20new%0Aavenues%20for%20enhancing%20reliability%20in%20predictive%20maintenance%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20823v1&entry.124074799=Read"},
{"title": "Quantifying the Noise of Structural Perturbations on Graph Adversarial\n  Attacks", "author": "Junyuan Fang and Han Yang and Haixian Wen and Jiajing Wu and Zibin Zheng and Chi K. Tse", "abstract": "  Graph neural networks have been widely utilized to solve graph-related tasks\nbecause of their strong learning power in utilizing the local information of\nneighbors. However, recent studies on graph adversarial attacks have proven\nthat current graph neural networks are not robust against malicious attacks.\nYet much of the existing work has focused on the optimization objective based\non attack performance to obtain (near) optimal perturbations, but paid less\nattention to the strength quantification of each perturbation such as the\ninjection of a particular node/link, which makes the choice of perturbations a\nblack-box model that lacks interpretability. In this work, we propose the\nconcept of noise to quantify the attack strength of each adversarial link.\nFurthermore, we propose three attack strategies based on the defined noise and\nclassification margins in terms of single and multiple steps optimization.\nExtensive experiments conducted on benchmark datasets against three\nrepresentative graph neural networks demonstrate the effectiveness of the\nproposed attack strategies. Particularly, we also investigate the preferred\npatterns of effective adversarial perturbations by analyzing the corresponding\nproperties of the selected perturbation nodes.\n", "link": "http://arxiv.org/abs/2504.20869v1", "date": "2025-04-29", "relevancy": 1.8572, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4859}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.464}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20the%20Noise%20of%20Structural%20Perturbations%20on%20Graph%20Adversarial%0A%20%20Attacks&body=Title%3A%20Quantifying%20the%20Noise%20of%20Structural%20Perturbations%20on%20Graph%20Adversarial%0A%20%20Attacks%0AAuthor%3A%20Junyuan%20Fang%20and%20Han%20Yang%20and%20Haixian%20Wen%20and%20Jiajing%20Wu%20and%20Zibin%20Zheng%20and%20Chi%20K.%20Tse%0AAbstract%3A%20%20%20Graph%20neural%20networks%20have%20been%20widely%20utilized%20to%20solve%20graph-related%20tasks%0Abecause%20of%20their%20strong%20learning%20power%20in%20utilizing%20the%20local%20information%20of%0Aneighbors.%20However%2C%20recent%20studies%20on%20graph%20adversarial%20attacks%20have%20proven%0Athat%20current%20graph%20neural%20networks%20are%20not%20robust%20against%20malicious%20attacks.%0AYet%20much%20of%20the%20existing%20work%20has%20focused%20on%20the%20optimization%20objective%20based%0Aon%20attack%20performance%20to%20obtain%20%28near%29%20optimal%20perturbations%2C%20but%20paid%20less%0Aattention%20to%20the%20strength%20quantification%20of%20each%20perturbation%20such%20as%20the%0Ainjection%20of%20a%20particular%20node/link%2C%20which%20makes%20the%20choice%20of%20perturbations%20a%0Ablack-box%20model%20that%20lacks%20interpretability.%20In%20this%20work%2C%20we%20propose%20the%0Aconcept%20of%20noise%20to%20quantify%20the%20attack%20strength%20of%20each%20adversarial%20link.%0AFurthermore%2C%20we%20propose%20three%20attack%20strategies%20based%20on%20the%20defined%20noise%20and%0Aclassification%20margins%20in%20terms%20of%20single%20and%20multiple%20steps%20optimization.%0AExtensive%20experiments%20conducted%20on%20benchmark%20datasets%20against%20three%0Arepresentative%20graph%20neural%20networks%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20attack%20strategies.%20Particularly%2C%20we%20also%20investigate%20the%20preferred%0Apatterns%20of%20effective%20adversarial%20perturbations%20by%20analyzing%20the%20corresponding%0Aproperties%20of%20the%20selected%20perturbation%20nodes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520the%2520Noise%2520of%2520Structural%2520Perturbations%2520on%2520Graph%2520Adversarial%250A%2520%2520Attacks%26entry.906535625%3DJunyuan%2520Fang%2520and%2520Han%2520Yang%2520and%2520Haixian%2520Wen%2520and%2520Jiajing%2520Wu%2520and%2520Zibin%2520Zheng%2520and%2520Chi%2520K.%2520Tse%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520have%2520been%2520widely%2520utilized%2520to%2520solve%2520graph-related%2520tasks%250Abecause%2520of%2520their%2520strong%2520learning%2520power%2520in%2520utilizing%2520the%2520local%2520information%2520of%250Aneighbors.%2520However%252C%2520recent%2520studies%2520on%2520graph%2520adversarial%2520attacks%2520have%2520proven%250Athat%2520current%2520graph%2520neural%2520networks%2520are%2520not%2520robust%2520against%2520malicious%2520attacks.%250AYet%2520much%2520of%2520the%2520existing%2520work%2520has%2520focused%2520on%2520the%2520optimization%2520objective%2520based%250Aon%2520attack%2520performance%2520to%2520obtain%2520%2528near%2529%2520optimal%2520perturbations%252C%2520but%2520paid%2520less%250Aattention%2520to%2520the%2520strength%2520quantification%2520of%2520each%2520perturbation%2520such%2520as%2520the%250Ainjection%2520of%2520a%2520particular%2520node/link%252C%2520which%2520makes%2520the%2520choice%2520of%2520perturbations%2520a%250Ablack-box%2520model%2520that%2520lacks%2520interpretability.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%250Aconcept%2520of%2520noise%2520to%2520quantify%2520the%2520attack%2520strength%2520of%2520each%2520adversarial%2520link.%250AFurthermore%252C%2520we%2520propose%2520three%2520attack%2520strategies%2520based%2520on%2520the%2520defined%2520noise%2520and%250Aclassification%2520margins%2520in%2520terms%2520of%2520single%2520and%2520multiple%2520steps%2520optimization.%250AExtensive%2520experiments%2520conducted%2520on%2520benchmark%2520datasets%2520against%2520three%250Arepresentative%2520graph%2520neural%2520networks%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520attack%2520strategies.%2520Particularly%252C%2520we%2520also%2520investigate%2520the%2520preferred%250Apatterns%2520of%2520effective%2520adversarial%2520perturbations%2520by%2520analyzing%2520the%2520corresponding%250Aproperties%2520of%2520the%2520selected%2520perturbation%2520nodes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20the%20Noise%20of%20Structural%20Perturbations%20on%20Graph%20Adversarial%0A%20%20Attacks&entry.906535625=Junyuan%20Fang%20and%20Han%20Yang%20and%20Haixian%20Wen%20and%20Jiajing%20Wu%20and%20Zibin%20Zheng%20and%20Chi%20K.%20Tse&entry.1292438233=%20%20Graph%20neural%20networks%20have%20been%20widely%20utilized%20to%20solve%20graph-related%20tasks%0Abecause%20of%20their%20strong%20learning%20power%20in%20utilizing%20the%20local%20information%20of%0Aneighbors.%20However%2C%20recent%20studies%20on%20graph%20adversarial%20attacks%20have%20proven%0Athat%20current%20graph%20neural%20networks%20are%20not%20robust%20against%20malicious%20attacks.%0AYet%20much%20of%20the%20existing%20work%20has%20focused%20on%20the%20optimization%20objective%20based%0Aon%20attack%20performance%20to%20obtain%20%28near%29%20optimal%20perturbations%2C%20but%20paid%20less%0Aattention%20to%20the%20strength%20quantification%20of%20each%20perturbation%20such%20as%20the%0Ainjection%20of%20a%20particular%20node/link%2C%20which%20makes%20the%20choice%20of%20perturbations%20a%0Ablack-box%20model%20that%20lacks%20interpretability.%20In%20this%20work%2C%20we%20propose%20the%0Aconcept%20of%20noise%20to%20quantify%20the%20attack%20strength%20of%20each%20adversarial%20link.%0AFurthermore%2C%20we%20propose%20three%20attack%20strategies%20based%20on%20the%20defined%20noise%20and%0Aclassification%20margins%20in%20terms%20of%20single%20and%20multiple%20steps%20optimization.%0AExtensive%20experiments%20conducted%20on%20benchmark%20datasets%20against%20three%0Arepresentative%20graph%20neural%20networks%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20attack%20strategies.%20Particularly%2C%20we%20also%20investigate%20the%20preferred%0Apatterns%20of%20effective%20adversarial%20perturbations%20by%20analyzing%20the%20corresponding%0Aproperties%20of%20the%20selected%20perturbation%20nodes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20869v1&entry.124074799=Read"},
{"title": "Deep Learning Characterizes Depression and Suicidal Ideation from Eye\n  Movements", "author": "Kleanthis Avramidis and Woojae Jeong and Aditya Kommineni and Sudarsana R. Kadiri and Marcus Ma and Colin McDaniel and Myzelle Hughes and Thomas McGee and Elsi Kaiser and Dani Byrd and Assal Habibi and B. Rael Cahn and Idan A. Blank and Kristina Lerman and Takfarinas Medani and Richard M. Leahy and Shrikanth Narayanan", "abstract": "  Identifying physiological and behavioral markers for mental health conditions\nis a longstanding challenge in psychiatry. Depression and suicidal ideation, in\nparticular, lack objective biomarkers, with screening and diagnosis primarily\nrelying on self-reports and clinical interviews. Here, we investigate eye\ntracking as a potential marker modality for screening purposes. Eye movements\nare directly modulated by neuronal networks and have been associated with\nattentional and mood-related patterns; however, their predictive value for\ndepression and suicidality remains unclear. We recorded eye-tracking sequences\nfrom 126 young adults as they read and responded to affective sentences, and\nsubsequently developed a deep learning framework to predict their clinical\nstatus. The proposed model included separate branches for trials of positive\nand negative sentiment, and used 2D time-series representations to account for\nboth intra-trial and inter-trial variations. We were able to identify\ndepression and suicidal ideation with an area under the receiver operating\ncurve (AUC) of 0.793 (95% CI: 0.765-0.819) against healthy controls, and\nsuicidality specifically with 0.826 AUC (95% CI: 0.797-0.852). The model also\nexhibited moderate, yet significant, accuracy in differentiating depressed from\nsuicidal participants, with 0.609 AUC (95% CI 0.571-0.646). Discriminative\npatterns emerge more strongly when assessing the data relative to response\ngeneration than relative to the onset time of the final word of the sentences.\nThe most pronounced effects were observed for negative-sentiment sentences,\nthat are congruent to depressed and suicidal participants. Our findings\nhighlight eye tracking as an objective tool for mental health assessment and\nunderscore the modulatory impact of emotional stimuli on cognitive processes\naffecting oculomotor control.\n", "link": "http://arxiv.org/abs/2504.20944v1", "date": "2025-04-29", "relevancy": 1.8121, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4533}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Characterizes%20Depression%20and%20Suicidal%20Ideation%20from%20Eye%0A%20%20Movements&body=Title%3A%20Deep%20Learning%20Characterizes%20Depression%20and%20Suicidal%20Ideation%20from%20Eye%0A%20%20Movements%0AAuthor%3A%20Kleanthis%20Avramidis%20and%20Woojae%20Jeong%20and%20Aditya%20Kommineni%20and%20Sudarsana%20R.%20Kadiri%20and%20Marcus%20Ma%20and%20Colin%20McDaniel%20and%20Myzelle%20Hughes%20and%20Thomas%20McGee%20and%20Elsi%20Kaiser%20and%20Dani%20Byrd%20and%20Assal%20Habibi%20and%20B.%20Rael%20Cahn%20and%20Idan%20A.%20Blank%20and%20Kristina%20Lerman%20and%20Takfarinas%20Medani%20and%20Richard%20M.%20Leahy%20and%20Shrikanth%20Narayanan%0AAbstract%3A%20%20%20Identifying%20physiological%20and%20behavioral%20markers%20for%20mental%20health%20conditions%0Ais%20a%20longstanding%20challenge%20in%20psychiatry.%20Depression%20and%20suicidal%20ideation%2C%20in%0Aparticular%2C%20lack%20objective%20biomarkers%2C%20with%20screening%20and%20diagnosis%20primarily%0Arelying%20on%20self-reports%20and%20clinical%20interviews.%20Here%2C%20we%20investigate%20eye%0Atracking%20as%20a%20potential%20marker%20modality%20for%20screening%20purposes.%20Eye%20movements%0Aare%20directly%20modulated%20by%20neuronal%20networks%20and%20have%20been%20associated%20with%0Aattentional%20and%20mood-related%20patterns%3B%20however%2C%20their%20predictive%20value%20for%0Adepression%20and%20suicidality%20remains%20unclear.%20We%20recorded%20eye-tracking%20sequences%0Afrom%20126%20young%20adults%20as%20they%20read%20and%20responded%20to%20affective%20sentences%2C%20and%0Asubsequently%20developed%20a%20deep%20learning%20framework%20to%20predict%20their%20clinical%0Astatus.%20The%20proposed%20model%20included%20separate%20branches%20for%20trials%20of%20positive%0Aand%20negative%20sentiment%2C%20and%20used%202D%20time-series%20representations%20to%20account%20for%0Aboth%20intra-trial%20and%20inter-trial%20variations.%20We%20were%20able%20to%20identify%0Adepression%20and%20suicidal%20ideation%20with%20an%20area%20under%20the%20receiver%20operating%0Acurve%20%28AUC%29%20of%200.793%20%2895%25%20CI%3A%200.765-0.819%29%20against%20healthy%20controls%2C%20and%0Asuicidality%20specifically%20with%200.826%20AUC%20%2895%25%20CI%3A%200.797-0.852%29.%20The%20model%20also%0Aexhibited%20moderate%2C%20yet%20significant%2C%20accuracy%20in%20differentiating%20depressed%20from%0Asuicidal%20participants%2C%20with%200.609%20AUC%20%2895%25%20CI%200.571-0.646%29.%20Discriminative%0Apatterns%20emerge%20more%20strongly%20when%20assessing%20the%20data%20relative%20to%20response%0Ageneration%20than%20relative%20to%20the%20onset%20time%20of%20the%20final%20word%20of%20the%20sentences.%0AThe%20most%20pronounced%20effects%20were%20observed%20for%20negative-sentiment%20sentences%2C%0Athat%20are%20congruent%20to%20depressed%20and%20suicidal%20participants.%20Our%20findings%0Ahighlight%20eye%20tracking%20as%20an%20objective%20tool%20for%20mental%20health%20assessment%20and%0Aunderscore%20the%20modulatory%20impact%20of%20emotional%20stimuli%20on%20cognitive%20processes%0Aaffecting%20oculomotor%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Characterizes%2520Depression%2520and%2520Suicidal%2520Ideation%2520from%2520Eye%250A%2520%2520Movements%26entry.906535625%3DKleanthis%2520Avramidis%2520and%2520Woojae%2520Jeong%2520and%2520Aditya%2520Kommineni%2520and%2520Sudarsana%2520R.%2520Kadiri%2520and%2520Marcus%2520Ma%2520and%2520Colin%2520McDaniel%2520and%2520Myzelle%2520Hughes%2520and%2520Thomas%2520McGee%2520and%2520Elsi%2520Kaiser%2520and%2520Dani%2520Byrd%2520and%2520Assal%2520Habibi%2520and%2520B.%2520Rael%2520Cahn%2520and%2520Idan%2520A.%2520Blank%2520and%2520Kristina%2520Lerman%2520and%2520Takfarinas%2520Medani%2520and%2520Richard%2520M.%2520Leahy%2520and%2520Shrikanth%2520Narayanan%26entry.1292438233%3D%2520%2520Identifying%2520physiological%2520and%2520behavioral%2520markers%2520for%2520mental%2520health%2520conditions%250Ais%2520a%2520longstanding%2520challenge%2520in%2520psychiatry.%2520Depression%2520and%2520suicidal%2520ideation%252C%2520in%250Aparticular%252C%2520lack%2520objective%2520biomarkers%252C%2520with%2520screening%2520and%2520diagnosis%2520primarily%250Arelying%2520on%2520self-reports%2520and%2520clinical%2520interviews.%2520Here%252C%2520we%2520investigate%2520eye%250Atracking%2520as%2520a%2520potential%2520marker%2520modality%2520for%2520screening%2520purposes.%2520Eye%2520movements%250Aare%2520directly%2520modulated%2520by%2520neuronal%2520networks%2520and%2520have%2520been%2520associated%2520with%250Aattentional%2520and%2520mood-related%2520patterns%253B%2520however%252C%2520their%2520predictive%2520value%2520for%250Adepression%2520and%2520suicidality%2520remains%2520unclear.%2520We%2520recorded%2520eye-tracking%2520sequences%250Afrom%2520126%2520young%2520adults%2520as%2520they%2520read%2520and%2520responded%2520to%2520affective%2520sentences%252C%2520and%250Asubsequently%2520developed%2520a%2520deep%2520learning%2520framework%2520to%2520predict%2520their%2520clinical%250Astatus.%2520The%2520proposed%2520model%2520included%2520separate%2520branches%2520for%2520trials%2520of%2520positive%250Aand%2520negative%2520sentiment%252C%2520and%2520used%25202D%2520time-series%2520representations%2520to%2520account%2520for%250Aboth%2520intra-trial%2520and%2520inter-trial%2520variations.%2520We%2520were%2520able%2520to%2520identify%250Adepression%2520and%2520suicidal%2520ideation%2520with%2520an%2520area%2520under%2520the%2520receiver%2520operating%250Acurve%2520%2528AUC%2529%2520of%25200.793%2520%252895%2525%2520CI%253A%25200.765-0.819%2529%2520against%2520healthy%2520controls%252C%2520and%250Asuicidality%2520specifically%2520with%25200.826%2520AUC%2520%252895%2525%2520CI%253A%25200.797-0.852%2529.%2520The%2520model%2520also%250Aexhibited%2520moderate%252C%2520yet%2520significant%252C%2520accuracy%2520in%2520differentiating%2520depressed%2520from%250Asuicidal%2520participants%252C%2520with%25200.609%2520AUC%2520%252895%2525%2520CI%25200.571-0.646%2529.%2520Discriminative%250Apatterns%2520emerge%2520more%2520strongly%2520when%2520assessing%2520the%2520data%2520relative%2520to%2520response%250Ageneration%2520than%2520relative%2520to%2520the%2520onset%2520time%2520of%2520the%2520final%2520word%2520of%2520the%2520sentences.%250AThe%2520most%2520pronounced%2520effects%2520were%2520observed%2520for%2520negative-sentiment%2520sentences%252C%250Athat%2520are%2520congruent%2520to%2520depressed%2520and%2520suicidal%2520participants.%2520Our%2520findings%250Ahighlight%2520eye%2520tracking%2520as%2520an%2520objective%2520tool%2520for%2520mental%2520health%2520assessment%2520and%250Aunderscore%2520the%2520modulatory%2520impact%2520of%2520emotional%2520stimuli%2520on%2520cognitive%2520processes%250Aaffecting%2520oculomotor%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Characterizes%20Depression%20and%20Suicidal%20Ideation%20from%20Eye%0A%20%20Movements&entry.906535625=Kleanthis%20Avramidis%20and%20Woojae%20Jeong%20and%20Aditya%20Kommineni%20and%20Sudarsana%20R.%20Kadiri%20and%20Marcus%20Ma%20and%20Colin%20McDaniel%20and%20Myzelle%20Hughes%20and%20Thomas%20McGee%20and%20Elsi%20Kaiser%20and%20Dani%20Byrd%20and%20Assal%20Habibi%20and%20B.%20Rael%20Cahn%20and%20Idan%20A.%20Blank%20and%20Kristina%20Lerman%20and%20Takfarinas%20Medani%20and%20Richard%20M.%20Leahy%20and%20Shrikanth%20Narayanan&entry.1292438233=%20%20Identifying%20physiological%20and%20behavioral%20markers%20for%20mental%20health%20conditions%0Ais%20a%20longstanding%20challenge%20in%20psychiatry.%20Depression%20and%20suicidal%20ideation%2C%20in%0Aparticular%2C%20lack%20objective%20biomarkers%2C%20with%20screening%20and%20diagnosis%20primarily%0Arelying%20on%20self-reports%20and%20clinical%20interviews.%20Here%2C%20we%20investigate%20eye%0Atracking%20as%20a%20potential%20marker%20modality%20for%20screening%20purposes.%20Eye%20movements%0Aare%20directly%20modulated%20by%20neuronal%20networks%20and%20have%20been%20associated%20with%0Aattentional%20and%20mood-related%20patterns%3B%20however%2C%20their%20predictive%20value%20for%0Adepression%20and%20suicidality%20remains%20unclear.%20We%20recorded%20eye-tracking%20sequences%0Afrom%20126%20young%20adults%20as%20they%20read%20and%20responded%20to%20affective%20sentences%2C%20and%0Asubsequently%20developed%20a%20deep%20learning%20framework%20to%20predict%20their%20clinical%0Astatus.%20The%20proposed%20model%20included%20separate%20branches%20for%20trials%20of%20positive%0Aand%20negative%20sentiment%2C%20and%20used%202D%20time-series%20representations%20to%20account%20for%0Aboth%20intra-trial%20and%20inter-trial%20variations.%20We%20were%20able%20to%20identify%0Adepression%20and%20suicidal%20ideation%20with%20an%20area%20under%20the%20receiver%20operating%0Acurve%20%28AUC%29%20of%200.793%20%2895%25%20CI%3A%200.765-0.819%29%20against%20healthy%20controls%2C%20and%0Asuicidality%20specifically%20with%200.826%20AUC%20%2895%25%20CI%3A%200.797-0.852%29.%20The%20model%20also%0Aexhibited%20moderate%2C%20yet%20significant%2C%20accuracy%20in%20differentiating%20depressed%20from%0Asuicidal%20participants%2C%20with%200.609%20AUC%20%2895%25%20CI%200.571-0.646%29.%20Discriminative%0Apatterns%20emerge%20more%20strongly%20when%20assessing%20the%20data%20relative%20to%20response%0Ageneration%20than%20relative%20to%20the%20onset%20time%20of%20the%20final%20word%20of%20the%20sentences.%0AThe%20most%20pronounced%20effects%20were%20observed%20for%20negative-sentiment%20sentences%2C%0Athat%20are%20congruent%20to%20depressed%20and%20suicidal%20participants.%20Our%20findings%0Ahighlight%20eye%20tracking%20as%20an%20objective%20tool%20for%20mental%20health%20assessment%20and%0Aunderscore%20the%20modulatory%20impact%20of%20emotional%20stimuli%20on%20cognitive%20processes%0Aaffecting%20oculomotor%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20944v1&entry.124074799=Read"},
{"title": "Optimal In-Network Distribution of Learning Functions for a\n  Secure-by-Design Programmable Data Plane of Next-Generation Networks", "author": "Mattia Giovanni Spina and Edoardo Scalzo and Floriano De Rango and Francesca Guerriero and Antonio Iera", "abstract": "  The rise of programmable data plane (PDP) and in-network computing (INC)\nparadigms paves the way for the development of network devices (switches,\nnetwork interface cards, etc.) capable of performing advanced processing tasks.\nThis allows running various types of algorithms, including machine learning,\nwithin the network itself to support user and network services. In particular,\nthis paper delves into the deployment of in-network learning models with the\naim of implementing fully distributed intrusion detection systems (IDS) or\nintrusion prevention systems (IPS). Specifically, a model is proposed for the\noptimal distribution of the IDS/IPS workload among data plane devices with the\naim of ensuring complete network security without excessively burdening the\nnormal operations of the devices. Furthermore, a meta-heuristic approach is\nproposed to reduce the long computation time required by the exact solution\nprovided by the mathematical model and its performance is evaluated. The\nanalysis conducted and the results obtained demonstrate the enormous potential\nof the proposed new approach for the creation of intelligent data planes that\nact effectively and autonomously as the first line of defense against cyber\nattacks, with minimal additional workload on the network devices involved.\n", "link": "http://arxiv.org/abs/2411.18384v2", "date": "2025-04-29", "relevancy": 1.7979, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4704}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4497}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20In-Network%20Distribution%20of%20Learning%20Functions%20for%20a%0A%20%20Secure-by-Design%20Programmable%20Data%20Plane%20of%20Next-Generation%20Networks&body=Title%3A%20Optimal%20In-Network%20Distribution%20of%20Learning%20Functions%20for%20a%0A%20%20Secure-by-Design%20Programmable%20Data%20Plane%20of%20Next-Generation%20Networks%0AAuthor%3A%20Mattia%20Giovanni%20Spina%20and%20Edoardo%20Scalzo%20and%20Floriano%20De%20Rango%20and%20Francesca%20Guerriero%20and%20Antonio%20Iera%0AAbstract%3A%20%20%20The%20rise%20of%20programmable%20data%20plane%20%28PDP%29%20and%20in-network%20computing%20%28INC%29%0Aparadigms%20paves%20the%20way%20for%20the%20development%20of%20network%20devices%20%28switches%2C%0Anetwork%20interface%20cards%2C%20etc.%29%20capable%20of%20performing%20advanced%20processing%20tasks.%0AThis%20allows%20running%20various%20types%20of%20algorithms%2C%20including%20machine%20learning%2C%0Awithin%20the%20network%20itself%20to%20support%20user%20and%20network%20services.%20In%20particular%2C%0Athis%20paper%20delves%20into%20the%20deployment%20of%20in-network%20learning%20models%20with%20the%0Aaim%20of%20implementing%20fully%20distributed%20intrusion%20detection%20systems%20%28IDS%29%20or%0Aintrusion%20prevention%20systems%20%28IPS%29.%20Specifically%2C%20a%20model%20is%20proposed%20for%20the%0Aoptimal%20distribution%20of%20the%20IDS/IPS%20workload%20among%20data%20plane%20devices%20with%20the%0Aaim%20of%20ensuring%20complete%20network%20security%20without%20excessively%20burdening%20the%0Anormal%20operations%20of%20the%20devices.%20Furthermore%2C%20a%20meta-heuristic%20approach%20is%0Aproposed%20to%20reduce%20the%20long%20computation%20time%20required%20by%20the%20exact%20solution%0Aprovided%20by%20the%20mathematical%20model%20and%20its%20performance%20is%20evaluated.%20The%0Aanalysis%20conducted%20and%20the%20results%20obtained%20demonstrate%20the%20enormous%20potential%0Aof%20the%20proposed%20new%20approach%20for%20the%20creation%20of%20intelligent%20data%20planes%20that%0Aact%20effectively%20and%20autonomously%20as%20the%20first%20line%20of%20defense%20against%20cyber%0Aattacks%2C%20with%20minimal%20additional%20workload%20on%20the%20network%20devices%20involved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.18384v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520In-Network%2520Distribution%2520of%2520Learning%2520Functions%2520for%2520a%250A%2520%2520Secure-by-Design%2520Programmable%2520Data%2520Plane%2520of%2520Next-Generation%2520Networks%26entry.906535625%3DMattia%2520Giovanni%2520Spina%2520and%2520Edoardo%2520Scalzo%2520and%2520Floriano%2520De%2520Rango%2520and%2520Francesca%2520Guerriero%2520and%2520Antonio%2520Iera%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520programmable%2520data%2520plane%2520%2528PDP%2529%2520and%2520in-network%2520computing%2520%2528INC%2529%250Aparadigms%2520paves%2520the%2520way%2520for%2520the%2520development%2520of%2520network%2520devices%2520%2528switches%252C%250Anetwork%2520interface%2520cards%252C%2520etc.%2529%2520capable%2520of%2520performing%2520advanced%2520processing%2520tasks.%250AThis%2520allows%2520running%2520various%2520types%2520of%2520algorithms%252C%2520including%2520machine%2520learning%252C%250Awithin%2520the%2520network%2520itself%2520to%2520support%2520user%2520and%2520network%2520services.%2520In%2520particular%252C%250Athis%2520paper%2520delves%2520into%2520the%2520deployment%2520of%2520in-network%2520learning%2520models%2520with%2520the%250Aaim%2520of%2520implementing%2520fully%2520distributed%2520intrusion%2520detection%2520systems%2520%2528IDS%2529%2520or%250Aintrusion%2520prevention%2520systems%2520%2528IPS%2529.%2520Specifically%252C%2520a%2520model%2520is%2520proposed%2520for%2520the%250Aoptimal%2520distribution%2520of%2520the%2520IDS/IPS%2520workload%2520among%2520data%2520plane%2520devices%2520with%2520the%250Aaim%2520of%2520ensuring%2520complete%2520network%2520security%2520without%2520excessively%2520burdening%2520the%250Anormal%2520operations%2520of%2520the%2520devices.%2520Furthermore%252C%2520a%2520meta-heuristic%2520approach%2520is%250Aproposed%2520to%2520reduce%2520the%2520long%2520computation%2520time%2520required%2520by%2520the%2520exact%2520solution%250Aprovided%2520by%2520the%2520mathematical%2520model%2520and%2520its%2520performance%2520is%2520evaluated.%2520The%250Aanalysis%2520conducted%2520and%2520the%2520results%2520obtained%2520demonstrate%2520the%2520enormous%2520potential%250Aof%2520the%2520proposed%2520new%2520approach%2520for%2520the%2520creation%2520of%2520intelligent%2520data%2520planes%2520that%250Aact%2520effectively%2520and%2520autonomously%2520as%2520the%2520first%2520line%2520of%2520defense%2520against%2520cyber%250Aattacks%252C%2520with%2520minimal%2520additional%2520workload%2520on%2520the%2520network%2520devices%2520involved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.18384v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20In-Network%20Distribution%20of%20Learning%20Functions%20for%20a%0A%20%20Secure-by-Design%20Programmable%20Data%20Plane%20of%20Next-Generation%20Networks&entry.906535625=Mattia%20Giovanni%20Spina%20and%20Edoardo%20Scalzo%20and%20Floriano%20De%20Rango%20and%20Francesca%20Guerriero%20and%20Antonio%20Iera&entry.1292438233=%20%20The%20rise%20of%20programmable%20data%20plane%20%28PDP%29%20and%20in-network%20computing%20%28INC%29%0Aparadigms%20paves%20the%20way%20for%20the%20development%20of%20network%20devices%20%28switches%2C%0Anetwork%20interface%20cards%2C%20etc.%29%20capable%20of%20performing%20advanced%20processing%20tasks.%0AThis%20allows%20running%20various%20types%20of%20algorithms%2C%20including%20machine%20learning%2C%0Awithin%20the%20network%20itself%20to%20support%20user%20and%20network%20services.%20In%20particular%2C%0Athis%20paper%20delves%20into%20the%20deployment%20of%20in-network%20learning%20models%20with%20the%0Aaim%20of%20implementing%20fully%20distributed%20intrusion%20detection%20systems%20%28IDS%29%20or%0Aintrusion%20prevention%20systems%20%28IPS%29.%20Specifically%2C%20a%20model%20is%20proposed%20for%20the%0Aoptimal%20distribution%20of%20the%20IDS/IPS%20workload%20among%20data%20plane%20devices%20with%20the%0Aaim%20of%20ensuring%20complete%20network%20security%20without%20excessively%20burdening%20the%0Anormal%20operations%20of%20the%20devices.%20Furthermore%2C%20a%20meta-heuristic%20approach%20is%0Aproposed%20to%20reduce%20the%20long%20computation%20time%20required%20by%20the%20exact%20solution%0Aprovided%20by%20the%20mathematical%20model%20and%20its%20performance%20is%20evaluated.%20The%0Aanalysis%20conducted%20and%20the%20results%20obtained%20demonstrate%20the%20enormous%20potential%0Aof%20the%20proposed%20new%20approach%20for%20the%20creation%20of%20intelligent%20data%20planes%20that%0Aact%20effectively%20and%20autonomously%20as%20the%20first%20line%20of%20defense%20against%20cyber%0Aattacks%2C%20with%20minimal%20additional%20workload%20on%20the%20network%20devices%20involved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.18384v2&entry.124074799=Read"},
{"title": "Invisible Servoing: a Visual Servoing Approach with Return-Conditioned\n  Latent Diffusion", "author": "Bishoy Gerges and Barbara Bazzana and Nicol\u00f2 Botteghi and Youssef Aboudorra and Antonio Franchi", "abstract": "  In this paper, we present a novel visual servoing (VS) approach based on\nlatent Denoising Diffusion Probabilistic Models (DDPMs), that explores the\napplication of generative models for vision-based navigation of UAVs (Uncrewed\nAerial Vehicles). Opposite to classical VS methods, the proposed approach\nallows reaching the desired target view, even when the target is initially not\nvisible. This is possible thanks to the learning of a latent representation\nthat the DDPM uses for planning and a dataset of trajectories encompassing\ntarget-invisible initial views. A compact representation is learned from raw\nimages using a Cross-Modal Variational Autoencoder. Given the current image,\nthe DDPM generates trajectories in the latent space driving the robotic\nplatform to the desired visual target. The approach has been validated in\nsimulation using two generic multi-rotor UAVs (a quadrotor and a hexarotor).\nThe results show that we can successfully reach the visual target, even if not\nvisible in the initial view.\n", "link": "http://arxiv.org/abs/2409.13337v3", "date": "2025-04-29", "relevancy": 1.7946, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6064}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invisible%20Servoing%3A%20a%20Visual%20Servoing%20Approach%20with%20Return-Conditioned%0A%20%20Latent%20Diffusion&body=Title%3A%20Invisible%20Servoing%3A%20a%20Visual%20Servoing%20Approach%20with%20Return-Conditioned%0A%20%20Latent%20Diffusion%0AAuthor%3A%20Bishoy%20Gerges%20and%20Barbara%20Bazzana%20and%20Nicol%C3%B2%20Botteghi%20and%20Youssef%20Aboudorra%20and%20Antonio%20Franchi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20visual%20servoing%20%28VS%29%20approach%20based%20on%0Alatent%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%2C%20that%20explores%20the%0Aapplication%20of%20generative%20models%20for%20vision-based%20navigation%20of%20UAVs%20%28Uncrewed%0AAerial%20Vehicles%29.%20Opposite%20to%20classical%20VS%20methods%2C%20the%20proposed%20approach%0Aallows%20reaching%20the%20desired%20target%20view%2C%20even%20when%20the%20target%20is%20initially%20not%0Avisible.%20This%20is%20possible%20thanks%20to%20the%20learning%20of%20a%20latent%20representation%0Athat%20the%20DDPM%20uses%20for%20planning%20and%20a%20dataset%20of%20trajectories%20encompassing%0Atarget-invisible%20initial%20views.%20A%20compact%20representation%20is%20learned%20from%20raw%0Aimages%20using%20a%20Cross-Modal%20Variational%20Autoencoder.%20Given%20the%20current%20image%2C%0Athe%20DDPM%20generates%20trajectories%20in%20the%20latent%20space%20driving%20the%20robotic%0Aplatform%20to%20the%20desired%20visual%20target.%20The%20approach%20has%20been%20validated%20in%0Asimulation%20using%20two%20generic%20multi-rotor%20UAVs%20%28a%20quadrotor%20and%20a%20hexarotor%29.%0AThe%20results%20show%20that%20we%20can%20successfully%20reach%20the%20visual%20target%2C%20even%20if%20not%0Avisible%20in%20the%20initial%20view.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13337v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvisible%2520Servoing%253A%2520a%2520Visual%2520Servoing%2520Approach%2520with%2520Return-Conditioned%250A%2520%2520Latent%2520Diffusion%26entry.906535625%3DBishoy%2520Gerges%2520and%2520Barbara%2520Bazzana%2520and%2520Nicol%25C3%25B2%2520Botteghi%2520and%2520Youssef%2520Aboudorra%2520and%2520Antonio%2520Franchi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520visual%2520servoing%2520%2528VS%2529%2520approach%2520based%2520on%250Alatent%2520Denoising%2520Diffusion%2520Probabilistic%2520Models%2520%2528DDPMs%2529%252C%2520that%2520explores%2520the%250Aapplication%2520of%2520generative%2520models%2520for%2520vision-based%2520navigation%2520of%2520UAVs%2520%2528Uncrewed%250AAerial%2520Vehicles%2529.%2520Opposite%2520to%2520classical%2520VS%2520methods%252C%2520the%2520proposed%2520approach%250Aallows%2520reaching%2520the%2520desired%2520target%2520view%252C%2520even%2520when%2520the%2520target%2520is%2520initially%2520not%250Avisible.%2520This%2520is%2520possible%2520thanks%2520to%2520the%2520learning%2520of%2520a%2520latent%2520representation%250Athat%2520the%2520DDPM%2520uses%2520for%2520planning%2520and%2520a%2520dataset%2520of%2520trajectories%2520encompassing%250Atarget-invisible%2520initial%2520views.%2520A%2520compact%2520representation%2520is%2520learned%2520from%2520raw%250Aimages%2520using%2520a%2520Cross-Modal%2520Variational%2520Autoencoder.%2520Given%2520the%2520current%2520image%252C%250Athe%2520DDPM%2520generates%2520trajectories%2520in%2520the%2520latent%2520space%2520driving%2520the%2520robotic%250Aplatform%2520to%2520the%2520desired%2520visual%2520target.%2520The%2520approach%2520has%2520been%2520validated%2520in%250Asimulation%2520using%2520two%2520generic%2520multi-rotor%2520UAVs%2520%2528a%2520quadrotor%2520and%2520a%2520hexarotor%2529.%250AThe%2520results%2520show%2520that%2520we%2520can%2520successfully%2520reach%2520the%2520visual%2520target%252C%2520even%2520if%2520not%250Avisible%2520in%2520the%2520initial%2520view.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13337v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invisible%20Servoing%3A%20a%20Visual%20Servoing%20Approach%20with%20Return-Conditioned%0A%20%20Latent%20Diffusion&entry.906535625=Bishoy%20Gerges%20and%20Barbara%20Bazzana%20and%20Nicol%C3%B2%20Botteghi%20and%20Youssef%20Aboudorra%20and%20Antonio%20Franchi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20visual%20servoing%20%28VS%29%20approach%20based%20on%0Alatent%20Denoising%20Diffusion%20Probabilistic%20Models%20%28DDPMs%29%2C%20that%20explores%20the%0Aapplication%20of%20generative%20models%20for%20vision-based%20navigation%20of%20UAVs%20%28Uncrewed%0AAerial%20Vehicles%29.%20Opposite%20to%20classical%20VS%20methods%2C%20the%20proposed%20approach%0Aallows%20reaching%20the%20desired%20target%20view%2C%20even%20when%20the%20target%20is%20initially%20not%0Avisible.%20This%20is%20possible%20thanks%20to%20the%20learning%20of%20a%20latent%20representation%0Athat%20the%20DDPM%20uses%20for%20planning%20and%20a%20dataset%20of%20trajectories%20encompassing%0Atarget-invisible%20initial%20views.%20A%20compact%20representation%20is%20learned%20from%20raw%0Aimages%20using%20a%20Cross-Modal%20Variational%20Autoencoder.%20Given%20the%20current%20image%2C%0Athe%20DDPM%20generates%20trajectories%20in%20the%20latent%20space%20driving%20the%20robotic%0Aplatform%20to%20the%20desired%20visual%20target.%20The%20approach%20has%20been%20validated%20in%0Asimulation%20using%20two%20generic%20multi-rotor%20UAVs%20%28a%20quadrotor%20and%20a%20hexarotor%29.%0AThe%20results%20show%20that%20we%20can%20successfully%20reach%20the%20visual%20target%2C%20even%20if%20not%0Avisible%20in%20the%20initial%20view.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13337v3&entry.124074799=Read"},
{"title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for\n  Operating System Verification", "author": "Shangyu Li and Juyong Jiang and Tiancheng Zhao and Jiasi Shen", "abstract": "  We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench.\n", "link": "http://arxiv.org/abs/2504.20964v1", "date": "2025-04-29", "relevancy": 1.7772, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OSVBench%3A%20Benchmarking%20LLMs%20on%20Specification%20Generation%20Tasks%20for%0A%20%20Operating%20System%20Verification&body=Title%3A%20OSVBench%3A%20Benchmarking%20LLMs%20on%20Specification%20Generation%20Tasks%20for%0A%20%20Operating%20System%20Verification%0AAuthor%3A%20Shangyu%20Li%20and%20Juyong%20Jiang%20and%20Tiancheng%20Zhao%20and%20Jiasi%20Shen%0AAbstract%3A%20%20%20We%20introduce%20OSVBench%2C%20a%20new%20benchmark%20for%20evaluating%20Large%20Language%20Models%0A%28LLMs%29%20in%20generating%20complete%20specification%20code%20pertaining%20to%20operating%20system%0Akernel%20verification%20tasks.%20The%20benchmark%20first%20defines%20the%20specification%0Ageneration%20problem%20into%20a%20program%20synthesis%20problem%20within%20a%20confined%20scope%20of%0Asyntax%20and%20semantics%20by%20providing%20LLMs%20with%20the%20programming%20model.%20The%20LLMs%20are%0Arequired%20to%20understand%20the%20provided%20verification%20assumption%20and%20the%20potential%0Asyntax%20and%20semantics%20space%20to%20search%20for%2C%20then%20generate%20the%20complete%0Aspecification%20for%20the%20potentially%20buggy%20operating%20system%20code%20implementation%0Aunder%20the%20guidance%20of%20the%20high-level%20functional%20description%20of%20the%20operating%0Asystem.%20This%20benchmark%20is%20built%20upon%20a%20real-world%20operating%20system%20kernel%2C%0AHyperkernel%2C%20and%20consists%20of%20245%20complex%20specification%20generation%20tasks%20in%0Atotal%2C%20each%20is%20a%20long%20context%20task%20of%20about%2020k-30k%20tokens.%20Our%20comprehensive%0Aevaluation%20of%2012%20LLMs%20exhibits%20the%20limited%20performance%20of%20the%20current%20LLMs%20on%0Athe%20specification%20generation%20tasks%20for%20operating%20system%20verification.%0ASignificant%20disparities%20in%20their%20performance%20on%20the%20benchmark%20highlight%0Adifferences%20in%20their%20ability%20to%20handle%20long-context%20code%20generation%20tasks.%20The%0Aevaluation%20toolkit%20and%20benchmark%20are%20available%20at%0Ahttps%3A//github.com/lishangyu-hkust/OSVBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOSVBench%253A%2520Benchmarking%2520LLMs%2520on%2520Specification%2520Generation%2520Tasks%2520for%250A%2520%2520Operating%2520System%2520Verification%26entry.906535625%3DShangyu%2520Li%2520and%2520Juyong%2520Jiang%2520and%2520Tiancheng%2520Zhao%2520and%2520Jiasi%2520Shen%26entry.1292438233%3D%2520%2520We%2520introduce%2520OSVBench%252C%2520a%2520new%2520benchmark%2520for%2520evaluating%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520in%2520generating%2520complete%2520specification%2520code%2520pertaining%2520to%2520operating%2520system%250Akernel%2520verification%2520tasks.%2520The%2520benchmark%2520first%2520defines%2520the%2520specification%250Ageneration%2520problem%2520into%2520a%2520program%2520synthesis%2520problem%2520within%2520a%2520confined%2520scope%2520of%250Asyntax%2520and%2520semantics%2520by%2520providing%2520LLMs%2520with%2520the%2520programming%2520model.%2520The%2520LLMs%2520are%250Arequired%2520to%2520understand%2520the%2520provided%2520verification%2520assumption%2520and%2520the%2520potential%250Asyntax%2520and%2520semantics%2520space%2520to%2520search%2520for%252C%2520then%2520generate%2520the%2520complete%250Aspecification%2520for%2520the%2520potentially%2520buggy%2520operating%2520system%2520code%2520implementation%250Aunder%2520the%2520guidance%2520of%2520the%2520high-level%2520functional%2520description%2520of%2520the%2520operating%250Asystem.%2520This%2520benchmark%2520is%2520built%2520upon%2520a%2520real-world%2520operating%2520system%2520kernel%252C%250AHyperkernel%252C%2520and%2520consists%2520of%2520245%2520complex%2520specification%2520generation%2520tasks%2520in%250Atotal%252C%2520each%2520is%2520a%2520long%2520context%2520task%2520of%2520about%252020k-30k%2520tokens.%2520Our%2520comprehensive%250Aevaluation%2520of%252012%2520LLMs%2520exhibits%2520the%2520limited%2520performance%2520of%2520the%2520current%2520LLMs%2520on%250Athe%2520specification%2520generation%2520tasks%2520for%2520operating%2520system%2520verification.%250ASignificant%2520disparities%2520in%2520their%2520performance%2520on%2520the%2520benchmark%2520highlight%250Adifferences%2520in%2520their%2520ability%2520to%2520handle%2520long-context%2520code%2520generation%2520tasks.%2520The%250Aevaluation%2520toolkit%2520and%2520benchmark%2520are%2520available%2520at%250Ahttps%253A//github.com/lishangyu-hkust/OSVBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OSVBench%3A%20Benchmarking%20LLMs%20on%20Specification%20Generation%20Tasks%20for%0A%20%20Operating%20System%20Verification&entry.906535625=Shangyu%20Li%20and%20Juyong%20Jiang%20and%20Tiancheng%20Zhao%20and%20Jiasi%20Shen&entry.1292438233=%20%20We%20introduce%20OSVBench%2C%20a%20new%20benchmark%20for%20evaluating%20Large%20Language%20Models%0A%28LLMs%29%20in%20generating%20complete%20specification%20code%20pertaining%20to%20operating%20system%0Akernel%20verification%20tasks.%20The%20benchmark%20first%20defines%20the%20specification%0Ageneration%20problem%20into%20a%20program%20synthesis%20problem%20within%20a%20confined%20scope%20of%0Asyntax%20and%20semantics%20by%20providing%20LLMs%20with%20the%20programming%20model.%20The%20LLMs%20are%0Arequired%20to%20understand%20the%20provided%20verification%20assumption%20and%20the%20potential%0Asyntax%20and%20semantics%20space%20to%20search%20for%2C%20then%20generate%20the%20complete%0Aspecification%20for%20the%20potentially%20buggy%20operating%20system%20code%20implementation%0Aunder%20the%20guidance%20of%20the%20high-level%20functional%20description%20of%20the%20operating%0Asystem.%20This%20benchmark%20is%20built%20upon%20a%20real-world%20operating%20system%20kernel%2C%0AHyperkernel%2C%20and%20consists%20of%20245%20complex%20specification%20generation%20tasks%20in%0Atotal%2C%20each%20is%20a%20long%20context%20task%20of%20about%2020k-30k%20tokens.%20Our%20comprehensive%0Aevaluation%20of%2012%20LLMs%20exhibits%20the%20limited%20performance%20of%20the%20current%20LLMs%20on%0Athe%20specification%20generation%20tasks%20for%20operating%20system%20verification.%0ASignificant%20disparities%20in%20their%20performance%20on%20the%20benchmark%20highlight%0Adifferences%20in%20their%20ability%20to%20handle%20long-context%20code%20generation%20tasks.%20The%0Aevaluation%20toolkit%20and%20benchmark%20are%20available%20at%0Ahttps%3A//github.com/lishangyu-hkust/OSVBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20964v1&entry.124074799=Read"},
{"title": "Activated LoRA: Fine-tuned LLMs for Intrinsics", "author": "Kristjan Greenewald and Luis Lastras and Thomas Parnell and Vraj Shah and Lucian Popa and Giulio Zizzo and Chulaka Gunasekara and Ambrish Rawat and David Cox", "abstract": "  Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.\n", "link": "http://arxiv.org/abs/2504.12397v2", "date": "2025-04-29", "relevancy": 1.7671, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.448}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activated%20LoRA%3A%20Fine-tuned%20LLMs%20for%20Intrinsics&body=Title%3A%20Activated%20LoRA%3A%20Fine-tuned%20LLMs%20for%20Intrinsics%0AAuthor%3A%20Kristjan%20Greenewald%20and%20Luis%20Lastras%20and%20Thomas%20Parnell%20and%20Vraj%20Shah%20and%20Lucian%20Popa%20and%20Giulio%20Zizzo%20and%20Chulaka%20Gunasekara%20and%20Ambrish%20Rawat%20and%20David%20Cox%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20emerged%20as%20a%20highly%20efficient%20framework%20for%0Afinetuning%20the%20weights%20of%20large%20foundation%20models%2C%20and%20has%20become%20the%20go-to%0Amethod%20for%20data-driven%20customization%20of%20LLMs.%20Despite%20the%20promise%20of%20highly%0Acustomized%20behaviors%20and%20capabilities%2C%20switching%20between%20relevant%20LoRAs%20in%20a%0Amultiturn%20setting%20is%20highly%20inefficient%2C%20as%20the%20key-value%20%28KV%29%20cache%20of%20the%0Aentire%20turn%20history%20must%20be%20recomputed%20with%20the%20LoRA%20weights%20before%20generation%0Acan%20begin.%20To%20address%20this%20problem%2C%20we%20propose%20Activated%20LoRA%20%28aLoRA%29%2C%20which%0Amodifies%20the%20LoRA%20framework%20to%20only%20adapt%20weights%20for%20the%20tokens%20in%20the%0Asequence%20%5Cemph%7Bafter%7D%20the%20aLoRA%20is%20invoked.%20This%20change%20crucially%20allows%20aLoRA%0Ato%20accept%20the%20base%20model%27s%20KV%20cache%20of%20the%20input%20string%2C%20meaning%20that%20aLoRA%20can%0Abe%20instantly%20activated%20whenever%20needed%20in%20a%20chain%20without%20recomputing%20the%0Acache.%20This%20enables%20building%20what%20we%20call%20%5Cemph%7Bintrinsics%7D%2C%20i.e.%20highly%0Aspecialized%20models%20invoked%20to%20perform%20well-defined%20operations%20on%20portions%20of%20an%0Ainput%20chain%20or%20conversation%20that%20otherwise%20uses%20the%20base%20model%20by%20default.%20We%0Ause%20aLoRA%20to%20train%20a%20set%20of%20intrinsics%20models%2C%20demonstrating%20competitive%0Aaccuracy%20with%20standard%20LoRA%20while%20achieving%20significant%20inference%20benefits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.12397v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivated%2520LoRA%253A%2520Fine-tuned%2520LLMs%2520for%2520Intrinsics%26entry.906535625%3DKristjan%2520Greenewald%2520and%2520Luis%2520Lastras%2520and%2520Thomas%2520Parnell%2520and%2520Vraj%2520Shah%2520and%2520Lucian%2520Popa%2520and%2520Giulio%2520Zizzo%2520and%2520Chulaka%2520Gunasekara%2520and%2520Ambrish%2520Rawat%2520and%2520David%2520Cox%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520has%2520emerged%2520as%2520a%2520highly%2520efficient%2520framework%2520for%250Afinetuning%2520the%2520weights%2520of%2520large%2520foundation%2520models%252C%2520and%2520has%2520become%2520the%2520go-to%250Amethod%2520for%2520data-driven%2520customization%2520of%2520LLMs.%2520Despite%2520the%2520promise%2520of%2520highly%250Acustomized%2520behaviors%2520and%2520capabilities%252C%2520switching%2520between%2520relevant%2520LoRAs%2520in%2520a%250Amultiturn%2520setting%2520is%2520highly%2520inefficient%252C%2520as%2520the%2520key-value%2520%2528KV%2529%2520cache%2520of%2520the%250Aentire%2520turn%2520history%2520must%2520be%2520recomputed%2520with%2520the%2520LoRA%2520weights%2520before%2520generation%250Acan%2520begin.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520Activated%2520LoRA%2520%2528aLoRA%2529%252C%2520which%250Amodifies%2520the%2520LoRA%2520framework%2520to%2520only%2520adapt%2520weights%2520for%2520the%2520tokens%2520in%2520the%250Asequence%2520%255Cemph%257Bafter%257D%2520the%2520aLoRA%2520is%2520invoked.%2520This%2520change%2520crucially%2520allows%2520aLoRA%250Ato%2520accept%2520the%2520base%2520model%2527s%2520KV%2520cache%2520of%2520the%2520input%2520string%252C%2520meaning%2520that%2520aLoRA%2520can%250Abe%2520instantly%2520activated%2520whenever%2520needed%2520in%2520a%2520chain%2520without%2520recomputing%2520the%250Acache.%2520This%2520enables%2520building%2520what%2520we%2520call%2520%255Cemph%257Bintrinsics%257D%252C%2520i.e.%2520highly%250Aspecialized%2520models%2520invoked%2520to%2520perform%2520well-defined%2520operations%2520on%2520portions%2520of%2520an%250Ainput%2520chain%2520or%2520conversation%2520that%2520otherwise%2520uses%2520the%2520base%2520model%2520by%2520default.%2520We%250Ause%2520aLoRA%2520to%2520train%2520a%2520set%2520of%2520intrinsics%2520models%252C%2520demonstrating%2520competitive%250Aaccuracy%2520with%2520standard%2520LoRA%2520while%2520achieving%2520significant%2520inference%2520benefits.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.12397v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activated%20LoRA%3A%20Fine-tuned%20LLMs%20for%20Intrinsics&entry.906535625=Kristjan%20Greenewald%20and%20Luis%20Lastras%20and%20Thomas%20Parnell%20and%20Vraj%20Shah%20and%20Lucian%20Popa%20and%20Giulio%20Zizzo%20and%20Chulaka%20Gunasekara%20and%20Ambrish%20Rawat%20and%20David%20Cox&entry.1292438233=%20%20Low-Rank%20Adaptation%20%28LoRA%29%20has%20emerged%20as%20a%20highly%20efficient%20framework%20for%0Afinetuning%20the%20weights%20of%20large%20foundation%20models%2C%20and%20has%20become%20the%20go-to%0Amethod%20for%20data-driven%20customization%20of%20LLMs.%20Despite%20the%20promise%20of%20highly%0Acustomized%20behaviors%20and%20capabilities%2C%20switching%20between%20relevant%20LoRAs%20in%20a%0Amultiturn%20setting%20is%20highly%20inefficient%2C%20as%20the%20key-value%20%28KV%29%20cache%20of%20the%0Aentire%20turn%20history%20must%20be%20recomputed%20with%20the%20LoRA%20weights%20before%20generation%0Acan%20begin.%20To%20address%20this%20problem%2C%20we%20propose%20Activated%20LoRA%20%28aLoRA%29%2C%20which%0Amodifies%20the%20LoRA%20framework%20to%20only%20adapt%20weights%20for%20the%20tokens%20in%20the%0Asequence%20%5Cemph%7Bafter%7D%20the%20aLoRA%20is%20invoked.%20This%20change%20crucially%20allows%20aLoRA%0Ato%20accept%20the%20base%20model%27s%20KV%20cache%20of%20the%20input%20string%2C%20meaning%20that%20aLoRA%20can%0Abe%20instantly%20activated%20whenever%20needed%20in%20a%20chain%20without%20recomputing%20the%0Acache.%20This%20enables%20building%20what%20we%20call%20%5Cemph%7Bintrinsics%7D%2C%20i.e.%20highly%0Aspecialized%20models%20invoked%20to%20perform%20well-defined%20operations%20on%20portions%20of%20an%0Ainput%20chain%20or%20conversation%20that%20otherwise%20uses%20the%20base%20model%20by%20default.%20We%0Ause%20aLoRA%20to%20train%20a%20set%20of%20intrinsics%20models%2C%20demonstrating%20competitive%0Aaccuracy%20with%20standard%20LoRA%20while%20achieving%20significant%20inference%20benefits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.12397v2&entry.124074799=Read"},
{"title": "Guessing Efficiently for Constrained Subspace Approximation", "author": "Aditya Bhaskara and Sepideh Mahabadi and Madhusudhan Reddy Pittu and Ali Vakilian and David P. Woodruff", "abstract": "  In this paper we study constrained subspace approximation problem. Given a\nset of $n$ points $\\{a_1,\\ldots,a_n\\}$ in $\\mathbb{R}^d$, the goal of the {\\em\nsubspace approximation} problem is to find a $k$ dimensional subspace that best\napproximates the input points. More precisely, for a given $p\\geq 1$, we aim to\nminimize the $p$th power of the $\\ell_p$ norm of the error vector\n$(\\|a_1-\\bm{P}a_1\\|,\\ldots,\\|a_n-\\bm{P}a_n\\|)$, where $\\bm{P}$ denotes the\nprojection matrix onto the subspace and the norms are Euclidean. In\n\\emph{constrained} subspace approximation (CSA), we additionally have\nconstraints on the projection matrix $\\bm{P}$. In its most general form, we\nrequire $\\bm{P}$ to belong to a given subset $\\mathcal{S}$ that is described\nexplicitly or implicitly.\n  We introduce a general framework for constrained subspace approximation. Our\napproach, that we term coreset-guess-solve, yields either\n$(1+\\varepsilon)$-multiplicative or $\\varepsilon$-additive approximations for a\nvariety of constraints. We show that it provides new algorithms for\npartition-constrained subspace approximation with applications to {\\it fair}\nsubspace approximation, $k$-means clustering, and projected non-negative matrix\nfactorization, among others. Specifically, while we reconstruct the best known\nbounds for $k$-means clustering in Euclidean spaces, we improve the known\nresults for the remainder of the problems.\n", "link": "http://arxiv.org/abs/2504.20883v1", "date": "2025-04-29", "relevancy": 1.7572, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4446}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.442}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guessing%20Efficiently%20for%20Constrained%20Subspace%20Approximation&body=Title%3A%20Guessing%20Efficiently%20for%20Constrained%20Subspace%20Approximation%0AAuthor%3A%20Aditya%20Bhaskara%20and%20Sepideh%20Mahabadi%20and%20Madhusudhan%20Reddy%20Pittu%20and%20Ali%20Vakilian%20and%20David%20P.%20Woodruff%0AAbstract%3A%20%20%20In%20this%20paper%20we%20study%20constrained%20subspace%20approximation%20problem.%20Given%20a%0Aset%20of%20%24n%24%20points%20%24%5C%7Ba_1%2C%5Cldots%2Ca_n%5C%7D%24%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20the%20goal%20of%20the%20%7B%5Cem%0Asubspace%20approximation%7D%20problem%20is%20to%20find%20a%20%24k%24%20dimensional%20subspace%20that%20best%0Aapproximates%20the%20input%20points.%20More%20precisely%2C%20for%20a%20given%20%24p%5Cgeq%201%24%2C%20we%20aim%20to%0Aminimize%20the%20%24p%24th%20power%20of%20the%20%24%5Cell_p%24%20norm%20of%20the%20error%20vector%0A%24%28%5C%7Ca_1-%5Cbm%7BP%7Da_1%5C%7C%2C%5Cldots%2C%5C%7Ca_n-%5Cbm%7BP%7Da_n%5C%7C%29%24%2C%20where%20%24%5Cbm%7BP%7D%24%20denotes%20the%0Aprojection%20matrix%20onto%20the%20subspace%20and%20the%20norms%20are%20Euclidean.%20In%0A%5Cemph%7Bconstrained%7D%20subspace%20approximation%20%28CSA%29%2C%20we%20additionally%20have%0Aconstraints%20on%20the%20projection%20matrix%20%24%5Cbm%7BP%7D%24.%20In%20its%20most%20general%20form%2C%20we%0Arequire%20%24%5Cbm%7BP%7D%24%20to%20belong%20to%20a%20given%20subset%20%24%5Cmathcal%7BS%7D%24%20that%20is%20described%0Aexplicitly%20or%20implicitly.%0A%20%20We%20introduce%20a%20general%20framework%20for%20constrained%20subspace%20approximation.%20Our%0Aapproach%2C%20that%20we%20term%20coreset-guess-solve%2C%20yields%20either%0A%24%281%2B%5Cvarepsilon%29%24-multiplicative%20or%20%24%5Cvarepsilon%24-additive%20approximations%20for%20a%0Avariety%20of%20constraints.%20We%20show%20that%20it%20provides%20new%20algorithms%20for%0Apartition-constrained%20subspace%20approximation%20with%20applications%20to%20%7B%5Cit%20fair%7D%0Asubspace%20approximation%2C%20%24k%24-means%20clustering%2C%20and%20projected%20non-negative%20matrix%0Afactorization%2C%20among%20others.%20Specifically%2C%20while%20we%20reconstruct%20the%20best%20known%0Abounds%20for%20%24k%24-means%20clustering%20in%20Euclidean%20spaces%2C%20we%20improve%20the%20known%0Aresults%20for%20the%20remainder%20of%20the%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuessing%2520Efficiently%2520for%2520Constrained%2520Subspace%2520Approximation%26entry.906535625%3DAditya%2520Bhaskara%2520and%2520Sepideh%2520Mahabadi%2520and%2520Madhusudhan%2520Reddy%2520Pittu%2520and%2520Ali%2520Vakilian%2520and%2520David%2520P.%2520Woodruff%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520study%2520constrained%2520subspace%2520approximation%2520problem.%2520Given%2520a%250Aset%2520of%2520%2524n%2524%2520points%2520%2524%255C%257Ba_1%252C%255Cldots%252Ca_n%255C%257D%2524%2520in%2520%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520the%2520goal%2520of%2520the%2520%257B%255Cem%250Asubspace%2520approximation%257D%2520problem%2520is%2520to%2520find%2520a%2520%2524k%2524%2520dimensional%2520subspace%2520that%2520best%250Aapproximates%2520the%2520input%2520points.%2520More%2520precisely%252C%2520for%2520a%2520given%2520%2524p%255Cgeq%25201%2524%252C%2520we%2520aim%2520to%250Aminimize%2520the%2520%2524p%2524th%2520power%2520of%2520the%2520%2524%255Cell_p%2524%2520norm%2520of%2520the%2520error%2520vector%250A%2524%2528%255C%257Ca_1-%255Cbm%257BP%257Da_1%255C%257C%252C%255Cldots%252C%255C%257Ca_n-%255Cbm%257BP%257Da_n%255C%257C%2529%2524%252C%2520where%2520%2524%255Cbm%257BP%257D%2524%2520denotes%2520the%250Aprojection%2520matrix%2520onto%2520the%2520subspace%2520and%2520the%2520norms%2520are%2520Euclidean.%2520In%250A%255Cemph%257Bconstrained%257D%2520subspace%2520approximation%2520%2528CSA%2529%252C%2520we%2520additionally%2520have%250Aconstraints%2520on%2520the%2520projection%2520matrix%2520%2524%255Cbm%257BP%257D%2524.%2520In%2520its%2520most%2520general%2520form%252C%2520we%250Arequire%2520%2524%255Cbm%257BP%257D%2524%2520to%2520belong%2520to%2520a%2520given%2520subset%2520%2524%255Cmathcal%257BS%257D%2524%2520that%2520is%2520described%250Aexplicitly%2520or%2520implicitly.%250A%2520%2520We%2520introduce%2520a%2520general%2520framework%2520for%2520constrained%2520subspace%2520approximation.%2520Our%250Aapproach%252C%2520that%2520we%2520term%2520coreset-guess-solve%252C%2520yields%2520either%250A%2524%25281%252B%255Cvarepsilon%2529%2524-multiplicative%2520or%2520%2524%255Cvarepsilon%2524-additive%2520approximations%2520for%2520a%250Avariety%2520of%2520constraints.%2520We%2520show%2520that%2520it%2520provides%2520new%2520algorithms%2520for%250Apartition-constrained%2520subspace%2520approximation%2520with%2520applications%2520to%2520%257B%255Cit%2520fair%257D%250Asubspace%2520approximation%252C%2520%2524k%2524-means%2520clustering%252C%2520and%2520projected%2520non-negative%2520matrix%250Afactorization%252C%2520among%2520others.%2520Specifically%252C%2520while%2520we%2520reconstruct%2520the%2520best%2520known%250Abounds%2520for%2520%2524k%2524-means%2520clustering%2520in%2520Euclidean%2520spaces%252C%2520we%2520improve%2520the%2520known%250Aresults%2520for%2520the%2520remainder%2520of%2520the%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guessing%20Efficiently%20for%20Constrained%20Subspace%20Approximation&entry.906535625=Aditya%20Bhaskara%20and%20Sepideh%20Mahabadi%20and%20Madhusudhan%20Reddy%20Pittu%20and%20Ali%20Vakilian%20and%20David%20P.%20Woodruff&entry.1292438233=%20%20In%20this%20paper%20we%20study%20constrained%20subspace%20approximation%20problem.%20Given%20a%0Aset%20of%20%24n%24%20points%20%24%5C%7Ba_1%2C%5Cldots%2Ca_n%5C%7D%24%20in%20%24%5Cmathbb%7BR%7D%5Ed%24%2C%20the%20goal%20of%20the%20%7B%5Cem%0Asubspace%20approximation%7D%20problem%20is%20to%20find%20a%20%24k%24%20dimensional%20subspace%20that%20best%0Aapproximates%20the%20input%20points.%20More%20precisely%2C%20for%20a%20given%20%24p%5Cgeq%201%24%2C%20we%20aim%20to%0Aminimize%20the%20%24p%24th%20power%20of%20the%20%24%5Cell_p%24%20norm%20of%20the%20error%20vector%0A%24%28%5C%7Ca_1-%5Cbm%7BP%7Da_1%5C%7C%2C%5Cldots%2C%5C%7Ca_n-%5Cbm%7BP%7Da_n%5C%7C%29%24%2C%20where%20%24%5Cbm%7BP%7D%24%20denotes%20the%0Aprojection%20matrix%20onto%20the%20subspace%20and%20the%20norms%20are%20Euclidean.%20In%0A%5Cemph%7Bconstrained%7D%20subspace%20approximation%20%28CSA%29%2C%20we%20additionally%20have%0Aconstraints%20on%20the%20projection%20matrix%20%24%5Cbm%7BP%7D%24.%20In%20its%20most%20general%20form%2C%20we%0Arequire%20%24%5Cbm%7BP%7D%24%20to%20belong%20to%20a%20given%20subset%20%24%5Cmathcal%7BS%7D%24%20that%20is%20described%0Aexplicitly%20or%20implicitly.%0A%20%20We%20introduce%20a%20general%20framework%20for%20constrained%20subspace%20approximation.%20Our%0Aapproach%2C%20that%20we%20term%20coreset-guess-solve%2C%20yields%20either%0A%24%281%2B%5Cvarepsilon%29%24-multiplicative%20or%20%24%5Cvarepsilon%24-additive%20approximations%20for%20a%0Avariety%20of%20constraints.%20We%20show%20that%20it%20provides%20new%20algorithms%20for%0Apartition-constrained%20subspace%20approximation%20with%20applications%20to%20%7B%5Cit%20fair%7D%0Asubspace%20approximation%2C%20%24k%24-means%20clustering%2C%20and%20projected%20non-negative%20matrix%0Afactorization%2C%20among%20others.%20Specifically%2C%20while%20we%20reconstruct%20the%20best%20known%0Abounds%20for%20%24k%24-means%20clustering%20in%20Euclidean%20spaces%2C%20we%20improve%20the%20known%0Aresults%20for%20the%20remainder%20of%20the%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20883v1&entry.124074799=Read"},
{"title": "Disjunctive and Conjunctive Normal Form Explanations of Clusters Using\n  Auxiliary Information", "author": "Robert F. Downey and S. S. Ravi", "abstract": "  We consider generating post-hoc explanations of clusters generated from\nvarious datasets using auxiliary information which was not used by clustering\nalgorithms. Following terminology used in previous work, we refer to the\nauxiliary information as tags. Our focus is on two forms of explanations,\nnamely disjunctive form (where the explanation for a cluster consists of a set\nof tags) and a two-clause conjunctive normal form (CNF) explanation (where the\nexplanation consists of two sets of tags, combined through the AND operator).\nWe use integer linear programming (ILP) as well as heuristic methods to\ngenerate these explanations. We experiment with a variety of datasets and\ndiscuss the insights obtained from our explanations. We also present\nexperimental results regarding the scalability of our explanation methods.\n", "link": "http://arxiv.org/abs/2504.20846v1", "date": "2025-04-29", "relevancy": 1.7444, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disjunctive%20and%20Conjunctive%20Normal%20Form%20Explanations%20of%20Clusters%20Using%0A%20%20Auxiliary%20Information&body=Title%3A%20Disjunctive%20and%20Conjunctive%20Normal%20Form%20Explanations%20of%20Clusters%20Using%0A%20%20Auxiliary%20Information%0AAuthor%3A%20Robert%20F.%20Downey%20and%20S.%20S.%20Ravi%0AAbstract%3A%20%20%20We%20consider%20generating%20post-hoc%20explanations%20of%20clusters%20generated%20from%0Avarious%20datasets%20using%20auxiliary%20information%20which%20was%20not%20used%20by%20clustering%0Aalgorithms.%20Following%20terminology%20used%20in%20previous%20work%2C%20we%20refer%20to%20the%0Aauxiliary%20information%20as%20tags.%20Our%20focus%20is%20on%20two%20forms%20of%20explanations%2C%0Anamely%20disjunctive%20form%20%28where%20the%20explanation%20for%20a%20cluster%20consists%20of%20a%20set%0Aof%20tags%29%20and%20a%20two-clause%20conjunctive%20normal%20form%20%28CNF%29%20explanation%20%28where%20the%0Aexplanation%20consists%20of%20two%20sets%20of%20tags%2C%20combined%20through%20the%20AND%20operator%29.%0AWe%20use%20integer%20linear%20programming%20%28ILP%29%20as%20well%20as%20heuristic%20methods%20to%0Agenerate%20these%20explanations.%20We%20experiment%20with%20a%20variety%20of%20datasets%20and%0Adiscuss%20the%20insights%20obtained%20from%20our%20explanations.%20We%20also%20present%0Aexperimental%20results%20regarding%20the%20scalability%20of%20our%20explanation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisjunctive%2520and%2520Conjunctive%2520Normal%2520Form%2520Explanations%2520of%2520Clusters%2520Using%250A%2520%2520Auxiliary%2520Information%26entry.906535625%3DRobert%2520F.%2520Downey%2520and%2520S.%2520S.%2520Ravi%26entry.1292438233%3D%2520%2520We%2520consider%2520generating%2520post-hoc%2520explanations%2520of%2520clusters%2520generated%2520from%250Avarious%2520datasets%2520using%2520auxiliary%2520information%2520which%2520was%2520not%2520used%2520by%2520clustering%250Aalgorithms.%2520Following%2520terminology%2520used%2520in%2520previous%2520work%252C%2520we%2520refer%2520to%2520the%250Aauxiliary%2520information%2520as%2520tags.%2520Our%2520focus%2520is%2520on%2520two%2520forms%2520of%2520explanations%252C%250Anamely%2520disjunctive%2520form%2520%2528where%2520the%2520explanation%2520for%2520a%2520cluster%2520consists%2520of%2520a%2520set%250Aof%2520tags%2529%2520and%2520a%2520two-clause%2520conjunctive%2520normal%2520form%2520%2528CNF%2529%2520explanation%2520%2528where%2520the%250Aexplanation%2520consists%2520of%2520two%2520sets%2520of%2520tags%252C%2520combined%2520through%2520the%2520AND%2520operator%2529.%250AWe%2520use%2520integer%2520linear%2520programming%2520%2528ILP%2529%2520as%2520well%2520as%2520heuristic%2520methods%2520to%250Agenerate%2520these%2520explanations.%2520We%2520experiment%2520with%2520a%2520variety%2520of%2520datasets%2520and%250Adiscuss%2520the%2520insights%2520obtained%2520from%2520our%2520explanations.%2520We%2520also%2520present%250Aexperimental%2520results%2520regarding%2520the%2520scalability%2520of%2520our%2520explanation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disjunctive%20and%20Conjunctive%20Normal%20Form%20Explanations%20of%20Clusters%20Using%0A%20%20Auxiliary%20Information&entry.906535625=Robert%20F.%20Downey%20and%20S.%20S.%20Ravi&entry.1292438233=%20%20We%20consider%20generating%20post-hoc%20explanations%20of%20clusters%20generated%20from%0Avarious%20datasets%20using%20auxiliary%20information%20which%20was%20not%20used%20by%20clustering%0Aalgorithms.%20Following%20terminology%20used%20in%20previous%20work%2C%20we%20refer%20to%20the%0Aauxiliary%20information%20as%20tags.%20Our%20focus%20is%20on%20two%20forms%20of%20explanations%2C%0Anamely%20disjunctive%20form%20%28where%20the%20explanation%20for%20a%20cluster%20consists%20of%20a%20set%0Aof%20tags%29%20and%20a%20two-clause%20conjunctive%20normal%20form%20%28CNF%29%20explanation%20%28where%20the%0Aexplanation%20consists%20of%20two%20sets%20of%20tags%2C%20combined%20through%20the%20AND%20operator%29.%0AWe%20use%20integer%20linear%20programming%20%28ILP%29%20as%20well%20as%20heuristic%20methods%20to%0Agenerate%20these%20explanations.%20We%20experiment%20with%20a%20variety%20of%20datasets%20and%0Adiscuss%20the%20insights%20obtained%20from%20our%20explanations.%20We%20also%20present%0Aexperimental%20results%20regarding%20the%20scalability%20of%20our%20explanation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20846v1&entry.124074799=Read"},
{"title": "Statistical and Predictive Analysis to Identify Risk Factors and Effects\n  of Post COVID-19 Syndrome", "author": "Milad Leyli-abadi and Jean-Patrick Brunet and Axel Tahmasebimoradi", "abstract": "  Based on recent studies, some COVID-19 symptoms can persist for months after\ninfection, leading to what is termed long COVID. Factors such as vaccination\ntiming, patient characteristics, and symptoms during the acute phase of\ninfection may contribute to the prolonged effects and intensity of long COVID.\nEach patient, based on their unique combination of factors, develops a specific\nrisk or intensity of long COVID. In this work, we aim to achieve two\nobjectives: (1) conduct a statistical analysis to identify relationships\nbetween various factors and long COVID, and (2) perform predictive analysis of\nlong COVID intensity using these factors. We benchmark and interpret various\ndata-driven approaches, including linear models, random forests, gradient\nboosting, and neural networks, using data from the Lifelines COVID-19 cohort.\nOur results show that Neural Networks (NN) achieve the best performance in\nterms of MAPE, with predictions averaging 19\\% error. Additionally,\ninterpretability analysis reveals key factors such as loss of smell, headache,\nmuscle pain, and vaccination timing as significant predictors, while chronic\ndisease and gender are critical risk factors. These insights provide valuable\nguidance for understanding long COVID and developing targeted interventions.\n", "link": "http://arxiv.org/abs/2504.20915v1", "date": "2025-04-29", "relevancy": 1.7114, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4443}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.42}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20and%20Predictive%20Analysis%20to%20Identify%20Risk%20Factors%20and%20Effects%0A%20%20of%20Post%20COVID-19%20Syndrome&body=Title%3A%20Statistical%20and%20Predictive%20Analysis%20to%20Identify%20Risk%20Factors%20and%20Effects%0A%20%20of%20Post%20COVID-19%20Syndrome%0AAuthor%3A%20Milad%20Leyli-abadi%20and%20Jean-Patrick%20Brunet%20and%20Axel%20Tahmasebimoradi%0AAbstract%3A%20%20%20Based%20on%20recent%20studies%2C%20some%20COVID-19%20symptoms%20can%20persist%20for%20months%20after%0Ainfection%2C%20leading%20to%20what%20is%20termed%20long%20COVID.%20Factors%20such%20as%20vaccination%0Atiming%2C%20patient%20characteristics%2C%20and%20symptoms%20during%20the%20acute%20phase%20of%0Ainfection%20may%20contribute%20to%20the%20prolonged%20effects%20and%20intensity%20of%20long%20COVID.%0AEach%20patient%2C%20based%20on%20their%20unique%20combination%20of%20factors%2C%20develops%20a%20specific%0Arisk%20or%20intensity%20of%20long%20COVID.%20In%20this%20work%2C%20we%20aim%20to%20achieve%20two%0Aobjectives%3A%20%281%29%20conduct%20a%20statistical%20analysis%20to%20identify%20relationships%0Abetween%20various%20factors%20and%20long%20COVID%2C%20and%20%282%29%20perform%20predictive%20analysis%20of%0Along%20COVID%20intensity%20using%20these%20factors.%20We%20benchmark%20and%20interpret%20various%0Adata-driven%20approaches%2C%20including%20linear%20models%2C%20random%20forests%2C%20gradient%0Aboosting%2C%20and%20neural%20networks%2C%20using%20data%20from%20the%20Lifelines%20COVID-19%20cohort.%0AOur%20results%20show%20that%20Neural%20Networks%20%28NN%29%20achieve%20the%20best%20performance%20in%0Aterms%20of%20MAPE%2C%20with%20predictions%20averaging%2019%5C%25%20error.%20Additionally%2C%0Ainterpretability%20analysis%20reveals%20key%20factors%20such%20as%20loss%20of%20smell%2C%20headache%2C%0Amuscle%20pain%2C%20and%20vaccination%20timing%20as%20significant%20predictors%2C%20while%20chronic%0Adisease%20and%20gender%20are%20critical%20risk%20factors.%20These%20insights%20provide%20valuable%0Aguidance%20for%20understanding%20long%20COVID%20and%20developing%20targeted%20interventions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520and%2520Predictive%2520Analysis%2520to%2520Identify%2520Risk%2520Factors%2520and%2520Effects%250A%2520%2520of%2520Post%2520COVID-19%2520Syndrome%26entry.906535625%3DMilad%2520Leyli-abadi%2520and%2520Jean-Patrick%2520Brunet%2520and%2520Axel%2520Tahmasebimoradi%26entry.1292438233%3D%2520%2520Based%2520on%2520recent%2520studies%252C%2520some%2520COVID-19%2520symptoms%2520can%2520persist%2520for%2520months%2520after%250Ainfection%252C%2520leading%2520to%2520what%2520is%2520termed%2520long%2520COVID.%2520Factors%2520such%2520as%2520vaccination%250Atiming%252C%2520patient%2520characteristics%252C%2520and%2520symptoms%2520during%2520the%2520acute%2520phase%2520of%250Ainfection%2520may%2520contribute%2520to%2520the%2520prolonged%2520effects%2520and%2520intensity%2520of%2520long%2520COVID.%250AEach%2520patient%252C%2520based%2520on%2520their%2520unique%2520combination%2520of%2520factors%252C%2520develops%2520a%2520specific%250Arisk%2520or%2520intensity%2520of%2520long%2520COVID.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520achieve%2520two%250Aobjectives%253A%2520%25281%2529%2520conduct%2520a%2520statistical%2520analysis%2520to%2520identify%2520relationships%250Abetween%2520various%2520factors%2520and%2520long%2520COVID%252C%2520and%2520%25282%2529%2520perform%2520predictive%2520analysis%2520of%250Along%2520COVID%2520intensity%2520using%2520these%2520factors.%2520We%2520benchmark%2520and%2520interpret%2520various%250Adata-driven%2520approaches%252C%2520including%2520linear%2520models%252C%2520random%2520forests%252C%2520gradient%250Aboosting%252C%2520and%2520neural%2520networks%252C%2520using%2520data%2520from%2520the%2520Lifelines%2520COVID-19%2520cohort.%250AOur%2520results%2520show%2520that%2520Neural%2520Networks%2520%2528NN%2529%2520achieve%2520the%2520best%2520performance%2520in%250Aterms%2520of%2520MAPE%252C%2520with%2520predictions%2520averaging%252019%255C%2525%2520error.%2520Additionally%252C%250Ainterpretability%2520analysis%2520reveals%2520key%2520factors%2520such%2520as%2520loss%2520of%2520smell%252C%2520headache%252C%250Amuscle%2520pain%252C%2520and%2520vaccination%2520timing%2520as%2520significant%2520predictors%252C%2520while%2520chronic%250Adisease%2520and%2520gender%2520are%2520critical%2520risk%2520factors.%2520These%2520insights%2520provide%2520valuable%250Aguidance%2520for%2520understanding%2520long%2520COVID%2520and%2520developing%2520targeted%2520interventions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20and%20Predictive%20Analysis%20to%20Identify%20Risk%20Factors%20and%20Effects%0A%20%20of%20Post%20COVID-19%20Syndrome&entry.906535625=Milad%20Leyli-abadi%20and%20Jean-Patrick%20Brunet%20and%20Axel%20Tahmasebimoradi&entry.1292438233=%20%20Based%20on%20recent%20studies%2C%20some%20COVID-19%20symptoms%20can%20persist%20for%20months%20after%0Ainfection%2C%20leading%20to%20what%20is%20termed%20long%20COVID.%20Factors%20such%20as%20vaccination%0Atiming%2C%20patient%20characteristics%2C%20and%20symptoms%20during%20the%20acute%20phase%20of%0Ainfection%20may%20contribute%20to%20the%20prolonged%20effects%20and%20intensity%20of%20long%20COVID.%0AEach%20patient%2C%20based%20on%20their%20unique%20combination%20of%20factors%2C%20develops%20a%20specific%0Arisk%20or%20intensity%20of%20long%20COVID.%20In%20this%20work%2C%20we%20aim%20to%20achieve%20two%0Aobjectives%3A%20%281%29%20conduct%20a%20statistical%20analysis%20to%20identify%20relationships%0Abetween%20various%20factors%20and%20long%20COVID%2C%20and%20%282%29%20perform%20predictive%20analysis%20of%0Along%20COVID%20intensity%20using%20these%20factors.%20We%20benchmark%20and%20interpret%20various%0Adata-driven%20approaches%2C%20including%20linear%20models%2C%20random%20forests%2C%20gradient%0Aboosting%2C%20and%20neural%20networks%2C%20using%20data%20from%20the%20Lifelines%20COVID-19%20cohort.%0AOur%20results%20show%20that%20Neural%20Networks%20%28NN%29%20achieve%20the%20best%20performance%20in%0Aterms%20of%20MAPE%2C%20with%20predictions%20averaging%2019%5C%25%20error.%20Additionally%2C%0Ainterpretability%20analysis%20reveals%20key%20factors%20such%20as%20loss%20of%20smell%2C%20headache%2C%0Amuscle%20pain%2C%20and%20vaccination%20timing%20as%20significant%20predictors%2C%20while%20chronic%0Adisease%20and%20gender%20are%20critical%20risk%20factors.%20These%20insights%20provide%20valuable%0Aguidance%20for%20understanding%20long%20COVID%20and%20developing%20targeted%20interventions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20915v1&entry.124074799=Read"},
{"title": "Provably faster randomized and quantum algorithms for k-means clustering\n  via uniform sampling", "author": "Tyler Chen and Archan Ray and Akshay Seshadri and Dylan Herman and Bao Bach and Pranav Deshpande and Abhishek Som and Niraj Kumar and Marco Pistoia", "abstract": "  The $k$-means algorithm (Lloyd's algorithm) is a widely used method for\nclustering unlabeled data. A key bottleneck of the $k$-means algorithm is that\neach iteration requires time linear in the number of data points, which can be\nexpensive in big data applications. This was improved in recent works proposing\nquantum and quantum-inspired classical algorithms to approximate the $k$-means\nalgorithm locally, in time depending only logarithmically on the number of data\npoints (along with data dependent parameters) [$q$-means: A quantum algorithm\nfor unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash,\nNeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this\nwork, we describe a simple randomized mini-batch $k$-means algorithm and a\nquantum algorithm inspired by the classical algorithm. We prove worse-case\nguarantees that significantly improve upon the bounds for previous algorithms.\nOur improvements are due to a careful use of uniform sampling, which preserves\ncertain symmetries of the $k$-means problem that are not preserved in previous\nalgorithms that use data norm-based sampling.\n", "link": "http://arxiv.org/abs/2504.20982v1", "date": "2025-04-29", "relevancy": 1.6971, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4463}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4138}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provably%20faster%20randomized%20and%20quantum%20algorithms%20for%20k-means%20clustering%0A%20%20via%20uniform%20sampling&body=Title%3A%20Provably%20faster%20randomized%20and%20quantum%20algorithms%20for%20k-means%20clustering%0A%20%20via%20uniform%20sampling%0AAuthor%3A%20Tyler%20Chen%20and%20Archan%20Ray%20and%20Akshay%20Seshadri%20and%20Dylan%20Herman%20and%20Bao%20Bach%20and%20Pranav%20Deshpande%20and%20Abhishek%20Som%20and%20Niraj%20Kumar%20and%20Marco%20Pistoia%0AAbstract%3A%20%20%20The%20%24k%24-means%20algorithm%20%28Lloyd%27s%20algorithm%29%20is%20a%20widely%20used%20method%20for%0Aclustering%20unlabeled%20data.%20A%20key%20bottleneck%20of%20the%20%24k%24-means%20algorithm%20is%20that%0Aeach%20iteration%20requires%20time%20linear%20in%20the%20number%20of%20data%20points%2C%20which%20can%20be%0Aexpensive%20in%20big%20data%20applications.%20This%20was%20improved%20in%20recent%20works%20proposing%0Aquantum%20and%20quantum-inspired%20classical%20algorithms%20to%20approximate%20the%20%24k%24-means%0Aalgorithm%20locally%2C%20in%20time%20depending%20only%20logarithmically%20on%20the%20number%20of%20data%0Apoints%20%28along%20with%20data%20dependent%20parameters%29%20%5B%24q%24-means%3A%20A%20quantum%20algorithm%0Afor%20unsupervised%20machine%20learning%3B%20Kerenidis%2C%20Landman%2C%20Luongo%2C%20and%20Prakash%2C%0ANeurIPS%202019%3B%20Do%20you%20know%20what%20%24q%24-means%3F%2C%20Doriguello%2C%20Luongo%2C%20Tang%5D.%20In%20this%0Awork%2C%20we%20describe%20a%20simple%20randomized%20mini-batch%20%24k%24-means%20algorithm%20and%20a%0Aquantum%20algorithm%20inspired%20by%20the%20classical%20algorithm.%20We%20prove%20worse-case%0Aguarantees%20that%20significantly%20improve%20upon%20the%20bounds%20for%20previous%20algorithms.%0AOur%20improvements%20are%20due%20to%20a%20careful%20use%20of%20uniform%20sampling%2C%20which%20preserves%0Acertain%20symmetries%20of%20the%20%24k%24-means%20problem%20that%20are%20not%20preserved%20in%20previous%0Aalgorithms%20that%20use%20data%20norm-based%20sampling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20982v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvably%2520faster%2520randomized%2520and%2520quantum%2520algorithms%2520for%2520k-means%2520clustering%250A%2520%2520via%2520uniform%2520sampling%26entry.906535625%3DTyler%2520Chen%2520and%2520Archan%2520Ray%2520and%2520Akshay%2520Seshadri%2520and%2520Dylan%2520Herman%2520and%2520Bao%2520Bach%2520and%2520Pranav%2520Deshpande%2520and%2520Abhishek%2520Som%2520and%2520Niraj%2520Kumar%2520and%2520Marco%2520Pistoia%26entry.1292438233%3D%2520%2520The%2520%2524k%2524-means%2520algorithm%2520%2528Lloyd%2527s%2520algorithm%2529%2520is%2520a%2520widely%2520used%2520method%2520for%250Aclustering%2520unlabeled%2520data.%2520A%2520key%2520bottleneck%2520of%2520the%2520%2524k%2524-means%2520algorithm%2520is%2520that%250Aeach%2520iteration%2520requires%2520time%2520linear%2520in%2520the%2520number%2520of%2520data%2520points%252C%2520which%2520can%2520be%250Aexpensive%2520in%2520big%2520data%2520applications.%2520This%2520was%2520improved%2520in%2520recent%2520works%2520proposing%250Aquantum%2520and%2520quantum-inspired%2520classical%2520algorithms%2520to%2520approximate%2520the%2520%2524k%2524-means%250Aalgorithm%2520locally%252C%2520in%2520time%2520depending%2520only%2520logarithmically%2520on%2520the%2520number%2520of%2520data%250Apoints%2520%2528along%2520with%2520data%2520dependent%2520parameters%2529%2520%255B%2524q%2524-means%253A%2520A%2520quantum%2520algorithm%250Afor%2520unsupervised%2520machine%2520learning%253B%2520Kerenidis%252C%2520Landman%252C%2520Luongo%252C%2520and%2520Prakash%252C%250ANeurIPS%25202019%253B%2520Do%2520you%2520know%2520what%2520%2524q%2524-means%253F%252C%2520Doriguello%252C%2520Luongo%252C%2520Tang%255D.%2520In%2520this%250Awork%252C%2520we%2520describe%2520a%2520simple%2520randomized%2520mini-batch%2520%2524k%2524-means%2520algorithm%2520and%2520a%250Aquantum%2520algorithm%2520inspired%2520by%2520the%2520classical%2520algorithm.%2520We%2520prove%2520worse-case%250Aguarantees%2520that%2520significantly%2520improve%2520upon%2520the%2520bounds%2520for%2520previous%2520algorithms.%250AOur%2520improvements%2520are%2520due%2520to%2520a%2520careful%2520use%2520of%2520uniform%2520sampling%252C%2520which%2520preserves%250Acertain%2520symmetries%2520of%2520the%2520%2524k%2524-means%2520problem%2520that%2520are%2520not%2520preserved%2520in%2520previous%250Aalgorithms%2520that%2520use%2520data%2520norm-based%2520sampling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20982v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provably%20faster%20randomized%20and%20quantum%20algorithms%20for%20k-means%20clustering%0A%20%20via%20uniform%20sampling&entry.906535625=Tyler%20Chen%20and%20Archan%20Ray%20and%20Akshay%20Seshadri%20and%20Dylan%20Herman%20and%20Bao%20Bach%20and%20Pranav%20Deshpande%20and%20Abhishek%20Som%20and%20Niraj%20Kumar%20and%20Marco%20Pistoia&entry.1292438233=%20%20The%20%24k%24-means%20algorithm%20%28Lloyd%27s%20algorithm%29%20is%20a%20widely%20used%20method%20for%0Aclustering%20unlabeled%20data.%20A%20key%20bottleneck%20of%20the%20%24k%24-means%20algorithm%20is%20that%0Aeach%20iteration%20requires%20time%20linear%20in%20the%20number%20of%20data%20points%2C%20which%20can%20be%0Aexpensive%20in%20big%20data%20applications.%20This%20was%20improved%20in%20recent%20works%20proposing%0Aquantum%20and%20quantum-inspired%20classical%20algorithms%20to%20approximate%20the%20%24k%24-means%0Aalgorithm%20locally%2C%20in%20time%20depending%20only%20logarithmically%20on%20the%20number%20of%20data%0Apoints%20%28along%20with%20data%20dependent%20parameters%29%20%5B%24q%24-means%3A%20A%20quantum%20algorithm%0Afor%20unsupervised%20machine%20learning%3B%20Kerenidis%2C%20Landman%2C%20Luongo%2C%20and%20Prakash%2C%0ANeurIPS%202019%3B%20Do%20you%20know%20what%20%24q%24-means%3F%2C%20Doriguello%2C%20Luongo%2C%20Tang%5D.%20In%20this%0Awork%2C%20we%20describe%20a%20simple%20randomized%20mini-batch%20%24k%24-means%20algorithm%20and%20a%0Aquantum%20algorithm%20inspired%20by%20the%20classical%20algorithm.%20We%20prove%20worse-case%0Aguarantees%20that%20significantly%20improve%20upon%20the%20bounds%20for%20previous%20algorithms.%0AOur%20improvements%20are%20due%20to%20a%20careful%20use%20of%20uniform%20sampling%2C%20which%20preserves%0Acertain%20symmetries%20of%20the%20%24k%24-means%20problem%20that%20are%20not%20preserved%20in%20previous%0Aalgorithms%20that%20use%20data%20norm-based%20sampling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20982v1&entry.124074799=Read"},
{"title": "XPG-RL: Reinforcement Learning with Explainable Priority Guidance for\n  Efficiency-Boosted Mechanical Search", "author": "Yiting Zhang and Shichen Li and Elena Shrestha", "abstract": "  Mechanical search (MS) in cluttered environments remains a significant\nchallenge for autonomous manipulators, requiring long-horizon planning and\nrobust state estimation under occlusions and partial observability. In this\nwork, we introduce XPG-RL, a reinforcement learning framework that enables\nagents to efficiently perform MS tasks through explainable, priority-guided\ndecision-making based on raw sensory inputs. XPG-RL integrates a task-driven\naction prioritization mechanism with a learned context-aware switching strategy\nthat dynamically selects from a discrete set of action primitives such as\ntarget grasping, occlusion removal, and viewpoint adjustment. Within this\nstrategy, a policy is optimized to output adaptive threshold values that govern\nthe discrete selection among action primitives. The perception module fuses\nRGB-D inputs with semantic and geometric features to produce a structured scene\nrepresentation for downstream decision-making. Extensive experiments in both\nsimulation and real-world settings demonstrate that XPG-RL consistently\noutperforms baseline methods in task success rates and motion efficiency,\nachieving up to 4.5$\\times$ higher efficiency in long-horizon tasks. These\nresults underscore the benefits of integrating domain knowledge with learnable\ndecision-making policies for robust and efficient robotic manipulation.\n", "link": "http://arxiv.org/abs/2504.20969v1", "date": "2025-04-29", "relevancy": 1.6924, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6286}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5703}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XPG-RL%3A%20Reinforcement%20Learning%20with%20Explainable%20Priority%20Guidance%20for%0A%20%20Efficiency-Boosted%20Mechanical%20Search&body=Title%3A%20XPG-RL%3A%20Reinforcement%20Learning%20with%20Explainable%20Priority%20Guidance%20for%0A%20%20Efficiency-Boosted%20Mechanical%20Search%0AAuthor%3A%20Yiting%20Zhang%20and%20Shichen%20Li%20and%20Elena%20Shrestha%0AAbstract%3A%20%20%20Mechanical%20search%20%28MS%29%20in%20cluttered%20environments%20remains%20a%20significant%0Achallenge%20for%20autonomous%20manipulators%2C%20requiring%20long-horizon%20planning%20and%0Arobust%20state%20estimation%20under%20occlusions%20and%20partial%20observability.%20In%20this%0Awork%2C%20we%20introduce%20XPG-RL%2C%20a%20reinforcement%20learning%20framework%20that%20enables%0Aagents%20to%20efficiently%20perform%20MS%20tasks%20through%20explainable%2C%20priority-guided%0Adecision-making%20based%20on%20raw%20sensory%20inputs.%20XPG-RL%20integrates%20a%20task-driven%0Aaction%20prioritization%20mechanism%20with%20a%20learned%20context-aware%20switching%20strategy%0Athat%20dynamically%20selects%20from%20a%20discrete%20set%20of%20action%20primitives%20such%20as%0Atarget%20grasping%2C%20occlusion%20removal%2C%20and%20viewpoint%20adjustment.%20Within%20this%0Astrategy%2C%20a%20policy%20is%20optimized%20to%20output%20adaptive%20threshold%20values%20that%20govern%0Athe%20discrete%20selection%20among%20action%20primitives.%20The%20perception%20module%20fuses%0ARGB-D%20inputs%20with%20semantic%20and%20geometric%20features%20to%20produce%20a%20structured%20scene%0Arepresentation%20for%20downstream%20decision-making.%20Extensive%20experiments%20in%20both%0Asimulation%20and%20real-world%20settings%20demonstrate%20that%20XPG-RL%20consistently%0Aoutperforms%20baseline%20methods%20in%20task%20success%20rates%20and%20motion%20efficiency%2C%0Aachieving%20up%20to%204.5%24%5Ctimes%24%20higher%20efficiency%20in%20long-horizon%20tasks.%20These%0Aresults%20underscore%20the%20benefits%20of%20integrating%20domain%20knowledge%20with%20learnable%0Adecision-making%20policies%20for%20robust%20and%20efficient%20robotic%20manipulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXPG-RL%253A%2520Reinforcement%2520Learning%2520with%2520Explainable%2520Priority%2520Guidance%2520for%250A%2520%2520Efficiency-Boosted%2520Mechanical%2520Search%26entry.906535625%3DYiting%2520Zhang%2520and%2520Shichen%2520Li%2520and%2520Elena%2520Shrestha%26entry.1292438233%3D%2520%2520Mechanical%2520search%2520%2528MS%2529%2520in%2520cluttered%2520environments%2520remains%2520a%2520significant%250Achallenge%2520for%2520autonomous%2520manipulators%252C%2520requiring%2520long-horizon%2520planning%2520and%250Arobust%2520state%2520estimation%2520under%2520occlusions%2520and%2520partial%2520observability.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520XPG-RL%252C%2520a%2520reinforcement%2520learning%2520framework%2520that%2520enables%250Aagents%2520to%2520efficiently%2520perform%2520MS%2520tasks%2520through%2520explainable%252C%2520priority-guided%250Adecision-making%2520based%2520on%2520raw%2520sensory%2520inputs.%2520XPG-RL%2520integrates%2520a%2520task-driven%250Aaction%2520prioritization%2520mechanism%2520with%2520a%2520learned%2520context-aware%2520switching%2520strategy%250Athat%2520dynamically%2520selects%2520from%2520a%2520discrete%2520set%2520of%2520action%2520primitives%2520such%2520as%250Atarget%2520grasping%252C%2520occlusion%2520removal%252C%2520and%2520viewpoint%2520adjustment.%2520Within%2520this%250Astrategy%252C%2520a%2520policy%2520is%2520optimized%2520to%2520output%2520adaptive%2520threshold%2520values%2520that%2520govern%250Athe%2520discrete%2520selection%2520among%2520action%2520primitives.%2520The%2520perception%2520module%2520fuses%250ARGB-D%2520inputs%2520with%2520semantic%2520and%2520geometric%2520features%2520to%2520produce%2520a%2520structured%2520scene%250Arepresentation%2520for%2520downstream%2520decision-making.%2520Extensive%2520experiments%2520in%2520both%250Asimulation%2520and%2520real-world%2520settings%2520demonstrate%2520that%2520XPG-RL%2520consistently%250Aoutperforms%2520baseline%2520methods%2520in%2520task%2520success%2520rates%2520and%2520motion%2520efficiency%252C%250Aachieving%2520up%2520to%25204.5%2524%255Ctimes%2524%2520higher%2520efficiency%2520in%2520long-horizon%2520tasks.%2520These%250Aresults%2520underscore%2520the%2520benefits%2520of%2520integrating%2520domain%2520knowledge%2520with%2520learnable%250Adecision-making%2520policies%2520for%2520robust%2520and%2520efficient%2520robotic%2520manipulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XPG-RL%3A%20Reinforcement%20Learning%20with%20Explainable%20Priority%20Guidance%20for%0A%20%20Efficiency-Boosted%20Mechanical%20Search&entry.906535625=Yiting%20Zhang%20and%20Shichen%20Li%20and%20Elena%20Shrestha&entry.1292438233=%20%20Mechanical%20search%20%28MS%29%20in%20cluttered%20environments%20remains%20a%20significant%0Achallenge%20for%20autonomous%20manipulators%2C%20requiring%20long-horizon%20planning%20and%0Arobust%20state%20estimation%20under%20occlusions%20and%20partial%20observability.%20In%20this%0Awork%2C%20we%20introduce%20XPG-RL%2C%20a%20reinforcement%20learning%20framework%20that%20enables%0Aagents%20to%20efficiently%20perform%20MS%20tasks%20through%20explainable%2C%20priority-guided%0Adecision-making%20based%20on%20raw%20sensory%20inputs.%20XPG-RL%20integrates%20a%20task-driven%0Aaction%20prioritization%20mechanism%20with%20a%20learned%20context-aware%20switching%20strategy%0Athat%20dynamically%20selects%20from%20a%20discrete%20set%20of%20action%20primitives%20such%20as%0Atarget%20grasping%2C%20occlusion%20removal%2C%20and%20viewpoint%20adjustment.%20Within%20this%0Astrategy%2C%20a%20policy%20is%20optimized%20to%20output%20adaptive%20threshold%20values%20that%20govern%0Athe%20discrete%20selection%20among%20action%20primitives.%20The%20perception%20module%20fuses%0ARGB-D%20inputs%20with%20semantic%20and%20geometric%20features%20to%20produce%20a%20structured%20scene%0Arepresentation%20for%20downstream%20decision-making.%20Extensive%20experiments%20in%20both%0Asimulation%20and%20real-world%20settings%20demonstrate%20that%20XPG-RL%20consistently%0Aoutperforms%20baseline%20methods%20in%20task%20success%20rates%20and%20motion%20efficiency%2C%0Aachieving%20up%20to%204.5%24%5Ctimes%24%20higher%20efficiency%20in%20long-horizon%20tasks.%20These%0Aresults%20underscore%20the%20benefits%20of%20integrating%20domain%20knowledge%20with%20learnable%0Adecision-making%20policies%20for%20robust%20and%20efficient%20robotic%20manipulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20969v1&entry.124074799=Read"},
{"title": "Exploiting inter-agent coupling information for efficient reinforcement\n  learning of cooperative LQR", "author": "Shahbaz P Qadri Syed and He Bai", "abstract": "  Developing scalable and efficient reinforcement learning algorithms for\ncooperative multi-agent control has received significant attention over the\npast years. Existing literature has proposed inexact decompositions of local\nQ-functions based on empirical information structures between the agents. In\nthis paper, we exploit inter-agent coupling information and propose a\nsystematic approach to exactly decompose the local Q-function of each agent. We\ndevelop an approximate least square policy iteration algorithm based on the\nproposed decomposition and identify two architectures to learn the local\nQ-function for each agent. We establish that the worst-case sample complexity\nof the decomposition is equal to the centralized case and derive necessary and\nsufficient graphical conditions on the inter-agent couplings to achieve better\nsample efficiency. We demonstrate the improved sample efficiency and\ncomputational efficiency on numerical examples.\n", "link": "http://arxiv.org/abs/2504.20927v1", "date": "2025-04-29", "relevancy": 1.6734, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5951}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5191}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20inter-agent%20coupling%20information%20for%20efficient%20reinforcement%0A%20%20learning%20of%20cooperative%20LQR&body=Title%3A%20Exploiting%20inter-agent%20coupling%20information%20for%20efficient%20reinforcement%0A%20%20learning%20of%20cooperative%20LQR%0AAuthor%3A%20Shahbaz%20P%20Qadri%20Syed%20and%20He%20Bai%0AAbstract%3A%20%20%20Developing%20scalable%20and%20efficient%20reinforcement%20learning%20algorithms%20for%0Acooperative%20multi-agent%20control%20has%20received%20significant%20attention%20over%20the%0Apast%20years.%20Existing%20literature%20has%20proposed%20inexact%20decompositions%20of%20local%0AQ-functions%20based%20on%20empirical%20information%20structures%20between%20the%20agents.%20In%0Athis%20paper%2C%20we%20exploit%20inter-agent%20coupling%20information%20and%20propose%20a%0Asystematic%20approach%20to%20exactly%20decompose%20the%20local%20Q-function%20of%20each%20agent.%20We%0Adevelop%20an%20approximate%20least%20square%20policy%20iteration%20algorithm%20based%20on%20the%0Aproposed%20decomposition%20and%20identify%20two%20architectures%20to%20learn%20the%20local%0AQ-function%20for%20each%20agent.%20We%20establish%20that%20the%20worst-case%20sample%20complexity%0Aof%20the%20decomposition%20is%20equal%20to%20the%20centralized%20case%20and%20derive%20necessary%20and%0Asufficient%20graphical%20conditions%20on%20the%20inter-agent%20couplings%20to%20achieve%20better%0Asample%20efficiency.%20We%20demonstrate%20the%20improved%20sample%20efficiency%20and%0Acomputational%20efficiency%20on%20numerical%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520inter-agent%2520coupling%2520information%2520for%2520efficient%2520reinforcement%250A%2520%2520learning%2520of%2520cooperative%2520LQR%26entry.906535625%3DShahbaz%2520P%2520Qadri%2520Syed%2520and%2520He%2520Bai%26entry.1292438233%3D%2520%2520Developing%2520scalable%2520and%2520efficient%2520reinforcement%2520learning%2520algorithms%2520for%250Acooperative%2520multi-agent%2520control%2520has%2520received%2520significant%2520attention%2520over%2520the%250Apast%2520years.%2520Existing%2520literature%2520has%2520proposed%2520inexact%2520decompositions%2520of%2520local%250AQ-functions%2520based%2520on%2520empirical%2520information%2520structures%2520between%2520the%2520agents.%2520In%250Athis%2520paper%252C%2520we%2520exploit%2520inter-agent%2520coupling%2520information%2520and%2520propose%2520a%250Asystematic%2520approach%2520to%2520exactly%2520decompose%2520the%2520local%2520Q-function%2520of%2520each%2520agent.%2520We%250Adevelop%2520an%2520approximate%2520least%2520square%2520policy%2520iteration%2520algorithm%2520based%2520on%2520the%250Aproposed%2520decomposition%2520and%2520identify%2520two%2520architectures%2520to%2520learn%2520the%2520local%250AQ-function%2520for%2520each%2520agent.%2520We%2520establish%2520that%2520the%2520worst-case%2520sample%2520complexity%250Aof%2520the%2520decomposition%2520is%2520equal%2520to%2520the%2520centralized%2520case%2520and%2520derive%2520necessary%2520and%250Asufficient%2520graphical%2520conditions%2520on%2520the%2520inter-agent%2520couplings%2520to%2520achieve%2520better%250Asample%2520efficiency.%2520We%2520demonstrate%2520the%2520improved%2520sample%2520efficiency%2520and%250Acomputational%2520efficiency%2520on%2520numerical%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20inter-agent%20coupling%20information%20for%20efficient%20reinforcement%0A%20%20learning%20of%20cooperative%20LQR&entry.906535625=Shahbaz%20P%20Qadri%20Syed%20and%20He%20Bai&entry.1292438233=%20%20Developing%20scalable%20and%20efficient%20reinforcement%20learning%20algorithms%20for%0Acooperative%20multi-agent%20control%20has%20received%20significant%20attention%20over%20the%0Apast%20years.%20Existing%20literature%20has%20proposed%20inexact%20decompositions%20of%20local%0AQ-functions%20based%20on%20empirical%20information%20structures%20between%20the%20agents.%20In%0Athis%20paper%2C%20we%20exploit%20inter-agent%20coupling%20information%20and%20propose%20a%0Asystematic%20approach%20to%20exactly%20decompose%20the%20local%20Q-function%20of%20each%20agent.%20We%0Adevelop%20an%20approximate%20least%20square%20policy%20iteration%20algorithm%20based%20on%20the%0Aproposed%20decomposition%20and%20identify%20two%20architectures%20to%20learn%20the%20local%0AQ-function%20for%20each%20agent.%20We%20establish%20that%20the%20worst-case%20sample%20complexity%0Aof%20the%20decomposition%20is%20equal%20to%20the%20centralized%20case%20and%20derive%20necessary%20and%0Asufficient%20graphical%20conditions%20on%20the%20inter-agent%20couplings%20to%20achieve%20better%0Asample%20efficiency.%20We%20demonstrate%20the%20improved%20sample%20efficiency%20and%0Acomputational%20efficiency%20on%20numerical%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20927v1&entry.124074799=Read"},
{"title": "Hierarchical Temporal Logic Task and Motion Planning for Multi-Robot\n  Systems", "author": "Zhongqi Wei and Xusheng Luo and Changliu Liu", "abstract": "  Task and motion planning (TAMP) for multi-robot systems, which integrates\ndiscrete task planning with continuous motion planning, remains a challenging\nproblem in robotics. Existing TAMP approaches often struggle to scale\neffectively for multi-robot systems with complex specifications, leading to\ninfeasible solutions and prolonged computation times. This work addresses the\nTAMP problem in multi-robot settings where tasks are specified using expressive\nhierarchical temporal logic and task assignments are not pre-determined. Our\napproach leverages the efficiency of hierarchical temporal logic specifications\nfor task-level planning and the optimization-based graph of convex sets method\nfor motion-level planning, integrating them within a product graph framework.\nAt the task level, we convert hierarchical temporal logic specifications into a\nsingle graph, embedding task allocation within its edges. At the motion level,\nwe represent the feasible motions of multiple robots through convex sets in the\nconfiguration space, guided by a sampling-based motion planner. This\nformulation allows us to define the TAMP problem as a shortest path search\nwithin the product graph, where efficient convex optimization techniques can be\napplied. We prove that our approach is both sound and complete under mild\nassumptions. Additionally, we extend our framework to cooperative\npick-and-place tasks involving object handovers between robots. We evaluate our\nmethod across various high-dimensional multi-robot scenarios, including\nsimulated and real-world environments with quadrupeds, robotic arms, and\nautomated conveyor systems. Our results show that our approach outperforms\nexisting methods in execution time and solution optimality while effectively\nscaling with task complexity.\n", "link": "http://arxiv.org/abs/2504.18899v2", "date": "2025-04-29", "relevancy": 1.6711, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5849}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5647}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Temporal%20Logic%20Task%20and%20Motion%20Planning%20for%20Multi-Robot%0A%20%20Systems&body=Title%3A%20Hierarchical%20Temporal%20Logic%20Task%20and%20Motion%20Planning%20for%20Multi-Robot%0A%20%20Systems%0AAuthor%3A%20Zhongqi%20Wei%20and%20Xusheng%20Luo%20and%20Changliu%20Liu%0AAbstract%3A%20%20%20Task%20and%20motion%20planning%20%28TAMP%29%20for%20multi-robot%20systems%2C%20which%20integrates%0Adiscrete%20task%20planning%20with%20continuous%20motion%20planning%2C%20remains%20a%20challenging%0Aproblem%20in%20robotics.%20Existing%20TAMP%20approaches%20often%20struggle%20to%20scale%0Aeffectively%20for%20multi-robot%20systems%20with%20complex%20specifications%2C%20leading%20to%0Ainfeasible%20solutions%20and%20prolonged%20computation%20times.%20This%20work%20addresses%20the%0ATAMP%20problem%20in%20multi-robot%20settings%20where%20tasks%20are%20specified%20using%20expressive%0Ahierarchical%20temporal%20logic%20and%20task%20assignments%20are%20not%20pre-determined.%20Our%0Aapproach%20leverages%20the%20efficiency%20of%20hierarchical%20temporal%20logic%20specifications%0Afor%20task-level%20planning%20and%20the%20optimization-based%20graph%20of%20convex%20sets%20method%0Afor%20motion-level%20planning%2C%20integrating%20them%20within%20a%20product%20graph%20framework.%0AAt%20the%20task%20level%2C%20we%20convert%20hierarchical%20temporal%20logic%20specifications%20into%20a%0Asingle%20graph%2C%20embedding%20task%20allocation%20within%20its%20edges.%20At%20the%20motion%20level%2C%0Awe%20represent%20the%20feasible%20motions%20of%20multiple%20robots%20through%20convex%20sets%20in%20the%0Aconfiguration%20space%2C%20guided%20by%20a%20sampling-based%20motion%20planner.%20This%0Aformulation%20allows%20us%20to%20define%20the%20TAMP%20problem%20as%20a%20shortest%20path%20search%0Awithin%20the%20product%20graph%2C%20where%20efficient%20convex%20optimization%20techniques%20can%20be%0Aapplied.%20We%20prove%20that%20our%20approach%20is%20both%20sound%20and%20complete%20under%20mild%0Aassumptions.%20Additionally%2C%20we%20extend%20our%20framework%20to%20cooperative%0Apick-and-place%20tasks%20involving%20object%20handovers%20between%20robots.%20We%20evaluate%20our%0Amethod%20across%20various%20high-dimensional%20multi-robot%20scenarios%2C%20including%0Asimulated%20and%20real-world%20environments%20with%20quadrupeds%2C%20robotic%20arms%2C%20and%0Aautomated%20conveyor%20systems.%20Our%20results%20show%20that%20our%20approach%20outperforms%0Aexisting%20methods%20in%20execution%20time%20and%20solution%20optimality%20while%20effectively%0Ascaling%20with%20task%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18899v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Temporal%2520Logic%2520Task%2520and%2520Motion%2520Planning%2520for%2520Multi-Robot%250A%2520%2520Systems%26entry.906535625%3DZhongqi%2520Wei%2520and%2520Xusheng%2520Luo%2520and%2520Changliu%2520Liu%26entry.1292438233%3D%2520%2520Task%2520and%2520motion%2520planning%2520%2528TAMP%2529%2520for%2520multi-robot%2520systems%252C%2520which%2520integrates%250Adiscrete%2520task%2520planning%2520with%2520continuous%2520motion%2520planning%252C%2520remains%2520a%2520challenging%250Aproblem%2520in%2520robotics.%2520Existing%2520TAMP%2520approaches%2520often%2520struggle%2520to%2520scale%250Aeffectively%2520for%2520multi-robot%2520systems%2520with%2520complex%2520specifications%252C%2520leading%2520to%250Ainfeasible%2520solutions%2520and%2520prolonged%2520computation%2520times.%2520This%2520work%2520addresses%2520the%250ATAMP%2520problem%2520in%2520multi-robot%2520settings%2520where%2520tasks%2520are%2520specified%2520using%2520expressive%250Ahierarchical%2520temporal%2520logic%2520and%2520task%2520assignments%2520are%2520not%2520pre-determined.%2520Our%250Aapproach%2520leverages%2520the%2520efficiency%2520of%2520hierarchical%2520temporal%2520logic%2520specifications%250Afor%2520task-level%2520planning%2520and%2520the%2520optimization-based%2520graph%2520of%2520convex%2520sets%2520method%250Afor%2520motion-level%2520planning%252C%2520integrating%2520them%2520within%2520a%2520product%2520graph%2520framework.%250AAt%2520the%2520task%2520level%252C%2520we%2520convert%2520hierarchical%2520temporal%2520logic%2520specifications%2520into%2520a%250Asingle%2520graph%252C%2520embedding%2520task%2520allocation%2520within%2520its%2520edges.%2520At%2520the%2520motion%2520level%252C%250Awe%2520represent%2520the%2520feasible%2520motions%2520of%2520multiple%2520robots%2520through%2520convex%2520sets%2520in%2520the%250Aconfiguration%2520space%252C%2520guided%2520by%2520a%2520sampling-based%2520motion%2520planner.%2520This%250Aformulation%2520allows%2520us%2520to%2520define%2520the%2520TAMP%2520problem%2520as%2520a%2520shortest%2520path%2520search%250Awithin%2520the%2520product%2520graph%252C%2520where%2520efficient%2520convex%2520optimization%2520techniques%2520can%2520be%250Aapplied.%2520We%2520prove%2520that%2520our%2520approach%2520is%2520both%2520sound%2520and%2520complete%2520under%2520mild%250Aassumptions.%2520Additionally%252C%2520we%2520extend%2520our%2520framework%2520to%2520cooperative%250Apick-and-place%2520tasks%2520involving%2520object%2520handovers%2520between%2520robots.%2520We%2520evaluate%2520our%250Amethod%2520across%2520various%2520high-dimensional%2520multi-robot%2520scenarios%252C%2520including%250Asimulated%2520and%2520real-world%2520environments%2520with%2520quadrupeds%252C%2520robotic%2520arms%252C%2520and%250Aautomated%2520conveyor%2520systems.%2520Our%2520results%2520show%2520that%2520our%2520approach%2520outperforms%250Aexisting%2520methods%2520in%2520execution%2520time%2520and%2520solution%2520optimality%2520while%2520effectively%250Ascaling%2520with%2520task%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18899v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Temporal%20Logic%20Task%20and%20Motion%20Planning%20for%20Multi-Robot%0A%20%20Systems&entry.906535625=Zhongqi%20Wei%20and%20Xusheng%20Luo%20and%20Changliu%20Liu&entry.1292438233=%20%20Task%20and%20motion%20planning%20%28TAMP%29%20for%20multi-robot%20systems%2C%20which%20integrates%0Adiscrete%20task%20planning%20with%20continuous%20motion%20planning%2C%20remains%20a%20challenging%0Aproblem%20in%20robotics.%20Existing%20TAMP%20approaches%20often%20struggle%20to%20scale%0Aeffectively%20for%20multi-robot%20systems%20with%20complex%20specifications%2C%20leading%20to%0Ainfeasible%20solutions%20and%20prolonged%20computation%20times.%20This%20work%20addresses%20the%0ATAMP%20problem%20in%20multi-robot%20settings%20where%20tasks%20are%20specified%20using%20expressive%0Ahierarchical%20temporal%20logic%20and%20task%20assignments%20are%20not%20pre-determined.%20Our%0Aapproach%20leverages%20the%20efficiency%20of%20hierarchical%20temporal%20logic%20specifications%0Afor%20task-level%20planning%20and%20the%20optimization-based%20graph%20of%20convex%20sets%20method%0Afor%20motion-level%20planning%2C%20integrating%20them%20within%20a%20product%20graph%20framework.%0AAt%20the%20task%20level%2C%20we%20convert%20hierarchical%20temporal%20logic%20specifications%20into%20a%0Asingle%20graph%2C%20embedding%20task%20allocation%20within%20its%20edges.%20At%20the%20motion%20level%2C%0Awe%20represent%20the%20feasible%20motions%20of%20multiple%20robots%20through%20convex%20sets%20in%20the%0Aconfiguration%20space%2C%20guided%20by%20a%20sampling-based%20motion%20planner.%20This%0Aformulation%20allows%20us%20to%20define%20the%20TAMP%20problem%20as%20a%20shortest%20path%20search%0Awithin%20the%20product%20graph%2C%20where%20efficient%20convex%20optimization%20techniques%20can%20be%0Aapplied.%20We%20prove%20that%20our%20approach%20is%20both%20sound%20and%20complete%20under%20mild%0Aassumptions.%20Additionally%2C%20we%20extend%20our%20framework%20to%20cooperative%0Apick-and-place%20tasks%20involving%20object%20handovers%20between%20robots.%20We%20evaluate%20our%0Amethod%20across%20various%20high-dimensional%20multi-robot%20scenarios%2C%20including%0Asimulated%20and%20real-world%20environments%20with%20quadrupeds%2C%20robotic%20arms%2C%20and%0Aautomated%20conveyor%20systems.%20Our%20results%20show%20that%20our%20approach%20outperforms%0Aexisting%20methods%20in%20execution%20time%20and%20solution%20optimality%20while%20effectively%0Ascaling%20with%20task%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18899v2&entry.124074799=Read"},
{"title": "Opinion-Driven Decision-Making for Multi-Robot Navigation through Narrow\n  Corridors", "author": "Norah K. Alghamdi and Shinkyu Park", "abstract": "  We propose an opinion-driven navigation framework for multi-robot traversal\nthrough a narrow corridor. Our approach leverages a multi-agent decision-making\nmodel known as the Nonlinear Opinion Dynamics (NOD) to address the narrow\ncorridor passage problem, formulated as a multi-robot navigation game. By\nintegrating the NOD model with a multi-robot path planning algorithm, we\ndemonstrate that the framework effectively reduces the likelihood of deadlocks\nduring corridor traversal. To ensure scalability with an increasing number of\nrobots, we introduce a game reduction technique that enables efficient\ncoordination in larger groups. Extensive simulation studies are conducted to\nvalidate the effectiveness of the proposed approach.\n", "link": "http://arxiv.org/abs/2504.20947v1", "date": "2025-04-29", "relevancy": 1.6607, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5568}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5537}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Opinion-Driven%20Decision-Making%20for%20Multi-Robot%20Navigation%20through%20Narrow%0A%20%20Corridors&body=Title%3A%20Opinion-Driven%20Decision-Making%20for%20Multi-Robot%20Navigation%20through%20Narrow%0A%20%20Corridors%0AAuthor%3A%20Norah%20K.%20Alghamdi%20and%20Shinkyu%20Park%0AAbstract%3A%20%20%20We%20propose%20an%20opinion-driven%20navigation%20framework%20for%20multi-robot%20traversal%0Athrough%20a%20narrow%20corridor.%20Our%20approach%20leverages%20a%20multi-agent%20decision-making%0Amodel%20known%20as%20the%20Nonlinear%20Opinion%20Dynamics%20%28NOD%29%20to%20address%20the%20narrow%0Acorridor%20passage%20problem%2C%20formulated%20as%20a%20multi-robot%20navigation%20game.%20By%0Aintegrating%20the%20NOD%20model%20with%20a%20multi-robot%20path%20planning%20algorithm%2C%20we%0Ademonstrate%20that%20the%20framework%20effectively%20reduces%20the%20likelihood%20of%20deadlocks%0Aduring%20corridor%20traversal.%20To%20ensure%20scalability%20with%20an%20increasing%20number%20of%0Arobots%2C%20we%20introduce%20a%20game%20reduction%20technique%20that%20enables%20efficient%0Acoordination%20in%20larger%20groups.%20Extensive%20simulation%20studies%20are%20conducted%20to%0Avalidate%20the%20effectiveness%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpinion-Driven%2520Decision-Making%2520for%2520Multi-Robot%2520Navigation%2520through%2520Narrow%250A%2520%2520Corridors%26entry.906535625%3DNorah%2520K.%2520Alghamdi%2520and%2520Shinkyu%2520Park%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520opinion-driven%2520navigation%2520framework%2520for%2520multi-robot%2520traversal%250Athrough%2520a%2520narrow%2520corridor.%2520Our%2520approach%2520leverages%2520a%2520multi-agent%2520decision-making%250Amodel%2520known%2520as%2520the%2520Nonlinear%2520Opinion%2520Dynamics%2520%2528NOD%2529%2520to%2520address%2520the%2520narrow%250Acorridor%2520passage%2520problem%252C%2520formulated%2520as%2520a%2520multi-robot%2520navigation%2520game.%2520By%250Aintegrating%2520the%2520NOD%2520model%2520with%2520a%2520multi-robot%2520path%2520planning%2520algorithm%252C%2520we%250Ademonstrate%2520that%2520the%2520framework%2520effectively%2520reduces%2520the%2520likelihood%2520of%2520deadlocks%250Aduring%2520corridor%2520traversal.%2520To%2520ensure%2520scalability%2520with%2520an%2520increasing%2520number%2520of%250Arobots%252C%2520we%2520introduce%2520a%2520game%2520reduction%2520technique%2520that%2520enables%2520efficient%250Acoordination%2520in%2520larger%2520groups.%2520Extensive%2520simulation%2520studies%2520are%2520conducted%2520to%250Avalidate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Opinion-Driven%20Decision-Making%20for%20Multi-Robot%20Navigation%20through%20Narrow%0A%20%20Corridors&entry.906535625=Norah%20K.%20Alghamdi%20and%20Shinkyu%20Park&entry.1292438233=%20%20We%20propose%20an%20opinion-driven%20navigation%20framework%20for%20multi-robot%20traversal%0Athrough%20a%20narrow%20corridor.%20Our%20approach%20leverages%20a%20multi-agent%20decision-making%0Amodel%20known%20as%20the%20Nonlinear%20Opinion%20Dynamics%20%28NOD%29%20to%20address%20the%20narrow%0Acorridor%20passage%20problem%2C%20formulated%20as%20a%20multi-robot%20navigation%20game.%20By%0Aintegrating%20the%20NOD%20model%20with%20a%20multi-robot%20path%20planning%20algorithm%2C%20we%0Ademonstrate%20that%20the%20framework%20effectively%20reduces%20the%20likelihood%20of%20deadlocks%0Aduring%20corridor%20traversal.%20To%20ensure%20scalability%20with%20an%20increasing%20number%20of%0Arobots%2C%20we%20introduce%20a%20game%20reduction%20technique%20that%20enables%20efficient%0Acoordination%20in%20larger%20groups.%20Extensive%20simulation%20studies%20are%20conducted%20to%0Avalidate%20the%20effectiveness%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20947v1&entry.124074799=Read"},
{"title": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving", "author": "Azam Ikram and Xiang Li and Sameh Elnikety and Saurabh Bagchi", "abstract": "  The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.\n", "link": "http://arxiv.org/abs/2504.20828v1", "date": "2025-04-29", "relevancy": 1.6567, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4199}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4132}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ascendra%3A%20Dynamic%20Request%20Prioritization%20for%20Efficient%20LLM%20Serving&body=Title%3A%20Ascendra%3A%20Dynamic%20Request%20Prioritization%20for%20Efficient%20LLM%20Serving%0AAuthor%3A%20Azam%20Ikram%20and%20Xiang%20Li%20and%20Sameh%20Elnikety%20and%20Saurabh%20Bagchi%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20driven%20the%20need%20for%0Amore%20efficient%20serving%20strategies.%20In%20this%20context%2C%20efficiency%20refers%20to%20the%0Aproportion%20of%20requests%20that%20meet%20their%20Service%20Level%20Objectives%20%28SLOs%29%2C%0Aparticularly%20for%20Time%20To%20First%20Token%20%28TTFT%29%20and%20Time%20Between%20Tokens%20%28TBT%29.%0AHowever%2C%20existing%20systems%20often%20prioritize%20one%20metric%20at%20the%20cost%20of%20the%20other.%0AWe%20present%20Ascendra%2C%20an%20LLM%20serving%20system%20designed%20to%20meet%20both%20TTFT%20and%20TBT%0ASLOs%20simultaneously.%20The%20core%20insight%20behind%20Ascendra%20is%20that%20a%20request%27s%0Aurgency%20evolves%20as%20it%20approaches%20its%20deadline.%20To%20leverage%20this%2C%20Ascendra%0Apartitions%20GPU%20resources%20into%20two%20types%20of%20instances%3A%20low-priority%20and%0Ahigh-priority.%20Low-priority%20instances%20maximize%20throughput%20by%20processing%0Arequests%20out%20of%20arrival%20order%2C%20but%20at%20the%20risk%20of%20request%20starvation.%20To%0Aaddress%20this%2C%20Ascendra%20employs%20a%20performance%20model%20to%20predict%20requests%20at%20risk%0Aof%20missing%20their%20SLOs%20and%20proactively%20offloads%20them%20to%20high-priority%20instances.%0AHigh-priority%20instances%20are%20optimized%20for%20low-latency%20execution%20and%20handle%0Aurgent%20requests%20nearing%20their%20deadlines.%20This%20partitioned%20architecture%20enables%0AAscendra%20to%20effectively%20balance%20high%20throughput%20and%20low%20latency.%20Extensive%0Aevaluation%20shows%20that%20Ascendra%20improves%20system%20throughput%20by%20up%20to%201.7x%0Acompared%20to%20vLLM%20and%20Sarathi-Serve%20while%20meeting%20both%20TTFT%20and%20TBT%20SLOs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20828v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAscendra%253A%2520Dynamic%2520Request%2520Prioritization%2520for%2520Efficient%2520LLM%2520Serving%26entry.906535625%3DAzam%2520Ikram%2520and%2520Xiang%2520Li%2520and%2520Sameh%2520Elnikety%2520and%2520Saurabh%2520Bagchi%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520driven%2520the%2520need%2520for%250Amore%2520efficient%2520serving%2520strategies.%2520In%2520this%2520context%252C%2520efficiency%2520refers%2520to%2520the%250Aproportion%2520of%2520requests%2520that%2520meet%2520their%2520Service%2520Level%2520Objectives%2520%2528SLOs%2529%252C%250Aparticularly%2520for%2520Time%2520To%2520First%2520Token%2520%2528TTFT%2529%2520and%2520Time%2520Between%2520Tokens%2520%2528TBT%2529.%250AHowever%252C%2520existing%2520systems%2520often%2520prioritize%2520one%2520metric%2520at%2520the%2520cost%2520of%2520the%2520other.%250AWe%2520present%2520Ascendra%252C%2520an%2520LLM%2520serving%2520system%2520designed%2520to%2520meet%2520both%2520TTFT%2520and%2520TBT%250ASLOs%2520simultaneously.%2520The%2520core%2520insight%2520behind%2520Ascendra%2520is%2520that%2520a%2520request%2527s%250Aurgency%2520evolves%2520as%2520it%2520approaches%2520its%2520deadline.%2520To%2520leverage%2520this%252C%2520Ascendra%250Apartitions%2520GPU%2520resources%2520into%2520two%2520types%2520of%2520instances%253A%2520low-priority%2520and%250Ahigh-priority.%2520Low-priority%2520instances%2520maximize%2520throughput%2520by%2520processing%250Arequests%2520out%2520of%2520arrival%2520order%252C%2520but%2520at%2520the%2520risk%2520of%2520request%2520starvation.%2520To%250Aaddress%2520this%252C%2520Ascendra%2520employs%2520a%2520performance%2520model%2520to%2520predict%2520requests%2520at%2520risk%250Aof%2520missing%2520their%2520SLOs%2520and%2520proactively%2520offloads%2520them%2520to%2520high-priority%2520instances.%250AHigh-priority%2520instances%2520are%2520optimized%2520for%2520low-latency%2520execution%2520and%2520handle%250Aurgent%2520requests%2520nearing%2520their%2520deadlines.%2520This%2520partitioned%2520architecture%2520enables%250AAscendra%2520to%2520effectively%2520balance%2520high%2520throughput%2520and%2520low%2520latency.%2520Extensive%250Aevaluation%2520shows%2520that%2520Ascendra%2520improves%2520system%2520throughput%2520by%2520up%2520to%25201.7x%250Acompared%2520to%2520vLLM%2520and%2520Sarathi-Serve%2520while%2520meeting%2520both%2520TTFT%2520and%2520TBT%2520SLOs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20828v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ascendra%3A%20Dynamic%20Request%20Prioritization%20for%20Efficient%20LLM%20Serving&entry.906535625=Azam%20Ikram%20and%20Xiang%20Li%20and%20Sameh%20Elnikety%20and%20Saurabh%20Bagchi&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20driven%20the%20need%20for%0Amore%20efficient%20serving%20strategies.%20In%20this%20context%2C%20efficiency%20refers%20to%20the%0Aproportion%20of%20requests%20that%20meet%20their%20Service%20Level%20Objectives%20%28SLOs%29%2C%0Aparticularly%20for%20Time%20To%20First%20Token%20%28TTFT%29%20and%20Time%20Between%20Tokens%20%28TBT%29.%0AHowever%2C%20existing%20systems%20often%20prioritize%20one%20metric%20at%20the%20cost%20of%20the%20other.%0AWe%20present%20Ascendra%2C%20an%20LLM%20serving%20system%20designed%20to%20meet%20both%20TTFT%20and%20TBT%0ASLOs%20simultaneously.%20The%20core%20insight%20behind%20Ascendra%20is%20that%20a%20request%27s%0Aurgency%20evolves%20as%20it%20approaches%20its%20deadline.%20To%20leverage%20this%2C%20Ascendra%0Apartitions%20GPU%20resources%20into%20two%20types%20of%20instances%3A%20low-priority%20and%0Ahigh-priority.%20Low-priority%20instances%20maximize%20throughput%20by%20processing%0Arequests%20out%20of%20arrival%20order%2C%20but%20at%20the%20risk%20of%20request%20starvation.%20To%0Aaddress%20this%2C%20Ascendra%20employs%20a%20performance%20model%20to%20predict%20requests%20at%20risk%0Aof%20missing%20their%20SLOs%20and%20proactively%20offloads%20them%20to%20high-priority%20instances.%0AHigh-priority%20instances%20are%20optimized%20for%20low-latency%20execution%20and%20handle%0Aurgent%20requests%20nearing%20their%20deadlines.%20This%20partitioned%20architecture%20enables%0AAscendra%20to%20effectively%20balance%20high%20throughput%20and%20low%20latency.%20Extensive%0Aevaluation%20shows%20that%20Ascendra%20improves%20system%20throughput%20by%20up%20to%201.7x%0Acompared%20to%20vLLM%20and%20Sarathi-Serve%20while%20meeting%20both%20TTFT%20and%20TBT%20SLOs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20828v1&entry.124074799=Read"},
{"title": "ACE: A Security Architecture for LLM-Integrated App Systems", "author": "Evan Li and Tushin Mallick and Evan Rose and William Robertson and Alina Oprea and Cristina Nita-Rotaru", "abstract": "  LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness.\n", "link": "http://arxiv.org/abs/2504.20984v1", "date": "2025-04-29", "relevancy": 1.654, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4418}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4128}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACE%3A%20A%20Security%20Architecture%20for%20LLM-Integrated%20App%20Systems&body=Title%3A%20ACE%3A%20A%20Security%20Architecture%20for%20LLM-Integrated%20App%20Systems%0AAuthor%3A%20Evan%20Li%20and%20Tushin%20Mallick%20and%20Evan%20Rose%20and%20William%20Robertson%20and%20Alina%20Oprea%20and%20Cristina%20Nita-Rotaru%0AAbstract%3A%20%20%20LLM-integrated%20app%20systems%20extend%20the%20utility%20of%20Large%20Language%20Models%20%28LLMs%29%0Awith%20third-party%20apps%20that%20are%20invoked%20by%20a%20system%20LLM%20using%20interleaved%0Aplanning%20and%20execution%20phases%20to%20answer%20user%20queries.%20These%20systems%20introduce%0Anew%20attack%20vectors%20where%20malicious%20apps%20can%20cause%20integrity%20violation%20of%0Aplanning%20or%20execution%2C%20availability%20breakdown%2C%20or%20privacy%20compromise%20during%0Aexecution.%0A%20%20In%20this%20work%2C%20we%20identify%20new%20attacks%20impacting%20the%20integrity%20of%20planning%2C%20as%0Awell%20as%20the%20integrity%20and%20availability%20of%20execution%20in%20LLM-integrated%20apps%2C%20and%0Ademonstrate%20them%20against%20IsolateGPT%2C%20a%20recent%20solution%20designed%20to%20mitigate%0Aattacks%20from%20malicious%20apps.%20We%20propose%20Abstract-Concrete-Execute%20%28ACE%29%2C%20a%20new%0Asecure%20architecture%20for%20LLM-integrated%20app%20systems%20that%20provides%20security%0Aguarantees%20for%20system%20planning%20and%20execution.%20Specifically%2C%20ACE%20decouples%0Aplanning%20into%20two%20phases%20by%20first%20creating%20an%20abstract%20execution%20plan%20using%0Aonly%20trusted%20information%2C%20and%20then%20mapping%20the%20abstract%20plan%20to%20a%20concrete%20plan%0Ausing%20installed%20system%20apps.%20We%20verify%20that%20the%20plans%20generated%20by%20our%20system%0Asatisfy%20user-specified%20secure%20information%20flow%20constraints%20via%20static%20analysis%0Aon%20the%20structured%20plan%20output.%20During%20execution%2C%20ACE%20enforces%20data%20and%0Acapability%20barriers%20between%20apps%2C%20and%20ensures%20that%20the%20execution%20is%20conducted%0Aaccording%20to%20the%20trusted%20abstract%20plan.%20We%20show%20experimentally%20that%20our%20system%0Ais%20secure%20against%20attacks%20from%20the%20INJECAGENT%20benchmark%2C%20a%20standard%20benchmark%0Afor%20control%20flow%20integrity%20in%20the%20face%20of%20indirect%20prompt%20injection%20attacks%2C%0Aand%20our%20newly%20introduced%20attacks.%20Our%20architecture%20represents%20a%20significant%0Aadvancement%20towards%20hardening%20LLM-based%20systems%20containing%20system%20facilities%20of%0Avarying%20levels%20of%20trustworthiness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACE%253A%2520A%2520Security%2520Architecture%2520for%2520LLM-Integrated%2520App%2520Systems%26entry.906535625%3DEvan%2520Li%2520and%2520Tushin%2520Mallick%2520and%2520Evan%2520Rose%2520and%2520William%2520Robertson%2520and%2520Alina%2520Oprea%2520and%2520Cristina%2520Nita-Rotaru%26entry.1292438233%3D%2520%2520LLM-integrated%2520app%2520systems%2520extend%2520the%2520utility%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%250Awith%2520third-party%2520apps%2520that%2520are%2520invoked%2520by%2520a%2520system%2520LLM%2520using%2520interleaved%250Aplanning%2520and%2520execution%2520phases%2520to%2520answer%2520user%2520queries.%2520These%2520systems%2520introduce%250Anew%2520attack%2520vectors%2520where%2520malicious%2520apps%2520can%2520cause%2520integrity%2520violation%2520of%250Aplanning%2520or%2520execution%252C%2520availability%2520breakdown%252C%2520or%2520privacy%2520compromise%2520during%250Aexecution.%250A%2520%2520In%2520this%2520work%252C%2520we%2520identify%2520new%2520attacks%2520impacting%2520the%2520integrity%2520of%2520planning%252C%2520as%250Awell%2520as%2520the%2520integrity%2520and%2520availability%2520of%2520execution%2520in%2520LLM-integrated%2520apps%252C%2520and%250Ademonstrate%2520them%2520against%2520IsolateGPT%252C%2520a%2520recent%2520solution%2520designed%2520to%2520mitigate%250Aattacks%2520from%2520malicious%2520apps.%2520We%2520propose%2520Abstract-Concrete-Execute%2520%2528ACE%2529%252C%2520a%2520new%250Asecure%2520architecture%2520for%2520LLM-integrated%2520app%2520systems%2520that%2520provides%2520security%250Aguarantees%2520for%2520system%2520planning%2520and%2520execution.%2520Specifically%252C%2520ACE%2520decouples%250Aplanning%2520into%2520two%2520phases%2520by%2520first%2520creating%2520an%2520abstract%2520execution%2520plan%2520using%250Aonly%2520trusted%2520information%252C%2520and%2520then%2520mapping%2520the%2520abstract%2520plan%2520to%2520a%2520concrete%2520plan%250Ausing%2520installed%2520system%2520apps.%2520We%2520verify%2520that%2520the%2520plans%2520generated%2520by%2520our%2520system%250Asatisfy%2520user-specified%2520secure%2520information%2520flow%2520constraints%2520via%2520static%2520analysis%250Aon%2520the%2520structured%2520plan%2520output.%2520During%2520execution%252C%2520ACE%2520enforces%2520data%2520and%250Acapability%2520barriers%2520between%2520apps%252C%2520and%2520ensures%2520that%2520the%2520execution%2520is%2520conducted%250Aaccording%2520to%2520the%2520trusted%2520abstract%2520plan.%2520We%2520show%2520experimentally%2520that%2520our%2520system%250Ais%2520secure%2520against%2520attacks%2520from%2520the%2520INJECAGENT%2520benchmark%252C%2520a%2520standard%2520benchmark%250Afor%2520control%2520flow%2520integrity%2520in%2520the%2520face%2520of%2520indirect%2520prompt%2520injection%2520attacks%252C%250Aand%2520our%2520newly%2520introduced%2520attacks.%2520Our%2520architecture%2520represents%2520a%2520significant%250Aadvancement%2520towards%2520hardening%2520LLM-based%2520systems%2520containing%2520system%2520facilities%2520of%250Avarying%2520levels%2520of%2520trustworthiness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACE%3A%20A%20Security%20Architecture%20for%20LLM-Integrated%20App%20Systems&entry.906535625=Evan%20Li%20and%20Tushin%20Mallick%20and%20Evan%20Rose%20and%20William%20Robertson%20and%20Alina%20Oprea%20and%20Cristina%20Nita-Rotaru&entry.1292438233=%20%20LLM-integrated%20app%20systems%20extend%20the%20utility%20of%20Large%20Language%20Models%20%28LLMs%29%0Awith%20third-party%20apps%20that%20are%20invoked%20by%20a%20system%20LLM%20using%20interleaved%0Aplanning%20and%20execution%20phases%20to%20answer%20user%20queries.%20These%20systems%20introduce%0Anew%20attack%20vectors%20where%20malicious%20apps%20can%20cause%20integrity%20violation%20of%0Aplanning%20or%20execution%2C%20availability%20breakdown%2C%20or%20privacy%20compromise%20during%0Aexecution.%0A%20%20In%20this%20work%2C%20we%20identify%20new%20attacks%20impacting%20the%20integrity%20of%20planning%2C%20as%0Awell%20as%20the%20integrity%20and%20availability%20of%20execution%20in%20LLM-integrated%20apps%2C%20and%0Ademonstrate%20them%20against%20IsolateGPT%2C%20a%20recent%20solution%20designed%20to%20mitigate%0Aattacks%20from%20malicious%20apps.%20We%20propose%20Abstract-Concrete-Execute%20%28ACE%29%2C%20a%20new%0Asecure%20architecture%20for%20LLM-integrated%20app%20systems%20that%20provides%20security%0Aguarantees%20for%20system%20planning%20and%20execution.%20Specifically%2C%20ACE%20decouples%0Aplanning%20into%20two%20phases%20by%20first%20creating%20an%20abstract%20execution%20plan%20using%0Aonly%20trusted%20information%2C%20and%20then%20mapping%20the%20abstract%20plan%20to%20a%20concrete%20plan%0Ausing%20installed%20system%20apps.%20We%20verify%20that%20the%20plans%20generated%20by%20our%20system%0Asatisfy%20user-specified%20secure%20information%20flow%20constraints%20via%20static%20analysis%0Aon%20the%20structured%20plan%20output.%20During%20execution%2C%20ACE%20enforces%20data%20and%0Acapability%20barriers%20between%20apps%2C%20and%20ensures%20that%20the%20execution%20is%20conducted%0Aaccording%20to%20the%20trusted%20abstract%20plan.%20We%20show%20experimentally%20that%20our%20system%0Ais%20secure%20against%20attacks%20from%20the%20INJECAGENT%20benchmark%2C%20a%20standard%20benchmark%0Afor%20control%20flow%20integrity%20in%20the%20face%20of%20indirect%20prompt%20injection%20attacks%2C%0Aand%20our%20newly%20introduced%20attacks.%20Our%20architecture%20represents%20a%20significant%0Aadvancement%20towards%20hardening%20LLM-based%20systems%20containing%20system%20facilities%20of%0Avarying%20levels%20of%20trustworthiness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20984v1&entry.124074799=Read"},
{"title": "An approach to melodic segmentation and classification based on\n  filtering with the Haar-wavelet", "author": "Gissel Velarde and Tillman Weyde and David Meredith", "abstract": "  We present a novel method of classification and segmentation of melodies in\nsymbolic representation. The method is based on filtering pitch as a signal\nover time with the Haar-wavelet, and we evaluate it on two tasks. The filtered\nsignal corresponds to a single-scale signal ws from the continuous Haar wavelet\ntransform. The melodies are first segmented using local maxima or\nzero-crossings of w_s. The segments of w_s are then classified using the\nk-nearest neighbour algorithm with Euclidian and city-block distances. The\nmethod proves more effective than using unfiltered pitch signals and\nGestalt-based segmentation when used to recognize the parent works of segments\nfrom Bach's Two-Part Inventions (BWV 772-786). When used to classify 360 Dutch\nfolk tunes into 26 tune families, the performance of the method is comparable\nto the use of pitch signals, but not as good as that of string-matching methods\nbased on multiple features.\n", "link": "http://arxiv.org/abs/2504.20822v1", "date": "2025-04-29", "relevancy": 1.6472, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4415}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3967}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20approach%20to%20melodic%20segmentation%20and%20classification%20based%20on%0A%20%20filtering%20with%20the%20Haar-wavelet&body=Title%3A%20An%20approach%20to%20melodic%20segmentation%20and%20classification%20based%20on%0A%20%20filtering%20with%20the%20Haar-wavelet%0AAuthor%3A%20Gissel%20Velarde%20and%20Tillman%20Weyde%20and%20David%20Meredith%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20of%20classification%20and%20segmentation%20of%20melodies%20in%0Asymbolic%20representation.%20The%20method%20is%20based%20on%20filtering%20pitch%20as%20a%20signal%0Aover%20time%20with%20the%20Haar-wavelet%2C%20and%20we%20evaluate%20it%20on%20two%20tasks.%20The%20filtered%0Asignal%20corresponds%20to%20a%20single-scale%20signal%20ws%20from%20the%20continuous%20Haar%20wavelet%0Atransform.%20The%20melodies%20are%20first%20segmented%20using%20local%20maxima%20or%0Azero-crossings%20of%20w_s.%20The%20segments%20of%20w_s%20are%20then%20classified%20using%20the%0Ak-nearest%20neighbour%20algorithm%20with%20Euclidian%20and%20city-block%20distances.%20The%0Amethod%20proves%20more%20effective%20than%20using%20unfiltered%20pitch%20signals%20and%0AGestalt-based%20segmentation%20when%20used%20to%20recognize%20the%20parent%20works%20of%20segments%0Afrom%20Bach%27s%20Two-Part%20Inventions%20%28BWV%20772-786%29.%20When%20used%20to%20classify%20360%20Dutch%0Afolk%20tunes%20into%2026%20tune%20families%2C%20the%20performance%20of%20the%20method%20is%20comparable%0Ato%20the%20use%20of%20pitch%20signals%2C%20but%20not%20as%20good%20as%20that%20of%20string-matching%20methods%0Abased%20on%20multiple%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520approach%2520to%2520melodic%2520segmentation%2520and%2520classification%2520based%2520on%250A%2520%2520filtering%2520with%2520the%2520Haar-wavelet%26entry.906535625%3DGissel%2520Velarde%2520and%2520Tillman%2520Weyde%2520and%2520David%2520Meredith%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520method%2520of%2520classification%2520and%2520segmentation%2520of%2520melodies%2520in%250Asymbolic%2520representation.%2520The%2520method%2520is%2520based%2520on%2520filtering%2520pitch%2520as%2520a%2520signal%250Aover%2520time%2520with%2520the%2520Haar-wavelet%252C%2520and%2520we%2520evaluate%2520it%2520on%2520two%2520tasks.%2520The%2520filtered%250Asignal%2520corresponds%2520to%2520a%2520single-scale%2520signal%2520ws%2520from%2520the%2520continuous%2520Haar%2520wavelet%250Atransform.%2520The%2520melodies%2520are%2520first%2520segmented%2520using%2520local%2520maxima%2520or%250Azero-crossings%2520of%2520w_s.%2520The%2520segments%2520of%2520w_s%2520are%2520then%2520classified%2520using%2520the%250Ak-nearest%2520neighbour%2520algorithm%2520with%2520Euclidian%2520and%2520city-block%2520distances.%2520The%250Amethod%2520proves%2520more%2520effective%2520than%2520using%2520unfiltered%2520pitch%2520signals%2520and%250AGestalt-based%2520segmentation%2520when%2520used%2520to%2520recognize%2520the%2520parent%2520works%2520of%2520segments%250Afrom%2520Bach%2527s%2520Two-Part%2520Inventions%2520%2528BWV%2520772-786%2529.%2520When%2520used%2520to%2520classify%2520360%2520Dutch%250Afolk%2520tunes%2520into%252026%2520tune%2520families%252C%2520the%2520performance%2520of%2520the%2520method%2520is%2520comparable%250Ato%2520the%2520use%2520of%2520pitch%2520signals%252C%2520but%2520not%2520as%2520good%2520as%2520that%2520of%2520string-matching%2520methods%250Abased%2520on%2520multiple%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20approach%20to%20melodic%20segmentation%20and%20classification%20based%20on%0A%20%20filtering%20with%20the%20Haar-wavelet&entry.906535625=Gissel%20Velarde%20and%20Tillman%20Weyde%20and%20David%20Meredith&entry.1292438233=%20%20We%20present%20a%20novel%20method%20of%20classification%20and%20segmentation%20of%20melodies%20in%0Asymbolic%20representation.%20The%20method%20is%20based%20on%20filtering%20pitch%20as%20a%20signal%0Aover%20time%20with%20the%20Haar-wavelet%2C%20and%20we%20evaluate%20it%20on%20two%20tasks.%20The%20filtered%0Asignal%20corresponds%20to%20a%20single-scale%20signal%20ws%20from%20the%20continuous%20Haar%20wavelet%0Atransform.%20The%20melodies%20are%20first%20segmented%20using%20local%20maxima%20or%0Azero-crossings%20of%20w_s.%20The%20segments%20of%20w_s%20are%20then%20classified%20using%20the%0Ak-nearest%20neighbour%20algorithm%20with%20Euclidian%20and%20city-block%20distances.%20The%0Amethod%20proves%20more%20effective%20than%20using%20unfiltered%20pitch%20signals%20and%0AGestalt-based%20segmentation%20when%20used%20to%20recognize%20the%20parent%20works%20of%20segments%0Afrom%20Bach%27s%20Two-Part%20Inventions%20%28BWV%20772-786%29.%20When%20used%20to%20classify%20360%20Dutch%0Afolk%20tunes%20into%2026%20tune%20families%2C%20the%20performance%20of%20the%20method%20is%20comparable%0Ato%20the%20use%20of%20pitch%20signals%2C%20but%20not%20as%20good%20as%20that%20of%20string-matching%20methods%0Abased%20on%20multiple%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20822v1&entry.124074799=Read"},
{"title": "Energy-Based Coarse-Graining in Molecular Dynamics: A Flow-Based\n  Framework Without Data", "author": "Maximilian Stupp and P. S. Koutsourelakis", "abstract": "  Coarse-grained (CG) models offer an effective route to reducing the\ncomplexity of molecular simulations, yet conventional approaches depend heavily\non long all-atom molecular dynamics (MD) trajectories to adequately sample\nconfigurational space. This data-driven dependence limits their accuracy and\ngeneralizability, as unvisited configurations remain excluded from the\nresulting CG model. We introduce a data-free generative framework for\ncoarse-graining that directly targets the all-atom Boltzmann distribution. Our\nmodel defines a structured latent space comprising slow collective variables,\nwhich are statistically associated with multimodal marginal densities capturing\nmetastable states, and fast variables, which represent the remaining degrees of\nfreedom with simple, unimodal conditional distributions. A potentially\nlearnable, bijective map from the full latent space to the all-atom\nconfiguration space enables automatic and accurate reconstruction of molecular\nstructures. The model is trained using an energy-based objective that minimizes\nthe reverse Kullback-Leibler divergence, relying solely on the interatomic\npotential rather than sampled trajectories. A tempering scheme is used to\nstabilize training and promote exploration of diverse configurations. Once\ntrained, the model can generate unbiased, one-shot equilibrium all-atom\nsamples. We validate the method on two synthetic systems-a double-well\npotential and a Gaussian mixture-as well as on the benchmark alanine dipeptide.\nThe model captures all relevant modes of the Boltzmann distribution, accurately\nreconstructs atomic configurations, and learns physically meaningful\ncoarse-grained representations, all without any simulation data.\n", "link": "http://arxiv.org/abs/2504.20940v1", "date": "2025-04-29", "relevancy": 1.639, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5548}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5442}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Energy-Based%20Coarse-Graining%20in%20Molecular%20Dynamics%3A%20A%20Flow-Based%0A%20%20Framework%20Without%20Data&body=Title%3A%20Energy-Based%20Coarse-Graining%20in%20Molecular%20Dynamics%3A%20A%20Flow-Based%0A%20%20Framework%20Without%20Data%0AAuthor%3A%20Maximilian%20Stupp%20and%20P.%20S.%20Koutsourelakis%0AAbstract%3A%20%20%20Coarse-grained%20%28CG%29%20models%20offer%20an%20effective%20route%20to%20reducing%20the%0Acomplexity%20of%20molecular%20simulations%2C%20yet%20conventional%20approaches%20depend%20heavily%0Aon%20long%20all-atom%20molecular%20dynamics%20%28MD%29%20trajectories%20to%20adequately%20sample%0Aconfigurational%20space.%20This%20data-driven%20dependence%20limits%20their%20accuracy%20and%0Ageneralizability%2C%20as%20unvisited%20configurations%20remain%20excluded%20from%20the%0Aresulting%20CG%20model.%20We%20introduce%20a%20data-free%20generative%20framework%20for%0Acoarse-graining%20that%20directly%20targets%20the%20all-atom%20Boltzmann%20distribution.%20Our%0Amodel%20defines%20a%20structured%20latent%20space%20comprising%20slow%20collective%20variables%2C%0Awhich%20are%20statistically%20associated%20with%20multimodal%20marginal%20densities%20capturing%0Ametastable%20states%2C%20and%20fast%20variables%2C%20which%20represent%20the%20remaining%20degrees%20of%0Afreedom%20with%20simple%2C%20unimodal%20conditional%20distributions.%20A%20potentially%0Alearnable%2C%20bijective%20map%20from%20the%20full%20latent%20space%20to%20the%20all-atom%0Aconfiguration%20space%20enables%20automatic%20and%20accurate%20reconstruction%20of%20molecular%0Astructures.%20The%20model%20is%20trained%20using%20an%20energy-based%20objective%20that%20minimizes%0Athe%20reverse%20Kullback-Leibler%20divergence%2C%20relying%20solely%20on%20the%20interatomic%0Apotential%20rather%20than%20sampled%20trajectories.%20A%20tempering%20scheme%20is%20used%20to%0Astabilize%20training%20and%20promote%20exploration%20of%20diverse%20configurations.%20Once%0Atrained%2C%20the%20model%20can%20generate%20unbiased%2C%20one-shot%20equilibrium%20all-atom%0Asamples.%20We%20validate%20the%20method%20on%20two%20synthetic%20systems-a%20double-well%0Apotential%20and%20a%20Gaussian%20mixture-as%20well%20as%20on%20the%20benchmark%20alanine%20dipeptide.%0AThe%20model%20captures%20all%20relevant%20modes%20of%20the%20Boltzmann%20distribution%2C%20accurately%0Areconstructs%20atomic%20configurations%2C%20and%20learns%20physically%20meaningful%0Acoarse-grained%20representations%2C%20all%20without%20any%20simulation%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnergy-Based%2520Coarse-Graining%2520in%2520Molecular%2520Dynamics%253A%2520A%2520Flow-Based%250A%2520%2520Framework%2520Without%2520Data%26entry.906535625%3DMaximilian%2520Stupp%2520and%2520P.%2520S.%2520Koutsourelakis%26entry.1292438233%3D%2520%2520Coarse-grained%2520%2528CG%2529%2520models%2520offer%2520an%2520effective%2520route%2520to%2520reducing%2520the%250Acomplexity%2520of%2520molecular%2520simulations%252C%2520yet%2520conventional%2520approaches%2520depend%2520heavily%250Aon%2520long%2520all-atom%2520molecular%2520dynamics%2520%2528MD%2529%2520trajectories%2520to%2520adequately%2520sample%250Aconfigurational%2520space.%2520This%2520data-driven%2520dependence%2520limits%2520their%2520accuracy%2520and%250Ageneralizability%252C%2520as%2520unvisited%2520configurations%2520remain%2520excluded%2520from%2520the%250Aresulting%2520CG%2520model.%2520We%2520introduce%2520a%2520data-free%2520generative%2520framework%2520for%250Acoarse-graining%2520that%2520directly%2520targets%2520the%2520all-atom%2520Boltzmann%2520distribution.%2520Our%250Amodel%2520defines%2520a%2520structured%2520latent%2520space%2520comprising%2520slow%2520collective%2520variables%252C%250Awhich%2520are%2520statistically%2520associated%2520with%2520multimodal%2520marginal%2520densities%2520capturing%250Ametastable%2520states%252C%2520and%2520fast%2520variables%252C%2520which%2520represent%2520the%2520remaining%2520degrees%2520of%250Afreedom%2520with%2520simple%252C%2520unimodal%2520conditional%2520distributions.%2520A%2520potentially%250Alearnable%252C%2520bijective%2520map%2520from%2520the%2520full%2520latent%2520space%2520to%2520the%2520all-atom%250Aconfiguration%2520space%2520enables%2520automatic%2520and%2520accurate%2520reconstruction%2520of%2520molecular%250Astructures.%2520The%2520model%2520is%2520trained%2520using%2520an%2520energy-based%2520objective%2520that%2520minimizes%250Athe%2520reverse%2520Kullback-Leibler%2520divergence%252C%2520relying%2520solely%2520on%2520the%2520interatomic%250Apotential%2520rather%2520than%2520sampled%2520trajectories.%2520A%2520tempering%2520scheme%2520is%2520used%2520to%250Astabilize%2520training%2520and%2520promote%2520exploration%2520of%2520diverse%2520configurations.%2520Once%250Atrained%252C%2520the%2520model%2520can%2520generate%2520unbiased%252C%2520one-shot%2520equilibrium%2520all-atom%250Asamples.%2520We%2520validate%2520the%2520method%2520on%2520two%2520synthetic%2520systems-a%2520double-well%250Apotential%2520and%2520a%2520Gaussian%2520mixture-as%2520well%2520as%2520on%2520the%2520benchmark%2520alanine%2520dipeptide.%250AThe%2520model%2520captures%2520all%2520relevant%2520modes%2520of%2520the%2520Boltzmann%2520distribution%252C%2520accurately%250Areconstructs%2520atomic%2520configurations%252C%2520and%2520learns%2520physically%2520meaningful%250Acoarse-grained%2520representations%252C%2520all%2520without%2520any%2520simulation%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Energy-Based%20Coarse-Graining%20in%20Molecular%20Dynamics%3A%20A%20Flow-Based%0A%20%20Framework%20Without%20Data&entry.906535625=Maximilian%20Stupp%20and%20P.%20S.%20Koutsourelakis&entry.1292438233=%20%20Coarse-grained%20%28CG%29%20models%20offer%20an%20effective%20route%20to%20reducing%20the%0Acomplexity%20of%20molecular%20simulations%2C%20yet%20conventional%20approaches%20depend%20heavily%0Aon%20long%20all-atom%20molecular%20dynamics%20%28MD%29%20trajectories%20to%20adequately%20sample%0Aconfigurational%20space.%20This%20data-driven%20dependence%20limits%20their%20accuracy%20and%0Ageneralizability%2C%20as%20unvisited%20configurations%20remain%20excluded%20from%20the%0Aresulting%20CG%20model.%20We%20introduce%20a%20data-free%20generative%20framework%20for%0Acoarse-graining%20that%20directly%20targets%20the%20all-atom%20Boltzmann%20distribution.%20Our%0Amodel%20defines%20a%20structured%20latent%20space%20comprising%20slow%20collective%20variables%2C%0Awhich%20are%20statistically%20associated%20with%20multimodal%20marginal%20densities%20capturing%0Ametastable%20states%2C%20and%20fast%20variables%2C%20which%20represent%20the%20remaining%20degrees%20of%0Afreedom%20with%20simple%2C%20unimodal%20conditional%20distributions.%20A%20potentially%0Alearnable%2C%20bijective%20map%20from%20the%20full%20latent%20space%20to%20the%20all-atom%0Aconfiguration%20space%20enables%20automatic%20and%20accurate%20reconstruction%20of%20molecular%0Astructures.%20The%20model%20is%20trained%20using%20an%20energy-based%20objective%20that%20minimizes%0Athe%20reverse%20Kullback-Leibler%20divergence%2C%20relying%20solely%20on%20the%20interatomic%0Apotential%20rather%20than%20sampled%20trajectories.%20A%20tempering%20scheme%20is%20used%20to%0Astabilize%20training%20and%20promote%20exploration%20of%20diverse%20configurations.%20Once%0Atrained%2C%20the%20model%20can%20generate%20unbiased%2C%20one-shot%20equilibrium%20all-atom%0Asamples.%20We%20validate%20the%20method%20on%20two%20synthetic%20systems-a%20double-well%0Apotential%20and%20a%20Gaussian%20mixture-as%20well%20as%20on%20the%20benchmark%20alanine%20dipeptide.%0AThe%20model%20captures%20all%20relevant%20modes%20of%20the%20Boltzmann%20distribution%2C%20accurately%0Areconstructs%20atomic%20configurations%2C%20and%20learns%20physically%20meaningful%0Acoarse-grained%20representations%2C%20all%20without%20any%20simulation%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20940v1&entry.124074799=Read"},
{"title": "DYNAMAX: Dynamic computing for Transformers and Mamba based\n  architectures", "author": "Miguel Nogales and Matteo Gambella and Manuel Roveri", "abstract": "  Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs.\n", "link": "http://arxiv.org/abs/2504.20922v1", "date": "2025-04-29", "relevancy": 1.639, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5731}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5156}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DYNAMAX%3A%20Dynamic%20computing%20for%20Transformers%20and%20Mamba%20based%0A%20%20architectures&body=Title%3A%20DYNAMAX%3A%20Dynamic%20computing%20for%20Transformers%20and%20Mamba%20based%0A%20%20architectures%0AAuthor%3A%20Miguel%20Nogales%20and%20Matteo%20Gambella%20and%20Manuel%20Roveri%0AAbstract%3A%20%20%20Early%20exits%20%28EEs%29%20offer%20a%20promising%20approach%20to%20reducing%20computational%20costs%0Aand%20latency%20by%20dynamically%20terminating%20inference%20once%20a%20satisfactory%20prediction%0Aconfidence%20on%20a%20data%20sample%20is%20achieved.%20Although%20many%20works%20integrate%20EEs%20into%0Aencoder-only%20Transformers%2C%20their%20application%20to%20decoder-only%20architectures%20and%2C%0Amore%20importantly%2C%20Mamba%20models%2C%20a%20novel%20family%20of%20state-space%20architectures%20in%0Athe%20LLM%20realm%2C%20remains%20insufficiently%20explored.%20This%20work%20introduces%20DYNAMAX%2C%0Athe%20first%20framework%20to%20exploit%20the%20unique%20properties%20of%20Mamba%20architectures%20for%0Aearly%20exit%20mechanisms.%20We%20not%20only%20integrate%20EEs%20into%20Mamba%20but%20also%20repurpose%0AMamba%20as%20an%20efficient%20EE%20classifier%20for%20both%20Mamba-based%20and%20transformer-based%0ALLMs%2C%20showcasing%20its%20versatility.%20Our%20experiments%20employ%20the%20Mistral%207B%0Atransformer%20compared%20to%20the%20Codestral%207B%20Mamba%20model%2C%20using%20data%20sets%20such%20as%0ATruthfulQA%2C%20CoQA%2C%20and%20TriviaQA%20to%20evaluate%20computational%20savings%2C%20accuracy%2C%20and%0Aconsistency.%20The%20results%20highlight%20the%20adaptability%20of%20Mamba%20as%20a%20powerful%20EE%0Aclassifier%20and%20its%20efficiency%20in%20balancing%20computational%20cost%20and%20performance%0Aquality%20across%20NLP%20tasks.%20By%20leveraging%20Mamba%27s%20inherent%20design%20for%20dynamic%0Aprocessing%2C%20we%20open%20pathways%20for%20scalable%20and%20efficient%20inference%20in%20embedded%0Aapplications%20and%20resource-constrained%20environments.%20This%20study%20underscores%20the%0Atransformative%20potential%20of%20Mamba%20in%20redefining%20dynamic%20computing%20paradigms%20for%0ALLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDYNAMAX%253A%2520Dynamic%2520computing%2520for%2520Transformers%2520and%2520Mamba%2520based%250A%2520%2520architectures%26entry.906535625%3DMiguel%2520Nogales%2520and%2520Matteo%2520Gambella%2520and%2520Manuel%2520Roveri%26entry.1292438233%3D%2520%2520Early%2520exits%2520%2528EEs%2529%2520offer%2520a%2520promising%2520approach%2520to%2520reducing%2520computational%2520costs%250Aand%2520latency%2520by%2520dynamically%2520terminating%2520inference%2520once%2520a%2520satisfactory%2520prediction%250Aconfidence%2520on%2520a%2520data%2520sample%2520is%2520achieved.%2520Although%2520many%2520works%2520integrate%2520EEs%2520into%250Aencoder-only%2520Transformers%252C%2520their%2520application%2520to%2520decoder-only%2520architectures%2520and%252C%250Amore%2520importantly%252C%2520Mamba%2520models%252C%2520a%2520novel%2520family%2520of%2520state-space%2520architectures%2520in%250Athe%2520LLM%2520realm%252C%2520remains%2520insufficiently%2520explored.%2520This%2520work%2520introduces%2520DYNAMAX%252C%250Athe%2520first%2520framework%2520to%2520exploit%2520the%2520unique%2520properties%2520of%2520Mamba%2520architectures%2520for%250Aearly%2520exit%2520mechanisms.%2520We%2520not%2520only%2520integrate%2520EEs%2520into%2520Mamba%2520but%2520also%2520repurpose%250AMamba%2520as%2520an%2520efficient%2520EE%2520classifier%2520for%2520both%2520Mamba-based%2520and%2520transformer-based%250ALLMs%252C%2520showcasing%2520its%2520versatility.%2520Our%2520experiments%2520employ%2520the%2520Mistral%25207B%250Atransformer%2520compared%2520to%2520the%2520Codestral%25207B%2520Mamba%2520model%252C%2520using%2520data%2520sets%2520such%2520as%250ATruthfulQA%252C%2520CoQA%252C%2520and%2520TriviaQA%2520to%2520evaluate%2520computational%2520savings%252C%2520accuracy%252C%2520and%250Aconsistency.%2520The%2520results%2520highlight%2520the%2520adaptability%2520of%2520Mamba%2520as%2520a%2520powerful%2520EE%250Aclassifier%2520and%2520its%2520efficiency%2520in%2520balancing%2520computational%2520cost%2520and%2520performance%250Aquality%2520across%2520NLP%2520tasks.%2520By%2520leveraging%2520Mamba%2527s%2520inherent%2520design%2520for%2520dynamic%250Aprocessing%252C%2520we%2520open%2520pathways%2520for%2520scalable%2520and%2520efficient%2520inference%2520in%2520embedded%250Aapplications%2520and%2520resource-constrained%2520environments.%2520This%2520study%2520underscores%2520the%250Atransformative%2520potential%2520of%2520Mamba%2520in%2520redefining%2520dynamic%2520computing%2520paradigms%2520for%250ALLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DYNAMAX%3A%20Dynamic%20computing%20for%20Transformers%20and%20Mamba%20based%0A%20%20architectures&entry.906535625=Miguel%20Nogales%20and%20Matteo%20Gambella%20and%20Manuel%20Roveri&entry.1292438233=%20%20Early%20exits%20%28EEs%29%20offer%20a%20promising%20approach%20to%20reducing%20computational%20costs%0Aand%20latency%20by%20dynamically%20terminating%20inference%20once%20a%20satisfactory%20prediction%0Aconfidence%20on%20a%20data%20sample%20is%20achieved.%20Although%20many%20works%20integrate%20EEs%20into%0Aencoder-only%20Transformers%2C%20their%20application%20to%20decoder-only%20architectures%20and%2C%0Amore%20importantly%2C%20Mamba%20models%2C%20a%20novel%20family%20of%20state-space%20architectures%20in%0Athe%20LLM%20realm%2C%20remains%20insufficiently%20explored.%20This%20work%20introduces%20DYNAMAX%2C%0Athe%20first%20framework%20to%20exploit%20the%20unique%20properties%20of%20Mamba%20architectures%20for%0Aearly%20exit%20mechanisms.%20We%20not%20only%20integrate%20EEs%20into%20Mamba%20but%20also%20repurpose%0AMamba%20as%20an%20efficient%20EE%20classifier%20for%20both%20Mamba-based%20and%20transformer-based%0ALLMs%2C%20showcasing%20its%20versatility.%20Our%20experiments%20employ%20the%20Mistral%207B%0Atransformer%20compared%20to%20the%20Codestral%207B%20Mamba%20model%2C%20using%20data%20sets%20such%20as%0ATruthfulQA%2C%20CoQA%2C%20and%20TriviaQA%20to%20evaluate%20computational%20savings%2C%20accuracy%2C%20and%0Aconsistency.%20The%20results%20highlight%20the%20adaptability%20of%20Mamba%20as%20a%20powerful%20EE%0Aclassifier%20and%20its%20efficiency%20in%20balancing%20computational%20cost%20and%20performance%0Aquality%20across%20NLP%20tasks.%20By%20leveraging%20Mamba%27s%20inherent%20design%20for%20dynamic%0Aprocessing%2C%20we%20open%20pathways%20for%20scalable%20and%20efficient%20inference%20in%20embedded%0Aapplications%20and%20resource-constrained%20environments.%20This%20study%20underscores%20the%0Atransformative%20potential%20of%20Mamba%20in%20redefining%20dynamic%20computing%20paradigms%20for%0ALLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20922v1&entry.124074799=Read"},
{"title": "UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order\n  Model, application to unsteady flows", "author": "Isma\u00ebl Zighed and Nicolas Thome and Patrick Gallinari and Taraneh Sayadi", "abstract": "  Reduced order models (ROMs) play a critical role in fluid mechanics by\nproviding low-cost predictions, making them an attractive tool for engineering\napplications. However, for ROMs to be widely applicable, they must not only\ngeneralise well across different regimes, but also provide a measure of\nconfidence in their predictions. While recent data-driven approaches have begun\nto address nonlinear reduction techniques to improve predictions in transient\nenvironments, challenges remain in terms of robustness and parametrisation. In\nthis work, we present a nonlinear reduction strategy specifically designed for\ntransient flows that incorporates parametrisation and uncertainty\nquantification. Our reduction strategy features a variational auto-encoder\n(VAE) that uses variational inference for confidence measurement. We use a\nlatent space transformer that incorporates recent advances in attention\nmechanisms to predict dynamical systems. Attention's versatility in learning\nsequences and capturing their dependence on external parameters enhances\ngeneralisation across a wide range of dynamics. Prediction, coupled with\nconfidence, enables more informed decision making and addresses the need for\nmore robust models. In addition, this confidence is used to cost-effectively\nsample the parameter space, improving model performance a priori across the\nentire parameter space without requiring evaluation data for the entire domain.\n", "link": "http://arxiv.org/abs/2503.23236v2", "date": "2025-04-29", "relevancy": 1.5871, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5417}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5332}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UP-dROM%20%3A%20Uncertainty-Aware%20and%20Parametrised%20dynamic%20Reduced-Order%0A%20%20Model%2C%20application%20to%20unsteady%20flows&body=Title%3A%20UP-dROM%20%3A%20Uncertainty-Aware%20and%20Parametrised%20dynamic%20Reduced-Order%0A%20%20Model%2C%20application%20to%20unsteady%20flows%0AAuthor%3A%20Isma%C3%ABl%20Zighed%20and%20Nicolas%20Thome%20and%20Patrick%20Gallinari%20and%20Taraneh%20Sayadi%0AAbstract%3A%20%20%20Reduced%20order%20models%20%28ROMs%29%20play%20a%20critical%20role%20in%20fluid%20mechanics%20by%0Aproviding%20low-cost%20predictions%2C%20making%20them%20an%20attractive%20tool%20for%20engineering%0Aapplications.%20However%2C%20for%20ROMs%20to%20be%20widely%20applicable%2C%20they%20must%20not%20only%0Ageneralise%20well%20across%20different%20regimes%2C%20but%20also%20provide%20a%20measure%20of%0Aconfidence%20in%20their%20predictions.%20While%20recent%20data-driven%20approaches%20have%20begun%0Ato%20address%20nonlinear%20reduction%20techniques%20to%20improve%20predictions%20in%20transient%0Aenvironments%2C%20challenges%20remain%20in%20terms%20of%20robustness%20and%20parametrisation.%20In%0Athis%20work%2C%20we%20present%20a%20nonlinear%20reduction%20strategy%20specifically%20designed%20for%0Atransient%20flows%20that%20incorporates%20parametrisation%20and%20uncertainty%0Aquantification.%20Our%20reduction%20strategy%20features%20a%20variational%20auto-encoder%0A%28VAE%29%20that%20uses%20variational%20inference%20for%20confidence%20measurement.%20We%20use%20a%0Alatent%20space%20transformer%20that%20incorporates%20recent%20advances%20in%20attention%0Amechanisms%20to%20predict%20dynamical%20systems.%20Attention%27s%20versatility%20in%20learning%0Asequences%20and%20capturing%20their%20dependence%20on%20external%20parameters%20enhances%0Ageneralisation%20across%20a%20wide%20range%20of%20dynamics.%20Prediction%2C%20coupled%20with%0Aconfidence%2C%20enables%20more%20informed%20decision%20making%20and%20addresses%20the%20need%20for%0Amore%20robust%20models.%20In%20addition%2C%20this%20confidence%20is%20used%20to%20cost-effectively%0Asample%20the%20parameter%20space%2C%20improving%20model%20performance%20a%20priori%20across%20the%0Aentire%20parameter%20space%20without%20requiring%20evaluation%20data%20for%20the%20entire%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.23236v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUP-dROM%2520%253A%2520Uncertainty-Aware%2520and%2520Parametrised%2520dynamic%2520Reduced-Order%250A%2520%2520Model%252C%2520application%2520to%2520unsteady%2520flows%26entry.906535625%3DIsma%25C3%25ABl%2520Zighed%2520and%2520Nicolas%2520Thome%2520and%2520Patrick%2520Gallinari%2520and%2520Taraneh%2520Sayadi%26entry.1292438233%3D%2520%2520Reduced%2520order%2520models%2520%2528ROMs%2529%2520play%2520a%2520critical%2520role%2520in%2520fluid%2520mechanics%2520by%250Aproviding%2520low-cost%2520predictions%252C%2520making%2520them%2520an%2520attractive%2520tool%2520for%2520engineering%250Aapplications.%2520However%252C%2520for%2520ROMs%2520to%2520be%2520widely%2520applicable%252C%2520they%2520must%2520not%2520only%250Ageneralise%2520well%2520across%2520different%2520regimes%252C%2520but%2520also%2520provide%2520a%2520measure%2520of%250Aconfidence%2520in%2520their%2520predictions.%2520While%2520recent%2520data-driven%2520approaches%2520have%2520begun%250Ato%2520address%2520nonlinear%2520reduction%2520techniques%2520to%2520improve%2520predictions%2520in%2520transient%250Aenvironments%252C%2520challenges%2520remain%2520in%2520terms%2520of%2520robustness%2520and%2520parametrisation.%2520In%250Athis%2520work%252C%2520we%2520present%2520a%2520nonlinear%2520reduction%2520strategy%2520specifically%2520designed%2520for%250Atransient%2520flows%2520that%2520incorporates%2520parametrisation%2520and%2520uncertainty%250Aquantification.%2520Our%2520reduction%2520strategy%2520features%2520a%2520variational%2520auto-encoder%250A%2528VAE%2529%2520that%2520uses%2520variational%2520inference%2520for%2520confidence%2520measurement.%2520We%2520use%2520a%250Alatent%2520space%2520transformer%2520that%2520incorporates%2520recent%2520advances%2520in%2520attention%250Amechanisms%2520to%2520predict%2520dynamical%2520systems.%2520Attention%2527s%2520versatility%2520in%2520learning%250Asequences%2520and%2520capturing%2520their%2520dependence%2520on%2520external%2520parameters%2520enhances%250Ageneralisation%2520across%2520a%2520wide%2520range%2520of%2520dynamics.%2520Prediction%252C%2520coupled%2520with%250Aconfidence%252C%2520enables%2520more%2520informed%2520decision%2520making%2520and%2520addresses%2520the%2520need%2520for%250Amore%2520robust%2520models.%2520In%2520addition%252C%2520this%2520confidence%2520is%2520used%2520to%2520cost-effectively%250Asample%2520the%2520parameter%2520space%252C%2520improving%2520model%2520performance%2520a%2520priori%2520across%2520the%250Aentire%2520parameter%2520space%2520without%2520requiring%2520evaluation%2520data%2520for%2520the%2520entire%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.23236v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UP-dROM%20%3A%20Uncertainty-Aware%20and%20Parametrised%20dynamic%20Reduced-Order%0A%20%20Model%2C%20application%20to%20unsteady%20flows&entry.906535625=Isma%C3%ABl%20Zighed%20and%20Nicolas%20Thome%20and%20Patrick%20Gallinari%20and%20Taraneh%20Sayadi&entry.1292438233=%20%20Reduced%20order%20models%20%28ROMs%29%20play%20a%20critical%20role%20in%20fluid%20mechanics%20by%0Aproviding%20low-cost%20predictions%2C%20making%20them%20an%20attractive%20tool%20for%20engineering%0Aapplications.%20However%2C%20for%20ROMs%20to%20be%20widely%20applicable%2C%20they%20must%20not%20only%0Ageneralise%20well%20across%20different%20regimes%2C%20but%20also%20provide%20a%20measure%20of%0Aconfidence%20in%20their%20predictions.%20While%20recent%20data-driven%20approaches%20have%20begun%0Ato%20address%20nonlinear%20reduction%20techniques%20to%20improve%20predictions%20in%20transient%0Aenvironments%2C%20challenges%20remain%20in%20terms%20of%20robustness%20and%20parametrisation.%20In%0Athis%20work%2C%20we%20present%20a%20nonlinear%20reduction%20strategy%20specifically%20designed%20for%0Atransient%20flows%20that%20incorporates%20parametrisation%20and%20uncertainty%0Aquantification.%20Our%20reduction%20strategy%20features%20a%20variational%20auto-encoder%0A%28VAE%29%20that%20uses%20variational%20inference%20for%20confidence%20measurement.%20We%20use%20a%0Alatent%20space%20transformer%20that%20incorporates%20recent%20advances%20in%20attention%0Amechanisms%20to%20predict%20dynamical%20systems.%20Attention%27s%20versatility%20in%20learning%0Asequences%20and%20capturing%20their%20dependence%20on%20external%20parameters%20enhances%0Ageneralisation%20across%20a%20wide%20range%20of%20dynamics.%20Prediction%2C%20coupled%20with%0Aconfidence%2C%20enables%20more%20informed%20decision%20making%20and%20addresses%20the%20need%20for%0Amore%20robust%20models.%20In%20addition%2C%20this%20confidence%20is%20used%20to%20cost-effectively%0Asample%20the%20parameter%20space%2C%20improving%20model%20performance%20a%20priori%20across%20the%0Aentire%20parameter%20space%20without%20requiring%20evaluation%20data%20for%20the%20entire%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.23236v2&entry.124074799=Read"},
{"title": "Modeling AI-Human Collaboration as a Multi-Agent Adaptation", "author": "Prothit Sen and Sai Mihir Jakkaraju", "abstract": "  We develop an agent-based simulation to formalize AI-human collaboration as a\nfunction of task structure, advancing a generalizable framework for strategic\ndecision-making in organizations. Distinguishing between heuristic-based human\nadaptation and rule-based AI search, we model interactions across modular\n(parallel) and sequenced (interdependent) tasks using an NK model. Our results\nreveal that in modular tasks, AI often substitutes for humans - delivering\nhigher payoffs unless human expertise is very high, and the AI search space is\neither narrowly focused or extremely broad. In sequenced tasks, interesting\ncomplementarities emerge. When an expert human initiates the search and AI\nsubsequently refines it, aggregate performance is maximized. Conversely, when\nAI leads, excessive heuristic refinement by the human can reduce payoffs. We\nalso show that even \"hallucinatory\" AI - lacking memory or structure - can\nimprove outcomes when augmenting low-capability humans by helping escape local\noptima. These results yield a robust implication: the effectiveness of AI-human\ncollaboration depends less on context or industry, and more on the underlying\ntask structure. By elevating task decomposition as the central unit of\nanalysis, our model provides a transferable lens for strategic decision-making\ninvolving humans and an agentic AI across diverse organizational settings.\n", "link": "http://arxiv.org/abs/2504.20903v1", "date": "2025-04-29", "relevancy": 1.5861, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5447}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5276}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20AI-Human%20Collaboration%20as%20a%20Multi-Agent%20Adaptation&body=Title%3A%20Modeling%20AI-Human%20Collaboration%20as%20a%20Multi-Agent%20Adaptation%0AAuthor%3A%20Prothit%20Sen%20and%20Sai%20Mihir%20Jakkaraju%0AAbstract%3A%20%20%20We%20develop%20an%20agent-based%20simulation%20to%20formalize%20AI-human%20collaboration%20as%20a%0Afunction%20of%20task%20structure%2C%20advancing%20a%20generalizable%20framework%20for%20strategic%0Adecision-making%20in%20organizations.%20Distinguishing%20between%20heuristic-based%20human%0Aadaptation%20and%20rule-based%20AI%20search%2C%20we%20model%20interactions%20across%20modular%0A%28parallel%29%20and%20sequenced%20%28interdependent%29%20tasks%20using%20an%20NK%20model.%20Our%20results%0Areveal%20that%20in%20modular%20tasks%2C%20AI%20often%20substitutes%20for%20humans%20-%20delivering%0Ahigher%20payoffs%20unless%20human%20expertise%20is%20very%20high%2C%20and%20the%20AI%20search%20space%20is%0Aeither%20narrowly%20focused%20or%20extremely%20broad.%20In%20sequenced%20tasks%2C%20interesting%0Acomplementarities%20emerge.%20When%20an%20expert%20human%20initiates%20the%20search%20and%20AI%0Asubsequently%20refines%20it%2C%20aggregate%20performance%20is%20maximized.%20Conversely%2C%20when%0AAI%20leads%2C%20excessive%20heuristic%20refinement%20by%20the%20human%20can%20reduce%20payoffs.%20We%0Aalso%20show%20that%20even%20%22hallucinatory%22%20AI%20-%20lacking%20memory%20or%20structure%20-%20can%0Aimprove%20outcomes%20when%20augmenting%20low-capability%20humans%20by%20helping%20escape%20local%0Aoptima.%20These%20results%20yield%20a%20robust%20implication%3A%20the%20effectiveness%20of%20AI-human%0Acollaboration%20depends%20less%20on%20context%20or%20industry%2C%20and%20more%20on%20the%20underlying%0Atask%20structure.%20By%20elevating%20task%20decomposition%20as%20the%20central%20unit%20of%0Aanalysis%2C%20our%20model%20provides%20a%20transferable%20lens%20for%20strategic%20decision-making%0Ainvolving%20humans%20and%20an%20agentic%20AI%20across%20diverse%20organizational%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520AI-Human%2520Collaboration%2520as%2520a%2520Multi-Agent%2520Adaptation%26entry.906535625%3DProthit%2520Sen%2520and%2520Sai%2520Mihir%2520Jakkaraju%26entry.1292438233%3D%2520%2520We%2520develop%2520an%2520agent-based%2520simulation%2520to%2520formalize%2520AI-human%2520collaboration%2520as%2520a%250Afunction%2520of%2520task%2520structure%252C%2520advancing%2520a%2520generalizable%2520framework%2520for%2520strategic%250Adecision-making%2520in%2520organizations.%2520Distinguishing%2520between%2520heuristic-based%2520human%250Aadaptation%2520and%2520rule-based%2520AI%2520search%252C%2520we%2520model%2520interactions%2520across%2520modular%250A%2528parallel%2529%2520and%2520sequenced%2520%2528interdependent%2529%2520tasks%2520using%2520an%2520NK%2520model.%2520Our%2520results%250Areveal%2520that%2520in%2520modular%2520tasks%252C%2520AI%2520often%2520substitutes%2520for%2520humans%2520-%2520delivering%250Ahigher%2520payoffs%2520unless%2520human%2520expertise%2520is%2520very%2520high%252C%2520and%2520the%2520AI%2520search%2520space%2520is%250Aeither%2520narrowly%2520focused%2520or%2520extremely%2520broad.%2520In%2520sequenced%2520tasks%252C%2520interesting%250Acomplementarities%2520emerge.%2520When%2520an%2520expert%2520human%2520initiates%2520the%2520search%2520and%2520AI%250Asubsequently%2520refines%2520it%252C%2520aggregate%2520performance%2520is%2520maximized.%2520Conversely%252C%2520when%250AAI%2520leads%252C%2520excessive%2520heuristic%2520refinement%2520by%2520the%2520human%2520can%2520reduce%2520payoffs.%2520We%250Aalso%2520show%2520that%2520even%2520%2522hallucinatory%2522%2520AI%2520-%2520lacking%2520memory%2520or%2520structure%2520-%2520can%250Aimprove%2520outcomes%2520when%2520augmenting%2520low-capability%2520humans%2520by%2520helping%2520escape%2520local%250Aoptima.%2520These%2520results%2520yield%2520a%2520robust%2520implication%253A%2520the%2520effectiveness%2520of%2520AI-human%250Acollaboration%2520depends%2520less%2520on%2520context%2520or%2520industry%252C%2520and%2520more%2520on%2520the%2520underlying%250Atask%2520structure.%2520By%2520elevating%2520task%2520decomposition%2520as%2520the%2520central%2520unit%2520of%250Aanalysis%252C%2520our%2520model%2520provides%2520a%2520transferable%2520lens%2520for%2520strategic%2520decision-making%250Ainvolving%2520humans%2520and%2520an%2520agentic%2520AI%2520across%2520diverse%2520organizational%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20AI-Human%20Collaboration%20as%20a%20Multi-Agent%20Adaptation&entry.906535625=Prothit%20Sen%20and%20Sai%20Mihir%20Jakkaraju&entry.1292438233=%20%20We%20develop%20an%20agent-based%20simulation%20to%20formalize%20AI-human%20collaboration%20as%20a%0Afunction%20of%20task%20structure%2C%20advancing%20a%20generalizable%20framework%20for%20strategic%0Adecision-making%20in%20organizations.%20Distinguishing%20between%20heuristic-based%20human%0Aadaptation%20and%20rule-based%20AI%20search%2C%20we%20model%20interactions%20across%20modular%0A%28parallel%29%20and%20sequenced%20%28interdependent%29%20tasks%20using%20an%20NK%20model.%20Our%20results%0Areveal%20that%20in%20modular%20tasks%2C%20AI%20often%20substitutes%20for%20humans%20-%20delivering%0Ahigher%20payoffs%20unless%20human%20expertise%20is%20very%20high%2C%20and%20the%20AI%20search%20space%20is%0Aeither%20narrowly%20focused%20or%20extremely%20broad.%20In%20sequenced%20tasks%2C%20interesting%0Acomplementarities%20emerge.%20When%20an%20expert%20human%20initiates%20the%20search%20and%20AI%0Asubsequently%20refines%20it%2C%20aggregate%20performance%20is%20maximized.%20Conversely%2C%20when%0AAI%20leads%2C%20excessive%20heuristic%20refinement%20by%20the%20human%20can%20reduce%20payoffs.%20We%0Aalso%20show%20that%20even%20%22hallucinatory%22%20AI%20-%20lacking%20memory%20or%20structure%20-%20can%0Aimprove%20outcomes%20when%20augmenting%20low-capability%20humans%20by%20helping%20escape%20local%0Aoptima.%20These%20results%20yield%20a%20robust%20implication%3A%20the%20effectiveness%20of%20AI-human%0Acollaboration%20depends%20less%20on%20context%20or%20industry%2C%20and%20more%20on%20the%20underlying%0Atask%20structure.%20By%20elevating%20task%20decomposition%20as%20the%20central%20unit%20of%0Aanalysis%2C%20our%20model%20provides%20a%20transferable%20lens%20for%20strategic%20decision-making%0Ainvolving%20humans%20and%20an%20agentic%20AI%20across%20diverse%20organizational%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20903v1&entry.124074799=Read"},
{"title": "DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge\n  Distillation for Plant Disease Recognition", "author": "Yanghui Song and Chengfu Yang", "abstract": "  Given the severe challenges confronting the global growth security of\neconomic crops, precise identification and prevention of plant diseases has\nemerged as a critical issue in artificial intelligence-enabled agricultural\ntechnology. To address the technical challenges in plant disease recognition,\nincluding small-sample learning, leaf occlusion, illumination variations, and\nhigh inter-class similarity, this study innovatively proposes a Dynamic\nDual-Stream Fusion Network (DS_FusionNet). The network integrates a\ndual-backbone architecture, deformable dynamic fusion modules, and\nbidirectional knowledge distillation strategy, significantly enhancing\nrecognition accuracy. Experimental results demonstrate that DS_FusionNet\nachieves classification accuracies exceeding 90% using only 10% of the\nPlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the\ncomplex PlantWild dataset, exhibiting exceptional generalization capabilities.\nThis research not only provides novel technical insights for fine-grained image\nclassification but also establishes a robust foundation for precise\nidentification and management of agricultural diseases.\n", "link": "http://arxiv.org/abs/2504.20948v1", "date": "2025-04-29", "relevancy": 1.5795, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5714}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5177}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DS_FusionNet%3A%20Dynamic%20Dual-Stream%20Fusion%20with%20Bidirectional%20Knowledge%0A%20%20Distillation%20for%20Plant%20Disease%20Recognition&body=Title%3A%20DS_FusionNet%3A%20Dynamic%20Dual-Stream%20Fusion%20with%20Bidirectional%20Knowledge%0A%20%20Distillation%20for%20Plant%20Disease%20Recognition%0AAuthor%3A%20Yanghui%20Song%20and%20Chengfu%20Yang%0AAbstract%3A%20%20%20Given%20the%20severe%20challenges%20confronting%20the%20global%20growth%20security%20of%0Aeconomic%20crops%2C%20precise%20identification%20and%20prevention%20of%20plant%20diseases%20has%0Aemerged%20as%20a%20critical%20issue%20in%20artificial%20intelligence-enabled%20agricultural%0Atechnology.%20To%20address%20the%20technical%20challenges%20in%20plant%20disease%20recognition%2C%0Aincluding%20small-sample%20learning%2C%20leaf%20occlusion%2C%20illumination%20variations%2C%20and%0Ahigh%20inter-class%20similarity%2C%20this%20study%20innovatively%20proposes%20a%20Dynamic%0ADual-Stream%20Fusion%20Network%20%28DS_FusionNet%29.%20The%20network%20integrates%20a%0Adual-backbone%20architecture%2C%20deformable%20dynamic%20fusion%20modules%2C%20and%0Abidirectional%20knowledge%20distillation%20strategy%2C%20significantly%20enhancing%0Arecognition%20accuracy.%20Experimental%20results%20demonstrate%20that%20DS_FusionNet%0Aachieves%20classification%20accuracies%20exceeding%2090%25%20using%20only%2010%25%20of%20the%0APlantDisease%20and%20CIFAR-10%20datasets%2C%20while%20maintaining%2085%25%20accuracy%20on%20the%0Acomplex%20PlantWild%20dataset%2C%20exhibiting%20exceptional%20generalization%20capabilities.%0AThis%20research%20not%20only%20provides%20novel%20technical%20insights%20for%20fine-grained%20image%0Aclassification%20but%20also%20establishes%20a%20robust%20foundation%20for%20precise%0Aidentification%20and%20management%20of%20agricultural%20diseases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDS_FusionNet%253A%2520Dynamic%2520Dual-Stream%2520Fusion%2520with%2520Bidirectional%2520Knowledge%250A%2520%2520Distillation%2520for%2520Plant%2520Disease%2520Recognition%26entry.906535625%3DYanghui%2520Song%2520and%2520Chengfu%2520Yang%26entry.1292438233%3D%2520%2520Given%2520the%2520severe%2520challenges%2520confronting%2520the%2520global%2520growth%2520security%2520of%250Aeconomic%2520crops%252C%2520precise%2520identification%2520and%2520prevention%2520of%2520plant%2520diseases%2520has%250Aemerged%2520as%2520a%2520critical%2520issue%2520in%2520artificial%2520intelligence-enabled%2520agricultural%250Atechnology.%2520To%2520address%2520the%2520technical%2520challenges%2520in%2520plant%2520disease%2520recognition%252C%250Aincluding%2520small-sample%2520learning%252C%2520leaf%2520occlusion%252C%2520illumination%2520variations%252C%2520and%250Ahigh%2520inter-class%2520similarity%252C%2520this%2520study%2520innovatively%2520proposes%2520a%2520Dynamic%250ADual-Stream%2520Fusion%2520Network%2520%2528DS_FusionNet%2529.%2520The%2520network%2520integrates%2520a%250Adual-backbone%2520architecture%252C%2520deformable%2520dynamic%2520fusion%2520modules%252C%2520and%250Abidirectional%2520knowledge%2520distillation%2520strategy%252C%2520significantly%2520enhancing%250Arecognition%2520accuracy.%2520Experimental%2520results%2520demonstrate%2520that%2520DS_FusionNet%250Aachieves%2520classification%2520accuracies%2520exceeding%252090%2525%2520using%2520only%252010%2525%2520of%2520the%250APlantDisease%2520and%2520CIFAR-10%2520datasets%252C%2520while%2520maintaining%252085%2525%2520accuracy%2520on%2520the%250Acomplex%2520PlantWild%2520dataset%252C%2520exhibiting%2520exceptional%2520generalization%2520capabilities.%250AThis%2520research%2520not%2520only%2520provides%2520novel%2520technical%2520insights%2520for%2520fine-grained%2520image%250Aclassification%2520but%2520also%2520establishes%2520a%2520robust%2520foundation%2520for%2520precise%250Aidentification%2520and%2520management%2520of%2520agricultural%2520diseases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DS_FusionNet%3A%20Dynamic%20Dual-Stream%20Fusion%20with%20Bidirectional%20Knowledge%0A%20%20Distillation%20for%20Plant%20Disease%20Recognition&entry.906535625=Yanghui%20Song%20and%20Chengfu%20Yang&entry.1292438233=%20%20Given%20the%20severe%20challenges%20confronting%20the%20global%20growth%20security%20of%0Aeconomic%20crops%2C%20precise%20identification%20and%20prevention%20of%20plant%20diseases%20has%0Aemerged%20as%20a%20critical%20issue%20in%20artificial%20intelligence-enabled%20agricultural%0Atechnology.%20To%20address%20the%20technical%20challenges%20in%20plant%20disease%20recognition%2C%0Aincluding%20small-sample%20learning%2C%20leaf%20occlusion%2C%20illumination%20variations%2C%20and%0Ahigh%20inter-class%20similarity%2C%20this%20study%20innovatively%20proposes%20a%20Dynamic%0ADual-Stream%20Fusion%20Network%20%28DS_FusionNet%29.%20The%20network%20integrates%20a%0Adual-backbone%20architecture%2C%20deformable%20dynamic%20fusion%20modules%2C%20and%0Abidirectional%20knowledge%20distillation%20strategy%2C%20significantly%20enhancing%0Arecognition%20accuracy.%20Experimental%20results%20demonstrate%20that%20DS_FusionNet%0Aachieves%20classification%20accuracies%20exceeding%2090%25%20using%20only%2010%25%20of%20the%0APlantDisease%20and%20CIFAR-10%20datasets%2C%20while%20maintaining%2085%25%20accuracy%20on%20the%0Acomplex%20PlantWild%20dataset%2C%20exhibiting%20exceptional%20generalization%20capabilities.%0AThis%20research%20not%20only%20provides%20novel%20technical%20insights%20for%20fine-grained%20image%0Aclassification%20but%20also%20establishes%20a%20robust%20foundation%20for%20precise%0Aidentification%20and%20management%20of%20agricultural%20diseases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20948v1&entry.124074799=Read"},
{"title": "Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax", "author": "Zayd M. K. Zuhri and Erland Hilman Fuadi and Alham Fikri Aji", "abstract": "  We introduce softpick, a rectified, not sum-to-one, drop-in replacement for\nsoftmax in transformer attention mechanisms that eliminates attention sink and\nmassive activations. Our experiments with 340M parameter models demonstrate\nthat softpick maintains performance parity with softmax on standard benchmarks\nwhile achieving 0% sink rate. The softpick transformer produces hidden states\nwith significantly lower kurtosis (340 vs 33,510) and creates sparse attention\nmaps (46.97% sparsity). Models using softpick consistently outperform softmax\nwhen quantized, with particularly pronounced advantages at lower bit\nprecisions. Our analysis and discussion shows how softpick has the potential to\nopen new possibilities for quantization, low-precision training, sparsity\noptimization, pruning, and interpretability. Our code is available at\nhttps://github.com/zaydzuhri/softpick-attention.\n", "link": "http://arxiv.org/abs/2504.20966v1", "date": "2025-04-29", "relevancy": 1.57, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5495}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5209}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Softpick%3A%20No%20Attention%20Sink%2C%20No%20Massive%20Activations%20with%20Rectified%0A%20%20Softmax&body=Title%3A%20Softpick%3A%20No%20Attention%20Sink%2C%20No%20Massive%20Activations%20with%20Rectified%0A%20%20Softmax%0AAuthor%3A%20Zayd%20M.%20K.%20Zuhri%20and%20Erland%20Hilman%20Fuadi%20and%20Alham%20Fikri%20Aji%0AAbstract%3A%20%20%20We%20introduce%20softpick%2C%20a%20rectified%2C%20not%20sum-to-one%2C%20drop-in%20replacement%20for%0Asoftmax%20in%20transformer%20attention%20mechanisms%20that%20eliminates%20attention%20sink%20and%0Amassive%20activations.%20Our%20experiments%20with%20340M%20parameter%20models%20demonstrate%0Athat%20softpick%20maintains%20performance%20parity%20with%20softmax%20on%20standard%20benchmarks%0Awhile%20achieving%200%25%20sink%20rate.%20The%20softpick%20transformer%20produces%20hidden%20states%0Awith%20significantly%20lower%20kurtosis%20%28340%20vs%2033%2C510%29%20and%20creates%20sparse%20attention%0Amaps%20%2846.97%25%20sparsity%29.%20Models%20using%20softpick%20consistently%20outperform%20softmax%0Awhen%20quantized%2C%20with%20particularly%20pronounced%20advantages%20at%20lower%20bit%0Aprecisions.%20Our%20analysis%20and%20discussion%20shows%20how%20softpick%20has%20the%20potential%20to%0Aopen%20new%20possibilities%20for%20quantization%2C%20low-precision%20training%2C%20sparsity%0Aoptimization%2C%20pruning%2C%20and%20interpretability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zaydzuhri/softpick-attention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoftpick%253A%2520No%2520Attention%2520Sink%252C%2520No%2520Massive%2520Activations%2520with%2520Rectified%250A%2520%2520Softmax%26entry.906535625%3DZayd%2520M.%2520K.%2520Zuhri%2520and%2520Erland%2520Hilman%2520Fuadi%2520and%2520Alham%2520Fikri%2520Aji%26entry.1292438233%3D%2520%2520We%2520introduce%2520softpick%252C%2520a%2520rectified%252C%2520not%2520sum-to-one%252C%2520drop-in%2520replacement%2520for%250Asoftmax%2520in%2520transformer%2520attention%2520mechanisms%2520that%2520eliminates%2520attention%2520sink%2520and%250Amassive%2520activations.%2520Our%2520experiments%2520with%2520340M%2520parameter%2520models%2520demonstrate%250Athat%2520softpick%2520maintains%2520performance%2520parity%2520with%2520softmax%2520on%2520standard%2520benchmarks%250Awhile%2520achieving%25200%2525%2520sink%2520rate.%2520The%2520softpick%2520transformer%2520produces%2520hidden%2520states%250Awith%2520significantly%2520lower%2520kurtosis%2520%2528340%2520vs%252033%252C510%2529%2520and%2520creates%2520sparse%2520attention%250Amaps%2520%252846.97%2525%2520sparsity%2529.%2520Models%2520using%2520softpick%2520consistently%2520outperform%2520softmax%250Awhen%2520quantized%252C%2520with%2520particularly%2520pronounced%2520advantages%2520at%2520lower%2520bit%250Aprecisions.%2520Our%2520analysis%2520and%2520discussion%2520shows%2520how%2520softpick%2520has%2520the%2520potential%2520to%250Aopen%2520new%2520possibilities%2520for%2520quantization%252C%2520low-precision%2520training%252C%2520sparsity%250Aoptimization%252C%2520pruning%252C%2520and%2520interpretability.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zaydzuhri/softpick-attention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Softpick%3A%20No%20Attention%20Sink%2C%20No%20Massive%20Activations%20with%20Rectified%0A%20%20Softmax&entry.906535625=Zayd%20M.%20K.%20Zuhri%20and%20Erland%20Hilman%20Fuadi%20and%20Alham%20Fikri%20Aji&entry.1292438233=%20%20We%20introduce%20softpick%2C%20a%20rectified%2C%20not%20sum-to-one%2C%20drop-in%20replacement%20for%0Asoftmax%20in%20transformer%20attention%20mechanisms%20that%20eliminates%20attention%20sink%20and%0Amassive%20activations.%20Our%20experiments%20with%20340M%20parameter%20models%20demonstrate%0Athat%20softpick%20maintains%20performance%20parity%20with%20softmax%20on%20standard%20benchmarks%0Awhile%20achieving%200%25%20sink%20rate.%20The%20softpick%20transformer%20produces%20hidden%20states%0Awith%20significantly%20lower%20kurtosis%20%28340%20vs%2033%2C510%29%20and%20creates%20sparse%20attention%0Amaps%20%2846.97%25%20sparsity%29.%20Models%20using%20softpick%20consistently%20outperform%20softmax%0Awhen%20quantized%2C%20with%20particularly%20pronounced%20advantages%20at%20lower%20bit%0Aprecisions.%20Our%20analysis%20and%20discussion%20shows%20how%20softpick%20has%20the%20potential%20to%0Aopen%20new%20possibilities%20for%20quantization%2C%20low-precision%20training%2C%20sparsity%0Aoptimization%2C%20pruning%2C%20and%20interpretability.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zaydzuhri/softpick-attention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20966v1&entry.124074799=Read"},
{"title": "A Domain-Agnostic Scalable AI Safety Ensuring Framework", "author": "Beomjun Kim and Kangyeon Kim and Sunwoo Kim and Heejin Ahn", "abstract": "  Ensuring the safety of AI systems has recently emerged as a critical priority\nfor real-world deployment, particularly in physical AI applications. Current\napproaches to AI safety typically address predefined domain-specific safety\nconditions, limiting their ability to generalize across contexts.\n  We propose a novel AI safety framework that ensures AI systems comply with\n\\textbf{any user-defined constraint}, with \\textbf{any desired probability},\nand across \\textbf{various domains}.\n  In this framework, we combine an AI component (e.g., neural network) with an\noptimization problem to produce responses that minimize objectives while\nsatisfying user-defined constraints with probabilities exceeding user-defined\nthresholds. For credibility assessment of the AI component, we propose\n\\textit{internal test data}, a supplementary set of safety-labeled data, and a\n\\textit{conservative testing} methodology that provides statistical validity of\nusing internal test data. We also present an approximation method of a loss\nfunction and how to compute its gradient for training.\n  We mathematically prove that probabilistic constraint satisfaction is\nguaranteed under specific, mild conditions and prove a scaling law between\nsafety and the number of internal test data. We demonstrate our framework's\neffectiveness through experiments in diverse domains: demand prediction for\nproduction decision, safe reinforcement learning within the SafetyGym\nsimulator, and guarding AI chatbot outputs. Through these experiments, we\ndemonstrate that our method guarantees safety for user-specified constraints,\noutperforms {for \\textbf{up to several order of magnitudes}} existing methods\nin low safety threshold regions, and scales effectively with respect to the\nsize of internal test data.\n", "link": "http://arxiv.org/abs/2504.20924v1", "date": "2025-04-29", "relevancy": 1.5384, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5374}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5189}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Domain-Agnostic%20Scalable%20AI%20Safety%20Ensuring%20Framework&body=Title%3A%20A%20Domain-Agnostic%20Scalable%20AI%20Safety%20Ensuring%20Framework%0AAuthor%3A%20Beomjun%20Kim%20and%20Kangyeon%20Kim%20and%20Sunwoo%20Kim%20and%20Heejin%20Ahn%0AAbstract%3A%20%20%20Ensuring%20the%20safety%20of%20AI%20systems%20has%20recently%20emerged%20as%20a%20critical%20priority%0Afor%20real-world%20deployment%2C%20particularly%20in%20physical%20AI%20applications.%20Current%0Aapproaches%20to%20AI%20safety%20typically%20address%20predefined%20domain-specific%20safety%0Aconditions%2C%20limiting%20their%20ability%20to%20generalize%20across%20contexts.%0A%20%20We%20propose%20a%20novel%20AI%20safety%20framework%20that%20ensures%20AI%20systems%20comply%20with%0A%5Ctextbf%7Bany%20user-defined%20constraint%7D%2C%20with%20%5Ctextbf%7Bany%20desired%20probability%7D%2C%0Aand%20across%20%5Ctextbf%7Bvarious%20domains%7D.%0A%20%20In%20this%20framework%2C%20we%20combine%20an%20AI%20component%20%28e.g.%2C%20neural%20network%29%20with%20an%0Aoptimization%20problem%20to%20produce%20responses%20that%20minimize%20objectives%20while%0Asatisfying%20user-defined%20constraints%20with%20probabilities%20exceeding%20user-defined%0Athresholds.%20For%20credibility%20assessment%20of%20the%20AI%20component%2C%20we%20propose%0A%5Ctextit%7Binternal%20test%20data%7D%2C%20a%20supplementary%20set%20of%20safety-labeled%20data%2C%20and%20a%0A%5Ctextit%7Bconservative%20testing%7D%20methodology%20that%20provides%20statistical%20validity%20of%0Ausing%20internal%20test%20data.%20We%20also%20present%20an%20approximation%20method%20of%20a%20loss%0Afunction%20and%20how%20to%20compute%20its%20gradient%20for%20training.%0A%20%20We%20mathematically%20prove%20that%20probabilistic%20constraint%20satisfaction%20is%0Aguaranteed%20under%20specific%2C%20mild%20conditions%20and%20prove%20a%20scaling%20law%20between%0Asafety%20and%20the%20number%20of%20internal%20test%20data.%20We%20demonstrate%20our%20framework%27s%0Aeffectiveness%20through%20experiments%20in%20diverse%20domains%3A%20demand%20prediction%20for%0Aproduction%20decision%2C%20safe%20reinforcement%20learning%20within%20the%20SafetyGym%0Asimulator%2C%20and%20guarding%20AI%20chatbot%20outputs.%20Through%20these%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20guarantees%20safety%20for%20user-specified%20constraints%2C%0Aoutperforms%20%7Bfor%20%5Ctextbf%7Bup%20to%20several%20order%20of%20magnitudes%7D%7D%20existing%20methods%0Ain%20low%20safety%20threshold%20regions%2C%20and%20scales%20effectively%20with%20respect%20to%20the%0Asize%20of%20internal%20test%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Domain-Agnostic%2520Scalable%2520AI%2520Safety%2520Ensuring%2520Framework%26entry.906535625%3DBeomjun%2520Kim%2520and%2520Kangyeon%2520Kim%2520and%2520Sunwoo%2520Kim%2520and%2520Heejin%2520Ahn%26entry.1292438233%3D%2520%2520Ensuring%2520the%2520safety%2520of%2520AI%2520systems%2520has%2520recently%2520emerged%2520as%2520a%2520critical%2520priority%250Afor%2520real-world%2520deployment%252C%2520particularly%2520in%2520physical%2520AI%2520applications.%2520Current%250Aapproaches%2520to%2520AI%2520safety%2520typically%2520address%2520predefined%2520domain-specific%2520safety%250Aconditions%252C%2520limiting%2520their%2520ability%2520to%2520generalize%2520across%2520contexts.%250A%2520%2520We%2520propose%2520a%2520novel%2520AI%2520safety%2520framework%2520that%2520ensures%2520AI%2520systems%2520comply%2520with%250A%255Ctextbf%257Bany%2520user-defined%2520constraint%257D%252C%2520with%2520%255Ctextbf%257Bany%2520desired%2520probability%257D%252C%250Aand%2520across%2520%255Ctextbf%257Bvarious%2520domains%257D.%250A%2520%2520In%2520this%2520framework%252C%2520we%2520combine%2520an%2520AI%2520component%2520%2528e.g.%252C%2520neural%2520network%2529%2520with%2520an%250Aoptimization%2520problem%2520to%2520produce%2520responses%2520that%2520minimize%2520objectives%2520while%250Asatisfying%2520user-defined%2520constraints%2520with%2520probabilities%2520exceeding%2520user-defined%250Athresholds.%2520For%2520credibility%2520assessment%2520of%2520the%2520AI%2520component%252C%2520we%2520propose%250A%255Ctextit%257Binternal%2520test%2520data%257D%252C%2520a%2520supplementary%2520set%2520of%2520safety-labeled%2520data%252C%2520and%2520a%250A%255Ctextit%257Bconservative%2520testing%257D%2520methodology%2520that%2520provides%2520statistical%2520validity%2520of%250Ausing%2520internal%2520test%2520data.%2520We%2520also%2520present%2520an%2520approximation%2520method%2520of%2520a%2520loss%250Afunction%2520and%2520how%2520to%2520compute%2520its%2520gradient%2520for%2520training.%250A%2520%2520We%2520mathematically%2520prove%2520that%2520probabilistic%2520constraint%2520satisfaction%2520is%250Aguaranteed%2520under%2520specific%252C%2520mild%2520conditions%2520and%2520prove%2520a%2520scaling%2520law%2520between%250Asafety%2520and%2520the%2520number%2520of%2520internal%2520test%2520data.%2520We%2520demonstrate%2520our%2520framework%2527s%250Aeffectiveness%2520through%2520experiments%2520in%2520diverse%2520domains%253A%2520demand%2520prediction%2520for%250Aproduction%2520decision%252C%2520safe%2520reinforcement%2520learning%2520within%2520the%2520SafetyGym%250Asimulator%252C%2520and%2520guarding%2520AI%2520chatbot%2520outputs.%2520Through%2520these%2520experiments%252C%2520we%250Ademonstrate%2520that%2520our%2520method%2520guarantees%2520safety%2520for%2520user-specified%2520constraints%252C%250Aoutperforms%2520%257Bfor%2520%255Ctextbf%257Bup%2520to%2520several%2520order%2520of%2520magnitudes%257D%257D%2520existing%2520methods%250Ain%2520low%2520safety%2520threshold%2520regions%252C%2520and%2520scales%2520effectively%2520with%2520respect%2520to%2520the%250Asize%2520of%2520internal%2520test%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Domain-Agnostic%20Scalable%20AI%20Safety%20Ensuring%20Framework&entry.906535625=Beomjun%20Kim%20and%20Kangyeon%20Kim%20and%20Sunwoo%20Kim%20and%20Heejin%20Ahn&entry.1292438233=%20%20Ensuring%20the%20safety%20of%20AI%20systems%20has%20recently%20emerged%20as%20a%20critical%20priority%0Afor%20real-world%20deployment%2C%20particularly%20in%20physical%20AI%20applications.%20Current%0Aapproaches%20to%20AI%20safety%20typically%20address%20predefined%20domain-specific%20safety%0Aconditions%2C%20limiting%20their%20ability%20to%20generalize%20across%20contexts.%0A%20%20We%20propose%20a%20novel%20AI%20safety%20framework%20that%20ensures%20AI%20systems%20comply%20with%0A%5Ctextbf%7Bany%20user-defined%20constraint%7D%2C%20with%20%5Ctextbf%7Bany%20desired%20probability%7D%2C%0Aand%20across%20%5Ctextbf%7Bvarious%20domains%7D.%0A%20%20In%20this%20framework%2C%20we%20combine%20an%20AI%20component%20%28e.g.%2C%20neural%20network%29%20with%20an%0Aoptimization%20problem%20to%20produce%20responses%20that%20minimize%20objectives%20while%0Asatisfying%20user-defined%20constraints%20with%20probabilities%20exceeding%20user-defined%0Athresholds.%20For%20credibility%20assessment%20of%20the%20AI%20component%2C%20we%20propose%0A%5Ctextit%7Binternal%20test%20data%7D%2C%20a%20supplementary%20set%20of%20safety-labeled%20data%2C%20and%20a%0A%5Ctextit%7Bconservative%20testing%7D%20methodology%20that%20provides%20statistical%20validity%20of%0Ausing%20internal%20test%20data.%20We%20also%20present%20an%20approximation%20method%20of%20a%20loss%0Afunction%20and%20how%20to%20compute%20its%20gradient%20for%20training.%0A%20%20We%20mathematically%20prove%20that%20probabilistic%20constraint%20satisfaction%20is%0Aguaranteed%20under%20specific%2C%20mild%20conditions%20and%20prove%20a%20scaling%20law%20between%0Asafety%20and%20the%20number%20of%20internal%20test%20data.%20We%20demonstrate%20our%20framework%27s%0Aeffectiveness%20through%20experiments%20in%20diverse%20domains%3A%20demand%20prediction%20for%0Aproduction%20decision%2C%20safe%20reinforcement%20learning%20within%20the%20SafetyGym%0Asimulator%2C%20and%20guarding%20AI%20chatbot%20outputs.%20Through%20these%20experiments%2C%20we%0Ademonstrate%20that%20our%20method%20guarantees%20safety%20for%20user-specified%20constraints%2C%0Aoutperforms%20%7Bfor%20%5Ctextbf%7Bup%20to%20several%20order%20of%20magnitudes%7D%7D%20existing%20methods%0Ain%20low%20safety%20threshold%20regions%2C%20and%20scales%20effectively%20with%20respect%20to%20the%0Asize%20of%20internal%20test%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20924v1&entry.124074799=Read"},
{"title": "Bayesian Optimization-based Tire Parameter and Uncertainty Estimation\n  for Real-World Data", "author": "Sven Goblirsch and Benedikt Ruhland and Johannes Betz and Markus Lienkamp", "abstract": "  This work presents a methodology to estimate tire parameters and their\nuncertainty using a Bayesian optimization approach. The literature mainly\nconsiders the estimation of tire parameters but lacks an evaluation of the\nparameter identification quality and the required slip ratios for an adequate\nmodel fit. Therefore, we examine the use of Stochastical Variational Inference\nas a methodology to estimate both - the parameters and their uncertainties. We\nevaluate the method compared to a state-of-the-art Nelder-Mead algorithm for\ntheoretical and real-world application. The theoretical study considers\nparameter fitting at different slip ratios to evaluate the required excitation\nfor an adequate fitting of each parameter. The results are compared to a\nsensitivity analysis for a Pacejka Magic Formula tire model. We show the\napplication of the algorithm on real-world data acquired during the Abu Dhabi\nAutonomous Racing League and highlight the uncertainties in identifying the\ncurvature and shape parameters due to insufficient excitation. The gathered\ninsights can help assess the acquired data's limitations and instead utilize\nstandardized parameters until higher slip ratios are captured. We show that our\nproposed method can be used to assess the mean values and the uncertainties of\ntire model parameters in real-world conditions and derive actions for the tire\nmodeling based on our simulative study.\n", "link": "http://arxiv.org/abs/2504.20863v1", "date": "2025-04-29", "relevancy": 1.5365, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5996}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4927}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Optimization-based%20Tire%20Parameter%20and%20Uncertainty%20Estimation%0A%20%20for%20Real-World%20Data&body=Title%3A%20Bayesian%20Optimization-based%20Tire%20Parameter%20and%20Uncertainty%20Estimation%0A%20%20for%20Real-World%20Data%0AAuthor%3A%20Sven%20Goblirsch%20and%20Benedikt%20Ruhland%20and%20Johannes%20Betz%20and%20Markus%20Lienkamp%0AAbstract%3A%20%20%20This%20work%20presents%20a%20methodology%20to%20estimate%20tire%20parameters%20and%20their%0Auncertainty%20using%20a%20Bayesian%20optimization%20approach.%20The%20literature%20mainly%0Aconsiders%20the%20estimation%20of%20tire%20parameters%20but%20lacks%20an%20evaluation%20of%20the%0Aparameter%20identification%20quality%20and%20the%20required%20slip%20ratios%20for%20an%20adequate%0Amodel%20fit.%20Therefore%2C%20we%20examine%20the%20use%20of%20Stochastical%20Variational%20Inference%0Aas%20a%20methodology%20to%20estimate%20both%20-%20the%20parameters%20and%20their%20uncertainties.%20We%0Aevaluate%20the%20method%20compared%20to%20a%20state-of-the-art%20Nelder-Mead%20algorithm%20for%0Atheoretical%20and%20real-world%20application.%20The%20theoretical%20study%20considers%0Aparameter%20fitting%20at%20different%20slip%20ratios%20to%20evaluate%20the%20required%20excitation%0Afor%20an%20adequate%20fitting%20of%20each%20parameter.%20The%20results%20are%20compared%20to%20a%0Asensitivity%20analysis%20for%20a%20Pacejka%20Magic%20Formula%20tire%20model.%20We%20show%20the%0Aapplication%20of%20the%20algorithm%20on%20real-world%20data%20acquired%20during%20the%20Abu%20Dhabi%0AAutonomous%20Racing%20League%20and%20highlight%20the%20uncertainties%20in%20identifying%20the%0Acurvature%20and%20shape%20parameters%20due%20to%20insufficient%20excitation.%20The%20gathered%0Ainsights%20can%20help%20assess%20the%20acquired%20data%27s%20limitations%20and%20instead%20utilize%0Astandardized%20parameters%20until%20higher%20slip%20ratios%20are%20captured.%20We%20show%20that%20our%0Aproposed%20method%20can%20be%20used%20to%20assess%20the%20mean%20values%20and%20the%20uncertainties%20of%0Atire%20model%20parameters%20in%20real-world%20conditions%20and%20derive%20actions%20for%20the%20tire%0Amodeling%20based%20on%20our%20simulative%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Optimization-based%2520Tire%2520Parameter%2520and%2520Uncertainty%2520Estimation%250A%2520%2520for%2520Real-World%2520Data%26entry.906535625%3DSven%2520Goblirsch%2520and%2520Benedikt%2520Ruhland%2520and%2520Johannes%2520Betz%2520and%2520Markus%2520Lienkamp%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520a%2520methodology%2520to%2520estimate%2520tire%2520parameters%2520and%2520their%250Auncertainty%2520using%2520a%2520Bayesian%2520optimization%2520approach.%2520The%2520literature%2520mainly%250Aconsiders%2520the%2520estimation%2520of%2520tire%2520parameters%2520but%2520lacks%2520an%2520evaluation%2520of%2520the%250Aparameter%2520identification%2520quality%2520and%2520the%2520required%2520slip%2520ratios%2520for%2520an%2520adequate%250Amodel%2520fit.%2520Therefore%252C%2520we%2520examine%2520the%2520use%2520of%2520Stochastical%2520Variational%2520Inference%250Aas%2520a%2520methodology%2520to%2520estimate%2520both%2520-%2520the%2520parameters%2520and%2520their%2520uncertainties.%2520We%250Aevaluate%2520the%2520method%2520compared%2520to%2520a%2520state-of-the-art%2520Nelder-Mead%2520algorithm%2520for%250Atheoretical%2520and%2520real-world%2520application.%2520The%2520theoretical%2520study%2520considers%250Aparameter%2520fitting%2520at%2520different%2520slip%2520ratios%2520to%2520evaluate%2520the%2520required%2520excitation%250Afor%2520an%2520adequate%2520fitting%2520of%2520each%2520parameter.%2520The%2520results%2520are%2520compared%2520to%2520a%250Asensitivity%2520analysis%2520for%2520a%2520Pacejka%2520Magic%2520Formula%2520tire%2520model.%2520We%2520show%2520the%250Aapplication%2520of%2520the%2520algorithm%2520on%2520real-world%2520data%2520acquired%2520during%2520the%2520Abu%2520Dhabi%250AAutonomous%2520Racing%2520League%2520and%2520highlight%2520the%2520uncertainties%2520in%2520identifying%2520the%250Acurvature%2520and%2520shape%2520parameters%2520due%2520to%2520insufficient%2520excitation.%2520The%2520gathered%250Ainsights%2520can%2520help%2520assess%2520the%2520acquired%2520data%2527s%2520limitations%2520and%2520instead%2520utilize%250Astandardized%2520parameters%2520until%2520higher%2520slip%2520ratios%2520are%2520captured.%2520We%2520show%2520that%2520our%250Aproposed%2520method%2520can%2520be%2520used%2520to%2520assess%2520the%2520mean%2520values%2520and%2520the%2520uncertainties%2520of%250Atire%2520model%2520parameters%2520in%2520real-world%2520conditions%2520and%2520derive%2520actions%2520for%2520the%2520tire%250Amodeling%2520based%2520on%2520our%2520simulative%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Optimization-based%20Tire%20Parameter%20and%20Uncertainty%20Estimation%0A%20%20for%20Real-World%20Data&entry.906535625=Sven%20Goblirsch%20and%20Benedikt%20Ruhland%20and%20Johannes%20Betz%20and%20Markus%20Lienkamp&entry.1292438233=%20%20This%20work%20presents%20a%20methodology%20to%20estimate%20tire%20parameters%20and%20their%0Auncertainty%20using%20a%20Bayesian%20optimization%20approach.%20The%20literature%20mainly%0Aconsiders%20the%20estimation%20of%20tire%20parameters%20but%20lacks%20an%20evaluation%20of%20the%0Aparameter%20identification%20quality%20and%20the%20required%20slip%20ratios%20for%20an%20adequate%0Amodel%20fit.%20Therefore%2C%20we%20examine%20the%20use%20of%20Stochastical%20Variational%20Inference%0Aas%20a%20methodology%20to%20estimate%20both%20-%20the%20parameters%20and%20their%20uncertainties.%20We%0Aevaluate%20the%20method%20compared%20to%20a%20state-of-the-art%20Nelder-Mead%20algorithm%20for%0Atheoretical%20and%20real-world%20application.%20The%20theoretical%20study%20considers%0Aparameter%20fitting%20at%20different%20slip%20ratios%20to%20evaluate%20the%20required%20excitation%0Afor%20an%20adequate%20fitting%20of%20each%20parameter.%20The%20results%20are%20compared%20to%20a%0Asensitivity%20analysis%20for%20a%20Pacejka%20Magic%20Formula%20tire%20model.%20We%20show%20the%0Aapplication%20of%20the%20algorithm%20on%20real-world%20data%20acquired%20during%20the%20Abu%20Dhabi%0AAutonomous%20Racing%20League%20and%20highlight%20the%20uncertainties%20in%20identifying%20the%0Acurvature%20and%20shape%20parameters%20due%20to%20insufficient%20excitation.%20The%20gathered%0Ainsights%20can%20help%20assess%20the%20acquired%20data%27s%20limitations%20and%20instead%20utilize%0Astandardized%20parameters%20until%20higher%20slip%20ratios%20are%20captured.%20We%20show%20that%20our%0Aproposed%20method%20can%20be%20used%20to%20assess%20the%20mean%20values%20and%20the%20uncertainties%20of%0Atire%20model%20parameters%20in%20real-world%20conditions%20and%20derive%20actions%20for%20the%20tire%0Amodeling%20based%20on%20our%20simulative%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20863v1&entry.124074799=Read"},
{"title": "End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based\n  Approach with Cross-Dataset Evaluation", "author": "Andrea Di Pierno and Luca Guarnera and Dario Allegra and Sebastiano Battiato", "abstract": "  Audio deepfakes represent a growing threat to digital security and trust,\nleveraging advanced generative models to produce synthetic speech that closely\nmimics real human voices. Detecting such manipulations is especially\nchallenging under open-world conditions, where spoofing methods encountered\nduring testing may differ from those seen during training. In this work, we\npropose an end-to-end deep learning framework for audio deepfake detection that\noperates directly on raw waveforms. Our model, RawNetLite, is a lightweight\nconvolutional-recurrent architecture designed to capture both spectral and\ntemporal features without handcrafted preprocessing. To enhance robustness, we\nintroduce a training strategy that combines data from multiple domains and\nadopts Focal Loss to emphasize difficult or ambiguous samples. We further\ndemonstrate that incorporating codec-based manipulations and applying\nwaveform-level audio augmentations (e.g., pitch shifting, noise, and time\nstretching) leads to significant generalization improvements under realistic\nacoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on\nin-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging\nout-of-distribution test set (AVSpoof2021 + CodecFake). These findings\nhighlight the importance of diverse training data, tailored objective functions\nand audio augmentations in building resilient and generalizable audio forgery\ndetectors. Code and pretrained models are available at\nhttps://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.\n", "link": "http://arxiv.org/abs/2504.20923v1", "date": "2025-04-29", "relevancy": 1.5232, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.521}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4989}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-end%20Audio%20Deepfake%20Detection%20from%20RAW%20Waveforms%3A%20a%20RawNet-Based%0A%20%20Approach%20with%20Cross-Dataset%20Evaluation&body=Title%3A%20End-to-end%20Audio%20Deepfake%20Detection%20from%20RAW%20Waveforms%3A%20a%20RawNet-Based%0A%20%20Approach%20with%20Cross-Dataset%20Evaluation%0AAuthor%3A%20Andrea%20Di%20Pierno%20and%20Luca%20Guarnera%20and%20Dario%20Allegra%20and%20Sebastiano%20Battiato%0AAbstract%3A%20%20%20Audio%20deepfakes%20represent%20a%20growing%20threat%20to%20digital%20security%20and%20trust%2C%0Aleveraging%20advanced%20generative%20models%20to%20produce%20synthetic%20speech%20that%20closely%0Amimics%20real%20human%20voices.%20Detecting%20such%20manipulations%20is%20especially%0Achallenging%20under%20open-world%20conditions%2C%20where%20spoofing%20methods%20encountered%0Aduring%20testing%20may%20differ%20from%20those%20seen%20during%20training.%20In%20this%20work%2C%20we%0Apropose%20an%20end-to-end%20deep%20learning%20framework%20for%20audio%20deepfake%20detection%20that%0Aoperates%20directly%20on%20raw%20waveforms.%20Our%20model%2C%20RawNetLite%2C%20is%20a%20lightweight%0Aconvolutional-recurrent%20architecture%20designed%20to%20capture%20both%20spectral%20and%0Atemporal%20features%20without%20handcrafted%20preprocessing.%20To%20enhance%20robustness%2C%20we%0Aintroduce%20a%20training%20strategy%20that%20combines%20data%20from%20multiple%20domains%20and%0Aadopts%20Focal%20Loss%20to%20emphasize%20difficult%20or%20ambiguous%20samples.%20We%20further%0Ademonstrate%20that%20incorporating%20codec-based%20manipulations%20and%20applying%0Awaveform-level%20audio%20augmentations%20%28e.g.%2C%20pitch%20shifting%2C%20noise%2C%20and%20time%0Astretching%29%20leads%20to%20significant%20generalization%20improvements%20under%20realistic%0Aacoustic%20conditions.%20The%20proposed%20model%20achieves%20over%2099.7%25%20F1%20and%200.25%25%20EER%20on%0Ain-domain%20data%20%28FakeOrReal%29%2C%20and%20up%20to%2083.4%25%20F1%20with%2016.4%25%20EER%20on%20a%20challenging%0Aout-of-distribution%20test%20set%20%28AVSpoof2021%20%2B%20CodecFake%29.%20These%20findings%0Ahighlight%20the%20importance%20of%20diverse%20training%20data%2C%20tailored%20objective%20functions%0Aand%20audio%20augmentations%20in%20building%20resilient%20and%20generalizable%20audio%20forgery%0Adetectors.%20Code%20and%20pretrained%20models%20are%20available%20at%0Ahttps%3A//iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-end%2520Audio%2520Deepfake%2520Detection%2520from%2520RAW%2520Waveforms%253A%2520a%2520RawNet-Based%250A%2520%2520Approach%2520with%2520Cross-Dataset%2520Evaluation%26entry.906535625%3DAndrea%2520Di%2520Pierno%2520and%2520Luca%2520Guarnera%2520and%2520Dario%2520Allegra%2520and%2520Sebastiano%2520Battiato%26entry.1292438233%3D%2520%2520Audio%2520deepfakes%2520represent%2520a%2520growing%2520threat%2520to%2520digital%2520security%2520and%2520trust%252C%250Aleveraging%2520advanced%2520generative%2520models%2520to%2520produce%2520synthetic%2520speech%2520that%2520closely%250Amimics%2520real%2520human%2520voices.%2520Detecting%2520such%2520manipulations%2520is%2520especially%250Achallenging%2520under%2520open-world%2520conditions%252C%2520where%2520spoofing%2520methods%2520encountered%250Aduring%2520testing%2520may%2520differ%2520from%2520those%2520seen%2520during%2520training.%2520In%2520this%2520work%252C%2520we%250Apropose%2520an%2520end-to-end%2520deep%2520learning%2520framework%2520for%2520audio%2520deepfake%2520detection%2520that%250Aoperates%2520directly%2520on%2520raw%2520waveforms.%2520Our%2520model%252C%2520RawNetLite%252C%2520is%2520a%2520lightweight%250Aconvolutional-recurrent%2520architecture%2520designed%2520to%2520capture%2520both%2520spectral%2520and%250Atemporal%2520features%2520without%2520handcrafted%2520preprocessing.%2520To%2520enhance%2520robustness%252C%2520we%250Aintroduce%2520a%2520training%2520strategy%2520that%2520combines%2520data%2520from%2520multiple%2520domains%2520and%250Aadopts%2520Focal%2520Loss%2520to%2520emphasize%2520difficult%2520or%2520ambiguous%2520samples.%2520We%2520further%250Ademonstrate%2520that%2520incorporating%2520codec-based%2520manipulations%2520and%2520applying%250Awaveform-level%2520audio%2520augmentations%2520%2528e.g.%252C%2520pitch%2520shifting%252C%2520noise%252C%2520and%2520time%250Astretching%2529%2520leads%2520to%2520significant%2520generalization%2520improvements%2520under%2520realistic%250Aacoustic%2520conditions.%2520The%2520proposed%2520model%2520achieves%2520over%252099.7%2525%2520F1%2520and%25200.25%2525%2520EER%2520on%250Ain-domain%2520data%2520%2528FakeOrReal%2529%252C%2520and%2520up%2520to%252083.4%2525%2520F1%2520with%252016.4%2525%2520EER%2520on%2520a%2520challenging%250Aout-of-distribution%2520test%2520set%2520%2528AVSpoof2021%2520%252B%2520CodecFake%2529.%2520These%2520findings%250Ahighlight%2520the%2520importance%2520of%2520diverse%2520training%2520data%252C%2520tailored%2520objective%2520functions%250Aand%2520audio%2520augmentations%2520in%2520building%2520resilient%2520and%2520generalizable%2520audio%2520forgery%250Adetectors.%2520Code%2520and%2520pretrained%2520models%2520are%2520available%2520at%250Ahttps%253A//iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-end%20Audio%20Deepfake%20Detection%20from%20RAW%20Waveforms%3A%20a%20RawNet-Based%0A%20%20Approach%20with%20Cross-Dataset%20Evaluation&entry.906535625=Andrea%20Di%20Pierno%20and%20Luca%20Guarnera%20and%20Dario%20Allegra%20and%20Sebastiano%20Battiato&entry.1292438233=%20%20Audio%20deepfakes%20represent%20a%20growing%20threat%20to%20digital%20security%20and%20trust%2C%0Aleveraging%20advanced%20generative%20models%20to%20produce%20synthetic%20speech%20that%20closely%0Amimics%20real%20human%20voices.%20Detecting%20such%20manipulations%20is%20especially%0Achallenging%20under%20open-world%20conditions%2C%20where%20spoofing%20methods%20encountered%0Aduring%20testing%20may%20differ%20from%20those%20seen%20during%20training.%20In%20this%20work%2C%20we%0Apropose%20an%20end-to-end%20deep%20learning%20framework%20for%20audio%20deepfake%20detection%20that%0Aoperates%20directly%20on%20raw%20waveforms.%20Our%20model%2C%20RawNetLite%2C%20is%20a%20lightweight%0Aconvolutional-recurrent%20architecture%20designed%20to%20capture%20both%20spectral%20and%0Atemporal%20features%20without%20handcrafted%20preprocessing.%20To%20enhance%20robustness%2C%20we%0Aintroduce%20a%20training%20strategy%20that%20combines%20data%20from%20multiple%20domains%20and%0Aadopts%20Focal%20Loss%20to%20emphasize%20difficult%20or%20ambiguous%20samples.%20We%20further%0Ademonstrate%20that%20incorporating%20codec-based%20manipulations%20and%20applying%0Awaveform-level%20audio%20augmentations%20%28e.g.%2C%20pitch%20shifting%2C%20noise%2C%20and%20time%0Astretching%29%20leads%20to%20significant%20generalization%20improvements%20under%20realistic%0Aacoustic%20conditions.%20The%20proposed%20model%20achieves%20over%2099.7%25%20F1%20and%200.25%25%20EER%20on%0Ain-domain%20data%20%28FakeOrReal%29%2C%20and%20up%20to%2083.4%25%20F1%20with%2016.4%25%20EER%20on%20a%20challenging%0Aout-of-distribution%20test%20set%20%28AVSpoof2021%20%2B%20CodecFake%29.%20These%20findings%0Ahighlight%20the%20importance%20of%20diverse%20training%20data%2C%20tailored%20objective%20functions%0Aand%20audio%20augmentations%20in%20building%20resilient%20and%20generalizable%20audio%20forgery%0Adetectors.%20Code%20and%20pretrained%20models%20are%20available%20at%0Ahttps%3A//iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20923v1&entry.124074799=Read"},
{"title": "NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture\n  for Neuro-Symbolic AI", "author": "Hanchen Yang and Zishen Wan and Ritik Raj and Joongun Park and Ziwei Li and Ananda Samajdar and Arijit Raychowdhury and Tushar Krishna", "abstract": "  Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural\nnetworks with symbolic reasoning to enhance the transparency, reasoning\ncapabilities, and data efficiency of AI systems. Recent NSAI systems have\ngained traction due to their exceptional performance in reasoning tasks and\nhuman-AI collaborative scenarios. Despite these algorithmic advancements,\nexecuting NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains\nchallenging, due to their heterogeneous computing kernels, high memory\nintensity, and unique memory access patterns. Moreover, current NSAI algorithms\nexhibit significant variation in operation types and scales, making them\nincompatible with existing ML accelerators. These challenges highlight the need\nfor a versatile and flexible acceleration framework tailored to NSAI workloads.\nIn this paper, we propose NSFlow, an FPGA-based acceleration framework designed\nto achieve high efficiency, scalability, and versatility across NSAI systems.\nNSFlow features a design architecture generator that identifies workload data\ndependencies and creates optimized dataflow architectures, as well as a\nreconfigurable array with flexible compute units, re-organizable memory, and\nmixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves\n31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like\nsystolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates\nenhanced scalability, with only 4x runtime increase when symbolic workloads\nscale by 150x. To the best of our knowledge, NSFlow is the first framework to\nenable real-time generalizable NSAI algorithms acceleration, demonstrating a\npromising solution for next-generation cognitive systems.\n", "link": "http://arxiv.org/abs/2504.19323v2", "date": "2025-04-29", "relevancy": 1.4485, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5454}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5045}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NSFlow%3A%20An%20End-to-End%20FPGA%20Framework%20with%20Scalable%20Dataflow%20Architecture%0A%20%20for%20Neuro-Symbolic%20AI&body=Title%3A%20NSFlow%3A%20An%20End-to-End%20FPGA%20Framework%20with%20Scalable%20Dataflow%20Architecture%0A%20%20for%20Neuro-Symbolic%20AI%0AAuthor%3A%20Hanchen%20Yang%20and%20Zishen%20Wan%20and%20Ritik%20Raj%20and%20Joongun%20Park%20and%20Ziwei%20Li%20and%20Ananda%20Samajdar%20and%20Arijit%20Raychowdhury%20and%20Tushar%20Krishna%0AAbstract%3A%20%20%20Neuro-Symbolic%20AI%20%28NSAI%29%20is%20an%20emerging%20paradigm%20that%20integrates%20neural%0Anetworks%20with%20symbolic%20reasoning%20to%20enhance%20the%20transparency%2C%20reasoning%0Acapabilities%2C%20and%20data%20efficiency%20of%20AI%20systems.%20Recent%20NSAI%20systems%20have%0Agained%20traction%20due%20to%20their%20exceptional%20performance%20in%20reasoning%20tasks%20and%0Ahuman-AI%20collaborative%20scenarios.%20Despite%20these%20algorithmic%20advancements%2C%0Aexecuting%20NSAI%20tasks%20on%20existing%20hardware%20%28e.g.%2C%20CPUs%2C%20GPUs%2C%20TPUs%29%20remains%0Achallenging%2C%20due%20to%20their%20heterogeneous%20computing%20kernels%2C%20high%20memory%0Aintensity%2C%20and%20unique%20memory%20access%20patterns.%20Moreover%2C%20current%20NSAI%20algorithms%0Aexhibit%20significant%20variation%20in%20operation%20types%20and%20scales%2C%20making%20them%0Aincompatible%20with%20existing%20ML%20accelerators.%20These%20challenges%20highlight%20the%20need%0Afor%20a%20versatile%20and%20flexible%20acceleration%20framework%20tailored%20to%20NSAI%20workloads.%0AIn%20this%20paper%2C%20we%20propose%20NSFlow%2C%20an%20FPGA-based%20acceleration%20framework%20designed%0Ato%20achieve%20high%20efficiency%2C%20scalability%2C%20and%20versatility%20across%20NSAI%20systems.%0ANSFlow%20features%20a%20design%20architecture%20generator%20that%20identifies%20workload%20data%0Adependencies%20and%20creates%20optimized%20dataflow%20architectures%2C%20as%20well%20as%20a%0Areconfigurable%20array%20with%20flexible%20compute%20units%2C%20re-organizable%20memory%2C%20and%0Amixed-precision%20capabilities.%20Evaluating%20across%20NSAI%20workloads%2C%20NSFlow%20achieves%0A31x%20speedup%20over%20Jetson%20TX2%2C%20more%20than%202x%20over%20GPU%2C%208x%20speedup%20over%20TPU-like%0Asystolic%20array%2C%20and%20more%20than%203x%20over%20Xilinx%20DPU.%20NSFlow%20also%20demonstrates%0Aenhanced%20scalability%2C%20with%20only%204x%20runtime%20increase%20when%20symbolic%20workloads%0Ascale%20by%20150x.%20To%20the%20best%20of%20our%20knowledge%2C%20NSFlow%20is%20the%20first%20framework%20to%0Aenable%20real-time%20generalizable%20NSAI%20algorithms%20acceleration%2C%20demonstrating%20a%0Apromising%20solution%20for%20next-generation%20cognitive%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19323v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNSFlow%253A%2520An%2520End-to-End%2520FPGA%2520Framework%2520with%2520Scalable%2520Dataflow%2520Architecture%250A%2520%2520for%2520Neuro-Symbolic%2520AI%26entry.906535625%3DHanchen%2520Yang%2520and%2520Zishen%2520Wan%2520and%2520Ritik%2520Raj%2520and%2520Joongun%2520Park%2520and%2520Ziwei%2520Li%2520and%2520Ananda%2520Samajdar%2520and%2520Arijit%2520Raychowdhury%2520and%2520Tushar%2520Krishna%26entry.1292438233%3D%2520%2520Neuro-Symbolic%2520AI%2520%2528NSAI%2529%2520is%2520an%2520emerging%2520paradigm%2520that%2520integrates%2520neural%250Anetworks%2520with%2520symbolic%2520reasoning%2520to%2520enhance%2520the%2520transparency%252C%2520reasoning%250Acapabilities%252C%2520and%2520data%2520efficiency%2520of%2520AI%2520systems.%2520Recent%2520NSAI%2520systems%2520have%250Agained%2520traction%2520due%2520to%2520their%2520exceptional%2520performance%2520in%2520reasoning%2520tasks%2520and%250Ahuman-AI%2520collaborative%2520scenarios.%2520Despite%2520these%2520algorithmic%2520advancements%252C%250Aexecuting%2520NSAI%2520tasks%2520on%2520existing%2520hardware%2520%2528e.g.%252C%2520CPUs%252C%2520GPUs%252C%2520TPUs%2529%2520remains%250Achallenging%252C%2520due%2520to%2520their%2520heterogeneous%2520computing%2520kernels%252C%2520high%2520memory%250Aintensity%252C%2520and%2520unique%2520memory%2520access%2520patterns.%2520Moreover%252C%2520current%2520NSAI%2520algorithms%250Aexhibit%2520significant%2520variation%2520in%2520operation%2520types%2520and%2520scales%252C%2520making%2520them%250Aincompatible%2520with%2520existing%2520ML%2520accelerators.%2520These%2520challenges%2520highlight%2520the%2520need%250Afor%2520a%2520versatile%2520and%2520flexible%2520acceleration%2520framework%2520tailored%2520to%2520NSAI%2520workloads.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520NSFlow%252C%2520an%2520FPGA-based%2520acceleration%2520framework%2520designed%250Ato%2520achieve%2520high%2520efficiency%252C%2520scalability%252C%2520and%2520versatility%2520across%2520NSAI%2520systems.%250ANSFlow%2520features%2520a%2520design%2520architecture%2520generator%2520that%2520identifies%2520workload%2520data%250Adependencies%2520and%2520creates%2520optimized%2520dataflow%2520architectures%252C%2520as%2520well%2520as%2520a%250Areconfigurable%2520array%2520with%2520flexible%2520compute%2520units%252C%2520re-organizable%2520memory%252C%2520and%250Amixed-precision%2520capabilities.%2520Evaluating%2520across%2520NSAI%2520workloads%252C%2520NSFlow%2520achieves%250A31x%2520speedup%2520over%2520Jetson%2520TX2%252C%2520more%2520than%25202x%2520over%2520GPU%252C%25208x%2520speedup%2520over%2520TPU-like%250Asystolic%2520array%252C%2520and%2520more%2520than%25203x%2520over%2520Xilinx%2520DPU.%2520NSFlow%2520also%2520demonstrates%250Aenhanced%2520scalability%252C%2520with%2520only%25204x%2520runtime%2520increase%2520when%2520symbolic%2520workloads%250Ascale%2520by%2520150x.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520NSFlow%2520is%2520the%2520first%2520framework%2520to%250Aenable%2520real-time%2520generalizable%2520NSAI%2520algorithms%2520acceleration%252C%2520demonstrating%2520a%250Apromising%2520solution%2520for%2520next-generation%2520cognitive%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19323v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NSFlow%3A%20An%20End-to-End%20FPGA%20Framework%20with%20Scalable%20Dataflow%20Architecture%0A%20%20for%20Neuro-Symbolic%20AI&entry.906535625=Hanchen%20Yang%20and%20Zishen%20Wan%20and%20Ritik%20Raj%20and%20Joongun%20Park%20and%20Ziwei%20Li%20and%20Ananda%20Samajdar%20and%20Arijit%20Raychowdhury%20and%20Tushar%20Krishna&entry.1292438233=%20%20Neuro-Symbolic%20AI%20%28NSAI%29%20is%20an%20emerging%20paradigm%20that%20integrates%20neural%0Anetworks%20with%20symbolic%20reasoning%20to%20enhance%20the%20transparency%2C%20reasoning%0Acapabilities%2C%20and%20data%20efficiency%20of%20AI%20systems.%20Recent%20NSAI%20systems%20have%0Agained%20traction%20due%20to%20their%20exceptional%20performance%20in%20reasoning%20tasks%20and%0Ahuman-AI%20collaborative%20scenarios.%20Despite%20these%20algorithmic%20advancements%2C%0Aexecuting%20NSAI%20tasks%20on%20existing%20hardware%20%28e.g.%2C%20CPUs%2C%20GPUs%2C%20TPUs%29%20remains%0Achallenging%2C%20due%20to%20their%20heterogeneous%20computing%20kernels%2C%20high%20memory%0Aintensity%2C%20and%20unique%20memory%20access%20patterns.%20Moreover%2C%20current%20NSAI%20algorithms%0Aexhibit%20significant%20variation%20in%20operation%20types%20and%20scales%2C%20making%20them%0Aincompatible%20with%20existing%20ML%20accelerators.%20These%20challenges%20highlight%20the%20need%0Afor%20a%20versatile%20and%20flexible%20acceleration%20framework%20tailored%20to%20NSAI%20workloads.%0AIn%20this%20paper%2C%20we%20propose%20NSFlow%2C%20an%20FPGA-based%20acceleration%20framework%20designed%0Ato%20achieve%20high%20efficiency%2C%20scalability%2C%20and%20versatility%20across%20NSAI%20systems.%0ANSFlow%20features%20a%20design%20architecture%20generator%20that%20identifies%20workload%20data%0Adependencies%20and%20creates%20optimized%20dataflow%20architectures%2C%20as%20well%20as%20a%0Areconfigurable%20array%20with%20flexible%20compute%20units%2C%20re-organizable%20memory%2C%20and%0Amixed-precision%20capabilities.%20Evaluating%20across%20NSAI%20workloads%2C%20NSFlow%20achieves%0A31x%20speedup%20over%20Jetson%20TX2%2C%20more%20than%202x%20over%20GPU%2C%208x%20speedup%20over%20TPU-like%0Asystolic%20array%2C%20and%20more%20than%203x%20over%20Xilinx%20DPU.%20NSFlow%20also%20demonstrates%0Aenhanced%20scalability%2C%20with%20only%204x%20runtime%20increase%20when%20symbolic%20workloads%0Ascale%20by%20150x.%20To%20the%20best%20of%20our%20knowledge%2C%20NSFlow%20is%20the%20first%20framework%20to%0Aenable%20real-time%20generalizable%20NSAI%20algorithms%20acceleration%2C%20demonstrating%20a%0Apromising%20solution%20for%20next-generation%20cognitive%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19323v2&entry.124074799=Read"},
{"title": "Evaluating Generative Models for Tabular Data: Novel Metrics and\n  Benchmarking", "author": "Dayananda Herurkar and Ahmad Ali and Andreas Dengel", "abstract": "  Generative models have revolutionized multiple domains, yet their application\nto tabular data remains underexplored. Evaluating generative models for tabular\ndata presents unique challenges due to structural complexity, large-scale\nvariability, and mixed data types, making it difficult to intuitively capture\nintricate patterns. Existing evaluation metrics offer only partial insights,\nlacking a comprehensive measure of generative performance. To address this\nlimitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS.\nOur extensive experimental analysis, conducted on three standard network\nintrusion detection datasets, compares these metrics with established\nevaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results\ndemonstrate that FAED effectively captures generative modeling issues\noverlooked by existing metrics. While FPCAD exhibits promising performance,\nfurther refinements are necessary to enhance its reliability. Our proposed\nframework provides a robust and practical approach for assessing generative\nmodels in tabular data applications.\n", "link": "http://arxiv.org/abs/2504.20900v1", "date": "2025-04-29", "relevancy": 1.4344, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4932}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4897}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Generative%20Models%20for%20Tabular%20Data%3A%20Novel%20Metrics%20and%0A%20%20Benchmarking&body=Title%3A%20Evaluating%20Generative%20Models%20for%20Tabular%20Data%3A%20Novel%20Metrics%20and%0A%20%20Benchmarking%0AAuthor%3A%20Dayananda%20Herurkar%20and%20Ahmad%20Ali%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20Generative%20models%20have%20revolutionized%20multiple%20domains%2C%20yet%20their%20application%0Ato%20tabular%20data%20remains%20underexplored.%20Evaluating%20generative%20models%20for%20tabular%0Adata%20presents%20unique%20challenges%20due%20to%20structural%20complexity%2C%20large-scale%0Avariability%2C%20and%20mixed%20data%20types%2C%20making%20it%20difficult%20to%20intuitively%20capture%0Aintricate%20patterns.%20Existing%20evaluation%20metrics%20offer%20only%20partial%20insights%2C%0Alacking%20a%20comprehensive%20measure%20of%20generative%20performance.%20To%20address%20this%0Alimitation%2C%20we%20propose%20three%20novel%20evaluation%20metrics%3A%20FAED%2C%20FPCAD%2C%20and%20RFIS.%0AOur%20extensive%20experimental%20analysis%2C%20conducted%20on%20three%20standard%20network%0Aintrusion%20detection%20datasets%2C%20compares%20these%20metrics%20with%20established%0Aevaluation%20methods%20such%20as%20Fidelity%2C%20Utility%2C%20TSTR%2C%20and%20TRTS.%20Our%20results%0Ademonstrate%20that%20FAED%20effectively%20captures%20generative%20modeling%20issues%0Aoverlooked%20by%20existing%20metrics.%20While%20FPCAD%20exhibits%20promising%20performance%2C%0Afurther%20refinements%20are%20necessary%20to%20enhance%20its%20reliability.%20Our%20proposed%0Aframework%20provides%20a%20robust%20and%20practical%20approach%20for%20assessing%20generative%0Amodels%20in%20tabular%20data%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20900v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Generative%2520Models%2520for%2520Tabular%2520Data%253A%2520Novel%2520Metrics%2520and%250A%2520%2520Benchmarking%26entry.906535625%3DDayananda%2520Herurkar%2520and%2520Ahmad%2520Ali%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520Generative%2520models%2520have%2520revolutionized%2520multiple%2520domains%252C%2520yet%2520their%2520application%250Ato%2520tabular%2520data%2520remains%2520underexplored.%2520Evaluating%2520generative%2520models%2520for%2520tabular%250Adata%2520presents%2520unique%2520challenges%2520due%2520to%2520structural%2520complexity%252C%2520large-scale%250Avariability%252C%2520and%2520mixed%2520data%2520types%252C%2520making%2520it%2520difficult%2520to%2520intuitively%2520capture%250Aintricate%2520patterns.%2520Existing%2520evaluation%2520metrics%2520offer%2520only%2520partial%2520insights%252C%250Alacking%2520a%2520comprehensive%2520measure%2520of%2520generative%2520performance.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520propose%2520three%2520novel%2520evaluation%2520metrics%253A%2520FAED%252C%2520FPCAD%252C%2520and%2520RFIS.%250AOur%2520extensive%2520experimental%2520analysis%252C%2520conducted%2520on%2520three%2520standard%2520network%250Aintrusion%2520detection%2520datasets%252C%2520compares%2520these%2520metrics%2520with%2520established%250Aevaluation%2520methods%2520such%2520as%2520Fidelity%252C%2520Utility%252C%2520TSTR%252C%2520and%2520TRTS.%2520Our%2520results%250Ademonstrate%2520that%2520FAED%2520effectively%2520captures%2520generative%2520modeling%2520issues%250Aoverlooked%2520by%2520existing%2520metrics.%2520While%2520FPCAD%2520exhibits%2520promising%2520performance%252C%250Afurther%2520refinements%2520are%2520necessary%2520to%2520enhance%2520its%2520reliability.%2520Our%2520proposed%250Aframework%2520provides%2520a%2520robust%2520and%2520practical%2520approach%2520for%2520assessing%2520generative%250Amodels%2520in%2520tabular%2520data%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20900v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Generative%20Models%20for%20Tabular%20Data%3A%20Novel%20Metrics%20and%0A%20%20Benchmarking&entry.906535625=Dayananda%20Herurkar%20and%20Ahmad%20Ali%20and%20Andreas%20Dengel&entry.1292438233=%20%20Generative%20models%20have%20revolutionized%20multiple%20domains%2C%20yet%20their%20application%0Ato%20tabular%20data%20remains%20underexplored.%20Evaluating%20generative%20models%20for%20tabular%0Adata%20presents%20unique%20challenges%20due%20to%20structural%20complexity%2C%20large-scale%0Avariability%2C%20and%20mixed%20data%20types%2C%20making%20it%20difficult%20to%20intuitively%20capture%0Aintricate%20patterns.%20Existing%20evaluation%20metrics%20offer%20only%20partial%20insights%2C%0Alacking%20a%20comprehensive%20measure%20of%20generative%20performance.%20To%20address%20this%0Alimitation%2C%20we%20propose%20three%20novel%20evaluation%20metrics%3A%20FAED%2C%20FPCAD%2C%20and%20RFIS.%0AOur%20extensive%20experimental%20analysis%2C%20conducted%20on%20three%20standard%20network%0Aintrusion%20detection%20datasets%2C%20compares%20these%20metrics%20with%20established%0Aevaluation%20methods%20such%20as%20Fidelity%2C%20Utility%2C%20TSTR%2C%20and%20TRTS.%20Our%20results%0Ademonstrate%20that%20FAED%20effectively%20captures%20generative%20modeling%20issues%0Aoverlooked%20by%20existing%20metrics.%20While%20FPCAD%20exhibits%20promising%20performance%2C%0Afurther%20refinements%20are%20necessary%20to%20enhance%20its%20reliability.%20Our%20proposed%0Aframework%20provides%20a%20robust%20and%20practical%20approach%20for%20assessing%20generative%0Amodels%20in%20tabular%20data%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20900v1&entry.124074799=Read"},
{"title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM\n  Security", "author": "Zikui Cai and Shayan Shabihi and Bang An and Zora Che and Brian R. Bartoldson and Bhavya Kailkhura and Tom Goldstein and Furong Huang", "abstract": "  We introduce AegisLLM, a cooperative multi-agent defense against adversarial\nattacks and information leakage. In AegisLLM, a structured workflow of\nautonomous agents - orchestrator, deflector, responder, and evaluator -\ncollaborate to ensure safe and compliant LLM outputs, while self-improving over\ntime through prompt optimization. We show that scaling agentic reasoning system\nat test-time - both by incorporating additional agent roles and by leveraging\nautomated prompt optimization (such as DSPy)- substantially enhances robustness\nwithout compromising model utility. This test-time defense enables real-time\nadaptability to evolving attacks, without requiring model retraining.\nComprehensive evaluations across key threat scenarios, including unlearning and\njailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning\nbenchmark, AegisLLM achieves near-perfect unlearning with only 20 training\nexamples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve\n51% improvement compared to the base model on StrongReject, with false refusal\nrates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our\nresults highlight the advantages of adaptive, agentic reasoning over static\ndefenses, establishing AegisLLM as a strong runtime alternative to traditional\napproaches based on model modifications. Code is available at\nhttps://github.com/zikuicai/aegisllm\n", "link": "http://arxiv.org/abs/2504.20965v1", "date": "2025-04-29", "relevancy": 1.4167, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.483}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4773}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AegisLLM%3A%20Scaling%20Agentic%20Systems%20for%20Self-Reflective%20Defense%20in%20LLM%0A%20%20Security&body=Title%3A%20AegisLLM%3A%20Scaling%20Agentic%20Systems%20for%20Self-Reflective%20Defense%20in%20LLM%0A%20%20Security%0AAuthor%3A%20Zikui%20Cai%20and%20Shayan%20Shabihi%20and%20Bang%20An%20and%20Zora%20Che%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Tom%20Goldstein%20and%20Furong%20Huang%0AAbstract%3A%20%20%20We%20introduce%20AegisLLM%2C%20a%20cooperative%20multi-agent%20defense%20against%20adversarial%0Aattacks%20and%20information%20leakage.%20In%20AegisLLM%2C%20a%20structured%20workflow%20of%0Aautonomous%20agents%20-%20orchestrator%2C%20deflector%2C%20responder%2C%20and%20evaluator%20-%0Acollaborate%20to%20ensure%20safe%20and%20compliant%20LLM%20outputs%2C%20while%20self-improving%20over%0Atime%20through%20prompt%20optimization.%20We%20show%20that%20scaling%20agentic%20reasoning%20system%0Aat%20test-time%20-%20both%20by%20incorporating%20additional%20agent%20roles%20and%20by%20leveraging%0Aautomated%20prompt%20optimization%20%28such%20as%20DSPy%29-%20substantially%20enhances%20robustness%0Awithout%20compromising%20model%20utility.%20This%20test-time%20defense%20enables%20real-time%0Aadaptability%20to%20evolving%20attacks%2C%20without%20requiring%20model%20retraining.%0AComprehensive%20evaluations%20across%20key%20threat%20scenarios%2C%20including%20unlearning%20and%0Ajailbreaking%2C%20demonstrate%20the%20effectiveness%20of%20AegisLLM.%20On%20the%20WMDP%20unlearning%0Abenchmark%2C%20AegisLLM%20achieves%20near-perfect%20unlearning%20with%20only%2020%20training%0Aexamples%20and%20fewer%20than%20300%20LM%20calls.%20For%20jailbreaking%20benchmarks%2C%20we%20achieve%0A51%25%20improvement%20compared%20to%20the%20base%20model%20on%20StrongReject%2C%20with%20false%20refusal%0Arates%20of%20only%207.9%25%20on%20PHTest%20compared%20to%2018-55%25%20for%20comparable%20methods.%20Our%0Aresults%20highlight%20the%20advantages%20of%20adaptive%2C%20agentic%20reasoning%20over%20static%0Adefenses%2C%20establishing%20AegisLLM%20as%20a%20strong%20runtime%20alternative%20to%20traditional%0Aapproaches%20based%20on%20model%20modifications.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zikuicai/aegisllm%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAegisLLM%253A%2520Scaling%2520Agentic%2520Systems%2520for%2520Self-Reflective%2520Defense%2520in%2520LLM%250A%2520%2520Security%26entry.906535625%3DZikui%2520Cai%2520and%2520Shayan%2520Shabihi%2520and%2520Bang%2520An%2520and%2520Zora%2520Che%2520and%2520Brian%2520R.%2520Bartoldson%2520and%2520Bhavya%2520Kailkhura%2520and%2520Tom%2520Goldstein%2520and%2520Furong%2520Huang%26entry.1292438233%3D%2520%2520We%2520introduce%2520AegisLLM%252C%2520a%2520cooperative%2520multi-agent%2520defense%2520against%2520adversarial%250Aattacks%2520and%2520information%2520leakage.%2520In%2520AegisLLM%252C%2520a%2520structured%2520workflow%2520of%250Aautonomous%2520agents%2520-%2520orchestrator%252C%2520deflector%252C%2520responder%252C%2520and%2520evaluator%2520-%250Acollaborate%2520to%2520ensure%2520safe%2520and%2520compliant%2520LLM%2520outputs%252C%2520while%2520self-improving%2520over%250Atime%2520through%2520prompt%2520optimization.%2520We%2520show%2520that%2520scaling%2520agentic%2520reasoning%2520system%250Aat%2520test-time%2520-%2520both%2520by%2520incorporating%2520additional%2520agent%2520roles%2520and%2520by%2520leveraging%250Aautomated%2520prompt%2520optimization%2520%2528such%2520as%2520DSPy%2529-%2520substantially%2520enhances%2520robustness%250Awithout%2520compromising%2520model%2520utility.%2520This%2520test-time%2520defense%2520enables%2520real-time%250Aadaptability%2520to%2520evolving%2520attacks%252C%2520without%2520requiring%2520model%2520retraining.%250AComprehensive%2520evaluations%2520across%2520key%2520threat%2520scenarios%252C%2520including%2520unlearning%2520and%250Ajailbreaking%252C%2520demonstrate%2520the%2520effectiveness%2520of%2520AegisLLM.%2520On%2520the%2520WMDP%2520unlearning%250Abenchmark%252C%2520AegisLLM%2520achieves%2520near-perfect%2520unlearning%2520with%2520only%252020%2520training%250Aexamples%2520and%2520fewer%2520than%2520300%2520LM%2520calls.%2520For%2520jailbreaking%2520benchmarks%252C%2520we%2520achieve%250A51%2525%2520improvement%2520compared%2520to%2520the%2520base%2520model%2520on%2520StrongReject%252C%2520with%2520false%2520refusal%250Arates%2520of%2520only%25207.9%2525%2520on%2520PHTest%2520compared%2520to%252018-55%2525%2520for%2520comparable%2520methods.%2520Our%250Aresults%2520highlight%2520the%2520advantages%2520of%2520adaptive%252C%2520agentic%2520reasoning%2520over%2520static%250Adefenses%252C%2520establishing%2520AegisLLM%2520as%2520a%2520strong%2520runtime%2520alternative%2520to%2520traditional%250Aapproaches%2520based%2520on%2520model%2520modifications.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/zikuicai/aegisllm%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AegisLLM%3A%20Scaling%20Agentic%20Systems%20for%20Self-Reflective%20Defense%20in%20LLM%0A%20%20Security&entry.906535625=Zikui%20Cai%20and%20Shayan%20Shabihi%20and%20Bang%20An%20and%20Zora%20Che%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Tom%20Goldstein%20and%20Furong%20Huang&entry.1292438233=%20%20We%20introduce%20AegisLLM%2C%20a%20cooperative%20multi-agent%20defense%20against%20adversarial%0Aattacks%20and%20information%20leakage.%20In%20AegisLLM%2C%20a%20structured%20workflow%20of%0Aautonomous%20agents%20-%20orchestrator%2C%20deflector%2C%20responder%2C%20and%20evaluator%20-%0Acollaborate%20to%20ensure%20safe%20and%20compliant%20LLM%20outputs%2C%20while%20self-improving%20over%0Atime%20through%20prompt%20optimization.%20We%20show%20that%20scaling%20agentic%20reasoning%20system%0Aat%20test-time%20-%20both%20by%20incorporating%20additional%20agent%20roles%20and%20by%20leveraging%0Aautomated%20prompt%20optimization%20%28such%20as%20DSPy%29-%20substantially%20enhances%20robustness%0Awithout%20compromising%20model%20utility.%20This%20test-time%20defense%20enables%20real-time%0Aadaptability%20to%20evolving%20attacks%2C%20without%20requiring%20model%20retraining.%0AComprehensive%20evaluations%20across%20key%20threat%20scenarios%2C%20including%20unlearning%20and%0Ajailbreaking%2C%20demonstrate%20the%20effectiveness%20of%20AegisLLM.%20On%20the%20WMDP%20unlearning%0Abenchmark%2C%20AegisLLM%20achieves%20near-perfect%20unlearning%20with%20only%2020%20training%0Aexamples%20and%20fewer%20than%20300%20LM%20calls.%20For%20jailbreaking%20benchmarks%2C%20we%20achieve%0A51%25%20improvement%20compared%20to%20the%20base%20model%20on%20StrongReject%2C%20with%20false%20refusal%0Arates%20of%20only%207.9%25%20on%20PHTest%20compared%20to%2018-55%25%20for%20comparable%20methods.%20Our%0Aresults%20highlight%20the%20advantages%20of%20adaptive%2C%20agentic%20reasoning%20over%20static%0Adefenses%2C%20establishing%20AegisLLM%20as%20a%20strong%20runtime%20alternative%20to%20traditional%0Aapproaches%20based%20on%20model%20modifications.%20Code%20is%20available%20at%0Ahttps%3A//github.com/zikuicai/aegisllm%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20965v1&entry.124074799=Read"},
{"title": "SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep\n  Features", "author": "Mete Erdogan and Sebnem Demirtas", "abstract": "  Accurate and early diagnosis of pneumonia through X-ray imaging is essential\nfor effective treatment and improved patient outcomes. Recent advancements in\nmachine learning have enabled automated diagnostic tools that assist\nradiologists in making more reliable and efficient decisions. In this work, we\npropose a Singular Value Decomposition-based Least Squares (SVD-LS) framework\nfor multi-class pneumonia classification, leveraging powerful feature\nrepresentations from state-of-the-art self-supervised and transfer learning\nmodels. Rather than relying on computationally expensive gradient based\nfine-tuning, we employ a closed-form, non-iterative classification approach\nthat ensures efficiency without compromising accuracy. Experimental results\ndemonstrate that SVD-LS achieves competitive performance while offering\nsignificantly reduced computational costs, making it a viable alternative for\nreal-time medical imaging applications.\n", "link": "http://arxiv.org/abs/2504.20970v1", "date": "2025-04-29", "relevancy": 1.4111, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5123}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4618}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVD%20Based%20Least%20Squares%20for%20X-Ray%20Pneumonia%20Classification%20Using%20Deep%0A%20%20Features&body=Title%3A%20SVD%20Based%20Least%20Squares%20for%20X-Ray%20Pneumonia%20Classification%20Using%20Deep%0A%20%20Features%0AAuthor%3A%20Mete%20Erdogan%20and%20Sebnem%20Demirtas%0AAbstract%3A%20%20%20Accurate%20and%20early%20diagnosis%20of%20pneumonia%20through%20X-ray%20imaging%20is%20essential%0Afor%20effective%20treatment%20and%20improved%20patient%20outcomes.%20Recent%20advancements%20in%0Amachine%20learning%20have%20enabled%20automated%20diagnostic%20tools%20that%20assist%0Aradiologists%20in%20making%20more%20reliable%20and%20efficient%20decisions.%20In%20this%20work%2C%20we%0Apropose%20a%20Singular%20Value%20Decomposition-based%20Least%20Squares%20%28SVD-LS%29%20framework%0Afor%20multi-class%20pneumonia%20classification%2C%20leveraging%20powerful%20feature%0Arepresentations%20from%20state-of-the-art%20self-supervised%20and%20transfer%20learning%0Amodels.%20Rather%20than%20relying%20on%20computationally%20expensive%20gradient%20based%0Afine-tuning%2C%20we%20employ%20a%20closed-form%2C%20non-iterative%20classification%20approach%0Athat%20ensures%20efficiency%20without%20compromising%20accuracy.%20Experimental%20results%0Ademonstrate%20that%20SVD-LS%20achieves%20competitive%20performance%20while%20offering%0Asignificantly%20reduced%20computational%20costs%2C%20making%20it%20a%20viable%20alternative%20for%0Areal-time%20medical%20imaging%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20970v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVD%2520Based%2520Least%2520Squares%2520for%2520X-Ray%2520Pneumonia%2520Classification%2520Using%2520Deep%250A%2520%2520Features%26entry.906535625%3DMete%2520Erdogan%2520and%2520Sebnem%2520Demirtas%26entry.1292438233%3D%2520%2520Accurate%2520and%2520early%2520diagnosis%2520of%2520pneumonia%2520through%2520X-ray%2520imaging%2520is%2520essential%250Afor%2520effective%2520treatment%2520and%2520improved%2520patient%2520outcomes.%2520Recent%2520advancements%2520in%250Amachine%2520learning%2520have%2520enabled%2520automated%2520diagnostic%2520tools%2520that%2520assist%250Aradiologists%2520in%2520making%2520more%2520reliable%2520and%2520efficient%2520decisions.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520Singular%2520Value%2520Decomposition-based%2520Least%2520Squares%2520%2528SVD-LS%2529%2520framework%250Afor%2520multi-class%2520pneumonia%2520classification%252C%2520leveraging%2520powerful%2520feature%250Arepresentations%2520from%2520state-of-the-art%2520self-supervised%2520and%2520transfer%2520learning%250Amodels.%2520Rather%2520than%2520relying%2520on%2520computationally%2520expensive%2520gradient%2520based%250Afine-tuning%252C%2520we%2520employ%2520a%2520closed-form%252C%2520non-iterative%2520classification%2520approach%250Athat%2520ensures%2520efficiency%2520without%2520compromising%2520accuracy.%2520Experimental%2520results%250Ademonstrate%2520that%2520SVD-LS%2520achieves%2520competitive%2520performance%2520while%2520offering%250Asignificantly%2520reduced%2520computational%2520costs%252C%2520making%2520it%2520a%2520viable%2520alternative%2520for%250Areal-time%2520medical%2520imaging%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20970v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVD%20Based%20Least%20Squares%20for%20X-Ray%20Pneumonia%20Classification%20Using%20Deep%0A%20%20Features&entry.906535625=Mete%20Erdogan%20and%20Sebnem%20Demirtas&entry.1292438233=%20%20Accurate%20and%20early%20diagnosis%20of%20pneumonia%20through%20X-ray%20imaging%20is%20essential%0Afor%20effective%20treatment%20and%20improved%20patient%20outcomes.%20Recent%20advancements%20in%0Amachine%20learning%20have%20enabled%20automated%20diagnostic%20tools%20that%20assist%0Aradiologists%20in%20making%20more%20reliable%20and%20efficient%20decisions.%20In%20this%20work%2C%20we%0Apropose%20a%20Singular%20Value%20Decomposition-based%20Least%20Squares%20%28SVD-LS%29%20framework%0Afor%20multi-class%20pneumonia%20classification%2C%20leveraging%20powerful%20feature%0Arepresentations%20from%20state-of-the-art%20self-supervised%20and%20transfer%20learning%0Amodels.%20Rather%20than%20relying%20on%20computationally%20expensive%20gradient%20based%0Afine-tuning%2C%20we%20employ%20a%20closed-form%2C%20non-iterative%20classification%20approach%0Athat%20ensures%20efficiency%20without%20compromising%20accuracy.%20Experimental%20results%0Ademonstrate%20that%20SVD-LS%20achieves%20competitive%20performance%20while%20offering%0Asignificantly%20reduced%20computational%20costs%2C%20making%20it%20a%20viable%20alternative%20for%0Areal-time%20medical%20imaging%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20970v1&entry.124074799=Read"},
{"title": "LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains", "author": "Giuseppe De Giacomo and Gianmarco Parretti and Shufang Zhu", "abstract": "  We study a variant of LTLf synthesis that synthesizes adaptive strategies for\nachieving a multi-tier goal, consisting of multiple increasingly challenging\nLTLf objectives in nondeterministic planning domains. Adaptive strategies are\nstrategies that at any point of their execution (i) enforce the satisfaction of\nas many objectives as possible in the multi-tier goal, and (ii) exploit\npossible cooperation from the environment to satisfy as many as possible of the\nremaining ones. This happens dynamically: if the environment cooperates (ii)\nand an objective becomes enforceable (i), then our strategies will enforce it.\nWe provide a game-theoretic technique to compute adaptive strategies that is\nsound and complete. Notably, our technique is polynomial, in fact quadratic, in\nthe number of objectives. In other words, it handles multi-tier goals with only\na minor overhead compared to standard LTLf synthesis.\n", "link": "http://arxiv.org/abs/2504.20983v1", "date": "2025-04-29", "relevancy": 1.3413, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4513}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4488}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LTLf%20Adaptive%20Synthesis%20for%20Multi-Tier%20Goals%20in%20Nondeterministic%20Domains&body=Title%3A%20LTLf%20Adaptive%20Synthesis%20for%20Multi-Tier%20Goals%20in%20Nondeterministic%20Domains%0AAuthor%3A%20Giuseppe%20De%20Giacomo%20and%20Gianmarco%20Parretti%20and%20Shufang%20Zhu%0AAbstract%3A%20%20%20We%20study%20a%20variant%20of%20LTLf%20synthesis%20that%20synthesizes%20adaptive%20strategies%20for%0Aachieving%20a%20multi-tier%20goal%2C%20consisting%20of%20multiple%20increasingly%20challenging%0ALTLf%20objectives%20in%20nondeterministic%20planning%20domains.%20Adaptive%20strategies%20are%0Astrategies%20that%20at%20any%20point%20of%20their%20execution%20%28i%29%20enforce%20the%20satisfaction%20of%0Aas%20many%20objectives%20as%20possible%20in%20the%20multi-tier%20goal%2C%20and%20%28ii%29%20exploit%0Apossible%20cooperation%20from%20the%20environment%20to%20satisfy%20as%20many%20as%20possible%20of%20the%0Aremaining%20ones.%20This%20happens%20dynamically%3A%20if%20the%20environment%20cooperates%20%28ii%29%0Aand%20an%20objective%20becomes%20enforceable%20%28i%29%2C%20then%20our%20strategies%20will%20enforce%20it.%0AWe%20provide%20a%20game-theoretic%20technique%20to%20compute%20adaptive%20strategies%20that%20is%0Asound%20and%20complete.%20Notably%2C%20our%20technique%20is%20polynomial%2C%20in%20fact%20quadratic%2C%20in%0Athe%20number%20of%20objectives.%20In%20other%20words%2C%20it%20handles%20multi-tier%20goals%20with%20only%0Aa%20minor%20overhead%20compared%20to%20standard%20LTLf%20synthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLTLf%2520Adaptive%2520Synthesis%2520for%2520Multi-Tier%2520Goals%2520in%2520Nondeterministic%2520Domains%26entry.906535625%3DGiuseppe%2520De%2520Giacomo%2520and%2520Gianmarco%2520Parretti%2520and%2520Shufang%2520Zhu%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520variant%2520of%2520LTLf%2520synthesis%2520that%2520synthesizes%2520adaptive%2520strategies%2520for%250Aachieving%2520a%2520multi-tier%2520goal%252C%2520consisting%2520of%2520multiple%2520increasingly%2520challenging%250ALTLf%2520objectives%2520in%2520nondeterministic%2520planning%2520domains.%2520Adaptive%2520strategies%2520are%250Astrategies%2520that%2520at%2520any%2520point%2520of%2520their%2520execution%2520%2528i%2529%2520enforce%2520the%2520satisfaction%2520of%250Aas%2520many%2520objectives%2520as%2520possible%2520in%2520the%2520multi-tier%2520goal%252C%2520and%2520%2528ii%2529%2520exploit%250Apossible%2520cooperation%2520from%2520the%2520environment%2520to%2520satisfy%2520as%2520many%2520as%2520possible%2520of%2520the%250Aremaining%2520ones.%2520This%2520happens%2520dynamically%253A%2520if%2520the%2520environment%2520cooperates%2520%2528ii%2529%250Aand%2520an%2520objective%2520becomes%2520enforceable%2520%2528i%2529%252C%2520then%2520our%2520strategies%2520will%2520enforce%2520it.%250AWe%2520provide%2520a%2520game-theoretic%2520technique%2520to%2520compute%2520adaptive%2520strategies%2520that%2520is%250Asound%2520and%2520complete.%2520Notably%252C%2520our%2520technique%2520is%2520polynomial%252C%2520in%2520fact%2520quadratic%252C%2520in%250Athe%2520number%2520of%2520objectives.%2520In%2520other%2520words%252C%2520it%2520handles%2520multi-tier%2520goals%2520with%2520only%250Aa%2520minor%2520overhead%2520compared%2520to%2520standard%2520LTLf%2520synthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LTLf%20Adaptive%20Synthesis%20for%20Multi-Tier%20Goals%20in%20Nondeterministic%20Domains&entry.906535625=Giuseppe%20De%20Giacomo%20and%20Gianmarco%20Parretti%20and%20Shufang%20Zhu&entry.1292438233=%20%20We%20study%20a%20variant%20of%20LTLf%20synthesis%20that%20synthesizes%20adaptive%20strategies%20for%0Aachieving%20a%20multi-tier%20goal%2C%20consisting%20of%20multiple%20increasingly%20challenging%0ALTLf%20objectives%20in%20nondeterministic%20planning%20domains.%20Adaptive%20strategies%20are%0Astrategies%20that%20at%20any%20point%20of%20their%20execution%20%28i%29%20enforce%20the%20satisfaction%20of%0Aas%20many%20objectives%20as%20possible%20in%20the%20multi-tier%20goal%2C%20and%20%28ii%29%20exploit%0Apossible%20cooperation%20from%20the%20environment%20to%20satisfy%20as%20many%20as%20possible%20of%20the%0Aremaining%20ones.%20This%20happens%20dynamically%3A%20if%20the%20environment%20cooperates%20%28ii%29%0Aand%20an%20objective%20becomes%20enforceable%20%28i%29%2C%20then%20our%20strategies%20will%20enforce%20it.%0AWe%20provide%20a%20game-theoretic%20technique%20to%20compute%20adaptive%20strategies%20that%20is%0Asound%20and%20complete.%20Notably%2C%20our%20technique%20is%20polynomial%2C%20in%20fact%20quadratic%2C%20in%0Athe%20number%20of%20objectives.%20In%20other%20words%2C%20it%20handles%20multi-tier%20goals%20with%20only%0Aa%20minor%20overhead%20compared%20to%20standard%20LTLf%20synthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20983v1&entry.124074799=Read"},
{"title": "MOSIC: Model-Agnostic Optimal Subgroup Identification with\n  Multi-Constraint for Improved Reliability", "author": "Wenxin Chen and Weishen Pan and Kyra Gan and Fei Wang", "abstract": "  Identifying subgroups that benefit from specific treatments using\nobservational data is a critical challenge in personalized medicine. Most\nexisting approaches solely focus on identifying a subgroup with an improved\ntreatment effect. However, practical considerations, such as ensuring a minimum\nsubgroup size for representativeness or achieving sufficient confounder balance\nfor reliability, are also important for making findings clinically meaningful\nand actionable. While some studies address these constraints individually, none\noffer a unified approach to handle them simultaneously. To bridge this gap, we\npropose a model-agnostic framework for optimal subgroup identification under\nmultiple constraints. We reformulate this combinatorial problem as an\nunconstrained min-max optimization problem with novel modifications and solve\nit by a gradient descent ascent algorithm. We further prove its convergence to\na feasible and locally optimal solution. Our method is stable and highly\nflexible, supporting various models and techniques for estimating and\noptimizing treatment effectiveness with observational data. Extensive\nexperiments on both synthetic and real-world datasets demonstrate its\neffectiveness in identifying subgroups that satisfy multiple constraints,\nachieving higher treatment effects and better confounder balancing results\nacross different group sizes.\n", "link": "http://arxiv.org/abs/2504.20908v1", "date": "2025-04-29", "relevancy": 1.3381, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4553}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4447}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOSIC%3A%20Model-Agnostic%20Optimal%20Subgroup%20Identification%20with%0A%20%20Multi-Constraint%20for%20Improved%20Reliability&body=Title%3A%20MOSIC%3A%20Model-Agnostic%20Optimal%20Subgroup%20Identification%20with%0A%20%20Multi-Constraint%20for%20Improved%20Reliability%0AAuthor%3A%20Wenxin%20Chen%20and%20Weishen%20Pan%20and%20Kyra%20Gan%20and%20Fei%20Wang%0AAbstract%3A%20%20%20Identifying%20subgroups%20that%20benefit%20from%20specific%20treatments%20using%0Aobservational%20data%20is%20a%20critical%20challenge%20in%20personalized%20medicine.%20Most%0Aexisting%20approaches%20solely%20focus%20on%20identifying%20a%20subgroup%20with%20an%20improved%0Atreatment%20effect.%20However%2C%20practical%20considerations%2C%20such%20as%20ensuring%20a%20minimum%0Asubgroup%20size%20for%20representativeness%20or%20achieving%20sufficient%20confounder%20balance%0Afor%20reliability%2C%20are%20also%20important%20for%20making%20findings%20clinically%20meaningful%0Aand%20actionable.%20While%20some%20studies%20address%20these%20constraints%20individually%2C%20none%0Aoffer%20a%20unified%20approach%20to%20handle%20them%20simultaneously.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20a%20model-agnostic%20framework%20for%20optimal%20subgroup%20identification%20under%0Amultiple%20constraints.%20We%20reformulate%20this%20combinatorial%20problem%20as%20an%0Aunconstrained%20min-max%20optimization%20problem%20with%20novel%20modifications%20and%20solve%0Ait%20by%20a%20gradient%20descent%20ascent%20algorithm.%20We%20further%20prove%20its%20convergence%20to%0Aa%20feasible%20and%20locally%20optimal%20solution.%20Our%20method%20is%20stable%20and%20highly%0Aflexible%2C%20supporting%20various%20models%20and%20techniques%20for%20estimating%20and%0Aoptimizing%20treatment%20effectiveness%20with%20observational%20data.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20its%0Aeffectiveness%20in%20identifying%20subgroups%20that%20satisfy%20multiple%20constraints%2C%0Aachieving%20higher%20treatment%20effects%20and%20better%20confounder%20balancing%20results%0Aacross%20different%20group%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20908v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOSIC%253A%2520Model-Agnostic%2520Optimal%2520Subgroup%2520Identification%2520with%250A%2520%2520Multi-Constraint%2520for%2520Improved%2520Reliability%26entry.906535625%3DWenxin%2520Chen%2520and%2520Weishen%2520Pan%2520and%2520Kyra%2520Gan%2520and%2520Fei%2520Wang%26entry.1292438233%3D%2520%2520Identifying%2520subgroups%2520that%2520benefit%2520from%2520specific%2520treatments%2520using%250Aobservational%2520data%2520is%2520a%2520critical%2520challenge%2520in%2520personalized%2520medicine.%2520Most%250Aexisting%2520approaches%2520solely%2520focus%2520on%2520identifying%2520a%2520subgroup%2520with%2520an%2520improved%250Atreatment%2520effect.%2520However%252C%2520practical%2520considerations%252C%2520such%2520as%2520ensuring%2520a%2520minimum%250Asubgroup%2520size%2520for%2520representativeness%2520or%2520achieving%2520sufficient%2520confounder%2520balance%250Afor%2520reliability%252C%2520are%2520also%2520important%2520for%2520making%2520findings%2520clinically%2520meaningful%250Aand%2520actionable.%2520While%2520some%2520studies%2520address%2520these%2520constraints%2520individually%252C%2520none%250Aoffer%2520a%2520unified%2520approach%2520to%2520handle%2520them%2520simultaneously.%2520To%2520bridge%2520this%2520gap%252C%2520we%250Apropose%2520a%2520model-agnostic%2520framework%2520for%2520optimal%2520subgroup%2520identification%2520under%250Amultiple%2520constraints.%2520We%2520reformulate%2520this%2520combinatorial%2520problem%2520as%2520an%250Aunconstrained%2520min-max%2520optimization%2520problem%2520with%2520novel%2520modifications%2520and%2520solve%250Ait%2520by%2520a%2520gradient%2520descent%2520ascent%2520algorithm.%2520We%2520further%2520prove%2520its%2520convergence%2520to%250Aa%2520feasible%2520and%2520locally%2520optimal%2520solution.%2520Our%2520method%2520is%2520stable%2520and%2520highly%250Aflexible%252C%2520supporting%2520various%2520models%2520and%2520techniques%2520for%2520estimating%2520and%250Aoptimizing%2520treatment%2520effectiveness%2520with%2520observational%2520data.%2520Extensive%250Aexperiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520its%250Aeffectiveness%2520in%2520identifying%2520subgroups%2520that%2520satisfy%2520multiple%2520constraints%252C%250Aachieving%2520higher%2520treatment%2520effects%2520and%2520better%2520confounder%2520balancing%2520results%250Aacross%2520different%2520group%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20908v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOSIC%3A%20Model-Agnostic%20Optimal%20Subgroup%20Identification%20with%0A%20%20Multi-Constraint%20for%20Improved%20Reliability&entry.906535625=Wenxin%20Chen%20and%20Weishen%20Pan%20and%20Kyra%20Gan%20and%20Fei%20Wang&entry.1292438233=%20%20Identifying%20subgroups%20that%20benefit%20from%20specific%20treatments%20using%0Aobservational%20data%20is%20a%20critical%20challenge%20in%20personalized%20medicine.%20Most%0Aexisting%20approaches%20solely%20focus%20on%20identifying%20a%20subgroup%20with%20an%20improved%0Atreatment%20effect.%20However%2C%20practical%20considerations%2C%20such%20as%20ensuring%20a%20minimum%0Asubgroup%20size%20for%20representativeness%20or%20achieving%20sufficient%20confounder%20balance%0Afor%20reliability%2C%20are%20also%20important%20for%20making%20findings%20clinically%20meaningful%0Aand%20actionable.%20While%20some%20studies%20address%20these%20constraints%20individually%2C%20none%0Aoffer%20a%20unified%20approach%20to%20handle%20them%20simultaneously.%20To%20bridge%20this%20gap%2C%20we%0Apropose%20a%20model-agnostic%20framework%20for%20optimal%20subgroup%20identification%20under%0Amultiple%20constraints.%20We%20reformulate%20this%20combinatorial%20problem%20as%20an%0Aunconstrained%20min-max%20optimization%20problem%20with%20novel%20modifications%20and%20solve%0Ait%20by%20a%20gradient%20descent%20ascent%20algorithm.%20We%20further%20prove%20its%20convergence%20to%0Aa%20feasible%20and%20locally%20optimal%20solution.%20Our%20method%20is%20stable%20and%20highly%0Aflexible%2C%20supporting%20various%20models%20and%20techniques%20for%20estimating%20and%0Aoptimizing%20treatment%20effectiveness%20with%20observational%20data.%20Extensive%0Aexperiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20its%0Aeffectiveness%20in%20identifying%20subgroups%20that%20satisfy%20multiple%20constraints%2C%0Aachieving%20higher%20treatment%20effects%20and%20better%20confounder%20balancing%20results%0Aacross%20different%20group%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20908v1&entry.124074799=Read"},
{"title": "The Leaderboard Illusion", "author": "Shivalika Singh and Yiyang Nan and Alex Wang and Daniel D'Souza and Sayash Kapoor and Ahmet \u00dcst\u00fcn and Sanmi Koyejo and Yuntian Deng and Shayne Longpre and Noah Smith and Beyza Ermis and Marzieh Fadaee and Sara Hooker", "abstract": "  Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field\n", "link": "http://arxiv.org/abs/2504.20879v1", "date": "2025-04-29", "relevancy": 1.3123, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4509}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4378}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Leaderboard%20Illusion&body=Title%3A%20The%20Leaderboard%20Illusion%0AAuthor%3A%20Shivalika%20Singh%20and%20Yiyang%20Nan%20and%20Alex%20Wang%20and%20Daniel%20D%27Souza%20and%20Sayash%20Kapoor%20and%20Ahmet%20%C3%9Cst%C3%BCn%20and%20Sanmi%20Koyejo%20and%20Yuntian%20Deng%20and%20Shayne%20Longpre%20and%20Noah%20Smith%20and%20Beyza%20Ermis%20and%20Marzieh%20Fadaee%20and%20Sara%20Hooker%0AAbstract%3A%20%20%20Measuring%20progress%20is%20fundamental%20to%20the%20advancement%20of%20any%20scientific%20field.%0AAs%20benchmarks%20play%20an%20increasingly%20central%20role%2C%20they%20also%20grow%20more%0Asusceptible%20to%20distortion.%20Chatbot%20Arena%20has%20emerged%20as%20the%20go-to%20leaderboard%0Afor%20ranking%20the%20most%20capable%20AI%20systems.%20Yet%2C%20in%20this%20work%20we%20identify%0Asystematic%20issues%20that%20have%20resulted%20in%20a%20distorted%20playing%20field.%20We%20find%20that%0Aundisclosed%20private%20testing%20practices%20benefit%20a%20handful%20of%20providers%20who%20are%0Aable%20to%20test%20multiple%20variants%20before%20public%20release%20and%20retract%20scores%20if%0Adesired.%20We%20establish%20that%20the%20ability%20of%20these%20providers%20to%20choose%20the%20best%0Ascore%20leads%20to%20biased%20Arena%20scores%20due%20to%20selective%20disclosure%20of%20performance%0Aresults.%20At%20an%20extreme%2C%20we%20identify%2027%20private%20LLM%20variants%20tested%20by%20Meta%20in%0Athe%20lead-up%20to%20the%20Llama-4%20release.%20We%20also%20establish%20that%20proprietary%20closed%0Amodels%20are%20sampled%20at%20higher%20rates%20%28number%20of%20battles%29%20and%20have%20fewer%20models%0Aremoved%20from%20the%20arena%20than%20open-weight%20and%20open-source%20alternatives.%20Both%0Athese%20policies%20lead%20to%20large%20data%20access%20asymmetries%20over%20time.%20Providers%20like%0AGoogle%20and%20OpenAI%20have%20received%20an%20estimated%2019.2%25%20and%2020.4%25%20of%20all%20data%20on%20the%0Aarena%2C%20respectively.%20In%20contrast%2C%20a%20combined%2083%20open-weight%20models%20have%20only%0Areceived%20an%20estimated%2029.7%25%20of%20the%20total%20data.%20We%20show%20that%20access%20to%20Chatbot%0AArena%20data%20yields%20substantial%20benefits%3B%20even%20limited%20additional%20data%20can%20result%0Ain%20relative%20performance%20gains%20of%20up%20to%20112%25%20on%20the%20arena%20distribution%2C%20based%20on%0Aour%20conservative%20estimates.%20Together%2C%20these%20dynamics%20result%20in%20overfitting%20to%0AArena-specific%20dynamics%20rather%20than%20general%20model%20quality.%20The%20Arena%20builds%20on%0Athe%20substantial%20efforts%20of%20both%20the%20organizers%20and%20an%20open%20community%20that%0Amaintains%20this%20valuable%20evaluation%20platform.%20We%20offer%20actionable%0Arecommendations%20to%20reform%20the%20Chatbot%20Arena%27s%20evaluation%20framework%20and%20promote%0Afairer%2C%20more%20transparent%20benchmarking%20for%20the%20field%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Leaderboard%2520Illusion%26entry.906535625%3DShivalika%2520Singh%2520and%2520Yiyang%2520Nan%2520and%2520Alex%2520Wang%2520and%2520Daniel%2520D%2527Souza%2520and%2520Sayash%2520Kapoor%2520and%2520Ahmet%2520%25C3%259Cst%25C3%25BCn%2520and%2520Sanmi%2520Koyejo%2520and%2520Yuntian%2520Deng%2520and%2520Shayne%2520Longpre%2520and%2520Noah%2520Smith%2520and%2520Beyza%2520Ermis%2520and%2520Marzieh%2520Fadaee%2520and%2520Sara%2520Hooker%26entry.1292438233%3D%2520%2520Measuring%2520progress%2520is%2520fundamental%2520to%2520the%2520advancement%2520of%2520any%2520scientific%2520field.%250AAs%2520benchmarks%2520play%2520an%2520increasingly%2520central%2520role%252C%2520they%2520also%2520grow%2520more%250Asusceptible%2520to%2520distortion.%2520Chatbot%2520Arena%2520has%2520emerged%2520as%2520the%2520go-to%2520leaderboard%250Afor%2520ranking%2520the%2520most%2520capable%2520AI%2520systems.%2520Yet%252C%2520in%2520this%2520work%2520we%2520identify%250Asystematic%2520issues%2520that%2520have%2520resulted%2520in%2520a%2520distorted%2520playing%2520field.%2520We%2520find%2520that%250Aundisclosed%2520private%2520testing%2520practices%2520benefit%2520a%2520handful%2520of%2520providers%2520who%2520are%250Aable%2520to%2520test%2520multiple%2520variants%2520before%2520public%2520release%2520and%2520retract%2520scores%2520if%250Adesired.%2520We%2520establish%2520that%2520the%2520ability%2520of%2520these%2520providers%2520to%2520choose%2520the%2520best%250Ascore%2520leads%2520to%2520biased%2520Arena%2520scores%2520due%2520to%2520selective%2520disclosure%2520of%2520performance%250Aresults.%2520At%2520an%2520extreme%252C%2520we%2520identify%252027%2520private%2520LLM%2520variants%2520tested%2520by%2520Meta%2520in%250Athe%2520lead-up%2520to%2520the%2520Llama-4%2520release.%2520We%2520also%2520establish%2520that%2520proprietary%2520closed%250Amodels%2520are%2520sampled%2520at%2520higher%2520rates%2520%2528number%2520of%2520battles%2529%2520and%2520have%2520fewer%2520models%250Aremoved%2520from%2520the%2520arena%2520than%2520open-weight%2520and%2520open-source%2520alternatives.%2520Both%250Athese%2520policies%2520lead%2520to%2520large%2520data%2520access%2520asymmetries%2520over%2520time.%2520Providers%2520like%250AGoogle%2520and%2520OpenAI%2520have%2520received%2520an%2520estimated%252019.2%2525%2520and%252020.4%2525%2520of%2520all%2520data%2520on%2520the%250Aarena%252C%2520respectively.%2520In%2520contrast%252C%2520a%2520combined%252083%2520open-weight%2520models%2520have%2520only%250Areceived%2520an%2520estimated%252029.7%2525%2520of%2520the%2520total%2520data.%2520We%2520show%2520that%2520access%2520to%2520Chatbot%250AArena%2520data%2520yields%2520substantial%2520benefits%253B%2520even%2520limited%2520additional%2520data%2520can%2520result%250Ain%2520relative%2520performance%2520gains%2520of%2520up%2520to%2520112%2525%2520on%2520the%2520arena%2520distribution%252C%2520based%2520on%250Aour%2520conservative%2520estimates.%2520Together%252C%2520these%2520dynamics%2520result%2520in%2520overfitting%2520to%250AArena-specific%2520dynamics%2520rather%2520than%2520general%2520model%2520quality.%2520The%2520Arena%2520builds%2520on%250Athe%2520substantial%2520efforts%2520of%2520both%2520the%2520organizers%2520and%2520an%2520open%2520community%2520that%250Amaintains%2520this%2520valuable%2520evaluation%2520platform.%2520We%2520offer%2520actionable%250Arecommendations%2520to%2520reform%2520the%2520Chatbot%2520Arena%2527s%2520evaluation%2520framework%2520and%2520promote%250Afairer%252C%2520more%2520transparent%2520benchmarking%2520for%2520the%2520field%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Leaderboard%20Illusion&entry.906535625=Shivalika%20Singh%20and%20Yiyang%20Nan%20and%20Alex%20Wang%20and%20Daniel%20D%27Souza%20and%20Sayash%20Kapoor%20and%20Ahmet%20%C3%9Cst%C3%BCn%20and%20Sanmi%20Koyejo%20and%20Yuntian%20Deng%20and%20Shayne%20Longpre%20and%20Noah%20Smith%20and%20Beyza%20Ermis%20and%20Marzieh%20Fadaee%20and%20Sara%20Hooker&entry.1292438233=%20%20Measuring%20progress%20is%20fundamental%20to%20the%20advancement%20of%20any%20scientific%20field.%0AAs%20benchmarks%20play%20an%20increasingly%20central%20role%2C%20they%20also%20grow%20more%0Asusceptible%20to%20distortion.%20Chatbot%20Arena%20has%20emerged%20as%20the%20go-to%20leaderboard%0Afor%20ranking%20the%20most%20capable%20AI%20systems.%20Yet%2C%20in%20this%20work%20we%20identify%0Asystematic%20issues%20that%20have%20resulted%20in%20a%20distorted%20playing%20field.%20We%20find%20that%0Aundisclosed%20private%20testing%20practices%20benefit%20a%20handful%20of%20providers%20who%20are%0Aable%20to%20test%20multiple%20variants%20before%20public%20release%20and%20retract%20scores%20if%0Adesired.%20We%20establish%20that%20the%20ability%20of%20these%20providers%20to%20choose%20the%20best%0Ascore%20leads%20to%20biased%20Arena%20scores%20due%20to%20selective%20disclosure%20of%20performance%0Aresults.%20At%20an%20extreme%2C%20we%20identify%2027%20private%20LLM%20variants%20tested%20by%20Meta%20in%0Athe%20lead-up%20to%20the%20Llama-4%20release.%20We%20also%20establish%20that%20proprietary%20closed%0Amodels%20are%20sampled%20at%20higher%20rates%20%28number%20of%20battles%29%20and%20have%20fewer%20models%0Aremoved%20from%20the%20arena%20than%20open-weight%20and%20open-source%20alternatives.%20Both%0Athese%20policies%20lead%20to%20large%20data%20access%20asymmetries%20over%20time.%20Providers%20like%0AGoogle%20and%20OpenAI%20have%20received%20an%20estimated%2019.2%25%20and%2020.4%25%20of%20all%20data%20on%20the%0Aarena%2C%20respectively.%20In%20contrast%2C%20a%20combined%2083%20open-weight%20models%20have%20only%0Areceived%20an%20estimated%2029.7%25%20of%20the%20total%20data.%20We%20show%20that%20access%20to%20Chatbot%0AArena%20data%20yields%20substantial%20benefits%3B%20even%20limited%20additional%20data%20can%20result%0Ain%20relative%20performance%20gains%20of%20up%20to%20112%25%20on%20the%20arena%20distribution%2C%20based%20on%0Aour%20conservative%20estimates.%20Together%2C%20these%20dynamics%20result%20in%20overfitting%20to%0AArena-specific%20dynamics%20rather%20than%20general%20model%20quality.%20The%20Arena%20builds%20on%0Athe%20substantial%20efforts%20of%20both%20the%20organizers%20and%20an%20open%20community%20that%0Amaintains%20this%20valuable%20evaluation%20platform.%20We%20offer%20actionable%0Arecommendations%20to%20reform%20the%20Chatbot%20Arena%27s%20evaluation%20framework%20and%20promote%0Afairer%2C%20more%20transparent%20benchmarking%20for%20the%20field%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20879v1&entry.124074799=Read"},
{"title": "Does Feedback Help in Bandits with Arm Erasures?", "author": "Merve Karakas and Osama Hanna and Lin F. Yang and Christina Fragouli", "abstract": "  We study a distributed multi-armed bandit (MAB) problem over arm erasure\nchannels, motivated by the increasing adoption of MAB algorithms over\ncommunication-constrained networks. In this setup, the learner communicates the\nchosen arm to play to an agent over an erasure channel with probability\n$\\epsilon \\in [0,1)$; if an erasure occurs, the agent continues pulling the\nlast successfully received arm; the learner always observes the reward of the\narm pulled. In past work, we considered the case where the agent cannot convey\nfeedback to the learner, and thus the learner does not know whether the arm\nplayed is the requested or the last successfully received one. In this paper,\nwe instead consider the case where the agent can send feedback to the learner\non whether the arm request was received, and thus the learner exactly knows\nwhich arm was played. Surprisingly, we prove that erasure feedback does not\nimprove the worst-case regret upper bound order over the previously studied\nno-feedback setting. In particular, we prove a regret lower bound of\n$\\Omega(\\sqrt{KT} + K / (1 - \\epsilon))$, where $K$ is the number of arms and\n$T$ the time horizon, that matches no-feedback upper bounds up to logarithmic\nfactors. We note however that the availability of feedback enables simpler\nalgorithm designs that may achieve better constants (albeit not better order)\nregret bounds; we design one such algorithm and evaluate its performance\nnumerically.\n", "link": "http://arxiv.org/abs/2504.20894v1", "date": "2025-04-29", "relevancy": 1.1673, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3924}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3871}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Feedback%20Help%20in%20Bandits%20with%20Arm%20Erasures%3F&body=Title%3A%20Does%20Feedback%20Help%20in%20Bandits%20with%20Arm%20Erasures%3F%0AAuthor%3A%20Merve%20Karakas%20and%20Osama%20Hanna%20and%20Lin%20F.%20Yang%20and%20Christina%20Fragouli%0AAbstract%3A%20%20%20We%20study%20a%20distributed%20multi-armed%20bandit%20%28MAB%29%20problem%20over%20arm%20erasure%0Achannels%2C%20motivated%20by%20the%20increasing%20adoption%20of%20MAB%20algorithms%20over%0Acommunication-constrained%20networks.%20In%20this%20setup%2C%20the%20learner%20communicates%20the%0Achosen%20arm%20to%20play%20to%20an%20agent%20over%20an%20erasure%20channel%20with%20probability%0A%24%5Cepsilon%20%5Cin%20%5B0%2C1%29%24%3B%20if%20an%20erasure%20occurs%2C%20the%20agent%20continues%20pulling%20the%0Alast%20successfully%20received%20arm%3B%20the%20learner%20always%20observes%20the%20reward%20of%20the%0Aarm%20pulled.%20In%20past%20work%2C%20we%20considered%20the%20case%20where%20the%20agent%20cannot%20convey%0Afeedback%20to%20the%20learner%2C%20and%20thus%20the%20learner%20does%20not%20know%20whether%20the%20arm%0Aplayed%20is%20the%20requested%20or%20the%20last%20successfully%20received%20one.%20In%20this%20paper%2C%0Awe%20instead%20consider%20the%20case%20where%20the%20agent%20can%20send%20feedback%20to%20the%20learner%0Aon%20whether%20the%20arm%20request%20was%20received%2C%20and%20thus%20the%20learner%20exactly%20knows%0Awhich%20arm%20was%20played.%20Surprisingly%2C%20we%20prove%20that%20erasure%20feedback%20does%20not%0Aimprove%20the%20worst-case%20regret%20upper%20bound%20order%20over%20the%20previously%20studied%0Ano-feedback%20setting.%20In%20particular%2C%20we%20prove%20a%20regret%20lower%20bound%20of%0A%24%5COmega%28%5Csqrt%7BKT%7D%20%2B%20K%20/%20%281%20-%20%5Cepsilon%29%29%24%2C%20where%20%24K%24%20is%20the%20number%20of%20arms%20and%0A%24T%24%20the%20time%20horizon%2C%20that%20matches%20no-feedback%20upper%20bounds%20up%20to%20logarithmic%0Afactors.%20We%20note%20however%20that%20the%20availability%20of%20feedback%20enables%20simpler%0Aalgorithm%20designs%20that%20may%20achieve%20better%20constants%20%28albeit%20not%20better%20order%29%0Aregret%20bounds%3B%20we%20design%20one%20such%20algorithm%20and%20evaluate%20its%20performance%0Anumerically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Feedback%2520Help%2520in%2520Bandits%2520with%2520Arm%2520Erasures%253F%26entry.906535625%3DMerve%2520Karakas%2520and%2520Osama%2520Hanna%2520and%2520Lin%2520F.%2520Yang%2520and%2520Christina%2520Fragouli%26entry.1292438233%3D%2520%2520We%2520study%2520a%2520distributed%2520multi-armed%2520bandit%2520%2528MAB%2529%2520problem%2520over%2520arm%2520erasure%250Achannels%252C%2520motivated%2520by%2520the%2520increasing%2520adoption%2520of%2520MAB%2520algorithms%2520over%250Acommunication-constrained%2520networks.%2520In%2520this%2520setup%252C%2520the%2520learner%2520communicates%2520the%250Achosen%2520arm%2520to%2520play%2520to%2520an%2520agent%2520over%2520an%2520erasure%2520channel%2520with%2520probability%250A%2524%255Cepsilon%2520%255Cin%2520%255B0%252C1%2529%2524%253B%2520if%2520an%2520erasure%2520occurs%252C%2520the%2520agent%2520continues%2520pulling%2520the%250Alast%2520successfully%2520received%2520arm%253B%2520the%2520learner%2520always%2520observes%2520the%2520reward%2520of%2520the%250Aarm%2520pulled.%2520In%2520past%2520work%252C%2520we%2520considered%2520the%2520case%2520where%2520the%2520agent%2520cannot%2520convey%250Afeedback%2520to%2520the%2520learner%252C%2520and%2520thus%2520the%2520learner%2520does%2520not%2520know%2520whether%2520the%2520arm%250Aplayed%2520is%2520the%2520requested%2520or%2520the%2520last%2520successfully%2520received%2520one.%2520In%2520this%2520paper%252C%250Awe%2520instead%2520consider%2520the%2520case%2520where%2520the%2520agent%2520can%2520send%2520feedback%2520to%2520the%2520learner%250Aon%2520whether%2520the%2520arm%2520request%2520was%2520received%252C%2520and%2520thus%2520the%2520learner%2520exactly%2520knows%250Awhich%2520arm%2520was%2520played.%2520Surprisingly%252C%2520we%2520prove%2520that%2520erasure%2520feedback%2520does%2520not%250Aimprove%2520the%2520worst-case%2520regret%2520upper%2520bound%2520order%2520over%2520the%2520previously%2520studied%250Ano-feedback%2520setting.%2520In%2520particular%252C%2520we%2520prove%2520a%2520regret%2520lower%2520bound%2520of%250A%2524%255COmega%2528%255Csqrt%257BKT%257D%2520%252B%2520K%2520/%2520%25281%2520-%2520%255Cepsilon%2529%2529%2524%252C%2520where%2520%2524K%2524%2520is%2520the%2520number%2520of%2520arms%2520and%250A%2524T%2524%2520the%2520time%2520horizon%252C%2520that%2520matches%2520no-feedback%2520upper%2520bounds%2520up%2520to%2520logarithmic%250Afactors.%2520We%2520note%2520however%2520that%2520the%2520availability%2520of%2520feedback%2520enables%2520simpler%250Aalgorithm%2520designs%2520that%2520may%2520achieve%2520better%2520constants%2520%2528albeit%2520not%2520better%2520order%2529%250Aregret%2520bounds%253B%2520we%2520design%2520one%2520such%2520algorithm%2520and%2520evaluate%2520its%2520performance%250Anumerically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Feedback%20Help%20in%20Bandits%20with%20Arm%20Erasures%3F&entry.906535625=Merve%20Karakas%20and%20Osama%20Hanna%20and%20Lin%20F.%20Yang%20and%20Christina%20Fragouli&entry.1292438233=%20%20We%20study%20a%20distributed%20multi-armed%20bandit%20%28MAB%29%20problem%20over%20arm%20erasure%0Achannels%2C%20motivated%20by%20the%20increasing%20adoption%20of%20MAB%20algorithms%20over%0Acommunication-constrained%20networks.%20In%20this%20setup%2C%20the%20learner%20communicates%20the%0Achosen%20arm%20to%20play%20to%20an%20agent%20over%20an%20erasure%20channel%20with%20probability%0A%24%5Cepsilon%20%5Cin%20%5B0%2C1%29%24%3B%20if%20an%20erasure%20occurs%2C%20the%20agent%20continues%20pulling%20the%0Alast%20successfully%20received%20arm%3B%20the%20learner%20always%20observes%20the%20reward%20of%20the%0Aarm%20pulled.%20In%20past%20work%2C%20we%20considered%20the%20case%20where%20the%20agent%20cannot%20convey%0Afeedback%20to%20the%20learner%2C%20and%20thus%20the%20learner%20does%20not%20know%20whether%20the%20arm%0Aplayed%20is%20the%20requested%20or%20the%20last%20successfully%20received%20one.%20In%20this%20paper%2C%0Awe%20instead%20consider%20the%20case%20where%20the%20agent%20can%20send%20feedback%20to%20the%20learner%0Aon%20whether%20the%20arm%20request%20was%20received%2C%20and%20thus%20the%20learner%20exactly%20knows%0Awhich%20arm%20was%20played.%20Surprisingly%2C%20we%20prove%20that%20erasure%20feedback%20does%20not%0Aimprove%20the%20worst-case%20regret%20upper%20bound%20order%20over%20the%20previously%20studied%0Ano-feedback%20setting.%20In%20particular%2C%20we%20prove%20a%20regret%20lower%20bound%20of%0A%24%5COmega%28%5Csqrt%7BKT%7D%20%2B%20K%20/%20%281%20-%20%5Cepsilon%29%29%24%2C%20where%20%24K%24%20is%20the%20number%20of%20arms%20and%0A%24T%24%20the%20time%20horizon%2C%20that%20matches%20no-feedback%20upper%20bounds%20up%20to%20logarithmic%0Afactors.%20We%20note%20however%20that%20the%20availability%20of%20feedback%20enables%20simpler%0Aalgorithm%20designs%20that%20may%20achieve%20better%20constants%20%28albeit%20not%20better%20order%29%0Aregret%20bounds%3B%20we%20design%20one%20such%20algorithm%20and%20evaluate%20its%20performance%0Anumerically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20894v1&entry.124074799=Read"},
{"title": "ForesightNav: Learning Scene Imagination for Efficient Exploration", "author": "Hardik Shah and Jiaxu Xing and Nico Messikommer and Boyang Sun and Marc Pollefeys and Davide Scaramuzza", "abstract": "  Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration.\n", "link": "http://arxiv.org/abs/2504.16062v2", "date": "2025-04-29", "relevancy": 1.2321, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6609}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ForesightNav%3A%20Learning%20Scene%20Imagination%20for%20Efficient%20Exploration&body=Title%3A%20ForesightNav%3A%20Learning%20Scene%20Imagination%20for%20Efficient%20Exploration%0AAuthor%3A%20Hardik%20Shah%20and%20Jiaxu%20Xing%20and%20Nico%20Messikommer%20and%20Boyang%20Sun%20and%20Marc%20Pollefeys%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Understanding%20how%20humans%20leverage%20prior%20knowledge%20to%20navigate%20unseen%0Aenvironments%20while%20making%20exploratory%20decisions%20is%20essential%20for%20developing%0Aautonomous%20robots%20with%20similar%20abilities.%20In%20this%20work%2C%20we%20propose%0AForesightNav%2C%20a%20novel%20exploration%20strategy%20inspired%20by%20human%20imagination%20and%0Areasoning.%20Our%20approach%20equips%20robotic%20agents%20with%20the%20capability%20to%20predict%0Acontextual%20information%2C%20such%20as%20occupancy%20and%20semantic%20details%2C%20for%20unexplored%0Aregions.%20These%20predictions%20enable%20the%20robot%20to%20efficiently%20select%20meaningful%0Along-term%20navigation%20goals%2C%20significantly%20enhancing%20exploration%20in%20unseen%0Aenvironments.%20We%20validate%20our%20imagination-based%20approach%20using%20the%20Structured3D%0Adataset%2C%20demonstrating%20accurate%20occupancy%20prediction%20and%20superior%20performance%0Ain%20anticipating%20unseen%20scene%20geometry.%20Our%20experiments%20show%20that%20the%0Aimagination%20module%20improves%20exploration%20efficiency%20in%20unseen%20environments%2C%0Aachieving%20a%20100%25%20completion%20rate%20for%20PointNav%20and%20an%20SPL%20of%2067%25%20for%20ObjectNav%0Aon%20the%20Structured3D%20Validation%20split.%20These%20contributions%20demonstrate%20the%20power%0Aof%20imagination-driven%20reasoning%20for%20autonomous%20systems%20to%20enhance%20generalizable%0Aand%20efficient%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.16062v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForesightNav%253A%2520Learning%2520Scene%2520Imagination%2520for%2520Efficient%2520Exploration%26entry.906535625%3DHardik%2520Shah%2520and%2520Jiaxu%2520Xing%2520and%2520Nico%2520Messikommer%2520and%2520Boyang%2520Sun%2520and%2520Marc%2520Pollefeys%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3D%2520%2520Understanding%2520how%2520humans%2520leverage%2520prior%2520knowledge%2520to%2520navigate%2520unseen%250Aenvironments%2520while%2520making%2520exploratory%2520decisions%2520is%2520essential%2520for%2520developing%250Aautonomous%2520robots%2520with%2520similar%2520abilities.%2520In%2520this%2520work%252C%2520we%2520propose%250AForesightNav%252C%2520a%2520novel%2520exploration%2520strategy%2520inspired%2520by%2520human%2520imagination%2520and%250Areasoning.%2520Our%2520approach%2520equips%2520robotic%2520agents%2520with%2520the%2520capability%2520to%2520predict%250Acontextual%2520information%252C%2520such%2520as%2520occupancy%2520and%2520semantic%2520details%252C%2520for%2520unexplored%250Aregions.%2520These%2520predictions%2520enable%2520the%2520robot%2520to%2520efficiently%2520select%2520meaningful%250Along-term%2520navigation%2520goals%252C%2520significantly%2520enhancing%2520exploration%2520in%2520unseen%250Aenvironments.%2520We%2520validate%2520our%2520imagination-based%2520approach%2520using%2520the%2520Structured3D%250Adataset%252C%2520demonstrating%2520accurate%2520occupancy%2520prediction%2520and%2520superior%2520performance%250Ain%2520anticipating%2520unseen%2520scene%2520geometry.%2520Our%2520experiments%2520show%2520that%2520the%250Aimagination%2520module%2520improves%2520exploration%2520efficiency%2520in%2520unseen%2520environments%252C%250Aachieving%2520a%2520100%2525%2520completion%2520rate%2520for%2520PointNav%2520and%2520an%2520SPL%2520of%252067%2525%2520for%2520ObjectNav%250Aon%2520the%2520Structured3D%2520Validation%2520split.%2520These%2520contributions%2520demonstrate%2520the%2520power%250Aof%2520imagination-driven%2520reasoning%2520for%2520autonomous%2520systems%2520to%2520enhance%2520generalizable%250Aand%2520efficient%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.16062v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ForesightNav%3A%20Learning%20Scene%20Imagination%20for%20Efficient%20Exploration&entry.906535625=Hardik%20Shah%20and%20Jiaxu%20Xing%20and%20Nico%20Messikommer%20and%20Boyang%20Sun%20and%20Marc%20Pollefeys%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Understanding%20how%20humans%20leverage%20prior%20knowledge%20to%20navigate%20unseen%0Aenvironments%20while%20making%20exploratory%20decisions%20is%20essential%20for%20developing%0Aautonomous%20robots%20with%20similar%20abilities.%20In%20this%20work%2C%20we%20propose%0AForesightNav%2C%20a%20novel%20exploration%20strategy%20inspired%20by%20human%20imagination%20and%0Areasoning.%20Our%20approach%20equips%20robotic%20agents%20with%20the%20capability%20to%20predict%0Acontextual%20information%2C%20such%20as%20occupancy%20and%20semantic%20details%2C%20for%20unexplored%0Aregions.%20These%20predictions%20enable%20the%20robot%20to%20efficiently%20select%20meaningful%0Along-term%20navigation%20goals%2C%20significantly%20enhancing%20exploration%20in%20unseen%0Aenvironments.%20We%20validate%20our%20imagination-based%20approach%20using%20the%20Structured3D%0Adataset%2C%20demonstrating%20accurate%20occupancy%20prediction%20and%20superior%20performance%0Ain%20anticipating%20unseen%20scene%20geometry.%20Our%20experiments%20show%20that%20the%0Aimagination%20module%20improves%20exploration%20efficiency%20in%20unseen%20environments%2C%0Aachieving%20a%20100%25%20completion%20rate%20for%20PointNav%20and%20an%20SPL%20of%2067%25%20for%20ObjectNav%0Aon%20the%20Structured3D%20Validation%20split.%20These%20contributions%20demonstrate%20the%20power%0Aof%20imagination-driven%20reasoning%20for%20autonomous%20systems%20to%20enhance%20generalizable%0Aand%20efficient%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.16062v2&entry.124074799=Read"},
{"title": "CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report\n  Generation with Multi-Agent RAG and Concept Bottleneck Models", "author": "Hasan Md Tusfiqur Alam and Devansh Srivastav and Abdulrahman Mohamed Selim and Md Abdul Kadir and Md Moktadiurl Hoque Shuvo and Daniel Sonntag", "abstract": "  Advancements in generative Artificial Intelligence (AI) hold great promise\nfor automating radiology workflows, yet challenges in interpretability and\nreliability hinder clinical adoption. This paper presents an automated\nradiology report generation framework that combines Concept Bottleneck Models\n(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge\nAI performance with clinical explainability. CBMs map chest X-ray features to\nhuman-understandable clinical concepts, enabling transparent disease\nclassification. Meanwhile, the RAG system integrates multi-agent collaboration\nand external knowledge to produce contextually rich, evidence-based reports.\nOur demonstration showcases the system's ability to deliver interpretable\npredictions, mitigate hallucinations, and generate high-quality, tailored\nreports with an interactive interface addressing accuracy, trust, and usability\nchallenges. This framework provides a pathway to improving diagnostic\nconsistency and empowering radiologists with actionable insights.\n", "link": "http://arxiv.org/abs/2504.20898v1", "date": "2025-04-29", "relevancy": 1.0112, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5151}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5019}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CBM-RAG%3A%20Demonstrating%20Enhanced%20Interpretability%20in%20Radiology%20Report%0A%20%20Generation%20with%20Multi-Agent%20RAG%20and%20Concept%20Bottleneck%20Models&body=Title%3A%20CBM-RAG%3A%20Demonstrating%20Enhanced%20Interpretability%20in%20Radiology%20Report%0A%20%20Generation%20with%20Multi-Agent%20RAG%20and%20Concept%20Bottleneck%20Models%0AAuthor%3A%20Hasan%20Md%20Tusfiqur%20Alam%20and%20Devansh%20Srivastav%20and%20Abdulrahman%20Mohamed%20Selim%20and%20Md%20Abdul%20Kadir%20and%20Md%20Moktadiurl%20Hoque%20Shuvo%20and%20Daniel%20Sonntag%0AAbstract%3A%20%20%20Advancements%20in%20generative%20Artificial%20Intelligence%20%28AI%29%20hold%20great%20promise%0Afor%20automating%20radiology%20workflows%2C%20yet%20challenges%20in%20interpretability%20and%0Areliability%20hinder%20clinical%20adoption.%20This%20paper%20presents%20an%20automated%0Aradiology%20report%20generation%20framework%20that%20combines%20Concept%20Bottleneck%20Models%0A%28CBMs%29%20with%20a%20Multi-Agent%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%20to%20bridge%0AAI%20performance%20with%20clinical%20explainability.%20CBMs%20map%20chest%20X-ray%20features%20to%0Ahuman-understandable%20clinical%20concepts%2C%20enabling%20transparent%20disease%0Aclassification.%20Meanwhile%2C%20the%20RAG%20system%20integrates%20multi-agent%20collaboration%0Aand%20external%20knowledge%20to%20produce%20contextually%20rich%2C%20evidence-based%20reports.%0AOur%20demonstration%20showcases%20the%20system%27s%20ability%20to%20deliver%20interpretable%0Apredictions%2C%20mitigate%20hallucinations%2C%20and%20generate%20high-quality%2C%20tailored%0Areports%20with%20an%20interactive%20interface%20addressing%20accuracy%2C%20trust%2C%20and%20usability%0Achallenges.%20This%20framework%20provides%20a%20pathway%20to%20improving%20diagnostic%0Aconsistency%20and%20empowering%20radiologists%20with%20actionable%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCBM-RAG%253A%2520Demonstrating%2520Enhanced%2520Interpretability%2520in%2520Radiology%2520Report%250A%2520%2520Generation%2520with%2520Multi-Agent%2520RAG%2520and%2520Concept%2520Bottleneck%2520Models%26entry.906535625%3DHasan%2520Md%2520Tusfiqur%2520Alam%2520and%2520Devansh%2520Srivastav%2520and%2520Abdulrahman%2520Mohamed%2520Selim%2520and%2520Md%2520Abdul%2520Kadir%2520and%2520Md%2520Moktadiurl%2520Hoque%2520Shuvo%2520and%2520Daniel%2520Sonntag%26entry.1292438233%3D%2520%2520Advancements%2520in%2520generative%2520Artificial%2520Intelligence%2520%2528AI%2529%2520hold%2520great%2520promise%250Afor%2520automating%2520radiology%2520workflows%252C%2520yet%2520challenges%2520in%2520interpretability%2520and%250Areliability%2520hinder%2520clinical%2520adoption.%2520This%2520paper%2520presents%2520an%2520automated%250Aradiology%2520report%2520generation%2520framework%2520that%2520combines%2520Concept%2520Bottleneck%2520Models%250A%2528CBMs%2529%2520with%2520a%2520Multi-Agent%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520system%2520to%2520bridge%250AAI%2520performance%2520with%2520clinical%2520explainability.%2520CBMs%2520map%2520chest%2520X-ray%2520features%2520to%250Ahuman-understandable%2520clinical%2520concepts%252C%2520enabling%2520transparent%2520disease%250Aclassification.%2520Meanwhile%252C%2520the%2520RAG%2520system%2520integrates%2520multi-agent%2520collaboration%250Aand%2520external%2520knowledge%2520to%2520produce%2520contextually%2520rich%252C%2520evidence-based%2520reports.%250AOur%2520demonstration%2520showcases%2520the%2520system%2527s%2520ability%2520to%2520deliver%2520interpretable%250Apredictions%252C%2520mitigate%2520hallucinations%252C%2520and%2520generate%2520high-quality%252C%2520tailored%250Areports%2520with%2520an%2520interactive%2520interface%2520addressing%2520accuracy%252C%2520trust%252C%2520and%2520usability%250Achallenges.%2520This%2520framework%2520provides%2520a%2520pathway%2520to%2520improving%2520diagnostic%250Aconsistency%2520and%2520empowering%2520radiologists%2520with%2520actionable%2520insights.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CBM-RAG%3A%20Demonstrating%20Enhanced%20Interpretability%20in%20Radiology%20Report%0A%20%20Generation%20with%20Multi-Agent%20RAG%20and%20Concept%20Bottleneck%20Models&entry.906535625=Hasan%20Md%20Tusfiqur%20Alam%20and%20Devansh%20Srivastav%20and%20Abdulrahman%20Mohamed%20Selim%20and%20Md%20Abdul%20Kadir%20and%20Md%20Moktadiurl%20Hoque%20Shuvo%20and%20Daniel%20Sonntag&entry.1292438233=%20%20Advancements%20in%20generative%20Artificial%20Intelligence%20%28AI%29%20hold%20great%20promise%0Afor%20automating%20radiology%20workflows%2C%20yet%20challenges%20in%20interpretability%20and%0Areliability%20hinder%20clinical%20adoption.%20This%20paper%20presents%20an%20automated%0Aradiology%20report%20generation%20framework%20that%20combines%20Concept%20Bottleneck%20Models%0A%28CBMs%29%20with%20a%20Multi-Agent%20Retrieval-Augmented%20Generation%20%28RAG%29%20system%20to%20bridge%0AAI%20performance%20with%20clinical%20explainability.%20CBMs%20map%20chest%20X-ray%20features%20to%0Ahuman-understandable%20clinical%20concepts%2C%20enabling%20transparent%20disease%0Aclassification.%20Meanwhile%2C%20the%20RAG%20system%20integrates%20multi-agent%20collaboration%0Aand%20external%20knowledge%20to%20produce%20contextually%20rich%2C%20evidence-based%20reports.%0AOur%20demonstration%20showcases%20the%20system%27s%20ability%20to%20deliver%20interpretable%0Apredictions%2C%20mitigate%20hallucinations%2C%20and%20generate%20high-quality%2C%20tailored%0Areports%20with%20an%20interactive%20interface%20addressing%20accuracy%2C%20trust%2C%20and%20usability%0Achallenges.%20This%20framework%20provides%20a%20pathway%20to%20improving%20diagnostic%0Aconsistency%20and%20empowering%20radiologists%20with%20actionable%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20898v1&entry.124074799=Read"},
{"title": "When Testing AI Tests Us: Safeguarding Mental Health on the Digital\n  Frontlines", "author": "Sachin R. Pendse and Darren Gergle and Rachel Kornfield and Jonah Meyerhoff and David Mohr and Jina Suh and Annie Wescott and Casey Williams and Jessica Schleider", "abstract": "  Red-teaming is a core part of the infrastructure that ensures that AI models\ndo not produce harmful content. Unlike past technologies, the black box nature\nof generative AI systems necessitates a uniquely interactional mode of testing,\none in which individuals on red teams actively interact with the system,\nleveraging natural language to simulate malicious actors and solicit harmful\noutputs. This interactional labor done by red teams can result in mental health\nharms that are uniquely tied to the adversarial engagement strategies necessary\nto effectively red team. The importance of ensuring that generative AI models\ndo not propagate societal or individual harm is widely recognized -- one less\nvisible foundation of end-to-end AI safety is also the protection of the mental\nhealth and wellbeing of those who work to keep model outputs safe. In this\npaper, we argue that the unmet mental health needs of AI red-teamers is a\ncritical workplace safety concern. Through analyzing the unique mental health\nimpacts associated with the labor done by red teams, we propose potential\nindividual and organizational strategies that could be used to meet these\nneeds, and safeguard the mental health of red-teamers. We develop our proposed\nstrategies through drawing parallels between common red-teaming practices and\ninteractional labor common to other professions (including actors, mental\nhealth professionals, conflict photographers, and content moderators),\ndescribing how individuals and organizations within these professional spaces\nsafeguard their mental health given similar psychological demands. Drawing on\nthese protective practices, we describe how safeguards could be adapted for the\ndistinct mental health challenges experienced by red teaming organizations as\nthey mitigate emerging technological risks on the new digital frontlines.\n", "link": "http://arxiv.org/abs/2504.20910v1", "date": "2025-04-29", "relevancy": 1.2461, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4189}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4161}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Testing%20AI%20Tests%20Us%3A%20Safeguarding%20Mental%20Health%20on%20the%20Digital%0A%20%20Frontlines&body=Title%3A%20When%20Testing%20AI%20Tests%20Us%3A%20Safeguarding%20Mental%20Health%20on%20the%20Digital%0A%20%20Frontlines%0AAuthor%3A%20Sachin%20R.%20Pendse%20and%20Darren%20Gergle%20and%20Rachel%20Kornfield%20and%20Jonah%20Meyerhoff%20and%20David%20Mohr%20and%20Jina%20Suh%20and%20Annie%20Wescott%20and%20Casey%20Williams%20and%20Jessica%20Schleider%0AAbstract%3A%20%20%20Red-teaming%20is%20a%20core%20part%20of%20the%20infrastructure%20that%20ensures%20that%20AI%20models%0Ado%20not%20produce%20harmful%20content.%20Unlike%20past%20technologies%2C%20the%20black%20box%20nature%0Aof%20generative%20AI%20systems%20necessitates%20a%20uniquely%20interactional%20mode%20of%20testing%2C%0Aone%20in%20which%20individuals%20on%20red%20teams%20actively%20interact%20with%20the%20system%2C%0Aleveraging%20natural%20language%20to%20simulate%20malicious%20actors%20and%20solicit%20harmful%0Aoutputs.%20This%20interactional%20labor%20done%20by%20red%20teams%20can%20result%20in%20mental%20health%0Aharms%20that%20are%20uniquely%20tied%20to%20the%20adversarial%20engagement%20strategies%20necessary%0Ato%20effectively%20red%20team.%20The%20importance%20of%20ensuring%20that%20generative%20AI%20models%0Ado%20not%20propagate%20societal%20or%20individual%20harm%20is%20widely%20recognized%20--%20one%20less%0Avisible%20foundation%20of%20end-to-end%20AI%20safety%20is%20also%20the%20protection%20of%20the%20mental%0Ahealth%20and%20wellbeing%20of%20those%20who%20work%20to%20keep%20model%20outputs%20safe.%20In%20this%0Apaper%2C%20we%20argue%20that%20the%20unmet%20mental%20health%20needs%20of%20AI%20red-teamers%20is%20a%0Acritical%20workplace%20safety%20concern.%20Through%20analyzing%20the%20unique%20mental%20health%0Aimpacts%20associated%20with%20the%20labor%20done%20by%20red%20teams%2C%20we%20propose%20potential%0Aindividual%20and%20organizational%20strategies%20that%20could%20be%20used%20to%20meet%20these%0Aneeds%2C%20and%20safeguard%20the%20mental%20health%20of%20red-teamers.%20We%20develop%20our%20proposed%0Astrategies%20through%20drawing%20parallels%20between%20common%20red-teaming%20practices%20and%0Ainteractional%20labor%20common%20to%20other%20professions%20%28including%20actors%2C%20mental%0Ahealth%20professionals%2C%20conflict%20photographers%2C%20and%20content%20moderators%29%2C%0Adescribing%20how%20individuals%20and%20organizations%20within%20these%20professional%20spaces%0Asafeguard%20their%20mental%20health%20given%20similar%20psychological%20demands.%20Drawing%20on%0Athese%20protective%20practices%2C%20we%20describe%20how%20safeguards%20could%20be%20adapted%20for%20the%0Adistinct%20mental%20health%20challenges%20experienced%20by%20red%20teaming%20organizations%20as%0Athey%20mitigate%20emerging%20technological%20risks%20on%20the%20new%20digital%20frontlines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Testing%2520AI%2520Tests%2520Us%253A%2520Safeguarding%2520Mental%2520Health%2520on%2520the%2520Digital%250A%2520%2520Frontlines%26entry.906535625%3DSachin%2520R.%2520Pendse%2520and%2520Darren%2520Gergle%2520and%2520Rachel%2520Kornfield%2520and%2520Jonah%2520Meyerhoff%2520and%2520David%2520Mohr%2520and%2520Jina%2520Suh%2520and%2520Annie%2520Wescott%2520and%2520Casey%2520Williams%2520and%2520Jessica%2520Schleider%26entry.1292438233%3D%2520%2520Red-teaming%2520is%2520a%2520core%2520part%2520of%2520the%2520infrastructure%2520that%2520ensures%2520that%2520AI%2520models%250Ado%2520not%2520produce%2520harmful%2520content.%2520Unlike%2520past%2520technologies%252C%2520the%2520black%2520box%2520nature%250Aof%2520generative%2520AI%2520systems%2520necessitates%2520a%2520uniquely%2520interactional%2520mode%2520of%2520testing%252C%250Aone%2520in%2520which%2520individuals%2520on%2520red%2520teams%2520actively%2520interact%2520with%2520the%2520system%252C%250Aleveraging%2520natural%2520language%2520to%2520simulate%2520malicious%2520actors%2520and%2520solicit%2520harmful%250Aoutputs.%2520This%2520interactional%2520labor%2520done%2520by%2520red%2520teams%2520can%2520result%2520in%2520mental%2520health%250Aharms%2520that%2520are%2520uniquely%2520tied%2520to%2520the%2520adversarial%2520engagement%2520strategies%2520necessary%250Ato%2520effectively%2520red%2520team.%2520The%2520importance%2520of%2520ensuring%2520that%2520generative%2520AI%2520models%250Ado%2520not%2520propagate%2520societal%2520or%2520individual%2520harm%2520is%2520widely%2520recognized%2520--%2520one%2520less%250Avisible%2520foundation%2520of%2520end-to-end%2520AI%2520safety%2520is%2520also%2520the%2520protection%2520of%2520the%2520mental%250Ahealth%2520and%2520wellbeing%2520of%2520those%2520who%2520work%2520to%2520keep%2520model%2520outputs%2520safe.%2520In%2520this%250Apaper%252C%2520we%2520argue%2520that%2520the%2520unmet%2520mental%2520health%2520needs%2520of%2520AI%2520red-teamers%2520is%2520a%250Acritical%2520workplace%2520safety%2520concern.%2520Through%2520analyzing%2520the%2520unique%2520mental%2520health%250Aimpacts%2520associated%2520with%2520the%2520labor%2520done%2520by%2520red%2520teams%252C%2520we%2520propose%2520potential%250Aindividual%2520and%2520organizational%2520strategies%2520that%2520could%2520be%2520used%2520to%2520meet%2520these%250Aneeds%252C%2520and%2520safeguard%2520the%2520mental%2520health%2520of%2520red-teamers.%2520We%2520develop%2520our%2520proposed%250Astrategies%2520through%2520drawing%2520parallels%2520between%2520common%2520red-teaming%2520practices%2520and%250Ainteractional%2520labor%2520common%2520to%2520other%2520professions%2520%2528including%2520actors%252C%2520mental%250Ahealth%2520professionals%252C%2520conflict%2520photographers%252C%2520and%2520content%2520moderators%2529%252C%250Adescribing%2520how%2520individuals%2520and%2520organizations%2520within%2520these%2520professional%2520spaces%250Asafeguard%2520their%2520mental%2520health%2520given%2520similar%2520psychological%2520demands.%2520Drawing%2520on%250Athese%2520protective%2520practices%252C%2520we%2520describe%2520how%2520safeguards%2520could%2520be%2520adapted%2520for%2520the%250Adistinct%2520mental%2520health%2520challenges%2520experienced%2520by%2520red%2520teaming%2520organizations%2520as%250Athey%2520mitigate%2520emerging%2520technological%2520risks%2520on%2520the%2520new%2520digital%2520frontlines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Testing%20AI%20Tests%20Us%3A%20Safeguarding%20Mental%20Health%20on%20the%20Digital%0A%20%20Frontlines&entry.906535625=Sachin%20R.%20Pendse%20and%20Darren%20Gergle%20and%20Rachel%20Kornfield%20and%20Jonah%20Meyerhoff%20and%20David%20Mohr%20and%20Jina%20Suh%20and%20Annie%20Wescott%20and%20Casey%20Williams%20and%20Jessica%20Schleider&entry.1292438233=%20%20Red-teaming%20is%20a%20core%20part%20of%20the%20infrastructure%20that%20ensures%20that%20AI%20models%0Ado%20not%20produce%20harmful%20content.%20Unlike%20past%20technologies%2C%20the%20black%20box%20nature%0Aof%20generative%20AI%20systems%20necessitates%20a%20uniquely%20interactional%20mode%20of%20testing%2C%0Aone%20in%20which%20individuals%20on%20red%20teams%20actively%20interact%20with%20the%20system%2C%0Aleveraging%20natural%20language%20to%20simulate%20malicious%20actors%20and%20solicit%20harmful%0Aoutputs.%20This%20interactional%20labor%20done%20by%20red%20teams%20can%20result%20in%20mental%20health%0Aharms%20that%20are%20uniquely%20tied%20to%20the%20adversarial%20engagement%20strategies%20necessary%0Ato%20effectively%20red%20team.%20The%20importance%20of%20ensuring%20that%20generative%20AI%20models%0Ado%20not%20propagate%20societal%20or%20individual%20harm%20is%20widely%20recognized%20--%20one%20less%0Avisible%20foundation%20of%20end-to-end%20AI%20safety%20is%20also%20the%20protection%20of%20the%20mental%0Ahealth%20and%20wellbeing%20of%20those%20who%20work%20to%20keep%20model%20outputs%20safe.%20In%20this%0Apaper%2C%20we%20argue%20that%20the%20unmet%20mental%20health%20needs%20of%20AI%20red-teamers%20is%20a%0Acritical%20workplace%20safety%20concern.%20Through%20analyzing%20the%20unique%20mental%20health%0Aimpacts%20associated%20with%20the%20labor%20done%20by%20red%20teams%2C%20we%20propose%20potential%0Aindividual%20and%20organizational%20strategies%20that%20could%20be%20used%20to%20meet%20these%0Aneeds%2C%20and%20safeguard%20the%20mental%20health%20of%20red-teamers.%20We%20develop%20our%20proposed%0Astrategies%20through%20drawing%20parallels%20between%20common%20red-teaming%20practices%20and%0Ainteractional%20labor%20common%20to%20other%20professions%20%28including%20actors%2C%20mental%0Ahealth%20professionals%2C%20conflict%20photographers%2C%20and%20content%20moderators%29%2C%0Adescribing%20how%20individuals%20and%20organizations%20within%20these%20professional%20spaces%0Asafeguard%20their%20mental%20health%20given%20similar%20psychological%20demands.%20Drawing%20on%0Athese%20protective%20practices%2C%20we%20describe%20how%20safeguards%20could%20be%20adapted%20for%20the%0Adistinct%20mental%20health%20challenges%20experienced%20by%20red%20teaming%20organizations%20as%0Athey%20mitigate%20emerging%20technological%20risks%20on%20the%20new%20digital%20frontlines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20910v1&entry.124074799=Read"},
{"title": "GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In\n  Industrial Control Systems", "author": "Sarad Venugopalan and Sridhar Adepu", "abstract": "  The continuous monitoring of the interactions between cyber-physical\ncomponents of any industrial control system (ICS) is required to secure\nautomation of the system controls, and to guarantee plant processes are\nfail-safe and remain in an acceptably safe state. Safety is achieved by\nmanaging actuation (where electric signals are used to trigger physical\nmovement), dependent on corresponding sensor readings; used as ground truth in\ndecision making. Timely detection of anomalies (attacks, faults and\nunascertained states) in ICSs is crucial for the safe running of a plant, the\nsafety of its personnel, and for the safe provision of any services provided.\nWe propose an anomaly detection method that involves accurate linearization of\nthe non-linear forms arising from sensor-actuator(s) relationships, primarily\nbecause solving linear models is easier and well understood. Further, the time\ncomplexity of the anomaly detection scenario/problem at hand is lowered using\ndimensionality reduction of the actuator(s) in relationship with a sensor. We\naccomplish this by using a well-known water treatment testbed as a use case.\nOur experiments show millisecond time response to detect anomalies and provide\nexplainability; that are not simultaneously achieved by other state of the art\nAI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we\npin-point the sensor(s) and its actuation state for which anomaly was detected.\n", "link": "http://arxiv.org/abs/2504.20906v1", "date": "2025-04-29", "relevancy": 1.0102, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5363}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5068}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GiBy%3A%20A%20Giant-Step%20Baby-Step%20Classifier%20For%20Anomaly%20Detection%20In%0A%20%20Industrial%20Control%20Systems&body=Title%3A%20GiBy%3A%20A%20Giant-Step%20Baby-Step%20Classifier%20For%20Anomaly%20Detection%20In%0A%20%20Industrial%20Control%20Systems%0AAuthor%3A%20Sarad%20Venugopalan%20and%20Sridhar%20Adepu%0AAbstract%3A%20%20%20The%20continuous%20monitoring%20of%20the%20interactions%20between%20cyber-physical%0Acomponents%20of%20any%20industrial%20control%20system%20%28ICS%29%20is%20required%20to%20secure%0Aautomation%20of%20the%20system%20controls%2C%20and%20to%20guarantee%20plant%20processes%20are%0Afail-safe%20and%20remain%20in%20an%20acceptably%20safe%20state.%20Safety%20is%20achieved%20by%0Amanaging%20actuation%20%28where%20electric%20signals%20are%20used%20to%20trigger%20physical%0Amovement%29%2C%20dependent%20on%20corresponding%20sensor%20readings%3B%20used%20as%20ground%20truth%20in%0Adecision%20making.%20Timely%20detection%20of%20anomalies%20%28attacks%2C%20faults%20and%0Aunascertained%20states%29%20in%20ICSs%20is%20crucial%20for%20the%20safe%20running%20of%20a%20plant%2C%20the%0Asafety%20of%20its%20personnel%2C%20and%20for%20the%20safe%20provision%20of%20any%20services%20provided.%0AWe%20propose%20an%20anomaly%20detection%20method%20that%20involves%20accurate%20linearization%20of%0Athe%20non-linear%20forms%20arising%20from%20sensor-actuator%28s%29%20relationships%2C%20primarily%0Abecause%20solving%20linear%20models%20is%20easier%20and%20well%20understood.%20Further%2C%20the%20time%0Acomplexity%20of%20the%20anomaly%20detection%20scenario/problem%20at%20hand%20is%20lowered%20using%0Adimensionality%20reduction%20of%20the%20actuator%28s%29%20in%20relationship%20with%20a%20sensor.%20We%0Aaccomplish%20this%20by%20using%20a%20well-known%20water%20treatment%20testbed%20as%20a%20use%20case.%0AOur%20experiments%20show%20millisecond%20time%20response%20to%20detect%20anomalies%20and%20provide%0Aexplainability%3B%20that%20are%20not%20simultaneously%20achieved%20by%20other%20state%20of%20the%20art%0AAI/ML%20models%20with%20eXplainable%20AI%20%28XAI%29%20used%20for%20the%20same%20purpose.%20Further%2C%20we%0Apin-point%20the%20sensor%28s%29%20and%20its%20actuation%20state%20for%20which%20anomaly%20was%20detected.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGiBy%253A%2520A%2520Giant-Step%2520Baby-Step%2520Classifier%2520For%2520Anomaly%2520Detection%2520In%250A%2520%2520Industrial%2520Control%2520Systems%26entry.906535625%3DSarad%2520Venugopalan%2520and%2520Sridhar%2520Adepu%26entry.1292438233%3D%2520%2520The%2520continuous%2520monitoring%2520of%2520the%2520interactions%2520between%2520cyber-physical%250Acomponents%2520of%2520any%2520industrial%2520control%2520system%2520%2528ICS%2529%2520is%2520required%2520to%2520secure%250Aautomation%2520of%2520the%2520system%2520controls%252C%2520and%2520to%2520guarantee%2520plant%2520processes%2520are%250Afail-safe%2520and%2520remain%2520in%2520an%2520acceptably%2520safe%2520state.%2520Safety%2520is%2520achieved%2520by%250Amanaging%2520actuation%2520%2528where%2520electric%2520signals%2520are%2520used%2520to%2520trigger%2520physical%250Amovement%2529%252C%2520dependent%2520on%2520corresponding%2520sensor%2520readings%253B%2520used%2520as%2520ground%2520truth%2520in%250Adecision%2520making.%2520Timely%2520detection%2520of%2520anomalies%2520%2528attacks%252C%2520faults%2520and%250Aunascertained%2520states%2529%2520in%2520ICSs%2520is%2520crucial%2520for%2520the%2520safe%2520running%2520of%2520a%2520plant%252C%2520the%250Asafety%2520of%2520its%2520personnel%252C%2520and%2520for%2520the%2520safe%2520provision%2520of%2520any%2520services%2520provided.%250AWe%2520propose%2520an%2520anomaly%2520detection%2520method%2520that%2520involves%2520accurate%2520linearization%2520of%250Athe%2520non-linear%2520forms%2520arising%2520from%2520sensor-actuator%2528s%2529%2520relationships%252C%2520primarily%250Abecause%2520solving%2520linear%2520models%2520is%2520easier%2520and%2520well%2520understood.%2520Further%252C%2520the%2520time%250Acomplexity%2520of%2520the%2520anomaly%2520detection%2520scenario/problem%2520at%2520hand%2520is%2520lowered%2520using%250Adimensionality%2520reduction%2520of%2520the%2520actuator%2528s%2529%2520in%2520relationship%2520with%2520a%2520sensor.%2520We%250Aaccomplish%2520this%2520by%2520using%2520a%2520well-known%2520water%2520treatment%2520testbed%2520as%2520a%2520use%2520case.%250AOur%2520experiments%2520show%2520millisecond%2520time%2520response%2520to%2520detect%2520anomalies%2520and%2520provide%250Aexplainability%253B%2520that%2520are%2520not%2520simultaneously%2520achieved%2520by%2520other%2520state%2520of%2520the%2520art%250AAI/ML%2520models%2520with%2520eXplainable%2520AI%2520%2528XAI%2529%2520used%2520for%2520the%2520same%2520purpose.%2520Further%252C%2520we%250Apin-point%2520the%2520sensor%2528s%2529%2520and%2520its%2520actuation%2520state%2520for%2520which%2520anomaly%2520was%2520detected.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GiBy%3A%20A%20Giant-Step%20Baby-Step%20Classifier%20For%20Anomaly%20Detection%20In%0A%20%20Industrial%20Control%20Systems&entry.906535625=Sarad%20Venugopalan%20and%20Sridhar%20Adepu&entry.1292438233=%20%20The%20continuous%20monitoring%20of%20the%20interactions%20between%20cyber-physical%0Acomponents%20of%20any%20industrial%20control%20system%20%28ICS%29%20is%20required%20to%20secure%0Aautomation%20of%20the%20system%20controls%2C%20and%20to%20guarantee%20plant%20processes%20are%0Afail-safe%20and%20remain%20in%20an%20acceptably%20safe%20state.%20Safety%20is%20achieved%20by%0Amanaging%20actuation%20%28where%20electric%20signals%20are%20used%20to%20trigger%20physical%0Amovement%29%2C%20dependent%20on%20corresponding%20sensor%20readings%3B%20used%20as%20ground%20truth%20in%0Adecision%20making.%20Timely%20detection%20of%20anomalies%20%28attacks%2C%20faults%20and%0Aunascertained%20states%29%20in%20ICSs%20is%20crucial%20for%20the%20safe%20running%20of%20a%20plant%2C%20the%0Asafety%20of%20its%20personnel%2C%20and%20for%20the%20safe%20provision%20of%20any%20services%20provided.%0AWe%20propose%20an%20anomaly%20detection%20method%20that%20involves%20accurate%20linearization%20of%0Athe%20non-linear%20forms%20arising%20from%20sensor-actuator%28s%29%20relationships%2C%20primarily%0Abecause%20solving%20linear%20models%20is%20easier%20and%20well%20understood.%20Further%2C%20the%20time%0Acomplexity%20of%20the%20anomaly%20detection%20scenario/problem%20at%20hand%20is%20lowered%20using%0Adimensionality%20reduction%20of%20the%20actuator%28s%29%20in%20relationship%20with%20a%20sensor.%20We%0Aaccomplish%20this%20by%20using%20a%20well-known%20water%20treatment%20testbed%20as%20a%20use%20case.%0AOur%20experiments%20show%20millisecond%20time%20response%20to%20detect%20anomalies%20and%20provide%0Aexplainability%3B%20that%20are%20not%20simultaneously%20achieved%20by%20other%20state%20of%20the%20art%0AAI/ML%20models%20with%20eXplainable%20AI%20%28XAI%29%20used%20for%20the%20same%20purpose.%20Further%2C%20we%0Apin-point%20the%20sensor%28s%29%20and%20its%20actuation%20state%20for%20which%20anomaly%20was%20detected.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20906v1&entry.124074799=Read"},
{"title": "Jekyll-and-Hyde Tipping Point in an AI's Behavior", "author": "Neil F. Johnson and Frank Yingjie Huo", "abstract": "  Trust in AI is undermined by the fact that there is no science that predicts\n-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is\nlikely to tip mid-response to become wrong, misleading, irrelevant or\ndangerous. With deaths and trauma already being blamed on LLMs, this\nuncertainty is even pushing people to treat their 'pet' LLM more politely to\n'dissuade' it (or its future Artificial General Intelligence offspring) from\nsuddenly turning on them. Here we address this acute need by deriving from\nfirst principles an exact formula for when a Jekyll-and-Hyde tipping point\noccurs at LLMs' most basic level. Requiring only secondary school mathematics,\nit shows the cause to be the AI's attention spreading so thin it suddenly\nsnaps. This exact formula provides quantitative predictions for how the\ntipping-point can be delayed or prevented by changing the prompt and the AI's\ntraining. Tailored generalizations will provide policymakers and the public\nwith a firm platform for discussing any of AI's broader uses and risks, e.g. as\na personal counselor, medical advisor, decision-maker for when to use force in\na conflict situation. It also meets the need for clear and transparent answers\nto questions like ''should I be polite to my LLM?''\n", "link": "http://arxiv.org/abs/2504.20980v1", "date": "2025-04-29", "relevancy": 1.2665, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4251}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4221}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jekyll-and-Hyde%20Tipping%20Point%20in%20an%20AI%27s%20Behavior&body=Title%3A%20Jekyll-and-Hyde%20Tipping%20Point%20in%20an%20AI%27s%20Behavior%0AAuthor%3A%20Neil%20F.%20Johnson%20and%20Frank%20Yingjie%20Huo%0AAbstract%3A%20%20%20Trust%20in%20AI%20is%20undermined%20by%20the%20fact%20that%20there%20is%20no%20science%20that%20predicts%0A--%20or%20that%20can%20explain%20to%20the%20public%20--%20when%20an%20LLM%27s%20output%20%28e.g.%20ChatGPT%29%20is%0Alikely%20to%20tip%20mid-response%20to%20become%20wrong%2C%20misleading%2C%20irrelevant%20or%0Adangerous.%20With%20deaths%20and%20trauma%20already%20being%20blamed%20on%20LLMs%2C%20this%0Auncertainty%20is%20even%20pushing%20people%20to%20treat%20their%20%27pet%27%20LLM%20more%20politely%20to%0A%27dissuade%27%20it%20%28or%20its%20future%20Artificial%20General%20Intelligence%20offspring%29%20from%0Asuddenly%20turning%20on%20them.%20Here%20we%20address%20this%20acute%20need%20by%20deriving%20from%0Afirst%20principles%20an%20exact%20formula%20for%20when%20a%20Jekyll-and-Hyde%20tipping%20point%0Aoccurs%20at%20LLMs%27%20most%20basic%20level.%20Requiring%20only%20secondary%20school%20mathematics%2C%0Ait%20shows%20the%20cause%20to%20be%20the%20AI%27s%20attention%20spreading%20so%20thin%20it%20suddenly%0Asnaps.%20This%20exact%20formula%20provides%20quantitative%20predictions%20for%20how%20the%0Atipping-point%20can%20be%20delayed%20or%20prevented%20by%20changing%20the%20prompt%20and%20the%20AI%27s%0Atraining.%20Tailored%20generalizations%20will%20provide%20policymakers%20and%20the%20public%0Awith%20a%20firm%20platform%20for%20discussing%20any%20of%20AI%27s%20broader%20uses%20and%20risks%2C%20e.g.%20as%0Aa%20personal%20counselor%2C%20medical%20advisor%2C%20decision-maker%20for%20when%20to%20use%20force%20in%0Aa%20conflict%20situation.%20It%20also%20meets%20the%20need%20for%20clear%20and%20transparent%20answers%0Ato%20questions%20like%20%27%27should%20I%20be%20polite%20to%20my%20LLM%3F%27%27%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJekyll-and-Hyde%2520Tipping%2520Point%2520in%2520an%2520AI%2527s%2520Behavior%26entry.906535625%3DNeil%2520F.%2520Johnson%2520and%2520Frank%2520Yingjie%2520Huo%26entry.1292438233%3D%2520%2520Trust%2520in%2520AI%2520is%2520undermined%2520by%2520the%2520fact%2520that%2520there%2520is%2520no%2520science%2520that%2520predicts%250A--%2520or%2520that%2520can%2520explain%2520to%2520the%2520public%2520--%2520when%2520an%2520LLM%2527s%2520output%2520%2528e.g.%2520ChatGPT%2529%2520is%250Alikely%2520to%2520tip%2520mid-response%2520to%2520become%2520wrong%252C%2520misleading%252C%2520irrelevant%2520or%250Adangerous.%2520With%2520deaths%2520and%2520trauma%2520already%2520being%2520blamed%2520on%2520LLMs%252C%2520this%250Auncertainty%2520is%2520even%2520pushing%2520people%2520to%2520treat%2520their%2520%2527pet%2527%2520LLM%2520more%2520politely%2520to%250A%2527dissuade%2527%2520it%2520%2528or%2520its%2520future%2520Artificial%2520General%2520Intelligence%2520offspring%2529%2520from%250Asuddenly%2520turning%2520on%2520them.%2520Here%2520we%2520address%2520this%2520acute%2520need%2520by%2520deriving%2520from%250Afirst%2520principles%2520an%2520exact%2520formula%2520for%2520when%2520a%2520Jekyll-and-Hyde%2520tipping%2520point%250Aoccurs%2520at%2520LLMs%2527%2520most%2520basic%2520level.%2520Requiring%2520only%2520secondary%2520school%2520mathematics%252C%250Ait%2520shows%2520the%2520cause%2520to%2520be%2520the%2520AI%2527s%2520attention%2520spreading%2520so%2520thin%2520it%2520suddenly%250Asnaps.%2520This%2520exact%2520formula%2520provides%2520quantitative%2520predictions%2520for%2520how%2520the%250Atipping-point%2520can%2520be%2520delayed%2520or%2520prevented%2520by%2520changing%2520the%2520prompt%2520and%2520the%2520AI%2527s%250Atraining.%2520Tailored%2520generalizations%2520will%2520provide%2520policymakers%2520and%2520the%2520public%250Awith%2520a%2520firm%2520platform%2520for%2520discussing%2520any%2520of%2520AI%2527s%2520broader%2520uses%2520and%2520risks%252C%2520e.g.%2520as%250Aa%2520personal%2520counselor%252C%2520medical%2520advisor%252C%2520decision-maker%2520for%2520when%2520to%2520use%2520force%2520in%250Aa%2520conflict%2520situation.%2520It%2520also%2520meets%2520the%2520need%2520for%2520clear%2520and%2520transparent%2520answers%250Ato%2520questions%2520like%2520%2527%2527should%2520I%2520be%2520polite%2520to%2520my%2520LLM%253F%2527%2527%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jekyll-and-Hyde%20Tipping%20Point%20in%20an%20AI%27s%20Behavior&entry.906535625=Neil%20F.%20Johnson%20and%20Frank%20Yingjie%20Huo&entry.1292438233=%20%20Trust%20in%20AI%20is%20undermined%20by%20the%20fact%20that%20there%20is%20no%20science%20that%20predicts%0A--%20or%20that%20can%20explain%20to%20the%20public%20--%20when%20an%20LLM%27s%20output%20%28e.g.%20ChatGPT%29%20is%0Alikely%20to%20tip%20mid-response%20to%20become%20wrong%2C%20misleading%2C%20irrelevant%20or%0Adangerous.%20With%20deaths%20and%20trauma%20already%20being%20blamed%20on%20LLMs%2C%20this%0Auncertainty%20is%20even%20pushing%20people%20to%20treat%20their%20%27pet%27%20LLM%20more%20politely%20to%0A%27dissuade%27%20it%20%28or%20its%20future%20Artificial%20General%20Intelligence%20offspring%29%20from%0Asuddenly%20turning%20on%20them.%20Here%20we%20address%20this%20acute%20need%20by%20deriving%20from%0Afirst%20principles%20an%20exact%20formula%20for%20when%20a%20Jekyll-and-Hyde%20tipping%20point%0Aoccurs%20at%20LLMs%27%20most%20basic%20level.%20Requiring%20only%20secondary%20school%20mathematics%2C%0Ait%20shows%20the%20cause%20to%20be%20the%20AI%27s%20attention%20spreading%20so%20thin%20it%20suddenly%0Asnaps.%20This%20exact%20formula%20provides%20quantitative%20predictions%20for%20how%20the%0Atipping-point%20can%20be%20delayed%20or%20prevented%20by%20changing%20the%20prompt%20and%20the%20AI%27s%0Atraining.%20Tailored%20generalizations%20will%20provide%20policymakers%20and%20the%20public%0Awith%20a%20firm%20platform%20for%20discussing%20any%20of%20AI%27s%20broader%20uses%20and%20risks%2C%20e.g.%20as%0Aa%20personal%20counselor%2C%20medical%20advisor%2C%20decision-maker%20for%20when%20to%20use%20force%20in%0Aa%20conflict%20situation.%20It%20also%20meets%20the%20need%20for%20clear%20and%20transparent%20answers%0Ato%20questions%20like%20%27%27should%20I%20be%20polite%20to%20my%20LLM%3F%27%27%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20980v1&entry.124074799=Read"},
{"title": "The When and How of Target Variable Transformations", "author": "Loren Nuyts and Jesse Davis", "abstract": "  The machine learning pipeline typically involves the iterative process of (1)\ncollecting the data, (2) preparing the data, (3) learning a model, and (4)\nevaluating a model. Practitioners recognize the importance of the data\npreparation phase in terms of its impact on the ability to learn accurate\nmodels. In this regard, significant attention is often paid to manipulating the\nfeature set (e.g., selection, transformations, dimensionality reduction). A\npoint that is less well appreciated is that transformations on the target\nvariable can also have a large impact on whether it is possible to learn a\nsuitable model. These transformations may include accounting for\nsubject-specific biases (e.g., in how someone uses a rating scale), contexts\n(e.g., population size effects), and general trends (e.g., inflation). However,\nthis point has received a much more cursory treatment in the existing\nliterature. The goal of this paper is three-fold. First, we aim to highlight\nthe importance of this problem by showing when transforming the target variable\nhas been useful in practice. Second, we will provide a set of generic ``rules\nof thumb'' that indicate situations when transforming the target variable may\nbe needed. Third, we will discuss which transformations should be considered in\na given situation.\n", "link": "http://arxiv.org/abs/2504.20821v1", "date": "2025-04-29", "relevancy": 1.3041, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4521}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4444}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20When%20and%20How%20of%20Target%20Variable%20Transformations&body=Title%3A%20The%20When%20and%20How%20of%20Target%20Variable%20Transformations%0AAuthor%3A%20Loren%20Nuyts%20and%20Jesse%20Davis%0AAbstract%3A%20%20%20The%20machine%20learning%20pipeline%20typically%20involves%20the%20iterative%20process%20of%20%281%29%0Acollecting%20the%20data%2C%20%282%29%20preparing%20the%20data%2C%20%283%29%20learning%20a%20model%2C%20and%20%284%29%0Aevaluating%20a%20model.%20Practitioners%20recognize%20the%20importance%20of%20the%20data%0Apreparation%20phase%20in%20terms%20of%20its%20impact%20on%20the%20ability%20to%20learn%20accurate%0Amodels.%20In%20this%20regard%2C%20significant%20attention%20is%20often%20paid%20to%20manipulating%20the%0Afeature%20set%20%28e.g.%2C%20selection%2C%20transformations%2C%20dimensionality%20reduction%29.%20A%0Apoint%20that%20is%20less%20well%20appreciated%20is%20that%20transformations%20on%20the%20target%0Avariable%20can%20also%20have%20a%20large%20impact%20on%20whether%20it%20is%20possible%20to%20learn%20a%0Asuitable%20model.%20These%20transformations%20may%20include%20accounting%20for%0Asubject-specific%20biases%20%28e.g.%2C%20in%20how%20someone%20uses%20a%20rating%20scale%29%2C%20contexts%0A%28e.g.%2C%20population%20size%20effects%29%2C%20and%20general%20trends%20%28e.g.%2C%20inflation%29.%20However%2C%0Athis%20point%20has%20received%20a%20much%20more%20cursory%20treatment%20in%20the%20existing%0Aliterature.%20The%20goal%20of%20this%20paper%20is%20three-fold.%20First%2C%20we%20aim%20to%20highlight%0Athe%20importance%20of%20this%20problem%20by%20showing%20when%20transforming%20the%20target%20variable%0Ahas%20been%20useful%20in%20practice.%20Second%2C%20we%20will%20provide%20a%20set%20of%20generic%20%60%60rules%0Aof%20thumb%27%27%20that%20indicate%20situations%20when%20transforming%20the%20target%20variable%20may%0Abe%20needed.%20Third%2C%20we%20will%20discuss%20which%20transformations%20should%20be%20considered%20in%0Aa%20given%20situation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20821v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520When%2520and%2520How%2520of%2520Target%2520Variable%2520Transformations%26entry.906535625%3DLoren%2520Nuyts%2520and%2520Jesse%2520Davis%26entry.1292438233%3D%2520%2520The%2520machine%2520learning%2520pipeline%2520typically%2520involves%2520the%2520iterative%2520process%2520of%2520%25281%2529%250Acollecting%2520the%2520data%252C%2520%25282%2529%2520preparing%2520the%2520data%252C%2520%25283%2529%2520learning%2520a%2520model%252C%2520and%2520%25284%2529%250Aevaluating%2520a%2520model.%2520Practitioners%2520recognize%2520the%2520importance%2520of%2520the%2520data%250Apreparation%2520phase%2520in%2520terms%2520of%2520its%2520impact%2520on%2520the%2520ability%2520to%2520learn%2520accurate%250Amodels.%2520In%2520this%2520regard%252C%2520significant%2520attention%2520is%2520often%2520paid%2520to%2520manipulating%2520the%250Afeature%2520set%2520%2528e.g.%252C%2520selection%252C%2520transformations%252C%2520dimensionality%2520reduction%2529.%2520A%250Apoint%2520that%2520is%2520less%2520well%2520appreciated%2520is%2520that%2520transformations%2520on%2520the%2520target%250Avariable%2520can%2520also%2520have%2520a%2520large%2520impact%2520on%2520whether%2520it%2520is%2520possible%2520to%2520learn%2520a%250Asuitable%2520model.%2520These%2520transformations%2520may%2520include%2520accounting%2520for%250Asubject-specific%2520biases%2520%2528e.g.%252C%2520in%2520how%2520someone%2520uses%2520a%2520rating%2520scale%2529%252C%2520contexts%250A%2528e.g.%252C%2520population%2520size%2520effects%2529%252C%2520and%2520general%2520trends%2520%2528e.g.%252C%2520inflation%2529.%2520However%252C%250Athis%2520point%2520has%2520received%2520a%2520much%2520more%2520cursory%2520treatment%2520in%2520the%2520existing%250Aliterature.%2520The%2520goal%2520of%2520this%2520paper%2520is%2520three-fold.%2520First%252C%2520we%2520aim%2520to%2520highlight%250Athe%2520importance%2520of%2520this%2520problem%2520by%2520showing%2520when%2520transforming%2520the%2520target%2520variable%250Ahas%2520been%2520useful%2520in%2520practice.%2520Second%252C%2520we%2520will%2520provide%2520a%2520set%2520of%2520generic%2520%2560%2560rules%250Aof%2520thumb%2527%2527%2520that%2520indicate%2520situations%2520when%2520transforming%2520the%2520target%2520variable%2520may%250Abe%2520needed.%2520Third%252C%2520we%2520will%2520discuss%2520which%2520transformations%2520should%2520be%2520considered%2520in%250Aa%2520given%2520situation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20821v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20When%20and%20How%20of%20Target%20Variable%20Transformations&entry.906535625=Loren%20Nuyts%20and%20Jesse%20Davis&entry.1292438233=%20%20The%20machine%20learning%20pipeline%20typically%20involves%20the%20iterative%20process%20of%20%281%29%0Acollecting%20the%20data%2C%20%282%29%20preparing%20the%20data%2C%20%283%29%20learning%20a%20model%2C%20and%20%284%29%0Aevaluating%20a%20model.%20Practitioners%20recognize%20the%20importance%20of%20the%20data%0Apreparation%20phase%20in%20terms%20of%20its%20impact%20on%20the%20ability%20to%20learn%20accurate%0Amodels.%20In%20this%20regard%2C%20significant%20attention%20is%20often%20paid%20to%20manipulating%20the%0Afeature%20set%20%28e.g.%2C%20selection%2C%20transformations%2C%20dimensionality%20reduction%29.%20A%0Apoint%20that%20is%20less%20well%20appreciated%20is%20that%20transformations%20on%20the%20target%0Avariable%20can%20also%20have%20a%20large%20impact%20on%20whether%20it%20is%20possible%20to%20learn%20a%0Asuitable%20model.%20These%20transformations%20may%20include%20accounting%20for%0Asubject-specific%20biases%20%28e.g.%2C%20in%20how%20someone%20uses%20a%20rating%20scale%29%2C%20contexts%0A%28e.g.%2C%20population%20size%20effects%29%2C%20and%20general%20trends%20%28e.g.%2C%20inflation%29.%20However%2C%0Athis%20point%20has%20received%20a%20much%20more%20cursory%20treatment%20in%20the%20existing%0Aliterature.%20The%20goal%20of%20this%20paper%20is%20three-fold.%20First%2C%20we%20aim%20to%20highlight%0Athe%20importance%20of%20this%20problem%20by%20showing%20when%20transforming%20the%20target%20variable%0Ahas%20been%20useful%20in%20practice.%20Second%2C%20we%20will%20provide%20a%20set%20of%20generic%20%60%60rules%0Aof%20thumb%27%27%20that%20indicate%20situations%20when%20transforming%20the%20target%20variable%20may%0Abe%20needed.%20Third%2C%20we%20will%20discuss%20which%20transformations%20should%20be%20considered%20in%0Aa%20given%20situation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20821v1&entry.124074799=Read"},
{"title": "Higher order definition of causality by optimally conditioned transfer\n  entropy", "author": "Jakub Ko\u0159enek and Pavel Sanda and Jaroslav Hlinka", "abstract": "  The description of the dynamics of complex systems, in particular the capture\nof the interaction structure and causal relationships between elements of the\nsystem, is one of the central questions of interdisciplinary research. While\nthe characterization of pairwise causal interactions is a relatively ripe field\nwith established theoretical concepts and the current focus is on technical\nissues of their efficient estimation, it turns out that the standard concepts\nsuch as Granger causality or transfer entropy may not faithfully reflect\npossible synergies or interactions of higher orders, phenomena highly relevant\nfor many real-world complex systems. In this paper, we propose a generalization\nand refinement of the information-theoretic approach to causal inference,\nenabling the description of truly multivariate, rather than multiple pairwise,\ncausal interactions, and moving thus from causal networks to causal\nhypernetworks. In particular, while keeping the ability to control for\nmediating variables or common causes, in case of purely synergetic interactions\nsuch as the exclusive disjunction, it ascribes the causal role to the\nmultivariate causal set but \\emph{not} to individual inputs, distinguishing it\nthus from the case of e.g. two additive univariate causes. We demonstrate this\nconcept by application to illustrative theoretical examples as well as a\nbiophysically realistic simulation of biological neuronal dynamics recently\nreported to employ synergetic computations.\n", "link": "http://arxiv.org/abs/2409.08295v2", "date": "2025-04-29", "relevancy": 0.8292, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4404}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4036}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Higher%20order%20definition%20of%20causality%20by%20optimally%20conditioned%20transfer%0A%20%20entropy&body=Title%3A%20Higher%20order%20definition%20of%20causality%20by%20optimally%20conditioned%20transfer%0A%20%20entropy%0AAuthor%3A%20Jakub%20Ko%C5%99enek%20and%20Pavel%20Sanda%20and%20Jaroslav%20Hlinka%0AAbstract%3A%20%20%20The%20description%20of%20the%20dynamics%20of%20complex%20systems%2C%20in%20particular%20the%20capture%0Aof%20the%20interaction%20structure%20and%20causal%20relationships%20between%20elements%20of%20the%0Asystem%2C%20is%20one%20of%20the%20central%20questions%20of%20interdisciplinary%20research.%20While%0Athe%20characterization%20of%20pairwise%20causal%20interactions%20is%20a%20relatively%20ripe%20field%0Awith%20established%20theoretical%20concepts%20and%20the%20current%20focus%20is%20on%20technical%0Aissues%20of%20their%20efficient%20estimation%2C%20it%20turns%20out%20that%20the%20standard%20concepts%0Asuch%20as%20Granger%20causality%20or%20transfer%20entropy%20may%20not%20faithfully%20reflect%0Apossible%20synergies%20or%20interactions%20of%20higher%20orders%2C%20phenomena%20highly%20relevant%0Afor%20many%20real-world%20complex%20systems.%20In%20this%20paper%2C%20we%20propose%20a%20generalization%0Aand%20refinement%20of%20the%20information-theoretic%20approach%20to%20causal%20inference%2C%0Aenabling%20the%20description%20of%20truly%20multivariate%2C%20rather%20than%20multiple%20pairwise%2C%0Acausal%20interactions%2C%20and%20moving%20thus%20from%20causal%20networks%20to%20causal%0Ahypernetworks.%20In%20particular%2C%20while%20keeping%20the%20ability%20to%20control%20for%0Amediating%20variables%20or%20common%20causes%2C%20in%20case%20of%20purely%20synergetic%20interactions%0Asuch%20as%20the%20exclusive%20disjunction%2C%20it%20ascribes%20the%20causal%20role%20to%20the%0Amultivariate%20causal%20set%20but%20%5Cemph%7Bnot%7D%20to%20individual%20inputs%2C%20distinguishing%20it%0Athus%20from%20the%20case%20of%20e.g.%20two%20additive%20univariate%20causes.%20We%20demonstrate%20this%0Aconcept%20by%20application%20to%20illustrative%20theoretical%20examples%20as%20well%20as%20a%0Abiophysically%20realistic%20simulation%20of%20biological%20neuronal%20dynamics%20recently%0Areported%20to%20employ%20synergetic%20computations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.08295v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigher%2520order%2520definition%2520of%2520causality%2520by%2520optimally%2520conditioned%2520transfer%250A%2520%2520entropy%26entry.906535625%3DJakub%2520Ko%25C5%2599enek%2520and%2520Pavel%2520Sanda%2520and%2520Jaroslav%2520Hlinka%26entry.1292438233%3D%2520%2520The%2520description%2520of%2520the%2520dynamics%2520of%2520complex%2520systems%252C%2520in%2520particular%2520the%2520capture%250Aof%2520the%2520interaction%2520structure%2520and%2520causal%2520relationships%2520between%2520elements%2520of%2520the%250Asystem%252C%2520is%2520one%2520of%2520the%2520central%2520questions%2520of%2520interdisciplinary%2520research.%2520While%250Athe%2520characterization%2520of%2520pairwise%2520causal%2520interactions%2520is%2520a%2520relatively%2520ripe%2520field%250Awith%2520established%2520theoretical%2520concepts%2520and%2520the%2520current%2520focus%2520is%2520on%2520technical%250Aissues%2520of%2520their%2520efficient%2520estimation%252C%2520it%2520turns%2520out%2520that%2520the%2520standard%2520concepts%250Asuch%2520as%2520Granger%2520causality%2520or%2520transfer%2520entropy%2520may%2520not%2520faithfully%2520reflect%250Apossible%2520synergies%2520or%2520interactions%2520of%2520higher%2520orders%252C%2520phenomena%2520highly%2520relevant%250Afor%2520many%2520real-world%2520complex%2520systems.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520generalization%250Aand%2520refinement%2520of%2520the%2520information-theoretic%2520approach%2520to%2520causal%2520inference%252C%250Aenabling%2520the%2520description%2520of%2520truly%2520multivariate%252C%2520rather%2520than%2520multiple%2520pairwise%252C%250Acausal%2520interactions%252C%2520and%2520moving%2520thus%2520from%2520causal%2520networks%2520to%2520causal%250Ahypernetworks.%2520In%2520particular%252C%2520while%2520keeping%2520the%2520ability%2520to%2520control%2520for%250Amediating%2520variables%2520or%2520common%2520causes%252C%2520in%2520case%2520of%2520purely%2520synergetic%2520interactions%250Asuch%2520as%2520the%2520exclusive%2520disjunction%252C%2520it%2520ascribes%2520the%2520causal%2520role%2520to%2520the%250Amultivariate%2520causal%2520set%2520but%2520%255Cemph%257Bnot%257D%2520to%2520individual%2520inputs%252C%2520distinguishing%2520it%250Athus%2520from%2520the%2520case%2520of%2520e.g.%2520two%2520additive%2520univariate%2520causes.%2520We%2520demonstrate%2520this%250Aconcept%2520by%2520application%2520to%2520illustrative%2520theoretical%2520examples%2520as%2520well%2520as%2520a%250Abiophysically%2520realistic%2520simulation%2520of%2520biological%2520neuronal%2520dynamics%2520recently%250Areported%2520to%2520employ%2520synergetic%2520computations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08295v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Higher%20order%20definition%20of%20causality%20by%20optimally%20conditioned%20transfer%0A%20%20entropy&entry.906535625=Jakub%20Ko%C5%99enek%20and%20Pavel%20Sanda%20and%20Jaroslav%20Hlinka&entry.1292438233=%20%20The%20description%20of%20the%20dynamics%20of%20complex%20systems%2C%20in%20particular%20the%20capture%0Aof%20the%20interaction%20structure%20and%20causal%20relationships%20between%20elements%20of%20the%0Asystem%2C%20is%20one%20of%20the%20central%20questions%20of%20interdisciplinary%20research.%20While%0Athe%20characterization%20of%20pairwise%20causal%20interactions%20is%20a%20relatively%20ripe%20field%0Awith%20established%20theoretical%20concepts%20and%20the%20current%20focus%20is%20on%20technical%0Aissues%20of%20their%20efficient%20estimation%2C%20it%20turns%20out%20that%20the%20standard%20concepts%0Asuch%20as%20Granger%20causality%20or%20transfer%20entropy%20may%20not%20faithfully%20reflect%0Apossible%20synergies%20or%20interactions%20of%20higher%20orders%2C%20phenomena%20highly%20relevant%0Afor%20many%20real-world%20complex%20systems.%20In%20this%20paper%2C%20we%20propose%20a%20generalization%0Aand%20refinement%20of%20the%20information-theoretic%20approach%20to%20causal%20inference%2C%0Aenabling%20the%20description%20of%20truly%20multivariate%2C%20rather%20than%20multiple%20pairwise%2C%0Acausal%20interactions%2C%20and%20moving%20thus%20from%20causal%20networks%20to%20causal%0Ahypernetworks.%20In%20particular%2C%20while%20keeping%20the%20ability%20to%20control%20for%0Amediating%20variables%20or%20common%20causes%2C%20in%20case%20of%20purely%20synergetic%20interactions%0Asuch%20as%20the%20exclusive%20disjunction%2C%20it%20ascribes%20the%20causal%20role%20to%20the%0Amultivariate%20causal%20set%20but%20%5Cemph%7Bnot%7D%20to%20individual%20inputs%2C%20distinguishing%20it%0Athus%20from%20the%20case%20of%20e.g.%20two%20additive%20univariate%20causes.%20We%20demonstrate%20this%0Aconcept%20by%20application%20to%20illustrative%20theoretical%20examples%20as%20well%20as%20a%0Abiophysically%20realistic%20simulation%20of%20biological%20neuronal%20dynamics%20recently%0Areported%20to%20employ%20synergetic%20computations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.08295v2&entry.124074799=Read"},
{"title": "Dual Explanations via Subgraph Matching for Malware Detection", "author": "Hossein Shokouhinejad and Roozbeh Razavi-Far and Griffin Higgins and Ali A. Ghorbani", "abstract": "  Interpretable malware detection is crucial for understanding harmful\nbehaviors and building trust in automated security systems. Traditional\nexplainable methods for Graph Neural Networks (GNNs) often highlight important\nregions within a graph but fail to associate them with known benign or\nmalicious behavioral patterns. This limitation reduces their utility in\nsecurity contexts, where alignment with verified prototypes is essential. In\nthis work, we introduce a novel dual prototype-driven explainable framework\nthat interprets GNN-based malware detection decisions. This dual explainable\nframework integrates a base explainer (a state-of-the-art explainer) with a\nnovel second-level explainer which is designed by subgraph matching technique,\ncalled SubMatch explainer. The proposed explainer assigns interpretable scores\nto nodes based on their association with matched subgraphs, offering a\nfine-grained distinction between benign and malicious regions. This\nprototype-guided scoring mechanism enables more interpretable, behavior-aligned\nexplanations. Experimental results demonstrate that our method preserves high\ndetection performance while significantly improving interpretability in malware\nanalysis.\n", "link": "http://arxiv.org/abs/2504.20904v1", "date": "2025-04-29", "relevancy": 1.2957, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4456}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4204}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Explanations%20via%20Subgraph%20Matching%20for%20Malware%20Detection&body=Title%3A%20Dual%20Explanations%20via%20Subgraph%20Matching%20for%20Malware%20Detection%0AAuthor%3A%20Hossein%20Shokouhinejad%20and%20Roozbeh%20Razavi-Far%20and%20Griffin%20Higgins%20and%20Ali%20A.%20Ghorbani%0AAbstract%3A%20%20%20Interpretable%20malware%20detection%20is%20crucial%20for%20understanding%20harmful%0Abehaviors%20and%20building%20trust%20in%20automated%20security%20systems.%20Traditional%0Aexplainable%20methods%20for%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20highlight%20important%0Aregions%20within%20a%20graph%20but%20fail%20to%20associate%20them%20with%20known%20benign%20or%0Amalicious%20behavioral%20patterns.%20This%20limitation%20reduces%20their%20utility%20in%0Asecurity%20contexts%2C%20where%20alignment%20with%20verified%20prototypes%20is%20essential.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20dual%20prototype-driven%20explainable%20framework%0Athat%20interprets%20GNN-based%20malware%20detection%20decisions.%20This%20dual%20explainable%0Aframework%20integrates%20a%20base%20explainer%20%28a%20state-of-the-art%20explainer%29%20with%20a%0Anovel%20second-level%20explainer%20which%20is%20designed%20by%20subgraph%20matching%20technique%2C%0Acalled%20SubMatch%20explainer.%20The%20proposed%20explainer%20assigns%20interpretable%20scores%0Ato%20nodes%20based%20on%20their%20association%20with%20matched%20subgraphs%2C%20offering%20a%0Afine-grained%20distinction%20between%20benign%20and%20malicious%20regions.%20This%0Aprototype-guided%20scoring%20mechanism%20enables%20more%20interpretable%2C%20behavior-aligned%0Aexplanations.%20Experimental%20results%20demonstrate%20that%20our%20method%20preserves%20high%0Adetection%20performance%20while%20significantly%20improving%20interpretability%20in%20malware%0Aanalysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Explanations%2520via%2520Subgraph%2520Matching%2520for%2520Malware%2520Detection%26entry.906535625%3DHossein%2520Shokouhinejad%2520and%2520Roozbeh%2520Razavi-Far%2520and%2520Griffin%2520Higgins%2520and%2520Ali%2520A.%2520Ghorbani%26entry.1292438233%3D%2520%2520Interpretable%2520malware%2520detection%2520is%2520crucial%2520for%2520understanding%2520harmful%250Abehaviors%2520and%2520building%2520trust%2520in%2520automated%2520security%2520systems.%2520Traditional%250Aexplainable%2520methods%2520for%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520often%2520highlight%2520important%250Aregions%2520within%2520a%2520graph%2520but%2520fail%2520to%2520associate%2520them%2520with%2520known%2520benign%2520or%250Amalicious%2520behavioral%2520patterns.%2520This%2520limitation%2520reduces%2520their%2520utility%2520in%250Asecurity%2520contexts%252C%2520where%2520alignment%2520with%2520verified%2520prototypes%2520is%2520essential.%2520In%250Athis%2520work%252C%2520we%2520introduce%2520a%2520novel%2520dual%2520prototype-driven%2520explainable%2520framework%250Athat%2520interprets%2520GNN-based%2520malware%2520detection%2520decisions.%2520This%2520dual%2520explainable%250Aframework%2520integrates%2520a%2520base%2520explainer%2520%2528a%2520state-of-the-art%2520explainer%2529%2520with%2520a%250Anovel%2520second-level%2520explainer%2520which%2520is%2520designed%2520by%2520subgraph%2520matching%2520technique%252C%250Acalled%2520SubMatch%2520explainer.%2520The%2520proposed%2520explainer%2520assigns%2520interpretable%2520scores%250Ato%2520nodes%2520based%2520on%2520their%2520association%2520with%2520matched%2520subgraphs%252C%2520offering%2520a%250Afine-grained%2520distinction%2520between%2520benign%2520and%2520malicious%2520regions.%2520This%250Aprototype-guided%2520scoring%2520mechanism%2520enables%2520more%2520interpretable%252C%2520behavior-aligned%250Aexplanations.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520preserves%2520high%250Adetection%2520performance%2520while%2520significantly%2520improving%2520interpretability%2520in%2520malware%250Aanalysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Explanations%20via%20Subgraph%20Matching%20for%20Malware%20Detection&entry.906535625=Hossein%20Shokouhinejad%20and%20Roozbeh%20Razavi-Far%20and%20Griffin%20Higgins%20and%20Ali%20A.%20Ghorbani&entry.1292438233=%20%20Interpretable%20malware%20detection%20is%20crucial%20for%20understanding%20harmful%0Abehaviors%20and%20building%20trust%20in%20automated%20security%20systems.%20Traditional%0Aexplainable%20methods%20for%20Graph%20Neural%20Networks%20%28GNNs%29%20often%20highlight%20important%0Aregions%20within%20a%20graph%20but%20fail%20to%20associate%20them%20with%20known%20benign%20or%0Amalicious%20behavioral%20patterns.%20This%20limitation%20reduces%20their%20utility%20in%0Asecurity%20contexts%2C%20where%20alignment%20with%20verified%20prototypes%20is%20essential.%20In%0Athis%20work%2C%20we%20introduce%20a%20novel%20dual%20prototype-driven%20explainable%20framework%0Athat%20interprets%20GNN-based%20malware%20detection%20decisions.%20This%20dual%20explainable%0Aframework%20integrates%20a%20base%20explainer%20%28a%20state-of-the-art%20explainer%29%20with%20a%0Anovel%20second-level%20explainer%20which%20is%20designed%20by%20subgraph%20matching%20technique%2C%0Acalled%20SubMatch%20explainer.%20The%20proposed%20explainer%20assigns%20interpretable%20scores%0Ato%20nodes%20based%20on%20their%20association%20with%20matched%20subgraphs%2C%20offering%20a%0Afine-grained%20distinction%20between%20benign%20and%20malicious%20regions.%20This%0Aprototype-guided%20scoring%20mechanism%20enables%20more%20interpretable%2C%20behavior-aligned%0Aexplanations.%20Experimental%20results%20demonstrate%20that%20our%20method%20preserves%20high%0Adetection%20performance%20while%20significantly%20improving%20interpretability%20in%20malware%0Aanalysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20904v1&entry.124074799=Read"},
{"title": "Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation", "author": "Harry Mead and Clarissa Costen and Bruno Lacerda and Nick Hawes", "abstract": "  When optimising for conditional value at risk (CVaR) using policy gradients\n(PG), current methods rely on discarding a large proportion of trajectories,\nresulting in poor sample efficiency. We propose a reformulation of the CVaR\noptimisation problem by capping the total return of trajectories used in\ntraining, rather than simply discarding them, and show that this is equivalent\nto the original problem if the cap is set appropriately. We show, with\nempirical results in an number of environments, that this reformulation of the\nproblem results in consistently improved performance compared to baselines.\n", "link": "http://arxiv.org/abs/2504.20887v1", "date": "2025-04-29", "relevancy": 1.2324, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4206}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4023}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.3948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Return%20Capping%3A%20Sample-Efficient%20CVaR%20Policy%20Gradient%20Optimisation&body=Title%3A%20Return%20Capping%3A%20Sample-Efficient%20CVaR%20Policy%20Gradient%20Optimisation%0AAuthor%3A%20Harry%20Mead%20and%20Clarissa%20Costen%20and%20Bruno%20Lacerda%20and%20Nick%20Hawes%0AAbstract%3A%20%20%20When%20optimising%20for%20conditional%20value%20at%20risk%20%28CVaR%29%20using%20policy%20gradients%0A%28PG%29%2C%20current%20methods%20rely%20on%20discarding%20a%20large%20proportion%20of%20trajectories%2C%0Aresulting%20in%20poor%20sample%20efficiency.%20We%20propose%20a%20reformulation%20of%20the%20CVaR%0Aoptimisation%20problem%20by%20capping%20the%20total%20return%20of%20trajectories%20used%20in%0Atraining%2C%20rather%20than%20simply%20discarding%20them%2C%20and%20show%20that%20this%20is%20equivalent%0Ato%20the%20original%20problem%20if%20the%20cap%20is%20set%20appropriately.%20We%20show%2C%20with%0Aempirical%20results%20in%20an%20number%20of%20environments%2C%20that%20this%20reformulation%20of%20the%0Aproblem%20results%20in%20consistently%20improved%20performance%20compared%20to%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.20887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReturn%2520Capping%253A%2520Sample-Efficient%2520CVaR%2520Policy%2520Gradient%2520Optimisation%26entry.906535625%3DHarry%2520Mead%2520and%2520Clarissa%2520Costen%2520and%2520Bruno%2520Lacerda%2520and%2520Nick%2520Hawes%26entry.1292438233%3D%2520%2520When%2520optimising%2520for%2520conditional%2520value%2520at%2520risk%2520%2528CVaR%2529%2520using%2520policy%2520gradients%250A%2528PG%2529%252C%2520current%2520methods%2520rely%2520on%2520discarding%2520a%2520large%2520proportion%2520of%2520trajectories%252C%250Aresulting%2520in%2520poor%2520sample%2520efficiency.%2520We%2520propose%2520a%2520reformulation%2520of%2520the%2520CVaR%250Aoptimisation%2520problem%2520by%2520capping%2520the%2520total%2520return%2520of%2520trajectories%2520used%2520in%250Atraining%252C%2520rather%2520than%2520simply%2520discarding%2520them%252C%2520and%2520show%2520that%2520this%2520is%2520equivalent%250Ato%2520the%2520original%2520problem%2520if%2520the%2520cap%2520is%2520set%2520appropriately.%2520We%2520show%252C%2520with%250Aempirical%2520results%2520in%2520an%2520number%2520of%2520environments%252C%2520that%2520this%2520reformulation%2520of%2520the%250Aproblem%2520results%2520in%2520consistently%2520improved%2520performance%2520compared%2520to%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.20887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Return%20Capping%3A%20Sample-Efficient%20CVaR%20Policy%20Gradient%20Optimisation&entry.906535625=Harry%20Mead%20and%20Clarissa%20Costen%20and%20Bruno%20Lacerda%20and%20Nick%20Hawes&entry.1292438233=%20%20When%20optimising%20for%20conditional%20value%20at%20risk%20%28CVaR%29%20using%20policy%20gradients%0A%28PG%29%2C%20current%20methods%20rely%20on%20discarding%20a%20large%20proportion%20of%20trajectories%2C%0Aresulting%20in%20poor%20sample%20efficiency.%20We%20propose%20a%20reformulation%20of%20the%20CVaR%0Aoptimisation%20problem%20by%20capping%20the%20total%20return%20of%20trajectories%20used%20in%0Atraining%2C%20rather%20than%20simply%20discarding%20them%2C%20and%20show%20that%20this%20is%20equivalent%0Ato%20the%20original%20problem%20if%20the%20cap%20is%20set%20appropriately.%20We%20show%2C%20with%0Aempirical%20results%20in%20an%20number%20of%20environments%2C%20that%20this%20reformulation%20of%20the%0Aproblem%20results%20in%20consistently%20improved%20performance%20compared%20to%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.20887v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


