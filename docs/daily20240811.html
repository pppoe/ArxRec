<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240808.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D\n  Facial Prior-guided Identity Alignment Network", "author": "Xiaozhong Ji and Chuming Lin and Zhonggan Ding and Ying Tai and Junwei Zhu and Xiaobin Hu and Donghao Luo and Yanhao Ge and Chengjie Wang", "abstract": "  Person-generic audio-driven face generation is a challenging task in computer\nvision. Previous methods have achieved remarkable progress in audio-visual\nsynchronization, but there is still a significant gap between current results\nand practical applications. The challenges are two-fold: 1) Preserving unique\nindividual traits for achieving high-precision lip synchronization. 2)\nGenerating high-quality facial renderings in real-time performance. In this\npaper, we propose a novel generalized audio-driven framework RealTalk, which\nconsists of an audio-to-expression transformer and a high-fidelity\nexpression-to-face renderer. In the first component, we consider both identity\nand intra-personal variation features related to speaking lip movements. By\nincorporating cross-modal attention on the enriched facial priors, we can\neffectively align lip movements with audio, thus attaining greater precision in\nexpression prediction. In the second component, we design a lightweight facial\nidentity alignment (FIA) module which includes a lip-shape control structure\nand a face texture reference structure. This novel design allows us to generate\nfine details in real-time, without depending on sophisticated and inefficient\nfeature alignment modules. Our experimental results, both quantitative and\nqualitative, on public datasets demonstrate the clear advantages of our method\nin terms of lip-speech synchronization and generation quality. Furthermore, our\nmethod is efficient and requires fewer computational resources, making it\nwell-suited to meet the needs of practical applications.\n", "link": "http://arxiv.org/abs/2406.18284v2", "date": "2024-08-08", "relevancy": 3.1126, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.625}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.625}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RealTalk%3A%20Real-time%20and%20Realistic%20Audio-driven%20Face%20Generation%20with%203D%0A%20%20Facial%20Prior-guided%20Identity%20Alignment%20Network&body=Title%3A%20RealTalk%3A%20Real-time%20and%20Realistic%20Audio-driven%20Face%20Generation%20with%203D%0A%20%20Facial%20Prior-guided%20Identity%20Alignment%20Network%0AAuthor%3A%20Xiaozhong%20Ji%20and%20Chuming%20Lin%20and%20Zhonggan%20Ding%20and%20Ying%20Tai%20and%20Junwei%20Zhu%20and%20Xiaobin%20Hu%20and%20Donghao%20Luo%20and%20Yanhao%20Ge%20and%20Chengjie%20Wang%0AAbstract%3A%20%20%20Person-generic%20audio-driven%20face%20generation%20is%20a%20challenging%20task%20in%20computer%0Avision.%20Previous%20methods%20have%20achieved%20remarkable%20progress%20in%20audio-visual%0Asynchronization%2C%20but%20there%20is%20still%20a%20significant%20gap%20between%20current%20results%0Aand%20practical%20applications.%20The%20challenges%20are%20two-fold%3A%201%29%20Preserving%20unique%0Aindividual%20traits%20for%20achieving%20high-precision%20lip%20synchronization.%202%29%0AGenerating%20high-quality%20facial%20renderings%20in%20real-time%20performance.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20generalized%20audio-driven%20framework%20RealTalk%2C%20which%0Aconsists%20of%20an%20audio-to-expression%20transformer%20and%20a%20high-fidelity%0Aexpression-to-face%20renderer.%20In%20the%20first%20component%2C%20we%20consider%20both%20identity%0Aand%20intra-personal%20variation%20features%20related%20to%20speaking%20lip%20movements.%20By%0Aincorporating%20cross-modal%20attention%20on%20the%20enriched%20facial%20priors%2C%20we%20can%0Aeffectively%20align%20lip%20movements%20with%20audio%2C%20thus%20attaining%20greater%20precision%20in%0Aexpression%20prediction.%20In%20the%20second%20component%2C%20we%20design%20a%20lightweight%20facial%0Aidentity%20alignment%20%28FIA%29%20module%20which%20includes%20a%20lip-shape%20control%20structure%0Aand%20a%20face%20texture%20reference%20structure.%20This%20novel%20design%20allows%20us%20to%20generate%0Afine%20details%20in%20real-time%2C%20without%20depending%20on%20sophisticated%20and%20inefficient%0Afeature%20alignment%20modules.%20Our%20experimental%20results%2C%20both%20quantitative%20and%0Aqualitative%2C%20on%20public%20datasets%20demonstrate%20the%20clear%20advantages%20of%20our%20method%0Ain%20terms%20of%20lip-speech%20synchronization%20and%20generation%20quality.%20Furthermore%2C%20our%0Amethod%20is%20efficient%20and%20requires%20fewer%20computational%20resources%2C%20making%20it%0Awell-suited%20to%20meet%20the%20needs%20of%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18284v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRealTalk%253A%2520Real-time%2520and%2520Realistic%2520Audio-driven%2520Face%2520Generation%2520with%25203D%250A%2520%2520Facial%2520Prior-guided%2520Identity%2520Alignment%2520Network%26entry.906535625%3DXiaozhong%2520Ji%2520and%2520Chuming%2520Lin%2520and%2520Zhonggan%2520Ding%2520and%2520Ying%2520Tai%2520and%2520Junwei%2520Zhu%2520and%2520Xiaobin%2520Hu%2520and%2520Donghao%2520Luo%2520and%2520Yanhao%2520Ge%2520and%2520Chengjie%2520Wang%26entry.1292438233%3D%2520%2520Person-generic%2520audio-driven%2520face%2520generation%2520is%2520a%2520challenging%2520task%2520in%2520computer%250Avision.%2520Previous%2520methods%2520have%2520achieved%2520remarkable%2520progress%2520in%2520audio-visual%250Asynchronization%252C%2520but%2520there%2520is%2520still%2520a%2520significant%2520gap%2520between%2520current%2520results%250Aand%2520practical%2520applications.%2520The%2520challenges%2520are%2520two-fold%253A%25201%2529%2520Preserving%2520unique%250Aindividual%2520traits%2520for%2520achieving%2520high-precision%2520lip%2520synchronization.%25202%2529%250AGenerating%2520high-quality%2520facial%2520renderings%2520in%2520real-time%2520performance.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520novel%2520generalized%2520audio-driven%2520framework%2520RealTalk%252C%2520which%250Aconsists%2520of%2520an%2520audio-to-expression%2520transformer%2520and%2520a%2520high-fidelity%250Aexpression-to-face%2520renderer.%2520In%2520the%2520first%2520component%252C%2520we%2520consider%2520both%2520identity%250Aand%2520intra-personal%2520variation%2520features%2520related%2520to%2520speaking%2520lip%2520movements.%2520By%250Aincorporating%2520cross-modal%2520attention%2520on%2520the%2520enriched%2520facial%2520priors%252C%2520we%2520can%250Aeffectively%2520align%2520lip%2520movements%2520with%2520audio%252C%2520thus%2520attaining%2520greater%2520precision%2520in%250Aexpression%2520prediction.%2520In%2520the%2520second%2520component%252C%2520we%2520design%2520a%2520lightweight%2520facial%250Aidentity%2520alignment%2520%2528FIA%2529%2520module%2520which%2520includes%2520a%2520lip-shape%2520control%2520structure%250Aand%2520a%2520face%2520texture%2520reference%2520structure.%2520This%2520novel%2520design%2520allows%2520us%2520to%2520generate%250Afine%2520details%2520in%2520real-time%252C%2520without%2520depending%2520on%2520sophisticated%2520and%2520inefficient%250Afeature%2520alignment%2520modules.%2520Our%2520experimental%2520results%252C%2520both%2520quantitative%2520and%250Aqualitative%252C%2520on%2520public%2520datasets%2520demonstrate%2520the%2520clear%2520advantages%2520of%2520our%2520method%250Ain%2520terms%2520of%2520lip-speech%2520synchronization%2520and%2520generation%2520quality.%2520Furthermore%252C%2520our%250Amethod%2520is%2520efficient%2520and%2520requires%2520fewer%2520computational%2520resources%252C%2520making%2520it%250Awell-suited%2520to%2520meet%2520the%2520needs%2520of%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18284v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RealTalk%3A%20Real-time%20and%20Realistic%20Audio-driven%20Face%20Generation%20with%203D%0A%20%20Facial%20Prior-guided%20Identity%20Alignment%20Network&entry.906535625=Xiaozhong%20Ji%20and%20Chuming%20Lin%20and%20Zhonggan%20Ding%20and%20Ying%20Tai%20and%20Junwei%20Zhu%20and%20Xiaobin%20Hu%20and%20Donghao%20Luo%20and%20Yanhao%20Ge%20and%20Chengjie%20Wang&entry.1292438233=%20%20Person-generic%20audio-driven%20face%20generation%20is%20a%20challenging%20task%20in%20computer%0Avision.%20Previous%20methods%20have%20achieved%20remarkable%20progress%20in%20audio-visual%0Asynchronization%2C%20but%20there%20is%20still%20a%20significant%20gap%20between%20current%20results%0Aand%20practical%20applications.%20The%20challenges%20are%20two-fold%3A%201%29%20Preserving%20unique%0Aindividual%20traits%20for%20achieving%20high-precision%20lip%20synchronization.%202%29%0AGenerating%20high-quality%20facial%20renderings%20in%20real-time%20performance.%20In%20this%0Apaper%2C%20we%20propose%20a%20novel%20generalized%20audio-driven%20framework%20RealTalk%2C%20which%0Aconsists%20of%20an%20audio-to-expression%20transformer%20and%20a%20high-fidelity%0Aexpression-to-face%20renderer.%20In%20the%20first%20component%2C%20we%20consider%20both%20identity%0Aand%20intra-personal%20variation%20features%20related%20to%20speaking%20lip%20movements.%20By%0Aincorporating%20cross-modal%20attention%20on%20the%20enriched%20facial%20priors%2C%20we%20can%0Aeffectively%20align%20lip%20movements%20with%20audio%2C%20thus%20attaining%20greater%20precision%20in%0Aexpression%20prediction.%20In%20the%20second%20component%2C%20we%20design%20a%20lightweight%20facial%0Aidentity%20alignment%20%28FIA%29%20module%20which%20includes%20a%20lip-shape%20control%20structure%0Aand%20a%20face%20texture%20reference%20structure.%20This%20novel%20design%20allows%20us%20to%20generate%0Afine%20details%20in%20real-time%2C%20without%20depending%20on%20sophisticated%20and%20inefficient%0Afeature%20alignment%20modules.%20Our%20experimental%20results%2C%20both%20quantitative%20and%0Aqualitative%2C%20on%20public%20datasets%20demonstrate%20the%20clear%20advantages%20of%20our%20method%0Ain%20terms%20of%20lip-speech%20synchronization%20and%20generation%20quality.%20Furthermore%2C%20our%0Amethod%20is%20efficient%20and%20requires%20fewer%20computational%20resources%2C%20making%20it%0Awell-suited%20to%20meet%20the%20needs%20of%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18284v2&entry.124074799=Read"},
{"title": "A Review of 3D Reconstruction Techniques for Deformable Tissues in\n  Robotic Surgery", "author": "Mengya Xu and Ziqi Guo and An Wang and Long Bai and Hongliang Ren", "abstract": "  As a crucial and intricate task in robotic minimally invasive surgery,\nreconstructing surgical scenes using stereo or monocular endoscopic video holds\nimmense potential for clinical applications. NeRF-based techniques have\nrecently garnered attention for the ability to reconstruct scenes implicitly.\nOn the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly\nusing 3D Gaussians and projects them onto a 2D plane as a replacement for the\ncomplex volume rendering in NeRF. However, these methods face challenges\nregarding surgical scene reconstruction, such as slow inference, dynamic\nscenes, and surgical tool occlusion. This work explores and reviews\nstate-of-the-art (SOTA) approaches, discussing their innovations and\nimplementation principles. Furthermore, we replicate the models and conduct\ntesting and evaluation on two datasets. The test results demonstrate that with\nadvancements in these techniques, achieving real-time, high-quality\nreconstructions becomes feasible.\n", "link": "http://arxiv.org/abs/2408.04426v1", "date": "2024-08-08", "relevancy": 3.0354, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6596}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5937}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20of%203D%20Reconstruction%20Techniques%20for%20Deformable%20Tissues%20in%0A%20%20Robotic%20Surgery&body=Title%3A%20A%20Review%20of%203D%20Reconstruction%20Techniques%20for%20Deformable%20Tissues%20in%0A%20%20Robotic%20Surgery%0AAuthor%3A%20Mengya%20Xu%20and%20Ziqi%20Guo%20and%20An%20Wang%20and%20Long%20Bai%20and%20Hongliang%20Ren%0AAbstract%3A%20%20%20As%20a%20crucial%20and%20intricate%20task%20in%20robotic%20minimally%20invasive%20surgery%2C%0Areconstructing%20surgical%20scenes%20using%20stereo%20or%20monocular%20endoscopic%20video%20holds%0Aimmense%20potential%20for%20clinical%20applications.%20NeRF-based%20techniques%20have%0Arecently%20garnered%20attention%20for%20the%20ability%20to%20reconstruct%20scenes%20implicitly.%0AOn%20the%20other%20hand%2C%20Gaussian%20splatting-based%203D-GS%20represents%20scenes%20explicitly%0Ausing%203D%20Gaussians%20and%20projects%20them%20onto%20a%202D%20plane%20as%20a%20replacement%20for%20the%0Acomplex%20volume%20rendering%20in%20NeRF.%20However%2C%20these%20methods%20face%20challenges%0Aregarding%20surgical%20scene%20reconstruction%2C%20such%20as%20slow%20inference%2C%20dynamic%0Ascenes%2C%20and%20surgical%20tool%20occlusion.%20This%20work%20explores%20and%20reviews%0Astate-of-the-art%20%28SOTA%29%20approaches%2C%20discussing%20their%20innovations%20and%0Aimplementation%20principles.%20Furthermore%2C%20we%20replicate%20the%20models%20and%20conduct%0Atesting%20and%20evaluation%20on%20two%20datasets.%20The%20test%20results%20demonstrate%20that%20with%0Aadvancements%20in%20these%20techniques%2C%20achieving%20real-time%2C%20high-quality%0Areconstructions%20becomes%20feasible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520of%25203D%2520Reconstruction%2520Techniques%2520for%2520Deformable%2520Tissues%2520in%250A%2520%2520Robotic%2520Surgery%26entry.906535625%3DMengya%2520Xu%2520and%2520Ziqi%2520Guo%2520and%2520An%2520Wang%2520and%2520Long%2520Bai%2520and%2520Hongliang%2520Ren%26entry.1292438233%3D%2520%2520As%2520a%2520crucial%2520and%2520intricate%2520task%2520in%2520robotic%2520minimally%2520invasive%2520surgery%252C%250Areconstructing%2520surgical%2520scenes%2520using%2520stereo%2520or%2520monocular%2520endoscopic%2520video%2520holds%250Aimmense%2520potential%2520for%2520clinical%2520applications.%2520NeRF-based%2520techniques%2520have%250Arecently%2520garnered%2520attention%2520for%2520the%2520ability%2520to%2520reconstruct%2520scenes%2520implicitly.%250AOn%2520the%2520other%2520hand%252C%2520Gaussian%2520splatting-based%25203D-GS%2520represents%2520scenes%2520explicitly%250Ausing%25203D%2520Gaussians%2520and%2520projects%2520them%2520onto%2520a%25202D%2520plane%2520as%2520a%2520replacement%2520for%2520the%250Acomplex%2520volume%2520rendering%2520in%2520NeRF.%2520However%252C%2520these%2520methods%2520face%2520challenges%250Aregarding%2520surgical%2520scene%2520reconstruction%252C%2520such%2520as%2520slow%2520inference%252C%2520dynamic%250Ascenes%252C%2520and%2520surgical%2520tool%2520occlusion.%2520This%2520work%2520explores%2520and%2520reviews%250Astate-of-the-art%2520%2528SOTA%2529%2520approaches%252C%2520discussing%2520their%2520innovations%2520and%250Aimplementation%2520principles.%2520Furthermore%252C%2520we%2520replicate%2520the%2520models%2520and%2520conduct%250Atesting%2520and%2520evaluation%2520on%2520two%2520datasets.%2520The%2520test%2520results%2520demonstrate%2520that%2520with%250Aadvancements%2520in%2520these%2520techniques%252C%2520achieving%2520real-time%252C%2520high-quality%250Areconstructions%2520becomes%2520feasible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20of%203D%20Reconstruction%20Techniques%20for%20Deformable%20Tissues%20in%0A%20%20Robotic%20Surgery&entry.906535625=Mengya%20Xu%20and%20Ziqi%20Guo%20and%20An%20Wang%20and%20Long%20Bai%20and%20Hongliang%20Ren&entry.1292438233=%20%20As%20a%20crucial%20and%20intricate%20task%20in%20robotic%20minimally%20invasive%20surgery%2C%0Areconstructing%20surgical%20scenes%20using%20stereo%20or%20monocular%20endoscopic%20video%20holds%0Aimmense%20potential%20for%20clinical%20applications.%20NeRF-based%20techniques%20have%0Arecently%20garnered%20attention%20for%20the%20ability%20to%20reconstruct%20scenes%20implicitly.%0AOn%20the%20other%20hand%2C%20Gaussian%20splatting-based%203D-GS%20represents%20scenes%20explicitly%0Ausing%203D%20Gaussians%20and%20projects%20them%20onto%20a%202D%20plane%20as%20a%20replacement%20for%20the%0Acomplex%20volume%20rendering%20in%20NeRF.%20However%2C%20these%20methods%20face%20challenges%0Aregarding%20surgical%20scene%20reconstruction%2C%20such%20as%20slow%20inference%2C%20dynamic%0Ascenes%2C%20and%20surgical%20tool%20occlusion.%20This%20work%20explores%20and%20reviews%0Astate-of-the-art%20%28SOTA%29%20approaches%2C%20discussing%20their%20innovations%20and%0Aimplementation%20principles.%20Furthermore%2C%20we%20replicate%20the%20models%20and%20conduct%0Atesting%20and%20evaluation%20on%20two%20datasets.%20The%20test%20results%20demonstrate%20that%20with%0Aadvancements%20in%20these%20techniques%2C%20achieving%20real-time%2C%20high-quality%0Areconstructions%20becomes%20feasible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04426v1&entry.124074799=Read"},
{"title": "MultiViPerFrOG: A Globally Optimized Multi-Viewpoint Perception\n  Framework for Camera Motion and Tissue Deformation", "author": "Guido Caccianiga and Julian Nubert and Cesar Cadena and Marco Hutter and Katherine J. Kuchenbecker", "abstract": "  Reconstructing the 3D shape of a deformable environment from the information\ncaptured by a moving depth camera is highly relevant to surgery. The underlying\nchallenge is the fact that simultaneously estimating camera motion and tissue\ndeformation in a fully deformable scene is an ill-posed problem, especially\nfrom a single arbitrarily moving viewpoint. Current solutions are often\norgan-specific and lack the robustness required to handle large deformations.\nHere we propose a multi-viewpoint global optimization framework that can\nflexibly integrate the output of low-level perception modules (data\nassociation, depth, and relative scene flow) with kinematic and scene-modeling\npriors to jointly estimate multiple camera motions and absolute scene flow. We\nuse simulated noisy data to show three practical examples that successfully\nconstrain the convergence to a unique solution. Overall, our method shows\nrobustness to combined noisy input measures and can process hundreds of points\nin a few milliseconds. MultiViPerFrOG builds a generalized learning-free\nscaffolding for spatio-temporal encoding that can unlock advanced surgical\nscene representations and will facilitate the development of the\ncomputer-assisted-surgery technologies of the future.\n", "link": "http://arxiv.org/abs/2408.04367v1", "date": "2024-08-08", "relevancy": 3.0176, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6139}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6139}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5828}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiViPerFrOG%3A%20A%20Globally%20Optimized%20Multi-Viewpoint%20Perception%0A%20%20Framework%20for%20Camera%20Motion%20and%20Tissue%20Deformation&body=Title%3A%20MultiViPerFrOG%3A%20A%20Globally%20Optimized%20Multi-Viewpoint%20Perception%0A%20%20Framework%20for%20Camera%20Motion%20and%20Tissue%20Deformation%0AAuthor%3A%20Guido%20Caccianiga%20and%20Julian%20Nubert%20and%20Cesar%20Cadena%20and%20Marco%20Hutter%20and%20Katherine%20J.%20Kuchenbecker%0AAbstract%3A%20%20%20Reconstructing%20the%203D%20shape%20of%20a%20deformable%20environment%20from%20the%20information%0Acaptured%20by%20a%20moving%20depth%20camera%20is%20highly%20relevant%20to%20surgery.%20The%20underlying%0Achallenge%20is%20the%20fact%20that%20simultaneously%20estimating%20camera%20motion%20and%20tissue%0Adeformation%20in%20a%20fully%20deformable%20scene%20is%20an%20ill-posed%20problem%2C%20especially%0Afrom%20a%20single%20arbitrarily%20moving%20viewpoint.%20Current%20solutions%20are%20often%0Aorgan-specific%20and%20lack%20the%20robustness%20required%20to%20handle%20large%20deformations.%0AHere%20we%20propose%20a%20multi-viewpoint%20global%20optimization%20framework%20that%20can%0Aflexibly%20integrate%20the%20output%20of%20low-level%20perception%20modules%20%28data%0Aassociation%2C%20depth%2C%20and%20relative%20scene%20flow%29%20with%20kinematic%20and%20scene-modeling%0Apriors%20to%20jointly%20estimate%20multiple%20camera%20motions%20and%20absolute%20scene%20flow.%20We%0Ause%20simulated%20noisy%20data%20to%20show%20three%20practical%20examples%20that%20successfully%0Aconstrain%20the%20convergence%20to%20a%20unique%20solution.%20Overall%2C%20our%20method%20shows%0Arobustness%20to%20combined%20noisy%20input%20measures%20and%20can%20process%20hundreds%20of%20points%0Ain%20a%20few%20milliseconds.%20MultiViPerFrOG%20builds%20a%20generalized%20learning-free%0Ascaffolding%20for%20spatio-temporal%20encoding%20that%20can%20unlock%20advanced%20surgical%0Ascene%20representations%20and%20will%20facilitate%20the%20development%20of%20the%0Acomputer-assisted-surgery%20technologies%20of%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiViPerFrOG%253A%2520A%2520Globally%2520Optimized%2520Multi-Viewpoint%2520Perception%250A%2520%2520Framework%2520for%2520Camera%2520Motion%2520and%2520Tissue%2520Deformation%26entry.906535625%3DGuido%2520Caccianiga%2520and%2520Julian%2520Nubert%2520and%2520Cesar%2520Cadena%2520and%2520Marco%2520Hutter%2520and%2520Katherine%2520J.%2520Kuchenbecker%26entry.1292438233%3D%2520%2520Reconstructing%2520the%25203D%2520shape%2520of%2520a%2520deformable%2520environment%2520from%2520the%2520information%250Acaptured%2520by%2520a%2520moving%2520depth%2520camera%2520is%2520highly%2520relevant%2520to%2520surgery.%2520The%2520underlying%250Achallenge%2520is%2520the%2520fact%2520that%2520simultaneously%2520estimating%2520camera%2520motion%2520and%2520tissue%250Adeformation%2520in%2520a%2520fully%2520deformable%2520scene%2520is%2520an%2520ill-posed%2520problem%252C%2520especially%250Afrom%2520a%2520single%2520arbitrarily%2520moving%2520viewpoint.%2520Current%2520solutions%2520are%2520often%250Aorgan-specific%2520and%2520lack%2520the%2520robustness%2520required%2520to%2520handle%2520large%2520deformations.%250AHere%2520we%2520propose%2520a%2520multi-viewpoint%2520global%2520optimization%2520framework%2520that%2520can%250Aflexibly%2520integrate%2520the%2520output%2520of%2520low-level%2520perception%2520modules%2520%2528data%250Aassociation%252C%2520depth%252C%2520and%2520relative%2520scene%2520flow%2529%2520with%2520kinematic%2520and%2520scene-modeling%250Apriors%2520to%2520jointly%2520estimate%2520multiple%2520camera%2520motions%2520and%2520absolute%2520scene%2520flow.%2520We%250Ause%2520simulated%2520noisy%2520data%2520to%2520show%2520three%2520practical%2520examples%2520that%2520successfully%250Aconstrain%2520the%2520convergence%2520to%2520a%2520unique%2520solution.%2520Overall%252C%2520our%2520method%2520shows%250Arobustness%2520to%2520combined%2520noisy%2520input%2520measures%2520and%2520can%2520process%2520hundreds%2520of%2520points%250Ain%2520a%2520few%2520milliseconds.%2520MultiViPerFrOG%2520builds%2520a%2520generalized%2520learning-free%250Ascaffolding%2520for%2520spatio-temporal%2520encoding%2520that%2520can%2520unlock%2520advanced%2520surgical%250Ascene%2520representations%2520and%2520will%2520facilitate%2520the%2520development%2520of%2520the%250Acomputer-assisted-surgery%2520technologies%2520of%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiViPerFrOG%3A%20A%20Globally%20Optimized%20Multi-Viewpoint%20Perception%0A%20%20Framework%20for%20Camera%20Motion%20and%20Tissue%20Deformation&entry.906535625=Guido%20Caccianiga%20and%20Julian%20Nubert%20and%20Cesar%20Cadena%20and%20Marco%20Hutter%20and%20Katherine%20J.%20Kuchenbecker&entry.1292438233=%20%20Reconstructing%20the%203D%20shape%20of%20a%20deformable%20environment%20from%20the%20information%0Acaptured%20by%20a%20moving%20depth%20camera%20is%20highly%20relevant%20to%20surgery.%20The%20underlying%0Achallenge%20is%20the%20fact%20that%20simultaneously%20estimating%20camera%20motion%20and%20tissue%0Adeformation%20in%20a%20fully%20deformable%20scene%20is%20an%20ill-posed%20problem%2C%20especially%0Afrom%20a%20single%20arbitrarily%20moving%20viewpoint.%20Current%20solutions%20are%20often%0Aorgan-specific%20and%20lack%20the%20robustness%20required%20to%20handle%20large%20deformations.%0AHere%20we%20propose%20a%20multi-viewpoint%20global%20optimization%20framework%20that%20can%0Aflexibly%20integrate%20the%20output%20of%20low-level%20perception%20modules%20%28data%0Aassociation%2C%20depth%2C%20and%20relative%20scene%20flow%29%20with%20kinematic%20and%20scene-modeling%0Apriors%20to%20jointly%20estimate%20multiple%20camera%20motions%20and%20absolute%20scene%20flow.%20We%0Ause%20simulated%20noisy%20data%20to%20show%20three%20practical%20examples%20that%20successfully%0Aconstrain%20the%20convergence%20to%20a%20unique%20solution.%20Overall%2C%20our%20method%20shows%0Arobustness%20to%20combined%20noisy%20input%20measures%20and%20can%20process%20hundreds%20of%20points%0Ain%20a%20few%20milliseconds.%20MultiViPerFrOG%20builds%20a%20generalized%20learning-free%0Ascaffolding%20for%20spatio-temporal%20encoding%20that%20can%20unlock%20advanced%20surgical%0Ascene%20representations%20and%20will%20facilitate%20the%20development%20of%20the%0Acomputer-assisted-surgery%20technologies%20of%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04367v1&entry.124074799=Read"},
{"title": "Sampling for View Synthesis: From Local Light Field Fusion to Neural\n  Radiance Fields and Beyond", "author": "Ravi Ramamoorthi", "abstract": "  Capturing and rendering novel views of complex real-world scenes is a\nlong-standing problem in computer graphics and vision, with applications in\naugmented and virtual reality, immersive experiences and 3D photography. The\nadvent of deep learning has enabled revolutionary advances in this area,\nclassically known as image-based rendering. However, previous approaches\nrequire intractably dense view sampling or provide little or no guidance for\nhow users should sample views of a scene to reliably render high-quality novel\nviews. Local light field fusion proposes an algorithm for practical view\nsynthesis from an irregular grid of sampled views that first expands each\nsampled view into a local light field via a multiplane image scene\nrepresentation, then renders novel views by blending adjacent local light\nfields. Crucially, we extend traditional plenoptic sampling theory to derive a\nbound that specifies precisely how densely users should sample views of a given\nscene when using our algorithm. We achieve the perceptual quality of Nyquist\nrate view sampling while using up to 4000x fewer views. Subsequent developments\nhave led to new scene representations for deep learning with view synthesis,\nnotably neural radiance fields, but the problem of sparse view synthesis from a\nsmall number of images has only grown in importance. We reprise some of the\nrecent results on sparse and even single image view synthesis, while posing the\nquestion of whether prescriptive sampling guidelines are feasible for the new\ngeneration of image-based rendering algorithms.\n", "link": "http://arxiv.org/abs/2408.04586v1", "date": "2024-08-08", "relevancy": 2.9219, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.592}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.592}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling%20for%20View%20Synthesis%3A%20From%20Local%20Light%20Field%20Fusion%20to%20Neural%0A%20%20Radiance%20Fields%20and%20Beyond&body=Title%3A%20Sampling%20for%20View%20Synthesis%3A%20From%20Local%20Light%20Field%20Fusion%20to%20Neural%0A%20%20Radiance%20Fields%20and%20Beyond%0AAuthor%3A%20Ravi%20Ramamoorthi%0AAbstract%3A%20%20%20Capturing%20and%20rendering%20novel%20views%20of%20complex%20real-world%20scenes%20is%20a%0Along-standing%20problem%20in%20computer%20graphics%20and%20vision%2C%20with%20applications%20in%0Aaugmented%20and%20virtual%20reality%2C%20immersive%20experiences%20and%203D%20photography.%20The%0Aadvent%20of%20deep%20learning%20has%20enabled%20revolutionary%20advances%20in%20this%20area%2C%0Aclassically%20known%20as%20image-based%20rendering.%20However%2C%20previous%20approaches%0Arequire%20intractably%20dense%20view%20sampling%20or%20provide%20little%20or%20no%20guidance%20for%0Ahow%20users%20should%20sample%20views%20of%20a%20scene%20to%20reliably%20render%20high-quality%20novel%0Aviews.%20Local%20light%20field%20fusion%20proposes%20an%20algorithm%20for%20practical%20view%0Asynthesis%20from%20an%20irregular%20grid%20of%20sampled%20views%20that%20first%20expands%20each%0Asampled%20view%20into%20a%20local%20light%20field%20via%20a%20multiplane%20image%20scene%0Arepresentation%2C%20then%20renders%20novel%20views%20by%20blending%20adjacent%20local%20light%0Afields.%20Crucially%2C%20we%20extend%20traditional%20plenoptic%20sampling%20theory%20to%20derive%20a%0Abound%20that%20specifies%20precisely%20how%20densely%20users%20should%20sample%20views%20of%20a%20given%0Ascene%20when%20using%20our%20algorithm.%20We%20achieve%20the%20perceptual%20quality%20of%20Nyquist%0Arate%20view%20sampling%20while%20using%20up%20to%204000x%20fewer%20views.%20Subsequent%20developments%0Ahave%20led%20to%20new%20scene%20representations%20for%20deep%20learning%20with%20view%20synthesis%2C%0Anotably%20neural%20radiance%20fields%2C%20but%20the%20problem%20of%20sparse%20view%20synthesis%20from%20a%0Asmall%20number%20of%20images%20has%20only%20grown%20in%20importance.%20We%20reprise%20some%20of%20the%0Arecent%20results%20on%20sparse%20and%20even%20single%20image%20view%20synthesis%2C%20while%20posing%20the%0Aquestion%20of%20whether%20prescriptive%20sampling%20guidelines%20are%20feasible%20for%20the%20new%0Ageneration%20of%20image-based%20rendering%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling%2520for%2520View%2520Synthesis%253A%2520From%2520Local%2520Light%2520Field%2520Fusion%2520to%2520Neural%250A%2520%2520Radiance%2520Fields%2520and%2520Beyond%26entry.906535625%3DRavi%2520Ramamoorthi%26entry.1292438233%3D%2520%2520Capturing%2520and%2520rendering%2520novel%2520views%2520of%2520complex%2520real-world%2520scenes%2520is%2520a%250Along-standing%2520problem%2520in%2520computer%2520graphics%2520and%2520vision%252C%2520with%2520applications%2520in%250Aaugmented%2520and%2520virtual%2520reality%252C%2520immersive%2520experiences%2520and%25203D%2520photography.%2520The%250Aadvent%2520of%2520deep%2520learning%2520has%2520enabled%2520revolutionary%2520advances%2520in%2520this%2520area%252C%250Aclassically%2520known%2520as%2520image-based%2520rendering.%2520However%252C%2520previous%2520approaches%250Arequire%2520intractably%2520dense%2520view%2520sampling%2520or%2520provide%2520little%2520or%2520no%2520guidance%2520for%250Ahow%2520users%2520should%2520sample%2520views%2520of%2520a%2520scene%2520to%2520reliably%2520render%2520high-quality%2520novel%250Aviews.%2520Local%2520light%2520field%2520fusion%2520proposes%2520an%2520algorithm%2520for%2520practical%2520view%250Asynthesis%2520from%2520an%2520irregular%2520grid%2520of%2520sampled%2520views%2520that%2520first%2520expands%2520each%250Asampled%2520view%2520into%2520a%2520local%2520light%2520field%2520via%2520a%2520multiplane%2520image%2520scene%250Arepresentation%252C%2520then%2520renders%2520novel%2520views%2520by%2520blending%2520adjacent%2520local%2520light%250Afields.%2520Crucially%252C%2520we%2520extend%2520traditional%2520plenoptic%2520sampling%2520theory%2520to%2520derive%2520a%250Abound%2520that%2520specifies%2520precisely%2520how%2520densely%2520users%2520should%2520sample%2520views%2520of%2520a%2520given%250Ascene%2520when%2520using%2520our%2520algorithm.%2520We%2520achieve%2520the%2520perceptual%2520quality%2520of%2520Nyquist%250Arate%2520view%2520sampling%2520while%2520using%2520up%2520to%25204000x%2520fewer%2520views.%2520Subsequent%2520developments%250Ahave%2520led%2520to%2520new%2520scene%2520representations%2520for%2520deep%2520learning%2520with%2520view%2520synthesis%252C%250Anotably%2520neural%2520radiance%2520fields%252C%2520but%2520the%2520problem%2520of%2520sparse%2520view%2520synthesis%2520from%2520a%250Asmall%2520number%2520of%2520images%2520has%2520only%2520grown%2520in%2520importance.%2520We%2520reprise%2520some%2520of%2520the%250Arecent%2520results%2520on%2520sparse%2520and%2520even%2520single%2520image%2520view%2520synthesis%252C%2520while%2520posing%2520the%250Aquestion%2520of%2520whether%2520prescriptive%2520sampling%2520guidelines%2520are%2520feasible%2520for%2520the%2520new%250Ageneration%2520of%2520image-based%2520rendering%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling%20for%20View%20Synthesis%3A%20From%20Local%20Light%20Field%20Fusion%20to%20Neural%0A%20%20Radiance%20Fields%20and%20Beyond&entry.906535625=Ravi%20Ramamoorthi&entry.1292438233=%20%20Capturing%20and%20rendering%20novel%20views%20of%20complex%20real-world%20scenes%20is%20a%0Along-standing%20problem%20in%20computer%20graphics%20and%20vision%2C%20with%20applications%20in%0Aaugmented%20and%20virtual%20reality%2C%20immersive%20experiences%20and%203D%20photography.%20The%0Aadvent%20of%20deep%20learning%20has%20enabled%20revolutionary%20advances%20in%20this%20area%2C%0Aclassically%20known%20as%20image-based%20rendering.%20However%2C%20previous%20approaches%0Arequire%20intractably%20dense%20view%20sampling%20or%20provide%20little%20or%20no%20guidance%20for%0Ahow%20users%20should%20sample%20views%20of%20a%20scene%20to%20reliably%20render%20high-quality%20novel%0Aviews.%20Local%20light%20field%20fusion%20proposes%20an%20algorithm%20for%20practical%20view%0Asynthesis%20from%20an%20irregular%20grid%20of%20sampled%20views%20that%20first%20expands%20each%0Asampled%20view%20into%20a%20local%20light%20field%20via%20a%20multiplane%20image%20scene%0Arepresentation%2C%20then%20renders%20novel%20views%20by%20blending%20adjacent%20local%20light%0Afields.%20Crucially%2C%20we%20extend%20traditional%20plenoptic%20sampling%20theory%20to%20derive%20a%0Abound%20that%20specifies%20precisely%20how%20densely%20users%20should%20sample%20views%20of%20a%20given%0Ascene%20when%20using%20our%20algorithm.%20We%20achieve%20the%20perceptual%20quality%20of%20Nyquist%0Arate%20view%20sampling%20while%20using%20up%20to%204000x%20fewer%20views.%20Subsequent%20developments%0Ahave%20led%20to%20new%20scene%20representations%20for%20deep%20learning%20with%20view%20synthesis%2C%0Anotably%20neural%20radiance%20fields%2C%20but%20the%20problem%20of%20sparse%20view%20synthesis%20from%20a%0Asmall%20number%20of%20images%20has%20only%20grown%20in%20importance.%20We%20reprise%20some%20of%20the%0Arecent%20results%20on%20sparse%20and%20even%20single%20image%20view%20synthesis%2C%20while%20posing%20the%0Aquestion%20of%20whether%20prescriptive%20sampling%20guidelines%20are%20feasible%20for%20the%20new%0Ageneration%20of%20image-based%20rendering%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04586v1&entry.124074799=Read"},
{"title": "TPA3D: Triplane Attention for Fast Text-to-3D Generation", "author": "Bin-Shih Wu and Hong-En Chen and Sheng-Yu Huang and Yu-Chiang Frank Wang", "abstract": "  Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed.\n", "link": "http://arxiv.org/abs/2312.02647v2", "date": "2024-08-08", "relevancy": 2.9151, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5914}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5788}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TPA3D%3A%20Triplane%20Attention%20for%20Fast%20Text-to-3D%20Generation&body=Title%3A%20TPA3D%3A%20Triplane%20Attention%20for%20Fast%20Text-to-3D%20Generation%0AAuthor%3A%20Bin-Shih%20Wu%20and%20Hong-En%20Chen%20and%20Sheng-Yu%20Huang%20and%20Yu-Chiang%20Frank%20Wang%0AAbstract%3A%20%20%20Due%20to%20the%20lack%20of%20large-scale%20text-3D%20correspondence%20data%2C%20recent%20text-to-3D%0Ageneration%20works%20mainly%20rely%20on%20utilizing%202D%20diffusion%20models%20for%20synthesizing%0A3D%20data.%20Since%20diffusion-based%20methods%20typically%20require%20significant%0Aoptimization%20time%20for%20both%20training%20and%20inference%2C%20the%20use%20of%20GAN-based%20models%0Awould%20still%20be%20desirable%20for%20fast%203D%20generation.%20In%20this%20work%2C%20we%20propose%0ATriplane%20Attention%20for%20text-guided%203D%20generation%20%28TPA3D%29%2C%20an%20end-to-end%0Atrainable%20GAN-based%20deep%20learning%20model%20for%20fast%20text-to-3D%20generation.%20With%0Aonly%203D%20shape%20data%20and%20their%20rendered%202D%20images%20observed%20during%20training%2C%20our%0ATPA3D%20is%20designed%20to%20retrieve%20detailed%20visual%20descriptions%20for%20synthesizing%20the%0Acorresponding%203D%20mesh%20data.%20This%20is%20achieved%20by%20the%20proposed%20attention%0Amechanisms%20on%20the%20extracted%20sentence%20and%20word-level%20text%20features.%20In%20our%0Aexperiments%2C%20we%20show%20that%20TPA3D%20generates%20high-quality%203D%20textured%20shapes%0Aaligned%20with%20fine-grained%20descriptions%2C%20while%20impressive%20computation%20efficiency%0Acan%20be%20observed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02647v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTPA3D%253A%2520Triplane%2520Attention%2520for%2520Fast%2520Text-to-3D%2520Generation%26entry.906535625%3DBin-Shih%2520Wu%2520and%2520Hong-En%2520Chen%2520and%2520Sheng-Yu%2520Huang%2520and%2520Yu-Chiang%2520Frank%2520Wang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520lack%2520of%2520large-scale%2520text-3D%2520correspondence%2520data%252C%2520recent%2520text-to-3D%250Ageneration%2520works%2520mainly%2520rely%2520on%2520utilizing%25202D%2520diffusion%2520models%2520for%2520synthesizing%250A3D%2520data.%2520Since%2520diffusion-based%2520methods%2520typically%2520require%2520significant%250Aoptimization%2520time%2520for%2520both%2520training%2520and%2520inference%252C%2520the%2520use%2520of%2520GAN-based%2520models%250Awould%2520still%2520be%2520desirable%2520for%2520fast%25203D%2520generation.%2520In%2520this%2520work%252C%2520we%2520propose%250ATriplane%2520Attention%2520for%2520text-guided%25203D%2520generation%2520%2528TPA3D%2529%252C%2520an%2520end-to-end%250Atrainable%2520GAN-based%2520deep%2520learning%2520model%2520for%2520fast%2520text-to-3D%2520generation.%2520With%250Aonly%25203D%2520shape%2520data%2520and%2520their%2520rendered%25202D%2520images%2520observed%2520during%2520training%252C%2520our%250ATPA3D%2520is%2520designed%2520to%2520retrieve%2520detailed%2520visual%2520descriptions%2520for%2520synthesizing%2520the%250Acorresponding%25203D%2520mesh%2520data.%2520This%2520is%2520achieved%2520by%2520the%2520proposed%2520attention%250Amechanisms%2520on%2520the%2520extracted%2520sentence%2520and%2520word-level%2520text%2520features.%2520In%2520our%250Aexperiments%252C%2520we%2520show%2520that%2520TPA3D%2520generates%2520high-quality%25203D%2520textured%2520shapes%250Aaligned%2520with%2520fine-grained%2520descriptions%252C%2520while%2520impressive%2520computation%2520efficiency%250Acan%2520be%2520observed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02647v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TPA3D%3A%20Triplane%20Attention%20for%20Fast%20Text-to-3D%20Generation&entry.906535625=Bin-Shih%20Wu%20and%20Hong-En%20Chen%20and%20Sheng-Yu%20Huang%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=%20%20Due%20to%20the%20lack%20of%20large-scale%20text-3D%20correspondence%20data%2C%20recent%20text-to-3D%0Ageneration%20works%20mainly%20rely%20on%20utilizing%202D%20diffusion%20models%20for%20synthesizing%0A3D%20data.%20Since%20diffusion-based%20methods%20typically%20require%20significant%0Aoptimization%20time%20for%20both%20training%20and%20inference%2C%20the%20use%20of%20GAN-based%20models%0Awould%20still%20be%20desirable%20for%20fast%203D%20generation.%20In%20this%20work%2C%20we%20propose%0ATriplane%20Attention%20for%20text-guided%203D%20generation%20%28TPA3D%29%2C%20an%20end-to-end%0Atrainable%20GAN-based%20deep%20learning%20model%20for%20fast%20text-to-3D%20generation.%20With%0Aonly%203D%20shape%20data%20and%20their%20rendered%202D%20images%20observed%20during%20training%2C%20our%0ATPA3D%20is%20designed%20to%20retrieve%20detailed%20visual%20descriptions%20for%20synthesizing%20the%0Acorresponding%203D%20mesh%20data.%20This%20is%20achieved%20by%20the%20proposed%20attention%0Amechanisms%20on%20the%20extracted%20sentence%20and%20word-level%20text%20features.%20In%20our%0Aexperiments%2C%20we%20show%20that%20TPA3D%20generates%20high-quality%203D%20textured%20shapes%0Aaligned%20with%20fine-grained%20descriptions%2C%20while%20impressive%20computation%20efficiency%0Acan%20be%20observed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02647v2&entry.124074799=Read"},
{"title": "Towards High-resolution 3D Anomaly Detection via Group-Level Feature\n  Contrastive Learning", "author": "Hongze Zhu and Guoyang Xie and Chengbin Hou and Tao Dai and Can Gao and Jinbao Wang and Linlin Shen", "abstract": "  High-resolution point clouds~(HRPCD) anomaly detection~(AD) plays a critical\nrole in precision machining and high-end equipment manufacturing. Despite\nconsiderable 3D-AD methods that have been proposed recently, they still cannot\nmeet the requirements of the HRPCD-AD task. There are several challenges: i) It\nis difficult to directly capture HRPCD information due to large amounts of\npoints at the sample level; ii) The advanced transformer-based methods usually\nobtain anisotropic features, leading to degradation of the representation; iii)\nThe proportion of abnormal areas is very small, which makes it difficult to\ncharacterize. To address these challenges, we propose a novel group-level\nfeature-based network, called Group3AD, which has a significantly efficient\nrepresentation ability. First, we design an Intercluster Uniformity\nNetwork~(IUN) to present the mapping of different groups in the feature space\nas several clusters, and obtain a more uniform distribution between clusters\nrepresenting different parts of the point clouds in the feature space. Then, an\nIntracluster Alignment Network~(IAN) is designed to encourage groups within the\ncluster to be distributed tightly in the feature space. In addition, we propose\nan Adaptive Group-Center Selection~(AGCS) based on geometric information to\nimprove the pixel density of potential anomalous regions during inference. The\nexperimental results verify the effectiveness of our proposed Group3AD, which\nsurpasses Reg3D-AD by the margin of 5\\% in terms of object-level AUROC on\nReal3D-AD. We provide the code and supplementary information on our website:\nhttps://github.com/M-3LAB/Group3AD.\n", "link": "http://arxiv.org/abs/2408.04604v1", "date": "2024-08-08", "relevancy": 2.8806, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6142}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5575}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20High-resolution%203D%20Anomaly%20Detection%20via%20Group-Level%20Feature%0A%20%20Contrastive%20Learning&body=Title%3A%20Towards%20High-resolution%203D%20Anomaly%20Detection%20via%20Group-Level%20Feature%0A%20%20Contrastive%20Learning%0AAuthor%3A%20Hongze%20Zhu%20and%20Guoyang%20Xie%20and%20Chengbin%20Hou%20and%20Tao%20Dai%20and%20Can%20Gao%20and%20Jinbao%20Wang%20and%20Linlin%20Shen%0AAbstract%3A%20%20%20High-resolution%20point%20clouds~%28HRPCD%29%20anomaly%20detection~%28AD%29%20plays%20a%20critical%0Arole%20in%20precision%20machining%20and%20high-end%20equipment%20manufacturing.%20Despite%0Aconsiderable%203D-AD%20methods%20that%20have%20been%20proposed%20recently%2C%20they%20still%20cannot%0Ameet%20the%20requirements%20of%20the%20HRPCD-AD%20task.%20There%20are%20several%20challenges%3A%20i%29%20It%0Ais%20difficult%20to%20directly%20capture%20HRPCD%20information%20due%20to%20large%20amounts%20of%0Apoints%20at%20the%20sample%20level%3B%20ii%29%20The%20advanced%20transformer-based%20methods%20usually%0Aobtain%20anisotropic%20features%2C%20leading%20to%20degradation%20of%20the%20representation%3B%20iii%29%0AThe%20proportion%20of%20abnormal%20areas%20is%20very%20small%2C%20which%20makes%20it%20difficult%20to%0Acharacterize.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20group-level%0Afeature-based%20network%2C%20called%20Group3AD%2C%20which%20has%20a%20significantly%20efficient%0Arepresentation%20ability.%20First%2C%20we%20design%20an%20Intercluster%20Uniformity%0ANetwork~%28IUN%29%20to%20present%20the%20mapping%20of%20different%20groups%20in%20the%20feature%20space%0Aas%20several%20clusters%2C%20and%20obtain%20a%20more%20uniform%20distribution%20between%20clusters%0Arepresenting%20different%20parts%20of%20the%20point%20clouds%20in%20the%20feature%20space.%20Then%2C%20an%0AIntracluster%20Alignment%20Network~%28IAN%29%20is%20designed%20to%20encourage%20groups%20within%20the%0Acluster%20to%20be%20distributed%20tightly%20in%20the%20feature%20space.%20In%20addition%2C%20we%20propose%0Aan%20Adaptive%20Group-Center%20Selection~%28AGCS%29%20based%20on%20geometric%20information%20to%0Aimprove%20the%20pixel%20density%20of%20potential%20anomalous%20regions%20during%20inference.%20The%0Aexperimental%20results%20verify%20the%20effectiveness%20of%20our%20proposed%20Group3AD%2C%20which%0Asurpasses%20Reg3D-AD%20by%20the%20margin%20of%205%5C%25%20in%20terms%20of%20object-level%20AUROC%20on%0AReal3D-AD.%20We%20provide%20the%20code%20and%20supplementary%20information%20on%20our%20website%3A%0Ahttps%3A//github.com/M-3LAB/Group3AD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520High-resolution%25203D%2520Anomaly%2520Detection%2520via%2520Group-Level%2520Feature%250A%2520%2520Contrastive%2520Learning%26entry.906535625%3DHongze%2520Zhu%2520and%2520Guoyang%2520Xie%2520and%2520Chengbin%2520Hou%2520and%2520Tao%2520Dai%2520and%2520Can%2520Gao%2520and%2520Jinbao%2520Wang%2520and%2520Linlin%2520Shen%26entry.1292438233%3D%2520%2520High-resolution%2520point%2520clouds~%2528HRPCD%2529%2520anomaly%2520detection~%2528AD%2529%2520plays%2520a%2520critical%250Arole%2520in%2520precision%2520machining%2520and%2520high-end%2520equipment%2520manufacturing.%2520Despite%250Aconsiderable%25203D-AD%2520methods%2520that%2520have%2520been%2520proposed%2520recently%252C%2520they%2520still%2520cannot%250Ameet%2520the%2520requirements%2520of%2520the%2520HRPCD-AD%2520task.%2520There%2520are%2520several%2520challenges%253A%2520i%2529%2520It%250Ais%2520difficult%2520to%2520directly%2520capture%2520HRPCD%2520information%2520due%2520to%2520large%2520amounts%2520of%250Apoints%2520at%2520the%2520sample%2520level%253B%2520ii%2529%2520The%2520advanced%2520transformer-based%2520methods%2520usually%250Aobtain%2520anisotropic%2520features%252C%2520leading%2520to%2520degradation%2520of%2520the%2520representation%253B%2520iii%2529%250AThe%2520proportion%2520of%2520abnormal%2520areas%2520is%2520very%2520small%252C%2520which%2520makes%2520it%2520difficult%2520to%250Acharacterize.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520group-level%250Afeature-based%2520network%252C%2520called%2520Group3AD%252C%2520which%2520has%2520a%2520significantly%2520efficient%250Arepresentation%2520ability.%2520First%252C%2520we%2520design%2520an%2520Intercluster%2520Uniformity%250ANetwork~%2528IUN%2529%2520to%2520present%2520the%2520mapping%2520of%2520different%2520groups%2520in%2520the%2520feature%2520space%250Aas%2520several%2520clusters%252C%2520and%2520obtain%2520a%2520more%2520uniform%2520distribution%2520between%2520clusters%250Arepresenting%2520different%2520parts%2520of%2520the%2520point%2520clouds%2520in%2520the%2520feature%2520space.%2520Then%252C%2520an%250AIntracluster%2520Alignment%2520Network~%2528IAN%2529%2520is%2520designed%2520to%2520encourage%2520groups%2520within%2520the%250Acluster%2520to%2520be%2520distributed%2520tightly%2520in%2520the%2520feature%2520space.%2520In%2520addition%252C%2520we%2520propose%250Aan%2520Adaptive%2520Group-Center%2520Selection~%2528AGCS%2529%2520based%2520on%2520geometric%2520information%2520to%250Aimprove%2520the%2520pixel%2520density%2520of%2520potential%2520anomalous%2520regions%2520during%2520inference.%2520The%250Aexperimental%2520results%2520verify%2520the%2520effectiveness%2520of%2520our%2520proposed%2520Group3AD%252C%2520which%250Asurpasses%2520Reg3D-AD%2520by%2520the%2520margin%2520of%25205%255C%2525%2520in%2520terms%2520of%2520object-level%2520AUROC%2520on%250AReal3D-AD.%2520We%2520provide%2520the%2520code%2520and%2520supplementary%2520information%2520on%2520our%2520website%253A%250Ahttps%253A//github.com/M-3LAB/Group3AD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20High-resolution%203D%20Anomaly%20Detection%20via%20Group-Level%20Feature%0A%20%20Contrastive%20Learning&entry.906535625=Hongze%20Zhu%20and%20Guoyang%20Xie%20and%20Chengbin%20Hou%20and%20Tao%20Dai%20and%20Can%20Gao%20and%20Jinbao%20Wang%20and%20Linlin%20Shen&entry.1292438233=%20%20High-resolution%20point%20clouds~%28HRPCD%29%20anomaly%20detection~%28AD%29%20plays%20a%20critical%0Arole%20in%20precision%20machining%20and%20high-end%20equipment%20manufacturing.%20Despite%0Aconsiderable%203D-AD%20methods%20that%20have%20been%20proposed%20recently%2C%20they%20still%20cannot%0Ameet%20the%20requirements%20of%20the%20HRPCD-AD%20task.%20There%20are%20several%20challenges%3A%20i%29%20It%0Ais%20difficult%20to%20directly%20capture%20HRPCD%20information%20due%20to%20large%20amounts%20of%0Apoints%20at%20the%20sample%20level%3B%20ii%29%20The%20advanced%20transformer-based%20methods%20usually%0Aobtain%20anisotropic%20features%2C%20leading%20to%20degradation%20of%20the%20representation%3B%20iii%29%0AThe%20proportion%20of%20abnormal%20areas%20is%20very%20small%2C%20which%20makes%20it%20difficult%20to%0Acharacterize.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20group-level%0Afeature-based%20network%2C%20called%20Group3AD%2C%20which%20has%20a%20significantly%20efficient%0Arepresentation%20ability.%20First%2C%20we%20design%20an%20Intercluster%20Uniformity%0ANetwork~%28IUN%29%20to%20present%20the%20mapping%20of%20different%20groups%20in%20the%20feature%20space%0Aas%20several%20clusters%2C%20and%20obtain%20a%20more%20uniform%20distribution%20between%20clusters%0Arepresenting%20different%20parts%20of%20the%20point%20clouds%20in%20the%20feature%20space.%20Then%2C%20an%0AIntracluster%20Alignment%20Network~%28IAN%29%20is%20designed%20to%20encourage%20groups%20within%20the%0Acluster%20to%20be%20distributed%20tightly%20in%20the%20feature%20space.%20In%20addition%2C%20we%20propose%0Aan%20Adaptive%20Group-Center%20Selection~%28AGCS%29%20based%20on%20geometric%20information%20to%0Aimprove%20the%20pixel%20density%20of%20potential%20anomalous%20regions%20during%20inference.%20The%0Aexperimental%20results%20verify%20the%20effectiveness%20of%20our%20proposed%20Group3AD%2C%20which%0Asurpasses%20Reg3D-AD%20by%20the%20margin%20of%205%5C%25%20in%20terms%20of%20object-level%20AUROC%20on%0AReal3D-AD.%20We%20provide%20the%20code%20and%20supplementary%20information%20on%20our%20website%3A%0Ahttps%3A//github.com/M-3LAB/Group3AD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04604v1&entry.124074799=Read"},
{"title": "DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization", "author": "Xin Sun and Liang Wang and Qiang Liu and Shu Wu and Zilei Wang and Liang Wang", "abstract": "  This paper addresses the challenge of out-of-distribution (OOD)\ngeneralization in graph machine learning, a field rapidly advancing yet\ngrappling with the discrepancy between source and target data distributions.\nTraditional graph learning algorithms, based on the assumption of uniform\ndistribution between training and test data, falter in real-world scenarios\nwhere this assumption fails, resulting in suboptimal performance. A principal\nfactor contributing to this suboptimal performance is the inherent simplicity\nbias of neural networks trained through Stochastic Gradient Descent (SGD),\nwhich prefer simpler features over more complex yet equally or more predictive\nones. This bias leads to a reliance on spurious correlations, adversely\naffecting OOD performance in various tasks such as image recognition, natural\nlanguage understanding, and graph classification. Current methodologies,\nincluding subgraph-mixup and information bottleneck approaches, have achieved\npartial success but struggle to overcome simplicity bias, often reinforcing\nspurious correlations. To tackle this, we propose DIVE, training a collection\nof models to focus on all label-predictive subgraphs by encouraging the models\nto foster divergence on the subgraph mask, which circumvents the limitation of\na model solely focusing on the subgraph corresponding to simple structural\npatterns. Specifically, we employs a regularizer to punish overlap in extracted\nsubgraphs across models, thereby encouraging different models to concentrate on\ndistinct structural patterns. Model selection for robust OOD performance is\nachieved through validation accuracy. Tested across four datasets from GOOD\nbenchmark and one dataset from DrugOOD benchmark, our approach demonstrates\nsignificant improvement over existing methods, effectively addressing the\nsimplicity bias and enhancing generalization in graph machine learning.\n", "link": "http://arxiv.org/abs/2408.04400v1", "date": "2024-08-08", "relevancy": 2.6425, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5535}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5209}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIVE%3A%20Subgraph%20Disagreement%20for%20Graph%20Out-of-Distribution%20Generalization&body=Title%3A%20DIVE%3A%20Subgraph%20Disagreement%20for%20Graph%20Out-of-Distribution%20Generalization%0AAuthor%3A%20Xin%20Sun%20and%20Liang%20Wang%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Zilei%20Wang%20and%20Liang%20Wang%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20out-of-distribution%20%28OOD%29%0Ageneralization%20in%20graph%20machine%20learning%2C%20a%20field%20rapidly%20advancing%20yet%0Agrappling%20with%20the%20discrepancy%20between%20source%20and%20target%20data%20distributions.%0ATraditional%20graph%20learning%20algorithms%2C%20based%20on%20the%20assumption%20of%20uniform%0Adistribution%20between%20training%20and%20test%20data%2C%20falter%20in%20real-world%20scenarios%0Awhere%20this%20assumption%20fails%2C%20resulting%20in%20suboptimal%20performance.%20A%20principal%0Afactor%20contributing%20to%20this%20suboptimal%20performance%20is%20the%20inherent%20simplicity%0Abias%20of%20neural%20networks%20trained%20through%20Stochastic%20Gradient%20Descent%20%28SGD%29%2C%0Awhich%20prefer%20simpler%20features%20over%20more%20complex%20yet%20equally%20or%20more%20predictive%0Aones.%20This%20bias%20leads%20to%20a%20reliance%20on%20spurious%20correlations%2C%20adversely%0Aaffecting%20OOD%20performance%20in%20various%20tasks%20such%20as%20image%20recognition%2C%20natural%0Alanguage%20understanding%2C%20and%20graph%20classification.%20Current%20methodologies%2C%0Aincluding%20subgraph-mixup%20and%20information%20bottleneck%20approaches%2C%20have%20achieved%0Apartial%20success%20but%20struggle%20to%20overcome%20simplicity%20bias%2C%20often%20reinforcing%0Aspurious%20correlations.%20To%20tackle%20this%2C%20we%20propose%20DIVE%2C%20training%20a%20collection%0Aof%20models%20to%20focus%20on%20all%20label-predictive%20subgraphs%20by%20encouraging%20the%20models%0Ato%20foster%20divergence%20on%20the%20subgraph%20mask%2C%20which%20circumvents%20the%20limitation%20of%0Aa%20model%20solely%20focusing%20on%20the%20subgraph%20corresponding%20to%20simple%20structural%0Apatterns.%20Specifically%2C%20we%20employs%20a%20regularizer%20to%20punish%20overlap%20in%20extracted%0Asubgraphs%20across%20models%2C%20thereby%20encouraging%20different%20models%20to%20concentrate%20on%0Adistinct%20structural%20patterns.%20Model%20selection%20for%20robust%20OOD%20performance%20is%0Aachieved%20through%20validation%20accuracy.%20Tested%20across%20four%20datasets%20from%20GOOD%0Abenchmark%20and%20one%20dataset%20from%20DrugOOD%20benchmark%2C%20our%20approach%20demonstrates%0Asignificant%20improvement%20over%20existing%20methods%2C%20effectively%20addressing%20the%0Asimplicity%20bias%20and%20enhancing%20generalization%20in%20graph%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIVE%253A%2520Subgraph%2520Disagreement%2520for%2520Graph%2520Out-of-Distribution%2520Generalization%26entry.906535625%3DXin%2520Sun%2520and%2520Liang%2520Wang%2520and%2520Qiang%2520Liu%2520and%2520Shu%2520Wu%2520and%2520Zilei%2520Wang%2520and%2520Liang%2520Wang%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520out-of-distribution%2520%2528OOD%2529%250Ageneralization%2520in%2520graph%2520machine%2520learning%252C%2520a%2520field%2520rapidly%2520advancing%2520yet%250Agrappling%2520with%2520the%2520discrepancy%2520between%2520source%2520and%2520target%2520data%2520distributions.%250ATraditional%2520graph%2520learning%2520algorithms%252C%2520based%2520on%2520the%2520assumption%2520of%2520uniform%250Adistribution%2520between%2520training%2520and%2520test%2520data%252C%2520falter%2520in%2520real-world%2520scenarios%250Awhere%2520this%2520assumption%2520fails%252C%2520resulting%2520in%2520suboptimal%2520performance.%2520A%2520principal%250Afactor%2520contributing%2520to%2520this%2520suboptimal%2520performance%2520is%2520the%2520inherent%2520simplicity%250Abias%2520of%2520neural%2520networks%2520trained%2520through%2520Stochastic%2520Gradient%2520Descent%2520%2528SGD%2529%252C%250Awhich%2520prefer%2520simpler%2520features%2520over%2520more%2520complex%2520yet%2520equally%2520or%2520more%2520predictive%250Aones.%2520This%2520bias%2520leads%2520to%2520a%2520reliance%2520on%2520spurious%2520correlations%252C%2520adversely%250Aaffecting%2520OOD%2520performance%2520in%2520various%2520tasks%2520such%2520as%2520image%2520recognition%252C%2520natural%250Alanguage%2520understanding%252C%2520and%2520graph%2520classification.%2520Current%2520methodologies%252C%250Aincluding%2520subgraph-mixup%2520and%2520information%2520bottleneck%2520approaches%252C%2520have%2520achieved%250Apartial%2520success%2520but%2520struggle%2520to%2520overcome%2520simplicity%2520bias%252C%2520often%2520reinforcing%250Aspurious%2520correlations.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520DIVE%252C%2520training%2520a%2520collection%250Aof%2520models%2520to%2520focus%2520on%2520all%2520label-predictive%2520subgraphs%2520by%2520encouraging%2520the%2520models%250Ato%2520foster%2520divergence%2520on%2520the%2520subgraph%2520mask%252C%2520which%2520circumvents%2520the%2520limitation%2520of%250Aa%2520model%2520solely%2520focusing%2520on%2520the%2520subgraph%2520corresponding%2520to%2520simple%2520structural%250Apatterns.%2520Specifically%252C%2520we%2520employs%2520a%2520regularizer%2520to%2520punish%2520overlap%2520in%2520extracted%250Asubgraphs%2520across%2520models%252C%2520thereby%2520encouraging%2520different%2520models%2520to%2520concentrate%2520on%250Adistinct%2520structural%2520patterns.%2520Model%2520selection%2520for%2520robust%2520OOD%2520performance%2520is%250Aachieved%2520through%2520validation%2520accuracy.%2520Tested%2520across%2520four%2520datasets%2520from%2520GOOD%250Abenchmark%2520and%2520one%2520dataset%2520from%2520DrugOOD%2520benchmark%252C%2520our%2520approach%2520demonstrates%250Asignificant%2520improvement%2520over%2520existing%2520methods%252C%2520effectively%2520addressing%2520the%250Asimplicity%2520bias%2520and%2520enhancing%2520generalization%2520in%2520graph%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIVE%3A%20Subgraph%20Disagreement%20for%20Graph%20Out-of-Distribution%20Generalization&entry.906535625=Xin%20Sun%20and%20Liang%20Wang%20and%20Qiang%20Liu%20and%20Shu%20Wu%20and%20Zilei%20Wang%20and%20Liang%20Wang&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20out-of-distribution%20%28OOD%29%0Ageneralization%20in%20graph%20machine%20learning%2C%20a%20field%20rapidly%20advancing%20yet%0Agrappling%20with%20the%20discrepancy%20between%20source%20and%20target%20data%20distributions.%0ATraditional%20graph%20learning%20algorithms%2C%20based%20on%20the%20assumption%20of%20uniform%0Adistribution%20between%20training%20and%20test%20data%2C%20falter%20in%20real-world%20scenarios%0Awhere%20this%20assumption%20fails%2C%20resulting%20in%20suboptimal%20performance.%20A%20principal%0Afactor%20contributing%20to%20this%20suboptimal%20performance%20is%20the%20inherent%20simplicity%0Abias%20of%20neural%20networks%20trained%20through%20Stochastic%20Gradient%20Descent%20%28SGD%29%2C%0Awhich%20prefer%20simpler%20features%20over%20more%20complex%20yet%20equally%20or%20more%20predictive%0Aones.%20This%20bias%20leads%20to%20a%20reliance%20on%20spurious%20correlations%2C%20adversely%0Aaffecting%20OOD%20performance%20in%20various%20tasks%20such%20as%20image%20recognition%2C%20natural%0Alanguage%20understanding%2C%20and%20graph%20classification.%20Current%20methodologies%2C%0Aincluding%20subgraph-mixup%20and%20information%20bottleneck%20approaches%2C%20have%20achieved%0Apartial%20success%20but%20struggle%20to%20overcome%20simplicity%20bias%2C%20often%20reinforcing%0Aspurious%20correlations.%20To%20tackle%20this%2C%20we%20propose%20DIVE%2C%20training%20a%20collection%0Aof%20models%20to%20focus%20on%20all%20label-predictive%20subgraphs%20by%20encouraging%20the%20models%0Ato%20foster%20divergence%20on%20the%20subgraph%20mask%2C%20which%20circumvents%20the%20limitation%20of%0Aa%20model%20solely%20focusing%20on%20the%20subgraph%20corresponding%20to%20simple%20structural%0Apatterns.%20Specifically%2C%20we%20employs%20a%20regularizer%20to%20punish%20overlap%20in%20extracted%0Asubgraphs%20across%20models%2C%20thereby%20encouraging%20different%20models%20to%20concentrate%20on%0Adistinct%20structural%20patterns.%20Model%20selection%20for%20robust%20OOD%20performance%20is%0Aachieved%20through%20validation%20accuracy.%20Tested%20across%20four%20datasets%20from%20GOOD%0Abenchmark%20and%20one%20dataset%20from%20DrugOOD%20benchmark%2C%20our%20approach%20demonstrates%0Asignificant%20improvement%20over%20existing%20methods%2C%20effectively%20addressing%20the%0Asimplicity%20bias%20and%20enhancing%20generalization%20in%20graph%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04400v1&entry.124074799=Read"},
{"title": "AggSS: An Aggregated Self-Supervised Approach for Class-Incremental\n  Learning", "author": "Jayateja Kalla and Soma Biswas", "abstract": "  This paper investigates the impact of self-supervised learning, specifically\nimage rotations, on various class-incremental learning paradigms. Here, each\nimage with a predefined rotation is considered as a new class for training. At\ninference, all image rotation predictions are aggregated for the final\nprediction, a strategy we term Aggregated Self-Supervision (AggSS). We observe\na shift in the deep neural network's attention towards intrinsic object\nfeatures as it learns through AggSS strategy. This learning approach\nsignificantly enhances class-incremental learning by promoting robust feature\nlearning. AggSS serves as a plug-and-play module that can be seamlessly\nincorporated into any class-incremental learning framework, leveraging its\npowerful feature learning capabilities to enhance performance across various\nclass-incremental learning approaches. Extensive experiments conducted on\nstandard incremental learning datasets CIFAR-100 and ImageNet-Subset\ndemonstrate the significant role of AggSS in improving performance within these\nparadigms.\n", "link": "http://arxiv.org/abs/2408.04347v1", "date": "2024-08-08", "relevancy": 2.6229, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5697}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5026}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AggSS%3A%20An%20Aggregated%20Self-Supervised%20Approach%20for%20Class-Incremental%0A%20%20Learning&body=Title%3A%20AggSS%3A%20An%20Aggregated%20Self-Supervised%20Approach%20for%20Class-Incremental%0A%20%20Learning%0AAuthor%3A%20Jayateja%20Kalla%20and%20Soma%20Biswas%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20impact%20of%20self-supervised%20learning%2C%20specifically%0Aimage%20rotations%2C%20on%20various%20class-incremental%20learning%20paradigms.%20Here%2C%20each%0Aimage%20with%20a%20predefined%20rotation%20is%20considered%20as%20a%20new%20class%20for%20training.%20At%0Ainference%2C%20all%20image%20rotation%20predictions%20are%20aggregated%20for%20the%20final%0Aprediction%2C%20a%20strategy%20we%20term%20Aggregated%20Self-Supervision%20%28AggSS%29.%20We%20observe%0Aa%20shift%20in%20the%20deep%20neural%20network%27s%20attention%20towards%20intrinsic%20object%0Afeatures%20as%20it%20learns%20through%20AggSS%20strategy.%20This%20learning%20approach%0Asignificantly%20enhances%20class-incremental%20learning%20by%20promoting%20robust%20feature%0Alearning.%20AggSS%20serves%20as%20a%20plug-and-play%20module%20that%20can%20be%20seamlessly%0Aincorporated%20into%20any%20class-incremental%20learning%20framework%2C%20leveraging%20its%0Apowerful%20feature%20learning%20capabilities%20to%20enhance%20performance%20across%20various%0Aclass-incremental%20learning%20approaches.%20Extensive%20experiments%20conducted%20on%0Astandard%20incremental%20learning%20datasets%20CIFAR-100%20and%20ImageNet-Subset%0Ademonstrate%20the%20significant%20role%20of%20AggSS%20in%20improving%20performance%20within%20these%0Aparadigms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAggSS%253A%2520An%2520Aggregated%2520Self-Supervised%2520Approach%2520for%2520Class-Incremental%250A%2520%2520Learning%26entry.906535625%3DJayateja%2520Kalla%2520and%2520Soma%2520Biswas%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520impact%2520of%2520self-supervised%2520learning%252C%2520specifically%250Aimage%2520rotations%252C%2520on%2520various%2520class-incremental%2520learning%2520paradigms.%2520Here%252C%2520each%250Aimage%2520with%2520a%2520predefined%2520rotation%2520is%2520considered%2520as%2520a%2520new%2520class%2520for%2520training.%2520At%250Ainference%252C%2520all%2520image%2520rotation%2520predictions%2520are%2520aggregated%2520for%2520the%2520final%250Aprediction%252C%2520a%2520strategy%2520we%2520term%2520Aggregated%2520Self-Supervision%2520%2528AggSS%2529.%2520We%2520observe%250Aa%2520shift%2520in%2520the%2520deep%2520neural%2520network%2527s%2520attention%2520towards%2520intrinsic%2520object%250Afeatures%2520as%2520it%2520learns%2520through%2520AggSS%2520strategy.%2520This%2520learning%2520approach%250Asignificantly%2520enhances%2520class-incremental%2520learning%2520by%2520promoting%2520robust%2520feature%250Alearning.%2520AggSS%2520serves%2520as%2520a%2520plug-and-play%2520module%2520that%2520can%2520be%2520seamlessly%250Aincorporated%2520into%2520any%2520class-incremental%2520learning%2520framework%252C%2520leveraging%2520its%250Apowerful%2520feature%2520learning%2520capabilities%2520to%2520enhance%2520performance%2520across%2520various%250Aclass-incremental%2520learning%2520approaches.%2520Extensive%2520experiments%2520conducted%2520on%250Astandard%2520incremental%2520learning%2520datasets%2520CIFAR-100%2520and%2520ImageNet-Subset%250Ademonstrate%2520the%2520significant%2520role%2520of%2520AggSS%2520in%2520improving%2520performance%2520within%2520these%250Aparadigms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AggSS%3A%20An%20Aggregated%20Self-Supervised%20Approach%20for%20Class-Incremental%0A%20%20Learning&entry.906535625=Jayateja%20Kalla%20and%20Soma%20Biswas&entry.1292438233=%20%20This%20paper%20investigates%20the%20impact%20of%20self-supervised%20learning%2C%20specifically%0Aimage%20rotations%2C%20on%20various%20class-incremental%20learning%20paradigms.%20Here%2C%20each%0Aimage%20with%20a%20predefined%20rotation%20is%20considered%20as%20a%20new%20class%20for%20training.%20At%0Ainference%2C%20all%20image%20rotation%20predictions%20are%20aggregated%20for%20the%20final%0Aprediction%2C%20a%20strategy%20we%20term%20Aggregated%20Self-Supervision%20%28AggSS%29.%20We%20observe%0Aa%20shift%20in%20the%20deep%20neural%20network%27s%20attention%20towards%20intrinsic%20object%0Afeatures%20as%20it%20learns%20through%20AggSS%20strategy.%20This%20learning%20approach%0Asignificantly%20enhances%20class-incremental%20learning%20by%20promoting%20robust%20feature%0Alearning.%20AggSS%20serves%20as%20a%20plug-and-play%20module%20that%20can%20be%20seamlessly%0Aincorporated%20into%20any%20class-incremental%20learning%20framework%2C%20leveraging%20its%0Apowerful%20feature%20learning%20capabilities%20to%20enhance%20performance%20across%20various%0Aclass-incremental%20learning%20approaches.%20Extensive%20experiments%20conducted%20on%0Astandard%20incremental%20learning%20datasets%20CIFAR-100%20and%20ImageNet-Subset%0Ademonstrate%20the%20significant%20role%20of%20AggSS%20in%20improving%20performance%20within%20these%0Aparadigms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04347v1&entry.124074799=Read"},
{"title": "Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to\n  Uncertain Human Behaving", "author": "Jia Hu and Shuhan Wang and Yiming Zhang and Haoran Wang", "abstract": "  Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a\npromising vehicle platooning technology in real-world implementation. By\nutilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces\nthe cost and enhances the reliability of perception and decision-making.\nHowever, state-of-the-art HL-CACC technology still has a great limitation on\ndriving safety for the lack of considering the leading human driver's uncertain\nbehaving. In this study, a HL-CACC controller is designed based on Stochastic\nModel Predictive Control (SMPC). It is enabled to predict the driving intention\nof the leading Connected Human-Driven Vehicle (CHV). The proposed controller\nhas the following features: i) enhanced perceived safety in oscillating\ntraffic; ii) guaranteed safety against hard brakes; iii) computational\nefficient for real-time implementation. The proposed controller is evaluated on\na PreScan&Simulink simulation platform. Real vehicle trajectory data is\ncollected for the calibration of simulation. Results reveal that the proposed\ncontroller: i) improves perceived safety by 19.17% in oscillating traffic; ii)\nenhances actual safety by 7.76% against hard brake; iii) is confirmed with\nstring stability. The computation time is approximately 3 milliseconds when\nrunning on a laptop equipped with an Intel i5-13500H CPU. This indicates the\nproposed controller is ready for real-time implementation.\n", "link": "http://arxiv.org/abs/2405.07556v2", "date": "2024-08-08", "relevancy": 2.6227, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5333}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety-Aware%20Human-Lead%20Vehicle%20Platooning%20by%20Proactively%20Reacting%20to%0A%20%20Uncertain%20Human%20Behaving&body=Title%3A%20Safety-Aware%20Human-Lead%20Vehicle%20Platooning%20by%20Proactively%20Reacting%20to%0A%20%20Uncertain%20Human%20Behaving%0AAuthor%3A%20Jia%20Hu%20and%20Shuhan%20Wang%20and%20Yiming%20Zhang%20and%20Haoran%20Wang%0AAbstract%3A%20%20%20Human-Lead%20Cooperative%20Adaptive%20Cruise%20Control%20%28HL-CACC%29%20is%20regarded%20as%20a%0Apromising%20vehicle%20platooning%20technology%20in%20real-world%20implementation.%20By%0Autilizing%20a%20Human-driven%20Vehicle%20%28HV%29%20as%20the%20platoon%20leader%2C%20HL-CACC%20reduces%0Athe%20cost%20and%20enhances%20the%20reliability%20of%20perception%20and%20decision-making.%0AHowever%2C%20state-of-the-art%20HL-CACC%20technology%20still%20has%20a%20great%20limitation%20on%0Adriving%20safety%20for%20the%20lack%20of%20considering%20the%20leading%20human%20driver%27s%20uncertain%0Abehaving.%20In%20this%20study%2C%20a%20HL-CACC%20controller%20is%20designed%20based%20on%20Stochastic%0AModel%20Predictive%20Control%20%28SMPC%29.%20It%20is%20enabled%20to%20predict%20the%20driving%20intention%0Aof%20the%20leading%20Connected%20Human-Driven%20Vehicle%20%28CHV%29.%20The%20proposed%20controller%0Ahas%20the%20following%20features%3A%20i%29%20enhanced%20perceived%20safety%20in%20oscillating%0Atraffic%3B%20ii%29%20guaranteed%20safety%20against%20hard%20brakes%3B%20iii%29%20computational%0Aefficient%20for%20real-time%20implementation.%20The%20proposed%20controller%20is%20evaluated%20on%0Aa%20PreScan%26Simulink%20simulation%20platform.%20Real%20vehicle%20trajectory%20data%20is%0Acollected%20for%20the%20calibration%20of%20simulation.%20Results%20reveal%20that%20the%20proposed%0Acontroller%3A%20i%29%20improves%20perceived%20safety%20by%2019.17%25%20in%20oscillating%20traffic%3B%20ii%29%0Aenhances%20actual%20safety%20by%207.76%25%20against%20hard%20brake%3B%20iii%29%20is%20confirmed%20with%0Astring%20stability.%20The%20computation%20time%20is%20approximately%203%20milliseconds%20when%0Arunning%20on%20a%20laptop%20equipped%20with%20an%20Intel%20i5-13500H%20CPU.%20This%20indicates%20the%0Aproposed%20controller%20is%20ready%20for%20real-time%20implementation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07556v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety-Aware%2520Human-Lead%2520Vehicle%2520Platooning%2520by%2520Proactively%2520Reacting%2520to%250A%2520%2520Uncertain%2520Human%2520Behaving%26entry.906535625%3DJia%2520Hu%2520and%2520Shuhan%2520Wang%2520and%2520Yiming%2520Zhang%2520and%2520Haoran%2520Wang%26entry.1292438233%3D%2520%2520Human-Lead%2520Cooperative%2520Adaptive%2520Cruise%2520Control%2520%2528HL-CACC%2529%2520is%2520regarded%2520as%2520a%250Apromising%2520vehicle%2520platooning%2520technology%2520in%2520real-world%2520implementation.%2520By%250Autilizing%2520a%2520Human-driven%2520Vehicle%2520%2528HV%2529%2520as%2520the%2520platoon%2520leader%252C%2520HL-CACC%2520reduces%250Athe%2520cost%2520and%2520enhances%2520the%2520reliability%2520of%2520perception%2520and%2520decision-making.%250AHowever%252C%2520state-of-the-art%2520HL-CACC%2520technology%2520still%2520has%2520a%2520great%2520limitation%2520on%250Adriving%2520safety%2520for%2520the%2520lack%2520of%2520considering%2520the%2520leading%2520human%2520driver%2527s%2520uncertain%250Abehaving.%2520In%2520this%2520study%252C%2520a%2520HL-CACC%2520controller%2520is%2520designed%2520based%2520on%2520Stochastic%250AModel%2520Predictive%2520Control%2520%2528SMPC%2529.%2520It%2520is%2520enabled%2520to%2520predict%2520the%2520driving%2520intention%250Aof%2520the%2520leading%2520Connected%2520Human-Driven%2520Vehicle%2520%2528CHV%2529.%2520The%2520proposed%2520controller%250Ahas%2520the%2520following%2520features%253A%2520i%2529%2520enhanced%2520perceived%2520safety%2520in%2520oscillating%250Atraffic%253B%2520ii%2529%2520guaranteed%2520safety%2520against%2520hard%2520brakes%253B%2520iii%2529%2520computational%250Aefficient%2520for%2520real-time%2520implementation.%2520The%2520proposed%2520controller%2520is%2520evaluated%2520on%250Aa%2520PreScan%2526Simulink%2520simulation%2520platform.%2520Real%2520vehicle%2520trajectory%2520data%2520is%250Acollected%2520for%2520the%2520calibration%2520of%2520simulation.%2520Results%2520reveal%2520that%2520the%2520proposed%250Acontroller%253A%2520i%2529%2520improves%2520perceived%2520safety%2520by%252019.17%2525%2520in%2520oscillating%2520traffic%253B%2520ii%2529%250Aenhances%2520actual%2520safety%2520by%25207.76%2525%2520against%2520hard%2520brake%253B%2520iii%2529%2520is%2520confirmed%2520with%250Astring%2520stability.%2520The%2520computation%2520time%2520is%2520approximately%25203%2520milliseconds%2520when%250Arunning%2520on%2520a%2520laptop%2520equipped%2520with%2520an%2520Intel%2520i5-13500H%2520CPU.%2520This%2520indicates%2520the%250Aproposed%2520controller%2520is%2520ready%2520for%2520real-time%2520implementation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07556v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety-Aware%20Human-Lead%20Vehicle%20Platooning%20by%20Proactively%20Reacting%20to%0A%20%20Uncertain%20Human%20Behaving&entry.906535625=Jia%20Hu%20and%20Shuhan%20Wang%20and%20Yiming%20Zhang%20and%20Haoran%20Wang&entry.1292438233=%20%20Human-Lead%20Cooperative%20Adaptive%20Cruise%20Control%20%28HL-CACC%29%20is%20regarded%20as%20a%0Apromising%20vehicle%20platooning%20technology%20in%20real-world%20implementation.%20By%0Autilizing%20a%20Human-driven%20Vehicle%20%28HV%29%20as%20the%20platoon%20leader%2C%20HL-CACC%20reduces%0Athe%20cost%20and%20enhances%20the%20reliability%20of%20perception%20and%20decision-making.%0AHowever%2C%20state-of-the-art%20HL-CACC%20technology%20still%20has%20a%20great%20limitation%20on%0Adriving%20safety%20for%20the%20lack%20of%20considering%20the%20leading%20human%20driver%27s%20uncertain%0Abehaving.%20In%20this%20study%2C%20a%20HL-CACC%20controller%20is%20designed%20based%20on%20Stochastic%0AModel%20Predictive%20Control%20%28SMPC%29.%20It%20is%20enabled%20to%20predict%20the%20driving%20intention%0Aof%20the%20leading%20Connected%20Human-Driven%20Vehicle%20%28CHV%29.%20The%20proposed%20controller%0Ahas%20the%20following%20features%3A%20i%29%20enhanced%20perceived%20safety%20in%20oscillating%0Atraffic%3B%20ii%29%20guaranteed%20safety%20against%20hard%20brakes%3B%20iii%29%20computational%0Aefficient%20for%20real-time%20implementation.%20The%20proposed%20controller%20is%20evaluated%20on%0Aa%20PreScan%26Simulink%20simulation%20platform.%20Real%20vehicle%20trajectory%20data%20is%0Acollected%20for%20the%20calibration%20of%20simulation.%20Results%20reveal%20that%20the%20proposed%0Acontroller%3A%20i%29%20improves%20perceived%20safety%20by%2019.17%25%20in%20oscillating%20traffic%3B%20ii%29%0Aenhances%20actual%20safety%20by%207.76%25%20against%20hard%20brake%3B%20iii%29%20is%20confirmed%20with%0Astring%20stability.%20The%20computation%20time%20is%20approximately%203%20milliseconds%20when%0Arunning%20on%20a%20laptop%20equipped%20with%20an%20Intel%20i5-13500H%20CPU.%20This%20indicates%20the%0Aproposed%20controller%20is%20ready%20for%20real-time%20implementation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07556v2&entry.124074799=Read"},
{"title": "Unsupervised Object Localization in the Era of Self-Supervised ViTs: A\n  Survey", "author": "Oriane Sim\u00e9oni and \u00c9loi Zablocki and Spyros Gidaris and Gilles Puy and Patrick P\u00e9rez", "abstract": "  The recent enthusiasm for open-world vision systems show the high interest of\nthe community to perform perception tasks outside of the closed-vocabulary\nbenchmark setups which have been so popular until now. Being able to discover\nobjects in images/videos without knowing in advance what objects populate the\ndataset is an exciting prospect. But how to find objects without knowing\nanything about them? Recent works show that it is possible to perform\nclass-agnostic unsupervised object localization by exploiting self-supervised\npre-trained features. We propose here a survey of unsupervised object\nlocalization methods that discover objects in images without requiring any\nmanual annotation in the era of self-supervised ViTs. We gather links of\ndiscussed methods in the repository\nhttps://github.com/valeoai/Awesome-Unsupervised-Object-Localization.\n", "link": "http://arxiv.org/abs/2310.12904v3", "date": "2024-08-08", "relevancy": 2.6048, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5299}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5295}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Object%20Localization%20in%20the%20Era%20of%20Self-Supervised%20ViTs%3A%20A%0A%20%20Survey&body=Title%3A%20Unsupervised%20Object%20Localization%20in%20the%20Era%20of%20Self-Supervised%20ViTs%3A%20A%0A%20%20Survey%0AAuthor%3A%20Oriane%20Sim%C3%A9oni%20and%20%C3%89loi%20Zablocki%20and%20Spyros%20Gidaris%20and%20Gilles%20Puy%20and%20Patrick%20P%C3%A9rez%0AAbstract%3A%20%20%20The%20recent%20enthusiasm%20for%20open-world%20vision%20systems%20show%20the%20high%20interest%20of%0Athe%20community%20to%20perform%20perception%20tasks%20outside%20of%20the%20closed-vocabulary%0Abenchmark%20setups%20which%20have%20been%20so%20popular%20until%20now.%20Being%20able%20to%20discover%0Aobjects%20in%20images/videos%20without%20knowing%20in%20advance%20what%20objects%20populate%20the%0Adataset%20is%20an%20exciting%20prospect.%20But%20how%20to%20find%20objects%20without%20knowing%0Aanything%20about%20them%3F%20Recent%20works%20show%20that%20it%20is%20possible%20to%20perform%0Aclass-agnostic%20unsupervised%20object%20localization%20by%20exploiting%20self-supervised%0Apre-trained%20features.%20We%20propose%20here%20a%20survey%20of%20unsupervised%20object%0Alocalization%20methods%20that%20discover%20objects%20in%20images%20without%20requiring%20any%0Amanual%20annotation%20in%20the%20era%20of%20self-supervised%20ViTs.%20We%20gather%20links%20of%0Adiscussed%20methods%20in%20the%20repository%0Ahttps%3A//github.com/valeoai/Awesome-Unsupervised-Object-Localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12904v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Object%2520Localization%2520in%2520the%2520Era%2520of%2520Self-Supervised%2520ViTs%253A%2520A%250A%2520%2520Survey%26entry.906535625%3DOriane%2520Sim%25C3%25A9oni%2520and%2520%25C3%2589loi%2520Zablocki%2520and%2520Spyros%2520Gidaris%2520and%2520Gilles%2520Puy%2520and%2520Patrick%2520P%25C3%25A9rez%26entry.1292438233%3D%2520%2520The%2520recent%2520enthusiasm%2520for%2520open-world%2520vision%2520systems%2520show%2520the%2520high%2520interest%2520of%250Athe%2520community%2520to%2520perform%2520perception%2520tasks%2520outside%2520of%2520the%2520closed-vocabulary%250Abenchmark%2520setups%2520which%2520have%2520been%2520so%2520popular%2520until%2520now.%2520Being%2520able%2520to%2520discover%250Aobjects%2520in%2520images/videos%2520without%2520knowing%2520in%2520advance%2520what%2520objects%2520populate%2520the%250Adataset%2520is%2520an%2520exciting%2520prospect.%2520But%2520how%2520to%2520find%2520objects%2520without%2520knowing%250Aanything%2520about%2520them%253F%2520Recent%2520works%2520show%2520that%2520it%2520is%2520possible%2520to%2520perform%250Aclass-agnostic%2520unsupervised%2520object%2520localization%2520by%2520exploiting%2520self-supervised%250Apre-trained%2520features.%2520We%2520propose%2520here%2520a%2520survey%2520of%2520unsupervised%2520object%250Alocalization%2520methods%2520that%2520discover%2520objects%2520in%2520images%2520without%2520requiring%2520any%250Amanual%2520annotation%2520in%2520the%2520era%2520of%2520self-supervised%2520ViTs.%2520We%2520gather%2520links%2520of%250Adiscussed%2520methods%2520in%2520the%2520repository%250Ahttps%253A//github.com/valeoai/Awesome-Unsupervised-Object-Localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12904v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Object%20Localization%20in%20the%20Era%20of%20Self-Supervised%20ViTs%3A%20A%0A%20%20Survey&entry.906535625=Oriane%20Sim%C3%A9oni%20and%20%C3%89loi%20Zablocki%20and%20Spyros%20Gidaris%20and%20Gilles%20Puy%20and%20Patrick%20P%C3%A9rez&entry.1292438233=%20%20The%20recent%20enthusiasm%20for%20open-world%20vision%20systems%20show%20the%20high%20interest%20of%0Athe%20community%20to%20perform%20perception%20tasks%20outside%20of%20the%20closed-vocabulary%0Abenchmark%20setups%20which%20have%20been%20so%20popular%20until%20now.%20Being%20able%20to%20discover%0Aobjects%20in%20images/videos%20without%20knowing%20in%20advance%20what%20objects%20populate%20the%0Adataset%20is%20an%20exciting%20prospect.%20But%20how%20to%20find%20objects%20without%20knowing%0Aanything%20about%20them%3F%20Recent%20works%20show%20that%20it%20is%20possible%20to%20perform%0Aclass-agnostic%20unsupervised%20object%20localization%20by%20exploiting%20self-supervised%0Apre-trained%20features.%20We%20propose%20here%20a%20survey%20of%20unsupervised%20object%0Alocalization%20methods%20that%20discover%20objects%20in%20images%20without%20requiring%20any%0Amanual%20annotation%20in%20the%20era%20of%20self-supervised%20ViTs.%20We%20gather%20links%20of%0Adiscussed%20methods%20in%20the%20repository%0Ahttps%3A//github.com/valeoai/Awesome-Unsupervised-Object-Localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12904v3&entry.124074799=Read"},
{"title": "HiLo: A Learning Framework for Generalized Category Discovery Robust to\n  Domain Shifts", "author": "Hongjun Wang and Sagar Vaze and Kai Han", "abstract": "  Generalized Category Discovery (GCD) is a challenging task in which, given a\npartially labelled dataset, models must categorize all unlabelled instances,\nregardless of whether they come from labelled categories or from new ones. In\nthis paper, we challenge a remaining assumption in this task: that all images\nshare the same domain. Specifically, we introduce a new task and method to\nhandle GCD when the unlabelled data also contains images from different domains\nto the labelled set. Our proposed `HiLo' networks extract High-level semantic\nand Low-level domain features, before minimizing the mutual information between\nthe representations. Our intuition is that the clusterings based on domain\ninformation and semantic information should be independent. We further extend\nour method with a specialized domain augmentation tailored for the GCD task, as\nwell as a curriculum learning approach. Finally, we construct a benchmark from\ncorrupted fine-grained datasets as well as a large-scale evaluation on\nDomainNet with real-world domain shifts, reimplementing a number of GCD\nbaselines in this setting. We demonstrate that HiLo outperforms SoTA category\ndiscovery models by a large margin on all evaluations.\n", "link": "http://arxiv.org/abs/2408.04591v1", "date": "2024-08-08", "relevancy": 2.5942, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5262}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.521}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiLo%3A%20A%20Learning%20Framework%20for%20Generalized%20Category%20Discovery%20Robust%20to%0A%20%20Domain%20Shifts&body=Title%3A%20HiLo%3A%20A%20Learning%20Framework%20for%20Generalized%20Category%20Discovery%20Robust%20to%0A%20%20Domain%20Shifts%0AAuthor%3A%20Hongjun%20Wang%20and%20Sagar%20Vaze%20and%20Kai%20Han%0AAbstract%3A%20%20%20Generalized%20Category%20Discovery%20%28GCD%29%20is%20a%20challenging%20task%20in%20which%2C%20given%20a%0Apartially%20labelled%20dataset%2C%20models%20must%20categorize%20all%20unlabelled%20instances%2C%0Aregardless%20of%20whether%20they%20come%20from%20labelled%20categories%20or%20from%20new%20ones.%20In%0Athis%20paper%2C%20we%20challenge%20a%20remaining%20assumption%20in%20this%20task%3A%20that%20all%20images%0Ashare%20the%20same%20domain.%20Specifically%2C%20we%20introduce%20a%20new%20task%20and%20method%20to%0Ahandle%20GCD%20when%20the%20unlabelled%20data%20also%20contains%20images%20from%20different%20domains%0Ato%20the%20labelled%20set.%20Our%20proposed%20%60HiLo%27%20networks%20extract%20High-level%20semantic%0Aand%20Low-level%20domain%20features%2C%20before%20minimizing%20the%20mutual%20information%20between%0Athe%20representations.%20Our%20intuition%20is%20that%20the%20clusterings%20based%20on%20domain%0Ainformation%20and%20semantic%20information%20should%20be%20independent.%20We%20further%20extend%0Aour%20method%20with%20a%20specialized%20domain%20augmentation%20tailored%20for%20the%20GCD%20task%2C%20as%0Awell%20as%20a%20curriculum%20learning%20approach.%20Finally%2C%20we%20construct%20a%20benchmark%20from%0Acorrupted%20fine-grained%20datasets%20as%20well%20as%20a%20large-scale%20evaluation%20on%0ADomainNet%20with%20real-world%20domain%20shifts%2C%20reimplementing%20a%20number%20of%20GCD%0Abaselines%20in%20this%20setting.%20We%20demonstrate%20that%20HiLo%20outperforms%20SoTA%20category%0Adiscovery%20models%20by%20a%20large%20margin%20on%20all%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiLo%253A%2520A%2520Learning%2520Framework%2520for%2520Generalized%2520Category%2520Discovery%2520Robust%2520to%250A%2520%2520Domain%2520Shifts%26entry.906535625%3DHongjun%2520Wang%2520and%2520Sagar%2520Vaze%2520and%2520Kai%2520Han%26entry.1292438233%3D%2520%2520Generalized%2520Category%2520Discovery%2520%2528GCD%2529%2520is%2520a%2520challenging%2520task%2520in%2520which%252C%2520given%2520a%250Apartially%2520labelled%2520dataset%252C%2520models%2520must%2520categorize%2520all%2520unlabelled%2520instances%252C%250Aregardless%2520of%2520whether%2520they%2520come%2520from%2520labelled%2520categories%2520or%2520from%2520new%2520ones.%2520In%250Athis%2520paper%252C%2520we%2520challenge%2520a%2520remaining%2520assumption%2520in%2520this%2520task%253A%2520that%2520all%2520images%250Ashare%2520the%2520same%2520domain.%2520Specifically%252C%2520we%2520introduce%2520a%2520new%2520task%2520and%2520method%2520to%250Ahandle%2520GCD%2520when%2520the%2520unlabelled%2520data%2520also%2520contains%2520images%2520from%2520different%2520domains%250Ato%2520the%2520labelled%2520set.%2520Our%2520proposed%2520%2560HiLo%2527%2520networks%2520extract%2520High-level%2520semantic%250Aand%2520Low-level%2520domain%2520features%252C%2520before%2520minimizing%2520the%2520mutual%2520information%2520between%250Athe%2520representations.%2520Our%2520intuition%2520is%2520that%2520the%2520clusterings%2520based%2520on%2520domain%250Ainformation%2520and%2520semantic%2520information%2520should%2520be%2520independent.%2520We%2520further%2520extend%250Aour%2520method%2520with%2520a%2520specialized%2520domain%2520augmentation%2520tailored%2520for%2520the%2520GCD%2520task%252C%2520as%250Awell%2520as%2520a%2520curriculum%2520learning%2520approach.%2520Finally%252C%2520we%2520construct%2520a%2520benchmark%2520from%250Acorrupted%2520fine-grained%2520datasets%2520as%2520well%2520as%2520a%2520large-scale%2520evaluation%2520on%250ADomainNet%2520with%2520real-world%2520domain%2520shifts%252C%2520reimplementing%2520a%2520number%2520of%2520GCD%250Abaselines%2520in%2520this%2520setting.%2520We%2520demonstrate%2520that%2520HiLo%2520outperforms%2520SoTA%2520category%250Adiscovery%2520models%2520by%2520a%2520large%2520margin%2520on%2520all%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiLo%3A%20A%20Learning%20Framework%20for%20Generalized%20Category%20Discovery%20Robust%20to%0A%20%20Domain%20Shifts&entry.906535625=Hongjun%20Wang%20and%20Sagar%20Vaze%20and%20Kai%20Han&entry.1292438233=%20%20Generalized%20Category%20Discovery%20%28GCD%29%20is%20a%20challenging%20task%20in%20which%2C%20given%20a%0Apartially%20labelled%20dataset%2C%20models%20must%20categorize%20all%20unlabelled%20instances%2C%0Aregardless%20of%20whether%20they%20come%20from%20labelled%20categories%20or%20from%20new%20ones.%20In%0Athis%20paper%2C%20we%20challenge%20a%20remaining%20assumption%20in%20this%20task%3A%20that%20all%20images%0Ashare%20the%20same%20domain.%20Specifically%2C%20we%20introduce%20a%20new%20task%20and%20method%20to%0Ahandle%20GCD%20when%20the%20unlabelled%20data%20also%20contains%20images%20from%20different%20domains%0Ato%20the%20labelled%20set.%20Our%20proposed%20%60HiLo%27%20networks%20extract%20High-level%20semantic%0Aand%20Low-level%20domain%20features%2C%20before%20minimizing%20the%20mutual%20information%20between%0Athe%20representations.%20Our%20intuition%20is%20that%20the%20clusterings%20based%20on%20domain%0Ainformation%20and%20semantic%20information%20should%20be%20independent.%20We%20further%20extend%0Aour%20method%20with%20a%20specialized%20domain%20augmentation%20tailored%20for%20the%20GCD%20task%2C%20as%0Awell%20as%20a%20curriculum%20learning%20approach.%20Finally%2C%20we%20construct%20a%20benchmark%20from%0Acorrupted%20fine-grained%20datasets%20as%20well%20as%20a%20large-scale%20evaluation%20on%0ADomainNet%20with%20real-world%20domain%20shifts%2C%20reimplementing%20a%20number%20of%20GCD%0Abaselines%20in%20this%20setting.%20We%20demonstrate%20that%20HiLo%20outperforms%20SoTA%20category%0Adiscovery%20models%20by%20a%20large%20margin%20on%20all%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04591v1&entry.124074799=Read"},
{"title": "Detection of Animal Movement from Weather Radar using Self-Supervised\n  Learning", "author": "Mubin Ul Haque and Joel Janek Dabrowski and Rebecca M. Rogers and Hazel Parry", "abstract": "  Detecting flying animals (e.g., birds, bats, and insects) using weather radar\nhelps gain insights into animal movement and migration patterns, aids in\nmanagement efforts (such as biosecurity) and enhances our understanding of the\necosystem.The conventional approach to detecting animals in weather radar\ninvolves thresholding: defining and applying thresholds for the radar\nvariables, based on expert opinion. More recently, Deep Learning approaches\nhave been shown to provide improved performance in detection. However,\nobtaining sufficient labelled weather radar data for flying animals to build\nlearning-based models is time-consuming and labor-intensive. To address the\nchallenge of data labelling, we propose a self-supervised learning method for\ndetecting animal movement. In our proposed method, we pre-train our model on a\nlarge dataset with noisy labels produced by a threshold approach. The key\nadvantage is that the pre-trained dataset size is limited only by the number of\nradar images available. We then fine-tune the model on a small human-labelled\ndataset. Our experiments on Australian weather radar data for waterbird\nsegmentation show that the proposed method outperforms the current state-of-the\nart approach by 43.53% in the dice co-efficient statistic.\n", "link": "http://arxiv.org/abs/2408.04424v1", "date": "2024-08-08", "relevancy": 2.5745, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5622}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detection%20of%20Animal%20Movement%20from%20Weather%20Radar%20using%20Self-Supervised%0A%20%20Learning&body=Title%3A%20Detection%20of%20Animal%20Movement%20from%20Weather%20Radar%20using%20Self-Supervised%0A%20%20Learning%0AAuthor%3A%20Mubin%20Ul%20Haque%20and%20Joel%20Janek%20Dabrowski%20and%20Rebecca%20M.%20Rogers%20and%20Hazel%20Parry%0AAbstract%3A%20%20%20Detecting%20flying%20animals%20%28e.g.%2C%20birds%2C%20bats%2C%20and%20insects%29%20using%20weather%20radar%0Ahelps%20gain%20insights%20into%20animal%20movement%20and%20migration%20patterns%2C%20aids%20in%0Amanagement%20efforts%20%28such%20as%20biosecurity%29%20and%20enhances%20our%20understanding%20of%20the%0Aecosystem.The%20conventional%20approach%20to%20detecting%20animals%20in%20weather%20radar%0Ainvolves%20thresholding%3A%20defining%20and%20applying%20thresholds%20for%20the%20radar%0Avariables%2C%20based%20on%20expert%20opinion.%20More%20recently%2C%20Deep%20Learning%20approaches%0Ahave%20been%20shown%20to%20provide%20improved%20performance%20in%20detection.%20However%2C%0Aobtaining%20sufficient%20labelled%20weather%20radar%20data%20for%20flying%20animals%20to%20build%0Alearning-based%20models%20is%20time-consuming%20and%20labor-intensive.%20To%20address%20the%0Achallenge%20of%20data%20labelling%2C%20we%20propose%20a%20self-supervised%20learning%20method%20for%0Adetecting%20animal%20movement.%20In%20our%20proposed%20method%2C%20we%20pre-train%20our%20model%20on%20a%0Alarge%20dataset%20with%20noisy%20labels%20produced%20by%20a%20threshold%20approach.%20The%20key%0Aadvantage%20is%20that%20the%20pre-trained%20dataset%20size%20is%20limited%20only%20by%20the%20number%20of%0Aradar%20images%20available.%20We%20then%20fine-tune%20the%20model%20on%20a%20small%20human-labelled%0Adataset.%20Our%20experiments%20on%20Australian%20weather%20radar%20data%20for%20waterbird%0Asegmentation%20show%20that%20the%20proposed%20method%20outperforms%20the%20current%20state-of-the%0Aart%20approach%20by%2043.53%25%20in%20the%20dice%20co-efficient%20statistic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetection%2520of%2520Animal%2520Movement%2520from%2520Weather%2520Radar%2520using%2520Self-Supervised%250A%2520%2520Learning%26entry.906535625%3DMubin%2520Ul%2520Haque%2520and%2520Joel%2520Janek%2520Dabrowski%2520and%2520Rebecca%2520M.%2520Rogers%2520and%2520Hazel%2520Parry%26entry.1292438233%3D%2520%2520Detecting%2520flying%2520animals%2520%2528e.g.%252C%2520birds%252C%2520bats%252C%2520and%2520insects%2529%2520using%2520weather%2520radar%250Ahelps%2520gain%2520insights%2520into%2520animal%2520movement%2520and%2520migration%2520patterns%252C%2520aids%2520in%250Amanagement%2520efforts%2520%2528such%2520as%2520biosecurity%2529%2520and%2520enhances%2520our%2520understanding%2520of%2520the%250Aecosystem.The%2520conventional%2520approach%2520to%2520detecting%2520animals%2520in%2520weather%2520radar%250Ainvolves%2520thresholding%253A%2520defining%2520and%2520applying%2520thresholds%2520for%2520the%2520radar%250Avariables%252C%2520based%2520on%2520expert%2520opinion.%2520More%2520recently%252C%2520Deep%2520Learning%2520approaches%250Ahave%2520been%2520shown%2520to%2520provide%2520improved%2520performance%2520in%2520detection.%2520However%252C%250Aobtaining%2520sufficient%2520labelled%2520weather%2520radar%2520data%2520for%2520flying%2520animals%2520to%2520build%250Alearning-based%2520models%2520is%2520time-consuming%2520and%2520labor-intensive.%2520To%2520address%2520the%250Achallenge%2520of%2520data%2520labelling%252C%2520we%2520propose%2520a%2520self-supervised%2520learning%2520method%2520for%250Adetecting%2520animal%2520movement.%2520In%2520our%2520proposed%2520method%252C%2520we%2520pre-train%2520our%2520model%2520on%2520a%250Alarge%2520dataset%2520with%2520noisy%2520labels%2520produced%2520by%2520a%2520threshold%2520approach.%2520The%2520key%250Aadvantage%2520is%2520that%2520the%2520pre-trained%2520dataset%2520size%2520is%2520limited%2520only%2520by%2520the%2520number%2520of%250Aradar%2520images%2520available.%2520We%2520then%2520fine-tune%2520the%2520model%2520on%2520a%2520small%2520human-labelled%250Adataset.%2520Our%2520experiments%2520on%2520Australian%2520weather%2520radar%2520data%2520for%2520waterbird%250Asegmentation%2520show%2520that%2520the%2520proposed%2520method%2520outperforms%2520the%2520current%2520state-of-the%250Aart%2520approach%2520by%252043.53%2525%2520in%2520the%2520dice%2520co-efficient%2520statistic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detection%20of%20Animal%20Movement%20from%20Weather%20Radar%20using%20Self-Supervised%0A%20%20Learning&entry.906535625=Mubin%20Ul%20Haque%20and%20Joel%20Janek%20Dabrowski%20and%20Rebecca%20M.%20Rogers%20and%20Hazel%20Parry&entry.1292438233=%20%20Detecting%20flying%20animals%20%28e.g.%2C%20birds%2C%20bats%2C%20and%20insects%29%20using%20weather%20radar%0Ahelps%20gain%20insights%20into%20animal%20movement%20and%20migration%20patterns%2C%20aids%20in%0Amanagement%20efforts%20%28such%20as%20biosecurity%29%20and%20enhances%20our%20understanding%20of%20the%0Aecosystem.The%20conventional%20approach%20to%20detecting%20animals%20in%20weather%20radar%0Ainvolves%20thresholding%3A%20defining%20and%20applying%20thresholds%20for%20the%20radar%0Avariables%2C%20based%20on%20expert%20opinion.%20More%20recently%2C%20Deep%20Learning%20approaches%0Ahave%20been%20shown%20to%20provide%20improved%20performance%20in%20detection.%20However%2C%0Aobtaining%20sufficient%20labelled%20weather%20radar%20data%20for%20flying%20animals%20to%20build%0Alearning-based%20models%20is%20time-consuming%20and%20labor-intensive.%20To%20address%20the%0Achallenge%20of%20data%20labelling%2C%20we%20propose%20a%20self-supervised%20learning%20method%20for%0Adetecting%20animal%20movement.%20In%20our%20proposed%20method%2C%20we%20pre-train%20our%20model%20on%20a%0Alarge%20dataset%20with%20noisy%20labels%20produced%20by%20a%20threshold%20approach.%20The%20key%0Aadvantage%20is%20that%20the%20pre-trained%20dataset%20size%20is%20limited%20only%20by%20the%20number%20of%0Aradar%20images%20available.%20We%20then%20fine-tune%20the%20model%20on%20a%20small%20human-labelled%0Adataset.%20Our%20experiments%20on%20Australian%20weather%20radar%20data%20for%20waterbird%0Asegmentation%20show%20that%20the%20proposed%20method%20outperforms%20the%20current%20state-of-the%0Aart%20approach%20by%2043.53%25%20in%20the%20dice%20co-efficient%20statistic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04424v1&entry.124074799=Read"},
{"title": "Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height\n  Estimation", "author": "Daniele Rege Cambrin and Isaac Corley and Paolo Garza", "abstract": "  Estimating global tree canopy height is crucial for forest conservation and\nclimate change applications. However, capturing high-resolution ground truth\ncanopy height using LiDAR is expensive and not available globally. An efficient\nalternative is to train a canopy height estimator to operate on single-view\nremotely sensed imagery. The primary obstacle to this approach is that these\nmethods require significant training data to generalize well globally and\nacross uncommon edge cases. Recent monocular depth estimation foundation models\nhave show strong zero-shot performance even for complex scenes. In this paper\nwe leverage the representations learned by these models to transfer to the\nremote sensing domain for measuring canopy height. Our findings suggest that\nour proposed Depth Any Canopy, the result of fine-tuning the Depth Anything v2\nmodel for canopy height estimation, provides a performant and efficient\nsolution, surpassing the current state-of-the-art with superior or comparable\nperformance using only a fraction of the computational resources and\nparameters. Furthermore, our approach requires less than \\$1.30 in compute and\nresults in an estimated carbon footprint of 0.14 kgCO2. Code, experimental\nresults, and model checkpoints are openly available at\nhttps://github.com/DarthReca/depth-any-canopy.\n", "link": "http://arxiv.org/abs/2408.04523v1", "date": "2024-08-08", "relevancy": 2.5302, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5091}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5091}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Any%20Canopy%3A%20Leveraging%20Depth%20Foundation%20Models%20for%20Canopy%20Height%0A%20%20Estimation&body=Title%3A%20Depth%20Any%20Canopy%3A%20Leveraging%20Depth%20Foundation%20Models%20for%20Canopy%20Height%0A%20%20Estimation%0AAuthor%3A%20Daniele%20Rege%20Cambrin%20and%20Isaac%20Corley%20and%20Paolo%20Garza%0AAbstract%3A%20%20%20Estimating%20global%20tree%20canopy%20height%20is%20crucial%20for%20forest%20conservation%20and%0Aclimate%20change%20applications.%20However%2C%20capturing%20high-resolution%20ground%20truth%0Acanopy%20height%20using%20LiDAR%20is%20expensive%20and%20not%20available%20globally.%20An%20efficient%0Aalternative%20is%20to%20train%20a%20canopy%20height%20estimator%20to%20operate%20on%20single-view%0Aremotely%20sensed%20imagery.%20The%20primary%20obstacle%20to%20this%20approach%20is%20that%20these%0Amethods%20require%20significant%20training%20data%20to%20generalize%20well%20globally%20and%0Aacross%20uncommon%20edge%20cases.%20Recent%20monocular%20depth%20estimation%20foundation%20models%0Ahave%20show%20strong%20zero-shot%20performance%20even%20for%20complex%20scenes.%20In%20this%20paper%0Awe%20leverage%20the%20representations%20learned%20by%20these%20models%20to%20transfer%20to%20the%0Aremote%20sensing%20domain%20for%20measuring%20canopy%20height.%20Our%20findings%20suggest%20that%0Aour%20proposed%20Depth%20Any%20Canopy%2C%20the%20result%20of%20fine-tuning%20the%20Depth%20Anything%20v2%0Amodel%20for%20canopy%20height%20estimation%2C%20provides%20a%20performant%20and%20efficient%0Asolution%2C%20surpassing%20the%20current%20state-of-the-art%20with%20superior%20or%20comparable%0Aperformance%20using%20only%20a%20fraction%20of%20the%20computational%20resources%20and%0Aparameters.%20Furthermore%2C%20our%20approach%20requires%20less%20than%20%5C%241.30%20in%20compute%20and%0Aresults%20in%20an%20estimated%20carbon%20footprint%20of%200.14%20kgCO2.%20Code%2C%20experimental%0Aresults%2C%20and%20model%20checkpoints%20are%20openly%20available%20at%0Ahttps%3A//github.com/DarthReca/depth-any-canopy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Any%2520Canopy%253A%2520Leveraging%2520Depth%2520Foundation%2520Models%2520for%2520Canopy%2520Height%250A%2520%2520Estimation%26entry.906535625%3DDaniele%2520Rege%2520Cambrin%2520and%2520Isaac%2520Corley%2520and%2520Paolo%2520Garza%26entry.1292438233%3D%2520%2520Estimating%2520global%2520tree%2520canopy%2520height%2520is%2520crucial%2520for%2520forest%2520conservation%2520and%250Aclimate%2520change%2520applications.%2520However%252C%2520capturing%2520high-resolution%2520ground%2520truth%250Acanopy%2520height%2520using%2520LiDAR%2520is%2520expensive%2520and%2520not%2520available%2520globally.%2520An%2520efficient%250Aalternative%2520is%2520to%2520train%2520a%2520canopy%2520height%2520estimator%2520to%2520operate%2520on%2520single-view%250Aremotely%2520sensed%2520imagery.%2520The%2520primary%2520obstacle%2520to%2520this%2520approach%2520is%2520that%2520these%250Amethods%2520require%2520significant%2520training%2520data%2520to%2520generalize%2520well%2520globally%2520and%250Aacross%2520uncommon%2520edge%2520cases.%2520Recent%2520monocular%2520depth%2520estimation%2520foundation%2520models%250Ahave%2520show%2520strong%2520zero-shot%2520performance%2520even%2520for%2520complex%2520scenes.%2520In%2520this%2520paper%250Awe%2520leverage%2520the%2520representations%2520learned%2520by%2520these%2520models%2520to%2520transfer%2520to%2520the%250Aremote%2520sensing%2520domain%2520for%2520measuring%2520canopy%2520height.%2520Our%2520findings%2520suggest%2520that%250Aour%2520proposed%2520Depth%2520Any%2520Canopy%252C%2520the%2520result%2520of%2520fine-tuning%2520the%2520Depth%2520Anything%2520v2%250Amodel%2520for%2520canopy%2520height%2520estimation%252C%2520provides%2520a%2520performant%2520and%2520efficient%250Asolution%252C%2520surpassing%2520the%2520current%2520state-of-the-art%2520with%2520superior%2520or%2520comparable%250Aperformance%2520using%2520only%2520a%2520fraction%2520of%2520the%2520computational%2520resources%2520and%250Aparameters.%2520Furthermore%252C%2520our%2520approach%2520requires%2520less%2520than%2520%255C%25241.30%2520in%2520compute%2520and%250Aresults%2520in%2520an%2520estimated%2520carbon%2520footprint%2520of%25200.14%2520kgCO2.%2520Code%252C%2520experimental%250Aresults%252C%2520and%2520model%2520checkpoints%2520are%2520openly%2520available%2520at%250Ahttps%253A//github.com/DarthReca/depth-any-canopy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Any%20Canopy%3A%20Leveraging%20Depth%20Foundation%20Models%20for%20Canopy%20Height%0A%20%20Estimation&entry.906535625=Daniele%20Rege%20Cambrin%20and%20Isaac%20Corley%20and%20Paolo%20Garza&entry.1292438233=%20%20Estimating%20global%20tree%20canopy%20height%20is%20crucial%20for%20forest%20conservation%20and%0Aclimate%20change%20applications.%20However%2C%20capturing%20high-resolution%20ground%20truth%0Acanopy%20height%20using%20LiDAR%20is%20expensive%20and%20not%20available%20globally.%20An%20efficient%0Aalternative%20is%20to%20train%20a%20canopy%20height%20estimator%20to%20operate%20on%20single-view%0Aremotely%20sensed%20imagery.%20The%20primary%20obstacle%20to%20this%20approach%20is%20that%20these%0Amethods%20require%20significant%20training%20data%20to%20generalize%20well%20globally%20and%0Aacross%20uncommon%20edge%20cases.%20Recent%20monocular%20depth%20estimation%20foundation%20models%0Ahave%20show%20strong%20zero-shot%20performance%20even%20for%20complex%20scenes.%20In%20this%20paper%0Awe%20leverage%20the%20representations%20learned%20by%20these%20models%20to%20transfer%20to%20the%0Aremote%20sensing%20domain%20for%20measuring%20canopy%20height.%20Our%20findings%20suggest%20that%0Aour%20proposed%20Depth%20Any%20Canopy%2C%20the%20result%20of%20fine-tuning%20the%20Depth%20Anything%20v2%0Amodel%20for%20canopy%20height%20estimation%2C%20provides%20a%20performant%20and%20efficient%0Asolution%2C%20surpassing%20the%20current%20state-of-the-art%20with%20superior%20or%20comparable%0Aperformance%20using%20only%20a%20fraction%20of%20the%20computational%20resources%20and%0Aparameters.%20Furthermore%2C%20our%20approach%20requires%20less%20than%20%5C%241.30%20in%20compute%20and%0Aresults%20in%20an%20estimated%20carbon%20footprint%20of%200.14%20kgCO2.%20Code%2C%20experimental%0Aresults%2C%20and%20model%20checkpoints%20are%20openly%20available%20at%0Ahttps%3A//github.com/DarthReca/depth-any-canopy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04523v1&entry.124074799=Read"},
{"title": "Automatic Target-Less Camera-LiDAR Calibration From Motion and Deep\n  Point Correspondences", "author": "K\u00fcrsat Petek and Niclas V\u00f6disch and Johannes Meyer and Daniele Cattaneo and Abhinav Valada and Wolfram Burgard", "abstract": "  Sensor setups of robotic platforms commonly include both camera and LiDAR as\nthey provide complementary information. However, fusing these two modalities\ntypically requires a highly accurate calibration between them. In this paper,\nwe propose MDPCalib which is a novel method for camera-LiDAR calibration that\nrequires neither human supervision nor any specific target objects. Instead, we\nutilize sensor motion estimates from visual and LiDAR odometry as well as deep\nlearning-based 2D-pixel-to-3D-point correspondences that are obtained without\nin-domain retraining. We represent camera-LiDAR calibration as an optimization\nproblem and minimize the costs induced by constraints from sensor motion and\npoint correspondences. In extensive experiments, we demonstrate that our\napproach yields highly accurate extrinsic calibration parameters and is robust\nto random initialization. Additionally, our approach generalizes to a wide\nrange of sensor setups, which we demonstrate by employing it on various robotic\nplatforms including a self-driving perception car, a quadruped robot, and a\nUAV. To make our calibration method publicly accessible, we release the code on\nour project website at http://calibration.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2404.17298v2", "date": "2024-08-08", "relevancy": 2.4555, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6198}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6179}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Target-Less%20Camera-LiDAR%20Calibration%20From%20Motion%20and%20Deep%0A%20%20Point%20Correspondences&body=Title%3A%20Automatic%20Target-Less%20Camera-LiDAR%20Calibration%20From%20Motion%20and%20Deep%0A%20%20Point%20Correspondences%0AAuthor%3A%20K%C3%BCrsat%20Petek%20and%20Niclas%20V%C3%B6disch%20and%20Johannes%20Meyer%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard%0AAbstract%3A%20%20%20Sensor%20setups%20of%20robotic%20platforms%20commonly%20include%20both%20camera%20and%20LiDAR%20as%0Athey%20provide%20complementary%20information.%20However%2C%20fusing%20these%20two%20modalities%0Atypically%20requires%20a%20highly%20accurate%20calibration%20between%20them.%20In%20this%20paper%2C%0Awe%20propose%20MDPCalib%20which%20is%20a%20novel%20method%20for%20camera-LiDAR%20calibration%20that%0Arequires%20neither%20human%20supervision%20nor%20any%20specific%20target%20objects.%20Instead%2C%20we%0Autilize%20sensor%20motion%20estimates%20from%20visual%20and%20LiDAR%20odometry%20as%20well%20as%20deep%0Alearning-based%202D-pixel-to-3D-point%20correspondences%20that%20are%20obtained%20without%0Ain-domain%20retraining.%20We%20represent%20camera-LiDAR%20calibration%20as%20an%20optimization%0Aproblem%20and%20minimize%20the%20costs%20induced%20by%20constraints%20from%20sensor%20motion%20and%0Apoint%20correspondences.%20In%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%0Aapproach%20yields%20highly%20accurate%20extrinsic%20calibration%20parameters%20and%20is%20robust%0Ato%20random%20initialization.%20Additionally%2C%20our%20approach%20generalizes%20to%20a%20wide%0Arange%20of%20sensor%20setups%2C%20which%20we%20demonstrate%20by%20employing%20it%20on%20various%20robotic%0Aplatforms%20including%20a%20self-driving%20perception%20car%2C%20a%20quadruped%20robot%2C%20and%20a%0AUAV.%20To%20make%20our%20calibration%20method%20publicly%20accessible%2C%20we%20release%20the%20code%20on%0Aour%20project%20website%20at%20http%3A//calibration.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17298v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Target-Less%2520Camera-LiDAR%2520Calibration%2520From%2520Motion%2520and%2520Deep%250A%2520%2520Point%2520Correspondences%26entry.906535625%3DK%25C3%25BCrsat%2520Petek%2520and%2520Niclas%2520V%25C3%25B6disch%2520and%2520Johannes%2520Meyer%2520and%2520Daniele%2520Cattaneo%2520and%2520Abhinav%2520Valada%2520and%2520Wolfram%2520Burgard%26entry.1292438233%3D%2520%2520Sensor%2520setups%2520of%2520robotic%2520platforms%2520commonly%2520include%2520both%2520camera%2520and%2520LiDAR%2520as%250Athey%2520provide%2520complementary%2520information.%2520However%252C%2520fusing%2520these%2520two%2520modalities%250Atypically%2520requires%2520a%2520highly%2520accurate%2520calibration%2520between%2520them.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520MDPCalib%2520which%2520is%2520a%2520novel%2520method%2520for%2520camera-LiDAR%2520calibration%2520that%250Arequires%2520neither%2520human%2520supervision%2520nor%2520any%2520specific%2520target%2520objects.%2520Instead%252C%2520we%250Autilize%2520sensor%2520motion%2520estimates%2520from%2520visual%2520and%2520LiDAR%2520odometry%2520as%2520well%2520as%2520deep%250Alearning-based%25202D-pixel-to-3D-point%2520correspondences%2520that%2520are%2520obtained%2520without%250Ain-domain%2520retraining.%2520We%2520represent%2520camera-LiDAR%2520calibration%2520as%2520an%2520optimization%250Aproblem%2520and%2520minimize%2520the%2520costs%2520induced%2520by%2520constraints%2520from%2520sensor%2520motion%2520and%250Apoint%2520correspondences.%2520In%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%250Aapproach%2520yields%2520highly%2520accurate%2520extrinsic%2520calibration%2520parameters%2520and%2520is%2520robust%250Ato%2520random%2520initialization.%2520Additionally%252C%2520our%2520approach%2520generalizes%2520to%2520a%2520wide%250Arange%2520of%2520sensor%2520setups%252C%2520which%2520we%2520demonstrate%2520by%2520employing%2520it%2520on%2520various%2520robotic%250Aplatforms%2520including%2520a%2520self-driving%2520perception%2520car%252C%2520a%2520quadruped%2520robot%252C%2520and%2520a%250AUAV.%2520To%2520make%2520our%2520calibration%2520method%2520publicly%2520accessible%252C%2520we%2520release%2520the%2520code%2520on%250Aour%2520project%2520website%2520at%2520http%253A//calibration.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17298v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Target-Less%20Camera-LiDAR%20Calibration%20From%20Motion%20and%20Deep%0A%20%20Point%20Correspondences&entry.906535625=K%C3%BCrsat%20Petek%20and%20Niclas%20V%C3%B6disch%20and%20Johannes%20Meyer%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada%20and%20Wolfram%20Burgard&entry.1292438233=%20%20Sensor%20setups%20of%20robotic%20platforms%20commonly%20include%20both%20camera%20and%20LiDAR%20as%0Athey%20provide%20complementary%20information.%20However%2C%20fusing%20these%20two%20modalities%0Atypically%20requires%20a%20highly%20accurate%20calibration%20between%20them.%20In%20this%20paper%2C%0Awe%20propose%20MDPCalib%20which%20is%20a%20novel%20method%20for%20camera-LiDAR%20calibration%20that%0Arequires%20neither%20human%20supervision%20nor%20any%20specific%20target%20objects.%20Instead%2C%20we%0Autilize%20sensor%20motion%20estimates%20from%20visual%20and%20LiDAR%20odometry%20as%20well%20as%20deep%0Alearning-based%202D-pixel-to-3D-point%20correspondences%20that%20are%20obtained%20without%0Ain-domain%20retraining.%20We%20represent%20camera-LiDAR%20calibration%20as%20an%20optimization%0Aproblem%20and%20minimize%20the%20costs%20induced%20by%20constraints%20from%20sensor%20motion%20and%0Apoint%20correspondences.%20In%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%0Aapproach%20yields%20highly%20accurate%20extrinsic%20calibration%20parameters%20and%20is%20robust%0Ato%20random%20initialization.%20Additionally%2C%20our%20approach%20generalizes%20to%20a%20wide%0Arange%20of%20sensor%20setups%2C%20which%20we%20demonstrate%20by%20employing%20it%20on%20various%20robotic%0Aplatforms%20including%20a%20self-driving%20perception%20car%2C%20a%20quadruped%20robot%2C%20and%20a%0AUAV.%20To%20make%20our%20calibration%20method%20publicly%20accessible%2C%20we%20release%20the%20code%20on%0Aour%20project%20website%20at%20http%3A//calibration.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17298v2&entry.124074799=Read"},
{"title": "Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from\n  User's Casual Sketches", "author": "Yongzhi Xu and Yonhon Ng and Yifu Wang and Inkyu Sa and Yunfei Duan and Yang Li and Pan Ji and Hongdong Li", "abstract": "  3D Content Generation is at the heart of many computer graphics applications,\nincluding video gaming, film-making, virtual and augmented reality, etc. This\npaper proposes a novel deep-learning based approach for automatically\ngenerating interactive and playable 3D game scenes, all from the user's casual\nprompts such as a hand-drawn sketch. Sketch-based input offers a natural, and\nconvenient way to convey the user's design intention in the content creation\nprocess. To circumvent the data-deficient challenge in learning (i.e. the lack\nof large training data of 3D scenes), our method leverages a pre-trained 2D\ndenoising diffusion model to generate a 2D image of the scene as the conceptual\nguidance. In this process, we adopt the isometric projection mode to factor out\nunknown camera poses while obtaining the scene layout. From the generated\nisometric image, we use a pre-trained image understanding method to segment the\nimage into meaningful parts, such as off-ground objects, trees, and buildings,\nand extract the 2D scene layout. These segments and layouts are subsequently\nfed into a procedural content generation (PCG) engine, such as a 3D video game\nengine like Unity or Unreal, to create the 3D scene. The resulting 3D scene can\nbe seamlessly integrated into a game development environment and is readily\nplayable. Extensive tests demonstrate that our method can efficiently generate\nhigh-quality and interactive 3D game scenes with layouts that closely follow\nthe user's intention.\n", "link": "http://arxiv.org/abs/2408.04567v1", "date": "2024-08-08", "relevancy": 2.4162, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6074}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6074}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketch2Scene%3A%20Automatic%20Generation%20of%20Interactive%203D%20Game%20Scenes%20from%0A%20%20User%27s%20Casual%20Sketches&body=Title%3A%20Sketch2Scene%3A%20Automatic%20Generation%20of%20Interactive%203D%20Game%20Scenes%20from%0A%20%20User%27s%20Casual%20Sketches%0AAuthor%3A%20Yongzhi%20Xu%20and%20Yonhon%20Ng%20and%20Yifu%20Wang%20and%20Inkyu%20Sa%20and%20Yunfei%20Duan%20and%20Yang%20Li%20and%20Pan%20Ji%20and%20Hongdong%20Li%0AAbstract%3A%20%20%203D%20Content%20Generation%20is%20at%20the%20heart%20of%20many%20computer%20graphics%20applications%2C%0Aincluding%20video%20gaming%2C%20film-making%2C%20virtual%20and%20augmented%20reality%2C%20etc.%20This%0Apaper%20proposes%20a%20novel%20deep-learning%20based%20approach%20for%20automatically%0Agenerating%20interactive%20and%20playable%203D%20game%20scenes%2C%20all%20from%20the%20user%27s%20casual%0Aprompts%20such%20as%20a%20hand-drawn%20sketch.%20Sketch-based%20input%20offers%20a%20natural%2C%20and%0Aconvenient%20way%20to%20convey%20the%20user%27s%20design%20intention%20in%20the%20content%20creation%0Aprocess.%20To%20circumvent%20the%20data-deficient%20challenge%20in%20learning%20%28i.e.%20the%20lack%0Aof%20large%20training%20data%20of%203D%20scenes%29%2C%20our%20method%20leverages%20a%20pre-trained%202D%0Adenoising%20diffusion%20model%20to%20generate%20a%202D%20image%20of%20the%20scene%20as%20the%20conceptual%0Aguidance.%20In%20this%20process%2C%20we%20adopt%20the%20isometric%20projection%20mode%20to%20factor%20out%0Aunknown%20camera%20poses%20while%20obtaining%20the%20scene%20layout.%20From%20the%20generated%0Aisometric%20image%2C%20we%20use%20a%20pre-trained%20image%20understanding%20method%20to%20segment%20the%0Aimage%20into%20meaningful%20parts%2C%20such%20as%20off-ground%20objects%2C%20trees%2C%20and%20buildings%2C%0Aand%20extract%20the%202D%20scene%20layout.%20These%20segments%20and%20layouts%20are%20subsequently%0Afed%20into%20a%20procedural%20content%20generation%20%28PCG%29%20engine%2C%20such%20as%20a%203D%20video%20game%0Aengine%20like%20Unity%20or%20Unreal%2C%20to%20create%20the%203D%20scene.%20The%20resulting%203D%20scene%20can%0Abe%20seamlessly%20integrated%20into%20a%20game%20development%20environment%20and%20is%20readily%0Aplayable.%20Extensive%20tests%20demonstrate%20that%20our%20method%20can%20efficiently%20generate%0Ahigh-quality%20and%20interactive%203D%20game%20scenes%20with%20layouts%20that%20closely%20follow%0Athe%20user%27s%20intention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketch2Scene%253A%2520Automatic%2520Generation%2520of%2520Interactive%25203D%2520Game%2520Scenes%2520from%250A%2520%2520User%2527s%2520Casual%2520Sketches%26entry.906535625%3DYongzhi%2520Xu%2520and%2520Yonhon%2520Ng%2520and%2520Yifu%2520Wang%2520and%2520Inkyu%2520Sa%2520and%2520Yunfei%2520Duan%2520and%2520Yang%2520Li%2520and%2520Pan%2520Ji%2520and%2520Hongdong%2520Li%26entry.1292438233%3D%2520%25203D%2520Content%2520Generation%2520is%2520at%2520the%2520heart%2520of%2520many%2520computer%2520graphics%2520applications%252C%250Aincluding%2520video%2520gaming%252C%2520film-making%252C%2520virtual%2520and%2520augmented%2520reality%252C%2520etc.%2520This%250Apaper%2520proposes%2520a%2520novel%2520deep-learning%2520based%2520approach%2520for%2520automatically%250Agenerating%2520interactive%2520and%2520playable%25203D%2520game%2520scenes%252C%2520all%2520from%2520the%2520user%2527s%2520casual%250Aprompts%2520such%2520as%2520a%2520hand-drawn%2520sketch.%2520Sketch-based%2520input%2520offers%2520a%2520natural%252C%2520and%250Aconvenient%2520way%2520to%2520convey%2520the%2520user%2527s%2520design%2520intention%2520in%2520the%2520content%2520creation%250Aprocess.%2520To%2520circumvent%2520the%2520data-deficient%2520challenge%2520in%2520learning%2520%2528i.e.%2520the%2520lack%250Aof%2520large%2520training%2520data%2520of%25203D%2520scenes%2529%252C%2520our%2520method%2520leverages%2520a%2520pre-trained%25202D%250Adenoising%2520diffusion%2520model%2520to%2520generate%2520a%25202D%2520image%2520of%2520the%2520scene%2520as%2520the%2520conceptual%250Aguidance.%2520In%2520this%2520process%252C%2520we%2520adopt%2520the%2520isometric%2520projection%2520mode%2520to%2520factor%2520out%250Aunknown%2520camera%2520poses%2520while%2520obtaining%2520the%2520scene%2520layout.%2520From%2520the%2520generated%250Aisometric%2520image%252C%2520we%2520use%2520a%2520pre-trained%2520image%2520understanding%2520method%2520to%2520segment%2520the%250Aimage%2520into%2520meaningful%2520parts%252C%2520such%2520as%2520off-ground%2520objects%252C%2520trees%252C%2520and%2520buildings%252C%250Aand%2520extract%2520the%25202D%2520scene%2520layout.%2520These%2520segments%2520and%2520layouts%2520are%2520subsequently%250Afed%2520into%2520a%2520procedural%2520content%2520generation%2520%2528PCG%2529%2520engine%252C%2520such%2520as%2520a%25203D%2520video%2520game%250Aengine%2520like%2520Unity%2520or%2520Unreal%252C%2520to%2520create%2520the%25203D%2520scene.%2520The%2520resulting%25203D%2520scene%2520can%250Abe%2520seamlessly%2520integrated%2520into%2520a%2520game%2520development%2520environment%2520and%2520is%2520readily%250Aplayable.%2520Extensive%2520tests%2520demonstrate%2520that%2520our%2520method%2520can%2520efficiently%2520generate%250Ahigh-quality%2520and%2520interactive%25203D%2520game%2520scenes%2520with%2520layouts%2520that%2520closely%2520follow%250Athe%2520user%2527s%2520intention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketch2Scene%3A%20Automatic%20Generation%20of%20Interactive%203D%20Game%20Scenes%20from%0A%20%20User%27s%20Casual%20Sketches&entry.906535625=Yongzhi%20Xu%20and%20Yonhon%20Ng%20and%20Yifu%20Wang%20and%20Inkyu%20Sa%20and%20Yunfei%20Duan%20and%20Yang%20Li%20and%20Pan%20Ji%20and%20Hongdong%20Li&entry.1292438233=%20%203D%20Content%20Generation%20is%20at%20the%20heart%20of%20many%20computer%20graphics%20applications%2C%0Aincluding%20video%20gaming%2C%20film-making%2C%20virtual%20and%20augmented%20reality%2C%20etc.%20This%0Apaper%20proposes%20a%20novel%20deep-learning%20based%20approach%20for%20automatically%0Agenerating%20interactive%20and%20playable%203D%20game%20scenes%2C%20all%20from%20the%20user%27s%20casual%0Aprompts%20such%20as%20a%20hand-drawn%20sketch.%20Sketch-based%20input%20offers%20a%20natural%2C%20and%0Aconvenient%20way%20to%20convey%20the%20user%27s%20design%20intention%20in%20the%20content%20creation%0Aprocess.%20To%20circumvent%20the%20data-deficient%20challenge%20in%20learning%20%28i.e.%20the%20lack%0Aof%20large%20training%20data%20of%203D%20scenes%29%2C%20our%20method%20leverages%20a%20pre-trained%202D%0Adenoising%20diffusion%20model%20to%20generate%20a%202D%20image%20of%20the%20scene%20as%20the%20conceptual%0Aguidance.%20In%20this%20process%2C%20we%20adopt%20the%20isometric%20projection%20mode%20to%20factor%20out%0Aunknown%20camera%20poses%20while%20obtaining%20the%20scene%20layout.%20From%20the%20generated%0Aisometric%20image%2C%20we%20use%20a%20pre-trained%20image%20understanding%20method%20to%20segment%20the%0Aimage%20into%20meaningful%20parts%2C%20such%20as%20off-ground%20objects%2C%20trees%2C%20and%20buildings%2C%0Aand%20extract%20the%202D%20scene%20layout.%20These%20segments%20and%20layouts%20are%20subsequently%0Afed%20into%20a%20procedural%20content%20generation%20%28PCG%29%20engine%2C%20such%20as%20a%203D%20video%20game%0Aengine%20like%20Unity%20or%20Unreal%2C%20to%20create%20the%203D%20scene.%20The%20resulting%203D%20scene%20can%0Abe%20seamlessly%20integrated%20into%20a%20game%20development%20environment%20and%20is%20readily%0Aplayable.%20Extensive%20tests%20demonstrate%20that%20our%20method%20can%20efficiently%20generate%0Ahigh-quality%20and%20interactive%203D%20game%20scenes%20with%20layouts%20that%20closely%20follow%0Athe%20user%27s%20intention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04567v1&entry.124074799=Read"},
{"title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior\n  for Part-Level Dynamics", "author": "Ruining Li and Chuanxia Zheng and Christian Rupprecht and Andrea Vedaldi", "abstract": "  We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.\n", "link": "http://arxiv.org/abs/2408.04631v1", "date": "2024-08-08", "relevancy": 2.408, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6259}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.599}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Puppet-Master%3A%20Scaling%20Interactive%20Video%20Generation%20as%20a%20Motion%20Prior%0A%20%20for%20Part-Level%20Dynamics&body=Title%3A%20Puppet-Master%3A%20Scaling%20Interactive%20Video%20Generation%20as%20a%20Motion%20Prior%0A%20%20for%20Part-Level%20Dynamics%0AAuthor%3A%20Ruining%20Li%20and%20Chuanxia%20Zheng%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20We%20present%20Puppet-Master%2C%20an%20interactive%20video%20generative%20model%20that%20can%0Aserve%20as%20a%20motion%20prior%20for%20part-level%20dynamics.%20At%20test%20time%2C%20given%20a%20single%0Aimage%20and%20a%20sparse%20set%20of%20motion%20trajectories%20%28i.e.%2C%20drags%29%2C%20Puppet-Master%20can%0Asynthesize%20a%20video%20depicting%20realistic%20part-level%20motion%20faithful%20to%20the%20given%0Adrag%20interactions.%20This%20is%20achieved%20by%20fine-tuning%20a%20large-scale%20pre-trained%0Avideo%20diffusion%20model%2C%20for%20which%20we%20propose%20a%20new%20conditioning%20architecture%20to%0Ainject%20the%20dragging%20control%20effectively.%20More%20importantly%2C%20we%20introduce%20the%0Aall-to-first%20attention%20mechanism%2C%20a%20drop-in%20replacement%20for%20the%20widely%20adopted%0Aspatial%20attention%20modules%2C%20which%20significantly%20improves%20generation%20quality%20by%0Aaddressing%20the%20appearance%20and%20background%20issues%20in%20existing%20models.%20Unlike%0Aother%20motion-conditioned%20video%20generators%20that%20are%20trained%20on%20in-the-wild%0Avideos%20and%20mostly%20move%20an%20entire%20object%2C%20Puppet-Master%20is%20learned%20from%0AObjaverse-Animation-HQ%2C%20a%20new%20dataset%20of%20curated%20part-level%20motion%20clips.%20We%0Apropose%20a%20strategy%20to%20automatically%20filter%20out%20sub-optimal%20animations%20and%0Aaugment%20the%20synthetic%20renderings%20with%20meaningful%20motion%20trajectories.%0APuppet-Master%20generalizes%20well%20to%20real%20images%20across%20various%20categories%20and%0Aoutperforms%20existing%20methods%20in%20a%20zero-shot%20manner%20on%20a%20real-world%20benchmark.%0ASee%20our%20project%20page%20for%20more%20results%3A%20vgg-puppetmaster.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04631v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPuppet-Master%253A%2520Scaling%2520Interactive%2520Video%2520Generation%2520as%2520a%2520Motion%2520Prior%250A%2520%2520for%2520Part-Level%2520Dynamics%26entry.906535625%3DRuining%2520Li%2520and%2520Chuanxia%2520Zheng%2520and%2520Christian%2520Rupprecht%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520We%2520present%2520Puppet-Master%252C%2520an%2520interactive%2520video%2520generative%2520model%2520that%2520can%250Aserve%2520as%2520a%2520motion%2520prior%2520for%2520part-level%2520dynamics.%2520At%2520test%2520time%252C%2520given%2520a%2520single%250Aimage%2520and%2520a%2520sparse%2520set%2520of%2520motion%2520trajectories%2520%2528i.e.%252C%2520drags%2529%252C%2520Puppet-Master%2520can%250Asynthesize%2520a%2520video%2520depicting%2520realistic%2520part-level%2520motion%2520faithful%2520to%2520the%2520given%250Adrag%2520interactions.%2520This%2520is%2520achieved%2520by%2520fine-tuning%2520a%2520large-scale%2520pre-trained%250Avideo%2520diffusion%2520model%252C%2520for%2520which%2520we%2520propose%2520a%2520new%2520conditioning%2520architecture%2520to%250Ainject%2520the%2520dragging%2520control%2520effectively.%2520More%2520importantly%252C%2520we%2520introduce%2520the%250Aall-to-first%2520attention%2520mechanism%252C%2520a%2520drop-in%2520replacement%2520for%2520the%2520widely%2520adopted%250Aspatial%2520attention%2520modules%252C%2520which%2520significantly%2520improves%2520generation%2520quality%2520by%250Aaddressing%2520the%2520appearance%2520and%2520background%2520issues%2520in%2520existing%2520models.%2520Unlike%250Aother%2520motion-conditioned%2520video%2520generators%2520that%2520are%2520trained%2520on%2520in-the-wild%250Avideos%2520and%2520mostly%2520move%2520an%2520entire%2520object%252C%2520Puppet-Master%2520is%2520learned%2520from%250AObjaverse-Animation-HQ%252C%2520a%2520new%2520dataset%2520of%2520curated%2520part-level%2520motion%2520clips.%2520We%250Apropose%2520a%2520strategy%2520to%2520automatically%2520filter%2520out%2520sub-optimal%2520animations%2520and%250Aaugment%2520the%2520synthetic%2520renderings%2520with%2520meaningful%2520motion%2520trajectories.%250APuppet-Master%2520generalizes%2520well%2520to%2520real%2520images%2520across%2520various%2520categories%2520and%250Aoutperforms%2520existing%2520methods%2520in%2520a%2520zero-shot%2520manner%2520on%2520a%2520real-world%2520benchmark.%250ASee%2520our%2520project%2520page%2520for%2520more%2520results%253A%2520vgg-puppetmaster.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04631v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Puppet-Master%3A%20Scaling%20Interactive%20Video%20Generation%20as%20a%20Motion%20Prior%0A%20%20for%20Part-Level%20Dynamics&entry.906535625=Ruining%20Li%20and%20Chuanxia%20Zheng%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20We%20present%20Puppet-Master%2C%20an%20interactive%20video%20generative%20model%20that%20can%0Aserve%20as%20a%20motion%20prior%20for%20part-level%20dynamics.%20At%20test%20time%2C%20given%20a%20single%0Aimage%20and%20a%20sparse%20set%20of%20motion%20trajectories%20%28i.e.%2C%20drags%29%2C%20Puppet-Master%20can%0Asynthesize%20a%20video%20depicting%20realistic%20part-level%20motion%20faithful%20to%20the%20given%0Adrag%20interactions.%20This%20is%20achieved%20by%20fine-tuning%20a%20large-scale%20pre-trained%0Avideo%20diffusion%20model%2C%20for%20which%20we%20propose%20a%20new%20conditioning%20architecture%20to%0Ainject%20the%20dragging%20control%20effectively.%20More%20importantly%2C%20we%20introduce%20the%0Aall-to-first%20attention%20mechanism%2C%20a%20drop-in%20replacement%20for%20the%20widely%20adopted%0Aspatial%20attention%20modules%2C%20which%20significantly%20improves%20generation%20quality%20by%0Aaddressing%20the%20appearance%20and%20background%20issues%20in%20existing%20models.%20Unlike%0Aother%20motion-conditioned%20video%20generators%20that%20are%20trained%20on%20in-the-wild%0Avideos%20and%20mostly%20move%20an%20entire%20object%2C%20Puppet-Master%20is%20learned%20from%0AObjaverse-Animation-HQ%2C%20a%20new%20dataset%20of%20curated%20part-level%20motion%20clips.%20We%0Apropose%20a%20strategy%20to%20automatically%20filter%20out%20sub-optimal%20animations%20and%0Aaugment%20the%20synthetic%20renderings%20with%20meaningful%20motion%20trajectories.%0APuppet-Master%20generalizes%20well%20to%20real%20images%20across%20various%20categories%20and%0Aoutperforms%20existing%20methods%20in%20a%20zero-shot%20manner%20on%20a%20real-world%20benchmark.%0ASee%20our%20project%20page%20for%20more%20results%3A%20vgg-puppetmaster.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04631v1&entry.124074799=Read"},
{"title": "Unsupervised Mastoidectomy for Cochlear CT Mesh Reconstruction Using\n  Highly Noisy Data", "author": "Yike Zhang and Dingjie Su and Eduardo Davalos and Jack H. Noble", "abstract": "  Cochlear Implant (CI) procedures involve inserting an array of electrodes\ninto the cochlea located inside the inner ear. Mastoidectomy is a surgical\nprocedure that uses a high-speed drill to remove part of the mastoid region of\nthe temporal bone, providing safe access to the cochlea through the middle and\ninner ear. We aim to develop an intraoperative navigation system that registers\nplans created using 3D preoperative Computerized Tomography (CT) volumes with\nthe 2D surgical microscope view. Herein, we propose a method to synthesize the\nmastoidectomy volume using only the preoperative CT scan, where the mastoid is\nintact. We introduce an unsupervised learning framework designed to synthesize\nmastoidectomy. For model training purposes, this method uses postoperative CT\nscans to avoid manual data cleaning or labeling, even when the region removed\nduring mastoidectomy is visible but affected by metal artifacts, low\nsignal-to-noise ratio, or electrode wiring. Our approach estimates\nmastoidectomy regions with a mean dice score of 70.0%. This approach represents\na major step forward for CI intraoperative navigation by predicting realistic\nmastoidectomy-removed regions in preoperative planning that can be used to\nregister the pre-surgery plan to intraoperative microscopy.\n", "link": "http://arxiv.org/abs/2407.15787v2", "date": "2024-08-08", "relevancy": 2.3843, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4772}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4772}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Mastoidectomy%20for%20Cochlear%20CT%20Mesh%20Reconstruction%20Using%0A%20%20Highly%20Noisy%20Data&body=Title%3A%20Unsupervised%20Mastoidectomy%20for%20Cochlear%20CT%20Mesh%20Reconstruction%20Using%0A%20%20Highly%20Noisy%20Data%0AAuthor%3A%20Yike%20Zhang%20and%20Dingjie%20Su%20and%20Eduardo%20Davalos%20and%20Jack%20H.%20Noble%0AAbstract%3A%20%20%20Cochlear%20Implant%20%28CI%29%20procedures%20involve%20inserting%20an%20array%20of%20electrodes%0Ainto%20the%20cochlea%20located%20inside%20the%20inner%20ear.%20Mastoidectomy%20is%20a%20surgical%0Aprocedure%20that%20uses%20a%20high-speed%20drill%20to%20remove%20part%20of%20the%20mastoid%20region%20of%0Athe%20temporal%20bone%2C%20providing%20safe%20access%20to%20the%20cochlea%20through%20the%20middle%20and%0Ainner%20ear.%20We%20aim%20to%20develop%20an%20intraoperative%20navigation%20system%20that%20registers%0Aplans%20created%20using%203D%20preoperative%20Computerized%20Tomography%20%28CT%29%20volumes%20with%0Athe%202D%20surgical%20microscope%20view.%20Herein%2C%20we%20propose%20a%20method%20to%20synthesize%20the%0Amastoidectomy%20volume%20using%20only%20the%20preoperative%20CT%20scan%2C%20where%20the%20mastoid%20is%0Aintact.%20We%20introduce%20an%20unsupervised%20learning%20framework%20designed%20to%20synthesize%0Amastoidectomy.%20For%20model%20training%20purposes%2C%20this%20method%20uses%20postoperative%20CT%0Ascans%20to%20avoid%20manual%20data%20cleaning%20or%20labeling%2C%20even%20when%20the%20region%20removed%0Aduring%20mastoidectomy%20is%20visible%20but%20affected%20by%20metal%20artifacts%2C%20low%0Asignal-to-noise%20ratio%2C%20or%20electrode%20wiring.%20Our%20approach%20estimates%0Amastoidectomy%20regions%20with%20a%20mean%20dice%20score%20of%2070.0%25.%20This%20approach%20represents%0Aa%20major%20step%20forward%20for%20CI%20intraoperative%20navigation%20by%20predicting%20realistic%0Amastoidectomy-removed%20regions%20in%20preoperative%20planning%20that%20can%20be%20used%20to%0Aregister%20the%20pre-surgery%20plan%20to%20intraoperative%20microscopy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15787v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Mastoidectomy%2520for%2520Cochlear%2520CT%2520Mesh%2520Reconstruction%2520Using%250A%2520%2520Highly%2520Noisy%2520Data%26entry.906535625%3DYike%2520Zhang%2520and%2520Dingjie%2520Su%2520and%2520Eduardo%2520Davalos%2520and%2520Jack%2520H.%2520Noble%26entry.1292438233%3D%2520%2520Cochlear%2520Implant%2520%2528CI%2529%2520procedures%2520involve%2520inserting%2520an%2520array%2520of%2520electrodes%250Ainto%2520the%2520cochlea%2520located%2520inside%2520the%2520inner%2520ear.%2520Mastoidectomy%2520is%2520a%2520surgical%250Aprocedure%2520that%2520uses%2520a%2520high-speed%2520drill%2520to%2520remove%2520part%2520of%2520the%2520mastoid%2520region%2520of%250Athe%2520temporal%2520bone%252C%2520providing%2520safe%2520access%2520to%2520the%2520cochlea%2520through%2520the%2520middle%2520and%250Ainner%2520ear.%2520We%2520aim%2520to%2520develop%2520an%2520intraoperative%2520navigation%2520system%2520that%2520registers%250Aplans%2520created%2520using%25203D%2520preoperative%2520Computerized%2520Tomography%2520%2528CT%2529%2520volumes%2520with%250Athe%25202D%2520surgical%2520microscope%2520view.%2520Herein%252C%2520we%2520propose%2520a%2520method%2520to%2520synthesize%2520the%250Amastoidectomy%2520volume%2520using%2520only%2520the%2520preoperative%2520CT%2520scan%252C%2520where%2520the%2520mastoid%2520is%250Aintact.%2520We%2520introduce%2520an%2520unsupervised%2520learning%2520framework%2520designed%2520to%2520synthesize%250Amastoidectomy.%2520For%2520model%2520training%2520purposes%252C%2520this%2520method%2520uses%2520postoperative%2520CT%250Ascans%2520to%2520avoid%2520manual%2520data%2520cleaning%2520or%2520labeling%252C%2520even%2520when%2520the%2520region%2520removed%250Aduring%2520mastoidectomy%2520is%2520visible%2520but%2520affected%2520by%2520metal%2520artifacts%252C%2520low%250Asignal-to-noise%2520ratio%252C%2520or%2520electrode%2520wiring.%2520Our%2520approach%2520estimates%250Amastoidectomy%2520regions%2520with%2520a%2520mean%2520dice%2520score%2520of%252070.0%2525.%2520This%2520approach%2520represents%250Aa%2520major%2520step%2520forward%2520for%2520CI%2520intraoperative%2520navigation%2520by%2520predicting%2520realistic%250Amastoidectomy-removed%2520regions%2520in%2520preoperative%2520planning%2520that%2520can%2520be%2520used%2520to%250Aregister%2520the%2520pre-surgery%2520plan%2520to%2520intraoperative%2520microscopy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15787v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Mastoidectomy%20for%20Cochlear%20CT%20Mesh%20Reconstruction%20Using%0A%20%20Highly%20Noisy%20Data&entry.906535625=Yike%20Zhang%20and%20Dingjie%20Su%20and%20Eduardo%20Davalos%20and%20Jack%20H.%20Noble&entry.1292438233=%20%20Cochlear%20Implant%20%28CI%29%20procedures%20involve%20inserting%20an%20array%20of%20electrodes%0Ainto%20the%20cochlea%20located%20inside%20the%20inner%20ear.%20Mastoidectomy%20is%20a%20surgical%0Aprocedure%20that%20uses%20a%20high-speed%20drill%20to%20remove%20part%20of%20the%20mastoid%20region%20of%0Athe%20temporal%20bone%2C%20providing%20safe%20access%20to%20the%20cochlea%20through%20the%20middle%20and%0Ainner%20ear.%20We%20aim%20to%20develop%20an%20intraoperative%20navigation%20system%20that%20registers%0Aplans%20created%20using%203D%20preoperative%20Computerized%20Tomography%20%28CT%29%20volumes%20with%0Athe%202D%20surgical%20microscope%20view.%20Herein%2C%20we%20propose%20a%20method%20to%20synthesize%20the%0Amastoidectomy%20volume%20using%20only%20the%20preoperative%20CT%20scan%2C%20where%20the%20mastoid%20is%0Aintact.%20We%20introduce%20an%20unsupervised%20learning%20framework%20designed%20to%20synthesize%0Amastoidectomy.%20For%20model%20training%20purposes%2C%20this%20method%20uses%20postoperative%20CT%0Ascans%20to%20avoid%20manual%20data%20cleaning%20or%20labeling%2C%20even%20when%20the%20region%20removed%0Aduring%20mastoidectomy%20is%20visible%20but%20affected%20by%20metal%20artifacts%2C%20low%0Asignal-to-noise%20ratio%2C%20or%20electrode%20wiring.%20Our%20approach%20estimates%0Amastoidectomy%20regions%20with%20a%20mean%20dice%20score%20of%2070.0%25.%20This%20approach%20represents%0Aa%20major%20step%20forward%20for%20CI%20intraoperative%20navigation%20by%20predicting%20realistic%0Amastoidectomy-removed%20regions%20in%20preoperative%20planning%20that%20can%20be%20used%20to%0Aregister%20the%20pre-surgery%20plan%20to%20intraoperative%20microscopy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15787v2&entry.124074799=Read"},
{"title": "NFDI4Health workflow and service for synthetic data generation,\n  assessment and risk management", "author": "Sobhan Moazemi and Tim Adams and Hwei Geok NG and Lisa K\u00fchnel and Julian Schneider and Anatol-Fiete N\u00e4her and Juliane Fluck and Holger Fr\u00f6hlich", "abstract": "  Individual health data is crucial for scientific advancements, particularly\nin developing Artificial Intelligence (AI); however, sharing real patient\ninformation is often restricted due to privacy concerns. A promising solution\nto this challenge is synthetic data generation. This technique creates entirely\nnew datasets that mimic the statistical properties of real data, while\npreserving confidential patient information. In this paper, we present the\nworkflow and different services developed in the context of Germany's National\nData Infrastructure project NFDI4Health. First, two state-of-the-art AI tools\n(namely, VAMBN and MultiNODEs) for generating synthetic health data are\noutlined. Further, we introduce SYNDAT (a public web-based tool) which allows\nusers to visualize and assess the quality and risk of synthetic data provided\nby desired generative models. Additionally, the utility of the proposed methods\nand the web-based tool is showcased using data from Alzheimer's Disease\nNeuroimaging Initiative (ADNI) and the Center for Cancer Registry Data of the\nRobert Koch Institute (RKI).\n", "link": "http://arxiv.org/abs/2408.04478v1", "date": "2024-08-08", "relevancy": 2.377, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4925}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4668}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4668}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NFDI4Health%20workflow%20and%20service%20for%20synthetic%20data%20generation%2C%0A%20%20assessment%20and%20risk%20management&body=Title%3A%20NFDI4Health%20workflow%20and%20service%20for%20synthetic%20data%20generation%2C%0A%20%20assessment%20and%20risk%20management%0AAuthor%3A%20Sobhan%20Moazemi%20and%20Tim%20Adams%20and%20Hwei%20Geok%20NG%20and%20Lisa%20K%C3%BChnel%20and%20Julian%20Schneider%20and%20Anatol-Fiete%20N%C3%A4her%20and%20Juliane%20Fluck%20and%20Holger%20Fr%C3%B6hlich%0AAbstract%3A%20%20%20Individual%20health%20data%20is%20crucial%20for%20scientific%20advancements%2C%20particularly%0Ain%20developing%20Artificial%20Intelligence%20%28AI%29%3B%20however%2C%20sharing%20real%20patient%0Ainformation%20is%20often%20restricted%20due%20to%20privacy%20concerns.%20A%20promising%20solution%0Ato%20this%20challenge%20is%20synthetic%20data%20generation.%20This%20technique%20creates%20entirely%0Anew%20datasets%20that%20mimic%20the%20statistical%20properties%20of%20real%20data%2C%20while%0Apreserving%20confidential%20patient%20information.%20In%20this%20paper%2C%20we%20present%20the%0Aworkflow%20and%20different%20services%20developed%20in%20the%20context%20of%20Germany%27s%20National%0AData%20Infrastructure%20project%20NFDI4Health.%20First%2C%20two%20state-of-the-art%20AI%20tools%0A%28namely%2C%20VAMBN%20and%20MultiNODEs%29%20for%20generating%20synthetic%20health%20data%20are%0Aoutlined.%20Further%2C%20we%20introduce%20SYNDAT%20%28a%20public%20web-based%20tool%29%20which%20allows%0Ausers%20to%20visualize%20and%20assess%20the%20quality%20and%20risk%20of%20synthetic%20data%20provided%0Aby%20desired%20generative%20models.%20Additionally%2C%20the%20utility%20of%20the%20proposed%20methods%0Aand%20the%20web-based%20tool%20is%20showcased%20using%20data%20from%20Alzheimer%27s%20Disease%0ANeuroimaging%20Initiative%20%28ADNI%29%20and%20the%20Center%20for%20Cancer%20Registry%20Data%20of%20the%0ARobert%20Koch%20Institute%20%28RKI%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNFDI4Health%2520workflow%2520and%2520service%2520for%2520synthetic%2520data%2520generation%252C%250A%2520%2520assessment%2520and%2520risk%2520management%26entry.906535625%3DSobhan%2520Moazemi%2520and%2520Tim%2520Adams%2520and%2520Hwei%2520Geok%2520NG%2520and%2520Lisa%2520K%25C3%25BChnel%2520and%2520Julian%2520Schneider%2520and%2520Anatol-Fiete%2520N%25C3%25A4her%2520and%2520Juliane%2520Fluck%2520and%2520Holger%2520Fr%25C3%25B6hlich%26entry.1292438233%3D%2520%2520Individual%2520health%2520data%2520is%2520crucial%2520for%2520scientific%2520advancements%252C%2520particularly%250Ain%2520developing%2520Artificial%2520Intelligence%2520%2528AI%2529%253B%2520however%252C%2520sharing%2520real%2520patient%250Ainformation%2520is%2520often%2520restricted%2520due%2520to%2520privacy%2520concerns.%2520A%2520promising%2520solution%250Ato%2520this%2520challenge%2520is%2520synthetic%2520data%2520generation.%2520This%2520technique%2520creates%2520entirely%250Anew%2520datasets%2520that%2520mimic%2520the%2520statistical%2520properties%2520of%2520real%2520data%252C%2520while%250Apreserving%2520confidential%2520patient%2520information.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%250Aworkflow%2520and%2520different%2520services%2520developed%2520in%2520the%2520context%2520of%2520Germany%2527s%2520National%250AData%2520Infrastructure%2520project%2520NFDI4Health.%2520First%252C%2520two%2520state-of-the-art%2520AI%2520tools%250A%2528namely%252C%2520VAMBN%2520and%2520MultiNODEs%2529%2520for%2520generating%2520synthetic%2520health%2520data%2520are%250Aoutlined.%2520Further%252C%2520we%2520introduce%2520SYNDAT%2520%2528a%2520public%2520web-based%2520tool%2529%2520which%2520allows%250Ausers%2520to%2520visualize%2520and%2520assess%2520the%2520quality%2520and%2520risk%2520of%2520synthetic%2520data%2520provided%250Aby%2520desired%2520generative%2520models.%2520Additionally%252C%2520the%2520utility%2520of%2520the%2520proposed%2520methods%250Aand%2520the%2520web-based%2520tool%2520is%2520showcased%2520using%2520data%2520from%2520Alzheimer%2527s%2520Disease%250ANeuroimaging%2520Initiative%2520%2528ADNI%2529%2520and%2520the%2520Center%2520for%2520Cancer%2520Registry%2520Data%2520of%2520the%250ARobert%2520Koch%2520Institute%2520%2528RKI%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NFDI4Health%20workflow%20and%20service%20for%20synthetic%20data%20generation%2C%0A%20%20assessment%20and%20risk%20management&entry.906535625=Sobhan%20Moazemi%20and%20Tim%20Adams%20and%20Hwei%20Geok%20NG%20and%20Lisa%20K%C3%BChnel%20and%20Julian%20Schneider%20and%20Anatol-Fiete%20N%C3%A4her%20and%20Juliane%20Fluck%20and%20Holger%20Fr%C3%B6hlich&entry.1292438233=%20%20Individual%20health%20data%20is%20crucial%20for%20scientific%20advancements%2C%20particularly%0Ain%20developing%20Artificial%20Intelligence%20%28AI%29%3B%20however%2C%20sharing%20real%20patient%0Ainformation%20is%20often%20restricted%20due%20to%20privacy%20concerns.%20A%20promising%20solution%0Ato%20this%20challenge%20is%20synthetic%20data%20generation.%20This%20technique%20creates%20entirely%0Anew%20datasets%20that%20mimic%20the%20statistical%20properties%20of%20real%20data%2C%20while%0Apreserving%20confidential%20patient%20information.%20In%20this%20paper%2C%20we%20present%20the%0Aworkflow%20and%20different%20services%20developed%20in%20the%20context%20of%20Germany%27s%20National%0AData%20Infrastructure%20project%20NFDI4Health.%20First%2C%20two%20state-of-the-art%20AI%20tools%0A%28namely%2C%20VAMBN%20and%20MultiNODEs%29%20for%20generating%20synthetic%20health%20data%20are%0Aoutlined.%20Further%2C%20we%20introduce%20SYNDAT%20%28a%20public%20web-based%20tool%29%20which%20allows%0Ausers%20to%20visualize%20and%20assess%20the%20quality%20and%20risk%20of%20synthetic%20data%20provided%0Aby%20desired%20generative%20models.%20Additionally%2C%20the%20utility%20of%20the%20proposed%20methods%0Aand%20the%20web-based%20tool%20is%20showcased%20using%20data%20from%20Alzheimer%27s%20Disease%0ANeuroimaging%20Initiative%20%28ADNI%29%20and%20the%20Center%20for%20Cancer%20Registry%20Data%20of%20the%0ARobert%20Koch%20Institute%20%28RKI%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04478v1&entry.124074799=Read"},
{"title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context", "author": " Gemini Team and Petko Georgiev and Ving Ian Lei and Ryan Burnell and Libin Bai and Anmol Gulati and Garrett Tanzer and Damien Vincent and Zhufeng Pan and Shibo Wang and Soroosh Mariooryad and Yifan Ding and Xinyang Geng and Fred Alcober and Roy Frostig and Mark Omernick and Lexi Walker and Cosmin Paduraru and Christina Sorokin and Andrea Tacchetti and Colin Gaffney and Samira Daruki and Olcan Sercinoglu and Zach Gleicher and Juliette Love and Paul Voigtlaender and Rohan Jain and Gabriela Surita and Kareem Mohamed and Rory Blevins and Junwhan Ahn and Tao Zhu and Kornraphop Kawintiranon and Orhan Firat and Yiming Gu and Yujing Zhang and Matthew Rahtz and Manaal Faruqui and Natalie Clay and Justin Gilmer and JD Co-Reyes and Ivo Penchev and Rui Zhu and Nobuyuki Morioka and Kevin Hui and Krishna Haridasan and Victor Campos and Mahdis Mahdieh and Mandy Guo and Samer Hassan and Kevin Kilgour and Arpi Vezer and Heng-Tze Cheng and Raoul de Liedekerke and Siddharth Goyal and Paul Barham and DJ Strouse and Seb Noury and Jonas Adler and Mukund Sundararajan and Sharad Vikram and Dmitry Lepikhin and Michela Paganini and Xavier Garcia and Fan Yang and Dasha Valter and Maja Trebacz and Kiran Vodrahalli and Chulayuth Asawaroengchai and Roman Ring and Norbert Kalb and Livio Baldini Soares and Siddhartha Brahma and David Steiner and Tianhe Yu and Fabian Mentzer and Antoine He and Lucas Gonzalez and Bibo Xu and Raphael Lopez Kaufman and Laurent El Shafey and Junhyuk Oh and Tom Hennigan and George van den Driessche and Seth Odoom and Mario Lucic and Becca Roelofs and Sid Lall and Amit Marathe and Betty Chan and Santiago Ontanon and Luheng He and Denis Teplyashin and Jonathan Lai and Phil Crone and Bogdan Damoc and Lewis Ho and Sebastian Riedel and Karel Lenc and Chih-Kuan Yeh and Aakanksha Chowdhery and Yang Xu and Mehran Kazemi and Ehsan Amid and Anastasia Petrushkina and Kevin Swersky and Ali Khodaei and Gowoon Chen and Chris Larkin and Mario Pinto and Geng Yan and Adria Puigdomenech Badia and Piyush Patil and Steven Hansen and Dave Orr and Sebastien M. R. Arnold and Jordan Grimstad and Andrew Dai and Sholto Douglas and Rishika Sinha and Vikas Yadav and Xi Chen and Elena Gribovskaya and Jacob Austin and Jeffrey Zhao and Kaushal Patel and Paul Komarek and Sophia Austin and Sebastian Borgeaud and Linda Friso and Abhimanyu Goyal and Ben Caine and Kris Cao and Da-Woon Chung and Matthew Lamm and Gabe Barth-Maron and Thais Kagohara and Kate Olszewska and Mia Chen and Kaushik Shivakumar and Rishabh Agarwal and Harshal Godhia and Ravi Rajwar and Javier Snaider and Xerxes Dotiwalla and Yuan Liu and Aditya Barua and Victor Ungureanu and Yuan Zhang and Bat-Orgil Batsaikhan and Mateo Wirth and James Qin and Ivo Danihelka and Tulsee Doshi and Martin Chadwick and Jilin Chen and Sanil Jain and Quoc Le and Arjun Kar and Madhu Gurumurthy and Cheng Li and Ruoxin Sang and Fangyu Liu and Lampros Lamprou and Rich Munoz and Nathan Lintz and Harsh Mehta and Heidi Howard and Malcolm Reynolds and Lora Aroyo and Quan Wang and Lorenzo Blanco and Albin Cassirer and Jordan Griffith and Dipanjan Das and Stephan Lee and Jakub Sygnowski and Zach Fisher and James Besley and Richard Powell and Zafarali Ahmed and Dominik Paulus and David Reitter and Zalan Borsos and Rishabh Joshi and Aedan Pope and Steven Hand and Vittorio Selo and Vihan Jain and Nikhil Sethi and Megha Goel and Takaki Makino and Rhys May and Zhen Yang and Johan Schalkwyk and Christina Butterfield and Anja Hauth and Alex Goldin and Will Hawkins and Evan Senter and Sergey Brin and Oliver Woodman and Marvin Ritter and Eric Noland and Minh Giang and Vijay Bolina and Lisa Lee and Tim Blyth and Ian Mackinnon and Machel Reid and Obaid Sarvana and David Silver and Alexander Chen and Lily Wang and Loren Maggiore and Oscar Chang and Nithya Attaluri and Gregory Thornton and Chung-Cheng Chiu and Oskar Bunyan and Nir Levine and Timothy Chung and Evgenii Eltyshev and Xiance Si and Timothy Lillicrap and Demetra Brady and Vaibhav Aggarwal and Boxi Wu and Yuanzhong Xu and Ross McIlroy and Kartikeya Badola and Paramjit Sandhu and Erica Moreira and Wojciech Stokowiec and Ross Hemsley and Dong Li and Alex Tudor and Pranav Shyam and Elahe Rahimtoroghi and Salem Haykal and Pablo Sprechmann and Xiang Zhou and Diana Mincu and Yujia Li and Ravi Addanki and Kalpesh Krishna and Xiao Wu and Alexandre Frechette and Matan Eyal and Allan Dafoe and Dave Lacey and Jay Whang and Thi Avrahami and Ye Zhang and Emanuel Taropa and Hanzhao Lin and Daniel Toyama and Eliza Rutherford and Motoki Sano and HyunJeong Choe and Alex Tomala and Chalence Safranek-Shrader and Nora Kassner and Mantas Pajarskas and Matt Harvey and Sean Sechrist and Meire Fortunato and Christina Lyu and Gamaleldin Elsayed and Chenkai Kuang and James Lottes and Eric Chu and Chao Jia and Chih-Wei Chen and Peter Humphreys and Kate Baumli and Connie Tao and Rajkumar Samuel and Cicero Nogueira dos Santos and Anders Andreassen and Nemanja Raki\u0107evi\u0107 and Dominik Grewe and Aviral Kumar and Stephanie Winkler and Jonathan Caton and Andrew Brock and Sid Dalmia and Hannah Sheahan and Iain Barr and Yingjie Miao and Paul Natsev and Jacob Devlin and Feryal Behbahani and Flavien Prost and Yanhua Sun and Artiom Myaskovsky and Thanumalayan Sankaranarayana Pillai and Dan Hurt and Angeliki Lazaridou and Xi Xiong and Ce Zheng and Fabio Pardo and Xiaowei Li and Dan Horgan and Joe Stanton and Moran Ambar and Fei Xia and Alejandro Lince and Mingqiu Wang and Basil Mustafa and Albert Webson and Hyo Lee and Rohan Anil and Martin Wicke and Timothy Dozat and Abhishek Sinha and Enrique Piqueras and Elahe Dabir and Shyam Upadhyay and Anudhyan Boral and Lisa Anne Hendricks and Corey Fry and Josip Djolonga and Yi Su and Jake Walker and Jane Labanowski and Ronny Huang and Vedant Misra and Jeremy Chen and RJ Skerry-Ryan and Avi Singh and Shruti Rijhwani and Dian Yu and Alex Castro-Ros and Beer Changpinyo and Romina Datta and Sumit Bagri and Arnar Mar Hrafnkelsson and Marcello Maggioni and Daniel Zheng and Yury Sulsky and Shaobo Hou and Tom Le Paine and Antoine Yang and Jason Riesa and Dominika Rogozinska and Dror Marcus and Dalia El Badawy and Qiao Zhang and Luyu Wang and Helen Miller and Jeremy Greer and Lars Lowe Sjos and Azade Nova and Heiga Zen and Rahma Chaabouni and Mihaela Rosca and Jiepu Jiang and Charlie Chen and Ruibo Liu and Tara Sainath and Maxim Krikun and Alex Polozov and Jean-Baptiste Lespiau and Josh Newlan and Zeyncep Cankara and Soo Kwak and Yunhan Xu and Phil Chen and Andy Coenen and Clemens Meyer and Katerina Tsihlas and Ada Ma and Juraj Gottweis and Jinwei Xing and Chenjie Gu and Jin Miao and Christian Frank and Zeynep Cankara and Sanjay Ganapathy and Ishita Dasgupta and Steph Hughes-Fitt and Heng Chen and David Reid and Keran Rong and Hongmin Fan and Joost van Amersfoort and Vincent Zhuang and Aaron Cohen and Shixiang Shane Gu and Anhad Mohananey and Anastasija Ilic and Taylor Tobin and John Wieting and Anna Bortsova and Phoebe Thacker and Emma Wang and Emily Caveness and Justin Chiu and Eren Sezener and Alex Kaskasoli and Steven Baker and Katie Millican and Mohamed Elhawaty and Kostas Aisopos and Carl Lebsack and Nathan Byrd and Hanjun Dai and Wenhao Jia and Matthew Wiethoff and Elnaz Davoodi and Albert Weston and Lakshman Yagati and Arun Ahuja and Isabel Gao and Golan Pundak and Susan Zhang and Michael Azzam and Khe Chai Sim and Sergi Caelles and James Keeling and Abhanshu Sharma and Andy Swing and YaGuang Li and Chenxi Liu and Carrie Grimes Bostock and Yamini Bansal and Zachary Nado and Ankesh Anand and Josh Lipschultz and Abhijit Karmarkar and Lev Proleev and Abe Ittycheriah and Soheil Hassas Yeganeh and George Polovets and Aleksandra Faust and Jiao Sun and Alban Rrustemi and Pen Li and Rakesh Shivanna and Jeremiah Liu and Chris Welty and Federico Lebron and Anirudh Baddepudi and Sebastian Krause and Emilio Parisotto and Radu Soricut and Zheng Xu and Dawn Bloxwich and Melvin Johnson and Behnam Neyshabur and Justin Mao-Jones and Renshen Wang and Vinay Ramasesh and Zaheer Abbas and Arthur Guez and Constant Segal and Duc Dung Nguyen and James Svensson and Le Hou and Sarah York and Kieran Milan and Sophie Bridgers and Wiktor Gworek and Marco Tagliasacchi and James Lee-Thorp and Michael Chang and Alexey Guseynov and Ale Jakse Hartman and Michael Kwong and Ruizhe Zhao and Sheleem Kashem and Elizabeth Cole and Antoine Miech and Richard Tanburn and Mary Phuong and Filip Pavetic and Sebastien Cevey and Ramona Comanescu and Richard Ives and Sherry Yang and Cosmo Du and Bo Li and Zizhao Zhang and Mariko Iinuma and Clara Huiyi Hu and Aurko Roy and Shaan Bijwadia and Zhenkai Zhu and Danilo Martins and Rachel Saputro and Anita Gergely and Steven Zheng and Dawei Jia and Ioannis Antonoglou and Adam Sadovsky and Shane Gu and Yingying Bi and Alek Andreev and Sina Samangooei and Mina Khan and Tomas Kocisky and Angelos Filos and Chintu Kumar and Colton Bishop and Adams Yu and Sarah Hodkinson and Sid Mittal and Premal Shah and Alexandre Moufarek and Yong Cheng and Adam Bloniarz and Jaehoon Lee and Pedram Pejman and Paul Michel and Stephen Spencer and Vladimir Feinberg and Xuehan Xiong and Nikolay Savinov and Charlotte Smith and Siamak Shakeri and Dustin Tran and Mary Chesus and Bernd Bohnet and George Tucker and Tamara von Glehn and Carrie Muir and Yiran Mao and Hideto Kazawa and Ambrose Slone and Kedar Soparkar and Disha Shrivastava and James Cobon-Kerr and Michael Sharman and Jay Pavagadhi and Carlos Araya and Karolis Misiunas and Nimesh Ghelani and Michael Laskin and David Barker and Qiujia Li and Anton Briukhov and Neil Houlsby and Mia Glaese and Balaji Lakshminarayanan and Nathan Schucher and Yunhao Tang and Eli Collins and Hyeontaek Lim and Fangxiaoyu Feng and Adria Recasens and Guangda Lai and Alberto Magni and Nicola De Cao and Aditya Siddhant and Zoe Ashwood and Jordi Orbay and Mostafa Dehghani and Jenny Brennan and Yifan He and Kelvin Xu and Yang Gao and Carl Saroufim and James Molloy and Xinyi Wu and Seb Arnold and Solomon Chang and Julian Schrittwieser and Elena Buchatskaya and Soroush Radpour and Martin Polacek and Skye Giordano and Ankur Bapna and Simon Tokumine and Vincent Hellendoorn and Thibault Sottiaux and Sarah Cogan and Aliaksei Severyn and Mohammad Saleh and Shantanu Thakoor and Laurent Shefey and Siyuan Qiao and Meenu Gaba and Shuo-yiin Chang and Craig Swanson and Biao Zhang and Benjamin Lee and Paul Kishan Rubenstein and Gan Song and Tom Kwiatkowski and Anna Koop and Ajay Kannan and David Kao and Parker Schuh and Axel Stjerngren and Golnaz Ghiasi and Gena Gibson and Luke Vilnis and Ye Yuan and Felipe Tiengo Ferreira and Aishwarya Kamath and Ted Klimenko and Ken Franko and Kefan Xiao and Indro Bhattacharya and Miteyan Patel and Rui Wang and Alex Morris and Robin Strudel and Vivek Sharma and Peter Choy and Sayed Hadi Hashemi and Jessica Landon and Mara Finkelstein and Priya Jhakra and Justin Frye and Megan Barnes and Matthew Mauger and Dennis Daun and Khuslen Baatarsukh and Matthew Tung and Wael Farhan and Henryk Michalewski and Fabio Viola and Felix de Chaumont Quitry and Charline Le Lan and Tom Hudson and Qingze Wang and Felix Fischer and Ivy Zheng and Elspeth White and Anca Dragan and Jean-baptiste Alayrac and Eric Ni and Alexander Pritzel and Adam Iwanicki and Michael Isard and Anna Bulanova and Lukas Zilka and Ethan Dyer and Devendra Sachan and Srivatsan Srinivasan and Hannah Muckenhirn and Honglong Cai and Amol Mandhane and Mukarram Tariq and Jack W. Rae and Gary Wang and Kareem Ayoub and Nicholas FitzGerald and Yao Zhao and Woohyun Han and Chris Alberti and Dan Garrette and Kashyap Krishnakumar and Mai Gimenez and Anselm Levskaya and Daniel Sohn and Josip Matak and Inaki Iturrate and Michael B. Chang and Jackie Xiang and Yuan Cao and Nishant Ranka and Geoff Brown and Adrian Hutter and Vahab Mirrokni and Nanxin Chen and Kaisheng Yao and Zoltan Egyed and Francois Galilee and Tyler Liechty and Praveen Kallakuri and Evan Palmer and Sanjay Ghemawat and Jasmine Liu and David Tao and Chloe Thornton and Tim Green and Mimi Jasarevic and Sharon Lin and Victor Cotruta and Yi-Xuan Tan and Noah Fiedel and Hongkun Yu and Ed Chi and Alexander Neitz and Jens Heitkaemper and Anu Sinha and Denny Zhou and Yi Sun and Charbel Kaed and Brice Hulse and Swaroop Mishra and Maria Georgaki and Sneha Kudugunta and Clement Farabet and Izhak Shafran and Daniel Vlasic and Anton Tsitsulin and Rajagopal Ananthanarayanan and Alen Carin and Guolong Su and Pei Sun and Shashank V and Gabriel Carvajal and Josef Broder and Iulia Comsa and Alena Repina and William Wong and Warren Weilun Chen and Peter Hawkins and Egor Filonov and Lucia Loher and Christoph Hirnschall and Weiyi Wang and Jingchen Ye and Andrea Burns and Hardie Cate and Diana Gage Wright and Federico Piccinini and Lei Zhang and Chu-Cheng Lin and Ionel Gog and Yana Kulizhskaya and Ashwin Sreevatsa and Shuang Song and Luis C. Cobo and Anand Iyer and Chetan Tekur and Guillermo Garrido and Zhuyun Xiao and Rupert Kemp and Huaixiu Steven Zheng and Hui Li and Ananth Agarwal and Christel Ngani and Kati Goshvadi and Rebeca Santamaria-Fernandez and Wojciech Fica and Xinyun Chen and Chris Gorgolewski and Sean Sun and Roopal Garg and Xinyu Ye and S. M. Ali Eslami and Nan Hua and Jon Simon and Pratik Joshi and Yelin Kim and Ian Tenney and Sahitya Potluri and Lam Nguyen Thiet and Quan Yuan and Florian Luisier and Alexandra Chronopoulou and Salvatore Scellato and Praveen Srinivasan and Minmin Chen and Vinod Koverkathu and Valentin Dalibard and Yaming Xu and Brennan Saeta and Keith Anderson and Thibault Sellam and Nick Fernando and Fantine Huot and Junehyuk Jung and Mani Varadarajan and Michael Quinn and Amit Raul and Maigo Le and Ruslan Habalov and Jon Clark and Komal Jalan and Kalesha Bullard and Achintya Singhal and Thang Luong and Boyu Wang and Sujeevan Rajayogam and Julian Eisenschlos and Johnson Jia and Daniel Finchelstein and Alex Yakubovich and Daniel Balle and Michael Fink and Sameer Agarwal and Jing Li and Dj Dvijotham and Shalini Pal and Kai Kang and Jaclyn Konzelmann and Jennifer Beattie and Olivier Dousse and Diane Wu and Remi Crocker and Chen Elkind and Siddhartha Reddy Jonnalagadda and Jong Lee and Dan Holtmann-Rice and Krystal Kallarackal and Rosanne Liu and Denis Vnukov and Neera Vats and Luca Invernizzi and Mohsen Jafari and Huanjie Zhou and Lilly Taylor and Jennifer Prendki and Marcus Wu and Tom Eccles and Tianqi Liu and Kavya Kopparapu and Francoise Beaufays and Christof Angermueller and Andreea Marzoca and Shourya Sarcar and Hilal Dib and Jeff Stanway and Frank Perbet and Nejc Trdin and Rachel Sterneck and Andrey Khorlin and Dinghua Li and Xihui Wu and Sonam Goenka and David Madras and Sasha Goldshtein and Willi Gierke and Tong Zhou and Yaxin Liu and Yannie Liang and Anais White and Yunjie Li and Shreya Singh and Sanaz Bahargam and Mark Epstein and Sujoy Basu and Li Lao and Adnan Ozturel and Carl Crous and Alex Zhai and Han Lu and Zora Tung and Neeraj Gaur and Alanna Walton and Lucas Dixon and Ming Zhang and Amir Globerson and Grant Uy and Andrew Bolt and Olivia Wiles and Milad Nasr and Ilia Shumailov and Marco Selvi and Francesco Piccinno and Ricardo Aguilar and Sara McCarthy and Misha Khalman and Mrinal Shukla and Vlado Galic and John Carpenter and Kevin Villela and Haibin Zhang and Harry Richardson and James Martens and Matko Bosnjak and Shreyas Rammohan Belle and Jeff Seibert and Mahmoud Alnahlawi and Brian McWilliams and Sankalp Singh and Annie Louis and Wen Ding and Dan Popovici and Lenin Simicich and Laura Knight and Pulkit Mehta and Nishesh Gupta and Chongyang Shi and Saaber Fatehi and Jovana Mitrovic and Alex Grills and Joseph Pagadora and Dessie Petrova and Danielle Eisenbud and Zhishuai Zhang and Damion Yates and Bhavishya Mittal and Nilesh Tripuraneni and Yannis Assael and Thomas Brovelli and Prateek Jain and Mihajlo Velimirovic and Canfer Akbulut and Jiaqi Mu and Wolfgang Macherey and Ravin Kumar and Jun Xu and Haroon Qureshi and Gheorghe Comanici and Jeremy Wiesner and Zhitao Gong and Anton Ruddock and Matthias Bauer and Nick Felt and Anirudh GP and Anurag Arnab and Dustin Zelle and Jonas Rothfuss and Bill Rosgen and Ashish Shenoy and Bryan Seybold and Xinjian Li and Jayaram Mudigonda and Goker Erdogan and Jiawei Xia and Jiri Simsa and Andrea Michi and Yi Yao and Christopher Yew and Steven Kan and Isaac Caswell and Carey Radebaugh and Andre Elisseeff and Pedro Valenzuela and Kay McKinney and Kim Paterson and Albert Cui and Eri Latorre-Chimoto and Solomon Kim and William Zeng and Ken Durden and Priya Ponnapalli and Tiberiu Sosea and Christopher A. Choquette-Choo and James Manyika and Brona Robenek and Harsha Vashisht and Sebastien Pereira and Hoi Lam and Marko Velic and Denese Owusu-Afriyie and Katherine Lee and Tolga Bolukbasi and Alicia Parrish and Shawn Lu and Jane Park and Balaji Venkatraman and Alice Talbert and Lambert Rosique and Yuchung Cheng and Andrei Sozanschi and Adam Paszke and Praveen Kumar and Jessica Austin and Lu Li and Khalid Salama and Wooyeol Kim and Nandita Dukkipati and Anthony Baryshnikov and Christos Kaplanis and XiangHai Sheng and Yuri Chervonyi and Caglar Unlu and Diego de Las Casas and Harry Askham and Kathryn Tunyasuvunakool and Felix Gimeno and Siim Poder and Chester Kwak and Matt Miecnikowski and Vahab Mirrokni and Alek Dimitriev and Aaron Parisi and Dangyi Liu and Tomy Tsai and Toby Shevlane and Christina Kouridi and Drew Garmon and Adrian Goedeckemeyer and Adam R. Brown and Anitha Vijayakumar and Ali Elqursh and Sadegh Jazayeri and Jin Huang and Sara Mc Carthy and Jay Hoover and Lucy Kim and Sandeep Kumar and Wei Chen and Courtney Biles and Garrett Bingham and Evan Rosen and Lisa Wang and Qijun Tan and David Engel and Francesco Pongetti and Dario de Cesare and Dongseong Hwang and Lily Yu and Jennifer Pullman and Srini Narayanan and Kyle Levin and Siddharth Gopal and Megan Li and Asaf Aharoni and Trieu Trinh and Jessica Lo and Norman Casagrande and Roopali Vij and Loic Matthey and Bramandia Ramadhana and Austin Matthews and CJ Carey and Matthew Johnson and Kremena Goranova and Rohin Shah and Shereen Ashraf and Kingshuk Dasgupta and Rasmus Larsen and Yicheng Wang and Manish Reddy Vuyyuru and Chong Jiang and Joana Ijazi and Kazuki Osawa and Celine Smith and Ramya Sree Boppana and Taylan Bilal and Yuma Koizumi and Ying Xu and Yasemin Altun and Nir Shabat and Ben Bariach and Alex Korchemniy and Kiam Choo and Olaf Ronneberger and Chimezie Iwuanyanwu and Shubin Zhao and David Soergel and Cho-Jui Hsieh and Irene Cai and Shariq Iqbal and Martin Sundermeyer and Zhe Chen and Elie Bursztein and Chaitanya Malaviya and Fadi Biadsy and Prakash Shroff and Inderjit Dhillon and Tejasi Latkar and Chris Dyer and Hannah Forbes and Massimo Nicosia and Vitaly Nikolaev and Somer Greene and Marin Georgiev and Pidong Wang and Nina Martin and Hanie Sedghi and John Zhang and Praseem Banzal and Doug Fritz and Vikram Rao and Xuezhi Wang and Jiageng Zhang and Viorica Patraucean and Dayou Du and Igor Mordatch and Ivan Jurin and Lewis Liu and Ayush Dubey and Abhi Mohan and Janek Nowakowski and Vlad-Doru Ion and Nan Wei and Reiko Tojo and Maria Abi Raad and Drew A. Hudson and Vaishakh Keshava and Shubham Agrawal and Kevin Ramirez and Zhichun Wu and Hoang Nguyen and Ji Liu and Madhavi Sewak and Bryce Petrini and DongHyun Choi and Ivan Philips and Ziyue Wang and Ioana Bica and Ankush Garg and Jarek Wilkiewicz and Priyanka Agrawal and Xiaowei Li and Danhao Guo and Emily Xue and Naseer Shaik and Andrew Leach and Sadh MNM Khan and Julia Wiesinger and Sammy Jerome and Abhishek Chakladar and Alek Wenjiao Wang and Tina Ornduff and Folake Abu and Alireza Ghaffarkhah and Marcus Wainwright and Mario Cortes and Frederick Liu and Joshua Maynez and Andreas Terzis and Pouya Samangouei and Riham Mansour and Tomasz K\u0119pa and Fran\u00e7ois-Xavier Aubet and Anton Algymr and Dan Banica and Agoston Weisz and Andras Orban and Alexandre Senges and Ewa Andrejczuk and Mark Geller and Niccolo Dal Santo and Valentin Anklin and Majd Al Merey and Martin Baeuml and Trevor Strohman and Junwen Bai and Slav Petrov and Yonghui Wu and Demis Hassabis and Koray Kavukcuoglu and Jeffrey Dean and Oriol Vinyals", "abstract": "  In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.\n", "link": "http://arxiv.org/abs/2403.05530v4", "date": "2024-08-08", "relevancy": 2.3548, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4794}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context&body=Title%3A%20Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context%0AAuthor%3A%20%20Gemini%20Team%20and%20Petko%20Georgiev%20and%20Ving%20Ian%20Lei%20and%20Ryan%20Burnell%20and%20Libin%20Bai%20and%20Anmol%20Gulati%20and%20Garrett%20Tanzer%20and%20Damien%20Vincent%20and%20Zhufeng%20Pan%20and%20Shibo%20Wang%20and%20Soroosh%20Mariooryad%20and%20Yifan%20Ding%20and%20Xinyang%20Geng%20and%20Fred%20Alcober%20and%20Roy%20Frostig%20and%20Mark%20Omernick%20and%20Lexi%20Walker%20and%20Cosmin%20Paduraru%20and%20Christina%20Sorokin%20and%20Andrea%20Tacchetti%20and%20Colin%20Gaffney%20and%20Samira%20Daruki%20and%20Olcan%20Sercinoglu%20and%20Zach%20Gleicher%20and%20Juliette%20Love%20and%20Paul%20Voigtlaender%20and%20Rohan%20Jain%20and%20Gabriela%20Surita%20and%20Kareem%20Mohamed%20and%20Rory%20Blevins%20and%20Junwhan%20Ahn%20and%20Tao%20Zhu%20and%20Kornraphop%20Kawintiranon%20and%20Orhan%20Firat%20and%20Yiming%20Gu%20and%20Yujing%20Zhang%20and%20Matthew%20Rahtz%20and%20Manaal%20Faruqui%20and%20Natalie%20Clay%20and%20Justin%20Gilmer%20and%20JD%20Co-Reyes%20and%20Ivo%20Penchev%20and%20Rui%20Zhu%20and%20Nobuyuki%20Morioka%20and%20Kevin%20Hui%20and%20Krishna%20Haridasan%20and%20Victor%20Campos%20and%20Mahdis%20Mahdieh%20and%20Mandy%20Guo%20and%20Samer%20Hassan%20and%20Kevin%20Kilgour%20and%20Arpi%20Vezer%20and%20Heng-Tze%20Cheng%20and%20Raoul%20de%20Liedekerke%20and%20Siddharth%20Goyal%20and%20Paul%20Barham%20and%20DJ%20Strouse%20and%20Seb%20Noury%20and%20Jonas%20Adler%20and%20Mukund%20Sundararajan%20and%20Sharad%20Vikram%20and%20Dmitry%20Lepikhin%20and%20Michela%20Paganini%20and%20Xavier%20Garcia%20and%20Fan%20Yang%20and%20Dasha%20Valter%20and%20Maja%20Trebacz%20and%20Kiran%20Vodrahalli%20and%20Chulayuth%20Asawaroengchai%20and%20Roman%20Ring%20and%20Norbert%20Kalb%20and%20Livio%20Baldini%20Soares%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Tianhe%20Yu%20and%20Fabian%20Mentzer%20and%20Antoine%20He%20and%20Lucas%20Gonzalez%20and%20Bibo%20Xu%20and%20Raphael%20Lopez%20Kaufman%20and%20Laurent%20El%20Shafey%20and%20Junhyuk%20Oh%20and%20Tom%20Hennigan%20and%20George%20van%20den%20Driessche%20and%20Seth%20Odoom%20and%20Mario%20Lucic%20and%20Becca%20Roelofs%20and%20Sid%20Lall%20and%20Amit%20Marathe%20and%20Betty%20Chan%20and%20Santiago%20Ontanon%20and%20Luheng%20He%20and%20Denis%20Teplyashin%20and%20Jonathan%20Lai%20and%20Phil%20Crone%20and%20Bogdan%20Damoc%20and%20Lewis%20Ho%20and%20Sebastian%20Riedel%20and%20Karel%20Lenc%20and%20Chih-Kuan%20Yeh%20and%20Aakanksha%20Chowdhery%20and%20Yang%20Xu%20and%20Mehran%20Kazemi%20and%20Ehsan%20Amid%20and%20Anastasia%20Petrushkina%20and%20Kevin%20Swersky%20and%20Ali%20Khodaei%20and%20Gowoon%20Chen%20and%20Chris%20Larkin%20and%20Mario%20Pinto%20and%20Geng%20Yan%20and%20Adria%20Puigdomenech%20Badia%20and%20Piyush%20Patil%20and%20Steven%20Hansen%20and%20Dave%20Orr%20and%20Sebastien%20M.%20R.%20Arnold%20and%20Jordan%20Grimstad%20and%20Andrew%20Dai%20and%20Sholto%20Douglas%20and%20Rishika%20Sinha%20and%20Vikas%20Yadav%20and%20Xi%20Chen%20and%20Elena%20Gribovskaya%20and%20Jacob%20Austin%20and%20Jeffrey%20Zhao%20and%20Kaushal%20Patel%20and%20Paul%20Komarek%20and%20Sophia%20Austin%20and%20Sebastian%20Borgeaud%20and%20Linda%20Friso%20and%20Abhimanyu%20Goyal%20and%20Ben%20Caine%20and%20Kris%20Cao%20and%20Da-Woon%20Chung%20and%20Matthew%20Lamm%20and%20Gabe%20Barth-Maron%20and%20Thais%20Kagohara%20and%20Kate%20Olszewska%20and%20Mia%20Chen%20and%20Kaushik%20Shivakumar%20and%20Rishabh%20Agarwal%20and%20Harshal%20Godhia%20and%20Ravi%20Rajwar%20and%20Javier%20Snaider%20and%20Xerxes%20Dotiwalla%20and%20Yuan%20Liu%20and%20Aditya%20Barua%20and%20Victor%20Ungureanu%20and%20Yuan%20Zhang%20and%20Bat-Orgil%20Batsaikhan%20and%20Mateo%20Wirth%20and%20James%20Qin%20and%20Ivo%20Danihelka%20and%20Tulsee%20Doshi%20and%20Martin%20Chadwick%20and%20Jilin%20Chen%20and%20Sanil%20Jain%20and%20Quoc%20Le%20and%20Arjun%20Kar%20and%20Madhu%20Gurumurthy%20and%20Cheng%20Li%20and%20Ruoxin%20Sang%20and%20Fangyu%20Liu%20and%20Lampros%20Lamprou%20and%20Rich%20Munoz%20and%20Nathan%20Lintz%20and%20Harsh%20Mehta%20and%20Heidi%20Howard%20and%20Malcolm%20Reynolds%20and%20Lora%20Aroyo%20and%20Quan%20Wang%20and%20Lorenzo%20Blanco%20and%20Albin%20Cassirer%20and%20Jordan%20Griffith%20and%20Dipanjan%20Das%20and%20Stephan%20Lee%20and%20Jakub%20Sygnowski%20and%20Zach%20Fisher%20and%20James%20Besley%20and%20Richard%20Powell%20and%20Zafarali%20Ahmed%20and%20Dominik%20Paulus%20and%20David%20Reitter%20and%20Zalan%20Borsos%20and%20Rishabh%20Joshi%20and%20Aedan%20Pope%20and%20Steven%20Hand%20and%20Vittorio%20Selo%20and%20Vihan%20Jain%20and%20Nikhil%20Sethi%20and%20Megha%20Goel%20and%20Takaki%20Makino%20and%20Rhys%20May%20and%20Zhen%20Yang%20and%20Johan%20Schalkwyk%20and%20Christina%20Butterfield%20and%20Anja%20Hauth%20and%20Alex%20Goldin%20and%20Will%20Hawkins%20and%20Evan%20Senter%20and%20Sergey%20Brin%20and%20Oliver%20Woodman%20and%20Marvin%20Ritter%20and%20Eric%20Noland%20and%20Minh%20Giang%20and%20Vijay%20Bolina%20and%20Lisa%20Lee%20and%20Tim%20Blyth%20and%20Ian%20Mackinnon%20and%20Machel%20Reid%20and%20Obaid%20Sarvana%20and%20David%20Silver%20and%20Alexander%20Chen%20and%20Lily%20Wang%20and%20Loren%20Maggiore%20and%20Oscar%20Chang%20and%20Nithya%20Attaluri%20and%20Gregory%20Thornton%20and%20Chung-Cheng%20Chiu%20and%20Oskar%20Bunyan%20and%20Nir%20Levine%20and%20Timothy%20Chung%20and%20Evgenii%20Eltyshev%20and%20Xiance%20Si%20and%20Timothy%20Lillicrap%20and%20Demetra%20Brady%20and%20Vaibhav%20Aggarwal%20and%20Boxi%20Wu%20and%20Yuanzhong%20Xu%20and%20Ross%20McIlroy%20and%20Kartikeya%20Badola%20and%20Paramjit%20Sandhu%20and%20Erica%20Moreira%20and%20Wojciech%20Stokowiec%20and%20Ross%20Hemsley%20and%20Dong%20Li%20and%20Alex%20Tudor%20and%20Pranav%20Shyam%20and%20Elahe%20Rahimtoroghi%20and%20Salem%20Haykal%20and%20Pablo%20Sprechmann%20and%20Xiang%20Zhou%20and%20Diana%20Mincu%20and%20Yujia%20Li%20and%20Ravi%20Addanki%20and%20Kalpesh%20Krishna%20and%20Xiao%20Wu%20and%20Alexandre%20Frechette%20and%20Matan%20Eyal%20and%20Allan%20Dafoe%20and%20Dave%20Lacey%20and%20Jay%20Whang%20and%20Thi%20Avrahami%20and%20Ye%20Zhang%20and%20Emanuel%20Taropa%20and%20Hanzhao%20Lin%20and%20Daniel%20Toyama%20and%20Eliza%20Rutherford%20and%20Motoki%20Sano%20and%20HyunJeong%20Choe%20and%20Alex%20Tomala%20and%20Chalence%20Safranek-Shrader%20and%20Nora%20Kassner%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Sean%20Sechrist%20and%20Meire%20Fortunato%20and%20Christina%20Lyu%20and%20Gamaleldin%20Elsayed%20and%20Chenkai%20Kuang%20and%20James%20Lottes%20and%20Eric%20Chu%20and%20Chao%20Jia%20and%20Chih-Wei%20Chen%20and%20Peter%20Humphreys%20and%20Kate%20Baumli%20and%20Connie%20Tao%20and%20Rajkumar%20Samuel%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Anders%20Andreassen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Dominik%20Grewe%20and%20Aviral%20Kumar%20and%20Stephanie%20Winkler%20and%20Jonathan%20Caton%20and%20Andrew%20Brock%20and%20Sid%20Dalmia%20and%20Hannah%20Sheahan%20and%20Iain%20Barr%20and%20Yingjie%20Miao%20and%20Paul%20Natsev%20and%20Jacob%20Devlin%20and%20Feryal%20Behbahani%20and%20Flavien%20Prost%20and%20Yanhua%20Sun%20and%20Artiom%20Myaskovsky%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Dan%20Hurt%20and%20Angeliki%20Lazaridou%20and%20Xi%20Xiong%20and%20Ce%20Zheng%20and%20Fabio%20Pardo%20and%20Xiaowei%20Li%20and%20Dan%20Horgan%20and%20Joe%20Stanton%20and%20Moran%20Ambar%20and%20Fei%20Xia%20and%20Alejandro%20Lince%20and%20Mingqiu%20Wang%20and%20Basil%20Mustafa%20and%20Albert%20Webson%20and%20Hyo%20Lee%20and%20Rohan%20Anil%20and%20Martin%20Wicke%20and%20Timothy%20Dozat%20and%20Abhishek%20Sinha%20and%20Enrique%20Piqueras%20and%20Elahe%20Dabir%20and%20Shyam%20Upadhyay%20and%20Anudhyan%20Boral%20and%20Lisa%20Anne%20Hendricks%20and%20Corey%20Fry%20and%20Josip%20Djolonga%20and%20Yi%20Su%20and%20Jake%20Walker%20and%20Jane%20Labanowski%20and%20Ronny%20Huang%20and%20Vedant%20Misra%20and%20Jeremy%20Chen%20and%20RJ%20Skerry-Ryan%20and%20Avi%20Singh%20and%20Shruti%20Rijhwani%20and%20Dian%20Yu%20and%20Alex%20Castro-Ros%20and%20Beer%20Changpinyo%20and%20Romina%20Datta%20and%20Sumit%20Bagri%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Marcello%20Maggioni%20and%20Daniel%20Zheng%20and%20Yury%20Sulsky%20and%20Shaobo%20Hou%20and%20Tom%20Le%20Paine%20and%20Antoine%20Yang%20and%20Jason%20Riesa%20and%20Dominika%20Rogozinska%20and%20Dror%20Marcus%20and%20Dalia%20El%20Badawy%20and%20Qiao%20Zhang%20and%20Luyu%20Wang%20and%20Helen%20Miller%20and%20Jeremy%20Greer%20and%20Lars%20Lowe%20Sjos%20and%20Azade%20Nova%20and%20Heiga%20Zen%20and%20Rahma%20Chaabouni%20and%20Mihaela%20Rosca%20and%20Jiepu%20Jiang%20and%20Charlie%20Chen%20and%20Ruibo%20Liu%20and%20Tara%20Sainath%20and%20Maxim%20Krikun%20and%20Alex%20Polozov%20and%20Jean-Baptiste%20Lespiau%20and%20Josh%20Newlan%20and%20Zeyncep%20Cankara%20and%20Soo%20Kwak%20and%20Yunhan%20Xu%20and%20Phil%20Chen%20and%20Andy%20Coenen%20and%20Clemens%20Meyer%20and%20Katerina%20Tsihlas%20and%20Ada%20Ma%20and%20Juraj%20Gottweis%20and%20Jinwei%20Xing%20and%20Chenjie%20Gu%20and%20Jin%20Miao%20and%20Christian%20Frank%20and%20Zeynep%20Cankara%20and%20Sanjay%20Ganapathy%20and%20Ishita%20Dasgupta%20and%20Steph%20Hughes-Fitt%20and%20Heng%20Chen%20and%20David%20Reid%20and%20Keran%20Rong%20and%20Hongmin%20Fan%20and%20Joost%20van%20Amersfoort%20and%20Vincent%20Zhuang%20and%20Aaron%20Cohen%20and%20Shixiang%20Shane%20Gu%20and%20Anhad%20Mohananey%20and%20Anastasija%20Ilic%20and%20Taylor%20Tobin%20and%20John%20Wieting%20and%20Anna%20Bortsova%20and%20Phoebe%20Thacker%20and%20Emma%20Wang%20and%20Emily%20Caveness%20and%20Justin%20Chiu%20and%20Eren%20Sezener%20and%20Alex%20Kaskasoli%20and%20Steven%20Baker%20and%20Katie%20Millican%20and%20Mohamed%20Elhawaty%20and%20Kostas%20Aisopos%20and%20Carl%20Lebsack%20and%20Nathan%20Byrd%20and%20Hanjun%20Dai%20and%20Wenhao%20Jia%20and%20Matthew%20Wiethoff%20and%20Elnaz%20Davoodi%20and%20Albert%20Weston%20and%20Lakshman%20Yagati%20and%20Arun%20Ahuja%20and%20Isabel%20Gao%20and%20Golan%20Pundak%20and%20Susan%20Zhang%20and%20Michael%20Azzam%20and%20Khe%20Chai%20Sim%20and%20Sergi%20Caelles%20and%20James%20Keeling%20and%20Abhanshu%20Sharma%20and%20Andy%20Swing%20and%20YaGuang%20Li%20and%20Chenxi%20Liu%20and%20Carrie%20Grimes%20Bostock%20and%20Yamini%20Bansal%20and%20Zachary%20Nado%20and%20Ankesh%20Anand%20and%20Josh%20Lipschultz%20and%20Abhijit%20Karmarkar%20and%20Lev%20Proleev%20and%20Abe%20Ittycheriah%20and%20Soheil%20Hassas%20Yeganeh%20and%20George%20Polovets%20and%20Aleksandra%20Faust%20and%20Jiao%20Sun%20and%20Alban%20Rrustemi%20and%20Pen%20Li%20and%20Rakesh%20Shivanna%20and%20Jeremiah%20Liu%20and%20Chris%20Welty%20and%20Federico%20Lebron%20and%20Anirudh%20Baddepudi%20and%20Sebastian%20Krause%20and%20Emilio%20Parisotto%20and%20Radu%20Soricut%20and%20Zheng%20Xu%20and%20Dawn%20Bloxwich%20and%20Melvin%20Johnson%20and%20Behnam%20Neyshabur%20and%20Justin%20Mao-Jones%20and%20Renshen%20Wang%20and%20Vinay%20Ramasesh%20and%20Zaheer%20Abbas%20and%20Arthur%20Guez%20and%20Constant%20Segal%20and%20Duc%20Dung%20Nguyen%20and%20James%20Svensson%20and%20Le%20Hou%20and%20Sarah%20York%20and%20Kieran%20Milan%20and%20Sophie%20Bridgers%20and%20Wiktor%20Gworek%20and%20Marco%20Tagliasacchi%20and%20James%20Lee-Thorp%20and%20Michael%20Chang%20and%20Alexey%20Guseynov%20and%20Ale%20Jakse%20Hartman%20and%20Michael%20Kwong%20and%20Ruizhe%20Zhao%20and%20Sheleem%20Kashem%20and%20Elizabeth%20Cole%20and%20Antoine%20Miech%20and%20Richard%20Tanburn%20and%20Mary%20Phuong%20and%20Filip%20Pavetic%20and%20Sebastien%20Cevey%20and%20Ramona%20Comanescu%20and%20Richard%20Ives%20and%20Sherry%20Yang%20and%20Cosmo%20Du%20and%20Bo%20Li%20and%20Zizhao%20Zhang%20and%20Mariko%20Iinuma%20and%20Clara%20Huiyi%20Hu%20and%20Aurko%20Roy%20and%20Shaan%20Bijwadia%20and%20Zhenkai%20Zhu%20and%20Danilo%20Martins%20and%20Rachel%20Saputro%20and%20Anita%20Gergely%20and%20Steven%20Zheng%20and%20Dawei%20Jia%20and%20Ioannis%20Antonoglou%20and%20Adam%20Sadovsky%20and%20Shane%20Gu%20and%20Yingying%20Bi%20and%20Alek%20Andreev%20and%20Sina%20Samangooei%20and%20Mina%20Khan%20and%20Tomas%20Kocisky%20and%20Angelos%20Filos%20and%20Chintu%20Kumar%20and%20Colton%20Bishop%20and%20Adams%20Yu%20and%20Sarah%20Hodkinson%20and%20Sid%20Mittal%20and%20Premal%20Shah%20and%20Alexandre%20Moufarek%20and%20Yong%20Cheng%20and%20Adam%20Bloniarz%20and%20Jaehoon%20Lee%20and%20Pedram%20Pejman%20and%20Paul%20Michel%20and%20Stephen%20Spencer%20and%20Vladimir%20Feinberg%20and%20Xuehan%20Xiong%20and%20Nikolay%20Savinov%20and%20Charlotte%20Smith%20and%20Siamak%20Shakeri%20and%20Dustin%20Tran%20and%20Mary%20Chesus%20and%20Bernd%20Bohnet%20and%20George%20Tucker%20and%20Tamara%20von%20Glehn%20and%20Carrie%20Muir%20and%20Yiran%20Mao%20and%20Hideto%20Kazawa%20and%20Ambrose%20Slone%20and%20Kedar%20Soparkar%20and%20Disha%20Shrivastava%20and%20James%20Cobon-Kerr%20and%20Michael%20Sharman%20and%20Jay%20Pavagadhi%20and%20Carlos%20Araya%20and%20Karolis%20Misiunas%20and%20Nimesh%20Ghelani%20and%20Michael%20Laskin%20and%20David%20Barker%20and%20Qiujia%20Li%20and%20Anton%20Briukhov%20and%20Neil%20Houlsby%20and%20Mia%20Glaese%20and%20Balaji%20Lakshminarayanan%20and%20Nathan%20Schucher%20and%20Yunhao%20Tang%20and%20Eli%20Collins%20and%20Hyeontaek%20Lim%20and%20Fangxiaoyu%20Feng%20and%20Adria%20Recasens%20and%20Guangda%20Lai%20and%20Alberto%20Magni%20and%20Nicola%20De%20Cao%20and%20Aditya%20Siddhant%20and%20Zoe%20Ashwood%20and%20Jordi%20Orbay%20and%20Mostafa%20Dehghani%20and%20Jenny%20Brennan%20and%20Yifan%20He%20and%20Kelvin%20Xu%20and%20Yang%20Gao%20and%20Carl%20Saroufim%20and%20James%20Molloy%20and%20Xinyi%20Wu%20and%20Seb%20Arnold%20and%20Solomon%20Chang%20and%20Julian%20Schrittwieser%20and%20Elena%20Buchatskaya%20and%20Soroush%20Radpour%20and%20Martin%20Polacek%20and%20Skye%20Giordano%20and%20Ankur%20Bapna%20and%20Simon%20Tokumine%20and%20Vincent%20Hellendoorn%20and%20Thibault%20Sottiaux%20and%20Sarah%20Cogan%20and%20Aliaksei%20Severyn%20and%20Mohammad%20Saleh%20and%20Shantanu%20Thakoor%20and%20Laurent%20Shefey%20and%20Siyuan%20Qiao%20and%20Meenu%20Gaba%20and%20Shuo-yiin%20Chang%20and%20Craig%20Swanson%20and%20Biao%20Zhang%20and%20Benjamin%20Lee%20and%20Paul%20Kishan%20Rubenstein%20and%20Gan%20Song%20and%20Tom%20Kwiatkowski%20and%20Anna%20Koop%20and%20Ajay%20Kannan%20and%20David%20Kao%20and%20Parker%20Schuh%20and%20Axel%20Stjerngren%20and%20Golnaz%20Ghiasi%20and%20Gena%20Gibson%20and%20Luke%20Vilnis%20and%20Ye%20Yuan%20and%20Felipe%20Tiengo%20Ferreira%20and%20Aishwarya%20Kamath%20and%20Ted%20Klimenko%20and%20Ken%20Franko%20and%20Kefan%20Xiao%20and%20Indro%20Bhattacharya%20and%20Miteyan%20Patel%20and%20Rui%20Wang%20and%20Alex%20Morris%20and%20Robin%20Strudel%20and%20Vivek%20Sharma%20and%20Peter%20Choy%20and%20Sayed%20Hadi%20Hashemi%20and%20Jessica%20Landon%20and%20Mara%20Finkelstein%20and%20Priya%20Jhakra%20and%20Justin%20Frye%20and%20Megan%20Barnes%20and%20Matthew%20Mauger%20and%20Dennis%20Daun%20and%20Khuslen%20Baatarsukh%20and%20Matthew%20Tung%20and%20Wael%20Farhan%20and%20Henryk%20Michalewski%20and%20Fabio%20Viola%20and%20Felix%20de%20Chaumont%20Quitry%20and%20Charline%20Le%20Lan%20and%20Tom%20Hudson%20and%20Qingze%20Wang%20and%20Felix%20Fischer%20and%20Ivy%20Zheng%20and%20Elspeth%20White%20and%20Anca%20Dragan%20and%20Jean-baptiste%20Alayrac%20and%20Eric%20Ni%20and%20Alexander%20Pritzel%20and%20Adam%20Iwanicki%20and%20Michael%20Isard%20and%20Anna%20Bulanova%20and%20Lukas%20Zilka%20and%20Ethan%20Dyer%20and%20Devendra%20Sachan%20and%20Srivatsan%20Srinivasan%20and%20Hannah%20Muckenhirn%20and%20Honglong%20Cai%20and%20Amol%20Mandhane%20and%20Mukarram%20Tariq%20and%20Jack%20W.%20Rae%20and%20Gary%20Wang%20and%20Kareem%20Ayoub%20and%20Nicholas%20FitzGerald%20and%20Yao%20Zhao%20and%20Woohyun%20Han%20and%20Chris%20Alberti%20and%20Dan%20Garrette%20and%20Kashyap%20Krishnakumar%20and%20Mai%20Gimenez%20and%20Anselm%20Levskaya%20and%20Daniel%20Sohn%20and%20Josip%20Matak%20and%20Inaki%20Iturrate%20and%20Michael%20B.%20Chang%20and%20Jackie%20Xiang%20and%20Yuan%20Cao%20and%20Nishant%20Ranka%20and%20Geoff%20Brown%20and%20Adrian%20Hutter%20and%20Vahab%20Mirrokni%20and%20Nanxin%20Chen%20and%20Kaisheng%20Yao%20and%20Zoltan%20Egyed%20and%20Francois%20Galilee%20and%20Tyler%20Liechty%20and%20Praveen%20Kallakuri%20and%20Evan%20Palmer%20and%20Sanjay%20Ghemawat%20and%20Jasmine%20Liu%20and%20David%20Tao%20and%20Chloe%20Thornton%20and%20Tim%20Green%20and%20Mimi%20Jasarevic%20and%20Sharon%20Lin%20and%20Victor%20Cotruta%20and%20Yi-Xuan%20Tan%20and%20Noah%20Fiedel%20and%20Hongkun%20Yu%20and%20Ed%20Chi%20and%20Alexander%20Neitz%20and%20Jens%20Heitkaemper%20and%20Anu%20Sinha%20and%20Denny%20Zhou%20and%20Yi%20Sun%20and%20Charbel%20Kaed%20and%20Brice%20Hulse%20and%20Swaroop%20Mishra%20and%20Maria%20Georgaki%20and%20Sneha%20Kudugunta%20and%20Clement%20Farabet%20and%20Izhak%20Shafran%20and%20Daniel%20Vlasic%20and%20Anton%20Tsitsulin%20and%20Rajagopal%20Ananthanarayanan%20and%20Alen%20Carin%20and%20Guolong%20Su%20and%20Pei%20Sun%20and%20Shashank%20V%20and%20Gabriel%20Carvajal%20and%20Josef%20Broder%20and%20Iulia%20Comsa%20and%20Alena%20Repina%20and%20William%20Wong%20and%20Warren%20Weilun%20Chen%20and%20Peter%20Hawkins%20and%20Egor%20Filonov%20and%20Lucia%20Loher%20and%20Christoph%20Hirnschall%20and%20Weiyi%20Wang%20and%20Jingchen%20Ye%20and%20Andrea%20Burns%20and%20Hardie%20Cate%20and%20Diana%20Gage%20Wright%20and%20Federico%20Piccinini%20and%20Lei%20Zhang%20and%20Chu-Cheng%20Lin%20and%20Ionel%20Gog%20and%20Yana%20Kulizhskaya%20and%20Ashwin%20Sreevatsa%20and%20Shuang%20Song%20and%20Luis%20C.%20Cobo%20and%20Anand%20Iyer%20and%20Chetan%20Tekur%20and%20Guillermo%20Garrido%20and%20Zhuyun%20Xiao%20and%20Rupert%20Kemp%20and%20Huaixiu%20Steven%20Zheng%20and%20Hui%20Li%20and%20Ananth%20Agarwal%20and%20Christel%20Ngani%20and%20Kati%20Goshvadi%20and%20Rebeca%20Santamaria-Fernandez%20and%20Wojciech%20Fica%20and%20Xinyun%20Chen%20and%20Chris%20Gorgolewski%20and%20Sean%20Sun%20and%20Roopal%20Garg%20and%20Xinyu%20Ye%20and%20S.%20M.%20Ali%20Eslami%20and%20Nan%20Hua%20and%20Jon%20Simon%20and%20Pratik%20Joshi%20and%20Yelin%20Kim%20and%20Ian%20Tenney%20and%20Sahitya%20Potluri%20and%20Lam%20Nguyen%20Thiet%20and%20Quan%20Yuan%20and%20Florian%20Luisier%20and%20Alexandra%20Chronopoulou%20and%20Salvatore%20Scellato%20and%20Praveen%20Srinivasan%20and%20Minmin%20Chen%20and%20Vinod%20Koverkathu%20and%20Valentin%20Dalibard%20and%20Yaming%20Xu%20and%20Brennan%20Saeta%20and%20Keith%20Anderson%20and%20Thibault%20Sellam%20and%20Nick%20Fernando%20and%20Fantine%20Huot%20and%20Junehyuk%20Jung%20and%20Mani%20Varadarajan%20and%20Michael%20Quinn%20and%20Amit%20Raul%20and%20Maigo%20Le%20and%20Ruslan%20Habalov%20and%20Jon%20Clark%20and%20Komal%20Jalan%20and%20Kalesha%20Bullard%20and%20Achintya%20Singhal%20and%20Thang%20Luong%20and%20Boyu%20Wang%20and%20Sujeevan%20Rajayogam%20and%20Julian%20Eisenschlos%20and%20Johnson%20Jia%20and%20Daniel%20Finchelstein%20and%20Alex%20Yakubovich%20and%20Daniel%20Balle%20and%20Michael%20Fink%20and%20Sameer%20Agarwal%20and%20Jing%20Li%20and%20Dj%20Dvijotham%20and%20Shalini%20Pal%20and%20Kai%20Kang%20and%20Jaclyn%20Konzelmann%20and%20Jennifer%20Beattie%20and%20Olivier%20Dousse%20and%20Diane%20Wu%20and%20Remi%20Crocker%20and%20Chen%20Elkind%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Jong%20Lee%20and%20Dan%20Holtmann-Rice%20and%20Krystal%20Kallarackal%20and%20Rosanne%20Liu%20and%20Denis%20Vnukov%20and%20Neera%20Vats%20and%20Luca%20Invernizzi%20and%20Mohsen%20Jafari%20and%20Huanjie%20Zhou%20and%20Lilly%20Taylor%20and%20Jennifer%20Prendki%20and%20Marcus%20Wu%20and%20Tom%20Eccles%20and%20Tianqi%20Liu%20and%20Kavya%20Kopparapu%20and%20Francoise%20Beaufays%20and%20Christof%20Angermueller%20and%20Andreea%20Marzoca%20and%20Shourya%20Sarcar%20and%20Hilal%20Dib%20and%20Jeff%20Stanway%20and%20Frank%20Perbet%20and%20Nejc%20Trdin%20and%20Rachel%20Sterneck%20and%20Andrey%20Khorlin%20and%20Dinghua%20Li%20and%20Xihui%20Wu%20and%20Sonam%20Goenka%20and%20David%20Madras%20and%20Sasha%20Goldshtein%20and%20Willi%20Gierke%20and%20Tong%20Zhou%20and%20Yaxin%20Liu%20and%20Yannie%20Liang%20and%20Anais%20White%20and%20Yunjie%20Li%20and%20Shreya%20Singh%20and%20Sanaz%20Bahargam%20and%20Mark%20Epstein%20and%20Sujoy%20Basu%20and%20Li%20Lao%20and%20Adnan%20Ozturel%20and%20Carl%20Crous%20and%20Alex%20Zhai%20and%20Han%20Lu%20and%20Zora%20Tung%20and%20Neeraj%20Gaur%20and%20Alanna%20Walton%20and%20Lucas%20Dixon%20and%20Ming%20Zhang%20and%20Amir%20Globerson%20and%20Grant%20Uy%20and%20Andrew%20Bolt%20and%20Olivia%20Wiles%20and%20Milad%20Nasr%20and%20Ilia%20Shumailov%20and%20Marco%20Selvi%20and%20Francesco%20Piccinno%20and%20Ricardo%20Aguilar%20and%20Sara%20McCarthy%20and%20Misha%20Khalman%20and%20Mrinal%20Shukla%20and%20Vlado%20Galic%20and%20John%20Carpenter%20and%20Kevin%20Villela%20and%20Haibin%20Zhang%20and%20Harry%20Richardson%20and%20James%20Martens%20and%20Matko%20Bosnjak%20and%20Shreyas%20Rammohan%20Belle%20and%20Jeff%20Seibert%20and%20Mahmoud%20Alnahlawi%20and%20Brian%20McWilliams%20and%20Sankalp%20Singh%20and%20Annie%20Louis%20and%20Wen%20Ding%20and%20Dan%20Popovici%20and%20Lenin%20Simicich%20and%20Laura%20Knight%20and%20Pulkit%20Mehta%20and%20Nishesh%20Gupta%20and%20Chongyang%20Shi%20and%20Saaber%20Fatehi%20and%20Jovana%20Mitrovic%20and%20Alex%20Grills%20and%20Joseph%20Pagadora%20and%20Dessie%20Petrova%20and%20Danielle%20Eisenbud%20and%20Zhishuai%20Zhang%20and%20Damion%20Yates%20and%20Bhavishya%20Mittal%20and%20Nilesh%20Tripuraneni%20and%20Yannis%20Assael%20and%20Thomas%20Brovelli%20and%20Prateek%20Jain%20and%20Mihajlo%20Velimirovic%20and%20Canfer%20Akbulut%20and%20Jiaqi%20Mu%20and%20Wolfgang%20Macherey%20and%20Ravin%20Kumar%20and%20Jun%20Xu%20and%20Haroon%20Qureshi%20and%20Gheorghe%20Comanici%20and%20Jeremy%20Wiesner%20and%20Zhitao%20Gong%20and%20Anton%20Ruddock%20and%20Matthias%20Bauer%20and%20Nick%20Felt%20and%20Anirudh%20GP%20and%20Anurag%20Arnab%20and%20Dustin%20Zelle%20and%20Jonas%20Rothfuss%20and%20Bill%20Rosgen%20and%20Ashish%20Shenoy%20and%20Bryan%20Seybold%20and%20Xinjian%20Li%20and%20Jayaram%20Mudigonda%20and%20Goker%20Erdogan%20and%20Jiawei%20Xia%20and%20Jiri%20Simsa%20and%20Andrea%20Michi%20and%20Yi%20Yao%20and%20Christopher%20Yew%20and%20Steven%20Kan%20and%20Isaac%20Caswell%20and%20Carey%20Radebaugh%20and%20Andre%20Elisseeff%20and%20Pedro%20Valenzuela%20and%20Kay%20McKinney%20and%20Kim%20Paterson%20and%20Albert%20Cui%20and%20Eri%20Latorre-Chimoto%20and%20Solomon%20Kim%20and%20William%20Zeng%20and%20Ken%20Durden%20and%20Priya%20Ponnapalli%20and%20Tiberiu%20Sosea%20and%20Christopher%20A.%20Choquette-Choo%20and%20James%20Manyika%20and%20Brona%20Robenek%20and%20Harsha%20Vashisht%20and%20Sebastien%20Pereira%20and%20Hoi%20Lam%20and%20Marko%20Velic%20and%20Denese%20Owusu-Afriyie%20and%20Katherine%20Lee%20and%20Tolga%20Bolukbasi%20and%20Alicia%20Parrish%20and%20Shawn%20Lu%20and%20Jane%20Park%20and%20Balaji%20Venkatraman%20and%20Alice%20Talbert%20and%20Lambert%20Rosique%20and%20Yuchung%20Cheng%20and%20Andrei%20Sozanschi%20and%20Adam%20Paszke%20and%20Praveen%20Kumar%20and%20Jessica%20Austin%20and%20Lu%20Li%20and%20Khalid%20Salama%20and%20Wooyeol%20Kim%20and%20Nandita%20Dukkipati%20and%20Anthony%20Baryshnikov%20and%20Christos%20Kaplanis%20and%20XiangHai%20Sheng%20and%20Yuri%20Chervonyi%20and%20Caglar%20Unlu%20and%20Diego%20de%20Las%20Casas%20and%20Harry%20Askham%20and%20Kathryn%20Tunyasuvunakool%20and%20Felix%20Gimeno%20and%20Siim%20Poder%20and%20Chester%20Kwak%20and%20Matt%20Miecnikowski%20and%20Vahab%20Mirrokni%20and%20Alek%20Dimitriev%20and%20Aaron%20Parisi%20and%20Dangyi%20Liu%20and%20Tomy%20Tsai%20and%20Toby%20Shevlane%20and%20Christina%20Kouridi%20and%20Drew%20Garmon%20and%20Adrian%20Goedeckemeyer%20and%20Adam%20R.%20Brown%20and%20Anitha%20Vijayakumar%20and%20Ali%20Elqursh%20and%20Sadegh%20Jazayeri%20and%20Jin%20Huang%20and%20Sara%20Mc%20Carthy%20and%20Jay%20Hoover%20and%20Lucy%20Kim%20and%20Sandeep%20Kumar%20and%20Wei%20Chen%20and%20Courtney%20Biles%20and%20Garrett%20Bingham%20and%20Evan%20Rosen%20and%20Lisa%20Wang%20and%20Qijun%20Tan%20and%20David%20Engel%20and%20Francesco%20Pongetti%20and%20Dario%20de%20Cesare%20and%20Dongseong%20Hwang%20and%20Lily%20Yu%20and%20Jennifer%20Pullman%20and%20Srini%20Narayanan%20and%20Kyle%20Levin%20and%20Siddharth%20Gopal%20and%20Megan%20Li%20and%20Asaf%20Aharoni%20and%20Trieu%20Trinh%20and%20Jessica%20Lo%20and%20Norman%20Casagrande%20and%20Roopali%20Vij%20and%20Loic%20Matthey%20and%20Bramandia%20Ramadhana%20and%20Austin%20Matthews%20and%20CJ%20Carey%20and%20Matthew%20Johnson%20and%20Kremena%20Goranova%20and%20Rohin%20Shah%20and%20Shereen%20Ashraf%20and%20Kingshuk%20Dasgupta%20and%20Rasmus%20Larsen%20and%20Yicheng%20Wang%20and%20Manish%20Reddy%20Vuyyuru%20and%20Chong%20Jiang%20and%20Joana%20Ijazi%20and%20Kazuki%20Osawa%20and%20Celine%20Smith%20and%20Ramya%20Sree%20Boppana%20and%20Taylan%20Bilal%20and%20Yuma%20Koizumi%20and%20Ying%20Xu%20and%20Yasemin%20Altun%20and%20Nir%20Shabat%20and%20Ben%20Bariach%20and%20Alex%20Korchemniy%20and%20Kiam%20Choo%20and%20Olaf%20Ronneberger%20and%20Chimezie%20Iwuanyanwu%20and%20Shubin%20Zhao%20and%20David%20Soergel%20and%20Cho-Jui%20Hsieh%20and%20Irene%20Cai%20and%20Shariq%20Iqbal%20and%20Martin%20Sundermeyer%20and%20Zhe%20Chen%20and%20Elie%20Bursztein%20and%20Chaitanya%20Malaviya%20and%20Fadi%20Biadsy%20and%20Prakash%20Shroff%20and%20Inderjit%20Dhillon%20and%20Tejasi%20Latkar%20and%20Chris%20Dyer%20and%20Hannah%20Forbes%20and%20Massimo%20Nicosia%20and%20Vitaly%20Nikolaev%20and%20Somer%20Greene%20and%20Marin%20Georgiev%20and%20Pidong%20Wang%20and%20Nina%20Martin%20and%20Hanie%20Sedghi%20and%20John%20Zhang%20and%20Praseem%20Banzal%20and%20Doug%20Fritz%20and%20Vikram%20Rao%20and%20Xuezhi%20Wang%20and%20Jiageng%20Zhang%20and%20Viorica%20Patraucean%20and%20Dayou%20Du%20and%20Igor%20Mordatch%20and%20Ivan%20Jurin%20and%20Lewis%20Liu%20and%20Ayush%20Dubey%20and%20Abhi%20Mohan%20and%20Janek%20Nowakowski%20and%20Vlad-Doru%20Ion%20and%20Nan%20Wei%20and%20Reiko%20Tojo%20and%20Maria%20Abi%20Raad%20and%20Drew%20A.%20Hudson%20and%20Vaishakh%20Keshava%20and%20Shubham%20Agrawal%20and%20Kevin%20Ramirez%20and%20Zhichun%20Wu%20and%20Hoang%20Nguyen%20and%20Ji%20Liu%20and%20Madhavi%20Sewak%20and%20Bryce%20Petrini%20and%20DongHyun%20Choi%20and%20Ivan%20Philips%20and%20Ziyue%20Wang%20and%20Ioana%20Bica%20and%20Ankush%20Garg%20and%20Jarek%20Wilkiewicz%20and%20Priyanka%20Agrawal%20and%20Xiaowei%20Li%20and%20Danhao%20Guo%20and%20Emily%20Xue%20and%20Naseer%20Shaik%20and%20Andrew%20Leach%20and%20Sadh%20MNM%20Khan%20and%20Julia%20Wiesinger%20and%20Sammy%20Jerome%20and%20Abhishek%20Chakladar%20and%20Alek%20Wenjiao%20Wang%20and%20Tina%20Ornduff%20and%20Folake%20Abu%20and%20Alireza%20Ghaffarkhah%20and%20Marcus%20Wainwright%20and%20Mario%20Cortes%20and%20Frederick%20Liu%20and%20Joshua%20Maynez%20and%20Andreas%20Terzis%20and%20Pouya%20Samangouei%20and%20Riham%20Mansour%20and%20Tomasz%20K%C4%99pa%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Anton%20Algymr%20and%20Dan%20Banica%20and%20Agoston%20Weisz%20and%20Andras%20Orban%20and%20Alexandre%20Senges%20and%20Ewa%20Andrejczuk%20and%20Mark%20Geller%20and%20Niccolo%20Dal%20Santo%20and%20Valentin%20Anklin%20and%20Majd%20Al%20Merey%20and%20Martin%20Baeuml%20and%20Trevor%20Strohman%20and%20Junwen%20Bai%20and%20Slav%20Petrov%20and%20Yonghui%20Wu%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Jeffrey%20Dean%20and%20Oriol%20Vinyals%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%201.5%20family%20of%20models%2C%20representing%0Athe%20next%20generation%20of%20highly%20compute-efficient%20multimodal%20models%20capable%20of%0Arecalling%20and%20reasoning%20over%20fine-grained%20information%20from%20millions%20of%20tokens%0Aof%20context%2C%20including%20multiple%20long%20documents%20and%20hours%20of%20video%20and%20audio.%20The%0Afamily%20includes%20two%20new%20models%3A%20%281%29%20an%20updated%20Gemini%201.5%20Pro%2C%20which%20exceeds%0Athe%20February%20version%20on%20the%20great%20majority%20of%20capabilities%20and%20benchmarks%3B%20%282%29%0AGemini%201.5%20Flash%2C%20a%20more%20lightweight%20variant%20designed%20for%20efficiency%20with%0Aminimal%20regression%20in%20quality.%20Gemini%201.5%20models%20achieve%20near-perfect%20recall%20on%0Along-context%20retrieval%20tasks%20across%20modalities%2C%20improve%20the%20state-of-the-art%20in%0Along-document%20QA%2C%20long-video%20QA%20and%20long-context%20ASR%2C%20and%20match%20or%20surpass%0AGemini%201.0%20Ultra%27s%20state-of-the-art%20performance%20across%20a%20broad%20set%20of%0Abenchmarks.%20Studying%20the%20limits%20of%20Gemini%201.5%27s%20long-context%20ability%2C%20we%20find%0Acontinued%20improvement%20in%20next-token%20prediction%20and%20near-perfect%20retrieval%0A%28%3E99%25%29%20up%20to%20at%20least%2010M%20tokens%2C%20a%20generational%20leap%20over%20existing%20models%20such%0Aas%20Claude%203.0%20%28200k%29%20and%20GPT-4%20Turbo%20%28128k%29.%20Finally%2C%20we%20highlight%20real-world%0Ause%20cases%2C%20such%20as%20Gemini%201.5%20collaborating%20with%20professionals%20on%20completing%0Atheir%20tasks%20achieving%2026%20to%2075%25%20time%20savings%20across%2010%20different%20job%0Acategories%2C%20as%20well%20as%20surprising%20new%20capabilities%20of%20large%20language%20models%20at%0Athe%20frontier%3B%20when%20given%20a%20grammar%20manual%20for%20Kalamang%2C%20a%20language%20with%20fewer%0Athan%20200%20speakers%20worldwide%2C%20the%20model%20learns%20to%20translate%20English%20to%20Kalamang%0Aat%20a%20similar%20level%20to%20a%20person%20who%20learned%20from%20the%20same%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05530v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGemini%25201.5%253A%2520Unlocking%2520multimodal%2520understanding%2520across%2520millions%2520of%2520tokens%250A%2520%2520of%2520context%26entry.906535625%3D%2520Gemini%2520Team%2520and%2520Petko%2520Georgiev%2520and%2520Ving%2520Ian%2520Lei%2520and%2520Ryan%2520Burnell%2520and%2520Libin%2520Bai%2520and%2520Anmol%2520Gulati%2520and%2520Garrett%2520Tanzer%2520and%2520Damien%2520Vincent%2520and%2520Zhufeng%2520Pan%2520and%2520Shibo%2520Wang%2520and%2520Soroosh%2520Mariooryad%2520and%2520Yifan%2520Ding%2520and%2520Xinyang%2520Geng%2520and%2520Fred%2520Alcober%2520and%2520Roy%2520Frostig%2520and%2520Mark%2520Omernick%2520and%2520Lexi%2520Walker%2520and%2520Cosmin%2520Paduraru%2520and%2520Christina%2520Sorokin%2520and%2520Andrea%2520Tacchetti%2520and%2520Colin%2520Gaffney%2520and%2520Samira%2520Daruki%2520and%2520Olcan%2520Sercinoglu%2520and%2520Zach%2520Gleicher%2520and%2520Juliette%2520Love%2520and%2520Paul%2520Voigtlaender%2520and%2520Rohan%2520Jain%2520and%2520Gabriela%2520Surita%2520and%2520Kareem%2520Mohamed%2520and%2520Rory%2520Blevins%2520and%2520Junwhan%2520Ahn%2520and%2520Tao%2520Zhu%2520and%2520Kornraphop%2520Kawintiranon%2520and%2520Orhan%2520Firat%2520and%2520Yiming%2520Gu%2520and%2520Yujing%2520Zhang%2520and%2520Matthew%2520Rahtz%2520and%2520Manaal%2520Faruqui%2520and%2520Natalie%2520Clay%2520and%2520Justin%2520Gilmer%2520and%2520JD%2520Co-Reyes%2520and%2520Ivo%2520Penchev%2520and%2520Rui%2520Zhu%2520and%2520Nobuyuki%2520Morioka%2520and%2520Kevin%2520Hui%2520and%2520Krishna%2520Haridasan%2520and%2520Victor%2520Campos%2520and%2520Mahdis%2520Mahdieh%2520and%2520Mandy%2520Guo%2520and%2520Samer%2520Hassan%2520and%2520Kevin%2520Kilgour%2520and%2520Arpi%2520Vezer%2520and%2520Heng-Tze%2520Cheng%2520and%2520Raoul%2520de%2520Liedekerke%2520and%2520Siddharth%2520Goyal%2520and%2520Paul%2520Barham%2520and%2520DJ%2520Strouse%2520and%2520Seb%2520Noury%2520and%2520Jonas%2520Adler%2520and%2520Mukund%2520Sundararajan%2520and%2520Sharad%2520Vikram%2520and%2520Dmitry%2520Lepikhin%2520and%2520Michela%2520Paganini%2520and%2520Xavier%2520Garcia%2520and%2520Fan%2520Yang%2520and%2520Dasha%2520Valter%2520and%2520Maja%2520Trebacz%2520and%2520Kiran%2520Vodrahalli%2520and%2520Chulayuth%2520Asawaroengchai%2520and%2520Roman%2520Ring%2520and%2520Norbert%2520Kalb%2520and%2520Livio%2520Baldini%2520Soares%2520and%2520Siddhartha%2520Brahma%2520and%2520David%2520Steiner%2520and%2520Tianhe%2520Yu%2520and%2520Fabian%2520Mentzer%2520and%2520Antoine%2520He%2520and%2520Lucas%2520Gonzalez%2520and%2520Bibo%2520Xu%2520and%2520Raphael%2520Lopez%2520Kaufman%2520and%2520Laurent%2520El%2520Shafey%2520and%2520Junhyuk%2520Oh%2520and%2520Tom%2520Hennigan%2520and%2520George%2520van%2520den%2520Driessche%2520and%2520Seth%2520Odoom%2520and%2520Mario%2520Lucic%2520and%2520Becca%2520Roelofs%2520and%2520Sid%2520Lall%2520and%2520Amit%2520Marathe%2520and%2520Betty%2520Chan%2520and%2520Santiago%2520Ontanon%2520and%2520Luheng%2520He%2520and%2520Denis%2520Teplyashin%2520and%2520Jonathan%2520Lai%2520and%2520Phil%2520Crone%2520and%2520Bogdan%2520Damoc%2520and%2520Lewis%2520Ho%2520and%2520Sebastian%2520Riedel%2520and%2520Karel%2520Lenc%2520and%2520Chih-Kuan%2520Yeh%2520and%2520Aakanksha%2520Chowdhery%2520and%2520Yang%2520Xu%2520and%2520Mehran%2520Kazemi%2520and%2520Ehsan%2520Amid%2520and%2520Anastasia%2520Petrushkina%2520and%2520Kevin%2520Swersky%2520and%2520Ali%2520Khodaei%2520and%2520Gowoon%2520Chen%2520and%2520Chris%2520Larkin%2520and%2520Mario%2520Pinto%2520and%2520Geng%2520Yan%2520and%2520Adria%2520Puigdomenech%2520Badia%2520and%2520Piyush%2520Patil%2520and%2520Steven%2520Hansen%2520and%2520Dave%2520Orr%2520and%2520Sebastien%2520M.%2520R.%2520Arnold%2520and%2520Jordan%2520Grimstad%2520and%2520Andrew%2520Dai%2520and%2520Sholto%2520Douglas%2520and%2520Rishika%2520Sinha%2520and%2520Vikas%2520Yadav%2520and%2520Xi%2520Chen%2520and%2520Elena%2520Gribovskaya%2520and%2520Jacob%2520Austin%2520and%2520Jeffrey%2520Zhao%2520and%2520Kaushal%2520Patel%2520and%2520Paul%2520Komarek%2520and%2520Sophia%2520Austin%2520and%2520Sebastian%2520Borgeaud%2520and%2520Linda%2520Friso%2520and%2520Abhimanyu%2520Goyal%2520and%2520Ben%2520Caine%2520and%2520Kris%2520Cao%2520and%2520Da-Woon%2520Chung%2520and%2520Matthew%2520Lamm%2520and%2520Gabe%2520Barth-Maron%2520and%2520Thais%2520Kagohara%2520and%2520Kate%2520Olszewska%2520and%2520Mia%2520Chen%2520and%2520Kaushik%2520Shivakumar%2520and%2520Rishabh%2520Agarwal%2520and%2520Harshal%2520Godhia%2520and%2520Ravi%2520Rajwar%2520and%2520Javier%2520Snaider%2520and%2520Xerxes%2520Dotiwalla%2520and%2520Yuan%2520Liu%2520and%2520Aditya%2520Barua%2520and%2520Victor%2520Ungureanu%2520and%2520Yuan%2520Zhang%2520and%2520Bat-Orgil%2520Batsaikhan%2520and%2520Mateo%2520Wirth%2520and%2520James%2520Qin%2520and%2520Ivo%2520Danihelka%2520and%2520Tulsee%2520Doshi%2520and%2520Martin%2520Chadwick%2520and%2520Jilin%2520Chen%2520and%2520Sanil%2520Jain%2520and%2520Quoc%2520Le%2520and%2520Arjun%2520Kar%2520and%2520Madhu%2520Gurumurthy%2520and%2520Cheng%2520Li%2520and%2520Ruoxin%2520Sang%2520and%2520Fangyu%2520Liu%2520and%2520Lampros%2520Lamprou%2520and%2520Rich%2520Munoz%2520and%2520Nathan%2520Lintz%2520and%2520Harsh%2520Mehta%2520and%2520Heidi%2520Howard%2520and%2520Malcolm%2520Reynolds%2520and%2520Lora%2520Aroyo%2520and%2520Quan%2520Wang%2520and%2520Lorenzo%2520Blanco%2520and%2520Albin%2520Cassirer%2520and%2520Jordan%2520Griffith%2520and%2520Dipanjan%2520Das%2520and%2520Stephan%2520Lee%2520and%2520Jakub%2520Sygnowski%2520and%2520Zach%2520Fisher%2520and%2520James%2520Besley%2520and%2520Richard%2520Powell%2520and%2520Zafarali%2520Ahmed%2520and%2520Dominik%2520Paulus%2520and%2520David%2520Reitter%2520and%2520Zalan%2520Borsos%2520and%2520Rishabh%2520Joshi%2520and%2520Aedan%2520Pope%2520and%2520Steven%2520Hand%2520and%2520Vittorio%2520Selo%2520and%2520Vihan%2520Jain%2520and%2520Nikhil%2520Sethi%2520and%2520Megha%2520Goel%2520and%2520Takaki%2520Makino%2520and%2520Rhys%2520May%2520and%2520Zhen%2520Yang%2520and%2520Johan%2520Schalkwyk%2520and%2520Christina%2520Butterfield%2520and%2520Anja%2520Hauth%2520and%2520Alex%2520Goldin%2520and%2520Will%2520Hawkins%2520and%2520Evan%2520Senter%2520and%2520Sergey%2520Brin%2520and%2520Oliver%2520Woodman%2520and%2520Marvin%2520Ritter%2520and%2520Eric%2520Noland%2520and%2520Minh%2520Giang%2520and%2520Vijay%2520Bolina%2520and%2520Lisa%2520Lee%2520and%2520Tim%2520Blyth%2520and%2520Ian%2520Mackinnon%2520and%2520Machel%2520Reid%2520and%2520Obaid%2520Sarvana%2520and%2520David%2520Silver%2520and%2520Alexander%2520Chen%2520and%2520Lily%2520Wang%2520and%2520Loren%2520Maggiore%2520and%2520Oscar%2520Chang%2520and%2520Nithya%2520Attaluri%2520and%2520Gregory%2520Thornton%2520and%2520Chung-Cheng%2520Chiu%2520and%2520Oskar%2520Bunyan%2520and%2520Nir%2520Levine%2520and%2520Timothy%2520Chung%2520and%2520Evgenii%2520Eltyshev%2520and%2520Xiance%2520Si%2520and%2520Timothy%2520Lillicrap%2520and%2520Demetra%2520Brady%2520and%2520Vaibhav%2520Aggarwal%2520and%2520Boxi%2520Wu%2520and%2520Yuanzhong%2520Xu%2520and%2520Ross%2520McIlroy%2520and%2520Kartikeya%2520Badola%2520and%2520Paramjit%2520Sandhu%2520and%2520Erica%2520Moreira%2520and%2520Wojciech%2520Stokowiec%2520and%2520Ross%2520Hemsley%2520and%2520Dong%2520Li%2520and%2520Alex%2520Tudor%2520and%2520Pranav%2520Shyam%2520and%2520Elahe%2520Rahimtoroghi%2520and%2520Salem%2520Haykal%2520and%2520Pablo%2520Sprechmann%2520and%2520Xiang%2520Zhou%2520and%2520Diana%2520Mincu%2520and%2520Yujia%2520Li%2520and%2520Ravi%2520Addanki%2520and%2520Kalpesh%2520Krishna%2520and%2520Xiao%2520Wu%2520and%2520Alexandre%2520Frechette%2520and%2520Matan%2520Eyal%2520and%2520Allan%2520Dafoe%2520and%2520Dave%2520Lacey%2520and%2520Jay%2520Whang%2520and%2520Thi%2520Avrahami%2520and%2520Ye%2520Zhang%2520and%2520Emanuel%2520Taropa%2520and%2520Hanzhao%2520Lin%2520and%2520Daniel%2520Toyama%2520and%2520Eliza%2520Rutherford%2520and%2520Motoki%2520Sano%2520and%2520HyunJeong%2520Choe%2520and%2520Alex%2520Tomala%2520and%2520Chalence%2520Safranek-Shrader%2520and%2520Nora%2520Kassner%2520and%2520Mantas%2520Pajarskas%2520and%2520Matt%2520Harvey%2520and%2520Sean%2520Sechrist%2520and%2520Meire%2520Fortunato%2520and%2520Christina%2520Lyu%2520and%2520Gamaleldin%2520Elsayed%2520and%2520Chenkai%2520Kuang%2520and%2520James%2520Lottes%2520and%2520Eric%2520Chu%2520and%2520Chao%2520Jia%2520and%2520Chih-Wei%2520Chen%2520and%2520Peter%2520Humphreys%2520and%2520Kate%2520Baumli%2520and%2520Connie%2520Tao%2520and%2520Rajkumar%2520Samuel%2520and%2520Cicero%2520Nogueira%2520dos%2520Santos%2520and%2520Anders%2520Andreassen%2520and%2520Nemanja%2520Raki%25C4%2587evi%25C4%2587%2520and%2520Dominik%2520Grewe%2520and%2520Aviral%2520Kumar%2520and%2520Stephanie%2520Winkler%2520and%2520Jonathan%2520Caton%2520and%2520Andrew%2520Brock%2520and%2520Sid%2520Dalmia%2520and%2520Hannah%2520Sheahan%2520and%2520Iain%2520Barr%2520and%2520Yingjie%2520Miao%2520and%2520Paul%2520Natsev%2520and%2520Jacob%2520Devlin%2520and%2520Feryal%2520Behbahani%2520and%2520Flavien%2520Prost%2520and%2520Yanhua%2520Sun%2520and%2520Artiom%2520Myaskovsky%2520and%2520Thanumalayan%2520Sankaranarayana%2520Pillai%2520and%2520Dan%2520Hurt%2520and%2520Angeliki%2520Lazaridou%2520and%2520Xi%2520Xiong%2520and%2520Ce%2520Zheng%2520and%2520Fabio%2520Pardo%2520and%2520Xiaowei%2520Li%2520and%2520Dan%2520Horgan%2520and%2520Joe%2520Stanton%2520and%2520Moran%2520Ambar%2520and%2520Fei%2520Xia%2520and%2520Alejandro%2520Lince%2520and%2520Mingqiu%2520Wang%2520and%2520Basil%2520Mustafa%2520and%2520Albert%2520Webson%2520and%2520Hyo%2520Lee%2520and%2520Rohan%2520Anil%2520and%2520Martin%2520Wicke%2520and%2520Timothy%2520Dozat%2520and%2520Abhishek%2520Sinha%2520and%2520Enrique%2520Piqueras%2520and%2520Elahe%2520Dabir%2520and%2520Shyam%2520Upadhyay%2520and%2520Anudhyan%2520Boral%2520and%2520Lisa%2520Anne%2520Hendricks%2520and%2520Corey%2520Fry%2520and%2520Josip%2520Djolonga%2520and%2520Yi%2520Su%2520and%2520Jake%2520Walker%2520and%2520Jane%2520Labanowski%2520and%2520Ronny%2520Huang%2520and%2520Vedant%2520Misra%2520and%2520Jeremy%2520Chen%2520and%2520RJ%2520Skerry-Ryan%2520and%2520Avi%2520Singh%2520and%2520Shruti%2520Rijhwani%2520and%2520Dian%2520Yu%2520and%2520Alex%2520Castro-Ros%2520and%2520Beer%2520Changpinyo%2520and%2520Romina%2520Datta%2520and%2520Sumit%2520Bagri%2520and%2520Arnar%2520Mar%2520Hrafnkelsson%2520and%2520Marcello%2520Maggioni%2520and%2520Daniel%2520Zheng%2520and%2520Yury%2520Sulsky%2520and%2520Shaobo%2520Hou%2520and%2520Tom%2520Le%2520Paine%2520and%2520Antoine%2520Yang%2520and%2520Jason%2520Riesa%2520and%2520Dominika%2520Rogozinska%2520and%2520Dror%2520Marcus%2520and%2520Dalia%2520El%2520Badawy%2520and%2520Qiao%2520Zhang%2520and%2520Luyu%2520Wang%2520and%2520Helen%2520Miller%2520and%2520Jeremy%2520Greer%2520and%2520Lars%2520Lowe%2520Sjos%2520and%2520Azade%2520Nova%2520and%2520Heiga%2520Zen%2520and%2520Rahma%2520Chaabouni%2520and%2520Mihaela%2520Rosca%2520and%2520Jiepu%2520Jiang%2520and%2520Charlie%2520Chen%2520and%2520Ruibo%2520Liu%2520and%2520Tara%2520Sainath%2520and%2520Maxim%2520Krikun%2520and%2520Alex%2520Polozov%2520and%2520Jean-Baptiste%2520Lespiau%2520and%2520Josh%2520Newlan%2520and%2520Zeyncep%2520Cankara%2520and%2520Soo%2520Kwak%2520and%2520Yunhan%2520Xu%2520and%2520Phil%2520Chen%2520and%2520Andy%2520Coenen%2520and%2520Clemens%2520Meyer%2520and%2520Katerina%2520Tsihlas%2520and%2520Ada%2520Ma%2520and%2520Juraj%2520Gottweis%2520and%2520Jinwei%2520Xing%2520and%2520Chenjie%2520Gu%2520and%2520Jin%2520Miao%2520and%2520Christian%2520Frank%2520and%2520Zeynep%2520Cankara%2520and%2520Sanjay%2520Ganapathy%2520and%2520Ishita%2520Dasgupta%2520and%2520Steph%2520Hughes-Fitt%2520and%2520Heng%2520Chen%2520and%2520David%2520Reid%2520and%2520Keran%2520Rong%2520and%2520Hongmin%2520Fan%2520and%2520Joost%2520van%2520Amersfoort%2520and%2520Vincent%2520Zhuang%2520and%2520Aaron%2520Cohen%2520and%2520Shixiang%2520Shane%2520Gu%2520and%2520Anhad%2520Mohananey%2520and%2520Anastasija%2520Ilic%2520and%2520Taylor%2520Tobin%2520and%2520John%2520Wieting%2520and%2520Anna%2520Bortsova%2520and%2520Phoebe%2520Thacker%2520and%2520Emma%2520Wang%2520and%2520Emily%2520Caveness%2520and%2520Justin%2520Chiu%2520and%2520Eren%2520Sezener%2520and%2520Alex%2520Kaskasoli%2520and%2520Steven%2520Baker%2520and%2520Katie%2520Millican%2520and%2520Mohamed%2520Elhawaty%2520and%2520Kostas%2520Aisopos%2520and%2520Carl%2520Lebsack%2520and%2520Nathan%2520Byrd%2520and%2520Hanjun%2520Dai%2520and%2520Wenhao%2520Jia%2520and%2520Matthew%2520Wiethoff%2520and%2520Elnaz%2520Davoodi%2520and%2520Albert%2520Weston%2520and%2520Lakshman%2520Yagati%2520and%2520Arun%2520Ahuja%2520and%2520Isabel%2520Gao%2520and%2520Golan%2520Pundak%2520and%2520Susan%2520Zhang%2520and%2520Michael%2520Azzam%2520and%2520Khe%2520Chai%2520Sim%2520and%2520Sergi%2520Caelles%2520and%2520James%2520Keeling%2520and%2520Abhanshu%2520Sharma%2520and%2520Andy%2520Swing%2520and%2520YaGuang%2520Li%2520and%2520Chenxi%2520Liu%2520and%2520Carrie%2520Grimes%2520Bostock%2520and%2520Yamini%2520Bansal%2520and%2520Zachary%2520Nado%2520and%2520Ankesh%2520Anand%2520and%2520Josh%2520Lipschultz%2520and%2520Abhijit%2520Karmarkar%2520and%2520Lev%2520Proleev%2520and%2520Abe%2520Ittycheriah%2520and%2520Soheil%2520Hassas%2520Yeganeh%2520and%2520George%2520Polovets%2520and%2520Aleksandra%2520Faust%2520and%2520Jiao%2520Sun%2520and%2520Alban%2520Rrustemi%2520and%2520Pen%2520Li%2520and%2520Rakesh%2520Shivanna%2520and%2520Jeremiah%2520Liu%2520and%2520Chris%2520Welty%2520and%2520Federico%2520Lebron%2520and%2520Anirudh%2520Baddepudi%2520and%2520Sebastian%2520Krause%2520and%2520Emilio%2520Parisotto%2520and%2520Radu%2520Soricut%2520and%2520Zheng%2520Xu%2520and%2520Dawn%2520Bloxwich%2520and%2520Melvin%2520Johnson%2520and%2520Behnam%2520Neyshabur%2520and%2520Justin%2520Mao-Jones%2520and%2520Renshen%2520Wang%2520and%2520Vinay%2520Ramasesh%2520and%2520Zaheer%2520Abbas%2520and%2520Arthur%2520Guez%2520and%2520Constant%2520Segal%2520and%2520Duc%2520Dung%2520Nguyen%2520and%2520James%2520Svensson%2520and%2520Le%2520Hou%2520and%2520Sarah%2520York%2520and%2520Kieran%2520Milan%2520and%2520Sophie%2520Bridgers%2520and%2520Wiktor%2520Gworek%2520and%2520Marco%2520Tagliasacchi%2520and%2520James%2520Lee-Thorp%2520and%2520Michael%2520Chang%2520and%2520Alexey%2520Guseynov%2520and%2520Ale%2520Jakse%2520Hartman%2520and%2520Michael%2520Kwong%2520and%2520Ruizhe%2520Zhao%2520and%2520Sheleem%2520Kashem%2520and%2520Elizabeth%2520Cole%2520and%2520Antoine%2520Miech%2520and%2520Richard%2520Tanburn%2520and%2520Mary%2520Phuong%2520and%2520Filip%2520Pavetic%2520and%2520Sebastien%2520Cevey%2520and%2520Ramona%2520Comanescu%2520and%2520Richard%2520Ives%2520and%2520Sherry%2520Yang%2520and%2520Cosmo%2520Du%2520and%2520Bo%2520Li%2520and%2520Zizhao%2520Zhang%2520and%2520Mariko%2520Iinuma%2520and%2520Clara%2520Huiyi%2520Hu%2520and%2520Aurko%2520Roy%2520and%2520Shaan%2520Bijwadia%2520and%2520Zhenkai%2520Zhu%2520and%2520Danilo%2520Martins%2520and%2520Rachel%2520Saputro%2520and%2520Anita%2520Gergely%2520and%2520Steven%2520Zheng%2520and%2520Dawei%2520Jia%2520and%2520Ioannis%2520Antonoglou%2520and%2520Adam%2520Sadovsky%2520and%2520Shane%2520Gu%2520and%2520Yingying%2520Bi%2520and%2520Alek%2520Andreev%2520and%2520Sina%2520Samangooei%2520and%2520Mina%2520Khan%2520and%2520Tomas%2520Kocisky%2520and%2520Angelos%2520Filos%2520and%2520Chintu%2520Kumar%2520and%2520Colton%2520Bishop%2520and%2520Adams%2520Yu%2520and%2520Sarah%2520Hodkinson%2520and%2520Sid%2520Mittal%2520and%2520Premal%2520Shah%2520and%2520Alexandre%2520Moufarek%2520and%2520Yong%2520Cheng%2520and%2520Adam%2520Bloniarz%2520and%2520Jaehoon%2520Lee%2520and%2520Pedram%2520Pejman%2520and%2520Paul%2520Michel%2520and%2520Stephen%2520Spencer%2520and%2520Vladimir%2520Feinberg%2520and%2520Xuehan%2520Xiong%2520and%2520Nikolay%2520Savinov%2520and%2520Charlotte%2520Smith%2520and%2520Siamak%2520Shakeri%2520and%2520Dustin%2520Tran%2520and%2520Mary%2520Chesus%2520and%2520Bernd%2520Bohnet%2520and%2520George%2520Tucker%2520and%2520Tamara%2520von%2520Glehn%2520and%2520Carrie%2520Muir%2520and%2520Yiran%2520Mao%2520and%2520Hideto%2520Kazawa%2520and%2520Ambrose%2520Slone%2520and%2520Kedar%2520Soparkar%2520and%2520Disha%2520Shrivastava%2520and%2520James%2520Cobon-Kerr%2520and%2520Michael%2520Sharman%2520and%2520Jay%2520Pavagadhi%2520and%2520Carlos%2520Araya%2520and%2520Karolis%2520Misiunas%2520and%2520Nimesh%2520Ghelani%2520and%2520Michael%2520Laskin%2520and%2520David%2520Barker%2520and%2520Qiujia%2520Li%2520and%2520Anton%2520Briukhov%2520and%2520Neil%2520Houlsby%2520and%2520Mia%2520Glaese%2520and%2520Balaji%2520Lakshminarayanan%2520and%2520Nathan%2520Schucher%2520and%2520Yunhao%2520Tang%2520and%2520Eli%2520Collins%2520and%2520Hyeontaek%2520Lim%2520and%2520Fangxiaoyu%2520Feng%2520and%2520Adria%2520Recasens%2520and%2520Guangda%2520Lai%2520and%2520Alberto%2520Magni%2520and%2520Nicola%2520De%2520Cao%2520and%2520Aditya%2520Siddhant%2520and%2520Zoe%2520Ashwood%2520and%2520Jordi%2520Orbay%2520and%2520Mostafa%2520Dehghani%2520and%2520Jenny%2520Brennan%2520and%2520Yifan%2520He%2520and%2520Kelvin%2520Xu%2520and%2520Yang%2520Gao%2520and%2520Carl%2520Saroufim%2520and%2520James%2520Molloy%2520and%2520Xinyi%2520Wu%2520and%2520Seb%2520Arnold%2520and%2520Solomon%2520Chang%2520and%2520Julian%2520Schrittwieser%2520and%2520Elena%2520Buchatskaya%2520and%2520Soroush%2520Radpour%2520and%2520Martin%2520Polacek%2520and%2520Skye%2520Giordano%2520and%2520Ankur%2520Bapna%2520and%2520Simon%2520Tokumine%2520and%2520Vincent%2520Hellendoorn%2520and%2520Thibault%2520Sottiaux%2520and%2520Sarah%2520Cogan%2520and%2520Aliaksei%2520Severyn%2520and%2520Mohammad%2520Saleh%2520and%2520Shantanu%2520Thakoor%2520and%2520Laurent%2520Shefey%2520and%2520Siyuan%2520Qiao%2520and%2520Meenu%2520Gaba%2520and%2520Shuo-yiin%2520Chang%2520and%2520Craig%2520Swanson%2520and%2520Biao%2520Zhang%2520and%2520Benjamin%2520Lee%2520and%2520Paul%2520Kishan%2520Rubenstein%2520and%2520Gan%2520Song%2520and%2520Tom%2520Kwiatkowski%2520and%2520Anna%2520Koop%2520and%2520Ajay%2520Kannan%2520and%2520David%2520Kao%2520and%2520Parker%2520Schuh%2520and%2520Axel%2520Stjerngren%2520and%2520Golnaz%2520Ghiasi%2520and%2520Gena%2520Gibson%2520and%2520Luke%2520Vilnis%2520and%2520Ye%2520Yuan%2520and%2520Felipe%2520Tiengo%2520Ferreira%2520and%2520Aishwarya%2520Kamath%2520and%2520Ted%2520Klimenko%2520and%2520Ken%2520Franko%2520and%2520Kefan%2520Xiao%2520and%2520Indro%2520Bhattacharya%2520and%2520Miteyan%2520Patel%2520and%2520Rui%2520Wang%2520and%2520Alex%2520Morris%2520and%2520Robin%2520Strudel%2520and%2520Vivek%2520Sharma%2520and%2520Peter%2520Choy%2520and%2520Sayed%2520Hadi%2520Hashemi%2520and%2520Jessica%2520Landon%2520and%2520Mara%2520Finkelstein%2520and%2520Priya%2520Jhakra%2520and%2520Justin%2520Frye%2520and%2520Megan%2520Barnes%2520and%2520Matthew%2520Mauger%2520and%2520Dennis%2520Daun%2520and%2520Khuslen%2520Baatarsukh%2520and%2520Matthew%2520Tung%2520and%2520Wael%2520Farhan%2520and%2520Henryk%2520Michalewski%2520and%2520Fabio%2520Viola%2520and%2520Felix%2520de%2520Chaumont%2520Quitry%2520and%2520Charline%2520Le%2520Lan%2520and%2520Tom%2520Hudson%2520and%2520Qingze%2520Wang%2520and%2520Felix%2520Fischer%2520and%2520Ivy%2520Zheng%2520and%2520Elspeth%2520White%2520and%2520Anca%2520Dragan%2520and%2520Jean-baptiste%2520Alayrac%2520and%2520Eric%2520Ni%2520and%2520Alexander%2520Pritzel%2520and%2520Adam%2520Iwanicki%2520and%2520Michael%2520Isard%2520and%2520Anna%2520Bulanova%2520and%2520Lukas%2520Zilka%2520and%2520Ethan%2520Dyer%2520and%2520Devendra%2520Sachan%2520and%2520Srivatsan%2520Srinivasan%2520and%2520Hannah%2520Muckenhirn%2520and%2520Honglong%2520Cai%2520and%2520Amol%2520Mandhane%2520and%2520Mukarram%2520Tariq%2520and%2520Jack%2520W.%2520Rae%2520and%2520Gary%2520Wang%2520and%2520Kareem%2520Ayoub%2520and%2520Nicholas%2520FitzGerald%2520and%2520Yao%2520Zhao%2520and%2520Woohyun%2520Han%2520and%2520Chris%2520Alberti%2520and%2520Dan%2520Garrette%2520and%2520Kashyap%2520Krishnakumar%2520and%2520Mai%2520Gimenez%2520and%2520Anselm%2520Levskaya%2520and%2520Daniel%2520Sohn%2520and%2520Josip%2520Matak%2520and%2520Inaki%2520Iturrate%2520and%2520Michael%2520B.%2520Chang%2520and%2520Jackie%2520Xiang%2520and%2520Yuan%2520Cao%2520and%2520Nishant%2520Ranka%2520and%2520Geoff%2520Brown%2520and%2520Adrian%2520Hutter%2520and%2520Vahab%2520Mirrokni%2520and%2520Nanxin%2520Chen%2520and%2520Kaisheng%2520Yao%2520and%2520Zoltan%2520Egyed%2520and%2520Francois%2520Galilee%2520and%2520Tyler%2520Liechty%2520and%2520Praveen%2520Kallakuri%2520and%2520Evan%2520Palmer%2520and%2520Sanjay%2520Ghemawat%2520and%2520Jasmine%2520Liu%2520and%2520David%2520Tao%2520and%2520Chloe%2520Thornton%2520and%2520Tim%2520Green%2520and%2520Mimi%2520Jasarevic%2520and%2520Sharon%2520Lin%2520and%2520Victor%2520Cotruta%2520and%2520Yi-Xuan%2520Tan%2520and%2520Noah%2520Fiedel%2520and%2520Hongkun%2520Yu%2520and%2520Ed%2520Chi%2520and%2520Alexander%2520Neitz%2520and%2520Jens%2520Heitkaemper%2520and%2520Anu%2520Sinha%2520and%2520Denny%2520Zhou%2520and%2520Yi%2520Sun%2520and%2520Charbel%2520Kaed%2520and%2520Brice%2520Hulse%2520and%2520Swaroop%2520Mishra%2520and%2520Maria%2520Georgaki%2520and%2520Sneha%2520Kudugunta%2520and%2520Clement%2520Farabet%2520and%2520Izhak%2520Shafran%2520and%2520Daniel%2520Vlasic%2520and%2520Anton%2520Tsitsulin%2520and%2520Rajagopal%2520Ananthanarayanan%2520and%2520Alen%2520Carin%2520and%2520Guolong%2520Su%2520and%2520Pei%2520Sun%2520and%2520Shashank%2520V%2520and%2520Gabriel%2520Carvajal%2520and%2520Josef%2520Broder%2520and%2520Iulia%2520Comsa%2520and%2520Alena%2520Repina%2520and%2520William%2520Wong%2520and%2520Warren%2520Weilun%2520Chen%2520and%2520Peter%2520Hawkins%2520and%2520Egor%2520Filonov%2520and%2520Lucia%2520Loher%2520and%2520Christoph%2520Hirnschall%2520and%2520Weiyi%2520Wang%2520and%2520Jingchen%2520Ye%2520and%2520Andrea%2520Burns%2520and%2520Hardie%2520Cate%2520and%2520Diana%2520Gage%2520Wright%2520and%2520Federico%2520Piccinini%2520and%2520Lei%2520Zhang%2520and%2520Chu-Cheng%2520Lin%2520and%2520Ionel%2520Gog%2520and%2520Yana%2520Kulizhskaya%2520and%2520Ashwin%2520Sreevatsa%2520and%2520Shuang%2520Song%2520and%2520Luis%2520C.%2520Cobo%2520and%2520Anand%2520Iyer%2520and%2520Chetan%2520Tekur%2520and%2520Guillermo%2520Garrido%2520and%2520Zhuyun%2520Xiao%2520and%2520Rupert%2520Kemp%2520and%2520Huaixiu%2520Steven%2520Zheng%2520and%2520Hui%2520Li%2520and%2520Ananth%2520Agarwal%2520and%2520Christel%2520Ngani%2520and%2520Kati%2520Goshvadi%2520and%2520Rebeca%2520Santamaria-Fernandez%2520and%2520Wojciech%2520Fica%2520and%2520Xinyun%2520Chen%2520and%2520Chris%2520Gorgolewski%2520and%2520Sean%2520Sun%2520and%2520Roopal%2520Garg%2520and%2520Xinyu%2520Ye%2520and%2520S.%2520M.%2520Ali%2520Eslami%2520and%2520Nan%2520Hua%2520and%2520Jon%2520Simon%2520and%2520Pratik%2520Joshi%2520and%2520Yelin%2520Kim%2520and%2520Ian%2520Tenney%2520and%2520Sahitya%2520Potluri%2520and%2520Lam%2520Nguyen%2520Thiet%2520and%2520Quan%2520Yuan%2520and%2520Florian%2520Luisier%2520and%2520Alexandra%2520Chronopoulou%2520and%2520Salvatore%2520Scellato%2520and%2520Praveen%2520Srinivasan%2520and%2520Minmin%2520Chen%2520and%2520Vinod%2520Koverkathu%2520and%2520Valentin%2520Dalibard%2520and%2520Yaming%2520Xu%2520and%2520Brennan%2520Saeta%2520and%2520Keith%2520Anderson%2520and%2520Thibault%2520Sellam%2520and%2520Nick%2520Fernando%2520and%2520Fantine%2520Huot%2520and%2520Junehyuk%2520Jung%2520and%2520Mani%2520Varadarajan%2520and%2520Michael%2520Quinn%2520and%2520Amit%2520Raul%2520and%2520Maigo%2520Le%2520and%2520Ruslan%2520Habalov%2520and%2520Jon%2520Clark%2520and%2520Komal%2520Jalan%2520and%2520Kalesha%2520Bullard%2520and%2520Achintya%2520Singhal%2520and%2520Thang%2520Luong%2520and%2520Boyu%2520Wang%2520and%2520Sujeevan%2520Rajayogam%2520and%2520Julian%2520Eisenschlos%2520and%2520Johnson%2520Jia%2520and%2520Daniel%2520Finchelstein%2520and%2520Alex%2520Yakubovich%2520and%2520Daniel%2520Balle%2520and%2520Michael%2520Fink%2520and%2520Sameer%2520Agarwal%2520and%2520Jing%2520Li%2520and%2520Dj%2520Dvijotham%2520and%2520Shalini%2520Pal%2520and%2520Kai%2520Kang%2520and%2520Jaclyn%2520Konzelmann%2520and%2520Jennifer%2520Beattie%2520and%2520Olivier%2520Dousse%2520and%2520Diane%2520Wu%2520and%2520Remi%2520Crocker%2520and%2520Chen%2520Elkind%2520and%2520Siddhartha%2520Reddy%2520Jonnalagadda%2520and%2520Jong%2520Lee%2520and%2520Dan%2520Holtmann-Rice%2520and%2520Krystal%2520Kallarackal%2520and%2520Rosanne%2520Liu%2520and%2520Denis%2520Vnukov%2520and%2520Neera%2520Vats%2520and%2520Luca%2520Invernizzi%2520and%2520Mohsen%2520Jafari%2520and%2520Huanjie%2520Zhou%2520and%2520Lilly%2520Taylor%2520and%2520Jennifer%2520Prendki%2520and%2520Marcus%2520Wu%2520and%2520Tom%2520Eccles%2520and%2520Tianqi%2520Liu%2520and%2520Kavya%2520Kopparapu%2520and%2520Francoise%2520Beaufays%2520and%2520Christof%2520Angermueller%2520and%2520Andreea%2520Marzoca%2520and%2520Shourya%2520Sarcar%2520and%2520Hilal%2520Dib%2520and%2520Jeff%2520Stanway%2520and%2520Frank%2520Perbet%2520and%2520Nejc%2520Trdin%2520and%2520Rachel%2520Sterneck%2520and%2520Andrey%2520Khorlin%2520and%2520Dinghua%2520Li%2520and%2520Xihui%2520Wu%2520and%2520Sonam%2520Goenka%2520and%2520David%2520Madras%2520and%2520Sasha%2520Goldshtein%2520and%2520Willi%2520Gierke%2520and%2520Tong%2520Zhou%2520and%2520Yaxin%2520Liu%2520and%2520Yannie%2520Liang%2520and%2520Anais%2520White%2520and%2520Yunjie%2520Li%2520and%2520Shreya%2520Singh%2520and%2520Sanaz%2520Bahargam%2520and%2520Mark%2520Epstein%2520and%2520Sujoy%2520Basu%2520and%2520Li%2520Lao%2520and%2520Adnan%2520Ozturel%2520and%2520Carl%2520Crous%2520and%2520Alex%2520Zhai%2520and%2520Han%2520Lu%2520and%2520Zora%2520Tung%2520and%2520Neeraj%2520Gaur%2520and%2520Alanna%2520Walton%2520and%2520Lucas%2520Dixon%2520and%2520Ming%2520Zhang%2520and%2520Amir%2520Globerson%2520and%2520Grant%2520Uy%2520and%2520Andrew%2520Bolt%2520and%2520Olivia%2520Wiles%2520and%2520Milad%2520Nasr%2520and%2520Ilia%2520Shumailov%2520and%2520Marco%2520Selvi%2520and%2520Francesco%2520Piccinno%2520and%2520Ricardo%2520Aguilar%2520and%2520Sara%2520McCarthy%2520and%2520Misha%2520Khalman%2520and%2520Mrinal%2520Shukla%2520and%2520Vlado%2520Galic%2520and%2520John%2520Carpenter%2520and%2520Kevin%2520Villela%2520and%2520Haibin%2520Zhang%2520and%2520Harry%2520Richardson%2520and%2520James%2520Martens%2520and%2520Matko%2520Bosnjak%2520and%2520Shreyas%2520Rammohan%2520Belle%2520and%2520Jeff%2520Seibert%2520and%2520Mahmoud%2520Alnahlawi%2520and%2520Brian%2520McWilliams%2520and%2520Sankalp%2520Singh%2520and%2520Annie%2520Louis%2520and%2520Wen%2520Ding%2520and%2520Dan%2520Popovici%2520and%2520Lenin%2520Simicich%2520and%2520Laura%2520Knight%2520and%2520Pulkit%2520Mehta%2520and%2520Nishesh%2520Gupta%2520and%2520Chongyang%2520Shi%2520and%2520Saaber%2520Fatehi%2520and%2520Jovana%2520Mitrovic%2520and%2520Alex%2520Grills%2520and%2520Joseph%2520Pagadora%2520and%2520Dessie%2520Petrova%2520and%2520Danielle%2520Eisenbud%2520and%2520Zhishuai%2520Zhang%2520and%2520Damion%2520Yates%2520and%2520Bhavishya%2520Mittal%2520and%2520Nilesh%2520Tripuraneni%2520and%2520Yannis%2520Assael%2520and%2520Thomas%2520Brovelli%2520and%2520Prateek%2520Jain%2520and%2520Mihajlo%2520Velimirovic%2520and%2520Canfer%2520Akbulut%2520and%2520Jiaqi%2520Mu%2520and%2520Wolfgang%2520Macherey%2520and%2520Ravin%2520Kumar%2520and%2520Jun%2520Xu%2520and%2520Haroon%2520Qureshi%2520and%2520Gheorghe%2520Comanici%2520and%2520Jeremy%2520Wiesner%2520and%2520Zhitao%2520Gong%2520and%2520Anton%2520Ruddock%2520and%2520Matthias%2520Bauer%2520and%2520Nick%2520Felt%2520and%2520Anirudh%2520GP%2520and%2520Anurag%2520Arnab%2520and%2520Dustin%2520Zelle%2520and%2520Jonas%2520Rothfuss%2520and%2520Bill%2520Rosgen%2520and%2520Ashish%2520Shenoy%2520and%2520Bryan%2520Seybold%2520and%2520Xinjian%2520Li%2520and%2520Jayaram%2520Mudigonda%2520and%2520Goker%2520Erdogan%2520and%2520Jiawei%2520Xia%2520and%2520Jiri%2520Simsa%2520and%2520Andrea%2520Michi%2520and%2520Yi%2520Yao%2520and%2520Christopher%2520Yew%2520and%2520Steven%2520Kan%2520and%2520Isaac%2520Caswell%2520and%2520Carey%2520Radebaugh%2520and%2520Andre%2520Elisseeff%2520and%2520Pedro%2520Valenzuela%2520and%2520Kay%2520McKinney%2520and%2520Kim%2520Paterson%2520and%2520Albert%2520Cui%2520and%2520Eri%2520Latorre-Chimoto%2520and%2520Solomon%2520Kim%2520and%2520William%2520Zeng%2520and%2520Ken%2520Durden%2520and%2520Priya%2520Ponnapalli%2520and%2520Tiberiu%2520Sosea%2520and%2520Christopher%2520A.%2520Choquette-Choo%2520and%2520James%2520Manyika%2520and%2520Brona%2520Robenek%2520and%2520Harsha%2520Vashisht%2520and%2520Sebastien%2520Pereira%2520and%2520Hoi%2520Lam%2520and%2520Marko%2520Velic%2520and%2520Denese%2520Owusu-Afriyie%2520and%2520Katherine%2520Lee%2520and%2520Tolga%2520Bolukbasi%2520and%2520Alicia%2520Parrish%2520and%2520Shawn%2520Lu%2520and%2520Jane%2520Park%2520and%2520Balaji%2520Venkatraman%2520and%2520Alice%2520Talbert%2520and%2520Lambert%2520Rosique%2520and%2520Yuchung%2520Cheng%2520and%2520Andrei%2520Sozanschi%2520and%2520Adam%2520Paszke%2520and%2520Praveen%2520Kumar%2520and%2520Jessica%2520Austin%2520and%2520Lu%2520Li%2520and%2520Khalid%2520Salama%2520and%2520Wooyeol%2520Kim%2520and%2520Nandita%2520Dukkipati%2520and%2520Anthony%2520Baryshnikov%2520and%2520Christos%2520Kaplanis%2520and%2520XiangHai%2520Sheng%2520and%2520Yuri%2520Chervonyi%2520and%2520Caglar%2520Unlu%2520and%2520Diego%2520de%2520Las%2520Casas%2520and%2520Harry%2520Askham%2520and%2520Kathryn%2520Tunyasuvunakool%2520and%2520Felix%2520Gimeno%2520and%2520Siim%2520Poder%2520and%2520Chester%2520Kwak%2520and%2520Matt%2520Miecnikowski%2520and%2520Vahab%2520Mirrokni%2520and%2520Alek%2520Dimitriev%2520and%2520Aaron%2520Parisi%2520and%2520Dangyi%2520Liu%2520and%2520Tomy%2520Tsai%2520and%2520Toby%2520Shevlane%2520and%2520Christina%2520Kouridi%2520and%2520Drew%2520Garmon%2520and%2520Adrian%2520Goedeckemeyer%2520and%2520Adam%2520R.%2520Brown%2520and%2520Anitha%2520Vijayakumar%2520and%2520Ali%2520Elqursh%2520and%2520Sadegh%2520Jazayeri%2520and%2520Jin%2520Huang%2520and%2520Sara%2520Mc%2520Carthy%2520and%2520Jay%2520Hoover%2520and%2520Lucy%2520Kim%2520and%2520Sandeep%2520Kumar%2520and%2520Wei%2520Chen%2520and%2520Courtney%2520Biles%2520and%2520Garrett%2520Bingham%2520and%2520Evan%2520Rosen%2520and%2520Lisa%2520Wang%2520and%2520Qijun%2520Tan%2520and%2520David%2520Engel%2520and%2520Francesco%2520Pongetti%2520and%2520Dario%2520de%2520Cesare%2520and%2520Dongseong%2520Hwang%2520and%2520Lily%2520Yu%2520and%2520Jennifer%2520Pullman%2520and%2520Srini%2520Narayanan%2520and%2520Kyle%2520Levin%2520and%2520Siddharth%2520Gopal%2520and%2520Megan%2520Li%2520and%2520Asaf%2520Aharoni%2520and%2520Trieu%2520Trinh%2520and%2520Jessica%2520Lo%2520and%2520Norman%2520Casagrande%2520and%2520Roopali%2520Vij%2520and%2520Loic%2520Matthey%2520and%2520Bramandia%2520Ramadhana%2520and%2520Austin%2520Matthews%2520and%2520CJ%2520Carey%2520and%2520Matthew%2520Johnson%2520and%2520Kremena%2520Goranova%2520and%2520Rohin%2520Shah%2520and%2520Shereen%2520Ashraf%2520and%2520Kingshuk%2520Dasgupta%2520and%2520Rasmus%2520Larsen%2520and%2520Yicheng%2520Wang%2520and%2520Manish%2520Reddy%2520Vuyyuru%2520and%2520Chong%2520Jiang%2520and%2520Joana%2520Ijazi%2520and%2520Kazuki%2520Osawa%2520and%2520Celine%2520Smith%2520and%2520Ramya%2520Sree%2520Boppana%2520and%2520Taylan%2520Bilal%2520and%2520Yuma%2520Koizumi%2520and%2520Ying%2520Xu%2520and%2520Yasemin%2520Altun%2520and%2520Nir%2520Shabat%2520and%2520Ben%2520Bariach%2520and%2520Alex%2520Korchemniy%2520and%2520Kiam%2520Choo%2520and%2520Olaf%2520Ronneberger%2520and%2520Chimezie%2520Iwuanyanwu%2520and%2520Shubin%2520Zhao%2520and%2520David%2520Soergel%2520and%2520Cho-Jui%2520Hsieh%2520and%2520Irene%2520Cai%2520and%2520Shariq%2520Iqbal%2520and%2520Martin%2520Sundermeyer%2520and%2520Zhe%2520Chen%2520and%2520Elie%2520Bursztein%2520and%2520Chaitanya%2520Malaviya%2520and%2520Fadi%2520Biadsy%2520and%2520Prakash%2520Shroff%2520and%2520Inderjit%2520Dhillon%2520and%2520Tejasi%2520Latkar%2520and%2520Chris%2520Dyer%2520and%2520Hannah%2520Forbes%2520and%2520Massimo%2520Nicosia%2520and%2520Vitaly%2520Nikolaev%2520and%2520Somer%2520Greene%2520and%2520Marin%2520Georgiev%2520and%2520Pidong%2520Wang%2520and%2520Nina%2520Martin%2520and%2520Hanie%2520Sedghi%2520and%2520John%2520Zhang%2520and%2520Praseem%2520Banzal%2520and%2520Doug%2520Fritz%2520and%2520Vikram%2520Rao%2520and%2520Xuezhi%2520Wang%2520and%2520Jiageng%2520Zhang%2520and%2520Viorica%2520Patraucean%2520and%2520Dayou%2520Du%2520and%2520Igor%2520Mordatch%2520and%2520Ivan%2520Jurin%2520and%2520Lewis%2520Liu%2520and%2520Ayush%2520Dubey%2520and%2520Abhi%2520Mohan%2520and%2520Janek%2520Nowakowski%2520and%2520Vlad-Doru%2520Ion%2520and%2520Nan%2520Wei%2520and%2520Reiko%2520Tojo%2520and%2520Maria%2520Abi%2520Raad%2520and%2520Drew%2520A.%2520Hudson%2520and%2520Vaishakh%2520Keshava%2520and%2520Shubham%2520Agrawal%2520and%2520Kevin%2520Ramirez%2520and%2520Zhichun%2520Wu%2520and%2520Hoang%2520Nguyen%2520and%2520Ji%2520Liu%2520and%2520Madhavi%2520Sewak%2520and%2520Bryce%2520Petrini%2520and%2520DongHyun%2520Choi%2520and%2520Ivan%2520Philips%2520and%2520Ziyue%2520Wang%2520and%2520Ioana%2520Bica%2520and%2520Ankush%2520Garg%2520and%2520Jarek%2520Wilkiewicz%2520and%2520Priyanka%2520Agrawal%2520and%2520Xiaowei%2520Li%2520and%2520Danhao%2520Guo%2520and%2520Emily%2520Xue%2520and%2520Naseer%2520Shaik%2520and%2520Andrew%2520Leach%2520and%2520Sadh%2520MNM%2520Khan%2520and%2520Julia%2520Wiesinger%2520and%2520Sammy%2520Jerome%2520and%2520Abhishek%2520Chakladar%2520and%2520Alek%2520Wenjiao%2520Wang%2520and%2520Tina%2520Ornduff%2520and%2520Folake%2520Abu%2520and%2520Alireza%2520Ghaffarkhah%2520and%2520Marcus%2520Wainwright%2520and%2520Mario%2520Cortes%2520and%2520Frederick%2520Liu%2520and%2520Joshua%2520Maynez%2520and%2520Andreas%2520Terzis%2520and%2520Pouya%2520Samangouei%2520and%2520Riham%2520Mansour%2520and%2520Tomasz%2520K%25C4%2599pa%2520and%2520Fran%25C3%25A7ois-Xavier%2520Aubet%2520and%2520Anton%2520Algymr%2520and%2520Dan%2520Banica%2520and%2520Agoston%2520Weisz%2520and%2520Andras%2520Orban%2520and%2520Alexandre%2520Senges%2520and%2520Ewa%2520Andrejczuk%2520and%2520Mark%2520Geller%2520and%2520Niccolo%2520Dal%2520Santo%2520and%2520Valentin%2520Anklin%2520and%2520Majd%2520Al%2520Merey%2520and%2520Martin%2520Baeuml%2520and%2520Trevor%2520Strohman%2520and%2520Junwen%2520Bai%2520and%2520Slav%2520Petrov%2520and%2520Yonghui%2520Wu%2520and%2520Demis%2520Hassabis%2520and%2520Koray%2520Kavukcuoglu%2520and%2520Jeffrey%2520Dean%2520and%2520Oriol%2520Vinyals%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520introduce%2520the%2520Gemini%25201.5%2520family%2520of%2520models%252C%2520representing%250Athe%2520next%2520generation%2520of%2520highly%2520compute-efficient%2520multimodal%2520models%2520capable%2520of%250Arecalling%2520and%2520reasoning%2520over%2520fine-grained%2520information%2520from%2520millions%2520of%2520tokens%250Aof%2520context%252C%2520including%2520multiple%2520long%2520documents%2520and%2520hours%2520of%2520video%2520and%2520audio.%2520The%250Afamily%2520includes%2520two%2520new%2520models%253A%2520%25281%2529%2520an%2520updated%2520Gemini%25201.5%2520Pro%252C%2520which%2520exceeds%250Athe%2520February%2520version%2520on%2520the%2520great%2520majority%2520of%2520capabilities%2520and%2520benchmarks%253B%2520%25282%2529%250AGemini%25201.5%2520Flash%252C%2520a%2520more%2520lightweight%2520variant%2520designed%2520for%2520efficiency%2520with%250Aminimal%2520regression%2520in%2520quality.%2520Gemini%25201.5%2520models%2520achieve%2520near-perfect%2520recall%2520on%250Along-context%2520retrieval%2520tasks%2520across%2520modalities%252C%2520improve%2520the%2520state-of-the-art%2520in%250Along-document%2520QA%252C%2520long-video%2520QA%2520and%2520long-context%2520ASR%252C%2520and%2520match%2520or%2520surpass%250AGemini%25201.0%2520Ultra%2527s%2520state-of-the-art%2520performance%2520across%2520a%2520broad%2520set%2520of%250Abenchmarks.%2520Studying%2520the%2520limits%2520of%2520Gemini%25201.5%2527s%2520long-context%2520ability%252C%2520we%2520find%250Acontinued%2520improvement%2520in%2520next-token%2520prediction%2520and%2520near-perfect%2520retrieval%250A%2528%253E99%2525%2529%2520up%2520to%2520at%2520least%252010M%2520tokens%252C%2520a%2520generational%2520leap%2520over%2520existing%2520models%2520such%250Aas%2520Claude%25203.0%2520%2528200k%2529%2520and%2520GPT-4%2520Turbo%2520%2528128k%2529.%2520Finally%252C%2520we%2520highlight%2520real-world%250Ause%2520cases%252C%2520such%2520as%2520Gemini%25201.5%2520collaborating%2520with%2520professionals%2520on%2520completing%250Atheir%2520tasks%2520achieving%252026%2520to%252075%2525%2520time%2520savings%2520across%252010%2520different%2520job%250Acategories%252C%2520as%2520well%2520as%2520surprising%2520new%2520capabilities%2520of%2520large%2520language%2520models%2520at%250Athe%2520frontier%253B%2520when%2520given%2520a%2520grammar%2520manual%2520for%2520Kalamang%252C%2520a%2520language%2520with%2520fewer%250Athan%2520200%2520speakers%2520worldwide%252C%2520the%2520model%2520learns%2520to%2520translate%2520English%2520to%2520Kalamang%250Aat%2520a%2520similar%2520level%2520to%2520a%2520person%2520who%2520learned%2520from%2520the%2520same%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05530v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gemini%201.5%3A%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%0A%20%20of%20context&entry.906535625=%20Gemini%20Team%20and%20Petko%20Georgiev%20and%20Ving%20Ian%20Lei%20and%20Ryan%20Burnell%20and%20Libin%20Bai%20and%20Anmol%20Gulati%20and%20Garrett%20Tanzer%20and%20Damien%20Vincent%20and%20Zhufeng%20Pan%20and%20Shibo%20Wang%20and%20Soroosh%20Mariooryad%20and%20Yifan%20Ding%20and%20Xinyang%20Geng%20and%20Fred%20Alcober%20and%20Roy%20Frostig%20and%20Mark%20Omernick%20and%20Lexi%20Walker%20and%20Cosmin%20Paduraru%20and%20Christina%20Sorokin%20and%20Andrea%20Tacchetti%20and%20Colin%20Gaffney%20and%20Samira%20Daruki%20and%20Olcan%20Sercinoglu%20and%20Zach%20Gleicher%20and%20Juliette%20Love%20and%20Paul%20Voigtlaender%20and%20Rohan%20Jain%20and%20Gabriela%20Surita%20and%20Kareem%20Mohamed%20and%20Rory%20Blevins%20and%20Junwhan%20Ahn%20and%20Tao%20Zhu%20and%20Kornraphop%20Kawintiranon%20and%20Orhan%20Firat%20and%20Yiming%20Gu%20and%20Yujing%20Zhang%20and%20Matthew%20Rahtz%20and%20Manaal%20Faruqui%20and%20Natalie%20Clay%20and%20Justin%20Gilmer%20and%20JD%20Co-Reyes%20and%20Ivo%20Penchev%20and%20Rui%20Zhu%20and%20Nobuyuki%20Morioka%20and%20Kevin%20Hui%20and%20Krishna%20Haridasan%20and%20Victor%20Campos%20and%20Mahdis%20Mahdieh%20and%20Mandy%20Guo%20and%20Samer%20Hassan%20and%20Kevin%20Kilgour%20and%20Arpi%20Vezer%20and%20Heng-Tze%20Cheng%20and%20Raoul%20de%20Liedekerke%20and%20Siddharth%20Goyal%20and%20Paul%20Barham%20and%20DJ%20Strouse%20and%20Seb%20Noury%20and%20Jonas%20Adler%20and%20Mukund%20Sundararajan%20and%20Sharad%20Vikram%20and%20Dmitry%20Lepikhin%20and%20Michela%20Paganini%20and%20Xavier%20Garcia%20and%20Fan%20Yang%20and%20Dasha%20Valter%20and%20Maja%20Trebacz%20and%20Kiran%20Vodrahalli%20and%20Chulayuth%20Asawaroengchai%20and%20Roman%20Ring%20and%20Norbert%20Kalb%20and%20Livio%20Baldini%20Soares%20and%20Siddhartha%20Brahma%20and%20David%20Steiner%20and%20Tianhe%20Yu%20and%20Fabian%20Mentzer%20and%20Antoine%20He%20and%20Lucas%20Gonzalez%20and%20Bibo%20Xu%20and%20Raphael%20Lopez%20Kaufman%20and%20Laurent%20El%20Shafey%20and%20Junhyuk%20Oh%20and%20Tom%20Hennigan%20and%20George%20van%20den%20Driessche%20and%20Seth%20Odoom%20and%20Mario%20Lucic%20and%20Becca%20Roelofs%20and%20Sid%20Lall%20and%20Amit%20Marathe%20and%20Betty%20Chan%20and%20Santiago%20Ontanon%20and%20Luheng%20He%20and%20Denis%20Teplyashin%20and%20Jonathan%20Lai%20and%20Phil%20Crone%20and%20Bogdan%20Damoc%20and%20Lewis%20Ho%20and%20Sebastian%20Riedel%20and%20Karel%20Lenc%20and%20Chih-Kuan%20Yeh%20and%20Aakanksha%20Chowdhery%20and%20Yang%20Xu%20and%20Mehran%20Kazemi%20and%20Ehsan%20Amid%20and%20Anastasia%20Petrushkina%20and%20Kevin%20Swersky%20and%20Ali%20Khodaei%20and%20Gowoon%20Chen%20and%20Chris%20Larkin%20and%20Mario%20Pinto%20and%20Geng%20Yan%20and%20Adria%20Puigdomenech%20Badia%20and%20Piyush%20Patil%20and%20Steven%20Hansen%20and%20Dave%20Orr%20and%20Sebastien%20M.%20R.%20Arnold%20and%20Jordan%20Grimstad%20and%20Andrew%20Dai%20and%20Sholto%20Douglas%20and%20Rishika%20Sinha%20and%20Vikas%20Yadav%20and%20Xi%20Chen%20and%20Elena%20Gribovskaya%20and%20Jacob%20Austin%20and%20Jeffrey%20Zhao%20and%20Kaushal%20Patel%20and%20Paul%20Komarek%20and%20Sophia%20Austin%20and%20Sebastian%20Borgeaud%20and%20Linda%20Friso%20and%20Abhimanyu%20Goyal%20and%20Ben%20Caine%20and%20Kris%20Cao%20and%20Da-Woon%20Chung%20and%20Matthew%20Lamm%20and%20Gabe%20Barth-Maron%20and%20Thais%20Kagohara%20and%20Kate%20Olszewska%20and%20Mia%20Chen%20and%20Kaushik%20Shivakumar%20and%20Rishabh%20Agarwal%20and%20Harshal%20Godhia%20and%20Ravi%20Rajwar%20and%20Javier%20Snaider%20and%20Xerxes%20Dotiwalla%20and%20Yuan%20Liu%20and%20Aditya%20Barua%20and%20Victor%20Ungureanu%20and%20Yuan%20Zhang%20and%20Bat-Orgil%20Batsaikhan%20and%20Mateo%20Wirth%20and%20James%20Qin%20and%20Ivo%20Danihelka%20and%20Tulsee%20Doshi%20and%20Martin%20Chadwick%20and%20Jilin%20Chen%20and%20Sanil%20Jain%20and%20Quoc%20Le%20and%20Arjun%20Kar%20and%20Madhu%20Gurumurthy%20and%20Cheng%20Li%20and%20Ruoxin%20Sang%20and%20Fangyu%20Liu%20and%20Lampros%20Lamprou%20and%20Rich%20Munoz%20and%20Nathan%20Lintz%20and%20Harsh%20Mehta%20and%20Heidi%20Howard%20and%20Malcolm%20Reynolds%20and%20Lora%20Aroyo%20and%20Quan%20Wang%20and%20Lorenzo%20Blanco%20and%20Albin%20Cassirer%20and%20Jordan%20Griffith%20and%20Dipanjan%20Das%20and%20Stephan%20Lee%20and%20Jakub%20Sygnowski%20and%20Zach%20Fisher%20and%20James%20Besley%20and%20Richard%20Powell%20and%20Zafarali%20Ahmed%20and%20Dominik%20Paulus%20and%20David%20Reitter%20and%20Zalan%20Borsos%20and%20Rishabh%20Joshi%20and%20Aedan%20Pope%20and%20Steven%20Hand%20and%20Vittorio%20Selo%20and%20Vihan%20Jain%20and%20Nikhil%20Sethi%20and%20Megha%20Goel%20and%20Takaki%20Makino%20and%20Rhys%20May%20and%20Zhen%20Yang%20and%20Johan%20Schalkwyk%20and%20Christina%20Butterfield%20and%20Anja%20Hauth%20and%20Alex%20Goldin%20and%20Will%20Hawkins%20and%20Evan%20Senter%20and%20Sergey%20Brin%20and%20Oliver%20Woodman%20and%20Marvin%20Ritter%20and%20Eric%20Noland%20and%20Minh%20Giang%20and%20Vijay%20Bolina%20and%20Lisa%20Lee%20and%20Tim%20Blyth%20and%20Ian%20Mackinnon%20and%20Machel%20Reid%20and%20Obaid%20Sarvana%20and%20David%20Silver%20and%20Alexander%20Chen%20and%20Lily%20Wang%20and%20Loren%20Maggiore%20and%20Oscar%20Chang%20and%20Nithya%20Attaluri%20and%20Gregory%20Thornton%20and%20Chung-Cheng%20Chiu%20and%20Oskar%20Bunyan%20and%20Nir%20Levine%20and%20Timothy%20Chung%20and%20Evgenii%20Eltyshev%20and%20Xiance%20Si%20and%20Timothy%20Lillicrap%20and%20Demetra%20Brady%20and%20Vaibhav%20Aggarwal%20and%20Boxi%20Wu%20and%20Yuanzhong%20Xu%20and%20Ross%20McIlroy%20and%20Kartikeya%20Badola%20and%20Paramjit%20Sandhu%20and%20Erica%20Moreira%20and%20Wojciech%20Stokowiec%20and%20Ross%20Hemsley%20and%20Dong%20Li%20and%20Alex%20Tudor%20and%20Pranav%20Shyam%20and%20Elahe%20Rahimtoroghi%20and%20Salem%20Haykal%20and%20Pablo%20Sprechmann%20and%20Xiang%20Zhou%20and%20Diana%20Mincu%20and%20Yujia%20Li%20and%20Ravi%20Addanki%20and%20Kalpesh%20Krishna%20and%20Xiao%20Wu%20and%20Alexandre%20Frechette%20and%20Matan%20Eyal%20and%20Allan%20Dafoe%20and%20Dave%20Lacey%20and%20Jay%20Whang%20and%20Thi%20Avrahami%20and%20Ye%20Zhang%20and%20Emanuel%20Taropa%20and%20Hanzhao%20Lin%20and%20Daniel%20Toyama%20and%20Eliza%20Rutherford%20and%20Motoki%20Sano%20and%20HyunJeong%20Choe%20and%20Alex%20Tomala%20and%20Chalence%20Safranek-Shrader%20and%20Nora%20Kassner%20and%20Mantas%20Pajarskas%20and%20Matt%20Harvey%20and%20Sean%20Sechrist%20and%20Meire%20Fortunato%20and%20Christina%20Lyu%20and%20Gamaleldin%20Elsayed%20and%20Chenkai%20Kuang%20and%20James%20Lottes%20and%20Eric%20Chu%20and%20Chao%20Jia%20and%20Chih-Wei%20Chen%20and%20Peter%20Humphreys%20and%20Kate%20Baumli%20and%20Connie%20Tao%20and%20Rajkumar%20Samuel%20and%20Cicero%20Nogueira%20dos%20Santos%20and%20Anders%20Andreassen%20and%20Nemanja%20Raki%C4%87evi%C4%87%20and%20Dominik%20Grewe%20and%20Aviral%20Kumar%20and%20Stephanie%20Winkler%20and%20Jonathan%20Caton%20and%20Andrew%20Brock%20and%20Sid%20Dalmia%20and%20Hannah%20Sheahan%20and%20Iain%20Barr%20and%20Yingjie%20Miao%20and%20Paul%20Natsev%20and%20Jacob%20Devlin%20and%20Feryal%20Behbahani%20and%20Flavien%20Prost%20and%20Yanhua%20Sun%20and%20Artiom%20Myaskovsky%20and%20Thanumalayan%20Sankaranarayana%20Pillai%20and%20Dan%20Hurt%20and%20Angeliki%20Lazaridou%20and%20Xi%20Xiong%20and%20Ce%20Zheng%20and%20Fabio%20Pardo%20and%20Xiaowei%20Li%20and%20Dan%20Horgan%20and%20Joe%20Stanton%20and%20Moran%20Ambar%20and%20Fei%20Xia%20and%20Alejandro%20Lince%20and%20Mingqiu%20Wang%20and%20Basil%20Mustafa%20and%20Albert%20Webson%20and%20Hyo%20Lee%20and%20Rohan%20Anil%20and%20Martin%20Wicke%20and%20Timothy%20Dozat%20and%20Abhishek%20Sinha%20and%20Enrique%20Piqueras%20and%20Elahe%20Dabir%20and%20Shyam%20Upadhyay%20and%20Anudhyan%20Boral%20and%20Lisa%20Anne%20Hendricks%20and%20Corey%20Fry%20and%20Josip%20Djolonga%20and%20Yi%20Su%20and%20Jake%20Walker%20and%20Jane%20Labanowski%20and%20Ronny%20Huang%20and%20Vedant%20Misra%20and%20Jeremy%20Chen%20and%20RJ%20Skerry-Ryan%20and%20Avi%20Singh%20and%20Shruti%20Rijhwani%20and%20Dian%20Yu%20and%20Alex%20Castro-Ros%20and%20Beer%20Changpinyo%20and%20Romina%20Datta%20and%20Sumit%20Bagri%20and%20Arnar%20Mar%20Hrafnkelsson%20and%20Marcello%20Maggioni%20and%20Daniel%20Zheng%20and%20Yury%20Sulsky%20and%20Shaobo%20Hou%20and%20Tom%20Le%20Paine%20and%20Antoine%20Yang%20and%20Jason%20Riesa%20and%20Dominika%20Rogozinska%20and%20Dror%20Marcus%20and%20Dalia%20El%20Badawy%20and%20Qiao%20Zhang%20and%20Luyu%20Wang%20and%20Helen%20Miller%20and%20Jeremy%20Greer%20and%20Lars%20Lowe%20Sjos%20and%20Azade%20Nova%20and%20Heiga%20Zen%20and%20Rahma%20Chaabouni%20and%20Mihaela%20Rosca%20and%20Jiepu%20Jiang%20and%20Charlie%20Chen%20and%20Ruibo%20Liu%20and%20Tara%20Sainath%20and%20Maxim%20Krikun%20and%20Alex%20Polozov%20and%20Jean-Baptiste%20Lespiau%20and%20Josh%20Newlan%20and%20Zeyncep%20Cankara%20and%20Soo%20Kwak%20and%20Yunhan%20Xu%20and%20Phil%20Chen%20and%20Andy%20Coenen%20and%20Clemens%20Meyer%20and%20Katerina%20Tsihlas%20and%20Ada%20Ma%20and%20Juraj%20Gottweis%20and%20Jinwei%20Xing%20and%20Chenjie%20Gu%20and%20Jin%20Miao%20and%20Christian%20Frank%20and%20Zeynep%20Cankara%20and%20Sanjay%20Ganapathy%20and%20Ishita%20Dasgupta%20and%20Steph%20Hughes-Fitt%20and%20Heng%20Chen%20and%20David%20Reid%20and%20Keran%20Rong%20and%20Hongmin%20Fan%20and%20Joost%20van%20Amersfoort%20and%20Vincent%20Zhuang%20and%20Aaron%20Cohen%20and%20Shixiang%20Shane%20Gu%20and%20Anhad%20Mohananey%20and%20Anastasija%20Ilic%20and%20Taylor%20Tobin%20and%20John%20Wieting%20and%20Anna%20Bortsova%20and%20Phoebe%20Thacker%20and%20Emma%20Wang%20and%20Emily%20Caveness%20and%20Justin%20Chiu%20and%20Eren%20Sezener%20and%20Alex%20Kaskasoli%20and%20Steven%20Baker%20and%20Katie%20Millican%20and%20Mohamed%20Elhawaty%20and%20Kostas%20Aisopos%20and%20Carl%20Lebsack%20and%20Nathan%20Byrd%20and%20Hanjun%20Dai%20and%20Wenhao%20Jia%20and%20Matthew%20Wiethoff%20and%20Elnaz%20Davoodi%20and%20Albert%20Weston%20and%20Lakshman%20Yagati%20and%20Arun%20Ahuja%20and%20Isabel%20Gao%20and%20Golan%20Pundak%20and%20Susan%20Zhang%20and%20Michael%20Azzam%20and%20Khe%20Chai%20Sim%20and%20Sergi%20Caelles%20and%20James%20Keeling%20and%20Abhanshu%20Sharma%20and%20Andy%20Swing%20and%20YaGuang%20Li%20and%20Chenxi%20Liu%20and%20Carrie%20Grimes%20Bostock%20and%20Yamini%20Bansal%20and%20Zachary%20Nado%20and%20Ankesh%20Anand%20and%20Josh%20Lipschultz%20and%20Abhijit%20Karmarkar%20and%20Lev%20Proleev%20and%20Abe%20Ittycheriah%20and%20Soheil%20Hassas%20Yeganeh%20and%20George%20Polovets%20and%20Aleksandra%20Faust%20and%20Jiao%20Sun%20and%20Alban%20Rrustemi%20and%20Pen%20Li%20and%20Rakesh%20Shivanna%20and%20Jeremiah%20Liu%20and%20Chris%20Welty%20and%20Federico%20Lebron%20and%20Anirudh%20Baddepudi%20and%20Sebastian%20Krause%20and%20Emilio%20Parisotto%20and%20Radu%20Soricut%20and%20Zheng%20Xu%20and%20Dawn%20Bloxwich%20and%20Melvin%20Johnson%20and%20Behnam%20Neyshabur%20and%20Justin%20Mao-Jones%20and%20Renshen%20Wang%20and%20Vinay%20Ramasesh%20and%20Zaheer%20Abbas%20and%20Arthur%20Guez%20and%20Constant%20Segal%20and%20Duc%20Dung%20Nguyen%20and%20James%20Svensson%20and%20Le%20Hou%20and%20Sarah%20York%20and%20Kieran%20Milan%20and%20Sophie%20Bridgers%20and%20Wiktor%20Gworek%20and%20Marco%20Tagliasacchi%20and%20James%20Lee-Thorp%20and%20Michael%20Chang%20and%20Alexey%20Guseynov%20and%20Ale%20Jakse%20Hartman%20and%20Michael%20Kwong%20and%20Ruizhe%20Zhao%20and%20Sheleem%20Kashem%20and%20Elizabeth%20Cole%20and%20Antoine%20Miech%20and%20Richard%20Tanburn%20and%20Mary%20Phuong%20and%20Filip%20Pavetic%20and%20Sebastien%20Cevey%20and%20Ramona%20Comanescu%20and%20Richard%20Ives%20and%20Sherry%20Yang%20and%20Cosmo%20Du%20and%20Bo%20Li%20and%20Zizhao%20Zhang%20and%20Mariko%20Iinuma%20and%20Clara%20Huiyi%20Hu%20and%20Aurko%20Roy%20and%20Shaan%20Bijwadia%20and%20Zhenkai%20Zhu%20and%20Danilo%20Martins%20and%20Rachel%20Saputro%20and%20Anita%20Gergely%20and%20Steven%20Zheng%20and%20Dawei%20Jia%20and%20Ioannis%20Antonoglou%20and%20Adam%20Sadovsky%20and%20Shane%20Gu%20and%20Yingying%20Bi%20and%20Alek%20Andreev%20and%20Sina%20Samangooei%20and%20Mina%20Khan%20and%20Tomas%20Kocisky%20and%20Angelos%20Filos%20and%20Chintu%20Kumar%20and%20Colton%20Bishop%20and%20Adams%20Yu%20and%20Sarah%20Hodkinson%20and%20Sid%20Mittal%20and%20Premal%20Shah%20and%20Alexandre%20Moufarek%20and%20Yong%20Cheng%20and%20Adam%20Bloniarz%20and%20Jaehoon%20Lee%20and%20Pedram%20Pejman%20and%20Paul%20Michel%20and%20Stephen%20Spencer%20and%20Vladimir%20Feinberg%20and%20Xuehan%20Xiong%20and%20Nikolay%20Savinov%20and%20Charlotte%20Smith%20and%20Siamak%20Shakeri%20and%20Dustin%20Tran%20and%20Mary%20Chesus%20and%20Bernd%20Bohnet%20and%20George%20Tucker%20and%20Tamara%20von%20Glehn%20and%20Carrie%20Muir%20and%20Yiran%20Mao%20and%20Hideto%20Kazawa%20and%20Ambrose%20Slone%20and%20Kedar%20Soparkar%20and%20Disha%20Shrivastava%20and%20James%20Cobon-Kerr%20and%20Michael%20Sharman%20and%20Jay%20Pavagadhi%20and%20Carlos%20Araya%20and%20Karolis%20Misiunas%20and%20Nimesh%20Ghelani%20and%20Michael%20Laskin%20and%20David%20Barker%20and%20Qiujia%20Li%20and%20Anton%20Briukhov%20and%20Neil%20Houlsby%20and%20Mia%20Glaese%20and%20Balaji%20Lakshminarayanan%20and%20Nathan%20Schucher%20and%20Yunhao%20Tang%20and%20Eli%20Collins%20and%20Hyeontaek%20Lim%20and%20Fangxiaoyu%20Feng%20and%20Adria%20Recasens%20and%20Guangda%20Lai%20and%20Alberto%20Magni%20and%20Nicola%20De%20Cao%20and%20Aditya%20Siddhant%20and%20Zoe%20Ashwood%20and%20Jordi%20Orbay%20and%20Mostafa%20Dehghani%20and%20Jenny%20Brennan%20and%20Yifan%20He%20and%20Kelvin%20Xu%20and%20Yang%20Gao%20and%20Carl%20Saroufim%20and%20James%20Molloy%20and%20Xinyi%20Wu%20and%20Seb%20Arnold%20and%20Solomon%20Chang%20and%20Julian%20Schrittwieser%20and%20Elena%20Buchatskaya%20and%20Soroush%20Radpour%20and%20Martin%20Polacek%20and%20Skye%20Giordano%20and%20Ankur%20Bapna%20and%20Simon%20Tokumine%20and%20Vincent%20Hellendoorn%20and%20Thibault%20Sottiaux%20and%20Sarah%20Cogan%20and%20Aliaksei%20Severyn%20and%20Mohammad%20Saleh%20and%20Shantanu%20Thakoor%20and%20Laurent%20Shefey%20and%20Siyuan%20Qiao%20and%20Meenu%20Gaba%20and%20Shuo-yiin%20Chang%20and%20Craig%20Swanson%20and%20Biao%20Zhang%20and%20Benjamin%20Lee%20and%20Paul%20Kishan%20Rubenstein%20and%20Gan%20Song%20and%20Tom%20Kwiatkowski%20and%20Anna%20Koop%20and%20Ajay%20Kannan%20and%20David%20Kao%20and%20Parker%20Schuh%20and%20Axel%20Stjerngren%20and%20Golnaz%20Ghiasi%20and%20Gena%20Gibson%20and%20Luke%20Vilnis%20and%20Ye%20Yuan%20and%20Felipe%20Tiengo%20Ferreira%20and%20Aishwarya%20Kamath%20and%20Ted%20Klimenko%20and%20Ken%20Franko%20and%20Kefan%20Xiao%20and%20Indro%20Bhattacharya%20and%20Miteyan%20Patel%20and%20Rui%20Wang%20and%20Alex%20Morris%20and%20Robin%20Strudel%20and%20Vivek%20Sharma%20and%20Peter%20Choy%20and%20Sayed%20Hadi%20Hashemi%20and%20Jessica%20Landon%20and%20Mara%20Finkelstein%20and%20Priya%20Jhakra%20and%20Justin%20Frye%20and%20Megan%20Barnes%20and%20Matthew%20Mauger%20and%20Dennis%20Daun%20and%20Khuslen%20Baatarsukh%20and%20Matthew%20Tung%20and%20Wael%20Farhan%20and%20Henryk%20Michalewski%20and%20Fabio%20Viola%20and%20Felix%20de%20Chaumont%20Quitry%20and%20Charline%20Le%20Lan%20and%20Tom%20Hudson%20and%20Qingze%20Wang%20and%20Felix%20Fischer%20and%20Ivy%20Zheng%20and%20Elspeth%20White%20and%20Anca%20Dragan%20and%20Jean-baptiste%20Alayrac%20and%20Eric%20Ni%20and%20Alexander%20Pritzel%20and%20Adam%20Iwanicki%20and%20Michael%20Isard%20and%20Anna%20Bulanova%20and%20Lukas%20Zilka%20and%20Ethan%20Dyer%20and%20Devendra%20Sachan%20and%20Srivatsan%20Srinivasan%20and%20Hannah%20Muckenhirn%20and%20Honglong%20Cai%20and%20Amol%20Mandhane%20and%20Mukarram%20Tariq%20and%20Jack%20W.%20Rae%20and%20Gary%20Wang%20and%20Kareem%20Ayoub%20and%20Nicholas%20FitzGerald%20and%20Yao%20Zhao%20and%20Woohyun%20Han%20and%20Chris%20Alberti%20and%20Dan%20Garrette%20and%20Kashyap%20Krishnakumar%20and%20Mai%20Gimenez%20and%20Anselm%20Levskaya%20and%20Daniel%20Sohn%20and%20Josip%20Matak%20and%20Inaki%20Iturrate%20and%20Michael%20B.%20Chang%20and%20Jackie%20Xiang%20and%20Yuan%20Cao%20and%20Nishant%20Ranka%20and%20Geoff%20Brown%20and%20Adrian%20Hutter%20and%20Vahab%20Mirrokni%20and%20Nanxin%20Chen%20and%20Kaisheng%20Yao%20and%20Zoltan%20Egyed%20and%20Francois%20Galilee%20and%20Tyler%20Liechty%20and%20Praveen%20Kallakuri%20and%20Evan%20Palmer%20and%20Sanjay%20Ghemawat%20and%20Jasmine%20Liu%20and%20David%20Tao%20and%20Chloe%20Thornton%20and%20Tim%20Green%20and%20Mimi%20Jasarevic%20and%20Sharon%20Lin%20and%20Victor%20Cotruta%20and%20Yi-Xuan%20Tan%20and%20Noah%20Fiedel%20and%20Hongkun%20Yu%20and%20Ed%20Chi%20and%20Alexander%20Neitz%20and%20Jens%20Heitkaemper%20and%20Anu%20Sinha%20and%20Denny%20Zhou%20and%20Yi%20Sun%20and%20Charbel%20Kaed%20and%20Brice%20Hulse%20and%20Swaroop%20Mishra%20and%20Maria%20Georgaki%20and%20Sneha%20Kudugunta%20and%20Clement%20Farabet%20and%20Izhak%20Shafran%20and%20Daniel%20Vlasic%20and%20Anton%20Tsitsulin%20and%20Rajagopal%20Ananthanarayanan%20and%20Alen%20Carin%20and%20Guolong%20Su%20and%20Pei%20Sun%20and%20Shashank%20V%20and%20Gabriel%20Carvajal%20and%20Josef%20Broder%20and%20Iulia%20Comsa%20and%20Alena%20Repina%20and%20William%20Wong%20and%20Warren%20Weilun%20Chen%20and%20Peter%20Hawkins%20and%20Egor%20Filonov%20and%20Lucia%20Loher%20and%20Christoph%20Hirnschall%20and%20Weiyi%20Wang%20and%20Jingchen%20Ye%20and%20Andrea%20Burns%20and%20Hardie%20Cate%20and%20Diana%20Gage%20Wright%20and%20Federico%20Piccinini%20and%20Lei%20Zhang%20and%20Chu-Cheng%20Lin%20and%20Ionel%20Gog%20and%20Yana%20Kulizhskaya%20and%20Ashwin%20Sreevatsa%20and%20Shuang%20Song%20and%20Luis%20C.%20Cobo%20and%20Anand%20Iyer%20and%20Chetan%20Tekur%20and%20Guillermo%20Garrido%20and%20Zhuyun%20Xiao%20and%20Rupert%20Kemp%20and%20Huaixiu%20Steven%20Zheng%20and%20Hui%20Li%20and%20Ananth%20Agarwal%20and%20Christel%20Ngani%20and%20Kati%20Goshvadi%20and%20Rebeca%20Santamaria-Fernandez%20and%20Wojciech%20Fica%20and%20Xinyun%20Chen%20and%20Chris%20Gorgolewski%20and%20Sean%20Sun%20and%20Roopal%20Garg%20and%20Xinyu%20Ye%20and%20S.%20M.%20Ali%20Eslami%20and%20Nan%20Hua%20and%20Jon%20Simon%20and%20Pratik%20Joshi%20and%20Yelin%20Kim%20and%20Ian%20Tenney%20and%20Sahitya%20Potluri%20and%20Lam%20Nguyen%20Thiet%20and%20Quan%20Yuan%20and%20Florian%20Luisier%20and%20Alexandra%20Chronopoulou%20and%20Salvatore%20Scellato%20and%20Praveen%20Srinivasan%20and%20Minmin%20Chen%20and%20Vinod%20Koverkathu%20and%20Valentin%20Dalibard%20and%20Yaming%20Xu%20and%20Brennan%20Saeta%20and%20Keith%20Anderson%20and%20Thibault%20Sellam%20and%20Nick%20Fernando%20and%20Fantine%20Huot%20and%20Junehyuk%20Jung%20and%20Mani%20Varadarajan%20and%20Michael%20Quinn%20and%20Amit%20Raul%20and%20Maigo%20Le%20and%20Ruslan%20Habalov%20and%20Jon%20Clark%20and%20Komal%20Jalan%20and%20Kalesha%20Bullard%20and%20Achintya%20Singhal%20and%20Thang%20Luong%20and%20Boyu%20Wang%20and%20Sujeevan%20Rajayogam%20and%20Julian%20Eisenschlos%20and%20Johnson%20Jia%20and%20Daniel%20Finchelstein%20and%20Alex%20Yakubovich%20and%20Daniel%20Balle%20and%20Michael%20Fink%20and%20Sameer%20Agarwal%20and%20Jing%20Li%20and%20Dj%20Dvijotham%20and%20Shalini%20Pal%20and%20Kai%20Kang%20and%20Jaclyn%20Konzelmann%20and%20Jennifer%20Beattie%20and%20Olivier%20Dousse%20and%20Diane%20Wu%20and%20Remi%20Crocker%20and%20Chen%20Elkind%20and%20Siddhartha%20Reddy%20Jonnalagadda%20and%20Jong%20Lee%20and%20Dan%20Holtmann-Rice%20and%20Krystal%20Kallarackal%20and%20Rosanne%20Liu%20and%20Denis%20Vnukov%20and%20Neera%20Vats%20and%20Luca%20Invernizzi%20and%20Mohsen%20Jafari%20and%20Huanjie%20Zhou%20and%20Lilly%20Taylor%20and%20Jennifer%20Prendki%20and%20Marcus%20Wu%20and%20Tom%20Eccles%20and%20Tianqi%20Liu%20and%20Kavya%20Kopparapu%20and%20Francoise%20Beaufays%20and%20Christof%20Angermueller%20and%20Andreea%20Marzoca%20and%20Shourya%20Sarcar%20and%20Hilal%20Dib%20and%20Jeff%20Stanway%20and%20Frank%20Perbet%20and%20Nejc%20Trdin%20and%20Rachel%20Sterneck%20and%20Andrey%20Khorlin%20and%20Dinghua%20Li%20and%20Xihui%20Wu%20and%20Sonam%20Goenka%20and%20David%20Madras%20and%20Sasha%20Goldshtein%20and%20Willi%20Gierke%20and%20Tong%20Zhou%20and%20Yaxin%20Liu%20and%20Yannie%20Liang%20and%20Anais%20White%20and%20Yunjie%20Li%20and%20Shreya%20Singh%20and%20Sanaz%20Bahargam%20and%20Mark%20Epstein%20and%20Sujoy%20Basu%20and%20Li%20Lao%20and%20Adnan%20Ozturel%20and%20Carl%20Crous%20and%20Alex%20Zhai%20and%20Han%20Lu%20and%20Zora%20Tung%20and%20Neeraj%20Gaur%20and%20Alanna%20Walton%20and%20Lucas%20Dixon%20and%20Ming%20Zhang%20and%20Amir%20Globerson%20and%20Grant%20Uy%20and%20Andrew%20Bolt%20and%20Olivia%20Wiles%20and%20Milad%20Nasr%20and%20Ilia%20Shumailov%20and%20Marco%20Selvi%20and%20Francesco%20Piccinno%20and%20Ricardo%20Aguilar%20and%20Sara%20McCarthy%20and%20Misha%20Khalman%20and%20Mrinal%20Shukla%20and%20Vlado%20Galic%20and%20John%20Carpenter%20and%20Kevin%20Villela%20and%20Haibin%20Zhang%20and%20Harry%20Richardson%20and%20James%20Martens%20and%20Matko%20Bosnjak%20and%20Shreyas%20Rammohan%20Belle%20and%20Jeff%20Seibert%20and%20Mahmoud%20Alnahlawi%20and%20Brian%20McWilliams%20and%20Sankalp%20Singh%20and%20Annie%20Louis%20and%20Wen%20Ding%20and%20Dan%20Popovici%20and%20Lenin%20Simicich%20and%20Laura%20Knight%20and%20Pulkit%20Mehta%20and%20Nishesh%20Gupta%20and%20Chongyang%20Shi%20and%20Saaber%20Fatehi%20and%20Jovana%20Mitrovic%20and%20Alex%20Grills%20and%20Joseph%20Pagadora%20and%20Dessie%20Petrova%20and%20Danielle%20Eisenbud%20and%20Zhishuai%20Zhang%20and%20Damion%20Yates%20and%20Bhavishya%20Mittal%20and%20Nilesh%20Tripuraneni%20and%20Yannis%20Assael%20and%20Thomas%20Brovelli%20and%20Prateek%20Jain%20and%20Mihajlo%20Velimirovic%20and%20Canfer%20Akbulut%20and%20Jiaqi%20Mu%20and%20Wolfgang%20Macherey%20and%20Ravin%20Kumar%20and%20Jun%20Xu%20and%20Haroon%20Qureshi%20and%20Gheorghe%20Comanici%20and%20Jeremy%20Wiesner%20and%20Zhitao%20Gong%20and%20Anton%20Ruddock%20and%20Matthias%20Bauer%20and%20Nick%20Felt%20and%20Anirudh%20GP%20and%20Anurag%20Arnab%20and%20Dustin%20Zelle%20and%20Jonas%20Rothfuss%20and%20Bill%20Rosgen%20and%20Ashish%20Shenoy%20and%20Bryan%20Seybold%20and%20Xinjian%20Li%20and%20Jayaram%20Mudigonda%20and%20Goker%20Erdogan%20and%20Jiawei%20Xia%20and%20Jiri%20Simsa%20and%20Andrea%20Michi%20and%20Yi%20Yao%20and%20Christopher%20Yew%20and%20Steven%20Kan%20and%20Isaac%20Caswell%20and%20Carey%20Radebaugh%20and%20Andre%20Elisseeff%20and%20Pedro%20Valenzuela%20and%20Kay%20McKinney%20and%20Kim%20Paterson%20and%20Albert%20Cui%20and%20Eri%20Latorre-Chimoto%20and%20Solomon%20Kim%20and%20William%20Zeng%20and%20Ken%20Durden%20and%20Priya%20Ponnapalli%20and%20Tiberiu%20Sosea%20and%20Christopher%20A.%20Choquette-Choo%20and%20James%20Manyika%20and%20Brona%20Robenek%20and%20Harsha%20Vashisht%20and%20Sebastien%20Pereira%20and%20Hoi%20Lam%20and%20Marko%20Velic%20and%20Denese%20Owusu-Afriyie%20and%20Katherine%20Lee%20and%20Tolga%20Bolukbasi%20and%20Alicia%20Parrish%20and%20Shawn%20Lu%20and%20Jane%20Park%20and%20Balaji%20Venkatraman%20and%20Alice%20Talbert%20and%20Lambert%20Rosique%20and%20Yuchung%20Cheng%20and%20Andrei%20Sozanschi%20and%20Adam%20Paszke%20and%20Praveen%20Kumar%20and%20Jessica%20Austin%20and%20Lu%20Li%20and%20Khalid%20Salama%20and%20Wooyeol%20Kim%20and%20Nandita%20Dukkipati%20and%20Anthony%20Baryshnikov%20and%20Christos%20Kaplanis%20and%20XiangHai%20Sheng%20and%20Yuri%20Chervonyi%20and%20Caglar%20Unlu%20and%20Diego%20de%20Las%20Casas%20and%20Harry%20Askham%20and%20Kathryn%20Tunyasuvunakool%20and%20Felix%20Gimeno%20and%20Siim%20Poder%20and%20Chester%20Kwak%20and%20Matt%20Miecnikowski%20and%20Vahab%20Mirrokni%20and%20Alek%20Dimitriev%20and%20Aaron%20Parisi%20and%20Dangyi%20Liu%20and%20Tomy%20Tsai%20and%20Toby%20Shevlane%20and%20Christina%20Kouridi%20and%20Drew%20Garmon%20and%20Adrian%20Goedeckemeyer%20and%20Adam%20R.%20Brown%20and%20Anitha%20Vijayakumar%20and%20Ali%20Elqursh%20and%20Sadegh%20Jazayeri%20and%20Jin%20Huang%20and%20Sara%20Mc%20Carthy%20and%20Jay%20Hoover%20and%20Lucy%20Kim%20and%20Sandeep%20Kumar%20and%20Wei%20Chen%20and%20Courtney%20Biles%20and%20Garrett%20Bingham%20and%20Evan%20Rosen%20and%20Lisa%20Wang%20and%20Qijun%20Tan%20and%20David%20Engel%20and%20Francesco%20Pongetti%20and%20Dario%20de%20Cesare%20and%20Dongseong%20Hwang%20and%20Lily%20Yu%20and%20Jennifer%20Pullman%20and%20Srini%20Narayanan%20and%20Kyle%20Levin%20and%20Siddharth%20Gopal%20and%20Megan%20Li%20and%20Asaf%20Aharoni%20and%20Trieu%20Trinh%20and%20Jessica%20Lo%20and%20Norman%20Casagrande%20and%20Roopali%20Vij%20and%20Loic%20Matthey%20and%20Bramandia%20Ramadhana%20and%20Austin%20Matthews%20and%20CJ%20Carey%20and%20Matthew%20Johnson%20and%20Kremena%20Goranova%20and%20Rohin%20Shah%20and%20Shereen%20Ashraf%20and%20Kingshuk%20Dasgupta%20and%20Rasmus%20Larsen%20and%20Yicheng%20Wang%20and%20Manish%20Reddy%20Vuyyuru%20and%20Chong%20Jiang%20and%20Joana%20Ijazi%20and%20Kazuki%20Osawa%20and%20Celine%20Smith%20and%20Ramya%20Sree%20Boppana%20and%20Taylan%20Bilal%20and%20Yuma%20Koizumi%20and%20Ying%20Xu%20and%20Yasemin%20Altun%20and%20Nir%20Shabat%20and%20Ben%20Bariach%20and%20Alex%20Korchemniy%20and%20Kiam%20Choo%20and%20Olaf%20Ronneberger%20and%20Chimezie%20Iwuanyanwu%20and%20Shubin%20Zhao%20and%20David%20Soergel%20and%20Cho-Jui%20Hsieh%20and%20Irene%20Cai%20and%20Shariq%20Iqbal%20and%20Martin%20Sundermeyer%20and%20Zhe%20Chen%20and%20Elie%20Bursztein%20and%20Chaitanya%20Malaviya%20and%20Fadi%20Biadsy%20and%20Prakash%20Shroff%20and%20Inderjit%20Dhillon%20and%20Tejasi%20Latkar%20and%20Chris%20Dyer%20and%20Hannah%20Forbes%20and%20Massimo%20Nicosia%20and%20Vitaly%20Nikolaev%20and%20Somer%20Greene%20and%20Marin%20Georgiev%20and%20Pidong%20Wang%20and%20Nina%20Martin%20and%20Hanie%20Sedghi%20and%20John%20Zhang%20and%20Praseem%20Banzal%20and%20Doug%20Fritz%20and%20Vikram%20Rao%20and%20Xuezhi%20Wang%20and%20Jiageng%20Zhang%20and%20Viorica%20Patraucean%20and%20Dayou%20Du%20and%20Igor%20Mordatch%20and%20Ivan%20Jurin%20and%20Lewis%20Liu%20and%20Ayush%20Dubey%20and%20Abhi%20Mohan%20and%20Janek%20Nowakowski%20and%20Vlad-Doru%20Ion%20and%20Nan%20Wei%20and%20Reiko%20Tojo%20and%20Maria%20Abi%20Raad%20and%20Drew%20A.%20Hudson%20and%20Vaishakh%20Keshava%20and%20Shubham%20Agrawal%20and%20Kevin%20Ramirez%20and%20Zhichun%20Wu%20and%20Hoang%20Nguyen%20and%20Ji%20Liu%20and%20Madhavi%20Sewak%20and%20Bryce%20Petrini%20and%20DongHyun%20Choi%20and%20Ivan%20Philips%20and%20Ziyue%20Wang%20and%20Ioana%20Bica%20and%20Ankush%20Garg%20and%20Jarek%20Wilkiewicz%20and%20Priyanka%20Agrawal%20and%20Xiaowei%20Li%20and%20Danhao%20Guo%20and%20Emily%20Xue%20and%20Naseer%20Shaik%20and%20Andrew%20Leach%20and%20Sadh%20MNM%20Khan%20and%20Julia%20Wiesinger%20and%20Sammy%20Jerome%20and%20Abhishek%20Chakladar%20and%20Alek%20Wenjiao%20Wang%20and%20Tina%20Ornduff%20and%20Folake%20Abu%20and%20Alireza%20Ghaffarkhah%20and%20Marcus%20Wainwright%20and%20Mario%20Cortes%20and%20Frederick%20Liu%20and%20Joshua%20Maynez%20and%20Andreas%20Terzis%20and%20Pouya%20Samangouei%20and%20Riham%20Mansour%20and%20Tomasz%20K%C4%99pa%20and%20Fran%C3%A7ois-Xavier%20Aubet%20and%20Anton%20Algymr%20and%20Dan%20Banica%20and%20Agoston%20Weisz%20and%20Andras%20Orban%20and%20Alexandre%20Senges%20and%20Ewa%20Andrejczuk%20and%20Mark%20Geller%20and%20Niccolo%20Dal%20Santo%20and%20Valentin%20Anklin%20and%20Majd%20Al%20Merey%20and%20Martin%20Baeuml%20and%20Trevor%20Strohman%20and%20Junwen%20Bai%20and%20Slav%20Petrov%20and%20Yonghui%20Wu%20and%20Demis%20Hassabis%20and%20Koray%20Kavukcuoglu%20and%20Jeffrey%20Dean%20and%20Oriol%20Vinyals&entry.1292438233=%20%20In%20this%20report%2C%20we%20introduce%20the%20Gemini%201.5%20family%20of%20models%2C%20representing%0Athe%20next%20generation%20of%20highly%20compute-efficient%20multimodal%20models%20capable%20of%0Arecalling%20and%20reasoning%20over%20fine-grained%20information%20from%20millions%20of%20tokens%0Aof%20context%2C%20including%20multiple%20long%20documents%20and%20hours%20of%20video%20and%20audio.%20The%0Afamily%20includes%20two%20new%20models%3A%20%281%29%20an%20updated%20Gemini%201.5%20Pro%2C%20which%20exceeds%0Athe%20February%20version%20on%20the%20great%20majority%20of%20capabilities%20and%20benchmarks%3B%20%282%29%0AGemini%201.5%20Flash%2C%20a%20more%20lightweight%20variant%20designed%20for%20efficiency%20with%0Aminimal%20regression%20in%20quality.%20Gemini%201.5%20models%20achieve%20near-perfect%20recall%20on%0Along-context%20retrieval%20tasks%20across%20modalities%2C%20improve%20the%20state-of-the-art%20in%0Along-document%20QA%2C%20long-video%20QA%20and%20long-context%20ASR%2C%20and%20match%20or%20surpass%0AGemini%201.0%20Ultra%27s%20state-of-the-art%20performance%20across%20a%20broad%20set%20of%0Abenchmarks.%20Studying%20the%20limits%20of%20Gemini%201.5%27s%20long-context%20ability%2C%20we%20find%0Acontinued%20improvement%20in%20next-token%20prediction%20and%20near-perfect%20retrieval%0A%28%3E99%25%29%20up%20to%20at%20least%2010M%20tokens%2C%20a%20generational%20leap%20over%20existing%20models%20such%0Aas%20Claude%203.0%20%28200k%29%20and%20GPT-4%20Turbo%20%28128k%29.%20Finally%2C%20we%20highlight%20real-world%0Ause%20cases%2C%20such%20as%20Gemini%201.5%20collaborating%20with%20professionals%20on%20completing%0Atheir%20tasks%20achieving%2026%20to%2075%25%20time%20savings%20across%2010%20different%20job%0Acategories%2C%20as%20well%20as%20surprising%20new%20capabilities%20of%20large%20language%20models%20at%0Athe%20frontier%3B%20when%20given%20a%20grammar%20manual%20for%20Kalamang%2C%20a%20language%20with%20fewer%0Athan%20200%20speakers%20worldwide%2C%20the%20model%20learns%20to%20translate%20English%20to%20Kalamang%0Aat%20a%20similar%20level%20to%20a%20person%20who%20learned%20from%20the%20same%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05530v4&entry.124074799=Read"},
{"title": "Deep Generative Models in Robotics: A Survey on Learning from Multimodal\n  Demonstrations", "author": "Julen Urain and Ajay Mandlekar and Yilun Du and Mahi Shafiullah and Danfei Xu and Katerina Fragkiadaki and Georgia Chalvatzaki and Jan Peters", "abstract": "  Learning from Demonstrations, the field that proposes to learn robot behavior\nmodels from data, is gaining popularity with the emergence of deep generative\nmodels. Although the problem has been studied for years under names such as\nImitation Learning, Behavioral Cloning, or Inverse Reinforcement Learning,\nclassical methods have relied on models that don't capture complex data\ndistributions well or don't scale well to large numbers of demonstrations. In\nrecent years, the robot learning community has shown increasing interest in\nusing deep generative models to capture the complexity of large datasets. In\nthis survey, we aim to provide a unified and comprehensive review of the last\nyear's progress in the use of deep generative models in robotics. We present\nthe different types of models that the community has explored, such as\nenergy-based models, diffusion models, action value maps, or generative\nadversarial networks. We also present the different types of applications in\nwhich deep generative models have been used, from grasp generation to\ntrajectory generation or cost learning. One of the most important elements of\ngenerative models is the generalization out of distributions. In our survey, we\nreview the different decisions the community has made to improve the\ngeneralization of the learned models. Finally, we highlight the research\nchallenges and propose a number of future directions for learning deep\ngenerative models in robotics.\n", "link": "http://arxiv.org/abs/2408.04380v1", "date": "2024-08-08", "relevancy": 2.3503, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5966}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5864}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5852}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Generative%20Models%20in%20Robotics%3A%20A%20Survey%20on%20Learning%20from%20Multimodal%0A%20%20Demonstrations&body=Title%3A%20Deep%20Generative%20Models%20in%20Robotics%3A%20A%20Survey%20on%20Learning%20from%20Multimodal%0A%20%20Demonstrations%0AAuthor%3A%20Julen%20Urain%20and%20Ajay%20Mandlekar%20and%20Yilun%20Du%20and%20Mahi%20Shafiullah%20and%20Danfei%20Xu%20and%20Katerina%20Fragkiadaki%20and%20Georgia%20Chalvatzaki%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Learning%20from%20Demonstrations%2C%20the%20field%20that%20proposes%20to%20learn%20robot%20behavior%0Amodels%20from%20data%2C%20is%20gaining%20popularity%20with%20the%20emergence%20of%20deep%20generative%0Amodels.%20Although%20the%20problem%20has%20been%20studied%20for%20years%20under%20names%20such%20as%0AImitation%20Learning%2C%20Behavioral%20Cloning%2C%20or%20Inverse%20Reinforcement%20Learning%2C%0Aclassical%20methods%20have%20relied%20on%20models%20that%20don%27t%20capture%20complex%20data%0Adistributions%20well%20or%20don%27t%20scale%20well%20to%20large%20numbers%20of%20demonstrations.%20In%0Arecent%20years%2C%20the%20robot%20learning%20community%20has%20shown%20increasing%20interest%20in%0Ausing%20deep%20generative%20models%20to%20capture%20the%20complexity%20of%20large%20datasets.%20In%0Athis%20survey%2C%20we%20aim%20to%20provide%20a%20unified%20and%20comprehensive%20review%20of%20the%20last%0Ayear%27s%20progress%20in%20the%20use%20of%20deep%20generative%20models%20in%20robotics.%20We%20present%0Athe%20different%20types%20of%20models%20that%20the%20community%20has%20explored%2C%20such%20as%0Aenergy-based%20models%2C%20diffusion%20models%2C%20action%20value%20maps%2C%20or%20generative%0Aadversarial%20networks.%20We%20also%20present%20the%20different%20types%20of%20applications%20in%0Awhich%20deep%20generative%20models%20have%20been%20used%2C%20from%20grasp%20generation%20to%0Atrajectory%20generation%20or%20cost%20learning.%20One%20of%20the%20most%20important%20elements%20of%0Agenerative%20models%20is%20the%20generalization%20out%20of%20distributions.%20In%20our%20survey%2C%20we%0Areview%20the%20different%20decisions%20the%20community%20has%20made%20to%20improve%20the%0Ageneralization%20of%20the%20learned%20models.%20Finally%2C%20we%20highlight%20the%20research%0Achallenges%20and%20propose%20a%20number%20of%20future%20directions%20for%20learning%20deep%0Agenerative%20models%20in%20robotics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04380v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Generative%2520Models%2520in%2520Robotics%253A%2520A%2520Survey%2520on%2520Learning%2520from%2520Multimodal%250A%2520%2520Demonstrations%26entry.906535625%3DJulen%2520Urain%2520and%2520Ajay%2520Mandlekar%2520and%2520Yilun%2520Du%2520and%2520Mahi%2520Shafiullah%2520and%2520Danfei%2520Xu%2520and%2520Katerina%2520Fragkiadaki%2520and%2520Georgia%2520Chalvatzaki%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Learning%2520from%2520Demonstrations%252C%2520the%2520field%2520that%2520proposes%2520to%2520learn%2520robot%2520behavior%250Amodels%2520from%2520data%252C%2520is%2520gaining%2520popularity%2520with%2520the%2520emergence%2520of%2520deep%2520generative%250Amodels.%2520Although%2520the%2520problem%2520has%2520been%2520studied%2520for%2520years%2520under%2520names%2520such%2520as%250AImitation%2520Learning%252C%2520Behavioral%2520Cloning%252C%2520or%2520Inverse%2520Reinforcement%2520Learning%252C%250Aclassical%2520methods%2520have%2520relied%2520on%2520models%2520that%2520don%2527t%2520capture%2520complex%2520data%250Adistributions%2520well%2520or%2520don%2527t%2520scale%2520well%2520to%2520large%2520numbers%2520of%2520demonstrations.%2520In%250Arecent%2520years%252C%2520the%2520robot%2520learning%2520community%2520has%2520shown%2520increasing%2520interest%2520in%250Ausing%2520deep%2520generative%2520models%2520to%2520capture%2520the%2520complexity%2520of%2520large%2520datasets.%2520In%250Athis%2520survey%252C%2520we%2520aim%2520to%2520provide%2520a%2520unified%2520and%2520comprehensive%2520review%2520of%2520the%2520last%250Ayear%2527s%2520progress%2520in%2520the%2520use%2520of%2520deep%2520generative%2520models%2520in%2520robotics.%2520We%2520present%250Athe%2520different%2520types%2520of%2520models%2520that%2520the%2520community%2520has%2520explored%252C%2520such%2520as%250Aenergy-based%2520models%252C%2520diffusion%2520models%252C%2520action%2520value%2520maps%252C%2520or%2520generative%250Aadversarial%2520networks.%2520We%2520also%2520present%2520the%2520different%2520types%2520of%2520applications%2520in%250Awhich%2520deep%2520generative%2520models%2520have%2520been%2520used%252C%2520from%2520grasp%2520generation%2520to%250Atrajectory%2520generation%2520or%2520cost%2520learning.%2520One%2520of%2520the%2520most%2520important%2520elements%2520of%250Agenerative%2520models%2520is%2520the%2520generalization%2520out%2520of%2520distributions.%2520In%2520our%2520survey%252C%2520we%250Areview%2520the%2520different%2520decisions%2520the%2520community%2520has%2520made%2520to%2520improve%2520the%250Ageneralization%2520of%2520the%2520learned%2520models.%2520Finally%252C%2520we%2520highlight%2520the%2520research%250Achallenges%2520and%2520propose%2520a%2520number%2520of%2520future%2520directions%2520for%2520learning%2520deep%250Agenerative%2520models%2520in%2520robotics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04380v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Generative%20Models%20in%20Robotics%3A%20A%20Survey%20on%20Learning%20from%20Multimodal%0A%20%20Demonstrations&entry.906535625=Julen%20Urain%20and%20Ajay%20Mandlekar%20and%20Yilun%20Du%20and%20Mahi%20Shafiullah%20and%20Danfei%20Xu%20and%20Katerina%20Fragkiadaki%20and%20Georgia%20Chalvatzaki%20and%20Jan%20Peters&entry.1292438233=%20%20Learning%20from%20Demonstrations%2C%20the%20field%20that%20proposes%20to%20learn%20robot%20behavior%0Amodels%20from%20data%2C%20is%20gaining%20popularity%20with%20the%20emergence%20of%20deep%20generative%0Amodels.%20Although%20the%20problem%20has%20been%20studied%20for%20years%20under%20names%20such%20as%0AImitation%20Learning%2C%20Behavioral%20Cloning%2C%20or%20Inverse%20Reinforcement%20Learning%2C%0Aclassical%20methods%20have%20relied%20on%20models%20that%20don%27t%20capture%20complex%20data%0Adistributions%20well%20or%20don%27t%20scale%20well%20to%20large%20numbers%20of%20demonstrations.%20In%0Arecent%20years%2C%20the%20robot%20learning%20community%20has%20shown%20increasing%20interest%20in%0Ausing%20deep%20generative%20models%20to%20capture%20the%20complexity%20of%20large%20datasets.%20In%0Athis%20survey%2C%20we%20aim%20to%20provide%20a%20unified%20and%20comprehensive%20review%20of%20the%20last%0Ayear%27s%20progress%20in%20the%20use%20of%20deep%20generative%20models%20in%20robotics.%20We%20present%0Athe%20different%20types%20of%20models%20that%20the%20community%20has%20explored%2C%20such%20as%0Aenergy-based%20models%2C%20diffusion%20models%2C%20action%20value%20maps%2C%20or%20generative%0Aadversarial%20networks.%20We%20also%20present%20the%20different%20types%20of%20applications%20in%0Awhich%20deep%20generative%20models%20have%20been%20used%2C%20from%20grasp%20generation%20to%0Atrajectory%20generation%20or%20cost%20learning.%20One%20of%20the%20most%20important%20elements%20of%0Agenerative%20models%20is%20the%20generalization%20out%20of%20distributions.%20In%20our%20survey%2C%20we%0Areview%20the%20different%20decisions%20the%20community%20has%20made%20to%20improve%20the%0Ageneralization%20of%20the%20learned%20models.%20Finally%2C%20we%20highlight%20the%20research%0Achallenges%20and%20propose%20a%20number%20of%20future%20directions%20for%20learning%20deep%0Agenerative%20models%20in%20robotics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04380v1&entry.124074799=Read"},
{"title": "GenAD: Generalized Predictive Model for Autonomous Driving", "author": "Jiazhi Yang and Shenyuan Gao and Yihang Qiu and Li Chen and Tianyu Li and Bo Dai and Kashyap Chitta and Penghao Wu and Jia Zeng and Ping Luo and Jun Zhang and Andreas Geiger and Yu Qiao and Hongyang Li", "abstract": "  In this paper, we introduce the first large-scale video prediction model in\nthe autonomous driving discipline. To eliminate the restriction of high-cost\ndata collection and empower the generalization ability of our model, we acquire\nmassive data from the web and pair it with diverse and high-quality text\ndescriptions. The resultant dataset accumulates over 2000 hours of driving\nvideos, spanning areas all over the world with diverse weather conditions and\ntraffic scenarios. Inheriting the merits from recent latent diffusion models,\nour model, dubbed GenAD, handles the challenging dynamics in driving scenes\nwith novel temporal reasoning blocks. We showcase that it can generalize to\nvarious unseen driving datasets in a zero-shot manner, surpassing general or\ndriving-specific video prediction counterparts. Furthermore, GenAD can be\nadapted into an action-conditioned prediction model or a motion planner,\nholding great potential for real-world driving applications.\n", "link": "http://arxiv.org/abs/2403.09630v2", "date": "2024-08-08", "relevancy": 2.291, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5984}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5672}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenAD%3A%20Generalized%20Predictive%20Model%20for%20Autonomous%20Driving&body=Title%3A%20GenAD%3A%20Generalized%20Predictive%20Model%20for%20Autonomous%20Driving%0AAuthor%3A%20Jiazhi%20Yang%20and%20Shenyuan%20Gao%20and%20Yihang%20Qiu%20and%20Li%20Chen%20and%20Tianyu%20Li%20and%20Bo%20Dai%20and%20Kashyap%20Chitta%20and%20Penghao%20Wu%20and%20Jia%20Zeng%20and%20Ping%20Luo%20and%20Jun%20Zhang%20and%20Andreas%20Geiger%20and%20Yu%20Qiao%20and%20Hongyang%20Li%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20the%20first%20large-scale%20video%20prediction%20model%20in%0Athe%20autonomous%20driving%20discipline.%20To%20eliminate%20the%20restriction%20of%20high-cost%0Adata%20collection%20and%20empower%20the%20generalization%20ability%20of%20our%20model%2C%20we%20acquire%0Amassive%20data%20from%20the%20web%20and%20pair%20it%20with%20diverse%20and%20high-quality%20text%0Adescriptions.%20The%20resultant%20dataset%20accumulates%20over%202000%20hours%20of%20driving%0Avideos%2C%20spanning%20areas%20all%20over%20the%20world%20with%20diverse%20weather%20conditions%20and%0Atraffic%20scenarios.%20Inheriting%20the%20merits%20from%20recent%20latent%20diffusion%20models%2C%0Aour%20model%2C%20dubbed%20GenAD%2C%20handles%20the%20challenging%20dynamics%20in%20driving%20scenes%0Awith%20novel%20temporal%20reasoning%20blocks.%20We%20showcase%20that%20it%20can%20generalize%20to%0Avarious%20unseen%20driving%20datasets%20in%20a%20zero-shot%20manner%2C%20surpassing%20general%20or%0Adriving-specific%20video%20prediction%20counterparts.%20Furthermore%2C%20GenAD%20can%20be%0Aadapted%20into%20an%20action-conditioned%20prediction%20model%20or%20a%20motion%20planner%2C%0Aholding%20great%20potential%20for%20real-world%20driving%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenAD%253A%2520Generalized%2520Predictive%2520Model%2520for%2520Autonomous%2520Driving%26entry.906535625%3DJiazhi%2520Yang%2520and%2520Shenyuan%2520Gao%2520and%2520Yihang%2520Qiu%2520and%2520Li%2520Chen%2520and%2520Tianyu%2520Li%2520and%2520Bo%2520Dai%2520and%2520Kashyap%2520Chitta%2520and%2520Penghao%2520Wu%2520and%2520Jia%2520Zeng%2520and%2520Ping%2520Luo%2520and%2520Jun%2520Zhang%2520and%2520Andreas%2520Geiger%2520and%2520Yu%2520Qiao%2520and%2520Hongyang%2520Li%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520first%2520large-scale%2520video%2520prediction%2520model%2520in%250Athe%2520autonomous%2520driving%2520discipline.%2520To%2520eliminate%2520the%2520restriction%2520of%2520high-cost%250Adata%2520collection%2520and%2520empower%2520the%2520generalization%2520ability%2520of%2520our%2520model%252C%2520we%2520acquire%250Amassive%2520data%2520from%2520the%2520web%2520and%2520pair%2520it%2520with%2520diverse%2520and%2520high-quality%2520text%250Adescriptions.%2520The%2520resultant%2520dataset%2520accumulates%2520over%25202000%2520hours%2520of%2520driving%250Avideos%252C%2520spanning%2520areas%2520all%2520over%2520the%2520world%2520with%2520diverse%2520weather%2520conditions%2520and%250Atraffic%2520scenarios.%2520Inheriting%2520the%2520merits%2520from%2520recent%2520latent%2520diffusion%2520models%252C%250Aour%2520model%252C%2520dubbed%2520GenAD%252C%2520handles%2520the%2520challenging%2520dynamics%2520in%2520driving%2520scenes%250Awith%2520novel%2520temporal%2520reasoning%2520blocks.%2520We%2520showcase%2520that%2520it%2520can%2520generalize%2520to%250Avarious%2520unseen%2520driving%2520datasets%2520in%2520a%2520zero-shot%2520manner%252C%2520surpassing%2520general%2520or%250Adriving-specific%2520video%2520prediction%2520counterparts.%2520Furthermore%252C%2520GenAD%2520can%2520be%250Aadapted%2520into%2520an%2520action-conditioned%2520prediction%2520model%2520or%2520a%2520motion%2520planner%252C%250Aholding%2520great%2520potential%2520for%2520real-world%2520driving%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenAD%3A%20Generalized%20Predictive%20Model%20for%20Autonomous%20Driving&entry.906535625=Jiazhi%20Yang%20and%20Shenyuan%20Gao%20and%20Yihang%20Qiu%20and%20Li%20Chen%20and%20Tianyu%20Li%20and%20Bo%20Dai%20and%20Kashyap%20Chitta%20and%20Penghao%20Wu%20and%20Jia%20Zeng%20and%20Ping%20Luo%20and%20Jun%20Zhang%20and%20Andreas%20Geiger%20and%20Yu%20Qiao%20and%20Hongyang%20Li&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20the%20first%20large-scale%20video%20prediction%20model%20in%0Athe%20autonomous%20driving%20discipline.%20To%20eliminate%20the%20restriction%20of%20high-cost%0Adata%20collection%20and%20empower%20the%20generalization%20ability%20of%20our%20model%2C%20we%20acquire%0Amassive%20data%20from%20the%20web%20and%20pair%20it%20with%20diverse%20and%20high-quality%20text%0Adescriptions.%20The%20resultant%20dataset%20accumulates%20over%202000%20hours%20of%20driving%0Avideos%2C%20spanning%20areas%20all%20over%20the%20world%20with%20diverse%20weather%20conditions%20and%0Atraffic%20scenarios.%20Inheriting%20the%20merits%20from%20recent%20latent%20diffusion%20models%2C%0Aour%20model%2C%20dubbed%20GenAD%2C%20handles%20the%20challenging%20dynamics%20in%20driving%20scenes%0Awith%20novel%20temporal%20reasoning%20blocks.%20We%20showcase%20that%20it%20can%20generalize%20to%0Avarious%20unseen%20driving%20datasets%20in%20a%20zero-shot%20manner%2C%20surpassing%20general%20or%0Adriving-specific%20video%20prediction%20counterparts.%20Furthermore%2C%20GenAD%20can%20be%0Aadapted%20into%20an%20action-conditioned%20prediction%20model%20or%20a%20motion%20planner%2C%0Aholding%20great%20potential%20for%20real-world%20driving%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09630v2&entry.124074799=Read"},
{"title": "SAM2-Adapter: Evaluating & Adapting Segment Anything 2 in Downstream\n  Tasks: Camouflage, Shadow, Medical Image Segmentation, and More", "author": "Tianrun Chen and Ankang Lu and Lanyun Zhu and Chaotao Ding and Chunan Yu and Deyi Ji and Zejian Li and Lingyun Sun and Papa Mao and Ying Zang", "abstract": "  The advent of large models, also known as foundation models, has\nsignificantly transformed the AI research landscape, with models like Segment\nAnything (SAM) achieving notable success in diverse image segmentation\nscenarios. Despite its advancements, SAM encountered limitations in handling\nsome complex low-level segmentation tasks like camouflaged object and medical\nimaging. In response, in 2023, we introduced SAM-Adapter, which demonstrated\nimproved performance on these challenging tasks. Now, with the release of\nSegment Anything 2 (SAM2), a successor with enhanced architecture and a larger\ntraining corpus, we reassess these challenges. This paper introduces\nSAM2-Adapter, the first adapter designed to overcome the persistent limitations\nobserved in SAM2 and achieve new state-of-the-art (SOTA) results in specific\ndownstream tasks including medical image segmentation, camouflaged (concealed)\nobject detection, and shadow detection. SAM2-Adapter builds on the\nSAM-Adapter's strengths, offering enhanced generalizability and composability\nfor diverse applications. We present extensive experimental results\ndemonstrating SAM2-Adapter's effectiveness. We show the potential and encourage\nthe research community to leverage the SAM2 model with our SAM2-Adapter for\nachieving superior segmentation outcomes. Code, pre-trained models, and data\nprocessing protocols are available at\nhttp://tianrun-chen.github.io/SAM-Adaptor/\n", "link": "http://arxiv.org/abs/2408.04579v1", "date": "2024-08-08", "relevancy": 2.2204, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6268}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5156}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM2-Adapter%3A%20Evaluating%20%26%20Adapting%20Segment%20Anything%202%20in%20Downstream%0A%20%20Tasks%3A%20Camouflage%2C%20Shadow%2C%20Medical%20Image%20Segmentation%2C%20and%20More&body=Title%3A%20SAM2-Adapter%3A%20Evaluating%20%26%20Adapting%20Segment%20Anything%202%20in%20Downstream%0A%20%20Tasks%3A%20Camouflage%2C%20Shadow%2C%20Medical%20Image%20Segmentation%2C%20and%20More%0AAuthor%3A%20Tianrun%20Chen%20and%20Ankang%20Lu%20and%20Lanyun%20Zhu%20and%20Chaotao%20Ding%20and%20Chunan%20Yu%20and%20Deyi%20Ji%20and%20Zejian%20Li%20and%20Lingyun%20Sun%20and%20Papa%20Mao%20and%20Ying%20Zang%0AAbstract%3A%20%20%20The%20advent%20of%20large%20models%2C%20also%20known%20as%20foundation%20models%2C%20has%0Asignificantly%20transformed%20the%20AI%20research%20landscape%2C%20with%20models%20like%20Segment%0AAnything%20%28SAM%29%20achieving%20notable%20success%20in%20diverse%20image%20segmentation%0Ascenarios.%20Despite%20its%20advancements%2C%20SAM%20encountered%20limitations%20in%20handling%0Asome%20complex%20low-level%20segmentation%20tasks%20like%20camouflaged%20object%20and%20medical%0Aimaging.%20In%20response%2C%20in%202023%2C%20we%20introduced%20SAM-Adapter%2C%20which%20demonstrated%0Aimproved%20performance%20on%20these%20challenging%20tasks.%20Now%2C%20with%20the%20release%20of%0ASegment%20Anything%202%20%28SAM2%29%2C%20a%20successor%20with%20enhanced%20architecture%20and%20a%20larger%0Atraining%20corpus%2C%20we%20reassess%20these%20challenges.%20This%20paper%20introduces%0ASAM2-Adapter%2C%20the%20first%20adapter%20designed%20to%20overcome%20the%20persistent%20limitations%0Aobserved%20in%20SAM2%20and%20achieve%20new%20state-of-the-art%20%28SOTA%29%20results%20in%20specific%0Adownstream%20tasks%20including%20medical%20image%20segmentation%2C%20camouflaged%20%28concealed%29%0Aobject%20detection%2C%20and%20shadow%20detection.%20SAM2-Adapter%20builds%20on%20the%0ASAM-Adapter%27s%20strengths%2C%20offering%20enhanced%20generalizability%20and%20composability%0Afor%20diverse%20applications.%20We%20present%20extensive%20experimental%20results%0Ademonstrating%20SAM2-Adapter%27s%20effectiveness.%20We%20show%20the%20potential%20and%20encourage%0Athe%20research%20community%20to%20leverage%20the%20SAM2%20model%20with%20our%20SAM2-Adapter%20for%0Aachieving%20superior%20segmentation%20outcomes.%20Code%2C%20pre-trained%20models%2C%20and%20data%0Aprocessing%20protocols%20are%20available%20at%0Ahttp%3A//tianrun-chen.github.io/SAM-Adaptor/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM2-Adapter%253A%2520Evaluating%2520%2526%2520Adapting%2520Segment%2520Anything%25202%2520in%2520Downstream%250A%2520%2520Tasks%253A%2520Camouflage%252C%2520Shadow%252C%2520Medical%2520Image%2520Segmentation%252C%2520and%2520More%26entry.906535625%3DTianrun%2520Chen%2520and%2520Ankang%2520Lu%2520and%2520Lanyun%2520Zhu%2520and%2520Chaotao%2520Ding%2520and%2520Chunan%2520Yu%2520and%2520Deyi%2520Ji%2520and%2520Zejian%2520Li%2520and%2520Lingyun%2520Sun%2520and%2520Papa%2520Mao%2520and%2520Ying%2520Zang%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520large%2520models%252C%2520also%2520known%2520as%2520foundation%2520models%252C%2520has%250Asignificantly%2520transformed%2520the%2520AI%2520research%2520landscape%252C%2520with%2520models%2520like%2520Segment%250AAnything%2520%2528SAM%2529%2520achieving%2520notable%2520success%2520in%2520diverse%2520image%2520segmentation%250Ascenarios.%2520Despite%2520its%2520advancements%252C%2520SAM%2520encountered%2520limitations%2520in%2520handling%250Asome%2520complex%2520low-level%2520segmentation%2520tasks%2520like%2520camouflaged%2520object%2520and%2520medical%250Aimaging.%2520In%2520response%252C%2520in%25202023%252C%2520we%2520introduced%2520SAM-Adapter%252C%2520which%2520demonstrated%250Aimproved%2520performance%2520on%2520these%2520challenging%2520tasks.%2520Now%252C%2520with%2520the%2520release%2520of%250ASegment%2520Anything%25202%2520%2528SAM2%2529%252C%2520a%2520successor%2520with%2520enhanced%2520architecture%2520and%2520a%2520larger%250Atraining%2520corpus%252C%2520we%2520reassess%2520these%2520challenges.%2520This%2520paper%2520introduces%250ASAM2-Adapter%252C%2520the%2520first%2520adapter%2520designed%2520to%2520overcome%2520the%2520persistent%2520limitations%250Aobserved%2520in%2520SAM2%2520and%2520achieve%2520new%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520in%2520specific%250Adownstream%2520tasks%2520including%2520medical%2520image%2520segmentation%252C%2520camouflaged%2520%2528concealed%2529%250Aobject%2520detection%252C%2520and%2520shadow%2520detection.%2520SAM2-Adapter%2520builds%2520on%2520the%250ASAM-Adapter%2527s%2520strengths%252C%2520offering%2520enhanced%2520generalizability%2520and%2520composability%250Afor%2520diverse%2520applications.%2520We%2520present%2520extensive%2520experimental%2520results%250Ademonstrating%2520SAM2-Adapter%2527s%2520effectiveness.%2520We%2520show%2520the%2520potential%2520and%2520encourage%250Athe%2520research%2520community%2520to%2520leverage%2520the%2520SAM2%2520model%2520with%2520our%2520SAM2-Adapter%2520for%250Aachieving%2520superior%2520segmentation%2520outcomes.%2520Code%252C%2520pre-trained%2520models%252C%2520and%2520data%250Aprocessing%2520protocols%2520are%2520available%2520at%250Ahttp%253A//tianrun-chen.github.io/SAM-Adaptor/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM2-Adapter%3A%20Evaluating%20%26%20Adapting%20Segment%20Anything%202%20in%20Downstream%0A%20%20Tasks%3A%20Camouflage%2C%20Shadow%2C%20Medical%20Image%20Segmentation%2C%20and%20More&entry.906535625=Tianrun%20Chen%20and%20Ankang%20Lu%20and%20Lanyun%20Zhu%20and%20Chaotao%20Ding%20and%20Chunan%20Yu%20and%20Deyi%20Ji%20and%20Zejian%20Li%20and%20Lingyun%20Sun%20and%20Papa%20Mao%20and%20Ying%20Zang&entry.1292438233=%20%20The%20advent%20of%20large%20models%2C%20also%20known%20as%20foundation%20models%2C%20has%0Asignificantly%20transformed%20the%20AI%20research%20landscape%2C%20with%20models%20like%20Segment%0AAnything%20%28SAM%29%20achieving%20notable%20success%20in%20diverse%20image%20segmentation%0Ascenarios.%20Despite%20its%20advancements%2C%20SAM%20encountered%20limitations%20in%20handling%0Asome%20complex%20low-level%20segmentation%20tasks%20like%20camouflaged%20object%20and%20medical%0Aimaging.%20In%20response%2C%20in%202023%2C%20we%20introduced%20SAM-Adapter%2C%20which%20demonstrated%0Aimproved%20performance%20on%20these%20challenging%20tasks.%20Now%2C%20with%20the%20release%20of%0ASegment%20Anything%202%20%28SAM2%29%2C%20a%20successor%20with%20enhanced%20architecture%20and%20a%20larger%0Atraining%20corpus%2C%20we%20reassess%20these%20challenges.%20This%20paper%20introduces%0ASAM2-Adapter%2C%20the%20first%20adapter%20designed%20to%20overcome%20the%20persistent%20limitations%0Aobserved%20in%20SAM2%20and%20achieve%20new%20state-of-the-art%20%28SOTA%29%20results%20in%20specific%0Adownstream%20tasks%20including%20medical%20image%20segmentation%2C%20camouflaged%20%28concealed%29%0Aobject%20detection%2C%20and%20shadow%20detection.%20SAM2-Adapter%20builds%20on%20the%0ASAM-Adapter%27s%20strengths%2C%20offering%20enhanced%20generalizability%20and%20composability%0Afor%20diverse%20applications.%20We%20present%20extensive%20experimental%20results%0Ademonstrating%20SAM2-Adapter%27s%20effectiveness.%20We%20show%20the%20potential%20and%20encourage%0Athe%20research%20community%20to%20leverage%20the%20SAM2%20model%20with%20our%20SAM2-Adapter%20for%0Aachieving%20superior%20segmentation%20outcomes.%20Code%2C%20pre-trained%20models%2C%20and%20data%0Aprocessing%20protocols%20are%20available%20at%0Ahttp%3A//tianrun-chen.github.io/SAM-Adaptor/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04579v1&entry.124074799=Read"},
{"title": "Dual-View Data Hallucination with Semantic Relation Guidance for\n  Few-Shot Image Recognition", "author": "Hefeng Wu and Guangzhi Ye and Ziyang Zhou and Ling Tian and Qing Wang and Liang Lin", "abstract": "  Learning to recognize novel concepts from just a few image samples is very\nchallenging as the learned model is easily overfitted on the few data and\nresults in poor generalizability. One promising but underexplored solution is\nto compensate the novel classes by generating plausible samples. However, most\nexisting works of this line exploit visual information only, rendering the\ngenerated data easy to be distracted by some challenging factors contained in\nthe few available samples. Being aware of the semantic information in the\ntextual modality that reflects human concepts, this work proposes a novel\nframework that exploits semantic relations to guide dual-view data\nhallucination for few-shot image recognition. The proposed framework enables\ngenerating more diverse and reasonable data samples for novel classes through\neffective information transfer from base classes. Specifically, an\ninstance-view data hallucination module hallucinates each sample of a novel\nclass to generate new data by employing local semantic correlated attention and\nglobal semantic feature fusion derived from base classes. Meanwhile, a\nprototype-view data hallucination module exploits semantic-aware measure to\nestimate the prototype of a novel class and the associated distribution from\nthe few samples, which thereby harvests the prototype as a more stable sample\nand enables resampling a large number of samples. We conduct extensive\nexperiments and comparisons with state-of-the-art methods on several popular\nfew-shot benchmarks to verify the effectiveness of the proposed framework.\n", "link": "http://arxiv.org/abs/2401.07061v2", "date": "2024-08-08", "relevancy": 2.2081, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5559}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5532}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-View%20Data%20Hallucination%20with%20Semantic%20Relation%20Guidance%20for%0A%20%20Few-Shot%20Image%20Recognition&body=Title%3A%20Dual-View%20Data%20Hallucination%20with%20Semantic%20Relation%20Guidance%20for%0A%20%20Few-Shot%20Image%20Recognition%0AAuthor%3A%20Hefeng%20Wu%20and%20Guangzhi%20Ye%20and%20Ziyang%20Zhou%20and%20Ling%20Tian%20and%20Qing%20Wang%20and%20Liang%20Lin%0AAbstract%3A%20%20%20Learning%20to%20recognize%20novel%20concepts%20from%20just%20a%20few%20image%20samples%20is%20very%0Achallenging%20as%20the%20learned%20model%20is%20easily%20overfitted%20on%20the%20few%20data%20and%0Aresults%20in%20poor%20generalizability.%20One%20promising%20but%20underexplored%20solution%20is%0Ato%20compensate%20the%20novel%20classes%20by%20generating%20plausible%20samples.%20However%2C%20most%0Aexisting%20works%20of%20this%20line%20exploit%20visual%20information%20only%2C%20rendering%20the%0Agenerated%20data%20easy%20to%20be%20distracted%20by%20some%20challenging%20factors%20contained%20in%0Athe%20few%20available%20samples.%20Being%20aware%20of%20the%20semantic%20information%20in%20the%0Atextual%20modality%20that%20reflects%20human%20concepts%2C%20this%20work%20proposes%20a%20novel%0Aframework%20that%20exploits%20semantic%20relations%20to%20guide%20dual-view%20data%0Ahallucination%20for%20few-shot%20image%20recognition.%20The%20proposed%20framework%20enables%0Agenerating%20more%20diverse%20and%20reasonable%20data%20samples%20for%20novel%20classes%20through%0Aeffective%20information%20transfer%20from%20base%20classes.%20Specifically%2C%20an%0Ainstance-view%20data%20hallucination%20module%20hallucinates%20each%20sample%20of%20a%20novel%0Aclass%20to%20generate%20new%20data%20by%20employing%20local%20semantic%20correlated%20attention%20and%0Aglobal%20semantic%20feature%20fusion%20derived%20from%20base%20classes.%20Meanwhile%2C%20a%0Aprototype-view%20data%20hallucination%20module%20exploits%20semantic-aware%20measure%20to%0Aestimate%20the%20prototype%20of%20a%20novel%20class%20and%20the%20associated%20distribution%20from%0Athe%20few%20samples%2C%20which%20thereby%20harvests%20the%20prototype%20as%20a%20more%20stable%20sample%0Aand%20enables%20resampling%20a%20large%20number%20of%20samples.%20We%20conduct%20extensive%0Aexperiments%20and%20comparisons%20with%20state-of-the-art%20methods%20on%20several%20popular%0Afew-shot%20benchmarks%20to%20verify%20the%20effectiveness%20of%20the%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.07061v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-View%2520Data%2520Hallucination%2520with%2520Semantic%2520Relation%2520Guidance%2520for%250A%2520%2520Few-Shot%2520Image%2520Recognition%26entry.906535625%3DHefeng%2520Wu%2520and%2520Guangzhi%2520Ye%2520and%2520Ziyang%2520Zhou%2520and%2520Ling%2520Tian%2520and%2520Qing%2520Wang%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520Learning%2520to%2520recognize%2520novel%2520concepts%2520from%2520just%2520a%2520few%2520image%2520samples%2520is%2520very%250Achallenging%2520as%2520the%2520learned%2520model%2520is%2520easily%2520overfitted%2520on%2520the%2520few%2520data%2520and%250Aresults%2520in%2520poor%2520generalizability.%2520One%2520promising%2520but%2520underexplored%2520solution%2520is%250Ato%2520compensate%2520the%2520novel%2520classes%2520by%2520generating%2520plausible%2520samples.%2520However%252C%2520most%250Aexisting%2520works%2520of%2520this%2520line%2520exploit%2520visual%2520information%2520only%252C%2520rendering%2520the%250Agenerated%2520data%2520easy%2520to%2520be%2520distracted%2520by%2520some%2520challenging%2520factors%2520contained%2520in%250Athe%2520few%2520available%2520samples.%2520Being%2520aware%2520of%2520the%2520semantic%2520information%2520in%2520the%250Atextual%2520modality%2520that%2520reflects%2520human%2520concepts%252C%2520this%2520work%2520proposes%2520a%2520novel%250Aframework%2520that%2520exploits%2520semantic%2520relations%2520to%2520guide%2520dual-view%2520data%250Ahallucination%2520for%2520few-shot%2520image%2520recognition.%2520The%2520proposed%2520framework%2520enables%250Agenerating%2520more%2520diverse%2520and%2520reasonable%2520data%2520samples%2520for%2520novel%2520classes%2520through%250Aeffective%2520information%2520transfer%2520from%2520base%2520classes.%2520Specifically%252C%2520an%250Ainstance-view%2520data%2520hallucination%2520module%2520hallucinates%2520each%2520sample%2520of%2520a%2520novel%250Aclass%2520to%2520generate%2520new%2520data%2520by%2520employing%2520local%2520semantic%2520correlated%2520attention%2520and%250Aglobal%2520semantic%2520feature%2520fusion%2520derived%2520from%2520base%2520classes.%2520Meanwhile%252C%2520a%250Aprototype-view%2520data%2520hallucination%2520module%2520exploits%2520semantic-aware%2520measure%2520to%250Aestimate%2520the%2520prototype%2520of%2520a%2520novel%2520class%2520and%2520the%2520associated%2520distribution%2520from%250Athe%2520few%2520samples%252C%2520which%2520thereby%2520harvests%2520the%2520prototype%2520as%2520a%2520more%2520stable%2520sample%250Aand%2520enables%2520resampling%2520a%2520large%2520number%2520of%2520samples.%2520We%2520conduct%2520extensive%250Aexperiments%2520and%2520comparisons%2520with%2520state-of-the-art%2520methods%2520on%2520several%2520popular%250Afew-shot%2520benchmarks%2520to%2520verify%2520the%2520effectiveness%2520of%2520the%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.07061v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-View%20Data%20Hallucination%20with%20Semantic%20Relation%20Guidance%20for%0A%20%20Few-Shot%20Image%20Recognition&entry.906535625=Hefeng%20Wu%20and%20Guangzhi%20Ye%20and%20Ziyang%20Zhou%20and%20Ling%20Tian%20and%20Qing%20Wang%20and%20Liang%20Lin&entry.1292438233=%20%20Learning%20to%20recognize%20novel%20concepts%20from%20just%20a%20few%20image%20samples%20is%20very%0Achallenging%20as%20the%20learned%20model%20is%20easily%20overfitted%20on%20the%20few%20data%20and%0Aresults%20in%20poor%20generalizability.%20One%20promising%20but%20underexplored%20solution%20is%0Ato%20compensate%20the%20novel%20classes%20by%20generating%20plausible%20samples.%20However%2C%20most%0Aexisting%20works%20of%20this%20line%20exploit%20visual%20information%20only%2C%20rendering%20the%0Agenerated%20data%20easy%20to%20be%20distracted%20by%20some%20challenging%20factors%20contained%20in%0Athe%20few%20available%20samples.%20Being%20aware%20of%20the%20semantic%20information%20in%20the%0Atextual%20modality%20that%20reflects%20human%20concepts%2C%20this%20work%20proposes%20a%20novel%0Aframework%20that%20exploits%20semantic%20relations%20to%20guide%20dual-view%20data%0Ahallucination%20for%20few-shot%20image%20recognition.%20The%20proposed%20framework%20enables%0Agenerating%20more%20diverse%20and%20reasonable%20data%20samples%20for%20novel%20classes%20through%0Aeffective%20information%20transfer%20from%20base%20classes.%20Specifically%2C%20an%0Ainstance-view%20data%20hallucination%20module%20hallucinates%20each%20sample%20of%20a%20novel%0Aclass%20to%20generate%20new%20data%20by%20employing%20local%20semantic%20correlated%20attention%20and%0Aglobal%20semantic%20feature%20fusion%20derived%20from%20base%20classes.%20Meanwhile%2C%20a%0Aprototype-view%20data%20hallucination%20module%20exploits%20semantic-aware%20measure%20to%0Aestimate%20the%20prototype%20of%20a%20novel%20class%20and%20the%20associated%20distribution%20from%0Athe%20few%20samples%2C%20which%20thereby%20harvests%20the%20prototype%20as%20a%20more%20stable%20sample%0Aand%20enables%20resampling%20a%20large%20number%20of%20samples.%20We%20conduct%20extensive%0Aexperiments%20and%20comparisons%20with%20state-of-the-art%20methods%20on%20several%20popular%0Afew-shot%20benchmarks%20to%20verify%20the%20effectiveness%20of%20the%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.07061v2&entry.124074799=Read"},
{"title": "Study of detecting behavioral signatures within DeepFake videos", "author": "Qiaomu Miao and Sinhwa Kang and Stacy Marsella and Steve DiPaola and Chao Wang and Ari Shapiro", "abstract": "  There is strong interest in the generation of synthetic video imagery of\npeople talking for various purposes, including entertainment, communication,\ntraining, and advertisement. With the development of deep fake generation\nmodels, synthetic video imagery will soon be visually indistinguishable to the\nnaked eye from a naturally capture video. In addition, many methods are\ncontinuing to improve to avoid more careful, forensic visual analysis. Some\ndeep fake videos are produced through the use of facial puppetry, which\ndirectly controls the head and face of the synthetic image through the\nmovements of the actor, allow the actor to 'puppet' the image of another. In\nthis paper, we address the question of whether one person's movements can be\ndistinguished from the original speaker by controlling the visual appearance of\nthe speaker but transferring the behavior signals from another source. We\nconduct a study by comparing synthetic imagery that: 1) originates from a\ndifferent person speaking a different utterance, 2) originates from the same\nperson speaking a different utterance, and 3) originates from a different\nperson speaking the same utterance. Our study shows that synthetic videos in\nall three cases are seen as less real and less engaging than the original\nsource video. Our results indicate that there could be a behavioral signature\nthat is detectable from a person's movements that is separate from their visual\nappearance, and that this behavioral signature could be used to distinguish a\ndeep fake from a properly captured video.\n", "link": "http://arxiv.org/abs/2208.03561v2", "date": "2024-08-08", "relevancy": 2.2059, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5761}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5551}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Study%20of%20detecting%20behavioral%20signatures%20within%20DeepFake%20videos&body=Title%3A%20Study%20of%20detecting%20behavioral%20signatures%20within%20DeepFake%20videos%0AAuthor%3A%20Qiaomu%20Miao%20and%20Sinhwa%20Kang%20and%20Stacy%20Marsella%20and%20Steve%20DiPaola%20and%20Chao%20Wang%20and%20Ari%20Shapiro%0AAbstract%3A%20%20%20There%20is%20strong%20interest%20in%20the%20generation%20of%20synthetic%20video%20imagery%20of%0Apeople%20talking%20for%20various%20purposes%2C%20including%20entertainment%2C%20communication%2C%0Atraining%2C%20and%20advertisement.%20With%20the%20development%20of%20deep%20fake%20generation%0Amodels%2C%20synthetic%20video%20imagery%20will%20soon%20be%20visually%20indistinguishable%20to%20the%0Anaked%20eye%20from%20a%20naturally%20capture%20video.%20In%20addition%2C%20many%20methods%20are%0Acontinuing%20to%20improve%20to%20avoid%20more%20careful%2C%20forensic%20visual%20analysis.%20Some%0Adeep%20fake%20videos%20are%20produced%20through%20the%20use%20of%20facial%20puppetry%2C%20which%0Adirectly%20controls%20the%20head%20and%20face%20of%20the%20synthetic%20image%20through%20the%0Amovements%20of%20the%20actor%2C%20allow%20the%20actor%20to%20%27puppet%27%20the%20image%20of%20another.%20In%0Athis%20paper%2C%20we%20address%20the%20question%20of%20whether%20one%20person%27s%20movements%20can%20be%0Adistinguished%20from%20the%20original%20speaker%20by%20controlling%20the%20visual%20appearance%20of%0Athe%20speaker%20but%20transferring%20the%20behavior%20signals%20from%20another%20source.%20We%0Aconduct%20a%20study%20by%20comparing%20synthetic%20imagery%20that%3A%201%29%20originates%20from%20a%0Adifferent%20person%20speaking%20a%20different%20utterance%2C%202%29%20originates%20from%20the%20same%0Aperson%20speaking%20a%20different%20utterance%2C%20and%203%29%20originates%20from%20a%20different%0Aperson%20speaking%20the%20same%20utterance.%20Our%20study%20shows%20that%20synthetic%20videos%20in%0Aall%20three%20cases%20are%20seen%20as%20less%20real%20and%20less%20engaging%20than%20the%20original%0Asource%20video.%20Our%20results%20indicate%20that%20there%20could%20be%20a%20behavioral%20signature%0Athat%20is%20detectable%20from%20a%20person%27s%20movements%20that%20is%20separate%20from%20their%20visual%0Aappearance%2C%20and%20that%20this%20behavioral%20signature%20could%20be%20used%20to%20distinguish%20a%0Adeep%20fake%20from%20a%20properly%20captured%20video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.03561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStudy%2520of%2520detecting%2520behavioral%2520signatures%2520within%2520DeepFake%2520videos%26entry.906535625%3DQiaomu%2520Miao%2520and%2520Sinhwa%2520Kang%2520and%2520Stacy%2520Marsella%2520and%2520Steve%2520DiPaola%2520and%2520Chao%2520Wang%2520and%2520Ari%2520Shapiro%26entry.1292438233%3D%2520%2520There%2520is%2520strong%2520interest%2520in%2520the%2520generation%2520of%2520synthetic%2520video%2520imagery%2520of%250Apeople%2520talking%2520for%2520various%2520purposes%252C%2520including%2520entertainment%252C%2520communication%252C%250Atraining%252C%2520and%2520advertisement.%2520With%2520the%2520development%2520of%2520deep%2520fake%2520generation%250Amodels%252C%2520synthetic%2520video%2520imagery%2520will%2520soon%2520be%2520visually%2520indistinguishable%2520to%2520the%250Anaked%2520eye%2520from%2520a%2520naturally%2520capture%2520video.%2520In%2520addition%252C%2520many%2520methods%2520are%250Acontinuing%2520to%2520improve%2520to%2520avoid%2520more%2520careful%252C%2520forensic%2520visual%2520analysis.%2520Some%250Adeep%2520fake%2520videos%2520are%2520produced%2520through%2520the%2520use%2520of%2520facial%2520puppetry%252C%2520which%250Adirectly%2520controls%2520the%2520head%2520and%2520face%2520of%2520the%2520synthetic%2520image%2520through%2520the%250Amovements%2520of%2520the%2520actor%252C%2520allow%2520the%2520actor%2520to%2520%2527puppet%2527%2520the%2520image%2520of%2520another.%2520In%250Athis%2520paper%252C%2520we%2520address%2520the%2520question%2520of%2520whether%2520one%2520person%2527s%2520movements%2520can%2520be%250Adistinguished%2520from%2520the%2520original%2520speaker%2520by%2520controlling%2520the%2520visual%2520appearance%2520of%250Athe%2520speaker%2520but%2520transferring%2520the%2520behavior%2520signals%2520from%2520another%2520source.%2520We%250Aconduct%2520a%2520study%2520by%2520comparing%2520synthetic%2520imagery%2520that%253A%25201%2529%2520originates%2520from%2520a%250Adifferent%2520person%2520speaking%2520a%2520different%2520utterance%252C%25202%2529%2520originates%2520from%2520the%2520same%250Aperson%2520speaking%2520a%2520different%2520utterance%252C%2520and%25203%2529%2520originates%2520from%2520a%2520different%250Aperson%2520speaking%2520the%2520same%2520utterance.%2520Our%2520study%2520shows%2520that%2520synthetic%2520videos%2520in%250Aall%2520three%2520cases%2520are%2520seen%2520as%2520less%2520real%2520and%2520less%2520engaging%2520than%2520the%2520original%250Asource%2520video.%2520Our%2520results%2520indicate%2520that%2520there%2520could%2520be%2520a%2520behavioral%2520signature%250Athat%2520is%2520detectable%2520from%2520a%2520person%2527s%2520movements%2520that%2520is%2520separate%2520from%2520their%2520visual%250Aappearance%252C%2520and%2520that%2520this%2520behavioral%2520signature%2520could%2520be%2520used%2520to%2520distinguish%2520a%250Adeep%2520fake%2520from%2520a%2520properly%2520captured%2520video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.03561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Study%20of%20detecting%20behavioral%20signatures%20within%20DeepFake%20videos&entry.906535625=Qiaomu%20Miao%20and%20Sinhwa%20Kang%20and%20Stacy%20Marsella%20and%20Steve%20DiPaola%20and%20Chao%20Wang%20and%20Ari%20Shapiro&entry.1292438233=%20%20There%20is%20strong%20interest%20in%20the%20generation%20of%20synthetic%20video%20imagery%20of%0Apeople%20talking%20for%20various%20purposes%2C%20including%20entertainment%2C%20communication%2C%0Atraining%2C%20and%20advertisement.%20With%20the%20development%20of%20deep%20fake%20generation%0Amodels%2C%20synthetic%20video%20imagery%20will%20soon%20be%20visually%20indistinguishable%20to%20the%0Anaked%20eye%20from%20a%20naturally%20capture%20video.%20In%20addition%2C%20many%20methods%20are%0Acontinuing%20to%20improve%20to%20avoid%20more%20careful%2C%20forensic%20visual%20analysis.%20Some%0Adeep%20fake%20videos%20are%20produced%20through%20the%20use%20of%20facial%20puppetry%2C%20which%0Adirectly%20controls%20the%20head%20and%20face%20of%20the%20synthetic%20image%20through%20the%0Amovements%20of%20the%20actor%2C%20allow%20the%20actor%20to%20%27puppet%27%20the%20image%20of%20another.%20In%0Athis%20paper%2C%20we%20address%20the%20question%20of%20whether%20one%20person%27s%20movements%20can%20be%0Adistinguished%20from%20the%20original%20speaker%20by%20controlling%20the%20visual%20appearance%20of%0Athe%20speaker%20but%20transferring%20the%20behavior%20signals%20from%20another%20source.%20We%0Aconduct%20a%20study%20by%20comparing%20synthetic%20imagery%20that%3A%201%29%20originates%20from%20a%0Adifferent%20person%20speaking%20a%20different%20utterance%2C%202%29%20originates%20from%20the%20same%0Aperson%20speaking%20a%20different%20utterance%2C%20and%203%29%20originates%20from%20a%20different%0Aperson%20speaking%20the%20same%20utterance.%20Our%20study%20shows%20that%20synthetic%20videos%20in%0Aall%20three%20cases%20are%20seen%20as%20less%20real%20and%20less%20engaging%20than%20the%20original%0Asource%20video.%20Our%20results%20indicate%20that%20there%20could%20be%20a%20behavioral%20signature%0Athat%20is%20detectable%20from%20a%20person%27s%20movements%20that%20is%20separate%20from%20their%20visual%0Aappearance%2C%20and%20that%20this%20behavioral%20signature%20could%20be%20used%20to%20distinguish%20a%0Adeep%20fake%20from%20a%20properly%20captured%20video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.03561v2&entry.124074799=Read"},
{"title": "Graph Matching via convex relaxation to the simplex", "author": "Ernesto Araya Valdivia and Hemant Tyagi", "abstract": "  This paper addresses the Graph Matching problem, which consists of finding\nthe best possible alignment between two input graphs, and has many applications\nin computer vision, network deanonymization and protein alignment. A common\napproach to tackle this problem is through convex relaxations of the NP-hard\n\\emph{Quadratic Assignment Problem} (QAP).\n  Here, we introduce a new convex relaxation onto the unit simplex and develop\nan efficient mirror descent scheme with closed-form iterations for solving this\nproblem. Under the correlated Gaussian Wigner model, we show that the simplex\nrelaxation admits a unique solution with high probability. In the noiseless\ncase, this is shown to imply exact recovery of the ground truth permutation.\nAdditionally, we establish a novel sufficiency condition for the input matrix\nin standard greedy rounding methods, which is less restrictive than the\ncommonly used `diagonal dominance' condition. We use this condition to show\nexact one-step recovery of the ground truth (holding almost surely) via the\nmirror descent scheme, in the noiseless setting. We also use this condition to\nobtain significantly improved conditions for the GRAMPA algorithm [Fan et al.\n2019] in the noiseless setting.\n", "link": "http://arxiv.org/abs/2310.20609v2", "date": "2024-08-08", "relevancy": 2.1954, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4449}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4431}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Matching%20via%20convex%20relaxation%20to%20the%20simplex&body=Title%3A%20Graph%20Matching%20via%20convex%20relaxation%20to%20the%20simplex%0AAuthor%3A%20Ernesto%20Araya%20Valdivia%20and%20Hemant%20Tyagi%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20Graph%20Matching%20problem%2C%20which%20consists%20of%20finding%0Athe%20best%20possible%20alignment%20between%20two%20input%20graphs%2C%20and%20has%20many%20applications%0Ain%20computer%20vision%2C%20network%20deanonymization%20and%20protein%20alignment.%20A%20common%0Aapproach%20to%20tackle%20this%20problem%20is%20through%20convex%20relaxations%20of%20the%20NP-hard%0A%5Cemph%7BQuadratic%20Assignment%20Problem%7D%20%28QAP%29.%0A%20%20Here%2C%20we%20introduce%20a%20new%20convex%20relaxation%20onto%20the%20unit%20simplex%20and%20develop%0Aan%20efficient%20mirror%20descent%20scheme%20with%20closed-form%20iterations%20for%20solving%20this%0Aproblem.%20Under%20the%20correlated%20Gaussian%20Wigner%20model%2C%20we%20show%20that%20the%20simplex%0Arelaxation%20admits%20a%20unique%20solution%20with%20high%20probability.%20In%20the%20noiseless%0Acase%2C%20this%20is%20shown%20to%20imply%20exact%20recovery%20of%20the%20ground%20truth%20permutation.%0AAdditionally%2C%20we%20establish%20a%20novel%20sufficiency%20condition%20for%20the%20input%20matrix%0Ain%20standard%20greedy%20rounding%20methods%2C%20which%20is%20less%20restrictive%20than%20the%0Acommonly%20used%20%60diagonal%20dominance%27%20condition.%20We%20use%20this%20condition%20to%20show%0Aexact%20one-step%20recovery%20of%20the%20ground%20truth%20%28holding%20almost%20surely%29%20via%20the%0Amirror%20descent%20scheme%2C%20in%20the%20noiseless%20setting.%20We%20also%20use%20this%20condition%20to%0Aobtain%20significantly%20improved%20conditions%20for%20the%20GRAMPA%20algorithm%20%5BFan%20et%20al.%0A2019%5D%20in%20the%20noiseless%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20609v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Matching%2520via%2520convex%2520relaxation%2520to%2520the%2520simplex%26entry.906535625%3DErnesto%2520Araya%2520Valdivia%2520and%2520Hemant%2520Tyagi%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520Graph%2520Matching%2520problem%252C%2520which%2520consists%2520of%2520finding%250Athe%2520best%2520possible%2520alignment%2520between%2520two%2520input%2520graphs%252C%2520and%2520has%2520many%2520applications%250Ain%2520computer%2520vision%252C%2520network%2520deanonymization%2520and%2520protein%2520alignment.%2520A%2520common%250Aapproach%2520to%2520tackle%2520this%2520problem%2520is%2520through%2520convex%2520relaxations%2520of%2520the%2520NP-hard%250A%255Cemph%257BQuadratic%2520Assignment%2520Problem%257D%2520%2528QAP%2529.%250A%2520%2520Here%252C%2520we%2520introduce%2520a%2520new%2520convex%2520relaxation%2520onto%2520the%2520unit%2520simplex%2520and%2520develop%250Aan%2520efficient%2520mirror%2520descent%2520scheme%2520with%2520closed-form%2520iterations%2520for%2520solving%2520this%250Aproblem.%2520Under%2520the%2520correlated%2520Gaussian%2520Wigner%2520model%252C%2520we%2520show%2520that%2520the%2520simplex%250Arelaxation%2520admits%2520a%2520unique%2520solution%2520with%2520high%2520probability.%2520In%2520the%2520noiseless%250Acase%252C%2520this%2520is%2520shown%2520to%2520imply%2520exact%2520recovery%2520of%2520the%2520ground%2520truth%2520permutation.%250AAdditionally%252C%2520we%2520establish%2520a%2520novel%2520sufficiency%2520condition%2520for%2520the%2520input%2520matrix%250Ain%2520standard%2520greedy%2520rounding%2520methods%252C%2520which%2520is%2520less%2520restrictive%2520than%2520the%250Acommonly%2520used%2520%2560diagonal%2520dominance%2527%2520condition.%2520We%2520use%2520this%2520condition%2520to%2520show%250Aexact%2520one-step%2520recovery%2520of%2520the%2520ground%2520truth%2520%2528holding%2520almost%2520surely%2529%2520via%2520the%250Amirror%2520descent%2520scheme%252C%2520in%2520the%2520noiseless%2520setting.%2520We%2520also%2520use%2520this%2520condition%2520to%250Aobtain%2520significantly%2520improved%2520conditions%2520for%2520the%2520GRAMPA%2520algorithm%2520%255BFan%2520et%2520al.%250A2019%255D%2520in%2520the%2520noiseless%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.20609v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Matching%20via%20convex%20relaxation%20to%20the%20simplex&entry.906535625=Ernesto%20Araya%20Valdivia%20and%20Hemant%20Tyagi&entry.1292438233=%20%20This%20paper%20addresses%20the%20Graph%20Matching%20problem%2C%20which%20consists%20of%20finding%0Athe%20best%20possible%20alignment%20between%20two%20input%20graphs%2C%20and%20has%20many%20applications%0Ain%20computer%20vision%2C%20network%20deanonymization%20and%20protein%20alignment.%20A%20common%0Aapproach%20to%20tackle%20this%20problem%20is%20through%20convex%20relaxations%20of%20the%20NP-hard%0A%5Cemph%7BQuadratic%20Assignment%20Problem%7D%20%28QAP%29.%0A%20%20Here%2C%20we%20introduce%20a%20new%20convex%20relaxation%20onto%20the%20unit%20simplex%20and%20develop%0Aan%20efficient%20mirror%20descent%20scheme%20with%20closed-form%20iterations%20for%20solving%20this%0Aproblem.%20Under%20the%20correlated%20Gaussian%20Wigner%20model%2C%20we%20show%20that%20the%20simplex%0Arelaxation%20admits%20a%20unique%20solution%20with%20high%20probability.%20In%20the%20noiseless%0Acase%2C%20this%20is%20shown%20to%20imply%20exact%20recovery%20of%20the%20ground%20truth%20permutation.%0AAdditionally%2C%20we%20establish%20a%20novel%20sufficiency%20condition%20for%20the%20input%20matrix%0Ain%20standard%20greedy%20rounding%20methods%2C%20which%20is%20less%20restrictive%20than%20the%0Acommonly%20used%20%60diagonal%20dominance%27%20condition.%20We%20use%20this%20condition%20to%20show%0Aexact%20one-step%20recovery%20of%20the%20ground%20truth%20%28holding%20almost%20surely%29%20via%20the%0Amirror%20descent%20scheme%2C%20in%20the%20noiseless%20setting.%20We%20also%20use%20this%20condition%20to%0Aobtain%20significantly%20improved%20conditions%20for%20the%20GRAMPA%20algorithm%20%5BFan%20et%20al.%0A2019%5D%20in%20the%20noiseless%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20609v2&entry.124074799=Read"},
{"title": "Transformer Explainer: Interactive Learning of Text-Generative Models", "author": "Aeree Cho and Grace C. Kim and Alexander Karpekov and Alec Helbling and Zijie J. Wang and Seongmin Lee and Benjamin Hoover and Duen Horng Chau", "abstract": "  Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.\n", "link": "http://arxiv.org/abs/2408.04619v1", "date": "2024-08-08", "relevancy": 2.1821, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.582}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5545}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformer%20Explainer%3A%20Interactive%20Learning%20of%20Text-Generative%20Models&body=Title%3A%20Transformer%20Explainer%3A%20Interactive%20Learning%20of%20Text-Generative%20Models%0AAuthor%3A%20Aeree%20Cho%20and%20Grace%20C.%20Kim%20and%20Alexander%20Karpekov%20and%20Alec%20Helbling%20and%20Zijie%20J.%20Wang%20and%20Seongmin%20Lee%20and%20Benjamin%20Hoover%20and%20Duen%20Horng%20Chau%0AAbstract%3A%20%20%20Transformers%20have%20revolutionized%20machine%20learning%2C%20yet%20their%20inner%20workings%0Aremain%20opaque%20to%20many.%20We%20present%20Transformer%20Explainer%2C%20an%20interactive%0Avisualization%20tool%20designed%20for%20non-experts%20to%20learn%20about%20Transformers%20through%0Athe%20GPT-2%20model.%20Our%20tool%20helps%20users%20understand%20complex%20Transformer%20concepts%0Aby%20integrating%20a%20model%20overview%20and%20enabling%20smooth%20transitions%20across%0Aabstraction%20levels%20of%20mathematical%20operations%20and%20model%20structures.%20It%20runs%20a%0Alive%20GPT-2%20instance%20locally%20in%20the%20user%27s%20browser%2C%20empowering%20users%20to%0Aexperiment%20with%20their%20own%20input%20and%20observe%20in%20real-time%20how%20the%20internal%0Acomponents%20and%20parameters%20of%20the%20Transformer%20work%20together%20to%20predict%20the%20next%0Atokens.%20Our%20tool%20requires%20no%20installation%20or%20special%20hardware%2C%20broadening%20the%0Apublic%27s%20education%20access%20to%20modern%20generative%20AI%20techniques.%20Our%20open-sourced%0Atool%20is%20available%20at%20https%3A//poloclub.github.io/transformer-explainer/.%20A%20video%0Ademo%20is%20available%20at%20https%3A//youtu.be/ECR4oAwocjs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformer%2520Explainer%253A%2520Interactive%2520Learning%2520of%2520Text-Generative%2520Models%26entry.906535625%3DAeree%2520Cho%2520and%2520Grace%2520C.%2520Kim%2520and%2520Alexander%2520Karpekov%2520and%2520Alec%2520Helbling%2520and%2520Zijie%2520J.%2520Wang%2520and%2520Seongmin%2520Lee%2520and%2520Benjamin%2520Hoover%2520and%2520Duen%2520Horng%2520Chau%26entry.1292438233%3D%2520%2520Transformers%2520have%2520revolutionized%2520machine%2520learning%252C%2520yet%2520their%2520inner%2520workings%250Aremain%2520opaque%2520to%2520many.%2520We%2520present%2520Transformer%2520Explainer%252C%2520an%2520interactive%250Avisualization%2520tool%2520designed%2520for%2520non-experts%2520to%2520learn%2520about%2520Transformers%2520through%250Athe%2520GPT-2%2520model.%2520Our%2520tool%2520helps%2520users%2520understand%2520complex%2520Transformer%2520concepts%250Aby%2520integrating%2520a%2520model%2520overview%2520and%2520enabling%2520smooth%2520transitions%2520across%250Aabstraction%2520levels%2520of%2520mathematical%2520operations%2520and%2520model%2520structures.%2520It%2520runs%2520a%250Alive%2520GPT-2%2520instance%2520locally%2520in%2520the%2520user%2527s%2520browser%252C%2520empowering%2520users%2520to%250Aexperiment%2520with%2520their%2520own%2520input%2520and%2520observe%2520in%2520real-time%2520how%2520the%2520internal%250Acomponents%2520and%2520parameters%2520of%2520the%2520Transformer%2520work%2520together%2520to%2520predict%2520the%2520next%250Atokens.%2520Our%2520tool%2520requires%2520no%2520installation%2520or%2520special%2520hardware%252C%2520broadening%2520the%250Apublic%2527s%2520education%2520access%2520to%2520modern%2520generative%2520AI%2520techniques.%2520Our%2520open-sourced%250Atool%2520is%2520available%2520at%2520https%253A//poloclub.github.io/transformer-explainer/.%2520A%2520video%250Ademo%2520is%2520available%2520at%2520https%253A//youtu.be/ECR4oAwocjs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20Explainer%3A%20Interactive%20Learning%20of%20Text-Generative%20Models&entry.906535625=Aeree%20Cho%20and%20Grace%20C.%20Kim%20and%20Alexander%20Karpekov%20and%20Alec%20Helbling%20and%20Zijie%20J.%20Wang%20and%20Seongmin%20Lee%20and%20Benjamin%20Hoover%20and%20Duen%20Horng%20Chau&entry.1292438233=%20%20Transformers%20have%20revolutionized%20machine%20learning%2C%20yet%20their%20inner%20workings%0Aremain%20opaque%20to%20many.%20We%20present%20Transformer%20Explainer%2C%20an%20interactive%0Avisualization%20tool%20designed%20for%20non-experts%20to%20learn%20about%20Transformers%20through%0Athe%20GPT-2%20model.%20Our%20tool%20helps%20users%20understand%20complex%20Transformer%20concepts%0Aby%20integrating%20a%20model%20overview%20and%20enabling%20smooth%20transitions%20across%0Aabstraction%20levels%20of%20mathematical%20operations%20and%20model%20structures.%20It%20runs%20a%0Alive%20GPT-2%20instance%20locally%20in%20the%20user%27s%20browser%2C%20empowering%20users%20to%0Aexperiment%20with%20their%20own%20input%20and%20observe%20in%20real-time%20how%20the%20internal%0Acomponents%20and%20parameters%20of%20the%20Transformer%20work%20together%20to%20predict%20the%20next%0Atokens.%20Our%20tool%20requires%20no%20installation%20or%20special%20hardware%2C%20broadening%20the%0Apublic%27s%20education%20access%20to%20modern%20generative%20AI%20techniques.%20Our%20open-sourced%0Atool%20is%20available%20at%20https%3A//poloclub.github.io/transformer-explainer/.%20A%20video%0Ademo%20is%20available%20at%20https%3A//youtu.be/ECR4oAwocjs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04619v1&entry.124074799=Read"},
{"title": "Color Mismatches in Stereoscopic Video: Real-World Dataset and Deep\n  Correction Method", "author": "Egor Chistov and Nikita Alutis and Dmitriy Vatolin", "abstract": "  Stereoscopic videos can contain color mismatches between the left and right\nviews due to minor variations in camera settings, lenses, and even object\nreflections captured from different positions. The presence of color mismatches\ncan lead to viewer discomfort and headaches. This problem can be solved by\ntransferring color between stereoscopic views, but traditional methods often\nlack quality, while neural-network-based methods can easily overfit on\nartificial data. The scarcity of stereoscopic videos with real-world color\nmismatches hinders the evaluation of different methods' performance. Therefore,\nwe filmed a video dataset, which includes both distorted frames with color\nmismatches and ground-truth data, using a beam-splitter. Our second\ncontribution is a deep multiscale neural network that solves the\ncolor-mismatch-correction task by leveraging stereo correspondences. The\nexperimental results demonstrate the effectiveness of the proposed method on a\nconventional dataset, but there remains room for improvement on challenging\nreal-world data.\n", "link": "http://arxiv.org/abs/2303.06657v3", "date": "2024-08-08", "relevancy": 2.162, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5457}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5408}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Color%20Mismatches%20in%20Stereoscopic%20Video%3A%20Real-World%20Dataset%20and%20Deep%0A%20%20Correction%20Method&body=Title%3A%20Color%20Mismatches%20in%20Stereoscopic%20Video%3A%20Real-World%20Dataset%20and%20Deep%0A%20%20Correction%20Method%0AAuthor%3A%20Egor%20Chistov%20and%20Nikita%20Alutis%20and%20Dmitriy%20Vatolin%0AAbstract%3A%20%20%20Stereoscopic%20videos%20can%20contain%20color%20mismatches%20between%20the%20left%20and%20right%0Aviews%20due%20to%20minor%20variations%20in%20camera%20settings%2C%20lenses%2C%20and%20even%20object%0Areflections%20captured%20from%20different%20positions.%20The%20presence%20of%20color%20mismatches%0Acan%20lead%20to%20viewer%20discomfort%20and%20headaches.%20This%20problem%20can%20be%20solved%20by%0Atransferring%20color%20between%20stereoscopic%20views%2C%20but%20traditional%20methods%20often%0Alack%20quality%2C%20while%20neural-network-based%20methods%20can%20easily%20overfit%20on%0Aartificial%20data.%20The%20scarcity%20of%20stereoscopic%20videos%20with%20real-world%20color%0Amismatches%20hinders%20the%20evaluation%20of%20different%20methods%27%20performance.%20Therefore%2C%0Awe%20filmed%20a%20video%20dataset%2C%20which%20includes%20both%20distorted%20frames%20with%20color%0Amismatches%20and%20ground-truth%20data%2C%20using%20a%20beam-splitter.%20Our%20second%0Acontribution%20is%20a%20deep%20multiscale%20neural%20network%20that%20solves%20the%0Acolor-mismatch-correction%20task%20by%20leveraging%20stereo%20correspondences.%20The%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20on%20a%0Aconventional%20dataset%2C%20but%20there%20remains%20room%20for%20improvement%20on%20challenging%0Areal-world%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06657v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DColor%2520Mismatches%2520in%2520Stereoscopic%2520Video%253A%2520Real-World%2520Dataset%2520and%2520Deep%250A%2520%2520Correction%2520Method%26entry.906535625%3DEgor%2520Chistov%2520and%2520Nikita%2520Alutis%2520and%2520Dmitriy%2520Vatolin%26entry.1292438233%3D%2520%2520Stereoscopic%2520videos%2520can%2520contain%2520color%2520mismatches%2520between%2520the%2520left%2520and%2520right%250Aviews%2520due%2520to%2520minor%2520variations%2520in%2520camera%2520settings%252C%2520lenses%252C%2520and%2520even%2520object%250Areflections%2520captured%2520from%2520different%2520positions.%2520The%2520presence%2520of%2520color%2520mismatches%250Acan%2520lead%2520to%2520viewer%2520discomfort%2520and%2520headaches.%2520This%2520problem%2520can%2520be%2520solved%2520by%250Atransferring%2520color%2520between%2520stereoscopic%2520views%252C%2520but%2520traditional%2520methods%2520often%250Alack%2520quality%252C%2520while%2520neural-network-based%2520methods%2520can%2520easily%2520overfit%2520on%250Aartificial%2520data.%2520The%2520scarcity%2520of%2520stereoscopic%2520videos%2520with%2520real-world%2520color%250Amismatches%2520hinders%2520the%2520evaluation%2520of%2520different%2520methods%2527%2520performance.%2520Therefore%252C%250Awe%2520filmed%2520a%2520video%2520dataset%252C%2520which%2520includes%2520both%2520distorted%2520frames%2520with%2520color%250Amismatches%2520and%2520ground-truth%2520data%252C%2520using%2520a%2520beam-splitter.%2520Our%2520second%250Acontribution%2520is%2520a%2520deep%2520multiscale%2520neural%2520network%2520that%2520solves%2520the%250Acolor-mismatch-correction%2520task%2520by%2520leveraging%2520stereo%2520correspondences.%2520The%250Aexperimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520on%2520a%250Aconventional%2520dataset%252C%2520but%2520there%2520remains%2520room%2520for%2520improvement%2520on%2520challenging%250Areal-world%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.06657v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Color%20Mismatches%20in%20Stereoscopic%20Video%3A%20Real-World%20Dataset%20and%20Deep%0A%20%20Correction%20Method&entry.906535625=Egor%20Chistov%20and%20Nikita%20Alutis%20and%20Dmitriy%20Vatolin&entry.1292438233=%20%20Stereoscopic%20videos%20can%20contain%20color%20mismatches%20between%20the%20left%20and%20right%0Aviews%20due%20to%20minor%20variations%20in%20camera%20settings%2C%20lenses%2C%20and%20even%20object%0Areflections%20captured%20from%20different%20positions.%20The%20presence%20of%20color%20mismatches%0Acan%20lead%20to%20viewer%20discomfort%20and%20headaches.%20This%20problem%20can%20be%20solved%20by%0Atransferring%20color%20between%20stereoscopic%20views%2C%20but%20traditional%20methods%20often%0Alack%20quality%2C%20while%20neural-network-based%20methods%20can%20easily%20overfit%20on%0Aartificial%20data.%20The%20scarcity%20of%20stereoscopic%20videos%20with%20real-world%20color%0Amismatches%20hinders%20the%20evaluation%20of%20different%20methods%27%20performance.%20Therefore%2C%0Awe%20filmed%20a%20video%20dataset%2C%20which%20includes%20both%20distorted%20frames%20with%20color%0Amismatches%20and%20ground-truth%20data%2C%20using%20a%20beam-splitter.%20Our%20second%0Acontribution%20is%20a%20deep%20multiscale%20neural%20network%20that%20solves%20the%0Acolor-mismatch-correction%20task%20by%20leveraging%20stereo%20correspondences.%20The%0Aexperimental%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method%20on%20a%0Aconventional%20dataset%2C%20but%20there%20remains%20room%20for%20improvement%20on%20challenging%0Areal-world%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06657v3&entry.124074799=Read"},
{"title": "P2LHAP:Wearable sensor-based human activity recognition, segmentation\n  and forecast through Patch-to-Label Seq2Seq Transformer", "author": "Shuangjian Li and Tao Zhu and Mingxing Nie and Huansheng Ning and Zhenyu Liu and Liming Chen", "abstract": "  Traditional deep learning methods struggle to simultaneously segment,\nrecognize, and forecast human activities from sensor data. This limits their\nusefulness in many fields such as healthcare and assisted living, where\nreal-time understanding of ongoing and upcoming activities is crucial. This\npaper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles\nall three tasks in a efficient single-task model. P2LHAP divides sensor data\nstreams into a sequence of \"patches\", served as input tokens, and outputs a\nsequence of patch-level activity labels including the predicted future\nactivities. A unique smoothing technique based on surrounding patch labels, is\nproposed to identify activity boundaries accurately. Additionally, P2LHAP\nlearns patch-level representation by sensor signal channel-independent\nTransformer encoders and decoders. All channels share embedding and Transformer\nweights across all sequences. Evaluated on three public datasets, P2LHAP\nsignificantly outperforms the state-of-the-art in all three tasks,\ndemonstrating its effectiveness and potential for real-world applications.\n", "link": "http://arxiv.org/abs/2403.08214v2", "date": "2024-08-08", "relevancy": 2.1553, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5754}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5154}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P2LHAP%3AWearable%20sensor-based%20human%20activity%20recognition%2C%20segmentation%0A%20%20and%20forecast%20through%20Patch-to-Label%20Seq2Seq%20Transformer&body=Title%3A%20P2LHAP%3AWearable%20sensor-based%20human%20activity%20recognition%2C%20segmentation%0A%20%20and%20forecast%20through%20Patch-to-Label%20Seq2Seq%20Transformer%0AAuthor%3A%20Shuangjian%20Li%20and%20Tao%20Zhu%20and%20Mingxing%20Nie%20and%20Huansheng%20Ning%20and%20Zhenyu%20Liu%20and%20Liming%20Chen%0AAbstract%3A%20%20%20Traditional%20deep%20learning%20methods%20struggle%20to%20simultaneously%20segment%2C%0Arecognize%2C%20and%20forecast%20human%20activities%20from%20sensor%20data.%20This%20limits%20their%0Ausefulness%20in%20many%20fields%20such%20as%20healthcare%20and%20assisted%20living%2C%20where%0Areal-time%20understanding%20of%20ongoing%20and%20upcoming%20activities%20is%20crucial.%20This%0Apaper%20introduces%20P2LHAP%2C%20a%20novel%20Patch-to-Label%20Seq2Seq%20framework%20that%20tackles%0Aall%20three%20tasks%20in%20a%20efficient%20single-task%20model.%20P2LHAP%20divides%20sensor%20data%0Astreams%20into%20a%20sequence%20of%20%22patches%22%2C%20served%20as%20input%20tokens%2C%20and%20outputs%20a%0Asequence%20of%20patch-level%20activity%20labels%20including%20the%20predicted%20future%0Aactivities.%20A%20unique%20smoothing%20technique%20based%20on%20surrounding%20patch%20labels%2C%20is%0Aproposed%20to%20identify%20activity%20boundaries%20accurately.%20Additionally%2C%20P2LHAP%0Alearns%20patch-level%20representation%20by%20sensor%20signal%20channel-independent%0ATransformer%20encoders%20and%20decoders.%20All%20channels%20share%20embedding%20and%20Transformer%0Aweights%20across%20all%20sequences.%20Evaluated%20on%20three%20public%20datasets%2C%20P2LHAP%0Asignificantly%20outperforms%20the%20state-of-the-art%20in%20all%20three%20tasks%2C%0Ademonstrating%20its%20effectiveness%20and%20potential%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08214v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP2LHAP%253AWearable%2520sensor-based%2520human%2520activity%2520recognition%252C%2520segmentation%250A%2520%2520and%2520forecast%2520through%2520Patch-to-Label%2520Seq2Seq%2520Transformer%26entry.906535625%3DShuangjian%2520Li%2520and%2520Tao%2520Zhu%2520and%2520Mingxing%2520Nie%2520and%2520Huansheng%2520Ning%2520and%2520Zhenyu%2520Liu%2520and%2520Liming%2520Chen%26entry.1292438233%3D%2520%2520Traditional%2520deep%2520learning%2520methods%2520struggle%2520to%2520simultaneously%2520segment%252C%250Arecognize%252C%2520and%2520forecast%2520human%2520activities%2520from%2520sensor%2520data.%2520This%2520limits%2520their%250Ausefulness%2520in%2520many%2520fields%2520such%2520as%2520healthcare%2520and%2520assisted%2520living%252C%2520where%250Areal-time%2520understanding%2520of%2520ongoing%2520and%2520upcoming%2520activities%2520is%2520crucial.%2520This%250Apaper%2520introduces%2520P2LHAP%252C%2520a%2520novel%2520Patch-to-Label%2520Seq2Seq%2520framework%2520that%2520tackles%250Aall%2520three%2520tasks%2520in%2520a%2520efficient%2520single-task%2520model.%2520P2LHAP%2520divides%2520sensor%2520data%250Astreams%2520into%2520a%2520sequence%2520of%2520%2522patches%2522%252C%2520served%2520as%2520input%2520tokens%252C%2520and%2520outputs%2520a%250Asequence%2520of%2520patch-level%2520activity%2520labels%2520including%2520the%2520predicted%2520future%250Aactivities.%2520A%2520unique%2520smoothing%2520technique%2520based%2520on%2520surrounding%2520patch%2520labels%252C%2520is%250Aproposed%2520to%2520identify%2520activity%2520boundaries%2520accurately.%2520Additionally%252C%2520P2LHAP%250Alearns%2520patch-level%2520representation%2520by%2520sensor%2520signal%2520channel-independent%250ATransformer%2520encoders%2520and%2520decoders.%2520All%2520channels%2520share%2520embedding%2520and%2520Transformer%250Aweights%2520across%2520all%2520sequences.%2520Evaluated%2520on%2520three%2520public%2520datasets%252C%2520P2LHAP%250Asignificantly%2520outperforms%2520the%2520state-of-the-art%2520in%2520all%2520three%2520tasks%252C%250Ademonstrating%2520its%2520effectiveness%2520and%2520potential%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08214v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P2LHAP%3AWearable%20sensor-based%20human%20activity%20recognition%2C%20segmentation%0A%20%20and%20forecast%20through%20Patch-to-Label%20Seq2Seq%20Transformer&entry.906535625=Shuangjian%20Li%20and%20Tao%20Zhu%20and%20Mingxing%20Nie%20and%20Huansheng%20Ning%20and%20Zhenyu%20Liu%20and%20Liming%20Chen&entry.1292438233=%20%20Traditional%20deep%20learning%20methods%20struggle%20to%20simultaneously%20segment%2C%0Arecognize%2C%20and%20forecast%20human%20activities%20from%20sensor%20data.%20This%20limits%20their%0Ausefulness%20in%20many%20fields%20such%20as%20healthcare%20and%20assisted%20living%2C%20where%0Areal-time%20understanding%20of%20ongoing%20and%20upcoming%20activities%20is%20crucial.%20This%0Apaper%20introduces%20P2LHAP%2C%20a%20novel%20Patch-to-Label%20Seq2Seq%20framework%20that%20tackles%0Aall%20three%20tasks%20in%20a%20efficient%20single-task%20model.%20P2LHAP%20divides%20sensor%20data%0Astreams%20into%20a%20sequence%20of%20%22patches%22%2C%20served%20as%20input%20tokens%2C%20and%20outputs%20a%0Asequence%20of%20patch-level%20activity%20labels%20including%20the%20predicted%20future%0Aactivities.%20A%20unique%20smoothing%20technique%20based%20on%20surrounding%20patch%20labels%2C%20is%0Aproposed%20to%20identify%20activity%20boundaries%20accurately.%20Additionally%2C%20P2LHAP%0Alearns%20patch-level%20representation%20by%20sensor%20signal%20channel-independent%0ATransformer%20encoders%20and%20decoders.%20All%20channels%20share%20embedding%20and%20Transformer%0Aweights%20across%20all%20sequences.%20Evaluated%20on%20three%20public%20datasets%2C%20P2LHAP%0Asignificantly%20outperforms%20the%20state-of-the-art%20in%20all%20three%20tasks%2C%0Ademonstrating%20its%20effectiveness%20and%20potential%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08214v2&entry.124074799=Read"},
{"title": "Smooth Deep Saliency", "author": "Rudolf Herdt and Maximilian Schmidt and Daniel Otero Baguer and Peter Maa\u00df", "abstract": "  In this work, we investigate methods to reduce the noise in deep saliency\nmaps coming from convolutional downsampling. Those methods make the\ninvestigated models more interpretable for gradient-based saliency maps,\ncomputed in hidden layers. We evaluate the faithfulness of those methods using\ninsertion and deletion metrics, finding that saliency maps computed in hidden\nlayers perform better compared to both the input layer and GradCAM. We test our\napproach on different models trained for image classification on ImageNet1K,\nand models trained for tumor detection on Camelyon16 and in-house real-world\ndigital pathology scans of stained tissue samples. Our results show that the\ncheckerboard noise in the gradient gets reduced, resulting in smoother and\ntherefore easier to interpret saliency maps.\n", "link": "http://arxiv.org/abs/2404.02282v3", "date": "2024-08-08", "relevancy": 2.1252, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.545}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5352}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smooth%20Deep%20Saliency&body=Title%3A%20Smooth%20Deep%20Saliency%0AAuthor%3A%20Rudolf%20Herdt%20and%20Maximilian%20Schmidt%20and%20Daniel%20Otero%20Baguer%20and%20Peter%20Maa%C3%9F%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20investigate%20methods%20to%20reduce%20the%20noise%20in%20deep%20saliency%0Amaps%20coming%20from%20convolutional%20downsampling.%20Those%20methods%20make%20the%0Ainvestigated%20models%20more%20interpretable%20for%20gradient-based%20saliency%20maps%2C%0Acomputed%20in%20hidden%20layers.%20We%20evaluate%20the%20faithfulness%20of%20those%20methods%20using%0Ainsertion%20and%20deletion%20metrics%2C%20finding%20that%20saliency%20maps%20computed%20in%20hidden%0Alayers%20perform%20better%20compared%20to%20both%20the%20input%20layer%20and%20GradCAM.%20We%20test%20our%0Aapproach%20on%20different%20models%20trained%20for%20image%20classification%20on%20ImageNet1K%2C%0Aand%20models%20trained%20for%20tumor%20detection%20on%20Camelyon16%20and%20in-house%20real-world%0Adigital%20pathology%20scans%20of%20stained%20tissue%20samples.%20Our%20results%20show%20that%20the%0Acheckerboard%20noise%20in%20the%20gradient%20gets%20reduced%2C%20resulting%20in%20smoother%20and%0Atherefore%20easier%20to%20interpret%20saliency%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02282v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmooth%2520Deep%2520Saliency%26entry.906535625%3DRudolf%2520Herdt%2520and%2520Maximilian%2520Schmidt%2520and%2520Daniel%2520Otero%2520Baguer%2520and%2520Peter%2520Maa%25C3%259F%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520investigate%2520methods%2520to%2520reduce%2520the%2520noise%2520in%2520deep%2520saliency%250Amaps%2520coming%2520from%2520convolutional%2520downsampling.%2520Those%2520methods%2520make%2520the%250Ainvestigated%2520models%2520more%2520interpretable%2520for%2520gradient-based%2520saliency%2520maps%252C%250Acomputed%2520in%2520hidden%2520layers.%2520We%2520evaluate%2520the%2520faithfulness%2520of%2520those%2520methods%2520using%250Ainsertion%2520and%2520deletion%2520metrics%252C%2520finding%2520that%2520saliency%2520maps%2520computed%2520in%2520hidden%250Alayers%2520perform%2520better%2520compared%2520to%2520both%2520the%2520input%2520layer%2520and%2520GradCAM.%2520We%2520test%2520our%250Aapproach%2520on%2520different%2520models%2520trained%2520for%2520image%2520classification%2520on%2520ImageNet1K%252C%250Aand%2520models%2520trained%2520for%2520tumor%2520detection%2520on%2520Camelyon16%2520and%2520in-house%2520real-world%250Adigital%2520pathology%2520scans%2520of%2520stained%2520tissue%2520samples.%2520Our%2520results%2520show%2520that%2520the%250Acheckerboard%2520noise%2520in%2520the%2520gradient%2520gets%2520reduced%252C%2520resulting%2520in%2520smoother%2520and%250Atherefore%2520easier%2520to%2520interpret%2520saliency%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02282v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smooth%20Deep%20Saliency&entry.906535625=Rudolf%20Herdt%20and%20Maximilian%20Schmidt%20and%20Daniel%20Otero%20Baguer%20and%20Peter%20Maa%C3%9F&entry.1292438233=%20%20In%20this%20work%2C%20we%20investigate%20methods%20to%20reduce%20the%20noise%20in%20deep%20saliency%0Amaps%20coming%20from%20convolutional%20downsampling.%20Those%20methods%20make%20the%0Ainvestigated%20models%20more%20interpretable%20for%20gradient-based%20saliency%20maps%2C%0Acomputed%20in%20hidden%20layers.%20We%20evaluate%20the%20faithfulness%20of%20those%20methods%20using%0Ainsertion%20and%20deletion%20metrics%2C%20finding%20that%20saliency%20maps%20computed%20in%20hidden%0Alayers%20perform%20better%20compared%20to%20both%20the%20input%20layer%20and%20GradCAM.%20We%20test%20our%0Aapproach%20on%20different%20models%20trained%20for%20image%20classification%20on%20ImageNet1K%2C%0Aand%20models%20trained%20for%20tumor%20detection%20on%20Camelyon16%20and%20in-house%20real-world%0Adigital%20pathology%20scans%20of%20stained%20tissue%20samples.%20Our%20results%20show%20that%20the%0Acheckerboard%20noise%20in%20the%20gradient%20gets%20reduced%2C%20resulting%20in%20smoother%20and%0Atherefore%20easier%20to%20interpret%20saliency%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02282v3&entry.124074799=Read"},
{"title": "An experimental comparative study of backpropagation and alternatives\n  for training binary neural networks for image classification", "author": "Ben Crulis and Barthelemy Serres and Cyril de Runz and Gilles Venturini", "abstract": "  Current artificial neural networks are trained with parameters encoded as\nfloating point numbers that occupy lots of memory space at inference time. Due\nto the increase in the size of deep learning models, it is becoming very\ndifficult to consider training and using artificial neural networks on edge\ndevices. Binary neural networks promise to reduce the size of deep neural\nnetwork models, as well as to increase inference speed while decreasing energy\nconsumption. Thus, they may allow the deployment of more powerful models on\nedge devices. However, binary neural networks are still proven to be difficult\nto train using the backpropagation-based gradient descent scheme. This paper\nextends the work of \\cite{crulis2023alternatives}, which proposed adapting to\nbinary neural networks two promising alternatives to backpropagation originally\ndesigned for continuous neural networks, and experimented with them on simple\nimage classification datasets. This paper proposes new experiments on the\nImageNette dataset, compares three different model architectures for image\nclassification, and adds two additional alternatives to backpropagation.\n", "link": "http://arxiv.org/abs/2408.04460v1", "date": "2024-08-08", "relevancy": 2.1143, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5395}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5224}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20experimental%20comparative%20study%20of%20backpropagation%20and%20alternatives%0A%20%20for%20training%20binary%20neural%20networks%20for%20image%20classification&body=Title%3A%20An%20experimental%20comparative%20study%20of%20backpropagation%20and%20alternatives%0A%20%20for%20training%20binary%20neural%20networks%20for%20image%20classification%0AAuthor%3A%20Ben%20Crulis%20and%20Barthelemy%20Serres%20and%20Cyril%20de%20Runz%20and%20Gilles%20Venturini%0AAbstract%3A%20%20%20Current%20artificial%20neural%20networks%20are%20trained%20with%20parameters%20encoded%20as%0Afloating%20point%20numbers%20that%20occupy%20lots%20of%20memory%20space%20at%20inference%20time.%20Due%0Ato%20the%20increase%20in%20the%20size%20of%20deep%20learning%20models%2C%20it%20is%20becoming%20very%0Adifficult%20to%20consider%20training%20and%20using%20artificial%20neural%20networks%20on%20edge%0Adevices.%20Binary%20neural%20networks%20promise%20to%20reduce%20the%20size%20of%20deep%20neural%0Anetwork%20models%2C%20as%20well%20as%20to%20increase%20inference%20speed%20while%20decreasing%20energy%0Aconsumption.%20Thus%2C%20they%20may%20allow%20the%20deployment%20of%20more%20powerful%20models%20on%0Aedge%20devices.%20However%2C%20binary%20neural%20networks%20are%20still%20proven%20to%20be%20difficult%0Ato%20train%20using%20the%20backpropagation-based%20gradient%20descent%20scheme.%20This%20paper%0Aextends%20the%20work%20of%20%5Ccite%7Bcrulis2023alternatives%7D%2C%20which%20proposed%20adapting%20to%0Abinary%20neural%20networks%20two%20promising%20alternatives%20to%20backpropagation%20originally%0Adesigned%20for%20continuous%20neural%20networks%2C%20and%20experimented%20with%20them%20on%20simple%0Aimage%20classification%20datasets.%20This%20paper%20proposes%20new%20experiments%20on%20the%0AImageNette%20dataset%2C%20compares%20three%20different%20model%20architectures%20for%20image%0Aclassification%2C%20and%20adds%20two%20additional%20alternatives%20to%20backpropagation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520experimental%2520comparative%2520study%2520of%2520backpropagation%2520and%2520alternatives%250A%2520%2520for%2520training%2520binary%2520neural%2520networks%2520for%2520image%2520classification%26entry.906535625%3DBen%2520Crulis%2520and%2520Barthelemy%2520Serres%2520and%2520Cyril%2520de%2520Runz%2520and%2520Gilles%2520Venturini%26entry.1292438233%3D%2520%2520Current%2520artificial%2520neural%2520networks%2520are%2520trained%2520with%2520parameters%2520encoded%2520as%250Afloating%2520point%2520numbers%2520that%2520occupy%2520lots%2520of%2520memory%2520space%2520at%2520inference%2520time.%2520Due%250Ato%2520the%2520increase%2520in%2520the%2520size%2520of%2520deep%2520learning%2520models%252C%2520it%2520is%2520becoming%2520very%250Adifficult%2520to%2520consider%2520training%2520and%2520using%2520artificial%2520neural%2520networks%2520on%2520edge%250Adevices.%2520Binary%2520neural%2520networks%2520promise%2520to%2520reduce%2520the%2520size%2520of%2520deep%2520neural%250Anetwork%2520models%252C%2520as%2520well%2520as%2520to%2520increase%2520inference%2520speed%2520while%2520decreasing%2520energy%250Aconsumption.%2520Thus%252C%2520they%2520may%2520allow%2520the%2520deployment%2520of%2520more%2520powerful%2520models%2520on%250Aedge%2520devices.%2520However%252C%2520binary%2520neural%2520networks%2520are%2520still%2520proven%2520to%2520be%2520difficult%250Ato%2520train%2520using%2520the%2520backpropagation-based%2520gradient%2520descent%2520scheme.%2520This%2520paper%250Aextends%2520the%2520work%2520of%2520%255Ccite%257Bcrulis2023alternatives%257D%252C%2520which%2520proposed%2520adapting%2520to%250Abinary%2520neural%2520networks%2520two%2520promising%2520alternatives%2520to%2520backpropagation%2520originally%250Adesigned%2520for%2520continuous%2520neural%2520networks%252C%2520and%2520experimented%2520with%2520them%2520on%2520simple%250Aimage%2520classification%2520datasets.%2520This%2520paper%2520proposes%2520new%2520experiments%2520on%2520the%250AImageNette%2520dataset%252C%2520compares%2520three%2520different%2520model%2520architectures%2520for%2520image%250Aclassification%252C%2520and%2520adds%2520two%2520additional%2520alternatives%2520to%2520backpropagation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20experimental%20comparative%20study%20of%20backpropagation%20and%20alternatives%0A%20%20for%20training%20binary%20neural%20networks%20for%20image%20classification&entry.906535625=Ben%20Crulis%20and%20Barthelemy%20Serres%20and%20Cyril%20de%20Runz%20and%20Gilles%20Venturini&entry.1292438233=%20%20Current%20artificial%20neural%20networks%20are%20trained%20with%20parameters%20encoded%20as%0Afloating%20point%20numbers%20that%20occupy%20lots%20of%20memory%20space%20at%20inference%20time.%20Due%0Ato%20the%20increase%20in%20the%20size%20of%20deep%20learning%20models%2C%20it%20is%20becoming%20very%0Adifficult%20to%20consider%20training%20and%20using%20artificial%20neural%20networks%20on%20edge%0Adevices.%20Binary%20neural%20networks%20promise%20to%20reduce%20the%20size%20of%20deep%20neural%0Anetwork%20models%2C%20as%20well%20as%20to%20increase%20inference%20speed%20while%20decreasing%20energy%0Aconsumption.%20Thus%2C%20they%20may%20allow%20the%20deployment%20of%20more%20powerful%20models%20on%0Aedge%20devices.%20However%2C%20binary%20neural%20networks%20are%20still%20proven%20to%20be%20difficult%0Ato%20train%20using%20the%20backpropagation-based%20gradient%20descent%20scheme.%20This%20paper%0Aextends%20the%20work%20of%20%5Ccite%7Bcrulis2023alternatives%7D%2C%20which%20proposed%20adapting%20to%0Abinary%20neural%20networks%20two%20promising%20alternatives%20to%20backpropagation%20originally%0Adesigned%20for%20continuous%20neural%20networks%2C%20and%20experimented%20with%20them%20on%20simple%0Aimage%20classification%20datasets.%20This%20paper%20proposes%20new%20experiments%20on%20the%0AImageNette%20dataset%2C%20compares%20three%20different%20model%20architectures%20for%20image%0Aclassification%2C%20and%20adds%20two%20additional%20alternatives%20to%20backpropagation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04460v1&entry.124074799=Read"},
{"title": "Guided Data Augmentation for Offline Reinforcement Learning and\n  Imitation Learning", "author": "Nicholas E. Corrado and Yuxiao Qu and John U. Balis and Adam Labiosa and Josiah P. Hanna", "abstract": "  In offline reinforcement learning (RL), an RL agent learns to solve a task\nusing only a fixed dataset of previously collected data. While offline RL has\nbeen successful in learning real-world robot control policies, it typically\nrequires large amounts of expert-quality data to learn effective policies that\ngeneralize to out-of-distribution states. Unfortunately, such data is often\ndifficult and expensive to acquire in real-world tasks. Several recent works\nhave leveraged data augmentation (DA) to inexpensively generate additional\ndata, but most DA works apply augmentations in a random fashion and ultimately\nproduce highly suboptimal augmented experience. In this work, we propose Guided\nData Augmentation (GuDA), a human-guided DA framework that generates\nexpert-quality augmented data. The key insight behind GuDA is that while it may\nbe difficult to demonstrate the sequence of actions required to produce expert\ndata, a user can often easily characterize when an augmented trajectory segment\nrepresents progress toward task completion. Thus, a user can restrict the space\nof possible augmentations to automatically reject suboptimal augmented data. To\nextract a policy from GuDA, we use off-the-shelf offline reinforcement learning\nand behavior cloning algorithms. We evaluate GuDA on a physical robot soccer\ntask as well as simulated D4RL navigation tasks, a simulated autonomous driving\ntask, and a simulated soccer task. Empirically, GuDA enables learning given a\nsmall initial dataset of potentially suboptimal experience and outperforms a\nrandom DA strategy as well as a model-based DA strategy.\n", "link": "http://arxiv.org/abs/2310.18247v3", "date": "2024-08-08", "relevancy": 2.0927, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5426}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5344}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Data%20Augmentation%20for%20Offline%20Reinforcement%20Learning%20and%0A%20%20Imitation%20Learning&body=Title%3A%20Guided%20Data%20Augmentation%20for%20Offline%20Reinforcement%20Learning%20and%0A%20%20Imitation%20Learning%0AAuthor%3A%20Nicholas%20E.%20Corrado%20and%20Yuxiao%20Qu%20and%20John%20U.%20Balis%20and%20Adam%20Labiosa%20and%20Josiah%20P.%20Hanna%0AAbstract%3A%20%20%20In%20offline%20reinforcement%20learning%20%28RL%29%2C%20an%20RL%20agent%20learns%20to%20solve%20a%20task%0Ausing%20only%20a%20fixed%20dataset%20of%20previously%20collected%20data.%20While%20offline%20RL%20has%0Abeen%20successful%20in%20learning%20real-world%20robot%20control%20policies%2C%20it%20typically%0Arequires%20large%20amounts%20of%20expert-quality%20data%20to%20learn%20effective%20policies%20that%0Ageneralize%20to%20out-of-distribution%20states.%20Unfortunately%2C%20such%20data%20is%20often%0Adifficult%20and%20expensive%20to%20acquire%20in%20real-world%20tasks.%20Several%20recent%20works%0Ahave%20leveraged%20data%20augmentation%20%28DA%29%20to%20inexpensively%20generate%20additional%0Adata%2C%20but%20most%20DA%20works%20apply%20augmentations%20in%20a%20random%20fashion%20and%20ultimately%0Aproduce%20highly%20suboptimal%20augmented%20experience.%20In%20this%20work%2C%20we%20propose%20Guided%0AData%20Augmentation%20%28GuDA%29%2C%20a%20human-guided%20DA%20framework%20that%20generates%0Aexpert-quality%20augmented%20data.%20The%20key%20insight%20behind%20GuDA%20is%20that%20while%20it%20may%0Abe%20difficult%20to%20demonstrate%20the%20sequence%20of%20actions%20required%20to%20produce%20expert%0Adata%2C%20a%20user%20can%20often%20easily%20characterize%20when%20an%20augmented%20trajectory%20segment%0Arepresents%20progress%20toward%20task%20completion.%20Thus%2C%20a%20user%20can%20restrict%20the%20space%0Aof%20possible%20augmentations%20to%20automatically%20reject%20suboptimal%20augmented%20data.%20To%0Aextract%20a%20policy%20from%20GuDA%2C%20we%20use%20off-the-shelf%20offline%20reinforcement%20learning%0Aand%20behavior%20cloning%20algorithms.%20We%20evaluate%20GuDA%20on%20a%20physical%20robot%20soccer%0Atask%20as%20well%20as%20simulated%20D4RL%20navigation%20tasks%2C%20a%20simulated%20autonomous%20driving%0Atask%2C%20and%20a%20simulated%20soccer%20task.%20Empirically%2C%20GuDA%20enables%20learning%20given%20a%0Asmall%20initial%20dataset%20of%20potentially%20suboptimal%20experience%20and%20outperforms%20a%0Arandom%20DA%20strategy%20as%20well%20as%20a%20model-based%20DA%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.18247v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Data%2520Augmentation%2520for%2520Offline%2520Reinforcement%2520Learning%2520and%250A%2520%2520Imitation%2520Learning%26entry.906535625%3DNicholas%2520E.%2520Corrado%2520and%2520Yuxiao%2520Qu%2520and%2520John%2520U.%2520Balis%2520and%2520Adam%2520Labiosa%2520and%2520Josiah%2520P.%2520Hanna%26entry.1292438233%3D%2520%2520In%2520offline%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520an%2520RL%2520agent%2520learns%2520to%2520solve%2520a%2520task%250Ausing%2520only%2520a%2520fixed%2520dataset%2520of%2520previously%2520collected%2520data.%2520While%2520offline%2520RL%2520has%250Abeen%2520successful%2520in%2520learning%2520real-world%2520robot%2520control%2520policies%252C%2520it%2520typically%250Arequires%2520large%2520amounts%2520of%2520expert-quality%2520data%2520to%2520learn%2520effective%2520policies%2520that%250Ageneralize%2520to%2520out-of-distribution%2520states.%2520Unfortunately%252C%2520such%2520data%2520is%2520often%250Adifficult%2520and%2520expensive%2520to%2520acquire%2520in%2520real-world%2520tasks.%2520Several%2520recent%2520works%250Ahave%2520leveraged%2520data%2520augmentation%2520%2528DA%2529%2520to%2520inexpensively%2520generate%2520additional%250Adata%252C%2520but%2520most%2520DA%2520works%2520apply%2520augmentations%2520in%2520a%2520random%2520fashion%2520and%2520ultimately%250Aproduce%2520highly%2520suboptimal%2520augmented%2520experience.%2520In%2520this%2520work%252C%2520we%2520propose%2520Guided%250AData%2520Augmentation%2520%2528GuDA%2529%252C%2520a%2520human-guided%2520DA%2520framework%2520that%2520generates%250Aexpert-quality%2520augmented%2520data.%2520The%2520key%2520insight%2520behind%2520GuDA%2520is%2520that%2520while%2520it%2520may%250Abe%2520difficult%2520to%2520demonstrate%2520the%2520sequence%2520of%2520actions%2520required%2520to%2520produce%2520expert%250Adata%252C%2520a%2520user%2520can%2520often%2520easily%2520characterize%2520when%2520an%2520augmented%2520trajectory%2520segment%250Arepresents%2520progress%2520toward%2520task%2520completion.%2520Thus%252C%2520a%2520user%2520can%2520restrict%2520the%2520space%250Aof%2520possible%2520augmentations%2520to%2520automatically%2520reject%2520suboptimal%2520augmented%2520data.%2520To%250Aextract%2520a%2520policy%2520from%2520GuDA%252C%2520we%2520use%2520off-the-shelf%2520offline%2520reinforcement%2520learning%250Aand%2520behavior%2520cloning%2520algorithms.%2520We%2520evaluate%2520GuDA%2520on%2520a%2520physical%2520robot%2520soccer%250Atask%2520as%2520well%2520as%2520simulated%2520D4RL%2520navigation%2520tasks%252C%2520a%2520simulated%2520autonomous%2520driving%250Atask%252C%2520and%2520a%2520simulated%2520soccer%2520task.%2520Empirically%252C%2520GuDA%2520enables%2520learning%2520given%2520a%250Asmall%2520initial%2520dataset%2520of%2520potentially%2520suboptimal%2520experience%2520and%2520outperforms%2520a%250Arandom%2520DA%2520strategy%2520as%2520well%2520as%2520a%2520model-based%2520DA%2520strategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.18247v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Data%20Augmentation%20for%20Offline%20Reinforcement%20Learning%20and%0A%20%20Imitation%20Learning&entry.906535625=Nicholas%20E.%20Corrado%20and%20Yuxiao%20Qu%20and%20John%20U.%20Balis%20and%20Adam%20Labiosa%20and%20Josiah%20P.%20Hanna&entry.1292438233=%20%20In%20offline%20reinforcement%20learning%20%28RL%29%2C%20an%20RL%20agent%20learns%20to%20solve%20a%20task%0Ausing%20only%20a%20fixed%20dataset%20of%20previously%20collected%20data.%20While%20offline%20RL%20has%0Abeen%20successful%20in%20learning%20real-world%20robot%20control%20policies%2C%20it%20typically%0Arequires%20large%20amounts%20of%20expert-quality%20data%20to%20learn%20effective%20policies%20that%0Ageneralize%20to%20out-of-distribution%20states.%20Unfortunately%2C%20such%20data%20is%20often%0Adifficult%20and%20expensive%20to%20acquire%20in%20real-world%20tasks.%20Several%20recent%20works%0Ahave%20leveraged%20data%20augmentation%20%28DA%29%20to%20inexpensively%20generate%20additional%0Adata%2C%20but%20most%20DA%20works%20apply%20augmentations%20in%20a%20random%20fashion%20and%20ultimately%0Aproduce%20highly%20suboptimal%20augmented%20experience.%20In%20this%20work%2C%20we%20propose%20Guided%0AData%20Augmentation%20%28GuDA%29%2C%20a%20human-guided%20DA%20framework%20that%20generates%0Aexpert-quality%20augmented%20data.%20The%20key%20insight%20behind%20GuDA%20is%20that%20while%20it%20may%0Abe%20difficult%20to%20demonstrate%20the%20sequence%20of%20actions%20required%20to%20produce%20expert%0Adata%2C%20a%20user%20can%20often%20easily%20characterize%20when%20an%20augmented%20trajectory%20segment%0Arepresents%20progress%20toward%20task%20completion.%20Thus%2C%20a%20user%20can%20restrict%20the%20space%0Aof%20possible%20augmentations%20to%20automatically%20reject%20suboptimal%20augmented%20data.%20To%0Aextract%20a%20policy%20from%20GuDA%2C%20we%20use%20off-the-shelf%20offline%20reinforcement%20learning%0Aand%20behavior%20cloning%20algorithms.%20We%20evaluate%20GuDA%20on%20a%20physical%20robot%20soccer%0Atask%20as%20well%20as%20simulated%20D4RL%20navigation%20tasks%2C%20a%20simulated%20autonomous%20driving%0Atask%2C%20and%20a%20simulated%20soccer%20task.%20Empirically%2C%20GuDA%20enables%20learning%20given%20a%0Asmall%20initial%20dataset%20of%20potentially%20suboptimal%20experience%20and%20outperforms%20a%0Arandom%20DA%20strategy%20as%20well%20as%20a%20model-based%20DA%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.18247v3&entry.124074799=Read"},
{"title": "Enhanced Prototypical Part Network (EPPNet) For Explainable Image\n  Classification Via Prototypes", "author": "Bhushan Atote and Victor Sanchez", "abstract": "  Explainable Artificial Intelligence (xAI) has the potential to enhance the\ntransparency and trust of AI-based systems. Although accurate predictions can\nbe made using Deep Neural Networks (DNNs), the process used to arrive at such\npredictions is usually hard to explain. In terms of perceptibly human-friendly\nrepresentations, such as word phrases in text or super-pixels in images,\nprototype-based explanations can justify a model's decision. In this work, we\nintroduce a DNN architecture for image classification, the Enhanced\nPrototypical Part Network (EPPNet), which achieves strong performance while\ndiscovering relevant prototypes that can be used to explain the classification\nresults. This is achieved by introducing a novel cluster loss that helps to\ndiscover more relevant human-understandable prototypes. We also introduce a\nfaithfulness score to evaluate the explainability of the results based on the\ndiscovered prototypes. Our score not only accounts for the relevance of the\nlearned prototypes but also the performance of a model. Our evaluations on the\nCUB-200-2011 dataset show that the EPPNet outperforms state-of-the-art\nxAI-based methods, in terms of both classification accuracy and explainability\n", "link": "http://arxiv.org/abs/2408.04606v1", "date": "2024-08-08", "relevancy": 2.0817, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5458}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5048}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Prototypical%20Part%20Network%20%28EPPNet%29%20For%20Explainable%20Image%0A%20%20Classification%20Via%20Prototypes&body=Title%3A%20Enhanced%20Prototypical%20Part%20Network%20%28EPPNet%29%20For%20Explainable%20Image%0A%20%20Classification%20Via%20Prototypes%0AAuthor%3A%20Bhushan%20Atote%20and%20Victor%20Sanchez%0AAbstract%3A%20%20%20Explainable%20Artificial%20Intelligence%20%28xAI%29%20has%20the%20potential%20to%20enhance%20the%0Atransparency%20and%20trust%20of%20AI-based%20systems.%20Although%20accurate%20predictions%20can%0Abe%20made%20using%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20the%20process%20used%20to%20arrive%20at%20such%0Apredictions%20is%20usually%20hard%20to%20explain.%20In%20terms%20of%20perceptibly%20human-friendly%0Arepresentations%2C%20such%20as%20word%20phrases%20in%20text%20or%20super-pixels%20in%20images%2C%0Aprototype-based%20explanations%20can%20justify%20a%20model%27s%20decision.%20In%20this%20work%2C%20we%0Aintroduce%20a%20DNN%20architecture%20for%20image%20classification%2C%20the%20Enhanced%0APrototypical%20Part%20Network%20%28EPPNet%29%2C%20which%20achieves%20strong%20performance%20while%0Adiscovering%20relevant%20prototypes%20that%20can%20be%20used%20to%20explain%20the%20classification%0Aresults.%20This%20is%20achieved%20by%20introducing%20a%20novel%20cluster%20loss%20that%20helps%20to%0Adiscover%20more%20relevant%20human-understandable%20prototypes.%20We%20also%20introduce%20a%0Afaithfulness%20score%20to%20evaluate%20the%20explainability%20of%20the%20results%20based%20on%20the%0Adiscovered%20prototypes.%20Our%20score%20not%20only%20accounts%20for%20the%20relevance%20of%20the%0Alearned%20prototypes%20but%20also%20the%20performance%20of%20a%20model.%20Our%20evaluations%20on%20the%0ACUB-200-2011%20dataset%20show%20that%20the%20EPPNet%20outperforms%20state-of-the-art%0AxAI-based%20methods%2C%20in%20terms%20of%20both%20classification%20accuracy%20and%20explainability%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Prototypical%2520Part%2520Network%2520%2528EPPNet%2529%2520For%2520Explainable%2520Image%250A%2520%2520Classification%2520Via%2520Prototypes%26entry.906535625%3DBhushan%2520Atote%2520and%2520Victor%2520Sanchez%26entry.1292438233%3D%2520%2520Explainable%2520Artificial%2520Intelligence%2520%2528xAI%2529%2520has%2520the%2520potential%2520to%2520enhance%2520the%250Atransparency%2520and%2520trust%2520of%2520AI-based%2520systems.%2520Although%2520accurate%2520predictions%2520can%250Abe%2520made%2520using%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%252C%2520the%2520process%2520used%2520to%2520arrive%2520at%2520such%250Apredictions%2520is%2520usually%2520hard%2520to%2520explain.%2520In%2520terms%2520of%2520perceptibly%2520human-friendly%250Arepresentations%252C%2520such%2520as%2520word%2520phrases%2520in%2520text%2520or%2520super-pixels%2520in%2520images%252C%250Aprototype-based%2520explanations%2520can%2520justify%2520a%2520model%2527s%2520decision.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520a%2520DNN%2520architecture%2520for%2520image%2520classification%252C%2520the%2520Enhanced%250APrototypical%2520Part%2520Network%2520%2528EPPNet%2529%252C%2520which%2520achieves%2520strong%2520performance%2520while%250Adiscovering%2520relevant%2520prototypes%2520that%2520can%2520be%2520used%2520to%2520explain%2520the%2520classification%250Aresults.%2520This%2520is%2520achieved%2520by%2520introducing%2520a%2520novel%2520cluster%2520loss%2520that%2520helps%2520to%250Adiscover%2520more%2520relevant%2520human-understandable%2520prototypes.%2520We%2520also%2520introduce%2520a%250Afaithfulness%2520score%2520to%2520evaluate%2520the%2520explainability%2520of%2520the%2520results%2520based%2520on%2520the%250Adiscovered%2520prototypes.%2520Our%2520score%2520not%2520only%2520accounts%2520for%2520the%2520relevance%2520of%2520the%250Alearned%2520prototypes%2520but%2520also%2520the%2520performance%2520of%2520a%2520model.%2520Our%2520evaluations%2520on%2520the%250ACUB-200-2011%2520dataset%2520show%2520that%2520the%2520EPPNet%2520outperforms%2520state-of-the-art%250AxAI-based%2520methods%252C%2520in%2520terms%2520of%2520both%2520classification%2520accuracy%2520and%2520explainability%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Prototypical%20Part%20Network%20%28EPPNet%29%20For%20Explainable%20Image%0A%20%20Classification%20Via%20Prototypes&entry.906535625=Bhushan%20Atote%20and%20Victor%20Sanchez&entry.1292438233=%20%20Explainable%20Artificial%20Intelligence%20%28xAI%29%20has%20the%20potential%20to%20enhance%20the%0Atransparency%20and%20trust%20of%20AI-based%20systems.%20Although%20accurate%20predictions%20can%0Abe%20made%20using%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20the%20process%20used%20to%20arrive%20at%20such%0Apredictions%20is%20usually%20hard%20to%20explain.%20In%20terms%20of%20perceptibly%20human-friendly%0Arepresentations%2C%20such%20as%20word%20phrases%20in%20text%20or%20super-pixels%20in%20images%2C%0Aprototype-based%20explanations%20can%20justify%20a%20model%27s%20decision.%20In%20this%20work%2C%20we%0Aintroduce%20a%20DNN%20architecture%20for%20image%20classification%2C%20the%20Enhanced%0APrototypical%20Part%20Network%20%28EPPNet%29%2C%20which%20achieves%20strong%20performance%20while%0Adiscovering%20relevant%20prototypes%20that%20can%20be%20used%20to%20explain%20the%20classification%0Aresults.%20This%20is%20achieved%20by%20introducing%20a%20novel%20cluster%20loss%20that%20helps%20to%0Adiscover%20more%20relevant%20human-understandable%20prototypes.%20We%20also%20introduce%20a%0Afaithfulness%20score%20to%20evaluate%20the%20explainability%20of%20the%20results%20based%20on%20the%0Adiscovered%20prototypes.%20Our%20score%20not%20only%20accounts%20for%20the%20relevance%20of%20the%0Alearned%20prototypes%20but%20also%20the%20performance%20of%20a%20model.%20Our%20evaluations%20on%20the%0ACUB-200-2011%20dataset%20show%20that%20the%20EPPNet%20outperforms%20state-of-the-art%0AxAI-based%20methods%2C%20in%20terms%20of%20both%20classification%20accuracy%20and%20explainability%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04606v1&entry.124074799=Read"},
{"title": "UNMuTe: Unifying Navigation and Multimodal Dialogue-like Text Generation", "author": "Niyati Rawal and Roberto Bigazzi and Lorenzo Baraldi and Rita Cucchiara", "abstract": "  Smart autonomous agents are becoming increasingly important in various\nreal-life applications, including robotics and autonomous vehicles. One crucial\nskill that these agents must possess is the ability to interact with their\nsurrounding entities, such as other agents or humans. In this work, we aim at\nbuilding an intelligent agent that can efficiently navigate in an environment\nwhile being able to interact with an oracle (or human) in natural language and\nask for directions when it is unsure about its navigation performance. The\ninteraction is started by the agent that produces a question, which is then\nanswered by the oracle on the basis of the shortest trajectory to the goal. The\nprocess can be performed multiple times during navigation, thus enabling the\nagent to hold a dialogue with the oracle. To this end, we propose a novel\ncomputational model, named UNMuTe, that consists of two main components: a\ndialogue model and a navigator. Specifically, the dialogue model is based on a\nGPT-2 decoder that handles multimodal data consisting of both text and images.\nFirst, the dialogue model is trained to generate question-answer pairs: the\nquestion is generated using the current image, while the answer is produced\nleveraging future images on the path toward the goal. Subsequently, a VLN model\nis trained to follow the dialogue predicting navigation actions or triggering\nthe dialogue model if it needs help. In our experimental analysis, we show that\nUNMuTe achieves state-of-the-art performance on the main navigation tasks\nimplying dialogue, i.e. Cooperative Vision and Dialogue Navigation (CVDN) and\nNavigation from Dialogue History (NDH), proving that our approach is effective\nin generating useful questions and answers to guide navigation.\n", "link": "http://arxiv.org/abs/2408.04423v1", "date": "2024-08-08", "relevancy": 2.0762, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5208}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5206}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNMuTe%3A%20Unifying%20Navigation%20and%20Multimodal%20Dialogue-like%20Text%20Generation&body=Title%3A%20UNMuTe%3A%20Unifying%20Navigation%20and%20Multimodal%20Dialogue-like%20Text%20Generation%0AAuthor%3A%20Niyati%20Rawal%20and%20Roberto%20Bigazzi%20and%20Lorenzo%20Baraldi%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20Smart%20autonomous%20agents%20are%20becoming%20increasingly%20important%20in%20various%0Areal-life%20applications%2C%20including%20robotics%20and%20autonomous%20vehicles.%20One%20crucial%0Askill%20that%20these%20agents%20must%20possess%20is%20the%20ability%20to%20interact%20with%20their%0Asurrounding%20entities%2C%20such%20as%20other%20agents%20or%20humans.%20In%20this%20work%2C%20we%20aim%20at%0Abuilding%20an%20intelligent%20agent%20that%20can%20efficiently%20navigate%20in%20an%20environment%0Awhile%20being%20able%20to%20interact%20with%20an%20oracle%20%28or%20human%29%20in%20natural%20language%20and%0Aask%20for%20directions%20when%20it%20is%20unsure%20about%20its%20navigation%20performance.%20The%0Ainteraction%20is%20started%20by%20the%20agent%20that%20produces%20a%20question%2C%20which%20is%20then%0Aanswered%20by%20the%20oracle%20on%20the%20basis%20of%20the%20shortest%20trajectory%20to%20the%20goal.%20The%0Aprocess%20can%20be%20performed%20multiple%20times%20during%20navigation%2C%20thus%20enabling%20the%0Aagent%20to%20hold%20a%20dialogue%20with%20the%20oracle.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Acomputational%20model%2C%20named%20UNMuTe%2C%20that%20consists%20of%20two%20main%20components%3A%20a%0Adialogue%20model%20and%20a%20navigator.%20Specifically%2C%20the%20dialogue%20model%20is%20based%20on%20a%0AGPT-2%20decoder%20that%20handles%20multimodal%20data%20consisting%20of%20both%20text%20and%20images.%0AFirst%2C%20the%20dialogue%20model%20is%20trained%20to%20generate%20question-answer%20pairs%3A%20the%0Aquestion%20is%20generated%20using%20the%20current%20image%2C%20while%20the%20answer%20is%20produced%0Aleveraging%20future%20images%20on%20the%20path%20toward%20the%20goal.%20Subsequently%2C%20a%20VLN%20model%0Ais%20trained%20to%20follow%20the%20dialogue%20predicting%20navigation%20actions%20or%20triggering%0Athe%20dialogue%20model%20if%20it%20needs%20help.%20In%20our%20experimental%20analysis%2C%20we%20show%20that%0AUNMuTe%20achieves%20state-of-the-art%20performance%20on%20the%20main%20navigation%20tasks%0Aimplying%20dialogue%2C%20i.e.%20Cooperative%20Vision%20and%20Dialogue%20Navigation%20%28CVDN%29%20and%0ANavigation%20from%20Dialogue%20History%20%28NDH%29%2C%20proving%20that%20our%20approach%20is%20effective%0Ain%20generating%20useful%20questions%20and%20answers%20to%20guide%20navigation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04423v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNMuTe%253A%2520Unifying%2520Navigation%2520and%2520Multimodal%2520Dialogue-like%2520Text%2520Generation%26entry.906535625%3DNiyati%2520Rawal%2520and%2520Roberto%2520Bigazzi%2520and%2520Lorenzo%2520Baraldi%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3D%2520%2520Smart%2520autonomous%2520agents%2520are%2520becoming%2520increasingly%2520important%2520in%2520various%250Areal-life%2520applications%252C%2520including%2520robotics%2520and%2520autonomous%2520vehicles.%2520One%2520crucial%250Askill%2520that%2520these%2520agents%2520must%2520possess%2520is%2520the%2520ability%2520to%2520interact%2520with%2520their%250Asurrounding%2520entities%252C%2520such%2520as%2520other%2520agents%2520or%2520humans.%2520In%2520this%2520work%252C%2520we%2520aim%2520at%250Abuilding%2520an%2520intelligent%2520agent%2520that%2520can%2520efficiently%2520navigate%2520in%2520an%2520environment%250Awhile%2520being%2520able%2520to%2520interact%2520with%2520an%2520oracle%2520%2528or%2520human%2529%2520in%2520natural%2520language%2520and%250Aask%2520for%2520directions%2520when%2520it%2520is%2520unsure%2520about%2520its%2520navigation%2520performance.%2520The%250Ainteraction%2520is%2520started%2520by%2520the%2520agent%2520that%2520produces%2520a%2520question%252C%2520which%2520is%2520then%250Aanswered%2520by%2520the%2520oracle%2520on%2520the%2520basis%2520of%2520the%2520shortest%2520trajectory%2520to%2520the%2520goal.%2520The%250Aprocess%2520can%2520be%2520performed%2520multiple%2520times%2520during%2520navigation%252C%2520thus%2520enabling%2520the%250Aagent%2520to%2520hold%2520a%2520dialogue%2520with%2520the%2520oracle.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%250Acomputational%2520model%252C%2520named%2520UNMuTe%252C%2520that%2520consists%2520of%2520two%2520main%2520components%253A%2520a%250Adialogue%2520model%2520and%2520a%2520navigator.%2520Specifically%252C%2520the%2520dialogue%2520model%2520is%2520based%2520on%2520a%250AGPT-2%2520decoder%2520that%2520handles%2520multimodal%2520data%2520consisting%2520of%2520both%2520text%2520and%2520images.%250AFirst%252C%2520the%2520dialogue%2520model%2520is%2520trained%2520to%2520generate%2520question-answer%2520pairs%253A%2520the%250Aquestion%2520is%2520generated%2520using%2520the%2520current%2520image%252C%2520while%2520the%2520answer%2520is%2520produced%250Aleveraging%2520future%2520images%2520on%2520the%2520path%2520toward%2520the%2520goal.%2520Subsequently%252C%2520a%2520VLN%2520model%250Ais%2520trained%2520to%2520follow%2520the%2520dialogue%2520predicting%2520navigation%2520actions%2520or%2520triggering%250Athe%2520dialogue%2520model%2520if%2520it%2520needs%2520help.%2520In%2520our%2520experimental%2520analysis%252C%2520we%2520show%2520that%250AUNMuTe%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520main%2520navigation%2520tasks%250Aimplying%2520dialogue%252C%2520i.e.%2520Cooperative%2520Vision%2520and%2520Dialogue%2520Navigation%2520%2528CVDN%2529%2520and%250ANavigation%2520from%2520Dialogue%2520History%2520%2528NDH%2529%252C%2520proving%2520that%2520our%2520approach%2520is%2520effective%250Ain%2520generating%2520useful%2520questions%2520and%2520answers%2520to%2520guide%2520navigation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04423v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNMuTe%3A%20Unifying%20Navigation%20and%20Multimodal%20Dialogue-like%20Text%20Generation&entry.906535625=Niyati%20Rawal%20and%20Roberto%20Bigazzi%20and%20Lorenzo%20Baraldi%20and%20Rita%20Cucchiara&entry.1292438233=%20%20Smart%20autonomous%20agents%20are%20becoming%20increasingly%20important%20in%20various%0Areal-life%20applications%2C%20including%20robotics%20and%20autonomous%20vehicles.%20One%20crucial%0Askill%20that%20these%20agents%20must%20possess%20is%20the%20ability%20to%20interact%20with%20their%0Asurrounding%20entities%2C%20such%20as%20other%20agents%20or%20humans.%20In%20this%20work%2C%20we%20aim%20at%0Abuilding%20an%20intelligent%20agent%20that%20can%20efficiently%20navigate%20in%20an%20environment%0Awhile%20being%20able%20to%20interact%20with%20an%20oracle%20%28or%20human%29%20in%20natural%20language%20and%0Aask%20for%20directions%20when%20it%20is%20unsure%20about%20its%20navigation%20performance.%20The%0Ainteraction%20is%20started%20by%20the%20agent%20that%20produces%20a%20question%2C%20which%20is%20then%0Aanswered%20by%20the%20oracle%20on%20the%20basis%20of%20the%20shortest%20trajectory%20to%20the%20goal.%20The%0Aprocess%20can%20be%20performed%20multiple%20times%20during%20navigation%2C%20thus%20enabling%20the%0Aagent%20to%20hold%20a%20dialogue%20with%20the%20oracle.%20To%20this%20end%2C%20we%20propose%20a%20novel%0Acomputational%20model%2C%20named%20UNMuTe%2C%20that%20consists%20of%20two%20main%20components%3A%20a%0Adialogue%20model%20and%20a%20navigator.%20Specifically%2C%20the%20dialogue%20model%20is%20based%20on%20a%0AGPT-2%20decoder%20that%20handles%20multimodal%20data%20consisting%20of%20both%20text%20and%20images.%0AFirst%2C%20the%20dialogue%20model%20is%20trained%20to%20generate%20question-answer%20pairs%3A%20the%0Aquestion%20is%20generated%20using%20the%20current%20image%2C%20while%20the%20answer%20is%20produced%0Aleveraging%20future%20images%20on%20the%20path%20toward%20the%20goal.%20Subsequently%2C%20a%20VLN%20model%0Ais%20trained%20to%20follow%20the%20dialogue%20predicting%20navigation%20actions%20or%20triggering%0Athe%20dialogue%20model%20if%20it%20needs%20help.%20In%20our%20experimental%20analysis%2C%20we%20show%20that%0AUNMuTe%20achieves%20state-of-the-art%20performance%20on%20the%20main%20navigation%20tasks%0Aimplying%20dialogue%2C%20i.e.%20Cooperative%20Vision%20and%20Dialogue%20Navigation%20%28CVDN%29%20and%0ANavigation%20from%20Dialogue%20History%20%28NDH%29%2C%20proving%20that%20our%20approach%20is%20effective%0Ain%20generating%20useful%20questions%20and%20answers%20to%20guide%20navigation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04423v1&entry.124074799=Read"},
{"title": "Uncertainty for Active Learning on Graphs", "author": "Dominik Fuchsgruber and Tom Wollschl\u00e4ger and Bertrand Charpentier and Antonio Oroz and Stephan G\u00fcnnemann", "abstract": "  Uncertainty Sampling is an Active Learning strategy that aims to improve the\ndata efficiency of machine learning models by iteratively acquiring labels of\ndata points with the highest uncertainty. While it has proven effective for\nindependent data its applicability to graphs remains under-explored. We propose\nthe first extensive study of Uncertainty Sampling for node classification: (1)\nWe benchmark Uncertainty Sampling beyond predictive uncertainty and highlight a\nsignificant performance gap to other Active Learning strategies. (2) We develop\nground-truth Bayesian uncertainty estimates in terms of the data generating\nprocess and prove their effectiveness in guiding Uncertainty Sampling toward\noptimal queries. We confirm our results on synthetic data and design an\napproximate approach that consistently outperforms other uncertainty estimators\non real datasets. (3) Based on this analysis, we relate pitfalls in modeling\nuncertainty to existing methods. Our analysis enables and informs the\ndevelopment of principled uncertainty estimation on graphs.\n", "link": "http://arxiv.org/abs/2405.01462v2", "date": "2024-08-08", "relevancy": 2.0458, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5344}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.521}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20for%20Active%20Learning%20on%20Graphs&body=Title%3A%20Uncertainty%20for%20Active%20Learning%20on%20Graphs%0AAuthor%3A%20Dominik%20Fuchsgruber%20and%20Tom%20Wollschl%C3%A4ger%20and%20Bertrand%20Charpentier%20and%20Antonio%20Oroz%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20Uncertainty%20Sampling%20is%20an%20Active%20Learning%20strategy%20that%20aims%20to%20improve%20the%0Adata%20efficiency%20of%20machine%20learning%20models%20by%20iteratively%20acquiring%20labels%20of%0Adata%20points%20with%20the%20highest%20uncertainty.%20While%20it%20has%20proven%20effective%20for%0Aindependent%20data%20its%20applicability%20to%20graphs%20remains%20under-explored.%20We%20propose%0Athe%20first%20extensive%20study%20of%20Uncertainty%20Sampling%20for%20node%20classification%3A%20%281%29%0AWe%20benchmark%20Uncertainty%20Sampling%20beyond%20predictive%20uncertainty%20and%20highlight%20a%0Asignificant%20performance%20gap%20to%20other%20Active%20Learning%20strategies.%20%282%29%20We%20develop%0Aground-truth%20Bayesian%20uncertainty%20estimates%20in%20terms%20of%20the%20data%20generating%0Aprocess%20and%20prove%20their%20effectiveness%20in%20guiding%20Uncertainty%20Sampling%20toward%0Aoptimal%20queries.%20We%20confirm%20our%20results%20on%20synthetic%20data%20and%20design%20an%0Aapproximate%20approach%20that%20consistently%20outperforms%20other%20uncertainty%20estimators%0Aon%20real%20datasets.%20%283%29%20Based%20on%20this%20analysis%2C%20we%20relate%20pitfalls%20in%20modeling%0Auncertainty%20to%20existing%20methods.%20Our%20analysis%20enables%20and%20informs%20the%0Adevelopment%20of%20principled%20uncertainty%20estimation%20on%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01462v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520for%2520Active%2520Learning%2520on%2520Graphs%26entry.906535625%3DDominik%2520Fuchsgruber%2520and%2520Tom%2520Wollschl%25C3%25A4ger%2520and%2520Bertrand%2520Charpentier%2520and%2520Antonio%2520Oroz%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520Uncertainty%2520Sampling%2520is%2520an%2520Active%2520Learning%2520strategy%2520that%2520aims%2520to%2520improve%2520the%250Adata%2520efficiency%2520of%2520machine%2520learning%2520models%2520by%2520iteratively%2520acquiring%2520labels%2520of%250Adata%2520points%2520with%2520the%2520highest%2520uncertainty.%2520While%2520it%2520has%2520proven%2520effective%2520for%250Aindependent%2520data%2520its%2520applicability%2520to%2520graphs%2520remains%2520under-explored.%2520We%2520propose%250Athe%2520first%2520extensive%2520study%2520of%2520Uncertainty%2520Sampling%2520for%2520node%2520classification%253A%2520%25281%2529%250AWe%2520benchmark%2520Uncertainty%2520Sampling%2520beyond%2520predictive%2520uncertainty%2520and%2520highlight%2520a%250Asignificant%2520performance%2520gap%2520to%2520other%2520Active%2520Learning%2520strategies.%2520%25282%2529%2520We%2520develop%250Aground-truth%2520Bayesian%2520uncertainty%2520estimates%2520in%2520terms%2520of%2520the%2520data%2520generating%250Aprocess%2520and%2520prove%2520their%2520effectiveness%2520in%2520guiding%2520Uncertainty%2520Sampling%2520toward%250Aoptimal%2520queries.%2520We%2520confirm%2520our%2520results%2520on%2520synthetic%2520data%2520and%2520design%2520an%250Aapproximate%2520approach%2520that%2520consistently%2520outperforms%2520other%2520uncertainty%2520estimators%250Aon%2520real%2520datasets.%2520%25283%2529%2520Based%2520on%2520this%2520analysis%252C%2520we%2520relate%2520pitfalls%2520in%2520modeling%250Auncertainty%2520to%2520existing%2520methods.%2520Our%2520analysis%2520enables%2520and%2520informs%2520the%250Adevelopment%2520of%2520principled%2520uncertainty%2520estimation%2520on%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01462v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20for%20Active%20Learning%20on%20Graphs&entry.906535625=Dominik%20Fuchsgruber%20and%20Tom%20Wollschl%C3%A4ger%20and%20Bertrand%20Charpentier%20and%20Antonio%20Oroz%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20Uncertainty%20Sampling%20is%20an%20Active%20Learning%20strategy%20that%20aims%20to%20improve%20the%0Adata%20efficiency%20of%20machine%20learning%20models%20by%20iteratively%20acquiring%20labels%20of%0Adata%20points%20with%20the%20highest%20uncertainty.%20While%20it%20has%20proven%20effective%20for%0Aindependent%20data%20its%20applicability%20to%20graphs%20remains%20under-explored.%20We%20propose%0Athe%20first%20extensive%20study%20of%20Uncertainty%20Sampling%20for%20node%20classification%3A%20%281%29%0AWe%20benchmark%20Uncertainty%20Sampling%20beyond%20predictive%20uncertainty%20and%20highlight%20a%0Asignificant%20performance%20gap%20to%20other%20Active%20Learning%20strategies.%20%282%29%20We%20develop%0Aground-truth%20Bayesian%20uncertainty%20estimates%20in%20terms%20of%20the%20data%20generating%0Aprocess%20and%20prove%20their%20effectiveness%20in%20guiding%20Uncertainty%20Sampling%20toward%0Aoptimal%20queries.%20We%20confirm%20our%20results%20on%20synthetic%20data%20and%20design%20an%0Aapproximate%20approach%20that%20consistently%20outperforms%20other%20uncertainty%20estimators%0Aon%20real%20datasets.%20%283%29%20Based%20on%20this%20analysis%2C%20we%20relate%20pitfalls%20in%20modeling%0Auncertainty%20to%20existing%20methods.%20Our%20analysis%20enables%20and%20informs%20the%0Adevelopment%20of%20principled%20uncertainty%20estimation%20on%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01462v2&entry.124074799=Read"},
{"title": "Learn To Learn More Precisely", "author": "Runxi Cheng and Yongxian Wei and Xianglong He and Wanyun Zhu and Songsong Huang and Fei Richard Yu and Fei Ma and Chun Yuan", "abstract": "  Meta-learning has been extensively applied in the domains of few-shot\nlearning and fast adaptation, achieving remarkable performance. While\nMeta-learning methods like Model-Agnostic Meta-Learning (MAML) and its variants\nprovide a good set of initial parameters for the model, the model still tends\nto learn shortcut features, which leads to poor generalization. In this paper,\nwe propose the formal conception of \"learn to learn more precisely\", which aims\nto make the model learn precise target knowledge from data and reduce the\neffect of noisy knowledge, such as background and noise. To achieve this\ntarget, we proposed a simple and effective meta-learning framework named Meta\nSelf-Distillation(MSD) to maximize the consistency of learned knowledge,\nenhancing the models' ability to learn precise target knowledge. In the inner\nloop, MSD uses different augmented views of the same support data to update the\nmodel respectively. Then in the outer loop, MSD utilizes the same query data to\noptimize the consistency of learned knowledge, enhancing the model's ability to\nlearn more precisely. Our experiment demonstrates that MSD exhibits remarkable\nperformance in few-shot classification tasks in both standard and augmented\nscenarios, effectively boosting the accuracy and consistency of knowledge\nlearned by the model.\n", "link": "http://arxiv.org/abs/2408.04590v1", "date": "2024-08-08", "relevancy": 2.0441, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5262}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5003}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20To%20Learn%20More%20Precisely&body=Title%3A%20Learn%20To%20Learn%20More%20Precisely%0AAuthor%3A%20Runxi%20Cheng%20and%20Yongxian%20Wei%20and%20Xianglong%20He%20and%20Wanyun%20Zhu%20and%20Songsong%20Huang%20and%20Fei%20Richard%20Yu%20and%20Fei%20Ma%20and%20Chun%20Yuan%0AAbstract%3A%20%20%20Meta-learning%20has%20been%20extensively%20applied%20in%20the%20domains%20of%20few-shot%0Alearning%20and%20fast%20adaptation%2C%20achieving%20remarkable%20performance.%20While%0AMeta-learning%20methods%20like%20Model-Agnostic%20Meta-Learning%20%28MAML%29%20and%20its%20variants%0Aprovide%20a%20good%20set%20of%20initial%20parameters%20for%20the%20model%2C%20the%20model%20still%20tends%0Ato%20learn%20shortcut%20features%2C%20which%20leads%20to%20poor%20generalization.%20In%20this%20paper%2C%0Awe%20propose%20the%20formal%20conception%20of%20%22learn%20to%20learn%20more%20precisely%22%2C%20which%20aims%0Ato%20make%20the%20model%20learn%20precise%20target%20knowledge%20from%20data%20and%20reduce%20the%0Aeffect%20of%20noisy%20knowledge%2C%20such%20as%20background%20and%20noise.%20To%20achieve%20this%0Atarget%2C%20we%20proposed%20a%20simple%20and%20effective%20meta-learning%20framework%20named%20Meta%0ASelf-Distillation%28MSD%29%20to%20maximize%20the%20consistency%20of%20learned%20knowledge%2C%0Aenhancing%20the%20models%27%20ability%20to%20learn%20precise%20target%20knowledge.%20In%20the%20inner%0Aloop%2C%20MSD%20uses%20different%20augmented%20views%20of%20the%20same%20support%20data%20to%20update%20the%0Amodel%20respectively.%20Then%20in%20the%20outer%20loop%2C%20MSD%20utilizes%20the%20same%20query%20data%20to%0Aoptimize%20the%20consistency%20of%20learned%20knowledge%2C%20enhancing%20the%20model%27s%20ability%20to%0Alearn%20more%20precisely.%20Our%20experiment%20demonstrates%20that%20MSD%20exhibits%20remarkable%0Aperformance%20in%20few-shot%20classification%20tasks%20in%20both%20standard%20and%20augmented%0Ascenarios%2C%20effectively%20boosting%20the%20accuracy%20and%20consistency%20of%20knowledge%0Alearned%20by%20the%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520To%2520Learn%2520More%2520Precisely%26entry.906535625%3DRunxi%2520Cheng%2520and%2520Yongxian%2520Wei%2520and%2520Xianglong%2520He%2520and%2520Wanyun%2520Zhu%2520and%2520Songsong%2520Huang%2520and%2520Fei%2520Richard%2520Yu%2520and%2520Fei%2520Ma%2520and%2520Chun%2520Yuan%26entry.1292438233%3D%2520%2520Meta-learning%2520has%2520been%2520extensively%2520applied%2520in%2520the%2520domains%2520of%2520few-shot%250Alearning%2520and%2520fast%2520adaptation%252C%2520achieving%2520remarkable%2520performance.%2520While%250AMeta-learning%2520methods%2520like%2520Model-Agnostic%2520Meta-Learning%2520%2528MAML%2529%2520and%2520its%2520variants%250Aprovide%2520a%2520good%2520set%2520of%2520initial%2520parameters%2520for%2520the%2520model%252C%2520the%2520model%2520still%2520tends%250Ato%2520learn%2520shortcut%2520features%252C%2520which%2520leads%2520to%2520poor%2520generalization.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520the%2520formal%2520conception%2520of%2520%2522learn%2520to%2520learn%2520more%2520precisely%2522%252C%2520which%2520aims%250Ato%2520make%2520the%2520model%2520learn%2520precise%2520target%2520knowledge%2520from%2520data%2520and%2520reduce%2520the%250Aeffect%2520of%2520noisy%2520knowledge%252C%2520such%2520as%2520background%2520and%2520noise.%2520To%2520achieve%2520this%250Atarget%252C%2520we%2520proposed%2520a%2520simple%2520and%2520effective%2520meta-learning%2520framework%2520named%2520Meta%250ASelf-Distillation%2528MSD%2529%2520to%2520maximize%2520the%2520consistency%2520of%2520learned%2520knowledge%252C%250Aenhancing%2520the%2520models%2527%2520ability%2520to%2520learn%2520precise%2520target%2520knowledge.%2520In%2520the%2520inner%250Aloop%252C%2520MSD%2520uses%2520different%2520augmented%2520views%2520of%2520the%2520same%2520support%2520data%2520to%2520update%2520the%250Amodel%2520respectively.%2520Then%2520in%2520the%2520outer%2520loop%252C%2520MSD%2520utilizes%2520the%2520same%2520query%2520data%2520to%250Aoptimize%2520the%2520consistency%2520of%2520learned%2520knowledge%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%250Alearn%2520more%2520precisely.%2520Our%2520experiment%2520demonstrates%2520that%2520MSD%2520exhibits%2520remarkable%250Aperformance%2520in%2520few-shot%2520classification%2520tasks%2520in%2520both%2520standard%2520and%2520augmented%250Ascenarios%252C%2520effectively%2520boosting%2520the%2520accuracy%2520and%2520consistency%2520of%2520knowledge%250Alearned%2520by%2520the%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20To%20Learn%20More%20Precisely&entry.906535625=Runxi%20Cheng%20and%20Yongxian%20Wei%20and%20Xianglong%20He%20and%20Wanyun%20Zhu%20and%20Songsong%20Huang%20and%20Fei%20Richard%20Yu%20and%20Fei%20Ma%20and%20Chun%20Yuan&entry.1292438233=%20%20Meta-learning%20has%20been%20extensively%20applied%20in%20the%20domains%20of%20few-shot%0Alearning%20and%20fast%20adaptation%2C%20achieving%20remarkable%20performance.%20While%0AMeta-learning%20methods%20like%20Model-Agnostic%20Meta-Learning%20%28MAML%29%20and%20its%20variants%0Aprovide%20a%20good%20set%20of%20initial%20parameters%20for%20the%20model%2C%20the%20model%20still%20tends%0Ato%20learn%20shortcut%20features%2C%20which%20leads%20to%20poor%20generalization.%20In%20this%20paper%2C%0Awe%20propose%20the%20formal%20conception%20of%20%22learn%20to%20learn%20more%20precisely%22%2C%20which%20aims%0Ato%20make%20the%20model%20learn%20precise%20target%20knowledge%20from%20data%20and%20reduce%20the%0Aeffect%20of%20noisy%20knowledge%2C%20such%20as%20background%20and%20noise.%20To%20achieve%20this%0Atarget%2C%20we%20proposed%20a%20simple%20and%20effective%20meta-learning%20framework%20named%20Meta%0ASelf-Distillation%28MSD%29%20to%20maximize%20the%20consistency%20of%20learned%20knowledge%2C%0Aenhancing%20the%20models%27%20ability%20to%20learn%20precise%20target%20knowledge.%20In%20the%20inner%0Aloop%2C%20MSD%20uses%20different%20augmented%20views%20of%20the%20same%20support%20data%20to%20update%20the%0Amodel%20respectively.%20Then%20in%20the%20outer%20loop%2C%20MSD%20utilizes%20the%20same%20query%20data%20to%0Aoptimize%20the%20consistency%20of%20learned%20knowledge%2C%20enhancing%20the%20model%27s%20ability%20to%0Alearn%20more%20precisely.%20Our%20experiment%20demonstrates%20that%20MSD%20exhibits%20remarkable%0Aperformance%20in%20few-shot%20classification%20tasks%20in%20both%20standard%20and%20augmented%0Ascenarios%2C%20effectively%20boosting%20the%20accuracy%20and%20consistency%20of%20knowledge%0Alearned%20by%20the%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04590v1&entry.124074799=Read"},
{"title": "Towards Synergistic Deep Learning Models for Volumetric Cirrhotic Liver\n  Segmentation in MRIs", "author": "Vandan Gorade and Onkar Susladkar and Gorkem Durak and Elif Keles and Ertugrul Aktas and Timurhan Cebeci and Alpay Medetalibeyoglu and Daniela Ladner and Debesh Jha and Ulas Bagci", "abstract": "  Liver cirrhosis, a leading cause of global mortality, requires precise\nsegmentation of ROIs for effective disease monitoring and treatment planning.\nExisting segmentation models often fail to capture complex feature interactions\nand generalize across diverse datasets. To address these limitations, we\npropose a novel synergistic theory that leverages complementary latent spaces\nfor enhanced feature interaction modeling. Our proposed architecture,\nnnSynergyNet3D integrates continuous and discrete latent spaces for 3D volumes\nand features auto-configured training. This approach captures both fine-grained\nand coarse features, enabling effective modeling of intricate feature\ninteractions. We empirically validated nnSynergyNet3D on a private dataset of\n628 high-resolution T1 abdominal MRI scans from 339 patients. Our model\noutperformed the baseline nnUNet3D by approximately 2%. Additionally, zero-shot\ntesting on healthy liver CT scans from the public LiTS dataset demonstrated\nsuperior cross-modal generalization capabilities. These results highlight the\npotential of synergistic latent space models to improve segmentation accuracy\nand robustness, thereby enhancing clinical workflows by ensuring consistency\nacross CT and MRI modalities.\n", "link": "http://arxiv.org/abs/2408.04491v1", "date": "2024-08-08", "relevancy": 2.0428, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5156}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5151}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Synergistic%20Deep%20Learning%20Models%20for%20Volumetric%20Cirrhotic%20Liver%0A%20%20Segmentation%20in%20MRIs&body=Title%3A%20Towards%20Synergistic%20Deep%20Learning%20Models%20for%20Volumetric%20Cirrhotic%20Liver%0A%20%20Segmentation%20in%20MRIs%0AAuthor%3A%20Vandan%20Gorade%20and%20Onkar%20Susladkar%20and%20Gorkem%20Durak%20and%20Elif%20Keles%20and%20Ertugrul%20Aktas%20and%20Timurhan%20Cebeci%20and%20Alpay%20Medetalibeyoglu%20and%20Daniela%20Ladner%20and%20Debesh%20Jha%20and%20Ulas%20Bagci%0AAbstract%3A%20%20%20Liver%20cirrhosis%2C%20a%20leading%20cause%20of%20global%20mortality%2C%20requires%20precise%0Asegmentation%20of%20ROIs%20for%20effective%20disease%20monitoring%20and%20treatment%20planning.%0AExisting%20segmentation%20models%20often%20fail%20to%20capture%20complex%20feature%20interactions%0Aand%20generalize%20across%20diverse%20datasets.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20novel%20synergistic%20theory%20that%20leverages%20complementary%20latent%20spaces%0Afor%20enhanced%20feature%20interaction%20modeling.%20Our%20proposed%20architecture%2C%0AnnSynergyNet3D%20integrates%20continuous%20and%20discrete%20latent%20spaces%20for%203D%20volumes%0Aand%20features%20auto-configured%20training.%20This%20approach%20captures%20both%20fine-grained%0Aand%20coarse%20features%2C%20enabling%20effective%20modeling%20of%20intricate%20feature%0Ainteractions.%20We%20empirically%20validated%20nnSynergyNet3D%20on%20a%20private%20dataset%20of%0A628%20high-resolution%20T1%20abdominal%20MRI%20scans%20from%20339%20patients.%20Our%20model%0Aoutperformed%20the%20baseline%20nnUNet3D%20by%20approximately%202%25.%20Additionally%2C%20zero-shot%0Atesting%20on%20healthy%20liver%20CT%20scans%20from%20the%20public%20LiTS%20dataset%20demonstrated%0Asuperior%20cross-modal%20generalization%20capabilities.%20These%20results%20highlight%20the%0Apotential%20of%20synergistic%20latent%20space%20models%20to%20improve%20segmentation%20accuracy%0Aand%20robustness%2C%20thereby%20enhancing%20clinical%20workflows%20by%20ensuring%20consistency%0Aacross%20CT%20and%20MRI%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04491v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Synergistic%2520Deep%2520Learning%2520Models%2520for%2520Volumetric%2520Cirrhotic%2520Liver%250A%2520%2520Segmentation%2520in%2520MRIs%26entry.906535625%3DVandan%2520Gorade%2520and%2520Onkar%2520Susladkar%2520and%2520Gorkem%2520Durak%2520and%2520Elif%2520Keles%2520and%2520Ertugrul%2520Aktas%2520and%2520Timurhan%2520Cebeci%2520and%2520Alpay%2520Medetalibeyoglu%2520and%2520Daniela%2520Ladner%2520and%2520Debesh%2520Jha%2520and%2520Ulas%2520Bagci%26entry.1292438233%3D%2520%2520Liver%2520cirrhosis%252C%2520a%2520leading%2520cause%2520of%2520global%2520mortality%252C%2520requires%2520precise%250Asegmentation%2520of%2520ROIs%2520for%2520effective%2520disease%2520monitoring%2520and%2520treatment%2520planning.%250AExisting%2520segmentation%2520models%2520often%2520fail%2520to%2520capture%2520complex%2520feature%2520interactions%250Aand%2520generalize%2520across%2520diverse%2520datasets.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520novel%2520synergistic%2520theory%2520that%2520leverages%2520complementary%2520latent%2520spaces%250Afor%2520enhanced%2520feature%2520interaction%2520modeling.%2520Our%2520proposed%2520architecture%252C%250AnnSynergyNet3D%2520integrates%2520continuous%2520and%2520discrete%2520latent%2520spaces%2520for%25203D%2520volumes%250Aand%2520features%2520auto-configured%2520training.%2520This%2520approach%2520captures%2520both%2520fine-grained%250Aand%2520coarse%2520features%252C%2520enabling%2520effective%2520modeling%2520of%2520intricate%2520feature%250Ainteractions.%2520We%2520empirically%2520validated%2520nnSynergyNet3D%2520on%2520a%2520private%2520dataset%2520of%250A628%2520high-resolution%2520T1%2520abdominal%2520MRI%2520scans%2520from%2520339%2520patients.%2520Our%2520model%250Aoutperformed%2520the%2520baseline%2520nnUNet3D%2520by%2520approximately%25202%2525.%2520Additionally%252C%2520zero-shot%250Atesting%2520on%2520healthy%2520liver%2520CT%2520scans%2520from%2520the%2520public%2520LiTS%2520dataset%2520demonstrated%250Asuperior%2520cross-modal%2520generalization%2520capabilities.%2520These%2520results%2520highlight%2520the%250Apotential%2520of%2520synergistic%2520latent%2520space%2520models%2520to%2520improve%2520segmentation%2520accuracy%250Aand%2520robustness%252C%2520thereby%2520enhancing%2520clinical%2520workflows%2520by%2520ensuring%2520consistency%250Aacross%2520CT%2520and%2520MRI%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04491v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Synergistic%20Deep%20Learning%20Models%20for%20Volumetric%20Cirrhotic%20Liver%0A%20%20Segmentation%20in%20MRIs&entry.906535625=Vandan%20Gorade%20and%20Onkar%20Susladkar%20and%20Gorkem%20Durak%20and%20Elif%20Keles%20and%20Ertugrul%20Aktas%20and%20Timurhan%20Cebeci%20and%20Alpay%20Medetalibeyoglu%20and%20Daniela%20Ladner%20and%20Debesh%20Jha%20and%20Ulas%20Bagci&entry.1292438233=%20%20Liver%20cirrhosis%2C%20a%20leading%20cause%20of%20global%20mortality%2C%20requires%20precise%0Asegmentation%20of%20ROIs%20for%20effective%20disease%20monitoring%20and%20treatment%20planning.%0AExisting%20segmentation%20models%20often%20fail%20to%20capture%20complex%20feature%20interactions%0Aand%20generalize%20across%20diverse%20datasets.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20novel%20synergistic%20theory%20that%20leverages%20complementary%20latent%20spaces%0Afor%20enhanced%20feature%20interaction%20modeling.%20Our%20proposed%20architecture%2C%0AnnSynergyNet3D%20integrates%20continuous%20and%20discrete%20latent%20spaces%20for%203D%20volumes%0Aand%20features%20auto-configured%20training.%20This%20approach%20captures%20both%20fine-grained%0Aand%20coarse%20features%2C%20enabling%20effective%20modeling%20of%20intricate%20feature%0Ainteractions.%20We%20empirically%20validated%20nnSynergyNet3D%20on%20a%20private%20dataset%20of%0A628%20high-resolution%20T1%20abdominal%20MRI%20scans%20from%20339%20patients.%20Our%20model%0Aoutperformed%20the%20baseline%20nnUNet3D%20by%20approximately%202%25.%20Additionally%2C%20zero-shot%0Atesting%20on%20healthy%20liver%20CT%20scans%20from%20the%20public%20LiTS%20dataset%20demonstrated%0Asuperior%20cross-modal%20generalization%20capabilities.%20These%20results%20highlight%20the%0Apotential%20of%20synergistic%20latent%20space%20models%20to%20improve%20segmentation%20accuracy%0Aand%20robustness%2C%20thereby%20enhancing%20clinical%20workflows%20by%20ensuring%20consistency%0Aacross%20CT%20and%20MRI%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04491v1&entry.124074799=Read"},
{"title": "RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein\n  Segmentation and Classification", "author": "Jos\u00e9 Morano and Guilherme Aresta and Hrvoje Bogunovi\u0107", "abstract": "  The caliber and configuration of retinal blood vessels serve as important\nbiomarkers for various diseases and medical conditions. A thorough analysis of\nthe retinal vasculature requires the segmentation of the blood vessels and\ntheir classification into arteries and veins, typically performed on color\nfundus images obtained by retinography. However, manually performing these\ntasks is labor-intensive and prone to human error. While several automated\nmethods have been proposed to address this task, the current state of art faces\nchallenges due to manifest classification errors affecting the topological\nconsistency of segmentation maps. In this work, we introduce RRWNet, a novel\nend-to-end deep learning framework that addresses this limitation. The\nframework consists of a fully convolutional neural network that recursively\nrefines semantic segmentation maps, correcting manifest classification errors\nand thus improving topological consistency. In particular, RRWNet is composed\nof two specialized subnetworks: a Base subnetwork that generates base\nsegmentation maps from the input images, and a Recursive Refinement subnetwork\nthat iteratively and recursively improves these maps. Evaluation on three\ndifferent public datasets demonstrates the state-of-the-art performance of the\nproposed method, yielding more topologically consistent segmentation maps with\nfewer manifest classification errors than existing approaches. In addition, the\nRecursive Refinement module within RRWNet proves effective in post-processing\nsegmentation maps from other methods, further demonstrating its potential. The\nmodel code, weights, and predictions will be publicly available at\nhttps://github.com/j-morano/rrwnet.\n", "link": "http://arxiv.org/abs/2402.03166v4", "date": "2024-08-08", "relevancy": 2.0392, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5189}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5038}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RRWNet%3A%20Recursive%20Refinement%20Network%20for%20Effective%20Retinal%20Artery/Vein%0A%20%20Segmentation%20and%20Classification&body=Title%3A%20RRWNet%3A%20Recursive%20Refinement%20Network%20for%20Effective%20Retinal%20Artery/Vein%0A%20%20Segmentation%20and%20Classification%0AAuthor%3A%20Jos%C3%A9%20Morano%20and%20Guilherme%20Aresta%20and%20Hrvoje%20Bogunovi%C4%87%0AAbstract%3A%20%20%20The%20caliber%20and%20configuration%20of%20retinal%20blood%20vessels%20serve%20as%20important%0Abiomarkers%20for%20various%20diseases%20and%20medical%20conditions.%20A%20thorough%20analysis%20of%0Athe%20retinal%20vasculature%20requires%20the%20segmentation%20of%20the%20blood%20vessels%20and%0Atheir%20classification%20into%20arteries%20and%20veins%2C%20typically%20performed%20on%20color%0Afundus%20images%20obtained%20by%20retinography.%20However%2C%20manually%20performing%20these%0Atasks%20is%20labor-intensive%20and%20prone%20to%20human%20error.%20While%20several%20automated%0Amethods%20have%20been%20proposed%20to%20address%20this%20task%2C%20the%20current%20state%20of%20art%20faces%0Achallenges%20due%20to%20manifest%20classification%20errors%20affecting%20the%20topological%0Aconsistency%20of%20segmentation%20maps.%20In%20this%20work%2C%20we%20introduce%20RRWNet%2C%20a%20novel%0Aend-to-end%20deep%20learning%20framework%20that%20addresses%20this%20limitation.%20The%0Aframework%20consists%20of%20a%20fully%20convolutional%20neural%20network%20that%20recursively%0Arefines%20semantic%20segmentation%20maps%2C%20correcting%20manifest%20classification%20errors%0Aand%20thus%20improving%20topological%20consistency.%20In%20particular%2C%20RRWNet%20is%20composed%0Aof%20two%20specialized%20subnetworks%3A%20a%20Base%20subnetwork%20that%20generates%20base%0Asegmentation%20maps%20from%20the%20input%20images%2C%20and%20a%20Recursive%20Refinement%20subnetwork%0Athat%20iteratively%20and%20recursively%20improves%20these%20maps.%20Evaluation%20on%20three%0Adifferent%20public%20datasets%20demonstrates%20the%20state-of-the-art%20performance%20of%20the%0Aproposed%20method%2C%20yielding%20more%20topologically%20consistent%20segmentation%20maps%20with%0Afewer%20manifest%20classification%20errors%20than%20existing%20approaches.%20In%20addition%2C%20the%0ARecursive%20Refinement%20module%20within%20RRWNet%20proves%20effective%20in%20post-processing%0Asegmentation%20maps%20from%20other%20methods%2C%20further%20demonstrating%20its%20potential.%20The%0Amodel%20code%2C%20weights%2C%20and%20predictions%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/j-morano/rrwnet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03166v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRRWNet%253A%2520Recursive%2520Refinement%2520Network%2520for%2520Effective%2520Retinal%2520Artery/Vein%250A%2520%2520Segmentation%2520and%2520Classification%26entry.906535625%3DJos%25C3%25A9%2520Morano%2520and%2520Guilherme%2520Aresta%2520and%2520Hrvoje%2520Bogunovi%25C4%2587%26entry.1292438233%3D%2520%2520The%2520caliber%2520and%2520configuration%2520of%2520retinal%2520blood%2520vessels%2520serve%2520as%2520important%250Abiomarkers%2520for%2520various%2520diseases%2520and%2520medical%2520conditions.%2520A%2520thorough%2520analysis%2520of%250Athe%2520retinal%2520vasculature%2520requires%2520the%2520segmentation%2520of%2520the%2520blood%2520vessels%2520and%250Atheir%2520classification%2520into%2520arteries%2520and%2520veins%252C%2520typically%2520performed%2520on%2520color%250Afundus%2520images%2520obtained%2520by%2520retinography.%2520However%252C%2520manually%2520performing%2520these%250Atasks%2520is%2520labor-intensive%2520and%2520prone%2520to%2520human%2520error.%2520While%2520several%2520automated%250Amethods%2520have%2520been%2520proposed%2520to%2520address%2520this%2520task%252C%2520the%2520current%2520state%2520of%2520art%2520faces%250Achallenges%2520due%2520to%2520manifest%2520classification%2520errors%2520affecting%2520the%2520topological%250Aconsistency%2520of%2520segmentation%2520maps.%2520In%2520this%2520work%252C%2520we%2520introduce%2520RRWNet%252C%2520a%2520novel%250Aend-to-end%2520deep%2520learning%2520framework%2520that%2520addresses%2520this%2520limitation.%2520The%250Aframework%2520consists%2520of%2520a%2520fully%2520convolutional%2520neural%2520network%2520that%2520recursively%250Arefines%2520semantic%2520segmentation%2520maps%252C%2520correcting%2520manifest%2520classification%2520errors%250Aand%2520thus%2520improving%2520topological%2520consistency.%2520In%2520particular%252C%2520RRWNet%2520is%2520composed%250Aof%2520two%2520specialized%2520subnetworks%253A%2520a%2520Base%2520subnetwork%2520that%2520generates%2520base%250Asegmentation%2520maps%2520from%2520the%2520input%2520images%252C%2520and%2520a%2520Recursive%2520Refinement%2520subnetwork%250Athat%2520iteratively%2520and%2520recursively%2520improves%2520these%2520maps.%2520Evaluation%2520on%2520three%250Adifferent%2520public%2520datasets%2520demonstrates%2520the%2520state-of-the-art%2520performance%2520of%2520the%250Aproposed%2520method%252C%2520yielding%2520more%2520topologically%2520consistent%2520segmentation%2520maps%2520with%250Afewer%2520manifest%2520classification%2520errors%2520than%2520existing%2520approaches.%2520In%2520addition%252C%2520the%250ARecursive%2520Refinement%2520module%2520within%2520RRWNet%2520proves%2520effective%2520in%2520post-processing%250Asegmentation%2520maps%2520from%2520other%2520methods%252C%2520further%2520demonstrating%2520its%2520potential.%2520The%250Amodel%2520code%252C%2520weights%252C%2520and%2520predictions%2520will%2520be%2520publicly%2520available%2520at%250Ahttps%253A//github.com/j-morano/rrwnet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03166v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RRWNet%3A%20Recursive%20Refinement%20Network%20for%20Effective%20Retinal%20Artery/Vein%0A%20%20Segmentation%20and%20Classification&entry.906535625=Jos%C3%A9%20Morano%20and%20Guilherme%20Aresta%20and%20Hrvoje%20Bogunovi%C4%87&entry.1292438233=%20%20The%20caliber%20and%20configuration%20of%20retinal%20blood%20vessels%20serve%20as%20important%0Abiomarkers%20for%20various%20diseases%20and%20medical%20conditions.%20A%20thorough%20analysis%20of%0Athe%20retinal%20vasculature%20requires%20the%20segmentation%20of%20the%20blood%20vessels%20and%0Atheir%20classification%20into%20arteries%20and%20veins%2C%20typically%20performed%20on%20color%0Afundus%20images%20obtained%20by%20retinography.%20However%2C%20manually%20performing%20these%0Atasks%20is%20labor-intensive%20and%20prone%20to%20human%20error.%20While%20several%20automated%0Amethods%20have%20been%20proposed%20to%20address%20this%20task%2C%20the%20current%20state%20of%20art%20faces%0Achallenges%20due%20to%20manifest%20classification%20errors%20affecting%20the%20topological%0Aconsistency%20of%20segmentation%20maps.%20In%20this%20work%2C%20we%20introduce%20RRWNet%2C%20a%20novel%0Aend-to-end%20deep%20learning%20framework%20that%20addresses%20this%20limitation.%20The%0Aframework%20consists%20of%20a%20fully%20convolutional%20neural%20network%20that%20recursively%0Arefines%20semantic%20segmentation%20maps%2C%20correcting%20manifest%20classification%20errors%0Aand%20thus%20improving%20topological%20consistency.%20In%20particular%2C%20RRWNet%20is%20composed%0Aof%20two%20specialized%20subnetworks%3A%20a%20Base%20subnetwork%20that%20generates%20base%0Asegmentation%20maps%20from%20the%20input%20images%2C%20and%20a%20Recursive%20Refinement%20subnetwork%0Athat%20iteratively%20and%20recursively%20improves%20these%20maps.%20Evaluation%20on%20three%0Adifferent%20public%20datasets%20demonstrates%20the%20state-of-the-art%20performance%20of%20the%0Aproposed%20method%2C%20yielding%20more%20topologically%20consistent%20segmentation%20maps%20with%0Afewer%20manifest%20classification%20errors%20than%20existing%20approaches.%20In%20addition%2C%20the%0ARecursive%20Refinement%20module%20within%20RRWNet%20proves%20effective%20in%20post-processing%0Asegmentation%20maps%20from%20other%20methods%2C%20further%20demonstrating%20its%20potential.%20The%0Amodel%20code%2C%20weights%2C%20and%20predictions%20will%20be%20publicly%20available%20at%0Ahttps%3A//github.com/j-morano/rrwnet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03166v4&entry.124074799=Read"},
{"title": "3DSS-Mamba: 3D-Spectral-Spatial Mamba for Hyperspectral Image\n  Classification", "author": "Yan He and Bing Tu and Bo Liu and Jun Li and Antonio Plaza", "abstract": "  Hyperspectral image (HSI) classification constitutes the fundamental research\nin remote sensing fields. Convolutional Neural Networks (CNNs) and Transformers\nhave demonstrated impressive capability in capturing spectral-spatial\ncontextual dependencies. However, these architectures suffer from limited\nreceptive fields and quadratic computational complexity, respectively.\nFortunately, recent Mamba architectures built upon the State Space Model\nintegrate the advantages of long-range sequence modeling and linear\ncomputational efficiency, exhibiting substantial potential in low-dimensional\nscenarios. Motivated by this, we propose a novel 3D-Spectral-Spatial Mamba\n(3DSS-Mamba) framework for HSI classification, allowing for global\nspectral-spatial relationship modeling with greater computational efficiency.\nTechnically, a spectral-spatial token generation (SSTG) module is designed to\nconvert the HSI cube into a set of 3D spectral-spatial tokens. To overcome the\nlimitations of traditional Mamba, which is confined to modeling causal\nsequences and inadaptable to high-dimensional scenarios, a 3D-Spectral-Spatial\nSelective Scanning (3DSS) mechanism is introduced, which performs pixel-wise\nselective scanning on 3D hyperspectral tokens along the spectral and spatial\ndimensions. Five scanning routes are constructed to investigate the impact of\ndimension prioritization. The 3DSS scanning mechanism combined with\nconventional mapping operations forms the 3D-spectral-spatial mamba block\n(3DMB), enabling the extraction of global spectral-spatial semantic\nrepresentations. Experimental results and analysis demonstrate that the\nproposed method outperforms the state-of-the-art methods on HSI classification\nbenchmarks.\n", "link": "http://arxiv.org/abs/2405.12487v2", "date": "2024-08-08", "relevancy": 2.0344, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5139}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DSS-Mamba%3A%203D-Spectral-Spatial%20Mamba%20for%20Hyperspectral%20Image%0A%20%20Classification&body=Title%3A%203DSS-Mamba%3A%203D-Spectral-Spatial%20Mamba%20for%20Hyperspectral%20Image%0A%20%20Classification%0AAuthor%3A%20Yan%20He%20and%20Bing%20Tu%20and%20Bo%20Liu%20and%20Jun%20Li%20and%20Antonio%20Plaza%0AAbstract%3A%20%20%20Hyperspectral%20image%20%28HSI%29%20classification%20constitutes%20the%20fundamental%20research%0Ain%20remote%20sensing%20fields.%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformers%0Ahave%20demonstrated%20impressive%20capability%20in%20capturing%20spectral-spatial%0Acontextual%20dependencies.%20However%2C%20these%20architectures%20suffer%20from%20limited%0Areceptive%20fields%20and%20quadratic%20computational%20complexity%2C%20respectively.%0AFortunately%2C%20recent%20Mamba%20architectures%20built%20upon%20the%20State%20Space%20Model%0Aintegrate%20the%20advantages%20of%20long-range%20sequence%20modeling%20and%20linear%0Acomputational%20efficiency%2C%20exhibiting%20substantial%20potential%20in%20low-dimensional%0Ascenarios.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%203D-Spectral-Spatial%20Mamba%0A%283DSS-Mamba%29%20framework%20for%20HSI%20classification%2C%20allowing%20for%20global%0Aspectral-spatial%20relationship%20modeling%20with%20greater%20computational%20efficiency.%0ATechnically%2C%20a%20spectral-spatial%20token%20generation%20%28SSTG%29%20module%20is%20designed%20to%0Aconvert%20the%20HSI%20cube%20into%20a%20set%20of%203D%20spectral-spatial%20tokens.%20To%20overcome%20the%0Alimitations%20of%20traditional%20Mamba%2C%20which%20is%20confined%20to%20modeling%20causal%0Asequences%20and%20inadaptable%20to%20high-dimensional%20scenarios%2C%20a%203D-Spectral-Spatial%0ASelective%20Scanning%20%283DSS%29%20mechanism%20is%20introduced%2C%20which%20performs%20pixel-wise%0Aselective%20scanning%20on%203D%20hyperspectral%20tokens%20along%20the%20spectral%20and%20spatial%0Adimensions.%20Five%20scanning%20routes%20are%20constructed%20to%20investigate%20the%20impact%20of%0Adimension%20prioritization.%20The%203DSS%20scanning%20mechanism%20combined%20with%0Aconventional%20mapping%20operations%20forms%20the%203D-spectral-spatial%20mamba%20block%0A%283DMB%29%2C%20enabling%20the%20extraction%20of%20global%20spectral-spatial%20semantic%0Arepresentations.%20Experimental%20results%20and%20analysis%20demonstrate%20that%20the%0Aproposed%20method%20outperforms%20the%20state-of-the-art%20methods%20on%20HSI%20classification%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12487v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DSS-Mamba%253A%25203D-Spectral-Spatial%2520Mamba%2520for%2520Hyperspectral%2520Image%250A%2520%2520Classification%26entry.906535625%3DYan%2520He%2520and%2520Bing%2520Tu%2520and%2520Bo%2520Liu%2520and%2520Jun%2520Li%2520and%2520Antonio%2520Plaza%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520%2528HSI%2529%2520classification%2520constitutes%2520the%2520fundamental%2520research%250Ain%2520remote%2520sensing%2520fields.%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Transformers%250Ahave%2520demonstrated%2520impressive%2520capability%2520in%2520capturing%2520spectral-spatial%250Acontextual%2520dependencies.%2520However%252C%2520these%2520architectures%2520suffer%2520from%2520limited%250Areceptive%2520fields%2520and%2520quadratic%2520computational%2520complexity%252C%2520respectively.%250AFortunately%252C%2520recent%2520Mamba%2520architectures%2520built%2520upon%2520the%2520State%2520Space%2520Model%250Aintegrate%2520the%2520advantages%2520of%2520long-range%2520sequence%2520modeling%2520and%2520linear%250Acomputational%2520efficiency%252C%2520exhibiting%2520substantial%2520potential%2520in%2520low-dimensional%250Ascenarios.%2520Motivated%2520by%2520this%252C%2520we%2520propose%2520a%2520novel%25203D-Spectral-Spatial%2520Mamba%250A%25283DSS-Mamba%2529%2520framework%2520for%2520HSI%2520classification%252C%2520allowing%2520for%2520global%250Aspectral-spatial%2520relationship%2520modeling%2520with%2520greater%2520computational%2520efficiency.%250ATechnically%252C%2520a%2520spectral-spatial%2520token%2520generation%2520%2528SSTG%2529%2520module%2520is%2520designed%2520to%250Aconvert%2520the%2520HSI%2520cube%2520into%2520a%2520set%2520of%25203D%2520spectral-spatial%2520tokens.%2520To%2520overcome%2520the%250Alimitations%2520of%2520traditional%2520Mamba%252C%2520which%2520is%2520confined%2520to%2520modeling%2520causal%250Asequences%2520and%2520inadaptable%2520to%2520high-dimensional%2520scenarios%252C%2520a%25203D-Spectral-Spatial%250ASelective%2520Scanning%2520%25283DSS%2529%2520mechanism%2520is%2520introduced%252C%2520which%2520performs%2520pixel-wise%250Aselective%2520scanning%2520on%25203D%2520hyperspectral%2520tokens%2520along%2520the%2520spectral%2520and%2520spatial%250Adimensions.%2520Five%2520scanning%2520routes%2520are%2520constructed%2520to%2520investigate%2520the%2520impact%2520of%250Adimension%2520prioritization.%2520The%25203DSS%2520scanning%2520mechanism%2520combined%2520with%250Aconventional%2520mapping%2520operations%2520forms%2520the%25203D-spectral-spatial%2520mamba%2520block%250A%25283DMB%2529%252C%2520enabling%2520the%2520extraction%2520of%2520global%2520spectral-spatial%2520semantic%250Arepresentations.%2520Experimental%2520results%2520and%2520analysis%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520outperforms%2520the%2520state-of-the-art%2520methods%2520on%2520HSI%2520classification%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12487v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DSS-Mamba%3A%203D-Spectral-Spatial%20Mamba%20for%20Hyperspectral%20Image%0A%20%20Classification&entry.906535625=Yan%20He%20and%20Bing%20Tu%20and%20Bo%20Liu%20and%20Jun%20Li%20and%20Antonio%20Plaza&entry.1292438233=%20%20Hyperspectral%20image%20%28HSI%29%20classification%20constitutes%20the%20fundamental%20research%0Ain%20remote%20sensing%20fields.%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformers%0Ahave%20demonstrated%20impressive%20capability%20in%20capturing%20spectral-spatial%0Acontextual%20dependencies.%20However%2C%20these%20architectures%20suffer%20from%20limited%0Areceptive%20fields%20and%20quadratic%20computational%20complexity%2C%20respectively.%0AFortunately%2C%20recent%20Mamba%20architectures%20built%20upon%20the%20State%20Space%20Model%0Aintegrate%20the%20advantages%20of%20long-range%20sequence%20modeling%20and%20linear%0Acomputational%20efficiency%2C%20exhibiting%20substantial%20potential%20in%20low-dimensional%0Ascenarios.%20Motivated%20by%20this%2C%20we%20propose%20a%20novel%203D-Spectral-Spatial%20Mamba%0A%283DSS-Mamba%29%20framework%20for%20HSI%20classification%2C%20allowing%20for%20global%0Aspectral-spatial%20relationship%20modeling%20with%20greater%20computational%20efficiency.%0ATechnically%2C%20a%20spectral-spatial%20token%20generation%20%28SSTG%29%20module%20is%20designed%20to%0Aconvert%20the%20HSI%20cube%20into%20a%20set%20of%203D%20spectral-spatial%20tokens.%20To%20overcome%20the%0Alimitations%20of%20traditional%20Mamba%2C%20which%20is%20confined%20to%20modeling%20causal%0Asequences%20and%20inadaptable%20to%20high-dimensional%20scenarios%2C%20a%203D-Spectral-Spatial%0ASelective%20Scanning%20%283DSS%29%20mechanism%20is%20introduced%2C%20which%20performs%20pixel-wise%0Aselective%20scanning%20on%203D%20hyperspectral%20tokens%20along%20the%20spectral%20and%20spatial%0Adimensions.%20Five%20scanning%20routes%20are%20constructed%20to%20investigate%20the%20impact%20of%0Adimension%20prioritization.%20The%203DSS%20scanning%20mechanism%20combined%20with%0Aconventional%20mapping%20operations%20forms%20the%203D-spectral-spatial%20mamba%20block%0A%283DMB%29%2C%20enabling%20the%20extraction%20of%20global%20spectral-spatial%20semantic%0Arepresentations.%20Experimental%20results%20and%20analysis%20demonstrate%20that%20the%0Aproposed%20method%20outperforms%20the%20state-of-the-art%20methods%20on%20HSI%20classification%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12487v2&entry.124074799=Read"},
{"title": "Optimization Dynamics of Equivariant and Augmented Neural Networks", "author": "Oskar Nordenfors and Fredrik Ohlsson Axel Flinth", "abstract": "  We investigate the optimization of neural networks on symmetric data, and\ncompare the strategy of constraining the architecture to be equivariant to that\nof using data augmentation. Our analysis reveals that that the relative\ngeometry of the admissible and the equivariant layers, respectively, plays a\nkey role. Under natural assumptions on the data, network, loss, and group of\nsymmetries, we show that compatibility of the spaces of admissible layers and\nequivariant layers, in the sense that the corresponding orthogonal projections\ncommute, implies that the sets of equivariant stationary points are identical\nfor the two strategies. If the linear layers of the network also are given a\nunitary parametrization, the set of equivariant layers is even invariant under\nthe gradient flow for augmented models. Our analysis however also reveals that\neven in the latter situation, stationary points may be unstable for augmented\ntraining although they are stable for the manifestly equivariant models.\n", "link": "http://arxiv.org/abs/2303.13458v3", "date": "2024-08-08", "relevancy": 2.0264, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5521}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4763}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimization%20Dynamics%20of%20Equivariant%20and%20Augmented%20Neural%20Networks&body=Title%3A%20Optimization%20Dynamics%20of%20Equivariant%20and%20Augmented%20Neural%20Networks%0AAuthor%3A%20Oskar%20Nordenfors%20and%20Fredrik%20Ohlsson%20Axel%20Flinth%0AAbstract%3A%20%20%20We%20investigate%20the%20optimization%20of%20neural%20networks%20on%20symmetric%20data%2C%20and%0Acompare%20the%20strategy%20of%20constraining%20the%20architecture%20to%20be%20equivariant%20to%20that%0Aof%20using%20data%20augmentation.%20Our%20analysis%20reveals%20that%20that%20the%20relative%0Ageometry%20of%20the%20admissible%20and%20the%20equivariant%20layers%2C%20respectively%2C%20plays%20a%0Akey%20role.%20Under%20natural%20assumptions%20on%20the%20data%2C%20network%2C%20loss%2C%20and%20group%20of%0Asymmetries%2C%20we%20show%20that%20compatibility%20of%20the%20spaces%20of%20admissible%20layers%20and%0Aequivariant%20layers%2C%20in%20the%20sense%20that%20the%20corresponding%20orthogonal%20projections%0Acommute%2C%20implies%20that%20the%20sets%20of%20equivariant%20stationary%20points%20are%20identical%0Afor%20the%20two%20strategies.%20If%20the%20linear%20layers%20of%20the%20network%20also%20are%20given%20a%0Aunitary%20parametrization%2C%20the%20set%20of%20equivariant%20layers%20is%20even%20invariant%20under%0Athe%20gradient%20flow%20for%20augmented%20models.%20Our%20analysis%20however%20also%20reveals%20that%0Aeven%20in%20the%20latter%20situation%2C%20stationary%20points%20may%20be%20unstable%20for%20augmented%0Atraining%20although%20they%20are%20stable%20for%20the%20manifestly%20equivariant%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.13458v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimization%2520Dynamics%2520of%2520Equivariant%2520and%2520Augmented%2520Neural%2520Networks%26entry.906535625%3DOskar%2520Nordenfors%2520and%2520Fredrik%2520Ohlsson%2520Axel%2520Flinth%26entry.1292438233%3D%2520%2520We%2520investigate%2520the%2520optimization%2520of%2520neural%2520networks%2520on%2520symmetric%2520data%252C%2520and%250Acompare%2520the%2520strategy%2520of%2520constraining%2520the%2520architecture%2520to%2520be%2520equivariant%2520to%2520that%250Aof%2520using%2520data%2520augmentation.%2520Our%2520analysis%2520reveals%2520that%2520that%2520the%2520relative%250Ageometry%2520of%2520the%2520admissible%2520and%2520the%2520equivariant%2520layers%252C%2520respectively%252C%2520plays%2520a%250Akey%2520role.%2520Under%2520natural%2520assumptions%2520on%2520the%2520data%252C%2520network%252C%2520loss%252C%2520and%2520group%2520of%250Asymmetries%252C%2520we%2520show%2520that%2520compatibility%2520of%2520the%2520spaces%2520of%2520admissible%2520layers%2520and%250Aequivariant%2520layers%252C%2520in%2520the%2520sense%2520that%2520the%2520corresponding%2520orthogonal%2520projections%250Acommute%252C%2520implies%2520that%2520the%2520sets%2520of%2520equivariant%2520stationary%2520points%2520are%2520identical%250Afor%2520the%2520two%2520strategies.%2520If%2520the%2520linear%2520layers%2520of%2520the%2520network%2520also%2520are%2520given%2520a%250Aunitary%2520parametrization%252C%2520the%2520set%2520of%2520equivariant%2520layers%2520is%2520even%2520invariant%2520under%250Athe%2520gradient%2520flow%2520for%2520augmented%2520models.%2520Our%2520analysis%2520however%2520also%2520reveals%2520that%250Aeven%2520in%2520the%2520latter%2520situation%252C%2520stationary%2520points%2520may%2520be%2520unstable%2520for%2520augmented%250Atraining%2520although%2520they%2520are%2520stable%2520for%2520the%2520manifestly%2520equivariant%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.13458v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimization%20Dynamics%20of%20Equivariant%20and%20Augmented%20Neural%20Networks&entry.906535625=Oskar%20Nordenfors%20and%20Fredrik%20Ohlsson%20Axel%20Flinth&entry.1292438233=%20%20We%20investigate%20the%20optimization%20of%20neural%20networks%20on%20symmetric%20data%2C%20and%0Acompare%20the%20strategy%20of%20constraining%20the%20architecture%20to%20be%20equivariant%20to%20that%0Aof%20using%20data%20augmentation.%20Our%20analysis%20reveals%20that%20that%20the%20relative%0Ageometry%20of%20the%20admissible%20and%20the%20equivariant%20layers%2C%20respectively%2C%20plays%20a%0Akey%20role.%20Under%20natural%20assumptions%20on%20the%20data%2C%20network%2C%20loss%2C%20and%20group%20of%0Asymmetries%2C%20we%20show%20that%20compatibility%20of%20the%20spaces%20of%20admissible%20layers%20and%0Aequivariant%20layers%2C%20in%20the%20sense%20that%20the%20corresponding%20orthogonal%20projections%0Acommute%2C%20implies%20that%20the%20sets%20of%20equivariant%20stationary%20points%20are%20identical%0Afor%20the%20two%20strategies.%20If%20the%20linear%20layers%20of%20the%20network%20also%20are%20given%20a%0Aunitary%20parametrization%2C%20the%20set%20of%20equivariant%20layers%20is%20even%20invariant%20under%0Athe%20gradient%20flow%20for%20augmented%20models.%20Our%20analysis%20however%20also%20reveals%20that%0Aeven%20in%20the%20latter%20situation%2C%20stationary%20points%20may%20be%20unstable%20for%20augmented%0Atraining%20although%20they%20are%20stable%20for%20the%20manifestly%20equivariant%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.13458v3&entry.124074799=Read"},
{"title": "Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of\n  Large Language Models", "author": "Yupeng Chang and Yi Chang and Yuan Wu", "abstract": "  Large language models (LLMs) have exhibited remarkable proficiency across a\ndiverse array of natural language processing (NLP) tasks. However, adapting\nLLMs to downstream applications typically necessitates computationally\nintensive and memory-demanding fine-tuning procedures. To mitigate these\nburdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a\npromising approach to tailor LLMs with minimal computational overhead. While\nPEFT methods offer substantial advantages, they do not fully address the\npervasive issue of bias propagation from pre-training data. In this work, we\nintroduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method\ndesigned to counteract bias inheritance. BA-LoRA incorporates three distinct\nregularization terms: (1) consistency regularizer, (2) diversity regularizer,\nand (3) singular vector decomposition regularizer. These regularizers\ncollectively aim to improve the generative models' consistency, diversity, and\ngeneralization capabilities during the fine-tuning process. Through extensive\nexperiments on a variety of natural language understanding (NLU) and natural\nlanguage generation (NLG) tasks, employing prominent LLMs such as LLaMA,\nMistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of\nLoRA and its state-of-the-art variants. Moreover, our method effectively\nmitigates the deleterious effects of pre-training bias, leading to more\nreliable and robust model outputs. The code is available at\nhttps://github.com/cyp-jlu-ai/BA-LoRA.\n", "link": "http://arxiv.org/abs/2408.04556v1", "date": "2024-08-08", "relevancy": 2.0024, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5077}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4979}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bias-Aware%20Low-Rank%20Adaptation%3A%20Mitigating%20Catastrophic%20Inheritance%20of%0A%20%20Large%20Language%20Models&body=Title%3A%20Bias-Aware%20Low-Rank%20Adaptation%3A%20Mitigating%20Catastrophic%20Inheritance%20of%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yupeng%20Chang%20and%20Yi%20Chang%20and%20Yuan%20Wu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20remarkable%20proficiency%20across%20a%0Adiverse%20array%20of%20natural%20language%20processing%20%28NLP%29%20tasks.%20However%2C%20adapting%0ALLMs%20to%20downstream%20applications%20typically%20necessitates%20computationally%0Aintensive%20and%20memory-demanding%20fine-tuning%20procedures.%20To%20mitigate%20these%0Aburdens%2C%20parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20have%20emerged%20as%20a%0Apromising%20approach%20to%20tailor%20LLMs%20with%20minimal%20computational%20overhead.%20While%0APEFT%20methods%20offer%20substantial%20advantages%2C%20they%20do%20not%20fully%20address%20the%0Apervasive%20issue%20of%20bias%20propagation%20from%20pre-training%20data.%20In%20this%20work%2C%20we%0Aintroduce%20Bias-Aware%20Low-Rank%20Adaptation%20%28BA-LoRA%29%2C%20a%20novel%20PEFT%20method%0Adesigned%20to%20counteract%20bias%20inheritance.%20BA-LoRA%20incorporates%20three%20distinct%0Aregularization%20terms%3A%20%281%29%20consistency%20regularizer%2C%20%282%29%20diversity%20regularizer%2C%0Aand%20%283%29%20singular%20vector%20decomposition%20regularizer.%20These%20regularizers%0Acollectively%20aim%20to%20improve%20the%20generative%20models%27%20consistency%2C%20diversity%2C%20and%0Ageneralization%20capabilities%20during%20the%20fine-tuning%20process.%20Through%20extensive%0Aexperiments%20on%20a%20variety%20of%20natural%20language%20understanding%20%28NLU%29%20and%20natural%0Alanguage%20generation%20%28NLG%29%20tasks%2C%20employing%20prominent%20LLMs%20such%20as%20LLaMA%2C%0AMistral%2C%20and%20Gemma%2C%20we%20demonstrate%20that%20BA-LoRA%20surpasses%20the%20performance%20of%0ALoRA%20and%20its%20state-of-the-art%20variants.%20Moreover%2C%20our%20method%20effectively%0Amitigates%20the%20deleterious%20effects%20of%20pre-training%20bias%2C%20leading%20to%20more%0Areliable%20and%20robust%20model%20outputs.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/cyp-jlu-ai/BA-LoRA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBias-Aware%2520Low-Rank%2520Adaptation%253A%2520Mitigating%2520Catastrophic%2520Inheritance%2520of%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYupeng%2520Chang%2520and%2520Yi%2520Chang%2520and%2520Yuan%2520Wu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520exhibited%2520remarkable%2520proficiency%2520across%2520a%250Adiverse%2520array%2520of%2520natural%2520language%2520processing%2520%2528NLP%2529%2520tasks.%2520However%252C%2520adapting%250ALLMs%2520to%2520downstream%2520applications%2520typically%2520necessitates%2520computationally%250Aintensive%2520and%2520memory-demanding%2520fine-tuning%2520procedures.%2520To%2520mitigate%2520these%250Aburdens%252C%2520parameter-efficient%2520fine-tuning%2520%2528PEFT%2529%2520techniques%2520have%2520emerged%2520as%2520a%250Apromising%2520approach%2520to%2520tailor%2520LLMs%2520with%2520minimal%2520computational%2520overhead.%2520While%250APEFT%2520methods%2520offer%2520substantial%2520advantages%252C%2520they%2520do%2520not%2520fully%2520address%2520the%250Apervasive%2520issue%2520of%2520bias%2520propagation%2520from%2520pre-training%2520data.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520Bias-Aware%2520Low-Rank%2520Adaptation%2520%2528BA-LoRA%2529%252C%2520a%2520novel%2520PEFT%2520method%250Adesigned%2520to%2520counteract%2520bias%2520inheritance.%2520BA-LoRA%2520incorporates%2520three%2520distinct%250Aregularization%2520terms%253A%2520%25281%2529%2520consistency%2520regularizer%252C%2520%25282%2529%2520diversity%2520regularizer%252C%250Aand%2520%25283%2529%2520singular%2520vector%2520decomposition%2520regularizer.%2520These%2520regularizers%250Acollectively%2520aim%2520to%2520improve%2520the%2520generative%2520models%2527%2520consistency%252C%2520diversity%252C%2520and%250Ageneralization%2520capabilities%2520during%2520the%2520fine-tuning%2520process.%2520Through%2520extensive%250Aexperiments%2520on%2520a%2520variety%2520of%2520natural%2520language%2520understanding%2520%2528NLU%2529%2520and%2520natural%250Alanguage%2520generation%2520%2528NLG%2529%2520tasks%252C%2520employing%2520prominent%2520LLMs%2520such%2520as%2520LLaMA%252C%250AMistral%252C%2520and%2520Gemma%252C%2520we%2520demonstrate%2520that%2520BA-LoRA%2520surpasses%2520the%2520performance%2520of%250ALoRA%2520and%2520its%2520state-of-the-art%2520variants.%2520Moreover%252C%2520our%2520method%2520effectively%250Amitigates%2520the%2520deleterious%2520effects%2520of%2520pre-training%2520bias%252C%2520leading%2520to%2520more%250Areliable%2520and%2520robust%2520model%2520outputs.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/cyp-jlu-ai/BA-LoRA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias-Aware%20Low-Rank%20Adaptation%3A%20Mitigating%20Catastrophic%20Inheritance%20of%0A%20%20Large%20Language%20Models&entry.906535625=Yupeng%20Chang%20and%20Yi%20Chang%20and%20Yuan%20Wu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20remarkable%20proficiency%20across%20a%0Adiverse%20array%20of%20natural%20language%20processing%20%28NLP%29%20tasks.%20However%2C%20adapting%0ALLMs%20to%20downstream%20applications%20typically%20necessitates%20computationally%0Aintensive%20and%20memory-demanding%20fine-tuning%20procedures.%20To%20mitigate%20these%0Aburdens%2C%20parameter-efficient%20fine-tuning%20%28PEFT%29%20techniques%20have%20emerged%20as%20a%0Apromising%20approach%20to%20tailor%20LLMs%20with%20minimal%20computational%20overhead.%20While%0APEFT%20methods%20offer%20substantial%20advantages%2C%20they%20do%20not%20fully%20address%20the%0Apervasive%20issue%20of%20bias%20propagation%20from%20pre-training%20data.%20In%20this%20work%2C%20we%0Aintroduce%20Bias-Aware%20Low-Rank%20Adaptation%20%28BA-LoRA%29%2C%20a%20novel%20PEFT%20method%0Adesigned%20to%20counteract%20bias%20inheritance.%20BA-LoRA%20incorporates%20three%20distinct%0Aregularization%20terms%3A%20%281%29%20consistency%20regularizer%2C%20%282%29%20diversity%20regularizer%2C%0Aand%20%283%29%20singular%20vector%20decomposition%20regularizer.%20These%20regularizers%0Acollectively%20aim%20to%20improve%20the%20generative%20models%27%20consistency%2C%20diversity%2C%20and%0Ageneralization%20capabilities%20during%20the%20fine-tuning%20process.%20Through%20extensive%0Aexperiments%20on%20a%20variety%20of%20natural%20language%20understanding%20%28NLU%29%20and%20natural%0Alanguage%20generation%20%28NLG%29%20tasks%2C%20employing%20prominent%20LLMs%20such%20as%20LLaMA%2C%0AMistral%2C%20and%20Gemma%2C%20we%20demonstrate%20that%20BA-LoRA%20surpasses%20the%20performance%20of%0ALoRA%20and%20its%20state-of-the-art%20variants.%20Moreover%2C%20our%20method%20effectively%0Amitigates%20the%20deleterious%20effects%20of%20pre-training%20bias%2C%20leading%20to%20more%0Areliable%20and%20robust%20model%20outputs.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/cyp-jlu-ai/BA-LoRA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04556v1&entry.124074799=Read"},
{"title": "LiDAR-Event Stereo Fusion with Hallucinations", "author": "Luca Bartolomei and Matteo Poggi and Andrea Conti and Stefano Mattoccia", "abstract": "  Event stereo matching is an emerging technique to estimate depth from\nneuromorphic cameras; however, events are unlikely to trigger in the absence of\nmotion or the presence of large, untextured regions, making the correspondence\nproblem extremely challenging. Purposely, we propose integrating a stereo event\ncamera with a fixed-frequency active sensor -- e.g., a LiDAR -- collecting\nsparse depth measurements, overcoming the aforementioned limitations. Such\ndepth hints are used by hallucinating -- i.e., inserting fictitious events --\nthe stacks or raw input streams, compensating for the lack of information in\nthe absence of brightness changes. Our techniques are general, can be adapted\nto any structured representation to stack events and outperform\nstate-of-the-art fusion methods applied to event-based stereo.\n", "link": "http://arxiv.org/abs/2408.04633v1", "date": "2024-08-08", "relevancy": 2.0012, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5091}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4972}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-Event%20Stereo%20Fusion%20with%20Hallucinations&body=Title%3A%20LiDAR-Event%20Stereo%20Fusion%20with%20Hallucinations%0AAuthor%3A%20Luca%20Bartolomei%20and%20Matteo%20Poggi%20and%20Andrea%20Conti%20and%20Stefano%20Mattoccia%0AAbstract%3A%20%20%20Event%20stereo%20matching%20is%20an%20emerging%20technique%20to%20estimate%20depth%20from%0Aneuromorphic%20cameras%3B%20however%2C%20events%20are%20unlikely%20to%20trigger%20in%20the%20absence%20of%0Amotion%20or%20the%20presence%20of%20large%2C%20untextured%20regions%2C%20making%20the%20correspondence%0Aproblem%20extremely%20challenging.%20Purposely%2C%20we%20propose%20integrating%20a%20stereo%20event%0Acamera%20with%20a%20fixed-frequency%20active%20sensor%20--%20e.g.%2C%20a%20LiDAR%20--%20collecting%0Asparse%20depth%20measurements%2C%20overcoming%20the%20aforementioned%20limitations.%20Such%0Adepth%20hints%20are%20used%20by%20hallucinating%20--%20i.e.%2C%20inserting%20fictitious%20events%20--%0Athe%20stacks%20or%20raw%20input%20streams%2C%20compensating%20for%20the%20lack%20of%20information%20in%0Athe%20absence%20of%20brightness%20changes.%20Our%20techniques%20are%20general%2C%20can%20be%20adapted%0Ato%20any%20structured%20representation%20to%20stack%20events%20and%20outperform%0Astate-of-the-art%20fusion%20methods%20applied%20to%20event-based%20stereo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-Event%2520Stereo%2520Fusion%2520with%2520Hallucinations%26entry.906535625%3DLuca%2520Bartolomei%2520and%2520Matteo%2520Poggi%2520and%2520Andrea%2520Conti%2520and%2520Stefano%2520Mattoccia%26entry.1292438233%3D%2520%2520Event%2520stereo%2520matching%2520is%2520an%2520emerging%2520technique%2520to%2520estimate%2520depth%2520from%250Aneuromorphic%2520cameras%253B%2520however%252C%2520events%2520are%2520unlikely%2520to%2520trigger%2520in%2520the%2520absence%2520of%250Amotion%2520or%2520the%2520presence%2520of%2520large%252C%2520untextured%2520regions%252C%2520making%2520the%2520correspondence%250Aproblem%2520extremely%2520challenging.%2520Purposely%252C%2520we%2520propose%2520integrating%2520a%2520stereo%2520event%250Acamera%2520with%2520a%2520fixed-frequency%2520active%2520sensor%2520--%2520e.g.%252C%2520a%2520LiDAR%2520--%2520collecting%250Asparse%2520depth%2520measurements%252C%2520overcoming%2520the%2520aforementioned%2520limitations.%2520Such%250Adepth%2520hints%2520are%2520used%2520by%2520hallucinating%2520--%2520i.e.%252C%2520inserting%2520fictitious%2520events%2520--%250Athe%2520stacks%2520or%2520raw%2520input%2520streams%252C%2520compensating%2520for%2520the%2520lack%2520of%2520information%2520in%250Athe%2520absence%2520of%2520brightness%2520changes.%2520Our%2520techniques%2520are%2520general%252C%2520can%2520be%2520adapted%250Ato%2520any%2520structured%2520representation%2520to%2520stack%2520events%2520and%2520outperform%250Astate-of-the-art%2520fusion%2520methods%2520applied%2520to%2520event-based%2520stereo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-Event%20Stereo%20Fusion%20with%20Hallucinations&entry.906535625=Luca%20Bartolomei%20and%20Matteo%20Poggi%20and%20Andrea%20Conti%20and%20Stefano%20Mattoccia&entry.1292438233=%20%20Event%20stereo%20matching%20is%20an%20emerging%20technique%20to%20estimate%20depth%20from%0Aneuromorphic%20cameras%3B%20however%2C%20events%20are%20unlikely%20to%20trigger%20in%20the%20absence%20of%0Amotion%20or%20the%20presence%20of%20large%2C%20untextured%20regions%2C%20making%20the%20correspondence%0Aproblem%20extremely%20challenging.%20Purposely%2C%20we%20propose%20integrating%20a%20stereo%20event%0Acamera%20with%20a%20fixed-frequency%20active%20sensor%20--%20e.g.%2C%20a%20LiDAR%20--%20collecting%0Asparse%20depth%20measurements%2C%20overcoming%20the%20aforementioned%20limitations.%20Such%0Adepth%20hints%20are%20used%20by%20hallucinating%20--%20i.e.%2C%20inserting%20fictitious%20events%20--%0Athe%20stacks%20or%20raw%20input%20streams%2C%20compensating%20for%20the%20lack%20of%20information%20in%0Athe%20absence%20of%20brightness%20changes.%20Our%20techniques%20are%20general%2C%20can%20be%20adapted%0Ato%20any%20structured%20representation%20to%20stack%20events%20and%20outperform%0Astate-of-the-art%20fusion%20methods%20applied%20to%20event-based%20stereo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04633v1&entry.124074799=Read"},
{"title": "Improving Network Interpretability via Explanation Consistency\n  Evaluation", "author": "Hefeng Wu and Hao Jiang and Keze Wang and Ziyi Tang and Xianghuan He and Liang Lin", "abstract": "  While deep neural networks have achieved remarkable performance, they tend to\nlack transparency in prediction. The pursuit of greater interpretability in\nneural networks often results in a degradation of their original performance.\nSome works strive to improve both interpretability and performance, but they\nprimarily depend on meticulously imposed conditions. In this paper, we propose\na simple yet effective framework that acquires more explainable activation\nheatmaps and simultaneously increase the model performance, without the need\nfor any extra supervision. Specifically, our concise framework introduces a new\nmetric, i.e., explanation consistency, to reweight the training samples\nadaptively in model learning. The explanation consistency metric is utilized to\nmeasure the similarity between the model's visual explanations of the original\nsamples and those of semantic-preserved adversarial samples, whose background\nregions are perturbed by using image adversarial attack techniques. Our\nframework then promotes the model learning by paying closer attention to those\ntraining samples with a high difference in explanations (i.e., low explanation\nconsistency), for which the current model cannot provide robust\ninterpretations. Comprehensive experimental results on various benchmarks\ndemonstrate the superiority of our framework in multiple aspects, including\nhigher recognition accuracy, greater data debiasing capability, stronger\nnetwork robustness, and more precise localization ability on both regular\nnetworks and interpretable networks. We also provide extensive ablation studies\nand qualitative analyses to unveil the detailed contribution of each component.\n", "link": "http://arxiv.org/abs/2408.04600v1", "date": "2024-08-08", "relevancy": 2.0012, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5054}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.503}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Network%20Interpretability%20via%20Explanation%20Consistency%0A%20%20Evaluation&body=Title%3A%20Improving%20Network%20Interpretability%20via%20Explanation%20Consistency%0A%20%20Evaluation%0AAuthor%3A%20Hefeng%20Wu%20and%20Hao%20Jiang%20and%20Keze%20Wang%20and%20Ziyi%20Tang%20and%20Xianghuan%20He%20and%20Liang%20Lin%0AAbstract%3A%20%20%20While%20deep%20neural%20networks%20have%20achieved%20remarkable%20performance%2C%20they%20tend%20to%0Alack%20transparency%20in%20prediction.%20The%20pursuit%20of%20greater%20interpretability%20in%0Aneural%20networks%20often%20results%20in%20a%20degradation%20of%20their%20original%20performance.%0ASome%20works%20strive%20to%20improve%20both%20interpretability%20and%20performance%2C%20but%20they%0Aprimarily%20depend%20on%20meticulously%20imposed%20conditions.%20In%20this%20paper%2C%20we%20propose%0Aa%20simple%20yet%20effective%20framework%20that%20acquires%20more%20explainable%20activation%0Aheatmaps%20and%20simultaneously%20increase%20the%20model%20performance%2C%20without%20the%20need%0Afor%20any%20extra%20supervision.%20Specifically%2C%20our%20concise%20framework%20introduces%20a%20new%0Ametric%2C%20i.e.%2C%20explanation%20consistency%2C%20to%20reweight%20the%20training%20samples%0Aadaptively%20in%20model%20learning.%20The%20explanation%20consistency%20metric%20is%20utilized%20to%0Ameasure%20the%20similarity%20between%20the%20model%27s%20visual%20explanations%20of%20the%20original%0Asamples%20and%20those%20of%20semantic-preserved%20adversarial%20samples%2C%20whose%20background%0Aregions%20are%20perturbed%20by%20using%20image%20adversarial%20attack%20techniques.%20Our%0Aframework%20then%20promotes%20the%20model%20learning%20by%20paying%20closer%20attention%20to%20those%0Atraining%20samples%20with%20a%20high%20difference%20in%20explanations%20%28i.e.%2C%20low%20explanation%0Aconsistency%29%2C%20for%20which%20the%20current%20model%20cannot%20provide%20robust%0Ainterpretations.%20Comprehensive%20experimental%20results%20on%20various%20benchmarks%0Ademonstrate%20the%20superiority%20of%20our%20framework%20in%20multiple%20aspects%2C%20including%0Ahigher%20recognition%20accuracy%2C%20greater%20data%20debiasing%20capability%2C%20stronger%0Anetwork%20robustness%2C%20and%20more%20precise%20localization%20ability%20on%20both%20regular%0Anetworks%20and%20interpretable%20networks.%20We%20also%20provide%20extensive%20ablation%20studies%0Aand%20qualitative%20analyses%20to%20unveil%20the%20detailed%20contribution%20of%20each%20component.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Network%2520Interpretability%2520via%2520Explanation%2520Consistency%250A%2520%2520Evaluation%26entry.906535625%3DHefeng%2520Wu%2520and%2520Hao%2520Jiang%2520and%2520Keze%2520Wang%2520and%2520Ziyi%2520Tang%2520and%2520Xianghuan%2520He%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520While%2520deep%2520neural%2520networks%2520have%2520achieved%2520remarkable%2520performance%252C%2520they%2520tend%2520to%250Alack%2520transparency%2520in%2520prediction.%2520The%2520pursuit%2520of%2520greater%2520interpretability%2520in%250Aneural%2520networks%2520often%2520results%2520in%2520a%2520degradation%2520of%2520their%2520original%2520performance.%250ASome%2520works%2520strive%2520to%2520improve%2520both%2520interpretability%2520and%2520performance%252C%2520but%2520they%250Aprimarily%2520depend%2520on%2520meticulously%2520imposed%2520conditions.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520simple%2520yet%2520effective%2520framework%2520that%2520acquires%2520more%2520explainable%2520activation%250Aheatmaps%2520and%2520simultaneously%2520increase%2520the%2520model%2520performance%252C%2520without%2520the%2520need%250Afor%2520any%2520extra%2520supervision.%2520Specifically%252C%2520our%2520concise%2520framework%2520introduces%2520a%2520new%250Ametric%252C%2520i.e.%252C%2520explanation%2520consistency%252C%2520to%2520reweight%2520the%2520training%2520samples%250Aadaptively%2520in%2520model%2520learning.%2520The%2520explanation%2520consistency%2520metric%2520is%2520utilized%2520to%250Ameasure%2520the%2520similarity%2520between%2520the%2520model%2527s%2520visual%2520explanations%2520of%2520the%2520original%250Asamples%2520and%2520those%2520of%2520semantic-preserved%2520adversarial%2520samples%252C%2520whose%2520background%250Aregions%2520are%2520perturbed%2520by%2520using%2520image%2520adversarial%2520attack%2520techniques.%2520Our%250Aframework%2520then%2520promotes%2520the%2520model%2520learning%2520by%2520paying%2520closer%2520attention%2520to%2520those%250Atraining%2520samples%2520with%2520a%2520high%2520difference%2520in%2520explanations%2520%2528i.e.%252C%2520low%2520explanation%250Aconsistency%2529%252C%2520for%2520which%2520the%2520current%2520model%2520cannot%2520provide%2520robust%250Ainterpretations.%2520Comprehensive%2520experimental%2520results%2520on%2520various%2520benchmarks%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520framework%2520in%2520multiple%2520aspects%252C%2520including%250Ahigher%2520recognition%2520accuracy%252C%2520greater%2520data%2520debiasing%2520capability%252C%2520stronger%250Anetwork%2520robustness%252C%2520and%2520more%2520precise%2520localization%2520ability%2520on%2520both%2520regular%250Anetworks%2520and%2520interpretable%2520networks.%2520We%2520also%2520provide%2520extensive%2520ablation%2520studies%250Aand%2520qualitative%2520analyses%2520to%2520unveil%2520the%2520detailed%2520contribution%2520of%2520each%2520component.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Network%20Interpretability%20via%20Explanation%20Consistency%0A%20%20Evaluation&entry.906535625=Hefeng%20Wu%20and%20Hao%20Jiang%20and%20Keze%20Wang%20and%20Ziyi%20Tang%20and%20Xianghuan%20He%20and%20Liang%20Lin&entry.1292438233=%20%20While%20deep%20neural%20networks%20have%20achieved%20remarkable%20performance%2C%20they%20tend%20to%0Alack%20transparency%20in%20prediction.%20The%20pursuit%20of%20greater%20interpretability%20in%0Aneural%20networks%20often%20results%20in%20a%20degradation%20of%20their%20original%20performance.%0ASome%20works%20strive%20to%20improve%20both%20interpretability%20and%20performance%2C%20but%20they%0Aprimarily%20depend%20on%20meticulously%20imposed%20conditions.%20In%20this%20paper%2C%20we%20propose%0Aa%20simple%20yet%20effective%20framework%20that%20acquires%20more%20explainable%20activation%0Aheatmaps%20and%20simultaneously%20increase%20the%20model%20performance%2C%20without%20the%20need%0Afor%20any%20extra%20supervision.%20Specifically%2C%20our%20concise%20framework%20introduces%20a%20new%0Ametric%2C%20i.e.%2C%20explanation%20consistency%2C%20to%20reweight%20the%20training%20samples%0Aadaptively%20in%20model%20learning.%20The%20explanation%20consistency%20metric%20is%20utilized%20to%0Ameasure%20the%20similarity%20between%20the%20model%27s%20visual%20explanations%20of%20the%20original%0Asamples%20and%20those%20of%20semantic-preserved%20adversarial%20samples%2C%20whose%20background%0Aregions%20are%20perturbed%20by%20using%20image%20adversarial%20attack%20techniques.%20Our%0Aframework%20then%20promotes%20the%20model%20learning%20by%20paying%20closer%20attention%20to%20those%0Atraining%20samples%20with%20a%20high%20difference%20in%20explanations%20%28i.e.%2C%20low%20explanation%0Aconsistency%29%2C%20for%20which%20the%20current%20model%20cannot%20provide%20robust%0Ainterpretations.%20Comprehensive%20experimental%20results%20on%20various%20benchmarks%0Ademonstrate%20the%20superiority%20of%20our%20framework%20in%20multiple%20aspects%2C%20including%0Ahigher%20recognition%20accuracy%2C%20greater%20data%20debiasing%20capability%2C%20stronger%0Anetwork%20robustness%2C%20and%20more%20precise%20localization%20ability%20on%20both%20regular%0Anetworks%20and%20interpretable%20networks.%20We%20also%20provide%20extensive%20ablation%20studies%0Aand%20qualitative%20analyses%20to%20unveil%20the%20detailed%20contribution%20of%20each%20component.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04600v1&entry.124074799=Read"},
{"title": "A Learning-Based Model Predictive Contouring Control for Vehicle Evasive\n  Manoeuvres", "author": "Alberto Bertipaglia and Mohsen Alirezaei and Riender Happee and Barys Shyrokau", "abstract": "  This paper presents a novel Learning-based Model Predictive Contouring\nControl (L-MPCC) algorithm for evasive manoeuvres at the limit of handling. The\nalgorithm uses the Student-t Process (STP) to minimise model mismatches and\nuncertainties online. The proposed STP captures the mismatches between the\nprediction model and the measured lateral tyre forces and yaw rate. The\nmismatches correspond to the posterior means provided to the prediction model\nto improve its accuracy. Simultaneously, the posterior covariances are\npropagated to the vehicle lateral velocity and yaw rate along the prediction\nhorizon. The STP posterior covariance directly depends on the variance of\nobserved data, so its variance is more significant when the online measurements\ndiffer from the recorded ones in the training set and smaller in the opposite\ncase. Thus, these covariances can be utilised in the L-MPCC's cost function to\nminimise the vehicle state uncertainties. In a high-fidelity simulation\nenvironment, we demonstrate that the proposed L-MPCC can successfully avoid\nobstacles, keeping the vehicle stable while driving a double lane change\nmanoeuvre at a higher velocity than an MPCC without STP. Furthermore, the\nproposed controller yields a significantly lower peak sideslip angle, improving\nthe vehicle's manoeuvrability compared to an L-MPCC with a Gaussian Process.\n", "link": "http://arxiv.org/abs/2408.04485v1", "date": "2024-08-08", "relevancy": 1.9929, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.499}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Learning-Based%20Model%20Predictive%20Contouring%20Control%20for%20Vehicle%20Evasive%0A%20%20Manoeuvres&body=Title%3A%20A%20Learning-Based%20Model%20Predictive%20Contouring%20Control%20for%20Vehicle%20Evasive%0A%20%20Manoeuvres%0AAuthor%3A%20Alberto%20Bertipaglia%20and%20Mohsen%20Alirezaei%20and%20Riender%20Happee%20and%20Barys%20Shyrokau%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20Learning-based%20Model%20Predictive%20Contouring%0AControl%20%28L-MPCC%29%20algorithm%20for%20evasive%20manoeuvres%20at%20the%20limit%20of%20handling.%20The%0Aalgorithm%20uses%20the%20Student-t%20Process%20%28STP%29%20to%20minimise%20model%20mismatches%20and%0Auncertainties%20online.%20The%20proposed%20STP%20captures%20the%20mismatches%20between%20the%0Aprediction%20model%20and%20the%20measured%20lateral%20tyre%20forces%20and%20yaw%20rate.%20The%0Amismatches%20correspond%20to%20the%20posterior%20means%20provided%20to%20the%20prediction%20model%0Ato%20improve%20its%20accuracy.%20Simultaneously%2C%20the%20posterior%20covariances%20are%0Apropagated%20to%20the%20vehicle%20lateral%20velocity%20and%20yaw%20rate%20along%20the%20prediction%0Ahorizon.%20The%20STP%20posterior%20covariance%20directly%20depends%20on%20the%20variance%20of%0Aobserved%20data%2C%20so%20its%20variance%20is%20more%20significant%20when%20the%20online%20measurements%0Adiffer%20from%20the%20recorded%20ones%20in%20the%20training%20set%20and%20smaller%20in%20the%20opposite%0Acase.%20Thus%2C%20these%20covariances%20can%20be%20utilised%20in%20the%20L-MPCC%27s%20cost%20function%20to%0Aminimise%20the%20vehicle%20state%20uncertainties.%20In%20a%20high-fidelity%20simulation%0Aenvironment%2C%20we%20demonstrate%20that%20the%20proposed%20L-MPCC%20can%20successfully%20avoid%0Aobstacles%2C%20keeping%20the%20vehicle%20stable%20while%20driving%20a%20double%20lane%20change%0Amanoeuvre%20at%20a%20higher%20velocity%20than%20an%20MPCC%20without%20STP.%20Furthermore%2C%20the%0Aproposed%20controller%20yields%20a%20significantly%20lower%20peak%20sideslip%20angle%2C%20improving%0Athe%20vehicle%27s%20manoeuvrability%20compared%20to%20an%20L-MPCC%20with%20a%20Gaussian%20Process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Learning-Based%2520Model%2520Predictive%2520Contouring%2520Control%2520for%2520Vehicle%2520Evasive%250A%2520%2520Manoeuvres%26entry.906535625%3DAlberto%2520Bertipaglia%2520and%2520Mohsen%2520Alirezaei%2520and%2520Riender%2520Happee%2520and%2520Barys%2520Shyrokau%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520Learning-based%2520Model%2520Predictive%2520Contouring%250AControl%2520%2528L-MPCC%2529%2520algorithm%2520for%2520evasive%2520manoeuvres%2520at%2520the%2520limit%2520of%2520handling.%2520The%250Aalgorithm%2520uses%2520the%2520Student-t%2520Process%2520%2528STP%2529%2520to%2520minimise%2520model%2520mismatches%2520and%250Auncertainties%2520online.%2520The%2520proposed%2520STP%2520captures%2520the%2520mismatches%2520between%2520the%250Aprediction%2520model%2520and%2520the%2520measured%2520lateral%2520tyre%2520forces%2520and%2520yaw%2520rate.%2520The%250Amismatches%2520correspond%2520to%2520the%2520posterior%2520means%2520provided%2520to%2520the%2520prediction%2520model%250Ato%2520improve%2520its%2520accuracy.%2520Simultaneously%252C%2520the%2520posterior%2520covariances%2520are%250Apropagated%2520to%2520the%2520vehicle%2520lateral%2520velocity%2520and%2520yaw%2520rate%2520along%2520the%2520prediction%250Ahorizon.%2520The%2520STP%2520posterior%2520covariance%2520directly%2520depends%2520on%2520the%2520variance%2520of%250Aobserved%2520data%252C%2520so%2520its%2520variance%2520is%2520more%2520significant%2520when%2520the%2520online%2520measurements%250Adiffer%2520from%2520the%2520recorded%2520ones%2520in%2520the%2520training%2520set%2520and%2520smaller%2520in%2520the%2520opposite%250Acase.%2520Thus%252C%2520these%2520covariances%2520can%2520be%2520utilised%2520in%2520the%2520L-MPCC%2527s%2520cost%2520function%2520to%250Aminimise%2520the%2520vehicle%2520state%2520uncertainties.%2520In%2520a%2520high-fidelity%2520simulation%250Aenvironment%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%2520L-MPCC%2520can%2520successfully%2520avoid%250Aobstacles%252C%2520keeping%2520the%2520vehicle%2520stable%2520while%2520driving%2520a%2520double%2520lane%2520change%250Amanoeuvre%2520at%2520a%2520higher%2520velocity%2520than%2520an%2520MPCC%2520without%2520STP.%2520Furthermore%252C%2520the%250Aproposed%2520controller%2520yields%2520a%2520significantly%2520lower%2520peak%2520sideslip%2520angle%252C%2520improving%250Athe%2520vehicle%2527s%2520manoeuvrability%2520compared%2520to%2520an%2520L-MPCC%2520with%2520a%2520Gaussian%2520Process.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Learning-Based%20Model%20Predictive%20Contouring%20Control%20for%20Vehicle%20Evasive%0A%20%20Manoeuvres&entry.906535625=Alberto%20Bertipaglia%20and%20Mohsen%20Alirezaei%20and%20Riender%20Happee%20and%20Barys%20Shyrokau&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20Learning-based%20Model%20Predictive%20Contouring%0AControl%20%28L-MPCC%29%20algorithm%20for%20evasive%20manoeuvres%20at%20the%20limit%20of%20handling.%20The%0Aalgorithm%20uses%20the%20Student-t%20Process%20%28STP%29%20to%20minimise%20model%20mismatches%20and%0Auncertainties%20online.%20The%20proposed%20STP%20captures%20the%20mismatches%20between%20the%0Aprediction%20model%20and%20the%20measured%20lateral%20tyre%20forces%20and%20yaw%20rate.%20The%0Amismatches%20correspond%20to%20the%20posterior%20means%20provided%20to%20the%20prediction%20model%0Ato%20improve%20its%20accuracy.%20Simultaneously%2C%20the%20posterior%20covariances%20are%0Apropagated%20to%20the%20vehicle%20lateral%20velocity%20and%20yaw%20rate%20along%20the%20prediction%0Ahorizon.%20The%20STP%20posterior%20covariance%20directly%20depends%20on%20the%20variance%20of%0Aobserved%20data%2C%20so%20its%20variance%20is%20more%20significant%20when%20the%20online%20measurements%0Adiffer%20from%20the%20recorded%20ones%20in%20the%20training%20set%20and%20smaller%20in%20the%20opposite%0Acase.%20Thus%2C%20these%20covariances%20can%20be%20utilised%20in%20the%20L-MPCC%27s%20cost%20function%20to%0Aminimise%20the%20vehicle%20state%20uncertainties.%20In%20a%20high-fidelity%20simulation%0Aenvironment%2C%20we%20demonstrate%20that%20the%20proposed%20L-MPCC%20can%20successfully%20avoid%0Aobstacles%2C%20keeping%20the%20vehicle%20stable%20while%20driving%20a%20double%20lane%20change%0Amanoeuvre%20at%20a%20higher%20velocity%20than%20an%20MPCC%20without%20STP.%20Furthermore%2C%20the%0Aproposed%20controller%20yields%20a%20significantly%20lower%20peak%20sideslip%20angle%2C%20improving%0Athe%20vehicle%27s%20manoeuvrability%20compared%20to%20an%20L-MPCC%20with%20a%20Gaussian%20Process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04485v1&entry.124074799=Read"},
{"title": "An Autonomous GIS Agent Framework for Geospatial Data Retrieval", "author": "Huan Ning and Zhenlong Li and Temitope Akinboyewa and M. Naser Lessani", "abstract": "  Powered by the emerging large language models (LLMs), autonomous geographic\ninformation systems (GIS) agents have the potential to accomplish spatial\nanalyses and cartographic tasks. However, a research gap exists to support\nfully autonomous GIS agents: how to enable agents to discover and download the\nnecessary data for geospatial analyses. This study proposes an autonomous GIS\nagent framework capable of retrieving required geospatial data by generating,\nexecuting, and debugging programs. The framework utilizes the LLM as the\ndecision-maker, selects the appropriate data source (s) from a pre-defined\nsource list, and fetches the data from the chosen source. Each data source has\na handbook that records the metadata and technical details for data retrieval.\nThe proposed framework is designed in a plug-and-play style to ensure\nflexibility and extensibility. Human users or autonomous data scrawlers can add\nnew data sources by adding new handbooks. We developed a prototype agent based\non the framework, released as a QGIS plugin (GeoData Retrieve Agent) and a\nPython program. Experiment results demonstrate its capability of retrieving\ndata from various sources including OpenStreetMap, administrative boundaries\nand demographic data from the US Census Bureau, satellite basemaps from ESRI\nWorld Imagery, global digital elevation model (DEM) from OpenTopography.org,\nweather data from a commercial provider, the COVID-19 cases from the NYTimes\nGitHub. Our study is among the first attempts to develop an autonomous\ngeospatial data retrieval agent.\n", "link": "http://arxiv.org/abs/2407.21024v2", "date": "2024-08-08", "relevancy": 1.9923, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5036}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5006}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Autonomous%20GIS%20Agent%20Framework%20for%20Geospatial%20Data%20Retrieval&body=Title%3A%20An%20Autonomous%20GIS%20Agent%20Framework%20for%20Geospatial%20Data%20Retrieval%0AAuthor%3A%20Huan%20Ning%20and%20Zhenlong%20Li%20and%20Temitope%20Akinboyewa%20and%20M.%20Naser%20Lessani%0AAbstract%3A%20%20%20Powered%20by%20the%20emerging%20large%20language%20models%20%28LLMs%29%2C%20autonomous%20geographic%0Ainformation%20systems%20%28GIS%29%20agents%20have%20the%20potential%20to%20accomplish%20spatial%0Aanalyses%20and%20cartographic%20tasks.%20However%2C%20a%20research%20gap%20exists%20to%20support%0Afully%20autonomous%20GIS%20agents%3A%20how%20to%20enable%20agents%20to%20discover%20and%20download%20the%0Anecessary%20data%20for%20geospatial%20analyses.%20This%20study%20proposes%20an%20autonomous%20GIS%0Aagent%20framework%20capable%20of%20retrieving%20required%20geospatial%20data%20by%20generating%2C%0Aexecuting%2C%20and%20debugging%20programs.%20The%20framework%20utilizes%20the%20LLM%20as%20the%0Adecision-maker%2C%20selects%20the%20appropriate%20data%20source%20%28s%29%20from%20a%20pre-defined%0Asource%20list%2C%20and%20fetches%20the%20data%20from%20the%20chosen%20source.%20Each%20data%20source%20has%0Aa%20handbook%20that%20records%20the%20metadata%20and%20technical%20details%20for%20data%20retrieval.%0AThe%20proposed%20framework%20is%20designed%20in%20a%20plug-and-play%20style%20to%20ensure%0Aflexibility%20and%20extensibility.%20Human%20users%20or%20autonomous%20data%20scrawlers%20can%20add%0Anew%20data%20sources%20by%20adding%20new%20handbooks.%20We%20developed%20a%20prototype%20agent%20based%0Aon%20the%20framework%2C%20released%20as%20a%20QGIS%20plugin%20%28GeoData%20Retrieve%20Agent%29%20and%20a%0APython%20program.%20Experiment%20results%20demonstrate%20its%20capability%20of%20retrieving%0Adata%20from%20various%20sources%20including%20OpenStreetMap%2C%20administrative%20boundaries%0Aand%20demographic%20data%20from%20the%20US%20Census%20Bureau%2C%20satellite%20basemaps%20from%20ESRI%0AWorld%20Imagery%2C%20global%20digital%20elevation%20model%20%28DEM%29%20from%20OpenTopography.org%2C%0Aweather%20data%20from%20a%20commercial%20provider%2C%20the%20COVID-19%20cases%20from%20the%20NYTimes%0AGitHub.%20Our%20study%20is%20among%20the%20first%20attempts%20to%20develop%20an%20autonomous%0Ageospatial%20data%20retrieval%20agent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Autonomous%2520GIS%2520Agent%2520Framework%2520for%2520Geospatial%2520Data%2520Retrieval%26entry.906535625%3DHuan%2520Ning%2520and%2520Zhenlong%2520Li%2520and%2520Temitope%2520Akinboyewa%2520and%2520M.%2520Naser%2520Lessani%26entry.1292438233%3D%2520%2520Powered%2520by%2520the%2520emerging%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520autonomous%2520geographic%250Ainformation%2520systems%2520%2528GIS%2529%2520agents%2520have%2520the%2520potential%2520to%2520accomplish%2520spatial%250Aanalyses%2520and%2520cartographic%2520tasks.%2520However%252C%2520a%2520research%2520gap%2520exists%2520to%2520support%250Afully%2520autonomous%2520GIS%2520agents%253A%2520how%2520to%2520enable%2520agents%2520to%2520discover%2520and%2520download%2520the%250Anecessary%2520data%2520for%2520geospatial%2520analyses.%2520This%2520study%2520proposes%2520an%2520autonomous%2520GIS%250Aagent%2520framework%2520capable%2520of%2520retrieving%2520required%2520geospatial%2520data%2520by%2520generating%252C%250Aexecuting%252C%2520and%2520debugging%2520programs.%2520The%2520framework%2520utilizes%2520the%2520LLM%2520as%2520the%250Adecision-maker%252C%2520selects%2520the%2520appropriate%2520data%2520source%2520%2528s%2529%2520from%2520a%2520pre-defined%250Asource%2520list%252C%2520and%2520fetches%2520the%2520data%2520from%2520the%2520chosen%2520source.%2520Each%2520data%2520source%2520has%250Aa%2520handbook%2520that%2520records%2520the%2520metadata%2520and%2520technical%2520details%2520for%2520data%2520retrieval.%250AThe%2520proposed%2520framework%2520is%2520designed%2520in%2520a%2520plug-and-play%2520style%2520to%2520ensure%250Aflexibility%2520and%2520extensibility.%2520Human%2520users%2520or%2520autonomous%2520data%2520scrawlers%2520can%2520add%250Anew%2520data%2520sources%2520by%2520adding%2520new%2520handbooks.%2520We%2520developed%2520a%2520prototype%2520agent%2520based%250Aon%2520the%2520framework%252C%2520released%2520as%2520a%2520QGIS%2520plugin%2520%2528GeoData%2520Retrieve%2520Agent%2529%2520and%2520a%250APython%2520program.%2520Experiment%2520results%2520demonstrate%2520its%2520capability%2520of%2520retrieving%250Adata%2520from%2520various%2520sources%2520including%2520OpenStreetMap%252C%2520administrative%2520boundaries%250Aand%2520demographic%2520data%2520from%2520the%2520US%2520Census%2520Bureau%252C%2520satellite%2520basemaps%2520from%2520ESRI%250AWorld%2520Imagery%252C%2520global%2520digital%2520elevation%2520model%2520%2528DEM%2529%2520from%2520OpenTopography.org%252C%250Aweather%2520data%2520from%2520a%2520commercial%2520provider%252C%2520the%2520COVID-19%2520cases%2520from%2520the%2520NYTimes%250AGitHub.%2520Our%2520study%2520is%2520among%2520the%2520first%2520attempts%2520to%2520develop%2520an%2520autonomous%250Ageospatial%2520data%2520retrieval%2520agent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Autonomous%20GIS%20Agent%20Framework%20for%20Geospatial%20Data%20Retrieval&entry.906535625=Huan%20Ning%20and%20Zhenlong%20Li%20and%20Temitope%20Akinboyewa%20and%20M.%20Naser%20Lessani&entry.1292438233=%20%20Powered%20by%20the%20emerging%20large%20language%20models%20%28LLMs%29%2C%20autonomous%20geographic%0Ainformation%20systems%20%28GIS%29%20agents%20have%20the%20potential%20to%20accomplish%20spatial%0Aanalyses%20and%20cartographic%20tasks.%20However%2C%20a%20research%20gap%20exists%20to%20support%0Afully%20autonomous%20GIS%20agents%3A%20how%20to%20enable%20agents%20to%20discover%20and%20download%20the%0Anecessary%20data%20for%20geospatial%20analyses.%20This%20study%20proposes%20an%20autonomous%20GIS%0Aagent%20framework%20capable%20of%20retrieving%20required%20geospatial%20data%20by%20generating%2C%0Aexecuting%2C%20and%20debugging%20programs.%20The%20framework%20utilizes%20the%20LLM%20as%20the%0Adecision-maker%2C%20selects%20the%20appropriate%20data%20source%20%28s%29%20from%20a%20pre-defined%0Asource%20list%2C%20and%20fetches%20the%20data%20from%20the%20chosen%20source.%20Each%20data%20source%20has%0Aa%20handbook%20that%20records%20the%20metadata%20and%20technical%20details%20for%20data%20retrieval.%0AThe%20proposed%20framework%20is%20designed%20in%20a%20plug-and-play%20style%20to%20ensure%0Aflexibility%20and%20extensibility.%20Human%20users%20or%20autonomous%20data%20scrawlers%20can%20add%0Anew%20data%20sources%20by%20adding%20new%20handbooks.%20We%20developed%20a%20prototype%20agent%20based%0Aon%20the%20framework%2C%20released%20as%20a%20QGIS%20plugin%20%28GeoData%20Retrieve%20Agent%29%20and%20a%0APython%20program.%20Experiment%20results%20demonstrate%20its%20capability%20of%20retrieving%0Adata%20from%20various%20sources%20including%20OpenStreetMap%2C%20administrative%20boundaries%0Aand%20demographic%20data%20from%20the%20US%20Census%20Bureau%2C%20satellite%20basemaps%20from%20ESRI%0AWorld%20Imagery%2C%20global%20digital%20elevation%20model%20%28DEM%29%20from%20OpenTopography.org%2C%0Aweather%20data%20from%20a%20commercial%20provider%2C%20the%20COVID-19%20cases%20from%20the%20NYTimes%0AGitHub.%20Our%20study%20is%20among%20the%20first%20attempts%20to%20develop%20an%20autonomous%0Ageospatial%20data%20retrieval%20agent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21024v2&entry.124074799=Read"},
{"title": "Synchronous Multi-modal Semantic CommunicationSystem with Packet-level\n  Coding", "author": "Yun Tian and Jingkai Ying and Zhijin Qin and Ye Jin and Xiaoming Tao", "abstract": "  Although the semantic communication with joint semantic-channel coding design\nhas shown promising performance in transmitting data of different modalities\nover physical layer channels, the synchronization and packet-level forward\nerror correction of multimodal semantics have not been well studied. Due to the\nindependent design of semantic encoders, synchronizing multimodal features in\nboth the semantic and time domains is a challenging problem. In this paper, we\ntake the facial video and speech transmission as an example and propose a\nSynchronous Multimodal Semantic Communication System (SyncSC) with Packet-Level\nCoding. To achieve semantic and time synchronization, 3D Morphable Mode (3DMM)\ncoefficients and text are transmitted as semantics, and we propose a semantic\ncodec that achieves similar quality of reconstruction and synchronization with\nlower bandwidth, compared to traditional methods. To protect semantic packets\nunder the erasure channel, we propose a packet-Level Forward Error Correction\n(FEC) method, called PacSC, that maintains a certain visual quality performance\neven at high packet loss rates. Particularly, for text packets, a text packet\nloss concealment module, called TextPC, based on Bidirectional Encoder\nRepresentations from Transformers (BERT) is proposed, which significantly\nimproves the performance of traditional FEC methods. The simulation results\nshow that our proposed SyncSC reduce transmission overhead and achieve\nhigh-quality synchronous transmission of video and speech over the packet loss\nnetwork.\n", "link": "http://arxiv.org/abs/2408.04535v1", "date": "2024-08-08", "relevancy": 1.98, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5203}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synchronous%20Multi-modal%20Semantic%20CommunicationSystem%20with%20Packet-level%0A%20%20Coding&body=Title%3A%20Synchronous%20Multi-modal%20Semantic%20CommunicationSystem%20with%20Packet-level%0A%20%20Coding%0AAuthor%3A%20Yun%20Tian%20and%20Jingkai%20Ying%20and%20Zhijin%20Qin%20and%20Ye%20Jin%20and%20Xiaoming%20Tao%0AAbstract%3A%20%20%20Although%20the%20semantic%20communication%20with%20joint%20semantic-channel%20coding%20design%0Ahas%20shown%20promising%20performance%20in%20transmitting%20data%20of%20different%20modalities%0Aover%20physical%20layer%20channels%2C%20the%20synchronization%20and%20packet-level%20forward%0Aerror%20correction%20of%20multimodal%20semantics%20have%20not%20been%20well%20studied.%20Due%20to%20the%0Aindependent%20design%20of%20semantic%20encoders%2C%20synchronizing%20multimodal%20features%20in%0Aboth%20the%20semantic%20and%20time%20domains%20is%20a%20challenging%20problem.%20In%20this%20paper%2C%20we%0Atake%20the%20facial%20video%20and%20speech%20transmission%20as%20an%20example%20and%20propose%20a%0ASynchronous%20Multimodal%20Semantic%20Communication%20System%20%28SyncSC%29%20with%20Packet-Level%0ACoding.%20To%20achieve%20semantic%20and%20time%20synchronization%2C%203D%20Morphable%20Mode%20%283DMM%29%0Acoefficients%20and%20text%20are%20transmitted%20as%20semantics%2C%20and%20we%20propose%20a%20semantic%0Acodec%20that%20achieves%20similar%20quality%20of%20reconstruction%20and%20synchronization%20with%0Alower%20bandwidth%2C%20compared%20to%20traditional%20methods.%20To%20protect%20semantic%20packets%0Aunder%20the%20erasure%20channel%2C%20we%20propose%20a%20packet-Level%20Forward%20Error%20Correction%0A%28FEC%29%20method%2C%20called%20PacSC%2C%20that%20maintains%20a%20certain%20visual%20quality%20performance%0Aeven%20at%20high%20packet%20loss%20rates.%20Particularly%2C%20for%20text%20packets%2C%20a%20text%20packet%0Aloss%20concealment%20module%2C%20called%20TextPC%2C%20based%20on%20Bidirectional%20Encoder%0ARepresentations%20from%20Transformers%20%28BERT%29%20is%20proposed%2C%20which%20significantly%0Aimproves%20the%20performance%20of%20traditional%20FEC%20methods.%20The%20simulation%20results%0Ashow%20that%20our%20proposed%20SyncSC%20reduce%20transmission%20overhead%20and%20achieve%0Ahigh-quality%20synchronous%20transmission%20of%20video%20and%20speech%20over%20the%20packet%20loss%0Anetwork.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynchronous%2520Multi-modal%2520Semantic%2520CommunicationSystem%2520with%2520Packet-level%250A%2520%2520Coding%26entry.906535625%3DYun%2520Tian%2520and%2520Jingkai%2520Ying%2520and%2520Zhijin%2520Qin%2520and%2520Ye%2520Jin%2520and%2520Xiaoming%2520Tao%26entry.1292438233%3D%2520%2520Although%2520the%2520semantic%2520communication%2520with%2520joint%2520semantic-channel%2520coding%2520design%250Ahas%2520shown%2520promising%2520performance%2520in%2520transmitting%2520data%2520of%2520different%2520modalities%250Aover%2520physical%2520layer%2520channels%252C%2520the%2520synchronization%2520and%2520packet-level%2520forward%250Aerror%2520correction%2520of%2520multimodal%2520semantics%2520have%2520not%2520been%2520well%2520studied.%2520Due%2520to%2520the%250Aindependent%2520design%2520of%2520semantic%2520encoders%252C%2520synchronizing%2520multimodal%2520features%2520in%250Aboth%2520the%2520semantic%2520and%2520time%2520domains%2520is%2520a%2520challenging%2520problem.%2520In%2520this%2520paper%252C%2520we%250Atake%2520the%2520facial%2520video%2520and%2520speech%2520transmission%2520as%2520an%2520example%2520and%2520propose%2520a%250ASynchronous%2520Multimodal%2520Semantic%2520Communication%2520System%2520%2528SyncSC%2529%2520with%2520Packet-Level%250ACoding.%2520To%2520achieve%2520semantic%2520and%2520time%2520synchronization%252C%25203D%2520Morphable%2520Mode%2520%25283DMM%2529%250Acoefficients%2520and%2520text%2520are%2520transmitted%2520as%2520semantics%252C%2520and%2520we%2520propose%2520a%2520semantic%250Acodec%2520that%2520achieves%2520similar%2520quality%2520of%2520reconstruction%2520and%2520synchronization%2520with%250Alower%2520bandwidth%252C%2520compared%2520to%2520traditional%2520methods.%2520To%2520protect%2520semantic%2520packets%250Aunder%2520the%2520erasure%2520channel%252C%2520we%2520propose%2520a%2520packet-Level%2520Forward%2520Error%2520Correction%250A%2528FEC%2529%2520method%252C%2520called%2520PacSC%252C%2520that%2520maintains%2520a%2520certain%2520visual%2520quality%2520performance%250Aeven%2520at%2520high%2520packet%2520loss%2520rates.%2520Particularly%252C%2520for%2520text%2520packets%252C%2520a%2520text%2520packet%250Aloss%2520concealment%2520module%252C%2520called%2520TextPC%252C%2520based%2520on%2520Bidirectional%2520Encoder%250ARepresentations%2520from%2520Transformers%2520%2528BERT%2529%2520is%2520proposed%252C%2520which%2520significantly%250Aimproves%2520the%2520performance%2520of%2520traditional%2520FEC%2520methods.%2520The%2520simulation%2520results%250Ashow%2520that%2520our%2520proposed%2520SyncSC%2520reduce%2520transmission%2520overhead%2520and%2520achieve%250Ahigh-quality%2520synchronous%2520transmission%2520of%2520video%2520and%2520speech%2520over%2520the%2520packet%2520loss%250Anetwork.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synchronous%20Multi-modal%20Semantic%20CommunicationSystem%20with%20Packet-level%0A%20%20Coding&entry.906535625=Yun%20Tian%20and%20Jingkai%20Ying%20and%20Zhijin%20Qin%20and%20Ye%20Jin%20and%20Xiaoming%20Tao&entry.1292438233=%20%20Although%20the%20semantic%20communication%20with%20joint%20semantic-channel%20coding%20design%0Ahas%20shown%20promising%20performance%20in%20transmitting%20data%20of%20different%20modalities%0Aover%20physical%20layer%20channels%2C%20the%20synchronization%20and%20packet-level%20forward%0Aerror%20correction%20of%20multimodal%20semantics%20have%20not%20been%20well%20studied.%20Due%20to%20the%0Aindependent%20design%20of%20semantic%20encoders%2C%20synchronizing%20multimodal%20features%20in%0Aboth%20the%20semantic%20and%20time%20domains%20is%20a%20challenging%20problem.%20In%20this%20paper%2C%20we%0Atake%20the%20facial%20video%20and%20speech%20transmission%20as%20an%20example%20and%20propose%20a%0ASynchronous%20Multimodal%20Semantic%20Communication%20System%20%28SyncSC%29%20with%20Packet-Level%0ACoding.%20To%20achieve%20semantic%20and%20time%20synchronization%2C%203D%20Morphable%20Mode%20%283DMM%29%0Acoefficients%20and%20text%20are%20transmitted%20as%20semantics%2C%20and%20we%20propose%20a%20semantic%0Acodec%20that%20achieves%20similar%20quality%20of%20reconstruction%20and%20synchronization%20with%0Alower%20bandwidth%2C%20compared%20to%20traditional%20methods.%20To%20protect%20semantic%20packets%0Aunder%20the%20erasure%20channel%2C%20we%20propose%20a%20packet-Level%20Forward%20Error%20Correction%0A%28FEC%29%20method%2C%20called%20PacSC%2C%20that%20maintains%20a%20certain%20visual%20quality%20performance%0Aeven%20at%20high%20packet%20loss%20rates.%20Particularly%2C%20for%20text%20packets%2C%20a%20text%20packet%0Aloss%20concealment%20module%2C%20called%20TextPC%2C%20based%20on%20Bidirectional%20Encoder%0ARepresentations%20from%20Transformers%20%28BERT%29%20is%20proposed%2C%20which%20significantly%0Aimproves%20the%20performance%20of%20traditional%20FEC%20methods.%20The%20simulation%20results%0Ashow%20that%20our%20proposed%20SyncSC%20reduce%20transmission%20overhead%20and%20achieve%0Ahigh-quality%20synchronous%20transmission%20of%20video%20and%20speech%20over%20the%20packet%20loss%0Anetwork.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04535v1&entry.124074799=Read"},
{"title": "Self-Taught Evaluators", "author": "Tianlu Wang and Ilia Kulikov and Olga Golovneva and Ping Yu and Weizhe Yuan and Jane Dwivedi-Yu and Richard Yuanzhe Pang and Maryam Fazel-Zarandi and Jason Weston and Xian Li", "abstract": "  Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.\n", "link": "http://arxiv.org/abs/2408.02666v2", "date": "2024-08-08", "relevancy": 1.9794, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5354}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4872}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Taught%20Evaluators&body=Title%3A%20Self-Taught%20Evaluators%0AAuthor%3A%20Tianlu%20Wang%20and%20Ilia%20Kulikov%20and%20Olga%20Golovneva%20and%20Ping%20Yu%20and%20Weizhe%20Yuan%20and%20Jane%20Dwivedi-Yu%20and%20Richard%20Yuanzhe%20Pang%20and%20Maryam%20Fazel-Zarandi%20and%20Jason%20Weston%20and%20Xian%20Li%0AAbstract%3A%20%20%20Model-based%20evaluation%20is%20at%20the%20heart%20of%20successful%20model%20development%20--%20as%0Aa%20reward%20model%20for%20training%2C%20and%20as%20a%20replacement%20for%20human%20evaluation.%20To%0Atrain%20such%20evaluators%2C%20the%20standard%20approach%20is%20to%20collect%20a%20large%20amount%20of%0Ahuman%20preference%20judgments%20over%20model%20responses%2C%20which%20is%20costly%20and%20the%20data%0Abecomes%20stale%20as%20models%20improve.%20In%20this%20work%2C%20we%20present%20an%20approach%20that%20aims%0Ato%20im-prove%20evaluators%20without%20human%20annotations%2C%20using%20synthetic%20training%20data%0Aonly.%20Starting%20from%20unlabeled%20instructions%2C%20our%20iterative%20self-improvement%0Ascheme%20generates%20contrasting%20model%20outputs%20and%20trains%20an%20LLM-as-a-Judge%20to%0Aproduce%20reasoning%20traces%20and%20final%20judgments%2C%20repeating%20this%20training%20at%20each%0Anew%20iteration%20using%20the%20improved%20predictions.%20Without%20any%20labeled%20preference%0Adata%2C%20our%20Self-Taught%20Evaluator%20can%20improve%20a%20strong%20LLM%20%28Llama3-70B-Instruct%29%0Afrom%2075.4%20to%2088.3%20%2888.7%20with%20majority%20vote%29%20on%20RewardBench.%20This%20outperforms%0Acommonly%20used%20LLM%20judges%20such%20as%20GPT-4%20and%20matches%20the%20performance%20of%20the%0Atop-performing%20reward%20models%20trained%20with%20labeled%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02666v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Taught%2520Evaluators%26entry.906535625%3DTianlu%2520Wang%2520and%2520Ilia%2520Kulikov%2520and%2520Olga%2520Golovneva%2520and%2520Ping%2520Yu%2520and%2520Weizhe%2520Yuan%2520and%2520Jane%2520Dwivedi-Yu%2520and%2520Richard%2520Yuanzhe%2520Pang%2520and%2520Maryam%2520Fazel-Zarandi%2520and%2520Jason%2520Weston%2520and%2520Xian%2520Li%26entry.1292438233%3D%2520%2520Model-based%2520evaluation%2520is%2520at%2520the%2520heart%2520of%2520successful%2520model%2520development%2520--%2520as%250Aa%2520reward%2520model%2520for%2520training%252C%2520and%2520as%2520a%2520replacement%2520for%2520human%2520evaluation.%2520To%250Atrain%2520such%2520evaluators%252C%2520the%2520standard%2520approach%2520is%2520to%2520collect%2520a%2520large%2520amount%2520of%250Ahuman%2520preference%2520judgments%2520over%2520model%2520responses%252C%2520which%2520is%2520costly%2520and%2520the%2520data%250Abecomes%2520stale%2520as%2520models%2520improve.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520approach%2520that%2520aims%250Ato%2520im-prove%2520evaluators%2520without%2520human%2520annotations%252C%2520using%2520synthetic%2520training%2520data%250Aonly.%2520Starting%2520from%2520unlabeled%2520instructions%252C%2520our%2520iterative%2520self-improvement%250Ascheme%2520generates%2520contrasting%2520model%2520outputs%2520and%2520trains%2520an%2520LLM-as-a-Judge%2520to%250Aproduce%2520reasoning%2520traces%2520and%2520final%2520judgments%252C%2520repeating%2520this%2520training%2520at%2520each%250Anew%2520iteration%2520using%2520the%2520improved%2520predictions.%2520Without%2520any%2520labeled%2520preference%250Adata%252C%2520our%2520Self-Taught%2520Evaluator%2520can%2520improve%2520a%2520strong%2520LLM%2520%2528Llama3-70B-Instruct%2529%250Afrom%252075.4%2520to%252088.3%2520%252888.7%2520with%2520majority%2520vote%2529%2520on%2520RewardBench.%2520This%2520outperforms%250Acommonly%2520used%2520LLM%2520judges%2520such%2520as%2520GPT-4%2520and%2520matches%2520the%2520performance%2520of%2520the%250Atop-performing%2520reward%2520models%2520trained%2520with%2520labeled%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02666v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Taught%20Evaluators&entry.906535625=Tianlu%20Wang%20and%20Ilia%20Kulikov%20and%20Olga%20Golovneva%20and%20Ping%20Yu%20and%20Weizhe%20Yuan%20and%20Jane%20Dwivedi-Yu%20and%20Richard%20Yuanzhe%20Pang%20and%20Maryam%20Fazel-Zarandi%20and%20Jason%20Weston%20and%20Xian%20Li&entry.1292438233=%20%20Model-based%20evaluation%20is%20at%20the%20heart%20of%20successful%20model%20development%20--%20as%0Aa%20reward%20model%20for%20training%2C%20and%20as%20a%20replacement%20for%20human%20evaluation.%20To%0Atrain%20such%20evaluators%2C%20the%20standard%20approach%20is%20to%20collect%20a%20large%20amount%20of%0Ahuman%20preference%20judgments%20over%20model%20responses%2C%20which%20is%20costly%20and%20the%20data%0Abecomes%20stale%20as%20models%20improve.%20In%20this%20work%2C%20we%20present%20an%20approach%20that%20aims%0Ato%20im-prove%20evaluators%20without%20human%20annotations%2C%20using%20synthetic%20training%20data%0Aonly.%20Starting%20from%20unlabeled%20instructions%2C%20our%20iterative%20self-improvement%0Ascheme%20generates%20contrasting%20model%20outputs%20and%20trains%20an%20LLM-as-a-Judge%20to%0Aproduce%20reasoning%20traces%20and%20final%20judgments%2C%20repeating%20this%20training%20at%20each%0Anew%20iteration%20using%20the%20improved%20predictions.%20Without%20any%20labeled%20preference%0Adata%2C%20our%20Self-Taught%20Evaluator%20can%20improve%20a%20strong%20LLM%20%28Llama3-70B-Instruct%29%0Afrom%2075.4%20to%2088.3%20%2888.7%20with%20majority%20vote%29%20on%20RewardBench.%20This%20outperforms%0Acommonly%20used%20LLM%20judges%20such%20as%20GPT-4%20and%20matches%20the%20performance%20of%20the%0Atop-performing%20reward%20models%20trained%20with%20labeled%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02666v2&entry.124074799=Read"},
{"title": "FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly\n  Detection in Tabular Data", "author": "Ahmed Anwar and Brian Moser and Dayananda Herurkar and Federico Raue and Vinit Hegiste and Tatjana Legler and Andreas Dengel", "abstract": "  The emergence of federated learning (FL) presents a promising approach to\nleverage decentralized data while preserving privacy. Furthermore, the\ncombination of FL and anomaly detection is particularly compelling because it\nallows for detecting rare and critical anomalies (usually also rare in locally\ngathered data) in sensitive data from multiple sources, such as cybersecurity\nand healthcare. However, benchmarking the performance of anomaly detection\nmethods in FL environments remains an underexplored area. This paper introduces\nFedAD-Bench, a unified benchmark for evaluating unsupervised anomaly detection\nalgorithms within the context of FL. We systematically analyze and compare the\nperformance of recent deep learning anomaly detection models under federated\nsettings, which were typically assessed solely in centralized settings.\nFedAD-Bench encompasses diverse datasets and metrics to provide a holistic\nevaluation. Through extensive experiments, we identify key challenges such as\nmodel aggregation inefficiencies and metric unreliability. We present insights\ninto FL's regularization effects, revealing scenarios in which it outperforms\ncentralized approaches due to its inherent ability to mitigate overfitting. Our\nwork aims to establish a standardized benchmark to guide future research and\ndevelopment in federated anomaly detection, promoting reproducibility and fair\ncomparison across studies.\n", "link": "http://arxiv.org/abs/2408.04442v1", "date": "2024-08-08", "relevancy": 1.9504, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.513}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4924}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedAD-Bench%3A%20A%20Unified%20Benchmark%20for%20Federated%20Unsupervised%20Anomaly%0A%20%20Detection%20in%20Tabular%20Data&body=Title%3A%20FedAD-Bench%3A%20A%20Unified%20Benchmark%20for%20Federated%20Unsupervised%20Anomaly%0A%20%20Detection%20in%20Tabular%20Data%0AAuthor%3A%20Ahmed%20Anwar%20and%20Brian%20Moser%20and%20Dayananda%20Herurkar%20and%20Federico%20Raue%20and%20Vinit%20Hegiste%20and%20Tatjana%20Legler%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20The%20emergence%20of%20federated%20learning%20%28FL%29%20presents%20a%20promising%20approach%20to%0Aleverage%20decentralized%20data%20while%20preserving%20privacy.%20Furthermore%2C%20the%0Acombination%20of%20FL%20and%20anomaly%20detection%20is%20particularly%20compelling%20because%20it%0Aallows%20for%20detecting%20rare%20and%20critical%20anomalies%20%28usually%20also%20rare%20in%20locally%0Agathered%20data%29%20in%20sensitive%20data%20from%20multiple%20sources%2C%20such%20as%20cybersecurity%0Aand%20healthcare.%20However%2C%20benchmarking%20the%20performance%20of%20anomaly%20detection%0Amethods%20in%20FL%20environments%20remains%20an%20underexplored%20area.%20This%20paper%20introduces%0AFedAD-Bench%2C%20a%20unified%20benchmark%20for%20evaluating%20unsupervised%20anomaly%20detection%0Aalgorithms%20within%20the%20context%20of%20FL.%20We%20systematically%20analyze%20and%20compare%20the%0Aperformance%20of%20recent%20deep%20learning%20anomaly%20detection%20models%20under%20federated%0Asettings%2C%20which%20were%20typically%20assessed%20solely%20in%20centralized%20settings.%0AFedAD-Bench%20encompasses%20diverse%20datasets%20and%20metrics%20to%20provide%20a%20holistic%0Aevaluation.%20Through%20extensive%20experiments%2C%20we%20identify%20key%20challenges%20such%20as%0Amodel%20aggregation%20inefficiencies%20and%20metric%20unreliability.%20We%20present%20insights%0Ainto%20FL%27s%20regularization%20effects%2C%20revealing%20scenarios%20in%20which%20it%20outperforms%0Acentralized%20approaches%20due%20to%20its%20inherent%20ability%20to%20mitigate%20overfitting.%20Our%0Awork%20aims%20to%20establish%20a%20standardized%20benchmark%20to%20guide%20future%20research%20and%0Adevelopment%20in%20federated%20anomaly%20detection%2C%20promoting%20reproducibility%20and%20fair%0Acomparison%20across%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedAD-Bench%253A%2520A%2520Unified%2520Benchmark%2520for%2520Federated%2520Unsupervised%2520Anomaly%250A%2520%2520Detection%2520in%2520Tabular%2520Data%26entry.906535625%3DAhmed%2520Anwar%2520and%2520Brian%2520Moser%2520and%2520Dayananda%2520Herurkar%2520and%2520Federico%2520Raue%2520and%2520Vinit%2520Hegiste%2520and%2520Tatjana%2520Legler%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520federated%2520learning%2520%2528FL%2529%2520presents%2520a%2520promising%2520approach%2520to%250Aleverage%2520decentralized%2520data%2520while%2520preserving%2520privacy.%2520Furthermore%252C%2520the%250Acombination%2520of%2520FL%2520and%2520anomaly%2520detection%2520is%2520particularly%2520compelling%2520because%2520it%250Aallows%2520for%2520detecting%2520rare%2520and%2520critical%2520anomalies%2520%2528usually%2520also%2520rare%2520in%2520locally%250Agathered%2520data%2529%2520in%2520sensitive%2520data%2520from%2520multiple%2520sources%252C%2520such%2520as%2520cybersecurity%250Aand%2520healthcare.%2520However%252C%2520benchmarking%2520the%2520performance%2520of%2520anomaly%2520detection%250Amethods%2520in%2520FL%2520environments%2520remains%2520an%2520underexplored%2520area.%2520This%2520paper%2520introduces%250AFedAD-Bench%252C%2520a%2520unified%2520benchmark%2520for%2520evaluating%2520unsupervised%2520anomaly%2520detection%250Aalgorithms%2520within%2520the%2520context%2520of%2520FL.%2520We%2520systematically%2520analyze%2520and%2520compare%2520the%250Aperformance%2520of%2520recent%2520deep%2520learning%2520anomaly%2520detection%2520models%2520under%2520federated%250Asettings%252C%2520which%2520were%2520typically%2520assessed%2520solely%2520in%2520centralized%2520settings.%250AFedAD-Bench%2520encompasses%2520diverse%2520datasets%2520and%2520metrics%2520to%2520provide%2520a%2520holistic%250Aevaluation.%2520Through%2520extensive%2520experiments%252C%2520we%2520identify%2520key%2520challenges%2520such%2520as%250Amodel%2520aggregation%2520inefficiencies%2520and%2520metric%2520unreliability.%2520We%2520present%2520insights%250Ainto%2520FL%2527s%2520regularization%2520effects%252C%2520revealing%2520scenarios%2520in%2520which%2520it%2520outperforms%250Acentralized%2520approaches%2520due%2520to%2520its%2520inherent%2520ability%2520to%2520mitigate%2520overfitting.%2520Our%250Awork%2520aims%2520to%2520establish%2520a%2520standardized%2520benchmark%2520to%2520guide%2520future%2520research%2520and%250Adevelopment%2520in%2520federated%2520anomaly%2520detection%252C%2520promoting%2520reproducibility%2520and%2520fair%250Acomparison%2520across%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedAD-Bench%3A%20A%20Unified%20Benchmark%20for%20Federated%20Unsupervised%20Anomaly%0A%20%20Detection%20in%20Tabular%20Data&entry.906535625=Ahmed%20Anwar%20and%20Brian%20Moser%20and%20Dayananda%20Herurkar%20and%20Federico%20Raue%20and%20Vinit%20Hegiste%20and%20Tatjana%20Legler%20and%20Andreas%20Dengel&entry.1292438233=%20%20The%20emergence%20of%20federated%20learning%20%28FL%29%20presents%20a%20promising%20approach%20to%0Aleverage%20decentralized%20data%20while%20preserving%20privacy.%20Furthermore%2C%20the%0Acombination%20of%20FL%20and%20anomaly%20detection%20is%20particularly%20compelling%20because%20it%0Aallows%20for%20detecting%20rare%20and%20critical%20anomalies%20%28usually%20also%20rare%20in%20locally%0Agathered%20data%29%20in%20sensitive%20data%20from%20multiple%20sources%2C%20such%20as%20cybersecurity%0Aand%20healthcare.%20However%2C%20benchmarking%20the%20performance%20of%20anomaly%20detection%0Amethods%20in%20FL%20environments%20remains%20an%20underexplored%20area.%20This%20paper%20introduces%0AFedAD-Bench%2C%20a%20unified%20benchmark%20for%20evaluating%20unsupervised%20anomaly%20detection%0Aalgorithms%20within%20the%20context%20of%20FL.%20We%20systematically%20analyze%20and%20compare%20the%0Aperformance%20of%20recent%20deep%20learning%20anomaly%20detection%20models%20under%20federated%0Asettings%2C%20which%20were%20typically%20assessed%20solely%20in%20centralized%20settings.%0AFedAD-Bench%20encompasses%20diverse%20datasets%20and%20metrics%20to%20provide%20a%20holistic%0Aevaluation.%20Through%20extensive%20experiments%2C%20we%20identify%20key%20challenges%20such%20as%0Amodel%20aggregation%20inefficiencies%20and%20metric%20unreliability.%20We%20present%20insights%0Ainto%20FL%27s%20regularization%20effects%2C%20revealing%20scenarios%20in%20which%20it%20outperforms%0Acentralized%20approaches%20due%20to%20its%20inherent%20ability%20to%20mitigate%20overfitting.%20Our%0Awork%20aims%20to%20establish%20a%20standardized%20benchmark%20to%20guide%20future%20research%20and%0Adevelopment%20in%20federated%20anomaly%20detection%2C%20promoting%20reproducibility%20and%20fair%0Acomparison%20across%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04442v1&entry.124074799=Read"},
{"title": "Optimal Layout-Aware CNOT Circuit Synthesis with Qubit Permutation", "author": "Irfansha Shaik and Jaco van de Pol", "abstract": "  CNOT optimization plays a significant role in noise reduction for Quantum\nCircuits. Several heuristic and exact approaches exist for CNOT optimization.\nIn this paper, we investigate more complicated variations of optimal synthesis\nby allowing qubit permutations and handling layout restrictions. We encode such\nproblems into Planning, SAT, and QBF. We provide optimization for both CNOT\ngate count and circuit depth. For experimental evaluation, we consider standard\nT-gate optimized benchmarks and optimize CNOT sub-circuits. We show that\nallowing qubit permutations can further reduce up to 56% in CNOT count and 46%\nin circuit depth. In the case of optimally mapped circuits under layout\nrestrictions, we observe a reduction up to 17% CNOT count and 19% CNOT depth.\n", "link": "http://arxiv.org/abs/2408.04349v1", "date": "2024-08-08", "relevancy": 1.9431, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4199}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.386}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.36}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Layout-Aware%20CNOT%20Circuit%20Synthesis%20with%20Qubit%20Permutation&body=Title%3A%20Optimal%20Layout-Aware%20CNOT%20Circuit%20Synthesis%20with%20Qubit%20Permutation%0AAuthor%3A%20Irfansha%20Shaik%20and%20Jaco%20van%20de%20Pol%0AAbstract%3A%20%20%20CNOT%20optimization%20plays%20a%20significant%20role%20in%20noise%20reduction%20for%20Quantum%0ACircuits.%20Several%20heuristic%20and%20exact%20approaches%20exist%20for%20CNOT%20optimization.%0AIn%20this%20paper%2C%20we%20investigate%20more%20complicated%20variations%20of%20optimal%20synthesis%0Aby%20allowing%20qubit%20permutations%20and%20handling%20layout%20restrictions.%20We%20encode%20such%0Aproblems%20into%20Planning%2C%20SAT%2C%20and%20QBF.%20We%20provide%20optimization%20for%20both%20CNOT%0Agate%20count%20and%20circuit%20depth.%20For%20experimental%20evaluation%2C%20we%20consider%20standard%0AT-gate%20optimized%20benchmarks%20and%20optimize%20CNOT%20sub-circuits.%20We%20show%20that%0Aallowing%20qubit%20permutations%20can%20further%20reduce%20up%20to%2056%25%20in%20CNOT%20count%20and%2046%25%0Ain%20circuit%20depth.%20In%20the%20case%20of%20optimally%20mapped%20circuits%20under%20layout%0Arestrictions%2C%20we%20observe%20a%20reduction%20up%20to%2017%25%20CNOT%20count%20and%2019%25%20CNOT%20depth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Layout-Aware%2520CNOT%2520Circuit%2520Synthesis%2520with%2520Qubit%2520Permutation%26entry.906535625%3DIrfansha%2520Shaik%2520and%2520Jaco%2520van%2520de%2520Pol%26entry.1292438233%3D%2520%2520CNOT%2520optimization%2520plays%2520a%2520significant%2520role%2520in%2520noise%2520reduction%2520for%2520Quantum%250ACircuits.%2520Several%2520heuristic%2520and%2520exact%2520approaches%2520exist%2520for%2520CNOT%2520optimization.%250AIn%2520this%2520paper%252C%2520we%2520investigate%2520more%2520complicated%2520variations%2520of%2520optimal%2520synthesis%250Aby%2520allowing%2520qubit%2520permutations%2520and%2520handling%2520layout%2520restrictions.%2520We%2520encode%2520such%250Aproblems%2520into%2520Planning%252C%2520SAT%252C%2520and%2520QBF.%2520We%2520provide%2520optimization%2520for%2520both%2520CNOT%250Agate%2520count%2520and%2520circuit%2520depth.%2520For%2520experimental%2520evaluation%252C%2520we%2520consider%2520standard%250AT-gate%2520optimized%2520benchmarks%2520and%2520optimize%2520CNOT%2520sub-circuits.%2520We%2520show%2520that%250Aallowing%2520qubit%2520permutations%2520can%2520further%2520reduce%2520up%2520to%252056%2525%2520in%2520CNOT%2520count%2520and%252046%2525%250Ain%2520circuit%2520depth.%2520In%2520the%2520case%2520of%2520optimally%2520mapped%2520circuits%2520under%2520layout%250Arestrictions%252C%2520we%2520observe%2520a%2520reduction%2520up%2520to%252017%2525%2520CNOT%2520count%2520and%252019%2525%2520CNOT%2520depth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Layout-Aware%20CNOT%20Circuit%20Synthesis%20with%20Qubit%20Permutation&entry.906535625=Irfansha%20Shaik%20and%20Jaco%20van%20de%20Pol&entry.1292438233=%20%20CNOT%20optimization%20plays%20a%20significant%20role%20in%20noise%20reduction%20for%20Quantum%0ACircuits.%20Several%20heuristic%20and%20exact%20approaches%20exist%20for%20CNOT%20optimization.%0AIn%20this%20paper%2C%20we%20investigate%20more%20complicated%20variations%20of%20optimal%20synthesis%0Aby%20allowing%20qubit%20permutations%20and%20handling%20layout%20restrictions.%20We%20encode%20such%0Aproblems%20into%20Planning%2C%20SAT%2C%20and%20QBF.%20We%20provide%20optimization%20for%20both%20CNOT%0Agate%20count%20and%20circuit%20depth.%20For%20experimental%20evaluation%2C%20we%20consider%20standard%0AT-gate%20optimized%20benchmarks%20and%20optimize%20CNOT%20sub-circuits.%20We%20show%20that%0Aallowing%20qubit%20permutations%20can%20further%20reduce%20up%20to%2056%25%20in%20CNOT%20count%20and%2046%25%0Ain%20circuit%20depth.%20In%20the%20case%20of%20optimally%20mapped%20circuits%20under%20layout%0Arestrictions%2C%20we%20observe%20a%20reduction%20up%20to%2017%25%20CNOT%20count%20and%2019%25%20CNOT%20depth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04349v1&entry.124074799=Read"},
{"title": "Model-Based Transfer Learning for Contextual Reinforcement Learning", "author": "Jung-Hoon Cho and Vindula Jayawardana and Sirui Li and Cathy Wu", "abstract": "  Deep reinforcement learning is a powerful approach to complex decision\nmaking. However, one issue that limits its practical application is its\nbrittleness, sometimes failing to train in the presence of small changes in the\nenvironment. This work is motivated by the empirical observation that directly\napplying an already trained model to a related task often works remarkably\nwell, also called zero-shot transfer. We take this practical trick one step\nfurther to consider how to systematically select good tasks to train,\nmaximizing overall performance across a range of tasks. Given the high cost of\ntraining, it is critical to choose a small set of training tasks. The key idea\nbehind our approach is to explicitly model the performance loss (generalization\ngap) incurred by transferring a trained model. We hence introduce Model-Based\nTransfer Learning (MBTL) for solving contextual RL problems. In this work, we\nmodel the performance loss as a simple linear function of task context\nsimilarity. Furthermore, we leverage Bayesian optimization techniques to\nefficiently model and estimate the unknown training performance of the task\nspace. We theoretically show that the method exhibits regret that is sublinear\nin the number of training tasks and discuss conditions to further tighten\nregret bounds. We experimentally validate our methods using urban traffic and\nstandard control benchmarks. Despite the conceptual simplicity, the\nexperimental results suggest that MBTL can achieve greater performance than\nstrong baselines, including exhaustive training on all tasks, multi-task\ntraining, and random selection of training tasks. This work lays the\nfoundations for investigating explicit modeling of generalization, thereby\nenabling principled yet effective methods for contextual RL.\n", "link": "http://arxiv.org/abs/2408.04498v1", "date": "2024-08-08", "relevancy": 1.9429, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4814}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model-Based%20Transfer%20Learning%20for%20Contextual%20Reinforcement%20Learning&body=Title%3A%20Model-Based%20Transfer%20Learning%20for%20Contextual%20Reinforcement%20Learning%0AAuthor%3A%20Jung-Hoon%20Cho%20and%20Vindula%20Jayawardana%20and%20Sirui%20Li%20and%20Cathy%20Wu%0AAbstract%3A%20%20%20Deep%20reinforcement%20learning%20is%20a%20powerful%20approach%20to%20complex%20decision%0Amaking.%20However%2C%20one%20issue%20that%20limits%20its%20practical%20application%20is%20its%0Abrittleness%2C%20sometimes%20failing%20to%20train%20in%20the%20presence%20of%20small%20changes%20in%20the%0Aenvironment.%20This%20work%20is%20motivated%20by%20the%20empirical%20observation%20that%20directly%0Aapplying%20an%20already%20trained%20model%20to%20a%20related%20task%20often%20works%20remarkably%0Awell%2C%20also%20called%20zero-shot%20transfer.%20We%20take%20this%20practical%20trick%20one%20step%0Afurther%20to%20consider%20how%20to%20systematically%20select%20good%20tasks%20to%20train%2C%0Amaximizing%20overall%20performance%20across%20a%20range%20of%20tasks.%20Given%20the%20high%20cost%20of%0Atraining%2C%20it%20is%20critical%20to%20choose%20a%20small%20set%20of%20training%20tasks.%20The%20key%20idea%0Abehind%20our%20approach%20is%20to%20explicitly%20model%20the%20performance%20loss%20%28generalization%0Agap%29%20incurred%20by%20transferring%20a%20trained%20model.%20We%20hence%20introduce%20Model-Based%0ATransfer%20Learning%20%28MBTL%29%20for%20solving%20contextual%20RL%20problems.%20In%20this%20work%2C%20we%0Amodel%20the%20performance%20loss%20as%20a%20simple%20linear%20function%20of%20task%20context%0Asimilarity.%20Furthermore%2C%20we%20leverage%20Bayesian%20optimization%20techniques%20to%0Aefficiently%20model%20and%20estimate%20the%20unknown%20training%20performance%20of%20the%20task%0Aspace.%20We%20theoretically%20show%20that%20the%20method%20exhibits%20regret%20that%20is%20sublinear%0Ain%20the%20number%20of%20training%20tasks%20and%20discuss%20conditions%20to%20further%20tighten%0Aregret%20bounds.%20We%20experimentally%20validate%20our%20methods%20using%20urban%20traffic%20and%0Astandard%20control%20benchmarks.%20Despite%20the%20conceptual%20simplicity%2C%20the%0Aexperimental%20results%20suggest%20that%20MBTL%20can%20achieve%20greater%20performance%20than%0Astrong%20baselines%2C%20including%20exhaustive%20training%20on%20all%20tasks%2C%20multi-task%0Atraining%2C%20and%20random%20selection%20of%20training%20tasks.%20This%20work%20lays%20the%0Afoundations%20for%20investigating%20explicit%20modeling%20of%20generalization%2C%20thereby%0Aenabling%20principled%20yet%20effective%20methods%20for%20contextual%20RL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel-Based%2520Transfer%2520Learning%2520for%2520Contextual%2520Reinforcement%2520Learning%26entry.906535625%3DJung-Hoon%2520Cho%2520and%2520Vindula%2520Jayawardana%2520and%2520Sirui%2520Li%2520and%2520Cathy%2520Wu%26entry.1292438233%3D%2520%2520Deep%2520reinforcement%2520learning%2520is%2520a%2520powerful%2520approach%2520to%2520complex%2520decision%250Amaking.%2520However%252C%2520one%2520issue%2520that%2520limits%2520its%2520practical%2520application%2520is%2520its%250Abrittleness%252C%2520sometimes%2520failing%2520to%2520train%2520in%2520the%2520presence%2520of%2520small%2520changes%2520in%2520the%250Aenvironment.%2520This%2520work%2520is%2520motivated%2520by%2520the%2520empirical%2520observation%2520that%2520directly%250Aapplying%2520an%2520already%2520trained%2520model%2520to%2520a%2520related%2520task%2520often%2520works%2520remarkably%250Awell%252C%2520also%2520called%2520zero-shot%2520transfer.%2520We%2520take%2520this%2520practical%2520trick%2520one%2520step%250Afurther%2520to%2520consider%2520how%2520to%2520systematically%2520select%2520good%2520tasks%2520to%2520train%252C%250Amaximizing%2520overall%2520performance%2520across%2520a%2520range%2520of%2520tasks.%2520Given%2520the%2520high%2520cost%2520of%250Atraining%252C%2520it%2520is%2520critical%2520to%2520choose%2520a%2520small%2520set%2520of%2520training%2520tasks.%2520The%2520key%2520idea%250Abehind%2520our%2520approach%2520is%2520to%2520explicitly%2520model%2520the%2520performance%2520loss%2520%2528generalization%250Agap%2529%2520incurred%2520by%2520transferring%2520a%2520trained%2520model.%2520We%2520hence%2520introduce%2520Model-Based%250ATransfer%2520Learning%2520%2528MBTL%2529%2520for%2520solving%2520contextual%2520RL%2520problems.%2520In%2520this%2520work%252C%2520we%250Amodel%2520the%2520performance%2520loss%2520as%2520a%2520simple%2520linear%2520function%2520of%2520task%2520context%250Asimilarity.%2520Furthermore%252C%2520we%2520leverage%2520Bayesian%2520optimization%2520techniques%2520to%250Aefficiently%2520model%2520and%2520estimate%2520the%2520unknown%2520training%2520performance%2520of%2520the%2520task%250Aspace.%2520We%2520theoretically%2520show%2520that%2520the%2520method%2520exhibits%2520regret%2520that%2520is%2520sublinear%250Ain%2520the%2520number%2520of%2520training%2520tasks%2520and%2520discuss%2520conditions%2520to%2520further%2520tighten%250Aregret%2520bounds.%2520We%2520experimentally%2520validate%2520our%2520methods%2520using%2520urban%2520traffic%2520and%250Astandard%2520control%2520benchmarks.%2520Despite%2520the%2520conceptual%2520simplicity%252C%2520the%250Aexperimental%2520results%2520suggest%2520that%2520MBTL%2520can%2520achieve%2520greater%2520performance%2520than%250Astrong%2520baselines%252C%2520including%2520exhaustive%2520training%2520on%2520all%2520tasks%252C%2520multi-task%250Atraining%252C%2520and%2520random%2520selection%2520of%2520training%2520tasks.%2520This%2520work%2520lays%2520the%250Afoundations%2520for%2520investigating%2520explicit%2520modeling%2520of%2520generalization%252C%2520thereby%250Aenabling%2520principled%2520yet%2520effective%2520methods%2520for%2520contextual%2520RL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model-Based%20Transfer%20Learning%20for%20Contextual%20Reinforcement%20Learning&entry.906535625=Jung-Hoon%20Cho%20and%20Vindula%20Jayawardana%20and%20Sirui%20Li%20and%20Cathy%20Wu&entry.1292438233=%20%20Deep%20reinforcement%20learning%20is%20a%20powerful%20approach%20to%20complex%20decision%0Amaking.%20However%2C%20one%20issue%20that%20limits%20its%20practical%20application%20is%20its%0Abrittleness%2C%20sometimes%20failing%20to%20train%20in%20the%20presence%20of%20small%20changes%20in%20the%0Aenvironment.%20This%20work%20is%20motivated%20by%20the%20empirical%20observation%20that%20directly%0Aapplying%20an%20already%20trained%20model%20to%20a%20related%20task%20often%20works%20remarkably%0Awell%2C%20also%20called%20zero-shot%20transfer.%20We%20take%20this%20practical%20trick%20one%20step%0Afurther%20to%20consider%20how%20to%20systematically%20select%20good%20tasks%20to%20train%2C%0Amaximizing%20overall%20performance%20across%20a%20range%20of%20tasks.%20Given%20the%20high%20cost%20of%0Atraining%2C%20it%20is%20critical%20to%20choose%20a%20small%20set%20of%20training%20tasks.%20The%20key%20idea%0Abehind%20our%20approach%20is%20to%20explicitly%20model%20the%20performance%20loss%20%28generalization%0Agap%29%20incurred%20by%20transferring%20a%20trained%20model.%20We%20hence%20introduce%20Model-Based%0ATransfer%20Learning%20%28MBTL%29%20for%20solving%20contextual%20RL%20problems.%20In%20this%20work%2C%20we%0Amodel%20the%20performance%20loss%20as%20a%20simple%20linear%20function%20of%20task%20context%0Asimilarity.%20Furthermore%2C%20we%20leverage%20Bayesian%20optimization%20techniques%20to%0Aefficiently%20model%20and%20estimate%20the%20unknown%20training%20performance%20of%20the%20task%0Aspace.%20We%20theoretically%20show%20that%20the%20method%20exhibits%20regret%20that%20is%20sublinear%0Ain%20the%20number%20of%20training%20tasks%20and%20discuss%20conditions%20to%20further%20tighten%0Aregret%20bounds.%20We%20experimentally%20validate%20our%20methods%20using%20urban%20traffic%20and%0Astandard%20control%20benchmarks.%20Despite%20the%20conceptual%20simplicity%2C%20the%0Aexperimental%20results%20suggest%20that%20MBTL%20can%20achieve%20greater%20performance%20than%0Astrong%20baselines%2C%20including%20exhaustive%20training%20on%20all%20tasks%2C%20multi-task%0Atraining%2C%20and%20random%20selection%20of%20training%20tasks.%20This%20work%20lays%20the%0Afoundations%20for%20investigating%20explicit%20modeling%20of%20generalization%2C%20thereby%0Aenabling%20principled%20yet%20effective%20methods%20for%20contextual%20RL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04498v1&entry.124074799=Read"},
{"title": "FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression", "author": "Alireza Furutanpey and Qiyang Zhang and Philipp Raith and Tobias Pfandzelter and Shangguang Wang and Schahram Dustdar", "abstract": "  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on quality measures at lower bitrates. We extensively\nevaluate transfer cost reduction by including the peculiarity of intermittently\navailable network connections in low earth orbit. Lastly, we test the\nfeasibility of our system for standardized nanosatellite form factors. We\ndemonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n", "link": "http://arxiv.org/abs/2403.16677v2", "date": "2024-08-08", "relevancy": 1.9347, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4957}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4788}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FOOL%3A%20Addressing%20the%20Downlink%20Bottleneck%20in%20Satellite%20Computing%20with%0A%20%20Neural%20Feature%20Compression&body=Title%3A%20FOOL%3A%20Addressing%20the%20Downlink%20Bottleneck%20in%20Satellite%20Computing%20with%0A%20%20Neural%20Feature%20Compression%0AAuthor%3A%20Alireza%20Furutanpey%20and%20Qiyang%20Zhang%20and%20Philipp%20Raith%20and%20Tobias%20Pfandzelter%20and%20Shangguang%20Wang%20and%20Schahram%20Dustdar%0AAbstract%3A%20%20%20Nanosatellite%20constellations%20equipped%20with%20sensors%20capturing%20large%20geographic%0Aregions%20provide%20unprecedented%20opportunities%20for%20Earth%20observation.%20As%0Aconstellation%20sizes%20increase%2C%20network%20contention%20poses%20a%20downlink%20bottleneck.%0AOrbital%20Edge%20Computing%20%28OEC%29%20leverages%20limited%20onboard%20compute%20resources%20to%0Areduce%20transfer%20costs%20by%20processing%20the%20raw%20captures%20at%20the%20source.%20However%2C%0Acurrent%20solutions%20have%20limited%20practicability%20due%20to%20reliance%20on%20crude%0Afiltering%20methods%20or%20over-prioritizing%20particular%20downstream%20tasks.%0A%20%20This%20work%20presents%20FOOL%2C%20an%20OEC-native%20and%20task-agnostic%20feature%20compression%0Amethod%20that%20preserves%20prediction%20performance.%20FOOL%20partitions%20high-resolution%0Asatellite%20imagery%20to%20maximize%20throughput.%20Further%2C%20it%20embeds%20context%20and%0Aleverages%20inter-tile%20dependencies%20to%20lower%20transfer%20costs%20with%20negligible%0Aoverhead.%20While%20FOOL%20is%20a%20feature%20compressor%2C%20it%20can%20recover%20images%20with%0Acompetitive%20scores%20on%20quality%20measures%20at%20lower%20bitrates.%20We%20extensively%0Aevaluate%20transfer%20cost%20reduction%20by%20including%20the%20peculiarity%20of%20intermittently%0Aavailable%20network%20connections%20in%20low%20earth%20orbit.%20Lastly%2C%20we%20test%20the%0Afeasibility%20of%20our%20system%20for%20standardized%20nanosatellite%20form%20factors.%20We%0Ademonstrate%20that%20FOOL%20permits%20downlinking%20over%20100x%20the%20data%20volume%20without%0Arelying%20on%20prior%20information%20on%20the%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16677v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFOOL%253A%2520Addressing%2520the%2520Downlink%2520Bottleneck%2520in%2520Satellite%2520Computing%2520with%250A%2520%2520Neural%2520Feature%2520Compression%26entry.906535625%3DAlireza%2520Furutanpey%2520and%2520Qiyang%2520Zhang%2520and%2520Philipp%2520Raith%2520and%2520Tobias%2520Pfandzelter%2520and%2520Shangguang%2520Wang%2520and%2520Schahram%2520Dustdar%26entry.1292438233%3D%2520%2520Nanosatellite%2520constellations%2520equipped%2520with%2520sensors%2520capturing%2520large%2520geographic%250Aregions%2520provide%2520unprecedented%2520opportunities%2520for%2520Earth%2520observation.%2520As%250Aconstellation%2520sizes%2520increase%252C%2520network%2520contention%2520poses%2520a%2520downlink%2520bottleneck.%250AOrbital%2520Edge%2520Computing%2520%2528OEC%2529%2520leverages%2520limited%2520onboard%2520compute%2520resources%2520to%250Areduce%2520transfer%2520costs%2520by%2520processing%2520the%2520raw%2520captures%2520at%2520the%2520source.%2520However%252C%250Acurrent%2520solutions%2520have%2520limited%2520practicability%2520due%2520to%2520reliance%2520on%2520crude%250Afiltering%2520methods%2520or%2520over-prioritizing%2520particular%2520downstream%2520tasks.%250A%2520%2520This%2520work%2520presents%2520FOOL%252C%2520an%2520OEC-native%2520and%2520task-agnostic%2520feature%2520compression%250Amethod%2520that%2520preserves%2520prediction%2520performance.%2520FOOL%2520partitions%2520high-resolution%250Asatellite%2520imagery%2520to%2520maximize%2520throughput.%2520Further%252C%2520it%2520embeds%2520context%2520and%250Aleverages%2520inter-tile%2520dependencies%2520to%2520lower%2520transfer%2520costs%2520with%2520negligible%250Aoverhead.%2520While%2520FOOL%2520is%2520a%2520feature%2520compressor%252C%2520it%2520can%2520recover%2520images%2520with%250Acompetitive%2520scores%2520on%2520quality%2520measures%2520at%2520lower%2520bitrates.%2520We%2520extensively%250Aevaluate%2520transfer%2520cost%2520reduction%2520by%2520including%2520the%2520peculiarity%2520of%2520intermittently%250Aavailable%2520network%2520connections%2520in%2520low%2520earth%2520orbit.%2520Lastly%252C%2520we%2520test%2520the%250Afeasibility%2520of%2520our%2520system%2520for%2520standardized%2520nanosatellite%2520form%2520factors.%2520We%250Ademonstrate%2520that%2520FOOL%2520permits%2520downlinking%2520over%2520100x%2520the%2520data%2520volume%2520without%250Arelying%2520on%2520prior%2520information%2520on%2520the%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16677v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FOOL%3A%20Addressing%20the%20Downlink%20Bottleneck%20in%20Satellite%20Computing%20with%0A%20%20Neural%20Feature%20Compression&entry.906535625=Alireza%20Furutanpey%20and%20Qiyang%20Zhang%20and%20Philipp%20Raith%20and%20Tobias%20Pfandzelter%20and%20Shangguang%20Wang%20and%20Schahram%20Dustdar&entry.1292438233=%20%20Nanosatellite%20constellations%20equipped%20with%20sensors%20capturing%20large%20geographic%0Aregions%20provide%20unprecedented%20opportunities%20for%20Earth%20observation.%20As%0Aconstellation%20sizes%20increase%2C%20network%20contention%20poses%20a%20downlink%20bottleneck.%0AOrbital%20Edge%20Computing%20%28OEC%29%20leverages%20limited%20onboard%20compute%20resources%20to%0Areduce%20transfer%20costs%20by%20processing%20the%20raw%20captures%20at%20the%20source.%20However%2C%0Acurrent%20solutions%20have%20limited%20practicability%20due%20to%20reliance%20on%20crude%0Afiltering%20methods%20or%20over-prioritizing%20particular%20downstream%20tasks.%0A%20%20This%20work%20presents%20FOOL%2C%20an%20OEC-native%20and%20task-agnostic%20feature%20compression%0Amethod%20that%20preserves%20prediction%20performance.%20FOOL%20partitions%20high-resolution%0Asatellite%20imagery%20to%20maximize%20throughput.%20Further%2C%20it%20embeds%20context%20and%0Aleverages%20inter-tile%20dependencies%20to%20lower%20transfer%20costs%20with%20negligible%0Aoverhead.%20While%20FOOL%20is%20a%20feature%20compressor%2C%20it%20can%20recover%20images%20with%0Acompetitive%20scores%20on%20quality%20measures%20at%20lower%20bitrates.%20We%20extensively%0Aevaluate%20transfer%20cost%20reduction%20by%20including%20the%20peculiarity%20of%20intermittently%0Aavailable%20network%20connections%20in%20low%20earth%20orbit.%20Lastly%2C%20we%20test%20the%0Afeasibility%20of%20our%20system%20for%20standardized%20nanosatellite%20form%20factors.%20We%0Ademonstrate%20that%20FOOL%20permits%20downlinking%20over%20100x%20the%20data%20volume%20without%0Arelying%20on%20prior%20information%20on%20the%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16677v2&entry.124074799=Read"},
{"title": "CM-DQN: A Value-Based Deep Reinforcement Learning Model to Simulate\n  Confirmation Bias", "author": "Jiacheng Shen and Lihan Feng", "abstract": "  In human decision-making tasks, individuals learn through trials and\nprediction errors. When individuals learn the task, some are more influenced by\ngood outcomes, while others weigh bad outcomes more heavily. Such confirmation\nbias can lead to different learning effects. In this study, we propose a new\nalgorithm in Deep Reinforcement Learning, CM-DQN, which applies the idea of\ndifferent update strategies for positive or negative prediction errors, to\nsimulate the human decision-making process when the task's states are\ncontinuous while the actions are discrete. We test in Lunar Lander environment\nwith confirmatory, disconfirmatory bias and non-biased to observe the learning\neffects. Moreover, we apply the confirmation model in a multi-armed bandit\nproblem (environment in discrete states and discrete actions), which utilizes\nthe same idea as our proposed algorithm, as a contrast experiment to\nalgorithmically simulate the impact of different confirmation bias in\ndecision-making process. In both experiments, confirmatory bias indicates a\nbetter learning effect.\n", "link": "http://arxiv.org/abs/2407.07454v3", "date": "2024-08-08", "relevancy": 1.9339, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5414}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4882}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CM-DQN%3A%20A%20Value-Based%20Deep%20Reinforcement%20Learning%20Model%20to%20Simulate%0A%20%20Confirmation%20Bias&body=Title%3A%20CM-DQN%3A%20A%20Value-Based%20Deep%20Reinforcement%20Learning%20Model%20to%20Simulate%0A%20%20Confirmation%20Bias%0AAuthor%3A%20Jiacheng%20Shen%20and%20Lihan%20Feng%0AAbstract%3A%20%20%20In%20human%20decision-making%20tasks%2C%20individuals%20learn%20through%20trials%20and%0Aprediction%20errors.%20When%20individuals%20learn%20the%20task%2C%20some%20are%20more%20influenced%20by%0Agood%20outcomes%2C%20while%20others%20weigh%20bad%20outcomes%20more%20heavily.%20Such%20confirmation%0Abias%20can%20lead%20to%20different%20learning%20effects.%20In%20this%20study%2C%20we%20propose%20a%20new%0Aalgorithm%20in%20Deep%20Reinforcement%20Learning%2C%20CM-DQN%2C%20which%20applies%20the%20idea%20of%0Adifferent%20update%20strategies%20for%20positive%20or%20negative%20prediction%20errors%2C%20to%0Asimulate%20the%20human%20decision-making%20process%20when%20the%20task%27s%20states%20are%0Acontinuous%20while%20the%20actions%20are%20discrete.%20We%20test%20in%20Lunar%20Lander%20environment%0Awith%20confirmatory%2C%20disconfirmatory%20bias%20and%20non-biased%20to%20observe%20the%20learning%0Aeffects.%20Moreover%2C%20we%20apply%20the%20confirmation%20model%20in%20a%20multi-armed%20bandit%0Aproblem%20%28environment%20in%20discrete%20states%20and%20discrete%20actions%29%2C%20which%20utilizes%0Athe%20same%20idea%20as%20our%20proposed%20algorithm%2C%20as%20a%20contrast%20experiment%20to%0Aalgorithmically%20simulate%20the%20impact%20of%20different%20confirmation%20bias%20in%0Adecision-making%20process.%20In%20both%20experiments%2C%20confirmatory%20bias%20indicates%20a%0Abetter%20learning%20effect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07454v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCM-DQN%253A%2520A%2520Value-Based%2520Deep%2520Reinforcement%2520Learning%2520Model%2520to%2520Simulate%250A%2520%2520Confirmation%2520Bias%26entry.906535625%3DJiacheng%2520Shen%2520and%2520Lihan%2520Feng%26entry.1292438233%3D%2520%2520In%2520human%2520decision-making%2520tasks%252C%2520individuals%2520learn%2520through%2520trials%2520and%250Aprediction%2520errors.%2520When%2520individuals%2520learn%2520the%2520task%252C%2520some%2520are%2520more%2520influenced%2520by%250Agood%2520outcomes%252C%2520while%2520others%2520weigh%2520bad%2520outcomes%2520more%2520heavily.%2520Such%2520confirmation%250Abias%2520can%2520lead%2520to%2520different%2520learning%2520effects.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520new%250Aalgorithm%2520in%2520Deep%2520Reinforcement%2520Learning%252C%2520CM-DQN%252C%2520which%2520applies%2520the%2520idea%2520of%250Adifferent%2520update%2520strategies%2520for%2520positive%2520or%2520negative%2520prediction%2520errors%252C%2520to%250Asimulate%2520the%2520human%2520decision-making%2520process%2520when%2520the%2520task%2527s%2520states%2520are%250Acontinuous%2520while%2520the%2520actions%2520are%2520discrete.%2520We%2520test%2520in%2520Lunar%2520Lander%2520environment%250Awith%2520confirmatory%252C%2520disconfirmatory%2520bias%2520and%2520non-biased%2520to%2520observe%2520the%2520learning%250Aeffects.%2520Moreover%252C%2520we%2520apply%2520the%2520confirmation%2520model%2520in%2520a%2520multi-armed%2520bandit%250Aproblem%2520%2528environment%2520in%2520discrete%2520states%2520and%2520discrete%2520actions%2529%252C%2520which%2520utilizes%250Athe%2520same%2520idea%2520as%2520our%2520proposed%2520algorithm%252C%2520as%2520a%2520contrast%2520experiment%2520to%250Aalgorithmically%2520simulate%2520the%2520impact%2520of%2520different%2520confirmation%2520bias%2520in%250Adecision-making%2520process.%2520In%2520both%2520experiments%252C%2520confirmatory%2520bias%2520indicates%2520a%250Abetter%2520learning%2520effect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07454v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CM-DQN%3A%20A%20Value-Based%20Deep%20Reinforcement%20Learning%20Model%20to%20Simulate%0A%20%20Confirmation%20Bias&entry.906535625=Jiacheng%20Shen%20and%20Lihan%20Feng&entry.1292438233=%20%20In%20human%20decision-making%20tasks%2C%20individuals%20learn%20through%20trials%20and%0Aprediction%20errors.%20When%20individuals%20learn%20the%20task%2C%20some%20are%20more%20influenced%20by%0Agood%20outcomes%2C%20while%20others%20weigh%20bad%20outcomes%20more%20heavily.%20Such%20confirmation%0Abias%20can%20lead%20to%20different%20learning%20effects.%20In%20this%20study%2C%20we%20propose%20a%20new%0Aalgorithm%20in%20Deep%20Reinforcement%20Learning%2C%20CM-DQN%2C%20which%20applies%20the%20idea%20of%0Adifferent%20update%20strategies%20for%20positive%20or%20negative%20prediction%20errors%2C%20to%0Asimulate%20the%20human%20decision-making%20process%20when%20the%20task%27s%20states%20are%0Acontinuous%20while%20the%20actions%20are%20discrete.%20We%20test%20in%20Lunar%20Lander%20environment%0Awith%20confirmatory%2C%20disconfirmatory%20bias%20and%20non-biased%20to%20observe%20the%20learning%0Aeffects.%20Moreover%2C%20we%20apply%20the%20confirmation%20model%20in%20a%20multi-armed%20bandit%0Aproblem%20%28environment%20in%20discrete%20states%20and%20discrete%20actions%29%2C%20which%20utilizes%0Athe%20same%20idea%20as%20our%20proposed%20algorithm%2C%20as%20a%20contrast%20experiment%20to%0Aalgorithmically%20simulate%20the%20impact%20of%20different%20confirmation%20bias%20in%0Adecision-making%20process.%20In%20both%20experiments%2C%20confirmatory%20bias%20indicates%20a%0Abetter%20learning%20effect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07454v3&entry.124074799=Read"},
{"title": "GMISeg: General Medical Image Segmentation without Re-Training", "author": "Jing Xu", "abstract": "  The online shopping behavior has the characteristics of rich granularity\ndimension and data sparsity and previous researches on user behavior prediction\ndid not seriously discuss feature selection and ensemble design. In this paper,\nwe proposed a SE-Stacking model based on information fusion and ensemble\nlearning for user purchase behavior prediction. After successfully utilizing\nthe ensemble feature selection method to screen purchase-related factors, we\nused the Stacking algorithm for user purchase behavior prediction. In our\nefforts to avoid the deviation of prediction results, we optimized the model by\nselecting ten different kinds of models as base learners and modifying relevant\nparameters specifically for them. The experiments conducted on a\npublicly-available dataset shows that the SE-Stacking model can achieve a\n98.40% F1-score, about 0.09% higher than the optimal base models. The\nSE-Stacking model not only has a good application in the prediction of user\npurchase behavior but also has practical value combining with the actual\ne-commerce scene. At the same time, it has important significance for academic\nresearch and the development of this field.\n", "link": "http://arxiv.org/abs/2311.12539v3", "date": "2024-08-08", "relevancy": 1.9151, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4962}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4675}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMISeg%3A%20General%20Medical%20Image%20Segmentation%20without%20Re-Training&body=Title%3A%20GMISeg%3A%20General%20Medical%20Image%20Segmentation%20without%20Re-Training%0AAuthor%3A%20Jing%20Xu%0AAbstract%3A%20%20%20The%20online%20shopping%20behavior%20has%20the%20characteristics%20of%20rich%20granularity%0Adimension%20and%20data%20sparsity%20and%20previous%20researches%20on%20user%20behavior%20prediction%0Adid%20not%20seriously%20discuss%20feature%20selection%20and%20ensemble%20design.%20In%20this%20paper%2C%0Awe%20proposed%20a%20SE-Stacking%20model%20based%20on%20information%20fusion%20and%20ensemble%0Alearning%20for%20user%20purchase%20behavior%20prediction.%20After%20successfully%20utilizing%0Athe%20ensemble%20feature%20selection%20method%20to%20screen%20purchase-related%20factors%2C%20we%0Aused%20the%20Stacking%20algorithm%20for%20user%20purchase%20behavior%20prediction.%20In%20our%0Aefforts%20to%20avoid%20the%20deviation%20of%20prediction%20results%2C%20we%20optimized%20the%20model%20by%0Aselecting%20ten%20different%20kinds%20of%20models%20as%20base%20learners%20and%20modifying%20relevant%0Aparameters%20specifically%20for%20them.%20The%20experiments%20conducted%20on%20a%0Apublicly-available%20dataset%20shows%20that%20the%20SE-Stacking%20model%20can%20achieve%20a%0A98.40%25%20F1-score%2C%20about%200.09%25%20higher%20than%20the%20optimal%20base%20models.%20The%0ASE-Stacking%20model%20not%20only%20has%20a%20good%20application%20in%20the%20prediction%20of%20user%0Apurchase%20behavior%20but%20also%20has%20practical%20value%20combining%20with%20the%20actual%0Ae-commerce%20scene.%20At%20the%20same%20time%2C%20it%20has%20important%20significance%20for%20academic%0Aresearch%20and%20the%20development%20of%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12539v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMISeg%253A%2520General%2520Medical%2520Image%2520Segmentation%2520without%2520Re-Training%26entry.906535625%3DJing%2520Xu%26entry.1292438233%3D%2520%2520The%2520online%2520shopping%2520behavior%2520has%2520the%2520characteristics%2520of%2520rich%2520granularity%250Adimension%2520and%2520data%2520sparsity%2520and%2520previous%2520researches%2520on%2520user%2520behavior%2520prediction%250Adid%2520not%2520seriously%2520discuss%2520feature%2520selection%2520and%2520ensemble%2520design.%2520In%2520this%2520paper%252C%250Awe%2520proposed%2520a%2520SE-Stacking%2520model%2520based%2520on%2520information%2520fusion%2520and%2520ensemble%250Alearning%2520for%2520user%2520purchase%2520behavior%2520prediction.%2520After%2520successfully%2520utilizing%250Athe%2520ensemble%2520feature%2520selection%2520method%2520to%2520screen%2520purchase-related%2520factors%252C%2520we%250Aused%2520the%2520Stacking%2520algorithm%2520for%2520user%2520purchase%2520behavior%2520prediction.%2520In%2520our%250Aefforts%2520to%2520avoid%2520the%2520deviation%2520of%2520prediction%2520results%252C%2520we%2520optimized%2520the%2520model%2520by%250Aselecting%2520ten%2520different%2520kinds%2520of%2520models%2520as%2520base%2520learners%2520and%2520modifying%2520relevant%250Aparameters%2520specifically%2520for%2520them.%2520The%2520experiments%2520conducted%2520on%2520a%250Apublicly-available%2520dataset%2520shows%2520that%2520the%2520SE-Stacking%2520model%2520can%2520achieve%2520a%250A98.40%2525%2520F1-score%252C%2520about%25200.09%2525%2520higher%2520than%2520the%2520optimal%2520base%2520models.%2520The%250ASE-Stacking%2520model%2520not%2520only%2520has%2520a%2520good%2520application%2520in%2520the%2520prediction%2520of%2520user%250Apurchase%2520behavior%2520but%2520also%2520has%2520practical%2520value%2520combining%2520with%2520the%2520actual%250Ae-commerce%2520scene.%2520At%2520the%2520same%2520time%252C%2520it%2520has%2520important%2520significance%2520for%2520academic%250Aresearch%2520and%2520the%2520development%2520of%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12539v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMISeg%3A%20General%20Medical%20Image%20Segmentation%20without%20Re-Training&entry.906535625=Jing%20Xu&entry.1292438233=%20%20The%20online%20shopping%20behavior%20has%20the%20characteristics%20of%20rich%20granularity%0Adimension%20and%20data%20sparsity%20and%20previous%20researches%20on%20user%20behavior%20prediction%0Adid%20not%20seriously%20discuss%20feature%20selection%20and%20ensemble%20design.%20In%20this%20paper%2C%0Awe%20proposed%20a%20SE-Stacking%20model%20based%20on%20information%20fusion%20and%20ensemble%0Alearning%20for%20user%20purchase%20behavior%20prediction.%20After%20successfully%20utilizing%0Athe%20ensemble%20feature%20selection%20method%20to%20screen%20purchase-related%20factors%2C%20we%0Aused%20the%20Stacking%20algorithm%20for%20user%20purchase%20behavior%20prediction.%20In%20our%0Aefforts%20to%20avoid%20the%20deviation%20of%20prediction%20results%2C%20we%20optimized%20the%20model%20by%0Aselecting%20ten%20different%20kinds%20of%20models%20as%20base%20learners%20and%20modifying%20relevant%0Aparameters%20specifically%20for%20them.%20The%20experiments%20conducted%20on%20a%0Apublicly-available%20dataset%20shows%20that%20the%20SE-Stacking%20model%20can%20achieve%20a%0A98.40%25%20F1-score%2C%20about%200.09%25%20higher%20than%20the%20optimal%20base%20models.%20The%0ASE-Stacking%20model%20not%20only%20has%20a%20good%20application%20in%20the%20prediction%20of%20user%0Apurchase%20behavior%20but%20also%20has%20practical%20value%20combining%20with%20the%20actual%0Ae-commerce%20scene.%20At%20the%20same%20time%2C%20it%20has%20important%20significance%20for%20academic%0Aresearch%20and%20the%20development%20of%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12539v3&entry.124074799=Read"},
{"title": "Better Alignment with Instruction Back-and-Forth Translation", "author": "Thao Nguyen and Jeffrey Li and Sewoong Oh and Ludwig Schmidt and Jason Weston and Luke Zettlemoyer and Xian Li", "abstract": "  We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.\n", "link": "http://arxiv.org/abs/2408.04614v1", "date": "2024-08-08", "relevancy": 1.8937, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4845}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4747}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Alignment%20with%20Instruction%20Back-and-Forth%20Translation&body=Title%3A%20Better%20Alignment%20with%20Instruction%20Back-and-Forth%20Translation%0AAuthor%3A%20Thao%20Nguyen%20and%20Jeffrey%20Li%20and%20Sewoong%20Oh%20and%20Ludwig%20Schmidt%20and%20Jason%20Weston%20and%20Luke%20Zettlemoyer%20and%20Xian%20Li%0AAbstract%3A%20%20%20We%20propose%20a%20new%20method%2C%20instruction%20back-and-forth%20translation%2C%20to%20construct%0Ahigh-quality%20synthetic%20data%20grounded%20in%20world%20knowledge%20for%20aligning%20large%0Alanguage%20models%20%28LLMs%29.%20Given%20documents%20from%20a%20web%20corpus%2C%20we%20generate%20and%0Acurate%20synthetic%20instructions%20using%20the%20backtranslation%20approach%20proposed%20by%20Li%0Aet%20al.%282023a%29%2C%20and%20rewrite%20the%20responses%20to%20improve%20their%20quality%20further%20based%0Aon%20the%20initial%20documents.%20Fine-tuning%20with%20the%20resulting%20%28backtranslated%0Ainstruction%2C%20rewritten%20response%29%20pairs%20yields%20higher%20win%20rates%20on%20AlpacaEval%0Athan%20using%20other%20common%20instruction%20datasets%20such%20as%20Humpback%2C%20ShareGPT%2C%20Open%0AOrca%2C%20Alpaca-GPT4%20and%20Self-instruct.%20We%20also%20demonstrate%20that%20rewriting%20the%0Aresponses%20with%20an%20LLM%20outperforms%20direct%20distillation%2C%20and%20the%20two%20generated%0Atext%20distributions%20exhibit%20significant%20distinction%20in%20embedding%20space.%20Further%0Aanalysis%20shows%20that%20our%20backtranslated%20instructions%20are%20of%20higher%20quality%20than%0Aother%20sources%20of%20synthetic%20instructions%2C%20while%20our%20responses%20are%20more%20diverse%0Aand%20complex%20than%20those%20obtained%20from%20distillation.%20Overall%20we%20find%20that%0Ainstruction%20back-and-forth%20translation%20combines%20the%20best%20of%20both%20worlds%20--%0Amaking%20use%20of%20the%20information%20diversity%20and%20quantity%20found%20on%20the%20web%2C%20while%0Aensuring%20the%20quality%20of%20the%20responses%20which%20is%20necessary%20for%20effective%0Aalignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04614v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Alignment%2520with%2520Instruction%2520Back-and-Forth%2520Translation%26entry.906535625%3DThao%2520Nguyen%2520and%2520Jeffrey%2520Li%2520and%2520Sewoong%2520Oh%2520and%2520Ludwig%2520Schmidt%2520and%2520Jason%2520Weston%2520and%2520Luke%2520Zettlemoyer%2520and%2520Xian%2520Li%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520method%252C%2520instruction%2520back-and-forth%2520translation%252C%2520to%2520construct%250Ahigh-quality%2520synthetic%2520data%2520grounded%2520in%2520world%2520knowledge%2520for%2520aligning%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520Given%2520documents%2520from%2520a%2520web%2520corpus%252C%2520we%2520generate%2520and%250Acurate%2520synthetic%2520instructions%2520using%2520the%2520backtranslation%2520approach%2520proposed%2520by%2520Li%250Aet%2520al.%25282023a%2529%252C%2520and%2520rewrite%2520the%2520responses%2520to%2520improve%2520their%2520quality%2520further%2520based%250Aon%2520the%2520initial%2520documents.%2520Fine-tuning%2520with%2520the%2520resulting%2520%2528backtranslated%250Ainstruction%252C%2520rewritten%2520response%2529%2520pairs%2520yields%2520higher%2520win%2520rates%2520on%2520AlpacaEval%250Athan%2520using%2520other%2520common%2520instruction%2520datasets%2520such%2520as%2520Humpback%252C%2520ShareGPT%252C%2520Open%250AOrca%252C%2520Alpaca-GPT4%2520and%2520Self-instruct.%2520We%2520also%2520demonstrate%2520that%2520rewriting%2520the%250Aresponses%2520with%2520an%2520LLM%2520outperforms%2520direct%2520distillation%252C%2520and%2520the%2520two%2520generated%250Atext%2520distributions%2520exhibit%2520significant%2520distinction%2520in%2520embedding%2520space.%2520Further%250Aanalysis%2520shows%2520that%2520our%2520backtranslated%2520instructions%2520are%2520of%2520higher%2520quality%2520than%250Aother%2520sources%2520of%2520synthetic%2520instructions%252C%2520while%2520our%2520responses%2520are%2520more%2520diverse%250Aand%2520complex%2520than%2520those%2520obtained%2520from%2520distillation.%2520Overall%2520we%2520find%2520that%250Ainstruction%2520back-and-forth%2520translation%2520combines%2520the%2520best%2520of%2520both%2520worlds%2520--%250Amaking%2520use%2520of%2520the%2520information%2520diversity%2520and%2520quantity%2520found%2520on%2520the%2520web%252C%2520while%250Aensuring%2520the%2520quality%2520of%2520the%2520responses%2520which%2520is%2520necessary%2520for%2520effective%250Aalignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04614v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Alignment%20with%20Instruction%20Back-and-Forth%20Translation&entry.906535625=Thao%20Nguyen%20and%20Jeffrey%20Li%20and%20Sewoong%20Oh%20and%20Ludwig%20Schmidt%20and%20Jason%20Weston%20and%20Luke%20Zettlemoyer%20and%20Xian%20Li&entry.1292438233=%20%20We%20propose%20a%20new%20method%2C%20instruction%20back-and-forth%20translation%2C%20to%20construct%0Ahigh-quality%20synthetic%20data%20grounded%20in%20world%20knowledge%20for%20aligning%20large%0Alanguage%20models%20%28LLMs%29.%20Given%20documents%20from%20a%20web%20corpus%2C%20we%20generate%20and%0Acurate%20synthetic%20instructions%20using%20the%20backtranslation%20approach%20proposed%20by%20Li%0Aet%20al.%282023a%29%2C%20and%20rewrite%20the%20responses%20to%20improve%20their%20quality%20further%20based%0Aon%20the%20initial%20documents.%20Fine-tuning%20with%20the%20resulting%20%28backtranslated%0Ainstruction%2C%20rewritten%20response%29%20pairs%20yields%20higher%20win%20rates%20on%20AlpacaEval%0Athan%20using%20other%20common%20instruction%20datasets%20such%20as%20Humpback%2C%20ShareGPT%2C%20Open%0AOrca%2C%20Alpaca-GPT4%20and%20Self-instruct.%20We%20also%20demonstrate%20that%20rewriting%20the%0Aresponses%20with%20an%20LLM%20outperforms%20direct%20distillation%2C%20and%20the%20two%20generated%0Atext%20distributions%20exhibit%20significant%20distinction%20in%20embedding%20space.%20Further%0Aanalysis%20shows%20that%20our%20backtranslated%20instructions%20are%20of%20higher%20quality%20than%0Aother%20sources%20of%20synthetic%20instructions%2C%20while%20our%20responses%20are%20more%20diverse%0Aand%20complex%20than%20those%20obtained%20from%20distillation.%20Overall%20we%20find%20that%0Ainstruction%20back-and-forth%20translation%20combines%20the%20best%20of%20both%20worlds%20--%0Amaking%20use%20of%20the%20information%20diversity%20and%20quantity%20found%20on%20the%20web%2C%20while%0Aensuring%20the%20quality%20of%20the%20responses%20which%20is%20necessary%20for%20effective%0Aalignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04614v1&entry.124074799=Read"},
{"title": "Unveiling the Power of Sparse Neural Networks for Feature Selection", "author": "Zahra Atashgahi and Tennison Liu and Mykola Pechenizkiy and Raymond Veldhuis and Decebal Constantin Mocanu and Mihaela van der Schaar", "abstract": "  Sparse Neural Networks (SNNs) have emerged as powerful tools for efficient\nfeature selection. Leveraging the dynamic sparse training (DST) algorithms\nwithin SNNs has demonstrated promising feature selection capabilities while\ndrastically reducing computational overheads. Despite these advancements,\nseveral critical aspects remain insufficiently explored for feature selection.\nQuestions persist regarding the choice of the DST algorithm for network\ntraining, the choice of metric for ranking features/neurons, and the\ncomparative performance of these methods across diverse datasets when compared\nto dense networks. This paper addresses these gaps by presenting a\ncomprehensive systematic analysis of feature selection with sparse neural\nnetworks. Moreover, we introduce a novel metric considering sparse neural\nnetwork characteristics, which is designed to quantify feature importance\nwithin the context of SNNs. Our findings show that feature selection with SNNs\ntrained with DST algorithms can achieve, on average, more than $50\\%$ memory\nand $55\\%$ FLOPs reduction compared to the dense networks, while outperforming\nthem in terms of the quality of the selected features. Our code and the\nsupplementary material are available on GitHub\n(\\url{https://github.com/zahraatashgahi/Neuron-Attribution}).\n", "link": "http://arxiv.org/abs/2408.04583v1", "date": "2024-08-08", "relevancy": 1.8897, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4811}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4774}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Power%20of%20Sparse%20Neural%20Networks%20for%20Feature%20Selection&body=Title%3A%20Unveiling%20the%20Power%20of%20Sparse%20Neural%20Networks%20for%20Feature%20Selection%0AAuthor%3A%20Zahra%20Atashgahi%20and%20Tennison%20Liu%20and%20Mykola%20Pechenizkiy%20and%20Raymond%20Veldhuis%20and%20Decebal%20Constantin%20Mocanu%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20%20%20Sparse%20Neural%20Networks%20%28SNNs%29%20have%20emerged%20as%20powerful%20tools%20for%20efficient%0Afeature%20selection.%20Leveraging%20the%20dynamic%20sparse%20training%20%28DST%29%20algorithms%0Awithin%20SNNs%20has%20demonstrated%20promising%20feature%20selection%20capabilities%20while%0Adrastically%20reducing%20computational%20overheads.%20Despite%20these%20advancements%2C%0Aseveral%20critical%20aspects%20remain%20insufficiently%20explored%20for%20feature%20selection.%0AQuestions%20persist%20regarding%20the%20choice%20of%20the%20DST%20algorithm%20for%20network%0Atraining%2C%20the%20choice%20of%20metric%20for%20ranking%20features/neurons%2C%20and%20the%0Acomparative%20performance%20of%20these%20methods%20across%20diverse%20datasets%20when%20compared%0Ato%20dense%20networks.%20This%20paper%20addresses%20these%20gaps%20by%20presenting%20a%0Acomprehensive%20systematic%20analysis%20of%20feature%20selection%20with%20sparse%20neural%0Anetworks.%20Moreover%2C%20we%20introduce%20a%20novel%20metric%20considering%20sparse%20neural%0Anetwork%20characteristics%2C%20which%20is%20designed%20to%20quantify%20feature%20importance%0Awithin%20the%20context%20of%20SNNs.%20Our%20findings%20show%20that%20feature%20selection%20with%20SNNs%0Atrained%20with%20DST%20algorithms%20can%20achieve%2C%20on%20average%2C%20more%20than%20%2450%5C%25%24%20memory%0Aand%20%2455%5C%25%24%20FLOPs%20reduction%20compared%20to%20the%20dense%20networks%2C%20while%20outperforming%0Athem%20in%20terms%20of%20the%20quality%20of%20the%20selected%20features.%20Our%20code%20and%20the%0Asupplementary%20material%20are%20available%20on%20GitHub%0A%28%5Curl%7Bhttps%3A//github.com/zahraatashgahi/Neuron-Attribution%7D%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Power%2520of%2520Sparse%2520Neural%2520Networks%2520for%2520Feature%2520Selection%26entry.906535625%3DZahra%2520Atashgahi%2520and%2520Tennison%2520Liu%2520and%2520Mykola%2520Pechenizkiy%2520and%2520Raymond%2520Veldhuis%2520and%2520Decebal%2520Constantin%2520Mocanu%2520and%2520Mihaela%2520van%2520der%2520Schaar%26entry.1292438233%3D%2520%2520Sparse%2520Neural%2520Networks%2520%2528SNNs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520efficient%250Afeature%2520selection.%2520Leveraging%2520the%2520dynamic%2520sparse%2520training%2520%2528DST%2529%2520algorithms%250Awithin%2520SNNs%2520has%2520demonstrated%2520promising%2520feature%2520selection%2520capabilities%2520while%250Adrastically%2520reducing%2520computational%2520overheads.%2520Despite%2520these%2520advancements%252C%250Aseveral%2520critical%2520aspects%2520remain%2520insufficiently%2520explored%2520for%2520feature%2520selection.%250AQuestions%2520persist%2520regarding%2520the%2520choice%2520of%2520the%2520DST%2520algorithm%2520for%2520network%250Atraining%252C%2520the%2520choice%2520of%2520metric%2520for%2520ranking%2520features/neurons%252C%2520and%2520the%250Acomparative%2520performance%2520of%2520these%2520methods%2520across%2520diverse%2520datasets%2520when%2520compared%250Ato%2520dense%2520networks.%2520This%2520paper%2520addresses%2520these%2520gaps%2520by%2520presenting%2520a%250Acomprehensive%2520systematic%2520analysis%2520of%2520feature%2520selection%2520with%2520sparse%2520neural%250Anetworks.%2520Moreover%252C%2520we%2520introduce%2520a%2520novel%2520metric%2520considering%2520sparse%2520neural%250Anetwork%2520characteristics%252C%2520which%2520is%2520designed%2520to%2520quantify%2520feature%2520importance%250Awithin%2520the%2520context%2520of%2520SNNs.%2520Our%2520findings%2520show%2520that%2520feature%2520selection%2520with%2520SNNs%250Atrained%2520with%2520DST%2520algorithms%2520can%2520achieve%252C%2520on%2520average%252C%2520more%2520than%2520%252450%255C%2525%2524%2520memory%250Aand%2520%252455%255C%2525%2524%2520FLOPs%2520reduction%2520compared%2520to%2520the%2520dense%2520networks%252C%2520while%2520outperforming%250Athem%2520in%2520terms%2520of%2520the%2520quality%2520of%2520the%2520selected%2520features.%2520Our%2520code%2520and%2520the%250Asupplementary%2520material%2520are%2520available%2520on%2520GitHub%250A%2528%255Curl%257Bhttps%253A//github.com/zahraatashgahi/Neuron-Attribution%257D%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Power%20of%20Sparse%20Neural%20Networks%20for%20Feature%20Selection&entry.906535625=Zahra%20Atashgahi%20and%20Tennison%20Liu%20and%20Mykola%20Pechenizkiy%20and%20Raymond%20Veldhuis%20and%20Decebal%20Constantin%20Mocanu%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=%20%20Sparse%20Neural%20Networks%20%28SNNs%29%20have%20emerged%20as%20powerful%20tools%20for%20efficient%0Afeature%20selection.%20Leveraging%20the%20dynamic%20sparse%20training%20%28DST%29%20algorithms%0Awithin%20SNNs%20has%20demonstrated%20promising%20feature%20selection%20capabilities%20while%0Adrastically%20reducing%20computational%20overheads.%20Despite%20these%20advancements%2C%0Aseveral%20critical%20aspects%20remain%20insufficiently%20explored%20for%20feature%20selection.%0AQuestions%20persist%20regarding%20the%20choice%20of%20the%20DST%20algorithm%20for%20network%0Atraining%2C%20the%20choice%20of%20metric%20for%20ranking%20features/neurons%2C%20and%20the%0Acomparative%20performance%20of%20these%20methods%20across%20diverse%20datasets%20when%20compared%0Ato%20dense%20networks.%20This%20paper%20addresses%20these%20gaps%20by%20presenting%20a%0Acomprehensive%20systematic%20analysis%20of%20feature%20selection%20with%20sparse%20neural%0Anetworks.%20Moreover%2C%20we%20introduce%20a%20novel%20metric%20considering%20sparse%20neural%0Anetwork%20characteristics%2C%20which%20is%20designed%20to%20quantify%20feature%20importance%0Awithin%20the%20context%20of%20SNNs.%20Our%20findings%20show%20that%20feature%20selection%20with%20SNNs%0Atrained%20with%20DST%20algorithms%20can%20achieve%2C%20on%20average%2C%20more%20than%20%2450%5C%25%24%20memory%0Aand%20%2455%5C%25%24%20FLOPs%20reduction%20compared%20to%20the%20dense%20networks%2C%20while%20outperforming%0Athem%20in%20terms%20of%20the%20quality%20of%20the%20selected%20features.%20Our%20code%20and%20the%0Asupplementary%20material%20are%20available%20on%20GitHub%0A%28%5Curl%7Bhttps%3A//github.com/zahraatashgahi/Neuron-Attribution%7D%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04583v1&entry.124074799=Read"},
{"title": "Finite sample learning of moving targets", "author": "Nikolaus Vertovec and Kostas Margellos and Maria Prandini", "abstract": "  We consider a moving target that we seek to learn from samples. Our results\nextend randomized techniques developed in control and optimization for a\nconstant target to the case where the target is changing. We derive a novel\nbound on the number of samples that are required to construct a probably\napproximately correct (PAC) estimate of the target. Furthermore, when the\nmoving target is a convex polytope, we provide a constructive method of\ngenerating the PAC estimate using a mixed integer linear program (MILP). The\nproposed method is demonstrated on an application to autonomous emergency\nbraking.\n", "link": "http://arxiv.org/abs/2408.04406v1", "date": "2024-08-08", "relevancy": 1.8761, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4736}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4707}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finite%20sample%20learning%20of%20moving%20targets&body=Title%3A%20Finite%20sample%20learning%20of%20moving%20targets%0AAuthor%3A%20Nikolaus%20Vertovec%20and%20Kostas%20Margellos%20and%20Maria%20Prandini%0AAbstract%3A%20%20%20We%20consider%20a%20moving%20target%20that%20we%20seek%20to%20learn%20from%20samples.%20Our%20results%0Aextend%20randomized%20techniques%20developed%20in%20control%20and%20optimization%20for%20a%0Aconstant%20target%20to%20the%20case%20where%20the%20target%20is%20changing.%20We%20derive%20a%20novel%0Abound%20on%20the%20number%20of%20samples%20that%20are%20required%20to%20construct%20a%20probably%0Aapproximately%20correct%20%28PAC%29%20estimate%20of%20the%20target.%20Furthermore%2C%20when%20the%0Amoving%20target%20is%20a%20convex%20polytope%2C%20we%20provide%20a%20constructive%20method%20of%0Agenerating%20the%20PAC%20estimate%20using%20a%20mixed%20integer%20linear%20program%20%28MILP%29.%20The%0Aproposed%20method%20is%20demonstrated%20on%20an%20application%20to%20autonomous%20emergency%0Abraking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04406v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinite%2520sample%2520learning%2520of%2520moving%2520targets%26entry.906535625%3DNikolaus%2520Vertovec%2520and%2520Kostas%2520Margellos%2520and%2520Maria%2520Prandini%26entry.1292438233%3D%2520%2520We%2520consider%2520a%2520moving%2520target%2520that%2520we%2520seek%2520to%2520learn%2520from%2520samples.%2520Our%2520results%250Aextend%2520randomized%2520techniques%2520developed%2520in%2520control%2520and%2520optimization%2520for%2520a%250Aconstant%2520target%2520to%2520the%2520case%2520where%2520the%2520target%2520is%2520changing.%2520We%2520derive%2520a%2520novel%250Abound%2520on%2520the%2520number%2520of%2520samples%2520that%2520are%2520required%2520to%2520construct%2520a%2520probably%250Aapproximately%2520correct%2520%2528PAC%2529%2520estimate%2520of%2520the%2520target.%2520Furthermore%252C%2520when%2520the%250Amoving%2520target%2520is%2520a%2520convex%2520polytope%252C%2520we%2520provide%2520a%2520constructive%2520method%2520of%250Agenerating%2520the%2520PAC%2520estimate%2520using%2520a%2520mixed%2520integer%2520linear%2520program%2520%2528MILP%2529.%2520The%250Aproposed%2520method%2520is%2520demonstrated%2520on%2520an%2520application%2520to%2520autonomous%2520emergency%250Abraking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04406v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finite%20sample%20learning%20of%20moving%20targets&entry.906535625=Nikolaus%20Vertovec%20and%20Kostas%20Margellos%20and%20Maria%20Prandini&entry.1292438233=%20%20We%20consider%20a%20moving%20target%20that%20we%20seek%20to%20learn%20from%20samples.%20Our%20results%0Aextend%20randomized%20techniques%20developed%20in%20control%20and%20optimization%20for%20a%0Aconstant%20target%20to%20the%20case%20where%20the%20target%20is%20changing.%20We%20derive%20a%20novel%0Abound%20on%20the%20number%20of%20samples%20that%20are%20required%20to%20construct%20a%20probably%0Aapproximately%20correct%20%28PAC%29%20estimate%20of%20the%20target.%20Furthermore%2C%20when%20the%0Amoving%20target%20is%20a%20convex%20polytope%2C%20we%20provide%20a%20constructive%20method%20of%0Agenerating%20the%20PAC%20estimate%20using%20a%20mixed%20integer%20linear%20program%20%28MILP%29.%20The%0Aproposed%20method%20is%20demonstrated%20on%20an%20application%20to%20autonomous%20emergency%0Abraking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04406v1&entry.124074799=Read"},
{"title": "Long and Short Guidance in Score identity Distillation for One-Step\n  Text-to-Image Generation", "author": "Mingyuan Zhou and Zhendong Wang and Huangjie Zheng and Hai Huang", "abstract": "  Diffusion-based text-to-image generation models trained on extensive\ntext-image pairs have shown the capacity to generate photorealistic images\nconsistent with textual descriptions. However, a significant limitation of\nthese models is their slow sample generation, which requires iterative\nrefinement through the same network. In this paper, we enhance Score identity\nDistillation (SiD) by developing long and short classifier-free guidance (LSG)\nto efficiently distill pretrained Stable Diffusion models without using real\ntraining data. SiD aims to optimize a model-based explicit score matching loss,\nutilizing a score-identity-based approximation alongside the proposed LSG for\npractical computation. By training exclusively with fake images synthesized\nwith its one-step generator, SiD equipped with LSG rapidly improves FID and\nCLIP scores, achieving state-of-the-art FID performance while maintaining a\ncompetitive CLIP score. Specifically, its data-free distillation of Stable\nDiffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation\nset, with a CLIP score of 0.304 at an LSG scale of 1.5, and an FID of 9.56 with\na CLIP score of 0.313 at an LSG scale of 2. Our code and distilled one-step\ntext-to-image generators are available at\nhttps://github.com/mingyuanzhou/SiD-LSG.\n", "link": "http://arxiv.org/abs/2406.01561v3", "date": "2024-08-08", "relevancy": 1.8738, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.634}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6337}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long%20and%20Short%20Guidance%20in%20Score%20identity%20Distillation%20for%20One-Step%0A%20%20Text-to-Image%20Generation&body=Title%3A%20Long%20and%20Short%20Guidance%20in%20Score%20identity%20Distillation%20for%20One-Step%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Mingyuan%20Zhou%20and%20Zhendong%20Wang%20and%20Huangjie%20Zheng%20and%20Hai%20Huang%0AAbstract%3A%20%20%20Diffusion-based%20text-to-image%20generation%20models%20trained%20on%20extensive%0Atext-image%20pairs%20have%20shown%20the%20capacity%20to%20generate%20photorealistic%20images%0Aconsistent%20with%20textual%20descriptions.%20However%2C%20a%20significant%20limitation%20of%0Athese%20models%20is%20their%20slow%20sample%20generation%2C%20which%20requires%20iterative%0Arefinement%20through%20the%20same%20network.%20In%20this%20paper%2C%20we%20enhance%20Score%20identity%0ADistillation%20%28SiD%29%20by%20developing%20long%20and%20short%20classifier-free%20guidance%20%28LSG%29%0Ato%20efficiently%20distill%20pretrained%20Stable%20Diffusion%20models%20without%20using%20real%0Atraining%20data.%20SiD%20aims%20to%20optimize%20a%20model-based%20explicit%20score%20matching%20loss%2C%0Autilizing%20a%20score-identity-based%20approximation%20alongside%20the%20proposed%20LSG%20for%0Apractical%20computation.%20By%20training%20exclusively%20with%20fake%20images%20synthesized%0Awith%20its%20one-step%20generator%2C%20SiD%20equipped%20with%20LSG%20rapidly%20improves%20FID%20and%0ACLIP%20scores%2C%20achieving%20state-of-the-art%20FID%20performance%20while%20maintaining%20a%0Acompetitive%20CLIP%20score.%20Specifically%2C%20its%20data-free%20distillation%20of%20Stable%0ADiffusion%201.5%20achieves%20a%20record%20low%20FID%20of%208.15%20on%20the%20COCO-2014%20validation%0Aset%2C%20with%20a%20CLIP%20score%20of%200.304%20at%20an%20LSG%20scale%20of%201.5%2C%20and%20an%20FID%20of%209.56%20with%0Aa%20CLIP%20score%20of%200.313%20at%20an%20LSG%20scale%20of%202.%20Our%20code%20and%20distilled%20one-step%0Atext-to-image%20generators%20are%20available%20at%0Ahttps%3A//github.com/mingyuanzhou/SiD-LSG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01561v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong%2520and%2520Short%2520Guidance%2520in%2520Score%2520identity%2520Distillation%2520for%2520One-Step%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DMingyuan%2520Zhou%2520and%2520Zhendong%2520Wang%2520and%2520Huangjie%2520Zheng%2520and%2520Hai%2520Huang%26entry.1292438233%3D%2520%2520Diffusion-based%2520text-to-image%2520generation%2520models%2520trained%2520on%2520extensive%250Atext-image%2520pairs%2520have%2520shown%2520the%2520capacity%2520to%2520generate%2520photorealistic%2520images%250Aconsistent%2520with%2520textual%2520descriptions.%2520However%252C%2520a%2520significant%2520limitation%2520of%250Athese%2520models%2520is%2520their%2520slow%2520sample%2520generation%252C%2520which%2520requires%2520iterative%250Arefinement%2520through%2520the%2520same%2520network.%2520In%2520this%2520paper%252C%2520we%2520enhance%2520Score%2520identity%250ADistillation%2520%2528SiD%2529%2520by%2520developing%2520long%2520and%2520short%2520classifier-free%2520guidance%2520%2528LSG%2529%250Ato%2520efficiently%2520distill%2520pretrained%2520Stable%2520Diffusion%2520models%2520without%2520using%2520real%250Atraining%2520data.%2520SiD%2520aims%2520to%2520optimize%2520a%2520model-based%2520explicit%2520score%2520matching%2520loss%252C%250Autilizing%2520a%2520score-identity-based%2520approximation%2520alongside%2520the%2520proposed%2520LSG%2520for%250Apractical%2520computation.%2520By%2520training%2520exclusively%2520with%2520fake%2520images%2520synthesized%250Awith%2520its%2520one-step%2520generator%252C%2520SiD%2520equipped%2520with%2520LSG%2520rapidly%2520improves%2520FID%2520and%250ACLIP%2520scores%252C%2520achieving%2520state-of-the-art%2520FID%2520performance%2520while%2520maintaining%2520a%250Acompetitive%2520CLIP%2520score.%2520Specifically%252C%2520its%2520data-free%2520distillation%2520of%2520Stable%250ADiffusion%25201.5%2520achieves%2520a%2520record%2520low%2520FID%2520of%25208.15%2520on%2520the%2520COCO-2014%2520validation%250Aset%252C%2520with%2520a%2520CLIP%2520score%2520of%25200.304%2520at%2520an%2520LSG%2520scale%2520of%25201.5%252C%2520and%2520an%2520FID%2520of%25209.56%2520with%250Aa%2520CLIP%2520score%2520of%25200.313%2520at%2520an%2520LSG%2520scale%2520of%25202.%2520Our%2520code%2520and%2520distilled%2520one-step%250Atext-to-image%2520generators%2520are%2520available%2520at%250Ahttps%253A//github.com/mingyuanzhou/SiD-LSG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01561v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long%20and%20Short%20Guidance%20in%20Score%20identity%20Distillation%20for%20One-Step%0A%20%20Text-to-Image%20Generation&entry.906535625=Mingyuan%20Zhou%20and%20Zhendong%20Wang%20and%20Huangjie%20Zheng%20and%20Hai%20Huang&entry.1292438233=%20%20Diffusion-based%20text-to-image%20generation%20models%20trained%20on%20extensive%0Atext-image%20pairs%20have%20shown%20the%20capacity%20to%20generate%20photorealistic%20images%0Aconsistent%20with%20textual%20descriptions.%20However%2C%20a%20significant%20limitation%20of%0Athese%20models%20is%20their%20slow%20sample%20generation%2C%20which%20requires%20iterative%0Arefinement%20through%20the%20same%20network.%20In%20this%20paper%2C%20we%20enhance%20Score%20identity%0ADistillation%20%28SiD%29%20by%20developing%20long%20and%20short%20classifier-free%20guidance%20%28LSG%29%0Ato%20efficiently%20distill%20pretrained%20Stable%20Diffusion%20models%20without%20using%20real%0Atraining%20data.%20SiD%20aims%20to%20optimize%20a%20model-based%20explicit%20score%20matching%20loss%2C%0Autilizing%20a%20score-identity-based%20approximation%20alongside%20the%20proposed%20LSG%20for%0Apractical%20computation.%20By%20training%20exclusively%20with%20fake%20images%20synthesized%0Awith%20its%20one-step%20generator%2C%20SiD%20equipped%20with%20LSG%20rapidly%20improves%20FID%20and%0ACLIP%20scores%2C%20achieving%20state-of-the-art%20FID%20performance%20while%20maintaining%20a%0Acompetitive%20CLIP%20score.%20Specifically%2C%20its%20data-free%20distillation%20of%20Stable%0ADiffusion%201.5%20achieves%20a%20record%20low%20FID%20of%208.15%20on%20the%20COCO-2014%20validation%0Aset%2C%20with%20a%20CLIP%20score%20of%200.304%20at%20an%20LSG%20scale%20of%201.5%2C%20and%20an%20FID%20of%209.56%20with%0Aa%20CLIP%20score%20of%200.313%20at%20an%20LSG%20scale%20of%202.%20Our%20code%20and%20distilled%20one-step%0Atext-to-image%20generators%20are%20available%20at%0Ahttps%3A//github.com/mingyuanzhou/SiD-LSG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01561v3&entry.124074799=Read"},
{"title": "Quantifying the Impact of Population Shift Across Age and Sex for\n  Abdominal Organ Segmentation", "author": "Kate \u010cevora and Ben Glocker and Wenjia Bai", "abstract": "  Deep learning-based medical image segmentation has seen tremendous progress\nover the last decade, but there is still relatively little transfer into\nclinical practice. One of the main barriers is the challenge of domain\ngeneralisation, which requires segmentation models to maintain high performance\nacross a wide distribution of image data. This challenge is amplified by the\nmany factors that contribute to the diverse appearance of medical images, such\nas acquisition conditions and patient characteristics. The impact of shifting\npatient characteristics such as age and sex on segmentation performance remains\nrelatively under-studied, especially for abdominal organs, despite that this is\ncrucial for ensuring the fairness of the segmentation model. We perform the\nfirst study to determine the impact of population shift with respect to age and\nsex on abdominal CT image segmentation, by leveraging two large public\ndatasets, and introduce a novel metric to quantify the impact. We find that\npopulation shift is a challenge similar in magnitude to cross-dataset shift for\nabdominal organ segmentation, and that the effect is asymmetric and\ndataset-dependent. We conclude that dataset diversity in terms of known patient\ncharacteristics is not necessarily equivalent to dataset diversity in terms of\nimage features. This implies that simple population matching to ensure good\ngeneralisation and fairness may be insufficient, and we recommend that fairness\nresearch should be directed towards better understanding and quantifying\nmedical image dataset diversity in terms of performance-relevant\ncharacteristics such as organ morphology.\n", "link": "http://arxiv.org/abs/2408.04610v1", "date": "2024-08-08", "relevancy": 1.8614, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4768}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.464}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20the%20Impact%20of%20Population%20Shift%20Across%20Age%20and%20Sex%20for%0A%20%20Abdominal%20Organ%20Segmentation&body=Title%3A%20Quantifying%20the%20Impact%20of%20Population%20Shift%20Across%20Age%20and%20Sex%20for%0A%20%20Abdominal%20Organ%20Segmentation%0AAuthor%3A%20Kate%20%C4%8Cevora%20and%20Ben%20Glocker%20and%20Wenjia%20Bai%0AAbstract%3A%20%20%20Deep%20learning-based%20medical%20image%20segmentation%20has%20seen%20tremendous%20progress%0Aover%20the%20last%20decade%2C%20but%20there%20is%20still%20relatively%20little%20transfer%20into%0Aclinical%20practice.%20One%20of%20the%20main%20barriers%20is%20the%20challenge%20of%20domain%0Ageneralisation%2C%20which%20requires%20segmentation%20models%20to%20maintain%20high%20performance%0Aacross%20a%20wide%20distribution%20of%20image%20data.%20This%20challenge%20is%20amplified%20by%20the%0Amany%20factors%20that%20contribute%20to%20the%20diverse%20appearance%20of%20medical%20images%2C%20such%0Aas%20acquisition%20conditions%20and%20patient%20characteristics.%20The%20impact%20of%20shifting%0Apatient%20characteristics%20such%20as%20age%20and%20sex%20on%20segmentation%20performance%20remains%0Arelatively%20under-studied%2C%20especially%20for%20abdominal%20organs%2C%20despite%20that%20this%20is%0Acrucial%20for%20ensuring%20the%20fairness%20of%20the%20segmentation%20model.%20We%20perform%20the%0Afirst%20study%20to%20determine%20the%20impact%20of%20population%20shift%20with%20respect%20to%20age%20and%0Asex%20on%20abdominal%20CT%20image%20segmentation%2C%20by%20leveraging%20two%20large%20public%0Adatasets%2C%20and%20introduce%20a%20novel%20metric%20to%20quantify%20the%20impact.%20We%20find%20that%0Apopulation%20shift%20is%20a%20challenge%20similar%20in%20magnitude%20to%20cross-dataset%20shift%20for%0Aabdominal%20organ%20segmentation%2C%20and%20that%20the%20effect%20is%20asymmetric%20and%0Adataset-dependent.%20We%20conclude%20that%20dataset%20diversity%20in%20terms%20of%20known%20patient%0Acharacteristics%20is%20not%20necessarily%20equivalent%20to%20dataset%20diversity%20in%20terms%20of%0Aimage%20features.%20This%20implies%20that%20simple%20population%20matching%20to%20ensure%20good%0Ageneralisation%20and%20fairness%20may%20be%20insufficient%2C%20and%20we%20recommend%20that%20fairness%0Aresearch%20should%20be%20directed%20towards%20better%20understanding%20and%20quantifying%0Amedical%20image%20dataset%20diversity%20in%20terms%20of%20performance-relevant%0Acharacteristics%20such%20as%20organ%20morphology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520the%2520Impact%2520of%2520Population%2520Shift%2520Across%2520Age%2520and%2520Sex%2520for%250A%2520%2520Abdominal%2520Organ%2520Segmentation%26entry.906535625%3DKate%2520%25C4%258Cevora%2520and%2520Ben%2520Glocker%2520and%2520Wenjia%2520Bai%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520medical%2520image%2520segmentation%2520has%2520seen%2520tremendous%2520progress%250Aover%2520the%2520last%2520decade%252C%2520but%2520there%2520is%2520still%2520relatively%2520little%2520transfer%2520into%250Aclinical%2520practice.%2520One%2520of%2520the%2520main%2520barriers%2520is%2520the%2520challenge%2520of%2520domain%250Ageneralisation%252C%2520which%2520requires%2520segmentation%2520models%2520to%2520maintain%2520high%2520performance%250Aacross%2520a%2520wide%2520distribution%2520of%2520image%2520data.%2520This%2520challenge%2520is%2520amplified%2520by%2520the%250Amany%2520factors%2520that%2520contribute%2520to%2520the%2520diverse%2520appearance%2520of%2520medical%2520images%252C%2520such%250Aas%2520acquisition%2520conditions%2520and%2520patient%2520characteristics.%2520The%2520impact%2520of%2520shifting%250Apatient%2520characteristics%2520such%2520as%2520age%2520and%2520sex%2520on%2520segmentation%2520performance%2520remains%250Arelatively%2520under-studied%252C%2520especially%2520for%2520abdominal%2520organs%252C%2520despite%2520that%2520this%2520is%250Acrucial%2520for%2520ensuring%2520the%2520fairness%2520of%2520the%2520segmentation%2520model.%2520We%2520perform%2520the%250Afirst%2520study%2520to%2520determine%2520the%2520impact%2520of%2520population%2520shift%2520with%2520respect%2520to%2520age%2520and%250Asex%2520on%2520abdominal%2520CT%2520image%2520segmentation%252C%2520by%2520leveraging%2520two%2520large%2520public%250Adatasets%252C%2520and%2520introduce%2520a%2520novel%2520metric%2520to%2520quantify%2520the%2520impact.%2520We%2520find%2520that%250Apopulation%2520shift%2520is%2520a%2520challenge%2520similar%2520in%2520magnitude%2520to%2520cross-dataset%2520shift%2520for%250Aabdominal%2520organ%2520segmentation%252C%2520and%2520that%2520the%2520effect%2520is%2520asymmetric%2520and%250Adataset-dependent.%2520We%2520conclude%2520that%2520dataset%2520diversity%2520in%2520terms%2520of%2520known%2520patient%250Acharacteristics%2520is%2520not%2520necessarily%2520equivalent%2520to%2520dataset%2520diversity%2520in%2520terms%2520of%250Aimage%2520features.%2520This%2520implies%2520that%2520simple%2520population%2520matching%2520to%2520ensure%2520good%250Ageneralisation%2520and%2520fairness%2520may%2520be%2520insufficient%252C%2520and%2520we%2520recommend%2520that%2520fairness%250Aresearch%2520should%2520be%2520directed%2520towards%2520better%2520understanding%2520and%2520quantifying%250Amedical%2520image%2520dataset%2520diversity%2520in%2520terms%2520of%2520performance-relevant%250Acharacteristics%2520such%2520as%2520organ%2520morphology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20the%20Impact%20of%20Population%20Shift%20Across%20Age%20and%20Sex%20for%0A%20%20Abdominal%20Organ%20Segmentation&entry.906535625=Kate%20%C4%8Cevora%20and%20Ben%20Glocker%20and%20Wenjia%20Bai&entry.1292438233=%20%20Deep%20learning-based%20medical%20image%20segmentation%20has%20seen%20tremendous%20progress%0Aover%20the%20last%20decade%2C%20but%20there%20is%20still%20relatively%20little%20transfer%20into%0Aclinical%20practice.%20One%20of%20the%20main%20barriers%20is%20the%20challenge%20of%20domain%0Ageneralisation%2C%20which%20requires%20segmentation%20models%20to%20maintain%20high%20performance%0Aacross%20a%20wide%20distribution%20of%20image%20data.%20This%20challenge%20is%20amplified%20by%20the%0Amany%20factors%20that%20contribute%20to%20the%20diverse%20appearance%20of%20medical%20images%2C%20such%0Aas%20acquisition%20conditions%20and%20patient%20characteristics.%20The%20impact%20of%20shifting%0Apatient%20characteristics%20such%20as%20age%20and%20sex%20on%20segmentation%20performance%20remains%0Arelatively%20under-studied%2C%20especially%20for%20abdominal%20organs%2C%20despite%20that%20this%20is%0Acrucial%20for%20ensuring%20the%20fairness%20of%20the%20segmentation%20model.%20We%20perform%20the%0Afirst%20study%20to%20determine%20the%20impact%20of%20population%20shift%20with%20respect%20to%20age%20and%0Asex%20on%20abdominal%20CT%20image%20segmentation%2C%20by%20leveraging%20two%20large%20public%0Adatasets%2C%20and%20introduce%20a%20novel%20metric%20to%20quantify%20the%20impact.%20We%20find%20that%0Apopulation%20shift%20is%20a%20challenge%20similar%20in%20magnitude%20to%20cross-dataset%20shift%20for%0Aabdominal%20organ%20segmentation%2C%20and%20that%20the%20effect%20is%20asymmetric%20and%0Adataset-dependent.%20We%20conclude%20that%20dataset%20diversity%20in%20terms%20of%20known%20patient%0Acharacteristics%20is%20not%20necessarily%20equivalent%20to%20dataset%20diversity%20in%20terms%20of%0Aimage%20features.%20This%20implies%20that%20simple%20population%20matching%20to%20ensure%20good%0Ageneralisation%20and%20fairness%20may%20be%20insufficient%2C%20and%20we%20recommend%20that%20fairness%0Aresearch%20should%20be%20directed%20towards%20better%20understanding%20and%20quantifying%0Amedical%20image%20dataset%20diversity%20in%20terms%20of%20performance-relevant%0Acharacteristics%20such%20as%20organ%20morphology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04610v1&entry.124074799=Read"},
{"title": "Detecting Car Speed using Object Detection and Depth Estimation: A Deep\n  Learning Framework", "author": "Subhasis Dasgupta and Arshi Naaz and Jayeeta Choudhury and Nancy Lahiri", "abstract": "  Road accidents are quite common in almost every part of the world, and, in\nmajority, fatal accidents are attributed to over speeding of vehicles. The\ntendency to over speeding is usually tried to be controlled using check points\nat various parts of the road but not all traffic police have the device to\ncheck speed with existing speed estimating devices such as LIDAR based, or\nRadar based guns. The current project tries to address the issue of vehicle\nspeed estimation with handheld devices such as mobile phones or wearable\ncameras with network connection to estimate the speed using deep learning\nframeworks.\n", "link": "http://arxiv.org/abs/2408.04360v1", "date": "2024-08-08", "relevancy": 1.8474, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4739}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4654}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Car%20Speed%20using%20Object%20Detection%20and%20Depth%20Estimation%3A%20A%20Deep%0A%20%20Learning%20Framework&body=Title%3A%20Detecting%20Car%20Speed%20using%20Object%20Detection%20and%20Depth%20Estimation%3A%20A%20Deep%0A%20%20Learning%20Framework%0AAuthor%3A%20Subhasis%20Dasgupta%20and%20Arshi%20Naaz%20and%20Jayeeta%20Choudhury%20and%20Nancy%20Lahiri%0AAbstract%3A%20%20%20Road%20accidents%20are%20quite%20common%20in%20almost%20every%20part%20of%20the%20world%2C%20and%2C%20in%0Amajority%2C%20fatal%20accidents%20are%20attributed%20to%20over%20speeding%20of%20vehicles.%20The%0Atendency%20to%20over%20speeding%20is%20usually%20tried%20to%20be%20controlled%20using%20check%20points%0Aat%20various%20parts%20of%20the%20road%20but%20not%20all%20traffic%20police%20have%20the%20device%20to%0Acheck%20speed%20with%20existing%20speed%20estimating%20devices%20such%20as%20LIDAR%20based%2C%20or%0ARadar%20based%20guns.%20The%20current%20project%20tries%20to%20address%20the%20issue%20of%20vehicle%0Aspeed%20estimation%20with%20handheld%20devices%20such%20as%20mobile%20phones%20or%20wearable%0Acameras%20with%20network%20connection%20to%20estimate%20the%20speed%20using%20deep%20learning%0Aframeworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Car%2520Speed%2520using%2520Object%2520Detection%2520and%2520Depth%2520Estimation%253A%2520A%2520Deep%250A%2520%2520Learning%2520Framework%26entry.906535625%3DSubhasis%2520Dasgupta%2520and%2520Arshi%2520Naaz%2520and%2520Jayeeta%2520Choudhury%2520and%2520Nancy%2520Lahiri%26entry.1292438233%3D%2520%2520Road%2520accidents%2520are%2520quite%2520common%2520in%2520almost%2520every%2520part%2520of%2520the%2520world%252C%2520and%252C%2520in%250Amajority%252C%2520fatal%2520accidents%2520are%2520attributed%2520to%2520over%2520speeding%2520of%2520vehicles.%2520The%250Atendency%2520to%2520over%2520speeding%2520is%2520usually%2520tried%2520to%2520be%2520controlled%2520using%2520check%2520points%250Aat%2520various%2520parts%2520of%2520the%2520road%2520but%2520not%2520all%2520traffic%2520police%2520have%2520the%2520device%2520to%250Acheck%2520speed%2520with%2520existing%2520speed%2520estimating%2520devices%2520such%2520as%2520LIDAR%2520based%252C%2520or%250ARadar%2520based%2520guns.%2520The%2520current%2520project%2520tries%2520to%2520address%2520the%2520issue%2520of%2520vehicle%250Aspeed%2520estimation%2520with%2520handheld%2520devices%2520such%2520as%2520mobile%2520phones%2520or%2520wearable%250Acameras%2520with%2520network%2520connection%2520to%2520estimate%2520the%2520speed%2520using%2520deep%2520learning%250Aframeworks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Car%20Speed%20using%20Object%20Detection%20and%20Depth%20Estimation%3A%20A%20Deep%0A%20%20Learning%20Framework&entry.906535625=Subhasis%20Dasgupta%20and%20Arshi%20Naaz%20and%20Jayeeta%20Choudhury%20and%20Nancy%20Lahiri&entry.1292438233=%20%20Road%20accidents%20are%20quite%20common%20in%20almost%20every%20part%20of%20the%20world%2C%20and%2C%20in%0Amajority%2C%20fatal%20accidents%20are%20attributed%20to%20over%20speeding%20of%20vehicles.%20The%0Atendency%20to%20over%20speeding%20is%20usually%20tried%20to%20be%20controlled%20using%20check%20points%0Aat%20various%20parts%20of%20the%20road%20but%20not%20all%20traffic%20police%20have%20the%20device%20to%0Acheck%20speed%20with%20existing%20speed%20estimating%20devices%20such%20as%20LIDAR%20based%2C%20or%0ARadar%20based%20guns.%20The%20current%20project%20tries%20to%20address%20the%20issue%20of%20vehicle%0Aspeed%20estimation%20with%20handheld%20devices%20such%20as%20mobile%20phones%20or%20wearable%0Acameras%20with%20network%20connection%20to%20estimate%20the%20speed%20using%20deep%20learning%0Aframeworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04360v1&entry.124074799=Read"},
{"title": "Analyzing Consumer Reviews for Understanding Drivers of Hotels Ratings:\n  An Indian Perspective", "author": "Subhasis Dasgupta and Soumya Roy and Jaydip Sen", "abstract": "  In the internet era, almost every business entity is trying to have its\ndigital footprint in digital media and other social media platforms. For these\nentities, word of mouse is also very important. Particularly, this is quite\ncrucial for the hospitality sector dealing with hotels, restaurants etc.\nConsumers do read other consumers reviews before making final decisions. This\nis where it becomes very important to understand which aspects are affecting\nmost in the minds of the consumers while giving their ratings. The current\nstudy focuses on the consumer reviews of Indian hotels to extract aspects\nimportant for final ratings. The study involves gathering data using web\nscraping methods, analyzing the texts using Latent Dirichlet Allocation for\ntopic extraction and sentiment analysis for aspect-specific sentiment mapping.\nFinally, it incorporates Random Forest to understand the importance of the\naspects in predicting the final rating of a user.\n", "link": "http://arxiv.org/abs/2408.04369v1", "date": "2024-08-08", "relevancy": 1.8409, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3793}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3627}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20Consumer%20Reviews%20for%20Understanding%20Drivers%20of%20Hotels%20Ratings%3A%0A%20%20An%20Indian%20Perspective&body=Title%3A%20Analyzing%20Consumer%20Reviews%20for%20Understanding%20Drivers%20of%20Hotels%20Ratings%3A%0A%20%20An%20Indian%20Perspective%0AAuthor%3A%20Subhasis%20Dasgupta%20and%20Soumya%20Roy%20and%20Jaydip%20Sen%0AAbstract%3A%20%20%20In%20the%20internet%20era%2C%20almost%20every%20business%20entity%20is%20trying%20to%20have%20its%0Adigital%20footprint%20in%20digital%20media%20and%20other%20social%20media%20platforms.%20For%20these%0Aentities%2C%20word%20of%20mouse%20is%20also%20very%20important.%20Particularly%2C%20this%20is%20quite%0Acrucial%20for%20the%20hospitality%20sector%20dealing%20with%20hotels%2C%20restaurants%20etc.%0AConsumers%20do%20read%20other%20consumers%20reviews%20before%20making%20final%20decisions.%20This%0Ais%20where%20it%20becomes%20very%20important%20to%20understand%20which%20aspects%20are%20affecting%0Amost%20in%20the%20minds%20of%20the%20consumers%20while%20giving%20their%20ratings.%20The%20current%0Astudy%20focuses%20on%20the%20consumer%20reviews%20of%20Indian%20hotels%20to%20extract%20aspects%0Aimportant%20for%20final%20ratings.%20The%20study%20involves%20gathering%20data%20using%20web%0Ascraping%20methods%2C%20analyzing%20the%20texts%20using%20Latent%20Dirichlet%20Allocation%20for%0Atopic%20extraction%20and%20sentiment%20analysis%20for%20aspect-specific%20sentiment%20mapping.%0AFinally%2C%20it%20incorporates%20Random%20Forest%20to%20understand%20the%20importance%20of%20the%0Aaspects%20in%20predicting%20the%20final%20rating%20of%20a%20user.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520Consumer%2520Reviews%2520for%2520Understanding%2520Drivers%2520of%2520Hotels%2520Ratings%253A%250A%2520%2520An%2520Indian%2520Perspective%26entry.906535625%3DSubhasis%2520Dasgupta%2520and%2520Soumya%2520Roy%2520and%2520Jaydip%2520Sen%26entry.1292438233%3D%2520%2520In%2520the%2520internet%2520era%252C%2520almost%2520every%2520business%2520entity%2520is%2520trying%2520to%2520have%2520its%250Adigital%2520footprint%2520in%2520digital%2520media%2520and%2520other%2520social%2520media%2520platforms.%2520For%2520these%250Aentities%252C%2520word%2520of%2520mouse%2520is%2520also%2520very%2520important.%2520Particularly%252C%2520this%2520is%2520quite%250Acrucial%2520for%2520the%2520hospitality%2520sector%2520dealing%2520with%2520hotels%252C%2520restaurants%2520etc.%250AConsumers%2520do%2520read%2520other%2520consumers%2520reviews%2520before%2520making%2520final%2520decisions.%2520This%250Ais%2520where%2520it%2520becomes%2520very%2520important%2520to%2520understand%2520which%2520aspects%2520are%2520affecting%250Amost%2520in%2520the%2520minds%2520of%2520the%2520consumers%2520while%2520giving%2520their%2520ratings.%2520The%2520current%250Astudy%2520focuses%2520on%2520the%2520consumer%2520reviews%2520of%2520Indian%2520hotels%2520to%2520extract%2520aspects%250Aimportant%2520for%2520final%2520ratings.%2520The%2520study%2520involves%2520gathering%2520data%2520using%2520web%250Ascraping%2520methods%252C%2520analyzing%2520the%2520texts%2520using%2520Latent%2520Dirichlet%2520Allocation%2520for%250Atopic%2520extraction%2520and%2520sentiment%2520analysis%2520for%2520aspect-specific%2520sentiment%2520mapping.%250AFinally%252C%2520it%2520incorporates%2520Random%2520Forest%2520to%2520understand%2520the%2520importance%2520of%2520the%250Aaspects%2520in%2520predicting%2520the%2520final%2520rating%2520of%2520a%2520user.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20Consumer%20Reviews%20for%20Understanding%20Drivers%20of%20Hotels%20Ratings%3A%0A%20%20An%20Indian%20Perspective&entry.906535625=Subhasis%20Dasgupta%20and%20Soumya%20Roy%20and%20Jaydip%20Sen&entry.1292438233=%20%20In%20the%20internet%20era%2C%20almost%20every%20business%20entity%20is%20trying%20to%20have%20its%0Adigital%20footprint%20in%20digital%20media%20and%20other%20social%20media%20platforms.%20For%20these%0Aentities%2C%20word%20of%20mouse%20is%20also%20very%20important.%20Particularly%2C%20this%20is%20quite%0Acrucial%20for%20the%20hospitality%20sector%20dealing%20with%20hotels%2C%20restaurants%20etc.%0AConsumers%20do%20read%20other%20consumers%20reviews%20before%20making%20final%20decisions.%20This%0Ais%20where%20it%20becomes%20very%20important%20to%20understand%20which%20aspects%20are%20affecting%0Amost%20in%20the%20minds%20of%20the%20consumers%20while%20giving%20their%20ratings.%20The%20current%0Astudy%20focuses%20on%20the%20consumer%20reviews%20of%20Indian%20hotels%20to%20extract%20aspects%0Aimportant%20for%20final%20ratings.%20The%20study%20involves%20gathering%20data%20using%20web%0Ascraping%20methods%2C%20analyzing%20the%20texts%20using%20Latent%20Dirichlet%20Allocation%20for%0Atopic%20extraction%20and%20sentiment%20analysis%20for%20aspect-specific%20sentiment%20mapping.%0AFinally%2C%20it%20incorporates%20Random%20Forest%20to%20understand%20the%20importance%20of%20the%0Aaspects%20in%20predicting%20the%20final%20rating%20of%20a%20user.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04369v1&entry.124074799=Read"},
{"title": "Judgment2vec: Apply Graph Analytics to Searching and Recommendation of\n  Similar Judgments", "author": "Hsuan-Lei Shao", "abstract": "  In court practice, legal professionals rely on their training to provide\nopinions that resolve cases, one of the most crucial aspects being the ability\nto identify similar judgments from previous courts efficiently. However,\nfinding a similar case is challenging and often depends on experience, legal\ndomain knowledge, and extensive labor hours, making veteran lawyers or judges\nindispensable. This research aims to automate the analysis of judgment text\nsimilarity. We utilized a judgment dataset labeled as the \"golden standard\" by\nexperts, which includes human-verified features that can be converted into an\n\"expert similarity score.\" We then constructed a knowledge graph based on\n\"case-article\" relationships, ranking each case using natural language\nprocessing to derive a \"Node2vec similarity score.\" By evaluating these two\nsimilarity scores, we identified their discrepancies and relationships. The\nresults can significantly reduce the labor hours required for legal searches\nand recommendations, with potential applications extending to various fields of\ninformation retrieval.\n", "link": "http://arxiv.org/abs/2408.04382v1", "date": "2024-08-08", "relevancy": 1.8052, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4808}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4614}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Judgment2vec%3A%20Apply%20Graph%20Analytics%20to%20Searching%20and%20Recommendation%20of%0A%20%20Similar%20Judgments&body=Title%3A%20Judgment2vec%3A%20Apply%20Graph%20Analytics%20to%20Searching%20and%20Recommendation%20of%0A%20%20Similar%20Judgments%0AAuthor%3A%20Hsuan-Lei%20Shao%0AAbstract%3A%20%20%20In%20court%20practice%2C%20legal%20professionals%20rely%20on%20their%20training%20to%20provide%0Aopinions%20that%20resolve%20cases%2C%20one%20of%20the%20most%20crucial%20aspects%20being%20the%20ability%0Ato%20identify%20similar%20judgments%20from%20previous%20courts%20efficiently.%20However%2C%0Afinding%20a%20similar%20case%20is%20challenging%20and%20often%20depends%20on%20experience%2C%20legal%0Adomain%20knowledge%2C%20and%20extensive%20labor%20hours%2C%20making%20veteran%20lawyers%20or%20judges%0Aindispensable.%20This%20research%20aims%20to%20automate%20the%20analysis%20of%20judgment%20text%0Asimilarity.%20We%20utilized%20a%20judgment%20dataset%20labeled%20as%20the%20%22golden%20standard%22%20by%0Aexperts%2C%20which%20includes%20human-verified%20features%20that%20can%20be%20converted%20into%20an%0A%22expert%20similarity%20score.%22%20We%20then%20constructed%20a%20knowledge%20graph%20based%20on%0A%22case-article%22%20relationships%2C%20ranking%20each%20case%20using%20natural%20language%0Aprocessing%20to%20derive%20a%20%22Node2vec%20similarity%20score.%22%20By%20evaluating%20these%20two%0Asimilarity%20scores%2C%20we%20identified%20their%20discrepancies%20and%20relationships.%20The%0Aresults%20can%20significantly%20reduce%20the%20labor%20hours%20required%20for%20legal%20searches%0Aand%20recommendations%2C%20with%20potential%20applications%20extending%20to%20various%20fields%20of%0Ainformation%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04382v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJudgment2vec%253A%2520Apply%2520Graph%2520Analytics%2520to%2520Searching%2520and%2520Recommendation%2520of%250A%2520%2520Similar%2520Judgments%26entry.906535625%3DHsuan-Lei%2520Shao%26entry.1292438233%3D%2520%2520In%2520court%2520practice%252C%2520legal%2520professionals%2520rely%2520on%2520their%2520training%2520to%2520provide%250Aopinions%2520that%2520resolve%2520cases%252C%2520one%2520of%2520the%2520most%2520crucial%2520aspects%2520being%2520the%2520ability%250Ato%2520identify%2520similar%2520judgments%2520from%2520previous%2520courts%2520efficiently.%2520However%252C%250Afinding%2520a%2520similar%2520case%2520is%2520challenging%2520and%2520often%2520depends%2520on%2520experience%252C%2520legal%250Adomain%2520knowledge%252C%2520and%2520extensive%2520labor%2520hours%252C%2520making%2520veteran%2520lawyers%2520or%2520judges%250Aindispensable.%2520This%2520research%2520aims%2520to%2520automate%2520the%2520analysis%2520of%2520judgment%2520text%250Asimilarity.%2520We%2520utilized%2520a%2520judgment%2520dataset%2520labeled%2520as%2520the%2520%2522golden%2520standard%2522%2520by%250Aexperts%252C%2520which%2520includes%2520human-verified%2520features%2520that%2520can%2520be%2520converted%2520into%2520an%250A%2522expert%2520similarity%2520score.%2522%2520We%2520then%2520constructed%2520a%2520knowledge%2520graph%2520based%2520on%250A%2522case-article%2522%2520relationships%252C%2520ranking%2520each%2520case%2520using%2520natural%2520language%250Aprocessing%2520to%2520derive%2520a%2520%2522Node2vec%2520similarity%2520score.%2522%2520By%2520evaluating%2520these%2520two%250Asimilarity%2520scores%252C%2520we%2520identified%2520their%2520discrepancies%2520and%2520relationships.%2520The%250Aresults%2520can%2520significantly%2520reduce%2520the%2520labor%2520hours%2520required%2520for%2520legal%2520searches%250Aand%2520recommendations%252C%2520with%2520potential%2520applications%2520extending%2520to%2520various%2520fields%2520of%250Ainformation%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04382v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Judgment2vec%3A%20Apply%20Graph%20Analytics%20to%20Searching%20and%20Recommendation%20of%0A%20%20Similar%20Judgments&entry.906535625=Hsuan-Lei%20Shao&entry.1292438233=%20%20In%20court%20practice%2C%20legal%20professionals%20rely%20on%20their%20training%20to%20provide%0Aopinions%20that%20resolve%20cases%2C%20one%20of%20the%20most%20crucial%20aspects%20being%20the%20ability%0Ato%20identify%20similar%20judgments%20from%20previous%20courts%20efficiently.%20However%2C%0Afinding%20a%20similar%20case%20is%20challenging%20and%20often%20depends%20on%20experience%2C%20legal%0Adomain%20knowledge%2C%20and%20extensive%20labor%20hours%2C%20making%20veteran%20lawyers%20or%20judges%0Aindispensable.%20This%20research%20aims%20to%20automate%20the%20analysis%20of%20judgment%20text%0Asimilarity.%20We%20utilized%20a%20judgment%20dataset%20labeled%20as%20the%20%22golden%20standard%22%20by%0Aexperts%2C%20which%20includes%20human-verified%20features%20that%20can%20be%20converted%20into%20an%0A%22expert%20similarity%20score.%22%20We%20then%20constructed%20a%20knowledge%20graph%20based%20on%0A%22case-article%22%20relationships%2C%20ranking%20each%20case%20using%20natural%20language%0Aprocessing%20to%20derive%20a%20%22Node2vec%20similarity%20score.%22%20By%20evaluating%20these%20two%0Asimilarity%20scores%2C%20we%20identified%20their%20discrepancies%20and%20relationships.%20The%0Aresults%20can%20significantly%20reduce%20the%20labor%20hours%20required%20for%20legal%20searches%0Aand%20recommendations%2C%20with%20potential%20applications%20extending%20to%20various%20fields%20of%0Ainformation%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04382v1&entry.124074799=Read"},
{"title": "Beyond Recommendations: From Backward to Forward AI Support of Pilots'\n  Decision-Making Process", "author": "Zelun Tony Zhang and Sebastian S. Feger and Lucas Dullenkopf and Rulu Liao and Lukas S\u00fcsslin and Yuanting Liu and Andreas Butz", "abstract": "  AI is anticipated to enhance human decision-making in high-stakes domains\nlike aviation, but adoption is often hindered by challenges such as\ninappropriate reliance and poor alignment with users' decision-making. Recent\nresearch suggests that a core underlying issue is the recommendation-centric\ndesign of many AI systems, i.e., they give end-to-end recommendations and\nignore the rest of the decision-making process. Alternative support paradigms\nare rare, and it remains unclear how the few that do exist compare to\nrecommendation-centric support. In this work, we aimed to empirically compare\nrecommendation-centric support to an alternative paradigm, continuous support,\nin the context of diversions in aviation. We conducted a mixed-methods study\nwith 32 professional pilots in a realistic setting. To ensure the quality of\nour study scenarios, we conducted a focus group with four additional pilots\nprior to the study. We found that continuous support can support pilots'\ndecision-making in a forward direction, allowing them to think more beyond the\nlimits of the system and make faster decisions when combined with\nrecommendations, though the forward support can be disrupted. Participants'\nstatements further suggest a shift in design goal away from providing\nrecommendations, to supporting quick information gathering. Our results show\nways to design more helpful and effective AI decision support that goes beyond\nend-to-end recommendations.\n", "link": "http://arxiv.org/abs/2406.08959v2", "date": "2024-08-08", "relevancy": 1.7998, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.456}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4493}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Recommendations%3A%20From%20Backward%20to%20Forward%20AI%20Support%20of%20Pilots%27%0A%20%20Decision-Making%20Process&body=Title%3A%20Beyond%20Recommendations%3A%20From%20Backward%20to%20Forward%20AI%20Support%20of%20Pilots%27%0A%20%20Decision-Making%20Process%0AAuthor%3A%20Zelun%20Tony%20Zhang%20and%20Sebastian%20S.%20Feger%20and%20Lucas%20Dullenkopf%20and%20Rulu%20Liao%20and%20Lukas%20S%C3%BCsslin%20and%20Yuanting%20Liu%20and%20Andreas%20Butz%0AAbstract%3A%20%20%20AI%20is%20anticipated%20to%20enhance%20human%20decision-making%20in%20high-stakes%20domains%0Alike%20aviation%2C%20but%20adoption%20is%20often%20hindered%20by%20challenges%20such%20as%0Ainappropriate%20reliance%20and%20poor%20alignment%20with%20users%27%20decision-making.%20Recent%0Aresearch%20suggests%20that%20a%20core%20underlying%20issue%20is%20the%20recommendation-centric%0Adesign%20of%20many%20AI%20systems%2C%20i.e.%2C%20they%20give%20end-to-end%20recommendations%20and%0Aignore%20the%20rest%20of%20the%20decision-making%20process.%20Alternative%20support%20paradigms%0Aare%20rare%2C%20and%20it%20remains%20unclear%20how%20the%20few%20that%20do%20exist%20compare%20to%0Arecommendation-centric%20support.%20In%20this%20work%2C%20we%20aimed%20to%20empirically%20compare%0Arecommendation-centric%20support%20to%20an%20alternative%20paradigm%2C%20continuous%20support%2C%0Ain%20the%20context%20of%20diversions%20in%20aviation.%20We%20conducted%20a%20mixed-methods%20study%0Awith%2032%20professional%20pilots%20in%20a%20realistic%20setting.%20To%20ensure%20the%20quality%20of%0Aour%20study%20scenarios%2C%20we%20conducted%20a%20focus%20group%20with%20four%20additional%20pilots%0Aprior%20to%20the%20study.%20We%20found%20that%20continuous%20support%20can%20support%20pilots%27%0Adecision-making%20in%20a%20forward%20direction%2C%20allowing%20them%20to%20think%20more%20beyond%20the%0Alimits%20of%20the%20system%20and%20make%20faster%20decisions%20when%20combined%20with%0Arecommendations%2C%20though%20the%20forward%20support%20can%20be%20disrupted.%20Participants%27%0Astatements%20further%20suggest%20a%20shift%20in%20design%20goal%20away%20from%20providing%0Arecommendations%2C%20to%20supporting%20quick%20information%20gathering.%20Our%20results%20show%0Aways%20to%20design%20more%20helpful%20and%20effective%20AI%20decision%20support%20that%20goes%20beyond%0Aend-to-end%20recommendations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Recommendations%253A%2520From%2520Backward%2520to%2520Forward%2520AI%2520Support%2520of%2520Pilots%2527%250A%2520%2520Decision-Making%2520Process%26entry.906535625%3DZelun%2520Tony%2520Zhang%2520and%2520Sebastian%2520S.%2520Feger%2520and%2520Lucas%2520Dullenkopf%2520and%2520Rulu%2520Liao%2520and%2520Lukas%2520S%25C3%25BCsslin%2520and%2520Yuanting%2520Liu%2520and%2520Andreas%2520Butz%26entry.1292438233%3D%2520%2520AI%2520is%2520anticipated%2520to%2520enhance%2520human%2520decision-making%2520in%2520high-stakes%2520domains%250Alike%2520aviation%252C%2520but%2520adoption%2520is%2520often%2520hindered%2520by%2520challenges%2520such%2520as%250Ainappropriate%2520reliance%2520and%2520poor%2520alignment%2520with%2520users%2527%2520decision-making.%2520Recent%250Aresearch%2520suggests%2520that%2520a%2520core%2520underlying%2520issue%2520is%2520the%2520recommendation-centric%250Adesign%2520of%2520many%2520AI%2520systems%252C%2520i.e.%252C%2520they%2520give%2520end-to-end%2520recommendations%2520and%250Aignore%2520the%2520rest%2520of%2520the%2520decision-making%2520process.%2520Alternative%2520support%2520paradigms%250Aare%2520rare%252C%2520and%2520it%2520remains%2520unclear%2520how%2520the%2520few%2520that%2520do%2520exist%2520compare%2520to%250Arecommendation-centric%2520support.%2520In%2520this%2520work%252C%2520we%2520aimed%2520to%2520empirically%2520compare%250Arecommendation-centric%2520support%2520to%2520an%2520alternative%2520paradigm%252C%2520continuous%2520support%252C%250Ain%2520the%2520context%2520of%2520diversions%2520in%2520aviation.%2520We%2520conducted%2520a%2520mixed-methods%2520study%250Awith%252032%2520professional%2520pilots%2520in%2520a%2520realistic%2520setting.%2520To%2520ensure%2520the%2520quality%2520of%250Aour%2520study%2520scenarios%252C%2520we%2520conducted%2520a%2520focus%2520group%2520with%2520four%2520additional%2520pilots%250Aprior%2520to%2520the%2520study.%2520We%2520found%2520that%2520continuous%2520support%2520can%2520support%2520pilots%2527%250Adecision-making%2520in%2520a%2520forward%2520direction%252C%2520allowing%2520them%2520to%2520think%2520more%2520beyond%2520the%250Alimits%2520of%2520the%2520system%2520and%2520make%2520faster%2520decisions%2520when%2520combined%2520with%250Arecommendations%252C%2520though%2520the%2520forward%2520support%2520can%2520be%2520disrupted.%2520Participants%2527%250Astatements%2520further%2520suggest%2520a%2520shift%2520in%2520design%2520goal%2520away%2520from%2520providing%250Arecommendations%252C%2520to%2520supporting%2520quick%2520information%2520gathering.%2520Our%2520results%2520show%250Aways%2520to%2520design%2520more%2520helpful%2520and%2520effective%2520AI%2520decision%2520support%2520that%2520goes%2520beyond%250Aend-to-end%2520recommendations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Recommendations%3A%20From%20Backward%20to%20Forward%20AI%20Support%20of%20Pilots%27%0A%20%20Decision-Making%20Process&entry.906535625=Zelun%20Tony%20Zhang%20and%20Sebastian%20S.%20Feger%20and%20Lucas%20Dullenkopf%20and%20Rulu%20Liao%20and%20Lukas%20S%C3%BCsslin%20and%20Yuanting%20Liu%20and%20Andreas%20Butz&entry.1292438233=%20%20AI%20is%20anticipated%20to%20enhance%20human%20decision-making%20in%20high-stakes%20domains%0Alike%20aviation%2C%20but%20adoption%20is%20often%20hindered%20by%20challenges%20such%20as%0Ainappropriate%20reliance%20and%20poor%20alignment%20with%20users%27%20decision-making.%20Recent%0Aresearch%20suggests%20that%20a%20core%20underlying%20issue%20is%20the%20recommendation-centric%0Adesign%20of%20many%20AI%20systems%2C%20i.e.%2C%20they%20give%20end-to-end%20recommendations%20and%0Aignore%20the%20rest%20of%20the%20decision-making%20process.%20Alternative%20support%20paradigms%0Aare%20rare%2C%20and%20it%20remains%20unclear%20how%20the%20few%20that%20do%20exist%20compare%20to%0Arecommendation-centric%20support.%20In%20this%20work%2C%20we%20aimed%20to%20empirically%20compare%0Arecommendation-centric%20support%20to%20an%20alternative%20paradigm%2C%20continuous%20support%2C%0Ain%20the%20context%20of%20diversions%20in%20aviation.%20We%20conducted%20a%20mixed-methods%20study%0Awith%2032%20professional%20pilots%20in%20a%20realistic%20setting.%20To%20ensure%20the%20quality%20of%0Aour%20study%20scenarios%2C%20we%20conducted%20a%20focus%20group%20with%20four%20additional%20pilots%0Aprior%20to%20the%20study.%20We%20found%20that%20continuous%20support%20can%20support%20pilots%27%0Adecision-making%20in%20a%20forward%20direction%2C%20allowing%20them%20to%20think%20more%20beyond%20the%0Alimits%20of%20the%20system%20and%20make%20faster%20decisions%20when%20combined%20with%0Arecommendations%2C%20though%20the%20forward%20support%20can%20be%20disrupted.%20Participants%27%0Astatements%20further%20suggest%20a%20shift%20in%20design%20goal%20away%20from%20providing%0Arecommendations%2C%20to%20supporting%20quick%20information%20gathering.%20Our%20results%20show%0Aways%20to%20design%20more%20helpful%20and%20effective%20AI%20decision%20support%20that%20goes%20beyond%0Aend-to-end%20recommendations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08959v2&entry.124074799=Read"},
{"title": "AExGym: Benchmarks and Environments for Adaptive Experimentation", "author": "Jimmy Wang and Ethan Che and Daniel R. Jiang and Hongseok Namkoong", "abstract": "  Innovations across science and industry are evaluated using randomized trials\n(a.k.a. A/B tests). While simple and robust, such static designs are\ninefficient or infeasible for testing many hypotheses. Adaptive designs can\ngreatly improve statistical power in theory, but they have seen limited\nadoption due to their fragility in practice. We present a benchmark for\nadaptive experimentation based on real-world datasets, highlighting prominent\npractical challenges to operationalizing adaptivity: non-stationarity,\nbatched/delayed feedback, multiple outcomes and objectives, and external\nvalidity. Our benchmark aims to spur methodological development that puts\npractical performance (e.g., robustness) as a central concern, rather than\nmathematical guarantees on contrived instances. We release an open source\nlibrary, AExGym, which is designed with modularity and extensibility in mind to\nallow experimentation practitioners to develop custom environments and\nalgorithms.\n", "link": "http://arxiv.org/abs/2408.04531v1", "date": "2024-08-08", "relevancy": 1.7814, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5097}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4436}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AExGym%3A%20Benchmarks%20and%20Environments%20for%20Adaptive%20Experimentation&body=Title%3A%20AExGym%3A%20Benchmarks%20and%20Environments%20for%20Adaptive%20Experimentation%0AAuthor%3A%20Jimmy%20Wang%20and%20Ethan%20Che%20and%20Daniel%20R.%20Jiang%20and%20Hongseok%20Namkoong%0AAbstract%3A%20%20%20Innovations%20across%20science%20and%20industry%20are%20evaluated%20using%20randomized%20trials%0A%28a.k.a.%20A/B%20tests%29.%20While%20simple%20and%20robust%2C%20such%20static%20designs%20are%0Ainefficient%20or%20infeasible%20for%20testing%20many%20hypotheses.%20Adaptive%20designs%20can%0Agreatly%20improve%20statistical%20power%20in%20theory%2C%20but%20they%20have%20seen%20limited%0Aadoption%20due%20to%20their%20fragility%20in%20practice.%20We%20present%20a%20benchmark%20for%0Aadaptive%20experimentation%20based%20on%20real-world%20datasets%2C%20highlighting%20prominent%0Apractical%20challenges%20to%20operationalizing%20adaptivity%3A%20non-stationarity%2C%0Abatched/delayed%20feedback%2C%20multiple%20outcomes%20and%20objectives%2C%20and%20external%0Avalidity.%20Our%20benchmark%20aims%20to%20spur%20methodological%20development%20that%20puts%0Apractical%20performance%20%28e.g.%2C%20robustness%29%20as%20a%20central%20concern%2C%20rather%20than%0Amathematical%20guarantees%20on%20contrived%20instances.%20We%20release%20an%20open%20source%0Alibrary%2C%20AExGym%2C%20which%20is%20designed%20with%20modularity%20and%20extensibility%20in%20mind%20to%0Aallow%20experimentation%20practitioners%20to%20develop%20custom%20environments%20and%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAExGym%253A%2520Benchmarks%2520and%2520Environments%2520for%2520Adaptive%2520Experimentation%26entry.906535625%3DJimmy%2520Wang%2520and%2520Ethan%2520Che%2520and%2520Daniel%2520R.%2520Jiang%2520and%2520Hongseok%2520Namkoong%26entry.1292438233%3D%2520%2520Innovations%2520across%2520science%2520and%2520industry%2520are%2520evaluated%2520using%2520randomized%2520trials%250A%2528a.k.a.%2520A/B%2520tests%2529.%2520While%2520simple%2520and%2520robust%252C%2520such%2520static%2520designs%2520are%250Ainefficient%2520or%2520infeasible%2520for%2520testing%2520many%2520hypotheses.%2520Adaptive%2520designs%2520can%250Agreatly%2520improve%2520statistical%2520power%2520in%2520theory%252C%2520but%2520they%2520have%2520seen%2520limited%250Aadoption%2520due%2520to%2520their%2520fragility%2520in%2520practice.%2520We%2520present%2520a%2520benchmark%2520for%250Aadaptive%2520experimentation%2520based%2520on%2520real-world%2520datasets%252C%2520highlighting%2520prominent%250Apractical%2520challenges%2520to%2520operationalizing%2520adaptivity%253A%2520non-stationarity%252C%250Abatched/delayed%2520feedback%252C%2520multiple%2520outcomes%2520and%2520objectives%252C%2520and%2520external%250Avalidity.%2520Our%2520benchmark%2520aims%2520to%2520spur%2520methodological%2520development%2520that%2520puts%250Apractical%2520performance%2520%2528e.g.%252C%2520robustness%2529%2520as%2520a%2520central%2520concern%252C%2520rather%2520than%250Amathematical%2520guarantees%2520on%2520contrived%2520instances.%2520We%2520release%2520an%2520open%2520source%250Alibrary%252C%2520AExGym%252C%2520which%2520is%2520designed%2520with%2520modularity%2520and%2520extensibility%2520in%2520mind%2520to%250Aallow%2520experimentation%2520practitioners%2520to%2520develop%2520custom%2520environments%2520and%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AExGym%3A%20Benchmarks%20and%20Environments%20for%20Adaptive%20Experimentation&entry.906535625=Jimmy%20Wang%20and%20Ethan%20Che%20and%20Daniel%20R.%20Jiang%20and%20Hongseok%20Namkoong&entry.1292438233=%20%20Innovations%20across%20science%20and%20industry%20are%20evaluated%20using%20randomized%20trials%0A%28a.k.a.%20A/B%20tests%29.%20While%20simple%20and%20robust%2C%20such%20static%20designs%20are%0Ainefficient%20or%20infeasible%20for%20testing%20many%20hypotheses.%20Adaptive%20designs%20can%0Agreatly%20improve%20statistical%20power%20in%20theory%2C%20but%20they%20have%20seen%20limited%0Aadoption%20due%20to%20their%20fragility%20in%20practice.%20We%20present%20a%20benchmark%20for%0Aadaptive%20experimentation%20based%20on%20real-world%20datasets%2C%20highlighting%20prominent%0Apractical%20challenges%20to%20operationalizing%20adaptivity%3A%20non-stationarity%2C%0Abatched/delayed%20feedback%2C%20multiple%20outcomes%20and%20objectives%2C%20and%20external%0Avalidity.%20Our%20benchmark%20aims%20to%20spur%20methodological%20development%20that%20puts%0Apractical%20performance%20%28e.g.%2C%20robustness%29%20as%20a%20central%20concern%2C%20rather%20than%0Amathematical%20guarantees%20on%20contrived%20instances.%20We%20release%20an%20open%20source%0Alibrary%2C%20AExGym%2C%20which%20is%20designed%20with%20modularity%20and%20extensibility%20in%20mind%20to%0Aallow%20experimentation%20practitioners%20to%20develop%20custom%20environments%20and%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04531v1&entry.124074799=Read"},
{"title": "Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models", "author": "Qirui Jiao and Daoyuan Chen and Yilun Huang and Yaliang Li and Ying Shen", "abstract": "  High-performance Multimodal Large Language Models (MLLMs) rely heavily on\ndata quality. This study introduces a novel dataset named Img-Diff, designed to\nenhance fine-grained image recognition in MLLMs by leveraging insights from\ncontrastive learning and image difference captioning. By analyzing object\ndifferences between similar images, we challenge models to identify both\nmatching and distinct components. We utilize the Stable-Diffusion-XL model and\nadvanced image editing techniques to create pairs of similar images that\nhighlight object replacements. Our methodology includes a Difference Area\nGenerator for object differences identifying, followed by a Difference Captions\nGenerator for detailed difference descriptions. The result is a relatively\nsmall but high-quality dataset of \"object replacement\" samples. We use the the\nproposed dataset to fine-tune state-of-the-art (SOTA) MLLMs such as MGM-7B,\nyielding comprehensive improvements of performance scores over SOTA models that\ntrained with larger-scale datasets, in numerous image difference and Visual\nQuestion Answering tasks. For instance, our trained models notably surpass the\nSOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigate\nalternative methods for generating image difference data through \"object\nremoval\" and conduct thorough evaluation to confirm the dataset's diversity,\nquality, and robustness, presenting several insights on synthesis of such\ncontrastive dataset. To encourage further research and advance the field of\nmultimodal data synthesis and enhancement of MLLMs' fundamental capabilities\nfor image understanding, we release our codes and dataset at\nhttps://github.com/modelscope/data-juicer/tree/ImgDiff.\n", "link": "http://arxiv.org/abs/2408.04594v1", "date": "2024-08-08", "relevancy": 1.7701, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6166}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5912}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5789}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Img-Diff%3A%20Contrastive%20Data%20Synthesis%20for%20Multimodal%20Large%20Language%0A%20%20Models&body=Title%3A%20Img-Diff%3A%20Contrastive%20Data%20Synthesis%20for%20Multimodal%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Yaliang%20Li%20and%20Ying%20Shen%0AAbstract%3A%20%20%20High-performance%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20rely%20heavily%20on%0Adata%20quality.%20This%20study%20introduces%20a%20novel%20dataset%20named%20Img-Diff%2C%20designed%20to%0Aenhance%20fine-grained%20image%20recognition%20in%20MLLMs%20by%20leveraging%20insights%20from%0Acontrastive%20learning%20and%20image%20difference%20captioning.%20By%20analyzing%20object%0Adifferences%20between%20similar%20images%2C%20we%20challenge%20models%20to%20identify%20both%0Amatching%20and%20distinct%20components.%20We%20utilize%20the%20Stable-Diffusion-XL%20model%20and%0Aadvanced%20image%20editing%20techniques%20to%20create%20pairs%20of%20similar%20images%20that%0Ahighlight%20object%20replacements.%20Our%20methodology%20includes%20a%20Difference%20Area%0AGenerator%20for%20object%20differences%20identifying%2C%20followed%20by%20a%20Difference%20Captions%0AGenerator%20for%20detailed%20difference%20descriptions.%20The%20result%20is%20a%20relatively%0Asmall%20but%20high-quality%20dataset%20of%20%22object%20replacement%22%20samples.%20We%20use%20the%20the%0Aproposed%20dataset%20to%20fine-tune%20state-of-the-art%20%28SOTA%29%20MLLMs%20such%20as%20MGM-7B%2C%0Ayielding%20comprehensive%20improvements%20of%20performance%20scores%20over%20SOTA%20models%20that%0Atrained%20with%20larger-scale%20datasets%2C%20in%20numerous%20image%20difference%20and%20Visual%0AQuestion%20Answering%20tasks.%20For%20instance%2C%20our%20trained%20models%20notably%20surpass%20the%0ASOTA%20models%20GPT-4V%20and%20Gemini%20on%20the%20MMVP%20benchmark.%20Besides%2C%20we%20investigate%0Aalternative%20methods%20for%20generating%20image%20difference%20data%20through%20%22object%0Aremoval%22%20and%20conduct%20thorough%20evaluation%20to%20confirm%20the%20dataset%27s%20diversity%2C%0Aquality%2C%20and%20robustness%2C%20presenting%20several%20insights%20on%20synthesis%20of%20such%0Acontrastive%20dataset.%20To%20encourage%20further%20research%20and%20advance%20the%20field%20of%0Amultimodal%20data%20synthesis%20and%20enhancement%20of%20MLLMs%27%20fundamental%20capabilities%0Afor%20image%20understanding%2C%20we%20release%20our%20codes%20and%20dataset%20at%0Ahttps%3A//github.com/modelscope/data-juicer/tree/ImgDiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImg-Diff%253A%2520Contrastive%2520Data%2520Synthesis%2520for%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DQirui%2520Jiao%2520and%2520Daoyuan%2520Chen%2520and%2520Yilun%2520Huang%2520and%2520Yaliang%2520Li%2520and%2520Ying%2520Shen%26entry.1292438233%3D%2520%2520High-performance%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520rely%2520heavily%2520on%250Adata%2520quality.%2520This%2520study%2520introduces%2520a%2520novel%2520dataset%2520named%2520Img-Diff%252C%2520designed%2520to%250Aenhance%2520fine-grained%2520image%2520recognition%2520in%2520MLLMs%2520by%2520leveraging%2520insights%2520from%250Acontrastive%2520learning%2520and%2520image%2520difference%2520captioning.%2520By%2520analyzing%2520object%250Adifferences%2520between%2520similar%2520images%252C%2520we%2520challenge%2520models%2520to%2520identify%2520both%250Amatching%2520and%2520distinct%2520components.%2520We%2520utilize%2520the%2520Stable-Diffusion-XL%2520model%2520and%250Aadvanced%2520image%2520editing%2520techniques%2520to%2520create%2520pairs%2520of%2520similar%2520images%2520that%250Ahighlight%2520object%2520replacements.%2520Our%2520methodology%2520includes%2520a%2520Difference%2520Area%250AGenerator%2520for%2520object%2520differences%2520identifying%252C%2520followed%2520by%2520a%2520Difference%2520Captions%250AGenerator%2520for%2520detailed%2520difference%2520descriptions.%2520The%2520result%2520is%2520a%2520relatively%250Asmall%2520but%2520high-quality%2520dataset%2520of%2520%2522object%2520replacement%2522%2520samples.%2520We%2520use%2520the%2520the%250Aproposed%2520dataset%2520to%2520fine-tune%2520state-of-the-art%2520%2528SOTA%2529%2520MLLMs%2520such%2520as%2520MGM-7B%252C%250Ayielding%2520comprehensive%2520improvements%2520of%2520performance%2520scores%2520over%2520SOTA%2520models%2520that%250Atrained%2520with%2520larger-scale%2520datasets%252C%2520in%2520numerous%2520image%2520difference%2520and%2520Visual%250AQuestion%2520Answering%2520tasks.%2520For%2520instance%252C%2520our%2520trained%2520models%2520notably%2520surpass%2520the%250ASOTA%2520models%2520GPT-4V%2520and%2520Gemini%2520on%2520the%2520MMVP%2520benchmark.%2520Besides%252C%2520we%2520investigate%250Aalternative%2520methods%2520for%2520generating%2520image%2520difference%2520data%2520through%2520%2522object%250Aremoval%2522%2520and%2520conduct%2520thorough%2520evaluation%2520to%2520confirm%2520the%2520dataset%2527s%2520diversity%252C%250Aquality%252C%2520and%2520robustness%252C%2520presenting%2520several%2520insights%2520on%2520synthesis%2520of%2520such%250Acontrastive%2520dataset.%2520To%2520encourage%2520further%2520research%2520and%2520advance%2520the%2520field%2520of%250Amultimodal%2520data%2520synthesis%2520and%2520enhancement%2520of%2520MLLMs%2527%2520fundamental%2520capabilities%250Afor%2520image%2520understanding%252C%2520we%2520release%2520our%2520codes%2520and%2520dataset%2520at%250Ahttps%253A//github.com/modelscope/data-juicer/tree/ImgDiff.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Img-Diff%3A%20Contrastive%20Data%20Synthesis%20for%20Multimodal%20Large%20Language%0A%20%20Models&entry.906535625=Qirui%20Jiao%20and%20Daoyuan%20Chen%20and%20Yilun%20Huang%20and%20Yaliang%20Li%20and%20Ying%20Shen&entry.1292438233=%20%20High-performance%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20rely%20heavily%20on%0Adata%20quality.%20This%20study%20introduces%20a%20novel%20dataset%20named%20Img-Diff%2C%20designed%20to%0Aenhance%20fine-grained%20image%20recognition%20in%20MLLMs%20by%20leveraging%20insights%20from%0Acontrastive%20learning%20and%20image%20difference%20captioning.%20By%20analyzing%20object%0Adifferences%20between%20similar%20images%2C%20we%20challenge%20models%20to%20identify%20both%0Amatching%20and%20distinct%20components.%20We%20utilize%20the%20Stable-Diffusion-XL%20model%20and%0Aadvanced%20image%20editing%20techniques%20to%20create%20pairs%20of%20similar%20images%20that%0Ahighlight%20object%20replacements.%20Our%20methodology%20includes%20a%20Difference%20Area%0AGenerator%20for%20object%20differences%20identifying%2C%20followed%20by%20a%20Difference%20Captions%0AGenerator%20for%20detailed%20difference%20descriptions.%20The%20result%20is%20a%20relatively%0Asmall%20but%20high-quality%20dataset%20of%20%22object%20replacement%22%20samples.%20We%20use%20the%20the%0Aproposed%20dataset%20to%20fine-tune%20state-of-the-art%20%28SOTA%29%20MLLMs%20such%20as%20MGM-7B%2C%0Ayielding%20comprehensive%20improvements%20of%20performance%20scores%20over%20SOTA%20models%20that%0Atrained%20with%20larger-scale%20datasets%2C%20in%20numerous%20image%20difference%20and%20Visual%0AQuestion%20Answering%20tasks.%20For%20instance%2C%20our%20trained%20models%20notably%20surpass%20the%0ASOTA%20models%20GPT-4V%20and%20Gemini%20on%20the%20MMVP%20benchmark.%20Besides%2C%20we%20investigate%0Aalternative%20methods%20for%20generating%20image%20difference%20data%20through%20%22object%0Aremoval%22%20and%20conduct%20thorough%20evaluation%20to%20confirm%20the%20dataset%27s%20diversity%2C%0Aquality%2C%20and%20robustness%2C%20presenting%20several%20insights%20on%20synthesis%20of%20such%0Acontrastive%20dataset.%20To%20encourage%20further%20research%20and%20advance%20the%20field%20of%0Amultimodal%20data%20synthesis%20and%20enhancement%20of%20MLLMs%27%20fundamental%20capabilities%0Afor%20image%20understanding%2C%20we%20release%20our%20codes%20and%20dataset%20at%0Ahttps%3A//github.com/modelscope/data-juicer/tree/ImgDiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04594v1&entry.124074799=Read"},
{"title": "LogogramNLP: Comparing Visual and Textual Representations of Ancient\n  Logographic Writing Systems for NLP", "author": "Danlu Chen and Freda Shi and Aditi Agarwal and Jacobo Myerston and Taylor Berg-Kirkpatrick", "abstract": "  Standard natural language processing (NLP) pipelines operate on symbolic\nrepresentations of language, which typically consist of sequences of discrete\ntokens. However, creating an analogous representation for ancient logographic\nwriting systems is an extremely labor intensive process that requires expert\nknowledge. At present, a large portion of logographic data persists in a purely\nvisual form due to the absence of transcription -- this issue poses a\nbottleneck for researchers seeking to apply NLP toolkits to study ancient\nlogographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations\nof language offers a potential solution. We introduce LogogramNLP, the first\nbenchmark enabling NLP analysis of ancient logographic languages, featuring\nboth transcribed and visual datasets for four writing systems along with\nannotations for tasks like classification, translation, and parsing. Our\nexperiments compare systems that employ recent visual and text encoding\nstrategies as backbones. The results demonstrate that visual representations\noutperform textual representations for some investigated tasks, suggesting that\nvisual processing pipelines may unlock a large amount of cultural heritage data\nof logographic languages for NLP-based analyses.\n", "link": "http://arxiv.org/abs/2408.04628v1", "date": "2024-08-08", "relevancy": 1.7669, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4602}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4381}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LogogramNLP%3A%20Comparing%20Visual%20and%20Textual%20Representations%20of%20Ancient%0A%20%20Logographic%20Writing%20Systems%20for%20NLP&body=Title%3A%20LogogramNLP%3A%20Comparing%20Visual%20and%20Textual%20Representations%20of%20Ancient%0A%20%20Logographic%20Writing%20Systems%20for%20NLP%0AAuthor%3A%20Danlu%20Chen%20and%20Freda%20Shi%20and%20Aditi%20Agarwal%20and%20Jacobo%20Myerston%20and%20Taylor%20Berg-Kirkpatrick%0AAbstract%3A%20%20%20Standard%20natural%20language%20processing%20%28NLP%29%20pipelines%20operate%20on%20symbolic%0Arepresentations%20of%20language%2C%20which%20typically%20consist%20of%20sequences%20of%20discrete%0Atokens.%20However%2C%20creating%20an%20analogous%20representation%20for%20ancient%20logographic%0Awriting%20systems%20is%20an%20extremely%20labor%20intensive%20process%20that%20requires%20expert%0Aknowledge.%20At%20present%2C%20a%20large%20portion%20of%20logographic%20data%20persists%20in%20a%20purely%0Avisual%20form%20due%20to%20the%20absence%20of%20transcription%20--%20this%20issue%20poses%20a%0Abottleneck%20for%20researchers%20seeking%20to%20apply%20NLP%20toolkits%20to%20study%20ancient%0Alogographic%20languages%3A%20most%20of%20the%20relevant%20data%20are%20images%20of%20writing.%0A%20%20This%20paper%20investigates%20whether%20direct%20processing%20of%20visual%20representations%0Aof%20language%20offers%20a%20potential%20solution.%20We%20introduce%20LogogramNLP%2C%20the%20first%0Abenchmark%20enabling%20NLP%20analysis%20of%20ancient%20logographic%20languages%2C%20featuring%0Aboth%20transcribed%20and%20visual%20datasets%20for%20four%20writing%20systems%20along%20with%0Aannotations%20for%20tasks%20like%20classification%2C%20translation%2C%20and%20parsing.%20Our%0Aexperiments%20compare%20systems%20that%20employ%20recent%20visual%20and%20text%20encoding%0Astrategies%20as%20backbones.%20The%20results%20demonstrate%20that%20visual%20representations%0Aoutperform%20textual%20representations%20for%20some%20investigated%20tasks%2C%20suggesting%20that%0Avisual%20processing%20pipelines%20may%20unlock%20a%20large%20amount%20of%20cultural%20heritage%20data%0Aof%20logographic%20languages%20for%20NLP-based%20analyses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04628v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogogramNLP%253A%2520Comparing%2520Visual%2520and%2520Textual%2520Representations%2520of%2520Ancient%250A%2520%2520Logographic%2520Writing%2520Systems%2520for%2520NLP%26entry.906535625%3DDanlu%2520Chen%2520and%2520Freda%2520Shi%2520and%2520Aditi%2520Agarwal%2520and%2520Jacobo%2520Myerston%2520and%2520Taylor%2520Berg-Kirkpatrick%26entry.1292438233%3D%2520%2520Standard%2520natural%2520language%2520processing%2520%2528NLP%2529%2520pipelines%2520operate%2520on%2520symbolic%250Arepresentations%2520of%2520language%252C%2520which%2520typically%2520consist%2520of%2520sequences%2520of%2520discrete%250Atokens.%2520However%252C%2520creating%2520an%2520analogous%2520representation%2520for%2520ancient%2520logographic%250Awriting%2520systems%2520is%2520an%2520extremely%2520labor%2520intensive%2520process%2520that%2520requires%2520expert%250Aknowledge.%2520At%2520present%252C%2520a%2520large%2520portion%2520of%2520logographic%2520data%2520persists%2520in%2520a%2520purely%250Avisual%2520form%2520due%2520to%2520the%2520absence%2520of%2520transcription%2520--%2520this%2520issue%2520poses%2520a%250Abottleneck%2520for%2520researchers%2520seeking%2520to%2520apply%2520NLP%2520toolkits%2520to%2520study%2520ancient%250Alogographic%2520languages%253A%2520most%2520of%2520the%2520relevant%2520data%2520are%2520images%2520of%2520writing.%250A%2520%2520This%2520paper%2520investigates%2520whether%2520direct%2520processing%2520of%2520visual%2520representations%250Aof%2520language%2520offers%2520a%2520potential%2520solution.%2520We%2520introduce%2520LogogramNLP%252C%2520the%2520first%250Abenchmark%2520enabling%2520NLP%2520analysis%2520of%2520ancient%2520logographic%2520languages%252C%2520featuring%250Aboth%2520transcribed%2520and%2520visual%2520datasets%2520for%2520four%2520writing%2520systems%2520along%2520with%250Aannotations%2520for%2520tasks%2520like%2520classification%252C%2520translation%252C%2520and%2520parsing.%2520Our%250Aexperiments%2520compare%2520systems%2520that%2520employ%2520recent%2520visual%2520and%2520text%2520encoding%250Astrategies%2520as%2520backbones.%2520The%2520results%2520demonstrate%2520that%2520visual%2520representations%250Aoutperform%2520textual%2520representations%2520for%2520some%2520investigated%2520tasks%252C%2520suggesting%2520that%250Avisual%2520processing%2520pipelines%2520may%2520unlock%2520a%2520large%2520amount%2520of%2520cultural%2520heritage%2520data%250Aof%2520logographic%2520languages%2520for%2520NLP-based%2520analyses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04628v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogogramNLP%3A%20Comparing%20Visual%20and%20Textual%20Representations%20of%20Ancient%0A%20%20Logographic%20Writing%20Systems%20for%20NLP&entry.906535625=Danlu%20Chen%20and%20Freda%20Shi%20and%20Aditi%20Agarwal%20and%20Jacobo%20Myerston%20and%20Taylor%20Berg-Kirkpatrick&entry.1292438233=%20%20Standard%20natural%20language%20processing%20%28NLP%29%20pipelines%20operate%20on%20symbolic%0Arepresentations%20of%20language%2C%20which%20typically%20consist%20of%20sequences%20of%20discrete%0Atokens.%20However%2C%20creating%20an%20analogous%20representation%20for%20ancient%20logographic%0Awriting%20systems%20is%20an%20extremely%20labor%20intensive%20process%20that%20requires%20expert%0Aknowledge.%20At%20present%2C%20a%20large%20portion%20of%20logographic%20data%20persists%20in%20a%20purely%0Avisual%20form%20due%20to%20the%20absence%20of%20transcription%20--%20this%20issue%20poses%20a%0Abottleneck%20for%20researchers%20seeking%20to%20apply%20NLP%20toolkits%20to%20study%20ancient%0Alogographic%20languages%3A%20most%20of%20the%20relevant%20data%20are%20images%20of%20writing.%0A%20%20This%20paper%20investigates%20whether%20direct%20processing%20of%20visual%20representations%0Aof%20language%20offers%20a%20potential%20solution.%20We%20introduce%20LogogramNLP%2C%20the%20first%0Abenchmark%20enabling%20NLP%20analysis%20of%20ancient%20logographic%20languages%2C%20featuring%0Aboth%20transcribed%20and%20visual%20datasets%20for%20four%20writing%20systems%20along%20with%0Aannotations%20for%20tasks%20like%20classification%2C%20translation%2C%20and%20parsing.%20Our%0Aexperiments%20compare%20systems%20that%20employ%20recent%20visual%20and%20text%20encoding%0Astrategies%20as%20backbones.%20The%20results%20demonstrate%20that%20visual%20representations%0Aoutperform%20textual%20representations%20for%20some%20investigated%20tasks%2C%20suggesting%20that%0Avisual%20processing%20pipelines%20may%20unlock%20a%20large%20amount%20of%20cultural%20heritage%20data%0Aof%20logographic%20languages%20for%20NLP-based%20analyses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04628v1&entry.124074799=Read"},
{"title": "The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums", "author": "Vanessa Clairoux-Trepanier and Isa-May Beauchamp and Estelle Ruellan and Masarah Paquet-Clouston and Serge-Olivier Paquette and Eric Clay", "abstract": "  Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the accuracy of an LLM system\nbuilt on the OpenAI GPT-3.5-turbo model [7] to extract CTI information. To do\nso, a random sample of 500 daily conversations from three cybercrime forums,\nXSS, Exploit_in, and RAMP, was extracted, and the LLM system was instructed to\nsummarize the conversations and code 10 key CTI variables, such as whether a\nlarge organization and/or a critical infrastructure is being targeted. Then,\ntwo coders reviewed each conversation and evaluated whether the information\nextracted by the LLM was accurate. The LLM system performed strikingly well,\nwith an average accuracy score of 98%. Various ways to enhance the model were\nuncovered, such as the need to help the LLM distinguish between stories and\npast events, as well as being careful with verb tenses in prompts.\nNevertheless, the results of this study highlight the efficiency and relevance\nof using LLMs for cyber threat intelligence.\n", "link": "http://arxiv.org/abs/2408.03354v2", "date": "2024-08-08", "relevancy": 1.7626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4637}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.45}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4139}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Use%20of%20Large%20Language%20Models%20%28LLM%29%20for%20Cyber%20Threat%20Intelligence%0A%20%20%28CTI%29%20in%20Cybercrime%20Forums&body=Title%3A%20The%20Use%20of%20Large%20Language%20Models%20%28LLM%29%20for%20Cyber%20Threat%20Intelligence%0A%20%20%28CTI%29%20in%20Cybercrime%20Forums%0AAuthor%3A%20Vanessa%20Clairoux-Trepanier%20and%20Isa-May%20Beauchamp%20and%20Estelle%20Ruellan%20and%20Masarah%20Paquet-Clouston%20and%20Serge-Olivier%20Paquette%20and%20Eric%20Clay%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20can%20be%20used%20to%20analyze%20cyber%20threat%20intelligence%0A%28CTI%29%20data%20from%20cybercrime%20forums%2C%20which%20contain%20extensive%20information%20and%20key%0Adiscussions%20about%20emerging%20cyber%20threats.%20However%2C%20to%20date%2C%20the%20level%20of%0Aaccuracy%20and%20efficiency%20of%20LLMs%20for%20such%20critical%20tasks%20has%20yet%20to%20be%0Athoroughly%20evaluated.%20Hence%2C%20this%20study%20assesses%20the%20accuracy%20of%20an%20LLM%20system%0Abuilt%20on%20the%20OpenAI%20GPT-3.5-turbo%20model%20%5B7%5D%20to%20extract%20CTI%20information.%20To%20do%0Aso%2C%20a%20random%20sample%20of%20500%20daily%20conversations%20from%20three%20cybercrime%20forums%2C%0AXSS%2C%20Exploit_in%2C%20and%20RAMP%2C%20was%20extracted%2C%20and%20the%20LLM%20system%20was%20instructed%20to%0Asummarize%20the%20conversations%20and%20code%2010%20key%20CTI%20variables%2C%20such%20as%20whether%20a%0Alarge%20organization%20and/or%20a%20critical%20infrastructure%20is%20being%20targeted.%20Then%2C%0Atwo%20coders%20reviewed%20each%20conversation%20and%20evaluated%20whether%20the%20information%0Aextracted%20by%20the%20LLM%20was%20accurate.%20The%20LLM%20system%20performed%20strikingly%20well%2C%0Awith%20an%20average%20accuracy%20score%20of%2098%25.%20Various%20ways%20to%20enhance%20the%20model%20were%0Auncovered%2C%20such%20as%20the%20need%20to%20help%20the%20LLM%20distinguish%20between%20stories%20and%0Apast%20events%2C%20as%20well%20as%20being%20careful%20with%20verb%20tenses%20in%20prompts.%0ANevertheless%2C%20the%20results%20of%20this%20study%20highlight%20the%20efficiency%20and%20relevance%0Aof%20using%20LLMs%20for%20cyber%20threat%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03354v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Use%2520of%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520for%2520Cyber%2520Threat%2520Intelligence%250A%2520%2520%2528CTI%2529%2520in%2520Cybercrime%2520Forums%26entry.906535625%3DVanessa%2520Clairoux-Trepanier%2520and%2520Isa-May%2520Beauchamp%2520and%2520Estelle%2520Ruellan%2520and%2520Masarah%2520Paquet-Clouston%2520and%2520Serge-Olivier%2520Paquette%2520and%2520Eric%2520Clay%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520can%2520be%2520used%2520to%2520analyze%2520cyber%2520threat%2520intelligence%250A%2528CTI%2529%2520data%2520from%2520cybercrime%2520forums%252C%2520which%2520contain%2520extensive%2520information%2520and%2520key%250Adiscussions%2520about%2520emerging%2520cyber%2520threats.%2520However%252C%2520to%2520date%252C%2520the%2520level%2520of%250Aaccuracy%2520and%2520efficiency%2520of%2520LLMs%2520for%2520such%2520critical%2520tasks%2520has%2520yet%2520to%2520be%250Athoroughly%2520evaluated.%2520Hence%252C%2520this%2520study%2520assesses%2520the%2520accuracy%2520of%2520an%2520LLM%2520system%250Abuilt%2520on%2520the%2520OpenAI%2520GPT-3.5-turbo%2520model%2520%255B7%255D%2520to%2520extract%2520CTI%2520information.%2520To%2520do%250Aso%252C%2520a%2520random%2520sample%2520of%2520500%2520daily%2520conversations%2520from%2520three%2520cybercrime%2520forums%252C%250AXSS%252C%2520Exploit_in%252C%2520and%2520RAMP%252C%2520was%2520extracted%252C%2520and%2520the%2520LLM%2520system%2520was%2520instructed%2520to%250Asummarize%2520the%2520conversations%2520and%2520code%252010%2520key%2520CTI%2520variables%252C%2520such%2520as%2520whether%2520a%250Alarge%2520organization%2520and/or%2520a%2520critical%2520infrastructure%2520is%2520being%2520targeted.%2520Then%252C%250Atwo%2520coders%2520reviewed%2520each%2520conversation%2520and%2520evaluated%2520whether%2520the%2520information%250Aextracted%2520by%2520the%2520LLM%2520was%2520accurate.%2520The%2520LLM%2520system%2520performed%2520strikingly%2520well%252C%250Awith%2520an%2520average%2520accuracy%2520score%2520of%252098%2525.%2520Various%2520ways%2520to%2520enhance%2520the%2520model%2520were%250Auncovered%252C%2520such%2520as%2520the%2520need%2520to%2520help%2520the%2520LLM%2520distinguish%2520between%2520stories%2520and%250Apast%2520events%252C%2520as%2520well%2520as%2520being%2520careful%2520with%2520verb%2520tenses%2520in%2520prompts.%250ANevertheless%252C%2520the%2520results%2520of%2520this%2520study%2520highlight%2520the%2520efficiency%2520and%2520relevance%250Aof%2520using%2520LLMs%2520for%2520cyber%2520threat%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03354v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Use%20of%20Large%20Language%20Models%20%28LLM%29%20for%20Cyber%20Threat%20Intelligence%0A%20%20%28CTI%29%20in%20Cybercrime%20Forums&entry.906535625=Vanessa%20Clairoux-Trepanier%20and%20Isa-May%20Beauchamp%20and%20Estelle%20Ruellan%20and%20Masarah%20Paquet-Clouston%20and%20Serge-Olivier%20Paquette%20and%20Eric%20Clay&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20can%20be%20used%20to%20analyze%20cyber%20threat%20intelligence%0A%28CTI%29%20data%20from%20cybercrime%20forums%2C%20which%20contain%20extensive%20information%20and%20key%0Adiscussions%20about%20emerging%20cyber%20threats.%20However%2C%20to%20date%2C%20the%20level%20of%0Aaccuracy%20and%20efficiency%20of%20LLMs%20for%20such%20critical%20tasks%20has%20yet%20to%20be%0Athoroughly%20evaluated.%20Hence%2C%20this%20study%20assesses%20the%20accuracy%20of%20an%20LLM%20system%0Abuilt%20on%20the%20OpenAI%20GPT-3.5-turbo%20model%20%5B7%5D%20to%20extract%20CTI%20information.%20To%20do%0Aso%2C%20a%20random%20sample%20of%20500%20daily%20conversations%20from%20three%20cybercrime%20forums%2C%0AXSS%2C%20Exploit_in%2C%20and%20RAMP%2C%20was%20extracted%2C%20and%20the%20LLM%20system%20was%20instructed%20to%0Asummarize%20the%20conversations%20and%20code%2010%20key%20CTI%20variables%2C%20such%20as%20whether%20a%0Alarge%20organization%20and/or%20a%20critical%20infrastructure%20is%20being%20targeted.%20Then%2C%0Atwo%20coders%20reviewed%20each%20conversation%20and%20evaluated%20whether%20the%20information%0Aextracted%20by%20the%20LLM%20was%20accurate.%20The%20LLM%20system%20performed%20strikingly%20well%2C%0Awith%20an%20average%20accuracy%20score%20of%2098%25.%20Various%20ways%20to%20enhance%20the%20model%20were%0Auncovered%2C%20such%20as%20the%20need%20to%20help%20the%20LLM%20distinguish%20between%20stories%20and%0Apast%20events%2C%20as%20well%20as%20being%20careful%20with%20verb%20tenses%20in%20prompts.%0ANevertheless%2C%20the%20results%20of%20this%20study%20highlight%20the%20efficiency%20and%20relevance%0Aof%20using%20LLMs%20for%20cyber%20threat%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03354v2&entry.124074799=Read"},
{"title": "SegXAL: Explainable Active Learning for Semantic Segmentation in Driving\n  Scene Scenarios", "author": "Sriram Mandalika and Athira Nambiar", "abstract": "  Most of the sophisticated AI models utilize huge amounts of annotated data\nand heavy training to achieve high-end performance. However, there are certain\nchallenges that hinder the deployment of AI models \"in-the-wild\" scenarios,\ni.e., inefficient use of unlabeled data, lack of incorporation of human\nexpertise, and lack of interpretation of the results. To mitigate these\nchallenges, we propose a novel Explainable Active Learning (XAL) model,\nXAL-based semantic segmentation model \"SegXAL\", that can (i) effectively\nutilize the unlabeled data, (ii) facilitate the \"Human-in-the-loop\" paradigm,\nand (iii) augment the model decisions in an interpretable way. In particular,\nwe investigate the application of the SegXAL model for semantic segmentation in\ndriving scene scenarios. The SegXAL model proposes the image regions that\nrequire labeling assistance from Oracle by dint of explainable AI (XAI) and\nuncertainty measures in a weakly-supervised manner. Specifically, we propose a\nnovel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty\n(EBU) module to get an Explainable Error Mask, which enables the machine\nteachers/human experts to provide intuitive reasoning behind the results and to\nsolicit feedback to the AI system via an active learning strategy. Such a\nmechanism bridges the semantic gap between man and machine through\ncollaborative intelligence, where humans and AI actively enhance each other's\ncomplementary strengths. A novel high-confidence sample selection technique\nbased on the DICE similarity coefficient is also presented within the SegXAL\nframework. Extensive quantitative and qualitative analyses are carried out in\nthe benchmarking Cityscape dataset. Results show the outperformance of our\nproposed SegXAL against other state-of-the-art models.\n", "link": "http://arxiv.org/abs/2408.04482v1", "date": "2024-08-08", "relevancy": 1.746, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5999}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5777}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegXAL%3A%20Explainable%20Active%20Learning%20for%20Semantic%20Segmentation%20in%20Driving%0A%20%20Scene%20Scenarios&body=Title%3A%20SegXAL%3A%20Explainable%20Active%20Learning%20for%20Semantic%20Segmentation%20in%20Driving%0A%20%20Scene%20Scenarios%0AAuthor%3A%20Sriram%20Mandalika%20and%20Athira%20Nambiar%0AAbstract%3A%20%20%20Most%20of%20the%20sophisticated%20AI%20models%20utilize%20huge%20amounts%20of%20annotated%20data%0Aand%20heavy%20training%20to%20achieve%20high-end%20performance.%20However%2C%20there%20are%20certain%0Achallenges%20that%20hinder%20the%20deployment%20of%20AI%20models%20%22in-the-wild%22%20scenarios%2C%0Ai.e.%2C%20inefficient%20use%20of%20unlabeled%20data%2C%20lack%20of%20incorporation%20of%20human%0Aexpertise%2C%20and%20lack%20of%20interpretation%20of%20the%20results.%20To%20mitigate%20these%0Achallenges%2C%20we%20propose%20a%20novel%20Explainable%20Active%20Learning%20%28XAL%29%20model%2C%0AXAL-based%20semantic%20segmentation%20model%20%22SegXAL%22%2C%20that%20can%20%28i%29%20effectively%0Autilize%20the%20unlabeled%20data%2C%20%28ii%29%20facilitate%20the%20%22Human-in-the-loop%22%20paradigm%2C%0Aand%20%28iii%29%20augment%20the%20model%20decisions%20in%20an%20interpretable%20way.%20In%20particular%2C%0Awe%20investigate%20the%20application%20of%20the%20SegXAL%20model%20for%20semantic%20segmentation%20in%0Adriving%20scene%20scenarios.%20The%20SegXAL%20model%20proposes%20the%20image%20regions%20that%0Arequire%20labeling%20assistance%20from%20Oracle%20by%20dint%20of%20explainable%20AI%20%28XAI%29%20and%0Auncertainty%20measures%20in%20a%20weakly-supervised%20manner.%20Specifically%2C%20we%20propose%20a%0Anovel%20Proximity-aware%20Explainable-AI%20%28PAE%29%20module%20and%20Entropy-based%20Uncertainty%0A%28EBU%29%20module%20to%20get%20an%20Explainable%20Error%20Mask%2C%20which%20enables%20the%20machine%0Ateachers/human%20experts%20to%20provide%20intuitive%20reasoning%20behind%20the%20results%20and%20to%0Asolicit%20feedback%20to%20the%20AI%20system%20via%20an%20active%20learning%20strategy.%20Such%20a%0Amechanism%20bridges%20the%20semantic%20gap%20between%20man%20and%20machine%20through%0Acollaborative%20intelligence%2C%20where%20humans%20and%20AI%20actively%20enhance%20each%20other%27s%0Acomplementary%20strengths.%20A%20novel%20high-confidence%20sample%20selection%20technique%0Abased%20on%20the%20DICE%20similarity%20coefficient%20is%20also%20presented%20within%20the%20SegXAL%0Aframework.%20Extensive%20quantitative%20and%20qualitative%20analyses%20are%20carried%20out%20in%0Athe%20benchmarking%20Cityscape%20dataset.%20Results%20show%20the%20outperformance%20of%20our%0Aproposed%20SegXAL%20against%20other%20state-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegXAL%253A%2520Explainable%2520Active%2520Learning%2520for%2520Semantic%2520Segmentation%2520in%2520Driving%250A%2520%2520Scene%2520Scenarios%26entry.906535625%3DSriram%2520Mandalika%2520and%2520Athira%2520Nambiar%26entry.1292438233%3D%2520%2520Most%2520of%2520the%2520sophisticated%2520AI%2520models%2520utilize%2520huge%2520amounts%2520of%2520annotated%2520data%250Aand%2520heavy%2520training%2520to%2520achieve%2520high-end%2520performance.%2520However%252C%2520there%2520are%2520certain%250Achallenges%2520that%2520hinder%2520the%2520deployment%2520of%2520AI%2520models%2520%2522in-the-wild%2522%2520scenarios%252C%250Ai.e.%252C%2520inefficient%2520use%2520of%2520unlabeled%2520data%252C%2520lack%2520of%2520incorporation%2520of%2520human%250Aexpertise%252C%2520and%2520lack%2520of%2520interpretation%2520of%2520the%2520results.%2520To%2520mitigate%2520these%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520Explainable%2520Active%2520Learning%2520%2528XAL%2529%2520model%252C%250AXAL-based%2520semantic%2520segmentation%2520model%2520%2522SegXAL%2522%252C%2520that%2520can%2520%2528i%2529%2520effectively%250Autilize%2520the%2520unlabeled%2520data%252C%2520%2528ii%2529%2520facilitate%2520the%2520%2522Human-in-the-loop%2522%2520paradigm%252C%250Aand%2520%2528iii%2529%2520augment%2520the%2520model%2520decisions%2520in%2520an%2520interpretable%2520way.%2520In%2520particular%252C%250Awe%2520investigate%2520the%2520application%2520of%2520the%2520SegXAL%2520model%2520for%2520semantic%2520segmentation%2520in%250Adriving%2520scene%2520scenarios.%2520The%2520SegXAL%2520model%2520proposes%2520the%2520image%2520regions%2520that%250Arequire%2520labeling%2520assistance%2520from%2520Oracle%2520by%2520dint%2520of%2520explainable%2520AI%2520%2528XAI%2529%2520and%250Auncertainty%2520measures%2520in%2520a%2520weakly-supervised%2520manner.%2520Specifically%252C%2520we%2520propose%2520a%250Anovel%2520Proximity-aware%2520Explainable-AI%2520%2528PAE%2529%2520module%2520and%2520Entropy-based%2520Uncertainty%250A%2528EBU%2529%2520module%2520to%2520get%2520an%2520Explainable%2520Error%2520Mask%252C%2520which%2520enables%2520the%2520machine%250Ateachers/human%2520experts%2520to%2520provide%2520intuitive%2520reasoning%2520behind%2520the%2520results%2520and%2520to%250Asolicit%2520feedback%2520to%2520the%2520AI%2520system%2520via%2520an%2520active%2520learning%2520strategy.%2520Such%2520a%250Amechanism%2520bridges%2520the%2520semantic%2520gap%2520between%2520man%2520and%2520machine%2520through%250Acollaborative%2520intelligence%252C%2520where%2520humans%2520and%2520AI%2520actively%2520enhance%2520each%2520other%2527s%250Acomplementary%2520strengths.%2520A%2520novel%2520high-confidence%2520sample%2520selection%2520technique%250Abased%2520on%2520the%2520DICE%2520similarity%2520coefficient%2520is%2520also%2520presented%2520within%2520the%2520SegXAL%250Aframework.%2520Extensive%2520quantitative%2520and%2520qualitative%2520analyses%2520are%2520carried%2520out%2520in%250Athe%2520benchmarking%2520Cityscape%2520dataset.%2520Results%2520show%2520the%2520outperformance%2520of%2520our%250Aproposed%2520SegXAL%2520against%2520other%2520state-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegXAL%3A%20Explainable%20Active%20Learning%20for%20Semantic%20Segmentation%20in%20Driving%0A%20%20Scene%20Scenarios&entry.906535625=Sriram%20Mandalika%20and%20Athira%20Nambiar&entry.1292438233=%20%20Most%20of%20the%20sophisticated%20AI%20models%20utilize%20huge%20amounts%20of%20annotated%20data%0Aand%20heavy%20training%20to%20achieve%20high-end%20performance.%20However%2C%20there%20are%20certain%0Achallenges%20that%20hinder%20the%20deployment%20of%20AI%20models%20%22in-the-wild%22%20scenarios%2C%0Ai.e.%2C%20inefficient%20use%20of%20unlabeled%20data%2C%20lack%20of%20incorporation%20of%20human%0Aexpertise%2C%20and%20lack%20of%20interpretation%20of%20the%20results.%20To%20mitigate%20these%0Achallenges%2C%20we%20propose%20a%20novel%20Explainable%20Active%20Learning%20%28XAL%29%20model%2C%0AXAL-based%20semantic%20segmentation%20model%20%22SegXAL%22%2C%20that%20can%20%28i%29%20effectively%0Autilize%20the%20unlabeled%20data%2C%20%28ii%29%20facilitate%20the%20%22Human-in-the-loop%22%20paradigm%2C%0Aand%20%28iii%29%20augment%20the%20model%20decisions%20in%20an%20interpretable%20way.%20In%20particular%2C%0Awe%20investigate%20the%20application%20of%20the%20SegXAL%20model%20for%20semantic%20segmentation%20in%0Adriving%20scene%20scenarios.%20The%20SegXAL%20model%20proposes%20the%20image%20regions%20that%0Arequire%20labeling%20assistance%20from%20Oracle%20by%20dint%20of%20explainable%20AI%20%28XAI%29%20and%0Auncertainty%20measures%20in%20a%20weakly-supervised%20manner.%20Specifically%2C%20we%20propose%20a%0Anovel%20Proximity-aware%20Explainable-AI%20%28PAE%29%20module%20and%20Entropy-based%20Uncertainty%0A%28EBU%29%20module%20to%20get%20an%20Explainable%20Error%20Mask%2C%20which%20enables%20the%20machine%0Ateachers/human%20experts%20to%20provide%20intuitive%20reasoning%20behind%20the%20results%20and%20to%0Asolicit%20feedback%20to%20the%20AI%20system%20via%20an%20active%20learning%20strategy.%20Such%20a%0Amechanism%20bridges%20the%20semantic%20gap%20between%20man%20and%20machine%20through%0Acollaborative%20intelligence%2C%20where%20humans%20and%20AI%20actively%20enhance%20each%20other%27s%0Acomplementary%20strengths.%20A%20novel%20high-confidence%20sample%20selection%20technique%0Abased%20on%20the%20DICE%20similarity%20coefficient%20is%20also%20presented%20within%20the%20SegXAL%0Aframework.%20Extensive%20quantitative%20and%20qualitative%20analyses%20are%20carried%20out%20in%0Athe%20benchmarking%20Cityscape%20dataset.%20Results%20show%20the%20outperformance%20of%20our%0Aproposed%20SegXAL%20against%20other%20state-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04482v1&entry.124074799=Read"},
{"title": "What could go wrong? Discovering and describing failure modes in\n  computer vision", "author": "Gabriela Csurka and Tyler L. Hayes and Diane Larlus and Riccardo Volpi", "abstract": "  Deep learning models are effective, yet brittle. Even carefully trained,\ntheir behavior tends to be hard to predict when confronted with\nout-of-distribution samples. In this work, our goal is to propose a simple yet\neffective solution to predict and describe via natural language potential\nfailure modes of computer vision models. Given a pretrained model and a set of\nsamples, our aim is to find sentences that accurately describe the visual\nconditions in which the model underperforms. In order to study this important\ntopic and foster future research on it, we formalize the problem of\nLanguage-Based Error Explainability (LBEE) and propose a set of metrics to\nevaluate and compare different methods for this task. We propose solutions that\noperate in a joint vision-and-language embedding space, and can characterize\nthrough language descriptions model failures caused, e.g., by objects unseen\nduring training or adverse visual conditions. We experiment with different\ntasks, such as classification under the presence of dataset bias and semantic\nsegmentation in unseen environments, and show that the proposed methodology\nisolates nontrivial sentences associated with specific error causes. We hope\nour work will help practitioners better understand the behavior of models,\nincreasing their overall safety and interpretability.\n", "link": "http://arxiv.org/abs/2408.04471v1", "date": "2024-08-08", "relevancy": 1.7118, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5732}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5677}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20could%20go%20wrong%3F%20Discovering%20and%20describing%20failure%20modes%20in%0A%20%20computer%20vision&body=Title%3A%20What%20could%20go%20wrong%3F%20Discovering%20and%20describing%20failure%20modes%20in%0A%20%20computer%20vision%0AAuthor%3A%20Gabriela%20Csurka%20and%20Tyler%20L.%20Hayes%20and%20Diane%20Larlus%20and%20Riccardo%20Volpi%0AAbstract%3A%20%20%20Deep%20learning%20models%20are%20effective%2C%20yet%20brittle.%20Even%20carefully%20trained%2C%0Atheir%20behavior%20tends%20to%20be%20hard%20to%20predict%20when%20confronted%20with%0Aout-of-distribution%20samples.%20In%20this%20work%2C%20our%20goal%20is%20to%20propose%20a%20simple%20yet%0Aeffective%20solution%20to%20predict%20and%20describe%20via%20natural%20language%20potential%0Afailure%20modes%20of%20computer%20vision%20models.%20Given%20a%20pretrained%20model%20and%20a%20set%20of%0Asamples%2C%20our%20aim%20is%20to%20find%20sentences%20that%20accurately%20describe%20the%20visual%0Aconditions%20in%20which%20the%20model%20underperforms.%20In%20order%20to%20study%20this%20important%0Atopic%20and%20foster%20future%20research%20on%20it%2C%20we%20formalize%20the%20problem%20of%0ALanguage-Based%20Error%20Explainability%20%28LBEE%29%20and%20propose%20a%20set%20of%20metrics%20to%0Aevaluate%20and%20compare%20different%20methods%20for%20this%20task.%20We%20propose%20solutions%20that%0Aoperate%20in%20a%20joint%20vision-and-language%20embedding%20space%2C%20and%20can%20characterize%0Athrough%20language%20descriptions%20model%20failures%20caused%2C%20e.g.%2C%20by%20objects%20unseen%0Aduring%20training%20or%20adverse%20visual%20conditions.%20We%20experiment%20with%20different%0Atasks%2C%20such%20as%20classification%20under%20the%20presence%20of%20dataset%20bias%20and%20semantic%0Asegmentation%20in%20unseen%20environments%2C%20and%20show%20that%20the%20proposed%20methodology%0Aisolates%20nontrivial%20sentences%20associated%20with%20specific%20error%20causes.%20We%20hope%0Aour%20work%20will%20help%20practitioners%20better%20understand%20the%20behavior%20of%20models%2C%0Aincreasing%20their%20overall%20safety%20and%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520could%2520go%2520wrong%253F%2520Discovering%2520and%2520describing%2520failure%2520modes%2520in%250A%2520%2520computer%2520vision%26entry.906535625%3DGabriela%2520Csurka%2520and%2520Tyler%2520L.%2520Hayes%2520and%2520Diane%2520Larlus%2520and%2520Riccardo%2520Volpi%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520are%2520effective%252C%2520yet%2520brittle.%2520Even%2520carefully%2520trained%252C%250Atheir%2520behavior%2520tends%2520to%2520be%2520hard%2520to%2520predict%2520when%2520confronted%2520with%250Aout-of-distribution%2520samples.%2520In%2520this%2520work%252C%2520our%2520goal%2520is%2520to%2520propose%2520a%2520simple%2520yet%250Aeffective%2520solution%2520to%2520predict%2520and%2520describe%2520via%2520natural%2520language%2520potential%250Afailure%2520modes%2520of%2520computer%2520vision%2520models.%2520Given%2520a%2520pretrained%2520model%2520and%2520a%2520set%2520of%250Asamples%252C%2520our%2520aim%2520is%2520to%2520find%2520sentences%2520that%2520accurately%2520describe%2520the%2520visual%250Aconditions%2520in%2520which%2520the%2520model%2520underperforms.%2520In%2520order%2520to%2520study%2520this%2520important%250Atopic%2520and%2520foster%2520future%2520research%2520on%2520it%252C%2520we%2520formalize%2520the%2520problem%2520of%250ALanguage-Based%2520Error%2520Explainability%2520%2528LBEE%2529%2520and%2520propose%2520a%2520set%2520of%2520metrics%2520to%250Aevaluate%2520and%2520compare%2520different%2520methods%2520for%2520this%2520task.%2520We%2520propose%2520solutions%2520that%250Aoperate%2520in%2520a%2520joint%2520vision-and-language%2520embedding%2520space%252C%2520and%2520can%2520characterize%250Athrough%2520language%2520descriptions%2520model%2520failures%2520caused%252C%2520e.g.%252C%2520by%2520objects%2520unseen%250Aduring%2520training%2520or%2520adverse%2520visual%2520conditions.%2520We%2520experiment%2520with%2520different%250Atasks%252C%2520such%2520as%2520classification%2520under%2520the%2520presence%2520of%2520dataset%2520bias%2520and%2520semantic%250Asegmentation%2520in%2520unseen%2520environments%252C%2520and%2520show%2520that%2520the%2520proposed%2520methodology%250Aisolates%2520nontrivial%2520sentences%2520associated%2520with%2520specific%2520error%2520causes.%2520We%2520hope%250Aour%2520work%2520will%2520help%2520practitioners%2520better%2520understand%2520the%2520behavior%2520of%2520models%252C%250Aincreasing%2520their%2520overall%2520safety%2520and%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20could%20go%20wrong%3F%20Discovering%20and%20describing%20failure%20modes%20in%0A%20%20computer%20vision&entry.906535625=Gabriela%20Csurka%20and%20Tyler%20L.%20Hayes%20and%20Diane%20Larlus%20and%20Riccardo%20Volpi&entry.1292438233=%20%20Deep%20learning%20models%20are%20effective%2C%20yet%20brittle.%20Even%20carefully%20trained%2C%0Atheir%20behavior%20tends%20to%20be%20hard%20to%20predict%20when%20confronted%20with%0Aout-of-distribution%20samples.%20In%20this%20work%2C%20our%20goal%20is%20to%20propose%20a%20simple%20yet%0Aeffective%20solution%20to%20predict%20and%20describe%20via%20natural%20language%20potential%0Afailure%20modes%20of%20computer%20vision%20models.%20Given%20a%20pretrained%20model%20and%20a%20set%20of%0Asamples%2C%20our%20aim%20is%20to%20find%20sentences%20that%20accurately%20describe%20the%20visual%0Aconditions%20in%20which%20the%20model%20underperforms.%20In%20order%20to%20study%20this%20important%0Atopic%20and%20foster%20future%20research%20on%20it%2C%20we%20formalize%20the%20problem%20of%0ALanguage-Based%20Error%20Explainability%20%28LBEE%29%20and%20propose%20a%20set%20of%20metrics%20to%0Aevaluate%20and%20compare%20different%20methods%20for%20this%20task.%20We%20propose%20solutions%20that%0Aoperate%20in%20a%20joint%20vision-and-language%20embedding%20space%2C%20and%20can%20characterize%0Athrough%20language%20descriptions%20model%20failures%20caused%2C%20e.g.%2C%20by%20objects%20unseen%0Aduring%20training%20or%20adverse%20visual%20conditions.%20We%20experiment%20with%20different%0Atasks%2C%20such%20as%20classification%20under%20the%20presence%20of%20dataset%20bias%20and%20semantic%0Asegmentation%20in%20unseen%20environments%2C%20and%20show%20that%20the%20proposed%20methodology%0Aisolates%20nontrivial%20sentences%20associated%20with%20specific%20error%20causes.%20We%20hope%0Aour%20work%20will%20help%20practitioners%20better%20understand%20the%20behavior%20of%20models%2C%0Aincreasing%20their%20overall%20safety%20and%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04471v1&entry.124074799=Read"},
{"title": "ESP-MedSAM: Efficient Self-Prompting SAM for Universal\n  Domain-Generalized Image Segmentation", "author": "Qing Xu and Jiaxuan Li and Xiangjian He and Ziyu Liu and Zhen Chen and Wenting Duan and Chenxin Li and Maggie M. He and Fiseha B. Tesema and Wooi P. Cheah and Yi Wang and Rong Qu and Jonathan M. Garibaldi", "abstract": "  The universality of deep neural networks across different modalities and\ntheir generalization capabilities to unseen domains play an essential role in\nmedical image segmentation. The recent Segment Anything Model (SAM) has\ndemonstrated its potential in both settings. However, the huge computational\ncosts, demand for manual annotations as prompts and conflict-prone decoding\nprocess of SAM degrade its generalizability and applicability in clinical\nscenarios. To address these issues, we propose an efficient self-prompting SAM\nfor universal domain-generalized medical image segmentation, named ESP-MedSAM.\nSpecifically, we first devise the Multi-Modal Decoupled Knowledge Distillation\n(MMDKD) strategy to construct a lightweight semi-parameter sharing image\nencoder that produces discriminative visual features for diverse modalities.\nFurther, we introduce the Self-Patch Prompt Generator (SPPG) to automatically\ngenerate high-quality dense prompt embeddings for guiding segmentation\ndecoding. Finally, we design the Query-Decoupled Modality Decoder (QDMD) that\nleverages a one-to-one strategy to provide an independent decoding channel for\nevery modality. Extensive experiments indicate that ESP-MedSAM outperforms\nstate-of-the-arts in diverse medical imaging segmentation tasks, displaying\nsuperior modality universality and generalization capabilities. Especially,\nESP-MedSAM uses only 4.5\\% parameters compared to SAM-H. The source code is\navailable at https://github.com/xq141839/ESP-MedSAM.\n", "link": "http://arxiv.org/abs/2407.14153v3", "date": "2024-08-08", "relevancy": 1.68, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5654}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5617}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ESP-MedSAM%3A%20Efficient%20Self-Prompting%20SAM%20for%20Universal%0A%20%20Domain-Generalized%20Image%20Segmentation&body=Title%3A%20ESP-MedSAM%3A%20Efficient%20Self-Prompting%20SAM%20for%20Universal%0A%20%20Domain-Generalized%20Image%20Segmentation%0AAuthor%3A%20Qing%20Xu%20and%20Jiaxuan%20Li%20and%20Xiangjian%20He%20and%20Ziyu%20Liu%20and%20Zhen%20Chen%20and%20Wenting%20Duan%20and%20Chenxin%20Li%20and%20Maggie%20M.%20He%20and%20Fiseha%20B.%20Tesema%20and%20Wooi%20P.%20Cheah%20and%20Yi%20Wang%20and%20Rong%20Qu%20and%20Jonathan%20M.%20Garibaldi%0AAbstract%3A%20%20%20The%20universality%20of%20deep%20neural%20networks%20across%20different%20modalities%20and%0Atheir%20generalization%20capabilities%20to%20unseen%20domains%20play%20an%20essential%20role%20in%0Amedical%20image%20segmentation.%20The%20recent%20Segment%20Anything%20Model%20%28SAM%29%20has%0Ademonstrated%20its%20potential%20in%20both%20settings.%20However%2C%20the%20huge%20computational%0Acosts%2C%20demand%20for%20manual%20annotations%20as%20prompts%20and%20conflict-prone%20decoding%0Aprocess%20of%20SAM%20degrade%20its%20generalizability%20and%20applicability%20in%20clinical%0Ascenarios.%20To%20address%20these%20issues%2C%20we%20propose%20an%20efficient%20self-prompting%20SAM%0Afor%20universal%20domain-generalized%20medical%20image%20segmentation%2C%20named%20ESP-MedSAM.%0ASpecifically%2C%20we%20first%20devise%20the%20Multi-Modal%20Decoupled%20Knowledge%20Distillation%0A%28MMDKD%29%20strategy%20to%20construct%20a%20lightweight%20semi-parameter%20sharing%20image%0Aencoder%20that%20produces%20discriminative%20visual%20features%20for%20diverse%20modalities.%0AFurther%2C%20we%20introduce%20the%20Self-Patch%20Prompt%20Generator%20%28SPPG%29%20to%20automatically%0Agenerate%20high-quality%20dense%20prompt%20embeddings%20for%20guiding%20segmentation%0Adecoding.%20Finally%2C%20we%20design%20the%20Query-Decoupled%20Modality%20Decoder%20%28QDMD%29%20that%0Aleverages%20a%20one-to-one%20strategy%20to%20provide%20an%20independent%20decoding%20channel%20for%0Aevery%20modality.%20Extensive%20experiments%20indicate%20that%20ESP-MedSAM%20outperforms%0Astate-of-the-arts%20in%20diverse%20medical%20imaging%20segmentation%20tasks%2C%20displaying%0Asuperior%20modality%20universality%20and%20generalization%20capabilities.%20Especially%2C%0AESP-MedSAM%20uses%20only%204.5%5C%25%20parameters%20compared%20to%20SAM-H.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/xq141839/ESP-MedSAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14153v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DESP-MedSAM%253A%2520Efficient%2520Self-Prompting%2520SAM%2520for%2520Universal%250A%2520%2520Domain-Generalized%2520Image%2520Segmentation%26entry.906535625%3DQing%2520Xu%2520and%2520Jiaxuan%2520Li%2520and%2520Xiangjian%2520He%2520and%2520Ziyu%2520Liu%2520and%2520Zhen%2520Chen%2520and%2520Wenting%2520Duan%2520and%2520Chenxin%2520Li%2520and%2520Maggie%2520M.%2520He%2520and%2520Fiseha%2520B.%2520Tesema%2520and%2520Wooi%2520P.%2520Cheah%2520and%2520Yi%2520Wang%2520and%2520Rong%2520Qu%2520and%2520Jonathan%2520M.%2520Garibaldi%26entry.1292438233%3D%2520%2520The%2520universality%2520of%2520deep%2520neural%2520networks%2520across%2520different%2520modalities%2520and%250Atheir%2520generalization%2520capabilities%2520to%2520unseen%2520domains%2520play%2520an%2520essential%2520role%2520in%250Amedical%2520image%2520segmentation.%2520The%2520recent%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520has%250Ademonstrated%2520its%2520potential%2520in%2520both%2520settings.%2520However%252C%2520the%2520huge%2520computational%250Acosts%252C%2520demand%2520for%2520manual%2520annotations%2520as%2520prompts%2520and%2520conflict-prone%2520decoding%250Aprocess%2520of%2520SAM%2520degrade%2520its%2520generalizability%2520and%2520applicability%2520in%2520clinical%250Ascenarios.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520an%2520efficient%2520self-prompting%2520SAM%250Afor%2520universal%2520domain-generalized%2520medical%2520image%2520segmentation%252C%2520named%2520ESP-MedSAM.%250ASpecifically%252C%2520we%2520first%2520devise%2520the%2520Multi-Modal%2520Decoupled%2520Knowledge%2520Distillation%250A%2528MMDKD%2529%2520strategy%2520to%2520construct%2520a%2520lightweight%2520semi-parameter%2520sharing%2520image%250Aencoder%2520that%2520produces%2520discriminative%2520visual%2520features%2520for%2520diverse%2520modalities.%250AFurther%252C%2520we%2520introduce%2520the%2520Self-Patch%2520Prompt%2520Generator%2520%2528SPPG%2529%2520to%2520automatically%250Agenerate%2520high-quality%2520dense%2520prompt%2520embeddings%2520for%2520guiding%2520segmentation%250Adecoding.%2520Finally%252C%2520we%2520design%2520the%2520Query-Decoupled%2520Modality%2520Decoder%2520%2528QDMD%2529%2520that%250Aleverages%2520a%2520one-to-one%2520strategy%2520to%2520provide%2520an%2520independent%2520decoding%2520channel%2520for%250Aevery%2520modality.%2520Extensive%2520experiments%2520indicate%2520that%2520ESP-MedSAM%2520outperforms%250Astate-of-the-arts%2520in%2520diverse%2520medical%2520imaging%2520segmentation%2520tasks%252C%2520displaying%250Asuperior%2520modality%2520universality%2520and%2520generalization%2520capabilities.%2520Especially%252C%250AESP-MedSAM%2520uses%2520only%25204.5%255C%2525%2520parameters%2520compared%2520to%2520SAM-H.%2520The%2520source%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/xq141839/ESP-MedSAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14153v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ESP-MedSAM%3A%20Efficient%20Self-Prompting%20SAM%20for%20Universal%0A%20%20Domain-Generalized%20Image%20Segmentation&entry.906535625=Qing%20Xu%20and%20Jiaxuan%20Li%20and%20Xiangjian%20He%20and%20Ziyu%20Liu%20and%20Zhen%20Chen%20and%20Wenting%20Duan%20and%20Chenxin%20Li%20and%20Maggie%20M.%20He%20and%20Fiseha%20B.%20Tesema%20and%20Wooi%20P.%20Cheah%20and%20Yi%20Wang%20and%20Rong%20Qu%20and%20Jonathan%20M.%20Garibaldi&entry.1292438233=%20%20The%20universality%20of%20deep%20neural%20networks%20across%20different%20modalities%20and%0Atheir%20generalization%20capabilities%20to%20unseen%20domains%20play%20an%20essential%20role%20in%0Amedical%20image%20segmentation.%20The%20recent%20Segment%20Anything%20Model%20%28SAM%29%20has%0Ademonstrated%20its%20potential%20in%20both%20settings.%20However%2C%20the%20huge%20computational%0Acosts%2C%20demand%20for%20manual%20annotations%20as%20prompts%20and%20conflict-prone%20decoding%0Aprocess%20of%20SAM%20degrade%20its%20generalizability%20and%20applicability%20in%20clinical%0Ascenarios.%20To%20address%20these%20issues%2C%20we%20propose%20an%20efficient%20self-prompting%20SAM%0Afor%20universal%20domain-generalized%20medical%20image%20segmentation%2C%20named%20ESP-MedSAM.%0ASpecifically%2C%20we%20first%20devise%20the%20Multi-Modal%20Decoupled%20Knowledge%20Distillation%0A%28MMDKD%29%20strategy%20to%20construct%20a%20lightweight%20semi-parameter%20sharing%20image%0Aencoder%20that%20produces%20discriminative%20visual%20features%20for%20diverse%20modalities.%0AFurther%2C%20we%20introduce%20the%20Self-Patch%20Prompt%20Generator%20%28SPPG%29%20to%20automatically%0Agenerate%20high-quality%20dense%20prompt%20embeddings%20for%20guiding%20segmentation%0Adecoding.%20Finally%2C%20we%20design%20the%20Query-Decoupled%20Modality%20Decoder%20%28QDMD%29%20that%0Aleverages%20a%20one-to-one%20strategy%20to%20provide%20an%20independent%20decoding%20channel%20for%0Aevery%20modality.%20Extensive%20experiments%20indicate%20that%20ESP-MedSAM%20outperforms%0Astate-of-the-arts%20in%20diverse%20medical%20imaging%20segmentation%20tasks%2C%20displaying%0Asuperior%20modality%20universality%20and%20generalization%20capabilities.%20Especially%2C%0AESP-MedSAM%20uses%20only%204.5%5C%25%20parameters%20compared%20to%20SAM-H.%20The%20source%20code%20is%0Aavailable%20at%20https%3A//github.com/xq141839/ESP-MedSAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14153v3&entry.124074799=Read"},
{"title": "A Diffusion Model Framework for Unsupervised Neural Combinatorial\n  Optimization", "author": "Sebastian Sanokowski and Sepp Hochreiter and Sebastian Lehner", "abstract": "  Learning to sample from intractable distributions over discrete sets without\nrelying on corresponding training data is a central problem in a wide range of\nfields, including Combinatorial Optimization. Currently, popular deep\nlearning-based approaches rely primarily on generative models that yield exact\nsample likelihoods. This work introduces a method that lifts this restriction\nand opens the possibility to employ highly expressive latent variable models\nlike diffusion models. Our approach is conceptually based on a loss that upper\nbounds the reverse Kullback-Leibler divergence and evades the requirement of\nexact sample likelihoods. We experimentally validate our approach in data-free\nCombinatorial Optimization and demonstrate that our method achieves a new\nstate-of-the-art on a wide range of benchmark problems.\n", "link": "http://arxiv.org/abs/2406.01661v2", "date": "2024-08-08", "relevancy": 1.677, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5949}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5537}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Diffusion%20Model%20Framework%20for%20Unsupervised%20Neural%20Combinatorial%0A%20%20Optimization&body=Title%3A%20A%20Diffusion%20Model%20Framework%20for%20Unsupervised%20Neural%20Combinatorial%0A%20%20Optimization%0AAuthor%3A%20Sebastian%20Sanokowski%20and%20Sepp%20Hochreiter%20and%20Sebastian%20Lehner%0AAbstract%3A%20%20%20Learning%20to%20sample%20from%20intractable%20distributions%20over%20discrete%20sets%20without%0Arelying%20on%20corresponding%20training%20data%20is%20a%20central%20problem%20in%20a%20wide%20range%20of%0Afields%2C%20including%20Combinatorial%20Optimization.%20Currently%2C%20popular%20deep%0Alearning-based%20approaches%20rely%20primarily%20on%20generative%20models%20that%20yield%20exact%0Asample%20likelihoods.%20This%20work%20introduces%20a%20method%20that%20lifts%20this%20restriction%0Aand%20opens%20the%20possibility%20to%20employ%20highly%20expressive%20latent%20variable%20models%0Alike%20diffusion%20models.%20Our%20approach%20is%20conceptually%20based%20on%20a%20loss%20that%20upper%0Abounds%20the%20reverse%20Kullback-Leibler%20divergence%20and%20evades%20the%20requirement%20of%0Aexact%20sample%20likelihoods.%20We%20experimentally%20validate%20our%20approach%20in%20data-free%0ACombinatorial%20Optimization%20and%20demonstrate%20that%20our%20method%20achieves%20a%20new%0Astate-of-the-art%20on%20a%20wide%20range%20of%20benchmark%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01661v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Diffusion%2520Model%2520Framework%2520for%2520Unsupervised%2520Neural%2520Combinatorial%250A%2520%2520Optimization%26entry.906535625%3DSebastian%2520Sanokowski%2520and%2520Sepp%2520Hochreiter%2520and%2520Sebastian%2520Lehner%26entry.1292438233%3D%2520%2520Learning%2520to%2520sample%2520from%2520intractable%2520distributions%2520over%2520discrete%2520sets%2520without%250Arelying%2520on%2520corresponding%2520training%2520data%2520is%2520a%2520central%2520problem%2520in%2520a%2520wide%2520range%2520of%250Afields%252C%2520including%2520Combinatorial%2520Optimization.%2520Currently%252C%2520popular%2520deep%250Alearning-based%2520approaches%2520rely%2520primarily%2520on%2520generative%2520models%2520that%2520yield%2520exact%250Asample%2520likelihoods.%2520This%2520work%2520introduces%2520a%2520method%2520that%2520lifts%2520this%2520restriction%250Aand%2520opens%2520the%2520possibility%2520to%2520employ%2520highly%2520expressive%2520latent%2520variable%2520models%250Alike%2520diffusion%2520models.%2520Our%2520approach%2520is%2520conceptually%2520based%2520on%2520a%2520loss%2520that%2520upper%250Abounds%2520the%2520reverse%2520Kullback-Leibler%2520divergence%2520and%2520evades%2520the%2520requirement%2520of%250Aexact%2520sample%2520likelihoods.%2520We%2520experimentally%2520validate%2520our%2520approach%2520in%2520data-free%250ACombinatorial%2520Optimization%2520and%2520demonstrate%2520that%2520our%2520method%2520achieves%2520a%2520new%250Astate-of-the-art%2520on%2520a%2520wide%2520range%2520of%2520benchmark%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01661v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Diffusion%20Model%20Framework%20for%20Unsupervised%20Neural%20Combinatorial%0A%20%20Optimization&entry.906535625=Sebastian%20Sanokowski%20and%20Sepp%20Hochreiter%20and%20Sebastian%20Lehner&entry.1292438233=%20%20Learning%20to%20sample%20from%20intractable%20distributions%20over%20discrete%20sets%20without%0Arelying%20on%20corresponding%20training%20data%20is%20a%20central%20problem%20in%20a%20wide%20range%20of%0Afields%2C%20including%20Combinatorial%20Optimization.%20Currently%2C%20popular%20deep%0Alearning-based%20approaches%20rely%20primarily%20on%20generative%20models%20that%20yield%20exact%0Asample%20likelihoods.%20This%20work%20introduces%20a%20method%20that%20lifts%20this%20restriction%0Aand%20opens%20the%20possibility%20to%20employ%20highly%20expressive%20latent%20variable%20models%0Alike%20diffusion%20models.%20Our%20approach%20is%20conceptually%20based%20on%20a%20loss%20that%20upper%0Abounds%20the%20reverse%20Kullback-Leibler%20divergence%20and%20evades%20the%20requirement%20of%0Aexact%20sample%20likelihoods.%20We%20experimentally%20validate%20our%20approach%20in%20data-free%0ACombinatorial%20Optimization%20and%20demonstrate%20that%20our%20method%20achieves%20a%20new%0Astate-of-the-art%20on%20a%20wide%20range%20of%20benchmark%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01661v2&entry.124074799=Read"},
{"title": "State Representations as Incentives for Reinforcement Learning Agents: A\n  Sim2Real Analysis on Robotic Grasping", "author": "Panagiotis Petropoulakis and Ludwig Gr\u00e4f and Mohammadhossein Malmir and Josip Josifovski and Alois Knoll", "abstract": "  Choosing an appropriate representation of the environment for the underlying\ndecision-making process of the reinforcement learning agent is not always\nstraightforward. The state representation should be inclusive enough to allow\nthe agent to informatively decide on its actions and disentangled enough to\nsimplify policy training and the corresponding sim2real transfer. Given this\noutlook, this work examines the effect of various representations in\nincentivizing the agent to solve a specific robotic task: antipodal and planar\nobject grasping. A continuum of state representations is defined, starting from\nhand-crafted numerical states to encoded image-based representations, with\ndecreasing levels of induced task-specific knowledge. The effects of each\nrepresentation on the ability of the agent to solve the task in simulation and\nthe transferability of the learned policy to the real robot are examined and\ncompared against a model-based approach with complete system knowledge. The\nresults show that reinforcement learning agents using numerical states can\nperform on par with non-learning baselines. Furthermore, we find that agents\nusing image-based representations from pre-trained environment embedding\nvectors perform better than end-to-end trained agents, and hypothesize that\nseparation of representation learning from reinforcement learning can benefit\nsim2real transfer. Finally, we conclude that incentivizing the state\nrepresentation with task-specific knowledge facilitates faster convergence for\nagent training and increases success rates in sim2real robot control.\n", "link": "http://arxiv.org/abs/2309.11984v3", "date": "2024-08-08", "relevancy": 1.6758, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6075}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5807}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20State%20Representations%20as%20Incentives%20for%20Reinforcement%20Learning%20Agents%3A%20A%0A%20%20Sim2Real%20Analysis%20on%20Robotic%20Grasping&body=Title%3A%20State%20Representations%20as%20Incentives%20for%20Reinforcement%20Learning%20Agents%3A%20A%0A%20%20Sim2Real%20Analysis%20on%20Robotic%20Grasping%0AAuthor%3A%20Panagiotis%20Petropoulakis%20and%20Ludwig%20Gr%C3%A4f%20and%20Mohammadhossein%20Malmir%20and%20Josip%20Josifovski%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Choosing%20an%20appropriate%20representation%20of%20the%20environment%20for%20the%20underlying%0Adecision-making%20process%20of%20the%20reinforcement%20learning%20agent%20is%20not%20always%0Astraightforward.%20The%20state%20representation%20should%20be%20inclusive%20enough%20to%20allow%0Athe%20agent%20to%20informatively%20decide%20on%20its%20actions%20and%20disentangled%20enough%20to%0Asimplify%20policy%20training%20and%20the%20corresponding%20sim2real%20transfer.%20Given%20this%0Aoutlook%2C%20this%20work%20examines%20the%20effect%20of%20various%20representations%20in%0Aincentivizing%20the%20agent%20to%20solve%20a%20specific%20robotic%20task%3A%20antipodal%20and%20planar%0Aobject%20grasping.%20A%20continuum%20of%20state%20representations%20is%20defined%2C%20starting%20from%0Ahand-crafted%20numerical%20states%20to%20encoded%20image-based%20representations%2C%20with%0Adecreasing%20levels%20of%20induced%20task-specific%20knowledge.%20The%20effects%20of%20each%0Arepresentation%20on%20the%20ability%20of%20the%20agent%20to%20solve%20the%20task%20in%20simulation%20and%0Athe%20transferability%20of%20the%20learned%20policy%20to%20the%20real%20robot%20are%20examined%20and%0Acompared%20against%20a%20model-based%20approach%20with%20complete%20system%20knowledge.%20The%0Aresults%20show%20that%20reinforcement%20learning%20agents%20using%20numerical%20states%20can%0Aperform%20on%20par%20with%20non-learning%20baselines.%20Furthermore%2C%20we%20find%20that%20agents%0Ausing%20image-based%20representations%20from%20pre-trained%20environment%20embedding%0Avectors%20perform%20better%20than%20end-to-end%20trained%20agents%2C%20and%20hypothesize%20that%0Aseparation%20of%20representation%20learning%20from%20reinforcement%20learning%20can%20benefit%0Asim2real%20transfer.%20Finally%2C%20we%20conclude%20that%20incentivizing%20the%20state%0Arepresentation%20with%20task-specific%20knowledge%20facilitates%20faster%20convergence%20for%0Aagent%20training%20and%20increases%20success%20rates%20in%20sim2real%20robot%20control.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11984v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DState%2520Representations%2520as%2520Incentives%2520for%2520Reinforcement%2520Learning%2520Agents%253A%2520A%250A%2520%2520Sim2Real%2520Analysis%2520on%2520Robotic%2520Grasping%26entry.906535625%3DPanagiotis%2520Petropoulakis%2520and%2520Ludwig%2520Gr%25C3%25A4f%2520and%2520Mohammadhossein%2520Malmir%2520and%2520Josip%2520Josifovski%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Choosing%2520an%2520appropriate%2520representation%2520of%2520the%2520environment%2520for%2520the%2520underlying%250Adecision-making%2520process%2520of%2520the%2520reinforcement%2520learning%2520agent%2520is%2520not%2520always%250Astraightforward.%2520The%2520state%2520representation%2520should%2520be%2520inclusive%2520enough%2520to%2520allow%250Athe%2520agent%2520to%2520informatively%2520decide%2520on%2520its%2520actions%2520and%2520disentangled%2520enough%2520to%250Asimplify%2520policy%2520training%2520and%2520the%2520corresponding%2520sim2real%2520transfer.%2520Given%2520this%250Aoutlook%252C%2520this%2520work%2520examines%2520the%2520effect%2520of%2520various%2520representations%2520in%250Aincentivizing%2520the%2520agent%2520to%2520solve%2520a%2520specific%2520robotic%2520task%253A%2520antipodal%2520and%2520planar%250Aobject%2520grasping.%2520A%2520continuum%2520of%2520state%2520representations%2520is%2520defined%252C%2520starting%2520from%250Ahand-crafted%2520numerical%2520states%2520to%2520encoded%2520image-based%2520representations%252C%2520with%250Adecreasing%2520levels%2520of%2520induced%2520task-specific%2520knowledge.%2520The%2520effects%2520of%2520each%250Arepresentation%2520on%2520the%2520ability%2520of%2520the%2520agent%2520to%2520solve%2520the%2520task%2520in%2520simulation%2520and%250Athe%2520transferability%2520of%2520the%2520learned%2520policy%2520to%2520the%2520real%2520robot%2520are%2520examined%2520and%250Acompared%2520against%2520a%2520model-based%2520approach%2520with%2520complete%2520system%2520knowledge.%2520The%250Aresults%2520show%2520that%2520reinforcement%2520learning%2520agents%2520using%2520numerical%2520states%2520can%250Aperform%2520on%2520par%2520with%2520non-learning%2520baselines.%2520Furthermore%252C%2520we%2520find%2520that%2520agents%250Ausing%2520image-based%2520representations%2520from%2520pre-trained%2520environment%2520embedding%250Avectors%2520perform%2520better%2520than%2520end-to-end%2520trained%2520agents%252C%2520and%2520hypothesize%2520that%250Aseparation%2520of%2520representation%2520learning%2520from%2520reinforcement%2520learning%2520can%2520benefit%250Asim2real%2520transfer.%2520Finally%252C%2520we%2520conclude%2520that%2520incentivizing%2520the%2520state%250Arepresentation%2520with%2520task-specific%2520knowledge%2520facilitates%2520faster%2520convergence%2520for%250Aagent%2520training%2520and%2520increases%2520success%2520rates%2520in%2520sim2real%2520robot%2520control.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11984v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=State%20Representations%20as%20Incentives%20for%20Reinforcement%20Learning%20Agents%3A%20A%0A%20%20Sim2Real%20Analysis%20on%20Robotic%20Grasping&entry.906535625=Panagiotis%20Petropoulakis%20and%20Ludwig%20Gr%C3%A4f%20and%20Mohammadhossein%20Malmir%20and%20Josip%20Josifovski%20and%20Alois%20Knoll&entry.1292438233=%20%20Choosing%20an%20appropriate%20representation%20of%20the%20environment%20for%20the%20underlying%0Adecision-making%20process%20of%20the%20reinforcement%20learning%20agent%20is%20not%20always%0Astraightforward.%20The%20state%20representation%20should%20be%20inclusive%20enough%20to%20allow%0Athe%20agent%20to%20informatively%20decide%20on%20its%20actions%20and%20disentangled%20enough%20to%0Asimplify%20policy%20training%20and%20the%20corresponding%20sim2real%20transfer.%20Given%20this%0Aoutlook%2C%20this%20work%20examines%20the%20effect%20of%20various%20representations%20in%0Aincentivizing%20the%20agent%20to%20solve%20a%20specific%20robotic%20task%3A%20antipodal%20and%20planar%0Aobject%20grasping.%20A%20continuum%20of%20state%20representations%20is%20defined%2C%20starting%20from%0Ahand-crafted%20numerical%20states%20to%20encoded%20image-based%20representations%2C%20with%0Adecreasing%20levels%20of%20induced%20task-specific%20knowledge.%20The%20effects%20of%20each%0Arepresentation%20on%20the%20ability%20of%20the%20agent%20to%20solve%20the%20task%20in%20simulation%20and%0Athe%20transferability%20of%20the%20learned%20policy%20to%20the%20real%20robot%20are%20examined%20and%0Acompared%20against%20a%20model-based%20approach%20with%20complete%20system%20knowledge.%20The%0Aresults%20show%20that%20reinforcement%20learning%20agents%20using%20numerical%20states%20can%0Aperform%20on%20par%20with%20non-learning%20baselines.%20Furthermore%2C%20we%20find%20that%20agents%0Ausing%20image-based%20representations%20from%20pre-trained%20environment%20embedding%0Avectors%20perform%20better%20than%20end-to-end%20trained%20agents%2C%20and%20hypothesize%20that%0Aseparation%20of%20representation%20learning%20from%20reinforcement%20learning%20can%20benefit%0Asim2real%20transfer.%20Finally%2C%20we%20conclude%20that%20incentivizing%20the%20state%0Arepresentation%20with%20task-specific%20knowledge%20facilitates%20faster%20convergence%20for%0Aagent%20training%20and%20increases%20success%20rates%20in%20sim2real%20robot%20control.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11984v3&entry.124074799=Read"},
{"title": "MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with\n  Large Language Models", "author": "Haoxuan Li and Zhengmao Yang and Yunshan Ma and Yi Bin and Yang Yang and Tat-Seng Chua", "abstract": "  We study an emerging and intriguing problem of multimodal temporal event\nforecasting with large language models. Compared to using text or graph\nmodalities, the investigation of utilizing images for temporal event\nforecasting has not been fully explored, especially in the era of large\nlanguage models (LLMs). To bridge this gap, we are particularly interested in\ntwo key questions of: 1) why images will help in temporal event forecasting,\nand 2) how to integrate images into the LLM-based forecasting framework. To\nanswer these research questions, we propose to identify two essential functions\nthat images play in the scenario of temporal event forecasting, i.e.,\nhighlighting and complementary. Then, we develop a novel framework, named\nMM-Forecast. It employs an Image Function Identification module to recognize\nthese functions as verbal descriptions using multimodal large language models\n(MLLMs), and subsequently incorporates these function descriptions into\nLLM-based forecasting models. To evaluate our approach, we construct a new\nmultimodal dataset, MidEast-TE-mm, by extending an existing event dataset\nMidEast-TE-mini with images. Empirical studies demonstrate that our MM-Forecast\ncan correctly identify the image functions, and further more, incorporating\nthese verbal function descriptions significantly improves the forecasting\nperformance. The dataset, code, and prompts are available at\nhttps://github.com/LuminosityX/MM-Forecast.\n", "link": "http://arxiv.org/abs/2408.04388v1", "date": "2024-08-08", "relevancy": 1.6617, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5807}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5522}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MM-Forecast%3A%20A%20Multimodal%20Approach%20to%20Temporal%20Event%20Forecasting%20with%0A%20%20Large%20Language%20Models&body=Title%3A%20MM-Forecast%3A%20A%20Multimodal%20Approach%20to%20Temporal%20Event%20Forecasting%20with%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Haoxuan%20Li%20and%20Zhengmao%20Yang%20and%20Yunshan%20Ma%20and%20Yi%20Bin%20and%20Yang%20Yang%20and%20Tat-Seng%20Chua%0AAbstract%3A%20%20%20We%20study%20an%20emerging%20and%20intriguing%20problem%20of%20multimodal%20temporal%20event%0Aforecasting%20with%20large%20language%20models.%20Compared%20to%20using%20text%20or%20graph%0Amodalities%2C%20the%20investigation%20of%20utilizing%20images%20for%20temporal%20event%0Aforecasting%20has%20not%20been%20fully%20explored%2C%20especially%20in%20the%20era%20of%20large%0Alanguage%20models%20%28LLMs%29.%20To%20bridge%20this%20gap%2C%20we%20are%20particularly%20interested%20in%0Atwo%20key%20questions%20of%3A%201%29%20why%20images%20will%20help%20in%20temporal%20event%20forecasting%2C%0Aand%202%29%20how%20to%20integrate%20images%20into%20the%20LLM-based%20forecasting%20framework.%20To%0Aanswer%20these%20research%20questions%2C%20we%20propose%20to%20identify%20two%20essential%20functions%0Athat%20images%20play%20in%20the%20scenario%20of%20temporal%20event%20forecasting%2C%20i.e.%2C%0Ahighlighting%20and%20complementary.%20Then%2C%20we%20develop%20a%20novel%20framework%2C%20named%0AMM-Forecast.%20It%20employs%20an%20Image%20Function%20Identification%20module%20to%20recognize%0Athese%20functions%20as%20verbal%20descriptions%20using%20multimodal%20large%20language%20models%0A%28MLLMs%29%2C%20and%20subsequently%20incorporates%20these%20function%20descriptions%20into%0ALLM-based%20forecasting%20models.%20To%20evaluate%20our%20approach%2C%20we%20construct%20a%20new%0Amultimodal%20dataset%2C%20MidEast-TE-mm%2C%20by%20extending%20an%20existing%20event%20dataset%0AMidEast-TE-mini%20with%20images.%20Empirical%20studies%20demonstrate%20that%20our%20MM-Forecast%0Acan%20correctly%20identify%20the%20image%20functions%2C%20and%20further%20more%2C%20incorporating%0Athese%20verbal%20function%20descriptions%20significantly%20improves%20the%20forecasting%0Aperformance.%20The%20dataset%2C%20code%2C%20and%20prompts%20are%20available%20at%0Ahttps%3A//github.com/LuminosityX/MM-Forecast.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04388v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMM-Forecast%253A%2520A%2520Multimodal%2520Approach%2520to%2520Temporal%2520Event%2520Forecasting%2520with%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DHaoxuan%2520Li%2520and%2520Zhengmao%2520Yang%2520and%2520Yunshan%2520Ma%2520and%2520Yi%2520Bin%2520and%2520Yang%2520Yang%2520and%2520Tat-Seng%2520Chua%26entry.1292438233%3D%2520%2520We%2520study%2520an%2520emerging%2520and%2520intriguing%2520problem%2520of%2520multimodal%2520temporal%2520event%250Aforecasting%2520with%2520large%2520language%2520models.%2520Compared%2520to%2520using%2520text%2520or%2520graph%250Amodalities%252C%2520the%2520investigation%2520of%2520utilizing%2520images%2520for%2520temporal%2520event%250Aforecasting%2520has%2520not%2520been%2520fully%2520explored%252C%2520especially%2520in%2520the%2520era%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520are%2520particularly%2520interested%2520in%250Atwo%2520key%2520questions%2520of%253A%25201%2529%2520why%2520images%2520will%2520help%2520in%2520temporal%2520event%2520forecasting%252C%250Aand%25202%2529%2520how%2520to%2520integrate%2520images%2520into%2520the%2520LLM-based%2520forecasting%2520framework.%2520To%250Aanswer%2520these%2520research%2520questions%252C%2520we%2520propose%2520to%2520identify%2520two%2520essential%2520functions%250Athat%2520images%2520play%2520in%2520the%2520scenario%2520of%2520temporal%2520event%2520forecasting%252C%2520i.e.%252C%250Ahighlighting%2520and%2520complementary.%2520Then%252C%2520we%2520develop%2520a%2520novel%2520framework%252C%2520named%250AMM-Forecast.%2520It%2520employs%2520an%2520Image%2520Function%2520Identification%2520module%2520to%2520recognize%250Athese%2520functions%2520as%2520verbal%2520descriptions%2520using%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%252C%2520and%2520subsequently%2520incorporates%2520these%2520function%2520descriptions%2520into%250ALLM-based%2520forecasting%2520models.%2520To%2520evaluate%2520our%2520approach%252C%2520we%2520construct%2520a%2520new%250Amultimodal%2520dataset%252C%2520MidEast-TE-mm%252C%2520by%2520extending%2520an%2520existing%2520event%2520dataset%250AMidEast-TE-mini%2520with%2520images.%2520Empirical%2520studies%2520demonstrate%2520that%2520our%2520MM-Forecast%250Acan%2520correctly%2520identify%2520the%2520image%2520functions%252C%2520and%2520further%2520more%252C%2520incorporating%250Athese%2520verbal%2520function%2520descriptions%2520significantly%2520improves%2520the%2520forecasting%250Aperformance.%2520The%2520dataset%252C%2520code%252C%2520and%2520prompts%2520are%2520available%2520at%250Ahttps%253A//github.com/LuminosityX/MM-Forecast.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04388v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MM-Forecast%3A%20A%20Multimodal%20Approach%20to%20Temporal%20Event%20Forecasting%20with%0A%20%20Large%20Language%20Models&entry.906535625=Haoxuan%20Li%20and%20Zhengmao%20Yang%20and%20Yunshan%20Ma%20and%20Yi%20Bin%20and%20Yang%20Yang%20and%20Tat-Seng%20Chua&entry.1292438233=%20%20We%20study%20an%20emerging%20and%20intriguing%20problem%20of%20multimodal%20temporal%20event%0Aforecasting%20with%20large%20language%20models.%20Compared%20to%20using%20text%20or%20graph%0Amodalities%2C%20the%20investigation%20of%20utilizing%20images%20for%20temporal%20event%0Aforecasting%20has%20not%20been%20fully%20explored%2C%20especially%20in%20the%20era%20of%20large%0Alanguage%20models%20%28LLMs%29.%20To%20bridge%20this%20gap%2C%20we%20are%20particularly%20interested%20in%0Atwo%20key%20questions%20of%3A%201%29%20why%20images%20will%20help%20in%20temporal%20event%20forecasting%2C%0Aand%202%29%20how%20to%20integrate%20images%20into%20the%20LLM-based%20forecasting%20framework.%20To%0Aanswer%20these%20research%20questions%2C%20we%20propose%20to%20identify%20two%20essential%20functions%0Athat%20images%20play%20in%20the%20scenario%20of%20temporal%20event%20forecasting%2C%20i.e.%2C%0Ahighlighting%20and%20complementary.%20Then%2C%20we%20develop%20a%20novel%20framework%2C%20named%0AMM-Forecast.%20It%20employs%20an%20Image%20Function%20Identification%20module%20to%20recognize%0Athese%20functions%20as%20verbal%20descriptions%20using%20multimodal%20large%20language%20models%0A%28MLLMs%29%2C%20and%20subsequently%20incorporates%20these%20function%20descriptions%20into%0ALLM-based%20forecasting%20models.%20To%20evaluate%20our%20approach%2C%20we%20construct%20a%20new%0Amultimodal%20dataset%2C%20MidEast-TE-mm%2C%20by%20extending%20an%20existing%20event%20dataset%0AMidEast-TE-mini%20with%20images.%20Empirical%20studies%20demonstrate%20that%20our%20MM-Forecast%0Acan%20correctly%20identify%20the%20image%20functions%2C%20and%20further%20more%2C%20incorporating%0Athese%20verbal%20function%20descriptions%20significantly%20improves%20the%20forecasting%0Aperformance.%20The%20dataset%2C%20code%2C%20and%20prompts%20are%20available%20at%0Ahttps%3A//github.com/LuminosityX/MM-Forecast.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04388v1&entry.124074799=Read"},
{"title": "Visual cognition in multimodal large language models", "author": "Luca M. Schulze Buschoff and Elif Akata and Matthias Bethge and Eric Schulz", "abstract": "  A chief goal of artificial intelligence is to build machines that think like\npeople. Yet it has been argued that deep neural network architectures fail to\naccomplish this. Researchers have asserted these models' limitations in the\ndomains of causal reasoning, intuitive physics, and intuitive psychology. Yet\nrecent advancements, namely the rise of large language models, particularly\nthose designed for visual processing, have rekindled interest in the potential\nto emulate human-like cognitive abilities. This paper evaluates the current\nstate of vision-based large language models in the domains of intuitive\nphysics, causal reasoning, and intuitive psychology. Through a series of\ncontrolled experiments, we investigate the extent to which these modern models\ngrasp complex physical interactions, causal relationships, and intuitive\nunderstanding of others' preferences. Our findings reveal that, while some of\nthese models demonstrate a notable proficiency in processing and interpreting\nvisual data, they still fall short of human capabilities in these areas. Our\nresults emphasize the need for integrating more robust mechanisms for\nunderstanding causality, physical dynamics, and social cognition into\nmodern-day, vision-based language models, and point out the importance of\ncognitively-inspired benchmarks.\n", "link": "http://arxiv.org/abs/2311.16093v3", "date": "2024-08-08", "relevancy": 1.6354, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.583}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5528}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20cognition%20in%20multimodal%20large%20language%20models&body=Title%3A%20Visual%20cognition%20in%20multimodal%20large%20language%20models%0AAuthor%3A%20Luca%20M.%20Schulze%20Buschoff%20and%20Elif%20Akata%20and%20Matthias%20Bethge%20and%20Eric%20Schulz%0AAbstract%3A%20%20%20A%20chief%20goal%20of%20artificial%20intelligence%20is%20to%20build%20machines%20that%20think%20like%0Apeople.%20Yet%20it%20has%20been%20argued%20that%20deep%20neural%20network%20architectures%20fail%20to%0Aaccomplish%20this.%20Researchers%20have%20asserted%20these%20models%27%20limitations%20in%20the%0Adomains%20of%20causal%20reasoning%2C%20intuitive%20physics%2C%20and%20intuitive%20psychology.%20Yet%0Arecent%20advancements%2C%20namely%20the%20rise%20of%20large%20language%20models%2C%20particularly%0Athose%20designed%20for%20visual%20processing%2C%20have%20rekindled%20interest%20in%20the%20potential%0Ato%20emulate%20human-like%20cognitive%20abilities.%20This%20paper%20evaluates%20the%20current%0Astate%20of%20vision-based%20large%20language%20models%20in%20the%20domains%20of%20intuitive%0Aphysics%2C%20causal%20reasoning%2C%20and%20intuitive%20psychology.%20Through%20a%20series%20of%0Acontrolled%20experiments%2C%20we%20investigate%20the%20extent%20to%20which%20these%20modern%20models%0Agrasp%20complex%20physical%20interactions%2C%20causal%20relationships%2C%20and%20intuitive%0Aunderstanding%20of%20others%27%20preferences.%20Our%20findings%20reveal%20that%2C%20while%20some%20of%0Athese%20models%20demonstrate%20a%20notable%20proficiency%20in%20processing%20and%20interpreting%0Avisual%20data%2C%20they%20still%20fall%20short%20of%20human%20capabilities%20in%20these%20areas.%20Our%0Aresults%20emphasize%20the%20need%20for%20integrating%20more%20robust%20mechanisms%20for%0Aunderstanding%20causality%2C%20physical%20dynamics%2C%20and%20social%20cognition%20into%0Amodern-day%2C%20vision-based%20language%20models%2C%20and%20point%20out%20the%20importance%20of%0Acognitively-inspired%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16093v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520cognition%2520in%2520multimodal%2520large%2520language%2520models%26entry.906535625%3DLuca%2520M.%2520Schulze%2520Buschoff%2520and%2520Elif%2520Akata%2520and%2520Matthias%2520Bethge%2520and%2520Eric%2520Schulz%26entry.1292438233%3D%2520%2520A%2520chief%2520goal%2520of%2520artificial%2520intelligence%2520is%2520to%2520build%2520machines%2520that%2520think%2520like%250Apeople.%2520Yet%2520it%2520has%2520been%2520argued%2520that%2520deep%2520neural%2520network%2520architectures%2520fail%2520to%250Aaccomplish%2520this.%2520Researchers%2520have%2520asserted%2520these%2520models%2527%2520limitations%2520in%2520the%250Adomains%2520of%2520causal%2520reasoning%252C%2520intuitive%2520physics%252C%2520and%2520intuitive%2520psychology.%2520Yet%250Arecent%2520advancements%252C%2520namely%2520the%2520rise%2520of%2520large%2520language%2520models%252C%2520particularly%250Athose%2520designed%2520for%2520visual%2520processing%252C%2520have%2520rekindled%2520interest%2520in%2520the%2520potential%250Ato%2520emulate%2520human-like%2520cognitive%2520abilities.%2520This%2520paper%2520evaluates%2520the%2520current%250Astate%2520of%2520vision-based%2520large%2520language%2520models%2520in%2520the%2520domains%2520of%2520intuitive%250Aphysics%252C%2520causal%2520reasoning%252C%2520and%2520intuitive%2520psychology.%2520Through%2520a%2520series%2520of%250Acontrolled%2520experiments%252C%2520we%2520investigate%2520the%2520extent%2520to%2520which%2520these%2520modern%2520models%250Agrasp%2520complex%2520physical%2520interactions%252C%2520causal%2520relationships%252C%2520and%2520intuitive%250Aunderstanding%2520of%2520others%2527%2520preferences.%2520Our%2520findings%2520reveal%2520that%252C%2520while%2520some%2520of%250Athese%2520models%2520demonstrate%2520a%2520notable%2520proficiency%2520in%2520processing%2520and%2520interpreting%250Avisual%2520data%252C%2520they%2520still%2520fall%2520short%2520of%2520human%2520capabilities%2520in%2520these%2520areas.%2520Our%250Aresults%2520emphasize%2520the%2520need%2520for%2520integrating%2520more%2520robust%2520mechanisms%2520for%250Aunderstanding%2520causality%252C%2520physical%2520dynamics%252C%2520and%2520social%2520cognition%2520into%250Amodern-day%252C%2520vision-based%2520language%2520models%252C%2520and%2520point%2520out%2520the%2520importance%2520of%250Acognitively-inspired%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16093v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20cognition%20in%20multimodal%20large%20language%20models&entry.906535625=Luca%20M.%20Schulze%20Buschoff%20and%20Elif%20Akata%20and%20Matthias%20Bethge%20and%20Eric%20Schulz&entry.1292438233=%20%20A%20chief%20goal%20of%20artificial%20intelligence%20is%20to%20build%20machines%20that%20think%20like%0Apeople.%20Yet%20it%20has%20been%20argued%20that%20deep%20neural%20network%20architectures%20fail%20to%0Aaccomplish%20this.%20Researchers%20have%20asserted%20these%20models%27%20limitations%20in%20the%0Adomains%20of%20causal%20reasoning%2C%20intuitive%20physics%2C%20and%20intuitive%20psychology.%20Yet%0Arecent%20advancements%2C%20namely%20the%20rise%20of%20large%20language%20models%2C%20particularly%0Athose%20designed%20for%20visual%20processing%2C%20have%20rekindled%20interest%20in%20the%20potential%0Ato%20emulate%20human-like%20cognitive%20abilities.%20This%20paper%20evaluates%20the%20current%0Astate%20of%20vision-based%20large%20language%20models%20in%20the%20domains%20of%20intuitive%0Aphysics%2C%20causal%20reasoning%2C%20and%20intuitive%20psychology.%20Through%20a%20series%20of%0Acontrolled%20experiments%2C%20we%20investigate%20the%20extent%20to%20which%20these%20modern%20models%0Agrasp%20complex%20physical%20interactions%2C%20causal%20relationships%2C%20and%20intuitive%0Aunderstanding%20of%20others%27%20preferences.%20Our%20findings%20reveal%20that%2C%20while%20some%20of%0Athese%20models%20demonstrate%20a%20notable%20proficiency%20in%20processing%20and%20interpreting%0Avisual%20data%2C%20they%20still%20fall%20short%20of%20human%20capabilities%20in%20these%20areas.%20Our%0Aresults%20emphasize%20the%20need%20for%20integrating%20more%20robust%20mechanisms%20for%0Aunderstanding%20causality%2C%20physical%20dynamics%2C%20and%20social%20cognition%20into%0Amodern-day%2C%20vision-based%20language%20models%2C%20and%20point%20out%20the%20importance%20of%0Acognitively-inspired%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16093v3&entry.124074799=Read"},
{"title": "RiskAwareBench: Towards Evaluating Physical Risk Awareness for\n  High-level Planning of LLM-based Embodied Agents", "author": "Zihao Zhu and Bingzhe Wu and Zhengyou Zhang and Baoyuan Wu", "abstract": "  The integration of large language models (LLMs) into robotics significantly\nenhances the capabilities of embodied agents in understanding and executing\ncomplex natural language instructions. However, the unmitigated deployment of\nLLM-based embodied systems in real-world environments may pose potential\nphysical risks, such as property damage and personal injury. Existing security\nbenchmarks for LLMs overlook risk awareness for LLM-based embodied agents. To\naddress this gap, we propose RiskAwareBench, an automated framework designed to\nassess physical risks awareness in LLM-based embodied agents. RiskAwareBench\nconsists of four modules: safety tips generation, risky scene generation, plan\ngeneration, and evaluation, enabling comprehensive risk assessment with minimal\nmanual intervention. Utilizing this framework, we compile the PhysicalRisk\ndataset, encompassing diverse scenarios with associated safety tips,\nobservations, and instructions. Extensive experiments reveal that most LLMs\nexhibit insufficient physical risk awareness, and baseline risk mitigation\nstrategies yield limited enhancement, which emphasizes the urgency and\ncruciality of improving risk awareness in LLM-based embodied agents in the\nfuture.\n", "link": "http://arxiv.org/abs/2408.04449v1", "date": "2024-08-08", "relevancy": 1.6328, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5989}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RiskAwareBench%3A%20Towards%20Evaluating%20Physical%20Risk%20Awareness%20for%0A%20%20High-level%20Planning%20of%20LLM-based%20Embodied%20Agents&body=Title%3A%20RiskAwareBench%3A%20Towards%20Evaluating%20Physical%20Risk%20Awareness%20for%0A%20%20High-level%20Planning%20of%20LLM-based%20Embodied%20Agents%0AAuthor%3A%20Zihao%20Zhu%20and%20Bingzhe%20Wu%20and%20Zhengyou%20Zhang%20and%20Baoyuan%20Wu%0AAbstract%3A%20%20%20The%20integration%20of%20large%20language%20models%20%28LLMs%29%20into%20robotics%20significantly%0Aenhances%20the%20capabilities%20of%20embodied%20agents%20in%20understanding%20and%20executing%0Acomplex%20natural%20language%20instructions.%20However%2C%20the%20unmitigated%20deployment%20of%0ALLM-based%20embodied%20systems%20in%20real-world%20environments%20may%20pose%20potential%0Aphysical%20risks%2C%20such%20as%20property%20damage%20and%20personal%20injury.%20Existing%20security%0Abenchmarks%20for%20LLMs%20overlook%20risk%20awareness%20for%20LLM-based%20embodied%20agents.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20RiskAwareBench%2C%20an%20automated%20framework%20designed%20to%0Aassess%20physical%20risks%20awareness%20in%20LLM-based%20embodied%20agents.%20RiskAwareBench%0Aconsists%20of%20four%20modules%3A%20safety%20tips%20generation%2C%20risky%20scene%20generation%2C%20plan%0Ageneration%2C%20and%20evaluation%2C%20enabling%20comprehensive%20risk%20assessment%20with%20minimal%0Amanual%20intervention.%20Utilizing%20this%20framework%2C%20we%20compile%20the%20PhysicalRisk%0Adataset%2C%20encompassing%20diverse%20scenarios%20with%20associated%20safety%20tips%2C%0Aobservations%2C%20and%20instructions.%20Extensive%20experiments%20reveal%20that%20most%20LLMs%0Aexhibit%20insufficient%20physical%20risk%20awareness%2C%20and%20baseline%20risk%20mitigation%0Astrategies%20yield%20limited%20enhancement%2C%20which%20emphasizes%20the%20urgency%20and%0Acruciality%20of%20improving%20risk%20awareness%20in%20LLM-based%20embodied%20agents%20in%20the%0Afuture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiskAwareBench%253A%2520Towards%2520Evaluating%2520Physical%2520Risk%2520Awareness%2520for%250A%2520%2520High-level%2520Planning%2520of%2520LLM-based%2520Embodied%2520Agents%26entry.906535625%3DZihao%2520Zhu%2520and%2520Bingzhe%2520Wu%2520and%2520Zhengyou%2520Zhang%2520and%2520Baoyuan%2520Wu%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%2520robotics%2520significantly%250Aenhances%2520the%2520capabilities%2520of%2520embodied%2520agents%2520in%2520understanding%2520and%2520executing%250Acomplex%2520natural%2520language%2520instructions.%2520However%252C%2520the%2520unmitigated%2520deployment%2520of%250ALLM-based%2520embodied%2520systems%2520in%2520real-world%2520environments%2520may%2520pose%2520potential%250Aphysical%2520risks%252C%2520such%2520as%2520property%2520damage%2520and%2520personal%2520injury.%2520Existing%2520security%250Abenchmarks%2520for%2520LLMs%2520overlook%2520risk%2520awareness%2520for%2520LLM-based%2520embodied%2520agents.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520propose%2520RiskAwareBench%252C%2520an%2520automated%2520framework%2520designed%2520to%250Aassess%2520physical%2520risks%2520awareness%2520in%2520LLM-based%2520embodied%2520agents.%2520RiskAwareBench%250Aconsists%2520of%2520four%2520modules%253A%2520safety%2520tips%2520generation%252C%2520risky%2520scene%2520generation%252C%2520plan%250Ageneration%252C%2520and%2520evaluation%252C%2520enabling%2520comprehensive%2520risk%2520assessment%2520with%2520minimal%250Amanual%2520intervention.%2520Utilizing%2520this%2520framework%252C%2520we%2520compile%2520the%2520PhysicalRisk%250Adataset%252C%2520encompassing%2520diverse%2520scenarios%2520with%2520associated%2520safety%2520tips%252C%250Aobservations%252C%2520and%2520instructions.%2520Extensive%2520experiments%2520reveal%2520that%2520most%2520LLMs%250Aexhibit%2520insufficient%2520physical%2520risk%2520awareness%252C%2520and%2520baseline%2520risk%2520mitigation%250Astrategies%2520yield%2520limited%2520enhancement%252C%2520which%2520emphasizes%2520the%2520urgency%2520and%250Acruciality%2520of%2520improving%2520risk%2520awareness%2520in%2520LLM-based%2520embodied%2520agents%2520in%2520the%250Afuture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RiskAwareBench%3A%20Towards%20Evaluating%20Physical%20Risk%20Awareness%20for%0A%20%20High-level%20Planning%20of%20LLM-based%20Embodied%20Agents&entry.906535625=Zihao%20Zhu%20and%20Bingzhe%20Wu%20and%20Zhengyou%20Zhang%20and%20Baoyuan%20Wu&entry.1292438233=%20%20The%20integration%20of%20large%20language%20models%20%28LLMs%29%20into%20robotics%20significantly%0Aenhances%20the%20capabilities%20of%20embodied%20agents%20in%20understanding%20and%20executing%0Acomplex%20natural%20language%20instructions.%20However%2C%20the%20unmitigated%20deployment%20of%0ALLM-based%20embodied%20systems%20in%20real-world%20environments%20may%20pose%20potential%0Aphysical%20risks%2C%20such%20as%20property%20damage%20and%20personal%20injury.%20Existing%20security%0Abenchmarks%20for%20LLMs%20overlook%20risk%20awareness%20for%20LLM-based%20embodied%20agents.%20To%0Aaddress%20this%20gap%2C%20we%20propose%20RiskAwareBench%2C%20an%20automated%20framework%20designed%20to%0Aassess%20physical%20risks%20awareness%20in%20LLM-based%20embodied%20agents.%20RiskAwareBench%0Aconsists%20of%20four%20modules%3A%20safety%20tips%20generation%2C%20risky%20scene%20generation%2C%20plan%0Ageneration%2C%20and%20evaluation%2C%20enabling%20comprehensive%20risk%20assessment%20with%20minimal%0Amanual%20intervention.%20Utilizing%20this%20framework%2C%20we%20compile%20the%20PhysicalRisk%0Adataset%2C%20encompassing%20diverse%20scenarios%20with%20associated%20safety%20tips%2C%0Aobservations%2C%20and%20instructions.%20Extensive%20experiments%20reveal%20that%20most%20LLMs%0Aexhibit%20insufficient%20physical%20risk%20awareness%2C%20and%20baseline%20risk%20mitigation%0Astrategies%20yield%20limited%20enhancement%2C%20which%20emphasizes%20the%20urgency%20and%0Acruciality%20of%20improving%20risk%20awareness%20in%20LLM-based%20embodied%20agents%20in%20the%0Afuture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04449v1&entry.124074799=Read"},
{"title": "Statistical Framework for Clustering MU-MIMO Wireless via Second Order\n  Statistics", "author": "Roberto Pereira and Xavier Mestre", "abstract": "  This work explores the clustering of wireless users by examining the\ndistances between their channel covariance matrices, which reside on the\nRiemannian manifold of positive definite matrices. Specifically, we consider an\nestimator of the Log-Euclidean distance between multiple sample covariance\nmatrices (SCMs) consistent when the number of samples and the observation size\ngrow unbounded at the same rate. Within the context of multi-user MIMO\n(MU-MIMO) wireless communication systems, we develop a statistical framework\nthat allows to accurate predictions of the clustering algorithm's performance\nunder realistic conditions. Specifically, we present a central limit theorem\nthat establishes the asymptotic Gaussianity of the consistent estimator of the\nlog-Euclidean distance computed over two sample covariance matrices.\n", "link": "http://arxiv.org/abs/2408.04484v1", "date": "2024-08-08", "relevancy": 1.6209, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4088}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4053}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistical%20Framework%20for%20Clustering%20MU-MIMO%20Wireless%20via%20Second%20Order%0A%20%20Statistics&body=Title%3A%20Statistical%20Framework%20for%20Clustering%20MU-MIMO%20Wireless%20via%20Second%20Order%0A%20%20Statistics%0AAuthor%3A%20Roberto%20Pereira%20and%20Xavier%20Mestre%0AAbstract%3A%20%20%20This%20work%20explores%20the%20clustering%20of%20wireless%20users%20by%20examining%20the%0Adistances%20between%20their%20channel%20covariance%20matrices%2C%20which%20reside%20on%20the%0ARiemannian%20manifold%20of%20positive%20definite%20matrices.%20Specifically%2C%20we%20consider%20an%0Aestimator%20of%20the%20Log-Euclidean%20distance%20between%20multiple%20sample%20covariance%0Amatrices%20%28SCMs%29%20consistent%20when%20the%20number%20of%20samples%20and%20the%20observation%20size%0Agrow%20unbounded%20at%20the%20same%20rate.%20Within%20the%20context%20of%20multi-user%20MIMO%0A%28MU-MIMO%29%20wireless%20communication%20systems%2C%20we%20develop%20a%20statistical%20framework%0Athat%20allows%20to%20accurate%20predictions%20of%20the%20clustering%20algorithm%27s%20performance%0Aunder%20realistic%20conditions.%20Specifically%2C%20we%20present%20a%20central%20limit%20theorem%0Athat%20establishes%20the%20asymptotic%20Gaussianity%20of%20the%20consistent%20estimator%20of%20the%0Alog-Euclidean%20distance%20computed%20over%20two%20sample%20covariance%20matrices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistical%2520Framework%2520for%2520Clustering%2520MU-MIMO%2520Wireless%2520via%2520Second%2520Order%250A%2520%2520Statistics%26entry.906535625%3DRoberto%2520Pereira%2520and%2520Xavier%2520Mestre%26entry.1292438233%3D%2520%2520This%2520work%2520explores%2520the%2520clustering%2520of%2520wireless%2520users%2520by%2520examining%2520the%250Adistances%2520between%2520their%2520channel%2520covariance%2520matrices%252C%2520which%2520reside%2520on%2520the%250ARiemannian%2520manifold%2520of%2520positive%2520definite%2520matrices.%2520Specifically%252C%2520we%2520consider%2520an%250Aestimator%2520of%2520the%2520Log-Euclidean%2520distance%2520between%2520multiple%2520sample%2520covariance%250Amatrices%2520%2528SCMs%2529%2520consistent%2520when%2520the%2520number%2520of%2520samples%2520and%2520the%2520observation%2520size%250Agrow%2520unbounded%2520at%2520the%2520same%2520rate.%2520Within%2520the%2520context%2520of%2520multi-user%2520MIMO%250A%2528MU-MIMO%2529%2520wireless%2520communication%2520systems%252C%2520we%2520develop%2520a%2520statistical%2520framework%250Athat%2520allows%2520to%2520accurate%2520predictions%2520of%2520the%2520clustering%2520algorithm%2527s%2520performance%250Aunder%2520realistic%2520conditions.%2520Specifically%252C%2520we%2520present%2520a%2520central%2520limit%2520theorem%250Athat%2520establishes%2520the%2520asymptotic%2520Gaussianity%2520of%2520the%2520consistent%2520estimator%2520of%2520the%250Alog-Euclidean%2520distance%2520computed%2520over%2520two%2520sample%2520covariance%2520matrices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistical%20Framework%20for%20Clustering%20MU-MIMO%20Wireless%20via%20Second%20Order%0A%20%20Statistics&entry.906535625=Roberto%20Pereira%20and%20Xavier%20Mestre&entry.1292438233=%20%20This%20work%20explores%20the%20clustering%20of%20wireless%20users%20by%20examining%20the%0Adistances%20between%20their%20channel%20covariance%20matrices%2C%20which%20reside%20on%20the%0ARiemannian%20manifold%20of%20positive%20definite%20matrices.%20Specifically%2C%20we%20consider%20an%0Aestimator%20of%20the%20Log-Euclidean%20distance%20between%20multiple%20sample%20covariance%0Amatrices%20%28SCMs%29%20consistent%20when%20the%20number%20of%20samples%20and%20the%20observation%20size%0Agrow%20unbounded%20at%20the%20same%20rate.%20Within%20the%20context%20of%20multi-user%20MIMO%0A%28MU-MIMO%29%20wireless%20communication%20systems%2C%20we%20develop%20a%20statistical%20framework%0Athat%20allows%20to%20accurate%20predictions%20of%20the%20clustering%20algorithm%27s%20performance%0Aunder%20realistic%20conditions.%20Specifically%2C%20we%20present%20a%20central%20limit%20theorem%0Athat%20establishes%20the%20asymptotic%20Gaussianity%20of%20the%20consistent%20estimator%20of%20the%0Alog-Euclidean%20distance%20computed%20over%20two%20sample%20covariance%20matrices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04484v1&entry.124074799=Read"},
{"title": "FORGE: Force-Guided Exploration for Robust Contact-Rich Manipulation\n  under Uncertainty", "author": "Michael Noseworthy and Bingjie Tang and Bowen Wen and Ankur Handa and Nicholas Roy and Dieter Fox and Fabio Ramos and Yashraj Narang and Iretiayo Akinola", "abstract": "  We present FORGE, a method that enables sim-to-real transfer of contact-rich\nmanipulation policies in the presence of significant pose uncertainty. FORGE\ncombines a force threshold mechanism with a dynamics randomization scheme\nduring policy learning in simulation, to enable the robust transfer of the\nlearned policies to the real robot. At deployment, FORGE policies, conditioned\non a maximum allowable force, adaptively perform contact-rich tasks while\nrespecting the specified force threshold, regardless of the controller gains.\nAdditionally, FORGE autonomously predicts a termination action once the task\nhas succeeded. We demonstrate that FORGE can be used to learn a variety of\nrobust contact-rich policies, enabling multi-stage assembly of a planetary gear\nsystem, which requires success across three assembly tasks: nut-threading,\ninsertion, and gear meshing. Project website can be accessed at\nhttps://noseworm.github.io/forge/.\n", "link": "http://arxiv.org/abs/2408.04587v1", "date": "2024-08-08", "relevancy": 1.6102, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5521}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5372}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5203}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FORGE%3A%20Force-Guided%20Exploration%20for%20Robust%20Contact-Rich%20Manipulation%0A%20%20under%20Uncertainty&body=Title%3A%20FORGE%3A%20Force-Guided%20Exploration%20for%20Robust%20Contact-Rich%20Manipulation%0A%20%20under%20Uncertainty%0AAuthor%3A%20Michael%20Noseworthy%20and%20Bingjie%20Tang%20and%20Bowen%20Wen%20and%20Ankur%20Handa%20and%20Nicholas%20Roy%20and%20Dieter%20Fox%20and%20Fabio%20Ramos%20and%20Yashraj%20Narang%20and%20Iretiayo%20Akinola%0AAbstract%3A%20%20%20We%20present%20FORGE%2C%20a%20method%20that%20enables%20sim-to-real%20transfer%20of%20contact-rich%0Amanipulation%20policies%20in%20the%20presence%20of%20significant%20pose%20uncertainty.%20FORGE%0Acombines%20a%20force%20threshold%20mechanism%20with%20a%20dynamics%20randomization%20scheme%0Aduring%20policy%20learning%20in%20simulation%2C%20to%20enable%20the%20robust%20transfer%20of%20the%0Alearned%20policies%20to%20the%20real%20robot.%20At%20deployment%2C%20FORGE%20policies%2C%20conditioned%0Aon%20a%20maximum%20allowable%20force%2C%20adaptively%20perform%20contact-rich%20tasks%20while%0Arespecting%20the%20specified%20force%20threshold%2C%20regardless%20of%20the%20controller%20gains.%0AAdditionally%2C%20FORGE%20autonomously%20predicts%20a%20termination%20action%20once%20the%20task%0Ahas%20succeeded.%20We%20demonstrate%20that%20FORGE%20can%20be%20used%20to%20learn%20a%20variety%20of%0Arobust%20contact-rich%20policies%2C%20enabling%20multi-stage%20assembly%20of%20a%20planetary%20gear%0Asystem%2C%20which%20requires%20success%20across%20three%20assembly%20tasks%3A%20nut-threading%2C%0Ainsertion%2C%20and%20gear%20meshing.%20Project%20website%20can%20be%20accessed%20at%0Ahttps%3A//noseworm.github.io/forge/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFORGE%253A%2520Force-Guided%2520Exploration%2520for%2520Robust%2520Contact-Rich%2520Manipulation%250A%2520%2520under%2520Uncertainty%26entry.906535625%3DMichael%2520Noseworthy%2520and%2520Bingjie%2520Tang%2520and%2520Bowen%2520Wen%2520and%2520Ankur%2520Handa%2520and%2520Nicholas%2520Roy%2520and%2520Dieter%2520Fox%2520and%2520Fabio%2520Ramos%2520and%2520Yashraj%2520Narang%2520and%2520Iretiayo%2520Akinola%26entry.1292438233%3D%2520%2520We%2520present%2520FORGE%252C%2520a%2520method%2520that%2520enables%2520sim-to-real%2520transfer%2520of%2520contact-rich%250Amanipulation%2520policies%2520in%2520the%2520presence%2520of%2520significant%2520pose%2520uncertainty.%2520FORGE%250Acombines%2520a%2520force%2520threshold%2520mechanism%2520with%2520a%2520dynamics%2520randomization%2520scheme%250Aduring%2520policy%2520learning%2520in%2520simulation%252C%2520to%2520enable%2520the%2520robust%2520transfer%2520of%2520the%250Alearned%2520policies%2520to%2520the%2520real%2520robot.%2520At%2520deployment%252C%2520FORGE%2520policies%252C%2520conditioned%250Aon%2520a%2520maximum%2520allowable%2520force%252C%2520adaptively%2520perform%2520contact-rich%2520tasks%2520while%250Arespecting%2520the%2520specified%2520force%2520threshold%252C%2520regardless%2520of%2520the%2520controller%2520gains.%250AAdditionally%252C%2520FORGE%2520autonomously%2520predicts%2520a%2520termination%2520action%2520once%2520the%2520task%250Ahas%2520succeeded.%2520We%2520demonstrate%2520that%2520FORGE%2520can%2520be%2520used%2520to%2520learn%2520a%2520variety%2520of%250Arobust%2520contact-rich%2520policies%252C%2520enabling%2520multi-stage%2520assembly%2520of%2520a%2520planetary%2520gear%250Asystem%252C%2520which%2520requires%2520success%2520across%2520three%2520assembly%2520tasks%253A%2520nut-threading%252C%250Ainsertion%252C%2520and%2520gear%2520meshing.%2520Project%2520website%2520can%2520be%2520accessed%2520at%250Ahttps%253A//noseworm.github.io/forge/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FORGE%3A%20Force-Guided%20Exploration%20for%20Robust%20Contact-Rich%20Manipulation%0A%20%20under%20Uncertainty&entry.906535625=Michael%20Noseworthy%20and%20Bingjie%20Tang%20and%20Bowen%20Wen%20and%20Ankur%20Handa%20and%20Nicholas%20Roy%20and%20Dieter%20Fox%20and%20Fabio%20Ramos%20and%20Yashraj%20Narang%20and%20Iretiayo%20Akinola&entry.1292438233=%20%20We%20present%20FORGE%2C%20a%20method%20that%20enables%20sim-to-real%20transfer%20of%20contact-rich%0Amanipulation%20policies%20in%20the%20presence%20of%20significant%20pose%20uncertainty.%20FORGE%0Acombines%20a%20force%20threshold%20mechanism%20with%20a%20dynamics%20randomization%20scheme%0Aduring%20policy%20learning%20in%20simulation%2C%20to%20enable%20the%20robust%20transfer%20of%20the%0Alearned%20policies%20to%20the%20real%20robot.%20At%20deployment%2C%20FORGE%20policies%2C%20conditioned%0Aon%20a%20maximum%20allowable%20force%2C%20adaptively%20perform%20contact-rich%20tasks%20while%0Arespecting%20the%20specified%20force%20threshold%2C%20regardless%20of%20the%20controller%20gains.%0AAdditionally%2C%20FORGE%20autonomously%20predicts%20a%20termination%20action%20once%20the%20task%0Ahas%20succeeded.%20We%20demonstrate%20that%20FORGE%20can%20be%20used%20to%20learn%20a%20variety%20of%0Arobust%20contact-rich%20policies%2C%20enabling%20multi-stage%20assembly%20of%20a%20planetary%20gear%0Asystem%2C%20which%20requires%20success%20across%20three%20assembly%20tasks%3A%20nut-threading%2C%0Ainsertion%2C%20and%20gear%20meshing.%20Project%20website%20can%20be%20accessed%20at%0Ahttps%3A//noseworm.github.io/forge/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04587v1&entry.124074799=Read"},
{"title": "SAM 2 in Robotic Surgery: An Empirical Evaluation for Robustness and\n  Generalization in Surgical Video Segmentation", "author": "Jieming Yu and An Wang and Wenzhen Dong and Mengya Xu and Mobarakol Islam and Jie Wang and Long Bai and Hongliang Ren", "abstract": "  The recent Segment Anything Model (SAM) 2 has demonstrated remarkable\nfoundational competence in semantic segmentation, with its memory mechanism and\nmask decoder further addressing challenges in video tracking and object\nocclusion, thereby achieving superior results in interactive segmentation for\nboth images and videos. Building upon our previous empirical studies, we\nfurther explore the zero-shot segmentation performance of SAM 2 in\nrobot-assisted surgery based on prompts, alongside its robustness against\nreal-world corruption. For static images, we employ two forms of prompts:\n1-point and bounding box, while for video sequences, the 1-point prompt is\napplied to the initial frame. Through extensive experimentation on the MICCAI\nEndoVis 2017 and EndoVis 2018 benchmarks, SAM 2, when utilizing bounding box\nprompts, outperforms state-of-the-art (SOTA) methods in comparative\nevaluations. The results with point prompts also exhibit a substantial\nenhancement over SAM's capabilities, nearing or even surpassing existing\nunprompted SOTA methodologies. Besides, SAM 2 demonstrates improved inference\nspeed and less performance degradation against various image corruption.\nAlthough slightly unsatisfactory results remain in specific edges or regions,\nSAM 2's robust adaptability to 1-point prompts underscores its potential for\ndownstream surgical tasks with limited prompt requirements.\n", "link": "http://arxiv.org/abs/2408.04593v1", "date": "2024-08-08", "relevancy": 1.6071, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5326}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAM%202%20in%20Robotic%20Surgery%3A%20An%20Empirical%20Evaluation%20for%20Robustness%20and%0A%20%20Generalization%20in%20Surgical%20Video%20Segmentation&body=Title%3A%20SAM%202%20in%20Robotic%20Surgery%3A%20An%20Empirical%20Evaluation%20for%20Robustness%20and%0A%20%20Generalization%20in%20Surgical%20Video%20Segmentation%0AAuthor%3A%20Jieming%20Yu%20and%20An%20Wang%20and%20Wenzhen%20Dong%20and%20Mengya%20Xu%20and%20Mobarakol%20Islam%20and%20Jie%20Wang%20and%20Long%20Bai%20and%20Hongliang%20Ren%0AAbstract%3A%20%20%20The%20recent%20Segment%20Anything%20Model%20%28SAM%29%202%20has%20demonstrated%20remarkable%0Afoundational%20competence%20in%20semantic%20segmentation%2C%20with%20its%20memory%20mechanism%20and%0Amask%20decoder%20further%20addressing%20challenges%20in%20video%20tracking%20and%20object%0Aocclusion%2C%20thereby%20achieving%20superior%20results%20in%20interactive%20segmentation%20for%0Aboth%20images%20and%20videos.%20Building%20upon%20our%20previous%20empirical%20studies%2C%20we%0Afurther%20explore%20the%20zero-shot%20segmentation%20performance%20of%20SAM%202%20in%0Arobot-assisted%20surgery%20based%20on%20prompts%2C%20alongside%20its%20robustness%20against%0Areal-world%20corruption.%20For%20static%20images%2C%20we%20employ%20two%20forms%20of%20prompts%3A%0A1-point%20and%20bounding%20box%2C%20while%20for%20video%20sequences%2C%20the%201-point%20prompt%20is%0Aapplied%20to%20the%20initial%20frame.%20Through%20extensive%20experimentation%20on%20the%20MICCAI%0AEndoVis%202017%20and%20EndoVis%202018%20benchmarks%2C%20SAM%202%2C%20when%20utilizing%20bounding%20box%0Aprompts%2C%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20in%20comparative%0Aevaluations.%20The%20results%20with%20point%20prompts%20also%20exhibit%20a%20substantial%0Aenhancement%20over%20SAM%27s%20capabilities%2C%20nearing%20or%20even%20surpassing%20existing%0Aunprompted%20SOTA%20methodologies.%20Besides%2C%20SAM%202%20demonstrates%20improved%20inference%0Aspeed%20and%20less%20performance%20degradation%20against%20various%20image%20corruption.%0AAlthough%20slightly%20unsatisfactory%20results%20remain%20in%20specific%20edges%20or%20regions%2C%0ASAM%202%27s%20robust%20adaptability%20to%201-point%20prompts%20underscores%20its%20potential%20for%0Adownstream%20surgical%20tasks%20with%20limited%20prompt%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04593v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAM%25202%2520in%2520Robotic%2520Surgery%253A%2520An%2520Empirical%2520Evaluation%2520for%2520Robustness%2520and%250A%2520%2520Generalization%2520in%2520Surgical%2520Video%2520Segmentation%26entry.906535625%3DJieming%2520Yu%2520and%2520An%2520Wang%2520and%2520Wenzhen%2520Dong%2520and%2520Mengya%2520Xu%2520and%2520Mobarakol%2520Islam%2520and%2520Jie%2520Wang%2520and%2520Long%2520Bai%2520and%2520Hongliang%2520Ren%26entry.1292438233%3D%2520%2520The%2520recent%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%25202%2520has%2520demonstrated%2520remarkable%250Afoundational%2520competence%2520in%2520semantic%2520segmentation%252C%2520with%2520its%2520memory%2520mechanism%2520and%250Amask%2520decoder%2520further%2520addressing%2520challenges%2520in%2520video%2520tracking%2520and%2520object%250Aocclusion%252C%2520thereby%2520achieving%2520superior%2520results%2520in%2520interactive%2520segmentation%2520for%250Aboth%2520images%2520and%2520videos.%2520Building%2520upon%2520our%2520previous%2520empirical%2520studies%252C%2520we%250Afurther%2520explore%2520the%2520zero-shot%2520segmentation%2520performance%2520of%2520SAM%25202%2520in%250Arobot-assisted%2520surgery%2520based%2520on%2520prompts%252C%2520alongside%2520its%2520robustness%2520against%250Areal-world%2520corruption.%2520For%2520static%2520images%252C%2520we%2520employ%2520two%2520forms%2520of%2520prompts%253A%250A1-point%2520and%2520bounding%2520box%252C%2520while%2520for%2520video%2520sequences%252C%2520the%25201-point%2520prompt%2520is%250Aapplied%2520to%2520the%2520initial%2520frame.%2520Through%2520extensive%2520experimentation%2520on%2520the%2520MICCAI%250AEndoVis%25202017%2520and%2520EndoVis%25202018%2520benchmarks%252C%2520SAM%25202%252C%2520when%2520utilizing%2520bounding%2520box%250Aprompts%252C%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520in%2520comparative%250Aevaluations.%2520The%2520results%2520with%2520point%2520prompts%2520also%2520exhibit%2520a%2520substantial%250Aenhancement%2520over%2520SAM%2527s%2520capabilities%252C%2520nearing%2520or%2520even%2520surpassing%2520existing%250Aunprompted%2520SOTA%2520methodologies.%2520Besides%252C%2520SAM%25202%2520demonstrates%2520improved%2520inference%250Aspeed%2520and%2520less%2520performance%2520degradation%2520against%2520various%2520image%2520corruption.%250AAlthough%2520slightly%2520unsatisfactory%2520results%2520remain%2520in%2520specific%2520edges%2520or%2520regions%252C%250ASAM%25202%2527s%2520robust%2520adaptability%2520to%25201-point%2520prompts%2520underscores%2520its%2520potential%2520for%250Adownstream%2520surgical%2520tasks%2520with%2520limited%2520prompt%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04593v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAM%202%20in%20Robotic%20Surgery%3A%20An%20Empirical%20Evaluation%20for%20Robustness%20and%0A%20%20Generalization%20in%20Surgical%20Video%20Segmentation&entry.906535625=Jieming%20Yu%20and%20An%20Wang%20and%20Wenzhen%20Dong%20and%20Mengya%20Xu%20and%20Mobarakol%20Islam%20and%20Jie%20Wang%20and%20Long%20Bai%20and%20Hongliang%20Ren&entry.1292438233=%20%20The%20recent%20Segment%20Anything%20Model%20%28SAM%29%202%20has%20demonstrated%20remarkable%0Afoundational%20competence%20in%20semantic%20segmentation%2C%20with%20its%20memory%20mechanism%20and%0Amask%20decoder%20further%20addressing%20challenges%20in%20video%20tracking%20and%20object%0Aocclusion%2C%20thereby%20achieving%20superior%20results%20in%20interactive%20segmentation%20for%0Aboth%20images%20and%20videos.%20Building%20upon%20our%20previous%20empirical%20studies%2C%20we%0Afurther%20explore%20the%20zero-shot%20segmentation%20performance%20of%20SAM%202%20in%0Arobot-assisted%20surgery%20based%20on%20prompts%2C%20alongside%20its%20robustness%20against%0Areal-world%20corruption.%20For%20static%20images%2C%20we%20employ%20two%20forms%20of%20prompts%3A%0A1-point%20and%20bounding%20box%2C%20while%20for%20video%20sequences%2C%20the%201-point%20prompt%20is%0Aapplied%20to%20the%20initial%20frame.%20Through%20extensive%20experimentation%20on%20the%20MICCAI%0AEndoVis%202017%20and%20EndoVis%202018%20benchmarks%2C%20SAM%202%2C%20when%20utilizing%20bounding%20box%0Aprompts%2C%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20in%20comparative%0Aevaluations.%20The%20results%20with%20point%20prompts%20also%20exhibit%20a%20substantial%0Aenhancement%20over%20SAM%27s%20capabilities%2C%20nearing%20or%20even%20surpassing%20existing%0Aunprompted%20SOTA%20methodologies.%20Besides%2C%20SAM%202%20demonstrates%20improved%20inference%0Aspeed%20and%20less%20performance%20degradation%20against%20various%20image%20corruption.%0AAlthough%20slightly%20unsatisfactory%20results%20remain%20in%20specific%20edges%20or%20regions%2C%0ASAM%202%27s%20robust%20adaptability%20to%201-point%20prompts%20underscores%20its%20potential%20for%0Adownstream%20surgical%20tasks%20with%20limited%20prompt%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04593v1&entry.124074799=Read"},
{"title": "Activation thresholds and expressiveness of polynomial neural networks", "author": "Bella Finkel and Jose Israel Rodriguez and Chenxi Wu and Thomas Yahl", "abstract": "  Polynomial neural networks have been implemented in a range of applications\nand present an advantageous framework for theoretical machine learning. A\npolynomial neural network of fixed architecture and activation degree gives an\nalgebraic map from the network's weights to a set of polynomials. The image of\nthis map is the space of functions representable by the network. Its Zariski\nclosure is an affine variety known as a neurovariety. The dimension of a\npolynomial neural network's neurovariety provides a measure of its\nexpressivity. In this work, we introduce the notion of the activation threshold\nof a network architecture which expresses when the dimension of a neurovariety\nachieves its theoretical maximum. In addition, we prove expressiveness results\nfor polynomial neural networks with equi-width~architectures.\n", "link": "http://arxiv.org/abs/2408.04569v1", "date": "2024-08-08", "relevancy": 1.5866, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.399}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3954}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3948}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activation%20thresholds%20and%20expressiveness%20of%20polynomial%20neural%20networks&body=Title%3A%20Activation%20thresholds%20and%20expressiveness%20of%20polynomial%20neural%20networks%0AAuthor%3A%20Bella%20Finkel%20and%20Jose%20Israel%20Rodriguez%20and%20Chenxi%20Wu%20and%20Thomas%20Yahl%0AAbstract%3A%20%20%20Polynomial%20neural%20networks%20have%20been%20implemented%20in%20a%20range%20of%20applications%0Aand%20present%20an%20advantageous%20framework%20for%20theoretical%20machine%20learning.%20A%0Apolynomial%20neural%20network%20of%20fixed%20architecture%20and%20activation%20degree%20gives%20an%0Aalgebraic%20map%20from%20the%20network%27s%20weights%20to%20a%20set%20of%20polynomials.%20The%20image%20of%0Athis%20map%20is%20the%20space%20of%20functions%20representable%20by%20the%20network.%20Its%20Zariski%0Aclosure%20is%20an%20affine%20variety%20known%20as%20a%20neurovariety.%20The%20dimension%20of%20a%0Apolynomial%20neural%20network%27s%20neurovariety%20provides%20a%20measure%20of%20its%0Aexpressivity.%20In%20this%20work%2C%20we%20introduce%20the%20notion%20of%20the%20activation%20threshold%0Aof%20a%20network%20architecture%20which%20expresses%20when%20the%20dimension%20of%20a%20neurovariety%0Aachieves%20its%20theoretical%20maximum.%20In%20addition%2C%20we%20prove%20expressiveness%20results%0Afor%20polynomial%20neural%20networks%20with%20equi-width~architectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivation%2520thresholds%2520and%2520expressiveness%2520of%2520polynomial%2520neural%2520networks%26entry.906535625%3DBella%2520Finkel%2520and%2520Jose%2520Israel%2520Rodriguez%2520and%2520Chenxi%2520Wu%2520and%2520Thomas%2520Yahl%26entry.1292438233%3D%2520%2520Polynomial%2520neural%2520networks%2520have%2520been%2520implemented%2520in%2520a%2520range%2520of%2520applications%250Aand%2520present%2520an%2520advantageous%2520framework%2520for%2520theoretical%2520machine%2520learning.%2520A%250Apolynomial%2520neural%2520network%2520of%2520fixed%2520architecture%2520and%2520activation%2520degree%2520gives%2520an%250Aalgebraic%2520map%2520from%2520the%2520network%2527s%2520weights%2520to%2520a%2520set%2520of%2520polynomials.%2520The%2520image%2520of%250Athis%2520map%2520is%2520the%2520space%2520of%2520functions%2520representable%2520by%2520the%2520network.%2520Its%2520Zariski%250Aclosure%2520is%2520an%2520affine%2520variety%2520known%2520as%2520a%2520neurovariety.%2520The%2520dimension%2520of%2520a%250Apolynomial%2520neural%2520network%2527s%2520neurovariety%2520provides%2520a%2520measure%2520of%2520its%250Aexpressivity.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520notion%2520of%2520the%2520activation%2520threshold%250Aof%2520a%2520network%2520architecture%2520which%2520expresses%2520when%2520the%2520dimension%2520of%2520a%2520neurovariety%250Aachieves%2520its%2520theoretical%2520maximum.%2520In%2520addition%252C%2520we%2520prove%2520expressiveness%2520results%250Afor%2520polynomial%2520neural%2520networks%2520with%2520equi-width~architectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activation%20thresholds%20and%20expressiveness%20of%20polynomial%20neural%20networks&entry.906535625=Bella%20Finkel%20and%20Jose%20Israel%20Rodriguez%20and%20Chenxi%20Wu%20and%20Thomas%20Yahl&entry.1292438233=%20%20Polynomial%20neural%20networks%20have%20been%20implemented%20in%20a%20range%20of%20applications%0Aand%20present%20an%20advantageous%20framework%20for%20theoretical%20machine%20learning.%20A%0Apolynomial%20neural%20network%20of%20fixed%20architecture%20and%20activation%20degree%20gives%20an%0Aalgebraic%20map%20from%20the%20network%27s%20weights%20to%20a%20set%20of%20polynomials.%20The%20image%20of%0Athis%20map%20is%20the%20space%20of%20functions%20representable%20by%20the%20network.%20Its%20Zariski%0Aclosure%20is%20an%20affine%20variety%20known%20as%20a%20neurovariety.%20The%20dimension%20of%20a%0Apolynomial%20neural%20network%27s%20neurovariety%20provides%20a%20measure%20of%20its%0Aexpressivity.%20In%20this%20work%2C%20we%20introduce%20the%20notion%20of%20the%20activation%20threshold%0Aof%20a%20network%20architecture%20which%20expresses%20when%20the%20dimension%20of%20a%20neurovariety%0Aachieves%20its%20theoretical%20maximum.%20In%20addition%2C%20we%20prove%20expressiveness%20results%0Afor%20polynomial%20neural%20networks%20with%20equi-width~architectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04569v1&entry.124074799=Read"},
{"title": "Random Walk Diffusion for Efficient Large-Scale Graph Generation", "author": "Tobias Bernecker and Ghalia Rehawi and Francesco Paolo Casale and Janine Knauer-Arloth and Annalisa Marsico", "abstract": "  Graph generation addresses the problem of generating new graphs that have a\ndata distribution similar to real-world graphs. While previous diffusion-based\ngraph generation methods have shown promising results, they often struggle to\nscale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive\nRandOm Walk Diffusion), a novel random walk-based diffusion approach for\nefficient large-scale graph generation. Our method encompasses two components\nin an iterative process of random walk sampling and graph pruning. We\ndemonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing\nother baseline methods in terms of both generation time and multiple graph\nstatistics, reflecting the high quality of the generated graphs.\n", "link": "http://arxiv.org/abs/2408.04461v1", "date": "2024-08-08", "relevancy": 1.5584, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5454}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4995}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Random%20Walk%20Diffusion%20for%20Efficient%20Large-Scale%20Graph%20Generation&body=Title%3A%20Random%20Walk%20Diffusion%20for%20Efficient%20Large-Scale%20Graph%20Generation%0AAuthor%3A%20Tobias%20Bernecker%20and%20Ghalia%20Rehawi%20and%20Francesco%20Paolo%20Casale%20and%20Janine%20Knauer-Arloth%20and%20Annalisa%20Marsico%0AAbstract%3A%20%20%20Graph%20generation%20addresses%20the%20problem%20of%20generating%20new%20graphs%20that%20have%20a%0Adata%20distribution%20similar%20to%20real-world%20graphs.%20While%20previous%20diffusion-based%0Agraph%20generation%20methods%20have%20shown%20promising%20results%2C%20they%20often%20struggle%20to%0Ascale%20to%20large%20graphs.%20In%20this%20work%2C%20we%20propose%20ARROW-Diff%20%28AutoRegressive%0ARandOm%20Walk%20Diffusion%29%2C%20a%20novel%20random%20walk-based%20diffusion%20approach%20for%0Aefficient%20large-scale%20graph%20generation.%20Our%20method%20encompasses%20two%20components%0Ain%20an%20iterative%20process%20of%20random%20walk%20sampling%20and%20graph%20pruning.%20We%0Ademonstrate%20that%20ARROW-Diff%20can%20scale%20to%20large%20graphs%20efficiently%2C%20surpassing%0Aother%20baseline%20methods%20in%20terms%20of%20both%20generation%20time%20and%20multiple%20graph%0Astatistics%2C%20reflecting%20the%20high%20quality%20of%20the%20generated%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRandom%2520Walk%2520Diffusion%2520for%2520Efficient%2520Large-Scale%2520Graph%2520Generation%26entry.906535625%3DTobias%2520Bernecker%2520and%2520Ghalia%2520Rehawi%2520and%2520Francesco%2520Paolo%2520Casale%2520and%2520Janine%2520Knauer-Arloth%2520and%2520Annalisa%2520Marsico%26entry.1292438233%3D%2520%2520Graph%2520generation%2520addresses%2520the%2520problem%2520of%2520generating%2520new%2520graphs%2520that%2520have%2520a%250Adata%2520distribution%2520similar%2520to%2520real-world%2520graphs.%2520While%2520previous%2520diffusion-based%250Agraph%2520generation%2520methods%2520have%2520shown%2520promising%2520results%252C%2520they%2520often%2520struggle%2520to%250Ascale%2520to%2520large%2520graphs.%2520In%2520this%2520work%252C%2520we%2520propose%2520ARROW-Diff%2520%2528AutoRegressive%250ARandOm%2520Walk%2520Diffusion%2529%252C%2520a%2520novel%2520random%2520walk-based%2520diffusion%2520approach%2520for%250Aefficient%2520large-scale%2520graph%2520generation.%2520Our%2520method%2520encompasses%2520two%2520components%250Ain%2520an%2520iterative%2520process%2520of%2520random%2520walk%2520sampling%2520and%2520graph%2520pruning.%2520We%250Ademonstrate%2520that%2520ARROW-Diff%2520can%2520scale%2520to%2520large%2520graphs%2520efficiently%252C%2520surpassing%250Aother%2520baseline%2520methods%2520in%2520terms%2520of%2520both%2520generation%2520time%2520and%2520multiple%2520graph%250Astatistics%252C%2520reflecting%2520the%2520high%2520quality%2520of%2520the%2520generated%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Random%20Walk%20Diffusion%20for%20Efficient%20Large-Scale%20Graph%20Generation&entry.906535625=Tobias%20Bernecker%20and%20Ghalia%20Rehawi%20and%20Francesco%20Paolo%20Casale%20and%20Janine%20Knauer-Arloth%20and%20Annalisa%20Marsico&entry.1292438233=%20%20Graph%20generation%20addresses%20the%20problem%20of%20generating%20new%20graphs%20that%20have%20a%0Adata%20distribution%20similar%20to%20real-world%20graphs.%20While%20previous%20diffusion-based%0Agraph%20generation%20methods%20have%20shown%20promising%20results%2C%20they%20often%20struggle%20to%0Ascale%20to%20large%20graphs.%20In%20this%20work%2C%20we%20propose%20ARROW-Diff%20%28AutoRegressive%0ARandOm%20Walk%20Diffusion%29%2C%20a%20novel%20random%20walk-based%20diffusion%20approach%20for%0Aefficient%20large-scale%20graph%20generation.%20Our%20method%20encompasses%20two%20components%0Ain%20an%20iterative%20process%20of%20random%20walk%20sampling%20and%20graph%20pruning.%20We%0Ademonstrate%20that%20ARROW-Diff%20can%20scale%20to%20large%20graphs%20efficiently%2C%20surpassing%0Aother%20baseline%20methods%20in%20terms%20of%20both%20generation%20time%20and%20multiple%20graph%0Astatistics%2C%20reflecting%20the%20high%20quality%20of%20the%20generated%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04461v1&entry.124074799=Read"},
{"title": "How Transformers Utilize Multi-Head Attention in In-Context Learning? A\n  Case Study on Sparse Linear Regression", "author": "Xingwu Chen and Lei Zhao and Difan Zou", "abstract": "  Despite the remarkable success of transformer-based models in various\nreal-world tasks, their underlying mechanisms remain poorly understood. Recent\nstudies have suggested that transformers can implement gradient descent as an\nin-context learner for linear regression problems and have developed various\ntheoretical analyses accordingly. However, these works mostly focus on the\nexpressive power of transformers by designing specific parameter constructions,\nlacking a comprehensive understanding of their inherent working mechanisms\npost-training. In this study, we consider a sparse linear regression problem\nand investigate how a trained multi-head transformer performs in-context\nlearning. We experimentally discover that the utilization of multi-heads\nexhibits different patterns across layers: multiple heads are utilized and\nessential in the first layer, while usually only a single head is sufficient\nfor subsequent layers. We provide a theoretical explanation for this\nobservation: the first layer preprocesses the context data, and the following\nlayers execute simple optimization steps based on the preprocessed context.\nMoreover, we demonstrate that such a preprocess-then-optimize algorithm can\nsignificantly outperform naive gradient descent and ridge regression\nalgorithms. Further experimental results support our explanations. Our findings\noffer insights into the benefits of multi-head attention and contribute to\nunderstanding the more intricate mechanisms hidden within trained transformers.\n", "link": "http://arxiv.org/abs/2408.04532v1", "date": "2024-08-08", "relevancy": 1.5524, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5514}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5121}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Transformers%20Utilize%20Multi-Head%20Attention%20in%20In-Context%20Learning%3F%20A%0A%20%20Case%20Study%20on%20Sparse%20Linear%20Regression&body=Title%3A%20How%20Transformers%20Utilize%20Multi-Head%20Attention%20in%20In-Context%20Learning%3F%20A%0A%20%20Case%20Study%20on%20Sparse%20Linear%20Regression%0AAuthor%3A%20Xingwu%20Chen%20and%20Lei%20Zhao%20and%20Difan%20Zou%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20success%20of%20transformer-based%20models%20in%20various%0Areal-world%20tasks%2C%20their%20underlying%20mechanisms%20remain%20poorly%20understood.%20Recent%0Astudies%20have%20suggested%20that%20transformers%20can%20implement%20gradient%20descent%20as%20an%0Ain-context%20learner%20for%20linear%20regression%20problems%20and%20have%20developed%20various%0Atheoretical%20analyses%20accordingly.%20However%2C%20these%20works%20mostly%20focus%20on%20the%0Aexpressive%20power%20of%20transformers%20by%20designing%20specific%20parameter%20constructions%2C%0Alacking%20a%20comprehensive%20understanding%20of%20their%20inherent%20working%20mechanisms%0Apost-training.%20In%20this%20study%2C%20we%20consider%20a%20sparse%20linear%20regression%20problem%0Aand%20investigate%20how%20a%20trained%20multi-head%20transformer%20performs%20in-context%0Alearning.%20We%20experimentally%20discover%20that%20the%20utilization%20of%20multi-heads%0Aexhibits%20different%20patterns%20across%20layers%3A%20multiple%20heads%20are%20utilized%20and%0Aessential%20in%20the%20first%20layer%2C%20while%20usually%20only%20a%20single%20head%20is%20sufficient%0Afor%20subsequent%20layers.%20We%20provide%20a%20theoretical%20explanation%20for%20this%0Aobservation%3A%20the%20first%20layer%20preprocesses%20the%20context%20data%2C%20and%20the%20following%0Alayers%20execute%20simple%20optimization%20steps%20based%20on%20the%20preprocessed%20context.%0AMoreover%2C%20we%20demonstrate%20that%20such%20a%20preprocess-then-optimize%20algorithm%20can%0Asignificantly%20outperform%20naive%20gradient%20descent%20and%20ridge%20regression%0Aalgorithms.%20Further%20experimental%20results%20support%20our%20explanations.%20Our%20findings%0Aoffer%20insights%20into%20the%20benefits%20of%20multi-head%20attention%20and%20contribute%20to%0Aunderstanding%20the%20more%20intricate%20mechanisms%20hidden%20within%20trained%20transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Transformers%2520Utilize%2520Multi-Head%2520Attention%2520in%2520In-Context%2520Learning%253F%2520A%250A%2520%2520Case%2520Study%2520on%2520Sparse%2520Linear%2520Regression%26entry.906535625%3DXingwu%2520Chen%2520and%2520Lei%2520Zhao%2520and%2520Difan%2520Zou%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520success%2520of%2520transformer-based%2520models%2520in%2520various%250Areal-world%2520tasks%252C%2520their%2520underlying%2520mechanisms%2520remain%2520poorly%2520understood.%2520Recent%250Astudies%2520have%2520suggested%2520that%2520transformers%2520can%2520implement%2520gradient%2520descent%2520as%2520an%250Ain-context%2520learner%2520for%2520linear%2520regression%2520problems%2520and%2520have%2520developed%2520various%250Atheoretical%2520analyses%2520accordingly.%2520However%252C%2520these%2520works%2520mostly%2520focus%2520on%2520the%250Aexpressive%2520power%2520of%2520transformers%2520by%2520designing%2520specific%2520parameter%2520constructions%252C%250Alacking%2520a%2520comprehensive%2520understanding%2520of%2520their%2520inherent%2520working%2520mechanisms%250Apost-training.%2520In%2520this%2520study%252C%2520we%2520consider%2520a%2520sparse%2520linear%2520regression%2520problem%250Aand%2520investigate%2520how%2520a%2520trained%2520multi-head%2520transformer%2520performs%2520in-context%250Alearning.%2520We%2520experimentally%2520discover%2520that%2520the%2520utilization%2520of%2520multi-heads%250Aexhibits%2520different%2520patterns%2520across%2520layers%253A%2520multiple%2520heads%2520are%2520utilized%2520and%250Aessential%2520in%2520the%2520first%2520layer%252C%2520while%2520usually%2520only%2520a%2520single%2520head%2520is%2520sufficient%250Afor%2520subsequent%2520layers.%2520We%2520provide%2520a%2520theoretical%2520explanation%2520for%2520this%250Aobservation%253A%2520the%2520first%2520layer%2520preprocesses%2520the%2520context%2520data%252C%2520and%2520the%2520following%250Alayers%2520execute%2520simple%2520optimization%2520steps%2520based%2520on%2520the%2520preprocessed%2520context.%250AMoreover%252C%2520we%2520demonstrate%2520that%2520such%2520a%2520preprocess-then-optimize%2520algorithm%2520can%250Asignificantly%2520outperform%2520naive%2520gradient%2520descent%2520and%2520ridge%2520regression%250Aalgorithms.%2520Further%2520experimental%2520results%2520support%2520our%2520explanations.%2520Our%2520findings%250Aoffer%2520insights%2520into%2520the%2520benefits%2520of%2520multi-head%2520attention%2520and%2520contribute%2520to%250Aunderstanding%2520the%2520more%2520intricate%2520mechanisms%2520hidden%2520within%2520trained%2520transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Transformers%20Utilize%20Multi-Head%20Attention%20in%20In-Context%20Learning%3F%20A%0A%20%20Case%20Study%20on%20Sparse%20Linear%20Regression&entry.906535625=Xingwu%20Chen%20and%20Lei%20Zhao%20and%20Difan%20Zou&entry.1292438233=%20%20Despite%20the%20remarkable%20success%20of%20transformer-based%20models%20in%20various%0Areal-world%20tasks%2C%20their%20underlying%20mechanisms%20remain%20poorly%20understood.%20Recent%0Astudies%20have%20suggested%20that%20transformers%20can%20implement%20gradient%20descent%20as%20an%0Ain-context%20learner%20for%20linear%20regression%20problems%20and%20have%20developed%20various%0Atheoretical%20analyses%20accordingly.%20However%2C%20these%20works%20mostly%20focus%20on%20the%0Aexpressive%20power%20of%20transformers%20by%20designing%20specific%20parameter%20constructions%2C%0Alacking%20a%20comprehensive%20understanding%20of%20their%20inherent%20working%20mechanisms%0Apost-training.%20In%20this%20study%2C%20we%20consider%20a%20sparse%20linear%20regression%20problem%0Aand%20investigate%20how%20a%20trained%20multi-head%20transformer%20performs%20in-context%0Alearning.%20We%20experimentally%20discover%20that%20the%20utilization%20of%20multi-heads%0Aexhibits%20different%20patterns%20across%20layers%3A%20multiple%20heads%20are%20utilized%20and%0Aessential%20in%20the%20first%20layer%2C%20while%20usually%20only%20a%20single%20head%20is%20sufficient%0Afor%20subsequent%20layers.%20We%20provide%20a%20theoretical%20explanation%20for%20this%0Aobservation%3A%20the%20first%20layer%20preprocesses%20the%20context%20data%2C%20and%20the%20following%0Alayers%20execute%20simple%20optimization%20steps%20based%20on%20the%20preprocessed%20context.%0AMoreover%2C%20we%20demonstrate%20that%20such%20a%20preprocess-then-optimize%20algorithm%20can%0Asignificantly%20outperform%20naive%20gradient%20descent%20and%20ridge%20regression%0Aalgorithms.%20Further%20experimental%20results%20support%20our%20explanations.%20Our%20findings%0Aoffer%20insights%20into%20the%20benefits%20of%20multi-head%20attention%20and%20contribute%20to%0Aunderstanding%20the%20more%20intricate%20mechanisms%20hidden%20within%20trained%20transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04532v1&entry.124074799=Read"},
{"title": "Saliency Detection in Educational Videos: Analyzing the Performance of\n  Current Models, Identifying Limitations and Advancement Directions", "author": "Evelyn Navarrete and Ralph Ewerth and Anett Hoppe", "abstract": "  Identifying the regions of a learning resource that a learner pays attention\nto is crucial for assessing the material's impact and improving its design and\nrelated support systems. Saliency detection in videos addresses the automatic\nrecognition of attention-drawing regions in single frames. In educational\nsettings, the recognition of pertinent regions in a video's visual stream can\nenhance content accessibility and information retrieval tasks such as video\nsegmentation, navigation, and summarization. Such advancements can pave the way\nfor the development of advanced AI-assisted technologies that support learning\nwith greater efficacy. However, this task becomes particularly challenging for\neducational videos due to the combination of unique characteristics such as\ntext, voice, illustrations, animations, and more. To the best of our knowledge,\nthere is currently no study that evaluates saliency detection approaches in\neducational videos. In this paper, we address this gap by evaluating four\nstate-of-the-art saliency detection approaches for educational videos. We\nreproduce the original studies and explore the replication capabilities for\ngeneral-purpose (non-educational) datasets. Then, we investigate the\ngeneralization capabilities of the models and evaluate their performance on\neducational videos. We conduct a comprehensive analysis to identify common\nfailure scenarios and possible areas of improvement. Our experimental results\nshow that educational videos remain a challenging context for generic video\nsaliency detection models.\n", "link": "http://arxiv.org/abs/2408.04515v1", "date": "2024-08-08", "relevancy": 1.5414, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5317}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5097}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Saliency%20Detection%20in%20Educational%20Videos%3A%20Analyzing%20the%20Performance%20of%0A%20%20Current%20Models%2C%20Identifying%20Limitations%20and%20Advancement%20Directions&body=Title%3A%20Saliency%20Detection%20in%20Educational%20Videos%3A%20Analyzing%20the%20Performance%20of%0A%20%20Current%20Models%2C%20Identifying%20Limitations%20and%20Advancement%20Directions%0AAuthor%3A%20Evelyn%20Navarrete%20and%20Ralph%20Ewerth%20and%20Anett%20Hoppe%0AAbstract%3A%20%20%20Identifying%20the%20regions%20of%20a%20learning%20resource%20that%20a%20learner%20pays%20attention%0Ato%20is%20crucial%20for%20assessing%20the%20material%27s%20impact%20and%20improving%20its%20design%20and%0Arelated%20support%20systems.%20Saliency%20detection%20in%20videos%20addresses%20the%20automatic%0Arecognition%20of%20attention-drawing%20regions%20in%20single%20frames.%20In%20educational%0Asettings%2C%20the%20recognition%20of%20pertinent%20regions%20in%20a%20video%27s%20visual%20stream%20can%0Aenhance%20content%20accessibility%20and%20information%20retrieval%20tasks%20such%20as%20video%0Asegmentation%2C%20navigation%2C%20and%20summarization.%20Such%20advancements%20can%20pave%20the%20way%0Afor%20the%20development%20of%20advanced%20AI-assisted%20technologies%20that%20support%20learning%0Awith%20greater%20efficacy.%20However%2C%20this%20task%20becomes%20particularly%20challenging%20for%0Aeducational%20videos%20due%20to%20the%20combination%20of%20unique%20characteristics%20such%20as%0Atext%2C%20voice%2C%20illustrations%2C%20animations%2C%20and%20more.%20To%20the%20best%20of%20our%20knowledge%2C%0Athere%20is%20currently%20no%20study%20that%20evaluates%20saliency%20detection%20approaches%20in%0Aeducational%20videos.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20evaluating%20four%0Astate-of-the-art%20saliency%20detection%20approaches%20for%20educational%20videos.%20We%0Areproduce%20the%20original%20studies%20and%20explore%20the%20replication%20capabilities%20for%0Ageneral-purpose%20%28non-educational%29%20datasets.%20Then%2C%20we%20investigate%20the%0Ageneralization%20capabilities%20of%20the%20models%20and%20evaluate%20their%20performance%20on%0Aeducational%20videos.%20We%20conduct%20a%20comprehensive%20analysis%20to%20identify%20common%0Afailure%20scenarios%20and%20possible%20areas%20of%20improvement.%20Our%20experimental%20results%0Ashow%20that%20educational%20videos%20remain%20a%20challenging%20context%20for%20generic%20video%0Asaliency%20detection%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaliency%2520Detection%2520in%2520Educational%2520Videos%253A%2520Analyzing%2520the%2520Performance%2520of%250A%2520%2520Current%2520Models%252C%2520Identifying%2520Limitations%2520and%2520Advancement%2520Directions%26entry.906535625%3DEvelyn%2520Navarrete%2520and%2520Ralph%2520Ewerth%2520and%2520Anett%2520Hoppe%26entry.1292438233%3D%2520%2520Identifying%2520the%2520regions%2520of%2520a%2520learning%2520resource%2520that%2520a%2520learner%2520pays%2520attention%250Ato%2520is%2520crucial%2520for%2520assessing%2520the%2520material%2527s%2520impact%2520and%2520improving%2520its%2520design%2520and%250Arelated%2520support%2520systems.%2520Saliency%2520detection%2520in%2520videos%2520addresses%2520the%2520automatic%250Arecognition%2520of%2520attention-drawing%2520regions%2520in%2520single%2520frames.%2520In%2520educational%250Asettings%252C%2520the%2520recognition%2520of%2520pertinent%2520regions%2520in%2520a%2520video%2527s%2520visual%2520stream%2520can%250Aenhance%2520content%2520accessibility%2520and%2520information%2520retrieval%2520tasks%2520such%2520as%2520video%250Asegmentation%252C%2520navigation%252C%2520and%2520summarization.%2520Such%2520advancements%2520can%2520pave%2520the%2520way%250Afor%2520the%2520development%2520of%2520advanced%2520AI-assisted%2520technologies%2520that%2520support%2520learning%250Awith%2520greater%2520efficacy.%2520However%252C%2520this%2520task%2520becomes%2520particularly%2520challenging%2520for%250Aeducational%2520videos%2520due%2520to%2520the%2520combination%2520of%2520unique%2520characteristics%2520such%2520as%250Atext%252C%2520voice%252C%2520illustrations%252C%2520animations%252C%2520and%2520more.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250Athere%2520is%2520currently%2520no%2520study%2520that%2520evaluates%2520saliency%2520detection%2520approaches%2520in%250Aeducational%2520videos.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520evaluating%2520four%250Astate-of-the-art%2520saliency%2520detection%2520approaches%2520for%2520educational%2520videos.%2520We%250Areproduce%2520the%2520original%2520studies%2520and%2520explore%2520the%2520replication%2520capabilities%2520for%250Ageneral-purpose%2520%2528non-educational%2529%2520datasets.%2520Then%252C%2520we%2520investigate%2520the%250Ageneralization%2520capabilities%2520of%2520the%2520models%2520and%2520evaluate%2520their%2520performance%2520on%250Aeducational%2520videos.%2520We%2520conduct%2520a%2520comprehensive%2520analysis%2520to%2520identify%2520common%250Afailure%2520scenarios%2520and%2520possible%2520areas%2520of%2520improvement.%2520Our%2520experimental%2520results%250Ashow%2520that%2520educational%2520videos%2520remain%2520a%2520challenging%2520context%2520for%2520generic%2520video%250Asaliency%2520detection%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Saliency%20Detection%20in%20Educational%20Videos%3A%20Analyzing%20the%20Performance%20of%0A%20%20Current%20Models%2C%20Identifying%20Limitations%20and%20Advancement%20Directions&entry.906535625=Evelyn%20Navarrete%20and%20Ralph%20Ewerth%20and%20Anett%20Hoppe&entry.1292438233=%20%20Identifying%20the%20regions%20of%20a%20learning%20resource%20that%20a%20learner%20pays%20attention%0Ato%20is%20crucial%20for%20assessing%20the%20material%27s%20impact%20and%20improving%20its%20design%20and%0Arelated%20support%20systems.%20Saliency%20detection%20in%20videos%20addresses%20the%20automatic%0Arecognition%20of%20attention-drawing%20regions%20in%20single%20frames.%20In%20educational%0Asettings%2C%20the%20recognition%20of%20pertinent%20regions%20in%20a%20video%27s%20visual%20stream%20can%0Aenhance%20content%20accessibility%20and%20information%20retrieval%20tasks%20such%20as%20video%0Asegmentation%2C%20navigation%2C%20and%20summarization.%20Such%20advancements%20can%20pave%20the%20way%0Afor%20the%20development%20of%20advanced%20AI-assisted%20technologies%20that%20support%20learning%0Awith%20greater%20efficacy.%20However%2C%20this%20task%20becomes%20particularly%20challenging%20for%0Aeducational%20videos%20due%20to%20the%20combination%20of%20unique%20characteristics%20such%20as%0Atext%2C%20voice%2C%20illustrations%2C%20animations%2C%20and%20more.%20To%20the%20best%20of%20our%20knowledge%2C%0Athere%20is%20currently%20no%20study%20that%20evaluates%20saliency%20detection%20approaches%20in%0Aeducational%20videos.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20evaluating%20four%0Astate-of-the-art%20saliency%20detection%20approaches%20for%20educational%20videos.%20We%0Areproduce%20the%20original%20studies%20and%20explore%20the%20replication%20capabilities%20for%0Ageneral-purpose%20%28non-educational%29%20datasets.%20Then%2C%20we%20investigate%20the%0Ageneralization%20capabilities%20of%20the%20models%20and%20evaluate%20their%20performance%20on%0Aeducational%20videos.%20We%20conduct%20a%20comprehensive%20analysis%20to%20identify%20common%0Afailure%20scenarios%20and%20possible%20areas%20of%20improvement.%20Our%20experimental%20results%0Ashow%20that%20educational%20videos%20remain%20a%20challenging%20context%20for%20generic%20video%0Asaliency%20detection%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04515v1&entry.124074799=Read"},
{"title": "Risk and cross validation in ridge regression with correlated samples", "author": "Alexander Atanasov and Jacob A. Zavatone-Veth and Cengiz Pehlevan", "abstract": "  Recent years have seen substantial advances in our understanding of\nhigh-dimensional ridge regression, but existing theories assume that training\nexamples are independent. By leveraging recent techniques from random matrix\ntheory and free probability, we provide sharp asymptotics for the in- and\nout-of-sample risks of ridge regression when the data points have arbitrary\ncorrelations. We demonstrate that in this setting, the generalized cross\nvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.\nHowever, in the case where the noise residuals have the same correlations as\nthe data points, one can modify the GCV to yield an efficiently-computable\nunbiased estimator that concentrates in the high-dimensional limit, which we\ndub CorrGCV. We further extend our asymptotic analysis to the case where the\ntest point has nontrivial correlations with the training set, a setting often\nencountered in time series forecasting. Assuming knowledge of the correlation\nstructure of the time series, this again yields an extension of the GCV\nestimator, and sharply characterizes the degree to which such test points yield\nan overly optimistic prediction of long-time risk. We validate the predictions\nof our theory across a variety of high dimensional data.\n", "link": "http://arxiv.org/abs/2408.04607v1", "date": "2024-08-08", "relevancy": 1.5336, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3982}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3843}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Risk%20and%20cross%20validation%20in%20ridge%20regression%20with%20correlated%20samples&body=Title%3A%20Risk%20and%20cross%20validation%20in%20ridge%20regression%20with%20correlated%20samples%0AAuthor%3A%20Alexander%20Atanasov%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Cengiz%20Pehlevan%0AAbstract%3A%20%20%20Recent%20years%20have%20seen%20substantial%20advances%20in%20our%20understanding%20of%0Ahigh-dimensional%20ridge%20regression%2C%20but%20existing%20theories%20assume%20that%20training%0Aexamples%20are%20independent.%20By%20leveraging%20recent%20techniques%20from%20random%20matrix%0Atheory%20and%20free%20probability%2C%20we%20provide%20sharp%20asymptotics%20for%20the%20in-%20and%0Aout-of-sample%20risks%20of%20ridge%20regression%20when%20the%20data%20points%20have%20arbitrary%0Acorrelations.%20We%20demonstrate%20that%20in%20this%20setting%2C%20the%20generalized%20cross%0Avalidation%20estimator%20%28GCV%29%20fails%20to%20correctly%20predict%20the%20out-of-sample%20risk.%0AHowever%2C%20in%20the%20case%20where%20the%20noise%20residuals%20have%20the%20same%20correlations%20as%0Athe%20data%20points%2C%20one%20can%20modify%20the%20GCV%20to%20yield%20an%20efficiently-computable%0Aunbiased%20estimator%20that%20concentrates%20in%20the%20high-dimensional%20limit%2C%20which%20we%0Adub%20CorrGCV.%20We%20further%20extend%20our%20asymptotic%20analysis%20to%20the%20case%20where%20the%0Atest%20point%20has%20nontrivial%20correlations%20with%20the%20training%20set%2C%20a%20setting%20often%0Aencountered%20in%20time%20series%20forecasting.%20Assuming%20knowledge%20of%20the%20correlation%0Astructure%20of%20the%20time%20series%2C%20this%20again%20yields%20an%20extension%20of%20the%20GCV%0Aestimator%2C%20and%20sharply%20characterizes%20the%20degree%20to%20which%20such%20test%20points%20yield%0Aan%20overly%20optimistic%20prediction%20of%20long-time%20risk.%20We%20validate%20the%20predictions%0Aof%20our%20theory%20across%20a%20variety%20of%20high%20dimensional%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRisk%2520and%2520cross%2520validation%2520in%2520ridge%2520regression%2520with%2520correlated%2520samples%26entry.906535625%3DAlexander%2520Atanasov%2520and%2520Jacob%2520A.%2520Zavatone-Veth%2520and%2520Cengiz%2520Pehlevan%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520seen%2520substantial%2520advances%2520in%2520our%2520understanding%2520of%250Ahigh-dimensional%2520ridge%2520regression%252C%2520but%2520existing%2520theories%2520assume%2520that%2520training%250Aexamples%2520are%2520independent.%2520By%2520leveraging%2520recent%2520techniques%2520from%2520random%2520matrix%250Atheory%2520and%2520free%2520probability%252C%2520we%2520provide%2520sharp%2520asymptotics%2520for%2520the%2520in-%2520and%250Aout-of-sample%2520risks%2520of%2520ridge%2520regression%2520when%2520the%2520data%2520points%2520have%2520arbitrary%250Acorrelations.%2520We%2520demonstrate%2520that%2520in%2520this%2520setting%252C%2520the%2520generalized%2520cross%250Avalidation%2520estimator%2520%2528GCV%2529%2520fails%2520to%2520correctly%2520predict%2520the%2520out-of-sample%2520risk.%250AHowever%252C%2520in%2520the%2520case%2520where%2520the%2520noise%2520residuals%2520have%2520the%2520same%2520correlations%2520as%250Athe%2520data%2520points%252C%2520one%2520can%2520modify%2520the%2520GCV%2520to%2520yield%2520an%2520efficiently-computable%250Aunbiased%2520estimator%2520that%2520concentrates%2520in%2520the%2520high-dimensional%2520limit%252C%2520which%2520we%250Adub%2520CorrGCV.%2520We%2520further%2520extend%2520our%2520asymptotic%2520analysis%2520to%2520the%2520case%2520where%2520the%250Atest%2520point%2520has%2520nontrivial%2520correlations%2520with%2520the%2520training%2520set%252C%2520a%2520setting%2520often%250Aencountered%2520in%2520time%2520series%2520forecasting.%2520Assuming%2520knowledge%2520of%2520the%2520correlation%250Astructure%2520of%2520the%2520time%2520series%252C%2520this%2520again%2520yields%2520an%2520extension%2520of%2520the%2520GCV%250Aestimator%252C%2520and%2520sharply%2520characterizes%2520the%2520degree%2520to%2520which%2520such%2520test%2520points%2520yield%250Aan%2520overly%2520optimistic%2520prediction%2520of%2520long-time%2520risk.%2520We%2520validate%2520the%2520predictions%250Aof%2520our%2520theory%2520across%2520a%2520variety%2520of%2520high%2520dimensional%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Risk%20and%20cross%20validation%20in%20ridge%20regression%20with%20correlated%20samples&entry.906535625=Alexander%20Atanasov%20and%20Jacob%20A.%20Zavatone-Veth%20and%20Cengiz%20Pehlevan&entry.1292438233=%20%20Recent%20years%20have%20seen%20substantial%20advances%20in%20our%20understanding%20of%0Ahigh-dimensional%20ridge%20regression%2C%20but%20existing%20theories%20assume%20that%20training%0Aexamples%20are%20independent.%20By%20leveraging%20recent%20techniques%20from%20random%20matrix%0Atheory%20and%20free%20probability%2C%20we%20provide%20sharp%20asymptotics%20for%20the%20in-%20and%0Aout-of-sample%20risks%20of%20ridge%20regression%20when%20the%20data%20points%20have%20arbitrary%0Acorrelations.%20We%20demonstrate%20that%20in%20this%20setting%2C%20the%20generalized%20cross%0Avalidation%20estimator%20%28GCV%29%20fails%20to%20correctly%20predict%20the%20out-of-sample%20risk.%0AHowever%2C%20in%20the%20case%20where%20the%20noise%20residuals%20have%20the%20same%20correlations%20as%0Athe%20data%20points%2C%20one%20can%20modify%20the%20GCV%20to%20yield%20an%20efficiently-computable%0Aunbiased%20estimator%20that%20concentrates%20in%20the%20high-dimensional%20limit%2C%20which%20we%0Adub%20CorrGCV.%20We%20further%20extend%20our%20asymptotic%20analysis%20to%20the%20case%20where%20the%0Atest%20point%20has%20nontrivial%20correlations%20with%20the%20training%20set%2C%20a%20setting%20often%0Aencountered%20in%20time%20series%20forecasting.%20Assuming%20knowledge%20of%20the%20correlation%0Astructure%20of%20the%20time%20series%2C%20this%20again%20yields%20an%20extension%20of%20the%20GCV%0Aestimator%2C%20and%20sharply%20characterizes%20the%20degree%20to%20which%20such%20test%20points%20yield%0Aan%20overly%20optimistic%20prediction%20of%20long-time%20risk.%20We%20validate%20the%20predictions%0Aof%20our%20theory%20across%20a%20variety%20of%20high%20dimensional%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04607v1&entry.124074799=Read"},
{"title": "Advancing Molecular Machine (Learned) Representations with\n  Stereoelectronics-Infused Molecular Graphs", "author": "Daniil A. Boiko and Thiago Resch\u00fctzegger and Benjamin Sanchez-Lengeling and Samuel M. Blau and Gabe Gomes", "abstract": "  Molecular representation is a foundational element in our understanding of\nthe physical world. Its importance ranges from the fundamentals of chemical\nreactions to the design of new therapies and materials. Previous molecular\nmachine learning models have employed strings, fingerprints, global features,\nand simple molecular graphs that are inherently information-sparse\nrepresentations. However, as the complexity of prediction tasks increases, the\nmolecular representation needs to encode higher fidelity information. This work\nintroduces a novel approach to infusing quantum-chemical-rich information into\nmolecular graphs via stereoelectronic effects. We show that the explicit\naddition of stereoelectronic interactions significantly improves the\nperformance of molecular machine learning models. Furthermore,\nstereoelectronics-infused representations can be learned and deployed with a\ntailored double graph neural network workflow, enabling its application to any\ndownstream molecular machine learning task. Finally, we show that the learned\nrepresentations allow for facile stereoelectronic evaluation of previously\nintractable systems, such as entire proteins, opening new avenues of molecular\ndesign.\n", "link": "http://arxiv.org/abs/2408.04520v1", "date": "2024-08-08", "relevancy": 1.52, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5306}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5035}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Molecular%20Machine%20%28Learned%29%20Representations%20with%0A%20%20Stereoelectronics-Infused%20Molecular%20Graphs&body=Title%3A%20Advancing%20Molecular%20Machine%20%28Learned%29%20Representations%20with%0A%20%20Stereoelectronics-Infused%20Molecular%20Graphs%0AAuthor%3A%20Daniil%20A.%20Boiko%20and%20Thiago%20Resch%C3%BCtzegger%20and%20Benjamin%20Sanchez-Lengeling%20and%20Samuel%20M.%20Blau%20and%20Gabe%20Gomes%0AAbstract%3A%20%20%20Molecular%20representation%20is%20a%20foundational%20element%20in%20our%20understanding%20of%0Athe%20physical%20world.%20Its%20importance%20ranges%20from%20the%20fundamentals%20of%20chemical%0Areactions%20to%20the%20design%20of%20new%20therapies%20and%20materials.%20Previous%20molecular%0Amachine%20learning%20models%20have%20employed%20strings%2C%20fingerprints%2C%20global%20features%2C%0Aand%20simple%20molecular%20graphs%20that%20are%20inherently%20information-sparse%0Arepresentations.%20However%2C%20as%20the%20complexity%20of%20prediction%20tasks%20increases%2C%20the%0Amolecular%20representation%20needs%20to%20encode%20higher%20fidelity%20information.%20This%20work%0Aintroduces%20a%20novel%20approach%20to%20infusing%20quantum-chemical-rich%20information%20into%0Amolecular%20graphs%20via%20stereoelectronic%20effects.%20We%20show%20that%20the%20explicit%0Aaddition%20of%20stereoelectronic%20interactions%20significantly%20improves%20the%0Aperformance%20of%20molecular%20machine%20learning%20models.%20Furthermore%2C%0Astereoelectronics-infused%20representations%20can%20be%20learned%20and%20deployed%20with%20a%0Atailored%20double%20graph%20neural%20network%20workflow%2C%20enabling%20its%20application%20to%20any%0Adownstream%20molecular%20machine%20learning%20task.%20Finally%2C%20we%20show%20that%20the%20learned%0Arepresentations%20allow%20for%20facile%20stereoelectronic%20evaluation%20of%20previously%0Aintractable%20systems%2C%20such%20as%20entire%20proteins%2C%20opening%20new%20avenues%20of%20molecular%0Adesign.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Molecular%2520Machine%2520%2528Learned%2529%2520Representations%2520with%250A%2520%2520Stereoelectronics-Infused%2520Molecular%2520Graphs%26entry.906535625%3DDaniil%2520A.%2520Boiko%2520and%2520Thiago%2520Resch%25C3%25BCtzegger%2520and%2520Benjamin%2520Sanchez-Lengeling%2520and%2520Samuel%2520M.%2520Blau%2520and%2520Gabe%2520Gomes%26entry.1292438233%3D%2520%2520Molecular%2520representation%2520is%2520a%2520foundational%2520element%2520in%2520our%2520understanding%2520of%250Athe%2520physical%2520world.%2520Its%2520importance%2520ranges%2520from%2520the%2520fundamentals%2520of%2520chemical%250Areactions%2520to%2520the%2520design%2520of%2520new%2520therapies%2520and%2520materials.%2520Previous%2520molecular%250Amachine%2520learning%2520models%2520have%2520employed%2520strings%252C%2520fingerprints%252C%2520global%2520features%252C%250Aand%2520simple%2520molecular%2520graphs%2520that%2520are%2520inherently%2520information-sparse%250Arepresentations.%2520However%252C%2520as%2520the%2520complexity%2520of%2520prediction%2520tasks%2520increases%252C%2520the%250Amolecular%2520representation%2520needs%2520to%2520encode%2520higher%2520fidelity%2520information.%2520This%2520work%250Aintroduces%2520a%2520novel%2520approach%2520to%2520infusing%2520quantum-chemical-rich%2520information%2520into%250Amolecular%2520graphs%2520via%2520stereoelectronic%2520effects.%2520We%2520show%2520that%2520the%2520explicit%250Aaddition%2520of%2520stereoelectronic%2520interactions%2520significantly%2520improves%2520the%250Aperformance%2520of%2520molecular%2520machine%2520learning%2520models.%2520Furthermore%252C%250Astereoelectronics-infused%2520representations%2520can%2520be%2520learned%2520and%2520deployed%2520with%2520a%250Atailored%2520double%2520graph%2520neural%2520network%2520workflow%252C%2520enabling%2520its%2520application%2520to%2520any%250Adownstream%2520molecular%2520machine%2520learning%2520task.%2520Finally%252C%2520we%2520show%2520that%2520the%2520learned%250Arepresentations%2520allow%2520for%2520facile%2520stereoelectronic%2520evaluation%2520of%2520previously%250Aintractable%2520systems%252C%2520such%2520as%2520entire%2520proteins%252C%2520opening%2520new%2520avenues%2520of%2520molecular%250Adesign.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Molecular%20Machine%20%28Learned%29%20Representations%20with%0A%20%20Stereoelectronics-Infused%20Molecular%20Graphs&entry.906535625=Daniil%20A.%20Boiko%20and%20Thiago%20Resch%C3%BCtzegger%20and%20Benjamin%20Sanchez-Lengeling%20and%20Samuel%20M.%20Blau%20and%20Gabe%20Gomes&entry.1292438233=%20%20Molecular%20representation%20is%20a%20foundational%20element%20in%20our%20understanding%20of%0Athe%20physical%20world.%20Its%20importance%20ranges%20from%20the%20fundamentals%20of%20chemical%0Areactions%20to%20the%20design%20of%20new%20therapies%20and%20materials.%20Previous%20molecular%0Amachine%20learning%20models%20have%20employed%20strings%2C%20fingerprints%2C%20global%20features%2C%0Aand%20simple%20molecular%20graphs%20that%20are%20inherently%20information-sparse%0Arepresentations.%20However%2C%20as%20the%20complexity%20of%20prediction%20tasks%20increases%2C%20the%0Amolecular%20representation%20needs%20to%20encode%20higher%20fidelity%20information.%20This%20work%0Aintroduces%20a%20novel%20approach%20to%20infusing%20quantum-chemical-rich%20information%20into%0Amolecular%20graphs%20via%20stereoelectronic%20effects.%20We%20show%20that%20the%20explicit%0Aaddition%20of%20stereoelectronic%20interactions%20significantly%20improves%20the%0Aperformance%20of%20molecular%20machine%20learning%20models.%20Furthermore%2C%0Astereoelectronics-infused%20representations%20can%20be%20learned%20and%20deployed%20with%20a%0Atailored%20double%20graph%20neural%20network%20workflow%2C%20enabling%20its%20application%20to%20any%0Adownstream%20molecular%20machine%20learning%20task.%20Finally%2C%20we%20show%20that%20the%20learned%0Arepresentations%20allow%20for%20facile%20stereoelectronic%20evaluation%20of%20previously%0Aintractable%20systems%2C%20such%20as%20entire%20proteins%2C%20opening%20new%20avenues%20of%20molecular%0Adesign.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04520v1&entry.124074799=Read"},
{"title": "Connecting Permutation Equivariant Neural Networks and Partition\n  Diagrams", "author": "Edward Pearce-Crump", "abstract": "  Permutation equivariant neural networks are often constructed using tensor\npowers of $\\mathbb{R}^{n}$ as their layer spaces. We show that all of the\nweight matrices that appear in these neural networks can be obtained from\nSchur-Weyl duality between the symmetric group and the partition algebra. In\nparticular, we adapt Schur-Weyl duality to derive a simple, diagrammatic method\nfor calculating the weight matrices themselves.\n", "link": "http://arxiv.org/abs/2212.08648v3", "date": "2024-08-08", "relevancy": 1.5187, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4011}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3796}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Connecting%20Permutation%20Equivariant%20Neural%20Networks%20and%20Partition%0A%20%20Diagrams&body=Title%3A%20Connecting%20Permutation%20Equivariant%20Neural%20Networks%20and%20Partition%0A%20%20Diagrams%0AAuthor%3A%20Edward%20Pearce-Crump%0AAbstract%3A%20%20%20Permutation%20equivariant%20neural%20networks%20are%20often%20constructed%20using%20tensor%0Apowers%20of%20%24%5Cmathbb%7BR%7D%5E%7Bn%7D%24%20as%20their%20layer%20spaces.%20We%20show%20that%20all%20of%20the%0Aweight%20matrices%20that%20appear%20in%20these%20neural%20networks%20can%20be%20obtained%20from%0ASchur-Weyl%20duality%20between%20the%20symmetric%20group%20and%20the%20partition%20algebra.%20In%0Aparticular%2C%20we%20adapt%20Schur-Weyl%20duality%20to%20derive%20a%20simple%2C%20diagrammatic%20method%0Afor%20calculating%20the%20weight%20matrices%20themselves.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.08648v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConnecting%2520Permutation%2520Equivariant%2520Neural%2520Networks%2520and%2520Partition%250A%2520%2520Diagrams%26entry.906535625%3DEdward%2520Pearce-Crump%26entry.1292438233%3D%2520%2520Permutation%2520equivariant%2520neural%2520networks%2520are%2520often%2520constructed%2520using%2520tensor%250Apowers%2520of%2520%2524%255Cmathbb%257BR%257D%255E%257Bn%257D%2524%2520as%2520their%2520layer%2520spaces.%2520We%2520show%2520that%2520all%2520of%2520the%250Aweight%2520matrices%2520that%2520appear%2520in%2520these%2520neural%2520networks%2520can%2520be%2520obtained%2520from%250ASchur-Weyl%2520duality%2520between%2520the%2520symmetric%2520group%2520and%2520the%2520partition%2520algebra.%2520In%250Aparticular%252C%2520we%2520adapt%2520Schur-Weyl%2520duality%2520to%2520derive%2520a%2520simple%252C%2520diagrammatic%2520method%250Afor%2520calculating%2520the%2520weight%2520matrices%2520themselves.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.08648v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Connecting%20Permutation%20Equivariant%20Neural%20Networks%20and%20Partition%0A%20%20Diagrams&entry.906535625=Edward%20Pearce-Crump&entry.1292438233=%20%20Permutation%20equivariant%20neural%20networks%20are%20often%20constructed%20using%20tensor%0Apowers%20of%20%24%5Cmathbb%7BR%7D%5E%7Bn%7D%24%20as%20their%20layer%20spaces.%20We%20show%20that%20all%20of%20the%0Aweight%20matrices%20that%20appear%20in%20these%20neural%20networks%20can%20be%20obtained%20from%0ASchur-Weyl%20duality%20between%20the%20symmetric%20group%20and%20the%20partition%20algebra.%20In%0Aparticular%2C%20we%20adapt%20Schur-Weyl%20duality%20to%20derive%20a%20simple%2C%20diagrammatic%20method%0Afor%20calculating%20the%20weight%20matrices%20themselves.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.08648v3&entry.124074799=Read"},
{"title": "MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image\n  Segmentation", "author": "Jing Xu", "abstract": "  Chest X-ray is one of the most common radiological examination types for the\ndiagnosis of chest diseases. Nowadays, the automatic classification technology\nof radiological images has been widely used in clinical diagnosis and treatment\nplans. However, each disease has its own different response characteristic\nreceptive field region, which is the main challenge for chest disease\nclassification tasks. Besides, the imbalance of sample data categories further\nincreases the difficulty of tasks. To solve these problems, we propose a new\nmulti-label chest disease image classification scheme based on a multi-scale\nattention network. In this scheme, multi-scale information is iteratively fused\nto focus on regions with a high probability of disease, to effectively mine\nmore meaningful information from data, and the classification performance can\nbe improved only by image level annotation. We also designed a new loss\nfunction to improve the rationality of visual perception and the performance of\nmulti-label image classification by forcing the consistency of attention\nregions before and after image transformation. A comprehensive experiment was\ncarried out on the public Chest X-Ray14 and CheXpert datasets to achieve state\nof the art results, which verified the effectiveness of this method in chest\nX-ray image classification.\n", "link": "http://arxiv.org/abs/2312.07128v2", "date": "2024-08-08", "relevancy": 1.5006, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5154}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4816}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MS-Twins%3A%20Multi-Scale%20Deep%20Self-Attention%20Networks%20for%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20MS-Twins%3A%20Multi-Scale%20Deep%20Self-Attention%20Networks%20for%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Jing%20Xu%0AAbstract%3A%20%20%20Chest%20X-ray%20is%20one%20of%20the%20most%20common%20radiological%20examination%20types%20for%20the%0Adiagnosis%20of%20chest%20diseases.%20Nowadays%2C%20the%20automatic%20classification%20technology%0Aof%20radiological%20images%20has%20been%20widely%20used%20in%20clinical%20diagnosis%20and%20treatment%0Aplans.%20However%2C%20each%20disease%20has%20its%20own%20different%20response%20characteristic%0Areceptive%20field%20region%2C%20which%20is%20the%20main%20challenge%20for%20chest%20disease%0Aclassification%20tasks.%20Besides%2C%20the%20imbalance%20of%20sample%20data%20categories%20further%0Aincreases%20the%20difficulty%20of%20tasks.%20To%20solve%20these%20problems%2C%20we%20propose%20a%20new%0Amulti-label%20chest%20disease%20image%20classification%20scheme%20based%20on%20a%20multi-scale%0Aattention%20network.%20In%20this%20scheme%2C%20multi-scale%20information%20is%20iteratively%20fused%0Ato%20focus%20on%20regions%20with%20a%20high%20probability%20of%20disease%2C%20to%20effectively%20mine%0Amore%20meaningful%20information%20from%20data%2C%20and%20the%20classification%20performance%20can%0Abe%20improved%20only%20by%20image%20level%20annotation.%20We%20also%20designed%20a%20new%20loss%0Afunction%20to%20improve%20the%20rationality%20of%20visual%20perception%20and%20the%20performance%20of%0Amulti-label%20image%20classification%20by%20forcing%20the%20consistency%20of%20attention%0Aregions%20before%20and%20after%20image%20transformation.%20A%20comprehensive%20experiment%20was%0Acarried%20out%20on%20the%20public%20Chest%20X-Ray14%20and%20CheXpert%20datasets%20to%20achieve%20state%0Aof%20the%20art%20results%2C%20which%20verified%20the%20effectiveness%20of%20this%20method%20in%20chest%0AX-ray%20image%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07128v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMS-Twins%253A%2520Multi-Scale%2520Deep%2520Self-Attention%2520Networks%2520for%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DJing%2520Xu%26entry.1292438233%3D%2520%2520Chest%2520X-ray%2520is%2520one%2520of%2520the%2520most%2520common%2520radiological%2520examination%2520types%2520for%2520the%250Adiagnosis%2520of%2520chest%2520diseases.%2520Nowadays%252C%2520the%2520automatic%2520classification%2520technology%250Aof%2520radiological%2520images%2520has%2520been%2520widely%2520used%2520in%2520clinical%2520diagnosis%2520and%2520treatment%250Aplans.%2520However%252C%2520each%2520disease%2520has%2520its%2520own%2520different%2520response%2520characteristic%250Areceptive%2520field%2520region%252C%2520which%2520is%2520the%2520main%2520challenge%2520for%2520chest%2520disease%250Aclassification%2520tasks.%2520Besides%252C%2520the%2520imbalance%2520of%2520sample%2520data%2520categories%2520further%250Aincreases%2520the%2520difficulty%2520of%2520tasks.%2520To%2520solve%2520these%2520problems%252C%2520we%2520propose%2520a%2520new%250Amulti-label%2520chest%2520disease%2520image%2520classification%2520scheme%2520based%2520on%2520a%2520multi-scale%250Aattention%2520network.%2520In%2520this%2520scheme%252C%2520multi-scale%2520information%2520is%2520iteratively%2520fused%250Ato%2520focus%2520on%2520regions%2520with%2520a%2520high%2520probability%2520of%2520disease%252C%2520to%2520effectively%2520mine%250Amore%2520meaningful%2520information%2520from%2520data%252C%2520and%2520the%2520classification%2520performance%2520can%250Abe%2520improved%2520only%2520by%2520image%2520level%2520annotation.%2520We%2520also%2520designed%2520a%2520new%2520loss%250Afunction%2520to%2520improve%2520the%2520rationality%2520of%2520visual%2520perception%2520and%2520the%2520performance%2520of%250Amulti-label%2520image%2520classification%2520by%2520forcing%2520the%2520consistency%2520of%2520attention%250Aregions%2520before%2520and%2520after%2520image%2520transformation.%2520A%2520comprehensive%2520experiment%2520was%250Acarried%2520out%2520on%2520the%2520public%2520Chest%2520X-Ray14%2520and%2520CheXpert%2520datasets%2520to%2520achieve%2520state%250Aof%2520the%2520art%2520results%252C%2520which%2520verified%2520the%2520effectiveness%2520of%2520this%2520method%2520in%2520chest%250AX-ray%2520image%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07128v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MS-Twins%3A%20Multi-Scale%20Deep%20Self-Attention%20Networks%20for%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Jing%20Xu&entry.1292438233=%20%20Chest%20X-ray%20is%20one%20of%20the%20most%20common%20radiological%20examination%20types%20for%20the%0Adiagnosis%20of%20chest%20diseases.%20Nowadays%2C%20the%20automatic%20classification%20technology%0Aof%20radiological%20images%20has%20been%20widely%20used%20in%20clinical%20diagnosis%20and%20treatment%0Aplans.%20However%2C%20each%20disease%20has%20its%20own%20different%20response%20characteristic%0Areceptive%20field%20region%2C%20which%20is%20the%20main%20challenge%20for%20chest%20disease%0Aclassification%20tasks.%20Besides%2C%20the%20imbalance%20of%20sample%20data%20categories%20further%0Aincreases%20the%20difficulty%20of%20tasks.%20To%20solve%20these%20problems%2C%20we%20propose%20a%20new%0Amulti-label%20chest%20disease%20image%20classification%20scheme%20based%20on%20a%20multi-scale%0Aattention%20network.%20In%20this%20scheme%2C%20multi-scale%20information%20is%20iteratively%20fused%0Ato%20focus%20on%20regions%20with%20a%20high%20probability%20of%20disease%2C%20to%20effectively%20mine%0Amore%20meaningful%20information%20from%20data%2C%20and%20the%20classification%20performance%20can%0Abe%20improved%20only%20by%20image%20level%20annotation.%20We%20also%20designed%20a%20new%20loss%0Afunction%20to%20improve%20the%20rationality%20of%20visual%20perception%20and%20the%20performance%20of%0Amulti-label%20image%20classification%20by%20forcing%20the%20consistency%20of%20attention%0Aregions%20before%20and%20after%20image%20transformation.%20A%20comprehensive%20experiment%20was%0Acarried%20out%20on%20the%20public%20Chest%20X-Ray14%20and%20CheXpert%20datasets%20to%20achieve%20state%0Aof%20the%20art%20results%2C%20which%20verified%20the%20effectiveness%20of%20this%20method%20in%20chest%0AX-ray%20image%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07128v2&entry.124074799=Read"},
{"title": "Arctic-TILT. Business Document Understanding at Sub-Billion Scale", "author": "\u0141ukasz Borchmann and Micha\u0142 Pietruszka and Wojciech Ja\u015bkowski and Dawid Jurkiewicz and Piotr Halama and Pawe\u0142 J\u00f3ziak and \u0141ukasz Garncarek and Pawe\u0142 Liskowski and Karolina Szyndler and Andrzej Gretkowski and Julita O\u0142tusek and Gabriela Nowakowska and Artur Zaw\u0142ocki and \u0141ukasz Duhr and Pawe\u0142 Dyda and Micha\u0142 Turski", "abstract": "  The vast portion of workloads employing LLMs involves answering questions\ngrounded on PDF or scan content. We introduce the Arctic-TILT achieving\naccuracy on par with models 1000$\\times$ its size on these use cases. It can be\nfine-tuned and deployed on a single 24GB GPU, lowering operational costs while\nprocessing Visually Rich Documents with up to 400k tokens. The model\nestablishes state-of-the-art results on seven diverse Document Understanding\nbenchmarks, as well as provides reliable confidence scores and quick inference,\nwhich are essential for processing files in large-scale or time-sensitive\nenterprise environments.\n", "link": "http://arxiv.org/abs/2408.04632v1", "date": "2024-08-08", "relevancy": 1.4901, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5054}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5017}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Arctic-TILT.%20Business%20Document%20Understanding%20at%20Sub-Billion%20Scale&body=Title%3A%20Arctic-TILT.%20Business%20Document%20Understanding%20at%20Sub-Billion%20Scale%0AAuthor%3A%20%C5%81ukasz%20Borchmann%20and%20Micha%C5%82%20Pietruszka%20and%20Wojciech%20Ja%C5%9Bkowski%20and%20Dawid%20Jurkiewicz%20and%20Piotr%20Halama%20and%20Pawe%C5%82%20J%C3%B3ziak%20and%20%C5%81ukasz%20Garncarek%20and%20Pawe%C5%82%20Liskowski%20and%20Karolina%20Szyndler%20and%20Andrzej%20Gretkowski%20and%20Julita%20O%C5%82tusek%20and%20Gabriela%20Nowakowska%20and%20Artur%20Zaw%C5%82ocki%20and%20%C5%81ukasz%20Duhr%20and%20Pawe%C5%82%20Dyda%20and%20Micha%C5%82%20Turski%0AAbstract%3A%20%20%20The%20vast%20portion%20of%20workloads%20employing%20LLMs%20involves%20answering%20questions%0Agrounded%20on%20PDF%20or%20scan%20content.%20We%20introduce%20the%20Arctic-TILT%20achieving%0Aaccuracy%20on%20par%20with%20models%201000%24%5Ctimes%24%20its%20size%20on%20these%20use%20cases.%20It%20can%20be%0Afine-tuned%20and%20deployed%20on%20a%20single%2024GB%20GPU%2C%20lowering%20operational%20costs%20while%0Aprocessing%20Visually%20Rich%20Documents%20with%20up%20to%20400k%20tokens.%20The%20model%0Aestablishes%20state-of-the-art%20results%20on%20seven%20diverse%20Document%20Understanding%0Abenchmarks%2C%20as%20well%20as%20provides%20reliable%20confidence%20scores%20and%20quick%20inference%2C%0Awhich%20are%20essential%20for%20processing%20files%20in%20large-scale%20or%20time-sensitive%0Aenterprise%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArctic-TILT.%2520Business%2520Document%2520Understanding%2520at%2520Sub-Billion%2520Scale%26entry.906535625%3D%25C5%2581ukasz%2520Borchmann%2520and%2520Micha%25C5%2582%2520Pietruszka%2520and%2520Wojciech%2520Ja%25C5%259Bkowski%2520and%2520Dawid%2520Jurkiewicz%2520and%2520Piotr%2520Halama%2520and%2520Pawe%25C5%2582%2520J%25C3%25B3ziak%2520and%2520%25C5%2581ukasz%2520Garncarek%2520and%2520Pawe%25C5%2582%2520Liskowski%2520and%2520Karolina%2520Szyndler%2520and%2520Andrzej%2520Gretkowski%2520and%2520Julita%2520O%25C5%2582tusek%2520and%2520Gabriela%2520Nowakowska%2520and%2520Artur%2520Zaw%25C5%2582ocki%2520and%2520%25C5%2581ukasz%2520Duhr%2520and%2520Pawe%25C5%2582%2520Dyda%2520and%2520Micha%25C5%2582%2520Turski%26entry.1292438233%3D%2520%2520The%2520vast%2520portion%2520of%2520workloads%2520employing%2520LLMs%2520involves%2520answering%2520questions%250Agrounded%2520on%2520PDF%2520or%2520scan%2520content.%2520We%2520introduce%2520the%2520Arctic-TILT%2520achieving%250Aaccuracy%2520on%2520par%2520with%2520models%25201000%2524%255Ctimes%2524%2520its%2520size%2520on%2520these%2520use%2520cases.%2520It%2520can%2520be%250Afine-tuned%2520and%2520deployed%2520on%2520a%2520single%252024GB%2520GPU%252C%2520lowering%2520operational%2520costs%2520while%250Aprocessing%2520Visually%2520Rich%2520Documents%2520with%2520up%2520to%2520400k%2520tokens.%2520The%2520model%250Aestablishes%2520state-of-the-art%2520results%2520on%2520seven%2520diverse%2520Document%2520Understanding%250Abenchmarks%252C%2520as%2520well%2520as%2520provides%2520reliable%2520confidence%2520scores%2520and%2520quick%2520inference%252C%250Awhich%2520are%2520essential%2520for%2520processing%2520files%2520in%2520large-scale%2520or%2520time-sensitive%250Aenterprise%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arctic-TILT.%20Business%20Document%20Understanding%20at%20Sub-Billion%20Scale&entry.906535625=%C5%81ukasz%20Borchmann%20and%20Micha%C5%82%20Pietruszka%20and%20Wojciech%20Ja%C5%9Bkowski%20and%20Dawid%20Jurkiewicz%20and%20Piotr%20Halama%20and%20Pawe%C5%82%20J%C3%B3ziak%20and%20%C5%81ukasz%20Garncarek%20and%20Pawe%C5%82%20Liskowski%20and%20Karolina%20Szyndler%20and%20Andrzej%20Gretkowski%20and%20Julita%20O%C5%82tusek%20and%20Gabriela%20Nowakowska%20and%20Artur%20Zaw%C5%82ocki%20and%20%C5%81ukasz%20Duhr%20and%20Pawe%C5%82%20Dyda%20and%20Micha%C5%82%20Turski&entry.1292438233=%20%20The%20vast%20portion%20of%20workloads%20employing%20LLMs%20involves%20answering%20questions%0Agrounded%20on%20PDF%20or%20scan%20content.%20We%20introduce%20the%20Arctic-TILT%20achieving%0Aaccuracy%20on%20par%20with%20models%201000%24%5Ctimes%24%20its%20size%20on%20these%20use%20cases.%20It%20can%20be%0Afine-tuned%20and%20deployed%20on%20a%20single%2024GB%20GPU%2C%20lowering%20operational%20costs%20while%0Aprocessing%20Visually%20Rich%20Documents%20with%20up%20to%20400k%20tokens.%20The%20model%0Aestablishes%20state-of-the-art%20results%20on%20seven%20diverse%20Document%20Understanding%0Abenchmarks%2C%20as%20well%20as%20provides%20reliable%20confidence%20scores%20and%20quick%20inference%2C%0Awhich%20are%20essential%20for%20processing%20files%20in%20large-scale%20or%20time-sensitive%0Aenterprise%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04632v1&entry.124074799=Read"},
{"title": "RL-ADN: A High-Performance Deep Reinforcement Learning Environment for\n  Optimal Energy Storage Systems Dispatch in Active Distribution Networks", "author": "Shengren Hou and Shuyi Gao and Weijie Xia and Edgar Mauricio Salazar Duque and Peter Palensky and Pedro P. Vergara", "abstract": "  Deep Reinforcement Learning (DRL) presents a promising avenue for optimizing\nEnergy Storage Systems (ESSs) dispatch in distribution networks. This paper\nintroduces RL-ADN, an innovative open-source library specifically designed for\nsolving the optimal ESSs dispatch in active distribution networks. RL-ADN\noffers unparalleled flexibility in modeling distribution networks, and ESSs,\naccommodating a wide range of research goals. A standout feature of RL-ADN is\nits data augmentation module, based on Gaussian Mixture Model and Copula (GMC)\nfunctions, which elevates the performance ceiling of DRL agents. Additionally,\nRL-ADN incorporates the Laurent power flow solver, significantly reducing the\ncomputational burden of power flow calculations during training without\nsacrificing accuracy. The effectiveness of RL-ADN is demonstrated using in\ndifferent sizes of distribution networks, showing marked performance\nimprovements in the adaptability of DRL algorithms for ESS dispatch tasks. This\nenhancement is particularly beneficial from the increased diversity of training\nscenarios. Furthermore, RL-ADN achieves a tenfold increase in computational\nefficiency during training, making it highly suitable for large-scale network\napplications. The library sets a new benchmark in DRL-based ESSs dispatch in\ndistribution networks and it is poised to advance DRL applications in\ndistribution network operations significantly. RL-ADN is available at:\nhttps://github.com/ShengrenHou/RL-ADN and\nhttps://github.com/distributionnetworksTUDelft/RL-ADN.\n", "link": "http://arxiv.org/abs/2408.03685v2", "date": "2024-08-08", "relevancy": 1.4901, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5224}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4649}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-ADN%3A%20A%20High-Performance%20Deep%20Reinforcement%20Learning%20Environment%20for%0A%20%20Optimal%20Energy%20Storage%20Systems%20Dispatch%20in%20Active%20Distribution%20Networks&body=Title%3A%20RL-ADN%3A%20A%20High-Performance%20Deep%20Reinforcement%20Learning%20Environment%20for%0A%20%20Optimal%20Energy%20Storage%20Systems%20Dispatch%20in%20Active%20Distribution%20Networks%0AAuthor%3A%20Shengren%20Hou%20and%20Shuyi%20Gao%20and%20Weijie%20Xia%20and%20Edgar%20Mauricio%20Salazar%20Duque%20and%20Peter%20Palensky%20and%20Pedro%20P.%20Vergara%0AAbstract%3A%20%20%20Deep%20Reinforcement%20Learning%20%28DRL%29%20presents%20a%20promising%20avenue%20for%20optimizing%0AEnergy%20Storage%20Systems%20%28ESSs%29%20dispatch%20in%20distribution%20networks.%20This%20paper%0Aintroduces%20RL-ADN%2C%20an%20innovative%20open-source%20library%20specifically%20designed%20for%0Asolving%20the%20optimal%20ESSs%20dispatch%20in%20active%20distribution%20networks.%20RL-ADN%0Aoffers%20unparalleled%20flexibility%20in%20modeling%20distribution%20networks%2C%20and%20ESSs%2C%0Aaccommodating%20a%20wide%20range%20of%20research%20goals.%20A%20standout%20feature%20of%20RL-ADN%20is%0Aits%20data%20augmentation%20module%2C%20based%20on%20Gaussian%20Mixture%20Model%20and%20Copula%20%28GMC%29%0Afunctions%2C%20which%20elevates%20the%20performance%20ceiling%20of%20DRL%20agents.%20Additionally%2C%0ARL-ADN%20incorporates%20the%20Laurent%20power%20flow%20solver%2C%20significantly%20reducing%20the%0Acomputational%20burden%20of%20power%20flow%20calculations%20during%20training%20without%0Asacrificing%20accuracy.%20The%20effectiveness%20of%20RL-ADN%20is%20demonstrated%20using%20in%0Adifferent%20sizes%20of%20distribution%20networks%2C%20showing%20marked%20performance%0Aimprovements%20in%20the%20adaptability%20of%20DRL%20algorithms%20for%20ESS%20dispatch%20tasks.%20This%0Aenhancement%20is%20particularly%20beneficial%20from%20the%20increased%20diversity%20of%20training%0Ascenarios.%20Furthermore%2C%20RL-ADN%20achieves%20a%20tenfold%20increase%20in%20computational%0Aefficiency%20during%20training%2C%20making%20it%20highly%20suitable%20for%20large-scale%20network%0Aapplications.%20The%20library%20sets%20a%20new%20benchmark%20in%20DRL-based%20ESSs%20dispatch%20in%0Adistribution%20networks%20and%20it%20is%20poised%20to%20advance%20DRL%20applications%20in%0Adistribution%20network%20operations%20significantly.%20RL-ADN%20is%20available%20at%3A%0Ahttps%3A//github.com/ShengrenHou/RL-ADN%20and%0Ahttps%3A//github.com/distributionnetworksTUDelft/RL-ADN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03685v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-ADN%253A%2520A%2520High-Performance%2520Deep%2520Reinforcement%2520Learning%2520Environment%2520for%250A%2520%2520Optimal%2520Energy%2520Storage%2520Systems%2520Dispatch%2520in%2520Active%2520Distribution%2520Networks%26entry.906535625%3DShengren%2520Hou%2520and%2520Shuyi%2520Gao%2520and%2520Weijie%2520Xia%2520and%2520Edgar%2520Mauricio%2520Salazar%2520Duque%2520and%2520Peter%2520Palensky%2520and%2520Pedro%2520P.%2520Vergara%26entry.1292438233%3D%2520%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520presents%2520a%2520promising%2520avenue%2520for%2520optimizing%250AEnergy%2520Storage%2520Systems%2520%2528ESSs%2529%2520dispatch%2520in%2520distribution%2520networks.%2520This%2520paper%250Aintroduces%2520RL-ADN%252C%2520an%2520innovative%2520open-source%2520library%2520specifically%2520designed%2520for%250Asolving%2520the%2520optimal%2520ESSs%2520dispatch%2520in%2520active%2520distribution%2520networks.%2520RL-ADN%250Aoffers%2520unparalleled%2520flexibility%2520in%2520modeling%2520distribution%2520networks%252C%2520and%2520ESSs%252C%250Aaccommodating%2520a%2520wide%2520range%2520of%2520research%2520goals.%2520A%2520standout%2520feature%2520of%2520RL-ADN%2520is%250Aits%2520data%2520augmentation%2520module%252C%2520based%2520on%2520Gaussian%2520Mixture%2520Model%2520and%2520Copula%2520%2528GMC%2529%250Afunctions%252C%2520which%2520elevates%2520the%2520performance%2520ceiling%2520of%2520DRL%2520agents.%2520Additionally%252C%250ARL-ADN%2520incorporates%2520the%2520Laurent%2520power%2520flow%2520solver%252C%2520significantly%2520reducing%2520the%250Acomputational%2520burden%2520of%2520power%2520flow%2520calculations%2520during%2520training%2520without%250Asacrificing%2520accuracy.%2520The%2520effectiveness%2520of%2520RL-ADN%2520is%2520demonstrated%2520using%2520in%250Adifferent%2520sizes%2520of%2520distribution%2520networks%252C%2520showing%2520marked%2520performance%250Aimprovements%2520in%2520the%2520adaptability%2520of%2520DRL%2520algorithms%2520for%2520ESS%2520dispatch%2520tasks.%2520This%250Aenhancement%2520is%2520particularly%2520beneficial%2520from%2520the%2520increased%2520diversity%2520of%2520training%250Ascenarios.%2520Furthermore%252C%2520RL-ADN%2520achieves%2520a%2520tenfold%2520increase%2520in%2520computational%250Aefficiency%2520during%2520training%252C%2520making%2520it%2520highly%2520suitable%2520for%2520large-scale%2520network%250Aapplications.%2520The%2520library%2520sets%2520a%2520new%2520benchmark%2520in%2520DRL-based%2520ESSs%2520dispatch%2520in%250Adistribution%2520networks%2520and%2520it%2520is%2520poised%2520to%2520advance%2520DRL%2520applications%2520in%250Adistribution%2520network%2520operations%2520significantly.%2520RL-ADN%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ShengrenHou/RL-ADN%2520and%250Ahttps%253A//github.com/distributionnetworksTUDelft/RL-ADN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03685v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-ADN%3A%20A%20High-Performance%20Deep%20Reinforcement%20Learning%20Environment%20for%0A%20%20Optimal%20Energy%20Storage%20Systems%20Dispatch%20in%20Active%20Distribution%20Networks&entry.906535625=Shengren%20Hou%20and%20Shuyi%20Gao%20and%20Weijie%20Xia%20and%20Edgar%20Mauricio%20Salazar%20Duque%20and%20Peter%20Palensky%20and%20Pedro%20P.%20Vergara&entry.1292438233=%20%20Deep%20Reinforcement%20Learning%20%28DRL%29%20presents%20a%20promising%20avenue%20for%20optimizing%0AEnergy%20Storage%20Systems%20%28ESSs%29%20dispatch%20in%20distribution%20networks.%20This%20paper%0Aintroduces%20RL-ADN%2C%20an%20innovative%20open-source%20library%20specifically%20designed%20for%0Asolving%20the%20optimal%20ESSs%20dispatch%20in%20active%20distribution%20networks.%20RL-ADN%0Aoffers%20unparalleled%20flexibility%20in%20modeling%20distribution%20networks%2C%20and%20ESSs%2C%0Aaccommodating%20a%20wide%20range%20of%20research%20goals.%20A%20standout%20feature%20of%20RL-ADN%20is%0Aits%20data%20augmentation%20module%2C%20based%20on%20Gaussian%20Mixture%20Model%20and%20Copula%20%28GMC%29%0Afunctions%2C%20which%20elevates%20the%20performance%20ceiling%20of%20DRL%20agents.%20Additionally%2C%0ARL-ADN%20incorporates%20the%20Laurent%20power%20flow%20solver%2C%20significantly%20reducing%20the%0Acomputational%20burden%20of%20power%20flow%20calculations%20during%20training%20without%0Asacrificing%20accuracy.%20The%20effectiveness%20of%20RL-ADN%20is%20demonstrated%20using%20in%0Adifferent%20sizes%20of%20distribution%20networks%2C%20showing%20marked%20performance%0Aimprovements%20in%20the%20adaptability%20of%20DRL%20algorithms%20for%20ESS%20dispatch%20tasks.%20This%0Aenhancement%20is%20particularly%20beneficial%20from%20the%20increased%20diversity%20of%20training%0Ascenarios.%20Furthermore%2C%20RL-ADN%20achieves%20a%20tenfold%20increase%20in%20computational%0Aefficiency%20during%20training%2C%20making%20it%20highly%20suitable%20for%20large-scale%20network%0Aapplications.%20The%20library%20sets%20a%20new%20benchmark%20in%20DRL-based%20ESSs%20dispatch%20in%0Adistribution%20networks%20and%20it%20is%20poised%20to%20advance%20DRL%20applications%20in%0Adistribution%20network%20operations%20significantly.%20RL-ADN%20is%20available%20at%3A%0Ahttps%3A//github.com/ShengrenHou/RL-ADN%20and%0Ahttps%3A//github.com/distributionnetworksTUDelft/RL-ADN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03685v2&entry.124074799=Read"},
{"title": "Duwak: Dual Watermarks in Large Language Models", "author": "Chaoyi Zhu and Jeroen Galjaard and Pin-Yu Chen and Lydia Y. Chen", "abstract": "  As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.\n", "link": "http://arxiv.org/abs/2403.13000v2", "date": "2024-08-08", "relevancy": 1.4844, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5061}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5046}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Duwak%3A%20Dual%20Watermarks%20in%20Large%20Language%20Models&body=Title%3A%20Duwak%3A%20Dual%20Watermarks%20in%20Large%20Language%20Models%0AAuthor%3A%20Chaoyi%20Zhu%20and%20Jeroen%20Galjaard%20and%20Pin-Yu%20Chen%20and%20Lydia%20Y.%20Chen%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLM%29%20are%20increasingly%20used%20for%20text%20generation%0Atasks%2C%20it%20is%20critical%20to%20audit%20their%20usages%2C%20govern%20their%20applications%2C%20and%0Amitigate%20their%20potential%20harms.%20Existing%20watermark%20techniques%20are%20shown%0Aeffective%20in%20embedding%20single%20human-imperceptible%20and%20machine-detectable%0Apatterns%20without%20significantly%20affecting%20generated%20text%20quality%20and%20semantics.%0AHowever%2C%20the%20efficiency%20in%20detecting%20watermarks%2C%20i.e.%2C%20the%20minimum%20number%20of%0Atokens%20required%20to%20assert%20detection%20with%20significance%20and%20robustness%20against%0Apost-editing%2C%20is%20still%20debatable.%20In%20this%20paper%2C%20we%20propose%2C%20Duwak%2C%20to%0Afundamentally%20enhance%20the%20efficiency%20and%20quality%20of%20watermarking%20by%20embedding%0Adual%20secret%20patterns%20in%20both%20token%20probability%20distribution%20and%20sampling%0Aschemes.%20To%20mitigate%20expression%20degradation%20caused%20by%20biasing%20toward%20certain%0Atokens%2C%20we%20design%20a%20contrastive%20search%20to%20watermark%20the%20sampling%20scheme%2C%20which%0Aminimizes%20the%20token%20repetition%20and%20enhances%20the%20diversity.%20We%20theoretically%0Aexplain%20the%20interdependency%20of%20the%20two%20watermarks%20within%20Duwak.%20We%20evaluate%0ADuwak%20extensively%20on%20Llama2%20under%20various%20post-editing%20attacks%2C%20against%20four%0Astate-of-the-art%20watermarking%20techniques%20and%20combinations%20of%20them.%20Our%20results%0Ashow%20that%20Duwak%20marked%20text%20achieves%20the%20highest%20watermarked%20text%20quality%20at%0Athe%20lowest%20required%20token%20count%20for%20detection%2C%20up%20to%2070%25%20tokens%20less%20than%0Aexisting%20approaches%2C%20especially%20under%20post%20paraphrasing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13000v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuwak%253A%2520Dual%2520Watermarks%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DChaoyi%2520Zhu%2520and%2520Jeroen%2520Galjaard%2520and%2520Pin-Yu%2520Chen%2520and%2520Lydia%2520Y.%2520Chen%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLM%2529%2520are%2520increasingly%2520used%2520for%2520text%2520generation%250Atasks%252C%2520it%2520is%2520critical%2520to%2520audit%2520their%2520usages%252C%2520govern%2520their%2520applications%252C%2520and%250Amitigate%2520their%2520potential%2520harms.%2520Existing%2520watermark%2520techniques%2520are%2520shown%250Aeffective%2520in%2520embedding%2520single%2520human-imperceptible%2520and%2520machine-detectable%250Apatterns%2520without%2520significantly%2520affecting%2520generated%2520text%2520quality%2520and%2520semantics.%250AHowever%252C%2520the%2520efficiency%2520in%2520detecting%2520watermarks%252C%2520i.e.%252C%2520the%2520minimum%2520number%2520of%250Atokens%2520required%2520to%2520assert%2520detection%2520with%2520significance%2520and%2520robustness%2520against%250Apost-editing%252C%2520is%2520still%2520debatable.%2520In%2520this%2520paper%252C%2520we%2520propose%252C%2520Duwak%252C%2520to%250Afundamentally%2520enhance%2520the%2520efficiency%2520and%2520quality%2520of%2520watermarking%2520by%2520embedding%250Adual%2520secret%2520patterns%2520in%2520both%2520token%2520probability%2520distribution%2520and%2520sampling%250Aschemes.%2520To%2520mitigate%2520expression%2520degradation%2520caused%2520by%2520biasing%2520toward%2520certain%250Atokens%252C%2520we%2520design%2520a%2520contrastive%2520search%2520to%2520watermark%2520the%2520sampling%2520scheme%252C%2520which%250Aminimizes%2520the%2520token%2520repetition%2520and%2520enhances%2520the%2520diversity.%2520We%2520theoretically%250Aexplain%2520the%2520interdependency%2520of%2520the%2520two%2520watermarks%2520within%2520Duwak.%2520We%2520evaluate%250ADuwak%2520extensively%2520on%2520Llama2%2520under%2520various%2520post-editing%2520attacks%252C%2520against%2520four%250Astate-of-the-art%2520watermarking%2520techniques%2520and%2520combinations%2520of%2520them.%2520Our%2520results%250Ashow%2520that%2520Duwak%2520marked%2520text%2520achieves%2520the%2520highest%2520watermarked%2520text%2520quality%2520at%250Athe%2520lowest%2520required%2520token%2520count%2520for%2520detection%252C%2520up%2520to%252070%2525%2520tokens%2520less%2520than%250Aexisting%2520approaches%252C%2520especially%2520under%2520post%2520paraphrasing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13000v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Duwak%3A%20Dual%20Watermarks%20in%20Large%20Language%20Models&entry.906535625=Chaoyi%20Zhu%20and%20Jeroen%20Galjaard%20and%20Pin-Yu%20Chen%20and%20Lydia%20Y.%20Chen&entry.1292438233=%20%20As%20large%20language%20models%20%28LLM%29%20are%20increasingly%20used%20for%20text%20generation%0Atasks%2C%20it%20is%20critical%20to%20audit%20their%20usages%2C%20govern%20their%20applications%2C%20and%0Amitigate%20their%20potential%20harms.%20Existing%20watermark%20techniques%20are%20shown%0Aeffective%20in%20embedding%20single%20human-imperceptible%20and%20machine-detectable%0Apatterns%20without%20significantly%20affecting%20generated%20text%20quality%20and%20semantics.%0AHowever%2C%20the%20efficiency%20in%20detecting%20watermarks%2C%20i.e.%2C%20the%20minimum%20number%20of%0Atokens%20required%20to%20assert%20detection%20with%20significance%20and%20robustness%20against%0Apost-editing%2C%20is%20still%20debatable.%20In%20this%20paper%2C%20we%20propose%2C%20Duwak%2C%20to%0Afundamentally%20enhance%20the%20efficiency%20and%20quality%20of%20watermarking%20by%20embedding%0Adual%20secret%20patterns%20in%20both%20token%20probability%20distribution%20and%20sampling%0Aschemes.%20To%20mitigate%20expression%20degradation%20caused%20by%20biasing%20toward%20certain%0Atokens%2C%20we%20design%20a%20contrastive%20search%20to%20watermark%20the%20sampling%20scheme%2C%20which%0Aminimizes%20the%20token%20repetition%20and%20enhances%20the%20diversity.%20We%20theoretically%0Aexplain%20the%20interdependency%20of%20the%20two%20watermarks%20within%20Duwak.%20We%20evaluate%0ADuwak%20extensively%20on%20Llama2%20under%20various%20post-editing%20attacks%2C%20against%20four%0Astate-of-the-art%20watermarking%20techniques%20and%20combinations%20of%20them.%20Our%20results%0Ashow%20that%20Duwak%20marked%20text%20achieves%20the%20highest%20watermarked%20text%20quality%20at%0Athe%20lowest%20required%20token%20count%20for%20detection%2C%20up%20to%2070%25%20tokens%20less%20than%0Aexisting%20approaches%2C%20especially%20under%20post%20paraphrasing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13000v2&entry.124074799=Read"},
{"title": "Knowledge-Aided Semantic Communication Leveraging Probabilistic\n  Graphical Modeling", "author": "Haowen Wan and Qianqian Yang and Jiancheng Tang and Zhiguo shi", "abstract": "  In this paper, we propose a semantic communication approach based on\nprobabilistic graphical model (PGM). The proposed approach involves\nconstructing a PGM from a training dataset, which is then shared as common\nknowledge between the transmitter and receiver. We evaluate the importance of\nvarious semantic features and present a PGM-based compression algorithm\ndesigned to eliminate predictable portions of semantic information.\nFurthermore, we introduce a technique to reconstruct the discarded semantic\ninformation at the receiver end, generating approximate results based on the\nPGM. Simulation results indicate a significant improvement in transmission\nefficiency over existing methods, while maintaining the quality of the\ntransmitted images.\n", "link": "http://arxiv.org/abs/2408.04499v1", "date": "2024-08-08", "relevancy": 1.4758, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5399}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4814}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge-Aided%20Semantic%20Communication%20Leveraging%20Probabilistic%0A%20%20Graphical%20Modeling&body=Title%3A%20Knowledge-Aided%20Semantic%20Communication%20Leveraging%20Probabilistic%0A%20%20Graphical%20Modeling%0AAuthor%3A%20Haowen%20Wan%20and%20Qianqian%20Yang%20and%20Jiancheng%20Tang%20and%20Zhiguo%20shi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20a%20semantic%20communication%20approach%20based%20on%0Aprobabilistic%20graphical%20model%20%28PGM%29.%20The%20proposed%20approach%20involves%0Aconstructing%20a%20PGM%20from%20a%20training%20dataset%2C%20which%20is%20then%20shared%20as%20common%0Aknowledge%20between%20the%20transmitter%20and%20receiver.%20We%20evaluate%20the%20importance%20of%0Avarious%20semantic%20features%20and%20present%20a%20PGM-based%20compression%20algorithm%0Adesigned%20to%20eliminate%20predictable%20portions%20of%20semantic%20information.%0AFurthermore%2C%20we%20introduce%20a%20technique%20to%20reconstruct%20the%20discarded%20semantic%0Ainformation%20at%20the%20receiver%20end%2C%20generating%20approximate%20results%20based%20on%20the%0APGM.%20Simulation%20results%20indicate%20a%20significant%20improvement%20in%20transmission%0Aefficiency%20over%20existing%20methods%2C%20while%20maintaining%20the%20quality%20of%20the%0Atransmitted%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge-Aided%2520Semantic%2520Communication%2520Leveraging%2520Probabilistic%250A%2520%2520Graphical%2520Modeling%26entry.906535625%3DHaowen%2520Wan%2520and%2520Qianqian%2520Yang%2520and%2520Jiancheng%2520Tang%2520and%2520Zhiguo%2520shi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520semantic%2520communication%2520approach%2520based%2520on%250Aprobabilistic%2520graphical%2520model%2520%2528PGM%2529.%2520The%2520proposed%2520approach%2520involves%250Aconstructing%2520a%2520PGM%2520from%2520a%2520training%2520dataset%252C%2520which%2520is%2520then%2520shared%2520as%2520common%250Aknowledge%2520between%2520the%2520transmitter%2520and%2520receiver.%2520We%2520evaluate%2520the%2520importance%2520of%250Avarious%2520semantic%2520features%2520and%2520present%2520a%2520PGM-based%2520compression%2520algorithm%250Adesigned%2520to%2520eliminate%2520predictable%2520portions%2520of%2520semantic%2520information.%250AFurthermore%252C%2520we%2520introduce%2520a%2520technique%2520to%2520reconstruct%2520the%2520discarded%2520semantic%250Ainformation%2520at%2520the%2520receiver%2520end%252C%2520generating%2520approximate%2520results%2520based%2520on%2520the%250APGM.%2520Simulation%2520results%2520indicate%2520a%2520significant%2520improvement%2520in%2520transmission%250Aefficiency%2520over%2520existing%2520methods%252C%2520while%2520maintaining%2520the%2520quality%2520of%2520the%250Atransmitted%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge-Aided%20Semantic%20Communication%20Leveraging%20Probabilistic%0A%20%20Graphical%20Modeling&entry.906535625=Haowen%20Wan%20and%20Qianqian%20Yang%20and%20Jiancheng%20Tang%20and%20Zhiguo%20shi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20a%20semantic%20communication%20approach%20based%20on%0Aprobabilistic%20graphical%20model%20%28PGM%29.%20The%20proposed%20approach%20involves%0Aconstructing%20a%20PGM%20from%20a%20training%20dataset%2C%20which%20is%20then%20shared%20as%20common%0Aknowledge%20between%20the%20transmitter%20and%20receiver.%20We%20evaluate%20the%20importance%20of%0Avarious%20semantic%20features%20and%20present%20a%20PGM-based%20compression%20algorithm%0Adesigned%20to%20eliminate%20predictable%20portions%20of%20semantic%20information.%0AFurthermore%2C%20we%20introduce%20a%20technique%20to%20reconstruct%20the%20discarded%20semantic%0Ainformation%20at%20the%20receiver%20end%2C%20generating%20approximate%20results%20based%20on%20the%0APGM.%20Simulation%20results%20indicate%20a%20significant%20improvement%20in%20transmission%0Aefficiency%20over%20existing%20methods%2C%20while%20maintaining%20the%20quality%20of%20the%0Atransmitted%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04499v1&entry.124074799=Read"},
{"title": "SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses", "author": "Chaolei Tan and Zihang Lin and Junfu Pu and Zhongang Qi and Wei-Yi Pei and Zhi Qu and Yexin Wang and Ying Shan and Wei-Shi Zheng and Jian-Fang Hu", "abstract": "  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n", "link": "http://arxiv.org/abs/2408.01669v3", "date": "2024-08-08", "relevancy": 1.474, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4956}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4903}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynopGround%3A%20A%20Large-Scale%20Dataset%20for%20Multi-Paragraph%20Video%20Grounding%0A%20%20from%20TV%20Dramas%20and%20Synopses&body=Title%3A%20SynopGround%3A%20A%20Large-Scale%20Dataset%20for%20Multi-Paragraph%20Video%20Grounding%0A%20%20from%20TV%20Dramas%20and%20Synopses%0AAuthor%3A%20Chaolei%20Tan%20and%20Zihang%20Lin%20and%20Junfu%20Pu%20and%20Zhongang%20Qi%20and%20Wei-Yi%20Pei%20and%20Zhi%20Qu%20and%20Yexin%20Wang%20and%20Ying%20Shan%20and%20Wei-Shi%20Zheng%20and%20Jian-Fang%20Hu%0AAbstract%3A%20%20%20Video%20grounding%20is%20a%20fundamental%20problem%20in%20multimodal%20content%20understanding%2C%0Aaiming%20to%20localize%20specific%20natural%20language%20queries%20in%20an%20untrimmed%20video.%0AHowever%2C%20current%20video%20grounding%20datasets%20merely%20focus%20on%20simple%20events%20and%20are%0Aeither%20limited%20to%20shorter%20videos%20or%20brief%20sentences%2C%20which%20hinders%20the%20model%0Afrom%20evolving%20toward%20stronger%20multimodal%20understanding%20capabilities.%20To%20address%0Athese%20limitations%2C%20we%20present%20a%20large-scale%20video%20grounding%20dataset%20named%0ASynopGround%2C%20in%20which%20more%20than%202800%20hours%20of%20videos%20are%20sourced%20from%20popular%0ATV%20dramas%20and%20are%20paired%20with%20accurately%20localized%20human-written%20synopses.%20Each%0Aparagraph%20in%20the%20synopsis%20serves%20as%20a%20language%20query%20and%20is%20manually%20annotated%0Awith%20precise%20temporal%20boundaries%20in%20the%20long%20video.%20These%20paragraph%20queries%20are%0Atightly%20correlated%20to%20each%20other%20and%20contain%20a%20wealth%20of%20abstract%20expressions%0Asummarizing%20video%20storylines%20and%20specific%20descriptions%20portraying%20event%0Adetails%2C%20which%20enables%20the%20model%20to%20learn%20multimodal%20perception%20on%20more%0Aintricate%20concepts%20over%20longer%20context%20dependencies.%20Based%20on%20the%20dataset%2C%20we%0Afurther%20introduce%20a%20more%20complex%20setting%20of%20video%20grounding%20dubbed%0AMulti-Paragraph%20Video%20Grounding%20%28MPVG%29%2C%20which%20takes%20as%20input%20multiple%0Aparagraphs%20and%20a%20long%20video%20for%20grounding%20each%20paragraph%20query%20to%20its%20temporal%0Ainterval.%20In%20addition%2C%20we%20propose%20a%20novel%20Local-Global%20Multimodal%20Reasoner%0A%28LGMR%29%20to%20explicitly%20model%20the%20local-global%20structures%20of%20long-term%20multimodal%0Ainputs%20for%20MPVG.%20Our%20method%20provides%20an%20effective%20baseline%20solution%20to%20the%0Amulti-paragraph%20video%20grounding%20problem.%20Extensive%20experiments%20verify%20the%0Aproposed%20model%27s%20effectiveness%20as%20well%20as%20its%20superiority%20in%20long-term%0Amulti-paragraph%20video%20grounding%20over%20prior%20state-of-the-arts.%20Dataset%20and%20code%0Aare%20publicly%20available.%20Project%20page%3A%20https%3A//synopground.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01669v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynopGround%253A%2520A%2520Large-Scale%2520Dataset%2520for%2520Multi-Paragraph%2520Video%2520Grounding%250A%2520%2520from%2520TV%2520Dramas%2520and%2520Synopses%26entry.906535625%3DChaolei%2520Tan%2520and%2520Zihang%2520Lin%2520and%2520Junfu%2520Pu%2520and%2520Zhongang%2520Qi%2520and%2520Wei-Yi%2520Pei%2520and%2520Zhi%2520Qu%2520and%2520Yexin%2520Wang%2520and%2520Ying%2520Shan%2520and%2520Wei-Shi%2520Zheng%2520and%2520Jian-Fang%2520Hu%26entry.1292438233%3D%2520%2520Video%2520grounding%2520is%2520a%2520fundamental%2520problem%2520in%2520multimodal%2520content%2520understanding%252C%250Aaiming%2520to%2520localize%2520specific%2520natural%2520language%2520queries%2520in%2520an%2520untrimmed%2520video.%250AHowever%252C%2520current%2520video%2520grounding%2520datasets%2520merely%2520focus%2520on%2520simple%2520events%2520and%2520are%250Aeither%2520limited%2520to%2520shorter%2520videos%2520or%2520brief%2520sentences%252C%2520which%2520hinders%2520the%2520model%250Afrom%2520evolving%2520toward%2520stronger%2520multimodal%2520understanding%2520capabilities.%2520To%2520address%250Athese%2520limitations%252C%2520we%2520present%2520a%2520large-scale%2520video%2520grounding%2520dataset%2520named%250ASynopGround%252C%2520in%2520which%2520more%2520than%25202800%2520hours%2520of%2520videos%2520are%2520sourced%2520from%2520popular%250ATV%2520dramas%2520and%2520are%2520paired%2520with%2520accurately%2520localized%2520human-written%2520synopses.%2520Each%250Aparagraph%2520in%2520the%2520synopsis%2520serves%2520as%2520a%2520language%2520query%2520and%2520is%2520manually%2520annotated%250Awith%2520precise%2520temporal%2520boundaries%2520in%2520the%2520long%2520video.%2520These%2520paragraph%2520queries%2520are%250Atightly%2520correlated%2520to%2520each%2520other%2520and%2520contain%2520a%2520wealth%2520of%2520abstract%2520expressions%250Asummarizing%2520video%2520storylines%2520and%2520specific%2520descriptions%2520portraying%2520event%250Adetails%252C%2520which%2520enables%2520the%2520model%2520to%2520learn%2520multimodal%2520perception%2520on%2520more%250Aintricate%2520concepts%2520over%2520longer%2520context%2520dependencies.%2520Based%2520on%2520the%2520dataset%252C%2520we%250Afurther%2520introduce%2520a%2520more%2520complex%2520setting%2520of%2520video%2520grounding%2520dubbed%250AMulti-Paragraph%2520Video%2520Grounding%2520%2528MPVG%2529%252C%2520which%2520takes%2520as%2520input%2520multiple%250Aparagraphs%2520and%2520a%2520long%2520video%2520for%2520grounding%2520each%2520paragraph%2520query%2520to%2520its%2520temporal%250Ainterval.%2520In%2520addition%252C%2520we%2520propose%2520a%2520novel%2520Local-Global%2520Multimodal%2520Reasoner%250A%2528LGMR%2529%2520to%2520explicitly%2520model%2520the%2520local-global%2520structures%2520of%2520long-term%2520multimodal%250Ainputs%2520for%2520MPVG.%2520Our%2520method%2520provides%2520an%2520effective%2520baseline%2520solution%2520to%2520the%250Amulti-paragraph%2520video%2520grounding%2520problem.%2520Extensive%2520experiments%2520verify%2520the%250Aproposed%2520model%2527s%2520effectiveness%2520as%2520well%2520as%2520its%2520superiority%2520in%2520long-term%250Amulti-paragraph%2520video%2520grounding%2520over%2520prior%2520state-of-the-arts.%2520Dataset%2520and%2520code%250Aare%2520publicly%2520available.%2520Project%2520page%253A%2520https%253A//synopground.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01669v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynopGround%3A%20A%20Large-Scale%20Dataset%20for%20Multi-Paragraph%20Video%20Grounding%0A%20%20from%20TV%20Dramas%20and%20Synopses&entry.906535625=Chaolei%20Tan%20and%20Zihang%20Lin%20and%20Junfu%20Pu%20and%20Zhongang%20Qi%20and%20Wei-Yi%20Pei%20and%20Zhi%20Qu%20and%20Yexin%20Wang%20and%20Ying%20Shan%20and%20Wei-Shi%20Zheng%20and%20Jian-Fang%20Hu&entry.1292438233=%20%20Video%20grounding%20is%20a%20fundamental%20problem%20in%20multimodal%20content%20understanding%2C%0Aaiming%20to%20localize%20specific%20natural%20language%20queries%20in%20an%20untrimmed%20video.%0AHowever%2C%20current%20video%20grounding%20datasets%20merely%20focus%20on%20simple%20events%20and%20are%0Aeither%20limited%20to%20shorter%20videos%20or%20brief%20sentences%2C%20which%20hinders%20the%20model%0Afrom%20evolving%20toward%20stronger%20multimodal%20understanding%20capabilities.%20To%20address%0Athese%20limitations%2C%20we%20present%20a%20large-scale%20video%20grounding%20dataset%20named%0ASynopGround%2C%20in%20which%20more%20than%202800%20hours%20of%20videos%20are%20sourced%20from%20popular%0ATV%20dramas%20and%20are%20paired%20with%20accurately%20localized%20human-written%20synopses.%20Each%0Aparagraph%20in%20the%20synopsis%20serves%20as%20a%20language%20query%20and%20is%20manually%20annotated%0Awith%20precise%20temporal%20boundaries%20in%20the%20long%20video.%20These%20paragraph%20queries%20are%0Atightly%20correlated%20to%20each%20other%20and%20contain%20a%20wealth%20of%20abstract%20expressions%0Asummarizing%20video%20storylines%20and%20specific%20descriptions%20portraying%20event%0Adetails%2C%20which%20enables%20the%20model%20to%20learn%20multimodal%20perception%20on%20more%0Aintricate%20concepts%20over%20longer%20context%20dependencies.%20Based%20on%20the%20dataset%2C%20we%0Afurther%20introduce%20a%20more%20complex%20setting%20of%20video%20grounding%20dubbed%0AMulti-Paragraph%20Video%20Grounding%20%28MPVG%29%2C%20which%20takes%20as%20input%20multiple%0Aparagraphs%20and%20a%20long%20video%20for%20grounding%20each%20paragraph%20query%20to%20its%20temporal%0Ainterval.%20In%20addition%2C%20we%20propose%20a%20novel%20Local-Global%20Multimodal%20Reasoner%0A%28LGMR%29%20to%20explicitly%20model%20the%20local-global%20structures%20of%20long-term%20multimodal%0Ainputs%20for%20MPVG.%20Our%20method%20provides%20an%20effective%20baseline%20solution%20to%20the%0Amulti-paragraph%20video%20grounding%20problem.%20Extensive%20experiments%20verify%20the%0Aproposed%20model%27s%20effectiveness%20as%20well%20as%20its%20superiority%20in%20long-term%0Amulti-paragraph%20video%20grounding%20over%20prior%20state-of-the-arts.%20Dataset%20and%20code%0Aare%20publicly%20available.%20Project%20page%3A%20https%3A//synopground.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01669v3&entry.124074799=Read"},
{"title": "Learning Fine-Grained Grounded Citations for Attributed Large Language\n  Models", "author": "Lei Huang and Xiaocheng Feng and Weitao Ma and Yuxuan Gu and Weihong Zhong and Xiachong Feng and Weijiang Yu and Weihua Peng and Duyu Tang and Dandan Tu and Bing Qin", "abstract": "  Despite the impressive performance on information-seeking tasks, large\nlanguage models (LLMs) still struggle with hallucinations. Attributed LLMs,\nwhich augment generated text with in-line citations, have shown potential in\nmitigating hallucinations and improving verifiability. However, current\napproaches suffer from suboptimal citation quality due to their reliance on\nin-context learning. Furthermore, the practice of citing only coarse document\nidentifiers makes it challenging for users to perform fine-grained\nverification. In this work, we introduce FRONT, a training framework designed\nto teach LLMs to generate Fine-Grained Grounded Citations. By grounding model\noutputs in fine-grained supporting quotes, these quotes guide the generation of\ngrounded and consistent responses, not only improving citation quality but also\nfacilitating fine-grained verification. Experiments on the ALCE benchmark\ndemonstrate the efficacy of FRONT in generating superior grounded responses and\nhighly supportive citations. With LLaMA-2-7B, the framework significantly\noutperforms all the baselines, achieving an average of 14.21% improvement in\ncitation quality across all datasets, even surpassing ChatGPT.\n", "link": "http://arxiv.org/abs/2408.04568v1", "date": "2024-08-08", "relevancy": 1.4684, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5113}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4848}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Fine-Grained%20Grounded%20Citations%20for%20Attributed%20Large%20Language%0A%20%20Models&body=Title%3A%20Learning%20Fine-Grained%20Grounded%20Citations%20for%20Attributed%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Lei%20Huang%20and%20Xiaocheng%20Feng%20and%20Weitao%20Ma%20and%20Yuxuan%20Gu%20and%20Weihong%20Zhong%20and%20Xiachong%20Feng%20and%20Weijiang%20Yu%20and%20Weihua%20Peng%20and%20Duyu%20Tang%20and%20Dandan%20Tu%20and%20Bing%20Qin%0AAbstract%3A%20%20%20Despite%20the%20impressive%20performance%20on%20information-seeking%20tasks%2C%20large%0Alanguage%20models%20%28LLMs%29%20still%20struggle%20with%20hallucinations.%20Attributed%20LLMs%2C%0Awhich%20augment%20generated%20text%20with%20in-line%20citations%2C%20have%20shown%20potential%20in%0Amitigating%20hallucinations%20and%20improving%20verifiability.%20However%2C%20current%0Aapproaches%20suffer%20from%20suboptimal%20citation%20quality%20due%20to%20their%20reliance%20on%0Ain-context%20learning.%20Furthermore%2C%20the%20practice%20of%20citing%20only%20coarse%20document%0Aidentifiers%20makes%20it%20challenging%20for%20users%20to%20perform%20fine-grained%0Averification.%20In%20this%20work%2C%20we%20introduce%20FRONT%2C%20a%20training%20framework%20designed%0Ato%20teach%20LLMs%20to%20generate%20Fine-Grained%20Grounded%20Citations.%20By%20grounding%20model%0Aoutputs%20in%20fine-grained%20supporting%20quotes%2C%20these%20quotes%20guide%20the%20generation%20of%0Agrounded%20and%20consistent%20responses%2C%20not%20only%20improving%20citation%20quality%20but%20also%0Afacilitating%20fine-grained%20verification.%20Experiments%20on%20the%20ALCE%20benchmark%0Ademonstrate%20the%20efficacy%20of%20FRONT%20in%20generating%20superior%20grounded%20responses%20and%0Ahighly%20supportive%20citations.%20With%20LLaMA-2-7B%2C%20the%20framework%20significantly%0Aoutperforms%20all%20the%20baselines%2C%20achieving%20an%20average%20of%2014.21%25%20improvement%20in%0Acitation%20quality%20across%20all%20datasets%2C%20even%20surpassing%20ChatGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04568v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Fine-Grained%2520Grounded%2520Citations%2520for%2520Attributed%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DLei%2520Huang%2520and%2520Xiaocheng%2520Feng%2520and%2520Weitao%2520Ma%2520and%2520Yuxuan%2520Gu%2520and%2520Weihong%2520Zhong%2520and%2520Xiachong%2520Feng%2520and%2520Weijiang%2520Yu%2520and%2520Weihua%2520Peng%2520and%2520Duyu%2520Tang%2520and%2520Dandan%2520Tu%2520and%2520Bing%2520Qin%26entry.1292438233%3D%2520%2520Despite%2520the%2520impressive%2520performance%2520on%2520information-seeking%2520tasks%252C%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520still%2520struggle%2520with%2520hallucinations.%2520Attributed%2520LLMs%252C%250Awhich%2520augment%2520generated%2520text%2520with%2520in-line%2520citations%252C%2520have%2520shown%2520potential%2520in%250Amitigating%2520hallucinations%2520and%2520improving%2520verifiability.%2520However%252C%2520current%250Aapproaches%2520suffer%2520from%2520suboptimal%2520citation%2520quality%2520due%2520to%2520their%2520reliance%2520on%250Ain-context%2520learning.%2520Furthermore%252C%2520the%2520practice%2520of%2520citing%2520only%2520coarse%2520document%250Aidentifiers%2520makes%2520it%2520challenging%2520for%2520users%2520to%2520perform%2520fine-grained%250Averification.%2520In%2520this%2520work%252C%2520we%2520introduce%2520FRONT%252C%2520a%2520training%2520framework%2520designed%250Ato%2520teach%2520LLMs%2520to%2520generate%2520Fine-Grained%2520Grounded%2520Citations.%2520By%2520grounding%2520model%250Aoutputs%2520in%2520fine-grained%2520supporting%2520quotes%252C%2520these%2520quotes%2520guide%2520the%2520generation%2520of%250Agrounded%2520and%2520consistent%2520responses%252C%2520not%2520only%2520improving%2520citation%2520quality%2520but%2520also%250Afacilitating%2520fine-grained%2520verification.%2520Experiments%2520on%2520the%2520ALCE%2520benchmark%250Ademonstrate%2520the%2520efficacy%2520of%2520FRONT%2520in%2520generating%2520superior%2520grounded%2520responses%2520and%250Ahighly%2520supportive%2520citations.%2520With%2520LLaMA-2-7B%252C%2520the%2520framework%2520significantly%250Aoutperforms%2520all%2520the%2520baselines%252C%2520achieving%2520an%2520average%2520of%252014.21%2525%2520improvement%2520in%250Acitation%2520quality%2520across%2520all%2520datasets%252C%2520even%2520surpassing%2520ChatGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04568v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Fine-Grained%20Grounded%20Citations%20for%20Attributed%20Large%20Language%0A%20%20Models&entry.906535625=Lei%20Huang%20and%20Xiaocheng%20Feng%20and%20Weitao%20Ma%20and%20Yuxuan%20Gu%20and%20Weihong%20Zhong%20and%20Xiachong%20Feng%20and%20Weijiang%20Yu%20and%20Weihua%20Peng%20and%20Duyu%20Tang%20and%20Dandan%20Tu%20and%20Bing%20Qin&entry.1292438233=%20%20Despite%20the%20impressive%20performance%20on%20information-seeking%20tasks%2C%20large%0Alanguage%20models%20%28LLMs%29%20still%20struggle%20with%20hallucinations.%20Attributed%20LLMs%2C%0Awhich%20augment%20generated%20text%20with%20in-line%20citations%2C%20have%20shown%20potential%20in%0Amitigating%20hallucinations%20and%20improving%20verifiability.%20However%2C%20current%0Aapproaches%20suffer%20from%20suboptimal%20citation%20quality%20due%20to%20their%20reliance%20on%0Ain-context%20learning.%20Furthermore%2C%20the%20practice%20of%20citing%20only%20coarse%20document%0Aidentifiers%20makes%20it%20challenging%20for%20users%20to%20perform%20fine-grained%0Averification.%20In%20this%20work%2C%20we%20introduce%20FRONT%2C%20a%20training%20framework%20designed%0Ato%20teach%20LLMs%20to%20generate%20Fine-Grained%20Grounded%20Citations.%20By%20grounding%20model%0Aoutputs%20in%20fine-grained%20supporting%20quotes%2C%20these%20quotes%20guide%20the%20generation%20of%0Agrounded%20and%20consistent%20responses%2C%20not%20only%20improving%20citation%20quality%20but%20also%0Afacilitating%20fine-grained%20verification.%20Experiments%20on%20the%20ALCE%20benchmark%0Ademonstrate%20the%20efficacy%20of%20FRONT%20in%20generating%20superior%20grounded%20responses%20and%0Ahighly%20supportive%20citations.%20With%20LLaMA-2-7B%2C%20the%20framework%20significantly%0Aoutperforms%20all%20the%20baselines%2C%20achieving%20an%20average%20of%2014.21%25%20improvement%20in%0Acitation%20quality%20across%20all%20datasets%2C%20even%20surpassing%20ChatGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04568v1&entry.124074799=Read"},
{"title": "DeepVol: Volatility Forecasting from High-Frequency Data with Dilated\n  Causal Convolutions", "author": "Fernando Moreno-Pino and Stefan Zohren", "abstract": "  Volatility forecasts play a central role among equity risk measures. Besides\ntraditional statistical models, modern forecasting techniques based on machine\nlearning can be employed when treating volatility as a univariate, daily\ntime-series. Moreover, econometric studies have shown that increasing the\nnumber of daily observations with high-frequency intraday data helps to improve\nvolatility predictions. In this work, we propose DeepVol, a model based on\nDilated Causal Convolutions that uses high-frequency data to forecast day-ahead\nvolatility. Our empirical findings demonstrate that dilated convolutional\nfilters are highly effective at extracting relevant information from intraday\nfinancial time-series, proving that this architecture can effectively leverage\npredictive information present in high-frequency data that would otherwise be\nlost if realised measures were precomputed. Simultaneously, dilated\nconvolutional filters trained with intraday high-frequency data help us avoid\nthe limitations of models that use daily data, such as model misspecification\nor manually designed handcrafted features, whose devise involves optimising the\ntrade-off between accuracy and computational efficiency and makes models prone\nto lack of adaptation into changing circumstances. In our analysis, we use two\nyears of intraday data from NASDAQ-100 to evaluate the performance of DeepVol.\nOur empirical results suggest that the proposed deep learning-based approach\neffectively learns global features from high-frequency data, resulting in more\naccurate predictions compared to traditional methodologies and producing more\naccurate risk measures.\n", "link": "http://arxiv.org/abs/2210.04797v3", "date": "2024-08-08", "relevancy": 1.3966, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4787}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4649}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepVol%3A%20Volatility%20Forecasting%20from%20High-Frequency%20Data%20with%20Dilated%0A%20%20Causal%20Convolutions&body=Title%3A%20DeepVol%3A%20Volatility%20Forecasting%20from%20High-Frequency%20Data%20with%20Dilated%0A%20%20Causal%20Convolutions%0AAuthor%3A%20Fernando%20Moreno-Pino%20and%20Stefan%20Zohren%0AAbstract%3A%20%20%20Volatility%20forecasts%20play%20a%20central%20role%20among%20equity%20risk%20measures.%20Besides%0Atraditional%20statistical%20models%2C%20modern%20forecasting%20techniques%20based%20on%20machine%0Alearning%20can%20be%20employed%20when%20treating%20volatility%20as%20a%20univariate%2C%20daily%0Atime-series.%20Moreover%2C%20econometric%20studies%20have%20shown%20that%20increasing%20the%0Anumber%20of%20daily%20observations%20with%20high-frequency%20intraday%20data%20helps%20to%20improve%0Avolatility%20predictions.%20In%20this%20work%2C%20we%20propose%20DeepVol%2C%20a%20model%20based%20on%0ADilated%20Causal%20Convolutions%20that%20uses%20high-frequency%20data%20to%20forecast%20day-ahead%0Avolatility.%20Our%20empirical%20findings%20demonstrate%20that%20dilated%20convolutional%0Afilters%20are%20highly%20effective%20at%20extracting%20relevant%20information%20from%20intraday%0Afinancial%20time-series%2C%20proving%20that%20this%20architecture%20can%20effectively%20leverage%0Apredictive%20information%20present%20in%20high-frequency%20data%20that%20would%20otherwise%20be%0Alost%20if%20realised%20measures%20were%20precomputed.%20Simultaneously%2C%20dilated%0Aconvolutional%20filters%20trained%20with%20intraday%20high-frequency%20data%20help%20us%20avoid%0Athe%20limitations%20of%20models%20that%20use%20daily%20data%2C%20such%20as%20model%20misspecification%0Aor%20manually%20designed%20handcrafted%20features%2C%20whose%20devise%20involves%20optimising%20the%0Atrade-off%20between%20accuracy%20and%20computational%20efficiency%20and%20makes%20models%20prone%0Ato%20lack%20of%20adaptation%20into%20changing%20circumstances.%20In%20our%20analysis%2C%20we%20use%20two%0Ayears%20of%20intraday%20data%20from%20NASDAQ-100%20to%20evaluate%20the%20performance%20of%20DeepVol.%0AOur%20empirical%20results%20suggest%20that%20the%20proposed%20deep%20learning-based%20approach%0Aeffectively%20learns%20global%20features%20from%20high-frequency%20data%2C%20resulting%20in%20more%0Aaccurate%20predictions%20compared%20to%20traditional%20methodologies%20and%20producing%20more%0Aaccurate%20risk%20measures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.04797v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepVol%253A%2520Volatility%2520Forecasting%2520from%2520High-Frequency%2520Data%2520with%2520Dilated%250A%2520%2520Causal%2520Convolutions%26entry.906535625%3DFernando%2520Moreno-Pino%2520and%2520Stefan%2520Zohren%26entry.1292438233%3D%2520%2520Volatility%2520forecasts%2520play%2520a%2520central%2520role%2520among%2520equity%2520risk%2520measures.%2520Besides%250Atraditional%2520statistical%2520models%252C%2520modern%2520forecasting%2520techniques%2520based%2520on%2520machine%250Alearning%2520can%2520be%2520employed%2520when%2520treating%2520volatility%2520as%2520a%2520univariate%252C%2520daily%250Atime-series.%2520Moreover%252C%2520econometric%2520studies%2520have%2520shown%2520that%2520increasing%2520the%250Anumber%2520of%2520daily%2520observations%2520with%2520high-frequency%2520intraday%2520data%2520helps%2520to%2520improve%250Avolatility%2520predictions.%2520In%2520this%2520work%252C%2520we%2520propose%2520DeepVol%252C%2520a%2520model%2520based%2520on%250ADilated%2520Causal%2520Convolutions%2520that%2520uses%2520high-frequency%2520data%2520to%2520forecast%2520day-ahead%250Avolatility.%2520Our%2520empirical%2520findings%2520demonstrate%2520that%2520dilated%2520convolutional%250Afilters%2520are%2520highly%2520effective%2520at%2520extracting%2520relevant%2520information%2520from%2520intraday%250Afinancial%2520time-series%252C%2520proving%2520that%2520this%2520architecture%2520can%2520effectively%2520leverage%250Apredictive%2520information%2520present%2520in%2520high-frequency%2520data%2520that%2520would%2520otherwise%2520be%250Alost%2520if%2520realised%2520measures%2520were%2520precomputed.%2520Simultaneously%252C%2520dilated%250Aconvolutional%2520filters%2520trained%2520with%2520intraday%2520high-frequency%2520data%2520help%2520us%2520avoid%250Athe%2520limitations%2520of%2520models%2520that%2520use%2520daily%2520data%252C%2520such%2520as%2520model%2520misspecification%250Aor%2520manually%2520designed%2520handcrafted%2520features%252C%2520whose%2520devise%2520involves%2520optimising%2520the%250Atrade-off%2520between%2520accuracy%2520and%2520computational%2520efficiency%2520and%2520makes%2520models%2520prone%250Ato%2520lack%2520of%2520adaptation%2520into%2520changing%2520circumstances.%2520In%2520our%2520analysis%252C%2520we%2520use%2520two%250Ayears%2520of%2520intraday%2520data%2520from%2520NASDAQ-100%2520to%2520evaluate%2520the%2520performance%2520of%2520DeepVol.%250AOur%2520empirical%2520results%2520suggest%2520that%2520the%2520proposed%2520deep%2520learning-based%2520approach%250Aeffectively%2520learns%2520global%2520features%2520from%2520high-frequency%2520data%252C%2520resulting%2520in%2520more%250Aaccurate%2520predictions%2520compared%2520to%2520traditional%2520methodologies%2520and%2520producing%2520more%250Aaccurate%2520risk%2520measures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.04797v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepVol%3A%20Volatility%20Forecasting%20from%20High-Frequency%20Data%20with%20Dilated%0A%20%20Causal%20Convolutions&entry.906535625=Fernando%20Moreno-Pino%20and%20Stefan%20Zohren&entry.1292438233=%20%20Volatility%20forecasts%20play%20a%20central%20role%20among%20equity%20risk%20measures.%20Besides%0Atraditional%20statistical%20models%2C%20modern%20forecasting%20techniques%20based%20on%20machine%0Alearning%20can%20be%20employed%20when%20treating%20volatility%20as%20a%20univariate%2C%20daily%0Atime-series.%20Moreover%2C%20econometric%20studies%20have%20shown%20that%20increasing%20the%0Anumber%20of%20daily%20observations%20with%20high-frequency%20intraday%20data%20helps%20to%20improve%0Avolatility%20predictions.%20In%20this%20work%2C%20we%20propose%20DeepVol%2C%20a%20model%20based%20on%0ADilated%20Causal%20Convolutions%20that%20uses%20high-frequency%20data%20to%20forecast%20day-ahead%0Avolatility.%20Our%20empirical%20findings%20demonstrate%20that%20dilated%20convolutional%0Afilters%20are%20highly%20effective%20at%20extracting%20relevant%20information%20from%20intraday%0Afinancial%20time-series%2C%20proving%20that%20this%20architecture%20can%20effectively%20leverage%0Apredictive%20information%20present%20in%20high-frequency%20data%20that%20would%20otherwise%20be%0Alost%20if%20realised%20measures%20were%20precomputed.%20Simultaneously%2C%20dilated%0Aconvolutional%20filters%20trained%20with%20intraday%20high-frequency%20data%20help%20us%20avoid%0Athe%20limitations%20of%20models%20that%20use%20daily%20data%2C%20such%20as%20model%20misspecification%0Aor%20manually%20designed%20handcrafted%20features%2C%20whose%20devise%20involves%20optimising%20the%0Atrade-off%20between%20accuracy%20and%20computational%20efficiency%20and%20makes%20models%20prone%0Ato%20lack%20of%20adaptation%20into%20changing%20circumstances.%20In%20our%20analysis%2C%20we%20use%20two%0Ayears%20of%20intraday%20data%20from%20NASDAQ-100%20to%20evaluate%20the%20performance%20of%20DeepVol.%0AOur%20empirical%20results%20suggest%20that%20the%20proposed%20deep%20learning-based%20approach%0Aeffectively%20learns%20global%20features%20from%20high-frequency%20data%2C%20resulting%20in%20more%0Aaccurate%20predictions%20compared%20to%20traditional%20methodologies%20and%20producing%20more%0Aaccurate%20risk%20measures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.04797v3&entry.124074799=Read"},
{"title": "Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs", "author": "Kevin Tan and Wei Fan and Yuting Wei", "abstract": "  Hybrid Reinforcement Learning (RL), where an agent learns from both an\noffline dataset and online explorations in an unknown environment, has garnered\nsignificant recent interest. A crucial question posed by Xie et al. (2022) is\nwhether hybrid RL can improve upon the existing lower bounds established in\npurely offline and purely online RL without relying on the single-policy\nconcentrability assumption. While Li et al. (2023) provided an affirmative\nanswer to this question in the tabular PAC RL case, the question remains\nunsettled for both the regret-minimizing RL case and the non-tabular case.\n  In this work, building upon recent advancements in offline RL and\nreward-agnostic exploration, we develop computationally efficient algorithms\nfor both PAC and regret-minimizing RL with linear function approximation,\nwithout single-policy concentrability. We demonstrate that these algorithms\nachieve sharper error or regret bounds that are no worse than, and can improve\non, the optimal sample complexity in offline RL (the first algorithm, for PAC\nRL) and online RL (the second algorithm, for regret-minimizing RL) in linear\nMarkov decision processes (MDPs), regardless of the quality of the behavior\npolicy. To our knowledge, this work establishes the tightest theoretical\nguarantees currently available for hybrid RL in linear MDPs.\n", "link": "http://arxiv.org/abs/2408.04526v1", "date": "2024-08-08", "relevancy": 1.3717, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4879}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4564}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Reinforcement%20Learning%20Breaks%20Sample%20Size%20Barriers%20in%20Linear%20MDPs&body=Title%3A%20Hybrid%20Reinforcement%20Learning%20Breaks%20Sample%20Size%20Barriers%20in%20Linear%20MDPs%0AAuthor%3A%20Kevin%20Tan%20and%20Wei%20Fan%20and%20Yuting%20Wei%0AAbstract%3A%20%20%20Hybrid%20Reinforcement%20Learning%20%28RL%29%2C%20where%20an%20agent%20learns%20from%20both%20an%0Aoffline%20dataset%20and%20online%20explorations%20in%20an%20unknown%20environment%2C%20has%20garnered%0Asignificant%20recent%20interest.%20A%20crucial%20question%20posed%20by%20Xie%20et%20al.%20%282022%29%20is%0Awhether%20hybrid%20RL%20can%20improve%20upon%20the%20existing%20lower%20bounds%20established%20in%0Apurely%20offline%20and%20purely%20online%20RL%20without%20relying%20on%20the%20single-policy%0Aconcentrability%20assumption.%20While%20Li%20et%20al.%20%282023%29%20provided%20an%20affirmative%0Aanswer%20to%20this%20question%20in%20the%20tabular%20PAC%20RL%20case%2C%20the%20question%20remains%0Aunsettled%20for%20both%20the%20regret-minimizing%20RL%20case%20and%20the%20non-tabular%20case.%0A%20%20In%20this%20work%2C%20building%20upon%20recent%20advancements%20in%20offline%20RL%20and%0Areward-agnostic%20exploration%2C%20we%20develop%20computationally%20efficient%20algorithms%0Afor%20both%20PAC%20and%20regret-minimizing%20RL%20with%20linear%20function%20approximation%2C%0Awithout%20single-policy%20concentrability.%20We%20demonstrate%20that%20these%20algorithms%0Aachieve%20sharper%20error%20or%20regret%20bounds%20that%20are%20no%20worse%20than%2C%20and%20can%20improve%0Aon%2C%20the%20optimal%20sample%20complexity%20in%20offline%20RL%20%28the%20first%20algorithm%2C%20for%20PAC%0ARL%29%20and%20online%20RL%20%28the%20second%20algorithm%2C%20for%20regret-minimizing%20RL%29%20in%20linear%0AMarkov%20decision%20processes%20%28MDPs%29%2C%20regardless%20of%20the%20quality%20of%20the%20behavior%0Apolicy.%20To%20our%20knowledge%2C%20this%20work%20establishes%20the%20tightest%20theoretical%0Aguarantees%20currently%20available%20for%20hybrid%20RL%20in%20linear%20MDPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Reinforcement%2520Learning%2520Breaks%2520Sample%2520Size%2520Barriers%2520in%2520Linear%2520MDPs%26entry.906535625%3DKevin%2520Tan%2520and%2520Wei%2520Fan%2520and%2520Yuting%2520Wei%26entry.1292438233%3D%2520%2520Hybrid%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520where%2520an%2520agent%2520learns%2520from%2520both%2520an%250Aoffline%2520dataset%2520and%2520online%2520explorations%2520in%2520an%2520unknown%2520environment%252C%2520has%2520garnered%250Asignificant%2520recent%2520interest.%2520A%2520crucial%2520question%2520posed%2520by%2520Xie%2520et%2520al.%2520%25282022%2529%2520is%250Awhether%2520hybrid%2520RL%2520can%2520improve%2520upon%2520the%2520existing%2520lower%2520bounds%2520established%2520in%250Apurely%2520offline%2520and%2520purely%2520online%2520RL%2520without%2520relying%2520on%2520the%2520single-policy%250Aconcentrability%2520assumption.%2520While%2520Li%2520et%2520al.%2520%25282023%2529%2520provided%2520an%2520affirmative%250Aanswer%2520to%2520this%2520question%2520in%2520the%2520tabular%2520PAC%2520RL%2520case%252C%2520the%2520question%2520remains%250Aunsettled%2520for%2520both%2520the%2520regret-minimizing%2520RL%2520case%2520and%2520the%2520non-tabular%2520case.%250A%2520%2520In%2520this%2520work%252C%2520building%2520upon%2520recent%2520advancements%2520in%2520offline%2520RL%2520and%250Areward-agnostic%2520exploration%252C%2520we%2520develop%2520computationally%2520efficient%2520algorithms%250Afor%2520both%2520PAC%2520and%2520regret-minimizing%2520RL%2520with%2520linear%2520function%2520approximation%252C%250Awithout%2520single-policy%2520concentrability.%2520We%2520demonstrate%2520that%2520these%2520algorithms%250Aachieve%2520sharper%2520error%2520or%2520regret%2520bounds%2520that%2520are%2520no%2520worse%2520than%252C%2520and%2520can%2520improve%250Aon%252C%2520the%2520optimal%2520sample%2520complexity%2520in%2520offline%2520RL%2520%2528the%2520first%2520algorithm%252C%2520for%2520PAC%250ARL%2529%2520and%2520online%2520RL%2520%2528the%2520second%2520algorithm%252C%2520for%2520regret-minimizing%2520RL%2529%2520in%2520linear%250AMarkov%2520decision%2520processes%2520%2528MDPs%2529%252C%2520regardless%2520of%2520the%2520quality%2520of%2520the%2520behavior%250Apolicy.%2520To%2520our%2520knowledge%252C%2520this%2520work%2520establishes%2520the%2520tightest%2520theoretical%250Aguarantees%2520currently%2520available%2520for%2520hybrid%2520RL%2520in%2520linear%2520MDPs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Reinforcement%20Learning%20Breaks%20Sample%20Size%20Barriers%20in%20Linear%20MDPs&entry.906535625=Kevin%20Tan%20and%20Wei%20Fan%20and%20Yuting%20Wei&entry.1292438233=%20%20Hybrid%20Reinforcement%20Learning%20%28RL%29%2C%20where%20an%20agent%20learns%20from%20both%20an%0Aoffline%20dataset%20and%20online%20explorations%20in%20an%20unknown%20environment%2C%20has%20garnered%0Asignificant%20recent%20interest.%20A%20crucial%20question%20posed%20by%20Xie%20et%20al.%20%282022%29%20is%0Awhether%20hybrid%20RL%20can%20improve%20upon%20the%20existing%20lower%20bounds%20established%20in%0Apurely%20offline%20and%20purely%20online%20RL%20without%20relying%20on%20the%20single-policy%0Aconcentrability%20assumption.%20While%20Li%20et%20al.%20%282023%29%20provided%20an%20affirmative%0Aanswer%20to%20this%20question%20in%20the%20tabular%20PAC%20RL%20case%2C%20the%20question%20remains%0Aunsettled%20for%20both%20the%20regret-minimizing%20RL%20case%20and%20the%20non-tabular%20case.%0A%20%20In%20this%20work%2C%20building%20upon%20recent%20advancements%20in%20offline%20RL%20and%0Areward-agnostic%20exploration%2C%20we%20develop%20computationally%20efficient%20algorithms%0Afor%20both%20PAC%20and%20regret-minimizing%20RL%20with%20linear%20function%20approximation%2C%0Awithout%20single-policy%20concentrability.%20We%20demonstrate%20that%20these%20algorithms%0Aachieve%20sharper%20error%20or%20regret%20bounds%20that%20are%20no%20worse%20than%2C%20and%20can%20improve%0Aon%2C%20the%20optimal%20sample%20complexity%20in%20offline%20RL%20%28the%20first%20algorithm%2C%20for%20PAC%0ARL%29%20and%20online%20RL%20%28the%20second%20algorithm%2C%20for%20regret-minimizing%20RL%29%20in%20linear%0AMarkov%20decision%20processes%20%28MDPs%29%2C%20regardless%20of%20the%20quality%20of%20the%20behavior%0Apolicy.%20To%20our%20knowledge%2C%20this%20work%20establishes%20the%20tightest%20theoretical%0Aguarantees%20currently%20available%20for%20hybrid%20RL%20in%20linear%20MDPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04526v1&entry.124074799=Read"},
{"title": "Reasoning about Study Regulations in Answer Set Programming", "author": "Susana Hahn and Cedric Martens and Amade Nemes and Henry Otunuya and Javier Romero and Torsten Schaub and Sebastian Schellhorn", "abstract": "  We are interested in automating reasoning with and about study regulations,\ncatering to various stakeholders, ranging from administrators, over faculty, to\nstudents at different stages. Our work builds on an extensive analysis of\nvarious study programs at the University of Potsdam. The conceptualization of\nthe underlying principles provides us with a formal account of study\nregulations. In particular, the formalization reveals the properties of\nadmissible study plans. With these at end, we propose an encoding of study\nregulations in Answer Set Programming that produces corresponding study plans.\nFinally, we show how this approach can be extended to a generic user interface\nfor exploring study plans.\n", "link": "http://arxiv.org/abs/2408.04528v1", "date": "2024-08-08", "relevancy": 1.2045, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4654}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4053}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20about%20Study%20Regulations%20in%20Answer%20Set%20Programming&body=Title%3A%20Reasoning%20about%20Study%20Regulations%20in%20Answer%20Set%20Programming%0AAuthor%3A%20Susana%20Hahn%20and%20Cedric%20Martens%20and%20Amade%20Nemes%20and%20Henry%20Otunuya%20and%20Javier%20Romero%20and%20Torsten%20Schaub%20and%20Sebastian%20Schellhorn%0AAbstract%3A%20%20%20We%20are%20interested%20in%20automating%20reasoning%20with%20and%20about%20study%20regulations%2C%0Acatering%20to%20various%20stakeholders%2C%20ranging%20from%20administrators%2C%20over%20faculty%2C%20to%0Astudents%20at%20different%20stages.%20Our%20work%20builds%20on%20an%20extensive%20analysis%20of%0Avarious%20study%20programs%20at%20the%20University%20of%20Potsdam.%20The%20conceptualization%20of%0Athe%20underlying%20principles%20provides%20us%20with%20a%20formal%20account%20of%20study%0Aregulations.%20In%20particular%2C%20the%20formalization%20reveals%20the%20properties%20of%0Aadmissible%20study%20plans.%20With%20these%20at%20end%2C%20we%20propose%20an%20encoding%20of%20study%0Aregulations%20in%20Answer%20Set%20Programming%20that%20produces%20corresponding%20study%20plans.%0AFinally%2C%20we%20show%20how%20this%20approach%20can%20be%20extended%20to%20a%20generic%20user%20interface%0Afor%20exploring%20study%20plans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04528v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520about%2520Study%2520Regulations%2520in%2520Answer%2520Set%2520Programming%26entry.906535625%3DSusana%2520Hahn%2520and%2520Cedric%2520Martens%2520and%2520Amade%2520Nemes%2520and%2520Henry%2520Otunuya%2520and%2520Javier%2520Romero%2520and%2520Torsten%2520Schaub%2520and%2520Sebastian%2520Schellhorn%26entry.1292438233%3D%2520%2520We%2520are%2520interested%2520in%2520automating%2520reasoning%2520with%2520and%2520about%2520study%2520regulations%252C%250Acatering%2520to%2520various%2520stakeholders%252C%2520ranging%2520from%2520administrators%252C%2520over%2520faculty%252C%2520to%250Astudents%2520at%2520different%2520stages.%2520Our%2520work%2520builds%2520on%2520an%2520extensive%2520analysis%2520of%250Avarious%2520study%2520programs%2520at%2520the%2520University%2520of%2520Potsdam.%2520The%2520conceptualization%2520of%250Athe%2520underlying%2520principles%2520provides%2520us%2520with%2520a%2520formal%2520account%2520of%2520study%250Aregulations.%2520In%2520particular%252C%2520the%2520formalization%2520reveals%2520the%2520properties%2520of%250Aadmissible%2520study%2520plans.%2520With%2520these%2520at%2520end%252C%2520we%2520propose%2520an%2520encoding%2520of%2520study%250Aregulations%2520in%2520Answer%2520Set%2520Programming%2520that%2520produces%2520corresponding%2520study%2520plans.%250AFinally%252C%2520we%2520show%2520how%2520this%2520approach%2520can%2520be%2520extended%2520to%2520a%2520generic%2520user%2520interface%250Afor%2520exploring%2520study%2520plans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04528v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20about%20Study%20Regulations%20in%20Answer%20Set%20Programming&entry.906535625=Susana%20Hahn%20and%20Cedric%20Martens%20and%20Amade%20Nemes%20and%20Henry%20Otunuya%20and%20Javier%20Romero%20and%20Torsten%20Schaub%20and%20Sebastian%20Schellhorn&entry.1292438233=%20%20We%20are%20interested%20in%20automating%20reasoning%20with%20and%20about%20study%20regulations%2C%0Acatering%20to%20various%20stakeholders%2C%20ranging%20from%20administrators%2C%20over%20faculty%2C%20to%0Astudents%20at%20different%20stages.%20Our%20work%20builds%20on%20an%20extensive%20analysis%20of%0Avarious%20study%20programs%20at%20the%20University%20of%20Potsdam.%20The%20conceptualization%20of%0Athe%20underlying%20principles%20provides%20us%20with%20a%20formal%20account%20of%20study%0Aregulations.%20In%20particular%2C%20the%20formalization%20reveals%20the%20properties%20of%0Aadmissible%20study%20plans.%20With%20these%20at%20end%2C%20we%20propose%20an%20encoding%20of%20study%0Aregulations%20in%20Answer%20Set%20Programming%20that%20produces%20corresponding%20study%20plans.%0AFinally%2C%20we%20show%20how%20this%20approach%20can%20be%20extended%20to%20a%20generic%20user%20interface%0Afor%20exploring%20study%20plans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04528v1&entry.124074799=Read"},
{"title": "SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals", "author": "Haoran Zheng and Utku Pamuksuz", "abstract": "  Explainable Artificial Intelligence (XAI) is essential for enhancing the\ntransparency and accountability of AI models, especially in natural language\nprocessing (NLP) tasks. This paper introduces SCENE (Soft Counterfactual\nEvaluation for Natural language Explainability), a novel evaluation method that\nleverages large language models (LLMs) to generate Soft Counterfactual\nexplanations in a zero-shot manner. By focusing on token-based substitutions,\nSCENE creates contextually appropriate and seman-tically meaningful Soft\nCounterfactuals without extensive fine-tuning. SCENE adopts Validitysoft and\nCsoft metrics to evaluate the effectiveness of model-agnostic XAI methods in\ntext classification tasks. Applied to CNN, RNN, and BERT architectures, SCENE\nprovides valuable insights into the strengths and limitations of various XAI\ntechniques.\n", "link": "http://arxiv.org/abs/2408.04575v1", "date": "2024-08-08", "relevancy": 1.3573, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5282}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4378}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCENE%3A%20Evaluating%20Explainable%20AI%20Techniques%20Using%20Soft%20Counterfactuals&body=Title%3A%20SCENE%3A%20Evaluating%20Explainable%20AI%20Techniques%20Using%20Soft%20Counterfactuals%0AAuthor%3A%20Haoran%20Zheng%20and%20Utku%20Pamuksuz%0AAbstract%3A%20%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20is%20essential%20for%20enhancing%20the%0Atransparency%20and%20accountability%20of%20AI%20models%2C%20especially%20in%20natural%20language%0Aprocessing%20%28NLP%29%20tasks.%20This%20paper%20introduces%20SCENE%20%28Soft%20Counterfactual%0AEvaluation%20for%20Natural%20language%20Explainability%29%2C%20a%20novel%20evaluation%20method%20that%0Aleverages%20large%20language%20models%20%28LLMs%29%20to%20generate%20Soft%20Counterfactual%0Aexplanations%20in%20a%20zero-shot%20manner.%20By%20focusing%20on%20token-based%20substitutions%2C%0ASCENE%20creates%20contextually%20appropriate%20and%20seman-tically%20meaningful%20Soft%0ACounterfactuals%20without%20extensive%20fine-tuning.%20SCENE%20adopts%20Validitysoft%20and%0ACsoft%20metrics%20to%20evaluate%20the%20effectiveness%20of%20model-agnostic%20XAI%20methods%20in%0Atext%20classification%20tasks.%20Applied%20to%20CNN%2C%20RNN%2C%20and%20BERT%20architectures%2C%20SCENE%0Aprovides%20valuable%20insights%20into%20the%20strengths%20and%20limitations%20of%20various%20XAI%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCENE%253A%2520Evaluating%2520Explainable%2520AI%2520Techniques%2520Using%2520Soft%2520Counterfactuals%26entry.906535625%3DHaoran%2520Zheng%2520and%2520Utku%2520Pamuksuz%26entry.1292438233%3D%2520%2520Explainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%2520is%2520essential%2520for%2520enhancing%2520the%250Atransparency%2520and%2520accountability%2520of%2520AI%2520models%252C%2520especially%2520in%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%2520tasks.%2520This%2520paper%2520introduces%2520SCENE%2520%2528Soft%2520Counterfactual%250AEvaluation%2520for%2520Natural%2520language%2520Explainability%2529%252C%2520a%2520novel%2520evaluation%2520method%2520that%250Aleverages%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520generate%2520Soft%2520Counterfactual%250Aexplanations%2520in%2520a%2520zero-shot%2520manner.%2520By%2520focusing%2520on%2520token-based%2520substitutions%252C%250ASCENE%2520creates%2520contextually%2520appropriate%2520and%2520seman-tically%2520meaningful%2520Soft%250ACounterfactuals%2520without%2520extensive%2520fine-tuning.%2520SCENE%2520adopts%2520Validitysoft%2520and%250ACsoft%2520metrics%2520to%2520evaluate%2520the%2520effectiveness%2520of%2520model-agnostic%2520XAI%2520methods%2520in%250Atext%2520classification%2520tasks.%2520Applied%2520to%2520CNN%252C%2520RNN%252C%2520and%2520BERT%2520architectures%252C%2520SCENE%250Aprovides%2520valuable%2520insights%2520into%2520the%2520strengths%2520and%2520limitations%2520of%2520various%2520XAI%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCENE%3A%20Evaluating%20Explainable%20AI%20Techniques%20Using%20Soft%20Counterfactuals&entry.906535625=Haoran%20Zheng%20and%20Utku%20Pamuksuz&entry.1292438233=%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20is%20essential%20for%20enhancing%20the%0Atransparency%20and%20accountability%20of%20AI%20models%2C%20especially%20in%20natural%20language%0Aprocessing%20%28NLP%29%20tasks.%20This%20paper%20introduces%20SCENE%20%28Soft%20Counterfactual%0AEvaluation%20for%20Natural%20language%20Explainability%29%2C%20a%20novel%20evaluation%20method%20that%0Aleverages%20large%20language%20models%20%28LLMs%29%20to%20generate%20Soft%20Counterfactual%0Aexplanations%20in%20a%20zero-shot%20manner.%20By%20focusing%20on%20token-based%20substitutions%2C%0ASCENE%20creates%20contextually%20appropriate%20and%20seman-tically%20meaningful%20Soft%0ACounterfactuals%20without%20extensive%20fine-tuning.%20SCENE%20adopts%20Validitysoft%20and%0ACsoft%20metrics%20to%20evaluate%20the%20effectiveness%20of%20model-agnostic%20XAI%20methods%20in%0Atext%20classification%20tasks.%20Applied%20to%20CNN%2C%20RNN%2C%20and%20BERT%20architectures%2C%20SCENE%0Aprovides%20valuable%20insights%20into%20the%20strengths%20and%20limitations%20of%20various%20XAI%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04575v1&entry.124074799=Read"},
{"title": "Dialectical Reconciliation via Structured Argumentative Dialogues", "author": "Stylianos Loukas Vasileiou and Ashwin Kumar and William Yeoh and Tran Cao Son and Francesca Toni", "abstract": "  We present a novel framework designed to extend model reconciliation\napproaches, commonly used in human-aware planning, for enhanced human-AI\ninteraction. By adopting a structured argumentation-based dialogue paradigm,\nour framework enables dialectical reconciliation to address knowledge\ndiscrepancies between an explainer (AI agent) and an explainee (human user),\nwhere the goal is for the explainee to understand the explainer's decision. We\nformally describe the operational semantics of our proposed framework,\nproviding theoretical guarantees. We then evaluate the framework's efficacy\n``in the wild'' via computational and human-subject experiments. Our findings\nsuggest that our framework offers a promising direction for fostering effective\nhuman-AI interactions in domains where explainability is important.\n", "link": "http://arxiv.org/abs/2306.14694v3", "date": "2024-08-08", "relevancy": 0.9051, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4951}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4392}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dialectical%20Reconciliation%20via%20Structured%20Argumentative%20Dialogues&body=Title%3A%20Dialectical%20Reconciliation%20via%20Structured%20Argumentative%20Dialogues%0AAuthor%3A%20Stylianos%20Loukas%20Vasileiou%20and%20Ashwin%20Kumar%20and%20William%20Yeoh%20and%20Tran%20Cao%20Son%20and%20Francesca%20Toni%0AAbstract%3A%20%20%20We%20present%20a%20novel%20framework%20designed%20to%20extend%20model%20reconciliation%0Aapproaches%2C%20commonly%20used%20in%20human-aware%20planning%2C%20for%20enhanced%20human-AI%0Ainteraction.%20By%20adopting%20a%20structured%20argumentation-based%20dialogue%20paradigm%2C%0Aour%20framework%20enables%20dialectical%20reconciliation%20to%20address%20knowledge%0Adiscrepancies%20between%20an%20explainer%20%28AI%20agent%29%20and%20an%20explainee%20%28human%20user%29%2C%0Awhere%20the%20goal%20is%20for%20the%20explainee%20to%20understand%20the%20explainer%27s%20decision.%20We%0Aformally%20describe%20the%20operational%20semantics%20of%20our%20proposed%20framework%2C%0Aproviding%20theoretical%20guarantees.%20We%20then%20evaluate%20the%20framework%27s%20efficacy%0A%60%60in%20the%20wild%27%27%20via%20computational%20and%20human-subject%20experiments.%20Our%20findings%0Asuggest%20that%20our%20framework%20offers%20a%20promising%20direction%20for%20fostering%20effective%0Ahuman-AI%20interactions%20in%20domains%20where%20explainability%20is%20important.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.14694v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDialectical%2520Reconciliation%2520via%2520Structured%2520Argumentative%2520Dialogues%26entry.906535625%3DStylianos%2520Loukas%2520Vasileiou%2520and%2520Ashwin%2520Kumar%2520and%2520William%2520Yeoh%2520and%2520Tran%2520Cao%2520Son%2520and%2520Francesca%2520Toni%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520framework%2520designed%2520to%2520extend%2520model%2520reconciliation%250Aapproaches%252C%2520commonly%2520used%2520in%2520human-aware%2520planning%252C%2520for%2520enhanced%2520human-AI%250Ainteraction.%2520By%2520adopting%2520a%2520structured%2520argumentation-based%2520dialogue%2520paradigm%252C%250Aour%2520framework%2520enables%2520dialectical%2520reconciliation%2520to%2520address%2520knowledge%250Adiscrepancies%2520between%2520an%2520explainer%2520%2528AI%2520agent%2529%2520and%2520an%2520explainee%2520%2528human%2520user%2529%252C%250Awhere%2520the%2520goal%2520is%2520for%2520the%2520explainee%2520to%2520understand%2520the%2520explainer%2527s%2520decision.%2520We%250Aformally%2520describe%2520the%2520operational%2520semantics%2520of%2520our%2520proposed%2520framework%252C%250Aproviding%2520theoretical%2520guarantees.%2520We%2520then%2520evaluate%2520the%2520framework%2527s%2520efficacy%250A%2560%2560in%2520the%2520wild%2527%2527%2520via%2520computational%2520and%2520human-subject%2520experiments.%2520Our%2520findings%250Asuggest%2520that%2520our%2520framework%2520offers%2520a%2520promising%2520direction%2520for%2520fostering%2520effective%250Ahuman-AI%2520interactions%2520in%2520domains%2520where%2520explainability%2520is%2520important.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.14694v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dialectical%20Reconciliation%20via%20Structured%20Argumentative%20Dialogues&entry.906535625=Stylianos%20Loukas%20Vasileiou%20and%20Ashwin%20Kumar%20and%20William%20Yeoh%20and%20Tran%20Cao%20Son%20and%20Francesca%20Toni&entry.1292438233=%20%20We%20present%20a%20novel%20framework%20designed%20to%20extend%20model%20reconciliation%0Aapproaches%2C%20commonly%20used%20in%20human-aware%20planning%2C%20for%20enhanced%20human-AI%0Ainteraction.%20By%20adopting%20a%20structured%20argumentation-based%20dialogue%20paradigm%2C%0Aour%20framework%20enables%20dialectical%20reconciliation%20to%20address%20knowledge%0Adiscrepancies%20between%20an%20explainer%20%28AI%20agent%29%20and%20an%20explainee%20%28human%20user%29%2C%0Awhere%20the%20goal%20is%20for%20the%20explainee%20to%20understand%20the%20explainer%27s%20decision.%20We%0Aformally%20describe%20the%20operational%20semantics%20of%20our%20proposed%20framework%2C%0Aproviding%20theoretical%20guarantees.%20We%20then%20evaluate%20the%20framework%27s%20efficacy%0A%60%60in%20the%20wild%27%27%20via%20computational%20and%20human-subject%20experiments.%20Our%20findings%0Asuggest%20that%20our%20framework%20offers%20a%20promising%20direction%20for%20fostering%20effective%0Ahuman-AI%20interactions%20in%20domains%20where%20explainability%20is%20important.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.14694v3&entry.124074799=Read"},
{"title": "Autonomous, Self-driving Multi-Step Growth of Semiconductor\n  Heterostructures Guided by Machine Learning", "author": "Chao Shen and Wenkang Zhan and Hongyu Sun and Kaiyao Xin and Bo Xu and Zhanguo Wang and Chao Zhao", "abstract": "  The semiconductor industry has prioritized automating repetitive tasks by\nclosed-loop, autonomous experimentation which enables accelerated optimization\nof complex multi-step processes. The emergence of machine learning (ML) has\nushered in automated process with minimal human intervention. In this work, we\ndevelop SemiEpi, a self-driving automation platform capable of executing\nmolecular beam epitaxy (MBE) growth with multi-steps, continuous in-situ\nmonitoring, and on-the-fly feedback control. By integrating standard hardware,\nhomemade software, curve fitting, and multiple ML models, SemiEpi operates\nautonomously, eliminating the need for extensive expertise in MBE processes to\nachieve optimal outcomes. The platform actively learns from previous\nexperimental results, identifying favorable conditions and proposing new\nexperiments to achieve the desired results. We standardize and optimize growth\nfor InAs/GaAs quantum dots (QDs) heterostructures to showcase the power of\nML-guided multi-step growth. A temperature calibration was implemented to get\nthe initial growth condition, and fine control of the process was executed\nusing ML. Leveraging RHEED movies acquired during the growth, SemiEpi\nsuccessfully identified and optimized a novel route for multi-step\nheterostructure growth. This work demonstrates the capabilities of closed-loop,\nML-guided systems in addressing challenges in multi-step growth for any device.\nOur method is critical to achieve repeatable materials growth using\ncommercially scalable tools. Our strategy facilitates the development of a\nhardware-independent process and enhancing process repeatability and stability,\neven without exhaustive knowledge of growth parameters.\n", "link": "http://arxiv.org/abs/2408.03508v2", "date": "2024-08-08", "relevancy": 1.3174, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.479}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4286}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autonomous%2C%20Self-driving%20Multi-Step%20Growth%20of%20Semiconductor%0A%20%20Heterostructures%20Guided%20by%20Machine%20Learning&body=Title%3A%20Autonomous%2C%20Self-driving%20Multi-Step%20Growth%20of%20Semiconductor%0A%20%20Heterostructures%20Guided%20by%20Machine%20Learning%0AAuthor%3A%20Chao%20Shen%20and%20Wenkang%20Zhan%20and%20Hongyu%20Sun%20and%20Kaiyao%20Xin%20and%20Bo%20Xu%20and%20Zhanguo%20Wang%20and%20Chao%20Zhao%0AAbstract%3A%20%20%20The%20semiconductor%20industry%20has%20prioritized%20automating%20repetitive%20tasks%20by%0Aclosed-loop%2C%20autonomous%20experimentation%20which%20enables%20accelerated%20optimization%0Aof%20complex%20multi-step%20processes.%20The%20emergence%20of%20machine%20learning%20%28ML%29%20has%0Aushered%20in%20automated%20process%20with%20minimal%20human%20intervention.%20In%20this%20work%2C%20we%0Adevelop%20SemiEpi%2C%20a%20self-driving%20automation%20platform%20capable%20of%20executing%0Amolecular%20beam%20epitaxy%20%28MBE%29%20growth%20with%20multi-steps%2C%20continuous%20in-situ%0Amonitoring%2C%20and%20on-the-fly%20feedback%20control.%20By%20integrating%20standard%20hardware%2C%0Ahomemade%20software%2C%20curve%20fitting%2C%20and%20multiple%20ML%20models%2C%20SemiEpi%20operates%0Aautonomously%2C%20eliminating%20the%20need%20for%20extensive%20expertise%20in%20MBE%20processes%20to%0Aachieve%20optimal%20outcomes.%20The%20platform%20actively%20learns%20from%20previous%0Aexperimental%20results%2C%20identifying%20favorable%20conditions%20and%20proposing%20new%0Aexperiments%20to%20achieve%20the%20desired%20results.%20We%20standardize%20and%20optimize%20growth%0Afor%20InAs/GaAs%20quantum%20dots%20%28QDs%29%20heterostructures%20to%20showcase%20the%20power%20of%0AML-guided%20multi-step%20growth.%20A%20temperature%20calibration%20was%20implemented%20to%20get%0Athe%20initial%20growth%20condition%2C%20and%20fine%20control%20of%20the%20process%20was%20executed%0Ausing%20ML.%20Leveraging%20RHEED%20movies%20acquired%20during%20the%20growth%2C%20SemiEpi%0Asuccessfully%20identified%20and%20optimized%20a%20novel%20route%20for%20multi-step%0Aheterostructure%20growth.%20This%20work%20demonstrates%20the%20capabilities%20of%20closed-loop%2C%0AML-guided%20systems%20in%20addressing%20challenges%20in%20multi-step%20growth%20for%20any%20device.%0AOur%20method%20is%20critical%20to%20achieve%20repeatable%20materials%20growth%20using%0Acommercially%20scalable%20tools.%20Our%20strategy%20facilitates%20the%20development%20of%20a%0Ahardware-independent%20process%20and%20enhancing%20process%20repeatability%20and%20stability%2C%0Aeven%20without%20exhaustive%20knowledge%20of%20growth%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.03508v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutonomous%252C%2520Self-driving%2520Multi-Step%2520Growth%2520of%2520Semiconductor%250A%2520%2520Heterostructures%2520Guided%2520by%2520Machine%2520Learning%26entry.906535625%3DChao%2520Shen%2520and%2520Wenkang%2520Zhan%2520and%2520Hongyu%2520Sun%2520and%2520Kaiyao%2520Xin%2520and%2520Bo%2520Xu%2520and%2520Zhanguo%2520Wang%2520and%2520Chao%2520Zhao%26entry.1292438233%3D%2520%2520The%2520semiconductor%2520industry%2520has%2520prioritized%2520automating%2520repetitive%2520tasks%2520by%250Aclosed-loop%252C%2520autonomous%2520experimentation%2520which%2520enables%2520accelerated%2520optimization%250Aof%2520complex%2520multi-step%2520processes.%2520The%2520emergence%2520of%2520machine%2520learning%2520%2528ML%2529%2520has%250Aushered%2520in%2520automated%2520process%2520with%2520minimal%2520human%2520intervention.%2520In%2520this%2520work%252C%2520we%250Adevelop%2520SemiEpi%252C%2520a%2520self-driving%2520automation%2520platform%2520capable%2520of%2520executing%250Amolecular%2520beam%2520epitaxy%2520%2528MBE%2529%2520growth%2520with%2520multi-steps%252C%2520continuous%2520in-situ%250Amonitoring%252C%2520and%2520on-the-fly%2520feedback%2520control.%2520By%2520integrating%2520standard%2520hardware%252C%250Ahomemade%2520software%252C%2520curve%2520fitting%252C%2520and%2520multiple%2520ML%2520models%252C%2520SemiEpi%2520operates%250Aautonomously%252C%2520eliminating%2520the%2520need%2520for%2520extensive%2520expertise%2520in%2520MBE%2520processes%2520to%250Aachieve%2520optimal%2520outcomes.%2520The%2520platform%2520actively%2520learns%2520from%2520previous%250Aexperimental%2520results%252C%2520identifying%2520favorable%2520conditions%2520and%2520proposing%2520new%250Aexperiments%2520to%2520achieve%2520the%2520desired%2520results.%2520We%2520standardize%2520and%2520optimize%2520growth%250Afor%2520InAs/GaAs%2520quantum%2520dots%2520%2528QDs%2529%2520heterostructures%2520to%2520showcase%2520the%2520power%2520of%250AML-guided%2520multi-step%2520growth.%2520A%2520temperature%2520calibration%2520was%2520implemented%2520to%2520get%250Athe%2520initial%2520growth%2520condition%252C%2520and%2520fine%2520control%2520of%2520the%2520process%2520was%2520executed%250Ausing%2520ML.%2520Leveraging%2520RHEED%2520movies%2520acquired%2520during%2520the%2520growth%252C%2520SemiEpi%250Asuccessfully%2520identified%2520and%2520optimized%2520a%2520novel%2520route%2520for%2520multi-step%250Aheterostructure%2520growth.%2520This%2520work%2520demonstrates%2520the%2520capabilities%2520of%2520closed-loop%252C%250AML-guided%2520systems%2520in%2520addressing%2520challenges%2520in%2520multi-step%2520growth%2520for%2520any%2520device.%250AOur%2520method%2520is%2520critical%2520to%2520achieve%2520repeatable%2520materials%2520growth%2520using%250Acommercially%2520scalable%2520tools.%2520Our%2520strategy%2520facilitates%2520the%2520development%2520of%2520a%250Ahardware-independent%2520process%2520and%2520enhancing%2520process%2520repeatability%2520and%2520stability%252C%250Aeven%2520without%2520exhaustive%2520knowledge%2520of%2520growth%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.03508v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autonomous%2C%20Self-driving%20Multi-Step%20Growth%20of%20Semiconductor%0A%20%20Heterostructures%20Guided%20by%20Machine%20Learning&entry.906535625=Chao%20Shen%20and%20Wenkang%20Zhan%20and%20Hongyu%20Sun%20and%20Kaiyao%20Xin%20and%20Bo%20Xu%20and%20Zhanguo%20Wang%20and%20Chao%20Zhao&entry.1292438233=%20%20The%20semiconductor%20industry%20has%20prioritized%20automating%20repetitive%20tasks%20by%0Aclosed-loop%2C%20autonomous%20experimentation%20which%20enables%20accelerated%20optimization%0Aof%20complex%20multi-step%20processes.%20The%20emergence%20of%20machine%20learning%20%28ML%29%20has%0Aushered%20in%20automated%20process%20with%20minimal%20human%20intervention.%20In%20this%20work%2C%20we%0Adevelop%20SemiEpi%2C%20a%20self-driving%20automation%20platform%20capable%20of%20executing%0Amolecular%20beam%20epitaxy%20%28MBE%29%20growth%20with%20multi-steps%2C%20continuous%20in-situ%0Amonitoring%2C%20and%20on-the-fly%20feedback%20control.%20By%20integrating%20standard%20hardware%2C%0Ahomemade%20software%2C%20curve%20fitting%2C%20and%20multiple%20ML%20models%2C%20SemiEpi%20operates%0Aautonomously%2C%20eliminating%20the%20need%20for%20extensive%20expertise%20in%20MBE%20processes%20to%0Aachieve%20optimal%20outcomes.%20The%20platform%20actively%20learns%20from%20previous%0Aexperimental%20results%2C%20identifying%20favorable%20conditions%20and%20proposing%20new%0Aexperiments%20to%20achieve%20the%20desired%20results.%20We%20standardize%20and%20optimize%20growth%0Afor%20InAs/GaAs%20quantum%20dots%20%28QDs%29%20heterostructures%20to%20showcase%20the%20power%20of%0AML-guided%20multi-step%20growth.%20A%20temperature%20calibration%20was%20implemented%20to%20get%0Athe%20initial%20growth%20condition%2C%20and%20fine%20control%20of%20the%20process%20was%20executed%0Ausing%20ML.%20Leveraging%20RHEED%20movies%20acquired%20during%20the%20growth%2C%20SemiEpi%0Asuccessfully%20identified%20and%20optimized%20a%20novel%20route%20for%20multi-step%0Aheterostructure%20growth.%20This%20work%20demonstrates%20the%20capabilities%20of%20closed-loop%2C%0AML-guided%20systems%20in%20addressing%20challenges%20in%20multi-step%20growth%20for%20any%20device.%0AOur%20method%20is%20critical%20to%20achieve%20repeatable%20materials%20growth%20using%0Acommercially%20scalable%20tools.%20Our%20strategy%20facilitates%20the%20development%20of%20a%0Ahardware-independent%20process%20and%20enhancing%20process%20repeatability%20and%20stability%2C%0Aeven%20without%20exhaustive%20knowledge%20of%20growth%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.03508v2&entry.124074799=Read"},
{"title": "Probabilistic energy forecasting through quantile regression in\n  reproducing kernel Hilbert spaces", "author": "Luca Pernigo and Rohan Sen and Davide Baroli", "abstract": "  Accurate energy demand forecasting is crucial for sustainable and resilient\nenergy development. To meet the Net Zero Representative Concentration Pathways\n(RCP) $4.5$ scenario in the DACH countries, increased renewable energy\nproduction, energy storage, and reduced commercial building consumption are\nneeded. This scenario's success depends on hydroelectric capacity and climatic\nfactors. Informed decisions require quantifying uncertainty in forecasts. This\nstudy explores a non-parametric method based on \\emph{reproducing kernel\nHilbert spaces (RKHS)}, known as kernel quantile regression, for energy\nprediction. Our experiments demonstrate its reliability and sharpness, and we\nbenchmark it against state-of-the-art methods in load and price forecasting for\nthe DACH region. We offer our implementation in conjunction with additional\nscripts to ensure the reproducibility of our research.\n", "link": "http://arxiv.org/abs/2408.04405v1", "date": "2024-08-08", "relevancy": 1.1459, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4413}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3674}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20energy%20forecasting%20through%20quantile%20regression%20in%0A%20%20reproducing%20kernel%20Hilbert%20spaces&body=Title%3A%20Probabilistic%20energy%20forecasting%20through%20quantile%20regression%20in%0A%20%20reproducing%20kernel%20Hilbert%20spaces%0AAuthor%3A%20Luca%20Pernigo%20and%20Rohan%20Sen%20and%20Davide%20Baroli%0AAbstract%3A%20%20%20Accurate%20energy%20demand%20forecasting%20is%20crucial%20for%20sustainable%20and%20resilient%0Aenergy%20development.%20To%20meet%20the%20Net%20Zero%20Representative%20Concentration%20Pathways%0A%28RCP%29%20%244.5%24%20scenario%20in%20the%20DACH%20countries%2C%20increased%20renewable%20energy%0Aproduction%2C%20energy%20storage%2C%20and%20reduced%20commercial%20building%20consumption%20are%0Aneeded.%20This%20scenario%27s%20success%20depends%20on%20hydroelectric%20capacity%20and%20climatic%0Afactors.%20Informed%20decisions%20require%20quantifying%20uncertainty%20in%20forecasts.%20This%0Astudy%20explores%20a%20non-parametric%20method%20based%20on%20%5Cemph%7Breproducing%20kernel%0AHilbert%20spaces%20%28RKHS%29%7D%2C%20known%20as%20kernel%20quantile%20regression%2C%20for%20energy%0Aprediction.%20Our%20experiments%20demonstrate%20its%20reliability%20and%20sharpness%2C%20and%20we%0Abenchmark%20it%20against%20state-of-the-art%20methods%20in%20load%20and%20price%20forecasting%20for%0Athe%20DACH%20region.%20We%20offer%20our%20implementation%20in%20conjunction%20with%20additional%0Ascripts%20to%20ensure%20the%20reproducibility%20of%20our%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04405v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520energy%2520forecasting%2520through%2520quantile%2520regression%2520in%250A%2520%2520reproducing%2520kernel%2520Hilbert%2520spaces%26entry.906535625%3DLuca%2520Pernigo%2520and%2520Rohan%2520Sen%2520and%2520Davide%2520Baroli%26entry.1292438233%3D%2520%2520Accurate%2520energy%2520demand%2520forecasting%2520is%2520crucial%2520for%2520sustainable%2520and%2520resilient%250Aenergy%2520development.%2520To%2520meet%2520the%2520Net%2520Zero%2520Representative%2520Concentration%2520Pathways%250A%2528RCP%2529%2520%25244.5%2524%2520scenario%2520in%2520the%2520DACH%2520countries%252C%2520increased%2520renewable%2520energy%250Aproduction%252C%2520energy%2520storage%252C%2520and%2520reduced%2520commercial%2520building%2520consumption%2520are%250Aneeded.%2520This%2520scenario%2527s%2520success%2520depends%2520on%2520hydroelectric%2520capacity%2520and%2520climatic%250Afactors.%2520Informed%2520decisions%2520require%2520quantifying%2520uncertainty%2520in%2520forecasts.%2520This%250Astudy%2520explores%2520a%2520non-parametric%2520method%2520based%2520on%2520%255Cemph%257Breproducing%2520kernel%250AHilbert%2520spaces%2520%2528RKHS%2529%257D%252C%2520known%2520as%2520kernel%2520quantile%2520regression%252C%2520for%2520energy%250Aprediction.%2520Our%2520experiments%2520demonstrate%2520its%2520reliability%2520and%2520sharpness%252C%2520and%2520we%250Abenchmark%2520it%2520against%2520state-of-the-art%2520methods%2520in%2520load%2520and%2520price%2520forecasting%2520for%250Athe%2520DACH%2520region.%2520We%2520offer%2520our%2520implementation%2520in%2520conjunction%2520with%2520additional%250Ascripts%2520to%2520ensure%2520the%2520reproducibility%2520of%2520our%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04405v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20energy%20forecasting%20through%20quantile%20regression%20in%0A%20%20reproducing%20kernel%20Hilbert%20spaces&entry.906535625=Luca%20Pernigo%20and%20Rohan%20Sen%20and%20Davide%20Baroli&entry.1292438233=%20%20Accurate%20energy%20demand%20forecasting%20is%20crucial%20for%20sustainable%20and%20resilient%0Aenergy%20development.%20To%20meet%20the%20Net%20Zero%20Representative%20Concentration%20Pathways%0A%28RCP%29%20%244.5%24%20scenario%20in%20the%20DACH%20countries%2C%20increased%20renewable%20energy%0Aproduction%2C%20energy%20storage%2C%20and%20reduced%20commercial%20building%20consumption%20are%0Aneeded.%20This%20scenario%27s%20success%20depends%20on%20hydroelectric%20capacity%20and%20climatic%0Afactors.%20Informed%20decisions%20require%20quantifying%20uncertainty%20in%20forecasts.%20This%0Astudy%20explores%20a%20non-parametric%20method%20based%20on%20%5Cemph%7Breproducing%20kernel%0AHilbert%20spaces%20%28RKHS%29%7D%2C%20known%20as%20kernel%20quantile%20regression%2C%20for%20energy%0Aprediction.%20Our%20experiments%20demonstrate%20its%20reliability%20and%20sharpness%2C%20and%20we%0Abenchmark%20it%20against%20state-of-the-art%20methods%20in%20load%20and%20price%20forecasting%20for%0Athe%20DACH%20region.%20We%20offer%20our%20implementation%20in%20conjunction%20with%20additional%0Ascripts%20to%20ensure%20the%20reproducibility%20of%20our%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04405v1&entry.124074799=Read"},
{"title": "Research Trends for the Interplay between Large Language Models and\n  Knowledge Graphs", "author": "Hanieh Khorashadizadeh and Fatima Zahra Amara and Morteza Ezzabady and Fr\u00e9d\u00e9ric Ieng and Sanju Tiwari and Nandana Mihindukulasooriya and Jinghua Groppe and Soror Sahri and Farah Benamara and Sven Groppe", "abstract": "  This survey investigates the synergistic relationship between Large Language\nModels (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's\ncapabilities in understanding, reasoning, and language processing. It aims to\naddress gaps in current research by exploring areas such as KG Question\nAnswering, ontology generation, KG validation, and the enhancement of KG\naccuracy and consistency through LLMs. The paper further examines the roles of\nLLMs in generating descriptive texts and natural language queries for KGs.\nThrough a structured analysis that includes categorizing LLM-KG interactions,\nexamining methodologies, and investigating collaborative uses and potential\nbiases, this study seeks to provide new insights into the combined potential of\nLLMs and KGs. It highlights the importance of their interaction for improving\nAI applications and outlines future research directions.\n", "link": "http://arxiv.org/abs/2406.08223v2", "date": "2024-08-08", "relevancy": 1.2814, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4532}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4289}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20Trends%20for%20the%20Interplay%20between%20Large%20Language%20Models%20and%0A%20%20Knowledge%20Graphs&body=Title%3A%20Research%20Trends%20for%20the%20Interplay%20between%20Large%20Language%20Models%20and%0A%20%20Knowledge%20Graphs%0AAuthor%3A%20Hanieh%20Khorashadizadeh%20and%20Fatima%20Zahra%20Amara%20and%20Morteza%20Ezzabady%20and%20Fr%C3%A9d%C3%A9ric%20Ieng%20and%20Sanju%20Tiwari%20and%20Nandana%20Mihindukulasooriya%20and%20Jinghua%20Groppe%20and%20Soror%20Sahri%20and%20Farah%20Benamara%20and%20Sven%20Groppe%0AAbstract%3A%20%20%20This%20survey%20investigates%20the%20synergistic%20relationship%20between%20Large%20Language%0AModels%20%28LLMs%29%20and%20Knowledge%20Graphs%20%28KGs%29%2C%20which%20is%20crucial%20for%20advancing%20AI%27s%0Acapabilities%20in%20understanding%2C%20reasoning%2C%20and%20language%20processing.%20It%20aims%20to%0Aaddress%20gaps%20in%20current%20research%20by%20exploring%20areas%20such%20as%20KG%20Question%0AAnswering%2C%20ontology%20generation%2C%20KG%20validation%2C%20and%20the%20enhancement%20of%20KG%0Aaccuracy%20and%20consistency%20through%20LLMs.%20The%20paper%20further%20examines%20the%20roles%20of%0ALLMs%20in%20generating%20descriptive%20texts%20and%20natural%20language%20queries%20for%20KGs.%0AThrough%20a%20structured%20analysis%20that%20includes%20categorizing%20LLM-KG%20interactions%2C%0Aexamining%20methodologies%2C%20and%20investigating%20collaborative%20uses%20and%20potential%0Abiases%2C%20this%20study%20seeks%20to%20provide%20new%20insights%20into%20the%20combined%20potential%20of%0ALLMs%20and%20KGs.%20It%20highlights%20the%20importance%20of%20their%20interaction%20for%20improving%0AAI%20applications%20and%20outlines%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08223v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520Trends%2520for%2520the%2520Interplay%2520between%2520Large%2520Language%2520Models%2520and%250A%2520%2520Knowledge%2520Graphs%26entry.906535625%3DHanieh%2520Khorashadizadeh%2520and%2520Fatima%2520Zahra%2520Amara%2520and%2520Morteza%2520Ezzabady%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Ieng%2520and%2520Sanju%2520Tiwari%2520and%2520Nandana%2520Mihindukulasooriya%2520and%2520Jinghua%2520Groppe%2520and%2520Soror%2520Sahri%2520and%2520Farah%2520Benamara%2520and%2520Sven%2520Groppe%26entry.1292438233%3D%2520%2520This%2520survey%2520investigates%2520the%2520synergistic%2520relationship%2520between%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520and%2520Knowledge%2520Graphs%2520%2528KGs%2529%252C%2520which%2520is%2520crucial%2520for%2520advancing%2520AI%2527s%250Acapabilities%2520in%2520understanding%252C%2520reasoning%252C%2520and%2520language%2520processing.%2520It%2520aims%2520to%250Aaddress%2520gaps%2520in%2520current%2520research%2520by%2520exploring%2520areas%2520such%2520as%2520KG%2520Question%250AAnswering%252C%2520ontology%2520generation%252C%2520KG%2520validation%252C%2520and%2520the%2520enhancement%2520of%2520KG%250Aaccuracy%2520and%2520consistency%2520through%2520LLMs.%2520The%2520paper%2520further%2520examines%2520the%2520roles%2520of%250ALLMs%2520in%2520generating%2520descriptive%2520texts%2520and%2520natural%2520language%2520queries%2520for%2520KGs.%250AThrough%2520a%2520structured%2520analysis%2520that%2520includes%2520categorizing%2520LLM-KG%2520interactions%252C%250Aexamining%2520methodologies%252C%2520and%2520investigating%2520collaborative%2520uses%2520and%2520potential%250Abiases%252C%2520this%2520study%2520seeks%2520to%2520provide%2520new%2520insights%2520into%2520the%2520combined%2520potential%2520of%250ALLMs%2520and%2520KGs.%2520It%2520highlights%2520the%2520importance%2520of%2520their%2520interaction%2520for%2520improving%250AAI%2520applications%2520and%2520outlines%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08223v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20Trends%20for%20the%20Interplay%20between%20Large%20Language%20Models%20and%0A%20%20Knowledge%20Graphs&entry.906535625=Hanieh%20Khorashadizadeh%20and%20Fatima%20Zahra%20Amara%20and%20Morteza%20Ezzabady%20and%20Fr%C3%A9d%C3%A9ric%20Ieng%20and%20Sanju%20Tiwari%20and%20Nandana%20Mihindukulasooriya%20and%20Jinghua%20Groppe%20and%20Soror%20Sahri%20and%20Farah%20Benamara%20and%20Sven%20Groppe&entry.1292438233=%20%20This%20survey%20investigates%20the%20synergistic%20relationship%20between%20Large%20Language%0AModels%20%28LLMs%29%20and%20Knowledge%20Graphs%20%28KGs%29%2C%20which%20is%20crucial%20for%20advancing%20AI%27s%0Acapabilities%20in%20understanding%2C%20reasoning%2C%20and%20language%20processing.%20It%20aims%20to%0Aaddress%20gaps%20in%20current%20research%20by%20exploring%20areas%20such%20as%20KG%20Question%0AAnswering%2C%20ontology%20generation%2C%20KG%20validation%2C%20and%20the%20enhancement%20of%20KG%0Aaccuracy%20and%20consistency%20through%20LLMs.%20The%20paper%20further%20examines%20the%20roles%20of%0ALLMs%20in%20generating%20descriptive%20texts%20and%20natural%20language%20queries%20for%20KGs.%0AThrough%20a%20structured%20analysis%20that%20includes%20categorizing%20LLM-KG%20interactions%2C%0Aexamining%20methodologies%2C%20and%20investigating%20collaborative%20uses%20and%20potential%0Abiases%2C%20this%20study%20seeks%20to%20provide%20new%20insights%20into%20the%20combined%20potential%20of%0ALLMs%20and%20KGs.%20It%20highlights%20the%20importance%20of%20their%20interaction%20for%20improving%0AAI%20applications%20and%20outlines%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08223v2&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning for the Design of Metamaterial Mechanisms\n  with Functional Compliance Control", "author": "Yejun Choi and Yeoneung Kim and Keun Park", "abstract": "  Metamaterial mechanisms are micro-architectured compliant structures that\noperate through the elastic deformation of specially designed flexible members.\nThis study develops an efficient design methodology for compliant mechanisms\nusing deep reinforcement learning (RL). For this purpose, design domains are\ndigitized into finite cells with various hinge connections, and finite element\nanalyses (FEAs) are conducted to evaluate the deformation behaviors of the\ncompliance mechanism with different cell combinations. The FEA data are learned\nthrough the RL method to obtain optimal compliant mechanisms for desired\nfunctional requirements. The RL algorithm is applied to the design of a\ncompliant door-latch mechanism, exploring the effect of human guidance and\ntiling direction. The optimal result is achieved with minimal human guidance\nand inward tiling, resulting in a threefold increase in the predefined reward\ncompared to human-designed mechanisms. The proposed approach is extended to the\ndesign of a soft gripper mechanism, where the effect of hinge connections is\nadditionally considered. The optimal design under hinge penalization reveals\nremarkably enhanced compliance, and its performance is validated by\nexperimental tests using an additively manufactured gripper. These findings\ndemonstrate that RL-optimized designs outperform those developed with human\ninsight, providing an efficient design methodology for cell-based compliant\nmechanisms in practical applications.\n", "link": "http://arxiv.org/abs/2408.04376v1", "date": "2024-08-08", "relevancy": 0.9613, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5094}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4772}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%20for%20the%20Design%20of%20Metamaterial%20Mechanisms%0A%20%20with%20Functional%20Compliance%20Control&body=Title%3A%20Deep%20Reinforcement%20Learning%20for%20the%20Design%20of%20Metamaterial%20Mechanisms%0A%20%20with%20Functional%20Compliance%20Control%0AAuthor%3A%20Yejun%20Choi%20and%20Yeoneung%20Kim%20and%20Keun%20Park%0AAbstract%3A%20%20%20Metamaterial%20mechanisms%20are%20micro-architectured%20compliant%20structures%20that%0Aoperate%20through%20the%20elastic%20deformation%20of%20specially%20designed%20flexible%20members.%0AThis%20study%20develops%20an%20efficient%20design%20methodology%20for%20compliant%20mechanisms%0Ausing%20deep%20reinforcement%20learning%20%28RL%29.%20For%20this%20purpose%2C%20design%20domains%20are%0Adigitized%20into%20finite%20cells%20with%20various%20hinge%20connections%2C%20and%20finite%20element%0Aanalyses%20%28FEAs%29%20are%20conducted%20to%20evaluate%20the%20deformation%20behaviors%20of%20the%0Acompliance%20mechanism%20with%20different%20cell%20combinations.%20The%20FEA%20data%20are%20learned%0Athrough%20the%20RL%20method%20to%20obtain%20optimal%20compliant%20mechanisms%20for%20desired%0Afunctional%20requirements.%20The%20RL%20algorithm%20is%20applied%20to%20the%20design%20of%20a%0Acompliant%20door-latch%20mechanism%2C%20exploring%20the%20effect%20of%20human%20guidance%20and%0Atiling%20direction.%20The%20optimal%20result%20is%20achieved%20with%20minimal%20human%20guidance%0Aand%20inward%20tiling%2C%20resulting%20in%20a%20threefold%20increase%20in%20the%20predefined%20reward%0Acompared%20to%20human-designed%20mechanisms.%20The%20proposed%20approach%20is%20extended%20to%20the%0Adesign%20of%20a%20soft%20gripper%20mechanism%2C%20where%20the%20effect%20of%20hinge%20connections%20is%0Aadditionally%20considered.%20The%20optimal%20design%20under%20hinge%20penalization%20reveals%0Aremarkably%20enhanced%20compliance%2C%20and%20its%20performance%20is%20validated%20by%0Aexperimental%20tests%20using%20an%20additively%20manufactured%20gripper.%20These%20findings%0Ademonstrate%20that%20RL-optimized%20designs%20outperform%20those%20developed%20with%20human%0Ainsight%2C%20providing%20an%20efficient%20design%20methodology%20for%20cell-based%20compliant%0Amechanisms%20in%20practical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04376v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning%2520for%2520the%2520Design%2520of%2520Metamaterial%2520Mechanisms%250A%2520%2520with%2520Functional%2520Compliance%2520Control%26entry.906535625%3DYejun%2520Choi%2520and%2520Yeoneung%2520Kim%2520and%2520Keun%2520Park%26entry.1292438233%3D%2520%2520Metamaterial%2520mechanisms%2520are%2520micro-architectured%2520compliant%2520structures%2520that%250Aoperate%2520through%2520the%2520elastic%2520deformation%2520of%2520specially%2520designed%2520flexible%2520members.%250AThis%2520study%2520develops%2520an%2520efficient%2520design%2520methodology%2520for%2520compliant%2520mechanisms%250Ausing%2520deep%2520reinforcement%2520learning%2520%2528RL%2529.%2520For%2520this%2520purpose%252C%2520design%2520domains%2520are%250Adigitized%2520into%2520finite%2520cells%2520with%2520various%2520hinge%2520connections%252C%2520and%2520finite%2520element%250Aanalyses%2520%2528FEAs%2529%2520are%2520conducted%2520to%2520evaluate%2520the%2520deformation%2520behaviors%2520of%2520the%250Acompliance%2520mechanism%2520with%2520different%2520cell%2520combinations.%2520The%2520FEA%2520data%2520are%2520learned%250Athrough%2520the%2520RL%2520method%2520to%2520obtain%2520optimal%2520compliant%2520mechanisms%2520for%2520desired%250Afunctional%2520requirements.%2520The%2520RL%2520algorithm%2520is%2520applied%2520to%2520the%2520design%2520of%2520a%250Acompliant%2520door-latch%2520mechanism%252C%2520exploring%2520the%2520effect%2520of%2520human%2520guidance%2520and%250Atiling%2520direction.%2520The%2520optimal%2520result%2520is%2520achieved%2520with%2520minimal%2520human%2520guidance%250Aand%2520inward%2520tiling%252C%2520resulting%2520in%2520a%2520threefold%2520increase%2520in%2520the%2520predefined%2520reward%250Acompared%2520to%2520human-designed%2520mechanisms.%2520The%2520proposed%2520approach%2520is%2520extended%2520to%2520the%250Adesign%2520of%2520a%2520soft%2520gripper%2520mechanism%252C%2520where%2520the%2520effect%2520of%2520hinge%2520connections%2520is%250Aadditionally%2520considered.%2520The%2520optimal%2520design%2520under%2520hinge%2520penalization%2520reveals%250Aremarkably%2520enhanced%2520compliance%252C%2520and%2520its%2520performance%2520is%2520validated%2520by%250Aexperimental%2520tests%2520using%2520an%2520additively%2520manufactured%2520gripper.%2520These%2520findings%250Ademonstrate%2520that%2520RL-optimized%2520designs%2520outperform%2520those%2520developed%2520with%2520human%250Ainsight%252C%2520providing%2520an%2520efficient%2520design%2520methodology%2520for%2520cell-based%2520compliant%250Amechanisms%2520in%2520practical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04376v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20for%20the%20Design%20of%20Metamaterial%20Mechanisms%0A%20%20with%20Functional%20Compliance%20Control&entry.906535625=Yejun%20Choi%20and%20Yeoneung%20Kim%20and%20Keun%20Park&entry.1292438233=%20%20Metamaterial%20mechanisms%20are%20micro-architectured%20compliant%20structures%20that%0Aoperate%20through%20the%20elastic%20deformation%20of%20specially%20designed%20flexible%20members.%0AThis%20study%20develops%20an%20efficient%20design%20methodology%20for%20compliant%20mechanisms%0Ausing%20deep%20reinforcement%20learning%20%28RL%29.%20For%20this%20purpose%2C%20design%20domains%20are%0Adigitized%20into%20finite%20cells%20with%20various%20hinge%20connections%2C%20and%20finite%20element%0Aanalyses%20%28FEAs%29%20are%20conducted%20to%20evaluate%20the%20deformation%20behaviors%20of%20the%0Acompliance%20mechanism%20with%20different%20cell%20combinations.%20The%20FEA%20data%20are%20learned%0Athrough%20the%20RL%20method%20to%20obtain%20optimal%20compliant%20mechanisms%20for%20desired%0Afunctional%20requirements.%20The%20RL%20algorithm%20is%20applied%20to%20the%20design%20of%20a%0Acompliant%20door-latch%20mechanism%2C%20exploring%20the%20effect%20of%20human%20guidance%20and%0Atiling%20direction.%20The%20optimal%20result%20is%20achieved%20with%20minimal%20human%20guidance%0Aand%20inward%20tiling%2C%20resulting%20in%20a%20threefold%20increase%20in%20the%20predefined%20reward%0Acompared%20to%20human-designed%20mechanisms.%20The%20proposed%20approach%20is%20extended%20to%20the%0Adesign%20of%20a%20soft%20gripper%20mechanism%2C%20where%20the%20effect%20of%20hinge%20connections%20is%0Aadditionally%20considered.%20The%20optimal%20design%20under%20hinge%20penalization%20reveals%0Aremarkably%20enhanced%20compliance%2C%20and%20its%20performance%20is%20validated%20by%0Aexperimental%20tests%20using%20an%20additively%20manufactured%20gripper.%20These%20findings%0Ademonstrate%20that%20RL-optimized%20designs%20outperform%20those%20developed%20with%20human%0Ainsight%2C%20providing%20an%20efficient%20design%20methodology%20for%20cell-based%20compliant%0Amechanisms%20in%20practical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04376v1&entry.124074799=Read"},
{"title": "Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon", "author": "Jiang You and Arben Cela and Ren\u00e9 Natowicz and Jacob Ouanounou and Patrick Siarry", "abstract": "  Detecting anomalies in time series data is a critical challenge across\nvarious domains. Traditional methods typically focus on identifying anomalies\nin immediate subsequent steps, often underestimating the significance of\ntemporal dynamics such as delay time and horizons of anomalies, which generally\nrequire extensive post-analysis. This paper introduces a novel approach for\ntime series anomaly prediction, incorporating temporal information directly\ninto the prediction results. We propose a new dataset specifically designed to\nevaluate this approach and conduct comprehensive experiments using several\nstate-of-the-art methods. results demonstrate the efficacy of our approach in\nproviding timely and accurate anomaly predictions, setting a new benchmark for\nfuture research in this field.\n", "link": "http://arxiv.org/abs/2408.04377v1", "date": "2024-08-08", "relevancy": 1.4187, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4629}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomaly%20Prediction%3A%20A%20Novel%20Approach%20with%20Explicit%20Delay%20and%20Horizon&body=Title%3A%20Anomaly%20Prediction%3A%20A%20Novel%20Approach%20with%20Explicit%20Delay%20and%20Horizon%0AAuthor%3A%20Jiang%20You%20and%20Arben%20Cela%20and%20Ren%C3%A9%20Natowicz%20and%20Jacob%20Ouanounou%20and%20Patrick%20Siarry%0AAbstract%3A%20%20%20Detecting%20anomalies%20in%20time%20series%20data%20is%20a%20critical%20challenge%20across%0Avarious%20domains.%20Traditional%20methods%20typically%20focus%20on%20identifying%20anomalies%0Ain%20immediate%20subsequent%20steps%2C%20often%20underestimating%20the%20significance%20of%0Atemporal%20dynamics%20such%20as%20delay%20time%20and%20horizons%20of%20anomalies%2C%20which%20generally%0Arequire%20extensive%20post-analysis.%20This%20paper%20introduces%20a%20novel%20approach%20for%0Atime%20series%20anomaly%20prediction%2C%20incorporating%20temporal%20information%20directly%0Ainto%20the%20prediction%20results.%20We%20propose%20a%20new%20dataset%20specifically%20designed%20to%0Aevaluate%20this%20approach%20and%20conduct%20comprehensive%20experiments%20using%20several%0Astate-of-the-art%20methods.%20results%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%0Aproviding%20timely%20and%20accurate%20anomaly%20predictions%2C%20setting%20a%20new%20benchmark%20for%0Afuture%20research%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04377v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomaly%2520Prediction%253A%2520A%2520Novel%2520Approach%2520with%2520Explicit%2520Delay%2520and%2520Horizon%26entry.906535625%3DJiang%2520You%2520and%2520Arben%2520Cela%2520and%2520Ren%25C3%25A9%2520Natowicz%2520and%2520Jacob%2520Ouanounou%2520and%2520Patrick%2520Siarry%26entry.1292438233%3D%2520%2520Detecting%2520anomalies%2520in%2520time%2520series%2520data%2520is%2520a%2520critical%2520challenge%2520across%250Avarious%2520domains.%2520Traditional%2520methods%2520typically%2520focus%2520on%2520identifying%2520anomalies%250Ain%2520immediate%2520subsequent%2520steps%252C%2520often%2520underestimating%2520the%2520significance%2520of%250Atemporal%2520dynamics%2520such%2520as%2520delay%2520time%2520and%2520horizons%2520of%2520anomalies%252C%2520which%2520generally%250Arequire%2520extensive%2520post-analysis.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520for%250Atime%2520series%2520anomaly%2520prediction%252C%2520incorporating%2520temporal%2520information%2520directly%250Ainto%2520the%2520prediction%2520results.%2520We%2520propose%2520a%2520new%2520dataset%2520specifically%2520designed%2520to%250Aevaluate%2520this%2520approach%2520and%2520conduct%2520comprehensive%2520experiments%2520using%2520several%250Astate-of-the-art%2520methods.%2520results%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach%2520in%250Aproviding%2520timely%2520and%2520accurate%2520anomaly%2520predictions%252C%2520setting%2520a%2520new%2520benchmark%2520for%250Afuture%2520research%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04377v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomaly%20Prediction%3A%20A%20Novel%20Approach%20with%20Explicit%20Delay%20and%20Horizon&entry.906535625=Jiang%20You%20and%20Arben%20Cela%20and%20Ren%C3%A9%20Natowicz%20and%20Jacob%20Ouanounou%20and%20Patrick%20Siarry&entry.1292438233=%20%20Detecting%20anomalies%20in%20time%20series%20data%20is%20a%20critical%20challenge%20across%0Avarious%20domains.%20Traditional%20methods%20typically%20focus%20on%20identifying%20anomalies%0Ain%20immediate%20subsequent%20steps%2C%20often%20underestimating%20the%20significance%20of%0Atemporal%20dynamics%20such%20as%20delay%20time%20and%20horizons%20of%20anomalies%2C%20which%20generally%0Arequire%20extensive%20post-analysis.%20This%20paper%20introduces%20a%20novel%20approach%20for%0Atime%20series%20anomaly%20prediction%2C%20incorporating%20temporal%20information%20directly%0Ainto%20the%20prediction%20results.%20We%20propose%20a%20new%20dataset%20specifically%20designed%20to%0Aevaluate%20this%20approach%20and%20conduct%20comprehensive%20experiments%20using%20several%0Astate-of-the-art%20methods.%20results%20demonstrate%20the%20efficacy%20of%20our%20approach%20in%0Aproviding%20timely%20and%20accurate%20anomaly%20predictions%2C%20setting%20a%20new%20benchmark%20for%0Afuture%20research%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04377v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


